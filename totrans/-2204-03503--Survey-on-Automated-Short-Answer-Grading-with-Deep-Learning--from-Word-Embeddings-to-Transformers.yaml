- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:47:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:47:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2204.03503] Survey on Automated Short Answer Grading with Deep Learning: from
    Word Embeddings to Transformers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2204.03503] 使用深度学习的自动化简答题评分调查：从词嵌入到变换器'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2204.03503](https://ar5iv.labs.arxiv.org/html/2204.03503)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2204.03503](https://ar5iv.labs.arxiv.org/html/2204.03503)
- en: 'Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings
    to Transformers'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习的自动化简答题评分调查：从词嵌入到变换器
- en: Stefan Haller University of TwenteEnschedeThe Netherlands [s.m.haller@student.utwente.nl](mailto:s.m.haller@student.utwente.nl)
    ,  Adina Aldea University of TwenteEnschedeThe Netherlands [a.aldea@utwente.nl](mailto:a.aldea@utwente.nl)
    ,  Christin Seifert University of TwenteEnschedeThe Netherlands - University of
    Duisburg-EssenEssenGermany [christin.seifert@uni-due.de](mailto:christin.seifert@uni-due.de)
     and  Nicola Strisciuglio University of TwenteEnschedeThe Netherlands [n.strisciuglio@utwente.nl](mailto:n.strisciuglio@utwente.nl)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Stefan Haller 荷兰特文特大学恩斯赫德 [s.m.haller@student.utwente.nl](mailto:s.m.haller@student.utwente.nl)，
    Adina Aldea 荷兰特文特大学恩斯赫德 [a.aldea@utwente.nl](mailto:a.aldea@utwente.nl)， Christin
    Seifert 荷兰特文特大学恩斯赫德 - 德国杜伊斯堡-埃森大学埃森 [christin.seifert@uni-due.de](mailto:christin.seifert@uni-due.de)
    和 Nicola Strisciuglio 荷兰特文特大学恩斯赫德 [n.strisciuglio@utwente.nl](mailto:n.strisciuglio@utwente.nl)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Automated short answer grading (ASAG) has gained attention in education as
    a means to scale educational tasks to the growing number of students. Recent progress
    in Natural Language Processing and Machine Learning has largely influenced the
    field of ASAG, of which we survey the recent research advancements. We complement
    previous surveys by providing a comprehensive analysis of recently published methods
    that deploy deep learning approaches. In particular, we focus our analysis on
    the transition from hand-engineered features to *representation learning* approaches,
    which learn representative features for the task at hand automatically from large
    corpora of data. We structure our analysis of deep learning methods along three
    categories: word embeddings, sequential models, and attention-based methods. Deep
    learning impacted ASAG differently than other fields of NLP, as we noticed that
    the learned representations alone do not contribute to achieve the best results,
    but they rather show to work in a complementary way with hand-engineered features.
    The best performance are indeed achieved by methods that combine the carefully
    hand-engineered features with the power of the semantic descriptions provided
    by the latest models, like transformers architectures. We identify challenges
    and provide an outlook on research direction that can be addressed in the future.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化简答题评分（ASAG）在教育领域引起了关注，因为它能够应对不断增长的学生数量。自然语言处理和机器学习的最新进展在很大程度上影响了ASAG领域，我们对最近的研究进展进行了调查。我们通过提供对最近发布的采用深度学习方法的全面分析，来补充之前的调查。特别是，我们将分析重点放在从手工设计特征到*表示学习*方法的过渡上，这些方法可以从大规模数据集中自动学习代表性特征。我们将对深度学习方法的分析分为三类：词嵌入、序列模型和基于注意力的方法。深度学习对ASAG的影响与其他NLP领域不同，因为我们发现仅凭学习到的表示不能获得最佳结果，而是与手工设计的特征以互补方式工作。最佳性能确实是通过将精心设计的特征与最新模型（如变换器架构）提供的语义描述相结合的方法来实现的。我们确定了挑战，并对未来的研究方向提供了展望。
- en: deep learning, machine learning, natural language processing, short answer grading,
    text representation learning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、机器学习、自然语言处理、简答题评分、文本表示学习
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: In the education field, large attention is dedicated to characterizing the learning
    process of students to determine the efficiency and success of learners in acquiring
    new knowledge. Assessing and quantifying the knowledge gain are fundamental aspects
    for evaluating the quality of the learning process. Due to the time-consuming
    and individual nature of the assessment process, it is crucial to render it as
    efficient as possible, e.g. by using some level of grading automation, while retaining
    or even improving its quality (Mohler and Mihalcea, [2009](#bib.bib45)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在教育领域，大量关注被投入到学生学习过程的特征化，以确定学习者获取新知识的效率和成功。评估和量化知识获取是评估学习过程质量的基本方面。由于评估过程耗时且具有个体性，因此必须尽可能提高其效率，例如，通过使用某种程度的评分自动化，同时保持或甚至提高其质量（Mohler和Mihalcea，[2009](#bib.bib45)）。
- en: 'Besides formative assessment (e.g. teacher feedback of assignments), the main
    method of assessment is via summative assessment (e.g. written examinations) (Suzen
    et al., [2018](#bib.bib70)), where knowledge acquired by the students can be tested
    in several ways. Different types of questions can be formulated: closed-form questions
    (e.g. multiple-choice) or open answer questions (e.g. essays or short answers) (Burrows
    et al., [2015](#bib.bib3)). Automated grading for multiple-choice questions is
    straightforward and immediate. Automated Essay Scoring (AES) and Automated Short
    Answer Grading (ASAG) are more challenging tasks because the assessment of these
    answers requires an understanding of the text and a more detailed analysis (Magooda
    et al., [2016](#bib.bib40); Suzen et al., [2018](#bib.bib70)). The main difference
    between AES and ASAG is that the latter deals with short answers usually graded
    against a reference answer, whereas the former is concerned with scoring longer
    textual answers and the grading is based on evaluating the quality in terms of
    spelling, grammar and coherence, and less on compact information content (Magooda
    et al., [2016](#bib.bib40)).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了形成性评估（例如作业的教师反馈），主要的评估方法是总结性评估（例如书面考试）（Suzen et al., [2018](#bib.bib70)），在这种方法中，学生获得的知识可以通过多种方式进行测试。可以制定不同类型的问题：封闭式问题（例如选择题）或开放式回答问题（例如作文或简答题）（Burrows
    et al., [2015](#bib.bib3)）。选择题的自动评分是直接且即时的。自动化作文评分（AES）和自动化简答题评分（ASAG）是更具挑战性的任务，因为这些答案的评估需要理解文本并进行更详细的分析（Magooda
    et al., [2016](#bib.bib40); Suzen et al., [2018](#bib.bib70)）。AES和ASAG之间的主要区别在于，后者处理的是通常与参考答案进行评分的简短回答，而前者则关注对较长文本答案的评分，评分基于拼写、语法和连贯性的质量评估，而不是信息内容的紧凑性（Magooda
    et al., [2016](#bib.bib40)）。
- en: The assessment of textual open answers is challenging and requires techniques
    and approaches from different fields, such as natural language processing (NLP),
    text understanding, and reading comprehension. Recent progress in Deep Learning
    and NLP facilitated the design and analysis of machine learning models for textual
    data (Suzen et al., [2018](#bib.bib70); Neterer and Guzide, [2018](#bib.bib48))
    and raised the question of their applicability to different application areas,
    such as AES and ASAG. These advancements and their application to ASAG have not
    been covered in recent surveys, which only reviewed methods and approaches based
    on feature extraction used in combination with classical machine learning models (Burrows
    et al., [2015](#bib.bib3); Galhardi and Brancher, [2018](#bib.bib20); Blessing
    et al., [2021](#bib.bib2)). This research focuses solely on ASAG, for which it
    is fundamental to construct an effective semantic representation of the answers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本开放性回答的评估具有挑战性，需要运用来自不同领域的技术和方法，如自然语言处理（NLP）、文本理解和阅读理解。深度学习和NLP的最新进展促进了针对文本数据的机器学习模型的设计和分析（Suzen
    et al., [2018](#bib.bib70); Neterer and Guzide, [2018](#bib.bib48)），并引发了它们在不同应用领域（如AES和ASAG）的适用性问题。这些进展及其在ASAG中的应用在最近的综述中没有涉及，这些综述仅回顾了基于特征提取并与经典机器学习模型结合使用的方法（Burrows
    et al., [2015](#bib.bib3); Galhardi and Brancher, [2018](#bib.bib20); Blessing
    et al., [2021](#bib.bib2)）。本研究专注于ASAG，对于ASAG而言，构建有效的语义表示是至关重要的。
- en: In this paper, we extend previous surveys by analyzing recently published methods
    for ASAG based on deep learning. We first provide a historical perspective with
    an overview of the changes and developments in the field over time and subsequently
    dive deeper into the contributions of the most recent methods. We pay attention
    to the role of text representations, i.e. features, their importance to effectively
    describe the characteristics of sentences and paragraphs, and the shift from hand-engineered
    to automatically learned features determined by the use of deep learning methodologies.
    We organize the works following a simple taxonomy that focuses on the type of
    text representation used. It includes methods based on *classical machine learning*
    , which combine hand-engineered features with classifiers, and *deep learning*
    methods that are able to learn relevant features directly from training data.
    We group the methods in the first category according to the type of features,
    namely lexical, syntactic, and semantic features. The methods in the second group,
    deep learning-based methods, are organized according to the mechanisms used to
    learn textual representation, such as word-embeddings, sequential models, and
    attention-based models. Further, we identify trends in the design of the network
    architectures and provide an outlook on future developments and challenges that
    need to be addressed, such as improving the textual understanding in cross-language
    settings and the generalization of actual methods to work with cross-domain texts
    (e.g. for different disciplines).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们通过分析基于深度学习的最新ASAG方法，扩展了之前的调研。我们首先从历史角度出发，概述了该领域随时间变化和发展的情况，然后深入探讨了最新方法的贡献。我们关注文本表示的作用，即特征，它们在有效描述句子和段落特征方面的重要性，以及从手工设计特征到通过深度学习方法自动学习特征的转变。我们按照一个简单的分类法组织这些工作，该分类法重点关注使用的文本表示类型。它包括基于*经典机器学习*的方法，这些方法将手工设计的特征与分类器相结合，以及*深度学习*方法，这些方法能够直接从训练数据中学习相关特征。我们根据特征类型将第一类方法分组，即词汇特征、句法特征和语义特征。第二类方法，即基于深度学习的方法，则根据用于学习文本表示的机制进行组织，例如词嵌入、序列模型和基于注意力的模型。此外，我们还识别了网络架构设计中的趋势，并对未来的发展和需要解决的挑战提供了展望，如在跨语言环境中改进文本理解以及实际方法在跨领域文本（例如不同学科）中的泛化。
- en: 'The remainder of the article is organized as follows. In Section [2](#S2 "2\.
    Contributions ‣ Survey on Automated Short Answer Grading with Deep Learning: from
    Word Embeddings to Transformers") we discuss the contributions of the present
    work with respect to previous survey approaches, while in Section [3](#S3 "3\.
    Historical perspective ‣ Survey on Automated Short Answer Grading with Deep Learning:
    from Word Embeddings to Transformers") we provide a view of the historical developments
    related to the research field of ASAG. In Section [4](#S4 "4\. Organization of
    the review ‣ Survey on Automated Short Answer Grading with Deep Learning: from
    Word Embeddings to Transformers"), we elaborate on the scope and organization
    of the survey, while in Section [5](#S5 "5\. Benchmark data sets for Short Answer
    Grading ‣ Survey on Automated Short Answer Grading with Deep Learning: from Word
    Embeddings to Transformers"), we present the details of the most used benchmark
    data sets. In Section [6](#S6 "6\. Taxonomy ‣ Survey on Automated Short Answer
    Grading with Deep Learning: from Word Embeddings to Transformers"), we provide
    a taxonomy for the organization of the existing works. In Section [7](#S7 "7\.
    Hand-engineered features and Machine Learning ‣ Survey on Automated Short Answer
    Grading with Deep Learning: from Word Embeddings to Transformers") and Section [8](#S8
    "8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers"), we analyze the works based on
    classical machine learning and deep learning approaches, respectively. We discuss
    actual trends and provide an outlook for future developments in the field of ASAG
    in Section [9](#S9 "9\. Discussion ‣ Survey on Automated Short Answer Grading
    with Deep Learning: from Word Embeddings to Transformers"), and draw conclusions
    in Section [10](#S10 "10\. Conclusions ‣ Survey on Automated Short Answer Grading
    with Deep Learning: from Word Embeddings to Transformers").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '文章的剩余部分组织如下。在第[2](#S2 "2\. Contributions ‣ Survey on Automated Short Answer
    Grading with Deep Learning: from Word Embeddings to Transformers")节中，我们讨论了当前工作的贡献与以往调查方法的对比，而在第[3](#S3
    "3\. Historical perspective ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers")节中，我们提供了与ASAG研究领域相关的历史发展视角。在第[4](#S4
    "4\. Organization of the review ‣ Survey on Automated Short Answer Grading with
    Deep Learning: from Word Embeddings to Transformers")节中，我们详细阐述了调查的范围和组织结构，而在第[5](#S5
    "5\. Benchmark data sets for Short Answer Grading ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers")节中，我们展示了最常用的基准数据集的细节。在第[6](#S6
    "6\. Taxonomy ‣ Survey on Automated Short Answer Grading with Deep Learning: from
    Word Embeddings to Transformers")节中，我们提供了现有工作的分类学。在第[7](#S7 "7\. Hand-engineered
    features and Machine Learning ‣ Survey on Automated Short Answer Grading with
    Deep Learning: from Word Embeddings to Transformers")和第[8](#S8 "8\. Deep learning
    methods ‣ Survey on Automated Short Answer Grading with Deep Learning: from Word
    Embeddings to Transformers")节中，我们分别分析了基于经典机器学习和深度学习方法的工作。在第[9](#S9 "9\. Discussion
    ‣ Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings
    to Transformers")节中，我们讨论了实际趋势并展望了ASAG领域未来的发展，在第[10](#S10 "10\. Conclusions ‣ Survey
    on Automated Short Answer Grading with Deep Learning: from Word Embeddings to
    Transformers")节中得出结论。'
- en: 2\. Contributions
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 贡献
- en: 'The developments and progress in the field of automated short answer grading
    (ASAG) have been previously reviewed in few survey papers (Burrows et al., [2015](#bib.bib3);
    Galhardi and Brancher, [2018](#bib.bib20); Blessing et al., [2021](#bib.bib2)).
    A unified and comprehensive overview of the entire field can be found in (Burrows
    et al., [2015](#bib.bib3)), where the developments of analytic components until
    2014 were addressed from a historical perspective. The authors organized the existing
    approaches into five groups, which correspond to temporal themes, also called
    *eras*. Methods categorized as belonging to a certain era share the approach and
    focus: word and sentence matching, feature extraction and comparison, use of semantic
    information, inference based on machine learning tools, and word embeddings.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化短答案评分（ASAG）领域的发展和进展在一些调查论文中已有回顾（Burrows等，[2015](#bib.bib3)；Galhardi和Brancher，[2018](#bib.bib20)；Blessing等，[2021](#bib.bib2)）。在（Burrows等，[2015](#bib.bib3)）中可以找到对整个领域的统一和全面的概述，其中从历史角度讨论了直到2014年的分析组件发展。作者将现有方法组织成五个组，这些组对应于时间主题，也称为*时代*。属于某个时代的分类方法共享相同的方法和重点：词汇和句子匹配、特征提取与比较、语义信息的使用、基于机器学习工具的推理以及词嵌入。
- en: A comprehensive analysis of approaches that combined feature engineering with
    machine learning-based predictive models was presented in (Galhardi and Brancher,
    [2018](#bib.bib20)). The authors provided a systematic literature review focusing
    on existing data sets, applied NLP techniques, and machine learning algorithms
    for ASAG. They highlighted the role and importance of a proper feature set design
    for effective training of machine learning models. Other published reviews (Blessing
    et al., [2021](#bib.bib2)) did not discuss further aspects of the developments
    in the field of ASAG.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在（Galhardi 和 Brancher，[2018](#bib.bib20)）中，提供了将特征工程与基于机器学习的预测模型相结合的方法的综合分析。作者进行了系统的文献综述，重点关注现有数据集、应用的NLP技术和ASAG的机器学习算法。他们强调了适当特征集设计在有效训练机器学习模型中的作用和重要性。其他已发表的评论（Blessing
    等，[2021](#bib.bib2)）没有讨论ASAG领域发展的进一步方面。
- en: 'More recent advancements in short answer grading with deep learning methods
    have not been covered yet, although they concern major progress and an increase
    of performance. In this work, we address this gap and aim at complementing previous
    surveys with a thorough analysis and organization of methods based on deep networks
    and representation learning techniques. We render the details of the transition
    from hand-engineered features to semantic-rich text representations as the key
    driver for a substantial improvement of performance on ASAG and NLP tasks. We
    trace the deep learning developments of ASAG systems back to three stages, related
    to the type of techniques used to learn textual representations: (1) models based
    on word embeddings, (2) sequence-based models that learn entire sentence representations,
    and (3) attention-based models. With this organization of existing models, the
    present survey enables future extensions while providing a comprehensive and structured
    overview of published work. The key contributions of this survey are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近在短答案评分方面的深度学习方法取得了重要进展，并提高了性能，但这些进展尚未得到覆盖。在这项工作中，我们弥补了这一空白，旨在通过对基于深度网络和表征学习技术的方法进行深入分析和组织，来补充之前的调查。我们详细描述了从手工工程特征到语义丰富的文本表示的过渡，作为在ASAG和NLP任务中显著提高性能的关键驱动因素。我们将ASAG系统的深度学习发展追溯到三个阶段，这些阶段与学习文本表示所使用的技术类型有关：（1）基于词嵌入的模型，（2）学习整个句子表示的序列模型，以及（3）基于注意力的模型。通过对现有模型的这种组织，本调查不仅为未来的扩展提供了可能性，还提供了已发布工作的全面而结构化的概述。本调查的关键贡献包括：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: we provide a comprehensive comparison of the latest methods for automated short
    answer grading and their performance on different data sets;
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对最新的自动化短答案评分方法及其在不同数据集上表现的全面比较；
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: we present an overview of the main benchmark data sets for evaluation of ASAG
    systems;
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们概述了用于评估ASAG系统的主要基准数据集；
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: we identify the actual trends and most promising model architecture for advancing
    performance in ASAG;
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们识别出实际的趋势和最有前景的模型架构，以推动ASAG的性能提升；
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: we present an analysis of the impact of progress in NLP and deep learning methods
    on the field of ASAG.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了NLP和深度学习方法的进展对ASAG领域的影响。
- en: 3\. Historical perspective
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 历史视角
- en: The development of the ASAG field has gone through a number of milestones. In (Burrows
    et al., [2015](#bib.bib3)), the authors identified some trends of the progress
    and associated them with specific time periods. Initially, researchers used concept
    mapping to compare answers to a reference answer by breaking them down into several
    concepts. The inferred concepts were used as basic reference models. Predictions
    of the correctness of the answers were made by inspecting their similarities with
    the reference answerss. These approaches were extended or partly replaced by methods
    developed in the area of information retrieval, which were more focused on the
    extraction of relevant characteristics, i.e., features, from the student answers
    for direct comparison with reference answers. Semantic information and relations
    between words in sentences were not taken into account during these initial stages
    of research. These mainly concerned analyses of text, based on regular expressions
    or parse trees (Burrows et al., [2015](#bib.bib3)). Subsequently, semantic similarities
    between sentences were modeled and exploited by using large corpora of text, e.g.
    WordNet  (Fellbaum, [2000](#bib.bib15)), word synonyms, and the development of
    knowledge-based methods. Semantic features improved the overall performance of
    ASAG methods and made them more flexible  (Magooda et al., [2016](#bib.bib40)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ASAG领域的发展经历了多个里程碑。在(Burrows et al., [2015](#bib.bib3))中，作者识别了一些进展趋势并将其与特定时间段关联起来。最初，研究者使用概念图将答案与参考答案进行比较，通过将其分解为几个概念。推断出的概念被用作基本参考模型。通过检查它们与参考答案的相似性来预测答案的正确性。这些方法被信息检索领域中更关注从学生答案中提取相关特征，即特征，以便与参考答案直接比较的方法扩展或部分替代。这些初期阶段的研究没有考虑句子中的语义信息和词汇之间的关系。这些主要涉及基于正则表达式或解析树的文本分析（Burrows
    et al., [2015](#bib.bib3)）。随后，通过使用大量文本语料库，例如WordNet（Fellbaum, [2000](#bib.bib15)）、词汇同义词和知识库方法，建模和利用了句子之间的语义相似性。语义特征提高了ASAG方法的整体性能，并使其更具灵活性（Magooda
    et al., [2016](#bib.bib40)）。
- en: The development of Machine Learning methods to construct predictive models also
    influenced the field of ASAG. The inference capabilities of trainable classification
    systems shifted the focus of researchers towards studying the effect of using
    different sets of features for the extraction of relevant characteristics of the
    text, namely lexical, syntactic, semantic features, and combinations of them.
    Extensive feature engineering was needed to construct reasonable sets of features
    that could perform well on a given problem and on data sets with specific characteristics.
    The construction of feature sets thus required domain knowledge by human experts,
    which could guarantee the design of systems that achieved good performance results
    on specific data sets (Ott et al., [2013](#bib.bib49)). Papers that proposed methods
    in this category demonstrated that semantic feature extraction methods were key
    to obtain high performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习方法的发展用于构建预测模型也影响了ASAG领域。可训练分类系统的推断能力使研究者将焦点转向研究使用不同特征集来提取文本的相关特征，即词汇、句法、语义特征及其组合的效果。构建合理的特征集需要大量的特征工程，以便在特定问题和具有特定特征的数据集上表现良好。因此，特征集的构建需要领域专家的知识，以确保系统在特定数据集上取得良好的性能结果（Ott
    et al., [2013](#bib.bib49)）。提出此类方法的论文表明，语义特征提取方法是获得高性能的关键。
- en: A major milestone in the field of NLP was the development of word embeddings
    methods, which consist of techniques for mapping words or phrases from a vocabulary
    into a vector space with interesting and useful properties for predictive tasks.
    The aim of word embeddings is to focus on the properties those feature spaces
    are supposed to retain, e.g. capturing semantic similarity. This had a significant
    impact on the field of ASAG, as it improved the capabilities of machine learning-based
    systems to extract meaningful semantic information from text, compared to previous
    methods based on hand-crafted features. The development of sequential machine
    learning models, e.g. Recurrent Neural Networks (RNNs) (De Mulder et al., [2015](#bib.bib9);
    Dyer et al., [2016](#bib.bib12)) and Long Short-Term Memory networks (LSTMs) (Cheng
    et al., [2016](#bib.bib6)), able to learn dependencies in sequences of words (sentences
    and paragraphs) contributed to substantial improvements of the computed text representations.
    Long-range dependencies in text were then successfully modeled via attention-based
    neural networks, first combined with recurrent networks (Yoon et al., [2017](#bib.bib81))
    and then as stand-alone models (Vaswani et al., [2017](#bib.bib74)). Attention-based
    approaches were applied in tasks like detection of semantic textual similarity (Lan
    et al., [2020](#bib.bib34); Liu et al., [2019c](#bib.bib39); Raffel et al., [2019](#bib.bib58)),
    paraphrase identification (Yang et al., [2019](#bib.bib80); Liu et al., [2019b](#bib.bib38);
    Ratner et al., [2019](#bib.bib60)), reading comprehension (Lan et al., [2020](#bib.bib34);
    Raffel et al., [2019](#bib.bib58); Zhang et al., [2020b](#bib.bib85); Devlin et al.,
    [2019](#bib.bib10)), and recognizing textual entailment (Liu et al., [2019c](#bib.bib39);
    Yang et al., [2019](#bib.bib80); Liu et al., [2019b](#bib.bib38)). Methods that
    model long-range dependencies in text became the state-of-the-art approaches for
    most NLP and ASAG tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理领域的一个重要里程碑是词嵌入方法的发展，这些方法将词语或短语从词汇表映射到具有有趣和有用属性的向量空间中，用于预测任务。词嵌入的目的是关注这些特征空间应保留的属性，例如捕捉语义相似性。这对ASAG领域产生了重大影响，因为它提高了基于机器学习的系统从文本中提取有意义的语义信息的能力，相较于之前基于手工制作特征的方法。顺序机器学习模型的发展，例如递归神经网络（RNNs）(De
    Mulder et al., [2015](#bib.bib9); Dyer et al., [2016](#bib.bib12))和长短期记忆网络（LSTMs）(Cheng
    et al., [2016](#bib.bib6))，能够学习词序列（句子和段落）中的依赖关系，这对计算文本表示的改进起到了重要作用。长距离依赖关系在文本中随后通过基于注意力的神经网络成功建模，这些网络首先与递归网络结合使用（Yoon
    et al., [2017](#bib.bib81)），然后作为独立模型（Vaswani et al., [2017](#bib.bib74)）使用。基于注意力的方法被应用于语义文本相似性检测（Lan
    et al., [2020](#bib.bib34); Liu et al., [2019c](#bib.bib39); Raffel et al., [2019](#bib.bib58)）、释义识别（Yang
    et al., [2019](#bib.bib80); Liu et al., [2019b](#bib.bib38); Ratner et al., [2019](#bib.bib60)）、阅读理解（Lan
    et al., [2020](#bib.bib34); Raffel et al., [2019](#bib.bib58); Zhang et al., [2020b](#bib.bib85);
    Devlin et al., [2019](#bib.bib10)）和文本蕴涵识别（Liu et al., [2019c](#bib.bib39); Yang
    et al., [2019](#bib.bib80); Liu et al., [2019b](#bib.bib38)）等任务。建模文本中的长距离依赖关系的方法已成为大多数自然语言处理和ASAG任务的最新技术。
- en: 4\. Organization of the review
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 综述的组织
- en: 'The survey is structured in three parts. The first part covers important benchmark
    data sets for the evaluation of short answer grading (ASAG) algorithms (cf. Section [5](#S5
    "5\. Benchmark data sets for Short Answer Grading ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers")). Each
    of the data sets has specific characteristics that allow to evaluate different
    aspects of ASAG systems related to their generalization properties, such as grading
    of answers to questions unseen during the training phase or about new topics or
    domains.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该调查分为三个部分。第一部分涵盖了用于短答案评分（ASAG）算法评估的重要基准数据集（参见第[5](#S5 "5\. 基准数据集用于短答案评分 ‣ 深度学习的自动短答案评分调查：从词嵌入到变换器")节）。每个数据集具有特定的特征，这些特征能够评估ASAG系统的不同方面，例如对训练阶段未见过的问题或新主题、新领域的答案评分等与其泛化能力相关的方面。
- en: 'We track the progress and performance improvements of methods based on classical
    machine learning tools and hand-engineered feature sets, and on more recent deep
    learning approaches in the second and third part of the survey (cf. Sections [7](#S7
    "7\. Hand-engineered features and Machine Learning ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers") and [8](#S8
    "8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers")), respectively. We pay particular
    attention to their capabilities to extract semantic-rich text representations.
    We analyze recently published methods and elaborate on the latest developments
    and trends in the field of ASAG.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们跟踪了基于经典机器学习工具和人工设计特征集的方法在进展和性能改进方面的表现，以及在调查的第二部分和第三部分中更近期的深度学习方法（参见第[7](#S7
    "7\. 手工设计特征与机器学习 ‣ 基于深度学习的自动短答案评分调查：从词嵌入到变压器")节和第[8](#S8 "8\. 深度学习方法 ‣ 基于深度学习的自动短答案评分调查：从词嵌入到变压器")节）。我们特别关注它们提取语义丰富文本表示的能力。我们分析了最近发布的方法，并详细阐述了ASAG领域的最新发展和趋势。
- en: 4.1\. Scope
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 范围
- en: 'We focus on the analysis of methods for automated grading of short answers,
    defined by the following criteria (Burrows et al., [2015](#bib.bib3)):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于自动短答案评分方法的分析，这些方法由以下标准定义（Burrows等，[2015](#bib.bib3)）。
- en: (1)
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: the answer has to reflect the student knowledge and should not report just passages
    from a provided prompt text;
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案必须反映学生的知识，而不仅仅是报告提供的提示文本中的段落；
- en: (2)
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: the answer is given in natural language;
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案以自然语言给出；
- en: (3)
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'the answer’s length is around $50$ words, but can contain up to about $100$¹¹1Some
    of the analyzed datasets contain outliers (see table [1](#S5.T1 "Table 1 ‣ 5\.
    Benchmark data sets for Short Answer Grading ‣ Survey on Automated Short Answer
    Grading with Deep Learning: from Word Embeddings to Transformers")) that exceed
    that threshold but were nevertheless taken into account, as the majority of responses
    were below;'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案的长度约为$50$词，但可以包含多达约$100$¹¹1某些分析的数据集中包含了超出该阈值的异常值（见表[1](#S5.T1 "表1 ‣ 5\. 短答案评分基准数据集
    ‣ 基于深度学习的自动短答案评分调查：从词嵌入到变压器")），但这些异常值仍然被考虑在内，因为大多数答案在此阈值以下；
- en: (4)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: grading of the answer is focused on the content rather than the writing quality;
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案的评分集中于内容而非写作质量；
- en: (5)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: the possible answers are restricted by the closed-form of the question.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可能的答案受到问题的封闭形式的限制。
- en: Most of the approaches for ASAG published in recent years were based on supervised
    learning strategies and designed to support grading efficiency in educational
    settings (Burrows et al., [2015](#bib.bib3); Galhardi and Brancher, [2018](#bib.bib20)).
    Unsupervised approaches, such as ranking or clustering, were used to group together
    student answers based on similarity, aiming at improving the uniformity of grading
    and can be used as a complementary tool to supervised learning methods.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年发布的大多数ASAG方法都是基于监督学习策略的，旨在支持教育环境中的评分效率（Burrows等，[2015](#bib.bib3)；Galhardi和Brancher，[2018](#bib.bib20)）。无监督方法，如排名或聚类，被用来根据相似性将学生答案分组，旨在提高评分的一致性，并可以作为监督学习方法的补充工具。
- en: In practice, in the case of courses whose content does not change drastically
    over time, the question pool used for examinations is slightly expanded or does
    not significantly change, and the same questions are re-used. This results in
    the availability of multiple answers to the same questions, which can be used
    as training inputs for the life-long optimization of the models. In this context,
    with the availability of labeled questions, reference answers, and student answers,
    automated grading systems based on supervised learning are mostly investigated.
    Hence, we focus on reviewing supervised methods, which are largely deployed for
    ASAG, and provide more precise evaluations due to the use of labeled correct answers
    for training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于那些内容不会随着时间剧烈变化的课程，考试所用的问题库会稍微扩展或不会发生显著变化，相同的问题会被重复使用。这导致了对同一问题有多个答案，这些答案可以作为模型终身优化的训练输入。在这种情况下，利用标记问题、参考答案和学生答案的自动评分系统主要是基于监督学习的。因此，我们重点审查了大量用于ASAG的监督方法，并提供了由于使用标记正确答案进行训练而更为精确的评估。
- en: 4.2\. Search methodology
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 搜索方法
- en: 'We applied a semi-systematic approach for the literature review, which consisted
    of the following steps: a) keyword-based search in scientific databases; b) focus
    on benchmark data sets and methods tested on them; c) backward search based on
    references of relevant papers.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了半系统的方法进行文献回顾，包括以下步骤：a) 在科学数据库中基于关键词的搜索；b) 重点关注基准数据集及其测试方法；c) 基于相关论文的参考文献进行倒查。
- en: 'We searched for relevant papers in the main scientific databases, namely ScienceDirect,
    Google Scholar, ResearchGate, Semantic Scholar, and arXiv. The main search terms
    that we used were ‘automatic grading system’, ‘automated assessment’, ‘digital
    assessment of students’, ‘short answer grading’, and adapted them depending on
    the results. We subsequently identified the most used benchmark data sets (see
    Section [5](#S5 "5\. Benchmark data sets for Short Answer Grading ‣ Survey on
    Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers")),
    and selected the papers that report results on them. This allowed to include in
    the review those papers that propose systems for automated grading, also in the
    case they present them using slightly different task definitions, such as inference
    on answer similarity or automatic question answering. Finally, we gathered further
    papers by investigating the reference lists of the already selected ones. The
    criteria for the selection were the number of citations and year of publication,
    agreement with the topic concerned and application, and testing on similar data
    sets.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在主要的科学数据库中搜索相关论文，即ScienceDirect、Google Scholar、ResearchGate、Semantic Scholar和arXiv。我们使用的主要搜索词包括‘自动评分系统’、‘自动化评估’、‘学生数字评估’、‘短答案评分’，并根据结果对其进行了调整。我们随后确定了最常用的基准数据集（见第[5](#S5
    "5\. Benchmark data sets for Short Answer Grading ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers")节），并选择了报告其结果的论文。这使我们能够在回顾中纳入那些提出自动评分系统的论文，即使它们使用了略微不同的任务定义，如答案相似性的推断或自动问答。最后，我们通过调查已经选定论文的参考文献列表，进一步收集了其他论文。选择标准包括引用次数和出版年份、与相关主题和应用的契合度，以及在类似数据集上的测试。'
- en: We especially focused on the papers that were published in the past five years,
    in order to identify the most recent developments and trends. The increasing number
    of published papers in the past years indicates a growing interest in ASAG systems
    and in their use for educational purposes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别关注了过去五年内发表的论文，以便识别最新的发展和趋势。过去几年发表论文数量的增加表明对ASAG系统及其在教育用途中的应用有着日益增长的兴趣。
- en: 5\. Benchmark data sets for Short Answer Grading
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 短答案评分的基准数据集
- en: 'Existing methods have been tested on different data sets, e.g. SciEntsBank,
    Beetle, Texas, ASAP-SAS, and various others. These data sets differ in many aspects:
    number of questions, number of answers, question type, domain, language, grading
    basis, and answer length. For our analysis, we rely on the results reported in
    the original papers of the methods that we surveyed. They were tested usually
    on sub-sets of the available data sets, and a direct comparison of the performance
    is sometimes not possible. This is mainly due to the different composition and
    characteristics of the available public data sets. Some data sets are proprietary
    or have an unknown source, thus we exclude them from our analysis (e.g. the Large-Scale
    Industry Dataset (Dhamecha et al., [2018](#bib.bib11); Saha et al., [2019](#bib.bib63))).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现有方法已经在不同的数据集上进行过测试，例如SciEntsBank、Beetle、Texas、ASAP-SAS等。这些数据集在许多方面有所不同：问题数量、答案数量、问题类型、领域、语言、评分依据和答案长度。对于我们的分析，我们依赖于我们调查的这些方法原始论文中报告的结果。它们通常在可用数据集的子集上进行测试，因此有时无法直接比较性能。这主要是由于可用公共数据集的不同组成和特性。一些数据集是专有的或来源不明，因此我们将其排除在我们的分析之外（例如，Large-Scale
    Industry Dataset (Dhamecha et al., [2018](#bib.bib11); Saha et al., [2019](#bib.bib63)))。
- en: 'In this review, we focus on the four most widely used data sets for benchmarking
    ASAG methods: SciEntsBank (Dzikovska et al., [2013](#bib.bib13)), Beetle (Dzikovska
    et al., [2013](#bib.bib13)), Texas (Mohler et al., [2011](#bib.bib44)), and ASAP-SAS (Kaggle,
    [[n. d.]](#bib.bib29)) data sets. Their public availability and diversity of the
    answer domains allow to evaluate different aspects of the performance and capabilities
    of automated grading algorithms. Furthermore, they guarantee a fair comparison
    of existing methods. In the following section, we describe the data sets and provide
    information on the specific task and applications for which they were designed.
    In Table [1](#S5.T1 "Table 1 ‣ 5\. Benchmark data sets for Short Answer Grading
    ‣ Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings
    to Transformers"), we summarize the characteristics and details of the composition
    of the considered data sets.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇综述中，我们重点介绍了用于基准测试ASAG方法的四个最广泛使用的数据集：SciEntsBank（Dzikovska et al., [2013](#bib.bib13)）、Beetle（Dzikovska
    et al., [2013](#bib.bib13)）、Texas（Mohler et al., [2011](#bib.bib44)）和ASAP-SAS（Kaggle,
    [[n. d.]](#bib.bib29)）数据集。它们的公开可用性和答案领域的多样性允许评估自动评分算法的不同性能和能力方面。此外，它们还保证了现有方法的公平比较。在接下来的部分中，我们将描述这些数据集，并提供有关它们设计的特定任务和应用的信息。在表[1](#S5.T1
    "表 1 ‣ 5\. 短答案评分的基准数据集 ‣ 自动短答案评分综述：从词嵌入到变换器")中，我们总结了所考虑数据集的特点和详细组成。
- en: Table 1\. Detailed characteristics of the SciEntsBank, Beetle, Texas2011 and
    ASAP-SAS benchmark data sets. ”Additional information” indicates whether additional
    textual information for the task besides the question itself is available.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. SciEntsBank、Beetle、Texas2011 和 ASAP-SAS 基准数据集的详细特征。“附加信息”指示是否除了问题本身之外，还有用于任务的额外文本信息。
- en: '| Characteristics | SciEntsBank | Beetle | Texas2011 | ASAP-SAS |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 特点 | SciEntsBank | Beetle | Texas2011 | ASAP-SAS |'
- en: '| Training question/answer pairs | 4,969 | 17,198 | 2,442 | 17,207 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 训练问答对 | 4,969 | 17,198 | 2,442 | 17,207 |'
- en: '| Percentage of correct answers | 40.41% | 42.49% | 44,22% | 21.57% |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 正确答案的百分比 | 40.41% | 42.49% | 44.22% | 21.57% |'
- en: '| Number of domains | 12 | 2 | 1 | 4 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 领域数量 | 12 | 2 | 1 | 4 |'
- en: '| Number of questions | 135 | 47 | 85 | 10 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 问题数量 | 135 | 47 | 85 | 10 |'
- en: '| Average number of answers per question | 37 | 366 | 29 | 1,721 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 每个问题的平均答案数量 | 37 | 366 | 29 | 1,721 |'
- en: '| Average answer length (in words) | 13 | 10 | 18 | 42 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 平均答案长度（以词数计） | 13 | 10 | 18 | 42 |'
- en: '| Maximum answer length (in words) | 110 | 80 | 173 | 325 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 最大答案长度（以词数计） | 110 | 80 | 173 | 325 |'
- en: '| Minimum answer length (in words) | 1 | 1 | 1 | 1 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 最小答案长度（以词数计） | 1 | 1 | 1 | 1 |'
- en: '| Additional information | No | No | No | Yes |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 附加信息 | 否 | 否 | 否 | 是 |'
- en: '| Scale of labels | 2-way, 3-way, 5-way |  |  |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 标签的规模 | 2类、3类、5类 |  |  |  |'
- en: '| classification | 2-way, 3-way, 5-way classification | Score between 0 and
    5 | Score between 0 and 2 or between 0 and 3 |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 2类、3类、5类分类 | 分数在0到5之间 | 分数在0到2或0到3之间 |  |'
- en: '| Publicly available | Yes | Yes | Yes | Yes |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 是否公开可用 | 是 | 是 | 是 | 是 |'
- en: 5.1\. SciEntsBank and Beetle data sets
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. SciEntsBank 和 Beetle 数据集
- en: The SciEntsBank and Beetle data sets are part of the SemEval 2013 challenge (Dzikovska
    et al., [2013](#bib.bib13)), which has the objective of identifying common mistakes,
    such as omissions and wrong or thematically irrelevant statements, in order to
    develop customized correction strategies. The challenge is structured to evaluate
    different aspects of short answer grading systems.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: SciEntsBank 和 Beetle 数据集是 SemEval 2013 挑战（Dzikovska et al., [2013](#bib.bib13)）的一部分，其目标是识别常见的错误，如遗漏、错误或主题不相关的陈述，以便开发定制的纠正策略。该挑战旨在评估短答案评分系统的不同方面。
- en: The data sets include three sets of labels, which serve to train models on 2-,3-
    and 5-way task problems. The 2-way task consists of classifying the answers either
    as correct or incorrect, whereas for the 3-way task each answer is labeled as
    either correct, contradictory or incorrect. The 2- and 3-way tasks concern the
    Recognizing Textual Entailment (RTE) task. The 5-way task aims at evaluating the
    identification of non-domain, correct, partially correct incomplete, contradictory
    and irrelevant answers. It is aimed at improving dialogue systems for tutoring.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含三种标签集，用于训练模型在2类、3类和5类任务问题上的表现。2类任务包括将答案分类为正确或错误，而3类任务中，每个答案被标记为正确、矛盾或错误。2类和3类任务涉及识别文本蕴涵（RTE）任务。5类任务旨在评估对非领域、正确、部分正确、不完整、矛盾和不相关答案的识别。这些任务旨在改进用于辅导的对话系统。
- en: 'The test protocol is designed to evaluate the performance of the algorithms
    on three sets of data, which represent possible challenging situations where the
    grading systems can be used, namely: a) unseen answers during the training phase,
    b) unseen questions not used to train the models, and c) questions/answers from
    unseen domains. These tests are meant to evaluate different aspects of the generalization
    properties of automated grading methods and their applicability to questions and
    domains other than those they are trained for.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 测试协议旨在评估算法在三组数据上的表现，这些数据代表了评分系统可能遇到的挑战性情况，即：a）训练阶段未见过的答案，b）未用于训练模型的未见过的问题，以及c）来自未见领域的问题/答案。这些测试旨在评估自动评分方法的泛化属性及其对训练领域以外的问题和领域的适用性。
- en: SciEntsBank
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SciEntsBank
- en: 'The SciEntsBank data set comprises student answers to questions collected as
    part of a standardized assessment in grades $3$ to $6$ in schools across North
    America. In total, the data set contains $4969$ answers to $135$ questions from
    $12$ domains (see Table [1](#S5.T1 "Table 1 ‣ 5\. Benchmark data sets for Short
    Answer Grading ‣ Survey on Automated Short Answer Grading with Deep Learning:
    from Word Embeddings to Transformers") for details). Depending on the domain category,
    the required grading concerns either a 2-way, 3-way, or 5-way classification.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'SciEntsBank数据集包括在北美学校的$3$到$6$年级的标准化评估中收集的学生答案。数据集总共包含$4969$个答案，涉及$12$个领域的$135$个问题（详见表[1](#S5.T1
    "Table 1 ‣ 5\. Benchmark data sets for Short Answer Grading ‣ Survey on Automated
    Short Answer Grading with Deep Learning: from Word Embeddings to Transformers")）。根据领域类别，所需的评分涉及2分类、3分类或5分类。'
- en: Beetle
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Beetle
- en: In contrast to the SciEntsBank data set, the Beetle data set is designed to
    test the interaction of students with a real tutorial dialogue system. The system
    teaches students in high-school physics and fundamentals of electricity and electronics.
    In order to create the data set, the dialogues have been revised and only relevant
    answers (not the interaction protocol) to questions were used. Questions are either
    factual questions, or explanation and definition questions. The corpus contains
    in total 47 questions with on average 366 student answers per question. The Beetle
    set only contains the categories of unseen answers and unseen questions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与SciEntsBank数据集相比，Beetle数据集旨在测试学生与真实教程对话系统的互动。该系统教授高中物理和电力电子学基础。为了创建这个数据集，对话被修订过，只使用了与问题相关的答案（而非互动协议）。问题包括事实性问题和解释定义问题。该语料库总共包含47个问题，每个问题的平均学生回答数量为366。Beetle数据集仅包含未见过的答案和未见过的问题类别。
- en: 5.2\. University of North Texas data set
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 北德克萨斯大学数据集
- en: The University of North Texas data set (Texas2011) is also widely used to evaluate
    and compare the performance of automated grading systems. It contains answers
    given by 30 students to 80 different questions, which result in approximately
    2400 question-answer pairs. The average answer length is 50 word tokens. The questions
    are collected from 10 assignments of two examinations of basic knowledge in the
    field of Computer Science. The answers are provided with a grade given by two
    assessors, which ranges between 0 and 5\. The grades are given as integer numbers.
    There were no clear rules in the grading process and the average grade between
    the two assessors is considered the ground truth (Mohler et al., [2011](#bib.bib44)).
    The data set has the purpose of mirroring real-world issues of the process of
    grading short answers as realistically as possible. This results in more complex
    and nested answer structures, which also include the use of tables.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 北德克萨斯大学数据集（Texas2011）也被广泛用于评估和比较自动评分系统的表现。它包含30名学生对80个不同问题的回答，产生了大约2400个问答对。答案的平均长度为50个词令。问题来源于计算机科学领域的两个考试的10个作业。这些答案由两位评估者评分，评分范围为0到5。评分以整数形式给出。评分过程中没有明确的规则，两位评估者的平均评分被认为是标准答案（Mohler
    et al., [2011](#bib.bib44)）。该数据集的目的是尽可能真实地反映短答案评分过程中的现实问题。这导致了更复杂和嵌套的答案结构，也包括了表格的使用。
- en: 5.3\. ASAP-SAS data set
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. ASAP-SAS数据集
- en: The Automated Student Assessment Prize Short Answer Scoring (ASAP-SAS) data
    set was released as part of a Kaggle competition in 2013, sponsored by the Hewlett
    Foundation (Kaggle, [[n. d.]](#bib.bib29)). It consists of ten questions from
    different domains, e.g., English, Biology, English Language Art, Science. In total
    the training data set contains $17,207$ answers (about $1,700$ per question) and
    the test set contains $5,224$ answers. On average an answer has $50$ words, although
    a small amount of answers ($<5\%$) also contains more than 100 words. Each of
    the questions is marked with a score in the range of $0$ to $2$ or $0$ to $3$.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化学生评估奖短答题评分（ASAP-SAS）数据集作为Kaggle比赛的一部分于2013年发布，由休利特基金会赞助（Kaggle, [[n. d.]](#bib.bib29)）。它包含来自不同领域的十个问题，例如英语、生物学、英语语言艺术、科学。总的来说，训练数据集包含$17,207$个答案（每个问题约$1,700$个答案），测试集包含$5,224$个答案。平均每个答案有$50$个单词，虽然少量答案（<$5\%$）也超过100个单词。每个问题的评分范围为$0$到$2$或$0$到$3$。
- en: 'The competition and the data set were designed to challenge assessment systems
    with realistic and diversified questions. Due to the diversity of the ten questions,
    it further aims to support the development of scoring systems and their real-world
    application in the educational sector. One unique characteristic of this data
    set is that the question structure varies a lot: for instance, some questions
    are formulated to ask for specific information contained in a prompt text of 1
    or 2 pages. Other questions also contain pictures or graphs and the student is
    requested to state her own observations and interpretation of the results.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛和数据集的设计旨在通过现实且多样化的问题来挑战评估系统。由于十个问题的多样性，它进一步旨在支持评分系统的发展及其在教育领域的实际应用。这个数据集的一个独特特点是问题结构差异很大：例如，有些问题要求从1或2页的提示文本中提取具体信息。其他问题还包含图片或图表，学生需要陈述自己的观察和结果解释。
- en: 6\. Taxonomy
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 分类
- en: 'Existing ASAG approaches fall into two broad categories, namely (1) early approaches
    which rely on hand-crafted features and classical machine learning (CML) approaches,
    e.g. logistic regression or support vector machines, and (2) deep learning (DL)
    approaches that formulate the feature design as a learning problem and combine
    it with training a predictive model (Sung et al., [2019a](#bib.bib67)). The second
    category of methods can be further divided into three sub-categories, which correspond
    to the phases of the development of NLP methods, namely word embedding, sequential
    models, and attention-based models (Young et al., [2018](#bib.bib82); Otter et al.,
    [2021](#bib.bib50)). Figure [1](#S6.F1 "Figure 1 ‣ 6\. Taxonomy ‣ Survey on Automated
    Short Answer Grading with Deep Learning: from Word Embeddings to Transformers")
    shows an overview of the taxonomy of methods for the ASAG method.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的ASAG方法大致分为两类，即（1）依赖于手工特征和经典机器学习（CML）方法的早期方法，例如逻辑回归或支持向量机，以及（2）将特征设计作为学习问题并结合训练预测模型的深度学习（DL）方法（Sung
    et al., [2019a](#bib.bib67)）。第二类方法可以进一步分为三个子类别，分别对应NLP方法的发展阶段，即词嵌入、序列模型和基于注意力的模型（Young
    et al., [2018](#bib.bib82); Otter et al., [2021](#bib.bib50)）。图[1](#S6.F1 "Figure
    1 ‣ 6\. Taxonomy ‣ Survey on Automated Short Answer Grading with Deep Learning:
    from Word Embeddings to Transformers")展示了ASAG方法分类的概述。'
- en: <svg version="1.1" fill="none" height="159.263871592639" stroke="none" width="458.281444582814"
    overflow="visible"><g transform="translate(0,159.263871592639) scale(1,-1)"><g
    transform="translate(0,0)"><g transform="translate(0,122) scale(1, -1)"><foreignobject
    width="352" height="122" overflow="visible">![Refer to caption](img/722498617bd5eaf14fe65d0f4b697389.png)</foreignobject></g></g><g
    transform="translate(110.8,142.06)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-weight="bold">Automated short answer grading</text></g><g transform="translate(17.64,101.47)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-weight="bold">classical
    machine learning</text></g><g transform="translate(293.55,100.06)"><text x="0"
    y="0" transform="scale(1, -1)" fill="black" font-weight="bold">deep learning</text></g><g
    transform="translate(310.38,76.69)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-weight="bold">word embeddings</text></g><g transform="translate(310.15,44.87)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-weight="bold">sequential-based
    models</text></g><g transform="translate(309.63,12.51)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black" font-weight="bold">attention-based models</text></g><g transform="translate(38.7,76.69)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-weight="bold">lexical features</text></g><g
    transform="translate(38.47,44.87)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-weight="bold">syntactic features</text></g><g transform="translate(37.95,12.51)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-weight="bold">semantic
    features</text></g></g></svg>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![请参见说明](img/722498617bd5eaf14fe65d0f4b697389.png)'
- en: Figure 1\. Taxonomy of Automated Short Answer Grading methods. The categorization
    of methods is based on the machine learning approach and on the type of representation
    (features) used. For classical machine learning approaches, the features are hand-engineered
    by domain experts, while in deep learning approaches the different types of representations
    are learned directly from the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 自动化简答评分方法的分类。这些方法的分类基于机器学习方法和使用的表示类型（特征）。对于经典的机器学习方法，特征由领域专家手工设计，而在深度学习方法中，不同类型的表示是直接从数据中学习得到的。
- en: Word embedding techniques, most prominently Word2Vec (Mikolov et al., [2013](#bib.bib42)),
    aim to represent semantically similar words with vectors that are close in a learned
    latent space. These automatically learned representations are able to characterize
    the information in large corpora of text effectively. Longer-range relations,
    in groups of words and longer sentences, were taken into account for the development
    of the second category of methods, which include approaches based on Recurrent
    Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs). These models
    capture relations between words of a sentence over longer distances and therefore
    provide richer sentence and paragraph representations. The models are usually
    trained for specific downstream tasks and can use the embedding results from methods
    such as Word2Vec (Sutskever et al., [2014](#bib.bib69)). However, due to the vanishing
    gradient problem, recurrent networks tend to focus more towards short-term context
    information, while not being able to robustly catch longer-range dependencies
    in sentences. LSTMs alleviated this issue to some extent. The limitation was finally
    addressed by neural network models based on attention mechanisms, which constitute
    the third category of deep learning-based methods that we identified. Attention-based
    models relax the strict sequential analysis of tokens in sentences and are able
    to model word dependencies without regard to their distance in the sentences,
    also allowing for parallel processing of longer text sequences by means of multi-head
    attention mechanisms (Galassi et al., [2020](#bib.bib19)). Architectures fully
    based on attention are known as Transformers (Vaswani et al., [2017](#bib.bib74)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入技术，最显著的如Word2Vec (Mikolov et al., [2013](#bib.bib42))，旨在用在学习的潜在空间中相近的向量来表示语义相似的词。这些自动学习的表示能够有效地表征大规模文本语料中的信息。更长范围的关系，在词组和较长的句子中，被纳入了第二类方法的发展中，这包括基于递归神经网络（RNN）和长短期记忆网络（LSTM）的方法。这些模型捕捉句子中词语之间的长距离关系，从而提供更丰富的句子和段落表示。这些模型通常为特定的下游任务进行训练，并可以使用诸如Word2Vec (Sutskever
    et al., [2014](#bib.bib69))的方法的嵌入结果。然而，由于梯度消失问题，递归网络往往更关注短期上下文信息，无法有效捕捉句子中的长距离依赖关系。LSTM在一定程度上缓解了这个问题。这个限制最终由基于注意力机制的神经网络模型解决，这些模型构成了我们识别出的第三类深度学习方法。基于注意力的模型放松了对句子中标记的严格顺序分析，能够在不考虑它们在句子中的距离的情况下建模词语依赖关系，同时通过多头注意力机制实现对较长文本序列的并行处理 (Galassi
    et al., [2020](#bib.bib19))。完全基于注意力的架构被称为Transformers (Vaswani et al., [2017](#bib.bib74))。
- en: 7\. Hand-engineered features and Machine Learning
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 手工工程特征与机器学习
- en: Approaches based on hand-engineered features rely on the extraction of lexical,
    syntactic, and semantic features from the text, using dependency and constituency
    parsers, or functions to compute the sentence overlap. The main goal of these
    features is to describe key components (e.g. specific terms, concepts) of good
    answers by detecting specific patterns.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于手工工程特征的方法依赖于从文本中提取词汇、句法和语义特征，使用依存和成分解析器，或计算句子重叠的函数。这些特征的主要目标是通过检测特定模式来描述良好答案的关键组件（例如特定术语、概念）。
- en: 'In most of the reviewed papers, the feature vectors extracted from the raw
    text were used in combination with a classical machine learning classifier, such
    as Logistic Regressor, Support Vector Machine, Random Forest, or Naïve Bayes classifiers.
    Some of the reviewed approaches were based on ensemble methods combining predictions
    of several classifiers. In the following, we revise these approaches, organized
    in sections according to the type of features used to represent the text, namely
    1) lexical, 2) syntactic and 3) semantic features, and elaborate on their relevance
    and use in existing methods. It is important to note that few approaches deployed
    features sets that were made of combinations of different types of features. This
    makes it difficult to draw absolute conclusions regarding the superiority of a
    certain feature or group of features over another. We provide an overview of the
    reviewed methods in Table [2](#S7.T2 "Table 2 ‣ 7.1\. Lexical features ‣ 7\. Hand-engineered
    features and Machine Learning ‣ Survey on Automated Short Answer Grading with
    Deep Learning: from Word Embeddings to Transformers") and the results that they
    achieved on benchmark data sets in Table [3](#S7.T3 "Table 3 ‣ 7.1\. Lexical features
    ‣ 7\. Hand-engineered features and Machine Learning ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '在大多数审查过的论文中，从原始文本中提取的特征向量与经典的机器学习分类器结合使用，例如逻辑回归、支持向量机、随机森林或朴素贝叶斯分类器。一些审查过的方法基于集成方法，结合了多个分类器的预测。在下面的部分中，我们将这些方法进行回顾，按所使用的文本表示特征类型组织为1）词汇特征、2）句法特征和3）语义特征，并详细说明它们在现有方法中的相关性和应用。需要注意的是，很少有方法使用了不同类型特征组合的特征集。这使得很难得出某一特征或特征组相对于其他特征的绝对优越性结论。我们在表[2](#S7.T2
    "Table 2 ‣ 7.1\. Lexical features ‣ 7\. Hand-engineered features and Machine Learning
    ‣ Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings
    to Transformers")中提供了审查方法的概述，并在表[3](#S7.T3 "Table 3 ‣ 7.1\. Lexical features ‣
    7\. Hand-engineered features and Machine Learning ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers")中列出了它们在基准数据集上的结果。'
- en: 7.1\. Lexical features
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 词汇特征
- en: Lexical features have been largely used for the description of the characteristics
    of short textual answers. They consist of, for example, single words, stemmed
    or lemmatized words, their prefix or suffix. The extraction of lexical features
    is a simple process and several algorithms were proposed that use them in ASAG
    tasks, e.g. methods to compute the degree of overlap between sentences, n-gram
    representations, or lexical statistics.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇特征已被广泛用于描述短文本答案的特征。例如，它们包括单词、词干或词形还原后的单词、以及其前缀或后缀。词汇特征的提取是一个简单的过程，并且已经提出了多种算法，在ASAG任务中使用这些特征，例如计算句子之间重叠程度的方法、n-gram表示或词汇统计。
- en: 'Algorithms for estimating word overlap were a fundamental component of early
    ASAG systems. Overlap-based features measure the coinciding words between sentences,
    i.e. the student and reference answer, using methods to estimate the word-wise
    or character-wise overlap of two or more sample sentences. Often these methods
    were combined with different pre-processing methods, such as lemmatization or
    stemming to further improve the quality of the results (Ott et al., [2013](#bib.bib49);
    Meurers et al., [2011](#bib.bib41); Mohler et al., [2011](#bib.bib44)). In  (Pribadi
    et al., [2017](#bib.bib55)), the authors compared three-word overlap computation
    methods: i) Dice coefficient, ii) Jaccard coefficient and iii) cosine coefficient,
    and studied their impact on the performance of an ASAG system. These methods count
    the overlapping words in two sentences and compute a score of the similarity between
    them. The authors found that the cosine coefficient contributed to obtaining the
    best performance on the estimation of sentence similarity. In (Pribadi et al.,
    [2016](#bib.bib54)), a weighted cosine coefficient was used, resulting in a further
    improvement of the performance of an ASAG system. Other approaches extracted the
    raw number of overlapping words and calculated several string similarity scores,
    e.g. cosine similarity and Lesk similarity (Dzikovska et al., [2012](#bib.bib14)).
    In addition, the authors computed sub-tree matching based on lexical features,
    in which they counted overlapping words and word stems. In (Meurers et al., [2011](#bib.bib41)),
    the authors used features that are based on the overlapping words between the
    reference answer and the student answer.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 估计词重叠的算法是早期ASAG系统的一个基本组成部分。基于重叠的特征测量句子之间的重合词，即学生答案和参考答案，使用方法来估计两个或多个样本句子的词级或字符级重叠。这些方法通常与不同的预处理方法结合使用，例如词形还原或词干提取，以进一步提高结果的质量（Ott
    et al., [2013](#bib.bib49); Meurers et al., [2011](#bib.bib41); Mohler et al.,
    [2011](#bib.bib44)）。在（Pribadi et al., [2017](#bib.bib55)）中，作者比较了三种词重叠计算方法：i）Dice系数，ii）Jaccard系数和iii）余弦系数，并研究了它们对ASAG系统性能的影响。这些方法计算两个句子中的重叠词，并计算它们之间的相似性得分。作者发现，余弦系数对句子相似性估计的性能贡献最大。在（Pribadi
    et al., [2016](#bib.bib54)）中，使用了加权余弦系数，从而进一步提高了ASAG系统的性能。其他方法提取了重叠词的原始数量，并计算了几种字符串相似性得分，例如余弦相似性和Lesk相似性（Dzikovska
    et al., [2012](#bib.bib14)）。此外，作者基于词汇特征计算了子树匹配，其中计数了重叠词和词干。在（Meurers et al., [2011](#bib.bib41)）中，作者使用了基于参考答案和学生答案之间重叠词的特征。
- en: Increasing the complexity and support of the overlap detection methods further
    improved performance. In (Jimenez et al., [2013](#bib.bib28)), the authors proposed
    a method that computes sentence overlap in a hierarchical fashion, called SoftCardinality.
    It computes several similarity scores by considering different character q-grams
    between words and sentences, and achieved state-of-the-art results on the SemEval
    data set. The achieved results demonstrated that a more fine-grained feature extraction
    technique describes more complex characteristics of the text and, subsequently,
    improves the overall performance of the scoring system. Similar conclusions were
    drawn in (Heilman and Madnani, [2013](#bib.bib25)), where the authors proposed
    a method focused on the textual lexical similarity, computed by using character
    and word n-grams. In particular, the similarity was calculated taking into account
    the number of overlaps in the two sentences combined with string edit-distance
    features. This approach uses additional features from (Heilman and Madnani, [2012](#bib.bib24)),
    which were computed according to the edit-effort it takes to align a student answer
    and a reference answer. The final score was predicted by a logistic regression
    classifier. In addition, domain adaption was used, where different features have
    individual weights for each domain.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 增加重叠检测方法的复杂性和支持进一步提高了性能。在 (Jimenez 等， [2013](#bib.bib28)) 中，作者提出了一种以分层方式计算句子重叠的方法，称为
    SoftCardinality。它通过考虑单词和句子之间的不同字符 q-gram 来计算多个相似性分数，并在 SemEval 数据集上取得了最先进的结果。取得的结果表明，更细致的特征提取技术能够描述文本的更复杂特征，从而提高评分系统的整体性能。在
    (Heilman 和 Madnani， [2013](#bib.bib25)) 中得出了类似结论，作者提出了一种基于文本词汇相似性的计算方法，该方法使用字符和单词
    n-gram。具体来说，相似性是通过考虑两个句子中的重叠数量以及字符串编辑距离特征来计算的。这种方法使用了来自 (Heilman 和 Madnani， [2012](#bib.bib24))
    的附加特征，这些特征是根据对齐学生答案和参考答案所需的编辑工作量计算的。最终分数由逻辑回归分类器预测。此外，还使用了领域适应，其中不同特征在每个领域有各自的权重。
- en: Table 2\. Details of the methods for ASAG based on lexical, syntactic and semantic
    features used in combinations with machine learning classifiers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 基于词汇、句法和语义特征的 ASAG 方法的详细信息，这些特征与机器学习分类器结合使用。
- en: '|  |  |  | Features |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 特征 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Ref. | Year | Classifier | Lexical | Syntactic | Semantic |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 年份 | 分类器 | 词汇 | 句法 | 语义 |'
- en: '| (Mohler et al., [2011](#bib.bib44)) | 2011 | SVM | word overlap | dependency
    parsers, POS tags, n-grams with POS tags | WordNet, LSA |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (Mohler 等， [2011](#bib.bib44)) | 2011 | 支持向量机 | 词汇重叠 | 依赖分析器，词性标注，带词性标注的
    n-gram | WordNet，LSA |'
- en: '| (Meurers et al., [2011](#bib.bib41)) | 2011 | $k$NN | Bg-of-words, n-grams,
    word overlap | dependency parsers, POS tags, n-grams with POS tags, word synonyms
    | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| (Meurers 等， [2011](#bib.bib41)) | 2011 | $k$NN | 词袋模型，n-gram，词汇重叠 | 依赖分析器，词性标注，带词性标注的
    n-gram，词汇同义词 | - |'
- en: '| (Dzikovska et al., [2012](#bib.bib14)) | 2012 | Decision Tree | Bag-of-words,
    n-grams, word overlap | dependency parsers, POS tags, n-grams with POS tags |
    - |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| (Dzikovska 等， [2012](#bib.bib14)) | 2012 | 决策树 | 词袋模型，n-gram，词汇重叠 | 依赖分析器，词性标注，带词性标注的
    n-gram | - |'
- en: '| (Kouylekov et al., [2013](#bib.bib31)) | 2013 | SVM, Naïve Bayes | word overlap
    | - | corpus-based statistical interrelations between words |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| (Kouylekov 等， [2013](#bib.bib31)) | 2013 | 支持向量机，朴素贝叶斯 | 词汇重叠 | - | 基于语料库的词汇统计关联
    |'
- en: '| (Levy et al., [2013](#bib.bib36)) | 2013 | Naïve Bayes | Bag-of-words, n-grams
    | dependency parsers, POS tags, n-grams with POS tags | WordNet, LSA, ESA |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (Levy 等， [2013](#bib.bib36)) | 2013 | 朴素贝叶斯 | 词袋模型，n-gram | 依赖分析器，词性标注，带词性标注的
    n-gram | WordNet，LSA，ESA |'
- en: '| (Heilman and Madnani, [2013](#bib.bib25)) | 2013 | Logistic Regression |
    word/character overlap | - | - |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| (Heilman 和 Madnani， [2013](#bib.bib25)) | 2013 | 逻辑回归 | 词汇/字符重叠 | - | - |'
- en: '| (Ott et al., [2013](#bib.bib49)) | 2013 | SVM, Logistic Regression | Bag-of-words,
    n-grams, word/character overlap | dependency parsers, POS tags, n-grams with POS
    tags, minimum edit distance | WordNet, LSA, ESA |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| (Ott 等， [2013](#bib.bib49)) | 2013 | 支持向量机，逻辑回归 | 词袋模型，n-gram，词汇/字符重叠 | 依赖分析器，词性标注，带词性标注的
    n-gram，最小编辑距离 | WordNet，LSA，ESA |'
- en: '| (Jimenez et al., [2013](#bib.bib28)) | 2013 | Bagged Decision Tree | word/character
    overlap | - | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| (Jimenez 等， [2013](#bib.bib28)) | 2013 | 装袋决策树 | 词汇/字符重叠 | - | - |'
- en: '| (Ramachandran et al., [2015](#bib.bib59)) | 2015 | Random Forest Regressor
    | Bag-of-words, n-grams, word overlap | dependency parsers, POS tags, word-graphs,
    phrase patterns | WordNet |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (Ramachandran 等, [2015](#bib.bib59)) | 2015 | 随机森林回归器 | 词袋模型, n-grams, 词重叠
    | 依存句法分析器, 词性标记, 词图, 短语模式 | WordNet |'
- en: '| (Magooda et al., [2016](#bib.bib40)) | 2016 | SVM | Bag-of-words, word/character
    overlap | - | word2vec, GloVE |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| (Magooda 等, [2016](#bib.bib40)) | 2016 | 支持向量机 | 词袋模型, 词/字符重叠 | - | word2vec,
    GloVE |'
- en: '| (Sultan et al., [2016](#bib.bib66)) | 2016 | Random Forest | bag-of-words,
    n-grams, word/character overlap | - | word2vec, GloVE |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| (Sultan 等, [2016](#bib.bib66)) | 2016 | 随机森林 | 词袋模型, n-grams, 词/字符重叠 | -
    | word2vec, GloVE |'
- en: '| (Roy et al., [2016](#bib.bib62)) | 2016 | Logistic Regression | word overlap
    | - | word2vec, WordNet, LSA |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| (Roy 等, [2016](#bib.bib62)) | 2016 | 逻辑回归 | 词重叠 | - | word2vec, WordNet,
    LSA |'
- en: '| (Galhardi et al., [2018](#bib.bib21)) | 2018 | Random Forests, Extreme Gradient
    Boosting | bag-of-words, n-grams, word/character overlap | dependency parsers,
    POS tags | WordNet (synonym similarity) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| (Galhardi 等, [2018](#bib.bib21)) | 2018 | 随机森林, 极端梯度提升 | 词袋模型, n-grams, 词/字符重叠
    | 依存句法分析器, 词性标记 | WordNet (同义词相似度) |'
- en: '| (Kumar et al., [2019](#bib.bib33)) | 2019 | Random Forest | Bag-of-words,
    n-grams, word overlap | dependency parsers, POS tags, n-grams with POS tags |
    word2vec, doc2vec |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| (Kumar 等, [2019](#bib.bib33)) | 2019 | 随机森林 | 词袋模型, n-grams, 词重叠 | 依存句法分析器,
    词性标记, 带词性标记的 n-grams | word2vec, doc2vec |'
- en: 'Table 3\. Overview of performance of the ASAG methods based on hand-engineered
    features listed in Table [2](#S7.T2 "Table 2 ‣ 7.1\. Lexical features ‣ 7\. Hand-engineered
    features and Machine Learning ‣ Survey on Automated Short Answer Grading with
    Deep Learning: from Word Embeddings to Transformers"). We report accuracy unless
    otherwise specified. $\mathbf{\hat{F}}$ is the F1 score, $\mathbf{F_{M}}$ and
    $\mathbf{F_{m}}$ are the macro-averaged and micro-averaged F1 score, QW-K is the
    quadratic weighted kappa measure, RMSE is the root mean square error, and $\mathbf{\rho}$
    is the Pearson’s correlation coefficient. Performance measures are reported in
    the same precision as in the original papers.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 基于手工设计特征的 ASAG 方法性能概述，详细信息见表 [2](#S7.T2 "表 2 ‣ 7.1\. 词汇特征 ‣ 7\. 手工设计特征与机器学习
    ‣ 自动短答案评分的深度学习综述：从词嵌入到变压器")。除非另有说明，我们报告准确率。$\mathbf{\hat{F}}$ 是 F1 分数，$\mathbf{F_{M}}$
    和 $\mathbf{F_{m}}$ 分别是宏平均和微平均 F1 分数，QW-K 是二次加权 Kappa 测度，RMSE 是均方根误差，$\mathbf{\rho}$
    是 Pearson 相关系数。性能度量以与原始论文相同的精度报告。
- en: '|  |  |  | SciEntsBank | Beetle | Texas2011 | Other |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | SciEntsBank | Beetle | Texas2011 | 其他 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Ref. | Year | Classifier | 2-way | 3-way | 5-way | 2-way | 3-way | 5-way
    |  |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 年份 | 分类器 | 2 分类 | 3 分类 | 5 分类 | 2 分类 | 3 分类 | 5 分类 |  |  |'
- en: '| (Mohler et al., [2011](#bib.bib44)) | 2011 | SVM | - | - | - | - | - | -
    | $0.518$ ($\mathbf{\rho}$) $0.998$ (RMSE) | - |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| (Mohler 等, [2011](#bib.bib44)) | 2011 | 支持向量机 | - | - | - | - | - | - | $0.518$
    ($\mathbf{\rho}$) $0.998$ (RMSE) | - |'
- en: '| (Meurers et al., [2011](#bib.bib41)) | 2011 | $k$NN | - | - | - | - | - |
    - | - | $0.79$ English Dev. Corpus |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| (Meurers 等, [2011](#bib.bib41)) | 2011 | $k$NN | - | - | - | - | - | - |
    - | $0.79$ 英语开发语料库 |'
- en: '| (Dzikovska et al., [2012](#bib.bib14)) | 2012 | Decision Tree | - | - | $0.29$ ($F_{M}$)
    $0.42$ ($F_{m}$) | - | - | - | - | - |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| (Dzikovska 等, [2012](#bib.bib14)) | 2012 | 决策树 | - | - | $0.29$ ($F_{M}$)
    $0.42$ ($F_{m}$) | - | - | - | - | - |'
- en: '| (Kouylekov et al., [2013](#bib.bib31)) | 2013 | SVM, Naïve Bayes | $0.612$
    | $0.55$ | $0.421$ | $0.648$ | $0.523$ | $0.464$ | - | - |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| (Kouylekov 等, [2013](#bib.bib31)) | 2013 | 支持向量机, 朴素贝叶斯 | $0.612$ | $0.55$
    | $0.421$ | $0.648$ | $0.523$ | $0.464$ | - | - |'
- en: '| (Levy et al., [2013](#bib.bib36)) | 2013 | Naïve Bayes | $0.696$ ($\hat{F}$)
    | $0.606$ ($\hat{F}$) | $0.464$ ($\hat{F}$) | - | - | - | - | - |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| (Levy 等, [2013](#bib.bib36)) | 2013 | 朴素贝叶斯 | $0.696$ ($\hat{F}$) | $0.606$
    ($\hat{F}$) | $0.464$ ($\hat{F}$) | - | - | - | - | - |'
- en: '| (Heilman and Madnani, [2013](#bib.bib25)) | 2013 | Logistic Regression |
    - | - | $0.524$ ($\hat{F}$) | - | - | $0.659$ ($\hat{F}$) | - | - |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (Heilman 和 Madnani, [2013](#bib.bib25)) | 2013 | 逻辑回归 | - | - | $0.524$ ($\hat{F}$)
    | - | - | $0.659$ ($\hat{F}$) | - | - |'
- en: '| (Ott et al., [2013](#bib.bib49)) | 2013 | SVM, Logistic Regression | $0.684$
    | $0.612$ | $0.486$ | $0.77$ | $0.624$ | $0.588$ | - | - |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| (Ott 等, [2013](#bib.bib49)) | 2013 | 支持向量机, 逻辑回归 | $0.684$ | $0.612$ | $0.486$
    | $0.77$ | $0.624$ | $0.588$ | - | - |'
- en: '| (Jimenez et al., [2013](#bib.bib28)) | 2013 | Bagged Decision Tree | $0.726$
    | $0.649$ | $0.527$ | $0.724$ | $0.538$ | $0.513$ | - | - |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| (Jimenez 等, [2013](#bib.bib28)) | 2013 | 带袋决策树 | $0.726$ | $0.649$ | $0.527$
    | $0.724$ | $0.538$ | $0.513$ | - | - |'
- en: '| (Ramachandran et al., [2015](#bib.bib59)) | 2015 | Random Forest Regressors
    | - | - | - | - | - | - | $0.61$ ($\mathbf{\rho}$) $0.86$ (RMSE) | $0.78$ (QW-K)
    ASAP-SAS |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| (Ramachandran et al., [2015](#bib.bib59)) | 2015 | 随机森林回归器 | - | - | - |
    - | - | - | $0.61$ ($\mathbf{\rho}$) $0.86$ (RMSE) | $0.78$ (QW-K) ASAP-SAS |'
- en: '| (Magooda et al., [2016](#bib.bib40)) | 2016 | SVM | $0.605$ ($\hat{F}$) |
    - | $0.48$ ($\hat{F}$) | - | - | - | - | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| (Magooda et al., [2016](#bib.bib40)) | 2016 | 支持向量机 | $0.605$ ($\hat{F}$)
    | - | $0.48$ ($\hat{F}$) | - | - | - | - | - |'
- en: '| (Sultan et al., [2016](#bib.bib66)) | 2016 | Random Forest | - | - | $0.56$
    ($\hat{F}$) | - | - | - | $0.85$ ($\mathbf{\rho}$) $0.63$ (RMSE) | - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| (Sultan et al., [2016](#bib.bib66)) | 2016 | 随机森林 | - | - | $0.56$ ($\hat{F}$)
    | - | - | - | $0.85$ ($\mathbf{\rho}$) $0.63$ (RMSE) | - |'
- en: '| (Roy et al., [2016](#bib.bib62)) | 2016 | Logistic Regression | - | - | $0.565$
    ($\hat{F}$) | - | - | - | $0.82$ (RMSE) | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| (Roy et al., [2016](#bib.bib62)) | 2016 | 逻辑回归 | - | - | $0.565$ ($\hat{F}$)
    | - | - | - | $0.82$ (RMSE) | - |'
- en: '| (Galhardi et al., [2018](#bib.bib21)) | 2018 | Random Forest, Extreme Gradient
    Boosting | $0.775$ | $0.719$ | $0.59$ | $0.81$ | $0.643$ | $0.644$ | - | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| (Galhardi et al., [2018](#bib.bib21)) | 2018 | 随机森林，极端梯度提升 | $0.775$ | $0.719$
    | $0.59$ | $0.81$ | $0.643$ | $0.644$ | - | - |'
- en: '| (Kumar et al., [2019](#bib.bib33)) | 2019 | Random Forest | - | - | - | -
    | - | - | - | $0.791$ (QW-K) ASAP-SAS |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| (Kumar et al., [2019](#bib.bib33)) | 2019 | 随机森林 | - | - | - | - | - | -
    | - | $0.791$ (QW-K) ASAP-SAS |'
- en: 7.2\. Syntactic features
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 句法特征
- en: Syntactic features are able to quantify important information about the meaning
    of a sentence, as they detect and describe the roles and dependencies of the words
    in it. The ability to characterize the relation between words enables to perform
    inference about the meaning of a textual answer (van der Lee and van den Bosch,
    [2017](#bib.bib73)). Basic approaches to extract syntactic features from text
    are using parse trees and part-of-speech tagging (POS tags) or dependency n-grams (Popović,
    [2011](#bib.bib53)). Dependency n-grams are derived from the syntactical relation
    between words by grouping, for instance, verb and subject together. In (Levy et al.,
    [2013](#bib.bib36)), syntactic features were computed by generating n-grams that
    consist of combinations of POS tags. The dependency of words in sequences was
    subsequently used to assess the content of an answer and evaluate its similarity
    with a reference answer. This method worked based on the concept of paraphrasing
    a sentence, the result of which consists of a sentence having different wording
    but similar meaning. In (Dzikovska et al., [2012](#bib.bib14)), the authors used
    different similarity scores based on POS tags. Other approaches focused on the
    syntactical dependency, used parse trees, and extracted graph alignments features (Mohler
    et al., [2011](#bib.bib44)). Similarity measures were enriched by generating word
    pairs with similar POS tags and calculating the corresponding similarity. The
    underlying assumption was that similarly structured answers are more likely to
    have a similar meaning.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 句法特征能够量化关于句子意义的重要信息，因为它们检测并描述句子中单词的角色和依赖关系。能够表征单词之间关系的能力使得对文本答案的意义进行推理成为可能（van
    der Lee 和 van den Bosch, [2017](#bib.bib73)）。从文本中提取句法特征的基本方法是使用解析树和词性标注（POS 标记）或依赖
    n-gram（Popović, [2011](#bib.bib53)）。依赖 n-gram 是通过对单词的句法关系进行分组来推导的，例如，将动词和主语放在一起。在（Levy
    et al., [2013](#bib.bib36)）中，句法特征是通过生成由 POS 标记组合组成的 n-gram 计算的。序列中单词的依赖关系随后用于评估答案的内容并评估其与参考答案的相似性。这种方法基于对句子进行释义的概念，其结果是一个具有不同措辞但相似意义的句子。在（Dzikovska
    et al., [2012](#bib.bib14)）中，作者使用了基于 POS 标记的不同相似性评分。其他方法则关注于句法依赖，使用了解析树，并提取了图对齐特征（Mohler
    et al., [2011](#bib.bib44)）。通过生成具有相似 POS 标记的单词对并计算相应的相似性，丰富了相似性测量。其基本假设是结构相似的答案更可能具有相似的意义。
- en: 7.3\. Semantic features
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 语义特征
- en: Lexical features can not capture the semantic content of sentences, and syntactic
    features are able to catch such characteristics only to a limited extent. Thus,
    more sophisticated features were designed exploiting knowledge-bases to recognize
    the meaning of words therefore more robustly computing similarity among sentences.
    These methods used knowledge sources like WordNet (Miller, [1995](#bib.bib43))
    and computational approaches based on Latent Semantic Analysis (LSA) (Landauer
    et al., [1998](#bib.bib35)) and Explicit Semantic Analysis (ESA) (Gabrilovich
    and Markovitch, [2007a](#bib.bib16)), combined with different similarity metrics.
    WordNet models the semantic relation between words inducing synonyms and hyponyms.
    Many methods used Wordnet in combination with similarity metrics to better incorporate
    the semantic meaning of words. LSA is a corpus-based similarity method, in which
    words are represented as vectors in a multi-dimensional semantic space (Kaur and
    Hornof, [2005](#bib.bib30)). This method gained popularity and was demonstrated
    to perform better than word and n-gram vectors (Mohler and Mihalcea, [2009](#bib.bib45)).
    LSA was used to estimate the similarity between words and combined with a word-weighting
    factor to enhance the relevance of specific words in (Kouylekov et al., [2013](#bib.bib31)).
    ESA was designed to use the knowledge extracted from Wikipedia (Gabrilovich and
    Markovitch, [2007b](#bib.bib17)) and was demonstrated to perform comparably to
    LSA or to outperform it in some cases. In (Levy et al., [2013](#bib.bib36)), the
    authors included the semantic similarity by using predefined context vectors based
    on WordNet. They proposed to first parse the answers according to the dependencies
    of words within the sentences, and subsequently calculate the semantic similarity
    using the closest common ancestor and shortest path length using ESA and WordNet.
    In (Ott et al., [2013](#bib.bib49)), WordNet was also used to extract similarity
    information between the reference and student answer. The authors highlighted
    the importance of using different features to extract semantic information and
    weight them for an effective classification task.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇特征无法捕捉句子的语义内容，而句法特征只能在有限的程度上捕捉这些特征。因此，设计了更复杂的特征，通过利用知识库来识别词汇的意义，从而更稳健地计算句子之间的相似性。这些方法使用了像WordNet （Miller,
    [1995](#bib.bib43)）这样的知识来源，以及基于潜在语义分析（LSA）（Landauer et al., [1998](#bib.bib35)）和显式语义分析（ESA）（Gabrilovich
    and Markovitch, [2007a](#bib.bib16)）的计算方法，并结合了不同的相似性度量。WordNet建模了词汇之间的语义关系，引导出同义词和下义词。许多方法将WordNet与相似性度量结合使用，以更好地结合词汇的语义含义。LSA是一种基于语料库的相似性方法，其中词汇在多维语义空间中表示为向量（Kaur
    and Hornof, [2005](#bib.bib30)）。这种方法获得了普及，并被证明在性能上优于词汇和n-gram向量（Mohler and Mihalcea,
    [2009](#bib.bib45)）。LSA被用来估计词汇之间的相似性，并结合词汇加权因子来增强特定词汇的相关性（Kouylekov et al., [2013](#bib.bib31)）。ESA的设计旨在利用从维基百科中提取的知识（Gabrilovich
    and Markovitch, [2007b](#bib.bib17)），并被证明在某些情况下表现得与LSA相当或更优。在（Levy et al., [2013](#bib.bib36)）中，作者通过使用基于WordNet的预定义上下文向量来包括语义相似性。他们提出首先根据句子中词汇的依赖关系来解析答案，然后使用ESA和WordNet计算最近公共祖先和最短路径长度来计算语义相似性。在（Ott
    et al., [2013](#bib.bib49)）中，WordNet也被用来提取参考答案和学生答案之间的相似性信息。作者强调了使用不同特征提取语义信息并加权它们以实现有效分类任务的重要性。
- en: 8\. Deep learning methods
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 深度学习方法
- en: The deep learning developments in the ASAG field align with the methodological
    advances in the field of NLP. We organize the deep learning methods for ASAG in
    three categories, that correspond to the historical development of NLP and the
    relation to the text representation methods.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ASAG领域的深度学习发展与NLP领域的方法论进展保持一致。我们将ASAG的深度学习方法分为三类，这些类别对应于NLP的历史发展及其与文本表示方法的关系。
- en: The first category contains methods based on word embedding models (e.g. Word2Vec (Mikolov
    et al., [2013](#bib.bib42))). These models compute representations that transform
    similar words into vectors that are close to each other in an embedded latent
    space, and generate sentence embeddings by either summing or averaging the single
    word embeddings. The word and sentence embeddings are able to capture semantic
    information in textual data more effectively than previous hand-engineered features (Hightower,
    [2020](#bib.bib26)). Methods in the second category deploy recurrent neural networks
    (RNNs), of which those based on long short-term memory (LSTM) networks are very
    popular, to model the sequential characteristics of textual data. These methods
    are able to capture semantic properties of the text by considering word sentences
    of different lengths, and longer-range relationships between words in sentences.
    This allowed the prediction models to carry out more robust and effective inferences
    on given answers (Saha et al., [2019](#bib.bib63)). The third category consists
    of methods that deploy attention-based mechanisms, able to describe long-range
    relationships between words in a sentence. Different from RNNs, attention-based
    methods do not need to explicitly model the sequential characteristics of words
    in sentences, but are able to process longer sentences in parallel. These architectural
    advancements were determined by the use of several self-attention components,
    each of which captures a specific relation between words. Architectures based
    only on attention are called Transformers (Vaswani et al., [2017](#bib.bib74)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类包含基于词嵌入模型的方法（例如 Word2Vec (Mikolov 等, [2013](#bib.bib42)））。这些模型计算表示，将相似的词转换为在嵌入潜在空间中彼此接近的向量，并通过对单词嵌入进行求和或平均来生成句子嵌入。词嵌入和句子嵌入能够比以前的人工设计特征（Hightower,
    [2020](#bib.bib26)）更有效地捕捉文本数据中的语义信息。第二类方法使用递归神经网络（RNNs），其中基于长短期记忆（LSTM）网络的方法非常流行，用于建模文本数据的顺序特性。这些方法通过考虑不同长度的单词句子以及句子中单词之间的长距离关系来捕捉文本的语义特性。这使得预测模型能够对给定答案进行更为稳健和有效的推断（Saha
    等, [2019](#bib.bib63)）。第三类方法部署基于注意力机制的技术，能够描述句子中单词之间的长距离关系。与 RNNs 不同，基于注意力的方法不需要显式建模句子中单词的顺序特性，而是能够并行处理较长的句子。这些架构进步由多个自注意力组件的使用决定，每个组件捕捉单词之间的特定关系。仅基于注意力的架构被称为
    Transformers (Vaswani 等, [2017](#bib.bib74))。
- en: We elaborate on the details of the three categories of methods in the remainder
    of this section, addressing the benefits, drawbacks, and performance of the learned
    text representations. In Table LABEL:tab:DLdesc, we provide an overview of the
    methodological aspects of the reviewed papers, while in Table LABEL:tab:DLresults
    we summarize their performance results on benchmark data sets.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节的其余部分详细阐述这三类方法的细节，讨论学习到的文本表示的优点、缺点和性能。在表 LABEL:tab:DLdesc 中，我们提供了所审阅论文的方法论方面的概述，而在表
    LABEL:tab:DLresults 中，我们总结了它们在基准数据集上的性能结果。
- en: 8.1\. Word embeddings
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1. 词嵌入
- en: Deep learning methods for ASAG based on word embeddings encode semantically
    similar words to close points in a latent space (Roy et al., [2016](#bib.bib62);
    Magooda et al., [2016](#bib.bib40); Sultan et al., [2016](#bib.bib66)). The general
    success of word embeddings is attributable to their ability to describe rich semantic
    features of text. These methods were demonstrated successful to improve the evaluation
    of word similarity, but did not clearly outperform previous methods for representation
    of entire sentences in ASAG systems. In (Magooda et al., [2016](#bib.bib40)),
    for instance, the authors compared different similarity measures on word vector
    representations of text obtained using pre-trained embedding models, such as Word2Vec (Mikolov
    et al., [2013](#bib.bib42)) and GloVe (Pennington et al., [2014](#bib.bib51)).
    They found that word and sentence embeddings achieved below-average results on
    the SemEval 5-way task. Other approaches that solely focused on embeddings did
    not outperform the models with hand-engineered features on ASAG tasks (Gomaa and
    Fahmy, [2020](#bib.bib22); Magooda et al., [2016](#bib.bib40)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词嵌入的 ASAG 深度学习方法将语义相似的词编码为潜在空间中的相近点（Roy 等，[2016](#bib.bib62)；Magooda 等，[2016](#bib.bib40)；Sultan
    等，[2016](#bib.bib66)）。词嵌入的一般成功归因于它们描述文本丰富语义特征的能力。这些方法已成功提高了词语相似度的评估，但在 ASAG 系统中对整个句子的表示方面并未明显优于以前的方法。例如，在（Magooda
    等，[2016](#bib.bib40)）中，作者比较了使用预训练嵌入模型（如 Word2Vec（Mikolov 等，[2013](#bib.bib42)）和
    GloVe（Pennington 等，[2014](#bib.bib51)）获得的文本词向量表示上的不同相似性度量。他们发现词和句子的嵌入在 SemEval
    5-way 任务中表现低于平均水平。其他仅专注于嵌入的 approaches 在 ASAG 任务中未能超越手工设计特征的模型（Gomaa 和 Fahmy，[2020](#bib.bib22)；Magooda
    等，[2016](#bib.bib40)）。
- en: To compensate for the shortcomings of word embeddings techniques to represent
    entire sentences, several methods that combine word embeddings with hand-engineered
    features were designed. The method proposed in (Roy et al., [2016](#bib.bib62)),
    for instance, combined an external knowledge-like paraphrase database and WordNet
    with syntactical similarity features. It achieved top performance results on the
    SciEntsBank dataset, with improved generalization capabilities. Other approaches,
    such as that proposed in (Sultan et al., [2016](#bib.bib66)), computed whole-sentence
    representations by combining (sum or average) single word embeddings. Lexically
    similar word pairs were obtained using an external paraphrase data set, with labeled
    graded short answers. The semantic similarity of words was computed using pre-trained
    word embeddings and the cosine similarity function, while the representation of
    entire sentences was computed by summing up the word embedding vectors. This approach
    resulted in relatively good results on a proprietary data set, but highlighted
    the need for further developments of sentence representations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补词嵌入技术在表示整个句子时的不足，设计了几种将词嵌入与手工设计特征结合的方法。例如，（Roy 等，[2016](#bib.bib62)）提出的方法结合了类似外部知识的同义词库和
    WordNet 以及句法相似性特征。在 SciEntsBank 数据集上取得了顶级性能结果，具有改进的泛化能力。其他方法，如（Sultan 等，[2016](#bib.bib66)）提出的方法，通过结合（求和或平均）单词嵌入计算了整个句子的表示。使用外部同义词数据集获得了词汇上相似的词对，带有标记的分级短答案。使用预训练词嵌入和余弦相似度函数计算词语的语义相似性，而通过对词嵌入向量求和来计算整个句子的表示。这种方法在专有数据集上取得了相对良好的结果，但突出了对句子表示进一步发展的需求。
- en: Table 4\. Overview of deep learning methods for ASAG, the type of semantics
    that they learn from the text, and the focus of their architecture.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. ASAG 的深度学习方法概述、它们从文本中学习的语义类型以及它们架构的重点。
- en: '|  |  |  |  |  |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |'
- en: '| Ref. | Cat. | Year | Model | Features | Focus |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 类别 | 年份 | 模型 | 特征 | 重点 |'
- en: '| (Riordan et al., [2017](#bib.bib61)) | DL3 | 2017 | LSTM+CNN with attention
    | Combination of sequence representation from different model architecture (CNN,
    LSTM) | Complex and stacked model to leverage benefit from different models |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| (Riordan 等，[2017](#bib.bib61)) | DL3 | 2017 | 带注意力的 LSTM+CNN | 结合了不同模型架构（CNN、LSTM）的序列表示
    | 复杂的堆叠模型以利用不同模型的优势 |'
- en: '| (Kumar et al., [2017](#bib.bib32)) | DL2 | 2017 | BiLSTM | Sequence representation
    with BiLSTM | Complex model architecture and included data augmentation |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| (Kumar 等，[2017](#bib.bib32)) | DL2 | 2017 | BiLSTM | 使用 BiLSTM 的序列表示 | 复杂的模型架构和包含数据增强
    |'
- en: '| (Saha et al., [2018](#bib.bib64)) | DL1 | 2018 | Random Forest | Sentence
    representation combined with engineered features | Combination of sequence representation
    and engineered features with focus on domain adaptation |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| (Saha 等, [2018](#bib.bib64)) | DL1 | 2018 | 随机森林 | 句子表示与工程特征的结合 | 关注领域适应的序列表示与工程特征的组合
    |'
- en: '| (Kumar et al., [2019](#bib.bib33)) | DL1 | 2019 | Random Forest | Sentence
    and word representation | Combination of sequence representation and various engineered
    features |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| (Kumar 等, [2019](#bib.bib33)) | DL1 | 2019 | 随机森林 | 句子和词语表示 | 序列表示与各种工程特征的组合
    |'
- en: '| (Wang et al., [2019](#bib.bib75)) | DL3 | 2019 | BiLSTM | Attention based
    sentence representation | Complex structure and incorporation of engineered features
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| (Wang 等, [2019](#bib.bib75)) | DL3 | 2019 | 双向LSTM（BiLSTM） | 基于注意力的句子表示 |
    复杂结构和工程特征的结合 |'
- en: '| (Saha et al., [2019](#bib.bib63)) | DL2 | 2019 | BiLSTM | Combination of
    different sentence representations (domain-specific and unspecific) | Complex
    structure and domain adaptation |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| (Saha 等, [2019](#bib.bib63)) | DL2 | 2019 | 双向长短期记忆网络（BiLSTM） | 不同句子表示（领域特定和非特定）的组合
    | 复杂结构和领域适应 |'
- en: '| (Gomaa and Fahmy, [2020](#bib.bib22)) | DL1 | 2019 | Logistic Regression
    | Sequence representation of words and sentences | Simple application of transfer
    learning |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| (Gomaa 和 Fahmy, [2020](#bib.bib22)) | DL1 | 2019 | 逻辑回归 | 单词和句子的序列表示 | 迁移学习的简单应用
    |'
- en: '| (Sung et al., [2019a](#bib.bib67)) | DL3 | 2019 | BERT | Attention based
    sequence representation | Simple application of transfer learning |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| (Sung 等, [2019a](#bib.bib67)) | DL3 | 2019 | BERT | 基于注意力的序列表示 | 迁移学习的简单应用
    |'
- en: '| (Qi et al., [2019](#bib.bib56)) | DL3 | 2019 | BiLSTM+CNN | attention based
    on BiLSTMs and CNNs | Complex and stacked BiLSTM and CNN |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| (Qi 等, [2019](#bib.bib56)) | DL3 | 2019 | BiLSTM+CNN | 基于BiLSTM和CNN的注意力 |
    复杂的堆叠BiLSTM和CNN |'
- en: '| (Liu et al., [2019a](#bib.bib37)) | DL3 | 2019 | Multiway-attention transformer
    | attention | Complex structure with incorporation of different attention mechanism
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| (Liu 等, [2019a](#bib.bib37)) | DL3 | 2019 | 多维注意力变换器 | 注意力 | 结合不同注意力机制的复杂结构
    |'
- en: '| (Sung et al., [2019b](#bib.bib68)) | DL3 | 2019 | BERT | attention | Transfer
    learning with domain adaptation |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| (Sung 等, [2019b](#bib.bib68)) | DL3 | 2019 | BERT | 注意力 | 领域适应的迁移学习 |'
- en: '| (Zhang et al., [2019](#bib.bib83)) | DL2 | 2019 | LSMT | CBOW word vectors
    | Incorporation of domain-specific knowledge and domain-general knowledge from
    wikipedia |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang 等, [2019](#bib.bib83)) | DL2 | 2019 | LSMT | CBOW词向量 | 结合来自维基百科的领域特定知识和领域通用知识
    |'
- en: '| (Zhang et al., [2020a](#bib.bib84)) | DL2 | 2020 | DBN | Application of Gaussian
    mixture model (GMM) and Cartesian product for feature composition | Complex feature
    engineering by incorporating different feature extraction methods |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang 等, [2020a](#bib.bib84)) | DL2 | 2020 | 深度信念网络（DBN） | 高斯混合模型（GMM）和笛卡尔积用于特征组合
    | 通过结合不同的特征提取方法进行复杂的特征工程 |'
- en: '| (Camus and Filighera, [2020](#bib.bib5)) | DL3 | 2020 | RoBERTa | Attention
    based sequence representation | Targeted transfer learning and incorporation of
    different learning methods (cross-lingual, NLI specific model training) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| (Camus 和 Filighera, [2020](#bib.bib5)) | DL3 | 2020 | RoBERTa | 基于注意力的序列表示
    | 目标迁移学习及结合不同学习方法（跨语言、NLI特定模型训练） |'
- en: '| (Condor, [2020](#bib.bib7)) | DL3 | 2020 | BERT | Attention based sequence
    representation | Simple application of transfer learning |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| (Condor, [2020](#bib.bib7)) | DL3 | 2020 | BERT | 基于注意力的序列表示 | 迁移学习的简单应用
    |'
- en: '| (Sahu and Bhowmick, [2020](#bib.bib65)) | DL1 | 2020 | stacked regression
    ensemble | Compilation of various engineered features | Ensemble of eight different
    regression models via multi-layer perceptron and various engineered features |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (Sahu 和 Bhowmick, [2020](#bib.bib65)) | DL1 | 2020 | 堆叠回归集成 | 各种工程特征的汇编 |
    通过多层感知器和各种工程特征的八种不同回归模型的集成 |'
- en: '| (Gaddipati et al., [2020](#bib.bib18)) | DL2 | 2020 | Stacked BiLSTM (ELMo)
    | Bidirectional LSTM based sequence representation | Simple application of transfer
    learning from pretrained model |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| (Gaddipati 等, [2020](#bib.bib18)) | DL2 | 2020 | 堆叠双向LSTM（ELMo） | 基于双向LSTM的序列表示
    | 从预训练模型简单应用迁移学习 |'
- en: Table 5\. Summary of the performance results of the ASAG methods based on deep
    learning listed in Table LABEL:tab:DLdesc. We report accuracy unless otherwise
    specified. $\mathbf{\hat{F}}$ is the F1 score, QW-K is the quadratic weighted
    kappa measure, C-K is the Cohen’s Kappa, RMSE is the root mean square error, and
    $\mathbf{\rho}$ is the Pearson’s correlation coefficient. Performance measures
    are reported in the same precision as in the original papers.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 总结了表 LABEL:tab:DLdesc 中列出的基于深度学习的ASAG方法的性能结果。除非另有说明，否则我们报告准确度。$\mathbf{\hat{F}}$
    是F1分数，QW-K 是二次加权κ度量，C-K 是科恩κ，RMSE 是均方根误差，而 $\mathbf{\rho}$ 是皮尔逊相关系数。性能测量以与原始论文相同的精度报告。
- en: '|  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |  |  |  |'
- en: '|  | SciEntsBank | Beetle | Texas2011 | Other |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | SciEntsBank | Beetle | Texas2011 | 其他 |'
- en: '|  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Ref. | Cat. | Year | 2-way | 3-way | 5-way | 2-way | 3-way | 5-way |  |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 分类 | 年份 | 2-way | 3-way | 5-way | 2-way | 3-way | 5-way |  |  |'
- en: '| (Riordan et al., [2017](#bib.bib61)) | DL3 | 2017 | $0.712$ | - | $0.533$
    | $0.79$ | - | $0.633$ | $0.518$ ($\mathbf{\rho}$) $0.998$ (RMSE) | $0.723$ (QW-K)
    ASAP-SAS |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| (瑞欧丹等, [2017](#bib.bib61)) | DL3 | 2017 | $0.712$ | - | $0.533$ | $0.79$
    | - | $0.633$ | $0.518$ ($\mathbf{\rho}$) $0.998$ (RMSE) | $0.723$ (QW-K) ASAP-SAS
    |'
- en: '| (Kumar et al., [2017](#bib.bib32)) | DL2 | 2017 | - | $0.634$(MAE) $0.904$(RMSE)
    $0.316$ ($\mathbf{\rho}$) | - | - | - | - | $0.61$ ($\mathbf{\rho}$), $0.77$ (RMSE)
    | - |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| (库马尔等, [2017](#bib.bib32)) | DL2 | 2017 | - | $0.634$(MAE) $0.904$(RMSE)
    $0.316$ ($\mathbf{\rho}$) | - | - | - | - | $0.61$ ($\mathbf{\rho}$), $0.77$ (RMSE)
    | - |'
- en: '| (Saha et al., [2018](#bib.bib64)) | DL1 | 2018 | $0.752$ | $0.654$ | $0.540$
    | - | - | - | $0.57$ ($\mathbf{\rho}$) $0.902$ (RMSE) |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| (萨哈等, [2018](#bib.bib64)) | DL1 | 2018 | $0.752$ | $0.654$ | $0.540$ | -
    | - | - | $0.57$ ($\mathbf{\rho}$) $0.902$ (RMSE) |  |'
- en: '| (Kumar et al., [2019](#bib.bib33)) | DL1 | 2019 | - | - | - | - | - | - |
    - | $0.791$ (QW-K) ASAP-SAS |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| (库马尔等, [2019](#bib.bib33)) | DL1 | 2019 | - | - | - | - | - | - | - | $0.791$ (QW-K)
    ASAP-SAS |'
- en: '| (Wang et al., [2019](#bib.bib75)) | DL3 | 2019 | - | - | - | - | - | - |
    - | $0.77$ (QW-K) ASAP-SAS |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| (王等, [2019](#bib.bib75)) | DL3 | 2019 | - | - | - | - | - | - | - | $0.77$ (QW-K)
    ASAP-SAS |'
- en: '| (Saha et al., [2019](#bib.bib63)) | DL2 | 2019 | $0.803$ ($\hat{F}$) | $0.744$ ($\hat{F}$)
    | $0.656$ ($\hat{F}$) | - | - | - | - | $0.721$ ($\hat{F}$) Large Scale Industry
    Dataset |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| (萨哈等, [2019](#bib.bib63)) | DL2 | 2019 | $0.803$ ($\hat{F}$) | $0.744$ ($\hat{F}$)
    | $0.656$ ($\hat{F}$) | - | - | - | - | $0.721$ ($\hat{F}$) 大规模工业数据集 |'
- en: '| (Gomaa and Fahmy, [2020](#bib.bib22)) | DL1 | 2019 | - | - | $0.503$ ($\hat{F}$)
    | - | - | - | $0.63$ ($\mathbf{\rho}$) $0.91$ (RMSE) | - |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| (戈马和法赫米, [2020](#bib.bib22)) | DL1 | 2019 | - | - | $0.503$ ($\hat{F}$) |
    - | - | - | $0.63$ ($\mathbf{\rho}$) $0.91$ (RMSE) | - |'
- en: '| (Sung et al., [2019a](#bib.bib67)) | DL3 | 2019 | - | $0.68$ ($\hat{F}$)
    | - | - | - | - | - | - |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| (宋等, [2019a](#bib.bib67)) | DL3 | 2019 | - | $0.68$ ($\hat{F}$) | - | - |
    - | - | - | - |'
- en: '| (Qi et al., [2019](#bib.bib56)) | DL3 | 2019 | - | - | - | - | - | - | -
    | $0.969$(Acc) $0.969$ ($\hat{F}$) Chinese data |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| (齐等, [2019](#bib.bib56)) | DL3 | 2019 | - | - | - | - | - | - | - | $0.969$(Acc)
    $0.969$ ($\hat{F}$) 中文数据 |'
- en: '| (Liu et al., [2019a](#bib.bib37)) | DL3 | 2019 | - | - | - | - | - | - |
    - | $0.889$(Acc) $0.944$ (AUC) Real world K-12 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| (刘等, [2019a](#bib.bib37)) | DL3 | 2019 | - | - | - | - | - | - | - | $0.889$(Acc)
    $0.944$ (AUC) 真实世界K-12 |'
- en: '| (Sung et al., [2019b](#bib.bib68)) | DL3 | 2019 | - | - | - | - | - | - |
    - | $0.799$ (Acc) Large scale industry dataset |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| (宋等, [2019b](#bib.bib68)) | DL3 | 2019 | - | - | - | - | - | - | - | $0.799$ (Acc)
    大规模工业数据集 |'
- en: '| (Zhang et al., [2019](#bib.bib83)) | DL2 | 2019 | - | - | - | - | - | - |
    - | $0.626$ (QW-K) ASAP-SAS |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| (张等, [2019](#bib.bib83)) | DL2 | 2019 | - | - | - | - | - | - | - | $0.626$ (QW-K)
    ASAP-SAS |'
- en: '| (Zhang et al., [2020a](#bib.bib84)) | DL2 | 2020 | - | - | - | - | - | -
    | - | $0.850$(Acc) $0.830$ ($\hat{F}$) Cordillera |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| (张等, [2020a](#bib.bib84)) | DL2 | 2020 | - | - | - | - | - | - | - | $0.850$(Acc)
    $0.830$ ($\hat{F}$) Cordillera |'
- en: '| (Camus and Filighera, [2020](#bib.bib5)) | DL3 | 2020 | - | 0.718 | - | -
    | - | - | - | - |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| (卡缪斯和菲利赫拉, [2020](#bib.bib5)) | DL3 | 2020 | - | 0.718 | - | - | - | - |
    - | - |'
- en: '| (Condor, [2020](#bib.bib7)) | DL3 | 2020 | - | - | - | - | - | - | - | $0.76$(Acc)
    $0.684$ (C-K) DT-Grade |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| (康多尔, [2020](#bib.bib7)) | DL3 | 2020 | - | - | - | - | - | - | - | $0.76$(Acc)
    $0.684$ (C-K) DT-Grade |'
- en: '| (Sahu and Bhowmick, [2020](#bib.bib65)) | DL1 | 2020 | - | - | 0.746 | -
    | - | $0.666$ | - | - |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| (萨胡和博米奇, [2020](#bib.bib65)) | DL1 | 2020 | - | - | 0.746 | - | - | $0.666$
    | - | - |'
- en: '| (Gaddipati et al., [2020](#bib.bib18)) | DL2 | 2020 | - | - | - | - | - |
    - | $0.485$ ($\mathbf{\rho}$) $0.978$ (RMSE) | - |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| (加迪帕提等, [2020](#bib.bib18)) | DL2 | 2020 | - | - | - | - | - | - | $0.485$ ($\mathbf{\rho}$)
    $0.978$ (RMSE) | - |'
- en: 'Previous works showed that ASAG systems perform well when word-embeddings and
    hand-engineered features are combined, while the sole use of word-embeddings does
    not contribute to consistently good results (Gomaa and Fahmy, [2020](#bib.bib22);
    Magooda et al., [2016](#bib.bib40)). For instance, in (Kumar et al., [2019](#bib.bib33)),
    good performance results were achieved by constructing a large feature set by
    combining hand-engineered and learned features. The authors proposed a method
    that combines commonly used text representations, such as Word2Vec, Doc2Vec, POS
    tagging, n-gram overlaps, with features that capture the diversity and style of
    formulation of the student answers. The design of this method was based on the
    hypothesis that the use of a more sophisticated textual form, in terms of the
    language used, is an indication for an answer of higher quality. They achieved
    state-of-the-art results on the ASAP-SAS data set and demonstrated that the Word2Vec
    and Doc2Vec embeddings, combined with features for quantification of text overlaps
    and weighted keywords play a relevant role in the automatic grading of answers.
    By including features that represent the complexity of the answers, the accuracy
    of the proposed system improved significantly. On the one hand, researchers agree
    on the importance and descriptive capabilities of embeddings (Kumar et al., [2019](#bib.bib33)).
    On the other hand, it was demonstrated that well hand-engineered features that
    capture complex properties of the text based on prior knowledge can be beneficial
    for the overall system performance: knowledge extracted from the text by embedding-based
    representations is complementary to that of previously proposed syntactical, semantic,
    and lexical features (Roy et al., [2016](#bib.bib62)).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究表明，当词嵌入和手工设计特征结合使用时，ASAG系统表现良好，而单独使用词嵌入则不能 consistently produce good results（Gomaa
    and Fahmy, [2020](#bib.bib22)；Magooda et al., [2016](#bib.bib40)）。例如，在（Kumar et
    al., [2019](#bib.bib33)）中，通过结合手工设计和学习的特征来构建一个大特征集，从而实现了良好的性能结果。作者提出了一种方法，该方法结合了常用的文本表示，如Word2Vec、Doc2Vec、词性标注、n-gram重叠，以及捕捉学生答案多样性和风格的特征。该方法的设计基于这样的假设：在语言使用方面使用更复杂的文本形式是高质量答案的一个指标。他们在ASAP-SAS数据集上取得了最先进的结果，并证明了Word2Vec和Doc2Vec嵌入结合用于文本重叠量化和加权关键词的特征在自动评分中发挥了重要作用。通过包括表示答案复杂性的特征，所提系统的准确性显著提高。一方面，研究人员一致认可嵌入的作用和描述能力（Kumar
    et al., [2019](#bib.bib33)）。另一方面，研究表明，基于先验知识捕捉文本复杂属性的手工设计特征对于整体系统性能有益：通过嵌入式表示从文本中提取的知识与之前提出的句法、语义和词汇特征是互补的（Roy
    et al., [2016](#bib.bib62)）。
- en: 8.2\. Sequence-based models
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 基于序列的模型
- en: Sequential machine learning models were applied to ASAG in order to improve
    the quality of the learned features and robustness of the word and sentence representation.
    Kumar et al. (Kumar et al., [2017](#bib.bib32)) proposed to use a siamese bidirectional-LSTM
    architecture combined with a pooling layer that uses earth-mover distance. The
    authors compared the pairwise distance between the latent vectors of the reference
    answer and the student answer. They showed that the extraction of semantic textual
    features using sequence-based models improves the quality of the learned representation
    and the performance of the automated grading systems.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序机器学习模型被应用于ASAG，以提高所学特征的质量和词语与句子表示的鲁棒性。Kumar等人（Kumar et al., [2017](#bib.bib32)）提出使用一种结合了地球移动者距离的双向LSTM架构和池化层的方法。作者比较了参考答案和学生答案的潜在向量之间的成对距离。他们展示了使用基于序列的模型提取语义文本特征能够提高所学表示的质量和自动评分系统的性能。
- en: Further focus was given to the training of sequential models for NLP and their
    adaptation to ASAG problems. Specifically, fine-tuning of pre-trained models for
    the analysis of text sequence from other, more general domains to the task of
    ASAG was explored using techniques for transfer learning (Cai, [2019](#bib.bib4);
    Tsiakmaki et al., [2020](#bib.bib72)). Transfer learning provides the possibility
    of exploiting features that are learned from large corpora of text data, and that
    have more powerful semantic representation capabilities. In this context, the
    Universal Sentence Representation model was often used in transfer learning settings
    to extract word representations (Conneau et al., [2017](#bib.bib8)). The authors
    trained a bi-directional LSTM network on the large Stanford Natural Language Inference
    corpus (Conneau et al., [2017](#bib.bib8)) and subsequently adapted the feature
    extraction on the SemEval ASAG tasks (Saha et al., [2018](#bib.bib64)). The authors
    achieved state-of-the-art performance on the SemEval tasks (see Table LABEL:tab:DLresults),
    by computing the semantic similarity between word- and sentence-embeddings of
    the reference and student answers computed via the adapted models. In (Saha et al.,
    [2018](#bib.bib64)), hand-engineered features were also used for different question
    types. These results indicate that combining learned sequential-based and hand-engineered
    features increase the performance of ASAG methods.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言处理（NLP）中的序列模型训练及其在ASAG问题上的适应，给予了更多关注。具体而言，通过迁移学习技术探索了将预训练模型从其他更一般领域的文本序列分析任务调整到ASAG任务中的细化（Cai,
    [2019](#bib.bib4); Tsiakmaki et al., [2020](#bib.bib72)）。迁移学习提供了利用从大型文本数据语料库中学到的特征的可能性，这些特征具有更强的语义表示能力。在这种背景下，通用句子表示模型通常在迁移学习设置中用于提取词汇表示（Conneau
    et al., [2017](#bib.bib8)）。作者在大型斯坦福自然语言推断语料库上训练了一个双向LSTM网络（Conneau et al., [2017](#bib.bib8)），然后将特征提取适应于SemEval
    ASAG任务（Saha et al., [2018](#bib.bib64)）。通过计算适应模型计算的参考答案和学生答案的词汇和句子嵌入之间的语义相似性，作者在SemEval任务上取得了**最先进的**性能（见表LABEL:tab:DLresults）。在(Saha
    et al., [2018](#bib.bib64))中，也使用了手工设计的特征来处理不同的问题类型。这些结果表明，将学习的基于序列的特征和手工设计的特征相结合，可以提高ASAG方法的性能。
- en: 'Domain adaptation of pre-trained sequence-based models was also explored in (Zhang
    et al., [2019](#bib.bib83)). The authors trained an LSTM with focus on the incorporation
    of domain-specific knowledge and domain-general knowledge from Wikipedia. In (Saha
    et al., [2019](#bib.bib63)), a combination of domain adaptation and transfer learning
    techniques were used (see Figure [2](#S8.F2 "Figure 2 ‣ 8.2\. Sequence-based models
    ‣ 8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers")). The authors argued that the
    performance of systems that rely on textual similarity, paraphrasing, or entailment
    depends on the domain in which they are applied. They, thus, propose a deep learning
    architecture that deploys an encoder with a BiLSTM layer to embed the reference
    answer and the student answer into a vector representation, trained on questions
    and answers from multiple domains. To compute answer similarity, the authors propose
    to train a generic scoring model on answers from all considered domains, and specific
    scoring models trained on answers drawn from specific domains. The final prediction
    on the correctness of a test answer is computed by summing up the scores given
    by the generic and domain-specific scorers. This method achieved state-of-the-art
    results on the SemEval tasks, demonstrating that a combined training of a prediction
    model with multi-domain and domain-specific data contributes to an increase in
    the overall performance of an ASAG system. This indicates that the performance
    results of models for ASAG tasks are influenced by the capabilities of the underlying
    word-embedding models to effectively capture the semantic properties of words
    and sentences. Thus, the use of models that can integrate longer sequential relations
    in textual input proved to enrich the semantic information of the embeddings,
    and improved model performance. However, this is usually accompanied by an increase
    in the complexity of the models  (Sahu and Bhowmick, [2020](#bib.bib65); Zhang
    et al., [2020a](#bib.bib84)).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '预训练序列模型的领域适应也在（Zhang et al., [2019](#bib.bib83)）中进行了探索。作者训练了一种 LSTM，重点是从维基百科中融合领域特定知识和领域通用知识。在（Saha
    et al., [2019](#bib.bib63)）中，使用了领域适应和迁移学习技术的组合（见图 [2](#S8.F2 "Figure 2 ‣ 8.2\.
    Sequence-based models ‣ 8\. Deep learning methods ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers")）。作者认为，依赖文本相似性、释义或蕴涵的系统性能取决于其应用的领域。因此，他们提出了一种深度学习架构，使用具有
    BiLSTM 层的编码器将参考答案和学生答案嵌入到向量表示中，训练时使用来自多个领域的问答数据。为了计算答案相似性，作者提议在所有考虑的领域的答案上训练一个通用评分模型，以及在特定领域的答案上训练的特定评分模型。最终对测试答案的正确性的预测是通过将通用评分器和领域特定评分器给出的分数相加来计算的。这种方法在
    SemEval 任务中取得了最先进的结果，表明对具有多领域和领域特定数据的预测模型的综合训练有助于提高 ASAG 系统的整体性能。这表明 ASAG 任务模型的性能结果受限于底层词嵌入模型有效捕捉词汇和句子的语义特性的能力。因此，能够整合更长序列关系的模型被证明能丰富嵌入的语义信息，并改善模型性能。然而，这通常伴随着模型复杂性的增加（Sahu
    和 Bhowmick, [2020](#bib.bib65); Zhang et al., [2020a](#bib.bib84)）。'
- en: '![Refer to caption](img/1b490251afa891ac453b16fa6b151000.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1b490251afa891ac453b16fa6b151000.png)'
- en: Figure 2\. Architecture of the method published in (Saha et al., [2019](#bib.bib63)).
    The authors used domain adaptation by training different similarity scorers on
    domain-specific subsets of answers and on a domain-independent data set.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 发表在（Saha et al., [2019](#bib.bib63)）中的方法架构。作者通过在领域特定的回答子集和领域无关的数据集上训练不同的相似性评分器来实现领域适应。
- en: Ensembles of sequence-based models were also considered as a way to enhance
    the representation power of learned features (Zhang et al., [2020a](#bib.bib84);
    Sahu and Bhowmick, [2020](#bib.bib65)). Both works used an ensemble of eight different
    stacked regression models whose predictions were combined using a multi-layer
    perceptron. The authors included summaries of the responses in the training data,
    which contributed to learning more robust models.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 序列模型的集成也被认为是一种增强学习特征表示能力的方法（Zhang et al., [2020a](#bib.bib84); Sahu 和 Bhowmick,
    [2020](#bib.bib65)）。这两项研究使用了八种不同的堆叠回归模型的集成，其预测结果通过多层感知机结合。作者在训练数据中包括了响应的总结，这有助于学习到更强健的模型。
- en: 8.3\. Attention-based models
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 基于注意力的模型
- en: More recent methods for ASAG explored more sophisticated feature representations,
    computed with attention-based and transformer models, to capture better and more
    descriptive semantic and structural characteristics from text. Attention enables
    the calculation of the relation and relative importance between each word within
    a sentence. The attention mechanism allows to model the dependencies of words
    and their importance for the prediction task at a longer range in sentences. The
    modeling does not take into account the sequentiality of words explicitly.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期的ASAG方法探索了更复杂的特征表示，这些表示通过基于注意力和变换器模型计算，以更好地捕捉和描述文本的语义和结构特征。注意力机制使得计算句子中每个词之间的关系和相对重要性成为可能。注意力机制允许建模词语之间的依赖关系及其在句子中的重要性，且对预测任务的长期依赖关系进行建模。建模过程中没有明确考虑词语的顺序性。
- en: Architectures relying purely on attention mechanisms were introduced in (Vaswani
    et al., [2017](#bib.bib74)), and are called transformers. They consist of an encoder-decoder
    structure able to characterize long-range characteristics and dependencies in
    sequential data. In transformer architectures, the attention mechanism is modeled
    via multiple parallel attention-head components, each of them able to learn different
    dependencies.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 纯粹依赖注意力机制的架构在（Vaswani et al., [2017](#bib.bib74)）中被引入，称为变换器。它们由一个能够表征序列数据中长期特征和依赖关系的编码器-解码器结构组成。在变换器架构中，注意力机制通过多个并行的注意力头组件来建模，每个组件能够学习不同的依赖关系。
- en: Attention-based models differ in their general architecture. In (Riordan et al.,
    [2017](#bib.bib61)), LSTMs and Convolutional Neural Networks (CNNs) were combined
    with attention mechanisms, and they were demonstrated to outperform previous methods.
    In particular, bidirectional LSTMs augmented with attention obtained competitive
    results for ASAG problems (see Table LABEL:tab:DLresults). According to the authors,
    the choice of the input embeddings and the correct fine-tuning of the pre-trained
    models on the task at hand is crucial to obtain good results. Several further
    works focused on consistently applying transfer learning techniques, with the
    aim of fine-tuning the feature embedding space towards the domain of interest.
    For this purpose, more complex architectures, made of stacked or ensemble networks
    and attention modules were proposed (Camus and Filighera, [2020](#bib.bib5); Pribadi
    et al., [2017](#bib.bib55); Riordan et al., [2017](#bib.bib61)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的模型在其总体架构上有所不同。在（Riordan et al., [2017](#bib.bib61)）中，将LSTM和卷积神经网络（CNN）与注意力机制结合，证明了它们优于之前的方法。特别是，增强了注意力的双向LSTM在ASAG问题上获得了竞争性的结果（见表LABEL:tab:DLresults）。根据作者的说法，输入嵌入的选择和对预训练模型在特定任务上的正确微调对获得良好结果至关重要。若干进一步的工作集中于一致地应用迁移学习技术，旨在将特征嵌入空间微调至感兴趣的领域。为此，提出了更复杂的架构，由堆叠或集成的网络和注意力模块组成（Camus
    and Filighera, [2020](#bib.bib5); Pribadi et al., [2017](#bib.bib55); Riordan
    et al., [2017](#bib.bib61)）。
- en: Other methods were based on the use of transformer models (Vaswani et al., [2017](#bib.bib74)).
    Their success and high-performance results are attributable to the high parallelization
    of the computations, which allows to train models on larger data sets, and the
    ability of modeling long-range dependencies. Further research work thus focused
    on exploring the use of transformers to compute text embeddings, and their effects
    on NLP and ASAG tasks  (Camus and Filighera, [2020](#bib.bib5)). In (Sung et al.,
    [2019a](#bib.bib67)), the Bidirectional Encoder Representation from Transformers
    (BERT) model was fine-tuned on an ASAG task, achieving state-of-the-art performance.
    The authors observed that a BERT model can be fine-tuned using only a few labeled
    examples on a target task and achieve very good performance, although it suffers
    from a lack of cross-domain generalization.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法基于变换器模型（Vaswani et al., [2017](#bib.bib74)）。它们的成功和高性能结果归功于计算的高度并行化，这允许在更大的数据集上训练模型，以及建模长期依赖关系的能力。因此，进一步的研究工作集中于探索使用变换器计算文本嵌入的效果及其对自然语言处理（NLP）和ASAG任务的影响（Camus
    and Filighera, [2020](#bib.bib5)）。在（Sung et al., [2019a](#bib.bib67)）中，双向编码器表示的变换器（BERT）模型在ASAG任务上进行了微调，取得了最先进的性能。作者观察到，BERT模型可以使用仅少量标记示例在目标任务上进行微调并取得非常好的性能，尽管它存在跨领域泛化不足的问题。
- en: The sparse characteristics of data in benchmark data sets required the use of
    transfer learning and domain adaptation techniques also for fine-tuning transformer
    models and tailoring the learning process to the task at hand or its specific
    domain. In (Sung et al., [2019b](#bib.bib68)), the effect of different training
    approaches was investigated for ASAG. The authors showed that utilizing unstructured
    domain text data and question-answer pairs to fine-tune the models resulted in
    better results than using task-specific data only. The method proposed in (Sung
    et al., [2019b](#bib.bib68)) achieved higher results than all previous works on
    the SemEval 3-way task, although results on the SemEval 5-way task were not reported.
    In (Camus and Filighera, [2020](#bib.bib5)), the authors compared the performance
    of several state-of-the-art transformers fine-tuned using different learning strategies,
    such as cross-lingual and task-specific training, and noticed a consistent enhancement
    of the results on ASAG tasks.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集中数据的稀疏特性要求使用迁移学习和领域适应技术，以对变换器模型进行微调，并将学习过程定制到当前任务或其特定领域。在 (Sung et al.,
    [2019b](#bib.bib68))中，研究了不同训练方法对ASAG的影响。作者表明，利用非结构化领域文本数据和问答对来微调模型，比仅使用任务特定数据取得了更好的结果。 (Sung
    et al., [2019b](#bib.bib68))中提出的方法在SemEval 3-way任务上取得了高于所有先前工作的结果，尽管SemEval 5-way任务的结果未被报告。在 (Camus
    and Filighera, [2020](#bib.bib5))中，作者比较了使用不同学习策略（如跨语言和任务特定训练）微调的几种最先进变换器的性能，并注意到ASAG任务上的结果一致得到提升。
- en: <svg version="1.1" fill="none" height="366.818873668189" stroke="none" width="277.708592777086"
    overflow="visible"><g transform="translate(0,366.818873668189) scale(1,-1)"><g
    transform="translate(0,0)"><g transform="translate(0,281) scale(1, -1)"><foreignobject
    width="213" height="281" overflow="visible">![Refer to caption](img/806ed27881b731208902d5e76f3c78e1.png)</foreignobject></g></g><g
    transform="translate(83.33,280.56)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="9.4091600940916" height="7.7487200774872"
    overflow="visible">$h_{1}$</foreignobject></g></g><g transform="translate(138.89,280.56)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="15.220700152207"
    height="7.7487200774872" overflow="visible">$h_{n-1}$</foreignobject></g></g><g
    transform="translate(202.78,280.56)"><g transform="translate(0,9.1324200913242)
    scale(1, -1)"><foreignobject width="10.1010101010101" height="7.7487200774872"
    overflow="visible">$h_{n}$</foreignobject></g></g><g transform="translate(113.89,231.94)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="9.4091600940916"
    height="7.7487200774872" overflow="visible">$h_{2}$</foreignobject></g></g><g
    transform="translate(166.67,231.94)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="15.220700152207" height="7.7487200774872"
    overflow="visible">$h_{n-1}$</foreignobject></g></g><g transform="translate(231.94,231.94)"><g
    transform="translate(0,9.1324200913242) scale(1, -1)"><foreignobject width="10.1010101010101"
    height="7.7487200774872" overflow="visible">$h_{n}$</foreignobject></g></g><g
    transform="translate(83.33,186.11)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="9.4091600940916" height="7.7487200774872"
    overflow="visible">$h_{2}$</foreignobject></g></g><g transform="translate(138.89,186.11)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="15.220700152207"
    height="7.7487200774872" overflow="visible">$h_{n-1}$</foreignobject></g></g><g
    transform="translate(202.78,186.11)"><g transform="translate(0,9.1324200913242)
    scale(1, -1)"><foreignobject width="10.1010101010101" height="7.7487200774872"
    overflow="visible">$h_{n}$</foreignobject></g></g><g transform="translate(22.22,347.22)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">output</text></g><g
    transform="translate(27.78,336.11)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">layer</text></g><g transform="translate(148.06,312.5)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">+</text></g><g
    transform="translate(22.22,308.33)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">attention</text></g><g transform="translate(33.33,297.22)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">layer</text></g><g
    transform="translate(33.33,241.67)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">BiLSTM</text></g><g transform="translate(22.22,230.56)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">hidden layer</text></g><g
    transform="translate(33.33,141.67)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">sentence</text></g><g transform="translate(19.44,130.56)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">representation</text></g><g
    transform="translate(27.78,37.5)"><text x="0" y="0" transform="scale(1, -1)" fill="black"
    font-size="80%">input layer</text></g></g></svg>
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: <svg version="1.1" fill="none" height="366.818873668189" stroke="none" width="277.708592777086"
    overflow="visible"><g transform="translate(0,366.818873668189) scale(1,-1)"><g
    transform="translate(0,0)"><g transform="translate(0,281) scale(1, -1)"><foreignobject
    width="213" height="281" overflow="visible">![参考标题](img/806ed27881b731208902d5e76f3c78e1.png)</foreignobject></g></g><g
    transform="translate(83.33,280.56)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="9.4091600940916" height="7.7487200774872"
    overflow="visible">$h_{1}$</foreignobject></g></g><g transform="translate(138.89,280.56)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="15.220700152207"
    height="7.7487200774872" overflow="visible">$h_{n-1}$</foreignobject></g></g><g
    transform="translate(202.78,280.56)"><g transform="translate(0,9.1324200913242)
    scale(1, -1)"><foreignobject width="10.1010101010101" height="7.7487200774872"
    overflow="visible">$h_{n}$</foreignobject></g></g><g transform="translate(113.89,231.94)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="9.4091600940916"
    height="7.7487200774872" overflow="visible">$h_{2}$</foreignobject></g></g><g
    transform="translate(166.67,231.94)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="15.220700152207" height="7.7487200774872"
    overflow="visible">$h_{n-1}$</foreignobject></g></g><g transform="translate(231.94,231.94)"><g
    transform="translate(0,9.1324200913242) scale(1, -1)"><foreignobject width="10.1010101010101"
    height="7.7487200774872" overflow="visible">$h_{n}$</foreignobject></g></g><g
    transform="translate(83.33,186.11)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="9.4091600940916" height="7.7487200774872"
    overflow="visible">$h_{2}$</foreignobject></g></g><g transform="translate(138.89,186.11)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="15.220700152207"
    height="7.7487200774872" overflow="visible">$h_{n-1}$</foreignobject></g></g><g
    transform="translate(202.78,186.11)"><g transform="translate(0,9.1324200913242)
    scale(1, -1)"><foreignobject width="10.1010101010101" height="7.7487200774872"
    overflow="visible">$h_{n}$</foreignobject></g></g><g transform="translate(22.22,347.22)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">输出</text></g><g
    transform="translate(27.78,336.11)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">层</text></g><g transform="translate(148.06,312.5)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">+</text></g><g
    transform="translate(22.22,308.33)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">注意力</text></g><g transform="translate(33.33,297.22)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">层</text></g><g
    transform="translate(33.33,241.67)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">双向LSTM</text></g><g transform="translate(22.22,230.56)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">隐藏层</text></g><g
    transform="translate(33.33,141.67)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">句子</text></g><g transform="translate(19.44,130.56)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">表示</text></g><g
    transform="translate(27.78,37.5)"><text x="0" y="0" transform="scale(1, -1)" fill="black"
    font-size="80%">输入层</text></g></g></svg>
- en: Figure 3\. Sketch of the model architecture proposed in (Qi et al., [2019](#bib.bib56)).
    The authors proposed a combined use of a CNN and a BiLSTM, together with an attention
    layer, which provides a refinement of the prediction before the output layer is
    computed.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 在 (Qi et al., [2019](#bib.bib56)) 中提出的模型架构示意图。作者提出将CNN和BiLSTM与注意力层结合使用，这在输出层计算之前对预测进行了优化。
- en: 'Since attention is instrumental for the success of transformer models (Conneau
    et al., [2017](#bib.bib8); Peters et al., [2018](#bib.bib52); Radford, [2018](#bib.bib57);
    Howard and Ruder, [2018](#bib.bib27); Devlin et al., [2019](#bib.bib10); Yang
    et al., [2019](#bib.bib80)) and their application to ASAG, researchers focused
    on how it can be used optimally in a model. In (Qi et al., [2019](#bib.bib56)),
    for instance, an attention layer was combined with a BiLSTM and a CNN architecture
    to compute question-answer representations. The structure of the model architecture
    is illustrated in Figure [3](#S8.F3 "Figure 3 ‣ 8.3\. Attention-based models ‣
    8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers"). Such embeddings were concatenated
    and used for scoring the student answers. With this approach, the authors leveraged
    textual semantic features and long-term dependencies in text and obtained good
    performance results. Furthermore, in (Wang et al., [2019](#bib.bib75)), it was
    shown that attention can be used to extract key elements from the student answers.
    The authors demonstrated that the performance could be improved by considering
    a larger observation window on the text rather than using only word-level attention.
    Combinations of different attention mechanisms were explored in (Liu et al., [2019a](#bib.bib37)).
    As illustrated in Figure [4](#S8.F4 "Figure 4 ‣ 8.3\. Attention-based models ‣
    8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers"), the model calculated separate
    representations for the student answer, the reference answer, and their cross-attention.
    These different representations are then aggregated position-wise and used as
    input representations for a basic transformer model. Their approach achieved overall
    good results on the Real World K-12 data set (see Table LABEL:tab:DLresults).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于注意力机制对于变换器模型的成功至关重要（Conneau et al., [2017](#bib.bib8); Peters et al., [2018](#bib.bib52);
    Radford, [2018](#bib.bib57); Howard and Ruder, [2018](#bib.bib27); Devlin et al.,
    [2019](#bib.bib10); Yang et al., [2019](#bib.bib80)），以及其在ASAG中的应用，研究者们关注如何在模型中优化使用注意力机制。例如，在
    (Qi et al., [2019](#bib.bib56)) 中，注意力层与BiLSTM和CNN架构结合，用于计算问题-答案表示。模型架构的结构如图[3](#S8.F3
    "图 3 ‣ 8.3\. 基于注意力的模型 ‣ 8\. 深度学习方法 ‣ 自动短答案评分调查：从词嵌入到变换器") 所示。这些嵌入被连接并用于对学生答案进行评分。通过这种方法，作者利用了文本的语义特征和长期依赖性，并取得了良好的性能结果。此外，在
    (Wang et al., [2019](#bib.bib75)) 中，研究表明，注意力机制可以用来提取学生答案中的关键元素。作者展示了通过在文本上考虑更大的观察窗口，而不是仅使用词级注意力，性能可以得到改善。在
    (Liu et al., [2019a](#bib.bib37)) 中，探索了不同的注意力机制组合。如图[4](#S8.F4 "图 4 ‣ 8.3\. 基于注意力的模型
    ‣ 8\. 深度学习方法 ‣ 自动短答案评分调查：从词嵌入到变换器") 所示，模型为学生答案、参考答案及其交叉注意力计算了独立的表示。这些不同的表示然后按位置进行聚合，并用作基础变换器模型的输入表示。他们的方法在真实世界K-12数据集上取得了总体良好的结果（见表LABEL:tab:DLresults）。
- en: '![Refer to caption](img/897ec615f2a0296e6949a305dce137b4.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/897ec615f2a0296e6949a305dce137b4.png)'
- en: Figure 4\. Sketch of the model architecture proposed in (Liu et al., [2019a](#bib.bib37)).
    The authors combined different representations of the student and reference answers.
    Subsequently, these representations are aggregated at the prediction stage.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 在 (Liu et al., [2019a](#bib.bib37)) 中提出的模型架构示意图。作者结合了学生答案和参考答案的不同表示。随后，这些表示在预测阶段被聚合。
- en: The use of attention-based models contributed to a substantial increase in performance
    on ASAG tasks, which is attributable to the more powerful representations learned
    using these models. The analysis and comparison of longer text data, however,
    remains a challenging aspect of ASAG systems. We discuss the challenges and outlook
    of further developments in this field in the following section.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于注意力的模型显著提高了ASAG任务的性能，这归因于这些模型学习到的更强大的表示。然而，对更长文本数据的分析和比较仍然是ASAG系统中的一个挑战。我们将在下一节讨论该领域进一步发展的挑战和前景。
- en: 9\. Discussion
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 讨论
- en: The field of ASAG has witnessed a transition from methodologies based on careful
    design of hand-engineered text features towards feature learning architectures
    based on deep learning. This evolution has been strongly influenced by the progress
    made in the field of NLP. Transfer learning from very big models trained on generic
    NLP tasks to target tasks has become a common practice. This, however, has benefits
    but also creates challenges in the field of ASAG.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ASAG领域经历了从基于精心设计的手工文本特征的方法到基于深度学习的特征学习架构的转变。这一演变受到NLP领域进展的强烈影响。从在通用NLP任务上训练的非常大的模型中进行迁移学习已成为一种常见的做法。然而，这带来了好处，同时也在ASAG领域创造了挑战。
- en: Word embeddings, sequential models, and attention-based and transformers models
    have progressively contributed to improving the semantic richness and descriptive
    power of textual representation. This resulted in impressive progress in the field
    of NLP (Sahu and Bhowmick, [2020](#bib.bib65)), mainly due to the ability to exploit
    the relations and the diversity of structures of vast amounts of data. The performance
    of ASAG methods has, however, not witnessed the same level of improvement. This
    is mainly attributable to the fact that the ASAG benchmark datasets are sparse,
    and very complex models developed for general NLP tasks do not generalize well
    to this application field. Although transfer-learning approaches are shown to
    be promising (Sung et al., [2019a](#bib.bib67)), fundamental aspects of ASAG like
    sparsity and domain differences need to be addressed explicitly.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入、序列模型以及基于注意力和transformer的模型逐渐有助于提高文本表示的语义丰富性和描述能力。这在自然语言处理领域取得了令人印象深刻的进展（Sahu和Bhowmick，[2020](#bib.bib65)），主要得益于能够利用大量数据的关系和结构的多样性。然而，ASAG方法的表现并未见同样水平的提升。这主要归因于ASAG基准数据集的稀疏性，以及为一般NLP任务开发的非常复杂的模型在这一应用领域的泛化能力不足。尽管迁移学习方法显示出有希望的前景（Sung等，[2019a](#bib.bib67)），但ASAG的基本方面如稀疏性和领域差异需要明确解决。
- en: 'Transformers and attention-based models have achieved astonishing results in
    NLP (Liu et al., [2019c](#bib.bib39)). For ASAG tasks, however, a fine-tuned transformer
    model alone does not achieve the highest performance on benchmark datasets. We
    hypothesize that these complex embeddings models are not able to effectively disentangle
    the semantically rich information contained in short answers. The best-performing
    methods, indeed, combine complex embeddings representation computed by transformers
    with sets of hand-crafted features that are specifically designed to address particular
    aspects and characteristics of short answers. Currently, the research community
    of NLP and ASAG is investigating ways to optimize the embeddings of paragraphs
    of medium size which best represent the semantic information contained in the
    student answers. Particular attention is given to: a) combination of different
    features to exploit their ability to b) using multi-classifier systems (e.g. stacked
    architectures, ensembles, and hybrid models) to exploit eventual complementary
    modeling and predictive capabilities, and c) optimizing the training process to
    influence what semantic information the model focuses more on (e.g. different
    domain adaptation strategies, general or task-focused pre-training).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer和基于注意力的模型在自然语言处理（NLP）中取得了惊人的成果（Liu等，[2019c](#bib.bib39)）。然而，对于ASAG任务，单独调整过的transformer模型在基准数据集上的表现并没有达到最佳水平。我们假设这些复杂的嵌入模型无法有效地解开短答案中包含的语义丰富的信息。表现最佳的方法确实将由transformers计算的复杂嵌入表示与专门设计来处理短答案特定方面和特征的一组手工特征相结合。目前，NLP和ASAG的研究社区正在探讨如何优化中等大小段落的嵌入，以最佳地代表学生答案中包含的语义信息。特别关注：a）不同特征的组合以利用它们的能力，b）使用多分类器系统（例如堆叠架构、集成和混合模型）以利用潜在的互补建模和预测能力，以及c）优化训练过程以影响模型更关注哪些语义信息（例如，不同的领域适应策略、一般或任务导向的预训练）。
- en: 9.1\. Open challenges
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1. 开放挑战
- en: We have recognized a number of challenges that ASAG methods have to deal with,
    particularly because of the peculiar characteristics of the task at hand. In the
    following paragraphs, we discuss them.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经认识到ASAG方法必须面对的许多挑战，特别是由于当前任务的特殊性。在接下来的段落中，我们将对此进行讨论。
- en: Semantic understanding.
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义理解。
- en: Existing methods have difficulties to model the rich semantic content of short
    answers effectively. This is due to the fact that, usually, short answers are
    written in such a way that a lot of information is provided in few sentences and
    in a very concise way. Therefore, ASAG systems require methods for better and
    deeper contextual and semantic understanding of the text, for which the application
    of Natural Language Understanding (NLU) techniques (Navigli, [2018](#bib.bib47))
    should be investigated.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现有方法难以有效建模短答案的丰富语义内容。这是因为通常短答案是以一种在少数句子中提供大量信息的非常简洁的方式编写的。因此，ASAG系统需要更好和更深入的文本上下文和语义理解的方法，应该研究自然语言理解（NLU）技术（Navigli,
    [2018](#bib.bib47)）的应用。
- en: Linguistic variations.
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言变异。
- en: The analysis of student answers presents several challenges related to the way
    sentences are formulated. A concept or intention can be expressed in different
    ways, such as with different words or with the construction of sentences (Roy
    et al., [2016](#bib.bib62)). Furthermore, answers may lack a complete sentential
    form or be deliberately written to trick the automatic evaluation systems. The
    grading of non-sentential answers is challenging because they might not be in
    conformance with the structural pattern of the reference answer or grammatically
    incorrect (Saha et al., [2018](#bib.bib64)). Grading systems need to be able to
    take into account these cases and interpret or infer complete answers from fragments.
    In addition, most of the existing methods are not able to distinguish between
    nonsense and relevant answers. Due to the intended automated nature of the system,
    it is crucial to detect if a student is trying to trick the system by answering
    the question with consecutive incoherent keywords, instead of formulating a complete
    answer.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 学生回答的分析面临几个与句子构造相关的挑战。一个概念或意图可以通过不同的方式表达，例如使用不同的词语或句子结构（Roy et al., [2016](#bib.bib62)）。此外，答案可能缺乏完整的句子形式，或者故意编写以迷惑自动评估系统。对非句子回答的评分具有挑战性，因为这些回答可能不符合参考答案的结构模式或语法上不正确（Saha
    et al., [2018](#bib.bib64)）。评分系统需要能够考虑这些情况，并从片段中解释或推断完整的答案。此外，大多数现有方法无法区分无意义和相关的答案。由于系统的自动化特性，关键是检测学生是否试图通过连续不连贯的关键词来欺骗系统，而不是提供完整的答案。
- en: Details of questions and reference answers.
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题和参考答案的细节。
- en: Training ASAG models rely on the correctness and reliability of the reference
    answers and labels. Some reference answers used to train the models might be too
    brief, not containing enough details, or even not completely meaningful. This
    creates issues for automated grading algorithms, which have to be trained to be
    robust to data uncertainty. Likewise, ASAG models should also be able to handle
    different question types and answer expectations. For instance, some questions
    may require providing a definition whereas other questions expect a more complex
    and detailed answer (Roy et al., [2016](#bib.bib62)). Especially open-ended questions
    are challenging since they expect the students to express their own thoughts based
    on facts. Hence, a reference answer might not be suitable because correct answers
    may vary a lot (Zhang et al., [2019](#bib.bib83)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 训练ASAG模型依赖于参考答案和标签的正确性和可靠性。一些用于训练模型的参考答案可能过于简略，不包含足够的细节，甚至不完全有意义。这对自动评分算法构成问题，因为这些算法必须经过训练以应对数据不确定性。同样，ASAG模型还应该能够处理不同的问题类型和答案期望。例如，一些问题可能要求提供定义，而其他问题则期望更复杂和详细的答案（Roy
    et al., [2016](#bib.bib62)）。特别是开放性问题具有挑战性，因为它们要求学生基于事实表达自己的想法。因此，参考答案可能不合适，因为正确答案可能差异很大（Zhang
    et al., [2019](#bib.bib83)）。
- en: Generalization across domains and answers.
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨领域和答案的概括。
- en: ASAG systems are required to maintain consistent levels of performance when
    evaluating the answers to questions from different domains (Saha et al., [2018](#bib.bib64)).
    This is not a straightforward task to be evaluated, as existing data sets are
    biased towards certain domains, or reference answers with higher scores (Wang
    et al., [2018](#bib.bib76)). The sparsity of available benchmark data sets thus
    poses challenges to effectively training models with robust generalization abilities.
    Furthermore, for a given question, multiple reference answers can be correct,
    meaning that there is no absolute gold standard to compare the student answers
    with (Roy et al., [2016](#bib.bib62)). Training models for ASAG needs to take
    into account the sparse characteristics of the available data and possible noisy
    labels, in order to avoid overfitting.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ASAG 系统在评估来自不同领域的问题答案时需要保持一致的性能水平（Saha 等，[2018](#bib.bib64)）。这不是一个简单的任务，因为现有的数据集对某些领域有偏向，或参考答案的分数较高（Wang
    等，[2018](#bib.bib76)）。因此，可用基准数据集的稀疏性对有效训练具有强泛化能力的模型提出了挑战。此外，对于给定的问题，多个参考答案都可以是正确的，这意味着没有绝对的黄金标准来与学生答案进行比较（Roy
    等，[2016](#bib.bib62)）。为 ASAG 训练模型需要考虑可用数据的稀疏特性和可能的噪声标签，以避免过拟合。
- en: 9.2\. Outlook and future work
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2\. 展望与未来工作
- en: Deep learning approaches have been beneficial for the improvement of the performance
    of the ASAG system and have been demonstrated to effectively complement the text
    representation capabilities of methods based on hand-engineered features. To further
    improve the possibilities and extent of application of deep learning methods for
    grading of short answers, necessary steps have to be taken that address the specific
    problems and challenges presented above.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法对 ASAG 系统性能的提升有很大帮助，并且已被证明能够有效地补充基于手工设计特征的方法的文本表示能力。为了进一步改善深度学习方法在短答案评分中的应用可能性和范围，需要采取必要的步骤来解决上述具体问题和挑战。
- en: The available benchmark data sets are rather sparse and are not representative
    enough of the variability of questions and short reference answers in different
    domains. This hinders the generalization capabilities of learning-based methods,
    which are thus subject to overfitting. This makes tasks like the extension of
    existing data sets, data augmentation (Wei and Zou, [2019](#bib.bib79)), and the
    creation of synthetic data via generative models (Tevet et al., [2019](#bib.bib71))
    very relevant to promote future progress in the field.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的基准数据集相当稀疏，且未能充分代表不同领域中问题和简短参考答案的变异性。这限制了基于学习的方法的泛化能力，从而导致过拟合。因此，像扩展现有数据集、数据增强（Wei
    和 Zou，[2019](#bib.bib79)）、以及通过生成模型创建合成数据（Tevet 等，[2019](#bib.bib71)）这样的任务，对于推动该领域的未来进展非常重要。
- en: The explainability (Guidotti et al., [2018](#bib.bib23)) and robustness (Wang
    et al., [2021](#bib.bib78)) of deep learning-based models are also important aspects
    that require further research. These can be indeed instrumental characteristics
    to assess whether a model is capable of robust decision making and can be applied
    successfully to grade answers in different topic domains. Furthermore, being able
    to consistently explain the decisions taken by a certain model also supports the
    analysis of what words or semantic constructs the score is determined by. Performance
    analysis and model inspection are thus complementary aspects that should be jointly
    taken into account for the evaluation of the developed methods, instead of only
    focusing on improving benchmark results. Promising research areas concern the
    identification of the points of strength of existing methods, to determine sets
    of methods that are suitable to be used for specific tasks and subsequently design
    and test ensemble methods for multi-domain cases. However, in order to achieve
    this goal, techniques to explain the model predictions should be put in place.
    These aspects are strictly linked with the need of characterizing the robustness
    of the prediction made by the trained models to variations of the test answers (Wang
    et al., [2020](#bib.bib77)). A research objective that needs to be taken into
    account is to study how sensitive ASAG models are to changes in the answers of
    students, in order to avoid that they can be tricked by using certain keywords
    or by swapping the order of words. This has to be coupled with a more extensive
    use of NLU techniques, to better evaluate the content and the semantics of the
    given answers (Namazifar et al., [2021](#bib.bib46)). This approach can also be
    used to identify weaknesses of the models and take appropriate countermeasures.
    This will be also beneficial to examine and control the relevance of spelling
    errors or meaningless sentences made of specific keywords to the computation of
    the answer score.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的可解释性（Guidotti et al., [2018](#bib.bib23)）和鲁棒性（Wang et al., [2021](#bib.bib78)）也是需要进一步研究的重要方面。这些确实是评估模型是否能够进行稳健决策并成功应用于不同主题领域评分的重要特征。此外，能够一致地解释某个模型所做的决策，还支持分析分数由哪些词汇或语义结构决定。因此，性能分析和模型检查是相辅相成的方面，应共同考虑，以评估开发的方法，而不是仅仅关注改进基准结果。前景广阔的研究领域包括识别现有方法的优势，确定适用于特定任务的方法集，并随后设计和测试多领域案例的集成方法。然而，为了实现这一目标，应采取技术来解释模型预测。这些方面与需要表征训练模型对测试答案变化的预测鲁棒性紧密相关（Wang
    et al., [2020](#bib.bib77)）。一个需要考虑的研究目标是研究ASAG模型对学生答案变化的敏感性，以避免它们被某些关键词或词序交换所欺骗。这必须结合更广泛使用自然语言理解（NLU）技术，以更好地评估给定答案的内容和语义（Namazifar
    et al., [2021](#bib.bib46)）。这种方法也可以用于识别模型的弱点并采取适当的对策。这也有助于检查和控制拼写错误或由特定关键词组成的无意义句子对答案评分计算的相关性。
- en: 10\. Conclusions
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10\. 结论
- en: We reviewed the recent progress in the field of Automated Short Answer Grading
    (ASAG) and provided an overview of the advancements made and results achieved
    using deep learning techniques. We added to previous literature analyses by identifying
    the key features and architectural choices that impacted the performance of ASAG
    systems in the era of Deep Learning. We linked the methodological improvements
    to the results that recent methods achieved on benchmark data sets.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了自动短答评分（ASAG）领域的近期进展，并概述了使用深度学习技术取得的进展和结果。我们通过识别关键特征和架构选择，丰富了先前的文献分析，这些特征和选择影响了ASAG系统在深度学习时代的表现。我们将方法改进与近期方法在基准数据集上取得的结果进行了关联。
- en: This survey provides a taxonomy of methods, from classical Machine Learning
    to Deep Learning approaches, and research trends and outlook. Deep Learning architectures
    for natural language processing, adapted to ASAG tasks by means of transfer learning
    and domain adaptation techniques, are not sufficient to deal with the challenges
    and requirements that this field presents. Deep learning approaches alone show
    difficulty in effectively catch the semantics of short answers for consistent
    comparison with reference answers. To compensate for this, an ensemble of classifiers,
    stacked models, and especially hybrid models that combine feature engineering
    with deep representation learning were developed. The embedding capabilities of
    Deep Learning models and the attention-based analysis of Transformers have been
    shown to be complementary to previously developed lexical, syntactic, and semantic
    features, to strengthen the performance of ASAG systems. However, to stimulate
    further developments, a uniform basis for benchmarking methods is necessary, e.g.
    designing a comprehensive benchmark data set.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查提供了从经典机器学习到深度学习方法的分类、研究趋势和展望。尽管深度学习架构通过迁移学习和领域适应技术被调整用于ASAG任务，但仍不足以应对该领域的挑战和要求。仅靠深度学习方法难以有效捕捉短答案的语义，以便与参考答案进行一致比较。为了弥补这一点，开发了分类器集成、堆叠模型，尤其是结合特征工程和深度表示学习的混合模型。深度学习模型的嵌入能力和基于注意力的Transformer分析被证明与先前开发的词汇、句法和语义特征互补，从而增强ASAG系统的性能。然而，为了促进进一步的发展，必要建立一个统一的基准方法，例如设计一个全面的基准数据集。
- en: References
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Blessing et al. (2021) Guembe Blessing, Ambrose Azeta, Sanjay Misra, Felix
    Chigozie, and Ravin Ahuja. 2021. A Machine Learning Prediction of Automatic Text
    Based Assessment for Open and Distance Learning: A Review. In *Innovations in
    Bio-Inspired Computing and Applications*. Springer International Publishing, Cham,
    369–380.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blessing et al. (2021) Guembe Blessing, Ambrose Azeta, Sanjay Misra, Felix Chigozie,
    和 Ravin Ahuja. 2021. 基于机器学习的自动文本评估预测用于开放和远程学习：综述。载于 *生物启发计算及应用创新*。Springer International
    Publishing, Cham, 369–380。
- en: Burrows et al. (2015) Steven Burrows, Iryna Gurevych, and Benno Stein. 2015.
    The Eras and Trends of Automatic Short Answer Grading. *International Journal
    of Artificial Intelligence in Education* 25, 1 (01 Mar 2015), 60–117. [https://doi.org/10.1007/s40593-014-0026-8](https://doi.org/10.1007/s40593-014-0026-8)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burrows et al. (2015) Steven Burrows, Iryna Gurevych, 和 Benno Stein. 2015. 自动短答案评分的时代与趋势。*国际人工智能教育杂志*
    25, 1 (2015年3月1日), 60–117。 [https://doi.org/10.1007/s40593-014-0026-8](https://doi.org/10.1007/s40593-014-0026-8)
- en: Cai (2019) Changzhi Cai. 2019. Automatic Essay Scoring with Recurrent Neural
    Network. In *Proceedings of the 3rd International Conference on High Performance
    Compilation, Computing and Communications*. Association for Computing Machinery,
    New York, NY, USA, 1–7. [https://doi.org/10.1145/3318265.3318296](https://doi.org/10.1145/3318265.3318296)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai (2019) Changzhi Cai. 2019. 使用递归神经网络的自动作文评分。载于 *第3届国际高性能编译、计算与通信会议论文集*。Association
    for Computing Machinery, New York, NY, USA, 1–7。 [https://doi.org/10.1145/3318265.3318296](https://doi.org/10.1145/3318265.3318296)
- en: Camus and Filighera (2020) Leon Camus and Anna Filighera. 2020. Investigating
    Transformers for Automatic Short Answer Grading. In *Artificial Intelligence in
    Education*, Ig Ibert Bittencourt, Mutlu Cukurova, Kasia Muldner, Rose Luckin,
    and Eva Millán (Eds.). Springer International Publishing, Cham, 43–48.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Camus 和 Filighera (2020) Leon Camus 和 Anna Filighera. 2020. 调查Transformer在自动短答案评分中的应用。载于
    *人工智能教育*，Ig Ibert Bittencourt, Mutlu Cukurova, Kasia Muldner, Rose Luckin, 和 Eva
    Millán (编)。Springer International Publishing, Cham, 43–48。
- en: Cheng et al. (2016) Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long
    Short-Term Memory-Networks for Machine Reading. In *Proceedings of the 2016 Conference
    on Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics, Austin, Texas, 551–561. [https://doi.org/10.18653/v1/D16-1053](https://doi.org/10.18653/v1/D16-1053)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2016) Jianpeng Cheng, Li Dong, 和 Mirella Lapata. 2016. 用于机器阅读的长短期记忆网络。载于
    *2016年自然语言处理经验方法会议论文集*。Association for Computational Linguistics, Austin, Texas,
    551–561。 [https://doi.org/10.18653/v1/D16-1053](https://doi.org/10.18653/v1/D16-1053)
- en: Condor (2020) Aubrey Condor. 2020. Exploring Automatic Short Answer Grading
    as a Tool to Assist in Human Rating. *Artificial Intelligence in Education* 12164
    (2020), 74 – 79.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Condor (2020) Aubrey Condor. 2020. 探索自动短答案评分作为辅助人工评分的工具。*人工智能教育* 12164 (2020),
    74 – 79。
- en: Conneau et al. (2017) Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault,
    and Antoine Bordes. 2017. Supervised Learning of Universal Sentence Representations
    from Natural Language Inference Data. In *Proceedings of the 2017 Conference on
    Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics, Copenhagen, Denmark, 670–680. [https://doi.org/10.18653/v1/D17-1070](https://doi.org/10.18653/v1/D17-1070)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau et al. (2017) Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault,
    和 Antoine Bordes. 2017. 《从自然语言推理数据中监督学习通用句子表示》。发表于 *2017年自然语言处理实证方法会议论文集*。计算语言学协会，丹麦哥本哈根，670–680。
    [https://doi.org/10.18653/v1/D17-1070](https://doi.org/10.18653/v1/D17-1070)
- en: De Mulder et al. (2015) Wim De Mulder, Steven Bethard, and Marie-Francine Moens.
    2015. A survey on the application of recurrent neural networks to statistical
    language modeling. *Computer Speech & Language* 30, 1 (2015), 61–98. [https://doi.org/10.1016/j.csl.2014.09.005](https://doi.org/10.1016/j.csl.2014.09.005)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Mulder et al. (2015) Wim De Mulder, Steven Bethard, 和 Marie-Francine Moens.
    2015. 《递归神经网络在统计语言建模中的应用调查》。*计算机语音与语言* 30, 1 (2015), 61–98。 [https://doi.org/10.1016/j.csl.2014.09.005](https://doi.org/10.1016/j.csl.2014.09.005)
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association
    for Computational Linguistics, 4171–4186. [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. 《BERT：深度双向变换器的语言理解预训练》。发表于 *2019年北美计算语言学协会：人类语言技术会议，NAACL-HLT 2019，2019年6月2-7日，明尼阿波利斯，MN，美国，第
    1 卷（长文和短文）*，Jill Burstein, Christy Doran, 和 Thamar Solorio (编辑)。计算语言学协会，4171–4186。
    [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)
- en: Dhamecha et al. (2018) Tejas I. Dhamecha, Smit Marvaniya, Swarnadeep Saha, Renuka
    Sindhgatta, and Bikram Sengupta. 2018. Balancing Human Efforts and Performance
    of Student Response Analyzer in Dialog-Based Tutors. In *Artificial Intelligence
    in Education*, Carolyn Penstein Rosé, Roberto Martínez-Maldonado, H. Ulrich Hoppe,
    Rose Luckin, Manolis Mavrikis, Kaska Porayska-Pomsta, Bruce McLaren, and Benedict
    du Boulay (Eds.). Springer International Publishing, Cham, 70–85.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhamecha et al. (2018) Tejas I. Dhamecha, Smit Marvaniya, Swarnadeep Saha, Renuka
    Sindhgatta, 和 Bikram Sengupta. 2018. 《在对话式辅导中平衡人力投入和学生回应分析器的性能》。发表于 *教育中的人工智能*，Carolyn
    Penstein Rosé, Roberto Martínez-Maldonado, H. Ulrich Hoppe, Rose Luckin, Manolis
    Mavrikis, Kaska Porayska-Pomsta, Bruce McLaren, 和 Benedict du Boulay (编辑)。Springer
    International Publishing，Cham，70–85。
- en: 'Dyer et al. (2016) Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A.
    Smith. 2016. Recurrent Neural Network Grammars. In *Proceedings of the 2016 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*. Association for Computational Linguistics, San Diego,
    California, 199–209. [https://doi.org/10.18653/v1/N16-1024](https://doi.org/10.18653/v1/N16-1024)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dyer et al. (2016) Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, 和 Noah
    A. Smith. 2016. 《递归神经网络语法》。发表于 *2016年北美计算语言学协会：人类语言技术会议论文集*。计算语言学协会，加利福尼亚州圣地亚哥，199–209。
    [https://doi.org/10.18653/v1/N16-1024](https://doi.org/10.18653/v1/N16-1024)
- en: 'Dzikovska et al. (2013) Myroslava Dzikovska, Rodney Nielsen, Chris Brew, Claudia
    Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang
    Dang. 2013. SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing
    Textual Entailment Challenge. In *Second Joint Conference on Lexical and Computational
    Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop
    on Semantic Evaluation (SemEval 2013)*. Association for Computational Linguistics,
    Atlanta, Georgia, USA, 263–274. [https://aclanthology.org/S13-2045](https://aclanthology.org/S13-2045)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dzikovska et al. (2013) Myroslava Dzikovska, Rodney Nielsen, Chris Brew, Claudia
    Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan, 和 Hoa Trang
    Dang. 2013. 《SemEval-2013 任务 7：联合学生回应分析与第八届文本蕴涵识别挑战》。发表于 *第二届词汇和计算语义学联合会议 (*SEM)，第
    2 卷：第七届语义评估国际研讨会 (SemEval 2013) 会议论文集*。计算语言学协会，美国乔治亚州亚特兰大，263–274。 [https://aclanthology.org/S13-2045](https://aclanthology.org/S13-2045)
- en: 'Dzikovska et al. (2012) Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
    Brew. 2012. Towards Effective Tutorial Feedback for Explanation Questions: A Dataset
    and Baselines. In *Proceedings of the 2012 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*.
    Association for Computational Linguistics, Montréal, Canada, 200–210. [https://aclanthology.org/N12-1021](https://aclanthology.org/N12-1021)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dzikovska 等（2012）Myroslava O. Dzikovska、Rodney D. Nielsen 和 Chris Brew。2012。面向解释性问题的有效教程反馈：数据集和基准。在
    *2012年北美计算语言学协会：人类语言技术会议论文集* 中。计算语言学协会，加拿大蒙特利尔，200–210。 [https://aclanthology.org/N12-1021](https://aclanthology.org/N12-1021)
- en: 'Fellbaum (2000) C. Fellbaum. 2000. WordNet : an electronic lexical database.
    *Language* 76 (2000), 706.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fellbaum（2000）C. Fellbaum。2000。WordNet：一个电子词汇数据库。*Language* 76（2000），706。
- en: Gabrilovich and Markovitch (2007a) Evgeniy Gabrilovich and Shaul Markovitch.
    2007a. Computing Semantic Relatedness Using Wikipedia-Based Explicit Semantic
    Analysis. In *Proceedings of the 20th International Joint Conference on Artifical
    Intelligence* *(IJCAI’07)*. Morgan Kaufmann Publishers Inc., San Francisco, CA,
    USA, 1606–1611.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gabrilovich 和 Markovitch（2007a）Evgeniy Gabrilovich 和 Shaul Markovitch。2007a。使用基于维基百科的显式语义分析计算语义相关性。在
    *第20届国际人工智能联合会议* *(IJCAI’07)* 中。Morgan Kaufmann Publishers Inc.，美国旧金山，1606–1611。
- en: Gabrilovich and Markovitch (2007b) Evgeniy Gabrilovich and Shaul Markovitch.
    2007b. Computing Semantic Relatedness Using Wikipedia-Based Explicit Semantic
    Analysis. In *Proceedings of the 20th International Joint Conference on Artifical
    Intelligence* *(IJCAI’07)*. Morgan Kaufmann Publishers Inc., San Francisco, CA,
    USA, 1606–1611.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gabrilovich 和 Markovitch（2007b）Evgeniy Gabrilovich 和 Shaul Markovitch。2007b。使用基于维基百科的显式语义分析计算语义相关性。在
    *第20届国际人工智能联合会议* *(IJCAI’07)* 中。Morgan Kaufmann Publishers Inc.，美国旧金山，1606–1611。
- en: Gaddipati et al. (2020) Sasi Kiran Gaddipati, Deebul Nair, and P. Plöger. 2020.
    Comparative Evaluation of Pretrained Transfer Learning Models on Automatic Short
    Answer Grading. *ArXiv* abs/2009.01303 (2020).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaddipati 等（2020）Sasi Kiran Gaddipati、Deebul Nair 和 P. Plöger。2020。对预训练迁移学习模型在自动短答案评分中的比较评估。*ArXiv*
    abs/2009.01303（2020）。
- en: Galassi et al. (2020) Andrea Galassi, Marco Lippi, and Paolo Torroni. 2020.
    Attention in Natural Language Processing. *IEEE Transactions on Neural Networks
    and Learning Systems* (2020), 1–18. [https://doi.org/10.1109/TNNLS.2020.3019893](https://doi.org/10.1109/TNNLS.2020.3019893)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galassi 等（2020）Andrea Galassi、Marco Lippi 和 Paolo Torroni。2020。自然语言处理中的注意力。*IEEE
    Transactions on Neural Networks and Learning Systems*（2020），1–18。 [https://doi.org/10.1109/TNNLS.2020.3019893](https://doi.org/10.1109/TNNLS.2020.3019893)
- en: 'Galhardi and Brancher (2018) Lucas Busatta Galhardi and Jacques Duílio Brancher.
    2018. Machine Learning Approach for Automatic Short Answer Grading: A Systematic
    Review. In *Advances in Artificial Intelligence - IBERAMIA 2018*, Guillermo R.
    Simari, Eduardo Fermé, Flabio Gutiérrez Segura, and José Antonio Rodríguez Melquiades
    (Eds.). Springer International Publishing, Cham, 380–391.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galhardi 和 Brancher（2018）Lucas Busatta Galhardi 和 Jacques Duílio Brancher。2018。用于自动短答案评分的机器学习方法：系统评估。在
    *人工智能进展 - IBERAMIA 2018* 中，Guillermo R. Simari、Eduardo Fermé、Flabio Gutiérrez
    Segura 和 José Antonio Rodríguez Melquiades（编辑）。Springer International Publishing，Cham，380–391。
- en: Galhardi et al. (2018) Lucas B. Galhardi, Helen Senefonte, Rodrigo de Souza,
    and Jacques Brancher. 2018. Exploring Distinct Features for Automatic Short Answer
    Grading. In *Anais do XV Encontro Nacional de Inteligência Artificial e Computacional*.
    SBC, Porto Alegre, RS, Brasil, 1–12. [https://doi.org/10.5753/eniac.2018.4399](https://doi.org/10.5753/eniac.2018.4399)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galhardi 等（2018）Lucas B. Galhardi、Helen Senefonte、Rodrigo de Souza 和 Jacques
    Brancher。2018。探索用于自动短答案评分的不同特征。在 *第十五届国家人工智能与计算会议论文集* 中。SBC，巴西阿雷格里港，1–12。 [https://doi.org/10.5753/eniac.2018.4399](https://doi.org/10.5753/eniac.2018.4399)
- en: 'Gomaa and Fahmy (2020) Wael Hassan Gomaa and Aly Aly Fahmy. 2020. Ans2vec:
    A Scoring System for Short Answers. In *The International Conference on Advanced
    Machine Learning Technologies and Applications (AMLTA2019)*, Aboul Ella Hassanien,
    Ahmad Taher Azar, Tarek Gaber, Roheet Bhatnagar, and Mohamed F. Tolba (Eds.).
    Springer International Publishing, Cham, 586–595.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gomaa 和 Fahmy（2020）Wael Hassan Gomaa 和 Aly Aly Fahmy。2020。Ans2vec：一个短答案评分系统。在
    *高级机器学习技术与应用国际会议（AMLTA2019）* 中，Aboul Ella Hassanien、Ahmad Taher Azar、Tarek Gaber、Roheet
    Bhatnagar 和 Mohamed F. Tolba（编辑）。Springer International Publishing，Cham，586–595。
- en: Guidotti et al. (2018) Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri,
    Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A Survey of Methods
    for Explaining Black Box Models. *ACM Comput. Surv.* 51, 5, Article 93 (2018),
    42 pages. [https://doi.org/10.1145/3236009](https://doi.org/10.1145/3236009)
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guidotti et al. (2018) Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri,
    Franco Turini, Fosca Giannotti 和 Dino Pedreschi. 2018. 黑匣子模型解释方法概述。 *ACM计算机调查*
    51, 5, 文章 93 (2018)，42 页。 [https://doi.org/10.1145/3236009](https://doi.org/10.1145/3236009)
- en: 'Heilman and Madnani (2012) Michael Heilman and Nitin Madnani. 2012. ETS: Discriminative
    Edit Models for Paraphrase Scoring. In **SEM 2012: The First Joint Conference
    on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference
    and the shared task, and Volume 2: Proceedings of the Sixth International Workshop
    on Semantic Evaluation (SemEval 2012)*. Association for Computational Linguistics,
    Montréal, Canada, 529–535. [https://aclanthology.org/S12-1076](https://aclanthology.org/S12-1076)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heilman and Madnani (2012) Michael Heilman 和 Nitin Madnani. 2012. 短语重述评分的ETS：歧视性编辑模型。
    在**SEM 2012:第一届词汇与计算语义联合会议-第1卷：主会议和共享任务的论文集，以及第2卷：第六届国际语义评估研讨会论文集（SemEval 2012）*。
    Association for Computational Linguistics，Montréal，Canada，529–535。 [https://aclanthology.org/S12-1076](https://aclanthology.org/S12-1076)
- en: 'Heilman and Madnani (2013) Michael Heilman and Nitin Madnani. 2013. ETS: Domain
    Adaptation and Stacking for Short Answer Scoring. In *Second Joint Conference
    on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh
    International Workshop on Semantic Evaluation (SemEval 2013)*. Association for
    Computational Linguistics, Atlanta, Georgia, USA, 275–279. [https://aclanthology.org/S13-2046](https://aclanthology.org/S13-2046)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heilman and Madnani (2013) Michael Heilman 和 Nitin Madnani. 2013. ETS：领域自适应和堆叠用于短答案评分。
    在*第2届词汇与计算语义联合会议（*SEM）第2卷：第七届国际语义评估研讨会论文集（SemEval 2013）*。 Association for Computational
    Linguistics，Atlanta，Georgia，USA，275–279。 [https://aclanthology.org/S13-2046](https://aclanthology.org/S13-2046)
- en: Hightower (2020) Mallory Hightower. 2020. High-Level History of NLP Models.
    [https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7](https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7)
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hightower (2020) Mallory Hightower. 2020. NLP模型的高层历史。[https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7](https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7)
- en: 'Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. Universal
    Language Model Fine-tuning for Text Classification. In *Proceedings of the 56th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*. Association for Computational Linguistics, Melbourne, Australia, 328–339.
    [https://doi.org/10.18653/v1/P18-1031](https://doi.org/10.18653/v1/P18-1031)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard and Ruder (2018) Jeremy Howard 和 Sebastian Ruder. 2018. 通用语言模型微调用于文本分类。
    在*第56届ACM年会会议论文集（第1卷：长篇论文）*。 Association for Computational Linguistics，Melbourne，Australia，328–339。
    [https://doi.org/10.18653/v1/P18-1031](https://doi.org/10.18653/v1/P18-1031)
- en: 'Jimenez et al. (2013) Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh.
    2013. SOFTCARDINALITY: Hierarchical Text Overlap for Student Response Analysis.
    In *Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume
    2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval
    2013)*. Association for Computational Linguistics, Atlanta, Georgia, USA, 280–284.
    [https://aclanthology.org/S13-2047](https://aclanthology.org/S13-2047)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jimenez et al. (2013) Sergio Jimenez, Claudia Becerra 和 Alexander Gelbukh.
    2013. SOFTCARDINALITY: 学生回答分析的层次文本重叠。 在*第2届词汇与计算语义联合会议（*SEM）第2卷：第七届国际语义评估研讨会论文集（SemEval
    2013）*。 Association for Computational Linguistics，Atlanta，Georgia，USA，280–284。
    [https://aclanthology.org/S13-2047](https://aclanthology.org/S13-2047)'
- en: 'Kaggle ([n. d.]) Kaggle. [n. d.]. The Hewlett Foundation: Short Answer Scoring.
    [https://www.kaggle.com/c/asap-sas/](https://www.kaggle.com/c/asap-sas/)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaggle ([n. d.]) Kaggle。[n. d.]。Hewlett Foundation：短答案评分。 [https://www.kaggle.com/c/asap-sas/](https://www.kaggle.com/c/asap-sas/)
- en: Kaur and Hornof (2005) Ishwinder Kaur and Anthony J. Hornof. 2005. *A Comparison
    of LSA, WordNet and PMI-IR for Predicting User Click Behavior*. Association for
    Computing Machinery, New York, NY, USA, 51–60. [https://doi.org/10.1145/1054972.1054980](https://doi.org/10.1145/1054972.1054980)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaur and Hornof (2005) Ishwinder Kaur 和 Anthony J. Hornof. 2005. *LSA、WordNet
    和 PMI-IR 用于预测用户点击行为的比较*。 Association for Computing Machinery，New York, NY，USA，51–60.
    [https://doi.org/10.1145/1054972.1054980](https://doi.org/10.1145/1054972.1054980)
- en: 'Kouylekov et al. (2013) Milen Kouylekov, Luca Dini, Alessio Bosca, and Marco
    Trevisan. 2013. Celi: EDITS and Generic Text Pair Classification. In *Second Joint
    Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings
    of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)*.
    Association for Computational Linguistics, Atlanta, Georgia, USA, 592–597. [https://aclanthology.org/S13-2099](https://aclanthology.org/S13-2099)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kouylekov 等人 (2013) Milen Kouylekov, Luca Dini, Alessio Bosca 和 Marco Trevisan。2013。Celi:
    EDITS 和通用文本对分类。在 *第二届联合词汇与计算语义学会议 (*SEM)，第 2 卷：第七届国际语义评估研讨会 (SemEval 2013) 论文集*。计算语言学协会，美国乔治亚州亚特兰大，592–597。
    [https://aclanthology.org/S13-2099](https://aclanthology.org/S13-2099)'
- en: Kumar et al. (2017) Sachin Kumar, Soumen Chakrabarti, and Shourya Roy. 2017.
    Earth Mover’s Distance Pooling over Siamese LSTMs for Automatic Short Answer Grading.
    In *Proceedings of the Twenty-Sixth International Joint Conference on Artificial
    Intelligence, IJCAI-17*. 2046–2052. [https://doi.org/10.24963/ijcai.2017/284](https://doi.org/10.24963/ijcai.2017/284)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 (2017) Sachin Kumar, Soumen Chakrabarti 和 Shourya Roy。2017。基于 Siamese
    LSTM 的 Earth Mover’s Distance 池化用于自动短答案评分。在 *第二十六届国际联合人工智能会议论文集，IJCAI-17*。2046–2052。
    [https://doi.org/10.24963/ijcai.2017/284](https://doi.org/10.24963/ijcai.2017/284)
- en: Kumar et al. (2019) Yaman Kumar, Swati Aggarwal, Debanjan Mahata, Rajiv Ratn
    Shah, Ponnurangam Kumaraguru, and Roger Zimmermann. 2019. Get IT Scored Using
    AutoSAS - An Automated System for Scoring Short Answers. In *AAAI*.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 (2019) Yaman Kumar, Swati Aggarwal, Debanjan Mahata, Rajiv Ratn Shah,
    Ponnurangam Kumaraguru 和 Roger Zimmermann。2019。使用 AutoSAS 评分短答案的自动化系统。在 *AAAI*。
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
    Learning of Language Representations. (2020). [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 等人 (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
    Sharma 和 Radu Soricut。2020。ALBERT：一种用于自监督学习语言表示的轻量级 BERT。(2020)。 [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS)
- en: Landauer et al. (1998) Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
    1998. An introduction to latent semantic analysis. *Discourse Processes* 25, 2-3
    (1998), 259–284. [https://doi.org/10.1080/01638539809545028](https://doi.org/10.1080/01638539809545028)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landauer 等人 (1998) Thomas K Landauer, Peter W. Foltz 和 Darrell Laham。1998。潜在语义分析简介。*Discourse
    Processes* 25, 2-3 (1998), 259–284。 [https://doi.org/10.1080/01638539809545028](https://doi.org/10.1080/01638539809545028)
- en: 'Levy et al. (2013) Omer Levy, Torsten Zesch, Ido Dagan, and Iryna Gurevych.
    2013. In *Second Joint Conference on Lexical and Computational Semantics (*SEM),
    Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation
    (SemEval 2013)*. Association for Computational Linguistics, Atlanta, Georgia,
    USA, 285–289.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy 等人 (2013) Omer Levy, Torsten Zesch, Ido Dagan 和 Iryna Gurevych。2013。在 *第二届联合词汇与计算语义学会议
    (*SEM)，第 2 卷：第七届国际语义评估研讨会 (SemEval 2013) 论文集*。计算语言学协会，美国乔治亚州亚特兰大，285–289。
- en: Liu et al. (2019a) Tianqiao Liu, Wenbiao Ding, Zhiwei Wang, Jiliang Tang, Gale Yan
    Huang, and Zitao Liu. 2019a. Automatic Short Answer Grading via Multiway Attention
    Networks. *ArXiv* abs/1909.10166 (2019).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2019a) Tianqiao Liu, Wenbiao Ding, Zhiwei Wang, Jiliang Tang, Gale Yan
    Huang 和 Zitao Liu。2019a。通过多维注意力网络自动短答案评分。*ArXiv* abs/1909.10166 (2019)。
- en: Liu et al. (2019b) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    2019b. Improving Multi-Task Deep Neural Networks via Knowledge Distillation for
    Natural Language Understanding. *ArXiv* abs/1904.09482 (2019).
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2019b) Xiaodong Liu, Pengcheng He, Weizhu Chen 和 Jianfeng Gao。2019b。通过知识蒸馏提高多任务深度神经网络用于自然语言理解。*ArXiv*
    abs/1904.09482 (2019)。
- en: 'Liu et al. (2019c) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019c.
    RoBERTa: A Robustly Optimized BERT Pretraining Approach. (2019). [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)
    cite arxiv:1907.11692.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2019c) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 和 Veselin Stoyanov。2019c。RoBERTa：一种强健优化的
    BERT 预训练方法。(2019)。 [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)
    cite arxiv:1907.11692。
- en: Magooda et al. (2016) Ahmed Ezzat Magooda, Mohamed A. Zahran, Mohsen A. Rashwan,
    Hazem M. Raafat, and Magda B. Fayek. 2016. Vector Based Techniques for Short Answer
    Grading. In *Proceedings of the Twenty-Ninth International Florida Artificial
    Intelligence Research Society Conference, FLAIRS 2016, Key Largo, Florida, USA,
    May 16-18, 2016*, Zdravko Markov and Ingrid Russell (Eds.). AAAI Press, 238–243.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magooda et al. (2016) Ahmed Ezzat Magooda, Mohamed A. Zahran, Mohsen A. Rashwan,
    Hazem M. Raafat, 和 Magda B. Fayek. 2016. 基于向量的短答案评分技术。在 *第二十九届国际佛罗里达人工智能研究学会会议（FLAIRS
    2016）论文集，佛罗里达州基拉戈，2016年5月16-18日*，Zdravko Markov 和 Ingrid Russell（编辑）。AAAI出版社，238–243。
- en: Meurers et al. (2011) Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey M Bailey.
    2011. Integrating parallel analysis modules to evaluate the meaning of answers
    to reading comprehension questions. *International journal of continuing engineering
    education and life-long learning* 21 (2011), 355–369.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meurers et al. (2011) Detmar Meurers, Ramon Ziai, Niels Ott, 和 Stacey M Bailey.
    2011. 整合平行分析模块以评估阅读理解问题答案的含义。*国际持续工程教育与终身学习期刊* 21 (2011), 355–369。
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, G. Corrado, and J. Dean. 2013.
    Efficient Estimation of Word Representations in Vector Space. In *ICLR*.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, G. Corrado, 和 J. Dean. 2013.
    向量空间中词表示的高效估计。在 *ICLR*。
- en: 'Miller (1995) George A. Miller. 1995. WordNet: A Lexical Database for English.
    *Commun. ACM* 38, 11 (Nov. 1995), 39–41. [https://doi.org/10.1145/219717.219748](https://doi.org/10.1145/219717.219748)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miller (1995) George A. Miller. 1995. WordNet: 一个英语词汇数据库。*Commun. ACM* 38,
    11 (1995年11月), 39–41. [https://doi.org/10.1145/219717.219748](https://doi.org/10.1145/219717.219748)'
- en: 'Mohler et al. (2011) Michael Mohler, Razvan Bunescu, and Rada Mihalcea. 2011.
    Learning to Grade Short Answer Questions using Semantic Similarity Measures and
    Dependency Graph Alignments. In *Proceedings of the 49th Annual Meeting of the
    Association for Computational Linguistics: Human Language Technologies*. Association
    for Computational Linguistics, Portland, Oregon, USA, 752–762.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohler et al. (2011) Michael Mohler, Razvan Bunescu, 和 Rada Mihalcea. 2011.
    利用语义相似性度量和依赖图对齐学习短答案问题评分。在 *第49届计算语言学协会年会：人类语言技术会议论文集*。计算语言学协会，美国俄勒冈州波特兰，752–762。
- en: Mohler and Mihalcea (2009) Michael Mohler and Rada Mihalcea. 2009. Text-to-Text
    Semantic Similarity for Automatic Short Answer Grading. In *Proceedings of the
    12th Conference of the European Chapter of the ACL (EACL 2009)*. Association for
    Computational Linguistics, Athens, Greece, 567–575.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohler 和 Mihalcea (2009) Michael Mohler 和 Rada Mihalcea. 2009. 用于自动短答案评分的文本到文本语义相似性。在
    *第十二届欧洲计算语言学学会会议（EACL 2009）论文集*。计算语言学协会，希腊雅典，567–575。
- en: 'Namazifar et al. (2021) Mahdi Namazifar, Alexandros Papangelis, Gokhan Tur,
    and Dilek Hakkani-Tür. 2021. Language Model is all You Need: Natural Language
    Understanding as Question Answering. In *ICASSP 2021 - 2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*. 7803–7807. [https://doi.org/10.1109/ICASSP39728.2021.9413810](https://doi.org/10.1109/ICASSP39728.2021.9413810)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Namazifar et al. (2021) Mahdi Namazifar, Alexandros Papangelis, Gokhan Tur,
    和 Dilek Hakkani-Tür. 2021. 语言模型是你所需的一切：自然语言理解作为问答系统。在 *ICASSP 2021 - 2021 IEEE国际声学、语音与信号处理会议（ICASSP）*。7803–7807.
    [https://doi.org/10.1109/ICASSP39728.2021.9413810](https://doi.org/10.1109/ICASSP39728.2021.9413810)
- en: 'Navigli (2018) Roberto Navigli. 2018. Natural Language Understanding: Instructions
    for (Present and Future) Use. In *Proceedings of the 27th International Joint
    Conference on Artificial Intelligence* *(IJCAI’18)*. AAAI Press, 5697–5702.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Navigli (2018) Roberto Navigli. 2018. 自然语言理解：使用说明（现在及未来）。在 *第27届国际联合人工智能会议*
    *(IJCAI’18)* 论文集。AAAI出版社，5697–5702。
- en: Neterer and Guzide (2018) Jacob Russell Neterer and Osman Guzide. 2018. Deep
    Learning in Natural Language Processing. *Proceedings of the West Virginia Academy
    of Science* 90, 1. [https://pwvas.org/index.php/pwvas/article/view/339](https://pwvas.org/index.php/pwvas/article/view/339)
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neterer 和 Guzide (2018) Jacob Russell Neterer 和 Osman Guzide. 2018. 自然语言处理中的深度学习。*西弗吉尼亚科学院会议论文集*
    90, 1. [https://pwvas.org/index.php/pwvas/article/view/339](https://pwvas.org/index.php/pwvas/article/view/339)
- en: 'Ott et al. (2013) Niels Ott, Ramon Ziai, Michael Hahn, and Detmar Meurers.
    2013. CoMeT: Integrating different levels of linguistic modeling for meaning assessment.
    In *Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume
    2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval
    2013)*. Association for Computational Linguistics, Atlanta, Georgia, USA, 608–616.
    [https://aclanthology.org/S13-2102](https://aclanthology.org/S13-2102)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ott et al. (2013) Niels Ott, Ramon Ziai, Michael Hahn, 和 Detmar Meurers. 2013.
    CoMeT: 整合不同层次的语言建模以进行意义评估。发表于 *第二届词汇与计算语义联合会议 (*SEM)，第2卷：第七届国际语义评估研讨会（SemEval
    2013）论文集*。计算语言学协会，亚特兰大，乔治亚州，美国，608–616。 [https://aclanthology.org/S13-2102](https://aclanthology.org/S13-2102)'
- en: Otter et al. (2021) Daniel W. Otter, Julian R. Medina, and Jugal K. Kalita.
    2021. A Survey of the Usages of Deep Learning for Natural Language Processing.
    *IEEE Transactions on Neural Networks and Learning Systems* 32, 2 (2021), 604–624.
    [https://doi.org/10.1109/TNNLS.2020.2979670](https://doi.org/10.1109/TNNLS.2020.2979670)
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Otter et al. (2021) Daniel W. Otter, Julian R. Medina, 和 Jugal K. Kalita. 2021.
    深度学习在自然语言处理中的应用综述。*IEEE 神经网络与学习系统汇刊* 32, 2 (2021), 604–624。 [https://doi.org/10.1109/TNNLS.2020.2979670](https://doi.org/10.1109/TNNLS.2020.2979670)
- en: 'Pennington et al. (2014) Jeffrey Pennington, R. Socher, and Christopher D.
    Manning. 2014. Glove: Global Vectors for Word Representation. In *EMNLP*.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pennington et al. (2014) Jeffrey Pennington, R. Socher, 和 Christopher D. Manning.
    2014. Glove: 全球词向量表示。发表于 *EMNLP*。'
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized
    Word Representations. (June 2018), 2227–2237. [https://doi.org/10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, 和 Luke Zettlemoyer. 2018. 深度上下文化的词表示。(2018年6月),
    2227–2237。 [https://doi.org/10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202)
- en: Popović (2011) Maja Popović. 2011. Morphemes and POS tags for n-gram based evaluation
    metrics. In *Proceedings of the Sixth Workshop on Statistical Machine Translation*.
    Association for Computational Linguistics, Edinburgh, Scotland, 104–107. [https://aclanthology.org/W11-2110](https://aclanthology.org/W11-2110)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Popović (2011) Maja Popović. 2011. 词素和基于 n-gram 的 POS 标签评估指标。发表于 *第六届统计机器翻译研讨会论文集*。计算语言学协会，爱丁堡，苏格兰，104–107。
    [https://aclanthology.org/W11-2110](https://aclanthology.org/W11-2110)
- en: Pribadi et al. (2016) Feddy Setio Pribadi, Teguh Bharata Adji, and Adhistya Erna
    Permanasari. 2016. Automated Short Answer Scoring using Weighted Cosine Coefficient.
    (2016), 70–74. [https://doi.org/10.1109/IC3e.2016.8009042](https://doi.org/10.1109/IC3e.2016.8009042)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pribadi et al. (2016) Feddy Setio Pribadi, Teguh Bharata Adji, 和 Adhistya Erna
    Permanasari. 2016. 使用加权余弦系数的自动简答评分。(2016), 70–74。 [https://doi.org/10.1109/IC3e.2016.8009042](https://doi.org/10.1109/IC3e.2016.8009042)
- en: Pribadi et al. (2017) Feddy Setio Pribadi, Teguh Bharata Adji, Adhistya Erna
    Permanasari, Anggraini Mulwinda, and Aryo Baskoro Utomo. 2017. Automatic short
    answer scoring using words overlapping methods. *AIP Conference Proceedings* 1818,
    1, 020042. [https://doi.org/10.1063/1.4976906](https://doi.org/10.1063/1.4976906)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pribadi et al. (2017) Feddy Setio Pribadi, Teguh Bharata Adji, Adhistya Erna
    Permanasari, Anggraini Mulwinda, 和 Aryo Baskoro Utomo. 2017. 使用词汇重叠方法的自动简答评分。*AIP
    会议论文集* 1818, 1, 020042。 [https://doi.org/10.1063/1.4976906](https://doi.org/10.1063/1.4976906)
- en: Qi et al. (2019) Hui Qi, Yue Wang, Jinyu Dai, Jinqing Li, and Xiaoqiang Di.
    2019. Attention-Based Hybrid Model for Automatic Short Answer Scoring. In *Simulation
    Tools and Techniques*, Houbing Song and Dingde Jiang (Eds.). Springer International
    Publishing, Cham, 385–394.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi et al. (2019) Hui Qi, Yue Wang, Jinyu Dai, Jinqing Li, 和 Xiaoqiang Di. 2019.
    基于注意力的混合模型用于自动简答评分。发表于 *模拟工具与技术*，Houbing Song 和 Dingde Jiang (编辑)。施普林格国际出版公司，Cham，385–394。
- en: Radford (2018) Alec Radford. 2018. Improving Language Understanding by Generative
    Pre-Training.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford (2018) Alec Radford. 2018. 通过生成预训练提高语言理解。
- en: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *ArXiv*
    abs/1910.10683 (2019).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J. Liu. 2019. 利用统一文本到文本的变换器探索迁移学习的极限。*ArXiv*
    abs/1910.10683 (2019)。
- en: Ramachandran et al. (2015) Lakshmi Ramachandran, Jian Cheng, and Peter Foltz.
    2015. Identifying Patterns For Short Answer Scoring Using Graph-based Lexico-Semantic
    Text Matching. In *Proceedings of the Tenth Workshop on Innovative Use of NLP
    for Building Educational Applications*. Association for Computational Linguistics,
    Denver, Colorado, 97–106. [https://doi.org/10.3115/v1/W15-0612](https://doi.org/10.3115/v1/W15-0612)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran et al. (2015) Lakshmi Ramachandran、Jian Cheng 和 Peter Foltz. 2015.
    使用基于图的词汇语义文本匹配识别短答案评分模式。在 *第十届自然语言处理创新应用研讨会论文集*。计算语言学协会，丹佛，科罗拉多州，97–106。 [https://doi.org/10.3115/v1/W15-0612](https://doi.org/10.3115/v1/W15-0612)
- en: Ratner et al. (2019) Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic
    Sala, Shreyash Pandey, and Christopher Ré. 2019. Training Complex Models with
    Multi-Task Weak Supervision. *Proceedings of the … AAAI Conference on Artificial
    Intelligence. AAAI Conference on Artificial Intelligence* 33 (2019), 4763–4771.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ratner et al. (2019) Alexander Ratner、Braden Hancock、Jared Dunnmon、Frederic
    Sala、Shreyash Pandey 和 Christopher Ré. 2019. 使用多任务弱监督训练复杂模型。*Proceedings of the
    … AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence*
    33 (2019), 4763–4771。
- en: Riordan et al. (2017) Brian Riordan, Andrea Horbach, Aoife Cahill, Torsten Zesch,
    and Chong Min Lee. 2017. Investigating neural architectures for short answer scoring.
    In *BEA@EMNLP*.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riordan et al. (2017) Brian Riordan、Andrea Horbach、Aoife Cahill、Torsten Zesch
    和 Chong Min Lee. 2017. 研究用于短答案评分的神经架构。在 *BEA@EMNLP*。
- en: Roy et al. (2016) Shourya Roy, Himanshu S. Bhatt, and Y. Narahari. 2016. An
    Iterative Transfer Learning Based Ensemble Technique for Automatic Short Answer
    Grading. *ArXiv* abs/1609.04909 (2016).
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy et al. (2016) Shourya Roy、Himanshu S. Bhatt 和 Y. Narahari. 2016. 基于迭代迁移学习的集成技术用于自动短答案评分。*ArXiv*
    abs/1609.04909 (2016)。
- en: Saha et al. (2019) Swarnadeep Saha, Tejas I. Dhamecha, Smit Marvaniya, Peter
    Foltz, Renuka Sindhgatta, and Bikram Sengupta. 2019. Joint Multi-Domain Learning
    for Automatic Short Answer Grading. *ArXiv* abs/1902.09183 (2019).
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha et al. (2019) Swarnadeep Saha、Tejas I. Dhamecha、Smit Marvaniya、Peter Foltz、Renuka
    Sindhgatta 和 Bikram Sengupta. 2019. 自动短答案评分的联合多领域学习。*ArXiv* abs/1902.09183 (2019)。
- en: 'Saha et al. (2018) Swarnadeep Saha, Tejas I. Dhamecha, Smit Marvaniya, Renuka
    Sindhgatta, and Bikram Sengupta. 2018. Sentence Level or Token Level Features
    for Automatic Short Answer Grading?: Use Both. In *Artificial Intelligence in
    Education*, Carolyn Penstein Rosé, Roberto Martínez-Maldonado, H. Ulrich Hoppe,
    Rose Luckin, Manolis Mavrikis, Kaska Porayska-Pomsta, Bruce McLaren, and Benedict
    du Boulay (Eds.). Springer International Publishing, Cham, 503–517.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha et al. (2018) Swarnadeep Saha、Tejas I. Dhamecha、Smit Marvaniya、Renuka Sindhgatta
    和 Bikram Sengupta. 2018. 句子级别或标记级别特征用于自动短答案评分？：两者皆用。在 *人工智能教育*，Carolyn Penstein
    Rosé、Roberto Martínez-Maldonado、H. Ulrich Hoppe、Rose Luckin、Manolis Mavrikis、Kaska
    Porayska-Pomsta、Bruce McLaren 和 Benedict du Boulay（编辑）。Springer International
    Publishing, Cham, 503–517。
- en: Sahu and Bhowmick (2020) A. Sahu and P. K. Bhowmick. 2020. Feature Engineering
    and Ensemble-Based Approach for Improving Automatic Short-Answer Grading Performance.
    *IEEE Transactions on Learning Technologies* 13 (2020), 77–90.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahu and Bhowmick (2020) A. Sahu 和 P. K. Bhowmick. 2020. 特征工程和基于集成的方法以改善自动短答案评分性能。*IEEE
    Transactions on Learning Technologies* 13 (2020), 77–90。
- en: 'Sultan et al. (2016) Md Arafat Sultan, Cristobal Salazar, and Tamara Sumner.
    2016. Fast and Easy Short Answer Grading with High Accuracy. In *Proceedings of
    the 2016 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*. Association for Computational Linguistics,
    San Diego, California, 1070–1075. [https://doi.org/10.18653/v1/N16-1123](https://doi.org/10.18653/v1/N16-1123)'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sultan et al. (2016) Md Arafat Sultan、Cristobal Salazar 和 Tamara Sumner. 2016.
    以高准确率快速简便地进行短答案评分。在 *2016年北美计算语言学协会：人类语言技术会议论文集*。计算语言学协会，圣地亚哥，加州，1070–1075。 [https://doi.org/10.18653/v1/N16-1123](https://doi.org/10.18653/v1/N16-1123)
- en: Sung et al. (2019a) Chul Sung, Tejas Indulal Dhamecha, and Nirmal Mukhi. 2019a.
    Improving Short Answer Grading Using Transformer-Based Pre-training. In *Artificial
    Intelligence in Education*, Seiji Isotani, Eva Millán, Amy Ogan, Peter Hastings,
    Bruce McLaren, and Rose Luckin (Eds.). Springer International Publishing, Cham,
    469–481.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung et al. (2019a) Chul Sung, Tejas Indulal Dhamecha, 和 Nirmal Mukhi. 2019a.
    使用基于 Transformer 的预训练改进短答案评分。在 *人工智能教育*，Seiji Isotani、Eva Millán、Amy Ogan、Peter
    Hastings、Bruce McLaren 和 Rose Luckin（编辑）。Springer International Publishing, Cham,
    469–481。
- en: Sung et al. (2019b) Chul Sung, Tejas I. Dhamecha, Swarnadeep Saha, Tengfei Ma,
    V. Pulla Reddy, and Rishi Arora. 2019b. Pre-Training BERT on Domain Resources
    for Short Answer Grading. In *EMNLP/IJCNLP*.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung 等 (2019b) Chul Sung, Tejas I. Dhamecha, Swarnadeep Saha, Tengfei Ma, V.
    Pulla Reddy, 和 Rishi Arora. 2019b. 在领域资源上预训练 BERT 用于短答案评分。载于 *EMNLP/IJCNLP*.
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
    Sequence to Sequence Learning with Neural Networks. In *Proceedings of the 27th
    International Conference on Neural Information Processing Systems - Volume 2*
    *(NIPS’14)*. MIT Press, Cambridge, MA, USA, 3104–3112.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等 (2014) Ilya Sutskever, Oriol Vinyals, 和 Quoc V. Le. 2014. 使用神经网络的序列到序列学习。载于
    *Proceedings of the 27th International Conference on Neural Information Processing
    Systems - Volume 2* *(NIPS’14)*. MIT Press, 美国剑桥, MA, 3104–3112.
- en: Suzen et al. (2018) Neslihan Suzen, Alexander N. Gorban, Jeremy Levesley, and
    Eugenij Moiseevich Mirkes. 2018. Automatic Short Answer Grading and Feedback Using
    Text Mining Methods. *ArXiv* abs/1807.10543 (2018).
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzen 等 (2018) Neslihan Suzen, Alexander N. Gorban, Jeremy Levesley, 和 Eugenij
    Moiseevich Mirkes. 2018. 使用文本挖掘方法的自动短答案评分和反馈。 *ArXiv* abs/1807.10543 (2018).
- en: 'Tevet et al. (2019) Guy Tevet, Gavriel Habib, Vered Shwartz, and Jonathan Berant.
    2019. Evaluating Text GANs as Language Models. In *Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers)*. Association for
    Computational Linguistics, Minneapolis, Minnesota, 2241–2247. [https://doi.org/10.18653/v1/N19-1233](https://doi.org/10.18653/v1/N19-1233)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tevet 等 (2019) Guy Tevet, Gavriel Habib, Vered Shwartz, 和 Jonathan Berant.
    2019. 评估文本生成对抗网络作为语言模型。载于 *Proceedings of the 2019 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*. 计算语言学协会, 美国明尼阿波利斯, 2241–2247. [https://doi.org/10.18653/v1/N19-1233](https://doi.org/10.18653/v1/N19-1233)'
- en: Tsiakmaki et al. (2020) Maria Tsiakmaki, Georgios Kostopoulos, Sotiris Kotsiantis,
    and Omiros Ragos. 2020. Transfer Learning from Deep Neural Networks for Predicting
    Student Performance. *Applied Sciences* 10, 6 (2020). [https://doi.org/10.3390/app10062145](https://doi.org/10.3390/app10062145)
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsiakmaki 等 (2020) Maria Tsiakmaki, Georgios Kostopoulos, Sotiris Kotsiantis,
    和 Omiros Ragos. 2020. 从深度神经网络迁移学习以预测学生表现。 *Applied Sciences* 10, 6 (2020). [https://doi.org/10.3390/app10062145](https://doi.org/10.3390/app10062145)
- en: van der Lee and van den Bosch (2017) Chris van der Lee and Antal van den Bosch.
    2017. Exploring Lexical and Syntactic Features for Language Variety Identification.
    In *Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
    and Dialects (VarDial)*. Association for Computational Linguistics, Valencia,
    Spain, 190–199. [https://doi.org/10.18653/v1/W17-1224](https://doi.org/10.18653/v1/W17-1224)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van der Lee 和 van den Bosch (2017) Chris van der Lee 和 Antal van den Bosch.
    2017. 探索语言变体识别中的词汇和句法特征。载于 *Proceedings of the Fourth Workshop on NLP for Similar
    Languages, Varieties and Dialects (VarDial)*. 计算语言学协会, 西班牙瓦伦西亚, 190–199. [https://doi.org/10.18653/v1/W17-1224](https://doi.org/10.18653/v1/W17-1224)
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is All you Need. 30 (2017). [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 《Attention
    is All you Need》。30 (2017). [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- en: Wang et al. (2019) Tianqi Wang, Naoya Inoue, Hiroki Ouchi, Tomoya Mizumoto,
    and Kentaro Inui. 2019. Inject Rubrics into Short Answer Grading System. In *Proceedings
    of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)*.
    Association for Computational Linguistics, Hong Kong, China, 175–182. [https://doi.org/10.18653/v1/D19-6119](https://doi.org/10.18653/v1/D19-6119)
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2019) Tianqi Wang, Naoya Inoue, Hiroki Ouchi, Tomoya Mizumoto, 和 Kentaro
    Inui. 2019. 将评估标准注入短答案评分系统。载于 *Proceedings of the 2nd Workshop on Deep Learning
    Approaches for Low-Resource NLP (DeepLo 2019)*. 计算语言学协会, 中国香港, 175–182. [https://doi.org/10.18653/v1/D19-6119](https://doi.org/10.18653/v1/D19-6119)
- en: Wang et al. (2018) Tianqi Wang, Tomoya Mizumoto, Naoya Inoue, and Kentaro Inui.
    2018. Identifying Current Issues in Short Answer Grading.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2018) Tianqi Wang, Tomoya Mizumoto, Naoya Inoue, 和 Kentaro Inui. 2018.
    识别短答案评分中的当前问题。
- en: 'Wang et al. (2020) Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li,
    Jilin Chen, Alex Beutel, and Ed Chi. 2020. CAT-Gen: Improving Robustness in NLP
    Models via Controlled Adversarial Text Generation. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP)*. Association
    for Computational Linguistics, Online, 5141–5146. [https://doi.org/10.18653/v1/2020.emnlp-main.417](https://doi.org/10.18653/v1/2020.emnlp-main.417)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li, Jilin
    Chen, Alex Beutel, 和 Ed Chi. 2020. CAT-Gen：通过受控对抗文本生成提高NLP模型的鲁棒性。在*2020年自然语言处理实证方法会议（EMNLP）论文集*。计算语言学协会，在线，5141–5146.
    [https://doi.org/10.18653/v1/2020.emnlp-main.417](https://doi.org/10.18653/v1/2020.emnlp-main.417)
- en: 'Wang et al. (2021) Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, and Aoshuang
    Ye. 2021. Towards a Robust Deep Neural Network in Texts: A Survey. arXiv:cs.CL/1902.07285'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021) Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, 和 Aoshuang Ye.
    2021. 面向文本的鲁棒深度神经网络：综述。arXiv:cs.CL/1902.07285
- en: 'Wei and Zou (2019) Jason Wei and Kai Zou. 2019. EDA: Easy Data Augmentation
    Techniques for Boosting Performance on Text Classification Tasks. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*.
    Hong Kong, China, 6382–6388. [https://doi.org/10.18653/v1/D19-1670](https://doi.org/10.18653/v1/D19-1670)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei and Zou (2019) Jason Wei 和 Kai Zou. 2019. EDA：用于提升文本分类任务性能的简单数据增强技术。在*2019年自然语言处理实证方法会议与第九届国际联合自然语言处理会议（EMNLP-IJCNLP）论文集*。中国香港，6382–6388.
    [https://doi.org/10.18653/v1/D19-1670](https://doi.org/10.18653/v1/D19-1670)
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining
    for Language Understanding. In *Advances in Neural Information Processing Systems*,
    H. Wallach, H. Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox, and R. Garnett
    (Eds.), Vol. 32\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ
    R Salakhutdinov, 和 Quoc V Le. 2019. XLNet：用于语言理解的广义自回归预训练。在*神经信息处理系统进展*，H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, 和 R. Garnett（编辑），第32卷。Curran
    Associates, Inc. [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)
- en: Yoon et al. (2017) Yoon, Carl Denton, Luong Hoang, and Alexander M. Rush. 2017.
    Structured Attention Networks. *CoRR* abs/1702.00887 (2017). arXiv:1702.00887
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoon et al. (2017) Yoon, Carl Denton, Luong Hoang, 和 Alexander M. Rush. 2017.
    结构化注意力网络。*CoRR* abs/1702.00887 (2017). arXiv:1702.00887
- en: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik
    Cambria. 2018. Recent Trends in Deep Learning Based Natural Language Processing
    [Review Article]. *IEEE Computational Intelligence Magazine* 13, 3 (2018), 55–75.
    [https://doi.org/10.1109/MCI.2018.2840738](https://doi.org/10.1109/MCI.2018.2840738)
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, 和 Erik Cambria.
    2018. 基于深度学习的自然语言处理的最新趋势 [综述文章]。*IEEE 计算智能杂志* 13, 3 (2018), 55–75. [https://doi.org/10.1109/MCI.2018.2840738](https://doi.org/10.1109/MCI.2018.2840738)
- en: Zhang et al. (2019) Lishan Zhang, Yuwei Huang, Xi Yang, Shengquan Yu, and Fuzhen
    Zhuang. 2019. An automatic short-answer grading model for semi-open-ended questions.
    *Interactive Learning Environments* 0, 0 (2019), 1–14. [https://doi.org/10.1080/10494820.2019.1648300](https://doi.org/10.1080/10494820.2019.1648300)
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2019) Lishan Zhang, Yuwei Huang, Xi Yang, Shengquan Yu, 和 Fuzhen
    Zhuang. 2019. 一种自动化短答案评分模型，用于半开放式问题。*互动学习环境* 0, 0 (2019), 1–14. [https://doi.org/10.1080/10494820.2019.1648300](https://doi.org/10.1080/10494820.2019.1648300)
- en: 'Zhang et al. (2020a) Yuan Zhang, Chen Lin, and Min Chi. 2020a. Going deeper:
    Automatic short-answer grading by combining student and question models. *User
    Modeling and User-Adapted Interaction* 30, 1 (01 Mar 2020), 51–80. [https://doi.org/10.1007/s11257-019-09251-6](https://doi.org/10.1007/s11257-019-09251-6)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020a) Yuan Zhang, Chen Lin, 和 Min Chi. 2020a. 更深入：通过结合学生和问题模型的自动短答案评分。*用户建模与用户适应交互*
    30, 1 (2020年3月1日), 51–80. [https://doi.org/10.1007/s11257-019-09251-6](https://doi.org/10.1007/s11257-019-09251-6)
- en: Zhang et al. (2020b) Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang
    Zhang, Xi Zhou, and Xiang Zhou. 2020b. Semantics-Aware BERT for Language Understanding.
    *Proceedings of the AAAI Conference on Artificial Intelligence* 34, 05 (Apr. 2020),
    9628–9635. [https://doi.org/10.1609/aaai.v34i05.6510](https://doi.org/10.1609/aaai.v34i05.6510)
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2020b）Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang,
    Xi Zhou和Xiang Zhou。2020b。语义感知的语言理解BERT。《人工智能AAAI会议论文集》34, 05（2020年4月），9628-9635。[https://doi.org/10.1609/aaai.v34i05.6510](https://doi.org/10.1609/aaai.v34i05.6510)
