- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:47:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2204.03503] Survey on Automated Short Answer Grading with Deep Learning: from
    Word Embeddings to Transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2204.03503](https://ar5iv.labs.arxiv.org/html/2204.03503)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings
    to Transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stefan Haller University of TwenteEnschedeThe Netherlands [s.m.haller@student.utwente.nl](mailto:s.m.haller@student.utwente.nl)
    ,  Adina Aldea University of TwenteEnschedeThe Netherlands [a.aldea@utwente.nl](mailto:a.aldea@utwente.nl)
    ,  Christin Seifert University of TwenteEnschedeThe Netherlands - University of
    Duisburg-EssenEssenGermany [christin.seifert@uni-due.de](mailto:christin.seifert@uni-due.de)
     and  Nicola Strisciuglio University of TwenteEnschedeThe Netherlands [n.strisciuglio@utwente.nl](mailto:n.strisciuglio@utwente.nl)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Automated short answer grading (ASAG) has gained attention in education as
    a means to scale educational tasks to the growing number of students. Recent progress
    in Natural Language Processing and Machine Learning has largely influenced the
    field of ASAG, of which we survey the recent research advancements. We complement
    previous surveys by providing a comprehensive analysis of recently published methods
    that deploy deep learning approaches. In particular, we focus our analysis on
    the transition from hand-engineered features to *representation learning* approaches,
    which learn representative features for the task at hand automatically from large
    corpora of data. We structure our analysis of deep learning methods along three
    categories: word embeddings, sequential models, and attention-based methods. Deep
    learning impacted ASAG differently than other fields of NLP, as we noticed that
    the learned representations alone do not contribute to achieve the best results,
    but they rather show to work in a complementary way with hand-engineered features.
    The best performance are indeed achieved by methods that combine the carefully
    hand-engineered features with the power of the semantic descriptions provided
    by the latest models, like transformers architectures. We identify challenges
    and provide an outlook on research direction that can be addressed in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: deep learning, machine learning, natural language processing, short answer grading,
    text representation learning
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the education field, large attention is dedicated to characterizing the learning
    process of students to determine the efficiency and success of learners in acquiring
    new knowledge. Assessing and quantifying the knowledge gain are fundamental aspects
    for evaluating the quality of the learning process. Due to the time-consuming
    and individual nature of the assessment process, it is crucial to render it as
    efficient as possible, e.g. by using some level of grading automation, while retaining
    or even improving its quality (Mohler and Mihalcea, [2009](#bib.bib45)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides formative assessment (e.g. teacher feedback of assignments), the main
    method of assessment is via summative assessment (e.g. written examinations) (Suzen
    et al., [2018](#bib.bib70)), where knowledge acquired by the students can be tested
    in several ways. Different types of questions can be formulated: closed-form questions
    (e.g. multiple-choice) or open answer questions (e.g. essays or short answers) (Burrows
    et al., [2015](#bib.bib3)). Automated grading for multiple-choice questions is
    straightforward and immediate. Automated Essay Scoring (AES) and Automated Short
    Answer Grading (ASAG) are more challenging tasks because the assessment of these
    answers requires an understanding of the text and a more detailed analysis (Magooda
    et al., [2016](#bib.bib40); Suzen et al., [2018](#bib.bib70)). The main difference
    between AES and ASAG is that the latter deals with short answers usually graded
    against a reference answer, whereas the former is concerned with scoring longer
    textual answers and the grading is based on evaluating the quality in terms of
    spelling, grammar and coherence, and less on compact information content (Magooda
    et al., [2016](#bib.bib40)).'
  prefs: []
  type: TYPE_NORMAL
- en: The assessment of textual open answers is challenging and requires techniques
    and approaches from different fields, such as natural language processing (NLP),
    text understanding, and reading comprehension. Recent progress in Deep Learning
    and NLP facilitated the design and analysis of machine learning models for textual
    data (Suzen et al., [2018](#bib.bib70); Neterer and Guzide, [2018](#bib.bib48))
    and raised the question of their applicability to different application areas,
    such as AES and ASAG. These advancements and their application to ASAG have not
    been covered in recent surveys, which only reviewed methods and approaches based
    on feature extraction used in combination with classical machine learning models (Burrows
    et al., [2015](#bib.bib3); Galhardi and Brancher, [2018](#bib.bib20); Blessing
    et al., [2021](#bib.bib2)). This research focuses solely on ASAG, for which it
    is fundamental to construct an effective semantic representation of the answers.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we extend previous surveys by analyzing recently published methods
    for ASAG based on deep learning. We first provide a historical perspective with
    an overview of the changes and developments in the field over time and subsequently
    dive deeper into the contributions of the most recent methods. We pay attention
    to the role of text representations, i.e. features, their importance to effectively
    describe the characteristics of sentences and paragraphs, and the shift from hand-engineered
    to automatically learned features determined by the use of deep learning methodologies.
    We organize the works following a simple taxonomy that focuses on the type of
    text representation used. It includes methods based on *classical machine learning*
    , which combine hand-engineered features with classifiers, and *deep learning*
    methods that are able to learn relevant features directly from training data.
    We group the methods in the first category according to the type of features,
    namely lexical, syntactic, and semantic features. The methods in the second group,
    deep learning-based methods, are organized according to the mechanisms used to
    learn textual representation, such as word-embeddings, sequential models, and
    attention-based models. Further, we identify trends in the design of the network
    architectures and provide an outlook on future developments and challenges that
    need to be addressed, such as improving the textual understanding in cross-language
    settings and the generalization of actual methods to work with cross-domain texts
    (e.g. for different disciplines).
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of the article is organized as follows. In Section [2](#S2 "2\.
    Contributions ‣ Survey on Automated Short Answer Grading with Deep Learning: from
    Word Embeddings to Transformers") we discuss the contributions of the present
    work with respect to previous survey approaches, while in Section [3](#S3 "3\.
    Historical perspective ‣ Survey on Automated Short Answer Grading with Deep Learning:
    from Word Embeddings to Transformers") we provide a view of the historical developments
    related to the research field of ASAG. In Section [4](#S4 "4\. Organization of
    the review ‣ Survey on Automated Short Answer Grading with Deep Learning: from
    Word Embeddings to Transformers"), we elaborate on the scope and organization
    of the survey, while in Section [5](#S5 "5\. Benchmark data sets for Short Answer
    Grading ‣ Survey on Automated Short Answer Grading with Deep Learning: from Word
    Embeddings to Transformers"), we present the details of the most used benchmark
    data sets. In Section [6](#S6 "6\. Taxonomy ‣ Survey on Automated Short Answer
    Grading with Deep Learning: from Word Embeddings to Transformers"), we provide
    a taxonomy for the organization of the existing works. In Section [7](#S7 "7\.
    Hand-engineered features and Machine Learning ‣ Survey on Automated Short Answer
    Grading with Deep Learning: from Word Embeddings to Transformers") and Section [8](#S8
    "8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers"), we analyze the works based on
    classical machine learning and deep learning approaches, respectively. We discuss
    actual trends and provide an outlook for future developments in the field of ASAG
    in Section [9](#S9 "9\. Discussion ‣ Survey on Automated Short Answer Grading
    with Deep Learning: from Word Embeddings to Transformers"), and draw conclusions
    in Section [10](#S10 "10\. Conclusions ‣ Survey on Automated Short Answer Grading
    with Deep Learning: from Word Embeddings to Transformers").'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Contributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The developments and progress in the field of automated short answer grading
    (ASAG) have been previously reviewed in few survey papers (Burrows et al., [2015](#bib.bib3);
    Galhardi and Brancher, [2018](#bib.bib20); Blessing et al., [2021](#bib.bib2)).
    A unified and comprehensive overview of the entire field can be found in (Burrows
    et al., [2015](#bib.bib3)), where the developments of analytic components until
    2014 were addressed from a historical perspective. The authors organized the existing
    approaches into five groups, which correspond to temporal themes, also called
    *eras*. Methods categorized as belonging to a certain era share the approach and
    focus: word and sentence matching, feature extraction and comparison, use of semantic
    information, inference based on machine learning tools, and word embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: A comprehensive analysis of approaches that combined feature engineering with
    machine learning-based predictive models was presented in (Galhardi and Brancher,
    [2018](#bib.bib20)). The authors provided a systematic literature review focusing
    on existing data sets, applied NLP techniques, and machine learning algorithms
    for ASAG. They highlighted the role and importance of a proper feature set design
    for effective training of machine learning models. Other published reviews (Blessing
    et al., [2021](#bib.bib2)) did not discuss further aspects of the developments
    in the field of ASAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'More recent advancements in short answer grading with deep learning methods
    have not been covered yet, although they concern major progress and an increase
    of performance. In this work, we address this gap and aim at complementing previous
    surveys with a thorough analysis and organization of methods based on deep networks
    and representation learning techniques. We render the details of the transition
    from hand-engineered features to semantic-rich text representations as the key
    driver for a substantial improvement of performance on ASAG and NLP tasks. We
    trace the deep learning developments of ASAG systems back to three stages, related
    to the type of techniques used to learn textual representations: (1) models based
    on word embeddings, (2) sequence-based models that learn entire sentence representations,
    and (3) attention-based models. With this organization of existing models, the
    present survey enables future extensions while providing a comprehensive and structured
    overview of published work. The key contributions of this survey are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we provide a comprehensive comparison of the latest methods for automated short
    answer grading and their performance on different data sets;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we present an overview of the main benchmark data sets for evaluation of ASAG
    systems;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we identify the actual trends and most promising model architecture for advancing
    performance in ASAG;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we present an analysis of the impact of progress in NLP and deep learning methods
    on the field of ASAG.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3\. Historical perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of the ASAG field has gone through a number of milestones. In (Burrows
    et al., [2015](#bib.bib3)), the authors identified some trends of the progress
    and associated them with specific time periods. Initially, researchers used concept
    mapping to compare answers to a reference answer by breaking them down into several
    concepts. The inferred concepts were used as basic reference models. Predictions
    of the correctness of the answers were made by inspecting their similarities with
    the reference answerss. These approaches were extended or partly replaced by methods
    developed in the area of information retrieval, which were more focused on the
    extraction of relevant characteristics, i.e., features, from the student answers
    for direct comparison with reference answers. Semantic information and relations
    between words in sentences were not taken into account during these initial stages
    of research. These mainly concerned analyses of text, based on regular expressions
    or parse trees (Burrows et al., [2015](#bib.bib3)). Subsequently, semantic similarities
    between sentences were modeled and exploited by using large corpora of text, e.g.
    WordNet  (Fellbaum, [2000](#bib.bib15)), word synonyms, and the development of
    knowledge-based methods. Semantic features improved the overall performance of
    ASAG methods and made them more flexible  (Magooda et al., [2016](#bib.bib40)).
  prefs: []
  type: TYPE_NORMAL
- en: The development of Machine Learning methods to construct predictive models also
    influenced the field of ASAG. The inference capabilities of trainable classification
    systems shifted the focus of researchers towards studying the effect of using
    different sets of features for the extraction of relevant characteristics of the
    text, namely lexical, syntactic, semantic features, and combinations of them.
    Extensive feature engineering was needed to construct reasonable sets of features
    that could perform well on a given problem and on data sets with specific characteristics.
    The construction of feature sets thus required domain knowledge by human experts,
    which could guarantee the design of systems that achieved good performance results
    on specific data sets (Ott et al., [2013](#bib.bib49)). Papers that proposed methods
    in this category demonstrated that semantic feature extraction methods were key
    to obtain high performance.
  prefs: []
  type: TYPE_NORMAL
- en: A major milestone in the field of NLP was the development of word embeddings
    methods, which consist of techniques for mapping words or phrases from a vocabulary
    into a vector space with interesting and useful properties for predictive tasks.
    The aim of word embeddings is to focus on the properties those feature spaces
    are supposed to retain, e.g. capturing semantic similarity. This had a significant
    impact on the field of ASAG, as it improved the capabilities of machine learning-based
    systems to extract meaningful semantic information from text, compared to previous
    methods based on hand-crafted features. The development of sequential machine
    learning models, e.g. Recurrent Neural Networks (RNNs) (De Mulder et al., [2015](#bib.bib9);
    Dyer et al., [2016](#bib.bib12)) and Long Short-Term Memory networks (LSTMs) (Cheng
    et al., [2016](#bib.bib6)), able to learn dependencies in sequences of words (sentences
    and paragraphs) contributed to substantial improvements of the computed text representations.
    Long-range dependencies in text were then successfully modeled via attention-based
    neural networks, first combined with recurrent networks (Yoon et al., [2017](#bib.bib81))
    and then as stand-alone models (Vaswani et al., [2017](#bib.bib74)). Attention-based
    approaches were applied in tasks like detection of semantic textual similarity (Lan
    et al., [2020](#bib.bib34); Liu et al., [2019c](#bib.bib39); Raffel et al., [2019](#bib.bib58)),
    paraphrase identification (Yang et al., [2019](#bib.bib80); Liu et al., [2019b](#bib.bib38);
    Ratner et al., [2019](#bib.bib60)), reading comprehension (Lan et al., [2020](#bib.bib34);
    Raffel et al., [2019](#bib.bib58); Zhang et al., [2020b](#bib.bib85); Devlin et al.,
    [2019](#bib.bib10)), and recognizing textual entailment (Liu et al., [2019c](#bib.bib39);
    Yang et al., [2019](#bib.bib80); Liu et al., [2019b](#bib.bib38)). Methods that
    model long-range dependencies in text became the state-of-the-art approaches for
    most NLP and ASAG tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Organization of the review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The survey is structured in three parts. The first part covers important benchmark
    data sets for the evaluation of short answer grading (ASAG) algorithms (cf. Section [5](#S5
    "5\. Benchmark data sets for Short Answer Grading ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers")). Each
    of the data sets has specific characteristics that allow to evaluate different
    aspects of ASAG systems related to their generalization properties, such as grading
    of answers to questions unseen during the training phase or about new topics or
    domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We track the progress and performance improvements of methods based on classical
    machine learning tools and hand-engineered feature sets, and on more recent deep
    learning approaches in the second and third part of the survey (cf. Sections [7](#S7
    "7\. Hand-engineered features and Machine Learning ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers") and [8](#S8
    "8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers")), respectively. We pay particular
    attention to their capabilities to extract semantic-rich text representations.
    We analyze recently published methods and elaborate on the latest developments
    and trends in the field of ASAG.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We focus on the analysis of methods for automated grading of short answers,
    defined by the following criteria (Burrows et al., [2015](#bib.bib3)):'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the answer has to reflect the student knowledge and should not report just passages
    from a provided prompt text;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the answer is given in natural language;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'the answer’s length is around $50$ words, but can contain up to about $100$¹¹1Some
    of the analyzed datasets contain outliers (see table [1](#S5.T1 "Table 1 ‣ 5\.
    Benchmark data sets for Short Answer Grading ‣ Survey on Automated Short Answer
    Grading with Deep Learning: from Word Embeddings to Transformers")) that exceed
    that threshold but were nevertheless taken into account, as the majority of responses
    were below;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: grading of the answer is focused on the content rather than the writing quality;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the possible answers are restricted by the closed-form of the question.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Most of the approaches for ASAG published in recent years were based on supervised
    learning strategies and designed to support grading efficiency in educational
    settings (Burrows et al., [2015](#bib.bib3); Galhardi and Brancher, [2018](#bib.bib20)).
    Unsupervised approaches, such as ranking or clustering, were used to group together
    student answers based on similarity, aiming at improving the uniformity of grading
    and can be used as a complementary tool to supervised learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, in the case of courses whose content does not change drastically
    over time, the question pool used for examinations is slightly expanded or does
    not significantly change, and the same questions are re-used. This results in
    the availability of multiple answers to the same questions, which can be used
    as training inputs for the life-long optimization of the models. In this context,
    with the availability of labeled questions, reference answers, and student answers,
    automated grading systems based on supervised learning are mostly investigated.
    Hence, we focus on reviewing supervised methods, which are largely deployed for
    ASAG, and provide more precise evaluations due to the use of labeled correct answers
    for training.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Search methodology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We applied a semi-systematic approach for the literature review, which consisted
    of the following steps: a) keyword-based search in scientific databases; b) focus
    on benchmark data sets and methods tested on them; c) backward search based on
    references of relevant papers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We searched for relevant papers in the main scientific databases, namely ScienceDirect,
    Google Scholar, ResearchGate, Semantic Scholar, and arXiv. The main search terms
    that we used were ‘automatic grading system’, ‘automated assessment’, ‘digital
    assessment of students’, ‘short answer grading’, and adapted them depending on
    the results. We subsequently identified the most used benchmark data sets (see
    Section [5](#S5 "5\. Benchmark data sets for Short Answer Grading ‣ Survey on
    Automated Short Answer Grading with Deep Learning: from Word Embeddings to Transformers")),
    and selected the papers that report results on them. This allowed to include in
    the review those papers that propose systems for automated grading, also in the
    case they present them using slightly different task definitions, such as inference
    on answer similarity or automatic question answering. Finally, we gathered further
    papers by investigating the reference lists of the already selected ones. The
    criteria for the selection were the number of citations and year of publication,
    agreement with the topic concerned and application, and testing on similar data
    sets.'
  prefs: []
  type: TYPE_NORMAL
- en: We especially focused on the papers that were published in the past five years,
    in order to identify the most recent developments and trends. The increasing number
    of published papers in the past years indicates a growing interest in ASAG systems
    and in their use for educational purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Benchmark data sets for Short Answer Grading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Existing methods have been tested on different data sets, e.g. SciEntsBank,
    Beetle, Texas, ASAP-SAS, and various others. These data sets differ in many aspects:
    number of questions, number of answers, question type, domain, language, grading
    basis, and answer length. For our analysis, we rely on the results reported in
    the original papers of the methods that we surveyed. They were tested usually
    on sub-sets of the available data sets, and a direct comparison of the performance
    is sometimes not possible. This is mainly due to the different composition and
    characteristics of the available public data sets. Some data sets are proprietary
    or have an unknown source, thus we exclude them from our analysis (e.g. the Large-Scale
    Industry Dataset (Dhamecha et al., [2018](#bib.bib11); Saha et al., [2019](#bib.bib63))).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this review, we focus on the four most widely used data sets for benchmarking
    ASAG methods: SciEntsBank (Dzikovska et al., [2013](#bib.bib13)), Beetle (Dzikovska
    et al., [2013](#bib.bib13)), Texas (Mohler et al., [2011](#bib.bib44)), and ASAP-SAS (Kaggle,
    [[n. d.]](#bib.bib29)) data sets. Their public availability and diversity of the
    answer domains allow to evaluate different aspects of the performance and capabilities
    of automated grading algorithms. Furthermore, they guarantee a fair comparison
    of existing methods. In the following section, we describe the data sets and provide
    information on the specific task and applications for which they were designed.
    In Table [1](#S5.T1 "Table 1 ‣ 5\. Benchmark data sets for Short Answer Grading
    ‣ Survey on Automated Short Answer Grading with Deep Learning: from Word Embeddings
    to Transformers"), we summarize the characteristics and details of the composition
    of the considered data sets.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Detailed characteristics of the SciEntsBank, Beetle, Texas2011 and
    ASAP-SAS benchmark data sets. ”Additional information” indicates whether additional
    textual information for the task besides the question itself is available.
  prefs: []
  type: TYPE_NORMAL
- en: '| Characteristics | SciEntsBank | Beetle | Texas2011 | ASAP-SAS |'
  prefs: []
  type: TYPE_TB
- en: '| Training question/answer pairs | 4,969 | 17,198 | 2,442 | 17,207 |'
  prefs: []
  type: TYPE_TB
- en: '| Percentage of correct answers | 40.41% | 42.49% | 44,22% | 21.57% |'
  prefs: []
  type: TYPE_TB
- en: '| Number of domains | 12 | 2 | 1 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of questions | 135 | 47 | 85 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Average number of answers per question | 37 | 366 | 29 | 1,721 |'
  prefs: []
  type: TYPE_TB
- en: '| Average answer length (in words) | 13 | 10 | 18 | 42 |'
  prefs: []
  type: TYPE_TB
- en: '| Maximum answer length (in words) | 110 | 80 | 173 | 325 |'
  prefs: []
  type: TYPE_TB
- en: '| Minimum answer length (in words) | 1 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Additional information | No | No | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Scale of labels | 2-way, 3-way, 5-way |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| classification | 2-way, 3-way, 5-way classification | Score between 0 and
    5 | Score between 0 and 2 or between 0 and 3 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Publicly available | Yes | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: 5.1\. SciEntsBank and Beetle data sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SciEntsBank and Beetle data sets are part of the SemEval 2013 challenge (Dzikovska
    et al., [2013](#bib.bib13)), which has the objective of identifying common mistakes,
    such as omissions and wrong or thematically irrelevant statements, in order to
    develop customized correction strategies. The challenge is structured to evaluate
    different aspects of short answer grading systems.
  prefs: []
  type: TYPE_NORMAL
- en: The data sets include three sets of labels, which serve to train models on 2-,3-
    and 5-way task problems. The 2-way task consists of classifying the answers either
    as correct or incorrect, whereas for the 3-way task each answer is labeled as
    either correct, contradictory or incorrect. The 2- and 3-way tasks concern the
    Recognizing Textual Entailment (RTE) task. The 5-way task aims at evaluating the
    identification of non-domain, correct, partially correct incomplete, contradictory
    and irrelevant answers. It is aimed at improving dialogue systems for tutoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test protocol is designed to evaluate the performance of the algorithms
    on three sets of data, which represent possible challenging situations where the
    grading systems can be used, namely: a) unseen answers during the training phase,
    b) unseen questions not used to train the models, and c) questions/answers from
    unseen domains. These tests are meant to evaluate different aspects of the generalization
    properties of automated grading methods and their applicability to questions and
    domains other than those they are trained for.'
  prefs: []
  type: TYPE_NORMAL
- en: SciEntsBank
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The SciEntsBank data set comprises student answers to questions collected as
    part of a standardized assessment in grades $3$ to $6$ in schools across North
    America. In total, the data set contains $4969$ answers to $135$ questions from
    $12$ domains (see Table [1](#S5.T1 "Table 1 ‣ 5\. Benchmark data sets for Short
    Answer Grading ‣ Survey on Automated Short Answer Grading with Deep Learning:
    from Word Embeddings to Transformers") for details). Depending on the domain category,
    the required grading concerns either a 2-way, 3-way, or 5-way classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Beetle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to the SciEntsBank data set, the Beetle data set is designed to
    test the interaction of students with a real tutorial dialogue system. The system
    teaches students in high-school physics and fundamentals of electricity and electronics.
    In order to create the data set, the dialogues have been revised and only relevant
    answers (not the interaction protocol) to questions were used. Questions are either
    factual questions, or explanation and definition questions. The corpus contains
    in total 47 questions with on average 366 student answers per question. The Beetle
    set only contains the categories of unseen answers and unseen questions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. University of North Texas data set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The University of North Texas data set (Texas2011) is also widely used to evaluate
    and compare the performance of automated grading systems. It contains answers
    given by 30 students to 80 different questions, which result in approximately
    2400 question-answer pairs. The average answer length is 50 word tokens. The questions
    are collected from 10 assignments of two examinations of basic knowledge in the
    field of Computer Science. The answers are provided with a grade given by two
    assessors, which ranges between 0 and 5\. The grades are given as integer numbers.
    There were no clear rules in the grading process and the average grade between
    the two assessors is considered the ground truth (Mohler et al., [2011](#bib.bib44)).
    The data set has the purpose of mirroring real-world issues of the process of
    grading short answers as realistically as possible. This results in more complex
    and nested answer structures, which also include the use of tables.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. ASAP-SAS data set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Automated Student Assessment Prize Short Answer Scoring (ASAP-SAS) data
    set was released as part of a Kaggle competition in 2013, sponsored by the Hewlett
    Foundation (Kaggle, [[n. d.]](#bib.bib29)). It consists of ten questions from
    different domains, e.g., English, Biology, English Language Art, Science. In total
    the training data set contains $17,207$ answers (about $1,700$ per question) and
    the test set contains $5,224$ answers. On average an answer has $50$ words, although
    a small amount of answers ($<5\%$) also contains more than 100 words. Each of
    the questions is marked with a score in the range of $0$ to $2$ or $0$ to $3$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The competition and the data set were designed to challenge assessment systems
    with realistic and diversified questions. Due to the diversity of the ten questions,
    it further aims to support the development of scoring systems and their real-world
    application in the educational sector. One unique characteristic of this data
    set is that the question structure varies a lot: for instance, some questions
    are formulated to ask for specific information contained in a prompt text of 1
    or 2 pages. Other questions also contain pictures or graphs and the student is
    requested to state her own observations and interpretation of the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Taxonomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Existing ASAG approaches fall into two broad categories, namely (1) early approaches
    which rely on hand-crafted features and classical machine learning (CML) approaches,
    e.g. logistic regression or support vector machines, and (2) deep learning (DL)
    approaches that formulate the feature design as a learning problem and combine
    it with training a predictive model (Sung et al., [2019a](#bib.bib67)). The second
    category of methods can be further divided into three sub-categories, which correspond
    to the phases of the development of NLP methods, namely word embedding, sequential
    models, and attention-based models (Young et al., [2018](#bib.bib82); Otter et al.,
    [2021](#bib.bib50)). Figure [1](#S6.F1 "Figure 1 ‣ 6\. Taxonomy ‣ Survey on Automated
    Short Answer Grading with Deep Learning: from Word Embeddings to Transformers")
    shows an overview of the taxonomy of methods for the ASAG method.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg version="1.1" fill="none" height="159.263871592639" stroke="none" width="458.281444582814"
     overflow="visible"><g transform="translate(0,159.263871592639)
    scale(1,-1)"><g transform="translate(0,0)"><g transform="translate(0,122) scale(1,
    -1)"><foreignobject width="352" height="122" overflow="visible">![Refer to caption](img/722498617bd5eaf14fe65d0f4b697389.png)</foreignobject></g></g><g
    transform="translate(110.8,142.06)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-weight="bold">Automated short answer grading</text></g><g transform="translate(17.64,101.47)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-weight="bold">classical
    machine learning</text></g><g transform="translate(293.55,100.06)"><text x="0"
    y="0" transform="scale(1, -1)" fill="black" font-weight="bold">deep learning</text></g><g
    transform="translate(310.38,76.69)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-weight="bold">word embeddings</text></g><g transform="translate(310.15,44.87)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-weight="bold">sequential-based
    models</text></g><g transform="translate(309.63,12.51)"><text x="0" y="0" transform="scale(1,
    -1)" fill="black" font-weight="bold">attention-based models</text></g><g transform="translate(38.7,76.69)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-weight="bold">lexical features</text></g><g
    transform="translate(38.47,44.87)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-weight="bold">syntactic features</text></g><g transform="translate(37.95,12.51)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-weight="bold">semantic
    features</text></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Taxonomy of Automated Short Answer Grading methods. The categorization
    of methods is based on the machine learning approach and on the type of representation
    (features) used. For classical machine learning approaches, the features are hand-engineered
    by domain experts, while in deep learning approaches the different types of representations
    are learned directly from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding techniques, most prominently Word2Vec (Mikolov et al., [2013](#bib.bib42)),
    aim to represent semantically similar words with vectors that are close in a learned
    latent space. These automatically learned representations are able to characterize
    the information in large corpora of text effectively. Longer-range relations,
    in groups of words and longer sentences, were taken into account for the development
    of the second category of methods, which include approaches based on Recurrent
    Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs). These models
    capture relations between words of a sentence over longer distances and therefore
    provide richer sentence and paragraph representations. The models are usually
    trained for specific downstream tasks and can use the embedding results from methods
    such as Word2Vec (Sutskever et al., [2014](#bib.bib69)). However, due to the vanishing
    gradient problem, recurrent networks tend to focus more towards short-term context
    information, while not being able to robustly catch longer-range dependencies
    in sentences. LSTMs alleviated this issue to some extent. The limitation was finally
    addressed by neural network models based on attention mechanisms, which constitute
    the third category of deep learning-based methods that we identified. Attention-based
    models relax the strict sequential analysis of tokens in sentences and are able
    to model word dependencies without regard to their distance in the sentences,
    also allowing for parallel processing of longer text sequences by means of multi-head
    attention mechanisms (Galassi et al., [2020](#bib.bib19)). Architectures fully
    based on attention are known as Transformers (Vaswani et al., [2017](#bib.bib74)).
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Hand-engineered features and Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Approaches based on hand-engineered features rely on the extraction of lexical,
    syntactic, and semantic features from the text, using dependency and constituency
    parsers, or functions to compute the sentence overlap. The main goal of these
    features is to describe key components (e.g. specific terms, concepts) of good
    answers by detecting specific patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most of the reviewed papers, the feature vectors extracted from the raw
    text were used in combination with a classical machine learning classifier, such
    as Logistic Regressor, Support Vector Machine, Random Forest, or Naïve Bayes classifiers.
    Some of the reviewed approaches were based on ensemble methods combining predictions
    of several classifiers. In the following, we revise these approaches, organized
    in sections according to the type of features used to represent the text, namely
    1) lexical, 2) syntactic and 3) semantic features, and elaborate on their relevance
    and use in existing methods. It is important to note that few approaches deployed
    features sets that were made of combinations of different types of features. This
    makes it difficult to draw absolute conclusions regarding the superiority of a
    certain feature or group of features over another. We provide an overview of the
    reviewed methods in Table [2](#S7.T2 "Table 2 ‣ 7.1\. Lexical features ‣ 7\. Hand-engineered
    features and Machine Learning ‣ Survey on Automated Short Answer Grading with
    Deep Learning: from Word Embeddings to Transformers") and the results that they
    achieved on benchmark data sets in Table [3](#S7.T3 "Table 3 ‣ 7.1\. Lexical features
    ‣ 7\. Hand-engineered features and Machine Learning ‣ Survey on Automated Short
    Answer Grading with Deep Learning: from Word Embeddings to Transformers").'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Lexical features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lexical features have been largely used for the description of the characteristics
    of short textual answers. They consist of, for example, single words, stemmed
    or lemmatized words, their prefix or suffix. The extraction of lexical features
    is a simple process and several algorithms were proposed that use them in ASAG
    tasks, e.g. methods to compute the degree of overlap between sentences, n-gram
    representations, or lexical statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithms for estimating word overlap were a fundamental component of early
    ASAG systems. Overlap-based features measure the coinciding words between sentences,
    i.e. the student and reference answer, using methods to estimate the word-wise
    or character-wise overlap of two or more sample sentences. Often these methods
    were combined with different pre-processing methods, such as lemmatization or
    stemming to further improve the quality of the results (Ott et al., [2013](#bib.bib49);
    Meurers et al., [2011](#bib.bib41); Mohler et al., [2011](#bib.bib44)). In  (Pribadi
    et al., [2017](#bib.bib55)), the authors compared three-word overlap computation
    methods: i) Dice coefficient, ii) Jaccard coefficient and iii) cosine coefficient,
    and studied their impact on the performance of an ASAG system. These methods count
    the overlapping words in two sentences and compute a score of the similarity between
    them. The authors found that the cosine coefficient contributed to obtaining the
    best performance on the estimation of sentence similarity. In (Pribadi et al.,
    [2016](#bib.bib54)), a weighted cosine coefficient was used, resulting in a further
    improvement of the performance of an ASAG system. Other approaches extracted the
    raw number of overlapping words and calculated several string similarity scores,
    e.g. cosine similarity and Lesk similarity (Dzikovska et al., [2012](#bib.bib14)).
    In addition, the authors computed sub-tree matching based on lexical features,
    in which they counted overlapping words and word stems. In (Meurers et al., [2011](#bib.bib41)),
    the authors used features that are based on the overlapping words between the
    reference answer and the student answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the complexity and support of the overlap detection methods further
    improved performance. In (Jimenez et al., [2013](#bib.bib28)), the authors proposed
    a method that computes sentence overlap in a hierarchical fashion, called SoftCardinality.
    It computes several similarity scores by considering different character q-grams
    between words and sentences, and achieved state-of-the-art results on the SemEval
    data set. The achieved results demonstrated that a more fine-grained feature extraction
    technique describes more complex characteristics of the text and, subsequently,
    improves the overall performance of the scoring system. Similar conclusions were
    drawn in (Heilman and Madnani, [2013](#bib.bib25)), where the authors proposed
    a method focused on the textual lexical similarity, computed by using character
    and word n-grams. In particular, the similarity was calculated taking into account
    the number of overlaps in the two sentences combined with string edit-distance
    features. This approach uses additional features from (Heilman and Madnani, [2012](#bib.bib24)),
    which were computed according to the edit-effort it takes to align a student answer
    and a reference answer. The final score was predicted by a logistic regression
    classifier. In addition, domain adaption was used, where different features have
    individual weights for each domain.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Details of the methods for ASAG based on lexical, syntactic and semantic
    features used in combinations with machine learning classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | Features |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ref. | Year | Classifier | Lexical | Syntactic | Semantic |'
  prefs: []
  type: TYPE_TB
- en: '| (Mohler et al., [2011](#bib.bib44)) | 2011 | SVM | word overlap | dependency
    parsers, POS tags, n-grams with POS tags | WordNet, LSA |'
  prefs: []
  type: TYPE_TB
- en: '| (Meurers et al., [2011](#bib.bib41)) | 2011 | $k$NN | Bg-of-words, n-grams,
    word overlap | dependency parsers, POS tags, n-grams with POS tags, word synonyms
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Dzikovska et al., [2012](#bib.bib14)) | 2012 | Decision Tree | Bag-of-words,
    n-grams, word overlap | dependency parsers, POS tags, n-grams with POS tags |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| (Kouylekov et al., [2013](#bib.bib31)) | 2013 | SVM, Naïve Bayes | word overlap
    | - | corpus-based statistical interrelations between words |'
  prefs: []
  type: TYPE_TB
- en: '| (Levy et al., [2013](#bib.bib36)) | 2013 | Naïve Bayes | Bag-of-words, n-grams
    | dependency parsers, POS tags, n-grams with POS tags | WordNet, LSA, ESA |'
  prefs: []
  type: TYPE_TB
- en: '| (Heilman and Madnani, [2013](#bib.bib25)) | 2013 | Logistic Regression |
    word/character overlap | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Ott et al., [2013](#bib.bib49)) | 2013 | SVM, Logistic Regression | Bag-of-words,
    n-grams, word/character overlap | dependency parsers, POS tags, n-grams with POS
    tags, minimum edit distance | WordNet, LSA, ESA |'
  prefs: []
  type: TYPE_TB
- en: '| (Jimenez et al., [2013](#bib.bib28)) | 2013 | Bagged Decision Tree | word/character
    overlap | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Ramachandran et al., [2015](#bib.bib59)) | 2015 | Random Forest Regressor
    | Bag-of-words, n-grams, word overlap | dependency parsers, POS tags, word-graphs,
    phrase patterns | WordNet |'
  prefs: []
  type: TYPE_TB
- en: '| (Magooda et al., [2016](#bib.bib40)) | 2016 | SVM | Bag-of-words, word/character
    overlap | - | word2vec, GloVE |'
  prefs: []
  type: TYPE_TB
- en: '| (Sultan et al., [2016](#bib.bib66)) | 2016 | Random Forest | bag-of-words,
    n-grams, word/character overlap | - | word2vec, GloVE |'
  prefs: []
  type: TYPE_TB
- en: '| (Roy et al., [2016](#bib.bib62)) | 2016 | Logistic Regression | word overlap
    | - | word2vec, WordNet, LSA |'
  prefs: []
  type: TYPE_TB
- en: '| (Galhardi et al., [2018](#bib.bib21)) | 2018 | Random Forests, Extreme Gradient
    Boosting | bag-of-words, n-grams, word/character overlap | dependency parsers,
    POS tags | WordNet (synonym similarity) |'
  prefs: []
  type: TYPE_TB
- en: '| (Kumar et al., [2019](#bib.bib33)) | 2019 | Random Forest | Bag-of-words,
    n-grams, word overlap | dependency parsers, POS tags, n-grams with POS tags |
    word2vec, doc2vec |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3\. Overview of performance of the ASAG methods based on hand-engineered
    features listed in Table [2](#S7.T2 "Table 2 ‣ 7.1\. Lexical features ‣ 7\. Hand-engineered
    features and Machine Learning ‣ Survey on Automated Short Answer Grading with
    Deep Learning: from Word Embeddings to Transformers"). We report accuracy unless
    otherwise specified. $\mathbf{\hat{F}}$ is the F1 score, $\mathbf{F_{M}}$ and
    $\mathbf{F_{m}}$ are the macro-averaged and micro-averaged F1 score, QW-K is the
    quadratic weighted kappa measure, RMSE is the root mean square error, and $\mathbf{\rho}$
    is the Pearson’s correlation coefficient. Performance measures are reported in
    the same precision as in the original papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | SciEntsBank | Beetle | Texas2011 | Other |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ref. | Year | Classifier | 2-way | 3-way | 5-way | 2-way | 3-way | 5-way
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (Mohler et al., [2011](#bib.bib44)) | 2011 | SVM | - | - | - | - | - | -
    | $0.518$ ($\mathbf{\rho}$) $0.998$ (RMSE) | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Meurers et al., [2011](#bib.bib41)) | 2011 | $k$NN | - | - | - | - | - |
    - | - | $0.79$ English Dev. Corpus |'
  prefs: []
  type: TYPE_TB
- en: '| (Dzikovska et al., [2012](#bib.bib14)) | 2012 | Decision Tree | - | - | $0.29$ ($F_{M}$)
    $0.42$ ($F_{m}$) | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Kouylekov et al., [2013](#bib.bib31)) | 2013 | SVM, Naïve Bayes | $0.612$
    | $0.55$ | $0.421$ | $0.648$ | $0.523$ | $0.464$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Levy et al., [2013](#bib.bib36)) | 2013 | Naïve Bayes | $0.696$ ($\hat{F}$)
    | $0.606$ ($\hat{F}$) | $0.464$ ($\hat{F}$) | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Heilman and Madnani, [2013](#bib.bib25)) | 2013 | Logistic Regression |
    - | - | $0.524$ ($\hat{F}$) | - | - | $0.659$ ($\hat{F}$) | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Ott et al., [2013](#bib.bib49)) | 2013 | SVM, Logistic Regression | $0.684$
    | $0.612$ | $0.486$ | $0.77$ | $0.624$ | $0.588$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Jimenez et al., [2013](#bib.bib28)) | 2013 | Bagged Decision Tree | $0.726$
    | $0.649$ | $0.527$ | $0.724$ | $0.538$ | $0.513$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Ramachandran et al., [2015](#bib.bib59)) | 2015 | Random Forest Regressors
    | - | - | - | - | - | - | $0.61$ ($\mathbf{\rho}$) $0.86$ (RMSE) | $0.78$ (QW-K)
    ASAP-SAS |'
  prefs: []
  type: TYPE_TB
- en: '| (Magooda et al., [2016](#bib.bib40)) | 2016 | SVM | $0.605$ ($\hat{F}$) |
    - | $0.48$ ($\hat{F}$) | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Sultan et al., [2016](#bib.bib66)) | 2016 | Random Forest | - | - | $0.56$
    ($\hat{F}$) | - | - | - | $0.85$ ($\mathbf{\rho}$) $0.63$ (RMSE) | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Roy et al., [2016](#bib.bib62)) | 2016 | Logistic Regression | - | - | $0.565$
    ($\hat{F}$) | - | - | - | $0.82$ (RMSE) | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Galhardi et al., [2018](#bib.bib21)) | 2018 | Random Forest, Extreme Gradient
    Boosting | $0.775$ | $0.719$ | $0.59$ | $0.81$ | $0.643$ | $0.644$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Kumar et al., [2019](#bib.bib33)) | 2019 | Random Forest | - | - | - | -
    | - | - | - | $0.791$ (QW-K) ASAP-SAS |'
  prefs: []
  type: TYPE_TB
- en: 7.2\. Syntactic features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Syntactic features are able to quantify important information about the meaning
    of a sentence, as they detect and describe the roles and dependencies of the words
    in it. The ability to characterize the relation between words enables to perform
    inference about the meaning of a textual answer (van der Lee and van den Bosch,
    [2017](#bib.bib73)). Basic approaches to extract syntactic features from text
    are using parse trees and part-of-speech tagging (POS tags) or dependency n-grams (Popović,
    [2011](#bib.bib53)). Dependency n-grams are derived from the syntactical relation
    between words by grouping, for instance, verb and subject together. In (Levy et al.,
    [2013](#bib.bib36)), syntactic features were computed by generating n-grams that
    consist of combinations of POS tags. The dependency of words in sequences was
    subsequently used to assess the content of an answer and evaluate its similarity
    with a reference answer. This method worked based on the concept of paraphrasing
    a sentence, the result of which consists of a sentence having different wording
    but similar meaning. In (Dzikovska et al., [2012](#bib.bib14)), the authors used
    different similarity scores based on POS tags. Other approaches focused on the
    syntactical dependency, used parse trees, and extracted graph alignments features (Mohler
    et al., [2011](#bib.bib44)). Similarity measures were enriched by generating word
    pairs with similar POS tags and calculating the corresponding similarity. The
    underlying assumption was that similarly structured answers are more likely to
    have a similar meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Semantic features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lexical features can not capture the semantic content of sentences, and syntactic
    features are able to catch such characteristics only to a limited extent. Thus,
    more sophisticated features were designed exploiting knowledge-bases to recognize
    the meaning of words therefore more robustly computing similarity among sentences.
    These methods used knowledge sources like WordNet (Miller, [1995](#bib.bib43))
    and computational approaches based on Latent Semantic Analysis (LSA) (Landauer
    et al., [1998](#bib.bib35)) and Explicit Semantic Analysis (ESA) (Gabrilovich
    and Markovitch, [2007a](#bib.bib16)), combined with different similarity metrics.
    WordNet models the semantic relation between words inducing synonyms and hyponyms.
    Many methods used Wordnet in combination with similarity metrics to better incorporate
    the semantic meaning of words. LSA is a corpus-based similarity method, in which
    words are represented as vectors in a multi-dimensional semantic space (Kaur and
    Hornof, [2005](#bib.bib30)). This method gained popularity and was demonstrated
    to perform better than word and n-gram vectors (Mohler and Mihalcea, [2009](#bib.bib45)).
    LSA was used to estimate the similarity between words and combined with a word-weighting
    factor to enhance the relevance of specific words in (Kouylekov et al., [2013](#bib.bib31)).
    ESA was designed to use the knowledge extracted from Wikipedia (Gabrilovich and
    Markovitch, [2007b](#bib.bib17)) and was demonstrated to perform comparably to
    LSA or to outperform it in some cases. In (Levy et al., [2013](#bib.bib36)), the
    authors included the semantic similarity by using predefined context vectors based
    on WordNet. They proposed to first parse the answers according to the dependencies
    of words within the sentences, and subsequently calculate the semantic similarity
    using the closest common ancestor and shortest path length using ESA and WordNet.
    In (Ott et al., [2013](#bib.bib49)), WordNet was also used to extract similarity
    information between the reference and student answer. The authors highlighted
    the importance of using different features to extract semantic information and
    weight them for an effective classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Deep learning methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deep learning developments in the ASAG field align with the methodological
    advances in the field of NLP. We organize the deep learning methods for ASAG in
    three categories, that correspond to the historical development of NLP and the
    relation to the text representation methods.
  prefs: []
  type: TYPE_NORMAL
- en: The first category contains methods based on word embedding models (e.g. Word2Vec (Mikolov
    et al., [2013](#bib.bib42))). These models compute representations that transform
    similar words into vectors that are close to each other in an embedded latent
    space, and generate sentence embeddings by either summing or averaging the single
    word embeddings. The word and sentence embeddings are able to capture semantic
    information in textual data more effectively than previous hand-engineered features (Hightower,
    [2020](#bib.bib26)). Methods in the second category deploy recurrent neural networks
    (RNNs), of which those based on long short-term memory (LSTM) networks are very
    popular, to model the sequential characteristics of textual data. These methods
    are able to capture semantic properties of the text by considering word sentences
    of different lengths, and longer-range relationships between words in sentences.
    This allowed the prediction models to carry out more robust and effective inferences
    on given answers (Saha et al., [2019](#bib.bib63)). The third category consists
    of methods that deploy attention-based mechanisms, able to describe long-range
    relationships between words in a sentence. Different from RNNs, attention-based
    methods do not need to explicitly model the sequential characteristics of words
    in sentences, but are able to process longer sentences in parallel. These architectural
    advancements were determined by the use of several self-attention components,
    each of which captures a specific relation between words. Architectures based
    only on attention are called Transformers (Vaswani et al., [2017](#bib.bib74)).
  prefs: []
  type: TYPE_NORMAL
- en: We elaborate on the details of the three categories of methods in the remainder
    of this section, addressing the benefits, drawbacks, and performance of the learned
    text representations. In Table LABEL:tab:DLdesc, we provide an overview of the
    methodological aspects of the reviewed papers, while in Table LABEL:tab:DLresults
    we summarize their performance results on benchmark data sets.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1\. Word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning methods for ASAG based on word embeddings encode semantically
    similar words to close points in a latent space (Roy et al., [2016](#bib.bib62);
    Magooda et al., [2016](#bib.bib40); Sultan et al., [2016](#bib.bib66)). The general
    success of word embeddings is attributable to their ability to describe rich semantic
    features of text. These methods were demonstrated successful to improve the evaluation
    of word similarity, but did not clearly outperform previous methods for representation
    of entire sentences in ASAG systems. In (Magooda et al., [2016](#bib.bib40)),
    for instance, the authors compared different similarity measures on word vector
    representations of text obtained using pre-trained embedding models, such as Word2Vec (Mikolov
    et al., [2013](#bib.bib42)) and GloVe (Pennington et al., [2014](#bib.bib51)).
    They found that word and sentence embeddings achieved below-average results on
    the SemEval 5-way task. Other approaches that solely focused on embeddings did
    not outperform the models with hand-engineered features on ASAG tasks (Gomaa and
    Fahmy, [2020](#bib.bib22); Magooda et al., [2016](#bib.bib40)).
  prefs: []
  type: TYPE_NORMAL
- en: To compensate for the shortcomings of word embeddings techniques to represent
    entire sentences, several methods that combine word embeddings with hand-engineered
    features were designed. The method proposed in (Roy et al., [2016](#bib.bib62)),
    for instance, combined an external knowledge-like paraphrase database and WordNet
    with syntactical similarity features. It achieved top performance results on the
    SciEntsBank dataset, with improved generalization capabilities. Other approaches,
    such as that proposed in (Sultan et al., [2016](#bib.bib66)), computed whole-sentence
    representations by combining (sum or average) single word embeddings. Lexically
    similar word pairs were obtained using an external paraphrase data set, with labeled
    graded short answers. The semantic similarity of words was computed using pre-trained
    word embeddings and the cosine similarity function, while the representation of
    entire sentences was computed by summing up the word embedding vectors. This approach
    resulted in relatively good results on a proprietary data set, but highlighted
    the need for further developments of sentence representations.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Overview of deep learning methods for ASAG, the type of semantics
    that they learn from the text, and the focus of their architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ref. | Cat. | Year | Model | Features | Focus |'
  prefs: []
  type: TYPE_TB
- en: '| (Riordan et al., [2017](#bib.bib61)) | DL3 | 2017 | LSTM+CNN with attention
    | Combination of sequence representation from different model architecture (CNN,
    LSTM) | Complex and stacked model to leverage benefit from different models |'
  prefs: []
  type: TYPE_TB
- en: '| (Kumar et al., [2017](#bib.bib32)) | DL2 | 2017 | BiLSTM | Sequence representation
    with BiLSTM | Complex model architecture and included data augmentation |'
  prefs: []
  type: TYPE_TB
- en: '| (Saha et al., [2018](#bib.bib64)) | DL1 | 2018 | Random Forest | Sentence
    representation combined with engineered features | Combination of sequence representation
    and engineered features with focus on domain adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| (Kumar et al., [2019](#bib.bib33)) | DL1 | 2019 | Random Forest | Sentence
    and word representation | Combination of sequence representation and various engineered
    features |'
  prefs: []
  type: TYPE_TB
- en: '| (Wang et al., [2019](#bib.bib75)) | DL3 | 2019 | BiLSTM | Attention based
    sentence representation | Complex structure and incorporation of engineered features
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Saha et al., [2019](#bib.bib63)) | DL2 | 2019 | BiLSTM | Combination of
    different sentence representations (domain-specific and unspecific) | Complex
    structure and domain adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| (Gomaa and Fahmy, [2020](#bib.bib22)) | DL1 | 2019 | Logistic Regression
    | Sequence representation of words and sentences | Simple application of transfer
    learning |'
  prefs: []
  type: TYPE_TB
- en: '| (Sung et al., [2019a](#bib.bib67)) | DL3 | 2019 | BERT | Attention based
    sequence representation | Simple application of transfer learning |'
  prefs: []
  type: TYPE_TB
- en: '| (Qi et al., [2019](#bib.bib56)) | DL3 | 2019 | BiLSTM+CNN | attention based
    on BiLSTMs and CNNs | Complex and stacked BiLSTM and CNN |'
  prefs: []
  type: TYPE_TB
- en: '| (Liu et al., [2019a](#bib.bib37)) | DL3 | 2019 | Multiway-attention transformer
    | attention | Complex structure with incorporation of different attention mechanism
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Sung et al., [2019b](#bib.bib68)) | DL3 | 2019 | BERT | attention | Transfer
    learning with domain adaptation |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2019](#bib.bib83)) | DL2 | 2019 | LSMT | CBOW word vectors
    | Incorporation of domain-specific knowledge and domain-general knowledge from
    wikipedia |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2020a](#bib.bib84)) | DL2 | 2020 | DBN | Application of Gaussian
    mixture model (GMM) and Cartesian product for feature composition | Complex feature
    engineering by incorporating different feature extraction methods |'
  prefs: []
  type: TYPE_TB
- en: '| (Camus and Filighera, [2020](#bib.bib5)) | DL3 | 2020 | RoBERTa | Attention
    based sequence representation | Targeted transfer learning and incorporation of
    different learning methods (cross-lingual, NLI specific model training) |'
  prefs: []
  type: TYPE_TB
- en: '| (Condor, [2020](#bib.bib7)) | DL3 | 2020 | BERT | Attention based sequence
    representation | Simple application of transfer learning |'
  prefs: []
  type: TYPE_TB
- en: '| (Sahu and Bhowmick, [2020](#bib.bib65)) | DL1 | 2020 | stacked regression
    ensemble | Compilation of various engineered features | Ensemble of eight different
    regression models via multi-layer perceptron and various engineered features |'
  prefs: []
  type: TYPE_TB
- en: '| (Gaddipati et al., [2020](#bib.bib18)) | DL2 | 2020 | Stacked BiLSTM (ELMo)
    | Bidirectional LSTM based sequence representation | Simple application of transfer
    learning from pretrained model |'
  prefs: []
  type: TYPE_TB
- en: Table 5\. Summary of the performance results of the ASAG methods based on deep
    learning listed in Table LABEL:tab:DLdesc. We report accuracy unless otherwise
    specified. $\mathbf{\hat{F}}$ is the F1 score, QW-K is the quadratic weighted
    kappa measure, C-K is the Cohen’s Kappa, RMSE is the root mean square error, and
    $\mathbf{\rho}$ is the Pearson’s correlation coefficient. Performance measures
    are reported in the same precision as in the original papers.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | SciEntsBank | Beetle | Texas2011 | Other |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ref. | Cat. | Year | 2-way | 3-way | 5-way | 2-way | 3-way | 5-way |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| (Riordan et al., [2017](#bib.bib61)) | DL3 | 2017 | $0.712$ | - | $0.533$
    | $0.79$ | - | $0.633$ | $0.518$ ($\mathbf{\rho}$) $0.998$ (RMSE) | $0.723$ (QW-K)
    ASAP-SAS |'
  prefs: []
  type: TYPE_TB
- en: '| (Kumar et al., [2017](#bib.bib32)) | DL2 | 2017 | - | $0.634$(MAE) $0.904$(RMSE)
    $0.316$ ($\mathbf{\rho}$) | - | - | - | - | $0.61$ ($\mathbf{\rho}$), $0.77$ (RMSE)
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Saha et al., [2018](#bib.bib64)) | DL1 | 2018 | $0.752$ | $0.654$ | $0.540$
    | - | - | - | $0.57$ ($\mathbf{\rho}$) $0.902$ (RMSE) |  |'
  prefs: []
  type: TYPE_TB
- en: '| (Kumar et al., [2019](#bib.bib33)) | DL1 | 2019 | - | - | - | - | - | - |
    - | $0.791$ (QW-K) ASAP-SAS |'
  prefs: []
  type: TYPE_TB
- en: '| (Wang et al., [2019](#bib.bib75)) | DL3 | 2019 | - | - | - | - | - | - |
    - | $0.77$ (QW-K) ASAP-SAS |'
  prefs: []
  type: TYPE_TB
- en: '| (Saha et al., [2019](#bib.bib63)) | DL2 | 2019 | $0.803$ ($\hat{F}$) | $0.744$ ($\hat{F}$)
    | $0.656$ ($\hat{F}$) | - | - | - | - | $0.721$ ($\hat{F}$) Large Scale Industry
    Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| (Gomaa and Fahmy, [2020](#bib.bib22)) | DL1 | 2019 | - | - | $0.503$ ($\hat{F}$)
    | - | - | - | $0.63$ ($\mathbf{\rho}$) $0.91$ (RMSE) | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Sung et al., [2019a](#bib.bib67)) | DL3 | 2019 | - | $0.68$ ($\hat{F}$)
    | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Qi et al., [2019](#bib.bib56)) | DL3 | 2019 | - | - | - | - | - | - | -
    | $0.969$(Acc) $0.969$ ($\hat{F}$) Chinese data |'
  prefs: []
  type: TYPE_TB
- en: '| (Liu et al., [2019a](#bib.bib37)) | DL3 | 2019 | - | - | - | - | - | - |
    - | $0.889$(Acc) $0.944$ (AUC) Real world K-12 |'
  prefs: []
  type: TYPE_TB
- en: '| (Sung et al., [2019b](#bib.bib68)) | DL3 | 2019 | - | - | - | - | - | - |
    - | $0.799$ (Acc) Large scale industry dataset |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2019](#bib.bib83)) | DL2 | 2019 | - | - | - | - | - | - |
    - | $0.626$ (QW-K) ASAP-SAS |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2020a](#bib.bib84)) | DL2 | 2020 | - | - | - | - | - | -
    | - | $0.850$(Acc) $0.830$ ($\hat{F}$) Cordillera |'
  prefs: []
  type: TYPE_TB
- en: '| (Camus and Filighera, [2020](#bib.bib5)) | DL3 | 2020 | - | 0.718 | - | -
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Condor, [2020](#bib.bib7)) | DL3 | 2020 | - | - | - | - | - | - | - | $0.76$(Acc)
    $0.684$ (C-K) DT-Grade |'
  prefs: []
  type: TYPE_TB
- en: '| (Sahu and Bhowmick, [2020](#bib.bib65)) | DL1 | 2020 | - | - | 0.746 | -
    | - | $0.666$ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| (Gaddipati et al., [2020](#bib.bib18)) | DL2 | 2020 | - | - | - | - | - |
    - | $0.485$ ($\mathbf{\rho}$) $0.978$ (RMSE) | - |'
  prefs: []
  type: TYPE_TB
- en: 'Previous works showed that ASAG systems perform well when word-embeddings and
    hand-engineered features are combined, while the sole use of word-embeddings does
    not contribute to consistently good results (Gomaa and Fahmy, [2020](#bib.bib22);
    Magooda et al., [2016](#bib.bib40)). For instance, in (Kumar et al., [2019](#bib.bib33)),
    good performance results were achieved by constructing a large feature set by
    combining hand-engineered and learned features. The authors proposed a method
    that combines commonly used text representations, such as Word2Vec, Doc2Vec, POS
    tagging, n-gram overlaps, with features that capture the diversity and style of
    formulation of the student answers. The design of this method was based on the
    hypothesis that the use of a more sophisticated textual form, in terms of the
    language used, is an indication for an answer of higher quality. They achieved
    state-of-the-art results on the ASAP-SAS data set and demonstrated that the Word2Vec
    and Doc2Vec embeddings, combined with features for quantification of text overlaps
    and weighted keywords play a relevant role in the automatic grading of answers.
    By including features that represent the complexity of the answers, the accuracy
    of the proposed system improved significantly. On the one hand, researchers agree
    on the importance and descriptive capabilities of embeddings (Kumar et al., [2019](#bib.bib33)).
    On the other hand, it was demonstrated that well hand-engineered features that
    capture complex properties of the text based on prior knowledge can be beneficial
    for the overall system performance: knowledge extracted from the text by embedding-based
    representations is complementary to that of previously proposed syntactical, semantic,
    and lexical features (Roy et al., [2016](#bib.bib62)).'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2\. Sequence-based models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sequential machine learning models were applied to ASAG in order to improve
    the quality of the learned features and robustness of the word and sentence representation.
    Kumar et al. (Kumar et al., [2017](#bib.bib32)) proposed to use a siamese bidirectional-LSTM
    architecture combined with a pooling layer that uses earth-mover distance. The
    authors compared the pairwise distance between the latent vectors of the reference
    answer and the student answer. They showed that the extraction of semantic textual
    features using sequence-based models improves the quality of the learned representation
    and the performance of the automated grading systems.
  prefs: []
  type: TYPE_NORMAL
- en: Further focus was given to the training of sequential models for NLP and their
    adaptation to ASAG problems. Specifically, fine-tuning of pre-trained models for
    the analysis of text sequence from other, more general domains to the task of
    ASAG was explored using techniques for transfer learning (Cai, [2019](#bib.bib4);
    Tsiakmaki et al., [2020](#bib.bib72)). Transfer learning provides the possibility
    of exploiting features that are learned from large corpora of text data, and that
    have more powerful semantic representation capabilities. In this context, the
    Universal Sentence Representation model was often used in transfer learning settings
    to extract word representations (Conneau et al., [2017](#bib.bib8)). The authors
    trained a bi-directional LSTM network on the large Stanford Natural Language Inference
    corpus (Conneau et al., [2017](#bib.bib8)) and subsequently adapted the feature
    extraction on the SemEval ASAG tasks (Saha et al., [2018](#bib.bib64)). The authors
    achieved state-of-the-art performance on the SemEval tasks (see Table LABEL:tab:DLresults),
    by computing the semantic similarity between word- and sentence-embeddings of
    the reference and student answers computed via the adapted models. In (Saha et al.,
    [2018](#bib.bib64)), hand-engineered features were also used for different question
    types. These results indicate that combining learned sequential-based and hand-engineered
    features increase the performance of ASAG methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Domain adaptation of pre-trained sequence-based models was also explored in (Zhang
    et al., [2019](#bib.bib83)). The authors trained an LSTM with focus on the incorporation
    of domain-specific knowledge and domain-general knowledge from Wikipedia. In (Saha
    et al., [2019](#bib.bib63)), a combination of domain adaptation and transfer learning
    techniques were used (see Figure [2](#S8.F2 "Figure 2 ‣ 8.2\. Sequence-based models
    ‣ 8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers")). The authors argued that the
    performance of systems that rely on textual similarity, paraphrasing, or entailment
    depends on the domain in which they are applied. They, thus, propose a deep learning
    architecture that deploys an encoder with a BiLSTM layer to embed the reference
    answer and the student answer into a vector representation, trained on questions
    and answers from multiple domains. To compute answer similarity, the authors propose
    to train a generic scoring model on answers from all considered domains, and specific
    scoring models trained on answers drawn from specific domains. The final prediction
    on the correctness of a test answer is computed by summing up the scores given
    by the generic and domain-specific scorers. This method achieved state-of-the-art
    results on the SemEval tasks, demonstrating that a combined training of a prediction
    model with multi-domain and domain-specific data contributes to an increase in
    the overall performance of an ASAG system. This indicates that the performance
    results of models for ASAG tasks are influenced by the capabilities of the underlying
    word-embedding models to effectively capture the semantic properties of words
    and sentences. Thus, the use of models that can integrate longer sequential relations
    in textual input proved to enrich the semantic information of the embeddings,
    and improved model performance. However, this is usually accompanied by an increase
    in the complexity of the models  (Sahu and Bhowmick, [2020](#bib.bib65); Zhang
    et al., [2020a](#bib.bib84)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b490251afa891ac453b16fa6b151000.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Architecture of the method published in (Saha et al., [2019](#bib.bib63)).
    The authors used domain adaptation by training different similarity scorers on
    domain-specific subsets of answers and on a domain-independent data set.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles of sequence-based models were also considered as a way to enhance
    the representation power of learned features (Zhang et al., [2020a](#bib.bib84);
    Sahu and Bhowmick, [2020](#bib.bib65)). Both works used an ensemble of eight different
    stacked regression models whose predictions were combined using a multi-layer
    perceptron. The authors included summaries of the responses in the training data,
    which contributed to learning more robust models.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3\. Attention-based models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More recent methods for ASAG explored more sophisticated feature representations,
    computed with attention-based and transformer models, to capture better and more
    descriptive semantic and structural characteristics from text. Attention enables
    the calculation of the relation and relative importance between each word within
    a sentence. The attention mechanism allows to model the dependencies of words
    and their importance for the prediction task at a longer range in sentences. The
    modeling does not take into account the sequentiality of words explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Architectures relying purely on attention mechanisms were introduced in (Vaswani
    et al., [2017](#bib.bib74)), and are called transformers. They consist of an encoder-decoder
    structure able to characterize long-range characteristics and dependencies in
    sequential data. In transformer architectures, the attention mechanism is modeled
    via multiple parallel attention-head components, each of them able to learn different
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Attention-based models differ in their general architecture. In (Riordan et al.,
    [2017](#bib.bib61)), LSTMs and Convolutional Neural Networks (CNNs) were combined
    with attention mechanisms, and they were demonstrated to outperform previous methods.
    In particular, bidirectional LSTMs augmented with attention obtained competitive
    results for ASAG problems (see Table LABEL:tab:DLresults). According to the authors,
    the choice of the input embeddings and the correct fine-tuning of the pre-trained
    models on the task at hand is crucial to obtain good results. Several further
    works focused on consistently applying transfer learning techniques, with the
    aim of fine-tuning the feature embedding space towards the domain of interest.
    For this purpose, more complex architectures, made of stacked or ensemble networks
    and attention modules were proposed (Camus and Filighera, [2020](#bib.bib5); Pribadi
    et al., [2017](#bib.bib55); Riordan et al., [2017](#bib.bib61)).
  prefs: []
  type: TYPE_NORMAL
- en: Other methods were based on the use of transformer models (Vaswani et al., [2017](#bib.bib74)).
    Their success and high-performance results are attributable to the high parallelization
    of the computations, which allows to train models on larger data sets, and the
    ability of modeling long-range dependencies. Further research work thus focused
    on exploring the use of transformers to compute text embeddings, and their effects
    on NLP and ASAG tasks  (Camus and Filighera, [2020](#bib.bib5)). In (Sung et al.,
    [2019a](#bib.bib67)), the Bidirectional Encoder Representation from Transformers
    (BERT) model was fine-tuned on an ASAG task, achieving state-of-the-art performance.
    The authors observed that a BERT model can be fine-tuned using only a few labeled
    examples on a target task and achieve very good performance, although it suffers
    from a lack of cross-domain generalization.
  prefs: []
  type: TYPE_NORMAL
- en: The sparse characteristics of data in benchmark data sets required the use of
    transfer learning and domain adaptation techniques also for fine-tuning transformer
    models and tailoring the learning process to the task at hand or its specific
    domain. In (Sung et al., [2019b](#bib.bib68)), the effect of different training
    approaches was investigated for ASAG. The authors showed that utilizing unstructured
    domain text data and question-answer pairs to fine-tune the models resulted in
    better results than using task-specific data only. The method proposed in (Sung
    et al., [2019b](#bib.bib68)) achieved higher results than all previous works on
    the SemEval 3-way task, although results on the SemEval 5-way task were not reported.
    In (Camus and Filighera, [2020](#bib.bib5)), the authors compared the performance
    of several state-of-the-art transformers fine-tuned using different learning strategies,
    such as cross-lingual and task-specific training, and noticed a consistent enhancement
    of the results on ASAG tasks.
  prefs: []
  type: TYPE_NORMAL
- en: <svg version="1.1" fill="none" height="366.818873668189" stroke="none" width="277.708592777086"
     overflow="visible"><g transform="translate(0,366.818873668189)
    scale(1,-1)"><g transform="translate(0,0)"><g transform="translate(0,281) scale(1,
    -1)"><foreignobject width="213" height="281" overflow="visible">![Refer to caption](img/806ed27881b731208902d5e76f3c78e1.png)</foreignobject></g></g><g
    transform="translate(83.33,280.56)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="9.4091600940916" height="7.7487200774872"
    overflow="visible">$h_{1}$</foreignobject></g></g><g transform="translate(138.89,280.56)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="15.220700152207"
    height="7.7487200774872" overflow="visible">$h_{n-1}$</foreignobject></g></g><g
    transform="translate(202.78,280.56)"><g transform="translate(0,9.1324200913242)
    scale(1, -1)"><foreignobject width="10.1010101010101" height="7.7487200774872"
    overflow="visible">$h_{n}$</foreignobject></g></g><g transform="translate(113.89,231.94)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="9.4091600940916"
    height="7.7487200774872" overflow="visible">$h_{2}$</foreignobject></g></g><g
    transform="translate(166.67,231.94)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="15.220700152207" height="7.7487200774872"
    overflow="visible">$h_{n-1}$</foreignobject></g></g><g transform="translate(231.94,231.94)"><g
    transform="translate(0,9.1324200913242) scale(1, -1)"><foreignobject width="10.1010101010101"
    height="7.7487200774872" overflow="visible">$h_{n}$</foreignobject></g></g><g
    transform="translate(83.33,186.11)"><g transform="translate(0,9.685900096859)
    scale(1, -1)"><foreignobject width="9.4091600940916" height="7.7487200774872"
    overflow="visible">$h_{2}$</foreignobject></g></g><g transform="translate(138.89,186.11)"><g
    transform="translate(0,9.685900096859) scale(1, -1)"><foreignobject width="15.220700152207"
    height="7.7487200774872" overflow="visible">$h_{n-1}$</foreignobject></g></g><g
    transform="translate(202.78,186.11)"><g transform="translate(0,9.1324200913242)
    scale(1, -1)"><foreignobject width="10.1010101010101" height="7.7487200774872"
    overflow="visible">$h_{n}$</foreignobject></g></g><g transform="translate(22.22,347.22)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">output</text></g><g
    transform="translate(27.78,336.11)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">layer</text></g><g transform="translate(148.06,312.5)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">+</text></g><g
    transform="translate(22.22,308.33)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">attention</text></g><g transform="translate(33.33,297.22)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">layer</text></g><g
    transform="translate(33.33,241.67)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">BiLSTM</text></g><g transform="translate(22.22,230.56)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">hidden layer</text></g><g
    transform="translate(33.33,141.67)"><text x="0" y="0" transform="scale(1, -1)"
    fill="black" font-size="80%">sentence</text></g><g transform="translate(19.44,130.56)"><text
    x="0" y="0" transform="scale(1, -1)" fill="black" font-size="80%">representation</text></g><g
    transform="translate(27.78,37.5)"><text x="0" y="0" transform="scale(1, -1)" fill="black"
    font-size="80%">input layer</text></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. Sketch of the model architecture proposed in (Qi et al., [2019](#bib.bib56)).
    The authors proposed a combined use of a CNN and a BiLSTM, together with an attention
    layer, which provides a refinement of the prediction before the output layer is
    computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since attention is instrumental for the success of transformer models (Conneau
    et al., [2017](#bib.bib8); Peters et al., [2018](#bib.bib52); Radford, [2018](#bib.bib57);
    Howard and Ruder, [2018](#bib.bib27); Devlin et al., [2019](#bib.bib10); Yang
    et al., [2019](#bib.bib80)) and their application to ASAG, researchers focused
    on how it can be used optimally in a model. In (Qi et al., [2019](#bib.bib56)),
    for instance, an attention layer was combined with a BiLSTM and a CNN architecture
    to compute question-answer representations. The structure of the model architecture
    is illustrated in Figure [3](#S8.F3 "Figure 3 ‣ 8.3\. Attention-based models ‣
    8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers"). Such embeddings were concatenated
    and used for scoring the student answers. With this approach, the authors leveraged
    textual semantic features and long-term dependencies in text and obtained good
    performance results. Furthermore, in (Wang et al., [2019](#bib.bib75)), it was
    shown that attention can be used to extract key elements from the student answers.
    The authors demonstrated that the performance could be improved by considering
    a larger observation window on the text rather than using only word-level attention.
    Combinations of different attention mechanisms were explored in (Liu et al., [2019a](#bib.bib37)).
    As illustrated in Figure [4](#S8.F4 "Figure 4 ‣ 8.3\. Attention-based models ‣
    8\. Deep learning methods ‣ Survey on Automated Short Answer Grading with Deep
    Learning: from Word Embeddings to Transformers"), the model calculated separate
    representations for the student answer, the reference answer, and their cross-attention.
    These different representations are then aggregated position-wise and used as
    input representations for a basic transformer model. Their approach achieved overall
    good results on the Real World K-12 data set (see Table LABEL:tab:DLresults).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/897ec615f2a0296e6949a305dce137b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Sketch of the model architecture proposed in (Liu et al., [2019a](#bib.bib37)).
    The authors combined different representations of the student and reference answers.
    Subsequently, these representations are aggregated at the prediction stage.
  prefs: []
  type: TYPE_NORMAL
- en: The use of attention-based models contributed to a substantial increase in performance
    on ASAG tasks, which is attributable to the more powerful representations learned
    using these models. The analysis and comparison of longer text data, however,
    remains a challenging aspect of ASAG systems. We discuss the challenges and outlook
    of further developments in this field in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field of ASAG has witnessed a transition from methodologies based on careful
    design of hand-engineered text features towards feature learning architectures
    based on deep learning. This evolution has been strongly influenced by the progress
    made in the field of NLP. Transfer learning from very big models trained on generic
    NLP tasks to target tasks has become a common practice. This, however, has benefits
    but also creates challenges in the field of ASAG.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings, sequential models, and attention-based and transformers models
    have progressively contributed to improving the semantic richness and descriptive
    power of textual representation. This resulted in impressive progress in the field
    of NLP (Sahu and Bhowmick, [2020](#bib.bib65)), mainly due to the ability to exploit
    the relations and the diversity of structures of vast amounts of data. The performance
    of ASAG methods has, however, not witnessed the same level of improvement. This
    is mainly attributable to the fact that the ASAG benchmark datasets are sparse,
    and very complex models developed for general NLP tasks do not generalize well
    to this application field. Although transfer-learning approaches are shown to
    be promising (Sung et al., [2019a](#bib.bib67)), fundamental aspects of ASAG like
    sparsity and domain differences need to be addressed explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers and attention-based models have achieved astonishing results in
    NLP (Liu et al., [2019c](#bib.bib39)). For ASAG tasks, however, a fine-tuned transformer
    model alone does not achieve the highest performance on benchmark datasets. We
    hypothesize that these complex embeddings models are not able to effectively disentangle
    the semantically rich information contained in short answers. The best-performing
    methods, indeed, combine complex embeddings representation computed by transformers
    with sets of hand-crafted features that are specifically designed to address particular
    aspects and characteristics of short answers. Currently, the research community
    of NLP and ASAG is investigating ways to optimize the embeddings of paragraphs
    of medium size which best represent the semantic information contained in the
    student answers. Particular attention is given to: a) combination of different
    features to exploit their ability to b) using multi-classifier systems (e.g. stacked
    architectures, ensembles, and hybrid models) to exploit eventual complementary
    modeling and predictive capabilities, and c) optimizing the training process to
    influence what semantic information the model focuses more on (e.g. different
    domain adaptation strategies, general or task-focused pre-training).'
  prefs: []
  type: TYPE_NORMAL
- en: 9.1\. Open challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have recognized a number of challenges that ASAG methods have to deal with,
    particularly because of the peculiar characteristics of the task at hand. In the
    following paragraphs, we discuss them.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic understanding.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Existing methods have difficulties to model the rich semantic content of short
    answers effectively. This is due to the fact that, usually, short answers are
    written in such a way that a lot of information is provided in few sentences and
    in a very concise way. Therefore, ASAG systems require methods for better and
    deeper contextual and semantic understanding of the text, for which the application
    of Natural Language Understanding (NLU) techniques (Navigli, [2018](#bib.bib47))
    should be investigated.
  prefs: []
  type: TYPE_NORMAL
- en: Linguistic variations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The analysis of student answers presents several challenges related to the way
    sentences are formulated. A concept or intention can be expressed in different
    ways, such as with different words or with the construction of sentences (Roy
    et al., [2016](#bib.bib62)). Furthermore, answers may lack a complete sentential
    form or be deliberately written to trick the automatic evaluation systems. The
    grading of non-sentential answers is challenging because they might not be in
    conformance with the structural pattern of the reference answer or grammatically
    incorrect (Saha et al., [2018](#bib.bib64)). Grading systems need to be able to
    take into account these cases and interpret or infer complete answers from fragments.
    In addition, most of the existing methods are not able to distinguish between
    nonsense and relevant answers. Due to the intended automated nature of the system,
    it is crucial to detect if a student is trying to trick the system by answering
    the question with consecutive incoherent keywords, instead of formulating a complete
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: Details of questions and reference answers.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training ASAG models rely on the correctness and reliability of the reference
    answers and labels. Some reference answers used to train the models might be too
    brief, not containing enough details, or even not completely meaningful. This
    creates issues for automated grading algorithms, which have to be trained to be
    robust to data uncertainty. Likewise, ASAG models should also be able to handle
    different question types and answer expectations. For instance, some questions
    may require providing a definition whereas other questions expect a more complex
    and detailed answer (Roy et al., [2016](#bib.bib62)). Especially open-ended questions
    are challenging since they expect the students to express their own thoughts based
    on facts. Hence, a reference answer might not be suitable because correct answers
    may vary a lot (Zhang et al., [2019](#bib.bib83)).
  prefs: []
  type: TYPE_NORMAL
- en: Generalization across domains and answers.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ASAG systems are required to maintain consistent levels of performance when
    evaluating the answers to questions from different domains (Saha et al., [2018](#bib.bib64)).
    This is not a straightforward task to be evaluated, as existing data sets are
    biased towards certain domains, or reference answers with higher scores (Wang
    et al., [2018](#bib.bib76)). The sparsity of available benchmark data sets thus
    poses challenges to effectively training models with robust generalization abilities.
    Furthermore, for a given question, multiple reference answers can be correct,
    meaning that there is no absolute gold standard to compare the student answers
    with (Roy et al., [2016](#bib.bib62)). Training models for ASAG needs to take
    into account the sparse characteristics of the available data and possible noisy
    labels, in order to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2\. Outlook and future work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning approaches have been beneficial for the improvement of the performance
    of the ASAG system and have been demonstrated to effectively complement the text
    representation capabilities of methods based on hand-engineered features. To further
    improve the possibilities and extent of application of deep learning methods for
    grading of short answers, necessary steps have to be taken that address the specific
    problems and challenges presented above.
  prefs: []
  type: TYPE_NORMAL
- en: The available benchmark data sets are rather sparse and are not representative
    enough of the variability of questions and short reference answers in different
    domains. This hinders the generalization capabilities of learning-based methods,
    which are thus subject to overfitting. This makes tasks like the extension of
    existing data sets, data augmentation (Wei and Zou, [2019](#bib.bib79)), and the
    creation of synthetic data via generative models (Tevet et al., [2019](#bib.bib71))
    very relevant to promote future progress in the field.
  prefs: []
  type: TYPE_NORMAL
- en: The explainability (Guidotti et al., [2018](#bib.bib23)) and robustness (Wang
    et al., [2021](#bib.bib78)) of deep learning-based models are also important aspects
    that require further research. These can be indeed instrumental characteristics
    to assess whether a model is capable of robust decision making and can be applied
    successfully to grade answers in different topic domains. Furthermore, being able
    to consistently explain the decisions taken by a certain model also supports the
    analysis of what words or semantic constructs the score is determined by. Performance
    analysis and model inspection are thus complementary aspects that should be jointly
    taken into account for the evaluation of the developed methods, instead of only
    focusing on improving benchmark results. Promising research areas concern the
    identification of the points of strength of existing methods, to determine sets
    of methods that are suitable to be used for specific tasks and subsequently design
    and test ensemble methods for multi-domain cases. However, in order to achieve
    this goal, techniques to explain the model predictions should be put in place.
    These aspects are strictly linked with the need of characterizing the robustness
    of the prediction made by the trained models to variations of the test answers (Wang
    et al., [2020](#bib.bib77)). A research objective that needs to be taken into
    account is to study how sensitive ASAG models are to changes in the answers of
    students, in order to avoid that they can be tricked by using certain keywords
    or by swapping the order of words. This has to be coupled with a more extensive
    use of NLU techniques, to better evaluate the content and the semantics of the
    given answers (Namazifar et al., [2021](#bib.bib46)). This approach can also be
    used to identify weaknesses of the models and take appropriate countermeasures.
    This will be also beneficial to examine and control the relevance of spelling
    errors or meaningless sentences made of specific keywords to the computation of
    the answer score.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We reviewed the recent progress in the field of Automated Short Answer Grading
    (ASAG) and provided an overview of the advancements made and results achieved
    using deep learning techniques. We added to previous literature analyses by identifying
    the key features and architectural choices that impacted the performance of ASAG
    systems in the era of Deep Learning. We linked the methodological improvements
    to the results that recent methods achieved on benchmark data sets.
  prefs: []
  type: TYPE_NORMAL
- en: This survey provides a taxonomy of methods, from classical Machine Learning
    to Deep Learning approaches, and research trends and outlook. Deep Learning architectures
    for natural language processing, adapted to ASAG tasks by means of transfer learning
    and domain adaptation techniques, are not sufficient to deal with the challenges
    and requirements that this field presents. Deep learning approaches alone show
    difficulty in effectively catch the semantics of short answers for consistent
    comparison with reference answers. To compensate for this, an ensemble of classifiers,
    stacked models, and especially hybrid models that combine feature engineering
    with deep representation learning were developed. The embedding capabilities of
    Deep Learning models and the attention-based analysis of Transformers have been
    shown to be complementary to previously developed lexical, syntactic, and semantic
    features, to strengthen the performance of ASAG systems. However, to stimulate
    further developments, a uniform basis for benchmarking methods is necessary, e.g.
    designing a comprehensive benchmark data set.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blessing et al. (2021) Guembe Blessing, Ambrose Azeta, Sanjay Misra, Felix
    Chigozie, and Ravin Ahuja. 2021. A Machine Learning Prediction of Automatic Text
    Based Assessment for Open and Distance Learning: A Review. In *Innovations in
    Bio-Inspired Computing and Applications*. Springer International Publishing, Cham,
    369–380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burrows et al. (2015) Steven Burrows, Iryna Gurevych, and Benno Stein. 2015.
    The Eras and Trends of Automatic Short Answer Grading. *International Journal
    of Artificial Intelligence in Education* 25, 1 (01 Mar 2015), 60–117. [https://doi.org/10.1007/s40593-014-0026-8](https://doi.org/10.1007/s40593-014-0026-8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai (2019) Changzhi Cai. 2019. Automatic Essay Scoring with Recurrent Neural
    Network. In *Proceedings of the 3rd International Conference on High Performance
    Compilation, Computing and Communications*. Association for Computing Machinery,
    New York, NY, USA, 1–7. [https://doi.org/10.1145/3318265.3318296](https://doi.org/10.1145/3318265.3318296)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Camus and Filighera (2020) Leon Camus and Anna Filighera. 2020. Investigating
    Transformers for Automatic Short Answer Grading. In *Artificial Intelligence in
    Education*, Ig Ibert Bittencourt, Mutlu Cukurova, Kasia Muldner, Rose Luckin,
    and Eva Millán (Eds.). Springer International Publishing, Cham, 43–48.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2016) Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long
    Short-Term Memory-Networks for Machine Reading. In *Proceedings of the 2016 Conference
    on Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics, Austin, Texas, 551–561. [https://doi.org/10.18653/v1/D16-1053](https://doi.org/10.18653/v1/D16-1053)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Condor (2020) Aubrey Condor. 2020. Exploring Automatic Short Answer Grading
    as a Tool to Assist in Human Rating. *Artificial Intelligence in Education* 12164
    (2020), 74 – 79.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conneau et al. (2017) Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault,
    and Antoine Bordes. 2017. Supervised Learning of Universal Sentence Representations
    from Natural Language Inference Data. In *Proceedings of the 2017 Conference on
    Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics, Copenhagen, Denmark, 670–680. [https://doi.org/10.18653/v1/D17-1070](https://doi.org/10.18653/v1/D17-1070)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Mulder et al. (2015) Wim De Mulder, Steven Bethard, and Marie-Francine Moens.
    2015. A survey on the application of recurrent neural networks to statistical
    language modeling. *Computer Speech & Language* 30, 1 (2015), 61–98. [https://doi.org/10.1016/j.csl.2014.09.005](https://doi.org/10.1016/j.csl.2014.09.005)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association
    for Computational Linguistics, 4171–4186. [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhamecha et al. (2018) Tejas I. Dhamecha, Smit Marvaniya, Swarnadeep Saha, Renuka
    Sindhgatta, and Bikram Sengupta. 2018. Balancing Human Efforts and Performance
    of Student Response Analyzer in Dialog-Based Tutors. In *Artificial Intelligence
    in Education*, Carolyn Penstein Rosé, Roberto Martínez-Maldonado, H. Ulrich Hoppe,
    Rose Luckin, Manolis Mavrikis, Kaska Porayska-Pomsta, Bruce McLaren, and Benedict
    du Boulay (Eds.). Springer International Publishing, Cham, 70–85.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dyer et al. (2016) Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A.
    Smith. 2016. Recurrent Neural Network Grammars. In *Proceedings of the 2016 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*. Association for Computational Linguistics, San Diego,
    California, 199–209. [https://doi.org/10.18653/v1/N16-1024](https://doi.org/10.18653/v1/N16-1024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dzikovska et al. (2013) Myroslava Dzikovska, Rodney Nielsen, Chris Brew, Claudia
    Leacock, Danilo Giampiccolo, Luisa Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang
    Dang. 2013. SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing
    Textual Entailment Challenge. In *Second Joint Conference on Lexical and Computational
    Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop
    on Semantic Evaluation (SemEval 2013)*. Association for Computational Linguistics,
    Atlanta, Georgia, USA, 263–274. [https://aclanthology.org/S13-2045](https://aclanthology.org/S13-2045)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dzikovska et al. (2012) Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
    Brew. 2012. Towards Effective Tutorial Feedback for Explanation Questions: A Dataset
    and Baselines. In *Proceedings of the 2012 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*.
    Association for Computational Linguistics, Montréal, Canada, 200–210. [https://aclanthology.org/N12-1021](https://aclanthology.org/N12-1021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fellbaum (2000) C. Fellbaum. 2000. WordNet : an electronic lexical database.
    *Language* 76 (2000), 706.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gabrilovich and Markovitch (2007a) Evgeniy Gabrilovich and Shaul Markovitch.
    2007a. Computing Semantic Relatedness Using Wikipedia-Based Explicit Semantic
    Analysis. In *Proceedings of the 20th International Joint Conference on Artifical
    Intelligence* *(IJCAI’07)*. Morgan Kaufmann Publishers Inc., San Francisco, CA,
    USA, 1606–1611.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gabrilovich and Markovitch (2007b) Evgeniy Gabrilovich and Shaul Markovitch.
    2007b. Computing Semantic Relatedness Using Wikipedia-Based Explicit Semantic
    Analysis. In *Proceedings of the 20th International Joint Conference on Artifical
    Intelligence* *(IJCAI’07)*. Morgan Kaufmann Publishers Inc., San Francisco, CA,
    USA, 1606–1611.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaddipati et al. (2020) Sasi Kiran Gaddipati, Deebul Nair, and P. Plöger. 2020.
    Comparative Evaluation of Pretrained Transfer Learning Models on Automatic Short
    Answer Grading. *ArXiv* abs/2009.01303 (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Galassi et al. (2020) Andrea Galassi, Marco Lippi, and Paolo Torroni. 2020.
    Attention in Natural Language Processing. *IEEE Transactions on Neural Networks
    and Learning Systems* (2020), 1–18. [https://doi.org/10.1109/TNNLS.2020.3019893](https://doi.org/10.1109/TNNLS.2020.3019893)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Galhardi and Brancher (2018) Lucas Busatta Galhardi and Jacques Duílio Brancher.
    2018. Machine Learning Approach for Automatic Short Answer Grading: A Systematic
    Review. In *Advances in Artificial Intelligence - IBERAMIA 2018*, Guillermo R.
    Simari, Eduardo Fermé, Flabio Gutiérrez Segura, and José Antonio Rodríguez Melquiades
    (Eds.). Springer International Publishing, Cham, 380–391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Galhardi et al. (2018) Lucas B. Galhardi, Helen Senefonte, Rodrigo de Souza,
    and Jacques Brancher. 2018. Exploring Distinct Features for Automatic Short Answer
    Grading. In *Anais do XV Encontro Nacional de Inteligência Artificial e Computacional*.
    SBC, Porto Alegre, RS, Brasil, 1–12. [https://doi.org/10.5753/eniac.2018.4399](https://doi.org/10.5753/eniac.2018.4399)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gomaa and Fahmy (2020) Wael Hassan Gomaa and Aly Aly Fahmy. 2020. Ans2vec:
    A Scoring System for Short Answers. In *The International Conference on Advanced
    Machine Learning Technologies and Applications (AMLTA2019)*, Aboul Ella Hassanien,
    Ahmad Taher Azar, Tarek Gaber, Roheet Bhatnagar, and Mohamed F. Tolba (Eds.).
    Springer International Publishing, Cham, 586–595.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidotti et al. (2018) Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri,
    Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A Survey of Methods
    for Explaining Black Box Models. *ACM Comput. Surv.* 51, 5, Article 93 (2018),
    42 pages. [https://doi.org/10.1145/3236009](https://doi.org/10.1145/3236009)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heilman and Madnani (2012) Michael Heilman and Nitin Madnani. 2012. ETS: Discriminative
    Edit Models for Paraphrase Scoring. In **SEM 2012: The First Joint Conference
    on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference
    and the shared task, and Volume 2: Proceedings of the Sixth International Workshop
    on Semantic Evaluation (SemEval 2012)*. Association for Computational Linguistics,
    Montréal, Canada, 529–535. [https://aclanthology.org/S12-1076](https://aclanthology.org/S12-1076)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heilman and Madnani (2013) Michael Heilman and Nitin Madnani. 2013. ETS: Domain
    Adaptation and Stacking for Short Answer Scoring. In *Second Joint Conference
    on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh
    International Workshop on Semantic Evaluation (SemEval 2013)*. Association for
    Computational Linguistics, Atlanta, Georgia, USA, 275–279. [https://aclanthology.org/S13-2046](https://aclanthology.org/S13-2046)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hightower (2020) Mallory Hightower. 2020. High-Level History of NLP Models.
    [https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7](https://towardsdatascience.com/high-level-history-of-nlp-models-bc8c8b142ef7)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. Universal
    Language Model Fine-tuning for Text Classification. In *Proceedings of the 56th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*. Association for Computational Linguistics, Melbourne, Australia, 328–339.
    [https://doi.org/10.18653/v1/P18-1031](https://doi.org/10.18653/v1/P18-1031)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jimenez et al. (2013) Sergio Jimenez, Claudia Becerra, and Alexander Gelbukh.
    2013. SOFTCARDINALITY: Hierarchical Text Overlap for Student Response Analysis.
    In *Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume
    2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval
    2013)*. Association for Computational Linguistics, Atlanta, Georgia, USA, 280–284.
    [https://aclanthology.org/S13-2047](https://aclanthology.org/S13-2047)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaggle ([n. d.]) Kaggle. [n. d.]. The Hewlett Foundation: Short Answer Scoring.
    [https://www.kaggle.com/c/asap-sas/](https://www.kaggle.com/c/asap-sas/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaur and Hornof (2005) Ishwinder Kaur and Anthony J. Hornof. 2005. *A Comparison
    of LSA, WordNet and PMI-IR for Predicting User Click Behavior*. Association for
    Computing Machinery, New York, NY, USA, 51–60. [https://doi.org/10.1145/1054972.1054980](https://doi.org/10.1145/1054972.1054980)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kouylekov et al. (2013) Milen Kouylekov, Luca Dini, Alessio Bosca, and Marco
    Trevisan. 2013. Celi: EDITS and Generic Text Pair Classification. In *Second Joint
    Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings
    of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)*.
    Association for Computational Linguistics, Atlanta, Georgia, USA, 592–597. [https://aclanthology.org/S13-2099](https://aclanthology.org/S13-2099)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2017) Sachin Kumar, Soumen Chakrabarti, and Shourya Roy. 2017.
    Earth Mover’s Distance Pooling over Siamese LSTMs for Automatic Short Answer Grading.
    In *Proceedings of the Twenty-Sixth International Joint Conference on Artificial
    Intelligence, IJCAI-17*. 2046–2052. [https://doi.org/10.24963/ijcai.2017/284](https://doi.org/10.24963/ijcai.2017/284)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2019) Yaman Kumar, Swati Aggarwal, Debanjan Mahata, Rajiv Ratn
    Shah, Ponnurangam Kumaraguru, and Roger Zimmermann. 2019. Get IT Scored Using
    AutoSAS - An Automated System for Scoring Short Answers. In *AAAI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
    Learning of Language Representations. (2020). [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Landauer et al. (1998) Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
    1998. An introduction to latent semantic analysis. *Discourse Processes* 25, 2-3
    (1998), 259–284. [https://doi.org/10.1080/01638539809545028](https://doi.org/10.1080/01638539809545028)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levy et al. (2013) Omer Levy, Torsten Zesch, Ido Dagan, and Iryna Gurevych.
    2013. In *Second Joint Conference on Lexical and Computational Semantics (*SEM),
    Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation
    (SemEval 2013)*. Association for Computational Linguistics, Atlanta, Georgia,
    USA, 285–289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019a) Tianqiao Liu, Wenbiao Ding, Zhiwei Wang, Jiliang Tang, Gale Yan
    Huang, and Zitao Liu. 2019a. Automatic Short Answer Grading via Multiway Attention
    Networks. *ArXiv* abs/1909.10166 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019b) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    2019b. Improving Multi-Task Deep Neural Networks via Knowledge Distillation for
    Natural Language Understanding. *ArXiv* abs/1904.09482 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019c) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019c.
    RoBERTa: A Robustly Optimized BERT Pretraining Approach. (2019). [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)
    cite arxiv:1907.11692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magooda et al. (2016) Ahmed Ezzat Magooda, Mohamed A. Zahran, Mohsen A. Rashwan,
    Hazem M. Raafat, and Magda B. Fayek. 2016. Vector Based Techniques for Short Answer
    Grading. In *Proceedings of the Twenty-Ninth International Florida Artificial
    Intelligence Research Society Conference, FLAIRS 2016, Key Largo, Florida, USA,
    May 16-18, 2016*, Zdravko Markov and Ingrid Russell (Eds.). AAAI Press, 238–243.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meurers et al. (2011) Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey M Bailey.
    2011. Integrating parallel analysis modules to evaluate the meaning of answers
    to reading comprehension questions. *International journal of continuing engineering
    education and life-long learning* 21 (2011), 355–369.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, G. Corrado, and J. Dean. 2013.
    Efficient Estimation of Word Representations in Vector Space. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miller (1995) George A. Miller. 1995. WordNet: A Lexical Database for English.
    *Commun. ACM* 38, 11 (Nov. 1995), 39–41. [https://doi.org/10.1145/219717.219748](https://doi.org/10.1145/219717.219748)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mohler et al. (2011) Michael Mohler, Razvan Bunescu, and Rada Mihalcea. 2011.
    Learning to Grade Short Answer Questions using Semantic Similarity Measures and
    Dependency Graph Alignments. In *Proceedings of the 49th Annual Meeting of the
    Association for Computational Linguistics: Human Language Technologies*. Association
    for Computational Linguistics, Portland, Oregon, USA, 752–762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohler and Mihalcea (2009) Michael Mohler and Rada Mihalcea. 2009. Text-to-Text
    Semantic Similarity for Automatic Short Answer Grading. In *Proceedings of the
    12th Conference of the European Chapter of the ACL (EACL 2009)*. Association for
    Computational Linguistics, Athens, Greece, 567–575.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Namazifar et al. (2021) Mahdi Namazifar, Alexandros Papangelis, Gokhan Tur,
    and Dilek Hakkani-Tür. 2021. Language Model is all You Need: Natural Language
    Understanding as Question Answering. In *ICASSP 2021 - 2021 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*. 7803–7807. [https://doi.org/10.1109/ICASSP39728.2021.9413810](https://doi.org/10.1109/ICASSP39728.2021.9413810)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigli (2018) Roberto Navigli. 2018. Natural Language Understanding: Instructions
    for (Present and Future) Use. In *Proceedings of the 27th International Joint
    Conference on Artificial Intelligence* *(IJCAI’18)*. AAAI Press, 5697–5702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neterer and Guzide (2018) Jacob Russell Neterer and Osman Guzide. 2018. Deep
    Learning in Natural Language Processing. *Proceedings of the West Virginia Academy
    of Science* 90, 1. [https://pwvas.org/index.php/pwvas/article/view/339](https://pwvas.org/index.php/pwvas/article/view/339)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ott et al. (2013) Niels Ott, Ramon Ziai, Michael Hahn, and Detmar Meurers.
    2013. CoMeT: Integrating different levels of linguistic modeling for meaning assessment.
    In *Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume
    2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval
    2013)*. Association for Computational Linguistics, Atlanta, Georgia, USA, 608–616.
    [https://aclanthology.org/S13-2102](https://aclanthology.org/S13-2102)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otter et al. (2021) Daniel W. Otter, Julian R. Medina, and Jugal K. Kalita.
    2021. A Survey of the Usages of Deep Learning for Natural Language Processing.
    *IEEE Transactions on Neural Networks and Learning Systems* 32, 2 (2021), 604–624.
    [https://doi.org/10.1109/TNNLS.2020.2979670](https://doi.org/10.1109/TNNLS.2020.2979670)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Jeffrey Pennington, R. Socher, and Christopher D.
    Manning. 2014. Glove: Global Vectors for Word Representation. In *EMNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized
    Word Representations. (June 2018), 2227–2237. [https://doi.org/10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popović (2011) Maja Popović. 2011. Morphemes and POS tags for n-gram based evaluation
    metrics. In *Proceedings of the Sixth Workshop on Statistical Machine Translation*.
    Association for Computational Linguistics, Edinburgh, Scotland, 104–107. [https://aclanthology.org/W11-2110](https://aclanthology.org/W11-2110)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pribadi et al. (2016) Feddy Setio Pribadi, Teguh Bharata Adji, and Adhistya Erna
    Permanasari. 2016. Automated Short Answer Scoring using Weighted Cosine Coefficient.
    (2016), 70–74. [https://doi.org/10.1109/IC3e.2016.8009042](https://doi.org/10.1109/IC3e.2016.8009042)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pribadi et al. (2017) Feddy Setio Pribadi, Teguh Bharata Adji, Adhistya Erna
    Permanasari, Anggraini Mulwinda, and Aryo Baskoro Utomo. 2017. Automatic short
    answer scoring using words overlapping methods. *AIP Conference Proceedings* 1818,
    1, 020042. [https://doi.org/10.1063/1.4976906](https://doi.org/10.1063/1.4976906)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2019) Hui Qi, Yue Wang, Jinyu Dai, Jinqing Li, and Xiaoqiang Di.
    2019. Attention-Based Hybrid Model for Automatic Short Answer Scoring. In *Simulation
    Tools and Techniques*, Houbing Song and Dingde Jiang (Eds.). Springer International
    Publishing, Cham, 385–394.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford (2018) Alec Radford. 2018. Improving Language Understanding by Generative
    Pre-Training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *ArXiv*
    abs/1910.10683 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramachandran et al. (2015) Lakshmi Ramachandran, Jian Cheng, and Peter Foltz.
    2015. Identifying Patterns For Short Answer Scoring Using Graph-based Lexico-Semantic
    Text Matching. In *Proceedings of the Tenth Workshop on Innovative Use of NLP
    for Building Educational Applications*. Association for Computational Linguistics,
    Denver, Colorado, 97–106. [https://doi.org/10.3115/v1/W15-0612](https://doi.org/10.3115/v1/W15-0612)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ratner et al. (2019) Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic
    Sala, Shreyash Pandey, and Christopher Ré. 2019. Training Complex Models with
    Multi-Task Weak Supervision. *Proceedings of the … AAAI Conference on Artificial
    Intelligence. AAAI Conference on Artificial Intelligence* 33 (2019), 4763–4771.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riordan et al. (2017) Brian Riordan, Andrea Horbach, Aoife Cahill, Torsten Zesch,
    and Chong Min Lee. 2017. Investigating neural architectures for short answer scoring.
    In *BEA@EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy et al. (2016) Shourya Roy, Himanshu S. Bhatt, and Y. Narahari. 2016. An
    Iterative Transfer Learning Based Ensemble Technique for Automatic Short Answer
    Grading. *ArXiv* abs/1609.04909 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saha et al. (2019) Swarnadeep Saha, Tejas I. Dhamecha, Smit Marvaniya, Peter
    Foltz, Renuka Sindhgatta, and Bikram Sengupta. 2019. Joint Multi-Domain Learning
    for Automatic Short Answer Grading. *ArXiv* abs/1902.09183 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saha et al. (2018) Swarnadeep Saha, Tejas I. Dhamecha, Smit Marvaniya, Renuka
    Sindhgatta, and Bikram Sengupta. 2018. Sentence Level or Token Level Features
    for Automatic Short Answer Grading?: Use Both. In *Artificial Intelligence in
    Education*, Carolyn Penstein Rosé, Roberto Martínez-Maldonado, H. Ulrich Hoppe,
    Rose Luckin, Manolis Mavrikis, Kaska Porayska-Pomsta, Bruce McLaren, and Benedict
    du Boulay (Eds.). Springer International Publishing, Cham, 503–517.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sahu and Bhowmick (2020) A. Sahu and P. K. Bhowmick. 2020. Feature Engineering
    and Ensemble-Based Approach for Improving Automatic Short-Answer Grading Performance.
    *IEEE Transactions on Learning Technologies* 13 (2020), 77–90.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sultan et al. (2016) Md Arafat Sultan, Cristobal Salazar, and Tamara Sumner.
    2016. Fast and Easy Short Answer Grading with High Accuracy. In *Proceedings of
    the 2016 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*. Association for Computational Linguistics,
    San Diego, California, 1070–1075. [https://doi.org/10.18653/v1/N16-1123](https://doi.org/10.18653/v1/N16-1123)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sung et al. (2019a) Chul Sung, Tejas Indulal Dhamecha, and Nirmal Mukhi. 2019a.
    Improving Short Answer Grading Using Transformer-Based Pre-training. In *Artificial
    Intelligence in Education*, Seiji Isotani, Eva Millán, Amy Ogan, Peter Hastings,
    Bruce McLaren, and Rose Luckin (Eds.). Springer International Publishing, Cham,
    469–481.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sung et al. (2019b) Chul Sung, Tejas I. Dhamecha, Swarnadeep Saha, Tengfei Ma,
    V. Pulla Reddy, and Rishi Arora. 2019b. Pre-Training BERT on Domain Resources
    for Short Answer Grading. In *EMNLP/IJCNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
    Sequence to Sequence Learning with Neural Networks. In *Proceedings of the 27th
    International Conference on Neural Information Processing Systems - Volume 2*
    *(NIPS’14)*. MIT Press, Cambridge, MA, USA, 3104–3112.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzen et al. (2018) Neslihan Suzen, Alexander N. Gorban, Jeremy Levesley, and
    Eugenij Moiseevich Mirkes. 2018. Automatic Short Answer Grading and Feedback Using
    Text Mining Methods. *ArXiv* abs/1807.10543 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tevet et al. (2019) Guy Tevet, Gavriel Habib, Vered Shwartz, and Jonathan Berant.
    2019. Evaluating Text GANs as Language Models. In *Proceedings of the 2019 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Volume 1 (Long and Short Papers)*. Association for
    Computational Linguistics, Minneapolis, Minnesota, 2241–2247. [https://doi.org/10.18653/v1/N19-1233](https://doi.org/10.18653/v1/N19-1233)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsiakmaki et al. (2020) Maria Tsiakmaki, Georgios Kostopoulos, Sotiris Kotsiantis,
    and Omiros Ragos. 2020. Transfer Learning from Deep Neural Networks for Predicting
    Student Performance. *Applied Sciences* 10, 6 (2020). [https://doi.org/10.3390/app10062145](https://doi.org/10.3390/app10062145)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van der Lee and van den Bosch (2017) Chris van der Lee and Antal van den Bosch.
    2017. Exploring Lexical and Syntactic Features for Language Variety Identification.
    In *Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
    and Dialects (VarDial)*. Association for Computational Linguistics, Valencia,
    Spain, 190–199. [https://doi.org/10.18653/v1/W17-1224](https://doi.org/10.18653/v1/W17-1224)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is All you Need. 30 (2017). [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Tianqi Wang, Naoya Inoue, Hiroki Ouchi, Tomoya Mizumoto,
    and Kentaro Inui. 2019. Inject Rubrics into Short Answer Grading System. In *Proceedings
    of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)*.
    Association for Computational Linguistics, Hong Kong, China, 175–182. [https://doi.org/10.18653/v1/D19-6119](https://doi.org/10.18653/v1/D19-6119)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Tianqi Wang, Tomoya Mizumoto, Naoya Inoue, and Kentaro Inui.
    2018. Identifying Current Issues in Short Answer Grading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li,
    Jilin Chen, Alex Beutel, and Ed Chi. 2020. CAT-Gen: Improving Robustness in NLP
    Models via Controlled Adversarial Text Generation. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP)*. Association
    for Computational Linguistics, Online, 5141–5146. [https://doi.org/10.18653/v1/2020.emnlp-main.417](https://doi.org/10.18653/v1/2020.emnlp-main.417)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, and Aoshuang
    Ye. 2021. Towards a Robust Deep Neural Network in Texts: A Survey. arXiv:cs.CL/1902.07285'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei and Zou (2019) Jason Wei and Kai Zou. 2019. EDA: Easy Data Augmentation
    Techniques for Boosting Performance on Text Classification Tasks. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*.
    Hong Kong, China, 6382–6388. [https://doi.org/10.18653/v1/D19-1670](https://doi.org/10.18653/v1/D19-1670)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining
    for Language Understanding. In *Advances in Neural Information Processing Systems*,
    H. Wallach, H. Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox, and R. Garnett
    (Eds.), Vol. 32\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoon et al. (2017) Yoon, Carl Denton, Luong Hoang, and Alexander M. Rush. 2017.
    Structured Attention Networks. *CoRR* abs/1702.00887 (2017). arXiv:1702.00887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik
    Cambria. 2018. Recent Trends in Deep Learning Based Natural Language Processing
    [Review Article]. *IEEE Computational Intelligence Magazine* 13, 3 (2018), 55–75.
    [https://doi.org/10.1109/MCI.2018.2840738](https://doi.org/10.1109/MCI.2018.2840738)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019) Lishan Zhang, Yuwei Huang, Xi Yang, Shengquan Yu, and Fuzhen
    Zhuang. 2019. An automatic short-answer grading model for semi-open-ended questions.
    *Interactive Learning Environments* 0, 0 (2019), 1–14. [https://doi.org/10.1080/10494820.2019.1648300](https://doi.org/10.1080/10494820.2019.1648300)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Yuan Zhang, Chen Lin, and Min Chi. 2020a. Going deeper:
    Automatic short-answer grading by combining student and question models. *User
    Modeling and User-Adapted Interaction* 30, 1 (01 Mar 2020), 51–80. [https://doi.org/10.1007/s11257-019-09251-6](https://doi.org/10.1007/s11257-019-09251-6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang
    Zhang, Xi Zhou, and Xiang Zhou. 2020b. Semantics-Aware BERT for Language Understanding.
    *Proceedings of the AAAI Conference on Artificial Intelligence* 34, 05 (Apr. 2020),
    9628–9635. [https://doi.org/10.1609/aaai.v34i05.6510](https://doi.org/10.1609/aaai.v34i05.6510)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
