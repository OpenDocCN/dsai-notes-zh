["```py\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2**from** **fastai.conv_learner** **import** *\nPATH = Path(\"data/cifar10/\")\nos.makedirs(PATH,exist_ok=**True**)\n```", "```py\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', \n           'horse', 'ship', 'truck')\nstats = (np.array([ 0.4914 ,  0.48216,  0.44653]), \n         np.array([ 0.24703,  0.24349,  0.26159]))\n\nnum_workers = num_cpus()//2\nbs=256\nsz=32\n```", "```py\ntfms = tfms_from_stats(stats, sz, aug_tfms=[RandomFlip()], \n                       pad=sz//8)\ndata = ImageClassifierData.from_paths(PATH, val_name='test', \n                                      tfms=tfms, bs=bs)\n```", "```py\n**def** conv_layer(ni, nf, ks=3, stride=1):\n    **return** nn.Sequential(\n        nn.Conv2d(ni, nf, kernel_size=ks, bias=**False**, stride=stride,\n                  padding=ks//2),\n        nn.BatchNorm2d(nf, momentum=0.01),\n        nn.LeakyReLU(negative_slope=0.1, inplace=**True**))**class** **ResLayer**(nn.Module):\n    **def** __init__(self, ni):\n        super().__init__()\n        self.conv1=conv_layer(ni, ni//2, ks=1)\n        self.conv2=conv_layer(ni//2, ni, ks=3)\n\n    **def** forward(self, x): **return** x.add_(self.conv2(self.conv1(x)))**class** **Darknet**(nn.Module):\n    **def** make_group_layer(self, ch_in, num_blocks, stride=1):\n        **return** [conv_layer(ch_in, ch_in*2,stride=stride)\n               ] + [(ResLayer(ch_in*2)) **for** i **in** range(num_blocks)]\n\n    **def** __init__(self, num_blocks, num_classes, nf=32):\n        super().__init__()\n        layers = [conv_layer(3, nf, ks=3, stride=1)]\n        **for** i,nb **in** enumerate(num_blocks):\n            layers += self.make_group_layer(nf, nb, stride=2-(i==1))\n            nf *= 2\n        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), \n                   nn.Linear(nf, num_classes)]\n        self.layers = nn.Sequential(*layers)\n\n    **def** forward(self, x): **return** self.layers(x)\n```", "```py\n**def** forward(self, x): **return** x + self.conv2(self.conv1(x))\n```", "```py\n**def** forward(self, x): **return** x.add_(self.conv2(self.conv1(x)))\n```", "```py\nm = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=32)\nm = nn.DataParallel(m, [1,2,3])\n```", "```py\nlr = 1.3\nlearn = ConvLearner.from_model_data(m, data)\nlearn.crit = nn.CrossEntropyLoss()\nlearn.metrics = [accuracy]\nwd=1e-4%time learn.fit(lr, 1, wds=wd, cycle_len=30, use_clr_beta=(20, 20, \n                0.95, 0.85))\n```", "```py\n**from** **fastai.conv_learner** **import** *\n**from** **fastai.dataset** **import** *\n**import** **gzip**\n```", "```py\ncurl 'http://lsun.cs.princeton.edu/htbin/download.cgi?tag=latest&category=bedroom&set=train' -o bedroom.zipunzip bedroom.zippip install lmdbpython lsun-data.py {PATH}/bedroom_train_lmdb --out_dir {PATH}/bedroom\n```", "```py\nPATH = Path('data/lsun/')\nIMG_PATH = PATH/'bedroom'\nCSV_PATH = PATH/'files.csv'\nTMP_PATH = PATH/'tmp'\nTMP_PATH.mkdir(exist_ok=**True**)\n```", "```py\nfiles = PATH.glob('bedroom/**/*.jpg')\n\n**with** CSV_PATH.open('w') **as** fo:\n    **for** f **in** files: fo.write(f'{f.relative_to(IMG_PATH)},0**\\n**')*# Optional - sampling a subset of files*\nCSV_PATH = PATH/'files_sample.csv'files = PATH.glob('bedroom/**/*.jpg')\n\n**with** CSV_PATH.open('w') **as** fo:\n    **for** f **in** files:\n        **if** random.random()<0.1: \n            fo.write(f'{f.relative_to(IMG_PATH)},0**\\n**')\n```", "```py\n**class** **ConvBlock**(nn.Module):\n    **def** __init__(self, ni, no, ks, stride, bn=**True**, pad=**None**):\n        super().__init__()\n        **if** pad **is** **None**: pad = ks//2//stride\n        self.conv = nn.Conv2d(ni, no, ks, stride, padding=pad, \n                              bias=**False**)\n        self.bn = nn.BatchNorm2d(no) **if** bn **else** **None**\n        self.relu = nn.LeakyReLU(0.2, inplace=**True**)\n\n    **def** forward(self, x):\n        x = self.relu(self.conv(x))\n        **return** self.bn(x) **if** self.bn **else** x\n```", "```py\n**class** **DCGAN_D**(nn.Module):\n    **def** __init__(self, isize, nc, ndf, n_extra_layers=0):\n        super().__init__()\n        **assert** isize % 16 == 0, \"isize has to be a multiple of 16\"\n\n        self.initial = ConvBlock(nc, ndf, 4, 2, bn=**False**)\n        csize,cndf = isize/2,ndf\n        self.extra = nn.Sequential(*[ConvBlock(cndf, cndf, 3, 1)\n                                    **for** t **in** range(n_extra_layers)])\n\n        pyr_layers = []\n        **while** csize > 4:\n            pyr_layers.append(ConvBlock(cndf, cndf*2, 4, 2))\n            cndf *= 2; csize /= 2\n        self.pyramid = nn.Sequential(*pyr_layers)\n\n        self.final = nn.Conv2d(cndf, 1, 4, padding=0, bias=**False**)\n\n    **def** forward(self, input):\n        x = self.initial(input)\n        x = self.extra(x)\n        x = self.pyramid(x)\n        **return** self.final(x).mean(0).view(1)\n```", "```py\n**class** **DeconvBlock**(nn.Module):\n    **def** __init__(self, ni, no, ks, stride, pad, bn=**True**):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ni, no, ks, stride, \n                         padding=pad, bias=**False**)\n        self.bn = nn.BatchNorm2d(no)\n        self.relu = nn.ReLU(inplace=**True**)\n\n    **def** forward(self, x):\n        x = self.relu(self.conv(x))\n        **return** self.bn(x) **if** self.bn **else** x**class** **DCGAN_G**(nn.Module):\n    **def** __init__(self, isize, nz, nc, ngf, n_extra_layers=0):\n        super().__init__()\n        **assert** isize % 16 == 0, \"isize has to be a multiple of 16\"\n\n        cngf, tisize = ngf//2, 4\n        **while** tisize!=isize: cngf*=2; tisize*=2\n        layers = [DeconvBlock(nz, cngf, 4, 1, 0)]\n\n        csize, cndf = 4, cngf\n        **while** csize < isize//2:\n            layers.append(DeconvBlock(cngf, cngf//2, 4, 2, 1))\n            cngf //= 2; csize *= 2\n\n        layers += [DeconvBlock(cngf, cngf, 3, 1, 1) \n                       **for** t **in** range(n_extra_layers)]\n        layers.append(nn.ConvTranspose2d(cngf, nc, 4, 2, 1,\n                                            bias=**False**))\n        self.features = nn.Sequential(*layers)\n\n    **def** forward(self, input): **return** F.tanh(self.features(input))\n```", "```py\nbs,sz,nz = 64,64,100tfms = tfms_from_stats(inception_stats, sz)\nmd = ImageClassifierData.from_csv(PATH, 'bedroom', CSV_PATH, \n         tfms=tfms, bs=128, skip_header=**False**, continuous=**True**)md = md.resize(128)x,_ = next(iter(md.val_dl))plt.imshow(md.trn_ds.denorm(x)[0]);\n```", "```py\nnetG = DCGAN_G(sz, nz, 3, 64, 1).cuda()\nnetD = DCGAN_D(sz, 3, 64, 1).cuda()\n```", "```py\n**def** create_noise(b): \n   **return** V(torch.zeros(b, nz, 1, 1).normal_(0, 1))preds = netG(create_noise(4))\npred_ims = md.trn_ds.denorm(preds)\n\nfig, axes = plt.subplots(2, 2, figsize=(6, 6))\n**for** i,ax **in** enumerate(axes.flat): ax.imshow(pred_ims[i])\n```", "```py\n**def** gallery(x, nc=3):\n    n,h,w,c = x.shape\n    nr = n//nc\n    **assert** n == nr*nc\n    **return** (x.reshape(nr, nc, h, w, c)\n              .swapaxes(1,2)\n              .reshape(h*nr, w*nc, c))\n```", "```py\noptimizerD = optim.RMSprop(netD.parameters(), lr = 1e-4)\noptimizerG = optim.RMSprop(netG.parameters(), lr = 1e-4)\n```", "```py\n**for** p **in** netD.parameters(): p.data.clamp_(-0.01, 0.01)\n```", "```py\n**def** train(niter, first=**True**):\n    gen_iterations = 0\n    **for** epoch **in** trange(niter):\n        netD.train(); netG.train()\n        data_iter = iter(md.trn_dl)\n        i,n = 0,len(md.trn_dl)\n        **with** tqdm(total=n) **as** pbar:\n            **while** i < n:\n                set_trainable(netD, **True**)\n                set_trainable(netG, **False**)\n                d_iters = 100 **if** (first **and** (gen_iterations < 25) \n                              **or** (gen_iterations % 500 == 0)) **else** 5\n                j = 0\n                **while** (j < d_iters) **and** (i < n):\n                    j += 1; i += 1\n                    **for** p **in** netD.parameters(): \n                        p.data.clamp_(-0.01, 0.01)\n                    real = V(next(data_iter)[0])\n                    real_loss = netD(real)\n                    fake = netG(create_noise(real.size(0)))\n                    fake_loss = netD(V(fake.data))\n                    netD.zero_grad()\n                    lossD = real_loss-fake_loss\n                    lossD.backward()\n                    optimizerD.step()\n                    pbar.update()\n\n                set_trainable(netD, **False**)\n                set_trainable(netG, **True**)\n                netG.zero_grad()\n                lossG = netD(netG(create_noise(bs))).mean(0).view(1)\n                lossG.backward()\n                optimizerG.step()\n                gen_iterations += 1\n\n        print(f'Loss_D {to_np(lossD)}; Loss_G {to_np(lossG)}; '\n              f'D_real {to_np(real_loss)}; Loss_D_fake\n              {to_np(fake_loss)}')\n```", "```py\nd_iters = 100 **if** (first **and** (gen_iterations < 25) \n                              **or** (gen_iterations % 500 == 0)) **else** 5\n```", "```py\ntorch.backends.cudnn.benchmark=**True**\n```", "```py\ntrain(1, **False**)0%|          | 0/1 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18957/18957 [19:48<00:00, 10.74it/s]\nLoss_D [-0.67574]; Loss_G [0.08612]; D_real [-0.1782]; Loss_D_fake [0.49754]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [19:49<00:00, 1189.02s/it]\n```", "```py\nfixed_noise = create_noise(bs)\n```", "```py\nset_trainable(netD, **True**)\nset_trainable(netG, **True**)\noptimizerD = optim.RMSprop(netD.parameters(), lr = 1e-5)\noptimizerG = optim.RMSprop(netG.parameters(), lr = 1e-5)train(1, **False**)0%|          | 0/1 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18957/18957 [23:31<00:00, 13.43it/s]\nLoss_D [-1.01657]; Loss_G [0.51333]; D_real [-0.50913]; Loss_D_fake [0.50744]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [23:31<00:00, 1411.84s/it]\n```", "```py\nnetD.eval(); netG.eval();\nfake = netG(fixed_noise).data.cpu()\nfaked = np.clip(md.trn_ds.denorm(fake),0,1)\n\nplt.figure(figsize=(9,9))\nplt.imshow(gallery(faked, 8));\n```", "```py\n**from** **fastai.conv_learner** **import** *\n**from** **fastai.dataset** **import** *\n**from** **cgan.options.train_options** **import** *\n```", "```py\nopt = TrainOptions().parse(['--dataroot',    \n   '/data0/datasets/cyclegan/horse2zebra', '--nThreads', '8', \n   '--no_dropout', '--niter', '100', '--niter_decay', '100', \n   '--name', 'nodrop', '--gpu_ids', '2'])\n```", "```py\n**from** **cgan.data.data_loader** **import** CreateDataLoader\n**from** **cgan.models.models** **import** create_model\n```", "```py\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\ndataset_size*1334*\n```", "```py\nmodel = create_model(opt)\n```", "```py\ntotal_steps = 0\n\n**for** epoch **in** range(opt.epoch_count, opt.niter + opt.niter_decay+1):\n    epoch_start_time = time.time()\n    iter_data_time = time.time()\n    epoch_iter = 0\n\n    **for** i, data **in** tqdm(enumerate(dataset)):\n        iter_start_time = time.time()\n        **if** total_steps % opt.print_freq == 0: \n             t_data = iter_start_time - iter_data_time\n        total_steps += opt.batchSize\n        epoch_iter += opt.batchSize\n        model.set_input(data)\n        model.optimize_parameters()\n\n        **if** total_steps % opt.display_freq == 0:\n            save_result = total_steps % opt.update_html_freq == 0\n\n        **if** total_steps % opt.print_freq == 0:\n            errors = model.get_current_errors()\n            t = (time.time() - iter_start_time) / opt.batchSize\n\n        **if** total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model(epoch **%d**,total_steps **%d**)'\n                    % (epoch, total_steps))\n            model.save('latest')\n\n        iter_data_time = time.time()\n    **if** epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch **%d**, iters **%d**' \n               % (epoch, total_steps))\n        model.save('latest')\n        model.save(epoch)\n\n    print('End of epoch **%d** / **%d** **\\t** Time Taken: **%d** sec' %\n          (epoch, opt.niter + opt.niter_decay, time.time() \n          - epoch_start_time))\n    model.update_learning_rate()\n```", "```py\n**def** show_img(im, ax=**None**, figsize=**None**):\n    **if** **not** ax: fig,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(**False**)\n    ax.get_yaxis().set_visible(**False**)\n    **return** ax**def** get_one(data):\n    model.set_input(data)\n    model.test()\n    **return** list(model.get_current_visuals().values())model.save(201)test_ims = []\n**for** i,o **in** enumerate(dataset):\n    **if** i>10: **break**\n    test_ims.append(get_one(o))**def** show_grid(ims):\n    fig,axes = plt.subplots(2,3,figsize=(9,6))\n    **for** i,ax **in** enumerate(axes.flat): show_img(ims[i], ax);\n    fig.tight_layout()**for** i **in** range(8): show_grid(test_ims[i])\n```", "```py\n#! wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n```"]