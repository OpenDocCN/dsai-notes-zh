- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:51:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.00111] Deep Learning-based Action Detection in Untrimmed Videos: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.00111] 基于深度学习的未裁剪视频动作检测：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.00111](https://ar5iv.labs.arxiv.org/html/2110.00111)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.00111](https://ar5iv.labs.arxiv.org/html/2110.00111)
- en: 'Deep Learning-based Action Detection in Untrimmed Videos: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的未裁剪视频动作检测：综述
- en: 'Elahe Vahdani and Yingli Tian^∗ E. Vahdani is with the Department of Computer
    Science, The Graduate Center, The City University of New York, NY, 10016\. E-mail:
    evahdani@gradcenter.cuny.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Elahe Vahdani 和 Yingli Tian^∗ E. Vahdani 在纽约市立大学研究生中心计算机科学系工作，地址为纽约，10016。电子邮件：evahdani@gradcenter.cuny.edu
- en: Y. Tian is with the Department of Electrical Engineering, The City College,
    and the Department of Computer Science, the Graduate Center, the City University
    of New York, NY, 10031\. E-mail:ytian@ccny.cuny.edu ^∗Corresponding authorThis
    material is based upon work supported by the National Science Foundation under
    award number IIS-2041307.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Y. Tian 目前在纽约市立大学电气工程系和计算机科学系（研究生中心）工作，地址为纽约，10031。电子邮件：ytian@ccny.cuny.edu
    ^∗通讯作者。本材料基于国家科学基金会在奖号IIS-2041307下的资助工作。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Understanding human behavior and activity facilitates advancement of numerous
    real-world applications, and is critical for video analysis. Despite the progress
    of action recognition algorithms in trimmed videos, the majority of real-world
    videos are lengthy and untrimmed with sparse segments of interest. The task of
    temporal activity detection in untrimmed videos aims to localize the temporal
    boundary of actions and classify the action categories. Temporal activity detection
    task has been investigated in full and limited supervision settings depending
    on the availability of action annotations. This paper provides an extensive overview
    of deep learning-based algorithms to tackle temporal action detection in untrimmed
    videos with different supervision levels including fully-supervised, weakly-supervised,
    unsupervised, self-supervised, and semi-supervised. In addition, this paper also
    reviews advances in spatio-temporal action detection where actions are localized
    in both temporal and spatial dimensions. Moreover, the commonly used action detection
    benchmark datasets and evaluation metrics are described, and the performance of
    the state-of-the-art methods are compared. Finally, real-world applications of
    temporal action detection in untrimmed videos and a set of future directions are
    discussed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 理解人类行为和活动有助于众多现实世界应用的进步，并且对视频分析至关重要。尽管在裁剪视频中动作识别算法取得了进展，但大多数现实世界视频是冗长的、未裁剪的，只有零星感兴趣的片段。未裁剪视频中的时间活动检测任务旨在定位动作的时间边界并分类动作类别。根据动作注释的可用性，时间活动检测任务已经在全监督和有限监督设置下进行研究。本文提供了基于深度学习的算法的广泛概述，以应对不同监督水平下的未裁剪视频中的时间动作检测，包括完全监督、弱监督、无监督、自监督和半监督。此外，本文还回顾了时空动作检测的进展，其中动作在时间和空间维度上都被定位。此外，描述了常用的动作检测基准数据集和评估指标，并比较了最先进方法的性能。最后，讨论了未裁剪视频中时间动作检测的现实世界应用和未来发展方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Action Understanding, Temporal Action Detection, Untrimmed Videos, Deep Learning,
    Full and Limited Supervision.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 动作理解、时间动作检测、未裁剪视频、深度学习、全监督和有限监督。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: This paper provides a comprehensive overview of temporal action detection. This
    task aims to detect the start and end of action instances in long untrimmed videos
    and predict the action categories. Temporal action detection is crucial for many
    video analysis applications such as sports analysis, autonomous driving, anomaly
    detection in surveillance cameras, understanding instructional videos, etc. Learning
    with limited supervision is a scheme where annotations of actions are unavailable
    or only partially available during the training phase. Because annotation of long
    untrimmed videos is very time-consuming, designing action detection methods with
    limited supervision has been very popular. This survey reviews temporal action
    detection methods with full and limited supervision signals.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了时间动作检测的全面概述。此任务旨在检测长时间未裁剪视频中动作实例的开始和结束，并预测动作类别。时间动作检测对于许多视频分析应用至关重要，如体育分析、自动驾驶、监控摄像头中的异常检测、理解教学视频等。有限监督学习是一种在训练阶段无法获得或只能部分获得动作注释的方案。由于对长时间未裁剪视频的注释非常耗时，因此设计具有有限监督的动作检测方法已变得非常流行。本调查回顾了具有完整和有限监督信号的时间动作检测方法。
- en: '![Refer to caption](img/ec29511b2607e6138e4b7c525de8680a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ec29511b2607e6138e4b7c525de8680a.png)'
- en: 'Figure 1: Temporal action detection aims to localize action instances in time
    and recognize their categories. The first row demonstrates an example of action
    “long jump” detected in an untrimmed video from THUMOS14 dataset [[1](#bib.bib1)].
    The second row is an example of an untrimmed video including several action instances
    of interest with various lengths.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：时间动作检测旨在定位时间中的动作实例并识别其类别。第一行展示了从 THUMOS14 数据集中未裁剪视频中检测到的“远跳”动作的示例。第二行是一个包含几个感兴趣动作实例的未裁剪视频示例，这些动作实例的长度各异。
- en: 1.1 Motivation
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 动机
- en: Social networks and digital cameras have led to substantial video and media
    content produced by individuals each day. Hence, video understanding and analysis
    continues to be one of the essential research subjects in computer vision. While
    deep learning has accomplished remarkable performance in many computer vision
    tasks, video understanding is still far from ideal. Action understanding, notably,
    as a vital element of video analysis, facilitates the advancement of numerous
    real-world applications. For instance, collaborative robots need to recognize
    how the human partner completes the job to cope with the variations in the task
    [[2](#bib.bib2)]. Sport analysis systems must comprehend game actions to report
    commentaries of live activities [[3](#bib.bib3)]. Autonomous driving cars demand
    an understanding of operations performed by the surrounding cars and pedestrians
    [[4](#bib.bib4)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络和数字相机使个人每天产生大量的视频和媒体内容。因此，视频理解和分析仍然是计算机视觉中的一个重要研究课题。尽管深度学习在许多计算机视觉任务中取得了显著成绩，但视频理解仍然远未理想。动作理解，作为视频分析的一个重要元素，促进了众多现实世界应用的进步。例如，协作机器人需要识别人类伙伴如何完成任务，以应对任务中的变化
    [[2](#bib.bib2)]。体育分析系统必须理解比赛动作，以报告现场活动的评论 [[3](#bib.bib3)]。自动驾驶汽车需要了解周围车辆和行人执行的操作
    [[4](#bib.bib4)]。
- en: In this paper, we define trimmed videos as pre-segmented video clips that each
    contains only one action instance. In other words, the context of the action,
    i.e., moments before or after the action are not included in the video. Therefore,
    action detection in trimmed videos only need to classify the action categories
    without the need to detect starting and ending timestamps. Recognizing actions
    in trimmed videos has many applications in video surveillance, robotics, medical
    diagnosis [[5](#bib.bib5)], and has achieved excellent performance in recent years
    [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)]. However, the majority of videos
    in the wild, i.e., recorded in unconstrained environments, are naturally untrimmed.
    Untrimmed videos are lengthy unsegmented videos that may include several action
    instances, the moments before or after each action, and the transition from one
    action to another. The action instances in one video can belong to several action
    classes and have different duration.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将修剪视频定义为预先分段的视频剪辑，每个剪辑仅包含一个动作实例。换句话说，视频中不包括动作的上下文，即动作前后的时刻。因此，在修剪视频中，动作检测只需对动作类别进行分类，而无需检测起始和结束时间戳。在修剪视频中识别动作在视频监控、机器人技术、医学诊断[[5](#bib.bib5)]等领域有许多应用，并且近年来已经取得了优异的表现[[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)]。然而，大多数自然环境下的视频，即在无约束环境中录制的视频，通常是未修剪的。未修剪的视频是长度不一的未分段视频，可能包含多个动作实例、每个动作前后的时刻，以及动作之间的过渡。在一个视频中的动作实例可能属于多个动作类别，并且持续时间各异。
- en: 'Temporal activity detection in untrimmed videos aims to localize the action
    instances in time and recognize their categories. This task is considerably more
    complicated than action recognition which merely seeks to classify the categories
    of trimmed video clips. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey") shows an example of temporal
    activity detection in an untrimmed video recorded in a stadium. The first row
    demonstrates the detection of action ”long jump” in temporal domain where the
    start and end time of the action are localized. The goal is to only detect the
    actions of interest, i.e., actions that belong to a predefined set of action classes.
    The temporal intervals of other activities that do not belong to this set of actions
    are called temporal background. For example, the segments right before or right
    after action ”long jump” may belong to other diverse activities such as crowd
    cheering in the stadium. In some cases, the frames right before or right after
    an action are visually very similar to the start or end of the action which makes
    the localization of action intervals very challenging. Another challenge (as shown
    in the second row of Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")) is that action instances may
    occur at any time of the video and have various duration, lasting from less than
    a second to several minutes [[9](#bib.bib9)].'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '未修剪视频中的时间活动检测旨在定位动作实例的时间并识别其类别。这个任务比仅仅对修剪视频剪辑的类别进行分类的动作识别要复杂得多。图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep Learning-based Action Detection in Untrimmed
    Videos: A Survey") 显示了一个在体育场录制的未修剪视频中的时间活动检测示例。第一行展示了在时间域中对“跳远”动作的检测，其中动作的开始和结束时间被定位。目标是只检测感兴趣的动作，即属于预定义动作类别集合的动作。其他不属于该动作集合的活动的时间区间称为时间背景。例如，“跳远”动作前后立即的片段可能属于其他不同的活动，如体育场中的观众欢呼。在某些情况下，动作前后紧接的帧在视觉上可能非常类似于动作的开始或结束，这使得动作区间的定位非常具有挑战性。另一个挑战（如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey") 第二行所示）是动作实例可能发生在视频的任何时间点，并且持续时间各不相同，从不到一秒到几分钟不等[[9](#bib.bib9)]。'
- en: 'Temporal action detection mainly targets activities of high-level semantics
    and videos with a sparse set of actions (e.g., actions only cover $30\%$ of the
    frames in [[10](#bib.bib10)]). However, in some cases, the goal is to predict
    action labels at every frame of the video. In such cases, the task is referred
    to as temporal action segmentation which targets the fine-grained actions and
    videos with dense occurrence of actions ($93\%$ of the frames in [[11](#bib.bib11)]).
    One can convert between a given segmentation and a set of detected instances in
    the temporal domain by simply adding or removing temporal background segments
    [[12](#bib.bib12)]. Temporal action detection similar to object detection belongs
    to the family of detection problems. Both of these problems aim to localize the
    instances of interest, i.e., action intervals in temporal domain versus object
    bounding boxes in spatial domain, Fig. [2](#S1.F2 "Figure 2 ‣ 1.1 Motivation ‣
    1 Introduction ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")
    (a and c). When targeting fine-grained actions, temporal action detection (segmentation)
    is similar to semantic segmentation as both aim to classify every single instance,
    i.e., frames in temporal domain versus pixels in spatial domain, Fig. [2](#S1.F2
    "Figure 2 ‣ 1.1 Motivation ‣ 1 Introduction ‣ Deep Learning-based Action Detection
    in Untrimmed Videos: A Survey") (b and d). As a result, many techniques for temporal
    action detection and segmentation are inspired by the advancements in object detection
    and semantic segmentation [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)].'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '时序动作检测主要针对高层次语义的活动以及具有稀疏动作集的视频（例如，动作仅覆盖[[10](#bib.bib10)]中$30\%$的帧）。然而，在某些情况下，目标是预测视频中每一帧的动作标签。在这种情况下，这个任务被称为时序动作分割，旨在处理细粒度动作和动作密集出现的视频（[[11](#bib.bib11)]中$93\%$的帧）。可以通过简单地添加或删除时序背景片段在给定的分割和检测到的实例集之间进行转换[[12](#bib.bib12)]。时序动作检测类似于目标检测，属于检测问题的范畴。这两个问题都旨在定位感兴趣的实例，即时序域中的动作间隔与空间域中的目标边界框，如图[2](#S1.F2
    "Figure 2 ‣ 1.1 Motivation ‣ 1 Introduction ‣ Deep Learning-based Action Detection
    in Untrimmed Videos: A Survey")（a和c）。当目标是细粒度动作时，时序动作检测（分割）类似于语义分割，因为两者都旨在分类每一个实例，即时序域中的帧与空间域中的像素，如图[2](#S1.F2
    "Figure 2 ‣ 1.1 Motivation ‣ 1 Introduction ‣ Deep Learning-based Action Detection
    in Untrimmed Videos: A Survey")（b和d）。因此，许多时序动作检测和分割技术都受到目标检测和语义分割进展的启发[[13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15)]。'
- en: Action detection has drawn much attention in recent years and has broad applications
    in video analysis tasks. As surveillance cameras are increasingly deployed in
    many places, the demand for anomaly detection has also surged. Anomalous events
    such as robbery or accidents occur less frequently compared with normal activities
    and it can be very time-consuming to detect such events by humans. Therefore,
    automatic detection of suspicious events has a great advantage. By growing popularity
    of social media many people follow online tutorials and instructional videos to
    learn how to perform a task such as “changing the car tire” properly for the first
    time. The instructional videos are usually untrimmed and include several steps
    of the main task, e.g., “jack up the car” and “put on the tire” for changing the
    tire. Automatic segmentation of these videos to the main action steps can facilitate
    and optimize the learning process. Another application is in sport video analysis
    to localize the salient actions and highlights of a game and analyze the strategies
    of specific teams. Furthermore, action detection has a critical role in self-driving
    cars to analyze the behavior of pedestrians, cyclists, and other surrounding vehicles
    to make safe autonomous decisions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 动作检测近年来受到广泛关注，并在视频分析任务中具有广泛应用。随着监控摄像头在许多地方的普及，对异常检测的需求也急剧增加。与正常活动相比，诸如抢劫或事故等异常事件发生频率较低，人力检测此类事件可能非常耗时。因此，自动检测可疑事件具有很大的优势。随着社交媒体的普及，许多人通过在线教程和教学视频学习如何正确地进行任务，例如“更换汽车轮胎”。这些教学视频通常是未剪辑的，包括主任务的几个步骤，例如“抬起汽车”和“安装轮胎”以更换轮胎。自动分割这些视频的主要动作步骤可以促进和优化学习过程。另一个应用是在体育视频分析中，以定位比赛中的显著动作和亮点，并分析特定队伍的策略。此外，动作检测在自动驾驶汽车中具有关键作用，用于分析行人、骑自行车的人和其他周围车辆的行为，以做出安全的自主决策。
- en: '![Refer to caption](img/f4bd19e97e02b7d44dd19d70b7b21e9f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f4bd19e97e02b7d44dd19d70b7b21e9f.png)'
- en: 'Figure 2: Task Relations: (a) Temporal detection of action “Long Jump” on THUMOS14
    [[1](#bib.bib1)]. (b) Temporal detection (segmentation) of fine-grained actions
    shown by different colors in a “making pancakes” video on Breakfast [[11](#bib.bib11)].
    (c) and (d) Results from [[16](#bib.bib16)] on PASCAL [[17](#bib.bib17)].'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：任务关系：（a）在 THUMOS14 [[1](#bib.bib1)] 上对动作“跳远”的时间检测。（b）在 Breakfast [[11](#bib.bib11)]
    上显示不同颜色的细粒度动作的时间检测（分割）。（c）和（d）来自 [[16](#bib.bib16)] 在 PASCAL [[17](#bib.bib17)]
    上的结果。
- en: 1.2 Taxonomy
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 分类
- en: 'To the best of our knowledge, this is the first comprehensive survey describing
    deep learning based algorithms for activity detection in untrimmed videos with
    different supervision levels. We describe the fully-supervised methods in Section
    [2.3](#S2.SS3 "2.3 Action Detection with Full Supervision ‣ 2 Temporal Action
    Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey") and methods with limited supervision (weakly-supervised, unsupervised,
    self-supervised, and semi-supervised) in Section [2.4](#S2.SS4 "2.4 Action Detection
    with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey"). Section [3](#S3 "3 Datasets
    and Evaluation ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")
    summarizes action detection benchmark datasets, evaluation metrics, and performance
    comparison between the-state-of-the-art methods. Finally, Section [4](#S4 "4 Discussions
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey") discusses
    the most common real-world applications of action detection and possible future
    directions. We provide a brief introduction of the tasks here.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '据我们所知，这是首次全面介绍基于深度学习的算法用于不同监督级别的未裁剪视频活动检测的调查。我们在第 [2.3](#S2.SS3 "2.3 Action
    Detection with Full Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey") 节描述了完全监督的方法，而在第 [2.4](#S2.SS4
    "2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey") 节描述了有限监督（弱监督、无监督、自监督和半监督）的方法。第
    [3](#S3 "3 Datasets and Evaluation ‣ Deep Learning-based Action Detection in Untrimmed
    Videos: A Survey") 节总结了动作检测基准数据集、评估指标以及最先进方法之间的性能比较。最后，第 [4](#S4 "4 Discussions
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey") 节讨论了动作检测最常见的现实世界应用和可能的未来方向。我们在这里简要介绍这些任务。'
- en: 'Temporal action detection aims to find the precise temporal boundary and label
    of action instances in untrimmed videos. Depending on annotation availability
    in train set, temporal action detection can be studied in the following settings
    (also listed in Table [I](#S1.T1 "TABLE I ‣ 6th item ‣ 1.2 Taxonomy ‣ 1 Introduction
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '时间动作检测的目的是在未裁剪的视频中找到动作实例的精确时间边界和标签。根据训练集中的注释可用性，时间动作检测可以在以下设置中进行研究（也列在表格 [I](#S1.T1
    "TABLE I ‣ 6th item ‣ 1.2 Taxonomy ‣ 1 Introduction ‣ Deep Learning-based Action
    Detection in Untrimmed Videos: A Survey")）。'
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fully supervised action detection: Temporal boundaries and labels of action
    instances are available for training.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完全监督动作检测：动作实例的时间边界和标签可用于训练。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Weakly-supervised action detection: Only the video-level labels of action instances
    are available. The order of action labels can be provided or not.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 弱监督动作检测：仅提供动作实例的视频级标签。动作标签的顺序可以提供，也可以不提供。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unsupervised action detection: There are no annotations for the action instances.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无监督动作检测：动作实例没有注释。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Semi-supervised action detection: The data is split to a small set $S_{1}$
    and a large set $S_{2}$. The videos in $S_{1}$ are fully annotated (as in fully-supervised)
    while the videos in $S_{2}$ are either not annotated (unsupervised) or only annotated
    with video-level labels (as in weakly-supervised).'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半监督动作检测：数据被分为小集 $S_{1}$ 和大集 $S_{2}$。$S_{1}$ 中的视频是完全标注的（如同完全监督），而 $S_{2}$ 中的视频要么没有标注（无监督），要么仅标注了视频级别的标签（如同弱监督）。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Self-supervised action detection: A pretext task is defined to extract information
    from the data in an unsupervised setting by leveraging its structure. Then, this
    information is used to improve the performance for temporal action detection (downstream
    task) which can be supervised, unsupervised, or semi-supervised.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自监督动作检测：定义了一个前提任务，通过利用数据的结构在无监督环境中提取信息。然后，这些信息用于提高时间动作检测（下游任务）的性能，该任务可以是监督的、无监督的或半监督的。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Action detection with limited supervision: Limited supervision is the opposite
    of full supervision where the annotations are unavailable or partially available.
    In this paper, we define limited supervision to include weakly-supervised, unsupervised,
    self-supervised, and semi-supervised settings as they are defined above.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有限监督下的行为检测：有限监督是完全监督的对立面，其中注释不可用或部分可用。在本文中，我们将有限监督定义为包括弱监督、无监督、自监督和半监督设置，如上所述。
- en: 'TABLE I: Main categories of temporal action detection task with different supervision
    levels in training set. “✓” indicates “available”; “✗” is for “unavailable”, and
    $\ast$ is “partially available”.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 I：训练集中具有不同监督级别的时间行为检测任务的主要类别。“✓”表示“可用”；“✗”表示“不可用”，而$\ast$表示“部分可用”。
- en: '| Supervision Level | Action Temporal Boundaries | Action Labels |'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 监督级别 | 行为时间边界 | 行为标签 |'
- en: '| Fully-supervised | ✓ | ✓ |'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 完全监督 | ✓ | ✓ |'
- en: '| Weakly-supervised | ✗ | ✓ |'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 弱监督 | ✗ | ✓ |'
- en: '| Unsupervised | ✗ | ✗ |'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 无监督 | ✗ | ✗ |'
- en: '| Semi-supervised | $\ast$ | $\ast$ |'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 半监督 | $\ast$ | $\ast$ |'
- en: '| Self-supervised | ✓$\ast$ ✗ | ✓$\ast$ ✗ |'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 自监督 | ✓$\ast$ ✗ | ✓$\ast$ ✗ |'
- en: 2 Temporal Action Detection Methods
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 时间行为检测方法
- en: 'We begin this section by introducing important technical terms in Section [2.1](#S2.SS1
    "2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey"). Given an input video, video
    feature encoding is necessary to extract representative visual features of the
    video (discussed in Section [2.2](#S2.SS2 "2.2 Video Feature Encoding ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")). Action detection methods with full supervision are described in Section
    [2.3](#S2.SS3 "2.3 Action Detection with Full Supervision ‣ 2 Temporal Action
    Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey") and action detection methods with limited supervision are reviewed
    in Section [2.4](#S2.SS4 "2.4 Action Detection with Limited Supervision ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节开始时介绍了第[2.1](#S2.SS1 "2.1 术语定义 ‣ 2 时间行为检测方法 ‣ 深度学习驱动的未修剪视频中的行为检测：综述")节中的重要技术术语。给定一个输入视频，视频特征编码是提取视频代表性视觉特征的必要步骤（讨论在第[2.2](#S2.SS2
    "2.2 视频特征编码 ‣ 2 时间行为检测方法 ‣ 深度学习驱动的未修剪视频中的行为检测：综述")节中）。完全监督的行为检测方法在第[2.3](#S2.SS3
    "2.3 完全监督的行为检测 ‣ 2 时间行为检测方法 ‣ 深度学习驱动的未修剪视频中的行为检测：综述")节中描述，有限监督的行为检测方法在第[2.4](#S2.SS4
    "2.4 有限监督的行为检测 ‣ 2 时间行为检测方法 ‣ 深度学习驱动的未修剪视频中的行为检测：综述")节中回顾。
- en: 2.1 Term Definition
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 术语定义
- en: To facilitate reading subsequent sections, we define common terms, scores, and
    loss functions here.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于阅读后续章节，我们在此定义常见的术语、评分和损失函数。
- en: Definition 1.
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1。
- en: 'Temporal action detection. This task aims to find the precise temporal boundaries
    and categories of action instances in untrimmed videos. Annotation of an input
    video is denoted by ${\Psi}_{g}$ and includes a set of action instances as the
    following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 时间行为检测。此任务旨在找出未修剪视频中行为实例的精确时间边界和类别。输入视频的注释表示为${\Psi}_{g}$，包括一组行为实例，如下所示：
- en: '|  | ${\Psi}_{g}=\{{\varphi}_{n}=(t_{s,n},t_{e,n},l_{n})\}_{n=1}^{N},$ |  |
    (1) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\Psi}_{g}=\{{\varphi}_{n}=(t_{s,n},t_{e,n},l_{n})\}_{n=1}^{N},$ |  |
    (1) |'
- en: where $N$ is the number of action instances, and ${\varphi}_{n}$ is the $n$-th
    action instance. The start time, end time, and label of ${\varphi}_{n}$ are denoted
    by $t_{s,n}$, $t_{e,n}$, and $l_{n}$, respectively. Label $l_{n}$ belongs to set
    $\{1,\cdots,C\}$, where $C$ is the number of action classes of interest in the
    whole dataset. The annotation ${\Psi}_{g}$ can be fully, partially, or not available
    for the videos of the training set.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是行为实例的数量，${\varphi}_{n}$ 是第 $n$ 个行为实例。${\varphi}_{n}$ 的开始时间、结束时间和标签分别表示为
    $t_{s,n}$、$t_{e,n}$ 和 $l_{n}$。标签 $l_{n}$ 属于集合 $\{1,\cdots,C\}$，其中 $C$ 是整个数据集中感兴趣的行为类别的数量。注释
    ${\Psi}_{g}$ 可以完全、部分或不适用于训练集中的视频。
- en: Definition 2.
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2。
- en: 'Temporal proposals. The temporal regions of input video that are likely to
    contain an action are called temporal proposals. Each temporal proposal $P_{n}$
    is an interval identified with a starting time $t_{s,n}$, an ending time $t_{e,n}$,
    and a confidence score $c_{n}$. The confidence score is the predicted probability
    that the interval contains an action. Proposal $P_{n}$ can be formulated as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 时间提议。输入视频中可能包含动作的时间区域称为时间提议。每个时间提议 $P_{n}$ 是一个区间，具有起始时间 $t_{s,n}$、结束时间 $t_{e,n}$
    和置信度得分 $c_{n}$。置信度得分是该区间包含动作的预测概率。提议 $P_{n}$ 可以表示为：
- en: '|  | $P_{n}=(t_{s,n},t_{e,n},c_{n}).$ |  | (2) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{n}=(t_{s,n},t_{e,n},c_{n}).$ |  | (2) |'
- en: Definition 3.
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.
- en: 'Temporal IoU (tIoU). This is the ratio of temporal intersection over union
    between two temporal intervals. It is often measured between a predicted proposal
    (interval $I_{p}$) and its closest ground-truth action instance (interval $I_{g}$),
    formulated as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 时间IoU（tIoU）。这是两个时间区间之间的时间交集与并集的比率。它通常在预测提议（区间 $I_{p}$）和其最接近的真实动作实例（区间 $I_{g}$）之间进行测量，公式如下：
- en: '|  | $tIoU(I_{p},I_{g})=\frac{I_{p}\cap I_{g}}{I_{p}\cup I_{g}}.$ |  | (3)
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $tIoU(I_{p},I_{g})=\frac{I_{p}\cap I_{g}}{I_{p}\cup I_{g}}.$ |  | (3)
    |'
- en: Definition 4.
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.
- en: Temporal proposal labeling. Label of proposal $I_{p}$ is determined by ground-truth
    action instance $I_{g}$ that has the maximum tIoU with $I_{p}$. Let’s denote the
    class label of $I_{g}$ with $c$. Then, depending on a predefined threshold $\sigma$,
    the proposal is declared as positive (true positive) with label $c$ if $tIoU\geq\sigma$.
    Otherwise, it is negative (or false positive). Also, if a ground-truth action
    instance is matched with several proposals, only the proposal with the highest
    confidence score is accepted as true positive and the others are declared as false
    positives.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 时间提议标注。提议 $I_{p}$ 的标注由与 $I_{p}$ 具有最大 tIoU 的真实动作实例 $I_{g}$ 决定。我们用 $c$ 表示 $I_{g}$
    的类别标签。然后，根据预定义的阈值 $\sigma$，如果 $tIoU\geq\sigma$，则该提议被标记为正（真实正例），并且标签为 $c$。否则，它是负（或假正例）。此外，如果一个真实动作实例与多个提议匹配，则只有置信度得分最高的提议被接受为真实正例，其它的被标记为假正例。
- en: Definition 5.
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 5.
- en: Precision and recall for proposal generation. Precision is the ratio of true
    positive proposals to the total number of predicted proposals. Precision must
    be high to avoid producing exhaustively many irrelevant proposals. Recall is the
    ratio of true positive proposals to the total number of ground-truth action instances.
    Recall must be high to avoid missing ground-truth instances.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 提议生成的精确度和召回率。精确度是正确提议的比例与预测提议总数之比。为了避免生成大量无关的提议，精确度必须很高。召回率是正确提议的比例与真实动作实例总数之比。为了避免漏掉真实实例，召回率必须很高。
- en: Definition 6.
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 6.
- en: Actionness score. Actionness score at a temporal position is the probability
    of occurrence of an action instance at that time. This score is often denoted
    by $a_{t}\in[0,1]$ for time $t$.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 动作性得分。在某个时间点的动作性得分是该时间点动作实例出现的概率。该得分通常用 $a_{t}\in[0,1]$ 表示时间 $t$。
- en: Definition 7.
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 7.
- en: Startness and endness scores. Startness score (endness score) at a temporal
    position is the probability of start (end) of an action instance at that time.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 起始性和结束性得分。某个时间点的起始性得分（结束性得分）是该时间点动作实例开始（结束）的概率。
- en: Definition 8.
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 8.
- en: Action completeness score. The maximum tIoU between a candidate proposal and
    ground truth action instances is called action completeness of that proposal.
    It was shown by [[18](#bib.bib18)] that incomplete proposals that have low tIoU
    with ground-truth intervals could have high classification scores. Therefore,
    action completeness must be considered to evaluate and rank the predicted proposals.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 动作完整性得分。候选提议与真实动作实例之间的最大 tIoU 被称为该提议的动作完整性。[[18](#bib.bib18)] 显示了具有低 tIoU 的不完整提议可能具有高分类得分。因此，必须考虑动作完整性来评估和排序预测提议。
- en: Definition 9.
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 9.
- en: Action classification score. Generated temporal proposals are fed to action
    classifiers to produce a probability distribution over all action classes. This
    can be represented by vector $(p^{1},\cdots,p^{C})$ where $p^{i}$ is the probability
    of action class $i$, and $C$ is the number of classes. For a fair comparison,
    researchers utilize classifiers from earlier work SCNN-classifier [[19](#bib.bib19)],
    UntrimmedNet [[19](#bib.bib19)], [[20](#bib.bib20)], etc. They uniformly sample
    a constant number of frames from the video segment and feed it to ConvNets such
    as C3D [[21](#bib.bib21)], two stream CNNs [[22](#bib.bib22)] or temporal segment
    networks [[23](#bib.bib23)]. In some cases, the recognition scores of sampled
    frames are aggregated with the Top-k pooling or weighted sum to yield the final
    prediction.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 动作分类分数。生成的时间提案被输入到动作分类器中，以产生所有动作类别的概率分布。这可以表示为向量$(p^{1},\cdots,p^{C})$，其中$p^{i}$是动作类别$i$的概率，$C$是类别的数量。为了公平比较，研究人员利用早期工作的分类器SCNN-classifier
    [[19](#bib.bib19)]、UntrimmedNet [[19](#bib.bib19)]、[[20](#bib.bib20)]等。他们从视频片段中均匀采样固定数量的帧，并将其输入到ConvNets中，如C3D
    [[21](#bib.bib21)]、两个流CNN [[22](#bib.bib22)]或时间段网络[[23](#bib.bib23)]。在某些情况下，采样帧的识别分数通过Top-k池化或加权求和进行汇总，以产生最终预测。
- en: 2.2 Video Feature Encoding
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 视频特征编码
- en: Untrimmed videos are often lengthy and can be as long as several minutes, and
    thus it is difficult to directly input the entire video to a visual encoder for
    feature extraction due to the limits of computational resources. For instance,
    popular video feature extractors such as 3D-CNNs can only operate on short clips
    spanning about 4 seconds. A common strategy for video representation is to partition
    the video into equally sized temporal intervals called snippets, and then apply
    a pre-trained visual encoder over each snippet. Formally, given input video $X$
    with $l$ frames, a sequence $S$ of snippets with regular duration $\sigma$ is
    generated where
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 未裁剪的视频通常很长，可能长达几分钟，因此由于计算资源的限制，直接将整个视频输入到视觉编码器进行特征提取是困难的。例如，流行的视频特征提取器如3D-CNN只能处理约4秒钟的短片段。视频表示的一种常见策略是将视频划分为大小相等的时间间隔称为片段，然后对每个片段应用预训练的视觉编码器。正式地，给定输入视频$X$与$l$帧，一个具有常规时长$\sigma$的片段序列$S$被生成，其中
- en: '|  | $S=\{s_{n}\}_{n=1}^{l_{s}}\hskip 7.22743pt,\hskip 7.22743ptl_{s}=\frac{l}{\sigma},$
    |  | (4) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $S=\{s_{n}\}_{n=1}^{l_{s}}\hskip 7.22743pt,\hskip 7.22743ptl_{s}=\frac{l}{\sigma},$
    |  | (4) |'
- en: and $s_{n}$ is the n-th snippet. Then, each snippet is fed to a pre-trained
    visual encoder such as two-stream [[22](#bib.bib22)], C3D [[21](#bib.bib21)],
    or I3D [[24](#bib.bib24)] for feature extraction. In two-stream network [[22](#bib.bib22)],
    snippet $s_{n}$ which is centered at $t_{n}$-th frame of the video, has an RGB
    frame $x_{t_{n}}$, and a stacked optical flow $o_{t_{n}}$ derived around the center
    frame. The RGB frame $x_{t_{n}}$ is fed to spatial network ResNet [[25](#bib.bib25)],
    extracting feature vector $f_{S,n}$. The optical flow $o_{t_{n}}$ is fed to temporal
    network BN-Inception [[26](#bib.bib26)], extracting feature $f_{T,n}$. The spatial
    and temporal features, $f_{S,n}$ and $f_{T,n}$, are concatenated to represent
    the visual feature $f_{n}$ for snippet $s_{n}$.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 并且$s_{n}$是第n个片段。然后，将每个片段输入到预训练的视觉编码器中，如两个流网络[[22](#bib.bib22)]、C3D [[21](#bib.bib21)]或I3D
    [[24](#bib.bib24)]进行特征提取。在两个流网络[[22](#bib.bib22)]中，以$t_{n}$-th帧为中心的片段$s_{n}$，具有RGB帧$x_{t_{n}}$和围绕中心帧衍生的叠加光流$o_{t_{n}}$。RGB帧$x_{t_{n}}$被输入到空间网络ResNet
    [[25](#bib.bib25)]中，提取特征向量$f_{S,n}$。光流$o_{t_{n}}$被输入到时间网络BN-Inception [[26](#bib.bib26)]中，提取特征$f_{T,n}$。空间和时间特征$f_{S,n}$和$f_{T,n}$被连接以表示片段$s_{n}$的视觉特征$f_{n}$。
- en: Similarly, in I3D [[24](#bib.bib24)], a stack of RGB and optical flow frames
    from each snippet $s_{n}$ are fed to I3D network, extracting spatial and temporal
    feature vectors $f_{S,n}$ and $f_{T,n}$ which are concatenated to create feature
    $f_{n}$. In C3D [[21](#bib.bib21)], the frames of each snippet $s_{n}$ are directly
    fed to a 3D-CNN architecture to capture spatio-temporal information, and extracting
    feature vector $f_{n}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在I3D [[24](#bib.bib24)]中，每个片段$s_{n}$的一组RGB和光流帧被输入到I3D网络中，提取空间和时间特征向量$f_{S,n}$和$f_{T,n}$，这些特征被连接以创建特征$f_{n}$。在C3D
    [[21](#bib.bib21)]中，每个片段$s_{n}$的帧直接输入到3D-CNN架构中以捕捉时空信息，并提取特征向量$f_{n}$。
- en: 2.3 Action Detection with Full Supervision
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 全监督下的动作检测
- en: 'In fully-supervised action detection, the annotation (${\Psi}_{g}$ in Eq. ([1](#S2.E1
    "In Definition 1\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey"))) of temporal
    boundaries and labels of action instances are provided for each video of training
    set. During inference, the goal is to find the temporal boundaries of action instances
    and predict their labels. A main step in action detection is temporal proposal
    generation to identify the temporal intervals of the video that are likely to
    include action instances. Fully-supervised temporal proposal generation methods
    can be categorized to anchor-based and anchor-free. Anchor-based methods generate
    action proposals by assigning dense and multi-scale intervals with pre-defined
    lengths at each temporal position of the video (Section [2.3.1](#S2.SS3.SSS1 "2.3.1
    Anchor-based Proposal Generation and Evaluation ‣ 2.3 Action Detection with Full
    Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action
    Detection in Untrimmed Videos: A Survey")). Anchor-free methods often predict
    action boundary confidence or actionness scores at temporal positions of the video,
    and employ a bottom-up grouping strategy to match pairs of start and end (Section
    [2.3.2](#S2.SS3.SSS2 "2.3.2 Anchor-free Proposal Generation and Evaluation ‣ 2.3
    Action Detection with Full Supervision ‣ 2 Temporal Action Detection Methods ‣
    Deep Learning-based Action Detection in Untrimmed Videos: A Survey")). There are
    also several methods that combine the advantages of anchor-free and anchor-based
    proposal generation methods (Section [2.3.3](#S2.SS3.SSS3 "2.3.3 Anchor-based
    and Anchor-free Combination ‣ 2.3 Action Detection with Full Supervision ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")). After generating the proposals, rich features must be extracted from
    the proposals to evaluate the quality of proposals. Section [2.3.4](#S2.SS3.SSS4
    "2.3.4 Common Loss Functions for Proposal Evaluation ‣ 2.3 Action Detection with
    Full Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action
    Detection in Untrimmed Videos: A Survey") reviews common loss functions that are
    used during training for proposal evaluation. Section [2.3.5](#S2.SS3.SSS5 "2.3.5
    Modeling Long-range Dependencies ‣ 2.3 Action Detection with Full Supervision
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey") discusses modeling long-range dependencies to capture
    the relation between video segments in untrimmed videos to improve action localization.
    Finally, Section [2.3.6](#S2.SS3.SSS6 "2.3.6 Spatio-temporal Action Detection
    ‣ 2.3 Action Detection with Full Supervision ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey") summarizes
    spatio-temporal action detection methods.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全监督的动作检测中，每个训练视频都提供了动作实例的时间边界和标签的注释（${\Psi}_{g}$，见公式 ([1](#S2.E1 "在定义 1\.
    ‣ 2.1 术语定义 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未剪辑视频动作检测：一项综述"))）。在推理阶段，目标是找到动作实例的时间边界并预测其标签。动作检测的一个主要步骤是生成时间提议，以识别视频中可能包含动作实例的时间区间。完全监督的时间提议生成方法可以分为基于锚点的方法和无锚点的方法。基于锚点的方法通过在视频的每个时间位置分配密集和多尺度的预定义长度区间来生成动作提议（见第
    [2.3.1](#S2.SS3.SSS1 "2.3.1 基于锚点的提议生成与评估 ‣ 2.3 完全监督的动作检测 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未剪辑视频动作检测：一项综述")节）。无锚点的方法通常在视频的时间位置预测动作边界置信度或动作分数，并采用自下而上的分组策略来匹配开始和结束对（见第
    [2.3.2](#S2.SS3.SSS2 "2.3.2 无锚点提议生成与评估 ‣ 2.3 完全监督的动作检测 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未剪辑视频动作检测：一项综述")节）。还有一些方法结合了无锚点和基于锚点的提议生成方法的优点（见第
    [2.3.3](#S2.SS3.SSS3 "2.3.3 基于锚点和无锚点的组合 ‣ 2.3 完全监督的动作检测 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未剪辑视频动作检测：一项综述")节）。生成提议后，必须从提议中提取丰富的特征来评估提议的质量。第
    [2.3.4](#S2.SS3.SSS4 "2.3.4 提议评估的常见损失函数 ‣ 2.3 完全监督的动作检测 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未剪辑视频动作检测：一项综述")节回顾了在训练过程中用于提议评估的常见损失函数。第
    [2.3.5](#S2.SS3.SSS5 "2.3.5 建模长程依赖关系 ‣ 2.3 完全监督的动作检测 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未剪辑视频动作检测：一项综述")节讨论了建模长程依赖关系，以捕捉未剪辑视频中视频片段之间的关系，从而提高动作定位的准确性。最后，第
    [2.3.6](#S2.SS3.SSS6 "2.3.6 时空动作检测 ‣ 2.3 完全监督的动作检测 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未剪辑视频动作检测：一项综述")节总结了时空动作检测方法。
- en: 2.3.1 Anchor-based Proposal Generation and Evaluation
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 基于锚点的提议生成和评估
- en: 'Anchor-based methods, also known as top-down methods, generate temporal proposals
    by assigning dense and multi-scale intervals with pre-defined lengths to uniformly
    distributed temporal locations in the input video. Formally, given a video with
    $T$ frames, $\frac{T}{\sigma}$ temporal positions, known as anchors, are uniformly
    sampled from every $\sigma$ frames. Then, several temporal windows with different
    duration $\{d_{1},d_{2},\cdots,d_{n}\}$ are centered around each anchor as initial
    temporal proposals. The proposal lengths ($d_{i}$ s) must have a wide range to
    align with action instances of various lengths that can last from less than a
    second to several minutes in untrimmed videos [[9](#bib.bib9)]. Then visual encoders
    and convolution layers are applied on the temporal proposals for feature extraction
    and features are used to evaluate the quality of temporal proposals and adjust
    their boundaries (Section [2.3.4](#S2.SS3.SSS4 "2.3.4 Common Loss Functions for
    Proposal Evaluation ‣ 2.3 Action Detection with Full Supervision ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '基于锚点的方法，也称为自上而下的方法，通过将具有预定义长度的密集且多尺度的间隔分配到输入视频中均匀分布的时间位置来生成时间提议。形式上，给定一个具有
    $T$ 帧的视频，从每 $\sigma$ 帧均匀采样 $\frac{T}{\sigma}$ 个时间位置，称为锚点。然后，以每个锚点为中心，围绕不同持续时间
    $\{d_{1},d_{2},\cdots,d_{n}\}$ 生成多个时间窗口作为初始时间提议。提议的长度（$d_{i}$）必须具有广泛的范围，以便与各种长度的动作实例对齐，这些动作实例在未修剪的视频中可能持续不到一秒到几分钟
    [[9](#bib.bib9)]。然后，将视觉编码器和卷积层应用于时间提议以提取特征，特征用于评估时间提议的质量并调整其边界（第 [2.3.4](#S2.SS3.SSS4
    "2.3.4 Common Loss Functions for Proposal Evaluation ‣ 2.3 Action Detection with
    Full Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action
    Detection in Untrimmed Videos: A Survey") 节）。'
- en: '![Refer to caption](img/5e4da76d32e6b286cf71a2aba0bd4f13.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5e4da76d32e6b286cf71a2aba0bd4f13.png)'
- en: 'Figure 3: Anchor-based methods assign multi-scale intervals with pre-defined
    lengths at uniformly distributed temporal positions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 基于锚点的方法在均匀分布的时间位置分配具有预定义长度的多尺度间隔。'
- en: 2.3.1.1   Feature Extraction of Multi-scale Proposals
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.1.1   多尺度提议的特征提取
- en: As mentioned earlier, temporal proposals have very diverse time spans to align
    with action instances. However, fixed-size features must be extracted from each
    proposal to be fed to fully connected layers for proposal evaluation (action classification
    and regression). Here, we review different strategies to extract fixed-size features
    from proposals with different lengths.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，时间提议具有非常多样化的时间跨度，以便与动作实例对齐。然而，必须从每个提议中提取固定大小的特征，以便将其输入到全连接层中进行提议评估（动作分类和回归）。在这里，我们回顾了从不同长度的提议中提取固定大小特征的不同策略。
- en: 'Sampling and Feature Concatenation: Shou et al. in SCNN [[19](#bib.bib19)]
    uniformly sampled a fixed number of frames from each proposal and fed them to
    a visual encoder for feature extraction. This is not computationally efficient
    because there are many overlapping proposals and overlapping segments are processed
    multiple times. To address this problem, Gao et al. in Turn-Tap [[27](#bib.bib27)]
    and CBR [[28](#bib.bib28)] decomposed the video into non-overlapping equal-length
    units and extracted the features of each unit only once. Different numbers of
    consecutive units are grouped together at each anchor unit to generate multi-scale
    proposals. To obtain the proposal features, the features of all units are concatenated.
    Using this approach, the proposal features are computed from unit-level features,
    which are calculated only once. However, concatenation of features within each
    proposal or sampling frames do not lead to rich feature extraction.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 采样和特征拼接：Shou 等人在 SCNN [[19](#bib.bib19)] 中从每个提议中均匀采样固定数量的帧，并将其输入到视觉编码器中进行特征提取。这种方法在计算上效率较低，因为存在许多重叠的提议，并且重叠的段被多次处理。为了解决这个问题，Gao
    等人在 Turn-Tap [[27](#bib.bib27)] 和 CBR [[28](#bib.bib28)] 中将视频分解成非重叠的等长单元，并仅提取每个单元的特征。不同数量的连续单元在每个锚点单元处被组合在一起以生成多尺度提议。为了获得提议特征，所有单元的特征被拼接在一起。使用这种方法，提议特征是从单元级特征计算的，这些特征仅计算一次。然而，在每个提议中拼接特征或采样帧并不会导致丰富的特征提取。
- en: '3D RoI Pooling: This approach extracts fixed size features from multi-scale
    proposals using 3D RoI pooling. Specifically, an input feature volume of size
    $l\times h\times w$ ( $l$ for temporal dimension, $h$ for height and $w$ for width
    dimensions) is divided into $l_{s}\times h_{s}\times w_{s}$ sub-volumes (where
    $l_{s},h_{s},$ and $w_{s}$ are fixed), and max pooling is performed inside each
    sub-volume. Therefore, proposals of various lengths generate output volume features
    of the same size, which is $d\times l_{s}\times h_{s}\times w_{s}$, where $d$
    is the channel dimension. The idea of 3D RoI pooling for action detection is an
    extension of the 2D RoI pooling for object detection in Faster R-CNN [[29](#bib.bib29)].
    This idea was first introduced in R-C3D[[30](#bib.bib30)] and used in other frameworks
    such as AGCN [[31](#bib.bib31)] and AFNet [[32](#bib.bib32)]. The limitation of
    this approach is that the multi-scale proposals at each location share the same
    receptive field, which may be too small or too large for some anchor scales.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 3D RoI池化：这种方法通过使用3D RoI池化从多尺度提议中提取固定大小的特征。具体来说，一个大小为$l\times h\times w$的输入特征体（$l$为时间维度，$h$为高度维度，$w$为宽度维度）被划分为$l_{s}\times
    h_{s}\times w_{s}$个子体积（其中$l_{s}, h_{s}$和$w_{s}$是固定的），并在每个子体积内进行最大池化。因此，各种长度的提议生成的输出体积特征大小相同，为$d\times
    l_{s}\times h_{s}\times w_{s}$，其中$d$为通道维度。3D RoI池化用于动作检测的理念是对Faster R-CNN中2D RoI池化的扩展[[29](#bib.bib29)]。这个理念最早在R-C3D[[30](#bib.bib30)]中提出，并被应用于其他框架，如AGCN
    [[31](#bib.bib31)]和AFNet [[32](#bib.bib32)]。这种方法的局限性在于每个位置的多尺度提议共享相同的感受野，这对于某些锚点尺度可能过小或过大。
- en: 2.3.1.2   Receptive Field Alignment with Proposal Span
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.1.2   感受野与提议跨度的对齐
- en: To address the variation of action duration, multi-scale anchors are assigned
    to each temporal location of the video. Before receptive field alignment, multi-scale
    anchors at any position share the same receptive field size. This is problematic
    because if the receptive field is too small or too large with respect to the anchor
    size, the extracted feature may not contain sufficient information or include
    too much irrelevant information. Here, we review the strategies to align the receptive
    field size with proposal span.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决动作持续时间的变化问题，多尺度锚点被分配到视频的每个时间位置。在感受野对齐之前，任何位置的多尺度锚点共享相同的感受野大小。这是有问题的，因为如果感受野相对于锚点大小过小或过大，提取的特征可能无法包含足够的信息或包含过多无关信息。在这里，我们回顾了将感受野大小与提议跨度对齐的策略。
- en: 'Multi-tower Network: TAL-Net [[13](#bib.bib13)] proposed a multi-tower network,
    compose of several temporal convNets, each one responsible for a certain anchor-size.
    Then, the receptive field of each anchor segment was aligned with its temporal
    span using dilated temporal convolutions. This idea was also used in TSA-Net [[33](#bib.bib33)].
    However, assigning pre-defined temporal intervals limits the accuracy of generated
    proposals.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 多塔网络：TAL-Net [[13](#bib.bib13)] 提出了一个多塔网络，由几个时间卷积网络组成，每个网络负责一个特定的锚点大小。然后，通过使用扩张的时间卷积将每个锚点段的感受野与其时间跨度对齐。这个想法也被用于TSA-Net
    [[33](#bib.bib33)]。然而，预定义时间间隔的分配限制了生成提议的准确性。
- en: 'Temporal Feature Pyramid Network: In a temporal feature pyramid network (TFPN),
    the predictions are yielded from multiple resolution feature maps. This idea was
    first introduced in Single Shot Detector (SSD) [[34](#bib.bib34)] for object detection,
    and then extended to temporal domain for action detection in SSAD [[35](#bib.bib35)]
    and $\text{S}^{3}\text{D}$ [[36](#bib.bib36)]. They proposed an end-to-end network
    where the lower-level feature maps with higher resolution and smaller receptive
    field are responsible to detect short action instances while the top layers with
    lower resolution and larger receptive field, detect long action instances. For
    each feature map cell, several anchor segments with multiple scales are considered
    around the center that are fed to convolutional layers for evaluation. The limitation
    of this approach is that lower layers in the pyramid are unaware of high-level
    semantic information, and top layers lack enough details, so they all fail to
    localize the actions accurately.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 时间特征金字塔网络：在时间特征金字塔网络 (TFPN) 中，预测是从多个分辨率特征图中得出的。这个想法最初在用于目标检测的单次检测器 (SSD) [[34](#bib.bib34)]
    中提出，随后在 SSAD [[35](#bib.bib35)] 和 $\text{S}^{3}\text{D}$ [[36](#bib.bib36)] 中扩展到时间领域进行动作检测。它们提出了一种端到端的网络，其中较低层的特征图具有更高的分辨率和较小的感受野，负责检测短时间动作实例，而顶部层具有较低的分辨率和更大的感受野，用于检测长时间动作实例。对于每个特征图单元，考虑中心周围的多个尺度的锚点段，并将其送入卷积层进行评估。这种方法的局限性在于金字塔中的低层对高级语义信息不了解，而高层缺乏足够的细节，因此都无法准确定位动作。
- en: 'U-shaped Temporal Feature Pyramid Network: In order to mitigate the problem
    with regular TFPNs, a U-shaped TFPN architecture was designed to connect high-level
    and low-level features. This idea was first introduced in Unet [[37](#bib.bib37)],
    FPN [[38](#bib.bib38)], and DSSD [[39](#bib.bib39)] for object detection and then
    was generalized to temporal domain in MGG [[40](#bib.bib40)], PBRNet [[41](#bib.bib41)],
    RapNet [[42](#bib.bib42)], C-TCN [[43](#bib.bib43)], and MLTPN [[44](#bib.bib44)].
    The video representation features are extracted using off-the-shelf feature extractors.
    Then temporal convolution and max pooling layers are applied to reduce the temporal
    dimension and increase the receptive field size. This is followed by temporal
    deconvolution layers for upscaling. Then, high-level features are combined with
    corresponding low-level features with lateral connections between the convolutional
    and deconvolutional layers. U-shaped TFPNs have drawn much attention recently
    and achieved state-of-the art results for temporal action detection task.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: U-shaped 时间特征金字塔网络：为了缓解常规 TFPN 的问题，设计了一种 U-shaped TFPN 架构来连接高级和低级特征。这个想法最初在用于目标检测的
    Unet [[37](#bib.bib37)]、FPN [[38](#bib.bib38)] 和 DSSD [[39](#bib.bib39)] 中提出，随后在
    MGG [[40](#bib.bib40)]、PBRNet [[41](#bib.bib41)]、RapNet [[42](#bib.bib42)]、C-TCN
    [[43](#bib.bib43)] 和 MLTPN [[44](#bib.bib44)] 中推广到时间领域。视频表示特征使用现成的特征提取器提取。接着应用时间卷积和最大池化层以减少时间维度并增加感受野大小。随后，通过时间反卷积层进行上采样。然后，高级特征通过卷积层和反卷积层之间的侧向连接与对应的低级特征结合。U-shaped
    TFPN 最近引起了广泛关注，并在时间动作检测任务中取得了**最先进**的结果。
- en: 2.3.2 Anchor-free Proposal Generation and Evaluation
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 无锚点提案生成与评估
- en: Anchor-free methods employ a bottom-up grouping strategy for proposal generation
    based on predicted boundary probability or actionness scores at temporal positions
    of the video. Anchor-free methods are capable to generate proposals with precise
    boundary and flexible duration because the proposal lengths are not predefined.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 无锚点方法采用自下而上的分组策略，根据视频的时间位置预测的边界概率或动作性分数生成提案。无锚点方法能够生成具有精准边界和灵活持续时间的提案，因为提案长度不是预定义的。
- en: 2.3.2.1   Proposal Generation with Actionness Scores
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.2.1   基于动作性分数的提案生成
- en: 'Zhao et al. in SSN [[18](#bib.bib18)] proposed to identify continuous temporal
    regions with high actionness scores (def [6](#Thmdefinition6 "Definition 6\. ‣
    2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")) as proposals (known as TAG proposals).
    Continuous temporal regions are grouped using a classic watershed algorithm [[45](#bib.bib45)]
    applied on the 1D signal formed by complemented actionness values. The proposals
    are fed to a temporal pyramid for feature extraction and proposal evaluation.
    The feature extraction process is too simple to capture rich features.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人在 SSN [[18](#bib.bib18)] 中提出了识别具有高动作性评分（def [6](#Thmdefinition6 "定义 6\.
    ‣ 2.1 术语定义 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未裁剪视频动作检测：一项调查")）的连续时间区域作为提案（称为 TAG 提案）。连续时间区域通过对由补充动作性值形成的
    1D 信号应用经典的分水岭算法 [[45](#bib.bib45)] 进行分组。提案被送入时间金字塔进行特征提取和提案评估。特征提取过程过于简单，无法捕捉丰富的特征。
- en: 2.3.2.2   Proposal Generation with Boundary Scores
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.2.2   带有边界评分的提案生成
- en: 'These methods predict three probability signals for actionness (def [6](#Thmdefinition6
    "Definition 6\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣
    Deep Learning-based Action Detection in Untrimmed Videos: A Survey")), startness
    and endness scores (def [7](#Thmdefinition7 "Definition 7\. ‣ 2.1 Term Definition
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey")). They generate temporal proposals by matching the
    temporal positions that are likely to be the start or end of an action (peak of
    startness and endness signals). In BSN [[46](#bib.bib46)] proposal features are
    constructed by concatenation of a fixed number of points, sampled from the actionness
    scores (def [6](#Thmdefinition6 "Definition 6\. ‣ 2.1 Term Definition ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")) by linear interpolation. BSN ignores the global information for actions
    with blurred boundaries, causing unreliable confidence scores. Also, proposal
    features are too weak to capture enough temporal context, and the feature construction
    and confidence evaluation are performed for each proposal separately, which is
    inefficient. BMN [[47](#bib.bib47)] explores the global context for simultaneously
    evaluating all proposals end-to-end. They construct a feature map by aggregating
    the features of all proposals together. The feature map is fed to convolution
    layers to simultaneously evaluate all proposals. The advantage of this approach
    is to extract rich feature and temporal context for each proposal and exploit
    the context of adjacent proposals. Also, proposal evaluation is very fast during
    inference. However, they use the same method as BSN [[46](#bib.bib46)] to generate
    boundary probabilities (start and end) which ignores the global information for
    actions with blurred boundaries. DBG [[48](#bib.bib48)] simultaneously evaluates
    all proposals to explore global context and extract rich features similar to BMN
    [[47](#bib.bib47)]. Moreover, instead of only exploiting the local information
    to predict boundary probabilities (probability of start and end), DBG proposed
    to employ global proposal-level features.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法预测了三种动作性（def [6](#Thmdefinition6 "定义 6\. ‣ 2.1 术语定义 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未裁剪视频动作检测：一项调查")）、起始性和结束性评分（def
    [7](#Thmdefinition7 "定义 7\. ‣ 2.1 术语定义 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未裁剪视频动作检测：一项调查")）的概率信号。它们通过匹配可能是动作开始或结束的时间位置（即起始性和结束性信号的峰值）生成时间提案。在
    BSN [[46](#bib.bib46)] 中，提案特征通过线性插值将从动作性评分（def [6](#Thmdefinition6 "定义 6\. ‣ 2.1
    术语定义 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未裁剪视频动作检测：一项调查")）中采样的固定数量点进行拼接构造。BSN 忽略了模糊边界动作的全局信息，导致置信度评分不可靠。此外，提案特征过于薄弱，无法捕捉足够的时间上下文，特征构造和置信度评估对每个提案单独进行，效率低下。BMN
    [[47](#bib.bib47)] 探索了全局上下文以端到端地同时评估所有提案。它们通过聚合所有提案的特征来构造特征图。特征图输入到卷积层中，以同时评估所有提案。这种方法的优点是为每个提案提取丰富的特征和时间上下文，并利用相邻提案的上下文。此外，在推理过程中，提案评估非常快速。然而，它们使用与
    BSN [[46](#bib.bib46)] 相同的方法生成边界概率（起始和结束），这忽略了模糊边界动作的全局信息。DBG [[48](#bib.bib48)]
    同时评估所有提案以探索全局上下文，并提取类似于 BMN [[47](#bib.bib47)] 的丰富特征。此外，DBG 提出了利用全局提案级特征，而不仅仅是利用局部信息来预测边界概率（起始和结束的概率）。
- en: '![Refer to caption](img/f323bae974c1d90abaf66da7a3ec68e9.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f323bae974c1d90abaf66da7a3ec68e9.png)'
- en: 'Figure 4: Anchor-free proposal generation with boundary matching. These methods
    predict action boundary probabilities at uniformly distributed temporal positions
    and match the start and end points with high probabilities as proposals.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：无锚点的提议生成与边界匹配。这些方法在均匀分布的时间位置预测动作边界概率，并将起点和终点与高概率的提议匹配。
- en: In order to model the relations between the boundary and action content of temporal
    proposals, BC-GNN [[49](#bib.bib49)] proposed a graph neural network where boundaries
    and content of proposals are taken as the nodes and edges of the graph, and their
    features are updated through graph operations. Then the updated edges and nodes
    are used to predict boundary probabilities and content confidence score to generate
    proposals. A2Net [[50](#bib.bib50)] and AFSD [[51](#bib.bib51)] visit the anchor-free
    mechanism, where the network predicts the distance to the temporal boundaries
    for each temporal location in the feature sequence. AFSD [[51](#bib.bib51)] also
    proposes a novel boundary refinement strategy for precise temporal localization.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建模时间提议的边界和动作内容之间的关系，BC-GNN [[49](#bib.bib49)] 提出了一个图神经网络，其中提议的边界和内容作为图的节点和边，特征通过图操作进行更新。然后，更新后的边和节点用于预测边界概率和内容置信度分数，以生成提议。A2Net
    [[50](#bib.bib50)] 和 AFSD [[51](#bib.bib51)] 探索了无锚点机制，其中网络预测特征序列中每个时间位置到时间边界的距离。AFSD
    [[51](#bib.bib51)] 还提出了一种新的边界细化策略，用于精确的时间定位。
- en: 2.3.3 Anchor-based and Anchor-free Combination
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 基于锚点和无锚点的组合
- en: Anchor-based methods consider segments of various lengths as initial proposals
    at regularly distributed temporal positions of the video. However, because the
    segment sizes are designed beforehand, they cannot accurately predict the temporal
    boundary of actions. Also, because the duration of action instances varies from
    seconds to minutes, covering all ground-truth instances with anchor-based methods
    is computationally expensive. Anchor-free methods predict action boundary confidence
    or actionness score at all temporal positions of the video, and employ a bottom-up
    grouping strategy to match pairs of start and end. Anchor-free methods are capable
    to generate proposals with precise boundaries and flexible duration. However,
    in some cases they only exploit local context to extract the boundary information.
    Therefore, they are sensitive to noise, likely to produce incomplete proposals,
    and fail to yield robust detection results.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基于锚点的方法将各种长度的片段视为视频中规则分布时间位置的初始提议。然而，由于片段大小是预先设计的，它们不能准确预测动作的时间边界。此外，由于动作实例的持续时间从秒到分钟不等，使用基于锚点的方法覆盖所有真实实例计算开销很大。无锚点的方法在视频的所有时间位置预测动作边界置信度或动作性分数，并采用自下而上的分组策略来匹配起点和终点。无锚点的方法能够生成具有精确边界和灵活持续时间的提议。然而，在某些情况下，它们仅利用局部上下文来提取边界信息。因此，它们对噪声敏感，容易产生不完整的提议，并且难以产生鲁棒的检测结果。
- en: 'Several methods such as CTAP [[52](#bib.bib52)], MGG [[40](#bib.bib40)], PBRNet
    [[41](#bib.bib41)], RapNet [[42](#bib.bib42)] balance the advantages and disadvantages
    between anchor-based and anchor-free approaches for proposal generation. CTAP
    [[52](#bib.bib52)] designed a complementary filter applied on the initial proposals
    to generate the probabilities of proposal detection by anchor-free TAG [[18](#bib.bib18)]
    (defined in [2.3.2](#S2.SS3.SSS2 "2.3.2 Anchor-free Proposal Generation and Evaluation
    ‣ 2.3 Action Detection with Full Supervision ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")). The original
    use of complementary filtering is to estimate a signal given two noisy measurements,
    where one of them is mostly high-frequency (maybe precise but not stable) similar
    to TAG proposals and the other one is mostly low-frequency (stable but not precise)
    similar to sliding-window proposals. Also, several temporal feature pyramid networks
    (TFPN, defined in [2.3.1](#S2.SS3.SSS1 "2.3.1 Anchor-based Proposal Generation
    and Evaluation ‣ 2.3 Action Detection with Full Supervision ‣ 2 Temporal Action
    Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")) such as MGG [[40](#bib.bib40)], PBRNet  [[41](#bib.bib41)], and RapNet
     [[42](#bib.bib42)] generate coarse segment proposals of various length with TFPN
    (anchor-based), and simultaneously predict fine-level frame actionness (anchor-free).
    The advantage of this idea is to adjust the segment boundary of proposals with
    frame actionness information during the inference.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '一些方法如 CTAP [[52](#bib.bib52)]、MGG [[40](#bib.bib40)]、PBRNet [[41](#bib.bib41)]、RapNet
    [[42](#bib.bib42)] 在提案生成中平衡了基于锚点的方法和无锚点方法的优缺点。CTAP [[52](#bib.bib52)] 设计了一种应用于初始提案的互补滤波器，用于生成由无锚点
    TAG [[18](#bib.bib18)]（定义在 [2.3.2](#S2.SS3.SSS2 "2.3.2 Anchor-free Proposal Generation
    and Evaluation ‣ 2.3 Action Detection with Full Supervision ‣ 2 Temporal Action
    Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")）进行检测的概率。互补滤波器的原始用途是根据两个噪声测量值来估计信号，其中一个测量值通常是高频（可能精确但不稳定）类似于 TAG 提案，另一个测量值通常是低频（稳定但不精确）类似于滑动窗口提案。此外，一些时间特征金字塔网络（TFPN，定义在
    [2.3.1](#S2.SS3.SSS1 "2.3.1 Anchor-based Proposal Generation and Evaluation ‣
    2.3 Action Detection with Full Supervision ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")）如 MGG [[40](#bib.bib40)]、PBRNet
    [[41](#bib.bib41)] 和 RapNet [[42](#bib.bib42)] 使用 TFPN（基于锚点）生成不同长度的粗略分段提案，并同时预测精细级别的帧行动性（无锚点）。这种方法的优点是在推理过程中根据帧行动性信息调整提案的段边界。'
- en: 2.3.4 Common Loss Functions for Proposal Evaluation
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 提案评估中的常见损失函数
- en: 'After generating the temporal proposals, rich features are extracted from the
    proposals to evaluate their quality. Several convolutional layers are applied
    on the features to predict actionness score (def [6](#Thmdefinition6 "Definition
    6\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")), completeness score (def [8](#Thmdefinition8
    "Definition 8\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣
    Deep Learning-based Action Detection in Untrimmed Videos: A Survey")), classification
    score (def [9](#Thmdefinition9 "Definition 9\. ‣ 2.1 Term Definition ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")), and to adjust the temporal boundary of the proposals. Here, we review
    common loss functions that are used during training to supervise these predicted
    scores and evaluate the quality of proposals.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '在生成时间提案后，从提案中提取丰富的特征以评估其质量。对特征应用几个卷积层以预测行动性得分（定义 [6](#Thmdefinition6 "Definition
    6\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")）、完整性得分（定义 [8](#Thmdefinition8
    "Definition 8\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣
    Deep Learning-based Action Detection in Untrimmed Videos: A Survey")）、分类得分（定义
    [9](#Thmdefinition9 "Definition 9\. ‣ 2.1 Term Definition ‣ 2 Temporal Action
    Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")），并调整提案的时间边界。在这里，我们回顾了在训练过程中用于监督这些预测得分并评估提案质量的常见损失函数。'
- en: Definition 10.
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 10。
- en: 'Actionness loss. This is a binary cross-entropy loss that classifies the temporal
    proposals as action or background. Given $N$ proposals, this loss is defined as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 行动性损失。这是一种二元交叉熵损失，用于将时间提案分类为动作或背景。给定$N$个提案，该损失定义为：
- en: '|  | $L_{\text{act}}=-\frac{1}{N}\sum_{i=1}^{N}b_{i}\log(a_{i})+(1-b_{i})\log(1-a_{i}),$
    |  | (5) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\text{act}}=-\frac{1}{N}\sum_{i=1}^{N}b_{i}\log(a_{i})+(1-b_{i})\log(1-a_{i}),$
    |  | (5) |'
- en: 'where $a_{i}$ is the predicted actionness score (def [6](#Thmdefinition6 "Definition
    6\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")), and $b_{i}\in\{0,1\}$ is a
    binary ground-truth label for the $i$-th proposal. If the proposal is positive
    (def [4](#Thmdefinition4 "Definition 4\. ‣ 2.1 Term Definition ‣ 2 Temporal Action
    Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")), then $b_{i}=1$. Otherwise, $b_{i}=0$.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a_{i}$ 是预测的动作性评分（定义 [6](#Thmdefinition6 "定义 6\. ‣ 2.1 术语定义 ‣ 2 时间动作检测方法
    ‣ 深度学习基础的未裁剪视频中的动作检测：综述")），$b_{i}\in\{0,1\}$ 是第 $i$ 个提议的二进制真实标签。如果提议是正样本（定义 [4](#Thmdefinition4
    "定义 4\. ‣ 2.1 术语定义 ‣ 2 时间动作检测方法 ‣ 深度学习基础的未裁剪视频中的动作检测：综述")），则 $b_{i}=1$。否则，$b_{i}=0$。
- en: Definition 11.
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 11。
- en: 'Action completeness loss. Given $N$ proposals, the completeness loss is defined
    as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 动作完整性损失。给定 $N$ 个提议，完整性损失定义为：
- en: '|  | $L_{\text{com}}=\frac{1}{N_{\text{pos}}}\sum_{i=1}^{N}d(c_{i},g_{i})\cdot[l_{i}>0],$
    |  | (6) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\text{com}}=\frac{1}{N_{\text{pos}}}\sum_{i=1}^{N}d(c_{i},g_{i})\cdot[l_{i}>0],$
    |  | (6) |'
- en: 'where $c_{i}$ is the predicted action completeness score (def [8](#Thmdefinition8
    "Definition 8\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣
    Deep Learning-based Action Detection in Untrimmed Videos: A Survey")) for $i$-th
    proposal, and $g_{i}$ is the ground-truth action completeness score. $d$ is a
    distance metric which is often $L_{2}$ or smooth $L_{1}$ loss. $l_{i}$ is the
    label of the $i$-th proposal and condition $[l_{i}>0]$ implies that action completeness
    is only considered for positive proposals (def [4](#Thmdefinition4 "Definition
    4\. ‣ 2.1 Term Definition ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")). $N_{\text{pos}}$ is the number
    of positive proposals during each mini-batch.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c_{i}$ 是第 $i$ 个提议的预测动作完整性评分（定义 [8](#Thmdefinition8 "定义 8\. ‣ 2.1 术语定义 ‣
    2 时间动作检测方法 ‣ 深度学习基础的未裁剪视频中的动作检测：综述")），$g_{i}$ 是真实动作完整性评分。$d$ 是一种距离度量，通常为 $L_{2}$
    或平滑的 $L_{1}$ 损失。$l_{i}$ 是第 $i$ 个提议的标签，条件 $[l_{i}>0]$ 表示动作完整性仅对正样本提议进行考虑（定义 [4](#Thmdefinition4
    "定义 4\. ‣ 2.1 术语定义 ‣ 2 时间动作检测方法 ‣ 深度学习基础的未裁剪视频中的动作检测：综述")）。$N_{\text{pos}}$ 是每个小批次中的正样本提议数量。
- en: Definition 12.
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 12。
- en: 'Action overlap loss. This is another variation of action completeness loss
    which rewards the proposals with higher temporal overlap with ground truths and
    is defined as the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 动作重叠损失。这是动作完整性损失的另一种变体，它对与真实值有更高时间重叠的提议给予奖励，定义如下：
- en: '|  | $\mathcal{L}_{overlap}=\frac{1}{N_{\text{pos}}}\sum_{i}\frac{1}{2}\cdot\Big{(}\frac{(p^{l_{i}}_{i})^{2}}{(g_{i})^{\alpha}}-1\Big{)}\cdot[l_{i}>0],$
    |  | (7) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{overlap}=\frac{1}{N_{\text{pos}}}\sum_{i}\frac{1}{2}\cdot\Big{(}\frac{(p^{l_{i}}_{i})^{2}}{(g_{i})^{\alpha}}-1\Big{)}\cdot[l_{i}>0],$
    |  | (7) |'
- en: 'where $p_{i}$ is the classification probability vector over action labels for
    the $i$-th proposal, and $p^{l_{i}}_{i}$ is the probability of action class $l_{i}$.
    Other notations are the same as in $L_{\text{com}}$ (def [11](#Thmdefinition11
    "Definition 11\. ‣ 2.3.4 Common Loss Functions for Proposal Evaluation ‣ 2.3 Action
    Detection with Full Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")) and $\alpha$ is a hyper-parameter.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{i}$ 是第 $i$ 个提议的动作标签分类概率向量，$p^{l_{i}}_{i}$ 是动作类别 $l_{i}$ 的概率。其他符号与 $L_{\text{com}}$
    相同（定义 [11](#Thmdefinition11 "定义 11\. ‣ 2.3.4 提议评估的常见损失函数 ‣ 2.3 完全监督的动作检测 ‣ 2 时间动作检测方法
    ‣ 深度学习基础的未裁剪视频中的动作检测：综述")），$\alpha$ 是一个超参数。
- en: Definition 13.
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 13。
- en: 'Action classification loss. This is the classification (cross-entropy) loss
    and the probability distribution is over all action classes as well as temporal
    background as the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 动作分类损失。这是分类（交叉熵）损失，概率分布覆盖所有动作类别以及时间背景，如下所示：
- en: '|  | $L_{\text{cls}}=-\frac{1}{N}\sum_{i=1}^{N}\log(p^{l_{i}}_{i}),$ |  | (8)
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\text{cls}}=-\frac{1}{N}\sum_{i=1}^{N}\log(p^{l_{i}}_{i}),$ |  | (8)
    |'
- en: where $l_{i}\in\{0,1,\cdots,C\}$ is the label of $i$-th proposal, and $p^{l_{i}}_{i}$
    is the probability of class $l_{i}$.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l_{i}\in\{0,1,\cdots,C\}$ 是第 $i$ 个提议的标签，$p^{l_{i}}_{i}$ 是类别 $l_{i}$ 的概率。
- en: Definition 14.
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 14。
- en: 'Action regression loss. To adjust the temporal boundary of proposals, the start
    and end offset of proposals are predicted and supervised by a regression loss
    as the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 动作回归损失。为了调整提议的时间边界，预测并通过回归损失对提议的开始和结束偏移量进行监督，如下所示：
- en: '|  | $L_{\text{reg}}=\frac{1}{N_{\text{pos}}}\sum_{i=1}^{N}&#124;(o_{s,i}-o^{\star}_{s,i})+(o_{e,i}-o^{\star}_{e,i})&#124;\cdot[l_{i}>0],$
    |  | (9) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\text{reg}}=\frac{1}{N_{\text{pos}}}\sum_{i=1}^{N}&#124;(o_{s,i}-o^{\star}_{s,i})+(o_{e,i}-o^{\star}_{e,i})&#124;\cdot[l_{i}>0],$
    |  | (9) |'
- en: 'where term $o_{s,i}$ is the difference between the start coordinate of $i$-th
    proposal and the start coordinate of the closest ground truth action instance.
    The term $o^{\star}_{s,i}$ is the predicted offset. Similarly, $o_{e,i}$ and $o^{\star}_{e,i}$
    are the ground-truth and predicted offset for end coordinate of the $i$-th proposal.
    The condition $[l_{i}>0]$ implies that boundary adjustment is only considered
    for positive proposals (def [4](#Thmdefinition4 "Definition 4\. ‣ 2.1 Term Definition
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey")).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中项 $o_{s,i}$ 是第 $i$ 个提议的开始坐标与最接近的真实动作实例的开始坐标之间的差值。项 $o^{\star}_{s,i}$ 是预测的偏移量。类似地，$o_{e,i}$
    和 $o^{\star}_{e,i}$ 是第 $i$ 个提议结束坐标的真实和预测偏移量。条件 $[l_{i}>0]$ 表示边界调整仅考虑正提议（参考 [4](#Thmdefinition4
    "定义 4\. ‣ 2.1 术语定义 ‣ 2 时间动作检测方法 ‣ 基于深度学习的未裁剪视频动作检测：综述")）。
- en: 2.3.5 Modeling Long-range Dependencies
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5 建模长距离依赖
- en: As mentioned earlier, untrimmed videos are often lengthy and must be partitioned
    into shorter clips for feature extraction. Processing these shorter clips independently
    can lead to loss of temporal or semantic dependencies between video segments.
    Therefore, several tools such as recurrent neural networks, graph convolutions,
    attention mechanism and transformers are used to capture these dependencies. The
    advantage of modeling dependencies is to refine the temporal boundary of proposals
    or predict their action category or action completeness given the information
    from other neighboring proposals.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，未裁剪的视频通常较长，需要分割成较短的片段以进行特征提取。独立处理这些较短的片段可能会导致视频片段之间的时间或语义依赖丧失。因此，使用了几种工具，如循环神经网络、图卷积、注意力机制和变换器，以捕捉这些依赖。建模依赖的优势在于可以细化提议的时间边界，或者根据其他邻近提议的信息预测其动作类别或动作完整性。
- en: 2.3.5.1   Recurrent Neural Networks
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.5.1   循环神经网络
- en: 'RNNs are used for sequence modeling and are capable of capturing long-term
    dependencies in videos. Buch et al. in Sst [[53](#bib.bib53)] and SS-TAD [[54](#bib.bib54)]
    used RNNs for action detection. They partition the video into non-overlapping
    equal-length segments and feed each segment to a visual encoder for feature extraction.
    At time $t$, visual feature $f_{t}$ and the hidden state of the previous time
    step ($h_{t-1}$) are fed to a Gated Recurrent Unit (GRU)-based architecture to
    produce hidden state $h_{t}$. This hidden state is then fed to fully connected
    layers to evaluate multi-scale proposals at time $t$ by producing actionness scores
    (def [6](#Thmdefinition6 "Definition 6\. ‣ 2.1 Term Definition ‣ 2 Temporal Action
    Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")). In an earlier work, Yuan et al. in PSDF [[55](#bib.bib55)] captured
    the motion information over multiple resolutions and utilized RNNs to improve
    inter-frame consistency. Yeung et al. learn decision policies for an RNN-based
    agent [[56](#bib.bib56)], and later proposed an LSTM model to process multiple
    input frames with temporal attention mechanism [[57](#bib.bib57)]. LSTMs are also
    used in other frameworks such as [[58](#bib.bib58)], [[59](#bib.bib59)], [[60](#bib.bib60)]
    to evaluate temporal proposals. The advantage of using RNNs is that hidden state
    at time $t$ encodes the information from previous time steps which is useful to
    capture temporal dependencies. However, RNNs are not capable to encode very long
    videos as the hidden vector gets saturated after some time steps.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'RNN 被用于序列建模，并且能够捕捉视频中的长期依赖性。Buch 等人在 Sst [[53](#bib.bib53)] 和 SS-TAD [[54](#bib.bib54)]
    中使用 RNN 进行动作检测。他们将视频划分为非重叠的等长片段，并将每个片段输入视觉编码器以提取特征。在时间 $t$，视觉特征 $f_{t}$ 和前一个时间步骤的隐藏状态
    ($h_{t-1}$) 被输入到基于门控循环单元（GRU）的架构中以生成隐藏状态 $h_{t}$。然后，这个隐藏状态被输入到全连接层中，通过产生动作性分数（定义
    [6](#Thmdefinition6 "Definition 6\. ‣ 2.1 Term Definition ‣ 2 Temporal Action
    Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")）来评估时间 $t$ 的多尺度提议。在早期的工作中，Yuan 等人在 PSDF [[55](#bib.bib55)] 中捕捉了多个分辨率上的运动信息，并利用
    RNN 改善了帧间一致性。Yeung 等人学习了基于 RNN 的代理的决策策略 [[56](#bib.bib56)]，并随后提出了一种 LSTM 模型，以时间注意机制处理多个输入帧
    [[57](#bib.bib57)]。LSTM 还在其他框架中被使用，如 [[58](#bib.bib58)]、[[59](#bib.bib59)]、[[60](#bib.bib60)]
    以评估时间提议。使用 RNN 的优点是时间 $t$ 的隐藏状态编码了来自先前时间步骤的信息，这有助于捕捉时间依赖性。然而，RNN 无法编码非常长的视频，因为隐藏向量在一些时间步骤后会饱和。'
- en: '![Refer to caption](img/98af4c177743295055174e1787fbfce8.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98af4c177743295055174e1787fbfce8.png)'
- en: 'Figure 5: Capturing temporal dependencies in untrimmed videos with RNNs. Hidden
    state at time $t$, $h_{t}$, encodes the information from previous time steps.
    This picture is regenerated from [[53](#bib.bib53)].'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用 RNN 捕捉未裁剪视频中的时间依赖性。时间 $t$ 的隐藏状态 $h_{t}$ 编码了来自先前时间步骤的信息。这张图片是从 [[53](#bib.bib53)]
    中重新生成的。
- en: 2.3.5.2   Graph Models
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.5.2   图模型
- en: A full action often consists of several sub-actions that may independently be
    detected in several overlapping proposals. Based on this observation, Zeng et
    al. in PGCN [[61](#bib.bib61)] captured proposal-proposal relations by applying
    graph-convolution networks (GCNs). They constructed a graph where the nodes are
    the proposals. The edges connect highly overlapped proposals as well as disjoint
    but nearby proposals to provide contextual information. The edge weights model
    the relation between the proposals by measuring cosine similarity of their features.
    Through graph convolutions feature of each proposal gets updated by aggregating
    the information from other proposals. The updated features are then used to predict
    action categories, completeness, and refining the boundaries.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的动作通常由多个子动作组成，这些子动作可能在几个重叠的提议中被独立检测。基于这一观察，Zeng 等人在 PGCN [[61](#bib.bib61)]
    中通过应用图卷积网络（GCNs）捕捉了提议之间的关系。他们构建了一个图，其中节点是提议，边连接高度重叠的提议以及不相交但相邻的提议，以提供上下文信息。边权重通过测量提议特征的余弦相似度来建模提议之间的关系。通过图卷积，每个提议的特征通过聚合来自其他提议的信息得到更新。更新后的特征随后用于预测动作类别、完整性，并精炼边界。
- en: '![Refer to caption](img/b91c1515255fd298e7725a014626ef5a.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b91c1515255fd298e7725a014626ef5a.png)'
- en: 'Figure 6: Modeling proposal-proposal relations with graph convolutional networks
    (GCNs), where the nodes are the proposals and the edges model the relations between
    proposals. The feature of proposal $p_{3}$ is influenced by the features of proposals
    $p_{1},p_{2}$, and $p_{4}$. Image is reproduced from PGCN [[61](#bib.bib61)].'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：使用图卷积网络 (GCNs) 建模提议-提议关系，其中节点是提议，边缘建模提议之间的关系。提议 $p_{3}$ 的特征受到提议 $p_{1},p_{2}$
    和 $p_{4}$ 特征的影响。图像转载自 PGCN [[61](#bib.bib61)]。
- en: Li et al. in AGCN [[31](#bib.bib31)] proposed an attention based GCN to model
    the inter and intra dependencies of the proposals. Intra attention learns the
    long-range dependencies among pixels inside each action proposal and inter attention
    learns the adaptive dependencies among the proposals to adjust the imprecise boundary.
    Bai et al. in BC-GNN [[49](#bib.bib49)] proposed a graph neural network to model
    the relations between the boundary and action content of temporal proposals.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人在 AGCN [[31](#bib.bib31)] 中提出了一种基于注意力的 GCN，用于建模提议的内部和外部依赖关系。内部注意力学习每个动作提议内部像素之间的长程依赖，而外部注意力学习提议之间的自适应依赖关系，以调整不精确的边界。Bai
    等人在 BC-GNN [[49](#bib.bib49)] 中提出了一种图神经网络，用于建模时间提议的边界与动作内容之间的关系。
- en: Xu et al. proposed G-TAD [[62](#bib.bib62)] to capture the relations between
    different snippets of input video. They constructed a graph where the nodes are
    temporal segments of the video and the edges are either temporal or semantic.
    The temporal edges are pre-defined according to the snippets’ temporal order but
    the semantic edges are dynamically updated between the nodes according to their
    feature distance. Temporal and semantic context of the snippets are aggregated
    using graph convolutions. All possible pairs of start and end with duration within
    a specific range are considered to generate the proposals. To evaluate each proposal,
    the temporal and semantic features of the corresponding sub-graph are extracted.
    Chang et al. in ATAG [[63](#bib.bib63)] also designed an adaptive GCN similar
    to G-TAD [[62](#bib.bib62)] to capture local temporal context where the graph
    nodes are the snippets and the edges model the relation between snippets. The
    temporal context is then captured through graph convolutions where the feature
    of each snippet is influenced and updated by the features of other snippets. VSGN
    [[64](#bib.bib64)] builds a graph on video snippets similar to G-TAD [[62](#bib.bib62)],
    but also exploits correlations between cross-scale snippets. They propose a cross-scale
    graph pyramid network which aggregates features from cross scales, and progressively
    enhances the features of original and magnified scales at multiple network levels.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人提出了 G-TAD [[62](#bib.bib62)]，以捕捉输入视频不同片段之间的关系。他们构建了一个图，其中节点是视频的时间段，边缘则是时间性或语义性的。时间边缘根据片段的时间顺序预定义，但语义边缘则根据节点之间的特征距离动态更新。片段的时间和语义上下文通过图卷积进行聚合。考虑所有可能的开始和结束对，并在特定范围内生成提议。为了评估每个提议，提取了相应子图的时间和语义特征。Chang
    等人在 ATAG [[63](#bib.bib63)] 中也设计了类似 G-TAD [[62](#bib.bib62)] 的自适应 GCN 来捕捉局部时间上下文，其中图节点是片段，边缘建模片段之间的关系。然后通过图卷积捕捉时间上下文，其中每个片段的特征受其他片段特征的影响并进行更新。VSGN
    [[64](#bib.bib64)] 在视频片段上构建了类似于 G-TAD [[62](#bib.bib62)] 的图，但还利用了跨尺度片段之间的相关性。他们提出了一个跨尺度图金字塔网络，该网络从跨尺度中聚合特征，并在多个网络层级中逐步增强原始和放大尺度的特征。
- en: 2.3.5.3   Transformers
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.5.3   变换器
- en: Some action instances in a video have non-sequential dependencies, meaning that
    they are related but are separated by other events in the video. Also, some action
    instances may have overlaps in their temporal extents. Based on these observations,
    Nawhal et al. in AGT [[65](#bib.bib65)] proposed an encoder decoder transformer
    to capture non-linear temporal structure by reasoning over videos as nonsequential
    entities. Their encoder generates a context graph where the nodes are initially
    video level features and the interactions among nodes are modeled as learnable
    edge weights. Also, positional information for each node is provided using learnable
    positional encodings. Their decoder learns the interactions between context graph
    (latent representation of the input video) and graph structured query embeddings
    (latent representations of the action queries).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 视频中的某些动作实例具有非顺序依赖关系，即它们是相关的但被视频中的其他事件隔开。此外，一些动作实例可能在时间范围上有重叠。基于这些观察，Nawhal等人在AGT
    [[65](#bib.bib65)]中提出了一种编码器-解码器变换器，通过将视频视为非顺序实体来捕捉非线性时间结构。他们的编码器生成一个上下文图，其中节点最初是视频级特征，节点之间的交互被建模为可学习的边权重。此外，为每个节点提供了使用可学习位置编码的位置信息。他们的解码器学习上下文图（输入视频的潜在表示）与图结构查询嵌入（动作查询的潜在表示）之间的交互。
- en: Tan et al. in RTD-Net [[66](#bib.bib66)] proposed a relaxed transformer to directly
    generate action proposals without the need to human prior knowledge for careful
    design of anchor placement or boundary matching mechanisms. The transformer encoder
    models long-range temporal context and captures inter-proposal relationships from
    a global view to precisely localize action instances. They also argued that the
    snippet features in a video change at a very slow speed and direct employment
    of self-attention in transformers can lead to over-smoothing. Therefore, they
    customized the encoder with a boundary-attentive architecture to enhance the discrimination
    capability of action boundary. Chang et al. in ATAG [[63](#bib.bib63)] designed
    an augmented transformer to mine long-range temporal context for noisy action
    instance localization. The snippet-level features generated by transformer are
    used to classify the snippets to action or background under supervision of a binary
    classification loss. Throughout this process the transformer learns to capture
    long-term dependencies at snippet level.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Tan等人在RTD-Net [[66](#bib.bib66)]中提出了一种放松的变换器，直接生成动作提议，无需依赖人工先验知识来精心设计锚点放置或边界匹配机制。变换器编码器建模了长范围的时间上下文，并从全局视角捕捉提议之间的关系，以精确定位动作实例。他们还认为视频中的片段特征变化非常缓慢，因此直接使用变换器中的自注意力可能导致过度平滑。因此，他们定制了一个具有边界关注架构的编码器，以增强动作边界的辨别能力。Chang等人在ATAG
    [[63](#bib.bib63)]中设计了一种增强型变换器来挖掘长范围的时间上下文，用于嘈杂动作实例定位。由变换器生成的片段级特征用于在二分类损失的监督下将片段分类为动作或背景。在此过程中，变换器学会在片段级别捕捉长期依赖关系。
- en: 2.3.6 Spatio-temporal Action Detection
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.6 时空动作检测
- en: 'Spatio-temporal action detection aims to localize action instances in both
    space and time, and recognize the action labels. In the fully-supervised setting
    of this task, the temporal boundary of action instances at the video-level, the
    spatial bounding box of actions at the frame-level, and action labels are provided
    during training and must be detected during inference. Fig. [7](#S2.F7 "Figure
    7 ‣ 2.3.6 Spatio-temporal Action Detection ‣ 2.3 Action Detection with Full Supervision
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey") shows an example of this task. The start and end
    of action “long jump” are detected in temporal domain. Also, bounding box of the
    actor performing the action is detected in each frame in spatial domain.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '时空动作检测旨在在空间和时间上定位动作实例，并识别动作标签。在此任务的完全监督设置中，训练期间提供视频级别的动作实例的时间边界、帧级别的动作空间边界框以及动作标签，并且在推断期间必须检测这些内容。图
    [7](#S2.F7 "Figure 7 ‣ 2.3.6 Spatio-temporal Action Detection ‣ 2.3 Action Detection
    with Full Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey") 显示了此任务的示例。“远跳”动作的开始和结束在时间域中被检测到。同时，在空间域中检测到每帧中执行动作的演员的边界框。'
- en: '![Refer to caption](img/27380e16153e30636ee6115b4feb7852.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/27380e16153e30636ee6115b4feb7852.png)'
- en: 'Figure 7: Spatio-temporal activity detection task: action ”long jump” is localized
    in time and space. Other than temporal interval of the action, bounding box of
    the person performing the action is detected in each frame.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 时空活动检测任务：动作“跳远”在时间和空间上被定位。除了动作的时间间隔外，还在每帧中检测到执行动作的人的边界框。'
- en: 2.3.6.1   Frame-level Action Detection
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.6.1   帧级动作检测
- en: Early methods [[67](#bib.bib67), [68](#bib.bib68)] were based on extensions
    of the sliding window scheme, requiring strong assumptions such as a cuboid shape,
    i.e., a fixed spatial extent of the actor across frames. Later, advancements in
    object detection inspired frame-level action detection methods to recognize human
    action classes at the frame level. In the first stage action proposals are produced
    by a region proposal algorithm or densely sampled anchors, and in the second stage
    the proposals are used for action classification and localization refinement.
    Hundreds of action proposals are extracted per video given low-level cues, such
    as super-voxels [[69](#bib.bib69), [70](#bib.bib70)] or dense trajectories [[71](#bib.bib71),
    [72](#bib.bib72), [73](#bib.bib73)], and then proposals are classified to localize
    actions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 早期方法 [[67](#bib.bib67), [68](#bib.bib68)] 基于滑动窗口方案的扩展，这要求对演员在各帧中的空间范围有强假设，例如立方体形状，即演员在帧中的空间范围是固定的。后来，物体检测的进展激发了帧级动作检测方法，使其能够在帧级别识别人的动作类别。在第一阶段，通过区域提议算法或密集采样的锚点生成动作提议，在第二阶段，这些提议用于动作分类和定位精化。根据低级线索，如超体素
    [[69](#bib.bib69), [70](#bib.bib70)] 或密集轨迹 [[71](#bib.bib71), [72](#bib.bib72),
    [73](#bib.bib73)]，每个视频中提取出数百个动作提议，然后对提议进行分类以定位动作。
- en: After detecting the action regions in the frames, some methods [[74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81)] use optical flow to capture motion cues. They
    employ linking algorithms to connect the frame-level bounding boxes into spatio-temporal
    action tubes. Gkioxari et al. [[74](#bib.bib74)] used dynamic programming approach
    to link the resulting per-frame detection. The cost function of the dynamic programming
    is based on detection scores of the boxes and overlap between detection of consecutive
    frames. Weinzaepfel et al. [[79](#bib.bib79)] replaced the linking algorithm by
    a tracking-by-detection method. Then, two-stream Faster R-CNN was introduced by
    [[76](#bib.bib76), [78](#bib.bib78)]. Saha et al. [[78](#bib.bib78)] fuse the
    scores of both streams based on overlap between the appearance and the motion.
    Peng et al. [[76](#bib.bib76)] combine proposals extracted from the two streams
    and then classify and regress them with fused RGB and multi-frame optical flow
    features. They also use multiple regions inside each action proposal and then
    link the detection across a video based on spatial overlap and classification
    score.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测到帧中的动作区域后，一些方法 [[74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)] 使用光流来捕捉运动线索。他们采用连接算法将帧级边界框连接成时空动作管。Gkioxari
    等 [[74](#bib.bib74)] 使用动态规划方法来链接结果的每帧检测。动态规划的成本函数基于边界框的检测得分和连续帧检测之间的重叠。Weinzaepfel
    等 [[79](#bib.bib79)] 用基于检测的跟踪方法替换了连接算法。随后，[[76](#bib.bib76), [78](#bib.bib78)]
    引入了双流 Faster R-CNN。Saha 等 [[78](#bib.bib78)] 基于外观和运动之间的重叠融合了两个流的得分。Peng 等 [[76](#bib.bib76)]
    结合了从两个流中提取的提议，然后使用融合的 RGB 和多帧光流特征对其进行分类和回归。他们还在每个动作提议中使用多个区域，然后根据空间重叠和分类得分在视频中链接检测。
- en: Another group [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)] rely on
    an actionness measure, i.e., a pixel-wise probability of containing any action.
    To estimate actionness, they use low-level cues such as optical flow [[84](#bib.bib84)],
    CNNs with a two-stream architecture [[83](#bib.bib83)] or RNNs [[83](#bib.bib83)].
    They extract action tubes by thresholding the actionness scores [[82](#bib.bib82)]
    or by using a maximum set coverage [[84](#bib.bib84)]. The output is a rough localization
    of the action as it is based on noisy pixel-level maps.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一些组 [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)] 依赖于动作性度量，即每个像素包含任何动作的概率。为了估计动作性，他们使用低级线索，如光流
    [[84](#bib.bib84)]、双流架构的 CNN [[83](#bib.bib83)] 或 RNN [[83](#bib.bib83)]。他们通过对动作性得分
    [[82](#bib.bib82)] 进行阈值处理或使用最大集合覆盖 [[84](#bib.bib84)] 来提取动作管。输出是动作的粗略定位，因为它基于噪声像素级图。
- en: The main disadvantage of these methods is that the temporal property of videos
    is not fully exploited as the detection is performed on each frame independently.
    Effective temporal modeling is crucial as a number of actions are only identifiable
    when temporal context information is available.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的主要缺点是视频的时间特性未被充分利用，因为检测是在每帧上独立执行的。有效的时间建模至关重要，因为许多动作只有在有时间上下文信息时才可识别。
- en: 2.3.6.2   Clip-level Action Detection
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.6.2   剪辑级别动作检测
- en: As mentioned earlier, temporal modeling is necessary for accurate action localization.
    Here, we discuss methods that exploit temporal information by performing action
    detection at the clip (i.e., a short video snippet) level.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，准确的动作定位需要时间建模。在这里，我们讨论通过在剪辑（即短视频片段）级别执行动作检测来利用时间信息的方法。
- en: Kalogeiton et al. [[85](#bib.bib85)] proposed action tubelet detector (ACT-detector)
    that takes as input a sequence of frames and outputs action categories and regressed
    tubelets, i.e., sequences of bounding boxes with associated scores. The tubelets
    are then linked to construct action tubes (sequence of bounding boxes of action).
    Gu et al. [[86](#bib.bib86)] further demonstrate the importance of temporal information
    by using longer clips and taking advantage of I3D pre-trained on the large-scale
    video dataset [[24](#bib.bib24)]. In order to generate action proposals, they
    extend 2D region proposals to 3D by replicating them over time, assuming that
    the spatial extent is fixed within a clip. However, this assumption would be violated
    for the action tubes with large spatial displacement over time, in particular
    when the clip is long or involves rapid movement of actors or camera. Thus, using
    long cuboids directly as action proposals is not optimal, since they introduce
    extra noise for action classification.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Kalogeiton等人[[85](#bib.bib85)]提出了一种动作管道检测器（ACT-detector），其输入为一系列帧，输出为动作类别和回归的管道，即带有关联得分的边界框序列。然后，将这些管道连接起来构建动作管道（动作的边界框序列）。Gu等人[[86](#bib.bib86)]通过使用更长的剪辑和利用在大规模视频数据集[[24](#bib.bib24)]上预训练的I3D进一步展示了时间信息的重要性。为了生成动作提议，他们通过在时间上复制2D区域提议将其扩展到3D，假设在剪辑内空间范围是固定的。然而，这一假设对于时间上有大空间位移的动作管道会被违反，特别是在剪辑很长或涉及演员或相机快速移动时。因此，直接使用长立方体作为动作提议并不是最终的选择，因为它们会引入额外的噪声来进行动作分类。
- en: Yang et al. [[87](#bib.bib87)] perform action detection at clip level, and then
    linked them to build action tubes across the video. They employ multi-step optimization
    process to progressively refine the initial proposals. Other methods [[6](#bib.bib6)],
    [[88](#bib.bib88)] exploited human proposals coming from pretrained image detectors
    and replicated them in time to build straight spatio-temporal tubes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 杨等人[[87](#bib.bib87)]在剪辑级别执行动作检测，然后将这些检测结果链接起来，构建视频中的动作管道。他们采用多步骤优化过程逐步细化初始提议。其他方法[[6](#bib.bib6)]，[[88](#bib.bib88)]利用来自预训练图像检测器的人类提议，并通过时间复制这些提议以构建直线时空管道。
- en: 2.3.6.3   Modeling Spatio-temporal Dependencies
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.3.6.3   建模时空依赖性
- en: Understanding human actions usually requires understanding the people and objects
    around them. Therefore, state-of-the-art methods model the relation between actors
    and the contextual information such as other people and other objects. Some methods
    used the graph-structured networks [[89](#bib.bib89), [90](#bib.bib90)] and attention
    mechanism [[91](#bib.bib91), [88](#bib.bib88), [92](#bib.bib92)] to aggregate
    the contextual information from other people and objects in the video.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 理解人类动作通常需要了解他们周围的人和物体。因此，最先进的方法建模演员与上下文信息之间的关系，例如其他人和其他物体。一些方法使用图结构网络[[89](#bib.bib89)，[90](#bib.bib90)]和注意力机制[[91](#bib.bib91)，[88](#bib.bib88)，[92](#bib.bib92)]来聚合视频中其他人和物体的上下文信息。
- en: Wu et al. [[88](#bib.bib88)] provided long-term supportive information that
    enables video models to better understand the present. The designed a long-term
    feature bank and a feature bank operator FBO that computes interactions between
    the short-term and long-term features. They integrate information over a long
    temporal support, lasting minutes or even the whole video. Girdhar et al. [[91](#bib.bib91)]
    proposed a transformer-style architecture to weight actors with features from
    the context around him. Tomei et al. [[93](#bib.bib93)] employed self-attention
    to encode people and object relationships in a graph structure, and use the spatio-temporal
    distance between proposals. Ji et al. proposed Action Genome [[94](#bib.bib94)]
    to model action-object interaction, by decomposing actions into spatio-temporal
    scene graphs. Ulutan et al. [[92](#bib.bib92)] suggested combining actor features
    with every spatio-temporal region in the scene to produce attention maps between
    the actor and the context. Pan et al. [[95](#bib.bib95)] proposed a relational
    reasoning module to capture the relation between the two actors based on their
    respective relations with the context. Tomei et al. [[96](#bib.bib96)] proposed
    a graph-based framework to learn high-level interactions between people and objects,
    in both space and time. Spatio-temporal relationships are learned through self-attention
    on a multi-layer graph structure which can connect entities from consecutive clips,
    thus considering long-range spatial and temporal dependencies.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 吴等人[[88](#bib.bib88)]提供了长期支持信息，使得视频模型能够更好地理解当前内容。他们设计了一个长期特征库和一个计算短期特征与长期特征之间交互的特征库操作符FBO。他们在长时间支持下整合信息，这种支持可以持续几分钟甚至整个视频。Girdhar等人[[91](#bib.bib91)]提出了一种类似变换器的架构，根据周围环境中的特征来加权演员。Tomei等人[[93](#bib.bib93)]采用自注意力机制在图结构中编码人物和物体的关系，并使用提议之间的时空距离。Ji等人提出了Action
    Genome[[94](#bib.bib94)]，通过将动作分解为时空场景图来建模动作-物体交互。Ulutan等人[[92](#bib.bib92)]建议将演员特征与场景中的每个时空区域结合，以生成演员与背景之间的注意力图。Pan等人[[95](#bib.bib95)]提出了一个关系推理模块，以捕捉两个演员之间基于它们与背景的各自关系的关系。Tomei等人[[96](#bib.bib96)]提出了一个基于图的框架，用于学习人和物体之间的高层次交互，既包括空间也包括时间。通过对多层图结构上的自注意力机制进行学习，以便连接来自连续片段的实体，从而考虑长范围的空间和时间依赖关系。
- en: 2.4 Action Detection with Limited Supervision
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 有限监督下的动作检测
- en: 'Fully supervised action detection requires the full annotation of temporal
    boundaries and action labels for all action instances in training videos, which
    is very time-consuming and costly. To eliminate the need for exhaustive annotations
    in the training phase, in recent years, researchers have explored the design of
    efficient models that require limited ground-truth annotations. We discuss weakly-supervised
    methods in Section [2.4.1](#S2.SS4.SSS1 "2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey"),
    and other learning methods with limited supervision (unsupervised, semi-supervised,
    and self-supervised) are described in Section [2.4.2](#S2.SS4.SSS2 "2.4.2 Unsupervised,
    Semi-supervised, and Self-supervised ‣ 2.4 Action Detection with Limited Supervision
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey").'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '完全监督的动作检测需要对训练视频中的所有动作实例进行完整的时间边界和动作标签注释，这非常耗时且成本高昂。为了消除训练阶段对全面注释的需求，近年来，研究人员探索了设计高效模型的方法，这些模型只需要有限的真实标签。我们在[2.4.1](#S2.SS4.SSS1
    "2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection with Limited
    Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action
    Detection in Untrimmed Videos: A Survey")节中讨论了弱监督方法，而其他有限监督（无监督、半监督和自监督）学习方法在[2.4.2](#S2.SS4.SSS2
    "2.4.2 Unsupervised, Semi-supervised, and Self-supervised ‣ 2.4 Action Detection
    with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")节中进行了描述。'
- en: 2.4.1 Weakly-supervised Action Detection
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 弱监督动作检测
- en: 'Weakly-supervised learning scheme requires coarse-grained or noisy labels during
    the training phase. Following the work of [[97](#bib.bib97)], weakly-supervised
    action detection in common settings requires only the video-level labels of actions
    during training while the temporal boundaries of action instances are not needed.
    During testing both labels and temporal boundaries of actions are predicted. In
    the following parts of this section, weakly-supervised action detection refers
    to this setting. There are also other weak signals utilized for action detection
    such as order of actions [[98](#bib.bib98)], [[99](#bib.bib99)], [[100](#bib.bib100)],
    [[101](#bib.bib101)], frequency of action labels [[102](#bib.bib102)], and total
    number of events in each video [[103](#bib.bib103)]. A common strategy in weakly-supervised
    action detection is to use attention mechanism to focus on discriminative snippets
    and combine salient snippet-level features into a video-level feature. The attention
    scores are used to localize the action regions and eliminate irrelevant background
    frames. There are two main strategies to extract attention signals from videos.
    First, class-specific attention approaches where attention scores are generated
    from class activation sequences (def [15](#Thmdefinition15 "Definition 15\. ‣
    2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action
    Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep
    Learning-based Action Detection in Untrimmed Videos: A Survey")) for each action
    class (Section [2.4.1.2](#S2.SS4.SSS1.P2 "2.4.1.2 Class-specific Attention for
    Action Localization ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection
    with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")). Second, class-agnostic attention
    approaches where attention scores are class-agnostic and are extracted from raw
    data (Section [2.4.1.3](#S2.SS4.SSS1.P3 "2.4.1.3 Class-agnostic Attention for
    Action Localization ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection
    with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")). We discuss these two attention
    strategies in this section.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '弱监督学习方案在训练阶段需要粗略或嘈杂的标签。根据[[97](#bib.bib97)]的工作，常见设置下的弱监督动作检测仅需要在训练时的动作视频级别标签，而不需要动作实例的时间边界。在测试过程中，预测动作的标签和时间边界。本节后续部分的弱监督动作检测指的就是这种设置。此外，还有其他弱信号用于动作检测，如动作顺序[[98](#bib.bib98)],
    [[99](#bib.bib99)], [[100](#bib.bib100)], [[101](#bib.bib101)], 动作标签的频率[[102](#bib.bib102)]，以及每个视频中的事件总数[[103](#bib.bib103)]。弱监督动作检测中的一个常见策略是使用注意力机制来关注具有辨别性的片段，并将显著的片段级特征结合成视频级特征。注意力分数用于定位动作区域并消除不相关的背景帧。从视频中提取注意力信号的主要策略有两种。首先是特定于类别的注意力方法，其中注意力分数是从每个动作类别的类别激活序列中生成的（定义
    [15](#Thmdefinition15 "Definition 15\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised
    Action Detection ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")）（第[2.4.1.2](#S2.SS4.SSS1.P2 "2.4.1.2 Class-specific Attention for Action
    Localization ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection
    with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")节）。其次是无类别注意力方法，其中注意力分数与类别无关，并从原始数据中提取（第[2.4.1.3](#S2.SS4.SSS1.P3
    "2.4.1.3 Class-agnostic Attention for Action Localization ‣ 2.4.1 Weakly-supervised
    Action Detection ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")节）。我们将在本节中讨论这两种注意力策略。'
- en: 2.4.1.1   Term Definition
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.4.1.1   术语定义
- en: To facilitate reading this section, we provide the definition of frequently
    used terminologies.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便阅读本节，我们提供了常用术语的定义。
- en: Definition 15.
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义15。
- en: 'Temporal class activation maps (T-CAM). For a given video, T-CAM is a matrix
    denoted by $A$ which represent the possibility of activities at each temporal
    position. Matrix $A$ has $n_{c}$ rows which is the total number of action classes,
    and $T$ columns which is the number of temporal positions in the video. Value
    of cell $A[c,t]$ is the activation of class $c$ at temporal position $t$. Formally
    $A$ is calculated by:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 时间类激活图（T-CAM）。对于给定的视频，T-CAM是一个矩阵，用$A$表示，代表每个时间位置的活动可能性。矩阵$A$有$n_{c}$行，即动作类别的总数，以及$T$列，即视频中的时间位置数。单元$A[c,t]$的值是类别$c$在时间位置$t$的激活值。正式地说，$A$的计算公式为：
- en: '|  | $A=WX\oplus b,$ |  | (10) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | $A=WX\oplus b,$ |  | (10) |'
- en: where $X\in{\rm I\!R}^{d\times T}$ is a video-level feature matrix, and $d$
    is the feature dimension. Also, $W\in{\rm I\!R}^{n_{c}\times d}$ and $b\in{\rm
    I\!R}^{n_{c}}$, are learnable parameters and $\oplus$ is the addition with broadcasting
    operator.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X\in{\rm I\!R}^{d\times T}$ 是视频级特征矩阵，$d$ 是特征维度。此外，$W\in{\rm I\!R}^{n_{c}\times
    d}$ 和 $b\in{\rm I\!R}^{n_{c}}$ 是可学习的参数，$\oplus$ 是带有广播操作符的加法。
- en: Definition 16.
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 16。
- en: 'Class-specific attention scores. In a given video, class-specific attention
    score is the occurrence probability of action class $c$ at temporal position $t$,
    denoted by $a[c,t]$. Formally, $a[c,t]$ is computed by normalizing the activation
    of class $c$ over temporal dimension:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 类别特定注意力得分。在给定的视频中，类别特定注意力得分是动作类别 $c$ 在时间位置 $t$ 的出现概率，用 $a[c,t]$ 表示。形式上，$a[c,t]$
    通过对类别 $c$ 在时间维度上的激活进行归一化来计算：
- en: '|  | $a[c,t]=\frac{\text{exp}(A[c,t])}{\sum_{t=1}^{T}\text{exp}(A[c,t])},$
    |  | (11) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $a[c,t]=\frac{\text{exp}(A[c,t])}{\sum_{t=1}^{T}\text{exp}(A[c,t])},$
    |  | (11) |'
- en: 'where $A$ is the T-CAM (def [15](#Thmdefinition15 "Definition 15\. ‣ 2.4.1.1
    Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection
    with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")), and $T$ is the number of temporal
    positions. Therefore, row $a_{c}$ is the probability distribution of occurrence
    of class $c$ over video length.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $A$ 是 T-CAM（定义 [15](#Thmdefinition15 "Definition 15\. ‣ 2.4.1.1 Term Definition
    ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection with Limited
    Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action
    Detection in Untrimmed Videos: A Survey")），$T$ 是时间位置的数量。因此，行 $a_{c}$ 是视频长度上类别
    $c$ 出现的概率分布。'
- en: Definition 17.
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 17。
- en: Class-agnostic attention score. In a given video, class-agnostic attention score,
    denoted by $\lambda_{t}$, is the occurrence probability of any action of interest
    at temporal position $t$, regardless of the action class. The attention vector
    for all temporal positions of the video is denoted by $\lambda$.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 类别无关注意力得分。在给定的视频中，类别无关注意力得分，用 $\lambda_{t}$ 表示，是时间位置 $t$ 上任何感兴趣的动作的出现概率，无论动作类别如何。视频中所有时间位置的注意力向量用
    $\lambda$ 表示。
- en: Definition 18.
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 18。
- en: 'Attention-based aggregated features. The video-level foreground and background
    features are generated using temporal pooling of embedded features weighted by
    attention scores. Class-specific features are defined based on class-specific
    attention scores $a_{c}$ (def [16](#Thmdefinition16 "Definition 16\. ‣ 2.4.1.1
    Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection
    with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")) for each class $c$ while class-agnostic
    features are defined based on class-agnostic attention vector $\lambda$ (def [17](#Thmdefinition17
    "Definition 17\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")).
    Aggregated foreground feature is most influenced by feature vectors with high
    attention that represent actions while background feature is impacted by features
    with low attention. $T$ is the video length and $X$ is the video feature matrix.
    These features are formulated as the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '基于注意力的聚合特征。视频级的前景和背景特征是通过对嵌入特征进行时间池化，权重由注意力得分决定。类别特定特征是基于每个类别 $c$ 的类别特定注意力得分
    $a_{c}$（定义 [16](#Thmdefinition16 "Definition 16\. ‣ 2.4.1.1 Term Definition ‣
    2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection with Limited Supervision
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey")）定义的，而类别无关特征则是基于类别无关注意力向量 $\lambda$（定义 [17](#Thmdefinition17
    "Definition 17\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")）定义的。聚合前景特征受到高注意力的特征向量的主要影响，这些特征向量表示动作，而背景特征则受到低注意力特征的影响。$T$
    是视频长度，$X$ 是视频特征矩阵。这些特征被表述为：'
- en: '|  | Foreground: | Background: |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | 前景： | 背景： |'
- en: '| Class-specific: | $f_{c}=Xa_{c}$ | $b_{c}=\frac{1}{T-1}X(\mathbb{1}-a_{c}),$
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 类别特定： | $f_{c}=Xa_{c}$ | $b_{c}=\frac{1}{T-1}X(\mathbb{1}-a_{c}),$ |'
- en: '| Class-agnostic: | $f=\frac{1}{T}X\lambda$ | $b=\frac{1}{T}X(\mathbb{1}-\lambda).$
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 类别无关： | $f=\frac{1}{T}X\lambda$ | $b=\frac{1}{T}X(\mathbb{1}-\lambda).$ |'
- en: 2.4.1.2   Class-specific Attention for Action Localization
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.4.1.2 类别特定注意力用于动作定位
- en: 'Class-specific attention module computes the attention weight $a[c,t]$ (def
    [16](#Thmdefinition16 "Definition 16\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised
    Action Detection ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey")) for all action classes $c$ and all temporal positions $t$ in each
    video. The attention scores attend to the portions of the video where an activity
    of a certain category occurs. Therefore, video segments with attention scores
    higher than a threshold are localized as action parts. Class-specific attention
    module is used in [[104](#bib.bib104)], [[105](#bib.bib105)], [[102](#bib.bib102)],
    [[106](#bib.bib106)] to localize the temporal boundary of action instances.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 类别特定注意力模块计算每个视频中所有动作类别 $c$ 和所有时间位置 $t$ 的注意力权重 $a[c,t]$（定义 [16](#Thmdefinition16
    "定义 16\. ‣ 2.4.1.1 术语定义 ‣ 2.4.1 弱监督动作检测 ‣ 2.4 有限监督下的动作检测 ‣ 2 时间动作检测方法 ‣ 深度学习在未裁剪视频中的动作检测：综述")）。注意力分数关注于视频中某一类别活动发生的部分。因此，注意力分数高于阈值的视频片段被定位为动作部分。类别特定注意力模块在
    [[104](#bib.bib104)]、[[105](#bib.bib105)]、[[102](#bib.bib102)]、[[106](#bib.bib106)]
    中用于定位动作实例的时间边界。
- en: 'Class-specific attention learning with MIL: In general scheme of MIL (multi-instance
    learning), training instances are arranged in sets, called bags, and a label is
    provided for the entire bag [[107](#bib.bib107)]. In the context of weakly-supervised
    temporal action detection, each video is treated as a bag of action instances
    and the video-level action labels are provided. In order to compute the loss for
    each bag (video in this task), each video should be represented using a single
    confidence score per category. The confidence score for each category is computed
    as the average of top $k$ activation scores over the temporal dimension for that
    category. In a given video, suppose set $\{t^{c}_{1},t^{c}_{2},\cdots,t^{c}_{k}\}$
    are $k$ temporal positions with highest activation scores for class $c$. Then,
    the video-level class-wise confidence score $s^{c}$ for class $c$ is defined as:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MIL 的类别特定注意力学习：在 MIL（多实例学习）的总体方案中，训练实例被安排在称为袋子的集合中，并为整个袋子提供一个标签 [[107](#bib.bib107)]。在弱监督时间动作检测的背景下，每个视频被视为动作实例的袋子，并且提供了视频级别的动作标签。为了计算每个袋子（本任务中的视频）的损失，每个视频应使用每个类别的单一置信度分数来表示。每个类别的置信度分数被计算为该类别在时间维度上的前
    $k$ 个激活分数的平均值。在给定的视频中，设 $\{t^{c}_{1},t^{c}_{2},\cdots,t^{c}_{k}\}$ 是类别 $c$ 的 $k$
    个具有最高激活分数的时间位置。那么，类别 $c$ 的视频级别类别置信度分数 $s^{c}$ 定义为：
- en: '|  | $s^{c}=\frac{1}{k}\sum_{l=1}^{k}A[c,t^{c}_{l}],$ |  | (12) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $s^{c}=\frac{1}{k}\sum_{l=1}^{k}A[c,t^{c}_{l}],$ |  | (12) |'
- en: 'where $A[c,t^{c}_{l}]$ is the activation (def [15](#Thmdefinition15 "Definition
    15\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4
    Action Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")) of class
    $c$ at temporal position $t^{c}_{l}$. Then, probability mass function (PMF) of
    action classes is computed by applying softmax function on $s^{c}$ scores over
    class dimension:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A[c,t^{c}_{l}]$ 是类别 $c$ 在时间位置 $t^{c}_{l}$ 的激活（定义 [15](#Thmdefinition15 "定义
    15\. ‣ 2.4.1.1 术语定义 ‣ 2.4.1 弱监督动作检测 ‣ 2.4 有限监督下的动作检测 ‣ 2 时间动作检测方法 ‣ 深度学习在未裁剪视频中的动作检测：综述")）。然后，通过对
    $s^{c}$ 分数应用 softmax 函数来计算动作类别的概率质量函数（PMF）：
- en: '|  | $p^{c}=\frac{\exp{(s^{c})}}{\sum_{c=1}^{n_{c}}\exp{(s^{c})}},$ |  | (13)
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{c}=\frac{\exp{(s^{c})}}{\sum_{c=1}^{n_{c}}\exp{(s^{c})}},$ |  | (13)
    |'
- en: 'where $n_{c}$ is the number of action classes. MIL loss is a cross-entropy
    loss applied over all videos and all action classes. For video $i$ and action
    class $c$, $p^{c}_{i}$ is the class-wise probability score, and $y^{c}_{i}$ is
    a normalized ground-truth binary label. MIL is defined as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n_{c}$ 是动作类别的数量。MIL 损失是应用于所有视频和所有动作类别的交叉熵损失。对于视频 $i$ 和动作类别 $c$，$p^{c}_{i}$
    是类别级概率分数，$y^{c}_{i}$ 是归一化的真实二值标签。MIL 定义为：
- en: '|  | $L_{MIL}=\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{n_{c}}-y^{c}_{i}\log(p^{c}_{i}),$
    |  | (14) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{MIL}=\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{n_{c}}-y^{c}_{i}\log(p^{c}_{i}),$
    |  | (14) |'
- en: 'where $n$ is the total number of videos. MIL loss supervises class-wise probability
    scores which are computed based on activation scores $A[c,t]$. Therefore, MIL
    learns activation scores and T-CAM (def [15](#Thmdefinition15 "Definition 15\.
    ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action
    Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep
    Learning-based Action Detection in Untrimmed Videos: A Survey")) for each video
    and is used in W-TALC [[105](#bib.bib105)], Action Graphs [[108](#bib.bib108)],
    UNet [[104](#bib.bib104)], and Actionbytes [[109](#bib.bib109)].'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $n$ 是视频的总数。MIL 损失函数监督了基于激活分数 $A[c,t]$ 计算的类别概率分数。因此，MIL 学习激活分数和 T-CAM（见 [15](#Thmdefinition15
    "Definition 15\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")）用于每个视频，并被应用于
    W-TALC [[105](#bib.bib105)]、Action Graphs [[108](#bib.bib108)]、UNet [[104](#bib.bib104)]
    和 Actionbytes [[109](#bib.bib109)]。'
- en: 'Class-specific attention learning with CASL: The CASL (co-activity similarity
    loss) was initially introduced in W-TALC[[105](#bib.bib105)] and then inspired
    others Deep Metric [[106](#bib.bib106)], Action Graphs [[108](#bib.bib108)], WOAD
    [[110](#bib.bib110)], Actionbytes [[109](#bib.bib109)]. The main idea is that
    for a pair of videos including the same action classes, the foreground features
    in both videos should be more similar than the foreground feature in one video
    and the background feature in the other video. For a pair of videos with indices
    $m$ and $n$ that include action class $c$, the foreground features are denoted
    by $f^{m}_{c}$, $f^{n}_{c}$ and the background features by $b^{m}_{c}$, $b^{n}_{c}$
    (def [18](#Thmdefinition18 "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1
    Weakly-supervised Action Detection ‣ 2.4 Action Detection with Limited Supervision
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey")). Then CASL is defined based on ranking hinge loss
    as the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '类别特定的注意力学习与 CASL：CASL（共活动相似性损失）最初在 W-TALC [[105](#bib.bib105)] 中引入，然后启发了其他方法如
    Deep Metric [[106](#bib.bib106)]、Action Graphs [[108](#bib.bib108)]、WOAD [[110](#bib.bib110)]
    和 Actionbytes [[109](#bib.bib109)]。主要思想是，对于包含相同动作类别的一对视频，两个视频中的前景特征应该比一个视频中的前景特征和另一个视频中的背景特征更相似。对于包含动作类别
    $c$ 的视频对 $m$ 和 $n$，前景特征用 $f^{m}_{c}$、$f^{n}_{c}$ 表示，背景特征用 $b^{m}_{c}$、$b^{n}_{c}$
    表示（见 [18](#Thmdefinition18 "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1
    Weakly-supervised Action Detection ‣ 2.4 Action Detection with Limited Supervision
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey")）。然后，CASL 基于排序铰链损失定义如下：'
- en: '|  | $\begin{split}L^{mn}_{c}&amp;=\frac{1}{2}\{\max\big{(}0,d(f^{m}_{c},f^{n}_{c})-d(f^{m}_{c},b^{n}_{c})+\delta\big{)}\\
    &amp;+\max\big{(}0,d(f^{m}_{c},f^{n}_{c})-d(b^{m}_{c},f^{n}_{c})+\delta\big{)}\},\end{split}$
    |  | (15) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}L^{mn}_{c}&amp;=\frac{1}{2}\{\max\big{(}0,d(f^{m}_{c},f^{n}_{c})-d(f^{m}_{c},b^{n}_{c})+\delta\big{)}\\
    &amp;+\max\big{(}0,d(f^{m}_{c},f^{n}_{c})-d(b^{m}_{c},f^{n}_{c})+\delta\big{)}\},\end{split}$
    |  | (15) |'
- en: 'where $d$ is a metric (e.g., cosine similarity) to measure the degree of similarity
    between two feature vectors and $\delta$ is a margin parameter. The average of
    $L^{mn}_{c}$ is computed over all video pairs that include action class $c$. This
    loss trains class-specific attention scores $a_{c}$ as foreground and background
    features $f_{c}$ and $b_{c}$ are defined based on $a_{c}$ (def [18](#Thmdefinition18
    "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $d$ 是一种度量（例如，余弦相似度），用于测量两个特征向量之间的相似度，而 $\delta$ 是一个边际参数。$L^{mn}_{c}$ 的平均值是对包括动作类别
    $c$ 的所有视频对进行计算的。这个损失函数训练了特定类别的注意力分数 $a_{c}$，前景和背景特征 $f_{c}$ 和 $b_{c}$ 是基于 $a_{c}$
    定义的（见 [18](#Thmdefinition18 "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1
    Weakly-supervised Action Detection ‣ 2.4 Action Detection with Limited Supervision
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey")）。'
- en: Islam et al. in Deep Metric [[106](#bib.bib106)] replaced metric $d$ with a
    class-specific metric $D_{c}$ defined for each class $c$. Rashid et al. in Action
    Graphs [[108](#bib.bib108)] applied a GCN to transform each temporal segment’s
    feature representation to a weighted average of its neighbors. Then updated features
    are used in CASL for localization. The advantage of this GCN is to model temporal
    dependencies, cluster the semantically-similar time segments, and pushing dissimilar
    segments apart.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Islam 等人在 Deep Metric [[106](#bib.bib106)] 中将度量 $d$ 替换为为每个类别 $c$ 定义的类特定度量 $D_{c}$。Rashid
    等人在 Action Graphs [[108](#bib.bib108)] 中应用了 GCN，将每个时间段的特征表示转换为其邻居的加权平均。然后，将更新后的特征用于
    CASL 进行定位。这个 GCN 的优点在于建模时间依赖性、聚类语义相似的时间段，并将不同的时间段推开。
- en: 'Class-specific attention learning with center loss: The center loss which was
    first introduced in [[111](#bib.bib111)], learns the class-specific centers and
    penalizes the distance between the features and their class centers. Narayan et
    al. in 3C-Net [[102](#bib.bib102)] employed center loss to enhance the feature
    discriminability and reduce the intra-class variations. For each video $i$ and
    each action class $c$, center loss computes the distance (L2 norm) between class-specific
    foreground feature $f^{i}_{c}$ (def [18](#Thmdefinition18 "Definition 18\. ‣ 2.4.1.1
    Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection
    with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")) and cluster center feature $z_{c}$
    as the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '类特定注意力学习与中心损失：中心损失首次在 [[111](#bib.bib111)] 中引入，学习类特定的中心并惩罚特征与其类中心之间的距离。Narayan
    等人在 3C-Net [[102](#bib.bib102)] 中使用中心损失来增强特征的可区分性并减少类内变异。对于每个视频 $i$ 和每个动作类别 $c$，中心损失计算类特定前景特征
    $f^{i}_{c}$ (def [18](#Thmdefinition18 "Definition 18\. ‣ 2.4.1.1 Term Definition
    ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action Detection with Limited
    Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action
    Detection in Untrimmed Videos: A Survey")) 和聚类中心特征 $z_{c}$ 之间的距离 (L2 范数)，计算公式如下：'
- en: '|  | $\mathcal{L}_{center}=\frac{1}{N}\sum_{i}\sum_{c:y^{i}(c)=1}\left\lVert
    f^{i}_{c}-z_{c}\right\rVert^{2}_{2},$ |  | (16) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{center}=\frac{1}{N}\sum_{i}\sum_{c:y^{i}(c)=1}\left\lVert
    f^{i}_{c}-z_{c}\right\rVert^{2}_{2},$ |  | (16) |'
- en: where cluster center feature $z_{c}$ is updated during training. Here, $N$ is
    the total number of videos, and condition $y^{i}(c)=1$ checks if action class
    $c$ occurs in video $i$.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，聚类中心特征 $z_{c}$ 在训练过程中会更新。这里，$N$ 是视频的总数，条件 $y^{i}(c)=1$ 检查动作类别 $c$ 是否出现在视频
    $i$ 中。
- en: 2.4.1.3   Class-agnostic Attention for Action Localization
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.4.1.3 类无关注意力用于动作定位
- en: 'Class-agnostic attention module computes attention vector $\lambda$ (def [17](#Thmdefinition17
    "Definition 17\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey"))
    directly from raw data, by applying fully connected and ReLU layers over video
    features, followed by a sigmoid function to scale attention weights to $[0,1]$.
    Learning class-agnostic attention weights is used in many methods such as RPN
    [[112](#bib.bib112)], BG modeling [[113](#bib.bib113)], AutoLoc[[114](#bib.bib114)],
    CleanNet [[115](#bib.bib115)], DGAM [[116](#bib.bib116)], STPN [[117](#bib.bib117)]
    , BaSNet [[118](#bib.bib118)] , MAAN [[119](#bib.bib119)], and CMCS [[120](#bib.bib120)].'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '类无关的注意力模块直接从原始数据中计算注意力向量 $\lambda$ (def [17](#Thmdefinition17 "Definition 17\.
    ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action
    Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep
    Learning-based Action Detection in Untrimmed Videos: A Survey"))，通过在视频特征上应用全连接和
    ReLU 层，然后通过 sigmoid 函数将注意力权重缩放到 $[0,1]$。许多方法如 RPN [[112](#bib.bib112)], BG 建模
    [[113](#bib.bib113)], AutoLoc[[114](#bib.bib114)], CleanNet [[115](#bib.bib115)],
    DGAM [[116](#bib.bib116)], STPN [[117](#bib.bib117)] , BaSNet [[118](#bib.bib118)]
    , MAAN [[119](#bib.bib119)] 和 CMCS [[120](#bib.bib120)] 中都使用了学习类无关的注意力权重。'
- en: 'Class-agnostic attention learning with cross-entropy: The video-level class-agnostic
    foreground and background features $f$ and $b$ (def [18](#Thmdefinition18 "Definition
    18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4
    Action Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")) are fed
    to a classification module, and supervised with a cross entropy loss:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '跨熵的无类关注学习：视频级别的无类前景和背景特征$f$和$b$（参见[18](#Thmdefinition18 "Definition 18\. ‣
    2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4 Action
    Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods ‣ Deep
    Learning-based Action Detection in Untrimmed Videos: A Survey")）被输入到分类模块中，并通过交叉熵损失进行监督：'
- en: '|  | $p_{fg}[c]=\frac{\exp{(w_{c}\cdot f)}}{\sum_{i=0}^{C}\exp{(w_{i}\cdot
    f)}},\mathcal{L}_{fg}=-\log(p_{fg}[y]),$ |  | (17) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{fg}[c]=\frac{\exp{(w_{c}\cdot f)}}{\sum_{i=0}^{C}\exp{(w_{i}\cdot
    f)}},\mathcal{L}_{fg}=-\log(p_{fg}[y]),$ |  | (17) |'
- en: 'where $w_{c}$ s are the weights of the classification module, $C$ is the number
    of action classes in the entire dataset, and $y$ is the label of action that happens
    in the video. Also, label $0$ represents the background class. Similarly, $\mathcal{L}_{bg}$
    is defined for $p_{bg}$ which is a softmax applied over multiplication of background
    feature $b$ and the classification module. This loss trains attention vector $\lambda$
    through class-agnostic features $f$ and $b$ (def [18](#Thmdefinition18 "Definition
    18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4
    Action Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")), and is
    used in STPN [[117](#bib.bib117)].'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$w_{c}$是分类模块的权重，$C$是整个数据集中动作类别的数量，$y$是视频中发生的动作的标签。同时，标签$0$表示背景类别。同样，$\mathcal{L}_{bg}$是针对$p_{bg}$定义的，它是对背景特征$b$和分类模块的乘积应用的softmax。这种损失通过无类特征$f$和$b$（参见[18](#Thmdefinition18
    "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")）训练注意力向量$\lambda$，并在STPN
    [[117](#bib.bib117)]中使用。'
- en: 'Class-agnostic attention learning with clustering loss: Nguyen et al in background
    modeling [[113](#bib.bib113)] propose a method to separate foreground and background
    using a clustering loss by penalizing the discriminative capacity of background
    features. Class-agnostic foreground and background features $f$ and $b$ (def [18](#Thmdefinition18
    "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey"))
    are encouraged to be distinct using a clustering loss:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '聚类损失的无类关注学习：Nguyen等人在背景建模中[[113](#bib.bib113)]提出了一种使用聚类损失通过惩罚背景特征的区分能力来分离前景和背景的方法。无类前景和背景特征$f$和$b$（参见[18](#Thmdefinition18
    "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")）通过聚类损失被鼓励变得不同：'
- en: '|  | $z_{f}=\frac{\exp(uf)}{\exp(uf)+\exp(vf)}\ ,\ z_{b}=\frac{\exp(vb)}{\exp(ub)+\exp(vb)},$
    |  | (18) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{f}=\frac{\exp(uf)}{\exp(uf)+\exp(vf)}\ ,\ z_{b}=\frac{\exp(vb)}{\exp(ub)+\exp(vb)},$
    |  | (18) |'
- en: '|  | $\mathcal{L}_{cluster}=-\log{z_{f}}-\log{z_{b}},$ |  | (19) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{cluster}=-\log{z_{f}}-\log{z_{b}},$ |  | (19) |'
- en: 'where $u,v\in{\rm I\!R}^{d}$ are trainable parameters. Attention $\lambda$
    is trained by separating class-agnostic features $f$ and $b$ (def [18](#Thmdefinition18
    "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$u,v\in{\rm I\!R}^{d}$是可训练参数。注意力$\lambda$通过分离无类特征$f$和$b$（参见[18](#Thmdefinition18
    "Definition 18\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection
    ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal Action Detection
    Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")）进行训练。'
- en: 'Class-agnostic attention learning with prototypes: Prototypical network which
    was introduced in [[121](#bib.bib121)] for classification task, represents each
    class as a prototype and matches each instance with a prototype with highest similarity.
    During training, the semantically-related prototypes are pushed closer than unrelated
    prototypes. Huang et al. in RPN [[112](#bib.bib112)] proposed a prototype learning
    scheme for action localization. For temporal position $t$ and action class $c$,
    the similarity score $s_{t,c}$ between feature $x_{t}$ and prototype $p_{c}$ is
    computed and similarity vector $s_{t}$ consists of $s_{t,c}$ for all classes.
    Then the similarity vector $s_{t}$ is fused with attention score $\lambda_{t}$
    into a video-level score $\hat{s}$:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 无类别注意力学习与原型：原型网络（Prototypical network），如在[[121](#bib.bib121)]中引入的用于分类任务，将每个类别表示为一个原型，并将每个实例与相似度最高的原型进行匹配。在训练过程中，语义相关的原型被推得比无关的原型更近。黄等人在RPN
    [[112](#bib.bib112)]中提出了一种用于动作定位的原型学习方案。对于时间位置$t$和动作类别$c$，计算特征$x_{t}$与原型$p_{c}$之间的相似度分数$s_{t,c}$，相似度向量$s_{t}$由所有类别的$s_{t,c}$组成。然后，相似度向量$s_{t}$与注意力分数$\lambda_{t}$融合成视频级别分数$\hat{s}$：
- en: '|  | $s_{t,c}=-\left\lVert x_{t}-p_{c}\right\rVert^{2}_{2}\ \ ,\ \ \hat{s}=\sum_{t=1}^{T}\lambda_{t}s_{t}.$
    |  | (20) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{t,c}=-\left\lVert x_{t}-p_{c}\right\rVert^{2}_{2}\ \ ,\ \ \hat{s}=\sum_{t=1}^{T}\lambda_{t}s_{t}.$
    |  | (20) |'
- en: Score $\hat{s}$ is supervised by a classification loss with respect to the video-level
    labels, training attention scores $\lambda_{t}$.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 分数$\hat{s}$通过分类损失对视频级别标签进行监督，训练注意力分数$\lambda_{t}$。
- en: 'Class-agnostic attention learning with CVAE: DGAM [[116](#bib.bib116)] aims
    to separate actions from context frames by imposing different attentions on different
    features using a generative model, conditional VAE (CVAE) [[122](#bib.bib122)].
    Formally, the objective of DGAM is:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 无类别注意力学习与CVAE：DGAM [[116](#bib.bib116)] 旨在通过使用生成模型（条件变分自编码器 CVAE）[[122](#bib.bib122)]
    对不同特征施加不同的注意力，从而将动作与背景帧分离。形式上，DGAM的目标是：
- en: '|  | $\max_{\lambda\in[0,1]}\underbrace{\log p(y&#124;X,\lambda)}_{\text{term
    1}}+\underbrace{\log p(X&#124;\lambda)}_{\text{term 2}},$ |  | (21) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\lambda\in[0,1]}\underbrace{\log p(y\mid X,\lambda)}_{\text{项 1}}+\underbrace{\log
    p(X\mid \lambda)}_{\text{项 2}},$ |  | (21) |'
- en: where $X$ denotes the features, $y$ is the video-level label, and $\lambda$
    is the attention signal. Term 1 encourages high discriminative capability of the
    foreground feature $f$ and punishes any discriminative capability of the background
    feature $b$. Term 2 is approximated by a generative model which forces the feature
    representation $X$ to be accurately reconstructed from the attention $\lambda$
    using CVAE. By maximizing this conditional probability with respect to the attention,
    the frame-wise attention is optimized by imposing different attentions on different
    features, leading to separation of action and context frames.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$X$表示特征，$y$是视频级标签，$\lambda$是注意力信号。项1鼓励前景特征$f$具有高的判别能力，并惩罚背景特征$b$的任何判别能力。项2由生成模型近似，该模型强制使用CVAE从注意力$\lambda$中准确重建特征表示$X$。通过最大化相对于注意力的条件概率，逐帧注意力通过对不同特征施加不同的注意力进行优化，从而实现动作和背景帧的分离。
- en: 2.4.1.4   Direct Action Proposal Generation
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.4.1.4   直接动作提议生成
- en: Many methods [[117](#bib.bib117)], [[105](#bib.bib105)], [[123](#bib.bib123)],
    [[104](#bib.bib104)] localize the actions by applying thresholds on attention
    scores. The disadvantage of thresholding is that the snippets are treated independently
    and their temporal relations are neglected. Also, thresholding may not be robust
    to noises in class activation maps. Shou et al. [[114](#bib.bib114)] in AutoLoc
    directly predict the temporal boundary of each action instance. A localization
    branch is designed to directly predict the action boundaries (inner boundaries).
    The outer boundaries are also obtained by inflating the inner boundaries. Knowing
    that a video includes action class $c$, an outer-inner-contrastive (OIC) loss
    is applied on the activation scores of action $c$. The OIC loss computes the average
    activation in the outer area minus the average activation in the inner area to
    encourage high activations inside and penalize high activations outside because
    a complete action clip should look different from its neighbours. Liu et al. [[115](#bib.bib115)]
    proposed CleanNet to exploit temporal contrast for action localization. A contrast
    score is generated by summing up action, starting and ending scores for each action
    proposal. The action localization is trained by maximizing the average contrast
    score of the proposals, which penalizes fragmented short proposals and promotes
    completeness and continuity in action proposals.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方法 [[117](#bib.bib117)]、[[105](#bib.bib105)]、[[123](#bib.bib123)]、[[104](#bib.bib104)]
    通过对注意力得分应用阈值来定位动作。阈值的缺点在于片段被独立处理，忽略了它们的时间关系。此外，阈值可能对类激活图中的噪声不够鲁棒。Shou 等人 [[114](#bib.bib114)]
    在 AutoLoc 中直接预测每个动作实例的时间边界。设计了一个定位分支来直接预测动作边界（内部边界）。外部边界也通过扩展内部边界来获得。知道一个视频包含动作类别
    $c$ 后，对动作 $c$ 的激活得分应用外部-内部-对比（OIC）损失。OIC 损失计算外部区域的平均激活减去内部区域的平均激活，以鼓励内部高激活并惩罚外部高激活，因为完整的动作片段应与其邻近片段有所不同。Liu
    等人 [[115](#bib.bib115)] 提出了 CleanNet 来利用时间对比进行动作定位。通过对每个动作提议的动作、开始和结束得分进行求和来生成对比得分。通过最大化提议的平均对比得分来训练动作定位，这会惩罚破碎的短提议，并促进动作提议的完整性和连续性。
- en: 2.4.1.5   Action Completeness Modeling
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.4.1.5   行为完整性建模
- en: 'Previous methods used random hiding and iterative removal to enforce action
    completeness. Singh et al. in Hide-and-seek [[123](#bib.bib123)] force the model
    to see different parts of the video by randomly masking different regions of the
    videos in each training epoch. However, randomly hiding frames does not always
    guarantee the discovery of new parts and also disrupts the training process. Zhong
    et al. in Step-by-step erasion [[124](#bib.bib124)] trained a series of classifiers
    iteratively to find complementary parts, by erasing the predictions of predecessor
    classifiers from input videos. The major draw-back with this approach is the extra
    time cost and computational expense to train multiple classifiers. Zeng et al.
    [[125](#bib.bib125)] propose an iterative-winners-out strategy that selects the
    most discriminative action instances in each training iteration and hide them
    in the next iteration. Liu et al. in CMCS [[120](#bib.bib120)] proposed to enforce
    multiple branches in parallel to discover complementary pieces of an action. Each
    branch generates a different class activation map (def [15](#Thmdefinition15 "Definition
    15\. ‣ 2.4.1.1 Term Definition ‣ 2.4.1 Weakly-supervised Action Detection ‣ 2.4
    Action Detection with Limited Supervision ‣ 2 Temporal Action Detection Methods
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey")). A diversity
    loss (introduced in [[126](#bib.bib126)]) is imposed on class activation maps,
    which computes cosine similarities between pairs of branches and all action categories.
    Minimizing the diversity loss, encourages the branches to produce activations
    on different action parts.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方法使用随机隐藏和迭代删除来强制动作完整性。Singh 等人在 Hide-and-seek [[123](#bib.bib123)] 中通过在每个训练周期随机遮挡视频的不同区域，强制模型查看视频的不同部分。然而，随机隐藏帧并不总能保证发现新部分，并且会干扰训练过程。Zhong
    等人在 Step-by-step erasion [[124](#bib.bib124)] 中通过从输入视频中擦除前置分类器的预测，迭代训练一系列分类器以找到补充部分。这种方法的主要缺点是训练多个分类器的额外时间成本和计算开销。Zeng
    等人 [[125](#bib.bib125)] 提出了一个迭代的优胜者策略，该策略在每次训练迭代中选择最具区分性的动作实例，并在下一次迭代中隐藏它们。Liu
    等人在 CMCS [[120](#bib.bib120)] 中提出了并行执行多个分支以发现动作的补充部分。每个分支生成不同的类别激活图（见 [15](#Thmdefinition15
    "定义 15\. ‣ 2.4.1.1 术语定义 ‣ 2.4.1 弱监督动作检测 ‣ 2.4 动作检测与有限监督 ‣ 2 时间动作检测方法 ‣ 基于深度学习的非裁剪视频中的动作检测：综述")）。对类别激活图施加了多样性损失（在
    [[126](#bib.bib126)] 中引入），该损失计算分支对和所有动作类别之间的余弦相似度。最小化多样性损失，鼓励分支在不同动作部分上产生激活。
- en: 2.4.2 Unsupervised, Semi-supervised, and Self-supervised
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 无监督、半监督和自监督
- en: Although weakly-supervised action detection has been extensively studied in
    recent years, there are fewer articles addressing action detection task in unsupervised,
    semi-supervised, or self-supervised setting that are briefly reviewed here.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管弱监督动作检测近年来已被广泛研究，但关于无监督、半监督或自监督设置下的动作检测任务的文章较少，这里简要回顾了一些相关研究。
- en: 2.4.2.1   Unsupervised Action Detection
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.4.2.1   无监督动作检测
- en: Unsupervised learning does not need any human-annotated labels during training.
    Seneret et al. [[127](#bib.bib127)] introduced an iterative approach which alternates
    between discriminative learning of the appearance of sub-activities from visual
    features and generative modeling of the temporal structure of sub-activities.
    Kukleva et al. [[128](#bib.bib128)] proposed a combination of temporal encoding
    (generated using a frame time stamp prediction network) and a Viterbi decoding
    for consistent frame-to-cluster assignment. Gong et al. in ACL [[129](#bib.bib129)]
    used only the total count of unique actions that appear in the video set as supervisory
    signal. They propose a two-step clustering and localization iterative procedure.
    The clustering step provides noisy pseudo-labels for the localization step, and
    the localization step provides temporal co-attention models to improve the clustering
    performance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习在训练过程中不需要任何人工标注的标签。Seneret 等人 [[127](#bib.bib127)] 引入了一种迭代方法，该方法在从视觉特征中区分子活动的外观的学习和对子活动时间结构的生成建模之间交替进行。Kukleva
    等人 [[128](#bib.bib128)] 提出了将时间编码（通过帧时间戳预测网络生成）与 Viterbi 解码相结合的方法，以实现一致的帧到聚类分配。Gong
    等人在 ACL [[129](#bib.bib129)] 中仅使用视频集中出现的独特动作的总数作为监督信号。他们提出了一种两步聚类和定位的迭代过程。聚类步骤为定位步骤提供噪声伪标签，而定位步骤则提供时间共注意力模型以提高聚类性能。
- en: 2.4.2.2   Self-supervised Action Detection
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.4.2.2   自监督动作检测
- en: Self-supervised learning refers to training with pseudo labels where pseudo
    labels are automatically generated for a pre-defined pretext task without involving
    any human annotations. Chen et al. in SSTDA [[130](#bib.bib130)] proposed self-supervised
    temporal domain adaptation method to address the spatio-temporal variations (different
    people performing the tasks in different styles) in action segmentation. They
    designed two self-supervised auxiliary tasks, binary and sequential domain prediction,
    to jointly align local and global embedded feature spaces across domains. The
    binary domain prediction task predicts a single domain for each frame-level feature,
    and the sequential domain prediction task predicts the permutation of domains
    for an untrimmed video, both trained by adversarial training with a gradient reversal
    layer (GRL) [[131](#bib.bib131), [132](#bib.bib132)]. Jain et al. in Actionbytes
    [[109](#bib.bib109)] only use short trimmed videos during the training and train
    an action localization network with cluster assignments as pseudo-labels to segments
    a long untrimmed videos into interpretable fragments (called ActionBytes). They
    adopt a self-supervised iterative approach for training boundary-aware models
    from short videos by decomposing a trimmed video into ActionBytes and generate
    pseudo-labels to train a CNN to localize ActionBytes within videos.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是指使用伪标签进行训练，其中伪标签是为预定义的前置任务自动生成的，无需任何人工标注。Chen 等人在 SSTDA [[130](#bib.bib130)]
    中提出了一种自监督的时间领域适应方法，以解决动作分割中的时空变化（不同的人以不同的风格执行任务）。他们设计了两个自监督辅助任务：二进制领域预测和顺序领域预测，以共同对齐跨领域的局部和全局嵌入特征空间。二进制领域预测任务为每个帧级特征预测一个单一领域，而顺序领域预测任务为未修剪的视频预测领域的排列，这两者都通过对抗训练和梯度反转层（GRL）
    [[131](#bib.bib131), [132](#bib.bib132)] 进行训练。Jain 等人在 Actionbytes [[109](#bib.bib109)]
    中仅使用短视频进行训练，并训练一个动作定位网络，使用簇分配作为伪标签，将长视频分割成可解释的片段（称为 ActionBytes）。他们采用了一种自监督迭代方法，通过将修剪过的视频分解为
    ActionBytes 并生成伪标签来训练 CNN，从而在视频中定位 ActionBytes。
- en: 'TABLE II: The benchmark datasets for temporal and spatio-temporal action detection.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 时间和时空动作检测的基准数据集。'
- en: '| Dataset | Activities Types |  # Videos | # Action Categories | Avg Video
    Length (Sec) | # Action Instances (avg per video) | Multi-label ( # labels per
    frame) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 活动类型 |  视频数量 | 动作类别数 | 平均视频时长（秒） | 动作实例数量（每视频平均） | 多标签（每帧标签数） |'
- en: '| THUMOS [[1](#bib.bib1)] | Sports | 413 | 20 | 212 | 15.5 | No |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| THUMOS [[1](#bib.bib1)] | 体育 | 413 | 20 | 212 | 15.5 | 否 |'
- en: '| MultiTHUMOS [[57](#bib.bib57)] | Sports | 413 | 65 | 212 | 97 | Yes |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| MultiTHUMOS [[57](#bib.bib57)] | 体育 | 413 | 65 | 212 | 97 | 是 |'
- en: '| ActivityNet [[133](#bib.bib133)] | Human Activities | 19,994 | 200 | 115
    | 1.54 | No |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| ActivityNet [[133](#bib.bib133)] | 人类活动 | 19,994 | 200 | 115 | 1.54 | 否 |'
- en: '| HACS Segment [[134](#bib.bib134)] | Human Activities | 50K | 200 | 156 |
    2.8 | No |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| HACS Segment [[134](#bib.bib134)] | 人类活动 | 50K | 200 | 156 | 2.8 | 否 |'
- en: '| Charades [[135](#bib.bib135)] | Daily Activities | 9,848 | 157 | 30 | 6.75
    | Yes |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Charades [[135](#bib.bib135)] | 日常活动 | 9,848 | 157 | 30 | 6.75 | 是 |'
- en: '| Breakfast [[11](#bib.bib11)] | Cooking | 1712 | 48 | 162 | 6 | No |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Breakfast [[11](#bib.bib11)] | 烹饪 | 1712 | 48 | 162 | 6 | 否 |'
- en: '| 50Salads [[136](#bib.bib136)] | Cooking | 50 | 17 | 384 | 20 | No |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 50Salads [[136](#bib.bib136)] | 烹饪 | 50 | 17 | 384 | 20 | 否 |'
- en: '| MPII cooking 2 [[137](#bib.bib137)] | Cooking | 273 | 59 | 356 | 51.6 | No
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| MPII cooking 2 [[137](#bib.bib137)] | 烹饪 | 273 | 59 | 356 | 51.6 | 否 |'
- en: '| COIN [[138](#bib.bib138)] | Daily Activities | 11,827 | 180 | 142 | 3.9 |
    No |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| COIN [[138](#bib.bib138)] | 日常活动 | 11,827 | 180 | 142 | 3.9 | 否 |'
- en: '| Ava [[86](#bib.bib86)] | Movies | 437 | 80 | 900 | 3361.5 | Yes |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Ava [[86](#bib.bib86)] | 电影 | 437 | 80 | 900 | 3361.5 | 是 |'
- en: 2.4.2.3   Semi-supervised Action Detection
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.4.2.3   半监督动作检测
- en: In Semi-supervised setting, a small number of videos are fully annotated with
    the temporal boundary of actions and class labels while a large number of videos
    are either unlabeled or include only video-level labels. Ji et al. [[139](#bib.bib139)]
    employ a fully supervised framework, known as BSN [[46](#bib.bib46)], to exploit
    the small set of labeled data. They encode the input video into a feature sequence
    and apply sequential perturbations (time warping and time masking [[140](#bib.bib140)])
    on it. Then, the student proposal model takes this perturbed sequence as the input
    but the teacher model predicts directly on the original feature sequence. In the
    end, the student model is jointly optimized with a supervised loss applied to
    labeled videos and a consistency loss to all videos.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督设置中，一小部分视频有完整的时间边界和类别标签，而大量视频则未标注或仅包含视频级标签。Ji等人[[139](#bib.bib139)]采用了一种全监督框架，称为BSN
    [[46](#bib.bib46)]，以利用少量的标记数据。他们将输入视频编码为特征序列，并对其应用序列扰动（时间扭曲和时间掩码[[140](#bib.bib140)]）。然后，学生提议模型以这种扰动序列作为输入，而教师模型则直接在原始特征序列上进行预测。最后，学生模型通过对标记视频应用监督损失和对所有视频应用一致性损失进行联合优化。
- en: 3 Datasets and Evaluation
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集与评估
- en: In this section, we describe the datasets collected for action detection and
    the evaluation metrics for this task.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们描述了用于动作检测的数据集以及该任务的评估指标。
- en: 3.1 Datasets
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集
- en: 'Gaidon et al. [[141](#bib.bib141), [142](#bib.bib142)] introduced the problem
    of temporally localizing the actions in untrimmed videos, focusing on limited
    actions such as “drinking and smoking” [[67](#bib.bib67)] and “open door and sitdown”
    [[143](#bib.bib143)]. Later, researchers worked on building the following datasets
    that include large number of untrimmed videos with multiple action categories
    and complex background information. Some of these datasets target activities of
    high-level semantics (such as sports) while others include fine-grained activities
    (such as cooking). The details are summarized in Table [II](#S2.T2 "TABLE II ‣
    2.4.2.2 Self-supervised Action Detection ‣ 2.4.2 Unsupervised, Semi-supervised,
    and Self-supervised ‣ 2.4 Action Detection with Limited Supervision ‣ 2 Temporal
    Action Detection Methods ‣ Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey").'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gaidon 等人[[141](#bib.bib141), [142](#bib.bib142)] 提出了在未裁剪视频中进行时间定位动作的问题，重点关注诸如“饮水和吸烟”[[67](#bib.bib67)]和“开门和坐下”[[143](#bib.bib143)]等有限动作。后来，研究人员致力于建立以下数据集，这些数据集包含大量未裁剪的视频，具有多种动作类别和复杂的背景信息。这些数据集中有些针对高层次语义的活动（如体育），而其他则包括细粒度的活动（如烹饪）。详细信息总结在表
    [II](#S2.T2 "TABLE II ‣ 2.4.2.2 Self-supervised Action Detection ‣ 2.4.2 Unsupervised,
    Semi-supervised, and Self-supervised ‣ 2.4 Action Detection with Limited Supervision
    ‣ 2 Temporal Action Detection Methods ‣ Deep Learning-based Action Detection in
    Untrimmed Videos: A Survey") 中。'
- en: $\bullet$ THUMOS14 [[1](#bib.bib1)] is the most widely used dataset for temporal
    action localization. There are $220$ and $213$ videos for training and testing
    with temporal annotations in $20$ classes. Action instances are rather sparsely
    distributed through the videos and about $70\%$ of all frames are labeled as background.
    The number of action instances per video on average is $15.5$ (and $1.1$ for distinct
    action instances). Also, maximum number of distinct actions per video is 3.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ THUMOS14 [[1](#bib.bib1)] 是用于时间动作定位的最广泛使用的数据集。该数据集包含 $220$ 个训练视频和
    $213$ 个测试视频，具有 $20$ 个类别的时间注释。动作实例在视频中分布较稀疏，约 $70\%$ 的帧被标记为背景。每个视频中的平均动作实例数量为 $15.5$（其中独特动作实例为
    $1.1$）。每个视频的最大独特动作数量为 3。
- en: $\bullet$ MultiTHUMOS [[57](#bib.bib57)] has the same set of videos as in THUMOS14
    [[1](#bib.bib1)], but it extends the latter from $20$ action classes with $0.3$
    labels per frame to $65$ classes with $1.5$ labels per frame. Also, the average
    number of distinct action classes in a video is $10.5$ (compared to $1.1$ in THUMOS14),
    making it a more challenging multi-label dataset. Also, maximum number of distinct
    actions per video is 25.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MultiTHUMOS [[57](#bib.bib57)] 拥有与 THUMOS14 [[1](#bib.bib1)] 相同的视频集，但将后者从
    $20$ 个动作类别和每帧 $0.3$ 个标签扩展到 $65$ 个类别和每帧 $1.5$ 个标签。此外，每个视频中的平均独特动作类别数量为 $10.5$（相比
    THUMOS14 中的 $1.1$），使其成为一个更具挑战性的多标签数据集。每个视频的最大独特动作数量为 25。
- en: $\bullet$ ActivityNet [[133](#bib.bib133)] has two versions, v1.2 and v1.3\.
    The former contains $9,682$ videos in $100$ classes, while the latter, which is
    a superset of v1.2 and was used in the ActivityNet Challenge 2016, contains $19,994$
    videos in $200$ classes. In each version, the dataset is divided into three disjoint
    subsets, training, validation, and testing, by 2:1:1.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ ActivityNet[[133](#bib.bib133)]有两个版本，v1.2和v1.3。前者包含$9,682$个视频，分为$100$类，而后者是v1.2的超集，并在ActivityNet
    Challenge 2016中使用，包含$19,994$个视频，分为$200$类。在每个版本中，数据集被分为三个不重叠的子集：训练、验证和测试，比例为2:1:1。
- en: $\bullet$ HACS [[134](#bib.bib134)] includes $504K$ untrimmed videos retrieved
    from YouTube where each one is strictly shorter than $4$ minutes. HACS clips consists
    of $1.5M$ annotated clips of 2-second duration and HACS Segments contains $139K$
    action segments densely annotated in $50K$ untrimmed videos spanning $200$ action
    categories.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ HACS[[134](#bib.bib134)]包括$504K$个从YouTube检索的未修剪视频，每个视频的时长均严格少于$4$分钟。HACS片段包含$1.5M$个注释时长为2秒的片段，HACS
    Segments包含$139K$个在$50K$个未修剪视频中密集标注的动作片段，涵盖$200$个动作类别。
- en: $\bullet$ CHARADES [[135](#bib.bib135)] consists of $9,848$ videos recorded
    by Amazon Mechanical Turk users based on provided scripts. This dataset contains
    videos with multiple actions and involves daily life activities from $157$ classes
    of $267$ people from three continents. Over $15\%$ of the videos have more than
    one person.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CHARADES[[135](#bib.bib135)]由$9,848$个视频组成，这些视频由Amazon Mechanical Turk用户根据提供的脚本录制。该数据集包含多动作视频，涉及来自三大洲$267$人的$157$类日常生活活动。超过$15\%$的视频中有不止一个人。
- en: $\bullet$ Breakfast [[11](#bib.bib11)] includes $1712$ videos for breakfast
    preparation activities performed by $52$ subjects. The videos were recorded in
    $18$ different kitchens and belong to $10$ different types of breakfast activities
    (such as fried egg or coffee) which consist of $48$ different fine-grained actions.
    Each video contains $6$ action instances on average and only $7\%$ of the frames
    are background.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 早餐[[11](#bib.bib11)]包括$1712$个由$52$名受试者执行的早餐准备活动的视频。这些视频在$18$个不同的厨房录制，属于$10$种不同类型的早餐活动（如煎蛋或咖啡），涵盖$48$种不同的细粒度动作。每个视频平均包含$6$个动作实例，且仅有$7\%$的画面是背景。
- en: $\bullet$ 50Salads [[136](#bib.bib136)] contains $50$ videos for salad preparation
    activities performed by $25$ subjects and with $17$ distinct action classes. On
    average, each video contains $20$ action instances and is $6.4$ minutes long.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 50Salads[[136](#bib.bib136)]包含$50$个由$25$名受试者执行的沙拉准备活动视频，涵盖$17$种不同的动作类别。平均而言，每个视频包含$20$个动作实例，时长为$6.4$分钟。
- en: $\bullet$ MPII Cooking 2 [[137](#bib.bib137)] consists of $273$ videos with
    about $2.8$ million frames. There are $59$ action classes and about $29\%$ of
    the frames are background. The dataset provides a fixed split into a train and
    test set, separating $220$ videos for training.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ MPII Cooking 2[[137](#bib.bib137)]包含$273$个视频，总计约$2.8$百万帧。共有$59$种动作类别，大约$29\%$的帧是背景。数据集提供了固定的训练和测试集划分，$220$个视频用于训练。
- en: $\bullet$ COIN dataset [[138](#bib.bib138)],[[144](#bib.bib144)] contains $180$
    tasks and $11,827$ videos and $46,354$ annotated segments. The videos are collected
    from YouTube in $12$ domains (e.g., vehicles, gadgets, etc.) related to daily
    activities.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ COIN数据集[[138](#bib.bib138)],[[144](#bib.bib144)]包含$180$个任务和$11,827$个视频以及$46,354$个注释片段。这些视频从YouTube收集，涉及$12$个领域（如车辆、设备等），与日常活动相关。
- en: $\bullet$ AVA [[86](#bib.bib86)] is designed for spatio-temporal action detection
    and consists of $437$ videos where each video is a $15$ minute segment taken from
    a movie. Each person appearing in a test video must be detected in each frame
    and the multi-label actions of the detected person must be predicted correctly.
    The action label space contains $80$ atomic action classes but often the results
    are reported on the most frequent $60$ classes.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ AVA[[86](#bib.bib86)]旨在进行时空动作检测，由$437$个视频组成，每个视频是从电影中提取的$15$分钟片段。测试视频中出现的每个人必须在每一帧中被检测到，并且必须准确预测被检测到的人的多标签动作。动作标签空间包含$80$个原子动作类别，但结果通常报告在最常见的$60$类上。
- en: 3.2 Evaluation Metrics
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估指标
- en: Here, we discuss the metrics designed to evaluate the performance of proposal
    generation, temporal action detection, and spatio-temporal action detection.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了用于评估提案生成、时间动作检测和时空动作检测性能的指标。
- en: 'Temporal Action Proposal Generation. For this task, Average Recall (AR) with
    multiple IoU thresholds is usually used as evaluation metrics. Most methods use
    IoU thresholds set $[0.5$ : $0.05$ : $0.95]$ in ActivityNet-1.3 [[133](#bib.bib133)]
    and $[0.5:0.05:1.0]$ in THUMOS14 [[1](#bib.bib1)]. To evaluate the relation between
    recall and proposals number, most methods evaluate AR with Average Number of proposals
    (AN) on both datasets, which is denoted as AR@AN. On ActivityNet-1.3, area under
    the AR vs. AN curve (AUC) is also used as metrics, where AN varies from $0$ to
    $100$.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '时间动作提议生成。对于这个任务，通常使用多个 IoU 阈值的平均召回率（AR）作为评估指标。大多数方法在 ActivityNet-1.3 [[133](#bib.bib133)]
    中使用 IoU 阈值集 $[0.5$ : $0.05$ : $0.95]$，在 THUMOS14 [[1](#bib.bib1)] 中使用 $[0.5:0.05:1.0]$。为了评估召回率与提议数量之间的关系，大多数方法在两个数据集上评估
    AR 和提议的平均数量（AN），表示为 AR@AN。在 ActivityNet-1.3 上，还使用 AR 对 AN 曲线下的面积（AUC）作为指标，其中 AN
    从 $0$ 变到 $100$。'
- en: Temporal Action Detection. For this task, mean Average Precision (mAP) is used
    as evaluation metric, where Average Precision (AP) is calculated on each action
    class, respectively. On ActivityNet-1.3 [[133](#bib.bib133)], mAP with IoU thresholds
    $\{0.5,0.75,0.95\}$ and average mAP with IoU thresholds set $[0.5:0.05:0.95]$
    are often used. On THUMOS14[[1](#bib.bib1)], mAP with IoU thresholds $\{0.3,0.4,0.5,0.6,0.7\}$
    is used.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 时间动作检测。对于这个任务，平均精度（mAP）作为评估指标，其中每个动作类别的平均精度（AP）分别计算。在 ActivityNet-1.3 [[133](#bib.bib133)]
    上，通常使用 IoU 阈值 $\{0.5,0.75,0.95\}$ 的 mAP 和 IoU 阈值集 $[0.5:0.05:0.95]$ 的平均 mAP。在
    THUMOS14[[1](#bib.bib1)] 上，使用 IoU 阈值 $\{0.3,0.4,0.5,0.6,0.7\}$ 的 mAP。
- en: Spatio-temporal Action Detection. Two metrics are frequently used for this task.
    First, frame-AP measures the area under the precision-recall curve of the detections
    for each frame. A detection is correct if the intersection-over-union with the
    ground truth at that frame is greater than a threshold and the action label is
    correctly predicted. Second, video-AP measures the area under the precision-recall
    curve of the action tubes predictions. A tube is correct if the mean per frame
    intersection-over-union with the ground truth across the frames of the video is
    greater than a threshold and the action label is correctly predicted.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 空间-时间动作检测。这个任务常用两个指标。首先，frame-AP衡量每一帧检测的精确度-召回率曲线下的面积。如果该帧与真实标签的交并比大于阈值且动作标签预测正确，则检测为正确。其次，video-AP衡量动作管道预测的精确度-召回率曲线下的面积。如果视频帧的平均交并比大于阈值且动作标签预测正确，则管道为正确。
- en: 3.3 Performance Analysis
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 性能分析
- en: 'Action detection results of the state-of-the-art methods on THUMOS14 [[1](#bib.bib1)]
    and ActivityNet [[133](#bib.bib133)] dataset are compared by mAP (%) in Tables
    [III](#S3.T3 "TABLE III ‣ 3.3 Performance Analysis ‣ 3 Datasets and Evaluation
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey") and [IV](#S3.T4
    "TABLE IV ‣ 3.3 Performance Analysis ‣ 3 Datasets and Evaluation ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey") respectively. The methods are
    categorized to fully-supervised, weakly-supervised, semi-supervised, self-supervised
    and US (unsupervised). We also summarize the advantageous and limitations of fully-supervised
    methods and methods with limited supervision in Tables [V](#S3.T5 "TABLE V ‣ 3.3.1
    Fully-supervised Methods ‣ 3.3 Performance Analysis ‣ 3 Datasets and Evaluation
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey") and [VI](#S3.T6
    "TABLE VI ‣ 3.3.2 Methods with Limited Supervision ‣ 3.3 Performance Analysis
    ‣ 3 Datasets and Evaluation ‣ Deep Learning-based Action Detection in Untrimmed
    Videos: A Survey").'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '最先进方法在THUMOS14 [[1](#bib.bib1)] 和 ActivityNet [[133](#bib.bib133)] 数据集上的动作检测结果通过mAP（%）进行比较，结果见表
    [III](#S3.T3 "TABLE III ‣ 3.3 Performance Analysis ‣ 3 Datasets and Evaluation
    ‣ Deep Learning-based Action Detection in Untrimmed Videos: A Survey") 和 [IV](#S3.T4
    "TABLE IV ‣ 3.3 Performance Analysis ‣ 3 Datasets and Evaluation ‣ Deep Learning-based
    Action Detection in Untrimmed Videos: A Survey")。这些方法被分类为完全监督、弱监督、半监督、自监督和US（无监督）。我们还在表
    [V](#S3.T5 "TABLE V ‣ 3.3.1 Fully-supervised Methods ‣ 3.3 Performance Analysis
    ‣ 3 Datasets and Evaluation ‣ Deep Learning-based Action Detection in Untrimmed
    Videos: A Survey") 和 [VI](#S3.T6 "TABLE VI ‣ 3.3.2 Methods with Limited Supervision
    ‣ 3.3 Performance Analysis ‣ 3 Datasets and Evaluation ‣ Deep Learning-based Action
    Detection in Untrimmed Videos: A Survey") 中总结了完全监督方法和有限监督方法的优缺点。'
- en: 'TABLE III: Action detection results of the-state-of-the-art on testing set
    of THUMOS-14, measured by mAP (%) at tIoU thresholds.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE III: 通过 mAP（%）在 tIoU 阈值下测量的 THUMOS-14 测试集上最先进的动作检测结果。'
- en: '| Supervision | Method | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 监督方式 | 方法 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 |'
- en: '| Fully supervised | Yeung et al. [[56](#bib.bib56)] | 36.0 | 26.4 | 17.1 |
    - | - |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 完全监督 | Yeung et al. [[56](#bib.bib56)] | 36.0 | 26.4 | 17.1 | - | - |'
- en: '| SMS [[145](#bib.bib145)] | 36.5 | 27.8 | 17.8 | - | - |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| SMS [[145](#bib.bib145)] | 36.5 | 27.8 | 17.8 | - | - |'
- en: '| SCNN [[19](#bib.bib19)] | 36.3 | 28.7 | 19 | - | - |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| SCNN [[19](#bib.bib19)] | 36.3 | 28.7 | 19 | - | - |'
- en: '| Sst [[53](#bib.bib53)] | - | - | 23.0 | - | - |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Sst [[53](#bib.bib53)] | - | - | 23.0 | - | - |'
- en: '| CDC [[14](#bib.bib14)] | 40.1 | 29.4 | 23.3 | 13.1 | 7.9 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| CDC [[14](#bib.bib14)] | 40.1 | 29.4 | 23.3 | 13.1 | 7.9 |'
- en: '| SSAD [[35](#bib.bib35)] | 43 | 35 | 24.6 | - | - |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| SSAD [[35](#bib.bib35)] | 43 | 35 | 24.6 | - | - |'
- en: '| TCN [[146](#bib.bib146)] | - | 33.3 | 25.6 | 15.9 | 9.0 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| TCN [[146](#bib.bib146)] | - | 33.3 | 25.6 | 15.9 | 9.0 |'
- en: '| TURN TAP [[27](#bib.bib27)] | 44.1 | 34.9 | 25.6 | - | - |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| TURN TAP [[27](#bib.bib27)] | 44.1 | 34.9 | 25.6 | - | - |'
- en: '| R-C3D [[30](#bib.bib30)] | 44.8 | 35.6 | 28.9 | - | - |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| R-C3D [[30](#bib.bib30)] | 44.8 | 35.6 | 28.9 | - | - |'
- en: '| SS-TAD [[54](#bib.bib54)] | 45.7 | - | 29.2 | - | 9.6 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| SS-TAD [[54](#bib.bib54)] | 45.7 | - | 29.2 | - | 9.6 |'
- en: '| SSN [[18](#bib.bib18)] | 51.9 | 41.0 | 29.8 | - | - |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| SSN [[18](#bib.bib18)] | 51.9 | 41.0 | 29.8 | - | - |'
- en: '| CTAP [[52](#bib.bib52)] | - | - | 29.9 | - | - |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| CTAP [[52](#bib.bib52)] | - | - | 29.9 | - | - |'
- en: '| CBR [[28](#bib.bib28)] | 50.1 | 41.3 | 31.0 | 19.1 | 9.9 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| CBR [[28](#bib.bib28)] | 50.1 | 41.3 | 31.0 | 19.1 | 9.9 |'
- en: '| S3D[[36](#bib.bib36)] | 47.9 | 41.2 | 32.6 | 23.3 | 14.3 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| S3D[[36](#bib.bib36)] | 47.9 | 41.2 | 32.6 | 23.3 | 14.3 |'
- en: '| DBS [[15](#bib.bib15)] | 50.6 | 43.1 | 34.3 | 24.4 | 14.7 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| DBS [[15](#bib.bib15)] | 50.6 | 43.1 | 34.3 | 24.4 | 14.7 |'
- en: '| BSN [[46](#bib.bib46)] | 53.5 | 45.0 | 36.9 | 28.4 | 20.0 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| BSN [[46](#bib.bib46)] | 53.5 | 45.0 | 36.9 | 28.4 | 20.0 |'
- en: '| MGG [[40](#bib.bib40)] | 53.9 | 46.8 | 37.4 | 29.5 | 21.3 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| MGG [[40](#bib.bib40)] | 53.9 | 46.8 | 37.4 | 29.5 | 21.3 |'
- en: '| AGCN [[31](#bib.bib31)] | 57.1 | 51.6 | 38.6 | 28.9 | 17.0 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| AGCN [[31](#bib.bib31)] | 57.1 | 51.6 | 38.6 | 28.9 | 17.0 |'
- en: '| GTAN [[147](#bib.bib147)] | 57.8 | 47.2 | 38.8 | - | - |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| GTAN [[147](#bib.bib147)] | 57.8 | 47.2 | 38.8 | - | - |'
- en: '| BMN [[47](#bib.bib47)] | 56.0 | 47.4 | 38.8 | 29.7 | 20.5 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| BMN [[47](#bib.bib47)] | 56.0 | 47.4 | 38.8 | 29.7 | 20.5 |'
- en: '| SRG[[148](#bib.bib148)] | 54.5 | 46.9 | 39.1 | 31.4 | 22.2 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| SRG[[148](#bib.bib148)] | 54.5 | 46.9 | 39.1 | 31.4 | 22.2 |'
- en: '| DBG [[48](#bib.bib48)] | 57.8 | 49.4 | 39.8 | 30.2 | 21.7 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| DBG [[48](#bib.bib48)] | 57.8 | 49.4 | 39.8 | 30.2 | 21.7 |'
- en: '| G-TAD [[62](#bib.bib62)] | 54.5 | 47.6 | 40.2 | 30.8 | 23.4 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| G-TAD [[62](#bib.bib62)] | 54.5 | 47.6 | 40.2 | 30.8 | 23.4 |'
- en: '| BC-GNN [[49](#bib.bib49)] | 57.1 | 49.1 | 40.4 | 31.2 | 23.1 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| BC-GNN [[49](#bib.bib49)] | 57.1 | 49.1 | 40.4 | 31.2 | 23.1 |'
- en: '| BSN++ [[149](#bib.bib149)] | 59.9 | 49.5 | 41.3 | 31.9 | 22.8 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| BSN++ [[149](#bib.bib149)] | 59.9 | 49.5 | 41.3 | 31.9 | 22.8 |'
- en: '| TAL-Net [[13](#bib.bib13)] | 53.2 | 48.5 | 42.8 | 33.8 | 20.8 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| TAL-Net [[13](#bib.bib13)] | 53.2 | 48.5 | 42.8 | 33.8 | 20.8 |'
- en: '| TSA-Net [[33](#bib.bib33)] | 55.8 | 52.0 | 44.1 | 33.0 | 21.8 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| TSA-Net [[33](#bib.bib33)] | 55.8 | 52.0 | 44.1 | 33.0 | 21.8 |'
- en: '| BU [[150](#bib.bib150)] | 53.9 | 50.7 | 45.4 | 38.0 | 28.5 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| BU [[150](#bib.bib150)] | 53.9 | 50.7 | 45.4 | 38.0 | 28.5 |'
- en: '| A2Net [[50](#bib.bib50)] | 58.6 | 54.1 | 45.5 | 32.5 | 17.2 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| A2Net [[50](#bib.bib50)] | 58.6 | 54.1 | 45.5 | 32.5 | 17.2 |'
- en: '| ATAG [[63](#bib.bib63)] | 62.0 | 53.1 | 47.3 | 38.0 | 28.0 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| ATAG [[63](#bib.bib63)] | 62.0 | 53.1 | 47.3 | 38.0 | 28.0 |'
- en: '| Lianli et al. [[151](#bib.bib151)] | 66.4 | 58.4 | 48.8 | 36.7 | 25.5 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Lianli et al. [[151](#bib.bib151)] | 66.4 | 58.4 | 48.8 | 36.7 | 25.5 |'
- en: '| PGCN [[61](#bib.bib61)] | 63.6 | 57.8 | 49.1 | - | - |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| PGCN [[61](#bib.bib61)] | 63.6 | 57.8 | 49.1 | - | - |'
- en: '|  | TadTR [[152](#bib.bib152)] | 62.4 | 57.4 | 49.2 | 37.8 | 26.3 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | TadTR [[152](#bib.bib152)] | 62.4 | 57.4 | 49.2 | 37.8 | 26.3 |'
- en: '|  | AFNet [[32](#bib.bib32)] | 63.4 | 58.5 | 49.5 | 36.9 | 23.5 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | AFNet [[32](#bib.bib32)] | 63.4 | 58.5 | 49.5 | 36.9 | 23.5 |'
- en: '|  | AGT [[65](#bib.bib65)] | 65.0 | 58.1 | 50.2 |  |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | AGT [[65](#bib.bib65)] | 65.0 | 58.1 | 50.2 |  |  |'
- en: '|  | PBRNet [[153](#bib.bib153)] | 58.5 | 54.6 | 51.3 | 41.8 | 29.5 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | PBRNet [[153](#bib.bib153)] | 58.5 | 54.6 | 51.3 | 41.8 | 29.5 |'
- en: '|  | RTD-Net[[66](#bib.bib66)] | 68.3 | 62.3 | 51.9 | 38.8 | 23.7 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | RTD-Net[[66](#bib.bib66)] | 68.3 | 62.3 | 51.9 | 38.8 | 23.7 |'
- en: '|  | C-TCN [[43](#bib.bib43)] | 68.0 | 62.3 | 52.1 | - | - |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | C-TCN [[43](#bib.bib43)] | 68.0 | 62.3 | 52.1 | - | - |'
- en: '|  | VSGN [[64](#bib.bib64)] | 66.7 | 60.4 | 52.4 | 41.0 | 30.4 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | VSGN [[64](#bib.bib64)] | 66.7 | 60.4 | 52.4 | 41.0 | 30.4 |'
- en: '|  | MLTPN [[44](#bib.bib44)] | 66.0 | 62.6 | 53.3 | 37.0 | 21.2 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | MLTPN [[44](#bib.bib44)] | 66.0 | 62.6 | 53.3 | 37.0 | 21.2 |'
- en: '|  | TSP [[154](#bib.bib154)] | 69.1 | 63.3 | 53.5 | 40.4 | 26.0 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | TSP [[154](#bib.bib154)] | 69.1 | 63.3 | 53.5 | 40.4 | 26.0 |'
- en: '|  | DaoTAD [[155](#bib.bib155)] | 62.8 | 59.5 | 53.8 | 43.6 | 30.1 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | DaoTAD [[155](#bib.bib155)] | 62.8 | 59.5 | 53.8 | 43.6 | 30.1 |'
- en: '|  | AFSD [[51](#bib.bib51)] | 67.3 | 62.4 | 55.5 | 43.7 | 31.1 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | AFSD [[51](#bib.bib51)] | 67.3 | 62.4 | 55.5 | 43.7 | 31.1 |'
- en: '|  | SP-TAD [[156](#bib.bib156)] | 69.2 | 63.3 | 55.9 | 45.7 | 33.4 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | SP-TAD [[156](#bib.bib156)] | 69.2 | 63.3 | 55.9 | 45.7 | 33.4 |'
- en: '|  | Liu et al.[[157](#bib.bib157)] | 68.9 | 64.0 | 56.9 | 46.3 | 31.0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | Liu et al.[[157](#bib.bib157)] | 68.9 | 64.0 | 56.9 | 46.3 | 31.0 |'
- en: '| Weakly supervised | Hide-Seek [[123](#bib.bib123)] | 19.5 | 12.7 | 6.8 |
    - | - |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 弱监督 | Hide-Seek [[123](#bib.bib123)] | 19.5 | 12.7 | 6.8 | - | - |'
- en: '| UNet [[104](#bib.bib104)] | 28.2 | 21.1 | 13.7 | - | - |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| UNet [[104](#bib.bib104)] | 28.2 | 21.1 | 13.7 | - | - |'
- en: '| Step-by-step [[124](#bib.bib124)] | 31.1 | 22.5 | 15.9 | - | - |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 逐步法 [[124](#bib.bib124)] | 31.1 | 22.5 | 15.9 | - | - |'
- en: '| STPN [[117](#bib.bib117)] | 35.5 | 25.8 | 16.9 | 9.9 | 4.3 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| STPN [[117](#bib.bib117)] | 35.5 | 25.8 | 16.9 | 9.9 | 4.3 |'
- en: '| MAAN [[119](#bib.bib119)] | 41.1 | 30.6 | 20.3 | 12 | 6.9 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| MAAN [[119](#bib.bib119)] | 41.1 | 30.6 | 20.3 | 12 | 6.9 |'
- en: '| AutoLoc [[114](#bib.bib114)] | 35.8 | 29 | 21.2 | 13.4 | 5.8 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| AutoLoc [[114](#bib.bib114)] | 35.8 | 29 | 21.2 | 13.4 | 5.8 |'
- en: '| W-TALC [[105](#bib.bib105)] | 40.1 | 31.1 | 22.8 | - | 7.6 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| W-TALC [[105](#bib.bib105)] | 40.1 | 31.1 | 22.8 | - | 7.6 |'
- en: '| STAR [[158](#bib.bib158)] | 48.7 | 34.7 | 23 | - | - |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| STAR [[158](#bib.bib158)] | 48.7 | 34.7 | 23 | - | - |'
- en: '| CMCS [[120](#bib.bib120)] | 41.2 | 32.1 | 23.1 | 15 | 7 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| CMCS [[120](#bib.bib120)] | 41.2 | 32.1 | 23.1 | 15 | 7 |'
- en: '| AdapNet [[159](#bib.bib159)] | 41.09 | 31.61 | 23.65 | 14.53 | 7.75 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| AdapNet [[159](#bib.bib159)] | 41.09 | 31.61 | 23.65 | 14.53 | 7.75 |'
- en: '| Cleannet [[115](#bib.bib115)] | 37 | 30.9 | 23.9 | 13.9 | 7.1 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Cleannet [[115](#bib.bib115)] | 37 | 30.9 | 23.9 | 13.9 | 7.1 |'
- en: '| TSM [[160](#bib.bib160)] | 39.5 | 31.9 | 24.5 | 13.8 | 7.1 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| TSM [[160](#bib.bib160)] | 39.5 | 31.9 | 24.5 | 13.8 | 7.1 |'
- en: '| 3C-Net [[102](#bib.bib102)] | 40.9 | 32.3 | 24.6 | - | 7.7 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 3C-Net [[102](#bib.bib102)] | 40.9 | 32.3 | 24.6 | - | 7.7 |'
- en: '| Shen et al [[161](#bib.bib161)] | 44 | 34.4 | 25.5 | 15.2 | 7.2 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Shen et al [[161](#bib.bib161)] | 44 | 34.4 | 25.5 | 15.2 | 7.2 |'
- en: '| Action Graphs [[108](#bib.bib108)] | 47.3 | 36.4 | 26.1 | - | - |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Action Graphs [[108](#bib.bib108)] | 47.3 | 36.4 | 26.1 | - | - |'
- en: '| BG modeling [[113](#bib.bib113)] | 46.6 | 37.5 | 26.8 | 17.6 | 9 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| BG modeling [[113](#bib.bib113)] | 46.6 | 37.5 | 26.8 | 17.6 | 9 |'
- en: '| BaSNet [[118](#bib.bib118)] | 44.6 | 36 | 27 | 18.6 | 10.4 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| BaSNet [[118](#bib.bib118)] | 44.6 | 36 | 27 | 18.6 | 10.4 |'
- en: '| RPN [[112](#bib.bib112)] | 48.2 | 37.2 | 27.9 | 16.7 | 8.1 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| RPN [[112](#bib.bib112)] | 48.2 | 37.2 | 27.9 | 16.7 | 8.1 |'
- en: '| TSCN [[162](#bib.bib162)] | 47.8 | 37.7 | 28.7 | 19.4 | 10.2 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| TSCN [[162](#bib.bib162)] | 47.8 | 37.7 | 28.7 | 19.4 | 10.2 |'
- en: '| DGAM [[116](#bib.bib116)] | 46.8 | 38.2 | 28.8 | 19.8 | 11.4 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| DGAM [[116](#bib.bib116)] | 46.8 | 38.2 | 28.8 | 19.8 | 11.4 |'
- en: '| ECM [[163](#bib.bib163)] | 46.5 | 38.2 | 29.1 | 19.5 | 10.9 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| ECM [[163](#bib.bib163)] | 46.5 | 38.2 | 29.1 | 19.5 | 10.9 |'
- en: '| Deep Metric [[106](#bib.bib106)] | 46.8 | - | 29.6 | - | 9.7 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| Deep Metric [[106](#bib.bib106)] | 46.8 | - | 29.6 | - | 9.7 |'
- en: '| A2CL-PT [[164](#bib.bib164)] | 48.1 | 39.0 | 30.1 | 19.2 | 10.6 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| A2CL-PT [[164](#bib.bib164)] | 48.1 | 39.0 | 30.1 | 19.2 | 10.6 |'
- en: '| EM-MIL [[165](#bib.bib165)] | 45.5 | 36.8 | 30.5 | 22.7 | 16.4 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| EM-MIL [[165](#bib.bib165)] | 45.5 | 36.8 | 30.5 | 22.7 | 16.4 |'
- en: '| Lee et al [[166](#bib.bib166)] | 46.9 | 39.2 | 30.7 | 20.8 | 12.5 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Lee et al [[166](#bib.bib166)] | 46.9 | 39.2 | 30.7 | 20.8 | 12.5 |'
- en: '|  | ASL [[167](#bib.bib167)] | 51.8 | - | 31.1 | - | 11.4 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | ASL [[167](#bib.bib167)] | 51.8 | - | 31.1 | - | 11.4 |'
- en: '|  | Huang et al [[168](#bib.bib168)] | 49.1 | 40.0 | 31.4 | 18.8 | 10.6 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | Huang et al [[168](#bib.bib168)] | 49.1 | 40.0 | 31.4 | 18.8 | 10.6 |'
- en: '|  | Ding et al [[169](#bib.bib169)] | 48.2 | 39.7 | 31.6 | 22.0 | 13.8 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | Ding et al [[169](#bib.bib169)] | 48.2 | 39.7 | 31.6 | 22.0 | 13.8 |'
- en: '|  | CoLA [[170](#bib.bib170)] | 51.5 | 41.9 | 32.2 | 22.0 | 13.1 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | CoLA [[170](#bib.bib170)] | 51.5 | 41.9 | 32.2 | 22.0 | 13.1 |'
- en: '|  | Acsnet [[171](#bib.bib171)] | 51.4 | 42.7 | 32.4 | 22.0 | 11.7 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | Acsnet [[171](#bib.bib171)] | 51.4 | 42.7 | 32.4 | 22.0 | 11.7 |'
- en: '|  | Lee et al. [[172](#bib.bib172)] | 52.3 | 43.4 | 33.7 | 22.9 | 12.1 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | Lee et al. [[172](#bib.bib172)] | 52.3 | 43.4 | 33.7 | 22.9 | 12.1 |'
- en: '|  | ACM-Net [[173](#bib.bib173)] | 55.0 | 44.6 | 34.6 | 21.8 | 10.8 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | ACM-Net [[173](#bib.bib173)] | 55.0 | 44.6 | 34.6 | 21.8 | 10.8 |'
- en: '|  | D2-Net [[174](#bib.bib174)] | 52.3 | 43.4 | 36.0 | - | - |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  | D2-Net [[174](#bib.bib174)] | 52.3 | 43.4 | 36.0 | - | - |'
- en: '| Semi supervised | TTC-Loc [[175](#bib.bib175)] | 52.8 | 44.4 | 35.9 | 24.7
    | 13.8 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 半监督 | TTC-Loc [[175](#bib.bib175)] | 52.8 | 44.4 | 35.9 | 24.7 | 13.8 |'
- en: '| Ji et al [[139](#bib.bib139)] | 53.4 | 45.2 | 37.2 | 29.5 | 20.5 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Ji et al [[139](#bib.bib139)] | 53.4 | 45.2 | 37.2 | 29.5 | 20.5 |'
- en: '| Self supervised | Actionbytes [[109](#bib.bib109)] | 43.0 | 35.8 | 29.0 |
    - | 9.5 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 自监督 | Actionbytes [[109](#bib.bib109)] | 43.0 | 35.8 | 29.0 | - | 9.5 |'
- en: '| Gong et al. [[176](#bib.bib176)] | 50.8 | 42.2 | 32.9 | 21.0 | 10.1 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Gong et al. [[176](#bib.bib176)] | 50.8 | 42.2 | 32.9 | 21.0 | 10.1 |'
- en: '| US | ACL [[129](#bib.bib129)] | 39.6 | 32.9 | 25.0 | 16.7 | 8.9 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| US | ACL [[129](#bib.bib129)] | 39.6 | 32.9 | 25.0 | 16.7 | 8.9 |'
- en: 'TABLE IV: Action detection results of the-state-of-the-art on validation set
    of ActivityNet (V is the version), measured by mAP (%) at tIoU thresholds. $\star$
    indicates utilization of weaker feature extractor (UNet [[104](#bib.bib104)]).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 活动网络验证集上最先进的动作检测结果（V为版本），以 mAP (%) 在 tIoU 阈值下进行测量。$\star$ 表示使用了较弱的特征提取器
    (UNet [[104](#bib.bib104)])。'
- en: '| Supervision | Method | V | 0.5 | 0.75 | 0.95 | Average |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 监督方式 | 方法 | V | 0.5 | 0.75 | 0.95 | 平均值 |'
- en: '| Fully supervised | R-C3D [[30](#bib.bib30)] | 1.3 | 26.8 | - | - | 12.7 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 完全监督 | R-C3D [[30](#bib.bib30)] | 1.3 | 26.8 | - | - | 12.7 |'
- en: '| AFNet [[32](#bib.bib32)] | 1.3 | 36.1 | 17.8 | 5.2 | 18.6 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| AFNet [[32](#bib.bib32)] | 1.3 | 36.1 | 17.8 | 5.2 | 18.6 |'
- en: '| TAL-Net [[13](#bib.bib13)] | 1.3 | 38.23 | 18.30 | 1.30 | 20.22 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| TAL-Net [[13](#bib.bib13)] | 1.3 | 38.23 | 18.30 | 1.30 | 20.22 |'
- en: '| TCN [[146](#bib.bib146)] | 1.3 | 37.49 | 23.47 | 4.47 | 23.58 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| CDC [[14](#bib.bib14)] | 1.3 | 45.3 | 26.0 | 0.2 | 23.8 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| SSN [[18](#bib.bib18)] | 1.3 | 39.12 | 23.48 | 5.49 | 23.98 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| DBS [[15](#bib.bib15)] | 1.3 | 43.2 | 25.8 | 6.1 | 26.1 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| A2Net [[50](#bib.bib50)] | 1.3 | 43.55 | 28.69 | 3.7 | 27.75 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| MLTPN [[44](#bib.bib44)] | 1.3 | 44.86 | 28.96 | 4.30 | 28.27 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| SRG[[148](#bib.bib148)] | 1.3 | 46.53 | 29.98 | 4.83 | 29.72 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| BSN [[46](#bib.bib46)] | 1.3 | 46.45 | 29.96 | 8.02 | 30.03 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| BU [[150](#bib.bib150)] | 1.3 | 43.47 | 33.91 | 9.21 | 30.12 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| AGCN [[31](#bib.bib31)] | 1.3 | - | - | - | 30.4 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| RTD-Net[[66](#bib.bib66)] | 1.3 | 47.21 | 30.68 | 8.61 | 30.83 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| Lianli et al. [[151](#bib.bib151)] | 1.3 | 47.01 | 30.52 | 8.21 | 30.88 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| C-TCN [[43](#bib.bib43)] | 1.3 | 47.6 | 31.9 | 6.2 | 31.1 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| PGCN [[61](#bib.bib61)] | 1.3 | 48.26 | 33.16 | 3.27 | 31.11 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '|  | TadTR [[152](#bib.bib152)] | 1.3 | 49.08 | 32.58 | 8.49 | 32.27 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '|  | SP-TAD [[156](#bib.bib156)] | 1.3 | 50.06 | 32.92 | 8.44 | 32.99 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '|  | BMN [[47](#bib.bib47)] | 1.3 | 50.07 | 34.78 | 8.29 | 33.85 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '|  | Liu et al.[[157](#bib.bib157)] | 1.3 | 50.02 | 34.97 | 6.57 | 33.99 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '|  | G-TAD [[62](#bib.bib62)] | 1.3 | 50.36 | 34.60 | 9.02 | 34.09 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '|  | BC-GNN [[49](#bib.bib49)] | 1.3 | 50.56 | 34.75 | 9.37 | 34.26 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '|  | GTAN [[147](#bib.bib147)] | 1.3 | 52.61 | 34.14 | 8.91 | 34.31 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '|  | AFSD [[51](#bib.bib51)] | 1.3 | 52.4 | 35.3 | 6.5 | 34.4 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '|  | ATAG [[63](#bib.bib63)] | 1.3 | 50.92 | 35.35 | 9.71 | 34.68 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '|  | BSN++ [[149](#bib.bib149)] | 1.3 | 51.27 | 35.70 | 8.33 | 34.88 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '|  | PBRNet [[153](#bib.bib153)] | 1.3 | 53.96 | 34.97 | 8.98 | 35.01 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '|  | VSGN [[64](#bib.bib64)] | 1.3 | 52.38 | 36.01 | 8.37 | 35.07 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '|  | TSP [[154](#bib.bib154)] | 1.3 | 51.26 | 37.12 | 9.29 | 35.81 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| Weakly supervised (V=1.2) | UNet^⋆ [[104](#bib.bib104)] | 1.2 | 7.4 | 3.2
    | 0.7 | 3.6 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| Step-by-step [[124](#bib.bib124)] | 1.2 | 27.3 | 14.7 | 2.9 | 15.6 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| AutoLoc^⋆ [[114](#bib.bib114)] | 1.2 | 27.3 | 15.1 | 3.3 | 16.0 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| TSM [[160](#bib.bib160)] | 1.2 | 28.3 | 17.0 | 3.5 | 17.1 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| Action Graphs [[108](#bib.bib108)] | 1.2 | 29.4 | - | - | - |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| W-TALC [[105](#bib.bib105)] | 1.2 | 37.0 |  |  | 18.0 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| EM-MIL [[165](#bib.bib165)] | 1.2 | 37.4 | - | - | 20.3 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| Cleannet [[115](#bib.bib115)] | 1.2 | 37.1 | 20.3 | 5.0 | 21.6 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| 3C-Net [[102](#bib.bib102)] | 1.2 | 37.2 | - | - | 21.7 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| Deep Metric [[106](#bib.bib106)] | 1.2 | 35.2 | - | - | - |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| CMCS [[120](#bib.bib120)] | 1.2 | 36.8 | 22.0 | 5.6 | 22.4 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| Shen et al [[161](#bib.bib161)] | 1.2 | 36.9 | 23.1 | 3.4 | 22.8 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| RPN [[112](#bib.bib112)] | 1.2 | 37.6 | 23.9 | 5.4 | 23.3 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| TSCN [[162](#bib.bib162)] | 1.2 | 37.6 | 23.7 | 5.7 | 23.6 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| BaSNet [[118](#bib.bib118)] | 1.2 | 38.5 | 24.2 | 5.6 | 24.3 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| DGAM [[116](#bib.bib116)] | 1.2 | 41.0 | 23.5 | 5.3 | 24.4 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| Acsnet [[171](#bib.bib171)] | 1.2 | 41.0 | 23.5 | 5.3 | 24.4 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '|  | ECM [[163](#bib.bib163)] | 1.2 | 41.0 | 24.9 | 6.5 | 25.5 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '|  | ASL [[167](#bib.bib167)] | 1.2 | 40.2 | - | - | 25.8 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | ASL [[167](#bib.bib167)] | 1.2 | 40.2 | - | - | 25.8 |'
- en: '|  | Lee et al. [[172](#bib.bib172)] | 1.2 | 41.2 | 25.6 | 6.0 | 25.9 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | Lee 等 [[172](#bib.bib172)] | 1.2 | 41.2 | 25.6 | 6.0 | 25.9 |'
- en: '|  | D2-Net [[174](#bib.bib174)] | 1.2 | 42.3 | 25.5 | 5.8 | 26.0 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | D2-Net [[174](#bib.bib174)] | 1.2 | 42.3 | 25.5 | 5.8 | 26.0 |'
- en: '|  | CoLA [[170](#bib.bib170)] | 1.2 | 42.7 | 25.7 | 5.8 | 26.1 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | CoLA [[170](#bib.bib170)] | 1.2 | 42.7 | 25.7 | 5.8 | 26.1 |'
- en: '|  | Ding et al [[169](#bib.bib169)] | 1.2 | 41.7 | 26.7 | 6.3 | 26.4 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  | Ding 等 [[169](#bib.bib169)] | 1.2 | 41.7 | 26.7 | 6.3 | 26.4 |'
- en: '| Weakly supervised (V=1.3) | STPN [[117](#bib.bib117)] | 1.3 | 29.3 | 16.9
    | 2.6 | - |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 弱监督 (V=1.3) | STPN [[117](#bib.bib117)] | 1.3 | 29.3 | 16.9 | 2.6 | - |'
- en: '| STAR [[158](#bib.bib158)] | 1.3 | 31.1 | 18.8 | 4.7 | - |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| STAR [[158](#bib.bib158)] | 1.3 | 31.1 | 18.8 | 4.7 | - |'
- en: '| AdapNet [[159](#bib.bib159)] | 1.3 | 33.61 | 18.75 | 3.40 | 21.97 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| AdapNet [[159](#bib.bib159)] | 1.3 | 33.61 | 18.75 | 3.40 | 21.97 |'
- en: '| MAAN [[119](#bib.bib119)] | 1.3 | 33.7 | 21.9 | 5.5 | - |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| MAAN [[119](#bib.bib119)] | 1.3 | 33.7 | 21.9 | 5.5 | - |'
- en: '| BG modeling [[113](#bib.bib113)] | 1.3 | 36.4 | 19.2 | 2.9 | - |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| BG 建模 [[113](#bib.bib113)] | 1.3 | 36.4 | 19.2 | 2.9 | - |'
- en: '| A2CL-PT [[164](#bib.bib164)] | 1.3 | 36.8 | 22.0 | 5.2 | 22.5 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| A2CL-PT [[164](#bib.bib164)] | 1.3 | 36.8 | 22.0 | 5.2 | 22.5 |'
- en: '| Huang et al [[168](#bib.bib168)] | 1.3 | 36.5 | 22.8 | 6.0 | 22.9 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Huang 等 [[168](#bib.bib168)] | 1.3 | 36.5 | 22.8 | 6.0 | 22.9 |'
- en: '|  | ACM-Net [[173](#bib.bib173)] | 1.3 | 40.1 | 24.2 | 6.2 | 24.6 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | ACM-Net [[173](#bib.bib173)] | 1.3 | 40.1 | 24.2 | 6.2 | 24.6 |'
- en: '| Semi | TTC-Loc [[175](#bib.bib175)] | 1.2 | 40.6 | 3.6 | 5.3 | 24.5 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 半监督 | TTC-Loc [[175](#bib.bib175)] | 1.2 | 40.6 | 3.6 | 5.3 | 24.5 |'
- en: '| Self supervised | Actionbytes [[109](#bib.bib109)] | 1.2 | 39.4 | - | - |
    - |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 自监督 | Actionbytes [[109](#bib.bib109)] | 1.2 | 39.4 | - | - | - |'
- en: '| Gong et al. [[176](#bib.bib176)] | 1.2 | 45.5 | 27.3 | 5.4 | 27.6 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| Gong 等 [[176](#bib.bib176)] | 1.2 | 45.5 | 27.3 | 5.4 | 27.6 |'
- en: '| US | ACL [[129](#bib.bib129)] | 1.2 | 35.2 | 21.4 | 3.1 | 21.1 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| US | ACL [[129](#bib.bib129)] | 1.2 | 35.2 | 21.4 | 3.1 | 21.1 |'
- en: 3.3.1 Fully-supervised Methods
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 完全监督方法
- en: Proposal Generation. Anchor-free methods such as SSN [[18](#bib.bib18)], BSN
    [[46](#bib.bib46)], BMN [[47](#bib.bib47)], DBG [[48](#bib.bib48)], BC-GNN [[49](#bib.bib49)],
    BU [[150](#bib.bib150)], and BSN++ [[149](#bib.bib149)], A2Net [[50](#bib.bib50)]
    and AFSD [[51](#bib.bib51)] achieved superior results compared with anchor-based
    methods such as Yeung et al. [[56](#bib.bib56)], SMS [[145](#bib.bib145)], TCN
    [[146](#bib.bib146)], SCNN [[19](#bib.bib19)], TURN TAP [[27](#bib.bib27)], CBR
    [[28](#bib.bib28)], and CDC [[14](#bib.bib14)]. This is because anchor-free methods
    generate temporal action proposals with more flexibility and precise temporal
    boundaries. Some methods such as CTAP[[52](#bib.bib52)], MGG[[40](#bib.bib40)],
    PBRNet [[41](#bib.bib41)], SRG[[148](#bib.bib148)], and RapNet [[42](#bib.bib42)]
    combine advantageous of anchor-based and anchor-free methods and attained a higher
    results.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 提议生成。无锚方法如 SSN [[18](#bib.bib18)]、BSN [[46](#bib.bib46)]、BMN [[47](#bib.bib47)]、DBG
    [[48](#bib.bib48)]、BC-GNN [[49](#bib.bib49)]、BU [[150](#bib.bib150)] 和 BSN++ [[149](#bib.bib149)]、A2Net
    [[50](#bib.bib50)] 和 AFSD [[51](#bib.bib51)] 相较于锚基方法如 Yeung 等 [[56](#bib.bib56)]、SMS
    [[145](#bib.bib145)]、TCN [[146](#bib.bib146)]、SCNN [[19](#bib.bib19)]、TURN TAP
    [[27](#bib.bib27)]、CBR [[28](#bib.bib28)] 和 CDC [[14](#bib.bib14)] 取得了更好的结果。这是因为无锚方法生成的时间动作提议具有更大的灵活性和精确的时间边界。一些方法如
    CTAP [[52](#bib.bib52)]、MGG [[40](#bib.bib40)]、PBRNet [[41](#bib.bib41)]、SRG [[148](#bib.bib148)]
    和 RapNet [[42](#bib.bib42)] 结合了锚基和无锚方法的优点，并取得了更高的结果。
- en: Proposal Feature Extraction. R-C3D [[30](#bib.bib30)] and AFNet [[32](#bib.bib32)]
    employ 3D RoI pooling for feature extraction and obtained low results on ActivityNet
    due to lack of receptive field alignment with proposal span. TAL-Net [[13](#bib.bib13)],
    TSA-Net [[33](#bib.bib33)] employ a multi-tower network and achieve a higher performance
    compared with 3D RoI pooling methods. The methods of SSAD [[35](#bib.bib35)],
    S3D[[36](#bib.bib36)], MGG [[40](#bib.bib40)], PBRNet [[153](#bib.bib153)], MLTPN
    [[44](#bib.bib44)], C-TCN [[43](#bib.bib43)], RapNet [[42](#bib.bib42)], SP-TAD
    [[156](#bib.bib156)], and DaoTAD [[155](#bib.bib155)] employ temporal feature
    pyramid to extract features from actions with different duration and achieved
    superior performance.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 提议特征提取。R-C3D [[30](#bib.bib30)] 和 AFNet [[32](#bib.bib32)] 使用 3D RoI 池化进行特征提取，并由于缺乏与提议跨度的感受野对齐，在
    ActivityNet 上获得了较低的结果。TAL-Net [[13](#bib.bib13)] 和 TSA-Net [[33](#bib.bib33)]
    使用多塔网络，相较于 3D RoI 池化方法取得了更高的性能。SSAD [[35](#bib.bib35)]、S3D [[36](#bib.bib36)]、MGG
    [[40](#bib.bib40)]、PBRNet [[153](#bib.bib153)]、MLTPN [[44](#bib.bib44)]、C-TCN
    [[43](#bib.bib43)]、RapNet [[42](#bib.bib42)]、SP-TAD [[156](#bib.bib156)] 和 DaoTAD
    [[155](#bib.bib155)] 等方法利用时间特征金字塔从不同时长的动作中提取特征，并取得了优越的性能。
- en: Modeling Long-term Dependencies. Sst [[53](#bib.bib53)] and SS-TAD [[54](#bib.bib54)]
    which are RNN-based methods achieve relatively lower results as they can not generate
    flexible proposals. PGCN [[61](#bib.bib61)], G-TAD [[62](#bib.bib62)], BC-GNN
    [[49](#bib.bib49)], AGCN [[31](#bib.bib31)], ATAG [[63](#bib.bib63)], and VSGN
    [[64](#bib.bib64)] are graph models that capture dependencies between proposals
    or video segments. Among them VSGN [[64](#bib.bib64)] achieved the best performance
    by exploiting correlations between cross-scale snippets (original and magnified)
    and aggregating their features with a graph pyramid network. AGT [[65](#bib.bib65)],
    RTD-Net [[66](#bib.bib66)], ATAG [[63](#bib.bib63)], and TadTR [[152](#bib.bib152)]
    use transformers to model long-range dependencies. Among them RTD-Net [[66](#bib.bib66)]
    achieved the best results (on THUMOS14) by customizing the encoder with a boundary-attentive
    architecture to enhance the discrimination capability of action boundary.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 建模长期依赖性。Sst [[53](#bib.bib53)] 和 SS-TAD [[54](#bib.bib54)] 是基于 RNN 的方法，由于无法生成灵活的提议，因此取得了相对较低的结果。PGCN
    [[61](#bib.bib61)], G-TAD [[62](#bib.bib62)], BC-GNN [[49](#bib.bib49)], AGCN
    [[31](#bib.bib31)], ATAG [[63](#bib.bib63)] 和 VSGN [[64](#bib.bib64)] 是图模型，用于捕捉提议或视频片段之间的依赖关系。其中，VSGN
    [[64](#bib.bib64)] 通过利用跨尺度片段（原始和放大的）之间的相关性，并使用图金字塔网络聚合它们的特征，取得了最佳性能。AGT [[65](#bib.bib65)],
    RTD-Net [[66](#bib.bib66)], ATAG [[63](#bib.bib63)] 和 TadTR [[152](#bib.bib152)]
    使用变换器来建模远程依赖性。其中，RTD-Net [[66](#bib.bib66)] 通过定制带有边界注意力架构的编码器来增强动作边界的判别能力，从而在
    THUMOS14 上取得了最佳结果。 |
- en: There are also two state-of-the-art (SOTA) methods that do not belong to the
    mentioned categories of methods. TSP [[154](#bib.bib154)] proposed a novel supervised
    pretraining paradigm for clip features, and improved the performance of SOTA using
    features trained with the proposed pretraining strategy. Liu et al. in [[157](#bib.bib157)]
    leverages temporal aggregation to improve the feature discriminative power of
    each snippet and enhance the feature coherence within a single instance.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两种最先进的（SOTA）方法不属于上述方法类别。TSP [[154](#bib.bib154)] 提出了一个新颖的监督预训练范式用于片段特征，并通过使用提出的预训练策略训练的特征提高了
    SOTA 性能。刘等人 [[157](#bib.bib157)] 利用时间聚合来提高每个片段的特征判别能力，并增强单个实例内的特征一致性。 |
- en: 'TABLE V: Summary of fully-supervised methods for temporal action detection.
    $(+)$ and $(-)$ denote the advantages and disadvantages.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：完全监督方法在时间动作检测中的总结。$(+)$ 和 $(-)$ 表示优点和缺点。
- en: '| Objective | Category | Methods | Advantages and Limitations |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 类别 | 方法 | 优势和局限性 |'
- en: '| Proposal Generation | Anchor-based | SCNN [[19](#bib.bib19)], CBR[[28](#bib.bib28)],
    Turn-Tap[[27](#bib.bib27)], CDC [[14](#bib.bib14)] | + Efficiently generate multiple-scales
    proposals, use global info of all anchors to generate reliable confidence scores.
    |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 提议生成 | 基于锚点 | SCNN [[19](#bib.bib19)], CBR[[28](#bib.bib28)], Turn-Tap[[27](#bib.bib27)],
    CDC [[14](#bib.bib14)] | + 高效生成多尺度提议，利用所有锚点的全局信息生成可靠的置信度评分。 |'
- en: '| - Proposals are not temporally flexible and precise. |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| - 提议在时间上不够灵活和精确。 |'
- en: '| Anchor-free | TAG [[18](#bib.bib18)], BSN [[46](#bib.bib46)], BMN [[47](#bib.bib47)],
    DBG [[48](#bib.bib48)] BC-GNN [[49](#bib.bib49)], BU [[150](#bib.bib150)] A2Net
    [[50](#bib.bib50)], AFSD [[51](#bib.bib51)] BSN++ [[149](#bib.bib149)] | + Generate
    proposals with flexible duration. |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 无锚点 | TAG [[18](#bib.bib18)], BSN [[46](#bib.bib46)], BMN [[47](#bib.bib47)],
    DBG [[48](#bib.bib48)] BC-GNN [[49](#bib.bib49)], BU [[150](#bib.bib150)] A2Net
    [[50](#bib.bib50)], AFSD [[51](#bib.bib51)] BSN++ [[149](#bib.bib149)] | + 生成具有灵活时长的提议。
    |'
- en: '| + Global context for proposal evaluation ( in BMN, DBG). |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| + 提议评估的全局上下文（在 BMN, DBG 中）。 |'
- en: '| + Global context for proposal generation (in DBG). |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| + 提议生成的全局上下文（在 DBG 中）。 |'
- en: '| - Proposal evaluation is not efficient in some cases. |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| - 在某些情况下提议评估效率不高。 |'
- en: '| - Distorting the information of short actions due to down-scaling. |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| - 由于下采样导致短动作的信息失真。 |'
- en: '|  | Anchor-based +Anchor-free | CTAP[[52](#bib.bib52)], MGG[[40](#bib.bib40)]
    PBRNet [[41](#bib.bib41)], RapNet [[42](#bib.bib42)] | + Combining advantageous
    of anchor-based and anchor-free. |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于锚点 + 无锚点 | CTAP[[52](#bib.bib52)], MGG[[40](#bib.bib40)] PBRNet [[41](#bib.bib41)],
    RapNet [[42](#bib.bib42)] | + 结合了基于锚点和无锚点方法的优点。 |'
- en: '|  | - Not modeling long-range dependencies. |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | - 未建模远程依赖性。 |'
- en: '| Proposal Feature Extraction | 3D RoI pooling | R-C3D [[30](#bib.bib30)],
    AFNet [[32](#bib.bib32)] | + Fast feature extraction from multi-scale proposals.
    |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 提议特征提取 | 3D RoI 池化 | R-C3D [[30](#bib.bib30)], AFNet [[32](#bib.bib32)] |
    + 从多尺度提议中快速提取特征。 |'
- en: '| - Proposal features may include insufficient or irrelevant info because of
    receptive field misalignment. |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| - 提议特征可能由于感受野不对齐而包含不足或无关的信息。 |'
- en: '| Multi-tower Network | TAL-Net [[13](#bib.bib13)], TSA-Net [[33](#bib.bib33)]
    | + Alignment of receptive field to proposal span to extract rich features from
    proposals. |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 多塔网络 | TAL-Net [[13](#bib.bib13)]、TSA-Net [[33](#bib.bib33)] | + 对感受野进行对齐以提取提议中的丰富特征。
    |'
- en: '| - Pre-defined temporal intervals limit the accuracy of proposals. |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| - 预定义的时间间隔限制了提议的准确性。 |'
- en: '| TFPN | SSAD [[35](#bib.bib35)], S3D[[36](#bib.bib36)] MGG [[40](#bib.bib40)],
    C-TCN [[43](#bib.bib43)] MLTPN [[44](#bib.bib44)], PBRNet [[41](#bib.bib41)] A2Net
    [[50](#bib.bib50)], AFSD [[51](#bib.bib51)] RapNet [[42](#bib.bib42)], SP-TAD
    [[156](#bib.bib156)] DaoTAD [[155](#bib.bib155)] | + Feature pyramids to detect
    different scales of actions. |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| TFPN | SSAD [[35](#bib.bib35)]、S3D[[36](#bib.bib36)] MGG [[40](#bib.bib40)]、C-TCN
    [[43](#bib.bib43)] MLTPN [[44](#bib.bib44)]、PBRNet [[41](#bib.bib41)] A2Net [[50](#bib.bib50)]、AFSD
    [[51](#bib.bib51)] RapNet [[42](#bib.bib42)]、SP-TAD [[156](#bib.bib156)] DaoTAD
    [[155](#bib.bib155)] | + 特征金字塔用于检测不同尺度的动作。 |'
- en: '| + Re-fine the proposal boundaries from coarse to fine (in MGG, PBRNet, and
    RapNet). |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| + 从粗到细细化提议边界（在MGG，PBRNet和RapNet中）。 |'
- en: '| + Combination with anchor-free pipeline for flexible and precise proposal
    generation (A2Net, AFSD). |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| + 与无锚点管道结合，以实现灵活和精确的提议生成（A2Net，AFSD）。 |'
- en: '| - No modeling of temporal dependencies in most cases. |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| - 大多数情况下未建模时间依赖性。 |'
- en: '| Modeling Long-term Dependencies | RNNs | Sst [[53](#bib.bib53)], SS-TAD [[54](#bib.bib54)]
    | + Modeling long-term dependencies for proposal generation. |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 建模长期依赖性 | RNNs | Sst [[53](#bib.bib53)]、SS-TAD [[54](#bib.bib54)] | + 对提议生成建模长期依赖性。
    |'
- en: '| - Proposals are not flexible and precise. |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| - 提议不够灵活和精确。 |'
- en: '| Graphs | PGCN [[61](#bib.bib61)], G-TAD [[62](#bib.bib62)], BC-GNN [[49](#bib.bib49)],
    AGCN [[31](#bib.bib31)] ATAG [[63](#bib.bib63)] , VSGN [[64](#bib.bib64)] | +
    Modeling temporal dependencies between proposals or video segments for proposal
    generation and refinement. |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 图 | PGCN [[61](#bib.bib61)]、G-TAD [[62](#bib.bib62)]、BC-GNN [[49](#bib.bib49)]、AGCN
    [[31](#bib.bib31)] ATAG [[63](#bib.bib63)]、VSGN [[64](#bib.bib64)] | + 建模提议或视频片段之间的时间依赖性，以进行提议生成和细化。
    |'
- en: '| - Proposal generation is inefficient or temporal dependencies are used only
    for proposal refinement. |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| - 提议生成效率低下或时间依赖性仅用于提议的细化。 |'
- en: '| Transformer | AGT [[65](#bib.bib65)], RTD-Net [[66](#bib.bib66)] ATAG [[63](#bib.bib63)],
    TadTR [[152](#bib.bib152)] | + Modeling non-linear temporal structure and inter-proposal
    relationships for proposal generation. |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | AGT [[65](#bib.bib65)]、RTD-Net [[66](#bib.bib66)] ATAG [[63](#bib.bib63)]、TadTR
    [[152](#bib.bib152)] | + 对提议生成建模非线性时间结构和提议之间的关系。 |'
- en: '| - High parametric complexity. |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| - 高参数复杂度。 |'
- en: 3.3.2 Methods with Limited Supervision
  id: totrans-437
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 有限监督的方法
- en: Action Localization with Class-specific Attention. UNet [[104](#bib.bib104)]
    is supervised with MIL loss which is not strong enough to predict accurate attention
    scores. The methods of W-TALC [[105](#bib.bib105)], Action Graphs [[108](#bib.bib108)],
    and Deep Metric [[106](#bib.bib106)] all target action-action separation by employing
    a co-activity similarity loss. 3C-Net [[102](#bib.bib102)] applied center loss
    on video-level aggregated features to enhance feature discriminability. Deep Metric
    [[106](#bib.bib106)] outperforms W-TALC [[105](#bib.bib105)], Action Graphs [[108](#bib.bib108)],
    and 3C-Net [[102](#bib.bib102)] by defining a class-specific metric for each action
    category.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 类别特定注意力的动作定位。UNet [[104](#bib.bib104)] 使用MIL损失进行监督，但其预测准确注意力分数的能力不足。W-TALC [[105](#bib.bib105)]、动作图
    [[108](#bib.bib108)] 和深度度量 [[106](#bib.bib106)] 等方法通过使用共同活动相似性损失来实现动作之间的分离。3C-Net
    [[102](#bib.bib102)] 在视频级聚合特征上应用了中心损失，以增强特征的可区分性。深度度量 [[106](#bib.bib106)] 通过为每个动作类别定义类别特定的度量，超越了W-TALC
    [[105](#bib.bib105)]、动作图 [[108](#bib.bib108)] 和3C-Net [[102](#bib.bib102)]。
- en: Action Localization with Class-agnostic Attention. STPN [[117](#bib.bib117)]
    proposed to learn attention through class-agnostic features but has a low performance
    as cross entropy loss alone does not train accurate attention signals. BG modeling
    [[113](#bib.bib113)] used a clustering loss to separate action from background.
    BG modeling [[113](#bib.bib113)] and BaSNet [[118](#bib.bib118)] force all background
    frames to belong to one specific class which is not desirable as they do not share
    any common semantics. RPN [[112](#bib.bib112)], and Huang et al. [[168](#bib.bib168)]
    increase inter-class separateness by pushing action (or sub-action) features to
    their prototypes. Huang et al. [[168](#bib.bib168)] outperforms RPN [[112](#bib.bib112)]
    by modeling the relations between sub-actions of each action. DGAM [[116](#bib.bib116)]
    addressed the action-context confusion through imposing different attentions on
    different features with a generative model. EM-MIL [[165](#bib.bib165)] employed
    Expectation-Maximization to capture complete action instances and outperformed
    DGAM [[116](#bib.bib116)] on THUMOS14 dataset.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 无类别注意力的行动定位。STPN [[117](#bib.bib117)] 提出了通过无类别特征学习注意力，但由于交叉熵损失无法单独训练准确的注意力信号，因此性能较低。BG
    建模 [[113](#bib.bib113)] 使用聚类损失将动作与背景分离。BG 建模 [[113](#bib.bib113)] 和 BaSNet [[118](#bib.bib118)]
    强制所有背景帧属于一个特定的类别，这是不理想的，因为它们没有共享任何共同的语义。RPN [[112](#bib.bib112)] 和 Huang 等人 [[168](#bib.bib168)]
    通过将动作（或子动作）特征推向其原型来增加类间分离度。Huang 等人 [[168](#bib.bib168)] 通过建模每个动作的子动作之间的关系，超越了
    RPN [[112](#bib.bib112)]。DGAM [[116](#bib.bib116)] 通过在不同特征上施加不同的注意力来解决行动上下文混淆问题，使用生成模型。EM-MIL
    [[165](#bib.bib165)] 采用期望最大化来捕获完整的动作实例，并在 THUMOS14 数据集上优于 DGAM [[116](#bib.bib116)]。
- en: Direct Action Localization. AutoLoc [[114](#bib.bib114)], and CleanNet [[115](#bib.bib115)]
    regress the intervals of action instances for proposal generation, instead of
    performing hard thresholding. They obtained a lower performance compared with
    most recent methods as they do not model action completeness or address action-context
    confusion.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 直接行动定位。AutoLoc [[114](#bib.bib114)] 和 CleanNet [[115](#bib.bib115)] 回归动作实例的时间间隔以生成提案，而不是进行硬阈值处理。由于没有建模行动完整性或解决行动上下文混淆，它们的性能低于大多数最近的方法。
- en: Action Completeness Modeling. The methods of CMCS [[120](#bib.bib120)], Hide-and-Seek
    [[123](#bib.bib123)], and Step-by-step [[124](#bib.bib124)] target the action
    completeness and CMCS [[120](#bib.bib120)] achieves a superior performance. This
    is because Hide-and-Seek [[123](#bib.bib123)] and Step-by-step [[124](#bib.bib124)]
    do not guarantee the discovery of new parts by randomly hiding or removing different
    video regions. In contrary, CMCS [[120](#bib.bib120)] employs a diversity loss
    to enforce the model to discover complementary action parts.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 行动完整性建模。CMCS [[120](#bib.bib120)]、Hide-and-Seek [[123](#bib.bib123)] 和 Step-by-step
    [[124](#bib.bib124)] 方法针对行动的完整性，其中 CMCS [[120](#bib.bib120)] 实现了更优的性能。这是因为 Hide-and-Seek
    [[123](#bib.bib123)] 和 Step-by-step [[124](#bib.bib124)] 通过随机隐藏或移除不同的视频区域，无法保证发现新部分。相反，CMCS
    [[120](#bib.bib120)] 通过使用多样性损失来强制模型发现互补的行动部分。
- en: ACL [[129](#bib.bib129)] is an unsupervised method and only uses the total count
    of unique actions that appear in the video, but it still achieves a comparable
    performance with respect to some of the weakly-supervised methods such as 3C-Net
    [[102](#bib.bib102)]. Gong et al. [[176](#bib.bib176)] is a self-supervised method
    that attained the state-of-the-art results on ActivityNet-1.2 among methods with
    limited supervision, confirming the advantageous of self-supervised learning.
    The recent state-of-the-art weakly supervised methods such as D2-Net [[174](#bib.bib174)]
    achieved comparable performance to the semi-supervised methods of Ji et al [[139](#bib.bib139)]
    and TTC-Loc [[175](#bib.bib175)]. This is interesting specially because D2-Net
    [[174](#bib.bib174)] does not use temporal annotation of actions at all while
    Ji et al [[139](#bib.bib139)] and TTC-Loc [[175](#bib.bib175)] use temporal annotations
    at least for a small percentage of videos in the dataset.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ACL [[129](#bib.bib129)] 是一种无监督方法，仅使用视频中出现的唯一动作的总计数，但在某些弱监督方法如 3C-Net [[102](#bib.bib102)]
    中，仍然能达到相当的性能。Gong 等人 [[176](#bib.bib176)] 是一种自监督方法，在有限监督的方法中在 ActivityNet-1.2
    数据集上取得了最先进的结果，证实了自监督学习的优势。近期最先进的弱监督方法如 D2-Net [[174](#bib.bib174)] 的性能与 Ji 等人
    [[139](#bib.bib139)] 和 TTC-Loc [[175](#bib.bib175)] 的半监督方法相当。这一点尤其有趣，因为 D2-Net
    [[174](#bib.bib174)] 完全不使用动作的时间标注，而 Ji 等人 [[139](#bib.bib139)] 和 TTC-Loc [[175](#bib.bib175)]
    至少在数据集中使用了小部分视频的时间标注。
- en: 'TABLE VI: Summary of methods with limited supervision for temporal action detection.
    $(+)$ and $(-)$ denote the advantages and disadvantages.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：有限监督的时间动作检测方法总结。$(+)$和$(-)$表示优点和缺点。
- en: '| Objective | Category | Method | Advantages and Limitations |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 类别 | 方法 | 优势和局限性 |'
- en: '| Localization with Class-specific Attention | MIL Loss | UNet [[104](#bib.bib104)],
    W-TALC [[105](#bib.bib105)] Action Graphs [[108](#bib.bib108)], BaSNet [[118](#bib.bib118)]
    3C-Net [[102](#bib.bib102)], Actionbytes [[109](#bib.bib109)] | + Learns temporal
    class activation maps. |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 使用类特定注意力进行定位 | MIL 损失 | UNet [[104](#bib.bib104)]，W-TALC [[105](#bib.bib105)]
    动作图 [[108](#bib.bib108)]，BaSNet [[118](#bib.bib118)] 3C-Net [[102](#bib.bib102)]，Actionbytes
    [[109](#bib.bib109)] | + 学习时间类激活图。 |'
- en: '| - MIL loss alone does not predict accurate attention scores. |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| - 仅使用 MIL 损失无法预测准确的注意力分数。 |'
- en: '| - Only supervising temporal positions with highest activation scores. |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| - 仅监督激活分数最高的时间位置。 |'
- en: '| Co-activity Similarity Loss (CASL) | W-TALC[[105](#bib.bib105)], Action Graphs
    [[108](#bib.bib108)] DM[[106](#bib.bib106)], Actionbytes [[109](#bib.bib109)]
    | + Action-background separation and reducing intra-class variations. |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 共活动相似性损失 (CASL) | W-TALC[[105](#bib.bib105)]，动作图 [[108](#bib.bib108)] DM[[106](#bib.bib106)]，Actionbytes
    [[109](#bib.bib109)] | + 动作背景分离和减少类内变化。 |'
- en: '| - Action-context confusion is not addressed. |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| - 未解决动作上下文混淆问题。 |'
- en: '| - Not modeling action completeness. |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| - 未建模动作的完整性。 |'
- en: '| Center Loss | 3C-Net [[102](#bib.bib102)] | + Reduce intra-class variations
    by pushing action features to class centers. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 中心损失 | 3C-Net [[102](#bib.bib102)] | + 通过将动作特征推向类中心来减少类内变化。 |'
- en: '|  | - Imprecise attention signal, supervises video-level aggregated features.
    |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  | - 不准确的注意信号，监督视频级聚合特征。 |'
- en: '| Localization with Class-agnostic Attention | CE Loss | STPN [[117](#bib.bib117)],
    RPN [[112](#bib.bib112)] BG modeling [[113](#bib.bib113)] | + Learns attention
    through class-agnostic features. |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 使用类无关注意力进行定位 | CE 损失 | STPN [[117](#bib.bib117)]，RPN [[112](#bib.bib112)]
    背景建模 [[113](#bib.bib113)] | + 通过类无关特征学习注意力。 |'
- en: '| - CE loss alone does not train accurate attention signals. |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| - 仅使用 CE 损失无法训练准确的注意力信号。 |'
- en: '| Clustering Loss | RPN [[112](#bib.bib112)] BG modeling [[113](#bib.bib113)]
    | + Separating foreground-background features. |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 聚类损失 | RPN [[112](#bib.bib112)] 背景建模 [[113](#bib.bib113)] | + 分离前景背景特征。 |'
- en: '| - Force all background frames to belong to one specific class, but they do
    not share any common semantics. |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| - 强制所有背景帧属于一个特定类别，但它们没有共享任何共同语义。 |'
- en: '| Prototype Learning | RPN [[112](#bib.bib112)] Huang et al. [[168](#bib.bib168)]
    | + Inter-class separateness by pushing action (or sub-action) features to their
    prototypes. |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 原型学习 | RPN [[112](#bib.bib112)] Huang 等 [[168](#bib.bib168)] | + 通过将动作（或子动作）特征推向其原型来实现类间分离。
    |'
- en: '| - Action-context confusion is not addressed. |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| - 未解决动作上下文混淆问题。 |'
- en: '|  | Generative Model | DGAM [[116](#bib.bib116)] EM-MIL [[165](#bib.bib165)]
    | + Conditional VAE / Expectation-Maximization to separate actions from context
    frames and capture complete action instances. |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '|  | 生成模型 | DGAM [[116](#bib.bib116)] EM-MIL [[165](#bib.bib165)] | + 条件变分自编码器/期望最大化用于将动作从上下文帧中分离，并捕捉完整的动作实例。
    |'
- en: '|  | - Not modeling temporal dependencies and relation between sub-actions.
    |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  | - 未建模时间依赖性和子动作之间的关系。'
- en: '| Direct Localization | Action-boundary Contrast | AutoLoc [[114](#bib.bib114)]
    CleanNet [[115](#bib.bib115)] | + Regress the intervals of action instances for
    proposal generation, instead of performing hard thresholding. |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 直接定位 | 动作边界对比 | AutoLoc [[114](#bib.bib114)] CleanNet [[115](#bib.bib115)]
    | + 回归动作实例的间隔以生成提议，而不是执行硬阈值处理。 |'
- en: '| - Not modeling action completeness. |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| - 未建模动作的完整性。 |'
- en: '| Action Completeness Modeling | Masking | Hide-and-seek [[123](#bib.bib123)]
    Step-by-step [[124](#bib.bib124)] | + Randomly hiding or removing different video
    regions to see different action parts. |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| 动作完整性建模 | 掩模 | 捉迷藏 [[123](#bib.bib123)] 逐步 [[124](#bib.bib124)] | + 随机隐藏或移除不同的视频区域以观察不同的动作部分。
    |'
- en: '| - Does not guarantee the discovery of new parts. |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| - 不保证发现新部分。 |'
- en: '| Diversity Loss | CMCS [[120](#bib.bib120)] | + Enforcing the model to discover
    complementary pieces of an action. |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 多样性损失 | CMCS [[120](#bib.bib120)] | + 强制模型发现动作的补充部分。 |'
- en: '|  | - Not modeling the relation between sub-actions. |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '|  | - 未建模子动作之间的关系。 |'
- en: 4 Discussions
  id: totrans-467
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: In this section, we describe the application of temporal action detection in
    real-world applications and introduce several directions for future work in this
    domain.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们描述了时序动作检测在现实世界应用中的应用，并介绍了该领域未来工作的几个方向。
- en: 4.1 Applications
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 应用
- en: Temporal action detection has numerous real-world applications as most of the
    videos in practice are untrimmed with a sparse set of actions. In this section,
    we describe several applications such as understanding instructional videos, anomaly
    detection in surveillance videos, action spotting in sports, and detection in
    self-driving cars.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 时序动作检测在现实世界中有众多应用，因为大多数实际视频都是未经剪辑的，且动作稀疏。在这一部分，我们描述了几个应用场景，如理解指导视频、监控视频中的异常检测、体育中的动作识别以及自动驾驶汽车中的检测。
- en: 4.1.1 Action Localization in Instructional videos
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 指导视频中的动作定位
- en: With the rising popularity of social media and video sharing sites such as YouTube,
    people worldwide upload numerous instructional videos in diverse categories. Millions
    of people watch these tutorials to learn new tasks such as ”making pancakes” or
    ”changing a flat tire.” Analysis of the instructional videos has drawn more attention
    in recent years, leading to the proposition of several tasks including step localization
    and action segmentation [[177](#bib.bib177)]. Based on the psychological studies,
    it has been shown that simplifying and segmenting the video into smaller steps
    (sub-actions) is a more effective way to learn a new task [[144](#bib.bib144),
    [178](#bib.bib178)]. For example, the task of ”making pancakes” can be segmented
    to action steps such as ”add the eggs,” ”pour the mixture into the pan,” ”heat
    a frying pan,” and such. Many datasets are designed to study action localization
    and action anticipation such as EPIC-Kitchen [[179](#bib.bib179)] and INRIA Instructional
    Videos Dataset [[180](#bib.bib180)]. Both of these tasks (step localization and
    action segmentation) are directly related to action detection. Step localization
    is the task of localizing the start and endpoints of a series of steps and recognizing
    their labels while action segmentation is the frame-level labeling.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 随着社交媒体和视频分享网站如 YouTube 的日益普及，全球范围内的人们上传了大量不同类别的指导视频。数以百万计的人观看这些教程，以学习新任务，如“做煎饼”或“更换轮胎”。对指导视频的分析近年来引起了更多关注，提出了多个任务，包括步骤定位和动作分割[[177](#bib.bib177)]。根据心理学研究，简化并将视频分割成较小的步骤（子动作）是一种更有效的学习新任务的方法[[144](#bib.bib144),
    [178](#bib.bib178)]。例如，“做煎饼”这一任务可以分解为诸如“加入鸡蛋”、“将混合物倒入锅中”、“加热平底锅”等动作步骤。许多数据集被设计用于研究动作定位和动作预测，如
    EPIC-Kitchen [[179](#bib.bib179)] 和 INRIA Instructional Videos Dataset [[180](#bib.bib180)]。这两个任务（步骤定位和动作分割）与动作检测直接相关。步骤定位是定位一系列步骤的起始和结束点并识别其标签的任务，而动作分割是帧级标注。
- en: 4.1.2 Anomaly Detection in Surveillance Videos
  id: totrans-473
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 监控视频中的异常检测
- en: Surveillance cameras are increasingly deployed in public places, monitoring
    the areas of interest to ensure security. With the stream of data from these video
    cameras, there has been a rise in video analysis and anomaly detection research.
    Anomalies are significant deviations of scene entities from normal behavior [[181](#bib.bib181),
    [182](#bib.bib182)]. Fighting, traffic accidents, burglary, and robbery are a
    few examples of anomalies. Compared to normal activities, anomalous events rarely
    occur. Therefore, intelligent computer vision algorithms are required to detect
    anomalous events automatically, to avoid the waste of time and labor. In some
    methods, anomaly detection models are trained with normal behaviors to learn distributions
    of normal patterns. These models identify anomalous activities based on dissimilarity
    to the standard data distributions [[183](#bib.bib183), [184](#bib.bib184)]. In
    other cases, normal and anomalous videos are used during training to automatically
    predict high anomaly scores [[185](#bib.bib185), [186](#bib.bib186)]. In many
    real-time applications, the system must detect anomalous events as soon as each
    video frame arrives, only based on history and the current data; for instance,
    an intelligent video surveillance application designed to raise the alarm when
    suspicious activity is detected. To this end, online action detection algorithms
    are developed to accumulate historical observations and predicted future information
    to analyze current events [[187](#bib.bib187)], [[188](#bib.bib188)], [[189](#bib.bib189)],
    [[190](#bib.bib190)],[[191](#bib.bib191)].
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 监控摄像头在公共场所的部署越来越多，监控关注区域以确保安全。随着这些视频摄像头数据流的增加，视频分析和异常检测研究也有所上升。异常是场景实体偏离正常行为的显著偏差[[181](#bib.bib181),
    [182](#bib.bib182)]。打架、交通事故、盗窃和抢劫是一些异常的例子。与正常活动相比，异常事件发生的频率较低。因此，需要智能计算机视觉算法来自动检测异常事件，以避免浪费时间和劳动。在一些方法中，异常检测模型通过正常行为进行训练，以学习正常模式的分布。这些模型根据与标准数据分布的差异性来识别异常活动[[183](#bib.bib183),
    [184](#bib.bib184)]。在其他情况下，训练过程中使用正常和异常视频来自动预测高异常分数[[185](#bib.bib185), [186](#bib.bib186)]。在许多实时应用中，系统必须在每个视频帧到达时立即检测异常事件，仅根据历史和当前数据，例如，当检测到可疑活动时，智能视频监控应用会发出警报。为此，开发了在线动作检测算法，以积累历史观察和预测未来信息来分析当前事件[[187](#bib.bib187)],
    [[188](#bib.bib188)], [[189](#bib.bib189)], [[190](#bib.bib190)], [[191](#bib.bib191)]。
- en: 4.1.3 Action Spotting in Sports
  id: totrans-475
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 体育中的动作识别
- en: Professional analysts utilize sports videos to investigate the strategies in
    a game, examine new players, and generate meaningful statistics. In order to analyze
    the videos, they watch many broadcasts to spot the highlights within a game, which
    is a time-consuming and costly process. Fortunately, automated sports analytic
    methods developed in the computer vision field can facilitate sports broadcasts
    understanding. In recent years, many automated methods have been proposed to help
    localize the salient actions of a game. They produce statistics of events within
    a game by either analyzing camera shots or semantic information. Human activity
    localization in sports videos is studied in [[192](#bib.bib192), [193](#bib.bib193),
    [194](#bib.bib194), [195](#bib.bib195)], salient game actions are identified in
    [[196](#bib.bib196), [197](#bib.bib197)], automatic game highlights identification
    and summarization are performed in [[198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200),
    [201](#bib.bib201), [202](#bib.bib202)]. Moreover, action spotting, which is the
    task of temporal localization of human-induced events, has been popular in soccer
    game broadcasts [[3](#bib.bib3), [203](#bib.bib203)] and some methods aimed to
    automatically detect goals, penalties, corner kicks, and card events [[204](#bib.bib204)].
    Action detection algorithms can inspire many of the tasks mentioned above.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 专业分析师利用体育视频研究比赛策略，审查新球员，并生成有意义的统计数据。为了分析这些视频，他们观看许多广播以发现比赛中的亮点，这是一个耗时且成本高昂的过程。幸运的是，计算机视觉领域开发的自动化体育分析方法可以促进体育广播的理解。近年来，已经提出了许多自动化方法来帮助定位比赛中的突出动作。这些方法通过分析镜头或语义信息来生成比赛事件的统计数据。体育视频中的人类活动定位在[[192](#bib.bib192)、[193](#bib.bib193)、[194](#bib.bib194)、[195](#bib.bib195)]中进行了研究，突出游戏动作的识别在[[196](#bib.bib196)、[197](#bib.bib197)]中进行了，自动游戏亮点的识别和总结在[[198](#bib.bib198)、[199](#bib.bib199)、[200](#bib.bib200)、[201](#bib.bib201)、[202](#bib.bib202)]中进行了。此外，动作识别，即人类引发事件的时间定位任务，在足球比赛广播中非常受欢迎[[3](#bib.bib3)、[203](#bib.bib203)]，一些方法旨在自动检测进球、罚球、角球和黄牌事件[[204](#bib.bib204)]。动作检测算法可以激发上述许多任务。
- en: 4.1.4 Action Detection in Autonomous Driving
  id: totrans-477
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 自动驾驶中的动作检测
- en: With the rapid development and advancement of cars and other vehicles in urban
    transportation, autonomous driving has attracted more attention in the last decades.
    The cameras assembled on the self-driving cars capture the real-time stream of
    videos that need to be processed with online algorithms. The car should be aware
    of the surrounding environment and spot road users, including pedestrians, cyclists,
    and other vehicles, to make safe autonomous decisions. Also, it should be able
    to detect and anticipate road users activities such as moving away, moving towards,
    crossing the road, and anomalous events in real-time to adjust the speed and handle
    the situation. Therefore, spatio-temporal action localization algorithms need
    to be developed to guarantee the safety of self-driving cars [[205](#bib.bib205)].
    Yao et al. [[206](#bib.bib206)] proposed a traffic anomaly detection with a when-where-what
    pipeline to detect, localize, and recognize anomalous events from egocentric videos.
    To improve the detection and prediction of pedestrian movements, Rasouli et al.
    [[4](#bib.bib4)] studied pedestrian behavior depending on various factors, such
    as demographics of the pedestrians, traffic dynamics, and environmental conditions.
    Moreover, Mahadevan et al. [[207](#bib.bib207)] proposed an immersive VR-based
    pedestrian mixed traffic simulator to examine pedestrian behavior in street crossing
    tasks.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 随着城市交通中汽车及其他车辆的快速发展和进步，自动驾驶在过去几十年中吸引了更多关注。装配在自动驾驶汽车上的摄像头捕捉实时视频流，这些视频流需要通过在线算法进行处理。汽车应能感知周围环境并识别道路使用者，包括行人、骑自行车的人和其他车辆，以做出安全的自动决策。此外，它还应能实时检测和预测道路使用者的活动，如远离、靠近、过马路以及异常事件，以调整速度和处理情况。因此，需要开发时空行为定位算法，以确保自动驾驶汽车的安全[[205](#bib.bib205)]。Yao等人[[206](#bib.bib206)]提出了一种基于何时何地何物的交通异常检测管道，从自我中心的视频中检测、定位和识别异常事件。为了改善行人运动的检测和预测，Rasouli等人[[4](#bib.bib4)]研究了依赖于各种因素的行人行为，例如行人的人口统计信息、交通动态和环境条件。此外，Mahadevan等人[[207](#bib.bib207)]提出了一种沉浸式虚拟现实行人混合交通模拟器，以考察街道过马路任务中的行人行为。
- en: 4.2 Future work
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 未来工作
- en: Weakly-supervised action localization in untrimmed videos has drawn much research
    attention by providing only video-level labels during training instead of exhaustive
    annotation of temporal boundaries in the training phase. Subsequently, knowledge
    transfer from publicly available trimmed videos is a promising trend to make up
    for the coarse-grained video-level annotations in weakly-supervised settings.
    Nevertheless, domain-adaptation schemes must fulfill the domain gap between trimmed
    and untrimmed videos to transfer robust and reliable knowledge. Only a few methods
    have explored knowledge transfer from trimmed videos [[109](#bib.bib109)], [[159](#bib.bib159)],
    [[208](#bib.bib208)], [[209](#bib.bib209)], but we expect to see more in the future.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在未经剪辑的视频中，弱监督的动作定位因仅提供视频级标签而非训练阶段的详尽时间边界注释而引起了大量研究关注。随后，从公开的剪辑视频中转移知识是一种有前景的趋势，以弥补弱监督设置中的粗粒度视频级注释。然而，领域适应方案必须弥合剪辑和未剪辑视频之间的领域差距，以传递可靠且稳健的知识。仅有少数方法探讨了从剪辑视频中转移知识[[109](#bib.bib109)]，[[159](#bib.bib159)]，[[208](#bib.bib208)]，[[209](#bib.bib209)]，但我们期望未来能看到更多。
- en: In recent years, zero-shot learning (ZSL) in the visual recognition domain has
    been emerging as a rising trend as it is challenging to collect a large number
    of samples for each class during training. ZSL works by transferring the knowledge
    from the seen classes with sufficiently many instances to generalize the models
    on unseen classes with no samples during training. The task of zero-shot temporal
    activity detection (ZSTAD) is introduced in [[210](#bib.bib210)] to generalize
    the applicability of action detection methods to newly emerging or rare events
    that are not included in the training set. The task of ZSTAD is highly challenging
    because each untrimmed video in the testing set possibly contains multiple novel
    action classes that must be localized and detected. It is worth mentioning that
    activity detection with few-shot learning has been recently explored in [[109](#bib.bib109)],
    [[211](#bib.bib211)], [[212](#bib.bib212)], [[213](#bib.bib213)], [[214](#bib.bib214)],
    [[215](#bib.bib215)]. The advancement of both zero-shot and few-shot action detection
    is anticipated in the near future.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，视觉识别领域的零样本学习（ZSL）作为一种新兴趋势逐渐受到关注，因为在训练期间收集每个类别的大量样本具有挑战性。ZSL通过将来自足够多实例的已见类别的知识转移到训练期间没有样本的未见类别上，从而使模型能够进行泛化。零样本时间活动检测（ZSTAD）任务在[[210](#bib.bib210)]中被提出，以将动作检测方法的适用性推广到训练集中未包含的新兴或稀有事件。ZSTAD任务非常具有挑战性，因为测试集中每个未剪辑视频可能包含多个需要定位和检测的新颖动作类别。值得一提的是，近期在[[109](#bib.bib109)]，[[211](#bib.bib211)]，[[212](#bib.bib212)]，[[213](#bib.bib213)]，[[214](#bib.bib214)]，[[215](#bib.bib215)]中也探索了少样本学习的活动检测。预计零样本和少样本动作检测的进展将在不久的将来实现。
- en: 5 Conclusion
  id: totrans-482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: Action detection schemes have expedited the progress in many real-world applications
    such as instructional video analysis, anomaly detection in surveillance videos,
    sports analysis, and autonomous driving. The advancement of learning methods with
    limited supervision has facilitated action detection by detachment from costly
    need to annotate the temporal boundary of actions in long videos. This survey
    has extensively studied recently developed deep learning-based methods for action
    detection from different aspects including fully-supervised schemes, methods with
    limited supervision, benchmark datasets, performance analysis, applications, and
    future directions. The performance analysis and future directions are summarized
    to inspire the design of new and efficient methods for action detection that serves
    the computer vision community.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 动作检测方案加快了许多现实应用的进展，如教学视频分析、监控视频中的异常检测、体育分析和自动驾驶。有限监督的学习方法的进步通过减少对长视频中动作时间边界注释的高成本需求，促进了动作检测。本调查从不同方面广泛研究了最近开发的基于深度学习的动作检测方法，包括全监督方案、有限监督方法、基准数据集、性能分析、应用和未来方向。性能分析和未来方向的总结旨在激励新的高效动作检测方法的设计，以服务于计算机视觉社区。
- en: References
  id: totrans-484
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev, M. Shah, and
    R. Sukthankar, “Thumos challenge: Action recognition with a large number of classes,”
    2014.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev, M. Shah, 和 R.
    Sukthankar，“Thumos挑战赛：具有大量类别的动作识别，” 2014。'
- en: '[2] F. Rea, A. Vignolo, A. Sciutti, and N. Noceti, “Human motion understanding
    for selecting action timing in collaborative human-robot interaction,” *Front.
    Robot. AI*, vol. 6, p. 58, 2019.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] F. Rea, A. Vignolo, A. Sciutti 和 N. Noceti，“用于选择协作人机交互中动作时机的人类运动理解”，*Front.
    Robot. AI*，第6卷，第58页，2019年。'
- en: '[3] A. Cioppa, A. Deliege, S. Giancola, B. Ghanem, M. V. Droogenbroeck, R. Gade,
    and T. B. Moeslund, “A context-aware loss function for action spotting in soccer
    videos,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, 2020, pp. 13 126–13 136.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Cioppa, A. Deliege, S. Giancola, B. Ghanem, M. V. Droogenbroeck, R.
    Gade 和 T. B. Moeslund，“用于足球视频动作检测的上下文感知损失函数”，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，pp.
    13 126–13 136。'
- en: '[4] A. Rasouli and J. K. Tsotsos, “Autonomous vehicles that interact with pedestrians:
    A survey of theory and practice,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 21, no. 3, pp. 900–918, 2019.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Rasouli 和 J. K. Tsotsos，“与行人互动的自动驾驶车辆：理论与实践综述”，*IEEE智能交通系统汇刊*，第21卷，第3期，pp.
    900–918，2019年。'
- en: '[5] S. Herath, M. Harandi, and F. Porikli, “Going deeper into action recognition:
    A survey,” *Image and vision computing*, vol. 60, pp. 4–21, 2017.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Herath, M. Harandi 和 F. Porikli，“深入探讨动作识别：综述”，*图像与视觉计算*，第60卷，pp. 4–21，2017年。'
- en: '[6] C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast networks for video
    recognition,” in *Proceedings of the IEEE international conference on computer
    vision*, 2019, pp. 6202–6211.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. Feichtenhofer, H. Fan, J. Malik 和 K. He，“用于视频识别的慢快网络”，发表于*IEEE国际计算机视觉会议论文集*，2019年，pp.
    6202–6211。'
- en: '[7] D. Ghadiyaram, D. Tran, and D. Mahajan, “Large-scale weakly-supervised
    pre-training for video action recognition,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 12 046–12 055.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. Ghadiyaram, D. Tran 和 D. Mahajan，“大规模弱监督预训练用于视频动作识别”，发表于*IEEE计算机视觉与模式识别会议论文集*，2019年，pp.
    12 046–12 055。'
- en: '[8] H. Duan, Y. Zhao, Y. Xiong, W. Liu, and D. Lin, “Omni-sourced webly-supervised
    learning for video recognition,” *arXiv preprint arXiv:2003.13042*, 2020.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Duan, Y. Zhao, Y. Xiong, W. Liu 和 D. Lin，“用于视频识别的全源网络弱监督学习”，*arXiv预印本arXiv:2003.13042*，2020年。'
- en: '[9] H. Idrees, A. R. Zamir, Y.-G. Jiang, A. Gorban, I. Laptev, R. Sukthankar,
    and M. Shah, “The thumos challenge on action recognition for videos “in the wild”,”
    *Computer Vision and Image Understanding*, vol. 155, pp. 1–23, 2017.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Idrees, A. R. Zamir, Y.-G. Jiang, A. Gorban, I. Laptev, R. Sukthankar
    和 M. Shah，“Thumos挑战：对“野外”视频的动作识别”，*计算机视觉与图像理解*，第155卷，pp. 1–23，2017年。'
- en: '[10] A. Gorban, H. Idrees, Y.-G. Jiang, A. R. Zamir, I. Laptev, M. Shah, and
    R. Sukthankar, “Thumos challenge: Action recognition with a large number of classes,”
    2015.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Gorban, H. Idrees, Y.-G. Jiang, A. R. Zamir, I. Laptev, M. Shah 和 R.
    Sukthankar，“Thumos挑战：具有大量类别的动作识别”，2015年。'
- en: '[11] H. Kuehne, A. Arslan, and T. Serre, “The language of actions: Recovering
    the syntax and semantics of goal-directed human activities,” in *Proceedings of
    the IEEE conference on computer vision and pattern recognition*, 2014, pp. 780–787.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] H. Kuehne, A. Arslan 和 T. Serre，“动作语言：恢复目标导向人类活动的句法和语义”，发表于*IEEE计算机视觉与模式识别会议论文集*，2014年，pp.
    780–787。'
- en: '[12] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, “Temporal convolutional
    networks for action segmentation and detection,” in *proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 156–165.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] C. Lea, M. D. Flynn, R. Vidal, A. Reiter 和 G. D. Hager，“用于动作分割和检测的时间卷积网络”，发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，pp.
    156–165。'
- en: '[13] Y.-W. Chao, S. Vijayanarasimhan, B. Seybold, D. A. Ross, J. Deng, and
    R. Sukthankar, “Rethinking the faster r-cnn architecture for temporal action localization,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 1130–1139.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y.-W. Chao, S. Vijayanarasimhan, B. Seybold, D. A. Ross, J. Deng 和 R.
    Sukthankar，“重新思考Faster R-CNN架构用于时间动作定位”，发表于*IEEE计算机视觉与模式识别会议论文集*，2018年，pp. 1130–1139。'
- en: '[14] Z. Shou, J. Chan, A. Zareian, K. Miyazawa, and S.-F. Chang, “Cdc: Convolutional-de-convolutional
    networks for precise temporal action localization in untrimmed videos,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2017, pp.
    5734–5743.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Z. Shou, J. Chan, A. Zareian, K. Miyazawa 和 S.-F. Chang，“CDC：卷积-反卷积网络用于未裁剪视频中的精确时间动作定位”，发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，pp.
    5734–5743。'
- en: '[15] Z. Gao, L. Wang, Q. Zhang, Z. Niu, N. Zheng, and G. Hua, “Video imprint
    segmentation for temporal action detection in untrimmed videos,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 33, 2019, pp. 8328–8335.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Z. Gao, L. Wang, Q. Zhang, Z. Niu, N. Zheng, 和 G. Hua, “视频印记分割用于未裁剪视频中的时间动作检测，”
    在 *AAAI人工智能会议论文集*，第33卷，2019年，第8328–8335页。'
- en: '[16] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2015, pp. 3431–3440.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Long, E. Shelhamer, 和 T. Darrell, “用于语义分割的全卷积网络，” 在 *IEEE计算机视觉与模式识别会议论文集*，2015年，第3431–3440页。'
- en: '[17] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman, “The
    pascal visual object classes challenge 2012 (voc2012) results (2012),” in *URL
    http://www. pascal-network. org/challenges/VOC/voc2011/workshop/index. html*,
    2011.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Everingham, L. Van Gool, C. Williams, J. Winn, 和 A. Zisserman, “Pascal视觉对象类别挑战2012
    (voc2012)结果 (2012),” 在 *URL http://www.pascal-network.org/challenges/VOC/voc2011/workshop/index.html*，2011年。'
- en: '[18] Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin, “Temporal action
    detection with structured segment networks,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2017, pp. 2914–2923.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, 和 D. Lin, “具有结构化段网络的时间动作检测，”
    在 *IEEE国际计算机视觉会议论文集*，2017年，第2914–2923页。'
- en: '[19] Z. Shou, D. Wang, and S.-F. Chang, “Temporal action localization in untrimmed
    videos via multi-stage cnns,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2016, pp. 1049–1058.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Z. Shou, D. Wang, 和 S.-F. Chang, “通过多阶段卷积神经网络在未裁剪视频中进行时间动作定位，” 在 *IEEE计算机视觉与模式识别会议论文集*，2016年，第1049–1058页。'
- en: '[20] Y. Xiong, L. Wang, Z. Wang, B. Zhang, H. Song, W. Li, D. Lin, Y. Qiao,
    L. Van Gool, and X. Tang, “Cuhk & ethz & siat submission to activitynet challenge
    2016,” *arXiv preprint arXiv:1608.00797*, 2016.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Xiong, L. Wang, Z. Wang, B. Zhang, H. Song, W. Li, D. Lin, Y. Qiao,
    L. Van Gool, 和 X. Tang, “CUHK & ETHZ & SIAT 提交至ActivityNet挑战2016，” *arXiv 预印本
    arXiv:1608.00797*，2016年。'
- en: '[21] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *Proceedings of the
    IEEE international conference on computer vision*, 2015, pp. 4489–4497.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] D. Tran, L. Bourdev, R. Fergus, L. Torresani, 和 M. Paluri, “利用3D卷积网络学习时空特征，”
    在 *IEEE国际计算机视觉会议论文集*，2015年，第4489–4497页。'
- en: '[22] K. Simonyan and A. Zisserman, “Two-stream convolutional networks for action
    recognition in videos,” in *Advances in neural information processing systems*,
    2014, pp. 568–576.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] K. Simonyan 和 A. Zisserman, “用于视频动作识别的双流卷积网络，” 在 *神经信息处理系统进展*，2014年，第568–576页。'
- en: '[23] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool,
    “Temporal segment networks: Towards good practices for deep action recognition,”
    in *European conference on computer vision*.   Springer, 2016, pp. 20–36.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, 和 L. Van Gool, “时间段网络：朝着深度动作识别的良好实践迈进，”
    在 *欧洲计算机视觉会议*。Springer，2016年，第20–36页。'
- en: '[24] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in *proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017, pp. 6299–6308.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Carreira 和 A. Zisserman, “Quo vadis, 动作识别？一种新模型及Kinetics数据集，” 在 *IEEE计算机视觉与模式识别会议论文集*，2017年，第6299–6308页。'
- en: '[25] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习，” 在 *IEEE计算机视觉与模式识别会议论文集*，2016年，第770–778页。'
- en: '[26] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” *arXiv preprint arXiv:1502.03167*,
    2015.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Ioffe 和 C. Szegedy, “批量归一化：通过减少内部协变量偏移加速深度网络训练，” *arXiv 预印本 arXiv:1502.03167*，2015年。'
- en: '[27] J. Gao, Z. Yang, K. Chen, C. Sun, and R. Nevatia, “Turn tap: Temporal
    unit regression network for temporal action proposals,” in *Proceedings of the
    IEEE International Conference on Computer Vision*, 2017, pp. 3628–3636.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. Gao, Z. Yang, K. Chen, C. Sun, 和 R. Nevatia, “Turn tap: 时间单元回归网络用于时间动作提议，”
    在 *IEEE国际计算机视觉会议论文集*，2017年，第3628–3636页。'
- en: '[28] J. Gao, Z. Yang, and R. Nevatia, “Cascaded boundary regression for temporal
    action detection,” *arXiv preprint arXiv:1705.01180*, 2017.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Gao, Z. Yang, 和 R. Nevatia, “用于时间动作检测的级联边界回归，” *arXiv 预印本 arXiv:1705.01180*，2017年。'
- en: '[29] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in *Advances in neural information
    processing systems*, 2015, pp. 91–99.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Ren, K. He, R. Girshick, 和 J. Sun，“Faster r-cnn：利用区域提案网络实现实时物体检测，”
    在 *神经信息处理系统进展*，2015年，第91–99页。'
- en: '[30] H. Xu, A. Das, and K. Saenko, “R-c3d: Region convolutional 3d network
    for temporal activity detection,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 5783–5792.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. Xu, A. Das, 和 K. Saenko，“R-c3d：用于时间活动检测的区域卷积 3d 网络，” 在 *IEEE 国际计算机视觉会议论文集*，2017年，第5783–5792页。'
- en: '[31] J. Li, X. Liu, Z. Zong, W. Zhao, M. Zhang, and J. Song, “Graph attention
    based proposal 3d convnets for action detection.” in *AAAI*, 2020, pp. 4626–4633.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Li, X. Liu, Z. Zong, W. Zhao, M. Zhang, 和 J. Song，“基于图注意力的提案 3d 卷积网络用于动作检测。”
    在 *AAAI*，2020年，第4626–4633页。'
- en: '[32] G. Chen, C. Zhang, and Y. Zou, “Afnet: Temporal locality-aware network
    with dual structure for accurate and fast action detection,” *IEEE Transactions
    on Multimedia*, 2020.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] G. Chen, C. Zhang, 和 Y. Zou，“Afnet：具有双结构的时间局部性感知网络，用于准确和快速的动作检测，” *IEEE
    多媒体汇刊*，2020年。'
- en: '[33] G. Gong, L. Zheng, and Y. Mu, “Scale matters: Temporal scale aggregation
    network for precise action localization in untrimmed videos,” in *2020 IEEE International
    Conference on Multimedia and Expo (ICME)*.   IEEE, 2020, pp. 1–6.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] G. Gong, L. Zheng, 和 Y. Mu，“尺度重要：用于精准动作定位的时间尺度聚合网络，” 在 *2020 IEEE 国际多媒体与展览会议（ICME）*。
    IEEE，2020年，第1–6页。'
- en: '[34] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in *European conference on computer
    vision*.   Springer, 2016, pp. 21–37.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, 和 A. C.
    Berg，“Ssd：单次多框检测器，” 在 *欧洲计算机视觉会议*。 Springer，2016年，第21–37页。'
- en: '[35] T. Lin, X. Zhao, and Z. Shou, “Single shot temporal action detection,”
    in *Proceedings of the 25th ACM international conference on Multimedia*, 2017,
    pp. 988–996.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Lin, X. Zhao, 和 Z. Shou，“单次时间动作检测，” 在 *第25届ACM国际多媒体会议论文集*，2017年，第988–996页。'
- en: '[36] D. Zhang, X. Dai, X. Wang, and Y.-F. Wang, “S3d: single shot multi-span
    detector via fully 3d convolutional networks,” *arXiv preprint arXiv:1807.08069*,
    2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Zhang, X. Dai, X. Wang, 和 Y.-F. Wang，“S3d：通过完全 3d 卷积网络的单次多跨度检测器，” *arXiv
    预印本 arXiv:1807.08069*，2018年。'
- en: '[37] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] O. Ronneberger, P. Fischer, 和 T. Brox，“U-net：用于生物医学图像分割的卷积网络，” 在 *国际医学图像计算与计算机辅助干预会议*。
    Springer，2015年，第234–241页。'
- en: '[38] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 2117–2125.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, 和 S. Belongie，“用于物体检测的特征金字塔网络，”
    在 *IEEE 计算机视觉与模式识别会议论文集*，2017年，第2117–2125页。'
- en: '[39] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “Dssd: Deconvolutional
    single shot detector,” *arXiv preprint arXiv:1701.06659*, 2017.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, 和 A. C. Berg，“Dssd：去卷积单次检测器，” *arXiv
    预印本 arXiv:1701.06659*，2017年。'
- en: '[40] Y. Liu, L. Ma, Y. Zhang, W. Liu, and S.-F. Chang, “Multi-granularity generator
    for temporal action proposal,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 3604–3613.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Y. Liu, L. Ma, Y. Zhang, W. Liu, 和 S.-F. Chang，“用于时间动作提案的多粒度生成器，” 在 *IEEE
    计算机视觉与模式识别会议论文集*，2019年，第3604–3613页。'
- en: '[41] Q. Liu and Z. Wang, “Progressive boundary refinement network for temporal
    action detection.”'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Q. Liu 和 Z. Wang，“用于时间动作检测的渐进边界精化网络。”'
- en: '[42] J. Gao, Z. Shi, G. Wang, J. Li, Y. Yuan, S. Ge, and X. Zhou, “Accurate
    temporal action proposal generation with relation-aware pyramid network.” in *AAAI*,
    2020, pp. 10 810–10 817.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Gao, Z. Shi, G. Wang, J. Li, Y. Yuan, S. Ge, 和 X. Zhou，“基于关系感知金字塔网络的准确时间动作提案生成。”
    在 *AAAI*，2020年，第10,810–10,817页。'
- en: '[43] X. Li, T. Lin, X. Liu, C. Gan, W. Zuo, C. Li, X. Long, D. He, F. Li, and
    S. Wen, “Deep concept-wise temporal convolutional networks for action localization,”
    *arXiv preprint arXiv:1908.09442*, 2019.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Li, T. Lin, X. Liu, C. Gan, W. Zuo, C. Li, X. Long, D. He, F. Li, 和
    S. Wen，“用于动作定位的深度概念级时间卷积网络，” *arXiv 预印本 arXiv:1908.09442*，2019年。'
- en: '[44] X. Wang, C. Gao, S. Zhang, and N. Sang, “Multi-level temporal pyramid
    network for action detection,” in *Chinese Conference on Pattern Recognition and
    Computer Vision (PRCV)*.   Springer, 2020, pp. 41–54.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. B. Roerdink and A. Meijster, “The watershed transform: Definitions,
    algorithms and parallelization strategies,” *Fundamenta informaticae*, vol. 41,
    no. 1, 2, pp. 187–228, 2000.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] T. Lin, X. Zhao, H. Su, C. Wang, and M. Yang, “Bsn: Boundary sensitive
    network for temporal action proposal generation,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 3–19.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] T. Lin, X. Liu, X. Li, E. Ding, and S. Wen, “Bmn: Boundary-matching network
    for temporal action proposal generation,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2019, pp. 3889–3898.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] C. Lin, J. Li, Y. Wang, Y. Tai, D. Luo, Z. Cui, C. Wang, J. Li, F. Huang,
    and R. Ji, “Fast learning of temporal action proposal via dense boundary generator.”
    in *AAAI*, 2020, pp. 11 499–11 506.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y. Bai, Y. Wang, Y. Tong, Y. Yang, Q. Liu, and J. Liu, “Boundary content
    graph neural network for temporal action proposal generation,” *arXiv preprint
    arXiv:2008.01432*, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] L. Yang, H. Peng, D. Zhang, J. Fu, and J. Han, “Revisiting anchor mechanisms
    for temporal action localization,” *IEEE Transactions on Image Processing*, vol. 29,
    pp. 8535–8548, 2020.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Lin, C. Xu, D. Luo, Y. Wang, Y. Tai, C. Wang, J. Li, F. Huang, and
    Y. Fu, “Learning salient boundary feature for anchor-free temporal action localization,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2021, pp. 3320–3329.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Gao, K. Chen, and R. Nevatia, “Ctap: Complementary temporal action
    proposal generation,” in *Proceedings of the European conference on computer vision
    (ECCV)*, 2018, pp. 68–83.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Buch, V. Escorcia, C. Shen, B. Ghanem, and J. Carlos Niebles, “Sst:
    Single-stream temporal action proposals,” in *Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 2911–2920.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Buch, V. Escorcia, B. Ghanem, L. Fei-Fei, and J. C. Niebles, “End-to-end,
    single-stream temporal action detection in untrimmed videos,” 2019.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Yuan, B. Ni, X. Yang, and A. A. Kassim, “Temporal action localization
    with pyramid of score distribution features,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 3093–3102.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei, “End-to-end learning
    of action detection from frame glimpses in videos,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2016, pp. 2678–2687.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-Fei,
    “Every moment counts: Dense detailed labeling of actions in complex videos,” *International
    Journal of Computer Vision*, vol. 126, no. 2-4, pp. 375–389, 2018.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] V. Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem, “Daps: Deep
    action proposals for action understanding,” in *European Conference on Computer
    Vision*.   Springer, 2016, pp. 768–784.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] V. Escorcia, F. C. Heilbron, J. C. Niebles 和 B. Ghanem，“DAPS: 用于动作理解的深度动作提案，”
    收录于 *欧洲计算机视觉会议*。  Springer，2016年，第768–784页。'
- en: '[59] B. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao, “A multi-stream
    bi-directional recurrent neural network for fine-grained action detection,” in
    *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2016, pp. 1961–1970.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] B. Singh, T. K. Marks, M. Jones, O. Tuzel 和 M. Shao，“用于细粒度动作检测的多流双向递归神经网络，”
    收录于 *IEEE 计算机视觉与模式识别会议论文集*，2016年，第1961–1970页。'
- en: '[60] S. Ma, L. Sigal, and S. Sclaroff, “Learning activity progression in lstms
    for activity detection and early detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 1942–1950.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Ma, L. Sigal 和 S. Sclaroff，“在LSTMs中学习活动进展以进行活动检测和早期检测，” 收录于 *IEEE 计算机视觉与模式识别会议论文集*，2016年，第1942–1950页。'
- en: '[61] R. Zeng, W. Huang, M. Tan, Y. Rong, P. Zhao, J. Huang, and C. Gan, “Graph
    convolutional networks for temporal action localization,” in *Proceedings of the
    IEEE International Conference on Computer Vision*, 2019, pp. 7094–7103.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] R. Zeng, W. Huang, M. Tan, Y. Rong, P. Zhao, J. Huang 和 C. Gan，“图卷积网络用于时间动作定位，”
    收录于 *IEEE 国际计算机视觉会议论文集*，2019年，第7094–7103页。'
- en: '[62] M. Xu, C. Zhao, D. S. Rojas, A. Thabet, and B. Ghanem, “G-tad: Sub-graph
    localization for temporal action detection,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 10 156–10 165.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] M. Xu, C. Zhao, D. S. Rojas, A. Thabet 和 B. Ghanem，“G-TAD: 用于时间动作检测的子图定位，”
    收录于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2020年，第10,156–10,165页。'
- en: '[63] S. Chang, P. Wang, F. Wang, H. Li, and J. Feng, “Augmented transformer
    with adaptive graph for temporal action proposal generation,” *arXiv preprint
    arXiv:2103.16024*, 2021.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Chang, P. Wang, F. Wang, H. Li 和 J. Feng，“增强变换器与自适应图用于时间动作提案生成，” *arXiv
    预印本 arXiv:2103.16024*，2021年。'
- en: '[64] C. Zhao, A. Thabet, and B. Ghanem, “Video self-stitching graph network
    for temporal action localization,” *arXiv preprint arXiv:2011.14598*, 2020.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] C. Zhao, A. Thabet 和 B. Ghanem，“视频自拼接图网络用于时间动作定位，” *arXiv 预印本 arXiv:2011.14598*，2020年。'
- en: '[65] M. Nawhal and G. Mori, “Activity graph transformer for temporal action
    localization,” *arXiv preprint arXiv:2101.08540*, 2021.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] M. Nawhal 和 G. Mori，“用于时间动作定位的活动图变换器，” *arXiv 预印本 arXiv:2101.08540*，2021年。'
- en: '[66] J. Tan, J. Tang, L. Wang, and G. Wu, “Relaxed transformer decoders for
    direct action proposal generation,” *arXiv preprint arXiv:2102.01894*, 2021.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Tan, J. Tang, L. Wang 和 G. Wu，“用于直接动作提案生成的松弛变换器解码器，” *arXiv 预印本 arXiv:2102.01894*，2021年。'
- en: '[67] I. Laptev and P. Pérez, “Retrieving actions in movies,” in *2007 IEEE
    11th International Conference on Computer Vision*.   IEEE, 2007, pp. 1–8.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] I. Laptev 和 P. Pérez，“在电影中检索动作，” 收录于 *2007 IEEE 第11届国际计算机视觉会议*。  IEEE，2007年，第1–8页。'
- en: '[68] L. Cao, Z. Liu, and T. S. Huang, “Cross-dataset action detection,” in
    *2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition*.   IEEE,
    2010, pp. 1998–2005.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] L. Cao, Z. Liu 和 T. S. Huang，“跨数据集动作检测，” 收录于 *2010 IEEE 计算机视觉与模式识别会议*。  IEEE，2010年，第1998–2005页。'
- en: '[69] M. Jain, J. Van Gemert, H. Jégou, P. Bouthemy, and C. G. Snoek, “Action
    localization with tubelets from motion,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2014, pp. 740–747.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. Jain, J. Van Gemert, H. Jégou, P. Bouthemy 和 C. G. Snoek，“利用运动中的小管道进行动作定位，”
    收录于 *IEEE 计算机视觉与模式识别会议论文集*，2014年，第740–747页。'
- en: '[70] D. Oneata, J. Revaud, J. Verbeek, and C. Schmid, “Spatio-temporal object
    detection proposals,” in *European conference on computer vision*.   Springer,
    2014, pp. 737–752.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] D. Oneata, J. Revaud, J. Verbeek 和 C. Schmid，“时空目标检测提案，” 收录于 *欧洲计算机视觉会议*。  Springer，2014年，第737–752页。'
- en: '[71] W. Chen and J. J. Corso, “Action detection by implicit intentional motion
    clustering,” in *Proceedings of the IEEE international conference on computer
    vision*, 2015, pp. 3298–3306.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] W. Chen 和 J. J. Corso，“通过隐式意图运动聚类进行动作检测，” 收录于 *IEEE 国际计算机视觉会议论文集*，2015年，第3298–3306页。'
- en: '[72] J. C. Van Gemert, M. Jain, E. Gati, C. G. Snoek *et al.*, “Apt: Action
    localization proposals from dense trajectories.” in *BMVC*, vol. 2, 2015, p. 4.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. C. Van Gemert, M. Jain, E. Gati, C. G. Snoek *等*，“APT: 从密集轨迹中生成动作定位提案。”
    收录于 *BMVC*，第2卷，2015年，第4页。'
- en: '[73] M. M. Puscas, E. Sangineto, D. Culibrk, and N. Sebe, “Unsupervised tube
    extraction using transductive learning and dense trajectories,” in *Proceedings
    of the IEEE international conference on computer vision*, 2015, pp. 1653–1661.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. M. Puscas, E. Sangineto, D. Culibrk, 和 N. Sebe，“使用迁移学习和密集轨迹的无监督管道提取”，发表于
    *IEEE国际计算机视觉会议论文集*，2015年，页码1653–1661。'
- en: '[74] G. Gkioxari and J. Malik, “Finding action tubes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 759–768.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] G. Gkioxari 和 J. Malik，“寻找动作管道”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2015年，页码759–768。'
- en: '[75] R. Hou, C. Chen, and M. Shah, “Tube convolutional neural network (t-cnn)
    for action detection in videos,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 5822–5831.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] R. Hou, C. Chen, 和 M. Shah，“用于视频中动作检测的管道卷积神经网络（t-cnn）”，发表于 *IEEE国际计算机视觉会议论文集*，2017年，页码5822–5831。'
- en: '[76] X. Peng and C. Schmid, “Multi-region two-stream r-cnn for action detection,”
    in *European conference on computer vision*.   Springer, 2016, pp. 744–759.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] X. Peng 和 C. Schmid，“用于动作检测的多区域双流R-CNN”，发表于 *欧洲计算机视觉会议*。 Springer，2016年，页码744–759。'
- en: '[77] G. Singh, S. Saha, M. Sapienza, P. H. Torr, and F. Cuzzolin, “Online real-time
    multiple spatiotemporal action localisation and prediction,” in *Proceedings of
    the IEEE International Conference on Computer Vision*, 2017, pp. 3637–3646.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] G. Singh, S. Saha, M. Sapienza, P. H. Torr, 和 F. Cuzzolin，“在线实时多重时空动作定位与预测”，发表于
    *IEEE国际计算机视觉会议论文集*，2017年，页码3637–3646。'
- en: '[78] S. Saha, G. Singh, M. Sapienza, P. H. Torr, and F. Cuzzolin, “Deep learning
    for detecting multiple space-time action tubes in videos,” *arXiv preprint arXiv:1608.01529*,
    2016.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] S. Saha, G. Singh, M. Sapienza, P. H. Torr, 和 F. Cuzzolin，“用于检测视频中多个时空动作管道的深度学习”，*arXiv预印本
    arXiv:1608.01529*，2016年。'
- en: '[79] P. Weinzaepfel, Z. Harchaoui, and C. Schmid, “Learning to track for spatio-temporal
    action localization,” in *Proceedings of the IEEE international conference on
    computer vision*, 2015, pp. 3164–3172.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] P. Weinzaepfel, Z. Harchaoui, 和 C. Schmid，“学习跟踪以进行时空动作定位”，发表于 *IEEE国际计算机视觉会议论文集*，2015年，页码3164–3172。'
- en: '[80] Z. Yang, J. Gao, and R. Nevatia, “Spatio-temporal action detection with
    cascade proposal and location anticipation,” *arXiv preprint arXiv:1708.00042*,
    2017.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Z. Yang, J. Gao, 和 R. Nevatia，“使用级联提议和位置预测的时空动作检测”，*arXiv预印本 arXiv:1708.00042*，2017年。'
- en: '[81] Y. Ye, X. Yang, and Y. Tian, “Discovering spatio-temporal action tubes,”
    *Journal of Visual Communication and Image Representation*, vol. 58, pp. 515–524,
    2019.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Ye, X. Yang, 和 Y. Tian，“发现时空动作管道”，*视觉通信与图像表示杂志*，第58卷，页码515–524，2019年。'
- en: '[82] Z. Li, K. Gavrilyuk, E. Gavves, M. Jain, and C. G. Snoek, “Videolstm convolves,
    attends and flows for action recognition,” *Computer Vision and Image Understanding*,
    vol. 166, pp. 41–50, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Z. Li, K. Gavrilyuk, E. Gavves, M. Jain, 和 C. G. Snoek，“Videolstm卷积、关注和流动用于动作识别”，*计算机视觉与图像理解*，第166卷，页码41–50，2018年。'
- en: '[83] L. Wang, Y. Qiao, X. Tang, and L. Van Gool, “Actionness estimation using
    hybrid fully convolutional networks,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2016, pp. 2708–2717.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] L. Wang, Y. Qiao, X. Tang, 和 L. Van Gool，“使用混合全卷积网络进行动作性估计”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2016年，页码2708–2717。'
- en: '[84] G. Yu and J. Yuan, “Fast action proposals for human action detection and
    search,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2015, pp. 1302–1311.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] G. Yu 和 J. Yuan，“用于人类动作检测和搜索的快速动作提议”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2015年，页码1302–1311。'
- en: '[85] V. Kalogeiton, P. Weinzaepfel, V. Ferrari, and C. Schmid, “Action tubelet
    detector for spatio-temporal action localization,” in *Proceedings of the IEEE
    International Conference on Computer Vision*, 2017, pp. 4405–4413.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] V. Kalogeiton, P. Weinzaepfel, V. Ferrari, 和 C. Schmid，“用于时空动作定位的动作小管检测器”，发表于
    *IEEE国际计算机视觉会议论文集*，2017年，页码4405–4413。'
- en: '[86] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *et al.*, “Ava: A video dataset of spatio-temporally
    localized atomic visual actions,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018, pp. 6047–6056.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *等*，“Ava: 一种时空定位的原子视觉动作视频数据集”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2018年，页码6047–6056。'
- en: '[87] X. Yang, X. Yang, M.-Y. Liu, F. Xiao, L. S. Davis, and J. Kautz, “Step:
    Spatio-temporal progressive learning for video action detection,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2019, pp.
    264–272.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] X. Yang, X. Yang, M.-Y. Liu, F. Xiao, L. S. Davis, 和 J. Kautz，“Step: 时空渐进学习用于视频动作检测，”见于*IEEE计算机视觉与模式识别会议论文集*，2019年，第264–272页。'
- en: '[88] C.-Y. Wu, C. Feichtenhofer, H. Fan, K. He, P. Krahenbuhl, and R. Girshick,
    “Long-term feature banks for detailed video understanding,” in *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition*, 2019, pp. 284–293.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C.-Y. Wu, C. Feichtenhofer, H. Fan, K. He, P. Krahenbuhl, 和 R. Girshick，“用于详细视频理解的长期特征库，”见于*IEEE计算机视觉与模式识别会议论文集*，2019年，第284–293页。'
- en: '[89] C. Sun, A. Shrivastava, C. Vondrick, K. Murphy, R. Sukthankar, and C. Schmid,
    “Actor-centric relation network,” in *Proceedings of the European Conference on
    Computer Vision (ECCV)*, 2018, pp. 318–334.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] C. Sun, A. Shrivastava, C. Vondrick, K. Murphy, R. Sukthankar, 和 C. Schmid，“以演员为中心的关系网络，”见于*欧洲计算机视觉会议（ECCV）*，2018年，第318–334页。'
- en: '[90] Y. Zhang, P. Tokmakov, M. Hebert, and C. Schmid, “A structured model for
    action detection,” in *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition*, 2019, pp. 9975–9984.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Y. Zhang, P. Tokmakov, M. Hebert, 和 C. Schmid，“一种结构化的动作检测模型，”见于*IEEE计算机视觉与模式识别会议论文集*，2019年，第9975–9984页。'
- en: '[91] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video action transformer
    network,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 244–253.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] R. Girdhar, J. Carreira, C. Doersch, 和 A. Zisserman，“视频动作变换器网络，”见于*IEEE计算机视觉与模式识别会议论文集*，2019年，第244–253页。'
- en: '[92] O. Ulutan, S. Rallapalli, M. Srivatsa, C. Torres, and B. Manjunath, “Actor
    conditioned attention maps for video action detection,” in *The IEEE Winter Conference
    on Applications of Computer Vision*, 2020, pp. 527–536.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] O. Ulutan, S. Rallapalli, M. Srivatsa, C. Torres, 和 B. Manjunath，“用于视频动作检测的演员条件注意力图，”见于*IEEE冬季计算机视觉应用会议*，2020年，第527–536页。'
- en: '[93] M. Tomei, L. Baraldi, S. Calderara, S. Bronzin, and R. Cucchiara, “Stage:
    Spatio-temporal attention on graph entities for video action detection,” *arXiv
    preprint arXiv:1912.04316*, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] M. Tomei, L. Baraldi, S. Calderara, S. Bronzin, 和 R. Cucchiara，“Stage:
    图实体的时空注意力用于视频动作检测，”*arXiv预印本 arXiv:1912.04316*，2019年。'
- en: '[94] J. Ji, R. Krishna, L. Fei-Fei, and J. C. Niebles, “Action genome: Actions
    as compositions of spatio-temporal scene graphs,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 10 236–10 247.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J. Ji, R. Krishna, L. Fei-Fei, 和 J. C. Niebles，“动作基因组：动作作为时空场景图的组合，”见于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第10 236–10 247页。'
- en: '[95] J. Pan, S. Chen, Z. Shou, J. Shao, and H. Li, “Actor-context-actor relation
    network for spatio-temporal action localization,” *arXiv preprint arXiv:2006.07976*,
    2020.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Pan, S. Chen, Z. Shou, J. Shao, 和 H. Li，“用于时空动作定位的演员-上下文-演员关系网络，”*arXiv预印本
    arXiv:2006.07976*，2020年。'
- en: '[96] M. Tomei, L. Baraldi, S. Calderara, S. Bronzin, and R. Cucchiara, “Video
    action detection by learning graph-based spatio-temporal interactions,” *Computer
    Vision and Image Understanding*, vol. 206, p. 103187, 2021.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] M. Tomei, L. Baraldi, S. Calderara, S. Bronzin, 和 R. Cucchiara，“通过学习基于图的时空交互进行视频动作检测，”*计算机视觉与图像理解*，第206卷，第103187页，2021年。'
- en: '[97] C. Sun, S. Shetty, R. Sukthankar, and R. Nevatia, “Temporal localization
    of fine-grained actions in videos by domain transfer from web images,” in *Proceedings
    of the 23rd ACM international conference on Multimedia*, 2015, pp. 371–380.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] C. Sun, S. Shetty, R. Sukthankar, 和 R. Nevatia，“通过从网页图像进行领域转移来对视频中的细粒度动作进行时间定位，”见于*第23届ACM国际多媒体会议论文集*，2015年，第371–380页。'
- en: '[98] P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce, C. Schmid, and
    J. Sivic, “Weakly supervised action labeling in videos under ordering constraints,”
    in *European Conference on Computer Vision*.   Springer, 2014, pp. 628–643.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce, C. Schmid, 和
    J. Sivic，“在顺序约束下的视频弱监督动作标注，”见于*欧洲计算机视觉会议*。 Springer，2014年，第628–643页。'
- en: '[99] D.-A. Huang, L. Fei-Fei, and J. C. Niebles, “Connectionist temporal modeling
    for weakly supervised action labeling,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 137–153.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] D.-A. Huang, L. Fei-Fei, 和 J. C. Niebles，“用于弱监督动作标注的连接主义时间建模，”见于*欧洲计算机视觉会议*。
    Springer，2016年，第137–153页。'
- en: '[100] A. Richard, H. Kuehne, and J. Gall, “Weakly supervised action learning
    with rnn based fine-to-coarse modeling,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 754–763.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] A. Richard, H. Kuehne, 和 J. Gall，“基于 RNN 的从细到粗建模的弱监督动作学习，” 收录于 *IEEE计算机视觉与模式识别会议论文集*，2017年，页码
    754–763。'
- en: '[101] H. Kuehne, A. Richard, and J. Gall, “Weakly supervised learning of actions
    from transcripts,” *Computer Vision and Image Understanding*, vol. 163, pp. 78–89,
    2017.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] H. Kuehne, A. Richard, 和 J. Gall，“从文字记录中弱监督学习动作，” *计算机视觉与图像理解*，第163卷，页码
    78–89，2017年。'
- en: '[102] S. Narayan, H. Cholakkal, F. S. Khan, and L. Shao, “3c-net: Category
    count and center loss for weakly-supervised action localization,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2019, pp. 8679–8687.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] S. Narayan, H. Cholakkal, F. S. Khan, 和 L. Shao，“3c-net: 类别计数和中心损失用于弱监督动作定位，”
    收录于 *IEEE国际计算机视觉会议论文集*，2019年，页码 8679–8687。'
- en: '[103] J. Schroeter, K. Sidorov, and D. Marshall, “Weakly-supervised temporal
    localization via occurrence count learning,” *arXiv preprint arXiv:1905.07293*,
    2019.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Schroeter, K. Sidorov, 和 D. Marshall，“通过发生次数学习进行弱监督时间定位，” *arXiv 预印本
    arXiv:1905.07293*，2019年。'
- en: '[104] L. Wang, Y. Xiong, D. Lin, and L. Van Gool, “Untrimmednets for weakly
    supervised action recognition and detection,” in *Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 4325–4334.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] L. Wang, Y. Xiong, D. Lin, 和 L. Van Gool，“用于弱监督动作识别与检测的 Untrimmednets，”
    收录于 *IEEE计算机视觉与模式识别会议论文集*，2017年，页码 4325–4334。'
- en: '[105] S. Paul, S. Roy, and A. K. Roy-Chowdhury, “W-talc: Weakly-supervised
    temporal activity localization and classification,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 563–579.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] S. Paul, S. Roy, 和 A. K. Roy-Chowdhury，“W-talc: 弱监督时间活动定位与分类，” 收录于 *欧洲计算机视觉会议
    (ECCV) 论文集*，2018年，页码 563–579。'
- en: '[106] A. Islam and R. Radke, “Weakly supervised temporal action localization
    using deep metric learning,” in *The IEEE Winter Conference on Applications of
    Computer Vision*, 2020, pp. 547–556.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Islam 和 R. Radke，“使用深度度量学习进行弱监督时间动作定位，” 收录于 *IEEE冬季计算机视觉应用会议论文集*，2020年，页码
    547–556。'
- en: '[107] M.-A. Carbonneau, V. Cheplygina, E. Granger, and G. Gagnon, “Multiple
    instance learning: A survey of problem characteristics and applications,” *Pattern
    Recognition*, vol. 77, pp. 329–353, 2018.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M.-A. Carbonneau, V. Cheplygina, E. Granger, 和 G. Gagnon，“多实例学习：问题特征与应用的综述，”
    *模式识别*，第77卷，页码 329–353，2018年。'
- en: '[108] M. Rashid, H. Kjellstrom, and Y. J. Lee, “Action graphs: Weakly-supervised
    action localization with graph convolution networks,” in *The IEEE Winter Conference
    on Applications of Computer Vision*, 2020, pp. 615–624.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] M. Rashid, H. Kjellstrom, 和 Y. J. Lee，“动作图：使用图卷积网络的弱监督动作定位，” 收录于 *IEEE冬季计算机视觉应用会议论文集*，2020年，页码
    615–624。'
- en: '[109] M. Jain, A. Ghodrati, and C. G. Snoek, “Actionbytes: Learning from trimmed
    videos to localize actions,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 1171–1180.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. Jain, A. Ghodrati, 和 C. G. Snoek，“Actionbytes: 从修剪视频中学习以定位动作，” 收录于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页码 1171–1180。'
- en: '[110] M. Gao, Y. Zhou, R. Xu, R. Socher, and C. Xiong, “Woad: Weakly supervised
    online action detection in untrimmed videos,” *arXiv preprint arXiv:2006.03732*,
    2020.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] M. Gao, Y. Zhou, R. Xu, R. Socher, 和 C. Xiong，“Woad: 在未修剪视频中进行弱监督在线动作检测，”
    *arXiv 预印本 arXiv:2006.03732*，2020年。'
- en: '[111] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature learning
    approach for deep face recognition,” in *European conference on computer vision*.   Springer,
    2016, pp. 499–515.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Y. Wen, K. Zhang, Z. Li, 和 Y. Qiao，“一种用于深度人脸识别的区分特征学习方法，” 收录于 *欧洲计算机视觉会议*。   Springer,
    2016年，页码 499–515。'
- en: '[112] L. Huang, Y. Huang, W. Ouyang, L. Wang *et al.*, “Relational prototypical
    network for weakly supervised temporal action localization,” 2020.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] L. Huang, Y. Huang, W. Ouyang, L. Wang *等*，“用于弱监督时间动作定位的关系原型网络，” 2020年。'
- en: '[113] P. X. Nguyen, D. Ramanan, and C. C. Fowlkes, “Weakly-supervised action
    localization with background modeling,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2019, pp. 5502–5511.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] P. X. Nguyen, D. Ramanan, 和 C. C. Fowlkes，“带背景建模的弱监督动作定位，” 收录于 *IEEE国际计算机视觉会议论文集*，2019年，页码
    5502–5511。'
- en: '[114] Z. Shou, H. Gao, L. Zhang, K. Miyazawa, and S.-F. Chang, “Autoloc: Weakly-supervised
    temporal action localization in untrimmed videos,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 154–171.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Z. Shou、H. Gao、L. Zhang、K. Miyazawa 和 S.-F. Chang，“Autoloc：在未修剪视频中进行弱监督时间动作定位”，见于
    *欧洲计算机视觉会议 (ECCV)*，2018年，页码154–171。'
- en: '[115] Z. Liu, L. Wang, Q. Zhang, Z. Gao, Z. Niu, N. Zheng, and G. Hua, “Weakly
    supervised temporal action localization through contrast based evaluation networks,”
    in *Proceedings of the IEEE International Conference on Computer Vision*, 2019,
    pp. 3899–3908.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Z. Liu、L. Wang、Q. Zhang、Z. Gao、Z. Niu、N. Zheng 和 G. Hua，“通过对比评估网络进行弱监督的时间动作定位”，见于
    *IEEE国际计算机视觉会议论文集*，2019年，页码3899–3908。'
- en: '[116] B. Shi, Q. Dai, Y. Mu, and J. Wang, “Weakly-supervised action localization
    by generative attention modeling,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 1009–1019.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] B. Shi、Q. Dai、Y. Mu 和 J. Wang，“通过生成注意力建模进行弱监督动作定位”，见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页码1009–1019。'
- en: '[117] P. Nguyen, T. Liu, G. Prasad, and B. Han, “Weakly supervised action localization
    by sparse temporal pooling network,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2018, pp. 6752–6761.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] P. Nguyen、T. Liu、G. Prasad 和 B. Han，“通过稀疏时间池网络进行弱监督动作定位”，见于 *IEEE计算机视觉与模式识别会议论文集*，2018年，页码6752–6761。'
- en: '[118] P. Lee, Y. Uh, and H. Byun, “Background suppression network for weakly-supervised
    temporal action localization.” in *AAAI*, 2020, pp. 11 320–11 327.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] P. Lee、Y. Uh 和 H. Byun，“用于弱监督时间动作定位的背景抑制网络”，见于 *AAAI*，2020年，页码11 320–11 327。'
- en: '[119] Y. Yuan, Y. Lyu, X. Shen, I. W. Tsang, and D.-Y. Yeung, “Marginalized
    average attentional network for weakly-supervised learning,” *arXiv preprint arXiv:1905.08586*,
    2019.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Y. Yuan、Y. Lyu、X. Shen、I. W. Tsang 和 D.-Y. Yeung，“用于弱监督学习的边际平均注意力网络”，*arXiv
    预印本 arXiv:1905.08586*，2019年。'
- en: '[120] D. Liu, T. Jiang, and Y. Wang, “Completeness modeling and context separation
    for weakly supervised temporal action localization,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2019, pp. 1298–1307.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] D. Liu、T. Jiang 和 Y. Wang，“用于弱监督时间动作定位的完整性建模和上下文分离”，见于 *IEEE计算机视觉与模式识别会议论文集*，2019年，页码1298–1307。'
- en: '[121] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot
    learning,” in *Advances in neural information processing systems*, 2017, pp. 4077–4087.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] J. Snell、K. Swersky 和 R. Zemel，“用于少样本学习的原型网络”，见于 *神经信息处理系统进展*，2017年，页码4077–4087。'
- en: '[122] K. Sohn, H. Lee, and X. Yan, “Learning structured output representation
    using deep conditional generative models,” in *Advances in neural information
    processing systems*, 2015, pp. 3483–3491.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] K. Sohn、H. Lee 和 X. Yan，“使用深度条件生成模型学习结构化输出表示”，见于 *神经信息处理系统进展*，2015年，页码3483–3491。'
- en: '[123] K. K. Singh and Y. J. Lee, “Hide-and-seek: Forcing a network to be meticulous
    for weakly-supervised object and action localization,” in *2017 IEEE international
    conference on computer vision (ICCV)*.   IEEE, 2017, pp. 3544–3553.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] K. K. Singh 和 Y. J. Lee，“捉迷藏：迫使网络在弱监督对象和动作定位中更加细致”，见于 *2017年IEEE国际计算机视觉大会
    (ICCV)*。IEEE，2017年，页码3544–3553。'
- en: '[124] J.-X. Zhong, N. Li, W. Kong, T. Zhang, T. H. Li, and G. Li, “Step-by-step
    erasion, one-by-one collection: a weakly supervised temporal action detector,”
    in *Proceedings of the 26th ACM international conference on Multimedia*, 2018,
    pp. 35–44.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J.-X. Zhong、N. Li、W. Kong、T. Zhang、T. H. Li 和 G. Li，“一步步擦除，一次性收集：一个弱监督的时间动作检测器”，见于
    *第26届ACM国际多媒体会议论文集*，2018年，页码35–44。'
- en: '[125] R. Zeng, C. Gan, P. Chen, W. Huang, Q. Wu, and M. Tan, “Breaking winner-takes-all:
    Iterative-winners-out networks for weakly supervised temporal action localization,”
    *IEEE Transactions on Image Processing*, vol. 28, no. 12, pp. 5797–5808, 2019.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] R. Zeng、C. Gan、P. Chen、W. Huang、Q. Wu 和 M. Tan，“打破赢家全拿：用于弱监督时间动作定位的迭代赢家淘汰网络”，*IEEE图像处理学报*，第28卷，第12期，页码5797–5808，2019年。'
- en: '[126] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio,
    “A structured self-attentive sentence embedding,” *arXiv preprint arXiv:1703.03130*,
    2017.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Z. Lin、M. Feng、C. N. d. Santos、M. Yu、B. Xiang、B. Zhou 和 Y. Bengio，“一种结构化的自注意句子嵌入”，*arXiv
    预印本 arXiv:1703.03130*，2017年。'
- en: '[127] F. Sener and A. Yao, “Unsupervised learning and segmentation of complex
    activities from video,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 8368–8376.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] F. Sener 和 A. Yao，“从视频中进行无监督学习和复杂活动分割”，见于 *IEEE计算机视觉与模式识别会议论文集*，2018年，页码8368–8376。'
- en: '[128] A. Kukleva, H. Kuehne, F. Sener, and J. Gall, “Unsupervised learning
    of action classes with continuous temporal embedding,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2019, pp. 12 066–12 074.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] G. Gong, X. Wang, Y. Mu, and Q. Tian, “Learning temporal co-attention
    models for unsupervised video action localization,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 9819–9828.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] M.-H. Chen, B. Li, Y. Bao, G. AlRegib, and Z. Kira, “Action segmentation
    with joint self-supervised temporal domain adaptation,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp. 9454–9463.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by backpropagation,”
    in *International conference on machine learning*, 2015, pp. 1180–1189.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, and V. Lempitsky, “Domain-adversarial training of neural networks,”
    *The Journal of Machine Learning Research*, vol. 17, no. 1, pp. 2096–2030, 2016.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles, “Activitynet:
    A large-scale video benchmark for human activity understanding,” in *Proceedings
    of the ieee conference on computer vision and pattern recognition*, 2015, pp.
    961–970.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] H. Zhao, A. Torralba, L. Torresani, and Z. Yan, “Hacs: Human action clips
    and segments dataset for recognition and temporal localization,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2019, pp. 8668–8678.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta,
    “Hollywood in homes: Crowdsourcing data collection for activity understanding,”
    in *European Conference on Computer Vision*.   Springer, 2016, pp. 510–526.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] S. Stein and S. J. McKenna, “Combining embedded accelerometers with computer
    vision for recognizing food preparation activities,” in *Proceedings of the 2013
    ACM international joint conference on Pervasive and ubiquitous computing*, 2013,
    pp. 729–738.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] M. Rohrbach, A. Rohrbach, M. Regneri, S. Amin, M. Andriluka, M. Pinkal,
    and B. Schiele, “Recognizing fine-grained and composite activities using hand-centric
    features and script data,” *International Journal of Computer Vision*, vol. 119,
    no. 3, pp. 346–373, 2016.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Y. Tang, D. Ding, Y. Rao, Y. Zheng, D. Zhang, L. Zhao, J. Lu, and J. Zhou,
    “Coin: A large-scale dataset for comprehensive instructional video analysis,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 1207–1216.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Ji, K. Cao, and J. C. Niebles, “Learning temporal action proposals
    with fewer labels,” in *Proceedings of the IEEE International Conference on Computer
    Vision*, 2019, pp. 7073–7082.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] A. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results,” in *Advances
    in neural information processing systems*, 2017, pp. 1195–1204.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] A. Gaidon, Z. Harchaoui, and C. Schmid, “Actom sequence models for efficient
    action detection,” in *CVPR 2011*.   IEEE, 2011, pp. 3201–3208.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] ——, “Temporal localization of actions with actoms,” *IEEE transactions
    on pattern analysis and machine intelligence*, vol. 35, no. 11, pp. 2782–2795,
    2013.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] O. Duchenne, I. Laptev, J. Sivic, F. Bach, and J. Ponce, “Automatic annotation
    of human actions in video,” in *2009 IEEE 12th International Conference on Computer
    Vision*.   IEEE, 2009, pp. 1491–1498.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Tang, J. Lu, and J. Zhou, “Comprehensive instructional video analysis:
    The coin dataset and performance evaluation,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 2020.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Z. Yuan, J. C. Stroud, T. Lu, and J. Deng, “Temporal action localization
    by structured maximal sums,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017, pp. 3684–3692.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] X. Dai, B. Singh, G. Zhang, L. S. Davis, and Y. Qiu Chen, “Temporal context
    network for activity localization in videos,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2017, pp. 5793–5802.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] F. Long, T. Yao, Z. Qiu, X. Tian, J. Luo, and T. Mei, “Gaussian temporal
    awareness networks for action localization,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 344–353.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] H. Eun, S. Lee, J. Moon, J. Park, C. Jung, and C. Kim, “Srg: Snippet
    relatedness-based temporal action proposal generator,” *IEEE Transactions on Circuits
    and Systems for Video Technology*, 2019.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] H. Su, “Bsn++: Complementary boundary regressor with scale-balanced relation
    modeling for temporal action proposal generation,” in *Proceedings of the Asian
    Conference on Computer Vision*, 2020.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] P. Zhao, L. Xie, C. Ju, Y. Zhang, Y. Wang, and Q. Tian, “Bottom-up temporal
    action localization with mutual regularization.”'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] L. Gao, T. Li, J. Song, Z. Zhao, and H. T. Shen, “Play and rewind: Context-aware
    video temporal action proposals,” *Pattern Recognition*, p. 107477, 2020.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] X. Liu, Q. Wang, Y. Hu, X. Tang, S. Bai, and X. Bai, “End-to-end temporal
    action detection with transformer,” *arXiv preprint arXiv:2106.10271*, 2021.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Q. Liu and Z. Wang, “Progressive boundary refinement network for temporal
    action detection.” in *AAAI*, 2020, pp. 11 612–11 619.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] H. Alwassel, S. Giancola, and B. Ghanem, “Tsp: Temporally-sensitive pretraining
    of video encoders for localization tasks,” *arXiv preprint arXiv:2011.11479*,
    2020.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] C. Wang, H. Cai, Y. Zou, and Y. Xiong, “Rgb stream is enough for temporal
    action detection,” *arXiv preprint arXiv:2107.04362*, 2021.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] J. Wu, P. Sun, S. Chen, J. Yang, Z. Qi, L. Ma, and P. Luo, “Towards high-quality
    temporal action detection with sparse proposals,” *arXiv preprint arXiv:2109.08847*,
    2021.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] X. Liu, Y. Hu, S. Bai, F. Ding, X. Bai, and P. H. Torr, “Multi-shot temporal
    event localization: a benchmark,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2021, pp. 12 596–12 606.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Y. Xu, C. Zhang, Z. Cheng, J. Xie, Y. Niu, S. Pu, and F. Wu, “Segregated
    temporal assembly recurrent networks for weakly supervised multiple action detection,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 33, 2019,
    pp. 9070–9078.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] X.-Y. Zhang, C. Li, H. Shi, X. Zhu, P. Li, and J. Dong, “Adapnet: Adaptability
    decomposing encoder-decoder network for weakly supervised action recognition and
    localization,” *IEEE transactions on neural networks and learning systems*, 2020.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] T. Yu, Z. Ren, Y. Li, E. Yan, N. Xu, and J. Yuan, “Temporal structure
    mining for weakly supervised action detection,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2019, pp. 5522–5531.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Z. Shen, F. Wang, and J. Dai, “Weakly supervised temporal action localization
    by multi-stage fusion network,” *IEEE Access*, vol. 8, pp. 17 287–17 298, 2020.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. Zhai, L. Wang, W. Tang, Q. Zhang, J. Yuan, and G. Hua, “Two-stream
    consensus network for weakly-supervised temporal action localization,” in *European
    conference on computer vision*.   Springer, 2020, pp. 37–54.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] L. Yang, D. Zhang, T. Zhao, and J. Han, “Equivalent classification mapping
    for weakly supervised temporal action localization,” *arXiv preprint arXiv:2008.07728*,
    2020.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] K. Min and J. J. Corso, “Adversarial background-aware loss for weakly-supervised
    temporal activity localization,” *arXiv preprint arXiv:2007.06643*, 2020.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Z. Luo, D. Guillory, B. Shi, W. Ke, F. Wan, T. Darrell, and H. Xu, “Weakly-supervised
    action localization with expectation-maximization multi-instance learning,” *arXiv
    preprint arXiv:2004.00163*, 2020.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] P. Lee, J. Wang, Y. Lu, and H. Byun, “Background modeling via uncertainty
    estimation for weakly-supervised action localization,” *arXiv preprint arXiv:2006.07006*,
    2020.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Ma, S. K. Gorti, M. Volkovs, and G. Yu, “Weakly supervised action
    selection learning in video,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2021, pp. 7587–7596.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] L. Huang, Y. Huang, W. Ouyang, and L. Wang, “Modeling sub-actions for
    weakly supervised temporal action localization,” *IEEE Transactions on Image Processing*,
    vol. 30, pp. 5154–5167, 2021.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] X. Ding, N. Wang, X. Gao, J. Li, X. Wang, and T. Liu, “Weakly supervised
    temporal action localization with segment-level labels,” *arXiv preprint arXiv:2007.01598*,
    2020.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] C. Zhang, M. Cao, D. Yang, J. Chen, and Y. Zou, “Cola: Weakly-supervised
    temporal action localization with snippet contrastive learning,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 16 010–16 019.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Z. Liu, L. Wang, Q. Zhang, W. Tang, J. Yuan, N. Zheng, and G. Hua, “Acsnet:
    Action-context separation network for weakly supervised temporal action localization,”
    *arXiv preprint arXiv:2103.15088*, 2021.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] P. Lee, J. Wang, Y. Lu, and H. Byun, “Weakly-supervised temporal action
    localization by uncertainty modeling,” *arXiv preprint arXiv:2006.07006*, 2020.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] S. Qu, G. Chen, Z. Li, L. Zhang, F. Lu, and A. Knoll, “Acm-net: Action
    context modeling network for weakly-supervised temporal action localization,”
    *arXiv preprint arXiv:2104.02967*, 2021.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. Narayan, H. Cholakkal, M. Hayat, F. S. Khan, M.-H. Yang, and L. Shao,
    “D2-net: Weakly-supervised action localization via discriminative embeddings and
    denoised activations,” *arXiv preprint arXiv:2012.06440*, 2020.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] X. Lin, Z. Shou, and S.-F. Chang, “Towards train-test consistency for
    semi-supervised temporal action localization,” *arXiv preprint arXiv:1910.11285*,
    2019.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] G. Gong, L. Zheng, W. Jiang, and Y. Mu, “Self-supervised video action
    localization with adversarial temporal transforms.”'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele, “A database for fine
    grained activity detection of cooking activities,” in *2012 IEEE Conference on
    Computer Vision and Pattern Recognition*.   IEEE, 2012, pp. 1194–1201.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] R. J. Nadolski, P. A. Kirschner, and J. J. Van Merriënboer, “Optimizing
    the number of steps in learning tasks for complex skills,” *British Journal of
    Educational Psychology*, vol. 75, no. 2, pp. 223–237, 2005.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] D. Damen, H. Doughty, G. Maria Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *et al.*, “Scaling egocentric vision:
    The epic-kitchens dataset,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 720–736.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] J.-B. Alayrac, P. Bojanowski, N. Agrawal, J. Sivic, I. Laptev, and S. Lacoste-Julien,
    “Unsupervised learning from narrated instruction videos,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2016, pp. 4575–4583.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] V. Chandola, A. Banerjee, and V. Kumar, “Outlier detection: A survey,”
    *ACM Computing Surveys*, vol. 14, p. 15, 2007.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] R. Chalapathy and S. Chawla, “Deep learning for anomaly detection: A
    survey,” *arXiv preprint arXiv:1901.03407*, 2019.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] W. Li, V. Mahadevan, and N. Vasconcelos, “Anomaly detection and localization
    in crowded scenes,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 36, no. 1, pp. 18–32, 2013.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Y. Zhu, N. M. Nayak, and A. K. Roy-Chowdhury, “Context-aware activity
    recognition and anomaly detection in video,” *IEEE Journal of Selected Topics
    in Signal Processing*, vol. 7, no. 1, pp. 91–101, 2012.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] W. Sultani, C. Chen, and M. Shah, “Real-world anomaly detection in surveillance
    videos,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2018, pp. 6479–6488.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] C. He, J. Shao, and J. Sun, “An anomaly-introduced learning method for
    abnormal event detection,” *Multimedia Tools and Applications*, vol. 77, no. 22,
    pp. 29 573–29 588, 2018.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] M. Sabokrou, M. Fayyaz, M. Fathy, Z. Moayed, and R. Klette, “Deep-anomaly:
    Fully convolutional neural network for fast anomaly detection in crowded scenes,”
    *Computer Vision and Image Understanding*, vol. 172, pp. 88–97, 2018.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] M. Sabokrou, M. Fayyaz, M. Fathy, and R. Klette, “Deep-cascade: Cascading
    3d deep neural networks for fast anomaly detection and localization in crowded
    scenes,” *IEEE Transactions on Image Processing*, vol. 26, no. 4, pp. 1992–2004,
    2017.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] M. Sabokrou, M. Pourreza, M. Fayyaz, R. Entezari, M. Fathy, J. Gall,
    and E. Adeli, “Avid: Adversarial visual irregularity detection,” in *Asian Conference
    on Computer Vision*.   Springer, 2018, pp. 488–505.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] K. Liu and H. Ma, “Exploring background-bias for anomaly detection in
    surveillance videos,” in *Proceedings of the 27th ACM International Conference
    on Multimedia*, 2019, pp. 1490–1499.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] M. Xu, M. Gao, Y.-T. Chen, L. S. Davis, and D. J. Crandall, “Temporal
    recurrent networks for online action detection,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2019, pp. 5532–5541.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] V. Bettadapura, C. Pantofaru, and I. Essa, “Leveraging contextual cues
    for generating basketball highlights,” in *Proceedings of the 24th ACM international
    conference on Multimedia*, 2016, pp. 908–917.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] F. C. Heilbron, W. Barrios, V. Escorcia, and B. Ghanem, “Scc: Semantic
    context cascade for efficient action detection,” in *2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*.   IEEE, 2017, pp. 3175–3184.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] P. Felsen, P. Agrawal, and J. Malik, “What will happen next? forecasting
    player moves in sports videos,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 3342–3351.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] R. Kapela, K. McGuinness, A. Swietlicka, and N. E. O’Connor, “Real-time
    event detection in field sport videos,” in *Computer vision in Sports*.   Springer,
    2014, pp. 293–316.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] A. Cioppa, A. Deliege, and M. Van Droogenbroeck, “A bottom-up approach
    based on semantics for the interpretation of the main camera stream in soccer
    games,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition Workshops*, 2018, pp. 1765–1774.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] T. Tsunoda, Y. Komori, M. Matsugu, and T. Harada, “Football action recognition
    using hierarchical lstm,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition workshops*, 2017, pp. 99–107.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Z. Cai, H. Neher, K. Vats, D. A. Clausi, and J. Zelek, “Temporal hockey
    action recognition via pose and optical flows,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops*, 2019, pp. 0–0.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] M. Sanabria, F. Precioso, and T. Menguy, “A deep architecture for multimodal
    summarization of soccer games,” in *Proceedings Proceedings of the 2nd International
    Workshop on Multimedia Content Analysis in Sports*, 2019, pp. 16–24.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] P. Shukla, H. Sadana, A. Bansal, D. Verma, C. Elmadjian, B. Raman, and
    M. Turk, “Automatic cricket highlight generation using event-driven and excitement-based
    features,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition Workshops*, 2018, pp. 1800–1808.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] G. Tsagkatakis, M. Jaber, and P. Tsakalides, “Goal!! event detection
    in sports video,” *Electronic Imaging*, vol. 2017, no. 16, pp. 15–20, 2017.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] F. Turchini, L. Seidenari, L. Galteri, A. Ferracani, G. Becchi, and A. Del Bimbo,
    “Flexible automatic football filming and summarization,” in *Proceedings Proceedings
    of the 2nd International Workshop on Multimedia Content Analysis in Sports*, 2019,
    pp. 108–114.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] S. Giancola, M. Amine, T. Dghaily, and B. Ghanem, “Soccernet: A scalable
    dataset for action spotting in soccer videos,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops*, 2018, pp. 1711–1721.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] C.-L. Huang, H.-C. Shih, and C.-Y. Chao, “Semantic analysis of soccer
    video using dynamic bayesian network,” *IEEE Transactions on Multimedia*, vol. 8,
    no. 4, pp. 749–760, 2006.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] V. Fontana, G. Singh, S. Akrigg, M. Di Maio, S. Saha, and F. Cuzzolin,
    “Action detection from a robot-car perspective,” *arXiv preprint arXiv:1807.11332*,
    2018.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Y. Yao, X. Wang, M. Xu, Z. Pu, E. Atkins, and D. Crandall, “When, where,
    and what? a new dataset for anomaly detection in driving videos,” *arXiv preprint
    arXiv:2004.03044*, 2020.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] K. Mahadevan, E. Sanoubari, S. Somanath, J. E. Young, and E. Sharlin,
    “Av-pedestrian interaction design using a pedestrian mixed traffic simulator,”
    in *Proceedings of the 2019 on Designing Interactive Systems Conference*, 2019,
    pp. 475–486.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] D. Cao, L. Xu, and H. Chen, “Action recognition in untrimmed videos with
    composite self-attention two-stream framework,” in *Asian Conference on Pattern
    Recognition*.   Springer, 2019, pp. 27–40.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] H. Shi, X. Zhang, and C. Li, “Weakly-supervised action recognition and
    localization via knowledge transfer,” in *Chinese Conference on Pattern Recognition
    and Computer Vision (PRCV)*.   Springer, 2019, pp. 205–216.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] L. Zhang, X. Chang, J. Liu, M. Luo, S. Wang, Z. Ge, and A. Hauptmann,
    “Zstad: Zero-shot temporal activity detection,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 879–888.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] H. Xu, B. Kang, X. Sun, J. Feng, K. Saenko, and T. Darrell, “Similarity
    r-c3d for few-shot temporal activity detection,” *arXiv preprint arXiv:1812.10000*,
    2018.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] H. Xu, X. Sun, E. Tzeng, A. Das, K. Saenko, and T. Darrell, “Revisiting
    few-shot activity detection with class similarity control,” *arXiv preprint arXiv:2004.00137*,
    2020.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] H. Yang, X. He, and F. Porikli, “One-shot action localization by learning
    sequence matching network,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018, pp. 1450–1459.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Y. Huang, Q. Dai, and Y. Lu, “Decoupling localization and classification
    in single shot temporal action detection,” in *2019 IEEE International Conference
    on Multimedia and Expo (ICME)*.   IEEE, 2019, pp. 1288–1293.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] D. Zhang, X. Dai, and Y.-F. Wang, “Metal: Minimum effort temporal activity
    localization in untrimmed videos,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 3882–3892.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
