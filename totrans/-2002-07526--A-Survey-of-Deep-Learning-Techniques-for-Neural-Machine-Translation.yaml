- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:02:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2002.07526] A Survey of Deep Learning Techniques for Neural Machine Translation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.07526](https://ar5iv.labs.arxiv.org/html/2002.07526)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning Techniques for Neural Machine Translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shuoheng Yang, Yuxin Wang, Xiaowen Chu Department of Computer science
  prefs: []
  type: TYPE_NORMAL
- en: Hong Kong Baptist University Hong Kong, China
  prefs: []
  type: TYPE_NORMAL
- en: yshuoheng@gmail.com, {yxwang, chxw}@comp.hkbu.edu.hk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In recent years, natural language processing (NLP) has got great development
    with deep learning techniques. In the sub-field of machine translation, a new
    approach named Neural Machine Translation (NMT) has emerged and got massive attention
    from both academia and industry. However, with a significant number of researches
    proposed in the past several years, there is little work in investigating the
    development process of this new technology trend. This literature survey traces
    back the origin and principal development timeline of NMT, investigates the important
    branches, categorizes different research orientations, and discusses some future
    research trends in this field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Neural Machine Translation, Deep Learning, Attention Mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I-A Introduction of Machine Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine translation (MT) is a classic sub-field in NLP that investigates how
    to use computer software to translate the text or speech from one language to
    another without human involvement. Since MT task has a similar objective with
    the final target of NLP and AI, i.e., to fully understand the human text (speech)
    at semantic level, it has received great attention in recent years. Besides the
    scientific value, MT also has huge potential of saving labor cost in many practical
    applications, such as scholarly communication and international business negotiation.
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation task has a long research history with many efficient methods
    proposed in the past decades. Recently, with the development of Deep Learning,
    a new kind of method called Neural Machine Translation (NMT) has emerged. Compared
    with the conventional method like Phrase-Based Statistical Machine Translation
    (PBSMT), NMT takes advantages in its simple architecture and ability in capturing
    long dependency in the sentence, which indicates a huge potential in becoming
    a new trend of the mainstream. After a primitive model in origin, there are a
    variety of NMT models being proposed, some of which have achieved great progresses
    with the state-of-the-art result. This paper summarizes the major branches and
    recent progresses in NMT and discusses the future trend in this field.
  prefs: []
  type: TYPE_NORMAL
- en: I-B Related Work and Our Contribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although there is little work in the literature survey of NMT, some other works
    are highly related. Lipton et al. have summarized the conventional methods in
    sequence learning[[120](#bib.bib120)], which provided essential information for
    the origin of NMT as well as the related base knowledge. Britz et al. and Tobias
    Domhan have done some model comparison work in NMT with experiment and evaluation
    in the practical performance of some wildly accepted technologies, but they have
    rare theoretical analysis, especially in presenting the relationship between different
    proposed models[[22](#bib.bib22)][[118](#bib.bib118)]. On the other hand, some
    researchers limited their survey work on a special part related to NMT — the Attention
    Mechanism, but both of them have a general scope that oriented to all kinds of
    AI tasks with Attention [[116](#bib.bib116)][[117](#bib.bib117)]. Maybe the most
    related work was an earlier doctoral thesis written by Minh-Thang Luong in 2016[[156](#bib.bib156)],
    which included a comprehensive description about the original structure of NMT
    as well as some wildly applied tips.
  prefs: []
  type: TYPE_NORMAL
- en: This paper, however, focuses on a direct and up-to-date literature survey about
    NMT. We have investigated a lot about the relevant literature in this new trend
    and provided comprehensive interpretation for current mainstream technology in
    NMT.
  prefs: []
  type: TYPE_NORMAL
- en: As for concrete components, this literature survey investigates the origin and
    recent progresses in NMT, categorizes these models by their different orientation
    in the model structure. Then we demonstrate the insight of these NMT types, summarize
    the strengths and weaknesses by reviewing their design principal and corresponding
    performance analysis in translation quality and speed. We also give a comprehensive
    overview of two components in NMT development, namely attention mechanism and
    vocabulary coverage mechanism, both of which are indispensable for current achievement.
    At last, we give concentration on some literature which proposed advanced models
    with comparison work; we introduce these considerable models as well as the potential
    direction in future work.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the survey scope, some subareas of NMT with less attention were deliberately
    left out of the scope except with brief description in future trend. These include
    but are not limited to the literature of robustness of NMT, domain adaptation
    in NMT and other applications that embed NMT method (such as speech translation,
    document translation). Although the research scope has been specifically designed,
    due to the numerous of researches and the inevitable expert selection bias, we
    believe that our work is merely a snapshot of part of current research rather
    than all of them. We are hoping that our work could provide convenience for further
    research.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining of the paper is organized as follows. Section [II](#S2 "II History
    of Machine Translation ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation") provides an introduction of machine translation and presents its
    history of development. Section [III](#S3 "III DNN based NMT ‣ A Survey of Deep
    Learning Techniques for Neural Machine Translation") introduces the structure
    of NMT and the procedure of training and testing. Section [IV](#S4 "IV NMT with
    Attention Mechanism ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation") discusses attention mechanism, an essential innovation in the development
    of NMT. Section [V](#S5 "V Vocabulary Coverage Mechanism ‣ A Survey of Deep Learning
    Techniques for Neural Machine Translation") surveys a variety of methods in handling
    word coverage problem and some fluent divisions. Section [VI](#S6 "VI Advanced
    models ‣ A Survey of Deep Learning Techniques for Neural Machine Translation")
    describes three advanced models in NMT. Finally, Section [VII](#S7 "VII Future
    trend ‣ A Survey of Deep Learning Techniques for Neural Machine Translation")
    discusses the future trend in this field.
  prefs: []
  type: TYPE_NORMAL
- en: II History of Machine Translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine translation (MT) has a long history; the origin of this field could
    be traced back to the 17th century. In 1629, René Descartes came up with a universal
    language that expressed the same meaning in different languages and shared one
    symbol.
  prefs: []
  type: TYPE_NORMAL
- en: The specific research of MT began at about 1950s, when the first researcher
    in the field, Yehoshua Bar-Hillel, began his research at MIT (1951) and organized
    the first International Conference on Machine Translation in 1952\. Since then,
    MT has experienced three primary waves in its development, the Rule-based Machine
    Translation[[2](#bib.bib2)], the Statistical Machine Translation[[3](#bib.bib3)][[4](#bib.bib4)],
    and the Neural Machine Translation[[7](#bib.bib7)]. We briefly review the development
    of these three stages in the following.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Development of Machine Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: II-A1 Rule-based Machine Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rule-based Machine Translation is the first design in MT, which is based on
    the hypothesis that all different languages have its symbol in representing the
    same meaning. Because in usual, a word in one language could find its corresponding
    word in another language with the same meaning.
  prefs: []
  type: TYPE_NORMAL
- en: In this method, the translation process could be treated as the word replacement
    in the source sentence. In terms of ’rule-based’, since different languages could
    represent the same meaning of sentence in different word order, the word replacement
    method should base on the syntax rules of both two languages. Thus every word
    in the source sentence should take its corresponding position in the target language.
  prefs: []
  type: TYPE_NORMAL
- en: The rule-based method has a beautiful theory but hardly achieves satisfactory
    performance in implementation. This is because of the computational inefficiency
    in determining the adaptive rule of one sentence. Besides, grammar rules are also
    hard to be organized, since linguists summarize the grammar rules, and there are
    too many syntax rules in one language (especially language with more relaxed grammar
    rules). It is even possible that two syntax rules conflict with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most severe drawback of rule-based method is that it has ignored the need
    of context information in the translation process, which destroys the robustness
    of rule-based machine translation. One famous example was given by Marvin Minsky
    in 1966, where he used two sentences given below:'
  prefs: []
  type: TYPE_NORMAL
- en: “$The$ $pen$ $is$ $in$ $the$ $box$”
  prefs: []
  type: TYPE_NORMAL
- en: “$The$ $box$ $is$ $in$ $the$ $pen$”
  prefs: []
  type: TYPE_NORMAL
- en: Both sentences have the same syntax structure. The first sentence is easy to
    understand; but the second one is more confusing, since the word “pen” is a polysemant,
    which also means “fence” in English. But it is difficult for the computer to translate
    the “pen” to that meaning; the word replacement is thus an unsuccessful method.
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Statistical Machine Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Statistical Machine Translation (SMT) has been the mainstream technology for
    the past 20 years. It has been successfully applied in the industry, including
    Google translation, Baidu translation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Different from Rule-based machine translation, SMT tackles the translation task
    from a statistical perspective. Concretely, the SMT model finds the words (or
    phrases) which have the same meaning through bilingual corpus by statistics. Given
    one sentence, SMT divides it into several sub-sentences, then every part could
    be replaced by target word or phrase.
  prefs: []
  type: TYPE_NORMAL
- en: The most prevalent version of SMT is Phrase-based SMT (PBSMT), which in general
    includes pre-processing, sentence alignment, word alignment, phrase extraction,
    phrase feature preparation, and language model training. The key component of
    a PBSMT model is a phrase-based lexicon, which pairs phrases in the source language
    with phrases in the target language. The lexicon is built from the training data
    set which is a bilingual corpus. By using phrases in this translation, the translation
    model could utilize the context information within phrases. Thus PBSMT could outperform
    the simple word-to-word translation methods.
  prefs: []
  type: TYPE_NORMAL
- en: II-A3 Neural Machine Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It has been a long time since the first try on MT task by neural network[[44](#bib.bib44)][[43](#bib.bib43)].
    Because of the poor performance in the early period and the computing hardware
    limitation, related research in translation by neural network has been ignored
    for many years.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the proliferation of Deep Learning in 2010, more and more NLP tasks have
    achieved great improvement. Using deep neural networks for MT task has received
    great attention as well. A successful DNN based Machine Translation (NMT) model
    was first proposed by Kalchbrenner and Blunsom[[8](#bib.bib8)], which is a totally
    new concept for MT by that time. Comparing with other models, the NMT model needs
    less linguistic knowledge but can produce a competitive performance. Since then,
    many researchers have reported that NMT can perform much better than the traditional
    SMT model[[1](#bib.bib1)][[112](#bib.bib112)][[113](#bib.bib113)][[114](#bib.bib114)][[115](#bib.bib115)],
    and it has also been massively applied to the industrial field[[24](#bib.bib24)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb41171e44e7a35ee1510d576dc2bb6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The training process of RNN based NMT. The symbol $<EOS>$ means end
    of sequence. The embedding layer is for pre-processing. The two RNN layers are
    used to represent the sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B Introduction of Neural Machine Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: II-B1 Motivation of NMT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The inspiration for neural machine translation comes from two aspects: the
    success of Deep Learning in other NLP tasks as we mentioned, and the unresolved
    problems in the development of MT itself.'
  prefs: []
  type: TYPE_NORMAL
- en: For the first reason, in many NLP tasks, traditional Machine Learning method
    is highly dependent on hand-crafted features that often come from linguistic intuition,
    which is definitely an empirical trial-and-error process[[133](#bib.bib133)][[134](#bib.bib134)]
    and is often far more incomplete in representing the nature of original data.
    For example, the context size of training language model is assigned by researchers
    with strong assumption in context relation[[136](#bib.bib136)]; and in text representation
    method, the classic bag-of-words (BOW) method has ignored the influence of word
    order[[135](#bib.bib135)]. However, when applying deep neural network (DNN) in
    the aforementioned tasks, the DNN requires minimum domain-knowledge and avoids
    some pre-processing steps in human feature engineering [[22](#bib.bib22)].
  prefs: []
  type: TYPE_NORMAL
- en: DNN is a powerful neural network which has achieved excellent performance in
    many complex learning tasks which are traditionally considered difficult[[137](#bib.bib137)][[138](#bib.bib138)].
    In NLP field, DNN has been applied in some traditional tasks, for example, speech
    recognition [[5](#bib.bib5)] and Named Entity Recognition (NER) [[133](#bib.bib133)].
    With the exceptional performance they got, DNN-based models have found many potential
    applications in other NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For the second reason, in MT field, PBSMT has got a pretty good performance
    in the past decades, but there are still some inherent weaknesses which require
    further improvement. First, since the PBSMT generates the translation by segmenting
    the source sentence into several phrases and doing phrase replacement, it may
    ignore the long dependency beyond the length of phrases and thus cause inconsistency
    in translation results such as incorrect gender agreements. Second, there are
    generally many intricate sub-components in current systems [[13](#bib.bib13)][[14](#bib.bib14)][[15](#bib.bib15)],
    e.g., language model, reordering model, length/unknown penalties, etc. With the
    increasing number of these sub-components, it is hard to fine-tune and combine
    each other to get a more stable result [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: 'All the above discussions have indicated the bottleneck in the development
    of SMT miniature. Specifically, this bottleneck mainly comes from the language
    model (LM). This is because, in MT task, language model actually can give the
    most important information: the emergence probability of a particular word (or
    phrase) that is conditioned on previous words. So building a better LM can definitely
    improve the translation performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The vast majority of conventional LM is based on the Markov assumption:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p\left(x_{1},x_{2},\ldots,x_{T}\right)$ | $\displaystyle=\prod_{t=1}^{T}p\left(x_{t}&#124;x_{1},\ldots,x_{t-1}\right)$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\approx\prod_{t=1}^{T}p\left(x_{t}&#124;x_{t-n},\ldots,x_{t-1}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $x_{1},x_{2},...,x_{T}$ is a sequence of words in a sentence and $T$ represents
    the length of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: In this assumption, the probability of the sentence is equal to the multiplication
    of probability of each word. $n$ is the total number of words that is chosen to
    simplify the model, which is also referred to as $context\ window$.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the dependency of words that exceed $n$ would be ignored, which implies
    that the conventional LM performs poorly on modeling long dependency. Moreover,
    since the experimental result has indicated that a modest context size (generally
    4-6 words) can be accepted, the first problem of traditional LM is the limited
    representation ability.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, the data sparsity for training has always been the problem that hinders
    an LM built with a larger size of context window. This is because the number of
    $n$-tuples for counting is exponential in $n$. In other words, when building an
    LM, with the increment of the number of order, the number of training samples
    we need would also increase remarkably, which is also referred to as ”curse of
    dimensionality”. For example, if one LM has the order of 5 with a vocabulary size
    of 10,000, then the possible combination of words for statistics should be about
    $10^{25}$, which requires enormous training data. And since most of these combinations
    have not been observed before, subsequent researches have used various trade-off
    and smoothing method to alleviate the sparsity problem[[129](#bib.bib129)][[128](#bib.bib128)][[127](#bib.bib127)][[130](#bib.bib130)][[131](#bib.bib131)].
  prefs: []
  type: TYPE_NORMAL
- en: While further research of the aforementioned LM with statistical method has
    become almost stagnant, Neural Language Model (NLM)[[6](#bib.bib6)], on the other
    hand, uses a neural network to build a language model that models text data directly.
    In initial stage, NLM used fixed-length of a feature vector to represent each
    word, and then the solid number of word vectors would concatenate together as
    a semantic metric to represent the context [[6](#bib.bib6)] [[38](#bib.bib38)][[39](#bib.bib39)],
    which is very similar to the $context\ window$. This work was enhanced later by
    injecting additional context information from source sentence[[12](#bib.bib12)][[132](#bib.bib132)][[126](#bib.bib126)].
    Comparing with the traditional LM, the original NLM alleviates the sample sparsity
    due to the distributed representation of the word, which enables them to share
    the statistical weights rather than being independent variables. And since words
    with similar meaning may occur in the similar context, the corresponding feature
    vector would have the similar value, which indicates that the semantic relation
    of words has been ”embedded” into the feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: New proposals in the next stage solve the long dependency problem by using Recurrent
    Neural Network (RNN). RNN based NLM (RNLM) models the whole sentence by reading
    each word once a time-step, thus it can model the true conditional probability
    without limitation of context size[[41](#bib.bib41)]. Before the emergence of
    NMT, the RNLM, as mentioned earlier, outperformed the conventional LM in the evaluation
    of text perplexity and brought better performance in many practical tasks[[41](#bib.bib41)][[26](#bib.bib26)].
  prefs: []
  type: TYPE_NORMAL
- en: The direct application of NLM in SMT has been naturally proposed [[12](#bib.bib12)][[36](#bib.bib36)][[37](#bib.bib37)][[40](#bib.bib40)][[58](#bib.bib58)],
    and the preliminary experiment indicated promising results. The potential of NLM
    motivates further exploration for a complete DNN based translation model. Subsequently,
    a more ”pure” model with the only neural network has emerged, with the DNN architecture
    that learns to do the translation task end-to-end. Section [III](#S3 "III DNN
    based NMT ‣ A Survey of Deep Learning Techniques for Neural Machine Translation")
    demonstrates its basic structure (in Fig. [4](#S3.F4 "Figure 4 ‣ III-D1 General
    decoding work flow (greedy) ‣ III-D Inference method ‣ III DNN based NMT ‣ A Survey
    of Deep Learning Techniques for Neural Machine Translation")), as well as its
    concrete details.
  prefs: []
  type: TYPE_NORMAL
- en: II-B2 Formulation of NMT Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Currently, NMT task is originally designed as an end-to-end learning task.
    It directly processes a source sequence to a target sequence. The learning objective
    is to find the correct target sequence given the source sequence, which can be
    seen as a high dimensional classification problem that tries to map the two sentences
    in the semantic space. In all mainstreams of modern NMT model, this process can
    be divided into two steps: encoding and decoding, and thus can functionally separate
    the whole model as Encoder and Decoder as illustrated in Fig. [2](#S2.F2 "Figure
    2 ‣ II-B2 Formulation of NMT Task ‣ II-B Introduction of Neural Machine Translation
    ‣ II History of Machine Translation ‣ A Survey of Deep Learning Techniques for
    Neural Machine Translation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8306bcbd1031610a0b846b0292722b2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: End-to-End structure in modern NMT model. The encoder is used to
    represent the source sentence to semantic vector, while the decoder makes prediction
    from this semantic vector to a target sentence. End-to-End means the model processes
    source data to target data directly, without explicable intermediate result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In perspective of probability, NMT generates the target sequence $T(t_{1},t_{2},...,t_{m})$
    from the max conditional probability given the source sequence $S(s_{1},s_{2},...,s_{n})$,
    where $n$ is the length of sequence $S$ and $m$ is the length of target sequence
    $T$. The whole task could be formulated as[[24](#bib.bib24)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $argmax\ P(T&#124;S).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'More concretely, when generating each word of the target sentence, it uses
    the information from both the word it predicted previously and the source sentence.
    In that case, each generating step could be described as when generating the $i$-th
    word:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $argmax\prod\limits_{i=1}^{m}P(t_{i}&#124;t_{j<i},S)$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Based on this formula and the discussion of NLM above, NMT task could be regarded
    as an NLM model with additional constraints (e.g., conditioned on a given source
    sequence).
  prefs: []
  type: TYPE_NORMAL
- en: II-C The Recent Development in NMT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We devide the recent developement of NMT in five main stages: (a) the original
    NMT with a shallow layer, (b) SMT assisted by NLM, (c) the DNN based NMT, (d)
    NMT with attention mechanism, (e) the attention-based NMT.'
  prefs: []
  type: TYPE_NORMAL
- en: NMT with Shallow Layer
  prefs: []
  type: TYPE_NORMAL
- en: Even before the Deep Learning, Allen has used binary encoding to train an NMT
    model in 1987[[44](#bib.bib44)]. Later in 1991, Chrisman used Dual-ported RAAM
    architecture[[42](#bib.bib42)] to build an original NMT model[[43](#bib.bib43)].
    Although both of them have a pretty primitive design with the limited result when
    looking back, their work has indicated the original idea of this field. The further
    related work has almost stagnated in the following decades, due to the huge progress
    that SMT method acquired at that period, as well as the limited computing power
    and data samples.
  prefs: []
  type: TYPE_NORMAL
- en: SMT assisted by NLM
  prefs: []
  type: TYPE_NORMAL
- en: Based on the above discussion, NLM has revolutionized the traditional LM even
    before the rise of deep learning. Later on, deep RNN based NLM has been applied
    in the SMT system. Cho et al. proposed an SMT model along with an NLM model[[18](#bib.bib18)].
    Although the main body is still SMT, this hybrid method provides a new direction
    for the emergence of a pure deep learning-based NMT.
  prefs: []
  type: TYPE_NORMAL
- en: NMT with Deep Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: Since the traditional SMT model with NLM has got the state-of-the-art performance
    at that time, a pure DNN based translation approach was proposed later with an
    end-to-end design to model the entire MT process[[8](#bib.bib8)][[16](#bib.bib16)].
    Using DNN based NMT could capture subtle irregularities in both two languages
    more efficiently [[24](#bib.bib24)], which is similar to the observation that
    DNNs often have a better performance than ’shallow’ neural networks [[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: NMT with Attention Mechanism
  prefs: []
  type: TYPE_NORMAL
- en: Although the initial DNN based NMT model has not outperformed the SMT completely,
    it still exhibited a huge potential for further research. When tracing back the
    major weakness, although one theoretical advantage of RNN is its ability in capturing
    the long dependency between words, in fact, the model performance would deteriorate
    with the increase of sentence length. This scenario is due to the limited feature
    representation ability in a fixed-length vector. Under the circumstances, since
    the original NMT has got a pretty good performance without any auxiliary, the
    idea of whether some variants in architecture could bring a breakthrough has led
    to the rise of Attention Mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Mechanism was originally proposed by Bahdanau et al. as an intermediate
    component[[21](#bib.bib21)], and the objective is to provide additional word alignment
    information in translating the long sentence. Surprisingly, NMT model has got
    a considerable improvement with the help of this simple method. Later on, with
    tremendous popularity among both academia and industry, many refinements in Attention
    Mechanism have emerged, and more details will be discussed in Section [IV](#S4
    "IV NMT with Attention Mechanism ‣ A Survey of Deep Learning Techniques for Neural
    Machine Translation").
  prefs: []
  type: TYPE_NORMAL
- en: Fully Attention based NMT
  prefs: []
  type: TYPE_NORMAL
- en: With the development of Attention Mechanism, fully Attention-based NMT has emerged
    as a great innovation in NMT history. In this new tendency, Attention mechanism
    has taken the dominate position in text feature extraction rather than a auxiliary
    component. And the representative model is Transformer [[25](#bib.bib25)], which
    is a fully attention-based model proposed by Vaswani et al.
  prefs: []
  type: TYPE_NORMAL
- en: Abandoning previous framework in neither RNN nor CNN based NMT models, Transformer
    is a that solely based on an intensified version of Attention Mechanism called
    Self-Attention with feed-forward connection, which got revolutionary progress
    in structure with state-of-the-art performance. Specifically, the innovative attention
    structure is the secret sauce to gain such significant improvement. The self-attention
    is a powerful feature extractor which also allows to ’read’ the entire sentence
    and model it once a time. In the perspective of model architecture, this character
    can be seen as a combination of advantages from both CNN and RNN, which endows
    it a good feature representation ability with high inference speed. More details
    about self-attention will be given in Section [IV](#S4 "IV NMT with Attention
    Mechanism ‣ A Survey of Deep Learning Techniques for Neural Machine Translation").
    The architecture of Transformer will be discussed in Section [VI](#S6 "VI Advanced
    models ‣ A Survey of Deep Learning Techniques for Neural Machine Translation").
  prefs: []
  type: TYPE_NORMAL
- en: III DNN based NMT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The emergence of DNN based NLM indicates the feasibility of building a pure
    DNN based translation model. The further implementation is the $defacto$ form
    of NMT in origin. This section reviews the basic concept of DNN based NMT, demonstrates
    a comprehensive introduction of the standard structure of the original DNN based
    NMT, and discusses the training and inferencing processes.
  prefs: []
  type: TYPE_NORMAL
- en: III-A Model Design of DNN based NMT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many variations of network design for NMT, which can be categorized
    into $recurrent$ or $non-recurrent$ models. More specifically, this category can
    be traced back to the early development of NMT, when RNN and CNN based models
    are the most common design. Many sophisticated models proposed afterwards also
    belong to either CNN or RNN family. This sub-section follows the development of
    NMT in the early years, and demonstrates some representative models by classifying
    them as RNN or CNN based models.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 RNN based NMT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although in theory, any network with enough feature extraction ability could
    be selected to build an NMT model, in $defacto$ implementations, RNN based NMT
    models have taken the dominant position in NMT development, and they have achieved
    state-of-the-art performance. Based on the discussion in Section [II](#S2 "II
    History of Machine Translation ‣ A Survey of Deep Learning Techniques for Neural
    Machine Translation"), since many NLM literature used RNN to model the sequence
    data, this design has intuitively motivated the further work to build an RNN based
    NMT model. In the initial experiment, an RNN based NLM was applied as a feature
    extractor to compress the source sentence into a feature vector, which is also
    referred to as thought vector. Then a similar RNN was applied to do the ’inverse
    work’ to find the target sentence that can match the previous thought vector in
    semantic space.
  prefs: []
  type: TYPE_NORMAL
- en: The first successful RNN based NMT was proposed by Sutskever et al., who used
    a pure deep RNN model and got a performance that approximates the best result
    achieved by SMT[[16](#bib.bib16)]. Further development proposed the Attention
    Mechanism, which improves the translation performance significantly and exceeds
    the best SMT model. GNMT model was an industry-level model applied in Google Translation,
    and it was regarded as a milestone in RNN based NMT.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the above mentioned work, other researchers have also proposed different
    architectures with excellent performance. Zhang et al. proposed Variational NMT
    method, which has an innovative perspective in modeling translation task, and
    the corresponding experiment has indicated a better performance than the baseline
    of original NMT in Chinese-English and English-German tasks[[89](#bib.bib89)].
    Zhou et al. have designed Fast-Forward Connections for RNN (LSTM), which can allow
    a deeper network in implementation and thus gets a better performance[[88](#bib.bib88)].
    Shazeer et al. incorporated Mixture-of-Expert (MoE) architecture into the GNMT
    model, which has outperformed the original GNMT model[[90](#bib.bib90)]. Concretely,
    MoE is one layer in the NMT model, which contains many sparsely combined experts
    (which are feed-forward neural networks in this experiment) and is connected with
    the RNN layer by a gate function. This method requires more parameters in total
    for the NMT model, but still maintains the efficiency in training speed. Since
    more parameters often imply a better representation ability, it demonstrates huge
    potential in the future.
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 CNN based NMT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Related work in trying other DNN models have also been proposed. Perhaps the
    most noted one is the Convolutional Neural Network (CNN) based NMT. In fact, CNN
    based models have also undergone many variations in its concrete architecture.
    But for a long while, most of these models can’t have competitive performance
    with RNN based model, especially when the Attention Mechanism has emerged.
  prefs: []
  type: TYPE_NORMAL
- en: In the development of CNN based NMT models, Kalchbrenner & Blunsom once tried
    a CNN encoder with RNN Decoder [[8](#bib.bib8)], and it’s maybe the earliest NMT
    architecture applied with CNN. Cho et al. tried a gated recursive CNN encoder
    with RNN decoder, but it has shown worse performance than RNN encoder[[18](#bib.bib18)].
    A fully CNN based NMT was proposed by Kaiser & Bengio later[[86](#bib.bib86)],
    which applied Extended Neural GPU[[119](#bib.bib119)]. The best performance in
    the early period of CNN based NMT was achieved by Gehring et al., which was a
    CNN encoder NMT and got the similar translation performance with RNN based model
    at that time[[19](#bib.bib19)]. Concurrently, Kalchbrenner et al. also proposed
    ByteNet (a kind of CNN) based NMT, which achieved the state-of-the-art performance
    on character-level translation but failed at word-level translation[[84](#bib.bib84)].
    In addition, Meng et al. and Tu et al. proposed a CNN based model separately,
    which provides additional alignment information for SMT [[20](#bib.bib20)][[83](#bib.bib83)].
  prefs: []
  type: TYPE_NORMAL
- en: Compared with RNN based NMT, CNN based models have its advantage in training
    speed; this is due to the intrinsic structure of CNN which allows parallel computations
    for its different filters when handling the input data. And also, the model structure
    has made CNN based models easier to resolve the gradient vanishing problem. However,
    there are two fatal drawbacks that affect their translation quality. First, since
    the original CNN based model can only capture the word dependencies within the
    width of its filters, the long dependency of words can only be found in high-level
    convolution layers; this unnatural character often causes a worse performance
    than the RNN based model. Second, since the original NMT model compresses a sentence
    into a fixed size of the vector, a large performance reduction would happen when
    the sentence becomes too long. This comes from the limited representation ability
    in fixed size of the vector. Similar phenomenon can also be found in early proposed
    RNN based models, which are later alleviated by Attention Mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Some advanced CNN based NMT models have also been proposed with corresponding
    solutions in addressing the above drawbacks. Kaiser et al. proposed the Depthwise
    separable convolutions based NMT. The SliceNet they created can get similar performance
    with Kaiser et al. (2016) [[85](#bib.bib85)]. Gehring et al. (2017) followed their
    previous work by proposing a CNN based NMT that is cooperated with Attention Mechanism.
    It even got a better result than RNN based model[[82](#bib.bib82)], but this achievement
    was soon outperformed by Transformer [[25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B Encoder-Decoder Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As is known, Encoder-Decoder is the most original and classic structure of NMT;
    it was directly inspired by NLM and proposed by Kalchbrenner & Blunsom[[8](#bib.bib8)]
    and Cho et al.[[18](#bib.bib18)]. Despite all kinds of refinements in details
    and small tips, it was wildly accepted by almost all modern NMT models. Based
    on the discussion above, since RNN based NMT has held the dominant position in
    NMT, and to avoid being overwhelmed in describing all kinds of small distinctions
    between models’ structures, we specifically focus our discussion just on the vanilla
    RNN based NMT, thus can help to trace back the development process of NMT.
  prefs: []
  type: TYPE_NORMAL
- en: The original structure of Encoder-Decoder structure is conceptually simple.
    It contains two connected networks (the encoder and the decoder) in its architecture,
    each for a different part of the translation process. When the encoder network
    receives one source sentence, it reads the source sentence word by word and compresses
    the variable-length sequence into a fixed-length vector in each hidden state.
    This process is called encoding. Then given the final hidden state of the encoder
    (referred to as thought vector), the decoder does the reverse work by transforming
    the thought vector to the target sentence word by word. Because Encoder-Decoder
    structure addresses the translation task from source data directly to the target
    result, which means there’s no visible result in the middle process, this is also
    called end-to-end translation. The principle of Encoder-Decoder structure of NMT
    can be seen as mapping the source sentence with the target sentence via an intermediate
    vector in semantic space. This intermediate vector actually can represent the
    same semantic meaning in both two languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'For specific details of this structure, besides the model selection in the
    network, RNN based NMT models also differ in three main terms: (a) the directionality;
    (b) the type of activation function; and (c) the depth of RNN layer[[156](#bib.bib156)].
    In the following, we give a detailed description.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth: For the depth of RNN, as we discussed in Section [II](#S2 "II History
    of Machine Translation ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation"), single layer RNN usually performs poorly comparing with multi-layer
    RNN. In recent years, almost all the models with competitive performance are using
    a deep network, which has indicated a trend of using a deeper model to get the
    state-of-the-art result. For example, Bahdanau et al.[[21](#bib.bib21)] used four
    layers RNN in their model.'
  prefs: []
  type: TYPE_NORMAL
- en: However, simply increasing more layers of RNN may not always be useful. In the
    proof proposed by Britz et al.[[22](#bib.bib22)], they found that using 4 layers
    RNN in the encoder for specific dataset would produce the best performance when
    there is no other auxiliary method in the whole model. Besides that, stacking
    RNN layers may make the network become too slow and difficult to train. One major
    challenges is the gradient exploding and vanishing problem[[28](#bib.bib28)],
    which will cause the gradient be amplified or diminished when processing back
    propagation in deep layers. Besides the additional gate structure in refined RNN
    (like LSTM and GRU), other methods have also been applied to alleviate this phenomenon.
    For example, in Wu et al.’s work, the residual connections are provided between
    layer, which can improve the value of gradient flow in the backward pass, thus
    can speed up the convergence process[[24](#bib.bib24)]. Another possible problem
    is that a deeper model often indicates larger model capacity, which may perform
    worse on comparatively less training data due to the over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Directionality: In respect of directionality, a simple unidirectional RNN has
    been chosen by some researchers. For example, Luong et al. have directly used
    unidirectional RNN to accept the input sentence[[23](#bib.bib23)]. In comparison,
    bidirectional RNN is another common choice that can empower the translation quality.
    This is because the model performance is affected by whether it ’knows’ well about
    the information in context word when predicting current word. A bidirectional
    RNN obviously could strengthen this ability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, both Bahdanau et al. and Wu et al. used bidirectional RNN on the
    bottom layer as an alternative to capture the context information[[21](#bib.bib21)][[24](#bib.bib24)].
    In this structure, the first layer reads the sentence “left to right”, and the
    second layer reads the sentence in a reverse direction. Then they are concatenated
    and fed to the next layer. This method generally has a better performance in experiment,
    although the explanation is intuitive: Based on the discussion of LM in Section [II](#S2
    "II History of Machine Translation ‣ A Survey of Deep Learning Techniques for
    Neural Machine Translation"), the emergence probability of a specific word is
    determined by all the other words in both the prior and the post positions. When
    applying unidirectional RNN, word dependency between the first word and the last
    word is hard to be captured by the thought vector, since the model has experienced
    too many states in all time steps. On the country, bidirectional RNN provides
    an additional layer of information with reverse direction of reading words, which
    could naturally reduce this relative length within steps.'
  prefs: []
  type: TYPE_NORMAL
- en: The most visible drawback of this method is that it’s hard to be paralleled,
    considering the time-consuming in its realization, both Bahdanau et al. and Wu
    et al. choose to apply just one layer bidirectional RNN in the bottom layer of
    the encoder, and other layers are all unidirectional layers[[24](#bib.bib24)][[21](#bib.bib21)].
    This choice makes a trade-off between the feature representation ability with
    model efficiency, due to it can still enable the model to be distributed on multi
    GPUs[[24](#bib.bib24)]. The basic concept of bidirectional RNN could find in Fig. [2](#S2.F2
    "Figure 2 ‣ II-B2 Formulation of NMT Task ‣ II-B Introduction of Neural Machine
    Translation ‣ II History of Machine Translation ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation").
  prefs: []
  type: TYPE_NORMAL
- en: 'Activation Function Selection: In respect of activation function selection,
    there are three common choices: vanilla RNN, Long Short Term Memory (LSTM) [[17](#bib.bib17)],
    and Gated Recurrent Unit (GRU) [[18](#bib.bib18)]. Comparing with the vanilla
    RNN, both the last two have some robustness in addessing the gradient exploding
    and vanishing problem[[27](#bib.bib27)][[28](#bib.bib28)]. Another sequence processing
    task has also indicated better performance achieved by GRU and LSTM[[26](#bib.bib26)].
    Besides, some innovative neural units have been proposed. Wang et al. proposed
    linear associative units, which can alleviate the gradient diffusion phenomenon
    in non-linear recurrent activation[[92](#bib.bib92)]. More recently, Zhang et
    al. have created addition-subtraction twin-gated recurrent network (ATR). This
    type of unit reduces the inefficiency in NMT training and inference by simplifying
    the weight matrices among units [[91](#bib.bib91)]. All in all, in NMT task, LSTM
    is the most common choice.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0be7ef4f7252472453d075e81a1dc11e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The concept of Bidirectional RNN'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Training method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before feeding the training data to the model, one pre-step is to transfer the
    words to vectors, which makes a proper form that the neural network could receive.
    Usually, the most frequent $V$ words in one language will be chosen, and each
    language generally has different word set. Despite that the embedding weights
    will be learned in the training period, the pre-trained word embedding vector
    such as word2vec[[9](#bib.bib9)][[10](#bib.bib10)] or Glove vector[[11](#bib.bib11)]
    can also be applied directly.
  prefs: []
  type: TYPE_NORMAL
- en: In the training period, this model is fed by a bilingual corpus for Encoder
    and Decoder. The learning objective is to map the input sequence with the corresponding
    sequence in the target language correctly. Like other DNN models, the input sentence
    pair is embedded as a list of word vectors, and the model parameters are initialized
    randomly. The training process could be formulated as trying to updating its parameters
    periodically until getting the minimum loss of the neural network. In the implementation,
    RNN will refine the parameters after it processes a subset of data that contain
    a batch of training samples; this subset is called the mini-batch set. To simplify
    the discussion of the training process, we take one sentence pair (one training
    sample) as example.
  prefs: []
  type: TYPE_NORMAL
- en: For the Encoder, the encoding RNN will receive one word in source sentence once
    a time-step. After several steps, all words will be compressed into the hidden
    state of the Encoder. Then the final vector will be transferred to the Decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Decoder, the input comes from two sources: the thought vector that is directly
    sent to Decoder, and the correct word in the last time-step (the first word is
    $<EOS>$). The output process in Decoder can be seen as a reverse work of Encoder;
    Decoder predicts one word in each time-step until the last symbol is $<EOS>$.'
  prefs: []
  type: TYPE_NORMAL
- en: III-D Inference method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the training period, the model could be used for translation, which is
    called inference. The inference procedure is quite similar to the training process.
    Nevertheless, there is still a clear distinction between training and inference:
    at decoding time, we only have access to the source sentence, i.e., encoder hidden
    state.'
  prefs: []
  type: TYPE_NORMAL
- en: There is more than one way to perform decoding. Proposed decoding strategies
    include Sampling and Greedy search, while the latter one is generally accepted
    and be evolved as Beam-search.
  prefs: []
  type: TYPE_NORMAL
- en: III-D1 General decoding work flow (greedy)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The idea of greedy strategy is simple, as we illustrate in Fig. [4](#S3.F4
    "Figure 4 ‣ III-D1 General decoding work flow (greedy) ‣ III-D Inference method
    ‣ III DNN based NMT ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation"). The Greedy strategy is only considering the predicted word with
    the highest probability. In the implementation of our illustration, the previously
    generated word would also be fed to the network together with the thought vector
    in the next time-step. The detailed steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e7be4d0ca1e6b8c6aa9ec0e0bc99fb26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The process of greedy decoding: each time the model would predict
    the word with highest probability, and use the current result as the input in
    next time step to get further prediction'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The model still encodes the source sentence in the same way as during the
    training period to obtain the thought vector, and this thought vector is used
    to initialize the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The decoding (translation) process will start as soon as the decoder receives
    the end-of-sentence marker $<EOS>$ of source sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. For each time-step on the decoder side, we treat the RNN’s output as a set
    of logits. We choose the word with the highest translation probability as the
    emitted word, whose ID is associated with the maximum logit value. For example,
    in Fig. [4](#S3.F4 "Figure 4 ‣ III-D1 General decoding work flow (greedy) ‣ III-D
    Inference method ‣ III DNN based NMT ‣ A Survey of Deep Learning Techniques for
    Neural Machine Translation"), the word “moi” has the highest probability in the
    first decoding step. We then feed this word as an input in the next time-step.
    The probability is thus conditioned on the previous prediction (this is why we
    call it “greedy” behavior).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The process will continue until the ending symbol $<EOS>$ is generated as
    an output symbol.
  prefs: []
  type: TYPE_NORMAL
- en: III-D2 Beam-search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While the Greedy search method has produced a pretty good result, Beam-search
    is a more elaborated one with better results. Although it is not a necessary component
    for NMT, Beam-search has been chosen by most of NMT models to get the best performance[[22](#bib.bib22)].
  prefs: []
  type: TYPE_NORMAL
- en: The beam-search method was proposed by other sequence learning task with successful
    application[[29](#bib.bib29)][[30](#bib.bib30)]. It’s also the conventional technique
    of MT task that has been used for years in finding the most appropriate translation
    result[[34](#bib.bib34)][[32](#bib.bib32)][[33](#bib.bib33)]. Beam-search can
    be simply described as retaining the top-$k$ possible translations as candidates
    at each time, where $k$ is called the beam-width. In the next time-step, each
    candidate word would be combined with a new word to form new possible translation.
    The new candidate translation would then compete with each other in log probability
    to get the new top-$k$ most reasonable results. The whole process continues until
    the end of translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concretely, the beam search can be formulated in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Beam Search
  prefs: []
  type: TYPE_NORMAL
- en: set Beamsize = K;  $h_{0}\Leftarrow encoder(S)$  $t\Leftarrow 1$  //  $L_{S}$
    means length of source sentence;  //  $\alpha$ is Length factor;  while $n\leq\alpha*L_{S}$ do     $y_{1,i}\Leftarrow\
    <EOS>$     while $i\leq K$ do        set $h_{t}\Leftarrow\ decoder(h_{t-1},y_{t,i})$;        set
    $P_{t,i}=Softmax(y_{t,i})$;        set $y_{t+1,i}\Leftarrow argTop\_K(P_{t,i})$;        set
    $i=i+1$     end while     set $i=0$     if $h_{t}==\ <EOS>$ then        break;     end if     set
    $t=t+1$  end while  select $argmax(p(Y))$ from $K$ candidates $Y_{i}$  return
     $Y_{i}$
  prefs: []
  type: TYPE_NORMAL
- en: Besides the standard Beam-search which finds the candidate translation only
    by sorting log probability, this evaluation function mathematically tends to find
    shorter sentence. This is because a negative log-probability would be added at
    each decoding step, which lowers the scores with the increasing length of sentences[[31](#bib.bib31)].
    An efficient variant for alleviating this scenario is to add a length normalization[[7](#bib.bib7)].
    A refined length normalization was also proposed by Wu et al.[[24](#bib.bib24)].
  prefs: []
  type: TYPE_NORMAL
- en: Another kind of refined method in Beam-search is adding coverage penalty, which
    helps to encourage the decoder to cover the words in the source sentence as much
    as possible when generating an output sentence[[24](#bib.bib24)][[35](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, since this method finds $k$ times of translation(rather than one)
    until getting the final result, it generally makes the decoding process more time-consuming.
    In practice, an intuitive solution is to limit the beam-width as a small constant,
    which is a trade-off between the decoding efficiency and the translation accuracy.
    As reported by a comparison work, an experimental beam width for best performance
    is 5 to 10[[22](#bib.bib22)].
  prefs: []
  type: TYPE_NORMAL
- en: IV NMT with Attention Mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IV-A Motivation of Attention Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the promising performance of NMT has indicated its great potential in
    capturing the dependencies inside the sequence, in practice, NMT still suffers
    a huge performance reduction when the source sentence becomes too long. Comparing
    with other feature extractors, the major weakness of the original NMT Encoder
    is that it has to compress one sentence into a fixed-length vector. When the input
    sentence becomes longer, the performance deteriorates because the final output
    of the network is a fixed-length vector, which may have limitation in representing
    the whole sentence and cause some information loss. And because of the limited
    length of vector, this information loss usually covers the long-range dependencies
    of words. While increasing the dimension of encoding vector is an intuitive solution,
    since the RNN training speed is naturally slow, a larger vector size would cause
    an even worse situation.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Mechanism emerged under this circumstance. Bahdanau et al. [[21](#bib.bib21)]
    initially used this method as a supplement that can provide additional word alignment
    information in the decoding process, thus can alleviate the information reduction
    when the input sentence is too long. Concretely, Attention Mechanism is an intermediate
    component between Encoder and Decoder, which can help to determine the word correlation
    (word alignment information) dynamically. In the encoding period, it extends the
    vector of the final state in the original NMT model with a weighted average of
    hidden state in each time state, and a score function is provided to get the weight
    we mention above by calculating the correlation of each word in source sentence
    with the current predicting word. Thus the decoder could adapt its concentration
    in different translation steps by ordering the importance of each word correlation
    in source sentence, and this method can help to capture the long-range dependencies
    for each word respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The inspiration for applying the Attention Mechanism on NMT comes from human
    behavior in reading and translating the text data. People generally read text
    repeatedly for mining the dependency within the sentence, which means each word
    has different dependency weight with each other. Comparing with other models in
    capturing word dependency information such as pooling layer in CNN or N-gram language
    model, attention mechanism has a global scope. When finding the dependency in
    one sequence, $N$-gram model will fix its the searching scope in a small range,
    usually the $N$ is equal to 2 or 3 in practice. Attention Mechanism, on the other
    hand, calculates the dependency between the current generating word with other
    words in source sentence. This more flexible method obviously bring a better result.
  prefs: []
  type: TYPE_NORMAL
- en: The practical application of Attention Mechanism is actually far beyond the
    NMT field, and it is even not an invention in NMT development. Some other tasks
    have also proposed similar methods that give weighted concentration on different
    position of input data, for example, Xu et al[[109](#bib.bib109)]. proposed similar
    mechanism in handling image caption task, which can helps to dynamically locate
    different entries in image feature vector when generating description of them.
    Due to the scope of this survey, the following discussion would only focus on
    the Attention Mechanism in NMT.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Structure of Attention Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many variants in the implementation of Attention Mechanism. Here we
    just give the detailed description of Attention Mechanism which has been widely
    accepted as bringing significant contribution in the development of NMT.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B1 basic structure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The structure of attention mechanism was originally proposed by Bahdanau et
    al. In later, Luong et al. proposed similar structure with small distinctions
    and extends this work[[23](#bib.bib23)][[21](#bib.bib21)].
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the discussion, here we take Luong et al.’s method as an example.
    Concretely, in encoding period, this mechanism receiving the input words like
    the basic NMT model, but instead of compressing all the information in one vector,
    every unit in the top layer of encoder will generate one vector that represents
    one time-step in the source sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a16ae4618dfae61d4816efaf76786abb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The concept of Attention Mechanism,which can provide additional alignment
    information rather than just using information in fixed-length of vector'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the decoding period, the decoder won’t predict the word just use its own
    information. However, it collaborates with the attention layer to get the translation.
    The input of attention mechanism is the hidden states in the top layer of the
    encoder and the current decoder. It gets the relativity order by calculating the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The current decoding hidden state $h_{t}$ will be used to compare with all
    source states $h_{s}$ to derive the attention weights score $s_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The attention weights $a_{t}$ is driven by normalization operation for all
    attention weight score.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Based on the attention weights, we then compute the weighted average of
    the source states as a context vector $c_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Concatenate the context vector with the current decoding hidden state to
    yield the final attention vector(the exact combination method can be different).
  prefs: []
  type: TYPE_NORMAL
- en: '5\. The attention vector is fed as an input to decoder in the next time-step
    (applicable for input feeding). The first three steps can be summarized by the
    equations below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\centering s_{t}=score(h_{t},h_{s})\;\;\;\left[{Attention\ function}\right]\@add@centering$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\centering a_{t}=\frac{exp(s_{t})}{\sum\limits_{s=0}^{S}exp(s_{t})}\qquad\left[{Attention\
    weight}\right]\@add@centering$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\centering c_{t}=\sum\limits_{s}a_{t}h_{s}\;\;\qquad\qquad\qquad\left[{Context\
    vector}\right]\@add@centering$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'Among the above function, The score function could be defined in different
    ways. Here, we two classic definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\centering score(h_{t},h_{s})=\begin{cases}h_{t}^{T}Wh_{s}&amp;[Luong^{\prime}s\
    version]\\ v_{a}^{T}tanh(W_{1}h_{t},h_{s})&amp;[Bahdanau^{\prime}s\ version]\end{cases}\@add@centering$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Back to the decoding period, it receives the information from both two sides,
    the decoder hidden state and the attention vector, given the current two vectors,
    it then predicts the words by alignment them to a new vector, then it usually
    has another layer to predict the current target word.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 Global Attention & Local Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Global Attention
  prefs: []
  type: TYPE_NORMAL
- en: Global Attention is the method of Attention Mechanism we mentioned above, and
    it’s also a fluent type in various of Attention mechanism. The idea of Global
    Attention is also the original form of attention mechanism, though it got this
    name by Luong et al.[[23](#bib.bib23)], the corresponding term is Local Attention.
    The term —— ”global” derives from it calculates the context vector by considering
    the relevance order of all words in the source sentence. This method has excellent
    performance because more alignment information will generally produce a better
    result. A straightforward presentation in Fig. [6](#S4.F6 "Figure 6 ‣ IV-B2 Global
    Attention & Local Attention ‣ IV-B Structure of Attention Mechanism ‣ IV NMT with
    Attention Mechanism ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation")
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f6211c30a3cfe37f8fbdb4663c58e48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The concept of Global attention, current decoder hidden state calculated
    with all the hidden states in source side to get the alignment information.'
  prefs: []
  type: TYPE_NORMAL
- en: As we have introduced in Section [IV](#S4 "IV NMT with Attention Mechanism ‣
    A Survey of Deep Learning Techniques for Neural Machine Translation"), this method
    considers all the source word in the decoding period. The main drawback is calculation
    speed deteriorates when the sequence is very long since one hidden state will
    be generated in one time-step in the Encoder, the cost of score function would
    be linear with the number of time-steps on the Encoder. When the input is a long
    sequence like a compound sentence or a paragraph, it may affect the decoding speed.
  prefs: []
  type: TYPE_NORMAL
- en: Local attention Local attention was first proposed by Luong et al.[[23](#bib.bib23)].
    As illustrated in Fig. [7](#S4.F7 "Figure 7 ‣ IV-B2 Global Attention & Local Attention
    ‣ IV-B Structure of Attention Mechanism ‣ IV NMT with Attention Mechanism ‣ A
    Survey of Deep Learning Techniques for Neural Machine Translation"), this model,
    on the other hand, will just calculate the relevance with a subset of the source
    sentence. Comparing with Global attention, it fixes the length of attention vector
    by giving a scope number, thus avoiding the expensive computation in getting context
    vectors. The experiment result indicated that local attention can keep a balance
    between model performance with computing speed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8adb12dad0128cc07d2665de17e7bd2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The concept of Local attention, current hidden state calculated with
    a subset of all the hidden states in source side.'
  prefs: []
  type: TYPE_NORMAL
- en: The inspiration of local attention comes from the soft attention and hard attention
    in image caption generation task, which was proposed by Xu et al.[[109](#bib.bib109)].
    While the global attention is very similar to soft attention, the local attention,
    on the other hand, can be seen as a mixture method of soft attention with hard
    attention.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, although covering more information would generally get a better result,
    the fantastic result of this method has indicated a comparable performance with
    global attention when it has been fine-tuned. This seems due to the common phenomenon
    in human language —— the current word would naturally have a high dependency with
    some of its nearby words, which is quite similar to the assumption of the n-gram
    language model.
  prefs: []
  type: TYPE_NORMAL
- en: In the details of calculation process, given the current target word’s position
    $p_{t}$, the model fixes the context vector in scope D. The context vector ct
    is then derived as a weighted average over the set of source hidden states within
    the range $[p_{t}-D,p_{t}+D]$; Scope D is selected by experience, and then it
    could be same steps in deriving the attention vector like Global attention.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B3 Input feeding approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Input feeding is a small tip in constructing the NMT structure, but from the
    perspective of providing alignment information, it can also be seen as a kind
    of attention.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of input feeding is simple. In the decoding period, besides using
    the previously predicted words as input, it also uses the attention vector that
    in the previous time-step as additional input in next time-step[[1](#bib.bib1)][[23](#bib.bib23)].
    This attention vectors will concatenate with input vector to get the final input
    vector; then this new vector will be fed as the input in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B4 Attention Mechanism in GNMT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GNMT is short for Google Neural Machine Translation, which is a well-known version
    of NMT with Attention Mechanism. GNMT was proposed by Wu et al.,[[24](#bib.bib24)]
    and famous for its successful application in industrial level NMT system. With
    the help of many kinds of advanced tips in model detail, it got state-of-the-art
    performance at that time. Besides, the elaborate architecture of GNMT makes it
    have a better inference speed, which helps it more applicable in satisfying the
    industry need.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of GNMT get the help of the current research in attention mechanism;
    it used Global Attention but was reconstruct by a more effective structure for
    model parallelization. The concrete details illustrated in the figure, it has
    two main points in this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d586f16ebc20079010b47468d569447a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Attention in GNMT, the Attention weight was driven by the bottom
    layer of Decoder and sent to all Decoder layers, which helps to improve computing
    parallelization'
  prefs: []
  type: TYPE_NORMAL
- en: First, this structure has canceled the connection between the encoder and the
    decoder. So that it can have more freedom in choosing the structure of the encoder
    and decoder, for example, the encoder could choose the different dimensions in
    each layer regardless of the dimension in the decoder, only the top layer of the
    both encoder and decoder should have same dimensions to guarantee that they can
    be calculated in mathematics for driving attention vector.
  prefs: []
  type: TYPE_NORMAL
- en: Second, this structure makes it easier for paralleling the model. Only the bottom
    layer of the decoder is used to get the context vector, then all of the remain
    decoding layers will use this context vector directly. This architecture can retain
    as much parallelism as possible.
  prefs: []
  type: TYPE_NORMAL
- en: For details of attention calculation, GNMT applying the Attention Mechanism
    like the way of calculating global attention, while the $score()$ function is
    a feed forward network with 1 hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B5 Self-attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/35ecc2d79cd37f9d0d2c077750f8d9fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The concept of Multi-head Self-attention'
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention is also called intra-attention, it is wildly known for its application
    in NMT task due to the emergence of Transformer. While other commonly noted Attention
    Mechanism driven the context information by calculating words dependency between
    source sequence with target sequence, Self-attention calculates the words dependency
    inside the sequence, and thus get an attention based sequence representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for calculation steps, Self-attention first gets 3 vectors based original
    embedding for different purpose, the 3 vectors are Query vector, Key vector, and
    Value vector. Then the attention weights was calculated in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{ Attention }(Q,K,V)=\operatorname{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where the $\frac{1}{\sqrt{d_{k}}}$ is a scaled factor for avoiding to have more
    stable gradients that caused by dot products operation. In addition, the above
    calculation can be implemented in metrics multiplication, so the words dependency
    can easily got in form of relation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Other related work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides the above description in significant progress, there are also some other
    refinements in a different perspective of attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: In perspective of attention structure, Yang et al. improved the traditional
    attention structure by providing a network to model the relationship of word with
    its previous and subsequent attention[[94](#bib.bib94)]. Feng et al. proposed
    a recurrent attention mechanism to improve the alignment accuracy, and it has
    been proved to outperformed vanilla models in large-scale Chinese–English task[[93](#bib.bib93)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, other researches focus on the training process. Cohn et al. extended
    the original attention structure by adding several structural biases, they including
    positional bias, Markov conditioning, fertility, and Bilingual Symmetry[[95](#bib.bib95)],
    model that integrated with these refinements have got better translation performance
    over the basic attention-based model. More concretely, the above methods can be
    seen as a kind of inheritance from the alignment model in SMT research, with more
    experiential assumption and intuition in linguistics: $Position\ Bias:$ It assumed
    words in source and target sentence with the same meaning would also have a similar
    relative position, especially when both two-sentence have a similar word order.
    As an adjustment of the original attention mechanism, it helps to improve the
    alignment accuracy by encouraging words in similar relative position to be aligned.
    Figure11111 demonstrated the phenomena strongly, where words in diagonal are tended
    to be aligned. $Markov\ Condition:$ Empirically, in one sentence, one word has
    higher Correlation with its nearby words rather than those far from it, this is
    also the basement in explaining $context\ capture$ of n-gram LM. As for translation
    task, it’s obvious that words are adjacent in source sentence would also map to
    the nearby position in target sentence, taking advantage of this property, this
    consideration thus improves the alignment accuracy by discouraging the huge jumps
    in finding the corresponding alignment of nearby words. In addition, the method
    with similar consideration but different implementation is local attention. $Fertility:$
    Fertility measures whether the word has been attended at the right level, it considers
    preventing both scenarios when the word hasn’t got enough attention or has been
    paid too much attention. This design comes from the fact that the poor translation
    result is commonly due to repeatedly translated some word or lack coverage of
    other words, which refers to Under-translation and Over-translation. $Bilingual\
    Symmetry:$ In theory, word alignment should be a reversible result, which means
    the same word alignment should be got when translation processing form A to B
    with translation from B to A. This motivates the parallel training for the model
    in both directions and encouraging the similar alignment result.'
  prefs: []
  type: TYPE_NORMAL
- en: The refinement infertility was further extended by Tu et al.[[35](#bib.bib35)],
    who proposed fertility prediction as a normalizer before decoding, this method
    adjusts the context vector in original NMT model by adding coverage information
    when calculating attention weights, thus can provide complementary information
    about the probability of source words have been translated in prior steps.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the intuition that heuristics from SMT, Cheng et al. applied the agreement-based
    learning method on NMT task, which encourages joint training in the agreement
    of word alignment with both translation directions [[96](#bib.bib96)]. In later,
    Mi et al. proposed a supervised method for attention component, and it utilized
    annotated data with additional alignment constraints in its objective function,
    experiments in Chinese-to-English task has proven to benefit for both translation
    performance and alignment accuracy[[97](#bib.bib97)].
  prefs: []
  type: TYPE_NORMAL
- en: V Vocabulary Coverage Mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides the long dependency problem in general MT tasks, the existence of unknown
    words is another problem that can severely affect the translation quality. Different
    from traditional SMT methods which support enormous vocabulary, most of NMT models
    suffer from the vocabulary coverage problem due to the nature that it can only
    choose candidate words in predefined vocabulary with a modest size. In terms of
    vocabulary building, the chosen words are usually frequent words, while the remaining
    words are called unknown words or out-of-vocabulary (OOV) words.
  prefs: []
  type: TYPE_NORMAL
- en: Empirically speaking, the vocabulary size in NMT varies between 30k-80k at most
    in each language, with one marked exception was proposed by Jean et al., who once
    used an efficient approximation for $softmax$ to accommodate for the immense size
    of vocabulary (500k)[[47](#bib.bib47)]. However, the vocabulary coverage problem
    still persists widely because of the far more number of OOV words in $de\ facto$
    translation task, such as proper nouns in different domains and a great number
    of rarely used verbs.
  prefs: []
  type: TYPE_NORMAL
- en: Since the vocabulary coverage in NMT is extremely limited, handling the OOV
    words is another research hot spot. This section demonstrates the intrinsic interpretation
    of the vocabulary coverage problem in NMT and the corresponding solutions proposed
    in the past several years.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Description of Vocabulary Coverage problem in NMT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the scenario as mentioned above, in the practical implementation of
    NMT, the initial way is choosing a small set of vocabulary and converting a large
    number of OOV words to one uniform “UNK” symbol (or other tags) as illustrated
    in Fig. [10](#S5.F10 "Figure 10 ‣ V-A Description of Vocabulary Coverage problem
    in NMT ‣ V Vocabulary Coverage Mechanism ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation"). This intuitive solution may hurt translation
    performance in the following two aspects. First, the existence of “UNK” symbol
    in translation may hurt the semantic completeness of sentence; ambiguity may emerge
    when “UNK” replace some crucial words[[48](#bib.bib48)]. Second, as the NMT model
    hard to learn information from OOV words, the prediction quality beyond the OOV
    words may also be affected[[49](#bib.bib49)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: BLEU Performance of NMT Models'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | BLEU |'
  prefs: []
  type: TYPE_TB
- en: '| EN-DE | EN-FR |'
  prefs: []
  type: TYPE_TB
- en: '| ByteNet | 23.75 |  |'
  prefs: []
  type: TYPE_TB
- en: '| Deep-Att + PosUnk |  | 39.2 |'
  prefs: []
  type: TYPE_TB
- en: '| GNMT + RL | 24.6 | 39.92 |'
  prefs: []
  type: TYPE_TB
- en: '| ConvS2S | 25.16 | 40.46 |'
  prefs: []
  type: TYPE_TB
- en: '| MoE | 26.03 | 40.56 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep-Att + PosUnk Ensemble |  | 40.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GNMT + RL Ensemble | 26.3 | 41.16 |'
  prefs: []
  type: TYPE_TB
- en: '| ConvS2S Ensemble | 26.36 | 41.29 |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer (base model) | 27.3 | 38.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer (big) | 28.4 | 41.8 |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/733039cc73816e85f2c30730edd0fed7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: An example of OOV words problem presented in[[23](#bib.bib23)].
    $en$ and $fr$ denote the source sentence in English and the corresponding target
    sentence in French, $nn$ denotes the neural network’s result.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the unsurprising observation that NMT performed poorly on sentence with
    more OOV words than with more frequent words, some other phenomena in MT task
    are also hard to be handled like multi-word alignment, transliteration, and spelling,
    .etc. [[16](#bib.bib16)][[21](#bib.bib21)]. They are seen as a similar phenomenon
    which is also caused by unknown words problem or suffers from rare training data[[50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: For most of NMT model, choosing a modest size of vocabulary list is virtually
    a trade-off between the computation cost with translation quality. Also, it has
    been found the same thing in training NLM[[53](#bib.bib53)][[52](#bib.bib52)].
    Concretely, the computation cost mainly comes from the nature of the method in
    getting predicting word — a normalization operation, which is used repeatedly
    in training of the DL model. Specifically, in NMT task, since DL model needs to
    adjust the parameters each time, the probability of current word thus would be
    calculated repeatedly to get the gradient, and since NMT model calculates the
    probability of current word when making a prediction, it needs to normalize the
    all of words in the vocabulary each time. Unfortunately, the normalization process
    is time-consuming due to its time complexity is linear with the vocabulary size,
    and this attribute has rendered the same time complexity in the training process.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Different Solutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Related researches have proposed various methods in both the training and inference
    process. Further, these methods can be roughly divided into three categories based
    on their different orientations. The first one is intuitively focused on finding
    solutions in improving computation speedup, which could support a more extensive
    vocabulary. The second one focus on using context information, this kind of method
    can address some of the unknown words(such as Proper Noun) by copying them to
    translation result as well as low-frequency words which cause a poor translation
    quality. The last one, which is more advanced, prefers to utilize information
    inside the word such as characters, because of their flexibility in handling morphological
    variants of words, this method can support translating OOV words in a more ”intelligent”
    way.
  prefs: []
  type: TYPE_NORMAL
- en: V-B1 methods by computation speedup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For computation speedup method, there are lots of literature that implement
    their idea in NLM training. The first thought in trying computation speedup is
    to scale the $softmax$ operation. Since an effective $softmax$ calculation could
    obviously support a larger vocabulary, this kind of trying has got a lot of attention
    in NLM literature. Morin & Bengio [[53](#bib.bib53)] proposed hierarchical models
    to get an exponential speedup in the computation of normalization factor, thus
    help to accelerate the gradient calculation of word probabilities. In concrete
    details, the original model has transformed vocabulary into a binary tree structure,
    which was built with pre-knowledge from WordNet[[54](#bib.bib54)].
  prefs: []
  type: TYPE_NORMAL
- en: The initial experiment result shows that this hierarchical method is comparable
    with traditional trigram LM but fails to exceed original NLM; this is partly because
    of utilizing handcrafted feature from WordNet in the tree building process. As
    a binary tree can provide a significant improvement in cost-effective between
    the speed with performance, further work still focuses on this trend to find better
    refinement. Later on, Mnih & Hinton followed this work by removing the requirement
    of expert knowledge in tree building process[[52](#bib.bib52)].
  prefs: []
  type: TYPE_NORMAL
- en: A more elegant method is to retain the original model but change the method
    in calculating the normalization factor. Bengio & Sen´ecal proposed importance
    sampling method to approximate the normalization factor[[55](#bib.bib55)]. However,
    this method is not stable unless with a careful control[[56](#bib.bib56)]. Mnih
    & Teh used noise-contrastive estimation to learn the normalization factor directly,
    which can be more stable in the training process of NLM [[57](#bib.bib57)]. Later,
    Vaswani et al. proposed a similar method with application in MT[[58](#bib.bib58)].
  prefs: []
  type: TYPE_NORMAL
- en: The above methods are difficult to be implemented parallelly by GPUs. Further
    consideration found solutions that are more GPU friendly. Jean et al. alleviated
    the computation time by utilizing a subset of vocabulary as candidate words list
    in the training process while used the whole vocabulary in the inference process.
    Based on the inspiration of using importance sampling in earlier work[[56](#bib.bib56)],
    they proposed a pure data segmentation method in the training process. Specifically,
    they pre-processed the training data sequentially, choosing a subset of vocabulary
    for each training example with the number of its distinct words reached threshold
    $t$(which is still far less than the size of original vocabulary). In the inference
    process, they still abandon using the whole vocabulary and proposing a hybrid
    candidate list alternatively. They composed candidate words list from two parts.
    The first part is some specific candidate target words that translated from a
    pre-defined dictionary with the others are the K-most frequent words. In the practical
    performance analysis, this method remains the similar modest size of candidate
    words in the training process; thus, it can maintain the computational efficiency
    while supporting an extremely larger size of candidate words[[47](#bib.bib47)].
    Similarly, Mi et al. proposed vocabulary manipulation method which provides a
    separate vocabulary for different sentences or batches, it contains candidate
    words from both word-to-word dictionary and phrase-to-phrase library[[104](#bib.bib104)].
  prefs: []
  type: TYPE_NORMAL
- en: Besides all kinds of corresponding drawbacks in the above method, the common
    weakness of all these methods is they still suffer from the OOV words despite
    a larger vocabulary size they can support. This is because the enlarged vocabulary
    is still size limited, and there’s no solution for complementary when encountering
    unknown words, whereas the following category of methods can partly handle it.
    In addition, simply increasing the vocabulary size can merely bring little improvement
    due to the Zipf’s Law, which means there is always a large tail of OOV words need
    to be addressed[[48](#bib.bib48)].
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 methods by using context information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides the above variants which focus on computation speed-up, a more advanced
    category is using context information. Luong et al. proposed a word alignment
    algorithm which collaborates with Copy Mechanism to post-processing the translation
    result. This old but useful operation was inspired by the common word(phrase)
    replacement method in SMT and has achieved a pretty considerable improvement in
    BLEU [[59](#bib.bib59)]. Concretely, in Luong’s method, for each of the OOV words,
    there’s a “pointer” which map to the corresponding word in the source sentence.
    In the post-processing stage, a predefined dictionary was provided with “pointer”
    to find the corresponding translation, while using directly copy mechanism to
    handle the OOV words that not in the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: The popularity of Luong et al. ’s method is partly because the Copy Mechanism
    actually provides an infinite vocabulary. Further research has refined this alignment
    algorithm for better replacement accuracy and generalization. Choi et al. extended
    Luong et al.’s approach by dividing OOV words into one of three subdivisions based
    on their linguistic features. [[70](#bib.bib70)] This method can help to remap
    the OOV words effectively. Gulcehre et al. done several refinements in this category,
    they applied Copy Mechanism similar to Luong et al. but cooperate the Attention
    Mechanism in determining the location of word alignment, which is more flexible
    in addressing alignment and could be directly utilized in other tasks which alignment
    location varies dramatically in both sides (like text summarization). Besides
    that, they synthesized Copy Mechanism with general translation operation by adding
    a so-called switching network to decide which operation should be applied in each
    time-step, this could be thought to improve the generalization of the whole model.
    [[48](#bib.bib48)]. Gu et al. made parallel efforts in integrating different mechanisms,
    they proposed a kind of Attention Mechanism called CopyNet with the vanilla encoder-decoder
    model, which can be naturally extended to handle OOV words in NMT task[[74](#bib.bib74)].
    Additionally, they found that the attention mechanism has driven more by the semantics
    and language model when using traditional word translation, but by location when
    using copying operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1244d0e5c8ec8f1dc18667c22ba9b96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An example of one kind of Copy Mechanism proposed by Luong et al.[[23](#bib.bib23)],
    the subscripts of $<unk>$ symbol (refer as $d$)is a relative position of corresponding
    source words, where the alignment relation is a target word at position $j$ is
    aligned to a source word at position $i$ $=$ $j$ $+$ $d$.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the Copy Mechanism, using extra knowledge is also useful in handling
    some other linguistic scenarios, which is highly related to the vocabulary coverage
    problem. Arthur et al. incorporated lexicons knowledge to assist with translation
    in low-frequency words[[72](#bib.bib72)]. Feng et al. proposed a similar method
    with a memory-augmented NMT (M-NMT) architecture, and it used novel attention
    mechanism to get the extra knowledge from the dictionary that constructed by SMT[[73](#bib.bib73)].
    Additionally, using context information can also be applied to improve the translation
    quality of ambiguous words (Homographs)[[75](#bib.bib75)].
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, there are many context-based refinements have been proposed,
    most of them using Copy Mechanism to handle the OOV words with various of alignment
    algorithm to locate the corresponding word in the target side. However, these
    kinds of methods have a limited room for further improvement because the Copy
    Mechanism is too crude to handle the sophisticated scenarios in different languages.
    Practically, these methods perform poorly in languages which are rich morphology
    like Finnish and Turkish, which motivated method with better generalization[[64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: V-B3 Methods in fine grit level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This sub-section introduce some more “intelligent” ways that focus on using
    additional information inside the word unit. It’s obviously that such additional
    information could enhance the ability in covering the various of the linguistic
    phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: In previous research, although using semantic information of word unit could
    provide the vast majority of learning features, other features in sub-word level
    are generally ignored. From the perspective of linguistic, the concept of “word”
    is the basic unit of language but not the minimum one in containing semantic information,
    and there are abundant experienced rules could be learned from the inside of word
    units like shape and suffix. Comparing with identity copy or dictionary translation
    which regards the rare words as an identical entity, the refined method using
    fine-grit information is more adaptable. Further, a more “radical” method was
    proposed, which just treats words in character level completely. It would be an
    innovative concern for future work.
  prefs: []
  type: TYPE_NORMAL
- en: In this category, one popular choice is using the sub-word unit, the most remarkable
    achievement was proposed by Sennrich et al., which has been proved to have the
    best performance in some shared result[[50](#bib.bib50)]. Concretely, in this
    design, they treat unknown words as sequences of sub-word units, which is reasonable
    in terms of the composition of the vast majority of these words(e.g., named entities,
    loanwords, and morphologically complex words). In order to represent these OOV
    words completely, one intuitive solution is to build a predefined sub-word dictionary
    that contains enough variants of units. However, restoring the sub-words cause
    massive space consumption in vocabulary size, which is effectively cancels out
    the whole purpose of reducing the computational efficiency in both time and space.
    Under this circumstances, a Byte Pair Encoding (BPE) based sub-words extraction
    method was applied for word segmentation operation in both sides of languages,
    which successfully adapted this old but effective data compression method in text
    pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In concrete details of this adapted BPE method, it alternated merging frequent
    pairs of bytes with characters or sequence of characters, and word segmentation
    process followed the below steps:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Prepare a large training corpus(generally are bilingual corpus).
  prefs: []
  type: TYPE_NORMAL
- en: (2) Determined the size of the sub-word vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Split the words to a sequence of characters (using a special space character
    for marking the original spaces).
  prefs: []
  type: TYPE_NORMAL
- en: (4) Merge the most frequent adjacent pair of characters (e.g., in English, this
    may be c and h into ch).
  prefs: []
  type: TYPE_NORMAL
- en: (5) Repeat step 4 until reaching the fixed given number of times or the defined
    size of the vocabulary. Each of these steps would increase the vocabulary by one.
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows a toy example of the method. As for the practical result of
    word segmentation, the most frequent words will be merged as single tokens, while
    the rare words(which is similar to the OOV words in previous categories’ work)
    may still contain un-merged sub-words. However, they have been found rare in processed
    text[[50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: Further work of BPE method has also been proposed to obtaining a better generalization.
    Taku proposed subword regularization in later as an alternative to handle the
    spurious ambiguity phenomenon in BPE, and they further proposed a new subword
    segmentation algorithm based on a uni-gram language model, which shares the same
    idea with BPE but was more flexible in getting multiple segmentation based on
    their probabilities. Similarly, Wu et al. used “workpieces” concept to handle
    the OOV words which were once applied on the Google speech recognition system
    to solve Japanese/Korean segmentation problem[[60](#bib.bib60)][[24](#bib.bib24)].
    This method breaks words into word pieces to get a balance between flexibility
    with efficiency when using single characters and full words separately.
  prefs: []
  type: TYPE_NORMAL
- en: Another notable concern is modeling sequence in characters-level. Inspired by
    using character-level information completely in building NLM[[65](#bib.bib65)],
    Costa-jussa‘ & Fonollosa used CNN with highway network to model the characters
    directly[[62](#bib.bib62)], they deployed this architecture in source side with
    a common word-level generation in target side. Similarly, Ling et al. and Ballesteros
    et al. have proposed model respectively that using RNN(LSTM) to build character
    level embedding and composes it into the word embedding [[64](#bib.bib64)] [[98](#bib.bib98)],
    this idea later has been applied in building an RNN based character-level NMT
    model [[63](#bib.bib63)]. More recently, Luong and Manning(2016) proposed a hybrid
    model that combines the word level RNN with character level RNN for assist[[99](#bib.bib99)].
    Concretely, Luongs’ method translates mostly at the word level, when encounter
    an OOV word, character level RNN would be used for the consult. The figure shows
    the detailed architecture of this model.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the trying of designing a fully character-level translation
    model has also got attention accordingly. Chung et al. used BPE method to extract
    a sequence of subword in encoder side, they just varied the decoder by using pure
    characters, and it has indicated to provide comparable performance with models
    uses sub-words [[61](#bib.bib61)]. Motivated by aforementioned work, Lee et al.
    proposed fully character-level NMT without any segmentation, it was based on CNN
    pooling with highway layers, which can solve the prohibitively slow speed of training
    in Luong and Manning’s work [[71](#bib.bib71)].
  prefs: []
  type: TYPE_NORMAL
- en: VI Advanced models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section gives a demonstration of some advanced models that have got the
    state-of-the-art performance, while all of them belong to different categories
    of model structure. Experimental result indicated that all these networks can
    achieve similar performance with different advantages in their corresponding aspects.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A ConvS2S
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ConvS2S is short for Convolutional Sequence to Sequence, which is an end-to-end
    NMT model proposed by Gehring et al.[[82](#bib.bib82)]. Different with most of
    RNN based NMT models, ConvS2S is entirely CNN based model both in encoder and
    decoder. In the network structure, ConvS2S stacked 15 layers of CNN in its encoder
    and decoder with fixed kernel width of 3\. This deep structure helps to mitigate
    the weakness in capturing context information.
  prefs: []
  type: TYPE_NORMAL
- en: In respect of network details, ConvS2S applied Gated Linear Units (GLU)[[100](#bib.bib100)]
    in building network, which provide a gated function for output of convolution
    layer. Specifically, the output of convolution layer $Y\in\mathbb{R}^{2d}$ which
    is a vector with double times dimensions (2$d$ numbers of dimensions) of each
    input element’s embedding ($d$ numbers of dimensions), the gated function processes
    the output $Y=[AB]\in\mathbb{R}^{2d}$ by implementing the equation $8$, where
    both $A$ and $B$ are $d$ dimensions vector, and the function $\sigma(B)$ is a
    gated function used to control which inputs $A$ of the current context are relevant.
    This non-linearity operation has been proved to be more effective in applying
    training language model[[100](#bib.bib100)] , surpassing those only applying $tanh$
    function on $A$ [[140](#bib.bib140)]. In addition, ConvS2S also used residual
    connection [[141](#bib.bib141)] between different convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $v([AB])=A\otimes\sigma(B)$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'Besides the innovation of CNN based encoder-decoder structure, ConvS2S also
    applied similar Attention Mechanism that has been wildly accepted by RNN model,
    called Multi-step Attention. Concretely, Multi-step Attention is a separate attention
    structure applied in each decoder layer. In the process of calculating attention,
    the current hidden state $d_{i}^{l}$ (i.e., the output of the $l$th layer) has
    combined with previous output embedding $g_{i}$ as a vector of decoder state summary
    $d_{i}^{l}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d_{i}^{l}=W_{d}^{l}h_{i}^{l}+b_{d}^{l}+g_{i}$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: Then the attention vector $a_{ij}^{l}$ (i.e., the attention of state $i$ with
    source element $j$ in decoder layer $l$) would be driven by the dot-product of
    the summary vector with the output of the final encoder layer $z_{j}^{u}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $a_{ij}^{l}=\frac{\exp\left(d_{i}^{l}\cdot z_{j}^{u}\right)}{\sum_{t=1}^{m}\exp\left(d_{i}^{l}\cdot
    z_{t}^{u}\right)}$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: Lastly, the context vector is calculated as the weighted average of the attention
    vector $a_{ij}^{l}$ with the encoder output $z_{j}^{u}$ as well as the encoder
    input $e_{j}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $c_{i}^{l}=\sum_{j=1}^{m}a_{ij}^{l}\left(z_{j}^{u}+e_{j}\right)$ |  |
    (12) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/611c6d8f863192a9ce9e6e403fa526cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The structure of ConvS2S model, a successful CNN based NMT model
    with competitive performance to the state-of-the-art'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B RNMT+
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RNMT+ was proposed by Chen et al.[[103](#bib.bib103)]. This model has directly
    inherited the structure of GNMT model that was proposed by Wu et al.[[24](#bib.bib24)].
    Specifically, RNMT+ can be seen as an enhanced GNMT model, which demonstrated
    the best performance of RNN based NMT model. In model structure, RNMT+ mainly
    differs from the GNMT model in the following several perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: First, RNMT+ used six bi-directional RNN (LSTM) in its decoder, whereas GNMT
    used one layer of bi-directional RNN with seven layers of unidirectional RNN.
    This structure has sacrificed the computation efficiency in return for the extreme
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Second, RNMT+ applied Multi-head additive attention instead of the single-head
    attention in conventional NMT model, which can be seen as taking advantage of
    Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Third, synchronous training strategy was provided in the training process, which
    improved the convergence speed with model performance based on empirical results[[102](#bib.bib102)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, inspired by Transformer model, per-gate layer normalization [[101](#bib.bib101)]
    was applied, which has indicated to be helpful in stabilizing model training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/728f362f71b6c713e1b0eb18e722c97e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The structure of RNMT+ model,which has the similar structure of
    GNMT with adaptive innovation in Attention Mechanism'
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Transformer and Transformer based models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformer is a new NMT structure proposed by Vaswani et al.[[25](#bib.bib25)].
    Different from existing NMT models, it has abandoned the standard RNN/CNN structures
    and designed an innovative multi-layer self-attention blocks that are incorporated
    with a positional encoding method. This new trend of structure design takes the
    advantages from both RNN and CNN based model, which has been further used for
    initializing the input representation for other NLP tasks. Notably, Transformer
    is a complete Attention based NMT model.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C1 Structure of the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transformer has its distinct structure in its model, where the major differences
    are the input representation and multi-head attention.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Input representation
  prefs: []
  type: TYPE_NORMAL
- en: Transformer has its unique representation in handling input data that quite
    different with recurrent or convolution model. For computing Self-Attention we
    mentioned, transformer handles the input as three kind of vectors for different
    purpose. They are Key,Value and Query vectors. And all these vectors are driven
    by multiplying the input embedding with three matrices that we trained during
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, there’s Positional Encoding method was applied for enhancing the modeling
    ability of sequence order, since Transformer has abandoned recurrence structure,
    this kind of method made a compensation by inject word order information in to
    feature vector, which can avoid the model to become invariant to sequence ordering
    [[148](#bib.bib148)]. Specifically, Transformer add Positional Encoding to the
    input embedding at the bottoms of the encoder and decoder stacks, the Positional
    Encoding has been designed to have the same dimension with model embedding and
    thus could be summed. Positional Encoding can be calculated by applying positional
    functions directly or be learned [[82](#bib.bib82)], with be proven to have similar
    performance in final evaluation. In Transformer, adding Positional Encoding by
    using $since$ and $cosine$ functions is finally chosen, and each position can
    be encoded in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{array}[]{r}{PE_{(pos,2i)}=\sin\left(pos/10000^{\left.2i/d_{\text{model
    }}\right)}\right.}\\ {PE_{(\text{pos, }2i+1)}=\cos\left(\text{pos}/10000^{2i/d_{\text{model
    }}}\right)}\end{array}$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where $pos$ indicates the position and $i$ indicates the dimension. That is,
    each dimension of the positional encoding corresponds to a sinusoid function,
    and the wavelengths form a geometric progression from $2\pi$ to $20000\pi$. The
    reason of choosing these functions is that they have been assumed theoretically
    in helping the model to learn to attend by relative positions easily, due to the
    characters that for any fixed offset $k$, $PE_{pos}+k$ can be represented as a
    linear function of $PE_{pos}$.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Multi-head Self-Attention
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention is the major innovation in Transformer. But in implementation,
    rather than just computing the Self-Attention once, the multi-head mechanism runs
    through the scaled dot-product attention multiple times in parallel, and the outputs
    of these independent attention are then concatenated and linearly transformed
    into the expected dimensions. This multiple times Self-Attention computation is
    called Multi-head Self-Attention, which is applied for allowing the model to jointly
    attend to information from different representation sub-spaces at different positions.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Encoder & Decoder Blocks
  prefs: []
  type: TYPE_NORMAL
- en: The encoder is built by 6 identical components, each of which contains one multi-head
    attention layer with a fully connected network above it. These two sub-layers
    are equipped with residual connection as well as layer normalization. All the
    sub-layers have the same dimension of 512 in output data.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder, however, is more complicated. There are also 6 components stacked;
    and in each component, three sub-layers are connected, including two sub-layers
    of multi-head self attention and one sub-layer of fully-connected neural network.
    Specifically, the bottom attention layer is modified with method called masked
    to prevent positions from attending to subsequent positions, which is used for
    avoiding the model to look into the future of the target sequence when predicting
    the current word. Additionally, the second attention layer (the top attention
    layer) performs multi-head attention over the output of the encoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C2 Transformer based NMT variants
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Due to the tremendous performance improvement by Transformer, related refinement
    has got huge attention for researchers. The well accepted weakness of vanilla
    Transformer includes: lacking of recurrence modeling, theoretically not Turing-complete,
    capturing position information, as well as large model complexity. All these drawbacks
    have hindered its further improvement of translation performance. In response
    to these problems, some adjustments have been proposed for getting a better model.'
  prefs: []
  type: TYPE_NORMAL
- en: In respect of model architecture, some proposed modifications focused on both
    in depth of attention layer and network composition. Bapna et al. has proposed
    2-3x deeper Transformer with a refined attention mechanism, which can be easier
    for the optimization of deeper models [[152](#bib.bib152)]. The refined attention
    mechanism extended its connection to each encoder layers, like a weighted residual
    connections along the encoder depth, which allows the model to flexibly adjust
    the gradient flow to different layers of encoder. Similarly, Wang et al. [[145](#bib.bib145)]
    proposed a more deeper Transformer model (25 layers of encoder), which continues
    the same line of Bapna et al.(2018)’s work with properly applying layer normalization
    and a novel output combination method.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the fixed layers of NMT model, Dehghani et al. proposed Universal
    Transformers, which cancelled to stack the constant number of layers by combining
    recurrent inductive bias of RNNs and Adaptive Computation Time halting mechanism,
    thus enhanced the original self-attention based representation for better learning
    iterative or recursive transformations. Notably, this adjustment has made the
    model be shown to be Turing-complete under certain assumptions[[142](#bib.bib142)].
  prefs: []
  type: TYPE_NORMAL
- en: As for refinement in network composition, inspired by the thinking of AutoML,
    So et al. applied neural architecture search (NAS) to find a comparable model
    with simplified architecture[[146](#bib.bib146)]. The Evolved Transformer proposed
    in [[146](#bib.bib146)] has an innovative combination of basic blocks achieves
    the same quality as the original Transformer-Big model with 37.6% less parameters.
  prefs: []
  type: TYPE_NORMAL
- en: While most modification is focus on changing model structure directly, some
    new literature has chosen to utilize different input representation to improve
    model performance. One direct method is using enhanced Position Encoding for sequence
    order injection, where vanilla Transformer has weakness in capturing position
    information. Shaw et al. proposed modified self-attention mechanism with awareness
    of utilizing representations of relative positions, which demonstrated to have
    a significant improvements in two MT tasks[[147](#bib.bib147)].
  prefs: []
  type: TYPE_NORMAL
- en: Concurrently, using pre-initialized input representation with fine tune is another
    orientation, where some attempt have been proposed in different NLP tasks such
    as applying ELMo[[150](#bib.bib150)] for encoder of NMT model[[155](#bib.bib155)].
    In terms of Transformer, one by-product of this innovative model is using self-attention
    for representing sequence, which can effectively fused word information with contextual
    information. In later, Two well-known Transformer based input representation methods
    were been proposed named Bert (Bidirectional Encoder Representation from Transformers)[[149](#bib.bib149)]
    and GPT (Generative Pre-trained Transformer)[[151](#bib.bib151)], which has been
    indicated to bring improvement in some downstream NLP tasks. As for applying Transformer
    based pre-trained model in NMT task, more recently, this kind of trying has also
    been realized by using Bert as additional embedding layer or applying Bert as
    pre-trained model directly[[154](#bib.bib154)], which has been indicated to provide
    a bit better performance than vanilla Transformer after fine tune. Additionally,
    directly applying Bert as pre-trained model has been proved to have similar performance
    and thus can be more convenient for encoder initialization.
  prefs: []
  type: TYPE_NORMAL
- en: The full structure of Transformer is illustrated in Fig. [14](#S6.F14 "Figure
    14 ‣ VI-C2 Transformer based NMT variants ‣ VI-C Transformer and Transformer based
    models ‣ VI Advanced models ‣ A Survey of Deep Learning Techniques for Neural
    Machine Translation").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3207628f63fa3c55c23f5c1f0475683f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The full structure of Transformer'
  prefs: []
  type: TYPE_NORMAL
- en: VII Future trend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we have witnessed the fast-growing research progresses in NMT, there
    are still many challenges. Based on the extensive investigation [[121](#bib.bib121)][[125](#bib.bib125)][[117](#bib.bib117)][[116](#bib.bib116)],
    we summarize the major challenges and list some potential directions in the following
    several aspects.
  prefs: []
  type: TYPE_NORMAL
- en: '(1) In terms of translation performance, NMT still doesn’t perform well in
    translating long sentences. This is mainly because of two reasons: the practical
    limitation in engineering and the learning ability of the model itself. For the
    first reason, some academic experiments have chosen to ignore part of the long
    sentence that exceeds the RNN length. But we do believe that it’s not the same
    thing when NMT has been deployed in industrial applications. For the second reason,
    as research progresses, the model architecture would be more complicated. For
    example, Transformer model has applied innovative structure in its design which
    brought significant improvement in translation quality and speed[[25](#bib.bib25)].
    We believe that more refinements in model structure would be proposed. As we all
    know that RNN based NMT takes its advantages in modeling sequence order but results
    in computational inefficiency. More future work would consider the trade off between
    these two aspects.'
  prefs: []
  type: TYPE_NORMAL
- en: (2) Alignment mechanism is essential for both SMT and NMT models. For the vast
    majority of NMT models, Attention Mechanism plays the functional role in the alignment
    task, while it arguably does broader work than conventional alignment models.
    We believe this advanced alignment method would still get attraction in future
    research, since powerful attention method can improve the model performance directly.
    Later research in attention mechanism would try to relieve the weakness in NMT
    such as interpretation ability[[116](#bib.bib116)].
  prefs: []
  type: TYPE_NORMAL
- en: (3) Vocabulary coverage problem has always affected most of the NMT models.
    The research trend in handling computation load of softmax operation would pursue.
    And we have also found new training strategy which supports large vocabulary size.
    Besides, research of NMT operating in sub-word or character level has also aroused
    in recent years, which provided additional solution beyond traditional scope.
    More importantly, solving sophisticated translation scenario such as informal
    spelling is also a hot spot. Current NMT model integrated with character-level
    network has alleviated this phenomenon. Future work should focus on handling all
    kinds of OOV words in a more flexible way.
  prefs: []
  type: TYPE_NORMAL
- en: (4) Low-Resource Neural Machine Translation[[125](#bib.bib125)] is another hot
    spot in current NMT, which tries to solve the severe performance reduction when
    NMT model is trained with rare bilingual corpus. Since the aforementioned scenario
    happened commonly in practice where some seldom-used languages don’t have enough
    data, we do believe this filed would be extended in further research. Multilingual
    translation method[[111](#bib.bib111)][[110](#bib.bib110)] is the commonly proposed
    method which incorporated multi-language pair of data to improve NMT performance.
    It may need more interpretation about the different results in choosing different
    language pairs. Besides, unsupervised method has utilized the additional dataset
    and provided pre-trained model. Further research could improve its effect and
    provide hybrid training strategy with traditional method [[122](#bib.bib122)][[123](#bib.bib123)][[124](#bib.bib124)].
  prefs: []
  type: TYPE_NORMAL
- en: (5) Finally, research in NMT applications would also become more abundant. Currently,
    many applications have been developed such as speech translation[[107](#bib.bib107)][[106](#bib.bib106)]
    and document level translation[[105](#bib.bib105)]. We believe that various applications
    (especially end-to-end tasks) would emerge in the future. We strongly hope that
    an AI based simultaneous translation system could be applied in large-scale, which
    can bring huge benefit to our human society[[108](#bib.bib108)].
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Klein, G., Kim, Y., Deng, Y., Senellart, J., & Rush, A. M. (2017). Opennmt:
    Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Forcada, M. L., Ginestí-Rosell, M., Nordfalk, J., O’Regan, J., Ortiz-Rojas,
    S., Pérez-Ortiz, J. A., … & Tyers, F. M. (2011). Apertium: a free/open-source
    platform for rule-based machine translation. Machine translation, 25(2), 127-144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Koehn, P., Och, F. J., & Marcu, D. (2003, May). Statistical phrase-based
    translation. In Proceedings of the 2003 Conference of the North American Chapter
    of the Association for Computational Linguistics on Human Language Technology-Volume
    1 (pp. 48-54). Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi,
    N., … & Dyer, C. (2007, June). Moses: Open source toolkit for statistical machine
    translation. In Proceedings of the 45th annual meeting of the association for
    computational linguistics companion volume proceedings of the demo and poster
    sessions (pp. 177-180).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Chorowski, J., Bahdanau, D., Cho, K., & Bengio, Y. (2014). End-to-end continuous
    speech recognition using attention-based recurrent nn: First results. arXiv preprint
    arXiv:1412.1602.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic
    language model. Journal of machine learning research, 3(Feb), 1137-1155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the
    properties of neural machine translation: Encoder-decoder approaches. arXiv preprint
    arXiv:1409.1259.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Kalchbrenner, N., & Blunsom, P. (2013). Recurrent continuous translation
    models. In Proceedings of the 2013 Conference on Empirical Methods in Natural
    Language Processing (pp. 1700-1709).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation
    of word representations in vector space. arXiv preprint arXiv:1301.3781.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Mikolov, T., Yih, W. T., & Zweig, G. (2013). Linguistic regularities in
    continuous space word representations. In Proceedings of the 2013 Conference of
    the North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies (pp. 746-751).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors
    for word representation. In Proceedings of the 2014 conference on empirical methods
    in natural language processing (EMNLP) (pp. 1532-1543).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., & Makhoul, J.
    (2014). Fast and robust neural network joint models for statistical machine translation.
    In Proceedings of the 52nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers) (Vol. 1, pp. 1370-1380).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Galley, M., and Manning, C. D. (2008, October). A simple and effective
    hierarchical phrase reordering model. In Proceedings of the Conference on Empirical
    Methods in Natural Language Processing (pp. 848-856). Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Chiang, D., Knight, K., & Wang, W. (2009, May). 11,001 new features for
    statistical machine translation. In Proceedings of human language technologies:
    The 2009 annual conference of the north american chapter of the association for
    computational linguistics (pp. 218-226). Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Green, S., Wang, S., Cer, D., & Manning, C. D. (2013). Fast and adaptive
    online training of feature-rich translation models. In Proceedings of the 51st
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers) (Vol. 1, pp. 311-321).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning
    with neural networks. In Advances in neural information processing systems (pp.
    3104-3112).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural
    computation, 9(8), 1735-1780.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
    Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder
    for statistical machine translation. arXiv preprint arXiv:1406.1078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Gehring, J., Auli, M., Grangier, D., & Dauphin, Y. N. (2016). A convolutional
    encoder model for neural machine translation. arXiv preprint arXiv:1611.02344.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Meng, F., Lu, Z., Wang, M., Li, H., Jiang, W., & Liu, Q. (2015). Encoding
    source language with convolutional neural network for machine translation. arXiv
    preprint arXiv:1503.01838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation
    by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Britz, D., Goldie, A., Luong, M. T., & Le, Q. (2017). Massive exploration
    of neural machine translation architectures. arXiv preprint arXiv:1703.03906.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches
    to attention-based neural machine translation. arXiv preprint arXiv:1508.04025.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W.,
    … & Klingner, J. (2016). Google’s neural machine translation system: Bridging
    the gap between human and machine translation. arXiv preprint arXiv:1609.08144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural
    Information Processing Systems(pp. 5998-6008).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation
    of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Wu, Y., Zhang, S., Zhang, Y., Bengio, Y., & Salakhutdinov, R. R. (2016).
    On multiplicative integration with recurrent neural networks. In Advances in neural
    information processing systems (pp. 2856-2864).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies
    with gradient descent is difficult. IEEE transactions on neural networks, 5(2),
    157-166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Graves, A. (2012). Sequence transduction with recurrent neural networks.
    arXiv preprint arXiv:1211.3711.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Boulanger-Lewandowski, N., Bengio, Y., & Vincent, P. (2013, November).
    Audio Chord Recognition with Recurrent Neural Networks. In ISMIR (pp. 335-340).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Graves, A. (2013). Generating sequences with recurrent neural networks.
    arXiv preprint arXiv:1308.0850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Koehn, P. (2004, September). Pharaoh: a beam search decoder for phrase-based
    statistical machine translation models. In Conference of the Association for Machine
    Translation in the Americas (pp. 115-124). Springer, Berlin, Heidelberg.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Chiang, D. (2005, June). A hierarchical phrase-based model for statistical
    machine translation. In Proceedings of the 43rd Annual Meeting on Association
    for Computational Linguistics (pp. 263-270). Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Och, F. J., Tillmann, C., & Ney, H. (1999). Improved alignment models
    for statistical machine translation. In 1999 Joint SIGDAT Conference on Empirical
    Methods in Natural Language Processing and Very Large Corpora.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Tu, Z., Lu, Z., Liu, Y., Liu, X., & Li, H. (2016). Modeling coverage for
    neural machine translation. arXiv preprint arXiv:1601.04811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Auli, M., Galley, M., Quirk, C.,& Zweig, G. (2013). Joint language and
    translation modeling with recurrent neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Auli, M., & Gao, J. (2014). Decoder integration and expected bleu training
    for recurrent neural network language models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Schwenk, H., & Gauvain, J. L. (2005, October). Training neural network
    language models on very large corpora. In Proceedings of the conference on Human
    Language Technology and Empirical Methods in Natural Language Processing (pp.
    201-208). Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Schwenk, H. (2007). Continuous space language models. Computer Speech
    & Language, 21(3), 492-518.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Schwenk, H., Dchelotte, D., & Gauvain, J. L. (2006, July). Continuous
    space language models for statistical machine translation. In Proceedings of the
    COLING/ACL on Main conference poster sessions (pp. 723-730). Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Mikolov, T., Karafiát, M., Burget, L., Černocký, J., & Khudanpur, S. (2010).
    Recurrent neural network based language model. In Eleventh annual conference of
    the international speech communication association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Pollack, J. B. (1990). Recursive distributed representations. Artificial
    Intelligence, 46(1-2), 77-105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Chrisman, L. (1991). Learning recursive distributed representations for
    holistic computation. Connection Science, 3(4), 345-366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Allen, R. B. (1987, June). Several studies on natural language and back-propagation.
    In Proceedings of the IEEE First International Conference on Neural Networks (Vol.
    2, No. S 335, p. 341). IEEE Piscataway, NJ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Elman, J. L. (1990). Finding structure in time. Cognitive science, 14(2),
    179-211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] JORDAN, M. (1986). Serial Order; a parallel distributed processing approach.
    ICS Report 8604, UC San Diego.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Jean, S., Cho, K., Memisevic, R., & Bengio, Y. (2014). On using very large
    target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Gulcehre, C., Ahn, S., Nallapati, R., Zhou, B.,& Bengio, Y. (2016). Pointing
    the unknown words. arXiv preprint arXiv:1603.08148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Jiajun, Z., & Chengqing, Z. (2016). Towards zero unknown word in neural
    machine translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation
    of rare words with subword units. arXiv preprint arXiv:1508.07909.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Gage, P. (1994). A new algorithm for data compression. The C Users Journal,
    12(2), 23-38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Mnih, A., & Hinton, G. E. (2009). A scalable hierarchical distributed
    language model. In Advances in neural information processing systems (pp. 1081-1088).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Morin, F., & Bengio, Y. (2005, January). Hierarchical probabilistic neural
    network language model. In Aistats (Vol. 5, pp. 246-252).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Miller, G. (1998). WordNet: An electronic lexical database. MIT press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Bengio, Y., & Senécal, J. S. (2003, January). Quick Training of Probabilistic
    Neural Nets by Importance Sampling. In AISTATS(pp. 1-9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Bengio, Y., & Senécal, J. S. (2008). Adaptive importance sampling to accelerate
    training of a neural probabilistic language model. IEEE Transactions on Neural
    Networks, 19(4), 713-722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Mnih, A., & Teh, Y. W. (2012). A fast and simple algorithm for training
    neural probabilistic language models. arXiv preprint arXiv:1206.6426.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Vaswani, A., Zhao, Y., Fossum, V., & Chiang, D. (2013). Decoding with
    large-scale neural language models improves translation. In Proceedings of the
    2013 Conference on Empirical Methods in Natural Language Processing (pp. 1387-1392).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Luong, M. T., Sutskever, I., Le, Q. V., Vinyals, O., & Zaremba, W. (2014).
    Addressing the rare word problem in neural machine translation. arXiv preprint
    arXiv:1410.8206.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Schuster, M., & Nakajima, K. (2012, March). Japanese and korean voice
    search. In 2012 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP) (pp. 5149-5152). IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Chung, J., Cho, K., & Bengio, Y. (2016). A character-level decoder without
    explicit segmentation for neural machine translation. arXiv preprint arXiv:1603.06147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Costa-Jussa, M. R., & Fonollosa, J. A. (2016). Character-based neural
    machine translation. arXiv preprint arXiv:1603.00810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Ling, W., Trancoso, I., Dyer, C., & Black, A. W. (2015). Character-based
    neural machine translation. arXiv preprint arXiv:1511.04586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Ling, W., Luís, T., Marujo, L., Astudillo, R. F., Amir, S., Dyer, C.,
    … & Trancoso, I. (2015). Finding function in form: Compositional character models
    for open vocabulary word representation. arXiv preprint arXiv:1508.02096.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Kim, Y., Jernite, Y., Sontag, D., & Rush, A. M. (2016, March). Character-aware
    neural language models. In Thirtieth AAAI Conference on Artificial Intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Kudo, T. (2018). Subword regularization: Improving neural network translation
    models with multiple subword candidates. arXiv preprint arXiv:1804.10959.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2015, June). Gated feedback
    recurrent neural networks. In International Conference on Machine Learning (pp.
    2067-2075).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Denkowski, M., & Neubig, G. (2017). Stronger baselines for trustable results
    in neural machine translation. arXiv preprint arXiv:1706.09733.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Nakazawa, T., Higashiyama, S., Ding, C., Mino, H., Goto, I., Kazawa, H.,
    … & Kurohashi, S. (2017, November). Overview of the 4th Workshop on Asian Translation.
    In Proceedings of the 4th Workshop on Asian Translation (WAT2017) (pp. 1-54).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Choi, H., Cho, K., & Bengio, Y. (2017). Context-dependent word representation
    for neural machine translation. Computer Speech & Language, 45, 149-160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Lee, J., Cho, K., & Hofmann, T. (2017). Fully character-level neural machine
    translation without explicit segmentation. Transactions of the Association for
    Computational Linguistics, 5, 365-378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Arthur, P., Neubig, G., & Nakamura, S. (2016). Incorporating discrete
    translation lexicons into neural machine translation. arXiv preprint arXiv:1606.02006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Feng, Y., Zhang, S., Zhang, A., Wang, D., & Abel, A. (2017). Memory-augmented
    neural machine translation. arXiv preprint arXiv:1708.02005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Gu, J., Lu, Z., Li, H., & Li, V. O. (2016). Incorporating copying mechanism
    in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Liu, F., Lu, H., & Neubig, G. (2017). Handling homographs in neural machine
    translation. arXiv preprint arXiv:1708.06510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Zhao, Y., Zhang, J., He, Z., Zong, C., & Wu, H. (2018). Addressing Troublesome
    Words in Neural Machine Translation. In Proceedings of the 2018 Conference on
    Empirical Methods in Natural Language Processing (pp. 391-400).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A. N., Gouws,
    S., … & Sepassi, R. (2018). Tensor2tensor for neural machine translation. arXiv
    preprint arXiv:1803.07416.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L. S.
    (2019). Learning Deep Transformer Models for Machine Translation. arXiv preprint
    arXiv:1906.01787.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] So, D. R., Liang, C., & Le, Q. V. (2019). The evolved transformer. arXiv
    preprint arXiv:1901.11117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018).
    Universal transformers. arXiv preprint arXiv:1807.03819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., &
    Salakhutdinov, R. (2019). Transformer-xl: Attentive language models beyond a fixed-length
    context. arXiv preprint arXiv:1901.02860.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y. N. (2017,
    August). Convolutional sequence to sequence learning. In Proceedings of the 34th
    International Conference on Machine Learning-Volume 70 (pp. 1243-1252). JMLR.
    org.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Hu, B., Tu, Z., Lu, Z., Li, H., & Chen, Q. (2015, July). Context-dependent
    translation selection using convolutional neural network. In Proceedings of the
    53rd Annual Meeting of the Association for Computational Linguistics and the 7th
    International Joint Conference on Natural Language Processing (Volume 2: Short
    Papers) (pp. 536-541).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Kalchbrenner, N., Espeholt, L., Simonyan, K., Oord, A. V. D., Graves,
    A., & Kavukcuoglu, K. (2016). Neural machine translation in linear time. arXiv
    preprint arXiv:1610.10099.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Kaiser, L., Gomez, A. N., & Chollet, F. (2017). Depthwise separable convolutions
    for neural machine translation. arXiv preprint arXiv:1706.03059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Kaiser, Ł., & Bengio, S. (2016). Can active memory replace attention?.
    In Advances in Neural Information Processing Systems (pp. 3781-3789).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Tran, K., Bisazza, A., & Monz, C. (2018). The importance of being recurrent
    for modeling hierarchical structure. arXiv preprint arXiv:1803.03585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Zhou, J., Cao, Y., Wang, X., Li, P., & Xu, W. (2016). Deep recurrent models
    with fast-forward connections for neural machine translation. Transactions of
    the Association for Computational Linguistics, 4, 371-383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Zhang, B., Xiong, D., Su, J., Duan, H., & Zhang, M. (2016). Variational
    neural machine translation. arXiv preprint arXiv:1605.07869.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G.,
    & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts
    layer. arXiv preprint arXiv:1701.06538.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Zhang, B., Xiong, D., Su, J., Lin, Q., & Zhang, H. (2018). Simplifying
    Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks.
    arXiv preprint arXiv:1810.12546.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Wang, M., Lu, Z., Zhou, J., & Liu, Q. (2017). Deep neural machine translation
    with linear associative unit. arXiv preprint arXiv:1705.00861.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Feng, S., Liu, S., Yang, N., Li, M., Zhou, M., & Zhu, K. Q. (2016, December).
    Improving attention modeling with implicit distortion and fertility for machine
    translation. In Proceedings of COLING 2016, the 26th International Conference
    on Computational Linguistics: Technical Papers (pp. 3082-3092).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Yang, Z., Hu, Z., Deng, Y., Dyer, C., & Smola, A. (2016). Neural machine
    translation with recurrent attention modeling. arXiv preprint arXiv:1607.05108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Cohn, T., Hoang, C. D. V., Vymolova, E., Yao, K., Dyer, C., & Haffari,
    G. (2016). Incorporating structural alignment biases into an attentional neural
    translation model. arXiv preprint arXiv:1601.01085.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Cheng, Y., Shen, S., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015).
    Agreement-based joint training for bidirectional attention-based neural machine
    translation. arXiv preprint arXiv:1512.04650.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Mi, H., Wang, Z., & Ittycheriah, A. (2016). Supervised attentions for
    neural machine translation. arXiv preprint arXiv:1608.00112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Ballesteros, M., Dyer, C., & Smith, N. A. (2015). Improved transition-based
    parsing by modeling characters instead of words with LSTMs. arXiv preprint arXiv:1508.00657.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Luong, M. T., & Manning, C. D. (2016). Achieving open vocabulary neural
    machine translation with hybrid word-character models. arXiv preprint arXiv:1604.00788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Dauphin, Y. N., Fan, A., Auli, M., & Grangier, D. (2017, August). Language
    modeling with gated convolutional networks. In Proceedings of the 34th International
    Conference on Machine Learning-Volume 70 (pp. 933-941). JMLR. org.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization.
    arXiv preprint arXiv:1607.06450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Chen, J., Pan, X., Monga, R., Bengio, S., & Jozefowicz, R. (2016). Revisiting
    distributed synchronous SGD. arXiv preprint arXiv:1604.00981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Chen, M. X., Firat, O., Bapna, A., Johnson, M., Macherey, W., Foster,
    G., … & Wu, Y. (2018). The best of both worlds: Combining recent advances in neural
    machine translation. arXiv preprint arXiv:1804.09849.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Mi, H., Wang, Z.,& Ittycheriah, A. (2016). Vocabulary manipulation for
    neural machine translation. arXiv preprint arXiv:1605.03209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Wang, L., Tu, Z., Way, A., & Liu, Q. (2017). Exploiting cross-sentence
    context for neural machine translation. arXiv preprint arXiv:1704.04347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Weiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., & Chen, Z. (2017). Sequence-to-sequence
    models can directly translate foreign speech. arXiv preprint arXiv:1703.08581.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Duong, L., Anastasopoulos, A., Chiang, D., Bird, S., & Cohn, T. (2016,
    June). An attentional model for speech translation without transcription. In Proceedings
    of the 2016 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (pp. 949-959).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Gu, J., Neubig, G., Cho, K., & Li, V. O. (2016). Learning to translate
    in real-time with neural machine translation. arXiv preprint arXiv:1610.00388.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R.,
    … & Bengio, Y. (2015, June). Show, attend and tell: Neural image caption generation
    with visual attention. In International conference on machine learning (pp. 2048-2057).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Dong, D., Wu, H., He, W., Yu, D., & Wang, H. (2015, July). Multi-task
    learning for multiple language translation. In Proceedings of the 53rd Annual
    Meeting of the Association for Computational Linguistics and the 7th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 1723-1732).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Firat, O., Cho, K., & Bengio, Y. (2016). Multi-way, multilingual neural
    machine translation with a shared attention mechanism. arXiv preprint arXiv:1601.01073.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Bojar, Ondrej, et al. “Findings of the 2015 Workshop on Statistical Machine
    Translation.” Proceedings of the Tenth Workshop on Statistical Machine Translation,
    2015, pp. 1–46.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Cettolo, Mauro, et al. “The IWSLT 2015 Evaluation Campaign.” IWSLT 2015,
    International Workshop on Spoken Language Translation, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Junczys-Dowmunt, M., Dwojak, T., & Hoang, H. (2016). Is neural machine
    translation ready for deployment? A case study on 30 translation directions. arXiv
    preprint arXiv:1610.01108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Bentivogli, L., Bisazza, A., Cettolo, M., & Federico, M. (2016). Neural
    versus phrase-based machine translation quality: a case study. arXiv preprint
    arXiv:1608.04631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Chaudhari, S., Polatkan, G., Ramanath, R., & Mithal, V. (2019). An attentive
    survey of attention models. arXiv preprint arXiv:1904.02874.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Galassi, A., Lippi, M., & Torroni, P. (2019). Attention, please! a critical
    review of neural attention models in natural language processing. arXiv preprint
    arXiv:1902.02181.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Domhan, T. (2018, July). How much attention do you need? a granular analysis
    of neural machine translation architectures. In Proceedings of the 56th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp.
    1799-1808).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Kaiser, Ł., & Sutskever, I. (2015). Neural gpus learn algorithms. arXiv
    preprint arXiv:1511.08228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Lipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of
    recurrent neural networks for sequence learning. arXiv preprint arXiv:1506.00019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Koehn, P., & Knowles, R. (2017). Six challenges for neural machine translation. arXiv
    preprint arXiv:1706.03872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] He, D., Xia, Y., Qin, T., Wang, L., Yu, N., Liu, T. Y., & Ma, W. Y. (2016).
    Dual learning for machine translation. In Advances in Neural Information Processing
    Systems (pp. 820-828).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Ramachandran, P., Liu, P. J., & Le, Q. V. (2016). Unsupervised pretraining
    for sequence to sequence learning. arXiv preprint arXiv:1611.02683.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Artetxe, M., Labaka, G., Agirre, E., & Cho, K. (2017). Unsupervised neural
    machine translation. arXiv preprint arXiv:1710.11041.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Sennrich, R., & Zhang, B. (2019). Revisiting Low-Resource Neural Machine
    Translation: A Case Study. arXiv preprint arXiv:1905.11901.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Schwenk, H. (2012, December). Continuous space translation models for
    phrase-based statistical machine translation. In Proceedings of COLING 2012: Posters (pp.
    1071-1080).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Rosenfeld, R. (2000). Two decades of statistical language modeling: Where
    do we go from here?. Proceedings of the IEEE, 88(8), 1270-1278.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Stolcke, A. (2002). SRILM-an extensible language modeling toolkit. In Seventh
    international conference on spoken language processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Teh, Y. W. (2006, July). A hierarchical Bayesian language model based
    on Pitman-Yor processes. In Proceedings of the 21st International Conference on
    Computational Linguistics and the 44th annual meeting of the Association for Computational
    Linguistics (pp. 985-992). Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Federico, M., Bertoldi, N., & Cettolo, M. (2008). IRSTLM: an open source
    toolkit for handling large scale language models. In Ninth Annual Conference of
    the International Speech Communication Association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Heafield, K. (2011, July). KenLM: Faster and smaller language model queries.
    In Proceedings of the sixth workshop on statistical machine translation (pp. 187-197).
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Son, L. H., Allauzen, A., & Yvon, F. (2012, June). Continuous space translation
    models with neural networks. In Proceedings of the 2012 conference of the north
    american chapter of the association for computational linguistics: Human language
    technologies (pp. 39-48). Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &
    Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of
    machine learning research, 12(Aug), 2493-2537.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Young, T., Hazarika, D., Poria, S., & Cambria, E. (2018). Recent trends
    in deep learning based natural language processing. ieee Computational intelligenCe
    magazine, 13(3), 55-75.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Wallach, H. M. (2006, June). Topic modeling: beyond bag-of-words. In Proceedings
    of the 23rd international conference on Machine learning (pp. 977-984). ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Song, F., & Croft, W. B. (1999, November). A general language model for
    information retrieval. In Proceedings of the eighth international conference on
    Information and knowledge management (pp. 316-321). ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems (pp. 1097-1105).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013, June). Rectifier nonlinearities
    improve neural network acoustic models. In Proc. icml (Vol. 30, No. 1, p. 3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Cheng, Y., Shen, S., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. Agreement-Based
    Joint Training for Bidirectional Attention-Based Neural Machine Translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). Pixel recurrent
    neural networks. arXiv preprint arXiv:1601.06759.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition (pp. 770-778).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018).
    Universal transformers. arXiv preprint arXiv:1807.03819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Xiao, F., Li, J., Zhao, H., Wang, R., & Chen, K. (2019). Lattice-Based
    Transformer Encoder for Neural Machine Translation. arXiv preprint arXiv:1906.01282.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Hao, J., Wang, X., Yang, B., Wang, L., Zhang, J., & Tu, Z. (2019). Modeling
    recurrence for transformer. arXiv preprint arXiv:1904.03092.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L.
    S. (2019). Learning Deep Transformer Models for Machine Translation. arXiv preprint
    arXiv:1906.01787.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] So, D. R., Liang, C., & Le, Q. V. (2019). The evolved transformer. arXiv
    preprint arXiv:1901.11117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative
    position representations. arXiv preprint arXiv:1803.02155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Parikh, A. P., Täckström, O., Das, D., & Uszkoreit, J. (2016). A decomposable
    attention model for natural language inference. arXiv preprint arXiv:1606.01933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training
    of deep bidirectional transformers for language understanding. arXiv preprint
    arXiv:1810.04805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,
    & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint
    arXiv:1802.05365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving
    language understanding by generative pre-training. URL https://s3-us-west-2\.
    amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding
    paper. pdf.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Bapna, A., Chen, M. X., Firat, O., Cao, Y., & Wu, Y. (2018). Training
    deeper neural machine translation models with transparent attention. arXiv preprint
    arXiv:1808.07561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., & Zhang, Z. (2019). Star-transformer.
    arXiv preprint arXiv:1902.09113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Clinchant, S., Jung, K. W., & Nikoulina, V. (2019). On the use of BERT
    for Neural Machine Translation. arXiv preprint arXiv:1909.12744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Edunov, S., Baevski, A., & Auli, M. (2019). Pre-trained language model
    representations for language generation. arXiv preprint arXiv:1903.09722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Luong, M. T. (2017).Neural Machine Translation. Unpublished doctoral
    dissertation, Stanford University, Stanford, CA 94305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
