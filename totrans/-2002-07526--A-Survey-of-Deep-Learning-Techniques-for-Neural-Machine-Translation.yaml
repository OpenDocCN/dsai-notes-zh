- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:02:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:02:29'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2002.07526] A Survey of Deep Learning Techniques for Neural Machine Translation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2002.07526] 神经机器翻译的深度学习技术综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.07526](https://ar5iv.labs.arxiv.org/html/2002.07526)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2002.07526](https://ar5iv.labs.arxiv.org/html/2002.07526)
- en: A Survey of Deep Learning Techniques for Neural Machine Translation
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经机器翻译的深度学习技术综述
- en: Shuoheng Yang, Yuxin Wang, Xiaowen Chu Department of Computer science
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 杨硕恒，王宇欣，朱晓文 计算机科学系
- en: Hong Kong Baptist University Hong Kong, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 香港浸会大学 中国香港
- en: yshuoheng@gmail.com, {yxwang, chxw}@comp.hkbu.edu.hk
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: yshuoheng@gmail.com, {yxwang, chxw}@comp.hkbu.edu.hk
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, natural language processing (NLP) has got great development
    with deep learning techniques. In the sub-field of machine translation, a new
    approach named Neural Machine Translation (NMT) has emerged and got massive attention
    from both academia and industry. However, with a significant number of researches
    proposed in the past several years, there is little work in investigating the
    development process of this new technology trend. This literature survey traces
    back the origin and principal development timeline of NMT, investigates the important
    branches, categorizes different research orientations, and discusses some future
    research trends in this field.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着深度学习技术的飞速发展，自然语言处理（NLP）也取得了巨大进展。在机器翻译子领域中，一种名为神经机器翻译（NMT）的方法出现，并受到学术界和工业界的广泛关注。然而，尽管过去几年提出了大量研究，但对这一新技术趋势的发展过程的研究仍较少。本文献综述追溯了NMT的起源和主要发展历程，调查了重要分支，分类了不同的研究方向，并讨论了该领域的一些未来研究趋势。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '索引词:'
- en: Neural Machine Translation, Deep Learning, Attention Mechanism.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经机器翻译，深度学习，注意力机制。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: I-A Introduction of Machine Translation
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 机器翻译介绍
- en: Machine translation (MT) is a classic sub-field in NLP that investigates how
    to use computer software to translate the text or speech from one language to
    another without human involvement. Since MT task has a similar objective with
    the final target of NLP and AI, i.e., to fully understand the human text (speech)
    at semantic level, it has received great attention in recent years. Besides the
    scientific value, MT also has huge potential of saving labor cost in many practical
    applications, such as scholarly communication and international business negotiation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译（MT）是自然语言处理（NLP）的一个经典子领域，研究如何利用计算机软件将文本或语音从一种语言翻译成另一种语言，而无需人工干预。由于机器翻译任务与NLP和AI的最终目标类似，即在语义层面上充分理解人类文本（语音），因此近年来受到了广泛关注。除了科学价值，机器翻译在许多实际应用中，如学术交流和国际商务谈判，也具有节省劳动力成本的巨大潜力。
- en: Machine translation task has a long research history with many efficient methods
    proposed in the past decades. Recently, with the development of Deep Learning,
    a new kind of method called Neural Machine Translation (NMT) has emerged. Compared
    with the conventional method like Phrase-Based Statistical Machine Translation
    (PBSMT), NMT takes advantages in its simple architecture and ability in capturing
    long dependency in the sentence, which indicates a huge potential in becoming
    a new trend of the mainstream. After a primitive model in origin, there are a
    variety of NMT models being proposed, some of which have achieved great progresses
    with the state-of-the-art result. This paper summarizes the major branches and
    recent progresses in NMT and discusses the future trend in this field.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译任务有着悠久的研究历史，过去几十年提出了许多有效的方法。近年来，随着深度学习的发展，一种新的方法——神经机器翻译（NMT）应运而生。与传统的基于短语的统计机器翻译（PBSMT）方法相比，NMT具有其简单的架构和捕捉句子中长期依赖的能力，这表明其成为主流的新趋势的巨大潜力。在原始模型的基础上，提出了各种NMT模型，其中一些已经取得了最先进的成果。本文总结了NMT的主要分支和近期进展，并讨论了该领域的未来趋势。
- en: I-B Related Work and Our Contribution
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 相关工作与我们的贡献
- en: Although there is little work in the literature survey of NMT, some other works
    are highly related. Lipton et al. have summarized the conventional methods in
    sequence learning[[120](#bib.bib120)], which provided essential information for
    the origin of NMT as well as the related base knowledge. Britz et al. and Tobias
    Domhan have done some model comparison work in NMT with experiment and evaluation
    in the practical performance of some wildly accepted technologies, but they have
    rare theoretical analysis, especially in presenting the relationship between different
    proposed models[[22](#bib.bib22)][[118](#bib.bib118)]. On the other hand, some
    researchers limited their survey work on a special part related to NMT — the Attention
    Mechanism, but both of them have a general scope that oriented to all kinds of
    AI tasks with Attention [[116](#bib.bib116)][[117](#bib.bib117)]. Maybe the most
    related work was an earlier doctoral thesis written by Minh-Thang Luong in 2016[[156](#bib.bib156)],
    which included a comprehensive description about the original structure of NMT
    as well as some wildly applied tips.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在NMT的文献综述中工作较少，但其他一些工作与之高度相关。Lipton等人总结了序列学习中的传统方法[[120](#bib.bib120)]，为NMT的起源以及相关基础知识提供了必要的信息。Britz等人和Tobias
    Domhan进行了NMT模型比较工作，通过实验和评估一些被广泛接受技术的实际性能，但他们的理论分析很少，特别是在呈现不同提出模型之间的关系方面[[22](#bib.bib22)][[118](#bib.bib118)]。另一方面，一些研究者将他们的综述工作限制在与NMT相关的特定部分——注意力机制上，但这两者都有一个面向各种AI任务的普遍范围[[116](#bib.bib116)][[117](#bib.bib117)]。也许最相关的工作是Minh-Thang
    Luong在2016年写的早期博士论文[[156](#bib.bib156)]，其中包括了对NMT原始结构以及一些被广泛应用技巧的全面描述。
- en: This paper, however, focuses on a direct and up-to-date literature survey about
    NMT. We have investigated a lot about the relevant literature in this new trend
    and provided comprehensive interpretation for current mainstream technology in
    NMT.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本文专注于关于NMT的直接和最新文献综述。我们对这一新趋势的相关文献进行了大量调查，并对NMT当前主流技术进行了全面的解读。
- en: As for concrete components, this literature survey investigates the origin and
    recent progresses in NMT, categorizes these models by their different orientation
    in the model structure. Then we demonstrate the insight of these NMT types, summarize
    the strengths and weaknesses by reviewing their design principal and corresponding
    performance analysis in translation quality and speed. We also give a comprehensive
    overview of two components in NMT development, namely attention mechanism and
    vocabulary coverage mechanism, both of which are indispensable for current achievement.
    At last, we give concentration on some literature which proposed advanced models
    with comparison work; we introduce these considerable models as well as the potential
    direction in future work.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 关于具体组件，这篇文献综述探讨了NMT的起源和最近进展，根据模型结构的不同方向对这些模型进行分类。然后，我们展示了这些NMT类型的洞察，概述了它们的优缺点，通过回顾它们的设计原理和对应的翻译质量及速度的性能分析。我们还全面概述了NMT发展的两个组成部分，即注意力机制和词汇覆盖机制，这两者对当前的成就都是不可或缺的。最后，我们重点介绍了一些提出了先进模型并进行了比较工作的文献；我们介绍了这些重要的模型以及未来工作中的潜在方向。
- en: Regarding the survey scope, some subareas of NMT with less attention were deliberately
    left out of the scope except with brief description in future trend. These include
    but are not limited to the literature of robustness of NMT, domain adaptation
    in NMT and other applications that embed NMT method (such as speech translation,
    document translation). Although the research scope has been specifically designed,
    due to the numerous of researches and the inevitable expert selection bias, we
    believe that our work is merely a snapshot of part of current research rather
    than all of them. We are hoping that our work could provide convenience for further
    research.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于调查范围，故意遗漏了一些NMT的子领域，除非在未来趋势中有简要描述。这些包括但不限于NMT的鲁棒性文献、NMT中的领域适应以及其他嵌入NMT方法的应用（如语音翻译、文档翻译）。尽管研究范围已被特别设计，但由于研究数量众多和不可避免的专家选择偏差，我们认为我们的工作仅是当前研究的一部分快照，而非全部。我们希望我们的工作能够为进一步研究提供便利。
- en: The remaining of the paper is organized as follows. Section [II](#S2 "II History
    of Machine Translation ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation") provides an introduction of machine translation and presents its
    history of development. Section [III](#S3 "III DNN based NMT ‣ A Survey of Deep
    Learning Techniques for Neural Machine Translation") introduces the structure
    of NMT and the procedure of training and testing. Section [IV](#S4 "IV NMT with
    Attention Mechanism ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation") discusses attention mechanism, an essential innovation in the development
    of NMT. Section [V](#S5 "V Vocabulary Coverage Mechanism ‣ A Survey of Deep Learning
    Techniques for Neural Machine Translation") surveys a variety of methods in handling
    word coverage problem and some fluent divisions. Section [VI](#S6 "VI Advanced
    models ‣ A Survey of Deep Learning Techniques for Neural Machine Translation")
    describes three advanced models in NMT. Finally, Section [VII](#S7 "VII Future
    trend ‣ A Survey of Deep Learning Techniques for Neural Machine Translation")
    discusses the future trend in this field.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。第[II](#S2 "II 机器翻译的历史 ‣ 神经机器翻译深度学习技术概述")节介绍了机器翻译及其发展历史。第[III](#S3
    "III 基于深度神经网络的神经机器翻译 ‣ 神经机器翻译深度学习技术概述")节介绍了神经机器翻译的结构及其训练和测试过程。第[IV](#S4 "IV 带注意力机制的神经机器翻译
    ‣ 神经机器翻译深度学习技术概述")节讨论了注意力机制，这是神经机器翻译发展中的一项重要创新。第[V](#S5 "V 词汇覆盖机制 ‣ 神经机器翻译深度学习技术概述")节调查了处理词汇覆盖问题的各种方法以及一些流利的分配。第[VI](#S6
    "VI 高级模型 ‣ 神经机器翻译深度学习技术概述")节描述了神经机器翻译中的三个高级模型。最后，第[VII](#S7 "VII 未来趋势 ‣ 神经机器翻译深度学习技术概述")节讨论了该领域的未来趋势。
- en: II History of Machine Translation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 机器翻译的历史
- en: Machine translation (MT) has a long history; the origin of this field could
    be traced back to the 17th century. In 1629, René Descartes came up with a universal
    language that expressed the same meaning in different languages and shared one
    symbol.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译（MT）有着悠久的历史；这一领域的起源可以追溯到17世纪。1629年，勒内·笛卡尔（René Descartes）提出了一种通用语言，这种语言在不同语言中表达相同的意义，并共享一个符号。
- en: The specific research of MT began at about 1950s, when the first researcher
    in the field, Yehoshua Bar-Hillel, began his research at MIT (1951) and organized
    the first International Conference on Machine Translation in 1952\. Since then,
    MT has experienced three primary waves in its development, the Rule-based Machine
    Translation[[2](#bib.bib2)], the Statistical Machine Translation[[3](#bib.bib3)][[4](#bib.bib4)],
    and the Neural Machine Translation[[7](#bib.bib7)]. We briefly review the development
    of these three stages in the following.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译的具体研究始于1950年代，当时该领域的第一位研究者叶胡沙·巴尔-希雷尔（Yehoshua Bar-Hillel）在麻省理工学院（MIT）开始了他的研究，并于1952年组织了第一次国际机器翻译会议。从那时起，机器翻译经历了三个主要的发展阶段，即基于规则的机器翻译[[2](#bib.bib2)]、统计机器翻译[[3](#bib.bib3)][[4](#bib.bib4)]和神经机器翻译[[7](#bib.bib7)]。我们将在下文中简要回顾这三个阶段的发展。
- en: II-A Development of Machine Translation
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 机器翻译的发展
- en: II-A1 Rule-based Machine Translation
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 基于规则的机器翻译
- en: Rule-based Machine Translation is the first design in MT, which is based on
    the hypothesis that all different languages have its symbol in representing the
    same meaning. Because in usual, a word in one language could find its corresponding
    word in another language with the same meaning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的机器翻译是机器翻译领域的第一个设计，基于的假设是所有不同语言都有其表示相同意义的符号。因为通常来说，一种语言中的一个词可以在另一种语言中找到其对应的词，并且意义相同。
- en: In this method, the translation process could be treated as the word replacement
    in the source sentence. In terms of ’rule-based’, since different languages could
    represent the same meaning of sentence in different word order, the word replacement
    method should base on the syntax rules of both two languages. Thus every word
    in the source sentence should take its corresponding position in the target language.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，翻译过程可以被视为源句中的词汇替换。就“基于规则”而言，由于不同语言可能以不同的词序来表达相同的句意，因此词汇替换方法应基于这两种语言的语法规则。因此，源句中的每个词都应在目标语言中占据相应的位置。
- en: The rule-based method has a beautiful theory but hardly achieves satisfactory
    performance in implementation. This is because of the computational inefficiency
    in determining the adaptive rule of one sentence. Besides, grammar rules are also
    hard to be organized, since linguists summarize the grammar rules, and there are
    too many syntax rules in one language (especially language with more relaxed grammar
    rules). It is even possible that two syntax rules conflict with each other.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法有一个优美的理论，但在实际应用中很难取得令人满意的表现。这是因为在确定一个句子的自适应规则时计算效率低。此外，语法规则也难以组织，因为语言学家总结了语法规则，而且一种语言中有太多的语法规则（特别是语法规则较为宽松的语言）。甚至可能会出现两个语法规则相互冲突的情况。
- en: 'The most severe drawback of rule-based method is that it has ignored the need
    of context information in the translation process, which destroys the robustness
    of rule-based machine translation. One famous example was given by Marvin Minsky
    in 1966, where he used two sentences given below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法最严重的缺点是忽视了翻译过程中对上下文信息的需求，这破坏了基于规则的机器翻译的鲁棒性。一个著名的例子由 Marvin Minsky 于 1966
    年给出，他使用了以下两个句子：
- en: “$The$ $pen$ $is$ $in$ $the$ $box$”
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: “$The$ $pen$ $is$ $in$ $the$ $box$”
- en: “$The$ $box$ $is$ $in$ $the$ $pen$”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: “$The$ $box$ $is$ $in$ $the$ $pen$”
- en: Both sentences have the same syntax structure. The first sentence is easy to
    understand; but the second one is more confusing, since the word “pen” is a polysemant,
    which also means “fence” in English. But it is difficult for the computer to translate
    the “pen” to that meaning; the word replacement is thus an unsuccessful method.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 两个句子具有相同的语法结构。第一个句子易于理解；但第二个句子更令人困惑，因为“pen”是一个多义词，在英语中也意味着“fence”。但计算机很难将“pen”翻译为那个含义，因此词汇替换是一种失败的方法。
- en: II-A2 Statistical Machine Translation
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 统计机器翻译
- en: Statistical Machine Translation (SMT) has been the mainstream technology for
    the past 20 years. It has been successfully applied in the industry, including
    Google translation, Baidu translation, etc.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 统计机器翻译（SMT）在过去 20 年里一直是主流技术。它已成功应用于工业界，包括谷歌翻译、百度翻译等。
- en: Different from Rule-based machine translation, SMT tackles the translation task
    from a statistical perspective. Concretely, the SMT model finds the words (or
    phrases) which have the same meaning through bilingual corpus by statistics. Given
    one sentence, SMT divides it into several sub-sentences, then every part could
    be replaced by target word or phrase.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于规则的机器翻译不同，SMT 从统计的角度处理翻译任务。具体而言，SMT 模型通过双语语料库统计找到具有相同意义的词汇（或短语）。给定一个句子，SMT
    将其分成几个子句，然后每个部分可以被目标词或短语替换。
- en: The most prevalent version of SMT is Phrase-based SMT (PBSMT), which in general
    includes pre-processing, sentence alignment, word alignment, phrase extraction,
    phrase feature preparation, and language model training. The key component of
    a PBSMT model is a phrase-based lexicon, which pairs phrases in the source language
    with phrases in the target language. The lexicon is built from the training data
    set which is a bilingual corpus. By using phrases in this translation, the translation
    model could utilize the context information within phrases. Thus PBSMT could outperform
    the simple word-to-word translation methods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的 SMT 版本是基于短语的 SMT（PBSMT），通常包括预处理、句子对齐、词汇对齐、短语提取、短语特征准备和语言模型训练。PBSMT 模型的关键组件是基于短语的词典，它将源语言中的短语与目标语言中的短语配对。词典是从训练数据集（即双语语料库）构建的。通过在翻译中使用短语，翻译模型可以利用短语中的上下文信息。因此，PBSMT
    可以优于简单的逐词翻译方法。
- en: II-A3 Neural Machine Translation
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 神经机器翻译
- en: It has been a long time since the first try on MT task by neural network[[44](#bib.bib44)][[43](#bib.bib43)].
    Because of the poor performance in the early period and the computing hardware
    limitation, related research in translation by neural network has been ignored
    for many years.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自从神经网络首次尝试 MT 任务以来已经有很长时间了[[44](#bib.bib44)][[43](#bib.bib43)]。由于早期表现不佳和计算硬件限制，神经网络翻译相关研究在许多年里被忽视了。
- en: Due to the proliferation of Deep Learning in 2010, more and more NLP tasks have
    achieved great improvement. Using deep neural networks for MT task has received
    great attention as well. A successful DNN based Machine Translation (NMT) model
    was first proposed by Kalchbrenner and Blunsom[[8](#bib.bib8)], which is a totally
    new concept for MT by that time. Comparing with other models, the NMT model needs
    less linguistic knowledge but can produce a competitive performance. Since then,
    many researchers have reported that NMT can perform much better than the traditional
    SMT model[[1](#bib.bib1)][[112](#bib.bib112)][[113](#bib.bib113)][[114](#bib.bib114)][[115](#bib.bib115)],
    and it has also been massively applied to the industrial field[[24](#bib.bib24)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于2010年深度学习的普及，越来越多的NLP任务取得了显著的改进。使用深度神经网络进行机器翻译任务也受到了极大的关注。Kalchbrenner和Blunsom首次提出了一个成功的基于DNN的机器翻译
    (NMT) 模型[[8](#bib.bib8)]，这是当时机器翻译的一个全新概念。与其他模型相比，NMT模型所需的语言学知识较少，但能够产生竞争力的表现。从那时起，许多研究人员报告称，NMT的表现远胜于传统的SMT模型[[1](#bib.bib1)][[112](#bib.bib112)][[113](#bib.bib113)][[114](#bib.bib114)][[115](#bib.bib115)]，并且它也被广泛应用于工业领域[[24](#bib.bib24)]。
- en: '![Refer to caption](img/bb41171e44e7a35ee1510d576dc2bb6c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/bb41171e44e7a35ee1510d576dc2bb6c.png)'
- en: 'Figure 1: The training process of RNN based NMT. The symbol $<EOS>$ means end
    of sequence. The embedding layer is for pre-processing. The two RNN layers are
    used to represent the sequence.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于RNN的神经机器翻译的训练过程。符号 $<EOS>$ 表示序列的结束。嵌入层用于预处理。这两层RNN用于表示序列。
- en: II-B Introduction of Neural Machine Translation
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 神经机器翻译的介绍
- en: II-B1 Motivation of NMT
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 神经机器翻译的动机
- en: 'The inspiration for neural machine translation comes from two aspects: the
    success of Deep Learning in other NLP tasks as we mentioned, and the unresolved
    problems in the development of MT itself.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 神经机器翻译的灵感来自两个方面：我们提到的深度学习在其他NLP任务中的成功，以及机器翻译本身发展中的未解决问题。
- en: For the first reason, in many NLP tasks, traditional Machine Learning method
    is highly dependent on hand-crafted features that often come from linguistic intuition,
    which is definitely an empirical trial-and-error process[[133](#bib.bib133)][[134](#bib.bib134)]
    and is often far more incomplete in representing the nature of original data.
    For example, the context size of training language model is assigned by researchers
    with strong assumption in context relation[[136](#bib.bib136)]; and in text representation
    method, the classic bag-of-words (BOW) method has ignored the influence of word
    order[[135](#bib.bib135)]. However, when applying deep neural network (DNN) in
    the aforementioned tasks, the DNN requires minimum domain-knowledge and avoids
    some pre-processing steps in human feature engineering [[22](#bib.bib22)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在许多NLP任务中，传统机器学习方法高度依赖于来自语言学直觉的手工特征，这显然是一个经验的试错过程[[133](#bib.bib133)][[134](#bib.bib134)]，并且通常在表示原始数据的性质方面远远不够完整。例如，训练语言模型的上下文大小是由研究人员基于强假设分配的[[136](#bib.bib136)];
    在文本表示方法中，经典的词袋 (BOW) 方法忽略了词序的影响[[135](#bib.bib135)]。然而，当在上述任务中应用深度神经网络 (DNN) 时，DNN只需最少的领域知识，并避免了人工特征工程的一些预处理步骤
    [[22](#bib.bib22)]。
- en: DNN is a powerful neural network which has achieved excellent performance in
    many complex learning tasks which are traditionally considered difficult[[137](#bib.bib137)][[138](#bib.bib138)].
    In NLP field, DNN has been applied in some traditional tasks, for example, speech
    recognition [[5](#bib.bib5)] and Named Entity Recognition (NER) [[133](#bib.bib133)].
    With the exceptional performance they got, DNN-based models have found many potential
    applications in other NLP tasks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DNN是一种强大的神经网络，在许多传统上认为困难的复杂学习任务中取得了卓越的表现[[137](#bib.bib137)][[138](#bib.bib138)]。在自然语言处理领域，DNN已被应用于一些传统任务，例如语音识别
    [[5](#bib.bib5)] 和命名实体识别 (NER) [[133](#bib.bib133)]。由于其卓越的表现，基于DNN的模型在其他NLP任务中也找到了许多潜在的应用。
- en: For the second reason, in MT field, PBSMT has got a pretty good performance
    in the past decades, but there are still some inherent weaknesses which require
    further improvement. First, since the PBSMT generates the translation by segmenting
    the source sentence into several phrases and doing phrase replacement, it may
    ignore the long dependency beyond the length of phrases and thus cause inconsistency
    in translation results such as incorrect gender agreements. Second, there are
    generally many intricate sub-components in current systems [[13](#bib.bib13)][[14](#bib.bib14)][[15](#bib.bib15)],
    e.g., language model, reordering model, length/unknown penalties, etc. With the
    increasing number of these sub-components, it is hard to fine-tune and combine
    each other to get a more stable result [[23](#bib.bib23)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个原因，在机器翻译领域，PBSMT在过去几十年中表现相当不错，但仍存在一些固有的弱点，需要进一步改进。首先，由于PBSMT通过将源句子分割成几个短语并进行短语替换来生成翻译，可能会忽略超出短语长度的长距离依赖，从而导致翻译结果不一致，例如性别不一致。其次，目前系统中通常有许多复杂的子组件[[13](#bib.bib13)][[14](#bib.bib14)][[15](#bib.bib15)]，例如语言模型、重新排序模型、长度/未知惩罚等。随着这些子组件数量的增加，很难对它们进行精细调整和组合，以获得更稳定的结果[[23](#bib.bib23)]。
- en: 'All the above discussions have indicated the bottleneck in the development
    of SMT miniature. Specifically, this bottleneck mainly comes from the language
    model (LM). This is because, in MT task, language model actually can give the
    most important information: the emergence probability of a particular word (or
    phrase) that is conditioned on previous words. So building a better LM can definitely
    improve the translation performance.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上述讨论已指出了SMT微型化发展中的瓶颈。具体来说，这个瓶颈主要来自语言模型（LM）。这是因为在机器翻译任务中，语言模型实际上可以提供最重要的信息：特定词（或短语）在前文词的条件下的出现概率。因此，建立更好的语言模型无疑能提高翻译性能。
- en: 'The vast majority of conventional LM is based on the Markov assumption:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大多数传统语言模型基于马尔科夫假设：
- en: '|  | $\displaystyle p\left(x_{1},x_{2},\ldots,x_{T}\right)$ | $\displaystyle=\prod_{t=1}^{T}p\left(x_{t}&#124;x_{1},\ldots,x_{t-1}\right)$
    |  | (1) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p\left(x_{1},x_{2},\ldots,x_{T}\right)$ | $\displaystyle=\prod_{t=1}^{T}p\left(x_{t}&#124;x_{1},\ldots,x_{t-1}\right)$
    |  | (1) |'
- en: '|  |  | $\displaystyle\approx\prod_{t=1}^{T}p\left(x_{t}&#124;x_{t-n},\ldots,x_{t-1}\right)$
    |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\approx\prod_{t=1}^{T}p\left(x_{t}&#124;x_{t-n},\ldots,x_{t-1}\right)$
    |  |'
- en: where $x_{1},x_{2},...,x_{T}$ is a sequence of words in a sentence and $T$ represents
    the length of the sentence.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{1},x_{2},...,x_{T}$ 是句子中的词序列，$T$ 代表句子的长度。
- en: In this assumption, the probability of the sentence is equal to the multiplication
    of probability of each word. $n$ is the total number of words that is chosen to
    simplify the model, which is also referred to as $context\ window$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个假设中，句子的概率等于每个词概率的乘积。$n$ 是为了简化模型而选择的总词数，也称为 $context\ window$。
- en: Obviously, the dependency of words that exceed $n$ would be ignored, which implies
    that the conventional LM performs poorly on modeling long dependency. Moreover,
    since the experimental result has indicated that a modest context size (generally
    4-6 words) can be accepted, the first problem of traditional LM is the limited
    representation ability.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，超出 $n$ 的词依赖会被忽略，这意味着传统语言模型在建模长距离依赖方面表现较差。此外，由于实验结果表明适度的上下文大小（通常为4-6个词）是可以接受的，因此传统语言模型的第一个问题是表示能力有限。
- en: Besides, the data sparsity for training has always been the problem that hinders
    an LM built with a larger size of context window. This is because the number of
    $n$-tuples for counting is exponential in $n$. In other words, when building an
    LM, with the increment of the number of order, the number of training samples
    we need would also increase remarkably, which is also referred to as ”curse of
    dimensionality”. For example, if one LM has the order of 5 with a vocabulary size
    of 10,000, then the possible combination of words for statistics should be about
    $10^{25}$, which requires enormous training data. And since most of these combinations
    have not been observed before, subsequent researches have used various trade-off
    and smoothing method to alleviate the sparsity problem[[129](#bib.bib129)][[128](#bib.bib128)][[127](#bib.bib127)][[130](#bib.bib130)][[131](#bib.bib131)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练的数据稀缺一直是阻碍使用更大上下文窗口构建LM的问题。这是因为用于计数的n元组的数量在n的指数增长。换句话说，当构建LM时，随着阶数的增加，需要的训练样本数量也会显著增加，这也被称为”维度灾难”。例如，如果一个LM的阶数为5，词汇量为10,000，则用于统计的单词组合可能约为$10^{25}$，这需要大量的训练数据。而且由于这些组合中大多数以前都没有被观察到，随后的研究采用了各种权衡和平滑方法来缓解稀疏性问题[[129](#bib.bib129)][[128](#bib.bib128)][[127](#bib.bib127)][[130](#bib.bib130)][[131](#bib.bib131)]。
- en: While further research of the aforementioned LM with statistical method has
    become almost stagnant, Neural Language Model (NLM)[[6](#bib.bib6)], on the other
    hand, uses a neural network to build a language model that models text data directly.
    In initial stage, NLM used fixed-length of a feature vector to represent each
    word, and then the solid number of word vectors would concatenate together as
    a semantic metric to represent the context [[6](#bib.bib6)] [[38](#bib.bib38)][[39](#bib.bib39)],
    which is very similar to the $context\ window$. This work was enhanced later by
    injecting additional context information from source sentence[[12](#bib.bib12)][[132](#bib.bib132)][[126](#bib.bib126)].
    Comparing with the traditional LM, the original NLM alleviates the sample sparsity
    due to the distributed representation of the word, which enables them to share
    the statistical weights rather than being independent variables. And since words
    with similar meaning may occur in the similar context, the corresponding feature
    vector would have the similar value, which indicates that the semantic relation
    of words has been ”embedded” into the feature vector.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前提到的基于统计方法的LM的进一步研究几乎停滞不前，但是神经语言模型（NLM）[[6](#bib.bib6)]却使用神经网络直接模拟文本数据的语言模型。在最初阶段，NLM使用固定长度的特征向量来表示每个单词，然后一系列单词向量会连接在一起，形成语义度量以表示上下文[[6](#bib.bib6)][[38](#bib.bib38)][[39](#bib.bib39)]，这与$context\
    window$非常相似。 这项工作后来通过从源句子注入额外的上下文信息得到增强[[12](#bib.bib12)][[132](#bib.bib132)][[126](#bib.bib126)]。与传统LM相比，原始的NLM通过单词的分布式表示减轻了样本稀疏性问题，这使它们能够共享统计权重，而不是独立变量。由于具有类似含义的单词可能出现在相似的上下文中，相应的特征向量将具有相似的值，这表明单词的语义关系已被“嵌入”到特征向量中。
- en: New proposals in the next stage solve the long dependency problem by using Recurrent
    Neural Network (RNN). RNN based NLM (RNLM) models the whole sentence by reading
    each word once a time-step, thus it can model the true conditional probability
    without limitation of context size[[41](#bib.bib41)]. Before the emergence of
    NMT, the RNLM, as mentioned earlier, outperformed the conventional LM in the evaluation
    of text perplexity and brought better performance in many practical tasks[[41](#bib.bib41)][[26](#bib.bib26)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下一阶段的新提案通过使用递归神经网络（RNN）解决了长期依赖问题。基于RNN的NLM（RNLM）通过逐个单词阅读整个句子，因此可以模拟真实的条件概率，而不受上下文大小的限制[[41](#bib.bib41)]。在NMT出现之前，如前所述，RNLM在文本困惑度评估方面优于传统的LM，并在许多实际任务中表现更好[[41](#bib.bib41)][[26](#bib.bib26)]。
- en: The direct application of NLM in SMT has been naturally proposed [[12](#bib.bib12)][[36](#bib.bib36)][[37](#bib.bib37)][[40](#bib.bib40)][[58](#bib.bib58)],
    and the preliminary experiment indicated promising results. The potential of NLM
    motivates further exploration for a complete DNN based translation model. Subsequently,
    a more ”pure” model with the only neural network has emerged, with the DNN architecture
    that learns to do the translation task end-to-end. Section [III](#S3 "III DNN
    based NMT ‣ A Survey of Deep Learning Techniques for Neural Machine Translation")
    demonstrates its basic structure (in Fig. [4](#S3.F4 "Figure 4 ‣ III-D1 General
    decoding work flow (greedy) ‣ III-D Inference method ‣ III DNN based NMT ‣ A Survey
    of Deep Learning Techniques for Neural Machine Translation")), as well as its
    concrete details.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: NLM 在 SMT 中的直接应用已被自然提出 [[12](#bib.bib12)][[36](#bib.bib36)][[37](#bib.bib37)][[40](#bib.bib40)][[58](#bib.bib58)]，初步实验表明了有希望的结果。NLM
    的潜力激励了对完整 DNN 基于的翻译模型的进一步探索。随后，出现了一个更“纯粹”的模型，它仅使用神经网络，并具有学习端到端翻译任务的 DNN 架构。第 [III](#S3
    "III DNN based NMT ‣ A Survey of Deep Learning Techniques for Neural Machine Translation")
    节展示了其基本结构（如图 [4](#S3.F4 "Figure 4 ‣ III-D1 General decoding work flow (greedy)
    ‣ III-D Inference method ‣ III DNN based NMT ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation") 所示），以及其具体细节。
- en: II-B2 Formulation of NMT Task
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 NMT 任务的公式化
- en: 'Currently, NMT task is originally designed as an end-to-end learning task.
    It directly processes a source sequence to a target sequence. The learning objective
    is to find the correct target sequence given the source sequence, which can be
    seen as a high dimensional classification problem that tries to map the two sentences
    in the semantic space. In all mainstreams of modern NMT model, this process can
    be divided into two steps: encoding and decoding, and thus can functionally separate
    the whole model as Encoder and Decoder as illustrated in Fig. [2](#S2.F2 "Figure
    2 ‣ II-B2 Formulation of NMT Task ‣ II-B Introduction of Neural Machine Translation
    ‣ II History of Machine Translation ‣ A Survey of Deep Learning Techniques for
    Neural Machine Translation").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，NMT 任务最初设计为端到端学习任务。它直接将源序列处理为目标序列。学习目标是找到正确的目标序列，这可以看作是一个高维分类问题，尝试将两个句子映射到语义空间。在所有主流现代
    NMT 模型中，这个过程可以分为两个步骤：编码和解码，从而可以功能性地将整个模型分为编码器和解码器，如图 [2](#S2.F2 "Figure 2 ‣ II-B2
    Formulation of NMT Task ‣ II-B Introduction of Neural Machine Translation ‣ II
    History of Machine Translation ‣ A Survey of Deep Learning Techniques for Neural
    Machine Translation") 所示。
- en: '![Refer to caption](img/8306bcbd1031610a0b846b0292722b2e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8306bcbd1031610a0b846b0292722b2e.png)'
- en: 'Figure 2: End-to-End structure in modern NMT model. The encoder is used to
    represent the source sentence to semantic vector, while the decoder makes prediction
    from this semantic vector to a target sentence. End-to-End means the model processes
    source data to target data directly, without explicable intermediate result.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：现代 NMT 模型中的端到端结构。编码器用于将源句子表示为语义向量，而解码器则从这个语义向量中生成目标句子。端到端意味着模型直接处理源数据到目标数据，没有可解释的中间结果。
- en: 'In perspective of probability, NMT generates the target sequence $T(t_{1},t_{2},...,t_{m})$
    from the max conditional probability given the source sequence $S(s_{1},s_{2},...,s_{n})$,
    where $n$ is the length of sequence $S$ and $m$ is the length of target sequence
    $T$. The whole task could be formulated as[[24](#bib.bib24)]:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从概率的角度看，NMT 根据源句子 $S(s_{1},s_{2},...,s_{n})$ 生成目标序列 $T(t_{1},t_{2},...,t_{m})$，其中
    $n$ 是序列 $S$ 的长度，$m$ 是目标序列 $T$ 的长度。整个任务可以公式化为[[24](#bib.bib24)]：
- en: '|  | $argmax\ P(T&#124;S).$ |  | (2) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $argmax\ P(T\vert S).$ |  | (2) |'
- en: 'More concretely, when generating each word of the target sentence, it uses
    the information from both the word it predicted previously and the source sentence.
    In that case, each generating step could be described as when generating the $i$-th
    word:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，当生成目标句子的每个词时，它利用了之前预测的词以及源句子中的信息。在这种情况下，每一步生成可以描述为生成第 $i$ 个词时：
- en: '|  | $argmax\prod\limits_{i=1}^{m}P(t_{i}&#124;t_{j<i},S)$ |  | (3) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $argmax\prod\limits_{i=1}^{m}P(t_{i}\vert t_{j<i},S)$ |  | (3) |'
- en: Based on this formula and the discussion of NLM above, NMT task could be regarded
    as an NLM model with additional constraints (e.g., conditioned on a given source
    sequence).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此公式以及上述对 NLM 的讨论，NMT 任务可以被视为一个具有额外约束（例如，依赖于给定源序列）的 NLM 模型。
- en: II-C The Recent Development in NMT
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C NMT 的最新发展
- en: 'We devide the recent developement of NMT in five main stages: (a) the original
    NMT with a shallow layer, (b) SMT assisted by NLM, (c) the DNN based NMT, (d)
    NMT with attention mechanism, (e) the attention-based NMT.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最近的神经机器翻译（NMT）发展分为五个主要阶段：（a）具有浅层的原始NMT，（b）由NLM辅助的SMT，（c）基于DNN的NMT，（d）具有注意力机制的NMT，（e）基于注意力的NMT。
- en: NMT with Shallow Layer
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 具有浅层的NMT
- en: Even before the Deep Learning, Allen has used binary encoding to train an NMT
    model in 1987[[44](#bib.bib44)]. Later in 1991, Chrisman used Dual-ported RAAM
    architecture[[42](#bib.bib42)] to build an original NMT model[[43](#bib.bib43)].
    Although both of them have a pretty primitive design with the limited result when
    looking back, their work has indicated the original idea of this field. The further
    related work has almost stagnated in the following decades, due to the huge progress
    that SMT method acquired at that period, as well as the limited computing power
    and data samples.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 早在深度学习之前，Allen在1987年就使用二进制编码训练了一个NMT模型[[44](#bib.bib44)]。后来在1991年，Chrisman使用双端口RAAM架构[[42](#bib.bib42)]建立了一个原始NMT模型[[43](#bib.bib43)]。尽管他们的设计相当原始且结果有限，但他们的工作指示了该领域的原始理念。由于SMT方法在那个时期取得了巨大的进展，以及计算能力和数据样本的限制，随后的相关工作几乎停滞不前。
- en: SMT assisted by NLM
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由NLM辅助的SMT
- en: Based on the above discussion, NLM has revolutionized the traditional LM even
    before the rise of deep learning. Later on, deep RNN based NLM has been applied
    in the SMT system. Cho et al. proposed an SMT model along with an NLM model[[18](#bib.bib18)].
    Although the main body is still SMT, this hybrid method provides a new direction
    for the emergence of a pure deep learning-based NMT.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述讨论，NLM在深度学习崛起之前已经革新了传统的语言模型（LM）。后来，深度RNN基础的NLM被应用于SMT系统。Cho等人提出了一种结合了NLM模型的SMT模型[[18](#bib.bib18)]。尽管主体仍然是SMT，但这种混合方法为纯深度学习基础的NMT的出现提供了新的方向。
- en: NMT with Deep Neural Network
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度神经网络的NMT
- en: Since the traditional SMT model with NLM has got the state-of-the-art performance
    at that time, a pure DNN based translation approach was proposed later with an
    end-to-end design to model the entire MT process[[8](#bib.bib8)][[16](#bib.bib16)].
    Using DNN based NMT could capture subtle irregularities in both two languages
    more efficiently [[24](#bib.bib24)], which is similar to the observation that
    DNNs often have a better performance than ’shallow’ neural networks [[21](#bib.bib21)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传统的带有NLM的SMT模型在当时达到了最先进的性能，后来提出了一种纯DNN基础的翻译方法，采用端到端设计来建模整个机器翻译过程[[8](#bib.bib8)][[16](#bib.bib16)]。使用基于DNN的NMT可以更高效地捕捉两种语言中的微妙不规则性[[24](#bib.bib24)]，这与DNN通常比“浅层”神经网络表现更好的观察结果类似[[21](#bib.bib21)]。
- en: NMT with Attention Mechanism
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 具有注意力机制的NMT
- en: Although the initial DNN based NMT model has not outperformed the SMT completely,
    it still exhibited a huge potential for further research. When tracing back the
    major weakness, although one theoretical advantage of RNN is its ability in capturing
    the long dependency between words, in fact, the model performance would deteriorate
    with the increase of sentence length. This scenario is due to the limited feature
    representation ability in a fixed-length vector. Under the circumstances, since
    the original NMT has got a pretty good performance without any auxiliary, the
    idea of whether some variants in architecture could bring a breakthrough has led
    to the rise of Attention Mechanism.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最初的基于DNN的NMT模型尚未完全超越SMT，但它仍然展示了巨大的进一步研究潜力。追溯其主要弱点，尽管RNN的一个理论优势是捕捉单词之间的长距离依赖，但实际上，随着句子长度的增加，模型性能会恶化。这种情况是由于固定长度向量的特征表示能力有限。在这种情况下，由于原始NMT在没有任何辅助的情况下表现相当出色，是否有某些架构变体能带来突破的想法导致了注意力机制的兴起。
- en: Attention Mechanism was originally proposed by Bahdanau et al. as an intermediate
    component[[21](#bib.bib21)], and the objective is to provide additional word alignment
    information in translating the long sentence. Surprisingly, NMT model has got
    a considerable improvement with the help of this simple method. Later on, with
    tremendous popularity among both academia and industry, many refinements in Attention
    Mechanism have emerged, and more details will be discussed in Section [IV](#S4
    "IV NMT with Attention Mechanism ‣ A Survey of Deep Learning Techniques for Neural
    Machine Translation").
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制最初由 Bahdanau 等人提出作为一个中间组件[[21](#bib.bib21)]，目的是在翻译长句子时提供额外的词对齐信息。令人惊讶的是，NMT
    模型在这种简单方法的帮助下得到了显著改进。后来，随着在学术界和工业界的广泛流行，注意力机制的许多改进也随之出现，更多细节将在第 [IV](#S4 "IV NMT
    with Attention Mechanism ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation") 节中讨论。
- en: Fully Attention based NMT
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 完全基于注意力的 NMT
- en: With the development of Attention Mechanism, fully Attention-based NMT has emerged
    as a great innovation in NMT history. In this new tendency, Attention mechanism
    has taken the dominate position in text feature extraction rather than a auxiliary
    component. And the representative model is Transformer [[25](#bib.bib25)], which
    is a fully attention-based model proposed by Vaswani et al.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 随着注意力机制的发展，完全基于注意力的 NMT 已成为 NMT 历史上的一项重大创新。在这一新趋势中，注意力机制在文本特征提取中已占据主导地位，而不再是辅助组件。代表性模型是
    Transformer [[25](#bib.bib25)]，这是 Vaswani 等人提出的完全基于注意力的模型。
- en: Abandoning previous framework in neither RNN nor CNN based NMT models, Transformer
    is a that solely based on an intensified version of Attention Mechanism called
    Self-Attention with feed-forward connection, which got revolutionary progress
    in structure with state-of-the-art performance. Specifically, the innovative attention
    structure is the secret sauce to gain such significant improvement. The self-attention
    is a powerful feature extractor which also allows to ’read’ the entire sentence
    and model it once a time. In the perspective of model architecture, this character
    can be seen as a combination of advantages from both CNN and RNN, which endows
    it a good feature representation ability with high inference speed. More details
    about self-attention will be given in Section [IV](#S4 "IV NMT with Attention
    Mechanism ‣ A Survey of Deep Learning Techniques for Neural Machine Translation").
    The architecture of Transformer will be discussed in Section [VI](#S6 "VI Advanced
    models ‣ A Survey of Deep Learning Techniques for Neural Machine Translation").
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 摒弃了之前的 RNN 和 CNN 基础的 NMT 模型框架，采用了基于一种名为 Self-Attention 的注意力机制的强化版本，并配有前馈连接，其结构上取得了革命性的进展，并具备最先进的性能。具体而言，创新的注意力结构是获得显著改进的关键。自注意力是一种强大的特征提取器，它还允许一次“读取”整个句子并进行建模。从模型架构的角度来看，这一特性可以视为
    CNN 和 RNN 优势的结合，使其具备良好的特征表示能力和高推理速度。关于自注意力的更多细节将在第 [IV](#S4 "IV NMT with Attention
    Mechanism ‣ A Survey of Deep Learning Techniques for Neural Machine Translation")
    节中给出。Transformer 的架构将在第 [VI](#S6 "VI Advanced models ‣ A Survey of Deep Learning
    Techniques for Neural Machine Translation") 节中讨论。
- en: III DNN based NMT
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基于 DNN 的 NMT
- en: The emergence of DNN based NLM indicates the feasibility of building a pure
    DNN based translation model. The further implementation is the $defacto$ form
    of NMT in origin. This section reviews the basic concept of DNN based NMT, demonstrates
    a comprehensive introduction of the standard structure of the original DNN based
    NMT, and discusses the training and inferencing processes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 DNN 的 NLM 的出现表明构建纯 DNN 基础的翻译模型是可行的。进一步的实现是 NMT 的 $defacto$ 形式。本节回顾了基于 DNN
    的 NMT 的基本概念，展示了原始 DNN 基础的 NMT 标准结构的全面介绍，并讨论了训练和推理过程。
- en: III-A Model Design of DNN based NMT
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于 DNN 的 NMT 模型设计
- en: There are many variations of network design for NMT, which can be categorized
    into $recurrent$ or $non-recurrent$ models. More specifically, this category can
    be traced back to the early development of NMT, when RNN and CNN based models
    are the most common design. Many sophisticated models proposed afterwards also
    belong to either CNN or RNN family. This sub-section follows the development of
    NMT in the early years, and demonstrates some representative models by classifying
    them as RNN or CNN based models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 网络设计在NMT中有许多变体，可以分为$递归$模型或$非递归$模型。更具体地说，这一分类可以追溯到NMT早期的发展，当时基于RNN和CNN的模型是最常见的设计。随后提出的许多复杂模型也属于CNN或RNN家族。本小节跟随NMT早期的发展，并通过将其分类为基于RNN或CNN的模型来展示一些代表性模型。
- en: III-A1 RNN based NMT
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 基于RNN的NMT
- en: Although in theory, any network with enough feature extraction ability could
    be selected to build an NMT model, in $defacto$ implementations, RNN based NMT
    models have taken the dominant position in NMT development, and they have achieved
    state-of-the-art performance. Based on the discussion in Section [II](#S2 "II
    History of Machine Translation ‣ A Survey of Deep Learning Techniques for Neural
    Machine Translation"), since many NLM literature used RNN to model the sequence
    data, this design has intuitively motivated the further work to build an RNN based
    NMT model. In the initial experiment, an RNN based NLM was applied as a feature
    extractor to compress the source sentence into a feature vector, which is also
    referred to as thought vector. Then a similar RNN was applied to do the ’inverse
    work’ to find the target sentence that can match the previous thought vector in
    semantic space.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然理论上，任何具备足够特征提取能力的网络都可以用来构建NMT模型，但在$实际$实现中，基于RNN的NMT模型在NMT发展中占据了主导地位，并且达到了最先进的性能。根据[II](#S2
    "II History of Machine Translation ‣ A Survey of Deep Learning Techniques for
    Neural Machine Translation")节中的讨论，由于许多NLM文献使用RNN来建模序列数据，这一设计直观地激发了进一步工作以构建基于RNN的NMT模型。在初步实验中，基于RNN的NLM被用作特征提取器，将源句子压缩成特征向量，也称为思维向量。然后应用类似的RNN来进行“逆向工作”，以找到能够匹配前述思维向量的目标句子。
- en: The first successful RNN based NMT was proposed by Sutskever et al., who used
    a pure deep RNN model and got a performance that approximates the best result
    achieved by SMT[[16](#bib.bib16)]. Further development proposed the Attention
    Mechanism, which improves the translation performance significantly and exceeds
    the best SMT model. GNMT model was an industry-level model applied in Google Translation,
    and it was regarded as a milestone in RNN based NMT.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个成功的基于RNN的NMT由Sutskever等人提出，他们使用了纯深度RNN模型，取得了接近SMT最佳结果的性能[[16](#bib.bib16)]。进一步的发展提出了注意力机制，这显著提高了翻译性能，并超过了最佳的SMT模型。GNMT模型是一个在Google翻译中应用的行业级模型，被视为基于RNN的NMT的一个里程碑。
- en: Besides the above mentioned work, other researchers have also proposed different
    architectures with excellent performance. Zhang et al. proposed Variational NMT
    method, which has an innovative perspective in modeling translation task, and
    the corresponding experiment has indicated a better performance than the baseline
    of original NMT in Chinese-English and English-German tasks[[89](#bib.bib89)].
    Zhou et al. have designed Fast-Forward Connections for RNN (LSTM), which can allow
    a deeper network in implementation and thus gets a better performance[[88](#bib.bib88)].
    Shazeer et al. incorporated Mixture-of-Expert (MoE) architecture into the GNMT
    model, which has outperformed the original GNMT model[[90](#bib.bib90)]. Concretely,
    MoE is one layer in the NMT model, which contains many sparsely combined experts
    (which are feed-forward neural networks in this experiment) and is connected with
    the RNN layer by a gate function. This method requires more parameters in total
    for the NMT model, but still maintains the efficiency in training speed. Since
    more parameters often imply a better representation ability, it demonstrates huge
    potential in the future.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除上述工作外，其他研究人员还提出了具有卓越性能的不同架构。Zhang 等人提出了变分NMT方法，该方法在建模翻译任务时具有创新视角，相关实验表明其在中英和英德任务中的表现优于原始NMT的基线[[89](#bib.bib89)]。Zhou
    等人设计了RNN（LSTM）的快速前馈连接，这使得网络实现更深，从而获得了更好的性能[[88](#bib.bib88)]。Shazeer 等人将混合专家（MoE）架构融入GNMT模型中，超越了原始GNMT模型[[90](#bib.bib90)]。具体而言，MoE是在NMT模型中的一层，包含多个稀疏组合的专家（在本实验中为前馈神经网络），并通过门控函数与RNN层连接。这种方法需要更多的总参数，但仍保持训练速度的效率。由于更多参数通常意味着更好的表示能力，它在未来展示了巨大的潜力。
- en: III-A2 CNN based NMT
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 基于CNN的NMT
- en: Related work in trying other DNN models have also been proposed. Perhaps the
    most noted one is the Convolutional Neural Network (CNN) based NMT. In fact, CNN
    based models have also undergone many variations in its concrete architecture.
    But for a long while, most of these models can’t have competitive performance
    with RNN based model, especially when the Attention Mechanism has emerged.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试其他DNN模型的相关工作也已提出。也许最著名的是基于卷积神经网络（CNN）的NMT。实际上，基于CNN的模型在具体架构上也经历了许多变化。但很长一段时间内，这些模型大多无法与基于RNN的模型竞争，特别是在注意力机制出现之后。
- en: In the development of CNN based NMT models, Kalchbrenner & Blunsom once tried
    a CNN encoder with RNN Decoder [[8](#bib.bib8)], and it’s maybe the earliest NMT
    architecture applied with CNN. Cho et al. tried a gated recursive CNN encoder
    with RNN decoder, but it has shown worse performance than RNN encoder[[18](#bib.bib18)].
    A fully CNN based NMT was proposed by Kaiser & Bengio later[[86](#bib.bib86)],
    which applied Extended Neural GPU[[119](#bib.bib119)]. The best performance in
    the early period of CNN based NMT was achieved by Gehring et al., which was a
    CNN encoder NMT and got the similar translation performance with RNN based model
    at that time[[19](#bib.bib19)]. Concurrently, Kalchbrenner et al. also proposed
    ByteNet (a kind of CNN) based NMT, which achieved the state-of-the-art performance
    on character-level translation but failed at word-level translation[[84](#bib.bib84)].
    In addition, Meng et al. and Tu et al. proposed a CNN based model separately,
    which provides additional alignment information for SMT [[20](#bib.bib20)][[83](#bib.bib83)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于CNN的NMT模型开发中，Kalchbrenner 和 Blunsom 曾尝试过一个带有RNN解码器的CNN编码器[[8](#bib.bib8)]，这可能是最早应用CNN的NMT架构。Cho
    等人尝试了一个带有RNN解码器的门控递归CNN编码器，但表现比RNN编码器更差[[18](#bib.bib18)]。后来，Kaiser 和 Bengio 提出了一个完全基于CNN的NMT模型[[86](#bib.bib86)]，并应用了扩展神经GPU[[119](#bib.bib119)]。在CNN基于NMT的早期阶段，Gehring
    等人取得了最佳性能，他们的CNN编码器NMT与当时的RNN模型翻译性能相似[[19](#bib.bib19)]。与此同时，Kalchbrenner 等人还提出了基于ByteNet（一种CNN）的NMT，该模型在字符级翻译上达到了最先进的性能，但在词级翻译上表现不佳[[84](#bib.bib84)]。此外，Meng
    等人和 Tu 等人分别提出了基于CNN的模型，为SMT提供了额外的对齐信息[[20](#bib.bib20)][[83](#bib.bib83)]。
- en: Compared with RNN based NMT, CNN based models have its advantage in training
    speed; this is due to the intrinsic structure of CNN which allows parallel computations
    for its different filters when handling the input data. And also, the model structure
    has made CNN based models easier to resolve the gradient vanishing problem. However,
    there are two fatal drawbacks that affect their translation quality. First, since
    the original CNN based model can only capture the word dependencies within the
    width of its filters, the long dependency of words can only be found in high-level
    convolution layers; this unnatural character often causes a worse performance
    than the RNN based model. Second, since the original NMT model compresses a sentence
    into a fixed size of the vector, a large performance reduction would happen when
    the sentence becomes too long. This comes from the limited representation ability
    in fixed size of the vector. Similar phenomenon can also be found in early proposed
    RNN based models, which are later alleviated by Attention Mechanism.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于 RNN 的 NMT 相比，基于 CNN 的模型在训练速度上具有优势；这是因为 CNN 的固有结构允许在处理输入数据时对不同的滤波器进行并行计算。而且，模型结构使得基于
    CNN 的模型更容易解决梯度消失问题。然而，它们的翻译质量受到两个致命缺陷的影响。首先，由于原始的基于 CNN 的模型只能捕捉滤波器宽度内的词依赖关系，词的长距离依赖只能在高级卷积层中找到；这种不自然的特性通常导致比基于
    RNN 的模型更差的性能。其次，由于原始的 NMT 模型将句子压缩为固定大小的向量，当句子变得过长时，性能会大幅下降。这是由于固定大小向量的表示能力有限。类似的现象也可以在早期提出的基于
    RNN 的模型中发现，这些问题后来通过注意力机制得到缓解。
- en: Some advanced CNN based NMT models have also been proposed with corresponding
    solutions in addressing the above drawbacks. Kaiser et al. proposed the Depthwise
    separable convolutions based NMT. The SliceNet they created can get similar performance
    with Kaiser et al. (2016) [[85](#bib.bib85)]. Gehring et al. (2017) followed their
    previous work by proposing a CNN based NMT that is cooperated with Attention Mechanism.
    It even got a better result than RNN based model[[82](#bib.bib82)], but this achievement
    was soon outperformed by Transformer [[25](#bib.bib25)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一些先进的基于 CNN 的 NMT 模型也已经提出，并针对上述缺陷提供了相应的解决方案。Kaiser 等人提出了基于深度可分离卷积的 NMT。他们创建的
    SliceNet 可以获得与 Kaiser 等人（2016）[[85](#bib.bib85)] 相似的性能。Gehring 等人（2017）在其之前的工作基础上，提出了一种与注意力机制协作的基于
    CNN 的 NMT。它甚至取得了比基于 RNN 的模型更好的结果[[82](#bib.bib82)]，但这一成就很快被 Transformer [[25](#bib.bib25)]
    超越。
- en: III-B Encoder-Decoder Structure
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 编码器-解码器结构
- en: As is known, Encoder-Decoder is the most original and classic structure of NMT;
    it was directly inspired by NLM and proposed by Kalchbrenner & Blunsom[[8](#bib.bib8)]
    and Cho et al.[[18](#bib.bib18)]. Despite all kinds of refinements in details
    and small tips, it was wildly accepted by almost all modern NMT models. Based
    on the discussion above, since RNN based NMT has held the dominant position in
    NMT, and to avoid being overwhelmed in describing all kinds of small distinctions
    between models’ structures, we specifically focus our discussion just on the vanilla
    RNN based NMT, thus can help to trace back the development process of NMT.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，编码器-解码器是 NMT 最原始和经典的结构；它直接受到 NLM 的启发，由 Kalchbrenner 和 Blunsom[[8](#bib.bib8)]
    以及 Cho 等人[[18](#bib.bib18)] 提出。尽管在细节和小技巧上进行了各种改进，但几乎所有现代 NMT 模型都广泛接受了它。基于上述讨论，由于基于
    RNN 的 NMT 在 NMT 中占据主导地位，为了避免在描述模型结构之间各种小区别时被压倒，我们特别关注基础的基于 RNN 的 NMT，从而有助于追溯 NMT
    的发展过程。
- en: The original structure of Encoder-Decoder structure is conceptually simple.
    It contains two connected networks (the encoder and the decoder) in its architecture,
    each for a different part of the translation process. When the encoder network
    receives one source sentence, it reads the source sentence word by word and compresses
    the variable-length sequence into a fixed-length vector in each hidden state.
    This process is called encoding. Then given the final hidden state of the encoder
    (referred to as thought vector), the decoder does the reverse work by transforming
    the thought vector to the target sentence word by word. Because Encoder-Decoder
    structure addresses the translation task from source data directly to the target
    result, which means there’s no visible result in the middle process, this is also
    called end-to-end translation. The principle of Encoder-Decoder structure of NMT
    can be seen as mapping the source sentence with the target sentence via an intermediate
    vector in semantic space. This intermediate vector actually can represent the
    same semantic meaning in both two languages.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器结构的原始结构在概念上是简单的。它的架构包含两个相连的网络（编码器和解码器），每个网络负责翻译过程中的不同部分。当编码器网络接收到一个源句子时，它逐字读取源句子，并将变长序列压缩成每个隐藏状态中的固定长度向量。这一过程称为编码。然后，解码器根据编码器的最终隐藏状态（称为思维向量）执行反向工作，将思维向量逐字转换为目标句子。由于编码器-解码器结构将翻译任务直接从源数据到目标结果，这意味着中间过程没有可见结果，这也被称为端到端翻译。NMT的编码器-解码器结构的原理可以看作是通过语义空间中的中间向量将源句子映射到目标句子。这个中间向量实际上可以在两种语言中表示相同的语义含义。
- en: 'For specific details of this structure, besides the model selection in the
    network, RNN based NMT models also differ in three main terms: (a) the directionality;
    (b) the type of activation function; and (c) the depth of RNN layer[[156](#bib.bib156)].
    In the following, we give a detailed description.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种结构的具体细节，除了网络中的模型选择外，基于RNN的NMT模型还在三个主要方面有所不同：（a）方向性；（b）激活函数的类型；（c）RNN层的深度[[156](#bib.bib156)]。接下来，我们将给出详细描述。
- en: 'Depth: For the depth of RNN, as we discussed in Section [II](#S2 "II History
    of Machine Translation ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation"), single layer RNN usually performs poorly comparing with multi-layer
    RNN. In recent years, almost all the models with competitive performance are using
    a deep network, which has indicated a trend of using a deeper model to get the
    state-of-the-art result. For example, Bahdanau et al.[[21](#bib.bib21)] used four
    layers RNN in their model.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 深度：对于RNN的深度，如我们在第[II](#S2 "II History of Machine Translation ‣ A Survey of Deep
    Learning Techniques for Neural Machine Translation")节中讨论的，单层RNN通常比多层RNN表现较差。近年来，几乎所有具有竞争力的模型都使用深度网络，这表明使用更深的模型以获得最先进的结果是一种趋势。例如，Bahdanau等人[[21](#bib.bib21)]在他们的模型中使用了四层RNN。
- en: However, simply increasing more layers of RNN may not always be useful. In the
    proof proposed by Britz et al.[[22](#bib.bib22)], they found that using 4 layers
    RNN in the encoder for specific dataset would produce the best performance when
    there is no other auxiliary method in the whole model. Besides that, stacking
    RNN layers may make the network become too slow and difficult to train. One major
    challenges is the gradient exploding and vanishing problem[[28](#bib.bib28)],
    which will cause the gradient be amplified or diminished when processing back
    propagation in deep layers. Besides the additional gate structure in refined RNN
    (like LSTM and GRU), other methods have also been applied to alleviate this phenomenon.
    For example, in Wu et al.’s work, the residual connections are provided between
    layer, which can improve the value of gradient flow in the backward pass, thus
    can speed up the convergence process[[24](#bib.bib24)]. Another possible problem
    is that a deeper model often indicates larger model capacity, which may perform
    worse on comparatively less training data due to the over-fitting.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，简单地增加更多的 RNN 层可能并不总是有效。在 Britz 等人提出的证明中[[22](#bib.bib22)]，他们发现，在编码器中使用 4
    层 RNN 对于特定数据集会产生最佳性能，当整个模型中没有其他辅助方法时。除此之外，堆叠 RNN 层可能会使网络变得过于缓慢且难以训练。一个主要的挑战是梯度爆炸和消失问题[[28](#bib.bib28)]，这会导致梯度在深层处理反向传播时被放大或减小。除了精炼的
    RNN（如 LSTM 和 GRU）中的附加门结构外，还有其他方法也已被应用以缓解这种现象。例如，在 Wu 等人的工作中，层之间提供了残差连接，这可以提高反向传播中的梯度流值，从而加快收敛过程[[24](#bib.bib24)]。另一个可能的问题是，更深的模型通常意味着更大的模型容量，这可能由于过拟合而在相对较少的训练数据上表现较差。
- en: 'Directionality: In respect of directionality, a simple unidirectional RNN has
    been chosen by some researchers. For example, Luong et al. have directly used
    unidirectional RNN to accept the input sentence[[23](#bib.bib23)]. In comparison,
    bidirectional RNN is another common choice that can empower the translation quality.
    This is because the model performance is affected by whether it ’knows’ well about
    the information in context word when predicting current word. A bidirectional
    RNN obviously could strengthen this ability.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 方向性：在方向性方面，一些研究人员选择了简单的单向 RNN。例如，Luong 等人直接使用单向 RNN 接受输入句子[[23](#bib.bib23)]。相比之下，双向
    RNN 是另一个常见的选择，可以增强翻译质量。这是因为模型的表现受到在预测当前词时是否“了解”上下文信息的影响。双向 RNN 显然可以增强这种能力。
- en: 'In practice, both Bahdanau et al. and Wu et al. used bidirectional RNN on the
    bottom layer as an alternative to capture the context information[[21](#bib.bib21)][[24](#bib.bib24)].
    In this structure, the first layer reads the sentence “left to right”, and the
    second layer reads the sentence in a reverse direction. Then they are concatenated
    and fed to the next layer. This method generally has a better performance in experiment,
    although the explanation is intuitive: Based on the discussion of LM in Section [II](#S2
    "II History of Machine Translation ‣ A Survey of Deep Learning Techniques for
    Neural Machine Translation"), the emergence probability of a specific word is
    determined by all the other words in both the prior and the post positions. When
    applying unidirectional RNN, word dependency between the first word and the last
    word is hard to be captured by the thought vector, since the model has experienced
    too many states in all time steps. On the country, bidirectional RNN provides
    an additional layer of information with reverse direction of reading words, which
    could naturally reduce this relative length within steps.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，Bahdanau 等人和 Wu 等人都在底层使用了双向 RNN 作为捕捉上下文信息的替代方案[[21](#bib.bib21)][[24](#bib.bib24)]。在这种结构中，第一层从“左到右”读取句子，第二层则从反方向读取句子。然后将它们连接起来，输入到下一层。这种方法在实验中通常具有更好的表现，尽管解释是直观的：根据第[II](#S2
    "II History of Machine Translation ‣ A Survey of Deep Learning Techniques for
    Neural Machine Translation)节对语言模型的讨论，一个特定词的出现概率由其前后所有其他词决定。当应用单向 RNN 时，首词和末词之间的词依赖关系难以通过思维向量捕捉，因为模型在所有时间步经历了太多状态。相反，双向
    RNN 提供了一个额外的信息层，通过反向阅读单词，这自然可以在步骤之间缩短相对长度。
- en: The most visible drawback of this method is that it’s hard to be paralleled,
    considering the time-consuming in its realization, both Bahdanau et al. and Wu
    et al. choose to apply just one layer bidirectional RNN in the bottom layer of
    the encoder, and other layers are all unidirectional layers[[24](#bib.bib24)][[21](#bib.bib21)].
    This choice makes a trade-off between the feature representation ability with
    model efficiency, due to it can still enable the model to be distributed on multi
    GPUs[[24](#bib.bib24)]. The basic concept of bidirectional RNN could find in Fig. [2](#S2.F2
    "Figure 2 ‣ II-B2 Formulation of NMT Task ‣ II-B Introduction of Neural Machine
    Translation ‣ II History of Machine Translation ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation").
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法最明显的缺点是难以并行化，考虑到其实现过程中的时间消耗，Bahdanau等人和Wu等人选择在编码器的底层只应用一层双向RNN，其他层均为单向层[[24](#bib.bib24)][[21](#bib.bib21)]。这种选择在特征表示能力与模型效率之间做出了权衡，因为它仍能使模型分布在多个GPU上[[24](#bib.bib24)]。双向RNN的基本概念可以在图[2](#S2.F2
    "Figure 2 ‣ II-B2 Formulation of NMT Task ‣ II-B Introduction of Neural Machine
    Translation ‣ II History of Machine Translation ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation")中找到。
- en: 'Activation Function Selection: In respect of activation function selection,
    there are three common choices: vanilla RNN, Long Short Term Memory (LSTM) [[17](#bib.bib17)],
    and Gated Recurrent Unit (GRU) [[18](#bib.bib18)]. Comparing with the vanilla
    RNN, both the last two have some robustness in addessing the gradient exploding
    and vanishing problem[[27](#bib.bib27)][[28](#bib.bib28)]. Another sequence processing
    task has also indicated better performance achieved by GRU and LSTM[[26](#bib.bib26)].
    Besides, some innovative neural units have been proposed. Wang et al. proposed
    linear associative units, which can alleviate the gradient diffusion phenomenon
    in non-linear recurrent activation[[92](#bib.bib92)]. More recently, Zhang et
    al. have created addition-subtraction twin-gated recurrent network (ATR). This
    type of unit reduces the inefficiency in NMT training and inference by simplifying
    the weight matrices among units [[91](#bib.bib91)]. All in all, in NMT task, LSTM
    is the most common choice.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数选择：关于激活函数的选择，常见的有三种选择：普通RNN、长短期记忆（LSTM）[[17](#bib.bib17)]，以及门控递归单元（GRU）[[18](#bib.bib18)]。与普通RNN相比，后两者在解决梯度爆炸和消失问题方面具有一些鲁棒性[[27](#bib.bib27)][[28](#bib.bib28)]。其他序列处理任务也表明，GRU和LSTM能够取得更好的性能[[26](#bib.bib26)]。此外，还提出了一些创新的神经单元。Wang等人提出了线性关联单元，这可以缓解非线性递归激活中的梯度扩散现象[[92](#bib.bib92)]。最近，Zhang等人创建了加减双重门控递归网络（ATR）。这种单元通过简化单元间的权重矩阵来减少NMT训练和推理中的低效问题[[91](#bib.bib91)]。总的来说，在NMT任务中，LSTM是最常见的选择。
- en: '![Refer to caption](img/0be7ef4f7252472453d075e81a1dc11e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0be7ef4f7252472453d075e81a1dc11e.png)'
- en: 'Figure 3: The concept of Bidirectional RNN'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：双向RNN的概念
- en: III-C Training method
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 训练方法
- en: Before feeding the training data to the model, one pre-step is to transfer the
    words to vectors, which makes a proper form that the neural network could receive.
    Usually, the most frequent $V$ words in one language will be chosen, and each
    language generally has different word set. Despite that the embedding weights
    will be learned in the training period, the pre-trained word embedding vector
    such as word2vec[[9](#bib.bib9)][[10](#bib.bib10)] or Glove vector[[11](#bib.bib11)]
    can also be applied directly.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在将训练数据输入模型之前，一个前置步骤是将单词转换为向量，这使得神经网络可以接受适当的形式。通常，选择某种语言中最频繁出现的$V$个单词，每种语言通常具有不同的词汇集合。尽管在训练期间会学习嵌入权重，但也可以直接应用预训练的词嵌入向量，例如word2vec[[9](#bib.bib9)][[10](#bib.bib10)]或Glove向量[[11](#bib.bib11)]。
- en: In the training period, this model is fed by a bilingual corpus for Encoder
    and Decoder. The learning objective is to map the input sequence with the corresponding
    sequence in the target language correctly. Like other DNN models, the input sentence
    pair is embedded as a list of word vectors, and the model parameters are initialized
    randomly. The training process could be formulated as trying to updating its parameters
    periodically until getting the minimum loss of the neural network. In the implementation,
    RNN will refine the parameters after it processes a subset of data that contain
    a batch of training samples; this subset is called the mini-batch set. To simplify
    the discussion of the training process, we take one sentence pair (one training
    sample) as example.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，该模型由双语语料库供给给编码器和解码器。学习目标是将输入序列正确地映射到目标语言中的相应序列。与其他DNN模型一样，输入句子对被嵌入为单词向量的列表，模型参数随机初始化。训练过程可以表述为试图周期性地更新其参数，直到达到神经网络的最小损失。在实现中，RNN会在处理包含一批训练样本的数据子集后调整参数；这个子集称为迷你批量集。为了简化训练过程的讨论，我们以一个句子对（一个训练样本）作为示例。
- en: For the Encoder, the encoding RNN will receive one word in source sentence once
    a time-step. After several steps, all words will be compressed into the hidden
    state of the Encoder. Then the final vector will be transferred to the Decoder.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编码器，编码RNN会在每个时间步接收源句子中的一个单词。经过几个步骤，所有单词将被压缩到编码器的隐藏状态中。然后，将最终向量传输到解码器。
- en: 'For Decoder, the input comes from two sources: the thought vector that is directly
    sent to Decoder, and the correct word in the last time-step (the first word is
    $<EOS>$). The output process in Decoder can be seen as a reverse work of Encoder;
    Decoder predicts one word in each time-step until the last symbol is $<EOS>$.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解码器，输入来自两个来源：直接发送到解码器的思维向量和上一个时间步的正确单词（第一个单词是$<EOS>$）。解码器中的输出过程可以看作是编码器的逆过程；解码器在每个时间步预测一个单词，直到最后一个符号是$<EOS>$。
- en: III-D Inference method
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 推理方法
- en: 'After the training period, the model could be used for translation, which is
    called inference. The inference procedure is quite similar to the training process.
    Nevertheless, there is still a clear distinction between training and inference:
    at decoding time, we only have access to the source sentence, i.e., encoder hidden
    state.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期之后，模型可以用于翻译，这被称为推理。推理过程与训练过程非常相似。然而，训练和推理之间仍然存在明显的区别：在解码时，我们只能访问源句子，即编码器隐藏状态。
- en: There is more than one way to perform decoding. Proposed decoding strategies
    include Sampling and Greedy search, while the latter one is generally accepted
    and be evolved as Beam-search.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 解码可以有多种方式。提出的解码策略包括采样和贪婪搜索，而后者通常被接受并发展为束搜索。
- en: III-D1 General decoding work flow (greedy)
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D1 一般解码工作流程（贪婪）
- en: 'The idea of greedy strategy is simple, as we illustrate in Fig. [4](#S3.F4
    "Figure 4 ‣ III-D1 General decoding work flow (greedy) ‣ III-D Inference method
    ‣ III DNN based NMT ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation"). The Greedy strategy is only considering the predicted word with
    the highest probability. In the implementation of our illustration, the previously
    generated word would also be fed to the network together with the thought vector
    in the next time-step. The detailed steps are as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪策略的思想很简单，正如图[4](#S3.F4 "Figure 4 ‣ III-D1 General decoding work flow (greedy)
    ‣ III-D Inference method ‣ III DNN based NMT ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation")中所示。贪婪策略仅考虑概率最高的预测单词。在我们的示例实现中，之前生成的单词也会与思维向量一起输入到网络中，作为下一时间步的输入。详细步骤如下：
- en: '![Refer to caption](img/e7be4d0ca1e6b8c6aa9ec0e0bc99fb26.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e7be4d0ca1e6b8c6aa9ec0e0bc99fb26.png)'
- en: 'Figure 4: The process of greedy decoding: each time the model would predict
    the word with highest probability, and use the current result as the input in
    next time step to get further prediction'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：贪婪解码过程：每次模型都会预测概率最高的单词，并将当前结果作为下一时间步的输入，以获得进一步的预测
- en: 1\. The model still encodes the source sentence in the same way as during the
    training period to obtain the thought vector, and this thought vector is used
    to initialize the decoder.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 模型仍以与训练期间相同的方式对源句子进行编码，以获得思维向量，并使用该思维向量初始化解码器。
- en: 2\. The decoding (translation) process will start as soon as the decoder receives
    the end-of-sentence marker $<EOS>$ of source sentence.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 解码（翻译）过程将在解码器收到源句子的结束符号 $<EOS>$ 后立即开始。
- en: 3\. For each time-step on the decoder side, we treat the RNN’s output as a set
    of logits. We choose the word with the highest translation probability as the
    emitted word, whose ID is associated with the maximum logit value. For example,
    in Fig. [4](#S3.F4 "Figure 4 ‣ III-D1 General decoding work flow (greedy) ‣ III-D
    Inference method ‣ III DNN based NMT ‣ A Survey of Deep Learning Techniques for
    Neural Machine Translation"), the word “moi” has the highest probability in the
    first decoding step. We then feed this word as an input in the next time-step.
    The probability is thus conditioned on the previous prediction (this is why we
    call it “greedy” behavior).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解码器端的每个时间步，我们将 RNN 的输出视为一组 logits。我们选择具有最高翻译概率的词作为发射词，其 ID 与最大 logit 值相关联。例如，在图[4](#S3.F4
    "图 4 ‣ III-D1 一般解码工作流程（贪婪） ‣ III-D 推断方法 ‣ III 基于 DNN 的 NMT ‣ 深度学习技术在神经机器翻译中的应用调查")中，词“moi”在第一次解码步骤中具有最高的概率。然后我们将这个词作为输入在下一个时间步中进行处理。概率因此依赖于之前的预测（这就是我们称之为“贪婪”行为的原因）。
- en: 4\. The process will continue until the ending symbol $<EOS>$ is generated as
    an output symbol.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 过程将继续进行，直到生成结束符号 $<EOS>$ 作为输出符号。
- en: III-D2 Beam-search
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D2 Beam-search
- en: While the Greedy search method has produced a pretty good result, Beam-search
    is a more elaborated one with better results. Although it is not a necessary component
    for NMT, Beam-search has been chosen by most of NMT models to get the best performance[[22](#bib.bib22)].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然贪婪搜索方法已产生了相当好的结果，但 beam-search 是一种更精细的方法，效果更佳。虽然它不是神经机器翻译的必要组件，但大多数神经机器翻译模型选择了
    beam-search 以获得最佳性能[参考文献22](#bib.bib22)。
- en: The beam-search method was proposed by other sequence learning task with successful
    application[[29](#bib.bib29)][[30](#bib.bib30)]. It’s also the conventional technique
    of MT task that has been used for years in finding the most appropriate translation
    result[[34](#bib.bib34)][[32](#bib.bib32)][[33](#bib.bib33)]. Beam-search can
    be simply described as retaining the top-$k$ possible translations as candidates
    at each time, where $k$ is called the beam-width. In the next time-step, each
    candidate word would be combined with a new word to form new possible translation.
    The new candidate translation would then compete with each other in log probability
    to get the new top-$k$ most reasonable results. The whole process continues until
    the end of translation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: beam-search 方法由其他序列学习任务提出，并成功应用于[参考文献29](#bib.bib29)和[参考文献30](#bib.bib30)。这也是机器翻译任务中常用的传统技术，多年来一直用于寻找最合适的翻译结果[参考文献34](#bib.bib34)、[参考文献32](#bib.bib32)、[参考文献33](#bib.bib33)。beam-search
    可以简单描述为在每个时刻保留前$k$个可能的翻译作为候选，其中 $k$ 被称为 beam-width。在下一个时间步中，每个候选词将与一个新词结合形成新的可能翻译。新的候选翻译将通过对数概率进行竞争，以获得新的前$k$个最合理的结果。整个过程持续进行直到翻译结束。
- en: 'Concretely, the beam search can be formulated in the following steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，beam search 可以通过以下步骤来表述：
- en: Algorithm 1 Beam Search
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 Beam Search
- en: set Beamsize = K;  $h_{0}\Leftarrow encoder(S)$  $t\Leftarrow 1$  //  $L_{S}$
    means length of source sentence;  //  $\alpha$ is Length factor;  while $n\leq\alpha*L_{S}$ do     $y_{1,i}\Leftarrow\
    <EOS>$     while $i\leq K$ do        set $h_{t}\Leftarrow\ decoder(h_{t-1},y_{t,i})$;        set
    $P_{t,i}=Softmax(y_{t,i})$;        set $y_{t+1,i}\Leftarrow argTop\_K(P_{t,i})$;        set
    $i=i+1$     end while     set $i=0$     if $h_{t}==\ <EOS>$ then        break;     end if     set
    $t=t+1$  end while  select $argmax(p(Y))$ from $K$ candidates $Y_{i}$  return
     $Y_{i}$
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Beamsize = K； $h_{0}\Leftarrow encoder(S)$  $t\Leftarrow 1$  //  $L_{S}$
    表示源句子的长度； //  $\alpha$ 是长度因子；  当 $n\leq\alpha*L_{S}$ 时，  $y_{1,i}\Leftarrow\ <EOS>$  当
    $i\leq K$ 时，  设置 $h_{t}\Leftarrow\ decoder(h_{t-1},y_{t,i})$；  设置 $P_{t,i}=Softmax(y_{t,i})$；  设置
    $y_{t+1,i}\Leftarrow argTop\_K(P_{t,i})$；  设置 $i=i+1$  结束 循环  设置 $i=0$  如果 $h_{t}==\
    <EOS>$ 则  退出；  结束 如果  设置 $t=t+1$  结束 循环  从 $K$ 个候选 $Y_{i}$ 中选择 $argmax(p(Y))$  返回
    $Y_{i}$
- en: Besides the standard Beam-search which finds the candidate translation only
    by sorting log probability, this evaluation function mathematically tends to find
    shorter sentence. This is because a negative log-probability would be added at
    each decoding step, which lowers the scores with the increasing length of sentences[[31](#bib.bib31)].
    An efficient variant for alleviating this scenario is to add a length normalization[[7](#bib.bib7)].
    A refined length normalization was also proposed by Wu et al.[[24](#bib.bib24)].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准的 Beam-search 仅通过排序对数概率来找到候选翻译外，该评估函数在数学上倾向于找到更短的句子。这是因为在每一步解码时都会添加一个负对数概率，这会随着句子长度的增加而降低分数[[31](#bib.bib31)]。缓解这种情况的有效变体是添加长度归一化[[7](#bib.bib7)]。Wu
    等人也提出了一种改进的长度归一化方法[[24](#bib.bib24)]。
- en: Another kind of refined method in Beam-search is adding coverage penalty, which
    helps to encourage the decoder to cover the words in the source sentence as much
    as possible when generating an output sentence[[24](#bib.bib24)][[35](#bib.bib35)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Beam-search 中另一种精炼的方法是添加覆盖惩罚，这有助于鼓励解码器在生成输出句子时尽可能覆盖源句子中的单词[[24](#bib.bib24)][[35](#bib.bib35)]。
- en: In addition, since this method finds $k$ times of translation(rather than one)
    until getting the final result, it generally makes the decoding process more time-consuming.
    In practice, an intuitive solution is to limit the beam-width as a small constant,
    which is a trade-off between the decoding efficiency and the translation accuracy.
    As reported by a comparison work, an experimental beam width for best performance
    is 5 to 10[[22](#bib.bib22)].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于该方法在获得最终结果之前会找到 $k$ 次翻译（而不是一次），这通常会使解码过程更耗时。实际上，一个直观的解决方案是将 beam-width
    限制为一个小常数，这在解码效率和翻译准确性之间进行权衡。根据一项对比工作报告，最佳性能的实验 beam-width 为 5 到 10[[22](#bib.bib22)]。
- en: IV NMT with Attention Mechanism
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 带有注意机制的 NMT
- en: IV-A Motivation of Attention Mechanism
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 注意机制的动机
- en: While the promising performance of NMT has indicated its great potential in
    capturing the dependencies inside the sequence, in practice, NMT still suffers
    a huge performance reduction when the source sentence becomes too long. Comparing
    with other feature extractors, the major weakness of the original NMT Encoder
    is that it has to compress one sentence into a fixed-length vector. When the input
    sentence becomes longer, the performance deteriorates because the final output
    of the network is a fixed-length vector, which may have limitation in representing
    the whole sentence and cause some information loss. And because of the limited
    length of vector, this information loss usually covers the long-range dependencies
    of words. While increasing the dimension of encoding vector is an intuitive solution,
    since the RNN training speed is naturally slow, a larger vector size would cause
    an even worse situation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 NMT 的良好表现表明其在捕捉序列内依赖关系方面具有巨大潜力，但实际上，当源句子过长时，NMT 仍然会遭遇巨大的性能下降。与其他特征提取器相比，原始
    NMT 编码器的主要弱点在于它必须将一个句子压缩成一个固定长度的向量。当输入句子变得更长时，性能会恶化，因为网络的最终输出是一个固定长度的向量，这可能在表示整个句子时有局限性并导致一些信息丢失。由于向量长度有限，这种信息丢失通常覆盖了词汇的长距离依赖关系。虽然增加编码向量的维度是一个直观的解决方案，但由于
    RNN 训练速度自然较慢，较大的向量尺寸可能会导致更糟的情况。
- en: Attention Mechanism emerged under this circumstance. Bahdanau et al. [[21](#bib.bib21)]
    initially used this method as a supplement that can provide additional word alignment
    information in the decoding process, thus can alleviate the information reduction
    when the input sentence is too long. Concretely, Attention Mechanism is an intermediate
    component between Encoder and Decoder, which can help to determine the word correlation
    (word alignment information) dynamically. In the encoding period, it extends the
    vector of the final state in the original NMT model with a weighted average of
    hidden state in each time state, and a score function is provided to get the weight
    we mention above by calculating the correlation of each word in source sentence
    with the current predicting word. Thus the decoder could adapt its concentration
    in different translation steps by ordering the importance of each word correlation
    in source sentence, and this method can help to capture the long-range dependencies
    for each word respectively.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在这种情况下应运而生。Bahdanau等人[[21](#bib.bib21)]最初使用这种方法作为一种补充，可以在解码过程中提供额外的词对齐信息，从而缓解输入句子过长时的信息减少。具体而言，注意力机制是编码器和解码器之间的一个中间组件，能够动态地确定词的相关性（词对齐信息）。在编码阶段，它通过对每个时间状态的隐藏状态进行加权平均，扩展了原始NMT模型中最终状态的向量，并提供了一个得分函数，通过计算源句子中每个词与当前预测词的相关性来获得上述权重。因此，解码器可以在不同的翻译步骤中通过排序源句子中每个词的相关性的重要性来调整其集中度，这种方法有助于分别捕捉每个词的长程依赖关系。
- en: The inspiration for applying the Attention Mechanism on NMT comes from human
    behavior in reading and translating the text data. People generally read text
    repeatedly for mining the dependency within the sentence, which means each word
    has different dependency weight with each other. Comparing with other models in
    capturing word dependency information such as pooling layer in CNN or N-gram language
    model, attention mechanism has a global scope. When finding the dependency in
    one sequence, $N$-gram model will fix its the searching scope in a small range,
    usually the $N$ is equal to 2 or 3 in practice. Attention Mechanism, on the other
    hand, calculates the dependency between the current generating word with other
    words in source sentence. This more flexible method obviously bring a better result.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在NMT中应用注意力机制的灵感来自于人类在阅读和翻译文本数据时的行为。人们通常会反复阅读文本以挖掘句子中的依赖关系，这意味着每个词之间具有不同的依赖权重。与捕捉词语依赖信息的其他模型（如CNN中的池化层或N-gram语言模型）相比，注意力机制具有全局范围。在寻找一个序列中的依赖关系时，$N$-gram模型会将搜索范围固定在一个较小的范围内，通常实践中$N$等于2或3。而注意力机制则计算当前生成词与源句子中其他词之间的依赖关系。这种更灵活的方法显然带来了更好的结果。
- en: The practical application of Attention Mechanism is actually far beyond the
    NMT field, and it is even not an invention in NMT development. Some other tasks
    have also proposed similar methods that give weighted concentration on different
    position of input data, for example, Xu et al[[109](#bib.bib109)]. proposed similar
    mechanism in handling image caption task, which can helps to dynamically locate
    different entries in image feature vector when generating description of them.
    Due to the scope of this survey, the following discussion would only focus on
    the Attention Mechanism in NMT.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的实际应用远远超出了NMT领域，甚至在NMT的发展中并不是一种发明。其他任务也提出了类似的方法，给输入数据的不同位置赋予权重，例如，Xu等[[109](#bib.bib109)]在处理图像描述任务时提出了类似的机制，这可以帮助在生成描述时动态地定位图像特征向量中的不同条目。由于本调查的范围，以下讨论将仅专注于NMT中的注意力机制。
- en: IV-B Structure of Attention Mechanism
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 注意力机制的结构
- en: There are many variants in the implementation of Attention Mechanism. Here we
    just give the detailed description of Attention Mechanism which has been widely
    accepted as bringing significant contribution in the development of NMT.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的实现有许多变体。这里我们仅详细描述了被广泛接受并在NMT发展中做出重要贡献的注意力机制。
- en: IV-B1 basic structure
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 基本结构
- en: The structure of attention mechanism was originally proposed by Bahdanau et
    al. In later, Luong et al. proposed similar structure with small distinctions
    and extends this work[[23](#bib.bib23)][[21](#bib.bib21)].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的结构最初由Bahdanau等人提出。后来，Luong等人提出了类似的结构，并对其进行了扩展[[23](#bib.bib23)][[21](#bib.bib21)]。
- en: To simplify the discussion, here we take Luong et al.’s method as an example.
    Concretely, in encoding period, this mechanism receiving the input words like
    the basic NMT model, but instead of compressing all the information in one vector,
    every unit in the top layer of encoder will generate one vector that represents
    one time-step in the source sentence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化讨论，这里以Luong等人的方法为例。具体来说，在编码阶段，这一机制接收输入词汇，类似于基本的NMT模型，但不是将所有信息压缩到一个向量中，而是编码器顶层的每个单元将生成一个代表源句子中每个时间步的向量。
- en: '![Refer to caption](img/a16ae4618dfae61d4816efaf76786abb.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a16ae4618dfae61d4816efaf76786abb.png)'
- en: 'Figure 5: The concept of Attention Mechanism,which can provide additional alignment
    information rather than just using information in fixed-length of vector'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：注意力机制的概念，它可以提供额外的对齐信息，而不仅仅是使用固定长度向量中的信息
- en: 'In the decoding period, the decoder won’t predict the word just use its own
    information. However, it collaborates with the attention layer to get the translation.
    The input of attention mechanism is the hidden states in the top layer of the
    encoder and the current decoder. It gets the relativity order by calculating the
    following steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码阶段，解码器不会仅仅使用自身的信息来预测词汇。相反，它与注意力层协作以获取翻译。注意力机制的输入是编码器顶层的隐藏状态和当前的解码器。它通过计算以下步骤来获得相关性顺序：
- en: 1\. The current decoding hidden state $h_{t}$ will be used to compare with all
    source states $h_{s}$ to derive the attention weights score $s_{t}$.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 当前解码隐藏状态 $h_{t}$ 将用于与所有源状态 $h_{s}$ 进行比较，以推导注意力权重分数 $s_{t}$。
- en: 2\. The attention weights $a_{t}$ is driven by normalization operation for all
    attention weight score.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 注意力权重 $a_{t}$ 是通过对所有注意力权重分数进行归一化操作得到的。
- en: 3\. Based on the attention weights, we then compute the weighted average of
    the source states as a context vector $c_{t}$.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 基于注意力权重，我们计算源状态的加权平均值作为上下文向量 $c_{t}$。
- en: 4\. Concatenate the context vector with the current decoding hidden state to
    yield the final attention vector(the exact combination method can be different).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 将上下文向量与当前解码隐藏状态连接以生成最终的注意力向量（具体的组合方法可能不同）。
- en: '5\. The attention vector is fed as an input to decoder in the next time-step
    (applicable for input feeding). The first three steps can be summarized by the
    equations below:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 注意力向量被作为输入传递给解码器的下一个时间步（适用于输入馈送）。前面三步可以通过以下方程总结：
- en: '|  | $\centering s_{t}=score(h_{t},h_{s})\;\;\;\left[{Attention\ function}\right]\@add@centering$
    |  | (4) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\centering s_{t}=score(h_{t},h_{s})\;\;\;\left[{注意力\ 函数}\right]\@add@centering$
    |  | (4) |'
- en: '|  | $\centering a_{t}=\frac{exp(s_{t})}{\sum\limits_{s=0}^{S}exp(s_{t})}\qquad\left[{Attention\
    weight}\right]\@add@centering$ |  | (5) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $\centering a_{t}=\frac{exp(s_{t})}{\sum\limits_{s=0}^{S}exp(s_{t})}\qquad\left[{注意力\
    权重}\right]\@add@centering$ |  | (5) |'
- en: '|  | $\centering c_{t}=\sum\limits_{s}a_{t}h_{s}\;\;\qquad\qquad\qquad\left[{Context\
    vector}\right]\@add@centering$ |  | (6) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $\centering c_{t}=\sum\limits_{s}a_{t}h_{s}\;\;\qquad\qquad\qquad\left[{上下文\
    向量}\right]\@add@centering$ |  | (6) |'
- en: 'Among the above function, The score function could be defined in different
    ways. Here, we two classic definitions:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述函数中，得分函数可以有不同的定义方式。这里我们介绍两种经典定义：
- en: '|  | $\centering score(h_{t},h_{s})=\begin{cases}h_{t}^{T}Wh_{s}&amp;[Luong^{\prime}s\
    version]\\ v_{a}^{T}tanh(W_{1}h_{t},h_{s})&amp;[Bahdanau^{\prime}s\ version]\end{cases}\@add@centering$
    |  | (7) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\centering score(h_{t},h_{s})=\begin{cases}h_{t}^{T}Wh_{s}&amp;[Luong^{\prime}s\
    version]\\ v_{a}^{T}tanh(W_{1}h_{t},h_{s})&amp;[Bahdanau^{\prime}s\ version]\end{cases}\@add@centering$
    |  | (7) |'
- en: Back to the decoding period, it receives the information from both two sides,
    the decoder hidden state and the attention vector, given the current two vectors,
    it then predicts the words by alignment them to a new vector, then it usually
    has another layer to predict the current target word.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 回到解码阶段，它接收来自两个方面的信息，即解码器隐藏状态和注意力向量。给定当前的两个向量，它通过将它们对齐到一个新的向量来预测词汇，然后通常会有另一个层来预测当前的目标词。
- en: IV-B2 Global Attention & Local Attention
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 全局注意力与局部注意力
- en: Global Attention
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 全局注意力
- en: Global Attention is the method of Attention Mechanism we mentioned above, and
    it’s also a fluent type in various of Attention mechanism. The idea of Global
    Attention is also the original form of attention mechanism, though it got this
    name by Luong et al.[[23](#bib.bib23)], the corresponding term is Local Attention.
    The term —— ”global” derives from it calculates the context vector by considering
    the relevance order of all words in the source sentence. This method has excellent
    performance because more alignment information will generally produce a better
    result. A straightforward presentation in Fig. [6](#S4.F6 "Figure 6 ‣ IV-B2 Global
    Attention & Local Attention ‣ IV-B Structure of Attention Mechanism ‣ IV NMT with
    Attention Mechanism ‣ A Survey of Deep Learning Techniques for Neural Machine
    Translation")
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Global Attention 是我们上面提到的注意力机制方法，它也是各种注意力机制中的一种流行类型。Global Attention 的思想也是注意力机制的原始形式，尽管它由Luong
    等人[[23](#bib.bib23)]命名，但对应的术语是 Local Attention。术语——“global”源于它通过考虑源句子中所有单词的相关顺序来计算上下文向量。这种方法表现优秀，因为更多的对齐信息通常会产生更好的结果。图[6](#S4.F6
    "Figure 6 ‣ IV-B2 Global Attention & Local Attention ‣ IV-B Structure of Attention
    Mechanism ‣ IV NMT with Attention Mechanism ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation")中的直观展示。
- en: '![Refer to caption](img/3f6211c30a3cfe37f8fbdb4663c58e48.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3f6211c30a3cfe37f8fbdb4663c58e48.png)'
- en: 'Figure 6: The concept of Global attention, current decoder hidden state calculated
    with all the hidden states in source side to get the alignment information.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Global Attention 的概念，当前解码器隐藏状态通过源端所有隐藏状态计算，以获得对齐信息。
- en: As we have introduced in Section [IV](#S4 "IV NMT with Attention Mechanism ‣
    A Survey of Deep Learning Techniques for Neural Machine Translation"), this method
    considers all the source word in the decoding period. The main drawback is calculation
    speed deteriorates when the sequence is very long since one hidden state will
    be generated in one time-step in the Encoder, the cost of score function would
    be linear with the number of time-steps on the Encoder. When the input is a long
    sequence like a compound sentence or a paragraph, it may affect the decoding speed.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在第[IV](#S4 "IV NMT with Attention Mechanism ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation")节中介绍的，该方法考虑了解码期间的所有源词。主要缺点是当序列非常长时，计算速度会降低，因为在编码器中每个时间步都会生成一个隐藏状态，评分函数的计算成本会随着编码器时间步数的增加而线性增长。当输入是像复合句或段落这样较长的序列时，可能会影响解码速度。
- en: Local attention Local attention was first proposed by Luong et al.[[23](#bib.bib23)].
    As illustrated in Fig. [7](#S4.F7 "Figure 7 ‣ IV-B2 Global Attention & Local Attention
    ‣ IV-B Structure of Attention Mechanism ‣ IV NMT with Attention Mechanism ‣ A
    Survey of Deep Learning Techniques for Neural Machine Translation"), this model,
    on the other hand, will just calculate the relevance with a subset of the source
    sentence. Comparing with Global attention, it fixes the length of attention vector
    by giving a scope number, thus avoiding the expensive computation in getting context
    vectors. The experiment result indicated that local attention can keep a balance
    between model performance with computing speed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Local Attention 是 Luong 等人[[23](#bib.bib23)] 首次提出的。如图[7](#S4.F7 "Figure 7 ‣
    IV-B2 Global Attention & Local Attention ‣ IV-B Structure of Attention Mechanism
    ‣ IV NMT with Attention Mechanism ‣ A Survey of Deep Learning Techniques for Neural
    Machine Translation")所示，这个模型会仅计算源句子的一部分的相关性。与 Global Attention 相比，它通过给定一个范围数字来固定注意力向量的长度，从而避免了获取上下文向量的昂贵计算。实验结果表明，Local
    Attention 可以在模型性能与计算速度之间保持平衡。
- en: '![Refer to caption](img/8adb12dad0128cc07d2665de17e7bd2e.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8adb12dad0128cc07d2665de17e7bd2e.png)'
- en: 'Figure 7: The concept of Local attention, current hidden state calculated with
    a subset of all the hidden states in source side.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Local Attention 的概念，当前隐藏状态通过源端所有隐藏状态的子集计算得到。
- en: The inspiration of local attention comes from the soft attention and hard attention
    in image caption generation task, which was proposed by Xu et al.[[109](#bib.bib109)].
    While the global attention is very similar to soft attention, the local attention,
    on the other hand, can be seen as a mixture method of soft attention with hard
    attention.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Local Attention 的灵感来自于 Xu 等人[[109](#bib.bib109)] 在图像描述生成任务中提出的软注意力和硬注意力。虽然 Global
    Attention 与软注意力非常相似，但 Local Attention 可以被视为软注意力与硬注意力的混合方法。
- en: In theory, although covering more information would generally get a better result,
    the fantastic result of this method has indicated a comparable performance with
    global attention when it has been fine-tuned. This seems due to the common phenomenon
    in human language —— the current word would naturally have a high dependency with
    some of its nearby words, which is quite similar to the assumption of the n-gram
    language model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，虽然覆盖更多信息通常会获得更好的结果，但这种方法的出色结果表明，当经过微调时，它的性能与全局注意力相当。这似乎是由于人类语言中的一种常见现象——当前单词自然与其附近的某些单词有较高的依赖性，这与n-gram语言模型的假设非常相似。
- en: In the details of calculation process, given the current target word’s position
    $p_{t}$, the model fixes the context vector in scope D. The context vector ct
    is then derived as a weighted average over the set of source hidden states within
    the range $[p_{t}-D,p_{t}+D]$; Scope D is selected by experience, and then it
    could be same steps in deriving the attention vector like Global attention.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算过程的细节中，给定当前目标词的位置 $p_{t}$，模型将上下文向量固定在范围D内。上下文向量ct然后作为源隐藏状态集 $[p_{t}-D,p_{t}+D]$
    的加权平均值得出；范围D由经验选择，然后可以用类似于全局注意力的步骤推导出注意力向量。
- en: IV-B3 Input feeding approach
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 输入馈送方法
- en: Input feeding is a small tip in constructing the NMT structure, but from the
    perspective of providing alignment information, it can also be seen as a kind
    of attention.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输入馈送是构建NMT结构的小技巧，但从提供对齐信息的角度来看，它也可以被视为一种注意力机制。
- en: The concept of input feeding is simple. In the decoding period, besides using
    the previously predicted words as input, it also uses the attention vector that
    in the previous time-step as additional input in next time-step[[1](#bib.bib1)][[23](#bib.bib23)].
    This attention vectors will concatenate with input vector to get the final input
    vector; then this new vector will be fed as the input in the next step.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 输入馈送的概念很简单。在解码期间，除了使用先前预测的单词作为输入外，还使用上一个时间步的注意力向量作为下一个时间步的附加输入[[1](#bib.bib1)][[23](#bib.bib23)]。这些注意力向量将与输入向量连接，以获取最终的输入向量；然后，这个新向量将作为下一步的输入。
- en: IV-B4 Attention Mechanism in GNMT
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B4 GNMT中的注意力机制
- en: GNMT is short for Google Neural Machine Translation, which is a well-known version
    of NMT with Attention Mechanism. GNMT was proposed by Wu et al.,[[24](#bib.bib24)]
    and famous for its successful application in industrial level NMT system. With
    the help of many kinds of advanced tips in model detail, it got state-of-the-art
    performance at that time. Besides, the elaborate architecture of GNMT makes it
    have a better inference speed, which helps it more applicable in satisfying the
    industry need.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: GNMT是Google Neural Machine Translation的缩写，它是一个著名的具有注意力机制的NMT版本。GNMT由Wu等人提出[[24](#bib.bib24)]，以其在工业级NMT系统中的成功应用而著称。借助许多先进的模型细节技巧，它在当时获得了最先进的性能。此外，GNMT的精心设计架构使其具有更好的推理速度，使其更适用于满足行业需求。
- en: The concept of GNMT get the help of the current research in attention mechanism;
    it used Global Attention but was reconstruct by a more effective structure for
    model parallelization. The concrete details illustrated in the figure, it has
    two main points in this architecture.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: GNMT的概念借助了当前的注意力机制研究；它使用了全局注意力，但通过更有效的结构进行重构以实现模型并行化。图中详细说明了两个主要点。
- en: '![Refer to caption](img/d586f16ebc20079010b47468d569447a.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d586f16ebc20079010b47468d569447a.png)'
- en: 'Figure 8: Attention in GNMT, the Attention weight was driven by the bottom
    layer of Decoder and sent to all Decoder layers, which helps to improve computing
    parallelization'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：GNMT中的注意力，注意力权重由解码器的底层驱动，并发送到所有解码器层，这有助于提高计算的并行化
- en: First, this structure has canceled the connection between the encoder and the
    decoder. So that it can have more freedom in choosing the structure of the encoder
    and decoder, for example, the encoder could choose the different dimensions in
    each layer regardless of the dimension in the decoder, only the top layer of the
    both encoder and decoder should have same dimensions to guarantee that they can
    be calculated in mathematics for driving attention vector.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这种结构取消了编码器和解码器之间的连接。因此，它可以在选择编码器和解码器的结构时有更多自由，例如，编码器可以选择每层的不同维度，而不考虑解码器中的维度，只需保证编码器和解码器的顶层具有相同的维度，以确保它们可以在数学上计算以驱动注意力向量。
- en: Second, this structure makes it easier for paralleling the model. Only the bottom
    layer of the decoder is used to get the context vector, then all of the remain
    decoding layers will use this context vector directly. This architecture can retain
    as much parallelism as possible.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这种结构使得模型并行化变得更容易。仅使用解码器的底层来获得上下文向量，然后其余的解码层将直接使用这个上下文向量。这种架构可以尽可能保留并行性。
- en: For details of attention calculation, GNMT applying the Attention Mechanism
    like the way of calculating global attention, while the $score()$ function is
    a feed forward network with 1 hidden layer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 关于注意力计算的详细信息，GNMT 应用了类似于计算全局注意力的注意力机制，而 $score()$ 函数是一个具有 1 层隐藏层的前馈网络。
- en: IV-B5 Self-attention
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B5 自注意力
- en: '![Refer to caption](img/35ecc2d79cd37f9d0d2c077750f8d9fa.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/35ecc2d79cd37f9d0d2c077750f8d9fa.png)'
- en: 'Figure 9: The concept of Multi-head Self-attention'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：多头自注意力的概念
- en: Self-attention is also called intra-attention, it is wildly known for its application
    in NMT task due to the emergence of Transformer. While other commonly noted Attention
    Mechanism driven the context information by calculating words dependency between
    source sequence with target sequence, Self-attention calculates the words dependency
    inside the sequence, and thus get an attention based sequence representation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力也称为内在注意力，由于 Transformer 的出现，它在 NMT 任务中的应用广为人知。其他常见的注意力机制通过计算源序列和目标序列之间的词语依赖来驱动上下文信息，而自注意力则计算序列内部的词语依赖，从而获得基于注意力的序列表示。
- en: 'As for calculation steps, Self-attention first gets 3 vectors based original
    embedding for different purpose, the 3 vectors are Query vector, Key vector, and
    Value vector. Then the attention weights was calculated in this way:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 就计算步骤而言，自注意力首先根据原始嵌入获得 3 个不同用途的向量，这 3 个向量分别是查询向量、键向量和数值向量。然后按照以下方式计算注意力权重：
- en: '|  | $\text{ Attention }(Q,K,V)=\operatorname{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$
    |  | (8) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{ Attention }(Q,K,V)=\operatorname{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V$
    |  | (8) |'
- en: where the $\frac{1}{\sqrt{d_{k}}}$ is a scaled factor for avoiding to have more
    stable gradients that caused by dot products operation. In addition, the above
    calculation can be implemented in metrics multiplication, so the words dependency
    can easily got in form of relation metrics.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\frac{1}{\sqrt{d_{k}}}$ 是一个缩放因子，用于避免由于点积操作引起的不稳定梯度。此外，上述计算可以通过矩阵乘法实现，因此可以轻松地以关系矩阵的形式获得词语依赖。
- en: IV-C Other related work
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 其他相关工作
- en: Besides the above description in significant progress, there are also some other
    refinements in a different perspective of attention mechanism.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述显著进展外，从不同角度对注意力机制还有一些其他的改进。
- en: In perspective of attention structure, Yang et al. improved the traditional
    attention structure by providing a network to model the relationship of word with
    its previous and subsequent attention[[94](#bib.bib94)]. Feng et al. proposed
    a recurrent attention mechanism to improve the alignment accuracy, and it has
    been proved to outperformed vanilla models in large-scale Chinese–English task[[93](#bib.bib93)].
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从注意力结构的角度来看，Yang 等人通过提供一个网络来建模词语与其前后注意力的关系，从而改进了传统的注意力结构[[94](#bib.bib94)]。Feng
    等人提出了一种递归注意力机制来提高对齐准确性，并且已被证明在大规模中英文任务中优于原始模型[[93](#bib.bib93)]。
- en: 'Moreover, other researches focus on the training process. Cohn et al. extended
    the original attention structure by adding several structural biases, they including
    positional bias, Markov conditioning, fertility, and Bilingual Symmetry[[95](#bib.bib95)],
    model that integrated with these refinements have got better translation performance
    over the basic attention-based model. More concretely, the above methods can be
    seen as a kind of inheritance from the alignment model in SMT research, with more
    experiential assumption and intuition in linguistics: $Position\ Bias:$ It assumed
    words in source and target sentence with the same meaning would also have a similar
    relative position, especially when both two-sentence have a similar word order.
    As an adjustment of the original attention mechanism, it helps to improve the
    alignment accuracy by encouraging words in similar relative position to be aligned.
    Figure11111 demonstrated the phenomena strongly, where words in diagonal are tended
    to be aligned. $Markov\ Condition:$ Empirically, in one sentence, one word has
    higher Correlation with its nearby words rather than those far from it, this is
    also the basement in explaining $context\ capture$ of n-gram LM. As for translation
    task, it’s obvious that words are adjacent in source sentence would also map to
    the nearby position in target sentence, taking advantage of this property, this
    consideration thus improves the alignment accuracy by discouraging the huge jumps
    in finding the corresponding alignment of nearby words. In addition, the method
    with similar consideration but different implementation is local attention. $Fertility:$
    Fertility measures whether the word has been attended at the right level, it considers
    preventing both scenarios when the word hasn’t got enough attention or has been
    paid too much attention. This design comes from the fact that the poor translation
    result is commonly due to repeatedly translated some word or lack coverage of
    other words, which refers to Under-translation and Over-translation. $Bilingual\
    Symmetry:$ In theory, word alignment should be a reversible result, which means
    the same word alignment should be got when translation processing form A to B
    with translation from B to A. This motivates the parallel training for the model
    in both directions and encouraging the similar alignment result.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，其他研究关注于训练过程。Cohn 等人通过添加几种结构性偏差扩展了原始的注意力结构，其中包括位置偏差、马尔可夫条件、繁殖率和双语对称性[[95](#bib.bib95)]。与这些改进相结合的模型在翻译性能上优于基础的基于注意力的模型。更具体地说，上述方法可以视为从
    SMT 研究中的对齐模型的一种继承，具有更多的经验假设和语言学直觉：$Position\ Bias:$ 假设源语言和目标语言中意义相同的单词也会有相似的相对位置，特别是当两句话具有类似的词序时。作为对原始注意力机制的调整，它通过鼓励在相似相对位置上的单词进行对齐，从而提高对齐准确性。图11111
    强烈展示了这一现象，其中对角线上的单词倾向于对齐。$Markov\ Condition:$ 从经验上看，在一个句子中，一个单词与其邻近单词的相关性比与远离它的单词更高，这也是解释
    n-gram LM 的 $context\ capture$ 的基础。对于翻译任务来说，很明显源语言中相邻的单词也会映射到目标语言中的相邻位置，利用这一特性，这一考虑通过抑制在寻找相应对齐时的巨大跳跃，从而提高对齐准确性。此外，具有类似考虑但实现不同的方法是局部注意力。$Fertility:$
    繁殖率衡量单词是否在正确的层次上被关注，它考虑防止两种情况：单词未获得足够的关注或过度关注。该设计源于一个事实，即翻译结果不佳通常是由于单词重复翻译或其他单词覆盖不足，这指的是翻译不足和过度翻译。$Bilingual\
    Symmetry:$ 理论上，单词对齐应该是可逆的，这意味着当从 A 到 B 的翻译处理与从 B 到 A 的翻译相同。此理论推动了模型在两个方向上的平行训练，并鼓励得到类似的对齐结果。
- en: The refinement infertility was further extended by Tu et al.[[35](#bib.bib35)],
    who proposed fertility prediction as a normalizer before decoding, this method
    adjusts the context vector in original NMT model by adding coverage information
    when calculating attention weights, thus can provide complementary information
    about the probability of source words have been translated in prior steps.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Tu 等人[[35](#bib.bib35)] 进一步扩展了繁殖率的不完善性，他们提出了在解码前作为标准化器的繁殖率预测，该方法通过在计算注意力权重时添加覆盖信息来调整原始
    NMT 模型中的上下文向量，从而提供关于源单词在先前步骤中已翻译概率的补充信息。
- en: Besides the intuition that heuristics from SMT, Cheng et al. applied the agreement-based
    learning method on NMT task, which encourages joint training in the agreement
    of word alignment with both translation directions [[96](#bib.bib96)]. In later,
    Mi et al. proposed a supervised method for attention component, and it utilized
    annotated data with additional alignment constraints in its objective function,
    experiments in Chinese-to-English task has proven to benefit for both translation
    performance and alignment accuracy[[97](#bib.bib97)].
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 除了SMT的启发直觉外，Cheng等人还在NMT任务中应用了基于一致性的学习方法，鼓励在两个翻译方向上进行单词对齐的一致性联合训练[[96](#bib.bib96)]。随后，Mi等人提出了一种针对注意力组件的监督方法，利用带有额外对齐约束的标注数据作为目标函数，中文到英语的实验证明了这一方法对翻译表现和对齐准确性都有益[[97](#bib.bib97)]。
- en: V Vocabulary Coverage Mechanism
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 词汇覆盖机制
- en: Besides the long dependency problem in general MT tasks, the existence of unknown
    words is another problem that can severely affect the translation quality. Different
    from traditional SMT methods which support enormous vocabulary, most of NMT models
    suffer from the vocabulary coverage problem due to the nature that it can only
    choose candidate words in predefined vocabulary with a modest size. In terms of
    vocabulary building, the chosen words are usually frequent words, while the remaining
    words are called unknown words or out-of-vocabulary (OOV) words.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了普通MT任务中的长期依赖问题，未知词的存在是另一个可能严重影响翻译质量的问题。与支持大量词汇的传统SMT方法不同，大多数NMT模型由于只能选择预定义词汇中的候选词（词汇量适中），因此受词汇覆盖问题困扰。在词汇构建方面，所选择的词汇通常是频繁使用的词汇，而剩余的词汇则被称为未知词或OOV词汇。
- en: Empirically speaking, the vocabulary size in NMT varies between 30k-80k at most
    in each language, with one marked exception was proposed by Jean et al., who once
    used an efficient approximation for $softmax$ to accommodate for the immense size
    of vocabulary (500k)[[47](#bib.bib47)]. However, the vocabulary coverage problem
    still persists widely because of the far more number of OOV words in $de\ facto$
    translation task, such as proper nouns in different domains and a great number
    of rarely used verbs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验上看，NMT中的词汇量在每种语言中最多在30k-80k之间，其中一个显著的例外是Jean等人提出的，他们曾使用一种高效的$softmax$近似方法来适应词汇量庞大的情况（500k）[[47](#bib.bib47)]。然而，由于$de\
    facto$翻译任务中OOV词汇数量远远更多，例如不同领域的专有名词和大量不常用的动词，词汇覆盖问题仍然广泛存在。
- en: Since the vocabulary coverage in NMT is extremely limited, handling the OOV
    words is another research hot spot. This section demonstrates the intrinsic interpretation
    of the vocabulary coverage problem in NMT and the corresponding solutions proposed
    in the past several years.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于NMT中的词汇覆盖范围极其有限，处理OOV词汇是另一个研究热点。本节展示了NMT中词汇覆盖问题的内在解释以及过去几年提出的相应解决方案。
- en: V-A Description of Vocabulary Coverage problem in NMT
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A NMT中词汇覆盖问题的描述
- en: Based on the scenario as mentioned above, in the practical implementation of
    NMT, the initial way is choosing a small set of vocabulary and converting a large
    number of OOV words to one uniform “UNK” symbol (or other tags) as illustrated
    in Fig. [10](#S5.F10 "Figure 10 ‣ V-A Description of Vocabulary Coverage problem
    in NMT ‣ V Vocabulary Coverage Mechanism ‣ A Survey of Deep Learning Techniques
    for Neural Machine Translation"). This intuitive solution may hurt translation
    performance in the following two aspects. First, the existence of “UNK” symbol
    in translation may hurt the semantic completeness of sentence; ambiguity may emerge
    when “UNK” replace some crucial words[[48](#bib.bib48)]. Second, as the NMT model
    hard to learn information from OOV words, the prediction quality beyond the OOV
    words may also be affected[[49](#bib.bib49)].
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述情况，在NMT的实际应用中，初步方法是选择一小部分词汇，并将大量OOV词汇转换为一个统一的“UNK”符号（或其他标签），如图[10](#S5.F10
    "图 10 ‣ V-A NMT中词汇覆盖问题的描述 ‣ V 词汇覆盖机制 ‣ 深度学习技术在神经机器翻译中的调查")所示。这种直观的解决方案可能在以下两个方面影响翻译表现。首先，“UNK”符号的存在可能会影响句子的语义完整性；当“UNK”替代一些关键字时，可能会出现歧义[[48](#bib.bib48)]。其次，由于NMT模型难以从OOV词汇中学习信息，超出OOV词汇的预测质量也可能受到影响[[49](#bib.bib49)]。
- en: 'TABLE I: BLEU Performance of NMT Models'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: NMT模型的BLEU表现'
- en: '| Model | BLEU |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | BLEU |'
- en: '| EN-DE | EN-FR |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| EN-DE | EN-FR |'
- en: '| ByteNet | 23.75 |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| ByteNet | 23.75 |  |'
- en: '| Deep-Att + PosUnk |  | 39.2 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Deep-Att + PosUnk |  | 39.2 |'
- en: '| GNMT + RL | 24.6 | 39.92 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| GNMT + RL | 24.6 | 39.92 |'
- en: '| ConvS2S | 25.16 | 40.46 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| ConvS2S | 25.16 | 40.46 |'
- en: '| MoE | 26.03 | 40.56 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| MoE | 26.03 | 40.56 |'
- en: '| Deep-Att + PosUnk Ensemble |  | 40.4 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Deep-Att + PosUnk Ensemble |  | 40.4 |'
- en: '| GNMT + RL Ensemble | 26.3 | 41.16 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| GNMT + RL Ensemble | 26.3 | 41.16 |'
- en: '| ConvS2S Ensemble | 26.36 | 41.29 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| ConvS2S Ensemble | 26.36 | 41.29 |'
- en: '| Transformer (base model) | 27.3 | 38.1 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Transformer (基础模型) | 27.3 | 38.1 |'
- en: '| Transformer (big) | 28.4 | 41.8 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Transformer (大) | 28.4 | 41.8 |'
- en: '![Refer to caption](img/733039cc73816e85f2c30730edd0fed7.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/733039cc73816e85f2c30730edd0fed7.png)'
- en: 'Figure 10: An example of OOV words problem presented in[[23](#bib.bib23)].
    $en$ and $fr$ denote the source sentence in English and the corresponding target
    sentence in French, $nn$ denotes the neural network’s result.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：展示在[[23](#bib.bib23)]中提出的OOV词汇问题的一个例子。$en$和$fr$分别表示英文的源句子和相应的法语目标句子，$nn$表示神经网络的结果。
- en: Besides the unsurprising observation that NMT performed poorly on sentence with
    more OOV words than with more frequent words, some other phenomena in MT task
    are also hard to be handled like multi-word alignment, transliteration, and spelling,
    .etc. [[16](#bib.bib16)][[21](#bib.bib21)]. They are seen as a similar phenomenon
    which is also caused by unknown words problem or suffers from rare training data[[50](#bib.bib50)].
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 除了NMT在包含更多OOV词汇的句子上的表现不如在包含更多频繁词汇的句子上的表现这一不令人意外的观察外，MT任务中的一些其他现象也很难处理，例如多词对齐、音译和拼写等[[16](#bib.bib16)][[21](#bib.bib21)]。这些现象被视为相似的现象，也都是由未知词汇问题或稀缺的训练数据[[50](#bib.bib50)]造成的。
- en: For most of NMT model, choosing a modest size of vocabulary list is virtually
    a trade-off between the computation cost with translation quality. Also, it has
    been found the same thing in training NLM[[53](#bib.bib53)][[52](#bib.bib52)].
    Concretely, the computation cost mainly comes from the nature of the method in
    getting predicting word — a normalization operation, which is used repeatedly
    in training of the DL model. Specifically, in NMT task, since DL model needs to
    adjust the parameters each time, the probability of current word thus would be
    calculated repeatedly to get the gradient, and since NMT model calculates the
    probability of current word when making a prediction, it needs to normalize the
    all of words in the vocabulary each time. Unfortunately, the normalization process
    is time-consuming due to its time complexity is linear with the vocabulary size,
    and this attribute has rendered the same time complexity in the training process.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数NMT模型而言，选择适中的词汇表大小几乎是一种在计算成本和翻译质量之间的权衡。同时，在训练NLM时也发现了相同的现象[[53](#bib.bib53)][[52](#bib.bib52)]。具体来说，计算成本主要来自于预测单词的方法的本质——一种归一化操作，这种操作在DL模型的训练中被反复使用。特别是在NMT任务中，由于DL模型每次都需要调整参数，因此当前单词的概率会被反复计算以获得梯度，并且由于NMT模型在进行预测时需要计算当前单词的概率，它每次都需要对词汇表中的所有单词进行归一化。不幸的是，由于归一化过程的时间复杂度与词汇表的大小成线性关系，因此它是耗时的，这一特性使得训练过程中的时间复杂度也相同。
- en: V-B Different Solutions
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 不同的解决方案
- en: Related researches have proposed various methods in both the training and inference
    process. Further, these methods can be roughly divided into three categories based
    on their different orientations. The first one is intuitively focused on finding
    solutions in improving computation speedup, which could support a more extensive
    vocabulary. The second one focus on using context information, this kind of method
    can address some of the unknown words(such as Proper Noun) by copying them to
    translation result as well as low-frequency words which cause a poor translation
    quality. The last one, which is more advanced, prefers to utilize information
    inside the word such as characters, because of their flexibility in handling morphological
    variants of words, this method can support translating OOV words in a more ”intelligent”
    way.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 相关研究提出了各种方法来改进训练和推理过程。此外，这些方法可以根据其不同的方向大致分为三类。第一类直观上专注于寻找提高计算速度的解决方案，这可以支持更广泛的词汇。第二类方法则关注于使用上下文信息，这种方法可以通过将未知词（如专有名词）复制到翻译结果中以及处理导致翻译质量较差的低频词来解决一些问题。最后一类方法则更先进，倾向于利用单词内部的信息，如字符，因为这些方法在处理单词形态变化方面更具灵活性，它们可以以更“智能”的方式支持翻译OOV词汇。
- en: V-B1 methods by computation speedup
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 通过计算加速的方法
- en: For computation speedup method, there are lots of literature that implement
    their idea in NLM training. The first thought in trying computation speedup is
    to scale the $softmax$ operation. Since an effective $softmax$ calculation could
    obviously support a larger vocabulary, this kind of trying has got a lot of attention
    in NLM literature. Morin & Bengio [[53](#bib.bib53)] proposed hierarchical models
    to get an exponential speedup in the computation of normalization factor, thus
    help to accelerate the gradient calculation of word probabilities. In concrete
    details, the original model has transformed vocabulary into a binary tree structure,
    which was built with pre-knowledge from WordNet[[54](#bib.bib54)].
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 关于计算加速方法，有大量文献在NLM训练中实现了他们的想法。最初考虑计算加速的想法是扩展 $softmax$ 操作。由于有效的 $softmax$ 计算显然可以支持更大的词汇量，这种尝试在NLM文献中受到了很多关注。Morin
    和 Bengio [[53](#bib.bib53)] 提出了层次化模型，以在归一化因子的计算中获得指数级加速，从而帮助加快单词概率的梯度计算。在具体细节中，原始模型将词汇转化为二叉树结构，该结构是利用来自WordNet的预知识构建的[[54](#bib.bib54)]。
- en: The initial experiment result shows that this hierarchical method is comparable
    with traditional trigram LM but fails to exceed original NLM; this is partly because
    of utilizing handcrafted feature from WordNet in the tree building process. As
    a binary tree can provide a significant improvement in cost-effective between
    the speed with performance, further work still focuses on this trend to find better
    refinement. Later on, Mnih & Hinton followed this work by removing the requirement
    of expert knowledge in tree building process[[52](#bib.bib52)].
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 初步实验结果显示，这种层次化方法与传统的三元语法模型（trigram LM）相当，但未能超越原始的NLM；这部分是由于在树构建过程中使用了来自WordNet的手工特征。由于二叉树可以在速度和性能之间提供显著的性价比改进，进一步的工作仍然集中在这个趋势上以寻找更好的优化方法。后来，Mnih
    和 Hinton 通过去除树构建过程中的专家知识要求跟进了这项工作[[52](#bib.bib52)]。
- en: A more elegant method is to retain the original model but change the method
    in calculating the normalization factor. Bengio & Sen´ecal proposed importance
    sampling method to approximate the normalization factor[[55](#bib.bib55)]. However,
    this method is not stable unless with a careful control[[56](#bib.bib56)]. Mnih
    & Teh used noise-contrastive estimation to learn the normalization factor directly,
    which can be more stable in the training process of NLM [[57](#bib.bib57)]. Later,
    Vaswani et al. proposed a similar method with application in MT[[58](#bib.bib58)].
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 更优雅的方法是保留原始模型，但改变归一化因子的计算方法。Bengio 和 Senécal 提出了重要性采样方法来近似归一化因子[[55](#bib.bib55)]。然而，除非仔细控制，否则这种方法不够稳定[[56](#bib.bib56)]。Mnih
    和 Teh 使用噪声对比估计来直接学习归一化因子，这在NLM的训练过程中可以更稳定[[57](#bib.bib57)]。后来，Vaswani 等人提出了一种类似的方法，并应用于机器翻译[[58](#bib.bib58)]。
- en: The above methods are difficult to be implemented parallelly by GPUs. Further
    consideration found solutions that are more GPU friendly. Jean et al. alleviated
    the computation time by utilizing a subset of vocabulary as candidate words list
    in the training process while used the whole vocabulary in the inference process.
    Based on the inspiration of using importance sampling in earlier work[[56](#bib.bib56)],
    they proposed a pure data segmentation method in the training process. Specifically,
    they pre-processed the training data sequentially, choosing a subset of vocabulary
    for each training example with the number of its distinct words reached threshold
    $t$(which is still far less than the size of original vocabulary). In the inference
    process, they still abandon using the whole vocabulary and proposing a hybrid
    candidate list alternatively. They composed candidate words list from two parts.
    The first part is some specific candidate target words that translated from a
    pre-defined dictionary with the others are the K-most frequent words. In the practical
    performance analysis, this method remains the similar modest size of candidate
    words in the training process; thus, it can maintain the computational efficiency
    while supporting an extremely larger size of candidate words[[47](#bib.bib47)].
    Similarly, Mi et al. proposed vocabulary manipulation method which provides a
    separate vocabulary for different sentences or batches, it contains candidate
    words from both word-to-word dictionary and phrase-to-phrase library[[104](#bib.bib104)].
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法难以通过 GPU 并行实现。进一步的考虑找到了更适合 GPU 的解决方案。Jean 等人通过在训练过程中利用词汇子集作为候选词列表来减轻计算时间，同时在推理过程中使用整个词汇表。基于早期工作中使用重要性采样的灵感[[56](#bib.bib56)]，他们提出了一种纯数据分割的方法。在训练过程中，他们对训练数据进行了顺序预处理，为每个训练示例选择一个词汇子集，其独特词汇数量达到阈值
    $t$（仍远低于原始词汇表的大小）。在推理过程中，他们仍然放弃使用整个词汇表，提出一种混合候选列表。他们将候选词列表分为两部分。第一部分是从预定义词典中翻译过来的特定候选目标词，其他的是出现频率最高的
    K 个词。在实际性能分析中，该方法在训练过程中保持了类似的适中候选词规模；因此，它可以保持计算效率，同时支持极大的候选词规模[[47](#bib.bib47)]。类似地，Mi
    等人提出了一种词汇操作方法，为不同的句子或批次提供了一个单独的词汇表，它包含来自单词到单词词典和短语到短语库的候选词[[104](#bib.bib104)]。
- en: Besides all kinds of corresponding drawbacks in the above method, the common
    weakness of all these methods is they still suffer from the OOV words despite
    a larger vocabulary size they can support. This is because the enlarged vocabulary
    is still size limited, and there’s no solution for complementary when encountering
    unknown words, whereas the following category of methods can partly handle it.
    In addition, simply increasing the vocabulary size can merely bring little improvement
    due to the Zipf’s Law, which means there is always a large tail of OOV words need
    to be addressed[[48](#bib.bib48)].
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述方法的各种相应缺陷，这些方法的共同弱点是尽管它们可以支持更大的词汇量，但仍然会遭遇 OOV 词。这是因为扩大词汇量仍然受到限制，当遇到未知词时没有补充的解决方案，而以下类别的方法可以部分解决这个问题。此外，仅仅增加词汇量由于
    Zipf 法则几乎不会带来改进，这意味着总会有一大尾 OOV 词需要处理[[48](#bib.bib48)]。
- en: V-B2 methods by using context information
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 方法使用上下文信息
- en: Besides the above variants which focus on computation speed-up, a more advanced
    category is using context information. Luong et al. proposed a word alignment
    algorithm which collaborates with Copy Mechanism to post-processing the translation
    result. This old but useful operation was inspired by the common word(phrase)
    replacement method in SMT and has achieved a pretty considerable improvement in
    BLEU [[59](#bib.bib59)]. Concretely, in Luong’s method, for each of the OOV words,
    there’s a “pointer” which map to the corresponding word in the source sentence.
    In the post-processing stage, a predefined dictionary was provided with “pointer”
    to find the corresponding translation, while using directly copy mechanism to
    handle the OOV words that not in the dictionary.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述专注于计算加速的变体，更高级的类别是使用上下文信息。Luong 等人提出了一种与复制机制协作的词对齐算法，用于后处理翻译结果。这种旧但有用的操作灵感来源于
    SMT 中常见的词（短语）替换方法，并在 BLEU 中取得了相当大的改进 [[59](#bib.bib59)]。具体来说，在Luong的方法中，对于每个 OOV
    词，有一个“指针”映射到源句子中的相应词。在后处理阶段，提供了带有“指针”的预定义词典来查找相应的翻译，同时直接使用复制机制处理词典中没有的 OOV 词。
- en: The popularity of Luong et al. ’s method is partly because the Copy Mechanism
    actually provides an infinite vocabulary. Further research has refined this alignment
    algorithm for better replacement accuracy and generalization. Choi et al. extended
    Luong et al.’s approach by dividing OOV words into one of three subdivisions based
    on their linguistic features. [[70](#bib.bib70)] This method can help to remap
    the OOV words effectively. Gulcehre et al. done several refinements in this category,
    they applied Copy Mechanism similar to Luong et al. but cooperate the Attention
    Mechanism in determining the location of word alignment, which is more flexible
    in addressing alignment and could be directly utilized in other tasks which alignment
    location varies dramatically in both sides (like text summarization). Besides
    that, they synthesized Copy Mechanism with general translation operation by adding
    a so-called switching network to decide which operation should be applied in each
    time-step, this could be thought to improve the generalization of the whole model.
    [[48](#bib.bib48)]. Gu et al. made parallel efforts in integrating different mechanisms,
    they proposed a kind of Attention Mechanism called CopyNet with the vanilla encoder-decoder
    model, which can be naturally extended to handle OOV words in NMT task[[74](#bib.bib74)].
    Additionally, they found that the attention mechanism has driven more by the semantics
    and language model when using traditional word translation, but by location when
    using copying operation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Luong等人的方法之所以受欢迎，部分原因是复制机制实际上提供了无限的词汇量。进一步的研究已经优化了这个对齐算法，以提高替换的准确性和泛化能力。Choi等人通过根据语言特征将OOV词划分为三种子类别，扩展了Luong等人的方法。[[70](#bib.bib70)]
    这种方法有助于有效地重新映射OOV词。Gulcehre等人在这一领域进行了多项改进，他们应用了类似Luong等人的复制机制，但结合了注意机制来确定词对齐的位置，这在处理对齐时更具灵活性，并且可以直接用于其他任务（如文本摘要），这些任务中的对齐位置在双方之间变化剧烈。此外，他们通过添加一种所谓的切换网络，将复制机制与一般的翻译操作结合在一起，以决定每个时间步应该应用哪种操作，这被认为可以提高整个模型的泛化能力。[[48](#bib.bib48)]。Gu等人致力于整合不同机制，他们提出了一种叫做CopyNet的注意机制，并将其与传统的编码器-解码器模型结合，这可以自然地扩展到处理NMT任务中的OOV词[[74](#bib.bib74)]。此外，他们发现，当使用传统的词翻译时，注意机制更多地由语义和语言模型驱动，而使用复制操作时则由位置驱动。
- en: '![Refer to caption](img/e1244d0e5c8ec8f1dc18667c22ba9b96.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e1244d0e5c8ec8f1dc18667c22ba9b96.png)'
- en: 'Figure 11: An example of one kind of Copy Mechanism proposed by Luong et al.[[23](#bib.bib23)],
    the subscripts of $<unk>$ symbol (refer as $d$)is a relative position of corresponding
    source words, where the alignment relation is a target word at position $j$ is
    aligned to a source word at position $i$ $=$ $j$ $+$ $d$.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：Luong等人提出的一种复制机制的示例[[23](#bib.bib23)]，$<unk>$符号的下标（表示为$d$）是对应源词的相对位置，其中对齐关系是目标词在位置$j$对齐到源词在位置$i$
    $=$ $j$ $+$ $d$。
- en: Besides the Copy Mechanism, using extra knowledge is also useful in handling
    some other linguistic scenarios, which is highly related to the vocabulary coverage
    problem. Arthur et al. incorporated lexicons knowledge to assist with translation
    in low-frequency words[[72](#bib.bib72)]. Feng et al. proposed a similar method
    with a memory-augmented NMT (M-NMT) architecture, and it used novel attention
    mechanism to get the extra knowledge from the dictionary that constructed by SMT[[73](#bib.bib73)].
    Additionally, using context information can also be applied to improve the translation
    quality of ambiguous words (Homographs)[[75](#bib.bib75)].
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 除了复制机制，使用额外知识在处理其他语言学场景中也很有用，这与词汇覆盖问题密切相关。Arthur等人结合了词典知识来协助低频词的翻译[[72](#bib.bib72)]。Feng等人提出了一种类似的方法，使用了增强记忆的NMT（M-NMT）架构，并使用新颖的注意机制从由SMT构建的词典中获取额外知识[[73](#bib.bib73)]。此外，使用上下文信息也可以改善歧义词（同形异义词）的翻译质量[[75](#bib.bib75)]。
- en: In a nutshell, there are many context-based refinements have been proposed,
    most of them using Copy Mechanism to handle the OOV words with various of alignment
    algorithm to locate the corresponding word in the target side. However, these
    kinds of methods have a limited room for further improvement because the Copy
    Mechanism is too crude to handle the sophisticated scenarios in different languages.
    Practically, these methods perform poorly in languages which are rich morphology
    like Finnish and Turkish, which motivated method with better generalization[[64](#bib.bib64)].
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，已经提出了许多基于上下文的改进方法，其中大多数使用复制机制来处理OOV词，并采用各种对齐算法来定位目标侧的相应词。然而，这些方法的进一步改进空间有限，因为复制机制处理不同语言中的复杂场景时过于粗糙。实际上，这些方法在形态丰富的语言（如芬兰语和土耳其语）中表现不佳，这促使了更具泛化能力的方法的提出[[64](#bib.bib64)]。
- en: V-B3 Methods in fine grit level
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3 细粒度水平的方法
- en: This sub-section introduce some more “intelligent” ways that focus on using
    additional information inside the word unit. It’s obviously that such additional
    information could enhance the ability in covering the various of the linguistic
    phenomenon.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节介绍了一些更“智能”的方法，重点使用词单元内部的额外信息。显然，这些额外信息可以增强覆盖各种语言现象的能力。
- en: In previous research, although using semantic information of word unit could
    provide the vast majority of learning features, other features in sub-word level
    are generally ignored. From the perspective of linguistic, the concept of “word”
    is the basic unit of language but not the minimum one in containing semantic information,
    and there are abundant experienced rules could be learned from the inside of word
    units like shape and suffix. Comparing with identity copy or dictionary translation
    which regards the rare words as an identical entity, the refined method using
    fine-grit information is more adaptable. Further, a more “radical” method was
    proposed, which just treats words in character level completely. It would be an
    innovative concern for future work.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的研究中，虽然使用词单元的语义信息可以提供绝大多数学习特征，但子词层面的其他特征通常被忽视。从语言学的角度来看，“词”的概念是语言的基本单位，但不是包含语义信息的最小单位，而且可以从词单元内部学习到丰富的经验规则，如形状和后缀。与将稀有词视为相同实体的身份复制或词典翻译相比，使用细粒度信息的精细方法更具适应性。此外，还提出了一种更“激进”的方法，即完全在字符级别处理词汇。这将成为未来工作的创新关注点。
- en: In this category, one popular choice is using the sub-word unit, the most remarkable
    achievement was proposed by Sennrich et al., which has been proved to have the
    best performance in some shared result[[50](#bib.bib50)]. Concretely, in this
    design, they treat unknown words as sequences of sub-word units, which is reasonable
    in terms of the composition of the vast majority of these words(e.g., named entities,
    loanwords, and morphologically complex words). In order to represent these OOV
    words completely, one intuitive solution is to build a predefined sub-word dictionary
    that contains enough variants of units. However, restoring the sub-words cause
    massive space consumption in vocabulary size, which is effectively cancels out
    the whole purpose of reducing the computational efficiency in both time and space.
    Under this circumstances, a Byte Pair Encoding (BPE) based sub-words extraction
    method was applied for word segmentation operation in both sides of languages,
    which successfully adapted this old but effective data compression method in text
    pre-processing.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类别中，一个流行的选择是使用子词单元，其中最显著的成就是由Sennrich等人提出的，这已被证明在一些共享结果中表现最佳[[50](#bib.bib50)]。具体来说，在这个设计中，他们将未知词视为子词单元的序列，这在绝大多数这些词的组成（例如命名实体、外来词和形态复杂词）上是合理的。为了完全表示这些OOV词，一个直观的解决方案是建立一个包含足够单元变体的预定义子词词典。然而，恢复子词会导致词汇大小的大量空间消耗，这实际上抵消了提高计算效率的全部目的。在这种情况下，应用了基于字节对编码（BPE）的子词提取方法进行两种语言的分词操作，成功地将这种古老但有效的数据压缩方法应用于文本预处理。
- en: 'In concrete details of this adapted BPE method, it alternated merging frequent
    pairs of bytes with characters or sequence of characters, and word segmentation
    process followed the below steps:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个改进的BPE方法的具体细节中，它交替合并频繁的字节对与字符或字符序列，词分割过程遵循以下步骤：
- en: (1) Prepare a large training corpus(generally are bilingual corpus).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 准备一个大型训练语料库（通常是双语语料库）。
- en: (2) Determined the size of the sub-word vocabulary.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 确定子词词汇表的大小。
- en: (3) Split the words to a sequence of characters (using a special space character
    for marking the original spaces).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 将单词拆分为字符序列（使用特殊的空格字符来标记原始空格）。
- en: (4) Merge the most frequent adjacent pair of characters (e.g., in English, this
    may be c and h into ch).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 合并最常见的相邻字符对（例如，在英语中，这可能是将c和h合并为ch）。
- en: (5) Repeat step 4 until reaching the fixed given number of times or the defined
    size of the vocabulary. Each of these steps would increase the vocabulary by one.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 重复步骤4，直到达到固定的次数或定义的词汇表大小。每一步都会使词汇表增加一个。
- en: The figure shows a toy example of the method. As for the practical result of
    word segmentation, the most frequent words will be merged as single tokens, while
    the rare words(which is similar to the OOV words in previous categories’ work)
    may still contain un-merged sub-words. However, they have been found rare in processed
    text[[50](#bib.bib50)].
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了该方法的一个玩具示例。至于词分割的实际结果，最常见的词将被合并为单个标记，而稀有词（类似于之前类别工作中的OOV词）可能仍包含未合并的子词。然而，这些在处理后的文本中发现较少[[50](#bib.bib50)]。
- en: Further work of BPE method has also been proposed to obtaining a better generalization.
    Taku proposed subword regularization in later as an alternative to handle the
    spurious ambiguity phenomenon in BPE, and they further proposed a new subword
    segmentation algorithm based on a uni-gram language model, which shares the same
    idea with BPE but was more flexible in getting multiple segmentation based on
    their probabilities. Similarly, Wu et al. used “workpieces” concept to handle
    the OOV words which were once applied on the Google speech recognition system
    to solve Japanese/Korean segmentation problem[[60](#bib.bib60)][[24](#bib.bib24)].
    This method breaks words into word pieces to get a balance between flexibility
    with efficiency when using single characters and full words separately.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: BPE方法的进一步工作也已被提出，以获得更好的泛化能力。Taku后来提出了子词正则化，作为处理BPE中虚假歧义现象的替代方案，并进一步提出了一种基于单元语言模型的新子词分割算法，该算法与BPE具有相同的思想，但在根据概率获得多个分割时更具灵活性。类似地，Wu等人使用“工件”概念来处理OOV词，这一方法曾应用于Google语音识别系统，以解决日语/韩语分割问题[[60](#bib.bib60)][[24](#bib.bib24)]。该方法将单词拆分为词片，以在分别使用单个字符和完整单词时在灵活性和效率之间取得平衡。
- en: Another notable concern is modeling sequence in characters-level. Inspired by
    using character-level information completely in building NLM[[65](#bib.bib65)],
    Costa-jussa‘ & Fonollosa used CNN with highway network to model the characters
    directly[[62](#bib.bib62)], they deployed this architecture in source side with
    a common word-level generation in target side. Similarly, Ling et al. and Ballesteros
    et al. have proposed model respectively that using RNN(LSTM) to build character
    level embedding and composes it into the word embedding [[64](#bib.bib64)] [[98](#bib.bib98)],
    this idea later has been applied in building an RNN based character-level NMT
    model [[63](#bib.bib63)]. More recently, Luong and Manning(2016) proposed a hybrid
    model that combines the word level RNN with character level RNN for assist[[99](#bib.bib99)].
    Concretely, Luongs’ method translates mostly at the word level, when encounter
    an OOV word, character level RNN would be used for the consult. The figure shows
    the detailed architecture of this model.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得关注的问题是字符级序列建模。受到完全使用字符级信息构建NLM的启发[[65](#bib.bib65)]，Costa-jussa‘ & Fonollosa使用CNN和高速公路网络直接对字符进行建模[[62](#bib.bib62)]，他们在源端部署了这种架构，同时在目标端使用了常见的词级生成。同样，Ling等人和Ballesteros等人分别提出了使用RNN（LSTM）构建字符级嵌入并将其组合成词嵌入的模型[[64](#bib.bib64)]
    [[98](#bib.bib98)]，这一思想后来被应用于构建基于RNN的字符级NMT模型[[63](#bib.bib63)]。最近，Luong和Manning（2016）提出了一种混合模型，将词级RNN与字符级RNN结合使用[[99](#bib.bib99)]。具体来说，Luong的方法主要在词级进行翻译，当遇到OOV词时，会使用字符级RNN进行处理。图示展示了该模型的详细架构。
- en: On the other hand, the trying of designing a fully character-level translation
    model has also got attention accordingly. Chung et al. used BPE method to extract
    a sequence of subword in encoder side, they just varied the decoder by using pure
    characters, and it has indicated to provide comparable performance with models
    uses sub-words [[61](#bib.bib61)]. Motivated by aforementioned work, Lee et al.
    proposed fully character-level NMT without any segmentation, it was based on CNN
    pooling with highway layers, which can solve the prohibitively slow speed of training
    in Luong and Manning’s work [[71](#bib.bib71)].
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，设计一个完全基于字符级的翻译模型也得到了相应的关注。Chung 等人使用了 BPE 方法在编码器端提取子词序列，他们只是通过使用纯字符来变化解码器，并且表明其性能与使用子词的模型相当
    [[61](#bib.bib61)]。受上述工作的启发，Lee 等人提出了完全基于字符级的 NMT，没有任何分词，基于 CNN 池化和高速公路层，这可以解决
    Luong 和 Manning 工作中训练速度极其缓慢的问题 [[71](#bib.bib71)]。
- en: VI Advanced models
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 高级模型
- en: This section gives a demonstration of some advanced models that have got the
    state-of-the-art performance, while all of them belong to different categories
    of model structure. Experimental result indicated that all these networks can
    achieve similar performance with different advantages in their corresponding aspects.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了一些具有最先进性能的高级模型，虽然它们都属于不同类别的模型结构。实验结果表明，这些网络在各自的方面具有不同的优势，能够实现相似的性能。
- en: VI-A ConvS2S
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A ConvS2S
- en: ConvS2S is short for Convolutional Sequence to Sequence, which is an end-to-end
    NMT model proposed by Gehring et al.[[82](#bib.bib82)]. Different with most of
    RNN based NMT models, ConvS2S is entirely CNN based model both in encoder and
    decoder. In the network structure, ConvS2S stacked 15 layers of CNN in its encoder
    and decoder with fixed kernel width of 3\. This deep structure helps to mitigate
    the weakness in capturing context information.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ConvS2S 是 Convolutional Sequence to Sequence 的缩写，是由 Gehring 等人提出的端到端 NMT 模型
    [[82](#bib.bib82)]。与大多数基于 RNN 的 NMT 模型不同，ConvS2S 完全基于 CNN，无论是在编码器还是解码器中。在网络结构中，ConvS2S
    在其编码器和解码器中堆叠了 15 层 CNN，卷积核宽度固定为 3。这个深层结构有助于减轻捕捉上下文信息的弱点。
- en: In respect of network details, ConvS2S applied Gated Linear Units (GLU)[[100](#bib.bib100)]
    in building network, which provide a gated function for output of convolution
    layer. Specifically, the output of convolution layer $Y\in\mathbb{R}^{2d}$ which
    is a vector with double times dimensions (2$d$ numbers of dimensions) of each
    input element’s embedding ($d$ numbers of dimensions), the gated function processes
    the output $Y=[AB]\in\mathbb{R}^{2d}$ by implementing the equation $8$, where
    both $A$ and $B$ are $d$ dimensions vector, and the function $\sigma(B)$ is a
    gated function used to control which inputs $A$ of the current context are relevant.
    This non-linearity operation has been proved to be more effective in applying
    training language model[[100](#bib.bib100)] , surpassing those only applying $tanh$
    function on $A$ [[140](#bib.bib140)]. In addition, ConvS2S also used residual
    connection [[141](#bib.bib141)] between different convolution layers.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络细节方面，ConvS2S 在构建网络时应用了 Gated Linear Units (GLU)[[100](#bib.bib100)]，它为卷积层的输出提供了一个门控函数。具体来说，卷积层的输出
    $Y\in\mathbb{R}^{2d}$ 是一个维度为双倍的向量（2$d$ 维），其中每个输入元素的嵌入维度为 $d$，门控函数通过实现方程 $8$ 处理输出
    $Y=[AB]\in\mathbb{R}^{2d}$，其中 $A$ 和 $B$ 都是 $d$ 维向量，函数 $\sigma(B)$ 是一个门控函数，用于控制当前上下文中哪些输入
    $A$ 是相关的。这种非线性操作已被证明在训练语言模型中更为有效[[100](#bib.bib100)]，超越了仅在 $A$ 上应用 $tanh$ 函数的模型
    [[140](#bib.bib140)]。此外，ConvS2S 还在不同的卷积层之间使用了残差连接 [[141](#bib.bib141)]。
- en: '|  | $v([AB])=A\otimes\sigma(B)$ |  | (9) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $v([AB])=A\otimes\sigma(B)$ |  | (9) |'
- en: 'Besides the innovation of CNN based encoder-decoder structure, ConvS2S also
    applied similar Attention Mechanism that has been wildly accepted by RNN model,
    called Multi-step Attention. Concretely, Multi-step Attention is a separate attention
    structure applied in each decoder layer. In the process of calculating attention,
    the current hidden state $d_{i}^{l}$ (i.e., the output of the $l$th layer) has
    combined with previous output embedding $g_{i}$ as a vector of decoder state summary
    $d_{i}^{l}$:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于 CNN 的编码器-解码器结构的创新外，ConvS2S 还应用了类似于 RNN 模型广泛接受的注意力机制，称为多步注意力。具体而言，多步注意力是一种在每个解码器层中应用的独立注意力结构。在计算注意力的过程中，当前隐藏状态
    $d_{i}^{l}$（即第 $l$ 层的输出）与先前的输出嵌入 $g_{i}$ 结合，形成解码器状态汇总 $d_{i}^{l}$ 的向量：
- en: '|  | $d_{i}^{l}=W_{d}^{l}h_{i}^{l}+b_{d}^{l}+g_{i}$ |  | (10) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{i}^{l}=W_{d}^{l}h_{i}^{l}+b_{d}^{l}+g_{i}$ |  | (10) |'
- en: Then the attention vector $a_{ij}^{l}$ (i.e., the attention of state $i$ with
    source element $j$ in decoder layer $l$) would be driven by the dot-product of
    the summary vector with the output of the final encoder layer $z_{j}^{u}$.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，注意力向量 $a_{ij}^{l}$（即解码器层 $l$ 中状态 $i$ 对源元素 $j$ 的注意力）将由总结向量与最终编码器层输出 $z_{j}^{u}$
    的点积决定。
- en: '|  | $a_{ij}^{l}=\frac{\exp\left(d_{i}^{l}\cdot z_{j}^{u}\right)}{\sum_{t=1}^{m}\exp\left(d_{i}^{l}\cdot
    z_{t}^{u}\right)}$ |  | (11) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $a_{ij}^{l}=\frac{\exp\left(d_{i}^{l}\cdot z_{j}^{u}\right)}{\sum_{t=1}^{m}\exp\left(d_{i}^{l}\cdot
    z_{t}^{u}\right)}$ |  | (11) |'
- en: Lastly, the context vector is calculated as the weighted average of the attention
    vector $a_{ij}^{l}$ with the encoder output $z_{j}^{u}$ as well as the encoder
    input $e_{j}$.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，上下文向量被计算为注意力向量 $a_{ij}^{l}$ 与编码器输出 $z_{j}^{u}$ 以及编码器输入 $e_{j}$ 的加权平均。
- en: '|  | $c_{i}^{l}=\sum_{j=1}^{m}a_{ij}^{l}\left(z_{j}^{u}+e_{j}\right)$ |  |
    (12) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{i}^{l}=\sum_{j=1}^{m}a_{ij}^{l}\left(z_{j}^{u}+e_{j}\right)$ |  |
    (12) |'
- en: '![Refer to caption](img/611c6d8f863192a9ce9e6e403fa526cd.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/611c6d8f863192a9ce9e6e403fa526cd.png)'
- en: 'Figure 12: The structure of ConvS2S model, a successful CNN based NMT model
    with competitive performance to the state-of-the-art'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：ConvS2S 模型的结构，这是一个成功的 CNN 基于 NMT 模型，与最先进技术具有竞争力的性能
- en: VI-B RNMT+
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B RNMT+
- en: 'RNMT+ was proposed by Chen et al.[[103](#bib.bib103)]. This model has directly
    inherited the structure of GNMT model that was proposed by Wu et al.[[24](#bib.bib24)].
    Specifically, RNMT+ can be seen as an enhanced GNMT model, which demonstrated
    the best performance of RNN based NMT model. In model structure, RNMT+ mainly
    differs from the GNMT model in the following several perspectives:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: RNMT+ 由 Chen 等人提出 [[103](#bib.bib103)]。该模型直接继承了 Wu 等人提出的 GNMT 模型的结构 [[24](#bib.bib24)]。具体来说，RNMT+
    可以看作是增强版的 GNMT 模型，展示了基于 RNN 的 NMT 模型的最佳性能。在模型结构上，RNMT+ 主要在以下几个方面与 GNMT 模型不同：
- en: First, RNMT+ used six bi-directional RNN (LSTM) in its decoder, whereas GNMT
    used one layer of bi-directional RNN with seven layers of unidirectional RNN.
    This structure has sacrificed the computation efficiency in return for the extreme
    performance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，RNMT+ 在其解码器中使用了六个双向 RNN（LSTM），而 GNMT 使用了一层双向 RNN 和七层单向 RNN。这种结构牺牲了计算效率，以换取极致的性能。
- en: Second, RNMT+ applied Multi-head additive attention instead of the single-head
    attention in conventional NMT model, which can be seen as taking advantage of
    Transformer model.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，RNMT+ 应用了多头加性注意力，而不是传统 NMT 模型中的单头注意力，这可以看作是利用了 Transformer 模型的优势。
- en: Third, synchronous training strategy was provided in the training process, which
    improved the convergence speed with model performance based on empirical results[[102](#bib.bib102)].
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，训练过程中提供了同步训练策略，基于经验结果[[102](#bib.bib102)]，提高了模型性能的收敛速度。
- en: In addition, inspired by Transformer model, per-gate layer normalization [[101](#bib.bib101)]
    was applied, which has indicated to be helpful in stabilizing model training.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，受到 Transformer 模型的启发，应用了每个门的层归一化 [[101](#bib.bib101)]，这已被证明对稳定模型训练有帮助。
- en: '![Refer to caption](img/728f362f71b6c713e1b0eb18e722c97e.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/728f362f71b6c713e1b0eb18e722c97e.png)'
- en: 'Figure 13: The structure of RNMT+ model,which has the similar structure of
    GNMT with adaptive innovation in Attention Mechanism'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：RNMT+ 模型的结构，与 GNMT 结构相似，并在注意力机制中进行了适应性创新
- en: VI-C Transformer and Transformer based models
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C Transformer 和基于 Transformer 的模型
- en: Transformer is a new NMT structure proposed by Vaswani et al.[[25](#bib.bib25)].
    Different from existing NMT models, it has abandoned the standard RNN/CNN structures
    and designed an innovative multi-layer self-attention blocks that are incorporated
    with a positional encoding method. This new trend of structure design takes the
    advantages from both RNN and CNN based model, which has been further used for
    initializing the input representation for other NLP tasks. Notably, Transformer
    is a complete Attention based NMT model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是 Vaswani 等人提出的一种新的 NMT 结构 [[25](#bib.bib25)]。与现有的 NMT 模型不同，它抛弃了标准的
    RNN/CNN 结构，设计了一种创新的多层自注意力块，并结合了位置编码方法。这种新的结构设计趋势兼具了 RNN 和 CNN 模型的优势，并进一步用于初始化其他
    NLP 任务的输入表示。值得注意的是，Transformer 是一个完全基于注意力的 NMT 模型。
- en: VI-C1 Structure of the model
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C1 模型结构
- en: Transformer has its distinct structure in its model, where the major differences
    are the input representation and multi-head attention.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 在其模型中具有独特的结构，其主要差异在于输入表示和多头注意力机制。
- en: (1) Input representation
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 输入表示
- en: Transformer has its unique representation in handling input data that quite
    different with recurrent or convolution model. For computing Self-Attention we
    mentioned, transformer handles the input as three kind of vectors for different
    purpose. They are Key,Value and Query vectors. And all these vectors are driven
    by multiplying the input embedding with three matrices that we trained during
    the training process.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 在处理输入数据方面具有独特的表现，与递归或卷积模型有很大不同。为了计算自注意力，Transformer 将输入处理为三种不同的向量：Key、Value
    和 Query。这些向量都是通过将输入嵌入与我们在训练过程中训练的三个矩阵相乘得到的。
- en: 'Also, there’s Positional Encoding method was applied for enhancing the modeling
    ability of sequence order, since Transformer has abandoned recurrence structure,
    this kind of method made a compensation by inject word order information in to
    feature vector, which can avoid the model to become invariant to sequence ordering
    [[148](#bib.bib148)]. Specifically, Transformer add Positional Encoding to the
    input embedding at the bottoms of the encoder and decoder stacks, the Positional
    Encoding has been designed to have the same dimension with model embedding and
    thus could be summed. Positional Encoding can be calculated by applying positional
    functions directly or be learned [[82](#bib.bib82)], with be proven to have similar
    performance in final evaluation. In Transformer, adding Positional Encoding by
    using $since$ and $cosine$ functions is finally chosen, and each position can
    be encoded in the following way:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了增强序列顺序的建模能力，使用了位置编码方法，因为 Transformer 已放弃了递归结构，这种方法通过将词序信息注入特征向量中来弥补，从而避免模型对序列顺序不变[[148](#bib.bib148)]。具体来说，Transformer
    在编码器和解码器堆栈底部的输入嵌入中添加位置编码，位置编码设计为与模型嵌入具有相同的维度，因此可以进行求和。位置编码可以通过直接应用位置函数计算或通过学习得到[[82](#bib.bib82)]，在最终评估中已被证明具有相似的性能。在
    Transformer 中，最终选择了使用 $sin$ 和 $cos$ 函数添加位置编码，每个位置可以按以下方式进行编码：
- en: '|  | $\begin{array}[]{r}{PE_{(pos,2i)}=\sin\left(pos/10000^{\left.2i/d_{\text{model
    }}\right)}\right.}\\ {PE_{(\text{pos, }2i+1)}=\cos\left(\text{pos}/10000^{2i/d_{\text{model
    }}}\right)}\end{array}$ |  | (13) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{array}[]{r}{PE_{(pos,2i)}=\sin\left(pos/10000^{\left.2i/d_{\text{model
    }}\right)}\right.}\\ {PE_{(\text{pos, }2i+1)}=\cos\left(\text{pos}/10000^{2i/d_{\text{model
    }}}\right)}\end{array}$ |  | (13) |'
- en: where $pos$ indicates the position and $i$ indicates the dimension. That is,
    each dimension of the positional encoding corresponds to a sinusoid function,
    and the wavelengths form a geometric progression from $2\pi$ to $20000\pi$. The
    reason of choosing these functions is that they have been assumed theoretically
    in helping the model to learn to attend by relative positions easily, due to the
    characters that for any fixed offset $k$, $PE_{pos}+k$ can be represented as a
    linear function of $PE_{pos}$.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $pos$ 表示位置，$i$ 表示维度。也就是说，每个位置编码的维度对应一个正弦函数，波长从 $2\pi$ 到 $20000\pi$ 形成几何级数。选择这些函数的原因是理论上它们有助于模型通过相对位置轻松学习关注，由于其特点，对于任何固定偏移量
    $k$，$PE_{pos}+k$ 可以表示为 $PE_{pos}$ 的线性函数。
- en: (2) Multi-head Self-Attention
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 多头自注意力
- en: Self-Attention is the major innovation in Transformer. But in implementation,
    rather than just computing the Self-Attention once, the multi-head mechanism runs
    through the scaled dot-product attention multiple times in parallel, and the outputs
    of these independent attention are then concatenated and linearly transformed
    into the expected dimensions. This multiple times Self-Attention computation is
    called Multi-head Self-Attention, which is applied for allowing the model to jointly
    attend to information from different representation sub-spaces at different positions.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是 Transformer 的主要创新。但在实现中，多头机制并非仅计算一次自注意力，而是通过多次并行运行缩放点积注意力，将这些独立注意力的输出拼接并线性变换为期望的维度。这种多次自注意力计算称为多头自注意力，它允许模型同时关注来自不同表示子空间的信息。
- en: (3) Encoder & Decoder Blocks
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 编码器和解码器模块
- en: The encoder is built by 6 identical components, each of which contains one multi-head
    attention layer with a fully connected network above it. These two sub-layers
    are equipped with residual connection as well as layer normalization. All the
    sub-layers have the same dimension of 512 in output data.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由6个相同的组件构建，每个组件包含一个多头注意力层和一个位于其上的全连接网络。这两个子层都配有残差连接和层归一化。所有子层的输出数据维度都是512。
- en: The decoder, however, is more complicated. There are also 6 components stacked;
    and in each component, three sub-layers are connected, including two sub-layers
    of multi-head self attention and one sub-layer of fully-connected neural network.
    Specifically, the bottom attention layer is modified with method called masked
    to prevent positions from attending to subsequent positions, which is used for
    avoiding the model to look into the future of the target sequence when predicting
    the current word. Additionally, the second attention layer (the top attention
    layer) performs multi-head attention over the output of the encoder stack.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解码器更为复杂。它也堆叠了6个组件；每个组件中连接了三个子层，包括两个多头自注意力子层和一个全连接神经网络子层。具体来说，底部的注意力层采用了称为
    masked 的方法，以防止位置关注后续位置，这用于避免模型在预测当前词时查看目标序列的未来。此外，第二个注意力层（顶部注意力层）对编码器堆栈的输出执行多头注意力。
- en: VI-C2 Transformer based NMT variants
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C2 基于 Transformer 的 NMT 变体
- en: 'Due to the tremendous performance improvement by Transformer, related refinement
    has got huge attention for researchers. The well accepted weakness of vanilla
    Transformer includes: lacking of recurrence modeling, theoretically not Turing-complete,
    capturing position information, as well as large model complexity. All these drawbacks
    have hindered its further improvement of translation performance. In response
    to these problems, some adjustments have been proposed for getting a better model.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Transformer 带来的巨大性能提升，相关的改进引起了研究人员的极大关注。标准 Transformer 的公认弱点包括：缺乏递归建模，理论上不是图灵完备，捕捉位置信息，以及模型复杂度大。所有这些缺点阻碍了其翻译性能的进一步提升。针对这些问题，已经提出了一些调整方案，以期获得更好的模型。
- en: In respect of model architecture, some proposed modifications focused on both
    in depth of attention layer and network composition. Bapna et al. has proposed
    2-3x deeper Transformer with a refined attention mechanism, which can be easier
    for the optimization of deeper models [[152](#bib.bib152)]. The refined attention
    mechanism extended its connection to each encoder layers, like a weighted residual
    connections along the encoder depth, which allows the model to flexibly adjust
    the gradient flow to different layers of encoder. Similarly, Wang et al. [[145](#bib.bib145)]
    proposed a more deeper Transformer model (25 layers of encoder), which continues
    the same line of Bapna et al.(2018)’s work with properly applying layer normalization
    and a novel output combination method.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型架构方面，一些提出的修改聚焦于注意力层的深度和网络组成。Bapna 等人提出了一个深度增加了2-3倍的 Transformer，具有改进的注意力机制，这使得优化更深模型变得更加容易[[152](#bib.bib152)]。改进的注意力机制扩展了与每个编码器层的连接，就像沿编码器深度的加权残差连接，这允许模型灵活地调整梯度流到不同的编码器层。同样，Wang
    等人[[145](#bib.bib145)] 提出了一个更深的 Transformer 模型（25层编码器），该模型在应用层归一化和一种新颖的输出组合方法时，延续了
    Bapna 等人（2018年）工作的方向。
- en: In contrast to the fixed layers of NMT model, Dehghani et al. proposed Universal
    Transformers, which cancelled to stack the constant number of layers by combining
    recurrent inductive bias of RNNs and Adaptive Computation Time halting mechanism,
    thus enhanced the original self-attention based representation for better learning
    iterative or recursive transformations. Notably, this adjustment has made the
    model be shown to be Turing-complete under certain assumptions[[142](#bib.bib142)].
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 与固定层数的 NMT 模型相比，Dehghani 等人提出了通用 Transformer，取消了通过结合 RNN 的递归归纳偏置和自适应计算时间停止机制来堆叠常数层数，从而增强了原始自注意力基础的表示，以更好地学习迭代或递归转换。值得注意的是，这一调整使得模型在某些假设下被证明是图灵完备的[[142](#bib.bib142)]。
- en: As for refinement in network composition, inspired by the thinking of AutoML,
    So et al. applied neural architecture search (NAS) to find a comparable model
    with simplified architecture[[146](#bib.bib146)]. The Evolved Transformer proposed
    in [[146](#bib.bib146)] has an innovative combination of basic blocks achieves
    the same quality as the original Transformer-Big model with 37.6% less parameters.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络组成方面，受 AutoML 思路的启发，So 等人应用了神经架构搜索（NAS）来寻找具有简化架构的可比模型[[146](#bib.bib146)]。在[[146](#bib.bib146)]中提出的演化
    Transformer 具有创新的基本模块组合，达到了与原始 Transformer-Big 模型相同的质量，但参数减少了37.6%。
- en: While most modification is focus on changing model structure directly, some
    new literature has chosen to utilize different input representation to improve
    model performance. One direct method is using enhanced Position Encoding for sequence
    order injection, where vanilla Transformer has weakness in capturing position
    information. Shaw et al. proposed modified self-attention mechanism with awareness
    of utilizing representations of relative positions, which demonstrated to have
    a significant improvements in two MT tasks[[147](#bib.bib147)].
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数修改集中在直接改变模型结构上，但一些新的文献选择利用不同的输入表示来提升模型性能。一种直接的方法是使用增强的位置编码来注入序列顺序，其中原始Transformer在捕捉位置信息方面存在弱点。Shaw等人提出了改进的自注意力机制，考虑到相对位置表示，这在两个MT任务中展示了显著的改进[[147](#bib.bib147)]。
- en: Concurrently, using pre-initialized input representation with fine tune is another
    orientation, where some attempt have been proposed in different NLP tasks such
    as applying ELMo[[150](#bib.bib150)] for encoder of NMT model[[155](#bib.bib155)].
    In terms of Transformer, one by-product of this innovative model is using self-attention
    for representing sequence, which can effectively fused word information with contextual
    information. In later, Two well-known Transformer based input representation methods
    were been proposed named Bert (Bidirectional Encoder Representation from Transformers)[[149](#bib.bib149)]
    and GPT (Generative Pre-trained Transformer)[[151](#bib.bib151)], which has been
    indicated to bring improvement in some downstream NLP tasks. As for applying Transformer
    based pre-trained model in NMT task, more recently, this kind of trying has also
    been realized by using Bert as additional embedding layer or applying Bert as
    pre-trained model directly[[154](#bib.bib154)], which has been indicated to provide
    a bit better performance than vanilla Transformer after fine tune. Additionally,
    directly applying Bert as pre-trained model has been proved to have similar performance
    and thus can be more convenient for encoder initialization.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，使用预初始化的输入表示并进行微调是另一个方向，在不同的NLP任务中提出了一些尝试，例如将ELMo[[150](#bib.bib150)]应用于NMT模型的编码器[[155](#bib.bib155)]。在Transformer方面，这种创新模型的副产品之一是使用自注意力来表示序列，这可以有效地融合单词信息和上下文信息。后来，提出了两个著名的基于Transformer的输入表示方法，分别是Bert（双向编码器表示从Transformer）[[149](#bib.bib149)]和GPT（生成预训练Transformer）[[151](#bib.bib151)]，这些方法在一些下游NLP任务中表现出改进。至于将基于Transformer的预训练模型应用于NMT任务，最近，这种尝试也通过将Bert作为附加嵌入层或直接将Bert作为预训练模型[[154](#bib.bib154)]实现，这在微调后比原始Transformer表现稍好。此外，直接将Bert作为预训练模型应用已被证明具有类似的性能，因此在编码器初始化时更为方便。
- en: The full structure of Transformer is illustrated in Fig. [14](#S6.F14 "Figure
    14 ‣ VI-C2 Transformer based NMT variants ‣ VI-C Transformer and Transformer based
    models ‣ VI Advanced models ‣ A Survey of Deep Learning Techniques for Neural
    Machine Translation").
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的完整结构如图[14](#S6.F14 "图14 ‣ VI-C2 基于Transformer的NMT变体 ‣ VI-C Transformer及基于Transformer的模型
    ‣ VI 高级模型 ‣ 深度学习技术在神经机器翻译中的调查")所示。
- en: '![Refer to caption](img/3207628f63fa3c55c23f5c1f0475683f.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3207628f63fa3c55c23f5c1f0475683f.png)'
- en: 'Figure 14: The full structure of Transformer'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：Transformer的完整结构
- en: VII Future trend
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 未来趋势
- en: Although we have witnessed the fast-growing research progresses in NMT, there
    are still many challenges. Based on the extensive investigation [[121](#bib.bib121)][[125](#bib.bib125)][[117](#bib.bib117)][[116](#bib.bib116)],
    we summarize the major challenges and list some potential directions in the following
    several aspects.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们见证了NMT领域快速增长的研究进展，但仍面临许多挑战。基于广泛的调查[[121](#bib.bib121)][[125](#bib.bib125)][[117](#bib.bib117)][[116](#bib.bib116)]，我们总结了主要挑战，并列出了以下几个方面的潜在方向。
- en: '(1) In terms of translation performance, NMT still doesn’t perform well in
    translating long sentences. This is mainly because of two reasons: the practical
    limitation in engineering and the learning ability of the model itself. For the
    first reason, some academic experiments have chosen to ignore part of the long
    sentence that exceeds the RNN length. But we do believe that it’s not the same
    thing when NMT has been deployed in industrial applications. For the second reason,
    as research progresses, the model architecture would be more complicated. For
    example, Transformer model has applied innovative structure in its design which
    brought significant improvement in translation quality and speed[[25](#bib.bib25)].
    We believe that more refinements in model structure would be proposed. As we all
    know that RNN based NMT takes its advantages in modeling sequence order but results
    in computational inefficiency. More future work would consider the trade off between
    these two aspects.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 就翻译性能而言，NMT 在翻译长句时仍表现不佳。这主要有两个原因：工程上的实际限制和模型本身的学习能力。针对第一个原因，一些学术实验选择忽略超出
    RNN 长度的长句部分。但我们认为，在工业应用中，情况并非如此。对于第二个原因，随着研究的进展，模型架构会变得更加复杂。例如，Transformer 模型在其设计中应用了创新结构，带来了翻译质量和速度的显著提升[[25](#bib.bib25)]。我们相信，模型结构的进一步优化将被提出。众所周知，基于
    RNN 的 NMT 在建模序列顺序方面有优势，但导致计算效率低下。未来的工作将更多地考虑这两个方面之间的权衡。
- en: (2) Alignment mechanism is essential for both SMT and NMT models. For the vast
    majority of NMT models, Attention Mechanism plays the functional role in the alignment
    task, while it arguably does broader work than conventional alignment models.
    We believe this advanced alignment method would still get attraction in future
    research, since powerful attention method can improve the model performance directly.
    Later research in attention mechanism would try to relieve the weakness in NMT
    such as interpretation ability[[116](#bib.bib116)].
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 对于 SMT 和 NMT 模型，**对齐机制**至关重要。对于绝大多数 NMT 模型而言，**注意机制**在对齐任务中发挥功能作用，而且它的工作范围超出了传统对齐模型。我们相信，这种先进的对齐方法在未来的研究中仍会受到关注，因为强大的注意方法可以直接提升模型性能。后续的注意机制研究将试图缓解
    NMT 中的弱点，如解释能力[[116](#bib.bib116)]。
- en: (3) Vocabulary coverage problem has always affected most of the NMT models.
    The research trend in handling computation load of softmax operation would pursue.
    And we have also found new training strategy which supports large vocabulary size.
    Besides, research of NMT operating in sub-word or character level has also aroused
    in recent years, which provided additional solution beyond traditional scope.
    More importantly, solving sophisticated translation scenario such as informal
    spelling is also a hot spot. Current NMT model integrated with character-level
    network has alleviated this phenomenon. Future work should focus on handling all
    kinds of OOV words in a more flexible way.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 词汇覆盖问题一直影响着大多数 NMT 模型。处理 softmax 操作计算负担的研究趋势将持续。同时，我们还发现了支持大词汇量的新训练策略。此外，近年来对
    NMT 在子词或字符级别操作的研究也引起了关注，提供了超越传统范围的额外解决方案。更重要的是，解决复杂的翻译场景，例如非正式拼写，也是一个热点。目前，集成了字符级网络的
    NMT 模型已经缓解了这一现象。未来的工作应专注于以更灵活的方式处理各种 OOV 词汇。
- en: (4) Low-Resource Neural Machine Translation[[125](#bib.bib125)] is another hot
    spot in current NMT, which tries to solve the severe performance reduction when
    NMT model is trained with rare bilingual corpus. Since the aforementioned scenario
    happened commonly in practice where some seldom-used languages don’t have enough
    data, we do believe this filed would be extended in further research. Multilingual
    translation method[[111](#bib.bib111)][[110](#bib.bib110)] is the commonly proposed
    method which incorporated multi-language pair of data to improve NMT performance.
    It may need more interpretation about the different results in choosing different
    language pairs. Besides, unsupervised method has utilized the additional dataset
    and provided pre-trained model. Further research could improve its effect and
    provide hybrid training strategy with traditional method [[122](#bib.bib122)][[123](#bib.bib123)][[124](#bib.bib124)].
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 低资源神经机器翻译[[125](#bib.bib125)]是当前 NMT 领域的另一个热点，旨在解决使用稀有双语语料训练 NMT 模型时的严重性能下降问题。由于上述情况在实践中常见，其中一些不常用的语言没有足够的数据，我们相信这个领域将在进一步研究中得到扩展。多语言翻译方法[[111](#bib.bib111)][[110](#bib.bib110)]是常见的提议方法，它结合了多语言对数据以提高
    NMT 性能。对于选择不同语言对的不同结果，可能需要更多的解释。此外，无监督方法利用了附加数据集并提供了预训练模型。进一步的研究可以改善其效果，并提供与传统方法结合的混合训练策略[[122](#bib.bib122)][[123](#bib.bib123)][[124](#bib.bib124)]。
- en: (5) Finally, research in NMT applications would also become more abundant. Currently,
    many applications have been developed such as speech translation[[107](#bib.bib107)][[106](#bib.bib106)]
    and document level translation[[105](#bib.bib105)]. We believe that various applications
    (especially end-to-end tasks) would emerge in the future. We strongly hope that
    an AI based simultaneous translation system could be applied in large-scale, which
    can bring huge benefit to our human society[[108](#bib.bib108)].
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 最终，NMT 应用研究也将变得更加丰富。目前，已经开发了许多应用程序，如语音翻译[[107](#bib.bib107)][[106](#bib.bib106)]和文档级翻译[[105](#bib.bib105)]。我们相信未来会出现各种应用（特别是端到端任务）。我们强烈希望基于
    AI 的实时翻译系统能够大规模应用，这将给人类社会带来巨大的好处[[108](#bib.bib108)]。
- en: References
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Klein, G., Kim, Y., Deng, Y., Senellart, J., & Rush, A. M. (2017). Opennmt:
    Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Klein, G., Kim, Y., Deng, Y., Senellart, J., & Rush, A. M. (2017). Opennmt：开源神经机器翻译工具包。arXiv
    预印本 arXiv:1701.02810。'
- en: '[2] Forcada, M. L., Ginestí-Rosell, M., Nordfalk, J., O’Regan, J., Ortiz-Rojas,
    S., Pérez-Ortiz, J. A., … & Tyers, F. M. (2011). Apertium: a free/open-source
    platform for rule-based machine translation. Machine translation, 25(2), 127-144.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Forcada, M. L., Ginestí-Rosell, M., Nordfalk, J., O’Regan, J., Ortiz-Rojas,
    S., Pérez-Ortiz, J. A., … & Tyers, F. M. (2011). Apertium：一个免费的开源规则基础机器翻译平台。机器翻译，25（2），127-144。'
- en: '[3] Koehn, P., Och, F. J., & Marcu, D. (2003, May). Statistical phrase-based
    translation. In Proceedings of the 2003 Conference of the North American Chapter
    of the Association for Computational Linguistics on Human Language Technology-Volume
    1 (pp. 48-54). Association for Computational Linguistics.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Koehn, P., Och, F. J., & Marcu, D. (2003年5月). 基于统计的短语翻译。第2003届北美计算语言学协会人类语言技术会议论文集-第1卷（第48-54页）。计算语言学协会。'
- en: '[4] Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi,
    N., … & Dyer, C. (2007, June). Moses: Open source toolkit for statistical machine
    translation. In Proceedings of the 45th annual meeting of the association for
    computational linguistics companion volume proceedings of the demo and poster
    sessions (pp. 177-180).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi,
    N., … & Dyer, C. (2007年6月). Moses：统计机器翻译的开源工具包。第45届计算语言学协会年会论文集（演示和海报环节）（第177-180页）。'
- en: '[5] Chorowski, J., Bahdanau, D., Cho, K., & Bengio, Y. (2014). End-to-end continuous
    speech recognition using attention-based recurrent nn: First results. arXiv preprint
    arXiv:1412.1602.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Chorowski, J., Bahdanau, D., Cho, K., & Bengio, Y. (2014). 基于注意力机制的端到端连续语音识别：初步结果。arXiv
    预印本 arXiv:1412.1602。'
- en: '[6] Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic
    language model. Journal of machine learning research, 3(Feb), 1137-1155.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). 一种神经概率语言模型。机器学习研究期刊，3（2月），1137-1155。'
- en: '[7] Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). On the
    properties of neural machine translation: Encoder-decoder approaches. arXiv preprint
    arXiv:1409.1259.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014)。关于神经机器翻译的性质：编码器-解码器方法。arXiv预印本
    arXiv:1409.1259。'
- en: '[8] Kalchbrenner, N., & Blunsom, P. (2013). Recurrent continuous translation
    models. In Proceedings of the 2013 Conference on Empirical Methods in Natural
    Language Processing (pp. 1700-1709).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Kalchbrenner, N., & Blunsom, P. (2013)。递归连续翻译模型。载于《2013年自然语言处理经验方法会议论文集》（第1700-1709页）。'
- en: '[9] Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation
    of word representations in vector space. arXiv preprint arXiv:1301.3781.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013)。高效的词表示向量空间估计。arXiv预印本
    arXiv:1301.3781。'
- en: '[10] Mikolov, T., Yih, W. T., & Zweig, G. (2013). Linguistic regularities in
    continuous space word representations. In Proceedings of the 2013 Conference of
    the North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies (pp. 746-751).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Mikolov, T., Yih, W. T., & Zweig, G. (2013)。连续空间词表示中的语言规律。载于《第2013年北美计算语言学协会年会论文集：人类语言技术》（第746-751页）。'
- en: '[11] Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors
    for word representation. In Proceedings of the 2014 conference on empirical methods
    in natural language processing (EMNLP) (pp. 1532-1543).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Pennington, J., Socher, R., & Manning, C. (2014)。GloVe：用于词表示的全局向量。载于《2014年自然语言处理经验方法会议论文集》（EMNLP）（第1532-1543页）。'
- en: '[12] Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., & Makhoul, J.
    (2014). Fast and robust neural network joint models for statistical machine translation.
    In Proceedings of the 52nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers) (Vol. 1, pp. 1370-1380).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., & Makhoul, J.
    (2014)。快速而稳健的神经网络联合模型用于统计机器翻译。载于《第52届计算语言学协会年会论文集（第1卷：长篇论文）》（第1卷，第1370-1380页）。'
- en: '[13] Galley, M., and Manning, C. D. (2008, October). A simple and effective
    hierarchical phrase reordering model. In Proceedings of the Conference on Empirical
    Methods in Natural Language Processing (pp. 848-856). Association for Computational
    Linguistics.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Galley, M., 和 Manning, C. D. (2008年10月)。一个简单而有效的层次短语重排模型。载于《自然语言处理经验方法会议论文集》（第848-856页）。计算语言学协会。'
- en: '[14] Chiang, D., Knight, K., & Wang, W. (2009, May). 11,001 new features for
    statistical machine translation. In Proceedings of human language technologies:
    The 2009 annual conference of the north american chapter of the association for
    computational linguistics (pp. 218-226). Association for Computational Linguistics.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Chiang, D., Knight, K., & Wang, W. (2009年5月)。11,001种用于统计机器翻译的新特征。载于《人类语言技术会议：2009年北美计算语言学协会年会论文集》（第218-226页）。计算语言学协会。'
- en: '[15] Green, S., Wang, S., Cer, D., & Manning, C. D. (2013). Fast and adaptive
    online training of feature-rich translation models. In Proceedings of the 51st
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers) (Vol. 1, pp. 311-321).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Green, S., Wang, S., Cer, D., & Manning, C. D. (2013)。快速而自适应的特征丰富翻译模型在线训练。载于《第51届计算语言学协会年会论文集（第1卷：长篇论文）》（第1卷，第311-321页）。'
- en: '[16] Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning
    with neural networks. In Advances in neural information processing systems (pp.
    3104-3112).'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Sutskever, I., Vinyals, O., & Le, Q. V. (2014)。基于神经网络的序列到序列学习。载于《神经信息处理系统进展》（第3104-3112页）。'
- en: '[17] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural
    computation, 9(8), 1735-1780.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Hochreiter, S., & Schmidhuber, J. (1997)。长短期记忆。神经计算，9(8)，1735-1780。'
- en: '[18] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
    Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder
    for statistical machine translation. arXiv preprint arXiv:1406.1078.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
    Schwenk, H., & Bengio, Y. (2014)。使用RNN编码器-解码器学习短语表示用于统计机器翻译。arXiv预印本 arXiv:1406.1078。'
- en: '[19] Gehring, J., Auli, M., Grangier, D., & Dauphin, Y. N. (2016). A convolutional
    encoder model for neural machine translation. arXiv preprint arXiv:1611.02344.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Gehring, J., Auli, M., Grangier, D., & Dauphin, Y. N. (2016)。用于神经机器翻译的卷积编码器模型。arXiv预印本
    arXiv:1611.02344。'
- en: '[20] Meng, F., Lu, Z., Wang, M., Li, H., Jiang, W., & Liu, Q. (2015). Encoding
    source language with convolutional neural network for machine translation. arXiv
    preprint arXiv:1503.01838.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 孟飞，陆志辉，王铭，李浩，姜伟，& 刘青. (2015). 使用卷积神经网络对源语言进行编码以进行机器翻译. arXiv预印本 arXiv:1503.01838.'
- en: '[21] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation
    by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 巴赫达瑙，乔，& 杨. (2014). 通过联合学习对齐和翻译进行神经机器翻译. arXiv预印本 arXiv:1409.0473.'
- en: '[22] Britz, D., Goldie, A., Luong, M. T., & Le, Q. (2017). Massive exploration
    of neural machine translation architectures. arXiv preprint arXiv:1703.03906.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 布里茨，戈尔迪，陆勇，& 乐聪慧. (2017). 对神经机器翻译架构的大规模探索. arXiv预印本 arXiv:1703.03906.'
- en: '[23] Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches
    to attention-based neural machine translation. arXiv preprint arXiv:1508.04025.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 陆勇，范浩，& 曼宁. (2015). 有效的注意力机制神经机器翻译方法. arXiv预印本 arXiv:1508.04025.'
- en: '[24] Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W.,
    … & Klingner, J. (2016). Google’s neural machine translation system: Bridging
    the gap between human and machine translation. arXiv preprint arXiv:1609.08144.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 吴越，施斯特，陈震，乐聪慧，诺鲁兹，马谢雷，… 和克林格纳. (2016). Google的神经机器翻译系统：弥合人类与机器翻译之间的差距。arXiv预印本
    arXiv:1609.08144.'
- en: '[25] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in Neural
    Information Processing Systems(pp. 5998-6008).'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 瓦斯瓦尼，沙泽尔，帕尔玛尔，乌斯科雷特，琼斯，戈麦斯，… & 波洛苏金. (2017). 注意力即是你所需要的一切. 在神经信息处理系统进展中（第5998-6008页）。'
- en: '[26] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation
    of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 郑俊，古尔切赫，乔，& 杨. (2014). 对门控递归神经网络在序列建模中的经验评估. arXiv预印本 arXiv:1412.3555.'
- en: '[27] Wu, Y., Zhang, S., Zhang, Y., Bengio, Y., & Salakhutdinov, R. R. (2016).
    On multiplicative integration with recurrent neural networks. In Advances in neural
    information processing systems (pp. 2856-2864).'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 吴越，张思，张颖，杨，& 萨拉胡丁诺夫. (2016). 关于递归神经网络中的乘法集成. 在神经信息处理系统进展中（第2856-2864页）。'
- en: '[28] Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies
    with gradient descent is difficult. IEEE transactions on neural networks, 5(2),
    157-166.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 杨，西马尔德，& 弗拉斯科尼. (1994). 使用梯度下降学习长期依赖关系是困难的. IEEE神经网络事务，5(2)，157-166.'
- en: '[29] Graves, A. (2012). Sequence transduction with recurrent neural networks.
    arXiv preprint arXiv:1211.3711.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 格雷夫斯. (2012). 使用递归神经网络的序列转导. arXiv预印本 arXiv:1211.3711.'
- en: '[30] Boulanger-Lewandowski, N., Bengio, Y., & Vincent, P. (2013, November).
    Audio Chord Recognition with Recurrent Neural Networks. In ISMIR (pp. 335-340).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 布朗杰-刘万多夫斯基，杨，& 文森特. (2013年11月). 使用递归神经网络进行音频和弦识别. 在ISMIR会议中（第335-340页）。'
- en: '[31] Graves, A. (2013). Generating sequences with recurrent neural networks.
    arXiv preprint arXiv:1308.0850.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 格雷夫斯. (2013). 使用递归神经网络生成序列. arXiv预印本 arXiv:1308.0850.'
- en: '[32] Koehn, P. (2004, September). Pharaoh: a beam search decoder for phrase-based
    statistical machine translation models. In Conference of the Association for Machine
    Translation in the Americas (pp. 115-124). Springer, Berlin, Heidelberg.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 柯恩. (2004年9月). Pharaoh：一个用于基于短语的统计机器翻译模型的束搜索解码器. 在美洲机器翻译协会会议中（第115-124页）。施普林格，柏林，海德堡.'
- en: '[33] Chiang, D. (2005, June). A hierarchical phrase-based model for statistical
    machine translation. In Proceedings of the 43rd Annual Meeting on Association
    for Computational Linguistics (pp. 263-270). Association for Computational Linguistics.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 蒋达. (2005年6月). 一种用于统计机器翻译的层次短语模型. 在第43届计算语言学协会年会上（第263-270页）。计算语言学协会.'
- en: '[34] Och, F. J., Tillmann, C., & Ney, H. (1999). Improved alignment models
    for statistical machine translation. In 1999 Joint SIGDAT Conference on Empirical
    Methods in Natural Language Processing and Very Large Corpora.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 奥赫，蒂尔曼，& 内伊. (1999). 改进的统计机器翻译对齐模型. 在1999年联合SIGDAT自然语言处理和大型语料库实证方法会议中.'
- en: '[35] Tu, Z., Lu, Z., Liu, Y., Liu, X., & Li, H. (2016). Modeling coverage for
    neural machine translation. arXiv preprint arXiv:1601.04811.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 涂志强，陆志辉，刘洋，刘轩，& 李浩. (2016). 为神经机器翻译建模覆盖度. arXiv预印本 arXiv:1601.04811.'
- en: '[36] Auli, M., Galley, M., Quirk, C.,& Zweig, G. (2013). Joint language and
    translation modeling with recurrent neural networks.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 奥利，盖利，奎克，& 兹维格. (2013). 使用递归神经网络的联合语言和翻译建模.'
- en: '[37] Auli, M., & Gao, J. (2014). Decoder integration and expected bleu training
    for recurrent neural network language models.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Auli, M., & Gao, J. (2014). 解码器集成和递归神经网络语言模型的期望蓝色训练。'
- en: '[38] Schwenk, H., & Gauvain, J. L. (2005, October). Training neural network
    language models on very large corpora. In Proceedings of the conference on Human
    Language Technology and Empirical Methods in Natural Language Processing (pp.
    201-208). Association for Computational Linguistics.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Schwenk, H., & Gauvain, J. L. (2005年10月). 在非常大语料库上训练神经网络语言模型。在人类语言技术和自然语言处理经验方法会议论文集（第201-208页）。计算语言学协会。'
- en: '[39] Schwenk, H. (2007). Continuous space language models. Computer Speech
    & Language, 21(3), 492-518.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Schwenk, H. (2007). 连续空间语言模型。计算机语音与语言，21(3)，492-518。'
- en: '[40] Schwenk, H., Dchelotte, D., & Gauvain, J. L. (2006, July). Continuous
    space language models for statistical machine translation. In Proceedings of the
    COLING/ACL on Main conference poster sessions (pp. 723-730). Association for Computational
    Linguistics.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Schwenk, H., Dchelotte, D., & Gauvain, J. L. (2006年7月). 用于统计机器翻译的连续空间语言模型。在COLING/ACL主会议海报会（第723-730页）上。计算语言学协会。'
- en: '[41] Mikolov, T., Karafiát, M., Burget, L., Černocký, J., & Khudanpur, S. (2010).
    Recurrent neural network based language model. In Eleventh annual conference of
    the international speech communication association.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Mikolov, T., Karafiát, M., Burget, L., Černocký, J., & Khudanpur, S. (2010).
    基于递归神经网络的语言模型。在国际语音通信协会第十一届年会上。'
- en: '[42] Pollack, J. B. (1990). Recursive distributed representations. Artificial
    Intelligence, 46(1-2), 77-105.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Pollack, J. B. (1990). 递归分布式表示。人工智能，46(1-2)，77-105。'
- en: '[43] Chrisman, L. (1991). Learning recursive distributed representations for
    holistic computation. Connection Science, 3(4), 345-366.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Chrisman, L. (1991). 学习递归分布式表示以进行整体计算。连接科学，3(4)，345-366。'
- en: '[44] Allen, R. B. (1987, June). Several studies on natural language and back-propagation.
    In Proceedings of the IEEE First International Conference on Neural Networks (Vol.
    2, No. S 335, p. 341). IEEE Piscataway, NJ.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Allen, R. B. (1987年6月). 多项自然语言和反向传播研究。在IEEE第一次国际神经网络会议论文集中（第2卷，第S 335号，第341页）。IEEE
    Piscataway, NJ。'
- en: '[45] Elman, J. L. (1990). Finding structure in time. Cognitive science, 14(2),
    179-211.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Elman, J. L. (1990). 发现时间中的结构。认知科学，14(2)，179-211。'
- en: '[46] JORDAN, M. (1986). Serial Order; a parallel distributed processing approach.
    ICS Report 8604, UC San Diego.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] JORDAN, M. (1986). 序列顺序；一种并行分布式处理方法。ICS报告8604，加州大学圣迭戈分校。'
- en: '[47] Jean, S., Cho, K., Memisevic, R., & Bengio, Y. (2014). On using very large
    target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Jean, S., Cho, K., Memisevic, R., & Bengio, Y. (2014). 使用非常大的目标词汇进行神经机器翻译。arXiv预印本
    arXiv:1412.2007。'
- en: '[48] Gulcehre, C., Ahn, S., Nallapati, R., Zhou, B.,& Bengio, Y. (2016). Pointing
    the unknown words. arXiv preprint arXiv:1603.08148.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Gulcehre, C., Ahn, S., Nallapati, R., Zhou, B., & Bengio, Y. (2016). 指向未知词。arXiv预印本
    arXiv:1603.08148。'
- en: '[49] Jiajun, Z., & Chengqing, Z. (2016). Towards zero unknown word in neural
    machine translation.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Jiajun, Z., & Chengqing, Z. (2016). 在神经机器翻译中趋向零未知词。'
- en: '[50] Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation
    of rare words with subword units. arXiv preprint arXiv:1508.07909.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Sennrich, R., Haddow, B., & Birch, A. (2015). 使用子词单元的稀有词神经机器翻译。arXiv预印本
    arXiv:1508.07909。'
- en: '[51] Gage, P. (1994). A new algorithm for data compression. The C Users Journal,
    12(2), 23-38.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Gage, P. (1994). 一种新的数据压缩算法。C用户期刊，12(2)，23-38。'
- en: '[52] Mnih, A., & Hinton, G. E. (2009). A scalable hierarchical distributed
    language model. In Advances in neural information processing systems (pp. 1081-1088).'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Mnih, A., & Hinton, G. E. (2009). 一种可扩展的层次分布式语言模型。在神经信息处理系统进展（第1081-1088页）。'
- en: '[53] Morin, F., & Bengio, Y. (2005, January). Hierarchical probabilistic neural
    network language model. In Aistats (Vol. 5, pp. 246-252).'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Morin, F., & Bengio, Y. (2005年1月). 层次概率神经网络语言模型。在Aistats（第5卷，第246-252页）。'
- en: '[54] Miller, G. (1998). WordNet: An electronic lexical database. MIT press.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Miller, G. (1998). WordNet: 电子词汇数据库。MIT出版社。'
- en: '[55] Bengio, Y., & Senécal, J. S. (2003, January). Quick Training of Probabilistic
    Neural Nets by Importance Sampling. In AISTATS(pp. 1-9).'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Bengio, Y., & Senécal, J. S. (2003年1月). 通过重要性采样快速训练概率神经网络。在AISTATS（第1-9页）。'
- en: '[56] Bengio, Y., & Senécal, J. S. (2008). Adaptive importance sampling to accelerate
    training of a neural probabilistic language model. IEEE Transactions on Neural
    Networks, 19(4), 713-722.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Bengio, Y., & Senécal, J. S. (2008). 自适应重要性采样加速神经概率语言模型的训练。IEEE神经网络汇刊，19(4)，713-722。'
- en: '[57] Mnih, A., & Teh, Y. W. (2012). A fast and simple algorithm for training
    neural probabilistic language models. arXiv preprint arXiv:1206.6426.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Mnih, A., & Teh, Y. W. (2012). 一种快速简单的神经概率语言模型训练算法。arXiv预印本 arXiv:1206.6426。'
- en: '[58] Vaswani, A., Zhao, Y., Fossum, V., & Chiang, D. (2013). Decoding with
    large-scale neural language models improves translation. In Proceedings of the
    2013 Conference on Empirical Methods in Natural Language Processing (pp. 1387-1392).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Vaswani, A., Zhao, Y., Fossum, V., & Chiang, D. (2013). 使用大规模神经语言模型进行解码可提高翻译效果。载于《2013年自然语言处理实证方法会议论文集》
    (第1387-1392页)。'
- en: '[59] Luong, M. T., Sutskever, I., Le, Q. V., Vinyals, O., & Zaremba, W. (2014).
    Addressing the rare word problem in neural machine translation. arXiv preprint
    arXiv:1410.8206.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Luong, M. T., Sutskever, I., Le, Q. V., Vinyals, O., & Zaremba, W. (2014).
    解决神经机器翻译中的稀有词问题。arXiv预印本 arXiv:1410.8206。'
- en: '[60] Schuster, M., & Nakajima, K. (2012, March). Japanese and korean voice
    search. In 2012 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP) (pp. 5149-5152). IEEE.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Schuster, M., & Nakajima, K. (2012年3月). 日语和韩语语音搜索。载于《2012年IEEE国际声学、语音和信号处理会议（ICASSP）》
    (第5149-5152页)。IEEE。'
- en: '[61] Chung, J., Cho, K., & Bengio, Y. (2016). A character-level decoder without
    explicit segmentation for neural machine translation. arXiv preprint arXiv:1603.06147.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Chung, J., Cho, K., & Bengio, Y. (2016). 无显式分段的字符级解码器用于神经机器翻译。arXiv预印本
    arXiv:1603.06147。'
- en: '[62] Costa-Jussa, M. R., & Fonollosa, J. A. (2016). Character-based neural
    machine translation. arXiv preprint arXiv:1603.00810.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Costa-Jussa, M. R., & Fonollosa, J. A. (2016). 基于字符的神经机器翻译。arXiv预印本 arXiv:1603.00810。'
- en: '[63] Ling, W., Trancoso, I., Dyer, C., & Black, A. W. (2015). Character-based
    neural machine translation. arXiv preprint arXiv:1511.04586.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Ling, W., Trancoso, I., Dyer, C., & Black, A. W. (2015). 基于字符的神经机器翻译。arXiv预印本
    arXiv:1511.04586。'
- en: '[64] Ling, W., Luís, T., Marujo, L., Astudillo, R. F., Amir, S., Dyer, C.,
    … & Trancoso, I. (2015). Finding function in form: Compositional character models
    for open vocabulary word representation. arXiv preprint arXiv:1508.02096.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Ling, W., Luís, T., Marujo, L., Astudillo, R. F., Amir, S., Dyer, C.,
    … & Trancoso, I. (2015). 在形式中寻找功能：用于开放词汇表示的组合字符模型。arXiv预印本 arXiv:1508.02096。'
- en: '[65] Kim, Y., Jernite, Y., Sontag, D., & Rush, A. M. (2016, March). Character-aware
    neural language models. In Thirtieth AAAI Conference on Artificial Intelligence.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Kim, Y., Jernite, Y., Sontag, D., & Rush, A. M. (2016年3月). 字符感知神经语言模型。载于《第30届AAAI人工智能会议》。'
- en: '[66] Kudo, T. (2018). Subword regularization: Improving neural network translation
    models with multiple subword candidates. arXiv preprint arXiv:1804.10959.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Kudo, T. (2018). 子词正则化：通过多个子词候选提高神经网络翻译模型。arXiv预印本 arXiv:1804.10959。'
- en: '[67] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2015, June). Gated feedback
    recurrent neural networks. In International Conference on Machine Learning (pp.
    2067-2075).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2015年6月). 门控反馈递归神经网络。载于《国际机器学习会议》
    (第2067-2075页)。'
- en: '[68] Denkowski, M., & Neubig, G. (2017). Stronger baselines for trustable results
    in neural machine translation. arXiv preprint arXiv:1706.09733.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Denkowski, M., & Neubig, G. (2017). 提升神经机器翻译可靠结果的基准。arXiv预印本 arXiv:1706.09733。'
- en: '[69] Nakazawa, T., Higashiyama, S., Ding, C., Mino, H., Goto, I., Kazawa, H.,
    … & Kurohashi, S. (2017, November). Overview of the 4th Workshop on Asian Translation.
    In Proceedings of the 4th Workshop on Asian Translation (WAT2017) (pp. 1-54).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Nakazawa, T., Higashiyama, S., Ding, C., Mino, H., Goto, I., Kazawa, H.,
    … & Kurohashi, S. (2017年11月). 第四届亚洲翻译研讨会概述。载于《第四届亚洲翻译研讨会论文集（WAT2017）》 (第1-54页)。'
- en: '[70] Choi, H., Cho, K., & Bengio, Y. (2017). Context-dependent word representation
    for neural machine translation. Computer Speech & Language, 45, 149-160.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Choi, H., Cho, K., & Bengio, Y. (2017). 用于神经机器翻译的上下文相关词汇表示。《计算机语音与语言》，45，149-160。'
- en: '[71] Lee, J., Cho, K., & Hofmann, T. (2017). Fully character-level neural machine
    translation without explicit segmentation. Transactions of the Association for
    Computational Linguistics, 5, 365-378.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Lee, J., Cho, K., & Hofmann, T. (2017). 无显式分段的完全字符级神经机器翻译。《计算语言学协会会刊》，5，365-378。'
- en: '[72] Arthur, P., Neubig, G., & Nakamura, S. (2016). Incorporating discrete
    translation lexicons into neural machine translation. arXiv preprint arXiv:1606.02006.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Arthur, P., Neubig, G., & Nakamura, S. (2016). 将离散翻译词典融入神经机器翻译。arXiv预印本
    arXiv:1606.02006。'
- en: '[73] Feng, Y., Zhang, S., Zhang, A., Wang, D., & Abel, A. (2017). Memory-augmented
    neural machine translation. arXiv preprint arXiv:1708.02005.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Feng, Y., Zhang, S., Zhang, A., Wang, D., & Abel, A. (2017). 记忆增强的神经机器翻译。arXiv预印本
    arXiv:1708.02005。'
- en: '[74] Gu, J., Lu, Z., Li, H., & Li, V. O. (2016). Incorporating copying mechanism
    in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Gu, J., Lu, Z., Li, H., & Li, V. O. (2016). 在序列到序列学习中融入复制机制。arXiv预印本 arXiv:1603.06393。'
- en: '[75] Liu, F., Lu, H., & Neubig, G. (2017). Handling homographs in neural machine
    translation. arXiv preprint arXiv:1708.06510.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Liu, F., Lu, H., & Neubig, G. (2017). 处理神经机器翻译中的同形异义词。arXiv预印本 arXiv:1708.06510。'
- en: '[76] Zhao, Y., Zhang, J., He, Z., Zong, C., & Wu, H. (2018). Addressing Troublesome
    Words in Neural Machine Translation. In Proceedings of the 2018 Conference on
    Empirical Methods in Natural Language Processing (pp. 391-400).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Zhao, Y., Zhang, J., He, Z., Zong, C., & Wu, H. (2018). 解决神经机器翻译中的问题词汇。发表于《2018年自然语言处理实证方法会议论文集》（第391-400页）。'
- en: '[77] Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A. N., Gouws,
    S., … & Sepassi, R. (2018). Tensor2tensor for neural machine translation. arXiv
    preprint arXiv:1803.07416.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A. N., Gouws,
    S., … & Sepassi, R. (2018). Tensor2tensor 用于神经机器翻译。arXiv预印本 arXiv:1803.07416。'
- en: '[78] Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L. S.
    (2019). Learning Deep Transformer Models for Machine Translation. arXiv preprint
    arXiv:1906.01787.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L. S.
    (2019). 为机器翻译学习深度变换器模型。arXiv预印本 arXiv:1906.01787。'
- en: '[79] So, D. R., Liang, C., & Le, Q. V. (2019). The evolved transformer. arXiv
    preprint arXiv:1901.11117.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] So, D. R., Liang, C., & Le, Q. V. (2019). 演变型变换器。arXiv预印本 arXiv:1901.11117。'
- en: '[80] Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018).
    Universal transformers. arXiv preprint arXiv:1807.03819.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018).
    通用变换器。arXiv预印本 arXiv:1807.03819。'
- en: '[81] Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., &
    Salakhutdinov, R. (2019). Transformer-xl: Attentive language models beyond a fixed-length
    context. arXiv preprint arXiv:1901.02860.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le, Q. V., &
    Salakhutdinov, R. (2019). Transformer-xl: 超越固定长度上下文的注意力语言模型。arXiv预印本 arXiv:1901.02860。'
- en: '[82] Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y. N. (2017,
    August). Convolutional sequence to sequence learning. In Proceedings of the 34th
    International Conference on Machine Learning-Volume 70 (pp. 1243-1252). JMLR.
    org.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Gehring, J., Auli, M., Grangier, D., Yarats, D., & Dauphin, Y. N. (2017年8月).
    卷积序列到序列学习。发表于《第34届国际机器学习会议论文集-第70卷》（第1243-1252页）。JMLR.org。'
- en: '[83] Hu, B., Tu, Z., Lu, Z., Li, H., & Chen, Q. (2015, July). Context-dependent
    translation selection using convolutional neural network. In Proceedings of the
    53rd Annual Meeting of the Association for Computational Linguistics and the 7th
    International Joint Conference on Natural Language Processing (Volume 2: Short
    Papers) (pp. 536-541).'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Hu, B., Tu, Z., Lu, Z., Li, H., & Chen, Q. (2015年7月). 使用卷积神经网络的上下文相关翻译选择。发表于《第53届计算语言学协会年会及第7届国际自然语言处理联合会议论文集（第2卷：短篇论文）》
    （第536-541页）。'
- en: '[84] Kalchbrenner, N., Espeholt, L., Simonyan, K., Oord, A. V. D., Graves,
    A., & Kavukcuoglu, K. (2016). Neural machine translation in linear time. arXiv
    preprint arXiv:1610.10099.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Kalchbrenner, N., Espeholt, L., Simonyan, K., Oord, A. V. D., Graves,
    A., & Kavukcuoglu, K. (2016). 线性时间内的神经机器翻译。arXiv预印本 arXiv:1610.10099。'
- en: '[85] Kaiser, L., Gomez, A. N., & Chollet, F. (2017). Depthwise separable convolutions
    for neural machine translation. arXiv preprint arXiv:1706.03059.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Kaiser, L., Gomez, A. N., & Chollet, F. (2017). 神经机器翻译中的深度可分离卷积。arXiv预印本
    arXiv:1706.03059。'
- en: '[86] Kaiser, Ł., & Bengio, S. (2016). Can active memory replace attention?.
    In Advances in Neural Information Processing Systems (pp. 3781-3789).'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Kaiser, Ł., & Bengio, S. (2016). 活跃记忆能否替代注意力机制？发表于《神经信息处理系统进展》（第3781-3789页）。'
- en: '[87] Tran, K., Bisazza, A., & Monz, C. (2018). The importance of being recurrent
    for modeling hierarchical structure. arXiv preprint arXiv:1803.03585.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Tran, K., Bisazza, A., & Monz, C. (2018). 模拟层级结构时递归的重要性。arXiv预印本 arXiv:1803.03585。'
- en: '[88] Zhou, J., Cao, Y., Wang, X., Li, P., & Xu, W. (2016). Deep recurrent models
    with fast-forward connections for neural machine translation. Transactions of
    the Association for Computational Linguistics, 4, 371-383.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Zhou, J., Cao, Y., Wang, X., Li, P., & Xu, W. (2016). 具有快进连接的深度递归模型用于神经机器翻译。《计算语言学协会会刊》，4，371-383。'
- en: '[89] Zhang, B., Xiong, D., Su, J., Duan, H., & Zhang, M. (2016). Variational
    neural machine translation. arXiv preprint arXiv:1605.07869.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Zhang, B., Xiong, D., Su, J., Duan, H., & Zhang, M. (2016). 变分神经机器翻译。arXiv预印本
    arXiv:1605.07869。'
- en: '[90] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G.,
    & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts
    layer. arXiv preprint arXiv:1701.06538.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G.,
    & Dean, J. (2017)。极大规模神经网络：稀疏门控专家混合层。arXiv 预印本 arXiv:1701.06538。'
- en: '[91] Zhang, B., Xiong, D., Su, J., Lin, Q., & Zhang, H. (2018). Simplifying
    Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks.
    arXiv preprint arXiv:1810.12546.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Zhang, B., Xiong, D., Su, J., Lin, Q., & Zhang, H. (2018)。通过加法-减法双门控递归网络简化神经机器翻译。arXiv
    预印本 arXiv:1810.12546。'
- en: '[92] Wang, M., Lu, Z., Zhou, J., & Liu, Q. (2017). Deep neural machine translation
    with linear associative unit. arXiv preprint arXiv:1705.00861.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Wang, M., Lu, Z., Zhou, J., & Liu, Q. (2017)。具有线性关联单元的深度神经机器翻译。arXiv 预印本
    arXiv:1705.00861。'
- en: '[93] Feng, S., Liu, S., Yang, N., Li, M., Zhou, M., & Zhu, K. Q. (2016, December).
    Improving attention modeling with implicit distortion and fertility for machine
    translation. In Proceedings of COLING 2016, the 26th International Conference
    on Computational Linguistics: Technical Papers (pp. 3082-3092).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Feng, S., Liu, S., Yang, N., Li, M., Zhou, M., & Zhu, K. Q. (2016年12月)。通过隐式失真和繁殖改进注意力建模以用于机器翻译。在
    COLING 2016 论文集，第26届计算语言学国际会议：技术论文（第3082-3092页）。'
- en: '[94] Yang, Z., Hu, Z., Deng, Y., Dyer, C., & Smola, A. (2016). Neural machine
    translation with recurrent attention modeling. arXiv preprint arXiv:1607.05108.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Yang, Z., Hu, Z., Deng, Y., Dyer, C., & Smola, A. (2016)。具有递归注意力建模的神经机器翻译。arXiv
    预印本 arXiv:1607.05108。'
- en: '[95] Cohn, T., Hoang, C. D. V., Vymolova, E., Yao, K., Dyer, C., & Haffari,
    G. (2016). Incorporating structural alignment biases into an attentional neural
    translation model. arXiv preprint arXiv:1601.01085.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Cohn, T., Hoang, C. D. V., Vymolova, E., Yao, K., Dyer, C., & Haffari,
    G. (2016)。将结构对齐偏置融入注意力神经翻译模型。arXiv 预印本 arXiv:1601.01085。'
- en: '[96] Cheng, Y., Shen, S., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015).
    Agreement-based joint training for bidirectional attention-based neural machine
    translation. arXiv preprint arXiv:1512.04650.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Cheng, Y., Shen, S., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015)。基于协议的双向注意力神经机器翻译联合训练。arXiv
    预印本 arXiv:1512.04650。'
- en: '[97] Mi, H., Wang, Z., & Ittycheriah, A. (2016). Supervised attentions for
    neural machine translation. arXiv preprint arXiv:1608.00112.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Mi, H., Wang, Z., & Ittycheriah, A. (2016)。神经机器翻译中的监督注意力。arXiv 预印本 arXiv:1608.00112。'
- en: '[98] Ballesteros, M., Dyer, C., & Smith, N. A. (2015). Improved transition-based
    parsing by modeling characters instead of words with LSTMs. arXiv preprint arXiv:1508.00657.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Ballesteros, M., Dyer, C., & Smith, N. A. (2015). 通过使用 LSTM 模型字符而非单词来改进基于过渡的解析。arXiv
    预印本 arXiv:1508.00657。'
- en: '[99] Luong, M. T., & Manning, C. D. (2016). Achieving open vocabulary neural
    machine translation with hybrid word-character models. arXiv preprint arXiv:1604.00788.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Luong, M. T., & Manning, C. D. (2016)。通过混合字-字符模型实现开放词汇的神经机器翻译。arXiv 预印本
    arXiv:1604.00788。'
- en: '[100] Dauphin, Y. N., Fan, A., Auli, M., & Grangier, D. (2017, August). Language
    modeling with gated convolutional networks. In Proceedings of the 34th International
    Conference on Machine Learning-Volume 70 (pp. 933-941). JMLR. org.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Dauphin, Y. N., Fan, A., Auli, M., & Grangier, D. (2017年8月)。使用门控卷积网络进行语言建模。在第34届国际机器学习会议论文集第70卷（第933-941页）。JMLR.org。'
- en: '[101] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization.
    arXiv preprint arXiv:1607.06450.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016)。层归一化。arXiv 预印本 arXiv:1607.06450。'
- en: '[102] Chen, J., Pan, X., Monga, R., Bengio, S., & Jozefowicz, R. (2016). Revisiting
    distributed synchronous SGD. arXiv preprint arXiv:1604.00981.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Chen, J., Pan, X., Monga, R., Bengio, S., & Jozefowicz, R. (2016)。重新审视分布式同步
    SGD。arXiv 预印本 arXiv:1604.00981。'
- en: '[103] Chen, M. X., Firat, O., Bapna, A., Johnson, M., Macherey, W., Foster,
    G., … & Wu, Y. (2018). The best of both worlds: Combining recent advances in neural
    machine translation. arXiv preprint arXiv:1804.09849.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Chen, M. X., Firat, O., Bapna, A., Johnson, M., Macherey, W., Foster,
    G., … & Wu, Y. (2018)。两全其美：结合神经机器翻译的最新进展。arXiv 预印本 arXiv:1804.09849。'
- en: '[104] Mi, H., Wang, Z.,& Ittycheriah, A. (2016). Vocabulary manipulation for
    neural machine translation. arXiv preprint arXiv:1605.03209.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Mi, H., Wang, Z., & Ittycheriah, A. (2016)。神经机器翻译中的词汇操作。arXiv 预印本 arXiv:1605.03209。'
- en: '[105] Wang, L., Tu, Z., Way, A., & Liu, Q. (2017). Exploiting cross-sentence
    context for neural machine translation. arXiv preprint arXiv:1704.04347.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Wang, L., Tu, Z., Way, A., & Liu, Q. (2017)。利用跨句上下文进行神经机器翻译。arXiv 预印本
    arXiv:1704.04347。'
- en: '[106] Weiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., & Chen, Z. (2017). Sequence-to-sequence
    models can directly translate foreign speech. arXiv preprint arXiv:1703.08581.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Weiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., & Chen, Z. (2017). 序列到序列模型可以直接翻译外语语音。arXiv
    预印本 arXiv:1703.08581。'
- en: '[107] Duong, L., Anastasopoulos, A., Chiang, D., Bird, S., & Cohn, T. (2016,
    June). An attentional model for speech translation without transcription. In Proceedings
    of the 2016 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (pp. 949-959).'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Duong, L., Anastasopoulos, A., Chiang, D., Bird, S., & Cohn, T. (2016年6月).
    一种无转录的语音翻译注意力模型。在《2016年北美计算语言学协会年会：人类语言技术论文集》（第949-959页）。'
- en: '[108] Gu, J., Neubig, G., Cho, K., & Li, V. O. (2016). Learning to translate
    in real-time with neural machine translation. arXiv preprint arXiv:1610.00388.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Gu, J., Neubig, G., Cho, K., & Li, V. O. (2016). 实时学习翻译的神经机器翻译。arXiv
    预印本 arXiv:1610.00388。'
- en: '[109] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R.,
    … & Bengio, Y. (2015, June). Show, attend and tell: Neural image caption generation
    with visual attention. In International conference on machine learning (pp. 2048-2057).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R.,
    … & Bengio, Y. (2015年6月). 展示、关注与讲述：具有视觉注意力的神经图像描述生成。在国际机器学习大会（第2048-2057页）。'
- en: '[110] Dong, D., Wu, H., He, W., Yu, D., & Wang, H. (2015, July). Multi-task
    learning for multiple language translation. In Proceedings of the 53rd Annual
    Meeting of the Association for Computational Linguistics and the 7th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers) (pp. 1723-1732).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Dong, D., Wu, H., He, W., Yu, D., & Wang, H. (2015年7月). 多语言翻译的多任务学习。在《第53届计算语言学协会年会暨第7届国际自然语言处理联合会议（第1卷：长篇论文集）》（第1723-1732页）。'
- en: '[111] Firat, O., Cho, K., & Bengio, Y. (2016). Multi-way, multilingual neural
    machine translation with a shared attention mechanism. arXiv preprint arXiv:1601.01073.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Firat, O., Cho, K., & Bengio, Y. (2016). 具有共享注意力机制的多方位、多语言神经机器翻译。arXiv
    预印本 arXiv:1601.01073。'
- en: '[112] Bojar, Ondrej, et al. “Findings of the 2015 Workshop on Statistical Machine
    Translation.” Proceedings of the Tenth Workshop on Statistical Machine Translation,
    2015, pp. 1–46.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Bojar, Ondrej, 等. “2015年统计机器翻译研讨会的发现。” 第十届统计机器翻译研讨会论文集，2015，第1–46页。'
- en: '[113] Cettolo, Mauro, et al. “The IWSLT 2015 Evaluation Campaign.” IWSLT 2015,
    International Workshop on Spoken Language Translation, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Cettolo, Mauro, 等. “IWSLT 2015 评估活动。” IWSLT 2015，国际口语语言翻译研讨会，2015。'
- en: '[114] Junczys-Dowmunt, M., Dwojak, T., & Hoang, H. (2016). Is neural machine
    translation ready for deployment? A case study on 30 translation directions. arXiv
    preprint arXiv:1610.01108.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Junczys-Dowmunt, M., Dwojak, T., & Hoang, H. (2016). 神经机器翻译是否准备好部署？关于30个翻译方向的案例研究。arXiv
    预印本 arXiv:1610.01108。'
- en: '[115] Bentivogli, L., Bisazza, A., Cettolo, M., & Federico, M. (2016). Neural
    versus phrase-based machine translation quality: a case study. arXiv preprint
    arXiv:1608.04631.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Bentivogli, L., Bisazza, A., Cettolo, M., & Federico, M. (2016). 神经网络与基于短语的机器翻译质量：案例研究。arXiv
    预印本 arXiv:1608.04631。'
- en: '[116] Chaudhari, S., Polatkan, G., Ramanath, R., & Mithal, V. (2019). An attentive
    survey of attention models. arXiv preprint arXiv:1904.02874.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Chaudhari, S., Polatkan, G., Ramanath, R., & Mithal, V. (2019). 注意力模型的全面调查。arXiv
    预印本 arXiv:1904.02874。'
- en: '[117] Galassi, A., Lippi, M., & Torroni, P. (2019). Attention, please! a critical
    review of neural attention models in natural language processing. arXiv preprint
    arXiv:1902.02181.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Galassi, A., Lippi, M., & Torroni, P. (2019). 注意力，请注意！对自然语言处理中的神经注意力模型的批判性回顾。arXiv
    预印本 arXiv:1902.02181。'
- en: '[118] Domhan, T. (2018, July). How much attention do you need? a granular analysis
    of neural machine translation architectures. In Proceedings of the 56th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp.
    1799-1808).'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Domhan, T. (2018年7月). 你需要多少注意力？神经机器翻译架构的细致分析。在《第56届计算语言学协会年会（第1卷：长篇论文集）》（第1799-1808页）。'
- en: '[119] Kaiser, Ł., & Sutskever, I. (2015). Neural gpus learn algorithms. arXiv
    preprint arXiv:1511.08228.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Kaiser, Ł., & Sutskever, I. (2015). 神经GPU学习算法。arXiv 预印本 arXiv:1511.08228。'
- en: '[120] Lipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of
    recurrent neural networks for sequence learning. arXiv preprint arXiv:1506.00019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Lipton, Z. C., Berkowitz, J., & Elkan, C. (2015). 循环神经网络用于序列学习的批判性回顾。arXiv
    预印本 arXiv:1506.00019。'
- en: '[121] Koehn, P., & Knowles, R. (2017). Six challenges for neural machine translation. arXiv
    preprint arXiv:1706.03872.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Koehn, P., & Knowles, R. (2017年)。神经机器翻译的六大挑战。arXiv预印本 arXiv:1706.03872。'
- en: '[122] He, D., Xia, Y., Qin, T., Wang, L., Yu, N., Liu, T. Y., & Ma, W. Y. (2016).
    Dual learning for machine translation. In Advances in Neural Information Processing
    Systems (pp. 820-828).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] He, D., Xia, Y., Qin, T., Wang, L., Yu, N., Liu, T. Y., & Ma, W. Y. (2016年)。机器翻译的双重学习。在神经信息处理系统进展会议论文集中（第820-828页）。'
- en: '[123] Ramachandran, P., Liu, P. J., & Le, Q. V. (2016). Unsupervised pretraining
    for sequence to sequence learning. arXiv preprint arXiv:1611.02683.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Ramachandran, P., Liu, P. J., & Le, Q. V. (2016年)。序列到序列学习的无监督预训练。arXiv预印本
    arXiv:1611.02683。'
- en: '[124] Artetxe, M., Labaka, G., Agirre, E., & Cho, K. (2017). Unsupervised neural
    machine translation. arXiv preprint arXiv:1710.11041.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Artetxe, M., Labaka, G., Agirre, E., & Cho, K. (2017年)。无监督神经机器翻译。arXiv预印本
    arXiv:1710.11041。'
- en: '[125] Sennrich, R., & Zhang, B. (2019). Revisiting Low-Resource Neural Machine
    Translation: A Case Study. arXiv preprint arXiv:1905.11901.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Sennrich, R., & Zhang, B. (2019年)。重新审视低资源神经机器翻译：一个案例研究。arXiv预印本 arXiv:1905.11901。'
- en: '[126] Schwenk, H. (2012, December). Continuous space translation models for
    phrase-based statistical machine translation. In Proceedings of COLING 2012: Posters (pp.
    1071-1080).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Schwenk, H. (2012年12月)。基于短语的统计机器翻译的连续空间翻译模型。在COLING 2012：海报论文集中（第1071-1080页）。'
- en: '[127] Rosenfeld, R. (2000). Two decades of statistical language modeling: Where
    do we go from here?. Proceedings of the IEEE, 88(8), 1270-1278.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Rosenfeld, R. (2000年)。统计语言建模的二十年：我们从这里走向何方？IEEE会议录，88(8)，1270-1278。'
- en: '[128] Stolcke, A. (2002). SRILM-an extensible language modeling toolkit. In Seventh
    international conference on spoken language processing.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Stolcke, A. (2002年)。SRILM——一个可扩展的语言建模工具包。在第七届国际语言处理大会上。'
- en: '[129] Teh, Y. W. (2006, July). A hierarchical Bayesian language model based
    on Pitman-Yor processes. In Proceedings of the 21st International Conference on
    Computational Linguistics and the 44th annual meeting of the Association for Computational
    Linguistics (pp. 985-992). Association for Computational Linguistics.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Teh, Y. W. (2006年7月)。基于Pitman-Yor过程的层次贝叶斯语言模型。在第21届国际计算语言学会议和第44届计算语言学协会年会上（第985-992页）。计算语言学协会。'
- en: '[130] Federico, M., Bertoldi, N., & Cettolo, M. (2008). IRSTLM: an open source
    toolkit for handling large scale language models. In Ninth Annual Conference of
    the International Speech Communication Association.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Federico, M., Bertoldi, N., & Cettolo, M. (2008年)。IRSTLM：一个开源工具包，用于处理大规模语言模型。在第九届国际语音通信协会年会上。'
- en: '[131] Heafield, K. (2011, July). KenLM: Faster and smaller language model queries.
    In Proceedings of the sixth workshop on statistical machine translation (pp. 187-197).
    Association for Computational Linguistics.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Heafield, K. (2011年7月)。KenLM：更快更小的语言模型查询。在第六届统计机器翻译研讨会论文集中（第187-197页）。计算语言学协会。'
- en: '[132] Son, L. H., Allauzen, A., & Yvon, F. (2012, June). Continuous space translation
    models with neural networks. In Proceedings of the 2012 conference of the north
    american chapter of the association for computational linguistics: Human language
    technologies (pp. 39-48). Association for Computational Linguistics.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Son, L. H., Allauzen, A., & Yvon, F. (2012年6月)。具有神经网络的连续空间翻译模型。在2012年北美计算语言学协会：人类语言技术会议论文集中（第39-48页）。计算语言学协会。'
- en: '[133] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &
    Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of
    machine learning research, 12(Aug), 2493-2537.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &
    Kuksa, P. (2011年)。几乎从零开始的自然语言处理。《机器学习研究杂志》，12（8月），2493-2537。'
- en: '[134] Young, T., Hazarika, D., Poria, S., & Cambria, E. (2018). Recent trends
    in deep learning based natural language processing. ieee Computational intelligenCe
    magazine, 13(3), 55-75.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Young, T., Hazarika, D., Poria, S., & Cambria, E. (2018年)。基于深度学习的自然语言处理的最新趋势。《IEEE计算智能杂志》，13(3)，55-75。'
- en: '[135] Wallach, H. M. (2006, June). Topic modeling: beyond bag-of-words. In Proceedings
    of the 23rd international conference on Machine learning (pp. 977-984). ACM.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Wallach, H. M. (2006年6月)。主题建模：超越词袋模型。在第23届国际机器学习会议论文集中（第977-984页）。ACM。'
- en: '[136] Song, F., & Croft, W. B. (1999, November). A general language model for
    information retrieval. In Proceedings of the eighth international conference on
    Information and knowledge management (pp. 316-321). ACM.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Song, F., & Croft, W. B. (1999年11月)。用于信息检索的通用语言模型。见第八届国际信息与知识管理会议论文集
    (第316-321页)。ACM。'
- en: '[137] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems (pp. 1097-1105).'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012)。使用深度卷积神经网络的Imagenet分类。见 Advances
    in neural information processing systems (第1097-1105页)。'
- en: '[138] Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013, June). Rectifier nonlinearities
    improve neural network acoustic models. In Proc. icml (Vol. 30, No. 1, p. 3).'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013年6月)。整流器非线性改善神经网络声学模型。见 Proc.
    icml (第30卷，第1期，第3页)。'
- en: '[139] Cheng, Y., Shen, S., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. Agreement-Based
    Joint Training for Bidirectional Attention-Based Neural Machine Translation.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Cheng, Y., Shen, S., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. 基于一致性的双向注意力神经机器翻译的联合训练。'
- en: '[140] Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). Pixel recurrent
    neural networks. arXiv preprint arXiv:1601.06759.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016)。像素递归神经网络。arXiv预印本arXiv:1601.06759。'
- en: '[141] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition (pp. 770-778).'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] He, K., Zhang, X., Ren, S., & Sun, J. (2016)。用于图像识别的深度残差学习。见 IEEE计算机视觉与模式识别会议论文集
    (第770-778页)。'
- en: '[142] Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018).
    Universal transformers. arXiv preprint arXiv:1807.03819.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018)。通用变换器。arXiv预印本arXiv:1807.03819。'
- en: '[143] Xiao, F., Li, J., Zhao, H., Wang, R., & Chen, K. (2019). Lattice-Based
    Transformer Encoder for Neural Machine Translation. arXiv preprint arXiv:1906.01282.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Xiao, F., Li, J., Zhao, H., Wang, R., & Chen, K. (2019)。基于格的变换器编码器用于神经机器翻译。arXiv预印本arXiv:1906.01282。'
- en: '[144] Hao, J., Wang, X., Yang, B., Wang, L., Zhang, J., & Tu, Z. (2019). Modeling
    recurrence for transformer. arXiv preprint arXiv:1904.03092.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Hao, J., Wang, X., Yang, B., Wang, L., Zhang, J., & Tu, Z. (2019)。为变换器建模递归。arXiv预印本arXiv:1904.03092。'
- en: '[145] Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L.
    S. (2019). Learning Deep Transformer Models for Machine Translation. arXiv preprint
    arXiv:1906.01787.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L.
    S. (2019)。为机器翻译学习深度变换器模型。arXiv预印本arXiv:1906.01787。'
- en: '[146] So, D. R., Liang, C., & Le, Q. V. (2019). The evolved transformer. arXiv
    preprint arXiv:1901.11117.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] So, D. R., Liang, C., & Le, Q. V. (2019)。演变的变换器。arXiv预印本arXiv:1901.11117。'
- en: '[147] Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative
    position representations. arXiv preprint arXiv:1803.02155.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Shaw, P., Uszkoreit, J., & Vaswani, A. (2018)。带有相对位置表示的自注意力。arXiv预印本arXiv:1803.02155。'
- en: '[148] Parikh, A. P., Täckström, O., Das, D., & Uszkoreit, J. (2016). A decomposable
    attention model for natural language inference. arXiv preprint arXiv:1606.01933.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Parikh, A. P., Täckström, O., Das, D., & Uszkoreit, J. (2016)。一种可分解的注意力模型用于自然语言推理。arXiv预印本arXiv:1606.01933。'
- en: '[149] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training
    of deep bidirectional transformers for language understanding. arXiv preprint
    arXiv:1810.04805.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018)。Bert：用于语言理解的深度双向变换器的预训练。arXiv预印本arXiv:1810.04805。'
- en: '[150] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,
    & Zettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint
    arXiv:1802.05365.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,
    & Zettlemoyer, L. (2018)。深度上下文化词表示。arXiv预印本arXiv:1802.05365。'
- en: '[151] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving
    language understanding by generative pre-training. URL https://s3-us-west-2\.
    amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding
    paper. pdf.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018)。通过生成预训练提高语言理解。URL
    https://s3-us-west-2\. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language
    understanding paper. pdf。'
- en: '[152] Bapna, A., Chen, M. X., Firat, O., Cao, Y., & Wu, Y. (2018). Training
    deeper neural machine translation models with transparent attention. arXiv preprint
    arXiv:1808.07561.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Bapna, A., Chen, M. X., Firat, O., Cao, Y., & Wu, Y. (2018)。使用透明注意力训练更深的神经机器翻译模型。arXiv预印本arXiv:1808.07561。'
- en: '[153] Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., & Zhang, Z. (2019). Star-transformer.
    arXiv preprint arXiv:1902.09113.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., & Zhang, Z. (2019). 星型变换器。arXiv
    预印本 arXiv:1902.09113。'
- en: '[154] Clinchant, S., Jung, K. W., & Nikoulina, V. (2019). On the use of BERT
    for Neural Machine Translation. arXiv preprint arXiv:1909.12744.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Clinchant, S., Jung, K. W., & Nikoulina, V. (2019). 关于使用 BERT 进行神经机器翻译的研究。arXiv
    预印本 arXiv:1909.12744。'
- en: '[155] Edunov, S., Baevski, A., & Auli, M. (2019). Pre-trained language model
    representations for language generation. arXiv preprint arXiv:1903.09722.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Edunov, S., Baevski, A., & Auli, M. (2019). 预训练语言模型在语言生成中的应用。arXiv 预印本
    arXiv:1903.09722。'
- en: '[156] Luong, M. T. (2017).Neural Machine Translation. Unpublished doctoral
    dissertation, Stanford University, Stanford, CA 94305.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Luong, M. T. (2017). 神经机器翻译。未出版的博士论文，斯坦福大学，加州斯坦福 94305。'
