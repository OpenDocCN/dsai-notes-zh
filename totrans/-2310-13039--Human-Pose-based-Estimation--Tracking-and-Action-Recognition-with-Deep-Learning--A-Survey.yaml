- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:20'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2310.13039] Human Pose-based Estimation, Tracking and Action Recognition with
    Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.13039](https://ar5iv.labs.arxiv.org/html/2310.13039)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \equalcont
  prefs: []
  type: TYPE_NORMAL
- en: These authors contributed equally to this work. \equalcontThese authors contributed
    equally to this work. \equalcontThese authors contributed equally to this work.
    [1]Zhimin Gao
  prefs: []
  type: TYPE_NORMAL
- en: 1]School of Computer and Artificial Intelligence, Zhengzhou University, China
    2]Amazon Prime Video, USA
  prefs: []
  type: TYPE_NORMAL
- en: 'Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lijuan Zhou [ieljzhou@zzu.edu.cn](mailto:ieljzhou@zzu.edu.cn)    Xiang Meng
    [mengxiangzzu@163.com](mailto:mengxiangzzu@163.com)    Zhihuan Liu [liuzhihuanzzu@163.com](mailto:liuzhihuanzzu@163.com)
       Mengqi Wu [mengqiwuzzu@163.com](mailto:mengqiwuzzu@163.com)    [iegaozhimin@zzu.edu.cn](mailto:iegaozhimin@zzu.edu.cn)
       Pichao Wang [pichaowang@gmail.com](mailto:pichaowang@gmail.com) [ [
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Human pose analysis has garnered significant attention within both the research
    community and practical applications, owing to its expanding array of uses, including
    gaming, video surveillance, sports performance analysis, and human-computer interactions,
    among others. The advent of deep learning has significantly improved the accuracy
    of pose capture, making pose-based applications increasingly practical. This paper
    presents a comprehensive survey of pose-based applications utilizing deep learning,
    encompassing pose estimation, pose tracking, and action recognition.Pose estimation
    involves the determination of human joint positions from images or image sequences.
    Pose tracking is an emerging research direction aimed at generating consistent
    human pose trajectories over time. Action recognition, on the other hand, targets
    the identification of action types using pose estimation or tracking data. These
    three tasks are intricately interconnected, with the latter often reliant on the
    former. In this survey, we comprehensively review related works, spanning from
    single-person pose estimation to multi-person pose estimation, from 2D pose estimation
    to 3D pose estimation, from single image to video, from mining temporal context
    gradually to pose tracking, and lastly from tracking to pose-based action recognition.
    As a survey centered on the application of deep learning to pose analysis, we
    explicitly discuss both the strengths and limitations of existing techniques.
    Notably, we emphasize methodologies for integrating these three tasks into a unified
    framework within video sequences. Additionally, we explore the challenges involved
    and outline potential directions for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Pose Estimation, Pose Tracking, Action Recognition, Deep Learning, Survey
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human pose estimation, tracking, and pose-based action recognition represent
    three fundamental research directions within the field of computer vision. These
    areas have a broad spectrum of applications, spanning from video surveillance,
    human-computer interactions, gaming, sports analysis, intelligent driving, and
    the emerging landscape of new retail stores. Articulated human pose estimation
    involves the task of estimating the configuration of the human body in a given
    image or video. Human pose tracking targets to generate consistent pose trajectories
    over time, which is usually used to analyze the motion proprieties of human. Human
    pose-based or skeleton-based action recognition is to recognize the types of actions
    based on the pose estimation or tracking data. Although these three tasks fall
    within the domain of human motion analysis, they are typically treated as distinct
    entities in the existing literature.
  prefs: []
  type: TYPE_NORMAL
- en: Human motion analysis is a long-standing research topic, and there are a vast
    of works and several surveys on this task [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. In these surveys, human detection, tracking, pose estimation
    and motion recognition are usually reviewed together. Several survey papers have
    summarized the research on human pose estimation [[10](#bib.bib10), [11](#bib.bib11)],
    tracking [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)], and action recognition [[17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20)]. With the development of deep learning, the
    three tasks have achieved significant improvements compared to hand-crafted feature
    era [[21](#bib.bib21), [22](#bib.bib22)]. The previous surveys either reviewed
    the whole vision-based human motion domain [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)],
    or have focused on specific tasks [[10](#bib.bib10), [11](#bib.bib11), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)].
    However, there is no such survey paper which simultaneously reviews pose estimation,
    pose tracking, and pose recognition. Inspired by Lagrangian viewpoint of motion
    analysis [[28](#bib.bib28)], pose information and tracking are beneficial for
    action recognition. Therefore, these three tasks are closely related each other.
    It is significantly useful for reviewing the methods linking the three tasks together,
    and providing a deep understanding for the separate solution of each task and
    more exploration for a unified solution of joint tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we will conduct a comprehensive review of previous works using
    deep learning approach on these three tasks individually, and discuss the strengths
    and weaknesses of previous research paper. Furthermore, we elucidate the inherent
    connections that bind these three tasks together, while championing the adoption
    of a deep learning-based framework that seamlessly integrates them. Specifically,
    we will review previous works with deep learning from 2D pose estimation to 3D
    pose estimation from single images to videos, from mining temporal contexts gradually
    to pose tracking, and lastly from tracking to pose-based action recognition. According
    to the number of persons for pose estimation, 2D/3D pose estimation can be divided
    into single-person and multi-person pose estimation. Depending on the input to
    the networks, each category can be further divided into image and video-based
    single-person/multi-person pose estimation. To link the poses across the frames,
    pose tracking can be divided into post-processing and integrated methods for single-person
    pose tracking, top-down and bottom-up approaches for multi-person pose tracking.
    After getting the trajectory of poses in the videos, pose-based action recognition
    could be naturally conducted which can be divided into estimated pose and skeleton-based
    action recognition. The former takes RGB videos as the input and jointly conducts
    pose estimation, tracking, and action recognition. The latter extracts skeleton
    sequences captured by sensors such as motion capture, time-of-flight, and structured
    light cameras for action recognition. For skeleton-based action recognition, four
    categories are identified including Convolutional Neural Networks (CNN), Recurrent
    Neural Networks (RNN), Graph Neural Networks (GCN) and Transformer-based approaches.
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") illustrates the taxonomy
    of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key novelty of this survey is the focus on three closely related tasks
    that use deep learning approach, which has never been done in previous surveys.
    In reviewing the various methods, consideration has been given to the connections
    between the three tasks, hence, this survey tends to discuss the advantages and
    limitations of the reviewed methods from the viewpoint of assembling them to get
    more practical applications. This is the first survey to put them together to
    analysis their inner connections in deep learning era. Besides, this survey distinguishes
    itself from other surveys through the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A thorough and all-encompassing coverage of the most advanced deep learning-based
    methodologies developed since 2014\. This extensive coverage affords readers a
    comprehensive overview of the latest research methodologies and their outcomes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An insightful categorization and analysis of methods on the three tasks, and
    highlights of the pros and cons, promoting potential exploration of better solutions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An extensive review of the most commonly used benchmark datasets for these three
    tasks, and the state-of-the-art results on the benchmark datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An earnest discussion of the challenges of three tasks and potential research
    directions through limitation analysis of available methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Subsequent sections of this survey are organized as follows. Sections  [2](#S2
    "2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") through  [4](#S4 "4 Action Recognition ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey")
    delve into the methods of pose estimation, pose tracking, and action recognition,
    respectively. Commonly used benchmark datasets and the performance comparison
    for three tasks are described in Section [5](#S5 "5 Benchmark datasets ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey").
    Challenges of these three tasks and pointers to future directions are presented
    in Section [6](#S6 "6 Challenges and Future Directions ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey"). The survey provides
    concluding remarks in Section [7](#S7 "7 Conclusion ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dec4d00b003244e8dc72c4b5cff9d710.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The taxonomy of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Pose estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Human representation can be approached through three distinct models: the kinematic
    model, the planar model, and the volumetric model. The kinematic model employs
    a combination of joint positions and limb orientations to faithfully depict the
    human body’s structure. In contrast, the planar model utilizes rectangles to represent
    both body shape and appearance, while the volumetric model leverages mesh data
    to capture the intricacies of the human body’s shape. It’s essential to underscore
    that this paper exclusively focuses on the kinematic model-based human representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pose estimation, pose tracking and action recognition are three intimately
    interrelated tasks. Fig. [2](#S2.F2 "Figure 2 ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") shows
    the relationship among the three tasks. Pose estimation aims to estimate joint
    coordinates from an image or a video. Pose tracking is an extension of pose estimation
    in the context of videos, which associates each estimated pose with its corresponding
    identity over time. It is interesting noting that a recent work [[29](#bib.bib29)]
    tends to estimate poses after tracking volumes of persons, which implies that
    the two-way relationship of pose estimation and tracking. Pose-based action recognition
    aims to give the tracked pose with an identity the corresponding action label.'
  prefs: []
  type: TYPE_NORMAL
- en: For pose estimation, we generally classify the reviewed methods into two categories,
    2D pose estimation and 3D pose estimation. The 2D pose estimation is to estimate
    a 2D pose $(x,y)$ coordinates for each joint from a RGB image or video while 3D
    pose estimation is to estimate a 3D pose $(x,y,z)$ coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20b41563a2ad8bd1c0034b05be741169.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The relationship among the three tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 2D pose estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For 2D pose estimation, two sub-divisions are identified, single-person pose
    estimation and multi-person pose estimation. Depending on the input to the networks,
    single (multi) person pose estimation could be further divided into image-based
    single (multi) person pose estimation and video-based single (multi) person pose
    estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Image-based single-person pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For image-based Single-Person Pose Estimation (SPPE), the task involves providing
    the position and a rough scale of a person or their bounding box as a precursor
    to the estimation process. Early works adopt the pictorial structures framework
    that represents an object by a collection of parts arranged in a deformable configuration,
    and a part in the collection is an appearance template matched in an image. Different
    from early works, the deep learning-based methods target to locate keypoints of
    human parts. Two typical frameworks, namely, direct regression and heatmap-based
    approaches, are available for image-based single-person pose estimation. In the
    direct regression-based approach, keypoints are directly predicted from the image
    features, whereas the heatmap-based approach initially generates heatmaps and
    subsequently infers keypoint locations based on these heatmaps. Fig. [3](#S2.F3
    "Figure 3 ‣ 2.1.1 Image-based single-person pose estimation ‣ 2.1 2D pose estimation
    ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") provides an illustrative overview of the general
    framework for image-based 2D SPPE, showcasing the two predominant approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5477a7a235b611fe722d44f979780b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The framework of two approaches for image-based 2D SPPE.'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Regression-based approach
  prefs: []
  type: TYPE_NORMAL
- en: 'The pioneer work [[30](#bib.bib30)], DeepPose, formulates pose estimation as
    a convolutional neural network(CNN)-based regression task towards body joints.
    A cascade of regressors are adopted to refine the pose estimates, as shown in
    Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.1 Image-based single-person pose estimation ‣
    2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey"). This work could reason
    about pose in a holistic fashion in occlusion situations. Carreira et al. [[31](#bib.bib31)]
    introduced the Iterative Error Feedback approach, wherein prediction errors were
    recursively fed back into the input space, resulting in progressively improved
    estimations. Sun et al. [[32](#bib.bib32)] presented a reparameterized pose representation
    using bones instead of joints. This method defines a compositional loss function
    that captures the long range interactions within the pose by exploiting the joint
    connection structure. In more recent developments,  [[33](#bib.bib33)] introduced
    a novel approach that employed softmax functions to convert heatmaps into coordinates
    in a fully differentiable manner. This innovative technique was coupled with a
    keypoint error distance-based loss function and context-based structures.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3b362baa8c613d36e1af8e7d6e960e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The DeepPose architecture [[30](#bib.bib30)].'
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, researchers [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)] began exploring pose estimation methods based on transformer
    architectures. The attention modules in transformers offered the ability to capture
    long-range dependencies and global evidence crucial for accurate pose estimation.
    For example, TFPose [[34](#bib.bib34)] first introduced Transformer to the pose
    estimation framework in a regression-based manner. PRTR [[35](#bib.bib35)] introduced
    a two-stage, end-to-end regression-based framework that employed cascading Transformers,
    achieving state-of-the-art performance among regression-based methods. Mao et
    al. [[36](#bib.bib36)] framed pose estimation as a sequence prediction task, which
    they addressed with the Poseur model.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s worth noting that these direct regression methods sometimes struggle
    in high-precision scenarios. This limitation may stem from the intricate mapping
    of RGB images to $(x,y)$ locations, adding unnecessary complexity to the learning
    process and hampering generalization. For instance, direct regression may encounter
    challenges when handling multi-modal outputs, where a valid joint appears in two
    distinct spatial locations. The constraint of producing a single output for a
    given regression input can limit the network’s ability to represent small errors,
    potentially leading to over-training.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Heatmap-based approach
  prefs: []
  type: TYPE_NORMAL
- en: Heatmaps have gained substantial attention due to its ability to provide comprehensive
    spatial information, making itself invaluable for training Convolutional Neural
    Networks (CNNs). This has spurred a surge of interest in the development of CNN
    architectures for pose estimation. Jain et al. [[38](#bib.bib38)] pioneered an
    approach where multiple CNNs were trained for independent binary body-part classification,
    with each network dedicated to a specific feature. This strategy effectively constrained
    the network’s outputs to a much smaller class of valid configurations, enhancing
    overall performance. Recognizing the importance of structural domain constraints,
    such as the geometric relationships between body joint locations, Tompson et al. [[39](#bib.bib39)]
    pursued a joint training approach, simultaneously training CNNs and graphical
    models for human pose estimation. Similarly, Chen and Yuille [[40](#bib.bib40)]
    adopt Convnets to learn conditional probabilities for the presence of parts and
    their spatial relationships within image patches. To address the limitations of
    pooling techniques in  [[39](#bib.bib39)] for improving spatial locality precision,
    Tompson et al. [[41](#bib.bib41)] proposed a position refinement model (namely,
    a multi-resolution Convents) that is trained to predict the joint offset location
    within a localized region of the image. The works of  [[39](#bib.bib39)], [[40](#bib.bib40)]
    and [[41](#bib.bib41)] sought to merge the representational flexibility inherent
    in graphical models with the efficiency and statistical power offered by CNNs.
    To avoid using graphical models, Wei et al. [[42](#bib.bib42)] introduced the
    Convolutional Pose Machines to learn long-range spatial relationships without
    explicitly adopting graphical models. Hu and Ramanan [[43](#bib.bib43)] proposed
    an architecture that could be used for multiple stages of predictions, and ties
    weights in the bottom-up and top-down portions of computation as well as across
    iteration. Similarly, Newell et al. [[44](#bib.bib44)] proposed the Stacked Hourglass
    Network (SHN) for single-person pose estimation. The SHN leverages a series of
    successive pooling and upsampling steps to generate a final set of predictions,
    showcasing its efficacy. In addressing challenging scenarios characterized by
    severe part occlusions, Bulat and Tzimiropoulos [[45](#bib.bib45)] presented a
    detection-followed-by-regression CNN cascade. This robust approach adeptly infers
    poses, even in the presence of significant occlusions. Lifshitz et al. [[46](#bib.bib46)]
    introduced a novel voting scheme that harnesses information from the entire image,
    allowing for the aggregation of numerous votes to yield highly accurate keypoint
    detections. Chu et al. [[47](#bib.bib47)] incorporated CNNs into their approach,
    enhancing it with a multi-context attention mechanism for pose estimation. This
    dynamic mechanism autonomously learns and infers contextual representations, directing
    the model’s focus toward regions of interest. Furthermore, Yang et al. [[48](#bib.bib48)]
    devised a Pyramid Residual Module (PRMs) to bolster the scale invariance of CNNs.
    PRMs effectively learn feature pyramids, which prove instrumental in precise pose
    estimation.
  prefs: []
  type: TYPE_NORMAL
- en: With the development of Generative Adversarial Networks (GAN) [[49](#bib.bib49)],
    Chen et al. [[50](#bib.bib50)] designed discriminators to distinguish the real
    poses from the fake ones to incorporate priors about the structure of human bodies.
    Ning et al. [[51](#bib.bib51)] proposed to explore external knowledge to guide
    the network training process using learned projections that impose proper prior.
    Sun et al. [[52](#bib.bib52)] presented a two-stage normalization scheme, human
    body normalization and limb normalization, to make the distribution of the relative
    joint locations compact, resulting in easier learning of convolutional spatial
    models and more accurate pose estimation. Marras et al. [[53](#bib.bib53)] introduced
    a Markov Random Field (MRF)-based spatial model network between the coarse and
    the refinement model that introduces geometric constraints on the relative locations
    of the body joints. To deal with annotating pose problem, Liu and Ferrari [[54](#bib.bib54)]
    presented an active learning framework for pose estimation. Ke et al. [[55](#bib.bib55)]
    proposed a multi-scale structure-aware network for human pose estimation. Peng
    et al. [[56](#bib.bib56)] proposed adversarial data augmentation for jointly optimize
    data augmentation and network training. The main idea is to design an augmentation
    network (generator) that competes against a target network (discriminator) by
    generating ”hard” augmentation operations online. Tang et al. [[57](#bib.bib57)]
    introduced a Deeply Learned Compositional Model for pose estimation by exploiting
    deep neural networks to learn compositions of human body. Nie et al. [[58](#bib.bib58)]
    proposed the parsing induced learner including a parsing encoder and a pose model
    parameter adapter, which estimates dynamic parameters in the pose model through
    joint learning to extract complementary useful features for more accurate pose
    estimation. Nie et al. [[59](#bib.bib59)] proposed to jointly conduct human parsing
    and pose estimation in one framework by incorporating information from their counterparts,
    giving more robust and accurate results. Tang and Wu [[60](#bib.bib60)] proposed
    a data-driven approach to group-related parts based on how much information they
    share, and then a part-based branching network (PBN) is introduced to learn representations
    specific to each part group. To speed up the pose estimation, Zhang et al. [[61](#bib.bib61)]
    presented a Fast Pose Distillation (FPD) model that trains a lightweight pose
    neural network architecture capable of executing rapidly with low computational
    cost, by effectively transferring pose structure knowledge of a robust teacher
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, regression-based methods have advantages in speed but disadvantages
    in accuracy on pose estimation task. Heatmap-based methods can explicitly learn
    spatial information by estimating heatmap likelihood, resulting in high accuracy.
    However, heatmap-based methods suffer seriously a long-standing challenge known
    as the quantization error problem, which is caused by mapping the continuous coordinate
    values into discretized downscaled heatmaps. To address this problem, Li et al [[62](#bib.bib62)]
    proposed a Simple Coordinate Classification (SimCC) method which formulates pose
    estimation as two classification tasks for horizontal and vertical coordinates.
    Despite the improvement in quantization error, the estimation of heatmaps requires
    exceptionally high computational cost, resulting in slow preprocessing operations.
    Therefore, how to take advantage of both heatmap-based and regression-based methods
    remains a challenging problem. Some works [[63](#bib.bib63), [64](#bib.bib64)]
    tend to solve the above problem by transferring the knowledge from heatmap-based
    to regression-based models. However, due to the different output spaces of regression
    models and heatmap models, directly transferring knowledge between heatmaps and
    vectors may result in information loss. To the end, DistilPose [[64](#bib.bib64)]
    (as shown in Fig. [5](#S2.F5 "Figure 5 ‣ 2.1.1 Image-based single-person pose
    estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey")) is proposed to
    transfer heatmap-based knowledge from a teacher model to a regression-based student
    model through token-distilling encoder and simulated heatmaps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ffc8e2370de50b81fc778ae91c7a1e20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The DistilPose framework [[64](#bib.bib64)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Image-based multi-person pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Compared with single-person pose estimation (SPPE), multi-person pose estimation
    (MPPE) is more difficult. First, the number or the position of the person is not
    given, and the pose can occur at any position or scale; second, interactions between
    people induce complex spatial interference, due to contact, occlusion, and limb
    articulations, making association of parts difficult; third, runtime complexity
    tends to grow with the number of people in the image, making realtime performance
    a challenge. MPPE must address both global (human-level) and local (keypoint-level)
    dependencies (as depicted in Fig. [6](#S2.F6 "Figure 6 ‣ 2.1.2 Image-based multi-person
    pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey")), which
    involve different levels of semantic granularity. Mainstream solutions are normally
    two-stage approaches, which divide the problem into two separate subproblems including
    global human detection and local keypoint regression. Typically, two primary frameworks
    have been proposed to tackle these subproblems, known as the top-down and bottom-up
    approaches. Inspired by the success of end-to-end object detection, another viable
    solution is the one-stage approach. This approach aims to develop a fully end-to-end
    trainable method capable of unifying the two disassembled subproblems.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7ab30e23ef8c250b552f845ba7e013a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Perception of multi-person pose estimation task [[65](#bib.bib65)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f42638a720496ebffaeddeb8e723d55c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The framework of two approaches for image-based 2D MPPE. Part of
    the figure is from [[66](#bib.bib66)].'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Top-down approach
  prefs: []
  type: TYPE_NORMAL
- en: 'Top-down approaches in multi-person pose estimation begin by detecting all
    individuals within a given image, as shown in Fig. [7](#S2.F7 "Figure 7 ‣ 2.1.2
    Image-based multi-person pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation
    ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey"), and subsequently employ single-person pose estimation techniques within
    each detected bounding box.'
  prefs: []
  type: TYPE_NORMAL
- en: A group of methods [[67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75),
    [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)] aim to designing and improving
    modules within pose estimation networks. Papandreou et al. [[67](#bib.bib67)]
    adopt Faster RCNN [[79](#bib.bib79)] for person detection and keypoints estimation
    within the bounding box. They introduce an aggregation procedure to obtain highly
    localized keypoint predictions, along with a keypoint-based Non-Maximum-Suppression
    (NMS) to prevent duplicate pose detection. Sun et al. [[71](#bib.bib71)] proposed
    a novel High-Resolution net(HRNet) to learn such representation. To address systematic
    errors in standard data transformation and encoding-decoding structures that degrade
    top-down pipeline performance, Huang et al. [[73](#bib.bib73)] proposed solutions
    to correct common biased data processing in human pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Human detectors may fail in the first step of top-down pipeline due to occlusion
    affected by the overlapping of limbs. Another group of works [[80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)] aim to
    address this issue. Fang et al. [[81](#bib.bib81)] proposed a novel Regional Multi-person
    Pose Estimation (RMPE) to facilitate pose estimation even when inaccurate human
    bounding boxes exist. Chen et al. [[82](#bib.bib82)] designed a Cascaded Pyramid
    Network (CPN) that contains GlobalNet and RefineNet for localizing simple and
    hard keypoints with occlusion respectively. Su et al. [[83](#bib.bib83)] proposed
    two novel modules to perform the enhancement of the information for the multi-person
    pose estimation under occluded scenes, namely, Channel Shuffle Module (CSM) and
    Spatial, Channel-wise Attention Residual Bottleneck (SCARB), where CSM promoting
    cross-channel information communication among the pyramid feature maps and SCARB
    highlighting the information of feature maps both in the spatial and channel-wise
    context. An occluded pose estimation and correction module [[84](#bib.bib84)]
    is proposed to solve the occlusion problem in crowd pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Much like single-person pose estimation, multi-person pose estimation has also
    undergone rapid advancements, transitioning from CNNs to vision transformer networks.
    Some recent works tend to treat transformer as a better decoder. TransPose [[85](#bib.bib85)]
    processes the features extracted by CNNs to model the global relationship. Zhou
    et al. [[86](#bib.bib86)] proposed a Bottom-Up Conditioned Top-Down pose estimation
    (BUCTD) method which modifies TransPose to accept conditions as side-information
    generated by CTD. Different from other top-down methods, BUCTD applies a bottom-up
    model as a person detector. TokenPose [[87](#bib.bib87)] proposes a token-based
    representation to estimate the locations of occluded keypoints and model the relationship
    among different keypoints. HRFormer [[88](#bib.bib88)] proposes to fuse multi-resolution
    features by a transformer module. The above works either require CNNs for feature
    extraction or careful designs of transformer structures. In contrast, a simple
    yet effective baseline model, ViTPose [[89](#bib.bib89)], is proposed based on
    the plain vision transformers.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Bottom-up approach
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the top-down approach, the bottom-up approach initially detects
    all individual body parts or keypoints and subsequently associates them with the
    corresponding subjects using part association strategies. The seminal work of
    Pishchulin et al. [[90](#bib.bib90)] proposed a bottom-up approach that jointly
    labels part detection candidates and associates them to individual people. However,
    solving the integer linear programming problem over a fully connected graph is
    an NP-hard problem and the average processing time is on the order of hours. In
    the work by Insafutdinov et al.  [[91](#bib.bib91)], a more robust part detector
    and innovative image-conditioned pairwise terms were proposed to enhance runtime
    efficiency. Nevertheless, this work encountered challenges in precisely regressing
    the pairwise representations and a separate logistic regression is required. Iqbal
    and Gall [[80](#bib.bib80)] considered multi-person pose estimation as a joint-to-person
    association problem. They construct a fully connected graph from a set of detected
    joint candidates in an image and resolve the joint-to-person association and outlier
    detection using integer linear programming. OpenPose [[92](#bib.bib92), [93](#bib.bib93)]
    proposes the first bottom-up representation of association scores via Part Affinity
    Fields (PAFs) which are a set of 2D vector fields that encode the location and
    orientation of limbs over the image domain. Kreiss et al. [[94](#bib.bib94)] proposed
    to use a Part Intensity Field (PIF) for body parts localization and a PAF for
    body part association with each other to form full human poses. To handle missed
    small-scale persons, Cheng et al. [[95](#bib.bib95)] proposed multi-scale training
    and dual anatomical canters to enhance the network. The above methods mainly apply
    heatmap prediction based on overall L2 loss to locate keypoints. However, minimizing
    L2 loss cannot always locate all keypoints since each heatmap often includes multiple
    body joints. To solve this problem, Qu et al. [[96](#bib.bib96)] proposed to optimize
    heatmap prediction based on minimizing the distance between the characteristic
    functions of the predicted and ground-truth heatmaps.
  prefs: []
  type: TYPE_NORMAL
- en: Different from the above two-stage bottom-up approach, some works focus on joint
    detection and grouping, which belong to single-stage bottom-up approach. Newell
    et al. [[97](#bib.bib97)] simultaneously produced score maps and pixel-wise embedding
    to group the candidate keypoints among different people to get final multi-person
    pose estimation. Kocabas et al. [[98](#bib.bib98)] designed a MultiPoseNet that
    jointly handle person detection, person segmentation and pose estimation problems,
    by the implementation of Pose Residual Network (PRN) which receives keypoint and
    person detections, and produces accurate poses by assigning keypoints to person
    instances. To deal with the crowded scene, Li et al. [[99](#bib.bib99)] built
    a new benchmark called CrowdPose and proposed two components, namely, joint-candidate
    single-person pose estimation and global maximum joints association, for crowded
    pose estimation. Jin et al. [[100](#bib.bib100)] proposed a new differentiable
    hierarchical graph grouping method to learn human part grouping. Cheng et al. [[101](#bib.bib101)]
    extended the HRNet and proposed a higher resolution network (HigherHRNet) by deconvolving
    the high-resolution hetamaps generated by HRNet to solve the variation challenge.
    Besides the above bottom-up methods, some methods directly regress a set of pose
    candidates from image pixels and the keypoints in each candidate might be from
    the same person. A post-processing step is required to generate the final poses
    which are more spatially accurate. For instance, single-stage multi-person Pose
    Machine (SPM) method [[102](#bib.bib102)] applies a hierarchical structured 2D/3D
    pose representation to assist the long-range regression. The keypoints are predicted
    based on person-agnostic heatmaps so that grouping post-processing is required
    to assemble keypoints to the full-body pose. Disentangled Keypoint Regression
    (DEKR) [[103](#bib.bib103)] regresses pose candidates by learning representations
    that focus on keypoint regions. The pose candidates were scored and ranked to
    generate the final poses based on keypoints and center heatmap estimation loss.
    PolarPose [[104](#bib.bib104)] aims to simplify 2D regression to a classification
    task by performing it in polar coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: (3) One-stage approach
  prefs: []
  type: TYPE_NORMAL
- en: The one-stage approach aims to learn an end-to-end network for MPPE without
    person detection and grouping post-processing. Tian et al. [[105](#bib.bib105)]
    first proposed a one-stage method based on DirectPose to directly predict instance
    aware keypoints for all persons from an image. To boost both accuracy and speed,
    Mao et al. [[106](#bib.bib106)] later presented a Fully Convolutional Pose (FCPose)
    estimation framework to build dynamic filters in compact keypoint heads. Meanwhile,
    Shi et al. [[107](#bib.bib107)] designed InsPose, which adaptively adjusts the
    network parameters for each instance. To reduce the effect of false positive poses
    in regression loss, the Single-stage Multi-person Pose Regression (SMPR) network [[108](#bib.bib108)]
    was presented by adapting three positive pose identification strategies for initial
    and final pose regression, and the Non-Maximum Suppression (NMS) step. These methods
    could avoid the need for heuristic grouping in bottom-up methods or bounding-box
    detection and region of interest (RoI) cropping in top-down ones. However, they
    still require hand-crafted operations, like NMS, to remove duplicates in the postprocessing
    stage. To further remove NMS, a multi-person Pose Estimation framework with TRansformers
    (PETR) [[109](#bib.bib109)] regards pose estimation as a set prediction, which
    is the first fully end-to-end framework without any postprocessing. The above
    one-stage methods adopts a pose decoder with randomly initialized pose queries,
    making keypoint matching across persons ambiguous and training convergence slow.
    To this end, Yang et al. [[65](#bib.bib65)] proposed an Explicit box Detection
    process for pose estimation (ED-pose) by realizing each box detection using a
    decoder and cascading them to form an end-to-end framework, making the model fast
    in convergence, precise and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Although the above end-to-end methods have achieved promising performance, they
    rely on complex decoders. For instance, ED-pose includes a human detection decoder
    and a human-to-keypoint detection decoder to detect human and keypoint boxes explicitly.PETR
    includes a pose decoder and a joint decoder. In contrast, Group Pose [[110](#bib.bib110)]
    only uses a simple transformer decoder for pursing efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, top-down approaches directly leverage existing techniques for single-person
    pose estimation, but suffer from early commitment: if the person detector fails
    as it is prone to do when people are in close proximity, there is no recourse
    to recovery. Furthermore, the runtime of these top-down approaches is proportional
    to the number of people. For each detection, a single-person pose estimator is
    run, thus, the more people there are, the greater the computational cost. In contrast,
    bottom-up approaches are attractive due to their robustness to early commitment
    and the potential to decouple runtime complexity from the number of people in
    the image. Yet, bottom-up approaches do not directly leverage global contextual
    cues from other body parts and individuals. One-stage methods eliminate the intermediate
    operations like grouping, ROI, bounding-box detection, NMS and bypass the major
    shortcomings of both top-down and bottom-up methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Video-based single-person pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Video-based pose estimation aims to estimate single or multiple poses in each
    video frame. Compared with image-based pose estimation, it is more challenging
    due to high variation in human pose and foreground appearance such as clothing
    and self-occlusion. For video-based pose estimation, human tracking is not considered
    in the video. Similar to image-based SPPE, direct regression and heatmap-based
    approaches are also available for video-based SPPE. However, differently, video-based
    pose estimation has the advantage of temporal information, which can enhance the
    accuracy of pose estimation but can also introduce additional computational overhead
    due to temporal redundancy. Therefore, achieving a balance between accuracy and
    efficiency is paramount for video-based pose estimation. Based on handling the
    efficiency, video-based SPPE approaches are categorized into the frame-by-frame
    approach and sample frames-based ones. Fig. [8](#S2.F8 "Figure 8 ‣ 2.1.3 Video-based
    single-person pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey")
    illustrates the general framework of two approaches for video-based SPPE.'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Frame-by-frame approach
  prefs: []
  type: TYPE_NORMAL
- en: 'The frame-by-frame approach, illustrated in Fig. [8](#S2.F8 "Figure 8 ‣ 2.1.3
    Video-based single-person pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation
    ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey"), focuses on estimating poses individually for each frame in the video
    sequence. With the success of image-based pose estimation, this category of methods
    mainly apply image-based pose estimation methods on each video frame by incorporating
    temporal information to keep geometric consistency across frames. The temporal
    information is normally captured by fusion from concatenated consecutive frames,
    applying 3D temporal convolution, using dense optical flow and pose propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37b6da96412bd3902bee616144addac8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The framework of two approaches for video-based 2D SPPE.'
  prefs: []
  type: TYPE_NORMAL
- en: In the early stages of this approach, Pfister et al. [[111](#bib.bib111)] proposed
    to use deep ConvNets for estimating human pose in videos. They designed a regression
    layer to predict the location of upper-body joints while considering temporal
    information through the direct processing of concatenated consecutive frames along
    the channel axis. Grinciunaite et al. [[112](#bib.bib112)] extended 2D convolution
    into 3D convolution and temporal information can be efficiently represented in
    the third dimension of 3D convolutional for video-based human pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Some works tend to use optical flow to produce smooth movement. Pfister et al. [[113](#bib.bib113)]
    used dense optical flow to predict joint positions for all neighboring frames
    and design spatial fusion layers to learn dependencies between the human parts
    locations. Song et al. [[114](#bib.bib114)] also utilized optical flow warping
    to capture high temporal consistency and propose spatio-temporal message passing
    layer to incorporate domain-specific knowledge into deep networks. Jain et al. [[115](#bib.bib115)]
    use Local Contrast Normalization and Local Motion Normalization to process the
    RGB image and optical-flow features respectively and then combine them to feed
    into Part-Detector network. These methods have high complexity due to dense flowing
    computation, making them not applicable in real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, some works [[116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118),
    [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122),
    [123](#bib.bib123), [124](#bib.bib124)] apply pose propagation which transfer
    features from previous frames to the current frame in an online fashion. For example,
    Charles et al. [[117](#bib.bib117)] proposed a personalized ConvNet to estimate
    human pose including four stages: initial annotation, spatial matching, temporal
    propagation, and self evaluation. In the initial annotation stage, high-precision
    pose estimation is obtained by using flowing Convnets. Then Image patches from
    the new frames without annotations are matched to image patches of body joints
    in frames with annotations by spatial matching process. Dense optical flow is
    used for temporal propagation. Finally, the quality of the spatial-temporal propagated
    annotations is automatically evaluated to optimize the model. Luo et al. [[118](#bib.bib118)]
    proposed Long Short-Term Memory (LSTM) pose machines by combining Convolutional
    Pose Machine (CPM) [[42](#bib.bib42)] and LSTM network learning the temporal dependency
    among video frames to effectively capture the geometric relationships of joints
    in space and time. Nie et al. [[119](#bib.bib119)] designed a Dynamic Kernel Distillation
    (DKD) model. The DKD model introduces a pose kernel distillator and transmits
    pose knowledge in time. Xu et al. [[122](#bib.bib122)] proposed a novel neural
    architecture search to select the most effective temporal feature fusion for optimizing
    the accuracy and speed across video frames. Dang et al. [[123](#bib.bib123)] proposed
    a Relation-based Pose Semantics Transfer Network (RPSTN) by designing a joint
    relation-guided pose semantic propagator to learn the temporal semantic continuity
    of poses. Despite various strategies are applied to reduce computation cost, this
    category of methods still leads to sub-optimal efficiency improvement due to the
    estimation frame by frame.'
  prefs: []
  type: TYPE_NORMAL
- en: (2) Sample frames-based approach
  prefs: []
  type: TYPE_NORMAL
- en: 'This category of approach aims to recover all poses based on the estimated
    poses from selected frames. As shown in Fig. [8](#S2.F8 "Figure 8 ‣ 2.1.3 Video-based
    single-person pose estimation ‣ 2.1 2D pose estimation ‣ 2 Pose estimation ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey"),
    the general workflow includes sample pose estimation and all poses recovering.
    One line of works generates sample poses by selecting keyframes and estimating
    the poses of keyframes. For example, Zhang et al [[125](#bib.bib125)] introduced
    a Key-Frame Proposal Network (K-FPN) to select informative frames and a human
    pose interpolation module to generate all poses from the poses in keyframes based
    on human pose dynamics. Pose dynamic-based dictionary formulation may become challenging
    when the pose sequence to be interpolated becomes complex. Therefore, to effectively
    exploit the dynamic information, REinforced MOtion Transformation nEtwork (REMOTE) [[126](#bib.bib126)]
    includes a motion transformer to conduct cross frame reconstruction. Although
    the computational efficiency of the above works is improved due to keyframes,
    they still require to take cost on keyframe selection, making it hard to further
    reduce the complexity. To solve this problem, Zeng et al. [[127](#bib.bib127)]
    proposed a novel Sample-Denoise-Recover pipeline (namely DeciWatch) to uniformly
    sample less than 10% of video frames for estimation. The estimatied poses based
    on sample frames are denoised with a Transformer architecture and the rest poses
    are also recovered by another Transformer network. DeciWatch can be used in both
    2D/3D pose estimation from videos and it can maintain or even improve the pose
    estimation accuracy as the previous methods with small cost on computation. Although
    uniform sampling reduces the cost of selecting keyframes, a refinement module
    is added to clean noisy poses. In contrast, MixSynthFormer [[128](#bib.bib128)]
    deletes the refinement module by combining a transformer encoder with an MLP-based
    mixed synthetic attention, thus pursing highly efficient 2D/3D video-based pose
    estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, frame-by-frame approaches could benefit from image-based pose estimation
    but suffer from the computation complexity. Sample frame-based approaches offer
    a solution to improve efficiency but raise questions about how to obtain sample
    frames and recover poses. The paper employs uniform sampling; however, considering
    the significant variations in joint movements under different actions, an adaptive
    sampling strategy might be more suitable for further enhancing efficiency. Additionally,
    the design of dynamic recovery methods should be explored to handle non-uniform
    sampling effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4 Video-based multi-person pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the video-based SPPE just introduced, it is natural to extend them to
    handle multiple individuals. Following the taxonomy of video-based SPPE, most
    video-based MPPE approaches fall into frame-by-frame category. They can be achieved
    by employing image-based MPPE frame by frame. Therefore, the approaches of video-based
    MPPE can be categorized into Top-down and Bottom-up approaches.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Top-down approach
  prefs: []
  type: TYPE_NORMAL
- en: 'Top-down approaches mainly estimate poses by first detecting all persons for
    all frames and then conducting image-based single-person pose estimation frame
    by frame. Xiao et al. [[69](#bib.bib69)] proposed a simple baseline based on ResNet
    to estimate poses in each frame and the estimated poses were then tracked based
    on optical flow. Xiu et al. [[129](#bib.bib129)] estimated multiple poses for
    each frame based on RMPE method which can be replaced by other top-down methods
    for image-based MPPE. With the estimated poses in each frame, a Pose Flow Builder
    (PF-Builder) is proposed for building the association of cross-frame poses by
    maximizing overall confidence along the temporal sequence (as shown in Fig. [9](#S2.F9
    "Figure 9 ‣ 2.1.4 Video-based multi-person pose estimation ‣ 2.1 2D pose estimation
    ‣ 2 Pose estimation ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey")), and a Pose Flow Non-Maximum Suppression (PF-NMS)
    is designed to robustly reduce redundant pose flows and re-link temporal disjoint
    ones. Girdhar et al. [[130](#bib.bib130)] estimated poses for each frame based
    on Mask R-CNN and then generated keypoint predictions linked over the video by
    lightweight tracking. Wang et al. [[131](#bib.bib131)] proposed a clip tracking
    network to perform pose estimation and tracking simultaneously. To construct the
    clip tracking network, the 3D HRNet is proposed for estimating poses which incorporating
    temporal dimension into the original HRNet. AlphaPose [[132](#bib.bib132)] is
    also proposed for joint pose estimation and tracking. In particular, all persons
    for each frame are firstly detected using off-the-shelf object detectors like
    YoloV3 or EfficientDet. To solve the quantization error, the symmetric integral
    keypoints regression method is then proposed to localize keypoints in different
    scales accurately. Pose-guided alignment module is applied on the predicted human
    re-id feature to obtain pose-aligned human re-id features after removing redundant
    poses based on NMS. At last, a pose-aware identity embedding is presented to produce
    tracking identity. Estimating poses frame by frame ignores motion dynamics which
    is fundamentally important for accurate pose estimation from videos. A recent
    method [[133](#bib.bib133)] presents Temporal Difference Learning based on Mutual
    Information (TDMI) for pose estimation. A multi-stage temporal difference encoder
    was designed for learning informative motion representations and a representation
    disentanglement module was introduced to distill task-relevant motion features
    to enhance frame representation for pose estimation. The temporal difference features
    can be applied in pose tracking by measuring the similarity of motions for data
    association. Gai et al. [[134](#bib.bib134)] proposed a Sptiotemporal Learning
    Transformer for video-based Pose estimation (SLT-Pose) to capture the shallow
    feature information. With the introduction of diffusion models in computer vision
    tasks (eg. image segmentation [[135](#bib.bib135)], object detection [[136](#bib.bib136)]),
    DiffPose [[137](#bib.bib137)] is the first diffusion model and formulates video-based
    pose estimation as a conditional heatmap generation problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/267096815811dc42f1024c0ab497fe5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The Pose Flow framework [[129](#bib.bib129)].'
  prefs: []
  type: TYPE_NORMAL
- en: (2) Bottom-up approach
  prefs: []
  type: TYPE_NORMAL
- en: Bottom-up approaches estimate poses by applying body part detection and grouping
    frame by frame. For example, one of the commonly used image-based MPPE methods,
    OpenPose[[93](#bib.bib93)], can be also applied for MPPE from video by directly
    estimating poses frame by frame. Jin et al. [[138](#bib.bib138)] proposed a Pose-Guided
    Grouping (PGG) network for joint pose estimation and tracking. PGG consists of
    two components including SpatialNet and TemporalNet. SpatialNet tackles multi-person
    pose estimation by body part detection and part-level spatial grouping for each
    frame. TemporalNet extends SpatialNet to deal with online human-level temporal
    grouping.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, 2D HPE has been significantly improved with the development of deep
    learning techniques. For the image-based SPPE, heatmap-based approaches generally
    outperform regression-based ones in accuracy but may be of challenge in the quantization
    error problem. When extending SPPE to MPPE, both top-down and bottom-up approaches
    have their advantages and disadvantages. Moreover, both approaches have a challenge
    of reliable detection of individual persons under significant occlusion. Person
    detector in top-down approaches may fail in identifying the boundaries of overlapped
    human bodies. Body part association for occluded scenes may fail in bottom-up
    approaches. One-stage approaches bypass both the shortcomings of top-down and
    bottom-up ones, yet they are still less frequently used. With the advancement
    of image-based pose estimation, it is natural to extend it to videos by directly
    applying off-the-shelf image-based pose estimation methods frame by frame or incorporating
    a temporal network. Sample frames-based methods are preferred for the pose estimation
    from videos since they can largely improve efficiency without looking at all frames,
    while they have been used less in the video-based MPPE. Considering the benefits
    of one-stage approaches for image-based MPPE, more effort is required to explore
    one-stage approaches for video-based ones.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 3D pose estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally speaking, recovering 3D pose is considered more difficult than 2D
    pose estimation, due to the larger 3D pose space and more ambiguities. An algorithm
    has to be invariant to some factors, including background scenes, lighting, clothing
    shape and texture, skin color, and image imperfections, among others.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Image-based single-person pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imaged-based single-person 3D human pose estimation (HPE) can be classified
    into skeleton-based and mesh-based approaches. The former one estimates 3D human
    joints as the final output and the latter one is required to reconstruct 3D human
    mesh representation. Since this paper focuses only on the kinematic model-based
    human representation, we only review skeleton-based approaches which can be further
    categorized into one-step pose estimation and two-steps pose estimation (recover
    3D pose from 2D pose). Fig. [10](#S2.F10 "Figure 10 ‣ 2.2.1 Image-based single-person
    pose estimation ‣ 2.2 3D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") shows
    the general framework of the two approaches for image-based 3D SPPE.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f46a04c720931320ef3cbd412de4d54a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The framework of two approaches for image-based 3D SPPE.'
  prefs: []
  type: TYPE_NORMAL
- en: (1) One-stage approach
  prefs: []
  type: TYPE_NORMAL
- en: 'This category of approaches directly infer 3D pose from images without estimating
    2D pose representation. Li and Chan [[139](#bib.bib139)] first proposed to estimate
    3D poses from monocular images using ConvNets. The framework consists of two types
    of tasks: joint point regression and joint point detection. Both tasks take bounding
    box images containing human subjects as input. The regression task aims to estimate
    the positions of joint points relative to the root joint position, while each
    detection task classifies whether one specific joint is present in the local window
    or not.'
  prefs: []
  type: TYPE_NORMAL
- en: The multi-task learning framework is the first to show that deep neural networks
    can be applied to 3D human pose estimation from single images. However, one drawback
    of these regression-based methods is their limitation in predicting only one pose
    for a given image This may cause difficulties in images where the pose is ambiguous
    due to partial self-occlusion, and hence several poses might be valid. In contrast,
    Li et al. [[140](#bib.bib140)] proposed a unified framework for maximum-margin
    structured learning with a deep neural network for 3D human pose estimation, where
    the unified framework can jointly learn the image and pose feature representations
    and the score function. Tekin et al. [[141](#bib.bib141)] introduced an architecture
    relying on an overcomplete auto-encoder to learn a high-dimensional latent pose
    representation for joint dependencies. Zhou et al. [[142](#bib.bib142)] proposed
    a novel method which directly embeds a kinematic object model into the deep neutral
    network learning, where the kinematic function is defined on the appropriately
    parameterized object motion variables. Mehta et al. [[143](#bib.bib143)] explored
    transfer learning to leverage the highly relevant middle and high-level features
    from 2D pose datasets in conjunction with the existing annotated 3D pose datasets.
    Similarly, Zhou et al. [[144](#bib.bib144)] introduced a Weakly-supervised Transfer
    Learning (WTL) method that employs mixed 2D and 3D labels in a unified deep neural
    network, which is end-to-end and fully exploits the correlation between the 2D
    pose and depth estimation sub-tasks. Since regressing directly from image space,
    one-step-based methods often require a high computation cost.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Two-stage approach
  prefs: []
  type: TYPE_NORMAL
- en: 'This category of approaches infer 3D pose from the intermediately estimated
    2D pose. They are often conducted in two steps: 1) estimating 2D pose based on
    image-based single-person 2D pose estimation methods. 2) Lifting the 2D pose to
    3D pose through a simple regressor. For instance, Martinez et al. [[145](#bib.bib145)]
    proposed a simple baseline based on a fully connected residual network to regress
    3D poses from 2D poses. This baseline method achieves good results at that time,
    however, it could fail due to reconstruction ambiguity of over-reliance on 2D
    pose detector. To overcome this problem, several techniques are applied such as
    replacing 2D poses with heatmaps for estimating 3D poses [[146](#bib.bib146),
    [147](#bib.bib147)], regressing 3D poses from 2D poses and depth information [[148](#bib.bib148),
    [149](#bib.bib149)], selecting best 3D poses from 3D pose hypotheses using ranking
    networks [[150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152)].'
  prefs: []
  type: TYPE_NORMAL
- en: With the introduction of Graph convolutional networks(GCN)-based representation
    for human joints, some methods [[153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156), [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159),
    [160](#bib.bib160), [161](#bib.bib161)] apply GCN for lifting 2D to 3D poses.
    To overcome the limitations of shared weights in GCN, a locally connected network
    (LCN) [[153](#bib.bib153)] was proposed which leverages a fully connected network
    and GCN to encode the relationship among joints. Similarly, Zhao et al. [[154](#bib.bib154)]
    proposed a semantic-GCN to learn channel-wise weights for edges. A Pose2Mesh [[155](#bib.bib155)]
    based on GCN was proposed to refine the intermediate 3D pose from its PoseNet.
    Xu and Takano [[159](#bib.bib159)] proposed a Graph Stacked Hourglass (GraphSH)
    networks which consists of repeated encoder-decoder for representing three different
    scales of human skeletons. To overcome the loss of joint interactions in current
    GCN methods, Zhai et al. [[162](#bib.bib162)] proposed Hop-wise GraphFormer with
    Intragroup Joint Refinement (HopFIR) for lifting 3D poses.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the recent success in the nature language field, there is a growing
    interest in exploring the use of Transformer architecture for vision tasks. Lin
    et al. [[163](#bib.bib163)] first applied Transformer for 3D pose estimation.
    A multi-layer Transformer with progressive dimensionality reduction was proposed
    to regress the 3D coordinates of joints. Here, the standard transformer ignores
    the interaction of adjacency nodes. To overcome this problem, Zhao et al. [[164](#bib.bib164)]
    proposed a graph-oriented Transformer which enlarges the receptive field through
    self-attention and models graph structure by GCN to improve the performance on
    3D pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: For in-the-wild data, it is difficult to obtain accurate 3D pose annotations.
    To deal with the lack of 3D pose annotation problem, some weakly supervised, self-supervised,
    or unsupervised methods [[144](#bib.bib144), [165](#bib.bib165), [166](#bib.bib166),
    [167](#bib.bib167), [168](#bib.bib168), [169](#bib.bib169), [170](#bib.bib170),
    [171](#bib.bib171), [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174)]
    were proposed for estimating 3D poses from in-the-wild images without 3D pose
    annotations. A weakly supervised transfer learning method [[144](#bib.bib144)]
    was proposed to transfer the knowledge from 3D annotations of indoor images to
    in-the-wild images. 3D bone length constraint-induced loss was applied in the
    weakly supervised learning. Habibie et al. [[166](#bib.bib166)] applied a projection
    loss to refine 3D pose without annotation. A lifting network [[167](#bib.bib167)]
    was proposed to recover 3D poses in a self-supervised mode by introducing a geometrical
    consistency loss based on the closure and invariance lifting property. The previous
    self-supervised methods have largely relied on weak supervisions like consistency
    loss to guide the learning, which inevitably leads to inferior results in real-world
    scenarios with unseen poses. Comparatively, Gong et al. [[173](#bib.bib173)] propose
    a PoseTriplet method that allows explicit generating 2D-3D pose pairs for augmenting
    supervision, through a self-enhancing dual-loop learning framework. Benefiting
    from the reliable 2D pose detection, two-step-based approaches generally outperform
    one-step-based ones.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Image-based multi-person pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similar to 2D multi-person pose estimation, 3D multi-person pose estimation
    for images can be also divided into: top-down approaches, bottom-up approaches
    and one-stage approaches. Top-down and bottom-up approaches involve two stages
    for pose estimation. Fig. [11](#S2.F11 "Figure 11 ‣ 2.2.2 Image-based multi-person
    pose estimation ‣ 2.2 3D pose estimation ‣ 2 Pose estimation ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") illustrates
    the general framework of the two approaches for image-based 3D MPPE.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a487777ed5dde5698d8f814f2812ec37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The framework of two approaches for image-based 3D MPPE. Part of
    the figure is from [[175](#bib.bib175)].'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Top-down approach
  prefs: []
  type: TYPE_NORMAL
- en: Top-down approaches first detect each person based on human detection networks
    and then generate 3D poses based on single-person estimation approaches. Localization
    Classification-Regression Network (LCR-Net) [[176](#bib.bib176), [177](#bib.bib177)]
    proposes a pose proposal network to generate human bounding boxes and a series
    of human pose hypotheses. The pose hypotheses were refined based on the cropped
    ROI features for generating 3D poses. Moon et al. [[178](#bib.bib178)] proposed
    a camera distance-aware method for estimating the camera-centric human poses which
    consists of human detection, absolute 3D human root localization, and root-relative
    3D single-person pose estimation modules. Here, the root-relative poses ignore
    the absolute locations of each pose. Comparatively, Lin and Lee [[179](#bib.bib179)]
    proposed the Human Depth Estimation Network (HDNet) for absolute root joint localization
    in the camera coordinate space. HDNet could estimate the human depth with considerably
    high performance based on the prior knowledge of the typical size of the human
    pose and body joints. The top-down methods mostly estimate poses based on each
    bounding box, which results in the doubt that the top-down models are not able
    to understand multi-person relationships and handle complex scenes. To address
    this limitation, Wang et al. [[180](#bib.bib180)] proposed a hierarchical multi-person
    ordinal relations (HMOR) to leverage the relationship among multiple persons for
    pose estimation. HMOR could encode the interaction information as ordinal relations,
    supervising the networks to output 3D poses in the correct order. Cha et al. [[181](#bib.bib181)]
    designed a transformer-based relation-aware refinement to capture the intra- and
    inter-person relationships. Although the top-down approaches achieve high accuracy,
    they suffer high computation costs as person number increases. Meanwhile, these
    methods may neglect global information (inter-person relationship) in the scene
    since poses are individually estimated.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Bottom-up approach
  prefs: []
  type: TYPE_NORMAL
- en: Bottom-up approaches first produce all body joint locations and then associate
    joints to each person according to root depth and part relative depth. Zanfir
    et al. [[182](#bib.bib182)] proposed MubyNet to group human joints according to
    body part scores based on integrated 2D and 3D information. One group of bottom-up
    approaches aim to group body joints belonging to each person. Learning on Compressed
    Output (LoCO) method [[183](#bib.bib183)] first applied volumetric heatmaps to
    produce joint locations with an encoder-decoder network for feature compression,
    and a distance-based heuristic was then applied to retrieve 3D pose for each person.
    A distance-based heuristic was applied for linking joints. The previous methods
    are trained in a fully-supervised fashion which requires 3D pose annotations,
    while Kundu et al. [[184](#bib.bib184)] proposed a unsupervised method for 3D
    pose estimation. Without paired 2D images and 3D pose annotations, a frozen network
    was applied to exploit the shared latent space between two different modalities
    based on cross-modal alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Another group of bottom-up approaches focus on occlusion. Mehta et al.[[185](#bib.bib185)]
    combined the joint location maps and the occlusion-robust pose-maps to infer the
    3D poses. The joint location redundancy is applied to infer occluded joints. XNect [[186](#bib.bib186)]
    encodes the immediate local context of joints in the kinematic tree to address
    occlusion. Zhen et al. [[187](#bib.bib187)] developed 3D part affinity field for
    depth-aware part association by reasoning about inter-person occlusion, and utilized
    a refined network to refine the 3D pose given predicted 2D and 3D joint coordinates.
    All of these methods handle occlusion from the perspective of single-person and
    require initial grouping joints into individuals, which results in error-prone
    estimates in multi-person scenarios. Liu et al. [[188](#bib.bib188)] proposed
    an occluded keypoints reasoning module based on a deeply supervised encoder distillation
    network to reason about the invisible information from the visible ones. Chen
    et al. [[189](#bib.bib189)] presented Articulation-aware Knowledge Exploration
    (AKE) for keypoints associated with a progressive scheme in the occlusion situation.
    In comparison to top-down approaches, bottom-up approaches offer the advantage
    of not requiring repeated single-person pose estimation and they enjoy linear
    computation. However, the bottom-up approaches require a second association stage
    for joint grouping. Furthermore, since all persons are processed at the same scale,
    these methods are inevitably sensitive to human scale variations, which limits
    their applicability in wild videos.
  prefs: []
  type: TYPE_NORMAL
- en: (3) One-stage approach
  prefs: []
  type: TYPE_NORMAL
- en: One-stage approaches treat pose estimation as parallel human center localizing
    and center-to-joint regression problem. Instead of separating joints localizing
    and grouping in the two-stage approaches, these approaches predict each of the
    joint offsets from the detected center points, which is usually set as the root
    joint of human. Since the joint offsets are directly correlated to estimated center
    points, this strategy avoids the manually designed grouping post-processing and
    is end-to-end trainable. Zhou et al.[[190](#bib.bib190)] modeled an object as
    a single point and regressed joints from image features at the human center. Wei
    et al. [[191](#bib.bib191)] proposed to regress joints from point-set anchors
    which serve as prior of basic human poses. Wang et al. [[175](#bib.bib175)] reconstructed
    joints from 2.5D human centers and 3D center-relative joint offsets. Jin et al. [[192](#bib.bib192)]
    proposed a Decoupled Regression Model (DRM) by solving 2D pose regression and
    depth regression. Recently, Qiu et al. [[193](#bib.bib193)] estimated 3D poses
    directly by fine-tuning a Weakly-Supervised Pre-training (WSP) network on 3D pose
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Video-based single-person pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of estimating 3D poses from images, videos can provide temporal information
    to improve the accuracy and robustness of pose estimation. Similar to image-based
    3D HPE, video-based 3D HPE can also be categorized into one-stage and two-stage
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: (1) One-stage approach
  prefs: []
  type: TYPE_NORMAL
- en: 'There are few research belong to this category of approaches. Tekin et al. [[194](#bib.bib194)]
    proposed a regression function to directly predict the 3D pose in a given frame
    of a sequence from a spatio-temporal volume centered around it. This volume comprises
    bounding boxes surrounding the person in consecutive frames coming before and
    after the central one. Mehta et al. [[195](#bib.bib195)] proposed the VNect, which
    is capable of obtaining a temporally consistent, full 3D skeletal pose of a human
    from a monocular RGB camera by Convents regression and kinematic skeleton fitting.
    The VNect could regress 2D and 3D joint locations simultaneously. Dabral et al. [[196](#bib.bib196)]
    proposed two structure-aware loss functions: illegal angle loss and left-right
    symmetry loss to directly predict 3D body pose from the video sequence. The illegal
    angle loss is to distinguish the internal and external angle of a 3D joint and
    the symmetry loss is defined as the difference in lengths of left/right bone pairs.
    Qiu [[197](#bib.bib197)] proposed an end-to-end framework based on Instance-guided
    Video Transformer (IVT) to predict 3D single and multiple poses directly from
    videos. An unsupervised feature extraction method [[198](#bib.bib198)] based on
    Constrastive Self-Supervised (CSS) learning was presented to capture rich temporal
    features for pose estimation. Time-variant and time-invariant latent features
    are learned using CSS by reconstructing the input video frames and time-variant
    features are then applied to predicting 3D poses.'
  prefs: []
  type: TYPE_NORMAL
- en: (2) Two-stage approach
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to two-step 3D poses estimated from images, two-step 3D HPE involves
    two stages: estimating 2D poses and lifting 3D poses from 2D poses. However, the
    difference is that a sequence of 2D poses is applied for lifting a sequence of
    3D poses in video-based 3D HPE. Based on different lifting methods, this category
    of approaches can be summarized into Seq2frame and Seq2seq-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Seq2frame-based methods pay attention to predicting the central frame of the
    input video to produce a robust prediction and less sensitivity to noise. Pavllo
    et al. [[199](#bib.bib199)] presented a Temporal Convolutional Network (TCN) on
    2D keypoint trajectories with semi-supervised training method. In the network,
    1D convolutions are used to capture temporal information with fewer parameters.
    In semi-supervised training, the 3D pose estimator is used as the encoder and
    the decoder maps the predicted pose back to the 2D space. Some following works
    improved the performance of TCN by solving the occlusion problem [[200](#bib.bib200)],
    utilizing the attention [[201](#bib.bib201)], or decomposing the pose estimation
    task into bone length and bone direction prediction [[202](#bib.bib202)]. Except
    TCN, Cai et al. [[203](#bib.bib203)] employs GCN for modeling temporal information
    in which learning multi-scale features for 3D human body estimation from a short
    sequence of 2D joint detection. Without convolution architecture involved, Zheng
    et al. [[204](#bib.bib204)] proposed a PoseFormer based on a spatial-temporal
    transformer for estimating the 3D pose of the center frame. To overcome the huge
    computational cost of PoseFormer when increasing the frame number for better performance,
    PoseFormerV2 [[205](#bib.bib205)] applies a frequency-domain representation of
    2D pose sequences for lifting 3D poses. Similarly, Li et al. [[206](#bib.bib206)]
    proposed a stridden transformer encoder to reconstruct 3D pose of the center frame
    by reducing the sequence redundancy and computation cost. Li et al. [[207](#bib.bib207)]
    further designed a Multi-Hypothesis transFormer (MHFormer) to exploit spatial-temporal
    representations of multiple pose hypotheses. Based on MHFormer, MHFormer++ [[208](#bib.bib208)]
    is proposed to further model local information of joints by incorporating graph
    Transformer encoder and effectively aggregate multi-hypothesis features by adding
    a fusion block. With the similar idea of pose hypothesis [[207](#bib.bib207),
    [208](#bib.bib208)], DiffPose [[209](#bib.bib209)] and Diffusion-based 3D Pose
    (D3DP) [[210](#bib.bib210)] aim to apply a diffusion model to predict multiple
    adjustable hypotheses for a given 2D pose due to its ability of high-field samples.
    The aforementioned Transformer-based methods [[204](#bib.bib204), [205](#bib.bib205),
    [206](#bib.bib206), [208](#bib.bib208)] mainly model spatial and temporal information
    sequentially by different stages of networks, thus resulting in insufficient learning
    of motion patterns. Therefore, Tang et al. [[211](#bib.bib211)] proposed Spatio-Temporal
    Criss-cross Transformer (STCFormer) by stacking multiple STC attention blocks
    to model spatial and temporal information in parallel with a two-pathway network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seq2seq-based methods reconstruct all frames of input sequence at once for
    improving coherence and efficiency of 3D pose estimation. The earlier methods
    apply recurrent neural network (RNN) or long short-term memory (LSTM) as the Seq2Seq
    network. Lin et al. [[212](#bib.bib212)] designed a Recurrent 3D Pose Sequence
    Machine(RPSM) for estimating 3D human poses from a sequence of images. The RPSM
    consists of three modules: a 2D pose module; a 3D pose recurrent module and a
    feature adaption module for transforming the pose representations from 2D to 3D
    domain. Hossain et al. [[213](#bib.bib213)] presented a sequence-to-sequence network
    by using LSTM units and residual connections on the decoder side. The sequence
    of 2D joint locations is as input to the sequence-to-sequence network to predict
    a temporally coherent sequence of 3D poses. Lee et al. [[214](#bib.bib214)] proposed
    propagating long short-term memory networks (p-LSTMs) to estimates depth information
    from 2D joint location through learning the intrinsic joint interdependency. Katircioglu
    et al. [[215](#bib.bib215)] proposed a deep learning regression architecture to
    learn a high-dimensional latent pose representation by using an autoencoder and
    a Long Short-Term Memory network is proposed to enforce temporal consistency on
    3D pose predictions. Raymond et al. [[216](#bib.bib216)] proposed Chirality Nets.
    In Chirality Nets, fully connected layers, convolutional layers, batch-normalization,
    and LSTM/GRU cells can be chiral. According to this kind of symmetry, it naturally
    estimates 3D pose by exploiting the left/right mirroring of the human body. Later,
    there are some methods [[217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219),
    [220](#bib.bib220), [221](#bib.bib221), [222](#bib.bib222)] apply GCN or transformer
    for Seq2seq learning. Wang et al. [[217](#bib.bib217)] exploited a GCN-based method
    combining a corresponding loss to model motion in both short temporal intervals
    and long temporal ranges. Zhang et al. [[219](#bib.bib219)] proposed a mixed spatio-temporal
    encoder(MixSTE) which includes a temporal transformer to model the temporal motion
    of each joint and a spatial transformer to learn inter-joint spatial correlations.
    The MixSTE directly reconstructs the entire frames to improve the coherence between
    input and output sequences. Chen et al. [[220](#bib.bib220)] proposed High-order
    Directed Transformer (HDFormer) to reconstruct 3D pose sequences from 2D pose
    sequences by incorporating self-attention and high-order attention to model joint-joint,
    bone-joint, and hyperbone-joint interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Video-based multi-person pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different from the image-based multi-person pose estimation, video-based multi-person
    pose estimation often suffers from fast motion, large variability in appearance
    and clothing, and person-to-person occlusion. A successful approach in this context
    must be capable of accurately identifying the number of individuals present in
    each video frame, as well as determining the precise joint locations for each
    person and effectively associating these joints over time.
  prefs: []
  type: TYPE_NORMAL
- en: With the improvement of video-based single-person 3D HPE, one method of video-based
    multi-based 3D HPE is two-step-based method that first detects each person based
    on human detection networks and then generates 3D poses based on video-based single-person
    3D HPE methods. Cheng et al. [[223](#bib.bib223)] proposed a novel framework for
    integrating graph convolutional network (GCN) and time convolutional network (TCN)
    to estimate multi-person 3D pose. In particular, bounding boxes are firstly detected
    for representing humans and 2D poses are then estimated based on the bounding
    box. The 3D poses for each frame are estimated by feeding 2D poses into joint-
    and bone-GCNs. The 3D pose sequence is finally fed into temporal TCN to enforce
    the temporal and human-dynamic constraints. This category of methods applies top-down
    technique to estimate 3D poses, which rely on detecting each person independently.
    Therefore, it is likely to suffer from inter-person occlusion and close interactions.
    To overcome this problem, the same author[[224](#bib.bib224)] later proposed an
    Multi-person Pose Estimation Integration (MPEI) network by adding a bottom-up
    branch for capturing global-awareness poses on the same top-down branch as the
    paper [[223](#bib.bib223)]. The final 3D poses are estimated based on matching
    the estimated 3D poses from both bottom-up and top-down branches. An interaction-aware
    discriminator was applied to enforce the natural interaction of two persons. To
    overcome the occlusion problem, Park et al. [[225](#bib.bib225)] presented POTR-3D
    to lift 3D pose sequences by directly processing 2D pose sequences rather than
    a single frame at a time, and devise a data augmentation strategy to generate
    occlusion-aware data with devise views. Capturing long-range temporal information
    normally requires computing on more frames, which results in high computational
    cost. To cope with this problem, a recent work, TEMporal POse estimation method
    (TEMPO) [[29](#bib.bib29)], learns a spatio-temporal representation by a recurrent
    architecture to speed up the inference time while preserving estimation accuracy.
    To be specific, persons are firstly detected and represented by feature volumes.
    A spatio-temporal pose representation is then learned by recurrently combining
    features from current and previous timesteps. It is finally decoded into an estimation
    of the current pose and poses at future timestaps. Note that the poses are estimated
    based on the tracking results of feature volumes, which hints that pose estimation
    performance can be improved by pose tracking. Moreover, TEMPO also provides a
    solution for action prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In the above two-step-based methods, the result of the latter step depends on
    the ones of the former step. Therefore, one-step pose estimation is proposed recently
    based on end-to-end network. IVT [[197](#bib.bib197)] can be also used to predict
    multiple poses directly from videos. The instance-guided tokens include deep features
    and instance 2D offsets (from body center to keypoints) which are sent into a
    video transformer to capture the contextual depth information between multi-person
    joints in spatial and temporal dimensions. A cross-scale instance-guided attention
    mechanism is introduced to handle the variational scales among multiple persons.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, 3D HPE has made significant advancements recent years. Due to the
    progress in 2D HPE, a large number of 3D image/video-based single-person HPE methods
    apply 2D to 3D lifting strategy. When extending single-person to multi-person
    in 3D image/video-based HPE, two step (top-down and bottom-up) and one-step methods
    are always applied. Although top-down methods could achieve promising results
    by the state-of-the-art person detection and single-person methods, they suffer
    from high computation cost as person number increases and the missing of inter-person
    relationship measurement. The bottom-up methods could enjoy linear computation,
    however, they are sensitive to human scale variations. Therefore, one-step based
    methods are preferable for 3D image/video-based multi-person HPE. When extending
    image-based 3D single/multi-person HPE to video-based ones, temporal information
    is measured for learning joint association across frames. Similar to images-methods,
    two-step-based methods are commonly used due to the success of 2D to 3D lifting
    strategy. Among them, Seq2seq-based methods are preferable, as they contribute
    to enhancing the coherence and efficiency of 3D pose estimation. To capture the
    temporal information, TCN (Temporal Convolutional Networks), RNN (Recurrent Neural
    Network)-related architectures, and Transformers are commonly used networks.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Pose tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pose tracking aims to estimate human poses from videos and link the poses across
    frames to obtain a number of trackers. It is related to video-based pose estimation,
    but it requires to capturing the association of estimated poses across frames
    which is different from video-based pose estimation. With the pose estimation
    methods reviewed in Section 2, the main task of pose tracking becomes pose linking.
    The fundamental problem of pose linking is to measure the similarity between pairs
    of poses in adjacent frames. The pose similarity is normally measured based on
    temporal information (eg. optical flow, temporal smoothness priors), and appearance
    information from images. Following the taxonomy of two kinds of estimated poses,
    we divide the pose tracking methods into two categories: 2D pose tracking and
    3D pose tracking.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 2D pose tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to the number of persons for tracking, 2D pose tracking can be divided
    into single-person and multi-person pose tracking. Fewer methods solve the problem
    of single-person pose tracking since they actually aim to update the estimated
    poses for obtaining more accurate poses with temporal consistency. Therefore,
    pose tracking mainly solves the tracking problem of multiple persons. Nevertheless,
    we will give a review of two categories of methods including single-person and
    multi-person pose tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Single-person pose tracking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the core idea of updating the estimated poses by tracking, this category
    of approaches can be usually divided into two types, post-processing and integration
    approaches. The post-processing approaches estimate the pose of each frame individually,
    and then correlation analysis is conducted on the estimated poses across different
    frames to reduce inconsistencies and generate a smooth result. The integrated
    approaches unite pose estimation and visual tracking within a single framework.
    Visual tracking ensures the temporal consistency of the poses, while pose estimation
    enhances the accuracy of the tracked body parts. By combining the strengths of
    both visual tracking and pose estimation, the integrated approaches achieve improved
    results in pose tracking. Fig. [12](#S3.F12 "Figure 12 ‣ 3.1.1 Single-person pose
    tracking ‣ 3.1 2D pose tracking ‣ 3 Pose tracking ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey") illustrates the
    general framework of the two approaches for single person pose tracking.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9a2b98220d40b0d30f62f617a0f52b61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The framework of two approaches for 2D Single person pose tracking.'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Post-processing approach
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et al. [[226](#bib.bib226)] proposed to track human body pose by adopting
    the max-margin Markov model. They proposed a spatio-temporal model composed of
    two sub-models for spatial parsing and temporal parsing respectively. Spatial
    parsing is used to estimate candidate human poses in a frame, while temporal parsing
    determines the most probable pose part locations over time. An inference iteration
    of sub-models is conducted to obtain the final result. Samanta et al.  [[227](#bib.bib227)]
    proposed a data-driven method for human body pose tracking in video data. They
    initially estimated the pose in the first frame of the video, and employed local
    object tracking to maintain spatial relationships between body parts across different
    frames.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Integrated approach
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et al. [[228](#bib.bib228)] proposed a two-step iterative method that combines
    pose estimation and visual tracking into a unified framework to compensate for
    each other, the pose estimation improves the accuracy of visual tracking, and
    the result of visual tracking facilitates the pose estimation. The two steps are
    performed iteratively to get the final pose. In addition, they designed a reinitialization
    mechanism to prevent pose tracking failures. Previous methods required future
    frames or entire sequences to refine the current pose and were difficult to track
    online. Ma et al. [[229](#bib.bib229)] solved the problem of online tracking human
    pose of joint motion in dynamic environments. They proposed a coupled-layer framework
    composed of a global layer for pose tracking and a local layer for pose estimation.
    The core idea is to decompose the global pose candidate in any particular frame
    into several local part candidates and then recombine selected local parts to
    obtain an accurate pose for the frame.
  prefs: []
  type: TYPE_NORMAL
- en: Post-processing approaches first obtain a set of plausible pose assumptions
    from the video and then stitch together compatible detections over time to form
    pose tracking. However, due to the multiplicative cost of using global information,
    models in this category can usually only include local spatio-temporal trajectories
    (evidence). These local spatio-temporal trajectories may be ambiguous, thus leading
    to the disadvantage of objective models. Furthermore, post-processing methods
    are difficult to track online, but integrated approaches allow for a more robust
    and accurate representation of the poses over time, ensuring that the tracked
    body retrains its appropriate configuration throughout the tracking process.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Multi-person pose tracking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike single-person pose tracking, multi-person pose tracking involves measuring
    human interactions, which can introduce challenges to the tracking process. The
    number of the tracking people is unknown, and the human interaction may cause
    the occlusion and overlap. Similar to multi-person pose estimation, existing methods
    can be divided into two categories, top-down and bottom-up approaches.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Top-down approach
  prefs: []
  type: TYPE_NORMAL
- en: Top-down approaches [[131](#bib.bib131), [132](#bib.bib132)] start by detecting
    the overall location and bounding box of the human body in frames and then estimates
    the keypoints of each person. Finally, the estimated human poses are associated
    according to similarity between poses in different frames. Girdhar et al. [[130](#bib.bib130)]
    proposed a two-stage method for estimating and tracking human keypoints in complex
    multi-person videos. The method utilizes Mask R-CNN to perform frame-level pose
    estimation which detects person tubes and estimates keypoints in predicted tubes,
    then performs a person-level tracking module by using lightweight optimization
    to connect estimated keypoints over time. However, this method does not consider
    motion and pose information, which causes difficulty in tracking the occasional
    truncated human. To address the issue, Xiu et al. [[129](#bib.bib129)] employed
    pose flow as a unit and proposed a new pose flow generator which consists of Pose
    Flow Builder and Pose Flow NMS. They initially estimated multi-person poses by
    employing an improved RMPE, and then maximizing overall confidence to construct
    pose flows. Finally, pose flows were purified by applying Plow Flow NMS to obtain
    reasonable multi-pose trajectories. To ease the complexity of method, Xiao et
    al. [[69](#bib.bib69)] proposed a simple but effective method for pose estimation
    and tracking. They adopted the pose propagation and similarity measurement based
    on optical flow to improve the greedy matching method for pose tracking. Zhang
    et al. [[230](#bib.bib230)] solved the articulated multi-person pose estimation
    and real-time velocity tracking. An end-to-end multi-task network (MTN) was designed
    for simultaneously performing human detection, pose estimation, and person re-identification
    (Re-ID) tasks. Given the detection box, keypoints and Re-ID feature provided by
    MTN, an occlusion-aware strategy is applied for pose tracking. Ning et al. [[231](#bib.bib231)]
    proposed a top-down approach that combines single-person pose tracking (SPT) and
    visual object tracking (VOT) into a unified online functional entity that can
    be easily implemented with a replaceable single person pose estimator. They processed
    each human candidate separately and associated the lost tracked candidate to the
    targets from the previous frames through pose matching. The human pose matching
    can be achieved by applying the Siamese Graph Convolution Network as the Re-ID
    module. Umer et al. [[232](#bib.bib232)] proposed a method that relies on the
    correspondence relationship of keypoints to associate the figures in the video.
    It is trained on large image data sets to use self-monitoring for body pose estimation.
    In combination with the top-down human pose estimation framework, keypoint correspondence
    is used to recover lost pose detection based on the temporal context and associate
    detected and recovered poses for pose tracking.
  prefs: []
  type: TYPE_NORMAL
- en: The methods discussed in this section typically begin by detecting the human
    body boundary, which can make them susceptible to challenges like occlusion and
    truncation. Moreover, most methods first estimate poses in each frame and then
    implement data association and refinement. This strategy essentially relies heavily
    on non-existent visual evidence in the case of occlusion, so detection is inevitably
    easy to miss. To this end, Yang et al. [[233](#bib.bib233)] derived dynamic predictions
    through GNN that explicitly takes into account spatio-temporal and visual information.
    It leverages historical pose tracklets as input and predicts corresponding poses
    in the following frames for each tracklet. The predicted poses will then be aggregated
    with the detected poses, so as to recover occluded joints that may have been missed
    by the estimator, significantly improving the robustness of the method.
  prefs: []
  type: TYPE_NORMAL
- en: The methods mentioned above primarily emphasize pose-based similarities for
    matching, which usually struggle to re-identify tracks that have been occluded
    for extended periods or significant pose deformations. In light of this, Doering
    et al. [[234](#bib.bib234)] proposed a novel gated attention approach which utilizes
    a duplicate-aware association, and automatically adapts the impact of pose-based
    similarities and appearance-based similarities according to the attention probabilities
    associated with each similarity metric.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Bottom-up approach
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, bottom-up approaches first detect keypoints of the human body and
    then group the keypoints into individuals. The grouped keypoints are then connected
    and associated across frames to generate the complete pose. Iqbal et al. [[235](#bib.bib235)]
    proposed a novel method which jointly models multi-person pose estimation and
    tracking in a single formula. They represented the detected body joints in the
    video by a spatio-temporal graph which can be divided into sub-graphs corresponding
    to the possible trajectories of each human body pose by solving an integer linear
    program. Raaj et al. [[236](#bib.bib236)] proposed Spatio-Temporal Affinity Fields
    (STAF) across a video sequence for online pose tracking. The connections across
    keypoints in each frame are represented by Part Affinity Fields (PAFs) and connections
    between keypoints across frames are represented by Temporal Affinity Fields. Jin
    et al. [[138](#bib.bib138)] viewed pose tracking as a hierarchical detection and
    grouping problem. They proposed a unified framework consisting of SpatialNet and
    TemporalNet. SpatialNet implements single-frame body part detection and part-level
    data association, and TemporalNet groups human instances in continuous frames
    into trajectories. The grouping process is modeled by a differentiable Pose-Guided
    Grouping (PGG) module to make the entire part detection and grouping pipeline
    fully end-to-end trainable.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom-up approach relates joints spatially and temporally without detecting
    bounding boxes. Therefore, the computational cost of the methods is almost unaffected
    by the change in the number of human candidates. However, they require significant
    computational resources and often suffers from the ambiguous keypoints assignment
    without the global pose view. The top-down approach enhances single-frame pose
    estimation by incorporating temporal context information to correlate estimated
    poses across different frames. It simplifies the complex task and improves the
    keypoints assignment accuracy, although it may increase calculation cost in case
    of a large number of human candidates. In summary, the top-down approach outperforms
    the bottom-up approach both in accuracy and tracking speed, so most of the state-of-the-art
    methods follow the top-down approach.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 3D pose tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the advancement of 3D pose estimation, pose tracking can be naturally
    extended into 3D space. Given that current methods primarily focus on multi-person
    scenarios, we categorize them into two groups without specifying single or multi-person
    tracking: multi-stage and one-stage approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Multi-stage approach
  prefs: []
  type: TYPE_NORMAL
- en: The multi-stage approaches generally track poses involving several steps such
    as 2D/3D pose estimation, lifting 2D to 3D poses and 3D pose linking. These tasks
    are served as independent sub-tasks. For example, Bridgeman et al. [[237](#bib.bib237)]
    performed independent 2D pose detection per frame and associated 2D pose detection
    between different camera views through a fast greedy algorithm. Then the associated
    poses are used to generate and track 3D pose. Zanfir et al. [[238](#bib.bib238)]
    first conducted a single person feedforward-feedback model to compute 2D and 3D
    pose, and then performed joint multiple person optimization under constraints
    to reconstruct and track multiple person 3D pose. Metha et al. [[186](#bib.bib186)]
    estimated 2D and 3D pose features and employed a fully-connected neural network
    to decode features into complete 3D poses, followed by a space-time skeletal model
    fitting.
  prefs: []
  type: TYPE_NORMAL
- en: The above works firstly estimate poses and then link poses across frames in
    which the concept of tracking is to associate joints of the same person together
    over time, using joints localized independently in each frame. By contrast, Sun
    et al. [[239](#bib.bib239)] improved joint localization based on the information
    from other frames. They proposed to first learn the spatio-temporal joint relationships
    and then formulated pose tracking as a simple linear optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: (2) One-stage approach
  prefs: []
  type: TYPE_NORMAL
- en: 'One-stage approach [[240](#bib.bib240), [241](#bib.bib241), [29](#bib.bib29),
    [242](#bib.bib242)] aims to train a single end-to-end framework for jointly estimating
    and linking 3D poses, which can propagate the errors of the sub-tasks in the multi-stage
    approaches back to the input image pixels of videos. For instance, Reddy et al. [[240](#bib.bib240)]
    introduced Tessetrack to jointly infer about 3D pose reconstructions and associations
    in space and time in a single end-to-end learnable framework. Tessetrack consists
    of three key components: person detection, pose tracking and pose estimation.
    With the detected persons, a spatial-temporal person-specific representation is
    learned for measuring similarity to link poses by solving an assignment problem
    based on bipartite graph matching. All matched representations are then merged
    into a single representation which is deconvolved into a 3D pose and taken as
    the estimated pose. To handle the occlusions, VoxelTrack [[241](#bib.bib241)]
    introduces an occlusion-aware multi-view feature fusion strategy for linking poses.
    Specifically, it jointly estimates and tracks 3D poses from a 3D voxel-based representation
    constructed from multi-view images. Poses are linked over time by bipartite graph
    matching based on fused representation from different views without occlusion.
    PHALP [[243](#bib.bib243)] accumulates 3D representations over time for better
    tracking. It relies on a backbone for estimating 3D representations for each human
    detection, aggregating representations over time and forecasting future states,
    and eventually associating tracklets with detections using predicted representations
    in a probabilistic framework. Snipper [[242](#bib.bib242)] conducts a deformable
    attention mechanism to aggregate spatiotemporal information for multi-person 3D
    pose estimation, tracking, and motion forecasting simultaneously in a single shot.
    Similar to Snipper, TEMPO [[29](#bib.bib29)] performs a recurrent architecture
    to fuse both spatial and temporal information into a single representation, which
    enabling pose estimation, tracking, and forecasting from multi-view information
    without sacrificing efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Although both approaches have achieved good performance on 3D multi-person pose
    tracking, for the first approach, solving each sub-problem independently leads
    to performance degradation. 1) 2D pose estimation easily suffers from noise, especially
    in the presence of occlusion. 2) The accuracy of 3D estimation depends on the
    2D estimates and associations across all views. 3) Occlusion-induced unreliable
    appearance features impact the accuracy of 3D pose tracking. As a result, the
    second approach has gained prominence in recent years in 3D multi-person pose
    tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Action Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/020cecdfac65686e9173117896dc3027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Two categories of approaches for action recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7342f51f96cadecfcb8ec0ae80cd6961.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Four approaches for skeleton-based action recognition. (1) RNN example [[244](#bib.bib244)].
    (2) CNN example [[245](#bib.bib245)]. (3) GCN example [[246](#bib.bib246)]. (4)
    Transformer example [[247](#bib.bib247)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action recognition aims to identify the class labels of human actions in the
    input images or videos. For the connection with pose estimation and tracking,
    this paper only reviews the action recognition methods based on poses. Pose-based
    action recognition can be categorized into two approaches: estimated pose-based
    and skeleton-based. Estimated pose-based action recognition approaches apply RGB
    videos as the input and classify actions using poses estimated from RGB videos.
    On the other hand, skeleton-based action recognition methods utilize skeletons
    as their input which can be obtained through various sensors, including motion
    capture devices, time-of-flight cameras, and structured light cameras. Fig. [13](#S4.F13
    "Figure 13 ‣ 4 Action Recognition ‣ Human Pose-based Estimation, Tracking and
    Action Recognition with Deep Learning: A Survey") illustrate the prevailing frameworks
    of these two categories approaches of pose-based action recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Estimated pose-based action recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pose features have been shown in performing much better than low/mid features
    and acting as discriminative cues for action recognition [[248](#bib.bib248)].
    With the success of pose estimation, some methods follow a two-stage strategy
    which first applies existing pose estimation methods to generate poses from videos
    and then conduct action recognition using pose features. Cheron et al. [[249](#bib.bib249)]
    proposed P-CNN to extract appearance and flow features conditioned on estimated
    human poses for action recognition. Mohammadreza et al. [[250](#bib.bib250)] designed
    a body part segmentation network to generate poses and then applied it to a multi-stream
    3D-CNN to integrate poses, optical flow and RGB visual information for action
    recognition. After generating joint heatmaps by pose estimator, Choutas et al.
     [[251](#bib.bib251)] proposed a Pose moTion (PoTion) representation by temporally
    aggregating the heatmaps for action recognition. To avoid relying on the inaccurate
    poses from pose estimation maps, Liu et al. [[252](#bib.bib252)] aggregated pose
    estimation maps to form poses and heatmaps, and then evolved them for action recognition.
    Moon et al. [[253](#bib.bib253)] proposed an algorithm for a pose-driven approach
    to integrate appearance and pre-estimated pose information for action recognition.
    Shah et al. [[254](#bib.bib254)] designed a Joint-Motion Reasoning Network (JMRN)
    for better capturing inter-joint dependencies of poses generated followed by running
    a pose detector on each video frame. This line of methods considers pose estimation
    and action recognition as two separate tasks so that action recognition performance
    may be affected by inaccurate pose estimation. Duan et al. [[255](#bib.bib255)]
    proposed PoseConv3D to form 3D heatmap volume by estimating 2D poses by existing
    pose estimator and stacking 2D heatmaps along the temporal dimension, and to classify
    actions by 3D CNN on top of the volume. Sato et al. [[256](#bib.bib256)] presented
    a user prompt-guided zero-shot learning method based on target domain-independent
    joint features and the joints are pre-extracted by the existing multi-person pose
    estimation technique. Rajasegaran et al. [[28](#bib.bib28)] proposed a Lagrangian
    Action Recognition with Tracking (LART) method to apply the tracking results for
    predicting actions. Pose and appearance features are firstly obtained by the PHALP
    tracking algorithm [[243](#bib.bib243)], and then fused as the input of a transformer
    network to predict actions. Hachiuma et al. [[257](#bib.bib257)] introduced a
    unified framework based on structured keypoint pooling for enhancing the adaptability
    and scalability of skeleton-based action recognition. Human keypoints and object
    contour points are initially obtained through multi-person pose estimation and
    object detection. A structured keypoint pooling is then applied to aggregate keypoint
    features to overcome skeleton detection and tracking errors. Addtionally, non-human
    object keypoints are severed as additional input for eliminating the variety restrictions
    of targeted actions. Finally, A pooling-switch trick is proposed for weakly supervised
    spatio-temporal action localization to achieve action recognition for each person
    in each frame.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of methods jointly solves pose estimation and action recognition
    tasks. Luvizon et al. [[258](#bib.bib258)] proposed a multi-task CNN for joint
    pose estimation from still images and action recognition from video sequences
    based on appearance and pose features. Due to the different output formats of
    the pose estimation and the action recognition tasks, Foo et al. [[259](#bib.bib259)]
    designed a Unified Pose Sequence (UPS) multi-task model, which constructs text-based
    action labels and coordinate-based poses into a heterogeneous output format, for
    simultaneously processing the two tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Skeleton-based Action Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Skeleton data is one form of 3D data commonly used for action recognition.
    It consists of a sequence of skeletons, representing a schematic model of the
    locations of trunk, head, and limbs of the human body. Compared with another two
    commonly used data including RGB and depth, skeleton data is robust to illumination
    change and invariant to camera location and subject appearance. With the development
    of deep learning techniques, skeleton-based action recognition has transitioned
    from hand-crafted features to deep learning-based features. This survey mainly
    reviews the recent methods based on different deep learning networks which can
    be categorized into CNN-based, RNN-based, GCN-based, and Transformer-based methods,
    as shown in Fig. [14](#S4.F14 "Figure 14 ‣ 4 Action Recognition ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 CNN-based approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNN), widely employed in the realm of computer
    vision, possess a natural advantage in image feature extraction due to their exceptional
    local perception and weight-sharing capabilities. Due to the success of CNN in
    image processing, CNN can better capture spatial information in skeleton sequences.
    CNN-based methods for skeleton-based action recognition can be categorized into
    2D and 3D CNN-based approaches, depending on the type of neural network utilized.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the 2D CNN-based methods [[260](#bib.bib260), [261](#bib.bib261), [262](#bib.bib262),
    [263](#bib.bib263), [264](#bib.bib264), [265](#bib.bib265), [245](#bib.bib245),
    [266](#bib.bib266)] first convert the skeleton sequence into a pseudo-image, in
    which the spatial-temporal information of the skeleton sequence is embedded in
    the colors and textures. Du et al. [[260](#bib.bib260)] mapped the Cartesian coordinates
    of the joints to RGB coordinates and then quantized the skeleton sequences into
    an image for feature extraction and action recognition. To reduce the inter-articular
    occlusion due to perspective transformations, some works  [[261](#bib.bib261),
    [262](#bib.bib262)] proposed to encode the spatial-temporal information of skeleton
    sequences into three orthogonal color texture images. The pair-wise distances
    between joints on single or multiple skeleton sequences are represented by Joint
    Distance Map (JDM) [[263](#bib.bib263)] which is encoded as a color change in
    the texture image. To explore better spatial feature representations, Ding et
    al. [[267](#bib.bib267)] encoded the distance, direction and angle of the joints
    as spatial features into the texture color images. Ke et al. [[268](#bib.bib268)]
    proposed to represent segments of skeleton sequences by images and classified
    actions using a multi-task learning network based on CNN. Similarly, Liang et
    al. [[269](#bib.bib269)] applied a multi-tasking learning based on three-stream
    CNN to encode skeletal fragment features, position and motion information.
  prefs: []
  type: TYPE_NORMAL
- en: When compressing skeleton sequences into images by 2D CNN, it is unavoidable
    to lose some temporal information. By contrast, 3D CNN-based methods [[270](#bib.bib270),
    [271](#bib.bib271)] are more excellent at learning spatio-temporal features. Hernandez
    et al. [[271](#bib.bib271)] encoded skeleton sequences as stacked Euclidean Distance
    Matrices (EDM) computed over joints and then performed convolution along time
    dimension for learning spatiao-temporal dynamics of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 RNN-based approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RNN-related networks are often used for processing time-series data to effectively
    capture the temporal information within skeleton sequences. Except for temporal
    information, spatial information is another important cue for action recognition
    which may be ignored by RNN-related networks. Some methods focus on solving this
    problem by spatial division of the human body. For exmaple, Du et al. [[272](#bib.bib272),
    [273](#bib.bib273)] proposed a hierarchical RNN for processing skeleton sequences
    of five body parts for action recognition. Shahroudy et al. [[274](#bib.bib274)]
    proposed a Partially-aware LSTM (P-LSTM) for separately modeling skeleton sequences
    of body parts and classified actions based on the concatenation of memory cells.
  prefs: []
  type: TYPE_NORMAL
- en: To better focus on the key spatial information in the skeleton data, some methods
    tend to incorporate attention mechanism. Song et al. [[275](#bib.bib275)] proposed
    a spatiotemporal attention model using LSTM which includes a spatial attention
    module to adaptively select key joints in each frame, and a temporal attention
    module to select keyframes in skeleton sequences. Similarly, Liu et al. [[276](#bib.bib276)]
    proposed a cyclic attention mechanism to iteratively enhance the performance of
    attention for focusing on key joints. The subsequent improvement work by Song
    et al. [[277](#bib.bib277)] used spatio-temporal regularization to encourage the
    exploration of relationships among all nodes rather than overemphasizing certain
    nodes and avoided an unbounded increase in temporal attention. Zhang et al. [[278](#bib.bib278)]
    proposed a simple, effective, and generalized Element Attention Gate (EleAttG)
    to enhance the attentional ability of RNN neurons. Si et al. [[279](#bib.bib279)]
    proposed an Attention enhanced Graph Convolutional LSTM (AGC-LSTM) to enhance
    the feature representations of key nodes.
  prefs: []
  type: TYPE_NORMAL
- en: To simultaneously exploit the temporal and spatial features of skeleton sequences,
    some methods aim to design spatial and/or temporal networks. Wang et al. [[244](#bib.bib244)]
    proposed a two-stream RNN for simultaneously learning spatial and temporal relationships
    of skeleton sequences and enhancing the generalization ability of the model through
    a skeleton data enhancement technique with 3D transformations. Liu et al. [[280](#bib.bib280)]
    proposed a spatial-temporal LSTM network, extending the traditional LSTM-based
    learning into the temporal and spatial domains. Considering the importance of
    the relationships between non-neighboring joints in the skeleton data, Zhang et
    al. [[281](#bib.bib281)] designed eight geometric relational features to model
    the spatial information and evaluated them in a three-layer LSTM network. Si et
    al. [[282](#bib.bib282)] proposed a spatial-based Reasoning and Temporal Stack
    Learning (SR-TSL) novel model to capture high-level spatial structural information
    within each frame, and model the detailed dynamic information by combining multiple
    jump-segment LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 GCN-based approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GCN is a recent popular network for skeleton-based action recognition due to
    the human skeleton is a natural graph structure. Compared with CNN and RNN-based
    methods, GCN-based methods could better capture the relationship between joints
    in the skeleton sequence. According to whether the topology (namely vertex connection
    relationship) is dynamically adjusted during inference, GCN-based methods can
    be classified into static methods [[246](#bib.bib246), [283](#bib.bib283), [284](#bib.bib284),
    [285](#bib.bib285)] and dynamic methods [[286](#bib.bib286), [287](#bib.bib287),
    [288](#bib.bib288), [289](#bib.bib289), [290](#bib.bib290), [291](#bib.bib291),
    [292](#bib.bib292), [293](#bib.bib293), [294](#bib.bib294), [295](#bib.bib295),
    [296](#bib.bib296), [297](#bib.bib297), [298](#bib.bib298), [299](#bib.bib299),
    [300](#bib.bib300)].
  prefs: []
  type: TYPE_NORMAL
- en: For static methods, the topologies of GCNs remian fixed during inference. For
    instance, an early application of graph convolutions, spatial-temporal GCN (ST-GCN) [[246](#bib.bib246)],
    is proposed which applies a predefined and fixed topology based on the human body
    structure. Liu et al. [[284](#bib.bib284)] proposed a multi-scale graph topology
    to GCNs for modeling multi-range joint relationships.
  prefs: []
  type: TYPE_NORMAL
- en: For dynamic methods, the topologies of GCNs are dynamically inferred during
    inference. Action structure graph convolution network (AS-GCN) [[286](#bib.bib286)]
    applies an A-link inference module to capture action-specific correlations. Two-stream
    adaptive GCN (2s-AGCN) [[287](#bib.bib287)] and semantics-guided network (SGN) [[301](#bib.bib301)]
    enhanced topology learning with self-attention mechanism for modeling correlations
    between two joints. Although topology dynamic modeling is beneficial for inferring
    intrinsic relations of joints, it may be difficult to encode the context of an
    action since the captured topologies are independent of a pose. Therefore, some
    methods focus on context-dependent intrinsic topology modeling. In Dynamic GCN [[302](#bib.bib302)],
    contextual features of all joints are incorporated to learn the relations of joints.
    Channel topology refinement GCN (CTR-GCN) [[290](#bib.bib290)] focuses on embedding
    joint topology in different channels, while InfoGCN [[291](#bib.bib291)] introduces
    attention-based graph convolution to capture the context-dependent topology based
    on the latent representation learned by information bottleneck. Multi-Level Spatial-Temporal
    excited Graph Network (ML-STGNet) [[298](#bib.bib298)] introduces a spatial data-driven
    excitation module based on Transformer to learn joint relations of different samples
    in a data-dependent way. Multi-View Interactional Graph Network (MV-IGNet) [[303](#bib.bib303)]
    designs a global context adaptation module for adaptive learning of topology structures
    on multi-level spatial skeleton contexts. Spatial Graph Diffusion Convolutional
    (S-GDC) network [[304](#bib.bib304)] aims to learn new graphs by graph diffusion
    for capturing the connections of distant joints on the same body and two interacting
    bodies. In the above dynamic methods, the topology modeling is based only on joint
    information. By contrast, a language model knowledge-assisted GCN (LA-GCN) [[305](#bib.bib305)]
    applies large-scale language model to incorporate action-related prior information
    to learn topology for action recognition.
  prefs: []
  type: TYPE_NORMAL
- en: No matter the static or dynamic methods, they aim to construct different GCNs
    for modeling spatial and temporal features of actions. In contrast, some papers
    work on strategies to assist the ability of different GCNs. For instance, Wang
    et al. [[306](#bib.bib306)] proposed neural Koopman pooling to replace the temporal
    average/max pooling for aggregating spatial-temporal features. The Koopman pooling
    learns class-wise dynamics for better classification. Zhou et al. [[307](#bib.bib307)]
    presented a Feature Refinement head (FR Head) based on contrastive learning to
    improve the discriminative power of ambiguous actions. With the FR Head, the performance
    of some existing methods (eg. 2s-AGCN [[287](#bib.bib287)], CTR-GCN [[290](#bib.bib290)])
    can be improved by about 1%.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, GCN-based methods can effectively utilize and handle the joint relations
    by topological networks but are generally limited to local spatial-temporal neighborhoods.
    Compared with static methods, dynamic methods have stronger generalization capabilities
    due to the dynamic topologies.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Transformer-based approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Transformer was originally designed for machine translation tasks in natural
    language processing. Vision Transformer (ViT) [[308](#bib.bib308)] is the first
    work to use a Transformer encoder to extract image features in computer vision.
    When introducing Transformer to skeleton-based action recognition, the core is
    how to design a better encoder for modeling spatial and temporal information of
    skeleton sequences. Compared with GCN-methods, Transformer-based methods can quickly
    obtain global topology information and enhance the correlation of non-physical
    joints. There are mainly three categories of methods: pure Transformer, hybid
    Transformer and unsupervised Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: The first category of methods applies the standard Transformer for learning
    spatial and temporal features. A spatial Transformer and a temporal Transformer
    are often applied alternately or together based on one stream [[309](#bib.bib309),
    [310](#bib.bib310), [311](#bib.bib311)] or two-stream [[312](#bib.bib312), [313](#bib.bib313),
    [314](#bib.bib314)] network. Shi et al. [[309](#bib.bib309)] proposed to decouple
    the data into spatial and temporal dimensions, where the spatial and temporal
    streams respectively include motion-irrelevant and motion-relevant features. A
    Decoupled Spatial-Temporal Attention Network (DSTA-Net) was proposed to encode
    the two streams sequentially based on the attention module. It allows modeling
    spatial-temporal dependencies between joints without the information about their
    positions or mutual connections. Ijaz et al. [[311](#bib.bib311)] proposed a multi-modal
    Transformer-based network for nursing activity recognition which fuses the encoding
    results of the spatial-temporal skeleton model and acceleration model. The spatial-temporal
    skeleton model comprises of spatial and temporal Transformer encoder in a sequential
    processing, which computes spatial and temporal features from joints. The acceleration
    model has one Transformer block, which computes correlation across acceleration
    data points for a given action sample. Zhang et al. [[312](#bib.bib312)] proposed
    a Spatial-Temporal Special Transformer (STST) to capture skeleton sequences in
    the temporal and spatial dimensions separately. STST is a two-stream structure
    including a spatial transformer block and a directional temporal transformer block.
    Relation-mining Self-Attention Network (RSA-Net) [[314](#bib.bib314)] applies
    seven RSA bolcks in spatial and temporal domains for learning intra-frame and
    inter-frame action features. Such a two-stream structure leads to the extension
    of the feature dimension and makes the network capture richer information, but
    at the same time increases the computational cost. To reduce the computational
    cost, Shi et al. [[313](#bib.bib313)] proposed a Sparse Transformer-based Action
    Recognition (ST-AR) model. ST-AR consists of a sparse self-attention module performed
    on sparse matrix multiplications for capturing spatial correlations, and a segmented
    linear self-attention module processed on variable lengths of sequences for capturing
    temporal correlations to further reduce the computation and memory cost.
  prefs: []
  type: TYPE_NORMAL
- en: Since Transformer is weak in extracting discriminative information from local
    features and short-term temporal information, the second category of methods [[247](#bib.bib247),
    [315](#bib.bib315), [316](#bib.bib316), [317](#bib.bib317), [318](#bib.bib318),
    [319](#bib.bib319), [320](#bib.bib320), [321](#bib.bib321), [306](#bib.bib306),
    [322](#bib.bib322)] integrate Transformer with GCN and CNN for better feature
    extraction, which is beneficial to utilize the advantages of different networks.
    Plizzari et al. [[247](#bib.bib247)] proposed a two-stream Spatial-Temporal TRansformer
    network (ST-TR) by integrating spatial and temporal Transformers with Temporal
    Convolution Network and GCN. Qiu et al. [[316](#bib.bib316)] proposed a Spatio-Temporal
    Tuples Transformer (STTFormer) which includes a spatio-temporal tuples self-attention
    module for capturing joint relationship in consecutive frames, and an Inter-Frame
    Feature Aggregation (IFFA) module for enhancing the ability to distinguish similar
    actions. Similar to ST-TR, the IFFA module applies TCN to aggregate features of
    sub-actions. Yang et al. [[318](#bib.bib318)] presented Zoom-Former for extending
    single-person action recognition to multi-person group activities. The Zoom-Former
    improves the traditional GCN by designing a Relation-aware Attention mechanism,
    which comprehensively leverages the prior knowledge of body structure and the
    global characteristic of human motion to exploit the multi-level features. With
    this improvement, Zoom-Former could hierarchically extract the low-level motion
    information of a single person and the high-level interaction information of multiple
    people. To effectively capture the relationship between key local joints and global
    contextual information in the spatial and temporal dimension, Gao et al. [[319](#bib.bib319)]
    proposed an end-to-end Focal and Global Spatial-Temporal transFormer (FG-STForm)
    by integrating temporal convolutions into a global self-attention mechanism. Liu
    et al. [[320](#bib.bib320)] proposed a Kernel Attention Adaptive Graph Transformer
    Network to use a graph transformer operator for modeling higher-order spatial
    dependencies between joints. Wang et al. [[306](#bib.bib306)] proposed a Multi-order
    Multi-mode Transformer (3Mformer) by applying a higher-order Transformer to process
    hypergraphs of skeleton data for better capturing higher-order motion patterns
    between body joints. SkeleTR [[322](#bib.bib322)] initially employs a GCN to capture
    intra-person dynamic information and then applies a stacked Transformer encoder
    to model the person interaction. It can handle different tasks including video-level
    action recognition, instance-level action detection and group activity recognition.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the generalization ability of features, the third category of methods [[323](#bib.bib323),
    [324](#bib.bib324), [325](#bib.bib325), [326](#bib.bib326), [327](#bib.bib327),
    [328](#bib.bib328)] focus on unsupervised or self-supervised action recognition
    based on Transformer which has demonstrated excellent performance in capturing
    global context and local joint dynamics. These methods normally apply contrastive
    learning or Encoder-Decoder architecture for learning a better representation
    of actions. Kim et al. [[323](#bib.bib323)] proposed GL-Transformer, which designs
    a global and local attention mechanism to learn the local joint motion changes
    and global contextual information of skeleton sequences. With the motion sequence
    representation, actions are classified based on their average pooling on the temporal
    axis. Anshul et al. [[325](#bib.bib325)] designed the HaLP module by generating
    hallucinating latent positive samples for self-supervised learning based on contrastive
    learning. This module can explore the potential space of human postures in the
    appropriate directions to generate new positive samples, and optimize the solution
    efficiency by a new approximation function.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the research on skeleton-based action recognition has made great
    progress in recent years. CNN-based methods mainly convert skeleton sequences
    into images, excelling at capturing spatial information of actions but potentially
    losing temporal information. With the help of RNN for representing temporal information,
    RNN-based methods focus on representing spatial information based on the spatial
    division of the human body combining attention mechanism. Compared with CNN and
    RNN-based methods, GCN and Transformer-based methods have greater advantages and
    become the mainstream methods. GCN-based methods are beneficial for representing
    joint relations by topological networks in which dynamic topology-based methods
    have stronger generalization ability than static ones. However, they are mostly
    confined to local spatial-temporal neighborhoods. Transformer-based methods can
    quickly obtain global topology information and enhance the correlation of non-physical
    joints. Combining Transformers with CNN and GCN represents a promising approach
    for extracting both local and global features, enhancing action recognition performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Benchmark datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section reviews the commonly used datasets for the three tasks and also
    compares the performance of different methods on some popular datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Datasets for 2D HPE. PCP: Percentage of Correct Localized Parts, PCPm:
    Mean Percentage of Correctly Localized Parts, PCK: Percentage of Correct Keypoints,
    PCKh: Percentage of Correct Keypoints with a specified head size, AP: Average
    Precision, mAP: mean Average Precision. IB: Image-based, VB: Video-based. SP:
    single person, MP: multi-person. Train, Val and Test represent frame numbers except
    for Penn Action and PoseTrack, and they represent video numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Dataset | Year | Citation | #Poses | #Joints | Train | Val | Test | SP/MP
    | Actions | Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| IB | LSP [[329](#bib.bib329)] | 2010 | 971 | 2,000 | 14 | 1k | - | 1k | SP
    | $\times$ | PCP/PCK |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LSPET [[330](#bib.bib330)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2011 | 509 | 10,000 | 14 | 10k | - | - | SP | $\times$ | PCP |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FLIC [[331](#bib.bib331)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2013 | 537 | 5,003 | 10 | 4k | - | 1k | SP | $\times$ | PCK/PCP |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MPII [[332](#bib.bib332)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 | 2583 | 26,429 | 16 | 29k | - | 12k | SP | ✓ | PCPm/PCKh |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MPII multi-person [[332](#bib.bib332)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 | 2583 | 14,993 | 16 | 3.8k | - | 1.7k | MP | ✓ | mAP |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSCOCO16 [[333](#bib.bib333)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 | 37862 | 105,698 | 17 | 45k | 22k | 80k | MP | $\times$ | AP |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSCOCO17 [[333](#bib.bib333)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 | 37862 | - | 17 | 64k | 2.7k | 40k | MP | $\times$ | AP |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LIP [[334](#bib.bib334)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | 482 | 50462 | 16 | 30k | 10k | 10k | SP | $\times$ | PCK |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CrowdPose [[335](#bib.bib335)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | 423 | 80000 | 14 | 10k | 2k | 8k | MP | $\times$ | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| VB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; J-HMDB [[248](#bib.bib248)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2013 | 849 | 31,838 | 15 | 2.4k | - | 0.8k | SP | ✓ | PCK |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Penn Action [[336](#bib.bib336)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2013 | 367 | 159,633 | 13 | 1k | - | 1k | SP | ✓ | PCK |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PoseTrack17 [[337](#bib.bib337)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | 420 | 153,615 | 15 | 292 | 50 | 208 | MP | ✓ | mAP |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PoseTrack18 [[337](#bib.bib337)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | 420 | - | 15 | 593 | 170 | 375 | MP | ✓ | mAP |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PoseTrack21 [[338](#bib.bib338)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2022 | 15 | - | 15 | 593 | 170 | - | MP | ✓ | mAP |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Datasets for 3D HPE. MPJPE:Mean Per Joint Position Error, PA-MPJPE:
    Procrustes Analysis Mean Per Joint Position Error, MPJAE: Mean Per Joint Angular
    Erro, 3DPCK: 3D Percentage of Correct Keypoints, MPJAE: Mean Per Joint Angular
    Error, AP: Average Precision.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Dataset | Year | Citation | #Joints | #Frames | SP/MP | Actions | Metrics
    |'
  prefs: []
  type: TYPE_TB
- en: '| VB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; HumanEva-I [[339](#bib.bib339)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2010 | 1678 | 15 | 37.6k | SP | ✓ | MPJPE/PA-MPJPE |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Human3.6M [[340](#bib.bib340)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 | 2677 | 17 | 3.6M | SP | ✓ | MPJPE |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MPI-INF-3DHP [[143](#bib.bib143)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | 851 | 15 | 1.3M | SP | ✓ | 3DPCK |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CMU Panoptic [[341](#bib.bib341)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | 680 | 15 | 1.5M | MP | ✓ | 3DPCK/MPJPE |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3DPW [[342](#bib.bib342)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | 674 | 18 | 51k | MP | $\times$ | MPJPE/MPJAE/PA-MPJPE |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MuPoTs-3D [[185](#bib.bib185)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | 346 | 15 | 8k | MP | $\times$ | 3DPCK |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MuCo-3DHP [[185](#bib.bib185)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | 346 | - | - | MP | $\times$ | 3DPCK |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Pose estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The datasets are reviewed based on 2D and 3D pose estimation tasks and the
    details are summarized in Table [1](#S5.T1 "Table 1 ‣ 5 Benchmark datasets ‣ Human
    Pose-based Estimation, Tracking and Action Recognition with Deep Learning: A Survey")
    and [2](#S5.T2 "Table 2 ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey"). Due to the page
    limit, we mainly review some popular and large-scale pose datasets in the following
    sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Datasets for 2D pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the image-based 2D pose estimation, Microsoft Common Objects in Context
    (COCO) [[333](#bib.bib333)] and Max Planck Institute for Informatics (MPII) [[332](#bib.bib332)]
    are popular datasets. Joint-annotated HMDB (J-HMDB) dataset [[248](#bib.bib248)]
    and Penn Action [[336](#bib.bib336)] datasets are often used for the 2D video-based
    single-person pose estimation (SPPE), while PoseTrack [[337](#bib.bib337)] is
    often used for video-based multiple-person pose estimation (MPPE).
  prefs: []
  type: TYPE_NORMAL
- en: 'The COCO dataset [[333](#bib.bib333)] is the most widely used large-scale dataset
    for pose estimation. It was created by extracting everyday scene images with common
    objects and labeling the objects using per-instance segmentation. This dataset
    consists of more than 330,000 images and 200,000 labeled persons, and each person
    is labeled with 17 keypoints. It has two versions for pose estimation including
    COCO2016 and COCO2017\. The two versions are different with the number of images
    for training, testing and validation as shown in Table [1](#S5.T1 "Table 1 ‣ 5
    Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey"). Except of pose estimation, this dataset can be
    also suitable for object detection, image segmentation and captioning.'
  prefs: []
  type: TYPE_NORMAL
- en: The MPII dataset [[332](#bib.bib332)] was collected from 3,913 YouTube videos
    by the Max Planck Institute for Informatics. It consists of 24,920 images including
    over 40,000 individuals with 16 annotated body joints. These images were collected
    by a two-level hierarchical method to capture everyday human activities. This
    dataset involves 491 activity samples in 21 classes and all the images are labeled.
    Except for joints, rich annotations including body occlusion, 3D torso and head
    orientations are also labeled on Amazon Mechanical Turk. The MPII dataset serves
    as a valuable resource for both 2D single-person and multi-person pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: The J-HMDB dataset  [[248](#bib.bib248)] was created by annotating human joints
    of the HMDB51 action dataset. From HMDB51, 928 videos including 21 actions of
    a single person were extracted and the human joints of each were annotated using
    a 2D articulated human puppet model. Each video consists of 15-40 frames. In total,
    there are 31,838 annotated frames. This dataset can serve as a benchmark for human
    detection, pose estimation, pose tracking and action recognition. It also presents
    a new challenge for video-based pose estimation or tracking since it includes
    more variations in camera motions, motion blur and partial or full-body visibility.
    Sub-J-HMDB dataset [[248](#bib.bib248)] is a subset of the J-HMDB dataset and
    contains 316 videos with a total of 11,200 frames.
  prefs: []
  type: TYPE_NORMAL
- en: The Penn Action dataset [[336](#bib.bib336)] is also an annotated sports action
    dataset collected by the University of Pennsylvania. It consists of 2,326 videos
    with 15 actions and each frame was annotated with 13 keypoints for each person.
    The dataset can be used for the tasks of pose estimation, action detection and
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PoseTrack Dataset [[337](#bib.bib337)] was collected from raw videos of
    the MPII Pose Dataset. For each frame in MPII, 41-298 neighboring frames with
    crowded scenes and multiple individuals were selected for PoseTrack dataset. The
    selected videos were annotated with person locations, identities, body pose and
    ignore regions. According to different number of videos, this dataset currently
    exists in three versions: PoseTrack2017, PoseTrack2018, and PoseTrack2021\. In
    total, PoseTrack2017 contains 292 videos for training, and 50 videos for validation
    and 208 videos for testing. Among them, 23,000 frames are labeled with a very
    lager number (i.e. 153,615) of annotated poses. PoseTrack2018 increases the number
    of the video and contains 593 videos for training, 170 videos for validation,
    and 315 videos for testing, and consists of 46,933 labeled frames. PoseTrack2021
    is an extension of PoseTrack2018 with more annotations (eg. bounding box of small
    persons, joint occlusions). With the person identities, this dataset has been
    widely used as a benchmark to evaluate multi-person pose estimation and tracking
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance comparison for 2D image-based pose estimation on COCO
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Category | Year | Method | COCO |'
  prefs: []
  type: TYPE_TB
- en: '|  | Backbone | Inputsize | AP | AP.5 | AP.75 | APM | APL |'
  prefs: []
  type: TYPE_TB
- en: '| SP | Regression-based | 2021 | TFPose [[34](#bib.bib34)] | ResNet-50 | 384×288
    | 72.2 | 90.9 | 80.1 | 69.1 | 78.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | PRTR [[35](#bib.bib35)] | HRNet-W32 | 512×384 | 72.1 | 90.4 | 79.6
    | 68.1 | 79.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Panteleris et al. [[37](#bib.bib37)] | - | 384×288 | 72.6 | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Heatmap-based | 2021 | Li et al. [[63](#bib.bib63)] | HRNet-W48 | - | 75.7
    | 92.3 | 82.9 | 72.3 | 81.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Li et al. [[62](#bib.bib62)] | HRNet-W48 | 384×288 | 76.0 | 92.4 |
    83.5 | 72.5 | 81.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | DistilPose [[64](#bib.bib64)] | HRNet-W48-stage3 | 256×192 | 73.7
    | 91.6 | 81.1 | 70.2 | 79.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MP | Top-down | 2017 | Papandreou et al. [[67](#bib.bib67)] | ResNet-101
    | 353×257 | 68.5 | 87.1 | 75.5 | 65.8 | 73.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | RMPE [[81](#bib.bib81)] | Hourglass | - | 61.8 | 83.7 | 69.8 | 58.6
    | 67.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Xiao et al. [[69](#bib.bib69)] | ResNet-152 | 384×288 | 73.7 | 91.9
    | 81.1 | 70.3 | 80.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | CPN [[82](#bib.bib82)] | ResNet | 384×288 | 73.0 | 91.7 | 80.9 | 69.5
    | 78.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Posefix [[70](#bib.bib70)] | ResNet-152 | 384×288 | 73.6 | 90.8 |
    81.0 | 70.3 | 79.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Sun et al. [[71](#bib.bib71)] | HRNet-W48 | 384×288 | 77 | 92.7 |
    84.5 | 73.4 | 83.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Su et al. [[83](#bib.bib83)] | ResNet-152 | 384×288 | 74.6 | 91.8
    | 82.1 | 70.9 | 80.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Cai et al. [[72](#bib.bib72)] | 4×RSN-50 | 384×288 | 78.6 | 94.3 |
    86.6 | 75.5 | 83.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Huang et al. [[73](#bib.bib73)] | HRNet | 384×288 | 77.5 | 92.7 |
    84.0 | 73.0 | 82.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Zhang et al. [[74](#bib.bib74)] | HRNet-W48 | 384×288 | 77.4 | 92.6
    | 84.6 | 73.6 | 83.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Graphpcnn [[75](#bib.bib75)] | HR48 | 384×288 | 76.8 | 92.6 | 84.3
    | 73.3 | 82.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Qiu et al. [[84](#bib.bib84)] | - | 384×288 | 74.1 | 91.9 | 82.2 |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | TransPose [[85](#bib.bib85)] | HRNet-W48 | 256×192 | 75.0 | 92.2 |
    82.3 | 71.3 | 81.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | TokenPose [[87](#bib.bib87)] | - | 384×288 | 75.9 | 92.3 | 83.4 |
    72.2 | 82.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | HRFormer [[88](#bib.bib88)] | - | 384×288 | 76.2 | 92.7 | 83.8 | 72.5
    | 82.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | ViTPose [[89](#bib.bib89)] | ViTAE-G | 576×432 | 81.1 | 95.0 | 88.2
    | 77.8 | 86.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Xu et al. [[76](#bib.bib76)] | HR48 | 384×288 | 76.6 | 92.4 | 84.3
    | 73.2 | 82.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | PGA-Net [[77](#bib.bib77)] | HRNet-W48 | 384x288 | 76.0 | 92.5 | 83.5
    | 72.4 | 82.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | BCIR [[78](#bib.bib78)] | HRNet-W48 | 384x288 | 76.1 | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Bottom-up | 2017 | Associative embedding [[97](#bib.bib97)] | Hourglass |
    512×512 | 65.5 | 86.8 | 72.3 | 60.6 | 72.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Multiposenet [[98](#bib.bib98)] | ResNet50 | 480×480 | 69.6 | 86.3
    | 76.6 | 65.0 | 76.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | OpenPose [[93](#bib.bib93)] | - | - | 61.8 | 84.9 | 67.5 | 57.1 |
    68.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Pifpaf [[94](#bib.bib94)] | ResNet50 | - | 55.0 | 76.0 | 57.9 | 39.4
    | 76.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Jin et al. [[100](#bib.bib100)] | Hourglass | 512×512 | 67.6 | 85.1
    | 73.7 | 62.7 | 74.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Higherhrnet [[101](#bib.bib101)] | HrHRNet-W48 | 640×640 | 72.3 |
    91.5 | 79.8 | 67.9 | 78.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DEKR [[103](#bib.bib103)] | HRNet-W48 | 640x640 | 71.0 | 89.2 | 78.0
    | 67.1 | 76.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | HOP [[96](#bib.bib96)] | HRNet-W48 | 640×640 | 70.5 | 89.3 | 77.2
    | 66.6 | 75.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | Cheng et al. [[95](#bib.bib95)] | HRNet-W48 | 640×640 | 71.5 | 89.1
    | 78.5 | 67.2 | 78.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | PolarPose [[104](#bib.bib104)] | HRNet-W48 | 640x640 | 70.2 | 89.5
    | 77.5 | 66.1 | 76.4 |'
  prefs: []
  type: TYPE_TB
- en: '| One-stage | 2019 | Directpose [[105](#bib.bib105)] | ResNet-101 | 800×800
    | 64.8 | 87.8 | 71.1 | 60.4 | 71.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | FCPose [[106](#bib.bib106)] | DLA-60 | 736 × 512 | 65.9 | 89.1 | 72.6
    | 60.9 | 74.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | InsPose [[107](#bib.bib107)] | HRNet-w32 | - | 71.0 | 91.3 | 78.0
    | 67.5 | 76.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | PETR [[109](#bib.bib109)] | Swin-L | - | 71.2 | 91.4 | 79.6 | 66.9
    | 78.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | ED-pose [[65](#bib.bib65)] | Swin-L | - | 72.7 | 92.3 | 80.9 |
    67.6 | 80.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | GroupPose [[110](#bib.bib110)] | Swin-L | - | 72.8 | 92.5 | 81.0
    | 67.7 | 80.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | SMPR [[108](#bib.bib108)] | HRNet-w32 | 800x800 | 70.2 | 89.7 |
    77.5 | 65.9 | 77.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance comparison for 2D video-based SPPE on Penn Action dataset
    and JHMDB dataset. FF: frame-by-frame; SF: sample frame-based.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Year | Method | Penn | JHMDB |'
  prefs: []
  type: TYPE_TB
- en: '| PCK | PCK |'
  prefs: []
  type: TYPE_TB
- en: '| FF | 2016 | Gkioxari et al. [[116](#bib.bib116)] | 91.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Song et al. [[114](#bib.bib114)] | 96.4 | 92.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | LSTM [[118](#bib.bib118)] | 97.7 | 93.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | DKD [[119](#bib.bib119)] | 97.8 | 94 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Li et al. [[120](#bib.bib120)] | - | 94.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | RPSTN [[123](#bib.bib123)] | 98.7 | 97.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | HANet [[124](#bib.bib124)] | - | 99.6 |'
  prefs: []
  type: TYPE_TB
- en: '| SF | 2020 | K-FPN [[125](#bib.bib125)] | 98 | 94.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | REMOTE [[126](#bib.bib126)] | 98.6 | 95.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | DeciWatch [[127](#bib.bib127)] | - | 98.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | MixSynthFormer [[128](#bib.bib128)] | - | 99.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Performance comparison for 2D video-based MPPE on PoseTrack2017 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Year | Method | Val | Test |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| mAP | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Top-down | 2018 | Xiao et al. [[69](#bib.bib69)] | 76.7 | 73.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Pose Flow [[129](#bib.bib129)] | 66.5 | 63.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Detect-Track [[130](#bib.bib130)] | - | 64.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Wang et al. [[131](#bib.bib131)] | 81.5 | 73.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | AlphaPose [[132](#bib.bib132)] | 74.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | SLT-Pose [[134](#bib.bib134)] | 81.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | DiffPose [[137](#bib.bib137)] | 83.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | TDMI [[133](#bib.bib133)] | 83.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Bottom-up | 2019 | PGG [[138](#bib.bib138)] | 77.0 | - |'
  prefs: []
  type: TYPE_TB
- en: 5.1.2 Datasets for 3D pose estimation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared with the 2D datasets, acquiring high-quality annotation for 3D poses
    is more challenging and requires motion caption systems (eg., Mocap, wearable
    IMUs). Therefore, 3D pose datasets are normally built in constrained environments.
    Currently, Human3.6M and MPI-INF-3DHP are widely used for the task of SPPE, and
    MuPoTs-3D is often used for MPPE task.
  prefs: []
  type: TYPE_NORMAL
- en: The Human3.6M dataset [[340](#bib.bib340)] is the largest and most representation
    indoor dataset for 3D single-person pose estimation. It was collected by recording
    videos of 11 human subjects performing 17 activities from 4 camera views, and
    capturing poses by marker-based Mocap systems. In total, this dataset consists
    of 3.6 million poses with one pose in one frame. This dataset is suitable for
    the HPE task from images or videos. With video-based HPE, a sequence of frames
    in a suitable receptive field is considered as the input. Protocol 1 is the most
    common protocol which applies frames of 5 subjects (S1, S5, S6, S7, S8) for training
    and the frames of 2 subjects (S9, S11) for test.
  prefs: []
  type: TYPE_NORMAL
- en: The MPI-INF-3DHP dataset [[143](#bib.bib143)] is a large 3D single-person pose
    dataset in both indoor and outdoor environments. It was captured by a maker-less
    MoCap system in a multi-camera studio. There are 8 subjects performing 8 activities
    from 14 camera views. This dataset provides 1.3 million frames, but more diverse
    motions than Human3.6M. Same as Human3.6M, this dataset is also suitable for the
    HPE task from images or videos. The test set includes the frames of 6 subjects
    with different scenes.
  prefs: []
  type: TYPE_NORMAL
- en: The MuPoTs-3D dataset [[185](#bib.bib185)] is a multi-person 3D pose dataset
    in both indoor and outdoor environments. Same as MPI-INF-3DHP, it was also captured
    by a multi-view marker-less MoCap system. Over 8,000 frames were collected in
    20 videos by 8 subjects. There are some challenging frames with occlusions, drastic
    illumination changes and lens flares in some outdoor scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Performance comparison for 3D SPPE on Human3.6M and MPI-INF-3DHP dataset.
    IB: Image-based, VB: Video-based.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Category | Year | Method |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Human3.6M &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MPJPE$\downarrow$   PMPJPE$\downarrow$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MPI-INF-3DHP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PCK      AUC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| IB | One-stage | 2015 | Li et al. [[140](#bib.bib140)] | 122.0 | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | Zhou et al. [[142](#bib.bib142)] | 107.3 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Mehta et al. [[143](#bib.bib143)] | 74.1 | - | 57.3 | 28.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | WTL [[144](#bib.bib144)] | 64.9 | - | 69.2 | 32.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Two-stage | 2017 | Martinez et al. [[145](#bib.bib145)] | 62.9 | 47.7 | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Tekin et al. [[146](#bib.bib146)] | 69.7 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Jahangiri et al. [[150](#bib.bib150)] | - | 68.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Drpose3d [[148](#bib.bib148)] | 57.8 | 42.9 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Yang et al. [[165](#bib.bib165)] | 58.6 | 37.7 | 80.1 | 45.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Habibie et al. [[166](#bib.bib166)] | 49.2 | - | 82.9 | 45.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Chen et al. [[167](#bib.bib167)] | - | 68.0 | 71.1 | 36.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | RepNet [[168](#bib.bib168)] | 80.9 | 65.1 | 82.5 | 58.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Hemlets pose [[147](#bib.bib147)] | - | - | 75.3 | 38.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Sharma et al. [[151](#bib.bib151)] | 58.0 | 40.9 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Li and Lee [[152](#bib.bib152)] | 52.7 | 42.6 | 67.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | LCN [[153](#bib.bib153)] | 52.7 | 42.2 | 74.0 | 36.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | semantic-GCN [[154](#bib.bib154)] | - | 57.6 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Iqbal et al. [[169](#bib.bib169)] | 67.4 | 54.5 | 79.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Pose2mesh [[155](#bib.bib155)] | 64.9 | 48.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Srnet [[156](#bib.bib156)] | 44.8 | - | 77.6 | 43.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Liu et al. [[157](#bib.bib157)] | 52.4 | 41.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Zou et al. [[158](#bib.bib158)] | 49.4 | 39.1 | 86.1 | 53.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphSH [[159](#bib.bib159)] | 51.9 | - | 80.1 | 45.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Lin et al. [[163](#bib.bib163)] | 54.0 | 36.7 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Yu et al. [[172](#bib.bib172)] | 92.4 | 52.3 | 86.2 | 51.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Graformer [[164](#bib.bib164)] | 51.8 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | PoseTriplet [[173](#bib.bib173)] | 78 | 51.8 | 89.1 | 53.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | HopFIR [[162](#bib.bib162)] | 48.5 | - | 87.2 | 57.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | SSP-Net [[149](#bib.bib149)] | 51.6 | - | 83.2 | 44.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 2023 | PHGANet [[160](#bib.bib160)] | 49.1 | - | 86.9 | 55.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 2023 | RS-Net [[161](#bib.bib161)] | 47.0 | 38.6 | 85.6 | 53.2 |'
  prefs: []
  type: TYPE_TB
- en: '| VB | One-stage | 2016 | Tekin et al. [[194](#bib.bib194)] | 125.0 | - | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Vnect [[195](#bib.bib195)] | 80.5 | - | 79.4 | 41.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Dabral et al. [[196](#bib.bib196)] | 52.1 | 36.3 | 76.7 | 39.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | IVT [[197](#bib.bib197)] | 40.2 | 28.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | CSS [[198](#bib.bib198)] | 60.1 | 46.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Two-stage | 2017 | RPSM [[212](#bib.bib212)] | 73.1 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Rayat et al. [[213](#bib.bib213)] | 51.9 | 42.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | p-LSTMs [[214](#bib.bib214)] | 55.8 | 46.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Katircioglu et al. [[215](#bib.bib215)] | 67.3 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Cheng et al. [[200](#bib.bib200)] | 42.9 | 32.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Cai et al. [[203](#bib.bib203)] | 48.8 | 39.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | TCN [[199](#bib.bib199)] | 46.8 | 36.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Chirality Nets [[216](#bib.bib216)] | 46.7 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | UGCN [[217](#bib.bib217)] | 42.6 | 32.7 | 86.9 | 62.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | GAST-Net [[201](#bib.bib201)] | 44.9 | 35.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Chen et al. [[202](#bib.bib202)] | 44.1 | 35.0 | 87.9 | 54.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | PoseFormer [[204](#bib.bib204)] | 44.3 | 34.6 | 88.6 | 56.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Strided [[206](#bib.bib206)] | 43.7 | 35.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Mhformer [[207](#bib.bib207)] | 43.0 | - | 93.8 | 63.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | MixSTE [[219](#bib.bib219)] | 39.8 | 30.6 | 94.4 | 66.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | UPS [[259](#bib.bib259)] | 40.8 | 32.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | DSTFormer [[222](#bib.bib222)] | 37.5 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | GLA-GCN [[218](#bib.bib218)] | 44.4 | 34.8 | 98.5 | 79.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | D3DP [[210](#bib.bib210)] | 35.4 | - | 98.0 | 79.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | DiffPose [[209](#bib.bib209)] | 43.3 | 32.0 | 84.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | STCFormer [[211](#bib.bib211)] | 40.5 | 31.8 | 98.7 | 83.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | PoseFormerV2 [[205](#bib.bib205)] | 45.2 | 35.6 | 97.9 | 78.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2023 | MTF-Transformer [[221](#bib.bib221)] | 26.2 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Performance comparison for 3D Image-based MPPE on MuPoTS-3D dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | MuPoTS-3D |'
  prefs: []
  type: TYPE_TB
- en: '| Category | Year | Method | All people | Matched people |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | PCKrel | PCKabs | PCKrel | PCKabs | PCKroot | AUCrel |'
  prefs: []
  type: TYPE_TB
- en: '| Top-down | 2019 | LCR-Net [[177](#bib.bib177)] | 70.6 | - | 74.0 | - | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Moon et al. [[178](#bib.bib178)] | 81.8 | 31.5 | 82.5 | 31.8 | 31.0
    | 40.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | HDNet [[179](#bib.bib179)] | - | - | 83.7 | 35.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | HMOR [[180](#bib.bib180)] | - | - | 82.0 | 43.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Cha et al. [[181](#bib.bib181)] | 89.9 | - | 91.7 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Bottom-up | 2018 | Mehta et al. [[185](#bib.bib185)] | 65.0 | - | 69.8 |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Kundu et al. [[184](#bib.bib184)] | 74.0 | 28.1 | 75.8 | - | - | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | XNect [[186](#bib.bib186)] | 70.4 | - | 75.8 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Smap [[187](#bib.bib187)] | 73.5 | 35.4 | 80.5 | 38.7 | 45.5 | 42.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Liu et al. [[188](#bib.bib188)] | 79.4 | 36.5 | 86.5 | 39.3 | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | AKE [[189](#bib.bib189)] | 74.7 | 37.2 | 81.1 | 40.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| One-stage | 2022 | Wang et al. [[175](#bib.bib175)] | 82.7 | 39.2 | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | DRM [[192](#bib.bib192)] | 80.9 | 39.3 | 85.1 | 41.0 | 45.6 | 45.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | WSP [[193](#bib.bib193)] | 82.4 | - | 83.2 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: 5.1.3 Performance comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [3](#S5.T3 "Table 3 ‣ 5.1.1 Datasets for 2D pose estimation ‣ 5.1
    Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey"), we present a comparison
    of different methods for 2D image-based SPPE and MPPE on the COCO dataset. For
    the SPPE task, the performance of heatmap-based methods generally outperforms
    the regression-based methods. This superiority can be attributed to the richer
    spatial information provided by heatmaps, where the probabilistic prediction of
    each pixel enhances the accuracy of keypoint localization. However, heatmap-based
    methods [[64](#bib.bib64)] suffer seriously from the quantization error problem
    and high-computational cost using high resolution heatmaps. For the MPPE task,
    the top-down methods overall outperform the bottom-up methods by the success of
    existing SPPE techniques after detecting individuals. However, they suffer from
    early commitment and have greater computational costs than bottom-up methods.
    One-stage methods speed up the process by eliminating the intermediate operations
    (eg., grouping, ROI, NMS) introduced by top-down and bottom-up methods, while
    their performance [[110](#bib.bib110)] is still lower (about 9% of AP score in
    the best case) than top-down methods [[89](#bib.bib89)]. Moreover, It is also
    observed that the backbone and input image size are two factors for the results.
    The commonly used backbone includes ResNet, HRNet and Hourglass. The recent Transformer-based
    network (eg., ViTAE-G, Swin-L) can be also used as the backbone and the method [[89](#bib.bib89)]
    based on ViTAE-G network achieves the best performance. When using the same backbone [[74](#bib.bib74),
    [85](#bib.bib85)] for the same category of methods, the larger the image size,
    the better the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S5.T4 "Table 4 ‣ 5.1.1 Datasets for 2D pose estimation ‣ 5.1 Pose
    estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and
    Action Recognition with Deep Learning: A Survey") and Table [5](#S5.T5 "Table
    5 ‣ 5.1.1 Datasets for 2D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark
    datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep
    Learning: A Survey") compare the different methods for 2D video-based SPPE and
    MPPE. Overall, two categories of methods for video-based SPPE achieve comparable
    results on two datasets. Yet sample frames-based methods[[127](#bib.bib127)] are
    generally faster than frame-by-frame ones by ignoring looking at all frames. Similar
    to image-based MPPE, the top-down methods achieve better performance than the
    bottom-up methods for video-based MPPE.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For 3D pose estimation, taken Human3.6M, MPI-INF-3DHP and MuPoTS-3D datasets
    as examples, Table [6](#S5.T6 "Table 6 ‣ 5.1.2 Datasets for 3D pose estimation
    ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") and Table [7](#S5.T7 "Table
    7 ‣ 5.1.2 Datasets for 3D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark
    datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep
    Learning: A Survey") respectively shows the comparisons for SPPE and MPPE from
    images or videos. The comparison for video-based MPPE was not conducted due to
    only fewer existing methods. For the SPPE task, two-stage methods normally lift
    3D poses from the estimated 2D poses, they generally outperform one-stage methods
    due to the success of the 2D pose estimation technique. It is also noted that
    the recent one-stage method based on Transformer network [[197](#bib.bib197)]
    also achieves pretty good results. Compared to the same category of methods between
    images and videos, the performance based on videos is better than the ones based
    on images. It demonstrates that the temporal information of videos is beneficial
    for estimating more accurate poses. From Table [7](#S5.T7 "Table 7 ‣ 5.1.2 Datasets
    for 3D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey"), good
    progress has been made in recent years for the MPPE task. Specifically, one-stage
    methods generally perform better than most top-down and bottom-up methods, which
    further implies that the end-to-end training could reduce intermediate errors
    such as human detection and joint grouping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Datasets for Pose tracking. MOTA: Multiple Object Tracking Accuracy,
    PCP: Percentage of Correct Parts, KLE: Keypoint Localization Error.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | Citation | #Joints | Size | 2D/3D | Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VideoPose2.0 [[343](#bib.bib343)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2011 | 198 | - | 44 videos | 2D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-Person PoseTrack [[235](#bib.bib235)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | 238 | 14 | 16 subjects, 60 videos | 2D | MOTA |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PoseTrack17 [[337](#bib.bib337)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | 420 | 15 | 40 subjects, 550 videos | 2D | MOTA |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PoseTrack18 [[337](#bib.bib337)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | 420 | 15 | 1138 videos | 2D | MOTA |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ICDPose [[130](#bib.bib130)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 | 250 | 14 | 60 videos | 2D | MOTA |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Campus dataset [[344](#bib.bib344)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2011 | 1253 | - | 3 subjects, 3 views, 6k frames | 3D | PCP |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Outdoor Pose [[345](#bib.bib345)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2013 | 61 | 14 | 4 subjects, 828 frames | 3D | PCP/KLE |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CMU Panoptic [[341](#bib.bib341)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | 680 | 15 | 8 subjects, 480 views, 65 videos | 3D | MOTA |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Performance comparison for 2D single person pose tracking on Videopose2.0.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Category | Year | AP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[226](#bib.bib226)] | Post-processing | 2015 | 85.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Samanta et al. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[227](#bib.bib227)] | Post-processing | 2016 | 89.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[228](#bib.bib228)] | Integrated | 2015 | 80.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Ma et al. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[229](#bib.bib229)] | Integrated | 2016 | 95.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Performance comparison for 2D multi-person pose tracking on PoseTrack2017
    and PoseTrack2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Category | Year |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 2017 Testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MOTA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2017 Validation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MOTA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2018 Testing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MOTA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2018 Validation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MOTA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Detect-and-Track |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[130](#bib.bib130)] | Top-down | 2018 | 51.8 | 55.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Pose Flow |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[129](#bib.bib129)] | Top-down | 2018 | 51.0 | 58.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Flow Track |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | Top-down | 2018 | 57.8 | 65.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Fastpose |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[230](#bib.bib230)] | Top-down | 2019 | 57.4 | 63.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LightTrack |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[231](#bib.bib231)] | Top-down | 2020 | 58.0 | - | - | 64.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Umer et al. |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[232](#bib.bib232)] | Top-down | 2020 | 60.0 | 68.3 | 60.7 | 69.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Clip Tracking |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[131](#bib.bib131)] | Top-down | 2020 | 64.1 | 71.6 | 64.3 | 68.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[233](#bib.bib233)] | Top-down | 2021 | - | 73.4 | - | 69.2 |'
  prefs: []
  type: TYPE_TB
- en: '| AlphaPose |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[132](#bib.bib132)] | Top-down | 2022 | - | 65.7 | - | 64.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GatedTrack |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[234](#bib.bib234)] | Top-down | 2023 | - | - | - | 64.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Posetrack |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[235](#bib.bib235)] | Bottom-up | 2017 | 48.4 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Raaj et al. |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[236](#bib.bib236)] | Bottom-up | 2019 | 53.8 | 62.7 | - | 60.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[138](#bib.bib138)] | Bottom-up | 2019 | - | 71.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Performance comparison for 3D multi-person pose tracking on CMU Panoptic
    and Campus dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Category | Year |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CMU &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MOTA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Campus &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PCP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Bridgeman et al. |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[237](#bib.bib237)] | Multi-stage | 2019 | - | 92.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Tessetrack |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[240](#bib.bib240)] | One-stage | 2021 | 94.1 | 97.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Voxeltrack |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[241](#bib.bib241)] | One-stage | 2022 | 98.5 | 96.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Snipper |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[242](#bib.bib242)] | One-stage | 2023 | 93.4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| TEMPO |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] | One-stage | 2023 | 98.4 | - |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Pose tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section reviews the datasets for pose tracking and also compares different
    methods on some datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table  [8](#S5.T8 "Table 8 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") summarizes the datasets, with a focus on the Campus,
    CMP Panoptic, and PoseTrack datasets, which are highly cited and frequently used
    for evaluating multi-person pose tracking. These datasets are preferred because
    multi-person poses are more representative of real-world scenarios. In the earlier
    stage, VideoPose2.0 was often applied for single-person pose tracking. The PoseTrack
    dataset has been discussed in Section [5.1.1](#S5.SS1.SSS1 "5.1.1 Datasets for
    2D pose estimation ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey"). In
    the following, we only review other three datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: The VideoPose2.0 dataset [[343](#bib.bib343)] is a video dataset for tracking
    the poses of upper and lower arms. The videos were collected from TV shows ”Friends”
    and ”Lost” and are normally with a single actor and a variety of movements. This
    dataset includes 44 videos, each lasting 2-3 seconds, totaling 1,286 frames. Each
    frame is hand-annotated with joint locations. This dataset is an extension of
    the VideoPose dataset [[346](#bib.bib346)], but more challenging since about 30%
    of lower arms are significantly foreshortened.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CMU Panoptic Dataset [[341](#bib.bib341)] was created by capturing subjects
    engaged in social interactions using the camera system with 480 views. Subjects
    were engaged in different games: Ultimatum (with 3 subjects), Prisoner’s dilemma
    (with 8 subjects), Mafia (with 8 subjects), Haggling (with 3 subjects), and 007-bang
    game (with 5 subjects). The number of subjects in each game varies from three
    to eight. In total, this dataset consists of 65 videos and 1.5 million 3D poses
    estimated using Kinects. It is often used for evaluating multi-person 3D pose
    estimation and pose tracking methods.'
  prefs: []
  type: TYPE_NORMAL
- en: The Campus Dataset [[347](#bib.bib347)] was collected by capturing interactions
    among three individuals in an outdoor environment using 3 cameras. It contains
    6,000 frames including 3 views, and each view provides 2,000 frames. It is widely
    used for 3D multi-person pose estimation and tracking. Due to a small number of
    cameras and wide baseline views, it is challenging for pose tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Performance comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [9](#S5.T9 "Table 9 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey") and Table [10](#S5.T10 "Table 10 ‣ 5.1.3 Performance
    comparison ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey") respectively show
    the comparison of 2D pose tracking methods. For 2D single-person pose tracking,
    integrated methods jointly optimize pose estimation and pose tracking within a
    unified framework, leveraging the benefits of each to achieve better results.
    From Table [9](#S5.T9 "Table 9 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation
    ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey"), it can be observed that one of the integrated
    methods [[229](#bib.bib229)] exhibits state-of-the-art performance. For 2D multi-person
    pose tracking, most methods follow the top-down strategy by well-estimated poses
    of single-person estimation technique. Undoubtedly, these methods outperform bottom-up
    ones about 2-15% of MOTA scores on the Posetrack2017 and 2018 datasets. Regarding
    3D multi-person pose tracking, there are currently fewer existing works. Among
    them, one-stage methods perform better than multi-stage methods shown in Table [11](#S5.T11
    "Table 11 ‣ 5.1.3 Performance comparison ‣ 5.1 Pose estimation ‣ 5 Benchmark datasets
    ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep Learning:
    A Survey"), and Voxeltrack  [[241](#bib.bib241)] achieves the best results. This
    is because one-stage methods jointly estimate and link 3D poses, which can propagate
    the errors of sub-tasks in the multi-stage methods back to the input image pixels
    of videos.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Action recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section reviews the datasets that are more commonly used for pose-based
    action recognition and also compares different categories of the methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 12: A review of human action recognition datasets. C: Colour, D: Depth,
    S: Skeleton, I: Infrared frame; LOSubO: Leave One Subject Out, CS: Cross Subject,
    CV: Cross Validation; tr: training, va: validation, te: test'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | Citation | Modality | Sensors | #Actions | #Subjects | #Samples
    | Protocol |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HDM05 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[348](#bib.bib348)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2007 | 503 | C,D,S | RRM | 130 | 5 | 2317 | 10-fold CV |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSR-Action3D &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[349](#bib.bib349)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2010 | 1736 | D,S | Kinect | 20 | 10 | 557 | CS(1/3 tr; 2/3 tr; half tr,
    half te) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSRC-12 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[350](#bib.bib350)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2012 | 494 | S | Kinect | 12 | 30 | 6244 | LOSubO |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; G3D &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[351](#bib.bib351)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2012 | 262 | C,D,S | Kinect | 20 | 10 | 659 | CS(4 tr, 1 va, 5 te) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SBU Kinect &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[352](#bib.bib352)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2012 | 575 | C,D,S | Kinect | 8 | 7 | 300 | 5-fold CV |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UTKinect-Action3D &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[353](#bib.bib353)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2012 | 1716 | C,D,S | Kinect | 10 | 10 | 200 | LOSubO |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Northwestern-UCLA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[354](#bib.bib354)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 | 497 | C,D,S | Kinect | 10 | 10 | 1494 | LOSubO; cross view(2 tr, 1
    te) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UTD-MHAD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[355](#bib.bib355)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2015 | 706 | C,D,S,I | Kinect | 27 | 8 | 861 | CS(odd tr, even te) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SYSU &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[356](#bib.bib356)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2015 | 594 | C,D,S | Kinect | 12 | 40 | 480 | CS(half tr, half te) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NTU-RGB+D &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[274](#bib.bib274)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 | 2452 | C,D,S,I | Kinect | 60 | 40 | 56880 | CS(half tr, half te);
    cross view(half tr, half te) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PKU-MMD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[357](#bib.bib357)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | 195 | C,D,S,I | Kinect | 51 | 66 | 1076 | CS(57 tr, 9 te); cross view(2
    tr, 1 te) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Kinetics &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[358](#bib.bib358)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 | 3402 | C,S | YouTube | 400 | - | 306245 | CV(250-1000 tr, 50 va, 100
    te per action) |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NTU RGB+D 120 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[359](#bib.bib359)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 | 907 | C,D,S,I | Kinect | 120 | 106 | 114480 | CS(half tr, half te);
    cross view(half tr, half te) |'
  prefs: []
  type: TYPE_TB
- en: 5.3.1 Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Section [4](#S4 "4 Action Recognition ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey"), we have reviewed the pose-based
    action recognition methods which can be divided into estimated pose-based and
    skeleton-based action recognition. The former one applies RGB data and the latter
    one directly uses skeleton data as the input. Table [12](#S5.T12 "Table 12 ‣ 5.3
    Action recognition ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking
    and Action Recognition with Deep Learning: A Survey") summaries the large-scale
    datasets that are prevalent in deep learning-based action recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NTU RGB+D dataset [[274](#bib.bib274)] was constructed by Nanyang Technological
    University, Singapore. Four modalities were collected using Mincrosoft Kinect
    v2 sensor including RGB, depth maps, skeletons and infrared frames. The dataset
    consists of 60 actions performed by 40 subjects. The actions can be divided into
    three groups including: 40 daily actions, 9 health-related actions and 11 person-person
    interaction actions. The age range of the subjects is from 10 to 35 years and
    each subject performs an action for several times. In total, there are 56880 samples
    which are captured in 80 distinct camera views. The large amount of variation
    in subjects and views makes it possible to have more cross-subject and cross-view
    evaluations for action recognition methods.'
  prefs: []
  type: TYPE_NORMAL
- en: NTU RGB+D 120 dataset [[359](#bib.bib359)] is an extension of the NTU RGB+D
    dataset [[274](#bib.bib274)]. An additional 60 action categories performed by
    another 66 subjects including 57,600 samples were added to the NTU RGB+D dataset.
    This dataset also provides four modalities including RGB, depth maps, skeletons
    and infrared frames. More number of actions, subjects and samples enable it more
    challenging than NTU RGB+D dataset in action recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 13: Performance of estimated pose-based action recognition methods on
    three datasets for showing the benefits of pose estimation or tracking for recognition.
    GT: ground-truth.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Highlights | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| JHMDB | PoTion | estimated poses | 58.5$\pm$1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GT poses | 62.1$\pm$1.1 |'
  prefs: []
  type: TYPE_TB
- en: '| [[251](#bib.bib251)] | GT poses + crop | 67.9$\pm$2.4 |'
  prefs: []
  type: TYPE_TB
- en: '| AVA | LART | -poses-tracking | 40.2 |'
  prefs: []
  type: TYPE_TB
- en: '| -poses | 41.4 |'
  prefs: []
  type: TYPE_TB
- en: '| [[28](#bib.bib28)] | full model | 42.3 |'
  prefs: []
  type: TYPE_TB
- en: '| NTU60 | UPS | separate training | 89.6 |'
  prefs: []
  type: TYPE_TB
- en: '| [[259](#bib.bib259)] | joint training | 92.6 |'
  prefs: []
  type: TYPE_TB
- en: PKU-MMD dataset [[357](#bib.bib357)] is a large-scale multi-modality dataset
    for action detection and recognition tasks. Four modalities including RGB, depth
    maps, skeletons and infrared frames were captured by Mincrosoft Kinect v2 sensor.
    This dataset consists of 1,076 videos composed of 51 actions which are performed
    by 66 subjects in 3 views. The action classes cover 41 daily actions and 10 person-person
    interaction actions. Each video contains more than twenty action samples. In total,
    this dataset includes 3,000 minutes and 5,400,000 frames. The large amount of
    actions in one untrimmed video makes the robustness of action detection methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kinetics-Skeleton dataset [[358](#bib.bib358)] is an extra large-scale action
    dataset captured by searching RGB videos from YouTube and generating skeletons
    by OpenPose. It has 400 actions, with 400-1150 clips for each action, each from
    a unique YouTube video. Each clip lasts around 10 seconds. The total number of
    video samples is 306,245\. The action classes include: person actions, person-person
    actions and person-object actions. Due to the source of YouTube, the videos are
    not as professional as the ones recorded in experimental background. Therefore,
    the dataset has considerable camera motion, illumination variations, shadows,
    background clutter and a large variety of subjects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 14: Performance comparison of action recognition methods on NTU RGB+D
    and NTU RGB+D 120 datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Category | Sub-category | Year |'
  prefs: []
  type: TYPE_TB
- en: '&#124; NTU RGB + D 60 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C-Sub      C-Set &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NTU RGB + D 120 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C-Sub      C-Set &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Zolfaghari et al. [[250](#bib.bib250)] | Estimated Pose-based | two-stage
    strategy | 2017 | 80.8 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[252](#bib.bib252)] | Estimated Pose-based | two-stage strategy
    | 2018 | 91.7 | 95.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| IntegralAction [[253](#bib.bib253)] | Estimated Pose-based | two-stage strategy
    | 2021 | 91.7 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PoseConv3D [[255](#bib.bib255)] | Estimated Pose-based | two-stage strategy
    | 2021 | 94.1 | 97.1 | 86.9 | 90.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Luvizonet al. [[258](#bib.bib258)] | Estimated Pose-based | one-stage strategy
    | 2018 | 85.5 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| UPS [[259](#bib.bib259)] | Estimated Pose-based | one-stage strategy | 2023
    | 92.6 | 97.0 | 89.3 | 91.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 Layere P-LSTM [[274](#bib.bib274)] | RNN-based | spatial division of human
    body | 2016 | 62.9 | 70.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Trust Gate ST-LSTM [[280](#bib.bib280)] | RNN-based | spatial and/or temporal
    networks | 2016 | 69.2 | 77.7 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Two-stream RNN [[244](#bib.bib244)] | RNN-based | spatial and/or temporal
    networks | 2017 | 71.3 | 79.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[281](#bib.bib281)] | RNN-based | spatial and/or temporal networks
    | 2017 | 70.3 | 82.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SR-TSL [[282](#bib.bib282)] | RNN-based | spatial and/or temporal networks
    | 2018 | 84.8 | 92.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| GCA-LSTM [[276](#bib.bib276)] | RNN-based | attention mechanism | 2017 |
    74.4 | 82.8 | 58.3 | 59.2 |'
  prefs: []
  type: TYPE_TB
- en: '| STA-LSTM [[277](#bib.bib277)] | RNN-based | attention mechanism | 2018 |
    73.4 | 81.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| EleAtt-GRU [[278](#bib.bib278)] | RNN-based | attention mechanism | 2019
    | 80.7 | 88.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2s AGC-LSTM [[279](#bib.bib279)] | RNN-based | attention mechanism | 2019
    | 89.2 | 95.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| JTM [[261](#bib.bib261)] | CNN-based | 2D CNN | 2017 | 73.4 | 75.2 | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| JDM [[263](#bib.bib263)] | CNN-based | 2D CNN | 2017 | 76.2 | 82.3 | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[264](#bib.bib264)] | CNN-based | 2D CNN | 2017 | 80.0 | 87.2
    | 60.3 | 63.2 |'
  prefs: []
  type: TYPE_TB
- en: '| SkeletonNet [[265](#bib.bib265)] | CNN-based | 2D CNN | 2017 | 75.9 | 81.2
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Ke et al. [[268](#bib.bib268)] | CNN-based | 2D CNN | 2017 | 79.6 | 86.8
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[360](#bib.bib360)] | CNN-based | 2D CNN | 2017 | 85.0 | 92.3
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Ding et al. [[267](#bib.bib267)] | CNN-based | 2D CNN | 2017 | - | 82.3 |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[266](#bib.bib266)] | CNN-based | 2D CNN | 2017 | 82.8 | 90.1
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TSRJI [[245](#bib.bib245)] | CNN-based | 2D CNN | 2019 | 73.3 | 80.3 | 65.5
    | 59.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SkeletonMotion [[245](#bib.bib245)] | CNN-based | 2D CNN | 2019 | 76.5 |
    84.7 | 67.7 | 66.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3SCNN [[269](#bib.bib269)] | CNN-based | 2D CNN | 2019 | 88.6 | 93.7 | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| DM-3DCNN [[271](#bib.bib271)] | CNN-based | 3D CNN | 2017 | 82.0 | 89.5 |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ST-GCN [[246](#bib.bib246)] | GCN-based | static method | 2018 | 81.5 | 88.3
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| STIGCN [[283](#bib.bib283)] | GCN-based | static method | 2020 | 90.1 | 96.1
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MS-G3D [[284](#bib.bib284)] | GCN-based | static method | 2020 | 91.5 | 96.2
    | 86.9 | 88.4 |'
  prefs: []
  type: TYPE_TB
- en: '| CA-GCN [[285](#bib.bib285)] | GCN-based | static method | 2020 | 83.5 | 91.4
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| AS-GCN [[286](#bib.bib286)] | GCN-based | dynamic method | 2018 | 86.8 |
    94.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2s-AGCN [[287](#bib.bib287)] | GCN-based | dynamic method | 2020 | 88.5 |
    95.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SGN [[301](#bib.bib301)] | GCN-based | dynamic method | 2020 | 89.0 | 94.5
    | 79.2 | 81.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 4s Shift-GCN [[288](#bib.bib288)] | GCN-based | dynamic method | 2020 | 90.7
    | 96.5 | 85.9 | 87.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DC-GCN+ADC [[361](#bib.bib361)] | GCN-based | dynamic method | 2020 | 90.8
    | 96.6 | 86.5 | 88.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DDGCN [[289](#bib.bib289)] | GCN-based | dynamic method | 2020 | 91.1 | 97.1
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamic GCN [[302](#bib.bib302)] | GCN-based | dynamic method | 2020 | 91.5
    | 96.0 | 87.3 | 88.6 |'
  prefs: []
  type: TYPE_TB
- en: '| CTR-GCN [[290](#bib.bib290)] | GCN-based | dynamic method | 2021 | 92.4 |
    96.8 | 88.9 | 90.6 |'
  prefs: []
  type: TYPE_TB
- en: '| InfoGCN [[291](#bib.bib291)] | GCN-based | dynamic method | 2021 | 93.0 |
    97.1 | 89.8 | 91.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DG-STGCN [[292](#bib.bib292)] | GCN-based | dynamic method | 2022 | 93.2
    | 97.5 | 89.6 | 91.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TCA-GCN [[293](#bib.bib293)] | GCN-based | dynamic method | 2022 | 92.8 |
    97.0 | 89.4 | 90.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ML-STGNet [[298](#bib.bib298)] | GCN-based | dynamic method | 2023 | 91.9
    | 96.2 | 88.6 | 90.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MV-IGNet [[303](#bib.bib303)] | GCN-based | dynamic method | 2023 | 89.2
    | 96.3 | 83.9 | 85.6 |'
  prefs: []
  type: TYPE_TB
- en: '| S-GDC [[304](#bib.bib304)] | GCN-based | dynamic method | 2023 | 88.6 | 94.9
    | 85.2 | 86.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Motif-GCN+TBs [[294](#bib.bib294)] | GCN-based | dynamic method | 2023 |
    90.5 | 96.1 | 87.1 | 87.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 3s-ActCLR [[295](#bib.bib295)] | GCN-based | dynamic method | 2023 | 84.3
    | 88.8 | 74.3 | 75.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GSTLN [[297](#bib.bib297)] | GCN-based | dynamic method | 2023 | 91.9 | 96.6
    | 88.1 | 89.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 4s STF-Net [[300](#bib.bib300)] | GCN-based | dynamic method | 2023 | 91.1
    | 96.5 | 86.5 | 88.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LA-GCN [[305](#bib.bib305)] | GCN-based | dynamic method | 2023 | 93.5 |
    97.2 | 90.7 | 91.8 |'
  prefs: []
  type: TYPE_TB
- en: '| DSTA-Net [[309](#bib.bib309)] | Transformer-based | pure Transformer | 2020
    | 91.5 | 96.4 | 86.6 | 89.0 |'
  prefs: []
  type: TYPE_TB
- en: '| STAR [[313](#bib.bib313)] | Transformer-based | pure Transformer | 2021 |
    83.4 | 89.0 | 78.3 | 80.2 |'
  prefs: []
  type: TYPE_TB
- en: '| STST [[312](#bib.bib312)] | Transformer-based | pure Transformer | 2021 |
    91.9 | 96.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| IIP-Former [[310](#bib.bib310)] | Transformer-based | pure Transformer |
    2022 | 92.3 | 96.4 | 88.4 | 89.7 |'
  prefs: []
  type: TYPE_TB
- en: '| RSA-Net [[314](#bib.bib314)] | Transformer-based | pure Transformer | 2023
    | 91.8 | 96.8 | 88.4 | 89.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ST-TR [[247](#bib.bib247)] | Transformer-based | hybrid Transformer | 2021
    | 89.9 | 96.1 | 81.9 | 84.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Zoom Transformer [[318](#bib.bib318)] | Transformer-based | hybrid Transformer
    | 2022 | 90.1 | 95.3 | 84.8 | 86.5 |'
  prefs: []
  type: TYPE_TB
- en: '| KA-AGTN [[320](#bib.bib320)] | Transformer-based | hybrid Transformer | 2022
    | 90.4 | 96.1 | 86.1 | 88.0 |'
  prefs: []
  type: TYPE_TB
- en: '| STTFormer [[316](#bib.bib316)] | Transformer-based | hybrid Transformer |
    2022 | 92.3 | 96.5 | 88.3 | 89.2 |'
  prefs: []
  type: TYPE_TB
- en: '| FG-STFormer [[319](#bib.bib319)] | Transformer-based | hybrid Transformer
    | 2022 | 92.6 | 96.7 | 89.0 | 90.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GSTN [[362](#bib.bib362)] | Transformer-based | hybrid Transformer | 2022
    | 91.3 | 96.6 | 86.4 | 88.7 |'
  prefs: []
  type: TYPE_TB
- en: '| IGFormer [[321](#bib.bib321)] | Transformer-based | hybrid Transformer |
    2022 | 93.6 | 96.5 | 85.4 | 86.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3Mformer [[306](#bib.bib306)] | Transformer-based | hybrid Transformer |
    2023 | 94.8 | 98.7 | 92.0 | 93.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SkeleTR [[322](#bib.bib322)] | Transformer-based | hybrid Transformer | 2023
    | 94.8 | 97.7 | 87.8 | 88.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GL-Transformer [[323](#bib.bib323)] | Transformer-based | unsupervised Transformer
    | 2022 | 76.3 | 83.8 | 66.0 | 68.7 |'
  prefs: []
  type: TYPE_TB
- en: '| HiCo-LSTM [[324](#bib.bib324)] | Transformer-based | unsupervised Transformer
    | 2023 | 81.4 | 88.8 | 73.7 | 74.5 |'
  prefs: []
  type: TYPE_TB
- en: '| HaLP+CMD [[325](#bib.bib325)] | Transformer-based | self-supervised Transformer
    | 2023 | 82.1 | 88.6 | 72.6 | 73.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SkeAttnCLR [[328](#bib.bib328)] | Transformer-based | self-supervised Transformer
    | 2023 | 82.0 | 86.5 | 77.1 | 80.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SkeletonMAE [[327](#bib.bib327)] | Transformer-based | self-supervised Transformer
    | 2023 | 86.6 | 92.9 | 76.8 | 79.1 |'
  prefs: []
  type: TYPE_TB
- en: 5.3.2 Performance comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [14](#S5.T14 "Table 14 ‣ 5.3.1 Datasets ‣ 5.3 Action recognition ‣
    5 Benchmark datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey"), we compare the results of different action recognition
    methods on two prominent datasets. Estimated poses-based methods apply RGB data
    as the input, and the best performance [[255](#bib.bib255), [259](#bib.bib259)]
    is lower than the ones [[306](#bib.bib306)] used skeletons as the input on two
    datasets (especially the larger one). This is reasonable because some facts (eg.
    illumination, background) could affect the performance when using RGB. In particular,
    methods based on one-stage strategy jointly address pose estimation and action
    recognition, thus reducing the errors of intermediate steps and generally achieving
    better results than the methods based on a two-stage strategy. Moreover, Table
    [13](#S5.T13 "Table 13 ‣ 5.3.1 Datasets ‣ 5.3 Action recognition ‣ 5 Benchmark
    datasets ‣ Human Pose-based Estimation, Tracking and Action Recognition with Deep
    Learning: A Survey") illustrates the effects of pose estimation (PE) and tracking
    on action recognition (AR). It can be easily seen that pose estimation and tracking
    results can improve the performance of action recognition, which further emphasizes
    the relationship of these three tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: For the skeleton-based methods, the recent methods mainly apply GCN and Transformer,
    consistently outperforming CNN and RNN-based methods. This improvement demonstrate
    the benefit of local and global feature learning based on GCN and Transformer
    for action recognition. Specifically, dynamic GCN-based methods generally perform
    better than static GCN-based ones due to stronger generalization capabilities.
    Hybrid Transformer-based methods outperform pure Transformer-based ones on large
    datasets since integrating the Transformer with GCN or CNN can better learn both
    local and global features. Specifically, the method [[306](#bib.bib306)] of applying
    transformer encoder on hypergraph achieved the best performance on two datasets,
    which provides a hint of representing actions using hypergraph for classification.
    It is also worth noting that the method [[305](#bib.bib305)] based on the guidance
    of natural language respectively achieves pretty good performance on two datasets,
    which implies the advantage of incorporating linguistic context for action recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper has reviewed recent deep learning-based approaches for pose estimation,
    tracking and action recognition. It also includes a discussion of commonly used
    datasets and a comparative analysis of various methods. Despite the the remarkable
    successes in these domains, there are still some challenges and corresponding
    research directions to promote advances for the three tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Pose estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are five main challenges for the pose estimation task as follows.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Occlusion
  prefs: []
  type: TYPE_NORMAL
- en: Although the current methods have achieved outstanding performance on public
    datasets, they still suffer from the occlusion problem. Occlusion results in unreliable
    human detection and declined performance for pose estimation. Person detectors
    in top-down approaches may fail in identifying the boundaries of overlapped human
    bodies and body part association for occluded scenes may fail in bottom-up approaches.
    Mutual occlusion in crowd scenarios caused largely declined performance for current
    3D HPE methods.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this problem, some methods [[363](#bib.bib363), [364](#bib.bib364),
    [365](#bib.bib365)] have been proposed based on multi-view learning. This is because
    the occluded part in one view may become visible in other views. However, these
    methods often need large memory and expensive computation costs, especially for
    3D MPPE under multi-view. Moreover, some methods based on multi-modal learning
    have also been demonstrated for robustness to occlusion, which could extract enrich
    features from different sensing modalities such as depth [[366](#bib.bib366)]
    and wearable inertial measurement units [[367](#bib.bib367)]. When applying pose
    estimation from different modalities, it may face another problem of few available
    datasets with different modalities. With the development of vision-language models,
    texts could provide semantics for pose estimation and also be easily generated
    by GPT, thus a better direction for another modality. Based on pose semantics,
    the occluded part can be inferred. With regard the semantics, human-scene relationships
    can also provide some semantic cues such as a person cannot be simultaneously
    present in the locations of other objects in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Low resolution
  prefs: []
  type: TYPE_NORMAL
- en: In the real-word application, low-resolution images or videos are often captured
    due to wide-view cameras, long-distance shooting capturing devices and so on.
    Obscured persons also exist due to environmental shadows. The current methods
    are usually trained on high-resolution input, which may cause low accuracy when
    applying them to low-resolution input. One solution for estimating poses from
    low-resolution input is to recover image resolution by applying super-resolution
    methods as image pre-processing. However, the optimization of super-resolution
    does not contribution to high-level human pose analysis. Wang et al. [[368](#bib.bib368)]
    observed that low-resolution would exaggerate the degree of quantization error,
    thus offset modeling may be helpful for pose estimation with low-resolution input.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Computation complexity
  prefs: []
  type: TYPE_NORMAL
- en: 'As reviewed in Section [2](#S2 "2 Pose estimation ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey"), many methods have
    been proposed for solving computation complexity. For example, one-stage methods
    for image-based MPPE are proposed to save the increased time consumption caused
    by intermediate steps. Sample frames-based methods for video-based pose estimation
    are proposed to reduce the complexity of processing each frame. However, such
    one-stage methods may sacrifice accuracy when improving efficiency (eg. the recent
    ED-pose network [[65](#bib.bib65)] takes the shortest time and would sacrifice
    about %4 AP on CoCO val2017 dataset). Therefore, more effort into one-stage methods
    for MPPE is required to achieve computationally efficient pose estimation while
    maintaining high accuracy. Sample frames-based methods [[127](#bib.bib127)] estimate
    poses based on three steps, which still results in more time consumption. Hence,
    an end-to-end network is preferred to incorporate with sample frames-based methods
    for video-based pose estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based architectures for video-based 3D pose estimation inevitably
    incur high computational costs. This is because that they typically regard each
    video frame as a pose token and apply extremely long video frames to achieve advanced
    performance. For instance, Strided [[206](#bib.bib206)] and Mhformer [[207](#bib.bib207)]
    require 351 frames, and MixSTE [[207](#bib.bib207)] and DSTformer [[222](#bib.bib222)]
    require 243 frames. Self-attention complexity increases quadratically with the
    number of tokens. Although directly reducing the frame number can reduce the cost,
    it may result in lower performance due to a small temporal receptive field. Therefore,
    it is preferable to design an efficient architecture while maintaining a large
    temporal receptive field for accurate estimation. Considering that similar tokens
    may exist in deep transformer blocks [[369](#bib.bib369)], one potential solution
    is to prune pose tokens to improve the efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: (4) Limited data for uncommon poses
  prefs: []
  type: TYPE_NORMAL
- en: The current public datasets have limited training data for uncommon poses (eg.
    falling), which results in model bias and further low accuracy on such poses.
    Data augmentation [[370](#bib.bib370), [371](#bib.bib371)] for uncommon poses
    is a common method for generating new samples with more diversity. Optimization-based
    methods [[372](#bib.bib372)] can mitigate the impact of domain gaps, by estimating
    poses case-by-case rather than learning. Therefore, deep-learning-based method
    combining optimization techniques might be helpful for uncommon pose estimation.
    Moreover, open vocabulary learning can be also applied to estimating uncommon
    poses by the semantic relationship between these poses with other common poses.
  prefs: []
  type: TYPE_NORMAL
- en: (5) High uncertainty of 3D poses
  prefs: []
  type: TYPE_NORMAL
- en: Predicting 3D poses from 2D poses is required to handle uncertainty and indeterminacy
    due to depth ambiguity and potential occlusion. However, most of the existing
    methods [[210](#bib.bib210)] belong to deterministic methods which aim to construct
    single and definite 3D poses from images. Therefore, how to handle uncertainty
    and indeterminacy of poses remains an open question. Inspired by the strong capability
    of diffusion models to generate samples with high uncertainty, applying diffusion
    models is a promising direction for pose estimation. Few methods [[373](#bib.bib373),
    [209](#bib.bib209), [137](#bib.bib137)] have been recently proposed by formulating
    3D pose estimation as a reverse diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Pose tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most pose tracking methods follow pose estimation and linking strategy, pose
    tracking performance highly depends on the results of pose estimation. Therefore,
    some challenges of pose estimation also exist in pose tracking, such as occlusion.
    Multi-view features fusion [[241](#bib.bib241)] is one method of eliminating unreliable
    appearances by occlusion for improving the results of pose linking. Linking every
    detection box rather than only high score detection boxes [[374](#bib.bib374)]
    is another method to make up non-negligible true poses by occlusion. In the following,
    we will present some more challenges for pose tracking.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Multi-person pose tracking under multiple cameras
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge is how to fuse the scenes of different views. Although Voxteltrack [[241](#bib.bib241)]
    tends to fuse multi-view features fusion, it would be researched more. If scenes
    from non-overlapping cameras are fused and projected in a virtual world, poses
    can be tracked in a long area continuously.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Similar appearance and diverse motion
  prefs: []
  type: TYPE_NORMAL
- en: To link poses across frames, the general solution is to measure the similarity
    between every pair of poses in neighboring frames based on appearance and motion.
    Persons sometimes have uniform appearance and diverse motions at the same time,
    such as group dancers, and sports players. They are highly similar and almost
    undistinguished in appearance by uniform clothes, and in complicated motion and
    interaction patterns. In this case, measuring the similarity is challenging. However,
    such poses with similar appearance can be easily distinguished by textual semantics.
    One possible solution is to incorporate some multi-modality pre-training models,
    such as Contrastive Language-Image Pre-training (CLIP) [[375](#bib.bib375)], for
    measuring similarity based on their semantic representation.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Fast camera motion
  prefs: []
  type: TYPE_NORMAL
- en: Existing methods mainly address pose tracking by assuming slow camera motion.
    However, fast camera motion with ego-camera capturing is very often in real-world
    application. How to address egocentric pose tracking with fast camera motion is
    a challenging problem. Khirodkar et al. [[376](#bib.bib376)] proposed a new benchmark
    (EgoHumans) for egocentric pose estimation and tracking, and designed a multi-stream
    transformer to track multiple persons. Experiments have shown that there is still
    a gap between the performance of static and dynamic capture systems due to camera
    synchronization and calibration. More effort can be made to bridge the gap.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Action recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the rapid advancement of deep learning techniques, promise results have
    been achieved on large-scale action datasets. There are still some open questions
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: (1) Computation complexity
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the performance comparison (Table [14](#S5.T14 "Table 14 ‣ 5.3.1
    Datasets ‣ 5.3 Action recognition ‣ 5 Benchmark datasets ‣ Human Pose-based Estimation,
    Tracking and Action Recognition with Deep Learning: A Survey")) of different methods,
    the method of integrating transformer with GCNs achieves the best accuracy. However,
    as mentioned before the computation required for a transformer and the amount
    of memory required increases on a quadratic scale with the number of tokens [[377](#bib.bib377)].
    Therefore, how to select significant tokens from video frames or skeletons is
    an open question for efficient transformer-based action recognition. Similar to
    transformer-based pose estimation, pruning tokens or discarding input matches [[378](#bib.bib378)]
    tend to reduce the cost. Moreover, integrating lightweight GCNs [[379](#bib.bib379)]
    can be further beneficial for efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: (2) Zero-shot learning on skeletons
  prefs: []
  type: TYPE_NORMAL
- en: Annotating and labeling large-amount data is expensive, and zero-shot learning
    is desirable in real-world applications. Existing zero-shot action recognition
    methods mainly apply RGB data as the input. However, skeleton data has become
    a promising alternative to RGB data due to its robustness to variations in appearance
    and background. Therefore, zero-shot skeleton-based action recognition is more
    desirable. Few methods [[380](#bib.bib380), [381](#bib.bib381)] were proposed
    to learn a mapping between skeletons and word embedding of class labels. Class
    labels may possess less semantics than textual descriptions which are natural
    languages for describing how an action is performed. In the future, new methods
    can be pursued based on textual descriptions for zero-shot skeleton-based action
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Multi-modality fusion
  prefs: []
  type: TYPE_NORMAL
- en: Estimated pose-based methods take RGB data as the input and recognize actions
    based on RGB and estimated skeletons. Moreover, text data can guide improving
    the performance of visually similar actions and zero-shot learning, which is another
    modality for action recognition. Due to the heterogeneity of different modalities,
    how to fully utilize them deserves to be further explored by researchers. Although
    some methods [[255](#bib.bib255)] tend to propose a particular model for fusing
    different modalities, such model lacks of generalization. In the future, a universal
    fusing method regardless of models is a better option.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Unified models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As reviewed in Section [4.1](#S4.SS1 "4.1 Estimated pose-based action recognition
    ‣ 4 Action Recognition ‣ Human Pose-based Estimation, Tracking and Action Recognition
    with Deep Learning: A Survey"), some methods tend to conduct action recognition
    based on results of pose estimation or tracking. Table [13](#S5.T13 "Table 13
    ‣ 5.3.1 Datasets ‣ 5.3 Action recognition ‣ 5 Benchmark datasets ‣ Human Pose-based
    Estimation, Tracking and Action Recognition with Deep Learning: A Survey") further
    demonstrates pose estimation and tracking can improve action recognition performance.
    These observations emphasize these three tasks are closely related together, which
    provides a direction for designing unified models for solving three tasks. Recently,
    a unified model (UPS [[259](#bib.bib259)]) has been proposed for 3D video-based
    pose estimation and estimated poses-based action recognition, however, their performance
    is largely lower than the ones of separate models. Hence, more unified models
    are preferable for jointly solving these three tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This survey has presented a systematic overview of recent works about human
    pose-based estimation, tracking and action recognition with deep learning. We
    have reviewed pose estimation approaches from 2D to 3D, from single-person to
    multi-person, and from images to videos. After estimating poses, we summarized
    the methods of linking poses across frames for tracking poses. Pose-based action
    recognition approaches have been also reviewed which are taken as the application
    of pose estimation and tracking. For each task, we have reviewed different categories
    of methods and discussed their advantages and disadvantages. Meanwhile, end-to-end
    methods were highlighted for jointly conducting pose estimation, tracking and
    action recognition in the category of estimated pose-based action recognition.
    Commonly used datasets have been reviewed and performance comparisons of different
    methods have been covered to further demonstrate the benefits of some methods.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the strengths and weaknesses of the existing works, we point out a
    few promising future directions. For pose estimation, more effort can be made
    on pose estimation with occlusion, low resolution, limited data with uncommon
    poses and balancing the performance with computation complexity. Multi-person
    pose tracking can be further resolved under multiple cameras, similar appearance,
    diverse motions and fast camera motion. Zero-shot learning on skeletons and multi-modality
    fusion can be also further explored for action recognition.
  prefs: []
  type: TYPE_NORMAL
- en: \bmhead
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements This work is supported by the National Natural Science Foundation
    of China (Grant No. 62006211, 61502491) and China Postdoctoral Science Foundation
    (Grant No. 2019TQ0286, 2020M682349).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \bibcommenthead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gavrila [1999] Gavrila, D.M.: The visual analysis of human movement: A survey.
    CVIU 73(1), 82–98 (1999)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aggarwal and Cai [1999] Aggarwal, J.K., Cai, Q.: Human motion analysis: A review.
    CVIU 73(3), 428–440 (1999)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moeslund and Granum [2001] Moeslund, T.B., Granum, E.: A survey of computer
    vision-based human motion capture. CVIU 81(3), 231–268 (2001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2003] Wang, L., Hu, W., Tan, T.: Recent developments in human
    motion analysis. PR 36(3), 585–601 (2003)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moeslund et al. [2006] Moeslund, T.B., Hilton, A., Krüger, V.: A survey of
    advances in vision-based human motion capture and analysis. CVIU 104(2-3), 90–126
    (2006)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poppe [2007] Poppe, R.: Vision-based human motion analysis: An overview. CVIU
    108(1-2), 4–18 (2007)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sminchisescu [2008] Sminchisescu, C.: 3d human motion analysis in monocular
    video: techniques and challenges. Human Motion: Understanding, Modelling, Capture,
    and Animation, 185–211 (2008)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji and Liu [2009] Ji, X., Liu, H.: Advances in view-invariant human motion
    analysis: a review. IEEE Transactions on Systems, Man, and Cybernetics 40(1),
    13–24 (2009)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moeslund et al. [2011] Moeslund, T.B., Hilton, A., Krüger, V., Sigal, L.: Visual
    Analysis of Humans. Springer, ??? (2011)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2015] Liu, Z., Zhu, J., Bu, J., Chen, C.: A survey of human pose
    estimation: The body parts parsing based methods. JVCIR 32, 10–19 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarafianos et al. [2016] Sarafianos, N., Boteanu, B., Ionescu, B., Kakadiaris,
    I.A.: 3d human pose estimation: A review of the literature and analysis of covariates.
    CVIU 152, 1–20 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yilmaz et al. [2006] Yilmaz, A., Javed, O., Shah, M.: Object tracking: A survey.
    ACM CSUR 38(4), 13 (2006)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Watada et al. [2010] Watada, J., Musa, Z., Jain, L.C., Fulcher, J.: Human tracking:
    A state-of-art survey. In: International Conference on Knowledge-Based and Intelligent
    Information and Engineering Systems, pp. 454–463 (2010)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salti et al. [2012] Salti, S., Cavallaro, A., Di Stefano, L.: Adaptive appearance
    modeling for video tracking: Survey and evaluation. TIP 21(10), 4334–4348 (2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smeulders et al. [2013] Smeulders, A.W., Chu, D.M., Cucchiara, R., Calderara,
    S., Dehghan, A., Shah, M.: Visual tracking: An experimental survey. TPAMI 36(7),
    1442–1468 (2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2015] Wu, Y., Lim, J., Yang, M.-H.: Object tracking benchmark. TPAMI
    37(9), 1834–1848 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cedras and Shah [1995] Cedras, C., Shah, M.: Motion-based recognition a survey.
    IVT 13(2), 129–155 (1995)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turaga et al. [2008] Turaga, P., Chellappa, R., Subrahmanian, V.S., Udrea,
    O.: Machine recognition of human activities: A survey. TCSVT 18(11), 1473 (2008)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poppe [2010] Poppe, R.: A survey on vision-based human action recognition.
    IVT 28(6), 976–990 (2010)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo and Lai [2014] Guo, G., Lai, A.: A survey on still image based human action
    recognition. PR 47(10), 3343–3361 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2016] Zhu, F., Shao, L., Xie, J., Fang, Y.: From handcrafted to
    learned representations for human action recognition: a survey. IVT 55, 42–52
    (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2018] Wang, P., Li, W., Ogunbona, P., Wan, J., Escalera, S.: Rgb-d-based
    human motion recognition with deep learning: A survey. CVIU 171, 118–139 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2020] Chen, Y., Tian, Y., He, M.: Monocular human pose estimation:
    A survey of deep learning-based methods. Computer vision and image understanding
    192, 102897 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022] Liu, W., Bao, Q., Sun, Y., Mei, T.: Recent advances of monocular
    2d and 3d human pose estimation: A deep learning perspective. ACM Computing Surveys
    55(4), 1–41 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2022] Sun, Z., Ke, Q., Rahmani, H., Bennamoun, M., Wang, G., Liu,
    J.: Human action recognition from various data modalities: A review. TPAMI (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2023] Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J.,
    Kehtarnavaz, N., Shah, M.: Deep learning-based human pose estimation: A survey.
    ACM Computing Surveys 56(1), 1–37 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xin et al. [2023] Xin, W., Liu, R., Liu, Y., Chen, Y., Yu, W., Miao, Q.: Transformer
    for skeleton-based action recognition: A review of recent advances. Neurocomputing
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajasegaran et al. [2023] Rajasegaran, J., Pavlakos, G., Kanazawa, A., Feichtenhofer,
    C., Malik, J.: On the benefits of 3d pose and tracking for human action recognition.
    In: CVPR, pp. 640–649 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choudhury et al. [2023] Choudhury, R., Kitani, K., Jeni, L.A.: TEMPO: Efficient
    multi-view pose estimation, tracking, and forecasting. In: ICCV, pp. 14750–14760
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Toshev and Szegedy [2014] Toshev, A., Szegedy, C.: Deeppose: Human pose estimation
    via deep neural networks. In: CVPR, pp. 1653–1660 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carreira et al. [2016] Carreira, J., Agrawal, P., Fragkiadaki, K., Malik, J.:
    Human pose estimation with iterative error feedback. In: CVPR, pp. 4733–4742 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2017] Sun, X., Shang, J., Liang, S., Wei, Y.: Compositional human
    pose regression. In: ICCV, pp. 2602–2611 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luvizon et al. [2019] Luvizon, D.C., Tabia, H., Picard, D.: Human pose regression
    by combining indirect part detection and contextual information. Computers & Graphics
    85, 15–22 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. [2021] Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z.:
    Tfpose: Direct human pose estimation with transformers. arXiv preprint arXiv:2103.15320
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Li, K., Wang, S., Zhang, X., Xu, Y., Xu, W., Tu, Z.: Pose
    recognition with cascade transformers. In: CVPR, pp. 1944–1953 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. [2022] Mao, W., Ge, Y., Shen, C., Tian, Z., Wang, X., Wang, Z.,
    Hengel, A.v.: Poseur: Direct human pose regression with transformers. In: ECCV,
    pp. 72–88 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Panteleris and Argyros [2022] Panteleris, P., Argyros, A.: Pe-former: Pose
    estimation transformer. In: ICPRAI, pp. 3–14 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain et al. [2014] Jain, A., Tompson, J., Andriluka, M., Taylor, G.W., Bregler,
    C.: Learning human pose estimation features with convolutional networks. In: ICLR
    (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tompson et al. [2014] Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint
    training of a convolutional network and a graphical model for human pose estimation.
    In: NIPS, pp. 1799–1807 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen and Yuille [2014] Chen, X., Yuille, A.L.: Articulated pose estimation
    by a graphical model with image dependent pairwise relations. In: NIPS, pp. 1736–1744
    (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tompson et al. [2015] Tompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler,
    C.: Efficient object localization using convolutional networks. In: CVPR, pp.
    648–656 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2016] Wei, S.-E., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional
    pose machines. In: CVPR, pp. 4724–4732 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu and Ramanan [2016] Hu, P., Ramanan, D.: Bottom-up and top-down reasoning
    with hierarchical rectified gaussians. In: CVPR, pp. 5600–5609 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Newell et al. [2016] Newell, A., Yang, K., Deng, J.: Stacked hourglass networks
    for human pose estimation. In: ECCV, pp. 483–499 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bulat and Tzimiropoulos [2016] Bulat, A., Tzimiropoulos, G.: Human pose estimation
    via convolutional part heatmap regression. In: ECCV, pp. 717–732 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lifshitz et al. [2016] Lifshitz, I., Fetaya, E., Ullman, S.: Human pose estimation
    using deep consensus voting. In: ECCV, pp. 246–260 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chu et al. [2017] Chu, X., Yang, W., Ouyang, W., Ma, C., Yuille, A.L., Wang,
    X.: Multi-context attention for human pose estimation. In: CVPR, pp. 1831–1840
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2017] Yang, W., Li, S., Ouyang, W., Li, H., Wang, X.: Learning
    feature pyramids for human pose estimation. In: ICCV, pp. 1281–1290 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow et al. [2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial
    nets. In: NIPS, pp. 2672–2680 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2017] Chen, Y., Shen, C., Wei, X.-S., Liu, L., Yang, J.: Adversarial
    posenet: A structure-aware convolutional network for human pose estimation. In:
    ICCV, pp. 1212–1221 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. [2017] Ning, G., Zhang, Z., He, Z.: Knowledge-guided deep fractal
    neural networks for human pose estimation. IEEE Transactions on Multimedia 20(5),
    1246–1259 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2017] Sun, K., Lan, C., Xing, J., Zeng, W., Liu, D., Wang, J.:
    Human pose estimation using global and local normalization. In: ICCV, pp. 5599–5607
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marras et al. [2017] Marras, I., Palasek, P., Patras, I.: Deep globally constrained
    mrfs for human pose estimation. In: ICCV, pp. 3466–3475 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Ferrari [2017] Liu, B., Ferrari, V.: Active learning for human pose
    estimation. In: ICCV, pp. 4363–4372 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ke et al. [2018] Ke, L., Chang, M.-C., Qi, H., Lyu, S.: Multi-scale structure-aware
    network for human pose estimation. In: ECCV, pp. 713–728 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. [2018] Peng, X., Tang, Z., Yang, F., Feris, R.S., Metaxas, D.:
    Jointly optimize data augmentation and network training: Adversarial data augmentation
    in human pose estimation. In: CVPR, pp. 2226–2234 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. [2018] Tang, W., Yu, P., Wu, Y.: Deeply learned compositional models
    for human pose estimation. In: ECCV, pp. 190–206 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. [2018a] Nie, X., Feng, J., Zuo, Y., Yan, S.: Human pose estimation
    with parsing induced learner. In: CVPR, pp. 2100–2108 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. [2018b] Nie, X., Feng, J., Yan, S.: Mutual learning to adapt for
    joint human parsing and pose estimation. In: ECCV, pp. 502–517 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang and Wu [2019] Tang, W., Wu, Y.: Does learning specific features for related
    parts help human pose estimation? In: CVPR, pp. 1107–1116 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019] Zhang, F., Zhu, X., Ye, M.: Fast human pose estimation.
    In: CVPR, pp. 3517–3526 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022] Li, Y., Yang, S., Liu, P., Zhang, S., Wang, Y., Wang, Z.,
    Yang, W., Xia, S.-T.: SimCC: A simple coordinate classification perspective for
    human pose estimation. In: ECCV, pp. 89–106 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Li, J., Bian, S., Zeng, A., Wang, C., Pang, B., Liu, W., Lu,
    C.: Human pose regression with residual log-likelihood estimation. In: ICCV, pp.
    11025–11034 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. [2023] Ye, S., Zhang, Y., Hu, J., Cao, L., Zhang, S., Shen, L., Wang,
    J., Ding, S., Ji, R.: Distilpose: Tokenized pose regression with heatmap distillation.
    In: CVPR, pp. 2163–2172 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2023] Yang, J., Zeng, A., Liu, S., Li, F., Zhang, R., Zhang, L.:
    Explicit box detection unifies end-to-end multi-person pose estimation. In: ICLR
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2020] Zheng, C., Wu, W., Chen, C., Yang, T., Zhu, S., Shen, J.,
    Kehtarnavaz, N., Shah, M.: Deep learning-based human pose estimation: A survey.
    ACM Computing Surveys (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papandreou et al. [2017] Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A.,
    Tompson, J., Bregler, C., Murphy, K.: Towards accurate multi-person pose estimation
    in the wild. In: CVPR, pp. 4903–4911 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2017] He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn.
    In: ICCV, pp. 2961–2969 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. [2018] Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose
    estimation and tracking. In: ECCV, pp. 466–481 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moon et al. [2019] Moon, G., Chang, J.Y., Lee, K.M.: Posefix: Model-agnostic
    general human pose refinement network. In: CVPR, pp. 7773–7781 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2019] Sun, K., Xiao, B., Liu, D., Wang, J.: Deep high-resolution
    representation learning for human pose estimation. In: CVPR, pp. 5693–5703 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. [2020] Cai, Y., Wang, Z., Luo, Z., Yin, B., Du, A., Wang, H., Zhang,
    X., Zhou, X., Zhou, E., Sun, J.: Learning delicate local representations for multi-person
    pose estimation. In: ECCV, pp. 455–472 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2020] Huang, J., Zhu, Z., Guo, F., Huang, G.: The devil is in
    the details: Delving into unbiased data processing for human pose estimation.
    In: CVPR, pp. 5700–5709 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Zhang, F., Zhu, X., Dai, H., Ye, M., Zhu, C.: Distribution-aware
    coordinate representation for human pose estimation. In: CVPR, pp. 7093–7102 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Wang, J., Long, X., Gao, Y., Ding, E., Wen, S.: Graph-pcnn:
    Two stage human pose estimation with graph pose refinement. In: ECCV, pp. 492–508
    (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2022] Xu, X., Zou, Q., Lin, X.: Adaptive hypergraph neural network
    for multi-person pose estimation. In: AAAI, pp. 2955–2963 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. [2023] Jiang, C., Huang, K., Zhang, S., Wang, X., Xiao, J., Goulermas,
    Y.: Aggregated pyramid gating network for human pose estimation without pre-training.
    PR 138, 109429 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. [2023] Gu, K., Yang, L., Mi, M.B., Yao, A.: Bias-compensated integral
    regression for human pose estimation. TPAMI 45(9), 10687–10702 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2015] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards
    real-time object detection with region proposal networks. In: NIPS, pp. 91–99
    (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iqbal and Gall [2016] Iqbal, U., Gall, J.: Multi-person pose estimation with
    local joint-to-person associations. In: ECCV, pp. 627–642 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. [2017] Fang, H.-S., Xie, S., Tai, Y.-W., Lu, C.: Rmpe: Regional
    multi-person pose estimation. In: ICCV, pp. 2334–2343 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2018] Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.:
    Cascaded pyramid network for multi-person pose estimation. In: CVPR, pp. 7103–7112
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. [2019] Su, K., Yu, D., Xu, Z., Geng, X., Wang, C.: Multi-person pose
    estimation with enhanced channel-wise and spatial information. In: CVPR, pp. 5674–5682
    (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. [2020] Qiu, L., Zhang, X., Li, Y., Li, G., Wu, X., Xiong, Z., Han,
    X., Cui, S.: Peeking into occluded joints: A novel framework for crowd pose estimation.
    In: ECCV, pp. 488–504 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2021] Yang, S., Quan, Z., Nie, M., Yang, W.: Transpose: Keypoint
    localization via transformer. In: ICCV, pp. 11802–11812 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2023] Zhou, M., Stoffl, L., Mathis, M., Mathis, A.: Rethinking
    pose estimation in crowds: overcoming the detection information-bottleneck and
    ambiguity. In: ICCV (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Li, Y., Zhang, S., Wang, Z., Yang, S., Yang, W., Xia, S.-T.,
    Zhou, E.: Tokenpose: Learning keypoint tokens for human pose estimation. In: ICCV,
    pp. 11313–11322 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2021] Yuan, Y., Fu, R., Huang, L., Lin, W., Zhang, C., Chen, X.,
    Wang, J.: Hrformer: High-resolution transformer for dense prediction. In: NIPS
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2022] Xu, Y., Zhang, J., Zhang, Q., Tao, D.: Vitpose: Simple vision
    transformer baselines for human pose estimation. In: NIPS, vol. 35, pp. 38571–38584
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pishchulin et al. [2016] Pishchulin, L., Insafutdinov, E., Tang, S., Andres,
    B., Andriluka, M., Gehler, P.V., Schiele, B.: Deepcut: Joint subset partition
    and labeling for multi person pose estimation. In: CVPR, pp. 4929–4937 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Insafutdinov et al. [2016] Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
    M., Schiele, B.: Deepercut: A deeper, stronger, and faster multi-person pose estimation
    model. In: ECCV, pp. 34–50 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2017a] Cao, Z., Simon, T., Wei, S.-E., Sheikh, Y.: Realtime multi-person
    2d pose estimation using part affinity fields. In: CVPR, pp. 7291–7299 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2017b] Cao, Z., Simon, T., Wei, S.-E., Sheikh, Y.: OpenPose:realtime
    multi-person 2D pose estimation using part affinity fields. In: CVPR, pp. 7291–7299
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kreiss et al. [2019] Kreiss, S., Bertoni, L., Alahi, A.: Pifpaf: Composite
    fields for human pose estimation. In: CVPR, pp. 11977–11986 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2023] Cheng, Y., Ai, Y., Wang, B., Wang, X., Tan, R.T.: Bottom-up
    2d pose estimation via dual anatomical centers for small-scale persons. PR 139,
    109403 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qu et al. [2023] Qu, H., Cai, Y., Foo, L.G., Kumar, A., Liu, J.: A characteristic
    function-based method for bottom-up human pose estimation. In: CVPR, pp. 13009–13018
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Newell et al. [2017] Newell, A., Huang, Z., Deng, J.: Associative embedding:
    End-to-end learning for joint detection and grouping. In: NIPS, pp. 2277–2287
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kocabas et al. [2018] Kocabas, M., Karagoz, S., Akbas, E.: Multiposenet: Fast
    multi-person pose estimation using pose residual network. In: ECCV, pp. 417–433
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019] Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.-S., Lu, C.: Crowdpose:
    Efficient crowded scenes pose estimation and a new benchmark. In: CVPR, pp. 10863–10872
    (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2020] Jin, S., Liu, W., Xie, E., Wang, W., Qian, C., Ouyang, W.,
    Luo, P.: Differentiable hierarchical graph grouping for multi-person pose estimation.
    In: ECCV, pp. 718–734 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2020] Cheng, B., Xiao, B., Wang, J., Shi, H., Huang, T.S., Zhang,
    L.: Higherhrnet: Scale-aware representation learning for bottom-up human pose
    estimation. In: CVPR, pp. 5386–5395 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. [2019] Nie, X., Feng, J., Zhang, J., Yan, S.: Single-stage multi-person
    pose machines. In: ICCV, pp. 6951–6960 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng et al. [2021] Geng, Z., Sun, K., Xiao, B., Zhang, Z., Wang, J.: Bottom-up
    human pose estimation via disentangled keypoint regression. In: CVPR, pp. 14676–14686
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Li, J., Wang, Y., Zhang, S.: PolarPose: Single-stage multi-person
    pose estimation in polar coordinates. IEEE Transactions on Image Processing 32,
    1108–1119 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. [2019] Tian, Z., Chen, H., Shen, C.: Directpose: Direct end-to-end
    multi-person pose estimation. arXiv preprint arXiv:1911.07451 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. [2021] Mao, W., Tian, Z., Wang, X., Shen, C.: Fcpose: Fully convolutional
    multi-person pose estimation with dynamic instance-aware convolutions. In: CVPR,
    pp. 9034–9043 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2021] Shi, D., Wei, X., Yu, X., Tan, W., Ren, Y., Pu, S.: Inspose:
    instance-aware networks for single-stage multi-person pose estimation. In: ACMMM,
    pp. 3079–3087 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miao et al. [2023] Miao, H., Lin, J., Cao, J., He, X., Su, Z., Liu, R.: Smpr:
    Single-stage multi-person pose regression. PR 143, 109743 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2022] Shi, D., Wei, X., Li, L., Ren, Y., Tan, W.: End-to-end multi-person
    pose estimation with transformers. In: CVPR, pp. 11069–11078 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023] Liu, H., Chen, Q., Tan, Z., Liu, J.-J., Wang, J., Su, X.,
    Li, X., Yao, K., Han, J., Ding, E., Zhao, Y., Wang, J.: Group pose: A simple baseline
    for end-to-end multi-person pose estimation. In: ICCV, pp. 15029–15038 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pfister et al. [2014] Pfister, T., Simonyan, K., Charles, J., Zisserman, A.:
    Deep convolutional neural networks for efficient pose estimation in gesture videos.
    In: ACCV, pp. 538–552 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grinciunaite et al. [2016] Grinciunaite, A., Gudi, A., Tasli, E., Den Uyl,
    M.: Human pose estimation in space and time using 3d cnn. In: ECCV, pp. 32–39
    (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pfister et al. [2015] Pfister, T., Charles, J., Zisserman, A.: Flowing convnets
    for human pose estimation in videos. In: ICCV, pp. 1913–1921 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2017] Song, J., Wang, L., Van Gool, L., Hilliges, O.: Thin-slicing
    network: A deep structured model for pose estimation in videos. In: CVPR, pp.
    4220–4229 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain et al. [2014] Jain, A., Tompson, J., LeCun, Y., Bregler, C.: Modeep: A
    deep learning framework using motion features for human pose estimation. In: ACCV,
    pp. 302–315 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gkioxari et al. [2016] Gkioxari, G., Toshev, A., Jaitly, N.: Chained predictions
    using convolutional neural networks. In: ECCV, pp. 728–743 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Charles et al. [2016] Charles, J., Pfister, T., Magee, D., Hogg, D., Zisserman,
    A.: Personalizing human video pose estimation. In: CVPR, pp. 3063–3072 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. [2018] Luo, Y., Ren, J., Wang, Z., Sun, W., Pan, J., Liu, J., Pang,
    J., Lin, L.: Lstm pose machines. In: CVPR, pp. 5207–5215 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. [2019] Nie, X., Li, Y., Luo, L., Zhang, N., Feng, J.: Dynamic kernel
    distillation for efficient pose estimation in videos. In: ICCV, pp. 6942–6950
    (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019a] Li, H., Yang, W., Liao, Q.: Temporal feature enhancing network
    for human pose estimation in videos. In: ICIP, pp. 579–583 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019b] Li, W., Xu, X., Zhang, Y.-J.: Temporal feature correlation
    for human pose estimation in videos. In: ICIP, pp. 599–603 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2021] Xu, L., Guan, Y., Jin, S., Liu, W., Qian, C., Luo, P., Ouyang,
    W., Wang, X.: Vipnas: Efficient video pose estimation via neural architecture
    search. In: CVPR, pp. 16072–16081 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dang et al. [2022] Dang, Y., Yin, J., Zhang, S.: Relation-based associative
    joint location for human pose estimation in videos. IEEE Transactions on Image
    Processing 31, 3973–3986 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2023] Jin, K.-M., Lim, B.-S., Lee, G.-H., Kang, T.-K., Lee, S.-W.:
    Kinematic-aware hierarchical attention network for human pose estimation in videos.
    In: WACV, pp. 5725–5734 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Zhang, Y., Wang, Y., Camps, O., Sznaier, M.: Key frame
    proposal network for efficient pose estimation in videos. In: ECCV, pp. 609–625
    (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2022] Ma, X., Rahmani, H., Fan, Z., Yang, B., Chen, J., Liu, J.:
    Remote: Reinforced motion transformation network for semi-supervised 2d pose estimation
    in videos. In: AAAI, pp. 1944–1952 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. [2022] Zeng, A., Ju, X., Yang, L., Gao, R., Zhu, X., Dai, B., Xu,
    Q.: Deciwatch: A simple baseline for 10$\times$ efficient 2d and 3d pose estimation.
    In: ECCV, pp. 607–624 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2023] Sun, Y., Dougherty, A.W., Zhang, Z., Choi, Y.K., Wu, C.:
    Mixsynthformer: A transformer encoder-like structure with mixed synthetic self-attention
    for efficient human pose estimation. In: ICCV, pp. 14884–14893 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiu et al. [2018] Xiu, Y., Li, J., Wang, H., Fang, Y., Lu, C.: Pose flow: Efficient
    online pose tracking. In: ECCV (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girdhar et al. [2018] Girdhar, R., Gkioxari, G., Torresani, L., Paluri, M.,
    Tran, D.: Detect-and-track: Efficient pose estimation in videos. In: CVPR, pp.
    350–359 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Wang, M., Tighe, J., Modolo, D.: Combining detection and
    tracking for human pose estimation in videos. In: CVPR, pp. 11088–11096 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. [2022] Fang, H.-S., Li, J., Tang, H., Xu, C., Zhu, H., Xiu, Y.,
    Li, Y.-L., Lu, C.: Alphapose: Whole-body regional multi-person pose estimation
    and tracking in real-time. TPAMI (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. [2023] Feng, R., Gao, Y., Ma, X., Tse, T.H.E., Chang, H.J.: Mutual
    information-based temporal difference learning for human pose estimation in video.
    In: CVPR, pp. 17131–17141 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gai et al. [2023] Gai, D., Feng, R., Min, W., Yang, X., Su, P., Wang, Q., Han,
    Q.: Spatiotemporal learning transformer for video-based human pose estimation.
    TCSVT (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amit et al. [2021] Amit, T., Shaharbany, T., Nachmani, E., Wolf, L.: Segdiff:
    Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Chen, S., Sun, P., Song, Y., Luo, P.: Diffusiondet: Diffusion
    model for object detection. ICCV, 19830–19843 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. [2023] Feng, R., Gao, Y., Tse, T.H.E., Ma, X., Chang, H.J.: Diffpose:
    Spatiotemporal diffusion model for video-based human pose estimation. In: ICCV,
    pp. 14861–14872 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2019] Jin, S., Liu, W., Ouyang, W., Qian, C.: Multi-person articulated
    tracking with spatial and temporal embeddings. In: CVPR, pp. 5664–5673 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Chan [2014] Li, S., Chan, A.B.: 3d human pose estimation from monocular
    images with deep convolutional neural network. In: ACCV, pp. 332–347 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2015] Li, S., Zhang, W., Chan, A.B.: Maximum-margin structured learning
    with deep networks for 3d human pose estimation. In: ICCV, pp. 2848–2856 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tekin et al. [2016] Tekin, B., Katircioglu, I., Salzmann, M., Lepetit, V.,
    Fua, P.: Structured prediction of 3d human pose with deep neural networks. In:
    BMVC, pp. 1–11 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2016] Zhou, X., Sun, X., Zhang, W., Liang, S., Wei, Y.: Deep kinematic
    pose regression. In: ECCV, pp. 186–201 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta et al. [2017] Mehta, D., Rhodin, H., Casas, D., Fua, P., Sotnychenko,
    O., Xu, W., Theobalt, C.: Monocular 3d human pose estimation in the wild using
    improved cnn supervision. In: 3DV, pp. 506–516 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2017] Zhou, X., Huang, Q., Sun, X., Xue, X., Wei, Y.: Towards
    3d human pose estimation in the wild: a weakly-supervised approach. In: ICCV,
    pp. 398–407 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martinez et al. [2017] Martinez, J., Hossain, R., Romero, J., Little, J.J.:
    A simple yet effective baseline for 3d human pose estimation. In: ICCV, pp. 2640–2649
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tekin et al. [2017] Tekin, B., Márquez-Neila, P., Salzmann, M., Fua, P.: Learning
    to fuse 2d and 3d image cues for monocular body pose estimation. In: ICCV, pp.
    3941–3950 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2019] Zhou, K., Han, X., Jiang, N., Jia, K., Lu, J.: Hemlets pose:
    Learning part-centric heatmap triplets for accurate 3d human pose estimation.
    In: ICCV, pp. 2344–2353 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2018] Wang, M., Chen, X., Liu, W., Qian, C., Lin, L., Ma, L.:
    Drpose3d: Depth ranking in 3d human pose estimation. arXiv preprint arXiv:1805.08973
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carbonera Luvizon et al. [2023] Carbonera Luvizon, D., Tabia, H., Picard, D.:
    SSP-Net: Scalable sequential pyramid networks for real-time 3d human pose regression.
    PR 142, 109714 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jahangiri and Yuille [2017] Jahangiri, E., Yuille, A.L.: Generating multiple
    diverse hypotheses for human 3d pose consistent with 2d joint detections. In:
    ICCV, pp. 805–814 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. [2019] Sharma, S., Varigonda, P.T., Bindal, P., Sharma, A., Jain,
    A.: Monocular 3d human pose estimation by generation and ordinal ranking. In:
    ICCV, pp. 2325–2334 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Lee [2019] Li, C., Lee, G.H.: Generating multiple hypotheses for 3d
    human pose estimation with mixture density network. In: CVPR, pp. 9887–9895 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ci et al. [2019] Ci, H., Wang, C., Ma, X., Wang, Y.: Optimizing network structure
    for 3d human pose estimation. In: ICCV, pp. 2262–2271 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2019] Zhao, L., Peng, X., Tian, Y., Kapadia, M., Metaxas, D.N.:
    Semantic graph convolutional networks for 3d human pose regression. In: CVPR,
    pp. 3425–3435 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. [2020] Choi, H., Moon, G., Lee, K.M.: Pose2mesh: Graph convolutional
    network for 3d human pose and mesh recovery from a 2d human pose. In: ECCV, pp.
    769–787 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. [2020] Zeng, A., Sun, X., Huang, F., Liu, M., Xu, Q., Lin, S.:
    Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine
    approach. In: ECCV, pp. 507–523 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2020] Liu, K., Ding, R., Zou, Z., Wang, L., Tang, W.: A comprehensive
    study of weight sharing in graph networks for 3d human pose estimation. In: Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part X 16, pp. 318–334 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou and Tang [2021] Zou, Z., Tang, W.: Modulated graph convolutional network
    for 3d human pose estimation. In: ICCV, pp. 11477–11487 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu and Takano [2021] Xu, T., Takano, W.: Graph stacked hourglass networks for
    3d human pose estimation. In: CVPR, pp. 16105–16114 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shengping et al. [2023] Shengping, Z., Chenyang, W., Liqiang, N., Hongxun,
    Y., Qingming, H., Qi, T.: Learning enriched hop-aware correlation for robust 3d
    human pose estimation. IJCV (6), 1566–1583 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hassan and Ben Hamza [2023] Hassan, M.T., Ben Hamza, A.: Regular splitting
    graph network for 3d human pose estimation. IEEE Transactions on Image Processing
    32, 4212–4222 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. [2023] Zhai, K., Nie, Q., Ouyang, B., Li, X., Yang, S.: Hopfir:
    Hop-wise graphformer with intragroup joint refinement for 3d human pose estimation.
    In: ICCV (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2021] Lin, K., Wang, L., Liu, Z.: End-to-end human pose and mesh
    reconstruction with transformers. In: CVPR, pp. 1954–1963 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2022] Zhao, W., Wang, W., Tian, Y.: Graformer: Graph-oriented
    transformer for 3d pose estimation. In: CVPR, pp. 20438–20447 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2018] Yang, W., Ouyang, W., Wang, X., Ren, J., Li, H., Wang, X.:
    3d human pose estimation in the wild by adversarial learning. In: CVPR, pp. 5255–5264
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Habibie et al. [2019] Habibie, I., Xu, W., Mehta, D., Pons-Moll, G., Theobalt,
    C.: In the wild human pose estimation using explicit 2d features and intermediate
    3d representations. In: CVPR, pp. 10905–10914 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2019] Chen, C.-H., Tyagi, A., Agrawal, A., Drover, D., Mv, R.,
    Stojanov, S., Rehg, J.M.: Unsupervised 3d pose estimation with geometric self-supervision.
    In: CVPR, pp. 5714–5724 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wandt and Rosenhahn [2019] Wandt, B., Rosenhahn, B.: Repnet: Weakly supervised
    training of an adversarial reprojection network for 3d human pose estimation.
    In: CVPR, pp. 7782–7791 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iqbal et al. [2020] Iqbal, U., Molchanov, P., Kautz, J.: Weakly-supervised
    3d human pose learning via multi-view images in the wild. In: CVPR, pp. 5243–5252
    (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kundu et al. [2020] Kundu, J.N., Seth, S., Jampani, V., Rakesh, M., Babu, R.V.,
    Chakraborty, A.: Self-supervised 3d human pose estimation via part guided novel
    image synthesis. In: CVPR, pp. 6152–6162 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidtke et al. [2021] Schmidtke, L., Vlontzos, A., Ellershaw, S., Lukens,
    A., Arichi, T., Kainz, B.: Unsupervised human pose estimation through transforming
    shape templates. In: CVPR, pp. 2484–2494 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2021] Yu, Z., Ni, B., Xu, J., Wang, J., Zhao, C., Zhang, W.: Towards
    alleviating the modeling ambiguity of unsupervised monocular 3d human pose estimation.
    In: ICCV, pp. 8651–8660 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. [2022] Gong, K., Li, B., Zhang, J., Wang, T., Huang, J., Mi, M.B.,
    Feng, J., Wang, X.: Posetriplet: co-evolving 3d human pose estimation, imitation,
    and hallucination under self-supervision. In: CVPR, pp. 11017–11027 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chai et al. [2023] Chai, W., Jiang, Z., Hwang, J.-N., Wang, G.: Global adaptation
    meets local generalization: Unsupervised domain adaptation for 3d human pose estimation.
    In: ICCV (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Wang, Z., Nie, X., Qu, X., Chen, Y., Liu, S.: Distribution-aware
    single-stage models for multi-person 3d pose estimation. In: CVPR, pp. 13096–13105
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rogez et al. [2017] Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net: Localization-classification-regression
    for human pose. In: CVPR, pp. 3433–3441 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rogez et al. [2019] Rogez, G., Weinzaepfel, P., Schmid, C.: Lcr-net++: Multi-person
    2d and 3d pose detection in natural images. TPAMI 42(5), 1146–1161 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moon et al. [2019] Moon, G., Chang, J.Y., Lee, K.M.: Camera distance-aware
    top-down approach for 3d multi-person pose estimation from a single rgb image.
    In: ICCV, pp. 10133–10142 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin and Lee [2020] Lin, J., Lee, G.H.: Hdnet: Human depth estimation for multi-person
    camera-space localization. In: ECCV, pp. 633–648 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Wang, C., Li, J., Liu, W., Qian, C., Lu, C.: Hmor: Hierarchical
    multi-person ordinal relations for monocular multi-person 3d pose estimation.
    In: ECCV, pp. 242–259 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cha et al. [2022] Cha, J., Saqlain, M., Kim, G., Shin, M., Baek, S.: Multi-person
    3d pose and shape estimation via inverse kinematics and refinement. In: ECCV,
    pp. 660–677 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zanfir et al. [2018] Zanfir, A., Marinoiu, E., Zanfir, M., Popa, A.-I., Sminchisescu,
    C.: Deep network for the integrated 3d sensing of multiple people in natural images.
    NIPS 31 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fabbri et al. [2020] Fabbri, M., Lanzi, F., Calderara, S., Alletto, S., Cucchiara,
    R.: Compressed volumetric heatmaps for multi-person 3d pose estimation. In: CVPR,
    pp. 7204–7213 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kundu et al. [2020] Kundu, J.N., Revanur, A., Waghmare, G.V., Venkatesh, R.M.,
    Babu, R.V.: Unsupervised cross-modal alignment for multi-person 3d pose estimation.
    In: ECCV, pp. 35–52 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta et al. [2018] Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar,
    S., Pons-Moll, G., Theobalt, C.: Single-shot multi-person 3d pose estimation from
    monocular rgb. In: 3DV, pp. 120–130 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta et al. [2020] Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Elgharib,
    M., Fua, P., Seidel, H.-P., Rhodin, H., Pons-Moll, G., Theobalt, C.: Xnect: Real-time
    multi-person 3d motion capture with a single rgb camera. TOG 39(4), 82–1 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhen et al. [2020] Zhen, J., Fang, Q., Sun, J., Liu, W., Jiang, W., Bao, H.,
    Zhou, X.: Smap: Single-shot multi-person absolute 3d pose estimation. In: ECCV,
    pp. 550–566 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022] Liu, Q., Zhang, Y., Bai, S., Yuille, A.: Explicit occlusion
    reasoning for multi-person 3d human pose estimation. In: ECCV, pp. 497–517 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Chen, X., Zhang, J., Wang, K., Wei, P., Lin, L.: Multi-person
    3d pose esitmation with occlusion reasoning. TMM, 1–13 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2019] Zhou, X., Wang, D., Krähenbühl, P.: Objects as points. arXiv
    preprint arXiv:1904.07850 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2020] Wei, F., Sun, X., Li, H., Wang, J., Lin, S.: Point-set anchors
    for object detection, instance segmentation and pose estimation. In: ECCV, pp.
    527–544 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2022] Jin, L., Xu, C., Wang, X., Xiao, Y., Guo, Y., Nie, X., Zhao,
    J.: Single-stage is enough: Multi-person absolute 3d pose estimation. In: CVPR,
    pp. 13086–13095 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. [2023] Qiu, Z., Qiu, K., Fu, J., Fu, D.: Weakly-supervised pre-training
    for 3d human pose estimation via perspective knowledge. PR 139, 109497 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tekin et al. [2016] Tekin, B., Rozantsev, A., Lepetit, V., Fua, P.: Direct
    prediction of 3d body poses from motion compensated sequences. In: CVPR, pp. 991–1000
    (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehta et al. [2017] Mehta, D., Sridhar, S., Sotnychenko, O., Rhodin, H., Shafiei,
    M., Seidel, H.-P., Xu, W., Casas, D., Theobalt, C.: Vnect: Real-time 3d human
    pose estimation with a single rgb camera. ACM Transactions on Graphics (TOG) 36(4),
    44 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dabral et al. [2018] Dabral, R., Mundhada, A., Kusupati, U., Afaque, S., Sharma,
    A., Jain, A.: Learning 3d human pose from structure and motion. In: ECCV, pp.
    668–683 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. [2022] Qiu, Z., Yang, Q., Wang, J., Fu, D.: Ivt: An end-to-end instance-guided
    video transformer for 3d pose estimation. In: ACM MM, pp. 6174–6182 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Honari et al. [2023] Honari, S., Constantin, V., Rhodin, H., Salzmann, M.,
    Fua, P.: Temporal representation learning on monocular videos for 3d human pose
    estimation. TPAMI 45(5), 6415–6427 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pavllo et al. [2019] Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.:
    3d human pose estimation in video with temporal convolutions and semi-supervised
    training. In: CVPR, pp. 7753–7762 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2019] Cheng, Y., Yang, B., Wang, B., Yan, W., Tan, R.T.: Occlusion-aware
    networks for 3d human pose estimation in video. In: CVPR, pp. 723–732 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2020] Liu, J., Guang, Y., Rojas, J.: Gast-net: Graph attention
    spatio-temporal convolutional networks for 3d human pose estimation in video.
    arXiv preprint arXiv:2003.14179 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021] Chen, T., Fang, C., Shen, X., Zhu, Y., Chen, Z., Luo, J.:
    Anatomy-aware 3d human pose estimation with bone-based pose decomposition. TCSVT
    32(1), 198–209 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. [2019] Cai, Y., Ge, L., Liu, J., Cai, J., Cham, T.-J., Yuan, J.,
    Thalmann, N.M.: Exploiting spatial-temporal relationships for 3d pose estimation
    via graph convolutional networks. In: ICCV, pp. 2272–2281 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2021] Zheng, C., Zhu, S., Mendieta, M., Yang, T., Chen, C., Ding,
    Z.: 3d human pose estimation with spatial and temporal transformers. In: ICCV,
    pp. 11656–11665 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2023] Zhao, Q., Zheng, C., Liu, M., Wang, P., Chen, C.: Poseformerv2:
    Exploring frequency domain for efficient and robust 3d human pose estimation.
    In: CVPR, pp. 8877–8886 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022a] Li, W., Liu, H., Ding, R., Liu, M., Wang, P., Yang, W.: Exploiting
    temporal contexts with strided transformer for 3d human pose estimation. IEEE
    Transactions on Multimedia (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022b] Li, W., Liu, H., Tang, H., Wang, P., Van Gool, L.: Mhformer:
    Multi-hypothesis transformer for 3d human pose estimation. In: CVPR, pp. 13147–13156
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Li, W., Liu, H., Tang, H., Wang, P.: Multi-hypothesis representation
    learning for transformer-based 3d human pose estimation. PR 141, 109631 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Holmquist and Wandt [2023] Holmquist, K., Wandt, B.: Diffpose: Multi-hypothesis
    human pose estimation using diffusion models. In: ICCV, pp. 15977–15987 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shan et al. [2023] Shan, W., Liu, Z., Zhang, X., Wang, Z., Han, K., Wang, S.,
    Ma, S., Gao, W.: Diffusion-based 3d human pose estimation with multi-hypothesis
    aggregation. In: ICCV (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. [2023] Tang, Z., Qiu, Z., Hao, Y., Hong, R., Yao, T.: 3d human
    pose estimation with spatio-temporal criss-cross attention. In: CVPR, pp. 4790–4799
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2017] Lin, M., Lin, L., Liang, X., Wang, K., Cheng, H.: Recurrent
    3d pose sequence machines. In: CVPR, pp. 810–819 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rayat Imtiaz Hossain and Little [2018] Rayat Imtiaz Hossain, M., Little, J.J.:
    Exploiting temporal information for 3d human pose estimation. In: ECCV, pp. 68–84
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. [2018] Lee, K., Lee, I., Lee, S.: Propagating lstm: 3d pose estimation
    based on joint interdependency. In: ECCV, pp. 119–135 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katircioglu et al. [2018] Katircioglu, I., Tekin, B., Salzmann, M., Lepetit,
    V., Fua, P.: Learning latent representations of 3d human pose with deep neural
    networks. IJCV 126(12), 1326–1341 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yeh et al. [2019] Yeh, R., Hu, Y.-T., Schwing, A.: Chirality nets for human
    pose regression. In: NIPS, pp. 8161–8171 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Wang, J., Yan, S., Xiong, Y., Lin, D.: Motion guided 3d
    pose estimation from videos. In: ECCV, pp. 764–780 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2023] Yu, B.X., Zhang, Z., Liu, Y., Zhong, S.-h., Liu, Y., Chen,
    C.W.: Gla-gcn: Global-local adaptive graph convolutional network for 3d human
    pose estimation from monocular video. In: ICCV, pp. 8818–8829 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Zhang, J., Tu, Z., Yang, J., Chen, Y., Yuan, J.: Mixste:
    Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video. In:
    CVPR, pp. 13232–13242 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Chen, H., He, J.-Y., Xiang, W., Cheng, Z.-Q., Liu, W., Liu,
    H., Luo, B., Geng, Y., Xie, X.: Hdformer: High-order directed transformer for
    3d human pose estimation. In: IJCAI, pp. 581–589 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuai et al. [2023] Shuai, H., Wu, L., Liu, Q.: Adaptive multi-view and temporal
    fusing transformer for 3d human pose estimation. TPAMI 45(4), 4122–4135 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2022] Zhu, W., Ma, X., Liu, Z., Liu, L., Wu, W., Wang, Y.: Motionbert:
    Unified pretraining for human motion analysis. arXiv preprint arXiv:2210.06551
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2021a] Cheng, Y., Wang, B., Yang, B., Tan, R.T.: Graph and temporal
    convolutional networks for 3d multi-person pose estimation in monocular videos.
    In: AAAI, pp. 1157–1165 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2021b] Cheng, Y., Wang, B., Yang, B., Tan, R.T.: Monocular 3d
    multi-person pose estimation by integrating top-down and bottom-up networks. In:
    CVPR, pp. 7649–7659 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2023] Park, S., You, E., Lee, I., Lee, J.: Towards robust and
    smooth 3d multi-person pose estimation from monocular videos in the wild. In:
    ICCV, pp. 14772–14782 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2015] Zhao, L., Gao, X., Tao, D., Li, X.: Tracking human pose
    using max-margin markov models. IEEE Transactions on Image Processing 24(12),
    5274–5287 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Samanta and Chanda [2016] Samanta, S., Chanda, B.: A data-driven approach for
    human pose tracking based on spatio-temporal pictorial structure. arXiv preprint
    arXiv:1608.00199 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2015] Zhao, L., Gao, X., Tao, D., Li, X.: Learning a tracking
    and estimation integrated graphical model for human pose tracking. IEEE transactions
    on neural networks and learning systems 26(12), 3176–3186 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2016] Ma, M., Marturi, N., Li, Y., Stolkin, R., Leonardis, A.: A
    local-global coupled-layer puppet model for robust online human pose tracking.
    Computer Vision and Image Understanding 153, 163–178 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019] Zhang, J., Zhu, Z., Zou, W., Li, P., Li, Y., Su, H., Huang,
    G.: Fastpose: Towards real-time pose estimation and tracking via scale-normalized
    multi-task networks. arXiv preprint arXiv:1908.05593 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ning et al. [2020] Ning, G., Pei, J., Huang, H.: Lighttrack: A generic framework
    for online top-down human pose tracking. In: CVPRW, pp. 1034–1035 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafi et al. [2020] Rafi, U., Doering, A., Leibe, B., Gall, J.: Self-supervised
    keypoint correspondences for multi-person pose estimation and tracking in videos.
    In: ECCV, pp. 36–52 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2021] Yang, Y., Ren, Z., Li, H., Zhou, C., Wang, X., Hua, G.:
    Learning dynamics via graph neural networks for human pose estimation and tracking.
    In: CVPR, pp. 8074–8084 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Doering and Gall [2023] Doering, A., Gall, J.: A gated attention transformer
    for multi-person pose tracking. In: ICCV, pp. 3189–3198 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iqbal et al. [2017] Iqbal, U., Milan, A., Gall, J.: Posetrack: Joint multi-person
    pose estimation and tracking. In: CVPR, pp. 2011–2020 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raaj et al. [2019] Raaj, Y., Idrees, H., Hidalgo, G., Sheikh, Y.: Efficient
    online multi-person 2d pose tracking with recurrent spatio-temporal affinity fields.
    In: CVPR, pp. 4620–4628 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bridgeman et al. [2019] Bridgeman, L., Volino, M., Guillemaut, J.-Y., Hilton,
    A.: Multi-person 3d pose estimation and tracking in sports. In: CVPRW, pp. 0–0
    (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zanfir et al. [2018] Zanfir, A., Marinoiu, E., Sminchisescu, C.: Monocular
    3d pose and shape estimation of multiple people in natural scenes-the importance
    of multiple scene constraints. In: CVPR, pp. 2148–2157 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2019] Sun, X., Li, C., Lin, S.: Explicit spatiotemporal joint relation
    learning for tracking human pose. In: ICCV (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reddy et al. [2021] Reddy, N.D., Guigues, L., Pishchulin, L., Eledath, J.,
    Narasimhan, S.G.: Tessetrack: End-to-end learnable multi-person articulated 3d
    pose tracking. In: CVPR, pp. 15190–15200 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Zhang, Y., Wang, C., Wang, X., Liu, W., Zeng, W.: Voxeltrack:
    Multi-person 3d human pose estimation and tracking in the wild. TPAMI 45(2), 2613–2626
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. [2023] Zou, S., Xu, Y., Li, C., Ma, L., Cheng, L., Vo, M.: Snipper:
    A spatiotemporal transformer for simultaneous multi-person 3d pose estimation
    tracking and forecasting on a video snippet. TCSVT (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajasegaran et al. [2022] Rajasegaran, J., Pavlakos, G., Kanazawa, A., Malik,
    J.: Tracking people by predicting 3d appearance, location and pose. In: CVPR,
    pp. 2740–2749 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Wang [2017] Wang, H., Wang, L.: Modeling temporal dynamics and spatial
    configurations of actions using two-stream recurrent neural networks. In: CVPR,
    pp. 499–508 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caetano et al. [2019] Caetano, C., Brémond, F., Schwartz, W.R.: Skeleton image
    representation for 3d action recognition based on tree structure and reference
    joints. In: SIBGRAPI, pp. 16–23 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. [2018] Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional
    networks for skeleton-based action recognition. In: AAAI, pp. 7444–7452 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plizzari et al. [2021] Plizzari, C., Cannici, M., Matteucci, M.: Spatial temporal
    transformer network for skeleton-based action recognition. In: ICPRW, pp. 694–701
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jhuang et al. [2013] Jhuang, H., Gall, J., Zuffi, S., Schmid, C., Black, M.J.:
    Towards understanding action recognition. In: ICCV, pp. 3192–3199 (2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chéron et al. [2015] Chéron, G., Laptev, I., Schmid, C.: P-cnn: Pose-based
    cnn features for action recognition. In: ICCV, pp. 3218–3226 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zolfaghari et al. [2017] Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox,
    T.: Chained multi-stream networks exploiting pose, motion, and appearance for
    action classification and detection. In: ICCV, pp. 2904–2913 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choutas et al. [2018] Choutas, V., Weinzaepfel, P., Revaud, J., Schmid, C.:
    Potion: Pose motion representation for action recognition. In: CVPR, pp. 7024–7033
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Yuan [2018] Liu, M., Yuan, J.: Recognizing human actions as the evolution
    of pose estimation maps. In: CVPR, pp. 1159–1168 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moon et al. [2021] Moon, G., Kwon, H., Lee, K.M., Cho, M.: Integralaction:
    Pose-driven feature integration for robust human action recognition in videos.
    In: CVPR, pp. 3339–3348 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shah et al. [2022] Shah, A., Mishra, S., Bansal, A., Chen, J.-C., Chellappa,
    R., Shrivastava, A.: Pose and joint-aware action recognition. In: WACV, pp. 3850–3860
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. [2022] Duan, H., Zhao, Y., Chen, K., Lin, D., Dai, B.: Revisiting
    skeleton-based action recognition. In: CVPR, pp. 2969–2978 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sato et al. [2023] Sato, F., Hachiuma, R., Sekii, T.: Prompt-guided zero-shot
    anomaly action recognition using pretrained deep skeleton features. In: CVPR,
    pp. 6471–6480 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hachiuma et al. [2023] Hachiuma, R., Sato, F., Sekii, T.: Unified keypoint-based
    action recognition framework via structured keypoint pooling. In: CVPR, pp. 22962–22971
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luvizon et al. [2018] Luvizon, D.C., Picard, D., Tabia, H.: 2d/3d pose estimation
    and action recognition using multitask deep learning. In: CVPR, pp. 5137–5146
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Foo et al. [2023] Foo, L.G., Li, T., Rahmani, H., Ke, Q., Liu, J.: Unified
    pose sequence modeling. In: CVPR, pp. 13019–13030 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2015] Du, Y., Fu, Y., Wang, L.: Skeleton based action recognition
    with convolutional neural network. In: ACPR, pp. 579–583 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2016] Wang, P., Li, Z., Hou, Y., Li, W.: Action recognition based
    on joint trajectory maps using convolutional neural networks. In: ACMMM, pp. 102–106
    (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2016] Hou, Y., Li, Z., Wang, P., Li, W.: Skeleton optical spectra-based
    action recognition using convolutional neural networks. TCSVT 28(3), 807–811 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2017] Li, C., Hou, Y., Wang, P., Li, W.: Joint distance maps based
    action recognition with convolutional neural networks. IEEE Signal Processing
    Letters 24(5), 624–628 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2017] Liu, M., Liu, H., Chen, C.: Enhanced skeleton visualization
    for view invariant human action recognition. PR 68, 346–362 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ke et al. [2017] Ke, Q., An, S., Bennamoun, M., Sohel, F., Boussaid, F.: Skeletonnet:
    Mining deep part features for 3d action recognition. IEEE Signal Processing Letters
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019] Li, Y., Xia, R., Liu, X., Huang, Q.: Learning shape-motion
    representations from geometric algebra spatio-temporal model for skeleton-based
    action recognition. In: ICME, pp. 1066–1071 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. [2017] Ding, Z., Wang, P., Ogunbona, P.O., Li, W.: Investigation
    of different skeleton features for cnn-based 3d action recognition. In: ICMEW,
    pp. 617–622 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ke et al. [2017] Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F.: A
    new representation of skeleton sequences for 3d action recognition. In: CVPR (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. [2019] Liang, D., Fan, G., Lin, G., Chen, W., Pan, X., Zhu, H.:
    Three-stream convolutional neural network with multi-task and ensemble learning
    for 3d action recognition. In: CVPRW, pp. 0–0 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2017] Liu, H., Tu, J., Liu, M.: Two-stream 3d convolutional neural
    network for skeleton-based action recognition. arXiv preprint arXiv:1705.08106
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hernandez Ruiz et al. [2017] Hernandez Ruiz, A., Porzi, L., Rota Bulò, S.,
    Moreno-Noguer, F.: 3d cnns on distance matrices for human action recognition.
    In: ACM MM, pp. 1087–1095 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2015] Du, Y., Wang, W., Wang, L.: Hierarchical recurrent neural
    network for skeleton based action recognition. In: CVPR, pp. 1110–1118 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2016] Du, Y., Fu, Y., Wang, L.: Representation learning of temporal
    dynamics for skeleton-based action recognition. IEEE Transactions on Image Processing
    25(7), 3010–3022 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shahroudy et al. [2016] Shahroudy, A., Liu, J., Ng, T.-T., Wang, G.: NTU RGB+
    D: A large scale dataset for 3D human activity analysis. In: CVPR, pp. 1010–1019
    (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2017] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: An end-to-end
    spatio-temporal attention model for human action recognition from skeleton data.
    In: AAAI, pp. 4263–4270 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2017] Liu, J., Wang, G., Hu, P., Duan, L.-Y., Kot, A.C.: Global
    context-aware attention lstm networks for 3d action recognition. In: CVPR, pp.
    1647–1656 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2018] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J.: Spatio-temporal
    attention-based lstm networks for 3d action recognition and detection. TIP 27(7),
    3459–3471 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019] Zhang, P., Xue, J., Lan, C., Zeng, W., Gao, Z., Zheng,
    N.: Eleatt-rnn: Adding attentiveness to neurons in recurrent neural networks.
    IEEE Transactions on Image Processing 29, 1061–1073 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Si et al. [2019] Si, C., Chen, W., Wang, W., Wang, L., Tan, T.: An attention
    enhanced graph convolutional lstm network for skeleton-based action recognition.
    In: CVPR, pp. 1227–1236 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2016] Liu, J., Shahroudy, A., Xu, D., Wang, G.: Spatio-temporal
    LSTM with trust gates for 3D human action recognition. In: ECCV, pp. 816–833 (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2017] Zhang, S., Liu, X., Xiao, J.: On geometric features for
    skeleton-based action recognition using multilayer lstm networks. In: WACV, pp.
    148–157 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Si et al. [2018] Si, C., Jing, Y., Wang, W., Wang, L., Tan, T.: Skeleton-based
    action recognition with spatial reasoning and temporal stack learning. In: ECCV,
    pp. 103–118 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2020] Huang, Z., Shen, X., Tian, X., Li, H., Huang, J., Hua,
    X.-S.: Spatio-temporal inception graph convolutional networks for skeleton-based
    action recognition. In: ACM MM, pp. 2122–2130 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2020] Liu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W.: Disentangling
    and unifying graph convolutions for skeleton-based action recognition. In: CVPR,
    pp. 143–152 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Zhang, X., Xu, C., Tao, D.: Context aware graph convolution
    for skeleton-based action recognition. In: CVPR, pp. 14333–14342 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019] Li, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q.:
    Actional-structural graph convolutional networks for skeleton-based action recognition.
    In: CVPR, pp. 3595–3603 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2019] Shi, L., Zhang, Y., Cheng, J., Lu, H.: Two-stream adaptive
    graph convolutional networks for skeleton-based action recognition. In: CVPR,
    pp. 12026–12035 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2020] Cheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., Lu,
    H.: Skeleton-based action recognition with shift graph convolutional network.
    In: CVPR, pp. 183–192 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Korban and Li [2020] Korban, M., Li, X.: Ddgcn: A dynamic directed graph convolutional
    network for action recognition. In: ECCV, pp. 761–776 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021] Chen, Y., Zhang, Z., Yuan, C., Li, B., Deng, Y., Hu, W.:
    Channel-wise topology refinement graph convolution for skeleton-based action recognition.
    In: ICCV, pp. 13359–13368 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chi et al. [2022] Chi, H.-g., Ha, M.H., Chi, S., Lee, S.W., Huang, Q., Ramani,
    K.: Infogcn: Representation learning for human skeleton-based action recognition.
    In: CVPR, pp. 20186–20196 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. [2022] Duan, H., Wang, J., Chen, K., Lin, D.: Dg-stgcn: dynamic
    spatial-temporal modeling for skeleton-based action recognition. arXiv preprint
    arXiv:2210.05895 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Wang, S., Zhang, Y., Wei, F., Wang, K., Zhao, M., Jiang,
    Y.: Skeleton-based action recognition via temporal-channel aggregation. arXiv
    preprint arXiv:2205.15936 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. [2023] Wen, Y.-H., Gao, L., Fu, H., Zhang, F.-L., Xia, S., Liu,
    Y.-J.: Motif-gcns with local and non-local temporal blocks for skeleton-based
    action recognition. TPAMI 45(2), 2009–2023 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2023] Lin, L., Zhang, J., Liu, J.: Actionlet-dependent contrastive
    learning for unsupervised skeleton-based action recognition. In: CVPR, pp. 2363–2372
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022] Li, Z., Gong, X., Song, R., Duan, P., Liu, J., Zhang, W.:
    SMAM: Self and mutual adaptive matching for skeleton-based few-shot action recognition.
    TIP 32, 392–402 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. [2023] Dai, M., Sun, Z., Wang, T., Feng, J., Jia, K.: Global spatio-temporal
    synergistic topology learning for skeleton-based action recognition. PR 140, 109540
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2023] Zhu, Y., Shuai, H., Liu, G., Liu, Q.: Multilevel spatial–temporal
    excited graph network for skeleton-based action recognition. TIP 32, 496–508 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shu et al. [2023] Shu, X., Xu, B., Zhang, L., Tang, J.: Multi-granularity anchor-contrastive
    representation learning for semi-supervised skeleton-based action recognition.
    TPAMI 45(6), 7559–7576 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Wu, L., Zhang, C., Zou, Y.: Spatiotemporal focus for skeleton-based
    action recognition. PR 136, 109231 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Zhang, P., Lan, C., Zeng, W., Xing, J., Xue, J., Zheng,
    N.: Semantics-guided neural networks for efficient skeleton-based human action
    recognition. In: CVPR, pp. 1112–1121 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. [2020] Ye, F., Pu, S., Zhong, Q., Li, C., Xie, D., Tang, H.: Dynamic
    gcn: Context-enriched topology learning for skeleton-based action recognition.
    In: ACM MM, pp. 55–63 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023] Wang, M., Ni, B., Yang, X.: Learning multi-view interactional
    skeleton graph for action recognition. TPAMI 45(6), 6940–6954 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023] Li, S., He, X., Song, W., Hao, A., Qin, H.: Graph diffusion
    convolutional network for skeleton based semantic recognition of two-person actions.
    TPAMI 45(7), 8477–8493 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2023] Xu, H., Gao, Y., Hui, Z., Li, J., Gao, X.: Language knowledge-assisted
    representation learning for skeleton-based action recognition. arXiv preprint
    arXiv:2305.12398 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023] Wang, X., Xu, X., Mu, Y.: Neural koopman pooling: Control-inspired
    temporal dynamics encoding for skeleton-based action recognition. In: CVPR, pp.
    10597–10607 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2023] Zhou, H., Liu, Q., Wang, Y.: Learning discriminative representations
    for skeleton based action recognition. In: CVPR, pp. 10608–10617 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. [2020] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al.: An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2020] Shi, L., Zhang, Y., Cheng, J., Lu, H.: Decoupled spatial-temporal
    attention network for skeleton-based action-gesture recognition. In: ACCV (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2021] Wang, Q., Peng, J., Shi, S., Liu, T., He, J., Weng, R.:
    Iip-transformer: Intra-inter-part transformer for skeleton-based action recognition.
    arXiv preprint arXiv:2110.13385 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ijaz et al. [2022] Ijaz, M., Diaz, R., Chen, C.: Multimodal transformer for
    nursing activity recognition. In: CVPR, pp. 2065–2074 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2021] Zhang, Y., Wu, B., Li, W., Duan, L., Gan, C.: Stst: Spatial-temporal
    specialized transformer for skeleton-based action recognition. In: ACMMM, pp.
    3229–3237 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2021] Shi, F., Lee, C., Qiu, L., Zhao, Y., Shen, T., Muralidhar,
    S., Han, T., Zhu, S.-C., Narayanan, V.: Star: Sparse transformer-based action
    recognition. arXiv preprint arXiv:2107.07089 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gedamu et al. [2023] Gedamu, K., Ji, Y., Gao, L., Yang, Y., Shen, H.T.: Relation-mining
    self-attention network for skeleton-based human action recognition. PR 139, 109455
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2022] Zhou, Y., Li, C., Cheng, Z.-Q., Geng, Y., Xie, X., Keuper,
    M.: Hypergraph transformer for skeleton-based action recognition. arXiv preprint
    arXiv:2211.09590 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. [2022] Qiu, H., Hou, B., Ren, B., Zhang, X.: Spatio-temporal tuples
    transformer for skeleton-based action recognition. arXiv preprint arXiv:2201.02849
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kong et al. [2022] Kong, J., Bian, Y., Jiang, M.: Mtt: Multi-scale temporal
    transformer for skeleton-based action recognition. IEEE Signal Processing Letters
    29, 528–532 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Zhang, J., Jia, Y., Xie, W., Tu, Z.: Zoom transformer for
    skeleton-based group activity recognition. TCSVT 32(12), 8646–8659 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2022] Gao, Z., Wang, P., Lv, P., Jiang, X., Liu, Q., Wang, P.,
    Xu, M., Li, W.: Focal and global spatial-temporal transformer for skeleton-based
    action recognition. In: ACCV, pp. 382–398 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2022] Liu, Y., Zhang, H., Xu, D., He, K.: Graph transformer network
    with temporal kernel attention for skeleton-based action recognition. Knowledge-Based
    Systems 240, 108146 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang et al. [2022] Pang, Y., Ke, Q., Rahmani, H., Bailey, J., Liu, J.: Igformer:
    Interaction graph transformer for skeleton-based human interaction recognition.
    In: ECCV, pp. 605–622 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. [2023] Duan, H., Xu, M., Shuai, B., Modolo, D., Tu, Z., Tighe,
    J., Bergamo, A.: Skeletr: Towards skeleton-based action recognition in the wild.
    In: ICCV, pp. 13634–13644 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. [2022] Kim, B., Chang, H.J., Kim, J., Choi, J.Y.: Global-local motion
    transformer for unsupervised skeleton-based action learning. In: ECCV, pp. 209–225
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. [2023] Dong, J., Sun, S., Liu, Z., Chen, S., Liu, B., Wang, X.:
    Hierarchical contrast for unsupervised skeleton-based action representation learning.
    In: AAAI, pp. 525–533 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shah et al. [2023] Shah, A., Roy, A., Shah, K., Mishra, S., Jacobs, D., Cherian,
    A., Chellappa, R.: Halp: Hallucinating latent positives for skeleton-based self-supervised
    learning of actions. In: CVPR, pp. 18846–18856 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2021] Cheng, Y.-B., Chen, X., Zhang, D., Lin, L.: Motion-transformer:
    Self-supervised pre-training for skeleton-based action recognition. In: ACM MM,
    pp. 1–6 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Wu, W., Hua, Y., Zheng, C., Wu, S., Chen, C., Lu, A.: Skeletonmae:
    Spatial-temporal masked autoencoders for self-supervised skeleton action recognition.
    In: ICMEW, pp. 224–229 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hua et al. [2023] Hua, Y., Wu, W., Zheng, C., Lu, A., Liu, M., Chen, C., Wu,
    S.: Part aware contrastive learning for self-supervised action recognition. In:
    IJCAI, pp. 855–863 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson and Everingham [2010] Johnson, S., Everingham, M.: Clustered pose and
    nonlinear appearance models for human pose estimation. In: BMVC, p. 5 (2010)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson and Everingham [2011] Johnson, S., Everingham, M.: Learning effective
    human pose estimation from inaccurate annotation. In: CVPR, pp. 1465–1472 (2011)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sapp and Taskar [2013] Sapp, B., Taskar, B.: Modec: Multimodal decomposable
    models for human pose estimation. In: CVPR, pp. 3674–3681 (2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andriluka et al. [2014] Andriluka, M., Pishchulin, L., Gehler, P., Schiele,
    B.: 2d human pose estimation: New benchmark and state of the art analysis. In:
    CVPR, pp. 3686–3693 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2014] Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context.
    In: ECCV, pp. 740–755 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. [2017] Gong, K., Liang, X., Zhang, D., Shen, X., Lin, L.: Look
    into person: Self-supervised structure-sensitive learning and a new benchmark
    for human parsing. In: CVPR, pp. 932–940 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019] Li, J., Wang, C., Zhu, H., Mao, Y., Fang, H.-S., Lu, C.: Crowdpose:
    Efficient crowded scenes pose estimation and a new benchmark. In: CVPR, pp. 10863–10872
    (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2013] Zhang, W., Zhu, M., Derpanis, K.G.: From actemes to action:
    A strongly-supervised representation for detailed action understanding. In: ICCV,
    pp. 2248–2255 (2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andriluka et al. [2018] Andriluka, M., Iqbal, U., Insafutdinov, E., Pishchulin,
    L., Milan, A., Gall, J., Schiele, B.: Posetrack: A benchmark for human pose estimation
    and tracking. In: CVPR, pp. 5167–5176 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Doering et al. [2022] Doering, A., Chen, D., Zhang, S., Schiele, B., Gall,
    J.: Posetrack21: A dataset for person search, multi-object tracking and multi-person
    pose tracking. In: CVPR, pp. 20963–20972 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sigal et al. [2010] Sigal, L., Balan, A.O., Black, M.J.: Humaneva: Synchronized
    video and motion capture dataset and baseline algorithm for evaluation of articulated
    human motion. IJCV 87(1-2), 4–27 (2010)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ionescu et al. [2013] Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.:
    Human3\. 6m: Large scale datasets and predictive methods for 3d human sensing
    in natural environments. TPAMI 36(7), 1325–1339 (2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joo et al. [2017] Joo, H., Simon, T., Li, X., Liu, H., Tan, L., Gui, L., Banerjee,
    S., Godisart, T., Nabbe, B., Matthews, I., et al.: Panoptic studio: A massively
    multiview system for social interaction capture. TPAMI 41(1), 190–204 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'von Marcard et al. [2018] Marcard, T., Henschel, R., Black, M.J., Rosenhahn,
    B., Pons-Moll, G.: Recovering accurate 3d human pose in the wild using imus and
    a moving camera. In: ECCV, pp. 601–617 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sapp et al. [2011] Sapp, B., Weiss, D., Taskar, B.: Parsing human motion with
    stretchable models. In: CVPR 2011, pp. 1281–1288 (2011)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Berclaz et al. [2011] Berclaz, J., Fleuret, F., Turetken, E., Fua, P.: Multiple
    object tracking using k-shortest paths optimization. TPAMI 33(9), 1806–1819 (2011)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramakrishna et al. [2013] Ramakrishna, V., Kanade, T., Sheikh, Y.: Tracking
    human pose by tracking symmetric parts. In: CVPR, pp. 3728–3735 (2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weiss et al. [2010] Weiss, D., Sapp, B., Taskar, B.: Sidestepping intractable
    inference with structured ensemble cascades. NIPS 23 (2010)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Belagiannis et al. [2014] Belagiannis, V., Amin, S., Andriluka, M., Schiele,
    B., Navab, N., Ilic, S.: 3d pictorial structures for multiple human pose estimation.
    In: CVPR, pp. 1669–1676 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Müller et al. [2007] Müller, M., Röder, T., Clausen, M., Eberhardt, B., Krüger,
    B., Weber, A.: Mocap database hdm05. Institut für Informatik II, Universität Bonn
    2(7) (2007)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2010] Li, W., Zhang, Z., Liu, Z.: Action recognition based on a
    bag of 3D points. In: CVPRW, pp. 9–14 (2010)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fothergill et al. [2012] Fothergill, S., Mentis, H., Kohli, P., Nowozin, S.:
    Instructing people for training gestural interactive systems. In: Proceedings
    of the SIGCHI Conference on Human Factors in Computing Systems, pp. 1737–1746
    (2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bloom et al. [2012] Bloom, V., Makris, D., Argyriou, V.: G3d: A gaming action
    dataset and real time action recognition evaluation framework. In: CVPR, pp. 7–12
    (2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yun et al. [2012] Yun, K., Honorio, J., Chattopadhyay, D., Berg, T.L., Samaras,
    D.: Two-person interaction detection using body-pose features and multiple instance
    learning. In: CVPRW, pp. 28–35 (2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. [2012] Xia, L., Chen, C.-C., Aggarwal, J.: View invariant human
    action recognition using histograms of 3D joints. In: CVPRW, pp. 20–27 (2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2014] Wang, J., Nie, X., Xia, Y., Wu, Y., Zhu, S.-C.: Cross-view
    action modeling, learning and recognition. In: CVPR, pp. 2649–2656 (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2015] Chen, C., Jafari, R., Kehtarnavaz, N.: Utd-mhad: A multimodal
    dataset for human action recognition utilizing a depth camera and a wearable inertial
    sensor. In: ICIP, pp. 168–172 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. [2015] Hu, J.-F., Zheng, W.-S., Lai, J., Zhang, J.: Jointly learning
    heterogeneous features for RGB-D activity recognition. In: CVPR (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chunhui et al. [2017] Chunhui, L., Yueyu, H., Yanghao, L., Sijie, S., Jiaying,
    L.: Pku-mmd: A large scale benchmark for continuous multi-modal human action understanding.
    arXiv preprint arXiv:1703.07475 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kay et al. [2017] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier,
    C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.:
    The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2019] Liu, J., Shahroudy, A., Perez, M., Wang, G., Duan, L.-Y.,
    Kot, A.C.: Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding.
    TPAMI 42(10), 2684–2701 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2017] Li, B., Dai, Y., Cheng, X., Chen, H., Lin, Y., He, M.: Skeleton
    based action recognition using translation-scale invariant image mapping and multi-scale
    deep cnn. In: ICMEW, pp. 601–604 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2020] Cheng, K., Zhang, Y., Cao, C., Shi, L., Cheng, J., Lu,
    H.: Decoupling gcn with dropgraph module for skeleton-based action recognition.
    In: ECCV, pp. 1–18 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. [2022] Jiang, Y., Sun, Z., Yu, S., Wang, S., Song, Y.: A graph
    skeleton transformer network for action recognition. Symmetry 14(8), 1547 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. [2019] Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast
    and robust multi-person 3d pose estimation from multiple views. In: CVPR, pp.
    7792–7801 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. [2020] Tu, H., Wang, C., Zeng, W.: Voxelpose: Towards multi-camera
    3d human pose estimation in wild environment. In: ECCV, pp. 197–212 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2021] Zhang, J., Cai, Y., Yan, S., Feng, J., et al.: Direct multi-view
    multi-person 3d pose estimation. NIPS 34, 13153–13164 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shah et al. [2019] Shah, S., Jain, N., Sharma, A., Jain, A.: On the robustness
    of human pose estimation. In: CVPRW (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Zhang, Z., Wang, C., Qin, W., Zeng, W.: Fusing wearable
    imus with multi-view images for human pose estimation: A geometric approach. In:
    CVPR, pp. 2200–2209 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022a] Wang, C., Zhang, F., Zhu, X., Ge, S.S.: Low-resolution
    human pose estimation. PR 126(10857), 108579 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022b] Wang, Z., Luo, H., Wang, P., Ding, F., Wang, F., Li, H.:
    Vtc-lfc: Vision transformer compression with low-frequency components. NIPS 35,
    13974–13988 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. [2022] Jiang, W., Jin, S., Liu, W., Qian, C., Luo, P., Liu, S.:
    Posetrans: A simple yet effective pose transformation augmentation for human pose
    estimation. In: ECCV, pp. 643–659 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Zhang, J., Gong, K., Wang, X., Feng, J.: Learning to augment
    poses for 3d human pose estimation in images and videos. TPAMI 45(8), 10012–10026
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. [2023] Jiang, Z., Zhou, Z., Li, L., Chai, W., Yang, C.-Y., Hwang,
    J.-N.: Back to optimization: Diffusion-based zero-shot 3d human pose estimation.
    arXiv preprint arXiv:2307.03833 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gong et al. [2023] Gong, J., Foo, L.G., Fan, Z., Ke, Q., Rahmani, H., Liu,
    J.: Diffpose: Toward more reliable 3d pose estimation. CVPR (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2022] Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan,
    Z., Luo, P., Liu, W., Wang, X.: Bytetrack: Multi-object tracking by associating
    every detection box. In: ECCV, pp. 1–21 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radford et al. [2021] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh,
    G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning
    transferable visual models from natural language supervision. In: International
    Conference on Machine Learning, pp. 8748–8763 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khirodkar et al. [2023] Khirodkar, R., Bansal, A., Ma, L., Newcombe, R., Vo,
    M., Kitani, K.: Egohumans: An egocentric 3d multi-human benchmark. arXiv preprint
    arXiv:2305.16487 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ulhaq et al. [2022] Ulhaq, A., Akhtar, N., Pogrebna, G., Mian, A.: Vision transformers
    for action recognition: A survey. arXiv preprint arXiv:2209.05700 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qing et al. [2023] Qing, Z., Zhang, S., Huang, Z., Wang, X., Wang, Y., Lv,
    Y., Gao, C., Sang, N.: Mar: Masked autoencoders for efficient action recognition.
    IEEE Transactions on Multimedia (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. [2023] Kang, M.-S., Kang, D., Kim, H.: Efficient skeleton-based
    action recognition via joint-mapping strategies. In: WACV, pp. 3403–3412 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. [2021] Gupta, P., Sharma, D., Sarvadevabhatla, R.K.: Syntactically
    guided generative embeddings for zero-shot skeleton action recognition. In: ICIP,
    pp. 439–443 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2023] Zhou, Y., Qiang, W., Rao, A., Lin, N., Su, B., Wang, J.:
    Zero-shot skeleton-based action recognition via mutual information estimation
    and maximization. arXiv preprint arXiv:2308.03950 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
