- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:42:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2302.02515] Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.02515](https://ar5iv.labs.arxiv.org/html/2302.02515)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navid Mohammadi Foumani [navid.foumani@monash.edu.com](mailto:navid.foumani@monash.edu.com)
    [0000-0003-2475-6040](https://orcid.org/0000-0003-2475-6040 "ORCID identifier")
    Monash UniversityAustralia ,  Lynn Miller [lynn.miller1@monash.edu](mailto:lynn.miller1@monash.edu)
    Monash UniversityAustralia ,  Chang Wei Tan [chang.tan@monash.edu](mailto:chang.tan@monash.edu)
    Monash UniversityAustralia ,  Geoffrey I. Webb [geoff.webb@monash.edu](mailto:geoff.webb@monash.edu)
    Monash UniversityAustralia ,  Germain Forestier [germain.forestier@uha.fr](mailto:germain.forestier@uha.fr)
    Monash UniversityAustralia IRIMAS, University of Haute-AlsaceFrance  and  Mahsa
    Salehi [mahsa.salehi@monash.edu](mailto:mahsa.salehi@monash.edu) Monash UniversityAustralia(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Time Series Classification and Extrinsic Regression are important and challenging
    machine learning tasks. Deep learning has revolutionized natural language processing
    and computer vision and holds great promise in other fields such as time series
    analysis where the relevant features must often be abstracted from the raw data
    but are not known a priori. This paper surveys the current state of the art in
    the fast-moving field of deep learning for time series classification and extrinsic
    regression. We review different network architectures and training methods used
    for these tasks and discuss the challenges and opportunities when applying deep
    learning to time series data. We also summarize two critical applications of time
    series classification and extrinsic regression, human activity recognition and
    satellite earth observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning, Time series, Classification, Extrinsic regression, Review^†^†copyright:
    acmcopyright^†^†journalyear: 2023^†^†doi: XXXXXXX.XXXXXXX^†^†ccs: Computing methodologies Machine
    learning approaches^†^†ccs: Computing methodologies Supervised learning'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series analysis has been identified as one of the ten most challenging
    research issues in the field of data mining in the 21st century ([yang200610,](#bib.bib1)
    ). Time series classification (TSC) is a key time series analysis task ([esling2012time,](#bib.bib2)
    ). TSC builds a machine learning model to predict categorical class labels for
    data consisting of ordered sets of real-valued attributes. The many applications
    of time series analysis include human activity recognition ([nweke2018deep,](#bib.bib3)
    ; [wang2019deep,](#bib.bib4) ; [chen2021deep,](#bib.bib5) ), diagnosis based on
    electronic health records ([schirrmeister2017deep,](#bib.bib6) ; [rajkomar2018scalable,](#bib.bib7)
    ), and systems monitoring problems ([bagnall2018uea,](#bib.bib8) ). The wide variety
    of dataset types in the University of California, Riverside (UCR) ([dau2019ucr,](#bib.bib9)
    ) and University of East Anglia (UEA) ([bagnall2018uea,](#bib.bib8) ) benchmark
    archive further illustrates the breadth of TSC applications. Time series extrinsic
    regression (TSER) ([tan2021time,](#bib.bib10) ) is the counterpart of TSC for
    which the output is numeric rather than categorical. It should be noted that the
    TSER is not a forecasting method but rather a method for understanding the relationship
    between the time series and the extrinsic variable. TSER is an emerging field
    with great potential to be used in a wide range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has been very successful, especially in computer vision and natural
    language processing. Many modern applications integrate deep learning. Deep learning
    can autonomously learn informative features from raw data, eliminating the need
    for manual feature engineering. Consequently, there has been much interest in
    developing deep TSC and TSER due to their ability to learn relevant latent feature
    representations. It is worth noting that the majority of TSC and TSER research
    has focused on non-deep learning approaches. A recent benchmark ([middlehurst2023bake,](#bib.bib11)
    ) shows that the deep learning method (InceptionTime ([fawaz2020inceptiontime,](#bib.bib12)
    )) is competitive but did not outperform the state of the art on benchmarking
    archives. One reason is that the popular UCR and UEA benchmarking archives were
    not designed for deep learning models. In particular, they are relatively small,
    while deep learning often excels when data quantities are large. Deep learning
    can also benefit from heightened compatibility with current hardware, particularly
    GPUs, leading to fast and efficient execution. Their exceptional scalability further
    allows seamless handling of growing data volumes and computational complexity,
    reinforcing their versatility in processing large datasets. Indeed, ConvTran ([Foumani2023,](#bib.bib13)
    ), a recent deep architecture for TSC, outperforms one of the fastest conventional
    models, ROCKET ([dempster2019rocket,](#bib.bib14) ), in terms of both speed and
    accuracy when there are more than 10k training samples.
  prefs: []
  type: TYPE_NORMAL
- en: A highly influential review paper on deep learning-based TSC ([fawaz2019deep,](#bib.bib15)
    ) was published in 2019\. However, the field of research is very fast-moving,
    and that prior survey does not cover the current state of the art. For example,
    it does not include InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) ), a
    system that consistently outperforms ResNet ([wang2017time,](#bib.bib16) ), the
    best performing system from the prior survey. Nor does it cover attention models,
    which have received huge interest in recent years and have shown excellent capacity
    to model long-range dependencies in sequential data, and are well suited for time-series
    modeling ([wen2022transformers,](#bib.bib17) ). Many attention variants have been
    proposed to address particular challenges in time series modeling and have been
    successfully applied to TSC ([hao2020new,](#bib.bib18) ; [zerveas2021transformer,](#bib.bib19)
    ; [Foumani2023,](#bib.bib13) ). Moreover, the previous survey does not include
    self-supervised learning, which is emerging as a new paradigm ([liu2021self,](#bib.bib20)
    ). Self-supervised learning induces supervision by designing pretext tasks instead
    of relying on predefined prior knowledge and has shown very promising results,
    especially in datasets with a low label regime ([eldele2021time,](#bib.bib21)
    ; [yang2021voice2series,](#bib.bib22) ; [yue2022ts2vec,](#bib.bib23) ; [foumani2023series2vec,](#bib.bib24)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: In light of the emergence of attention mechanisms, self-supervised learning,
    and various new network configurations for TSC, a systematic and comprehensive
    survey on deep learning in TSC would greatly benefit the time series community.
    This article aims to fill that gap by summarizing recent developments in deep
    learning-based time series analytics, specifically TSC and TSER. Following definitions
    and a brief introduction to the time series classification and extrinsic regression
    tasks, we propose a new taxonomy based on various methodological perspectives.
    Diverse architectures, including multilayer perceptrons (MLP), convolutional neural
    networks (CNN), recurrent neural networks (RNN), Graph Neural Network (GNN), and
    attention-based models, are discussed, along with refinements made to improve
    performance. Additionally, various types of self-supervised learning pretexts,
    such as contrastive learning and self-prediction, are explored. We also conduct
    a review of useful data augmentation and transfer learning strategies for time
    series data. Furthermore, we provide a summary of two key applications of TSC
    and TSER, namely Human Activity Recognition and Earth Observation.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background and Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section begins by providing the necessary definitions and background information
    to understand the topic of training deep neural networks (DNNs) for TSC and TSER
    tasks. We begin by defining key terms and concepts, such as time series data and
    time series supervised learning. Finally, we present our proposed taxonomy of
    the different deep learning methods that have been used for TSC and TSER tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Time series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time series data are sequences of data points indexed by time.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A time series $X$ is an ordered collection of $T$ pairs of measurements and
    timestamps,
  prefs: []
  type: TYPE_NORMAL
- en: $X=\{(x_{1},t_{1}),(x_{2},t_{2}),...,(x_{T},t_{T})\}$, where $x_{i}\in\mathbb{R}^{D}$
    and $t_{1}$ to $t_{T}$ are the timestamps for some measurements $x_{1}$ to $x_{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: Each $x_{i}$ is a $D$-dimensional vector of values, one for each feature captured
    in the series. When $D=1$ the series is called *univariate*. When $D>1$ the series
    is called *multivariate*.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Time series supervised learning tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This paper focuses on two time series learning tasks: time series extrinsic
    regression and time series classification. Classification and regression are both
    supervised learning tasks that learn the relationship between a target variable
    and a set of time series. We consider learning from a dataset $D=\left\{(X_{1},Y_{1}),(X_{2},Y_{2}),...,(X_{N},Y_{N})\right\}$
    of $N$ time series where $Y_{i}$ denotes the target variable for each $X_{i}$.
    It is important to note that for ease of exposition, we assume in our discussion
    that the series are of the same length, but most methods extend trivially to the
    case of unequal-length series. The main difference between TSER and TSC is that
    TSC predicts a categorical value for a time series from a set of finite categories,
    while TSER predicts a continuous value for a variable external to the input time
    series. Typically $Y_{i}$ is a one hot encoded vector for TSC or a numeric value
    for TSER.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of deep learning, a supervised learning model is a neural network
    that executes the following functions to map the input time series to a target
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $f_{L}(\theta_{L},X)=f_{L-1}(\theta_{L-1},f_{L-2}(\theta_{L-2},\ldots,f_{1}(\theta_{1},X)))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $f_{i}$ represents the non-linear function and $\theta_{i}$ denotes the
    parameters at layer $i$. For TSC the neural network model is trained to map a
    time series dataset $D$ to a set of class labels $Y$ with $C$ class labels. After
    training, the neural network outputs a vector of $C$ values that estimates the
    probability of a series $X$ belonging to each class. This is typically achieved
    using the softmax activation function in the final layer of the neural network.
    The softmax function estimates probabilities for all of the dependent classes
    such that they always sum to 1 across all classes. The cross-entropy loss is commonly
    used for training neural networks with softmax outputs or classification type
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, TSER trains the neural network model to map a time series
    dataset $D$ to a set of numeric values $Y$. Instead of outputting probabilities,
    a regression neural network outputs a numerical value for the time series. It
    is typically used with a linear activation function in the final layer of the
    neural network. However, any non-linear functions with a single value output such
    as sigmoid, or ReLU can also be used. A regression neural network typically trains
    using the mean square error or mean absolute error loss function. However, depending
    on the distribution of the target variable and the choice of final activation
    functions, other loss functions can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. TSC and TSER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TSC is a fast-growing field, with hundreds of papers being published every
    year ([bagnall2018uea,](#bib.bib8) ; [dau2019ucr,](#bib.bib9) ; [bagnall2017great,](#bib.bib25)
    ; [fawaz2019deep,](#bib.bib15) ; [ruiz2020great,](#bib.bib26) ). The majority
    of work in TSC are non-deep learning based. In this survey, we focus on deep learning
    approaches and refer interested readers to Appendix [A](#A1 "Appendix A Non-Deep
    Learning Time Series Classification ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey") and benchmark papers ([middlehurst2023bake,](#bib.bib11)
    ; [bagnall2017great,](#bib.bib25) ; [ruiz2020great,](#bib.bib26) ) for more details
    on non-deep learning approaches. Most deep learning approaches to TSC have real-valued
    outputs that are mapped to a class label. TSER ([tan2021time,](#bib.bib10) ; [tan2020monash,](#bib.bib27)
    ) is a less widely studied task in which the predicted values are numeric, rather
    than categorical. While the majority of the architectures covered in this survey
    were designed for TSC, it is important to note that it is trivial to adapt most
    of them for TSER.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning-based TSC methods can be classified into two main types: generative
    and discriminative ([langkvist2014review,](#bib.bib28) ). In the TSC community,
    generative methods are often considered model-based ([bagnall2017great,](#bib.bib25)
    ), aiming to understand and model the joint probability distribution of input
    series $X$ and output labels $Y$, denoted as $p(X,Y)$. On the other hand, discriminative
    models focus on modeling the conditional probability of output labels $Y$ given
    input series $X$, expressed as $p(Y|X)$.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative models, such as the Stacked Denoising Auto-encoders (SDAE) have been
    proposed by Bengio et al. ([bengio2013generalized,](#bib.bib29) ) to identify
    the salient structure of input data distributions, and Hu et al. ([hu2016transfer,](#bib.bib30)
    ) used the same model for the pre-training phase before training a classifier
    for time series tasks. A universal neural network encoder has been developed to
    convert variable-length time series to a fixed-length representation ([serra2018towards,](#bib.bib31)
    ). Also, a Deep Belief Network (DBN) combined with a transfer learning method
    was used in an unsupervised manner to model the latent features of time series ([banerjee2019deep,](#bib.bib32)
    ). An Echo State Network (ESN) has been used to learn the appropriate time series
    representation by reconstructing the original raw time series prior to training
    the classifier ([aswolinskiy2018time,](#bib.bib33) ). Generative Adversarial Networks
    (GANs) are one of the popular generative models that generate new examples by
    learning to discriminate between real and synthetic examples. Various GANs have
    been developed for time series and have been reviewed in a recent survey ([GanSurvey2021,](#bib.bib34)
    ). Often, implementing generative methods is more complex due to an additional
    step of training. Furthermore, generative methods are typically less efficient
    than discriminative methods, which directly map raw time series to class probability
    distributions. Due to these barriers, researchers tend to focus on discriminative
    methods. Therefore, this survey mainly focuses on the end-to-end discriminative
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. Taxonomy of Deep Learning in TSC and TSER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: 'for tree= parent anchor=children, child anchor=parent, anchor=north, draw,
    align=left, inner sep=1.75pt, rounded corners=2pt, font=, , where level=0s sep=10pt,fill=red!5!white!80!green!40,
    where level=1s sep=6pt,fill=red!60!white!80!yellow!40, where level=2s sep=2pt,fill=red!40!white!80!blue!40,
    where level=3s sep=2pt,fill=red!10!white!80!blue!40, where level=4fill=red!5!white!80!green!40,
    forked edges, [Deep Learning methods for Time Series Classification and Extrinsic
    Regression, [Supervised (Sec.[3](#S3 "3\. Supervised Models ‣ Deep Learning for
    Time Series Classification and Extrinsic Regression: A Current Survey")), name=S,
    s sep=6pt, [Multi-Layer'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron, name=MLP,rotate=0,anchor=north] [Convolutional
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network, name=CNN, for tree=grow’=0,folder,draw, [Adapted
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network, name=ACNN] [Imaging Time
  prefs: []
  type: TYPE_NORMAL
- en: Series, name=ITS] [Multi-Scale
  prefs: []
  type: TYPE_NORMAL
- en: Operation, name=MSO] ] [Recurrent
  prefs: []
  type: TYPE_NORMAL
- en: Neural
  prefs: []
  type: TYPE_NORMAL
- en: Network, name=RNN,for tree=grow’=0,folder,draw, [Vanilla Recurrent
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network, name=RRN] [Long Short Term Memory, name=LSTM] [Gated Recurrent
    Unit, name=GRU] [Hybrid, name=RCNN] ] [Graph Neural
  prefs: []
  type: TYPE_NORMAL
- en: Network, name=GNN,rotate=0,anchor=north] [Attention, name=Attn, for tree=grow’=0,folder,draw,
    [Self-Attention, name=SA] [Transformers, name=Trans] ] ] [Self-Supervised
  prefs: []
  type: TYPE_NORMAL
- en: '(Sec.[4](#S4 "4\. Self-supervised Models ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")), name=SS, for tree=grow’=0,folder,draw,
    [Self-Prediction, name=SSCNN] [Contrastive-'
  prefs: []
  type: TYPE_NORMAL
- en: Learning, name=SSAttn] [Other pretext
  prefs: []
  type: TYPE_NORMAL
- en: tasks, name=SSGNN, ] ] [Data
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation
  prefs: []
  type: TYPE_NORMAL
- en: '(Sec.[5](#S5 "5\. Data augmentation ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")), name=DA, for tree=grow’=0,folder,draw,
    [Random'
  prefs: []
  type: TYPE_NORMAL
- en: Transformations, name=RT] [Window methods, name=WM] [Averaging methods, name=AM]
    ] [Transfer
  prefs: []
  type: TYPE_NORMAL
- en: Learning
  prefs: []
  type: TYPE_NORMAL
- en: '(Sec.[6](#S6 "6\. Transfer learning ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")), name=TL,rotate=0,anchor=north ]
    ]'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Taxonomy of Deep Learning (DL) for TSC/TSER from the perspectives
    of network configuration and application domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide an organized summary of the existing deep learning models for TSC,
    we propose a taxonomy that categorizes these models based on deep learning methods
    and application domains. This taxonomy is illustrated in Fig. [1](#S2.F1 "Figure
    1 ‣ 2.4\. Taxonomy of Deep Learning in TSC and TSER ‣ 2\. Background and Definitions
    ‣ Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey"). In section [3](#S3 "3\. Supervised Models ‣ Deep Learning for Time Series
    Classification and Extrinsic Regression: A Current Survey"), we review various
    network architectures used for TSC, including multilayer perceptrons, convolutional
    neural networks, recurrent neural networks, graph neural networks, and attention-based
    models. We also discuss refinements made to these models to improve their performance
    on time series tasks. Additionally, various types of self-supervised learning
    pretexts, such as contrastive learning and self-prediction, are explored in section
    [4](#S4 "4\. Self-supervised Models ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey"). We also conduct a review of useful
    data augmentation and transfer learning strategies for time series data in section [5](#S5
    "5\. Data augmentation ‣ Deep Learning for Time Series Classification and Extrinsic
    Regression: A Current Survey") and  [6](#S6 "6\. Transfer learning ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey"). In
    addition to methods, we summarize key applications of TSC and TSER in section
    [7](#S7 "7\. Applications - recent developments and challenges ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey") of
    this paper. These applications include human activity recognition and satellite
    earth observation, which are important and challenging tasks that can benefit
    from the use of deep learning models. Overall, our proposed taxonomy and the discussions
    in these sections provide a comprehensive overview of the current state of the
    art in deep learning for time series analysis and outline future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Supervised Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section reviews the deep learning-based models for TSC and discusses their
    architectures by highlighting their strengths as well as limitations. More details
    on deep model architectures and their adaptations to time series data are available
    in Appendix [B](#A2 "Appendix B DNN Architectures for Time Series ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Multi-Layer Perceptron (MLP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most straightforward neural network architecture is a fully connected network
    (FC), also called a multilayer perceptron (MLP). The number of layers and neurons
    are defined as hyperparameters in MLP models. However, studies such as auto-adaptive
    MLP ([del2021auto,](#bib.bib35) ) have attempted to determine the number of neurons
    in the hidden layers automatically, based on the nature of the training time series
    data. This allows the network to adapt to the training data’s characteristics
    and optimize its performance on the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main limitations of using multilayer perceptrons (MLPs) for time
    series data is that they are not well-suited to capturing the temporal dependencies
    in this type of data. MLPs are feedforward networks that process input data in
    a fixed and predetermined order without considering the temporal relationships
    between the input values. Various studies used MLPs alongside other feature extractors
    like Dynamic Time Warping (DTW) to address this problem ([iwana2016robust,](#bib.bib36)
    ; [iwana2020dtw,](#bib.bib37) ). DTW-NN is a feedforward neural network that exploits
    DTW’s elastic matching ability to dynamically align a layer’s inputs to the weights
    instead of using a fixed and predetermined input-to-weight mapping. This weight
    alignment replaces the standard dot product within a neuron with DTW. In this
    way, the DTW-NN is able to tackle difficulties with time series recognition, such
    as temporal distortions and variable pattern length within a feedforward architecture ([iwana2020dtw,](#bib.bib37)
    ). Similarly, Symbolic Aggregate Approximation (SAX) is used to transform time
    series into a symbolic representation and produce sequences of words based on
    the symbolic representation ([tabassum2022time,](#bib.bib38) ). The symbolic time
    series-based words are later used as input for training a two-layer MLP for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Although the models mentioned above attempt to resolve the shortage of capturing
    temporal dependencies in MLP models, they still have other limitations on capturing
    time-invariant features ([wang2017time,](#bib.bib16) ). Additionally, MLP models
    do not have the ability to process input data in a hierarchical or multi-scale
    manner. Time series data often exhibits patterns and structures at different scales,
    such as long-term trends and short-term fluctuations. MLP models fail to capture
    these patterns, as they are only able to process input data in a single, fixed-length
    representation. In addition, MLPs may encounter difficulties when confronted with
    irregularly sampled time series data, where observations are not uniformly recorded
    in time. Many other deep learning models are better suited to handle time series
    data, such as recurrent neural networks (RNNs), convolutional neural networks
    (CNNs), and transformers, specifically designed to capture the temporal dependencies
    and patterns in time series data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. CNN based models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several improvements have been made to CNN since the success of AlexNet in
    2012 ([krizhevsky2012imagenet,](#bib.bib39) ) such as using deeper networks, applying
    smaller and more efficient convolutional filters, adding pooling layers to reduce
    the dimensionality of the feature maps, and utilizing batch normalization to improve
    the stability of training ([gu2018recent,](#bib.bib40) ). They have been demonstrated
    to be very successful in many domains, such as computer vision, speech recognition,
    and natural language processing problems ([lecun2015deep,](#bib.bib41) ; [gu2018recent,](#bib.bib40)
    ). As a result of the success of CNN architectures in these various domains, researchers
    have also started adopting them for TSC. See table [1](#S3.T1 "Table 1 ‣ 3.2.2\.
    Imaging time series ‣ 3.2\. CNN based models ‣ 3\. Supervised Models ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey") for
    a list of reviewed CNN models in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Adapted CNNs for TSC and TSER
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section presents the first category, which we refer to as Adapted CNNs
    for TSC and TSER. The papers discussed here are mostly adaptations without any
    particular preprocessing or mathematical characteristics, such as transforming
    the series to an image or using multi-scale convolution and therefore do not fit
    into one of the other categories.
  prefs: []
  type: TYPE_NORMAL
- en: The first CNN for TSC was the Multi-Channel Deep Convolutional Neural Network
    (MC-DCNN) ([zheng2014time,](#bib.bib42) ). It handles multivariate data by independently
    applying convolutions to each input channel. Each input dimension undergoes two
    convolutional stages with ReLU activation, followed by max pooling. The output
    from each dimension is concatenated and passed to a fully connected layer which
    is then fed to a final softmax classifier for classification. Similar to MC-DCNN,
    a three-layer convolution neural network was proposed for Human activity recognition
    (MC-CNN)([yang2015deep,](#bib.bib43) ). Unlike the MC-DCNN, this model applies
    1D convolutions to all input channels simultaneously to capture the temporal and
    spatial relationships in the early stages. The 2-stage version of MC-CNN architecture
    was used by Zhao et al. ([zhao2017convolutional,](#bib.bib44) ) on the earliest
    version of the UCR Time Series Data Mining Archive. The authors also conducted
    an ablation study to evaluate the performance of the CNN models with differing
    numbers of convolution filters and pooling types.
  prefs: []
  type: TYPE_NORMAL
- en: Fully Convolutional Networks (FCN) ([long2015fully,](#bib.bib45) ), and Residual
    Networks (Resnet) ([he2016deep,](#bib.bib46) ) are two deep neural networks that
    are commonly used for image and video recognition tasks and have been adapted
    for end-to-end TSC ([wang2017time,](#bib.bib16) ). FCNs are a variant of convolutional
    neural networks (CNNs) designed to operate on inputs of arbitrary size rather
    than being constrained to fixed-size inputs like traditional CNNs. This is achieved
    by replacing the fully connected layers in a traditional CNN with a Global Average
    Pooling (GAP) ([long2015fully,](#bib.bib45) ). FCN was adapted for univariate
    TSC ([wang2017time,](#bib.bib16) ), and similar to the original model, it contains
    three convolution blocks where each block contains a convolution layer followed
    by batch normalization and ReLU activation. Each block uses 128, 256, 128 filters
    with 8, 5, 3 filter lengths respectively. The output from the last convolution
    block is averaged with a GAP layer and passed to a final softmax classifier. The
    GAP layer has the property of reducing the spatial dimensions of the input while
    retaining the channel-wise information, which allows it to be used in conjunction
    with a class activation map (CAM) ([zhou2016learning,](#bib.bib47) ) to highlight
    the regions in the input that are most important for the predicted class. This
    can provide useful insights into how the network is making its predictions and
    help identify potential improvement areas. Similar to FCN, the Residual Network
    (ResNet) was also proposed in ([wang2017time,](#bib.bib16) ) for univariate TSC.
    ResNet is a deep architecture containing three residual blocks followed by a GAP
    layer and a softmax classifier. It uses residual connections between blocks to
    reduce the vanishing gradient effect that affects deep learning models. The structure
    of each residual block is similar to the FCN architecture, containing three convolution
    layers followed by batch normalization and ReLU activation. Each convolution layer
    uses 64 filters with 8, 5, 3 filter lengths, respectively. ResNet was found to
    be one of the most accurate deep learning TSC architectures on 85 univariate TSC
    datasets ([fawaz2019deep,](#bib.bib15) ; [bagnall2017great,](#bib.bib25) ). Additionally,
    integration of ResNet and FCN has been proposed to combine the strength of both
    networks ([zou2019integration,](#bib.bib48) ).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to adapting the network architecture, some research has focused
    on modifying the convolution kernel to suit TSC tasks better. Dilated convolutions
    neural networks (DCNNs) ([li2018csrnet,](#bib.bib49) ) are a type of CNN that
    uses dilated convolutions to increase the receptive field of the network without
    increasing the number of parameters. Dilated convolutions create gaps between
    elements of the kernel and perform convolution, thereby covering a larger area
    of the input. This allows the network to capture long-range dependencies in the
    data, making it well-suited to TSC tasks ([yazdanbakhsh2019multivariate,](#bib.bib50)
    ). Recently, Disjoint-CNN ([foumani2021disjoint,](#bib.bib51) ) showed that factorization
    of 1D convolution kernels into disjoint temporal and spatial components yields
    accuracy improvements with almost no additional computational cost. Applying disjoint
    temporal convolution and then spatial convolution behaves similarly to Inverted
    Bottleneck ([sandler2018mobilenetv2,](#bib.bib52) ). Like the Inverted Bottleneck,
    the temporal convolutions expand the number of input channels, and spatial convolutions
    later project the expanded hidden state back to the original size to capture the
    temporal and spatial interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Imaging time series
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In TSC, a common approach is to convert the time series data into a fixed-length
    representation, such as a vector or matrix, which can then be input to a deep
    learning model. However, this can be challenging for time series data that vary
    in length or have complex temporal dependencies. One solution to this problem
    is to represent the time series data in an image-like format, where each time
    step is treated as a separate channel in the image. This allows the model to learn
    from the spatial relationships within the data rather than just the temporal relationships.
    In this context, the term spatial refers to the relationships between different
    variables or features within a single time step of the time series.
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative to using raw time series data as input, Wang and Oates encoded
    univariate time series data into different types of images that were then processed
    by a regular CNN ([wang2015encoding,](#bib.bib53) ). This image-based framework
    initiated a new branch of deep learning approaches for time series, which consider
    image transformation as one of the feature engineering techniques. Wang and Oates
    presented two approaches for transforming a time series into an image. The first
    generates a Gramian Angular Field (GAF), while the second generates a Markov Transition
    Field (MTF). GAF represents time series data in a polar coordinate and uses various
    operations to convert these angles into a symmetry matrix and MTF encodes the
    matrix entries using the transition probability of a data point from one time
    step to another time step ([wang2015encoding,](#bib.bib53) ). In both cases, the
    image generation increases the time series size, making the images potentially
    prohibitively large. Therefore they propose strategies to reduce their size without
    losing too much information. Afterward, the two types of images are combined in
    a two-channel image that is then used to produce better results than those achieved
    when using each image separately. Finally, a Tiled CNN model is applied to classify
    the time-series images. In other studies, a variety of transformation methods,
    including Recurrence Plots (RP) ([hatami2018classification,](#bib.bib54) ), Gramian
    Angular Difference Field (GADF) ([karimi2018scalable,](#bib.bib55) ), bilinear
    interpolation ([zhao2019classify,](#bib.bib56) ), and Gramian Angular Summation
    Field (GASF) ([yang2019sensor,](#bib.bib57) ) have been proposed to transfer time
    series to input images expecting that the two-dimensional images could reveal
    features and patterns not found in the one-dimensional sequence of the original
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hatami et al. ([hatami2018classification,](#bib.bib54) ) propose a representation
    method based on RP ([kamphorst1987recurrence,](#bib.bib58) ) to convert the time
    series to 2D images with a CNN model for TSC. In their study, time series are
    regarded as distinct recurrent behaviors such as periodicities and irregular cyclicities,
    which are the typical phenomena of dynamic systems. The main idea of using the
    RP method is to reveal at which points some trajectories return to a previous
    state. Finally, two-stage convolution and two fully connected layers are applied
    to classify the images generated by RP. Subsequently, pre-trained Inception v3 ([szegedy2016rethinking,](#bib.bib59)
    ) was used to map the GADF images into a 2048-dimensional vector space. The final
    stage used an MLP with three hidden layers, followed by a softmax activation function
     ([karimi2018scalable,](#bib.bib55) ). Following the same framework, Chen and
    Shi ([chen2019deep,](#bib.bib60) ) adopted the Relative Position Matrix and VGGNet
    (RPMCNN) to classify time series data using transform 2D images. Their results
    showed promising performances by converting univariate time series data to 2D
    images using relative positions between two time stamps. Following the convention,
    three image encoding methods: GASF, GADF, and MTF, were used to encode MTS data
    into two-dimensional images ([yang2019sensor,](#bib.bib57) ). They showed that
    the simple structure of ConvNet is sufficient for classification as it performed
    equally well with the complex structure of VGGNet.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, representing time series data as 2D images can be difficult because
    preserving the temporal relationships and patterns in the data can be challenging.
    This transformation can also result in a loss of information, making it difficult
    for the model to classify the data accurately. Chen and Shi ([chen2019deep,](#bib.bib60)
    ) have also shown that the specific transformation methods like GASF, GADF, and
    MTF used in this process do not significantly improve the prediction outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Baseline Architecture | Other features |'
  prefs: []
  type: TYPE_TB
- en: '| Adapted |'
  prefs: []
  type: TYPE_TB
- en: '| MC-DCNN ([zheng2014time,](#bib.bib42) ) | 2014 | 2-Stage Conv | Independent
    convolutions per Channel |'
  prefs: []
  type: TYPE_TB
- en: '| MC-CNN ([yang2015deep,](#bib.bib43) ) | 2015 | 3-Stage Conv | 1D-Convolutions
    on all Channel |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. ([zhao2017convolutional,](#bib.bib44) ) | 2015 | 2-Stage Conv
    | Similar architecture to MC-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| FCN ([wang2017time,](#bib.bib16) ) | 2017 | FCN | Using GAP instead of FC
    Layer |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet ([wang2017time,](#bib.bib16) ) | 2017 | ResNet 9 | Using 3-Residual
    block |'
  prefs: []
  type: TYPE_TB
- en: '| Res-CNN ([zou2019integration,](#bib.bib48) ) | 2019 | RezNet+FCN | Using
    1-Residual block + FCN |'
  prefs: []
  type: TYPE_TB
- en: '| DCNNs ([yazdanbakhsh2019multivariate,](#bib.bib50) ) | 2019 | 4-Stage Conv
    | Using dilated convolutions |'
  prefs: []
  type: TYPE_TB
- en: '| Disjoint-CNN ([wang2017time,](#bib.bib16) ) | 2021 | 4-Stage Conv | Disjoint
    temporal and spatial convolution |'
  prefs: []
  type: TYPE_TB
- en: '| Series To Image |'
  prefs: []
  type: TYPE_TB
- en: '| Wang&Oates([wang2015encoding,](#bib.bib53) ) | 2015 | Tiled CNN | GAF, MT
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hatami et al.([hatami2018classification,](#bib.bib54) ) | 2017 | 2-Stage
    Conv | Recurrence Plots |'
  prefs: []
  type: TYPE_TB
- en: '| Karimi et al.([karimi2018scalable,](#bib.bib55) ) | 2018 | Inception V3 |
    GADF |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. ([zhao2019classify,](#bib.bib56) ) | 2019 | ResNet18, ShuffleNet
    V2 | Bilinear interpolation |'
  prefs: []
  type: TYPE_TB
- en: '| RPMCNN ([chen2019deep,](#bib.bib60) ) | 2019 | VGGNet, 2-Stage Conv | Relative
    Position Matrix |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([yang2019sensor,](#bib.bib57) ) | 2019 | VGGNet | GASF, GADF,
    MTF |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Scale Operation |'
  prefs: []
  type: TYPE_TB
- en: '| MCNN ([cui2016multi,](#bib.bib61) ) | 2016 | 2-Stage Conv | Identity mapping,
    Smoothing, Down-sampling |'
  prefs: []
  type: TYPE_TB
- en: '| t-LeNet ([le2016data,](#bib.bib62) ) | 2016 | 2-Stage Conv | Squeeze and
    Dilation |'
  prefs: []
  type: TYPE_TB
- en: '| MVCNN ([liu2018time,](#bib.bib63) ) | 2019 | 4-stage Conv | Inception V1
    based |'
  prefs: []
  type: TYPE_TB
- en: '| Brunel et al. ([brunel2019cnn,](#bib.bib64) ) | 2019 | Inception V1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) ) | 2019 | Inception
    V4 | Ensemble |'
  prefs: []
  type: TYPE_TB
- en: '| EEG-inception ([sun2021prototypical,](#bib.bib65) ) | 2021 | InceptionTime
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Inception-FCN ([usmankhujaev2021time,](#bib.bib66) ) | 2021 | InceptionTime
    + FCN |  |'
  prefs: []
  type: TYPE_TB
- en: '| KDCTime ([gong2022kdctime,](#bib.bib67) ) | 2022 | InceptionTime | Knowledge
    Distillation, Label smoothing |'
  prefs: []
  type: TYPE_TB
- en: '| LITE ([ismail2023lite,](#bib.bib68) ) | 2023 | InceptionTime | Multiplexing,
    dilated, and custom filters |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. Summary of CNN models for time series classification and extrinsic
    regression
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Multi-Scale Operation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The papers discussed here apply a multi-scale convolutional kernel to the input
    series or apply regular convolutions on the input series at different scales.
    Multi-scale CNNs (MCNN) ([cui2016multi,](#bib.bib61) ) and Time LeNet (t-LeNet) ([le2016data,](#bib.bib62)
    ) were considered the first models that preprocess the input series to apply convolution
    on multi-scale series rather than raw series. The design of both MCNNs and t-LeNet
    were inspired by computer vision models, which means that they were adapted from
    models originally developed for image recognition tasks. These models may not
    be well-suited to TSC tasks and may not perform as well as models specifically
    designed for this purpose. One potential reason for this is the use of progressive
    pooling layers in these models, commonly used in computer vision models, to reduce
    the input data size and make it easier to process. However, these pooling layers
    may not be as effective when applied to time series data and may limit the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'MCNN has simple architecture and comprises two convolutions and a pooling layer,
    followed by a fully connected and softmax layer. However, this approach involves
    heavy data preprocessing. Specifically, before any training, they use a sliding
    window to extract a time series subsequence, and later, the subsequence will undergo
    three transformations: (1) identity mapping, (2) down-sampling, and (3) smoothing,
    which results in the transformation of a univariate input time series into a multivariate
    one. Finally, the transformed output is fed to the CNN model to train a classifier ([cui2016multi,](#bib.bib61)
    ). t-LeNet uses two data augmentation techniques: window slicing (WS) and window
    warping (WW), to prevent overfitting ([le2016data,](#bib.bib62) ). The WS method
    is identical to MCNN’s data augmentation. The second data augmentation technique,
    WW, employs a warping technique that squeezes or dilates the time series. WS is
    also adopted to ensure that subsequences of the same length are extracted for
    training the network to deal with multi-length time series. Therefore, a given
    input time series of length $L$ is first dilated $(\times 2)$ and then squeezed
    $(\times 1/2)$ using WW, resulting in three time series of length $L,2L,1/2L$
    that are fed to WS to extract equal length subsequences for training. Finally,
    as both MCNN and t-LeNet predict a class for each extracted subsequence, majority
    voting is applied to obtain the class prediction for the full time series.'
  prefs: []
  type: TYPE_NORMAL
- en: Inception was first proposed by Szegedy et al. ([szegedy2015going,](#bib.bib69)
    ) for end-to-end image classification. Now the network has evolved to become Inception-v4,
    where Inception was coupled with residual connections to improve further the performance ([szegedy2017inception,](#bib.bib70)
    ). Inspired by inception architecture, a multivariate convolutional neural network
    (MVCNN) is designed using multi-scale convolution kernels to find the optimal
    local construction ([liu2018time,](#bib.bib63) ). MVCNN uses three scales of filters,
    including $2\times 2$, $3\times 3$, and $5\times 5$, to extract features of the
    interaction between sensors. A one-dimensional Inception model was used for Supernovae
    classification using the light flux of a region in space as an input MTS for the
    network ([brunel2019cnn,](#bib.bib64) ). However, the authors limited the conception
    of their Inception architecture to the first version of this model ([szegedy2015going,](#bib.bib69)
    ). The Inception-ResNet ([ronald2021isplinception,](#bib.bib71) ) architecture
    includes convolutional layers, followed by Inception modules and residual blocks.
    The Inception modules are used to learn multiple scales and aspects of the data,
    allowing the network to capture more complex patterns. The residual blocks are
    then used to learn the residuals, or differences, between the input and output
    of the network, improving its performance.
  prefs: []
  type: TYPE_NORMAL
- en: InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) ) explores much larger
    filters than any previously proposed network for TSC to reach state-of-the-art
    performance on the UCR benchmark. InceptionTime is an ensemble of five randomly
    initialized inception network models, each of which consists of two blocks of
    inception modules. Each inception module first reduces the dimensionality of a
    multivariate time series using a bottleneck layer with a length and stride of
    1 while maintaining the same length. Then, 1D convolutions of different lengths
    are applied to the output of the bottleneck layer to extract patterns at different
    sizes. In parallel, a max pooling layer followed by a bottleneck layer are also
    applied to the original time series to increase the robustness of the model to
    small perturbations. The output from the convolution and max pooling layers are
    stacked to form a new multivariate time series which is then passed to the next
    layer. Residual connections are used between each inception block to reduce the
    vanishing gradient effect. The output of the second inception block is passed
    to a GAP layer before feeding into a softmax classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strong performance of InceptionTime has inspired a number of extensions.
    Like InceptionTime, EEG-inception ([sun2021prototypical,](#bib.bib65) ) uses several
    inception layers and residual connections as its backbone. Additionally, noise
    addition-based data augmentation of EEG signals is proposed, which increases the
    average accuracy. InceptionFCN ([usmankhujaev2021time,](#bib.bib66) ) focuses
    on combining two well-known deep learning techniques, namely the Inception module
    and the Fully Convolutional Network ([usmankhujaev2021time,](#bib.bib66) ). In
    KDCTime ([gong2022kdctime,](#bib.bib67) ), label smoothing (LSTime) and knowledge
    distillation (KDTime) were introduced for InceptionTime, automatically generated
    while compressing the inference model. Additionally, knowledge distillation with
    calibration (KDC) in KDCTime offers two calibrating strategies: KDC by translating
    (KDCT) and KDC by reordering (KDCR). LITE ([ismail2023lite,](#bib.bib68) ) addresses
    InceptionTime’s complexity while preserving its TSC performance. Utilizing DepthWise
    Separable Convolutions, LITE incorporates multiplexing, dilated convolution, and
    custom filters ([ismail2022deep,](#bib.bib72) ) to enhance efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Recurrent Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recurrent Neural Networks are types of neural networks built with internal memory
    to work with time series and sequential data. Conceptually similar to feed-forward
    neural networks (FFNs), RNNs differ in their ability to handle variable-length
    inputs and produce variable-length outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Vanilla Recurrent Neural Networks (Vanilla RNNs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recurrent neural networks for TSC have been proposed in ([husken2003recurrent,](#bib.bib73)
    ). Using RNNs, the input series have been classified based on their dynamic behavior.
    They used sequence-to-sequence architecture in which each sub-series of input
    series is classified in the first step. Then the argmax function is applied to
    the entire output, and finally, the neuron with the highest rate specifies the
    classification result. In order to improve the model parallelization and capacity ([dennis2019shallow,](#bib.bib74)
    ) proposed a two-layer RNN. In the first layer, the input sequence is split into
    several independent RNNs to improve parallelization, followed by a second layer
    that utilizes the first layer’s output to capture long-term dependencies ([dennis2019shallow,](#bib.bib74)
    ). Further, RNNs have been used in some hierarchical architectures ([fernandez2007sequence,](#bib.bib75)
    ; [hermans2013training,](#bib.bib76) ). Hermans and Schrauwen showed a deeper
    version of recurrent neural networks could perform hierarchical processing on
    complex temporal tasks and capture the time series structure more naturally than
    a shallow version ([hermans2013training,](#bib.bib76) ). RNNs are usually trained
    iteratively using a procedure known as backpropagation through time (BPTT). When
    unfolded in time, RNNs look like very deep networks with shared parameters. With
    deeper neural layers in RNN and sharing weights across different RNN cells, the
    gradients are summed up at each time step to train the model. Thus, gradients
    undergo continuous matrix multiplication due to the chain rule and either shrink
    exponentially and have small values called vanishing gradients or blow up to a
    very large value, referred to as exploding gradients ([pascanu2013difficulty,](#bib.bib77)
    ). These problems motivated the development of second-order methods for deep architectures
    named long short-term memory (LSTM) ([hochreiter1997long,](#bib.bib78) ) and Gated
    Recurrent Unit (GRU) ([chung2014empirical,](#bib.bib79) ).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Long Short Term Memory (LSTM)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LSTM addresses the common vanishing/exploding gradient issue in vanilla RNNs
    by integrating memory cells with gate control into their state dynamics ([hochreiter1997long,](#bib.bib78)
    ). Due to its design nature, LSTM is suited to problems involving sequence data,
    such as language translation ([sutskever2014sequence,](#bib.bib80) ), video representation
    learning ([donahue2015long,](#bib.bib81) ), and image caption generation ([karpathy2015deep,](#bib.bib82)
    ). The TSC problem is not an exception and mainly adopts a similar model to the
    language translation ([sutskever2014sequence,](#bib.bib80) ). Sequence-to-Sequence
    with Attention (S2SwA) ([tang2016sequence,](#bib.bib83) ) incorporates two LSTMs,
    one encoder and one decoder, in a sequence-to-sequence fashion for TSC. In this
    model, the encoder LSTM accepts input time series of arbitrary lengths and extracts
    information from the raw data based on which the decoder LSTM constructs fixed-length
    sequences that can be regarded as automatically extracted features for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Gated Recurrent Unit (GRU)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GRU, another widely-used variant of RNNs, shares similarities with LSTM in its
    ability to control information flow and memorize context across multiple time
    steps ([chung2014empirical,](#bib.bib79) ). Similar to S2SwA ([tang2016sequence,](#bib.bib83)
    ) sequence auto-encoder (SAE) based on GRU has been defined to deal with TSC problem ([malhotra2017timenet,](#bib.bib84)
    ). A fixed-size output is produced by processing the various input lengths using
    GRU as the encoder and decoder. The model’s accuracy was also improved by pre-training
    the parameters on massive unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4\. Hybrid Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CNN’s and RNNs are often combined for TSC because they have complementary strengths.
    As mentioned previously, CNNs are well-suited for learning from spatial relationships
    in data, such as the patterns and correlations between the channels of different
    time steps in a time series. This allows them to learn useful features from the
    time series data that can help improve the classification performance. RNNs, on
    the other hand, are well-suited for learning from temporal dependencies in data,
    such as the past values of a time series that can help predict its future values.
    This allows them to capture the dynamic nature of time series data and make more
    accurate predictions. Combining the strengths of CNNs and RNNs makes it possible
    to learn spatial and temporal features from the time series data, improving the
    model’s performance for TSC. Additionally, the two models can be trained together,
    allowing them to learn from each other and improve the model’s overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Various extensions like MLSTM-FCN ([karim2019multivariate,](#bib.bib85) ), TapNet ([zhang2020tapnet,](#bib.bib86)
    ), and SMATE ([zuo2021smate,](#bib.bib87) ) were proposed later to deal with time-series
    data. MLSTM-FCN extends the univariate LSTM-FCN model ([karim2017lstm,](#bib.bib88)
    ) to the multivariate case. Like the LSTM-FCN, the multivariate version comprises
    LSTM blocks and fully convolutional blocks for extracting features from input
    series. A squeeze and excite block is also added to the FCN block, and can execute
    a form of self-attention on the output feature maps of previous layers ([karim2019multivariate,](#bib.bib85)
    ). Two further proposals for multivariate TSC are the Time series attentional
    prototype Network (TapNet) and Semi-Supervised Spatio-Temporal (SMATE)  ([zhang2020tapnet,](#bib.bib86)
    ; [zuo2021smate,](#bib.bib87) ). These methods combine and seek to leverage the
    relative strengths of both traditional distance-based and deep-learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: MLSTM-FCN, TapNet, and SMATE were designed in dual-network architectures. The
    input is separately fed into the CNN and RNN models, and their output is concentrated
    before the fully connected layer for the final task. However, one branch cannot
    fully use the hidden states of the other during feature extraction since the final
    classification results are generated by concatenating the outputs of the two branches.
    That motivates different types of architecture that try layer-wise integration
    of CNN and RNN models. This motivates different architectures, such as GCRNN ([lin2017gcrnn,](#bib.bib89)
    ) and CNN-LSTM ([mutegeki2020cnn,](#bib.bib90) ), which aim to integrate CNNs
    and RNNs in a layer-wise fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'While recurrent neural networks are commonly used for time series forecasting,
    only a few studies have applied them to TSC, mainly due to four reasons: (1) RNNs
    typically struggle with the gradient vanishing and exploding problem due to training
    on long-time series ([pascanu2012understanding,](#bib.bib91) ). (2) RNNs are considered
    difficult to train and parallelize, so researchers are less likely to use them
    as they are computationally expensive ([pascanu2013difficulty,](#bib.bib77) ).
    (3) Recurrent architectures are designed mainly to learn from the previous data
    to make predictions about the future ([langkvist2014review,](#bib.bib28) ). (4)
    RNN models can fail to effectively capture and utilize long-range dependencies
    in long sequences ([tang2016sequence,](#bib.bib83) ).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Attention based model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite the excellent performance of CNN models for capturing local temporal/spatial
    correlations, these models can not effectively capture and utilize long-range
    dependencies. Additionally, they only consider the local order of data points
    rather than the overall order of all data points. Therefore, many recent studies
    have embedded recurrent neural networks (RNN) such as LSTMs alongside the CNNs
    to capture this information ([karim2017lstm,](#bib.bib88) ; [karim2019multivariate,](#bib.bib85)
    ; [zhang2020tapnet,](#bib.bib86) ). The disadvantage of RNN-based models is that
    they are computationally expensive, and their capability to capture long-range
    dependencies is limited ([vaswani2017attention,](#bib.bib92) ; [hao2020new,](#bib.bib18)
    ). On the other hand, attention models can capture long-range dependencies, and
    their broader receptive fields provide more contextual information, which can
    improve the models’ learning capacity. The attention mechanism aims to enhance
    a network’s representation ability by focusing on essential features and suppressing
    unnecessary ones. Not surprisingly, with the success of attention models in natural
    language processing ([vaswani2017attention,](#bib.bib92) ; [devlin2018bert,](#bib.bib93)
    ), many previous studies have attempted to bring the power of attention models
    into various domains such as computer vision ([dosovitskiy2020image,](#bib.bib94)
    ) and time series analysis ([hao2020new,](#bib.bib18) ; [li2019enhancing,](#bib.bib95)
    ; [zhou2021informer,](#bib.bib96) ; [zerveas2021transformer,](#bib.bib19) ; [kostas2021bendr,](#bib.bib97)
    ). Table [2](#S3.T2 "Table 2 ‣ 3.4.2\. Transformers ‣ 3.4\. Attention based model
    ‣ 3\. Supervised Models ‣ Deep Learning for Time Series Classification and Extrinsic
    Regression: A Current Survey") presents a list of the attention-based models reviewed
    in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1\. Self-Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Self-attention has been demonstrated to be effective in various natural language
    processing tasks due to its ability to capture long-term dependencies in text ([vaswani2017attention,](#bib.bib92)
    ). Recently, it has also been shown to be effective for TSC tasks ([hao2020new,](#bib.bib18)
    ; [yuan2018muvan,](#bib.bib98) ; [hsieh2021explainable,](#bib.bib99) ; [chen2021multi,](#bib.bib100)
    ). As we mentioned, the self-attention module is embedded in the encoder-decoder
    models to improve the model performance. However, only the encoder and the self-attention
    module have been used for TSC. Early models of TSC follow the same backbone of
    natural language processing models and use the Recurrent-based models such as
    RNN ([yuan2018novel,](#bib.bib101) ), GRU([yuan2018muvan,](#bib.bib98) ) and LSTM([liang2018geoman,](#bib.bib102)
    ; [hu2020multistage,](#bib.bib103) ) for encoding the input series. For example,
    the Multi-View Attention Network (MuVAN) applies bidirectional GRUs independently
    to each input dimension as the encoder and then feeds all the representations
    into a self-attention bock ([yuan2018muvan,](#bib.bib98) ).
  prefs: []
  type: TYPE_NORMAL
- en: As a result of the excellent performance of the CNN models, many studies have
    attempted to encode the time series using CNN before applying attention ([hao2020new,](#bib.bib18)
    ; [hsieh2021explainable,](#bib.bib99) ; [cheng2020novel,](#bib.bib104) ; [xiao2021rtfn,](#bib.bib105)
    ). Cross Attention Stabilized Fully Convolutional Neural Network (CA-SFCN) ([hao2020new,](#bib.bib18)
    ) and Locality Aware eXplainable Convolutional ATtention network (LAXCAT) ([hsieh2021explainable,](#bib.bib99)
    ) applied the self-attention mechanism to leverage the long-term dependencies
    for the MTSC task. CA-SFCN combines FCN and two types of self-attention - temporal
    attention (TA) and variable attention (VA), which interact to capture the long-range
    dependencies and variables interactions. LAXCAT also used temporal and variable
    attention to identify informative variables and the time intervals where they
    have informative patterns for classification. WaveletDTW Hybrid attEntion Networks
    (WHEN) ([wang2023wavelet,](#bib.bib106) ) integrate two attention mechanisms,
    namely wavelet attention and DTW attention, into the BiLSTM to enhance model performance.
    In wavelet attention, they leverage wavelets to compute attention scores, specifically
    targeting the analysis of dynamic frequency components in nonstationary time series.
    Simultaneously, DTW attention employs the DTW distance to calculate attention
    scores, addressing the challenge of time distortion in multiple time series.
  prefs: []
  type: TYPE_NORMAL
- en: Several self-attention models have been developed to improve network performance ([jaderberg2015spatial,](#bib.bib107)
    ; [woo2018cbam,](#bib.bib108) ), including Squeeze-and-Excitation (SE) ([hu2018squeeze,](#bib.bib109)
    ), which focuses on channel attention and is often used to classify time series
    data ([karim2019multivariate,](#bib.bib85) ; [chen2021multi,](#bib.bib100) ; [wang2021time,](#bib.bib110)
    ). The SE block allows the whole network to use global information to selectively
    focus on the informative feature maps and suppress less important ones ([hu2018squeeze,](#bib.bib109)
    ). More importantly, the SE block can increase the quality of the shared lower-level
    representations in the early layers and becomes increasingly specialized when
    responding to different inputs in later layers. The weight of each feature map
    is automatically learned at each layer of the network, and the SE block can boost
    feature discrimination throughout the whole network. Multi-scale Attention Convolutional
    Neural Network (MACNN) ([chen2021multi,](#bib.bib100) ) applies the different
    kernel size convolutions to capture different scales of information along the
    time axis by generating feature maps at differing scales. Then an SE block is
    used to enhance useful feature maps and suppress less useful ones by automatically
    learning each feature map’s importance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2\. Transformers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The impressive performance of multi-headed attention has led to numerous attempts
    to adapt multi-headed attention to the TSC domain. Transformers for classification
    usually employ a simple encoder structure consisting of attention and feed-forward
    layers. SAnD (Simply Attend and Diagnose) ([song2018attend,](#bib.bib111) ) architecture
    adopted a multi-head attention mechanism similar to a vanilla transformer ([vaswani2017attention,](#bib.bib92)
    ) to classify clinical time series for the first time. The model uses both positional
    encoding and a dense interpolation embedding technique to incorporate temporal
    order into representation learning. In another study that classified vibration
    signals ([jin2021end,](#bib.bib112) ), time-frequency features such as Frequency
    Coefficients and Short Time Fourier Transformation (STFT) spectrums are used as
    input embeddings to the transformers. A multi-head attention-based model was applied
    to raw optical satellite TSC using Gaussian Process Interpolation ([Rasmussen2004,](#bib.bib113)
    ) embedding and outperformed convolution, and recurrent neural networks ([allam2021paying,](#bib.bib114)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Gated Transformer Networks (GTN) ([liu2021gated,](#bib.bib115) ) use two-tower
    multi-headed attention to capture the discriminative information from the input
    series. Also, they merged the output of two towers using a learnable matrix named
    gating. To enhance locality awareness of transformers for TSC, flexible multi-head
    linear attention (FMLA) ([zhao2022rethinking,](#bib.bib116) ) integrates deformable
    convolutional blocks and online knowledge distillation, as well as a random mask
    to reduce noise. For each TSC dataset, AutoTransformer searches for the suitable
    network architecture using the neural architecture search (NAS) algorithm before
    feeding the output to the multi-headed attention blocks. ConvTran ([Foumani2023,](#bib.bib13)
    ) currently stands as the state of the art in multivariate TSC. They conducted
    a review of existing absolute and relative position encoding methods in TSC. Based
    on the limitations of the current position encodings for time series, they introduced
    two novel ones named tAPE and eRPE for absolute and relative positions, respectively.
    Integrating these proposed position encodings into a transformer block and combining
    them with a convolution layer, they presented a novel deep-learning framework
    for multivariate time series classification—ConvTran.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Embedding | Attention |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MuVAN ([yuan2018muvan,](#bib.bib98) ) | 2018 | Bi-GRU | Self-attention |'
  prefs: []
  type: TYPE_TB
- en: '| ChannelAtt ([yuan2018novel,](#bib.bib101) ) | 2018 | RNN | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| GeoMAN ([liang2018geoman,](#bib.bib102) ) | 2018 | LSTM | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Stage-Att ([hu2020multistage,](#bib.bib103) ) | 2020 | LSTM | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| CT_CAM ([cheng2020novel,](#bib.bib104) ) | 2020 | FCN + Bi-GRU | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| CA-SFCN ([hao2020new,](#bib.bib18) ) | 2020 | FCN | Self-attention |'
  prefs: []
  type: TYPE_TB
- en: '| RTFN ([xiao2021rtfn,](#bib.bib105) ) | 2021 | CNN + LSTM | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| LAXCAT ([hsieh2021explainable,](#bib.bib99) ) | 2021 | CNN | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| MACNN ([chen2021multi,](#bib.bib100) ) | 2021 | Multi-scale CNN | Squeeze-and-Excitation
    |'
  prefs: []
  type: TYPE_TB
- en: '| WHEN ([wang2023wavelet,](#bib.bib106) ) | 2023 | CNN + BiLSTM | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| SAnD ([song2018attend,](#bib.bib111) ) | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Linear Embedding &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multi-Head |'
  prefs: []
  type: TYPE_TB
- en: '| T2 ([allam2021paying,](#bib.bib114) ) | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Gaussian Process Regression + 1D Conv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multi-Head |'
  prefs: []
  type: TYPE_TB
- en: '| GTN ([liu2021gated,](#bib.bib115) ) | 2021 | Linear Embedding | Multi-Head
    |'
  prefs: []
  type: TYPE_TB
- en: '| TRANS_tf ([jin2021end,](#bib.bib112) ) | 2021 | time-frequency features |
    Multi-Head |'
  prefs: []
  type: TYPE_TB
- en: '| FMLA ([zhao2022rethinking,](#bib.bib116) ) | 2022 | Deformable CNN | Multi-Head
    |'
  prefs: []
  type: TYPE_TB
- en: '| AutoTransformer ([ren2022autotransformer,](#bib.bib117) ) | 2022 | Multi-scale
    CNN + NAS | Multi-Head |'
  prefs: []
  type: TYPE_TB
- en: '| ConvTran ([Foumani2023,](#bib.bib13) ) | 2023 | Disjoint-CNN | Multi-Head
    |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. Summary of Attention-based Models for Time Series Classification and
    Extrinsic Regression
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Graph Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While both CNNs and RNNs perform well on Euclidean data, many time series problems
    have data that are more naturally represented as graphs ([Jin2023graph,](#bib.bib118)
    ). For example, in a network of sensors, the sensors may be irregularly spaced,
    instead of the sensors forming a regular grid. A graph representation of data
    collected by this network can model this irregular layout more accurately than
    can be done using a Euclidean space. However, using standard deep learning algorithms
    to learn from graph structures is challenging ([Wu2021graph,](#bib.bib119) ).
    For example, nodes may have a varying number of neighbouring nodes, making it
    difficult to apply a convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph Neural Networks (GNNs) ([Scarselli2009graph,](#bib.bib120) ) are methods
    that adapt deep learning techniques to the graph domain. Much of the early research
    using GNNs for time series analysis concentrated on forecasting tasks ([Jin2023graph,](#bib.bib118)
    ). However, recent works consider GNNs for TSC ([Xi2023graph,](#bib.bib121) ;
    [Liu2023graph,](#bib.bib122) ) and TSER ([Bloemheuvel2023graph,](#bib.bib123)
    ) tasks. A list of the GNN models reviewed in this paper is provided in table
    [3](#S3.T3 "Table 3 ‣ 3.5\. Graph Neural Networks ‣ 3\. Supervised Models ‣ Deep
    Learning for Time Series Classification and Extrinsic Regression: A Current Survey").
    Time2Graph+ ([Cheng2021graph,](#bib.bib124) ) transforms each time series into
    a shapelet graph. Shapelets are extracted from the time series and form the graph
    nodes. The graph edges are weighted based on transition probabilities between
    the two shapelets. Once the input graphs have been constructed, a graph attention
    network is used to create a representation of the time series that is fed into
    a classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | GNN Type | Other Components |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TGCN ([covert2019temporal,](#bib.bib125) ) | 2019 | Graph convolutional network
    | 1D-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| DGCNN ([Song2020graph,](#bib.bib126) ) | 2020 | Graph convolutional network
    | 1x1 CNN |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSleepNet ([jia2020graphsleepnet,](#bib.bib127) ) | 2020 | Graph convolutional
    network | Temporal attention |'
  prefs: []
  type: TYPE_TB
- en: '| T-GCN ([ma2021deep,](#bib.bib128) ) | 2021 | Graph convolutional network
    | GRU |'
  prefs: []
  type: TYPE_TB
- en: '| MRF-GCN ([li2020multireceptive,](#bib.bib129) ) | 2021 | Graph convolutional
    network | Fast Fourier Transforms (FFT) |'
  prefs: []
  type: TYPE_TB
- en: '| Nhu et al. ([nhu2021graph,](#bib.bib130) ) | 2021 | Graph convolutional network
    | 1D-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| DCRNN ([Tang2021graph,](#bib.bib131) ) | 2021 | Graph convolutional network
    | GRU |'
  prefs: []
  type: TYPE_TB
- en: '| Time2Graph+ ([Cheng2021graph,](#bib.bib124) ) | 2021 | Graph attention |
    Shapelet transform |'
  prefs: []
  type: TYPE_TB
- en: '| RAINDROP ([Zhang2021graph,](#bib.bib132) ) | 2021 | Graph guided network
    | Temporal attention |'
  prefs: []
  type: TYPE_TB
- en: '| STEGON ([Censi2021graph,](#bib.bib133) ) | 2021 | Graph attention | 1D-CNN
    |'
  prefs: []
  type: TYPE_TB
- en: '| Azevedo et al. ([Azevedo2022graph,](#bib.bib134) ) | 2022 | Graph network
    block with pooling | 1D-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| MTPool ([Duan2022graph,](#bib.bib135) ) | 2022 | Variational Graph Pooling
    | 1D-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| SimTSC ([Zha2022graph,](#bib.bib136) ) | 2022 | Graph convolutional network
    | DTW, ResNet |'
  prefs: []
  type: TYPE_TB
- en: '| Tulczyjew et al. ([Tulczyjew2022graph,](#bib.bib137) ) | 2022 | Graph convolutional
    network | Adaptive pooling |'
  prefs: []
  type: TYPE_TB
- en: '| C-DGAM ([Sun2023graph,](#bib.bib138) ) | 2023 | Graph attention | 1D-CNN
    with attention |'
  prefs: []
  type: TYPE_TB
- en: '| Dufourg et al. ([Dufourg2023graph,](#bib.bib139) ) | 2023 | Spatio-temporal
    graph | Simple Linear Iterative Clustering |'
  prefs: []
  type: TYPE_TB
- en: '| TISER-GCN ([Bloemheuvel2023graph,](#bib.bib123) ) | 2023 | Graph convolutional
    network | 1D-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| TodyNet ([Liu2023graph,](#bib.bib122) ) | 2023 | Dynamic graph neural network
    | 1D-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| LB-SimTSC ([Xi2023graph,](#bib.bib121) ) | 2023 | Graph convolutional network
    | Lower-bound DTW, ResNet |'
  prefs: []
  type: TYPE_TB
- en: Table 3\. Summary of graph neural network models for time series classification
    and extrinsic regression
  prefs: []
  type: TYPE_NORMAL
- en: SimTSC ([Zha2022graph,](#bib.bib136) ) constructs a pairwise similarity graph
    where each time series forms a node and edge weights are computed based on the
    DTW distance measure. Node attributes are generated using a feature vector encoder.
    GNN operations are used to enhance the node features based on similarities between
    adjacent time series. These representations are then used for the final classification
    step, which produces a classification for each node. LB-SimTSC ([Xi2023graph,](#bib.bib121)
    ) replaces the expensive DTW computation with the LB-Keogh lower-bounding method ([Keogh2005exact,](#bib.bib140)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Spatiotemporal GNNs model both spatial (or inter-variable) and temporal dependencies
    using two modules that work in tandem. The spatial module models the dependencies
    between the time series by applying graph convolutions over a GNN (graph convolutional
    networks or GCNs ([Kipf2016graph,](#bib.bib141) )). The temporal module models
    the dependencies within the time series using an RNN ([ma2021deep,](#bib.bib128)
    ; [Tang2021graph,](#bib.bib131) ), 1D-CNN ([Censi2021graph,](#bib.bib133) ; [Azevedo2022graph,](#bib.bib134)
    ), Attention ([Zhang2021graph,](#bib.bib132) ; [Sun2023graph,](#bib.bib138) ),
    or a combination of these ([Jin2023graph,](#bib.bib118) ). The features extracted
    from the graph layers are then fed into the classification or regression layers
    to make either a single prediction ([Tang2021graph,](#bib.bib131) ; [Azevedo2022graph,](#bib.bib134)
    ; [Zhang2021graph,](#bib.bib132) ; [Sun2023graph,](#bib.bib138) ) or a prediction
    for each node ([ma2021deep,](#bib.bib128) ; [Censi2021graph,](#bib.bib133) ).
  prefs: []
  type: TYPE_NORMAL
- en: Spatiotemporal GCNs are often used to analyse sensor arrays, where the graph
    structure models the physical layout of the sensors. A common example is electroencephalogram
    (EEG) data, where the location of EEG electrodes is represented as a graph that
    is used to analyse the EEG signal. Some of these applications are epilepsy detection ([nhu2021graph,](#bib.bib130)
    ), seizure detection ([covert2019temporal,](#bib.bib125) ; [Tang2021graph,](#bib.bib131)
    ), emotion recognition ([Song2020graph,](#bib.bib126) ), and sleep classification ([jia2020graphsleepnet,](#bib.bib127)
    ). Besides EEG, GCNs have also been applied to engineering applications such as
    machine fault diagnosis ([li2020multireceptive,](#bib.bib129) ), slope deformation
    prediction ([ma2021deep,](#bib.bib128) ) and seismic activity prediction ([Bloemheuvel2023graph,](#bib.bib123)
    ). MTPool ([Duan2022graph,](#bib.bib135) ) uses a spatiotemporal GCN for multivariate
    time series classification. In this study, each channel in the time series is
    represented by a node in the graph and the graph edges model the correlations
    between the channels. The GCN is combined with temporal convolutions and a hierarchical
    graph pooling technique. Spatiotemporal GNNs have also been used for object-based
    image analysis ([Censi2021graph,](#bib.bib133) ) and semantic segmentation ([Tulczyjew2022graph,](#bib.bib137)
    ) of image time series. However, these assume the labels and spatial relationships
    are static over time. In many cases these may both change. Spatiotemporal *graphs*
    (STGs), which include temporal edges as well as spatial edges, can model these
    dynamic relationships ([Dufourg2023graph,](#bib.bib139) ). In STGs, each node
    represents an object at one timestamp. Spatial edges connect the object to adjacent
    objects and temporal edges connect two objects in consecutive images if they have
    common pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Self-supervised Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Obtaining labeled data for large time series datasets poses significant costs
    and challenges. Machine learning models trained on large labeled time series datasets
    often exhibit superior performance compared to models trained on sparsely labeled
    datasets, small datasets with limited labels, or those without supervision, leading
    to suboptimal performance across various time series machine learning tasks ([yue2022ts2vec,](#bib.bib23)
    ; [yang2022unsupervised,](#bib.bib142) ). As a result, rather than depending on
    high-quality annotations for large datasets, researchers and practitioners are
    increasingly shifting their focus toward self-supervised representation learning
    for time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-supervised representation learning, a subfield of machine learning, focuses
    on learning representations from data without explicit supervision ([foumani2023series2vec,](#bib.bib24)
    ). In contrast to supervised learning, which relies on labeled data, self-supervised
    learning methods utilize the inherent structure of the data to learn valuable
    representations in an unsupervised manner. The learned representations can then
    be used for a variety of downstream tasks including classification, anomaly detection,
    and forecasting. This survey specifically emphasizes classification as a downstream
    task. We categorized self-supervised learning approaches for TSC into three groups
    based on the pretext. Table [4](#S4.T4 "Table 4 ‣ 4.1\. Contrastive Learning ‣
    4\. Self-supervised Models ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey") shows a list of the self-supervised models
    reviewed in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Contrastive Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrastive learning involves model learning to differentiate between positive
    and negative time series examples. Time-Contrastive Learning (TCL) ([hyvarinen2016unsupervised,](#bib.bib143)
    ), Scalable Representation Learning (SRL or T-Loss) ([franceschi2019unsupervised,](#bib.bib144)
    ) and Temporal Neighborhood Coding (TNC) ([tonekaboni2021unsupervised,](#bib.bib145)
    ) apply a subsequence-based sampling and assume that distant segments are negative
    pairs and neighbor segments are positive pairs. TNC takes advantage of the local
    smoothness of a signal’s generative process to define neighborhoods in time with
    stationary properties to further improve the sampling quality for the contrastive
    loss function. TS2Vec ([yue2022ts2vec,](#bib.bib23) ) uses contrastive learning
    to obtain robust contextual representations for each timestamp hierarchically.
    It involves randomly sampling two overlapping subseries from input and encouraging
    consistency of contextual representations on the common segment. The encoder is
    optimized using both temporal contrastive loss and instance-wise contrastive loss.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the subsequence-based methods, other models employ instance-based
    sampling ([eldele2021time,](#bib.bib21) ; [wickstrom2022mixing,](#bib.bib146)
    ; [yang2022timeclr,](#bib.bib147) ; [yang2022unsupervised,](#bib.bib142) ; [zhang2022self,](#bib.bib148)
    ; [meng2023mhccl,](#bib.bib149) ), treating each sample individually to generate
    positive and negative samples for contrastive loss. Time-series Temporal and Contextual
    Contrasting (TS-TCC) ([eldele2021time,](#bib.bib21) ) uses weak and strong augmentations
    to transform the input series into two views and then uses a temporal contrasting
    module to learn robust temporal representations. The contrasting contextual module
    is then built upon the contexts from the temporal contrasting module and aims
    to maximize similarity among contexts of the same sample while minimizing similarity
    among contexts of different samples. Similarly, TimeCLR ([yang2022timeclr,](#bib.bib147)
    ) introduces DTW data augmentation to enhance robustness against phase shift and
    amplitude change phenomena. Bilinear Temporal-Spectral Fusion (BTSF) ([yang2022unsupervised,](#bib.bib142)
    ) uses simple dropout as the augmentation method and aims to incorporate spectral
    information into the feature representation. Similarly, Time-Frequency Consistency
    (TF-C) ([zhang2022self,](#bib.bib148) ) is a self-supervised learning method that
    leverages the frequency domain to achieve better representation. It proposes that
    the time-based and frequency-based representations, learned from the same time
    series sample, should be more similar to each other in the time-frequency space
    compared to representations of different time series samples.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Encoder Backbones |  |'
  prefs: []
  type: TYPE_TB
- en: '| Contrastive Learning |  |  | Other features |'
  prefs: []
  type: TYPE_TB
- en: '| TCL ([hyvarinen2016unsupervised,](#bib.bib143) ) | 2016 | MLP | Sequence-based
    contrast |'
  prefs: []
  type: TYPE_TB
- en: '| T-Loss/SRL ([franceschi2019unsupervised,](#bib.bib144) ) | 2019 | Causal
    CNN | Sequence-based contrast |'
  prefs: []
  type: TYPE_TB
- en: '| TNC ([tonekaboni2021unsupervised,](#bib.bib145) ) | 2021 | Bidirectional
    RNN | Sequence-based contrast |'
  prefs: []
  type: TYPE_TB
- en: '| TS-TCC ([eldele2021time,](#bib.bib21) ) | 2021 | CNN + Transformers | Instance/Sequence-based
    contrast |'
  prefs: []
  type: TYPE_TB
- en: '| MCL ([wickstrom2022mixing,](#bib.bib146) ) | 2021 | FCN | Instance-based
    contrast |'
  prefs: []
  type: TYPE_TB
- en: '| TimeCLR ([yang2022timeclr,](#bib.bib147) ) | 2021 | InceptionTime | Instance-based
    contrast |'
  prefs: []
  type: TYPE_TB
- en: '| TS2Vec ([yue2022ts2vec,](#bib.bib23) ) | 2021 | Dilated CNN | Sequence-based
    contrast |'
  prefs: []
  type: TYPE_TB
- en: '| BTSF ([yang2022unsupervised,](#bib.bib142) ) | 2022 | Causal CNN | Instance-based
    contrast |'
  prefs: []
  type: TYPE_TB
- en: '| TF-C ([zhang2022self,](#bib.bib148) ) | 2022 | ResNets | Instance-based contrast
    |'
  prefs: []
  type: TYPE_TB
- en: '| MHCCL ([meng2023mhccl,](#bib.bib149) ) | 2023 | ResNet | Instance-based contrast
    |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Prediction |'
  prefs: []
  type: TYPE_TB
- en: '| BENDR ([kostas2021bendr,](#bib.bib97) ) | 2021 | CNN + Transformers | Sequence
    masking |'
  prefs: []
  type: TYPE_TB
- en: '| Voice2Series ([yang2021voice2series,](#bib.bib22) ) | 2021 | CNN+Transformers
    | Binary masking |'
  prefs: []
  type: TYPE_TB
- en: '| TST ([zerveas2021transformer,](#bib.bib19) ) | 2021 | Transformers | Binary
    masking |'
  prefs: []
  type: TYPE_TB
- en: '| TARNet ([chowdhury2022tarnet,](#bib.bib150) ) | 2022 | Transformers | Binary
    masking |'
  prefs: []
  type: TYPE_TB
- en: '| TimeMAE ([cheng2023timemae,](#bib.bib151) ) | 2023 | CNN + Transformers |
    Sequence masking |'
  prefs: []
  type: TYPE_TB
- en: '| CRT ([zhang2023self,](#bib.bib152) ) | 2023 | Transformers | Sequence masking
    |'
  prefs: []
  type: TYPE_TB
- en: '| Other Pretext tasks |'
  prefs: []
  type: TYPE_TB
- en: '| PHIT ([ismail2023finding,](#bib.bib153) ) | 2023 | H-InceptionTime |  |'
  prefs: []
  type: TYPE_TB
- en: '| Series2Vec ([foumani2023series2vec,](#bib.bib24) ) | 2023 | Disjoint CNN
    | Similarity based representation learning |'
  prefs: []
  type: TYPE_TB
- en: Table 4\. Summary of self-supervised models for time series classification and
    extrinsic regression
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Self-Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primary objective of self-prediction-based self-supervised models is to
    reconstruct the input or representation of input data. Studies have explored using
    transformer-based self-supervised learning methods for TSC ([kostas2021bendr,](#bib.bib97)
    ; [yang2021voice2series,](#bib.bib22) ; [zerveas2021transformer,](#bib.bib19)
    ; [chowdhury2022tarnet,](#bib.bib150) ; [cheng2023timemae,](#bib.bib151) ; [zhang2023self,](#bib.bib152)
    ), following the success of models like BERT ([devlin2018bert,](#bib.bib93) ).
    BErt-inspired Neural Data Representations (BENDER)([kostas2021bendr,](#bib.bib97)
    ) uses the transformer structure to model EEG sequences and shows that it can
    effectively handle massive amounts of EEG data recorded with differing hardware.
    Another study, Voice-to-Series with Transformer-based Attention (V2Sa)([yang2021voice2series,](#bib.bib22)
    ), utilizes a large-scale pre-trained speech processing model for TSC.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based Framework (TST)([zerveas2021transformer,](#bib.bib19) ) and
    TARNet ([chowdhury2022tarnet,](#bib.bib150) ) adapts vanilla transformers to the
    multivariate time series domain and uses a self-prediction-based self-supervised
    pre-training approach with masked data. These studies demonstrate the potential
    of using transformer-based self-supervised learning methods for TSC.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Other Pretext tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While many pretext tasks in self-supervised learning are typically contrastive
    or self-predictive, specific tasks are tailored for time series data. In image-based
    self-supervised learning, synthetic transformations (augmentation) of an image
    are created, and the model learns to contrast the image and its transforms with
    other images in the training data, which works well for object interpretation.
    However, time series analysis fundamentally differs from vision or natural language
    processing concerning the definition of meaningful self-supervised learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Guided by this insight, Foumani et al. ([foumani2023series2vec,](#bib.bib24)
    ) introduce Series2Vec, a novel self-supervised representation learning approach.
    Unlike other contrastive self-supervised methods in time series, which carry the
    risk of positive sample variants being less similar to the anchor sample than
    series in the negative set, Series2Vec is trained to predict the similarity between
    two series in both temporal and spectral domains through a self-supervised task.
    Series2Vec relies primarily on the consistency of the unsupervised similarity
    step, rather than the intrinsic quality of the similarity measurement, without
    the need for hand-crafted data augmentation. Pre-trained H-InceptionTime (PHIT) ([ismail2023finding,](#bib.bib153)
    ) is pre-trained using a novel pretext task designed to identify the originating
    dataset of each time series sample. The objective is to generate flexible convolution
    filters that can be applied across diverse datasets. Furthermore, PHIT demonstrates
    its capability to mitigate overfitting in small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Data augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field of deep learning, the concept of data augmentation has emerged
    as an important tool for enhancing performance, particularly in scenarios where
    the availability of training data is limited ([shorten2019survey,](#bib.bib154)
    ). Originally proposed in computer vision, data augmentation involves a variety
    of transformations to images, such as cropping, rotating, flipping, and applying
    filters like blurring and sharpening. These transformations serve to introduce
    a diverse range of scenarios within the training data, thereby aiding in the development
    of more robust and generalizable models. However, the direct application of these
    image-based augmentation techniques to time series data often proves to be inadequate
    or inappropriate. Operations like rotation may disrupt the intrinsic temporal
    structure of time series data.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of overfitting is particularly pronounced in the field of deep
    learning models for TSC. These models are characterized by a high number of trainable
    parameters, which can lead to a model that performs well on training data but
    fails to generalize to unseen data. In such cases, data augmentation can be a
    valuable strategy. It offers an alternative to the costly and sometimes impractical
    approach of collecting additional real-world data. By generating synthetic samples
    from existing datasets, we can effectively augment the size and variety of our
    training data. The following details different investigated methods to produce
    synthetic time series for data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Random Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several augmentations have been developed for the magnitude domain. Jittering,
    as explored by Um et al. ([um2017data,](#bib.bib155) ), involves the addition
    of random noise to the time series. Another method, flipping ([rashid2019window,](#bib.bib156)
    ), reverses the time series values. Scaling is a technique where the time series
    is multiplied by a factor from a Gaussian distribution. Magnitude warping, which
    shares similarities with scaling, distorts the series along a curve that varies
    smoothly. For time domain transformations, permutation algorithms play a significant
    role. For example, the slicing transformation involves removing sub-sequence from
    the series. There are also various warping methods like Random Warping ([iwana2021time,](#bib.bib157)
    ), Time Warping ([um2017data,](#bib.bib155) ), Time Stretching ([nguyen2020improving,](#bib.bib158)
    ), and Time Perturbation ([vachhani2018data,](#bib.bib159) ), each introducing
    different forms of distortion to the time series. Finally, in the frequency domain,
    transformations often utilize the Fourier transform. For example, Gao et al. ([gao2020robusttad,](#bib.bib160)
    ) introduce perturbations to both the magnitude and phase spectrum following a
    Fourier transform.
  prefs: []
  type: TYPE_NORMAL
- en: Window methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A primary approach in window methods is to create new time series by combining
    segments from various series of the same class. This technique effectively enriches
    the data pool with a variety of samples. Window slicing, as introduced by Cui
    et al. ([cui2016multiscale,](#bib.bib161) ) involves dividing a time series into
    smaller segments, with each segment retaining the class label of the original
    series. These segments are then used to train classifiers, offering a detailed
    view of the data. During classification, each segment is evaluated individually,
    and a collective decision on the final label is reached through a voting system
    among the slices. Another technique is window warping, based on the DTW algorithm.
    This method adjusts segments of a time series along the temporal axis, either
    stretching or compressing them. This introduces variability in the time dimension
    of the data. Le Guennec et al. ([leguennec2016data,](#bib.bib162) ) work provides
    examples of the application of both window slicing and window warping, showcasing
    their effectiveness in enhancing the diversity and representativeness of time
    series datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Averaging methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Averaging methods in time series data augmentation combine multiple series to
    form a new, unified series. This process is more difficult than it might seem,
    as it requires careful consideration of factors like noise and distortions in
    both the time and magnitude aspects of the data. In this context, weighted Dynamic
    Time Warping (DTW) Barycenter Averaging (wDBA) introduced by Forestier et al. ([forestier2017generating,](#bib.bib163)
    ) provides an averaging method by aligning time series in a way that accounts
    for their temporal dynamics. The practical application of wDBA is illustrated
    in the study by Ismail Fawaz et al. ([fawaz2018data,](#bib.bib164) ), where it
    is employed in conjunction with a ResNet classifier, demonstrating its effectiveness.
    Additionally, the research conducted by Terefe et al. ([terefe2020time,](#bib.bib165)
    ) uses an auto-encoder for averaging a set of time series. This method represents
    a more advanced approach in time series data augmentation, exploiting the auto-encoder’s
    capacity for learning and reconstructing data to generate averaged representations
    of time series.
  prefs: []
  type: TYPE_NORMAL
- en: Selection of data augmentation methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The selection of the appropriate data augmentation technique is critical and
    must be adapted to the specific characteristics of the dataset and the architecture
    of the neural network being used. Studies like those conducted by Iwana et al. ([iwana2021empirical,](#bib.bib166)
    ), Pialla et al. ([pialla2022data,](#bib.bib167) ) and Gao et al ([gao2023data,](#bib.bib168)
    ) highlight the complexity of this task. These studies demonstrate that the effectiveness
    of augmentation techniques can vary significantly across different datasets and
    neural network architectures. Consequently, a method that proves effective in
    one scenario may not necessarily yield similar results in another. To this end,
    practitioners in the field of TSC must engage in a careful and informed process
    of method selection and tuning. While the array of available data augmentation
    techniques offers a comprehensive toolkit for tackling the challenges of limited
    data and overfitting, their successful application depends heavily on a nuanced
    understanding of both the methods themselves and the specific demands of the task
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning, initially popularized in the field of computer vision, is
    increasingly becoming relevant in the domain of TSC. In computer vision, this
    approach involves using a pre-trained network, typically on large datasets like
    ImageNet ([deng2009imagenet,](#bib.bib169) ), as a starting point rather than
    initiating with random network weights. This method is also related to the concept
    of foundation or base models, which are large-scale machine learning models trained
    on extensive data, often using self-supervised or semi-supervised learning. These
    models are adaptable to a wide array of tasks, showcasing their versatility. The
    principle of transfer learning is also closely associated with domain adaptation
    which focuses on applying a model trained on a source data distribution to a different,
    but related, target data distribution. This approach is crucial in leveraging
    pre-trained models for various applications, particularly in scenarios where data
    is scarce or specific to certain domains.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of TSC, insights have been contributed by the work of Ismail
    Fawaz et al. ([fawaz2018transfer,](#bib.bib170) ), who conducted a study using
    the UCR archive. Their extensive experiments demonstrated that transfer learning
    could lead to positive or negative outcomes, depending on the chosen datasets
    for transfer. This finding underscores the importance of the relationship between
    source and target datasets in transfer learning efficacy. Ismail Fawaz et al. ([fawaz2018transfer,](#bib.bib170)
    ) also introduced an approach to predict the success of transfer learning in TSC
    by using DTW to measure similarities between datasets. This metric serves as a
    guide to select the most appropriate source dataset for a given target dataset,
    thereby enhancing accuracy in a majority of cases.
  prefs: []
  type: TYPE_NORMAL
- en: Other researchers have also explored transfer learning in TSC. Spiegel ([spiegel2016transfer,](#bib.bib171)
    ) work on using dissimilarity spaces to enrich feature representations in TSC
    set a precedent for employing unconventional data sources. This approach of enhancing
    learning with diverse data types finds a parallel in Li et al. ([li2020deep,](#bib.bib172)
    ) method, which leverages sensor modality labels from various fields to train
    a deep network, emphasizing the importance of versatile data in transfer learning.
    Building on the concept of data diversity, Rotem et al. ([rotem2022transfer,](#bib.bib173)
    ) pushed the boundaries further by generating a synthetic univariate time series
    dataset for transfer learning. This synthetic dataset, used for regression tasks,
    underscores the potential of artificial data in overcoming the limitations of
    real-world datasets. Furthermore, Senanayaka et al. ([senanayaka2022similarity,](#bib.bib174)
    ) introduced the similarity-based multi-source transfer learning (SiMuS-TL) approach.
    By establishing a ’mixed domain’ to model similarities among various sources,
    Senanayaka et al. demonstrated the effectiveness of carefully selected and related
    data sources in transfer learning. Finally, Kashiparekh et al. ([kashiparekh2019convtimenet,](#bib.bib175)
    ) with their ConvTimeNet (CTN) focused on the adaptability of pre-trained networks
    across diverse time scales.
  prefs: []
  type: TYPE_NORMAL
- en: While the explored studies collectively advance our understanding of transfer
    learning in TSC, the field remains open for further investigation. A key challenge
    lies in determining the most suitable source models for transfer, a task complicated
    by the relative scarcity of large, curated, and annotated datasets in time series
    analysis compared to the field of computer vision. This restricts the utility
    of transfer learning in TSC, as the availability of extensive and diverse datasets
    is crucial for developing robust and generalizable models. Furthermore, the question
    of developing filters that are generic enough to be effective across a wide range
    of applications remains unresolved. This aspect is critical for the success of
    transfer learning, as the applicability of a pre-trained model to new tasks depends
    on the universality of its learned features. Additionally, the strategy of whether
    to freeze certain layers of the network during transfer or to fine-tune the entire
    network is another area that warrants deeper exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Applications - recent developments and challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TSC and TSER techniques have been used to analyze and model time-dependent data
    in a wide range of applications. These include human activity recognition, Earth
    observation, medical diagnosis including Electroencephalogram (EEG) ([MerlinPraveena2022app,](#bib.bib176)
    ) and Electrocardiogram (ECG) ([Liu2021app,](#bib.bib177) ) monitoring, air quality
    and pollution prediction ([Zaini2022app,](#bib.bib178) ; [Zhang2022app,](#bib.bib179)
    ), structural and machine health monitoring ([Toh2020app,](#bib.bib180) ; [Thoppil2021app,](#bib.bib181)
    ), Industrial Internet of Things (IIOT) ([Ren2023app,](#bib.bib182) ), energy
    consumption and anomaly detection ([Himeur2021app,](#bib.bib183) ), and bio-acoustics
    ([Stowell2022app,](#bib.bib184) ).
  prefs: []
  type: TYPE_NORMAL
- en: Due to the extensive range of applications that use TSC and TSER, it is infeasible
    to cover them all in detail in a single review. Therefore, in this survey, we
    focus on just two applications – human activity recognition and satellite Earth
    observation. (References to recent reviews have been provided for the other applications
    mentioned above.) These are two important but quite different domains and were
    chosen to give the reader an idea of the diverseness of time series use in deep
    learning. The following sections provide an overview of the use of TSC and TSER,
    the latest developments, and challenges in these two applications.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Human Activity Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human activity recognition (HAR), is the identification or monitoring of human
    activity through the analysis of data collected by sensors or other instruments ([Gupta2022survey,](#bib.bib185)
    ). The recent growth of wearable technologies and the Internet of Things has resulted
    not only in the collection of large volumes of activity data ([Ramanujam2021survey,](#bib.bib186)
    ), but also easy deployment of applications utilising this data to improve the
    safety and quality of human life ([chen2021deep,](#bib.bib5) ; [Gupta2022survey,](#bib.bib185)
    ). HAR is therefore an important field of research with applications including
    healthcare, fitness monitoring, smart homes ([Lockhart2012appl,](#bib.bib187)
    ), and assisted living ([Tapia2004appl,](#bib.bib188) ).
  prefs: []
  type: TYPE_NORMAL
- en: Devices used to collect HAR data can be categorised as visual or sensor-based ([chen2021deep,](#bib.bib5)
    ; [wang2019deep,](#bib.bib4) ). Sensor-based devices can be further categorised
    as object sensors (for example RFIDs embedded into objects), ambient sensors (motion
    sensors, WiFi or Bluetooth devices in fixed locations) and wearable sensors ([wang2019deep,](#bib.bib4)
    ), including smartphones ([nweke2018deep,](#bib.bib3) ). However, the majority
    of HAR studies use data from wearable sensors or visual devices ([Gupta2022survey,](#bib.bib185)
    ). Additionally, human activity recognition from visual device data requires the
    use of computer vision techniques and is therefore out of scope for this review.
    Accordingly, this section reviews wearable sensor-based methods of HAR. For reviews
    of vision-based HAR, refer to Kong and Fu([Kong2022vision,](#bib.bib189) ) or
    Zhang et al. ([Zhang2019vision,](#bib.bib190) ).
  prefs: []
  type: TYPE_NORMAL
- en: The main sensors used in wearable devices are accelerometers, gyroscopes and
    magnetic sensors ([Ordonez2016deep,](#bib.bib191) ), which each collect three-dimensional
    spatial data over time. Inertial measurement units (IMUs) are wearable devices
    that combine all three sensors in one unit ([Reiss2012pamap,](#bib.bib192) ; [Zhang2012uschad,](#bib.bib193)
    ). Wearable device studies typically collect data from multiple IMUs located on
    different parts of the body ([Roggen2010opp,](#bib.bib194) ; [Sztyler2017position,](#bib.bib195)
    ). To create a dataset suitable for HAR modelling, the sensor data is split into
    (usually equally size) time windows ([Lara2013survey,](#bib.bib196) ). The task
    is then to learn a function that maps the multi-variate sensor data for each time
    window to a set of activities. Thus, the data forms multi-variate time series
    suited to TSC.
  prefs: []
  type: TYPE_NORMAL
- en: Given the broad scope of our survey, this section necessarily only provides
    a brief overview of the studies using deep learning for HAR. However, there are
    several surveys that provide a more in-depth review of machine learning and deep
    learning for HAR. Lara and Labrador ([Lara2013survey,](#bib.bib196) ) provide
    a comprehensive introduction to HAR, including machine learning methods used and
    the principal issues and challenges. Both Nweke et al. ([nweke2018deep,](#bib.bib3)
    ) and Wang et al. ([wang2019deep,](#bib.bib4) ) provide a summary of deep learning
    methods, highlighting their advantages and limitations. Chen et al. ([chen2021deep,](#bib.bib5)
    ) discuss challenges in HAR and the appropriate deep learning methods for addressing
    each challenge. They also provide a comprehensive list of publicly-available HAR
    datasets. Gu et al. ([Gu2022survey,](#bib.bib197) ) focus on deep learning methods,
    reviewing preprocessing and evaluation techniques as well as the deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The deep learning methods used for HAR include both CNNs and RNNs, as well
    as hybrid CNN-RNN models. While some of the models include an attention module,
    we did not find any studies proposing a full attention or transformer model. A
    summary of the studies reviewed and the type of model built is provided in table
    [5](#S7.T5 "Table 5 ‣ 7.1\. Human Activity Recognition ‣ 7\. Applications - recent
    developments and challenges ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey"). Hammerla et al. ([Hammerla2016deep,](#bib.bib198)
    ) compared several deep learning model for HAR, include three LSTM variants, a
    CNN model, and DNN model. They found a bi-directional LSTM performed best on naturalistic
    datasets where long-term effects are important. However, they found some applications
    need to focus on short-term movement patterns and suggested CNNs are more appropriate
    for these applications. Thus, research across all model types is beneficial for
    the on-going development of models for HAR applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Embedding | Other features |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Zeng et al. ([Zeng2014cnn,](#bib.bib199) ) | 2014 | CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| DCNN ([Jiang2015cnn,](#bib.bib200) ) | 2015 | CNN | Discrete Fourier Transform
    |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([Yang2015cnn,](#bib.bib201) ) | 2015 | CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| DeepConvLSTM ([Ordonez2016deep,](#bib.bib191) ) | 2016 | CNN, LSTM |  |'
  prefs: []
  type: TYPE_TB
- en: '| Hammerla et al. ([Hammerla2016deep,](#bib.bib198) ) | 2016 | CNN, LSTM |
    Bi-directional |'
  prefs: []
  type: TYPE_TB
- en: '| Ronao et al. ([Ronao2016har,](#bib.bib202) ) | 2016 | CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Guan and Plötz ([Guan2017ensemble,](#bib.bib203) ) | 2017 | LSTM | Ensemble
    |'
  prefs: []
  type: TYPE_TB
- en: '| Lee et al. ([Lee2017har,](#bib.bib204) ) | 2017 | CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Murad and Pyun ([Murad2017deep,](#bib.bib205) ) | 2017 | LSTM | Uni- & bi-directional
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ignatov ([Ignatov2018realtime,](#bib.bib206) ) | 2018 | CNN | Statistical
    features |'
  prefs: []
  type: TYPE_TB
- en: '| Moya Rueda et al. ([Rueda2018cnn,](#bib.bib207) ) | 2018 | CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yao et al. ([Yao2018cnn,](#bib.bib208) ) | 2018 | CNN | Fully convolutional
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zeng et al. ([Zeng2018rnn,](#bib.bib209) ) | 2018 | LSTM | 2 attention layers
    |'
  prefs: []
  type: TYPE_TB
- en: '| AttnSense ([Ma2019attnsense,](#bib.bib210) ) | 2019 | CNN, GRU | Fast Fourier
    Transform, 2 attention layers |'
  prefs: []
  type: TYPE_TB
- en: '| InnoHAR ([Xu2019innohar,](#bib.bib211) ) | 2019 | CNN, GRU | Inception |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. ([Zhang2020novel,](#bib.bib212) ) | 2020 | CNN | Attention |'
  prefs: []
  type: TYPE_TB
- en: '| Challa et al. ([Challa2021multi,](#bib.bib213) ) | 2021 | CNN, LSTM | Bi-directional
    |'
  prefs: []
  type: TYPE_TB
- en: '| CNN-biGRU ([Mekruksavanich2021deep,](#bib.bib214) ) | 2021 | CNN, GRU | Bi-directional
    |'
  prefs: []
  type: TYPE_TB
- en: '| DEBONAIR ([Chen2021har,](#bib.bib215) ) | 2021 | ConvLSTM |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mekruksavanich and Jitpattanakul ([Mekruksavanich2021lstm,](#bib.bib216)
    ) | 2021 | CNN, LSTM |  |'
  prefs: []
  type: TYPE_TB
- en: '| Mekruksavanich and Jitpattanakul ([Mekruksavanich2021biometric,](#bib.bib217)
    ) | 2021 | CNN, LSTM | Ensemble |'
  prefs: []
  type: TYPE_TB
- en: '| Nafea et al. ([Nafea2021sensor,](#bib.bib218) ) | 2021 | CNN, LSTM | Bi-directional
    |'
  prefs: []
  type: TYPE_TB
- en: '| Singh et al. ([Singh2021convlstm,](#bib.bib219) ) | 2021 | CNN, LSTM | Attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([Wang2022deep,](#bib.bib220) ) | 2022 | CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. ([Xu2022deform,](#bib.bib221) ) | 2022 | CNN, Resnet | Deformable
    convolutions |'
  prefs: []
  type: TYPE_TB
- en: Table 5\. Summary of HAR deep learning models
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the papers reviewed in this section used commonly available datasets
    to build and evaluate their models. A summary of the most commonly used datasets
    is provided in section [C.1](#A3.SS1 "C.1\. HAR Datasets ‣ Appendix C Datasets
    ‣ Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey") of the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1\. Convolutional neural networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most common types of convolutional kernels for HAR is the $k\times
    1$ kernel. This kernel convolves $k$ time steps together, moving along each time
    series in the input features in turn ([Wang2022deep,](#bib.bib220) ), so while
    weights are shared between the input features, there is no mixing between features.
    The outputs from the final convolutional layer are flattened and processed by
    fully-connected layers before the final classification is made. Ronao et al. ([Ronao2016har,](#bib.bib202)
    ) performed a comprehensive evaluation of CNN models for HAR, evaluating the effect
    of changing the number of layers, filters and filter sizes. The input data was
    collected from smartphone accelerometer and gyroscope sensors. Ignatov ([Ignatov2018realtime,](#bib.bib206)
    ) used a one-layer CNN, and augmented the extracted features with statistical
    features before being passed to fully-connected layers. The architecture was effective
    with short time series (1 second) so useful for real time activity modelling.
  prefs: []
  type: TYPE_NORMAL
- en: One drawback of the above method is that it forces weight-sharing across all
    the input features. This may not be optimal, especially when using data collected
    from multiple devices. In this case, using a separate CNN for each device ([Rueda2018cnn,](#bib.bib207)
    ) allows independent weighting of the features. Similarly, as each sensor is typically
    tri-axial, a separate CNN can be used for each axis ([Zeng2014cnn,](#bib.bib199)
    ; [Zhang2020novel,](#bib.bib212) ). The features extracted by each CNN are then
    concatenated and processed either by fully-connected layers ([Zeng2014cnn,](#bib.bib199)
    ) or an attention head ([Zhang2020novel,](#bib.bib212) ).
  prefs: []
  type: TYPE_NORMAL
- en: While the above two methods are the most common, other studies have proposed
    alternative CNNs for HAR. DCNN ([Jiang2015cnn,](#bib.bib200) ) pre-processes the
    sensor data using a Discrete Fourier Transform to convert IMU data to frequency
    signals, then uses two-dimensional convolutions to extract combined temporal and
    frequency features. Lee et al. ([Lee2017har,](#bib.bib204) ) pre-processed the
    tri-axial accelerometer data to a magnitude vector, which was then processed in
    parallel by CNNs with varying kernel sizes, extracting features at different scales.
    Xu et al. ([Xu2022deform,](#bib.bib221) ) used deformable convolutions ([Dai2017deform,](#bib.bib222)
    ) in both a 2D-CNN and a ResNet model and found these models performed better
    than their non-deformable counterparts. Yao et al. ([Yao2018cnn,](#bib.bib208)
    ) proposed a fully convolutional model using two-dimensional temporal and feature
    convolutions. Their model has two advantages as (1) it handles arbitrary length
    input sequences and (2) it makes a prediction for each timestep, which avoids
    the need to pre-process the data into windows and can detect transitions between
    activities.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2\. Recurrent neural networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several long short-term memory (LSTM) models have been proposed for HAR. Murad
    and Pyun ([Murad2017deep,](#bib.bib205) ) designed and compared three multi-layered
    LSTMs, a uni-directional LSTM, a bi-directional LSTM, and a “cascading” LSTM,
    which has a bi-directional first layer, followed by uni-directional layers. In
    each case the output from all time steps is used as input to the classification
    layer. Zeng et al. ([Zeng2018rnn,](#bib.bib209) ) added two attention layers to
    an LSTM, a sensor attention layer before the LSTM and a temporal attention layer
    after the LSTM. They include a regularisation term they called “continuous attention”
    to smooth the transition between attention weights. Guan and Plötz ([Guan2017ensemble,](#bib.bib203)
    ) created an ensemble of LSTM models by saving the models at every training epoch,
    then selecting the best “M” models based on validation set results, thus aiming
    to reduce model variance.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3\. Hybrid models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many recent studies have focussed on hybrid models, combining both CNNs and
    RNNs. DeepConvLSTM ([Ordonez2016deep,](#bib.bib191) ) comprises four temporal
    convolutional layers followed by two LSTM layers, which the authors found to perform
    better than an equivalent CNN (replacing the LSTM layers with fully-connected
    layers). As the LSTM layers have fewer parameters than fully-connected layers,
    the DeepConvLSTM model was also much smaller. Singh et al. ([Singh2021convlstm,](#bib.bib219)
    ) used a CNN to encode the spatial data (i.e. the sensor readings at each timestamp)
    followed by a single LSTM layer to encode the temporal data, then a self-attention
    layer to weight the time steps. They found this model performed better than an
    equivalent model using temporal convolutions in the CNN layers. Challa et al. ([Challa2021multi,](#bib.bib213)
    ) proposed using three 1D-CNNs with different kernel sizes in parallel, followed
    by 2 bi-directional LSTM layers and a fully-connected layer. Nafea et al. ([Nafea2021sensor,](#bib.bib218)
    ) also used 1D-CNNs with different kernel sizes and bi-directional LSTMs. However,
    they used separate branches for the CNNs and LSTMs, merging the features extracted
    in each branch for the final fully connected layer. Mekruksavanich and Jitpattanakul ([Mekruksavanich2021lstm,](#bib.bib216)
    ) compared a 4-layer CNN-LSTM model with a smaller CNN-LSTM model and LSTM models,
    finding the extra convolutional layers improved performance over the smaller models.
    DEBONAIR ([Chen2021har,](#bib.bib215) ) is another multi-layered model. It uses
    parallel 1D-CNNs, each having different kernel, filter, and pooling sizes to extract
    different types of features associated with different types of activity. These
    are followed by a combined 1D-CNN, then two LSTM layers. Mekruksavanich and Jitpattanakul ([Mekruksavanich2021biometric,](#bib.bib217)
    ) ensembled four different models: a CNN, an LSTM, a CNN-LSTM, and a ConvLSTM
    model. They aimed to produce a model for boimetric user identification that could
    not only identify the activity being performed, but also the participant performing
    the activity.'
  prefs: []
  type: TYPE_NORMAL
- en: A few hybrid models use GRUs instead of LSTMs. InnoHAR ([Xu2019innohar,](#bib.bib211)
    ) is a modified DeepConvLSTM ([Ordonez2016deep,](#bib.bib191) ), replacing the
    four CNN layers with inception layers and the two LSTM layers with GRU layers.
    The authors found this inception model performed better than both the original
    DeepConvLSTM model and a straight CNN model ([Yang2015cnn,](#bib.bib201) ). AttnSense ([Ma2019attnsense,](#bib.bib210)
    ) uses a Fast Fourier transform to generate frequency features which are then
    convolved separately for each time step. Attention layers are used to weight the
    extracted frequency features. These are then passed through a GRU with temporal
    attention to extract temporal features. CNN-BiGRU ([Mekruksavanich2021deep,](#bib.bib214)
    ) uses a CNN layer to extract spatial features from the sensor data, then one
    or more GRU layers extract temporal features. The final section of the model is
    a fully-connected module consisting of one or more hidden layers and a softmax
    output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Satellite Earth Observation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ever since NASA launched the first Landsat satellite in 1972 ([Wulder2008landsat,](#bib.bib223)
    ), Earth-observing satellites have been recording images of the Earth’s surface,
    providing 50 years of continuous Earth observation (EO) data that can be used
    to estimate environmental variables informing us about the state of the Earth.
    Instruments on board the satellites record reflected or emitted electromagnetic
    radiation from the Earth’s surface and vegetation ([Emery2017emr,](#bib.bib224)
    ). The regular, repeated observations from these instruments form satellite image
    time series (SITS) that are useful for analysing the dynamic properties of some
    variables, such as plant phenology. The main modalities used for SITS analysis
    are multispectral spectrometers and spectroradiometers, which observe the visible
    and infrared (IR) frequencies and Synthetic Aperture Radar (SAR) systems which
    emit a microwave signal and measure the backscatter. A list of the main satellites
    and instruments used in the studies reviewed is provided in section [C.2](#A3.SS2
    "C.2\. Earth observation satellites and instruments ‣ Appendix C Datasets ‣ Deep
    Learning for Time Series Classification and Extrinsic Regression: A Current Survey")
    of the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: Raw data collected by satellite instruments needs to be pre-processed before
    being used in machine learning. This is frequently done by the data providers
    to produce analysis ready datasets (ARD). With the increasing availability of
    compatible ARD datasets from sources such as Google Earth Engine ([Gorelick2017gee,](#bib.bib225)
    ) and various data cubes ([Giuliani2017cube,](#bib.bib226) ; [Lewis2017cube,](#bib.bib227)
    ), models combining data from multiple data sources (multi-modal) are becoming
    more common. These data sources make it straightforward to obtain data that are
    co-registered (spatially aligned and with the same resolution and projection),
    thus avoiding the need for complex pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: Satellite image time series can be processed either (1) as two-dimensional temporal
    and spectral data, processing each pixel independently and ignoring the spatial
    dimensions, or (2) as four-dimensional data, including the two spatial dimensions,
    thus models extract spatio-temporal features. This latter method allows estimates
    to be made at pixel, patch, or object level, however it requires either more complex
    models, or spatial features to be extracted in a pre-processing step. Feature
    extraction can be as simple as extracting the mean value for each band. However,
    both clustering (TASSEL, ([Ienco2020tassel,](#bib.bib228) )), and neural-network
    based methods, such as the Pixel-Set Encoder ([Garnot2020tae,](#bib.bib229) )
    have been used for more complex feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: The most common use of SITS deep learning is for the classification of the Earth’s
    surface by land cover and agricultural land by crop types. The classes used can
    range from very broad land cover categories (such as forest, grasslands, agriculture)
    through to specific crops types. Other classification tasks include identifying
    specific features, such as sink-holes ([Kulshrestha2022hole,](#bib.bib230) ),
    burnt areas ([Ban2020fire,](#bib.bib231) ), flooded areas ([Rambour2020flood,](#bib.bib232)
    ), roads ([KamdemDeTeyou2020road,](#bib.bib233) ), deforestation ([Matosak2022forest,](#bib.bib234)
    ), vegetation quality ([Minh2017vege,](#bib.bib235) ) and forest understory and
    litter types ([Labenski2022under,](#bib.bib236) ).
  prefs: []
  type: TYPE_NORMAL
- en: Extrinsic regression tasks are less common than classification tasks, but several
    recent studies have investigated methods of estimating water content in vegetation,
    as measured by the variable Live Fuel Moisture Content (LFMC) ([Rao2020lfmc,](#bib.bib237)
    ; [Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239) ; [Xie2022lfmc,](#bib.bib240)
    ). Other regression tasks include estimating the wood volume of forests ([Lahssini2022wood,](#bib.bib241)
    ) by using a hybrid CNN-MLP model combining a time series of Sentinel-2 images
    with a single LiDAR image and crop yield ([Sun2020yield,](#bib.bib242) ) which
    uses a hybrid of CNN and LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many different approaches to learning from SITS data have been studied, with
    studies using all the main deep learning architectures, adapting them for multi-modal
    learning, and combining architectures in hybrid and ensemble models. The rest
    of this section reviews the architectures that have been used to model SITS data.
    A summary of these papers and architectures is provided in table [6](#S7.T6 "Table
    6 ‣ 7.2\. Satellite Earth Observation ‣ 7\. Applications - recent developments
    and challenges ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Embedding | Other features |'
  prefs: []
  type: TYPE_TB
- en: '| Crop type classification |'
  prefs: []
  type: TYPE_TB
- en: '| TAN ([Li2019tan,](#bib.bib243) ) | 2019 | 2D-CNN & GRU | Attention — temporal
    |'
  prefs: []
  type: TYPE_TB
- en: '| TGA ([Li2020tga,](#bib.bib244) ) | 2020 | 2D-CNN | Attention — squeeze and
    excitation |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-CNN ([Ji20183dcnn,](#bib.bib245) ) | 2018 | 3D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| DCM ([Xu2020dcm,](#bib.bib246) ) | 2020 | LSTM | Self-attention |'
  prefs: []
  type: TYPE_TB
- en: '| HierbiLSTM ([Barriere2022lstm,](#bib.bib247) ) | 2022 | LSTM | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| L-TAE ([Garnot2020ltae,](#bib.bib248) ) | 2020 | MLP | Attention — temporal
    |'
  prefs: []
  type: TYPE_TB
- en: '| PSE-TAE ([Garnot2020tae,](#bib.bib229) ; [Ofori-Ampofo2021att,](#bib.bib249)
    ) | 2020 | MLP | Attention — temporal optionally Multi-modal |'
  prefs: []
  type: TYPE_TB
- en: '| SITS-BERT ([Yuan2021sitsbert,](#bib.bib250) ) | 2021 |  | Pre-trained transformer
    |'
  prefs: []
  type: TYPE_TB
- en: '| Land Cover classification |'
  prefs: []
  type: TYPE_TB
- en: '| 1D-CNN ([DiMauro2017tiselc,](#bib.bib251) ) | 2017 | 1D-CNN & MLP | Hybrid
    model |'
  prefs: []
  type: TYPE_TB
- en: '| 1D & 2D-CNNs ([Kussul2017cnn,](#bib.bib252) ) | 2017 | 1D-CNN; 2D-CNN | Ensemble
    model |'
  prefs: []
  type: TYPE_TB
- en: '| TempCNN ([Pelletier2019tempcnn,](#bib.bib253) ) | 2019 | 1D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| TASSEL ([Ienco2020tassel,](#bib.bib228) ) | 2020 | 1D-CNN | Self-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| TSI ([Dou2021tsi,](#bib.bib254) ) | 2021 | 1D-CNN; LSTM | Ensemble model
    |'
  prefs: []
  type: TYPE_TB
- en: '| TWINNS ([Ienco2019twinns,](#bib.bib255) ) | 2019 | 2D-CNN & GRU | Attention
    — temporal; Multi-modal |'
  prefs: []
  type: TYPE_TB
- en: '| DuPLO ([Interdonato2019duplo,](#bib.bib256) ) | 2019 | 2D-CNN & GRU | Attention
    — temporal |'
  prefs: []
  type: TYPE_TB
- en: '| Sequential RNN ([Russwurm2018seqrnn,](#bib.bib257) ) | 2018 | 2D-FCN & LSTM
    | Hybrid model |'
  prefs: []
  type: TYPE_TB
- en: '| FG-UNET ([Stoian2019fgunet,](#bib.bib258) ) | 2019 | UNet & 2D-CNN | Hybrid
    model |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM ([Ienco2017rnn,](#bib.bib259) ) | 2017 | LSTM |  |'
  prefs: []
  type: TYPE_TB
- en: '| HOb2sRNN ([Gbodjo2020hob2srnn,](#bib.bib260) ) | 2020 | GRU | Attention —
    temporal |'
  prefs: []
  type: TYPE_TB
- en: '| OD2RNN ([Ienco2019od2rnn,](#bib.bib261) ) | 2019 | GRU | Attention — temporal;
    Multi-modal |'
  prefs: []
  type: TYPE_TB
- en: '| SITS-Former ([Yuan2022sitsformer,](#bib.bib262) ) | 2022 | 3D-CNN | Pre-trained
    transformer |'
  prefs: []
  type: TYPE_TB
- en: '| Other classification tasks |'
  prefs: []
  type: TYPE_TB
- en: '| Deforestation ([Matosak2022forest,](#bib.bib234) ) | 2022 | U-Net & LSTM
    | Hybrid model |'
  prefs: []
  type: TYPE_TB
- en: '| Flood detection ([Rambour2020flood,](#bib.bib232) ) | 2020 | Resnet & GRU
    | Hybrid model |'
  prefs: []
  type: TYPE_TB
- en: '| Forest understory ([Labenski2022under,](#bib.bib236) ) | 2022 | 2D-CNN &
    LSTM | Ensemble model |'
  prefs: []
  type: TYPE_TB
- en: '| Road detection ([KamdemDeTeyou2020road,](#bib.bib233) ) | 2020 | U-Net &
    convLSTM | Hybrid model |'
  prefs: []
  type: TYPE_TB
- en: '| Vegetation quality ([Minh2017vege,](#bib.bib235) ) | 2017 | LSTM; GRU |  |'
  prefs: []
  type: TYPE_TB
- en: '| Extrinsic regression tasks |'
  prefs: []
  type: TYPE_TB
- en: '| TempCNN-LFMC ([Zhu2020lfmc,](#bib.bib238) ) | 2021 | 1D-CNN |  |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-tempCNN ([Miller2022lfmc,](#bib.bib239) ) | 2022 | 1D-CNN | Multi-modal,
    ensemble model |'
  prefs: []
  type: TYPE_TB
- en: '| LFMC estimation ([Rao2020lfmc,](#bib.bib237) ) | 2020 | LSTM | Multi-modal
    |'
  prefs: []
  type: TYPE_TB
- en: '| LFMC estimation ([Xie2022lfmc,](#bib.bib240) ) | 2022 | 1D-CNN & LSTM | Multi-modal,
    hybrid, ensemble |'
  prefs: []
  type: TYPE_TB
- en: '| MLDL-net ([Sun2020yield,](#bib.bib242) ) | 2020 | 2D-CNN & LSTM | Hybrid
    model |'
  prefs: []
  type: TYPE_TB
- en: '| SSTNN ([Qiao2021yield,](#bib.bib263) ) | 2021 | 3D-CNN & LSTM | Hybrid model
    |'
  prefs: []
  type: TYPE_TB
- en: '| MMFVE ([Lahssini2022wood,](#bib.bib241) ) | 2022 | 2D-CNN | Hybrid model
    |'
  prefs: []
  type: TYPE_TB
- en: Table 6\. Summary of SITS deep learning models
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1\. Recurrent Neural Networks (RNNs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the first papers to use RNNs for land cover classification was Ienco
    et al. ([Ienco2017rnn,](#bib.bib259) ), who showed an LSTM model out-performed
    non deep learning methods such as Random Forest (RF) and Support Vector Machines
    (SVM). However, they also showed that the performance of both RF and SVM improves
    if trained on features extracted by the LSTM model, and in some cases were more
    accurate than the straight LSTM model. Rao et al. ([Rao2020lfmc,](#bib.bib237)
    ) used an extrinsic regression LSTM model to estimate LFMC in the western United
    States.
  prefs: []
  type: TYPE_NORMAL
- en: More commonly, however, RNNs are combined with an attention layer to allow the
    model to focus on the most important time steps. The OD2RNN model ([Ienco2019od2rnn,](#bib.bib261)
    ), used separate GRU layers followed by attention layers to process Sentinel-1
    and Sentinel-2 data, combining the features extracted by each source for the final
    fully-connected layers. HOb2sRNN ([Gbodjo2020hob2srnn,](#bib.bib260) ) refined
    OD2RNN by using a hierarchy of land cover classifications; the model was pretrained
    using broad land cover classifications, then further trained using the finer-grained
    classifications. DCM ([Xu2020dcm,](#bib.bib246) ) and HierbiLSTM ([Barriere2022lstm,](#bib.bib247)
    ) both use a bi-directional LSTM, processing the time series in both directions,
    followed by a self-attention transformer for a pixel-level crop-mapping model.
    All these studies found adding the attention layers improved model performance
    over a straight GRU or LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2\. Convolutional Neural Networks (CNNs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While many authors have claimed that RNNs out-perform CNNs for land cover and
    crop type classification, most of these comparisons are to 2-dimensional CNNs
    (2D-CNN), that ignore the temporal ordering of SITS data ([Pelletier2019tempcnn,](#bib.bib253)
    ). However, other studies show using 1-dimensional CNNs (1D-CNNs) to extract temporal
    information or 3-dimensional CNNs (3D-CNNs) to extract spatio-temporal information
    are both effective methods of learning from SITS data. TempCNN ([Pelletier2019tempcnn,](#bib.bib253)
    ) consists of three 1D convolutional layers. The output from the final convolutional
    layer is passed through a fully-connected layer, then the final softmax classification
    layer. TASSEL ([Ienco2020tassel,](#bib.bib228) ), is an adaptation of TempCNN
    for OBIA classification, using TempCNN models to process features extracted from
    the objects, followed by an attention layer to weight the convolved features.
    TempCNN has also been adapted for extrinsic regression ([Zhu2020lfmc,](#bib.bib238)
    ) and used for LFMC estimation ([Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239)
    ; [Xie2022lfmc,](#bib.bib240) ).
  prefs: []
  type: TYPE_NORMAL
- en: 2D-CNNs are mainly used to extract spatial or spatio-temporal features for both
    pixel and object classification. The model input is usually 4-dimensional and
    the data is convolved spatially, with two main methods used to handle the temporal
    dimension. In the first method, each time step is convolved separately and the
    extracted features are merged in later stages of the model ([Li2020tga,](#bib.bib244)
    ). In the second method, the time steps and channels are flattened to form a large
    multivariate image ([Kussul2017cnn,](#bib.bib252) ; [Lahssini2022wood,](#bib.bib241)
    ). FG-UNet ([Stoian2019fgunet,](#bib.bib258) ) is a fully-convolutional model
    that combines both the above methods, first grouping time steps by threes to produce
    images with 30 channels (10 spectral $\times$ 3 temporal), which are passed through
    both U-Net and 2D-CNN layers.
  prefs: []
  type: TYPE_NORMAL
- en: Ji et al. ([Ji20183dcnn,](#bib.bib245) ) used a three-dimensional CNN (3D-CNN)
    to convolve the spatial and temporal dimensions together, combining the strengths
    of 1D-CNN and 2D-CNNs. The study found a 3D-CNN crop classification model performed
    significantly better than the 2D-CNN, again showing the importance of the temporal
    features. Another study, SSTNN ([Qiao2021yield,](#bib.bib263) ) obtained good
    results for crop yield prediction by using a 3D-CNN to convolve the spatial and
    spectral dimensions, extracting spatio-spectral features for each time step. These
    features were then processed by LSTM layers to perform the temporal modelling.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3\. Transformer and Attention Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an alternative to including attention layers with a CNN or RNN, several studies
    have designed models that process temporal information using only attention layers.
    PSE-TAE ([Garnot2020tae,](#bib.bib229) ) used a modified transformer called a
    temporal attention encoder (TAE) for crop mapping and found the TAE performed
    better than either a CNN or an RNN. L-TAE ([Garnot2020ltae,](#bib.bib248) ) replaced
    the TAE with a light-weight transformer which is both computationally efficient
    and more accurate than the full TAE. Ofori-Ampofo et al. ([Ofori-Ampofo2021att,](#bib.bib249)
    ) adapted the TAE model for multi-modal inputs, using Sentinel-1 and Sentinel-2
    data for crop type mapping. Rußwurm and Körner([Russwurm2020att,](#bib.bib264)
    ) compared a self-attention model with RNN and CNN architectures. They found that
    this model was more robust to noise than either RNN or CNN and suggested self-attention
    is suitable for processing raw, cloud-affected satellite data.
  prefs: []
  type: TYPE_NORMAL
- en: Building on the success of pre-trained transformers for natural language processing
    (NLP) such as BERT ([devlin2018bert,](#bib.bib93) ), pre-trained transformers
    have been proposed for EO tasks ([Yuan2021sitsbert,](#bib.bib250) ). Earth observation
    tasks are particularly suited for pre-trained models as large quantities of EO
    data are readily available, while labelled data can be difficult to obtain ([Tuia2016da,](#bib.bib265)
    ), especially in remote locations. SITS-BERT ([Yuan2021sitsbert,](#bib.bib250)
    ) is an adaptation of BERT ([devlin2018bert,](#bib.bib93) ) for pixel-based SITS
    classification. For the pretext task, random noise is added to the pixels, and
    the model is trained to identify and remove this noise. The pre-trained model
    is then further trained for required tasks such as crop type or land cover mapping.
    SITS-Former ([Yuan2022sitsformer,](#bib.bib262) ) modifies SITS-BERT for patch
    classification by using 3D-Conv layers to encode the spatial-spectral information,
    which is then passed through the temporal attention layers. The pretext task used
    for SITS-Former is to predict randomly masked pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4\. Hybrid Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A common use of hybrid models is to use a CNN to extract spatial features and
    an RNN to extract temporal features. Garnot et al. ([Garnot2019time,](#bib.bib266)
    ) compared a straight 2D-CNN model (thus ignoring the temporal aspect), a straight
    GRU model (thus ignoring the spatial aspect) and a combined 2D-CNN and GRU model
    (thus using both spatial and temporal information) and found the combined model
    gave the best results, demonstrating that both the spatial and temporal dimensions
    provide useful information for land cover mapping and crop classification. DuPLO ([Interdonato2019duplo,](#bib.bib256)
    ) was one of the first models to exploit this method, running a CNN and ConvGRU
    model in parallel, then fusing the outputs using a fully-connected network for
    the final classifier. During training, an auxiliary classifier for each component
    was used to enhance the discriminative power. TWINNS ([Ienco2019twinns,](#bib.bib255)
    ) extended DuPLO to a multi-modal model, using time series of both Sentinel-1
    (SAR) and Sentinel-2 (Optical) images. Each modality was processed by separate
    CNN and convGRU models, then the output features from all four models were fused
    for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Other hybrid models include Li et al. ([Li2019tan,](#bib.bib243) ), who used
    a CNN for spatial and spectral unification of Landsat-8 and Sentinel-2 images
    which were then processed by a GRU. MLDL-Net ([Sun2020yield,](#bib.bib242) ) is
    a 2D-CNN extrinsic regression model, using CNNs to extract time step features,
    which are then passed through an LSTM model to extract temporal features. Fully
    connected layers combine the feature sets to predict crop yield. Rußwurm and Körner ([Russwurm2018seqrnn,](#bib.bib257)
    ) extracted temporal features first, using a bi-directional LSTM, then used a
    fully-convolutional 2D-CNN to incorporate spatial information and classify each
    pixel in the input patch.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.5\. Ensemble Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the easiest ways to ensemble DL models is to train multiple homogeneous
    models, that vary only in the random weight initialisation ([Fawaz2019ensembles,](#bib.bib267)
    ). Di Mauro et al. ([DiMauro2017tiselc,](#bib.bib251) ) ensembled 100 LULC models
    with different weight initialisations by averaging the softmax predictions. They
    found this produced a more stable and stronger classifier that outperformed the
    individual models. Multi-tempCNN ([Miller2022lfmc,](#bib.bib239) ), a model for
    LFMC estimation, is an ensemble of homogeneous models for extrinsic regression.
    The authors suggested that as an additional benefit, the variance of the individual
    model predictions can be used to obtain a measure of uncertainty of the estimates.
    TSI ([Dou2021tsi,](#bib.bib254) ) also ensembles a set of homogeneous models,
    but instead of relying on random weight initialisation to introduce model diversity,
    the time series are segmented and models trained on each segment.
  prefs: []
  type: TYPE_NORMAL
- en: Other methods create ensembles of heterogeneous models. Kussul et al ([Kussul2017cnn,](#bib.bib252)
    ) compared ensembles of 1D-CNNs and 2D-CNNs models for land cover classification.
    Each model in the ensemble used a different number of filters, so finding different
    feature sets useful for classification. Xie et al. ([Xie2022lfmc,](#bib.bib240)
    ) ensembled three heterogeneous models — a causal temporal convolutional neural
    network (TCN), an LSTM, and a hybrid TCN-LSTM model — for an extrinsic regression
    model to estimate LFMC. The ensembles were created using stacking ([Wolpert1992stack,](#bib.bib268)
    ). The authors compared this method to boosting their TCN-LSTM model, using Adaboost ([Freund1996ada,](#bib.bib269)
    ) to create a three-member ensemble, and found that stacking a diverse set of
    models out-performed boosting.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.6\. EO Surveys and Reviews
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This survey is one of very few that include a section focusing specifically
    on deep learning TSC and TSER tasks using SITS data. However, there are other
    reviews that provide further information about related topics. Gomez et al. ([Gomez2016review,](#bib.bib270)
    ) is an older review highlighting the importance role of SITS data for land cover
    classification. Zhu et al. ([Zhu2017review,](#bib.bib271) ) reviewed the advances
    and challenges in DL for remote sensing, and the resources available that are
    potentially useful to help DL address some of the major challenges facing humanity.
    Ma et al. ([Ma2019review,](#bib.bib272) ) studies the role of deep learning in
    Earth observation using remotely sensed data. It covers a broad range of tasks
    including image fusion, image segmentation and object-based analysis, as well
    as classification tasks. Yuan et al. ([Yuan2020review,](#bib.bib273) ) provide
    a review of DL applications for remote sensing, comparing the role of DL versus
    physical modelling of environmental variables and highlighting challenges in DL
    for remote sensing that need to be addressed. Chaves et al. ([Chaves2020review,](#bib.bib274)
    ) reviewed recent research using Landsat 8 and/or Sentinel-2 data for land cover
    mapping. While not focused on SITS DL methods, the review notes the growing importance
    of these methods. Moskolai et al. ([Moskolai2021apps,](#bib.bib275) ) is a review
    of forecasting applications using DL with SITS data that provides an analysis
    of the main DL architectures that are relevant for classification as well as forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, this survey paper has discussed a variety of deep network architectures
    for time series classification and extrinsic regression tasks, including multilayer
    perceptrons, convolutional neural networks, recurrent neural networks, and attention-based
    models. We have also highlighted refinements that have been made to improve the
    performance of these models on time series tasks. Additionally, we have discussed
    two critical applications of time series classification and regression, human
    activity recognition and satellite Earth observation. Overall, using deep network
    architectures and refinements has enabled significant progress in the field of
    time series classification and will continue to be essential for addressing a
    wide range of real-world problems. We hope this survey will stimulate further
    research using deep learning techniques for time series classification and extrinsic
    regression. Additionally, we provide a carefully curated collection of sources,
    available at [https://github.com/Navidfoumani/TSC_Survey](https://github.com/Navidfoumani/TSC_Survey),
    to further support the research community.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This work was supported by an Australian Government Research Training Program
    (RTP) scholarship.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1) Q. Yang and X. Wu, “10 challenging problems in data mining research,” *Int.
    J. Inf. Tech. & Decision Making*, vol. 5, no. 04, pp. 597–604, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) P. Esling and C. Agon, “Time-series data mining,” *ACM Computing Surveys
    (CSUR)*, vol. 45, no. 1, pp. 1–34, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(3) H. F. Nweke, Y. W. Teh, M. A. Al-Garadi, and U. R. Alo, “Deep learning
    algorithms for human activity recognition using mobile and wearable sensor networks:
    State of the art and research challenges,” *Expert Systems with Applications*,
    vol. 105, pp. 233–261, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(4) J. Wang, Y. Chen, S. Hao, X. Peng, and L. Hu, “Deep learning for sensor-based
    activity recognition: A survey,” *Pattern recognition letters*, vol. 119, pp.
    3–11, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(5) K. Chen, D. Zhang, L. Yao, B. Guo, Z. Yu, and Y. Liu, “Deep learning for
    sensor-based human activity recognition: Overview, challenges, and opportunities,”
    *ACM Computing Surveys (CSUR)*, vol. 54, no. 4, pp. 1–40, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (6) R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,
    K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball, “Deep learning
    with convolutional neural networks for EEG decoding and visualization,” *Human
    brain mapping*, vol. 38, no. 11, pp. 5391–5420, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (7) A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
    X. Liu, J. Marcus, M. Sun *et al.*, “Scalable and accurate deep learning with
    electronic health records,” *NPJ digital medicine*, vol. 1, no. 1, pp. 1–10, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (8) A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom, P. Southam,
    and E. Keogh, “The UEA multivariate time series classification archive, 2018,”
    *arXiv preprint:1811.00075*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (9) H. A. Dau, A. Bagnall, K. Kamgar, C.-C. M. Yeh, Y. Zhu, S. Gharghabi, C. A.
    Ratanamahatana, and E. Keogh, “The UCR time series archive,” *IEEE/CAA Journal
    of Automatica Sinica*, vol. 6, no. 6, pp. 1293–1305, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (10) C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb, “Time series extrinsic
    regression,” *Data Min. Knowl. Discov.*, vol. 35, no. 3, pp. 1032–1060, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(11) M. Middlehurst, P. Schäfer, and A. Bagnall, “Bake off redux: a review
    and experimental evaluation of recent time series classification algorithms,”
    *arXiv preprint arXiv:2304.13029*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(12) H. I. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt, J. Weber,
    G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean, “Inceptiontime: Finding
    alexnet for time series classification,” *Data Min. Knowl. Discov.*, vol. 34,
    no. 6, pp. 1936–1962, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (13) N. M. Foumani, C. W. Tan, G. I. Webb, and M. Salehi, “Improving position
    encoding of transformers for multivariate time series classification,” *Data Min.
    Knowl. Discov.*, Sep 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(14) A. Dempster, F. Petitjean, and G. I. Webb, “ROCKET: exceptionally fast
    and accurate time series classification using random convolutional kernels,” *Data
    Min. Knowl. Discov.*, vol. 34, no. 5, pp. 1454–1495, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(15) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, “Deep
    learning for time series classification: a review,” *Data Min Knowl Discov*, vol. 33,
    no. 4, pp. 917–963, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(16) Z. Wang, W. Yan, and T. Oates, “Time series classification from scratch
    with deep neural networks: A strong baseline,” in *2017 International joint conference
    on neural networks (IJCNN)*.   IEEE, 2017, pp. 1578–1585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(17) Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun, “Transformers
    in time series: A survey,” *arXiv preprint:2202.07125*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (18) Y. Hao and H. Cao, “A new attention mechanism to classify multivariate
    time series,” in *29th Int. Joint Conf. Artificial Intelligence*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (19) G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, “A
    transformer-based framework for multivariate time series representation learning,”
    in *27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining*, 2021, pp.
    2114–2124.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(20) X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, “Self-supervised
    learning: Generative or contrastive,” *IEEE transactions on knowledge and data
    engineering*, vol. 35, no. 1, pp. 857–876, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (21) E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh, X. Li, and C. Guan, “Time-series
    representation learning via temporal and contextual contrasting,” in *Proceedings
    of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21*,
    2021, pp. 2352–2359.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(22) C.-H. H. Yang, Y.-Y. Tsai, and P.-Y. Chen, “Voice2series: Reprogramming
    acoustic models for time series classification,” in *Int. conf. mach. learn.*   PMLR,
    2021, pp. 11 808–11 819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(23) Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, and B. Xu, “Ts2vec:
    Towards universal representation of time series,” in *AAAI*, vol. 36, no. 8, 2022,
    pp. 8980–8987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(24) N. M. Foumani, C. W. Tan, G. I. Webb, and M. Salehi, “Series2vec: Similarity-based
    self-supervised representation learning for time series classification,” *arXiv
    preprint arXiv:2312.03998*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(25) A. Bagnall, J. Lines, A. Bostrom, J. Large, and E. Keogh, “The great time
    series classification bake off: a review and experimental evaluation of recent
    algorithmic advances,” *Data Min. Knowl. Discov.*, vol. 31, no. 3, pp. 606–660,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(26) A. P. Ruiz, M. Flynn, J. Large, M. Middlehurst, and A. Bagnall, “The great
    multivariate time series classification bake off: a review and experimental evaluation
    of recent algorithmic advances,” *Data Min. Knowl. Discov.*, pp. 1–49, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (27) C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb, “Monash University,
    UEA, UCR time series regression archive,” *arXiv preprint:2006.10996*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (28) M. Längkvist, L. Karlsson, and A. Loutfi, “A review of unsupervised feature
    learning and deep learning for time-series modeling,” *Pattern Recognition Letters*,
    vol. 42, pp. 11–24, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (29) Y. Bengio, L. Yao, G. Alain, and P. Vincent, “Generalized denoising auto-encoders
    as generative models,” *Advances neural inf. process. syst.*, vol. 26, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (30) Q. Hu, R. Zhang, and Y. Zhou, “Transfer learning for short-term wind speed
    prediction with deep neural networks,” *Renewable Energy*, vol. 85, pp. 83–95,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (31) J. Serrà, S. Pascual, and A. Karatzoglou, “Towards a universal neural network
    encoder for time series.” in *CCIA*, 2018, pp. 120–129.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (32) D. Banerjee, K. Islam, K. Xue, G. Mei, L. Xiao, G. Zhang, R. Xu, C. Lei,
    S. Ji, and J. Li, “A deep transfer learning approach for improved post-traumatic
    stress disorder diagnosis,” *Knowledge and Information Systems*, vol. 60, no. 3,
    pp. 1693–1724, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (33) W. Aswolinskiy, R. F. Reinhart, and J. Steil, “Time series classification
    in reservoir-and model-space,” *Neural Processing Letters*, vol. 48, no. 2, pp.
    789–809, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(34) E. Brophy, Z. Wang, Q. She, and T. Ward, “Generative adversarial networks
    in time series: A systematic literature review,” *ACM Comput. Surv.*, vol. 55,
    no. 10, feb 2023\. [Online]. Available: [https://doi.org/10.1145/3559540](https://doi.org/10.1145/3559540)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (35) F. A. Del Campo, M. C. G. Neri, O. O. V. Villegas, V. G. C. Sánchez, H. d.
    J. O. Domínguez, and V. G. Jiménez, “Auto-adaptive multilayer perceptron for univariate
    time series classification,” *Expert Systems with Applications*, vol. 181, p.
    115147, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (36) B. K. Iwana, V. Frinken, and S. Uchida, “A robust dissimilarity-based neural
    network for temporal pattern recognition,” in *2016 15th International Conference
    on Frontiers in Handwriting Recognition (ICFHR)*.   IEEE, 2016, pp. 265–270.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(37) ——, “DTW-NN: A novel neural network for time series recognition using
    dynamic alignment between inputs and weights,” *Knowledge-Based Systems*, vol.
    188, p. 104971, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(38) N. Tabassum, S. Menon, and A. Jastrzebska, “Time-series classification
    with safe: Simple and fast segmented word embedding-based neural time series classifier,”
    *Information Processing & Management*, vol. 59, no. 5, p. 103044, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (39) A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Advances neural inf. process. syst.*,
    vol. 25, pp. 1097–1105, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (40) J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern recognition*, vol. 77, pp. 354–377, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (41) Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (42) Y. Zheng, Q. Liu, E. Chen, Y. Ge, and J. L. Zhao, “Time series classification
    using multi-channels deep convolutional neural networks,” in *International Conference
    on Web-Age Information Management*.   Springer, 2014, pp. 298–310.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (43) J. Yang, M. N. Nguyen, P. P. San, X. L. Li, and S. Krishnaswamy, “Deep
    convolutional neural networks on multichannel time series for human activity recognition,”
    in *Twenty-fourth international joint conference on artificial intelligence*,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (44) B. Zhao, H. Lu, S. Chen, J. Liu, and D. Wu, “Convolutional neural networks
    for time series classification,” *Journal of Systems Engineering and Electronics*,
    vol. 28, no. 1, pp. 162–169, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (45) J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *IEEE conf. comp. vision patt. recognit.*, 2015, pp.
    3431–3440.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (46) K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *IEEE conf. comp. vision patt. recognit.*, 2016, pp. 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (47) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *IEEE conf. comp. vision patt.
    recognit.*, 2016, pp. 2921–2929.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (48) X. Zou, Z. Wang, Q. Li, and W. Sheng, “Integration of residual network
    and convolutional neural network along with various activation functions and global
    pooling for time series classification,” *Neurocomputing*, vol. 367, pp. 39–45,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(49) Y. Li, X. Zhang, and D. Chen, “Csrnet: Dilated convolutional neural networks
    for understanding the highly congested scenes,” in *IEEE conf. comp. vision patt.
    recognit.*, 2018, pp. 1091–1100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (50) O. Yazdanbakhsh and S. Dick, “Multivariate time series classification using
    dilated convolutional neural network,” *arXiv preprint:1905.01697*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (51) S. N. M. Foumani, C. W. Tan, and M. Salehi, “Disjoint-cnn for multivariate
    time series classification,” in *2021 Int. Conf. Data Min. Workshops (ICDMW)*.   IEEE,
    2021, pp. 760–769.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(52) M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “MobileNet-V2:
    Inverted residuals and linear bottlenecks,” in *IEEE conf. comp. vision patt.
    recognit.*, 2018, pp. 4510–4520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (53) Z. Wang and T. Oates, “Encoding time series as images for visual inspection
    and classification using tiled convolutional neural networks,” in *Workshops at
    the twenty-ninth AAAI conference on artificial intelligence*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (54) N. Hatami, Y. Gavet, and J. Debayle, “Classification of time-series images
    using deep convolutional neural networks,” in *Tenth international conference
    on machine vision (ICMV 2017)*, vol. 10696.   SPIE, 2018, pp. 242–249.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (55) S. Karimi-Bidhendi, F. Munshi, and A. Munshi, “Scalable classification
    of univariate and multivariate time series,” in *2018 IEEE International Conference
    on Big Data (Big Data)*.   IEEE, 2018, pp. 1598–1605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (56) Y. Zhao and Z. Cai, “Classify multivariate time series by deep neural network
    image classification,” in *2019 2nd China Symposium on Cognitive Computing and
    Hybrid Intelligence (CCHI)*.   IEEE, 2019, pp. 93–98.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (57) C.-L. Yang, Z.-X. Chen, and C.-Y. Yang, “Sensor classification using convolutional
    neural network by encoding multivariate time series as two-dimensional colored
    images,” *Sensors*, vol. 20, no. 1, p. 168, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (58) J.-P. E. S. O. Kamphorst, D. Ruelle *et al.*, “Recurrence plots of dynamical
    systems,” *Europhysics Letters*, vol. 4, no. 9, p. 17, 1987.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (59) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *IEEE conf. comp. vision patt.
    recognit.*, 2016, pp. 2818–2826.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (60) W. Chen and K. Shi, “A deep learning framework for time series classification
    using relative position matrix and convolutional neural network,” *Neurocomputing*,
    vol. 359, pp. 384–394, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (61) Z. Cui, W. Chen, and Y. Chen, “Multi-scale convolutional neural networks
    for time series classification,” *arXiv preprint:1603.06995*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (62) A. Le Guennec, S. Malinowski, and R. Tavenard, “Data augmentation for time
    series classification using convolutional neural networks,” in *ECML/PKDD workshop
    on advanced analytics and learning on temporal data*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (63) C.-L. Liu, W.-H. Hsaio, and Y.-C. Tu, “Time series classification with
    multivariate convolutional neural network,” *IEEE Transactions on Industrial Electronics*,
    vol. 66, no. 6, pp. 4788–4797, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (64) A. Brunel, J. Pasquet, J. PASQUET, N. Rodriguez, F. Comby, D. Fouchez,
    and M. Chaumont, “A cnn adapted to time series for the classification of supernovae,”
    *Electronic imaging*, vol. 2019, no. 14, pp. 90–1, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (65) J. Sun, S. Takeuchi, and I. Yamasaki, “Prototypical inception network with
    cross branch attention for time series classification,” in *2021 International
    Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2021, pp. 1–7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (66) S. Usmankhujaev, B. Ibrokhimov, S. Baydadaev, and J. Kwon, “Time series
    classification with inceptionfcn,” *Sensors*, vol. 22, no. 1, p. 157, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(67) X. Gong, Y.-W. Si, Y. Tian, C. Lin, X. Zhang, and X. Liu, “Kdctime: Knowledge
    distillation with calibration on inceptiontime for time-series classification,”
    *Inf. Sci.*, vol. 613, pp. 184–203, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(68) A. Ismail-Fawaz, M. Devanne, S. Berretti, J. Weber, and G. Forestier,
    “Lite: Light inception with boosting techniques for time series classification,”
    in *2023 IEEE 10th International Conference on Data Science and Advanced Analytics
    (DSAA)*.   IEEE, 2023, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (69) C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *IEEE conf.
    comp. vision patt. recognit.*, 2015, pp. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (70) C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, inception-resnet
    and the impact of residual connections on learning,” in *Thirty-first AAAI conference
    on artificial intelligence*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(71) M. Ronald, A. Poulose, and D. S. Han, “isplinception: an inception-resnet
    deep learning architecture for human activity recognition,” *IEEE Access*, vol. 9,
    pp. 68 985–69 001, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (72) A. Ismail-Fawaz, M. Devanne, J. Weber, and G. Forestier, “Deep learning
    for time series classification using new hand-crafted convolution filters,” in
    *2022 IEEE International Conference on Big Data (Big Data)*.   IEEE, 2022, pp.
    972–981.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (73) M. Hüsken and P. Stagge, “Recurrent neural networks for time series classification,”
    *Neurocomputing*, vol. 50, pp. 223–235, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(74) D. Dennis, D. A. E. Acar, V. Mandikal, V. S. Sadasivan, V. Saligrama,
    H. V. Simhadri, and P. Jain, “Shallow rnn: accurate time-series classification
    on resource constrained devices,” *Advances neural inf. process. syst.*, vol. 32,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (75) S. Fernández, A. Graves, and J. Schmidhuber, “Sequence labelling in structured
    domains with hierarchical recurrent neural networks,” in *20th International Joint
    Conference on Artificial Intelligence, IJCAI 2007*, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (76) M. Hermans and B. Schrauwen, “Training and analysing deep recurrent neural
    networks,” *Advances neural inf. process. syst.*, vol. 26, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (77) R. Pascanu, T. Mikolov, and Y. Bengio, “On the difficulty of training recurrent
    neural networks,” in *Int. conf. mach. learn.*   PMLR, 2013, pp. 1310–1318.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (78) S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (79) J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint:1412.3555*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (80) I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” *Advances neural inf. process. syst.*, vol. 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (81) J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
    K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for visual
    recognition and description,” in *IEEE conf. comp. vision patt. recognit.*, 2015,
    pp. 2625–2634.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (82) A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating
    image descriptions,” in *IEEE conf. comp. vision patt. recognit.*, 2015, pp. 3128–3137.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (83) Y. Tang, J. Xu, K. Matsumoto, and C. Ono, “Sequence-to-sequence model with
    attention for time series classification,” in *2016 IEEE 16th Int. Conf. Data
    Min. Workshops (ICDMW)*.   IEEE, 2016, pp. 503–510.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(84) P. Malhotra, V. TV, L. Vig, P. Agarwal, and G. Shroff, “Timenet: Pre-trained
    deep recurrent neural network for time series classification,” *arXiv preprint:1706.08838*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (85) F. Karim, S. Majumdar, H. Darabi, and S. Harford, “Multivariate LSTM-FCNs
    for time series classification,” *Neural Networks*, vol. 116, pp. 237–245, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(86) X. Zhang, Y. Gao, J. Lin, and C.-T. Lu, “Tapnet: Multivariate time series
    classification with attentional prototypical network,” in *AAAI Conference on
    Artificial Intelligence*, vol. 34, no. 04, 2020, pp. 6845–6852.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(87) J. Zuo, K. Zeitouni, and Y. Taher, “Smate: Semi-supervised spatio-temporal
    representation learning on multivariate time series,” in *2021 IEEE International
    Conference on Data Mining (ICDM)*.   IEEE, 2021, pp. 1565–1570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (88) F. Karim, S. Majumdar, H. Darabi, and S. Chen, “LSTM fully convolutional
    networks for time series classification,” *IEEE access*, vol. 6, pp. 1662–1669,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(89) S. Lin and G. C. Runger, “Gcrnn: Group-constrained convolutional recurrent
    neural network,” *IEEE transactions on neural networks and learning systems*,
    vol. 29, no. 10, pp. 4709–4718, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (90) R. Mutegeki and D. S. Han, “A CNN-LSTM approach to human activity recognition,”
    in *2020 IEEE Int. Conf. Comput. Intell. Commun. Technol. (ICAIIC)*.   IEEE, 2020,
    pp. 362–366.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (91) R. Pascanu, T. Mikolov, and Y. Bengio, “Understanding the exploding gradient
    problem,” *CoRR, abs/1211.5063*, vol. 2, no. 417, p. 1, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (92) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances neural inf.
    process. syst.*, vol. 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(93) J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” in *Proceedings
    of NAACL-HLT 2019*, vol. 1.   Stroudsburg, PA, USA: Association for Computational
    Linguistics, 2019, pp. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(94) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” *arXiv preprint:2010.11929*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (95) S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan, “Enhancing
    the locality and breaking the memory bottleneck of transformer on time series
    forecasting,” *Advances neural inf. process. syst.*, vol. 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(96) H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang, “Informer:
    Beyond efficient transformer for long sequence time-series forecasting,” in *Proceedings
    of AAAI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(97) D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, “Bendr: using transformers
    and a contrastive self-supervised learning task to learn from massive amounts
    of eeg data,” *Frontiers in Human Neuroscience*, vol. 15, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(98) Y. Yuan, G. Xun, F. Ma, Y. Wang, N. Du, K. Jia, L. Su, and A. Zhang, “Muvan:
    A multi-view attention network for multivariate temporal data,” in *2018 IEEE
    International Conference on Data Mining (ICDM)*.   IEEE, 2018, pp. 717–726.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(99) T.-Y. Hsieh, S. Wang, Y. Sun, and V. Honavar, “Explainable multivariate
    time series classification: A deep neural network which learns to attend to important
    variables as well as time intervals,” in *14th ACM International Conference on
    Web Search and Data Mining*, 2021, pp. 607–615.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (100) W. Chen and K. Shi, “Multi-scale attention convolutional neural network
    for time series classification,” *Neural Networks*, vol. 136, pp. 126–140, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (101) Y. Yuan, G. Xun, F. Ma, Q. Suo, H. Xue, K. Jia, and A. Zhang, “A novel
    channel-aware attention framework for multi-channel eeg seizure detection via
    multi-view deep learning,” in *2018 IEEE EMBS International Conference on Biomedical
    & Health Informatics (BHI)*.   IEEE, 2018, pp. 206–209.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(102) Y. Liang, S. Ke, J. Zhang, X. Yi, and Y. Zheng, “Geoman: Multi-level
    attention networks for geo-sensory time series prediction,” in *IJCAI*, vol. 2018,
    2018, pp. 3428–3434.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (103) J. Hu and W. Zheng, “Multistage attention network for multivariate time
    series prediction,” *Neurocomputing*, vol. 383, pp. 122–137, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (104) X. Cheng, P. Han, G. Li, S. Chen, and H. Zhang, “A novel channel and temporal-wise
    attention in convolutional networks for multivariate time series classification,”
    *IEEE Access*, vol. 8, pp. 212 247–212 257, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(105) Z. Xiao, X. Xu, H. Xing, S. Luo, P. Dai, and D. Zhan, “Rtfn: a robust
    temporal feature network for time series classification,” *Inf. Sci.*, vol. 571,
    pp. 65–86, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(106) J. Wang, C. Yang, X. Jiang, and J. Wu, “When: A wavelet-dtw hybrid attention
    network for heterogeneous time series analysis,” in *Proceedings of the 29th ACM
    SIGKDD Conference on Knowledge Discovery and Data Mining*, 2023, pp. 2361–2373.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (107) M. Jaderberg, K. Simonyan, A. Zisserman *et al.*, “Spatial transformer
    networks,” *Advances neural inf. process. syst.*, vol. 28, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(108) S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional block
    attention module,” in *European conference on computer vision*, 2018, pp. 3–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (109) J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *IEEE
    conf. comp. vision patt. recognit.*, 2018, pp. 7132–7141.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (110) T. Wang, Z. Liu, T. Zhang, and Y. Li, “Time series classification based
    on multi-scale dynamic convolutional features and distance features,” in *2021
    2nd Asia Symposium on Signal Processing (ASSP)*.   IEEE, 2021, pp. 239–246.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(111) H. Song, D. Rajan, J. Thiagarajan, and A. Spanias, “Attend and diagnose:
    Clinical time series analysis using attention models,” in *AAAI conference on
    artificial intelligence*, vol. 32, no. 1, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (112) C.-c. Jin and X. Chen, “An end-to-end framework combining time–frequency
    expert knowledge and modified transformer networks for vibration signal classification,”
    *Expert Systems with Applications*, vol. 171, p. 114570, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(113) C. E. Rasmussen, *Gaussian Processes in Machine Learning*.   Berlin,
    Heidelberg: Springer Berlin Heidelberg, 2004, pp. 63–71.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(114) T. Allam Jr and J. D. McEwen, “Paying attention to astronomical transients:
    Photometric classification with the time-series transformer,” *arXiv preprint:2105.06178*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (115) M. Liu, S. Ren, S. Ma, J. Jiao, Y. Chen, Z. Wang, and W. Song, “Gated
    transformer networks for multivariate time series classification,” *arXiv preprint:2103.14438*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (116) B. Zhao, H. Xing, X. Wang, F. Song, and Z. Xiao, “Rethinking attention
    mechanism in time series classification,” *arXiv preprint:2207.07564*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(117) Y. Ren, L. Li, X. Yang, and J. Zhou, “Autotransformer: Automatic transformer
    architecture design for time series classification,” in *Pacific-Asia Conference
    on Knowledge Discovery and Data Mining*.   Springer, 2022, pp. 143–155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(118) M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb, I. King,
    and S. Pan, “A Survey on Graph Neural Networks for Time Series: Forecasting, Classification,
    Imputation, and Anomaly Detection,” *arXiv*, vol. 14, no. 8, pp. 1–27, jul 2023\.
    [Online]. Available: [http://arxiv.org/abs/2307.03759](http://arxiv.org/abs/2307.03759)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (119) Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A Comprehensive
    Survey on Graph Neural Networks,” *IEEE Transactions on Neural Networks and Learning
    Systems*, vol. 32, no. 1, pp. 4–24, jan 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (120) F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The Graph Neural Network Model,” *IEEE Transactions on Neural Networks*, vol. 20,
    no. 1, pp. 61–80, jan 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(121) W. Xi, A. Jain, L. Zhang, and J. Lin, “LB-SimTSC: An Efficient Similarity-Aware
    Graph Neural Network for Semi-Supervised Time Series Classification,” *arXiv*,
    jan 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(122) H. Liu, X. Liu, D. Yang, Z. Liang, H. Wang, Y. Cui, and J. Gu, “TodyNet:
    Temporal Dynamic Graph Neural Network for Multivariate Time Series Classification,”
    *arXiv*, vol. XX, no. Xx, pp. 1–10, apr 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (123) S. Bloemheuvel, J. van den Hoogen, D. Jozinović, A. Michelini, and M. Atzmueller,
    “Graph neural networks for multivariate time series regression with application
    to seismic data,” *International Journal of Data Science and Analytics*, vol. 16,
    no. 3, pp. 317–332, sep 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(124) Z. Cheng, Y. Yang, S. Jiang, W. Hu, Z. Ying, Z. Chai, and C. Wang, “Time2Graph+:
    Bridging Time Series and Graph Representation Learning via Multiple Attentions,”
    *IEEE Transactions on Knowledge and Data Engineering*, vol. 35, no. 2, pp. 1–1,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (125) I. C. Covert, B. Krishnan, I. Najm, J. Zhan, M. Shore, J. Hixson, and
    M. J. Po, “Temporal graph convolutional networks for automatic seizure detection,”
    in *Machine Learning for Healthcare Conference*.   PMLR, 2019, pp. 160–180.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (126) T. Song, W. Zheng, P. Song, and Z. Cui, “EEG Emotion Recognition Using
    Dynamical Graph Convolutional Neural Networks,” *IEEE Transactions on Affective
    Computing*, vol. 11, no. 3, pp. 532–541, jul 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(127) Z. Jia, Y. Lin, J. Wang, R. Zhou, X. Ning, Y. He, and Y. Zhao, “Graphsleepnet:
    Adaptive spatial-temporal graph convolutional networks for sleep stage classification.”
    in *IJCAI*, 2020, pp. 1324–1330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (128) Z. Ma, G. Mei, E. Prezioso, Z. Zhang, and N. Xu, “A deep learning approach
    using graph convolutional networks for slope deformation prediction based on time-series
    displacement data,” *Neural Computing and Applications*, vol. 33, no. 21, pp.
    14 441–14 457, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (129) T. Li, Z. Zhao, C. Sun, R. Yan, and X. Chen, “Multireceptive field graph
    convolutional networks for machine fault diagnosis,” *IEEE Transactions on Industrial
    Electronics*, vol. 68, no. 12, pp. 12 739–12 749, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (130) D. Nhu, M. Janmohamed, P. Perucca, A. Gilligan, P. Kwan, T. O’Brien, C. Tan,
    and L. Kuhlmann, “Graph convolutional network for generalized epileptiform abnormality
    detection on eeg,” in *2021 IEEE Signal Processing in Medicine and Biology Symposium
    (SPMB)*.   IEEE, 2021, pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (131) S. Tang, J. A. Dunnmon, K. Saab, X. Zhang, Q. Huang, F. Dubost, D. L.
    Rubin, and C. Lee-Messer, “Self-Supervised Graph Neural Networks for Improved
    Electroencephalographic Seizure Analysis,” *ICLR 2022 - 10th Int. Conf. Learning
    Representations*, pp. 1–23, apr 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (132) X. Zhang, M. Zeman, T. Tsiligkaridis, and M. Zitnik, “Graph-Guided Network
    for Irregularly Sampled Multivariate Time Series,” *ICLR 2022 - 10th International
    Conference on Learning Representations*, pp. 1–21, oct 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (133) A. M. Censi, D. Ienco, Y. J. E. Gbodjo, R. G. Pensa, R. Interdonato, and
    R. Gaetano, “Attentive spatial temporal graph CNN for land cover mapping from
    multi temporal remote sensing data,” *IEEE Access*, vol. 9, pp. 23 070–23 082,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (134) T. Azevedo, A. Campbell, R. Romero-Garcia, L. Passamonti, R. A. Bethlehem,
    P. Liò, and N. Toschi, “A deep graph neural network architecture for modelling
    spatio-temporal dynamics in resting-state functional MRI data,” *Medical Image
    Analysis*, vol. 79, p. 102471, jul 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (135) Z. Duan, H. Xu, Y. Wang, Y. Huang, A. Ren, Z. Xu, Y. Sun, and W. Wang,
    “Multivariate time-series classification with hierarchical variational graph pooling,”
    *Neural Networks*, vol. 154, pp. 481–490, oct 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (136) D. Zha, K.-h. Lai, K. Zhou, and X. Hu, “Towards Similarity-Aware Time-Series
    Classification,” in *Proceedings of the 2022 SIAM International Conference on
    Data Mining (SDM)*, Philadelphia, PA, jan 2022, pp. 199–207.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (137) L. Tulczyjew, M. Kawulok, N. Longepe, B. Le Saux, and J. Nalepa, “Graph
    Neural Networks Extract High-Resolution Cultivated Land Maps From Sentinel-2 Image
    Series,” *IEEE Geoscience and Remote Sensing Letters*, vol. 19, pp. 1–5, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (138) L. Sun, C. Li, B. Liu, and Y. Zhang, “Class-driven Graph Attention Network
    for Multi-label Time Series Classification in Mobile Health Digital Twins,” *IEEE
    Journal on Selected Areas in Communications*, vol. 41, no. 10, pp. 3267–3278,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(139) C. Dufourg, C. Pelletier, S. May, and S. Lefèvre, “Graph Dynamic Earth
    Net: Spatio-Temporal Graph Benchmark for Satellite Image Time Series,” in *IGARSS
    2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium*.   IEEE,
    jul 2023, pp. 7164–7167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (140) E. Keogh and C. A. Ratanamahatana, “Exact indexing of dynamic time warping,”
    *Knowl. Inform. Systems*, vol. 7, no. 3, pp. 358–386, 2005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (141) T. N. Kipf and M. Welling, “Semi-Supervised Classification with Graph
    Convolutional Networks,” *5th International Conference on Learning Representations,
    ICLR 2017 - Conference Track Proceedings*, pp. 1–14, sep 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (142) L. Yang and S. Hong, “Unsupervised time-series representation learning
    with iterative bilinear temporal-spectral fusion,” in *ICML*, 2022, pp. 25 038–25 054.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (143) A. Hyvarinen and H. Morioka, “Unsupervised feature extraction by time-contrastive
    learning and nonlinear ica,” *Advances in neural information processing systems*,
    vol. 29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (144) J.-Y. Franceschi, A. Dieuleveut, and M. Jaggi, “Unsupervised scalable
    representation learning for multivariate time series,” *NeurIPS*, vol. 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (145) S. Tonekaboni, D. Eytan, and A. Goldenberg, “Unsupervised representation
    learning for time series with temporal neighborhood coding,” *arXiv preprint arXiv:2106.00750*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(146) K. Wickstrøm, M. Kampffmeyer, K. Ø. Mikalsen, and R. Jenssen, “Mixing
    up contrastive learning: Self-supervised representation learning for time series,”
    *Pattern Recognition Letters*, vol. 155, pp. 54–61, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(147) X. Yang, Z. Zhang, and R. Cui, “Timeclr: A self-supervised contrastive
    learning framework for univariate time series representation,” *Knowledge-Based
    Systems*, vol. 245, p. 108606, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (148) X. Zhang, Z. Zhao, T. Tsiligkaridis, and M. Zitnik, “Self-supervised contrastive
    pre-training for time series via time-frequency consistency,” in *Proceedings
    of Neural Information Processing Systems, NeurIPS*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(149) Q. Meng, H. Qian, Y. Liu, L. Cui, Y. Xu, and Z. Shen, “Mhccl: masked
    hierarchical cluster-wise contrastive learning for multivariate time series,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 37, no. 8,
    2023, pp. 9153–9161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(150) R. R. Chowdhury, X. Zhang, J. Shang, R. K. Gupta, and D. Hong, “Tarnet:
    Task-aware reconstruction for time-series transformer,” in *28th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining, Washington, DC, USA*, 2022, pp. 14–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(151) M. Cheng, Q. Liu, Z. Liu, H. Zhang, R. Zhang, and E. Chen, “Timemae:
    Self-supervised representations of time series with decoupled masked autoencoders,”
    *arXiv preprint arXiv:2303.00320*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (152) W. Zhang, L. Yang, S. Geng, and S. Hong, “Self-supervised time series
    representation learning via cross reconstruction transformer,” *IEEE Transactions
    on Neural Networks and Learning Systems*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (153) A. Ismail-Fawaz, M. Devanne, S. Berretti, J. Weber, and G. Forestier,
    “Finding foundation models for time series classification with a pretext task,”
    *arXiv preprint arXiv:2311.14534*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (154) C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation
    for deep learning,” *Journal of big data*, vol. 6, no. 1, pp. 1–48, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (155) T. T. Um, F. M. Pfister, D. Pichler, S. Endo, M. Lang, S. Hirche, U. Fietzek,
    and D. Kulić, “Data augmentation of wearable sensor data for parkinson’s disease
    monitoring using convolutional neural networks,” in *Proc. 19th ACM int. conf.
    multimodal interaction*, 2017, pp. 216–220.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(156) K. M. Rashid and J. Louis, “Window-warping: a time series data augmentation
    of imu data for construction equipment activity identification,” in *ISARC. Proceedings
    of the International Symposium on Automation and Robotics in Construction*, vol. 36.   IAARC
    Publications, 2019, pp. 651–657.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (157) B. K. Iwana and S. Uchida, “Time series data augmentation for neural networks
    by time warping with a discriminative teacher,” in *2020 25th International Conference
    on Pattern Recognition (ICPR)*.   IEEE, 2021, pp. 3558–3565.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (158) T.-S. Nguyen, S. Stueker, J. Niehues, and A. Waibel, “Improving sequence-to-sequence
    speech recognition training with on-the-fly data augmentation,” in *ICASSP 2020-2020
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE,
    2020, pp. 7689–7693.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (159) B. Vachhani, C. Bhat, and S. K. Kopparapu, “Data augmentation using healthy
    speech for dysarthric speech recognition.” in *Interspeech*, 2018, pp. 471–475.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(160) J. Gao, X. Song, Q. Wen, P. Wang, L. Sun, and H. Xu, “Robusttad: Robust
    time series anomaly detection via decomposition and convolutional neural networks,”
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (161) Z. Cui, W. Chen, and Y. Chen, “Multi-scale convolutional neural networks
    for time series classification,” 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (162) A. Le Guennec, S. Malinowski, and R. Tavenard, “Data Augmentation for
    Time Series Classification using Convolutional Neural Networks,” in *ECML/PKDD
    on Advanced Analytics and Learning on Temporal Data*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (163) G. Forestier, F. Petitjean, H. A. Dau, G. I. Webb, and E. Keogh, “Generating
    synthetic time series to augment sparse datasets,” in *2017 IEEE international
    conference on data mining (ICDM)*.   IEEE, 2017, pp. 865–870.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (164) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, “Data
    augmentation using synthetic data for time series classification with deep residual
    networks,” 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (165) T. Terefe, M. Devanne, J. Weber, D. Hailemariam, and G. Forestier, “Time
    series averaging using multi-tasking autoencoder,” in *2020 IEEE 32nd International
    Conference on Tools with Artificial Intelligence (ICTAI)*.   IEEE, 2020, pp. 1065–1072.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (166) B. K. Iwana and S. Uchida, “An empirical survey of data augmentation for
    time series classification with neural networks,” *Plos one*, vol. 16, no. 7,
    p. e0254841, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (167) G. Pialla, M. Devanne, J. Weber, L. Idoumghar, and G. Forestier, “Data
    augmentation for time series classification with deep learning models,” in *International
    Workshop on Advanced Analytics and Learning on Temporal Data*.   Springer, 2022,
    pp. 117–132.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(168) Z. Gao, L. Li, and T. Xu, “Data augmentation for time-series classification:
    An extensive empirical study and comprehensive survey,” *arXiv preprint arXiv:2310.10060*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(169) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE conf. comp. vision patt.
    recognit.*   Ieee, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (170) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, “Transfer
    learning for time series classification,” in *2018 IEEE international conference
    on big data (Big Data)*.   IEEE, 2018, pp. 1367–1376.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (171) S. Spiegel, “Transfer learning for time series classification in dissimilarity
    spaces,” *Proceedings of AALTD*, vol. 78, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (172) F. Li, K. Shirahama, M. A. Nisar, X. Huang, and M. Grzegorzek, “Deep transfer
    learning for time series data based on sensor modality classification,” *Sensors*,
    vol. 20, no. 15, p. 4271, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (173) Y. Rotem, N. Shimoni, L. Rokach, and B. Shapira, “Transfer learning for
    time series classification using synthetic data generation,” in *International
    Symposium on Cyber Security, Cryptology, and Machine Learning*.   Springer, 2022,
    pp. 232–246.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (174) A. Senanayaka, A. Al Mamun, G. Bond, W. Tian, H. Wang, S. Fuller, T. Falls,
    S. Rahimi, and L. Bian, “Similarity-based multi-source transfer learning approach
    for time series classification,” *International Journal of Prognostics and Health
    Management*, vol. 13, no. 2, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(175) K. Kashiparekh, J. Narwariya, P. Malhotra, L. Vig, and G. Shroff, “Convtimenet:
    A pre-trained deep convolutional neural network for time series classification,”
    in *2019 International Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2019,
    pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (176) D. Merlin Praveena, D. Angelin Sarah, and S. Thomas George, “Deep Learning
    Techniques for EEG Signal Applications–A Review,” *IETE Journal of Research*,
    vol. 68, no. 4, pp. 3030–3037, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(177) X. Liu, H. Wang, Z. Li, and L. Qin, “Deep learning in ECG diagnosis:
    A review,” *Knowledge-Based Systems*, vol. 227, p. 107187, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (178) N. Zaini, L. W. Ean, A. N. Ahmed, and M. A. Malek, “A systematic literature
    review of deep learning neural network for time series air quality forecasting,”
    *Environmental Science and Pollution Research*, vol. 29, no. 4, pp. 4958–4990,
    jan 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(179) B. Zhang, Y. Rong, R. Yong, D. Qin, M. Li, G. Zou, and J. Pan, “Deep
    learning for air pollutant concentration prediction: A review,” *Atmospheric Environment*,
    vol. 290, p. 119347, dec 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (180) G. Toh and J. Park, “Review of Vibration-Based Structural Health Monitoring
    Using Deep Learning,” *Appl. Sci.*, vol. 10, no. 5, p. 1680, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(181) N. M. Thoppil, V. Vasu, and C. S. P. Rao, “Deep Learning Algorithms for
    Machinery Health Prognostics Using Time-Series Data: A Review,” *Journal of Vibration
    Engineering & Technologies*, vol. 9, no. 6, pp. 1123–1145, sep 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(182) L. Ren, Z. Jia, Y. Laili, and D. Huang, “Deep Learning for Time-Series
    Prediction in IIoT: Progress, Challenges, and Prospects,” *IEEE Transactions on
    Neural Networks and Learning Systems*, vol. PP, pp. 1–20, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(183) Y. Himeur, K. Ghanem, A. Alsalemi, F. Bensaali, and A. Amira, “Artificial
    intelligence based anomaly detection of energy consumption in buildings: A review,
    current trends and new perspectives,” *Applied Energy*, vol. 287, p. 116601, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(184) D. Stowell, “Computational bioacoustics with deep learning: a review
    and roadmap,” *PeerJ*, vol. 10, p. e13152, mar 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(185) N. Gupta, S. K. Gupta, R. K. Pathak, V. Jain, P. Rashidi, and J. S. Suri,
    “Human activity recognition in artificial intelligence framework: a narrative
    review,” *Artificial Intelligence Review*, vol. 55, no. 6, pp. 4755–4808, aug
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(186) E. Ramanujam, T. Perumal, and S. Padmavathi, “Human activity recognition
    with smartphone and wearable sensors using deep learning techniques: A review,”
    *IEEE Sensors Journal*, vol. 21, no. 12, pp. 13 029–13 040, jun 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(187) J. W. Lockhart, T. Pulickal, and G. M. Weiss, “Applications of mobile
    activity recognition,” in *2012 ACM Conference on Ubiquitous Computing - UbiComp
    ’12*.   New York, New York, USA: ACM Press, 2012, p. 1054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(188) E. M. Tapia, S. S. Intille, and K. Larson, “Activity recognition in the
    home using simple and ubiquitous sensors,” in *Lecture Notes in Computer Science*.   Berlin,
    Heidelberg: Springer, 2004, vol. 3001, pp. 158–175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(189) Y. Kong and Y. Fu, “Human action recognition and prediction: A survey,”
    *International Journal of Computer Vision*, vol. 130, no. 5, pp. 1366–1401, may
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (190) H.-B. Zhang, Y.-X. Zhang, B. Zhong, Q. Lei, L. Yang, J.-X. Du, and D.-S.
    Chen, “A comprehensive survey of vision-based human action recognition methods,”
    *Sensors*, vol. 19, no. 5, p. 1005, feb 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (191) F. Ordóñez and D. Roggen, “Deep convolutional and LSTM recurrent neural
    networks for multimodal wearable activity recognition,” *Sensors*, vol. 16, no. 1,
    p. 115, jan 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (192) A. Reiss and D. Stricker, “Introducing a new benchmarked dataset for activity
    monitoring,” in *16th Int. Symp. Wearable Computers*, 2012, pp. 108–109.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(193) M. Zhang and A. A. Sawchuk, “USC-HAD: A daily activity dataset for ubiquitous
    activity recognition using wearable sensors,” in *2012 ACM Conference on Ubiquitous
    Computing - UbiComp ’12*.   New York, New York, USA: ACM Press, 2012, p. 1036.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (194) D. Roggen, A. Calatroni, M. Rossi, T. Holleczek, K. Förster, G. Tröster,
    P. Lukowicz, D. Bannach, G. Pirkl *et al.*, “Collecting complex activity datasets
    in highly rich networked sensor environments,” in *Seventh international conference
    on networked sensing systems*.   IEEE, 2010, pp. 233–240.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (195) T. Sztyler, H. Stuckenschmidt, and W. Petrich, “Position-aware activity
    recognition with wearable devices,” *Pervasive and Mobile Computing*, vol. 38,
    pp. 281–295, jul 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (196) O. D. Lara and M. A. Labrador, “A survey on human activity recognition
    using wearable sensors,” *IEEE Communications Surveys & Tutorials*, vol. 15, no. 3,
    pp. 1192–1209, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (197) F. Gu, M.-H. Chung, M. Chignell, S. Valaee, B. Zhou, and X. Liu, “A survey
    on deep learning for human activity recognition,” *ACM Computing Surveys*, vol. 54,
    no. 8, pp. 1–34, nov 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (198) N. Y. Hammerla, S. Halloran, and T. Ploetz, “Deep, convolutional, and
    recurrent Models for human activity recognition using wearables,” *IJCAI International
    Joint Conference on Artificial Intelligence*, vol. 2016-Janua, pp. 1533–1540,
    apr 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (199) M. Zeng, L. T. Nguyen, B. Yu, O. J. Mengshoel, J. Zhu, P. Wu, and J. Zhang,
    “Convolutional neural networks for human activity recognition using mobile sensors,”
    in *6th International Conference on Mobile Computing, Applications and Services*.   ICST,
    2014, pp. 718–737.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(200) W. Jiang and Z. Yin, “Human activity recognition using wearable sensors
    by deep convolutional neural Networks,” in *23rd ACM international conference
    on Multimedia*.   New York, NY, USA: ACM, oct 2015, pp. 1307–1310.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (201) J. B. Yang, M. N. Nguyen, P. P. San, X. L. Li, and S. Krishnaswamy, “Deep
    convolutional neural networks on multichannel time series for human activity recognition,”
    *IJCAI International Joint Conference on Artificial Intelligence*, vol. 2015-Janua,
    pp. 3995–4001, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (202) C. A. Ronao and S.-B. Cho, “Human activity recognition with smartphone
    sensors using deep learning neural networks,” *Expert Systems with Applications*,
    vol. 59, pp. 235–244, oct 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (203) Y. Guan and T. Plötz, “Ensembles of deep LSTM learners for activity recognition
    using wearables,” *ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    vol. 1, no. 2, pp. 1–28, jun 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (204) S.-M. Lee, S. M. Yoon, and H. Cho, “Human activity recognition from accelerometer
    data using Convolutional Neural Network,” in *2017 IEEE International Conference
    on Big Data and Smart Computing (BigComp)*, vol. 83.   IEEE, feb 2017, pp. 131–134.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (205) A. Murad and J.-Y. Pyun, “Deep recurrent neural networks for human activity
    recognition,” *Sensors*, vol. 17, no. 11, p. 2556, nov 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (206) A. Ignatov, “Real-time human activity recognition from accelerometer data
    using Convolutional Neural Networks,” *Applied Soft Computing*, vol. 62, pp. 915–922,
    jan 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (207) F. Moya Rueda, R. Grzeszick, G. Fink, S. Feldhorst, and M. ten Hompel,
    “Convolutional neural networks for human activity recognition using body-worn
    sensors,” *Informatics*, vol. 5, no. 2, p. 26, may 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (208) R. Yao, G. Lin, Q. Shi, and D. C. Ranasinghe, “Efficient dense labelling
    of human activity sequences from wearables using fully convolutional networks,”
    *Pattern Recognition*, vol. 78, pp. 252–266, jun 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (209) M. Zeng, H. Gao, T. Yu, O. J. Mengshoel, H. Langseth, I. Lane, and X. Liu,
    “Understanding and improving recurrent networks for human activity recognition
    by continuous attention,” in *ACM International Symposium on Wearable Computers*,
    New York, NY, USA, 2018, pp. 56–63.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(210) H. Ma, W. Li, X. Zhang, S. Gao, and S. Lu, “AttnSense: Multi-level attention
    mechanism for multimodal human activity recognition,” in *Twenty-Eighth International
    Joint Conference on Artificial Intelligence*, California, 2019, pp. 3109–3115.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(211) C. Xu, D. Chai, J. He, X. Zhang, and S. Duan, “InnoHAR: A deep neural
    network for complex human activity recognition,” *IEEE Access*, vol. 7, pp. 9893–9902,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (212) H. Zhang, Z. Xiao, J. Wang, F. Li, and E. Szczerbicki, “A novel IoT-perceptive
    human activity recognition (HAR) approach using multihead convolutional attention,”
    *IEEE Internet of Things Journal*, vol. 7, no. 2, pp. 1072–1080, feb 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (213) S. K. Challa, A. Kumar, and V. B. Semwal, “A multibranch CNN-BiLSTM model
    for human activity recognition using wearable sensor data,” *The Visual Computer*,
    no. 0123456789, aug 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (214) S. Mekruksavanich and A. Jitpattanakul, “Deep Convolutional Neural Network
    with RNNs for complex activity recognition using wrist-worn wearable sensor data,”
    *Electronics*, vol. 10, no. 14, p. 1685, jul 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (215) L. Chen, X. Liu, L. Peng, and M. Wu, “Deep learning based multimodal complex
    human activity recognition using wearable devices,” *Applied Intelligence*, vol. 51,
    no. 6, pp. 4029–4042, jun 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (216) S. Mekruksavanich and A. Jitpattanakul, “LSTM networks using smartphone
    data for sensor-based human activity recognition in smart homes,” *Sensors*, vol. 21,
    no. 5, p. 1636, feb 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(217) ——, “Biometric user identification based on human activity recognition
    using wearable sensors: An experiment using deep learning models,” *Electronics*,
    vol. 10, no. 3, p. 308, jan 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (218) O. Nafea, W. Abdul, G. Muhammad, and M. Alsulaiman, “Sensor-based human
    activity recognition with spatio-temporal deep learning,” *Sensors*, vol. 21,
    no. 6, p. 2141, mar 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (219) S. P. Singh, M. K. Sharma, A. Lay-Ekuakille, D. Gangwar, and S. Gupta,
    “Deep ConvLSTM with self-attention for human activity decoding using wearable
    sensors,” *IEEE Sensors Journal*, vol. 21, no. 6, pp. 8575–8582, mar 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (220) X. Wang, L. Zhang, W. Huang, S. Wang, H. Wu, J. He, and A. Song, “Deep
    convolutional networks with tunable speed–accuracy tradeoff for human activity
    recognition using wearables,” *IEEE Transactions on Instrumentation and Measurement*,
    vol. 71, pp. 1–12, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (221) S. Xu, L. Zhang, W. Huang, H. Wu, and A. Song, “Deformable convolutional
    networks for multimodal human activity recognition using wearable sensors,” *IEEE
    Transactions on Instrumentation and Measurement*, vol. 71, pp. 1–14, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (222) J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable
    convolutional networks,” in *2017 IEEE Int. Conf. Computer Vision (ICCV)*, 2017,
    pp. 764–773.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(223) M. A. Wulder, J. C. White, S. N. Goward, J. G. Masek, J. R. Irons, M. Herold,
    W. B. Cohen, T. R. Loveland, and C. E. Woodcock, “Landsat continuity: Issues and
    opportunities for land cover monitoring,” *Remote Sensing of Environment*, vol.
    112, no. 3, pp. 955–969, mar 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (224) W. Emery and A. Camps, “Basic electromagnetic concepts and applications
    to optical sensors,” in *Introduction to Satellite Remote Sensing*, W. Emery and
    A. Camps, Eds.   Elsevier, jan 2017, ch. 2, pp. 43–83.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(225) N. Gorelick, M. Hancher, M. Dixon, S. Ilyushchenko, D. Thau, and R. Moore,
    “Google Earth Engine: Planetary-scale geospatial analysis for everyone,” *Remote
    Sensing of Environment*, vol. 202, pp. 18–27, dec 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(226) G. Giuliani, B. Chatenoux, A. De Bono, D. Rodila, J.-P. Richard, K. Allenbach,
    H. Dao, and P. Peduzzi, “Building an Earth observations data cube: lessons learned
    from the Swiss data cube (SDC) on generating analysis ready data (ARD),” *Big
    Earth Data*, vol. 1, no. 1-2, pp. 100–117, dec 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (227) A. Lewis, S. Oliver, L. Lymburner, B. Evans, L. Wyborn, N. Mueller, G. Raevksi,
    J. Hooke, R. Woodcock, J. Sixsmith *et al.*, “The australian geoscience data cube—foundations
    and lessons learned,” *Remote Sensing of Environment*, vol. 202, pp. 276–292,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (228) D. Ienco, Y. J. E. Gbodjo, R. Interdonato, and R. Gaetano, “Attentive
    weakly supervised land cover mapping for object-based satellite image time series
    data with spatial interpretation,” *arXiv*, pp. 1–12, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (229) V. Sainte Fare Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Satellite
    image time Series classification With Pixel-Set encoders and temporal self-attention,”
    in *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.   IEEE,
    jun 2020, pp. 12 322–12 331.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (230) A. Kulshrestha, L. Chang, and A. Stein, “Use of LSTM for sinkhole-related
    anomaly detection and classification of InSAR deformation time series,” *IEEE
    Journal of Selected Topics in Applied Earth Observations and Remote Sensing*,
    vol. 15, pp. 4559–4570, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (231) Y. Ban, P. Zhang, A. Nascetti, A. R. Bevington, and M. A. Wulder, “Near
    real-time wildfire progression monitoring with Sentinel-1 SAR time series and
    deep learning,” *Scientific Reports*, vol. 10, no. 1, p. 1322, dec 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (232) C. Rambour, N. Audebert, E. Koeniguer, B. Le Saux, M. Crucianu, and M. Datcu,
    “Flood detection in time series of optical and SAR images,” *Int. Archives Photogrammetry,
    Remote Sens. & Spatial Inf. Sci.*, vol. XLIII-B2-2, no. B2, pp. 1343–1346, aug
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (233) G. Kamdem De Teyou, Y. Tarabalka, I. Manighetti, R. Almar, and S. Tripodi,
    “Deep neural networks for automatic extraction of features in time series optical
    satellite images,” *Int. Archives Photogrammetry, Remote Sens. & Spatial Inf.
    Sci.*, vol. 43, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (234) B. M. Matosak, L. M. G. Fonseca, E. C. Taquary, R. V. Maretto, H. D. N.
    Bendini, and M. Adami, “Mapping deforestation in Cerrado based on hybrid deep
    learning architecture and medium spatial resolution satellite time series,” *Remote
    Sensing*, vol. 14, no. 1, pp. 1–22, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (235) D. Ho Tong Minh, D. Ienco, R. Gaetano, N. Lalande, E. Ndikumana, F. Osman,
    and P. Maurel, “Deep recurrent neural networks for winter vegetation quality mapping
    via multitemporal SAR Sentinel-1,” *IEEE Geoscience and Remote Sensing Letters*,
    vol. 15, no. 3, pp. 464–468, mar 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (236) P. Labenski, M. Ewald, S. Schmidtlein, and F. E. Fassnacht, “Classifying
    surface fuel types based on forest stand photographs and satellite time series
    using deep learning,” *International Journal of Applied Earth Observation and
    Geoinformation*, vol. 109, p. 102799, may 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (237) K. Rao, A. P. Williams, J. F. Flefil, and A. G. Konings, “SAR-enhanced
    mapping of live fuel moisture content,” *Remote Sens. Environ.*, vol. 245, p.
    111797, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(238) L. Zhu, G. I. Webb, M. Yebra, G. Scortechini, L. Miller, and F. Petitjean,
    “Live fuel moisture content estimation from MODIS: A deep learning approach,”
    *ISPRS J. Photogramm. Remote Sens.*, vol. 179, pp. 81–91, sep 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (239) L. Miller, L. Zhu, M. Yebra, C. Rüdiger, and G. I. Webb, “Multi-modal
    temporal CNNs for live fuel moisture content estimation,” *Environmental Modelling
    & Software*, vol. 156, p. 105467, oct 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (240) J. Xie, T. Qi, W. Hu, H. Huang, B. Chen, and J. Zhang, “Retrieval of live
    fuel moisture content based on multi-source remote sensing data and ensemble deep
    learning model,” *Remote Sensing*, vol. 14, no. 17, p. 4378, sep 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (241) K. Lahssini, F. Teste, K. R. Dayal, S. Durrieu, D. Ienco, and J.-M. Monnet,
    “Combining LiDAR metrics and Sentinel-2 imagery to estimate basal area and wood
    volume in complex forest environment via neural networks,” *IEEE J. Selected Topics
    Applied Earth Obs. Remote Sens.*, vol. 15, pp. 4337–4348, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (242) J. Sun, Z. Lai, L. Di, Z. Sun, J. Tao, and Y. Shen, “Multilevel deep learning
    network for county-level corn yield estimation in the U.S. Corn Belt,” *IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing*, vol. 13,
    pp. 5048–5060, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (243) Z. Li, G. Chen, and T. Zhang, “Temporal attention networks for multitemporal
    multisensor crop classification,” *IEEE Access*, vol. 7, pp. 134 677–134 690,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (244) Z. Li, G. Zhou, and Q. Song, “A temporal group attention approach for
    multitemporal multisensor crop classification,” *Infrared Physics and Technology*,
    vol. 105, p. 103152, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (245) S. Ji, C. Zhang, A. Xu, Y. Shi, and Y. Duan, “3D convolutional neural
    networks for crop classification with multi-temporal remote sensing images,” *Remote
    Sensing*, vol. 10, no. 2, p. 75, jan 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(246) J. Xu, Y. Zhu, R. Zhong, Z. Lin, J. Xu, H. Jiang, J. Huang, H. Li, and
    T. Lin, “DeepCropMapping: A multi-temporal deep learning approach with improved
    spatial generalizability for dynamic corn and soybean mapping,” *Remote Sensing
    of Environment*, vol. 247, p. 111946, sep 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (247) V. Barriere and M. Claverie, “Multimodal crop type classification fusing
    multi-spectral satellite time series with farmers crop rotations and local crop
    distribution,” *arXiv preprint:2208.10838*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (248) V. S. F. Garnot and L. Landrieu, “Lightweight temporal self-attention
    for classifying satellite images time series,” in *Lecture Notes in Computer Science*.   Springer
    International Publishing, 2020, vol. 12588 LNAI, pp. 171–181.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (249) S. Ofori-Ampofo, C. Pelletier, and S. Lang, “Crop type mapping from optical
    and radar time series using attention-based deep learning,” *Remote Sensing*,
    vol. 13, no. 22, p. 4668, nov 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (250) Y. Yuan and L. Lin, “Self-Supervised pretraining of transformers for satellite
    image time series classification,” *IEEE Journal of Selected Topics in Applied
    Earth Observations and Remote Sensing*, vol. 14, pp. 474–487, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (251) N. Di Mauro, A. Vergari, T. M. A. Basile, F. G. Ventola, and F. Esposito,
    “End-to-end learning of deep spatio-temporal representations for satellite image
    time series classification.” in *DC@ PKDD/ECML*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (252) N. Kussul, M. Lavreniuk, S. Skakun, and A. Shelestov, “Deep learning classification
    of land cover and crop types using remote sensing data,” *IEEE Geoscience and
    Remote Sensing Letters*, vol. 14, no. 5, pp. 778–782, may 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (253) C. Pelletier, G. Webb, and F. Petitjean, “Temporal convolutional neural
    network for the classification of satellite image time series,” *Remote Sensing*,
    vol. 11, no. 5, p. 523, mar 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (254) P. Dou, H. Shen, Z. Li, and X. Guan, “Time series remote sensing image
    classification framework using combination of deep learning and multiple classifiers
    system,” *International Journal of Applied Earth Observation and Geoinformation*,
    vol. 103, p. 102477, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (255) D. Ienco, R. Interdonato, R. Gaetano, and D. Ho Tong Minh, “Combining
    Sentinel-1 and Sentinel-2 satellite image time series for land cover mapping via
    a multi-source deep learning architecture,” *ISPRS J. Photogramm. Remote Sens.*,
    vol. 158, pp. 11–22, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(256) R. Interdonato, D. Ienco, R. Gaetano, and K. Ose, “DuPLO: A DUal view
    Point deep Learning architecture for time series classificatiOn,” *ISPRS J. Photogramm.
    Remote Sens.*, vol. 149, pp. 91–104, mar 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (257) M. Rußwurm and M. Körner, “Multi-Temporal land cover classification with
    sequential recurrent encoders,” *ISPRS International Journal of Geo-Information*,
    vol. 7, no. 4, p. 129, mar 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(258) A. Stoian, V. Poulain, J. Inglada, V. Poughon, and D. Derksen, “Land
    cover maps production with high resolution satellite image time series and convolutional
    neural networks: Adaptations and limits for operational systems,” *Remote Sensing*,
    vol. 11, no. 17, pp. 1–26, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (259) D. Ienco, R. Gaetano, C. Dupaquier, and P. Maurel, “Land cover classification
    via multitemporal spatial data by deep recurrent neural networks,” *IEEE Geoscience
    and Remote Sensing Letters*, vol. 14, no. 10, pp. 1685–1689, oct 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (260) Y. J. E. Gbodjo, D. Ienco, L. Leroux, R. Interdonato, R. Gaetano, and
    B. Ndao, “Object-based multi-temporal and multi-source land cover mapping leveraging
    hierarchical class relationships,” *Remote Sensing*, vol. 12, no. 17, p. 2814,
    aug 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (261) D. Ienco, R. Gaetano, R. Interdonato, K. Ose, and D. Ho Tong Minh, “Combining
    Sentinel-1 and Sentinel-2 time series via RNN for object-based land cover classification,”
    in *IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium*.   IEEE,
    jul 2019, pp. 4881–4884.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(262) Y. Yuan, L. Lin, Q. Liu, R. Hang, and Z.-G. Zhou, “SITS-Former: A pre-trained
    spatio-spectral-temporal representation model for Sentinel-2 time series classification,”
    *International Journal of Applied Earth Observation and Geoinformation*, vol.
    106, p. 102651, feb 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (263) M. Qiao, X. He, X. Cheng, P. Li, H. Luo, L. Zhang, and Z. Tian, “Crop
    yield prediction from multi-spectral, multi-temporal remotely sensed imagery using
    recurrent 3d convolutional neural networks,” *International Journal of Applied
    Earth Observation and Geoinformation*, vol. 102, p. 102436, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (264) M. Rußwurm and M. Körner, “Self-attention for raw optical satellite time
    series classification,” *ISPRS J. Photogramm. Remote Sens.*, vol. 169, pp. 421–435,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(265) D. Tuia, C. Persello, and L. Bruzzone, “Domain adaptation for the classification
    of remote sensing data: An overview of recent advances,” *IEEE Geoscience and
    Remote Sensing Magazine*, vol. 4, no. 2, pp. 41–57, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (266) V. S. F. Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Time-space
    tradeoff in deep learning models for crop classification on satellite multi-spectral
    image time series,” in *IGARSS 2019-2019 IEEE International Geoscience and Remote
    Sensing Symposium*.   IEEE, 2019, pp. 6247–6250.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (267) H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller,
    “Deep neural network ensembles for time series classification,” in *2019 International
    Joint Conference on Neural Networks (IJCNN)*, vol. 2019-July.   IEEE, jul 2019,
    pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (268) D. H. Wolpert, “Stacked generalization,” *Neural Networks*, vol. 5, no. 2,
    pp. 241–259, jan 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (269) Y. Freund and R. E. Schapire, “Experiments with a new boosting algorithm,”
    in *13th Int. conf. mach. learn.*, 1996, pp. 148–156.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(270) C. Gómez, J. C. White, and M. A. Wulder, “Optical remotely sensed time
    series data for land cover classification: A review,” *ISPRS J. Photogramm. Remote
    Sens.*, vol. 116, pp. 55–72, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(271) X. X. Zhu, D. Tuia, L. Mou, G. S. Xia, L. Zhang, F. Xu, and F. Fraundorfer,
    “Deep learning in remote sensing: A comprehensive review and list of resources,”
    *IEEE Geoscience and Remote Sensing Magazine*, vol. 5, no. 4, pp. 8–36, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(272) L. Ma, Y. Liu, X. Zhang, Y. Ye, G. Yin, and B. A. Johnson, “Deep learning
    in remote sensing applications: A meta-analysis and review,” *ISPRS J. Photogramm.
    Remote Sens.*, vol. 152, pp. 166–177, jun 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(273) Q. Yuan, H. Shen, T. Li, Z. Li, S. Li, Y. Jiang, H. Xu, W. Tan, Q. Yang,
    J. Wang, J. Gao, and L. Zhang, “Deep learning in environmental remote sensing:
    Achievements and challenges,” *Remote Sensing of Environment*, vol. 241, p. 111716,
    may 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(274) M. E. D. Chaves, M. C. A. Picoli, and I. D. Sanches, “Recent applications
    of Landsat 8/OLI and Sentinel-2/MSI for land use and land cover mapping: A systematic
    review,” *Remote Sensing*, vol. 12, no. 18, p. 3062, sep 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(275) W. R. Moskolaï, W. Abdou, A. Dipanda, and Kolyang, “Application of deep
    learning architectures for satellite image time series prediction: A review,”
    *Remote Sensing*, vol. 13, no. 23, p. 4822, nov 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (276) J. Lines and A. Bagnall, “Time series classification with ensembles of
    elastic distance measures,” *Data Min. Knowl. Discov.*, vol. 29, no. 3, pp. 565–592,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(277) C. W. Tan, F. Petitjean, and G. I. Webb, “FastEE: Fast Ensembles of Elastic
    Distances for time series classification,” *Data Min. Knowl. Discov.*, vol. 34,
    no. 1, pp. 231–272, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(278) M. Herrmann and G. I. Webb, “Amercing: An intuitive, elegant and effective
    constraint for dynamic time warping,” *arXiv preprint:2111.13314*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (279) A. Bagnall, M. Flynn, J. Large, J. Lines, and M. Middlehurst, “On the
    usage and performance of the Hierarchical Vote Collective of Transformation-based
    Ensembles version 1.0 (hive-cote v1\. 0),” in *International Workshop on Advanced
    Analytics and Learning on Temporal Data*, 2020, pp. 3–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(280) M. Middlehurst, J. Large, M. Flynn, J. Lines, A. Bostrom, and A. Bagnall,
    “HIVE-COTE 2.0: a new meta ensemble for time series classification,” *Machine
    Learning*, vol. 110, no. 11, pp. 3211–3243, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(281) A. Bagnall, J. Lines, J. Hills, and A. Bostrom, “Time-series classification
    with COTE: the collective of transformation-based ensembles,” *IEEE Transactions
    on Knowledge and Data Engineering*, vol. 27, no. 9, pp. 2522–2535, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(282) J. Lines, S. Taylor, and A. Bagnall, “Time series classification with
    HIVE-COTE: The hierarchical vote collective of transformation-based ensembles,”
    *ACM Transactions on Knowledge Discovery from Data*, vol. 12, no. 5, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(283) ——, “Hive-Cote: The hierarchical vote collective of transformation-based
    ensembles for time series classification,” in *2016 IEEE 16th international conference
    on data mining (ICDM)*.   IEEE, 2016, pp. 1041–1046.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (284) R. J. Kate, “Using dynamic time warping distances as features for improved
    time series classification,” *Data Min. Knowl. Discov.*, vol. 30, no. 2, pp. 283–312,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (285) A. Bostrom and A. Bagnall, “Binary shapelet transform for multiclass time
    series classification,” in *Int. conf. big data analytics .knowl. disco.*   Springer,
    2015, pp. 257–269.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (286) P. Schäfer, “The boss is concerned with time series classification in
    the presence of noise,” *Data Min. Knowl. Discov.*, vol. 29, no. 6, pp. 1505–1530,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (287) J. Hills, J. Lines, E. Baranauskas, J. Mapp, and A. Bagnall, “Classification
    of time series by shapelet transformation,” *Data Min. Knowl. Discov.*, vol. 28,
    no. 4, pp. 851–881, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (288) H. Deng, G. Runger, E. Tuv, and M. Vladimir, “A time series forest for
    classification and feature extraction,” *Inf. Sci.*, vol. 239, pp. 142–153, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (289) M. G. Baydogan, G. Runger, and E. Tuv, “A bag-of-features framework to
    classify time series,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 35, no. 11, pp. 2796–2802, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(290) A. Dempster, D. F. Schmidt, and G. I. Webb, “Minirocket: A very fast
    (almost) deterministic transform for time series classification,” in *27th ACM
    SIGKDD Conference on Knowledge Discovery & Data Mining*, 2021, pp. 248–257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(291) C. W. Tan, A. Dempster, C. Bergmeir, and G. I. Webb, “MultiRocket: multiple
    pooling operators and transformations for fast and effective time series classification,”
    *Data Min. Knowl. Discov.*, jun 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(292) A. Dempster, D. F. Schmidt, and G. I. Webb, “Hydra: Competing convolutional
    kernels for fast and accurate time series classification,” *Data Mining and Knowledge
    Discovery*, pp. 1–27, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(293) B. Lucas, A. Shifaz, C. Pelletier, L. O’Neill, N. Zaidi, B. Goethals,
    F. Petitjean, and G. I. Webb, “Proximity forest: an effective and scalable distance-based
    classifier for time series,” *Data Mining and Knowledge Discovery*, vol. 33, no. 3,
    pp. 607–635, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(294) M. Herrmann, C. W. Tan, M. Salehi, and G. I. Webb, “Proximity forest
    2.0: A new effective and scalable similarity-based classifier for time series,”
    *arXiv preprint arXiv:2304.05800*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(295) K. Fukushima and S. Miyake, “Neocognitron: A self-organizing neural network
    model for a mechanism of visual pattern recognition,” in *Competition and cooperation
    in neural nets*.   Springer, 1982, pp. 267–285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (296) D. H. Hubel and T. N. Wiesel, “Receptive fields, binocular interaction
    and functional architecture in the cat’s visual cortex,” *The Journal of physiology*,
    vol. 160, no. 1, p. 106, 1962.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (297) Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proc. IEEE*, vol. 86, no. 11, pp. 2278–2324,
    1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (298) V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann
    machines,” in *Icml*, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (299) S. Hihi and Y. Bengio, “Hierarchical recurrent neural networks for long-term
    dependencies,” *Advances neural inf. process. syst.*, vol. 8, 1995.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (300) R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to construct deep
    recurrent neural networks,” *arXiv preprint:1312.6026*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (301) K. Kawakami, “Supervised sequence labelling with recurrent neural networks,”
    Ph.D. dissertation, Technical University of Munich, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (302) D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” *arXiv preprint:1409.0473*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (303) K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” *arXiv preprint:1406.1078*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (304) M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based
    neural machine translation,” *arXiv preprint:1508.04025*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(305) J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral Networks and
    Locally Connected Networks on Graphs,” pp. 1–14, 2013\. [Online]. Available: [http://arxiv.org/abs/1312.6203](http://arxiv.org/abs/1312.6203)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(306) D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,
    “The emerging field of signal processing on graphs: Extending high-dimensional
    data analysis to networks and other irregular domains,” *IEEE Signal Processing
    Magazine*, vol. 30, no. 3, pp. 83–98, 2013\. [Online]. Available: [http://ieeexplore.ieee.org/document/6494675/](http://ieeexplore.ieee.org/document/6494675/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(307) A. Longa, V. Lachi, G. Santin, M. Bianchini, B. Lepri, P. Lio, F. Scarselli,
    and A. Passerini, “Graph Neural Networks for temporal graphs: State of the art,
    open challenges, and opportunities,” 2023\. [Online]. Available: [http://arxiv.org/abs/2302.01018](http://arxiv.org/abs/2302.01018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (308) M. Bachlin, D. Roggen, G. Troster, M. Plotnik, N. Inbar, I. Meidan, T. Herman,
    M. Brozgol, E. Shaviv, N. Giladi, and J. M. Hausdorff, “Potentials of enhanced
    context awareness in wearable assistants for Parkinson’s Disease patients with
    the freezing of gait syndrome,” in *2009 International Symposium on Wearable Computers*.   IEEE,
    sep 2009, pp. 123–130.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(309) D. Micucci, M. Mobilio, and P. Napoletano, “UniMiB SHAR: A dataset for
    human activity recognition using acceleration data from smartphones,” *Applied
    Sciences*, vol. 7, no. 10, p. 1101, oct 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(310) P. Zappi, C. Lombriser, T. Stiefmeier, E. Farella, D. Roggen, L. Benini,
    and G. Tröster, “Activity recognition from on-body sensors: accuracy-power trade-off
    by dynamic sensor selection,” in *European Conference on Wireless Sensor Networks*.   Springer,
    2008, pp. 17–33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(311) R. Chavarriaga, H. Sagha, A. Calatroni, S. T. Digumarti, G. Tröster,
    J. D. R. Millán, and D. Roggen, “The Opportunity challenge: A benchmark database
    for on-body sensor-based activity recognition,” *Pattern Recognition Letters*,
    vol. 34, no. 15, pp. 2033–2042, nov 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(312) A. Reiss and D. Stricker, “Creating and benchmarking a new dataset for
    physical activity monitoring,” in *5th Int. Conf. PErvasive Technologies Related
    to Assistive Environments - PETRA ’12*.   New York, New York, USA: ACM Press,
    2012, p. 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (313) D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A public
    domain dataset for human activity recognition using smartphones,” in *21th European
    Symposium on Artificial Neural Networks, Computational Intelligence and Machine
    Learning, ESANN*, Bruges, Belgium, 2013, pp. 437–442.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (314) J. R. Kwapisz, G. M. Weiss, and S. A. Moore, “Activity recognition using
    cell phone accelerometers,” *ACM SIGKDD Explorations Newsletter*, vol. 12, no. 2,
    pp. 74–82, mar 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(315) U.S. Geological Survey, “Landsat Satellite Missions.” [Online]. Available:
    [https://www.usgs.gov/landsat-missions/landsat-satellite-missions](https://www.usgs.gov/landsat-missions/landsat-satellite-missions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(316) NASA, “MODIS Moderate Resolution Imaging Spectrometer.” [Online]. Available:
    [https://modis.gsfc.nasa.gov/](https://modis.gsfc.nasa.gov/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(317) European Space Agency, “Sentinel Online,” 2019\. [Online]. Available:
    [https://sentinel.esa.int/web/sentinel/home](https://sentinel.esa.int/web/sentinel/home)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(318) ——, “Pleiades - Earth Online.” [Online]. Available: [https://earth.esa.int/eogateway/missions/pleiades](https://earth.esa.int/eogateway/missions/pleiades)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(319) National Space Organization, “FORMOSAT-2,” 2020\. [Online]. Available:
    [https://www.nspo.narl.org.tw/history{_}prog.php?c=20030402{&}ln=en](https://www.nspo.narl.org.tw/history%7B_%7Dprog.php?c=20030402%7B&%7Dln=en)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(320) EoPortal, “Gaofen-1,” 2014\. [Online]. Available: [https://www.eoportal.org/satellite-missions/gaofen-1](https://www.eoportal.org/satellite-missions/gaofen-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(321) ——, “Gaofen-2,” 2015\. [Online]. Available: [https://www.eoportal.org/satellite-missions/gaofen-2](https://www.eoportal.org/satellite-missions/gaofen-2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Non-Deep Learning Time Series Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we aim to give a brief introduction to the field of TSC and
    discuss its current status. We refer interested readers to the ‘bake-off’ papers
    ([bagnall2017great,](#bib.bib25) ; [middlehurst2023bake,](#bib.bib11) ; [ruiz2020great,](#bib.bib26)
    ) that describes TSC methods in much details and benchmark them.
  prefs: []
  type: TYPE_NORMAL
- en: Research in TSC started with distance-based approaches that find discriminating
    patterns in the shape of the time series. Distance-based approaches usually consist
    of coupling a 1-nearest neighbour (1NN) classifier with a time series distance
    measure ([lines2015time,](#bib.bib276) ; [tan2020fastee,](#bib.bib277) ). Small
    distortions in the time series can lead to false matches when measuring the distance
    between time series using standard distance measurements such as Euclidean distance ([lines2015time,](#bib.bib276)
    ). A time series distance measure aims to compensate for these distortions by
    aligning two time series such that the alignment cost between the two are minimised.
    There are many time series distances proposed in the literature; among these,
    the Dynamic Time Warping ($DTW$) distance is one of the most popular choices for
    many time series tasks, due to its intuitiveness and effectiveness in aligning
    two time series. The 1NN-$DTW$ has been the go-to method for TSC for decades.
    However, by comparing several time series distance measures, the work in ([lines2015time,](#bib.bib276)
    ) showed that as of 2015, there was no single distance that significantly outperformed
    $DTW$ when used with a 1NN classifier. The recent Amerced $DTW$ ([herrmann2021amercing,](#bib.bib278)
    ) distance is the first distance that is significantly more accurate than $DTW$.
    These individual 1NN classifiers with different distances can be ensembled together
    to create an ensemble, such as the Ensemble of Elastic distances (EE), that significantly
    outperforms each of them individually ([tan2020fastee,](#bib.bib277) ; [lines2015time,](#bib.bib276)
    ). However, since most distances have a complexity of $O(L^{2})$ where $L$ is
    the length of the series, performing a nearest neighbour search becomes very costly.
    Hence, distance-based approaches are considered to be one of the slowest methods
    for TSC ([bagnall2020usage,](#bib.bib279) ; [middlehurst2021hive,](#bib.bib280)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: As a result of EE, recent studies have focused mainly on developing ensembling
    methods that significantly outperform 1NN-$DTW$ ([tan2020fastee,](#bib.bib277)
    ; [bagnall2020usage,](#bib.bib279) ; [middlehurst2021hive,](#bib.bib280) ; [bagnall2015time,](#bib.bib281)
    ; [lines2018time,](#bib.bib282) ; [lines2016hive,](#bib.bib283) ; [kate2016using,](#bib.bib284)
    ; [bostrom2015binary,](#bib.bib285) ; [schafer2015boss,](#bib.bib286) ; [hills2014classification,](#bib.bib287)
    ; [deng2013time,](#bib.bib288) ; [baydogan2013bag,](#bib.bib289) ). These approaches
    use either an ensemble of tree-based approaches ([baydogan2013bag,](#bib.bib289)
    ; [deng2013time,](#bib.bib288) ) or an ensemble of different types of discriminant
    classifiers, such as NN with several distances and Support Vector Machine (SVM)
    on one or several feature spaces ([bagnall2015time,](#bib.bib281) ; [bostrom2015binary,](#bib.bib285)
    ; [schafer2015boss,](#bib.bib286) ; [kate2016using,](#bib.bib284) ). All these
    approaches share a common property – the data transformation phase where the time
    series is transformed into a new feature space such as the shapelets transform ([bostrom2015binary,](#bib.bib285)
    ) or DTW features ([kate2016using,](#bib.bib284) ). Taking advantage of this notion
    led to the development of the Hierarchical Vote Collective of Transformation-based
    Ensembles (HIVE-COTE) ([lines2016hive,](#bib.bib283) ; [middlehurst2021hive,](#bib.bib280)
    ). HIVE-COTE is a meta ensemble for TSC and forms its ensemble from ensemble classifiers
    of multiple domains. Since its introduction in 2016 ([lines2016hive,](#bib.bib283)
    ), HIVE-COTE has gone through a few iterations. Recently, the latest HIVE-COTE
    version, HIVE-COTEv2.0 (HC2) was proposed ([middlehurst2021hive,](#bib.bib280)
    ). It is comprised of 4 ensemble members, each of them being the then state of
    the art in their respective domains. It is currently one of the most accurate
    classifiers for both univariate and multivariate TSC tasks ([middlehurst2021hive,](#bib.bib280)
    ). Despite being accurate on 26 multivariate and 142 univariate TSC benchmark
    datasets, that are relatively small, HC2 scales poorly on large datasets with
    long time series as well as datasets with large numbers of channels.
  prefs: []
  type: TYPE_NORMAL
- en: Various work has been done on speeding up TSC methods without sacrificing accuracy
    ([dempster2019rocket,](#bib.bib14) ; [dempster2021minirocket,](#bib.bib290) ;
    [tan2020fastee,](#bib.bib277) ; [tan2021multirocket,](#bib.bib291) ; [dempster2023hydra,](#bib.bib292)
    ; [lucas2019proximity,](#bib.bib293) ; [herrmann2023proximity,](#bib.bib294) ).
    A recent breakthrough is the development of Rocket ([dempster2019rocket,](#bib.bib14)
    ) that was able to process 109 univariate time series datasets under 4 hours while
    the previous fastest took days. Rocket leverages large number of random convolutional
    filters to extract features from each series that might be relevant to classifying
    a series. These features are then passed to a linear model for classification.
    Rocket has been improved to be faster (Minirocket ([dempster2021minirocket,](#bib.bib290)
    )) and more accurate (Multirocket ([tan2021multirocket,](#bib.bib291) ) and Hydra
    ([dempster2023hydra,](#bib.bib292) )). Hydra when combined with Multirocket is
    now one of the fastest and most accurate method for TSC.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B DNN Architectures for Time Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide a descriptive overview of deep learning-based models
    for TSC. The focus is on clarifying their architectures and outlining their adaptations
    to the specific characteristics of time series data.
  prefs: []
  type: TYPE_NORMAL
- en: B.1\. Multi-Layer Perceptron (MLP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The simplest Neural Network architecture is a fully connected network (FC),
    also known as a multilayer perceptron (MLP). As shown in Fig. [2](#A2.F2 "Figure
    2 ‣ B.1\. Multi-Layer Perceptron (MLP) ‣ Appendix B DNN Architectures for Time
    Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey") all neurons of one layer $l-1$ are connected to all neurons
    of the following layer $l$ with $l\in[1,L]$. The weights model these connections
    in a neural network. A general equation for applying a non-linearity to an input
    $A^{l-1}$ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $A^{l}=f(W^{l}\times A^{l-1}+b)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $A^{l}$ the activation of the neurons in layer $l$ where $A^{1}$ is equal
    to input series $X$. Also, $W$ and $b$ are the neuron weights and biases, and
    $f$ is the nonlinear activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eae8bd7623dd31ec7ac38fd4c167e44e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Multilayer perceptron for univariate time series classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main limitations of using multilayer perceptrons (MLPs) for time
    series data is that they are not well-suited to capturing the temporal dependencies
    in this type of data. MLPs are feedforward networks that process input data in
    a fixed and predetermined order without considering the temporal relationships
    between the input values. As shown in Fig. [2](#A2.F2 "Figure 2 ‣ B.1\. Multi-Layer
    Perceptron (MLP) ‣ Appendix B DNN Architectures for Time Series ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey"), each
    time step is weighted individually, and time series elements are treated independently
    from each other.'
  prefs: []
  type: TYPE_NORMAL
- en: B.2\. Convolution Neural Networks (CNNs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The convolutional neural network (CNN) was first proposed by Kunihiko Fukushima
    in 1982 ([fukushima1982neocognitron,](#bib.bib295) ). It was inspired by the structure
    and function of the visual cortex in animals, specifically the cat’s cortex, as
    described by David Hubel and Torsten Wiesel in their influential work from 1962 ([hubel1962receptive,](#bib.bib296)
    ). Convolutional neural networks have been widely used for visual pattern recognition,
    but their ability to process large images was constrained by computational limitations
    until the emergence of GPU technology. Following the development of Graphics Processing
    Unit (GPU) technology, Krizhevsky et al. ([krizhevsky2012imagenet,](#bib.bib39)
    ) implemented an efficient GPU-based program and won the ImageNet competition
    in 2012, bringing the convolution neural network back into the spotlight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many variants of CNN architectures have been proposed in the literature, but
    their primary components are very similar. Using the LeNet-5 ([lecun1998gradient,](#bib.bib297)
    ) as an example, it consists of three types of layers: convolutional, pooling,
    and fully connected. The purpose of the convolutional layer is to learn feature
    representations of the inputs. Fig. [3](#A2.F3 "Figure 3 ‣ B.2\. Convolution Neural
    Networks (CNNs) ‣ Appendix B DNN Architectures for Time Series ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey") shows
    the architecture of the t-LeNet network, which is a time series-specific version
    of LeNet. This figure shows that the convolution layer is composed of several
    convolution kernels (or filters) used to compute different feature maps. In particular,
    each neuron of a feature map is connected to a region of neighboring neurons in
    the previous layer called the receptive field. Feature maps can be created by
    first convolving inputs with learned kernels and then applying an element-wise
    nonlinear activation function to the convolved results. It is important to note
    that all spatial locations of the input share the kernel for each feature map,
    and several kernels are used to obtain the entire feature map.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature value of the $l$^(th) layer of $k$^(th) feature map at location
    $(i,j)$ is obtained by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $Z^{l}_{i,j,k}={\textbf{W}^{l}_{k}}^{T}\textbf{A}^{l-1}_{i,j}+b^{l}_{k}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Where $\textbf{W}^{l}_{k}$ and $b^{l}_{k}$ are the weight vector and bias term
    of the $k$^(th) filter of the $l$^(th) layer, respectively, and $\textbf{A}^{l-1}_{i,j}$
    is the input patch centered at location $(i,j)$ of the $l$ layer. Note that the
    kernel $\textbf{W}^{l}_{k}$ that generates the feature map $Z^{l}_{:,:,k}$ is
    shared. A weight-sharing mechanism has several advantages, such as reducing model
    complexity and making the network easier to train. Let $f\left(.\right)$ denote
    the nonlinear activation function. The activation value of convolutional feature
    $Z^{l}_{i,j,k}$ can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\textbf{A}^{l}_{i,j,k}=f(Z^{l}_{i,j,k})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The most common activation functions are sigmoid, tanh and ReLU ([nair2010rectified,](#bib.bib298)
    ). As shown in Fig. [3](#A2.F3 "Figure 3 ‣ B.2\. Convolution Neural Networks (CNNs)
    ‣ Appendix B DNN Architectures for Time Series ‣ Deep Learning for Time Series
    Classification and Extrinsic Regression: A Current Survey"), a pooling layer is
    often placed between two convolution layers to reduce the resolution of the feature
    maps and to achieve shift-invariance. Following several convolution stages $-$the
    block comprising convolution, activation, and pooling is called convolution $stage$
    $-$ there may be one or more fully-connected layers that aim to perform high-level
    reasoning. As discussed in section [3.1](#S3.SS1 "3.1\. Multi-Layer Perceptron
    (MLP) ‣ 3\. Supervised Models ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey"), each neuron in the previous layer is
    connected to every neuron in the current layer to generate global semantic information.
    In the final layer of CNNs, there is the output layer in which the Softmax operators
    are commonly used for classification tasks ([gu2018recent,](#bib.bib40) ).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9228b68351f24d08b57a5a0c70acb0b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. The architecture of the t-LeNet network (time series specific version
    of LeNet)
  prefs: []
  type: TYPE_NORMAL
- en: B.3\. Recurrent Neural Networks (RNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RNNs are types of neural networks that are specifically designed to process
    time series and other sequential data. RNNs are conceptually similar to feed-forward
    neural networks (FFNs). While FFNs map from fixed-size inputs to fixed-size outputs,
    RNNs can process variable-length inputs and produce variable-length outputs. This
    capability is enabled by sharing parameters over time through directed connections
    between individual layers. RNN models for TSC can be classified as sequence to
    sequence or sequence-to-one based on their outputs. Fig. [4](#A2.F4 "Figure 4
    ‣ B.3\. Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures for Time
    Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey") shows sequence to sequence architectures for RNN models, with
    an output for each input sub-series. On the other hand, in sequence-to-one architecture,
    decisions are made using only $y^{T}$ and ignoring the other outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c112da5457c17e89bdc16ee0170c07f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. The architecture of two layer Recurrent Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: 'At each time step $t$, RNNs maintain a hidden vector $h$ which updates as follows ([hihi1995hierarchical,](#bib.bib299)
    ; [pascanu2013construct,](#bib.bib300) ):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $h_{t}=tanh(Wh_{t-1}+Ix^{t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Where $X=\left\{x^{1},...,x^{t-1},x^{t},...,x^{T}\right\}$ contains all of
    the observation, $tanh$ denotes the hyperbolic tangent function, and the recurrent
    weight and the projection matrix are shown by $W$ and $I$, respectively. The hidden-to-hidden
    connections also model the short-term time dependency. The hidden state $h$ is
    used to make a prediction as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $y^{t}=\sigma_{s}(Wh_{t-1})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\sigma_{s}$ is a softmax function and provides a normalized probability
    distribution over the possible classes. As depicted in Fig. [4](#A2.F4 "Figure
    4 ‣ B.3\. Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures for Time
    Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey"), the hidden state $h$ can be used to stack RNNs in order to
    build deeper networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $h_{t}^{l}=\sigma(Wh^{l}_{t-1}+Ih^{l-1}_{t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma$ is the logistic sigmoid function. As an alternative to feeding
    each time step to the RNN, the data can be divided into time windows of $\omega$
    observations, with the option for variable overlaps. Each time window is labeled
    with the majority response labels within the $\omega$ window.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.1\. Long Short Term Memory (LSTM)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LSTM deals with the vanishing/exploding gradient problem commonly found in
    standard recurrent neural networks through the incorporation of gate-controlled
    memory cells into their state dynamics ([hochreiter1997long,](#bib.bib78) ). As
    shown in Fig. [5](#A2.F5 "Figure 5 ‣ B.3.2\. Gated Recurrent Unit (GRU) ‣ B.3\.
    Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures for Time Series
    ‣ Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey") (a) LSTM uses a hidden vector $h$ and a memory vector $m$ to control
    state updates and outputs for each time step. Specifically, the computation at
    time step $t$ is formulated as follows ([kawakami2008supervised,](#bib.bib301)
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | <math   alttext="\begin{split}&amp;\Gamma^{c}=tanh(W^{c}h_{t-1}+I^{c}x^{t})\\
    &amp;\Gamma^{u}=\sigma(W^{u}h_{t-1}+I^{u}x^{t})\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\Gamma^{f}=\sigma(W^{f}h_{t-1}+I^{f}x^{t})\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\Gamma^{o}=\sigma(W^{o}h_{t-1}+I^{o}x^{t})\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;m_{t}=\Gamma^{f}\otimes m_{t-1}+\Gamma^{u}\otimes\Gamma^{c}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;h_{t}=tanh(\Gamma^{o}\otimes m_{t})\end{split}" display="block"><semantics
    ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
     ><mtr 
    ><mtd  columnalign="left" 
    ><mrow  ><msup
     ><mi mathvariant="normal"
     >Γ</mi><mi 
    >c</mi></msup><mo 
    >=</mo><mrow 
    ><mi  >t</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >n</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >h</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><mrow  ><msup
     ><mi 
    >W</mi><mi  >c</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi 
    >h</mi><mrow 
    ><mi 
    >t</mi><mo 
    >−</mo><mn 
    >1</mn></mrow></msub></mrow><mo 
    >+</mo><mrow 
    ><msup 
    ><mi  >I</mi><mi
     >c</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msup
     ><mi 
    >x</mi><mi 
    >t</mi></msup></mrow></mrow><mo stretchy="false"
     >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow
     ><msup 
    ><mi mathvariant="normal" 
    >Γ</mi><mi  >u</mi></msup><mo
     >=</mo><mrow 
    ><mi  >σ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><mrow  ><msup
     ><mi 
    >W</mi><mi  >u</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi 
    >h</mi><mrow 
    ><mi  >t</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub></mrow><mo
     >+</mo><mrow
     ><msup 
    ><mi  >I</mi><mi
     >u</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msup
     ><mi 
    >x</mi><mi 
    >t</mi></msup></mrow></mrow><mo stretchy="false"
     >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow
     ><msup 
    ><mi mathvariant="normal" 
    >Γ</mi><mi  >f</mi></msup><mo
     >=</mo><mrow 
    ><mi  >σ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><mrow  ><msup
     ><mi 
    >W</mi><mi  >f</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi 
    >h</mi><mrow 
    ><mi  >t</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub></mrow><mo
     >+</mo><mrow
     ><msup 
    ><mi  >I</mi><mi
     >f</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msup
     ><mi 
    >x</mi><mi 
    >t</mi></msup></mrow></mrow><mo stretchy="false"
     >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow
     ><msup 
    ><mi mathvariant="normal" 
    >Γ</mi><mi  >o</mi></msup><mo
     >=</mo><mrow 
    ><mi  >σ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><mrow 
    ><msup 
    ><mi  >W</mi><mi
     >o</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >h</mi><mrow 
    ><mi  >t</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub></mrow><mo
     >+</mo><mrow
     ><msup
     ><mi
     >I</mi><mi
     >o</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msup
     ><mi
     >x</mi><mi
     >t</mi></msup></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow
     ><msub 
    ><mi  >m</mi><mi
     >t</mi></msub><mo
     >=</mo><mrow 
    ><mrow  ><msup
     ><mi mathvariant="normal"
     >Γ</mi><mi 
    >f</mi></msup><mo lspace="0.222em" rspace="0.222em"
     >⊗</mo><msub 
    ><mi  >m</mi><mrow
     ><mi 
    >t</mi><mo 
    >−</mo><mn 
    >1</mn></mrow></msub></mrow><mo 
    >+</mo><mrow 
    ><msup  ><mi
    mathvariant="normal"  >Γ</mi><mi
     >u</mi></msup><mo
    lspace="0.222em" rspace="0.222em"  >⊗</mo><msup
     ><mi mathvariant="normal"
     >Γ</mi><mi
     >c</mi></msup></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow
     ><msub 
    ><mi  >h</mi><mi
     >t</mi></msub><mo
     >=</mo><mrow 
    ><mi  >t</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >n</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >h</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><msup 
    ><mi mathvariant="normal" 
    >Γ</mi><mi 
    >o</mi></msup><mo lspace="0.222em" rspace="0.222em"
     >⊗</mo><msub
     ><mi
     >m</mi><mi
     >t</mi></msub></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >Γ</ci><ci 
    >𝑐</ci></apply><apply 
    ><ci  >𝑡</ci><ci
     >𝑎</ci><ci 
    >𝑛</ci><ci  >ℎ</ci><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝑊</ci><ci 
    >𝑐</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℎ</ci><apply 
    ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply></apply><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝐼</ci><ci
     >𝑐</ci></apply><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >𝑥</ci><ci
     >𝑡</ci></apply></apply></apply><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >Γ</ci><ci 
    >𝑢</ci></apply></apply></apply><apply 
    ><apply  ><ci
     >𝜎</ci><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝑊</ci><ci 
    >𝑢</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℎ</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐼</ci><ci 
    >𝑢</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑥</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >Γ</ci><ci  >𝑓</ci></apply></apply></apply><apply
     ><apply 
    ><ci  >𝜎</ci><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝑊</ci><ci 
    >𝑓</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℎ</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐼</ci><ci 
    >𝑓</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑥</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >Γ</ci><ci  >𝑜</ci></apply></apply></apply><apply
     ><apply 
    ><ci  >𝜎</ci><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝑊</ci><ci 
    >𝑜</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℎ</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐼</ci><ci 
    >𝑜</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑥</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑚</ci><ci  >𝑡</ci></apply></apply></apply><apply
     ><apply 
    ><apply  ><csymbol
    cd="latexml"  >tensor-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >Γ</ci><ci 
    >𝑓</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑚</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><apply 
    ><csymbol cd="latexml" 
    >tensor-product</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >Γ</ci><ci 
    >𝑢</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >Γ</ci><ci 
    >𝑐</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℎ</ci><ci  >𝑡</ci></apply></apply></apply></apply><apply
     ><apply 
    ><ci  >𝑡</ci><ci
     >𝑎</ci><ci 
    >𝑛</ci><ci  >ℎ</ci><apply
     ><csymbol cd="latexml"
     >tensor-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >Γ</ci><ci 
    >𝑜</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑚</ci><ci 
    >𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&\Gamma^{c}=tanh(W^{c}h_{t-1}+I^{c}x^{t})\\
    &\Gamma^{u}=\sigma(W^{u}h_{t-1}+I^{u}x^{t})\\ &\Gamma^{f}=\sigma(W^{f}h_{t-1}+I^{f}x^{t})\\
    &\Gamma^{o}=\sigma(W^{o}h_{t-1}+I^{o}x^{t})\\ &m_{t}=\Gamma^{f}\otimes m_{t-1}+\Gamma^{u}\otimes\Gamma^{c}\\
    &h_{t}=tanh(\Gamma^{o}\otimes m_{t})\end{split}</annotation></semantics></math>
    |  |'
  prefs: []
  type: TYPE_NORMAL
- en: where $\Gamma^{c}$ is a cell state gate and $\Gamma^{u},\Gamma^{f}$ and $\Gamma^{o}$
    are the activation vector of the input, forget and output gate respectively. $\sigma$
    is the logistic sigmoid function and $\otimes$ shows the element-wise product.
    $W^{u},W^{f},W^{o},W^{c}$ represent the recurrent weight matrices, and $I^{u},I^{f},I^{o},I^{c}$
    represent the projection matrices.
  prefs: []
  type: TYPE_NORMAL
- en: B.3.2\. Gated Recurrent Unit (GRU)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'GRU is another widely-used variant of RNNs, and similar to LSTM it can control
    the flow of information like memorizing the context over multiple time steps ([chung2014empirical,](#bib.bib79)
    ). While GRU was introduced later than LSTM, it has a simpler architecture. As
    shown in Fig [5](#A2.F5 "Figure 5 ‣ B.3.2\. Gated Recurrent Unit (GRU) ‣ B.3\.
    Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures for Time Series
    ‣ Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey") (b), compared to LSTMs, GRUs have two gates, reset and update gates,
    which are more computationally efficient and require fewer data to generalize
    and defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | <math   alttext="\begin{split}&amp;\Gamma^{z}=\sigma(W^{z}h_{t-1}+I^{z}x^{t})\\
    &amp;\Gamma^{r}=\sigma(W^{r}h_{t-1}+I^{r}x^{t})\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\widetilde{h}_{t}=tanh(W[h_{t-1}\otimes\Gamma^{r},x^{t}])\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;h_{t}=(1-\Gamma^{z})\otimes h_{t-1}+\Gamma^{z}\otimes\widetilde{h}_{t}\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr 
    ><mtd  columnalign="left" 
    ><mrow  ><msup
     ><mi mathvariant="normal"
     >Γ</mi><mi 
    >z</mi></msup><mo 
    >=</mo><mrow 
    ><mi  >σ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><mrow  ><msup
     ><mi 
    >W</mi><mi  >z</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi 
    >h</mi><mrow  ><mi
     >t</mi><mo 
    >−</mo><mn  >1</mn></mrow></msub></mrow><mo
     >+</mo><mrow
     ><msup 
    ><mi  >I</mi><mi
     >z</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msup
     ><mi 
    >x</mi><mi 
    >t</mi></msup></mrow></mrow><mo stretchy="false"
     >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow 
    ><msup  ><mi
    mathvariant="normal"  >Γ</mi><mi
     >r</mi></msup><mo
     >=</mo><mrow 
    ><mi  >σ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><mrow  ><msup
     ><mi 
    >W</mi><mi  >r</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi 
    >h</mi><mrow 
    ><mi  >t</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub></mrow><mo
     >+</mo><mrow
     ><msup 
    ><mi  >I</mi><mi
     >r</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msup
     ><mi 
    >x</mi><mi 
    >t</mi></msup></mrow></mrow><mo stretchy="false"
     >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow 
    ><msub  ><mover
    accent="true"  ><mi
     >h</mi><mo
     >~</mo></mover><mi
     >t</mi></msub><mo
     >=</mo><mrow 
    ><mi  >t</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >n</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >h</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><mi  >W</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >[</mo><mrow 
    ><msub 
    ><mi  >h</mi><mrow
     ><mi
     >t</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub><mo
    lspace="0.222em" rspace="0.222em"  >⊗</mo><msup
     ><mi
    mathvariant="normal"  >Γ</mi><mi
     >r</mi></msup></mrow><mo
     >,</mo><msup 
    ><mi  >x</mi><mi
     >t</mi></msup><mo
    stretchy="false"  >]</mo></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow 
    ><msub  ><mi
     >h</mi><mi 
    >t</mi></msub><mo 
    >=</mo><mrow 
    ><mrow  ><mrow
     ><mo stretchy="false"
     >(</mo><mrow 
    ><mn  >1</mn><mo
     >−</mo><msup 
    ><mi mathvariant="normal" 
    >Γ</mi><mi  >z</mi></msup></mrow><mo
    rspace="0.055em" stretchy="false"  >)</mo></mrow><mo
    rspace="0.222em"  >⊗</mo><msub
     ><mi 
    >h</mi><mrow 
    ><mi 
    >t</mi><mo 
    >−</mo><mn 
    >1</mn></mrow></msub></mrow><mo 
    >+</mo><mrow 
    ><msup  ><mi
    mathvariant="normal"  >Γ</mi><mi
     >z</mi></msup><mo
    lspace="0.222em" rspace="0.222em"  >⊗</mo><msub
     ><mover accent="true"
     ><mi 
    >h</mi><mo 
    >~</mo></mover><mi 
    >t</mi></msub></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >Γ</ci><ci  >𝑧</ci></apply><apply
     ><ci 
    >𝜎</ci><apply  ><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑊</ci><ci  >𝑧</ci></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >ℎ</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐼</ci><ci 
    >𝑧</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑥</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >Γ</ci><ci  >𝑟</ci></apply></apply></apply><apply
     ><apply 
    ><ci  >𝜎</ci><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝑊</ci><ci 
    >𝑟</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℎ</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐼</ci><ci 
    >𝑟</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑥</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><ci  >~</ci><ci
     >ℎ</ci></apply><ci
     >𝑡</ci></apply></apply></apply><apply
     ><apply 
    ><ci  >𝑡</ci><ci
     >𝑎</ci><ci 
    >𝑛</ci><ci  >ℎ</ci><apply
     ><ci 
    >𝑊</ci><interval closure="closed" 
    ><apply  ><csymbol
    cd="latexml"  >tensor-product</csymbol><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >ℎ</ci><apply
     ><ci
     >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >Γ</ci><ci
     >𝑟</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >𝑥</ci><ci
     >𝑡</ci></apply></interval></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >ℎ</ci><ci 
    >𝑡</ci></apply></apply></apply><apply 
    ><apply  ><apply
     ><csymbol cd="latexml" 
    >tensor-product</csymbol><apply 
    ><cn type="integer"  >1</cn><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >Γ</ci><ci 
    >𝑧</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ℎ</ci><apply 
    ><ci 
    >𝑡</ci><cn type="integer" 
    >1</cn></apply></apply></apply><apply 
    ><csymbol cd="latexml" 
    >tensor-product</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >Γ</ci><ci 
    >𝑧</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><ci  >~</ci><ci
     >ℎ</ci></apply><ci
     >𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&\Gamma^{z}=\sigma(W^{z}h_{t-1}+I^{z}x^{t})\\
    &\Gamma^{r}=\sigma(W^{r}h_{t-1}+I^{r}x^{t})\\ &\widetilde{h}_{t}=tanh(W[h_{t-1}\otimes\Gamma^{r},x^{t}])\\
    &h_{t}=(1-\Gamma^{z})\otimes h_{t-1}+\Gamma^{z}\otimes\widetilde{h}_{t}\end{split}</annotation></semantics></math>
    |  |'
  prefs: []
  type: TYPE_NORMAL
- en: Where $W^{z}$ and $W^{r}$ are the weight matrices associated with gates, and
    $\Gamma^{z}$ and $\Gamma^{r}$ represent the update and reset gates, respectively.
    The function $\sigma$ denotes the logistic sigmoid, and $\otimes$ shows the element-wise
    product.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5\. The architecture of LSTM (a) and GRU (b) units
  prefs: []
  type: TYPE_NORMAL
- en: B.4\. Attention Based Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: B.4.1\. Self-Attention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The attention mechanism was introduced by ([bahdanau2014neural,](#bib.bib302)
    ) for improving the performance of encoder-decoder models ([cho2014learning,](#bib.bib303)
    ) in neural machine translation. The encoder-decoder in neural machine translation
    encodes a source sentence into a vector in latent space and decodes the latent
    vector into a target language sentence. As shown in Fig. [6](#A2.F6 "Figure 6
    ‣ B.4.1\. Self-Attention ‣ B.4\. Attention Based Model ‣ Appendix B DNN Architectures
    for Time Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey"), the attention mechanism allows the decoder to pay attention
    to the segments of the source for each target through a context vector $c_{t}$.
    For this model, a variable-length attention vector $\alpha_{t}$, equal to the
    number of source time steps, is derived by comparing the current target hidden
    state $h_{t}$ with each source hidden state $\overline{h}_{s}$ as follows ([luong2015effective,](#bib.bib304)
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\alpha_{t}(s)=\frac{{exp(score(h_{t},\overline{h}_{s})})}{\sum_{s^{\prime}}exp(score(h_{t},\overline{h}_{s^{\prime}}))}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The term $score$ is referred to as an alignment model and used to compare the
    target hidden state $h_{t}$ with each of the source hidden states $\overline{h}_{s}$,
    and the result is normalized to produced attention weights (a distribution over
    source positions). There are various choices of the scoring function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $score(h_{t},\overline{h}_{s})=\begin{cases}h_{t}^{T}W\overline{h}_{s}\\
    v_{\alpha}^{T}tanh(W_{\alpha}[h_{t};\overline{h}_{s}])\end{cases}$ |  |'
  prefs: []
  type: TYPE_TB
- en: These scores influence the attention distribution, impacting how the model attends
    to different parts of the input sequence during predictions. As shown above, the
    score function is parameterized as a feedforward neural network that is jointly
    trained with all the other components of the model. The model directly computes
    soft attention, allowing the cost function’s gradient to be backpropagated ([bahdanau2014neural,](#bib.bib302)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the alignment vector as weights, the context vector $c_{t}$ is computed
    as the weighted average over all the source hidden state:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $c_{t}=\sum_{s}\alpha_{ts}\overline{h}_{s}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Accordingly, the computation path goes from $h_{t}\rightarrow\alpha_{t}\rightarrow
    c_{t}\rightarrow\widetilde{h}_{t}$ then make a prediction using a $Softmax$ function ([luong2015effective,](#bib.bib304)
    ). Note that $\widetilde{h}_{t}$ is a refined hidden state that incorporates both
    the original hidden state $h_{t}$ and the context information $c_{t}$ obtained
    through attention mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09e51ab0a8e45d63d08e287c4b6f4b21.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Self-Attention mechanism
  prefs: []
  type: TYPE_NORMAL
- en: B.4.2\. Transformers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to self-attention and other competitive neural sequence models, the
    original transformer developed for NLP (hereinafter the vanilla transformer) has
    an encoder-decoder structure that takes as input a sequence of words from the
    source language and then generates the translation in the target language ([vaswani2017attention,](#bib.bib92)
    ). Both the encoder and decoder are composed of multiple identical blocks. Each
    encoder block consists of a multi-head self-attention module and a position-wise
    feed-forward network (FFN), while each decoder block inserts cross-attention models
    between the multi-head self-attention module and the position-wise feed-forward
    network (FFN). Unlike RNNs, Transformers do not use recurrence and instead model
    sequence information using the positional encoding in the input embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformer architecture is based on finding associations or correlations
    between various input segments using the dot product. As shown in Fig. [7](#A2.F7
    "Figure 7 ‣ B.4.2\. Transformers ‣ B.4\. Attention Based Model ‣ Appendix B DNN
    Architectures for Time Series ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey"), the attention operation in transformers
    starts with building three different linearly-weighted vectors from the input
    $x_{i}$, referred to as query ($q_{i}$), key ($k_{i}$), and value ($v_{i}$):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $\textbf{q}_{i}=W_{q}\textbf{x}_{i},\quad\textbf{k}_{i}=W_{k}\textbf{x}_{i},\quad\textbf{v}_{i}=W_{v}\textbf{x}_{i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $W_{q},W_{k}$ and $W_{v}$ learnable weight matrices. The output vectors
    $\textbf{z}_{i}$ are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $\textbf{z}_{i}=\sum_{j}softmax\left(\frac{\textbf{q}_{i}^{T}\textbf{k}_{j}}{\sqrt{d_{q}}}\right)\textbf{v}_{i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Note that the weighting of the value vector $\textbf{v}_{i}$ depends on the
    mapped correlation between the query vector $\textbf{q}_{i}$ at position $i$ and
    the key vector $\textbf{k}_{j}$ at position $j$. The value of the dot product
    tends to grow with the increasing size of the query and key vectors. As the softmax
    function is sensitive to large values, the attention weights are scaled by the
    square root of the size of the query and key vectors $d_{q}$. The input data may
    contain several levels of correlation information, and the learning process may
    benefit from processing the input data in multiple different ways. Multiple attention
    heads are introduced that operate on the same input in parallel and use different
    weight matrices $W_{q}$,$W_{k}$, and $W_{v}$ to extract various levels of correlation
    between the input data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fbabde38bdb9a91670cfd07af6bb8d32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7\. Multi-head attention block: the example consists of eight heads,
    and the input sequence comprises two time steps.'
  prefs: []
  type: TYPE_NORMAL
- en: B.5\. Graph Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A graph consists of a set of nodes and a set of edges, each of which connects
    two nodes. Both nodes and edges may have attributes associated with them. The
    edges may be directional or unidirectional, and may be weighted. Graphs are useful
    for representing data that cannot be represented in Euclidean space, such as molecular
    structures, social networks, and spatial temporal data (for example, electroencephalogram
    (EEG) or traffic monitoring networks).
  prefs: []
  type: TYPE_NORMAL
- en: Graph neural networks (GNNs) were first proposed by Scarcelli et al. ([Scarselli2009graph,](#bib.bib120)
    ) to learn directly from graph representations of data. Prior to the use of GNNs,
    techniques such as recursive neural networks and Markov chains (random walk models)
    were used to incorporate graph structures. However, these methods required a pre-processing
    step, rather than learning directly from the graph structure. GNNs take as input
    the graph structure and any associated node and edge attributes. Depending on
    the required task, the GNN output can be per node, per edge or a single output
    per graph ([Wu2021graph,](#bib.bib119) ).
  prefs: []
  type: TYPE_NORMAL
- en: Scarcelli et al.’s proposed GNN combines recursive neural networks and Markov
    chains to deal directly with the graph structure without any pre-processing requirement.
    While the network structure is predefined, the edge weights are parameters learned
    during training. During training, units exchange information and update their
    states until reaching equilibrium.
  prefs: []
  type: TYPE_NORMAL
- en: Graph convolutional networks (GCN) were proposed by Bruna et al. ([Bruna2013graph,](#bib.bib305)
    ) and extend CNNs to graph structures. Bruna et al. proposed two methods of constructing
    the graph, spatial and spectral. The spatial technique simply applies the convolution
    operator to the local neighbourhood of each node, followed by a pooling operator.
    Although this reduces the spatial resolution, successive layers compensate for
    this by increasing the number of filters. Spectral construction first transforms
    the graph to a matrix V, which consists of the eigenvectors of the graph Laplacian,
    order by eigenvalue. The eigenvectors represent frequency components of the original
    graphs, so lower-order eigenvectors modulate slowly, thus neighbouring nodes have
    similar values. Higher-order eigenvectors modulate more rapidly and connected
    nodes are likely to have dissimilar values ([Shuman2013graph,](#bib.bib306) ).
  prefs: []
  type: TYPE_NORMAL
- en: Many real-world graph datasets evolve over time – edges and nodes may come into
    existence, disappear or attributes may change value. Dynamic or temporal graphs
    build this information into the graph structure, for instance by the use of temporal
    nodes and edges, that include initial and final timestamps ([Longa2023graph,](#bib.bib307)
    ). Alternatively, spatiotemporal GNNs model the spatial and temporal aspects in
    separate layers, using GCN layers to learn spatial representations and RNN or
    1D-CNN layers for the temporal representations ([Wu2021graph,](#bib.bib119) ).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1\. HAR Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many of the studies reviewed in the subsection reviewing the use of deep learning
    for human activity recognition time series use publicly available datasets. Some
    of the most commonly used datasets are listed in table [7](#A3.T7 "Table 7 ‣ C.1\.
    HAR Datasets ‣ Appendix C Datasets ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey"), together with the number of participants,
    the sensors used to collect the data, a description of the activities recorded,
    and references to the studies using each dataset. Larger lists of datasets are
    provided in ([chen2021deep,](#bib.bib5) ; [Ramanujam2021survey,](#bib.bib186)
    ). Common activity sets include activities of daily living (ADL) or basic activities
    (e.g. walking, running, sitting, standing, ascending/descending stairs. However,
    activities for more specialised events such as gait freezing in Parkinson’s Disease
    patients ([Bachlin2009daphnet,](#bib.bib308) ), falls ([Micucci2017unimib,](#bib.bib309)
    ), and manufacturing activities ([Zappi2008skoda,](#bib.bib310) ) are also collected.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Partici- pants | Sensors | Activities/ Description | Used by |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DAPHNet FoG ([Bachlin2009daphnet,](#bib.bib308) ) | 10 | 3 accelerometers
    attached to the body | Detection of freezing of gait events in Parkinson’s Disease
    patients. | ([Murad2017deep,](#bib.bib205) ; [Zeng2018rnn,](#bib.bib209) ; [Hammerla2016deep,](#bib.bib198)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| Opportunity challenge ([Chavarriaga2013opp,](#bib.bib311) ; [Roggen2010opp,](#bib.bib194)
    ) | 4 | 5 on-body IMUs each with 3 sensors | A variety of activities of daily
    living. | ([Ordonez2016deep,](#bib.bib191) ; [Wang2022deep,](#bib.bib220) ; [Rueda2018cnn,](#bib.bib207)
    ; [Zeng2014cnn,](#bib.bib199) ; [Xu2022deform,](#bib.bib221) )  ([Yao2018cnn,](#bib.bib208)
    ; [Murad2017deep,](#bib.bib205) ; [Guan2017ensemble,](#bib.bib203) ; [Yang2015cnn,](#bib.bib201)
    ; [Hammerla2016deep,](#bib.bib198) ) |'
  prefs: []
  type: TYPE_TB
- en: '| PAMAP2 ([Reiss2012pamap,](#bib.bib192) ; [Reiss2012pamap2,](#bib.bib312)
    ) | 9 | 3 IMUs and heart rate monitor | 18 basic and daily living activities.
    | ([Wang2022deep,](#bib.bib220) ; [Rueda2018cnn,](#bib.bib207) ; [Zeng2018rnn,](#bib.bib209)
    ; [Guan2017ensemble,](#bib.bib203) ) ([Challa2021multi,](#bib.bib213) ; [Chen2021har,](#bib.bib215)
    ; [Ma2019attnsense,](#bib.bib210) ; [Hammerla2016deep,](#bib.bib198) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Skoda ([Zappi2008skoda,](#bib.bib310) ) | 1 | 19 body-worn sensors | 10 quality
    assurance activities from car production plant. | ([Ordonez2016deep,](#bib.bib191)
    ; [Zeng2014cnn,](#bib.bib199) ; [Murad2017deep,](#bib.bib205) ) ([Zeng2018rnn,](#bib.bib209)
    ; [Guan2017ensemble,](#bib.bib203) ; [Ma2019attnsense,](#bib.bib210) ) |'
  prefs: []
  type: TYPE_TB
- en: '| UCI-HAR ([Anguita2013ucihar,](#bib.bib313) ) | 30 | Smartphone accelerometer
    and gyroscope | 6 basic activities. | ([Wang2022deep,](#bib.bib220) ; [Ronao2016har,](#bib.bib202)
    ; [Ignatov2018realtime,](#bib.bib206) ; [Jiang2015cnn,](#bib.bib200) ; [Murad2017deep,](#bib.bib205)
    ) ([Challa2021multi,](#bib.bib213) ; [Nafea2021sensor,](#bib.bib218) ; [Mekruksavanich2021lstm,](#bib.bib216)
    ; [Mekruksavanich2021biometric,](#bib.bib217) ) |'
  prefs: []
  type: TYPE_TB
- en: '| UniMiB SHAR ([Micucci2017unimib,](#bib.bib309) ) | 30 | Smartphone accelerometers
    | Activities of daily living and falls. | ([Wang2022deep,](#bib.bib220) ; [Xu2022deform,](#bib.bib221)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| USC-HAD ([Zhang2012uschad,](#bib.bib193) ) | 14 | Motion node (accelerometer,
    gyroscope, and magnetometer) attached to hip | 12 basic activities. | ([Jiang2015cnn,](#bib.bib200)
    ; [Xu2022deform,](#bib.bib221) ; [Murad2017deep,](#bib.bib205) ; [Singh2021convlstm,](#bib.bib219)
    ; [Mekruksavanich2021biometric,](#bib.bib217) ) |'
  prefs: []
  type: TYPE_TB
- en: '| WISDM ([Kwapisz2011wisdm,](#bib.bib314) ) | 29 | Smartphone accelerometer
    | 6 basic activities. | ([Wang2022deep,](#bib.bib220) ; [Ignatov2018realtime,](#bib.bib206)
    ; [Zhang2020novel,](#bib.bib212) ; [Xu2022deform,](#bib.bib221) ) ([Singh2021convlstm,](#bib.bib219)
    ; [Challa2021multi,](#bib.bib213) ; [Nafea2021sensor,](#bib.bib218) ) |'
  prefs: []
  type: TYPE_TB
- en: Table 7\. Commonly used human activity recognition datasets
  prefs: []
  type: TYPE_NORMAL
- en: C.2\. Earth observation satellites and instruments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [8](#A3.T8 "Table 8 ‣ C.2\. Earth observation satellites and instruments
    ‣ Appendix C Datasets ‣ Deep Learning for Time Series Classification and Extrinsic
    Regression: A Current Survey") lists the main satellites and instruments used
    in the studies reviewed for this survey. The table lists references for each source,
    which provide more details about the data collected, plus a list of the studies
    using each source.'
  prefs: []
  type: TYPE_NORMAL
- en: Agency Satellite & Instruments Type Used by NASA Landsat-7 ETM+ ([USGALandsat,](#bib.bib315)
    ) Optical ([Xu2020dcm,](#bib.bib246) ; [Dou2021tsi,](#bib.bib254) ) NASA Landsat-8
    OLI ([USGALandsat,](#bib.bib315) ) Optical ([Xu2020dcm,](#bib.bib246) ; [Dou2021tsi,](#bib.bib254)
    ; [Li2019tan,](#bib.bib243) ; [Li2020tga,](#bib.bib244) ; [Kussul2017cnn,](#bib.bib252)
    ),
  prefs: []
  type: TYPE_NORMAL
- en: ([DiMauro2017tiselc,](#bib.bib251) ; [Matosak2022forest,](#bib.bib234) ; [Rao2020lfmc,](#bib.bib237)
    ; [Xie2022lfmc,](#bib.bib240) ) NASA Terra/Aqua MODIS ([NASAMODIS,](#bib.bib316)
    ) Optical ([Xie2022lfmc,](#bib.bib240) ; [Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239)
    ; [Sun2020yield,](#bib.bib242) ; [Qiao2021yield,](#bib.bib263) ) ESA Sentinel-1A/1B
  prefs: []
  type: TYPE_NORMAL
- en: SAR-C ([ESA2019Sentinel,](#bib.bib317) ) Microwave
  prefs: []
  type: TYPE_NORMAL
- en: SAR ([Kussul2017cnn,](#bib.bib252) ; [Rao2020lfmc,](#bib.bib237) ; [Xie2022lfmc,](#bib.bib240)
    ; [Ofori-Ampofo2021att,](#bib.bib249) ; [Ienco2019twinns,](#bib.bib255) ; [Gbodjo2020hob2srnn,](#bib.bib260)
    ),
  prefs: []
  type: TYPE_NORMAL
- en: ([Ienco2019od2rnn,](#bib.bib261) ; [Ban2020fire,](#bib.bib231) ; [Rambour2020flood,](#bib.bib232)
    ; [Kulshrestha2022hole,](#bib.bib230) ; [Minh2017vege,](#bib.bib235) ) ESA Sentinel-2A/2B
    MSI ([ESA2019Sentinel,](#bib.bib317) ) Optical ([Li2019tan,](#bib.bib243) ; [Li2020tga,](#bib.bib244)
    ; [Matosak2022forest,](#bib.bib234) ; [Ofori-Ampofo2021att,](#bib.bib249) ; [Ienco2019twinns,](#bib.bib255)
    ; [Ienco2019od2rnn,](#bib.bib261) ; [Rambour2020flood,](#bib.bib232) ),
  prefs: []
  type: TYPE_NORMAL
- en: ([Garnot2019time,](#bib.bib266) ; [Garnot2020tae,](#bib.bib229) ; [Garnot2020ltae,](#bib.bib248)
    ; [Yuan2021sitsbert,](#bib.bib250) ; [Ienco2020tassel,](#bib.bib228) ; [Interdonato2019duplo,](#bib.bib256)
    ; [Barriere2022lstm,](#bib.bib247) ), ([Russwurm2018seqrnn,](#bib.bib257) ; [Russwurm2020att,](#bib.bib264)
    ; [Yuan2022sitsformer,](#bib.bib262) ; [Stoian2019fgunet,](#bib.bib258) ; [Labenski2022under,](#bib.bib236)
    ; [KamdemDeTeyou2020road,](#bib.bib233) ; [Lahssini2022wood,](#bib.bib241) ) CNES
    (France) Pléiades-1A/1B HiRI ([ESAPleiades,](#bib.bib318) ) Optical ([Ienco2017rnn,](#bib.bib259)
    ) NSPO (Taiwan) Formosat-2 RSI ([NSPO2020Formosat,](#bib.bib319) ) Optical ([Pelletier2019tempcnn,](#bib.bib253)
    ) CRESDA (China) Gaofen-1/2 MUX, PAN,
  prefs: []
  type: TYPE_NORMAL
- en: WFV ([EoPortal2014Gaofen1,](#bib.bib320) ; [EoPortal2015Gaofen2,](#bib.bib321)
    ) Optical  ([Ji20183dcnn,](#bib.bib245) )
  prefs: []
  type: TYPE_NORMAL
- en: Table 8\. Earth Observation satellites and instruments collecting the data used
    in the studies reviewed.
  prefs: []
  type: TYPE_NORMAL
