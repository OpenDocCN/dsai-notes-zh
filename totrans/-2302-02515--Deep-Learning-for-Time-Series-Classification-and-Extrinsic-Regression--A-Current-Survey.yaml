- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:42:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:42:00'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.02515] Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.02515] 深度学习在时间序列分类和外部回归中的应用：当前调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.02515](https://ar5iv.labs.arxiv.org/html/2302.02515)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.02515](https://ar5iv.labs.arxiv.org/html/2302.02515)
- en: 'Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在时间序列分类和外部回归中的应用：当前调查
- en: Navid Mohammadi Foumani [navid.foumani@monash.edu.com](mailto:navid.foumani@monash.edu.com)
    [0000-0003-2475-6040](https://orcid.org/0000-0003-2475-6040 "ORCID identifier")
    Monash UniversityAustralia ,  Lynn Miller [lynn.miller1@monash.edu](mailto:lynn.miller1@monash.edu)
    Monash UniversityAustralia ,  Chang Wei Tan [chang.tan@monash.edu](mailto:chang.tan@monash.edu)
    Monash UniversityAustralia ,  Geoffrey I. Webb [geoff.webb@monash.edu](mailto:geoff.webb@monash.edu)
    Monash UniversityAustralia ,  Germain Forestier [germain.forestier@uha.fr](mailto:germain.forestier@uha.fr)
    Monash UniversityAustralia IRIMAS, University of Haute-AlsaceFrance  and  Mahsa
    Salehi [mahsa.salehi@monash.edu](mailto:mahsa.salehi@monash.edu) Monash UniversityAustralia(2023)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Navid Mohammadi Foumani [navid.foumani@monash.edu.com](mailto:navid.foumani@monash.edu.com)
    [0000-0003-2475-6040](https://orcid.org/0000-0003-2475-6040 "ORCID identifier")
    莫纳什大学 澳大利亚，Lynn Miller [lynn.miller1@monash.edu](mailto:lynn.miller1@monash.edu)
    莫纳什大学 澳大利亚，Chang Wei Tan [chang.tan@monash.edu](mailto:chang.tan@monash.edu) 莫纳什大学
    澳大利亚，Geoffrey I. Webb [geoff.webb@monash.edu](mailto:geoff.webb@monash.edu) 莫纳什大学
    澳大利亚，Germain Forestier [germain.forestier@uha.fr](mailto:germain.forestier@uha.fr)
    莫纳什大学 澳大利亚 IRIMAS，奥特-阿尔萨斯大学 法国和 Mahsa Salehi [mahsa.salehi@monash.edu](mailto:mahsa.salehi@monash.edu)
    莫纳什大学 澳大利亚（2023）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Time Series Classification and Extrinsic Regression are important and challenging
    machine learning tasks. Deep learning has revolutionized natural language processing
    and computer vision and holds great promise in other fields such as time series
    analysis where the relevant features must often be abstracted from the raw data
    but are not known a priori. This paper surveys the current state of the art in
    the fast-moving field of deep learning for time series classification and extrinsic
    regression. We review different network architectures and training methods used
    for these tasks and discuss the challenges and opportunities when applying deep
    learning to time series data. We also summarize two critical applications of time
    series classification and extrinsic regression, human activity recognition and
    satellite earth observation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分类和外部回归是重要且具有挑战性的机器学习任务。深度学习已经彻底改变了自然语言处理和计算机视觉，并在其他领域如时间序列分析中展现出巨大的潜力，其中相关特征常常需要从原始数据中抽象出来，而这些特征在事先并不为人所知。本文调查了深度学习在时间序列分类和外部回归领域的最新进展。我们回顾了这些任务中使用的不同网络架构和训练方法，并讨论了将深度学习应用于时间序列数据时面临的挑战和机遇。我们还总结了时间序列分类和外部回归的两个关键应用，即人类活动识别和卫星地球观测。
- en: 'Deep Learning, Time series, Classification, Extrinsic regression, Review^†^†copyright:
    acmcopyright^†^†journalyear: 2023^†^†doi: XXXXXXX.XXXXXXX^†^†ccs: Computing methodologies Machine
    learning approaches^†^†ccs: Computing methodologies Supervised learning'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，时间序列，分类，外部回归，综述^†^†版权：acmcopyright^†^†期刊年份：2023^†^†doi：XXXXXXX.XXXXXXX^†^†ccs：计算方法
    机器学习方法^†^†ccs：计算方法 监督学习
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Time series analysis has been identified as one of the ten most challenging
    research issues in the field of data mining in the 21st century ([yang200610,](#bib.bib1)
    ). Time series classification (TSC) is a key time series analysis task ([esling2012time,](#bib.bib2)
    ). TSC builds a machine learning model to predict categorical class labels for
    data consisting of ordered sets of real-valued attributes. The many applications
    of time series analysis include human activity recognition ([nweke2018deep,](#bib.bib3)
    ; [wang2019deep,](#bib.bib4) ; [chen2021deep,](#bib.bib5) ), diagnosis based on
    electronic health records ([schirrmeister2017deep,](#bib.bib6) ; [rajkomar2018scalable,](#bib.bib7)
    ), and systems monitoring problems ([bagnall2018uea,](#bib.bib8) ). The wide variety
    of dataset types in the University of California, Riverside (UCR) ([dau2019ucr,](#bib.bib9)
    ) and University of East Anglia (UEA) ([bagnall2018uea,](#bib.bib8) ) benchmark
    archive further illustrates the breadth of TSC applications. Time series extrinsic
    regression (TSER) ([tan2021time,](#bib.bib10) ) is the counterpart of TSC for
    which the output is numeric rather than categorical. It should be noted that the
    TSER is not a forecasting method but rather a method for understanding the relationship
    between the time series and the extrinsic variable. TSER is an emerging field
    with great potential to be used in a wide range of applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分析已被认定为21世纪数据挖掘领域十大最具挑战性的研究问题之一 ([yang200610,](#bib.bib1) )。时间序列分类（TSC）是一个关键的时间序列分析任务 ([esling2012time,](#bib.bib2)
    )。TSC构建一个机器学习模型，以预测由有序的实值属性集合组成的数据的类别标签。时间序列分析的许多应用包括人类活动识别 ([nweke2018deep,](#bib.bib3)
    ; [wang2019deep,](#bib.bib4) ; [chen2021deep,](#bib.bib5) )、基于电子健康记录的诊断 ([schirrmeister2017deep,](#bib.bib6)
    ; [rajkomar2018scalable,](#bib.bib7) )，以及系统监控问题 ([bagnall2018uea,](#bib.bib8)
    )。加州大学河滨分校（UCR） ([dau2019ucr,](#bib.bib9) )和东安格利亚大学（UEA） ([bagnall2018uea,](#bib.bib8)
    )基准档案中数据集类型的广泛多样性进一步展示了TSC应用的广度。时间序列外生回归（TSER） ([tan2021time,](#bib.bib10) )是TSC的对应方法，其输出为数值而非类别。需要注意的是，TSER不是预测方法，而是理解时间序列与外生变量之间关系的方法。TSER是一个新兴领域，具有广泛的应用潜力。
- en: Deep learning has been very successful, especially in computer vision and natural
    language processing. Many modern applications integrate deep learning. Deep learning
    can autonomously learn informative features from raw data, eliminating the need
    for manual feature engineering. Consequently, there has been much interest in
    developing deep TSC and TSER due to their ability to learn relevant latent feature
    representations. It is worth noting that the majority of TSC and TSER research
    has focused on non-deep learning approaches. A recent benchmark ([middlehurst2023bake,](#bib.bib11)
    ) shows that the deep learning method (InceptionTime ([fawaz2020inceptiontime,](#bib.bib12)
    )) is competitive but did not outperform the state of the art on benchmarking
    archives. One reason is that the popular UCR and UEA benchmarking archives were
    not designed for deep learning models. In particular, they are relatively small,
    while deep learning often excels when data quantities are large. Deep learning
    can also benefit from heightened compatibility with current hardware, particularly
    GPUs, leading to fast and efficient execution. Their exceptional scalability further
    allows seamless handling of growing data volumes and computational complexity,
    reinforcing their versatility in processing large datasets. Indeed, ConvTran ([Foumani2023,](#bib.bib13)
    ), a recent deep architecture for TSC, outperforms one of the fastest conventional
    models, ROCKET ([dempster2019rocket,](#bib.bib14) ), in terms of both speed and
    accuracy when there are more than 10k training samples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习取得了显著成功，尤其是在计算机视觉和自然语言处理方面。许多现代应用都整合了深度学习。深度学习可以从原始数据中自动学习信息特征，消除了手动特征工程的需求。因此，由于其能够学习相关的潜在特征表示，开发深度TSC和TSER受到了极大的关注。值得注意的是，大多数TSC和TSER的研究集中在非深度学习方法上。一项最新的基准测试 ([middlehurst2023bake,](#bib.bib11)
    )显示，深度学习方法（InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) )）具有竞争力，但在基准档案中的表现未超越现有技术。一个原因是流行的UCR和UEA基准档案并未为深度学习模型设计。特别是，它们相对较小，而深度学习通常在数据量大的情况下表现更佳。深度学习还可以从与当前硬件，特别是GPU的高度兼容中受益，从而实现快速高效的执行。其卓越的可扩展性进一步允许无缝处理不断增长的数据量和计算复杂性，增强了其在处理大数据集中的多功能性。确实，最近的一种用于TSC的深度架构ConvTran ([Foumani2023,](#bib.bib13)
    )在训练样本超过1万的情况下，在速度和准确性方面超越了最快的传统模型ROCKET ([dempster2019rocket,](#bib.bib14) )。
- en: A highly influential review paper on deep learning-based TSC ([fawaz2019deep,](#bib.bib15)
    ) was published in 2019\. However, the field of research is very fast-moving,
    and that prior survey does not cover the current state of the art. For example,
    it does not include InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) ), a
    system that consistently outperforms ResNet ([wang2017time,](#bib.bib16) ), the
    best performing system from the prior survey. Nor does it cover attention models,
    which have received huge interest in recent years and have shown excellent capacity
    to model long-range dependencies in sequential data, and are well suited for time-series
    modeling ([wen2022transformers,](#bib.bib17) ). Many attention variants have been
    proposed to address particular challenges in time series modeling and have been
    successfully applied to TSC ([hao2020new,](#bib.bib18) ; [zerveas2021transformer,](#bib.bib19)
    ; [Foumani2023,](#bib.bib13) ). Moreover, the previous survey does not include
    self-supervised learning, which is emerging as a new paradigm ([liu2021self,](#bib.bib20)
    ). Self-supervised learning induces supervision by designing pretext tasks instead
    of relying on predefined prior knowledge and has shown very promising results,
    especially in datasets with a low label regime ([eldele2021time,](#bib.bib21)
    ; [yang2021voice2series,](#bib.bib22) ; [yue2022ts2vec,](#bib.bib23) ; [foumani2023series2vec,](#bib.bib24)
    ).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇关于基于深度学习的TSC的高度影响力综述论文（[fawaz2019deep](#bib.bib15)）于2019年发表。然而，该研究领域发展迅速，之前的综述未能涵盖当前的最新技术。例如，它没有包括InceptionTime（[fawaz2020inceptiontime](#bib.bib12)），这一系统在性能上始终优于ResNet（[wang2017time](#bib.bib16)），即之前综述中表现最佳的系统。此外，它也未涉及注意力模型，这些模型近年来受到极大关注，并展示了建模顺序数据中长期依赖关系的优异能力，非常适合时间序列建模（[wen2022transformers](#bib.bib17)）。许多注意力变体已经提出，以应对时间序列建模中的特定挑战，并成功应用于TSC（[hao2020new](#bib.bib18)；[zerveas2021transformer](#bib.bib19)；[Foumani2023](#bib.bib13)）。此外，之前的综述未包括自监督学习，这一新兴范式（[liu2021self](#bib.bib20)）通过设计前置任务来引导监督，而不是依赖预定义的先验知识，并在低标签数据集上表现出非常有前景的结果（[eldele2021time](#bib.bib21)；[yang2021voice2series](#bib.bib22)；[yue2022ts2vec](#bib.bib23)；[foumani2023series2vec](#bib.bib24)）。
- en: In light of the emergence of attention mechanisms, self-supervised learning,
    and various new network configurations for TSC, a systematic and comprehensive
    survey on deep learning in TSC would greatly benefit the time series community.
    This article aims to fill that gap by summarizing recent developments in deep
    learning-based time series analytics, specifically TSC and TSER. Following definitions
    and a brief introduction to the time series classification and extrinsic regression
    tasks, we propose a new taxonomy based on various methodological perspectives.
    Diverse architectures, including multilayer perceptrons (MLP), convolutional neural
    networks (CNN), recurrent neural networks (RNN), Graph Neural Network (GNN), and
    attention-based models, are discussed, along with refinements made to improve
    performance. Additionally, various types of self-supervised learning pretexts,
    such as contrastive learning and self-prediction, are explored. We also conduct
    a review of useful data augmentation and transfer learning strategies for time
    series data. Furthermore, we provide a summary of two key applications of TSC
    and TSER, namely Human Activity Recognition and Earth Observation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着注意力机制、自监督学习和各种新的时间序列分类（TSC）网络配置的出现，对深度学习在TSC中的系统性和全面性调查将极大地惠及时间序列社区。本文旨在填补这一空白，通过总结基于深度学习的时间序列分析的最新进展，特别是TSC和时间序列回归（TSER）。在定义和对时间序列分类及外部回归任务的简要介绍之后，我们提出了一种基于不同方法学视角的新分类法。讨论了多层感知器（MLP）、卷积神经网络（CNN）、递归神经网络（RNN）、图神经网络（GNN）和基于注意力的模型等多种架构，并对提升性能所做的改进进行了探讨。此外，还探讨了各种自监督学习前置任务，例如对比学习和自我预测。我们还回顾了适用于时间序列数据的有用数据增强和迁移学习策略。此外，我们还总结了TSC和TSER的两个关键应用，即人类活动识别和地球观测。
- en: 2\. Background and Definitions
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景与定义
- en: This section begins by providing the necessary definitions and background information
    to understand the topic of training deep neural networks (DNNs) for TSC and TSER
    tasks. We begin by defining key terms and concepts, such as time series data and
    time series supervised learning. Finally, we present our proposed taxonomy of
    the different deep learning methods that have been used for TSC and TSER tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先提供了理解训练深度神经网络（DNNs）用于时间序列分类（TSC）和时间序列回归（TSER）任务的必要定义和背景信息。我们从定义关键术语和概念开始，如时间序列数据和时间序列监督学习。最后，我们提出了用于
    TSC 和 TSER 任务的不同深度学习方法的分类法。
- en: 2.1\. Time series
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 时间序列
- en: Time series data are sequences of data points indexed by time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据是按时间索引的数据点序列。
- en: Definition 2.1.
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.1。
- en: A time series $X$ is an ordered collection of $T$ pairs of measurements and
    timestamps,
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列 $X$ 是 $T$ 对测量和时间戳的有序集合，
- en: $X=\{(x_{1},t_{1}),(x_{2},t_{2}),...,(x_{T},t_{T})\}$, where $x_{i}\in\mathbb{R}^{D}$
    and $t_{1}$ to $t_{T}$ are the timestamps for some measurements $x_{1}$ to $x_{T}$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: $X=\{(x_{1},t_{1}),(x_{2},t_{2}),...,(x_{T},t_{T})\}$，其中 $x_{i}\in\mathbb{R}^{D}$
    且 $t_{1}$ 到 $t_{T}$ 是一些测量 $x_{1}$ 到 $x_{T}$ 的时间戳。
- en: Each $x_{i}$ is a $D$-dimensional vector of values, one for each feature captured
    in the series. When $D=1$ the series is called *univariate*. When $D>1$ the series
    is called *multivariate*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 $x_{i}$ 是一个 $D$ 维值向量，表示序列中捕获的每个特征。当 $D=1$ 时，序列称为 *单变量*。当 $D>1$ 时，序列称为 *多变量*。
- en: 2.2\. Time series supervised learning tasks
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 时间序列监督学习任务
- en: 'This paper focuses on two time series learning tasks: time series extrinsic
    regression and time series classification. Classification and regression are both
    supervised learning tasks that learn the relationship between a target variable
    and a set of time series. We consider learning from a dataset $D=\left\{(X_{1},Y_{1}),(X_{2},Y_{2}),...,(X_{N},Y_{N})\right\}$
    of $N$ time series where $Y_{i}$ denotes the target variable for each $X_{i}$.
    It is important to note that for ease of exposition, we assume in our discussion
    that the series are of the same length, but most methods extend trivially to the
    case of unequal-length series. The main difference between TSER and TSC is that
    TSC predicts a categorical value for a time series from a set of finite categories,
    while TSER predicts a continuous value for a variable external to the input time
    series. Typically $Y_{i}$ is a one hot encoded vector for TSC or a numeric value
    for TSER.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文关注两个时间序列学习任务：时间序列外部回归和时间序列分类。分类和回归都是监督学习任务，它们学习目标变量与一组时间序列之间的关系。我们考虑从一个数据集
    $D=\left\{(X_{1},Y_{1}),(X_{2},Y_{2}),...,(X_{N},Y_{N})\right\}$ 中学习，该数据集包含 $N$
    个时间序列，其中 $Y_{i}$ 表示每个 $X_{i}$ 的目标变量。需要注意的是，为了便于描述，我们在讨论中假设序列长度相同，但大多数方法可以轻松扩展到长度不等的序列情况。TSER
    和 TSC 之间的主要区别在于，TSC 从有限类别集合中为时间序列预测一个类别值，而 TSER 为输入时间序列外部的变量预测一个连续值。通常 $Y_{i}$
    是一个用于 TSC 的 one hot 编码向量或用于 TSER 的数值。
- en: 'In the context of deep learning, a supervised learning model is a neural network
    that executes the following functions to map the input time series to a target
    variable:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的背景下，监督学习模型是一个神经网络，它执行以下函数将输入时间序列映射到目标变量：
- en: '| (1) |  | $f_{L}(\theta_{L},X)=f_{L-1}(\theta_{L-1},f_{L-2}(\theta_{L-2},\ldots,f_{1}(\theta_{1},X)))$
    |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $f_{L}(\theta_{L},X)=f_{L-1}(\theta_{L-1},f_{L-2}(\theta_{L-2},\ldots,f_{1}(\theta_{1},X)))$
    |  |'
- en: where $f_{i}$ represents the non-linear function and $\theta_{i}$ denotes the
    parameters at layer $i$. For TSC the neural network model is trained to map a
    time series dataset $D$ to a set of class labels $Y$ with $C$ class labels. After
    training, the neural network outputs a vector of $C$ values that estimates the
    probability of a series $X$ belonging to each class. This is typically achieved
    using the softmax activation function in the final layer of the neural network.
    The softmax function estimates probabilities for all of the dependent classes
    such that they always sum to 1 across all classes. The cross-entropy loss is commonly
    used for training neural networks with softmax outputs or classification type
    neural networks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f_{i}$ 代表非线性函数，$\theta_{i}$ 表示第 $i$ 层的参数。对于 TSC，神经网络模型被训练来将时间序列数据集 $D$ 映射到一组包含
    $C$ 个类别标签的 $Y$。训练后，神经网络输出一个包含 $C$ 个值的向量，估计序列 $X$ 属于每个类别的概率。这通常通过神经网络最后一层的 softmax
    激活函数来实现。softmax 函数为所有相关类别估计概率，使它们在所有类别中总和为 1。交叉熵损失通常用于训练具有 softmax 输出或分类类型的神经网络。
- en: On the other hand, TSER trains the neural network model to map a time series
    dataset $D$ to a set of numeric values $Y$. Instead of outputting probabilities,
    a regression neural network outputs a numerical value for the time series. It
    is typically used with a linear activation function in the final layer of the
    neural network. However, any non-linear functions with a single value output such
    as sigmoid, or ReLU can also be used. A regression neural network typically trains
    using the mean square error or mean absolute error loss function. However, depending
    on the distribution of the target variable and the choice of final activation
    functions, other loss functions can be used.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，TSER 训练神经网络模型，将时间序列数据集 $D$ 映射到一组数值 $Y$。与输出概率不同，回归神经网络为时间序列输出一个数值。它通常在神经网络的最终层使用线性激活函数。然而，也可以使用任何具有单一值输出的非线性函数，如
    sigmoid 或 ReLU。回归神经网络通常使用均方误差或均绝对误差损失函数进行训练。然而，根据目标变量的分布和最终激活函数的选择，也可以使用其他损失函数。
- en: 2.3\. TSC and TSER
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. TSC 和 TSER
- en: 'TSC is a fast-growing field, with hundreds of papers being published every
    year ([bagnall2018uea,](#bib.bib8) ; [dau2019ucr,](#bib.bib9) ; [bagnall2017great,](#bib.bib25)
    ; [fawaz2019deep,](#bib.bib15) ; [ruiz2020great,](#bib.bib26) ). The majority
    of work in TSC are non-deep learning based. In this survey, we focus on deep learning
    approaches and refer interested readers to Appendix [A](#A1 "Appendix A Non-Deep
    Learning Time Series Classification ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey") and benchmark papers ([middlehurst2023bake,](#bib.bib11)
    ; [bagnall2017great,](#bib.bib25) ; [ruiz2020great,](#bib.bib26) ) for more details
    on non-deep learning approaches. Most deep learning approaches to TSC have real-valued
    outputs that are mapped to a class label. TSER ([tan2021time,](#bib.bib10) ; [tan2020monash,](#bib.bib27)
    ) is a less widely studied task in which the predicted values are numeric, rather
    than categorical. While the majority of the architectures covered in this survey
    were designed for TSC, it is important to note that it is trivial to adapt most
    of them for TSER.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'TSC 是一个快速发展的领域，每年都有数百篇论文发表（[bagnall2018uea](#bib.bib8)；[dau2019ucr](#bib.bib9)；[bagnall2017great](#bib.bib25)；[fawaz2019deep](#bib.bib15)；[ruiz2020great](#bib.bib26)）。TSC
    中大多数工作不基于深度学习。在这项调查中，我们关注于深度学习方法，并将有兴趣的读者参考附录 [A](#A1 "Appendix A Non-Deep Learning
    Time Series Classification ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey") 和基准论文（[middlehurst2023bake](#bib.bib11)；[bagnall2017great](#bib.bib25)；[ruiz2020great](#bib.bib26)），以获取更多有关非深度学习方法的细节。大多数深度学习方法在
    TSC 中具有实值输出，这些输出被映射到类别标签上。TSER（[tan2021time](#bib.bib10)；[tan2020monash](#bib.bib27)）是一个研究较少的任务，其中预测值为数值而非类别值。尽管本调查中涵盖的大多数架构是为
    TSC 设计的，但需要注意的是，将它们大多数适配于 TSER 是很简单的。'
- en: 'Deep learning-based TSC methods can be classified into two main types: generative
    and discriminative ([langkvist2014review,](#bib.bib28) ). In the TSC community,
    generative methods are often considered model-based ([bagnall2017great,](#bib.bib25)
    ), aiming to understand and model the joint probability distribution of input
    series $X$ and output labels $Y$, denoted as $p(X,Y)$. On the other hand, discriminative
    models focus on modeling the conditional probability of output labels $Y$ given
    input series $X$, expressed as $p(Y|X)$.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的 TSC 方法可以分为两种主要类型：生成型和判别型（[langkvist2014review](#bib.bib28)）。在 TSC 社区中，生成型方法通常被认为是基于模型的（[bagnall2017great](#bib.bib25)），旨在理解和建模输入序列
    $X$ 和输出标签 $Y$ 的联合概率分布，表示为 $p(X,Y)$。另一方面，判别模型则专注于建模给定输入序列 $X$ 时输出标签 $Y$ 的条件概率，表示为
    $p(Y|X)$。
- en: Generative models, such as the Stacked Denoising Auto-encoders (SDAE) have been
    proposed by Bengio et al. ([bengio2013generalized,](#bib.bib29) ) to identify
    the salient structure of input data distributions, and Hu et al. ([hu2016transfer,](#bib.bib30)
    ) used the same model for the pre-training phase before training a classifier
    for time series tasks. A universal neural network encoder has been developed to
    convert variable-length time series to a fixed-length representation ([serra2018towards,](#bib.bib31)
    ). Also, a Deep Belief Network (DBN) combined with a transfer learning method
    was used in an unsupervised manner to model the latent features of time series ([banerjee2019deep,](#bib.bib32)
    ). An Echo State Network (ESN) has been used to learn the appropriate time series
    representation by reconstructing the original raw time series prior to training
    the classifier ([aswolinskiy2018time,](#bib.bib33) ). Generative Adversarial Networks
    (GANs) are one of the popular generative models that generate new examples by
    learning to discriminate between real and synthetic examples. Various GANs have
    been developed for time series and have been reviewed in a recent survey ([GanSurvey2021,](#bib.bib34)
    ). Often, implementing generative methods is more complex due to an additional
    step of training. Furthermore, generative methods are typically less efficient
    than discriminative methods, which directly map raw time series to class probability
    distributions. Due to these barriers, researchers tend to focus on discriminative
    methods. Therefore, this survey mainly focuses on the end-to-end discriminative
    approaches.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型，如堆叠去噪自编码器（SDAE），由Bengio等人提出（[bengio2013generalized,](#bib.bib29)），用于识别输入数据分布的显著结构，Hu等人（[hu2016transfer,](#bib.bib30)）在为时间序列任务训练分类器之前，使用了相同的模型进行预训练。已经开发了一个通用神经网络编码器，将可变长度的时间序列转换为固定长度的表示（[serra2018towards,](#bib.bib31)）。此外，结合转移学习方法的深度置信网络（DBN）以无监督的方式建模时间序列的潜在特征（[banerjee2019deep,](#bib.bib32)）。回声状态网络（ESN）用于通过重建原始原始时间序列来学习适当的时间序列表示，然后训练分类器（[aswolinskiy2018time,](#bib.bib33)）。生成对抗网络（GANs）是流行的生成模型之一，通过学习区分真实和合成示例来生成新示例。各种GANs已被开发用于时间序列，并在最近的调查中进行了回顾（[GanSurvey2021,](#bib.bib34)）。通常，实现生成方法更复杂，因为它需要额外的训练步骤。此外，生成方法通常不如判别方法高效，后者直接将原始时间序列映射到类别概率分布。由于这些障碍，研究人员往往集中于判别方法。因此，本调查主要关注端到端的判别方法。
- en: 2.4\. Taxonomy of Deep Learning in TSC and TSER
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 深度学习在TSC和TSER中的分类
- en: '{forest}'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: 'for tree= parent anchor=children, child anchor=parent, anchor=north, draw,
    align=left, inner sep=1.75pt, rounded corners=2pt, font=, , where level=0s sep=10pt,fill=red!5!white!80!green!40,
    where level=1s sep=6pt,fill=red!60!white!80!yellow!40, where level=2s sep=2pt,fill=red!40!white!80!blue!40,
    where level=3s sep=2pt,fill=red!10!white!80!blue!40, where level=4fill=red!5!white!80!green!40,
    forked edges, [Deep Learning methods for Time Series Classification and Extrinsic
    Regression, [Supervised (Sec.[3](#S3 "3\. Supervised Models ‣ Deep Learning for
    Time Series Classification and Extrinsic Regression: A Current Survey")), name=S,
    s sep=6pt, [Multi-Layer'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树=父节点 锚点=子节点，子节点锚点=父节点，锚点=北，绘制，对齐=左，内边距=1.75pt，圆角=2pt，字体=， ，其中级别=0的间隔=10pt，填充=红色!5!白色!80!绿色!40，其中级别=1的间隔=6pt，填充=红色!60!白色!80!黄色!40，其中级别=2的间隔=2pt，填充=红色!40!白色!80!蓝色!40，其中级别=3的间隔=2pt，填充=红色!10!白色!80!蓝色!40，其中级别=4填充=红色!5!白色!80!绿色!40，分叉边，[时间序列分类和外部回归的深度学习方法，[监督（第[3](#S3
    "3\. 监督模型 ‣ 深度学习用于时间序列分类和外部回归：当前调查")节），名称=S，间隔=6pt，[多层
- en: Perceptron, name=MLP,rotate=0,anchor=north] [Convolutional
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机，名称=MLP，旋转=0，锚点=北] [卷积
- en: Neural Network, name=CNN, for tree=grow’=0,folder,draw, [Adapted
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，名称=CNN，对于树=生长’=0，文件夹，绘制，[适应的
- en: Convolutional
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积
- en: Neural Network, name=ACNN] [Imaging Time
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，名称=ACNN] [成像时间
- en: Series, name=ITS] [Multi-Scale
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 系列，名称=ITS] [多尺度
- en: Operation, name=MSO] ] [Recurrent
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 操作，名称=MSO] ] [递归
- en: Neural
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经
- en: Network, name=RNN,for tree=grow’=0,folder,draw, [Vanilla Recurrent
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 网络，名称=RNN，对于树=生长’=0，文件夹，绘制，[原始递归
- en: Neural Network, name=RRN] [Long Short Term Memory, name=LSTM] [Gated Recurrent
    Unit, name=GRU] [Hybrid, name=RCNN] ] [Graph Neural
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，名称=RRN] [长短期记忆，名称=LSTM] [门控递归单元，名称=GRU] [混合，名称=RCNN] ] [图神经
- en: Network, name=GNN,rotate=0,anchor=north] [Attention, name=Attn, for tree=grow’=0,folder,draw,
    [Self-Attention, name=SA] [Transformers, name=Trans] ] ] [Self-Supervised
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 网络，名称=GNN，旋转=0，锚点=北] [注意力，名称=Attn，对于树=生长’=0，文件夹，绘制，[自注意力，名称=SA] [变换器，名称=Trans]
    ] ] [自监督
- en: '(Sec.[4](#S4 "4\. Self-supervised Models ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")), name=SS, for tree=grow’=0,folder,draw,
    [Self-Prediction, name=SSCNN] [Contrastive-'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (Sec.[4](#S4 "4\. 自监督模型 ‣ 时间序列分类和外部回归的深度学习：现状调查")), name=SS, for tree=grow’=0,folder,draw,
    [自预测，name=SSCNN] [对比-
- en: Learning, name=SSAttn] [Other pretext
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 学习，name=SSAttn] [其他前提
- en: tasks, name=SSGNN, ] ] [Data
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 任务，name=SSGNN, ] ] [数据
- en: Augmentation
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 增强
- en: '(Sec.[5](#S5 "5\. Data augmentation ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")), name=DA, for tree=grow’=0,folder,draw,
    [Random'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (Sec.[5](#S5 "5\. 数据增强 ‣ 时间序列分类和外部回归的深度学习：现状调查")), name=DA, for tree=grow’=0,folder,draw,
    [随机
- en: Transformations, name=RT] [Window methods, name=WM] [Averaging methods, name=AM]
    ] [Transfer
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 变换，name=RT] [窗口方法，name=WM] [平均方法，name=AM] ] [迁移
- en: Learning
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 学习
- en: '(Sec.[6](#S6 "6\. Transfer learning ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")), name=TL,rotate=0,anchor=north ]
    ]'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (Sec.[6](#S6 "6\. 迁移学习 ‣ 时间序列分类和外部回归的深度学习：现状调查")), name=TL,rotate=0,anchor=north
    ] ]
- en: Figure 1\. Taxonomy of Deep Learning (DL) for TSC/TSER from the perspectives
    of network configuration and application domains.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 从网络配置和应用领域的角度来看，时间序列分类/外部回归的深度学习（DL）分类学。
- en: 'To provide an organized summary of the existing deep learning models for TSC,
    we propose a taxonomy that categorizes these models based on deep learning methods
    and application domains. This taxonomy is illustrated in Fig. [1](#S2.F1 "Figure
    1 ‣ 2.4\. Taxonomy of Deep Learning in TSC and TSER ‣ 2\. Background and Definitions
    ‣ Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey"). In section [3](#S3 "3\. Supervised Models ‣ Deep Learning for Time Series
    Classification and Extrinsic Regression: A Current Survey"), we review various
    network architectures used for TSC, including multilayer perceptrons, convolutional
    neural networks, recurrent neural networks, graph neural networks, and attention-based
    models. We also discuss refinements made to these models to improve their performance
    on time series tasks. Additionally, various types of self-supervised learning
    pretexts, such as contrastive learning and self-prediction, are explored in section
    [4](#S4 "4\. Self-supervised Models ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey"). We also conduct a review of useful
    data augmentation and transfer learning strategies for time series data in section [5](#S5
    "5\. Data augmentation ‣ Deep Learning for Time Series Classification and Extrinsic
    Regression: A Current Survey") and  [6](#S6 "6\. Transfer learning ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey"). In
    addition to methods, we summarize key applications of TSC and TSER in section
    [7](#S7 "7\. Applications - recent developments and challenges ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey") of
    this paper. These applications include human activity recognition and satellite
    earth observation, which are important and challenging tasks that can benefit
    from the use of deep learning models. Overall, our proposed taxonomy and the discussions
    in these sections provide a comprehensive overview of the current state of the
    art in deep learning for time series analysis and outline future research directions.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提供对现有时间序列分类（TSC）深度学习模型的有组织总结，我们提出了一种分类法，根据深度学习方法和应用领域对这些模型进行分类。该分类法在图 [1](#S2.F1
    "Figure 1 ‣ 2.4\. Taxonomy of Deep Learning in TSC and TSER ‣ 2\. Background and
    Definitions ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey")中进行了说明。在第 [3](#S3 "3\. Supervised Models ‣ Deep Learning for
    Time Series Classification and Extrinsic Regression: A Current Survey")节中，我们回顾了用于TSC的各种网络架构，包括多层感知器、卷积神经网络、递归神经网络、图神经网络以及基于注意力的模型。我们还讨论了对这些模型进行的改进，以提高它们在时间序列任务上的性能。此外，第
    [4](#S4 "4\. Self-supervised Models ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")节探讨了各种类型的自监督学习前置任务，如对比学习和自我预测。我们还在第
    [5](#S5 "5\. Data augmentation ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")和 [6](#S6 "6\. Transfer learning ‣
    Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey")节中回顾了时间序列数据的有用数据增强和迁移学习策略。除了方法，我们在第 [7](#S7 "7\. Applications - recent
    developments and challenges ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey")节总结了TSC和TSER的关键应用。这些应用包括人类活动识别和卫星地球观测，这些任务重要且具有挑战性，可以从深度学习模型的使用中受益。总体而言，我们提出的分类法以及这些章节中的讨论提供了关于时间序列分析深度学习的现状的全面概述，并概述了未来的研究方向。'
- en: 3\. Supervised Models
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 监督模型
- en: 'This section reviews the deep learning-based models for TSC and discusses their
    architectures by highlighting their strengths as well as limitations. More details
    on deep model architectures and their adaptations to time series data are available
    in Appendix [B](#A2 "Appendix B DNN Architectures for Time Series ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '本节回顾了基于深度学习的TSC模型，并通过突出其优缺点来讨论它们的架构。有关深度模型架构及其对时间序列数据的适应的更多细节，请参见附录 [B](#A2
    "Appendix B DNN Architectures for Time Series ‣ Deep Learning for Time Series
    Classification and Extrinsic Regression: A Current Survey")。'
- en: 3.1\. Multi-Layer Perceptron (MLP)
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 多层感知器（MLP）
- en: The most straightforward neural network architecture is a fully connected network
    (FC), also called a multilayer perceptron (MLP). The number of layers and neurons
    are defined as hyperparameters in MLP models. However, studies such as auto-adaptive
    MLP ([del2021auto,](#bib.bib35) ) have attempted to determine the number of neurons
    in the hidden layers automatically, based on the nature of the training time series
    data. This allows the network to adapt to the training data’s characteristics
    and optimize its performance on the task at hand.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的神经网络架构是全连接网络（FC），也称为多层感知器（MLP）。在MLP模型中，层数和神经元的数量被定义为超参数。然而，诸如自适应MLP ([del2021auto,](#bib.bib35))等研究已经尝试根据训练时间序列数据的性质自动确定隐藏层中的神经元数量。这使得网络能够适应训练数据的特性，并优化其在当前任务上的表现。
- en: One of the main limitations of using multilayer perceptrons (MLPs) for time
    series data is that they are not well-suited to capturing the temporal dependencies
    in this type of data. MLPs are feedforward networks that process input data in
    a fixed and predetermined order without considering the temporal relationships
    between the input values. Various studies used MLPs alongside other feature extractors
    like Dynamic Time Warping (DTW) to address this problem ([iwana2016robust,](#bib.bib36)
    ; [iwana2020dtw,](#bib.bib37) ). DTW-NN is a feedforward neural network that exploits
    DTW’s elastic matching ability to dynamically align a layer’s inputs to the weights
    instead of using a fixed and predetermined input-to-weight mapping. This weight
    alignment replaces the standard dot product within a neuron with DTW. In this
    way, the DTW-NN is able to tackle difficulties with time series recognition, such
    as temporal distortions and variable pattern length within a feedforward architecture ([iwana2020dtw,](#bib.bib37)
    ). Similarly, Symbolic Aggregate Approximation (SAX) is used to transform time
    series into a symbolic representation and produce sequences of words based on
    the symbolic representation ([tabassum2022time,](#bib.bib38) ). The symbolic time
    series-based words are later used as input for training a two-layer MLP for classification.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多层感知器（MLPs）处理时间序列数据的主要限制之一是它们不适合捕捉这种数据中的时间依赖性。MLP是前馈网络，以固定和预定的顺序处理输入数据，而不考虑输入值之间的时间关系。各种研究将MLP与其他特征提取器如动态时间规整（DTW）一起使用以解决这个问题 ([iwana2016robust,](#bib.bib36)
    ; [iwana2020dtw,](#bib.bib37))。DTW-NN是一个前馈神经网络，它利用DTW的弹性匹配能力，将层的输入动态对齐到权重，而不是使用固定的输入到权重的映射。这种权重对齐用DTW替代了神经元中的标准点积。通过这种方式，DTW-NN能够解决时间序列识别中的困难，如时间扭曲和前馈架构中的可变模式长度 ([iwana2020dtw,](#bib.bib37))。类似地，符号聚合近似（SAX）被用来将时间序列转换为符号表示，并基于符号表示生成单词序列 ([tabassum2022time,](#bib.bib38))。这些基于符号时间序列的单词随后被用作训练一个两层MLP进行分类的输入。
- en: Although the models mentioned above attempt to resolve the shortage of capturing
    temporal dependencies in MLP models, they still have other limitations on capturing
    time-invariant features ([wang2017time,](#bib.bib16) ). Additionally, MLP models
    do not have the ability to process input data in a hierarchical or multi-scale
    manner. Time series data often exhibits patterns and structures at different scales,
    such as long-term trends and short-term fluctuations. MLP models fail to capture
    these patterns, as they are only able to process input data in a single, fixed-length
    representation. In addition, MLPs may encounter difficulties when confronted with
    irregularly sampled time series data, where observations are not uniformly recorded
    in time. Many other deep learning models are better suited to handle time series
    data, such as recurrent neural networks (RNNs), convolutional neural networks
    (CNNs), and transformers, specifically designed to capture the temporal dependencies
    and patterns in time series data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述模型尝试解决MLP模型在捕捉时间依赖性方面的不足，但它们在捕捉时间不变特征方面仍然存在其他限制 ([wang2017time,](#bib.bib16))。此外，MLP模型无法以层次化或多尺度的方式处理输入数据。时间序列数据通常在不同尺度上展示模式和结构，例如长期趋势和短期波动。MLP模型无法捕捉这些模式，因为它们只能以单一的、固定长度的表示处理输入数据。此外，当面对不规则采样的时间序列数据时，MLP可能会遇到困难，其中观测数据并不是在均匀的时间间隔内记录的。许多其他深度学习模型更适合处理时间序列数据，例如递归神经网络（RNNs）、卷积神经网络（CNNs）和专门设计用于捕捉时间序列数据中的时间依赖性和模式的变换器。
- en: 3.2\. CNN based models
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于CNN的模型
- en: 'Several improvements have been made to CNN since the success of AlexNet in
    2012 ([krizhevsky2012imagenet,](#bib.bib39) ) such as using deeper networks, applying
    smaller and more efficient convolutional filters, adding pooling layers to reduce
    the dimensionality of the feature maps, and utilizing batch normalization to improve
    the stability of training ([gu2018recent,](#bib.bib40) ). They have been demonstrated
    to be very successful in many domains, such as computer vision, speech recognition,
    and natural language processing problems ([lecun2015deep,](#bib.bib41) ; [gu2018recent,](#bib.bib40)
    ). As a result of the success of CNN architectures in these various domains, researchers
    have also started adopting them for TSC. See table [1](#S3.T1 "Table 1 ‣ 3.2.2\.
    Imaging time series ‣ 3.2\. CNN based models ‣ 3\. Supervised Models ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey") for
    a list of reviewed CNN models in this paper.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '自2012年AlexNet取得成功以来，CNN已经进行了一些改进（[krizhevsky2012imagenet,](#bib.bib39)），例如使用更深的网络、应用更小且更高效的卷积滤波器、添加池化层以减少特征图的维度，以及利用批量归一化来提高训练的稳定性（[gu2018recent,](#bib.bib40)）。这些改进在许多领域，如计算机视觉、语音识别和自然语言处理问题中已经证明非常成功（[lecun2015deep,](#bib.bib41)；[gu2018recent,](#bib.bib40)）。由于CNN架构在这些不同领域的成功，研究人员也开始将其应用于时间序列分类（TSC）。请参见表[1](#S3.T1
    "Table 1 ‣ 3.2.2\. Imaging time series ‣ 3.2\. CNN based models ‣ 3\. Supervised
    Models ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey")，了解本文中评审的CNN模型列表。'
- en: 3.2.1\. Adapted CNNs for TSC and TSER
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 适应性CNN用于TSC和TSER
- en: This section presents the first category, which we refer to as Adapted CNNs
    for TSC and TSER. The papers discussed here are mostly adaptations without any
    particular preprocessing or mathematical characteristics, such as transforming
    the series to an image or using multi-scale convolution and therefore do not fit
    into one of the other categories.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍第一个类别，我们称之为针对TSC和TSER的适应性CNN。这里讨论的论文主要是一些没有特定预处理或数学特征的适应性改编，例如将系列转换为图像或使用多尺度卷积，因此不适合其他类别。
- en: The first CNN for TSC was the Multi-Channel Deep Convolutional Neural Network
    (MC-DCNN) ([zheng2014time,](#bib.bib42) ). It handles multivariate data by independently
    applying convolutions to each input channel. Each input dimension undergoes two
    convolutional stages with ReLU activation, followed by max pooling. The output
    from each dimension is concatenated and passed to a fully connected layer which
    is then fed to a final softmax classifier for classification. Similar to MC-DCNN,
    a three-layer convolution neural network was proposed for Human activity recognition
    (MC-CNN)([yang2015deep,](#bib.bib43) ). Unlike the MC-DCNN, this model applies
    1D convolutions to all input channels simultaneously to capture the temporal and
    spatial relationships in the early stages. The 2-stage version of MC-CNN architecture
    was used by Zhao et al. ([zhao2017convolutional,](#bib.bib44) ) on the earliest
    version of the UCR Time Series Data Mining Archive. The authors also conducted
    an ablation study to evaluate the performance of the CNN models with differing
    numbers of convolution filters and pooling types.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个针对TSC的CNN是多通道深度卷积神经网络（MC-DCNN）（[zheng2014time,](#bib.bib42)）。它通过独立地对每个输入通道应用卷积来处理多变量数据。每个输入维度经历两个卷积阶段，使用ReLU激活，然后进行最大池化。每个维度的输出被连接并传递给一个全连接层，然后输入最终的softmax分类器进行分类。类似于MC-DCNN，提出了一种用于人体活动识别的三层卷积神经网络（MC-CNN）（[yang2015deep,](#bib.bib43)）。与MC-DCNN不同，这个模型在早期阶段同时对所有输入通道应用1D卷积，以捕捉时间和空间关系。MC-CNN架构的2阶段版本被赵等人（[zhao2017convolutional,](#bib.bib44)）用于UCR时间序列数据挖掘档案的最早版本。作者还进行了消融研究，以评估具有不同数量卷积滤波器和池化类型的CNN模型的性能。
- en: Fully Convolutional Networks (FCN) ([long2015fully,](#bib.bib45) ), and Residual
    Networks (Resnet) ([he2016deep,](#bib.bib46) ) are two deep neural networks that
    are commonly used for image and video recognition tasks and have been adapted
    for end-to-end TSC ([wang2017time,](#bib.bib16) ). FCNs are a variant of convolutional
    neural networks (CNNs) designed to operate on inputs of arbitrary size rather
    than being constrained to fixed-size inputs like traditional CNNs. This is achieved
    by replacing the fully connected layers in a traditional CNN with a Global Average
    Pooling (GAP) ([long2015fully,](#bib.bib45) ). FCN was adapted for univariate
    TSC ([wang2017time,](#bib.bib16) ), and similar to the original model, it contains
    three convolution blocks where each block contains a convolution layer followed
    by batch normalization and ReLU activation. Each block uses 128, 256, 128 filters
    with 8, 5, 3 filter lengths respectively. The output from the last convolution
    block is averaged with a GAP layer and passed to a final softmax classifier. The
    GAP layer has the property of reducing the spatial dimensions of the input while
    retaining the channel-wise information, which allows it to be used in conjunction
    with a class activation map (CAM) ([zhou2016learning,](#bib.bib47) ) to highlight
    the regions in the input that are most important for the predicted class. This
    can provide useful insights into how the network is making its predictions and
    help identify potential improvement areas. Similar to FCN, the Residual Network
    (ResNet) was also proposed in ([wang2017time,](#bib.bib16) ) for univariate TSC.
    ResNet is a deep architecture containing three residual blocks followed by a GAP
    layer and a softmax classifier. It uses residual connections between blocks to
    reduce the vanishing gradient effect that affects deep learning models. The structure
    of each residual block is similar to the FCN architecture, containing three convolution
    layers followed by batch normalization and ReLU activation. Each convolution layer
    uses 64 filters with 8, 5, 3 filter lengths, respectively. ResNet was found to
    be one of the most accurate deep learning TSC architectures on 85 univariate TSC
    datasets ([fawaz2019deep,](#bib.bib15) ; [bagnall2017great,](#bib.bib25) ). Additionally,
    integration of ResNet and FCN has been proposed to combine the strength of both
    networks ([zou2019integration,](#bib.bib48) ).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 全
- en: In addition to adapting the network architecture, some research has focused
    on modifying the convolution kernel to suit TSC tasks better. Dilated convolutions
    neural networks (DCNNs) ([li2018csrnet,](#bib.bib49) ) are a type of CNN that
    uses dilated convolutions to increase the receptive field of the network without
    increasing the number of parameters. Dilated convolutions create gaps between
    elements of the kernel and perform convolution, thereby covering a larger area
    of the input. This allows the network to capture long-range dependencies in the
    data, making it well-suited to TSC tasks ([yazdanbakhsh2019multivariate,](#bib.bib50)
    ). Recently, Disjoint-CNN ([foumani2021disjoint,](#bib.bib51) ) showed that factorization
    of 1D convolution kernels into disjoint temporal and spatial components yields
    accuracy improvements with almost no additional computational cost. Applying disjoint
    temporal convolution and then spatial convolution behaves similarly to Inverted
    Bottleneck ([sandler2018mobilenetv2,](#bib.bib52) ). Like the Inverted Bottleneck,
    the temporal convolutions expand the number of input channels, and spatial convolutions
    later project the expanded hidden state back to the original size to capture the
    temporal and spatial interaction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调整网络架构外，一些研究还专注于修改卷积核以更好地适应TSC任务。膨胀卷积神经网络（DCNNs）([li2018csrnet,](#bib.bib49)
    ) 是一种使用膨胀卷积来增加网络接收场的CNN，但不增加参数数量的网络。膨胀卷积在卷积过程中在卷积核的元素之间创建间隙，从而覆盖输入的更大区域。这使得网络能够捕捉数据中的长距离依赖关系，使其非常适合TSC任务
    ([yazdanbakhsh2019multivariate,](#bib.bib50) )。最近，Disjoint-CNN ([foumani2021disjoint,](#bib.bib51)
    ) 表明将1D卷积核分解为不相交的时间和空间组件，可以提高准确性而几乎没有额外的计算成本。应用不相交的时间卷积然后进行空间卷积的效果类似于倒置瓶颈 ([sandler2018mobilenetv2,](#bib.bib52)
    )。与倒置瓶颈类似，时间卷积扩展了输入通道的数量，而空间卷积则将扩展的隐藏状态投影回原始大小，以捕捉时间和空间的交互关系。
- en: 3.2.2\. Imaging time series
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 成像时间序列
- en: In TSC, a common approach is to convert the time series data into a fixed-length
    representation, such as a vector or matrix, which can then be input to a deep
    learning model. However, this can be challenging for time series data that vary
    in length or have complex temporal dependencies. One solution to this problem
    is to represent the time series data in an image-like format, where each time
    step is treated as a separate channel in the image. This allows the model to learn
    from the spatial relationships within the data rather than just the temporal relationships.
    In this context, the term spatial refers to the relationships between different
    variables or features within a single time step of the time series.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在TSC中，一种常见的方法是将时间序列数据转换为固定长度的表示，如向量或矩阵，然后将其输入到深度学习模型中。然而，对于长度不同或具有复杂时间依赖关系的时间序列数据，这可能是具有挑战性的。解决这个问题的一种方法是将时间序列数据表示为类似图像的格式，其中每个时间步骤被视为图像中的一个单独通道。这使得模型能够从数据中的空间关系中学习，而不仅仅是时间关系。在这种情况下，空间一词指的是时间序列中单个时间步骤内不同变量或特征之间的关系。
- en: As an alternative to using raw time series data as input, Wang and Oates encoded
    univariate time series data into different types of images that were then processed
    by a regular CNN ([wang2015encoding,](#bib.bib53) ). This image-based framework
    initiated a new branch of deep learning approaches for time series, which consider
    image transformation as one of the feature engineering techniques. Wang and Oates
    presented two approaches for transforming a time series into an image. The first
    generates a Gramian Angular Field (GAF), while the second generates a Markov Transition
    Field (MTF). GAF represents time series data in a polar coordinate and uses various
    operations to convert these angles into a symmetry matrix and MTF encodes the
    matrix entries using the transition probability of a data point from one time
    step to another time step ([wang2015encoding,](#bib.bib53) ). In both cases, the
    image generation increases the time series size, making the images potentially
    prohibitively large. Therefore they propose strategies to reduce their size without
    losing too much information. Afterward, the two types of images are combined in
    a two-channel image that is then used to produce better results than those achieved
    when using each image separately. Finally, a Tiled CNN model is applied to classify
    the time-series images. In other studies, a variety of transformation methods,
    including Recurrence Plots (RP) ([hatami2018classification,](#bib.bib54) ), Gramian
    Angular Difference Field (GADF) ([karimi2018scalable,](#bib.bib55) ), bilinear
    interpolation ([zhao2019classify,](#bib.bib56) ), and Gramian Angular Summation
    Field (GASF) ([yang2019sensor,](#bib.bib57) ) have been proposed to transfer time
    series to input images expecting that the two-dimensional images could reveal
    features and patterns not found in the one-dimensional sequence of the original
    time series.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用原始时间序列数据作为输入的替代方案，王和奥茨将单变量时间序列数据编码成不同类型的图像，然后由常规卷积神经网络（[wang2015encoding](#bib.bib53)）处理。这个基于图像的框架开启了时间序列深度学习方法的新分支，将图像变换视为特征工程技术之一。王和奥茨提出了将时间序列转化为图像的两种方法。第一种生成一个Gramian
    Angular Field（GAF），而第二种生成一个Markov Transition Field（MTF）。GAF在极坐标系中表示时间序列数据，并使用各种操作将这些角度转换为对称矩阵，而MTF使用数据点从一个时间步到另一个时间步的转移概率对矩阵条目进行编码（[wang2015encoding](#bib.bib53)）。在这两种情况下，图像生成增加了时间序列的大小，使得图像可能变得过于庞大。因此，他们提出了在不丢失太多信息的情况下减少图像大小的策略。随后，将这两种图像合并为一个双通道图像，然后用以生成比单独使用每张图像时更好的结果。最后，应用一个Tiled
    CNN模型来对时间序列图像进行分类。在其他研究中，提出了多种转换方法，包括Recurrence Plots（RP）（[hatami2018classification](#bib.bib54)）、Gramian
    Angular Difference Field（GADF）（[karimi2018scalable](#bib.bib55)）、双线性插值（[zhao2019classify](#bib.bib56)）和Gramian
    Angular Summation Field（GASF）（[yang2019sensor](#bib.bib57)），旨在将时间序列转换为输入图像，以期望二维图像能够揭示在原始时间序列的一维序列中未发现的特征和模式。
- en: 'Hatami et al. ([hatami2018classification,](#bib.bib54) ) propose a representation
    method based on RP ([kamphorst1987recurrence,](#bib.bib58) ) to convert the time
    series to 2D images with a CNN model for TSC. In their study, time series are
    regarded as distinct recurrent behaviors such as periodicities and irregular cyclicities,
    which are the typical phenomena of dynamic systems. The main idea of using the
    RP method is to reveal at which points some trajectories return to a previous
    state. Finally, two-stage convolution and two fully connected layers are applied
    to classify the images generated by RP. Subsequently, pre-trained Inception v3 ([szegedy2016rethinking,](#bib.bib59)
    ) was used to map the GADF images into a 2048-dimensional vector space. The final
    stage used an MLP with three hidden layers, followed by a softmax activation function
     ([karimi2018scalable,](#bib.bib55) ). Following the same framework, Chen and
    Shi ([chen2019deep,](#bib.bib60) ) adopted the Relative Position Matrix and VGGNet
    (RPMCNN) to classify time series data using transform 2D images. Their results
    showed promising performances by converting univariate time series data to 2D
    images using relative positions between two time stamps. Following the convention,
    three image encoding methods: GASF, GADF, and MTF, were used to encode MTS data
    into two-dimensional images ([yang2019sensor,](#bib.bib57) ). They showed that
    the simple structure of ConvNet is sufficient for classification as it performed
    equally well with the complex structure of VGGNet.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hatami et al. ([hatami2018classification,](#bib.bib54) ) 提出了一种基于RP ([kamphorst1987recurrence,](#bib.bib58)
    )的表示方法，将时间序列转换为2D图像，并使用CNN模型进行时间序列分类。在他们的研究中，时间序列被视为不同的递归行为，如周期性和不规则的周期性，这些都是动态系统的典型现象。使用RP方法的主要思想是揭示某些轨迹在何处返回到先前的状态。最后，应用了两阶段卷积和两个全连接层来分类RP生成的图像。随后，使用预训练的Inception
    v3 ([szegedy2016rethinking,](#bib.bib59) )将GADF图像映射到2048维向量空间。最终阶段使用了一个包含三层隐藏层的MLP，后接softmax激活函数 ([karimi2018scalable,](#bib.bib55)
    )。按照相同的框架，陈和石 ([chen2019deep,](#bib.bib60) ) 采用了相对位置矩阵和VGGNet（RPMCNN）来使用转换2D图像对时间序列数据进行分类。他们的结果显示，通过将单变量时间序列数据转换为使用两个时间戳之间的相对位置的2D图像，取得了良好的效果。按照惯例，使用了三种图像编码方法：GASF、GADF和MTF，将多变量时间序列数据编码为二维图像 ([yang2019sensor,](#bib.bib57)
    )。结果表明，ConvNet的简单结构足以进行分类，因为其表现与VGGNet的复杂结构一样出色。
- en: Overall, representing time series data as 2D images can be difficult because
    preserving the temporal relationships and patterns in the data can be challenging.
    This transformation can also result in a loss of information, making it difficult
    for the model to classify the data accurately. Chen and Shi ([chen2019deep,](#bib.bib60)
    ) have also shown that the specific transformation methods like GASF, GADF, and
    MTF used in this process do not significantly improve the prediction outcome.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，将时间序列数据表示为2D图像可能很困难，因为在数据中保留时间关系和模式可能具有挑战性。这种转换也可能导致信息丢失，使得模型难以准确分类数据。陈和石 ([chen2019deep,](#bib.bib60)
    ) 也展示了在此过程中使用的GASF、GADF和MTF等特定转换方法并未显著改善预测结果。
- en: '| Model | Year | Baseline Architecture | Other features |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 基线架构 | 其他特性 |'
- en: '| Adapted |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 改编 |'
- en: '| MC-DCNN ([zheng2014time,](#bib.bib42) ) | 2014 | 2-Stage Conv | Independent
    convolutions per Channel |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| MC-DCNN ([zheng2014time,](#bib.bib42) ) | 2014 | 2阶段卷积 | 每个通道独立卷积 |'
- en: '| MC-CNN ([yang2015deep,](#bib.bib43) ) | 2015 | 3-Stage Conv | 1D-Convolutions
    on all Channel |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| MC-CNN ([yang2015deep,](#bib.bib43) ) | 2015 | 3阶段卷积 | 在所有通道上进行1D卷积 |'
- en: '| Zhao et al. ([zhao2017convolutional,](#bib.bib44) ) | 2015 | 2-Stage Conv
    | Similar architecture to MC-CNN |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. ([zhao2017convolutional,](#bib.bib44) ) | 2015 | 2阶段卷积 | 与MC-CNN结构相似
    |'
- en: '| FCN ([wang2017time,](#bib.bib16) ) | 2017 | FCN | Using GAP instead of FC
    Layer |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| FCN ([wang2017time,](#bib.bib16) ) | 2017 | FCN | 使用GAP代替FC层 |'
- en: '| ResNet ([wang2017time,](#bib.bib16) ) | 2017 | ResNet 9 | Using 3-Residual
    block |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ResNet ([wang2017time,](#bib.bib16) ) | 2017 | ResNet 9 | 使用3个残差块 |'
- en: '| Res-CNN ([zou2019integration,](#bib.bib48) ) | 2019 | RezNet+FCN | Using
    1-Residual block + FCN |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Res-CNN ([zou2019integration,](#bib.bib48) ) | 2019 | RezNet+FCN | 使用1个残差块+FCN
    |'
- en: '| DCNNs ([yazdanbakhsh2019multivariate,](#bib.bib50) ) | 2019 | 4-Stage Conv
    | Using dilated convolutions |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| DCNNs ([yazdanbakhsh2019multivariate,](#bib.bib50) ) | 2019 | 4阶段卷积 | 使用膨胀卷积
    |'
- en: '| Disjoint-CNN ([wang2017time,](#bib.bib16) ) | 2021 | 4-Stage Conv | Disjoint
    temporal and spatial convolution |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Disjoint-CNN ([wang2017time,](#bib.bib16) ) | 2021 | 4阶段卷积 | 不同的时间和空间卷积 |'
- en: '| Series To Image |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 序列到图像 |'
- en: '| Wang&Oates([wang2015encoding,](#bib.bib53) ) | 2015 | Tiled CNN | GAF, MT
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Wang&Oates([wang2015encoding,](#bib.bib53) ) | 2015 | 瓦片CNN | GAF, MT |'
- en: '| Hatami et al.([hatami2018classification,](#bib.bib54) ) | 2017 | 2-Stage
    Conv | Recurrence Plots |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Hatami et al.([hatami2018classification,](#bib.bib54) ) | 2017 | 2级卷积 | 递归图
    |'
- en: '| Karimi et al.([karimi2018scalable,](#bib.bib55) ) | 2018 | Inception V3 |
    GADF |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Karimi et al.([karimi2018scalable,](#bib.bib55) ) | 2018 | Inception V3 |
    GADF |'
- en: '| Zhao et al. ([zhao2019classify,](#bib.bib56) ) | 2019 | ResNet18, ShuffleNet
    V2 | Bilinear interpolation |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. ([zhao2019classify,](#bib.bib56) ) | 2019 | ResNet18, ShuffleNet
    V2 | 双线性插值 |'
- en: '| RPMCNN ([chen2019deep,](#bib.bib60) ) | 2019 | VGGNet, 2-Stage Conv | Relative
    Position Matrix |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| RPMCNN ([chen2019deep,](#bib.bib60) ) | 2019 | VGGNet, 2级卷积 | 相对位置矩阵 |'
- en: '| Yang et al. ([yang2019sensor,](#bib.bib57) ) | 2019 | VGGNet | GASF, GADF,
    MTF |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. ([yang2019sensor,](#bib.bib57) ) | 2019 | VGGNet | GASF, GADF,
    MTF |'
- en: '| Multi-Scale Operation |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 多尺度操作 |'
- en: '| MCNN ([cui2016multi,](#bib.bib61) ) | 2016 | 2-Stage Conv | Identity mapping,
    Smoothing, Down-sampling |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MCNN ([cui2016multi,](#bib.bib61) ) | 2016 | 2级卷积 | 恒等映射、平滑、降采样 |'
- en: '| t-LeNet ([le2016data,](#bib.bib62) ) | 2016 | 2-Stage Conv | Squeeze and
    Dilation |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| t-LeNet ([le2016data,](#bib.bib62) ) | 2016 | 2级卷积 | 压缩和扩张 |'
- en: '| MVCNN ([liu2018time,](#bib.bib63) ) | 2019 | 4-stage Conv | Inception V1
    based |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MVCNN ([liu2018time,](#bib.bib63) ) | 2019 | 4级卷积 | 基于Inception V1 |'
- en: '| Brunel et al. ([brunel2019cnn,](#bib.bib64) ) | 2019 | Inception V1 |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Brunel et al. ([brunel2019cnn,](#bib.bib64) ) | 2019 | Inception V1 |  |'
- en: '| InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) ) | 2019 | Inception
    V4 | Ensemble |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) ) | 2019 | Inception
    V4 | 集成 |'
- en: '| EEG-inception ([sun2021prototypical,](#bib.bib65) ) | 2021 | InceptionTime
    |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| EEG-inception ([sun2021prototypical,](#bib.bib65) ) | 2021 | InceptionTime
    |  |'
- en: '| Inception-FCN ([usmankhujaev2021time,](#bib.bib66) ) | 2021 | InceptionTime
    + FCN |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Inception-FCN ([usmankhujaev2021time,](#bib.bib66) ) | 2021 | InceptionTime
    + FCN |  |'
- en: '| KDCTime ([gong2022kdctime,](#bib.bib67) ) | 2022 | InceptionTime | Knowledge
    Distillation, Label smoothing |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| KDCTime ([gong2022kdctime,](#bib.bib67) ) | 2022 | InceptionTime | 知识蒸馏、标签平滑
    |'
- en: '| LITE ([ismail2023lite,](#bib.bib68) ) | 2023 | InceptionTime | Multiplexing,
    dilated, and custom filters |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| LITE ([ismail2023lite,](#bib.bib68) ) | 2023 | InceptionTime | 多路复用、扩张和自定义滤波器
    |'
- en: Table 1\. Summary of CNN models for time series classification and extrinsic
    regression
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 时间序列分类和外部回归的CNN模型总结
- en: 3.2.3\. Multi-Scale Operation
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 多尺度操作
- en: The papers discussed here apply a multi-scale convolutional kernel to the input
    series or apply regular convolutions on the input series at different scales.
    Multi-scale CNNs (MCNN) ([cui2016multi,](#bib.bib61) ) and Time LeNet (t-LeNet) ([le2016data,](#bib.bib62)
    ) were considered the first models that preprocess the input series to apply convolution
    on multi-scale series rather than raw series. The design of both MCNNs and t-LeNet
    were inspired by computer vision models, which means that they were adapted from
    models originally developed for image recognition tasks. These models may not
    be well-suited to TSC tasks and may not perform as well as models specifically
    designed for this purpose. One potential reason for this is the use of progressive
    pooling layers in these models, commonly used in computer vision models, to reduce
    the input data size and make it easier to process. However, these pooling layers
    may not be as effective when applied to time series data and may limit the performance
    of the model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的论文应用了多尺度卷积核来处理输入序列或在不同尺度上对输入序列进行常规卷积。多尺度CNN（MCNN） ([cui2016multi,](#bib.bib61)
    )和时间LeNet（t-LeNet） ([le2016data,](#bib.bib62) )被认为是首批将输入序列预处理以应用多尺度卷积而非原始序列的模型。MCNN和t-LeNet的设计灵感来自计算机视觉模型，这意味着它们是从最初为图像识别任务开发的模型中改编而来的。这些模型可能不太适合TSC任务，性能可能不如专门为此目的设计的模型。一个潜在的原因是这些模型中使用的渐进池化层，这在计算机视觉模型中很常见，用于减少输入数据的大小并使其更易处理。然而，当这些池化层应用于时间序列数据时，效果可能不如预期，可能会限制模型的性能。
- en: 'MCNN has simple architecture and comprises two convolutions and a pooling layer,
    followed by a fully connected and softmax layer. However, this approach involves
    heavy data preprocessing. Specifically, before any training, they use a sliding
    window to extract a time series subsequence, and later, the subsequence will undergo
    three transformations: (1) identity mapping, (2) down-sampling, and (3) smoothing,
    which results in the transformation of a univariate input time series into a multivariate
    one. Finally, the transformed output is fed to the CNN model to train a classifier ([cui2016multi,](#bib.bib61)
    ). t-LeNet uses two data augmentation techniques: window slicing (WS) and window
    warping (WW), to prevent overfitting ([le2016data,](#bib.bib62) ). The WS method
    is identical to MCNN’s data augmentation. The second data augmentation technique,
    WW, employs a warping technique that squeezes or dilates the time series. WS is
    also adopted to ensure that subsequences of the same length are extracted for
    training the network to deal with multi-length time series. Therefore, a given
    input time series of length $L$ is first dilated $(\times 2)$ and then squeezed
    $(\times 1/2)$ using WW, resulting in three time series of length $L,2L,1/2L$
    that are fed to WS to extract equal length subsequences for training. Finally,
    as both MCNN and t-LeNet predict a class for each extracted subsequence, majority
    voting is applied to obtain the class prediction for the full time series.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: MCNN具有简单的架构，包括两个卷积层和一个池化层，之后是一个全连接层和一个softmax层。然而，这种方法涉及大量的数据预处理。具体来说，在任何训练之前，他们使用滑动窗口提取时间序列子序列，随后，这些子序列将经历三种转换：（1）身份映射，（2）下采样，以及（3）平滑，这导致将单变量输入时间序列转换为多变量序列。最后，转换后的输出被输入到CNN模型中以训练分类器
    ([cui2016multi,](#bib.bib61) )。t-LeNet使用两种数据增强技术：窗口切割（WS）和窗口扭曲（WW），以防止过拟合 ([le2016data,](#bib.bib62)
    )。WS方法与MCNN的数据增强相同。第二种数据增强技术WW，使用一种扭曲技术来压缩或扩展时间序列。WS还用于确保提取的子序列具有相同的长度，以便训练网络处理不同长度的时间序列。因此，给定的长度为$L$的输入时间序列首先通过WW扩展
    $(\times 2)$，然后通过WW压缩 $(\times 1/2)$，生成三条长度为$L,2L,1/2L$的时间序列，这些时间序列被输入到WS中以提取相等长度的子序列进行训练。最后，由于MCNN和t-LeNet都对每个提取的子序列进行分类预测，因此采用多数投票来获得整个时间序列的类别预测。
- en: Inception was first proposed by Szegedy et al. ([szegedy2015going,](#bib.bib69)
    ) for end-to-end image classification. Now the network has evolved to become Inception-v4,
    where Inception was coupled with residual connections to improve further the performance ([szegedy2017inception,](#bib.bib70)
    ). Inspired by inception architecture, a multivariate convolutional neural network
    (MVCNN) is designed using multi-scale convolution kernels to find the optimal
    local construction ([liu2018time,](#bib.bib63) ). MVCNN uses three scales of filters,
    including $2\times 2$, $3\times 3$, and $5\times 5$, to extract features of the
    interaction between sensors. A one-dimensional Inception model was used for Supernovae
    classification using the light flux of a region in space as an input MTS for the
    network ([brunel2019cnn,](#bib.bib64) ). However, the authors limited the conception
    of their Inception architecture to the first version of this model ([szegedy2015going,](#bib.bib69)
    ). The Inception-ResNet ([ronald2021isplinception,](#bib.bib71) ) architecture
    includes convolutional layers, followed by Inception modules and residual blocks.
    The Inception modules are used to learn multiple scales and aspects of the data,
    allowing the network to capture more complex patterns. The residual blocks are
    then used to learn the residuals, or differences, between the input and output
    of the network, improving its performance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Inception最早由Szegedy等人提出 ([szegedy2015going,](#bib.bib69) ) 用于端到端的图像分类。现在，该网络已经发展成为Inception-v4，其中Inception与残差连接结合，以进一步提高性能
    ([szegedy2017inception,](#bib.bib70) )。受到Inception架构的启发，设计了一个多变量卷积神经网络（MVCNN），使用多尺度卷积核来寻找最佳局部构造
    ([liu2018time,](#bib.bib63) )。MVCNN使用三种尺度的滤波器，包括$2\times 2$、$3\times 3$和$5\times
    5$，来提取传感器之间的交互特征。一维Inception模型被用于超新星分类，利用空间区域的光流作为网络的输入MTS ([brunel2019cnn,](#bib.bib64)
    )。然而，作者将他们的Inception架构的概念限制在这个模型的第一个版本 ([szegedy2015going,](#bib.bib69) )。Inception-ResNet
    ([ronald2021isplinception,](#bib.bib71) )架构包括卷积层，接着是Inception模块和残差块。Inception模块用于学习数据的多种尺度和方面，使网络能够捕捉到更复杂的模式。残差块用于学习网络输入和输出之间的残差或差异，从而提高其性能。
- en: InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) ) explores much larger
    filters than any previously proposed network for TSC to reach state-of-the-art
    performance on the UCR benchmark. InceptionTime is an ensemble of five randomly
    initialized inception network models, each of which consists of two blocks of
    inception modules. Each inception module first reduces the dimensionality of a
    multivariate time series using a bottleneck layer with a length and stride of
    1 while maintaining the same length. Then, 1D convolutions of different lengths
    are applied to the output of the bottleneck layer to extract patterns at different
    sizes. In parallel, a max pooling layer followed by a bottleneck layer are also
    applied to the original time series to increase the robustness of the model to
    small perturbations. The output from the convolution and max pooling layers are
    stacked to form a new multivariate time series which is then passed to the next
    layer. Residual connections are used between each inception block to reduce the
    vanishing gradient effect. The output of the second inception block is passed
    to a GAP layer before feeding into a softmax classifier.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: InceptionTime ([fawaz2020inceptiontime,](#bib.bib12) ) 探索了比任何先前提出的网络更大的滤波器，以在
    UCR 基准上达到最先进的性能。InceptionTime 是五个随机初始化的 inception 网络模型的集成，每个模型由两个 inception 模块块组成。每个
    inception 模块首先使用长度和步幅为 1 的瓶颈层减少多变量时间序列的维度，同时保持相同的长度。然后，对瓶颈层的输出应用不同长度的 1D 卷积，以提取不同大小的模式。与此同时，对原始时间序列应用一个最大池化层，之后是一个瓶颈层，以提高模型对小扰动的鲁棒性。卷积层和最大池化层的输出被堆叠形成一个新的多变量时间序列，然后传递到下一层。每个
    inception 块之间使用残差连接以减少梯度消失效应。第二个 inception 块的输出被传递到 GAP 层，然后输入到 softmax 分类器中。
- en: 'The strong performance of InceptionTime has inspired a number of extensions.
    Like InceptionTime, EEG-inception ([sun2021prototypical,](#bib.bib65) ) uses several
    inception layers and residual connections as its backbone. Additionally, noise
    addition-based data augmentation of EEG signals is proposed, which increases the
    average accuracy. InceptionFCN ([usmankhujaev2021time,](#bib.bib66) ) focuses
    on combining two well-known deep learning techniques, namely the Inception module
    and the Fully Convolutional Network ([usmankhujaev2021time,](#bib.bib66) ). In
    KDCTime ([gong2022kdctime,](#bib.bib67) ), label smoothing (LSTime) and knowledge
    distillation (KDTime) were introduced for InceptionTime, automatically generated
    while compressing the inference model. Additionally, knowledge distillation with
    calibration (KDC) in KDCTime offers two calibrating strategies: KDC by translating
    (KDCT) and KDC by reordering (KDCR). LITE ([ismail2023lite,](#bib.bib68) ) addresses
    InceptionTime’s complexity while preserving its TSC performance. Utilizing DepthWise
    Separable Convolutions, LITE incorporates multiplexing, dilated convolution, and
    custom filters ([ismail2022deep,](#bib.bib72) ) to enhance efficiency.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: InceptionTime 的强大性能激发了许多扩展。像 InceptionTime 一样，EEG-inception ([sun2021prototypical,](#bib.bib65)
    ) 使用几个 inception 层和残差连接作为其骨干。此外，提出了基于噪声添加的数据增强 EEG 信号的方法，这提高了平均准确性。InceptionFCN ([usmankhujaev2021time,](#bib.bib66)
    ) 专注于结合两个著名的深度学习技术，即 Inception 模块和全卷积网络 ([usmankhujaev2021time,](#bib.bib66) )。在
    KDCTime ([gong2022kdctime,](#bib.bib67) ) 中，引入了标签平滑（LSTime）和知识蒸馏（KDTime），这些在压缩推理模型时自动生成。此外，KDCTime
    中的带校准的知识蒸馏（KDC）提供了两种校准策略：通过翻译的 KDC（KDCT）和通过重新排序的 KDC（KDCR）。LITE ([ismail2023lite,](#bib.bib68)
    ) 解决了 InceptionTime 的复杂性，同时保持了其 TSC 性能。利用 DepthWise 可分离卷积，LITE 结合了多路复用、扩张卷积和自定义滤波器 ([ismail2022deep,](#bib.bib72)
    ) 以提高效率。
- en: 3.3\. Recurrent Neural Network
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 递归神经网络
- en: Recurrent Neural Networks are types of neural networks built with internal memory
    to work with time series and sequential data. Conceptually similar to feed-forward
    neural networks (FFNs), RNNs differ in their ability to handle variable-length
    inputs and produce variable-length outputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络是内置内存的神经网络类型，用于处理时间序列和顺序数据。概念上类似于前馈神经网络（FFNs），RNNs 在处理可变长度输入和生成可变长度输出方面有所不同。
- en: 3.3.1\. Vanilla Recurrent Neural Networks (Vanilla RNNs)
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 原始递归神经网络（Vanilla RNNs）
- en: Recurrent neural networks for TSC have been proposed in ([husken2003recurrent,](#bib.bib73)
    ). Using RNNs, the input series have been classified based on their dynamic behavior.
    They used sequence-to-sequence architecture in which each sub-series of input
    series is classified in the first step. Then the argmax function is applied to
    the entire output, and finally, the neuron with the highest rate specifies the
    classification result. In order to improve the model parallelization and capacity ([dennis2019shallow,](#bib.bib74)
    ) proposed a two-layer RNN. In the first layer, the input sequence is split into
    several independent RNNs to improve parallelization, followed by a second layer
    that utilizes the first layer’s output to capture long-term dependencies ([dennis2019shallow,](#bib.bib74)
    ). Further, RNNs have been used in some hierarchical architectures ([fernandez2007sequence,](#bib.bib75)
    ; [hermans2013training,](#bib.bib76) ). Hermans and Schrauwen showed a deeper
    version of recurrent neural networks could perform hierarchical processing on
    complex temporal tasks and capture the time series structure more naturally than
    a shallow version ([hermans2013training,](#bib.bib76) ). RNNs are usually trained
    iteratively using a procedure known as backpropagation through time (BPTT). When
    unfolded in time, RNNs look like very deep networks with shared parameters. With
    deeper neural layers in RNN and sharing weights across different RNN cells, the
    gradients are summed up at each time step to train the model. Thus, gradients
    undergo continuous matrix multiplication due to the chain rule and either shrink
    exponentially and have small values called vanishing gradients or blow up to a
    very large value, referred to as exploding gradients ([pascanu2013difficulty,](#bib.bib77)
    ). These problems motivated the development of second-order methods for deep architectures
    named long short-term memory (LSTM) ([hochreiter1997long,](#bib.bib78) ) and Gated
    Recurrent Unit (GRU) ([chung2014empirical,](#bib.bib79) ).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 针对时间序列分类（TSC）已经提出了递归神经网络（RNN）的方案（[husken2003recurrent](#bib.bib73)）。利用RNN，输入序列根据其动态行为进行了分类。他们使用了序列到序列的架构，在该架构中，输入序列的每个子序列在第一步被分类。然后将`argmax`函数应用于整个输出，最终，具有最高值的神经元指定了分类结果。为了提高模型的并行化和容量，[dennis2019shallow](#bib.bib74)
    提出了一个两层RNN。在第一层中，输入序列被拆分为多个独立的RNN，以提高并行化，然后第二层利用第一层的输出捕捉长期依赖关系（[dennis2019shallow](#bib.bib74)）。此外，RNN也被用于一些层次结构（[fernandez2007sequence](#bib.bib75)；[hermans2013training](#bib.bib76)）。Hermans和Schrauwen展示了更深版本的递归神经网络能够对复杂的时间任务进行层次处理，并比浅层版本更自然地捕捉时间序列结构（[hermans2013training](#bib.bib76)）。RNN通常通过一种称为时间反向传播（BPTT）的过程进行迭代训练。当在时间上展开时，RNN看起来像是具有共享参数的非常深的网络。在RNN的更深神经层中，由于链式法则，梯度在每个时间步上被求和以训练模型。因此，梯度由于链式法则而经历连续的矩阵乘法，要么指数级地缩小并具有小值，称为消失梯度，要么膨胀到非常大的值，称为爆炸梯度（[pascanu2013difficulty](#bib.bib77)）。这些问题促使了名为长短期记忆（LSTM）（[hochreiter1997long](#bib.bib78)）和门控递归单元（GRU）（[chung2014empirical](#bib.bib79)）的深度架构的二阶方法的发展。
- en: 3.3.2\. Long Short Term Memory (LSTM)
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 长短期记忆网络（LSTM）
- en: LSTM addresses the common vanishing/exploding gradient issue in vanilla RNNs
    by integrating memory cells with gate control into their state dynamics ([hochreiter1997long,](#bib.bib78)
    ). Due to its design nature, LSTM is suited to problems involving sequence data,
    such as language translation ([sutskever2014sequence,](#bib.bib80) ), video representation
    learning ([donahue2015long,](#bib.bib81) ), and image caption generation ([karpathy2015deep,](#bib.bib82)
    ). The TSC problem is not an exception and mainly adopts a similar model to the
    language translation ([sutskever2014sequence,](#bib.bib80) ). Sequence-to-Sequence
    with Attention (S2SwA) ([tang2016sequence,](#bib.bib83) ) incorporates two LSTMs,
    one encoder and one decoder, in a sequence-to-sequence fashion for TSC. In this
    model, the encoder LSTM accepts input time series of arbitrary lengths and extracts
    information from the raw data based on which the decoder LSTM constructs fixed-length
    sequences that can be regarded as automatically extracted features for classification.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM通过将记忆单元和门控控制集成到状态动态中，解决了普通RNN中的常见梯度消失/爆炸问题 ([hochreiter1997long,](#bib.bib78)
    )。由于其设计特性，LSTM适用于涉及序列数据的问题，如语言翻译 ([sutskever2014sequence,](#bib.bib80) )、视频表示学习 ([donahue2015long,](#bib.bib81)
    ) 和图像描述生成 ([karpathy2015deep,](#bib.bib82) )。TSC问题也不例外，主要采用类似于语言翻译的模型 ([sutskever2014sequence,](#bib.bib80)
    )。序列到序列模型与注意力机制（S2SwA） ([tang2016sequence,](#bib.bib83) )在TSC中包含两个LSTM，一个编码器和一个解码器，以序列到序列的方式工作。在这个模型中，编码器LSTM接受任意长度的输入时间序列，并从原始数据中提取信息，基于这些信息解码器LSTM构建固定长度的序列，这些序列可以视为用于分类的自动提取特征。
- en: 3.3.3\. Gated Recurrent Unit (GRU)
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. 门控循环单元（GRU）
- en: GRU, another widely-used variant of RNNs, shares similarities with LSTM in its
    ability to control information flow and memorize context across multiple time
    steps ([chung2014empirical,](#bib.bib79) ). Similar to S2SwA ([tang2016sequence,](#bib.bib83)
    ) sequence auto-encoder (SAE) based on GRU has been defined to deal with TSC problem ([malhotra2017timenet,](#bib.bib84)
    ). A fixed-size output is produced by processing the various input lengths using
    GRU as the encoder and decoder. The model’s accuracy was also improved by pre-training
    the parameters on massive unlabeled data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GRU是另一种广泛使用的RNN变体，其在控制信息流和记忆跨多个时间步的上下文方面与LSTM相似 ([chung2014empirical,](#bib.bib79)
    )。类似于S2SwA ([tang2016sequence,](#bib.bib83) )，基于GRU的序列自动编码器（SAE）也被定义用来处理TSC问题 ([malhotra2017timenet,](#bib.bib84)
    )。通过使用GRU作为编码器和解码器处理各种输入长度，从而生成固定大小的输出。通过在大量未标记数据上预训练参数，模型的准确性也得到了提高。
- en: 3.3.4\. Hybrid Models
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. 混合模型
- en: CNN’s and RNNs are often combined for TSC because they have complementary strengths.
    As mentioned previously, CNNs are well-suited for learning from spatial relationships
    in data, such as the patterns and correlations between the channels of different
    time steps in a time series. This allows them to learn useful features from the
    time series data that can help improve the classification performance. RNNs, on
    the other hand, are well-suited for learning from temporal dependencies in data,
    such as the past values of a time series that can help predict its future values.
    This allows them to capture the dynamic nature of time series data and make more
    accurate predictions. Combining the strengths of CNNs and RNNs makes it possible
    to learn spatial and temporal features from the time series data, improving the
    model’s performance for TSC. Additionally, the two models can be trained together,
    allowing them to learn from each other and improve the model’s overall performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: CNN和RNN经常结合使用于TSC，因为它们具有互补的优势。如前所述，CNN非常适合从数据中的空间关系中学习，例如时间序列中不同时间步之间的通道模式和相关性。这使得它们能够从时间序列数据中学习有用的特征，从而帮助提高分类性能。另一方面，RNN非常适合从数据中的时间依赖性中学习，例如时间序列的过去值，这些值可以帮助预测未来值。这使得它们能够捕捉时间序列数据的动态特性，并做出更准确的预测。结合CNN和RNN的优势，使得可以从时间序列数据中学习空间和时间特征，从而提高TSC模型的性能。此外，这两种模型可以一起训练，使它们能够相互学习，提高模型的整体性能。
- en: Various extensions like MLSTM-FCN ([karim2019multivariate,](#bib.bib85) ), TapNet ([zhang2020tapnet,](#bib.bib86)
    ), and SMATE ([zuo2021smate,](#bib.bib87) ) were proposed later to deal with time-series
    data. MLSTM-FCN extends the univariate LSTM-FCN model ([karim2017lstm,](#bib.bib88)
    ) to the multivariate case. Like the LSTM-FCN, the multivariate version comprises
    LSTM blocks and fully convolutional blocks for extracting features from input
    series. A squeeze and excite block is also added to the FCN block, and can execute
    a form of self-attention on the output feature maps of previous layers ([karim2019multivariate,](#bib.bib85)
    ). Two further proposals for multivariate TSC are the Time series attentional
    prototype Network (TapNet) and Semi-Supervised Spatio-Temporal (SMATE)  ([zhang2020tapnet,](#bib.bib86)
    ; [zuo2021smate,](#bib.bib87) ). These methods combine and seek to leverage the
    relative strengths of both traditional distance-based and deep-learning approaches.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 后来提出了各种扩展，如 MLSTM-FCN ([karim2019multivariate,](#bib.bib85) )、TapNet ([zhang2020tapnet,](#bib.bib86)
    ) 和 SMATE ([zuo2021smate,](#bib.bib87) ) 来处理时间序列数据。MLSTM-FCN 将单变量 LSTM-FCN 模型 ([karim2017lstm,](#bib.bib88)
    ) 扩展到多变量情况。与 LSTM-FCN 类似，多变量版本包含 LSTM 块和完全卷积块，用于从输入序列中提取特征。FCN 块中还添加了一个 squeeze
    和 excite 块，可以对前面层的输出特征图执行自注意力形式 ([karim2019multivariate,](#bib.bib85) )。另外两个多变量
    TSC 的提案是时间序列注意力原型网络（TapNet）和半监督时空（SMATE） ([zhang2020tapnet,](#bib.bib86) ; [zuo2021smate,](#bib.bib87)
    )。这些方法结合了传统基于距离的方法和深度学习方法的相对优势。
- en: MLSTM-FCN, TapNet, and SMATE were designed in dual-network architectures. The
    input is separately fed into the CNN and RNN models, and their output is concentrated
    before the fully connected layer for the final task. However, one branch cannot
    fully use the hidden states of the other during feature extraction since the final
    classification results are generated by concatenating the outputs of the two branches.
    That motivates different types of architecture that try layer-wise integration
    of CNN and RNN models. This motivates different architectures, such as GCRNN ([lin2017gcrnn,](#bib.bib89)
    ) and CNN-LSTM ([mutegeki2020cnn,](#bib.bib90) ), which aim to integrate CNNs
    and RNNs in a layer-wise fashion.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: MLSTM-FCN、TapNet 和 SMATE 采用了双网络架构。输入被分别送入 CNN 和 RNN 模型，然后在最终任务的全连接层之前进行集中处理。然而，由于最终分类结果是通过连接两个分支的输出生成的，一个分支无法完全利用另一个分支的隐藏状态进行特征提取。这激发了不同类型的架构，尝试逐层整合
    CNN 和 RNN 模型。这催生了不同的架构，例如 GCRNN ([lin2017gcrnn,](#bib.bib89) ) 和 CNN-LSTM ([mutegeki2020cnn,](#bib.bib90)
    )，旨在逐层整合 CNN 和 RNN。
- en: 'While recurrent neural networks are commonly used for time series forecasting,
    only a few studies have applied them to TSC, mainly due to four reasons: (1) RNNs
    typically struggle with the gradient vanishing and exploding problem due to training
    on long-time series ([pascanu2012understanding,](#bib.bib91) ). (2) RNNs are considered
    difficult to train and parallelize, so researchers are less likely to use them
    as they are computationally expensive ([pascanu2013difficulty,](#bib.bib77) ).
    (3) Recurrent architectures are designed mainly to learn from the previous data
    to make predictions about the future ([langkvist2014review,](#bib.bib28) ). (4)
    RNN models can fail to effectively capture and utilize long-range dependencies
    in long sequences ([tang2016sequence,](#bib.bib83) ).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然递归神经网络通常用于时间序列预测，但仅有少数研究将其应用于 TSC，主要由于以下四个原因：（1）RNN 通常面临梯度消失和爆炸问题，因其在长时间序列上进行训练 ([pascanu2012understanding,](#bib.bib91)
    )。（2）RNN 被认为难以训练和并行化，因此研究人员不太可能使用它们，因为它们计算成本高 ([pascanu2013difficulty,](#bib.bib77)
    )。（3）递归架构主要是从以前的数据中学习以对未来进行预测 ([langkvist2014review,](#bib.bib28) )。（4）RNN 模型可能无法有效捕捉和利用长序列中的长期依赖关系 ([tang2016sequence,](#bib.bib83)
    )。
- en: 3.4\. Attention based model
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 基于注意力的模型
- en: 'Despite the excellent performance of CNN models for capturing local temporal/spatial
    correlations, these models can not effectively capture and utilize long-range
    dependencies. Additionally, they only consider the local order of data points
    rather than the overall order of all data points. Therefore, many recent studies
    have embedded recurrent neural networks (RNN) such as LSTMs alongside the CNNs
    to capture this information ([karim2017lstm,](#bib.bib88) ; [karim2019multivariate,](#bib.bib85)
    ; [zhang2020tapnet,](#bib.bib86) ). The disadvantage of RNN-based models is that
    they are computationally expensive, and their capability to capture long-range
    dependencies is limited ([vaswani2017attention,](#bib.bib92) ; [hao2020new,](#bib.bib18)
    ). On the other hand, attention models can capture long-range dependencies, and
    their broader receptive fields provide more contextual information, which can
    improve the models’ learning capacity. The attention mechanism aims to enhance
    a network’s representation ability by focusing on essential features and suppressing
    unnecessary ones. Not surprisingly, with the success of attention models in natural
    language processing ([vaswani2017attention,](#bib.bib92) ; [devlin2018bert,](#bib.bib93)
    ), many previous studies have attempted to bring the power of attention models
    into various domains such as computer vision ([dosovitskiy2020image,](#bib.bib94)
    ) and time series analysis ([hao2020new,](#bib.bib18) ; [li2019enhancing,](#bib.bib95)
    ; [zhou2021informer,](#bib.bib96) ; [zerveas2021transformer,](#bib.bib19) ; [kostas2021bendr,](#bib.bib97)
    ). Table [2](#S3.T2 "Table 2 ‣ 3.4.2\. Transformers ‣ 3.4\. Attention based model
    ‣ 3\. Supervised Models ‣ Deep Learning for Time Series Classification and Extrinsic
    Regression: A Current Survey") presents a list of the attention-based models reviewed
    in this paper.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNN模型在捕获局部时空相关性方面表现出色，但这些模型无法有效捕获和利用长程依赖关系。此外，它们只考虑数据点的局部顺序，而不是所有数据点的整体顺序。因此，许多最近的研究都嵌入了循环神经网络（RNN），如LSTMs，以捕获这些信息（[karim2017lstm,](#bib.bib88)
    ; [karim2019multivariate,](#bib.bib85) ; [zhang2020tapnet,](#bib.bib86) ）。基于RNN的模型的缺点是计算开销大，并且它们捕获长程依赖性的能力受到限制（[vaswani2017attention,](#bib.bib92)
    ; [hao2020new,](#bib.bib18)）。另一方面，注意力模型可以捕获长程依赖性，并且它们更广泛的感受野提供了更多的上下文信息，这可以提高模型的学习能力。注意力机制旨在通过聚焦于关键特征并抑制不必要的特征来增强网络的表示能力。不足为奇的是，在注意力模型在自然语言处理中取得成功后（[vaswani2017attention,](#bib.bib92)
    ; [devlin2018bert,](#bib.bib93)），许多先前的研究尝试将注意力模型的能力引入到各种领域，如计算机视觉（[dosovitskiy2020image,](#bib.bib94)）和时间序列分析（[hao2020new,](#bib.bib18)
    ; [li2019enhancing,](#bib.bib95) ; [zhou2021informer,](#bib.bib96) ; [zerveas2021transformer,](#bib.bib19)
    ; [kostas2021bendr,](#bib.bib97)）。表[2](#S3.T2 "Table 2 ‣ 3.4.2\. Transformers
    ‣ 3.4\. 基于注意力的模型 ‣ 3\. 监督模型 ‣ 时间序列分类和外在回归的深度学习：当前调查")列出了本文中审查的基于注意力的模型的列表。
- en: 3.4.1\. Self-Attention
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 自注意力
- en: Self-attention has been demonstrated to be effective in various natural language
    processing tasks due to its ability to capture long-term dependencies in text ([vaswani2017attention,](#bib.bib92)
    ). Recently, it has also been shown to be effective for TSC tasks ([hao2020new,](#bib.bib18)
    ; [yuan2018muvan,](#bib.bib98) ; [hsieh2021explainable,](#bib.bib99) ; [chen2021multi,](#bib.bib100)
    ). As we mentioned, the self-attention module is embedded in the encoder-decoder
    models to improve the model performance. However, only the encoder and the self-attention
    module have been used for TSC. Early models of TSC follow the same backbone of
    natural language processing models and use the Recurrent-based models such as
    RNN ([yuan2018novel,](#bib.bib101) ), GRU([yuan2018muvan,](#bib.bib98) ) and LSTM([liang2018geoman,](#bib.bib102)
    ; [hu2020multistage,](#bib.bib103) ) for encoding the input series. For example,
    the Multi-View Attention Network (MuVAN) applies bidirectional GRUs independently
    to each input dimension as the encoder and then feeds all the representations
    into a self-attention bock ([yuan2018muvan,](#bib.bib98) ).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力由于能够捕捉文本中的长期依赖，在各种自然语言处理任务中已被证明有效 ([vaswani2017attention,](#bib.bib92) )。最近，它在
    TSC 任务中也被证明是有效的 ([hao2020new,](#bib.bib18) ; [yuan2018muvan,](#bib.bib98) ; [hsieh2021explainable,](#bib.bib99)
    ; [chen2021multi,](#bib.bib100) )。正如我们所提到的，自注意力模块被嵌入到编码器-解码器模型中以提高模型性能。然而，TSC
    任务中只使用了编码器和自注意力模块。早期的 TSC 模型遵循与自然语言处理模型相同的骨干结构，使用基于递归的模型，如 RNN ([yuan2018novel,](#bib.bib101)
    ), GRU([yuan2018muvan,](#bib.bib98) ) 和 LSTM([liang2018geoman,](#bib.bib102) ;
    [hu2020multistage,](#bib.bib103) ) 对输入序列进行编码。例如，Multi-View Attention Network (MuVAN)
    将双向 GRU 独立应用于每个输入维度作为编码器，然后将所有表示输入到自注意力块中 ([yuan2018muvan,](#bib.bib98) )。
- en: As a result of the excellent performance of the CNN models, many studies have
    attempted to encode the time series using CNN before applying attention ([hao2020new,](#bib.bib18)
    ; [hsieh2021explainable,](#bib.bib99) ; [cheng2020novel,](#bib.bib104) ; [xiao2021rtfn,](#bib.bib105)
    ). Cross Attention Stabilized Fully Convolutional Neural Network (CA-SFCN) ([hao2020new,](#bib.bib18)
    ) and Locality Aware eXplainable Convolutional ATtention network (LAXCAT) ([hsieh2021explainable,](#bib.bib99)
    ) applied the self-attention mechanism to leverage the long-term dependencies
    for the MTSC task. CA-SFCN combines FCN and two types of self-attention - temporal
    attention (TA) and variable attention (VA), which interact to capture the long-range
    dependencies and variables interactions. LAXCAT also used temporal and variable
    attention to identify informative variables and the time intervals where they
    have informative patterns for classification. WaveletDTW Hybrid attEntion Networks
    (WHEN) ([wang2023wavelet,](#bib.bib106) ) integrate two attention mechanisms,
    namely wavelet attention and DTW attention, into the BiLSTM to enhance model performance.
    In wavelet attention, they leverage wavelets to compute attention scores, specifically
    targeting the analysis of dynamic frequency components in nonstationary time series.
    Simultaneously, DTW attention employs the DTW distance to calculate attention
    scores, addressing the challenge of time distortion in multiple time series.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CNN 模型的出色表现，许多研究尝试在应用注意力机制之前使用 CNN 对时间序列进行编码 ([hao2020new,](#bib.bib18) ;
    [hsieh2021explainable,](#bib.bib99) ; [cheng2020novel,](#bib.bib104) ; [xiao2021rtfn,](#bib.bib105)
    )。Cross Attention Stabilized Fully Convolutional Neural Network (CA-SFCN) ([hao2020new,](#bib.bib18)
    ) 和 Locality Aware eXplainable Convolutional ATtention network (LAXCAT) ([hsieh2021explainable,](#bib.bib99)
    ) 将自注意力机制应用于 MTSC 任务，以利用长期依赖。CA-SFCN 结合了 FCN 和两种类型的自注意力——时间注意力 (TA) 和变量注意力 (VA)，它们相互作用以捕捉长期依赖和变量交互。LAXCAT
    也使用了时间注意力和变量注意力，以识别信息量大的变量及其在分类中有用的时间间隔。WaveletDTW Hybrid attEntion Networks (WHEN) ([wang2023wavelet,](#bib.bib106)
    ) 将两种注意力机制，即小波注意力和 DTW 注意力，集成到 BiLSTM 中，以增强模型性能。在小波注意力中，他们利用小波计算注意力分数，特别是针对非平稳时间序列中动态频率成分的分析。同时，DTW
    注意力使用 DTW 距离来计算注意力分数，解决了多时间序列中时间失真的问题。
- en: Several self-attention models have been developed to improve network performance ([jaderberg2015spatial,](#bib.bib107)
    ; [woo2018cbam,](#bib.bib108) ), including Squeeze-and-Excitation (SE) ([hu2018squeeze,](#bib.bib109)
    ), which focuses on channel attention and is often used to classify time series
    data ([karim2019multivariate,](#bib.bib85) ; [chen2021multi,](#bib.bib100) ; [wang2021time,](#bib.bib110)
    ). The SE block allows the whole network to use global information to selectively
    focus on the informative feature maps and suppress less important ones ([hu2018squeeze,](#bib.bib109)
    ). More importantly, the SE block can increase the quality of the shared lower-level
    representations in the early layers and becomes increasingly specialized when
    responding to different inputs in later layers. The weight of each feature map
    is automatically learned at each layer of the network, and the SE block can boost
    feature discrimination throughout the whole network. Multi-scale Attention Convolutional
    Neural Network (MACNN) ([chen2021multi,](#bib.bib100) ) applies the different
    kernel size convolutions to capture different scales of information along the
    time axis by generating feature maps at differing scales. Then an SE block is
    used to enhance useful feature maps and suppress less useful ones by automatically
    learning each feature map’s importance.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高网络性能，已经开发了几种自注意力模型（[jaderberg2015spatial,](#bib.bib107) ; [woo2018cbam,](#bib.bib108)
    ），包括专注于通道注意力的Squeeze-and-Excitation (SE) ([hu2018squeeze,](#bib.bib109) ），这通常用于对时间序列数据进行分类（[karim2019multivariate,](#bib.bib85)
    ; [chen2021multi,](#bib.bib100) ; [wang2021time,](#bib.bib110) ）。SE块使整个网络能够利用全局信息，有选择地关注信息丰富的特征图，并抑制不那么重要的特征图（[hu2018squeeze,](#bib.bib109)
    ）。更重要的是，SE块可以提高早期层中共享低级表示的质量，并在后期层对不同输入响应时变得越来越专业。每个特征图的权重在网络的每一层中自动学习，SE块可以提升整个网络中的特征辨别能力。多尺度注意力卷积神经网络（MACNN） ([chen2021multi,](#bib.bib100)
    )应用不同核大小的卷积来捕捉沿时间轴的不同尺度信息，通过生成不同尺度的特征图。然后使用SE块来增强有用的特征图，并通过自动学习每个特征图的重要性来抑制不太有用的特征图。
- en: 3.4.2\. Transformers
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. 变换器
- en: The impressive performance of multi-headed attention has led to numerous attempts
    to adapt multi-headed attention to the TSC domain. Transformers for classification
    usually employ a simple encoder structure consisting of attention and feed-forward
    layers. SAnD (Simply Attend and Diagnose) ([song2018attend,](#bib.bib111) ) architecture
    adopted a multi-head attention mechanism similar to a vanilla transformer ([vaswani2017attention,](#bib.bib92)
    ) to classify clinical time series for the first time. The model uses both positional
    encoding and a dense interpolation embedding technique to incorporate temporal
    order into representation learning. In another study that classified vibration
    signals ([jin2021end,](#bib.bib112) ), time-frequency features such as Frequency
    Coefficients and Short Time Fourier Transformation (STFT) spectrums are used as
    input embeddings to the transformers. A multi-head attention-based model was applied
    to raw optical satellite TSC using Gaussian Process Interpolation ([Rasmussen2004,](#bib.bib113)
    ) embedding and outperformed convolution, and recurrent neural networks ([allam2021paying,](#bib.bib114)
    ).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力的显著性能促使了大量将多头注意力适应于时间序列分类（TSC）领域的尝试。用于分类的变换器通常采用由注意力和前馈层组成的简单编码器结构。SAnD（Simply
    Attend and Diagnose） ([song2018attend,](#bib.bib111) )架构采用了一种类似于原始变换器的多头注意力机制（[vaswani2017attention,](#bib.bib92)
    ），首次用于临床时间序列分类。该模型使用位置编码和稠密插值嵌入技术将时间顺序融入到表示学习中。在另一个研究中（[jin2021end,](#bib.bib112)
    ），对振动信号的分类中，使用了时间频率特征，如频率系数和短时傅里叶变换（STFT）谱，作为变换器的输入嵌入。一个基于多头注意力的模型被应用于原始光学卫星TSC，使用高斯过程插值（[Rasmussen2004,](#bib.bib113)
    ）嵌入，并且优于卷积和递归神经网络（[allam2021paying,](#bib.bib114) ）。
- en: Gated Transformer Networks (GTN) ([liu2021gated,](#bib.bib115) ) use two-tower
    multi-headed attention to capture the discriminative information from the input
    series. Also, they merged the output of two towers using a learnable matrix named
    gating. To enhance locality awareness of transformers for TSC, flexible multi-head
    linear attention (FMLA) ([zhao2022rethinking,](#bib.bib116) ) integrates deformable
    convolutional blocks and online knowledge distillation, as well as a random mask
    to reduce noise. For each TSC dataset, AutoTransformer searches for the suitable
    network architecture using the neural architecture search (NAS) algorithm before
    feeding the output to the multi-headed attention blocks. ConvTran ([Foumani2023,](#bib.bib13)
    ) currently stands as the state of the art in multivariate TSC. They conducted
    a review of existing absolute and relative position encoding methods in TSC. Based
    on the limitations of the current position encodings for time series, they introduced
    two novel ones named tAPE and eRPE for absolute and relative positions, respectively.
    Integrating these proposed position encodings into a transformer block and combining
    them with a convolution layer, they presented a novel deep-learning framework
    for multivariate time series classification—ConvTran.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Gated Transformer Networks (GTN) ([liu2021gated,](#bib.bib115) ) 使用双塔多头注意力捕捉输入序列中的判别信息。此外，它们使用一个可学习的矩阵命名为
    gating 来合并两个塔的输出。为了增强变换器对时间序列分类的局部感知，灵活的多头线性注意力 (FMLA) ([zhao2022rethinking,](#bib.bib116)
    ) 集成了可变形卷积块和在线知识蒸馏，以及随机掩码以减少噪声。对于每个时间序列分类数据集，AutoTransformer 在将输出送入多头注意力块之前使用神经架构搜索
    (NAS) 算法搜索合适的网络架构。ConvTran ([Foumani2023,](#bib.bib13) ) 目前在多变量时间序列分类中处于**最先进**水平。他们对现有的绝对和相对位置编码方法进行了综述。基于当前位置编码在时间序列中的限制，他们分别引入了两个新的编码方式，称为
    tAPE 和 eRPE，用于绝对位置和相对位置。将这些提出的位置编码集成到变换器块中，并与卷积层结合，他们提出了一种新的深度学习框架用于多变量时间序列分类—ConvTran。
- en: '| Model | Year | Embedding | Attention |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 嵌入 | 注意力 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| MuVAN ([yuan2018muvan,](#bib.bib98) ) | 2018 | Bi-GRU | Self-attention |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| MuVAN ([yuan2018muvan,](#bib.bib98) ) | 2018 | Bi-GRU | 自注意力 |'
- en: '| ChannelAtt ([yuan2018novel,](#bib.bib101) ) | 2018 | RNN | Self-attention
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| ChannelAtt ([yuan2018novel,](#bib.bib101) ) | 2018 | RNN | 自注意力 |'
- en: '| GeoMAN ([liang2018geoman,](#bib.bib102) ) | 2018 | LSTM | Self-attention
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| GeoMAN ([liang2018geoman,](#bib.bib102) ) | 2018 | LSTM | 自注意力 |'
- en: '| Multi-Stage-Att ([hu2020multistage,](#bib.bib103) ) | 2020 | LSTM | Self-attention
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Multi-Stage-Att ([hu2020multistage,](#bib.bib103) ) | 2020 | LSTM | 自注意力
    |'
- en: '| CT_CAM ([cheng2020novel,](#bib.bib104) ) | 2020 | FCN + Bi-GRU | Self-attention
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| CT_CAM ([cheng2020novel,](#bib.bib104) ) | 2020 | FCN + Bi-GRU | 自注意力 |'
- en: '| CA-SFCN ([hao2020new,](#bib.bib18) ) | 2020 | FCN | Self-attention |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| CA-SFCN ([hao2020new,](#bib.bib18) ) | 2020 | FCN | 自注意力 |'
- en: '| RTFN ([xiao2021rtfn,](#bib.bib105) ) | 2021 | CNN + LSTM | Self-attention
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| RTFN ([xiao2021rtfn,](#bib.bib105) ) | 2021 | CNN + LSTM | 自注意力 |'
- en: '| LAXCAT ([hsieh2021explainable,](#bib.bib99) ) | 2021 | CNN | Self-attention
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| LAXCAT ([hsieh2021explainable,](#bib.bib99) ) | 2021 | CNN | 自注意力 |'
- en: '| MACNN ([chen2021multi,](#bib.bib100) ) | 2021 | Multi-scale CNN | Squeeze-and-Excitation
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| MACNN ([chen2021multi,](#bib.bib100) ) | 2021 | 多尺度 CNN | Squeeze-and-Excitation
    |'
- en: '| WHEN ([wang2023wavelet,](#bib.bib106) ) | 2023 | CNN + BiLSTM | Self-attention
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| WHEN ([wang2023wavelet,](#bib.bib106) ) | 2023 | CNN + BiLSTM | 自注意力 |'
- en: '| SAnD ([song2018attend,](#bib.bib111) ) | 2018 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SAnD ([song2018attend,](#bib.bib111) ) | 2018 |'
- en: '&#124; Linear Embedding &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 线性嵌入 &#124;'
- en: '| Multi-Head |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 多头 |'
- en: '| T2 ([allam2021paying,](#bib.bib114) ) | 2021 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| T2 ([allam2021paying,](#bib.bib114) ) | 2021 |'
- en: '&#124; Gaussian Process Regression + 1D Conv &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高斯过程回归 + 1D 卷积 &#124;'
- en: '| Multi-Head |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 多头 |'
- en: '| GTN ([liu2021gated,](#bib.bib115) ) | 2021 | Linear Embedding | Multi-Head
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| GTN ([liu2021gated,](#bib.bib115) ) | 2021 | 线性嵌入 | 多头 |'
- en: '| TRANS_tf ([jin2021end,](#bib.bib112) ) | 2021 | time-frequency features |
    Multi-Head |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| TRANS_tf ([jin2021end,](#bib.bib112) ) | 2021 | 时间-频率特征 | 多头 |'
- en: '| FMLA ([zhao2022rethinking,](#bib.bib116) ) | 2022 | Deformable CNN | Multi-Head
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| FMLA ([zhao2022rethinking,](#bib.bib116) ) | 2022 | 可变形 CNN | 多头 |'
- en: '| AutoTransformer ([ren2022autotransformer,](#bib.bib117) ) | 2022 | Multi-scale
    CNN + NAS | Multi-Head |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| AutoTransformer ([ren2022autotransformer,](#bib.bib117) ) | 2022 | 多尺度 CNN
    + NAS | 多头 |'
- en: '| ConvTran ([Foumani2023,](#bib.bib13) ) | 2023 | Disjoint-CNN | Multi-Head
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| ConvTran ([Foumani2023,](#bib.bib13) ) | 2023 | 不相交的 CNN | 多头 |'
- en: Table 2\. Summary of Attention-based Models for Time Series Classification and
    Extrinsic Regression
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 基于注意力的时间序列分类与外部回归模型总结
- en: 3.5\. Graph Neural Networks
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 图神经网络
- en: While both CNNs and RNNs perform well on Euclidean data, many time series problems
    have data that are more naturally represented as graphs ([Jin2023graph,](#bib.bib118)
    ). For example, in a network of sensors, the sensors may be irregularly spaced,
    instead of the sensors forming a regular grid. A graph representation of data
    collected by this network can model this irregular layout more accurately than
    can be done using a Euclidean space. However, using standard deep learning algorithms
    to learn from graph structures is challenging ([Wu2021graph,](#bib.bib119) ).
    For example, nodes may have a varying number of neighbouring nodes, making it
    difficult to apply a convolution operation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CNNs 和 RNNs 在欧几里得数据上表现良好，但许多时间序列问题中的数据更自然地表示为图形 ([Jin2023graph,](#bib.bib118)
    )。例如，在传感器网络中，传感器可能不规则分布，而不是形成一个规则的网格。通过该网络收集的数据的图形表示比使用欧几里得空间更准确地建模这种不规则布局。然而，使用标准深度学习算法从图结构中学习具有挑战性 ([Wu2021graph,](#bib.bib119)
    )。例如，节点可能具有不同数量的邻近节点，这使得应用卷积操作变得困难。
- en: 'Graph Neural Networks (GNNs) ([Scarselli2009graph,](#bib.bib120) ) are methods
    that adapt deep learning techniques to the graph domain. Much of the early research
    using GNNs for time series analysis concentrated on forecasting tasks ([Jin2023graph,](#bib.bib118)
    ). However, recent works consider GNNs for TSC ([Xi2023graph,](#bib.bib121) ;
    [Liu2023graph,](#bib.bib122) ) and TSER ([Bloemheuvel2023graph,](#bib.bib123)
    ) tasks. A list of the GNN models reviewed in this paper is provided in table
    [3](#S3.T3 "Table 3 ‣ 3.5\. Graph Neural Networks ‣ 3\. Supervised Models ‣ Deep
    Learning for Time Series Classification and Extrinsic Regression: A Current Survey").
    Time2Graph+ ([Cheng2021graph,](#bib.bib124) ) transforms each time series into
    a shapelet graph. Shapelets are extracted from the time series and form the graph
    nodes. The graph edges are weighted based on transition probabilities between
    the two shapelets. Once the input graphs have been constructed, a graph attention
    network is used to create a representation of the time series that is fed into
    a classifier.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络 (GNNs) ([Scarselli2009graph,](#bib.bib120) ) 是将深度学习技术适配于图领域的方法。早期使用 GNNs
    进行时间序列分析的研究主要集中在预测任务上 ([Jin2023graph,](#bib.bib118) )。然而，近期的研究开始考虑 GNNs 在 TSC ([Xi2023graph,](#bib.bib121)
    ; [Liu2023graph,](#bib.bib122) ) 和 TSER ([Bloemheuvel2023graph,](#bib.bib123)
    ) 任务中的应用。本文评审的 GNN 模型列表见表 [3](#S3.T3 "表 3 ‣ 3.5\. 图神经网络 ‣ 3\. 监督模型 ‣ 深度学习在时间序列分类和外部回归中的当前调查")。Time2Graph+ ([Cheng2021graph,](#bib.bib124)
    ) 将每个时间序列转换为形状图。形状从时间序列中提取，并形成图节点。图的边缘基于两个形状之间的转移概率加权。一旦构建了输入图，就使用图注意力网络创建时间序列的表示，然后输入到分类器中。
- en: '| Model | Year | GNN Type | Other Components |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | GNN 类型 | 其他组件 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| TGCN ([covert2019temporal,](#bib.bib125) ) | 2019 | Graph convolutional network
    | 1D-CNN |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| TGCN ([covert2019temporal,](#bib.bib125) ) | 2019 | 图卷积网络 | 1D-CNN |'
- en: '| DGCNN ([Song2020graph,](#bib.bib126) ) | 2020 | Graph convolutional network
    | 1x1 CNN |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN ([Song2020graph,](#bib.bib126) ) | 2020 | 图卷积网络 | 1x1 CNN |'
- en: '| GraphSleepNet ([jia2020graphsleepnet,](#bib.bib127) ) | 2020 | Graph convolutional
    network | Temporal attention |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| GraphSleepNet ([jia2020graphsleepnet,](#bib.bib127) ) | 2020 | 图卷积网络 | 时间注意力
    |'
- en: '| T-GCN ([ma2021deep,](#bib.bib128) ) | 2021 | Graph convolutional network
    | GRU |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| T-GCN ([ma2021deep,](#bib.bib128) ) | 2021 | 图卷积网络 | GRU |'
- en: '| MRF-GCN ([li2020multireceptive,](#bib.bib129) ) | 2021 | Graph convolutional
    network | Fast Fourier Transforms (FFT) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| MRF-GCN ([li2020multireceptive,](#bib.bib129) ) | 2021 | 图卷积网络 | 快速傅里叶变换
    (FFT) |'
- en: '| Nhu et al. ([nhu2021graph,](#bib.bib130) ) | 2021 | Graph convolutional network
    | 1D-CNN |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Nhu et al. ([nhu2021graph,](#bib.bib130) ) | 2021 | 图卷积网络 | 1D-CNN |'
- en: '| DCRNN ([Tang2021graph,](#bib.bib131) ) | 2021 | Graph convolutional network
    | GRU |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DCRNN ([Tang2021graph,](#bib.bib131) ) | 2021 | 图卷积网络 | GRU |'
- en: '| Time2Graph+ ([Cheng2021graph,](#bib.bib124) ) | 2021 | Graph attention |
    Shapelet transform |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Time2Graph+ ([Cheng2021graph,](#bib.bib124) ) | 2021 | 图注意力 | 形状变换 |'
- en: '| RAINDROP ([Zhang2021graph,](#bib.bib132) ) | 2021 | Graph guided network
    | Temporal attention |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| RAINDROP ([Zhang2021graph,](#bib.bib132) ) | 2021 | 图导向网络 | 时间注意力 |'
- en: '| STEGON ([Censi2021graph,](#bib.bib133) ) | 2021 | Graph attention | 1D-CNN
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| STEGON ([Censi2021graph,](#bib.bib133) ) | 2021 | 图注意力 | 1D-CNN |'
- en: '| Azevedo et al. ([Azevedo2022graph,](#bib.bib134) ) | 2022 | Graph network
    block with pooling | 1D-CNN |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Azevedo et al. ([Azevedo2022graph,](#bib.bib134) ) | 2022 | 图网络块与池化 | 1D-CNN
    |'
- en: '| MTPool ([Duan2022graph,](#bib.bib135) ) | 2022 | Variational Graph Pooling
    | 1D-CNN |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| MTPool ([Duan2022graph,](#bib.bib135) ) | 2022 | 变分图池化 | 1D-CNN |'
- en: '| SimTSC ([Zha2022graph,](#bib.bib136) ) | 2022 | Graph convolutional network
    | DTW, ResNet |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| SimTSC ([Zha2022graph,](#bib.bib136) ) | 2022 | 图卷积网络 | DTW, ResNet |'
- en: '| Tulczyjew et al. ([Tulczyjew2022graph,](#bib.bib137) ) | 2022 | Graph convolutional
    network | Adaptive pooling |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Tulczyjew 等人 ([Tulczyjew2022graph,](#bib.bib137) ) | 2022 | 图卷积网络 | 自适应池化
    |'
- en: '| C-DGAM ([Sun2023graph,](#bib.bib138) ) | 2023 | Graph attention | 1D-CNN
    with attention |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| C-DGAM ([Sun2023graph,](#bib.bib138) ) | 2023 | 图注意力 | 带注意力的 1D-CNN |'
- en: '| Dufourg et al. ([Dufourg2023graph,](#bib.bib139) ) | 2023 | Spatio-temporal
    graph | Simple Linear Iterative Clustering |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Dufourg 等人 ([Dufourg2023graph,](#bib.bib139) ) | 2023 | 时空图 | 简单线性迭代聚类 |'
- en: '| TISER-GCN ([Bloemheuvel2023graph,](#bib.bib123) ) | 2023 | Graph convolutional
    network | 1D-CNN |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| TISER-GCN ([Bloemheuvel2023graph,](#bib.bib123) ) | 2023 | 图卷积网络 | 1D-CNN
    |'
- en: '| TodyNet ([Liu2023graph,](#bib.bib122) ) | 2023 | Dynamic graph neural network
    | 1D-CNN |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| TodyNet ([Liu2023graph,](#bib.bib122) ) | 2023 | 动态图神经网络 | 1D-CNN |'
- en: '| LB-SimTSC ([Xi2023graph,](#bib.bib121) ) | 2023 | Graph convolutional network
    | Lower-bound DTW, ResNet |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| LB-SimTSC ([Xi2023graph,](#bib.bib121) ) | 2023 | 图卷积网络 | 下界 DTW, ResNet
    |'
- en: Table 3\. Summary of graph neural network models for time series classification
    and extrinsic regression
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 时间序列分类和外部回归的图神经网络模型总结
- en: SimTSC ([Zha2022graph,](#bib.bib136) ) constructs a pairwise similarity graph
    where each time series forms a node and edge weights are computed based on the
    DTW distance measure. Node attributes are generated using a feature vector encoder.
    GNN operations are used to enhance the node features based on similarities between
    adjacent time series. These representations are then used for the final classification
    step, which produces a classification for each node. LB-SimTSC ([Xi2023graph,](#bib.bib121)
    ) replaces the expensive DTW computation with the LB-Keogh lower-bounding method ([Keogh2005exact,](#bib.bib140)
    ).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: SimTSC ([Zha2022graph,](#bib.bib136) ) 构建了一个成对相似性图，其中每个时间序列形成一个节点，边的权重基于 DTW
    距离度量计算。节点属性通过特征向量编码器生成。使用 GNN 操作根据相邻时间序列之间的相似性来增强节点特征。这些表示随后用于最终分类步骤，为每个节点生成分类。LB-SimTSC ([Xi2023graph,](#bib.bib121)
    ) 用 LB-Keogh 下界方法 ([Keogh2005exact,](#bib.bib140) ) 替代了昂贵的 DTW 计算。
- en: Spatiotemporal GNNs model both spatial (or inter-variable) and temporal dependencies
    using two modules that work in tandem. The spatial module models the dependencies
    between the time series by applying graph convolutions over a GNN (graph convolutional
    networks or GCNs ([Kipf2016graph,](#bib.bib141) )). The temporal module models
    the dependencies within the time series using an RNN ([ma2021deep,](#bib.bib128)
    ; [Tang2021graph,](#bib.bib131) ), 1D-CNN ([Censi2021graph,](#bib.bib133) ; [Azevedo2022graph,](#bib.bib134)
    ), Attention ([Zhang2021graph,](#bib.bib132) ; [Sun2023graph,](#bib.bib138) ),
    or a combination of these ([Jin2023graph,](#bib.bib118) ). The features extracted
    from the graph layers are then fed into the classification or regression layers
    to make either a single prediction ([Tang2021graph,](#bib.bib131) ; [Azevedo2022graph,](#bib.bib134)
    ; [Zhang2021graph,](#bib.bib132) ; [Sun2023graph,](#bib.bib138) ) or a prediction
    for each node ([ma2021deep,](#bib.bib128) ; [Censi2021graph,](#bib.bib133) ).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 时空 GNN 通过两个模块同时建模空间（或变量间）和时间依赖性。空间模块通过对 GNN（图卷积网络或 GCNs ([Kipf2016graph,](#bib.bib141)
    )）应用图卷积来建模时间序列之间的依赖关系。时间模块使用 RNN ([ma2021deep,](#bib.bib128) ; [Tang2021graph,](#bib.bib131)
    ), 1D-CNN ([Censi2021graph,](#bib.bib133) ; [Azevedo2022graph,](#bib.bib134) ),
    Attention ([Zhang2021graph,](#bib.bib132) ; [Sun2023graph,](#bib.bib138) )，或这些的组合 ([Jin2023graph,](#bib.bib118)
    ) 来建模时间序列内的依赖性。然后，将从图层中提取的特征输入到分类或回归层，以生成单一预测 ([Tang2021graph,](#bib.bib131) ;
    [Azevedo2022graph,](#bib.bib134) ; [Zhang2021graph,](#bib.bib132) ; [Sun2023graph,](#bib.bib138)
    ) 或每个节点的预测 ([ma2021deep,](#bib.bib128) ; [Censi2021graph,](#bib.bib133) )。
- en: Spatiotemporal GCNs are often used to analyse sensor arrays, where the graph
    structure models the physical layout of the sensors. A common example is electroencephalogram
    (EEG) data, where the location of EEG electrodes is represented as a graph that
    is used to analyse the EEG signal. Some of these applications are epilepsy detection ([nhu2021graph,](#bib.bib130)
    ), seizure detection ([covert2019temporal,](#bib.bib125) ; [Tang2021graph,](#bib.bib131)
    ), emotion recognition ([Song2020graph,](#bib.bib126) ), and sleep classification ([jia2020graphsleepnet,](#bib.bib127)
    ). Besides EEG, GCNs have also been applied to engineering applications such as
    machine fault diagnosis ([li2020multireceptive,](#bib.bib129) ), slope deformation
    prediction ([ma2021deep,](#bib.bib128) ) and seismic activity prediction ([Bloemheuvel2023graph,](#bib.bib123)
    ). MTPool ([Duan2022graph,](#bib.bib135) ) uses a spatiotemporal GCN for multivariate
    time series classification. In this study, each channel in the time series is
    represented by a node in the graph and the graph edges model the correlations
    between the channels. The GCN is combined with temporal convolutions and a hierarchical
    graph pooling technique. Spatiotemporal GNNs have also been used for object-based
    image analysis ([Censi2021graph,](#bib.bib133) ) and semantic segmentation ([Tulczyjew2022graph,](#bib.bib137)
    ) of image time series. However, these assume the labels and spatial relationships
    are static over time. In many cases these may both change. Spatiotemporal *graphs*
    (STGs), which include temporal edges as well as spatial edges, can model these
    dynamic relationships ([Dufourg2023graph,](#bib.bib139) ). In STGs, each node
    represents an object at one timestamp. Spatial edges connect the object to adjacent
    objects and temporal edges connect two objects in consecutive images if they have
    common pixels.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 时空GCNs通常用于分析传感器阵列，其中图结构模型表示传感器的物理布局。一个常见的例子是脑电图（EEG）数据，其中EEG电极的位置被表示为一个图，用于分析EEG信号。这些应用包括癫痫检测 ([nhu2021graph,](#bib.bib130)
    )、发作检测 ([covert2019temporal,](#bib.bib125) ; [Tang2021graph,](#bib.bib131) )、情感识别 ([Song2020graph,](#bib.bib126)
    )和睡眠分类 ([jia2020graphsleepnet,](#bib.bib127) )。除了EEG，GCNs还被应用于工程应用，如机器故障诊断 ([li2020multireceptive,](#bib.bib129)
    )、坡度变形预测 ([ma2021deep,](#bib.bib128) )和地震活动预测 ([Bloemheuvel2023graph,](#bib.bib123)
    )。MTPool ([Duan2022graph,](#bib.bib135) )使用时空GCN进行多变量时间序列分类。在这项研究中，时间序列中的每个通道由图中的一个节点表示，图的边缘建模通道之间的相关性。GCN与时间卷积和分层图池化技术相结合。时空GNNs还被用于基于对象的图像分析 ([Censi2021graph,](#bib.bib133)
    )和图像时间序列的语义分割 ([Tulczyjew2022graph,](#bib.bib137) )。然而，这些方法假设标签和空间关系在时间上是静态的。在许多情况下，这些可能会发生变化。时空*图*（STGs），包括时间边缘和空间边缘，可以建模这些动态关系 ([Dufourg2023graph,](#bib.bib139)
    )。在STGs中，每个节点表示一个时间戳下的对象。空间边缘将对象连接到相邻对象，而时间边缘连接到连续图像中的两个对象，如果它们具有共同的像素。
- en: 4\. Self-supervised Models
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 自监督模型
- en: Obtaining labeled data for large time series datasets poses significant costs
    and challenges. Machine learning models trained on large labeled time series datasets
    often exhibit superior performance compared to models trained on sparsely labeled
    datasets, small datasets with limited labels, or those without supervision, leading
    to suboptimal performance across various time series machine learning tasks ([yue2022ts2vec,](#bib.bib23)
    ; [yang2022unsupervised,](#bib.bib142) ). As a result, rather than depending on
    high-quality annotations for large datasets, researchers and practitioners are
    increasingly shifting their focus toward self-supervised representation learning
    for time series.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 获取大型时间序列数据集的标记数据面临重大成本和挑战。与在标记稀少的数据集、小型数据集或没有监督的数据集上训练的模型相比，在大规模标记时间序列数据集上训练的机器学习模型通常表现出更优越的性能，这导致了各种时间序列机器学习任务中的亚最佳性能
    ([yue2022ts2vec,](#bib.bib23) ; [yang2022unsupervised,](#bib.bib142) )。因此，研究人员和从业者越来越倾向于将注意力转向时间序列的自监督表示学习，而不是依赖于大规模数据集的高质量标注。
- en: 'Self-supervised representation learning, a subfield of machine learning, focuses
    on learning representations from data without explicit supervision ([foumani2023series2vec,](#bib.bib24)
    ). In contrast to supervised learning, which relies on labeled data, self-supervised
    learning methods utilize the inherent structure of the data to learn valuable
    representations in an unsupervised manner. The learned representations can then
    be used for a variety of downstream tasks including classification, anomaly detection,
    and forecasting. This survey specifically emphasizes classification as a downstream
    task. We categorized self-supervised learning approaches for TSC into three groups
    based on the pretext. Table [4](#S4.T4 "Table 4 ‣ 4.1\. Contrastive Learning ‣
    4\. Self-supervised Models ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey") shows a list of the self-supervised models
    reviewed in this paper.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '自监督表示学习是机器学习的一个子领域，专注于从数据中学习表示，而不需要明确的监督（[foumani2023series2vec](#bib.bib24)）。与依赖标记数据的监督学习不同，自监督学习方法利用数据的固有结构以无监督的方式学习有价值的表示。学到的表示可以用于各种下游任务，包括分类、异常检测和预测。本调查特别强调分类作为下游任务。我们根据前提将TSC的自监督学习方法分为三组。表[4](#S4.T4
    "Table 4 ‣ 4.1\. Contrastive Learning ‣ 4\. Self-supervised Models ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey")列出了本文回顾的自监督模型。'
- en: 4.1\. Contrastive Learning
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 对比学习
- en: Contrastive learning involves model learning to differentiate between positive
    and negative time series examples. Time-Contrastive Learning (TCL) ([hyvarinen2016unsupervised,](#bib.bib143)
    ), Scalable Representation Learning (SRL or T-Loss) ([franceschi2019unsupervised,](#bib.bib144)
    ) and Temporal Neighborhood Coding (TNC) ([tonekaboni2021unsupervised,](#bib.bib145)
    ) apply a subsequence-based sampling and assume that distant segments are negative
    pairs and neighbor segments are positive pairs. TNC takes advantage of the local
    smoothness of a signal’s generative process to define neighborhoods in time with
    stationary properties to further improve the sampling quality for the contrastive
    loss function. TS2Vec ([yue2022ts2vec,](#bib.bib23) ) uses contrastive learning
    to obtain robust contextual representations for each timestamp hierarchically.
    It involves randomly sampling two overlapping subseries from input and encouraging
    consistency of contextual representations on the common segment. The encoder is
    optimized using both temporal contrastive loss and instance-wise contrastive loss.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习涉及模型学习区分正负时间序列示例。时间对比学习（TCL）（[hyvarinen2016unsupervised](#bib.bib143)）、可扩展表示学习（SRL或T-Loss）（[franceschi2019unsupervised](#bib.bib144)）和时间邻域编码（TNC）（[tonekaboni2021unsupervised](#bib.bib145)）应用了基于子序列的采样，并假设远离的段是负对，邻近的段是正对。TNC利用信号生成过程的局部平滑性，定义具有平稳特性的时间邻域，以进一步提高对比损失函数的采样质量。TS2Vec（[yue2022ts2vec](#bib.bib23)）使用对比学习层次性地获得每个时间戳的鲁棒上下文表示。它涉及从输入中随机采样两个重叠的子序列，并鼓励公共段上的上下文表示的一致性。编码器通过时间对比损失和实例对比损失进行优化。
- en: In addition to the subsequence-based methods, other models employ instance-based
    sampling ([eldele2021time,](#bib.bib21) ; [wickstrom2022mixing,](#bib.bib146)
    ; [yang2022timeclr,](#bib.bib147) ; [yang2022unsupervised,](#bib.bib142) ; [zhang2022self,](#bib.bib148)
    ; [meng2023mhccl,](#bib.bib149) ), treating each sample individually to generate
    positive and negative samples for contrastive loss. Time-series Temporal and Contextual
    Contrasting (TS-TCC) ([eldele2021time,](#bib.bib21) ) uses weak and strong augmentations
    to transform the input series into two views and then uses a temporal contrasting
    module to learn robust temporal representations. The contrasting contextual module
    is then built upon the contexts from the temporal contrasting module and aims
    to maximize similarity among contexts of the same sample while minimizing similarity
    among contexts of different samples. Similarly, TimeCLR ([yang2022timeclr,](#bib.bib147)
    ) introduces DTW data augmentation to enhance robustness against phase shift and
    amplitude change phenomena. Bilinear Temporal-Spectral Fusion (BTSF) ([yang2022unsupervised,](#bib.bib142)
    ) uses simple dropout as the augmentation method and aims to incorporate spectral
    information into the feature representation. Similarly, Time-Frequency Consistency
    (TF-C) ([zhang2022self,](#bib.bib148) ) is a self-supervised learning method that
    leverages the frequency domain to achieve better representation. It proposes that
    the time-based and frequency-based representations, learned from the same time
    series sample, should be more similar to each other in the time-frequency space
    compared to representations of different time series samples.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于子序列的方法外，其他模型采用基于实例的采样 ([eldele2021time,](#bib.bib21) ; [wickstrom2022mixing,](#bib.bib146)
    ; [yang2022timeclr,](#bib.bib147) ; [yang2022unsupervised,](#bib.bib142) ; [zhang2022self,](#bib.bib148)
    ; [meng2023mhccl,](#bib.bib149) )，将每个样本单独处理，以生成用于对比损失的正负样本。时间序列时间和上下文对比（TS-TCC） ([eldele2021time,](#bib.bib21)
    ) 使用弱增强和强增强将输入序列转换为两个视图，然后使用时间对比模块学习稳健的时间表示。对比上下文模块则基于时间对比模块的上下文构建，旨在最大化相同样本上下文之间的相似性，同时最小化不同样本上下文之间的相似性。同样，TimeCLR ([yang2022timeclr,](#bib.bib147)
    ) 引入了DTW数据增强，以增强对相位偏移和幅度变化现象的鲁棒性。双线性时间-频谱融合（BTSF） ([yang2022unsupervised,](#bib.bib142)
    ) 使用简单的dropout作为增强方法，旨在将频谱信息融入特征表示。同样，时间-频率一致性（TF-C） ([zhang2022self,](#bib.bib148)
    ) 是一种自监督学习方法，利用频率域实现更好的表示。它提出，基于时间的和基于频率的表示，从相同时间序列样本中学习的，应该在时间-频率空间中彼此更相似，而不是不同时间序列样本的表示。
- en: '| Model | Year | Encoder Backbones |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 编码器骨干 |  |'
- en: '| Contrastive Learning |  |  | Other features |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 对比学习 |  |  | 其他特性 |'
- en: '| TCL ([hyvarinen2016unsupervised,](#bib.bib143) ) | 2016 | MLP | Sequence-based
    contrast |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| TCL ([hyvarinen2016unsupervised,](#bib.bib143) ) | 2016 | MLP | 基于序列的对比 |'
- en: '| T-Loss/SRL ([franceschi2019unsupervised,](#bib.bib144) ) | 2019 | Causal
    CNN | Sequence-based contrast |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| T-Loss/SRL ([franceschi2019unsupervised,](#bib.bib144) ) | 2019 | 因果卷积神经网络
    | 基于序列的对比 |'
- en: '| TNC ([tonekaboni2021unsupervised,](#bib.bib145) ) | 2021 | Bidirectional
    RNN | Sequence-based contrast |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| TNC ([tonekaboni2021unsupervised,](#bib.bib145) ) | 2021 | 双向RNN | 基于序列的对比
    |'
- en: '| TS-TCC ([eldele2021time,](#bib.bib21) ) | 2021 | CNN + Transformers | Instance/Sequence-based
    contrast |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| TS-TCC ([eldele2021time,](#bib.bib21) ) | 2021 | CNN + Transformer | 基于实例/序列的对比
    |'
- en: '| MCL ([wickstrom2022mixing,](#bib.bib146) ) | 2021 | FCN | Instance-based
    contrast |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| MCL ([wickstrom2022mixing,](#bib.bib146) ) | 2021 | FCN | 基于实例的对比 |'
- en: '| TimeCLR ([yang2022timeclr,](#bib.bib147) ) | 2021 | InceptionTime | Instance-based
    contrast |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| TimeCLR ([yang2022timeclr,](#bib.bib147) ) | 2021 | InceptionTime | 基于实例的对比
    |'
- en: '| TS2Vec ([yue2022ts2vec,](#bib.bib23) ) | 2021 | Dilated CNN | Sequence-based
    contrast |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| TS2Vec ([yue2022ts2vec,](#bib.bib23) ) | 2021 | 膨胀卷积神经网络 | 基于序列的对比 |'
- en: '| BTSF ([yang2022unsupervised,](#bib.bib142) ) | 2022 | Causal CNN | Instance-based
    contrast |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| BTSF ([yang2022unsupervised,](#bib.bib142) ) | 2022 | 因果卷积神经网络 | 基于实例的对比
    |'
- en: '| TF-C ([zhang2022self,](#bib.bib148) ) | 2022 | ResNets | Instance-based contrast
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| TF-C ([zhang2022self,](#bib.bib148) ) | 2022 | ResNet | 基于实例的对比 |'
- en: '| MHCCL ([meng2023mhccl,](#bib.bib149) ) | 2023 | ResNet | Instance-based contrast
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| MHCCL ([meng2023mhccl,](#bib.bib149) ) | 2023 | ResNet | 基于实例的对比 |'
- en: '| Self-Prediction |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 自我预测 |'
- en: '| BENDR ([kostas2021bendr,](#bib.bib97) ) | 2021 | CNN + Transformers | Sequence
    masking |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| BENDR ([kostas2021bendr,](#bib.bib97) ) | 2021 | CNN + Transformer | 序列掩码
    |'
- en: '| Voice2Series ([yang2021voice2series,](#bib.bib22) ) | 2021 | CNN+Transformers
    | Binary masking |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Voice2Series ([yang2021voice2series,](#bib.bib22) ) | 2021 | CNN+Transformer
    | 二值掩码 |'
- en: '| TST ([zerveas2021transformer,](#bib.bib19) ) | 2021 | Transformers | Binary
    masking |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| TST ([zerveas2021transformer,](#bib.bib19) ) | 2021 | Transformers | 二进制掩蔽
    |'
- en: '| TARNet ([chowdhury2022tarnet,](#bib.bib150) ) | 2022 | Transformers | Binary
    masking |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| TARNet ([chowdhury2022tarnet,](#bib.bib150) ) | 2022 | Transformers | 二进制掩蔽
    |'
- en: '| TimeMAE ([cheng2023timemae,](#bib.bib151) ) | 2023 | CNN + Transformers |
    Sequence masking |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| TimeMAE ([cheng2023timemae,](#bib.bib151) ) | 2023 | CNN + Transformers |
    序列掩蔽 |'
- en: '| CRT ([zhang2023self,](#bib.bib152) ) | 2023 | Transformers | Sequence masking
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| CRT ([zhang2023self,](#bib.bib152) ) | 2023 | Transformers | 序列掩蔽 |'
- en: '| Other Pretext tasks |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 其他预训练任务 |'
- en: '| PHIT ([ismail2023finding,](#bib.bib153) ) | 2023 | H-InceptionTime |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| PHIT ([ismail2023finding,](#bib.bib153) ) | 2023 | H-InceptionTime |  |'
- en: '| Series2Vec ([foumani2023series2vec,](#bib.bib24) ) | 2023 | Disjoint CNN
    | Similarity based representation learning |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Series2Vec ([foumani2023series2vec,](#bib.bib24) ) | 2023 | 不相交CNN | 基于相似性的表示学习
    |'
- en: Table 4\. Summary of self-supervised models for time series classification and
    extrinsic regression
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表4\. 时间序列分类和外部回归的自监督模型总结
- en: 4.2\. Self-Prediction
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 自预测
- en: The primary objective of self-prediction-based self-supervised models is to
    reconstruct the input or representation of input data. Studies have explored using
    transformer-based self-supervised learning methods for TSC ([kostas2021bendr,](#bib.bib97)
    ; [yang2021voice2series,](#bib.bib22) ; [zerveas2021transformer,](#bib.bib19)
    ; [chowdhury2022tarnet,](#bib.bib150) ; [cheng2023timemae,](#bib.bib151) ; [zhang2023self,](#bib.bib152)
    ), following the success of models like BERT ([devlin2018bert,](#bib.bib93) ).
    BErt-inspired Neural Data Representations (BENDER)([kostas2021bendr,](#bib.bib97)
    ) uses the transformer structure to model EEG sequences and shows that it can
    effectively handle massive amounts of EEG data recorded with differing hardware.
    Another study, Voice-to-Series with Transformer-based Attention (V2Sa)([yang2021voice2series,](#bib.bib22)
    ), utilizes a large-scale pre-trained speech processing model for TSC.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 自预测基础的自监督模型的主要目标是重建输入或输入数据的表示。研究探讨了使用基于变压器的自监督学习方法进行时间序列分类（[kostas2021bendr,](#bib.bib97)
    ; [yang2021voice2series,](#bib.bib22) ; [zerveas2021transformer,](#bib.bib19)
    ; [chowdhury2022tarnet,](#bib.bib150) ; [cheng2023timemae,](#bib.bib151) ; [zhang2023self,](#bib.bib152)
    ），借鉴了像BERT（[devlin2018bert,](#bib.bib93) ）这样的模型的成功。受BERT启发的神经数据表示（BENDER）（[kostas2021bendr,](#bib.bib97)
    ）使用变压器结构来建模EEG序列，并表明它可以有效处理使用不同硬件记录的大量EEG数据。另一项研究，基于变压器的语音处理模型（V2Sa）（[yang2021voice2series,](#bib.bib22)
    ）利用大规模预训练的语音处理模型进行时间序列分类。
- en: Transformer-based Framework (TST)([zerveas2021transformer,](#bib.bib19) ) and
    TARNet ([chowdhury2022tarnet,](#bib.bib150) ) adapts vanilla transformers to the
    multivariate time series domain and uses a self-prediction-based self-supervised
    pre-training approach with masked data. These studies demonstrate the potential
    of using transformer-based self-supervised learning methods for TSC.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变压器的框架（TST）（[zerveas2021transformer,](#bib.bib19) ）和TARNet ([chowdhury2022tarnet,](#bib.bib150)
    ）将普通变压器适应于多变量时间序列领域，并使用基于自预测的自监督预训练方法，数据被掩蔽。这些研究展示了使用基于变压器的自监督学习方法进行时间序列分类的潜力。
- en: 4.3\. Other Pretext tasks
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 其他预训练任务
- en: While many pretext tasks in self-supervised learning are typically contrastive
    or self-predictive, specific tasks are tailored for time series data. In image-based
    self-supervised learning, synthetic transformations (augmentation) of an image
    are created, and the model learns to contrast the image and its transforms with
    other images in the training data, which works well for object interpretation.
    However, time series analysis fundamentally differs from vision or natural language
    processing concerning the definition of meaningful self-supervised learning tasks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自监督学习中的许多预训练任务通常是对比性的或自预测性的，但某些任务是专门为时间序列数据量身定制的。在基于图像的自监督学习中，会创建图像的合成变换（数据增强），模型通过对比图像及其变换与训练数据中的其他图像来学习，这对于对象解释效果良好。然而，时间序列分析在定义有意义的自监督学习任务方面与视觉或自然语言处理根本不同。
- en: Guided by this insight, Foumani et al. ([foumani2023series2vec,](#bib.bib24)
    ) introduce Series2Vec, a novel self-supervised representation learning approach.
    Unlike other contrastive self-supervised methods in time series, which carry the
    risk of positive sample variants being less similar to the anchor sample than
    series in the negative set, Series2Vec is trained to predict the similarity between
    two series in both temporal and spectral domains through a self-supervised task.
    Series2Vec relies primarily on the consistency of the unsupervised similarity
    step, rather than the intrinsic quality of the similarity measurement, without
    the need for hand-crafted data augmentation. Pre-trained H-InceptionTime (PHIT) ([ismail2023finding,](#bib.bib153)
    ) is pre-trained using a novel pretext task designed to identify the originating
    dataset of each time series sample. The objective is to generate flexible convolution
    filters that can be applied across diverse datasets. Furthermore, PHIT demonstrates
    its capability to mitigate overfitting in small datasets.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一见解的指导下，Foumani 等人（[foumani2023series2vec](#bib.bib24)）介绍了 Series2Vec，这是一种新颖的自监督表示学习方法。与时间序列中的其他对比自监督方法不同，这些方法可能存在正样本变体与锚样本的相似度低于负样本集中的序列的风险，Series2Vec
    通过自监督任务训练来预测两个序列在时间域和频谱域中的相似度。Series2Vec 主要依赖于无监督相似度步骤的一致性，而非相似度测量的固有质量，无需手工设计的数据增强。预训练的
    H-InceptionTime（PHIT）([ismail2023finding](#bib.bib153)）使用一种新颖的前置任务进行预训练，该任务旨在识别每个时间序列样本的来源数据集。其目标是生成可以在不同数据集上应用的灵活卷积滤波器。此外，PHIT
    展示了其在小数据集上缓解过拟合的能力。
- en: 5\. Data augmentation
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 数据增强
- en: In the field of deep learning, the concept of data augmentation has emerged
    as an important tool for enhancing performance, particularly in scenarios where
    the availability of training data is limited ([shorten2019survey,](#bib.bib154)
    ). Originally proposed in computer vision, data augmentation involves a variety
    of transformations to images, such as cropping, rotating, flipping, and applying
    filters like blurring and sharpening. These transformations serve to introduce
    a diverse range of scenarios within the training data, thereby aiding in the development
    of more robust and generalizable models. However, the direct application of these
    image-based augmentation techniques to time series data often proves to be inadequate
    or inappropriate. Operations like rotation may disrupt the intrinsic temporal
    structure of time series data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习领域，数据增强的概念已成为提高性能的重要工具，特别是在训练数据有限的情况下（[shorten2019survey](#bib.bib154)）。最初提出于计算机视觉领域，数据增强包括对图像进行各种变换，如裁剪、旋转、翻转以及应用模糊和锐化等滤镜。这些变换旨在引入训练数据中的多样化场景，从而有助于开发更强大和更具泛化能力的模型。然而，将这些基于图像的增强技术直接应用于时间序列数据通常被证明是不足够或不适当的。像旋转这样的操作可能会破坏时间序列数据的固有时间结构。
- en: The challenge of overfitting is particularly pronounced in the field of deep
    learning models for TSC. These models are characterized by a high number of trainable
    parameters, which can lead to a model that performs well on training data but
    fails to generalize to unseen data. In such cases, data augmentation can be a
    valuable strategy. It offers an alternative to the costly and sometimes impractical
    approach of collecting additional real-world data. By generating synthetic samples
    from existing datasets, we can effectively augment the size and variety of our
    training data. The following details different investigated methods to produce
    synthetic time series for data augmentation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合的挑战在时间序列分类（TSC）深度学习模型领域尤为明显。这些模型的特点是具有大量的可训练参数，这可能导致模型在训练数据上表现良好，但在未见过的数据上泛化能力不足。在这种情况下，数据增强可能是一种有价值的策略。它提供了一个替代收集额外真实世界数据的高成本且有时不切实际的方法。通过从现有数据集中生成合成样本，我们可以有效地增加训练数据的数量和多样性。以下详细介绍了几种用于数据增强的合成时间序列生成方法。
- en: Random Transformations
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机变换
- en: Several augmentations have been developed for the magnitude domain. Jittering,
    as explored by Um et al. ([um2017data,](#bib.bib155) ), involves the addition
    of random noise to the time series. Another method, flipping ([rashid2019window,](#bib.bib156)
    ), reverses the time series values. Scaling is a technique where the time series
    is multiplied by a factor from a Gaussian distribution. Magnitude warping, which
    shares similarities with scaling, distorts the series along a curve that varies
    smoothly. For time domain transformations, permutation algorithms play a significant
    role. For example, the slicing transformation involves removing sub-sequence from
    the series. There are also various warping methods like Random Warping ([iwana2021time,](#bib.bib157)
    ), Time Warping ([um2017data,](#bib.bib155) ), Time Stretching ([nguyen2020improving,](#bib.bib158)
    ), and Time Perturbation ([vachhani2018data,](#bib.bib159) ), each introducing
    different forms of distortion to the time series. Finally, in the frequency domain,
    transformations often utilize the Fourier transform. For example, Gao et al. ([gao2020robusttad,](#bib.bib160)
    ) introduce perturbations to both the magnitude and phase spectrum following a
    Fourier transform.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 多种增强方法已被开发用于幅度域。Jittering，如 Um 等人研究的 ([um2017data,](#bib.bib155))，涉及向时间序列中添加随机噪声。另一种方法，翻转 ([rashid2019window,](#bib.bib156))，反转时间序列值。缩放是一种将时间序列乘以来自高斯分布的因子的技术。幅度扭曲与缩放类似，它沿着平滑变化的曲线扭曲序列。对于时间域变换，排列算法扮演着重要角色。例如，切片变换涉及从序列中移除子序列。还有各种扭曲方法，如
    Random Warping ([iwana2021time,](#bib.bib157))、Time Warping ([um2017data,](#bib.bib155))、Time
    Stretching ([nguyen2020improving,](#bib.bib158)) 和 Time Perturbation ([vachhani2018data,](#bib.bib159))，每种方法都引入了对时间序列的不同形式的扭曲。最后，在频率域中，变换通常利用傅里叶变换。例如，Gao
    等人 ([gao2020robusttad,](#bib.bib160)) 引入了对幅度和相位谱的扰动，之后进行傅里叶变换。
- en: Window methods
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 窗口方法
- en: A primary approach in window methods is to create new time series by combining
    segments from various series of the same class. This technique effectively enriches
    the data pool with a variety of samples. Window slicing, as introduced by Cui
    et al. ([cui2016multiscale,](#bib.bib161) ) involves dividing a time series into
    smaller segments, with each segment retaining the class label of the original
    series. These segments are then used to train classifiers, offering a detailed
    view of the data. During classification, each segment is evaluated individually,
    and a collective decision on the final label is reached through a voting system
    among the slices. Another technique is window warping, based on the DTW algorithm.
    This method adjusts segments of a time series along the temporal axis, either
    stretching or compressing them. This introduces variability in the time dimension
    of the data. Le Guennec et al. ([leguennec2016data,](#bib.bib162) ) work provides
    examples of the application of both window slicing and window warping, showcasing
    their effectiveness in enhancing the diversity and representativeness of time
    series datasets.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口方法中的一种主要方法是通过将来自同一类别的不同序列的片段组合来创建新的时间序列。这种技术有效地丰富了数据池中的样本种类。窗口切片，如 Cui 等人提出的 ([cui2016multiscale,](#bib.bib161))，涉及将时间序列划分为较小的片段，每个片段保留原始序列的类别标签。这些片段随后用于训练分类器，提供对数据的详细视图。在分类过程中，每个片段单独评估，最终标签通过片段间的投票系统达成集体决定。另一种技术是窗口扭曲，基于
    DTW 算法。这种方法沿时间轴调整时间序列的片段，进行拉伸或压缩。这引入了数据时间维度的变异性。Le Guennec 等人 ([leguennec2016data,](#bib.bib162))
    的研究提供了窗口切片和窗口扭曲应用的例子，展示了它们在增强时间序列数据集多样性和代表性方面的有效性。
- en: Averaging methods
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平均方法
- en: Averaging methods in time series data augmentation combine multiple series to
    form a new, unified series. This process is more difficult than it might seem,
    as it requires careful consideration of factors like noise and distortions in
    both the time and magnitude aspects of the data. In this context, weighted Dynamic
    Time Warping (DTW) Barycenter Averaging (wDBA) introduced by Forestier et al. ([forestier2017generating,](#bib.bib163)
    ) provides an averaging method by aligning time series in a way that accounts
    for their temporal dynamics. The practical application of wDBA is illustrated
    in the study by Ismail Fawaz et al. ([fawaz2018data,](#bib.bib164) ), where it
    is employed in conjunction with a ResNet classifier, demonstrating its effectiveness.
    Additionally, the research conducted by Terefe et al. ([terefe2020time,](#bib.bib165)
    ) uses an auto-encoder for averaging a set of time series. This method represents
    a more advanced approach in time series data augmentation, exploiting the auto-encoder’s
    capacity for learning and reconstructing data to generate averaged representations
    of time series.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据增强中的平均方法将多个序列结合形成一个新的统一序列。这一过程比看起来更复杂，因为它需要仔细考虑数据在时间和幅度方面的噪声和失真因素。在这个背景下，由Forestier等人提出的加权动态时间规整（DTW）重心平均（wDBA）方法通过对齐时间序列以考虑其时间动态，提供了一种平均方法。wDBA的实际应用在Ismail
    Fawaz等人的研究中得到了说明，研究中它与ResNet分类器结合使用，展示了其有效性。此外，Terefe等人的研究使用自编码器对一组时间序列进行平均。这种方法代表了时间序列数据增强中的一种更先进的方法，利用自编码器学习和重建数据的能力来生成时间序列的平均表示。
- en: Selection of data augmentation methods
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据增强方法选择
- en: The selection of the appropriate data augmentation technique is critical and
    must be adapted to the specific characteristics of the dataset and the architecture
    of the neural network being used. Studies like those conducted by Iwana et al. ([iwana2021empirical,](#bib.bib166)
    ), Pialla et al. ([pialla2022data,](#bib.bib167) ) and Gao et al ([gao2023data,](#bib.bib168)
    ) highlight the complexity of this task. These studies demonstrate that the effectiveness
    of augmentation techniques can vary significantly across different datasets and
    neural network architectures. Consequently, a method that proves effective in
    one scenario may not necessarily yield similar results in another. To this end,
    practitioners in the field of TSC must engage in a careful and informed process
    of method selection and tuning. While the array of available data augmentation
    techniques offers a comprehensive toolkit for tackling the challenges of limited
    data and overfitting, their successful application depends heavily on a nuanced
    understanding of both the methods themselves and the specific demands of the task
    at hand.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的数据增强技术至关重要，必须根据数据集的特定特征和使用的神经网络架构进行调整。Iwana等人、Pialla等人和Gao等人的研究突出了这一任务的复杂性。这些研究表明，增强技术的有效性在不同的数据集和神经网络架构之间可能会有显著差异。因此，在一种场景下有效的方法在另一种场景中可能不会产生相似的结果。为此，TSC领域的从业者必须进行仔细和知情的方法选择与调整过程。虽然可用的数据增强技术提供了应对数据有限和过拟合挑战的全面工具，但其成功应用在很大程度上依赖于对这些方法及任务具体需求的细致理解。
- en: 6\. Transfer learning
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 迁移学习
- en: Transfer learning, initially popularized in the field of computer vision, is
    increasingly becoming relevant in the domain of TSC. In computer vision, this
    approach involves using a pre-trained network, typically on large datasets like
    ImageNet ([deng2009imagenet,](#bib.bib169) ), as a starting point rather than
    initiating with random network weights. This method is also related to the concept
    of foundation or base models, which are large-scale machine learning models trained
    on extensive data, often using self-supervised or semi-supervised learning. These
    models are adaptable to a wide array of tasks, showcasing their versatility. The
    principle of transfer learning is also closely associated with domain adaptation
    which focuses on applying a model trained on a source data distribution to a different,
    but related, target data distribution. This approach is crucial in leveraging
    pre-trained models for various applications, particularly in scenarios where data
    is scarce or specific to certain domains.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习最初在计算机视觉领域受到关注，现在在时序分类（TSC）领域越来越相关。在计算机视觉中，这种方法涉及使用预训练的网络，通常是在像 ImageNet
    这样的大型数据集上训练的（[deng2009imagenet](#bib.bib169)），作为起点，而不是从随机网络权重开始。这种方法也与基础模型或基础模型的概念相关，这些模型是在广泛的数据上训练的大规模机器学习模型，通常使用自监督或半监督学习。这些模型可以适应各种任务，展示了它们的多功能性。转移学习的原理也与领域适应密切相关，领域适应专注于将训练在源数据分布上的模型应用到不同但相关的目标数据分布上。这种方法在利用预训练模型进行各种应用，特别是在数据稀缺或特定领域的场景中至关重要。
- en: In the context of TSC, insights have been contributed by the work of Ismail
    Fawaz et al. ([fawaz2018transfer,](#bib.bib170) ), who conducted a study using
    the UCR archive. Their extensive experiments demonstrated that transfer learning
    could lead to positive or negative outcomes, depending on the chosen datasets
    for transfer. This finding underscores the importance of the relationship between
    source and target datasets in transfer learning efficacy. Ismail Fawaz et al. ([fawaz2018transfer,](#bib.bib170)
    ) also introduced an approach to predict the success of transfer learning in TSC
    by using DTW to measure similarities between datasets. This metric serves as a
    guide to select the most appropriate source dataset for a given target dataset,
    thereby enhancing accuracy in a majority of cases.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TSC 的背景下，Ismail Fawaz 等人（[fawaz2018transfer](#bib.bib170)）的研究提供了洞见，他们使用了 UCR
    归档数据进行研究。他们的大量实验表明，转移学习可能导致积极或消极的结果，取决于选择的转移数据集。这一发现突显了源数据集和目标数据集之间关系在转移学习效果中的重要性。Ismail
    Fawaz 等人（[fawaz2018transfer](#bib.bib170)）还引入了一种方法，通过使用 DTW 来测量数据集之间的相似性，预测转移学习在
    TSC 中的成功。这一度量标准作为选择最合适的源数据集用于给定目标数据集的指南，从而在大多数情况下提高了准确性。
- en: Other researchers have also explored transfer learning in TSC. Spiegel ([spiegel2016transfer,](#bib.bib171)
    ) work on using dissimilarity spaces to enrich feature representations in TSC
    set a precedent for employing unconventional data sources. This approach of enhancing
    learning with diverse data types finds a parallel in Li et al. ([li2020deep,](#bib.bib172)
    ) method, which leverages sensor modality labels from various fields to train
    a deep network, emphasizing the importance of versatile data in transfer learning.
    Building on the concept of data diversity, Rotem et al. ([rotem2022transfer,](#bib.bib173)
    ) pushed the boundaries further by generating a synthetic univariate time series
    dataset for transfer learning. This synthetic dataset, used for regression tasks,
    underscores the potential of artificial data in overcoming the limitations of
    real-world datasets. Furthermore, Senanayaka et al. ([senanayaka2022similarity,](#bib.bib174)
    ) introduced the similarity-based multi-source transfer learning (SiMuS-TL) approach.
    By establishing a ’mixed domain’ to model similarities among various sources,
    Senanayaka et al. demonstrated the effectiveness of carefully selected and related
    data sources in transfer learning. Finally, Kashiparekh et al. ([kashiparekh2019convtimenet,](#bib.bib175)
    ) with their ConvTimeNet (CTN) focused on the adaptability of pre-trained networks
    across diverse time scales.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究人员也探索了在时间序列分类（TSC）中的迁移学习。Spiegel ([spiegel2016transfer,](#bib.bib171)) 在利用异质性空间丰富
    TSC 特征表示方面的工作为采用非常规数据源设立了先例。这种通过多样数据类型增强学习的方法与 Li 等人 ([li2020deep,](#bib.bib172))
    的方法相呼应，该方法利用来自各个领域的传感器模态标签来训练深度网络，强调了多功能数据在迁移学习中的重要性。在数据多样性的概念基础上，Rotem 等人 ([rotem2022transfer,](#bib.bib173))
    更进一步，通过生成一个用于迁移学习的合成单变量时间序列数据集来推动研究进展。这个用于回归任务的合成数据集突显了人工数据在克服真实数据集局限性方面的潜力。此外，Senanayaka
    等人 ([senanayaka2022similarity,](#bib.bib174)) 引入了基于相似性的多源迁移学习（SiMuS-TL）方法。通过建立一个“混合领域”来建模各种来源之间的相似性，Senanayaka
    等人展示了精心选择的相关数据源在迁移学习中的有效性。最后，Kashiparekh 等人 ([kashiparekh2019convtimenet,](#bib.bib175))
    的 ConvTimeNet (CTN) 关注于预训练网络在不同时间尺度上的适应性。
- en: While the explored studies collectively advance our understanding of transfer
    learning in TSC, the field remains open for further investigation. A key challenge
    lies in determining the most suitable source models for transfer, a task complicated
    by the relative scarcity of large, curated, and annotated datasets in time series
    analysis compared to the field of computer vision. This restricts the utility
    of transfer learning in TSC, as the availability of extensive and diverse datasets
    is crucial for developing robust and generalizable models. Furthermore, the question
    of developing filters that are generic enough to be effective across a wide range
    of applications remains unresolved. This aspect is critical for the success of
    transfer learning, as the applicability of a pre-trained model to new tasks depends
    on the universality of its learned features. Additionally, the strategy of whether
    to freeze certain layers of the network during transfer or to fine-tune the entire
    network is another area that warrants deeper exploration.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些研究共同推进了我们对 TSC 中迁移学习的理解，但该领域仍然需要进一步的探索。一个关键挑战是确定最合适的源模型进行迁移，这一任务因时间序列分析中大规模、精心策划和标注数据集相对稀缺而复杂。与计算机视觉领域相比，这限制了迁移学习在
    TSC 中的实用性，因为开发强大且具有通用性的模型需要大量且多样的数据集。此外，如何开发足够通用的过滤器，以便在广泛应用中有效仍然没有解决。这一方面对于迁移学习的成功至关重要，因为预训练模型对新任务的适用性取决于其学习特征的普遍性。此外，是否在迁移过程中冻结某些网络层，或者对整个网络进行微调的策略也是另一个值得深入探讨的领域。
- en: 7\. Applications - recent developments and challenges
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 应用 - 最近的发展与挑战
- en: TSC and TSER techniques have been used to analyze and model time-dependent data
    in a wide range of applications. These include human activity recognition, Earth
    observation, medical diagnosis including Electroencephalogram (EEG) ([MerlinPraveena2022app,](#bib.bib176)
    ) and Electrocardiogram (ECG) ([Liu2021app,](#bib.bib177) ) monitoring, air quality
    and pollution prediction ([Zaini2022app,](#bib.bib178) ; [Zhang2022app,](#bib.bib179)
    ), structural and machine health monitoring ([Toh2020app,](#bib.bib180) ; [Thoppil2021app,](#bib.bib181)
    ), Industrial Internet of Things (IIOT) ([Ren2023app,](#bib.bib182) ), energy
    consumption and anomaly detection ([Himeur2021app,](#bib.bib183) ), and bio-acoustics
    ([Stowell2022app,](#bib.bib184) ).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: TSC 和 TSER 技术已被应用于分析和建模时间依赖数据，涉及的应用范围广泛，包括人类活动识别、地球观测、医疗诊断（如脑电图（EEG）([MerlinPraveena2022app,](#bib.bib176)
    )和心电图（ECG）([Liu2021app,](#bib.bib177) )监测）、空气质量和污染预测（[Zaini2022app,](#bib.bib178)
    ; [Zhang2022app,](#bib.bib179) ）、结构与机器健康监测（[Toh2020app,](#bib.bib180) ; [Thoppil2021app,](#bib.bib181)
    ）、工业物联网（IIOT）（[Ren2023app,](#bib.bib182) ）、能源消耗和异常检测（[Himeur2021app,](#bib.bib183)
    ）以及生物声学（[Stowell2022app,](#bib.bib184) ）。
- en: Due to the extensive range of applications that use TSC and TSER, it is infeasible
    to cover them all in detail in a single review. Therefore, in this survey, we
    focus on just two applications – human activity recognition and satellite Earth
    observation. (References to recent reviews have been provided for the other applications
    mentioned above.) These are two important but quite different domains and were
    chosen to give the reader an idea of the diverseness of time series use in deep
    learning. The following sections provide an overview of the use of TSC and TSER,
    the latest developments, and challenges in these two applications.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TSC 和 TSER 应用范围广泛，难以在一篇综述中详尽覆盖所有应用。因此，在本调查中，我们仅关注两个应用领域——人类活动识别和卫星地球观测。（对于上述提到的其他应用，已提供了近期综述的参考文献。）这两个领域都很重要，但各有不同，选择它们是为了让读者了解时间序列在深度学习中的多样性。以下部分将概述
    TSC 和 TSER 的应用、最新进展以及这两个应用中的挑战。
- en: 7.1\. Human Activity Recognition
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 人类活动识别
- en: Human activity recognition (HAR), is the identification or monitoring of human
    activity through the analysis of data collected by sensors or other instruments ([Gupta2022survey,](#bib.bib185)
    ). The recent growth of wearable technologies and the Internet of Things has resulted
    not only in the collection of large volumes of activity data ([Ramanujam2021survey,](#bib.bib186)
    ), but also easy deployment of applications utilising this data to improve the
    safety and quality of human life ([chen2021deep,](#bib.bib5) ; [Gupta2022survey,](#bib.bib185)
    ). HAR is therefore an important field of research with applications including
    healthcare, fitness monitoring, smart homes ([Lockhart2012appl,](#bib.bib187)
    ), and assisted living ([Tapia2004appl,](#bib.bib188) ).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 人类活动识别（HAR）是通过分析传感器或其他仪器收集的数据来识别或监测人类活动（[Gupta2022survey,](#bib.bib185) ）。近年来，可穿戴技术和物联网的迅猛发展不仅导致了大量活动数据的收集（[Ramanujam2021survey,](#bib.bib186)
    ），还使得利用这些数据的应用得以轻松部署，从而改善人类生活的安全性和质量（[chen2021deep,](#bib.bib5) ; [Gupta2022survey,](#bib.bib185)
    ）。因此，HAR 是一个重要的研究领域，其应用包括医疗保健、健身监测、智能家居（[Lockhart2012appl,](#bib.bib187) ）和辅助生活（[Tapia2004appl,](#bib.bib188)
    ）。
- en: Devices used to collect HAR data can be categorised as visual or sensor-based ([chen2021deep,](#bib.bib5)
    ; [wang2019deep,](#bib.bib4) ). Sensor-based devices can be further categorised
    as object sensors (for example RFIDs embedded into objects), ambient sensors (motion
    sensors, WiFi or Bluetooth devices in fixed locations) and wearable sensors ([wang2019deep,](#bib.bib4)
    ), including smartphones ([nweke2018deep,](#bib.bib3) ). However, the majority
    of HAR studies use data from wearable sensors or visual devices ([Gupta2022survey,](#bib.bib185)
    ). Additionally, human activity recognition from visual device data requires the
    use of computer vision techniques and is therefore out of scope for this review.
    Accordingly, this section reviews wearable sensor-based methods of HAR. For reviews
    of vision-based HAR, refer to Kong and Fu([Kong2022vision,](#bib.bib189) ) or
    Zhang et al. ([Zhang2019vision,](#bib.bib190) ).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 收集 HAR 数据的设备可以分为视觉设备或传感器设备 ([chen2021deep,](#bib.bib5) ; [wang2019deep,](#bib.bib4)
    )。传感器设备可以进一步分为物体传感器（例如嵌入物体中的 RFID）、环境传感器（固定位置的运动传感器、WiFi 或蓝牙设备）和可穿戴传感器 ([wang2019deep,](#bib.bib4)
    )，包括智能手机 ([nweke2018deep,](#bib.bib3) )。然而，大多数 HAR 研究使用的是来自可穿戴传感器或视觉设备的数据 ([Gupta2022survey,](#bib.bib185)
    )。此外，从视觉设备数据中进行人类活动识别需要使用计算机视觉技术，因此超出了本综述的范围。因此，本节回顾了基于可穿戴传感器的 HAR 方法。有关基于视觉的
    HAR 综述，请参考 Kong 和 Fu ([Kong2022vision,](#bib.bib189) ) 或 Zhang 等 ([Zhang2019vision,](#bib.bib190)
    )。
- en: The main sensors used in wearable devices are accelerometers, gyroscopes and
    magnetic sensors ([Ordonez2016deep,](#bib.bib191) ), which each collect three-dimensional
    spatial data over time. Inertial measurement units (IMUs) are wearable devices
    that combine all three sensors in one unit ([Reiss2012pamap,](#bib.bib192) ; [Zhang2012uschad,](#bib.bib193)
    ). Wearable device studies typically collect data from multiple IMUs located on
    different parts of the body ([Roggen2010opp,](#bib.bib194) ; [Sztyler2017position,](#bib.bib195)
    ). To create a dataset suitable for HAR modelling, the sensor data is split into
    (usually equally size) time windows ([Lara2013survey,](#bib.bib196) ). The task
    is then to learn a function that maps the multi-variate sensor data for each time
    window to a set of activities. Thus, the data forms multi-variate time series
    suited to TSC.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 可穿戴设备中使用的主要传感器是加速度计、陀螺仪和磁力传感器 ([Ordonez2016deep,](#bib.bib191) )，这些传感器各自收集三维空间数据。惯性测量单元（IMUs）是将所有三种传感器集成在一个单元中的可穿戴设备
    ([Reiss2012pamap,](#bib.bib192) ; [Zhang2012uschad,](#bib.bib193) )。可穿戴设备研究通常从身体不同部位的多个
    IMUs 收集数据 ([Roggen2010opp,](#bib.bib194) ; [Sztyler2017position,](#bib.bib195)
    )。为了创建适合 HAR 建模的数据集，传感器数据被分割成（通常是大小相等的）时间窗口 ([Lara2013survey,](#bib.bib196) )。任务是学习一个函数，将每个时间窗口的多变量传感器数据映射到一组活动。因此，这些数据形成了适合时间序列分类（TSC）的多变量时间序列。
- en: Given the broad scope of our survey, this section necessarily only provides
    a brief overview of the studies using deep learning for HAR. However, there are
    several surveys that provide a more in-depth review of machine learning and deep
    learning for HAR. Lara and Labrador ([Lara2013survey,](#bib.bib196) ) provide
    a comprehensive introduction to HAR, including machine learning methods used and
    the principal issues and challenges. Both Nweke et al. ([nweke2018deep,](#bib.bib3)
    ) and Wang et al. ([wang2019deep,](#bib.bib4) ) provide a summary of deep learning
    methods, highlighting their advantages and limitations. Chen et al. ([chen2021deep,](#bib.bib5)
    ) discuss challenges in HAR and the appropriate deep learning methods for addressing
    each challenge. They also provide a comprehensive list of publicly-available HAR
    datasets. Gu et al. ([Gu2022survey,](#bib.bib197) ) focus on deep learning methods,
    reviewing preprocessing and evaluation techniques as well as the deep learning
    models.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们调查的广泛范围，本节仅提供了使用深度学习进行 HAR 研究的简要概述。然而，还有几篇综述提供了有关机器学习和深度学习在 HAR 中应用的更深入审查。Lara
    和 Labrador ([Lara2013survey,](#bib.bib196) ) 提供了 HAR 的综合介绍，包括所用的机器学习方法以及主要问题和挑战。Nweke
    等 ([nweke2018deep,](#bib.bib3) ) 和 Wang 等 ([wang2019deep,](#bib.bib4) ) 对深度学习方法进行了总结，突出了它们的优点和局限性。Chen
    等 ([chen2021deep,](#bib.bib5) ) 讨论了 HAR 中的挑战以及应对每个挑战的深度学习方法。他们还提供了公开可用的 HAR 数据集的综合列表。Gu
    等 ([Gu2022survey,](#bib.bib197) ) 重点关注深度学习方法，回顾了预处理和评估技术以及深度学习模型。
- en: 'The deep learning methods used for HAR include both CNNs and RNNs, as well
    as hybrid CNN-RNN models. While some of the models include an attention module,
    we did not find any studies proposing a full attention or transformer model. A
    summary of the studies reviewed and the type of model built is provided in table
    [5](#S7.T5 "Table 5 ‣ 7.1\. Human Activity Recognition ‣ 7\. Applications - recent
    developments and challenges ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey"). Hammerla et al. ([Hammerla2016deep,](#bib.bib198)
    ) compared several deep learning model for HAR, include three LSTM variants, a
    CNN model, and DNN model. They found a bi-directional LSTM performed best on naturalistic
    datasets where long-term effects are important. However, they found some applications
    need to focus on short-term movement patterns and suggested CNNs are more appropriate
    for these applications. Thus, research across all model types is beneficial for
    the on-going development of models for HAR applications.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法用于HAR中包括CNN和RNN以及混合的CNN-RNN模型。尽管一些模型包含注意力模块，但我们没有找到任何提出完整注意力或转换器模型的研究。表格[5](#S7.T5
    "表5 ‣ 7.1\. 人体活动识别 ‣ 7\. 应用 - 最新发展和挑战 ‣ 时间序列分类和外部回归的深度学习：当前调查")给出了审查的研究和构建模型的类型的摘要。Hammerla等（[Hammerla2016deep，](#bib.bib198)）对HAR进行了几个深度学习模型的比较，包括三个LSTM变体、一个CNN模型和一个DNN模型。他们发现在重视长期效应的自然数据集上，双向LSTM表现最好。然而，他们发现一些应用需要关注短期运动模式，并建议CNN更适合这些应用。因此，全面研究所有模型类型有利于HAR应用模型的持续发展。
- en: '| Model | Year | Embedding | Other features |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Model | Year | Embedding | Other features |'
- en: '| --- | --- | --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Zeng et al. ([Zeng2014cnn,](#bib.bib199) ) | 2014 | CNN |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Zeng et al. ([Zeng2014cnn,](#bib.bib199) ) | 2014 | CNN |  |'
- en: '| DCNN ([Jiang2015cnn,](#bib.bib200) ) | 2015 | CNN | Discrete Fourier Transform
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| DCNN ([Jiang2015cnn,](#bib.bib200) ) | 2015 | CNN | Discrete Fourier Transform
    |'
- en: '| Yang et al. ([Yang2015cnn,](#bib.bib201) ) | 2015 | CNN |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. ([Yang2015cnn,](#bib.bib201) ) | 2015 | CNN |  |'
- en: '| DeepConvLSTM ([Ordonez2016deep,](#bib.bib191) ) | 2016 | CNN, LSTM |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| DeepConvLSTM ([Ordonez2016deep,](#bib.bib191) ) | 2016 | CNN, LSTM |  |'
- en: '| Hammerla et al. ([Hammerla2016deep,](#bib.bib198) ) | 2016 | CNN, LSTM |
    Bi-directional |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Hammerla et al. ([Hammerla2016deep,](#bib.bib198) ) | 2016 | CNN, LSTM |
    Bi-directional |'
- en: '| Ronao et al. ([Ronao2016har,](#bib.bib202) ) | 2016 | CNN |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Ronao et al. ([Ronao2016har,](#bib.bib202) ) | 2016 | CNN |  |'
- en: '| Guan and Plötz ([Guan2017ensemble,](#bib.bib203) ) | 2017 | LSTM | Ensemble
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Guan and Plötz ([Guan2017ensemble,](#bib.bib203) ) | 2017 | LSTM | Ensemble
    |'
- en: '| Lee et al. ([Lee2017har,](#bib.bib204) ) | 2017 | CNN |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Lee et al. ([Lee2017har,](#bib.bib204) ) | 2017 | CNN |  |'
- en: '| Murad and Pyun ([Murad2017deep,](#bib.bib205) ) | 2017 | LSTM | Uni- & bi-directional
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Murad and Pyun ([Murad2017deep,](#bib.bib205) ) | 2017 | LSTM | Uni- & bi-directional
    |'
- en: '| Ignatov ([Ignatov2018realtime,](#bib.bib206) ) | 2018 | CNN | Statistical
    features |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Ignatov ([Ignatov2018realtime,](#bib.bib206) ) | 2018 | CNN | Statistical
    features |'
- en: '| Moya Rueda et al. ([Rueda2018cnn,](#bib.bib207) ) | 2018 | CNN |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Moya Rueda et al. ([Rueda2018cnn,](#bib.bib207) ) | 2018 | CNN |  |'
- en: '| Yao et al. ([Yao2018cnn,](#bib.bib208) ) | 2018 | CNN | Fully convolutional
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Yao et al. ([Yao2018cnn,](#bib.bib208) ) | 2018 | CNN | Fully convolutional
    |'
- en: '| Zeng et al. ([Zeng2018rnn,](#bib.bib209) ) | 2018 | LSTM | 2 attention layers
    |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Zeng et al. ([Zeng2018rnn,](#bib.bib209) ) | 2018 | LSTM | 2 attention layers
    |'
- en: '| AttnSense ([Ma2019attnsense,](#bib.bib210) ) | 2019 | CNN, GRU | Fast Fourier
    Transform, 2 attention layers |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| AttnSense ([Ma2019attnsense,](#bib.bib210) ) | 2019 | CNN, GRU | Fast Fourier
    Transform, 2 attention layers |'
- en: '| InnoHAR ([Xu2019innohar,](#bib.bib211) ) | 2019 | CNN, GRU | Inception |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| InnoHAR ([Xu2019innohar,](#bib.bib211) ) | 2019 | CNN, GRU | Inception |'
- en: '| Zhang et al. ([Zhang2020novel,](#bib.bib212) ) | 2020 | CNN | Attention |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. ([Zhang2020novel,](#bib.bib212) ) | 2020 | CNN | Attention |'
- en: '| Challa et al. ([Challa2021multi,](#bib.bib213) ) | 2021 | CNN, LSTM | Bi-directional
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Challa et al. ([Challa2021multi,](#bib.bib213) ) | 2021 | CNN, LSTM | Bi-directional
    |'
- en: '| CNN-biGRU ([Mekruksavanich2021deep,](#bib.bib214) ) | 2021 | CNN, GRU | Bi-directional
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| CNN-biGRU ([Mekruksavanich2021deep,](#bib.bib214) ) | 2021 | CNN, GRU | Bi-directional
    |'
- en: '| DEBONAIR ([Chen2021har,](#bib.bib215) ) | 2021 | ConvLSTM |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| DEBONAIR ([Chen2021har,](#bib.bib215) ) | 2021 | ConvLSTM |  |'
- en: '| Mekruksavanich and Jitpattanakul ([Mekruksavanich2021lstm,](#bib.bib216)
    ) | 2021 | CNN, LSTM |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Mekruksavanich and Jitpattanakul ([Mekruksavanich2021lstm,](#bib.bib216)
    ) | 2021 | CNN, LSTM |  |'
- en: '| Mekruksavanich and Jitpattanakul ([Mekruksavanich2021biometric,](#bib.bib217)
    ) | 2021 | CNN, LSTM | Ensemble |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Mekruksavanich and Jitpattanakul ([Mekruksavanich2021biometric,](#bib.bib217)
    ) | 2021 | CNN, LSTM | Ensemble |'
- en: '| Nafea et al. ([Nafea2021sensor,](#bib.bib218) ) | 2021 | CNN, LSTM | Bi-directional
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Nafea et al. ([Nafea2021sensor,](#bib.bib218) ) | 2021 | CNN, LSTM | Bi-directional
    |'
- en: '| Singh et al. ([Singh2021convlstm,](#bib.bib219) ) | 2021 | CNN, LSTM | Attention
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Singh 等人 ([Singh2021convlstm,](#bib.bib219) ) | 2021 | CNN, LSTM | Attention
    |'
- en: '| Wang et al. ([Wang2022deep,](#bib.bib220) ) | 2022 | CNN |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 ([Wang2022deep,](#bib.bib220) ) | 2022 | CNN |  |'
- en: '| Xu et al. ([Xu2022deform,](#bib.bib221) ) | 2022 | CNN, Resnet | Deformable
    convolutions |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Xu 等人 ([Xu2022deform,](#bib.bib221) ) | 2022 | CNN, Resnet | 可变形卷积 |'
- en: Table 5\. Summary of HAR deep learning models
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. HAR 深度学习模型总结
- en: 'Many of the papers reviewed in this section used commonly available datasets
    to build and evaluate their models. A summary of the most commonly used datasets
    is provided in section [C.1](#A3.SS1 "C.1\. HAR Datasets ‣ Appendix C Datasets
    ‣ Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey") of the Appendix.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾的许多论文使用了常用的数据集来构建和评估其模型。最常用的数据集总结见附录的 [C.1](#A3.SS1 "C.1\. HAR 数据集 ‣ 附录
    C 数据集 ‣ 时间序列分类和外部回归的深度学习：当前调查") 部分。
- en: 7.1.1\. Convolutional neural networks
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 卷积神经网络
- en: One of the most common types of convolutional kernels for HAR is the $k\times
    1$ kernel. This kernel convolves $k$ time steps together, moving along each time
    series in the input features in turn ([Wang2022deep,](#bib.bib220) ), so while
    weights are shared between the input features, there is no mixing between features.
    The outputs from the final convolutional layer are flattened and processed by
    fully-connected layers before the final classification is made. Ronao et al. ([Ronao2016har,](#bib.bib202)
    ) performed a comprehensive evaluation of CNN models for HAR, evaluating the effect
    of changing the number of layers, filters and filter sizes. The input data was
    collected from smartphone accelerometer and gyroscope sensors. Ignatov ([Ignatov2018realtime,](#bib.bib206)
    ) used a one-layer CNN, and augmented the extracted features with statistical
    features before being passed to fully-connected layers. The architecture was effective
    with short time series (1 second) so useful for real time activity modelling.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: HAR 中最常见的卷积核类型之一是 $k\times 1$ 卷积核。该卷积核将 $k$ 个时间步的特征进行卷积，依次沿着输入特征中的每个时间序列移动 ([Wang2022deep,](#bib.bib220)
    )，因此虽然输入特征之间的权重是共享的，但特征之间没有混合。最终卷积层的输出被展平，并通过全连接层进行处理，然后进行最终分类。Ronao 等人 ([Ronao2016har,](#bib.bib202)
    ) 对 HAR 的 CNN 模型进行了全面评估，评估了更改层数、滤波器数量和滤波器大小的影响。输入数据来自智能手机的加速度计和陀螺仪传感器。Ignatov ([Ignatov2018realtime,](#bib.bib206)
    ) 使用了一个单层 CNN，并在传递到全连接层之前用统计特征增强了提取的特征。该架构在短时间序列（1秒）上有效，因此适用于实时活动建模。
- en: One drawback of the above method is that it forces weight-sharing across all
    the input features. This may not be optimal, especially when using data collected
    from multiple devices. In this case, using a separate CNN for each device ([Rueda2018cnn,](#bib.bib207)
    ) allows independent weighting of the features. Similarly, as each sensor is typically
    tri-axial, a separate CNN can be used for each axis ([Zeng2014cnn,](#bib.bib199)
    ; [Zhang2020novel,](#bib.bib212) ). The features extracted by each CNN are then
    concatenated and processed either by fully-connected layers ([Zeng2014cnn,](#bib.bib199)
    ) or an attention head ([Zhang2020novel,](#bib.bib212) ).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法的一个缺点是，它强制所有输入特征之间的权重共享。这可能不是最优的，特别是当使用来自多个设备的数据时。在这种情况下，为每个设备使用单独的 CNN ([Rueda2018cnn,](#bib.bib207)
    ) 允许对特征进行独立加权。同样，由于每个传感器通常是三轴的，因此可以为每个轴使用单独的 CNN ([Zeng2014cnn,](#bib.bib199)
    ; [Zhang2020novel,](#bib.bib212) )。每个 CNN 提取的特征随后被拼接，并通过全连接层 ([Zeng2014cnn,](#bib.bib199)
    ) 或注意力头 ([Zhang2020novel,](#bib.bib212) ) 进行处理。
- en: While the above two methods are the most common, other studies have proposed
    alternative CNNs for HAR. DCNN ([Jiang2015cnn,](#bib.bib200) ) pre-processes the
    sensor data using a Discrete Fourier Transform to convert IMU data to frequency
    signals, then uses two-dimensional convolutions to extract combined temporal and
    frequency features. Lee et al. ([Lee2017har,](#bib.bib204) ) pre-processed the
    tri-axial accelerometer data to a magnitude vector, which was then processed in
    parallel by CNNs with varying kernel sizes, extracting features at different scales.
    Xu et al. ([Xu2022deform,](#bib.bib221) ) used deformable convolutions ([Dai2017deform,](#bib.bib222)
    ) in both a 2D-CNN and a ResNet model and found these models performed better
    than their non-deformable counterparts. Yao et al. ([Yao2018cnn,](#bib.bib208)
    ) proposed a fully convolutional model using two-dimensional temporal and feature
    convolutions. Their model has two advantages as (1) it handles arbitrary length
    input sequences and (2) it makes a prediction for each timestep, which avoids
    the need to pre-process the data into windows and can detect transitions between
    activities.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述两种方法是最常见的，但其他研究也提出了用于HAR的替代CNN。DCNN ([Jiang2015cnn,](#bib.bib200) ) 使用离散傅里叶变换对传感器数据进行预处理，将IMU数据转换为频率信号，然后使用二维卷积提取时间和频率特征的组合。Lee
    等人 ([Lee2017har,](#bib.bib204) ) 将三轴加速度计数据预处理为一个幅度向量，然后用不同卷积核大小的CNN并行处理，提取不同尺度的特征。Xu
    等人 ([Xu2022deform,](#bib.bib221) ) 在2D-CNN和ResNet模型中使用了可变形卷积 ([Dai2017deform,](#bib.bib222)
    )，发现这些模型的表现优于它们的非可变形对应物。Yao 等人 ([Yao2018cnn,](#bib.bib208) ) 提出了一个完全卷积模型，使用二维时间和特征卷积。该模型有两个优点：(1)
    它处理任意长度的输入序列，(2) 它对每个时间步进行预测，避免了将数据预处理成窗口的需求，并能检测活动之间的过渡。
- en: 7.1.2\. Recurrent neural networks
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 循环神经网络
- en: Several long short-term memory (LSTM) models have been proposed for HAR. Murad
    and Pyun ([Murad2017deep,](#bib.bib205) ) designed and compared three multi-layered
    LSTMs, a uni-directional LSTM, a bi-directional LSTM, and a “cascading” LSTM,
    which has a bi-directional first layer, followed by uni-directional layers. In
    each case the output from all time steps is used as input to the classification
    layer. Zeng et al. ([Zeng2018rnn,](#bib.bib209) ) added two attention layers to
    an LSTM, a sensor attention layer before the LSTM and a temporal attention layer
    after the LSTM. They include a regularisation term they called “continuous attention”
    to smooth the transition between attention weights. Guan and Plötz ([Guan2017ensemble,](#bib.bib203)
    ) created an ensemble of LSTM models by saving the models at every training epoch,
    then selecting the best “M” models based on validation set results, thus aiming
    to reduce model variance.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 针对HAR（人类活动识别），已经提出了几种长短期记忆（LSTM）模型。Murad 和 Pyun ([Murad2017deep,](#bib.bib205)
    ) 设计并比较了三种多层LSTM模型：单向LSTM、双向LSTM 和一个“级联”LSTM，该模型具有一个双向的第一层，之后是单向层。在每种情况下，所有时间步的输出都作为分类层的输入。Zeng
    等人 ([Zeng2018rnn,](#bib.bib209) ) 在LSTM中增加了两个注意力层，一个是在LSTM之前的传感器注意力层，另一个是在LSTM之后的时间注意力层。他们包含了一个名为“连续注意力”的正则化项，以平滑注意力权重之间的过渡。Guan
    和 Plötz ([Guan2017ensemble,](#bib.bib203) ) 通过在每个训练周期保存模型，然后根据验证集结果选择最佳的“M”个模型，创建了一个LSTM模型的集成，旨在减少模型的方差。
- en: 7.1.3\. Hybrid models
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3\. 混合模型
- en: 'Many recent studies have focussed on hybrid models, combining both CNNs and
    RNNs. DeepConvLSTM ([Ordonez2016deep,](#bib.bib191) ) comprises four temporal
    convolutional layers followed by two LSTM layers, which the authors found to perform
    better than an equivalent CNN (replacing the LSTM layers with fully-connected
    layers). As the LSTM layers have fewer parameters than fully-connected layers,
    the DeepConvLSTM model was also much smaller. Singh et al. ([Singh2021convlstm,](#bib.bib219)
    ) used a CNN to encode the spatial data (i.e. the sensor readings at each timestamp)
    followed by a single LSTM layer to encode the temporal data, then a self-attention
    layer to weight the time steps. They found this model performed better than an
    equivalent model using temporal convolutions in the CNN layers. Challa et al. ([Challa2021multi,](#bib.bib213)
    ) proposed using three 1D-CNNs with different kernel sizes in parallel, followed
    by 2 bi-directional LSTM layers and a fully-connected layer. Nafea et al. ([Nafea2021sensor,](#bib.bib218)
    ) also used 1D-CNNs with different kernel sizes and bi-directional LSTMs. However,
    they used separate branches for the CNNs and LSTMs, merging the features extracted
    in each branch for the final fully connected layer. Mekruksavanich and Jitpattanakul ([Mekruksavanich2021lstm,](#bib.bib216)
    ) compared a 4-layer CNN-LSTM model with a smaller CNN-LSTM model and LSTM models,
    finding the extra convolutional layers improved performance over the smaller models.
    DEBONAIR ([Chen2021har,](#bib.bib215) ) is another multi-layered model. It uses
    parallel 1D-CNNs, each having different kernel, filter, and pooling sizes to extract
    different types of features associated with different types of activity. These
    are followed by a combined 1D-CNN, then two LSTM layers. Mekruksavanich and Jitpattanakul ([Mekruksavanich2021biometric,](#bib.bib217)
    ) ensembled four different models: a CNN, an LSTM, a CNN-LSTM, and a ConvLSTM
    model. They aimed to produce a model for boimetric user identification that could
    not only identify the activity being performed, but also the participant performing
    the activity.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的许多研究集中在混合模型上，结合了 CNN 和 RNN。DeepConvLSTM ([Ordonez2016deep,](#bib.bib191))
    包含四个时间卷积层，后接两个 LSTM 层，作者发现它的表现优于等效的 CNN（将 LSTM 层替换为全连接层）。由于 LSTM 层的参数比全连接层少，DeepConvLSTM
    模型也更小。Singh 等人 ([Singh2021convlstm,](#bib.bib219)) 使用 CNN 对空间数据（即每个时间戳的传感器读数）进行编码，然后用一个
    LSTM 层对时间数据进行编码，再用一个自注意力层对时间步骤进行加权。他们发现这种模型的表现优于在 CNN 层中使用时间卷积的等效模型。Challa 等人
    ([Challa2021multi,](#bib.bib213)) 提出了使用三个具有不同卷积核大小的 1D-CNN 并行处理，随后是两个双向 LSTM 层和一个全连接层。Nafea
    等人 ([Nafea2021sensor,](#bib.bib218)) 也使用了不同卷积核大小的 1D-CNN 和双向 LSTM。然而，他们为 CNN 和
    LSTM 使用了不同的分支，将每个分支提取的特征合并用于最终的全连接层。Mekruksavanich 和 Jitpattanakul ([Mekruksavanich2021lstm,](#bib.bib216))
    比较了一个 4 层 CNN-LSTM 模型与一个较小的 CNN-LSTM 模型和 LSTM 模型，发现额外的卷积层提高了性能。DEBONAIR ([Chen2021har,](#bib.bib215))
    是另一个多层模型。它使用并行的 1D-CNN，每个具有不同的卷积核、滤波器和池化大小，以提取与不同类型活动相关的不同特征。接着是一个组合的 1D-CNN，然后是两个
    LSTM 层。Mekruksavanich 和 Jitpattanakul ([Mekruksavanich2021biometric,](#bib.bib217))
    集成了四种不同的模型：CNN、LSTM、CNN-LSTM 和 ConvLSTM 模型。他们的目标是生成一个生物识别用户识别模型，不仅能够识别正在进行的活动，还能识别执行活动的参与者。
- en: A few hybrid models use GRUs instead of LSTMs. InnoHAR ([Xu2019innohar,](#bib.bib211)
    ) is a modified DeepConvLSTM ([Ordonez2016deep,](#bib.bib191) ), replacing the
    four CNN layers with inception layers and the two LSTM layers with GRU layers.
    The authors found this inception model performed better than both the original
    DeepConvLSTM model and a straight CNN model ([Yang2015cnn,](#bib.bib201) ). AttnSense ([Ma2019attnsense,](#bib.bib210)
    ) uses a Fast Fourier transform to generate frequency features which are then
    convolved separately for each time step. Attention layers are used to weight the
    extracted frequency features. These are then passed through a GRU with temporal
    attention to extract temporal features. CNN-BiGRU ([Mekruksavanich2021deep,](#bib.bib214)
    ) uses a CNN layer to extract spatial features from the sensor data, then one
    or more GRU layers extract temporal features. The final section of the model is
    a fully-connected module consisting of one or more hidden layers and a softmax
    output layer.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 一些混合模型使用GRUs代替LSTMs。InnoHAR（[Xu2019innohar,](#bib.bib211)）是一个修改版的DeepConvLSTM（[Ordonez2016deep,](#bib.bib191)），将四层CNN替换为Inception层，并将两层LSTM替换为GRU层。作者发现该Inception模型的表现优于原始的DeepConvLSTM模型和直接的CNN模型（[Yang2015cnn,](#bib.bib201)）。AttnSense（[Ma2019attnsense,](#bib.bib210)）使用快速傅里叶变换生成频率特征，然后分别为每个时间步进行卷积。注意力层用于加权提取的频率特征。这些特征随后通过具有时间注意力的GRU提取时间特征。CNN-BiGRU（[Mekruksavanich2021deep,](#bib.bib214)）使用CNN层从传感器数据中提取空间特征，然后一个或多个GRU层提取时间特征。模型的最后部分是一个全连接模块，由一个或多个隐藏层和一个softmax输出层组成。
- en: 7.2\. Satellite Earth Observation
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 卫星地球观测
- en: 'Ever since NASA launched the first Landsat satellite in 1972 ([Wulder2008landsat,](#bib.bib223)
    ), Earth-observing satellites have been recording images of the Earth’s surface,
    providing 50 years of continuous Earth observation (EO) data that can be used
    to estimate environmental variables informing us about the state of the Earth.
    Instruments on board the satellites record reflected or emitted electromagnetic
    radiation from the Earth’s surface and vegetation ([Emery2017emr,](#bib.bib224)
    ). The regular, repeated observations from these instruments form satellite image
    time series (SITS) that are useful for analysing the dynamic properties of some
    variables, such as plant phenology. The main modalities used for SITS analysis
    are multispectral spectrometers and spectroradiometers, which observe the visible
    and infrared (IR) frequencies and Synthetic Aperture Radar (SAR) systems which
    emit a microwave signal and measure the backscatter. A list of the main satellites
    and instruments used in the studies reviewed is provided in section [C.2](#A3.SS2
    "C.2\. Earth observation satellites and instruments ‣ Appendix C Datasets ‣ Deep
    Learning for Time Series Classification and Extrinsic Regression: A Current Survey")
    of the Appendix.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '自1972年NASA发射首颗Landsat卫星以来（[Wulder2008landsat,](#bib.bib223)），地球观测卫星一直在记录地球表面的图像，提供了50年的连续地球观测（EO）数据，这些数据可以用来估算环境变量，从而告知我们地球的状态。卫星上的仪器记录来自地球表面和植被的反射或辐射的电磁辐射（[Emery2017emr,](#bib.bib224)）。这些仪器的定期、重复观测形成了卫星图像时间序列（SITS），这对分析一些变量的动态特性如植物物候很有用。用于SITS分析的主要方法是多光谱光谱仪和光谱辐射计，这些仪器观察可见光和红外（IR）频率，以及合成孔径雷达（SAR）系统，它们发射微波信号并测量回波。关于在回顾的研究中使用的主要卫星和仪器的列表见附录[C.2](#A3.SS2
    "C.2\. Earth observation satellites and instruments ‣ Appendix C Datasets ‣ Deep
    Learning for Time Series Classification and Extrinsic Regression: A Current Survey")的第七节。'
- en: Raw data collected by satellite instruments needs to be pre-processed before
    being used in machine learning. This is frequently done by the data providers
    to produce analysis ready datasets (ARD). With the increasing availability of
    compatible ARD datasets from sources such as Google Earth Engine ([Gorelick2017gee,](#bib.bib225)
    ) and various data cubes ([Giuliani2017cube,](#bib.bib226) ; [Lewis2017cube,](#bib.bib227)
    ), models combining data from multiple data sources (multi-modal) are becoming
    more common. These data sources make it straightforward to obtain data that are
    co-registered (spatially aligned and with the same resolution and projection),
    thus avoiding the need for complex pre-processing.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 卫星仪器收集的原始数据在用于机器学习之前需要进行预处理。这通常由数据提供者完成，以生成分析准备好的数据集（ARD）。随着来自Google Earth Engine
    ([Gorelick2017gee,](#bib.bib225)) 和各种数据立方体 ([Giuliani2017cube,](#bib.bib226) ;
    [Lewis2017cube,](#bib.bib227)) 的兼容ARD数据集的日益增加，结合来自多个数据源的数据（多模态）变得越来越普遍。这些数据源使得获取共同注册的数据（空间对齐且具有相同分辨率和投影）变得简单，从而避免了复杂的预处理。
- en: Satellite image time series can be processed either (1) as two-dimensional temporal
    and spectral data, processing each pixel independently and ignoring the spatial
    dimensions, or (2) as four-dimensional data, including the two spatial dimensions,
    thus models extract spatio-temporal features. This latter method allows estimates
    to be made at pixel, patch, or object level, however it requires either more complex
    models, or spatial features to be extracted in a pre-processing step. Feature
    extraction can be as simple as extracting the mean value for each band. However,
    both clustering (TASSEL, ([Ienco2020tassel,](#bib.bib228) )), and neural-network
    based methods, such as the Pixel-Set Encoder ([Garnot2020tae,](#bib.bib229) )
    have been used for more complex feature extraction.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 卫星图像时间序列可以处理为（1）二维的时间和光谱数据，独立处理每个像素并忽略空间维度，或（2）四维数据，包括两个空间维度，从而模型提取时空特征。后者方法允许在像素、区域或对象层面进行估计，但需要更复杂的模型，或在预处理步骤中提取空间特征。特征提取可以简单到提取每个波段的平均值。然而，聚类（TASSEL,
    ([Ienco2020tassel,](#bib.bib228))) 和基于神经网络的方法，如Pixel-Set Encoder ([Garnot2020tae,](#bib.bib229))，已被用于更复杂的特征提取。
- en: The most common use of SITS deep learning is for the classification of the Earth’s
    surface by land cover and agricultural land by crop types. The classes used can
    range from very broad land cover categories (such as forest, grasslands, agriculture)
    through to specific crops types. Other classification tasks include identifying
    specific features, such as sink-holes ([Kulshrestha2022hole,](#bib.bib230) ),
    burnt areas ([Ban2020fire,](#bib.bib231) ), flooded areas ([Rambour2020flood,](#bib.bib232)
    ), roads ([KamdemDeTeyou2020road,](#bib.bib233) ), deforestation ([Matosak2022forest,](#bib.bib234)
    ), vegetation quality ([Minh2017vege,](#bib.bib235) ) and forest understory and
    litter types ([Labenski2022under,](#bib.bib236) ).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: SITS深度学习最常见的用途是按土地覆盖和作物类型分类地球表面和农业用地。使用的分类可以从非常广泛的土地覆盖类别（如森林、草地、农业）到具体的作物类型。其他分类任务包括识别特定特征，如沉降坑
    ([Kulshrestha2022hole,](#bib.bib230))、烧焦区域 ([Ban2020fire,](#bib.bib231))、洪水区域
    ([Rambour2020flood,](#bib.bib232))、道路 ([KamdemDeTeyou2020road,](#bib.bib233))、森林砍伐
    ([Matosak2022forest,](#bib.bib234))、植被质量 ([Minh2017vege,](#bib.bib235)) 和森林下层植被及枯枝落叶类型
    ([Labenski2022under,](#bib.bib236))。
- en: Extrinsic regression tasks are less common than classification tasks, but several
    recent studies have investigated methods of estimating water content in vegetation,
    as measured by the variable Live Fuel Moisture Content (LFMC) ([Rao2020lfmc,](#bib.bib237)
    ; [Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239) ; [Xie2022lfmc,](#bib.bib240)
    ). Other regression tasks include estimating the wood volume of forests ([Lahssini2022wood,](#bib.bib241)
    ) by using a hybrid CNN-MLP model combining a time series of Sentinel-2 images
    with a single LiDAR image and crop yield ([Sun2020yield,](#bib.bib242) ) which
    uses a hybrid of CNN and LSTM.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 外在回归任务不如分类任务常见，但最近的几项研究已探讨了估计植被水分含量的方法，这些方法通过变量实时燃料湿度含量（LFMC）来衡量 ([Rao2020lfmc,](#bib.bib237)
    ; [Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239) ; [Xie2022lfmc,](#bib.bib240))。其他回归任务包括通过结合时间序列的Sentinel-2图像和单个LiDAR图像的混合CNN-MLP模型估计森林木材体积
    ([Lahssini2022wood,](#bib.bib241)) 和使用CNN和LSTM的混合模型来预测作物产量 ([Sun2020yield,](#bib.bib242))。
- en: 'Many different approaches to learning from SITS data have been studied, with
    studies using all the main deep learning architectures, adapting them for multi-modal
    learning, and combining architectures in hybrid and ensemble models. The rest
    of this section reviews the architectures that have been used to model SITS data.
    A summary of these papers and architectures is provided in table [6](#S7.T6 "Table
    6 ‣ 7.2\. Satellite Earth Observation ‣ 7\. Applications - recent developments
    and challenges ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey").'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '针对 SITS 数据的学习已经研究了多种不同的方法，包括使用所有主要的深度学习架构，调整它们以适应多模态学习，并在混合和集成模型中组合架构。本节其余部分回顾了用于建模
    SITS 数据的架构。这些论文和架构的总结见表格 [6](#S7.T6 "Table 6 ‣ 7.2\. Satellite Earth Observation
    ‣ 7\. Applications - recent developments and challenges ‣ Deep Learning for Time
    Series Classification and Extrinsic Regression: A Current Survey")。'
- en: '| Model | Year | Embedding | Other features |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 嵌入 | 其他特征 |'
- en: '| Crop type classification |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 作物类型分类 |'
- en: '| TAN ([Li2019tan,](#bib.bib243) ) | 2019 | 2D-CNN & GRU | Attention — temporal
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| TAN ([Li2019tan,](#bib.bib243) ) | 2019 | 2D-CNN & GRU | 注意力 — 时间 |'
- en: '| TGA ([Li2020tga,](#bib.bib244) ) | 2020 | 2D-CNN | Attention — squeeze and
    excitation |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| TGA ([Li2020tga,](#bib.bib244) ) | 2020 | 2D-CNN | 注意力 — 压缩与激励 |'
- en: '| 3D-CNN ([Ji20183dcnn,](#bib.bib245) ) | 2018 | 3D-CNN |  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 3D-CNN ([Ji20183dcnn,](#bib.bib245) ) | 2018 | 3D-CNN |  |'
- en: '| DCM ([Xu2020dcm,](#bib.bib246) ) | 2020 | LSTM | Self-attention |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| DCM ([Xu2020dcm,](#bib.bib246) ) | 2020 | LSTM | 自注意力 |'
- en: '| HierbiLSTM ([Barriere2022lstm,](#bib.bib247) ) | 2022 | LSTM | Self-attention
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| HierbiLSTM ([Barriere2022lstm,](#bib.bib247) ) | 2022 | LSTM | 自注意力 |'
- en: '| L-TAE ([Garnot2020ltae,](#bib.bib248) ) | 2020 | MLP | Attention — temporal
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| L-TAE ([Garnot2020ltae,](#bib.bib248) ) | 2020 | MLP | 注意力 — 时间 |'
- en: '| PSE-TAE ([Garnot2020tae,](#bib.bib229) ; [Ofori-Ampofo2021att,](#bib.bib249)
    ) | 2020 | MLP | Attention — temporal optionally Multi-modal |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| PSE-TAE ([Garnot2020tae,](#bib.bib229) ; [Ofori-Ampofo2021att,](#bib.bib249)
    ) | 2020 | MLP | 注意力 — 时间（可选）多模态 |'
- en: '| SITS-BERT ([Yuan2021sitsbert,](#bib.bib250) ) | 2021 |  | Pre-trained transformer
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| SITS-BERT ([Yuan2021sitsbert,](#bib.bib250) ) | 2021 |  | 预训练变换器 |'
- en: '| Land Cover classification |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 土地覆盖分类 |'
- en: '| 1D-CNN ([DiMauro2017tiselc,](#bib.bib251) ) | 2017 | 1D-CNN & MLP | Hybrid
    model |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 1D-CNN ([DiMauro2017tiselc,](#bib.bib251) ) | 2017 | 1D-CNN & MLP | 混合模型
    |'
- en: '| 1D & 2D-CNNs ([Kussul2017cnn,](#bib.bib252) ) | 2017 | 1D-CNN; 2D-CNN | Ensemble
    model |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 1D & 2D-CNNs ([Kussul2017cnn,](#bib.bib252) ) | 2017 | 1D-CNN；2D-CNN | 集成模型
    |'
- en: '| TempCNN ([Pelletier2019tempcnn,](#bib.bib253) ) | 2019 | 1D-CNN |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| TempCNN ([Pelletier2019tempcnn,](#bib.bib253) ) | 2019 | 1D-CNN |  |'
- en: '| TASSEL ([Ienco2020tassel,](#bib.bib228) ) | 2020 | 1D-CNN | Self-attention
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| TASSEL ([Ienco2020tassel,](#bib.bib228) ) | 2020 | 1D-CNN | 自注意力 |'
- en: '| TSI ([Dou2021tsi,](#bib.bib254) ) | 2021 | 1D-CNN; LSTM | Ensemble model
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| TSI ([Dou2021tsi,](#bib.bib254) ) | 2021 | 1D-CNN；LSTM | 集成模型 |'
- en: '| TWINNS ([Ienco2019twinns,](#bib.bib255) ) | 2019 | 2D-CNN & GRU | Attention
    — temporal; Multi-modal |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| TWINNS ([Ienco2019twinns,](#bib.bib255) ) | 2019 | 2D-CNN & GRU | 注意力 — 时间；多模态
    |'
- en: '| DuPLO ([Interdonato2019duplo,](#bib.bib256) ) | 2019 | 2D-CNN & GRU | Attention
    — temporal |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| DuPLO ([Interdonato2019duplo,](#bib.bib256) ) | 2019 | 2D-CNN & GRU | 注意力
    — 时间 |'
- en: '| Sequential RNN ([Russwurm2018seqrnn,](#bib.bib257) ) | 2018 | 2D-FCN & LSTM
    | Hybrid model |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 顺序 RNN ([Russwurm2018seqrnn,](#bib.bib257) ) | 2018 | 2D-FCN & LSTM | 混合模型
    |'
- en: '| FG-UNET ([Stoian2019fgunet,](#bib.bib258) ) | 2019 | UNet & 2D-CNN | Hybrid
    model |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| FG-UNET ([Stoian2019fgunet,](#bib.bib258) ) | 2019 | UNet & 2D-CNN | 混合模型
    |'
- en: '| LSTM ([Ienco2017rnn,](#bib.bib259) ) | 2017 | LSTM |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| LSTM ([Ienco2017rnn,](#bib.bib259) ) | 2017 | LSTM |  |'
- en: '| HOb2sRNN ([Gbodjo2020hob2srnn,](#bib.bib260) ) | 2020 | GRU | Attention —
    temporal |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| HOb2sRNN ([Gbodjo2020hob2srnn,](#bib.bib260) ) | 2020 | GRU | 注意力 — 时间 |'
- en: '| OD2RNN ([Ienco2019od2rnn,](#bib.bib261) ) | 2019 | GRU | Attention — temporal;
    Multi-modal |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| OD2RNN ([Ienco2019od2rnn,](#bib.bib261) ) | 2019 | GRU | 注意力 — 时间；多模态 |'
- en: '| SITS-Former ([Yuan2022sitsformer,](#bib.bib262) ) | 2022 | 3D-CNN | Pre-trained
    transformer |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| SITS-Former ([Yuan2022sitsformer,](#bib.bib262) ) | 2022 | 3D-CNN | 预训练变换器
    |'
- en: '| Other classification tasks |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 其他分类任务 |'
- en: '| Deforestation ([Matosak2022forest,](#bib.bib234) ) | 2022 | U-Net & LSTM
    | Hybrid model |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 森林砍伐 ([Matosak2022forest,](#bib.bib234) ) | 2022 | U-Net & LSTM | 混合模型 |'
- en: '| Flood detection ([Rambour2020flood,](#bib.bib232) ) | 2020 | Resnet & GRU
    | Hybrid model |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 洪水检测 ([Rambour2020flood,](#bib.bib232) ) | 2020 | Resnet & GRU | 混合模型 |'
- en: '| Forest understory ([Labenski2022under,](#bib.bib236) ) | 2022 | 2D-CNN &
    LSTM | Ensemble model |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 森林下层 ([Labenski2022under,](#bib.bib236) ) | 2022 | 2D-CNN & LSTM | 集成模型 |'
- en: '| Road detection ([KamdemDeTeyou2020road,](#bib.bib233) ) | 2020 | U-Net &
    convLSTM | Hybrid model |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 道路检测 ([KamdemDeTeyou2020road,](#bib.bib233) ) | 2020 | U-Net & convLSTM |
    混合模型 |'
- en: '| Vegetation quality ([Minh2017vege,](#bib.bib235) ) | 2017 | LSTM; GRU |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 植被质量 ([Minh2017vege,](#bib.bib235) ) | 2017 | LSTM；GRU |  |'
- en: '| Extrinsic regression tasks |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 外部回归任务 |'
- en: '| TempCNN-LFMC ([Zhu2020lfmc,](#bib.bib238) ) | 2021 | 1D-CNN |  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| TempCNN-LFMC ([Zhu2020lfmc,](#bib.bib238) ) | 2021 | 1D-CNN |  |'
- en: '| Multi-tempCNN ([Miller2022lfmc,](#bib.bib239) ) | 2022 | 1D-CNN | Multi-modal,
    ensemble model |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Multi-tempCNN ([Miller2022lfmc,](#bib.bib239) ) | 2022 | 1D-CNN | 多模态、集成模型
    |'
- en: '| LFMC estimation ([Rao2020lfmc,](#bib.bib237) ) | 2020 | LSTM | Multi-modal
    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| LFMC 估计 ([Rao2020lfmc,](#bib.bib237) ) | 2020 | LSTM | 多模态 |'
- en: '| LFMC estimation ([Xie2022lfmc,](#bib.bib240) ) | 2022 | 1D-CNN & LSTM | Multi-modal,
    hybrid, ensemble |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| LFMC 估计 ([Xie2022lfmc,](#bib.bib240) ) | 2022 | 1D-CNN & LSTM | 多模态、混合、集成
    |'
- en: '| MLDL-net ([Sun2020yield,](#bib.bib242) ) | 2020 | 2D-CNN & LSTM | Hybrid
    model |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| MLDL-net ([Sun2020yield,](#bib.bib242) ) | 2020 | 2D-CNN & LSTM | 混合模型 |'
- en: '| SSTNN ([Qiao2021yield,](#bib.bib263) ) | 2021 | 3D-CNN & LSTM | Hybrid model
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| SSTNN ([Qiao2021yield,](#bib.bib263) ) | 2021 | 3D-CNN & LSTM | 混合模型 |'
- en: '| MMFVE ([Lahssini2022wood,](#bib.bib241) ) | 2022 | 2D-CNN | Hybrid model
    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| MMFVE ([Lahssini2022wood,](#bib.bib241) ) | 2022 | 2D-CNN | 混合模型 |'
- en: Table 6\. Summary of SITS deep learning models
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. SITS 深度学习模型汇总
- en: 7.2.1\. Recurrent Neural Networks (RNNs)
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 循环神经网络 (RNNs)
- en: One of the first papers to use RNNs for land cover classification was Ienco
    et al. ([Ienco2017rnn,](#bib.bib259) ), who showed an LSTM model out-performed
    non deep learning methods such as Random Forest (RF) and Support Vector Machines
    (SVM). However, they also showed that the performance of both RF and SVM improves
    if trained on features extracted by the LSTM model, and in some cases were more
    accurate than the straight LSTM model. Rao et al. ([Rao2020lfmc,](#bib.bib237)
    ) used an extrinsic regression LSTM model to estimate LFMC in the western United
    States.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一篇使用 RNN 进行土地覆盖分类的早期论文是 Ienco 等人 ([Ienco2017rnn,](#bib.bib259) )，他们展示了 LSTM
    模型的表现优于随机森林 (RF) 和支持向量机 (SVM) 等非深度学习方法。然而，他们也表明，如果在由 LSTM 模型提取的特征上训练，RF 和 SVM
    的性能会有所提高，有时比直接使用 LSTM 模型更为准确。Rao 等人 ([Rao2020lfmc,](#bib.bib237) ) 使用了一种外部回归 LSTM
    模型来估计美国西部的 LFMC。
- en: More commonly, however, RNNs are combined with an attention layer to allow the
    model to focus on the most important time steps. The OD2RNN model ([Ienco2019od2rnn,](#bib.bib261)
    ), used separate GRU layers followed by attention layers to process Sentinel-1
    and Sentinel-2 data, combining the features extracted by each source for the final
    fully-connected layers. HOb2sRNN ([Gbodjo2020hob2srnn,](#bib.bib260) ) refined
    OD2RNN by using a hierarchy of land cover classifications; the model was pretrained
    using broad land cover classifications, then further trained using the finer-grained
    classifications. DCM ([Xu2020dcm,](#bib.bib246) ) and HierbiLSTM ([Barriere2022lstm,](#bib.bib247)
    ) both use a bi-directional LSTM, processing the time series in both directions,
    followed by a self-attention transformer for a pixel-level crop-mapping model.
    All these studies found adding the attention layers improved model performance
    over a straight GRU or LSTM model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的是，RNN 被与注意力层结合，以使模型能够专注于最重要的时间步骤。OD2RNN 模型 ([Ienco2019od2rnn,](#bib.bib261)
    ) 使用了独立的 GRU 层，之后是注意力层来处理 Sentinel-1 和 Sentinel-2 数据，将每个源提取的特征结合到最终的全连接层中。HOb2sRNN
    ([Gbodjo2020hob2srnn,](#bib.bib260) ) 通过使用土地覆盖分类的层次结构来改进 OD2RNN；该模型首先使用广泛的土地覆盖分类进行预训练，然后使用更细化的分类进一步训练。DCM
    ([Xu2020dcm,](#bib.bib246) ) 和 HierbiLSTM ([Barriere2022lstm,](#bib.bib247) )
    都使用双向 LSTM，处理双向时间序列，然后使用自注意力变换器进行像素级作物映射模型。所有这些研究发现，添加注意力层可以改善模型性能，相比直接使用 GRU
    或 LSTM 模型有显著提升。
- en: 7.2.2\. Convolutional Neural Networks (CNNs)
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2\. 卷积神经网络 (CNNs)
- en: While many authors have claimed that RNNs out-perform CNNs for land cover and
    crop type classification, most of these comparisons are to 2-dimensional CNNs
    (2D-CNN), that ignore the temporal ordering of SITS data ([Pelletier2019tempcnn,](#bib.bib253)
    ). However, other studies show using 1-dimensional CNNs (1D-CNNs) to extract temporal
    information or 3-dimensional CNNs (3D-CNNs) to extract spatio-temporal information
    are both effective methods of learning from SITS data. TempCNN ([Pelletier2019tempcnn,](#bib.bib253)
    ) consists of three 1D convolutional layers. The output from the final convolutional
    layer is passed through a fully-connected layer, then the final softmax classification
    layer. TASSEL ([Ienco2020tassel,](#bib.bib228) ), is an adaptation of TempCNN
    for OBIA classification, using TempCNN models to process features extracted from
    the objects, followed by an attention layer to weight the convolved features.
    TempCNN has also been adapted for extrinsic regression ([Zhu2020lfmc,](#bib.bib238)
    ) and used for LFMC estimation ([Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239)
    ; [Xie2022lfmc,](#bib.bib240) ).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多作者声称RNN在土地覆盖和作物类型分类上优于CNN，但大多数这些比较都是针对忽略SITS数据时间顺序的二维CNN（2D-CNN）进行的 ([Pelletier2019tempcnn,](#bib.bib253)
    )。然而，其他研究表明，使用一维CNN（1D-CNN）提取时间信息或使用三维CNN（3D-CNN）提取时空信息都是从SITS数据中学习的有效方法。TempCNN ([Pelletier2019tempcnn,](#bib.bib253)
    ) 由三层1D卷积层组成。最终卷积层的输出通过一个全连接层，然后经过最终的softmax分类层。TASSEL ([Ienco2020tassel,](#bib.bib228)
    ) 是对TempCNN的适配，用于OBIA分类，使用TempCNN模型处理从对象中提取的特征，随后通过注意力层加权卷积特征。TempCNN还被适配用于外部回归 ([Zhu2020lfmc,](#bib.bib238)
    ) 并用于LFMC估计 ([Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239) ; [Xie2022lfmc,](#bib.bib240)
    )。
- en: 2D-CNNs are mainly used to extract spatial or spatio-temporal features for both
    pixel and object classification. The model input is usually 4-dimensional and
    the data is convolved spatially, with two main methods used to handle the temporal
    dimension. In the first method, each time step is convolved separately and the
    extracted features are merged in later stages of the model ([Li2020tga,](#bib.bib244)
    ). In the second method, the time steps and channels are flattened to form a large
    multivariate image ([Kussul2017cnn,](#bib.bib252) ; [Lahssini2022wood,](#bib.bib241)
    ). FG-UNet ([Stoian2019fgunet,](#bib.bib258) ) is a fully-convolutional model
    that combines both the above methods, first grouping time steps by threes to produce
    images with 30 channels (10 spectral $\times$ 3 temporal), which are passed through
    both U-Net and 2D-CNN layers.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 2D-CNN主要用于提取空间或时空特征，用于像素和对象分类。模型输入通常是4维的，数据在空间上进行卷积，主要有两种方法用于处理时间维度。在第一种方法中，每个时间步单独进行卷积，并在模型的后续阶段合并提取的特征 ([Li2020tga,](#bib.bib244)
    )。在第二种方法中，将时间步和通道展平形成大型多变量图像 ([Kussul2017cnn,](#bib.bib252) ; [Lahssini2022wood,](#bib.bib241)
    )。FG-UNet ([Stoian2019fgunet,](#bib.bib258) ) 是一个完全卷积的模型，结合了上述两种方法，首先将时间步分组为三，生成具有30个通道（10光谱
    $\times$ 3时间）的图像，然后通过U-Net和2D-CNN层进行处理。
- en: Ji et al. ([Ji20183dcnn,](#bib.bib245) ) used a three-dimensional CNN (3D-CNN)
    to convolve the spatial and temporal dimensions together, combining the strengths
    of 1D-CNN and 2D-CNNs. The study found a 3D-CNN crop classification model performed
    significantly better than the 2D-CNN, again showing the importance of the temporal
    features. Another study, SSTNN ([Qiao2021yield,](#bib.bib263) ) obtained good
    results for crop yield prediction by using a 3D-CNN to convolve the spatial and
    spectral dimensions, extracting spatio-spectral features for each time step. These
    features were then processed by LSTM layers to perform the temporal modelling.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Ji 等人 ([Ji20183dcnn,](#bib.bib245) ) 使用了三维卷积神经网络（3D-CNN）将空间和时间维度进行卷积，结合了1D-CNN和2D-CNN的优势。研究发现，3D-CNN作物分类模型的表现显著优于2D-CNN，再次显示了时间特征的重要性。另一项研究，SSTNN ([Qiao2021yield,](#bib.bib263)
    ) 通过使用3D-CNN卷积空间和光谱维度，为每个时间步提取了时空光谱特征，从而获得了良好的作物产量预测结果。这些特征随后由LSTM层处理以进行时间建模。
- en: 7.2.3\. Transformer and Attention Models
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3\. Transformer 和注意力模型
- en: As an alternative to including attention layers with a CNN or RNN, several studies
    have designed models that process temporal information using only attention layers.
    PSE-TAE ([Garnot2020tae,](#bib.bib229) ) used a modified transformer called a
    temporal attention encoder (TAE) for crop mapping and found the TAE performed
    better than either a CNN or an RNN. L-TAE ([Garnot2020ltae,](#bib.bib248) ) replaced
    the TAE with a light-weight transformer which is both computationally efficient
    and more accurate than the full TAE. Ofori-Ampofo et al. ([Ofori-Ampofo2021att,](#bib.bib249)
    ) adapted the TAE model for multi-modal inputs, using Sentinel-1 and Sentinel-2
    data for crop type mapping. Rußwurm and Körner([Russwurm2020att,](#bib.bib264)
    ) compared a self-attention model with RNN and CNN architectures. They found that
    this model was more robust to noise than either RNN or CNN and suggested self-attention
    is suitable for processing raw, cloud-affected satellite data.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 作为包含注意力层的 CNN 或 RNN 的替代方案，一些研究设计了仅使用注意力层处理时间信息的模型。PSE-TAE ([Garnot2020tae,](#bib.bib229)
    ) 使用了一种称为时间注意力编码器（TAE）的修改版变压器进行作物映射，并发现 TAE 的表现优于 CNN 或 RNN。L-TAE ([Garnot2020ltae,](#bib.bib248)
    ) 用一种轻量级的变压器取代了 TAE，这种变压器在计算效率和准确性上都优于完整的 TAE。Ofori-Ampofo 等人 ([Ofori-Ampofo2021att,](#bib.bib249)
    ) 将 TAE 模型调整为多模态输入，利用 Sentinel-1 和 Sentinel-2 数据进行作物类型映射。Rußwurm 和 Körner([Russwurm2020att,](#bib.bib264)
    ) 比较了自注意力模型与 RNN 和 CNN 架构。他们发现该模型对噪声的鲁棒性优于 RNN 或 CNN，并建议自注意力适合处理原始的、受云影响的卫星数据。
- en: Building on the success of pre-trained transformers for natural language processing
    (NLP) such as BERT ([devlin2018bert,](#bib.bib93) ), pre-trained transformers
    have been proposed for EO tasks ([Yuan2021sitsbert,](#bib.bib250) ). Earth observation
    tasks are particularly suited for pre-trained models as large quantities of EO
    data are readily available, while labelled data can be difficult to obtain ([Tuia2016da,](#bib.bib265)
    ), especially in remote locations. SITS-BERT ([Yuan2021sitsbert,](#bib.bib250)
    ) is an adaptation of BERT ([devlin2018bert,](#bib.bib93) ) for pixel-based SITS
    classification. For the pretext task, random noise is added to the pixels, and
    the model is trained to identify and remove this noise. The pre-trained model
    is then further trained for required tasks such as crop type or land cover mapping.
    SITS-Former ([Yuan2022sitsformer,](#bib.bib262) ) modifies SITS-BERT for patch
    classification by using 3D-Conv layers to encode the spatial-spectral information,
    which is then passed through the temporal attention layers. The pretext task used
    for SITS-Former is to predict randomly masked pixels.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练变压器（如 BERT ([devlin2018bert,](#bib.bib93) )）在自然语言处理（NLP）中的成功基础上，已经提出了用于地球观测（EO）任务的预训练变压器 ([Yuan2021sitsbert,](#bib.bib250)
    )。地球观测任务特别适合使用预训练模型，因为大量的 EO 数据随时可用，而标记数据可能难以获取 ([Tuia2016da,](#bib.bib265) )，尤其是在偏远地区。SITS-BERT ([Yuan2021sitsbert,](#bib.bib250)
    ) 是 BERT ([devlin2018bert,](#bib.bib93) ) 的一种改编，专用于基于像素的 SITS 分类。对于前置任务，在像素中添加随机噪声，并训练模型识别和去除这些噪声。然后，进一步训练预训练模型以完成诸如作物类型或土地覆盖映射等任务。SITS-Former ([Yuan2022sitsformer,](#bib.bib262)
    ) 修改了 SITS-BERT 以进行块分类，通过使用 3D-Conv 层来编码空间-光谱信息，然后将其传递通过时间注意力层。SITS-Former 使用的前置任务是预测随机掩盖的像素。
- en: 7.2.4\. Hybrid Models
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.4\. 混合模型
- en: A common use of hybrid models is to use a CNN to extract spatial features and
    an RNN to extract temporal features. Garnot et al. ([Garnot2019time,](#bib.bib266)
    ) compared a straight 2D-CNN model (thus ignoring the temporal aspect), a straight
    GRU model (thus ignoring the spatial aspect) and a combined 2D-CNN and GRU model
    (thus using both spatial and temporal information) and found the combined model
    gave the best results, demonstrating that both the spatial and temporal dimensions
    provide useful information for land cover mapping and crop classification. DuPLO ([Interdonato2019duplo,](#bib.bib256)
    ) was one of the first models to exploit this method, running a CNN and ConvGRU
    model in parallel, then fusing the outputs using a fully-connected network for
    the final classifier. During training, an auxiliary classifier for each component
    was used to enhance the discriminative power. TWINNS ([Ienco2019twinns,](#bib.bib255)
    ) extended DuPLO to a multi-modal model, using time series of both Sentinel-1
    (SAR) and Sentinel-2 (Optical) images. Each modality was processed by separate
    CNN and convGRU models, then the output features from all four models were fused
    for classification.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型的一个常见用途是使用 CNN 提取空间特征，使用 RNN 提取时间特征。Garnot 等人 ([Garnot2019time,](#bib.bib266)
    ) 比较了一个纯 2D-CNN 模型（因此忽略了时间方面），一个纯 GRU 模型（因此忽略了空间方面）和一个结合 2D-CNN 和 GRU 的模型（因此同时使用空间和时间信息），发现结合模型给出了最佳结果，证明了空间和时间维度都提供了有用的信息用于土地覆盖映射和作物分类。DuPLO ([Interdonato2019duplo,](#bib.bib256)
    ) 是第一个利用这种方法的模型之一，平行运行 CNN 和 ConvGRU 模型，然后使用全连接网络融合输出作为最终分类器。在训练期间，使用了每个组件的辅助分类器来增强区分能力。TWINNS ([Ienco2019twinns,](#bib.bib255)
    ) 将 DuPLO 扩展为一个多模态模型，使用 Sentinel-1 (SAR) 和 Sentinel-2 (光学) 图像的时间序列。每种模态通过单独的 CNN
    和 convGRU 模型处理，然后融合所有四个模型的输出特征进行分类。
- en: Other hybrid models include Li et al. ([Li2019tan,](#bib.bib243) ), who used
    a CNN for spatial and spectral unification of Landsat-8 and Sentinel-2 images
    which were then processed by a GRU. MLDL-Net ([Sun2020yield,](#bib.bib242) ) is
    a 2D-CNN extrinsic regression model, using CNNs to extract time step features,
    which are then passed through an LSTM model to extract temporal features. Fully
    connected layers combine the feature sets to predict crop yield. Rußwurm and Körner ([Russwurm2018seqrnn,](#bib.bib257)
    ) extracted temporal features first, using a bi-directional LSTM, then used a
    fully-convolutional 2D-CNN to incorporate spatial information and classify each
    pixel in the input patch.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 其他混合模型包括 Li 等人 ([Li2019tan,](#bib.bib243) )，他们使用 CNN 对 Landsat-8 和 Sentinel-2
    图像进行空间和光谱统一，然后由 GRU 进行处理。MLDL-Net ([Sun2020yield,](#bib.bib242) ) 是一个 2D-CNN 外部回归模型，使用
    CNN 提取时间步特征，然后通过 LSTM 模型提取时间特征。全连接层将特征集合并以预测作物产量。Rußwurm 和 Körner ([Russwurm2018seqrnn,](#bib.bib257)
    ) 首先使用双向 LSTM 提取时间特征，然后使用全卷积 2D-CNN 来融入空间信息，并对输入块中的每个像素进行分类。
- en: 7.2.5\. Ensemble Models
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.5\. 集成模型
- en: One of the easiest ways to ensemble DL models is to train multiple homogeneous
    models, that vary only in the random weight initialisation ([Fawaz2019ensembles,](#bib.bib267)
    ). Di Mauro et al. ([DiMauro2017tiselc,](#bib.bib251) ) ensembled 100 LULC models
    with different weight initialisations by averaging the softmax predictions. They
    found this produced a more stable and stronger classifier that outperformed the
    individual models. Multi-tempCNN ([Miller2022lfmc,](#bib.bib239) ), a model for
    LFMC estimation, is an ensemble of homogeneous models for extrinsic regression.
    The authors suggested that as an additional benefit, the variance of the individual
    model predictions can be used to obtain a measure of uncertainty of the estimates.
    TSI ([Dou2021tsi,](#bib.bib254) ) also ensembles a set of homogeneous models,
    but instead of relying on random weight initialisation to introduce model diversity,
    the time series are segmented and models trained on each segment.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 集成深度学习模型的一个最简单方法是训练多个同质模型，这些模型仅在随机权重初始化上有所不同 ([Fawaz2019ensembles,](#bib.bib267)
    )。Di Mauro 等人 ([DiMauro2017tiselc,](#bib.bib251) ) 通过平均 softmax 预测，将 100 个不同权重初始化的
    LULC 模型进行集成。他们发现这样可以产生一个更稳定、更强的分类器，超越了单独的模型。Multi-tempCNN ([Miller2022lfmc,](#bib.bib239)
    ) 是一个用于 LFMC 估计的模型，它是一个用于外部回归的同质模型集成。作者还建议，作为额外的好处，单个模型预测的方差可以用来获得估计的不确定性测量。TSI ([Dou2021tsi,](#bib.bib254)
    ) 也集成了一组同质模型，但不是依赖于随机权重初始化来引入模型多样性，而是将时间序列进行分段，并在每个段上训练模型。
- en: Other methods create ensembles of heterogeneous models. Kussul et al ([Kussul2017cnn,](#bib.bib252)
    ) compared ensembles of 1D-CNNs and 2D-CNNs models for land cover classification.
    Each model in the ensemble used a different number of filters, so finding different
    feature sets useful for classification. Xie et al. ([Xie2022lfmc,](#bib.bib240)
    ) ensembled three heterogeneous models — a causal temporal convolutional neural
    network (TCN), an LSTM, and a hybrid TCN-LSTM model — for an extrinsic regression
    model to estimate LFMC. The ensembles were created using stacking ([Wolpert1992stack,](#bib.bib268)
    ). The authors compared this method to boosting their TCN-LSTM model, using Adaboost ([Freund1996ada,](#bib.bib269)
    ) to create a three-member ensemble, and found that stacking a diverse set of
    models out-performed boosting.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法创建了异质模型的集成。Kussul等人 ([Kussul2017cnn,](#bib.bib252)) 比较了用于土地覆盖分类的1D-CNN和2D-CNN模型的集成。集成中的每个模型使用了不同数量的滤波器，因此找到不同的特征集对分类有用。Xie等人
    ([Xie2022lfmc,](#bib.bib240)) 集成了三种异质模型——因果时间卷积神经网络（TCN）、LSTM和混合TCN-LSTM模型——用于估计LFMC的外部回归模型。集成使用了堆叠方法
    ([Wolpert1992stack,](#bib.bib268))。作者将此方法与提升其TCN-LSTM模型的方法进行了比较，使用Adaboost ([Freund1996ada,](#bib.bib269))
    创建了一个三成员的集成，并发现堆叠多样化模型集的表现优于提升方法。
- en: 7.2.6\. EO Surveys and Reviews
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.6\. EO调查和回顾
- en: This survey is one of very few that include a section focusing specifically
    on deep learning TSC and TSER tasks using SITS data. However, there are other
    reviews that provide further information about related topics. Gomez et al. ([Gomez2016review,](#bib.bib270)
    ) is an older review highlighting the importance role of SITS data for land cover
    classification. Zhu et al. ([Zhu2017review,](#bib.bib271) ) reviewed the advances
    and challenges in DL for remote sensing, and the resources available that are
    potentially useful to help DL address some of the major challenges facing humanity.
    Ma et al. ([Ma2019review,](#bib.bib272) ) studies the role of deep learning in
    Earth observation using remotely sensed data. It covers a broad range of tasks
    including image fusion, image segmentation and object-based analysis, as well
    as classification tasks. Yuan et al. ([Yuan2020review,](#bib.bib273) ) provide
    a review of DL applications for remote sensing, comparing the role of DL versus
    physical modelling of environmental variables and highlighting challenges in DL
    for remote sensing that need to be addressed. Chaves et al. ([Chaves2020review,](#bib.bib274)
    ) reviewed recent research using Landsat 8 and/or Sentinel-2 data for land cover
    mapping. While not focused on SITS DL methods, the review notes the growing importance
    of these methods. Moskolai et al. ([Moskolai2021apps,](#bib.bib275) ) is a review
    of forecasting applications using DL with SITS data that provides an analysis
    of the main DL architectures that are relevant for classification as well as forecasting.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这项调查是为数不多的专门关注使用SITS数据进行深度学习TSC和TSER任务的文献之一。然而，还有其他评论提供了关于相关主题的更多信息。Gomez等人
    ([Gomez2016review,](#bib.bib270)) 是一篇较早的评论，强调了SITS数据在土地覆盖分类中的重要作用。Zhu等人 ([Zhu2017review,](#bib.bib271))
    回顾了深度学习在遥感中的进展和挑战，以及可用的资源，这些资源可能对帮助深度学习解决一些人类面临的主要挑战有帮助。Ma等人 ([Ma2019review,](#bib.bib272))
    研究了深度学习在地球观测中使用遥感数据的作用，涵盖了包括图像融合、图像分割、基于对象的分析以及分类任务在内的广泛任务。Yuan等人 ([Yuan2020review,](#bib.bib273))
    提供了对遥感深度学习应用的回顾，比较了深度学习与环境变量物理建模的作用，并强调了需要解决的遥感深度学习中的挑战。Chaves等人 ([Chaves2020review,](#bib.bib274))
    回顾了使用Landsat 8和/或Sentinel-2数据进行土地覆盖制图的最新研究。虽然不专注于SITS深度学习方法，但该评论指出了这些方法日益重要的趋势。Moskolai等人
    ([Moskolai2021apps,](#bib.bib275)) 是一篇使用SITS数据进行预测应用的评论，分析了与分类和预测相关的主要深度学习架构。
- en: 8\. Conclusion
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: In conclusion, this survey paper has discussed a variety of deep network architectures
    for time series classification and extrinsic regression tasks, including multilayer
    perceptrons, convolutional neural networks, recurrent neural networks, and attention-based
    models. We have also highlighted refinements that have been made to improve the
    performance of these models on time series tasks. Additionally, we have discussed
    two critical applications of time series classification and regression, human
    activity recognition and satellite Earth observation. Overall, using deep network
    architectures and refinements has enabled significant progress in the field of
    time series classification and will continue to be essential for addressing a
    wide range of real-world problems. We hope this survey will stimulate further
    research using deep learning techniques for time series classification and extrinsic
    regression. Additionally, we provide a carefully curated collection of sources,
    available at [https://github.com/Navidfoumani/TSC_Survey](https://github.com/Navidfoumani/TSC_Survey),
    to further support the research community.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，这篇综述论文讨论了多种用于时间序列分类和外部回归任务的深度网络架构，包括多层感知器、卷积神经网络、递归神经网络和基于注意力的模型。我们还突出了为提高这些模型在时间序列任务中的表现所做的改进。此外，我们讨论了时间序列分类和回归的两个关键应用：人类活动识别和卫星地球观测。总体而言，使用深度网络架构和改进已经在时间序列分类领域取得了显著进展，并将继续在解决各种现实问题中发挥重要作用。我们希望这篇综述能够激发进一步使用深度学习技术进行时间序列分类和外部回归的研究。此外，我们提供了精心策划的资源集合，供研究社区进一步支持，[https://github.com/Navidfoumani/TSC_Survey](https://github.com/Navidfoumani/TSC_Survey)。
- en: Acknowledgements.
  id: totrans-347
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: This work was supported by an Australian Government Research Training Program
    (RTP) scholarship.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了澳大利亚政府研究培训计划（RTP）奖学金的资助。
- en: References
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Q. Yang and X. Wu, “10 challenging problems in data mining research,” *Int.
    J. Inf. Tech. & Decision Making*, vol. 5, no. 04, pp. 597–604, 2006.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Q. Yang 和 X. Wu, “数据挖掘研究中的10个挑战性问题，” *国际信息技术与决策制定期刊*, 第5卷，第04期，第597–604页,
    2006年。
- en: (2) P. Esling and C. Agon, “Time-series data mining,” *ACM Computing Surveys
    (CSUR)*, vol. 45, no. 1, pp. 1–34, 2012.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) P. Esling 和 C. Agon, “时间序列数据挖掘，” *ACM计算机调查（CSUR）*, 第45卷，第1期，第1–34页, 2012年。
- en: '(3) H. F. Nweke, Y. W. Teh, M. A. Al-Garadi, and U. R. Alo, “Deep learning
    algorithms for human activity recognition using mobile and wearable sensor networks:
    State of the art and research challenges,” *Expert Systems with Applications*,
    vol. 105, pp. 233–261, 2018.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) H. F. Nweke, Y. W. Teh, M. A. Al-Garadi, 和 U. R. Alo, “基于移动和可穿戴传感器网络的人类活动识别深度学习算法：现状与研究挑战，”
    *专家系统与应用*, 第105卷，第233–261页, 2018年。
- en: '(4) J. Wang, Y. Chen, S. Hao, X. Peng, and L. Hu, “Deep learning for sensor-based
    activity recognition: A survey,” *Pattern recognition letters*, vol. 119, pp.
    3–11, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) J. Wang, Y. Chen, S. Hao, X. Peng, 和 L. Hu, “传感器基础活动识别的深度学习：综述，” *模式识别快报*,
    第119卷，第3–11页, 2019年。
- en: '(5) K. Chen, D. Zhang, L. Yao, B. Guo, Z. Yu, and Y. Liu, “Deep learning for
    sensor-based human activity recognition: Overview, challenges, and opportunities,”
    *ACM Computing Surveys (CSUR)*, vol. 54, no. 4, pp. 1–40, 2021.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) K. Chen, D. Zhang, L. Yao, B. Guo, Z. Yu, 和 Y. Liu, “传感器基础人类活动识别的深度学习：概述、挑战与机遇，”
    *ACM计算机调查（CSUR）*, 第54卷，第4期，第1–40页, 2021年。
- en: (6) R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,
    K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball, “Deep learning
    with convolutional neural networks for EEG decoding and visualization,” *Human
    brain mapping*, vol. 38, no. 11, pp. 5391–5420, 2017.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,
    K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, 和 T. Ball, “使用卷积神经网络进行EEG解码和可视化的深度学习，”
    *人脑映射*, 第38卷，第11期，第5391–5420页, 2017年。
- en: (7) A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
    X. Liu, J. Marcus, M. Sun *et al.*, “Scalable and accurate deep learning with
    electronic health records,” *NPJ digital medicine*, vol. 1, no. 1, pp. 1–10, 2018.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu,
    X. Liu, J. Marcus, M. Sun *等*, “使用电子健康记录的可扩展和准确的深度学习，” *NPJ数字医学*, 第1卷，第1期，第1–10页,
    2018年。
- en: (8) A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom, P. Southam,
    and E. Keogh, “The UEA multivariate time series classification archive, 2018,”
    *arXiv preprint:1811.00075*, 2018.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) A. Bagnall, H. A. Dau, J. Lines, M. Flynn, J. Large, A. Bostrom, P. Southam,
    和 E. Keogh, “UEA多变量时间序列分类档案，2018年，” *arXiv预印本:1811.00075*, 2018年。
- en: (9) H. A. Dau, A. Bagnall, K. Kamgar, C.-C. M. Yeh, Y. Zhu, S. Gharghabi, C. A.
    Ratanamahatana, and E. Keogh, “The UCR time series archive,” *IEEE/CAA Journal
    of Automatica Sinica*, vol. 6, no. 6, pp. 1293–1305, 2019.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) H. A. Dau, A. Bagnall, K. Kamgar, C.-C. M. Yeh, Y. Zhu, S. Gharghabi, C.
    A. Ratanamahatana, 和 E. Keogh，“UCR时间序列档案，” *IEEE/CAA自动化学报*，第6卷，第6期，页1293–1305，2019年。
- en: (10) C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb, “Time series extrinsic
    regression,” *Data Min. Knowl. Discov.*, vol. 35, no. 3, pp. 1032–1060, 2021.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) C. W. Tan, C. Bergmeir, F. Petitjean, 和 G. I. Webb，“时间序列外在回归，” *数据挖掘与知识发现*，第35卷，第3期，页1032–1060，2021年。
- en: '(11) M. Middlehurst, P. Schäfer, and A. Bagnall, “Bake off redux: a review
    and experimental evaluation of recent time series classification algorithms,”
    *arXiv preprint arXiv:2304.13029*, 2023.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) M. Middlehurst, P. Schäfer, 和 A. Bagnall，“比赛回顾：对近期时间序列分类算法的综述和实验评估，” *arXiv预印本
    arXiv:2304.13029*，2023年。
- en: '(12) H. I. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt, J. Weber,
    G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean, “Inceptiontime: Finding
    alexnet for time series classification,” *Data Min. Knowl. Discov.*, vol. 34,
    no. 6, pp. 1936–1962, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) H. I. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt, J. Weber,
    G. I. Webb, L. Idoumghar, P.-A. Muller, 和 F. Petitjean，“Inceptiontime：为时间序列分类寻找AlexNet，”
    *数据挖掘与知识发现*，第34卷，第6期，页1936–1962，2020年。
- en: (13) N. M. Foumani, C. W. Tan, G. I. Webb, and M. Salehi, “Improving position
    encoding of transformers for multivariate time series classification,” *Data Min.
    Knowl. Discov.*, Sep 2023.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) N. M. Foumani, C. W. Tan, G. I. Webb, 和 M. Salehi，“改进变换器的位置编码以用于多变量时间序列分类，”
    *数据挖掘与知识发现*，2023年9月。
- en: '(14) A. Dempster, F. Petitjean, and G. I. Webb, “ROCKET: exceptionally fast
    and accurate time series classification using random convolutional kernels,” *Data
    Min. Knowl. Discov.*, vol. 34, no. 5, pp. 1454–1495, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(14) A. Dempster, F. Petitjean, 和 G. I. Webb，“ROCKET: 使用随机卷积核进行异常快速且准确的时间序列分类，”
    *数据挖掘与知识发现*，第34卷，第5期，页1454–1495，2020年。'
- en: '(15) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, “Deep
    learning for time series classification: a review,” *Data Min Knowl Discov*, vol. 33,
    no. 4, pp. 917–963, 2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, 和 P.-A. Muller，“时间序列分类中的深度学习：综述，”
    *数据挖掘与知识发现*，第33卷，第4期，页917–963，2019年。
- en: '(16) Z. Wang, W. Yan, and T. Oates, “Time series classification from scratch
    with deep neural networks: A strong baseline,” in *2017 International joint conference
    on neural networks (IJCNN)*.   IEEE, 2017, pp. 1578–1585.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Z. Wang, W. Yan, 和 T. Oates，“从零开始的时间序列分类与深度神经网络：一个强基线，” 在 *2017年国际神经网络联合会议（IJCNN）*，IEEE，2017年，页1578–1585。
- en: '(17) Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun, “Transformers
    in time series: A survey,” *arXiv preprint:2202.07125*, 2022.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, 和 L. Sun，“时间序列中的变换器：综述，”
    *arXiv预印本:2202.07125*，2022年。
- en: (18) Y. Hao and H. Cao, “A new attention mechanism to classify multivariate
    time series,” in *29th Int. Joint Conf. Artificial Intelligence*, 2020.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) Y. Hao 和 H. Cao，“一种用于多变量时间序列分类的新型注意力机制，” 在 *第29届国际联合人工智能会议*，2020年。
- en: (19) G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff, “A
    transformer-based framework for multivariate time series representation learning,”
    in *27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining*, 2021, pp.
    2114–2124.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, 和 C. Eickhoff，“基于变换器的多变量时间序列表示学习框架，”
    在 *第27届ACM SIGKDD知识发现与数据挖掘会议*，2021年，页2114–2124。
- en: '(20) X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, “Self-supervised
    learning: Generative or contrastive,” *IEEE transactions on knowledge and data
    engineering*, vol. 35, no. 1, pp. 857–876, 2021.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, 和 J. Tang，“自监督学习：生成还是对比，”
    *IEEE知识与数据工程学报*，第35卷，第1期，页857–876，2021年。
- en: (21) E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh, X. Li, and C. Guan, “Time-series
    representation learning via temporal and contextual contrasting,” in *Proceedings
    of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21*,
    2021, pp. 2352–2359.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh, X. Li, 和 C. Guan，“通过时间和上下文对比进行时间序列表示学习，”
    在 *第三十届国际联合人工智能会议（IJCAI-21）论文集*，2021年，页2352–2359。
- en: '(22) C.-H. H. Yang, Y.-Y. Tsai, and P.-Y. Chen, “Voice2series: Reprogramming
    acoustic models for time series classification,” in *Int. conf. mach. learn.*   PMLR,
    2021, pp. 11 808–11 819.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (22) C.-H. H. Yang, Y.-Y. Tsai, 和 P.-Y. Chen，“Voice2series：为时间序列分类重新编程声学模型，”
    在 *国际机器学习会议*，PMLR，2021年，页11,808–11,819。
- en: '(23) Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, and B. Xu, “Ts2vec:
    Towards universal representation of time series,” in *AAAI*, vol. 36, no. 8, 2022,
    pp. 8980–8987.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, 和 B. Xu，“Ts2vec：迈向时间序列的通用表示，”
    发表在 *AAAI*，第36卷，第8期，2022年，第8980–8987页。
- en: '(24) N. M. Foumani, C. W. Tan, G. I. Webb, and M. Salehi, “Series2vec: Similarity-based
    self-supervised representation learning for time series classification,” *arXiv
    preprint arXiv:2312.03998*, 2023.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (24) N. M. Foumani, C. W. Tan, G. I. Webb, 和 M. Salehi，“Series2vec：基于相似性的自监督学习用于时间序列分类，”
    *arXiv预印本 arXiv:2312.03998*，2023年。
- en: '(25) A. Bagnall, J. Lines, A. Bostrom, J. Large, and E. Keogh, “The great time
    series classification bake off: a review and experimental evaluation of recent
    algorithmic advances,” *Data Min. Knowl. Discov.*, vol. 31, no. 3, pp. 606–660,
    2017.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) A. Bagnall, J. Lines, A. Bostrom, J. Large, 和 E. Keogh，“伟大的时间序列分类大赛：对近期算法进展的综述与实验评估，”
    *数据挖掘与知识发现*，第31卷，第3期，第606–660页，2017年。
- en: '(26) A. P. Ruiz, M. Flynn, J. Large, M. Middlehurst, and A. Bagnall, “The great
    multivariate time series classification bake off: a review and experimental evaluation
    of recent algorithmic advances,” *Data Min. Knowl. Discov.*, pp. 1–49, 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) A. P. Ruiz, M. Flynn, J. Large, M. Middlehurst, 和 A. Bagnall，“伟大的多变量时间序列分类大赛：对近期算法进展的综述与实验评估，”
    *数据挖掘与知识发现*，第1–49页，2020年。
- en: (27) C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb, “Monash University,
    UEA, UCR time series regression archive,” *arXiv preprint:2006.10996*, 2020.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) C. W. Tan, C. Bergmeir, F. Petitjean, 和 G. I. Webb，“莫纳什大学、UEA、UCR时间序列回归档案，”
    *arXiv预印本:2006.10996*，2020年。
- en: (28) M. Längkvist, L. Karlsson, and A. Loutfi, “A review of unsupervised feature
    learning and deep learning for time-series modeling,” *Pattern Recognition Letters*,
    vol. 42, pp. 11–24, 2014.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) M. Längkvist, L. Karlsson, 和 A. Loutfi， “无监督特征学习和深度学习在时间序列建模中的综述，” *模式识别快报*，第42卷，第11–24页，2014年。
- en: (29) Y. Bengio, L. Yao, G. Alain, and P. Vincent, “Generalized denoising auto-encoders
    as generative models,” *Advances neural inf. process. syst.*, vol. 26, 2013.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (29) Y. Bengio, L. Yao, G. Alain, 和 P. Vincent，“作为生成模型的广义去噪自编码器，” *神经信息处理系统进展*，第26卷，2013年。
- en: (30) Q. Hu, R. Zhang, and Y. Zhou, “Transfer learning for short-term wind speed
    prediction with deep neural networks,” *Renewable Energy*, vol. 85, pp. 83–95,
    2016.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) Q. Hu, R. Zhang, 和 Y. Zhou，“基于深度神经网络的短期风速预测中的迁移学习，” *可再生能源*，第85卷，第83–95页，2016年。
- en: (31) J. Serrà, S. Pascual, and A. Karatzoglou, “Towards a universal neural network
    encoder for time series.” in *CCIA*, 2018, pp. 120–129.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (31) J. Serrà, S. Pascual, 和 A. Karatzoglou，“迈向通用神经网络编码器以处理时间序列。” 发表在 *CCIA*，2018年，第120–129页。
- en: (32) D. Banerjee, K. Islam, K. Xue, G. Mei, L. Xiao, G. Zhang, R. Xu, C. Lei,
    S. Ji, and J. Li, “A deep transfer learning approach for improved post-traumatic
    stress disorder diagnosis,” *Knowledge and Information Systems*, vol. 60, no. 3,
    pp. 1693–1724, 2019.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) D. Banerjee, K. Islam, K. Xue, G. Mei, L. Xiao, G. Zhang, R. Xu, C. Lei,
    S. Ji, 和 J. Li，“一种改进创伤后应激障碍诊断的深度迁移学习方法，” *知识与信息系统*，第60卷，第3期，第1693–1724页，2019年。
- en: (33) W. Aswolinskiy, R. F. Reinhart, and J. Steil, “Time series classification
    in reservoir-and model-space,” *Neural Processing Letters*, vol. 48, no. 2, pp.
    789–809, 2018.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (33) W. Aswolinskiy, R. F. Reinhart, 和 J. Steil，“在储层和模型空间中的时间序列分类，” *神经处理快报*，第48卷，第2期，第789–809页，2018年。
- en: '(34) E. Brophy, Z. Wang, Q. She, and T. Ward, “Generative adversarial networks
    in time series: A systematic literature review,” *ACM Comput. Surv.*, vol. 55,
    no. 10, feb 2023\. [Online]. Available: [https://doi.org/10.1145/3559540](https://doi.org/10.1145/3559540)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (34) E. Brophy, Z. Wang, Q. She, 和 T. Ward，“时间序列中的生成对抗网络：系统文献综述，” *ACM计算机调查*，第55卷，第10期，2023年2月。
    [在线]. 可用：[https://doi.org/10.1145/3559540](https://doi.org/10.1145/3559540)
- en: (35) F. A. Del Campo, M. C. G. Neri, O. O. V. Villegas, V. G. C. Sánchez, H. d.
    J. O. Domínguez, and V. G. Jiménez, “Auto-adaptive multilayer perceptron for univariate
    time series classification,” *Expert Systems with Applications*, vol. 181, p.
    115147, 2021.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (35) F. A. Del Campo, M. C. G. Neri, O. O. V. Villegas, V. G. C. Sánchez, H.
    d. J. O. Domínguez, 和 V. G. Jiménez，“用于单变量时间序列分类的自适应多层感知机，” *专家系统应用*，第181卷，115147，2021年。
- en: (36) B. K. Iwana, V. Frinken, and S. Uchida, “A robust dissimilarity-based neural
    network for temporal pattern recognition,” in *2016 15th International Conference
    on Frontiers in Handwriting Recognition (ICFHR)*.   IEEE, 2016, pp. 265–270.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (36) B. K. Iwana, V. Frinken, 和 S. Uchida，“一种稳健的基于不相似性的神经网络用于时间模式识别，” 发表在 *2016年第15届国际手写识别前沿会议（ICFHR）*。IEEE，2016年，第265–270页。
- en: '(37) ——, “DTW-NN: A novel neural network for time series recognition using
    dynamic alignment between inputs and weights,” *Knowledge-Based Systems*, vol.
    188, p. 104971, 2020.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (37) ——, “DTW-NN：一种用于时间序列识别的新型神经网络，通过输入和权重之间的动态对齐，” *Knowledge-Based Systems*,
    第 188 卷, 页码 104971, 2020 年。
- en: '(38) N. Tabassum, S. Menon, and A. Jastrzebska, “Time-series classification
    with safe: Simple and fast segmented word embedding-based neural time series classifier,”
    *Information Processing & Management*, vol. 59, no. 5, p. 103044, 2022.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (38) N. Tabassum, S. Menon, 和 A. Jastrzebska, “使用 SAFE 进行时间序列分类：一种简单且快速的基于分段词嵌入的神经时间序列分类器，”
    *信息处理与管理*, 第 59 卷, 第 5 期, 页码 103044, 2022 年。
- en: (39) A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Advances neural inf. process. syst.*,
    vol. 25, pp. 1097–1105, 2012.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行 Imagenet 分类，”
    *Advances neural inf. process. syst.*, 第 25 卷, 页码 1097–1105, 2012 年。
- en: (40) J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern recognition*, vol. 77, pp. 354–377, 2018.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (40) J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *等*, “卷积神经网络的最新进展，” *模式识别*, 第 77 卷, 页码 354–377, 2018 年。
- en: (41) Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (41) Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习，” *自然*, 第 521 卷, 第 7553 期, 页码 436–444,
    2015 年。
- en: (42) Y. Zheng, Q. Liu, E. Chen, Y. Ge, and J. L. Zhao, “Time series classification
    using multi-channels deep convolutional neural networks,” in *International Conference
    on Web-Age Information Management*.   Springer, 2014, pp. 298–310.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (42) Y. Zheng, Q. Liu, E. Chen, Y. Ge, 和 J. L. Zhao, “使用多通道深度卷积神经网络进行时间序列分类，”
    见 *国际网页信息管理会议*。Springer, 2014 年, 页码 298–310。
- en: (43) J. Yang, M. N. Nguyen, P. P. San, X. L. Li, and S. Krishnaswamy, “Deep
    convolutional neural networks on multichannel time series for human activity recognition,”
    in *Twenty-fourth international joint conference on artificial intelligence*,
    2015.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (43) J. Yang, M. N. Nguyen, P. P. San, X. L. Li, 和 S. Krishnaswamy, “用于人类活动识别的多通道时间序列上的深度卷积神经网络，”
    见 *第二十四届国际人工智能联合会议*，2015 年。
- en: (44) B. Zhao, H. Lu, S. Chen, J. Liu, and D. Wu, “Convolutional neural networks
    for time series classification,” *Journal of Systems Engineering and Electronics*,
    vol. 28, no. 1, pp. 162–169, 2017.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (44) B. Zhao, H. Lu, S. Chen, J. Liu, 和 D. Wu, “用于时间序列分类的卷积神经网络，” *系统工程与电子学杂志*,
    第 28 卷, 第 1 期, 页码 162–169, 2017 年。
- en: (45) J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *IEEE conf. comp. vision patt. recognit.*, 2015, pp.
    3431–3440.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (45) J. Long, E. Shelhamer, 和 T. Darrell, “用于语义分割的全卷积网络，” 见 *IEEE 计算机视觉与模式识别会议*，2015
    年, 页码 3431–3440。
- en: (46) K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *IEEE conf. comp. vision patt. recognit.*, 2016, pp. 770–778.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (46) K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习，” 见 *IEEE 计算机视觉与模式识别会议*，2016
    年, 页码 770–778。
- en: (47) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *IEEE conf. comp. vision patt.
    recognit.*, 2016, pp. 2921–2929.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, 和 A. Torralba, “学习深度特征以进行区分性定位，”
    见 *IEEE 计算机视觉与模式识别会议*，2016 年, 页码 2921–2929。
- en: (48) X. Zou, Z. Wang, Q. Li, and W. Sheng, “Integration of residual network
    and convolutional neural network along with various activation functions and global
    pooling for time series classification,” *Neurocomputing*, vol. 367, pp. 39–45,
    2019.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (48) X. Zou, Z. Wang, Q. Li, 和 W. Sheng, “残差网络与卷积神经网络的集成，以及各种激活函数和全局池化用于时间序列分类，”
    *Neurocomputing*, 第 367 卷, 页码 39–45, 2019 年。
- en: '(49) Y. Li, X. Zhang, and D. Chen, “Csrnet: Dilated convolutional neural networks
    for understanding the highly congested scenes,” in *IEEE conf. comp. vision patt.
    recognit.*, 2018, pp. 1091–1100.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (49) Y. Li, X. Zhang, 和 D. Chen, “Csrnet：用于理解高度拥挤场景的膨胀卷积神经网络，” 见 *IEEE 计算机视觉与模式识别会议*，2018
    年, 页码 1091–1100。
- en: (50) O. Yazdanbakhsh and S. Dick, “Multivariate time series classification using
    dilated convolutional neural network,” *arXiv preprint:1905.01697*, 2019.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (50) O. Yazdanbakhsh 和 S. Dick, “使用膨胀卷积神经网络进行多变量时间序列分类，” *arXiv 预印本:1905.01697*，2019
    年。
- en: (51) S. N. M. Foumani, C. W. Tan, and M. Salehi, “Disjoint-cnn for multivariate
    time series classification,” in *2021 Int. Conf. Data Min. Workshops (ICDMW)*.   IEEE,
    2021, pp. 760–769.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (51) S. N. M. Foumani, C. W. Tan, 和 M. Salehi, “用于多变量时间序列分类的 Disjoint-cnn，”
    见 *2021 年国际数据挖掘会议研讨会（ICDMW）*。IEEE, 2021 年, 页码 760–769。
- en: '(52) M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “MobileNet-V2:
    Inverted residuals and linear bottlenecks,” in *IEEE conf. comp. vision patt.
    recognit.*, 2018, pp. 4510–4520.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (52) M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, 和 L.-C. Chen，“MobileNet-V2：倒置残差和线性瓶颈，”发表于
    *IEEE 计算机视觉与模式识别会议*，2018年，第4510–4520页。
- en: (53) Z. Wang and T. Oates, “Encoding time series as images for visual inspection
    and classification using tiled convolutional neural networks,” in *Workshops at
    the twenty-ninth AAAI conference on artificial intelligence*, 2015.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (53) Z. Wang 和 T. Oates，“将时间序列编码为图像以便视觉检查和分类，使用平铺卷积神经网络，”发表于 *第二十九届 AAAI 人工智能会议研讨会*，2015年。
- en: (54) N. Hatami, Y. Gavet, and J. Debayle, “Classification of time-series images
    using deep convolutional neural networks,” in *Tenth international conference
    on machine vision (ICMV 2017)*, vol. 10696.   SPIE, 2018, pp. 242–249.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) N. Hatami, Y. Gavet, 和 J. Debayle，“使用深度卷积神经网络的时间序列图像分类，”发表于 *第十届国际机器视觉会议
    (ICMV 2017)*，第10696卷，SPIE，2018年，第242–249页。
- en: (55) S. Karimi-Bidhendi, F. Munshi, and A. Munshi, “Scalable classification
    of univariate and multivariate time series,” in *2018 IEEE International Conference
    on Big Data (Big Data)*.   IEEE, 2018, pp. 1598–1605.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) S. Karimi-Bidhendi, F. Munshi, 和 A. Munshi，“单变量和多变量时间序列的可扩展分类，”发表于 *2018
    IEEE 大数据国际会议 (Big Data)*，IEEE，2018年，第1598–1605页。
- en: (56) Y. Zhao and Z. Cai, “Classify multivariate time series by deep neural network
    image classification,” in *2019 2nd China Symposium on Cognitive Computing and
    Hybrid Intelligence (CCHI)*.   IEEE, 2019, pp. 93–98.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Y. Zhao 和 Z. Cai，“通过深度神经网络图像分类分类多变量时间序列，”发表于 *2019第二届中国认知计算与混合智能研讨会 (CCHI)*，IEEE，2019年，第93–98页。
- en: (57) C.-L. Yang, Z.-X. Chen, and C.-Y. Yang, “Sensor classification using convolutional
    neural network by encoding multivariate time series as two-dimensional colored
    images,” *Sensors*, vol. 20, no. 1, p. 168, 2019.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (57) C.-L. Yang, Z.-X. Chen, 和 C.-Y. Yang，“通过将多变量时间序列编码为二维彩色图像使用卷积神经网络进行传感器分类，”*传感器*，第20卷，第1期，第168页，2019年。
- en: (58) J.-P. E. S. O. Kamphorst, D. Ruelle *et al.*, “Recurrence plots of dynamical
    systems,” *Europhysics Letters*, vol. 4, no. 9, p. 17, 1987.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) J.-P. E. S. O. Kamphorst, D. Ruelle *等*，“动态系统的重复图，”*欧物理快报*，第4卷，第9期，第17页，1987年。
- en: (59) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *IEEE conf. comp. vision patt.
    recognit.*, 2016, pp. 2818–2826.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (59) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, 和 Z. Wojna，“重新思考用于计算机视觉的
    Inception 架构，”发表于 *IEEE 计算机视觉与模式识别会议*，2016年，第2818–2826页。
- en: (60) W. Chen and K. Shi, “A deep learning framework for time series classification
    using relative position matrix and convolutional neural network,” *Neurocomputing*,
    vol. 359, pp. 384–394, 2019.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (60) W. Chen 和 K. Shi，“基于相对位置矩阵和卷积神经网络的时间序列分类深度学习框架，”*神经计算*，第359卷，第384–394页，2019年。
- en: (61) Z. Cui, W. Chen, and Y. Chen, “Multi-scale convolutional neural networks
    for time series classification,” *arXiv preprint:1603.06995*, 2016.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (61) Z. Cui, W. Chen, 和 Y. Chen，“用于时间序列分类的多尺度卷积神经网络，”*arXiv 预印本:1603.06995*，2016年。
- en: (62) A. Le Guennec, S. Malinowski, and R. Tavenard, “Data augmentation for time
    series classification using convolutional neural networks,” in *ECML/PKDD workshop
    on advanced analytics and learning on temporal data*, 2016.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (62) A. Le Guennec, S. Malinowski, 和 R. Tavenard，“使用卷积神经网络进行时间序列分类的数据增强，”发表于
    *ECML/PKDD 时间数据高级分析与学习研讨会*，2016年。
- en: (63) C.-L. Liu, W.-H. Hsaio, and Y.-C. Tu, “Time series classification with
    multivariate convolutional neural network,” *IEEE Transactions on Industrial Electronics*,
    vol. 66, no. 6, pp. 4788–4797, 2018.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (63) C.-L. Liu, W.-H. Hsaio, 和 Y.-C. Tu，“基于多变量卷积神经网络的时间序列分类，”*IEEE 工业电子学报*，第66卷，第6期，第4788–4797页，2018年。
- en: (64) A. Brunel, J. Pasquet, J. PASQUET, N. Rodriguez, F. Comby, D. Fouchez,
    and M. Chaumont, “A cnn adapted to time series for the classification of supernovae,”
    *Electronic imaging*, vol. 2019, no. 14, pp. 90–1, 2019.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (64) A. Brunel, J. Pasquet, J. PASQUET, N. Rodriguez, F. Comby, D. Fouchez,
    和 M. Chaumont，“一种适应时间序列的 CNN 用于超新星分类，”*电子成像*，第2019卷，第14期，第90–1页，2019年。
- en: (65) J. Sun, S. Takeuchi, and I. Yamasaki, “Prototypical inception network with
    cross branch attention for time series classification,” in *2021 International
    Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2021, pp. 1–7.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (65) J. Sun, S. Takeuchi, 和 I. Yamasaki，“具有交叉分支注意力的原型 Inception 网络用于时间序列分类，”发表于
    *2021年国际联合神经网络大会 (IJCNN)*，IEEE，2021年，第1–7页。
- en: (66) S. Usmankhujaev, B. Ibrokhimov, S. Baydadaev, and J. Kwon, “Time series
    classification with inceptionfcn,” *Sensors*, vol. 22, no. 1, p. 157, 2021.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) S. Usmankhujaev, B. Ibrokhimov, S. Baydadaev 和 J. Kwon，“使用 inceptionfcn
    进行时间序列分类”，*传感器*，第 22 卷，第 1 期，页码 157，2021 年。
- en: '(67) X. Gong, Y.-W. Si, Y. Tian, C. Lin, X. Zhang, and X. Liu, “Kdctime: Knowledge
    distillation with calibration on inceptiontime for time-series classification,”
    *Inf. Sci.*, vol. 613, pp. 184–203, 2022.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (67) X. Gong, Y.-W. Si, Y. Tian, C. Lin, X. Zhang 和 X. Liu，“Kdctime：基于校准的知识蒸馏用于时间序列分类”，*信息科学*，第
    613 卷，页码 184–203，2022 年。
- en: '(68) A. Ismail-Fawaz, M. Devanne, S. Berretti, J. Weber, and G. Forestier,
    “Lite: Light inception with boosting techniques for time series classification,”
    in *2023 IEEE 10th International Conference on Data Science and Advanced Analytics
    (DSAA)*.   IEEE, 2023, pp. 1–10.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (68) A. Ismail-Fawaz, M. Devanne, S. Berretti, J. Weber 和 G. Forestier，“Lite：使用提升技术进行时间序列分类的轻量级
    inception”，见 *2023 IEEE 第十届数据科学与高级分析国际会议 (DSAA)*，IEEE，2023 年，页码 1–10。
- en: (69) C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *IEEE conf.
    comp. vision patt. recognit.*, 2015, pp. 1–9.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (69) C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke 和 A. Rabinovich，“通过卷积进行更深入的探索”，见 *IEEE 计算机视觉与模式识别会议*，2015 年，页码 1–9。
- en: (70) C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, inception-resnet
    and the impact of residual connections on learning,” in *Thirty-first AAAI conference
    on artificial intelligence*, 2017.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (70) C. Szegedy, S. Ioffe, V. Vanhoucke 和 A. A. Alemi，“Inception-v4, inception-resnet
    及残差连接对学习的影响”，见 *第 31 届 AAAI 人工智能会议*，2017 年。
- en: '(71) M. Ronald, A. Poulose, and D. S. Han, “isplinception: an inception-resnet
    deep learning architecture for human activity recognition,” *IEEE Access*, vol. 9,
    pp. 68 985–69 001, 2021.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (71) M. Ronald, A. Poulose 和 D. S. Han，“isplinception：一种用于人类活动识别的 inception-resnet
    深度学习架构”，*IEEE 访问*，第 9 卷，页码 68,985–69,001，2021 年。
- en: (72) A. Ismail-Fawaz, M. Devanne, J. Weber, and G. Forestier, “Deep learning
    for time series classification using new hand-crafted convolution filters,” in
    *2022 IEEE International Conference on Big Data (Big Data)*.   IEEE, 2022, pp.
    972–981.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (72) A. Ismail-Fawaz, M. Devanne, J. Weber 和 G. Forestier，“使用新手工设计的卷积滤波器进行时间序列分类的深度学习”，见
    *2022 IEEE 国际大数据会议 (Big Data)*，IEEE，2022 年，页码 972–981。
- en: (73) M. Hüsken and P. Stagge, “Recurrent neural networks for time series classification,”
    *Neurocomputing*, vol. 50, pp. 223–235, 2003.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (73) M. Hüsken 和 P. Stagge，“用于时间序列分类的递归神经网络”，*神经计算*，第 50 卷，页码 223–235，2003 年。
- en: '(74) D. Dennis, D. A. E. Acar, V. Mandikal, V. S. Sadasivan, V. Saligrama,
    H. V. Simhadri, and P. Jain, “Shallow rnn: accurate time-series classification
    on resource constrained devices,” *Advances neural inf. process. syst.*, vol. 32,
    2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (74) D. Dennis, D. A. E. Acar, V. Mandikal, V. S. Sadasivan, V. Saligrama, H.
    V. Simhadri 和 P. Jain，“浅层 RNN：在资源受限设备上进行准确的时间序列分类”，*神经信息处理系统进展*，第 32 卷，2019 年。
- en: (75) S. Fernández, A. Graves, and J. Schmidhuber, “Sequence labelling in structured
    domains with hierarchical recurrent neural networks,” in *20th International Joint
    Conference on Artificial Intelligence, IJCAI 2007*, 2007.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (75) S. Fernández, A. Graves 和 J. Schmidhuber，“在结构化领域中使用层次递归神经网络进行序列标注”，见 *第
    20 届国际人工智能联合会议，IJCAI 2007*，2007 年。
- en: (76) M. Hermans and B. Schrauwen, “Training and analysing deep recurrent neural
    networks,” *Advances neural inf. process. syst.*, vol. 26, 2013.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (76) M. Hermans 和 B. Schrauwen，“训练和分析深层递归神经网络”，*神经信息处理系统进展*，第 26 卷，2013 年。
- en: (77) R. Pascanu, T. Mikolov, and Y. Bengio, “On the difficulty of training recurrent
    neural networks,” in *Int. conf. mach. learn.*   PMLR, 2013, pp. 1310–1318.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (77) R. Pascanu, T. Mikolov 和 Y. Bengio，“训练递归神经网络的难点”，见 *国际机器学习会议*，PMLR，2013
    年，页码 1310–1318。
- en: (78) S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (78) S. Hochreiter 和 J. Schmidhuber，“长短期记忆”，*神经计算*，第 9 卷，第 8 期，页码 1735–1780，1997
    年。
- en: (79) J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint:1412.3555*,
    2014.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (79) J. Chung, C. Gulcehre, K. Cho 和 Y. Bengio，“门控递归神经网络在序列建模中的经验评估”，*arXiv
    预印本：1412.3555*，2014 年。
- en: (80) I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” *Advances neural inf. process. syst.*, vol. 27, 2014.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (80) I. Sutskever, O. Vinyals 和 Q. V. Le，“使用神经网络进行序列到序列学习”，*神经信息处理系统进展*，第 27
    卷，2014 年。
- en: (81) J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
    K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for visual
    recognition and description,” in *IEEE conf. comp. vision patt. recognit.*, 2015,
    pp. 2625–2634.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (81) J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
    K. Saenko, 和 T. Darrell，“用于视觉识别和描述的长期递归卷积网络，” 收录于 *IEEE 计算机视觉与模式识别会议*，2015年，第2625–2634页。
- en: (82) A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating
    image descriptions,” in *IEEE conf. comp. vision patt. recognit.*, 2015, pp. 3128–3137.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (82) A. Karpathy 和 L. Fei-Fei，“用于生成图像描述的深度视觉-语义对齐，” 收录于 *IEEE 计算机视觉与模式识别会议*，2015年，第3128–3137页。
- en: (83) Y. Tang, J. Xu, K. Matsumoto, and C. Ono, “Sequence-to-sequence model with
    attention for time series classification,” in *2016 IEEE 16th Int. Conf. Data
    Min. Workshops (ICDMW)*.   IEEE, 2016, pp. 503–510.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (83) Y. Tang, J. Xu, K. Matsumoto, 和 C. Ono，“具有注意力的序列到序列模型用于时间序列分类，” 收录于 *2016
    IEEE 第16届国际数据挖掘研讨会 (ICDMW)*。 IEEE，2016年，第503–510页。
- en: '(84) P. Malhotra, V. TV, L. Vig, P. Agarwal, and G. Shroff, “Timenet: Pre-trained
    deep recurrent neural network for time series classification,” *arXiv preprint:1706.08838*,
    2017.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(84) P. Malhotra, V. TV, L. Vig, P. Agarwal, 和 G. Shroff，“Timenet: 用于时间序列分类的预训练深度递归神经网络，”
    *arXiv 预印本:1706.08838*，2017年。'
- en: (85) F. Karim, S. Majumdar, H. Darabi, and S. Harford, “Multivariate LSTM-FCNs
    for time series classification,” *Neural Networks*, vol. 116, pp. 237–245, 2019.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (85) F. Karim, S. Majumdar, H. Darabi, 和 S. Harford，“用于时间序列分类的多变量LSTM-FCNs，”
    *神经网络*，第116卷，第237–245页，2019年。
- en: '(86) X. Zhang, Y. Gao, J. Lin, and C.-T. Lu, “Tapnet: Multivariate time series
    classification with attentional prototypical network,” in *AAAI Conference on
    Artificial Intelligence*, vol. 34, no. 04, 2020, pp. 6845–6852.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(86) X. Zhang, Y. Gao, J. Lin, 和 C.-T. Lu，“Tapnet: 带有注意力原型网络的多变量时间序列分类，” 收录于
    *AAAI 人工智能会议*，第34卷，第04期，2020年，第6845–6852页。'
- en: '(87) J. Zuo, K. Zeitouni, and Y. Taher, “Smate: Semi-supervised spatio-temporal
    representation learning on multivariate time series,” in *2021 IEEE International
    Conference on Data Mining (ICDM)*.   IEEE, 2021, pp. 1565–1570.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(87) J. Zuo, K. Zeitouni, 和 Y. Taher，“Smate: 对多变量时间序列的半监督时空表示学习，” 收录于 *2021
    IEEE 国际数据挖掘会议 (ICDM)*。 IEEE，2021年，第1565–1570页。'
- en: (88) F. Karim, S. Majumdar, H. Darabi, and S. Chen, “LSTM fully convolutional
    networks for time series classification,” *IEEE access*, vol. 6, pp. 1662–1669,
    2017.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (88) F. Karim, S. Majumdar, H. Darabi, 和 S. Chen，“用于时间序列分类的LSTM完全卷积网络，” *IEEE
    Access*，第6卷，第1662–1669页，2017年。
- en: '(89) S. Lin and G. C. Runger, “Gcrnn: Group-constrained convolutional recurrent
    neural network,” *IEEE transactions on neural networks and learning systems*,
    vol. 29, no. 10, pp. 4709–4718, 2017.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(89) S. Lin 和 G. C. Runger， “Gcrnn: 组约束卷积递归神经网络，” *IEEE 神经网络与学习系统汇刊*，第29卷，第10期，第4709–4718页，2017年。'
- en: (90) R. Mutegeki and D. S. Han, “A CNN-LSTM approach to human activity recognition,”
    in *2020 IEEE Int. Conf. Comput. Intell. Commun. Technol. (ICAIIC)*.   IEEE, 2020,
    pp. 362–366.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (90) R. Mutegeki 和 D. S. Han，“CNN-LSTM方法用于人类活动识别，” 收录于 *2020 IEEE 国际计算智能与通信技术会议
    (ICAIIC)*。 IEEE，2020年，第362–366页。
- en: (91) R. Pascanu, T. Mikolov, and Y. Bengio, “Understanding the exploding gradient
    problem,” *CoRR, abs/1211.5063*, vol. 2, no. 417, p. 1, 2012.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (91) R. Pascanu, T. Mikolov, 和 Y. Bengio，“理解梯度爆炸问题，” *CoRR, abs/1211.5063*，第2卷，第417期，第1页，2012年。
- en: (92) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances neural inf.
    process. syst.*, vol. 30, 2017.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (92) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力即你所需，” *神经信息处理系统进展*，第30卷，2017年。
- en: '(93) J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” in *Proceedings
    of NAACL-HLT 2019*, vol. 1.   Stroudsburg, PA, USA: Association for Computational
    Linguistics, 2019, pp. 4171–4186.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(93) J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“BERT: 用于语言理解的深度双向变换器的预训练，”
    收录于 *NAACL-HLT 2019 会议录*，第1卷。 斯特劳斯堡，PA，美国：计算语言学协会，2019年，第4171–4186页。'
- en: '(94) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” *arXiv preprint:2010.11929*,
    2020.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (94) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“一张图像值16x16个词：用于大规模图像识别的变换器，”
    *arXiv 预印本:2010.11929*，2020年。
- en: (95) S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan, “Enhancing
    the locality and breaking the memory bottleneck of transformer on time series
    forecasting,” *Advances neural inf. process. syst.*, vol. 32, 2019.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (95) S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, 和 X. Yan, “增强变换器在时间序列预测中的局部性和打破记忆瓶颈”，*神经信息处理系统进展*，第32卷，2019年。
- en: '(96) H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang, “Informer:
    Beyond efficient transformer for long sequence time-series forecasting,” in *Proceedings
    of AAAI*, 2021.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (96) H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, 和 W. Zhang, “Informer：超越高效变换器的长序列时间序列预测”，见于*AAAI会议论文集*，2021年。
- en: '(97) D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, “Bendr: using transformers
    and a contrastive self-supervised learning task to learn from massive amounts
    of eeg data,” *Frontiers in Human Neuroscience*, vol. 15, 2021.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (97) D. Kostas, S. Aroca-Ouellette, 和 F. Rudzicz, “Bendr：利用变换器和对比自监督学习任务从大量脑电数据中学习”，*前沿人类神经科学*，第15卷，2021年。
- en: '(98) Y. Yuan, G. Xun, F. Ma, Y. Wang, N. Du, K. Jia, L. Su, and A. Zhang, “Muvan:
    A multi-view attention network for multivariate temporal data,” in *2018 IEEE
    International Conference on Data Mining (ICDM)*.   IEEE, 2018, pp. 717–726.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (98) Y. Yuan, G. Xun, F. Ma, Y. Wang, N. Du, K. Jia, L. Su, 和 A. Zhang, “Muvan：用于多变量时间数据的多视角注意力网络”，见于*2018年IEEE国际数据挖掘会议（ICDM）*。
    IEEE，2018年，第717–726页。
- en: '(99) T.-Y. Hsieh, S. Wang, Y. Sun, and V. Honavar, “Explainable multivariate
    time series classification: A deep neural network which learns to attend to important
    variables as well as time intervals,” in *14th ACM International Conference on
    Web Search and Data Mining*, 2021, pp. 607–615.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (99) T.-Y. Hsieh, S. Wang, Y. Sun, 和 V. Honavar, “可解释的多变量时间序列分类：一个深度神经网络，它学会关注重要变量及时间间隔”，见于*第14届ACM国际网络搜索与数据挖掘会议*，2021年，第607–615页。
- en: (100) W. Chen and K. Shi, “Multi-scale attention convolutional neural network
    for time series classification,” *Neural Networks*, vol. 136, pp. 126–140, 2021.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (100) W. Chen 和 K. Shi, “多尺度注意力卷积神经网络用于时间序列分类”，*神经网络*，第136卷，第126–140页，2021年。
- en: (101) Y. Yuan, G. Xun, F. Ma, Q. Suo, H. Xue, K. Jia, and A. Zhang, “A novel
    channel-aware attention framework for multi-channel eeg seizure detection via
    multi-view deep learning,” in *2018 IEEE EMBS International Conference on Biomedical
    & Health Informatics (BHI)*.   IEEE, 2018, pp. 206–209.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (101) Y. Yuan, G. Xun, F. Ma, Q. Suo, H. Xue, K. Jia, 和 A. Zhang, “一种新颖的通道感知注意力框架用于多通道脑电癫痫检测，通过多视角深度学习”，见于*2018年IEEE
    EMBS国际生物医学与健康信息学会议（BHI）*。 IEEE，2018年，第206–209页。
- en: '(102) Y. Liang, S. Ke, J. Zhang, X. Yi, and Y. Zheng, “Geoman: Multi-level
    attention networks for geo-sensory time series prediction,” in *IJCAI*, vol. 2018,
    2018, pp. 3428–3434.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (102) Y. Liang, S. Ke, J. Zhang, X. Yi, 和 Y. Zheng, “Geoman：用于地理传感时间序列预测的多级注意力网络”，见于*IJCAI*，第2018卷，2018年，第3428–3434页。
- en: (103) J. Hu and W. Zheng, “Multistage attention network for multivariate time
    series prediction,” *Neurocomputing*, vol. 383, pp. 122–137, 2020.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (103) J. Hu 和 W. Zheng, “多阶段注意力网络用于多变量时间序列预测”，*神经计算*，第383卷，第122–137页，2020年。
- en: (104) X. Cheng, P. Han, G. Li, S. Chen, and H. Zhang, “A novel channel and temporal-wise
    attention in convolutional networks for multivariate time series classification,”
    *IEEE Access*, vol. 8, pp. 212 247–212 257, 2020.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (104) X. Cheng, P. Han, G. Li, S. Chen, 和 H. Zhang, “一种新颖的通道和时间注意力在卷积网络中用于多变量时间序列分类”，*IEEE
    Access*，第8卷，第212247–212257页，2020年。
- en: '(105) Z. Xiao, X. Xu, H. Xing, S. Luo, P. Dai, and D. Zhan, “Rtfn: a robust
    temporal feature network for time series classification,” *Inf. Sci.*, vol. 571,
    pp. 65–86, 2021.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (105) Z. Xiao, X. Xu, H. Xing, S. Luo, P. Dai, 和 D. Zhan, “Rtfn：一种稳健的时间特征网络用于时间序列分类”，*信息科学*，第571卷，第65–86页，2021年。
- en: '(106) J. Wang, C. Yang, X. Jiang, and J. Wu, “When: A wavelet-dtw hybrid attention
    network for heterogeneous time series analysis,” in *Proceedings of the 29th ACM
    SIGKDD Conference on Knowledge Discovery and Data Mining*, 2023, pp. 2361–2373.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (106) J. Wang, C. Yang, X. Jiang, 和 J. Wu, “When：用于异质时间序列分析的Wavelet-DTW混合注意力网络”，见于*第29届ACM
    SIGKDD知识发现与数据挖掘会议论文集*，2023年，第2361–2373页。
- en: (107) M. Jaderberg, K. Simonyan, A. Zisserman *et al.*, “Spatial transformer
    networks,” *Advances neural inf. process. syst.*, vol. 28, 2015.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (107) M. Jaderberg, K. Simonyan, A. Zisserman *等*，“空间变换网络”，*神经信息处理系统进展*，第28卷，2015年。
- en: '(108) S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional block
    attention module,” in *European conference on computer vision*, 2018, pp. 3–19.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (108) S. Woo, J. Park, J.-Y. Lee, 和 I. S. Kweon, “Cbam：卷积块注意力模块”，见于*欧洲计算机视觉会议*，2018年，第3–19页。
- en: (109) J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *IEEE
    conf. comp. vision patt. recognit.*, 2018, pp. 7132–7141.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (109) J. Hu, L. Shen 和 G. Sun，“挤压与激励网络”，发表于 *IEEE计算机视觉与模式识别会议*，2018年，页码7132–7141。
- en: (110) T. Wang, Z. Liu, T. Zhang, and Y. Li, “Time series classification based
    on multi-scale dynamic convolutional features and distance features,” in *2021
    2nd Asia Symposium on Signal Processing (ASSP)*.   IEEE, 2021, pp. 239–246.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (110) T. Wang, Z. Liu, T. Zhang 和 Y. Li，“基于多尺度动态卷积特征和距离特征的时间序列分类”，发表于 *2021年第二届亚洲信号处理研讨会（ASSP）*。   IEEE，2021年，页码239–246。
- en: '(111) H. Song, D. Rajan, J. Thiagarajan, and A. Spanias, “Attend and diagnose:
    Clinical time series analysis using attention models,” in *AAAI conference on
    artificial intelligence*, vol. 32, no. 1, 2018.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (111) H. Song, D. Rajan, J. Thiagarajan 和 A. Spanias，“关注与诊断：使用注意力模型进行临床时间序列分析”，发表于
    *AAAI人工智能会议*，第32卷，第1期，2018年。
- en: (112) C.-c. Jin and X. Chen, “An end-to-end framework combining time–frequency
    expert knowledge and modified transformer networks for vibration signal classification,”
    *Expert Systems with Applications*, vol. 171, p. 114570, 2021.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (112) C.-c. Jin 和 X. Chen，“结合时间–频率专家知识与改进变换器网络的端到端框架用于振动信号分类”，*专家系统应用*，第171卷，页码114570，2021年。
- en: '(113) C. E. Rasmussen, *Gaussian Processes in Machine Learning*.   Berlin,
    Heidelberg: Springer Berlin Heidelberg, 2004, pp. 63–71.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (113) C. E. Rasmussen，*机器学习中的高斯过程*。   柏林，海德堡：Springer Berlin Heidelberg，2004年，页码63–71。
- en: '(114) T. Allam Jr and J. D. McEwen, “Paying attention to astronomical transients:
    Photometric classification with the time-series transformer,” *arXiv preprint:2105.06178*,
    2021.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (114) T. Allam Jr 和 J. D. McEwen，“关注天文瞬变：利用时间序列变换器进行光度分类”，*arXiv预印本:2105.06178*，2021年。
- en: (115) M. Liu, S. Ren, S. Ma, J. Jiao, Y. Chen, Z. Wang, and W. Song, “Gated
    transformer networks for multivariate time series classification,” *arXiv preprint:2103.14438*,
    2021.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (115) M. Liu, S. Ren, S. Ma, J. Jiao, Y. Chen, Z. Wang 和 W. Song，“用于多变量时间序列分类的门控变换器网络”，*arXiv预印本:2103.14438*，2021年。
- en: (116) B. Zhao, H. Xing, X. Wang, F. Song, and Z. Xiao, “Rethinking attention
    mechanism in time series classification,” *arXiv preprint:2207.07564*, 2022.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (116) B. Zhao, H. Xing, X. Wang, F. Song 和 Z. Xiao，“重新思考时间序列分类中的注意机制”，*arXiv预印本:2207.07564*，2022年。
- en: '(117) Y. Ren, L. Li, X. Yang, and J. Zhou, “Autotransformer: Automatic transformer
    architecture design for time series classification,” in *Pacific-Asia Conference
    on Knowledge Discovery and Data Mining*.   Springer, 2022, pp. 143–155.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (117) Y. Ren, L. Li, X. Yang 和 J. Zhou，“自变换器：用于时间序列分类的自动变换器架构设计”，发表于 *亚太知识发现与数据挖掘会议*。   Springer，2022年，页码143–155。
- en: '(118) M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb, I. King,
    and S. Pan, “A Survey on Graph Neural Networks for Time Series: Forecasting, Classification,
    Imputation, and Anomaly Detection,” *arXiv*, vol. 14, no. 8, pp. 1–27, jul 2023\.
    [Online]. Available: [http://arxiv.org/abs/2307.03759](http://arxiv.org/abs/2307.03759)'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (118) M. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb, I. King 和
    S. Pan，“图神经网络在时间序列中的调查：预测、分类、插补和异常检测”，*arXiv*，第14卷，第8期，页码1–27，2023年7月。 [在线] 可用：[http://arxiv.org/abs/2307.03759](http://arxiv.org/abs/2307.03759)
- en: (119) Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, “A Comprehensive
    Survey on Graph Neural Networks,” *IEEE Transactions on Neural Networks and Learning
    Systems*, vol. 32, no. 1, pp. 4–24, jan 2021.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (119) Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang 和 P. S. Yu，“图神经网络的综合调查”，*IEEE神经网络与学习系统汇刊*，第32卷，第1期，页码4–24，2021年1月。
- en: (120) F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The Graph Neural Network Model,” *IEEE Transactions on Neural Networks*, vol. 20,
    no. 1, pp. 61–80, jan 2009.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (120) F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner 和 G. Monfardini，"图神经网络模型"，*IEEE神经网络汇刊*，第20卷，第1期，页码61–80，2009年1月。
- en: '(121) W. Xi, A. Jain, L. Zhang, and J. Lin, “LB-SimTSC: An Efficient Similarity-Aware
    Graph Neural Network for Semi-Supervised Time Series Classification,” *arXiv*,
    jan 2023.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (121) W. Xi, A. Jain, L. Zhang 和 J. Lin，“LB-SimTSC：一种高效的相似性感知图神经网络用于半监督时间序列分类”，*arXiv*，2023年1月。
- en: '(122) H. Liu, X. Liu, D. Yang, Z. Liang, H. Wang, Y. Cui, and J. Gu, “TodyNet:
    Temporal Dynamic Graph Neural Network for Multivariate Time Series Classification,”
    *arXiv*, vol. XX, no. Xx, pp. 1–10, apr 2023.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(122) H. Liu, X. Liu, D. Yang, Z. Liang, H. Wang, Y. Cui 和 J. Gu，“TodyNet:
    用于多变量时间序列分类的时间动态图神经网络”，*arXiv*，第XX卷，第Xx期，页码1–10，2023年4月。'
- en: (123) S. Bloemheuvel, J. van den Hoogen, D. Jozinović, A. Michelini, and M. Atzmueller,
    “Graph neural networks for multivariate time series regression with application
    to seismic data,” *International Journal of Data Science and Analytics*, vol. 16,
    no. 3, pp. 317–332, sep 2023.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (123) S. Bloemheuvel, J. van den Hoogen, D. Jozinović, A. Michelini, 和 M. Atzmueller,
    “用于多变量时间序列回归的图神经网络及其在地震数据中的应用，” *数据科学与分析国际期刊*，第16卷，第3期，第317–332页，2023年9月。
- en: '(124) Z. Cheng, Y. Yang, S. Jiang, W. Hu, Z. Ying, Z. Chai, and C. Wang, “Time2Graph+:
    Bridging Time Series and Graph Representation Learning via Multiple Attentions,”
    *IEEE Transactions on Knowledge and Data Engineering*, vol. 35, no. 2, pp. 1–1,
    2021.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(124) Z. Cheng, Y. Yang, S. Jiang, W. Hu, Z. Ying, Z. Chai, 和 C. Wang, “Time2Graph+:
    通过多重注意力桥接时间序列和图表示学习，” *IEEE知识与数据工程学报*，第35卷，第2期，第1–1页，2021年。'
- en: (125) I. C. Covert, B. Krishnan, I. Najm, J. Zhan, M. Shore, J. Hixson, and
    M. J. Po, “Temporal graph convolutional networks for automatic seizure detection,”
    in *Machine Learning for Healthcare Conference*.   PMLR, 2019, pp. 160–180.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (125) I. C. Covert, B. Krishnan, I. Najm, J. Zhan, M. Shore, J. Hixson, 和 M.
    J. Po, “用于自动癫痫检测的时序图卷积网络，” 见于 *医疗保健会议中的机器学习*。PMLR，2019，第160–180页。
- en: (126) T. Song, W. Zheng, P. Song, and Z. Cui, “EEG Emotion Recognition Using
    Dynamical Graph Convolutional Neural Networks,” *IEEE Transactions on Affective
    Computing*, vol. 11, no. 3, pp. 532–541, jul 2020.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (126) T. Song, W. Zheng, P. Song, 和 Z. Cui, “使用动态图卷积神经网络的脑电图情感识别，” *IEEE情感计算学报*，第11卷，第3期，第532–541页，2020年7月。
- en: '(127) Z. Jia, Y. Lin, J. Wang, R. Zhou, X. Ning, Y. He, and Y. Zhao, “Graphsleepnet:
    Adaptive spatial-temporal graph convolutional networks for sleep stage classification.”
    in *IJCAI*, 2020, pp. 1324–1330.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(127) Z. Jia, Y. Lin, J. Wang, R. Zhou, X. Ning, Y. He, 和 Y. Zhao, “Graphsleepnet:
    自适应空间-时间图卷积网络用于睡眠阶段分类。” 见于 *IJCAI*，2020年，第1324–1330页。'
- en: (128) Z. Ma, G. Mei, E. Prezioso, Z. Zhang, and N. Xu, “A deep learning approach
    using graph convolutional networks for slope deformation prediction based on time-series
    displacement data,” *Neural Computing and Applications*, vol. 33, no. 21, pp.
    14 441–14 457, 2021.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (128) Z. Ma, G. Mei, E. Prezioso, Z. Zhang, 和 N. Xu, “基于时间序列位移数据的图卷积网络深度学习方法用于坡度变形预测，”
    *神经计算与应用*，第33卷，第21期，第14,441–14,457页，2021年。
- en: (129) T. Li, Z. Zhao, C. Sun, R. Yan, and X. Chen, “Multireceptive field graph
    convolutional networks for machine fault diagnosis,” *IEEE Transactions on Industrial
    Electronics*, vol. 68, no. 12, pp. 12 739–12 749, 2020.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (129) T. Li, Z. Zhao, C. Sun, R. Yan, 和 X. Chen, “用于机器故障诊断的多感受野图卷积网络，” *IEEE工业电子学报*，第68卷，第12期，第12,739–12,749页，2020年。
- en: (130) D. Nhu, M. Janmohamed, P. Perucca, A. Gilligan, P. Kwan, T. O’Brien, C. Tan,
    and L. Kuhlmann, “Graph convolutional network for generalized epileptiform abnormality
    detection on eeg,” in *2021 IEEE Signal Processing in Medicine and Biology Symposium
    (SPMB)*.   IEEE, 2021, pp. 1–6.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (130) D. Nhu, M. Janmohamed, P. Perucca, A. Gilligan, P. Kwan, T. O’Brien, C.
    Tan, 和 L. Kuhlmann, “用于脑电图中广泛癫痫异常检测的图卷积网络，” 见于 *2021 IEEE医学与生物学信号处理研讨会（SPMB）*。IEEE，2021，第1–6页。
- en: (131) S. Tang, J. A. Dunnmon, K. Saab, X. Zhang, Q. Huang, F. Dubost, D. L.
    Rubin, and C. Lee-Messer, “Self-Supervised Graph Neural Networks for Improved
    Electroencephalographic Seizure Analysis,” *ICLR 2022 - 10th Int. Conf. Learning
    Representations*, pp. 1–23, apr 2021.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (131) S. Tang, J. A. Dunnmon, K. Saab, X. Zhang, Q. Huang, F. Dubost, D. L.
    Rubin, 和 C. Lee-Messer, “自监督图神经网络以改进脑电图癫痫分析，” *ICLR 2022 - 第10届国际学习表征会议*，第1–23页，2021年4月。
- en: (132) X. Zhang, M. Zeman, T. Tsiligkaridis, and M. Zitnik, “Graph-Guided Network
    for Irregularly Sampled Multivariate Time Series,” *ICLR 2022 - 10th International
    Conference on Learning Representations*, pp. 1–21, oct 2021.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (132) X. Zhang, M. Zeman, T. Tsiligkaridis, 和 M. Zitnik, “用于不规则采样多变量时间序列的图引导网络，”
    *ICLR 2022 - 第10届国际学习表征会议*，第1–21页，2021年10月。
- en: (133) A. M. Censi, D. Ienco, Y. J. E. Gbodjo, R. G. Pensa, R. Interdonato, and
    R. Gaetano, “Attentive spatial temporal graph CNN for land cover mapping from
    multi temporal remote sensing data,” *IEEE Access*, vol. 9, pp. 23 070–23 082,
    2021.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (133) A. M. Censi, D. Ienco, Y. J. E. Gbodjo, R. G. Pensa, R. Interdonato, 和
    R. Gaetano, “用于从多时相遥感数据中进行土地覆盖映射的关注空间时序图CNN，” *IEEE Access*，第9卷，第23,070–23,082页，2021年。
- en: (134) T. Azevedo, A. Campbell, R. Romero-Garcia, L. Passamonti, R. A. Bethlehem,
    P. Liò, and N. Toschi, “A deep graph neural network architecture for modelling
    spatio-temporal dynamics in resting-state functional MRI data,” *Medical Image
    Analysis*, vol. 79, p. 102471, jul 2022.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (134) T. Azevedo, A. Campbell, R. Romero-Garcia, L. Passamonti, R. A. Bethlehem,
    P. Liò, 和 N. Toschi, “用于建模静息态功能MRI数据中的时空动态的深度图神经网络架构，” *医学图像分析*，第79卷，第102471页，2022年7月。
- en: (135) Z. Duan, H. Xu, Y. Wang, Y. Huang, A. Ren, Z. Xu, Y. Sun, and W. Wang,
    “Multivariate time-series classification with hierarchical variational graph pooling,”
    *Neural Networks*, vol. 154, pp. 481–490, oct 2022.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (135) Z. Duan, H. Xu, Y. Wang, Y. Huang, A. Ren, Z. Xu, Y. Sun, 和 W. Wang，“带有分层变分图池化的多变量时间序列分类，”
    *神经网络*，第154卷，第481–490页，2022年10月。
- en: (136) D. Zha, K.-h. Lai, K. Zhou, and X. Hu, “Towards Similarity-Aware Time-Series
    Classification,” in *Proceedings of the 2022 SIAM International Conference on
    Data Mining (SDM)*, Philadelphia, PA, jan 2022, pp. 199–207.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (136) D. Zha, K.-h. Lai, K. Zhou, 和 X. Hu，“面向相似性感知的时间序列分类，” 收录于 *2022年SIAM国际数据挖掘会议（SDM）*，费城，PA，2022年1月，第199–207页。
- en: (137) L. Tulczyjew, M. Kawulok, N. Longepe, B. Le Saux, and J. Nalepa, “Graph
    Neural Networks Extract High-Resolution Cultivated Land Maps From Sentinel-2 Image
    Series,” *IEEE Geoscience and Remote Sensing Letters*, vol. 19, pp. 1–5, 2022.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (137) L. Tulczyjew, M. Kawulok, N. Longepe, B. Le Saux, 和 J. Nalepa，“图神经网络从哨兵-2图像序列中提取高分辨率耕地地图，”
    *IEEE地球科学与遥感快报*，第19卷，第1–5页，2022年。
- en: (138) L. Sun, C. Li, B. Liu, and Y. Zhang, “Class-driven Graph Attention Network
    for Multi-label Time Series Classification in Mobile Health Digital Twins,” *IEEE
    Journal on Selected Areas in Communications*, vol. 41, no. 10, pp. 3267–3278,
    2023.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (138) L. Sun, C. Li, B. Liu, 和 Y. Zhang，“面向移动健康数字双胞胎的多标签时间序列分类的类别驱动图注意力网络，”
    *IEEE选择领域通信杂志*，第41卷，第10期，第3267–3278页，2023年。
- en: '(139) C. Dufourg, C. Pelletier, S. May, and S. Lefèvre, “Graph Dynamic Earth
    Net: Spatio-Temporal Graph Benchmark for Satellite Image Time Series,” in *IGARSS
    2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium*.   IEEE,
    jul 2023, pp. 7164–7167.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (139) C. Dufourg, C. Pelletier, S. May, 和 S. Lefèvre，“图动态地球网：卫星图像时间序列的时空图基准测试，”
    收录于 *IGARSS 2023 - 2023年IEEE国际地球科学与遥感研讨会*。 IEEE，2023年7月，第7164–7167页。
- en: (140) E. Keogh and C. A. Ratanamahatana, “Exact indexing of dynamic time warping,”
    *Knowl. Inform. Systems*, vol. 7, no. 3, pp. 358–386, 2005.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (140) E. Keogh 和 C. A. Ratanamahatana，“动态时间规整的精确索引，” *知识与信息系统*，第7卷，第3期，第358–386页，2005年。
- en: (141) T. N. Kipf and M. Welling, “Semi-Supervised Classification with Graph
    Convolutional Networks,” *5th International Conference on Learning Representations,
    ICLR 2017 - Conference Track Proceedings*, pp. 1–14, sep 2016.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (141) T. N. Kipf 和 M. Welling，“基于图卷积网络的半监督分类，” *第五届国际学习表示大会，ICLR 2017 - 会议论文集*，第1–14页，2016年9月。
- en: (142) L. Yang and S. Hong, “Unsupervised time-series representation learning
    with iterative bilinear temporal-spectral fusion,” in *ICML*, 2022, pp. 25 038–25 054.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (142) L. Yang 和 S. Hong，“基于迭代双线性时间-频谱融合的无监督时间序列表示学习，” 收录于 *ICML*，2022年，第25 038–25 054页。
- en: (143) A. Hyvarinen and H. Morioka, “Unsupervised feature extraction by time-contrastive
    learning and nonlinear ica,” *Advances in neural information processing systems*,
    vol. 29, 2016.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (143) A. Hyvarinen 和 H. Morioka，“通过时间对比学习和非线性ICA进行无监督特征提取，” *神经信息处理系统进展*，第29卷，2016年。
- en: (144) J.-Y. Franceschi, A. Dieuleveut, and M. Jaggi, “Unsupervised scalable
    representation learning for multivariate time series,” *NeurIPS*, vol. 32, 2019.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (144) J.-Y. Franceschi, A. Dieuleveut, 和 M. Jaggi，“无监督的可扩展多变量时间序列表示学习，” *NeurIPS*，第32卷，2019年。
- en: (145) S. Tonekaboni, D. Eytan, and A. Goldenberg, “Unsupervised representation
    learning for time series with temporal neighborhood coding,” *arXiv preprint arXiv:2106.00750*,
    2021.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (145) S. Tonekaboni, D. Eytan, 和 A. Goldenberg，“用于时间序列的无监督表示学习与时间邻域编码，” *arXiv预印本arXiv:2106.00750*，2021年。
- en: '(146) K. Wickstrøm, M. Kampffmeyer, K. Ø. Mikalsen, and R. Jenssen, “Mixing
    up contrastive learning: Self-supervised representation learning for time series,”
    *Pattern Recognition Letters*, vol. 155, pp. 54–61, 2022.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (146) K. Wickstrøm, M. Kampffmeyer, K. Ø. Mikalsen, 和 R. Jenssen，“混合对比学习：时间序列的自监督表示学习，”
    *模式识别快报*，第155卷，第54–61页，2022年。
- en: '(147) X. Yang, Z. Zhang, and R. Cui, “Timeclr: A self-supervised contrastive
    learning framework for univariate time series representation,” *Knowledge-Based
    Systems*, vol. 245, p. 108606, 2022.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(147) X. Yang, Z. Zhang, 和 R. Cui，“Timeclr: 自监督对比学习框架用于单变量时间序列表示，” *知识基础系统*，第245卷，第108606页，2022年。'
- en: (148) X. Zhang, Z. Zhao, T. Tsiligkaridis, and M. Zitnik, “Self-supervised contrastive
    pre-training for time series via time-frequency consistency,” in *Proceedings
    of Neural Information Processing Systems, NeurIPS*, 2022.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (148) X. Zhang, Z. Zhao, T. Tsiligkaridis, 和 M. Zitnik，“基于时间-频率一致性的自监督对比预训练时间序列，”
    收录于 *NeurIPS神经信息处理系统会议论文集*，2022年。
- en: '(149) Q. Meng, H. Qian, Y. Liu, L. Cui, Y. Xu, and Z. Shen, “Mhccl: masked
    hierarchical cluster-wise contrastive learning for multivariate time series,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 37, no. 8,
    2023, pp. 9153–9161.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (149) Q. Meng, H. Qian, Y. Liu, L. Cui, Y. Xu, 和 Z. Shen，“Mhccl：用于多变量时间序列的遮蔽分层集群对比学习”，发表于*AAAI人工智能会议论文集*，第37卷，第8期，2023年，第9153–9161页。
- en: '(150) R. R. Chowdhury, X. Zhang, J. Shang, R. K. Gupta, and D. Hong, “Tarnet:
    Task-aware reconstruction for time-series transformer,” in *28th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining, Washington, DC, USA*, 2022, pp. 14–18.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (150) R. R. Chowdhury, X. Zhang, J. Shang, R. K. Gupta, 和 D. Hong，“Tarnet：任务感知重建的时间序列变换器”，发表于*第28届ACM
    SIGKDD知识发现与数据挖掘会议，华盛顿特区，美国*，2022年，第14–18页。
- en: '(151) M. Cheng, Q. Liu, Z. Liu, H. Zhang, R. Zhang, and E. Chen, “Timemae:
    Self-supervised representations of time series with decoupled masked autoencoders,”
    *arXiv preprint arXiv:2303.00320*, 2023.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (151) M. Cheng, Q. Liu, Z. Liu, H. Zhang, R. Zhang, 和 E. Chen，“Timemae：使用解耦遮蔽自编码器的时间序列自监督表示”，*arXiv预印本
    arXiv:2303.00320*，2023年。
- en: (152) W. Zhang, L. Yang, S. Geng, and S. Hong, “Self-supervised time series
    representation learning via cross reconstruction transformer,” *IEEE Transactions
    on Neural Networks and Learning Systems*, 2023.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (152) W. Zhang, L. Yang, S. Geng, 和 S. Hong，“通过交叉重构变换器进行自监督时间序列表示学习”，*IEEE神经网络与学习系统汇刊*，2023年。
- en: (153) A. Ismail-Fawaz, M. Devanne, S. Berretti, J. Weber, and G. Forestier,
    “Finding foundation models for time series classification with a pretext task,”
    *arXiv preprint arXiv:2311.14534*, 2023.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (153) A. Ismail-Fawaz, M. Devanne, S. Berretti, J. Weber, 和 G. Forestier，“寻找用于时间序列分类的基础模型及其预文本任务”，*arXiv预印本
    arXiv:2311.14534*，2023年。
- en: (154) C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation
    for deep learning,” *Journal of big data*, vol. 6, no. 1, pp. 1–48, 2019.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (154) C. Shorten 和 T. M. Khoshgoftaar，“关于深度学习的图像数据增强调查”，*大数据期刊*，第6卷，第1期，第1–48页，2019年。
- en: (155) T. T. Um, F. M. Pfister, D. Pichler, S. Endo, M. Lang, S. Hirche, U. Fietzek,
    and D. Kulić, “Data augmentation of wearable sensor data for parkinson’s disease
    monitoring using convolutional neural networks,” in *Proc. 19th ACM int. conf.
    multimodal interaction*, 2017, pp. 216–220.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (155) T. T. Um, F. M. Pfister, D. Pichler, S. Endo, M. Lang, S. Hirche, U. Fietzek,
    和 D. Kulić，“使用卷积神经网络进行帕金森病监测的可穿戴传感器数据增强”，发表于*第19届ACM国际多模态交互会议论文集*，2017年，第216–220页。
- en: '(156) K. M. Rashid and J. Louis, “Window-warping: a time series data augmentation
    of imu data for construction equipment activity identification,” in *ISARC. Proceedings
    of the International Symposium on Automation and Robotics in Construction*, vol. 36.   IAARC
    Publications, 2019, pp. 651–657.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (156) K. M. Rashid 和 J. Louis，“Window-warping：用于施工设备活动识别的IMU数据时间序列数据增强”，发表于*国际自动化与机器人建设研讨会（ISARC）*，第36卷。IAARC出版物，2019年，第651–657页。
- en: (157) B. K. Iwana and S. Uchida, “Time series data augmentation for neural networks
    by time warping with a discriminative teacher,” in *2020 25th International Conference
    on Pattern Recognition (ICPR)*.   IEEE, 2021, pp. 3558–3565.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (157) B. K. Iwana 和 S. Uchida，“通过时间扭曲与判别教师进行神经网络的时间序列数据增强”，发表于*2020年第25届国际模式识别会议（ICPR）*。IEEE，2021，第3558–3565页。
- en: (158) T.-S. Nguyen, S. Stueker, J. Niehues, and A. Waibel, “Improving sequence-to-sequence
    speech recognition training with on-the-fly data augmentation,” in *ICASSP 2020-2020
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE,
    2020, pp. 7689–7693.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (158) T.-S. Nguyen, S. Stueker, J. Niehues, 和 A. Waibel，“通过即时数据增强改善序列到序列的语音识别训练”，发表于*ICASSP
    2020-2020 IEEE国际声学、语音与信号处理会议（ICASSP）*。IEEE，2020年，第7689–7693页。
- en: (159) B. Vachhani, C. Bhat, and S. K. Kopparapu, “Data augmentation using healthy
    speech for dysarthric speech recognition.” in *Interspeech*, 2018, pp. 471–475.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (159) B. Vachhani, C. Bhat, 和 S. K. Kopparapu，“使用健康语音进行的言语数据增强以提升构音障碍言语识别。”发表于*Interspeech*，2018年，第471–475页。
- en: '(160) J. Gao, X. Song, Q. Wen, P. Wang, L. Sun, and H. Xu, “Robusttad: Robust
    time series anomaly detection via decomposition and convolutional neural networks,”
    2020.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (160) J. Gao, X. Song, Q. Wen, P. Wang, L. Sun, 和 H. Xu，“Robusttad：通过分解和卷积神经网络进行稳健的时间序列异常检测”，2020年。
- en: (161) Z. Cui, W. Chen, and Y. Chen, “Multi-scale convolutional neural networks
    for time series classification,” 2016.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (161) Z. Cui, W. Chen, 和 Y. Chen，“用于时间序列分类的多尺度卷积神经网络”，2016年。
- en: (162) A. Le Guennec, S. Malinowski, and R. Tavenard, “Data Augmentation for
    Time Series Classification using Convolutional Neural Networks,” in *ECML/PKDD
    on Advanced Analytics and Learning on Temporal Data*, 2016.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (162) A. Le Guennec, S. Malinowski 和 R. Tavenard, “使用卷积神经网络的时间序列分类数据增强,” 收录于*ECML/PKDD
    时间数据高级分析与学习*，2016.
- en: (163) G. Forestier, F. Petitjean, H. A. Dau, G. I. Webb, and E. Keogh, “Generating
    synthetic time series to augment sparse datasets,” in *2017 IEEE international
    conference on data mining (ICDM)*.   IEEE, 2017, pp. 865–870.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (163) G. Forestier, F. Petitjean, H. A. Dau, G. I. Webb 和 E. Keogh, “生成合成时间序列以增强稀疏数据集,”
    收录于*2017 IEEE 国际数据挖掘大会（ICDM）*。 IEEE, 2017, 页. 865–870.
- en: (164) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, “Data
    augmentation using synthetic data for time series classification with deep residual
    networks,” 2018.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (164) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar 和 P.-A. Muller, “使用合成数据进行时间序列分类的数据增强与深度残差网络,”
    2018.
- en: (165) T. Terefe, M. Devanne, J. Weber, D. Hailemariam, and G. Forestier, “Time
    series averaging using multi-tasking autoencoder,” in *2020 IEEE 32nd International
    Conference on Tools with Artificial Intelligence (ICTAI)*.   IEEE, 2020, pp. 1065–1072.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (165) T. Terefe, M. Devanne, J. Weber, D. Hailemariam 和 G. Forestier, “使用多任务自编码器的时间序列平均,”
    收录于*2020 IEEE 第32届国际人工智能工具大会（ICTAI）*。 IEEE, 2020, 页. 1065–1072.
- en: (166) B. K. Iwana and S. Uchida, “An empirical survey of data augmentation for
    time series classification with neural networks,” *Plos one*, vol. 16, no. 7,
    p. e0254841, 2021.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (166) B. K. Iwana 和 S. Uchida, “基于神经网络的时间序列分类数据增强的实证调研,” *Plos one*, 卷. 16,
    期. 7, 页. e0254841, 2021.
- en: (167) G. Pialla, M. Devanne, J. Weber, L. Idoumghar, and G. Forestier, “Data
    augmentation for time series classification with deep learning models,” in *International
    Workshop on Advanced Analytics and Learning on Temporal Data*.   Springer, 2022,
    pp. 117–132.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (167) G. Pialla, M. Devanne, J. Weber, L. Idoumghar 和 G. Forestier, “使用深度学习模型的时间序列分类数据增强,”
    收录于*国际时间数据高级分析与学习研讨会*。 Springer, 2022, 页. 117–132.
- en: '(168) Z. Gao, L. Li, and T. Xu, “Data augmentation for time-series classification:
    An extensive empirical study and comprehensive survey,” *arXiv preprint arXiv:2310.10060*,
    2023.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(168) Z. Gao, L. Li 和 T. Xu, “时间序列分类的数据增强: 大规模实证研究与综合调查,” *arXiv 预印本 arXiv:2310.10060*,
    2023.'
- en: '(169) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE conf. comp. vision patt.
    recognit.*   Ieee, 2009, pp. 248–255.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(169) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li 和 L. Fei-Fei, “Imagenet:
    一个大规模的层次图像数据库,” 收录于*2009 IEEE 计算机视觉与模式识别大会*。 Ieee, 2009, 页. 248–255.'
- en: (170) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, “Transfer
    learning for time series classification,” in *2018 IEEE international conference
    on big data (Big Data)*.   IEEE, 2018, pp. 1367–1376.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (170) H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar 和 P.-A. Muller, “时间序列分类的转移学习,”
    收录于*2018 IEEE 国际大数据大会（Big Data）*。 IEEE, 2018, 页. 1367–1376.
- en: (171) S. Spiegel, “Transfer learning for time series classification in dissimilarity
    spaces,” *Proceedings of AALTD*, vol. 78, 2016.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (171) S. Spiegel, “在不相似空间中的时间序列分类转移学习,” *AALTD 会议论文集*, 卷. 78, 2016.
- en: (172) F. Li, K. Shirahama, M. A. Nisar, X. Huang, and M. Grzegorzek, “Deep transfer
    learning for time series data based on sensor modality classification,” *Sensors*,
    vol. 20, no. 15, p. 4271, 2020.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (172) F. Li, K. Shirahama, M. A. Nisar, X. Huang 和 M. Grzegorzek, “基于传感器模态分类的深度转移学习用于时间序列数据,”
    *传感器*, 卷. 20, 期. 15, 页. 4271, 2020.
- en: (173) Y. Rotem, N. Shimoni, L. Rokach, and B. Shapira, “Transfer learning for
    time series classification using synthetic data generation,” in *International
    Symposium on Cyber Security, Cryptology, and Machine Learning*.   Springer, 2022,
    pp. 232–246.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (173) Y. Rotem, N. Shimoni, L. Rokach 和 B. Shapira, “利用合成数据生成的转移学习用于时间序列分类,”
    收录于*国际网络安全、密码学与机器学习研讨会*。 Springer, 2022, 页. 232–246.
- en: (174) A. Senanayaka, A. Al Mamun, G. Bond, W. Tian, H. Wang, S. Fuller, T. Falls,
    S. Rahimi, and L. Bian, “Similarity-based multi-source transfer learning approach
    for time series classification,” *International Journal of Prognostics and Health
    Management*, vol. 13, no. 2, 2022.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (174) A. Senanayaka, A. Al Mamun, G. Bond, W. Tian, H. Wang, S. Fuller, T. Falls,
    S. Rahimi 和 L. Bian, “基于相似性的多源转移学习方法用于时间序列分类,” *国际预警与健康管理杂志*, 卷. 13, 期. 2, 2022.
- en: '(175) K. Kashiparekh, J. Narwariya, P. Malhotra, L. Vig, and G. Shroff, “Convtimenet:
    A pre-trained deep convolutional neural network for time series classification,”
    in *2019 International Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2019,
    pp. 1–8.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(175) K. Kashiparekh, J. Narwariya, P. Malhotra, L. Vig, 和 G. Shroff, “Convtimenet:
    用于时间序列分类的预训练深度卷积神经网络，” 收录于 *2019国际联合神经网络大会 (IJCNN)*. IEEE, 2019, pp. 1–8。'
- en: (176) D. Merlin Praveena, D. Angelin Sarah, and S. Thomas George, “Deep Learning
    Techniques for EEG Signal Applications–A Review,” *IETE Journal of Research*,
    vol. 68, no. 4, pp. 3030–3037, 2022.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (176) D. Merlin Praveena, D. Angelin Sarah, 和 S. Thomas George, “用于EEG信号应用的深度学习技术综述，”
    *IETE研究杂志*, vol. 68, no. 4, pp. 3030–3037, 2022年。
- en: '(177) X. Liu, H. Wang, Z. Li, and L. Qin, “Deep learning in ECG diagnosis:
    A review,” *Knowledge-Based Systems*, vol. 227, p. 107187, 2021.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (177) X. Liu, H. Wang, Z. Li, 和 L. Qin, “ECG诊断中的深度学习：综述，” *知识基础系统*, vol. 227,
    p. 107187, 2021年。
- en: (178) N. Zaini, L. W. Ean, A. N. Ahmed, and M. A. Malek, “A systematic literature
    review of deep learning neural network for time series air quality forecasting,”
    *Environmental Science and Pollution Research*, vol. 29, no. 4, pp. 4958–4990,
    jan 2022.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (178) N. Zaini, L. W. Ean, A. N. Ahmed, 和 M. A. Malek, “时间序列空气质量预测的深度学习神经网络系统文献综述，”
    *环境科学与污染研究*, vol. 29, no. 4, pp. 4958–4990, 2022年1月。
- en: '(179) B. Zhang, Y. Rong, R. Yong, D. Qin, M. Li, G. Zou, and J. Pan, “Deep
    learning for air pollutant concentration prediction: A review,” *Atmospheric Environment*,
    vol. 290, p. 119347, dec 2022.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (179) B. Zhang, Y. Rong, R. Yong, D. Qin, M. Li, G. Zou, 和 J. Pan, “空气污染物浓度预测的深度学习综述，”
    *大气环境*, vol. 290, p. 119347, 2022年12月。
- en: (180) G. Toh and J. Park, “Review of Vibration-Based Structural Health Monitoring
    Using Deep Learning,” *Appl. Sci.*, vol. 10, no. 5, p. 1680, 2020.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (180) G. Toh 和 J. Park, “基于振动的结构健康监测的深度学习综述，” *应用科学*, vol. 10, no. 5, p. 1680,
    2020年。
- en: '(181) N. M. Thoppil, V. Vasu, and C. S. P. Rao, “Deep Learning Algorithms for
    Machinery Health Prognostics Using Time-Series Data: A Review,” *Journal of Vibration
    Engineering & Technologies*, vol. 9, no. 6, pp. 1123–1145, sep 2021.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (181) N. M. Thoppil, V. Vasu, 和 C. S. P. Rao, “用于机器健康预测的深度学习算法综述：基于时间序列数据，”
    *振动工程与技术杂志*, vol. 9, no. 6, pp. 1123–1145, 2021年9月。
- en: '(182) L. Ren, Z. Jia, Y. Laili, and D. Huang, “Deep Learning for Time-Series
    Prediction in IIoT: Progress, Challenges, and Prospects,” *IEEE Transactions on
    Neural Networks and Learning Systems*, vol. PP, pp. 1–20, 2023.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (182) L. Ren, Z. Jia, Y. Laili, 和 D. Huang, “IIoT中时间序列预测的深度学习：进展、挑战与前景，” *IEEE神经网络与学习系统汇刊*,
    vol. PP, pp. 1–20, 2023年。
- en: '(183) Y. Himeur, K. Ghanem, A. Alsalemi, F. Bensaali, and A. Amira, “Artificial
    intelligence based anomaly detection of energy consumption in buildings: A review,
    current trends and new perspectives,” *Applied Energy*, vol. 287, p. 116601, 2021.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (183) Y. Himeur, K. Ghanem, A. Alsalemi, F. Bensaali, 和 A. Amira, “基于人工智能的建筑能源消耗异常检测：综述、当前趋势及新视角，”
    *应用能源*, vol. 287, p. 116601, 2021年。
- en: '(184) D. Stowell, “Computational bioacoustics with deep learning: a review
    and roadmap,” *PeerJ*, vol. 10, p. e13152, mar 2022.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (184) D. Stowell, “深度学习的计算生物声学：综述与路线图，” *PeerJ*, vol. 10, p. e13152, 2022年3月。
- en: '(185) N. Gupta, S. K. Gupta, R. K. Pathak, V. Jain, P. Rashidi, and J. S. Suri,
    “Human activity recognition in artificial intelligence framework: a narrative
    review,” *Artificial Intelligence Review*, vol. 55, no. 6, pp. 4755–4808, aug
    2022.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (185) N. Gupta, S. K. Gupta, R. K. Pathak, V. Jain, P. Rashidi, 和 J. S. Suri,
    “人工智能框架中的人类活动识别：叙述性综述，” *人工智能综述*, vol. 55, no. 6, pp. 4755–4808, 2022年8月。
- en: '(186) E. Ramanujam, T. Perumal, and S. Padmavathi, “Human activity recognition
    with smartphone and wearable sensors using deep learning techniques: A review,”
    *IEEE Sensors Journal*, vol. 21, no. 12, pp. 13 029–13 040, jun 2021.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (186) E. Ramanujam, T. Perumal, 和 S. Padmavathi, “使用深度学习技术的智能手机和可穿戴传感器进行人类活动识别：综述，”
    *IEEE传感器杂志*, vol. 21, no. 12, pp. 13 029–13 040, 2021年6月。
- en: '(187) J. W. Lockhart, T. Pulickal, and G. M. Weiss, “Applications of mobile
    activity recognition,” in *2012 ACM Conference on Ubiquitous Computing - UbiComp
    ’12*.   New York, New York, USA: ACM Press, 2012, p. 1054.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(187) J. W. Lockhart, T. Pulickal, 和 G. M. Weiss, “移动活动识别的应用，” 收录于 *2012年ACM普及计算会议
    - UbiComp ’12*. 纽约, 美国: ACM Press, 2012, p. 1054。'
- en: '(188) E. M. Tapia, S. S. Intille, and K. Larson, “Activity recognition in the
    home using simple and ubiquitous sensors,” in *Lecture Notes in Computer Science*.   Berlin,
    Heidelberg: Springer, 2004, vol. 3001, pp. 158–175.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(188) E. M. Tapia, S. S. Intille, 和 K. Larson, “使用简单而普遍的传感器在家庭中进行活动识别，” 收录于
    *计算机科学讲义*.  柏林, 海德堡: Springer, 2004, vol. 3001, pp. 158–175。'
- en: '(189) Y. Kong and Y. Fu, “Human action recognition and prediction: A survey,”
    *International Journal of Computer Vision*, vol. 130, no. 5, pp. 1366–1401, may
    2022.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (189) Y. Kong 和 Y. Fu，“人类动作识别与预测：综述，” *国际计算机视觉期刊*，第 130 卷，第 5 期，第 1366–1401
    页，2022 年 5 月。
- en: (190) H.-B. Zhang, Y.-X. Zhang, B. Zhong, Q. Lei, L. Yang, J.-X. Du, and D.-S.
    Chen, “A comprehensive survey of vision-based human action recognition methods,”
    *Sensors*, vol. 19, no. 5, p. 1005, feb 2019.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (190) H.-B. Zhang, Y.-X. Zhang, B. Zhong, Q. Lei, L. Yang, J.-X. Du 和 D.-S.
    Chen，“基于视觉的人类动作识别方法综述，” *Sensors*，第 19 卷，第 5 期，第 1005 页，2019 年 2 月。
- en: (191) F. Ordóñez and D. Roggen, “Deep convolutional and LSTM recurrent neural
    networks for multimodal wearable activity recognition,” *Sensors*, vol. 16, no. 1,
    p. 115, jan 2016.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (191) F. Ordóñez 和 D. Roggen，“用于多模态可穿戴活动识别的深度卷积和 LSTM 递归神经网络，” *Sensors*，第 16
    卷，第 1 期，第 115 页，2016 年 1 月。
- en: (192) A. Reiss and D. Stricker, “Introducing a new benchmarked dataset for activity
    monitoring,” in *16th Int. Symp. Wearable Computers*, 2012, pp. 108–109.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (192) A. Reiss 和 D. Stricker，“引入一个新的活动监测基准数据集，” 见于 *第 16 届可穿戴计算国际研讨会*，2012 年，第
    108–109 页。
- en: '(193) M. Zhang and A. A. Sawchuk, “USC-HAD: A daily activity dataset for ubiquitous
    activity recognition using wearable sensors,” in *2012 ACM Conference on Ubiquitous
    Computing - UbiComp ’12*.   New York, New York, USA: ACM Press, 2012, p. 1036.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (193) M. Zhang 和 A. A. Sawchuk，“USC-HAD：用于普遍活动识别的每日活动数据集，利用可穿戴传感器，” 见于 *2012
    ACM 计算机普及会议 - UbiComp ’12*。纽约，纽约，美国：ACM Press，2012 年，第 1036 页。
- en: (194) D. Roggen, A. Calatroni, M. Rossi, T. Holleczek, K. Förster, G. Tröster,
    P. Lukowicz, D. Bannach, G. Pirkl *et al.*, “Collecting complex activity datasets
    in highly rich networked sensor environments,” in *Seventh international conference
    on networked sensing systems*.   IEEE, 2010, pp. 233–240.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (194) D. Roggen, A. Calatroni, M. Rossi, T. Holleczek, K. Förster, G. Tröster,
    P. Lukowicz, D. Bannach, G. Pirkl *等*，“在高度丰富的网络传感器环境中收集复杂活动数据集，” 见于 *第七届网络传感系统国际会议*。IEEE，2010
    年，第 233–240 页。
- en: (195) T. Sztyler, H. Stuckenschmidt, and W. Petrich, “Position-aware activity
    recognition with wearable devices,” *Pervasive and Mobile Computing*, vol. 38,
    pp. 281–295, jul 2017.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (195) T. Sztyler, H. Stuckenschmidt 和 W. Petrich，“基于可穿戴设备的位置信息感知活动识别，” *Pervasive
    and Mobile Computing*，第 38 卷，第 281–295 页，2017 年 7 月。
- en: (196) O. D. Lara and M. A. Labrador, “A survey on human activity recognition
    using wearable sensors,” *IEEE Communications Surveys & Tutorials*, vol. 15, no. 3,
    pp. 1192–1209, 2013.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (196) O. D. Lara 和 M. A. Labrador，“基于可穿戴传感器的人类活动识别综述，” *IEEE Communications
    Surveys & Tutorials*，第 15 卷，第 3 期，第 1192–1209 页，2013 年。
- en: (197) F. Gu, M.-H. Chung, M. Chignell, S. Valaee, B. Zhou, and X. Liu, “A survey
    on deep learning for human activity recognition,” *ACM Computing Surveys*, vol. 54,
    no. 8, pp. 1–34, nov 2022.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (197) F. Gu, M.-H. Chung, M. Chignell, S. Valaee, B. Zhou 和 X. Liu，“关于人类活动识别的深度学习综述，”
    *ACM Computing Surveys*，第 54 卷，第 8 期，第 1–34 页，2022 年 11 月。
- en: (198) N. Y. Hammerla, S. Halloran, and T. Ploetz, “Deep, convolutional, and
    recurrent Models for human activity recognition using wearables,” *IJCAI International
    Joint Conference on Artificial Intelligence*, vol. 2016-Janua, pp. 1533–1540,
    apr 2016.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (198) N. Y. Hammerla, S. Halloran 和 T. Ploetz，“用于人类活动识别的深度卷积和递归模型，” *IJCAI 国际人工智能联合会议*，第
    2016-Janua 卷，第 1533–1540 页，2016 年 4 月。
- en: (199) M. Zeng, L. T. Nguyen, B. Yu, O. J. Mengshoel, J. Zhu, P. Wu, and J. Zhang,
    “Convolutional neural networks for human activity recognition using mobile sensors,”
    in *6th International Conference on Mobile Computing, Applications and Services*.   ICST,
    2014, pp. 718–737.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (199) M. Zeng, L. T. Nguyen, B. Yu, O. J. Mengshoel, J. Zhu, P. Wu 和 J. Zhang，“利用移动传感器的卷积神经网络进行人类活动识别，”
    见于 *第六届移动计算、应用与服务国际会议*。ICST，2014 年，第 718–737 页。
- en: '(200) W. Jiang and Z. Yin, “Human activity recognition using wearable sensors
    by deep convolutional neural Networks,” in *23rd ACM international conference
    on Multimedia*.   New York, NY, USA: ACM, oct 2015, pp. 1307–1310.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (200) W. Jiang 和 Z. Yin，“利用可穿戴传感器通过深度卷积神经网络进行人类活动识别，” 见于 *第 23 届 ACM 国际多媒体会议*。纽约，NY，美国：ACM，2015
    年 10 月，第 1307–1310 页。
- en: (201) J. B. Yang, M. N. Nguyen, P. P. San, X. L. Li, and S. Krishnaswamy, “Deep
    convolutional neural networks on multichannel time series for human activity recognition,”
    *IJCAI International Joint Conference on Artificial Intelligence*, vol. 2015-Janua,
    pp. 3995–4001, 2015.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (201) J. B. Yang, M. N. Nguyen, P. P. San, X. L. Li 和 S. Krishnaswamy，“基于多通道时间序列的深度卷积神经网络进行人类活动识别，”
    *IJCAI 国际人工智能联合会议*，第 2015-Janua 卷，第 3995–4001 页，2015 年。
- en: (202) C. A. Ronao and S.-B. Cho, “Human activity recognition with smartphone
    sensors using deep learning neural networks,” *Expert Systems with Applications*,
    vol. 59, pp. 235–244, oct 2016.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (202) C. A. Ronao 和 S.-B. Cho，“利用深度学习神经网络通过智能手机传感器进行人类活动识别”，*应用专家系统*，第59卷，第235–244页，2016年10月。
- en: (203) Y. Guan and T. Plötz, “Ensembles of deep LSTM learners for activity recognition
    using wearables,” *ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    vol. 1, no. 2, pp. 1–28, jun 2017.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (203) Y. Guan 和 T. Plötz，“用于活动识别的深度LSTM学习器集成，基于可穿戴设备”，*ACM 互动、移动、可穿戴与普适技术*，第1卷，第2期，第1–28页，2017年6月。
- en: (204) S.-M. Lee, S. M. Yoon, and H. Cho, “Human activity recognition from accelerometer
    data using Convolutional Neural Network,” in *2017 IEEE International Conference
    on Big Data and Smart Computing (BigComp)*, vol. 83.   IEEE, feb 2017, pp. 131–134.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (204) S.-M. Lee, S. M. Yoon, 和 H. Cho，“利用卷积神经网络从加速度计数据中进行人类活动识别”，在 *2017 IEEE
    国际大数据与智能计算会议 (BigComp)*，第83卷。IEEE，2017年2月，第131–134页。
- en: (205) A. Murad and J.-Y. Pyun, “Deep recurrent neural networks for human activity
    recognition,” *Sensors*, vol. 17, no. 11, p. 2556, nov 2017.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (205) A. Murad 和 J.-Y. Pyun，“用于人类活动识别的深度递归神经网络”，*传感器*，第17卷，第11期，第2556页，2017年11月。
- en: (206) A. Ignatov, “Real-time human activity recognition from accelerometer data
    using Convolutional Neural Networks,” *Applied Soft Computing*, vol. 62, pp. 915–922,
    jan 2018.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (206) A. Ignatov，“利用卷积神经网络从加速度计数据中进行实时人类活动识别”，*应用软计算*，第62卷，第915–922页，2018年1月。
- en: (207) F. Moya Rueda, R. Grzeszick, G. Fink, S. Feldhorst, and M. ten Hompel,
    “Convolutional neural networks for human activity recognition using body-worn
    sensors,” *Informatics*, vol. 5, no. 2, p. 26, may 2018.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (207) F. Moya Rueda, R. Grzeszick, G. Fink, S. Feldhorst, 和 M. ten Hompel，“利用体穿传感器的卷积神经网络进行人类活动识别”，*信息学*，第5卷，第2期，第26页，2018年5月。
- en: (208) R. Yao, G. Lin, Q. Shi, and D. C. Ranasinghe, “Efficient dense labelling
    of human activity sequences from wearables using fully convolutional networks,”
    *Pattern Recognition*, vol. 78, pp. 252–266, jun 2018.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (208) R. Yao, G. Lin, Q. Shi, 和 D. C. Ranasinghe，“使用全卷积网络高效地对来自可穿戴设备的人类活动序列进行密集标注”，*模式识别*，第78卷，第252–266页，2018年6月。
- en: (209) M. Zeng, H. Gao, T. Yu, O. J. Mengshoel, H. Langseth, I. Lane, and X. Liu,
    “Understanding and improving recurrent networks for human activity recognition
    by continuous attention,” in *ACM International Symposium on Wearable Computers*,
    New York, NY, USA, 2018, pp. 56–63.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (209) M. Zeng, H. Gao, T. Yu, O. J. Mengshoel, H. Langseth, I. Lane, 和 X. Liu，“通过持续关注理解和改进递归网络用于人类活动识别”，在
    *ACM 国际可穿戴计算机研讨会*，纽约，美国，2018年，第56–63页。
- en: '(210) H. Ma, W. Li, X. Zhang, S. Gao, and S. Lu, “AttnSense: Multi-level attention
    mechanism for multimodal human activity recognition,” in *Twenty-Eighth International
    Joint Conference on Artificial Intelligence*, California, 2019, pp. 3109–3115.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (210) H. Ma, W. Li, X. Zhang, S. Gao, 和 S. Lu，“AttnSense：用于多模态人类活动识别的多层次注意机制”，在
    *第二十八届国际人工智能联合会议*，加州，2019年，第3109–3115页。
- en: '(211) C. Xu, D. Chai, J. He, X. Zhang, and S. Duan, “InnoHAR: A deep neural
    network for complex human activity recognition,” *IEEE Access*, vol. 7, pp. 9893–9902,
    2019.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (211) C. Xu, D. Chai, J. He, X. Zhang, 和 S. Duan，“InnoHAR：用于复杂人类活动识别的深度神经网络”，*IEEE
    Access*，第7卷，第9893–9902页，2019年。
- en: (212) H. Zhang, Z. Xiao, J. Wang, F. Li, and E. Szczerbicki, “A novel IoT-perceptive
    human activity recognition (HAR) approach using multihead convolutional attention,”
    *IEEE Internet of Things Journal*, vol. 7, no. 2, pp. 1072–1080, feb 2020.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (212) H. Zhang, Z. Xiao, J. Wang, F. Li, 和 E. Szczerbicki，“一种新颖的IoT感知人类活动识别（HAR）方法，基于多头卷积注意力”，*IEEE物联网期刊*，第7卷，第2期，第1072–1080页，2020年2月。
- en: (213) S. K. Challa, A. Kumar, and V. B. Semwal, “A multibranch CNN-BiLSTM model
    for human activity recognition using wearable sensor data,” *The Visual Computer*,
    no. 0123456789, aug 2021.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (213) S. K. Challa, A. Kumar, 和 V. B. Semwal，“用于人类活动识别的多分支CNN-BiLSTM模型，基于可穿戴传感器数据”，*视觉计算*，第0123456789号，2021年8月。
- en: (214) S. Mekruksavanich and A. Jitpattanakul, “Deep Convolutional Neural Network
    with RNNs for complex activity recognition using wrist-worn wearable sensor data,”
    *Electronics*, vol. 10, no. 14, p. 1685, jul 2021.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (214) S. Mekruksavanich 和 A. Jitpattanakul，“结合递归神经网络的深度卷积神经网络用于复杂活动识别，基于腕戴可穿戴传感器数据”，*电子学*，第10卷，第14期，第1685页，2021年7月。
- en: (215) L. Chen, X. Liu, L. Peng, and M. Wu, “Deep learning based multimodal complex
    human activity recognition using wearable devices,” *Applied Intelligence*, vol. 51,
    no. 6, pp. 4029–4042, jun 2021.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (215) L. Chen, X. Liu, L. Peng, 和 M. Wu，“基于深度学习的多模态复杂人类活动识别，使用可穿戴设备”，*应用智能*，第51卷，第6期，第4029–4042页，2021年6月。
- en: (216) S. Mekruksavanich and A. Jitpattanakul, “LSTM networks using smartphone
    data for sensor-based human activity recognition in smart homes,” *Sensors*, vol. 21,
    no. 5, p. 1636, feb 2021.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (216) S. Mekruksavanich 和 A. Jitpattanakul，“使用智能手机数据的LSTM网络用于智能家居中的基于传感器的人类活动识别，”
    *传感器*，第21卷，第5期，页1636，2021年2月。
- en: '(217) ——, “Biometric user identification based on human activity recognition
    using wearable sensors: An experiment using deep learning models,” *Electronics*,
    vol. 10, no. 3, p. 308, jan 2021.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (217) ——，“基于可穿戴传感器的人类活动识别的生物识别用户身份验证：使用深度学习模型的实验，” *电子学*，第10卷，第3期，页308，2021年1月。
- en: (218) O. Nafea, W. Abdul, G. Muhammad, and M. Alsulaiman, “Sensor-based human
    activity recognition with spatio-temporal deep learning,” *Sensors*, vol. 21,
    no. 6, p. 2141, mar 2021.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (218) O. Nafea, W. Abdul, G. Muhammad, 和 M. Alsulaiman，“基于传感器的人类活动识别与时空深度学习，”
    *传感器*，第21卷，第6期，页2141，2021年3月。
- en: (219) S. P. Singh, M. K. Sharma, A. Lay-Ekuakille, D. Gangwar, and S. Gupta,
    “Deep ConvLSTM with self-attention for human activity decoding using wearable
    sensors,” *IEEE Sensors Journal*, vol. 21, no. 6, pp. 8575–8582, mar 2021.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (219) S. P. Singh, M. K. Sharma, A. Lay-Ekuakille, D. Gangwar, 和 S. Gupta，“具有自注意力的深度ConvLSTM用于可穿戴传感器的人类活动解码，”
    *IEEE传感器学报*，第21卷，第6期，页8575–8582，2021年3月。
- en: (220) X. Wang, L. Zhang, W. Huang, S. Wang, H. Wu, J. He, and A. Song, “Deep
    convolutional networks with tunable speed–accuracy tradeoff for human activity
    recognition using wearables,” *IEEE Transactions on Instrumentation and Measurement*,
    vol. 71, pp. 1–12, 2022.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (220) X. Wang, L. Zhang, W. Huang, S. Wang, H. Wu, J. He, 和 A. Song，“具有可调速度–准确度权衡的深度卷积网络用于基于可穿戴设备的人类活动识别，”
    *IEEE仪器与测量学报*，第71卷，页1–12，2022年。
- en: (221) S. Xu, L. Zhang, W. Huang, H. Wu, and A. Song, “Deformable convolutional
    networks for multimodal human activity recognition using wearable sensors,” *IEEE
    Transactions on Instrumentation and Measurement*, vol. 71, pp. 1–14, 2022.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (221) S. Xu, L. Zhang, W. Huang, H. Wu, 和 A. Song，“用于多模态人类活动识别的可变形卷积网络，使用可穿戴传感器，”
    *IEEE仪器与测量学报*，第71卷，页1–14，2022年。
- en: (222) J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable
    convolutional networks,” in *2017 IEEE Int. Conf. Computer Vision (ICCV)*, 2017,
    pp. 764–773.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (222) J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, 和 Y. Wei，“可变形卷积网络”，见
    *2017 IEEE国际计算机视觉会议 (ICCV)*，2017年，页764–773。
- en: '(223) M. A. Wulder, J. C. White, S. N. Goward, J. G. Masek, J. R. Irons, M. Herold,
    W. B. Cohen, T. R. Loveland, and C. E. Woodcock, “Landsat continuity: Issues and
    opportunities for land cover monitoring,” *Remote Sensing of Environment*, vol.
    112, no. 3, pp. 955–969, mar 2008.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (223) M. A. Wulder, J. C. White, S. N. Goward, J. G. Masek, J. R. Irons, M. Herold,
    W. B. Cohen, T. R. Loveland, 和 C. E. Woodcock，“Landsat的连续性：土地覆盖监测的问题与机会，” *环境遥感*，第112卷，第3期，页955–969，2008年3月。
- en: (224) W. Emery and A. Camps, “Basic electromagnetic concepts and applications
    to optical sensors,” in *Introduction to Satellite Remote Sensing*, W. Emery and
    A. Camps, Eds.   Elsevier, jan 2017, ch. 2, pp. 43–83.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (224) W. Emery 和 A. Camps，“基本电磁概念及其在光学传感器中的应用，”见 *卫星遥感简介*，W. Emery 和 A. Camps
    编辑，Elsevier，2017年1月，第2章，页43–83。
- en: '(225) N. Gorelick, M. Hancher, M. Dixon, S. Ilyushchenko, D. Thau, and R. Moore,
    “Google Earth Engine: Planetary-scale geospatial analysis for everyone,” *Remote
    Sensing of Environment*, vol. 202, pp. 18–27, dec 2017.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (225) N. Gorelick, M. Hancher, M. Dixon, S. Ilyushchenko, D. Thau, 和 R. Moore，“Google
    Earth Engine：为每个人提供的行星级地理空间分析，” *环境遥感*，第202卷，页18–27，2017年12月。
- en: '(226) G. Giuliani, B. Chatenoux, A. De Bono, D. Rodila, J.-P. Richard, K. Allenbach,
    H. Dao, and P. Peduzzi, “Building an Earth observations data cube: lessons learned
    from the Swiss data cube (SDC) on generating analysis ready data (ARD),” *Big
    Earth Data*, vol. 1, no. 1-2, pp. 100–117, dec 2017.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (226) G. Giuliani, B. Chatenoux, A. De Bono, D. Rodila, J.-P. Richard, K. Allenbach,
    H. Dao, 和 P. Peduzzi，“构建地球观测数据立方体：瑞士数据立方体 (SDC) 在生成分析就绪数据 (ARD) 中的经验教训，” *大地数据*，第1卷，第1-2期，页100–117，2017年12月。
- en: (227) A. Lewis, S. Oliver, L. Lymburner, B. Evans, L. Wyborn, N. Mueller, G. Raevksi,
    J. Hooke, R. Woodcock, J. Sixsmith *et al.*, “The australian geoscience data cube—foundations
    and lessons learned,” *Remote Sensing of Environment*, vol. 202, pp. 276–292,
    2017.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (227) A. Lewis, S. Oliver, L. Lymburner, B. Evans, L. Wyborn, N. Mueller, G. Raevksi,
    J. Hooke, R. Woodcock, J. Sixsmith *等*，“澳大利亚地球科学数据立方体——基础与经验教训，” *环境遥感*，第202卷，页276–292，2017年。
- en: (228) D. Ienco, Y. J. E. Gbodjo, R. Interdonato, and R. Gaetano, “Attentive
    weakly supervised land cover mapping for object-based satellite image time series
    data with spatial interpretation,” *arXiv*, pp. 1–12, 2020.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (228) D. Ienco, Y. J. E. Gbodjo, R. Interdonato, 和 R. Gaetano，“基于空间解释的对象级卫星图像时间序列数据的注意力弱监督土地覆盖映射，”
    *arXiv*，页1–12，2020年。
- en: (229) V. Sainte Fare Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Satellite
    image time Series classification With Pixel-Set encoders and temporal self-attention,”
    in *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.   IEEE,
    jun 2020, pp. 12 322–12 331.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (229) V. Sainte Fare Garnot, L. Landrieu, S. Giordano 和 N. Chehata, “使用像素集编码器和时间自注意力的卫星图像时间序列分类，”
    在 *2020 IEEE/CVF计算机视觉与模式识别会议（CVPR）*。 IEEE，2020年6月，第12 322–12 331页。
- en: (230) A. Kulshrestha, L. Chang, and A. Stein, “Use of LSTM for sinkhole-related
    anomaly detection and classification of InSAR deformation time series,” *IEEE
    Journal of Selected Topics in Applied Earth Observations and Remote Sensing*,
    vol. 15, pp. 4559–4570, 2022.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (230) A. Kulshrestha, L. Chang 和 A. Stein, “使用LSTM进行与地面沉降相关的异常检测和InSAR变形时间序列分类，”
    *IEEE应用地球观测与遥感精选话题期刊*，第15卷，第4559–4570页，2022年。
- en: (231) Y. Ban, P. Zhang, A. Nascetti, A. R. Bevington, and M. A. Wulder, “Near
    real-time wildfire progression monitoring with Sentinel-1 SAR time series and
    deep learning,” *Scientific Reports*, vol. 10, no. 1, p. 1322, dec 2020.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (231) Y. Ban, P. Zhang, A. Nascetti, A. R. Bevington 和 M. A. Wulder, “利用Sentinel-1
    SAR时间序列和深度学习进行近实时野火进展监测，” *科学报告*，第10卷，第1期，第1322页，2020年12月。
- en: (232) C. Rambour, N. Audebert, E. Koeniguer, B. Le Saux, M. Crucianu, and M. Datcu,
    “Flood detection in time series of optical and SAR images,” *Int. Archives Photogrammetry,
    Remote Sens. & Spatial Inf. Sci.*, vol. XLIII-B2-2, no. B2, pp. 1343–1346, aug
    2020.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (232) C. Rambour, N. Audebert, E. Koeniguer, B. Le Saux, M. Crucianu 和 M. Datcu,
    “时间序列光学和SAR图像中的洪水检测，” *国际摄影测量、遥感与空间信息科学档案*，第XLIII-B2-2卷，第B2期，第1343–1346页，2020年8月。
- en: (233) G. Kamdem De Teyou, Y. Tarabalka, I. Manighetti, R. Almar, and S. Tripodi,
    “Deep neural networks for automatic extraction of features in time series optical
    satellite images,” *Int. Archives Photogrammetry, Remote Sens. & Spatial Inf.
    Sci.*, vol. 43, 2020.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (233) G. Kamdem De Teyou, Y. Tarabalka, I. Manighetti, R. Almar 和 S. Tripodi,
    “用于时间序列光学卫星图像特征自动提取的深度神经网络，” *国际摄影测量、遥感与空间信息科学档案*，第43卷，2020年。
- en: (234) B. M. Matosak, L. M. G. Fonseca, E. C. Taquary, R. V. Maretto, H. D. N.
    Bendini, and M. Adami, “Mapping deforestation in Cerrado based on hybrid deep
    learning architecture and medium spatial resolution satellite time series,” *Remote
    Sensing*, vol. 14, no. 1, pp. 1–22, 2022.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (234) B. M. Matosak, L. M. G. Fonseca, E. C. Taquary, R. V. Maretto, H. D. N.
    Bendini 和 M. Adami, “基于混合深度学习架构和中等空间分辨率卫星时间序列的Cerrado地区森林砍伐映射，” *遥感*，第14卷，第1期，第1–22页，2022年。
- en: (235) D. Ho Tong Minh, D. Ienco, R. Gaetano, N. Lalande, E. Ndikumana, F. Osman,
    and P. Maurel, “Deep recurrent neural networks for winter vegetation quality mapping
    via multitemporal SAR Sentinel-1,” *IEEE Geoscience and Remote Sensing Letters*,
    vol. 15, no. 3, pp. 464–468, mar 2018.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (235) D. Ho Tong Minh, D. Ienco, R. Gaetano, N. Lalande, E. Ndikumana, F. Osman
    和 P. Maurel, “基于多时相SAR Sentinel-1的冬季植被质量映射的深度递归神经网络，” *IEEE地球科学与遥感通讯快报*，第15卷，第3期，第464–468页，2018年3月。
- en: (236) P. Labenski, M. Ewald, S. Schmidtlein, and F. E. Fassnacht, “Classifying
    surface fuel types based on forest stand photographs and satellite time series
    using deep learning,” *International Journal of Applied Earth Observation and
    Geoinformation*, vol. 109, p. 102799, may 2022.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (236) P. Labenski, M. Ewald, S. Schmidtlein 和 F. E. Fassnacht, “基于森林立木照片和卫星时间序列的地表燃料类型分类使用深度学习，”
    *国际应用地球观测与地理信息学期刊*，第109卷，第102799页，2022年5月。
- en: (237) K. Rao, A. P. Williams, J. F. Flefil, and A. G. Konings, “SAR-enhanced
    mapping of live fuel moisture content,” *Remote Sens. Environ.*, vol. 245, p.
    111797, 2020.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (237) K. Rao, A. P. Williams, J. F. Flefil 和 A. G. Konings, “增强合成孔径雷达的实时燃料湿度内容映射，”
    *遥感环境*，第245卷，第111797页，2020年。
- en: '(238) L. Zhu, G. I. Webb, M. Yebra, G. Scortechini, L. Miller, and F. Petitjean,
    “Live fuel moisture content estimation from MODIS: A deep learning approach,”
    *ISPRS J. Photogramm. Remote Sens.*, vol. 179, pp. 81–91, sep 2021.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (238) L. Zhu, G. I. Webb, M. Yebra, G. Scortechini, L. Miller 和 F. Petitjean,
    “基于MODIS的实时燃料湿度内容估计：一种深度学习方法，” *国际摄影测量与遥感学报*，第179卷，第81–91页，2021年9月。
- en: (239) L. Miller, L. Zhu, M. Yebra, C. Rüdiger, and G. I. Webb, “Multi-modal
    temporal CNNs for live fuel moisture content estimation,” *Environmental Modelling
    & Software*, vol. 156, p. 105467, oct 2022.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (239) L. Miller, L. Zhu, M. Yebra, C. Rüdiger 和 G. I. Webb, “用于实时燃料湿度估计的多模态时间卷积神经网络，”
    *环境建模与软件*，第156卷，第105467页，2022年10月。
- en: (240) J. Xie, T. Qi, W. Hu, H. Huang, B. Chen, and J. Zhang, “Retrieval of live
    fuel moisture content based on multi-source remote sensing data and ensemble deep
    learning model,” *Remote Sensing*, vol. 14, no. 17, p. 4378, sep 2022.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (240) J. Xie, T. Qi, W. Hu, H. Huang, B. Chen 和 J. Zhang, “基于多源遥感数据和集成深度学习模型的实时燃料湿度内容检索，”
    *遥感*，第14卷，第17期，第4378页，2022年9月。
- en: (241) K. Lahssini, F. Teste, K. R. Dayal, S. Durrieu, D. Ienco, and J.-M. Monnet,
    “Combining LiDAR metrics and Sentinel-2 imagery to estimate basal area and wood
    volume in complex forest environment via neural networks,” *IEEE J. Selected Topics
    Applied Earth Obs. Remote Sens.*, vol. 15, pp. 4337–4348, 2022.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (241) K. Lahssini, F. Teste, K. R. Dayal, S. Durrieu, D. Ienco 和 J.-M. Monnet，“结合LiDAR度量和Sentinel-2影像，通过神经网络估算复杂森林环境中的基面积和木材体积”，*IEEE
    J. Selected Topics Applied Earth Obs. Remote Sens.*，第15卷，第4337–4348页，2022年。
- en: (242) J. Sun, Z. Lai, L. Di, Z. Sun, J. Tao, and Y. Shen, “Multilevel deep learning
    network for county-level corn yield estimation in the U.S. Corn Belt,” *IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing*, vol. 13,
    pp. 5048–5060, 2020.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (242) J. Sun, Z. Lai, L. Di, Z. Sun, J. Tao 和 Y. Shen，“用于美国玉米带县级玉米产量估算的多层次深度学习网络”，*IEEE
    Journal of Selected Topics in Applied Earth Observations and Remote Sensing*，第13卷，第5048–5060页，2020年。
- en: (243) Z. Li, G. Chen, and T. Zhang, “Temporal attention networks for multitemporal
    multisensor crop classification,” *IEEE Access*, vol. 7, pp. 134 677–134 690,
    2019.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (243) Z. Li, G. Chen 和 T. Zhang，“用于多时相多传感器作物分类的时序注意力网络”，*IEEE Access*，第7卷，第134677–134690页，2019年。
- en: (244) Z. Li, G. Zhou, and Q. Song, “A temporal group attention approach for
    multitemporal multisensor crop classification,” *Infrared Physics and Technology*,
    vol. 105, p. 103152, 2020.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (244) Z. Li, G. Zhou 和 Q. Song，“一种用于多时相多传感器作物分类的时序组注意力方法”，*Infrared Physics
    and Technology*，第105卷，第103152页，2020年。
- en: (245) S. Ji, C. Zhang, A. Xu, Y. Shi, and Y. Duan, “3D convolutional neural
    networks for crop classification with multi-temporal remote sensing images,” *Remote
    Sensing*, vol. 10, no. 2, p. 75, jan 2018.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (245) S. Ji, C. Zhang, A. Xu, Y. Shi 和 Y. Duan，“使用多时相遥感图像的3D卷积神经网络进行作物分类”，*Remote
    Sensing*，第10卷，第2期，第75页，2018年1月。
- en: '(246) J. Xu, Y. Zhu, R. Zhong, Z. Lin, J. Xu, H. Jiang, J. Huang, H. Li, and
    T. Lin, “DeepCropMapping: A multi-temporal deep learning approach with improved
    spatial generalizability for dynamic corn and soybean mapping,” *Remote Sensing
    of Environment*, vol. 247, p. 111946, sep 2020.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (246) J. Xu, Y. Zhu, R. Zhong, Z. Lin, J. Xu, H. Jiang, J. Huang, H. Li 和 T.
    Lin，“DeepCropMapping：一种具有改进空间泛化能力的多时相深度学习方法，用于动态玉米和大豆映射”，*Remote Sensing of Environment*，第247卷，第111946页，2020年9月。
- en: (247) V. Barriere and M. Claverie, “Multimodal crop type classification fusing
    multi-spectral satellite time series with farmers crop rotations and local crop
    distribution,” *arXiv preprint:2208.10838*, 2022.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (247) V. Barriere 和 M. Claverie，“多模态作物类型分类，融合多光谱卫星时间序列与农民作物轮作和本地作物分布”，*arXiv
    preprint:2208.10838*，2022年。
- en: (248) V. S. F. Garnot and L. Landrieu, “Lightweight temporal self-attention
    for classifying satellite images time series,” in *Lecture Notes in Computer Science*.   Springer
    International Publishing, 2020, vol. 12588 LNAI, pp. 171–181.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (248) V. S. F. Garnot 和 L. Landrieu，“用于卫星图像时间序列分类的轻量级时序自注意力”，发表于 *Lecture Notes
    in Computer Science*。施普林格国际出版公司，2020年，第12588卷LNAI，第171–181页。
- en: (249) S. Ofori-Ampofo, C. Pelletier, and S. Lang, “Crop type mapping from optical
    and radar time series using attention-based deep learning,” *Remote Sensing*,
    vol. 13, no. 22, p. 4668, nov 2021.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (249) S. Ofori-Ampofo, C. Pelletier 和 S. Lang，“基于注意力的深度学习从光学和雷达时间序列中进行作物类型映射”，*Remote
    Sensing*，第13卷，第22期，第4668页，2021年11月。
- en: (250) Y. Yuan and L. Lin, “Self-Supervised pretraining of transformers for satellite
    image time series classification,” *IEEE Journal of Selected Topics in Applied
    Earth Observations and Remote Sensing*, vol. 14, pp. 474–487, 2021.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (250) Y. Yuan 和 L. Lin，“用于卫星图像时间序列分类的自监督变换器预训练”，*IEEE Journal of Selected Topics
    in Applied Earth Observations and Remote Sensing*，第14卷，第474–487页，2021年。
- en: (251) N. Di Mauro, A. Vergari, T. M. A. Basile, F. G. Ventola, and F. Esposito,
    “End-to-end learning of deep spatio-temporal representations for satellite image
    time series classification.” in *DC@ PKDD/ECML*, 2017.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (251) N. Di Mauro, A. Vergari, T. M. A. Basile, F. G. Ventola 和 F. Esposito，“端到端学习深度时空表示，用于卫星图像时间序列分类”，发表于
    *DC@ PKDD/ECML*，2017年。
- en: (252) N. Kussul, M. Lavreniuk, S. Skakun, and A. Shelestov, “Deep learning classification
    of land cover and crop types using remote sensing data,” *IEEE Geoscience and
    Remote Sensing Letters*, vol. 14, no. 5, pp. 778–782, may 2017.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (252) N. Kussul, M. Lavreniuk, S. Skakun 和 A. Shelestov，“基于深度学习的土地覆盖和作物类型分类，使用遥感数据”，*IEEE
    Geoscience and Remote Sensing Letters*，第14卷，第5期，第778–782页，2017年5月。
- en: (253) C. Pelletier, G. Webb, and F. Petitjean, “Temporal convolutional neural
    network for the classification of satellite image time series,” *Remote Sensing*,
    vol. 11, no. 5, p. 523, mar 2019.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (253) C. Pelletier, G. Webb 和 F. Petitjean，“用于卫星图像时间序列分类的时序卷积神经网络”，*Remote Sensing*，第11卷，第5期，第523页，2019年3月。
- en: (254) P. Dou, H. Shen, Z. Li, and X. Guan, “Time series remote sensing image
    classification framework using combination of deep learning and multiple classifiers
    system,” *International Journal of Applied Earth Observation and Geoinformation*,
    vol. 103, p. 102477, 2021.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (254) P. Dou, H. Shen, Z. Li, 和 X. Guan， “基于深度学习和多分类器系统结合的时间序列遥感图像分类框架，” *国际应用地球观测与地理信息期刊*，第
    103 卷，第 102477 页，2021 年。
- en: (255) D. Ienco, R. Interdonato, R. Gaetano, and D. Ho Tong Minh, “Combining
    Sentinel-1 and Sentinel-2 satellite image time series for land cover mapping via
    a multi-source deep learning architecture,” *ISPRS J. Photogramm. Remote Sens.*,
    vol. 158, pp. 11–22, 2019.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (255) D. Ienco, R. Interdonato, R. Gaetano, 和 D. Ho Tong Minh， “结合 Sentinel-1
    和 Sentinel-2 卫星图像时间序列，通过多源深度学习架构进行土地覆盖映射，” *ISPRS 摄影测量与遥感期刊*，第 158 卷，第 11–22 页，2019
    年。
- en: '(256) R. Interdonato, D. Ienco, R. Gaetano, and K. Ose, “DuPLO: A DUal view
    Point deep Learning architecture for time series classificatiOn,” *ISPRS J. Photogramm.
    Remote Sens.*, vol. 149, pp. 91–104, mar 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (256) R. Interdonato, D. Ienco, R. Gaetano, 和 K. Ose， “DuPLO：一种用于时间序列分类的双视点深度学习架构，”
    *ISPRS 摄影测量与遥感期刊*，第 149 卷，第 91–104 页，2019 年 3 月。
- en: (257) M. Rußwurm and M. Körner, “Multi-Temporal land cover classification with
    sequential recurrent encoders,” *ISPRS International Journal of Geo-Information*,
    vol. 7, no. 4, p. 129, mar 2018.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (257) M. Rußwurm 和 M. Körner， “利用序列递归编码器进行多时相土地覆盖分类，” *ISPRS 国际地理信息期刊*，第 7 卷，第
    4 期，第 129 页，2018 年 3 月。
- en: '(258) A. Stoian, V. Poulain, J. Inglada, V. Poughon, and D. Derksen, “Land
    cover maps production with high resolution satellite image time series and convolutional
    neural networks: Adaptations and limits for operational systems,” *Remote Sensing*,
    vol. 11, no. 17, pp. 1–26, 2019.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (258) A. Stoian, V. Poulain, J. Inglada, V. Poughon, 和 D. Derksen， “利用高分辨率卫星图像时间序列和卷积神经网络生成土地覆盖图：操作系统的适应性和局限性，”
    *遥感*，第 11 卷，第 17 期，第 1–26 页，2019 年。
- en: (259) D. Ienco, R. Gaetano, C. Dupaquier, and P. Maurel, “Land cover classification
    via multitemporal spatial data by deep recurrent neural networks,” *IEEE Geoscience
    and Remote Sensing Letters*, vol. 14, no. 10, pp. 1685–1689, oct 2017.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (259) D. Ienco, R. Gaetano, C. Dupaquier, 和 P. Maurel， “通过深度递归神经网络利用多时相空间数据进行土地覆盖分类，”
    *IEEE 地球科学与遥感快报*，第 14 卷，第 10 期，第 1685–1689 页，2017 年 10 月。
- en: (260) Y. J. E. Gbodjo, D. Ienco, L. Leroux, R. Interdonato, R. Gaetano, and
    B. Ndao, “Object-based multi-temporal and multi-source land cover mapping leveraging
    hierarchical class relationships,” *Remote Sensing*, vol. 12, no. 17, p. 2814,
    aug 2020.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (260) Y. J. E. Gbodjo, D. Ienco, L. Leroux, R. Interdonato, R. Gaetano, 和 B.
    Ndao， “基于对象的多时相和多源土地覆盖映射，利用层次化类别关系，” *遥感*，第 12 卷，第 17 期，第 2814 页，2020 年 8 月。
- en: (261) D. Ienco, R. Gaetano, R. Interdonato, K. Ose, and D. Ho Tong Minh, “Combining
    Sentinel-1 and Sentinel-2 time series via RNN for object-based land cover classification,”
    in *IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium*.   IEEE,
    jul 2019, pp. 4881–4884.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (261) D. Ienco, R. Gaetano, R. Interdonato, K. Ose, 和 D. Ho Tong Minh， “通过 RNN
    结合 Sentinel-1 和 Sentinel-2 时间序列进行基于对象的土地覆盖分类，” 见于 *IGARSS 2019 - 2019 IEEE 国际地球科学与遥感研讨会*。
    IEEE，2019 年 7 月，第 4881–4884 页。
- en: '(262) Y. Yuan, L. Lin, Q. Liu, R. Hang, and Z.-G. Zhou, “SITS-Former: A pre-trained
    spatio-spectral-temporal representation model for Sentinel-2 time series classification,”
    *International Journal of Applied Earth Observation and Geoinformation*, vol.
    106, p. 102651, feb 2022.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (262) Y. Yuan, L. Lin, Q. Liu, R. Hang, 和 Z.-G. Zhou， “SITS-Former：一种用于 Sentinel-2
    时间序列分类的预训练时空光谱表示模型，” *国际应用地球观测与地理信息期刊*，第 106 卷，第 102651 页，2022 年 2 月。
- en: (263) M. Qiao, X. He, X. Cheng, P. Li, H. Luo, L. Zhang, and Z. Tian, “Crop
    yield prediction from multi-spectral, multi-temporal remotely sensed imagery using
    recurrent 3d convolutional neural networks,” *International Journal of Applied
    Earth Observation and Geoinformation*, vol. 102, p. 102436, 2021.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (263) M. Qiao, X. He, X. Cheng, P. Li, H. Luo, L. Zhang, 和 Z. Tian， “基于递归 3D
    卷积神经网络的多光谱、多时间遥感影像作物产量预测，” *国际应用地球观测与地理信息期刊*，第 102 卷，第 102436 页，2021 年。
- en: (264) M. Rußwurm and M. Körner, “Self-attention for raw optical satellite time
    series classification,” *ISPRS J. Photogramm. Remote Sens.*, vol. 169, pp. 421–435,
    2020.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (264) M. Rußwurm 和 M. Körner， “用于原始光学卫星时间序列分类的自注意力机制，” *ISPRS 摄影测量与遥感期刊*，第 169
    卷，第 421–435 页，2020 年。
- en: '(265) D. Tuia, C. Persello, and L. Bruzzone, “Domain adaptation for the classification
    of remote sensing data: An overview of recent advances,” *IEEE Geoscience and
    Remote Sensing Magazine*, vol. 4, no. 2, pp. 41–57, 2016.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (265) D. Tuia, C. Persello, 和 L. Bruzzone， “遥感数据分类的领域适应：近期进展概述，” *IEEE 地球科学与遥感杂志*，第
    4 卷，第 2 期，第 41–57 页，2016 年。
- en: (266) V. S. F. Garnot, L. Landrieu, S. Giordano, and N. Chehata, “Time-space
    tradeoff in deep learning models for crop classification on satellite multi-spectral
    image time series,” in *IGARSS 2019-2019 IEEE International Geoscience and Remote
    Sensing Symposium*.   IEEE, 2019, pp. 6247–6250.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (266) V. S. F. Garnot, L. Landrieu, S. Giordano, 和 N. Chehata，“用于作物分类的深度学习模型中的时间-空间权衡，基于卫星多光谱图像时间序列，”
    在 *IGARSS 2019-2019 IEEE国际地球科学与遥感研讨会*。 IEEE, 2019年，pp. 6247–6250。
- en: (267) H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller,
    “Deep neural network ensembles for time series classification,” in *2019 International
    Joint Conference on Neural Networks (IJCNN)*, vol. 2019-July.   IEEE, jul 2019,
    pp. 1–6.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (267) H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, 和 P.-A. Muller，“时间序列分类的深度神经网络集成，”
    在 *2019国际神经网络联合会议（IJCNN）*，vol. 2019年7月。 IEEE, 2019年7月，pp. 1–6。
- en: (268) D. H. Wolpert, “Stacked generalization,” *Neural Networks*, vol. 5, no. 2,
    pp. 241–259, jan 1992.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (268) D. H. Wolpert，“堆叠泛化，” *Neural Networks*, vol. 5, no. 2, pp. 241–259, 1992年1月。
- en: (269) Y. Freund and R. E. Schapire, “Experiments with a new boosting algorithm,”
    in *13th Int. conf. mach. learn.*, 1996, pp. 148–156.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (269) Y. Freund 和 R. E. Schapire，“一种新的提升算法实验，” 在 *第13届国际机器学习会议*，1996年，pp. 148–156。
- en: '(270) C. Gómez, J. C. White, and M. A. Wulder, “Optical remotely sensed time
    series data for land cover classification: A review,” *ISPRS J. Photogramm. Remote
    Sens.*, vol. 116, pp. 55–72, 2016.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (270) C. Gómez, J. C. White, 和 M. A. Wulder，“用于土地覆盖分类的光学遥感时间序列数据：综述，” *ISPRS
    J. Photogramm. Remote Sens.*, vol. 116, pp. 55–72, 2016年。
- en: '(271) X. X. Zhu, D. Tuia, L. Mou, G. S. Xia, L. Zhang, F. Xu, and F. Fraundorfer,
    “Deep learning in remote sensing: A comprehensive review and list of resources,”
    *IEEE Geoscience and Remote Sensing Magazine*, vol. 5, no. 4, pp. 8–36, 2017.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (271) X. X. Zhu, D. Tuia, L. Mou, G. S. Xia, L. Zhang, F. Xu, 和 F. Fraundorfer，“遥感中的深度学习：综合评述与资源列表，”
    *IEEE Geoscience and Remote Sensing Magazine*, vol. 5, no. 4, pp. 8–36, 2017年。
- en: '(272) L. Ma, Y. Liu, X. Zhang, Y. Ye, G. Yin, and B. A. Johnson, “Deep learning
    in remote sensing applications: A meta-analysis and review,” *ISPRS J. Photogramm.
    Remote Sens.*, vol. 152, pp. 166–177, jun 2019.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (272) L. Ma, Y. Liu, X. Zhang, Y. Ye, G. Yin, 和 B. A. Johnson，“深度学习在遥感应用中的应用：元分析与综述，”
    *ISPRS J. Photogramm. Remote Sens.*, vol. 152, pp. 166–177, 2019年6月。
- en: '(273) Q. Yuan, H. Shen, T. Li, Z. Li, S. Li, Y. Jiang, H. Xu, W. Tan, Q. Yang,
    J. Wang, J. Gao, and L. Zhang, “Deep learning in environmental remote sensing:
    Achievements and challenges,” *Remote Sensing of Environment*, vol. 241, p. 111716,
    may 2020.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (273) Q. Yuan, H. Shen, T. Li, Z. Li, S. Li, Y. Jiang, H. Xu, W. Tan, Q. Yang,
    J. Wang, J. Gao, 和 L. Zhang，“环境遥感中的深度学习：成就与挑战，” *Remote Sensing of Environment*,
    vol. 241, p. 111716, 2020年5月。
- en: '(274) M. E. D. Chaves, M. C. A. Picoli, and I. D. Sanches, “Recent applications
    of Landsat 8/OLI and Sentinel-2/MSI for land use and land cover mapping: A systematic
    review,” *Remote Sensing*, vol. 12, no. 18, p. 3062, sep 2020.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (274) M. E. D. Chaves, M. C. A. Picoli, 和 I. D. Sanches，“Landsat 8/OLI 和 Sentinel-2/MSI
    在土地利用和土地覆盖制图中的近期应用：系统评述，” *Remote Sensing*, vol. 12, no. 18, p. 3062, 2020年9月。
- en: '(275) W. R. Moskolaï, W. Abdou, A. Dipanda, and Kolyang, “Application of deep
    learning architectures for satellite image time series prediction: A review,”
    *Remote Sensing*, vol. 13, no. 23, p. 4822, nov 2021.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (275) W. R. Moskolaï, W. Abdou, A. Dipanda, 和 Kolyang，“深度学习架构在卫星图像时间序列预测中的应用：综述，”
    *Remote Sensing*, vol. 13, no. 23, p. 4822, 2021年11月。
- en: (276) J. Lines and A. Bagnall, “Time series classification with ensembles of
    elastic distance measures,” *Data Min. Knowl. Discov.*, vol. 29, no. 3, pp. 565–592,
    2015.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (276) J. Lines 和 A. Bagnall，“使用弹性距离度量的时间序列分类集成，” *Data Min. Knowl. Discov.*,
    vol. 29, no. 3, pp. 565–592, 2015年。
- en: '(277) C. W. Tan, F. Petitjean, and G. I. Webb, “FastEE: Fast Ensembles of Elastic
    Distances for time series classification,” *Data Min. Knowl. Discov.*, vol. 34,
    no. 1, pp. 231–272, 2020.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(277) C. W. Tan, F. Petitjean, 和 G. I. Webb，“FastEE: 用于时间序列分类的快速弹性距离集成，” *Data
    Min. Knowl. Discov.*, vol. 34, no. 1, pp. 231–272, 2020年。'
- en: '(278) M. Herrmann and G. I. Webb, “Amercing: An intuitive, elegant and effective
    constraint for dynamic time warping,” *arXiv preprint:2111.13314*, 2021.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(278) M. Herrmann 和 G. I. Webb，“Amercing: 一种直观、优雅且有效的动态时间扭曲约束，” *arXiv preprint:2111.13314*,
    2021年。'
- en: (279) A. Bagnall, M. Flynn, J. Large, J. Lines, and M. Middlehurst, “On the
    usage and performance of the Hierarchical Vote Collective of Transformation-based
    Ensembles version 1.0 (hive-cote v1\. 0),” in *International Workshop on Advanced
    Analytics and Learning on Temporal Data*, 2020, pp. 3–18.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (279) A. Bagnall, M. Flynn, J. Large, J. Lines, 和 M. Middlehurst，“关于基于变换的集成的层次投票集合版本1.0（hive-cote
    v1.0）的使用与性能，” 在 *国际高级分析与时间数据学习研讨会*，2020年，pp. 3–18。
- en: '(280) M. Middlehurst, J. Large, M. Flynn, J. Lines, A. Bostrom, and A. Bagnall,
    “HIVE-COTE 2.0: a new meta ensemble for time series classification,” *Machine
    Learning*, vol. 110, no. 11, pp. 3211–3243, 2021.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (280) M. Middlehurst, J. Large, M. Flynn, J. Lines, A. Bostrom, 和 A. Bagnall，“HIVE-COTE
    2.0：一种新的时间序列分类元集成，” *机器学习*，第 110 卷，第 11 期，第 3211–3243 页，2021 年。
- en: '(281) A. Bagnall, J. Lines, J. Hills, and A. Bostrom, “Time-series classification
    with COTE: the collective of transformation-based ensembles,” *IEEE Transactions
    on Knowledge and Data Engineering*, vol. 27, no. 9, pp. 2522–2535, 2015.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (281) A. Bagnall, J. Lines, J. Hills, 和 A. Bostrom，“使用 COTE 进行时间序列分类：基于转换的集成的集合，”
    *IEEE 知识与数据工程汇刊*，第 27 卷，第 9 期，第 2522–2535 页，2015 年。
- en: '(282) J. Lines, S. Taylor, and A. Bagnall, “Time series classification with
    HIVE-COTE: The hierarchical vote collective of transformation-based ensembles,”
    *ACM Transactions on Knowledge Discovery from Data*, vol. 12, no. 5, 2018.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (282) J. Lines, S. Taylor, 和 A. Bagnall，“使用 HIVE-COTE 进行时间序列分类：基于转换的集成的层次投票集合，”
    *ACM 知识发现与数据期刊*，第 12 卷，第 5 期，2018 年。
- en: '(283) ——, “Hive-Cote: The hierarchical vote collective of transformation-based
    ensembles for time series classification,” in *2016 IEEE 16th international conference
    on data mining (ICDM)*.   IEEE, 2016, pp. 1041–1046.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (283) ——，“Hive-Cote：用于时间序列分类的基于转换的集成的层次投票集合，” 载于 *2016 IEEE 第 16 届国际数据挖掘会议 (ICDM)*，IEEE，2016
    年，第 1041–1046 页。
- en: (284) R. J. Kate, “Using dynamic time warping distances as features for improved
    time series classification,” *Data Min. Knowl. Discov.*, vol. 30, no. 2, pp. 283–312,
    2016.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (284) R. J. Kate，“使用动态时间规整距离作为特征以改进时间序列分类，” *数据挖掘与知识发现*，第 30 卷，第 2 期，第 283–312
    页，2016 年。
- en: (285) A. Bostrom and A. Bagnall, “Binary shapelet transform for multiclass time
    series classification,” in *Int. conf. big data analytics .knowl. disco.*   Springer,
    2015, pp. 257–269.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (285) A. Bostrom 和 A. Bagnall，“用于多类时间序列分类的二进制形状变换，” 载于 *国际大数据分析与知识发现会议*，Springer，2015
    年，第 257–269 页。
- en: (286) P. Schäfer, “The boss is concerned with time series classification in
    the presence of noise,” *Data Min. Knowl. Discov.*, vol. 29, no. 6, pp. 1505–1530,
    2015.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (286) P. Schäfer，“在存在噪声的情况下，老板关心时间序列分类，” *数据挖掘与知识发现*，第 29 卷，第 6 期，第 1505–1530
    页，2015 年。
- en: (287) J. Hills, J. Lines, E. Baranauskas, J. Mapp, and A. Bagnall, “Classification
    of time series by shapelet transformation,” *Data Min. Knowl. Discov.*, vol. 28,
    no. 4, pp. 851–881, 2014.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (287) J. Hills, J. Lines, E. Baranauskas, J. Mapp, 和 A. Bagnall，“通过形状变换对时间序列进行分类，”
    *数据挖掘与知识发现*，第 28 卷，第 4 期，第 851–881 页，2014 年。
- en: (288) H. Deng, G. Runger, E. Tuv, and M. Vladimir, “A time series forest for
    classification and feature extraction,” *Inf. Sci.*, vol. 239, pp. 142–153, 2013.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (288) H. Deng, G. Runger, E. Tuv, 和 M. Vladimir，“用于分类和特征提取的时间序列森林，” *信息科学*，第
    239 卷，第 142–153 页，2013 年。
- en: (289) M. G. Baydogan, G. Runger, and E. Tuv, “A bag-of-features framework to
    classify time series,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 35, no. 11, pp. 2796–2802, 2013.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (289) M. G. Baydogan, G. Runger, 和 E. Tuv，“一种用于分类时间序列的特征袋框架，” *IEEE 模式分析与机器智能汇刊*，第
    35 卷，第 11 期，第 2796–2802 页，2013 年。
- en: '(290) A. Dempster, D. F. Schmidt, and G. I. Webb, “Minirocket: A very fast
    (almost) deterministic transform for time series classification,” in *27th ACM
    SIGKDD Conference on Knowledge Discovery & Data Mining*, 2021, pp. 248–257.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (290) A. Dempster, D. F. Schmidt, 和 G. I. Webb，“Minirocket：一种非常快速（几乎）确定性的时间序列分类变换，”
    载于 *第 27 届 ACM SIGKDD 知识发现与数据挖掘大会*，2021 年，第 248–257 页。
- en: '(291) C. W. Tan, A. Dempster, C. Bergmeir, and G. I. Webb, “MultiRocket: multiple
    pooling operators and transformations for fast and effective time series classification,”
    *Data Min. Knowl. Discov.*, jun 2022.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (291) C. W. Tan, A. Dempster, C. Bergmeir, 和 G. I. Webb，“MultiRocket：用于快速有效的时间序列分类的多重池化操作符和变换，”
    *数据挖掘与知识发现*，2022 年 6 月。
- en: '(292) A. Dempster, D. F. Schmidt, and G. I. Webb, “Hydra: Competing convolutional
    kernels for fast and accurate time series classification,” *Data Mining and Knowledge
    Discovery*, pp. 1–27, 2023.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (292) A. Dempster, D. F. Schmidt, 和 G. I. Webb，“Hydra：用于快速和准确的时间序列分类的竞争卷积核，”
    *数据挖掘与知识发现*，第 1–27 页，2023 年。
- en: '(293) B. Lucas, A. Shifaz, C. Pelletier, L. O’Neill, N. Zaidi, B. Goethals,
    F. Petitjean, and G. I. Webb, “Proximity forest: an effective and scalable distance-based
    classifier for time series,” *Data Mining and Knowledge Discovery*, vol. 33, no. 3,
    pp. 607–635, 2019.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (293) B. Lucas, A. Shifaz, C. Pelletier, L. O’Neill, N. Zaidi, B. Goethals,
    F. Petitjean, 和 G. I. Webb，“邻近森林：一种有效且可扩展的基于距离的时间序列分类器，” *数据挖掘与知识发现*，第 33 卷，第
    3 期，第 607–635 页，2019 年。
- en: '(294) M. Herrmann, C. W. Tan, M. Salehi, and G. I. Webb, “Proximity forest
    2.0: A new effective and scalable similarity-based classifier for time series,”
    *arXiv preprint arXiv:2304.05800*, 2023.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (294) M. Herrmann、C. W. Tan、M. Salehi 和 G. I. Webb， “Proximity Forest 2.0：一种新型有效且可扩展的基于相似性的时间序列分类器，”
    *arXiv 预印本 arXiv:2304.05800*，2023。
- en: '(295) K. Fukushima and S. Miyake, “Neocognitron: A self-organizing neural network
    model for a mechanism of visual pattern recognition,” in *Competition and cooperation
    in neural nets*.   Springer, 1982, pp. 267–285.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (295) K. Fukushima 和 S. Miyake， “Neocognitron：一种用于视觉模式识别机制的自组织神经网络模型，” 在 *神经网络中的竞争与合作*。
     Springer，1982，第 267–285 页。
- en: (296) D. H. Hubel and T. N. Wiesel, “Receptive fields, binocular interaction
    and functional architecture in the cat’s visual cortex,” *The Journal of physiology*,
    vol. 160, no. 1, p. 106, 1962.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (296) D. H. Hubel 和 T. N. Wiesel， “猫视觉皮层中的感受野、双眼交互和功能结构，” *生理学杂志*，第 160 卷，第
    1 期，第 106 页，1962。
- en: (297) Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proc. IEEE*, vol. 86, no. 11, pp. 2278–2324,
    1998.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (297) Y. LeCun、L. Bottou、Y. Bengio 和 P. Haffner， “应用于文档识别的基于梯度的学习，” *IEEE 会议录*，第
    86 卷，第 11 期，第 2278–2324 页，1998。
- en: (298) V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann
    machines,” in *Icml*, 2010.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (298) V. Nair 和 G. E. Hinton， “修正线性单元提高了限制玻尔兹曼机的性能，” 在 *Icml*，2010。
- en: (299) S. Hihi and Y. Bengio, “Hierarchical recurrent neural networks for long-term
    dependencies,” *Advances neural inf. process. syst.*, vol. 8, 1995.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (299) S. Hihi 和 Y. Bengio， “用于长期依赖的层次递归神经网络，” *神经信息处理系统进展*，第 8 卷，1995。
- en: (300) R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio, “How to construct deep
    recurrent neural networks,” *arXiv preprint:1312.6026*, 2013.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (300) R. Pascanu、C. Gulcehre、K. Cho 和 Y. Bengio， “如何构建深度递归神经网络，” *arXiv 预印本:1312.6026*，2013。
- en: (301) K. Kawakami, “Supervised sequence labelling with recurrent neural networks,”
    Ph.D. dissertation, Technical University of Munich, 2008.
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (301) K. Kawakami， “使用递归神经网络进行监督序列标注，” 博士学位论文，慕尼黑工业大学，2008。
- en: (302) D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” *arXiv preprint:1409.0473*, 2014.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (302) D. Bahdanau、K. Cho 和 Y. Bengio， “通过联合学习对齐和翻译的神经机器翻译，” *arXiv 预印本:1409.0473*，2014。
- en: (303) K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” *arXiv preprint:1406.1078*, 2014.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (303) K. Cho、B. Van Merriënboer、C. Gulcehre、D. Bahdanau、F. Bougares、H. Schwenk
    和 Y. Bengio， “使用 RNN 编码器-解码器学习短语表示以进行统计机器翻译，” *arXiv 预印本:1406.1078*，2014。
- en: (304) M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based
    neural machine translation,” *arXiv preprint:1508.04025*, 2015.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (304) M.-T. Luong、H. Pham 和 C. D. Manning， “基于注意力的神经机器翻译的有效方法，” *arXiv 预印本:1508.04025*，2015。
- en: '(305) J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral Networks and
    Locally Connected Networks on Graphs,” pp. 1–14, 2013\. [Online]. Available: [http://arxiv.org/abs/1312.6203](http://arxiv.org/abs/1312.6203)'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(305) J. Bruna、W. Zaremba、A. Szlam 和 Y. LeCun， “图上的谱网络和局部连接网络，” 第 1–14 页，2013。
    [在线]. 可用: [http://arxiv.org/abs/1312.6203](http://arxiv.org/abs/1312.6203)'
- en: '(306) D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst,
    “The emerging field of signal processing on graphs: Extending high-dimensional
    data analysis to networks and other irregular domains,” *IEEE Signal Processing
    Magazine*, vol. 30, no. 3, pp. 83–98, 2013\. [Online]. Available: [http://ieeexplore.ieee.org/document/6494675/](http://ieeexplore.ieee.org/document/6494675/)'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(306) D. I. Shuman、S. K. Narang、P. Frossard、A. Ortega 和 P. Vandergheynst， “图上的信号处理新兴领域：将高维数据分析扩展到网络和其他不规则领域，”
    *IEEE 信号处理杂志*，第 30 卷，第 3 期，第 83–98 页，2013。 [在线]. 可用: [http://ieeexplore.ieee.org/document/6494675/](http://ieeexplore.ieee.org/document/6494675/)'
- en: '(307) A. Longa, V. Lachi, G. Santin, M. Bianchini, B. Lepri, P. Lio, F. Scarselli,
    and A. Passerini, “Graph Neural Networks for temporal graphs: State of the art,
    open challenges, and opportunities,” 2023\. [Online]. Available: [http://arxiv.org/abs/2302.01018](http://arxiv.org/abs/2302.01018)'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(307) A. Longa、V. Lachi、G. Santin、M. Bianchini、B. Lepri、P. Lio、F. Scarselli
    和 A. Passerini， “用于时间图的图神经网络：现状、开放挑战和机会，” 2023。 [在线]. 可用: [http://arxiv.org/abs/2302.01018](http://arxiv.org/abs/2302.01018)'
- en: (308) M. Bachlin, D. Roggen, G. Troster, M. Plotnik, N. Inbar, I. Meidan, T. Herman,
    M. Brozgol, E. Shaviv, N. Giladi, and J. M. Hausdorff, “Potentials of enhanced
    context awareness in wearable assistants for Parkinson’s Disease patients with
    the freezing of gait syndrome,” in *2009 International Symposium on Wearable Computers*.   IEEE,
    sep 2009, pp. 123–130.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (308) M. Bachlin, D. Roggen, G. Troster, M. Plotnik, N. Inbar, I. Meidan, T.
    Herman, M. Brozgol, E. Shaviv, N. Giladi, 和 J. M. Hausdorff，“针对帕金森病患者步态冻结综合征的可穿戴助手增强上下文感知的潜力，”在
    *2009 国际可穿戴计算机研讨会* 中。 IEEE，2009年9月，第123–130页。
- en: '(309) D. Micucci, M. Mobilio, and P. Napoletano, “UniMiB SHAR: A dataset for
    human activity recognition using acceleration data from smartphones,” *Applied
    Sciences*, vol. 7, no. 10, p. 1101, oct 2017.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (309) D. Micucci, M. Mobilio, 和 P. Napoletano，“UniMiB SHAR：一个用于基于智能手机加速度数据的人类活动识别数据集，”
    *应用科学*，第7卷，第10期，第1101页，2017年10月。
- en: '(310) P. Zappi, C. Lombriser, T. Stiefmeier, E. Farella, D. Roggen, L. Benini,
    and G. Tröster, “Activity recognition from on-body sensors: accuracy-power trade-off
    by dynamic sensor selection,” in *European Conference on Wireless Sensor Networks*.   Springer,
    2008, pp. 17–33.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (310) P. Zappi, C. Lombriser, T. Stiefmeier, E. Farella, D. Roggen, L. Benini,
    和 G. Tröster，“基于身体传感器的活动识别：通过动态传感器选择实现准确性与功耗的权衡，”在 *欧洲无线传感器网络会议* 中。 Springer，2008年，第17–33页。
- en: '(311) R. Chavarriaga, H. Sagha, A. Calatroni, S. T. Digumarti, G. Tröster,
    J. D. R. Millán, and D. Roggen, “The Opportunity challenge: A benchmark database
    for on-body sensor-based activity recognition,” *Pattern Recognition Letters*,
    vol. 34, no. 15, pp. 2033–2042, nov 2013.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (311) R. Chavarriaga, H. Sagha, A. Calatroni, S. T. Digumarti, G. Tröster, J.
    D. R. Millán, 和 D. Roggen，“机会挑战：一个用于基于身体传感器活动识别的基准数据库，” *模式识别通讯*，第34卷，第15期，第2033–2042页，2013年11月。
- en: '(312) A. Reiss and D. Stricker, “Creating and benchmarking a new dataset for
    physical activity monitoring,” in *5th Int. Conf. PErvasive Technologies Related
    to Assistive Environments - PETRA ’12*.   New York, New York, USA: ACM Press,
    2012, p. 1.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (312) A. Reiss 和 D. Stricker，“创建和基准测试一个新的身体活动监测数据集，”在 *第五届与辅助环境相关的普适技术国际会议 -
    PETRA ’12* 中。 纽约，美国：ACM出版社，2012年，第1页。
- en: (313) D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A public
    domain dataset for human activity recognition using smartphones,” in *21th European
    Symposium on Artificial Neural Networks, Computational Intelligence and Machine
    Learning, ESANN*, Bruges, Belgium, 2013, pp. 437–442.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (313) D. Anguita, A. Ghio, L. Oneto, X. Parra, 和 J. L. Reyes-Ortiz，“用于人类活动识别的公共领域数据集，”在
    *第21届欧洲人工神经网络、计算智能与机器学习研讨会（ESANN）* 中，比利时布鲁日，2013年，第437–442页。
- en: (314) J. R. Kwapisz, G. M. Weiss, and S. A. Moore, “Activity recognition using
    cell phone accelerometers,” *ACM SIGKDD Explorations Newsletter*, vol. 12, no. 2,
    pp. 74–82, mar 2011.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (314) J. R. Kwapisz, G. M. Weiss, 和 S. A. Moore，“使用手机加速度计进行活动识别，” *ACM SIGKDD
    探索通讯*，第12卷，第2期，第74–82页，2011年3月。
- en: '(315) U.S. Geological Survey, “Landsat Satellite Missions.” [Online]. Available:
    [https://www.usgs.gov/landsat-missions/landsat-satellite-missions](https://www.usgs.gov/landsat-missions/landsat-satellite-missions)'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (315) 美国地质调查局，“Landsat 卫星任务。” [在线]. 可用： [https://www.usgs.gov/landsat-missions/landsat-satellite-missions](https://www.usgs.gov/landsat-missions/landsat-satellite-missions)
- en: '(316) NASA, “MODIS Moderate Resolution Imaging Spectrometer.” [Online]. Available:
    [https://modis.gsfc.nasa.gov/](https://modis.gsfc.nasa.gov/)'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (316) NASA，“MODIS 中分辨率成像光谱仪。” [在线]. 可用： [https://modis.gsfc.nasa.gov/](https://modis.gsfc.nasa.gov/)
- en: '(317) European Space Agency, “Sentinel Online,” 2019\. [Online]. Available:
    [https://sentinel.esa.int/web/sentinel/home](https://sentinel.esa.int/web/sentinel/home)'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (317) 欧洲航天局，“哨兵在线，” 2019\. [在线]. 可用： [https://sentinel.esa.int/web/sentinel/home](https://sentinel.esa.int/web/sentinel/home)
- en: '(318) ——, “Pleiades - Earth Online.” [Online]. Available: [https://earth.esa.int/eogateway/missions/pleiades](https://earth.esa.int/eogateway/missions/pleiades)'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (318) ——，“昴星团 - 地球在线。” [在线]. 可用： [https://earth.esa.int/eogateway/missions/pleiades](https://earth.esa.int/eogateway/missions/pleiades)
- en: '(319) National Space Organization, “FORMOSAT-2,” 2020\. [Online]. Available:
    [https://www.nspo.narl.org.tw/history{_}prog.php?c=20030402{&}ln=en](https://www.nspo.narl.org.tw/history%7B_%7Dprog.php?c=20030402%7B&%7Dln=en)'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (319) 国家太空组织，“福尔摩沙二号，” 2020\. [在线]. 可用： [https://www.nspo.narl.org.tw/history{_}prog.php?c=20030402{&}ln=en](https://www.nspo.narl.org.tw/history%7B_%7Dprog.php?c=20030402%7B&%7Dln=en)
- en: '(320) EoPortal, “Gaofen-1,” 2014\. [Online]. Available: [https://www.eoportal.org/satellite-missions/gaofen-1](https://www.eoportal.org/satellite-missions/gaofen-1)'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (320) EoPortal，“高分一号，” 2014\. [在线]. 可用： [https://www.eoportal.org/satellite-missions/gaofen-1](https://www.eoportal.org/satellite-missions/gaofen-1)
- en: '(321) ——, “Gaofen-2,” 2015\. [Online]. Available: [https://www.eoportal.org/satellite-missions/gaofen-2](https://www.eoportal.org/satellite-missions/gaofen-2)'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (321) ——，“高分二号，”2015年。[在线]. 可用：[https://www.eoportal.org/satellite-missions/gaofen-2](https://www.eoportal.org/satellite-missions/gaofen-2)
- en: Appendix
  id: totrans-671
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Non-Deep Learning Time Series Classification
  id: totrans-672
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 非深度学习时间序列分类
- en: In this section, we aim to give a brief introduction to the field of TSC and
    discuss its current status. We refer interested readers to the ‘bake-off’ papers
    ([bagnall2017great,](#bib.bib25) ; [middlehurst2023bake,](#bib.bib11) ; [ruiz2020great,](#bib.bib26)
    ) that describes TSC methods in much details and benchmark them.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们旨在简要介绍TSC领域并讨论其当前状态。我们建议感兴趣的读者参考“对比”论文（[bagnall2017great](#bib.bib25)；[middlehurst2023bake](#bib.bib11)；[ruiz2020great](#bib.bib26)），这些论文详细描述了TSC方法并对其进行了基准测试。
- en: Research in TSC started with distance-based approaches that find discriminating
    patterns in the shape of the time series. Distance-based approaches usually consist
    of coupling a 1-nearest neighbour (1NN) classifier with a time series distance
    measure ([lines2015time,](#bib.bib276) ; [tan2020fastee,](#bib.bib277) ). Small
    distortions in the time series can lead to false matches when measuring the distance
    between time series using standard distance measurements such as Euclidean distance ([lines2015time,](#bib.bib276)
    ). A time series distance measure aims to compensate for these distortions by
    aligning two time series such that the alignment cost between the two are minimised.
    There are many time series distances proposed in the literature; among these,
    the Dynamic Time Warping ($DTW$) distance is one of the most popular choices for
    many time series tasks, due to its intuitiveness and effectiveness in aligning
    two time series. The 1NN-$DTW$ has been the go-to method for TSC for decades.
    However, by comparing several time series distance measures, the work in ([lines2015time,](#bib.bib276)
    ) showed that as of 2015, there was no single distance that significantly outperformed
    $DTW$ when used with a 1NN classifier. The recent Amerced $DTW$ ([herrmann2021amercing,](#bib.bib278)
    ) distance is the first distance that is significantly more accurate than $DTW$.
    These individual 1NN classifiers with different distances can be ensembled together
    to create an ensemble, such as the Ensemble of Elastic distances (EE), that significantly
    outperforms each of them individually ([tan2020fastee,](#bib.bib277) ; [lines2015time,](#bib.bib276)
    ). However, since most distances have a complexity of $O(L^{2})$ where $L$ is
    the length of the series, performing a nearest neighbour search becomes very costly.
    Hence, distance-based approaches are considered to be one of the slowest methods
    for TSC ([bagnall2020usage,](#bib.bib279) ; [middlehurst2021hive,](#bib.bib280)
    ).
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: TSC研究始于基于距离的方法，这些方法通过寻找时间序列的形状中的区分模式。基于距离的方法通常包括将1-最近邻（1NN）分类器与时间序列距离度量结合使用（[lines2015time](#bib.bib276)；[tan2020fastee](#bib.bib277)）。在使用标准距离度量（如欧几里得距离）测量时间序列之间的距离时，时间序列中的小扭曲可能导致错误匹配（[lines2015time](#bib.bib276)）。时间序列距离度量旨在通过对齐两个时间序列来补偿这些扭曲，从而最小化两个序列之间的对齐成本。文献中提出了许多时间序列距离；其中，动态时间规整（$DTW$）距离是许多时间序列任务中最受欢迎的选择之一，因为它在对齐两个时间序列时具有直观性和有效性。1NN-$DTW$已成为TSC的首选方法已有几十年。然而，通过比较几种时间序列距离度量，工作（[lines2015time](#bib.bib276)）表明，截至2015年，没有一种距离在与1NN分类器结合使用时显著优于$DTW$。最近的Amerced
    $DTW$（[herrmann2021amercing](#bib.bib278)）距离是第一个显著比$DTW$更准确的距离。这些具有不同距离的单独1NN分类器可以组合成一个集成体，如弹性距离集成（EE），显著优于每个单独的分类器（[tan2020fastee](#bib.bib277)；[lines2015time](#bib.bib276)）。然而，由于大多数距离的复杂度为$O(L^{2})$，其中$L$是序列的长度，进行最近邻搜索变得非常昂贵。因此，基于距离的方法被认为是TSC最慢的方法之一（[bagnall2020usage](#bib.bib279)；[middlehurst2021hive](#bib.bib280)）。
- en: As a result of EE, recent studies have focused mainly on developing ensembling
    methods that significantly outperform 1NN-$DTW$ ([tan2020fastee,](#bib.bib277)
    ; [bagnall2020usage,](#bib.bib279) ; [middlehurst2021hive,](#bib.bib280) ; [bagnall2015time,](#bib.bib281)
    ; [lines2018time,](#bib.bib282) ; [lines2016hive,](#bib.bib283) ; [kate2016using,](#bib.bib284)
    ; [bostrom2015binary,](#bib.bib285) ; [schafer2015boss,](#bib.bib286) ; [hills2014classification,](#bib.bib287)
    ; [deng2013time,](#bib.bib288) ; [baydogan2013bag,](#bib.bib289) ). These approaches
    use either an ensemble of tree-based approaches ([baydogan2013bag,](#bib.bib289)
    ; [deng2013time,](#bib.bib288) ) or an ensemble of different types of discriminant
    classifiers, such as NN with several distances and Support Vector Machine (SVM)
    on one or several feature spaces ([bagnall2015time,](#bib.bib281) ; [bostrom2015binary,](#bib.bib285)
    ; [schafer2015boss,](#bib.bib286) ; [kate2016using,](#bib.bib284) ). All these
    approaches share a common property – the data transformation phase where the time
    series is transformed into a new feature space such as the shapelets transform ([bostrom2015binary,](#bib.bib285)
    ) or DTW features ([kate2016using,](#bib.bib284) ). Taking advantage of this notion
    led to the development of the Hierarchical Vote Collective of Transformation-based
    Ensembles (HIVE-COTE) ([lines2016hive,](#bib.bib283) ; [middlehurst2021hive,](#bib.bib280)
    ). HIVE-COTE is a meta ensemble for TSC and forms its ensemble from ensemble classifiers
    of multiple domains. Since its introduction in 2016 ([lines2016hive,](#bib.bib283)
    ), HIVE-COTE has gone through a few iterations. Recently, the latest HIVE-COTE
    version, HIVE-COTEv2.0 (HC2) was proposed ([middlehurst2021hive,](#bib.bib280)
    ). It is comprised of 4 ensemble members, each of them being the then state of
    the art in their respective domains. It is currently one of the most accurate
    classifiers for both univariate and multivariate TSC tasks ([middlehurst2021hive,](#bib.bib280)
    ). Despite being accurate on 26 multivariate and 142 univariate TSC benchmark
    datasets, that are relatively small, HC2 scales poorly on large datasets with
    long time series as well as datasets with large numbers of channels.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 由于EE的结果，近期的研究主要集中在开发显著超越1NN-$DTW$的集成方法上（[tan2020fastee,](#bib.bib277) ; [bagnall2020usage,](#bib.bib279)
    ; [middlehurst2021hive,](#bib.bib280) ; [bagnall2015time,](#bib.bib281) ; [lines2018time,](#bib.bib282)
    ; [lines2016hive,](#bib.bib283) ; [kate2016using,](#bib.bib284) ; [bostrom2015binary,](#bib.bib285)
    ; [schafer2015boss,](#bib.bib286) ; [hills2014classification,](#bib.bib287) ;
    [deng2013time,](#bib.bib288) ; [baydogan2013bag,](#bib.bib289)）。这些方法要么使用基于树的方法的集成（[baydogan2013bag,](#bib.bib289)
    ; [deng2013time,](#bib.bib288)），要么使用不同类型的判别分类器的集成，例如具有多种距离的NN和在一个或多个特征空间上的支持向量机（SVM）（[bagnall2015time,](#bib.bib281)
    ; [bostrom2015binary,](#bib.bib285) ; [schafer2015boss,](#bib.bib286) ; [kate2016using,](#bib.bib284)）。所有这些方法都有一个共同的特点——数据变换阶段，在这个阶段，时间序列被转换到一个新的特征空间，如形状变换（[bostrom2015binary,](#bib.bib285)）或DTW特征（[kate2016using,](#bib.bib284)）。利用这一概念发展出了基于变换的集成的层次投票集体（HIVE-COTE）（[lines2016hive,](#bib.bib283)
    ; [middlehurst2021hive,](#bib.bib280)）。HIVE-COTE是一个用于时间序列分类（TSC）的元集成，它由多个领域的集成分类器组成。自2016年推出以来（[lines2016hive,](#bib.bib283)），HIVE-COTE经历了几次迭代。最近，提出了最新版本的HIVE-COTE，HIVE-COTEv2.0（HC2）（[middlehurst2021hive,](#bib.bib280)）。它由4个集成成员组成，每个成员在其各自领域中都代表了当时的最先进技术。目前，它是单变量和多变量TSC任务中最准确的分类器之一（[middlehurst2021hive,](#bib.bib280)）。尽管在26个多变量和142个单变量TSC基准数据集上表现准确，这些数据集相对较小，但HC2在具有长时间序列的大型数据集以及具有大量通道的数据集上扩展性较差。
- en: Various work has been done on speeding up TSC methods without sacrificing accuracy
    ([dempster2019rocket,](#bib.bib14) ; [dempster2021minirocket,](#bib.bib290) ;
    [tan2020fastee,](#bib.bib277) ; [tan2021multirocket,](#bib.bib291) ; [dempster2023hydra,](#bib.bib292)
    ; [lucas2019proximity,](#bib.bib293) ; [herrmann2023proximity,](#bib.bib294) ).
    A recent breakthrough is the development of Rocket ([dempster2019rocket,](#bib.bib14)
    ) that was able to process 109 univariate time series datasets under 4 hours while
    the previous fastest took days. Rocket leverages large number of random convolutional
    filters to extract features from each series that might be relevant to classifying
    a series. These features are then passed to a linear model for classification.
    Rocket has been improved to be faster (Minirocket ([dempster2021minirocket,](#bib.bib290)
    )) and more accurate (Multirocket ([tan2021multirocket,](#bib.bib291) ) and Hydra
    ([dempster2023hydra,](#bib.bib292) )). Hydra when combined with Multirocket is
    now one of the fastest and most accurate method for TSC.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 已经做了各种工作来加速 TSC 方法而不牺牲准确性（[dempster2019rocket](#bib.bib14) ; [dempster2021minirocket](#bib.bib290)
    ; [tan2020fastee](#bib.bib277) ; [tan2021multirocket](#bib.bib291) ; [dempster2023hydra](#bib.bib292)
    ; [lucas2019proximity](#bib.bib293) ; [herrmann2023proximity](#bib.bib294)）。最近的突破是
    Rocket 的开发（[dempster2019rocket](#bib.bib14)），它能够在不到 4 小时内处理 109 个单变量时间序列数据集，而之前最快的方法需要几天时间。Rocket
    利用大量随机卷积滤波器从每个序列中提取可能与分类相关的特征。然后，这些特征会传递给线性模型进行分类。Rocket 已经改进为更快的 Minirocket（[dempster2021minirocket](#bib.bib290)）和更准确的
    Multirocket（[tan2021multirocket](#bib.bib291)）以及 Hydra（[dempster2023hydra](#bib.bib292)）。Hydra
    与 Multirocket 结合后，现已成为 TSC 中最快和最准确的方法之一。
- en: Appendix B DNN Architectures for Time Series
  id: totrans-677
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 时间序列的 DNN 架构
- en: In this section, we provide a descriptive overview of deep learning-based models
    for TSC. The focus is on clarifying their architectures and outlining their adaptations
    to the specific characteristics of time series data.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了基于深度学习的 TSC 模型的描述性概述。重点在于阐明它们的架构，并概述它们对时间序列数据特定特征的适应。
- en: B.1\. Multi-Layer Perceptron (MLP)
  id: totrans-679
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1\. 多层感知器（MLP）
- en: 'The simplest Neural Network architecture is a fully connected network (FC),
    also known as a multilayer perceptron (MLP). As shown in Fig. [2](#A2.F2 "Figure
    2 ‣ B.1\. Multi-Layer Perceptron (MLP) ‣ Appendix B DNN Architectures for Time
    Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey") all neurons of one layer $l-1$ are connected to all neurons
    of the following layer $l$ with $l\in[1,L]$. The weights model these connections
    in a neural network. A general equation for applying a non-linearity to an input
    $A^{l-1}$ is:'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '最简单的神经网络架构是全连接网络（FC），也称为多层感知器（MLP）。如图 [2](#A2.F2 "Figure 2 ‣ B.1\. Multi-Layer
    Perceptron (MLP) ‣ Appendix B DNN Architectures for Time Series ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey") 所示，第
    $l-1$ 层的所有神经元都与下一层 $l$ 的所有神经元相连，其中 $l\in[1,L]$。权重用于建模神经网络中的这些连接。将非线性应用于输入 $A^{l-1}$
    的通用方程是：'
- en: '| (2) |  | $A^{l}=f(W^{l}\times A^{l-1}+b)$ |  |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $A^{l}=f(W^{l}\times A^{l-1}+b)$ |  |'
- en: where $A^{l}$ the activation of the neurons in layer $l$ where $A^{1}$ is equal
    to input series $X$. Also, $W$ and $b$ are the neuron weights and biases, and
    $f$ is the nonlinear activation function.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A^{l}$ 是第 $l$ 层神经元的激活值，其中 $A^{1}$ 等于输入序列 $X$。此外，$W$ 和 $b$ 分别是神经元的权重和偏置，$f$
    是非线性激活函数。
- en: '![Refer to caption](img/eae8bd7623dd31ec7ac38fd4c167e44e.png)'
  id: totrans-683
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eae8bd7623dd31ec7ac38fd4c167e44e.png)'
- en: Figure 2\. Multilayer perceptron for univariate time series classification.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 单变量时间序列分类的多层感知器。
- en: 'One of the main limitations of using multilayer perceptrons (MLPs) for time
    series data is that they are not well-suited to capturing the temporal dependencies
    in this type of data. MLPs are feedforward networks that process input data in
    a fixed and predetermined order without considering the temporal relationships
    between the input values. As shown in Fig. [2](#A2.F2 "Figure 2 ‣ B.1\. Multi-Layer
    Perceptron (MLP) ‣ Appendix B DNN Architectures for Time Series ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey"), each
    time step is weighted individually, and time series elements are treated independently
    from each other.'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '使用多层感知器（MLPs）处理时间序列数据的主要限制之一是它们不适合捕捉这种数据中的时间依赖性。MLPs 是前馈网络，它们以固定的预定顺序处理输入数据，而不考虑输入值之间的时间关系。如图
    [2](#A2.F2 "Figure 2 ‣ B.1\. Multi-Layer Perceptron (MLP) ‣ Appendix B DNN Architectures
    for Time Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey") 所示，每个时间步都被单独加权，时间序列元素彼此独立对待。'
- en: B.2\. Convolution Neural Networks (CNNs)
  id: totrans-686
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2\. 卷积神经网络（CNNs）
- en: The convolutional neural network (CNN) was first proposed by Kunihiko Fukushima
    in 1982 ([fukushima1982neocognitron,](#bib.bib295) ). It was inspired by the structure
    and function of the visual cortex in animals, specifically the cat’s cortex, as
    described by David Hubel and Torsten Wiesel in their influential work from 1962 ([hubel1962receptive,](#bib.bib296)
    ). Convolutional neural networks have been widely used for visual pattern recognition,
    but their ability to process large images was constrained by computational limitations
    until the emergence of GPU technology. Following the development of Graphics Processing
    Unit (GPU) technology, Krizhevsky et al. ([krizhevsky2012imagenet,](#bib.bib39)
    ) implemented an efficient GPU-based program and won the ImageNet competition
    in 2012, bringing the convolution neural network back into the spotlight.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）首次由福岛邦彦在1982年提出（[fukushima1982neocognitron,](#bib.bib295)）。其灵感来源于动物的视觉皮层结构和功能，特别是猫的皮层，这一点由大卫·休伯尔和托斯滕·维塞尔在他们1962年的重要工作中描述过（[hubel1962receptive,](#bib.bib296)）。卷积神经网络被广泛应用于视觉模式识别，但在GPU技术出现之前，其处理大图像的能力受到计算限制。随着图形处理单元（GPU）技术的发展，克里热夫斯基等人（[krizhevsky2012imagenet,](#bib.bib39)）实现了一个高效的基于GPU的程序，并在2012年赢得了ImageNet竞赛，使卷积神经网络重新成为焦点。
- en: 'Many variants of CNN architectures have been proposed in the literature, but
    their primary components are very similar. Using the LeNet-5 ([lecun1998gradient,](#bib.bib297)
    ) as an example, it consists of three types of layers: convolutional, pooling,
    and fully connected. The purpose of the convolutional layer is to learn feature
    representations of the inputs. Fig. [3](#A2.F3 "Figure 3 ‣ B.2\. Convolution Neural
    Networks (CNNs) ‣ Appendix B DNN Architectures for Time Series ‣ Deep Learning
    for Time Series Classification and Extrinsic Regression: A Current Survey") shows
    the architecture of the t-LeNet network, which is a time series-specific version
    of LeNet. This figure shows that the convolution layer is composed of several
    convolution kernels (or filters) used to compute different feature maps. In particular,
    each neuron of a feature map is connected to a region of neighboring neurons in
    the previous layer called the receptive field. Feature maps can be created by
    first convolving inputs with learned kernels and then applying an element-wise
    nonlinear activation function to the convolved results. It is important to note
    that all spatial locations of the input share the kernel for each feature map,
    and several kernels are used to obtain the entire feature map.'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '文献中提出了许多CNN架构的变体，但它们的主要组成部分非常相似。以LeNet-5（[lecun1998gradient,](#bib.bib297)）为例，它由三种类型的层组成：卷积层、池化层和全连接层。卷积层的目的是学习输入的特征表示。图[3](#A2.F3
    "Figure 3 ‣ B.2\. Convolution Neural Networks (CNNs) ‣ Appendix B DNN Architectures
    for Time Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey")展示了t-LeNet网络的架构，这是LeNet的时间序列特定版本。该图显示，卷积层由多个卷积核（或滤波器）组成，用于计算不同的特征图。特别是，特征图的每个神经元与前一层中称为感受野的相邻神经元区域相连接。特征图可以通过首先将输入与学习到的卷积核进行卷积，然后对卷积结果应用逐元素的非线性激活函数来创建。需要注意的是，输入的所有空间位置为每个特征图共享内核，并且使用多个内核来获取整个特征图。'
- en: 'The feature value of the $l$^(th) layer of $k$^(th) feature map at location
    $(i,j)$ is obtained by:'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 在位置$(i,j)$的$k$^(th)特征图的$l$^(th)层的特征值通过以下公式获得：
- en: '| (3) |  | $Z^{l}_{i,j,k}={\textbf{W}^{l}_{k}}^{T}\textbf{A}^{l-1}_{i,j}+b^{l}_{k}$
    |  |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $Z^{l}_{i,j,k}={\textbf{W}^{l}_{k}}^{T}\textbf{A}^{l-1}_{i,j}+b^{l}_{k}$
    |  |'
- en: 'Where $\textbf{W}^{l}_{k}$ and $b^{l}_{k}$ are the weight vector and bias term
    of the $k$^(th) filter of the $l$^(th) layer, respectively, and $\textbf{A}^{l-1}_{i,j}$
    is the input patch centered at location $(i,j)$ of the $l$ layer. Note that the
    kernel $\textbf{W}^{l}_{k}$ that generates the feature map $Z^{l}_{:,:,k}$ is
    shared. A weight-sharing mechanism has several advantages, such as reducing model
    complexity and making the network easier to train. Let $f\left(.\right)$ denote
    the nonlinear activation function. The activation value of convolutional feature
    $Z^{l}_{i,j,k}$ can be computed as:'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\textbf{W}^{l}_{k}$和$b^{l}_{k}$分别是$l$^(th)层的$k$^(th)滤波器的权重向量和偏置项，$\textbf{A}^{l-1}_{i,j}$是$l$层在位置$(i,j)$处以$(i,j)$为中心的输入块。请注意，生成特征图$Z^{l}_{:,:,k}$的内核$\textbf{W}^{l}_{k}$是共享的。权重共享机制具有多个优点，例如降低模型复杂度并使网络更易于训练。令$f\left(.\right)$表示非线性激活函数。卷积特征$Z^{l}_{i,j,k}$的激活值可以通过以下公式计算：
- en: '| (4) |  | $\textbf{A}^{l}_{i,j,k}=f(Z^{l}_{i,j,k})$ |  |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\textbf{A}^{l}_{i,j,k}=f(Z^{l}_{i,j,k})$ |  |'
- en: 'The most common activation functions are sigmoid, tanh and ReLU ([nair2010rectified,](#bib.bib298)
    ). As shown in Fig. [3](#A2.F3 "Figure 3 ‣ B.2\. Convolution Neural Networks (CNNs)
    ‣ Appendix B DNN Architectures for Time Series ‣ Deep Learning for Time Series
    Classification and Extrinsic Regression: A Current Survey"), a pooling layer is
    often placed between two convolution layers to reduce the resolution of the feature
    maps and to achieve shift-invariance. Following several convolution stages $-$the
    block comprising convolution, activation, and pooling is called convolution $stage$
    $-$ there may be one or more fully-connected layers that aim to perform high-level
    reasoning. As discussed in section [3.1](#S3.SS1 "3.1\. Multi-Layer Perceptron
    (MLP) ‣ 3\. Supervised Models ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey"), each neuron in the previous layer is
    connected to every neuron in the current layer to generate global semantic information.
    In the final layer of CNNs, there is the output layer in which the Softmax operators
    are commonly used for classification tasks ([gu2018recent,](#bib.bib40) ).'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的激活函数是 sigmoid、tanh 和 ReLU ([nair2010rectified,](#bib.bib298))。如图 [3](#A2.F3
    "图 3 ‣ B.2\. 卷积神经网络（CNN） ‣ 附录 B 时间序列的深度学习架构 ‣ 时间序列分类和外部回归的深度学习：当前调查") 所示，通常在两个卷积层之间放置一个池化层，以减少特征图的分辨率并实现平移不变性。在几个卷积阶段之后——这个阶段包括卷积、激活和池化，称为卷积
    $stage$——可能会有一个或多个全连接层，旨在进行高层次的推理。如在 [3.1](#S3.SS1 "3.1\. 多层感知器（MLP） ‣ 3\. 监督模型
    ‣ 时间序列分类和外部回归的深度学习：当前调查") 节中讨论的那样，前一层中的每个神经元都连接到当前层中的每个神经元，以生成全局语义信息。在 CNN 的最后一层中，通常使用
    Softmax 操作符进行分类任务 ([gu2018recent,](#bib.bib40))。
- en: '![Refer to caption](img/9228b68351f24d08b57a5a0c70acb0b0.png)'
  id: totrans-694
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9228b68351f24d08b57a5a0c70acb0b0.png)'
- en: Figure 3\. The architecture of the t-LeNet network (time series specific version
    of LeNet)
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. t-LeNet 网络的架构（LeNet 的时间序列特定版本）
- en: B.3\. Recurrent Neural Networks (RNN)
  id: totrans-696
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3\. 循环神经网络（RNN）
- en: 'RNNs are types of neural networks that are specifically designed to process
    time series and other sequential data. RNNs are conceptually similar to feed-forward
    neural networks (FFNs). While FFNs map from fixed-size inputs to fixed-size outputs,
    RNNs can process variable-length inputs and produce variable-length outputs. This
    capability is enabled by sharing parameters over time through directed connections
    between individual layers. RNN models for TSC can be classified as sequence to
    sequence or sequence-to-one based on their outputs. Fig. [4](#A2.F4 "Figure 4
    ‣ B.3\. Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures for Time
    Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey") shows sequence to sequence architectures for RNN models, with
    an output for each input sub-series. On the other hand, in sequence-to-one architecture,
    decisions are made using only $y^{T}$ and ignoring the other outputs.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 是专门设计用于处理时间序列和其他顺序数据的神经网络类型。RNN 在概念上类似于前馈神经网络（FFN）。虽然 FFN 将固定大小的输入映射到固定大小的输出，RNN
    可以处理可变长度的输入并生成可变长度的输出。这种能力是通过在个别层之间的定向连接上共享参数来实现的。针对时间序列分类的 RNN 模型可以根据其输出被分类为序列到序列或序列到单一输出架构。图
    [4](#A2.F4 "图 4 ‣ B.3\. 循环神经网络（RNN） ‣ 附录 B 时间序列的深度学习架构 ‣ 时间序列分类和外部回归的深度学习：当前调查")
    显示了 RNN 模型的序列到序列架构，每个输入子序列都有一个输出。另一方面，在序列到单一输出架构中，仅使用 $y^{T}$ 进行决策，忽略其他输出。
- en: '![Refer to caption](img/c112da5457c17e89bdc16ee0170c07f5.png)'
  id: totrans-698
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c112da5457c17e89bdc16ee0170c07f5.png)'
- en: Figure 4\. The architecture of two layer Recurrent Neural Network
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 两层循环神经网络的架构
- en: 'At each time step $t$, RNNs maintain a hidden vector $h$ which updates as follows ([hihi1995hierarchical,](#bib.bib299)
    ; [pascanu2013construct,](#bib.bib300) ):'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步 $t$，RNN 维护一个隐藏向量 $h$，其更新如下 ([hihi1995hierarchical,](#bib.bib299) ; [pascanu2013construct,](#bib.bib300))：
- en: '| (5) |  | $h_{t}=tanh(Wh_{t-1}+Ix^{t})$ |  |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $h_{t}=tanh(Wh_{t-1}+Ix^{t})$ |  |'
- en: 'Where $X=\left\{x^{1},...,x^{t-1},x^{t},...,x^{T}\right\}$ contains all of
    the observation, $tanh$ denotes the hyperbolic tangent function, and the recurrent
    weight and the projection matrix are shown by $W$ and $I$, respectively. The hidden-to-hidden
    connections also model the short-term time dependency. The hidden state $h$ is
    used to make a prediction as:'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X=\left\{x^{1},...,x^{t-1},x^{t},...,x^{T}\right\}$ 包含了所有观测值，$tanh$ 表示双曲正切函数，递归权重和投影矩阵分别由
    $W$ 和 $I$ 表示。隐藏层之间的连接还模拟了短期时间依赖关系。隐藏状态 $h$ 用于进行预测，如下所示：
- en: '| (6) |  | $y^{t}=\sigma_{s}(Wh_{t-1})$ |  |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $y^{t}=\sigma_{s}(Wh_{t-1})$ |  |'
- en: 'where $\sigma_{s}$ is a softmax function and provides a normalized probability
    distribution over the possible classes. As depicted in Fig. [4](#A2.F4 "Figure
    4 ‣ B.3\. Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures for Time
    Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey"), the hidden state $h$ can be used to stack RNNs in order to
    build deeper networks:'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma_{s}$ 是 softmax 函数，提供了可能类别的标准化概率分布。如图 [4](#A2.F4 "图 4 ‣ B.3\. 递归神经网络
    (RNN) ‣ 附录 B DNN 架构用于时间序列 ‣ 深度学习用于时间序列分类和外部回归：当前调查") 所示，隐藏状态 $h$ 可用于堆叠 RNN 以构建更深的网络：
- en: '| (7) |  | $h_{t}^{l}=\sigma(Wh^{l}_{t-1}+Ih^{l-1}_{t})$ |  |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $h_{t}^{l}=\sigma(Wh^{l}_{t-1}+Ih^{l-1}_{t})$ |  |'
- en: where $\sigma$ is the logistic sigmoid function. As an alternative to feeding
    each time step to the RNN, the data can be divided into time windows of $\omega$
    observations, with the option for variable overlaps. Each time window is labeled
    with the majority response labels within the $\omega$ window.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 是逻辑 sigmoid 函数。作为将每个时间步输入 RNN 的替代方案，可以将数据分成 $\omega$ 个观测的时间窗口，并可选择可变重叠。每个时间窗口都标记为窗口内多数响应标签。
- en: B.3.1\. Long Short Term Memory (LSTM)
  id: totrans-707
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.1\. 长短期记忆 (LSTM)
- en: 'LSTM deals with the vanishing/exploding gradient problem commonly found in
    standard recurrent neural networks through the incorporation of gate-controlled
    memory cells into their state dynamics ([hochreiter1997long,](#bib.bib78) ). As
    shown in Fig. [5](#A2.F5 "Figure 5 ‣ B.3.2\. Gated Recurrent Unit (GRU) ‣ B.3\.
    Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures for Time Series
    ‣ Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey") (a) LSTM uses a hidden vector $h$ and a memory vector $m$ to control
    state updates and outputs for each time step. Specifically, the computation at
    time step $t$ is formulated as follows ([kawakami2008supervised,](#bib.bib301)
    ):'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 'LSTM 通过将门控记忆单元引入状态动态中，处理标准递归神经网络中常见的梯度消失/爆炸问题 ([hochreiter1997long,](#bib.bib78)
    )。如图 [5](#A2.F5 "图 5 ‣ B.3.2\. 门控递归单元 (GRU) ‣ B.3\. 递归神经网络 (RNN) ‣ 附录 B DNN 架构用于时间序列
    ‣ 深度学习用于时间序列分类和外部回归：当前调查") (a) LSTM 使用隐藏向量 $h$ 和记忆向量 $m$ 来控制每个时间步的状态更新和输出。具体来说，时间步
    $t$ 的计算公式如下 ([kawakami2008supervised,](#bib.bib301) ):'
- en: '| (8) |  | <math   alttext="\begin{split}&amp;\Gamma^{c}=tanh(W^{c}h_{t-1}+I^{c}x^{t})\\
    &amp;\Gamma^{u}=\sigma(W^{u}h_{t-1}+I^{u}x^{t})\\'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '| (8) |  | <math   alttext="\begin{split}&amp;\Gamma^{c}=tanh(W^{c}h_{t-1}+I^{c}x^{t})\\
    &amp;\Gamma^{u}=\sigma(W^{u}h_{t-1}+I^{u}x^{t})\\'
- en: '&amp;\Gamma^{f}=\sigma(W^{f}h_{t-1}+I^{f}x^{t})\\'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\Gamma^{f}=\sigma(W^{f}h_{t-1}+I^{f}x^{t})\\'
- en: '&amp;\Gamma^{o}=\sigma(W^{o}h_{t-1}+I^{o}x^{t})\\'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\Gamma^{o}=\sigma(W^{o}h_{t-1}+I^{o}x^{t})\\'
- en: '&amp;m_{t}=\Gamma^{f}\otimes m_{t-1}+\Gamma^{u}\otimes\Gamma^{c}\\'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;m_{t}=\Gamma^{f}\otimes m_{t-1}+\Gamma^{u}\otimes\Gamma^{c}\\'
- en: '&amp;h_{t}=tanh(\Gamma^{o}\otimes m_{t})\end{split}" display="block"><semantics
    ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr ><mtd  columnalign="left"
    ><mrow  ><msup ><mi mathvariant="normal" >Γ</mi><mi >c</mi></msup><mo >=</mo><mrow
    ><mi  >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >h</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mrow  ><msup
    ><mi >W</mi><mi  >c</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >h</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo >+</mo><mrow
    ><msup ><mi  >I</mi><mi >c</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >x</mi><mi >t</mi></msup></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><msup ><mi mathvariant="normal" >Γ</mi><mi  >u</mi></msup><mo
    >=</mo><mrow ><mi  >σ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mrow  ><msup ><mi >W</mi><mi  >u</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >h</mi><mrow ><mi  >t</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo
    >+</mo><mrow ><msup ><mi  >I</mi><mi >u</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >x</mi><mi >t</mi></msup></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><msup ><mi mathvariant="normal" >Γ</mi><mi  >f</mi></msup><mo
    >=</mo><mrow ><mi  >σ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mrow  ><msup ><mi >W</mi><mi  >f</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >h</mi><mrow ><mi  >t</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo
    >+</mo><mrow ><msup ><mi  >I</mi><mi >f</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >x</mi><mi >t</mi></msup></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><msup ><mi mathvariant="normal" >Γ</mi><mi  >o</mi></msup><mo
    >=</mo><mrow ><mi  >σ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mrow ><msup ><mi  >W</mi><mi >o</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >h</mi><mrow ><mi  >t</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo
    >+</mo><mrow ><msup ><mi >I</mi><mi >o</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >x</mi><mi >t</mi></msup></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><msub ><mi  >m</mi><mi >t</mi></msub><mo >=</mo><mrow
    ><mrow  ><msup ><mi mathvariant="normal" >Γ</mi><mi >f</mi></msup><mo lspace="0.222em"
    rspace="0.222em" >⊗</mo><msub ><mi  >m</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo
    >+</mo><mrow ><msup  ><mi mathvariant="normal"  >Γ</mi><mi >u</mi></msup><mo lspace="0.222em"
    rspace="0.222em"  >⊗</mo><msup ><mi mathvariant="normal" >Γ</mi><mi >c</mi></msup></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><msub ><mi  >h</mi><mi >t</mi></msub><mo >=</mo><mrow
    ><mi  >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >h</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msup
    ><mi mathvariant="normal" >Γ</mi><mi >o</mi></msup><mo lspace="0.222em" rspace="0.222em"
    >⊗</mo><msub ><mi >m</mi><mi >t</mi></msub></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >Γ</ci><ci >𝑐</ci></apply><apply ><ci  >𝑡</ci><ci >𝑎</ci><ci >𝑛</ci><ci  >ℎ</ci><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑊</ci><ci
    >𝑐</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℎ</ci><apply
    ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci >𝐼</ci><ci >𝑐</ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >Γ</ci><ci >𝑢</ci></apply></apply></apply><apply
    ><apply  ><ci >𝜎</ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑊</ci><ci >𝑢</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℎ</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐼</ci><ci >𝑢</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >Γ</ci><ci  >𝑓</ci></apply></apply></apply><apply
    ><apply ><ci  >𝜎</ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑊</ci><ci >𝑓</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℎ</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐼</ci><ci >𝑓</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >Γ</ci><ci  >𝑜</ci></apply></apply></apply><apply
    ><apply ><ci  >𝜎</ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑊</ci><ci >𝑜</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℎ</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐼</ci><ci >𝑜</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑚</ci><ci  >𝑡</ci></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="latexml"  >tensor-product</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >Γ</ci><ci >𝑓</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑚</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="latexml" >tensor-product</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >Γ</ci><ci >𝑢</ci></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >Γ</ci><ci >𝑐</ci></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ℎ</ci><ci  >𝑡</ci></apply></apply></apply></apply><apply
    ><apply ><ci  >𝑡</ci><ci >𝑎</ci><ci >𝑛</ci><ci  >ℎ</ci><apply ><csymbol cd="latexml"
    >tensor-product</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >Γ</ci><ci >𝑜</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑚</ci><ci >𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&\Gamma^{c}=tanh(W^{c}h_{t-1}+I^{c}x^{t})\\
    &\Gamma^{u}=\sigma(W^{u}h_{t-1}+I^{u}x^{t})\\ &\Gamma^{f}=\sigma(W^{f}h_{t-1}+I^{f}x^{t})\\
    &\Gamma^{o}=\sigma(W^{o}h_{t-1}+I^{o}x^{t})\\ &m_{t}=\Gamma^{f}\otimes m_{t-1}+\Gamma^{u}\otimes\Gamma^{c}\\
    &h_{t}=tanh(\Gamma^{o}\otimes m_{t})\end{split}</annotation></semantics></math>
    |  |'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Γ^{c}=tanh(W^{c}h_{t-1}+I^{c}x^{t})\\ &Γ^{u}=\sigma(W^{u}h_{t-1}+I^{u}x^{t})\\
    &Γ^{f}=\sigma(W^{f}h_{t-1}+I^{f}x^{t})\\ &Γ^{o}=\sigma(W^{o}h_{t-1}+I^{o}x^{t})\\
    &m_{t}=Γ^{f}⊗m_{t-1}+Γ^{u}⊗Γ^{c}\\ &h_{t}=tanh(Γ^{o}⊗m_{t})'
- en: where $\Gamma^{c}$ is a cell state gate and $\Gamma^{u},\Gamma^{f}$ and $\Gamma^{o}$
    are the activation vector of the input, forget and output gate respectively. $\sigma$
    is the logistic sigmoid function and $\otimes$ shows the element-wise product.
    $W^{u},W^{f},W^{o},W^{c}$ represent the recurrent weight matrices, and $I^{u},I^{f},I^{o},I^{c}$
    represent the projection matrices.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Gamma^{c}$ 是细胞状态门，$\Gamma^{u},\Gamma^{f}$ 和 $\Gamma^{o}$ 分别是输入门、遗忘门和输出门的激活向量。$\sigma$
    是逻辑 sigmoid 函数，$\otimes$ 表示逐元素乘积。$W^{u},W^{f},W^{o},W^{c}$ 代表递归权重矩阵，$I^{u},I^{f},I^{o},I^{c}$
    代表投影矩阵。
- en: B.3.2\. Gated Recurrent Unit (GRU)
  id: totrans-715
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.2\. 门控递归单元（GRU）
- en: 'GRU is another widely-used variant of RNNs, and similar to LSTM it can control
    the flow of information like memorizing the context over multiple time steps ([chung2014empirical,](#bib.bib79)
    ). While GRU was introduced later than LSTM, it has a simpler architecture. As
    shown in Fig [5](#A2.F5 "Figure 5 ‣ B.3.2\. Gated Recurrent Unit (GRU) ‣ B.3\.
    Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures for Time Series
    ‣ Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey") (b), compared to LSTMs, GRUs have two gates, reset and update gates,
    which are more computationally efficient and require fewer data to generalize
    and defined as follows:'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 'GRU 是 RNN 的另一种广泛使用的变体，类似于 LSTM，它能够控制信息流，如记住多时间步的上下文 ([chung2014empirical,](#bib.bib79)
    )。虽然 GRU 比 LSTM 推出的时间晚，但其架构更简单。如图 [5](#A2.F5 "Figure 5 ‣ B.3.2\. Gated Recurrent
    Unit (GRU) ‣ B.3\. Recurrent Neural Networks (RNN) ‣ Appendix B DNN Architectures
    for Time Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey") (b) 所示，与 LSTM 相比，GRU 具有两个门控：重置门和更新门，这些门控计算更高效，并且需要更少的数据来进行泛化，其定义如下：'
- en: '| (9) |  | <math   alttext="\begin{split}&amp;\Gamma^{z}=\sigma(W^{z}h_{t-1}+I^{z}x^{t})\\
    &amp;\Gamma^{r}=\sigma(W^{r}h_{t-1}+I^{r}x^{t})\\'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '| (9) |  | <math   alttext="\begin{split}&amp;\Gamma^{z}=\sigma(W^{z}h_{t-1}+I^{z}x^{t})\\
    &amp;\Gamma^{r}=\sigma(W^{r}h_{t-1}+I^{r}x^{t})\\'
- en: '&amp;\widetilde{h}_{t}=tanh(W[h_{t-1}\otimes\Gamma^{r},x^{t}])\\'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\widetilde{h}_{t}=tanh(W[h_{t-1}\otimes\Gamma^{r},x^{t}])\\'
- en: '&amp;h_{t}=(1-\Gamma^{z})\otimes h_{t-1}+\Gamma^{z}\otimes\widetilde{h}_{t}\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  columnalign="left" ><mrow  ><msup ><mi mathvariant="normal" >Γ</mi><mi
    >z</mi></msup><mo >=</mo><mrow ><mi  >σ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><mrow  ><msup ><mi >W</mi><mi  >z</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >h</mi><mrow  ><mi >t</mi><mo >−</mo><mn  >1</mn></mrow></msub></mrow><mo
    >+</mo><mrow ><msup ><mi  >I</mi><mi >z</mi></msup><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >x</mi><mi >t</mi></msup></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><msup  ><mi mathvariant="normal"  >Γ</mi><mi
    >r</mi></msup><mo >=</mo><mrow ><mi  >σ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><mrow  ><msup ><mi >W</mi><mi  >r</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >h</mi><mrow ><mi  >t</mi><mo >−</mo><mn
    >1</mn></mrow></msub></mrow><mo >+</mo><mrow ><msup ><mi  >I</mi><mi >r</mi></msup><mo
    lspace="0em" rspace="0em"  >​</mo><msup ><mi >x</mi><mi >t</mi></msup></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><msub  ><mover accent="true"  ><mi >h</mi><mo >~</mo></mover><mi >t</mi></msub><mo
    >=</mo><mrow ><mi  >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >h</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mrow
    ><mi  >W</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >[</mo><mrow ><msub ><mi  >h</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow></msub><mo
    lspace="0.222em" rspace="0.222em"  >⊗</mo><msup ><mi mathvariant="normal"  >Γ</mi><mi
    >r</mi></msup></mrow><mo >,</mo><msup ><mi  >x</mi><mi >t</mi></msup><mo stretchy="false"  >]</mo></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><msub  ><mi >h</mi><mi >t</mi></msub><mo >=</mo><mrow ><mrow  ><mrow ><mo stretchy="false"
    >(</mo><mrow ><mn  >1</mn><mo >−</mo><msup ><mi mathvariant="normal" >Γ</mi><mi  >z</mi></msup></mrow><mo
    rspace="0.055em" stretchy="false"  >)</mo></mrow><mo rspace="0.222em"  >⊗</mo><msub
    ><mi >h</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow><mo >+</mo><mrow
    ><msup  ><mi mathvariant="normal"  >Γ</mi><mi >z</mi></msup><mo lspace="0.222em"
    rspace="0.222em"  >⊗</mo><msub ><mover accent="true" ><mi >h</mi><mo >~</mo></mover><mi
    >t</mi></msub></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >Γ</ci><ci  >𝑧</ci></apply><apply
    ><ci >𝜎</ci><apply  ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑊</ci><ci  >𝑧</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℎ</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐼</ci><ci >𝑧</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >Γ</ci><ci  >𝑟</ci></apply></apply></apply><apply
    ><apply ><ci  >𝜎</ci><apply ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑊</ci><ci >𝑟</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℎ</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐼</ci><ci >𝑟</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑥</ci><ci >𝑡</ci></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><ci  >~</ci><ci >ℎ</ci></apply><ci
    >𝑡</ci></apply></apply></apply><apply ><apply ><ci  >𝑡</ci><ci >𝑎</ci><ci >𝑛</ci><ci  >ℎ</ci><apply
    ><ci >𝑊</ci><interval closure="closed" ><apply  ><csymbol cd="latexml"  >tensor-product</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ℎ</ci><apply ><ci >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >Γ</ci><ci >𝑟</ci></apply></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑥</ci><ci >𝑡</ci></apply></interval></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ℎ</ci><ci >𝑡</ci></apply></apply></apply><apply ><apply  ><apply ><csymbol cd="latexml"
    >tensor-product</csymbol><apply ><cn type="integer"  >1</cn><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >Γ</ci><ci >𝑧</ci></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ℎ</ci><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply></apply></apply><apply
    ><csymbol cd="latexml" >tensor-product</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >Γ</ci><ci >𝑧</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><ci  >~</ci><ci >ℎ</ci></apply><ci >𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}&\Gamma^{z}=\sigma(W^{z}h_{t-1}+I^{z}x^{t})\\
    &\Gamma^{r}=\sigma(W^{r}h_{t-1}+I^{r}x^{t})\\ &\widetilde{h}_{t}=tanh(W[h_{t-1}\otimes\Gamma^{r},x^{t}])\\
    &h_{t}=(1-\Gamma^{z})\otimes h_{t-1}+\Gamma^{z}\otimes\widetilde{h}_{t}\end{split}</annotation></semantics></math>
    |  |'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: Where $W^{z}$ and $W^{r}$ are the weight matrices associated with gates, and
    $\Gamma^{z}$ and $\Gamma^{r}$ represent the update and reset gates, respectively.
    The function $\sigma$ denotes the logistic sigmoid, and $\otimes$ shows the element-wise
    product.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$W^{z}$和$W^{r}$是与门相关的权重矩阵，$\Gamma^{z}$和$\Gamma^{r}$分别表示更新门和重置门。函数$\sigma$表示逻辑
    sigmoid，$\otimes$表示逐元素乘积。
- en: Figure 5\. The architecture of LSTM (a) and GRU (b) units
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. LSTM (a) 和 GRU (b) 单元的架构
- en: B.4\. Attention Based Model
  id: totrans-722
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4\. 基于注意力的模型
- en: B.4.1\. Self-Attention
  id: totrans-723
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.4.1\. 自注意力
- en: 'The attention mechanism was introduced by ([bahdanau2014neural,](#bib.bib302)
    ) for improving the performance of encoder-decoder models ([cho2014learning,](#bib.bib303)
    ) in neural machine translation. The encoder-decoder in neural machine translation
    encodes a source sentence into a vector in latent space and decodes the latent
    vector into a target language sentence. As shown in Fig. [6](#A2.F6 "Figure 6
    ‣ B.4.1\. Self-Attention ‣ B.4\. Attention Based Model ‣ Appendix B DNN Architectures
    for Time Series ‣ Deep Learning for Time Series Classification and Extrinsic Regression:
    A Current Survey"), the attention mechanism allows the decoder to pay attention
    to the segments of the source for each target through a context vector $c_{t}$.
    For this model, a variable-length attention vector $\alpha_{t}$, equal to the
    number of source time steps, is derived by comparing the current target hidden
    state $h_{t}$ with each source hidden state $\overline{h}_{s}$ as follows ([luong2015effective,](#bib.bib304)
    ):'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制由 ([bahdanau2014neural,](#bib.bib302)) 引入，以提高编码器-解码器模型 ([cho2014learning,](#bib.bib303))
    在神经机器翻译中的性能。神经机器翻译中的编码器-解码器将源句子编码为潜在空间中的一个向量，并将潜在向量解码为目标语言句子。如图 [6](#A2.F6 "图
    6 ‣ B.4.1\. 自注意力 ‣ B.4\. 基于注意力的模型 ‣ 附录 B 时间序列的 DNN 架构 ‣ 深度学习在时间序列分类和外部回归中的当前调查")
    所示，注意力机制允许解码器通过上下文向量$c_{t}$关注源句子的各个片段。对于这个模型，通过将当前目标隐藏状态$h_{t}$与每个源隐藏状态$\overline{h}_{s}$进行比较，得到一个等于源时间步骤数量的可变长度注意力向量$\alpha_{t}$
    ([luong2015effective,](#bib.bib304))：
- en: '| (10) |  | $\alpha_{t}(s)=\frac{{exp(score(h_{t},\overline{h}_{s})})}{\sum_{s^{\prime}}exp(score(h_{t},\overline{h}_{s^{\prime}}))}$
    |  |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\alpha_{t}(s)=\frac{{exp(score(h_{t},\overline{h}_{s})})}{\sum_{s^{\prime}}exp(score(h_{t},\overline{h}_{s^{\prime}}))}$
    |  |'
- en: 'The term $score$ is referred to as an alignment model and used to compare the
    target hidden state $h_{t}$ with each of the source hidden states $\overline{h}_{s}$,
    and the result is normalized to produced attention weights (a distribution over
    source positions). There are various choices of the scoring function:'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 术语$score$被称为对齐模型，用于将目标隐藏状态$h_{t}$与每个源隐藏状态$\overline{h}_{s}$进行比较，并且结果被归一化以生成注意力权重（在源位置上的分布）。评分函数有多种选择：
- en: '| (11) |  | $score(h_{t},\overline{h}_{s})=\begin{cases}h_{t}^{T}W\overline{h}_{s}\\
    v_{\alpha}^{T}tanh(W_{\alpha}[h_{t};\overline{h}_{s}])\end{cases}$ |  |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $score(h_{t},\overline{h}_{s})=\begin{cases}h_{t}^{T}W\overline{h}_{s}\\
    v_{\alpha}^{T}tanh(W_{\alpha}[h_{t};\overline{h}_{s}])\end{cases}$ |  |'
- en: These scores influence the attention distribution, impacting how the model attends
    to different parts of the input sequence during predictions. As shown above, the
    score function is parameterized as a feedforward neural network that is jointly
    trained with all the other components of the model. The model directly computes
    soft attention, allowing the cost function’s gradient to be backpropagated ([bahdanau2014neural,](#bib.bib302)
    ).
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分数影响注意力分布，进而影响模型在预测过程中如何关注输入序列的不同部分。如上所示，评分函数被参数化为一个前馈神经网络，与模型的其他组件一起进行联合训练。模型直接计算软注意力，使得成本函数的梯度可以被反向传播
    ([bahdanau2014neural,](#bib.bib302))。
- en: 'Given the alignment vector as weights, the context vector $c_{t}$ is computed
    as the weighted average over all the source hidden state:'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 给定对齐向量作为权重，上下文向量$c_{t}$被计算为所有源隐藏状态的加权平均：
- en: '| (12) |  | $c_{t}=\sum_{s}\alpha_{ts}\overline{h}_{s}$ |  |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $c_{t}=\sum_{s}\alpha_{ts}\overline{h}_{s}$ |  |'
- en: Accordingly, the computation path goes from $h_{t}\rightarrow\alpha_{t}\rightarrow
    c_{t}\rightarrow\widetilde{h}_{t}$ then make a prediction using a $Softmax$ function ([luong2015effective,](#bib.bib304)
    ). Note that $\widetilde{h}_{t}$ is a refined hidden state that incorporates both
    the original hidden state $h_{t}$ and the context information $c_{t}$ obtained
    through attention mechanisms.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，计算路径从 $h_{t}\rightarrow\alpha_{t}\rightarrow c_{t}\rightarrow\widetilde{h}_{t}$
    然后使用 $Softmax$ 函数进行预测 ([luong2015effective,](#bib.bib304))。请注意，$\widetilde{h}_{t}$
    是一个精炼的隐藏状态，它结合了原始隐藏状态 $h_{t}$ 和通过注意力机制获得的上下文信息 $c_{t}$。
- en: '![Refer to caption](img/09e51ab0a8e45d63d08e287c4b6f4b21.png)'
  id: totrans-732
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/09e51ab0a8e45d63d08e287c4b6f4b21.png)'
- en: Figure 6\. Self-Attention mechanism
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 自注意力机制
- en: B.4.2\. Transformers
  id: totrans-734
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.4.2\. Transformers
- en: Similar to self-attention and other competitive neural sequence models, the
    original transformer developed for NLP (hereinafter the vanilla transformer) has
    an encoder-decoder structure that takes as input a sequence of words from the
    source language and then generates the translation in the target language ([vaswani2017attention,](#bib.bib92)
    ). Both the encoder and decoder are composed of multiple identical blocks. Each
    encoder block consists of a multi-head self-attention module and a position-wise
    feed-forward network (FFN), while each decoder block inserts cross-attention models
    between the multi-head self-attention module and the position-wise feed-forward
    network (FFN). Unlike RNNs, Transformers do not use recurrence and instead model
    sequence information using the positional encoding in the input embeddings.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 与自注意力和其他竞争性的神经序列模型类似，最初为 NLP 开发的 transformer（以下简称 vanilla transformer）具有一个编码器-解码器结构，它以源语言的单词序列为输入，然后生成目标语言中的翻译
    ([vaswani2017attention,](#bib.bib92))。编码器和解码器都由多个相同的模块组成。每个编码器模块包括一个多头自注意力模块和一个位置-wise
    前馈网络（FFN），而每个解码器模块在多头自注意力模块和位置-wise 前馈网络（FFN）之间插入交叉注意力模型。与 RNN 不同，Transformers
    不使用递归，而是通过输入嵌入中的位置编码来建模序列信息。
- en: 'The transformer architecture is based on finding associations or correlations
    between various input segments using the dot product. As shown in Fig. [7](#A2.F7
    "Figure 7 ‣ B.4.2\. Transformers ‣ B.4\. Attention Based Model ‣ Appendix B DNN
    Architectures for Time Series ‣ Deep Learning for Time Series Classification and
    Extrinsic Regression: A Current Survey"), the attention operation in transformers
    starts with building three different linearly-weighted vectors from the input
    $x_{i}$, referred to as query ($q_{i}$), key ($k_{i}$), and value ($v_{i}$):'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer 架构基于使用点积在各种输入片段之间寻找关联或相关性。如图 [7](#A2.F7 "Figure 7 ‣ B.4.2\. Transformers
    ‣ B.4\. Attention Based Model ‣ Appendix B DNN Architectures for Time Series ‣
    Deep Learning for Time Series Classification and Extrinsic Regression: A Current
    Survey") 所示，transformer 中的注意力操作从输入 $x_{i}$ 构建三个不同的线性加权向量，这些向量分别称为查询（$q_{i}$）、键（$k_{i}$）和值（$v_{i}$）：'
- en: '| (13) |  | $\textbf{q}_{i}=W_{q}\textbf{x}_{i},\quad\textbf{k}_{i}=W_{k}\textbf{x}_{i},\quad\textbf{v}_{i}=W_{v}\textbf{x}_{i}$
    |  |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\textbf{q}_{i}=W_{q}\textbf{x}_{i},\quad\textbf{k}_{i}=W_{k}\textbf{x}_{i},\quad\textbf{v}_{i}=W_{v}\textbf{x}_{i}$
    |  |'
- en: 'where $W_{q},W_{k}$ and $W_{v}$ learnable weight matrices. The output vectors
    $\textbf{z}_{i}$ are given by:'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{q},W_{k}$ 和 $W_{v}$ 是可学习的权重矩阵。输出向量 $\textbf{z}_{i}$ 由以下公式给出：
- en: '| (14) |  | $\textbf{z}_{i}=\sum_{j}softmax\left(\frac{\textbf{q}_{i}^{T}\textbf{k}_{j}}{\sqrt{d_{q}}}\right)\textbf{v}_{i}$
    |  |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\textbf{z}_{i}=\sum_{j}softmax\left(\frac{\textbf{q}_{i}^{T}\textbf{k}_{j}}{\sqrt{d_{q}}}\right)\textbf{v}_{i}$
    |  |'
- en: Note that the weighting of the value vector $\textbf{v}_{i}$ depends on the
    mapped correlation between the query vector $\textbf{q}_{i}$ at position $i$ and
    the key vector $\textbf{k}_{j}$ at position $j$. The value of the dot product
    tends to grow with the increasing size of the query and key vectors. As the softmax
    function is sensitive to large values, the attention weights are scaled by the
    square root of the size of the query and key vectors $d_{q}$. The input data may
    contain several levels of correlation information, and the learning process may
    benefit from processing the input data in multiple different ways. Multiple attention
    heads are introduced that operate on the same input in parallel and use different
    weight matrices $W_{q}$,$W_{k}$, and $W_{v}$ to extract various levels of correlation
    between the input data.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，值向量 $\textbf{v}_{i}$ 的加权依赖于查询向量 $\textbf{q}_{i}$ 在位置 $i$ 和键向量 $\textbf{k}_{j}$
    在位置 $j$ 之间的映射相关性。点积的值随着查询和键向量的大小增加而增加。由于 softmax 函数对大值敏感，注意力权重被缩放为查询和键向量大小 $d_{q}$
    的平方根。输入数据可能包含多个层次的相关信息，学习过程可能会受益于以多种不同方式处理输入数据。引入了多个注意力头，这些头在相同输入上并行操作，并使用不同的权重矩阵
    $W_{q}$、$W_{k}$ 和 $W_{v}$ 提取输入数据之间的各种层次的相关性。
- en: '![Refer to caption](img/fbabde38bdb9a91670cfd07af6bb8d32.png)'
  id: totrans-741
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fbabde38bdb9a91670cfd07af6bb8d32.png)'
- en: 'Figure 7\. Multi-head attention block: the example consists of eight heads,
    and the input sequence comprises two time steps.'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 多头注意力块：该示例由八个头组成，输入序列包含两个时间步。
- en: B.5\. Graph Neural Networks
  id: totrans-743
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.5. 图神经网络
- en: A graph consists of a set of nodes and a set of edges, each of which connects
    two nodes. Both nodes and edges may have attributes associated with them. The
    edges may be directional or unidirectional, and may be weighted. Graphs are useful
    for representing data that cannot be represented in Euclidean space, such as molecular
    structures, social networks, and spatial temporal data (for example, electroencephalogram
    (EEG) or traffic monitoring networks).
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 图由一组节点和一组边组成，每条边连接两个节点。节点和边可能有与之相关的属性。边可以是有向或无向的，并且可以加权。图对表示不能用欧几里得空间表示的数据很有用，例如分子结构、社交网络和时空数据（例如脑电图（EEG）或交通监控网络）。
- en: Graph neural networks (GNNs) were first proposed by Scarcelli et al. ([Scarselli2009graph,](#bib.bib120)
    ) to learn directly from graph representations of data. Prior to the use of GNNs,
    techniques such as recursive neural networks and Markov chains (random walk models)
    were used to incorporate graph structures. However, these methods required a pre-processing
    step, rather than learning directly from the graph structure. GNNs take as input
    the graph structure and any associated node and edge attributes. Depending on
    the required task, the GNN output can be per node, per edge or a single output
    per graph ([Wu2021graph,](#bib.bib119) ).
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）最初由 Scarcelli 等人 ([Scarselli2009graph,](#bib.bib120)) 提出，以直接从数据的图表示中学习。在使用
    GNNs 之前，诸如递归神经网络和马尔可夫链（随机游走模型）等技术被用来融入图结构。然而，这些方法需要预处理步骤，而不是直接从图结构中学习。GNNs 接收图结构及任何相关的节点和边属性作为输入。根据任务的需求，GNN
    输出可以是每个节点、每条边或每个图的单一输出 ([Wu2021graph,](#bib.bib119))。
- en: Scarcelli et al.’s proposed GNN combines recursive neural networks and Markov
    chains to deal directly with the graph structure without any pre-processing requirement.
    While the network structure is predefined, the edge weights are parameters learned
    during training. During training, units exchange information and update their
    states until reaching equilibrium.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: Scarcelli 等人提出的 GNN 结合了递归神经网络和马尔可夫链，直接处理图结构，无需任何预处理要求。虽然网络结构是预定义的，但边权重是训练过程中学习的参数。在训练期间，单元交换信息并更新其状态，直到达到平衡。
- en: Graph convolutional networks (GCN) were proposed by Bruna et al. ([Bruna2013graph,](#bib.bib305)
    ) and extend CNNs to graph structures. Bruna et al. proposed two methods of constructing
    the graph, spatial and spectral. The spatial technique simply applies the convolution
    operator to the local neighbourhood of each node, followed by a pooling operator.
    Although this reduces the spatial resolution, successive layers compensate for
    this by increasing the number of filters. Spectral construction first transforms
    the graph to a matrix V, which consists of the eigenvectors of the graph Laplacian,
    order by eigenvalue. The eigenvectors represent frequency components of the original
    graphs, so lower-order eigenvectors modulate slowly, thus neighbouring nodes have
    similar values. Higher-order eigenvectors modulate more rapidly and connected
    nodes are likely to have dissimilar values ([Shuman2013graph,](#bib.bib306) ).
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积网络（GCN）由Bruna等人提出（[Bruna2013graph,](#bib.bib305)），并将CNN扩展到图结构。Bruna等人提出了两种构建图的方法，空间和谱方法。空间技术简单地将卷积操作应用于每个节点的局部邻域，然后进行池化操作。虽然这会减少空间分辨率，但后续层通过增加滤波器的数量来弥补这一点。谱构建首先将图转换为矩阵V，该矩阵由图拉普拉斯算子的特征向量组成，按特征值排序。特征向量表示原始图的频率分量，因此低阶特征向量变化缓慢，相邻节点的值相似。高阶特征向量变化更快，连接的节点可能具有不同的值（[Shuman2013graph,](#bib.bib306)）。
- en: Many real-world graph datasets evolve over time – edges and nodes may come into
    existence, disappear or attributes may change value. Dynamic or temporal graphs
    build this information into the graph structure, for instance by the use of temporal
    nodes and edges, that include initial and final timestamps ([Longa2023graph,](#bib.bib307)
    ). Alternatively, spatiotemporal GNNs model the spatial and temporal aspects in
    separate layers, using GCN layers to learn spatial representations and RNN or
    1D-CNN layers for the temporal representations ([Wu2021graph,](#bib.bib119) ).
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界中的图数据集会随着时间的推移而演变——边和节点可能会出现、消失或属性值可能会发生变化。动态或时间图将这些信息纳入图结构中，例如通过使用时间节点和边，包括初始和最终时间戳（[Longa2023graph,](#bib.bib307)）。另外，时空图神经网络（GNN）在不同的层中建模空间和时间方面，使用GCN层来学习空间表示，使用RNN或1D-CNN层来学习时间表示（[Wu2021graph,](#bib.bib119)）。
- en: Appendix C Datasets
  id: totrans-749
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 数据集
- en: C.1\. HAR Datasets
  id: totrans-750
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1\. HAR 数据集
- en: 'Many of the studies reviewed in the subsection reviewing the use of deep learning
    for human activity recognition time series use publicly available datasets. Some
    of the most commonly used datasets are listed in table [7](#A3.T7 "Table 7 ‣ C.1\.
    HAR Datasets ‣ Appendix C Datasets ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey"), together with the number of participants,
    the sensors used to collect the data, a description of the activities recorded,
    and references to the studies using each dataset. Larger lists of datasets are
    provided in ([chen2021deep,](#bib.bib5) ; [Ramanujam2021survey,](#bib.bib186)
    ). Common activity sets include activities of daily living (ADL) or basic activities
    (e.g. walking, running, sitting, standing, ascending/descending stairs. However,
    activities for more specialised events such as gait freezing in Parkinson’s Disease
    patients ([Bachlin2009daphnet,](#bib.bib308) ), falls ([Micucci2017unimib,](#bib.bib309)
    ), and manufacturing activities ([Zappi2008skoda,](#bib.bib310) ) are also collected.'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '在回顾使用深度学习进行人类活动识别时间序列的子章节中，许多研究使用了公开可用的数据集。表[7](#A3.T7 "Table 7 ‣ C.1\. HAR
    Datasets ‣ Appendix C Datasets ‣ Deep Learning for Time Series Classification
    and Extrinsic Regression: A Current Survey")列出了最常用的数据集，包括参与者人数、用于收集数据的传感器、记录的活动描述以及使用每个数据集的研究参考文献。更大的数据集列表可以在（[chen2021deep,](#bib.bib5)
    ; [Ramanujam2021survey,](#bib.bib186) ）中找到。常见的活动集包括日常生活活动（ADL）或基本活动（例如步行、跑步、坐着、站立、上下楼梯）。然而，还收集了更专业事件的活动数据，如帕金森病患者的步态冻结（[Bachlin2009daphnet,](#bib.bib308)）、跌倒（[Micucci2017unimib,](#bib.bib309)）和制造活动（[Zappi2008skoda,](#bib.bib310)）。'
- en: '| Name | Partici- pants | Sensors | Activities/ Description | Used by |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 参与者 | 传感器 | 活动/描述 | 使用者 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| DAPHNet FoG ([Bachlin2009daphnet,](#bib.bib308) ) | 10 | 3 accelerometers
    attached to the body | Detection of freezing of gait events in Parkinson’s Disease
    patients. | ([Murad2017deep,](#bib.bib205) ; [Zeng2018rnn,](#bib.bib209) ; [Hammerla2016deep,](#bib.bib198)
    ) |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '| DAPHNet FoG ([Bachlin2009daphnet,](#bib.bib308) ) | 10 | 3个加速度计附着在身体上 | 检测帕金森病患者的步态冻结事件。
    | ([Murad2017deep,](#bib.bib205) ; [Zeng2018rnn,](#bib.bib209) ; [Hammerla2016deep,](#bib.bib198)
    ) |'
- en: '| Opportunity challenge ([Chavarriaga2013opp,](#bib.bib311) ; [Roggen2010opp,](#bib.bib194)
    ) | 4 | 5 on-body IMUs each with 3 sensors | A variety of activities of daily
    living. | ([Ordonez2016deep,](#bib.bib191) ; [Wang2022deep,](#bib.bib220) ; [Rueda2018cnn,](#bib.bib207)
    ; [Zeng2014cnn,](#bib.bib199) ; [Xu2022deform,](#bib.bib221) )  ([Yao2018cnn,](#bib.bib208)
    ; [Murad2017deep,](#bib.bib205) ; [Guan2017ensemble,](#bib.bib203) ; [Yang2015cnn,](#bib.bib201)
    ; [Hammerla2016deep,](#bib.bib198) ) |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '| Opportunity challenge ([Chavarriaga2013opp,](#bib.bib311) ; [Roggen2010opp,](#bib.bib194)
    ) | 4 | 5个配备3个传感器的穿戴IMU | 各种日常生活活动。 | ([Ordonez2016deep,](#bib.bib191) ; [Wang2022deep,](#bib.bib220)
    ; [Rueda2018cnn,](#bib.bib207) ; [Zeng2014cnn,](#bib.bib199) ; [Xu2022deform,](#bib.bib221)
    )  ([Yao2018cnn,](#bib.bib208) ; [Murad2017deep,](#bib.bib205) ; [Guan2017ensemble,](#bib.bib203)
    ; [Yang2015cnn,](#bib.bib201) ; [Hammerla2016deep,](#bib.bib198) ) |'
- en: '| PAMAP2 ([Reiss2012pamap,](#bib.bib192) ; [Reiss2012pamap2,](#bib.bib312)
    ) | 9 | 3 IMUs and heart rate monitor | 18 basic and daily living activities.
    | ([Wang2022deep,](#bib.bib220) ; [Rueda2018cnn,](#bib.bib207) ; [Zeng2018rnn,](#bib.bib209)
    ; [Guan2017ensemble,](#bib.bib203) ) ([Challa2021multi,](#bib.bib213) ; [Chen2021har,](#bib.bib215)
    ; [Ma2019attnsense,](#bib.bib210) ; [Hammerla2016deep,](#bib.bib198) ) |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| PAMAP2 ([Reiss2012pamap,](#bib.bib192) ; [Reiss2012pamap2,](#bib.bib312)
    ) | 9 | 3个IMU和心率监测器 | 18种基础和日常生活活动。 | ([Wang2022deep,](#bib.bib220) ; [Rueda2018cnn,](#bib.bib207)
    ; [Zeng2018rnn,](#bib.bib209) ; [Guan2017ensemble,](#bib.bib203) ) ([Challa2021multi,](#bib.bib213)
    ; [Chen2021har,](#bib.bib215) ; [Ma2019attnsense,](#bib.bib210) ; [Hammerla2016deep,](#bib.bib198)
    ) |'
- en: '| Skoda ([Zappi2008skoda,](#bib.bib310) ) | 1 | 19 body-worn sensors | 10 quality
    assurance activities from car production plant. | ([Ordonez2016deep,](#bib.bib191)
    ; [Zeng2014cnn,](#bib.bib199) ; [Murad2017deep,](#bib.bib205) ) ([Zeng2018rnn,](#bib.bib209)
    ; [Guan2017ensemble,](#bib.bib203) ; [Ma2019attnsense,](#bib.bib210) ) |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| Skoda ([Zappi2008skoda,](#bib.bib310) ) | 1 | 19个佩戴式传感器 | 10种汽车生产厂的质量保证活动。
    | ([Ordonez2016deep,](#bib.bib191) ; [Zeng2014cnn,](#bib.bib199) ; [Murad2017deep,](#bib.bib205)
    ) ([Zeng2018rnn,](#bib.bib209) ; [Guan2017ensemble,](#bib.bib203) ; [Ma2019attnsense,](#bib.bib210)
    ) |'
- en: '| UCI-HAR ([Anguita2013ucihar,](#bib.bib313) ) | 30 | Smartphone accelerometer
    and gyroscope | 6 basic activities. | ([Wang2022deep,](#bib.bib220) ; [Ronao2016har,](#bib.bib202)
    ; [Ignatov2018realtime,](#bib.bib206) ; [Jiang2015cnn,](#bib.bib200) ; [Murad2017deep,](#bib.bib205)
    ) ([Challa2021multi,](#bib.bib213) ; [Nafea2021sensor,](#bib.bib218) ; [Mekruksavanich2021lstm,](#bib.bib216)
    ; [Mekruksavanich2021biometric,](#bib.bib217) ) |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| UCI-HAR ([Anguita2013ucihar,](#bib.bib313) ) | 30 | 智能手机加速度计和陀螺仪 | 6种基础活动。
    | ([Wang2022deep,](#bib.bib220) ; [Ronao2016har,](#bib.bib202) ; [Ignatov2018realtime,](#bib.bib206)
    ; [Jiang2015cnn,](#bib.bib200) ; [Murad2017deep,](#bib.bib205) ) ([Challa2021multi,](#bib.bib213)
    ; [Nafea2021sensor,](#bib.bib218) ; [Mekruksavanich2021lstm,](#bib.bib216) ; [Mekruksavanich2021biometric,](#bib.bib217)
    ) |'
- en: '| UniMiB SHAR ([Micucci2017unimib,](#bib.bib309) ) | 30 | Smartphone accelerometers
    | Activities of daily living and falls. | ([Wang2022deep,](#bib.bib220) ; [Xu2022deform,](#bib.bib221)
    ) |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '| UniMiB SHAR ([Micucci2017unimib,](#bib.bib309) ) | 30 | 智能手机加速度计 | 日常生活活动和跌倒。
    | ([Wang2022deep,](#bib.bib220) ; [Xu2022deform,](#bib.bib221) ) |'
- en: '| USC-HAD ([Zhang2012uschad,](#bib.bib193) ) | 14 | Motion node (accelerometer,
    gyroscope, and magnetometer) attached to hip | 12 basic activities. | ([Jiang2015cnn,](#bib.bib200)
    ; [Xu2022deform,](#bib.bib221) ; [Murad2017deep,](#bib.bib205) ; [Singh2021convlstm,](#bib.bib219)
    ; [Mekruksavanich2021biometric,](#bib.bib217) ) |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '| USC-HAD ([Zhang2012uschad,](#bib.bib193) ) | 14 | 运动节点（加速度计、陀螺仪和磁力计）附着在臀部
    | 12种基础活动。 | ([Jiang2015cnn,](#bib.bib200) ; [Xu2022deform,](#bib.bib221) ; [Murad2017deep,](#bib.bib205)
    ; [Singh2021convlstm,](#bib.bib219) ; [Mekruksavanich2021biometric,](#bib.bib217)
    ) |'
- en: '| WISDM ([Kwapisz2011wisdm,](#bib.bib314) ) | 29 | Smartphone accelerometer
    | 6 basic activities. | ([Wang2022deep,](#bib.bib220) ; [Ignatov2018realtime,](#bib.bib206)
    ; [Zhang2020novel,](#bib.bib212) ; [Xu2022deform,](#bib.bib221) ) ([Singh2021convlstm,](#bib.bib219)
    ; [Challa2021multi,](#bib.bib213) ; [Nafea2021sensor,](#bib.bib218) ) |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '| WISDM ([Kwapisz2011wisdm,](#bib.bib314) ) | 29 | 智能手机加速度计 | 6种基础活动。 | ([Wang2022deep,](#bib.bib220)
    ; [Ignatov2018realtime,](#bib.bib206) ; [Zhang2020novel,](#bib.bib212) ; [Xu2022deform,](#bib.bib221)
    ) ([Singh2021convlstm,](#bib.bib219) ; [Challa2021multi,](#bib.bib213) ; [Nafea2021sensor,](#bib.bib218)
    ) |'
- en: Table 7\. Commonly used human activity recognition datasets
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 常用的人体活动识别数据集
- en: C.2\. Earth observation satellites and instruments
  id: totrans-763
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2\. 地球观测卫星和仪器
- en: 'Table [8](#A3.T8 "Table 8 ‣ C.2\. Earth observation satellites and instruments
    ‣ Appendix C Datasets ‣ Deep Learning for Time Series Classification and Extrinsic
    Regression: A Current Survey") lists the main satellites and instruments used
    in the studies reviewed for this survey. The table lists references for each source,
    which provide more details about the data collected, plus a list of the studies
    using each source.'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [8](#A3.T8 "Table 8 ‣ C.2\. Earth observation satellites and instruments
    ‣ Appendix C Datasets ‣ Deep Learning for Time Series Classification and Extrinsic
    Regression: A Current Survey") 列出了用于本次调查中回顾研究的主要卫星和仪器。表格列出了每个来源的参考文献，提供了有关收集的数据的更多细节，以及使用每个来源的研究列表。'
- en: Agency Satellite & Instruments Type Used by NASA Landsat-7 ETM+ ([USGALandsat,](#bib.bib315)
    ) Optical ([Xu2020dcm,](#bib.bib246) ; [Dou2021tsi,](#bib.bib254) ) NASA Landsat-8
    OLI ([USGALandsat,](#bib.bib315) ) Optical ([Xu2020dcm,](#bib.bib246) ; [Dou2021tsi,](#bib.bib254)
    ; [Li2019tan,](#bib.bib243) ; [Li2020tga,](#bib.bib244) ; [Kussul2017cnn,](#bib.bib252)
    ),
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 机构 卫星 & 仪器 类型 NASA Landsat-7 ETM+ ([USGALandsat,](#bib.bib315) ) 光学 ([Xu2020dcm,](#bib.bib246)
    ; [Dou2021tsi,](#bib.bib254) ) NASA Landsat-8 OLI ([USGALandsat,](#bib.bib315)
    ) 光学 ([Xu2020dcm,](#bib.bib246) ; [Dou2021tsi,](#bib.bib254) ; [Li2019tan,](#bib.bib243)
    ; [Li2020tga,](#bib.bib244) ; [Kussul2017cnn,](#bib.bib252) ),
- en: ([DiMauro2017tiselc,](#bib.bib251) ; [Matosak2022forest,](#bib.bib234) ; [Rao2020lfmc,](#bib.bib237)
    ; [Xie2022lfmc,](#bib.bib240) ) NASA Terra/Aqua MODIS ([NASAMODIS,](#bib.bib316)
    ) Optical ([Xie2022lfmc,](#bib.bib240) ; [Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239)
    ; [Sun2020yield,](#bib.bib242) ; [Qiao2021yield,](#bib.bib263) ) ESA Sentinel-1A/1B
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: ([DiMauro2017tiselc,](#bib.bib251) ; [Matosak2022forest,](#bib.bib234) ; [Rao2020lfmc,](#bib.bib237)
    ; [Xie2022lfmc,](#bib.bib240) ) NASA Terra/Aqua MODIS ([NASAMODIS,](#bib.bib316)
    ) 光学 ([Xie2022lfmc,](#bib.bib240) ; [Zhu2020lfmc,](#bib.bib238) ; [Miller2022lfmc,](#bib.bib239)
    ; [Sun2020yield,](#bib.bib242) ; [Qiao2021yield,](#bib.bib263) ) ESA Sentinel-1A/1B
- en: SAR-C ([ESA2019Sentinel,](#bib.bib317) ) Microwave
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: SAR-C ([ESA2019Sentinel,](#bib.bib317) ) 微波
- en: SAR ([Kussul2017cnn,](#bib.bib252) ; [Rao2020lfmc,](#bib.bib237) ; [Xie2022lfmc,](#bib.bib240)
    ; [Ofori-Ampofo2021att,](#bib.bib249) ; [Ienco2019twinns,](#bib.bib255) ; [Gbodjo2020hob2srnn,](#bib.bib260)
    ),
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: SAR ([Kussul2017cnn,](#bib.bib252) ; [Rao2020lfmc,](#bib.bib237) ; [Xie2022lfmc,](#bib.bib240)
    ; [Ofori-Ampofo2021att,](#bib.bib249) ; [Ienco2019twinns,](#bib.bib255) ; [Gbodjo2020hob2srnn,](#bib.bib260)
    ),
- en: ([Ienco2019od2rnn,](#bib.bib261) ; [Ban2020fire,](#bib.bib231) ; [Rambour2020flood,](#bib.bib232)
    ; [Kulshrestha2022hole,](#bib.bib230) ; [Minh2017vege,](#bib.bib235) ) ESA Sentinel-2A/2B
    MSI ([ESA2019Sentinel,](#bib.bib317) ) Optical ([Li2019tan,](#bib.bib243) ; [Li2020tga,](#bib.bib244)
    ; [Matosak2022forest,](#bib.bib234) ; [Ofori-Ampofo2021att,](#bib.bib249) ; [Ienco2019twinns,](#bib.bib255)
    ; [Ienco2019od2rnn,](#bib.bib261) ; [Rambour2020flood,](#bib.bib232) ),
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: ([Ienco2019od2rnn,](#bib.bib261) ; [Ban2020fire,](#bib.bib231) ; [Rambour2020flood,](#bib.bib232)
    ; [Kulshrestha2022hole,](#bib.bib230) ; [Minh2017vege,](#bib.bib235) ) ESA Sentinel-2A/2B
    MSI ([ESA2019Sentinel,](#bib.bib317) ) 光学 ([Li2019tan,](#bib.bib243) ; [Li2020tga,](#bib.bib244)
    ; [Matosak2022forest,](#bib.bib234) ; [Ofori-Ampofo2021att,](#bib.bib249) ; [Ienco2019twinns,](#bib.bib255)
    ; [Ienco2019od2rnn,](#bib.bib261) ; [Rambour2020flood,](#bib.bib232) ),
- en: ([Garnot2019time,](#bib.bib266) ; [Garnot2020tae,](#bib.bib229) ; [Garnot2020ltae,](#bib.bib248)
    ; [Yuan2021sitsbert,](#bib.bib250) ; [Ienco2020tassel,](#bib.bib228) ; [Interdonato2019duplo,](#bib.bib256)
    ; [Barriere2022lstm,](#bib.bib247) ), ([Russwurm2018seqrnn,](#bib.bib257) ; [Russwurm2020att,](#bib.bib264)
    ; [Yuan2022sitsformer,](#bib.bib262) ; [Stoian2019fgunet,](#bib.bib258) ; [Labenski2022under,](#bib.bib236)
    ; [KamdemDeTeyou2020road,](#bib.bib233) ; [Lahssini2022wood,](#bib.bib241) ) CNES
    (France) Pléiades-1A/1B HiRI ([ESAPleiades,](#bib.bib318) ) Optical ([Ienco2017rnn,](#bib.bib259)
    ) NSPO (Taiwan) Formosat-2 RSI ([NSPO2020Formosat,](#bib.bib319) ) Optical ([Pelletier2019tempcnn,](#bib.bib253)
    ) CRESDA (China) Gaofen-1/2 MUX, PAN,
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: ([Garnot2019time,](#bib.bib266) ; [Garnot2020tae,](#bib.bib229) ; [Garnot2020ltae,](#bib.bib248)
    ; [Yuan2021sitsbert,](#bib.bib250) ; [Ienco2020tassel,](#bib.bib228) ; [Interdonato2019duplo,](#bib.bib256)
    ; [Barriere2022lstm,](#bib.bib247) ), ([Russwurm2018seqrnn,](#bib.bib257) ; [Russwurm2020att,](#bib.bib264)
    ; [Yuan2022sitsformer,](#bib.bib262) ; [Stoian2019fgunet,](#bib.bib258) ; [Labenski2022under,](#bib.bib236)
    ; [KamdemDeTeyou2020road,](#bib.bib233) ; [Lahssini2022wood,](#bib.bib241) ) CNES
    (法国) Pléiades-1A/1B HiRI ([ESAPleiades,](#bib.bib318) ) 光学 ([Ienco2017rnn,](#bib.bib259)
    ) NSPO (台湾) Formosat-2 RSI ([NSPO2020Formosat,](#bib.bib319) ) 光学 ([Pelletier2019tempcnn,](#bib.bib253)
    ) CRESDA (中国) Gaofen-1/2 MUX, PAN,
- en: WFV ([EoPortal2014Gaofen1,](#bib.bib320) ; [EoPortal2015Gaofen2,](#bib.bib321)
    ) Optical  ([Ji20183dcnn,](#bib.bib245) )
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: WFV ([EoPortal2014Gaofen1,](#bib.bib320) ; [EoPortal2015Gaofen2,](#bib.bib321)
    ) 光学 ([Ji20183dcnn,](#bib.bib245) )
- en: Table 8\. Earth Observation satellites and instruments collecting the data used
    in the studies reviewed.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8\. 地球观测卫星和仪器，用于收集研究中使用的数据。
