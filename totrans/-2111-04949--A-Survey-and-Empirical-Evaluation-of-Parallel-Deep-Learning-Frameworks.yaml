- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:49:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:49:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2111.04949] A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2111.04949] 并行深度学习框架的综述与实证评估'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.04949](https://ar5iv.labs.arxiv.org/html/2111.04949)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2111.04949](https://ar5iv.labs.arxiv.org/html/2111.04949)
- en: A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行深度学习框架的综述与实证评估
- en: Daniel Nichols, Siddharth Singh, Shu-Huai Lin, Abhinav Bhatele ^†Department
    of Computer Science, University of MarylandCollege ParkUSA [dnicho, ssingh37,
    slin185@umd.edu, bhatele@cs.umd.edu](mailto:dnicho,%20ssingh37,%20slin185@umd.edu,%20bhatele@cs.umd.edu)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Daniel Nichols, Siddharth Singh, Shu-Huai Lin, Abhinav Bhatele ^†计算机科学系，马里兰大学帕克分校，美国
    [dnicho, ssingh37, slin185@umd.edu, bhatele@cs.umd.edu](mailto:dnicho,%20ssingh37,%20slin185@umd.edu,%20bhatele@cs.umd.edu)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: The field of deep learning has witnessed a remarkable shift towards extremely
    compute- and memory-intensive neural networks. These newer larger models have
    enabled researchers to advance state-of-the-art tools across a variety of fields.
    This phenomenon has spurred the development of algorithms for distributed training
    of neural networks over a larger number of hardware accelerators. In this paper,
    we discuss and compare current state-of-the-art frameworks for large scale distributed
    deep learning. First, we survey current practices in distributed learning and
    identify the different types of parallelism used. Then, we present empirical results
    comparing their performance on large image and language training tasks. Additionally,
    we address their statistical efficiency and memory consumption behavior. Based
    on our results, we discuss algorithmic and implementation portions of each framework
    which hinder performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域经历了向极其计算和内存密集型神经网络的显著转变。这些更新的大型模型使研究人员能够在各种领域推进最先进的工具。这一现象催生了针对在更多硬件加速器上进行分布式训练的神经网络算法的发展。在本文中，我们讨论并比较了当前最先进的大规模分布式深度学习框架。首先，我们回顾了当前的分布式学习实践，并识别了不同类型的并行性。然后，我们展示了在大规模图像和语言训练任务上比较它们性能的实证结果。此外，我们还讨论了它们的统计效率和内存消耗行为。基于我们的结果，我们讨论了每个框架的算法和实现部分，这些部分制约了性能。
- en: 'neural networks, deep learning, distributed training, GPUs, performance, survey^†^†conference:
    ; ;'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '神经网络，深度学习，分布式训练，GPU，性能，综述^†^†会议: ; ;'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'The previous decade witnessed an explosion in the development of machine learning
    algorithms. In particular, deep learning (DL), a subset of machine learning focused
    on using neural networks for function approximation, has gained widespread popularity.
    Deep neural networks (DNNs) have enabled the advancement of the state of the art
    in a plethora of research areas: ranging from visual recognition (Kolesnikov et al.,
    [2020](#bib.bib29); Simonyan and Zisserman, [2015](#bib.bib56); Tao et al., [2020](#bib.bib62);
    Zhao et al., [2019](#bib.bib73); Vijayanarasimhan et al., [2017](#bib.bib65))
    and natural language processing (Devlin et al., [2019](#bib.bib14); Radford et al.,
    [2019](#bib.bib46); Wu et al., [2016](#bib.bib67); Nikolentzos et al., [2020](#bib.bib41))
    to computational chemistry and computer systems (Jain et al., [2013](#bib.bib22);
    Bhatele et al., [2015](#bib.bib5); Islam et al., [2016](#bib.bib20); Yeom et al.,
    [2016](#bib.bib68); Marathe et al., [2017](#bib.bib35); Thiagarajan et al., [2018b](#bib.bib64),
    [a](#bib.bib63); Menon et al., [2020](#bib.bib37)). Their popularity stems from
    the DNN’s ability to automatically learn low-dimensional representations from
    high-dimensional unstructured data such as images, text and audio. Given enough
    data, the representations learned by these models are often superior to handcrafted
    features designed by domain experts.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的十年见证了机器学习算法的爆炸式发展。特别是深度学习（DL），作为机器学习的一个子集，专注于利用神经网络进行函数逼近，已经广泛流行起来。深度神经网络（DNNs）在许多研究领域推动了最新技术的进步：从视觉识别（Kolesnikov等人，[2020](#bib.bib29)；
    Simonyan and Zisserman，[2015](#bib.bib56)； Tao等人，[2020](#bib.bib62)； Zhao等人，[2019](#bib.bib73)；
    Vijayanarasimhan等人，[2017](#bib.bib65)）和自然语言处理（Devlin等人，[2019](#bib.bib14)； Radford等人，[2019](#bib.bib46)；
    Wu等人，[2016](#bib.bib67)； Nikolentzos等人，[2020](#bib.bib41)）到计算化学和计算机系统（Jain等人，[2013](#bib.bib22)；
    Bhatele等人，[2015](#bib.bib5)； Islam等人，[2016](#bib.bib20)； Yeom等人，[2016](#bib.bib68)；
    Marathe等人，[2017](#bib.bib35)； Thiagarajan等人，[2018b](#bib.bib64), [a](#bib.bib63)；
    Menon等人，[2020](#bib.bib37)）。它们的流行源于DNN的能力，可以从高维度的非结构化数据（如图像、文本和音频）中自动学习低维度的表示。在有足够的数据的情况下，这些模型学到的表示通常比领域专家设计的手工特征更优秀。
- en: 'The advances in accelerator technology, increased memory capacity per accelerator,
    and faster networks have encouraged users of deep learning to train neural networks
    with increasingly larger numbers of parameters. Figure [1](#S1.F1 "Figure 1 ‣
    1\. Introduction ‣ A Survey and Empirical Evaluation of Parallel Deep Learning
    Frameworks") shows the increasing number of parameters in the largest networks
    since 2012\. Often times, it is impossible to train such networks on a single
    accelerator either due to large execution time or insufficient memory capacity
    to fit these models. The latter problem is further exacerbated for contemporary
    neural architectures. For example, GPT-2, an extremely popular neural network
    used in NLP requires 84 GB of GPU DRAM for training. This has motivated recent
    works in parallelizing the task of deep learning: training large models using
    multiple GPUs on a single node (Huang et al., [2019](#bib.bib19); Kim et al.,
    [2020](#bib.bib26)) or across multiple nodes connected by a network (Rajbhandari
    et al., [2020](#bib.bib47); Shoeybi et al., [2020](#bib.bib55); Li et al., [2020](#bib.bib32);
    Narayanan et al., [2019](#bib.bib40); You et al., [2018](#bib.bib71); Jia et al.,
    [2019](#bib.bib23); Dryden et al., [2019a](#bib.bib15)).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器技术的进步，每个加速器的内存容量的增加，以及更快的网络，鼓励深度学习的用户训练具有越来越大参数数量的神经网络。图[1](#S1.F1 "Figure
    1 ‣ 1\. Introduction ‣ A Survey and Empirical Evaluation of Parallel Deep Learning
    Frameworks")显示自2012年以来最大网络参数的增加。往往情况下，由于执行时间长或内存容量不足以容纳这些模型，单个加速器无法训练这样的网络。对于当代的神经架构，这一问题进一步加剧。例如，GPT-2是一种在自然语言处理中广泛使用的神经网络，需要84GB的GPU
    DRAM进行训练。这促使最近的工作在并行化深度学习任务方面：在单个节点上使用多个GPU训练大型模型（Huang等人，[2019](#bib.bib19)；
    Kim等人，[2020](#bib.bib26)），或者跨越通过网络连接的多个节点进行训练（Rajbhandari等人，[2020](#bib.bib47)；
    Shoeybi等人，[2020](#bib.bib55)； Li等人，[2020](#bib.bib32)； Narayanan等人，[2019](#bib.bib40)；
    You等人，[2018](#bib.bib71)； Jia等人，[2019](#bib.bib23)； Dryden等人，[2019a](#bib.bib15))。
- en: '![Refer to caption](img/b7791e6214b64e8a68f4ea70c5078d21.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b7791e6214b64e8a68f4ea70c5078d21.png)'
- en: Figure 1\. Neural networks have continued to grow in size in terms of the number
    of parameters. Recent language networks have further contributed to this trend.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 神经网络的参数数量继续增长。最近的语言网络进一步推动了这一趋势。
- en: Different parallel frameworks offer different strengths and weaknesses in terms
    of performance (execution time for training), memory consumption, and statistical
    efficiency. Ben-Nun et al. (Ben-Nun and Hoefler, [2019](#bib.bib4)) surveyed parallel
    DL frameworks and the different ways of exploiting the concurrency in neural networks
    in 2018\. However, many new frameworks have emerged in the last three years, and
    the authors limited their discussion to a qualitative analysis. In this paper,
    we survey the most popular parallel DL frameworks available today and perform
    an empirical evaluation for the ones with open-source implementations to compare
    various metrics. This comparative evaluation can help users of deep learning select
    the best parallel framework for their training tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的并行框架在性能（训练执行时间）、内存消耗和统计效率方面具有不同的优缺点。Ben-Nun等人（Ben-Nun和Hoefler，[2019](#bib.bib4)）在2018年调查了并行深度学习框架以及在神经网络中利用并发的不同方法。然而，过去三年中出现了许多新框架，作者将讨论限定为定性分析。本文调查了目前最流行的并行深度学习框架，并对具有开源实现的框架进行了实证评估，以比较各种指标。这种比较评估可以帮助深度学习用户选择最适合其训练任务的并行框架。
- en: 'We first present a comprehensive qualitative survey of the state of the art
    in parallel deep learning. We classify approaches for parallelization into three
    categories (defined in Section [2](#S2 "2\. Background ‣ A Survey and Empirical
    Evaluation of Parallel Deep Learning Frameworks")): data parallelism, intra-layer
    parallelism (sometimes referred to as model parallelism), and inter-layer parallelism
    (sometimes referred to as pipelining,). We present the advantages and disadvantages
    of using each approach and discuss the capabilities of different frameworks that
    implement each type of parallelism.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提供了关于并行深度学习的现状的全面定性调查。我们将并行化的方法分类为三种类型（在第[2](#S2 "2\. Background ‣ A Survey
    and Empirical Evaluation of Parallel Deep Learning Frameworks")节中定义）：数据并行、层内并行（有时称为模型并行）和层间并行（有时称为流水线）。我们介绍了每种方法的优缺点，并讨论了实现每种类型并行的不同框架的能力。
- en: An end user who needs a scalable DL framework for their training experiments
    needs to know which frameworks provide the best statistical efficiency in the
    shortest possible time. To the best of our knowledge, an empirical comparison
    of parallel DL frameworks has not been attempted before. We identify two popular
    training datasets and two neural networks to benchmark several open-source DL
    frameworks including DDP (Li et al., [2020](#bib.bib32)), PipeDream (Narayanan
    et al., [2019](#bib.bib40)), ZeRO (Rajbhandari et al., [2020](#bib.bib47)), Megatron (Shoeybi
    et al., [2020](#bib.bib55)), TorchGPipe (Kim et al., [2020](#bib.bib26)), and
    LBANN (Essen et al., [2015](#bib.bib17)). We use metrics that matter the most
    to a deep learning researcher – epoch execution times, statistical efficiency,
    and memory consumption. We run our experiments on two different supercomputers
    and clusters that are built using different generations of NVIDIA GPUs (A100s,
    V100s). Through these experiments, we seek to develop a consensus on the suitability
    of parallel frameworks to different scenarios.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 需要可扩展深度学习框架来进行训练实验的最终用户需要知道哪些框架在最短时间内提供最佳的统计效率。据我们了解，以前没有进行过并行深度学习框架的实证比较。我们选择了两个流行的训练数据集和两个神经网络，对多个开源深度学习框架（包括DDP（Li等人，[2020](#bib.bib32)），PipeDream（Narayanan等人，[2019](#bib.bib40)），ZeRO（Rajbhandari等人，[2020](#bib.bib47)），Megatron（Shoeybi等人，[2020](#bib.bib55)），TorchGPipe（Kim等人，[2020](#bib.bib26)），和LBANN（Essen等人，[2015](#bib.bib17)））进行基准测试。我们使用对深度学习研究人员最重要的指标——训练周期执行时间、统计效率和内存消耗。在两台不同的超级计算机和集群上运行实验，这些计算机和集群使用不同代的NVIDIA
    GPU（A100s、V100s）。通过这些实验，我们力求达成对并行框架在不同场景中适用性的共识。
- en: 'In this paper we contribute:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的贡献包括：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A comprehensive survey of current state-of-the art techniques in distributed
    deep learning organized by parallelization strategy.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对当前分布式深度学习前沿技术的全面调查，按并行化策略组织。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An empirical evaluation of these techniques across vision and language tasks
    on 2 different clusters that, to our knowledge, has not been done before.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对这些技术在视觉和语言任务上的实证评估，涉及2个不同的集群，据我们所知，这在之前没有进行过。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A comparison of metrics, recorded across frameworks and architectures, that
    concern both the HPC and deep learning communities: runtime, scaling, statistical
    efficiency, and memory consumption.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对涉及高性能计算（HPC）和深度学习社区的指标进行比较，这些指标包括：运行时间、扩展性、统计效率和内存消耗。
- en: 2\. Background
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: 'In this section, we first give brief descriptions of deep learning terminology.
    We refer the reader to (Goodfellow-et-al-2016) for an in-depth review of deep
    learning. We then provide an outline of the three ways in which training of a
    deep neural network can be parallelized: data parallelism, intra-layer parallelism
    and inter-layer parallelism.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先简要描述深度学习术语。有关深度学习的深入评审，请参阅 (Goodfellow-et-al-2016)。然后，我们概述了深度神经网络训练可以并行化的三种方式：数据并行性、层内并行性和层间并行性。
- en: 2.1\. Definitions
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 定义
- en: 'Neural networks: Neural networks are parameterized functions for predicting
    properties of some input data. They excel at learning low dimensional representations
    of complex, high dimensional data.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络：神经网络是用于预测某些输入数据属性的参数化函数。它们擅长学习复杂、高维数据的低维表示。
- en: 'Layers: Networks are composed of a sequence of layers, which take the previous
    layer’s output as input and computes some non-linear transformation.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 层：网络由一系列层组成，这些层将前一层的输出作为输入，并计算某种非线性变换。
- en: 'Training and Loss: The processing of finding the best parameters for a neural
    network is called training. This is done by minimizing a loss function over an
    input data set. Loss functions, such as mean squared error, are typically chosen
    to represent the prediction capability of the network.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和损失：寻找神经网络最佳参数的过程称为训练。这是通过在输入数据集上最小化损失函数来完成的。损失函数，如均方误差，通常选择用于表示网络的预测能力。
- en: 'Backpropagation: Backpropagation is a dynamic programming algorithm based on
    reverse-mode automatic differentiation that computes the gradients of each layer
    with respect to the loss function.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播：反向传播是一种基于反向模式自动微分的动态规划算法，它计算每一层相对于损失函数的梯度。
- en: 'Gradient Descent and Learning Rate: Many training algorithms use variations
    of gradient descent to minimize the loss function. Gradient descent iteratively
    updates the parameters of the neural network based on the negative gradient such
    that the loss moves towards a minima. The distance moved in the direction of the
    negative gradient is scaled by a value called the learning rate.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降和学习率：许多训练算法使用梯度下降的变种来最小化损失函数。梯度下降通过负梯度迭代更新神经网络的参数，使得损失趋向于最小值。负梯度方向上的移动距离被一个称为学习率的值进行缩放。
- en: 'Mini-Batches, Epochs and Stochastic Gradient Descent: Computing gradients of
    the entire data set is expensive, so approximate gradients are computed using
    random mini-batches of data. This version of gradient descent is called batched
    stochastic gradient descent. Each time the entirety of the data set is iterated
    over is called an epoch.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量、迭代和随机梯度下降：计算整个数据集的梯度是昂贵的，因此使用随机小批量数据来计算近似梯度。这种版本的梯度下降被称为批量随机梯度下降。每次迭代整个数据集称为一个迭代。
- en: 'Statistical Efficiency: Statistical efficiency is a measure of the relationship
    between epochs and accuracy/loss. A training algorithm is said to be statistically
    efficient if it requires a low number of epochs to converge to a target validation
    loss.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 统计效率：统计效率是衡量迭代次数与准确率/损失之间关系的指标。如果一个训练算法需要较少的迭代次数即可收敛到目标验证损失，则称其为统计上有效的。
- en: 2.2\. Parallel Deep Learning Methods
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 并行深度学习方法
- en: 'Data Parallelism: Data parallelism refers to an even division of training data
    among worker GPUs. Each GPU possesses a copy of the neural network along with
    it’s parameters. Gradient calculation via backpropagation proceeds independently
    on all GPUs. These gradients are then subject to a collective all-reduce operation
    before the weight update step of the optimizer. The all-reduce step can either
    take place synchronously after each mini-batch, or asynchronously using a central
    parameter server. Implementations of data parallelism are widely available in
    popular deep learning frameworks like PyTorch (Li et al., [2020](#bib.bib32)),
    and TensorFlow (Abadi et al., [2016](#bib.bib2)). Figure [2](#S2.F2 "Figure 2
    ‣ 2.2\. Parallel Deep Learning Methods ‣ 2\. Background ‣ A Survey and Empirical
    Evaluation of Parallel Deep Learning Frameworks") illustrates data parallelism
    across 4 GPUs.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性：数据并行性指的是将训练数据均匀分配到工作GPU之间。每个GPU拥有神经网络及其参数的副本。通过反向传播进行的梯度计算在所有GPU上独立进行。这些梯度在优化器的权重更新步骤之前，经过一次集体全规约操作。全规约步骤可以在每个迷你批次后同步进行，或者使用中心参数服务器异步进行。数据并行性的实现广泛存在于流行的深度学习框架中，如PyTorch（Li等，[2020](#bib.bib32)）和TensorFlow（Abadi等，[2016](#bib.bib2)）。图[2](#S2.F2
    "图 2 ‣ 2.2\. 并行深度学习方法 ‣ 2\. 背景 ‣ 并行深度学习框架的调查与实证评估")展示了4个GPU上的数据并行性。
- en: '![Refer to caption](img/dd22fe7061e9f6b07a44b6c448ef1d53.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd22fe7061e9f6b07a44b6c448ef1d53.png)'
- en: Figure 2\. Processing of mini-batches over time in data parallelism. Each GPU
    has a copy of all the layers (shown in different colors) and different mini-batches
    (numbered) are processed by different GPUs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 数据并行性中迷你批次随时间的处理。每个GPU拥有所有层的副本（以不同颜色显示），不同的迷你批次（编号）由不同的GPU处理。
- en: 'Intra-layer Parallelism: Intra-layer parallelism distributes the work of a
    layer by dividing its computation across multiple GPUs. Parallelizing an entire
    neural network entails applying intra-layer parallelism to some or all of its
    constituent layers. Research in this area is focused on optimizing the multi-GPU
    execution of different kinds of layers - Fully Connected, Convolutional (Coates
    et al., [2013](#bib.bib12); Oyama et al., [2020](#bib.bib42); Shazeer et al.,
    [2018](#bib.bib54)) and more recently the Transformer (Shoeybi et al., [2020](#bib.bib55)).
    Intra-layer parallelism enables us to train neural networks that would not fit
    inside the DRAM of a single GPU.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 层内并行性：层内并行性通过将一个层的计算任务分配到多个GPU上来分散工作的负担。对整个神经网络进行并行化涉及对其部分或所有组成层应用层内并行性。该领域的研究集中于优化不同类型层的多GPU执行——全连接层、卷积层（Coates等，[2013](#bib.bib12);
    Oyama等，[2020](#bib.bib42); Shazeer等，[2018](#bib.bib54)）以及最近的Transformer（Shoeybi等，[2020](#bib.bib55)）。层内并行性使我们能够训练那些无法容纳在单个GPU的DRAM中的神经网络。
- en: 'Inter-layer Parallelism: In inter-layer parallelism contiguous subsets of layers
    are mapped to individual GPUs. Each GPU is thus tasked with operating on a subset
    of the neural network. Exchange of activations and gradients among consecutive
    layers on different GPUs takes place via point-to-point communication primitives.
    To achieve true parallelism more than one mini-batch should be active on different
    GPUs at a time since the processing of a mini-batch across layers is sequential
    and cannot be parallelized. This is called pipelining. The maximum number of mini-batches
    active in the system at any given point of time is called the pipeline limit.
    Figure [3](#S2.F3 "Figure 3 ‣ 2.2\. Parallel Deep Learning Methods ‣ 2\. Background
    ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks") shows
    inter-layer parallelism in action with four GPUs and a pipeline limit of four.
    Just like intra-layer parallelism inter-layer parallelism makes it possible to
    train models whose memory requirements exceed the DRAM capacity of a single GPU.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 层间并行性：在层间并行性中，连续的层子集被映射到各个GPU上。每个GPU被分配到神经网络的一个子集进行操作。不同GPU上连续层之间的激活值和梯度的交换通过点对点通信原语进行。为了实现真正的并行性，必须在不同的GPU上同时激活多个迷你批次，因为迷你批次在层间的处理是顺序的，不能并行化。这被称为流水线技术。系统中任何时刻活跃的迷你批次的最大数量被称为流水线限制。图[3](#S2.F3
    "图 3 ‣ 2.2\. 并行深度学习方法 ‣ 2\. 背景 ‣ 并行深度学习框架的调查与实证评估")展示了四个GPU和四个流水线限制下的层间并行性。与层内并行性一样，层间并行性也使得训练内存需求超过单个GPU的DRAM容量的模型成为可能。
- en: '![Refer to caption](img/4a5906b9806007851b049c0db5038b42.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4a5906b9806007851b049c0db5038b42.png)'
- en: Figure 3\. Processing of micro-batches in inter-layer parallelism. Each GPU
    holds one or more layers in the network and all mini-batches pass through all
    the layers/GPUs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 层间并行中的微批处理处理。每个 GPU 持有网络中的一层或多层，所有小批次通过所有层/GPUs。
- en: 2.3\. Related Work
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 相关工作
- en: Pouyanfar et al. (Pouyanfar et al., [2018](#bib.bib44)) and Ben-Nun et al. (Ben-Nun
    and Hoefler, [2019](#bib.bib4)) comprehensively survey established techniques
    in sequential deep learning as well as distributed. Another survey (Sze et al.,
    [2017](#bib.bib60)) covers work in processing neural networks efficiently. Distributed
    training on big data software stacks (such as Spark and Hadoop) is explored by
    Lu et al. (Lu et al., [2018](#bib.bib33)). The network demands of parallel training
    are presented in (Awan et al., [2020](#bib.bib3)) where typical communication
    workloads are profiled and characterized. Tang et al. (Tang et al., [2020](#bib.bib61))
    further character distributed training communication via analytical models and
    survey current practices. We also point the reader to the MLPerf benchmarks¹¹1[https://mlcommons.org/en/training-normal-07/](https://mlcommons.org/en/training-normal-07/),
    which have become popular for comparing deep learning algorithms, frameworks,
    and hardware.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Pouyanfar 等人（Pouyanfar et al., [2018](#bib.bib44)）和 Ben-Nun 等人（Ben-Nun and Hoefler,
    [2019](#bib.bib4)）全面综述了顺序深度学习和分布式深度学习中的成熟技术。另一项综述（Sze et al., [2017](#bib.bib60)）涵盖了高效处理神经网络的工作。Lu
    等人（Lu et al., [2018](#bib.bib33)）探讨了在大数据软件堆栈（如 Spark 和 Hadoop）上的分布式训练。Awan 等人（Awan
    et al., [2020](#bib.bib3)）展示了并行训练的网络需求，描述了典型的通信工作负载。Tang 等人（Tang et al., [2020](#bib.bib61)）通过分析模型进一步描述了分布式训练通信，并综述了当前实践。我们还向读者推荐了
    MLPerf 基准¹¹1[https://mlcommons.org/en/training-normal-07/](https://mlcommons.org/en/training-normal-07/)，它们已成为比较深度学习算法、框架和硬件的热门工具。
- en: 3\. Literature Survey
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 文献综述
- en: In this section we present a survey of current state-of-the-art techniques and
    implementations for each type of distributed learning. Table [1](#S3.T1 "Table
    1 ‣ 3\. Literature Survey ‣ A Survey and Empirical Evaluation of Parallel Deep
    Learning Frameworks") provides an overview of each discussed framework.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了当前每种分布式学习类型的最新技术和实现的综述。表[1](#S3.T1 "表 1 ‣ 3\. 文献综述 ‣ 并行深度学习框架的综述和实证评估")提供了每个讨论的框架的概述。
- en: Table 1\. Summary of Literature Review on Parallel Deep Learning
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 并行深度学习的文献综述总结
- en: '| Framework | Type of Parallelism |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 并行类型 |'
- en: '&#124; Largest &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最大 &#124;'
- en: '&#124; Accelerator Count &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加速器数量 &#124;'
- en: '|'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Largest Trained Network &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最大训练网络 &#124;'
- en: '&#124; (No. of Parameters) &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （参数数量） &#124;'
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| FlexFlow | Hybrid | 64 GPUs | 24M^∗ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| FlexFlow | Hybrid | 64 GPUs | 24M^∗ |'
- en: '| PipeDream^(∗∗) | Inter-Layer | 16 GPUs | 138M |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| PipeDream^(∗∗) | 层间 | 16 GPUs | 138M |'
- en: '| DDP^(∗∗) | Data | 256 GPUs | 345M |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| DDP^(∗∗) | 数据 | 256 GPUs | 345M |'
- en: '| GPipe | Inter-Layer | 8 GPUs | 557M |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| GPipe | 层间 | 8 GPUs | 557M |'
- en: '| MeshTensorFlow | Intra-Layer | 512-core TPUv2 | 4.9B |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| MeshTensorFlow | 层内 | 512核 TPUv2 | 4.9B |'
- en: '| Megatron^(∗∗) | Intra-Layer | 512 GPUs | 8.3B |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Megatron^(∗∗) | 层内 | 512 GPUs | 8.3B |'
- en: '| TorchGPipe^(∗∗) | Inter-Layer | 8 GPUs | 15.8B |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| TorchGPipe^(∗∗) | 层间 | 8 GPUs | 15.8B |'
- en: '| KARMA | Data | 2048 GPUs | 17B |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| KARMA | 数据 | 2048 GPUs | 17B |'
- en: '| LBANN^(∗∗) | Data | 3072 CPUs | 78.6B |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| LBANN^(∗∗) | 数据 | 3072 CPUs | 78.6B |'
- en: '| ZeRO^(∗∗) | Data | 400 GPUs | 100B |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ZeRO^(∗∗) | 数据 | 400 GPUs | 100B |'
- en: '| ZeRO-Infinity | Data | 512 GPUs | 32T |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ZeRO-Infinity | 数据 | 512 GPUs | 32T |'
- en: '| AxoNN | Inter-Layer | 384 GPUs | 100B |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| AxoNN | 层间 | 384 GPUs | 100B |'
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '^∗Note: FlexFlow does not provide a parameter size for the largest network
    it trains. We have defaulted to the largest network with a known network size
    cited in their paper.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ^∗注意：FlexFlow 未提供其训练的最大网络的参数大小。我们使用了其论文中引用的已知网络大小的最大网络。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ^(∗∗)The following frameworks are compared quantitatively in Section [4](#S4
    "4\. Experimental Setup ‣ A Survey and Empirical Evaluation of Parallel Deep Learning
    Frameworks")
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ^(∗∗)以下框架在第[4](#S4 "4\. 实验设置 ‣ 并行深度学习框架的综述和实证评估")节中进行了定量比较
- en: 3.1\. Data Parallelism
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 数据并行
- en: Data parallelism has been the go-to algorithm for parallelizing neural network
    training. It is simple in design and performs well with the correct settings.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行一直是并行化神经网络训练的首选算法。它设计简单，并在正确的设置下表现良好。
- en: 3.1.1\. Small Models
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 小型模型
- en: Data parallelism hinges on a synchronous all-reduce operation to gather the
    gradients across all GPUs. Naturally, this can become a bottleneck as the size
    of the gradients being being shared grows. This problem is further exacerbated
    by the increasing computational capabilities of hardware accelerators. The ensuing
    decrease in the computation to communication ratio increases the severity of this
    problem.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行依赖于同步的全量缩减操作，以在所有GPU之间收集梯度。自然地，随着共享梯度的大小增加，这可能成为一个瓶颈。这个问题随着硬件加速器计算能力的提升而进一步加剧。计算与通信比率的下降使得这个问题变得更加严重。
- en: Initial attempts to reduce the communication overhead targeted introducing asynchrony
    in the stochastic gradient descent (SGD) algorithm (Recht et al., [2011](#bib.bib50);
    Chilimbi et al., [2014](#bib.bib11); Dean et al., [2012](#bib.bib13)). However,
    Chen et al. (Chen et al., [2016](#bib.bib7)) demonstrate that synchronous SGD
    and its variants converged faster with higher accuracy than their asynchronous
    counterparts.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 初期减少通信开销的尝试集中于在随机梯度下降（SGD）算法中引入异步性（Recht et al., [2011](#bib.bib50); Chilimbi
    et al., [2014](#bib.bib11); Dean et al., [2012](#bib.bib13)）。然而，Chen et al.（Chen
    et al., [2016](#bib.bib7)）展示了同步SGD及其变体比异步对手收敛更快且精度更高。
- en: Efforts to minimize communication bottlenecks continued. Zhang et al. (Zhang
    et al., [2017](#bib.bib72)) devise a strategy known as Wait-Free Backpropagation
    (WFBP) to interleave GPU and CPU computation and communication. WFBP reduces bursts
    in network traffic and lowers overall network strain. Using WFBP, Zhang et al.
    achieve speed-ups in training times in 16 and 32 single-GPU machines. WFBP has
    become the de-facto approach for data parallelism frameworks.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 减少通信瓶颈的努力持续进行。Zhang et al.（Zhang et al., [2017](#bib.bib72)）设计了一种称为无等待反向传播（WFBP）的策略，以交错GPU和CPU计算及通信。WFBP减少了网络流量的突发，并降低了整体网络负担。使用WFBP，Zhang
    et al.在16和32台单GPU机器上实现了训练时间的加速。WFBP已成为数据并行框架的**最终**方法。
- en: PyTorch DistributedDataParallel (DDP) (Li et al., [2020](#bib.bib32)), Horovod (Sergeev
    and Balso, [2018](#bib.bib53)) and Livermore Big Artificial Neural Network (LBANN) (Essen
    et al., [2015](#bib.bib17)) toolkit are three open source frameworks designed
    to assist in transitioning models into a distributed environment. Out of these
    frameworks PyTorch DDP has been extremely popular among the deep learning community
    due to its seamless integration with PyTorch (Paszke et al., [2019](#bib.bib43)).
    Horovod is an implementation of WFBP for TensorFlow by Uber. LBANN accelerates
    parallelized deep learning by taking advantage of high performance computing hardware.
    These implementations share an uncanny similarity in the way they optimize WFBP.
    Instead of having an individual all-reduce call for each parameter tensor, they
    fuse parameter tensors into fixed size bins. All reduce calls are made at the
    granularity of these fused parameter bins. This increases network bandwidth utilization
    and thus the overall performance of these frameworks. Although the fused tensor
    bin-size is kept as a tunable hyperparameter, Li et al. (Li et al., [2020](#bib.bib32))
    demonstrate that the default bucket size of PyTorch DDP i.e. 25MB is a reasonable
    choice for efficient scaling.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch DistributedDataParallel (DDP)（Li et al., [2020](#bib.bib32)），Horovod（Sergeev
    and Balso, [2018](#bib.bib53)）和Livermore Big Artificial Neural Network (LBANN)（Essen
    et al., [2015](#bib.bib17)）工具包是三个旨在协助将模型过渡到分布式环境中的开源框架。在这些框架中，PyTorch DDP因其与PyTorch的无缝集成而在深度学习社区中非常受欢迎（Paszke
    et al., [2019](#bib.bib43)）。Horovod是Uber为TensorFlow实现的WFBP。LBANN通过利用高性能计算硬件来加速并行深度学习。这些实现方式在优化WFBP的方式上有着惊人的相似之处。它们不是对每个参数张量进行单独的全量缩减调用，而是将参数张量融合到固定大小的箱中。在这些融合的参数箱的粒度上进行所有缩减调用。这提高了网络带宽利用率，从而提升了这些框架的整体性能。尽管融合张量箱的大小保持为可调超参数，Li
    et al.（Li et al., [2020](#bib.bib32)）展示了PyTorch DDP的默认桶大小，即25MB，是一个合理的高效扩展选择。
- en: 3.1.2\. Large Models
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 大型模型
- en: Given the abundance of large training datasets neural networks with increasingly
    larger number of parameters have led to tremendous gains in performance on a variety
    of training tasks. As models and datasets grow in size GPU memory capacity becomes
    a major bottleneck. Data parallelism requires each GPU to store its own copy of
    the neural network. With larger models and datasets the memory required to house
    the activations, gradients and parameters of these neural networks often exceeds
    the capacity of a single GPU DRAM. Data parallelism is thus rendered infeasible
    for training large models without memory optimizations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大量的大型训练数据集，参数越来越多的神经网络在各种训练任务上取得了巨大的性能提升。随着模型和数据集的增大，GPU 内存容量成为主要瓶颈。数据并行性要求每个
    GPU 存储其自身的神经网络副本。随着模型和数据集的增大，用于存放这些神经网络的激活、梯度和参数的内存通常超过单个 GPU DRAM 的容量。因此，在没有内存优化的情况下，数据并行性使得训练大型模型变得不可行。
- en: 'Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., [2020](#bib.bib47)) is
    a framework built over PyTorch to reduce per-GPU memory consumption. The paper
    observes that most memory during training is occupied by optimizer states, gradients,
    and parameters. ZeRO partitions these model states across GPUs to remove memory
    redundancies. With ZeRO, memory reduction scales proportionally with the number
    of GPUs while communication overhead only increases by a constant factor of 1.5x.
    The paper finds improvements in model size, training performance, and scalability
    with 100 billion parameter models on up to 400 GPUs using the Adam optimizer (Kingma
    and Ba, [2015](#bib.bib28)) and mixed precision. Researchers at Microsoft have
    used ZeRO to train one of the largest neural networks in language modeling literature:
    a 17B parameter neural network called the Turing-NLG.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., [2020](#bib.bib47)) 是一个建立在
    PyTorch 之上的框架，用于减少每个 GPU 的内存消耗。论文观察到，在训练过程中，大部分内存被优化器状态、梯度和参数占据。ZeRO 将这些模型状态在
    GPUs 之间进行分区，以消除内存冗余。使用 ZeRO，内存减少与 GPU 数量成比例，而通信开销仅增加 1.5 倍的常数因子。论文发现，在使用 Adam
    优化器 (Kingma and Ba, [2015](#bib.bib28)) 和混合精度的情况下，1000 亿参数的模型在最多 400 个 GPU 上有了模型大小、训练性能和可扩展性的改进。微软的研究人员使用
    ZeRO 训练了语言建模文献中最大的神经网络之一：一个名为 Turing-NLG 的 170 亿参数神经网络。
- en: Out-of core training algorithms like NVIDIA’s vDNN (Rhu et al., [2016](#bib.bib51))
    are often used to train neural networks on a single GPU with insufficient DRAM
    capacity. These algorithms move data back and forth between the CPU and the GPU
    to free up space on the GPU. KARMA (Wahib et al., [2020](#bib.bib66)) is a framework
    built over PyTorch that extends this out-of-core approach to data parallelism
    on multiple GPUs. They design an efficient algorithm for automatic offloading
    and prefetching of activations and parameters of the neural network to and from
    the CPU DRAM. These capabilities are further extended to support multi-GPU models
    by performing weight updates on the CPU. KARMA sees a 1.52x speed-up against other
    state-of-the-art out-of-core methods. It provides an efficient way to utilize
    data parallelism for large models that would otherwise necessitate other frameworks.
    Zero-Infinity (Rajbhandari et al., [2021](#bib.bib48)) is another framework that
    provides support for out-of-core data parallel training for multi-billion parameter
    models. Using their memory optimizations, The authors are able to deploy a 32
    trillion parameter model on as little as 512 GPUs while maintaining a decent throughput
    of around 40% of the peak.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 像 NVIDIA 的 vDNN (Rhu et al., [2016](#bib.bib51)) 这样的核心外训练算法常用于在内存不足的单个 GPU 上训练神经网络。这些算法在
    CPU 和 GPU 之间来回移动数据，以释放 GPU 上的空间。KARMA (Wahib et al., [2020](#bib.bib66)) 是一个建立在
    PyTorch 之上的框架，它将这种核心外方法扩展到多个 GPU 上的数据并行性。他们设计了一种高效的自动卸载和预取神经网络激活和参数的算法。这些能力进一步扩展以支持多
    GPU 模型，通过在 CPU 上执行权重更新。KARMA 相对于其他最先进的核心外方法有 1.52 倍的速度提升。它为大模型提供了一种高效的数据并行性利用方式，否则将需要其他框架。Zero-Infinity
    (Rajbhandari et al., [2021](#bib.bib48)) 是另一个支持多亿参数模型核心外数据并行训练的框架。使用他们的内存优化，作者能够在仅有
    512 个 GPU 的情况下部署一个 32 万亿参数的模型，同时保持大约 40% 的峰值吞吐量。
- en: 3.1.3\. Large Effective Mini-Batch Sizes
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 大型有效小批量尺寸
- en: Data parallelism is most efficient with high per-GPU workloads. This is ensured
    by fixing the per-GPU mini-batch size. As an example, suppose a ResNet model with
    a per-GPU mini-batch size of 128 is trained over 64 GPUs. This is equivalent to
    an effective mini-batch size of 8192 on a single GPU. It has been empirically
    shown that an extremely large effective mini-batch size has an adverse effect
    on the statistical efficiency of neural network training (Goyal et al., [2017](#bib.bib18)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性在高每GPU工作负载下最为高效。这通过固定每GPU的小批量大小来确保。例如，假设一个 ResNet 模型在 64 个 GPU 上以每 GPU
    小批量大小为 128 进行训练。这相当于在单个 GPU 上有效的小批量大小为 8192。实证研究表明，极大的有效小批量大小对神经网络训练的统计效率有不利影响（Goyal
    等，[2017](#bib.bib18)）。
- en: 'The naive approach to compensate for this is to increase the learning rate
    (LR). Krizhevsky (Krizhevsky et al., [2017](#bib.bib30)) proposes to scale LR
    linearly with mini-batch size. Problems emerge as more workers are added to accelerate
    training: large LR values result in accuracy losses and training instability.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的简单方法是增加学习率（LR）。Krizhevsky（Krizhevsky 等，[2017](#bib.bib30)）建议根据小批量大小线性缩放
    LR。随着更多工作节点的加入以加速训练，问题出现：较大的 LR 值会导致准确性损失和训练不稳定。
- en: Goyal et al. (Goyal et al., [2017](#bib.bib18)) propose a LR warmup scheme to
    combat accuracy loss. Training begins with a lower LR that slowly builds up to
    a target value following the linear scaling rule. The paper was able to train
    ResNet-50 with a mini-batch size of 8K and accuracy matching smaller mini-batch
    models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Goyal 等（Goyal 等，[2017](#bib.bib18)）提出了一种 LR 预热方案来应对准确性损失。训练开始时使用较低的学习率（LR），然后根据线性缩放规则逐渐增加到目标值。该论文能够用小批量大小为
    8K 的 ResNet-50 进行训练，并达到与较小小批量模型相匹配的准确性。
- en: You et al.(You et al., [2017](#bib.bib69), [2018](#bib.bib71)) devise Layer-wise
    Adaptive Rate Scaling (LARS) as an alternate approach to LR warmup. LARS adapts
    the global LR to create separate LRs per model layer based on the ratio between
    layer weights and gradient updates. The paper observes this ratio varies across
    layers and provides insight into the efficacy of a layer’s weight updates. You
    et al. utilize LARS to train AlexNet and ResNet-50 with a mini-batch size of 32K
    without accuracy loss.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: You 等（You 等，[2017](#bib.bib69)，[2018](#bib.bib71)）提出了分层自适应学习率缩放（LARS）作为 LR 预热的替代方法。LARS
    通过根据层权重和梯度更新的比率调整全局 LR，为每个模型层创建单独的学习率。该论文观察到这个比率在层间变化，并提供了对层权重更新有效性的见解。You 等利用
    LARS 训练 AlexNet 和 ResNet-50，以 32K 的小批量大小进行训练且没有准确性损失。
- en: LARS experiences inconsistent performance gains across different deep learning
    tasks. You et. al (You et al., [2019](#bib.bib70)) propose a general strategy
    to adapt any iterative optimizer for large mini-batch training. They apply this
    strategy to create LAMB using the Adam optimizer as a base. Using LAMB, You et
    al. scale BERT training to a mini-batch size of 32K without performance degradation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: LARS 在不同深度学习任务中表现出不一致的性能提升。You 等（You 等，[2019](#bib.bib70)）提出了一种通用策略，以适应任何迭代优化器用于大小批量训练。他们应用该策略创建了
    LAMB，并以 Adam 优化器作为基础。使用 LAMB，You 等将 BERT 训练扩展到 32K 的小批量大小而不降低性能。
- en: 3.2\. Intra-Layer Parallelism
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 层内并行性
- en: State of the art training techniques in intra-layer parallelism span from fine-grained
    parallel implementations of numerical kernels to dividing the coarse-grained work
    of a single layer across processes. It is often used in conjunction with other
    parallelization strategies such as data or inter-layer parallelism.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 层内并行性的最新训练技术包括从数值内核的细粒度并行实现到将单层的粗粒度工作划分到多个进程中。它通常与其他并行化策略，如数据并行或层间并行，结合使用。
- en: 3.2.1\. Fine-Grained Parallelism
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 细粒度并行性
- en: At the fine-grained level many techniques draw from existing numerical methods
    and adapt them to deep learning. Matrix multiplication and convolutions are the
    most utilized kernels and have been the focus of much optimization from the ML
    and broader scientific community. Many accelerators and processors have paired
    software libraries which implement these kernels tuned to their hardware such
    as CuDNN(Chetlur et al., [2014](#bib.bib10)), MIOpen(Khan et al., [2019](#bib.bib25)),
    and OneDNN.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度层面，许多技术借鉴了现有的数值方法，并将其调整用于深度学习。矩阵乘法和卷积是最常用的内核，并且得到了机器学习及更广泛科学界的广泛优化。许多加速器和处理器配备了实现这些内核的软件库，这些库根据其硬件进行调优，例如
    CuDNN（Chetlur 等，[2014](#bib.bib10)）、MIOpen（Khan 等，[2019](#bib.bib25)）和 OneDNN。
- en: Accelerators have been at the core of fine-grained parallelism within a layer.
    Several works have introduced techniques, some ML based, for mapping layer computations
    to the hardware optimally(Kao and Krishna, [2020](#bib.bib24); Steuwer et al.,
    [2017](#bib.bib59); Kwon et al., [2020](#bib.bib31)). Here a mapping is the tiling
    strategy, computation order, and parallelization strategy, hence, the search space
    for optimal mappings can be immense.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 加速器一直是层内细粒度并行性的核心。几项工作引入了技术，一些基于机器学习的方法，用于将层计算最优地映射到硬件(Kao and Krishna, [2020](#bib.bib24);
    Steuwer et al., [2017](#bib.bib59); Kwon et al., [2020](#bib.bib31))。这里的映射是指平铺策略、计算顺序和并行化策略，因此，最佳映射的搜索空间可能非常巨大。
- en: There has been recent interest in using hardware accelerators other than GPGPUs
    to train deep networks. FPGAs have emerged as a viable candidate in DNN acceleration
    due to their lower energy consumption than GPUs and the flexibility provided by
    their reconfigurability. Recent work has explored optimizing DNN operations on
    FPGA hardware(Ma et al., [2017](#bib.bib34)). More recently, novel architectures
    have been proposed to improve memory re-use and parallel performance(Kim et al.,
    [2016](#bib.bib27); Chen et al., [2016](#bib.bib8), [2017](#bib.bib9)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，人们对使用 GPGPU 之外的硬件加速器来训练深度网络产生了兴趣。FPGA 由于其比 GPU 更低的能耗以及可重构性所提供的灵活性，已成为 DNN
    加速的一个可行候选者。最近的研究探索了在 FPGA 硬件上优化 DNN 操作(Ma et al., [2017](#bib.bib34))。更近期，提出了新颖的架构以提高内存重用和并行性能(Kim
    et al., [2016](#bib.bib27); Chen et al., [2016](#bib.bib8), [2017](#bib.bib9))。
- en: 3.2.2\. Coarse-Grained Parallelism
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 粗粒度并行性
- en: Orthogonal to the fine-grained compute kernels there have been techniques developed
    to divide work inside a layer along coarser tensor dimensions. These typically
    involve using optimization algorithms and/or ML to identify optimal partitions
    of computation and data within a layer and then developing a parallel strategy
    for execution. Song et al. propose a method for finding communication optimal
    parallel strategies on accelerator arrays in linear time(Song et al., [2019](#bib.bib58)).
    Similarly, Jia et al. introduce a novel Markov Chain Monte Carlo based search
    for finding optimal parallelization strategies, which encompasses intra-layer
    in its operator dimension(Jia et al., [2019](#bib.bib23)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了细粒度计算内核之外，还开发了在层内部沿着较粗的张量维度划分工作的技术。这些技术通常涉及使用优化算法和/或机器学习来识别层内计算和数据的最佳分区，然后为执行制定并行策略。Song
    等人提出了一种在加速器阵列上以线性时间找到通信最优并行策略的方法(Song et al., [2019](#bib.bib58))。类似地，Jia 等人引入了一种基于马尔可夫链蒙特卡罗的搜索方法，用于寻找最优的并行化策略，该方法涵盖了操作维度内的层内并行(Jia
    et al., [2019](#bib.bib23))。
- en: MeshTensorFlow accomplishes a similar effect by mapping tensor dimensions to
    a n-dimensional processor array or ”mesh”(Shazeer et al., [2018](#bib.bib54)).
    These tensors are split and/or replicated across the mesh, such that the computation
    can be done in parallel using the processor array. The framework itself provides
    an interface for users to define a layout. Any layout will produce the same results
    for the same problem, however, the memory footprint and performance can be greatly
    improved with an optimal layout.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: MeshTensorFlow 通过将张量维度映射到 n 维处理器阵列或“网格”来实现类似的效果(Shazeer et al., [2018](#bib.bib54))。这些张量在网格上被拆分和/或复制，从而可以利用处理器阵列并行计算。该框架本身提供了一个接口，供用户定义布局。任何布局对于相同的问题都会产生相同的结果，但是，具有最优布局的内存占用和性能可以得到极大的改善。
- en: Dryden et al(Dryden et al., [2019b](#bib.bib16)) also propose several algorithms
    for partitioning convolution tensor dimensions with the goal of reducing all-reduce
    time during training. Their algorithms are available in the LBANN framework. Convolutions
    are also parallelized in (Oyama et al., [2020](#bib.bib42)) with a hybrid parallelism
    by extending data parallelism with parallelism in the spatial domain. For language-based
    models Megatron(Shoeybi et al., [2020](#bib.bib55)) achieves a similar parallelism
    by partitioning the blocks in transformer layers across processors. Megatron has
    been increasingly used as language models become more common and larger (see Figure
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey and Empirical Evaluation of
    Parallel Deep Learning Frameworks")). It has shown up to 74% weak scaling coefficient
    on 512 GPUs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Dryden 等人（Dryden 等人，[2019b](#bib.bib16)）还提出了几种算法用于划分卷积张量维度，目的是减少训练过程中全减少的时间。他们的算法可在
    LBANN 框架中使用。卷积也在（Oyama 等人，[2020](#bib.bib42)）中通过在空间域扩展数据并行性实现了混合并行。对于基于语言的模型，Megatron（Shoeybi
    等人，[2020](#bib.bib55)）通过在处理器之间划分变换器层的块实现了类似的并行性。随着语言模型变得越来越常见且规模越来越大，Megatron
    的使用也越来越广泛（见图 [1](#S1.F1 "图 1 ‣ 1\. 介绍 ‣ 并行深度学习框架的调查与实证评估")）。它在 512 个 GPU 上表现出了高达
    74% 的弱缩放系数。
- en: Dividing layer tensor dimensions across processors is, however, very sensitive
    to the layer type. For instance, fully connected layers involve an all-to-all
    computation and therefore all-to-all communication, which is more expensive the
    data parallelism’s allreduce. Thus, it is hard to generalize coarser grained intra-layer
    parallelism for models with custom layers. To combat this some methods look strictly
    at compute graph operations and not model layers (Jia et al., [2019](#bib.bib23)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理器之间划分层张量维度时，非常依赖于层的类型。例如，完全连接层涉及全对全计算，因此也涉及全对全通信，这比数据并行的全减少更加昂贵。因此，对于具有自定义层的模型，很难将粗粒度的层内并行性进行泛化。为了解决这个问题，一些方法严格关注计算图操作而不是模型层（Jia
    等，[2019](#bib.bib23)）。
- en: 3.3\. Inter-Layer Parallelism
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 层间并行
- en: 'True inter-layer parallelism can only be achieved by pipelining i.e. having
    multiple mini-batches active in the system at any given instance. There are two
    ways to achieve pipelining: with and without flushing. In this section, we discuss
    the pros and cons of both approaches. We also provide an overview of frameworks
    that implement these approaches.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的层间并行只能通过流水线实现，即在系统的任何给定时刻保持多个小批量数据处于活动状态。实现流水线有两种方式：有刷新和无刷新。在本节中，我们讨论这两种方法的优缺点。我们还提供了实现这些方法的框架的概述。
- en: 3.3.1\. Pipelining with Flushing
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 流水线处理与刷新
- en: Pipelining with flushing divides a mini-batch into micro-batches of equal size.
    These micro-batches are injected one by one into the system. GPUs accumulate gradients
    from all the micro-batches in the system. A GPU updates its weights only after
    it has finished the backward pass of the last micro-batch. The next mini-batch
    and its corresponding micro-batches are injected after all the GPUs have finished
    updating their weights. This approach to pipelining is also called micro-batching.
    The number of micro-batches is usually kept to be much larger than the number
    of workers so that each worker can compute concurrently. Ensuring optimum hardware
    utilization requires having a large mini-batch size. To maintain statistical efficiency
    at large mini-batch sizes the same set of solutions discussed in Section [3.1.3](#S3.SS1.SSS3
    "3.1.3\. Large Effective Mini-Batch Sizes ‣ 3.1\. Data Parallelism ‣ 3\. Literature
    Survey ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")
    can be used. Figure [3](#S2.F3 "Figure 3 ‣ 2.2\. Parallel Deep Learning Methods
    ‣ 2\. Background ‣ A Survey and Empirical Evaluation of Parallel Deep Learning
    Frameworks") shows pipelining with flushing in action. Worker GPUs incur idle
    time between the forward pass of the last micro-batch and the backward pass of
    the first micro-batch. These are called pipeline bubbles. They reduce the overall
    hardware utilization of the system A load balanced mapping of layers to GPUs is
    absolutely critical to maximize performance. The load balancing algorithm must
    also be communication-aware. This is because activations and gradients exchanged
    at GPU boundaries can be in the magnitudes of GBs for large neural networks. An
    efficient implementation of pipelining with flushing must have load balancing
    support.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带有刷新功能的流水线将一个小批量划分为相等大小的微批量。这些微批量一个接一个地注入系统中。GPU 从系统中的所有微批量中累积梯度。GPU 仅在完成最后一个微批量的反向传播后才更新其权重。在所有
    GPU 完成权重更新后，下一小批量及其对应的微批量才会被注入。这个流水线的方法也被称为微批量处理。微批量的数量通常保持远大于工作者的数量，以便每个工作者可以并行计算。为了确保硬件的最佳利用，需要较大的小批量大小。为了在大小批量情况下保持统计效率，可以使用第
    [3.1.3](#S3.SS1.SSS3 "3.1.3\. 大有效小批量大小 ‣ 3.1\. 数据并行 ‣ 3\. 文献综述 ‣ 并行深度学习框架的调查与实证评估")节中讨论的相同解决方案。图
    [3](#S2.F3 "图 3 ‣ 2.2\. 并行深度学习方法 ‣ 2\. 背景 ‣ 并行深度学习框架的调查与实证评估") 显示了带有刷新功能的流水线的实际情况。工作
    GPU 在最后一个微批量的前向传播和第一个微批量的反向传播之间会产生空闲时间。这些称为流水线气泡。它们会降低系统的整体硬件利用率。将层负载均衡映射到 GPU
    是最大化性能的绝对关键。负载均衡算法还必须具备通信意识。这是因为在 GPU 边界交换的激活值和梯度可能达到 GB 级别，对于大型神经网络尤为如此。有效的带有刷新功能的流水线实现必须具备负载均衡支持。
- en: Table 2\. System information about the HPC platforms used for the experiments.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 关于用于实验的 HPC 平台的系统信息。
- en: '| System | No. of Nodes | CPU |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 节点数量 | CPU |'
- en: '&#124; Cores/ &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 核心/ &#124;'
- en: '&#124; node &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 节点 &#124;'
- en: '| GPU |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GPU |'
- en: '&#124; GPUs/ &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPUs/ &#124;'
- en: '&#124; node &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 节点 &#124;'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CPU Mem. / &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CPU Mem. / &#124;'
- en: '&#124; Node (GB) &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 节点 (GB) &#124;'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GPU Mem. / &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CPU 内存 / &#124;'
- en: '&#124; Node (GB) &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 节点 (GB) &#124;'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GPU FP64 &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPU FP64 &#124;'
- en: '&#124; Peak (TFlop/s) &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 峰值 (TFlop/s) &#124;'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Lassen | 795 | IBM Power9 | 44 | NVIDIA V100 | 4 | 256 | 64 | 7.0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Lassen | 795 | IBM Power9 | 44 | NVIDIA V100 | 4 | 256 | 64 | 7.0 |'
- en: '| ThetaGPU | 24 | AMD Rome | 64 | NVIDIA A100 | 8 | 1024 | 320 | 9.7 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ThetaGPU | 24 | AMD Rome | 64 | NVIDIA A100 | 8 | 1024 | 320 | 9.7 |'
- en: This idea was first introduced by Huang et al. in GPipe (Huang et al., [2019](#bib.bib19)).
    Using GPipe they trained a 557M parameter neural network - AmoebaNet-B (Real et al.,
    [2019](#bib.bib49)) on the ImageNet (Russakovsky et al., [2015](#bib.bib52)) dataset
    and surpassed the state of the art in a number of downstream image classification
    tasks. TorchGPipe (Kim et al., [2020](#bib.bib26)) is an unofficial open-source
    implementation of GPipe built on the PyTorch (Paszke et al., [2019](#bib.bib43))
    backend. GEMS (GPU-Enabled Memory Aware Model-Parallelism System) (Jain et al.,
    [2020](#bib.bib21)) introduces a novel approach to increase hardware utilization.
    This framework proposes an algorithm to train two neural networks concurrently
    using pipelining without flushing on multiple GPUs. They double the throughput
    of the system by overlapping the forward and backward passes of the two neural
    networks. We refer the reader to their paper for the details of their implementation.
    Recently ZeRO (Rajbhandari et al., [2020](#bib.bib47)) and Megatron (Shoeybi et al.,
    [2020](#bib.bib55)) also extended support for this approach towards inter-layer
    parallelism. TorchGPipe (Kim et al., [2020](#bib.bib26)) provides a load balancing
    algorithm that seeks to balance the net execution time of the forward and backward
    pass of a micro-batch on each GPU. However, their algorithm ignores the communication
    overhead of exchanging tensors across GPU boundaries. Megatron divides the layers
    of a transformer across GPUs, which is optimal because all the layers of a transformer
    are identical. ZeRO also provides an identical strategy that divides the layers
    equally across GPUs. Additionally, they also support a load balancing algorithm
    that equalizes GPU memory consumption across GPUs. AxoNN (Singh and Bhatele, [pear](#bib.bib57))
    introduced a novel asynchronous communication backend for inter-layer parallelism.
    To the best of our knowledge this is the first work that utilizes asychrony for
    increasing hardware utilization by opting for MPI instead of NCCL. They also introduce
    a memory optimization algorithm that they use to decrease the pipeline depth,
    increase data parallelism and outperform the state-of-art by 15%-25% on models
    with as many as 100 billion parameters.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法最初由黄等人提出，在GPipe中（黄等人，[2019](#bib.bib19)）。他们使用GPipe训练了一个557M参数的神经网络——AmoebaNet-B（Real等人，[2019](#bib.bib49)），在ImageNet（Russakovsky等人，[2015](#bib.bib52)）数据集上超越了许多下游图像分类任务的现有水平。TorchGPipe（Kim等人，[2020](#bib.bib26)）是一个非官方的开源GPipe实现，构建在PyTorch（Paszke等人，[2019](#bib.bib43)）后端上。GEMS（GPU-Enabled
    Memory Aware Model-Parallelism System）（Jain等人，[2020](#bib.bib21)）引入了一种新方法以提高硬件利用率。该框架提出了一种算法，通过在多个GPU上使用流水线而无需刷新来并行训练两个神经网络。他们通过重叠两个神经网络的前向和反向传播，达到了系统吞吐量的翻倍。我们建议读者查阅他们的论文以获取实现细节。最近，ZeRO（Rajbhandari等人，[2020](#bib.bib47)）和Megatron（Shoeybi等人，[2020](#bib.bib55)）也扩展了对这一方法的支持，向层间并行ism迈进。TorchGPipe（Kim等人，[2020](#bib.bib26)）提供了一种负载均衡算法，旨在平衡每个GPU上微批次的前向和反向传播的净执行时间。然而，他们的算法忽略了在GPU边界之间交换张量的通信开销。Megatron将变换器的层划分到GPU上，这是最优的，因为变换器的所有层都是相同的。ZeRO也提供了一种相同的策略，将层均等划分到GPU上。此外，他们还支持一种负载均衡算法，以在GPU之间平衡内存消耗。AxoNN（Singh和Bhatele，[pear](#bib.bib57)）引入了一种新颖的异步通信后端，用于层间并行ism。根据我们所知，这是首次利用异步性来提高硬件利用率，选择MPI而非NCCL。他们还引入了一种内存优化算法，用于减少流水线深度，提高数据并行性，并在多达1000亿参数的模型上超越现有水平15%-25%。
- en: 3.3.2\. Pipelining without Flushing
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. 无需刷新流水线
- en: In this approach the number of mini-batches active in the system is kept constant.
    As soon as a mini-batch finishes its backward pass on the first GPU a new mini-batch
    is injected into the system to maintain full pipeline occupancy. Unlike pipelining
    with flushing, weight updates on a GPU take place as soon as it is done with the
    backward pass of a mini-batch. This method of pipelining seeks to increase hardware
    utilization by removing flushing induced bubbles in the pipeline. However, statistical
    efficiency of such a training algorithm reduces drastically. This is due to a
    problem called weight staleness that occurs when newer mini-batches in a pipeline
    encounter stale weights in forward passes which are yet to be updated with the
    backward pass of older mini-batches. This is one of the major reasons why pipelining
    without flushing has not seen widespread adoption. PipeDream (Narayanan et al.,
    [2019](#bib.bib40)) is a framework that implements pipelining without flushing.
    It employs an algorithm called weight stashing to counter weight staleness. We
    refer the reader to their paper for exact details of the implementation. Chen
    et al. (Chen et al., [2019](#bib.bib6)) suggest predicting future weights from
    stale weights using a variant of SGD with momentum (Qian, [1999](#bib.bib45)).
    PipeDream additionally proposes a static load balancing algorithm that is communication
    aware. It instruments each layer and uses the profiling data in its load balancer.
    Their framework also has an additional provision to replicate compute-intensive
    layers across GPUs to increase their throughput. Replicated layers synchronize
    their gradients via all-reduce after each backward pass.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，系统中活跃的小批量的数量保持不变。一旦一个小批量在第一个 GPU 上完成了反向传播，一个新的小批量会被注入系统，以保持流水线的满负荷运行。与具有冲洗的流水线不同，在
    GPU 上的权重更新发生在它完成一个小批量的反向传播后。这种流水线方法旨在通过消除流水线中的冲洗引起的气泡来提高硬件利用率。然而，这种训练算法的统计效率会急剧下降。这是由于所谓的权重过时问题，当流水线中的新小批量在前向传播中遇到尚未用旧小批量的反向传播更新的过时权重时，就会出现这种问题。这是为什么没有冲洗的流水线没有得到广泛采用的主要原因之一。PipeDream (Narayanan
    et al., [2019](#bib.bib40)) 是一个实现无冲洗流水线的框架。它采用了一种称为权重存储的算法来对抗权重过时问题。有关实现的详细信息，我们建议读者参考他们的论文。Chen
    et al. (Chen et al., [2019](#bib.bib6)) 提议使用具有动量的 SGD 变体 (Qian, [1999](#bib.bib45))
    从过时的权重中预测未来的权重。PipeDream 还提出了一种静态负载均衡算法，该算法考虑了通信。它为每一层做了仪器化，并在其负载均衡器中使用了剖面数据。它的框架还额外提供了一个在
    GPU 之间复制计算密集型层的机制，以提高其吞吐量。复制的层在每次反向传播后通过全归约同步其梯度。
- en: Table 3\. Training datasets and network hyperparameters used for benchmarking
    in the paper
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 训练数据集和网络超参数，用于文中的基准测试
- en: '| Dataset |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 |'
- en: '&#124; Training &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; Split Size &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 切分大小 &#124;'
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Validation &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 验证 &#124;'
- en: '&#124; Split Size &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 切分大小 &#124;'
- en: '| Network |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 网络 |'
- en: '&#124; Mini-Batch Size &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 小批量大小 &#124;'
- en: '&#124; per GPU &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每个 GPU &#124;'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Optimizer^(††) &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 优化器^(††) &#124;'
- en: '|'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Learning &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '&#124; Rate &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 速率 &#124;'
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; No. of &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数量 &#124;'
- en: '&#124; Epochs &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 纪元 &#124;'
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; L2 Decay &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; L2 衰减 &#124;'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ImageNet | 1,281,167 | 50,000 | VGG-16 | 64^† | SGD^† | 0.01^† | 90^† | 0.0001^†
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | 1,281,167 | 50,000 | VGG-16 | 64^† | SGD^† | 0.01^† | 90^† | 0.0001^†
    |'
- en: '| Wikitext-103 | 103,227,021 | 217,646 | GPT2-medium | 32^(∗∗) | LAMB^∗ | 0.001^∗
    | 100^(∗∗) | 0.01^∗ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Wikitext-103 | 103,227,021 | 217,646 | GPT2-medium | 32^(∗∗) | LAMB^∗ | 0.001^∗
    | 100^(∗∗) | 0.01^∗ |'
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ^∗ Values directly taken from MLPerf
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ^∗ 值直接取自 MLPerf
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ^(∗∗) Values defined as unconstrained in MLPerf
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ^(∗∗) 在 MLPerf 中定义为无限制的值
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ^† Values directly taken from torchvision - [https://github.com/pytorch/vision/tree/master/references/classification](https://github.com/pytorch/vision/tree/master/references/classification)
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ^† 值直接取自 torchvision - [https://github.com/pytorch/vision/tree/master/references/classification](https://github.com/pytorch/vision/tree/master/references/classification)
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ^(††) For ZeRO, we use the Adam optimizer with 0.001 learning rate and 0.01
    l2 decay as it’s memory optimizations only work with Adam
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ^(††) 对于 ZeRO，我们使用学习率为 0.001 和 L2 衰减为 0.01 的 Adam 优化器，因为它的内存优化仅适用于 Adam
- en: 4\. Experimental Setup
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验设置
- en: In this section we present a detailed overview of our empirical evaluation of
    a number of parallel deep learning frameworks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了对多个并行深度学习框架的实证评估。
- en: 4.1\. Choice of Frameworks
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 框架选择
- en: We use DDP²²2[https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch) @1.8.0 (Li
    et al., [2020](#bib.bib32)), ZeRO³³3[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed) @0.3.13 (Rajbhandari
    et al., [2020](#bib.bib47)), Megatron⁴⁴4[https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM) @2.3 (Shoeybi
    et al., [2020](#bib.bib55)), PipeDream⁵⁵5[https://github.com/siddharth9820/pipedream](https://github.com/siddharth9820/pipedream) @00931df (Narayanan
    et al., [2019](#bib.bib40)), TorchGPipe⁶⁶6[https://github.com/kakaobrain/torchgpipe](https://github.com/kakaobrain/torchgpipe) @a1b4ee2 (Kim
    et al., [2020](#bib.bib26)), LBANN⁷⁷7[https://github.com/LLNL/lbann](https://github.com/LLNL/lbann) @0.101 (Essen
    et al., [2015](#bib.bib17)), and AxoNN⁸⁸8https://github.com/hpcgroup/axonn/ @db1c6a0 (Singh
    and Bhatele, [pear](#bib.bib57)) for our empirical analysis. For Megatron we profile
    it’s implementations of data-parallelism and intra-layer parallel implementations
    separately. We refer to these as Megatron-data and Megatron-intra respectively.
    This subset is representative of the three types of parallelism discussed in Section
    [3](#S3 "3\. Literature Survey ‣ A Survey and Empirical Evaluation of Parallel
    Deep Learning Frameworks"). We select frameworks which have open-source implementations,
    are easy to setup, and have a relatively large user-base. We also tried to include
    MeshTensorFlow (Shazeer et al., [2018](#bib.bib54)) and FlexFlow  (Jia et al.,
    [2019](#bib.bib23)) in our set of frameworks. However, despite our best efforts
    we could not set them up successfully for experimentation on our machines.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了DDP²²2[https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch) @1.8.0 （Li等，[2020](#bib.bib32)），ZeRO³³3[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed) @0.3.13 （Rajbhandari等，[2020](#bib.bib47)），Megatron⁴⁴4[https://github.com/NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM) @2.3 （Shoeybi等，[2020](#bib.bib55)），PipeDream⁵⁵5[https://github.com/siddharth9820/pipedream](https://github.com/siddharth9820/pipedream) @00931df （Narayanan等，[2019](#bib.bib40)），TorchGPipe⁶⁶6[https://github.com/kakaobrain/torchgpipe](https://github.com/kakaobrain/torchgpipe) @a1b4ee2 （Kim等，[2020](#bib.bib26)），LBANN⁷⁷7[https://github.com/LLNL/lbann](https://github.com/LLNL/lbann) @0.101 （Essen等，[2015](#bib.bib17)），和AxoNN⁸⁸8https://github.com/hpcgroup/axonn/ @db1c6a0 （Singh和Bhatele，[pear](#bib.bib57)）进行我们的实证分析。对于Megatron，我们分别分析了其数据并行和层内并行实现。我们称之为Megatron-data和Megatron-intra。这一子集代表了第[3](#S3
    "3\. 文献综述 ‣ 并行深度学习框架的调查与实证评估")节中讨论的三种并行类型。我们选择了那些具有开源实现、易于设置且用户基数较大的框架。我们还尝试将MeshTensorFlow （Shazeer等，[2018](#bib.bib54)）和FlexFlow （Jia等，[2019](#bib.bib23)）纳入我们的框架集合。然而，尽管我们尽了最大努力，仍未能成功在我们的机器上进行实验。
- en: To prevent dataloading from being a bottleneck we copy training datasets into
    node-local SSDs before training. Data is loaded using PyTorch’s distributed data
    loader with several worker processes. We defaulted to four processes, separate
    from the main process, to read in data. MegatronLM implements their own data loaders,
    which we used with Megatron rather than PyTorch’s. In practice we found these
    to be much faster than the default PyTorch data loaders.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止数据加载成为瓶颈，我们在训练之前将训练数据集复制到节点本地的SSD中。数据使用PyTorch的分布式数据加载器加载，配备了多个工作进程。我们默认为四个进程，与主进程分开，以读取数据。MegatronLM实现了自己的数据加载器，我们在使用Megatron时使用了它们，而不是PyTorch的。在实践中，我们发现这些数据加载器比默认的PyTorch数据加载器快得多。
- en: For a fair performance evaluation of each framework we used mixed precision
    on the V100 and A100 cards on Lassen and ThetaGPU (Micikevicius et al., [2018](#bib.bib39)).
    Of the frameworks we ran DDP, Megatron, LBANN, and ZeRO were the only ones that
    supported mixed precision with distributed training.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平评估每个框架的性能，我们在Lassen和ThetaGPU（Micikevicius等，[2018](#bib.bib39)）上使用了混合精度。在我们运行的框架中，只有DDP、Megatron、LBANN和ZeRO支持混合精度分布式训练。
- en: All of the listed frameworks use Pytorch 1.8.0, CUDA 11.0, and CuDNN 8.0 for
    launching computation on GPUs. For inter-GPU communication, PipeDream uses the
    gloo communication library shipped with Pytorch 1.8.0, whereas all of the other
    frameworks use NCCL 2.7.8.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 所有列出的框架都使用Pytorch 1.8.0、CUDA 11.0和CuDNN 8.0来启动GPU上的计算。对于GPU间通信，PipeDream使用了随Pytorch
    1.8.0一起提供的gloo通信库，而所有其他框架则使用NCCL 2.7.8。
- en: 4.2\. System Hardware
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 系统硬件
- en: Table [2](#S3.T2 "Table 2 ‣ 3.3.1\. Pipelining with Flushing ‣ 3.3\. Inter-Layer
    Parallelism ‣ 3\. Literature Survey ‣ A Survey and Empirical Evaluation of Parallel
    Deep Learning Frameworks") describes the systems and hardware used in our training.
    Lassen is an IBM machine at Lawrence Livermore National Laboratory with a Mellanox
    network. It currently sits at number 26 on the Top500 list. ThetaGPU is a GPU
    extension of the Cray XC40 Theta system.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S3.T2 "表2 ‣ 3.3.1\. 带冲刷的流水线 ‣ 3.3\. 层间并行 ‣ 3\. 文献调研 ‣ 并行深度学习框架的调查和实证评估")描述了我们训练中使用的系统和硬件。拉森是劳伦斯利弗莫尔国家实验室的IBM机器，配备Mellanox网络。它目前在Top500榜单上排名第26。ThetaGPU是Cray
    XC40 Theta系统的GPU扩展。
- en: Each system was selected to be representative of typical machines used for DL
    training. Lassen is similar to other leadership HPC systems with GPU-dense nodes.
    The ThetaGPU extension of Theta with dense A100 nodes is more typical of current
    cutting edge AI machines.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个系统都被选为DL训练中典型机器的代表。Lassen类似于其他GPU密集节点的领先HPC系统。具有密集A100节点的ThetaGPU扩展适用于当前尖端AI机器。
- en: 4.3\. Datasets and Neural Networks
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 数据集和神经网络
- en: 'We evaluate the aforementioned subset of frameworks on two popular deep learning
    tasks: image classification and language modeling. For the former task we use
    The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset (Russakovsky
    et al., [2015](#bib.bib52)). This dataset has been widely used to train large
    state of the art image classification neural networks throughout the last decade.
    It consists of more than a million RGB images of dimension 224x224 evenly divided
    across 1000 image classes. We use this dataset to train the VGG-16 (Simonyan and
    Zisserman, [2015](#bib.bib56)) architecture on our selected subset of frameworks.
    Language modeling is an unsupervised learning task wherein models are trained
    to predict the next word in a sentence given all of the previously occurring words.
    We use the Wikitext-103 (Merity et al., [2016](#bib.bib38)) dataset for our language
    modeling training workloads. This dataset is comprised of more than 28000 articles
    from the English Wikipedia amounting to a total of 100 million English words.
    Language modeling has gained immense popularity recently in NLP for training extremely
    large neural networks. Researchers have achieved stellar performance with these
    models in a variety of downstream tasks like question answering, textual entailment,
    translation, reading comprehension, etc… We train the GPT-2-medium architecture
    proposed by OpenAI in their paper (Radford et al., [2019](#bib.bib46)) on the
    Wikitext-103 (Merity et al., [2016](#bib.bib38)) dataset. Table [3](#S3.T3 "Table
    3 ‣ 3.3.2\. Pipelining without Flushing ‣ 3.3\. Inter-Layer Parallelism ‣ 3\.
    Literature Survey ‣ A Survey and Empirical Evaluation of Parallel Deep Learning
    Frameworks") provides an overview of the datasets used across our experiments.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个流行的深度学习任务上评估上述子集：图像分类和语言建模。对于前者的任务，我们使用ImageNet大规模视觉识别挑战(ILSVRC)2012数据集（Russakovsky等，[2015](#bib.bib52)）。这个数据集在过去十年中被广泛用于训练大规模最新图像分类神经网络。它由1000个图像类均匀分布的224x224的RGB图像组成超过一百万张。我们使用此数据集来训练VGG-16（Simonyan和Zisserman，[2015](#bib.bib56)）架构在我们选择的框架子集上。语言建模是一项无监督学习任务，其中模型被训练以预测给定之前所有单词的句子中的下一个单词。我们在我们的语言建模训练工作负载中使用Wikitext-103（Merity等，[2016](#bib.bib38)）数据集。这个数据集由超过28000篇英文维基百科文章组成，共计一亿个英文单词。最近，语言建模在NLP中获得了巨大的流行度，用于训练极大的神经网络。研究人员用这些模型在各种下游任务中取得了卓越的性能，如回答问题，文本蕴涵，翻译，阅读理解等...我们在Wikitext-103（Merity等，[2016](#bib.bib38)）数据集上训练OpenAI在其论文中提出的GPT-2-medium架构（Radford等，[2019](#bib.bib46)）。表[3](#S3.T3
    "表3 ‣ 3.3.2\. 无冲刷的流水线 ‣ 3.3\. 层间并行 ‣ 3\. 文献调研 ‣ 并行深度学习框架的调查和实证评估")概述了我们实验中使用的数据集。
- en: 4.3.1\. Hyperparameters
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 超参数
- en: The epoch execution times and statistical efficiency of a training algorithm
    are very sensitive to the choice of hyperparameters. Learning rate schedules,
    optimizer choices and weight decay values can have a large impact on the statistical
    efficiency. Larger mini-batch sizes reduce epoch execution times at the expense
    of statistical efficiency.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法的纪元执行时间和统计效率对超参数的选择非常敏感。学习率调度，优化器选择和权重衰减值可能对统计效率产生很大影响。较大的mini-batch大小可以降低纪元执行时间，但会牺牲统计效率。
- en: Hyperparameters were chosen based on corresponding MLPerf (Mattson et al., [2019](#bib.bib36))
    benchmarks, which are a standard means of comparison for DL training. Because
    of this we keep the parameters fixed between frameworks. For parameters not included
    in the MLPerf description we choose them based on the values given in their respective
    papers. We ensure that training with our hyperparameters gives us reasonable performance
    on the validation set. Table [3](#S3.T3 "Table 3 ‣ 3.3.2\. Pipelining without
    Flushing ‣ 3.3\. Inter-Layer Parallelism ‣ 3\. Literature Survey ‣ A Survey and
    Empirical Evaluation of Parallel Deep Learning Frameworks") provides an overview
    of the hyperparameters applied to each model. It is possible further tuning could
    improve the performance and/or statistical efficiencies.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是根据相应的 MLPerf（Mattson 等人，[2019](#bib.bib36)）基准选择的，这些基准是深度学习训练的标准比较手段。因为这个原因，我们在不同框架之间保持参数固定。对于
    MLPerf 描述中未包含的参数，我们根据各自论文中给出的值进行选择。我们确保使用我们的超参数进行训练时，在验证集上能够获得合理的性能。表格 [3](#S3.T3
    "Table 3 ‣ 3.3.2\. Pipelining without Flushing ‣ 3.3\. Inter-Layer Parallelism
    ‣ 3\. Literature Survey ‣ A Survey and Empirical Evaluation of Parallel Deep Learning
    Frameworks") 提供了每个模型应用的超参数概述。进一步的调整可能会提高性能和/或统计效率。
- en: 'For efficient scaling to larger GPU counts, data parallel algorithms typically
    use a fixed mini-batch size per GPU to maintain a constant computational workload
    per GPU. Thus, to ensure a fair comparison of other frameworks with DDP, AxoNN,
    ZeRO, LBANN and Megatron-data we do the following for each framework:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地扩展到更大的 GPU 数量，数据并行算法通常使用固定的小批量大小来保持每个 GPU 上的计算工作负载不变。因此，为了确保与 DDP、AxoNN、ZeRO、LBANN
    和 Megatron-data 的其他框架的公平比较，我们对每个框架采取以下措施：
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Megatron-intra - We linearly scale the mini-batch size with increasing number
    of GPUs.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Megatron-intra - 我们随着 GPU 数量的增加线性地调整小批量的大小。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: TorchGPipe - We fix the size of a micro-batch and set the number of micro-batches
    to 4 times that of the GPU count.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TorchGPipe - 我们固定微批量的大小，并将微批量的数量设置为 GPU 数量的 4 倍。
- en: •
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: PipeDream - We fix the size of a mini-batch. PipeDream ensures constant computational
    workload on each GPU by increasing it’s pipeline limit automatically.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PipeDream - 我们固定小批量的大小。PipeDream 通过自动增加其管道限制来确保每个 GPU 上的计算工作负载保持恒定。
- en: 4.4\. Exceptions
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 例外
- en: We make the following exceptions to the experimental setups listed above. We
    only show results for PipeDream on a subset of the GPUs due to the framework deadlocking
    on higher GPU counts. We only show results for TorchGPipe upto 8 GPUs on ThetaGPU
    and 4 GPUs on Lassen as it is only applicable to a single node. We only show results
    for LBANN on Lassen as we had difficulties building the framework on ThetaGPU.
    Likewise, we only show AxoNN results on Lassen due to jobs not finishing on ThetaGPU.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对上述实验设置做出以下例外。由于框架在更高 GPU 数量上出现死锁，我们仅在部分 GPU 上展示 PipeDream 的结果。由于 TorchGPipe
    仅适用于单节点，我们仅在 ThetaGPU 上展示最多 8 个 GPU 的结果，在 Lassen 上展示 4 个 GPU 的结果。由于在 ThetaGPU
    上构建框架遇到困难，我们仅在 Lassen 上展示 LBANN 的结果。同样，由于在 ThetaGPU 上作业未完成，我们仅在 Lassen 上展示 AxoNN
    的结果。
- en: 4.5\. Evaluation Metrics
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 评估指标
- en: For our analysis we use metrics that matter the most to a deep learning researcher
    - epoch execution times, statistical efficiency, and GPU memory consumption. Statistically
    efficient training algorithms or frameworks require less number of epochs to reach
    a certain target accuracy on the validation data. When comparing parallel DL frameworks
    it is absolutely imperative to compare both the epoch execution times and statistical
    efficiency of the training runs. We have discussed the tradeoffs that parallel
    DL algorithms incur between these two metrics in Section [3](#S3 "3\. Literature
    Survey ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks").
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的分析，我们使用对深度学习研究人员最重要的指标 - 训练周期执行时间、统计效率和 GPU 内存消耗。统计效率高的训练算法或框架需要较少的周期才能在验证数据上达到某个目标准确率。在比较并行深度学习框架时，比较训练运行的周期执行时间和统计效率是绝对必要的。我们在第
    [3](#S3 "3\. Literature Survey ‣ A Survey and Empirical Evaluation of Parallel
    Deep Learning Frameworks") 节中讨论了并行深度学习算法在这两个指标之间的权衡。
- en: We profile epoch execution times on 1, 2, 4, 8, 16, 32 and 64 GPUs on Lassen
    and ThetaGPU. While profiling the statistical efficiency for a particular framework,
    we use the GPU count where it has the minimum epoch execution times. For gathering
    memory utilization data we use 1, 2, 4, 8, 16, 32 and 64 GPUs on ThetaGPU. Table
    [3](#S3.T3 "Table 3 ‣ 3.3.2\. Pipelining without Flushing ‣ 3.3\. Inter-Layer
    Parallelism ‣ 3\. Literature Survey ‣ A Survey and Empirical Evaluation of Parallel
    Deep Learning Frameworks") and Table [2](#S3.T2 "Table 2 ‣ 3.3.1\. Pipelining
    with Flushing ‣ 3.3\. Inter-Layer Parallelism ‣ 3\. Literature Survey ‣ A Survey
    and Empirical Evaluation of Parallel Deep Learning Frameworks") gives an overview
    of the neural networks and machines we used for evaluating these metrics.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Lassen 和 ThetaGPU 上对 1、2、4、8、16、32 和 64 个 GPU 的每个 epoch 执行时间进行分析。为了评估特定框架的统计效率，我们使用其具有最小
    epoch 执行时间的 GPU 数量。为了收集内存利用率数据，我们在 ThetaGPU 上使用 1、2、4、8、16、32 和 64 个 GPU。表格 [3](#S3.T3
    "Table 3 ‣ 3.3.2\. Pipelining without Flushing ‣ 3.3\. Inter-Layer Parallelism
    ‣ 3\. Literature Survey ‣ A Survey and Empirical Evaluation of Parallel Deep Learning
    Frameworks") 和表格 [2](#S3.T2 "Table 2 ‣ 3.3.1\. Pipelining with Flushing ‣ 3.3\.
    Inter-Layer Parallelism ‣ 3\. Literature Survey ‣ A Survey and Empirical Evaluation
    of Parallel Deep Learning Frameworks") 给出了我们用来评估这些指标的神经网络和机器的概述。
- en: To measure the statistical efficiency we record the accuracy and loss for the
    vision tasks and perplexity for the language tasks. Loss is the output of the
    loss function used for training. Its magnitude depends on its definition, but
    the training loss should decrease towards zero as the model improves in predictive
    capacity. Accuracy measures the ratio of samples accurately predicted to total
    samples. We use the validation accuracy, which is calculated based on samples
    exclusive to the training set. Perplexity is commonly used in NLP to measure how
    well a model predicts for a certain corpus based on the cross-entropy of the model.
    It is defined as the exponential of the cross entropy loss on the dataset.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量统计效率，我们记录了视觉任务的准确性和损失，以及语言任务的困惑度。损失是用于训练的损失函数的输出。其大小取决于定义，但随着模型预测能力的提升，训练损失应减少接近零。准确性衡量的是准确预测样本与总样本的比率。我们使用基于训练集独占样本计算的验证准确性。困惑度在
    NLP 中常用于衡量模型对某个语料库的预测能力，基于模型的交叉熵定义为数据集上的交叉熵损失的指数。
- en: 5\. Comparative Evaluation
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 比较评估
- en: In this section we present and discuss the results from our experiments on epoch
    execution times, statistical efficiency, and memory utilization.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示并讨论了关于 epoch 执行时间、统计效率和内存利用率的实验结果。
- en: '![Refer to caption](img/8c71b32446f766d400ef9bef19a368c5.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8c71b32446f766d400ef9bef19a368c5.png)'
- en: Figure 4\. Comparison of single GPU performance and 4 GPU speedup on Lassen
    for VGG-16 and GPT2-medium. The labels list the speedup of each framework relative
    to their own 1 GPU performance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. Lassen 上 VGG-16 和 GPT2-medium 的单 GPU 性能与 4 GPU 加速比比较。标签列出了每个框架相对于其自身 1
    GPU 性能的加速比。
- en: 5.1\. Execution Time Comparison
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 执行时间比较
- en: We first look at the baseline performance of each framework. Figure [4](#S5.F4
    "Figure 4 ‣ 5\. Comparative Evaluation ‣ A Survey and Empirical Evaluation of
    Parallel Deep Learning Frameworks") presents the sequential single GPU execution
    times on the two neural networks on Lassen. In this test TorchGPipe performs the
    worst on both VGG-16 and GPT2-medium by up to 1.8x and 5.2x, respectively. We
    also observe Pipedream is the second slowest framework. The single GPU performances
    differ significantly largely due to these two not supporting mixed precision.
    The difference is exacerbated for extremely compute intensive neural networks
    like the GPT2-medium.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先查看每个框架的基线性能。图 [4](#S5.F4 "Figure 4 ‣ 5\. Comparative Evaluation ‣ A Survey
    and Empirical Evaluation of Parallel Deep Learning Frameworks") 展示了在 Lassen 上两个神经网络的单
    GPU 顺序执行时间。在此测试中，TorchGPipe 在 VGG-16 和 GPT2-medium 上表现最差，分别慢了 1.8 倍和 5.2 倍。我们还观察到
    Pipedream 是第二慢的框架。单 GPU 性能差异显著，主要是由于这两个框架不支持混合精度。对于像 GPT2-medium 这样计算密集型的神经网络，这种差异更为严重。
- en: While Megatron, DDP, ZeRO and AxoNN employ mixed precision, Megatron is considerably
    faster as it uses its own optimized implementation of the transformer encoder
    layer and Adam optimizer. Figure [4](#S5.F4 "Figure 4 ‣ 5\. Comparative Evaluation
    ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks") exemplifies
    this, where we observe a 2x speedup on a single GPU over the native PyTorch kernel
    used by DDP and ZeRO. The PyTorch implementation performs worse due to its handling
    of the computationally intensive final softmax layer in GPT2-medium. While DDP
    and AxoNN compute this layer in full precision, ZeRO’s mixed precision strategy
    computes this layer in half precision,, leading to the difference in performance
    between the two.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Megatron、DDP、ZeRO和AxoNN使用混合精度，Megatron的速度显著更快，因为它使用了自己优化的变压器编码器层和Adam优化器的实现。图 [4](#S5.F4
    "图 4 ‣ 5\. 比较评估 ‣ 并行深度学习框架的调查与实证评估") 证明了这一点，我们观察到在单个GPU上相对于DDP和ZeRO使用的本地PyTorch内核有2倍的加速。PyTorch实现的性能较差，原因在于它处理GPT2-medium中计算密集的最终softmax层。虽然DDP和AxoNN以全精度计算此层，但ZeRO的混合精度策略则以半精度计算此层，这导致了两者之间性能的差异。
- en: Out of all the frameworks TorchGPipe has the worst single GPU performance. This
    is because micro-batching provides no performance benefits as operations of different
    microbatches are serialized on a single GPU. It however does save memory used
    for stashing activations during the forward pass. We discuss this in Section [5.3](#S5.SS3
    "5.3\. Memory Utilization ‣ 5\. Comparative Evaluation ‣ A Survey and Empirical
    Evaluation of Parallel Deep Learning Frameworks").
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有框架中，TorchGPipe的单GPU性能最差。这是因为微批量处理在单个GPU上序列化了不同微批量的操作，未能提供性能上的好处。然而，它确实节省了在前向传播过程中用于存储激活的内存。我们在第[5.3节](#S5.SS3
    "5.3\. 内存利用 ‣ 5\. 比较评估 ‣ 并行深度学习框架的调查与实证评估")中对此进行了讨论。
- en: '![Refer to caption](img/688d504ae8ca579aca78889831faa3ac.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/688d504ae8ca579aca78889831faa3ac.png)'
- en: Figure 5\. Breakdown of time spent in training on 1, 2, 4, and 8 GPUs of ThetaGPU
    for GPT2-medium. We use NVIDIA’s NVTX SDK for annotating events and Nsight Systems
    for instrumentation. Megatron refers to Megatron-intra.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 在ThetaGPU的1、2、4和8个GPU上训练GPT2-medium的时间分解。我们使用NVIDIA的NVTX SDK来注释事件，并使用Nsight
    Systems进行仪器化。Megatron指的是Megatron-intra。
- en: '![Refer to caption](img/80bbee79864d94018c4a99c936e8e86f.png)![Refer to caption](img/a6ee0d8ebbfc19950c6ad1917046ece6.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/80bbee79864d94018c4a99c936e8e86f.png)![参考说明](img/a6ee0d8ebbfc19950c6ad1917046ece6.png)'
- en: Figure 6\. Performance results on Lassen for VGG-16 and GPT2-medium.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. Lassen上的VGG-16和GPT2-medium的性能结果。
- en: '![Refer to caption](img/ea04ffb5c624e9067ad4d54277d423b9.png)![Refer to caption](img/3a79d8cce57c983c90f1f76d36df75a2.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ea04ffb5c624e9067ad4d54277d423b9.png)![参考说明](img/3a79d8cce57c983c90f1f76d36df75a2.png)'
- en: Figure 7\. Performance results on ThetaGPU for VGG-16 and GPT2-medium.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. ThetaGPU上的VGG-16和GPT2-medium的性能结果。
- en: Figure [5](#S5.F5 "Figure 5 ‣ 5.1\. Execution Time Comparison ‣ 5\. Comparative
    Evaluation ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")
    shows the time spent by each framework in the forward pass, backward pass, and
    I/O for GPT2-medium on ThetaGPU. We observe a marked improvement in Megatron’s
    I/O performance due to its custom data loaders (see Section [4.1](#S4.SS1 "4.1\.
    Choice of Frameworks ‣ 4\. Experimental Setup ‣ A Survey and Empirical Evaluation
    of Parallel Deep Learning Frameworks")), however, these are a negligible part
    of the overall time per iteration. Across all frameworks, we see that the backward
    pass is more computationally intensive than the forward pass. This is because
    for each layer we not only compute the gradients for its parameters but also for
    its input activations which need to be backpropagated to previous layers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S5.F5 "图 5 ‣ 5.1\. 执行时间比较 ‣ 5\. 比较评估 ‣ 并行深度学习框架的调查与实证评估") 显示了每个框架在ThetaGPU上进行前向传播、反向传播和I/O的时间。我们观察到由于Megatron使用了自定义的数据加载器，其I/O性能有了显著提高（见第[4.1节](#S4.SS1
    "4.1\. 框架选择 ‣ 4\. 实验设置 ‣ 并行深度学习框架的调查与实证评估")），但这些改进在每次迭代的总体时间中占比微乎其微。在所有框架中，我们发现反向传播的计算密集度高于前向传播。这是因为对于每一层，我们不仅需要计算其参数的梯度，还需要计算其输入激活的梯度，这些激活需要反向传播到之前的层。
- en: Single GPU profiles in the figure also highlight the difference in the absolute
    computation time of the forward and backward passes for these frameworks. It further
    supports our above explanation for the differences in sequential performance in
    Figure [4](#S5.F4 "Figure 4 ‣ 5\. Comparative Evaluation ‣ A Survey and Empirical
    Evaluation of Parallel Deep Learning Frameworks").
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的单GPU配置还突出了这些框架在前向和反向传播计算时间上的绝对差异。这进一步支持了我们对图[4](#S5.F4 "Figure 4 ‣ 5\. Comparative
    Evaluation ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")中序列性能差异的解释。
- en: Figures [6](#S5.F6 "Figure 6 ‣ 5.1\. Execution Time Comparison ‣ 5\. Comparative
    Evaluation ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")
    and [7](#S5.F7 "Figure 7 ‣ 5.1\. Execution Time Comparison ‣ 5\. Comparative Evaluation
    ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks") detail
    the results from the performance tests on each machine. We present number of seconds
    per epoch for each neural network as the GPU count increases from 1 to 64.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](#S5.F6 "Figure 6 ‣ 5.1\. Execution Time Comparison ‣ 5\. Comparative Evaluation
    ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")和[7](#S5.F7
    "Figure 7 ‣ 5.1\. Execution Time Comparison ‣ 5\. Comparative Evaluation ‣ A Survey
    and Empirical Evaluation of Parallel Deep Learning Frameworks")详细展示了每台机器上的性能测试结果。我们展示了随着GPU数量从1增加到64时，每个神经网络每个周期所需的秒数。
- en: Across both machines and neural networks we observe two separate trends amongst
    the frameworks. First, DDP, ZeRO, LBANN, AxoNN and Megatron-data all perform similarly
    with only constant deviations from each other. Second, PipeDream and TorchGPipe
    are slower, more erratic, and scale worse than the others. Third, Megatron-intra’s
    speedup seems to plateau when we try to scale it across multiple nodes.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的机器和神经网络上，我们观察到框架之间有两个不同的趋势。首先，DDP、ZeRO、LBANN、AxoNN和Megatron-data的性能相似，仅有恒定的偏差。其次，PipeDream和TorchGPipe比其他方法更慢、更不稳定，并且扩展性较差。第三，Megatron-intra的加速似乎在跨多个节点扩展时趋于平稳。
- en: Within this first trend we observe that ZeRO’s performance trends the same as
    DDP and AxoNN with only 10-15% difference in absolute run time. These variations
    can be attributed to the different mixed precision implementations and ZeRO’s
    memory optimizations. As noted previously in Section [3.1.2](#S3.SS1.SSS2 "3.1.2\.
    Large Models ‣ 3.1\. Data Parallelism ‣ 3\. Literature Survey ‣ A Survey and Empirical
    Evaluation of Parallel Deep Learning Frameworks"), ZeRO reduces the per GPU memory
    footprint of data parallelism at the expense of added communication. However,
    we see that this communication overhead scales the same as standard DDP.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个趋势中，我们观察到ZeRO的性能趋势与DDP和AxoNN相同，绝对运行时间差异仅为10-15%。这些差异可以归因于不同的混合精度实现和ZeRO的内存优化。如在第[3.1.2](#S3.SS1.SSS2
    "3.1.2\. Large Models ‣ 3.1\. Data Parallelism ‣ 3\. Literature Survey ‣ A Survey
    and Empirical Evaluation of Parallel Deep Learning Frameworks")节中所述，ZeRO通过增加通信来减少每个GPU的数据并行内存占用。然而，我们发现这种通信开销与标准DDP的扩展性相同。
- en: It is immediately apparent that these data parallel approaches strongly outperform
    the other frameworks in scaling. This is notably due to the embarrassingly parallel
    workload in data parallelism when the entire model fits within GPU memory. We
    also see an expected slight reduction in speedup on Lassen and ThetaGPU (shown
    in Figure [4](#S5.F4 "Figure 4 ‣ 5\. Comparative Evaluation ‣ A Survey and Empirical
    Evaluation of Parallel Deep Learning Frameworks")) for data parallelism as the
    number of GPUs surpassed that of a single node. This happens as the all-reduce
    communication now occurs outside the fast intra-node NVLink and has to use the
    system network. This is a negligible issue due to how much better the data parallel
    algorithms scale.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这些数据并行方法在扩展性方面显著优于其他框架。这主要是因为当整个模型适合在GPU内存中时，数据并行中的工作负载具有明显的并行性。我们还可以看到，随着GPU数量超过单个节点，在Lassen和ThetaGPU上的加速略有下降（见图[4](#S5.F4
    "Figure 4 ‣ 5\. Comparative Evaluation ‣ A Survey and Empirical Evaluation of
    Parallel Deep Learning Frameworks")）。这是因为全归约通信现在发生在快速的节点间NVLink之外，需要使用系统网络。由于数据并行算法的扩展性更佳，这个问题可以忽略不计。
- en: Due to the lack of mixed precision support, PipeDream and TorchGPipe have the
    largest epoch execution times at all GPU counts across all machines. PipeDream
    seems to scale erratically relative to its own single GPU execution. The poor
    scaling can be attributed to two factors. Firstly, PipeDream uses the relatively
    slow Gloo library as its communication backend. Secondly, erratic scaling is usually
    a sign of load imbalance. Our experiments show that their communication-aware
    load balancing algorithm does not perform satisfactorily in practice.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏混合精度支持，PipeDream 和 TorchGPipe 在所有 GPU 数量下的每轮执行时间最长。相较于自身的单 GPU 执行，PipeDream
    的扩展似乎不稳定。性能不佳可以归因于两个因素。首先，PipeDream 使用相对较慢的 Gloo 库作为其通信后端。其次，不稳定的扩展通常是负载不平衡的标志。我们的实验显示，他们的通信感知负载均衡算法在实践中表现不佳。
- en: Along with these two major trends we also observe that Megatron-intra plateaus
    once it runs on multiple nodes. For larger GPU counts it scales worse than DDP,
    ZeRO and AxoNN. We observed that the communication overhead of Megatron-intra
    increases rapidly with increasing number of GPUs, ultimately reaching 52.5% of
    the total execution time on 16 GPUs. Based on our observations we recommend that
    researchers who wish to train large transformer models on language modeling task
    use Megatron-intra for their single GPU sequential implementations. If the model
    surpasses the memory capacity of a single GPU, we recommend employing Megatron’s
    intra-layer parallelism to fit the model inside the GPUs of a single node. Scaling
    to large GPU counts should be done by integrating Megatron’s intra-layer parallelism
    with data parallelism.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这两个主要趋势外，我们还观察到当 Megatron-intra 在多个节点上运行时会出现平台效应。对于更大的 GPU 数量，它的扩展性能不如 DDP、ZeRO
    和 AxoNN。我们观察到，随着 GPU 数量的增加，Megatron-intra 的通信开销迅速增加，*最终*在 16 个 GPU 上达到了总执行时间的
    52.5%。根据我们的观察，我们建议希望在语言建模任务上训练大型变换器模型的研究人员，使用 Megatron-intra 进行单 GPU 的顺序实现。如果模型超出了单个
    GPU 的内存容量，我们建议采用 Megatron 的层内并行性将模型适配到单节点的 GPU 中。要实现大规模 GPU 扩展，应通过将 Megatron 的层内并行性与数据并行性相结合来完成。
- en: 5.2\. Statistical Efficiency
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 统计效率
- en: Figure [8](#S5.F8 "Figure 8 ‣ 5.2\. Statistical Efficiency ‣ 5\. Comparative
    Evaluation ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")
    illustrates the results of our statistical efficiency experiments. Following standard
    practice we measure the validation accuracy and perplexity at each epoch for the
    image classification and language modeling tasks respectively. We report the epoch
    number as well as the total training time. On observing the performance of PipeDream
    on both the tasks it is apparent that weight staleness is a huge roadblock in
    the path of algorithms that seek to implement pipelining without flushing. PipeDream’s
    proposed weight stashing approach does not mitigate this problem satisfactorily.
    ZeRO, DDP and LBANN exhibit near identical validation curves. The slight variations
    in the validation curves are likely due to differences in the mixed precision
    implementations in these frameworks. TorchGPipe and Megatron-intra exhibit greater
    statistical efficiencies than the data parallel frameworks on the language modeling
    task. We attribute the fast convergence of these frameworks due to their training
    runs being carried out on a small GPU count. The data parallel frameworks being
    trained at 64 GPUs take a slight hit in their convergence speeds due to the problem
    of increase effective mini-batch sizes that we highlighted in Section [3.1.3](#S3.SS1.SSS3
    "3.1.3\. Large Effective Mini-Batch Sizes ‣ 3.1\. Data Parallelism ‣ 3\. Literature
    Survey ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks").
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#S5.F8 "Figure 8 ‣ 5.2\. Statistical Efficiency ‣ 5\. Comparative Evaluation
    ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks") 展示了我们统计效率实验的结果。按照标准实践，我们分别在每轮对图像分类和语言建模任务测量验证准确率和困惑度。我们报告了轮次编号以及总训练时间。在观察
    PipeDream 在这两项任务上的表现时，很明显权重陈旧是寻求实现管道化而不刷新算法的巨大障碍。PipeDream 提出的权重暂存方法没有有效缓解这一问题。ZeRO、DDP
    和 LBANN 的验证曲线几乎相同。验证曲线的轻微差异可能是由于这些框架中混合精度实现的差异。TorchGPipe 和 Megatron-intra 在语言建模任务中展现出了比数据并行框架更高的统计效率。我们将这些框架的快速收敛归因于它们的训练运行在较少的
    GPU 数量上进行。数据并行框架在 64 个 GPU 上训练时，由于我们在第 [3.1.3](#S3.SS1.SSS3 "3.1.3\. Large Effective
    Mini-Batch Sizes ‣ 3.1\. Data Parallelism ‣ 3\. Literature Survey ‣ A Survey and
    Empirical Evaluation of Parallel Deep Learning Frameworks") 节中强调的有效小批量大小增加问题，其收敛速度略有下降。
- en: Figure [8](#S5.F8 "Figure 8 ‣ 5.2\. Statistical Efficiency ‣ 5\. Comparative
    Evaluation ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")
    further details how the accuracies and perplexities behave over time rather than
    epoch. PipeDream is much slower to accuracies than the other frameworks. Such
    a figure presents a combined picture of the statistical efficiency and epoch execution
    times of a framework. We argue that plotting validation metrics against epoch
    times is the best way to evaluate the performance of any distributed deep learning
    framework. It also clearly demonstrates the superiority of data parallelism over
    other classes of parallel deep learning algorithms.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图[8](#S5.F8 "Figure 8 ‣ 5.2\. Statistical Efficiency ‣ 5\. Comparative Evaluation
    ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")进一步详细说明了准确性和困惑度随时间的变化情况，而非随训练轮次变化。PipeDream在达到准确性方面明显比其他框架慢。这种图展示了一个框架的统计效率和轮次执行时间的综合情况。我们认为，将验证指标与轮次时间绘制在一起是评估任何分布式深度学习框架性能的最佳方式。这也清楚地展示了数据并行性优于其他类别的并行深度学习算法。
- en: '![Refer to caption](img/3d290f80c7b1a9386cfda3094096ed8e.png)![Refer to caption](img/91fb588a5f50e95e34ca4319e838a3e3.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3d290f80c7b1a9386cfda3094096ed8e.png)![参见标题](img/91fb588a5f50e95e34ca4319e838a3e3.png)'
- en: Figure 8\. Validation performance by time for training VGG-16 and GPT2-medium
    on ThetaGPU. Epoch numbers are shown in labels.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 在ThetaGPU上训练VGG-16和GPT2-medium的时间验证性能。轮次数字显示在标签中。
- en: 5.3\. Memory Utilization
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 内存利用率
- en: Figure [9](#S5.F9 "Figure 9 ‣ 5.3\. Memory Utilization ‣ 5\. Comparative Evaluation
    ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks") details
    the per GPU memory usage of each framework during the training tasks. ZeRO, while
    having similar performance and scaling to DDP, had between 42% and 66% of the
    memory footprint. We also see this improving as more GPUs are added similar to
    the layer parallel runs, while DDP remains fixed as it simply duplicates the models
    across GPUs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S5.F9 "Figure 9 ‣ 5.3\. Memory Utilization ‣ 5\. Comparative Evaluation
    ‣ A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks")详细展示了每个框架在训练任务中的每个GPU内存使用情况。虽然ZeRO在性能和扩展性方面与DDP相似，但其内存占用在42%到66%之间。我们也看到，随着更多GPU的加入，这种情况有所改善，类似于层级并行运行，而DDP保持固定，因为它仅仅在GPU之间复制模型。
- en: The pipelining implementations both experienced over 2x better memory usage
    with more resources. More of the models were able to be partitioned amongst the
    GPUs. However, the memory savings begin to plateau as more GPUs are added since
    increase in the activation memory due to increasing batch sizes balances out the
    decrease in parameter memory.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 管道化实现的内存使用效率在资源更多的情况下提高了超过2倍。更多的模型能够在GPU之间进行分区。然而，随着GPU数量的增加，内存节省效果开始趋于平稳，因为随着批量大小的增加，激活内存的增加抵消了参数内存的减少。
- en: The U-shaped per GPU memory curve of Megatron can be attributed to the inner
    workings of their intra-layer parallelism implementation. While the computation
    of a transformer layer is divided across multiple GPUs, the output of the last
    layer needs to be present in its entirety on every GPU. Since the per GPU mini-batch
    size is fixed the memory occupied by the input for any layer on each GPU increases
    linearly with an increase in GPU count. At lower GPU counts this increase is offset
    by the decrease in parameter memory due to the division of the layer computation
    across GPUs. After a while, however, the decrease is not enough to completely
    offset the increasing input activation memory.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Megatron的每个GPU内存曲线呈U形可以归因于其层内并行实现的内部机制。虽然一个transformer层的计算被分配到多个GPU上，但最后一层的输出需要在每个GPU上完整存在。由于每个GPU的小批量大小是固定的，因此每个GPU上任何层的输入内存随着GPU数量的增加而线性增长。在较低的GPU数量下，这种增加被由于跨GPU层计算的划分而减少的参数内存所抵消。然而，经过一段时间后，这种减少不足以完全抵消增加的输入激活内存。
- en: '![Refer to caption](img/3c88ba96135a72e9df50f703de6b8630.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3c88ba96135a72e9df50f703de6b8630.png)'
- en: Figure 9\. Memory consumption by different frameworks on ThetaGPU for GPT2-medium.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 不同框架在ThetaGPU上对GPT2-medium的内存消耗。
- en: 6\. Conclusion
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: The increasing size of contemporary neural network architectures has necessitated
    the development of efficient algorithms for parallelizing neural networks. The
    performance of parallel training of neural networks is heavily dependent on the
    algorithm, implementation, hyperparameters, and hardware used. In this paper we
    provide a comprehensive survey of parallel deep learning frameworks that have
    demonstrated scaling on parallel systems. We use two dataset-network combinations
    to study various properties of parallel deep learning frameworks such as scalability,
    memory requirements, and statistical efficiency as a function of performance.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当代神经网络架构的规模不断扩大，这促使了高效并行神经网络算法的开发。神经网络并行训练的性能在很大程度上依赖于算法、实现、超参数和使用的硬件。在本文中，我们提供了对在并行系统上展示扩展性的并行深度学习框架的全面调查。我们使用两个数据集-网络组合来研究并行深度学习框架的各种属性，例如可扩展性、内存需求和性能的统计效率。
- en: Our benchmarking studies presents some interesting observations. When the entire
    model can fit within a single GPU, it is best to use data parallel approaches
    as they perform and scale well. In memory constrained environments, ZeRO (Rajbhandari
    et al., [2020](#bib.bib47)) can save us a decent amount of memory. Their memory
    optimizations only add substantial cost to the computation for non-transformer
    models. For saving more memory we recommend using intra or inter-layer parallelism
    to deploy a model across a few number of GPUs and then scale it in a hybrid fashion
    with data parallelism.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基准测试研究提出了一些有趣的观察结果。当整个模型可以容纳在一个单独的 GPU 中时，最好使用数据并行方法，因为它们表现良好且可扩展。在内存受限的环境中，ZeRO
    (Rajbhandari 等，[2020](#bib.bib47)) 可以节省大量内存。他们的内存优化仅对非变换器模型的计算增加了实质性的成本。为了节省更多内存，我们推荐使用层内或层间并行性将模型部署到少数
    GPU 上，然后通过数据并行的混合方式进行扩展。
- en: References
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,
    Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,
    Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
    Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Machine Learning.
    In *Proceedings of the 12th USENIX Conference on Operating Systems Design and
    Implementation* (Savannah, GA, USA) *(OSDI’16)*. USENIX Association, USA, 265–283.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abadi 等 (2016) Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy
    Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,
    Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
    Yu, 和 Xiaoqiang Zheng. 2016. TensorFlow: 一个大规模机器学习系统。在 *第12届USENIX操作系统设计与实现会议*
    (Savannah, GA, USA) *(OSDI’16)* 上。USENIX协会，美国，265–283。'
- en: Awan et al. (2020) A. A. Awan, A. Jain, C. Chu, H. Subramoni, and D. K. Panda.
    2020. Communication Profiling and Characterization of Deep-Learning Workloads
    on Clusters With High-Performance Interconnects. *IEEE Micro* 40, 1 (2020), 35–43.
    [https://doi.org/10.1109/MM.2019.2949986](https://doi.org/10.1109/MM.2019.2949986)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Awan 等 (2020) A. A. Awan, A. Jain, C. Chu, H. Subramoni, 和 D. K. Panda. 2020.
    集群上深度学习工作负载的通信分析与特征描述。*IEEE Micro* 40, 1 (2020), 35–43. [https://doi.org/10.1109/MM.2019.2949986](https://doi.org/10.1109/MM.2019.2949986)
- en: 'Ben-Nun and Hoefler (2019) Tal Ben-Nun and Torsten Hoefler. 2019. Demystifying
    Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis. *ACM
    Comput. Surv.* 52, 4, Article 65 (Aug. 2019), 43 pages. [https://doi.org/10.1145/3320060](https://doi.org/10.1145/3320060)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben-Nun 和 Hoefler (2019) Tal Ben-Nun 和 Torsten Hoefler. 2019. 揭示并行和分布式深度学习的奥秘：深入的并发分析。*ACM计算机调查*
    52, 4, 第65篇文章 (2019年8月), 43 页。 [https://doi.org/10.1145/3320060](https://doi.org/10.1145/3320060)
- en: Bhatele et al. (2015) Abhinav Bhatele, Andrew R. Titus, Jayaraman J. Thiagarajan,
    Nikhil Jain, Todd Gamblin, Peer-Timo Bremer, Martin Schulz, and Laxmikant V. Kale.
    2015. Identifying the Culprits behind Network Congestion. In *Proceedings of the
    IEEE International Parallel & Distributed Processing Symposium* *(IPDPS ’15)*.
    IEEE Computer Society. [http://doi.ieeecomputersociety.org/10.1109/IPDPS.2015.92](http://doi.ieeecomputersociety.org/10.1109/IPDPS.2015.92)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatele 等 (2015) Abhinav Bhatele, Andrew R. Titus, Jayaraman J. Thiagarajan,
    Nikhil Jain, Todd Gamblin, Peer-Timo Bremer, Martin Schulz, 和 Laxmikant V. Kale.
    2015. 识别网络拥塞的罪魁祸首。在 *IEEE国际并行与分布处理研讨会* *(IPDPS ’15)* 上。IEEE计算机协会。 [http://doi.ieeecomputersociety.org/10.1109/IPDPS.2015.92](http://doi.ieeecomputersociety.org/10.1109/IPDPS.2015.92)
- en: Chen et al. (2019) Chi-Chung Chen, Chia-Lin Yang, and Hsiang-Yun Cheng. 2019.
    Efficient and Robust Parallel DNN Training through Model Parallelism on Multi-GPU
    Platform. arXiv:1809.02839 [cs.DC]
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2019）Chi-Chung Chen、Chia-Lin Yang和Hsiang-Yun Cheng。2019年。通过模型并行在多GPU平台上进行高效且稳健的并行DNN训练。arXiv:1809.02839
    [cs.DC]
- en: Chen et al. (2016) Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
    2016. Revisiting Distributed Synchronous SGD. In *International Conference on
    Learning Representations Workshop Track*. [https://arxiv.org/abs/1604.00981](https://arxiv.org/abs/1604.00981)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2016）陈建民、拉贾特·蒙戈、萨米·本吉奥和拉法尔·约泽夫维茨。2016年。重访分布式同步SGD。在*国际学习表示会议工作坊轨道*。 [https://arxiv.org/abs/1604.00981](https://arxiv.org/abs/1604.00981)
- en: 'Chen et al. (2016) Y. Chen, J. Emer, and V. Sze. 2016. Eyeriss: A Spatial Architecture
    for Energy-Efficient Dataflow for Convolutional Neural Networks. In *2016 ACM/IEEE
    43rd Annual International Symposium on Computer Architecture (ISCA)*. 367–379.
    [https://doi.org/10.1109/ISCA.2016.40](https://doi.org/10.1109/ISCA.2016.40)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2016）Y. 陈、J. Emer和V. Sze。2016年。Eyeriss：一种用于卷积神经网络的节能数据流空间架构。在*2016 ACM/IEEE
    第43届国际计算机体系结构年会（ISCA）*。367–379。 [https://doi.org/10.1109/ISCA.2016.40](https://doi.org/10.1109/ISCA.2016.40)
- en: 'Chen et al. (2017) Y. Chen, T. Krishna, J. S. Emer, and V. Sze. 2017. Eyeriss:
    An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks.
    *IEEE Journal of Solid-State Circuits* 52, 1 (2017), 127–138. [https://doi.org/10.1109/JSSC.2016.2616357](https://doi.org/10.1109/JSSC.2016.2616357)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2017）Y. 陈、T. 克里希纳、J. S. Emer和V. Sze。2017年。Eyeriss：一种节能的可重构加速器，用于深度卷积神经网络。*IEEE
    固态电路期刊* 52, 1（2017年），127–138。 [https://doi.org/10.1109/JSSC.2016.2616357](https://doi.org/10.1109/JSSC.2016.2616357)
- en: 'Chetlur et al. (2014) Sharan Chetlur, Cliff Woolley, Philippe Vandermersch,
    Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: Efficient
    Primitives for Deep Learning. *CoRR* abs/1410.0759 (2014). arXiv:1410.0759 [http://arxiv.org/abs/1410.0759](http://arxiv.org/abs/1410.0759)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chetlur等人（2014）Sharan Chetlur、Cliff Woolley、Philippe Vandermersch、Jonathan Cohen、John
    Tran、Bryan Catanzaro和Evan Shelhamer。2014年。cuDNN：深度学习的高效原语。*CoRR* abs/1410.0759（2014年）。arXiv:1410.0759
    [http://arxiv.org/abs/1410.0759](http://arxiv.org/abs/1410.0759)
- en: 'Chilimbi et al. (2014) Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and
    Karthik Kalyanaraman. 2014. Project Adam: Building an Efficient and Scalable Deep
    Learning Training System. In *11th USENIX Symposium on Operating Systems Design
    and Implementation (OSDI 14)*. USENIX Association, Broomfield, CO, 571–582. [https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi](https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chilimbi等人（2014）Trishul Chilimbi、Yutaka Suzue、Johnson Apacible和Karthik Kalyanaraman。2014年。Project
    Adam：构建一个高效且可扩展的深度学习训练系统。在*第11届USENIX操作系统设计与实现研讨会（OSDI 14）*。USENIX协会，布鲁姆菲尔德，科罗拉多州，571–582。
    [https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi](https://www.usenix.org/conference/osdi14/technical-sessions/presentation/chilimbi)
- en: Coates et al. (2013) Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro,
    and Ng Andrew. 2013. Deep learning with COTS HPC systems *(Proceedings of Machine
    Learning Research, Vol. 28)*, Sanjoy Dasgupta and David McAllester (Eds.). PMLR,
    Atlanta, Georgia, USA, 1337–1345. [http://proceedings.mlr.press/v28/coates13.html](http://proceedings.mlr.press/v28/coates13.html)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coates等人（2013）Adam Coates、Brody Huval、Tao Wang、David Wu、Bryan Catanzaro和Ng Andrew。2013年。使用COTS
    HPC系统进行深度学习 *(机器学习研究会论文集，第28卷)*，Sanjoy Dasgupta和David McAllester（编辑）。PMLR，美国乔治亚州亚特兰大，1337–1345。
    [http://proceedings.mlr.press/v28/coates13.html](http://proceedings.mlr.press/v28/coates13.html)
- en: Dean et al. (2012) Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu
    Devin, Quoc V. Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker,
    Ke Yang, and Andrew Y. Ng. 2012. Large Scale Distributed Deep Networks. In *NIPS*.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dean等人（2012）Jeffrey Dean、Greg S. Corrado、Rajat Monga、Kai Chen、Matthieu Devin、Quoc
    V. Le、Mark Z. Mao、Marc’Aurelio Ranzato、Andrew Senior、Paul Tucker、Ke Yang和Andrew
    Y. Ng。2012年。大规模分布式深度网络。在*NIPS*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*. Association for Computational Linguistics,
    Minneapolis, Minnesota, 4171–4186. [https://doi.org/10.18653/v1/N19-1423](https://doi.org/10.18653/v1/N19-1423)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. BERT: 用于语言理解的深度双向变换器的预训练。发表于*2019年北美计算语言学协会会议论文集：人类语言技术，第1卷（长短篇论文）*。计算语言学协会，明尼阿波利斯,
    明尼苏达州, 4171–4186。 [https://doi.org/10.18653/v1/N19-1423](https://doi.org/10.18653/v1/N19-1423)'
- en: Dryden et al. (2019a) Nikoli Dryden, Naoya Maruyama, Tim Moon, Tom Benson, Marc
    Snir, and Brian Van Essen. 2019a. Channel and Filter Parallelism for Large-Scale
    CNN Training. In *Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis* (Denver, Colorado) *(SC ’19)*. Association
    for Computing Machinery, New York, NY, USA, Article 10, 20 pages. [https://doi.org/10.1145/3295500.3356207](https://doi.org/10.1145/3295500.3356207)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dryden et al. (2019a) Nikoli Dryden, Naoya Maruyama, Tim Moon, Tom Benson, Marc
    Snir, 和 Brian Van Essen. 2019a. 大规模CNN训练的通道和滤波器并行性。发表于*国际高性能计算、网络、存储与分析会议论文集*（丹佛,
    科罗拉多州）*(SC ’19)*。计算机协会, 纽约, NY, USA，第10篇文章，20页。 [https://doi.org/10.1145/3295500.3356207](https://doi.org/10.1145/3295500.3356207)
- en: Dryden et al. (2019b) Nikoli Dryden, Naoya Maruyama, Tim Moon, Tom Benson, Marc
    Snir, and Brian Van Essen. 2019b. Channel and Filter Parallelism for Large-Scale
    CNN Training. In *Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis* (Denver, Colorado) *(SC ’19)*. Association
    for Computing Machinery, New York, NY, USA, Article 10, 20 pages. [https://doi.org/10.1145/3295500.3356207](https://doi.org/10.1145/3295500.3356207)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dryden et al. (2019b) Nikoli Dryden, Naoya Maruyama, Tim Moon, Tom Benson, Marc
    Snir, 和 Brian Van Essen. 2019b. 大规模CNN训练的通道和滤波器并行性。发表于*国际高性能计算、网络、存储与分析会议论文集*（丹佛,
    科罗拉多州）*(SC ’19)*。计算机协会, 纽约, NY, USA，第10篇文章，20页。 [https://doi.org/10.1145/3295500.3356207](https://doi.org/10.1145/3295500.3356207)
- en: 'Essen et al. (2015) Brian Van Essen, Hyojin Kim, Roger A. Pearce, Kofi Boakye,
    and Barry Chen. 2015. LBANN: livermore big artificial neural network HPC toolkit.
    In *Proceedings of the Workshop on Machine Learning in High-Performance Computing
    Environments, MLHPC 2015, Austin, Texas, USA, November 15, 2015*. ACM, 5:1–5:6.
    [https://doi.org/10.1145/2834892.2834897](https://doi.org/10.1145/2834892.2834897)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Essen et al. (2015) Brian Van Essen, Hyojin Kim, Roger A. Pearce, Kofi Boakye,
    和 Barry Chen. 2015. LBANN: Livermore大规模人工神经网络HPC工具包。发表于*高性能计算环境中的机器学习研讨会论文集，MLHPC
    2015，奥斯丁，德克萨斯州，美国，2015年11月15日*。ACM，第5:1–5:6。 [https://doi.org/10.1145/2834892.2834897](https://doi.org/10.1145/2834892.2834897)'
- en: 'Goyal et al. (2017) Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis,
    Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
    2017. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. *CoRR* abs/1706.02677
    (2017). arXiv:1706.02677 [http://arxiv.org/abs/1706.02677](http://arxiv.org/abs/1706.02677)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal et al. (2017) Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis,
    Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, 和 Kaiming He. 2017.
    精确的大规模小批量SGD：1小时内训练ImageNet。*CoRR* abs/1706.02677 (2017). arXiv:1706.02677 [http://arxiv.org/abs/1706.02677](http://arxiv.org/abs/1706.02677)
- en: 'Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and
    zhifeng Chen. 2019. GPipe: Efficient Training of Giant Neural Networks using Pipeline
    Parallelism. In *Advances in Neural Information Processing Systems*, H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox, and R. Garnett (Eds.),
    Vol. 32\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, 和 zhifeng
    Chen. 2019. GPipe: 使用管道并行性高效训练大型神经网络。发表于*神经信息处理系统进展*，H. Wallach, H. Larochelle,
    A. Beygelzimer, F. d''Alché-Buc, E. Fox, 和 R. Garnett (编辑)，第32卷。Curran Associates,
    Inc. [https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf)'
- en: Islam et al. (2016) Tanzima Z. Islam, Jayaraman J. Thiagarajan, Abhinav Bhatele,
    Martin Schulz, and Todd Gamblin. 2016. A Machine Learning Framework for Performance
    Coverage Analysis of Proxy Applications. In *Proceedings of the ACM/IEEE International
    Conference for High Performance Computing, Networking, Storage and Analysis* *(SC
    ’16)*. IEEE Computer Society. [http://doi.ieeecomputersociety.org/10.1109/SC.2016.45](http://doi.ieeecomputersociety.org/10.1109/SC.2016.45)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam 等（2016）Tanzima Z. Islam, Jayaraman J. Thiagarajan, Abhinav Bhatele, Martin
    Schulz 和 Todd Gamblin. 2016. 用于代理应用性能覆盖分析的机器学习框架。发表于*ACM/IEEE国际高性能计算、网络、存储与分析会议论文集*（*SC
    ’16*）。IEEE计算机学会。[http://doi.ieeecomputersociety.org/10.1109/SC.2016.45](http://doi.ieeecomputersociety.org/10.1109/SC.2016.45)
- en: 'Jain et al. (2020) Arpan Jain, Ammar Ahmad Awan, Asmaa M. Aljuhani, Jahanzeb Maqbool
    Hashmi, Quentin G. Anthony, Hari Subramoni, Dhableswar K. Panda, Raghu Machiraju,
    and Anil Parwani. 2020. GEMS: GPU-ENabled MEmory-Aware Model-Parallelism SYstem
    for Distributed DNN Training. In *Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis* (Atlanta, Georgia)
    *(SC ’20)*. IEEE Press, Article 45, 15 pages.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等（2020）Arpan Jain, Ammar Ahmad Awan, Asmaa M. Aljuhani, Jahanzeb Maqbool
    Hashmi, Quentin G. Anthony, Hari Subramoni, Dhableswar K. Panda, Raghu Machiraju
    和 Anil Parwani. 2020. GEMS：一个支持内存感知的 GPU-ENabled 模型并行系统用于分布式 DNN 训练。发表于*国际高性能计算、网络、存储与分析会议论文集*（乔治亚州亚特兰大）（*SC
    ’20*）。IEEE出版社，第45篇文章，15页。
- en: Jain et al. (2013) Nikhil Jain, Abhinav Bhatele, Michael P. Robson, Todd Gamblin,
    and Laxmikant V. Kale. 2013. Predicting application performance using supervised
    learning on communication features. In *ACM/IEEE International Conference for
    High Performance Computing, Networking, Storage and Analysis* *(SC ’13)*. IEEE
    Computer Society.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等（2013）Nikhil Jain, Abhinav Bhatele, Michael P. Robson, Todd Gamblin 和
    Laxmikant V. Kale. 2013. 使用通信特征的监督学习预测应用性能。发表于*ACM/IEEE国际高性能计算、网络、存储与分析会议*（*SC
    ’13*）。IEEE计算机学会。
- en: Jia et al. (2019) Zhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data
    and Model Parallelism for Deep Neural Networks.. In *Proceedings of Machine Learning
    and Systems*, A. Talwalkar, V. Smith, and M. Zaharia (Eds.), Vol. 1\. 1–13. [https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf](https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等（2019）Zhihao Jia, Matei Zaharia 和 Alex Aiken. 2019. 超越数据和模型并行性以实现深度神经网络。发表于*机器学习与系统会议论文集*，A.
    Talwalkar, V. Smith 和 M. Zaharia（编），第1卷，第1–13页。[https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf](https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf)
- en: 'Kao and Krishna (2020) Sheng-Chun Kao and Tushar Krishna. 2020. GAMMA: Automating
    the HW Mapping of DNN Models on Accelerators via Genetic Algorithm. In *Proceedings
    of the 39th International Conference on Computer-Aided Design* (Virtual Event,
    USA) *(ICCAD ’20)*. Association for Computing Machinery, New York, NY, USA, Article
    44, 9 pages. [https://doi.org/10.1145/3400302.3415639](https://doi.org/10.1145/3400302.3415639)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kao 和 Krishna（2020）Sheng-Chun Kao 和 Tushar Krishna. 2020. GAMMA：通过遗传算法自动化 DNN
    模型在加速器上的硬件映射。发表于*第39届国际计算机辅助设计会议论文集*（虚拟会议，美国）（*ICCAD ’20*）。计算机协会，纽约，NY，美国，第44篇文章，9页。[https://doi.org/10.1145/3400302.3415639](https://doi.org/10.1145/3400302.3415639)
- en: 'Khan et al. (2019) Jehandad Khan, Paul Fultz, Artem Tamazov, Daniel Lowell,
    Chao Liu, Michael Melesse, Murali Nandhimandalam, Kamil Nasyrov, Ilya Perminov,
    Tejash Shah, Vasilii Filippov, Jing Zhang, Jing Zhou, Bragadeesh Natarajan, and
    Mayank Daga. 2019. MIOpen: An Open Source Library For Deep Learning Primitives.
    arXiv:1910.00078 [cs.LG]'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等（2019）Jehandad Khan, Paul Fultz, Artem Tamazov, Daniel Lowell, Chao Liu,
    Michael Melesse, Murali Nandhimandalam, Kamil Nasyrov, Ilya Perminov, Tejash Shah,
    Vasilii Filippov, Jing Zhang, Jing Zhou, Bragadeesh Natarajan 和 Mayank Daga. 2019.
    MIOpen：一个深度学习原语的开源库。arXiv:1910.00078 [cs.LG]
- en: 'Kim et al. (2020) Chiheon Kim, Heungsub Lee, Myungryong Jeong, Woonhyuk Baek,
    Boogeon Yoon, Ildoo Kim, Sungbin Lim, and Sungwoong Kim. 2020. torchgpipe: On-the-fly
    Pipeline Parallelism for Training Giant Models. (2020). arXiv:2004.09910'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2020）Chiheon Kim, Heungsub Lee, Myungryong Jeong, Woonhyuk Baek, Boogeon
    Yoon, Ildoo Kim, Sungbin Lim 和 Sungwoong Kim. 2020. torchgpipe：用于训练大型模型的即时流水线并行性。（2020）。arXiv:2004.09910
- en: 'Kim et al. (2016) D. Kim, J. Kung, S. Chai, S. Yalamanchili, and S. Mukhopadhyay.
    2016. Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density
    3D Memory. In *2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture
    (ISCA)*. 380–392. [https://doi.org/10.1109/ISCA.2016.41](https://doi.org/10.1109/ISCA.2016.41)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2016）D. Kim、J. Kung、S. Chai、S. Yalamanchili 和 S. Mukhopadhyay。2016。《Neurocube：一种可编程的数字神经形态架构，具有高密度3D存储器》。见于
    *2016 ACM/IEEE 第43届年度国际计算机架构研讨会（ISCA）*。380–392。 [https://doi.org/10.1109/ISCA.2016.41](https://doi.org/10.1109/ISCA.2016.41)
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method
    for Stochastic Optimization. In *3rd International Conference on Learning Representations,
    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*, Yoshua
    Bengio and Yann LeCun (Eds.). [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba（2015）Diederik P. Kingma 和 Jimmy Ba。2015。《Adam：一种随机优化方法》。见于 *第3届国际学习表征会议，ICLR
    2015，加州圣地亚哥，美国，2015年5月7-9日，会议论文集*，Yoshua Bengio 和 Yann LeCun（编辑）。 [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980)
- en: 'Kolesnikov et al. (2020) Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
    Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. 2020. Big Transfer
    (BiT): General Visual Representation Learning. In *Computer Vision – ECCV 2020*,
    Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer
    International Publishing, Cham, 491–507.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolesnikov 等（2020）Alexander Kolesnikov、Lucas Beyer、Xiaohua Zhai、Joan Puigcerver、Jessica
    Yung、Sylvain Gelly 和 Neil Houlsby。2020。《大迁移（BiT）：通用视觉表征学习》。见于 *计算机视觉 – ECCV 2020*，Andrea
    Vedaldi、Horst Bischof、Thomas Brox 和 Jan-Michael Frahm（编辑）。Springer International
    Publishing，Cham，491–507。
- en: Krizhevsky et al. (2017) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
    2017. ImageNet Classification with Deep Convolutional Neural Networks. *Commun.
    ACM* 60, 6 (May 2017), 84–90. [https://doi.org/10.1145/3065386](https://doi.org/10.1145/3065386)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2017）Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E. Hinton。2017。《使用深度卷积神经网络进行ImageNet分类》。*Commun.
    ACM* 60, 6（2017年5月），84–90。 [https://doi.org/10.1145/3065386](https://doi.org/10.1145/3065386)
- en: 'Kwon et al. (2020) H. Kwon, P. Chatarasi, V. Sarkar, T. Krishna, M. Pellauer,
    and A. Parashar. 2020. MAESTRO: A Data-Centric Approach to Understand Reuse, Performance,
    and Hardware Cost of DNN Mappings. *IEEE Micro* 40, 3 (2020), 20–29. [https://doi.org/10.1109/MM.2020.2985963](https://doi.org/10.1109/MM.2020.2985963)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等（2020）H. Kwon、P. Chatarasi、V. Sarkar、T. Krishna、M. Pellauer 和 A. Parashar。2020。《MAESTRO：一种以数据为中心的方法来理解DNN映射的重用、性能和硬件成本》。*IEEE
    Micro* 40, 3（2020年），20–29。 [https://doi.org/10.1109/MM.2020.2985963](https://doi.org/10.1109/MM.2020.2985963)
- en: 'Li et al. (2020) Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis,
    Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala.
    2020. PyTorch Distributed: Experiences on Accelerating Data Parallel Training.
    *Proc. VLDB Endow.* 13, 12 (Aug. 2020), 3005–3018. [https://doi.org/10.14778/3415478.3415530](https://doi.org/10.14778/3415478.3415530)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Shen Li、Yanli Zhao、Rohan Varma、Omkar Salpekar、Pieter Noordhuis、Teng
    Li、Adam Paszke、Jeff Smith、Brian Vaughan、Pritam Damania 和 Soumith Chintala。2020。《PyTorch
    Distributed：加速数据并行训练的经验》。*Proc. VLDB Endow.* 13, 12（2020年8月），3005–3018。 [https://doi.org/10.14778/3415478.3415530](https://doi.org/10.14778/3415478.3415530)
- en: 'Lu et al. (2018) X. Lu, H. Shi, R. Biswas, M. H. Javed, and D. K. Panda. 2018.
    DLoBD: A Comprehensive Study of Deep Learning over Big Data Stacks on HPC Clusters.
    *IEEE Transactions on Multi-Scale Computing Systems* 4, 4 (2018), 635–648. [https://doi.org/10.1109/TMSCS.2018.2845886](https://doi.org/10.1109/TMSCS.2018.2845886)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2018）X. Lu、H. Shi、R. Biswas、M. H. Javed 和 D. K. Panda。2018。《DLoBD：对HPC集群上大数据堆栈中的深度学习的综合研究》。*IEEE
    多尺度计算系统学报* 4, 4（2018年），635–648。 [https://doi.org/10.1109/TMSCS.2018.2845886](https://doi.org/10.1109/TMSCS.2018.2845886)
- en: Ma et al. (2017) Yufei Ma, Yu Cao, Sarma Vrudhula, and Jae-sun Seo. 2017. Optimizing
    Loop Operation and Dataflow in FPGA Acceleration of Deep Convolutional Neural
    Networks. In *Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable
    Gate Arrays* (Monterey, California, USA) *(FPGA ’17)*. Association for Computing
    Machinery, New York, NY, USA, 45–54. [https://doi.org/10.1145/3020078.3021736](https://doi.org/10.1145/3020078.3021736)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2017）Yufei Ma、Yu Cao、Sarma Vrudhula 和 Jae-sun Seo。2017。《优化FPGA加速深度卷积神经网络中的循环操作和数据流》。见于
    *2017年ACM/SIGDA国际现场可编程门阵列研讨会*（加利福尼亚州蒙特雷，美国） *(FPGA ’17)*。计算机协会，纽约，NY，美国，45–54。
    [https://doi.org/10.1145/3020078.3021736](https://doi.org/10.1145/3020078.3021736)
- en: Marathe et al. (2017) Aniruddha Marathe, Rushil Anirudh, Nikhil Jain, Abhinav
    Bhatele, Jayaraman Thiagarajan, Bhavya Kailkhura, Jae-Seung Yeom, Barry Rountree,
    and Todd Gamblin. 2017. Performance Modeling under Resource Constraints Using
    Deep Transfer Learning. In *Proceedings of the ACM/IEEE International Conference
    for High Performance Computing, Networking, Storage and Analysis* *(SC ’17)*.
    IEEE Computer Society. [http://doi.acm.org/10.1145/3126908.3126969](http://doi.acm.org/10.1145/3126908.3126969)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marathe 等人（2017）**阿尼鲁德·马拉特**、**鲁希尔·阿尼鲁德**、**尼基尔·贾因**、**阿宾纳夫·巴泰尔**、**贾亚拉曼·提亚加拉詹**、**巴夫雅·凯尔库拉**、**崔圣·叶**、**巴里·朗特里**
    和 **托德·甘布林**。2017。《在资源约束下使用深度迁移学习进行性能建模》。在 *ACM/IEEE 高性能计算、网络、存储和分析国际会议论文集* *(SC
    ’17)*。IEEE 计算机学会。 [http://doi.acm.org/10.1145/3126908.3126969](http://doi.acm.org/10.1145/3126908.3126969)
- en: Mattson et al. (2019) Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos,
    Paulius Micikevicius, David A. Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis,
    Victor Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim M.
    Hazelwood, Andrew Hock, Xinyuan Huang, Bill Jia, Daniel Kang, David Kanter, Naveen
    Kumar, Jeffery Liao, Guokai Ma, Deepak Narayanan, Tayo Oguntebi, Gennady Pekhimenko,
    Lillian Pentecost, Vijay Janapa Reddi, Taylor Robie, Tom St. John, Carole-Jean
    Wu, Lingjie Xu, Cliff Young, and Matei Zaharia. 2019. MLPerf Training Benchmark.
    *CoRR* abs/1910.01500 (2019). arXiv:1910.01500 [http://arxiv.org/abs/1910.01500](http://arxiv.org/abs/1910.01500)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mattson 等人（2019）**彼得·马特森**、**克里斯汀·程**、**科迪·科尔曼**、**格雷戈里·迪阿莫斯**、**保利乌斯·米西凯维丘斯**、**大卫·A·帕特森**、**韩林·唐**、**桂延·魏**、**彼得·贝利斯**、**维克托·比托夫**、**大卫·布鲁克斯**、**德豪·陈**、**德博伊约提·杜塔**、**乌迪特·古普塔**、**金·M·哈泽尔伍德**、**安德鲁·霍克**、**辛元·黄**、**比尔·贾**、**丹尼尔·康**、**大卫·坎特**、**纳维恩·库玛**、**杰弗里·廖**、**郭凯·马**、**迪帕克·纳拉延**、**塔约·奥贡特比**、**格纳迪·佩希门科**、**莉莲·彭蒂科斯特**、**维贾·贾纳帕·雷迪**、**泰勒·罗比**、**汤姆·圣约翰**、**卡罗尔-简·吴**、**凌杰·徐**、**克利夫·杨**
    和 **马泰伊·扎哈里亚**。2019。《MLPerf 训练基准》。*CoRR* abs/1910.01500 (2019)。arXiv:1910.01500
    [http://arxiv.org/abs/1910.01500](http://arxiv.org/abs/1910.01500)
- en: Menon et al. (2020) Harshitha Menon, Abhinav Bhatele, and Todd Gamblin. 2020.
    Auto-Tuning Parameter Choices using Bayesian Optimization. In *Proceedings of
    the IEEE International Parallel & Distributed Processing Symposium* *(IPDPS ’20)*.
    IEEE Computer Society.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menon 等人（2020）**哈希塔·梅农**、**阿宾纳夫·巴泰尔** 和 **托德·甘布林**。2020。《使用贝叶斯优化进行自动调优参数选择》。在
    *IEEE 国际并行与分布处理研讨会论文集* *(IPDPS ’20)*。IEEE 计算机学会。
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. Pointer Sentinel Mixture Models. *CoRR* abs/1609.07843 (2016). arXiv:1609.07843
    [http://arxiv.org/abs/1609.07843](http://arxiv.org/abs/1609.07843)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人（2016）**斯蒂芬·梅里蒂**、**蔡明熊**、**詹姆斯·布拉德伯里** 和 **理查德·索彻**。2016。《指针守卫混合模型》。*CoRR*
    abs/1609.07843 (2016)。arXiv:1609.07843 [http://arxiv.org/abs/1609.07843](http://arxiv.org/abs/1609.07843)
- en: Micikevicius et al. (2018) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii
    Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed Precision Training. In *International
    Conference on Learning Representations*. [https://openreview.net/forum?id=r1gs9JgRZ](https://openreview.net/forum?id=r1gs9JgRZ)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius 等人（2018）**保利乌斯·米西凯维丘斯**、**沙兰·纳朗**、**乔纳·阿尔本**、**格雷戈里·迪阿莫斯**、**埃里希·埃尔森**、**大卫·加西亚**、**鲍里斯·金斯堡**、**迈克尔·休斯顿**、**奥列克斯·库查耶夫**、**加内什·文卡特什**
    和 **郝武**。2018。《混合精度训练》。在 *国际学习表征会议*。 [https://openreview.net/forum?id=r1gs9JgRZ](https://openreview.net/forum?id=r1gs9JgRZ)
- en: 'Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil Devanur, Greg Granger, Phil Gibbons, and Matei Zaharia. 2019.
    PipeDream: Generalized Pipeline Parallelism for DNN Training. In *ACM Symposium
    on Operating Systems Principles (SOSP 2019)*. [https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/](https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayanan 等人（2019）**迪帕克·纳拉延**、**亚伦·哈拉普**、**阿玛尔·帕尼沙耶**、**维韦克·塞沙德里**、**尼基尔·德万努尔**、**格雷格·格兰杰**、**菲尔·吉本斯**
    和 **马泰伊·扎哈里亚**。2019。《PipeDream：DNN 训练的广义管道并行性》。在 *ACM 操作系统原理研讨会 (SOSP 2019)*。
    [https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/](https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/)
- en: Nikolentzos et al. (2020) Giannis Nikolentzos, Antoine Tixier, and Michalis
    Vazirgiannis. 2020. Message Passing Attention Networks for Document Understanding.
    *Proceedings of the AAAI Conference on Artificial Intelligence* 34, 05 (Apr. 2020),
    8544–8551. [https://doi.org/10.1609/aaai.v34i05.6376](https://doi.org/10.1609/aaai.v34i05.6376)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nikolentzos 等人（2020）**吉安尼斯·尼科伦特佐斯**、**安托万·蒂克斯耶** 和 **米哈利斯·瓦齐吉安尼斯**。2020。《用于文档理解的消息传递注意力网络》。*AAAI
    人工智能会议论文集* 34, 05 (2020年4月)，8544–8551。 [https://doi.org/10.1609/aaai.v34i05.6376](https://doi.org/10.1609/aaai.v34i05.6376)
- en: 'Oyama et al. (2020) Yosuke Oyama, Naoya Maruyama, Nikoli Dryden, Erin McCarthy,
    Peter Harrington, Jan Balewski, Satoshi Matsuoka, Peter Nugent, and Brian Van
    Essen. 2020. The Case for Strong Scaling in Deep Learning: Training Large 3D CNNs
    with Hybrid Parallelism. arXiv:2007.12856 [cs.DC]'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oyama 等 (2020) Yosuke Oyama, Naoya Maruyama, Nikoli Dryden, Erin McCarthy, Peter
    Harrington, Jan Balewski, Satoshi Matsuoka, Peter Nugent, 和 Brian Van Essen. 2020.
    深度学习中的强缩放案例：使用混合并行训练大型 3D CNNs. arXiv:2007.12856 [cs.DC]
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
    Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
    Library. In *Advances in Neural Information Processing Systems*, H. Wallach, H. Larochelle,
    A. Beygelzimer, F. d''Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32\. Curran
    Associates, Inc. [https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke 等 (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, 和 Soumith
    Chintala. 2019. PyTorch: 一种命令式风格的高性能深度学习库。见 *神经信息处理系统进展*, H. Wallach, H. Larochelle,
    A. Beygelzimer, F. d''Alché-Buc, E. Fox, 和 R. Garnett (编), 第32卷. Curran Associates,
    Inc. [https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf)'
- en: 'Pouyanfar et al. (2018) Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian,
    Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, and S. S. Iyengar.
    2018. A Survey on Deep Learning: Algorithms, Techniques, and Applications. *ACM
    Comput. Surv.* 51, 5, Article 92 (Sept. 2018), 36 pages. [https://doi.org/10.1145/3234150](https://doi.org/10.1145/3234150)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pouyanfar 等 (2018) Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong
    Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, 和 S. S. Iyengar. 2018.
    深度学习调查：算法、技术和应用。*ACM 计算机调查* 51, 5, 文章92 (2018年9月), 36页. [https://doi.org/10.1145/3234150](https://doi.org/10.1145/3234150)
- en: Qian (1999) Ning Qian. 1999. On the momentum term in gradient descent learning
    algorithms. *Neural Networks* 12, 1 (1999), 145–151. [https://doi.org/10.1016/S0893-6080(98)00116-6](https://doi.org/10.1016/S0893-6080(98)00116-6)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钱宁 (1999) 钱宁. 1999. 关于梯度下降学习算法中的动量项。*神经网络* 12, 1 (1999), 145–151. [https://doi.org/10.1016/S0893-6080(98)00116-6](https://doi.org/10.1016/S0893-6080(98)00116-6)
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.
    (2019).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei,
    和 Ilya Sutskever. 2019. 语言模型是无监督的多任务学习者。(2019).
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. 2020. ZeRO: Memory Optimizations toward Training Trillion Parameter
    Models. In *Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis* (Atlanta, Georgia) *(SC ’20)*. IEEE Press, Article
    20, 16 pages.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajbhandari 等 (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, 和 Yuxiong
    He. 2020. ZeRO：面向训练万亿参数模型的内存优化。见 *国际高性能计算、网络、存储与分析会议论文集* (乔治亚州亚特兰大) *(SC ’20)*.
    IEEE Press, 文章20, 16页.
- en: 'Rajbhandari et al. (2021) Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley,
    Shaden Smith, and Yuxiong He. 2021. ZeRO-Infinity: Breaking the GPU Memory Wall
    for Extreme Scale Deep Learning *(SC ’21)*. Association for Computing Machinery,
    New York, NY, USA, Article 59, 14 pages. [https://doi.org/10.1145/3458817.3476205](https://doi.org/10.1145/3458817.3476205)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari 等 (2021) Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden
    Smith, 和 Yuxiong He. 2021. ZeRO-Infinity: 打破 GPU 内存瓶颈以实现极大规模深度学习 *(SC ’21)*. 计算机协会,
    纽约, NY, USA, 文章59, 14页. [https://doi.org/10.1145/3458817.3476205](https://doi.org/10.1145/3458817.3476205)'
- en: Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le.
    2019. Regularized Evolution for Image Classifier Architecture Search. *Proceedings
    of the AAAI Conference on Artificial Intelligence* 33, 01 (Jul. 2019), 4780–4789.
    [https://doi.org/10.1609/aaai.v33i01.33014780](https://doi.org/10.1609/aaai.v33i01.33014780)
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real 等 (2019) Esteban Real, Alok Aggarwal, Yanping Huang, 和 Quoc V. Le. 2019.
    图像分类器架构搜索的正则化进化。*AAAI 人工智能会议论文集* 33, 01 (2019年7月), 4780–4789. [https://doi.org/10.1609/aaai.v33i01.33014780](https://doi.org/10.1609/aaai.v33i01.33014780)
- en: 'Recht et al. (2011) Benjamin Recht, Christopher Re, Stephen Wright, and Feng
    Niu. 2011. Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient
    Descent. In *Advances in Neural Information Processing Systems*, J. Shawe-Taylor,
    R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger (Eds.), Vol. 24\. Curran
    Associates, Inc. [https://proceedings.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Recht 等 (2011) Benjamin Recht, Christopher Re, Stephen Wright 和 Feng Niu. 2011.
    Hogwild!: 一种无锁并行化随机梯度下降的方法。在 *神经信息处理系统进展* 中，J. Shawe-Taylor, R. Zemel, P. Bartlett,
    F. Pereira 和 K. Q. Weinberger (编), 第 24 卷。Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf)'
- en: 'Rhu et al. (2016) Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar,
    and Stephen W. Keckler. 2016. VDNN: Virtualized Deep Neural Networks for Scalable,
    Memory-Efficient Neural Network Design. In *The 49th Annual IEEE/ACM International
    Symposium on Microarchitecture* (Taipei, Taiwan) *(MICRO-49)*. IEEE Press, Article
    18, 13 pages.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rhu 等 (2016) Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar
    和 Stephen W. Keckler. 2016. VDNN: 用于可扩展、内存高效的神经网络设计的虚拟化深度神经网络。在 *第49届IEEE/ACM国际微架构研讨会*
    (台北, 台湾) *(MICRO-49)*。IEEE Press, 第 18 篇，13 页。'
- en: Russakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
    Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
    Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual
    Recognition Challenge. *International Journal of Computer Vision (IJCV)* 115,
    3 (2015), 211–252. [https://doi.org/10.1007/s11263-015-0816-y](https://doi.org/10.1007/s11263-015-0816-y)
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russakovsky 等 (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
    Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
    Alexander C. Berg 和 Li Fei-Fei. 2015. ImageNet 大规模视觉识别挑战。*计算机视觉国际期刊 (IJCV)* 115,
    3 (2015), 211–252. [https://doi.org/10.1007/s11263-015-0816-y](https://doi.org/10.1007/s11263-015-0816-y)
- en: 'Sergeev and Balso (2018) Alexander Sergeev and Mike Del Balso. 2018. Horovod:
    fast and easy distributed deep learning in TensorFlow. arXiv:1802.05799 [cs.LG]'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sergeev 和 Balso (2018) Alexander Sergeev 和 Mike Del Balso. 2018. Horovod: 在
    TensorFlow 中快速而简便的分布式深度学习。arXiv:1802.05799 [cs.LG]'
- en: 'Shazeer et al. (2018) Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran,
    Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng
    Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. 2018. Mesh-TensorFlow: Deep
    Learning for Supercomputers. In *Advances in Neural Information Processing Systems*,
    S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
    (Eds.), Vol. 31. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2018/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shazeer 等 (2018) Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish
    Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong,
    Cliff Young, Ryan Sepassi, 和 Blake Hechtman. 2018. Mesh-TensorFlow: 超级计算机的深度学习。在
    *神经信息处理系统进展* 中，S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi
    和 R. Garnett (编), 第 31 卷。Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2018/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/3a37abdeefe1dab1b30f7c5c7e581b93-Paper.pdf)'
- en: 'Shoeybi et al. (2020) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, and Bryan Catanzaro. 2020. Megatron-LM: Training Multi-Billion
    Parameter Language Models Using Model Parallelism. arXiv:1909.08053 [cs.CL]'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shoeybi 等 (2020) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,
    Jared Casper 和 Bryan Catanzaro. 2020. Megatron-LM: 使用模型并行训练多亿参数语言模型。arXiv:1909.08053
    [cs.CL]'
- en: Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. Very
    Deep Convolutional Networks for Large-Scale Image Recognition. In *3rd International
    Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
    2015, Conference Track Proceedings*, Yoshua Bengio and Yann LeCun (Eds.). [http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman (2015) Karen Simonyan 和 Andrew Zisserman. 2015. 用于大规模图像识别的非常深的卷积网络。在
    *第三届国际学习表示会议，ICLR 2015，圣地亚哥，加州，美国，2015年5月7-9日，会议论文集* 中，Yoshua Bengio 和 Yann LeCun
    (编)。 [http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556)
- en: 'Singh and Bhatele (pear) Siddharth Singh and Abhinav Bhatele. 2022 (to appear).
    AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep
    learning. In *Proceedings of the IEEE International Parallel & Distributed Processing
    Symposium* *(IPDPS ’22)*. IEEE Computer Society.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 和 Bhatele (2022) Siddharth Singh 和 Abhinav Bhatele. 2022 (待发表). AxoNN:
    一种异步、消息驱动的极大规模深度学习并行框架。在 *IEEE 国际并行与分布式处理研讨会* *(IPDPS ’22)* 上。IEEE 计算机协会。'
- en: 'Song et al. (2019) L. Song, J. Mao, Y. Zhuo, X. Qian, H. Li, and Y. Chen. 2019.
    HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array. In *2019
    IEEE International Symposium on High Performance Computer Architecture (HPCA)*.
    56–68. [https://doi.org/10.1109/HPCA.2019.00027](https://doi.org/10.1109/HPCA.2019.00027)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等人（2019）**L. Song**、**J. Mao**、**Y. Zhuo**、**X. Qian**、**H. Li** 和 **Y.
    Chen**。2019。《HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array》。在
    *2019 IEEE高性能计算架构国际研讨会* (HPCA)。56–68。 [https://doi.org/10.1109/HPCA.2019.00027](https://doi.org/10.1109/HPCA.2019.00027)'
- en: 'Steuwer et al. (2017) Michel Steuwer, Toomas Remmelg, and Christophe Dubach.
    2017. Lift: A Functional Data-Parallel IR for High-Performance GPU Code Generation.
    In *Proceedings of the 2017 International Symposium on Code Generation and Optimization*
    (Austin, USA) *(CGO ’17)*. IEEE Press, 74–85.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Steuwer 等人（2017）**Michel Steuwer**、**Toomas Remmelg** 和 **Christophe Dubach**。2017。《Lift:
    A Functional Data-Parallel IR for High-Performance GPU Code Generation》。在 *2017年国际代码生成与优化研讨会*（美国奥斯丁）
    *(CGO ’17)*。IEEE出版社，74–85。'
- en: 'Sze et al. (2017) V. Sze, Y. Chen, T. Yang, and J. S. Emer. 2017. Efficient
    Processing of Deep Neural Networks: A Tutorial and Survey. *Proc. IEEE* 105, 12
    (2017), 2295–2329. [https://doi.org/10.1109/JPROC.2017.2761740](https://doi.org/10.1109/JPROC.2017.2761740)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sze 等人（2017）**V. Sze**、**Y. Chen**、**T. Yang** 和 **J. S. Emer**。2017。《Efficient
    Processing of Deep Neural Networks: A Tutorial and Survey》。*Proc. IEEE* 105，第12期（2017年），2295–2329。
    [https://doi.org/10.1109/JPROC.2017.2761740](https://doi.org/10.1109/JPROC.2017.2761740)'
- en: 'Tang et al. (2020) Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, and
    Bo Li. 2020. Communication-Efficient Distributed Deep Learning: A Comprehensive
    Survey. arXiv:2003.06307 [cs.DC]'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang 等人（2020）**Zhenheng Tang**、**Shaohuai Shi**、**Xiaowen Chu**、**Wei Wang**
    和 **Bo Li**。2020。《Communication-Efficient Distributed Deep Learning: A Comprehensive
    Survey》。arXiv:2003.06307 [cs.DC]'
- en: Tao et al. (2020) Andrew Tao, Karan Sapra, and Bryan Catanzaro. 2020. Hierarchical
    Multi-Scale Attention for Semantic Segmentation. arXiv:2005.10821 [cs.CV]
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao 等人（2020）**Andrew Tao**、**Karan Sapra** 和 **Bryan Catanzaro**。2020。《Hierarchical
    Multi-Scale Attention for Semantic Segmentation》。arXiv:2005.10821 [cs.CV]
- en: 'Thiagarajan et al. (2018a) Jayaraman J. Thiagarajan, Rushil Anirudh, Bhavya
    Kailkhura, Nikhil Jain, Tanzima Islam, Abhinav Bhatele, Jae-Seung Yeom, and Todd
    Gamblin. 2018a. PADDLE: Performance Analysis using a Data-driven Learning Environment.
    In *Proceedings of the IEEE International Parallel & Distributed Processing Symposium*
    *(IPDPS ’18)*. IEEE Computer Society. [http://doi.ieeecomputersociety.org/10.1109/IPDPS.2018.00088](http://doi.ieeecomputersociety.org/10.1109/IPDPS.2018.00088)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thiagarajan 等人（2018a）**Jayaraman J. Thiagarajan**、**Rushil Anirudh**、**Bhavya
    Kailkhura**、**Nikhil Jain**、**Tanzima Islam**、**Abhinav Bhatele**、**Jae-Seung
    Yeom** 和 **Todd Gamblin**。2018a。《PADDLE: Performance Analysis using a Data-driven
    Learning Environment》。在 *IEEE国际并行与分布式处理研讨会* *(IPDPS ’18)*。IEEE计算机学会。 [http://doi.ieeecomputersociety.org/10.1109/IPDPS.2018.00088](http://doi.ieeecomputersociety.org/10.1109/IPDPS.2018.00088)'
- en: Thiagarajan et al. (2018b) Jayaraman J. Thiagarajan, Nikhil Jain, Rushil Anirudh,
    Alfredo Giménez, Rahul Sridhar, Aniruddha Marathe, Tao Wang, Murali Emani, Abhinav
    Bhatele, and Todd Gamblin. 2018b. Bootstrapping Parameter Space Exploration for
    Fast Tuning. In *Proceedings of the International Conference on Supercomputing*
    *(ICS ’18)*. [http://doi.acm.org/10.1145/3205289.3205321](http://doi.acm.org/10.1145/3205289.3205321)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thiagarajan 等人（2018b）**Jayaraman J. Thiagarajan**、**Nikhil Jain**、**Rushil Anirudh**、**Alfredo
    Giménez**、**Rahul Sridhar**、**Aniruddha Marathe**、**Tao Wang**、**Murali Emani**、**Abhinav
    Bhatele** 和 **Todd Gamblin**。2018b。《Bootstrapping Parameter Space Exploration
    for Fast Tuning》。在 *国际超级计算大会* *(ICS ’18)*。 [http://doi.acm.org/10.1145/3205289.3205321](http://doi.acm.org/10.1145/3205289.3205321)
- en: 'Vijayanarasimhan et al. (2017) Sudheendra Vijayanarasimhan, Susanna Ricco,
    Cordelia Schmid, Rahul Sukthankar, and Katerina Fragkiadaki. 2017. SfM-Net: Learning
    of Structure and Motion from Video. arXiv:1704.07804 [cs.CV]'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vijayanarasimhan 等人（2017）**Sudheendra Vijayanarasimhan**、**Susanna Ricco**、**Cordelia
    Schmid**、**Rahul Sukthankar** 和 **Katerina Fragkiadaki**。2017。《SfM-Net: Learning
    of Structure and Motion from Video》。arXiv:1704.07804 [cs.CV]'
- en: Wahib et al. (2020) Mohamed Wahib, Haoyu Zhang, Truong Thao Nguyen, Aleksandr
    Drozd, Jens Domke, Lingqi Zhang, Ryousei Takano, and Satoshi Matsuoka. 2020. Scaling
    Distributed Deep Learning Workloads beyond the Memory Capacity with KARMA. In
    *Proceedings of the International Conference for High Performance Computing, Networking,
    Storage and Analysis* (Atlanta, Georgia) *(SC ’20)*. IEEE Press, Article 19, 15 pages.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wahib 等人（2020）**Mohamed Wahib**、**Haoyu Zhang**、**Truong Thao Nguyen**、**Aleksandr
    Drozd**、**Jens Domke**、**Lingqi Zhang**、**Ryousei Takano** 和 **Satoshi Matsuoka**。2020。《Scaling
    Distributed Deep Learning Workloads beyond the Memory Capacity with KARMA》。在 *国际高性能计算、网络、存储与分析大会*（美国乔治亚州亚特兰大）
    *(SC ’20)*。IEEE出版社，第19篇，15页。
- en: 'Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad
    Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
    Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,
    Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant
    Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
    Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s Neural Machine
    Translation System: Bridging the Gap between Human and Machine Translation. *CoRR*
    abs/1609.08144 (2016). [http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
    Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner,
    Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo
    Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei
    Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg
    Corrado, Macduff Hughes, 和 Jeffrey Dean。2016年。谷歌神经机器翻译系统：弥合人类与机器翻译之间的差距。*CoRR*
    abs/1609.08144 (2016)。 [http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144)
- en: Yeom et al. (2016) Jae-Seung Yeom, Jayaraman J. Thiagarajan, Abhinav Bhatele,
    Greg Bronevetsky, and Tzanio Kolev. 2016. Data-dependent Performance Modeling
    of Linear Solvers for Sparse Matrices. In *Proceedings of the 7th International
    Workshop in Performance Modeling, Benchmarking and Simulation of High Performance
    Computer Systems* *(PMBS ’16)*.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeom 等人 (2016) Jae-Seung Yeom, Jayaraman J. Thiagarajan, Abhinav Bhatele, Greg
    Bronevetsky, 和 Tzanio Kolev。2016年。基于数据的稀疏矩阵线性求解器性能建模。发表于*第七届国际性能建模、基准测试和高性能计算系统仿真研讨会*(PMBS
    ’16)。
- en: You et al. (2017) Yang You, Igor Gitman, and Boris Ginsburg. 2017. Large Batch
    Training of Convolutional Networks. arXiv:1708.03888 [cs.CV]
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等人 (2017) Yang You, Igor Gitman, 和 Boris Ginsburg。2017年。卷积网络的大批量训练。arXiv:1708.03888
    [cs.CV]
- en: 'You et al. (2019) Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar,
    Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
    2019. Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.
    arXiv:1904.00962 [cs.LG]'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等人 (2019) Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar,
    Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, 和 Cho-Jui Hsieh。2019年。大批量优化深度学习：76分钟内训练BERT。arXiv:1904.00962
    [cs.LG]
- en: You et al. (2018) Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt
    Keutzer. 2018. ImageNet Training in Minutes. In *Proceedings of the 47th International
    Conference on Parallel Processing* (Eugene, OR, USA) *(ICPP 2018)*. Association
    for Computing Machinery, New York, NY, USA, Article 1, 10 pages. [https://doi.org/10.1145/3225058.3225069](https://doi.org/10.1145/3225058.3225069)
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等人 (2018) Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, 和 Kurt Keutzer。2018年。ImageNet训练仅需几分钟。发表于*第47届国际并行处理会议*
    (尤金, OR, USA) *(ICPP 2018)*。计算机协会, 纽约, NY, USA, 文章1, 10页。[https://doi.org/10.1145/3225058.3225069](https://doi.org/10.1145/3225058.3225069)
- en: 'Zhang et al. (2017) Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho,
    Xiaodan Liang, Zhiting Hu, Jinliang Wei, Pengtao Xie, and Eric P. Xing. 2017.
    Poseidon: An Efficient Communication Architecture for Distributed Deep Learning
    on GPU Clusters. In *2017 USENIX Annual Technical Conference (USENIX ATC 17)*.
    USENIX Association, Santa Clara, CA, 181–193. [https://www.usenix.org/conference/atc17/technical-sessions/presentation/zhang](https://www.usenix.org/conference/atc17/technical-sessions/presentation/zhang)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2017) Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan
    Liang, Zhiting Hu, Jinliang Wei, Pengtao Xie, 和 Eric P. Xing。2017年。Poseidon：用于GPU集群分布式深度学习的高效通信架构。发表于*2017年USENIX年会技术会议
    (USENIX ATC 17)*。USENIX协会，加利福尼亚州圣克拉拉，181–193。[https://www.usenix.org/conference/atc17/technical-sessions/presentation/zhang](https://www.usenix.org/conference/atc17/technical-sessions/presentation/zhang)
- en: 'Zhao et al. (2019) Zhong-Qiu Zhao, Peng Zheng, Shou-Tao Xu, and Xindong Wu.
    2019. Object Detection With Deep Learning: A Review. *IEEE Transactions on Neural
    Networks and Learning Systems* 30, 11 (2019), 3212–3232. [https://doi.org/10.1109/TNNLS.2018.2876865](https://doi.org/10.1109/TNNLS.2018.2876865)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 (2019) Zhong-Qiu Zhao, Peng Zheng, Shou-Tao Xu, 和 Xindong Wu。2019年。深度学习的目标检测：综述。*IEEE神经网络与学习系统汇刊*
    30, 11 (2019), 3212–3232。[https://doi.org/10.1109/TNNLS.2018.2876865](https://doi.org/10.1109/TNNLS.2018.2876865)
