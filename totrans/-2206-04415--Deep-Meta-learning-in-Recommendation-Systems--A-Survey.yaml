- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:45:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:45:49
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2206.04415] Deep Meta-learning in Recommendation Systems: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2206.04415] 深度元学习在推荐系统中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2206.04415](https://ar5iv.labs.arxiv.org/html/2206.04415)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2206.04415](https://ar5iv.labs.arxiv.org/html/2206.04415)
- en: 'Deep Meta-learning in Recommendation Systems: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度元学习在推荐系统中的应用：综述
- en: Chunyang Wang, Yanmin Zhu, Haobing Liu, Tianzi Zang, Jiadi Yu, Feilong Tang
    [wangchy@sjtu.edu.cn, yzhu@sjtu.edu.cn, liuhaobing@sjtu.edu.cn, zangtianzi@sjtu.edu.cn,
    jiadiyu@sjtu.edu.cn, tang-fl@cs.sjtu.edu.cn](mailto:wangchy@sjtu.edu.cn,%20yzhu@sjtu.edu.cn,%20liuhaobing@sjtu.edu.cn,%20zangtianzi@sjtu.edu.cn,%20jiadiyu@sjtu.edu.cn,%20tang-fl@cs.sjtu.edu.cn)
    Shanghai Jiao Tong UniversityShanghaiChina(2018)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 王春阳，朱彦敏，刘浩冰，臧天子，余佳迪，唐飞龙 [wangchy@sjtu.edu.cn, yzhu@sjtu.edu.cn, liuhaobing@sjtu.edu.cn,
    zangtianzi@sjtu.edu.cn, jiadiyu@sjtu.edu.cn, tang-fl@cs.sjtu.edu.cn](mailto:wangchy@sjtu.edu.cn,%20yzhu@sjtu.edu.cn,%20liuhaobing@sjtu.edu.cn,%20zangtianzi@sjtu.edu.cn,%20jiadiyu@sjtu.edu.cn,%20tang-fl@cs.sjtu.edu.cn)
    上海交通大学 上海 中国（2018）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep neural network based recommendation systems have achieved great success
    as information filtering techniques in recent years. However, since model training
    from scratch requires sufficient data, deep learning-based recommendation methods
    still face the bottlenecks of insufficient data and computational inefficiency.
    Meta-learning, as an emerging paradigm that learns to improve the learning efficiency
    and generalization ability of algorithms, has shown its strength in tackling the
    data sparsity issue. Recently, a growing number of studies on deep meta-learning
    based recommenddation systems have emerged for improving the performance under
    recommendation scenarios where available data is limited, e.g. user cold-start
    and item cold-start. Therefore, this survey provides a timely and comprehensive
    overview of current deep meta-learning based recommendation methods. Specifically,
    we propose a taxonomy to discuss existing methods according to recommendation
    scenarios, meta-learning techniques, and meta-knowledge representations, which
    could provide the design space for meta-learning based recommendation methods.
    For each recommendation scenario, we further discuss technical details about how
    existing methods apply meta-learning to improve the generalization ability of
    recommendation models. Finally, we also point out several limitations in current
    research and highlight some promising directions for future research in this area.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度神经网络的推荐系统近年来在信息过滤技术中取得了巨大成功。然而，由于从头开始训练模型需要足够的数据，基于深度学习的推荐方法仍面临数据不足和计算效率低下的瓶颈。元学习作为一种新兴的范式，通过学习提升算法的学习效率和泛化能力，在解决数据稀疏问题方面表现出了其优势。近期，越来越多关于深度元学习推荐系统的研究出现，以提高在数据有限的推荐场景下的表现，例如用户冷启动和项目冷启动。因此，本综述提供了对当前深度元学习推荐方法的及时和全面的概述。具体来说，我们提出了一个分类法，根据推荐场景、元学习技术和元知识表示讨论现有方法，这可以为基于元学习的推荐方法提供设计空间。对于每个推荐场景，我们进一步讨论了现有方法如何应用元学习以提高推荐模型的泛化能力的技术细节。最后，我们还指出了当前研究中的一些局限性，并强调了未来研究的一些有前景的方向。
- en: 'Recommendation Systems; Meta-learning; Learning-to-Learn; Survey; Cold-start;
    Few-shot Learning^†^†copyright: acmcopyright^†^†journalyear: 2018^†^†doi: 10.1145/1122445.1122456^†^†journal:
    JACM^†^†journalvolume: 37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth:
    8^†^†ccs: Information systems Recommender systems'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统；元学习；学习如何学习；综述；冷启动；少样本学习^†^†版权：acmcopyright^†^†期刊年份：2018^†^†doi：10.1145/1122445.1122456^†^†期刊：JACM^†^†期刊卷号：37^†^†期刊号：4^†^†文章编号：111^†^†出版月份：8^†^†ccs：信息系统
    推荐系统
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: In recent years, recommendation systems working as filtering systems for alleviating
    information overload have been widely applied in various online applications including
    e-commence, entertainment services, news, and so on. By presenting personalized
    suggestions among a large number of candidates, recommendation systems have achieved
    great success in improving user experience and increasing the attractiveness of
    online platforms. With the development of data-driven machine learning algorithms
    (Su and Khoshgoftaar, [2009](#bib.bib91); Billsus et al., [1998](#bib.bib4)),
    especially deep learning based methods (Zhang et al., [2019](#bib.bib122); He
    et al., [2017](#bib.bib33); Cheng et al., [2016](#bib.bib10)), academic and industrial
    research in this field has greatly improved the performance of recommendation
    systems in terms of accuracy, diversity, interpretability, and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，作为信息过滤系统以减轻信息过载的推荐系统被广泛应用于各种在线应用，包括电子商务、娱乐服务、新闻等。通过在大量候选项中提供个性化建议，推荐系统在提升用户体验和增加在线平台吸引力方面取得了巨大的成功。随着数据驱动的机器学习算法（Su
    和 Khoshgoftaar，[2009](#bib.bib91)；Billsus 等，[1998](#bib.bib4)）的发展，特别是基于深度学习的方法（Zhang
    等，[2019](#bib.bib122)；He 等，[2017](#bib.bib33)；Cheng 等，[2016](#bib.bib10)），该领域的学术和工业研究在推荐系统的准确性、多样性、可解释性等方面显著提升了性能。
- en: Due to expressive representation learning abilities to discover hidden dependencies
    from sufficient data, deep learning based methods have been largely introduced
    in contemporary recommendation models (Zhang et al., [2019](#bib.bib122); Gao
    et al., [2021](#bib.bib27)). By leveraging a great number of training instances
    with diverse data structures (e.g., interaction pairs (Zhang et al., [2019](#bib.bib122)),
    sequences(Fang et al., [2020](#bib.bib21)), and graphs (Gao et al., [2021](#bib.bib27))),
    recommendation models with deep neural architectures are usually designed to effectively
    capture nonlinear and nontrivial user/item relationships. However, conventional
    deep learning based recommendation models are usually trained from scratch with
    sufficient data based on predefined learning algorithms. For instance, the regular
    supervised learning paradigm typically trains a unified recommendation model with
    interactions collected from all users and performs recommendation over unseen
    interactions based on learned feature representations. Such deep learning based
    methods are usually data-hungry and computation-hungry. In other words, the performance
    of deep learning based recommendation systems heavily relies on the availability
    of a great amount of training data and sufficient computation. In practical recommendation
    applications, data collection mainly originates from users’ interactions observed
    during their visits to online platforms. There exist recommendation scenarios
    where available user interaction data is sparse (e.g. cold-start recommendation)
    and computation for model training is restrained (e.g. online recommendation).
    Consequently, both data insufficiency and computation inefficiency issues bottleneck
    deep learning based recommendation models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习方法在发现隐藏依赖关系方面具有表达能力，因此在当代推荐模型中得到了广泛应用（Zhang 等，[2019](#bib.bib122)；Gao
    等，[2021](#bib.bib27)）。通过利用具有多样数据结构的大量训练实例（例如，交互对（Zhang 等，[2019](#bib.bib122)），序列（Fang
    等，[2020](#bib.bib21)）和图（Gao 等，[2021](#bib.bib27)）），具有深度神经网络架构的推荐模型通常被设计为有效捕捉非线性和复杂的用户/项目关系。然而，传统的基于深度学习的推荐模型通常需要从头开始训练，并依赖于预定义的学习算法。比如，常规的监督学习范式通常使用从所有用户收集的交互数据来训练一个统一的推荐模型，并根据学习到的特征表示对未见过的交互进行推荐。这些基于深度学习的方法通常对数据和计算资源的需求很大。换句话说，基于深度学习的推荐系统的性能在很大程度上依赖于大量训练数据的可用性和足够的计算能力。在实际推荐应用中，数据收集主要来自用户在访问在线平台时的交互行为。存在一些推荐场景，其中可用的用户交互数据稀缺（例如冷启动推荐），而模型训练的计算资源有限（例如在线推荐）。因此，数据不足和计算效率低下的问题成为了基于深度学习的推荐模型的瓶颈。
- en: Recently, meta-learning provides an appealing learning paradigm that focuses
    on strengthening the generalization ability of machine learning methods against
    the insufficiency of data and computation (Vanschoren, [2018](#bib.bib99); Hospedales
    et al., [2020](#bib.bib37)). The key idea of meta-learning is to gain prior knowledge
    (named *meta-knowledge*) about efficient task learning from previous learning
    processes of multiple tasks. Then, the meta-knowledge could help facilitate fast
    learning over new tasks, which is supposed to have good generalization performance
    on unseen tasks. Here, a task usually refers to a set of instances belonging to
    the same class or having the same property, involving an individual learning process
    on it. Different from improving the representation learning capacity of deep learning
    models, meta-learning focuses on learning better learning strategies to substitute
    for fixed learning algorithms, known as the concept of *learn to learn*. Due to
    its great potential for fast adaptation over unseen tasks, meta-learning techniques
    have been applied in a wide range of research domains including image recognition
    (Cai et al., [2018](#bib.bib5); Zhu et al., [2020a](#bib.bib131)), image segmentation
    (Luo et al., [2022](#bib.bib61)), natural language processing (Lee et al., [2022](#bib.bib49)),
    reinforcement learning (Qu et al., [2021](#bib.bib76); Wang et al., [2020](#bib.bib104))
    and so on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，元学习提供了一种引人注目的学习范式，专注于增强机器学习方法在数据和计算不足情况下的泛化能力（Vanschoren，[2018](#bib.bib99)；Hospedales
    等，[2020](#bib.bib37)）。元学习的核心思想是从多个任务的先前学习过程中获取有关有效任务学习的先验知识（称为*元知识*）。然后，这些元知识可以帮助加快新任务的学习速度，这些新任务应该具有良好的泛化性能。这里，任务通常指的是属于同一类别或具有相同属性的一组实例，涉及对其进行的个体学习过程。与提高深度学习模型的表征学习能力不同，元学习专注于学习更好的学习策略，以替代固定的学习算法，这就是*学习如何学习*的概念。由于其在快速适应未知任务方面的巨大潜力，元学习技术已经在广泛的研究领域中得到了应用，包括图像识别（Cai
    等，[2018](#bib.bib5)；Zhu 等，[2020a](#bib.bib131)）、图像分割（Luo 等，[2022](#bib.bib61)）、自然语言处理（Lee
    等，[2022](#bib.bib49)）、强化学习（Qu 等，[2021](#bib.bib76)；Wang 等，[2020](#bib.bib104)）等。
- en: The benefits of meta-learning are well-aligned with the need of promoting recommendation
    models over scenarios suffering from limited instances and inefficient computation.
    Early efforts on meta-learning based recommendation methods mainly fall into personalized
    recommendation algorithm selection (Ren et al., [2019](#bib.bib79); Cunha et al.,
    [2018](#bib.bib14)), which extracts meta dataset features and selects suitable
    recommendation algorithms for different datasets (or tasks). Though applying the
    idea of extracting meta-knowledge and generating task-specific models, this definition
    of meta-learning is closer to studies in automated machine learning (Hutter et al.,
    [[n. d.]](#bib.bib40); Yao et al., [2018](#bib.bib116)). Afterward, deep meta-learning
    (Huisman et al., [2021](#bib.bib39)) or neural network meta-learning (Hospedales
    et al., [2020](#bib.bib37)) emerges and gradually become the mainstream of meta-learning
    techniques typically discussed in the recommendation models (Lee et al., [2019](#bib.bib48);
    Pan et al., [2019](#bib.bib70)). As introduced in (Huisman et al., [2021](#bib.bib39);
    Hospedales et al., [2020](#bib.bib37)), *Deep Meta-Learning* aims to extract meta-knowledge
    to allow for fast learning of deep neural networks, which brings enhancement to
    the currently popular deep learning paradigm. Since 2017, deep meta-learning has
    gained attention in the research community of recommendation systems. Advanced
    meta-learning techniques are firstly applied to alleviate data insufficiency (i.e.,
    cold-start issue) when training conventional deep recommendation models. For example,
    the most successful optimization-based meta-learning framework MAML which learns
    meta-knowledge in the form of parameter initialization of neural networks firstly
    shows great effectiveness in the cold-start recommendation scenario (Lee et al.,
    [2019](#bib.bib48)). Besides that, diverse recommendation scenarios such as click-through-rate
    prediction (Pan et al., [2019](#bib.bib70)), online recommendation (Zhang et al.,
    [2020](#bib.bib124)), and sequential recommendation (Zheng et al., [2021](#bib.bib126))
    are also studied under the meta-learning schema, to improve the learning ability
    in the setting of data insufficiency and computation inefficiency.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习的好处与提升推荐模型在实例有限和计算效率低下场景中的需求高度一致。早期的元学习推荐方法主要集中在个性化推荐算法选择（Ren et al., [2019](#bib.bib79);
    Cunha et al., [2018](#bib.bib14)），该方法提取元数据集特征，并为不同数据集（或任务）选择合适的推荐算法。虽然应用了提取元知识和生成任务特定模型的理念，这种元学习定义更接近于自动化机器学习的研究（Hutter
    et al., [[n. d.]](#bib.bib40); Yao et al., [2018](#bib.bib116)）。随后，深度元学习（Huisman
    et al., [2021](#bib.bib39)）或神经网络元学习（Hospedales et al., [2020](#bib.bib37)）逐渐成为推荐模型中讨论的主流元学习技术（Lee
    et al., [2019](#bib.bib48); Pan et al., [2019](#bib.bib70)）。正如（Huisman et al.,
    [2021](#bib.bib39); Hospedales et al., [2020](#bib.bib37)）中介绍的，*深度元学习*旨在提取元知识，以实现对深度神经网络的快速学习，从而提升当前流行的深度学习范式。自2017年以来，深度元学习在推荐系统研究领域获得了关注。先进的元学习技术首先应用于缓解数据不足（即冷启动问题），在训练传统深度推荐模型时。例如，最成功的基于优化的元学习框架MAML，它以神经网络参数初始化的形式学习元知识，首次在冷启动推荐场景中显示出极大的效果（Lee
    et al., [2019](#bib.bib48)）。此外，还在元学习框架下研究了点击率预测（Pan et al., [2019](#bib.bib70)）、在线推荐（Zhang
    et al., [2020](#bib.bib124)）和序列推荐（Zheng et al., [2021](#bib.bib126)）等多种推荐场景，以提高在数据不足和计算低效环境中的学习能力。
- en: In this paper, we provide a timely and comprehensive survey of the rapidly growing
    studies of deep meta-learning based recommendation systems. As we investigated,
    although there have been some surveys on meta-learning or deep meta-learning that
    summarize details of general meta-learning methods and their applications (Vanschoren,
    [2018](#bib.bib99); Huisman et al., [2021](#bib.bib39); Hospedales et al., [2020](#bib.bib37)),
    there still lacks attention to recent advances in recommendation systems. In addition,
    there are several surveys on meta-learning methods in other application domains,
    such as Natural Language Processing (Lee et al., [2022](#bib.bib49); Yin, [2020](#bib.bib118)),
    Multimodality (Ma et al., [2022](#bib.bib62)) and Image Segmentation (Luo et al.,
    [2022](#bib.bib61)). However, no previous survey centers on the deep meta-learning
    in recommendation systems. Compared with them, our survey is the first attempt
    to fill the gap, providing a systematic review of up-to-date papers on the combination
    of meta-learning and recommendation systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了对快速增长的基于深度元学习的推荐系统研究的及时且全面的调查。我们调查发现，虽然已有一些关于元学习或深度元学习的调查，总结了元学习方法及其应用的细节（Vanschoren,
    [2018](#bib.bib99); Huisman et al., [2021](#bib.bib39); Hospedales et al., [2020](#bib.bib37)），但对推荐系统的最新进展仍缺乏关注。此外，还有一些关于其他应用领域元学习方法的调查，如自然语言处理（Lee
    et al., [2022](#bib.bib49); Yin, [2020](#bib.bib118)）、多模态（Ma et al., [2022](#bib.bib62)）和图像分割（Luo
    et al., [2022](#bib.bib61)）。然而，之前没有任何调查专注于推荐系统中的深度元学习。与这些调查相比，我们的调查首次尝试填补这一空白，提供了对元学习与推荐系统结合的最新论文的系统性综述。
- en: In our survey, we aim to thoroughly review the literature on the deep meta-learning
    based recommendation systems, which can benefit readers and researchers for a
    comprehensive understanding of this topic. To carefully position works in this
    field, we provide a taxonomy with three perspectives including recommendation
    scenarios, meta-learning techniques, and meta-knowledge representations. Moreover,
    we mainly discuss related methods according to recommendation scenarios and present
    how different works utilize meta-learning techniques to extract specific meta-knowledge
    with diverse forms such as parameter initialization, parameter modulation, hyperparameter
    optimization, .etc. We hope our taxonomy could provide a design space for developing
    new deep meta-learning based recommendation methods. In addition, we also summarize
    common ways for meta-learning task construction which is a necessary setup of
    the meta-learning paradigm.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的调查中，我们旨在彻底审视基于深度元学习的推荐系统的文献，这将有助于读者和研究人员对这一主题有全面的理解。为了精确定位该领域的工作，我们提供了一个包含推荐场景、元学习技术和元知识表示三种视角的分类法。此外，我们主要根据推荐场景讨论相关方法，并展示不同工作如何利用元学习技术提取具有多种形式的特定元知识，例如参数初始化、参数调节、超参数优化等。我们希望我们的分类法能够为开发新的基于深度元学习的推荐方法提供设计空间。此外，我们还总结了元学习任务构建的常见方法，这是元学习范式的必要设置。
- en: 'The structure of this survey is organized as follows. In Section [2](#S2 "2\.
    Foundations ‣ Deep Meta-learning in Recommendation Systems: A Survey"), we introduce
    the common foundations of meta-learning techniques and typical recommendation
    scenarios in which meta-learning methods have been studied to alleviate data insufficiency
    and computation inefficiency. In Section [3](#S3 "3\. Taxonomy ‣ Deep Meta-learning
    in Recommendation Systems: A Survey"), we present our taxonomy consisting of three
    independent axes. In Section [4](#S4 "4\. Meta-learning Task Construction for
    Recommendation ‣ Deep Meta-learning in Recommendation Systems: A Survey"), we
    summarize different ways of meta-learning recommendation task construction used
    in the literature. Then we elaborate on methodological details of existing methods
    applying meta-learning techniques in different recommendation scenarios in Section [5](#S5
    "5\. Meta-leanring Methods for Recommendation Systems ‣ Deep Meta-learning in
    Recommendation Systems: A Survey"). Finally, we discuss promising directions for
    future research in this field in Section [6](#S6 "6\. Future Directions ‣ Deep
    Meta-learning in Recommendation Systems: A Survey") and conclude this survey in
    Section [7](#S7 "7\. Conclusion ‣ Deep Meta-learning in Recommendation Systems:
    A Survey").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的结构安排如下。在第[2](#S2 "2\. Foundations ‣ Deep Meta-learning in Recommendation
    Systems: A Survey")节中，我们介绍了元学习技术的共同基础以及元学习方法已被研究应用于缓解数据不足和计算低效的典型推荐场景。在第[3](#S3
    "3\. Taxonomy ‣ Deep Meta-learning in Recommendation Systems: A Survey")节中，我们展示了由三个独立轴组成的分类法。在第[4](#S4
    "4\. Meta-learning Task Construction for Recommendation ‣ Deep Meta-learning in
    Recommendation Systems: A Survey")节中，我们总结了文献中用于推荐任务构建的不同元学习方法。随后，在第[5](#S5 "5\.
    Meta-leanring Methods for Recommendation Systems ‣ Deep Meta-learning in Recommendation
    Systems: A Survey")节中，我们详细阐述了现有方法在不同推荐场景中应用元学习技术的方法论细节。最后，我们在第[6](#S6 "6\. Future
    Directions ‣ Deep Meta-learning in Recommendation Systems: A Survey")节中讨论了该领域未来研究的有前景方向，并在第[7](#S7
    "7\. Conclusion ‣ Deep Meta-learning in Recommendation Systems: A Survey")节中总结了本综述。'
- en: Paper Collection. We summarize over 50 high-quality papers which are highly
    related to deep meta-learning based recommendation systems. We carefully retrieve
    these papers using Google Scholar and DBLP as main search engines with major keywords
    including meta-learning + recommendation, meta + recommendation, meta + CTR, meta
    + recommender, etc. We particularly pay attention to top-tier conferences and
    journals including KDD, SIGIR, WWW, AAAI, IJCAI, WSDM, CIKM, ICDM, TKDE, TKDD,
    TOIS, so as to ensure that high-profile papers are covered.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 论文综述。我们总结了50多篇与深度元学习推荐系统高度相关的高质量论文。我们使用Google Scholar和DBLP作为主要搜索引擎，检索这些论文，主要关键词包括meta-learning
    + recommendation, meta + recommendation, meta + CTR, meta + recommender等。我们特别关注顶级会议和期刊，包括KDD,
    SIGIR, WWW, AAAI, IJCAI, WSDM, CIKM, ICDM, TKDE, TKDD, TOIS，以确保涵盖高水平论文。
- en: 2\. Foundations
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 基础
- en: In this section, we present the necessary foundations for discussing deep meta-learning
    based recommendation methods. Firstly, we summarize the core ideas and representative
    works of different categories of meta-learning techniques. Afterward, we introduce
    typical recommendation scenarios in which meta-learning techniques have been studied
    and applied.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了讨论深度元学习推荐方法所需的基础知识。首先，我们总结了不同类别元学习技术的核心思想和代表性工作。随后，我们介绍了元学习技术已被研究和应用的典型推荐场景。
- en: 2.1\. Meta-learning
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 元学习
- en: 'To comprehensively understand the concept of meta-learning, we first formalize
    the paradigm of meta-learning and contrast the conventional machine learning paradigm
    with the meta-learning paradigm in detail. Then, we briefly present three mainstreams
    of meta-learning techniques, including *metric-based*, *model-based* and *optimization-based*
    meta-learning techniques by summarizing their core ideas and introducing several
    typical related works. For convenience, we list some general symbols and their
    descriptions in Table [1](#S2.T1 "Table 1 ‣ 2.1\. Meta-learning ‣ 2\. Foundations
    ‣ Deep Meta-learning in Recommendation Systems: A Survey").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '为了全面理解元学习的概念，我们首先形式化了元学习的范式，并详细对比了传统机器学习范式与元学习范式。然后，我们简要介绍了包括*基于度量*、*基于模型*和*基于优化*的三种主流元学习技术，总结了它们的核心思想，并介绍了几个典型相关工作。为了方便起见，我们在表[1](#S2.T1
    "Table 1 ‣ 2.1\. Meta-learning ‣ 2\. Foundations ‣ Deep Meta-learning in Recommendation
    Systems: A Survey")中列出了一些通用符号及其描述。'
- en: Table 1\. Notations used in this paper.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 本文使用的符号。
- en: '| Notations | Descriptions |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 |'
- en: '| $u_{i}$ | User $i$ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| $u_{i}$ | 用户 $i$ |'
- en: '| $v_{j}$ | Item $j$ |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| $v_{j}$ | 项目 $j$ |'
- en: '| $r_{u_{i},v_{j}}$ | An interaction between $u_{i}$ and $v_{j}$ (explicit
    rating or implicit feedback) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| $r_{u_{i},v_{j}}$ | $u_{i}$ 和 $v_{j}$ 之间的交互（显式评分或隐式反馈） |'
- en: '| $\bm{x}_{k},y_{k}$ | Representation and label of $k$-th instance (e.g. an
    interaction) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| $\bm{x}_{k},y_{k}$ | 第 $k$ 个实例（例如交互）的表示和标签 |'
- en: '| $\mathcal{T}_{i}$ | $i$-th recommendation task |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{T}_{i}$ | 第 $i$ 个推荐任务 |'
- en: '| $\mathcal{S}_{i}$ | Support set of a task $\mathcal{T}_{i}$ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}_{i}$ | 任务 $\mathcal{T}_{i}$ 的支持集 |'
- en: '| $\mathcal{Q}_{i}$ | Query set of a task $\mathcal{T}_{i}$ |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{Q}_{i}$ | 任务 $\mathcal{T}_{i}$ 的查询集 |'
- en: '| $\mathcal{D}^{train}$ | Meta-training dataset |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{D}^{train}$ | 元训练数据集 |'
- en: '| $\mathcal{D}^{test}$ | Meta-testing dataset |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{D}^{test}$ | 元测试数据集 |'
- en: '| $f_{\theta}$ | Base recommendation model/function |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| $f_{\theta}$ | 基础推荐模型/函数 |'
- en: '| $\theta$ | Parameters of the base recommendation model |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $\theta$ | 基础推荐模型的参数 |'
- en: '| $\theta_{\mathcal{T}_{i}}$ | Task-specific parameters of a personalized model
    for $\mathcal{T}_{i}$ |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| $\theta_{\mathcal{T}_{i}}$ | 针对任务 $\mathcal{T}_{i}$ 的个性化模型的任务特定参数 |'
- en: '| $\alpha$ | Local update rate in the optimization-based meta-learning |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ | 基于优化的元学习中的局部更新率 |'
- en: '| $\beta$ | Global update rate in the optimization-based meta-learning |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| $\beta$ | 基于优化的元学习中的全局更新率 |'
- en: '| $\mathcal{L}(f_{\theta},*)$ | Loss function of the base recommendation model
    over a given dataset |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}(f_{\theta},*)$ | 给定数据集上的基础推荐模型的损失函数 |'
- en: '| $\mathcal{F}_{\omega}$ | Meta-learner parameterized with $\omega$ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{F}_{\omega}$ | 用$\omega$参数化的元学习器 |'
- en: '| $\omega$ | Meta-knowledge obtained with the meta-learner |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| $\omega$ | 元学习器获得的元知识 |'
- en: 2.1.1\. Formalizing Meta-learning
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1. 形式化元学习
- en: As commonly understood as the concept of *learning to learn*, meta-learning
    mainly contributes to improving the generalization ability of base learning models
    or algorithms, so as to learn new tasks better or more quickly. Generally, the
    core idea of meta-learning paradigm is learning prior knowledge, i.e. *meta-knowledge*,
    across multiple tasks, where each task refers to a learning process which tries
    to perform well on its own instances. The learning processes of different tasks
    are treated as training instances observed by meta-learning methods. By defining
    the form of meta-knowledge and extracting meta-knowledge across multiple existing
    tasks, meta-learning methods enable the learning processes of new tasks to be
    more effective.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常理解为*学习如何学习*的概念，元学习主要有助于提高基础学习模型或算法的泛化能力，从而更好或更快地学习新任务。一般来说，元学习范式的核心思想是在多个任务之间学习先验知识，即*元知识*，其中每个任务指的是尝试在其自身实例上表现良好的学习过程。不同任务的学习过程被视为元学习方法观察到的训练实例。通过定义元知识的形式并从多个现有任务中提取元知识，元学习方法使新任务的学习过程变得更有效。
- en: 'Formally, in the training phase of meta-learning paradigm, we assume that a
    set of training tasks sampled from a task distirbution $p(\mathcal{T})$ are available
    as a meta-training dataset which is denoted as $\mathcal{D}^{train}=\{\mathcal{T}_{i}\}_{i=1}^{M}$.
    All instances $\mathcal{D}_{i}$ of a task $\mathcal{T}_{i}$ consists of its own
    training instances denoted as the support set $\mathcal{S}_{i}$ and evaluation
    instances denoted the query set $\mathcal{Q}_{i}$. Take a task under the supervied
    learning scheme as example. Given the support set $\mathcal{S}_{i}=\{(\bm{x}_{k},y_{k})\}_{k=1}^{S}$
    consisting of $S$ training instances, the task $\mathcal{T}_{i}$ aims to learn
    a mapping function(or model) $f_{\theta}:\mathcal{X}\rightarrow\mathcal{Y}$ by
    minimizing the empirical loss $\mathcal{L}(f_{\theta},\mathcal{S}_{i})$. The task-specific
    parameters $\theta_{\mathcal{T}_{i}}$ of the mapping function for the task $\mathcal{T}_{i}$
    is obtained as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，在元学习范式的训练阶段，我们假设从任务分布 $p(\mathcal{T})$ 中采样的一组训练任务可用作元训练数据集，记作 $\mathcal{D}^{train}=\{\mathcal{T}_{i}\}_{i=1}^{M}$。任务
    $\mathcal{T}_{i}$ 的所有实例 $\mathcal{D}_{i}$ 由其自身的训练实例组成，记作支持集 $\mathcal{S}_{i}$，以及评估实例，记作查询集
    $\mathcal{Q}_{i}$。以监督学习方案下的任务为例。给定支持集 $\mathcal{S}_{i}=\{(\bm{x}_{k},y_{k})\}_{k=1}^{S}$，其中包含
    $S$ 个训练实例，任务 $\mathcal{T}_{i}$ 旨在通过最小化经验损失 $\mathcal{L}(f_{\theta},\mathcal{S}_{i})$
    来学习一个映射函数（或模型）$f_{\theta}:\mathcal{X}\rightarrow\mathcal{Y}$。任务 $\mathcal{T}_{i}$
    的映射函数的任务特定参数 $\theta_{\mathcal{T}_{i}}$ 如下所示：
- en: '| (1) |  | $\theta_{\mathcal{T}_{i}}=\mathop{\arg\min}\limits_{\theta}\mathcal{L}(f_{{\theta}},\mathcal{S}_{i})$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\theta_{\mathcal{T}_{i}}=\mathop{\arg\min}\limits_{\theta}\mathcal{L}(f_{{\theta}},\mathcal{S}_{i})$
    |  |'
- en: where the loss function $\mathcal{L}(f_{\theta},*)$ could be a cross-entropy
    loss for classification tasks or regression loss such as mean squared error for
    regression tasks. Note that the training process of each task is usually conducted
    the same as the regular supervised learning. To measure the generalization performance
    of the trained model over unseen instances, a set of evaluation instances $\mathcal{Q}_{i}=\{(\bm{x}_{k},y_{k})\}_{k=1}^{Q}$
    are sampled from the same distribution of the task $\mathcal{T}_{i}$. The learned
    mapping function $f_{\theta_{\mathcal{T}_{i}}}$ is supposed to perform well by
    investigating the empirical loss $\mathcal{L}(f_{\theta_{\mathcal{T}_{i}}},\mathcal{Q}_{i})$
    or other evaluation metrics in different settings. To be mentioned, learning tasks
    in other schemes such as reinforcement learning (Bello et al., [2017](#bib.bib2))
    and unsupervised learning (Metz et al., [2018](#bib.bib64)) are also studied.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中损失函数 $\mathcal{L}(f_{\theta},*)$ 可以是用于分类任务的交叉熵损失，或用于回归任务的回归损失，如均方误差。注意，每个任务的训练过程通常与常规监督学习相同。为了测量训练模型在未见实例上的泛化性能，从任务
    $\mathcal{T}_{i}$ 的相同分布中抽取一组评估实例 $\mathcal{Q}_{i}=\{(\bm{x}_{k},y_{k})\}_{k=1}^{Q}$。通过在不同设置中检查经验损失
    $\mathcal{L}(f_{\theta_{\mathcal{T}_{i}}},\mathcal{Q}_{i})$ 或其他评估指标，期望学到的映射函数
    $f_{\theta_{\mathcal{T}_{i}}}$ 能表现良好。需要提及的是，其他方案中的学习任务，例如强化学习（Bello et al., [2017](#bib.bib2)）和无监督学习（Metz
    et al., [2018](#bib.bib64)）也被研究。
- en: 'In the training processes of different meta-training tasks in $\mathcal{D}^{train}$,
    even if the form of the mapping functions could be the same, how to learn task-specific
    models is still distinct and guided by learnable settings about task learning.
    For example, approximating using neural networks with the same structure requires
    suitable hyper-parameters or initialization settings which are likely to be different
    for different tasks. In other words, the learning of each task $\mathcal{T}_{i}$
    also depends on *how to learn*, which is defined as meta-knowledge $\omega$ under
    the meta-learning paradigm. Therefore, the task-specific learning of $\mathcal{T}_{i}$
    could be formalized as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $\mathcal{D}^{train}$ 中不同的元训练任务的训练过程中，即使映射函数的形式可能相同，如何学习任务特定的模型仍然是不同的，并由关于任务学习的可学习设置指导。例如，使用具有相同结构的神经网络进行逼近时，需要合适的超参数或初始化设置，这些设置对于不同任务可能是不同的。换句话说，每个任务
    $\mathcal{T}_{i}$ 的学习还依赖于 *如何学习*，这在元学习范式下被定义为元知识 $\omega$。因此，$\mathcal{T}_{i}$
    的任务特定学习可以形式化为如下：
- en: '| (2) |  | $\theta_{\mathcal{T}_{i}}=h_{\omega}(f_{\theta},\mathcal{T}_{i},\mathcal{L})$
    |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\theta_{\mathcal{T}_{i}}=h_{\omega}(f_{\theta},\mathcal{T}_{i},\mathcal{L})$
    |  |'
- en: where $h_{\omega}(*)$ denotes the meta-learning approches of utilizing meta-knowledge
    to ensure effective learning of task $\mathcal{T}_{i}$ with the same mapping function
    $f_{\theta}$ and loss function $\mathcal{L}$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{\omega}(*)$ 表示利用元知识确保有效学习任务 $\mathcal{T}_{i}$ 的元学习方法，使用相同的映射函数 $f_{\theta}$
    和损失函数 $\mathcal{L}$。
- en: 'Instead of assuming the meta-knowledge $\omega$ is pre-defined and fixed for
    all tasks, meta-learning allows for learning $\omega$ to enable each task to be
    learned better. Manually searching in the whole meta-knowledge space is impractical
    in most cases. The goal of meta-learning is to learn the optimal $\omega$ which
    could be utilized to guide task-specific learning of all tasks to perform better.
    Formally, given all training tasks $\mathcal{D}^{train}=\{\mathcal{T}_{i}\}_{i=1}^{M}$,
    the optimal meta-knowledge $\omega^{*}$ are obtained as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与其假设元知识 $\omega$ 是为所有任务预定义并固定的，元学习允许学习 $\omega$ 以使每个任务能更好地学习。在大多数情况下，手动搜索整个元知识空间是不切实际的。元学习的目标是学习最优的
    $\omega$，以便能够指导所有任务的特定任务学习，从而提高其性能。形式上，给定所有训练任务 $\mathcal{D}^{train}=\{\mathcal{T}_{i}\}_{i=1}^{M}$，最优的元知识
    $\omega^{*}$ 可以通过以下方式获得：
- en: '| (3) |  | $\omega^{*}=\mathop{\arg\min}\limits_{\bm{\omega}}\sum_{\mathcal{T}_{i}\in\mathcal{D}^{train}}\mathcal{L}(f_{\theta_{\mathcal{T}_{i}}},\mathcal{Q}_{i})=\mathop{\arg\min}\limits_{\bm{\omega}}\sum_{\mathcal{T}_{i}\in\mathcal{D}^{train}}\mathcal{L}(f_{h_{\omega}(f_{\theta},\mathcal{T}_{i},\mathcal{L})},\mathcal{Q}_{i})$
    |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\omega^{*}=\mathop{\arg\min}\limits_{\bm{\omega}}\sum_{\mathcal{T}_{i}\in\mathcal{D}^{train}}\mathcal{L}(f_{\theta_{\mathcal{T}_{i}}},\mathcal{Q}_{i})=\mathop{\arg\min}\limits_{\bm{\omega}}\sum_{\mathcal{T}_{i}\in\mathcal{D}^{train}}\mathcal{L}(f_{h_{\omega}(f_{\theta},\mathcal{T}_{i},\mathcal{L})},\mathcal{Q}_{i})$
    |  |'
- en: where the objective of training meta-learning methods is to observe better performance
    (e.g., lower empirical loss) over the corresponding query set $\mathcal{Q}_{i}$
    of each task. Note that, the meta-knowledge is learned across multiple tasks since
    it is supposed to mine across-task characteristics of different task learning
    processes and has great generalization ability against the task differences.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练元学习方法的目标是观察在每个任务的相应查询集$\mathcal{Q}_{i}$上的更好表现（例如，较低的经验损失）。注意，元知识是跨多个任务学习的，因为它旨在挖掘不同任务学习过程中的跨任务特征，并对任务差异具有很强的泛化能力。
- en: 'In contrast with conventional machine learning, (e.g., regular supervised learning
    paradigm), the meta-learning paradigm mainly has the following properties: 1)
    Learning objective. The learning objective of meta-learning, i.e., *meta-optimization
    objecive*, is to facilitate the learning over unseen tasks, while conventional
    machine learning aims to facilitate the learning over unseen instances of the
    same task. 2) Setup of task division. For regular supervised machine learning,
    all instances are usually sampled from the data distribution of a single task.
    There are also multi-task learning (Caruana, [1997](#bib.bib7)) or transfer learning
    frameworks (Weiss et al., [2016](#bib.bib110)) which consider knowledge transfer
    across multiple tasks. However, these frameworks mainly consider a pair of tasks
    or a small number of known tasks, and transfer knowledge from other tasks as additional
    information, such as pretraining techniques or joint optimization strategies.
    In comparison, under the meta-learning paradigm, a larger number of tasks with
    relatively fewer instances are explicitly split according to specific properties
    (e.g., classes, attributes, or time), so as to extract prior knowledge about task
    learning at a higher level, i.e., learn to learn. 3) Learning framework. A common
    framework of meta-learning follows a bi-level learning structure consistent with
    meta-optimization objectives. The inner-level learning focuses on task-specific
    learning to generate training instances of the outer-level learning. The outer-level
    learning is responsible for learning the meta-knowledge across multiple instances.
    For most regular machine learning, only one level of learning is conducted over
    all supervised instances through batch learning, which is the same as the inner-level
    learning in the meta-learning paradigm.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统机器学习（例如，常规监督学习范式）相比，元学习范式主要具有以下特性：1）学习目标。元学习的学习目标，即*元优化目标*，是促进对未见任务的学习，而传统机器学习则旨在促进对相同任务未见实例的学习。2）任务划分设置。对于常规监督机器学习，所有实例通常都来自单一任务的数据分布。还有多任务学习（Caruana,
    [1997](#bib.bib7)）或迁移学习框架（Weiss et al., [2016](#bib.bib110)），这些框架考虑了跨多个任务的知识迁移。然而，这些框架主要考虑一对任务或少量已知任务，并将其他任务的知识作为额外信息进行迁移，例如预训练技术或联合优化策略。相比之下，在元学习范式下，大量任务会根据特定属性（例如，类别、属性或时间）进行明确划分，以便在更高层次上提取任务学习的先验知识，即*学习如何学习*。3）学习框架。元学习的常见框架遵循与元优化目标一致的双层学习结构。内层学习专注于任务特定学习，以生成外层学习的训练实例。外层学习负责跨多个实例学习元知识。对于大多数常规机器学习，通过批量学习只进行一个学习层级，这与元学习范式中的内层学习相同。
- en: 2.1.2\. Mainstream Frameworks of Meta-learning Techniques
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2. 主流的元学习技术框架
- en: As summarized by previous meta-learning surveys (Vanschoren, [2018](#bib.bib99);
    Huisman et al., [2021](#bib.bib39)), meta-learning techniques mainly fall into
    three categories, named metric-based, model-based, and optimization-based meta-learning
    methods. Next, we will elaborate on the formalization, technical details, and
    representative works of each category and discuss their pros and cons compared
    with each other.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前的元学习调查总结的那样（Vanschoren, [2018](#bib.bib99); Huisman et al., [2021](#bib.bib39)），元学习技术主要分为三类，分别是基于度量的、基于模型的和基于优化的元学习方法。接下来，我们将详细介绍每一类的形式化、技术细节和代表性工作，并讨论它们之间的优缺点。
- en: 'Metric-based Meta-learning resorts to the idea of metric learning and mainly
    represents meta-knowledge $\omega$ in the form of a meta-learned feature space
    where the similarity of support instances and query instances are compared. Specifically,
    task-specific learning in metric-based techniques is conducted in the form of
    non-parametric learning. In other words, in the inner-level learning of each task,
    the parameters of the mapping function $f_{\theta}$ are not optimized to fit the
    training instances $\mathcal{S}_{i}$ but directly utilized to generate labels
    of evaluation instances $\mathcal{Q}_{i}$. For the mapping function $f_{\theta}$,
    metric-based methods mainly rely on a similarity scoring function $sim(\bm{x}_{i},\bm{x}_{j})$
    which takes embeddings of two instances (e.g., a training (support) instance and
    a evaluation (query) instance) as inputs and calculates a similarity weight in
    the meta-learned feature space. Then the label of an evaluation instance is assigned
    by the weighted combination of labels from all training (support) instances. Formally,
    the predicted label vector $\hat{\bm{y}}_{i}$ of a query instance $\bm{x}_{i}$
    in the task $\mathcal{T}_{i}$ could be obtained as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于度量的元学习借鉴了度量学习的思想，主要以元学习的特征空间的形式表示元知识 $\omega$，在该空间中比较支持实例和查询实例的相似性。具体而言，基于度量的技术中的任务特定学习是以非参数学习的形式进行的。换句话说，在每个任务的内层学习中，映射函数
    $f_{\theta}$ 的参数并不被优化以适应训练实例 $\mathcal{S}_{i}$，而是直接用于生成评估实例 $\mathcal{Q}_{i}$
    的标签。对于映射函数 $f_{\theta}$，基于度量的方法主要依赖于相似性评分函数 $sim(\bm{x}_{i},\bm{x}_{j})$，该函数将两个实例的嵌入（例如，一个训练（支持）实例和一个评估（查询）实例）作为输入，并在元学习的特征空间中计算相似性权重。然后，通过加权组合所有训练（支持）实例的标签来分配评估实例的标签。形式上，任务
    $\mathcal{T}_{i}$ 中查询实例 $\bm{x}_{i}$ 的预测标签向量 $\hat{\bm{y}}_{i}$ 可以如下获得：
- en: '| (4) |  | $\hat{\bm{y}}_{i}=\sum\limits_{(\bm{x}_{j},\bm{y}_{j})\in\mathcal{S}_{i}}sim(\bm{x}_{i},\bm{x}_{j})\bm{y}_{j}$
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\hat{\bm{y}}_{i}=\sum\limits_{(\bm{x}_{j},\bm{y}_{j})\in\mathcal{S}_{i}}sim(\bm{x}_{i},\bm{x}_{j})\bm{y}_{j}$
    |  |'
- en: Note that we simply present a basic form of metric-based meta-learning. In the
    litertature, the similarity function $sim(\bm{x}_{i},\bm{x}_{j})$ and label generating
    could be achieved in different forms such as siamese nets (Koch et al., [2015](#bib.bib46)),
    matching nets (Vinyals et al., [2016](#bib.bib101)), prototypical nets (Snell
    et al., [2017](#bib.bib89)), relation nets (Sung et al., [2018](#bib.bib96)),
    and graph neural networks (Satorras and Estrach, [2018](#bib.bib85)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们仅展示了基于度量的元学习的基本形式。在文献中，相似性函数 $sim(\bm{x}_{i},\bm{x}_{j})$ 和标签生成可以通过不同的形式实现，如孪生网络（Koch
    et al., [2015](#bib.bib46)）、匹配网络（Vinyals et al., [2016](#bib.bib101)）、原型网络（Snell
    et al., [2017](#bib.bib89)）、关系网络（Sung et al., [2018](#bib.bib96)）和图神经网络（Satorras
    and Estrach, [2018](#bib.bib85)）。
- en: For outer-level learning, metric-based meta-learning aims to learn the feature
    space for effectively comparing instance similarity in new tasks. Therefore, the
    meta-knowledge $\omega$ coincides with the parameters $\theta$ in the mapping
    function of the inner-level learning. Then $\theta$ are optimized by minimizing
    the empirical loss over query set of multiple training tasks as equation 3\. To
    be mentioned, the $\theta_{\mathcal{T}_{i}}$ is the same as the $\bm{\theta}$
    since the inner-level task-specific learning is non-parametric.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于外层学习，基于度量的元学习旨在学习特征空间，以有效地比较新任务中的实例相似性。因此，元知识 $\omega$ 与内层学习中的映射函数的参数 $\theta$
    相符。然后通过最小化多个训练任务的查询集上的经验损失来优化 $\theta$，如方程 3 所示。需要提到的是，$\theta_{\mathcal{T}_{i}}$
    与 $\bm{\theta}$ 相同，因为内层任务特定学习是非参数的。
- en: Model-based Meta-learning is another widely used meta-learning technique with
    the help of the powerful representation ability of neural network structures.
    The key idea of model-based methods is to meta-learn a model or a module to encode
    the internal states of a task by observing its support instances. Conditioned
    on the internal states, the model-based meta-learner could capture task-specific
    information and guide task-adaptive predictions for evaluation instances.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的元学习是另一种广泛使用的元学习技术，借助神经网络结构强大的表示能力。基于模型的方法的关键思想是通过观察任务的支持实例来元学习一个模型或模块，以编码任务的内部状态。在内部状态的条件下，基于模型的元学习者可以捕捉任务特定的信息，并指导任务自适应的预测。
- en: 'In the model-based meta-learning, inner-level learning mainly focuses on encoding
    the support instances (or gradients) of the task into representations of the internal
    state with a neural network structured model such as feed-forward networks, recurrent
    neural networks (Ravi and Larochelle, [2016](#bib.bib78); Hochreiter et al., [2001](#bib.bib36)),
    convolutional neural networks (Mishra et al., [2018](#bib.bib65)) or hypernetworks
    (Qiao et al., [2018](#bib.bib75); Gidaris and Komodakis, [2018](#bib.bib28)).
    The predictions of query instances are usually obtained with a modulated predictor
    conditioned on the encoded task-specific state representation. Formally, the prediction
    of a query instance $\bm{x}_{i}$ in the task $\mathcal{T}_{i}$ could be obtained
    as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于模型的元学习中，内层学习主要集中在使用结构化的神经网络模型（如前馈网络、递归神经网络（Ravi 和 Larochelle，[2016](#bib.bib78);
    Hochreiter 等人，[2001](#bib.bib36)）、卷积神经网络（Mishra 等人，[2018](#bib.bib65)）或超网络（Qiao
    等人，[2018](#bib.bib75); Gidaris 和 Komodakis，[2018](#bib.bib28)）对任务的支持实例（或梯度）进行编码。查询实例的预测通常是通过条件化在编码的任务特定状态表示上的调制预测器来获得的。形式上，任务
    $\mathcal{T}_{i}$ 中查询实例 $\bm{x}_{i}$ 的预测可以通过以下方式获得：
- en: '| (5) |  | $\hat{\bm{y}}_{i}=f_{g_{\omega}(\theta,\mathcal{D}_{i})}(\bm{x}_{i})$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\hat{\bm{y}}_{i}=f_{g_{\omega}(\theta,\mathcal{D}_{i})}(\bm{x}_{i})$
    |  |'
- en: where the meta-knowledge $\omega$ plays a role in mapping task-specific states
    to modulation signals to predictors or optimization strategies. In general, $\omega$
    is represented in the form of a external meta model $g$. The meta model $g$ could
    be instantiated with nerual networks (Wang et al., [2016](#bib.bib105)) or external
    memories (Santoro et al., [2016](#bib.bib84)). For the outer-level learning, the
    optimization of the meta-learner is usually coupled into the training of the inner-level
    mapping function since the outputs of the inner-level learning relies on the outputs
    of the meta-learner.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，元知识 $\omega$ 在将任务特定状态映射到调制信号以预测或优化策略方面发挥作用。通常，$\omega$ 以外部元模型 $g$ 的形式表示。元模型
    $g$ 可以通过神经网络（Wang 等人，[2016](#bib.bib105)）或外部记忆（Santoro 等人，[2016](#bib.bib84)）进行实例化。在外层学习中，元学习者的优化通常与内层映射函数的训练相结合，因为内层学习的输出依赖于元学习者的输出。
- en: 'Optimization-based Meta-learning strictly follows a bi-level optimization structure
    and separates the inner-level learning and outer-level learning via different
    gradient descent steps. We take a famous framework namly model-agnostic meta-learning
    (MAML) as an example. These are many studies extend the MAML frameworks. Specifically,
    in the inner-level learning, a base model performs as the predictor and conducts
    a few steps of local optimization based on the emperical loss over support instances
    as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优化的元学习严格遵循双层优化结构，并通过不同的梯度下降步骤将内层学习和外层学习分开。我们以一个著名的框架模型无关的元学习（MAML）为例。许多研究扩展了
    MAML 框架。具体来说，在内层学习中，基本模型作为预测器，基于支持实例的经验损失进行若干步局部优化，如下所示：
- en: '| (6) |  | $\theta_{\mathcal{T}_{i}}=\theta-\gamma\nabla_{\theta}\mathcal{L}(f_{\theta},\mathcal{S}_{i}).$
    |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\theta_{\mathcal{T}_{i}}=\theta-\gamma\nabla_{\theta}\mathcal{L}(f_{\theta},\mathcal{S}_{i}).$
    |  |'
- en: where $\theta$ is the initialization of the base model parameters. We simply
    show one step of gradient descent. By performing the local update of the base
    model, $\theta_{\mathcal{T}_{i}}$ is utilized as the learned model after task-specific
    learning of the task $\mathcal{T}_{i}$. Here, task-specific learning refers to
    regular gradient descent based optimization, which is also the reason why this
    category is called optimization-based meta-learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\theta$ 是基本模型参数的初始化。我们简单展示了一步梯度下降。通过执行基本模型的局部更新，$\theta_{\mathcal{T}_{i}}$
    被用作任务 $\mathcal{T}_{i}$ 特定学习后的学习模型。在这里，任务特定学习指的是常规梯度下降优化，这也是该类别被称为基于优化的元学习的原因。
- en: 'The meta-knowledge $\omega$ is represented in the form of parameter initialization
    in MAML, i.e., $\theta$. There are also other types of representation of meta-knowledge
    been studied. The $\theta$ is assigned to each task as meta-learned global initialization
    before task-specific learning. Therefore, in the outer-level learning, the $\theta$
    is optimized by minimizing evaluation loss across different tasks to ensure that
    the initialization has generalization capacity as the meta-knowledge. Formally,
    the outer-level optimization, i.e., meta-optimization is conducted as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 元知识 $\omega$ 在 MAML 中以参数初始化的形式表示，即 $\theta$。也有其他类型的元知识表示方法被研究过。在每个任务中，$\theta$
    被赋值为任务特定学习之前的元学习全局初始化。因此，在外层学习中，$\theta$ 通过最小化不同任务之间的评估损失来优化，以确保初始化作为元知识具有泛化能力。形式上，外层优化，即元优化的过程如下：
- en: '| (7) |  | $\theta\leftarrow\theta-\alpha\nabla_{\theta}\sum\nolimits_{\mathcal{T}_{i}\in\mathcal{D}^{train}}\mathcal{L}(f_{\theta_{\mathcal{T}_{i}}},\mathcal{Q}_{i})$
    |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\theta\leftarrow\theta-\alpha\nabla_{\theta}\sum\nolimits_{\mathcal{T}_{i}\in\mathcal{D}^{train}}\mathcal{L}(f_{\theta_{\mathcal{T}_{i}}},\mathcal{Q}_{i})$
    |  |'
- en: where the global intialization $\theta$ is updated across all tasks in the meta-training
    dataset $\mathcal{D}^{train}$ with second-order gradients, since $\theta_{\mathcal{T}_{i}}$
    is obainted with gradient descents as equation (6).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在元训练数据集 $\mathcal{D}^{train}$ 中，全球初始化 $\theta$ 会通过二阶梯度在所有任务中更新，因为 $\theta_{\mathcal{T}_{i}}$
    是通过梯度下降获得的，如公式 (6) 所示。
- en: 'Discussion: Pros and Cons These three frameworks of meta-learning techniques
    discussed above roughly covers most of the existing meta-learning methods. We
    conclude their advantages and disadvantages in terms of computation efficiency,
    the sensitivity of task distribution, and applicability. First, metric-based meta-learning
    has a small computational burden since simple similarity calculation requires
    no additional task-specific model update over new tasks. However, when task distribution
    is complex, metric-based methods usually perform unstably in the meta-test phase
    since no task information is absorbed to cope with task differences. Second, model-based
    meta-learning has relatively simple optimization steps compared with optimization-based
    meta-learning which requires second-order gradients. In addition, developed with
    diverse neural network structures, model-based methods usually have broader applicability
    compared to the other two. However, this category is criticized to perform worse
    over out-of-distribution tasks, i.e., is sensitive to task distribution. Third,
    the key advantage of optimization-based meta-learning is that it is usually agnostic
    to base model structure and could be compatible with diverse based models. In
    practice, optimization-based meta-learning show better generalization ability
    when task distribution is complex. However, this category of methods mainly suffers
    from heavy computation due to two levels of gradient descents.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论：优缺点 上述讨论的这三种元学习技术框架大致涵盖了大多数现有的元学习方法。我们根据计算效率、任务分布的敏感性和适用性总结了它们的优缺点。首先，基于度量的元学习计算负担较小，因为简单的相似性计算不需要对新任务进行额外的任务特定模型更新。然而，当任务分布复杂时，基于度量的方法通常在元测试阶段表现不稳定，因为没有吸收任务信息来应对任务差异。其次，与需要二阶梯度的基于优化的元学习相比，基于模型的元学习具有相对简单的优化步骤。此外，基于多种神经网络结构开发的基于模型的方法通常具有比其他两种方法更广泛的适用性。然而，这一类别被批评在处理分布外任务时表现较差，即对任务分布敏感。第三，基于优化的元学习的主要优点是它通常对基础模型结构没有特定要求，并且可以兼容多种基础模型。在实践中，当任务分布复杂时，基于优化的元学习显示出更好的泛化能力。然而，这一类别的方法主要由于两个梯度下降级别而遭受重计算的困扰。
- en: 2.2\. Recommendation Scenarios
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 推荐场景
- en: 'In the following, we will introduce typical scenarios of recommender systems
    that have been studied from the perspective of meta-learning, including cold-start
    recommendation, click-through rate prediction, online recommendation, point-of-interest
    recommendation, and sequential recommendation. There also exist sporadic studies
    discussing meta-learning in other recommendation scenarios (e.g. cross-domain
    recommendation, multi-behavior recommendation), but we only give them a brief
    introduction when dicussing concrete methods in Section [5](#S5 "5\. Meta-leanring
    Methods for Recommendation Systems ‣ Deep Meta-learning in Recommendation Systems:
    A Survey").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们将从元学习的角度介绍已研究的推荐系统的典型场景，包括冷启动推荐、点击率预测、在线推荐、兴趣点推荐和序列推荐。也存在一些零散的研究讨论了其他推荐场景中的元学习（例如跨领域推荐、多行为推荐），但在讨论具体方法时，我们只会在第[5](#S5
    "5\. Meta-leanring Methods for Recommendation Systems ‣ Deep Meta-learning in
    Recommendation Systems: A Survey)节中简要介绍它们。'
- en: Cold-start recommendation. Despite the successful development of deep learning
    in general recommendation methods, one critical challenge to be addressed is the
    cold-start problem. Typically, the data scarcity issue commonly exists under the
    cold-start situations where new users come to visit the online platforms or new
    items are presented. Because observed user-item interactions are usually limited,
    traditional collaborative filtering methods (Billsus et al., [1998](#bib.bib4))
    or deep learning methods (He et al., [2017](#bib.bib33); Cheng et al., [2016](#bib.bib10)),
    which require abundant training data are hard to perform well. Instead of being
    confined to interaction records, content-based methods depict users and items
    based on diverse content information, such as user and item attributes (Gantner
    et al., [2010](#bib.bib26)), textual information (Fu et al., [2019](#bib.bib25)),
    knowledge graphs (Zhang et al., [2021b](#bib.bib121)), social networks (Li et al.,
    [2021](#bib.bib50)) and so on. By doing this, representations of users and items
    are enhanced with additional semantic information so that the demand for interaction
    data is weakened to some extent. Besides that, the cold-start recommendation could
    be treated as an application in few-shot learning where only a small number of
    samples are observed in each task. Similarly, recommendation tasks for new users
    or items with sparse interactions are naturally divided into meta-training tasks,
    and meta-learning techniques are widely utilized to alleviate the data insufficiency
    of cold-start recommendation tasks (Lee et al., [2019](#bib.bib48); Dong et al.,
    [2020](#bib.bib16)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 冷启动推荐。尽管深度学习在通用推荐方法中的发展取得了成功，但需要解决的一个关键挑战是冷启动问题。通常，数据稀缺问题在冷启动情况下普遍存在，例如新用户访问在线平台或新项目出现时。由于观察到的用户-项目交互通常有限，传统的协同过滤方法（Billsus
    et al., [1998](#bib.bib4)）或深度学习方法（He et al., [2017](#bib.bib33); Cheng et al.,
    [2016](#bib.bib10)），由于需要大量训练数据，因此很难表现良好。内容驱动的方法不局限于交互记录，而是基于多样的内容信息描述用户和项目，例如用户和项目属性（Gantner
    et al., [2010](#bib.bib26)）、文本信息（Fu et al., [2019](#bib.bib25)）、知识图谱（Zhang et
    al., [2021b](#bib.bib121)）、社交网络（Li et al., [2021](#bib.bib50)）等。通过这样做，用户和项目的表示得到了额外的语义信息，从而在一定程度上减弱了对交互数据的需求。此外，冷启动推荐可以被视为一种少样本学习的应用，其中每个任务中仅观察到少量样本。类似地，对于新用户或交互稀少的项目的推荐任务，自然地被划分为元训练任务，而元学习技术被广泛应用于缓解冷启动推荐任务的数据不足问题（Lee
    et al., [2019](#bib.bib48); Dong et al., [2020](#bib.bib16)）。
- en: Click-through-rate prediction. In online advertising applications, click-through
    rate (CTR) is a key index to determine the values of published ads (Cheng et al.,
    [2016](#bib.bib10); Shen et al., [2022](#bib.bib88); Zhou et al., [2018a](#bib.bib127)).
    A rational ad auction mechanism should spend more cost on ads with higher CTRs,
    so as to ensure greater benefits. Therefore, accurate CTR prediction provided
    by advertisement publishers could assist investors with subsequent resource allocations.
    To estimate the click probability of a user-ad pair, recent CTR prediction models
    usually follow a general framework consisting of two parts including an embedding
    layer and a prediction layer (Cheng et al., [2016](#bib.bib10)). Specifically,
    the embedding layer first learns latent embedding vectors for both ad/user ids
    and other rich features. Then the prediction layer is utilized to model feature
    interaction or dependencies with sophisticated models which are usually well designed
    as deep neural structures. Despite its success in both academia and industry,
    the majority of these methods work poorly on new ads due to the lack of embedding
    learning (Pan et al., [2019](#bib.bib70)). Known as the cold-start problem in
    CTR prediction, embeddings (especially identity embeddings) of new ads which have
    limited click records, are hard to be trained as well as other existing ads. As
    we investigated, meta-learning methods have been studied to strengthen the embedding
    learning for cold-start ads.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 点击率预测。在在线广告应用中，点击率（CTR）是确定发布广告价值的关键指标（Cheng et al., [2016](#bib.bib10); Shen
    et al., [2022](#bib.bib88); Zhou et al., [2018a](#bib.bib127)）。一个合理的广告竞价机制应该在点击率较高的广告上花费更多成本，以确保更大的收益。因此，广告发布者提供的准确CTR预测可以帮助投资者进行后续的资源分配。为了估计用户-广告对的点击概率，最近的CTR预测模型通常遵循一个包含嵌入层和预测层的通用框架（Cheng
    et al., [2016](#bib.bib10)）。具体来说，嵌入层首先为广告/用户ID及其他丰富特征学习潜在嵌入向量。然后，预测层用于利用复杂的模型建模特征交互或依赖关系，这些模型通常设计为深度神经网络结构。尽管这些方法在学术界和工业界取得了成功，但由于缺乏嵌入学习，大多数这些方法在新广告上表现不佳（Pan
    et al., [2019](#bib.bib70)）。在CTR预测中，这被称为冷启动问题，具有有限点击记录的新广告的嵌入（特别是身份嵌入）很难像其他现有广告一样被训练。我们调查发现，元学习方法已被研究用于加强冷启动广告的嵌入学习。
- en: Online recommendation. In practical large-scale recommender systems, real-time
    user interaction data are generated and collected continuously. It is necessary
    to timely refresh the recommendation models previously learned so that dynamic
    user preferences and trends could be captured (Guo et al., [2020](#bib.bib29);
    He et al., [2016](#bib.bib34)). Instead of offline training a model purely based
    on historical logs, online recommendation attempts to continuously update current
    recommendation models based on newly arrived data in an online fashion. Online
    learning strategies and model retraining mechanisms are explored in this field
    to meet the needs. Due to practical requirements in real-world applications, computation
    efficiency is a critical factor that should be emphasized. For instance, full
    retraining over both historical and new samples is an ideal strategy for model
    refreshing but is pretty impractical for unacceptable time cost (Zhang et al.,
    [2020](#bib.bib124)). Therefore, to improve the ability of fast learning, meta-learning
    has been introduced into online recommendation scenarios and used to quickly capture
    dynamic preference trends from real-time user interaction data (Zhang et al.,
    [2020](#bib.bib124); Peng et al., [2021](#bib.bib72)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在线推荐。在实际的大规模推荐系统中，实时用户互动数据会不断生成和收集。需要及时刷新之前学习的推荐模型，以便捕捉动态的用户偏好和趋势（Guo et al.,
    [2020](#bib.bib29); He et al., [2016](#bib.bib34)）。在线推荐试图基于新到达的数据以在线方式不断更新当前推荐模型，而不是仅基于历史日志离线训练模型。这个领域探讨了在线学习策略和模型重训练机制以满足需求。由于实际应用中的要求，计算效率是一个需要强调的关键因素。例如，针对历史和新样本进行全面重训练是模型刷新的理想策略，但由于不可接受的时间成本，实际上并不实用（Zhang
    et al., [2020](#bib.bib124)）。因此，为了提高快速学习的能力，元学习已被引入到在线推荐场景中，并用于迅速捕捉来自实时用户互动数据的动态偏好趋势（Zhang
    et al., [2020](#bib.bib124); Peng et al., [2021](#bib.bib72)）。
- en: Point-of-interest recommendation With the emergence of location-based social
    networks (LBSNs), users are willing to share their visited point-of-interests
    (POIs) through check-in records. LBSN services are supposed to provide personalized
    recommendations on other POIs that users have not visited. Compared with general
    item (e.g., product, music, and movie) recommendation, POI recommendation relies
    more on discovering spatial-temporal dependencies from historical check-in data.
    This phenomenon is also very intuitive since users’ activities are largely influenced
    by geospatial and temporal constraints. By incorporating geographical and time
    information of check-in data, a series of approaches involving spatio-temporal
    modeling are proposed for POI recommendation (Sun et al., [2020](#bib.bib93);
    Zhao et al., [2019](#bib.bib125)). Despite their success, the data sparsity issue
    is obvious in this recommendation scenario since users must arrive at the location
    of shared POIs. In other words, it is common that users just visited a small number
    of POIs because of the high cost of data generation. Therefore, meta-learning
    based POI recommendation methods have been studied to face severe data sparsity
    (Sun et al., [2021b](#bib.bib92); Cui et al., [2021](#bib.bib12)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 兴趣点推荐 随着基于位置的社交网络（LBSNs）的出现，用户愿意通过签到记录分享他们访问的兴趣点（POIs）。LBSN服务应该提供个性化的推荐，推荐用户尚未访问的其他POIs。与一般的物品（如产品、音乐和电影）推荐相比，POI推荐更多地依赖于从历史签到数据中发现时空依赖关系。这一现象也非常直观，因为用户的活动在很大程度上受到地理空间和时间限制的影响。通过结合签到数据的地理和时间信息，提出了一系列涉及时空建模的方法用于POI推荐（Sun
    et al., [2020](#bib.bib93); Zhao et al., [2019](#bib.bib125)）。尽管这些方法取得了成功，但数据稀疏问题在这一推荐场景中依然显著，因为用户必须到达共享POIs的位置。换句话说，由于数据生成成本高，用户通常只访问少量POIs。因此，基于元学习的POI推荐方法已经被研究，以应对严重的数据稀疏问题（Sun
    et al., [2021b](#bib.bib92); Cui et al., [2021](#bib.bib12)）。
- en: Sequential recommendation The heart of sequential recommendation is to capture
    evolving user preferences from users’ interaction sequences. Different from traditional
    collaborative filtering methods which organize interactions in the form of user-item
    pairs, sequential recommendation methods mainly utilize the sequence of previously
    interacted items of a user as input, and make efforts to discover sequential patterns
    of user interest evolution. Specifically, representative sequential modeling methods
    including Markov Chains (He and McAuley, [2016](#bib.bib32); Rendle et al., [2010](#bib.bib80)),
    recurrent neural networks (Hidasi et al., [2016](#bib.bib35); Li et al., [2017](#bib.bib51)),
    and self-attention based networks (Kang and McAuley, [2018](#bib.bib42); Xu et al.,
    [2019](#bib.bib113)), have achieved promising performance in modeling both short-term
    and long-term interests based on interaction sequences. However, the performances
    of sequential recommenders usually rely on sufficient items in the sequences.
    When the number of historical interactions is relatively small, model performance
    tends to degrade significantly and fluctuate greatly. Consequently, the data sparsity
    issue also brings stubborn obstacles in the sequential recommendation scenario.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序推荐 顺序推荐的核心是捕捉用户交互序列中的不断变化的用户偏好。与传统的协同过滤方法（将交互组织为用户-物品对）不同，顺序推荐方法主要利用用户先前交互的物品序列作为输入，并努力发现用户兴趣演变的顺序模式。具体而言，代表性的顺序建模方法包括马尔可夫链（He
    和 McAuley, [2016](#bib.bib32); Rendle et al., [2010](#bib.bib80)）、递归神经网络（Hidasi
    et al., [2016](#bib.bib35); Li et al., [2017](#bib.bib51)）和基于自注意力的网络（Kang 和 McAuley,
    [2018](#bib.bib42); Xu et al., [2019](#bib.bib113)），在基于交互序列建模短期和长期兴趣方面取得了良好的效果。然而，顺序推荐系统的性能通常依赖于序列中的足够物品。当历史交互数量相对较少时，模型性能往往显著下降并波动很大。因此，数据稀疏问题也在顺序推荐场景中带来了顽固的障碍。
- en: '![Refer to caption](img/3bc0365a5bc8cb87c7f3d451afb286fd.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/3bc0365a5bc8cb87c7f3d451afb286fd.png)'
- en: Figure 1\. Taxonomy of deep meta-learning based recommendation systems.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 基于深度元学习的推荐系统分类法。
- en: 3\. Taxonomy
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 分类法
- en: In this section, we establish our taxonomy of deep meta-learning based recommendation
    systems and summarize the characteristics of existing methods according to the
    taxonomy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们建立了基于深度元学习的推荐系统的分类法，并根据该分类法总结了现有方法的特点。
- en: 'In general, we define our taxonomy in terms of three independent axes, including
    recommendation scenarios, meta-learning techniques, and meta-knowledge representation.
    Fig.1 shows the taxonomy. The previous taxonomy of general meta-learning methods
    proposed in (Huisman et al., [2021](#bib.bib39); Vanschoren, [2018](#bib.bib99))
    cares more about three categories of meta-learning frameworks as introduced in
    section 2.1 but pays limited attention to practical applications of meta-learning
    techniques. In addition, (Hospedales et al., [2020](#bib.bib37)) propose a new
    taxonomy involving three perspectives including meta-representation, meta-optimizer
    and meta-objective. They provide a more comprehensive breakdown that can orient
    the development of new meta-learning methods. However, it focuses on the whole
    meta-learning landscape and is inappropriate to reflect the current research status
    and application scenarios in deep meta-learning based recommendation systems.
    Therefore, we concentrate on the recommendation system community and summarize
    the characteristics of existing works following three dimensions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们将分类法定义为三个独立的轴，包括推荐场景、元学习技术和元知识表示。图1展示了分类法。之前的通用元学习方法分类法（Huisman et al.,
    [2021](#bib.bib39); Vanschoren, [2018](#bib.bib99)）更关注第2.1节介绍的三类元学习框架，但对元学习技术的实际应用关注较少。此外，（Hospedales
    et al., [2020](#bib.bib37)）提出了一种包含三个视角的新分类法，包括元表示、元优化器和元目标。他们提供了一个更全面的分类，可以指导新元学习方法的发展。然而，它关注的是整个元学习领域，不适合反映深度元学习基础的推荐系统的当前研究状态和应用场景。因此，我们专注于推荐系统社区，总结现有工作的特征，按照三个维度：
- en: 'Recommendation scenarios (Where): This axis presents the specific scenario
    *where* the meta-learning based recommendation methods are proposed and applied.
    As introduced in section 2.2, we summarize typical recommendation scenarios into
    the following groups 1) cold-start recommendation, 2) click-through-rate prediction,
    3) online recommendation, 4) point of interest recommendation, 5) sequential recommendation,
    and 6) others. For clarity, we do not display all involved recommendation scenarios
    one by one but include less studied scenarios together and denote them as *others*.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐场景（哪里）：这个轴展示了基于元学习的推荐方法提出和应用的具体场景。如第2.2节所述，我们将典型的推荐场景总结为以下几个组：1) 冷启动推荐，2)
    点击率预测，3) 在线推荐，4) 兴趣点推荐，5) 序列推荐，以及6) 其他。为了清晰起见，我们没有逐一显示所有涉及的推荐场景，而是将研究较少的场景归纳为*其他*。
- en: 'Meta-learning techniques (How): This axis presents the way how to apply meta-learning
    to enhance generalization ability over new recommendation tasks. Following the
    taxonomy in (Huisman et al., [2021](#bib.bib39); Vanschoren, [2018](#bib.bib99)),
    we also divide meta-learning techniques into three categories including metric-based
    meta-learning, model-based meta-learning, and optimization-based meta-learning.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习技术（如何）：这个轴展示了如何应用元学习来增强对新推荐任务的泛化能力。按照(Huisman et al., [2021](#bib.bib39);
    Vanschoren, [2018](#bib.bib99))的分类法，我们也将元学习技术分为三类，包括基于指标的元学习、基于模型的元学习和基于优化的元学习。
- en: 'Table 2\. Summarization of all meta-learning based recommendation methods.
    We organize all these methods from hierarchical perspectives of scenarios and
    meta-learning techniques. We use the following abbreviations. Optimi.: Optimization-based.
    Model: Model-based. Para. Init.: Parameter Initialization. Para. Modu.: Parameter
    Modulation. Hyperpara.: Hyperparameter. Embedd. Space.: Embedding Space.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 元学习基础的推荐方法总结。我们从场景和元学习技术的层次角度组织所有这些方法。我们使用以下缩写。优化方法：基于优化的。模型：基于模型的。参数初始化：Parameter
    Initialization。参数调节：Parameter Modulation。超参数：Hyperparameter。嵌入空间：Embedding Space。
- en: '| Scenario | Method |  Venue | Year | Meta-learning Technique | Meta-learning
    Representions |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 方法 | 场所 | 年份 | 元学习技术 | 元学习表示 |'
- en: '| Optimi. | Model | Metric | Para. Init. | Para. Modu. | Hyper- para. | Meta-
    Model | Embed. space | Sample Weight |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 优化方法 | 模型 | 指标 | 参数初始化 | 参数调节 | 超参数 | 元模型 | 嵌入空间 | 样本权重 |'
- en: '| Cold-start Recommendation | LWA (Vartak et al., [2017](#bib.bib100)) | NIPS
    | 2017 |  | ✓ |  |  | ✓ |  |  |  |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 冷启动推荐 | LWA (Vartak et al., [2017](#bib.bib100)) | NIPS | 2017 |  | ✓ |  |  |
    ✓ |  |  |  |  |'
- en: '| MeLU (Lee et al., [2019](#bib.bib48)) | KDD | 2019 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| MeLU (Lee et al., [2019](#bib.bib48)) | KDD | 2019 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| MetaCS (Bharadhwaj, [2019](#bib.bib3)) | IJCNN | 2019 | ✓ |  |  | ✓ |  |
    ✓ |  |  |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| MetaCS (Bharadhwaj, [2019](#bib.bib3)) | IJCNN | 2019 | ✓ |  |  | ✓ |  |
    ✓ |  |  |  |'
- en: '| MetaHIN (Lu et al., [2020](#bib.bib59)) | KDD | 2020 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| MetaHIN (Lu et al., [2020](#bib.bib59)) | KDD | 2020 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
- en: '| MAMO (Dong et al., [2020](#bib.bib16)) | KDD | 2020 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MAMO (Dong et al., [2020](#bib.bib16)) | KDD | 2020 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
- en: '| MetaCF(Wei et al., [2020](#bib.bib108)) | ICDM | 2020 | ✓ |  |  | ✓ |  |
    ✓ |  |  |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MetaCF(Wei et al., [2020](#bib.bib108)) | ICDM | 2020 | ✓ |  |  | ✓ |  |
    ✓ |  |  |  |'
- en: '| TaNP (Lin et al., [2021](#bib.bib54)) | WWW | 2021 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| TaNP (Lin et al., [2021](#bib.bib54)) | WWW | 2021 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
- en: '| PALRML (Yu et al., [2021](#bib.bib120)) | AAAI | 2021 | ✓ |  |  | ✓ |  |
    ✓ |  |  |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| PALRML (Yu et al., [2021](#bib.bib120)) | AAAI | 2021 | ✓ |  |  | ✓ |  |
    ✓ |  |  |  |'
- en: '| MIRec (Zhang et al., [2021a](#bib.bib123)) | WWW | 2021 |  | ✓ |  |  | ✓
    |  | ✓ |  |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| MIRec (Zhang et al., [2021a](#bib.bib123)) | WWW | 2021 |  | ✓ |  |  | ✓
    |  | ✓ |  |  |'
- en: '| MPML (Chen et al., [2021a](#bib.bib9)) | ECIR | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MPML (Chen et al., [2021a](#bib.bib9)) | ECIR | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| PAML(Wang et al., [2021b](#bib.bib106)) | IJCAI | 2021 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| PAML(Wang et al., [2021b](#bib.bib106)) | IJCAI | 2021 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
- en: '| CMML (Feng et al., [2021](#bib.bib22)) | CIKM | 2021 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| CMML (Feng et al., [2021](#bib.bib22)) | CIKM | 2021 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
- en: '| Heater (Zhu et al., [2020b](#bib.bib135)) | SIGIR | 2021 |  | ✓ |  |  | ✓
    |  | ✓ |  |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Heater (Zhu et al., [2020b](#bib.bib135)) | SIGIR | 2021 |  | ✓ |  |  | ✓
    |  | ✓ |  |  |'
- en: '| PreTraining (Hao et al., [2021](#bib.bib31)) | SIGIR | 2021 |  |  | ✓ |  |  |  |  |
    ✓ |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| PreTraining (Hao et al., [2021](#bib.bib31)) | SIGIR | 2021 |  |  | ✓ |  |  |  |  |
    ✓ |  |'
- en: '| ProtoCF (Sankar et al., [2021](#bib.bib83)) | Recsys | 2021 |  |  | ✓ |  |  |  |  |
    ✓ |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ProtoCF (Sankar et al., [2021](#bib.bib83)) | Recsys | 2021 |  |  | ✓ |  |  |  |  |
    ✓ |  |'
- en: '| MetaEDL (Neupane et al., [2021](#bib.bib68)) | ICDM | 2021 | ✓ |  |  | ✓
    |  |  |  |  |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| MetaEDL (Neupane et al., [2021](#bib.bib68)) | ICDM | 2021 | ✓ |  |  | ✓
    |  |  |  |  |  |'
- en: '|  | DML (Neupane et al., [2022](#bib.bib67)) | AAAI | 2022 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | DML (Neupane et al., [2022](#bib.bib67)) | AAAI | 2022 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '|  | PNMTA (Pang et al., [2022](#bib.bib71)) | WWW | 2022 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | PNMTA (Pang et al., [2022](#bib.bib71)) | WWW | 2022 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
- en: '| Click Through Rate Prediction | Meta-Embed. (Pan et al., [2019](#bib.bib70))
    | SIGIR | 2019 | ✓ |  |  | ✓ |  |  | ✓ |  |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Click Through Rate Prediction | Meta-Embed. (Pan et al., [2019](#bib.bib70))
    | SIGIR | 2019 | ✓ |  |  | ✓ |  |  | ✓ |  |  |'
- en: '| TDAML (Cao et al., [2020](#bib.bib6)) | ACMMM | 2020 | ✓ |  |  | ✓ |  |  |
    ✓ |  | ✓ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| TDAML (Cao et al., [2020](#bib.bib6)) | ACMMM | 2020 | ✓ |  |  | ✓ |  |  |
    ✓ |  | ✓ |'
- en: '| MWUF (Zhu et al., [2021d](#bib.bib134)) | SIGIR | 2021 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| MWUF (Zhu et al., [2021d](#bib.bib134)) | SIGIR | 2021 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
- en: '| DisNet (Li et al., [2020](#bib.bib52)) | Complexity | 2021 | ✓ |  |  | ✓
    |  |  | ✓ |  |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| DisNet (Li et al., [2020](#bib.bib52)) | Complexity | 2021 | ✓ |  |  | ✓
    |  |  | ✓ |  |  |'
- en: '| GME (Ouyang et al., [2021](#bib.bib69)) | SIGIR | 2021 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GME (Ouyang et al., [2021](#bib.bib69)) | SIGIR | 2021 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
- en: '| Meta-SSIN (Sun et al., [2021c](#bib.bib95)) | SIGIR(short) | 2021 | ✓ |  |  |
    ✓ |  |  |  |  |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Meta-SSIN (Sun et al., [2021c](#bib.bib95)) | SIGIR(short) | 2021 | ✓ |  |  |
    ✓ |  |  |  |  |  |'
- en: '| Point of Interest Recommendation | PREMERE (Kim et al., [2021](#bib.bib44))
    | AAAI | 2021 |  | ✓ |  |  |  |  | ✓ |  | ✓ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Point of Interest Recommendation | PREMERE (Kim et al., [2021](#bib.bib44))
    | AAAI | 2021 |  | ✓ |  |  |  |  | ✓ |  | ✓ |'
- en: '| MetaODE (Tan et al., [2021](#bib.bib98)) | MDM | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MetaODE (Tan et al., [2021](#bib.bib98)) | MDM | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| MFNP (Sun et al., [2021b](#bib.bib92)) | IJCAI | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MFNP (Sun et al., [2021b](#bib.bib92)) | IJCAI | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| CHAML (Chen et al., [2021b](#bib.bib8)) | KDD | 2021 | ✓ |  |  | ✓ |  |  |  |  |
    ✓ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| CHAML (Chen et al., [2021b](#bib.bib8)) | KDD | 2021 | ✓ |  |  | ✓ |  |  |  |  |
    ✓ |'
- en: '| Meta-SKR (Cui et al., [2021](#bib.bib12)) | TOIS | 2022 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Meta-SKR (Cui et al., [2021](#bib.bib12)) | TOIS | 2022 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
- en: 'Table 3\. Summarization of all meta-learning based recommendation methods.
    We organize all these methods from hierarchical perspectives of scenarios and
    meta-learning techniques. We use the following abbreviations. Optimi.: Optimization-based.
    Model: Model-based. Para. Init.: Parameter Initialization. Para. Modu.: Parameter
    Modulation. Hyperpara.: Hyperparameter. Embedd. Space.: Embedding Space.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 所有基于元学习的推荐方法的汇总。我们从场景和元学习技术的层次角度组织所有这些方法。我们使用以下缩写：Optimi.：基于优化的。Model：基于模型的。Para.
    Init.：参数初始化。Para. Modu.：参数调节。Hyperpara.：超参数。Embedd. Space.：嵌入空间。
- en: '| Scenario | Method |  Venue | Year | Meta-learning Technique | Meta-learning
    Representions |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 方法 |  会议 | 年份 | 元学习技术 | 元学习表示 |'
- en: '| Optimi. | Model | Metric | Para. Init. | Para. Modu. | Hyper- para. | Meta-
    Model | Embed. space | Sample Weight |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 优化 | 模型 | 评测指标 | 参数初始化 | 参数调节 | 超参数 | 元模型 | 嵌入空间 | 样本权重 |'
- en: '| Online Recommendation | S2Meta (Du et al., [2019](#bib.bib18)) | KDD | 2019
    | ✓ |  |  | ✓ |  | ✓ | ✓ |  |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 在线推荐 | S2Meta (Du et al., [2019](#bib.bib18)) | KDD | 2019 | ✓ |  |  | ✓
    |  | ✓ | ✓ |  |  |'
- en: '| SML (Zhang et al., [2020](#bib.bib124)) | SIGIR | 2020 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| SML (Zhang et al., [2020](#bib.bib124)) | SIGIR | 2020 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
- en: '| FLIP (Liu et al., [2020b](#bib.bib58)) | IJCAI | 2020 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| FLIP (Liu et al., [2020b](#bib.bib58)) | IJCAI | 2020 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| FORM (Sun et al., [2021a](#bib.bib94)) | SIGIR | 2021 | ✓ |  |  | ✓ |  |
    ✓ |  |  |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| FORM (Sun et al., [2021a](#bib.bib94)) | SIGIR | 2021 | ✓ |  |  | ✓ |  |
    ✓ |  |  |  |'
- en: '| LSTTM (Xie et al., [2021](#bib.bib112)) | WSDM | 2022 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| LSTTM (Xie et al., [2021](#bib.bib112)) | WSDM | 2022 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| ASMG (Peng et al., [2021](#bib.bib72)) | Recsys | 2021 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ASMG (Peng et al., [2021](#bib.bib72)) | Recsys | 2021 |  | ✓ |  |  | ✓ |  |
    ✓ |  |  |'
- en: '| MeLON (Kim et al., [2022](#bib.bib45)) | AAAI | 2022 |  | ✓ |  |  |  | ✓
    | ✓ |  |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| MeLON (Kim et al., [2022](#bib.bib45)) | AAAI | 2022 |  | ✓ |  |  |  | ✓
    | ✓ |  |  |'
- en: '| Sequential Recommendation | Mecos (Zheng et al., [2021](#bib.bib126)) | AAAI
    | 2021 |  |  | ✓ |  |  |  |  | ✓ |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 顺序推荐 | Mecos (Zheng et al., [2021](#bib.bib126)) | AAAI | 2021 |  |  | ✓
    |  |  |  |  | ✓ |  |'
- en: '| MetaTL (Wang et al., [2021a](#bib.bib103)) | SIGIR(short) | 2021 | ✓ |  |  |
    ✓ |  |  |  |  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| MetaTL (Wang et al., [2021a](#bib.bib103)) | SIGIR（短篇） | 2021 | ✓ |  |  |
    ✓ |  |  |  |  |  |'
- en: '| CBML (Song et al., [2021](#bib.bib90)) | CIKM | 2021 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| CBML (Song et al., [2021](#bib.bib90)) | CIKM | 2021 | ✓ |  |  | ✓ |  |  |
    ✓ |  |  |'
- en: '| metaCSR (Huang et al., [2022](#bib.bib38)) | TOIS | 2022 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| metaCSR (Huang et al., [2022](#bib.bib38)) | TOIS | 2022 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| Cross Domain Recommendation | TMCDR (Zhu et al., [2021a](#bib.bib130)) |
    SIGIR(short) | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 跨领域推荐 | TMCDR (Zhu et al., [2021a](#bib.bib130)) | SIGIR（短篇） | 2021 | ✓ |  |  |
    ✓ |  |  |  |  |  |'
- en: '| PTUPCDR (Zhu et al., [2021c](#bib.bib133)) | WSDM | 2022 |  | ✓ |  |  | ✓
    |  | ✓ |  |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| PTUPCDR (Zhu et al., [2021c](#bib.bib133)) | WSDM | 2022 |  | ✓ |  |  | ✓
    |  | ✓ |  |  |'
- en: '| Multi-behavior Recommendation | CML (Wei et al., [2022](#bib.bib109)) | WSDM
    | 2022 |  | ✓ |  |  |  | ✓ | ✓ |  |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 多行为推荐 | CML (Wei et al., [2022](#bib.bib109)) | WSDM | 2022 |  | ✓ |  |  |  |
    ✓ | ✓ |  |  |'
- en: '| MB-GMN (Xia et al., [2021](#bib.bib111)) | SIGIR | 2021 |  | ✓ |  |  | ✓
    |  | ✓ |  |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| MB-GMN (Xia et al., [2021](#bib.bib111)) | SIGIR | 2021 |  | ✓ |  |  | ✓
    |  | ✓ |  |  |'
- en: '| Others | MetaKG (Du et al., [2022](#bib.bib17)) | TKDE | 2022 | ✓ |  |  |
    ✓ |  |  |  |  |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | MetaKG (Du et al., [2022](#bib.bib17)) | TKDE | 2022 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| MetaSelector (Luo et al., [2020](#bib.bib60)) | WWW | 2020 | ✓ |  |  | ✓
    |  | ✓ |  |  |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| MetaSelector (Luo et al., [2020](#bib.bib60)) | WWW | 2020 | ✓ |  |  | ✓
    |  | ✓ |  |  |  |'
- en: '| Meta-SF (Lasserre et al., [2020](#bib.bib47)) | SDM | 2019 | ✓ |  |  |  |  |  |
    ✓ |  |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Meta-SF (Lasserre et al., [2020](#bib.bib47)) | SDM | 2019 | ✓ |  |  |  |  |  |
    ✓ |  |  |'
- en: '| MetaMF (Lin et al., [2020](#bib.bib55)) | SIGIR | 2020 | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| MetaMF (Lin et al., [2020](#bib.bib55)) | SIGIR | 2020 | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |'
- en: '| MetaHeac (Zhu et al., [2021b](#bib.bib132)) | KDD | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| MetaHeac (Zhu et al., [2021b](#bib.bib132)) | KDD | 2021 | ✓ |  |  | ✓ |  |  |  |  |  |'
- en: '| NICF (Zou et al., [2020](#bib.bib136)) | SIGIR | 2021 |  | ✓ |  |  |  |  |
    ✓ |  |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| NICF (Zou et al., [2020](#bib.bib136)) | SIGIR | 2021 |  | ✓ |  |  |  |  |
    ✓ |  |  |'
- en: 'Meta-knowledge representations (What): This axis presents the form of meta-knowledge
    to be represented so that it could be beneficial for improving the fast learning
    of recommendation models. After distilling from existing works, we summarize common
    representations of meta-knowledge as parameter initialization, parameter modulation,
    hyperparameters, sample weights, embedding space, and meta model. Generally speaking,
    different meta-learning techniques have distinct characteristics of meta-knowledge
    representation. For example, parameter initialization is usually achieved under
    the optimization-based meta-learning while parameter modulation is more likely
    to belong to model-based meta-learning. However, there are also situations where
    multiple types of meta-knowledge representations are learned simultaneously in
    a hybrid manner.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 元知识表示（What）：这一维度展示了元知识的表示形式，以便有助于提升推荐模型的快速学习。经过对现有工作的提炼，我们总结了元知识的常见表示方式，包括参数初始化、参数调节、超参数、样本权重、嵌入空间和元模型。一般来说，不同的元学习技术具有不同的元知识表示特征。例如，参数初始化通常是在基于优化的元学习下实现的，而参数调节更可能属于基于模型的元学习。然而，也有多种类型的元知识表示同时以混合方式进行学习的情况。
- en: 'By investigating existing works from the three independent dimensions above,
    our taxonomy is expected to be able to provide a clear design space for deep meta-learning
    based recommendation methods. we organize papers according to recommendation scenarios
    and present characteristics of these works along with the taxonomy in table [2](#S3.T2
    "Table 2 ‣ 3\. Taxonomy ‣ Deep Meta-learning in Recommendation Systems: A Survey")
    and [3](#S3.T3 "Table 3 ‣ 3\. Taxonomy ‣ Deep Meta-learning in Recommendation
    Systems: A Survey"), which lists detailed publication information, and highlights
    major meta-learning techniques and the forms of meta-knowledge representations.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '通过从上述三个独立维度调查现有工作，我们的分类法有望提供一个清晰的设计空间，用于深度元学习推荐方法。我们根据推荐场景组织论文，并展示这些工作的特征，以及表格[2](#S3.T2
    "Table 2 ‣ 3\. Taxonomy ‣ Deep Meta-learning in Recommendation Systems: A Survey")和[3](#S3.T3
    "Table 3 ‣ 3\. Taxonomy ‣ Deep Meta-learning in Recommendation Systems: A Survey")中的分类法，列出了详细的出版信息，并突出了主要的元学习技术和元知识表示的形式。'
- en: 4\. Meta-learning Task Construction for Recommendation
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 元学习任务构建用于推荐
- en: In this section, we summarize different ways of meta-learning recommendation
    task construction used in the literature. As discussed in section 2.1, one major
    difference between the meta-learning paradigm and the regular deep learning paradigm
    is the setup of task division. We will first introduce the general form of constructing
    meta-learning tasks and then present practical ways adopted in deep meta-learning
    based recommendation methods, which are quite different from other fields.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们总结了文献中使用的不同元学习推荐任务构建方法。如第2.1节讨论的那样，元学习范式与常规深度学习范式之间的主要区别在于任务分割的设置。我们将首先介绍构建元学习任务的一般形式，然后展示在深度元学习推荐方法中采用的实际方法，这些方法与其他领域有很大不同。
- en: In general, meta-learning methods usually follow the setting of constructing
    disjoint meta-training tasks $\mathcal{D}^{train}$ and meta-test tasks $\mathcal{D}^{test}$.
    Each task is split into a set of training instances (named support set $\mathcal{S}_{i}$)
    and a disjoint set of evaluation instances (named query set $\mathcal{Q}_{i}$).
    The objective of each task is to learn quickly from the support set $\mathcal{S}_{i}$,
    so as to perform better over unseen instances in the query set $\mathcal{Q}_{i}$.
    At a single task level, its learning objective is similar to the regular deep
    learning paradigm, except that data insufficiency of the task is usually emphasized
    in meta-learning. When considering the whole task distribution (or multiple tasks),
    a higher level of learning objective (i.e., meta-optimization objective) is defined
    as better performance on evaluation instances of unseen tasks (i.e., $\mathcal{D}^{test}$).
    Consequently, the setup of task division above is consistent with the meta-optimization
    objective, facilitating the evaluation of generalization ability and the fast
    learning ability of meta-learning methods over multiple new tasks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，元学习方法通常遵循构建不相交的元训练任务$\mathcal{D}^{train}$和元测试任务$\mathcal{D}^{test}$的设置。每个任务被拆分为一组训练实例（称为支持集$\mathcal{S}_{i}$）和一组不相交的评估实例（称为查询集$\mathcal{Q}_{i}$）。每个任务的目标是从支持集$\mathcal{S}_{i}$中快速学习，以便在查询集$\mathcal{Q}_{i}$中的未见实例上表现更好。在单个任务级别，其学习目标类似于常规深度学习范式，只是元学习中通常强调任务的数据不足。当考虑整个任务分布（或多个任务）时，定义了一个更高层次的学习目标（即，元优化目标），即在未见任务的评估实例上取得更好的表现（即，$\mathcal{D}^{test}$）。因此，上述任务分割的设置与元优化目标一致，促进了对元学习方法在多个新任务中的泛化能力和快速学习能力的评估。
- en: Different from meta-learning task construction settings in other application
    domains, constructing meta-learning recommendation tasks should meet the special
    needs of different recommendation scenarios. For popular few-shot classification
    tasks such as image recognition and objective detection, a commonly used setting
    is $N$-way, $K$-shot classification (Finn et al., [2017](#bib.bib23)). Specifically,
    based on a pool with a large number of classes, a task is obtained by randomly
    sampling $N$ classes first and then sampling K instances belonging to each class.
    The $K$ is usually set as a small number to meet the requirements of a few-shot
    task. For meta-learning tasks in natural language processing, Lee et al. (Lee
    et al., [2022](#bib.bib49)) summarize different settings of task construction
    including cross-domain, cross-lingual, cross-problem, domain-generalization, and
    homogenous task augmentation. For instance, tasks in the cross-domain setting
    are from different domains (e.g., texts from news and laws are considered as different
    domains), while tasks in the cross-lingual setting are divided based on different
    languages. Overall, The settings of meta-learning tasks in the other fields mentioned
    above are closely related to the task objectives and data characteristics. Therefore,
    we specially discuss the construction of meta-learning recommendation tasks and
    present how existing meta-learning methods perform task division with interaction
    data from recommendation systems.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他应用领域中的元学习任务构建设置不同，构建元学习推荐任务应满足不同推荐场景的特殊需求。对于图像识别和目标检测等流行的少样本分类任务，常用的设置是$N$-way,
    $K$-shot分类（Finn等，[2017](#bib.bib23)）。具体来说，在具有大量类别的池基础上，首先随机抽取$N$个类别，然后抽取每个类别的$K$个实例，从而获得一个任务。$K$通常设置为一个较小的数字，以满足少样本任务的要求。对于自然语言处理中的元学习任务，Lee等（Lee等，[2022](#bib.bib49)）总结了不同的任务构建设置，包括跨领域、跨语言、跨问题、领域泛化和同质任务增强。例如，跨领域设置中的任务来自不同领域（例如，新闻和法律的文本被视为不同领域），而跨语言设置中的任务则基于不同语言进行划分。总体而言，上述其他领域中的元学习任务设置与任务目标和数据特征密切相关。因此，我们特别讨论了元学习推荐任务的构建，并展示了现有的元学习方法如何利用推荐系统中的交互数据进行任务划分。
- en: 'According to common properties belonging to interactions in a task, we mainly
    summarize the task construction ways into four categories, including *user-specific*
    task, *item-specific*, *time-specific* task and *sequence-specific* task. To be
    mentioned, there are a few works that have tried other ways but the number is
    relatively small. We organize them all in the category named $others$. Table [4](#S4.T4
    "Table 4 ‣ 4\. Meta-learning Task Construction for Recommendation ‣ Deep Meta-learning
    in Recommendation Systems: A Survey") shows the summary of works adapting each
    category of task construction.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '根据任务中的交互所具有的共同属性，我们主要将任务构建方式总结为四类，包括*用户特定*任务、*项目特定*任务、*时间特定*任务和*序列特定*任务。值得一提的是，还有一些工作尝试了其他方式，但数量相对较少。我们将这些工作整理在名为$others$的类别中。表[4](#S4.T4
    "Table 4 ‣ 4\. Meta-learning Task Construction for Recommendation ‣ Deep Meta-learning
    in Recommendation Systems: A Survey")展示了适应每个任务构建类别的工作的总结。'
- en: Table 4\. Summary of task construction in meta-learning based recommendation
    methods.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表4\. 元学习基础的推荐方法中的任务构建总结。
- en: '| Task Construcution | Methods |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 任务构建 | 方法 |'
- en: '| User-specific | LWA (Vartak et al., [2017](#bib.bib100)),MeLU (Vartak et al.,
    [2017](#bib.bib100)),MetaCS (Bharadhwaj, [2019](#bib.bib3)), MetaHIN (Lu et al.,
    [2020](#bib.bib59)), MAMO (Dong et al., [2020](#bib.bib16)) TaNP (Lin et al.,
    [2021](#bib.bib54)) PALRML (Yu et al., [2021](#bib.bib120)), MPML (Chen et al.,
    [2021a](#bib.bib9)), PAML (Wang et al., [2021b](#bib.bib106)), CMML (Feng et al.,
    [2021](#bib.bib22)), Heater (Zhu et al., [2020b](#bib.bib135)), PNMTA (Pang et al.,
    [2022](#bib.bib71)), Meta-SSIN (Sun et al., [2021c](#bib.bib95)), MFNP (Sun et al.,
    [2021b](#bib.bib92)), FORM (Sun et al., [2021a](#bib.bib94)), PTUPCDR (Zhu et al.,
    [2021c](#bib.bib133)), MetaKG (Du et al., [2022](#bib.bib17)), MetaEDL (Neupane
    et al., [2021](#bib.bib68)) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 用户特定 | LWA (Vartak等，[2017](#bib.bib100)), MeLU (Vartak等，[2017](#bib.bib100)),
    MetaCS (Bharadhwaj，[2019](#bib.bib3)), MetaHIN (Lu等，[2020](#bib.bib59)), MAMO
    (Dong等，[2020](#bib.bib16)), TaNP (Lin等，[2021](#bib.bib54)) PALRML (Yu等，[2021](#bib.bib120)),
    MPML (Chen等，[2021a](#bib.bib9)), PAML (Wang等，[2021b](#bib.bib106)), CMML (Feng等，[2021](#bib.bib22)),
    Heater (Zhu等，[2020b](#bib.bib135)), PNMTA (Pang等，[2022](#bib.bib71)), Meta-SSIN
    (Sun等，[2021c](#bib.bib95)), MFNP (Sun等，[2021b](#bib.bib92)), FORM (Sun等，[2021a](#bib.bib94)),
    PTUPCDR (Zhu等，[2021c](#bib.bib133)), MetaKG (Du等，[2022](#bib.bib17)), MetaEDL
    (Neupane等，[2021](#bib.bib68)) |'
- en: '| Item-specific | MIRec (Zhang et al., [2021a](#bib.bib123)), ProtoCF (Sankar
    et al., [2021](#bib.bib83)), Meta-Embed. (Pan et al., [2019](#bib.bib70)), TDAML
    (Cao et al., [2020](#bib.bib6)), MWUF (Zhu et al., [2021d](#bib.bib134)), DisNet
    (Li et al., [2020](#bib.bib52)), GME (Ouyang et al., [2021](#bib.bib69)), Mecos
    (Zheng et al., [2021](#bib.bib126)) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 项目特定 | MIRec (Zhang et al., [2021a](#bib.bib123))，ProtoCF (Sankar et al.,
    [2021](#bib.bib83))，Meta-Embed. (Pan et al., [2019](#bib.bib70))，TDAML (Cao et al.,
    [2020](#bib.bib6))，MWUF (Zhu et al., [2021d](#bib.bib134))，DisNet (Li et al.,
    [2020](#bib.bib52))，GME (Ouyang et al., [2021](#bib.bib69))，Mecos (Zheng et al.,
    [2021](#bib.bib126)) |'
- en: '| Time-specific | DML (Neupane et al., [2022](#bib.bib67)), SML (Zhang et al.,
    [2020](#bib.bib124)), LSTTM (Xie et al., [2021](#bib.bib112)), ASMG (Peng et al.,
    [2021](#bib.bib72)), MeLON (Kim et al., [2022](#bib.bib45)) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 时间特定 | DML (Neupane et al., [2022](#bib.bib67))，SML (Zhang et al., [2020](#bib.bib124))，LSTTM
    (Xie et al., [2021](#bib.bib112))，ASMG (Peng et al., [2021](#bib.bib72))，MeLON
    (Kim et al., [2022](#bib.bib45)) |'
- en: '| Sequence-specific | FLIP (Liu et al., [2020b](#bib.bib58)), Meta-SKR (Cui
    et al., [2021](#bib.bib12)), MetaTL (Wang et al., [2021a](#bib.bib103)), CBML
    (Song et al., [2021](#bib.bib90)), MetaCSR (Huang et al., [2022](#bib.bib38))
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 序列特定 | FLIP (Liu et al., [2020b](#bib.bib58))，Meta-SKR (Cui et al., [2021](#bib.bib12))，MetaTL
    (Wang et al., [2021a](#bib.bib103))，CBML (Song et al., [2021](#bib.bib90))，MetaCSR
    (Huang et al., [2022](#bib.bib38)) |'
- en: '| Others | PreTraining (Hao et al., [2021](#bib.bib31)), PREMERE (Kim et al.,
    [2021](#bib.bib44)), MetaODE (Tan et al., [2021](#bib.bib98)), CHAML (Chen et al.,
    [2021b](#bib.bib8)), S2Meta (Du et al., [2019](#bib.bib18)), TMCDR (Zhu et al.,
    [2021a](#bib.bib130)) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | PreTraining (Hao et al., [2021](#bib.bib31))，PREMERE (Kim et al., [2021](#bib.bib44))，MetaODE
    (Tan et al., [2021](#bib.bib98))，CHAML (Chen et al., [2021b](#bib.bib8))，S2Meta
    (Du et al., [2019](#bib.bib18))，TMCDR (Zhu et al., [2021a](#bib.bib130)) |'
- en: 'User-specific Task. As observed in Table 4, the most typical way of task construction
    is based on users. Since the user cold-start issue is the most long-standing problem
    in recommendation systems, quickly learning preferences from users’ limited interactions
    is a critical task to be solved. In the setting of user-specific task $\mathcal{T}_{i}$,
    all instances of a task including both the support set $\mathcal{S}_{i}$ and the
    query set $\mathcal{Q}_{i}$ are belonging to the same user. Learning preferences
    of different users are naturally treated as different tasks. Give a illustrative
    example shown in Fig [2](#S4.F2 "Figure 2 ‣ 4\. Meta-learning Task Construction
    for Recommendation ‣ Deep Meta-learning in Recommendation Systems: A Survey")
    (a). For a user-specific task of a specific user $u_{1}$, all his interactions
    are split into a support set $\mathcal{S}_{1}=\{(v_{j},i^{u_{1}}_{v_{j}})\}_{j=1}^{3}$
    and a query set $\mathcal{Q}_{1}=\{(v_{j},r^{u_{1}}_{v_{j}})\}_{j=4}^{5}$, where
    $i^{u_{1}}_{v_{j}}$ could be a explicit rating score or a implicit feedback between
    user $u_{1}$ and item ${v_{j}}$. The goal of each user-specific task is to train
    a model on the support set and evaluation on the interactions in the query set
    of the same user. From the perspective of the meta-optimization objective, meta-learning
    methods are expected to extract meta-knowledge about user preference learning
    from a sufficient number of user-specific tasks $\mathcal{D}^{train}$. Then when
    faced with unseen user-specific tasks from new users, the meta-knowledge should
    work as prior experiences to facilitate preference learning.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 用户特定任务。正如表 4 所示，任务构建的最典型方式是基于用户的。由于用户冷启动问题是推荐系统中最长期存在的问题，快速从用户有限的交互中学习偏好是一个关键任务。在用户特定任务
    $\mathcal{T}_{i}$ 的设置中，任务的所有实例，包括支持集 $\mathcal{S}_{i}$ 和查询集 $\mathcal{Q}_{i}$，都属于同一个用户。不同用户的偏好自然被视为不同的任务。举一个图
    [2](#S4.F2 "图 2 ‣ 4\. 推荐的元学习任务构建 ‣ 推荐系统中的深度元学习：综述") (a) 中的示例。对于特定用户 $u_{1}$ 的用户特定任务，所有他的交互被划分为支持集
    $\mathcal{S}_{1}=\{(v_{j},i^{u_{1}}_{v_{j}})\}_{j=1}^{3}$ 和查询集 $\mathcal{Q}_{1}=\{(v_{j},r^{u_{1}}_{v_{j}})\}_{j=4}^{5}$，其中
    $i^{u_{1}}_{v_{j}}$ 可以是用户 $u_{1}$ 对项目 ${v_{j}}$ 的明确评分或隐含反馈。每个用户特定任务的目标是训练一个模型在支持集上，并在相同用户的查询集中的交互上进行评估。从元优化目标的角度来看，元学习方法被期望从足够数量的用户特定任务
    $\mathcal{D}^{train}$ 中提取关于用户偏好学习的元知识。然后，当面对来自新用户的未见过的用户特定任务时，元知识应该作为先验经验来促进偏好学习。
- en: 'Item-specific Task. Symmetric with the user-specific task, an item-specific
    task is constructed based on all instances involving the same item. From the view
    of an item, interaction instances are grouped based on different items. As illustrated
    in Fig [2](#S4.F2 "Figure 2 ‣ 4\. Meta-learning Task Construction for Recommendation
    ‣ Deep Meta-learning in Recommendation Systems: A Survey") (b), three item-specific
    tasks are constructed according to three different items including a shirt, a
    shoe, and a phone. Similar to user-specific tasks, meta-learning based item-specific
    tasks usually aim at tackling the item cold-start problem. In this setting, the
    support set and the query set of a task cover all interactions between multiple
    users and the same item. The goal of each item-specific task is to predict the
    ratings or interaction probabilities of evaluation instances in the query set
    after observing the support set. By extracting meta-knowledge across multiple
    item-specific tasks, meta-learning methods could quickly perceive the overall
    preference for cold-start items, making accurate predictions and recommendations.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '项目特定任务。与用户特定任务对称，项目特定任务是基于涉及相同项目的所有实例构建的。从项目的角度来看，交互实例根据不同的项目进行分组。如图 [2](#S4.F2
    "Figure 2 ‣ 4\. Meta-learning Task Construction for Recommendation ‣ Deep Meta-learning
    in Recommendation Systems: A Survey") (b) 所示，三个项目特定任务是根据三种不同的项目（包括一件衬衫、一双鞋和一部手机）构建的。类似于用户特定任务，基于元学习的项目特定任务通常旨在解决项目冷启动问题。在这种设置下，任务的支持集和查询集涵盖了多个用户与同一项目之间的所有交互。每个项目特定任务的目标是在观察支持集之后，预测查询集中的评价实例的评分或交互概率。通过提取多个项目特定任务中的元知识，元学习方法可以快速感知对冷启动项目的整体偏好，从而做出准确的预测和推荐。'
- en: '![Refer to caption](img/6b2325704a33ba4b66b92265ca12e032.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6b2325704a33ba4b66b92265ca12e032.png)'
- en: Figure 2\. Illustration of task construction for user-specific tasks and item-specific
    tasks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 用户特定任务和项目特定任务的任务构建示意图。
- en: 'Time-specific Task. In this setting, interaction data in recommendation systems
    are split into different tasks according to different time slots. Specifically,
    interaction data are considered as collected continually and arrived in the form
    of data streaming. Formally, at the time $t$, data currently collected is denoted
    as $I_{i}=\{(u_{i},v_{j},i^{u_{i}}_{v_{j}})\}^{M}$. Different from user-specific
    or item-specific settings, interactions in time-specific tasks are no longer distinguished
    by users or items. As shown in Fig [3](#S4.F3 "Figure 3 ‣ 4\. Meta-learning Task
    Construction for Recommendation ‣ Deep Meta-learning in Recommendation Systems:
    A Survey") (a), time-specific tasks are sequentially constructed with data in
    two successive time slots. For instance, for the task at time $2$, the support
    set consists of the data block $I_{2}$, i.e., data collected at the current time
    slot. For the query set, data block $I_{2}$ in the next time slot 3 is utilized
    as evaluation data. The reason for this setting is that the goal of a time-specific
    task is usually to efficiently update models in an online setting so that the
    updated model could still perform well in the next period. Meta-learning can also
    be used to facilitate the efficiency of model online updates by gradually extracting
    meta-knowledge from sequential time-specific tasks.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '时间特定任务。在这种设置下，推荐系统中的交互数据根据不同的时间段被拆分为不同的任务。具体来说，交互数据被视为连续收集，并以数据流的形式到达。形式上，在时间
    $t$，当前收集的数据记作 $I_{i}=\{(u_{i},v_{j},i^{u_{i}}_{v_{j}})\}^{M}$。与用户特定或项目特定设置不同，时间特定任务中的交互不再按用户或项目区分。如图
    [3](#S4.F3 "Figure 3 ‣ 4\. Meta-learning Task Construction for Recommendation
    ‣ Deep Meta-learning in Recommendation Systems: A Survey") (a) 所示，时间特定任务是按两个连续时间段的数据顺序构建的。例如，对于时间
    $2$ 的任务，支持集由数据块 $I_{2}$ 组成，即当前时间段收集的数据。对于查询集，数据块 $I_{2}$ 在下一个时间段 $3$ 被用作评价数据。这种设置的原因是，时间特定任务的目标通常是在在线环境中高效更新模型，以便更新后的模型在下一个周期仍能表现良好。元学习也可以通过逐步提取序列时间特定任务中的元知识来促进模型在线更新的效率。'
- en: 'Sequence-specific Task. As illustrated in Fig [3](#S4.F3 "Figure 3 ‣ 4\. Meta-learning
    Task Construction for Recommendation ‣ Deep Meta-learning in Recommendation Systems:
    A Survey") (b), sequence-specific tasks are also constructed with temporal information
    considered. Different from time-specific tasks which collect data at the system
    level, the sequence-specific setting treats interaction sequences of different
    users or different sessions as different tasks. For example, the whole interaction
    sequence of user $u_{1}$ is denoted as $\{(v_{1},i^{u_{1}}_{v_{1}}),(v_{2},i^{u_{1}}_{v_{3}}),...,(v_{t},i^{u_{1}}_{v_{t}})\}$
    which is ordered by interaction timestamps. For constructing a sequence-specific
    task, the interaction sequence with the length $t$ is usually split into two parts.
    The former $K$ interactions are allocated as the support set, while the latter
    $t-K$ interactions are allocated as the query set. There are two major differences
    between user-specific tasks and sequence-specific tasks. First, sequence-specific
    tasks are not restricted by integrating interaction users’ history. Anonymous
    sessions can also be independent interaction sequences. Second, the form of instances
    in sequence-specific tasks are usually subsequences of the whole interaction sequence,
    while user-specific tasks have interaction pairs.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '序列特定任务。如图 [3](#S4.F3 "Figure 3 ‣ 4\. Meta-learning Task Construction for Recommendation
    ‣ Deep Meta-learning in Recommendation Systems: A Survey") (b) 所示，序列特定任务也考虑了时间信息的构建。与在系统层面收集数据的时间特定任务不同，序列特定设置将不同用户或不同会话的交互序列视为不同的任务。例如，用户
    $u_{1}$ 的整个交互序列表示为 $\{(v_{1},i^{u_{1}}_{v_{1}}),(v_{2},i^{u_{1}}_{v_{3}}),...,(v_{t},i^{u_{1}}_{v_{t}})\}$，这些序列按交互时间戳排序。在构建序列特定任务时，长度为
    $t$ 的交互序列通常被分为两部分。前 $K$ 次交互被分配为支持集，而后 $t-K$ 次交互被分配为查询集。用户特定任务和序列特定任务之间有两个主要区别。首先，序列特定任务不受整合交互用户历史的限制。匿名会话也可以是独立的交互序列。其次，序列特定任务中的实例形式通常是整个交互序列的子序列，而用户特定任务则具有交互对。'
- en: '![Refer to caption](img/090ffe5b4542dddf4b58ecc55f2966ca.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/090ffe5b4542dddf4b58ecc55f2966ca.png)'
- en: Figure 3\. Illustration of task construction for time-specific tasks and sequence-specific
    tasks.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 时间特定任务和序列特定任务构建的说明。
- en: Others. Besides the four types of tasks mentioned above, several works also
    explore other ways of task construction. Scenario-specific tasks (Du et al., [2019](#bib.bib18))
    are divided according to different scenarios (e.g., tags, themes, or categories
    of items) in recommendation systems. Special for POI recommendation, city-specific
    tasks (Tan et al., [2021](#bib.bib98); Chen et al., [2021b](#bib.bib8)) organize
    interactions according to different cities, so that meta-knowledge could be extracted
    across multiple city-specific tasks and benefits data-sparse cities. Different
    from user-specific tasks which utilize interactions of a single user as a task,
    interactions of multiple users could also be combined and treated as one task
    (Zhu et al., [2021a](#bib.bib130); Kim et al., [2021](#bib.bib44)). Specifically,
    in cross-domain recommendation systems, Zhu et al. (Zhu et al., [2021a](#bib.bib130))
    randomly sample two groups of overlapping users (denoted as $U_{a}$ and $U_{b}$)
    and construct a cross-domain meta-learning task by gathering all interactions
    of multiple users as a support set (i.e., $S_{i}=D_{a}$) and a query set (i.e.,
    $Q_{i}=D_{b}$), respectively. The goal of each task is to learn an embedding mapping
    model from a source domain to a target domain for better performance over cold-start
    users in the target domain (simulated with $D_{b}$), while meta-learning contributes
    to the learning of the mapping model across multiple tasks. With a similar strategy
    of task construction, Kim et al. (Kim et al., [2021](#bib.bib44)) also separately
    samples two groups of multiple users as training data in two update phases of
    a meta-learning task. Besides recommendation tasks, Hao et al. (Hao et al., [2021](#bib.bib31))
    construct reconstruction tasks as pretraining tasks in their proposed meta-learning
    based cold-start recommendation method. Each reconstruction task consists of a
    target user and $K$ samples neighboring users and aims to reconstruct the target
    user’s embedding with his neighbors.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其他。除了上述四种任务类型，还有一些研究探索了其他任务构建方式。情境特定任务（Du et al., [2019](#bib.bib18)）根据推荐系统中的不同情境（例如，标签、主题或项目类别）进行划分。针对POI推荐，城市特定任务（Tan
    et al., [2021](#bib.bib98); Chen et al., [2021b](#bib.bib8)）根据不同城市组织交互，以便可以跨多个城市特定任务提取元知识，从而惠及数据稀疏的城市。不同于利用单一用户交互作为任务的用户特定任务，还可以将多个用户的交互组合起来作为一个任务（Zhu
    et al., [2021a](#bib.bib130); Kim et al., [2021](#bib.bib44)）。具体而言，在跨领域推荐系统中，Zhu
    et al.（Zhu et al., [2021a](#bib.bib130)）随机抽取两组重叠用户（记作$U_{a}$和$U_{b}$），通过将多个用户的所有交互作为支持集（即$S_{i}=D_{a}$）和查询集（即$Q_{i}=D_{b}$）来构建跨领域元学习任务。每个任务的目标是学习一个从源领域到目标领域的嵌入映射模型，以在目标领域的冷启动用户（用$D_{b}$模拟）上获得更好的性能，同时元学习有助于在多个任务之间学习映射模型。采用类似的任务构建策略，Kim
    et al.（Kim et al., [2021](#bib.bib44)）也在元学习任务的两个更新阶段分别抽取两组多个用户作为训练数据。除了推荐任务外，Hao
    et al.（Hao et al., [2021](#bib.bib31)）在其提出的基于元学习的冷启动推荐方法中构建了重建任务作为预训练任务。每个重建任务包括一个目标用户和$K$个相邻用户样本，旨在用其邻居重建目标用户的嵌入。
- en: 5\. Meta-leanring Methods for Recommendation Systems
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 元学习方法在推荐系统中的应用
- en: In this section, we look in more detail at meta-learning based recommendation
    methods in the literature. In general, we introduce how meta-learning methods
    facilitate the progress of recommendation systems in different recommendation
    scenarios. In each recommendation scenario, we summarize characteristics of related
    works and discuss methods about their ways of applying meta-learning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将更详细地探讨文献中的基于元学习的推荐方法。一般而言，我们介绍了元学习方法如何在不同推荐场景中促进推荐系统的进展。在每个推荐场景中，我们总结了相关工作的特点，并讨论了关于元学习应用方式的方法。
- en: 5.1\. Meta-learning in Cold-start Recommendation
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 冷启动推荐中的元学习
- en: In cold-start recommendation scenarios, users who conduct a small number of
    interactions or items which are involved in few interactions are emphasized when
    making recommendations, so as to boost the overall performance of recommendation
    systems. As commonly known, few-shot learning is the most common application of
    meta-learning. In recommendation systems, as an analogy to the few-shot learning
    problem, cold-start recommendation is also paid more attention and well studied
    by meta-learning based methods. Here, we summarize how existing works apply meta-learning
    to alleviate the cold-start issues for both cold-start users and items into different
    groups, including *optimization-based parameter initialization*, *optimization-based
    parameterized hyperparameters*, *model-based parameter modulation* and *metric-based
    embedding space learning*. Next, we will elaborate on different categories of
    methods and introduce details of concrete methods.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在冷启动推荐场景中，强调的是进行少量交互的用户或涉及少量交互的项目，以提升推荐系统的整体性能。众所周知，少样本学习是元学习最常见的应用。在推荐系统中，作为少样本学习问题的类比，冷启动推荐也受到更多关注，并由基于元学习的方法进行深入研究。在这里，我们总结了现有工作如何将元学习应用于缓解冷启动问题，包括*基于优化的参数初始化*、*基于优化的参数超参数*、*基于模型的参数调节*和*基于度量的嵌入空间学习*。接下来，我们将详细阐述不同类别的方法，并介绍具体方法的细节。
- en: '![Refer to caption](img/18e6b12fe3305ff2ad77c29ab7a841cf.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/18e6b12fe3305ff2ad77c29ab7a841cf.png)'
- en: Figure 4\. Illustration of the framework of Optimization-based Parameter Initialization
    and Adaptive Hyperparameters. Based on two levels of optomization including local
    adaptation and global optimization, the optimization-based meta-learner is updated
    across meta-training tasks. Both parameter initialization and adaptive hyperparameters
    could be learned according to different designs of meta-learners.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 基于优化的参数初始化和自适应超参数的框架示意图。基于包括局部适应和全局优化的两个层次的优化，优化型元学习者在元训练任务中进行更新。参数初始化和自适应超参数可以根据不同的元学习者设计进行学习。
- en: Table 5\. Details of recommendation models with optimization-based meta-learning
    methods in cold-start recommendation. The key techniques in both inner-level update
    and outer-level optimization are presented.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 基于优化的元学习方法在冷启动推荐中的推荐模型详细信息。展示了内层更新和外层优化中的关键技术。
- en: '| Method | Cold-start Object | Meta-knowledge Representation | Key Techniques
    in Bi-level Optimization |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 冷启动对象 | 元知识表示 | 双层优化中的关键技术 |'
- en: '| MeLU (Lee et al., [2019](#bib.bib48)) | User & Item | Parameter Initialization
    | *Inner*: FCN *Outer:* MAML |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| MeLU (Lee 等，[2019](#bib.bib48)) | 用户 & 项目 | 参数初始化 | *内层*: FCN *外层:* MAML
    |'
- en: '| MetaCS (Bharadhwaj, [2019](#bib.bib3)) | User | Parameters Initialization
    & Hyperparameter | *Inner*: FCN *Outer:* MAML |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| MetaCS (Bharadhwaj，[2019](#bib.bib3)) | 用户 | 参数初始化 & 超参数 | *内层*: FCN *外层:*
    MAML |'
- en: '| MetaHIN (Lu et al., [2020](#bib.bib59)) | User & Item | Parameter Initialization
    & Meta Model | *Inner*: FCN *Outer:* MAML + HIN |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| MetaHIN (Lu 等，[2020](#bib.bib59)) | 用户 & 项目 | 参数初始化 & 元模型 | *内层*: FCN *外层:*
    MAML + HIN |'
- en: '| MAMO (Dong et al., [2020](#bib.bib16)) | User & Item | Parameter Initialization
    & Meta Model | *Inner*: FCN *Outer:* MAML + Memories |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| MAMO (Dong 等，[2020](#bib.bib16)) | 用户 & 项目 | 参数初始化 & 元模型 | *内层*: FCN *外层:*
    MAML + 记忆 |'
- en: '| MetaCF (Wei et al., [2020](#bib.bib108)) | User | Parameter Initialization
    & Hyperparameter | *Inner*: FISM (Kabbur et al., [2013](#bib.bib41)) or NGCF (Wang
    et al., [2019](#bib.bib107)) *Outer:* MAML |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| MetaCF (Wei 等，[2020](#bib.bib108)) | 用户 | 参数初始化 & 超参数 | *内层*: FISM (Kabbur
    等，[2013](#bib.bib41)) 或 NGCF (Wang 等，[2019](#bib.bib107)) *外层:* MAML |'
- en: '| PALRML (Yu et al., [2021](#bib.bib120)) | User | Parameter Initialization
    & Hyperparameter | *Inner*: FCN *Outer:* MAML + Adaptive Learning Rate |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| PALRML (Yu 等，[2021](#bib.bib120)) | 用户 | 参数初始化 & 超参数 | *内层*: FCN *外层:* MAML
    + 自适应学习率 |'
- en: '| MPML (Chen et al., [2021a](#bib.bib9)) | User & Item | Parameter Initialization
    | *Inner*: FCN *Outer:* MAML + Clustering |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| MPML (Chen 等，[2021a](#bib.bib9)) | 用户 & 项目 | 参数初始化 | *内层*: FCN *外层:* MAML
    + 聚类 |'
- en: '| PAML(Wang et al., [2021b](#bib.bib106)) | User & Item | Parameter Initialization
    & Meta Model | *Inner*: HIN + Social *Outer:* MAML + Gating |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| PAML (Wang 等，[2021b](#bib.bib106)) | 用户 & 项目 | 参数初始化 & 元模型 | *内层*: HIN +
    社会 *外层:* MAML + 门控 |'
- en: '| MetaEDL (Neupane et al., [2021](#bib.bib68)) | User | Parameter Initialization
    | *Inner*: FCN *Outer:* MAML |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| MetaEDL (Neupane 等，[2021](#bib.bib68)) | 用户 | 参数初始化 | *内层*: FCN *外层:* MAML
    |'
- en: '| DML (Neupane et al., [2022](#bib.bib67)) | User | Parameter Initialization
    | *Inner*: FCN + RNN *Outer:* MAML |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| DML (Neupane et al., [2022](#bib.bib67)) | 用户 | 参数初始化 | *内层*: FCN + RNN *外层:*
    MAML |'
- en: '| PNMTA (Pang et al., [2022](#bib.bib71)) | User | Parameter Initialization
    & Meta-Model | *Inner*: FCN *Outer:* MAML + Parameter Modulation |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| PNMTA (Pang et al., [2022](#bib.bib71)) | 用户 | 参数初始化 & 元模型 | *内层*: FCN *外层:*
    MAML + 参数调节 |'
- en: 'Optimization-based Parameter Initialization. Table [5](#S5.T5 "Table 5 ‣ 5.1\.
    Meta-learning in Cold-start Recommendation ‣ 5\. Meta-leanring Methods for Recommendation
    Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey") shows the summary
    of optimization-based meta-learning methods in cold-start recommendation from
    three perspectives, i.e., cold-start object, meta-knowledge representation, and
    key techniques used in the bi-level optimization framework. Existing methods generally
    fall into two categories according to two forms of meta-knowledge representations,
    including *parameter initialization* and *adaptive hyperparameters*. We present
    a general framework for both optimization-based parameter initialization and adaptive
    hyperparameters in Fig [4](#S5.F4 "Figure 4 ‣ 5.1\. Meta-learning in Cold-start
    Recommendation ‣ 5\. Meta-leanring Methods for Recommendation Systems ‣ Deep Meta-learning
    in Recommendation Systems: A Survey"). In the following, We discuss concrete methods
    for parameter initialization in this part and adaptive hyperparameters in the
    next part.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '基于优化的参数初始化。表 [5](#S5.T5 "Table 5 ‣ 5.1\. Meta-learning in Cold-start Recommendation
    ‣ 5\. Meta-leanring Methods for Recommendation Systems ‣ Deep Meta-learning in
    Recommendation Systems: A Survey") 显示了从三个角度，即冷启动对象、元知识表示以及双层优化框架中使用的关键技术，来总结基于优化的元学习方法。现有方法通常根据两种元知识表示形式分为两类，包括
    *参数初始化* 和 *自适应超参数*。我们在图 [4](#S5.F4 "Figure 4 ‣ 5.1\. Meta-learning in Cold-start
    Recommendation ‣ 5\. Meta-leanring Methods for Recommendation Systems ‣ Deep Meta-learning
    in Recommendation Systems: A Survey") 中展示了基于优化的参数初始化和自适应超参数的通用框架。接下来，我们在这一部分讨论参数初始化的具体方法，而在下一部分讨论自适应超参数。'
- en: The basic idea of *optimization-based parameter initialization* is defining
    the meta-knowledge $\omega$ as the initial parameters of base recommendation models
    and then updating the parameter initialization in the form of bi-level optimization.
    Inspired by the idea of model-agnostic meta-learning(Finn et al., [2017](#bib.bib23)),
    Lee et.al (Lee et al., [2019](#bib.bib48)) firstly introduce the MAML framework
    to cold-start recommendation and propose MeLU, which aims to learn global parameter
    initialization of a neural network based recommendation model as prior knowledge.
    The base model $f_{\theta}$ is implemented using fully connected neural networks
    (FCNs), which act as a personalized user preference estimation model. Here, $\theta$
    include transformation parameters $\bm{W}$ and bias parameters $\bm{b}$ of both
    hidden layers and the final output layer in the base recommendation model, which
    are to be initialized with globally learned parameter initialization $\omega$
    via $\theta\leftarrow\omega$. Following the bi-level optimization procedure, MeLU
    constructs user cold-start tasks and locally updates the parameters of the personalized
    recommendation model for each user $u_{i}$ as the equation (6). After the local
    update process, a user-specific recommendation model $f_{\theta_{\mathcal{T}_{i}}}$
    is especially learned for the task $\mathcal{T}_{i}$, and employed to make preference
    predictions on its unseen query set $\mathcal{Q}_{i}$. In the global optimization
    procedure, global parameter initialization $\theta$, which is applied to the local
    update processes of multiple meta-training tasks simultaneously, is optimized
    by minimizing the summed loss on query sets as equation (7). After iterative global
    update steps during the meta-training phase, the global parameter initialization
    $\omega$ is supposed to have abilities to quickly adapt to new cold-start recommendation
    tasks in the meta-testing set $\mathcal{D}^{test}$. In MeLU, the parameters of
    the user preference estimation model are optimized under the MAML framework while
    user/item embeddings are only globally updated. In addition, MeLU is evaluated
    as effective in handling both user and item cold-start issues by dividing both
    user and item into existing groups and new groups.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于优化的参数初始化*的基本思想是将元知识$\omega$定义为基础推荐模型的初始参数，然后以双层优化的形式更新参数初始化。受到模型无关元学习（Finn等，
    [2017](#bib.bib23)）的启发，Lee等（Lee等，[2019](#bib.bib48)）首次将MAML框架引入冷启动推荐，并提出MeLU，旨在将神经网络基础推荐模型的全局参数初始化作为先验知识进行学习。基础模型$f_{\theta}$使用全连接神经网络（FCNs）实现，作为个性化用户偏好估计模型。在这里，$\theta$包括基础推荐模型中隐藏层和最终输出层的转换参数$\bm{W}$和偏置参数$\bm{b}$，这些参数通过$\theta\leftarrow\omega$以全局学习的参数初始化$\omega$进行初始化。按照双层优化程序，MeLU构建用户冷启动任务，并根据方程（6）局部更新每个用户$u_{i}$的个性化推荐模型的参数。在局部更新过程后，为任务$\mathcal{T}_{i}$特别学习一个用户特定的推荐模型$f_{\theta_{\mathcal{T}_{i}}}$，并用于在其未见的查询集$\mathcal{Q}_{i}$上进行偏好预测。在全局优化过程中，全局参数初始化$\theta$，应用于多个元训练任务的局部更新过程，同时通过最小化查询集上的总损失来优化，如方程（7）。在元训练阶段的迭代全局更新步骤后，全局参数初始化$\omega$应具有快速适应元测试集$\mathcal{D}^{test}$中新的冷启动推荐任务的能力。在MeLU中，用户偏好估计模型的参数在MAML框架下进行优化，而用户/项目嵌入则仅进行全局更新。此外，通过将用户和项目划分为现有组和新组，MeLU被评估为在处理用户和项目冷启动问题上有效。'
- en: Drawing on the idea of globally learning model initialization parameters across
    multiple cold-start tasks, some other works are also proposed with the help of
    the original MAML framework. On the basis of MeLU, Chen et al.(Chen et al., [2021a](#bib.bib9))
    propose a multi-prior meta-learning approach MPML which equips multiple sets of
    initialization parameters. For a cold-start task, which set of initialization
    to be assigned depends on which performs better after local update over its support
    set. Besides simple FCN-based collaborative filtering models, optimization-based
    meta-learning also have been utilized to learn initialization for different forms
    of recommendation models. For instance, MetaEDL (Neupane et al., [2021](#bib.bib68))
    adopts the MAML framework to learn initialization parameters of an evidential
    learning enhanced recommendation model which additionally assigns evidence to
    predicted interactions. Considering the temporal evolution of user preferences,
    DML (Neupane et al., [2022](#bib.bib67)) is designed to continuously capture time-evolving
    factors from all historical interactions of a user and fastly learn time-specific
    factors based on a small number of current interactions. Specifically, the module
    for capturing time-specific factors is learned under the MAML framework in order
    to fastly adapt to each time period where the number of the user’s interactions
    is usually small.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴在多个冷启动任务中全局学习模型初始化参数的思想，其他一些工作也在原始 MAML 框架的帮助下提出了。基于 MeLU，Chen 等人（Chen 等，[2021a](#bib.bib9)）提出了一种多先验元学习方法
    MPML，该方法配备了多组初始化参数。对于冷启动任务，分配哪个初始化集取决于在其支持集上局部更新后表现更好的那个。除了简单的基于 FCN 的协同过滤模型外，基于优化的元学习还被用于学习不同形式推荐模型的初始化。例如，MetaEDL（Neupane
    等，[2021](#bib.bib68)）采用 MAML 框架学习一个增强证据学习的推荐模型的初始化参数，该模型额外为预测交互分配证据。考虑到用户偏好的时间演变，DML（Neupane
    等，[2022](#bib.bib67)）旨在不断捕捉用户所有历史交互中的时间演变因素，并基于少量当前交互快速学习时间特定因素。具体而言，捕捉时间特定因素的模块在
    MAML 框架下进行学习，以便快速适应用户交互通常较少的每个时间段。
- en: 'One promising line of extending the MAML framework is to take the *task heterogeneity*
    issue into consideration by tailoring task-specific initialization for different
    tasks (Dong et al., [2020](#bib.bib16); Wang et al., [2021b](#bib.bib106); Pang
    et al., [2022](#bib.bib71)). We present the core idea of initialization strategies
    in two representative works in Fig [5](#S5.F5 "Figure 5 ‣ 5.1\. Meta-learning
    in Cold-start Recommendation ‣ 5\. Meta-leanring Methods for Recommendation Systems
    ‣ Deep Meta-learning in Recommendation Systems: A Survey"). One representative
    work MAMO (Dong et al., [2020](#bib.bib16)) is proposed to provide a personalized
    bias term when initializing the recommendation model parameters. Specifically,
    memory networks are introduced into optimization-based meta-learning as external
    memory units to store task-specific fast weight memories. Before assigning the
    global initialization learned under the MAML framework to base model, MAMO applies
    memory units to generate a personalized bias term $b_{u_{i}}$ and obtain a task-specific
    initialization $\theta_{u_{i}}\leftarrow\omega-\tau b_{u_{i}}$. $b_{u_{i}}$ is
    generated by querying fast weights memories $M_{W}$ with profile representation
    $p_{u_{i}}$ of a given user ${u_{i}}$ as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展 MAML 框架的一个有前景的方向是通过为不同任务量身定制任务特定的初始化来考虑*任务异质性*问题（Dong 等， [2020](#bib.bib16)；Wang
    等，[2021b](#bib.bib106)；Pang 等，[2022](#bib.bib71)）。我们在图 [5](#S5.F5 "图 5 ‣ 5.1\.
    冷启动推荐中的元学习 ‣ 5\. 推荐系统的元学习方法 ‣ 推荐系统中的深度元学习：综述") 中展示了两项代表性工作的初始化策略的核心思想。其中一个代表性工作
    MAMO（Dong 等，[2020](#bib.bib16)）被提出用于在初始化推荐模型参数时提供个性化的偏差项。具体而言，记忆网络被引入到基于优化的元学习中，作为外部记忆单元来存储任务特定的快速权重记忆。在将
    MAML 框架下学习到的全局初始化分配给基础模型之前，MAMO 应用记忆单元生成个性化的偏差项 $b_{u_{i}}$ 并获得任务特定的初始化 $\theta_{u_{i}}\leftarrow\omega-\tau
    b_{u_{i}}$。$b_{u_{i}}$ 是通过用给定用户 ${u_{i}}$ 的个人资料表示 $p_{u_{i}}$ 查询快速权重记忆 $M_{W}$
    生成的，具体如下：
- en: '| (8) |  | $\displaystyle b_{u_{i}}$ | $\displaystyle=$ | $\displaystyle a_{u_{i}}^{T}M_{W}$
    |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\displaystyle b_{u_{i}}$ | $\displaystyle=$ | $\displaystyle a_{u_{i}}^{T}M_{W}$
    |  |'
- en: '| (9) |  | $\displaystyle a_{u_{i}}$ | $\displaystyle=$ | $\displaystyle attention(p_{u_{i}},M_{P})$
    |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\displaystyle a_{u_{i}}$ | $\displaystyle=$ | $\displaystyle attention(p_{u_{i}},M_{P})$
    |  |'
- en: 'where $M_{P}$ is the profile memories stored in the training process and $M_{W}$
    is the fast weights memories storing training gradients as fast weights. As for
    the model and memories optimization, two memory matrices are updated over the
    training task of ${u_{i}}$ as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M_{P}$ 是在训练过程中存储的配置文件记忆，$M_{W}$ 是以快速权重形式存储训练梯度的快速权重记忆。至于模型和记忆的优化，两个记忆矩阵在
    ${u_{i}}$ 的训练任务中更新如下：
- en: '| (10) |  | $\displaystyle M_{P}$ | $\displaystyle=$ | $\displaystyle\lambda(a_{u_{i}}p_{u_{i}}^{T})+(1-\lambda)M_{P}$
    |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle M_{P}$ | $\displaystyle=$ | $\displaystyle\lambda(a_{u_{i}}p_{u_{i}}^{T})+(1-\lambda)M_{P}$
    |  |'
- en: '| (11) |  | $\displaystyle M_{W}$ | $\displaystyle=$ | $\displaystyle\delta(a_{u_{i}}\nabla_{\theta}\mathcal{L}(f_{\theta},\mathcal{S}_{i}))+(1-\delta)M_{W}$
    |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle M_{W}$ | $\displaystyle=$ | $\displaystyle\delta(a_{u_{i}}\nabla_{\theta}\mathcal{L}(f_{\theta},\mathcal{S}_{i}))+(1-\delta)M_{W}$
    |  |'
- en: where $\lambda$ and $\delta$ are hyperparameters as memory update ratios. Note
    that we only present one part of the utilization of memories in MAMO, while more
    details and extensions could be seen in the original paper (Dong et al., [2020](#bib.bib16)).
    Consequently, by injecting the profile-aware initialization bias $b_{u_{i}}$,
    MAMO tailors task-specific initialization $\theta_{u_{i}}$ to copy with task heterogeneity
    issue w.r.t. user profiles.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 和 $\delta$ 是作为记忆更新比率的超参数。注意，我们仅展示了 MAMO 中记忆利用的一部分，更多细节和扩展可见原文（Dong
    et al., [2020](#bib.bib16)）。因此，通过注入配置文件感知初始化偏差 $b_{u_{i}}$，MAMO 将任务特定的初始化 $\theta_{u_{i}}$
    调整为应对用户配置文件相关的任务异质性问题。
- en: '![Refer to caption](img/3a7d027fcd04fff9ae6513391018e318.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3a7d027fcd04fff9ae6513391018e318.png)'
- en: Figure 5\. Illustration of different parameter initializationn strategies in
    three representative methods including MeLU, MAMO and PAML. In short, MeLU shares
    global initialization among all tasks while MAMO and PAML tailor task-specific
    initialization considering user profile and user preferences respectively.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 展示了三种代表性方法（包括 MeLU、MAMO 和 PAML）中的不同参数初始化策略。简而言之，MeLU 在所有任务之间共享全局初始化，而
    MAMO 和 PAML 则根据用户配置文件和用户偏好分别定制任务特定的初始化。
- en: 'Following the same idea of customizing task-specific initialization, Wang et
    al. (Wang et al., [2021b](#bib.bib106)) also argue that similar prior knowledge
    should be shared by users with similar preferences. Therefore, a preference-adaptive
    meta-learning approach PAML is proposed to adjust the globally shared prior initialization
    $\theta$ to the preference-specific initialization $\theta_{u_{i}}$ by applying
    an external meta model. Specifically, the meta model acts as a preference-specific
    adapter by incorporating social relations from social networks and semantic relations
    from heterogeneous information networks (HINs). When customizing the preference-specific
    initialization $\theta_{u_{i}}$, a series of preference-specific gates $\bm{g_{u_{i}}}$
    are designed to control how much prior knowledge is shared, implemented as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 按照定制任务特定初始化的相同思路，Wang 等人（Wang et al., [2021b](#bib.bib106)）也认为类似的先验知识应由具有相似偏好的用户共享。因此，提出了一种偏好自适应的元学习方法
    PAML，通过应用外部元模型将全局共享的先验初始化 $\theta$ 调整为偏好特定的初始化 $\theta_{u_{i}}$。具体而言，元模型作为偏好特定的适配器，通过结合社交网络的社交关系和异质信息网络（HINs）的语义关系来实现。在定制偏好特定初始化
    $\theta_{u_{i}}$ 时，设计了一系列偏好特定的门控 $\bm{g_{u_{i}}}$ 来控制共享多少先验知识，具体实现如下：
- en: '| (12) |  | $\displaystyle\bm{g}_{u_{i}}$ | $\displaystyle=$ | $\displaystyle\sigma(\bm{W}_{g}\bm{u}_{i}+\bm{b}_{g})$
    |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\displaystyle\bm{g}_{u_{i}}$ | $\displaystyle=$ | $\displaystyle\sigma(\bm{W}_{g}\bm{u}_{i}+\bm{b}_{g})$
    |  |'
- en: '| (13) |  | $\displaystyle\theta_{u_{i}}$ | $\displaystyle=$ | $\displaystyle\theta\circ\bm{g}_{u_{i}}$
    |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\displaystyle\theta_{u_{i}}$ | $\displaystyle=$ | $\displaystyle\theta\circ\bm{g}_{u_{i}}$
    |  |'
- en: where $\bm{u}_{i}$ is the user preference representation learned from not only
    interactions of the user as well as representations of his/her explicit friends
    extracted based on social relations and implicit friends extracted from semantic
    relations, respectively. Since user relations are comprehensively modeled by incorporating
    both social networks and HINs, final user preference representation $\bm{u}_{i}$
    is supposed to trigger similar gates for users who share similar preferences.
    Finally, after obtaining preference-specific initialization $\theta_{u_{i}}$,
    optimization-based meta-learning (i.e., MAML framework) is utilized to optimize
    parameters of both the base recommendation model and the meta model. Here, the
    base recommendation model includes the preference modeling module previously discussed
    and an FCN-based rating prediction module. Different from MAMO which focuses on
    user profile information, PAML distinguishes different tasks mainly based on multiple
    types of user relations.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{u}_{i}$ 是从用户的交互以及基于社交关系提取的显式朋友表示和从语义关系提取的隐式朋友表示中学习到的用户偏好表示。由于用户关系通过整合社交网络和异构信息网络（HINs）得到全面建模，最终的用户偏好表示
    $\bm{u}_{i}$ 应触发对具有相似偏好的用户类似的门控。最终，在获得特定偏好初始化 $\theta_{u_{i}}$ 后，基于优化的元学习（即 MAML
    框架）用于优化基础推荐模型和元模型的参数。这里，基础推荐模型包括之前讨论的偏好建模模块和基于 FCN 的评分预测模块。不同于侧重于用户资料信息的 MAMO，PAML
    主要基于多种用户关系区分不同任务。
- en: 'Without incorporating external task relations for revealing differences among
    tasks, Pang et al. (Pang et al., [2022](#bib.bib71)) propose PNMTA to discover
    implicit task distribution from users’ interaction contexts and perform task-adaptive
    initialization adjustment. Specifically, a meta model $\mathcal{F}_{\omega}$ is
    designed to generate task-specific initialization $\theta_{u_{i}}$ for the base
    prediction model by conducting parameter modulation as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 不通过引入外部任务关系来揭示任务间差异，Pang 等（Pang et al., [2022](#bib.bib71)）提出了 PNMTA，以从用户的交互上下文中发现隐式任务分布并执行任务自适应初始化调整。具体而言，设计了一个元模型
    $\mathcal{F}_{\omega}$ 来生成基础预测模型的任务特定初始化 $\theta_{u_{i}}$，其参数调制如下：
- en: '| (14) |  | $\displaystyle\bm{w}_{i},\bm{b}_{i}$ | $\displaystyle=$ | $\displaystyle\mathcal{F}_{\omega}(\bm{t_{i}})$
    |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\displaystyle\bm{w}_{i},\bm{b}_{i}$ | $\displaystyle=$ | $\displaystyle\mathcal{F}_{\omega}(\bm{t_{i}})$
    |  |'
- en: '| (15) |  | $\displaystyle\theta_{u_{i}}$ | $\displaystyle=$ | $\displaystyle\bm{w}_{i}\odot\theta+\bm{b}_{i}$
    |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\displaystyle\theta_{u_{i}}$ | $\displaystyle=$ | $\displaystyle\bm{w}_{i}\odot\theta+\bm{b}_{i}$
    |  |'
- en: where $\bm{t_{i}}$ is the task vector learned by aggregating all interaction
    representations. Conditioned on the task representation, the meta model generates
    task-adaptive modulation signals, i.e., parameters of the modulation function.
    Here, we present feature-wise linear modulation (FiLM) while other types of modulation
    functions such as channel-wise modulation and soft attention modulation are also
    discussed in the original paper. In the meta-training phase, both parameters of
    the meta-model $\omega$ and global initialization $\theta$ of the base model are
    optimized under the MAML framework.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{t_{i}}$ 是通过聚合所有交互表示学习到的任务向量。在任务表示的条件下，元模型生成任务自适应调制信号，即调制函数的参数。在此，我们展示了特征-wise
    线性调制（FiLM），而原文中还讨论了其他类型的调制函数，如通道-wise 调制和软注意力调制。在元训练阶段，元模型 $\omega$ 和基础模型的全局初始化
    $\theta$ 的参数在 MAML 框架下进行优化。
- en: Besides the extension over the meta-learning framework, MetaHIN (Lu et al.,
    [2020](#bib.bib59)) is proposed to augment cold-start tasks from the perspective
    of task construction. Specifically, different from merely regarding interacted
    items of a user as the support set $\mathcal{S}_{i}$, MetaHIN incorporates multifaceted
    semantic contexts $\mathcal{S}_{i}^{\mathcal{P}}$ into tasks based on multiple
    meta-paths $\mathcal{P}=\{p_{1},p_{2},...,p_{n}\}$ of heterogeneous information
    network (HIN). For each meta-path $p_{k}$, a set of items that are reachable from
    user $u_{i}$ are obtained via $p_{k}$, denoted as $\mathcal{S}_{i}^{p_{k}}$. By
    doing this, the semantic-enhanced support set is obtained as $(\mathcal{S}_{i},\mathcal{S}_{i}^{\mathcal{P}})$,
    and semantic-enhanced query set is obtained similarly as $(\mathcal{Q}_{i},\mathcal{Q}_{i}^{\mathcal{P}})$.
    After constructing the semantic-enhanced tasks above, a co-adaptation meta-learner
    is designed to perform both semantic- and task-wise adaptation to enhance the
    ability of local adaptation for each user. task-wise adaptation to enhance the
    ability of local adaptation for each user. The co-adaptation adaptation focuses
    on adapting to different semantic spaces induced by different meta-paths, respectively.
    Overall, the conventional local adaptation phase in MAML is first augmented from
    the data level by constructing semantic-enriched tasks and then enhanced with
    a co-adaptation meta-learner by designing two levels of local adaptation.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对元学习框架的扩展，MetaHIN（Lu et al.，[2020](#bib.bib59)）从任务构建的角度提出了增强冷启动任务的方法。具体而言，与仅将用户的互动项视为支持集$\mathcal{S}_{i}$不同，MetaHIN将基于异构信息网络（HIN）的多个元路径$\mathcal{P}=\{p_{1},p_{2},...,p_{n}\}$将多方面的语义上下文$\mathcal{S}_{i}^{\mathcal{P}}$纳入任务中。对于每个元路径$p_{k}$，通过$p_{k}$获得的用户$u_{i}$可达的一组项，记为$\mathcal{S}_{i}^{p_{k}}$。通过这种方式，得到语义增强的支持集$(\mathcal{S}_{i},\mathcal{S}_{i}^{\mathcal{P}})$，语义增强的查询集类似地得到$(\mathcal{Q}_{i},\mathcal{Q}_{i}^{\mathcal{P}})$。在构建上述语义增强任务后，设计了一个共适应的元学习器，以同时进行语义和任务的适应，以增强每个用户的局部适应能力。共适应的适应聚焦于分别适应由不同元路径引起的不同语义空间。总体而言，MAML中的传统局部适应阶段首先通过构建语义丰富的任务从数据层面上进行了增强，然后通过设计两个层次的局部适应进一步使用共适应元学习器进行了增强。
- en: 'Optimization-based Adaptive Hyperparameters. Besides parameter initialization
    of based recommendation models, several works also leverage meta-learning to learn
    adaptive hyperparameters for different cold-start tasks. For instance, MetaCS
    (Bharadhwaj, [2019](#bib.bib3)) adopts the similar bi-level optimization procedure
    as the MeLU, and additionally meta-update the value of local learning rate $\alpha$
    when performing global optimization. The updating equation of the local learning
    rate is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优化的自适应超参数。除了基于推荐模型的参数初始化外，一些研究还利用元学习来学习不同冷启动任务的自适应超参数。例如，MetaCS（Bharadhwaj，[2019](#bib.bib3)）采用了与MeLU类似的双层优化过程，并在进行全局优化时，额外地对局部学习率$\alpha$进行元更新。局部学习率的更新方程如下：
- en: '| (16) |  | $\alpha\leftarrow\alpha-\beta\nabla_{\alpha}\sum\nolimits_{\mathcal{T}_{i}\in\mathcal{D}^{train}}\mathcal{L}(f_{\theta_{\mathcal{T}_{i}}},\mathcal{Q}_{i}),$
    |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\alpha\leftarrow\alpha-\beta\nabla_{\alpha}\sum\nolimits_{\mathcal{T}_{i}\in\mathcal{D}^{train}}\mathcal{L}(f_{\theta_{\mathcal{T}_{i}}},\mathcal{Q}_{i}),$
    |  |'
- en: where $\alpha$ is the parameterized learning rate for the local update and $\beta$
    is a fixed learning rate for the global update. They argue that the manually fixed
    learning rate may make the model unable to converge. In this way, not only model
    parameters of the base model but also hyperparameters, e.g., learning rates, are
    meta-learned to provide prior knowledge. To be mentioned, the learnable update
    ratio here is merely globally optimized but not updated during the local adaptation
    of each task.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$是用于局部更新的参数化学习率，$\beta$是用于全局更新的固定学习率。他们认为，手动固定的学习率可能会导致模型无法收敛。通过这种方式，不仅基础模型的参数，还包括超参数，如学习率，都经过元学习以提供先验知识。需要提到的是，这里的可学习更新比率仅在全局优化时进行优化，而在每个任务的局部适应过程中不会更新。
- en: With collaborative filtering methods as the base model, MetaCF (Wei et al.,
    [2020](#bib.bib108)) also leverages MAML framework to meta-learn initialization
    for learnable parameters such as item embeddings in FISM (Kabbur et al., [2013](#bib.bib41))
    and embedding transformation parameters in NGCF (Wang et al., [2019](#bib.bib107)).
    Similar to MetaCS (Bharadhwaj, [2019](#bib.bib3)), MetaCF also adopts a flexible
    update strategy by learning appropriate learning rates automatically. While performing
    task construction, MetaCF adopts another two strategies including dynamic subgraph
    sampling and potential interactions extraction, which inject dynamicity and semantics
    into the recommendation tasks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基本模型的协同过滤方法，MetaCF（Wei et al., [2020](#bib.bib108)）还利用MAML框架来元学习可学习参数的初始化，例如FISM中的项目嵌入（Kabbur
    et al., [2013](#bib.bib41))和NGCF中的嵌入变换参数（Wang et al., [2019](#bib.bib107)）。与MetaCS（Bharadhwaj，[2019](#bib.bib3)）类似，MetaCF还通过学习适当的学习率自动采用灵活的更新策略。在执行任务构建时，MetaCF采用了另外两种策略，包括动态子图采样和潜在交互提取，从而向推荐任务注入动态性和语义。
- en: 'Similarly, Yu et al. (Yu et al., [2021](#bib.bib120)) proposes a personalized
    adaptive learning rate meta-learning approach PALRML which sets different learning
    rates for different users to find task-adaptive parameters for each task. They
    argue that assuming uniform user distribution in recommendation systems may lead
    to the over-fitting problem of major users with similar features. In other words,
    minor users whose features are different from the major ones may not be focused
    on. Therefore, PALRML performs user-adaptive learning rate based meta-learning
    to improve the performance of the basic MAML framework. Specifically, the local
    adaptation on each task $\mathcal{T}_{i}$ is adjusted as:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，余等人（Yu et al., [2021](#bib.bib120)）提出了一种个性化自适应学习率元学习方法PALRML，该方法为不同用户设置不同的学习率，以找到每个任务的任务自适应参数。他们认为，在推荐系统中假设用户分布是均匀的可能会导致具有相似特征的主要用户过拟合的问题。换句话说，具有不同特征的次要用户可能不会受到关注。因此，PALRML执行了基于用户自适应学习率的元学习，以改善基本MAML框架的性能。具体来说，每个任务$\mathcal{T}_{i}$上的局部调整适应如下：
- en: '| (17) |  | $\theta_{\mathcal{T}_{i}}=\theta-\alpha(h_{i})\nabla_{\theta}\mathcal{L}(f_{\theta},\mathcal{S}_{i}).$
    |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| （17）| |$\theta_{\mathcal{T}_{i}}=\theta-\alpha(h_{i})\nabla_{\theta}\mathcal{L}(f_{\theta},\mathcal{S}_{i}).$
    |  |'
- en: where $\alpha(h_{i})$ is a mapping function for assigning an appropriate learning
    rate for each user $u_{i}$ according to the user’s feature embedding $h_{i}$.
    Three different strategies including adaptive learning rate based, approximated
    tree-based, and regularizer-based are designed to provide personalized learning
    rates. Low space complexity and good prediction performance are supposed to be
    achieved simultaneously.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha(h_{i})$是一个映射函数，根据用户的特征嵌入$h_{i}$为每个用户$u_{i}$分配适当的学习率。设计了三种不同的策略，包括基于自适应学习率、近似基于树的和基于正则化器的，以提供个性化学习率。同时实现了低空间复杂度和良好的预测性能。
- en: Table 6\. Details of recommendation models with model-based meta-learning methods
    in cold-start recommendation. The key role of designed meta models in different
    methods is summarized.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表6. 冷启动推荐中具有基于模型的元学习方法的推荐模型的详细信息。总结了不同方法中设计的元模型的关键角色。
- en: '| Method | Cold-start object | Base Model | Key Role of Meta Model |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 冷启动对象 | 基础模型 | 元模型的关键角色 |'
- en: '| LWA (Vartak et al., [2017](#bib.bib100)) | Item | LR / FCN | Task-dependent
    Parameter Generation |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| LWA（Vartak et al., [2017](#bib.bib100)） | 项目 | LR / FCN | 任务相关的参数生成 |'
- en: '| TaNP (Lin et al., [2021](#bib.bib54)) | User | Encoder & Decoder | Task Relevance
    aware Parameter Modification |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| TaNP（Lin et al., [2021](#bib.bib54)） | 用户 | 编码器和解码器 | 任务相关的参数修改 |'
- en: '| MIRec (Zhang et al., [2021a](#bib.bib123)) | Item | FCN | Parameter Generation
    from few-shot models to many-shot models |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| MIRec（Zhang et al., [2021a](#bib.bib123)） | 项目 | FCN | 从少样本模型到多样本模型的参数生成
    |'
- en: '| CMML (Feng et al., [2021](#bib.bib22)) | User | FCN | Task-dependent Parameter
    Modification |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| CMML（Feng et al., [2021](#bib.bib22)） | 用户 | FCN | 任务相关的参数修改 |'
- en: '| Heater (Zhu et al., [2020b](#bib.bib135)) | User & Item | FCN | Mixture-of-Experts
    based Parameter Integration |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Heater（Zhu et al., [2020b](#bib.bib135)） | 用户和项目 | FCN | 基于专家混合的参数集成 |'
- en: 'Model-based Parameter Modulation. Another category of meta-learning based approaches
    for cold-start recommendation adopts model-based meta-learning for parameter modulation.
    The core idea to is train a meta model $\mathcal{F}_{\omega}$ which directly controls
    or alters the state of base recommendation models without relying on inner-level
    optimization. More specifically, the form of meta model is usually a learnable
    neural network that takes interactions in the support set of a task and other
    useful information (such as losses or gradients) as input to learn task-specific
    information. The ways of altering states of the base model for a task depend on
    the design of different methods, i.e., the output form of the meta model. For
    instance, some works adopt parameter-generation strategies, which directly treat
    the outputs of the meta model as the task-specific parameters of the base model.
    Meanwhile, some works take more indirect ways such as gating-based modification
    of globally shared parameters. We summarize three categories of parameter modulation
    strategies including parameter generation, parameter modification, and parameter
    integration, which are illustrated in Fig [6](#S5.F6 "Figure 6 ‣ 5.1\. Meta-learning
    in Cold-start Recommendation ‣ 5\. Meta-leanring Methods for Recommendation Systems
    ‣ Deep Meta-learning in Recommendation Systems: A Survey"). Table [6](#S5.T6 "Table
    6 ‣ 5.1\. Meta-learning in Cold-start Recommendation ‣ 5\. Meta-leanring Methods
    for Recommendation Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey")
    shows the summary of model-based parameter modulation methods.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的参数调节。另一类冷启动推荐的元学习方法采用基于模型的元学习来进行参数调节。核心思想是训练一个元模型$\mathcal{F}_{\omega}$，该模型直接控制或改变基础推荐模型的状态，而无需依赖内部优化。更具体地说，元模型的形式通常是一个可学习的神经网络，它将任务支持集中的交互和其他有用信息（如损失或梯度）作为输入，以学习任务特定的信息。基础模型状态的改变方式取决于不同方法的设计，即元模型的输出形式。例如，一些研究采用参数生成策略，将元模型的输出直接视为基础模型的任务特定参数。同时，也有一些研究采取更间接的方法，如基于门控的全球共享参数修改。我们总结了三类参数调节策略，包括参数生成、参数修改和参数集成，这些在图
    [6](#S5.F6 "图 6 ‣ 5.1. 冷启动推荐中的元学习 ‣ 5. 元学习方法 ‣ 推荐系统中的深度元学习：综述") 中进行了说明。表 [6](#S5.T6
    "表 6 ‣ 5.1. 冷启动推荐中的元学习 ‣ 5. 元学习方法 ‣ 推荐系统中的深度元学习：综述") 显示了基于模型的参数调节方法的总结。
- en: One strategy for designing meta-models for parameter modulation is to directly
    generate task-specific parameters of base models. For instance, Vartak et.at,
    (Vartak et al., [2017](#bib.bib100)) propose two models named LWA and NLBA, to
    address item cold-start problem. Both LWA and NLBA adopt similar deep neural network
    architectures as meta models to implement parameter generation strategies. The
    differences of these two models are the form of recommendation models and parameters
    to be adjusted. Specifically, take the LWA as the example, the meta-leaner $\mathcal{F}_{\omega}$
    consists of two sub-networks $\mathcal{G}(.)$ and $\mathcal{H}(.)$. The first
    sub-network $\mathcal{G}(.)$ learns task representations based on interacted items
    of a given user. Embeddings of positive interactions and negative interactions
    are aggregated as $R^{p}_{i}=\mathcal{G}(I^{p})$ and $R^{n}_{i}=\mathcal{G}(I^{n})$
    respectively. The second sub-network $\mathcal{H}(.)$ directly adjusts the base
    model based on $R^{p}_{i}$ and $R^{n}_{i}$ by learining a vector $\bm{w_{i}}=\bm{w}_{p}R^{p}_{i}+\bm{w}_{n}R^{n}_{i}$.
    Here, $\bm{w_{i}}$ are the generated linear transformation parameters of a logistic
    regression (LR) function, which is specific for user $u_{i}$. Then the logistic
    regression function will act as the user-specific recommendation model to predict
    the interaction probablity of a new item. Similarly, NLBA utilize a neural network
    classifier as the base model and generate bias parameters of all hidden layers
    to implement paramter generation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 设计参数调制的元模型的一种策略是直接生成基础模型的任务特定参数。例如，Vartak 等（Vartak 等，[2017](#bib.bib100)）提出了两个模型，分别为
    LWA 和 NLBA，以解决项目冷启动问题。LWA 和 NLBA 都采用类似的深度神经网络架构作为元模型来实现参数生成策略。这两个模型的区别在于推荐模型的形式和需要调整的参数。具体来说，以
    LWA 为例，元学习器 $\mathcal{F}_{\omega}$ 由两个子网络 $\mathcal{G}(.)$ 和 $\mathcal{H}(.)$
    组成。第一个子网络 $\mathcal{G}(.)$ 基于给定用户的互动项学习任务表示。正交互和负交互的嵌入分别聚合为 $R^{p}_{i}=\mathcal{G}(I^{p})$
    和 $R^{n}_{i}=\mathcal{G}(I^{n})$。第二个子网络 $\mathcal{H}(.)$ 直接基于 $R^{p}_{i}$ 和 $R^{n}_{i}$
    调整基础模型，通过学习一个向量 $\bm{w_{i}}=\bm{w}_{p}R^{p}_{i}+\bm{w}_{n}R^{n}_{i}$。这里，$\bm{w_{i}}$
    是逻辑回归（LR）函数的生成线性变换参数，特定于用户 $u_{i}$。然后，逻辑回归函数将作为用户特定的推荐模型来预测新项的互动概率。类似地，NLBA 使用神经网络分类器作为基础模型，并生成所有隐藏层的偏置参数来实现参数生成。
- en: 'To improve tail-item recommendation, i.e., item cold-start recommendation,
    Zhang et.at (Zhang et al., [2021a](#bib.bib123)) propose MIREC, which focuses
    on transferring knowledge from head items with rich user feedback to tail items
    with few interactions. Following the parameter-generation strategy in model-based
    meta-learning, a meta-mapping module is designed to transfer parameters of a few-shot
    model to a many-shot model, which achieves the model-level augmentation. Specifically,
    a meta model $\mathcal{F}_{\omega}$ learns to capture the model parameter mapping
    from a few-shot model to a many-shot model. The meta-knowledge to be learned in
    MIREC could be explained as the knowledge about model transformation when more
    training data are observed. Given a base model $g_{\theta}$, many-shot model $g_{\theta^{*}}$
    parameterized with $\theta^{*}$ is learned by feeding all user feedback. Then,
    to learn meta-knowledge of model transformation, the meta model $\mathcal{F}_{\omega}$
    is incorporated into the training process of a few-shot model $g_{\theta_{k}}$
    (trained with tail items that have less than $k$ interactions) to by minimizing
    the following objective function:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进尾项推荐，即项目冷启动推荐，张等（张等，[2021a](#bib.bib123)）提出了 MIREC，该方法专注于将来自用户反馈丰富的头项的知识转移到互动较少的尾项。根据基于模型的元学习中的参数生成策略，设计了一个元映射模块，将少样本模型的参数转移到多样本模型，从而实现模型级别的增强。具体来说，元模型
    $\mathcal{F}_{\omega}$ 学习从少样本模型到多样本模型的模型参数映射。在 MIREC 中要学习的元知识可以解释为在观察到更多训练数据时关于模型转换的知识。给定一个基础模型
    $g_{\theta}$，通过提供所有用户反馈来学习参数化为 $\theta^{*}$ 的多样本模型 $g_{\theta^{*}}$。然后，为了学习模型转换的元知识，将元模型
    $\mathcal{F}_{\omega}$ 融入到少样本模型 $g_{\theta_{k}}$（训练于少于 $k$ 次交互的尾项）的训练过程中，通过最小化以下目标函数来实现：
- en: '| (18) |  | $\mathcal{L}(\omega,\theta_{k})=&#124;&#124;\mathcal{F}_{\omega}(\theta_{k})-\theta^{*}&#124;&#124;^{2}+\mathcal{L}_{rec}(g_{\theta_{k}},D_{k})$
    |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\mathcal{L}(\omega,\theta_{k})=&#124;&#124;\mathcal{F}_{\omega}(\theta_{k})-\theta^{*}&#124;&#124;^{2}+\mathcal{L}_{rec}(g_{\theta_{k}},D_{k})$
    |  |'
- en: where $\mathcal{F}_{\omega}(.)$ tasks the parameters $\theta_{k}$ of the few-shot
    model as input and generate many-shot model parameters. The first L2 normalization
    term is utilized to train the parameter mapping ability of $\mathcal{F}_{\omega}$
    from few-shot models and many-shot models. After training, the final recommendation
    model is obtained by integrating both the original many-shot model $g_{\theta^{*}}$
    and the meta-mapped few-shot model $g_{\mathcal{F}_{\omega}(\theta_{k})}$, in
    order to perform well on both head and tail items.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}_{\omega}(.)$ 以少量样本模型的参数 $\theta_{k}$ 作为输入，并生成大量样本模型参数。第一个 L2
    正则化项用于训练 $\mathcal{F}_{\omega}$ 从少量样本模型到大量样本模型的参数映射能力。训练后，通过整合原始大量样本模型 $g_{\theta^{*}}$
    和元映射的少量样本模型 $g_{\mathcal{F}_{\omega}(\theta_{k})}$，得到最终的推荐模型，以在头部和尾部项目上表现良好。
- en: '![Refer to caption](img/3ca4a5ae4f91a9278d70be53e5671732.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3ca4a5ae4f91a9278d70be53e5671732.png)'
- en: Figure 6\. Illustration of different parameter modulation strategies including
    parameter generation, parameter modification, and parameter integration. One basic
    example is presented for each category.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 展示了不同参数调制策略的示意图，包括参数生成、参数修改和参数整合。每种类别都提供了一个基本示例。
- en: 'Another common strategy of designing meta models for parameter modulation is
    to modify globally shared parameters into task-specific ones. Instead of directly
    taking the outputs of meta models as parameters of base models, the core idea
    of the parameter-modification strategy is to tailor global parameters into task-specific
    ones under the control of meta models. Lin et.at (Lin et al., [2021](#bib.bib54))
    propose TaNP, which designs a task relevance aware parameter modulation mechanism
    to customize task-adaptive parameters for base recommendation models. Specifically,
    TaNP approximates each task as an instantiation of a stochastic process and utilizes
    an encoder and decoder structure as the preference estimation module, i.e., the
    base recommendation model. The meta model is designed for modulating parameters
    of the decoder module. Specifically, the meta model $\mathcal{F}_{\omega}$ first
    leverages a task identity network to encode interactions and a learnable global
    pool to automatically learn the relevance of different tasks. By doing this, the
    task representation is obtained as $\bm{o}_{i}$ and utilized to provide task relevance
    aware information for parameter modulation. Two candidate modulation strategies
    including FiLM (Perez et al., [2018](#bib.bib73)) and an extended Gating-FiLM
    are discussed to scale and shift the parameters of hidden layers of the decoder.
    Take the FiLM as an example, for the user $u_{i}$ the adjustment of the $l$-th
    hidden layer can be defined as:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的元模型设计策略是将全局共享参数修改为特定任务的参数。参数修改策略的核心思想不是直接将元模型的输出作为基础模型的参数，而是将全局参数调整为在元模型控制下的任务特定参数。Lin
    等人（Lin et al., [2021](#bib.bib54)）提出了 TaNP，它设计了一种任务相关的参数调制机制，以为基础推荐模型定制任务自适应参数。具体而言，TaNP
    将每个任务近似为随机过程的实例，并利用编码器和解码器结构作为偏好估计模块，即基础推荐模型。元模型旨在调制解码器模块的参数。具体来说，元模型 $\mathcal{F}_{\omega}$
    首先利用任务身份网络来编码交互，并使用可学习的全局池来自动学习不同任务的相关性。通过这样做，获得任务表示为 $\bm{o}_{i}$ 并用于为参数调制提供任务相关信息。讨论了包括
    FiLM（Perez et al., [2018](#bib.bib73)）和扩展的 Gating-FiLM 在内的两个候选调制策略，以缩放和移动解码器隐藏层的参数。以
    FiLM 为例，对于用户 $u_{i}$，第 $l$ 层隐藏层的调整可以定义为：
- en: '| (19) |  | $\displaystyle scale_{i}^{l}=tanh(\bm{W}_{a}^{l}\bm{o}_{i}),shift_{i}^{l}=tanh(\bm{W}_{b}^{l}\bm{o}_{i}),$
    |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\displaystyle scale_{i}^{l}=tanh(\bm{W}_{a}^{l}\bm{o}_{i}), shift_{i}^{l}=tanh(\bm{W}_{b}^{l}\bm{o}_{i}),$
    |  |'
- en: '| (20) |  | $\displaystyle\bm{x}^{l+1}_{i}=ReLU(scale_{i}^{l}\odot(\bm{W}_{dec}^{l}\bm{x}^{l}_{i}+\bm{b}_{dec}^{l})+shift_{i}^{l})$
    |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\displaystyle\bm{x}^{l+1}_{i}=ReLU(scale_{i}^{l}\odot(\bm{W}_{dec}^{l}\bm{x}^{l}_{i}+\bm{b}_{dec}^{l})+shift_{i}^{l})$
    |  |'
- en: where $\bm{W}_{dec}^{l}$ and $\bm{b}_{dec}^{l}$ are global parameters of the
    encoder. $scale_{i}^{l}$ and $shift_{i}^{l}$ are generated model modulation signals
    by the meta model. $\bm{x}^{l}_{i}$ is the inputs of the $l$-th layer of the decoder.
    In this way, TaNP achieves the task-adaptive parameter modulation leveraging model-based
    meta-learning.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{W}_{dec}^{l}$ 和 $\bm{b}_{dec}^{l}$ 是编码器的全局参数。$scale_{i}^{l}$ 和 $shift_{i}^{l}$
    是由元模型生成的模型调制信号。$\bm{x}^{l}_{i}$ 是解码器第 $l$ 层的输入。通过这种方式，TaNP 利用基于模型的元学习实现任务自适应参数调制。
- en: Similar parameter-modification strategy is also utilized in CMML (Feng et al.,
    [2021](#bib.bib22)), which utilizes two context encoders and a contextual modulation
    network as the meta model. Specifically, these two context encoders focus on extracting
    task context information of the cold-start tasks at the task level and instances(or
    interaction)-level, respectively. Then, the final context representation $c_{u_{i},v_{j}}$
    will be inputted as a hyper-network to generate modulation weights for the specific
    interaction $r_{u_{i},v_{j}}$. Three network modulation strategies provided by
    CMML is *Weight Modulation*, *Layer Modulation* and *Soft Modulation*. Specifically,
    weight modulation only generates weights and bias for the final linear layer.
    Layer modulation follows FilM and generates weights and bias for linear modulation
    on layers’ output similar to equation (19-20). Soft Modulation is conducted by
    introducing mixture of experts networks to generate dynamic routing weights for
    aggregating outputs of multiple subnetworks. Details of three network modulation
    strategies can be seen in the original paper.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的参数修改策略也被用于 CMML (Feng et al., [2021](#bib.bib22))，该方法利用两个上下文编码器和一个上下文调制网络作为元模型。具体而言，这两个上下文编码器分别专注于从任务级别和实例（或交互）级别提取冷启动任务的任务上下文信息。然后，最终的上下文表示
    $c_{u_{i},v_{j}}$ 将作为超网络输入，以生成特定交互 $r_{u_{i},v_{j}}$ 的调制权重。CMML 提供的三种网络调制策略是 *权重调制*、*层调制*
    和 *软调制*。具体来说，权重调制仅生成最终线性层的权重和偏置。层调制遵循 FilM，并生成线性调制的权重和偏置，类似于方程（19-20）。软调制通过引入专家混合网络生成动态路由权重，以汇聚多个子网络的输出。三种网络调制策略的详细信息请参见原始论文。
- en: Another work Heater (Zhu et al., [2020b](#bib.bib135)) leverages the parameter-integration
    strategy in model-based meta-learning for cold-start recommendation. By incorporating
    auxiliary information of cold-start users and items, Heater mainly transforms
    user/item auxiliary representations into collaborative filtering (CF) space and
    estimates the preference probability. They argue that personalized transformations
    to different users or items are required. Therefore, they propose to adopt a Mixture-of-Experts
    (Shazeer et al., [2017](#bib.bib87))to act as the meta-model for implementing
    personalized user transformation function $f^{U}_{i}$ and item transformation
    function $f^{I}_{j}$. Take the user side as an example, the Mixture-of-Experts
    consists of $M$ parallel experts with the same structure. Each expert $f^{m}$
    takes the user representation $\bm{u}_{i}$ as input, and outputs a transformed
    representation $f^{m}(\bm{u}_{i})$ of the user. The parameter-modification strategy
    works by adaptively combining outputs of all experts $\{f^{1}(\bm{u}_{i}),...,f^{M}(\bm{u}_{i})\}$
    with learnable weights. This is equivalent to an adaptive integration of the parameters
    of multiple experts. As a result, the final transformation function $f^{U}_{i}$
    is user-specific for each user.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作 Heater (Zhu et al., [2020b](#bib.bib135)) 利用基于模型的元学习中的参数整合策略进行冷启动推荐。通过结合冷启动用户和项目的辅助信息，Heater
    主要将用户/项目辅助表示转换到协同过滤（CF）空间，并估计偏好概率。他们认为需要对不同用户或项目进行个性化转换。因此，他们建议采用 Mixture-of-Experts
    (Shazeer et al., [2017](#bib.bib87)) 作为元模型，以实现个性化用户转换函数 $f^{U}_{i}$ 和项目转换函数 $f^{I}_{j}$。以用户端为例，Mixture-of-Experts
    包含 $M$ 个结构相同的并行专家。每个专家 $f^{m}$ 以用户表示 $\bm{u}_{i}$ 作为输入，并输出用户的转换表示 $f^{m}(\bm{u}_{i})$。参数修改策略通过自适应地组合所有专家的输出
    $\{f^{1}(\bm{u}_{i}),...,f^{M}(\bm{u}_{i})\}$，结合可学习的权重。这相当于对多个专家参数的自适应整合。因此，最终的转换函数
    $f^{U}_{i}$ 对每个用户都是特定的。
- en: Metric-based Embedding Space Learning. Metric-based meta-learning is also utilized
    in cold-start recommendation to meta-learn embedding space for embedding similarity
    comparison. To alleviate cold-start problem in long-tail item recommendation,
    Sankar et al.(Sankar et al., [2021](#bib.bib83)) proposes ProtoCF which learns
    a shared metric space for measuring embedding similarities between candidate cold-start
    items and users. Specifically, inspired by the Prototypical Networks (Snell et al.,
    [2017](#bib.bib89)), ProtoCF learns to compose discriminative prototypes for tail
    items from their few-shot interactions. Based on the support set $\mathcal{S}_{i}$,
    the prototype representation for each item $v_{i}$ is first computed as the mean
    vector of pretrained user embeddings. Then, a fixed number of group embeddings
    are learned as external memories to enrich prototype representations of each item.
    Finally, following the framework of metric learning, given a query user, the similarities
    between prototype representations $\{\bm{p}_{1},...,\bm{p}_{N}\}$ of candidate
    items and the user representation $\bm{u}_{i}$ are computed in the meta-learned
    metric space.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 基于度量的嵌入空间学习。基于度量的元学习也用于冷启动推荐，以元学习嵌入空间用于嵌入相似性比较。为了缓解长尾物品推荐中的冷启动问题，Sankar等（Sankar
    et al., [2021](#bib.bib83)）提出了ProtoCF，该方法学习一个共享的度量空间，用于测量候选冷启动物品与用户之间的嵌入相似性。具体而言，受到原型网络（Snell
    et al., [2017](#bib.bib89)）的启发，ProtoCF从少量交互中学习区分性的原型，以处理长尾物品。基于支持集$\mathcal{S}_{i}$，首先计算每个物品$v_{i}$的原型表示，作为预训练用户嵌入的均值向量。然后，学习固定数量的组嵌入作为外部记忆，以丰富每个物品的原型表示。最后，遵循度量学习的框架，给定一个查询用户，计算候选物品的原型表示$\{\bm{p}_{1},...,\bm{p}_{N}\}$与用户表示$\bm{u}_{i}$之间的相似性，这些相似性在元学习的度量空间中进行计算。
- en: Borrowing the idea of measuring embedding similarity, Hao et al. (Hao et al.,
    [2021](#bib.bib31)) study how to pretrain GNNs to learn embeddings for cold-start
    users and items via few-shot reconstruction tasks. Instead of learning embedding
    space for calculating embedding similarity between users and items, the PreTraining
    approach focuses on learning reconstruction space for comparing reconstructed
    embeddings of few-shot users/items and their ground truth embeddings learned from
    abundant interactions. Reconstruction tasks first select target users/items that
    have sufficient interactions and simulate cold-start situations by sampling a
    few neighbors for each target user/item. Assuming embeddings trained with abundant
    interactions are ground truths, the goal of the reconstruction tasks is to reconstruct
    embeddings based on few-shot neighbors. By measuring and maximizing the similarities
    among the reconstructed embeddings and the ground truths, the pretrained GNNs
    are supposed to learn effective embedding space for cold-start users and items.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 借鉴嵌入相似性测量的思想，Hao等（Hao et al., [2021](#bib.bib31)）研究了如何通过少量重建任务预训练GNNs以学习冷启动用户和物品的嵌入。与其学习嵌入空间以计算用户和物品之间的嵌入相似性，PreTraining方法专注于学习重建空间，以比较少量用户/物品的重建嵌入与从大量交互中学习到的真实嵌入。重建任务首先选择具有足够交互的目标用户/物品，并通过为每个目标用户/物品采样少量邻居来模拟冷启动情况。假设通过大量交互训练的嵌入是真实值，重建任务的目标是基于少量邻居重建嵌入。通过测量和最大化重建嵌入与真实值之间的相似性，预训练的GNNs被期望学习到有效的冷启动用户和物品的嵌入空间。
- en: 5.2\. Meta-learning in CTR Prediction
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. CTR预测中的元学习
- en: 'We summarize details of meta-learning methods in click-through rate prediction
    from three perspectives, i.e., meta-learning techniques, used auxiliary information,
    and meta-knowledge representations, as shown in Table [7](#S5.T7 "Table 7 ‣ 5.2\.
    Meta-learning in CTR Prediction ‣ 5\. Meta-leanring Methods for Recommendation
    Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey"). Next, we will
    elaborate on two groups of methods including *Optimization-based Item Embedding
    Initialization* and *Model-based Item Embedding Genetation*.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从三个角度总结了点击率预测中的元学习方法的细节，即元学习技术、使用的辅助信息和元知识表示，如表[7](#S5.T7 "Table 7 ‣ 5.2\.
    Meta-learning in CTR Prediction ‣ 5\. Meta-leanring Methods for Recommendation
    Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey")所示。接下来，我们将详细阐述包括*基于优化的物品嵌入初始化*和*基于模型的物品嵌入生成*在内的两组方法。'
- en: Table 7\. Details of recommendation models with meta-learning methods in click
    through rate prediction.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表7\. 使用元学习方法的推荐模型在点击率预测中的详细信息。
- en: '| Method | Meta-learning Technique | Auxiliary Information | Meta-knowledge
    representation |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 元学习技术 | 辅助信息 | 元知识表示 |'
- en: '| Meta-Embedding (Pan et al., [2019](#bib.bib70)) | Optimization-based | Item
    Attributes | Embedding Initialization & Meta Model |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Meta-Embedding（潘等， [2019](#bib.bib70)） | 基于优化 | 项目属性 | 嵌入初始化与元模型 |'
- en: '| DisNet (Li et al., [2020](#bib.bib52)) | Optimization-based | Revelant Items
    | Embedding Initialization & Meta Model |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| DisNet（李等， [2020](#bib.bib52)） | 基于优化 | 相关项目 | 嵌入初始化与元模型 |'
- en: '| GME (Ouyang et al., [2021](#bib.bib69)) | Optimization-based | Item Attributes
    & Relevant Items | Embedding Initialization & Meta Model |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| GME（欧阳等， [2021](#bib.bib69)） | 基于优化 | 项目属性与相关项目 | 嵌入初始化与元模型 |'
- en: '| TDAML (Cao et al., [2020](#bib.bib6)) | Optimization-based | Item Attributes
    | Embedding Initialization & Meta Model & Sample Weights |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| TDAML（曹等， [2020](#bib.bib6)） | 基于优化 | 项目属性 | 嵌入初始化与元模型与样本权重 |'
- en: '| MWUF (Zhu et al., [2021d](#bib.bib134)) | Model-based | Item Attributes &
    Interacted Users | Meta Model |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| MWUF（朱等， [2021d](#bib.bib134)） | 基于模型 | 项目属性与互动用户 | 元模型 |'
- en: '| Meta-SSIN (Sun et al., [2021c](#bib.bib95)) | Optimization-based | Historical
    Items | Parameters Initialization |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Meta-SSIN（孙等， [2021c](#bib.bib95)） | 基于优化 | 历史项目 | 参数初始化 |'
- en: '![Refer to caption](img/0f31e607e4790a40940115d42932d629.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f31e607e4790a40940115d42932d629.png)'
- en: Figure 7\. Illustration of different structures of embedding generators in meta-learning
    methods for CTR prediction. We mainly compare them from what kind of auxiliary
    information is considered when generating initial embeddings or warm embeddings
    for new items.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 不同嵌入生成器在CTR预测的元学习方法中的结构示意图。我们主要从生成新项目的初始嵌入或预热嵌入时考虑了哪些辅助信息进行比较。
- en: Optimization-based Item Embedding Initialization. This category of methods mainly
    focuses on learning initial embeddings for new items, so as to achieve better
    cold-start and warm-up performance. The main idea of this category is to design
    an external ID embedding generator as a meta-learner, and apply it to generate
    adaptive initial ID embeddings for different items newly arrived. The meta-learner
    is trained under the optimization-based meta-learning framework.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优化的项目嵌入初始化。此类别的方法主要集中于学习新项目的初始嵌入，以实现更好的冷启动和预热性能。该类别的主要思想是设计一个外部ID嵌入生成器作为元学习者，并将其应用于为不同的新项目生成自适应的初始ID嵌入。元学习者在基于优化的元学习框架下进行训练。
- en: 'Pan et al., (Pan et al., [2019](#bib.bib70)) firstly propose the idea of meta-learning
    a initial embedding generator to replace the randomly intialization strategy for
    click-through rate prediction problem. Specificially, as shown in Fig [7](#S5.F7
    "Figure 7 ‣ 5.2\. Meta-learning in CTR Prediction ‣ 5\. Meta-leanring Methods
    for Recommendation Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey")
    (b), an item/Ad features based embedding generator named Meta-Embedding is designed
    to take Ad attributes as inputs and generate item-specific initial embeddings
    $\bm{v}^{ini}_{i}$. Then the generated user ID embedding $\bm{v}^{ini}_{i}$ is
    combined with other feature embeddings such as user embeddings, item attribute
    embedddings and context embeddings and fed into pretrained predcition models,
    e.g., DeepFM (Guo et al., [2017](#bib.bib30)), PNN (Qu et al., [2016](#bib.bib77)),
    Wide&Deep (Cheng et al., [2016](#bib.bib10)). For the meta-optimization of the
    Meta-Embedding generator, two batches of labeled instances are sampled for each
    cold-start item. The first batch $\mathcal{D}_{i}^{a}$ is utilized to evaluate
    the cold-start performance by directly making predictions with $\bm{v}^{ini}_{i}$.
    The second batch $\mathcal{D}_{i}^{b}$ is utilized to evaluate the warm-up performance
    by making predictions with item embedding $\bm{v}^{warm}_{i}$ which is locally
    updated over the first batch data $\mathcal{D}_{i}^{a}$. By doing this, two losses
    $\mathcal{L}_{cold}(\bm{v}^{ini}_{i},\mathcal{D}_{i}^{a})$ and $\mathcal{L}_{warm}(\bm{v}^{warm}_{i},\mathcal{D}_{i}^{b})$
    are obtained in cold-start phase and warm-up phase, respectively. Based on a unified
    loss, i.e., $\mathcal{L}_{meta}=\delta\mathcal{L}_{cold}+(1-\delta)\mathcal{L}_{warm}$,
    outer-level update of optmization-based meta-learning is performed to globally
    optimize the generator through gradient descent.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pan等人（Pan et al., [2019](#bib.bib70)）首次提出了利用元学习（meta-learning）来替代随机初始化策略，用于点击率预测问题的初始嵌入生成器的想法。具体来说，如图[7](#S5.F7
    "Figure 7 ‣ 5.2\. Meta-learning in CTR Prediction ‣ 5\. Meta-leanring Methods
    for Recommendation Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey")
    (b)所示，设计了一种基于项目/广告特征的嵌入生成器，称为Meta-Embedding，用于接收广告属性作为输入并生成特定项目的初始嵌入$\bm{v}^{ini}_{i}$。然后，将生成的用户ID嵌入$\bm{v}^{ini}_{i}$与其他特征嵌入（如用户嵌入、项目属性嵌入和上下文嵌入）结合，并输入到预训练预测模型中，例如DeepFM
    (Guo et al., [2017](#bib.bib30))，PNN (Qu et al., [2016](#bib.bib77))，Wide&Deep
    (Cheng et al., [2016](#bib.bib10))。对于Meta-Embedding生成器的元优化，针对每个冷启动项目，抽取了两批标记实例。第一批$\mathcal{D}_{i}^{a}$用于通过直接使用$\bm{v}^{ini}_{i}$进行预测来评估冷启动性能。第二批$\mathcal{D}_{i}^{b}$用于通过使用在第一批数据$\mathcal{D}_{i}^{a}$上局部更新的项目嵌入$\bm{v}^{warm}_{i}$进行预测来评估热身性能。通过这种方式，分别在冷启动阶段和热身阶段得到两个损失值$\mathcal{L}_{cold}(\bm{v}^{ini}_{i},\mathcal{D}_{i}^{a})$和$\mathcal{L}_{warm}(\bm{v}^{warm}_{i},\mathcal{D}_{i}^{b})$。基于统一损失，即$\mathcal{L}_{meta}=\delta\mathcal{L}_{cold}+(1-\delta)\mathcal{L}_{warm}$，通过梯度下降对生成器进行全局优化，完成优化基础的元学习的外部级别更新。'
- en: 'Following the idea of item ID embedding generation, several works mainly extend
    the forms of embedding generators by leveraging other auxiliary information besides
    item attributes, especially information from relevant users and relevant items.
    Ouyang et al. (Ouyang et al., [2021](#bib.bib69)) propose a series of graph meta
    embedding (GMEs) models to learn initial item embeddings based on not only item
    attributes but also existing relevant items. As shown in Fig [7](#S5.F7 "Figure
    7 ‣ 5.2\. Meta-learning in CTR Prediction ‣ 5\. Meta-leanring Methods for Recommendation
    Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey") (b), GMEs first
    connect existing items with new items with graphs through shared item attributes
    and then apply the graph attention networks to distill neighborhood information
    for generating embeddings of cold-start items. Three different strategies for
    distilling information from existing items including pre-defining item embeddings,
    generating item embeddings from item attributes, and directly aggregating attribute
    embeddings without learning ID embeddings, are discussed in different variants
    of GMEs. Similar to the Meta-Embedding, GMEs also resort to the optimization-based
    meta-learning framework to train the graph neural network based embedding generator
    with two sampled batches of each task. Similarly, Li et al. (Li et al., [2020](#bib.bib52))
    proposes a deep interest-shifting network DisNet which includes a meta-Id-embedding
    generator (RM-IdEG) module as the initial ID embedding generator. RM-IdEG mainly
    collects a set of existing items relevant to the target cold-start item through
    item relations and learns an attentional representation as to the initial ID embedding.
    Similar to the Meta-Embedding, the optimization of RM-IdEG is separated from pretraining
    the whole DisNet model and conducted by minimizing both cold-start loss and warm-up
    loss with optimization-based meta-learning.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '基于项 ID 嵌入生成的思路，若干研究主要通过利用除项属性之外的其他辅助信息，特别是相关用户和相关项的信息，来扩展嵌入生成器的形式。Ouyang 等人（Ouyang
    et al., [2021](#bib.bib69)）提出了一系列图元嵌入（GMEs）模型，以基于项属性和现有相关项来学习初始项嵌入。如图 [7](#S5.F7
    "Figure 7 ‣ 5.2\. Meta-learning in CTR Prediction ‣ 5\. Meta-leanring Methods
    for Recommendation Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey")
    (b) 所示，GMEs 首先通过共享项属性将现有项与新项连接起来，然后应用图注意力网络提炼邻域信息，以生成冷启动项的嵌入。讨论了从现有项中提炼信息的三种不同策略，包括预定义项嵌入、从项属性生成项嵌入以及直接聚合属性嵌入而不学习
    ID 嵌入，这些策略在不同变种的 GMEs 中被探讨。类似于 Meta-Embedding，GMEs 也利用基于优化的元学习框架来训练图神经网络基础的嵌入生成器，每个任务有两个采样批次。类似地，Li
    等人（Li et al., [2020](#bib.bib52)）提出了一种深度兴趣转移网络 DisNet，其中包括一个元 ID 嵌入生成器（RM-IdEG）模块作为初始
    ID 嵌入生成器。RM-IdEG 主要通过项关系收集一组与目标冷启动项相关的现有项，并学习一个注意力表示作为初始 ID 嵌入。类似于 Meta-Embedding，RM-IdEG
    的优化与预训练整个 DisNet 模型分开进行，通过最小化冷启动损失和热身损失来进行优化。'
- en: Under the framework of optimization-based item embedding initialization, i.e.
    Meta-Embedding, the optimization strategy is also studied to improve the adaptation
    ability against the diversity of the task difficulty. Cao et al. (Cao et al.,
    [2020](#bib.bib6)) proposed a task-distribution-aware meta-learning method (shorted
    as TDAML) to ensure the consistency between the loss weight and task difficulty
    when globally updating the embedding generator. They argue that different tasks
    should have different difficulties in the meta-training phase and assigning equal
    weights to all tasks may pay limited attention to the hard tasks. On top of the
    meta-embedding framework, TDAML proposes to adaptively assign different weights
    when summing the meta losses of different tasks. By modeling the weights $\bm{p}_{i}$
    of meta-losses as the description of task difficulty, extra constraints expecting
    strong consistency between $\bm{p}_{i}$ and meta-loss of the task, i.e., $\mathcal{L}_{meta}^{i}$,
    are added to find an adaptive loss weight which replaces the uniform weight. As
    a result, the meta-optimization phase could pay more attention to the harder tasks
    and achieves better performance improvement.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于优化的物品嵌入初始化框架下，即元嵌入，优化策略也被研究以提高对任务难度多样性的适应能力。Cao et al. (Cao et al., [2020](#bib.bib6))
    提出了任务分布感知元学习方法（简称 TDAML），以确保在全局更新嵌入生成器时损失权重与任务难度之间的一致性。他们认为不同的任务在元训练阶段应该具有不同的难度，给所有任务分配相同的权重可能会对困难任务关注有限。在元嵌入框架之上，TDAML
    提出了在汇总不同任务的元损失时自适应地分配不同的权重。通过将元损失的权重 $\bm{p}_{i}$ 建模为任务难度的描述，增加了额外的约束，期望 $\bm{p}_{i}$
    与任务的元损失，即 $\mathcal{L}_{meta}^{i}$，之间有较强的一致性，以找到替代均匀权重的自适应损失权重。因此，元优化阶段可以更多关注困难任务，从而获得更好的性能提升。
- en: 'Model-based Item Embedding Generation. Besides optimization-based techniques,
    model-based meta-learning is also applied to generate initial item embeddings
    for better click-through rate prediction performance. Zhu et al. (Zhu et al.,
    [2021d](#bib.bib134)) propose MWUF which aims to meta-learn scaling and shifting
    functions for generating ID embeddings of cold-start items. As shown in Fig [7](#S5.F7
    "Figure 7 ‣ 5.2\. Meta-learning in CTR Prediction ‣ 5\. Meta-leanring Methods
    for Recommendation Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey")
    (c), different from optimization-based item embedding initialization above, MWUF
    directly transforms the cold item ID embedding $\bm{v}^{cold}_{i}$ of the item
    $v_{i}$ to a warm item ID embedding $\bm{v}^{warm}_{i}$ by applying a scaling
    and shifting function as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '基于模型的物品嵌入生成。除了基于优化的技术外，基于模型的元学习也被应用于生成初始物品嵌入，以提高点击率预测性能。Zhu et al. (Zhu et
    al., [2021d](#bib.bib134)) 提出了 MWUF，旨在元学习缩放和偏移函数，以生成冷启动物品的 ID 嵌入。如图 [7](#S5.F7
    "Figure 7 ‣ 5.2\. Meta-learning in CTR Prediction ‣ 5\. Meta-leanring Methods
    for Recommendation Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey")
    (c) 所示，与上述基于优化的物品嵌入初始化不同，MWUF 通过应用以下缩放和平移函数，直接将冷物品 ID 嵌入 $\bm{v}^{cold}_{i}$ 转换为温暖的物品
    ID 嵌入 $\bm{v}^{warm}_{i}$：'
- en: '| (21) |  | $\displaystyle\bm{v}^{warm}_{i}=\bm{v}^{cold}_{i}\cdot h^{scale}(\bm{x}_{i})+h^{shift}(\bm{U}_{i}),$
    |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $\displaystyle\bm{v}^{warm}_{i}=\bm{v}^{cold}_{i}\cdot h^{scale}(\bm{x}_{i})+h^{shift}(\bm{U}_{i}),$
    |  |'
- en: where $\bm{x}_{i}$ denotes the item feature embedding of the item $v_{i}$ and
    $\bm{U}_{i}$ denotes embeddings of its interacted users. Here a meta scaling network
    $h^{scale}(*)$ takes $\bm{x}_{i}$ as input and generate personalized scaling parameters.
    A meta shifting network $h^{shift}(*)$ takes $\bm{U}_{i}$ as input and generate
    personalized shifting parameters. After obtaining the warm ID embedding $\bm{v}^{warm}_{i}$,
    MWUF directly make predictions based on pretrained recommendation models such
    as Wide&Deep (Cheng et al., [2016](#bib.bib10)), DIN (Zhou et al., [2018b](#bib.bib128))
    and AFM (Cheng et al., [2020](#bib.bib11)). The meta models, i.e., two meta networks,
    are optimized by minimizing the warm loss, which is obtained by making predictions
    with $\bm{v}^{warm}_{i}$ over observed interactions of the item $v_{i}$.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\bm{x}_{i}$ 表示物品 $v_{i}$ 的特征嵌入，$\bm{U}_{i}$ 表示与其交互的用户的嵌入。这里，元缩放网络 $h^{scale}(*)$
    以 $\bm{x}_{i}$ 作为输入，并生成个性化的缩放参数。元平移网络 $h^{shift}(*)$ 以 $\bm{U}_{i}$ 作为输入，并生成个性化的平移参数。获得温暖的
    ID 嵌入 $\bm{v}^{warm}_{i}$ 后，MWUF 直接基于预训练的推荐模型如 Wide&Deep (Cheng et al., [2016](#bib.bib10))、DIN
    (Zhou et al., [2018b](#bib.bib128)) 和 AFM (Cheng et al., [2020](#bib.bib11)) 进行预测。这些元模型，即两个元网络，通过最小化温暖损失进行优化，温暖损失是通过对物品
    $v_{i}$ 的观测交互使用 $\bm{v}^{warm}_{i}$ 进行预测得到的。
- en: 5.3\. Meta-learning in Online Recommendation
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 在线推荐中的元学习
- en: 'In practical large-scale recommender systems, new interaction data are collected
    continuously. Therefore, newly arrived data should be leveraged to update the
    recommendation models timely, so as to capture evolving preference trends. Meta-learning
    methods are also studied in such online settings in order to enhance the ability
    to efficiently update recommendation models. Table [8](#S5.T8 "Table 8 ‣ 5.3\.
    Meta-learning in Online Recommendation ‣ 5\. Meta-leanring Methods for Recommendation
    Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey") summarize meta-learning
    based methods in online recommendation from three perspectives. Next, we will
    elaborate on three groups of methods which are divided according to different
    levels of modeling updating in the following.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '在实际的大规模推荐系统中，新的交互数据不断被收集。因此，新到的数据应被利用以及时更新推荐模型，从而捕捉不断变化的偏好趋势。元学习方法也在这种在线环境中进行了研究，以增强高效更新推荐模型的能力。表[8](#S5.T8
    "Table 8 ‣ 5.3\. Meta-learning in Online Recommendation ‣ 5\. Meta-leanring Methods
    for Recommendation Systems ‣ Deep Meta-learning in Recommendation Systems: A Survey")从三个方面总结了在线推荐中的元学习方法。接下来，我们将详细阐述按照不同的建模更新水平划分的三组方法。'
- en: Table 8\. Details of recommendation models with meta-learning methods in online
    recommendation.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表8\. 在线推荐中使用元学习方法的推荐模型详细信息。
- en: '| Method | Meta-learning Technique | Task Division | Meta-knowledge representation
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 元学习技术 | 任务划分 | 元知识表示 |'
- en: '| S2Meta (Du et al., [2019](#bib.bib18)) | Optimization-based | Scenario-specific
    | Parameters Initialization & Meta-learner & Hyperparameter |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| S2Meta (Du et al., [2019](#bib.bib18)) | 基于优化 | 场景特定 | 参数初始化与元学习者与超参数 |'
- en: '| FLIP (Liu et al., [2020b](#bib.bib58)) | Optimization-based | Sequence-specific
    | Parameters Initialization |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| FLIP (Liu et al., [2020b](#bib.bib58)) | 基于优化 | 序列特定 | 参数初始化 |'
- en: '| FORM (Sun et al., [2021a](#bib.bib94)) | Optimization-based | User-specific
    | Parameter Initialization & Hyperparameter |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| FORM (Sun et al., [2021a](#bib.bib94)) | 基于优化 | 用户特定 | 参数初始化与超参数 |'
- en: '| SML (Zhang et al., [2020](#bib.bib124)) | Model-based | Time-specific | Meta
    Model |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| SML (Zhang et al., [2020](#bib.bib124)) | 基于模型 | 时间特定 | 元模型 |'
- en: '| ASMG (Peng et al., [2021](#bib.bib72)) | Model-based | Time-specific | Meta
    Model |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| ASMG (Peng et al., [2021](#bib.bib72)) | 基于模型 | 时间特定 | 元模型 |'
- en: '| LSTTM (Xie et al., [2021](#bib.bib112)) | Optimization-based | Time-specific
    | Parameter Initialization |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| LSTTM (Xie et al., [2021](#bib.bib112)) | 基于优化 | 时间特定 | 参数初始化 |'
- en: '| MeLON (Kim et al., [2022](#bib.bib45)) | Model-based | Time-specific | Meta
    Model & Hyperparameter |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| MeLON (Kim et al., [2022](#bib.bib45)) | 基于模型 | 时间特定 | 元模型与超参数 |'
- en: User-level Preference Updating. This group of methods mainly divides new interactions
    according to different users, and designs online learning strategies to learn
    dynamic user preferences changing over time. Liu et al. (Liu et al., [2020b](#bib.bib58))
    propose FLIP which aims to decouple the learning of user intent (i.e., dynamic
    short-term interest) and preference (i.e., stable long-term interest) by treating
    user intents of different user sessions as meta-learning tasks. Instead of jointly
    learning user intent and preference from newly arrived user visit sequences, FLIP
    separately learns intent embeddings only based on the interactions of the current
    session while learning the preference embedding of the user during the whole online
    learning procedure. Specifically, inspired by an optimization-based meta-learning
    framework under online setting Online MAML (Finn et al., [2019](#bib.bib24)),
    FLIP learns the initial intent embedding for all sessions which is expected to
    quickly adapt to each new session. The support set of a task consists of the first
    $m$ interactions in the session, and the rest is treated as the query set. The
    outer-level update of the initial intent embedding is performed across a batch
    of tasks. Therefore, by learning user intent embedding with optimization-based
    meta-learning techniques, FLIP enhances the ability of user-level preference updating,
    especially capturing short-term preference evolution during the online learning
    procedure.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 用户级别的偏好更新。这组方法主要根据不同用户划分新的交互，并设计在线学习策略来学习随时间变化的动态用户偏好。Liu 等（Liu 等，[2020b](#bib.bib58)）提出了
    FLIP，其旨在通过将不同用户会话的用户意图（即动态短期兴趣）和偏好（即稳定长期兴趣）作为元学习任务来解耦用户意图和偏好的学习。FLIP 不是从新到达的用户访问序列中联合学习用户意图和偏好，而是仅根据当前会话的交互单独学习意图嵌入，同时在整个在线学习过程中学习用户的偏好嵌入。具体而言，受到在线设置下基于优化的元学习框架
    Online MAML（Finn 等，[2019](#bib.bib24)）的启发，FLIP 为所有会话学习初始意图嵌入，期望能够快速适应每个新会话。任务的支持集由会话中的前
    $m$ 次交互组成，其余部分被视为查询集。初始意图嵌入的外部级更新是在一批任务之间进行的。因此，通过使用基于优化的元学习技术学习用户意图嵌入，FLIP 提升了用户级别偏好更新的能力，特别是在在线学习过程中捕捉短期偏好演变的能力。
- en: Another work FORM (Sun et al., [2021a](#bib.bib94)) also studies meta-learning-based
    online recommendation based on user-specific task division. To adapt the optimization-based
    meta-learning to fluctuating online scenarios, FORM enhances the MAML framework
    to provide a more stable training process in the following directions. First,
    during local updates of current interactions of a user, a follow the online meta
    leader (FTOML) algorithm is designed to preserve prior knowledge extracted from
    all historical interactions of the user. In this way, the updated model during
    the online training procedure is expected to perform well on not only current
    data but also prior data, which stables user preference learning. Second, to ensure
    a consistent update process, a regularized term is added to the loss function
    to restrict the model parameters as sparse. Third, considering that users with
    abundant interactions have fewer fluctuations, FORM is designed to assign larger
    learning rates to users who have larger record lengths and smaller variance of
    gradients. With the three designs for tackling the fluctuating and noisy nature
    of online scenarios, FORM is expected to provide a more stable meta-optimization
    phase for online recommenders.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项工作 FORM（Sun 等，[2021a](#bib.bib94)）也研究了基于用户特定任务划分的元学习在线推荐。为了将基于优化的元学习适应于波动的在线场景，FORM
    在以下几个方向上增强了 MAML 框架，以提供更稳定的训练过程。首先，在用户当前交互的本地更新过程中，设计了一种跟随在线元学习领导者（FTOML）算法，以保留从用户所有历史交互中提取的先前知识。这样，在在线训练过程中更新的模型预计不仅对当前数据表现良好，对先前数据也能表现出色，从而稳定用户偏好学习。其次，为确保一致的更新过程，向损失函数中添加了一个正则化项，以限制模型参数为稀疏。第三，考虑到交互丰富的用户波动较小，FORM
    设计为给与交互记录较长和梯度方差较小的用户分配更大的学习率。通过这三种应对在线场景波动和噪声性质的设计，FORM 预计能为在线推荐系统提供更稳定的元优化阶段。
- en: Scenairo-level Model Updating. Besides conducting user-level preference learning,
    Du et al. (Du et al., [2019](#bib.bib18)) considers scenario-specific recommendation
    tasks and proposes a sequential meta-learner S2Meta to automatically learn personalized
    models for newly appeared scenarios. For instance, scenario-specific tasks could
    be defined according to item category, item tag, theme events, and so on. When
    a small size of interactions are collected online in a new scenario $s_{i}$, S2Meta
    aims to fastly update an initial base model $f_{\theta}$ to a scenario-specific
    recommendation model $f_{\theta_{i}}$. Specifically, the meta-knowledge to be
    globally learned is defined as three factors controlling the inner-level learning,
    including initial parameters, learning rates, and early-stop policy. The local
    update of each recommendation task is considered as a sequential learning process
    consisting of initializing, finetuning with adaptive learning rates, and stopping
    timely. The sequential learning process is automatically controlled under three
    parts of a designed meta model which is learned under the optimization-based meta-learning
    framework.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 场景级模型更新。除了进行用户级别的偏好学习外，Du et al.（Du et al., [2019](#bib.bib18)）考虑了场景特定的推荐任务，并提出了一种序列元学习器S2Meta，以自动学习新出现场景的个性化模型。例如，可以根据项目类别、项目标签、主题事件等定义场景特定任务。当在新的场景$s_{i}$中收集到少量交互数据时，S2Meta旨在快速将初始基础模型$f_{\theta}$更新为场景特定的推荐模型$f_{\theta_{i}}$。具体而言，全球学习的元知识被定义为控制内部学习的三个因素，包括初始参数、学习率和早停策略。每个推荐任务的本地更新被视为一个序列学习过程，包括初始化、使用自适应学习率的微调和及时停止。序列学习过程在一个基于优化的元学习框架下设计的元模型的三部分下自动控制。
- en: System-level Model Retraining. Online recommendation systems usually require
    periodical model retraining with new instances to capture current trends effectively.
    Recently, several works formalize the model retraining tasks from the perspective
    of meta-learning and study meta-learning based model retraining in the online
    recommendation (Zhang et al., [2020](#bib.bib124); Peng et al., [2021](#bib.bib72);
    Xie et al., [2021](#bib.bib112); Kim et al., [2022](#bib.bib45)).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 系统级模型再训练。在线推荐系统通常需要定期用新实例进行模型再训练，以有效捕捉当前趋势。最近，一些研究从元学习的角度正式化了模型再训练任务，并研究了基于元学习的模型再训练在在线推荐中的应用（Zhang
    et al., [2020](#bib.bib124); Peng et al., [2021](#bib.bib72); Xie et al., [2021](#bib.bib112);
    Kim et al., [2022](#bib.bib45)）。
- en: Zhang et.al (Zhang et al., [2020](#bib.bib124)) firstly investigate the model
    retraining mechanism from the scheme of meta-learning. At a time period $t$, the
    model retraining task $\mathcal{T}_{t}$ is constructed with interactions $D_{t}$
    collected currently as the support set, and interactions $D_{t+1}$ in the next
    time period as the query set. The goal of the model retraining task $\mathcal{T}_{t}$
    is to incrementally update the recommendation model $f_{\theta_{t-1}}$ obtained
    in the $t-1$ time period to a new one $f_{\theta_{t}}$ which is expected to achieve
    better performance in the next time period, i.e., $t+1$. Zhang et al. apply model-based
    meta-learning techniques to directly transfer parameters $\theta_{t-1}$ to model
    parameters $\theta_{t}$ with a meta model. Specifically, the meta model utilizes
    convolutional neural networks as a transfer component which inputs previous parameters
    $\theta_{t-1}$ and parameters $\hat{\theta_{t}}$ that are locally updated over
    $D_{t}$. The parameters of the next recommendation model $f_{\theta_{t}}$ are
    generated from the outputs of the transfer component. To make the learned model
    serve well in the next time period, the loss over $D_{t+1}$ is observed to update
    the parameters of the meta model. Since the meta-learning based model retraining
    framework above is operated in a sequential manner, thus the method is named Sequential
    Meta-Learning (SML).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang等人（Zhang et al., [2020](#bib.bib124)）首先从元学习的角度研究了模型重训练机制。在某一时间段$t$，模型重训练任务$\mathcal{T}_{t}$由当前收集的交互$D_{t}$作为支持集构建，并将下一个时间段的交互$D_{t+1}$作为查询集。模型重训练任务$\mathcal{T}_{t}$的目标是将$t-1$时间段获得的推荐模型$f_{\theta_{t-1}}$增量更新为新的模型$f_{\theta_{t}}$，期望在下一个时间段，即$t+1$，达到更好的性能。Zhang等人应用基于模型的元学习技术，通过元模型直接将参数$\theta_{t-1}$转移到模型参数$\theta_{t}$。具体而言，元模型利用卷积神经网络作为转移组件，输入前期参数$\theta_{t-1}$和在$D_{t}$上局部更新的参数$\hat{\theta_{t}}$。下一个推荐模型$f_{\theta_{t}}$的参数由转移组件的输出生成。为了使学习到的模型在下一个时间段表现良好，通过观察$D_{t+1}$上的损失来更新元模型的参数。由于上述基于元学习的模型重训练框架是以顺序方式操作的，因此该方法被称为顺序元学习（SML）。
- en: Following the idea of SML, Peng et al. (Peng et al., [2021](#bib.bib72)) propose
    another model retraining method ASMG, which is devise to generate the current
    model $f_{\theta_{t}}$ based on a sequence of historical models $\{f_{\theta_{1}},...,f_{\theta_{t-1}}\}$.
    Different from SML, ASMG replaces the CNN-based transfer module with gated recurrent
    units (GRU) as a meta-generator that captures long-term sequential patterns in
    model evolution. The meta generator inputs a truncated sequence of historical
    models of previous periods sequentially. Then the final hidden state $\bm{h}_{t}$
    of the GRU is transformed to generate the parameters of current model $f_{\theta_{t}}$.
    Similar to SML, the meta-generator in ASMG is also optimized towards better performance
    over interactions of the next time period $t+1$.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SML的理念，Peng等人（Peng et al., [2021](#bib.bib72)）提出了一种新的模型重训练方法ASMG，该方法旨在基于一系列历史模型$\{f_{\theta_{1}},...,f_{\theta_{t-1}}\}$生成当前模型$f_{\theta_{t}}$。与SML不同，ASMG用门控递归单元（GRU）替代了基于CNN的转移模块，作为一个元生成器，以捕捉模型演化中的长期序列模式。元生成器顺序输入前期历史模型的截断序列，然后将GRU的最终隐藏状态$\bm{h}_{t}$转换以生成当前模型$f_{\theta_{t}}$的参数。与SML类似，ASMG中的元生成器也通过对下一个时间段$t+1$的交互优化以提高性能。
- en: Different from SML which focuses on updating parameters based on the whole data
    in the current time, one up-to-date approach MeLON (Kim et al., [2022](#bib.bib45))
    further distinguishes the importance of different interactions in the data of
    the same time. Specifically, given an interaction $r$, MeLON aims to learn a adaptive
    learning rate $\alpha_{r,m}$ for $m$-th dimension $\theta_{t}^{m}$ of current
    model parameters $\theta_{t}$. A meta model is designed to generate the adaptive
    learning rate based on information from both the interaction (e.g., relevant historical
    interactions) and the parameter (e.g. loss and gradient). By assigning adaptive
    learning rates for each interaction-parameter pair, MeLON hopes to be able to
    update recommendation models more flexibly in online scenarios.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 与SML（侧重于基于当前时间的全部数据更新参数）不同，最新的方法MeLON (Kim et al., [2022](#bib.bib45))进一步区分了同一时间数据中不同交互的重要性。具体而言，给定一个交互$r$，MeLON旨在为当前模型参数$\theta_{t}$的第$m$维$\theta_{t}^{m}$学习一个自适应学习率$\alpha_{r,m}$。一个元模型被设计用来基于交互（例如相关历史交互）和参数（例如损失和梯度）信息生成自适应学习率。通过为每个交互-参数对分配自适应学习率，MeLON希望能够在在线场景中更灵活地更新推荐模型。
- en: Besides the model-based meta-learning techniques above, model retraining is
    also studied under the optimization-based meta-learning framework. Xie et al.
    (Xie et al., [2021](#bib.bib112)) propose LSSTM for online recommendation, which
    relies on graph neural networks based recommendation models to extract user short-term
    and long-term preferences. Considering the dynamic nature of short-term preferences
    in online scenarios, LSTTM constructs model retraining tasks according to different
    time periods and applies optimization-based meta-learning to learn better initialization
    of a short-term graph module. Instead of training only based on current data with
    meta-learning, the global long-term graph module is trained constantly during
    the whole online learning phase. In this way, short-term preference for new trends
    or hot topics is captured timely from the recent interactions while long-term
    preference which reflects users’ stable interests is also maintained after the
    model retraining.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述基于模型的元学习技术，模型重训练也在基于优化的元学习框架下进行研究。Xie et al. (Xie et al., [2021](#bib.bib112))提出了用于在线推荐的LSSTM，该方法依赖于图神经网络基于的推荐模型来提取用户的短期和长期偏好。考虑到在线场景中短期偏好的动态特性，LSTTM根据不同时间段构建模型重训练任务，并应用基于优化的元学习来学习短期图模块的更好初始化。与仅基于当前数据进行元学习不同，整个在线学习阶段中全局长期图模块持续训练。这样，能够及时捕捉到来自最近交互的新趋势或热门话题的短期偏好，同时也维护了模型重训练后的长期偏好，这反映了用户稳定的兴趣。
- en: 5.4\. Meta-learning in Point of Interest Recommendation
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 兴趣点推荐中的元学习
- en: 'As shown in Table [9](#S5.T9 "Table 9 ‣ 5.4\. Meta-learning in Point of Interest
    Recommendation ‣ 5\. Meta-leanring Methods for Recommendation Systems ‣ Deep Meta-learning
    in Recommendation Systems: A Survey"), we summarize meta-learning based methods
    in POI recommendation from three perspectives, i.e., task division and sequential
    information, and meta-knowledge representations. Next, we will elaborate on two
    groups of methods that study optimization-based sample reweighting and optimization-based
    parameter initialization, respectively.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[9](#S5.T9 "表 9 ‣ 5.4\. 兴趣点推荐中的元学习 ‣ 5\. 推荐系统的元学习方法 ‣ 推荐系统中的深度元学习：综述")所示，我们从任务划分、序列信息和元知识表示三个角度总结了基于元学习的方法。接下来，我们将详细讨论两组方法，分别研究基于优化的样本重加权和基于优化的参数初始化。
- en: Table 9\. Details of recommendation models with meta-learning methods in POI
    recommendation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9\. 包含元学习方法的兴趣点推荐模型详情。
- en: '| Method | Task Division | Sequential Information | Meta-knowledge representation
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 任务划分 | 序列信息 | 元知识表示 |'
- en: '| PREMERE (Kim et al., [2021](#bib.bib44)) | User-specific | Sequential-free
    | Meta model & Sample Weight |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| PREMERE (Kim et al., [2021](#bib.bib44)) | 用户特定 | 无序列感知 | 元模型 & 样本权重 |'
- en: '| MFNP (Sun et al., [2021b](#bib.bib92)) | User-specific | Sequential-aware
    | Parameter Initialization |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| MFNP (Sun et al., [2021b](#bib.bib92)) | 用户特定 | 序列感知 | 参数初始化 |'
- en: '| CHAML (Chen et al., [2021b](#bib.bib8)) | City-specific | Sequential-aware
    | Parameter Initialization & Sample Weight |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| CHAML (Chen et al., [2021b](#bib.bib8)) | 城市特定 | 序列感知 | 参数初始化与样本权重 |'
- en: '| Meta-SKR (Cui et al., [2021](#bib.bib12)) | User-specific | Sequential-aware
    | Parameter Initialization & Meta Model |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Meta-SKR（Cui 等，[2021](#bib.bib12)） | 用户特定 | 顺序感知 | 参数初始化与元模型 |'
- en: '| MetaODE (Tan et al., [2021](#bib.bib98)) | City-specific | Sequential-aware
    | Parameter Initialization |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| MetaODE（Tan 等，[2021](#bib.bib98)） | 城市特定 | 顺序感知 | 参数初始化 |'
- en: Optimization-based Sample Reweighting. Due to the sparse and noisy nature of
    check-in data, it is beneficial to assign higher weights to effective instances
    for better model training. Considering that harder tasks have higher values for
    boosting model performance, Chen et al. (Chen et al., [2021b](#bib.bib8)) proposes
    a meta-learning framework CHAML for net POI recommendation which incorporates
    hardness-aware sampling into optimization-based meta-learning. This work focuses
    on extracting meta-knowledge from existing cities with sufficient data to cold-start
    cities with limited check-in instances. By treating POI recommendation in each
    city as a task, CHAML extends the MAML framework to learn the initial weights
    of an attention-based sequential recommendation model in order to quickly adapt
    to cold-start cities. For enhancing the efficiency of model training, the idea
    of hardness-aware sampling is to sample difficult tasks which have low accuracies.
    Specifically, the batch of training tasks are not sampled randomly but conditioned
    on the difficulties of different users and different cities. When generating each
    task batch, both city-level hardness and user-level hardness are considered via
    two sampling steps. For the first step, with a group of hard tasks $\mathcal{T}_{hard\_city}$,
    some hardest users with the lowest prediction accuracies are kept and others are
    re-sampled to form a new batch of tasks $\mathcal{T}_{hard\_user}$ with harder
    users. For the second step, a step of the global update is performed over $\mathcal{T}_{hard\_user}$
    and then another batch of tasks $\mathcal{T}_{hard\_city}$ are constructed by
    keeping some harder cities with lower accuracies and resampling others. In addition,
    curriculum learning is adopted to measure city-level difficulties with a pretrained
    teacher so as to generate an easy-to-hard training curriculum.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优化的样本重标定。由于签到数据的稀疏和噪声特性，将更高的权重分配给有效实例以提高模型训练效果是有益的。考虑到更困难的任务具有提升模型性能的更高价值，Chen
    等（Chen 等，[2021b](#bib.bib8)）提出了一种元学习框架 CHAML 用于网络 POI 推荐，该框架将困难感知采样融入基于优化的元学习中。这项工作专注于从已有数据充分的城市中提取元知识，以冷启动数据有限的城市。通过将每个城市的
    POI 推荐视为一个任务，CHAML 扩展了 MAML 框架，以学习基于注意力的顺序推荐模型的初始权重，从而快速适应冷启动城市。为了提高模型训练的效率，困难感知采样的思想是采样那些准确率较低的困难任务。具体而言，训练任务的批次不是随机采样的，而是根据不同用户和不同城市的难度进行条件采样。在生成每个任务批次时，通过两个采样步骤考虑城市级和用户级的难度。在第一步中，针对一组困难任务
    $\mathcal{T}_{hard\_city}$，保留一些预测准确率最低的最困难用户，其余用户则重新采样，以形成一个包含更困难用户的新任务批次 $\mathcal{T}_{hard\_user}$。在第二步中，对
    $\mathcal{T}_{hard\_user}$ 进行全局更新，然后通过保留一些准确率较低的更困难城市并重新采样其他城市来构建另一个任务批次 $\mathcal{T}_{hard\_city}$。此外，采用课程学习通过预训练教师来测量城市级难度，从而生成从易到难的训练课程。
- en: Another work PREMERE proposes an adaptive reweighting scheme based on model-based
    meta-learning in the POI recommendation problem. A meta model $\mathcal{F}_{\omega}$
    is designed to generate sample weights which induce the learning phase of the
    recommendation model to focus more on valuable samples. The generated weight $w_{i}=\mathcal{F}_{\omega}(\bm{x}_{i})$
    is utilized as the weight of loss summation during recommendation model training.
    Specifically, $\bm{x}_{i}$ represents the context of a sample (e.g., user visit
    entropy, geographical similarity, and temporal similarity) and its loss obtained
    by the recommendation model. In this way, samples justified as more effective
    for model training could be adaptively assigned higher weights. Different from
    CHAML which evaluates the importance of samples during the sampling phase, PREMERE
    randomly sample instances but focuses on reweighting losses of instances in the
    sampled batch.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Another work PREMERE proposes an adaptive reweighting scheme based on model-based
    meta-learning in the POI recommendation problem. A meta model $\mathcal{F}_{\omega}$
    is designed to generate sample weights which induce the learning phase of the
    recommendation model to focus more on valuable samples. The generated weight $w_{i}=\mathcal{F}_{\omega}(\bm{x}_{i})$
    is utilized as the weight of loss summation during recommendation model training.
    Specifically, $\bm{x}_{i}$ represents the context of a sample (e.g., user visit
    entropy, geographical similarity, and temporal similarity) and its loss obtained
    by the recommendation model. In this way, samples justified as more effective
    for model training could be adaptively assigned higher weights. Different from
    CHAML which evaluates the importance of samples during the sampling phase, PREMERE
    randomly sample instances but focuses on reweighting losses of instances in the
    sampled batch.
- en: Optimization-based Parameter Initialization. Recently, optimization-based meta-learning
    methods are also leveraged to learn parameter initialization of specific modules
    in the next POI recommendation models. Sun et al. (Sun et al., [2021b](#bib.bib92))
    propose MFNP, which captures user-specific preferences and region-specific preferences
    with two LSTM-based modeling modules, respectively. By initializing the parameters
    of the recommendation model, MFNP locally updates models on corresponding support
    sets for different users and globally optimizes the initialization via the MAML
    framework. Another work (Cui et al., [2021](#bib.bib12)) proposes a sequential
    knowledge graph based recommendation model Meta-SKR for the next POI recommendation.
    By jointly modeling sequential, geographical, temporal, and social information
    with designed sequential knowledge graphs, the next POI recommendation problem
    is considered as a link prediction based on graph embedding learning. To alleviate
    the check-in sparsity problem in embedding learning, an optimization-based meta-learning
    framework LEO (Rusu et al., [2018](#bib.bib82)) is adopted to generate the weights
    of the GRU-based and GAT-based sequential embedding network which learns node
    embeddings from the sequential knowledge graphs. In addition, optimization-based
    meta-learning is also utilized in MetaODE (Tan et al., [2021](#bib.bib98)) to
    learn parameter initialization across multiple source cities with sufficient data,
    so as to gain better generalization over data-insufficient cities.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优化的参数初始化。最近，也开始利用基于优化的元学习方法来学习下一个POI推荐模型中特定模块的参数初始化。Sun等人（Sun et al., [2021b](#bib.bib92)）提出了MFNP，该方法利用两个基于LSTM的建模模块分别捕捉用户特定偏好和区域特定偏好。通过初始化推荐模型的参数，MFNP在不同用户的相应支持集上进行局部模型更新，并通过MAML框架全局优化初始化。另一项工作（Cui
    et al., [2021](#bib.bib12)）提出了基于顺序知识图的推荐模型Meta-SKR，用于下一个POI推荐。通过设计顺序知识图模型联合建模顺序、地理、时间和社交信息，将下一个POI推荐问题视为基于图嵌入学习的链接预测问题。为了减轻嵌入学习中的签到稀疏性问题，采用了基于优化的元学习框架LEO（Rusu
    et al., [2018](#bib.bib82)）来生成基于GRU和GAT的顺序嵌入网络的权重，该网络从顺序知识图中学习节点嵌入。此外，基于优化的元学习还在MetaODE（Tan
    et al., [2021](#bib.bib98)）中被用于学习跨多个具有足够数据的源城市的参数初始化，以获得对数据不足城市更好的泛化能力。
- en: 5.5\. Meta-learning in Sequential Recommendation
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 顺序推荐中的元学习
- en: Sequential recommendation mainly focuses on modeling user behavior sequences
    to capture the dynamic evolution of user preferences. Several recent studies incorporate
    meta-learning to alleviate the cold-start issues in sequential recommendation
    scenarios (Huang et al., [2022](#bib.bib38); Song et al., [2021](#bib.bib90);
    Wang et al., [2021a](#bib.bib103); Zheng et al., [2021](#bib.bib126)).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序推荐主要关注建模用户行为序列，以捕捉用户偏好的动态演化。近期的研究将元学习应用于顺序推荐场景，以缓解冷启动问题（Huang et al., [2022](#bib.bib38);
    Song et al., [2021](#bib.bib90); Wang et al., [2021a](#bib.bib103); Zheng et al.,
    [2021](#bib.bib126)）。
- en: To tackle the data sparseness issues of new users, HUANG et al. (Huang et al.,
    [2022](#bib.bib38)) propose a cold-start sequential recommendation model metaCSR
    to learn global inItialization of a sequential recommender with the MAML framework.
    The sequential recommender is designed to have a GCN-based representation learning
    module for learning user and item representations and a self-attention based sequential
    modeling module for encoding user interaction sequences. The MAML framework is
    leveraged to globally learn parameters of the sequential recommender across different
    sequential recommendation tasks. Each task utilizes the first $K_{1}$ interactions
    in the user behavior sequence of a user as the support set and the rest $K_{2}$
    interactions as the query set. For each interaction, the sequential recommender
    relies on a historical interaction sequence to predict the current item.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对新用户数据稀疏性问题，HUANG 等人（Huang 等人，[2022](#bib.bib38)）提出了一种冷启动序列推荐模型 metaCSR，以使用
    MAML 框架学习序列推荐器的全局初始化。序列推荐器被设计为具有基于 GCN 的表示学习模块，用于学习用户和项目表示，以及基于自注意力的序列建模模块，用于编码用户交互序列。MAML
    框架被利用来在不同的序列推荐任务中全局学习序列推荐器的参数。每个任务利用用户行为序列中的前 $K_{1}$ 次交互作为支持集，其余的 $K_{2}$ 次交互作为查询集。对于每次交互，序列推荐器依赖历史交互序列来预测当前项目。
- en: Similarly, another work CBML (Song et al., [2021](#bib.bib90)) also applies
    optimization-based meta-learning into self-attention based sequential recommendation
    models. CBML utilizes two self-attention layers to learn sequential transition
    patterns at both the item level and the feature level. Based on the base sequential
    recommendation model above, a cluster-based meta-learning framework is designed
    to transfer meta-knowledge shared across similar sequential/session-based tasks.
    Specifically, CBML adaptively learns a soft-clustering assignment for each task
    which is constructed with a session and generates parameter gates to guide cluster-aware
    initialization of the base sequential recommendation model. Here, CBML simply
    tailors cluster-aware initialization of a prediction layer and assigns global
    initialization for the rest modules of the sequential recommendation model including
    embedding layers and self-attention layers.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，另一项工作 CBML（Song 等人，[2021](#bib.bib90)）也将基于优化的元学习应用于自注意力基础的序列推荐模型。CBML 利用两个自注意力层在项目级别和特征级别上学习序列转换模式。基于上述基础序列推荐模型，设计了一个基于集群的元学习框架，以转移在类似序列/会话任务中共享的元知识。具体而言，CBML
    自适应地为每个任务学习一个软集群分配，该任务由一个会话构建，并生成参数门以引导基础序列推荐模型的集群感知初始化。在这里，CBML 简单地调整了预测层的集群感知初始化，并为序列推荐模型的其他模块（包括嵌入层和自注意力层）分配全局初始化。
- en: Instead of learning sequential patterns with self-attention models, Wang et
    al.(Wang et al., [2021a](#bib.bib103)) propose a MetaTL framework on top of a
    transition-based sequential recommendation architecture. To capture short-range
    transition dynamics from sequences with limited interactions of cold-start users,
    MetaTL resorts to the idea of transition-based recommendation. The sequential
    recommendation task for a cold-start user is formulated to predict the tail item
    $i_{t+1}$ in a transition pair $\{i_{t}\rightarrow i_{t+1}\}$ (i.e., the query
    set) given previous transition pairs $\{i_{j}\rightarrow i_{j+1}\}_{j=1}^{t-1}$
    (i.e., the support set). The transition-based recommendation model aggregates
    the trainstional information of the user $u_{i}$ based on multiple pairs in the
    support set to obtain a relation representation $\bm{r}_{u_{i}}$ and calculates
    the preference score as $-||\bm{i}_{t}+\bm{r}_{u_{i}}-\bm{i}_{t+1}||^{2}$. MetaTL
    also applies MAML framwork to learn effective global intialization of the transition
    model for all cold-start users.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 与其使用自注意力模型学习序列模式，不如 Wang 等人（Wang 等人，[2021a](#bib.bib103)）在基于转换的序列推荐架构上提出了一个
    MetaTL 框架。为了捕捉来自冷启动用户有限交互的序列中的短期转换动态，MetaTL 采用了基于转换的推荐思想。冷启动用户的序列推荐任务被制定为预测在转换对
    $\{i_{t}\rightarrow i_{t+1}\}$ 中的尾部项目 $i_{t+1}$（即查询集），给定先前的转换对 $\{i_{j}\rightarrow
    i_{j+1}\}_{j=1}^{t-1}$（即支持集）。基于转换的推荐模型基于支持集中的多个对聚合用户 $u_{i}$ 的训练信息，以获得关系表示 $\bm{r}_{u_{i}}$，并计算偏好评分为
    $-||\bm{i}_{t}+\bm{r}_{u_{i}}-\bm{i}_{t+1}||^{2}$。MetaTL 还应用了 MAML 框架，以学习所有冷启动用户的转换模型的有效全局初始化。
- en: Different from applying optimization-based meta-learning to learn suitable initialization
    of sequential model, metric-based meta-learning is also studied in the cold-start
    sequential recommendation scenario. Zheng et al.(Zheng et al., [2021](#bib.bib126))
    propose Mecos to address the item cold-start issue in the sequential recommendation.
    They firstly construct $N$-way $K$-shot classification task by sampling K sequences
    for N cold-start items, respectively. Then, Mecos learns holistic representations
    for support sets and query sets of different items and leverages a matching network
    to calculate the similarity scores between each support and query pair, so as
    to generate classification results of the $N$ query sets according to the similarity
    metric. The matching network is optimized in the meta-training phase with constructed
    classification tasks and could be directedly utilized to make predictions without
    local adaptation over meta-testing tasks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 与将基于优化的元学习应用于学习序列模型的合适初始化不同，基于度量的元学习也在冷启动序列推荐场景中进行了研究。Zheng 等（Zheng 等，[2021](#bib.bib126)）提出了Mecos来解决序列推荐中的项目冷启动问题。他们首先通过对N个冷启动项目分别采样K个序列来构建$N$-way
    $K$-shot分类任务。然后，Mecos为不同项目的支持集和查询集学习整体表示，并利用匹配网络计算每个支持和查询对之间的相似度得分，从而根据相似度度量生成$N$个查询集的分类结果。匹配网络在元训练阶段通过构建的分类任务进行优化，并可直接用于预测，而无需在元测试任务上进行本地调整。
- en: 5.6\. Meta-learning in Cross Domain Recommendation
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6. 跨领域推荐中的元学习
- en: Cross-domain Recommendation (CDR) which aims to transfer knowledge from an informative
    source domain to the target domain is a promising solution to alleviate the cold-start
    problem. Several studies (Zhu et al., [2021a](#bib.bib130), [c](#bib.bib133))
    introduce meta-learning into cross-domain recommendation methods to achieve better
    knowledge transfer under cross-domain settings by extracting prior knowledge.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域推荐（CDR）旨在将知识从信息丰富的源领域转移到目标领域，是缓解冷启动问题的一个有前景的解决方案。一些研究（Zhu 等，[2021a](#bib.bib130)，[c](#bib.bib133)）将元学习引入跨领域推荐方法，通过提取先验知识来实现跨领域环境下更好的知识转移。
- en: Under the framework of Embedding and Mapping methods for CDR (EMCDR (Man et al.,
    [2017](#bib.bib63))) which explicitly learns representation mapping function based
    on overlapping users, Zhu et al. (Zhu et al., [2021a](#bib.bib130)) propose a
    transfer-meta framework TMCDR to enhance the training process of EMCDR-based methods.
    Specifically, similar to the embedding step in the general EMCDR framework, TMCDR
    firstly learns domain-specific embedding models for both source and target domains,
    respectively. The idea of meta-learning is utilized in a meta stage, which trains
    a meta network to transform source embeddings into the target feature space. In
    the meta stage, TMCDR samples two groups of overlapping users to construct meta-training
    tasks which utilize one group as a support set and another group as a query set.
    The meta network is optimized across tasks under the framework of optimization-based
    meta-learning. Compared with the original mapping function of EMCDR, the meta
    network is supposed to have better generalization when transforming user embeddings
    from a source domain for cold-start users in the target domain.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在CDR的嵌入与映射方法框架（EMCDR (Man 等，[2017](#bib.bib63)）中，该框架显式地基于重叠用户学习表示映射函数，Zhu 等（Zhu
    等，[2021a](#bib.bib130)）提出了一个转移元框架TMCDR，以增强基于EMCDR的方法的训练过程。具体而言，类似于一般EMCDR框架中的嵌入步骤，TMCDR首先分别为源领域和目标领域学习领域特定的嵌入模型。元学习的思想在元阶段中被利用，该阶段训练一个元网络，将源领域的嵌入转化为目标特征空间。在元阶段，TMCDR从两个重叠用户组中采样，以构建元训练任务，其中一个组用作支持集，另一个组用作查询集。元网络在基于优化的元学习框架下跨任务进行优化。与原始的EMCDR映射函数相比，元网络在将源领域的用户嵌入转化为目标领域的冷启动用户时应具有更好的泛化能力。
- en: Instead of applying an optimization-based framework, another work PTUPCDR (Zhu
    et al., [2021c](#bib.bib133)) proposes to directly generate user-personalized
    bridge functions with a meta network. Following the similar idea of mapping-based
    knowledge transfer, PTUPCDR also focuses on transferring user preferences from
    an informative source domain to a sparse target domain. Different from learning
    a common mapping function for all users, this work considers that the preference
    transfer should be personalized. Specifically, for a user $u_{i}$, the personalized
    parameters $\bm{w}_{u_{i}}$ of the mapping function are generated with a meta
    network, in order to transform a user embedding in the source domain $\bm{u}^{s}_{i}$
    to an initial user embedding in the target domain $\bm{u}^{t}_{i}$. The meta network
    takes representations of users’ personalized characteristics which are extracted
    from user interactions in the source domain as inputs and generates $\bm{w}_{u_{i}}$
    as the parameters of a mapping function. By applying the personalized mapping
    function on the embedding transfer for the user $u_{i}$, $\bm{u}^{t}_{i}$ could
    be utilized for predictions on the target domain. The optimization procedure across
    different cross-domain recommendation tasks enables the meta network to learn
    the meta-knowledge about personalized parameter generation.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 与其应用基于优化的框架，另一项工作 PTUPCDR (Zhu et al., [2021c](#bib.bib133)) 提出了通过元网络直接生成用户个性化的桥接函数。沿用基于映射的知识转移的类似思想，PTUPCDR
    也专注于将用户偏好从信息丰富的源领域转移到稀疏的目标领域。与为所有用户学习一个通用映射函数不同，这项工作认为偏好转移应该是个性化的。具体来说，对于用户 $u_{i}$，映射函数的个性化参数
    $\bm{w}_{u_{i}}$ 是通过元网络生成的，以将源领域中的用户嵌入 $\bm{u}^{s}_{i}$ 转换为目标领域中的初始用户嵌入 $\bm{u}^{t}_{i}$。元网络将从源领域中用户交互中提取的用户个性化特征表示作为输入，并生成
    $\bm{w}_{u_{i}}$ 作为映射函数的参数。通过在用户 $u_{i}$ 的嵌入转移中应用个性化映射函数，$\bm{u}^{t}_{i}$ 可以用于目标领域的预测。跨不同跨领域推荐任务的优化过程使得元网络能够学习关于个性化参数生成的元知识。
- en: 5.7\. Meta-learning in other Recommendation Scenarios
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7\. 其他推荐场景中的元学习
- en: Besides the recommendation scenarios mentioned above, We will briefly discuss
    some typical scenarios else, including Multi-behavior recommendation, Knowledge
    graph based recommendation, and Recommendation Model Selection. Other sporadic
    works involving federated recommendation (Lin et al., [2020](#bib.bib55)), size
    and fit recommendation (Lasserre et al., [2020](#bib.bib47)), audience expansion
    in recommendation (Zhu et al., [2021b](#bib.bib132)) and interactive recommendation
    (Zou et al., [2020](#bib.bib136)) will not be presented in detail.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述推荐场景外，我们还将简要讨论一些其他典型场景，包括多行为推荐、基于知识图谱的推荐和推荐模型选择。其他涉及联邦推荐 (Lin et al., [2020](#bib.bib55))、尺寸和适配推荐
    (Lasserre et al., [2020](#bib.bib47))、推荐中的受众扩展 (Zhu et al., [2021b](#bib.bib132))
    和互动推荐 (Zou et al., [2020](#bib.bib136)) 的零星工作将不作详细介绍。
- en: 5.7.1\. Multi-behavior recommendation
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.1\. 多行为推荐
- en: Multiple types of user behaviors (e.g., click, add-to-cart and purchase) are
    considered to be able to reflect multi-view user preferences in real-world scenarios.
    Multi-behavior recommendation aims to capture multi-typed behavior patterns and
    comprehensively learn users’ preferences from their diverse behaviors.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 多种类型的用户行为（例如点击、加入购物车和购买）被认为能够反映现实场景中的多视角用户偏好。多行为推荐旨在捕捉多种类型的行为模式，并从用户的多样化行为中全面学习用户的偏好。
- en: Although previous studies have made efforts to learn complex dependencies among
    different types of behaviors, two recent works MB-GMN (Xia et al., [2021](#bib.bib111))
    and CML (Wei et al., [2022](#bib.bib109)) argue that the multi-behavior patterns
    should be diverse and personalized for different users. Therefore, both of them
    study the multi-behavior recommendation problem with the meta-learning paradigm.
    Specifically, by applying model-based meta-learning, MB-GMN designs two meta networks
    to directly generate personalized parameters of different users for both a multi-behavior
    pattern representation learning module and a prediction module. The former meta-network
    generates personalized weights of behavior-specific context projection layers
    by taking user-specific behavior characteristics as input. The latter meta-network
    generates personalized parameters of final prediction networks by encoding the
    target user-item pair as the state representation of the current instance. Following
    the similar idea of model-based parameter generation, CML leverages a meta weight
    network to generate personalized weights for integrating contrastive losses in
    different behavior views. By generating the weighting function based on user-specific
    behavior characteristics, the meta weight network is designed to adaptively customize
    the contrastive learning phase for different users.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以往的研究已经努力学习不同类型行为之间的复杂依赖关系，但最近的两项工作 MB-GMN（Xia et al., [2021](#bib.bib111)）和
    CML（Wei et al., [2022](#bib.bib109)）认为多行为模式应针对不同用户具有多样性和个性化。因此，这两项研究都采用了元学习范式来研究多行为推荐问题。具体而言，通过应用基于模型的元学习，MB-GMN
    设计了两个元网络，直接生成不同用户的个性化参数，用于多行为模式表示学习模块和预测模块。前一个元网络通过以用户特定的行为特征作为输入，生成行为特定上下文投影层的个性化权重。后一个元网络通过将目标用户-项目对编码为当前实例的状态表示，生成最终预测网络的个性化参数。沿用类似的基于模型的参数生成思想，CML
    利用一个元权重网络生成用于整合不同行为视图中的对比损失的个性化权重。通过基于用户特定行为特征生成加权函数，元权重网络旨在为不同用户自适应地定制对比学习阶段。
- en: 5.7.2\. Knowledge Graph based Recommendation
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.2\. 基于知识图谱的推荐
- en: To tackle the cold-start problem in knowledge graph based recommendation, Du
    et al. (Du et al., [2022](#bib.bib17)) firstly attempt to incorporate an optimization-based
    meta-learning paradigm to simultaneously derive prior knowledge from both collaborative
    information in interactions and semantic information in knowledge graphs. Specifically,
    a graph attention network based recommendation model MetaKG, which aggregates
    information of neighboring entities in a collaborative knowledge graph to learn
    user and item representations, is utilized as the base model. Then the parameters
    of the base model are optimized through an optimization-based meta-learning schema.
    Specifically, the parameters of the base model are divided into a knowledge-aware
    part and another collaborative-aware part and optimized in different strategies.
    For the knowledge-aware part involving entity representation learning, parameters
    are globally optimized to learn shared semantic information of the whole knowledge
    graph. Differently, the collaborative-aware part involving preference aggregation
    is first locally adapted to each task and then globally optimized across different
    tasks, so as to ensure fast adaptation over cold-start users by learning effective
    global initialization.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决基于知识图谱的推荐中的冷启动问题，Du 等（Du et al., [2022](#bib.bib17)）首先尝试结合基于优化的元学习范式，以同时从交互中的协同信息和知识图谱中的语义信息中推导先验知识。具体而言，使用一个基于图注意力网络的推荐模型
    MetaKG，该模型聚合了协同知识图谱中邻近实体的信息以学习用户和项目的表示，作为基础模型。然后，通过基于优化的元学习方案优化基础模型的参数。具体地，基础模型的参数分为知识感知部分和协同感知部分，并采用不同的策略进行优化。涉及实体表示学习的知识感知部分，参数会被全局优化以学习整个知识图谱的共享语义信息。不同的是，涉及偏好聚合的协同感知部分首先在每个任务上进行局部适应，然后在不同任务之间进行全局优化，从而通过学习有效的全局初始化来确保对冷启动用户的快速适应。
- en: 5.7.3\. Recommendation Model Selection
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.3\. 推荐模型选择
- en: In practical recommendation systems, a single model is unlikely to always achieve
    the best performance over every dataset (Cunha et al., [2016](#bib.bib13)) or
    every user (Luo et al., [2020](#bib.bib60)). Recommendation model selection is
    a realistic solution, which aims to suitably select or combine different recommendation
    models in different scopes by discovering relationships between data characteristics
    and model performance. In previous works (Cunha et al., [2016](#bib.bib13); Prudêncio
    and Ludermir, [2004](#bib.bib74); Rossi et al., [2014](#bib.bib81)), meta-learning
    has been understood as a kind of methodology that extracts diverse forms of meta-features
    from given datasets and induces meta models to predict the best recommendation
    model based on these meta-features. This line of methods heavily relies on manual
    extraction of meta-features and thus is out of the range of deep meta-learning
    that we discussed in this survey. More related works could be found in (Ren et al.,
    [2019](#bib.bib79); Cunha et al., [2018](#bib.bib14)).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的推荐系统中，单一模型不太可能在每个数据集（Cunha et al., [2016](#bib.bib13)）或每个用户（Luo et al.,
    [2020](#bib.bib60)）上始终达到最佳性能。推荐模型选择是一种现实的解决方案，旨在通过发现数据特征与模型性能之间的关系，适当地选择或组合不同的推荐模型。在之前的工作中（Cunha
    et al., [2016](#bib.bib13)；Prudêncio 和 Ludermir, [2004](#bib.bib74)；Rossi et al.,
    [2014](#bib.bib81)），元学习被理解为一种方法论，该方法从给定的数据集中提取各种形式的元特征，并引入元模型以基于这些元特征预测最佳推荐模型。这类方法严重依赖于手动提取元特征，因此超出了我们在本调查中讨论的深度元学习的范围。更多相关工作可参见（Ren
    et al., [2019](#bib.bib79)；Cunha et al., [2018](#bib.bib14)）。
- en: Recently, Luo et al.(Luo et al., [2020](#bib.bib60)) have studied recommendation
    model selection problem under the framework of optimization-based meta-learning.
    Given a collection of recommendation models, a model selector MetaSelector is
    designed to adaptively ensemble all models by generating soft selection weights.
    By regarding each task as learning suitable model selection weights for a user,
    the model selector is optimized across different model selection tasks under an
    adaptive learning rate augmented MAML framework. In the local adaptation phase,
    for each task, the model selector is first locally updated with the support set
    of the user and then generate personalized model selection weights to evaluate
    its effectiveness over the query set. In the global optimization phase, the initialization
    of the model selector is updated across multiple tasks to make sure fast adaptation
    to new model selection tasks. Note that these recommendation models should be
    pretrained with all data and kept fixed in the meta-training phase.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Luo et al.（Luo et al., [2020](#bib.bib60)）在基于优化的元学习框架下研究了推荐模型选择问题。给定一组推荐模型，设计了一个模型选择器
    MetaSelector，以通过生成软选择权重自适应地集成所有模型。通过将每个任务视为学习适合用户的模型选择权重，模型选择器在增强的 MAML 框架下对不同模型选择任务进行优化。在局部适应阶段，对于每个任务，模型选择器首先使用用户的支持集进行局部更新，然后生成个性化的模型选择权重，以评估其在查询集上的有效性。在全局优化阶段，模型选择器的初始化在多个任务中更新，以确保快速适应新的模型选择任务。注意，这些推荐模型应使用所有数据进行预训练，并在元训练阶段保持不变。
- en: 6\. Future Directions
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 未来方向
- en: In this section, we analyze the limitations of existing deep meta-learning based
    recommendation methods and outline some prospective research directions which
    worth exploring in the future.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了现有深度元学习基础的推荐方法的局限性，并概述了一些值得未来探索的研究方向。
- en: 6.1\. Meta-Overfitting
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 元学习过拟合
- en: Generalization across different tasks is the key capacity of meta-learning,
    and it mainly depends on how well meta-learners fit the whole task distribution
    with meta-training tasks. Similar to overfitting over training instances in conventional
    machine learning, the meta-overfitting issue occurs when meta-learners merely
    memorize all meta-training tasks but fail to adapt to novel tasks (i.e, meta-testing
    tasks) (Yin et al., [2019](#bib.bib117)). Since the number of training tasks is
    usually much smaller than the number of instances, the meta-overfitting problem
    is more severe in meta-learning compared with regular supervised learning (Hospedales
    et al., [2020](#bib.bib37)). In the field of recommendation systems, existing
    meta-learning methods mainly construct a fixed and limited number of tasks as
    summarized in section 4, and thus are likely to suffer from meta-overfitting over
    meta-training tasks. One straightforward strategy against meta-overfitting is
    conducting task augmentation during task construction. For instance, for constructing
    typical few-shot classification tasks, $N$ classes are randomly sampled and $K$
    instances of each class are also randomly sampled. In this way, not only the volume
    of available tasks is greatly increased, but also these tasks are kept mutually
    exclusive. Some other efforts of task augmentation (Zhu et al., [2022](#bib.bib129);
    Liu et al., [2020a](#bib.bib57); Murty et al., [2021](#bib.bib66)), meta-regularization
    (Yin et al., [2019](#bib.bib117)) and Bayesian meta-learning (Yoon et al., [2018](#bib.bib119))
    are also studied and proven effective in addressing the meta-overfitting issue.
    Therefore, it is a promising direction for developing meta-learning based recommendation
    models with better meta-generalization abilities.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 跨任务的泛化能力是元学习的关键能力，这主要依赖于元学习者如何通过元训练任务来拟合整个任务分布。类似于传统机器学习中对训练实例的过拟合，元过拟合问题发生在元学习者仅仅记住所有元训练任务而无法适应新的任务（即元测试任务）时（Yin
    等，[2019](#bib.bib117)）。由于训练任务的数量通常远小于实例的数量，元过拟合问题在元学习中比在常规监督学习中更为严重（Hospedales
    等，[2020](#bib.bib37)）。在推荐系统领域，现有的元学习方法主要构建一个固定且有限数量的任务，如第4节所总结的，因此可能会遭遇对元训练任务的元过拟合。对抗元过拟合的一种直接策略是在任务构建过程中进行任务增强。例如，构建典型的少样本分类任务时，$N$
    类别是随机采样的，每个类别的 $K$ 个实例也随机采样。通过这种方式，不仅可用任务的数量大大增加，而且这些任务保持互斥。其他一些任务增强的努力（Zhu 等，[2022](#bib.bib129)；Liu
    等，[2020a](#bib.bib57)；Murty 等，[2021](#bib.bib66)），元正则化（Yin 等，[2019](#bib.bib117)）和贝叶斯元学习（Yoon
    等，[2018](#bib.bib119)）也已被研究并证明在解决元过拟合问题上有效。因此，开发具有更好元泛化能力的基于元学习的推荐模型是一个有前途的方向。
- en: 6.2\. Task Heterogeneity
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 任务异质性
- en: The majority of meta-learning methods adopted in recommendation models mainly
    focus on globally learning meta-knowledge across different tasks without considering
    the task heterogeneity problem. However, globally learned meta-learners usually
    perform well when task distribution is uni-modal, but lack the ability to provide
    desirable prior knowledge to heterogeneous tasks from the multi-modal distribution
    (Vuorio et al., [2019](#bib.bib102)). Considering the huge differences from the
    perspectives of both user interests and item attributes in recommender systems,
    the distributions of user-specific tasks or item-specific tasks are often complex.
    Moreover, different from image or NLP tasks, the distribution of recommendation
    tasks shows strong dynamics with time evolving. Therefore, properly handling the
    task heterogeneity is essential for learning high-quality meta-knowledge across
    different tasks. In recommendation systems, several recent works (Dong et al.,
    [2020](#bib.bib16); Lin et al., [2021](#bib.bib54); Wang et al., [2021b](#bib.bib106))
    have explored the task heterogeneity issue under user cold-start scenarios. They
    mainly trigger user-specific adjustments to the globally shared knowledge (e.g.
    initialization or parameters modulation) conditioned on the user profile information
    or interaction information. On this basis, more efforts on how to effectively
    distinguish different tasks under diverse task distributions are desired. Recent
    research resorts to more reasonable task clustering structures, such as hierarchical
    structure (Yao et al., [2019a](#bib.bib114)) and meta-knowledge graph (Yao et al.,
    [2019b](#bib.bib115)), to capture complex relations between tasks. In addition,
    external domain knowledge (e.g., knowledge graphs on the item side or social networks
    on the user side) also could be incorporated to facilitate identifying task relationships
    (Suo et al., [2020](#bib.bib97)). Besides, task heterogeneity in other recommendation
    scenarios such as online recommendation and POI recommendation is also worth exploring.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 目前大多数推荐模型中采用的元学习方法主要集中于全球性地学习不同任务间的元知识，而没有考虑任务异质性问题。然而，当任务分布是单模态时，全球学习的元学习者通常表现良好，但在面对来自多模态分布的异质任务时缺乏提供理想先验知识的能力（Vuorio
    et al., [2019](#bib.bib102)）。鉴于推荐系统中用户兴趣和项目属性的巨大差异，用户特定任务或项目特定任务的分布通常是复杂的。此外，与图像或自然语言处理任务不同，推荐任务的分布随着时间演变显示出强烈的动态性。因此，妥善处理任务异质性对于在不同任务中学习高质量的元知识至关重要。在推荐系统中，最近的一些研究（Dong
    et al., [2020](#bib.bib16); Lin et al., [2021](#bib.bib54); Wang et al., [2021b](#bib.bib106)）已经在用户冷启动场景下探讨了任务异质性问题。它们主要在用户资料信息或交互信息的条件下，对全球共享知识（例如初始化或参数调节）进行用户特定的调整。在此基础上，更加有效区分不同任务在多样任务分布下的研究仍然是必要的。近期研究利用更合理的任务聚类结构，如层次结构（Yao
    et al., [2019a](#bib.bib114)）和元知识图谱（Yao et al., [2019b](#bib.bib115)），来捕捉任务之间的复杂关系。此外，外部领域知识（例如项目侧的知识图谱或用户侧的社交网络）也可以被纳入，以帮助识别任务关系（Suo
    et al., [2020](#bib.bib97)）。此外，其他推荐场景中的任务异质性，如在线推荐和兴趣点推荐，也值得进一步探讨。
- en: 6.3\. Task Augmentation with Auxiliary Information
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 任务增强与辅助信息
- en: Recent meta-learning based recommendation models mainly leverage interaction
    data as the information source to construct meta-learning tasks. In practical,
    the data in recommendation systems could be diverse and multi-modal. Data from
    other sources (e.g., knowledge base, social networks, user/item side information,
    cross-domain information) and different modalities (e.g., video, image, and text)
    could be incorporated to provide auxiliary information. Besides simply enhancing
    user/item representations by inputting auxiliary data into the base recommendation
    model, another possible strategy is to perform task augmentation with auxiliary
    information in order to enrich the context of tasks. One relevant work (Lu et al.,
    [2020](#bib.bib59)) is proposed to incorporate multifaceted semantic contexts
    into tasks by extending both support set and query set based on the item attribute
    information. From the user side, the user social network is also utilized to extract
    preference information from friends, which implicitly augment user-specific tasks
    (Wang et al., [2021b](#bib.bib106)). Therefore, we believe that developing new-type
    task construction beyond interaction data not only injects auxiliary information
    to alleviate data insufficiency issue but also provides motivation for designing
    novel meta-learning methods from the level of task construction. Meanwhile, in
    recommendation scenarios where own rich auxiliary information but have not yet
    been widely studied under the meta-learning paradigm, e.g., knowledge graph based
    recommendation, review-based recommendation, and cross-domain recommendation,
    it is necessary to design appropriate meta-learning tasks according to the characteristics
    of auxiliary information such as structural information, textual information,
    and cross-domain information.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的基于元学习的推荐模型主要利用交互数据作为信息来源来构建元学习任务。在实际应用中，推荐系统中的数据可能是多样的和多模态的。来自其他来源（例如，知识库、社交网络、用户/项目信息、跨领域信息）和不同模态（例如，视频、图像和文本）的数据可以被整合以提供辅助信息。除了通过将辅助数据输入基础推荐模型来简单地增强用户/项目信息外，另一种可能的策略是通过辅助信息进行任务扩展，以丰富任务的背景。相关的工作（Lu
    et al., [2020](#bib.bib59)）提出了通过扩展支持集和查询集来将多方面的语义上下文融入任务中，这些扩展基于项目属性信息。从用户方面来看，用户的社交网络也被用来从朋友那里提取偏好信息，这隐式地增强了用户特定的任务（Wang
    et al., [2021b](#bib.bib106)）。因此，我们认为，开发超越交互数据的新型任务构建不仅注入了辅助信息以缓解数据不足的问题，还提供了从任务构建层面设计新型元学习方法的动机。同时，在拥有丰富辅助信息但在元学习范式下尚未被广泛研究的推荐场景中，例如，基于知识图谱的推荐、基于评论的推荐和跨领域推荐，需要根据辅助信息的特点（如结构信息、文本信息和跨领域信息）设计适当的元学习任务。
- en: 6.4\. Neural Network Architecture Search for Reocmmendation Models
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4. 神经网络架构搜索用于推荐模型
- en: Neural network architecture search (NAS) (Elsken et al., [2019](#bib.bib19))
    is also a popular application where meta-learning techniques have been well studied
    in computer vision and natural language processing domains. Recent meta-learning
    based NAS methods mainly focus on learning meta-knowledge about specifying an
    architecture of a neural network for each task. For instance, one representative
    work (Liu et al., [2018](#bib.bib56)) designed a bilevel optimization to solve
    the network architecture search problem for image classification. As a result,
    task-specific neural architecture could be adapted to each task from a general
    meta-architecture. While meta-learning has been seen as a powerful solution for
    network architecture search for deep neural networks (Lian et al., [2019](#bib.bib53);
    Shaw et al., [2019](#bib.bib86); Ding et al., [2022](#bib.bib15); Kim et al.,
    [2018](#bib.bib43); Elsken et al., [2020](#bib.bib20)), the architecture search
    of neural recommendation models has not been well studied. The most relevant work
    (Luo et al., [2020](#bib.bib60)) is closer to the topic of meta-learning about
    recommendation model selection. For developing meta-learning based NAS for recommendation
    models, two key points are search space and search strategy. Considering the neural
    structure of popular recommendation models, the search space might involve FNN
    structures, RNN structures, CNN structures, and attention-based structures. As
    for the search strategy, both initial conditions of architectures (Lian et al.,
    [2019](#bib.bib53); Ding et al., [2022](#bib.bib15)) and meta-models for learning
    task-agnostic representations (Shaw et al., [2019](#bib.bib86)) are studied with
    meta-learning techniques. In addition, the mutual impact of structural connections
    and model weights is also proven beneficial to each other for better optimization
    (Ding et al., [2022](#bib.bib15)). Thus, designing a neural network architecture
    search framework to automatically specify recommendation models for different
    tasks or datasets could be another future direction.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构搜索（NAS）（Elsken 等，[2019](#bib.bib19)）也是一个流行的应用领域，其中元学习技术在计算机视觉和自然语言处理领域得到了很好的研究。最近基于元学习的NAS方法主要集中在学习关于为每个任务指定神经网络架构的元知识。例如，一项代表性工作（Liu
    等，[2018](#bib.bib56)）设计了一种双层优化来解决图像分类的网络架构搜索问题。结果是，任务特定的神经架构可以从一般的元架构中适应到每个任务。虽然元学习被视为深度神经网络网络架构搜索的强大解决方案（Lian
    等，[2019](#bib.bib53)；Shaw 等，[2019](#bib.bib86)；Ding 等，[2022](#bib.bib15)；Kim 等，[2018](#bib.bib43)；Elsken
    等，[2020](#bib.bib20)），但神经推荐模型的架构搜索尚未得到充分研究。最相关的工作（Luo 等，[2020](#bib.bib60)）更接近于关于推荐模型选择的元学习主题。为了开发基于元学习的NAS用于推荐模型，两个关键点是搜索空间和搜索策略。考虑到流行推荐模型的神经结构，搜索空间可能涉及FNN结构、RNN结构、CNN结构和基于注意力的结构。至于搜索策略，架构的初始条件（Lian
    等，[2019](#bib.bib53)；Ding 等，[2022](#bib.bib15)）和用于学习任务无关表示的元模型（Shaw 等，[2019](#bib.bib86)）都通过元学习技术进行了研究。此外，结构连接和模型权重的相互影响也被证明对彼此的优化有益（Ding
    等，[2022](#bib.bib15)）。因此，设计一个神经网络架构搜索框架，以自动为不同任务或数据集指定推荐模型可能是另一个未来的方向。
- en: 7\. Conclusion
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: The rapid development of deep meta-learning methods has propelled the progress
    in the research field of recommender systems in recent years. This paper provides
    a timely survey after systematically investigating a large number of related papers
    in this area. We broke it down into a taxonomy of recommendation scenarios, meta-learning
    techniques, and meta-knowledge representations. For each recommendation scenario,
    technical details about how to apply meta-learning are introduced for existing
    methods. Finally, we point out several limitations in current research and highlight
    some promising future directions to promote research in meta-learning based recommendation
    methods. We hope our survey can be beneficial for both junior and experienced
    researchers in the relative areas.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 深度元学习方法的快速发展推动了近年来推荐系统研究领域的进步。本文在系统调查了大量相关论文后，提供了一个及时的综述。我们将其分解为推荐场景、元学习技术和元知识表示的分类。对于每个推荐场景，介绍了如何应用元学习的技术细节。最后，我们指出了当前研究中的几个局限性，并强调了一些有前景的未来方向，以促进基于元学习的推荐方法的研究。我们希望我们的综述对初级和资深研究人员都能有所帮助。
- en: References
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Bello et al. (2017) Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le.
    2017. Neural optimizer search with reinforcement learning. In *International Conference
    on Machine Learning*. PMLR, 459–468.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝洛等（2017）伊尔万·贝洛、巴雷特·佐普、维贾伊·瓦苏德万和阮阔·V·李。2017。使用强化学习的神经优化器搜索。见于*国际机器学习会议*。PMLR,
    459–468。
- en: Bharadhwaj (2019) Homanga Bharadhwaj. 2019. Meta-learning for user cold-start
    recommendation. In *2019 International Joint Conference on Neural Networks (IJCNN)*.
    IEEE, 1–8.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴拉德瓦杰（2019）霍曼加·巴拉德瓦杰。2019。用户冷启动推荐的元学习。见于*2019年国际神经网络联合会议（IJCNN）*。IEEE, 1–8。
- en: Billsus et al. (1998) Daniel Billsus, Michael J Pazzani, et al. 1998. Learning
    collaborative information filters.. In *Icml*, Vol. 98. 46–54.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比卢斯等（1998）丹尼尔·比卢斯、迈克尔·J·帕扎尼等。1998。学习协作信息过滤器。见于*ICML*，第98卷。46–54。
- en: Cai et al. (2018) Qi Cai, Yingwei Pan, Ting Yao, Chenggang Yan, and Tao Mei.
    2018. Memory matching networks for one-shot image recognition. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 4080–4088.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蔡等（2018）蔡奇、潘英伟、姚婷、阎成刚和梅涛。2018。用于一例图像识别的记忆匹配网络。见于*IEEE计算机视觉与模式识别会议论文集*。4080–4088。
- en: Cao et al. (2020) Tianwei Cao, Qianqian Xu, Zhiyong Yang, and Qingming Huang.
    2020. Task-distribution-aware Meta-learning for Cold-start CTR Prediction. In
    *Proceedings of the 28th ACM International Conference on Multimedia*. 3514–3522.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曹等（2020）曹天伟、徐倩倩、杨志勇和黄庆明。2020。针对冷启动CTR预测的任务分布感知元学习。见于*第28届ACM国际多媒体会议论文集*。3514–3522。
- en: Caruana (1997) Rich Caruana. 1997. Multitask learning. *Machine learning* 28,
    1 (1997), 41–75.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡鲁阿纳（1997）里奇·卡鲁阿纳。1997。多任务学习。*机器学习* 28, 1 (1997)，41–75。
- en: Chen et al. (2021b) Yudong Chen, Xin Wang, Miao Fan, Jizhou Huang, Shengwen
    Yang, and Wenwu Zhu. 2021b. Curriculum meta-learning for next POI recommendation.
    In *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
    Mining*. 2692–2702.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021b）余栋陈、辛王、苗凡、纪舟黄、盛文杨和温武朱。2021b。用于下一个POI推荐的课程元学习。见于*第27届ACM SIGKDD知识发现与数据挖掘会议论文集*。2692–2702。
- en: Chen et al. (2021a) Zhengyu Chen, Donglin Wang, and Shiqian Yin. 2021a. Improving
    cold-start recommendation via multi-prior meta-learning. In *European Conference
    on Information Retrieval*. Springer, 249–256.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021a）郑宇陈、董林王和石倩银。2021a。通过多先验元学习改进冷启动推荐。见于*欧洲信息检索会议*。Springer, 249–256。
- en: Cheng et al. (2016) Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked,
    Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa
    Ispir, et al. 2016. Wide & deep learning for recommender systems. In *Proceedings
    of the 1st workshop on deep learning for recommender systems*. 7–10.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程等（2016）程恒泽、列文特·科克、杰里迈亚·哈姆森、塔尔·沙凯德、图沙尔·钱德拉、赫里希·阿拉德赫、格伦·安德森、格雷格·科拉多、魏柴、穆斯塔法·伊斯比尔等。2016。用于推荐系统的宽度与深度学习。见于*第一届推荐系统深度学习研讨会论文集*。7–10。
- en: 'Cheng et al. (2020) Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Adaptive
    factorization network: Learning adaptive-order feature interactions. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 34\. 3609–3616.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程等（2020）韦宇程、燕燕沈和林鹏黄。2020。自适应因子分解网络：学习自适应顺序特征交互。见于*AAAI人工智能会议论文集*，第34卷。3609–3616。
- en: 'Cui et al. (2021) Yue Cui, Hao Sun, Yan Zhao, Hongzhi Yin, and Kai Zheng. 2021.
    Sequential-knowledge-aware next POI recommendation: A meta-learning approach.
    *ACM Transactions on Information Systems (TOIS)* 40, 2 (2021), 1–22.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 崔等（2021）崔跃、孙浩、赵艳、尹洪智和郑凯。2021。顺序知识感知下一个POI推荐：一种元学习方法。*ACM信息系统交易（TOIS）* 40, 2
    (2021)，1–22。
- en: Cunha et al. (2016) Tiago Cunha, Carlos Soares, and André CPLF de Carvalho.
    2016. Selecting collaborative filtering algorithms using metalearning. In *Joint
    European Conference on Machine Learning and Knowledge Discovery in Databases*.
    Springer, 393–409.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库尼亚等（2016）蒂亚戈·库尼亚、卡洛斯·索亚雷斯和安德烈·CPLF·德·卡瓦略。2016。使用元学习选择协同过滤算法。见于*欧洲联合机器学习与数据库知识发现会议*。Springer,
    393–409。
- en: 'Cunha et al. (2018) Tiago Cunha, Carlos Soares, and André CPLF de Carvalho.
    2018. Metalearning and Recommender Systems: A literature review and empirical
    study on the algorithm selection problem for Collaborative Filtering. *Information
    Sciences* 423 (2018), 128–144.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库尼亚等（2018）蒂亚戈·库尼亚、卡洛斯·索亚雷斯和安德烈·CPLF·德·卡瓦略。2018。元学习与推荐系统：协同过滤算法选择问题的文献综述与实证研究。*信息科学*
    423 (2018)，128–144。
- en: Ding et al. (2022) Yadong Ding, Yu Wu, Chengyue Huang, Siliang Tang, Yi Yang,
    Longhui Wei, Yueting Zhuang, and Qi Tian. 2022. Learning to Learn by Jointly Optimizing
    Neural Architecture and Weights. In *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, Vol. 2.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2022) Yadong Ding, Yu Wu, Chengyue Huang, Siliang Tang, Yi Yang,
    Longhui Wei, Yueting Zhuang, 和 Qi Tian. 2022. 通过联合优化神经架构和权重来学习学习。见于*IEEE/CVF 计算机视觉与模式识别会议论文集*，第2卷。
- en: 'Dong et al. (2020) Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, and Liming
    Zhu. 2020. Mamo: Memory-augmented meta-optimization for cold-start recommendation.
    In *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*. 688–697.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2020) Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, 和 Liming Zhu.
    2020. Mamo：用于冷启动推荐的记忆增强元优化。见于*第26届 ACM SIGKDD 知识发现与数据挖掘国际会议论文集*。688–697。
- en: 'Du et al. (2022) Yuntao Du, Xinjun Zhu, Lu Chen, Ziquan Fang, and Yunjun Gao.
    2022. MetaKG: Meta-learning on Knowledge Graph for Cold-start Recommendation.
    *IEEE Transactions on Knowledge and Data Engineering* (2022).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2022) Yuntao Du, Xinjun Zhu, Lu Chen, Ziquan Fang, 和 Yunjun Gao.
    2022. MetaKG：知识图谱上的元学习用于冷启动推荐。*IEEE 知识与数据工程汇刊* (2022)。
- en: Du et al. (2019) Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, and
    Jie Tang. 2019. Sequential scenario-specific meta learner for online recommendation.
    In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*. 2895–2904.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2019) Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, 和 Jie
    Tang. 2019. 针对在线推荐的顺序场景特定元学习器。见于*第25届 ACM SIGKDD 知识发现与数据挖掘国际会议论文集*。2895–2904。
- en: 'Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. 2019.
    Neural architecture search: A survey. *The Journal of Machine Learning Research*
    20, 1 (2019), 1997–2017.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, 和 Frank Hutter. 2019.
    神经架构搜索：综述。*机器学习研究杂志* 20, 1 (2019), 1997–2017。
- en: Elsken et al. (2020) Thomas Elsken, Benedikt Staffler, Jan Hendrik Metzen, and
    Frank Hutter. 2020. Meta-learning of neural architectures for few-shot learning.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*.
    12365–12375.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsken et al. (2020) Thomas Elsken, Benedikt Staffler, Jan Hendrik Metzen, 和
    Frank Hutter. 2020. 用于少样本学习的神经架构元学习。见于*IEEE/CVF 计算机视觉与模式识别大会论文集*。12365–12375。
- en: 'Fang et al. (2020) Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020.
    Deep learning for sequential recommendation: Algorithms, influential factors,
    and evaluations. *ACM Transactions on Information Systems (TOIS)* 39, 1 (2020),
    1–42.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2020) Hui Fang, Danning Zhang, Yiheng Shu, 和 Guibing Guo. 2020.
    序列推荐的深度学习：算法、影响因素及评估。*ACM 信息系统交易（TOIS）* 39, 1 (2020), 1–42。
- en: 'Feng et al. (2021) Xidong Feng, Chen Chen, Dong Li, Mengchen Zhao, Jianye Hao,
    and Jun Wang. 2021. CMML: Contextual Modulation Meta Learning for Cold-Start Recommendation.
    In *Proceedings of the 30th ACM International Conference on Information & Knowledge
    Management*. 484–493.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2021) Xidong Feng, Chen Chen, Dong Li, Mengchen Zhao, Jianye Hao,
    和 Jun Wang. 2021. CMML：用于冷启动推荐的上下文调制元学习。见于*第30届 ACM 国际信息与知识管理会议论文集*。484–493。
- en: Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *International conference
    on machine learning*. PMLR, 1126–1135.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. (2017) Chelsea Finn, Pieter Abbeel, 和 Sergey Levine. 2017. 模型无关的元学习用于深度网络的快速适应。见于*国际机器学习大会*。PMLR，1126–1135。
- en: Finn et al. (2019) Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey
    Levine. 2019. Online meta-learning. In *International Conference on Machine Learning*.
    PMLR, 1920–1930.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. (2019) Chelsea Finn, Aravind Rajeswaran, Sham Kakade, 和 Sergey Levine.
    2019. 在线元学习。见于*国际机器学习大会*。PMLR，1920–1930。
- en: Fu et al. (2019) Wenjing Fu, Zhaohui Peng, Senzhang Wang, Yang Xu, and Jin Li.
    2019. Deeply Fusing Reviews and Contents for Cold Start Users in Cross-Domain
    Recommendation Systems. In *The Thirty-Third AAAI Conference on Artificial Intelligence,
    AAAI*. AAAI Press, 94–101.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2019) Wenjing Fu, Zhaohui Peng, Senzhang Wang, Yang Xu, 和 Jin Li.
    2019. 深度融合评论和内容用于跨领域推荐系统中的冷启动用户。见于*第三十三届 AAAI 人工智能大会，AAAI*。AAAI Press，94–101。
- en: Gantner et al. (2010) Zeno Gantner, Lucas Drumond, Christoph Freudenthaler,
    Steffen Rendle, and Lars Schmidt-Thieme. 2010. Learning Attribute-to-Feature Mappings
    for Cold-Start Recommendations. In *ICDM 2010, The 10th IEEE International Conference
    on Data Mining,*. IEEE Computer Society, 176–185.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gantner et al. (2010) Zeno Gantner, Lucas Drumond, Christoph Freudenthaler,
    Steffen Rendle, 和 Lars Schmidt-Thieme. 2010. 学习属性到特征映射的冷启动推荐。见于*ICDM 2010，第十届
    IEEE 数据挖掘国际会议*。IEEE 计算机学会，176–185。
- en: 'Gao et al. (2021) Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua
    Piao, Yuhan Quan, Jianxin Chang, Depeng Jin, Xiangnan He, et al. 2021. Graph Neural
    Networks for Recommender Systems: Challenges, Methods, and Directions. *arXiv
    e-prints* (2021), arXiv–2109.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）Chen Gao, Yu Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao,
    Yuhan Quan, Jianxin Chang, Depeng Jin, Xiangnan He 等. 2021. 图神经网络在推荐系统中的应用：挑战、方法和方向。*arXiv
    e-prints*（2021），arXiv–2109。
- en: Gidaris and Komodakis (2018) Spyros Gidaris and Nikos Komodakis. 2018. Dynamic
    few-shot visual learning without forgetting. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 4367–4375.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gidaris 和 Komodakis（2018）Spyros Gidaris 和 Nikos Komodakis. 2018. 动态少样本视觉学习，无遗忘。发表于
    *IEEE计算机视觉与模式识别会议论文集*。4367–4375。
- en: 'Guo et al. (2020) Dalin Guo, Sofia Ira Ktena, Pranay Kumar Myana, Ferenc Huszar,
    Wenzhe Shi, Alykhan Tejani, Michael Kneier, and Sourav Das. 2020. Deep Bayesian
    Bandits: Exploring in Online Personalized Recommendations. In *RecSys 2020: Fourteenth
    ACM Conference on Recommender Systems*. ACM, 456–461.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2020）Dalin Guo, Sofia Ira Ktena, Pranay Kumar Myana, Ferenc Huszar, Wenzhe
    Shi, Alykhan Tejani, Michael Kneier 和 Sourav Das. 2020. 深度贝叶斯老虎机：在在线个性化推荐中的探索。发表于
    *RecSys 2020：第十四届ACM推荐系统会议*。ACM，456–461。
- en: 'Guo et al. (2017) Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang
    He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction.
    *arXiv preprint arXiv:1703.04247* (2017).'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等（2017）Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li 和 Xiuqiang He.
    2017. DeepFM: 一种基于因子分解机的神经网络用于CTR预测。*arXiv预印本 arXiv:1703.04247*（2017）。'
- en: Hao et al. (2021) Bowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li, and Hong Chen.
    2021. Pre-Training Graph Neural Networks for Cold-Start Users and Items Representation.
    In *Proceedings of the 14th ACM International Conference on Web Search and Data
    Mining*. 265–273.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等（2021）Bowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li 和 Hong Chen. 2021.
    预训练图神经网络用于冷启动用户和项目的表示。发表于 *第14届ACM国际网络搜索与数据挖掘会议论文集*。265–273。
- en: He and McAuley (2016) Ruining He and Julian J. McAuley. 2016. Fusing Similarity
    Models with Markov Chains for Sparse Sequential Recommendation. In *IEEE 16th
    International Conference on Data Mining, ICDM 2016,*. IEEE Computer Society, 191–200.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 和 McAuley（2016）Ruining He 和 Julian J. McAuley. 2016. 将相似性模型与马尔可夫链融合用于稀疏序列推荐。发表于
    *IEEE第16届国际数据挖掘会议，ICDM 2016*。IEEE计算机学会，191–200。
- en: He et al. (2017) Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu,
    and Tat-Seng Chua. 2017. Neural collaborative filtering. In *Proceedings of the
    26th international conference on world wide web*. 173–182.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2017）Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu 和 Tat-Seng
    Chua. 2017. 神经协同过滤。发表于 *第26届国际万维网会议论文集*。173–182。
- en: He et al. (2016) Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua.
    2016. Fast Matrix Factorization for Online Recommendation with Implicit Feedback.
    In *Proceedings of the 39th International ACM SIGIR conference on Research and
    Development in Information Retrieval*. ACM, 549–558.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2016）Xiangnan He, Hanwang Zhang, Min-Yen Kan 和 Tat-Seng Chua. 2016. 针对隐式反馈的在线推荐的快速矩阵分解。发表于
    *第39届国际ACM SIGIR信息检索研究与开发会议论文集*。ACM，549–558。
- en: Hidasi et al. (2016) Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas,
    and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks.
    In *4th International Conference on Learning Representations, ICLR*.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hidasi 等（2016）Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas 和 Domonkos
    Tikk. 2016. 基于会话的推荐与递归神经网络。发表于 *第四届国际学习表征会议，ICLR*。
- en: Hochreiter et al. (2001) Sepp Hochreiter, A Steven Younger, and Peter R Conwell.
    2001. Learning to learn using gradient descent. In *International Conference on
    Artificial Neural Networks*. Springer, 87–94.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 等（2001）Sepp Hochreiter, A Steven Younger 和 Peter R Conwell. 2001.
    使用梯度下降学习学习。发表于 *国际人工神经网络会议*。Springer，87–94。
- en: 'Hospedales et al. (2020) Timothy Hospedales, Antreas Antoniou, Paul Micaelli,
    and Amos Storkey. 2020. Meta-learning in neural networks: A survey. *arXiv preprint
    arXiv:2004.05439* (2020).'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hospedales 等（2020）Timothy Hospedales, Antreas Antoniou, Paul Micaelli 和 Amos
    Storkey. 2020. 神经网络中的元学习：综述。*arXiv预印本 arXiv:2004.05439*（2020）。
- en: Huang et al. (2022) Xiaowen Huang, Jitao Sang, Jian Yu, and Changsheng Xu. 2022.
    Learning to learn a cold-start sequential recommender. *ACM Transactions on Information
    Systems (TOIS)* 40, 2 (2022), 1–25.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2022）Xiaowen Huang, Jitao Sang, Jian Yu 和 Changsheng Xu. 2022. 学习构建冷启动序列推荐系统。*ACM信息系统交易*（TOIS）40，第2期（2022），1–25。
- en: Huisman et al. (2021) Mike Huisman, Jan N Van Rijn, and Aske Plaat. 2021. A
    survey of deep meta-learning. *Artificial Intelligence Review* 54, 6 (2021), 4483–4541.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huisman 等人（2021）Mike Huisman, Jan N Van Rijn, 和 Aske Plaat. 2021. 深度元学习的调查.
    *人工智能评论* 54, 6 (2021), 4483–4541.
- en: Hutter et al. ([n. d.]) Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren.
    [n. d.]. Automated Machine Learning Methods, Systems, Challenges. ([n. d.]).
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hutter 等人（[n. d.]）Frank Hutter, Lars Kotthoff, 和 Joaquin Vanschoren. [n. d.].
    自动化机器学习方法、系统、挑战. ([n. d.])
- en: 'Kabbur et al. (2013) Santosh Kabbur, Xia Ning, and George Karypis. 2013. Fism:
    factored item similarity models for top-n recommender systems. In *Proceedings
    of the 19th ACM SIGKDD international conference on Knowledge discovery and data
    mining*. 659–667.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kabbur 等人（2013）Santosh Kabbur, Xia Ning, 和 George Karypis. 2013. Fism: 用于Top-N推荐系统的分解项相似度模型.
    见 *第19届ACM SIGKDD国际知识发现与数据挖掘大会论文集*. 659–667.'
- en: Kang and McAuley (2018) Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive
    Sequential Recommendation. In *IEEE International Conference on Data Mining, ICDM*.
    IEEE Computer Society, 197–206.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 和 McAuley（2018）Wang-Cheng Kang 和 Julian J. McAuley. 2018. 自注意序列推荐. 见 *IEEE国际数据挖掘会议,
    ICDM*. IEEE计算机学会, 197–206.
- en: 'Kim et al. (2018) Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon
    Lee, Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, and Jiwon Kim. 2018. Auto-meta:
    Automated gradient based meta learner search. *arXiv preprint arXiv:1806.06927*
    (2018).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人（2018）Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee,
    Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, 和 Jiwon Kim. 2018. Auto-meta: 自动梯度基础元学习者搜索.
    *arXiv预印本 arXiv:1806.06927* (2018).'
- en: 'Kim et al. (2021) Minseok Kim, Hwanjun Song, Doyoung Kim, Kijung Shin, and
    Jae-Gil Lee. 2021. PREMERE: Meta-Reweighting via Self-Ensembling for Point-of-Interest
    Recommendation. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 35\. 4164–4171.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人（2021）Minseok Kim, Hwanjun Song, Doyoung Kim, Kijung Shin, 和 Jae-Gil
    Lee. 2021. PREMERE: 通过自我集成的元重标定用于兴趣点推荐. 见 *AAAI人工智能会议论文集*, 第35卷. 4164–4171.'
- en: Kim et al. (2022) Minseok Kim, Hwanjun Song, Yooju Shin, Dongmin Park, Kijung
    Shin, and Jae-Gil Lee. 2022. Meta-Learning for Online Update of Recommender Systems.
    (2022).
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2022）Minseok Kim, Hwanjun Song, Yooju Shin, Dongmin Park, Kijung Shin,
    和 Jae-Gil Lee. 2022. 在线更新推荐系统的元学习. (2022).
- en: Koch et al. (2015) Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, et al.
    2015. Siamese neural networks for one-shot image recognition. In *ICML deep learning
    workshop*, Vol. 2\. Lille, 0.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koch 等人（2015）Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, 等. 2015. 用于单次图像识别的Siamese神经网络.
    见 *ICML深度学习研讨会*, 第2卷. Lille, 0.
- en: Lasserre et al. (2020) Julia Lasserre, Abdul-Saboor Sheikh, Evgenii Koriagin,
    Urs Bergman, Roland Vollgraf, and Reza Shirvany. 2020. Meta-learning for size
    and fit recommendation in fashion. In *Proceedings of the 2020 SIAM international
    conference on data mining*. SIAM, 55–63.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lasserre 等人（2020）Julia Lasserre, Abdul-Saboor Sheikh, Evgenii Koriagin, Urs
    Bergman, Roland Vollgraf, 和 Reza Shirvany. 2020. 时尚领域的尺寸和合身推荐的元学习. 见 *2020年SIAM国际数据挖掘会议论文集*.
    SIAM, 55–63.
- en: 'Lee et al. (2019) Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee
    Chung. 2019. Melu: Meta-learned user preference estimator for cold-start recommendation.
    In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*. 1073–1082.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等人（2019）Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, 和 Sehee Chung.
    2019. Melu: 冷启动推荐的元学习用户偏好估计器. 见 *第25届ACM SIGKDD国际知识发现与数据挖掘大会论文集*. 1073–1082.'
- en: 'Lee et al. (2022) Hung-yi Lee, Shang-Wen Li, and Ngoc Thang Vu. 2022. Meta
    Learning for Natural Language Processing: A Survey. *arXiv preprint arXiv:2205.01500*
    (2022).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2022）Hung-yi Lee, Shang-Wen Li, 和 Ngoc Thang Vu. 2022. 自然语言处理的元学习：综述.
    *arXiv预印本 arXiv:2205.01500* (2022).
- en: Li et al. (2021) Jingjing Li, Ke Lu, Zi Huang, and Heng Tao Shen. 2021. On Both
    Cold-Start and Long-Tail Recommendation with Social Data. *IEEE Trans. Knowl.
    Data Eng.* 33, 1 (2021), 194–208.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2021）Jingjing Li, Ke Lu, Zi Huang, 和 Heng Tao Shen. 2021. 关于冷启动和长尾推荐的社交数据.
    *IEEE知识与数据工程汇刊* 33, 1 (2021), 194–208.
- en: Li et al. (2017) Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian,
    and Jun Ma. 2017. Neural Attentive Session-based Recommendation. In *Proceedings
    of the 2017 ACM on Conference on Information and Knowledge Management*. ACM, 1419–1428.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2017）Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, 和 Jun
    Ma. 2017. 神经注意力会话推荐. 见 *2017年ACM信息与知识管理会议论文集*. ACM, 1419–1428.
- en: Li et al. (2020) Zhao Li, Haobo Wang, Donghui Ding, Shichang Hu, Zhen Zhang,
    Weiwei Liu, Jianliang Gao, Zhiqiang Zhang, and Ji Zhang. 2020. Deep Interest-Shifting
    Network with Meta-Embeddings for Fresh Item Recommendation. *Complexity* 2020
    (2020).
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020) Zhao Li, Haobo Wang, Donghui Ding, Shichang Hu, Zhen Zhang,
    Weiwei Liu, Jianliang Gao, Zhiqiang Zhang, 和 Ji Zhang. 2020. 基于元嵌入的深度兴趣转移网络用于新鲜项推荐。*复杂性*
    2020 (2020)。
- en: Lian et al. (2019) Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin,
    Peilin Zhao, Junzhou Huang, and Shenghua Gao. 2019. Towards fast adaptation of
    neural architectures with meta learning. In *International Conference on Learning
    Representations*.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lian et al. (2019) Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin,
    Peilin Zhao, Junzhou Huang, 和 Shenghua Gao. 2019. 通过元学习实现神经架构的快速适应。见于 *国际学习表示会议*。
- en: Lin et al. (2021) Xixun Lin, Jia Wu, Chuan Zhou, Shirui Pan, Yanan Cao, and
    Bin Wang. 2021. Task-adaptive Neural Process for User Cold-Start Recommendation.
    In *Proceedings of the Web Conference 2021*. 1306–1316.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2021) Xixun Lin, Jia Wu, Chuan Zhou, Shirui Pan, Yanan Cao, 和 Bin
    Wang. 2021. 用户冷启动推荐的任务自适应神经过程。见于 *Web会议2021论文集*。1306–1316。
- en: Lin et al. (2020) Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Dongxiao
    Yu, Jun Ma, Maarten de Rijke, and Xiuzhen Cheng. 2020. Meta Matrix Factorization
    for Federated Rating Predictions. In *Proceedings of the 43rd International ACM
    SIGIR Conference on Research and Development in Information Retrieval*. 981–990.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2020) Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Dongxiao
    Yu, Jun Ma, Maarten de Rijke, 和 Xiuzhen Cheng. 2020. 联邦评分预测的元矩阵分解。见于 *第43届国际ACM
    SIGIR信息检索研究与开发会议论文集*。981–990。
- en: 'Liu et al. (2018) Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. DARTS:
    Differentiable Architecture Search. In *International Conference on Learning Representations*.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018) Hanxiao Liu, Karen Simonyan, 和 Yiming Yang. 2018. DARTS：可微分架构搜索。见于
    *国际学习表示会议*。
- en: Liu et al. (2020a) Jialin Liu, Fei Chao, and Chih-Min Lin. 2020a. Task augmentation
    by rotating for meta-learning. *arXiv preprint arXiv:2003.00804* (2020).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020a) Jialin Liu, Fei Chao, 和 Chih-Min Lin. 2020a. 通过旋转进行任务增强以用于元学习。*arXiv
    预印本 arXiv:2003.00804* (2020)。
- en: Liu et al. (2020b) Zhaoyang Liu, Haokun Chen, Fei Sun, Xu Xie, Jinyang Gao,
    Bolin Ding, and Yanyan Shen. 2020b. Intent Preference Decoupling for User Representation
    on Online Recommender System.. In *IJCAI*. 2575–2582.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020b) Zhaoyang Liu, Haokun Chen, Fei Sun, Xu Xie, Jinyang Gao,
    Bolin Ding, 和 Yanyan Shen. 2020b. 在线推荐系统中用户表示的意图偏好解耦。见于 *IJCAI*。2575–2582。
- en: Lu et al. (2020) Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on
    heterogeneous information networks for cold-start recommendation. In *Proceedings
    of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. 1563–1573.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2020) Yuanfu Lu, Yuan Fang, 和 Chuan Shi. 2020. 在异构信息网络上进行冷启动推荐的元学习。见于
    *第26届ACM SIGKDD国际知识发现与数据挖掘大会论文集*。1563–1573。
- en: 'Luo et al. (2020) Mi Luo, Fei Chen, Pengxiang Cheng, Zhenhua Dong, Xiuqiang
    He, Jiashi Feng, and Zhenguo Li. 2020. Metaselector: Meta-learning for recommendation
    with user-level adaptive model selection. In *Proceedings of The Web Conference
    2020*. 2507–2513.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2020) Mi Luo, Fei Chen, Pengxiang Cheng, Zhenhua Dong, Xiuqiang
    He, Jiashi Feng, 和 Zhenguo Li. 2020. Metaselector：基于用户级适应性模型选择的推荐系统元学习。见于 *The
    Web Conference 2020 会议论文集*。2507–2513。
- en: 'Luo et al. (2022) Shuai Luo, Yujie Li, Pengxiang Gao, Yichuan Wang, and Seiichi
    Serikawa. 2022. Meta-seg: A survey of meta-learning for image segmentation. *Pattern
    Recognition* (2022), 108586.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2022) Shuai Luo, Yujie Li, Pengxiang Gao, Yichuan Wang, 和 Seiichi
    Serikawa. 2022. Meta-seg：元学习在图像分割中的综述。*模式识别* (2022)，108586。
- en: 'Ma et al. (2022) Yao Ma, Shilin Zhao, Weixiao Wang, Yaoman Li, and Irwin King.
    2022. Multimodality in meta-learning: A comprehensive survey. *Knowledge-Based
    Systems* (2022), 108976.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2022) Yao Ma, Shilin Zhao, Weixiao Wang, Yaoman Li, 和 Irwin King.
    2022. 元学习中的多模态性：全面综述。*知识基础系统* (2022)，108976。
- en: 'Man et al. (2017) Tong Man, Huawei Shen, Xiaolong Jin, and Xueqi Cheng. 2017.
    Cross-domain recommendation: An embedding and mapping approach.. In *IJCAI*, Vol. 17\.
    2464–2470.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Man et al. (2017) Tong Man, Huawei Shen, Xiaolong Jin, 和 Xueqi Cheng. 2017.
    跨领域推荐：一种嵌入和映射方法。见于 *IJCAI*，第17卷。2464–2470。
- en: Metz et al. (2018) Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha
    Sohl-Dickstein. 2018. Meta-Learning Update Rules for Unsupervised Representation
    Learning. In *International Conference on Learning Representations*.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metz et al. (2018) Luke Metz, Niru Maheswaranathan, Brian Cheung, 和 Jascha Sohl-Dickstein.
    2018. 无监督表示学习的元学习更新规则。见于 *国际学习表示会议*。
- en: Mishra et al. (2018) Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter
    Abbeel. 2018. A Simple Neural Attentive Meta-Learner. In *International Conference
    on Learning Representations*.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra et al. (2018) Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, 和 Pieter Abbeel.
    2018. 一种简单的神经注意力元学习器。见于*国际学习表征会议*。
- en: 'Murty et al. (2021) Shikhar Murty, Tatsunori B Hashimoto, and Christopher D
    Manning. 2021. Dreca: A general task augmentation strategy for few-shot natural
    language inference. In *Proceedings of the 2021 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*.
    1113–1125.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Murty et al. (2021) Shikhar Murty, Tatsunori B Hashimoto, 和 Christopher D Manning.
    2021. Dreca: 一种用于少样本自然语言推理的通用任务增强策略。见于*2021 年北美计算语言学协会人类语言技术会议论文集*。1113–1125。'
- en: Neupane et al. (2022) Krishna Prasad Neupane, Ervine Zheng, Yu Kong, and Qi
    Yu. 2022. A Dynamic Meta-Learning Model for Time-Sensitive Cold-Start Recommendations.
    *Genre* 2 (2022), 3–0.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neupane et al. (2022) Krishna Prasad Neupane, Ervine Zheng, Yu Kong, 和 Qi Yu.
    2022. 一种动态元学习模型，用于时间敏感的冷启动推荐。*Genre* 2 (2022)，3–0。
- en: 'Neupane et al. (2021) Krishna Prasad Neupane, Ervine Zheng, and Qi Yu. 2021.
    MetaEDL: Meta Evidential Learning For Uncertainty-Aware Cold-Start Recommendations.
    In *2021 IEEE International Conference on Data Mining (ICDM)*. IEEE, 1258–1263.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Neupane et al. (2021) Krishna Prasad Neupane, Ervine Zheng, 和 Qi Yu. 2021.
    MetaEDL: 具有不确定性感知的冷启动推荐的元证据学习。见于*2021 IEEE 数据挖掘国际会议（ICDM）*。IEEE，1258–1263。'
- en: 'Ouyang et al. (2021) Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Li Li, Kun Zhang,
    Jinmei Luo, Zhaojie Liu, and Yanlong Du. 2021. Learning Graph Meta Embeddings
    for Cold-Start Ads in Click-Through Rate Prediction. In *SIGIR ’21: The 44th International
    ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual
    Event, Canada, July 11-15, 2021*, Fernando Diaz, Chirag Shah, Torsten Suel, Pablo
    Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 1157–1166. [https://doi.org/10.1145/3404835.3462879](https://doi.org/10.1145/3404835.3462879)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ouyang et al. (2021) Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Li Li, Kun Zhang,
    Jinmei Luo, Zhaojie Liu, 和 Yanlong Du. 2021. 在点击率预测中为冷启动广告学习图形元嵌入。见于*SIGIR ’21:
    第44届国际 ACM SIGIR 信息检索研究与发展会议，虚拟会议，加拿大，2021年7月11-15日*，Fernando Diaz, Chirag Shah,
    Torsten Suel, Pablo Castells, Rosie Jones, 和 Tetsuya Sakai (编)。ACM，1157–1166。
    [https://doi.org/10.1145/3404835.3462879](https://doi.org/10.1145/3404835.3462879)'
- en: 'Pan et al. (2019) Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing
    He. 2019. Warm up cold-start advertisements: Improving ctr predictions via learning
    to learn id embeddings. In *Proceedings of the 42nd International ACM SIGIR Conference
    on Research and Development in Information Retrieval*. 695–704.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. (2019) Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, 和 Qing
    He. 2019. 热身冷启动广告：通过学习学习 ID 嵌入来改善 CTR 预测。见于*第42届国际 ACM SIGIR 信息检索研究与发展会议论文集*。695–704。
- en: 'Pang et al. (2022) Haoyu Pang, Fausto Giunchiglia, Ximing Li, Renchu Guan,
    and Xiaoyue Feng. 2022. PNMTA: A Pretrained Network Modulation and Task Adaptation
    Approach for User Cold-Start Recommendation. In *Proceedings of the ACM Web Conference
    2022*. 348–359.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pang et al. (2022) Haoyu Pang, Fausto Giunchiglia, Ximing Li, Renchu Guan,
    和 Xiaoyue Feng. 2022. PNMTA: 一种用于用户冷启动推荐的预训练网络调制和任务适配方法。见于*2022 ACM 网络会议论文集*。348–359。'
- en: Peng et al. (2021) Danni Peng, Sinno Jialin Pan, Jie Zhang, and Anxiang Zeng.
    2021. Learning an Adaptive Meta Model-Generator for Incrementally Updating Recommender
    Systems. In *Fifteenth ACM Conference on Recommender Systems*. 411–421.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2021) Danni Peng, Sinno Jialin Pan, Jie Zhang, 和 Anxiang Zeng.
    2021. 学习一种自适应的元模型生成器，用于增量更新推荐系统。见于*第十五届 ACM 推荐系统会议*。411–421。
- en: 'Perez et al. (2018) Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin,
    and Aaron Courville. 2018. Film: Visual reasoning with a general conditioning
    layer. In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 32.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Perez et al. (2018) Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin,
    和 Aaron Courville. 2018. Film: 使用通用条件层进行视觉推理。见于*AAAI 人工智能会议论文集*，第32卷。'
- en: Prudêncio and Ludermir (2004) Ricardo BC Prudêncio and Teresa B Ludermir. 2004.
    Meta-learning approaches to selecting time series models. *Neurocomputing* 61
    (2004), 121–137.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prudêncio and Ludermir (2004) Ricardo BC Prudêncio 和 Teresa B Ludermir. 2004.
    选择时间序列模型的元学习方法。*神经计算* 61 (2004)，121–137。
- en: Qiao et al. (2018) Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L Yuille. 2018.
    Few-shot image recognition by predicting parameters from activations. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 7229–7238.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao et al. (2018) Siyuan Qiao, Chenxi Liu, Wei Shen, 和 Alan L Yuille. 2018.
    通过从激活中预测参数进行少样本图像识别。见于*IEEE 计算机视觉与模式识别会议论文集*。7229–7238。
- en: 'Qu et al. (2021) Guanjin Qu, Huaming Wu, Ruidong Li, and Pengfei Jiao. 2021.
    Dmro: A deep meta reinforcement learning-based task offloading framework for edge-cloud
    computing. *IEEE Transactions on Network and Service Management* 18, 3 (2021),
    3448–3459.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qu 等 (2021) Guanjin Qu, Huaming Wu, Ruidong Li 和 Pengfei Jiao. 2021. Dmro:
    基于深度元强化学习的任务卸载框架用于边缘云计算。*IEEE 网络与服务管理汇刊* 18, 3 (2021), 3448–3459。'
- en: Qu et al. (2016) Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen,
    and Jun Wang. 2016. Product-based neural networks for user response prediction.
    In *2016 IEEE 16th International Conference on Data Mining (ICDM)*. IEEE, 1149–1154.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qu 等 (2016) Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen 和 Jun
    Wang. 2016. 基于产品的神经网络用于用户响应预测。发表于 *2016 IEEE 第十六届国际数据挖掘会议 (ICDM)*。IEEE, 1149–1154。
- en: Ravi and Larochelle (2016) Sachin Ravi and Hugo Larochelle. 2016. Optimization
    as a model for few-shot learning. (2016).
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravi 和 Larochelle (2016) Sachin Ravi 和 Hugo Larochelle. 2016. 作为少量样本学习模型的优化。(2016)。
- en: Ren et al. (2019) Yi Ren, Cuirong Chi, and Zhang Jintao. 2019. A Survey of Personalized
    Recommendation Algorithm Selection Based on Meta-learning. In *The International
    Conference on Cyber Security Intelligence and Analytics*. Springer, 1383–1388.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等 (2019) Yi Ren, Cuirong Chi 和 Zhang Jintao. 2019. 基于元学习的个性化推荐算法选择调查。发表于
    *国际网络安全智能与分析会议*。Springer, 1383–1388。
- en: Rendle et al. (2010) Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme.
    2010. Factorizing personalized Markov chains for next-basket recommendation. In
    *Proceedings of the 19th International Conference on World Wide Web,*. ACM, 811–820.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rendle 等 (2010) Steffen Rendle, Christoph Freudenthaler 和 Lars Schmidt-Thieme.
    2010. 为下一篮子推荐因式分解个性化马尔可夫链。发表于 *第十九届国际万维网会议论文集*。ACM, 811–820。
- en: 'Rossi et al. (2014) André Luis Debiaso Rossi, André Carlos Ponce de Leon Ferreira,
    Carlos Soares, Bruno Feres De Souza, et al. 2014. MetaStream: A meta-learning
    based method for periodic algorithm selection in time-changing data. *Neurocomputing*
    127 (2014), 52–64.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rossi 等 (2014) André Luis Debiaso Rossi, André Carlos Ponce de Leon Ferreira,
    Carlos Soares, Bruno Feres De Souza 等. 2014. MetaStream: 一种基于元学习的周期性算法选择方法用于时间变化数据。*神经计算*
    127 (2014), 52–64。'
- en: Rusu et al. (2018) Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals,
    Razvan Pascanu, Simon Osindero, and Raia Hadsell. 2018. Meta-Learning with Latent
    Embedding Optimization. In *International Conference on Learning Representations*.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rusu 等 (2018) Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan
    Pascanu, Simon Osindero 和 Raia Hadsell. 2018. 带有潜在嵌入优化的元学习。发表于 *国际学习表征会议*。
- en: 'Sankar et al. (2021) Aravind Sankar, Junting Wang, Adit Krishnan, and Hari
    Sundaram. 2021. ProtoCF: Prototypical Collaborative Filtering for Few-shot Recommendation.
    In *Fifteenth ACM Conference on Recommender Systems*. 166–175.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sankar 等 (2021) Aravind Sankar, Junting Wang, Adit Krishnan 和 Hari Sundaram.
    2021. ProtoCF: 原型协同过滤用于少量样本推荐。发表于 *第十五届 ACM 推荐系统会议*。166–175。'
- en: Santoro et al. (2016) Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan
    Wierstra, and Timothy Lillicrap. 2016. Meta-learning with memory-augmented neural
    networks. In *International conference on machine learning*. PMLR, 1842–1850.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santoro 等 (2016) Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra
    和 Timothy Lillicrap. 2016. 具有记忆增强神经网络的元学习。发表于 *国际机器学习会议*。PMLR, 1842–1850。
- en: Satorras and Estrach (2018) Victor Garcia Satorras and Joan Bruna Estrach. 2018.
    Few-Shot Learning with Graph Neural Networks. In *International Conference on
    Learning Representations*.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Satorras 和 Estrach (2018) Victor Garcia Satorras 和 Joan Bruna Estrach. 2018.
    使用图神经网络的少量样本学习。发表于 *国际学习表征会议*。
- en: Shaw et al. (2019) Albert Shaw, Wei Wei, Weiyang Liu, Le Song, and Bo Dai. 2019.
    Meta architecture search. *Advances in Neural Information Processing Systems*
    32 (2019).
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaw 等 (2019) Albert Shaw, Wei Wei, Weiyang Liu, Le Song 和 Bo Dai. 2019. 元架构搜索。*神经信息处理系统进展*
    32 (2019)。
- en: 'Shazeer et al. (2017) Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
    Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural
    networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint arXiv:1701.06538*
    (2017).'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer 等 (2017) Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,
    Quoc Le, Geoffrey Hinton 和 Jeff Dean. 2017. 极大的神经网络：稀疏门控专家混合层。*arXiv 预印本 arXiv:1701.06538*
    (2017)。
- en: 'Shen et al. (2022) Qijie Shen, Hong Wen, Wanjie Tao, Jing Zhang, Fuyu Lv, Zulong
    Chen, and Zhao Li. 2022. Deep Interest Highlight Network for Click-Through Rate
    Prediction in Trigger-Induced Recommendation. In *WWW ’22: The ACM Web Conference
    2022*. ACM, 422–430.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等 (2022) Qijie Shen, Hong Wen, Wanjie Tao, Jing Zhang, Fuyu Lv, Zulong
    Chen 和 Zhao Li. 2022. 触发推荐中的点击率预测深度兴趣高亮网络。发表于 *WWW ’22: ACM 网络会议 2022*。ACM, 422–430。'
- en: Snell et al. (2017) Jake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical
    networks for few-shot learning. *Advances in neural information processing systems*
    30 (2017).
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell et al. (2017) 杰克 Snell, 凯文 Swersky, 和 理查德 Zemel. 2017. 针对少样本学习的原型网络. *神经信息处理系统进展*
    30 (2017).
- en: 'Song et al. (2021) Jiayu Song, Jiajie Xu, Rui Zhou, Lu Chen, Jianxin Li, and
    Chengfei Liu. 2021. CBML: A Cluster-based Meta-learning Model for Session-based
    Recommendation. In *Proceedings of the 30th ACM International Conference on Information
    & Knowledge Management*. 1713–1722.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2021) 嘉瑜 Song, 佳杰 Xu, 瑞 Zhou, 陆 Chen, 健新 Li, 和 成飞 Liu. 2021. CBML:
    一种基于聚类的元学习模型用于会话推荐. 载于 *第30届 ACM 国际信息与知识管理会议论文集*. 1713–1722.'
- en: Su and Khoshgoftaar (2009) Xiaoyuan Su and Taghi M Khoshgoftaar. 2009. A survey
    of collaborative filtering techniques. *Advances in artificial intelligence* 2009
    (2009).
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su and Khoshgoftaar (2009) 萧远 Su 和 塔赫 M Khoshgoftaar. 2009. 协同过滤技术综述. *人工智能进展*
    2009 (2009).
- en: 'Sun et al. (2021b) Huimin Sun, Jiajie Xu, Kai Zheng, Pengpeng Zhao, Pingfu
    Chao, and Xiaofang Zhou. 2021b. MFNP: A Meta-optimized Model for Few-shot Next
    POI Recommendation. In *Proceedings of the Thirtieth International Joint Conference
    on Artificial Intelligence (IJCAI-21)*.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2021b) 胡敏 Sun, 佳杰 Xu, 凯 Zheng, 彭鹏 Zhao, 平福 Chao, 和 晓芳 Zhou. 2021b.
    MFNP: 一种针对少量样本下的下一兴趣点推荐的元优化模型. 载于 *第三十届国际人工智能联合会议论文集 (IJCAI-21)*.'
- en: 'Sun et al. (2020) Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung
    Nguyen, and Hongzhi Yin. 2020. Where to Go Next: Modeling Long- and Short-Term
    User Preferences for Point-of-Interest Recommendation. In *The Thirty-Fourth AAAI
    Conference on Artificial Intelligence*. AAAI Press, 214–221.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2020) 柯 Sun, 铁云 Qian, 通 Chen, 宜乐 Liang, 阮国越 Hung Nguyen, 和 洪志 Yin.
    2020. 下一步去哪里：建模长短期用户偏好用于兴趣点推荐. 载于 *第三十四届 AAAI 人工智能大会*. AAAI Press, 214–221.
- en: 'Sun et al. (2021a) Xuehan Sun, Tianyao Shi, Xiaofeng Gao, Yanrong Kang, and
    Guihai Chen. 2021a. FORM: Follow the Online Regularized Meta-Leader for Cold-Start
    Recommendation. In *Proceedings of the 44th International ACM SIGIR Conference
    on Research and Development in Information Retrieval*. 1177–1186.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2021a) 雪汉 Sun, 天曜 Shi, 小锋 Gao, 雁蓉 Kang, 和 桂海 Chen. 2021a. FORM:
    追随在线正则化的元学习者用于冷启动推荐. 载于 *第44届国际 ACM SIGIR 信息检索研究与发展会议论文集*. 1177–1186.'
- en: Sun et al. (2021c) Yinan Sun, Kang Yin, Hehuan Liu, Si Li, Yajing Xu, and Jun
    Guo. 2021c. Meta-Learned Specific Scenario Interest Network for User Preference
    Prediction. In *Proceedings of the 44th International ACM SIGIR Conference on
    Research and Development in Information Retrieval*. 1970–1974.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2021c) 尹南 Sun, 康 Yin, 赫焕 Liu, 思 Li, 雅婧 Xu, 和 军 Guo. 2021c. 针对用户偏好的元学习特定场景兴趣网络.
    载于 *第44届国际 ACM SIGIR 信息检索研究与发展会议论文集*. 1970–1974.
- en: 'Sung et al. (2018) Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS
    Torr, and Timothy M Hospedales. 2018. Learning to compare: Relation network for
    few-shot learning. In *Proceedings of the IEEE conference on computer vision and
    pattern recognition*. 1199–1208.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung et al. (2018) 洪 Sung, 永鑫 Yang, 李 Zhang, 陶 Xiang, 菲利普 HS Torr, 和 提摩太 M Hospedales.
    2018. 学习比较：用于少样本学习的关系网络. 载于 *IEEE 计算机视觉与模式识别会议论文集*. 1199–1208.
- en: 'Suo et al. (2020) Qiuling Suo, Jingyuan Chou, Weida Zhong, and Aidong Zhang.
    2020. Tadanet: Task-adaptive network for graph-enriched meta-learning. In *Proceedings
    of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. 1789–1799.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Suo et al. (2020) 秋玲 Suo, 景元 Chou, 伟达 Zhong, 和 爱东 Zhang. 2020. Tadanet: 任务自适应网络用于图增强元学习.
    载于 *第26届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*. 1789–1799.'
- en: Tan et al. (2021) Haining Tan, Di Yao, Tao Huang, Baoli Wang, Quanliang Jing,
    and Jingping Bi. 2021. Meta-Learning Enhanced Neural ODE for Citywide Next POI
    Recommendation. In *2021 22nd IEEE International Conference on Mobile Data Management
    (MDM)*. IEEE, 89–98.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. (2021) 海宁 Tan, 翟尧 Yao, 陶 Huang, 保力 Wang, 全亮 Jing, 和 敬平 Bi. 2021.
    元学习增强的神经ODE用于城市范围的下一兴趣点推荐. 载于 *2021年第22届 IEEE 国际移动数据管理会议 (MDM)*. IEEE, 89–98.
- en: 'Vanschoren (2018) Joaquin Vanschoren. 2018. Meta-learning: A survey. *arXiv
    preprint arXiv:1810.03548* (2018).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vanschoren (2018) 乔奎因 Vanschoren. 2018. 元学习：综述. *arXiv 预印本 arXiv:1810.03548*
    (2018).
- en: Vartak et al. (2017) Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua
    Bratman, and Hugo Larochelle. 2017. A Meta-Learning Perspective on Cold-Start
    Recommendations for Items. *Advances in Neural Information Processing Systems*
    30 (2017), 6904–6914.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vartak et al. (2017) 玛纳西 Vartak, 阿尔文德 Thiagarajan, 康拉多 Miranda, 杰舒亚 Bratman,
    和 雨果 Larochelle. 2017. 从元学习的角度看冷启动推荐问题. *神经信息处理系统进展* 30 (2017), 6904–6914.
- en: Vinyals et al. (2016) Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
    Wierstra, et al. 2016. Matching networks for one shot learning. *Advances in neural
    information processing systems* 29 (2016).
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2016) 奥里奥尔·维尼亚尔斯、查尔斯·布伦德尔、蒂莫西·利利克拉普、达恩·维尔斯特拉等。2016。用于一次性学习的匹配网络。*神经信息处理系统进展*
    29（2016）。
- en: Vuorio et al. (2019) Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J Lim.
    2019. Multimodal model-agnostic meta-learning via task-aware modulation. *Advances
    in Neural Information Processing Systems* 32 (2019).
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vuorio et al. (2019) 丽斯托·沃里奥、邵华孙、赫相胡和约瑟夫·J·林。2019。通过任务感知调制的多模态模型无关元学习。*神经信息处理系统进展*
    32（2019）。
- en: Wang et al. (2021a) Jianling Wang, Kaize Ding, and James Caverlee. 2021a. Sequential
    Recommendation for Cold-start Users with Meta Transitional Learning. In *Proceedings
    of the 44th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 1783–1787.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) 建玲旺、凯泽丁和詹姆斯·卡弗利。2021a。基于元转移学习的冷启动用户序列推荐。见于*第44届国际ACM SIGIR信息检索研究与发展会议论文集*，1783–1787。
- en: Wang et al. (2020) Jin Wang, Jia Hu, Geyong Min, Albert Y Zomaya, and Nektarios
    Georgalas. 2020. Fast adaptive task offloading in edge computing based on meta
    reinforcement learning. *IEEE Transactions on Parallel and Distributed Systems*
    32, 1 (2020), 242–253.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) 金旺、贾胡、戈勇敏、阿尔伯特·Y·佐玛亚和内克塔里奥斯·乔治拉斯。2020。基于元强化学习的边缘计算快速自适应任务卸载。*IEEE并行与分布式系统汇刊*
    32，第1期（2020），242–253。
- en: Wang et al. (2016) Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer,
    Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.
    2016. Learning to reinforcement learn. *arXiv preprint arXiv:1611.05763* (2016).
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016) 简·X·旺、泽布·库尔特-纳尔逊、德鲁瓦·蒂鲁马拉、休伯特·索耶、乔尔·Z·雷博、雷米·穆诺斯、查尔斯·布伦德尔、达尔尚·库马然和马特·博特维尼克。2016。学习强化学习。*arXiv预印本
    arXiv:1611.05763*（2016）。
- en: Wang et al. (2021b) Li Wang, Binbin Jin, Zhenya Huang, Hongke Zhao, Defu Lian,
    Qi Liu, and Enhong Chen. 2021b. Preference-Adaptive Meta-Learning for Cold-Start
    Recommendation. In *Proceedings of the Thirtieth International Joint Conference
    on Artificial Intelligence, IJCAI-21*, Zhi-Hua Zhou (Ed.). International Joint
    Conferences on Artificial Intelligence Organization, 1607–1614. Main Track.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021b) 李旺、彬彬金、振亚黄、宏科赵、德福莲、齐刘和恩宏陈。2021b。基于偏好自适应的冷启动推荐元学习。见于*第三十届国际人工智能联合会议论文集，IJCAI-21*，朱志华（编辑）。国际人工智能联合会议组织，1607–1614。主会场。
- en: Wang et al. (2019) Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng
    Chua. 2019. Neural graph collaborative filtering. In *Proceedings of the 42nd
    international ACM SIGIR conference on Research and development in Information
    Retrieval*. 165–174.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) 向旺、向南何、萌旺、富力冯和达成蔡。2019。神经图协同过滤。见于*第42届国际ACM SIGIR信息检索研究与发展会议论文集*，165–174。
- en: Wei et al. (2020) Tianxin Wei, Ziwei Wu, Ruirui Li, Ziniu Hu, Fuli Feng, Xiangnan
    He, Yizhou Sun, and Wei Wang. 2020. Fast Adaptation for Cold-start Collaborative
    Filtering with Meta-learning. In *2020 IEEE International Conference on Data Mining
    (ICDM)*. IEEE, 661–670.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2020) 天鑫魏、子维吴、瑞瑞李、子牛胡、富力冯、向南何、艺洲孙和伟王。2020。基于元学习的冷启动协同过滤快速适应。见于*2020
    IEEE国际数据挖掘会议（ICDM）*。IEEE，661–670。
- en: Wei et al. (2022) Wei Wei, Chao Huang, Lianghao Xia, Yong Xu, Jiashu Zhao, and
    Dawei Yin. 2022. Contrastive meta learning with behavior multiplicity for recommendation.
    In *Proceedings of the Fifteenth ACM International Conference on Web Search and
    Data Mining*. 1120–1128.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) 魏伟、超黄、良浩夏、永徐、佳书赵和大伟尹。2022。具有行为多样性的对比元学习推荐。见于*第十五届ACM国际网页搜索与数据挖掘会议论文集*，1120–1128。
- en: Weiss et al. (2016) Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. 2016.
    A survey of transfer learning. *Journal of Big data* 3, 1 (2016), 1–40.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weiss et al. (2016) 卡尔·魏斯、塔吉·M·赫什戈夫塔尔和丁丁·旺。2016。迁移学习综述。*大数据杂志* 3，第1期（2016），1–40。
- en: Xia et al. (2021) Lianghao Xia, Yong Xu, Chao Huang, Peng Dai, and Liefeng Bo.
    2021. Graph meta network for multi-behavior recommendation. In *Proceedings of
    the 44th International ACM SIGIR Conference on Research and Development in Information
    Retrieval*. 757–766.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. (2021) 良浩夏、永徐、超黄、鹏戴和李丰博。2021。用于多行为推荐的图元网络。见于*第44届国际ACM SIGIR信息检索研究与发展会议论文集*，757–766。
- en: Xie et al. (2021) Ruobing Xie, Yalong Wang, Rui Wang, Yuanfu Lu, Yuanhang Zou,
    Feng Xia, and Leyu Lin. 2021. Long Short-Term Temporal Meta-learning in Online
    Recommendation. *arXiv preprint arXiv:2105.03686* (2021).
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等 (2021) Ruobing Xie, Yalong Wang, Rui Wang, Yuanfu Lu, Yuanhang Zou, Feng
    Xia, 和 Leyu Lin. 2021. 在线推荐中的长短期时间元学习。*arXiv 预印本 arXiv:2105.03686* (2021)。
- en: Xu et al. (2019) Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Jiajie
    Xu, Fuzhen Zhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph Contextualized
    Self-Attention Network for Session-based Recommendation. In *Proceedings of the
    Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI*.
    ijcai.org, 3940–3946.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2019) Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Jiajie
    Xu, Fuzhen Zhuang, Junhua Fang, 和 Xiaofang Zhou. 2019. 会话推荐的图上下文化自注意力网络。发表于 *第二十八届国际联合人工智能会议*，IJCAI。ijcai.org,
    3940–3946。
- en: Yao et al. (2019a) Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. 2019a.
    Hierarchically structured meta-learning. In *International Conference on Machine
    Learning*. PMLR, 7045–7054.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2019a) Huaxiu Yao, Ying Wei, Junzhou Huang, 和 Zhenhui Li. 2019a. 分层结构的元学习。发表于
    *国际机器学习会议*。PMLR, 7045–7054。
- en: Yao et al. (2019b) Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding,
    Ruirui Li, and Zhenhui Li. 2019b. Automated Relational Meta-learning. In *International
    Conference on Learning Representations*.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2019b) Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui
    Li, 和 Zhenhui Li. 2019b. 自动化关系元学习。发表于 *国际学习表征会议*。
- en: 'Yao et al. (2018) Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Yu-Feng
    Li, Wei-Wei Tu, Qiang Yang, and Yang Yu. 2018. Taking human out of learning applications:
    A survey on automated machine learning. *arXiv preprint arXiv:1810.13306* (2018).'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2018) Quanming Yao, Mengshuo Wang, Yuqiang Chen, Wenyuan Dai, Yu-Feng
    Li, Wei-Wei Tu, Qiang Yang, 和 Yang Yu. 2018. 将人类从学习应用中移除：关于自动化机器学习的调查。*arXiv 预印本
    arXiv:1810.13306* (2018)。
- en: Yin et al. (2019) Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine,
    and Chelsea Finn. 2019. Meta-Learning without Memorization. In *International
    Conference on Learning Representations*.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等 (2019) Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, 和 Chelsea
    Finn. 2019. 无需记忆的元学习。发表于 *国际学习表征会议*。
- en: 'Yin (2020) Wenpeng Yin. 2020. Meta-learning for few-shot natural language processing:
    A survey. *arXiv preprint arXiv:2007.09604* (2020).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin (2020) Wenpeng Yin. 2020. 少样本自然语言处理的元学习：一项调查。*arXiv 预印本 arXiv:2007.09604*
    (2020)。
- en: Yoon et al. (2018) Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua
    Bengio, and Sungjin Ahn. 2018. Bayesian model-agnostic meta-learning. *Advances
    in neural information processing systems* 31 (2018).
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoon 等 (2018) Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio,
    和 Sungjin Ahn. 2018. 贝叶斯模型无关元学习。*神经信息处理系统进展* 31 (2018)。
- en: Yu et al. (2021) Runsheng Yu, Yu Gong, Xu He, Yu Zhu, Qingwen Liu, Wenwu Ou,
    and Bo An. 2021. Personalized Adaptive Meta Learning for Cold-start User Preference
    Prediction. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 35. 10772–10780.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2021) Runsheng Yu, Yu Gong, Xu He, Yu Zhu, Qingwen Liu, Wenwu Ou, 和 Bo
    An. 2021. 用于冷启动用户偏好预测的个性化自适应元学习。发表于 *AAAI 人工智能会议论文集*，第 35 卷。10772–10780。
- en: 'Zhang et al. (2021b) Qianqian Zhang, Zhuoming Xu, Hanlin Liu, and Yan Tang.
    2021b. KGAT-SR: Knowledge-Enhanced Graph Attention Network for Session-based Recommendation.
    In *33rd IEEE International Conference on Tools with Artificial Intelligence,
    ICTAI*. IEEE, 1026–1033.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2021b) Qianqian Zhang, Zhuoming Xu, Hanlin Liu, 和 Yan Tang. 2021b.
    KGAT-SR：知识增强图注意力网络用于会话推荐。发表于 *第 33 届 IEEE 人工智能工具国际会议，ICTAI*。IEEE, 1026–1033。
- en: 'Zhang et al. (2019) Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep
    learning based recommender system: A survey and new perspectives. *ACM Computing
    Surveys (CSUR)* 52, 1 (2019), 1–38.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019) Shuai Zhang, Lina Yao, Aixin Sun, 和 Yi Tay. 2019. 基于深度学习的推荐系统：调查与新视角。*ACM
    计算机调查 (CSUR)* 52, 1 (2019), 1–38。
- en: 'Zhang et al. (2021a) Yin Zhang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang
    Yi, Lichan Hong, and Ed H Chi. 2021a. A Model of Two Tales: Dual Transfer Learning
    Framework for Improved Long-tail Item Recommendation. In *Proceedings of the Web
    Conference 2021*. 2220–2231.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2021a) Yin Zhang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan
    Hong, 和 Ed H Chi. 2021a. 两个故事的模型：改进长尾物品推荐的双重迁移学习框架。发表于 *2021年网络会议论文集*。2220–2231。
- en: Zhang et al. (2020) Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang,
    Yan Li, and Yongdong Zhang. 2020. How to retrain recommender system? A sequential
    meta-learning method. In *Proceedings of the 43rd International ACM SIGIR Conference
    on Research and Development in Information Retrieval*. 1479–1488.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang,
    Yan Li, 和 Yongdong Zhang. 2020. 如何重新训练推荐系统？一种序列元学习方法。发表于 *第43届国际ACM SIGIR信息检索研究与发展会议*。1479–1488。
- en: 'Zhao et al. (2019) Pengpeng Zhao, Haifeng Zhu, Yanchi Liu, Jiajie Xu, Zhixu
    Li, Fuzhen Zhuang, Victor S. Sheng, and Xiaofang Zhou. 2019. Where to Go Next:
    A Spatio-Temporal Gated Network for Next POI Recommendation. In *The Thirty-Third
    AAAI Conference on Artificial Intelligence*. AAAI Press, 5877–5884.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2019) Pengpeng Zhao, Haifeng Zhu, Yanchi Liu, Jiajie Xu, Zhixu
    Li, Fuzhen Zhuang, Victor S. Sheng, 和 Xiaofang Zhou. 2019. 下一步去哪儿：一种用于下一POI推荐的时空门控网络。发表于
    *第33届AAAI人工智能会议*。AAAI Press，5877–5884。
- en: Zheng et al. (2021) Yujia Zheng, Siyi Liu, Zekun Li, and Shu Wu. 2021. Cold-start
    Sequential Recommendation via Meta Learner. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 35. 4706–4713.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2021) Yujia Zheng, Siyi Liu, Zekun Li, 和 Shu Wu. 2021. 通过元学习者进行冷启动序列推荐。发表于
    *AAAI人工智能会议论文集*，第35卷。4706–4713。
- en: Zhou et al. (2018a) Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han
    Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018a. Deep Interest
    Network for Click-Through Rate Prediction. In *Proceedings of the 24th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining, KDD*. ACM, 1059–1068.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2018a) Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han
    Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, 和 Kun Gai. 2018a. 用于点击率预测的深度兴趣网络。发表于
    *第24届ACM SIGKDD国际知识发现与数据挖掘会议，KDD*。ACM，1059–1068。
- en: Zhou et al. (2018b) Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu,
    Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018b. Deep interest network
    for click-through rate prediction. In *Proceedings of the 24th ACM SIGKDD international
    conference on knowledge discovery & data mining*. 1059–1068.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2018b) Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu,
    Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, 和 Kun Gai. 2018b. 用于点击率预测的深度兴趣网络。发表于
    *第24届ACM SIGKDD国际知识发现与数据挖掘会议*。1059–1068。
- en: Zhu et al. (2022) Yunzheng Zhu, Ruchao Fan, and Abeer Alwan. 2022. Towards Better
    Meta-Initialization with Task Augmentation for Kindergarten-aged Speech Recognition.
    In *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*. IEEE, 8582–8586.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2022) Yunzheng Zhu, Ruchao Fan, 和 Abeer Alwan. 2022. 通过任务增强的更好元初始化用于学前儿童语音识别。发表于
    *ICASSP 2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。IEEE，8582–8586。
- en: 'Zhu et al. (2021a) Yongchun Zhu, Kaikai Ge, Fuzhen Zhuang, Ruobing Xie, Dongbo
    Xi, Xu Zhang, Leyu Lin, and Qing He. 2021a. Transfer-Meta Framework for Cross-domain
    Recommendation to Cold-Start Users. In *SIGIR ’21: The 44th International ACM
    SIGIR Conference on Research and Development in Information Retrieval, Virtual
    Event, Canada, July 11-15, 2021*, Fernando Diaz, Chirag Shah, Torsten Suel, Pablo
    Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 1813–1817. [https://doi.org/10.1145/3404835.3463010](https://doi.org/10.1145/3404835.3463010)'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2021a) Yongchun Zhu, Kaikai Ge, Fuzhen Zhuang, Ruobing Xie, Dongbo
    Xi, Xu Zhang, Leyu Lin, 和 Qing He. 2021a. 面向冷启动用户的跨领域推荐转移-元框架。发表于 *SIGIR ’21：第44届国际ACM
    SIGIR信息检索研究与发展会议，虚拟会议，加拿大，2021年7月11-15日*，Fernando Diaz, Chirag Shah, Torsten Suel,
    Pablo Castells, Rosie Jones, 和 Tetsuya Sakai (编者)。ACM，1813–1817。 [https://doi.org/10.1145/3404835.3463010](https://doi.org/10.1145/3404835.3463010)
- en: Zhu et al. (2020a) Yaohui Zhu, Chenlong Liu, and Shuqiang Jiang. 2020a. Multi-attention
    Meta Learning for Few-shot Fine-grained Image Recognition.. In *IJCAI*. 1090–1096.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2020a) Yaohui Zhu, Chenlong Liu, 和 Shuqiang Jiang. 2020a. 面向少样本细粒度图像识别的多重注意力元学习。发表于
    *IJCAI*。1090–1096。
- en: 'Zhu et al. (2021b) Yongchun Zhu, Yudan Liu, Ruobing Xie, Fuzhen Zhuang, Xiaobo
    Hao, Kaikai Ge, Xu Zhang, Leyu Lin, and Juan Cao. 2021b. Learning to Expand Audience
    via Meta Hybrid Experts and Critics for Recommendation and Advertising. In *KDD
    ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual
    Event, Singapore, August 14-18, 2021*, Feida Zhu, Beng Chin Ooi, and Chunyan Miao
    (Eds.). ACM, 4005–4013. [https://doi.org/10.1145/3447548.3467093](https://doi.org/10.1145/3447548.3467093)'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2021b) Yongchun Zhu, Yudan Liu, Ruobing Xie, Fuzhen Zhuang, Xiaobo
    Hao, Kaikai Ge, Xu Zhang, Leyu Lin, 和 Juan Cao. 2021b. 通过元混合专家和评论员学习扩展受众，用于推荐和广告。发表于
    *KDD ’21：第27届ACM SIGKDD知识发现与数据挖掘会议，虚拟会议，新加坡，2021年8月14-18日*，Feida Zhu, Beng Chin
    Ooi, 和 Chunyan Miao (编者)。ACM，4005–4013。 [https://doi.org/10.1145/3447548.3467093](https://doi.org/10.1145/3447548.3467093)
- en: Zhu et al. (2021c) Yongchun Zhu, Zhenwei Tang, Yudan Liu, Fuzhen Zhuang, Ruobing
    Xie, Xu Zhang, Leyu Lin, and Qing He. 2021c. Personalized Transfer of User Preferences
    for Cross-domain Recommendation. *CoRR* abs/2110.11154 (2021). arXiv:2110.11154
    [https://arxiv.org/abs/2110.11154](https://arxiv.org/abs/2110.11154)
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2021c) 宗春 Zhu, 振伟 Tang, 郁丹 Liu, 付震 Zhuang, 若冰 Xie, 旭 Zhang, 乐宇 Lin,
    和 青 He. 2021c. 用户偏好的个性化迁移用于跨领域推荐。*CoRR* abs/2110.11154 (2021)。arXiv:2110.11154
    [https://arxiv.org/abs/2110.11154](https://arxiv.org/abs/2110.11154)
- en: Zhu et al. (2021d) Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying
    Sun, Xu Zhang, Leyu Lin, and Juan Cao. 2021d. Learning to Warm Up Cold Item Embeddings
    for Cold-start Recommendation with Meta Scaling and Shifting Networks. *arXiv
    preprint arXiv:2105.04790* (2021).
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2021d) 宗春 Zhu, 若冰 Xie, 付震 Zhuang, 凯凯 Ge, 盈 Sun, 旭 Zhang, 乐宇 Lin,
    和 娟 Cao. 2021d. 利用元扩展和位移网络为冷启动推荐预热冷项目嵌入。*arXiv 预印本 arXiv:2105.04790* (2021)。
- en: Zhu et al. (2020b) Ziwei Zhu, Shahin Sefati, Parsa Saadatpanah, and James Caverlee.
    2020b. Recommendation for new users and new items via randomized training and
    mixture-of-experts transformation. In *Proceedings of the 43rd International ACM
    SIGIR Conference on Research and Development in Information Retrieval*. 1121–1130.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2020b) 子伟 Zhu, Shahin Sefati, 帕尔萨 Saadatpanah, 和 詹姆斯 Caverlee. 2020b.
    通过随机训练和专家混合转换进行新用户和新项目的推荐。见于 *第43届国际 ACM SIGIR 信息检索研究与开发会议论文集*。1121–1130。
- en: Zou et al. (2020) Lixin Zou, Long Xia, Yulong Gu, Xiangyu Zhao, Weidong Liu,
    Jimmy Xiangji Huang, and Dawei Yin. 2020. Neural interactive collaborative filtering.
    In *Proceedings of the 43rd International ACM SIGIR Conference on Research and
    Development in Information Retrieval*. 749–758.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. (2020) 李鑫 Zou, 龙 Xia, 玉龙 Gu, 祥宇 Zhao, 伟东 Liu, 吉米 Xiangji Huang, 和
    大伟 Yin. 2020. 神经交互式协同过滤。见于 *第43届国际 ACM SIGIR 信息检索研究与开发会议论文集*。749–758。
