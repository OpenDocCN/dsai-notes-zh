- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:47:51'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2202.13589] Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2202.13589](https://ar5iv.labs.arxiv.org/html/2202.13589)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Aoran Xiao^∗, Jiaxing Huang^∗, Dayan Guan, Xiaoqin Zhang, Shijian Lu,and Ling Shao
    Aoran Xiao and Jiaxing Huang are co-first authors. Aoran Xiao, Jiaxing Huang and
    Shijian Lu are with the School of Computer Science and Engineering, Nanyang Technological
    University, Singapore. Dayan Guan is with Mohamed bin Zayed University of Artificial
    Intelligence, United Arab Emirates. Xiaoqin Zhang is with Key Laboratory of Intelligent
    Informatics for Safety & Emergency of Zhejiang Province, Wenzhou University, China.
    Ling Shao is with UCAS-Terminus AI Lab, UCAS. Corresponding authors: Shijian Lu
    (shijian.lu@ntu.edu.sg) and Xiaoqin Zhang (zhangxiaoqinnan@gmail.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Point cloud data have been widely explored due to its superior accuracy and
    robustness under various adverse situations. Meanwhile, deep neural networks (DNNs)
    have achieved very impressive success in various applications such as surveillance
    and autonomous driving. The convergence of point cloud and DNNs has led to many
    deep point cloud models, largely trained under the supervision of large-scale
    and densely-labelled point cloud data. Unsupervised point cloud representation
    learning, which aims to learn general and useful point cloud representations from
    unlabelled point cloud data, has recently attracted increasing attention due to
    the constraint in large-scale point cloud labelling. This paper provides a comprehensive
    review of unsupervised point cloud representation learning using DNNs. It first
    describes the motivation, general pipelines as well as terminologies of the recent
    studies. Relevant background including widely adopted point cloud datasets and
    DNN architectures is then briefly presented. This is followed by an extensive
    discussion of existing unsupervised point cloud representation learning methods
    according to their technical approaches. We also quantitatively benchmark and
    discuss the reviewed methods over multiple widely adopted point cloud datasets.
    Finally, we share our humble opinion about several challenges and problems that
    could be pursued in the future research in unsupervised point cloud representation
    learning. A project associated with this survey has been built at [https://github.com/xiaoaoran/3d_url_survey](https://github.com/xiaoaoran/3d_url_survey).
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Point cloud, unsupervised representation learning, self-supervised learning,
    deep learning, transfer learning, 3D vision, pre-training, deep neural network
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3D acquisition technologies have experienced fast development in recent years.
    This can be witnessed by different 3D sensors that have become increasingly popular
    in both industrial and our daily lives such as LiDAR sensors in autonomous vehicles,
    RGB-D cameras in Kinect and Apple devices, 3D scanners in various reconstruction
    tasks, etc. Meanwhile, 3D data of different modalities such as meshes, point clouds,
    depth images and volumetric grids, which capture accurate geometric information
    for both objects and scenes, have been collected and widely applied in different
    areas such as autonomous driving, robotics, medical treatment, remote sensing,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/01c28ef2de4ec45f95014268bd339db7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The general pipeline of unsupervised representation learning on point
    clouds: Deep neural networks are first pre-trained with unannotated point clouds
    via unsupervised learning over certain pre-text tasks. The learned unsupervised
    point cloud representations are then transferred to various downstream tasks to
    provide network initialization, with which the pre-trained networks are fine-tuned
    with a small amount of annotated task-specific point cloud data.'
  prefs: []
  type: TYPE_NORMAL
- en: Point cloud as one source of ubiquitous and widely used 3D data can be directly
    captured with entry-level depth sensors before triangulating into meshes or converting
    to voxels. This makes it easily applicable to various 3D scene understanding tasks [[1](#bib.bib1)]
    such as 3D object detection and shape analysis, semantic segmentation, etc. With
    the advance of deep neural networks (DNNs), point cloud understanding has attracted
    increasing attention as observed by a large number of deep architectures and deep
    models developed in recent years [[2](#bib.bib2)]. On the other hand, effective
    training of deep networks requires large-scale human-annotated training data such
    as 3D bounding boxes for object detection and point-wise annotations for semantic
    segmentation, which are usually laborious and time-consuming to collect due to
    3D view changes and visual inconsistency between human perception and point cloud
    display. Efficient collection of large-scale annotated point clouds has become
    one bottleneck for effective design, evaluations, and deployment of deep networks
    while handling various real-world tasks [[3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98272c6d65096f2f15aa86424da5b083.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Taxonomy of existing unsupervised methods in point cloud representation
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised representation learning (URL), which aims to learn robust and
    general feature representations from unlabelled data, has recently been studied
    intensively for mitigating the laborious and time-consuming data annotation challenge.
    As Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") shows, URL works in a similar way
    to pre-training which learns useful knowledge from unlabelled data and transfers
    the learned knowledge to various downstream tasks [[4](#bib.bib4)]. More specifically,
    URL can provide useful network initialization with which well-performing network
    models can be trained with a small amount of labelled and task-specific training
    data without suffering from much over-fitting as compared with training from random
    initialization. URL can thus help reduce training data and annotations which has
    demonstrated great effectiveness in the areas of natural language processing (NLP) [[5](#bib.bib5),
    [6](#bib.bib6)], 2D computer vision [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10)], etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to URL from other types of data such as texts and 2D images, URL of
    point clouds has recently attracted increasing attention in the computer vision
    research community. A number of URL techniques have been reported which are typically
    achieved by designing different pre-text tasks such as 3D object reconstruction [[11](#bib.bib11)],
    partial object completion [[12](#bib.bib12)], 3D jigsaws solving [[13](#bib.bib13)],
    etc. However, URL of point clouds still lags far behind as compared with its counterparts
    in NLP and 2D computer vision tasks. For the time being, training from scratch
    on various target new data is still the prevalent approach in most existing 3D
    scene understanding development. At the other end, URL from point cloud data is
    facing increasing problems and challenges, largely due to the lack of large-scale
    and high-quality point cloud data, unified deep backbone architectures, generalizable
    technical approaches, as well as comprehensive public benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, URL for point clouds is still short of systematic survey that
    can offer a clear big picture about this new yet challenging task. To fill up
    this gap, this paper presents a comprehensive survey on the recent progress in
    unsupervised point cloud representation learning from the perspective of datasets,
    network architectures, technical approaches, performance benchmarking, and future
    research directions. As shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣
    Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A
    Survey"), we broadly group existing methods into four categories based on their
    pretext tasks, including URL methods using data generation, global and local contexts,
    multimodality data and local descriptors, more details to be discussed in the
    ensuing subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The major contributions of this work are threefold:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It presents a comprehensive review of the recent development in unsupervised
    point cloud representation learning. To the best of our knowledge, it is the first
    survey that provides an overview and big picture for this exciting research topic.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It studies the most recent progress of unsupervised point cloud representation
    learning, including a comprehensive benchmarking and discussion of existing methods
    over multiple public datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It shares several research challenges and potential research directions that
    could be pursued in unsupervised point cloud representation learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of this survey is organized as follows: In Section [2](#S2 "2 Background
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), we introduce background knowledge of unsupervised point cloud learning
    including term definition, common tasks of point cloud understanding and relevant
    surveys to this work. Section [3](#S3 "3 Point cloud datasets ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey") introduces
    widely-used datasets and their characteristics. Section [4](#S4 "4 Common deep
    architectures ‣ Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey") introduces commonly used deep point cloud architectures with
    typical models that are frequently used for point cloud URL. In Section [5](#S5
    "5 Unsupervised point cloud representation learning ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey") we systematically
    review the methods for point cloud URL. Section [6](#S6 "6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") summarizes and compares the performances of existing methods on multiple
    benchmark datasets. At last, we list several promising future directions for unsupervised
    point cloud representation learning in Section [7](#S7 "7 Future direction ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Basic concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first define all relevant terms and concepts that are to be used in the ensuing
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Point cloud data: A point cloud $P$ is a set of vectors $P=\{p_{1},...,p_{N}\}$
    where each vector represents one point $p_{i}=[C_{i},A_{i}]$. Here, $C_{i}\in\mathbf{R}^{1\times
    3}$ refers to 3D coordinate $(x_{i},y_{i},z_{i})$ of the point, and $A_{i}$ refers
    to feature attributes of the point such as RGB values, LiDAR intensity, normal
    values, etc., which are optional and variational depending on 3D sensors as well
    as applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning: Under the paradigm of deep learning, supervised learning
    aims to train deep network models by using labelled training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised learning: Unsupervised learning aims to train networks by using
    unlabelled training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised representation learning: URL is a subset of unsupervised learning.
    It aims to learn meaningful representations from data without using any data labels/annotations,
    where the learned representations can be transferred to different downstream tasks.
    Some literature alternatively uses the term “self-supervised learning”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Semi-supervised learning: In semi-supervised learning, deep networks are trained
    with a small amount of labelled data and a large amount of unlabelled data. It
    aims to mitigate data annotation constraints by learning from a small amount of
    labelled data and a large amount of unlabelled data that have similar distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-training: Network pre-training learns with certain pre-text tasks over
    other datasets. The learned parameters are often employed for model initialization
    for further fine-tuning with various task-specific data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer learning: Transfer learning aims to transfer knowledge across tasks,
    modalities or datasets. A typical scenario related to this survey is to perform
    unsupervised learning for pre-training for transferring the learned knowledge
    from unlabelled data to various downstream networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Common 3D understanding tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This subsection introduces common 3D understanding tasks including object-level
    tasks in object classification and object part segmentation and scene-level tasks
    in 3D object detection, semantic segmentation and instance segmentation. These
    tasks have been widely adopted to evaluate the quality of point cloud representations
    that are learned via various unsupervised learning methods, which will be discussed
    in detail in Section [6](#S6 "6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0eec5bef1faa87c2916fcbfe84a65ca9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of object part segmentation: The first row shows a few
    object samples including airplane, motorcycle, and table from the ShapeNetPart
    dataset [[14](#bib.bib14)]. The second row shows segmentation ground truth with
    different parts as highlighted by different colors.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Object classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Object classification aims to classify point cloud objects into a number of
    pre-defined categories. Two evaluation metrics are most frequently used: The overall
    Accuracy (OA) represents the averaged accuracy for all instances in the test set;
    The mean class accuracy (mAcc) represents the mean accuracy of all object classes
    for the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Object part segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Object part segmentation is an important task for point cloud representation
    learning. It aims to assign a part category label (e.g., airplane wing, table
    leg, etc.) to each point as illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Common
    3D understanding tasks ‣ 2 Background ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). The mean Intersection over Union
    (mIoU) [[15](#bib.bib15)] is the most widely adopted evaluation metric. For each
    instance, IoU is computed for each part belonging to that object category. The
    mean of the part IoUs represents the IoU of that object instance. The overall
    IoU is computed as the average of IoUs over all test instances while category-wise
    IoU (or class IoU) is calculated as the mean over instances under that category.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 3D object detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '3D object detection on point clouds is a crucial and indispensable task for
    many real-world applications, such as autonomous driving and domestic robots.
    The task aims to localize objects in the 3D space, i.e. 3D object bounding boxes
    as illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ 2.2.3 3D object detection ‣ 2.2
    Common 3D understanding tasks ‣ 2 Background ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). The average precision (AP) metric
    has been widely used for evaluations in 3D object detection [[16](#bib.bib16),
    [17](#bib.bib17)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/68f6f21346e738050cbcefdd3599f402.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) ScanNet-V2 dataset
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da28e0a769580951dc8da07370f0253f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) KITTI dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Illustration of 3D bounding boxes in point cloud object detection:
    The two graphs show 3D bounding boxes in datasets ScanNet-V2 [[18](#bib.bib18)]
    and KITTI [[19](#bib.bib19)] which are cropped from [[16](#bib.bib16)] and [[20](#bib.bib20)],
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7db22cb8112a95c7753db79dfb191cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) A raw sample
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f57619e9a8e6953799617b4c9b0794c8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Semantic annotations
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Illustration of semantic point cloud segmentation: For the point
    cloud sample from S3DIS [[21](#bib.bib21)] on the left, the graph on the right
    shows the corresponding ground truth where different categories are highlighted
    by different colors.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 3D semantic segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '3D semantic segmentation on point clouds is another critical task for 3D understanding
    as illustrated in Fig. [5](#S2.F5 "Figure 5 ‣ 2.2.3 3D object detection ‣ 2.2
    Common 3D understanding tasks ‣ 2 Background ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). Different from the object part
    segmentation that segments point cloud objects, 3D semantic segmentation aims
    to assign a category label to each point in scene-level point clouds with much
    higher complexity. The widely adopted evaluation metrics includes OA, mIoU over
    semantic categories and mAcc.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 3D instance segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '3D instance segmentation aims to detect and delineate each distinct object
    of interest in scene-level point clouds as illustrated in Fig. [6](#S2.F6 "Figure
    6 ‣ 2.2.5 3D instance segmentation ‣ 2.2 Common 3D understanding tasks ‣ 2 Background
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"). On top of semantic segmentation that considers the semantic category
    only, instance segmentation assigns each object a unique identity. Mean Average
    Precision (mAP) has been widely adopted for the quantitative evaluation of this
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73c2e1ef70510ad21722234db5bb0018.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) A raw sample
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/374e39ad295bfb345b3dc9c9b94e2450.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Instance annotations
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Illustration of instance segmentation on point clouds: For the point
    cloud sample from ScanNet-V2 [[18](#bib.bib18)] on the left, the graph on the
    right shows the corresponding ground truth with different instances highlighted
    by different colors.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Relevant surveys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, this paper is the first survey that reviews unsupervised
    point cloud learning comprehensively. Several relevant but different surveys have
    been performed. For example, several papers reviewed recent advances for deep
    supervised learning on point clouds: Ioannidou et al. [[22](#bib.bib22)] reviewed
    deep learning approaches on 3D data; Xie et al. [[23](#bib.bib23)] provided a
    literature review on point cloud segmentation task; Guo et al. [[2](#bib.bib2)]
    provided a comprehensive and detailed survey on deep learning of point cloud for
    multiple tasks including classification, detection, tracking, and segmentation.
    In addition, several works reviewed unsupervised representation learning on other
    data modalities: Jing et al. [[24](#bib.bib24)] introduced advances on unsupervised
    representation learning in 2D computer vision; Liu et al. [[25](#bib.bib25)] looked
    into latest progress about unsupervised representation learning methods in 2D
    computer vision, NLP, and graph learning; Qi et al. [[26](#bib.bib26)] introduced
    recent progress on small data learning including unsupervised- and semi-supervised
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Point cloud datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE I: Summary of commonly used datasets for training and evaluations in
    prior URL studies with point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | #Samples | #Classes | Type | Representation | Label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI [[19](#bib.bib19)] | 2013 | 15K frames | 8 | Outdoor driving | RGB
    & LiDAR | Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| ModelNet10 [[27](#bib.bib27)] | 2015 | 4,899 objects | 10 | Synthetic object
    | Mesh | Object category label |'
  prefs: []
  type: TYPE_TB
- en: '| ModelNet40 [[27](#bib.bib27)] | 2015 | 12,311 objects | 40 | Synthetic object
    | Mesh | Object category label |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeNet [[14](#bib.bib14)] | 2015 | 51,190 objects | 55 | Synthetic object
    | Mesh | Object/part category label |'
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D [[28](#bib.bib28)] | 2015 | 5K frames | 37 | Indoor scene | RGB-D
    | Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| S3DIS [[21](#bib.bib21)] | 2016 | 272 scans | 13 | Indoor scene | RGB-D |
    Point category label |'
  prefs: []
  type: TYPE_TB
- en: '| ScanNet [[18](#bib.bib18)] | 2017 | 1,513 scans | 20 | Indoor scene | RGB-D
    & mesh | Point category label & Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| ScanObjectNN [[29](#bib.bib29)] | 2019 | 2,902 objects | 15 | Real-world
    object | Points | Object category label |'
  prefs: []
  type: TYPE_TB
- en: '| ONCE [[30](#bib.bib30)] | 2021 | 1M scenes | 5 | Outdoor driving | RGB &
    LiDAR | Bounding box |'
  prefs: []
  type: TYPE_TB
- en: 'In this section, we summarize the commonly used datasets for training and evaluating
    unsupervised point cloud representation learning. As listed in Table [I](#S3.T1
    "TABLE I ‣ 3 Point cloud datasets ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey"), existing work learns unsupervised point
    cloud representations mainly from 1) synthetic object datasets including ModelNet [[27](#bib.bib27)]
    and ShapeNet [[14](#bib.bib14)], or 2) real scene datasets including ScanNet [[18](#bib.bib18)]
    and KITTI [[19](#bib.bib19)]. In addition, various tasks-specific datasets have
    been collected which can be used for fine-tuning downstream models, such as ScanObjectNN [[29](#bib.bib29)],
    ModelNet40 [[27](#bib.bib27)], and ShapeNet [[14](#bib.bib14)] for point cloud
    classification, ShapeNetPart [[14](#bib.bib14)] for part segmentation, S3DIS [[21](#bib.bib21)],
    ScanNet [[18](#bib.bib18)], or Synthia4D [[31](#bib.bib31)] for semantic segmentation,
    indoor datasets SUNRGB-D [[28](#bib.bib28)] and ScanNet [[18](#bib.bib18)] as
    well as outdoor dataset ONCE [[30](#bib.bib30)] for object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ModelNet10/ModelNet40 [[27](#bib.bib27)]: ModelNet is a synthetic
    object-level dataset for 3D classification. The original ModelNet provides CAD
    models represented by vertices and faces. Point clouds are generated by sampling
    from the models uniformly. ModelNet40 contains 13,834 objects of 40 categories,
    among which 9,843 objects form the training set and the rest form the test set.
    ModelNet10 consists of 3,377 samples of 10 categories, which are split into 2,468
    training samples and 909 testing samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ShapeNet [[14](#bib.bib14)]: ShapeNet contains synthetic 3D objects
    of 55 categories. It was curated by collecting CAD models from online open-sourced
    3D repositories. Similar to ModelNet, synthetic objects in ShapeNet are complete,
    aligned, and with no occlusion or background. Its extension ShapeNetPart has 16,881
    objects of 16 categories and is represented by point clouds. Each object consists
    of 2 to 6 parts, and in total there are 50 part categories in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ScanObjectNN [[29](#bib.bib29)]: ScanObjectNN is a real object-level
    dataset, where 2,902 3D point cloud objects of 15 categories are constructed from
    the scans captured in real indoor scenes. Different from synthetic object datasets,
    point cloud objects in ScanObjectNN are noisy (including background points, occlusions,
    and holes in objects) and not axis-aligned.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$S3DIS [[21](#bib.bib21)]: Stanford Large-Scale 3D Indoor Spaces (S3DIS)
    dataset contains over 215 million points scanned from 6 large-scale indoor areas
    in 3 office buildings, where each area is 6,000 square meters. The scans are represented
    as point clouds with point-wise semantic labels of 13 object categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ScanNet-V2 [[18](#bib.bib18)]: ScanNet-V2 is an RGB-D video dataset
    containing 2.5 million views in more than 1500 scans, which are captured in indoor
    scenes such as offices and living rooms and annotated with 3D camera poses, surface
    reconstructions, as well as semantic and instance labels for segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$SUN RGB-D [[28](#bib.bib28)]: SUN RGB-D dataset is a collection of
    single view RGB-D images collected from indoor environments. There are in total
    10,335 RGB-D images annotated with amodal, and 3D oriented object bounding boxes
    of 37 categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$KITTI [[19](#bib.bib19)]: KITTI is a pioneer outdoor dataset providing
    dense point clouds from a LiDAR sensor together with other modalities including
    front-facing stereo images and GPS/IMU data. It provides 200k 3D boxes over 22
    scenes for 3D object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ONCE [[30](#bib.bib30)]: ONCE dataset has 1 million LiDAR scenes and
    7 million corresponding camera images. There are 581 sequences in total, where
    560 sequences are unlabelled and used for unsupervised learning, and 10 sequences
    are annotated and used for testing. It provides an unsupervised learning benchmark
    for object detection in outdoor environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The publicly available datasets for URL of point clouds are still limited in
    both data size and scene variety, especially compared with the image and text
    datasets that have been used for 2D computer vision and NLP research. For example,
    there are 800 million words in BooksCorpus and 2,500 million words in English
    Wikipedia that is able to provide comprehensive data sources for unsupervised
    representation learning in NLP [[32](#bib.bib32)]; ImageNet [[33](#bib.bib33)]
    has more than 10 million images for unsupervised visual representation learning.
    Large-scale and high-quality point cloud data are highly demanded for future research
    on this topic, and we provide a detailed discussion of this issue in Section [7](#S7
    "7 Future direction ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Common deep architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the last decade, deep learning has been playing a more important role in
    point-cloud processing and understanding. This can be observed by the abundance
    of deep architectures that have been developed in recent years. Different from
    traditional 3D vision that transforms point clouds to structures like Octrees
    [[34](#bib.bib34)] or Hashed Voxel Lists [[35](#bib.bib35)], deep learning favors
    more amenable structures for differentiability and/or efficient neural processing
    which have achieved very impressive performance over various 3D tasks.
  prefs: []
  type: TYPE_NORMAL
- en: At the other end, DNN-based point cloud processing and understanding lags far
    behind as compared with its counterparts in NLP and 2D computer vision. This is
    especially true for the task of unsupervised representation learning, largely
    due to the lack of regular representations in point cloud data. Specifically,
    word embeddings and 2D images have regular and well-defined structures, but point
    clouds represented by unordered point sets have no such universal and structural
    data format.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduce deep architectures that have been explored for
    the URL of point clouds. Deep learning for point clouds achieved significant progress
    during the last decade and we see the abundance of 3D deep architectures and 3D
    models being proposed. However, we do not have universal and ubiquitous “3D backbones”
    like VGG [[36](#bib.bib36)] or ResNet [[37](#bib.bib37)] in 2D computer vision.
    We thus focus on those frequently used architectures in the URL of point clouds
    in this survey. For clarity of description, we group them into five categories
    broadly, namely, point-based architectures, graph-based architectures, sparse
    voxel-based architectures, spatial CNN-based architectures, and Transformer-based
    architectures. Note other deep architectures also exist for various 3D tasks as
    discussed in [[2](#bib.bib2)], such as projection-based networks [[38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)],
    recurrent neural networks [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)],
    3D capsule networks [[47](#bib.bib47)], etc. However, they were not often employed
    for the URL task and thus are not detailed in this survey.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Point-based deep architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Point-based networks were designed to process raw point clouds directly without
    point data transformations beforehand. Independent point features are usually
    first extracted by stacking networks with Multi-Layer Perceptrons (MLPs), which
    are then aggregated into global features with symmetric aggregation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'PointNet [[15](#bib.bib15)] is a pioneer point-based network as shown in Fig.
    [7](#S4.F7 "Figure 7 ‣ 4.1 Point-based deep architectures ‣ 4 Common deep architectures
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"). It stacks several MLP layers to learn point-wise features independently
    and forwards the learned features to a max-pooling layer to extract global features
    for permutation invariance. To improve PointNet, Qi et al. proposed PointNet++
    [[48](#bib.bib48)] to learn local geometry details from the neighborhood of points,
    where the set abstraction level includes sampling layer, grouping layer, and PointNet
    layer for learning local and hierarchical features. PointNet++ achieves great
    success in multiple 3D tasks including object classification and semantic segmentation.
    By taking PointNet++ as the backbone, Qi et al. designed VoteNet [[16](#bib.bib16)],
    the first point-based 3D object detection network. VoteNet adopts the Hough voting
    strategy, which generates new points around object centers and groups them with
    the surrounding points to produce 3D box proposals.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/97ae10bcc632279441afaa90cbf683d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A simplified architecture of PointNet [[15](#bib.bib15)] for point
    cloud object classification, where parameters $n$ and $m$ denote point number
    and feature dimension, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Graph-based deep architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Graph-based networks treat point clouds as graphs in Euclidean space with vertexes
    being points and edges capturing neighboring point relations as illustrated in
    Fig. [8](#S4.F8 "Figure 8 ‣ 4.2 Graph-based deep architectures ‣ 4 Common deep
    architectures ‣ Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey"). It works with graph convolution where filter weights are
    conditioned on edge labels and dynamically generated for individual input samples.
    This allows to reduce the degrees of freedom in the learned models by enforcing
    weight sharing and extracting localized features that can capture dependencies
    among neighboring points.'
  prefs: []
  type: TYPE_NORMAL
- en: The Dynamic Graph Convolutional Neural Network (DGCNN) [[49](#bib.bib49)] is
    a typical graph-based network that has been frequently used for URL for point
    clouds. It is stacked with a graph convolution module named EdgeConv that performs
    convolution on graph dynamically in the feature space. DGCNN integrates EdgeConv
    into the basic version of PointNet structures for learning global shape properties
    and semantic characteristics for point cloud understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d562b2a1a6505d9ce8b4c5298d6a749.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Schematic depiction of graph convolutional network (GCN): Each graph
    consists of multiple vertexes representing points $X_{i}$ or features $Z_{i}$
    (highlighted by circular dots), as well as edges connecting the vertexes representing
    point relations (shown as black lines). $C$ denotes input channels, $F$ denotes
    output feature dimensions, and $Y_{i}$ denotes labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Sparse voxel-based deep architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The voxel-based architecture voxelizes point clouds into 3D grids before applying
    3D CNN on the volumetric representations. Due to the sparseness of point cloud
    data, It often involves huge computation redundancy or sacrifices the representation
    accuracy while processing a large number of points. To overcome this constrain,
     [[50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)] adopt
    sparse tensor as the basic unit where point clouds are represented with a data
    list and an index list. Unlike standard convolution operation that employs sliding
    windows (im2col function in PyTorch and TensorFlow) to build the computational
    pipeline, sparse convolution [[50](#bib.bib50)] collects all atomic operations
    including convolution kernel elements and saves them in a Rulebook as computation
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Choy et al. proposed Minkowski Engine [[51](#bib.bib51)] that introduces
    generalized sparse convolution and an auto-differentiation library for sparse
    tensors. On top of that, Xie et al. [[54](#bib.bib54)] adopted a unified U-Net [[55](#bib.bib55)]
    architecture and built a backbone network (SR-UNet as shown in Fig. [9](#S4.F9
    "Figure 9 ‣ 4.3 Sparse voxel-based deep architectures ‣ 4 Common deep architectures
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey")) for unsupervised pre-training. The learned encoder can be transferred
    to different downstream tasks such as classification, object detection, and semantic
    segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a2cd74a19831c9355ef79c38e606b91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: An illustration of SR-UNet [[54](#bib.bib54)] that adopts a unified
    U-Net [[55](#bib.bib55)] architecture for sparse convolution. The graph is reproduced
    based on [[54](#bib.bib54)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Spatial CNN-based deep architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spatial CNN-based networks have been developed to extend the capabilities of
    regular-grid CNNs to analyze irregularly spaced point clouds. They can be divided
    into continuous and discrete convolutional networks according to the convolutional
    kernels [[2](#bib.bib2)]. As Fig. [10](#S4.F10 "Figure 10 ‣ 4.4 Spatial CNN-based
    deep architectures ‣ 4 Common deep architectures ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") shows, continuous convolutional
    networks define the convolutional kernels in a continuous space, where the weights
    of neighboring points are determined by their spatial distribution relative to
    the center point. Differently, discrete convolutional networks operate on regular
    grids and define the convolutional kernels in a discrete space where neighboring
    points have fixed offsets relative to the center point. One typical example of
    continuous convolution models is RS-CNN [[56](#bib.bib56)] which has been widely
    adopted for URL of point clouds. Specifically, RS-CNN extracts geometric topology
    relations among local centers with their surrounding points, and it learns dynamic
    weights for convolutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fc717c3f21c36980ab5666688fd3b575.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: An illustration of 3D spatial convolution including continuous and
    discrete convolutions. Parameters $p$ and $q_{i}$ denote the center point and
    its neighboring points, respectively. The graph is reproduced based on [[2](#bib.bib2)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Transformer-based deep architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce546d4b1ec2c4f089f76dcbb85f2857.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The architecture of point cloud Transformer that was used for unsupervised
    pre-training in Point-BERT [[57](#bib.bib57)]. More network details can be found
    in [[57](#bib.bib57)]. The figure is reproduced based on [[57](#bib.bib57), [58](#bib.bib58)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the last few years, Transformers have made astounding progress in the
    research areas of NLP [[59](#bib.bib59), [32](#bib.bib32)] and 2D image processing [[58](#bib.bib58),
    [60](#bib.bib60)] due to their structural superiority and versatility. They have
    also been introduced into the area of point cloud processing [[61](#bib.bib61),
    [57](#bib.bib57)] recently. Fig. [11](#S4.F11 "Figure 11 ‣ 4.5 Transformer-based
    deep architectures ‣ 4 Common deep architectures ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") shows a standard Transformer architecture
    for URL of point clouds [[57](#bib.bib57)], which contains a stack of Transformer
    blocks [[59](#bib.bib59)] and each block consists of a multi-head self-attention
    layer and a feed-forward network. The unsupervised pre-trained Transformer encoder
    can be used for fine-tuning downstream tasks such as object classification and
    semantic segmentation, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Unsupervised point cloud representation learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we review existing URL methods for point clouds. As shown
    in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"), we broadly group existing methods
    into four categories according to their pretext tasks, including generative-based
    methods, context-based methods, multiple modal-based methods, and local descriptor-based
    methods. With this taxonomy, we sort out existing methods and systematically introduce
    them in the ensuing subsections of this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Generation-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE II: Summary of generation-based methods for unsupervised representation
    learning of point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Published in | Category | Contribution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| VConv-DAE [[62](#bib.bib62)] | ECCV 2016 | Completion | Learning by predicting
    missing parts in 3D grids |'
  prefs: []
  type: TYPE_TB
- en: '| TL-Net [[63](#bib.bib63)] | ECCV 2016 | Reconstruction | Learning by 3D generation
    and 2D prediction |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-GAN [[64](#bib.bib64)] | NeurIPS 2016 | GAN | Pioneer GAN for 3D voxels
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-DescriptorNet [[65](#bib.bib65)] | CVPR 2018 | Completion | learning with
    energy-based models for point cloud completion |'
  prefs: []
  type: TYPE_TB
- en: '| FoldingNet [[66](#bib.bib66)] | CVPR 2018 | Reconstruction | learning by
    folding 3D object surfaces |'
  prefs: []
  type: TYPE_TB
- en: '| SO-Net [[67](#bib.bib67)] | CVPR 2018 | Reconstruction | Performing hierarchical
    feature extraction on individual points and SOM nodes |'
  prefs: []
  type: TYPE_TB
- en: '| Latent-GAN [[68](#bib.bib68)] | ICML 2018 | GAN | Pioneer GAN for raw point
    clouds and latent embeddings |'
  prefs: []
  type: TYPE_TB
- en: '| MRT [[69](#bib.bib69)] | ECCV 2018 | Reconstruction | A new point cloud autoencoder
    with multi-grid architecture |'
  prefs: []
  type: TYPE_TB
- en: '| VIP-GAN [[70](#bib.bib70)] | AAAI 2019 | GAN | Learning by solving multi-views
    inter-prediction tasks for objects |'
  prefs: []
  type: TYPE_TB
- en: '| G-GAN [[11](#bib.bib11)] | ICLR 2019 | GAN | Pioneer GAN with graph convolution
    for point clouds |'
  prefs: []
  type: TYPE_TB
- en: '| 3DCapsuleNet [[47](#bib.bib47)] | CVPR 2019 | Reconstruction | Learning with
    3D point-capsule network |'
  prefs: []
  type: TYPE_TB
- en: '| L2G-AE [[71](#bib.bib71)] | ACM MM 2019 | Reconstruction | Learning by global
    and local reconstruction of point clouds |'
  prefs: []
  type: TYPE_TB
- en: '| MAP-VAE [[72](#bib.bib72)] | ICCV 2019 | Reconstruction | Learning by 3D
    reconstruction and half-to-half prediction |'
  prefs: []
  type: TYPE_TB
- en: '| PointFlow [[73](#bib.bib73)] | ICCV 2019 | Reconstruction | Learning by modeling
    point clouds as a distribution of distributions |'
  prefs: []
  type: TYPE_TB
- en: '| PDL [[74](#bib.bib74)] | CVPR 2020 | reconstruction | A probabilistic framework
    for point distribution learning |'
  prefs: []
  type: TYPE_TB
- en: '| GraphTER [[75](#bib.bib75)] | CVPR 2020 | Reconstruction | Proposed a graph-based
    autoencoder for point clouds |'
  prefs: []
  type: TYPE_TB
- en: '| SA-Net [[76](#bib.bib76)] | CVPR 2020 | Completion | Learning by completing
    point cloud objects with a skip-attention mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| PointGrow [[77](#bib.bib77)] | WACV 2020 | Reconstruction | An autoregressive
    model that can recurrently generate point cloud samples. |'
  prefs: []
  type: TYPE_TB
- en: '| PSG-Net [[78](#bib.bib78)] | ICCV 2021 | Reconstruction | Learning by reconstruct
    point cloud objects with seed generation |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | ICCV 2021 | Completion | Learning by completing
    occluded point cloud objects |'
  prefs: []
  type: TYPE_TB
- en: '| Point-Bert [[57](#bib.bib57)] | CVPR 2022 | Reconstruction | Learning for
    Transformers by recovering masked tokens of 3D objects |'
  prefs: []
  type: TYPE_TB
- en: '| Point-MAE [[79](#bib.bib79)] | ECCV 2022 | Reconstruction | Autoencoder transformer
    recovers masked parts from input data |'
  prefs: []
  type: TYPE_TB
- en: '| Point-M2AE [[80](#bib.bib80)] | NeurIPS 2022 | Reconstruction | Masked autoencoder
    with hierarchical point cloud encoding and reconstruction. |'
  prefs: []
  type: TYPE_TB
- en: 'Generation-based URL methods for point clouds involve the process of generating
    point cloud objects in training. According to the employed pre-text tasks, they
    can be further grouped into four subcategories including point cloud self-reconstruction
    (for generating point cloud objects that are the same as the input), point cloud
    GAN (for generating fake point cloud objects), point cloud up-sampling (for generating
    objects with denser point clouds but similar shapes) and point cloud completion
    (for predicting missing parts from incomplete point cloud objects). The ground
    truth of these URL methods are point clouds themselves. Hence, these methods require
    no human annotations and can learn in an unsupervised manner. Table [II](#S5.T2
    "TABLE II ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") shows a list of generation-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Learning through point cloud self-reconstruction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Networks for self-reconstruction usually encode point cloud samples into representation
    vectors and decode them back to the original input data, where shape information
    and semantic structures are extracted during this process. It belongs to one typical
    URL approach since it does not involve any human annotations. One representative
    network is autoencoder [[81](#bib.bib81)] which has an encoder network and a decoder
    network as illustrated in Fig. [12](#S5.F12 "Figure 12 ‣ 5.1.1 Learning through
    point cloud self-reconstruction ‣ 5.1 Generation-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). The encoder compresses and encodes
    a point cloud object into a low-dimensional embedding vector (i.e., codeword) [[66](#bib.bib66)],
    which is then decoded back to the 3D space by the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d3a4a1377d70663115269ea1aaac5f1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: An illustration of AutoEncoder in unsupervised point cloud representation
    learning: The Encoder learns to represent a point cloud object by a Codeword vector
    while the Decoder reconstructs the Output Object from the Codeword.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is optimized by forcing the final output to be the same as the input.
    During this process, the encoding is validated and learns by attempting to regenerate
    the input from the encoding whereas the autoencoder learns low-dimension representations
    by training the network to ignore insignificant data (“noise”) [[82](#bib.bib82)].
    Permutation invariant losses [[83](#bib.bib83)] are widely adopted as the training
    objective to describe how the input and output point cloud objects are similar
    to each other. They can be measured by Chamfer Distance $L_{\mathrm{CD}}$ or Earth
    Mover’s Distance $L_{\mathrm{EMD}}$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{\mathrm{CD}}=\sum_{p\in P}\min_{p^{\prime}\in P^{\prime}}{&#124;&#124;p-p^{\prime}&#124;&#124;}^{2}+\sum_{p^{\prime}\in
    P^{\prime}}\min_{p\in P}{&#124;&#124;p^{\prime}-p&#124;&#124;}^{2}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $L_{\mathrm{EMD}}=\min_{\phi:P\rightarrow P^{\prime}}\sum_{x\in P}{&#124;&#124;p-\phi(p)_{2}&#124;&#124;}_{2}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Where $P$ and $P^{\prime}$ denote input and output point clouds of the same
    size, $\phi:P\rightarrow P^{\prime}$ is bijection, and $p$ & $p^{\prime}$ are
    points.
  prefs: []
  type: TYPE_NORMAL
- en: Self-reconstruction has been one of the most widely adopted pre-text tasks for
    URL from point clouds over the last decade. By assuming that point cloud representations
    should be generative in 3D space and predictable from 2D space, Girdhar et al.
    proposed TL-Net [[63](#bib.bib63)] that employs a 3D autoencoder to reconstruct
    3D volumetric grids and a 2D convolutional network to learn 2D features from the
    projected images. Yang et al. designed FoldingNet [[66](#bib.bib66)] that introduces
    a folding-based decoder that deforms a canonical 2D grid onto the underlying 3D
    object surface of a point cloud object. Li et al. proposed SO-Net [[67](#bib.bib67)]
    that introduces self-organizing map to learn hierarchical features of point clouds
    via self-reconstruction. Zhao et al. [[47](#bib.bib47)] extended the capsule network [[84](#bib.bib84)]
    into 3D point cloud processing and the designed 3D capsule network can learn generic
    representations from unstructured 3D data. Gao et al. [[75](#bib.bib75)] proposed
    a graph-based autoencoder that can learn intrinsic patterns of point-cloud structures
    under both global and local transformations. Chen et al. [[85](#bib.bib85)] designed
    a deep autoencoder that exploits graph topology inference and filtering for extracting
    compact representations from 3D point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: Several studies explore global and local geometries to learn robust representations
    from point cloud objects [[71](#bib.bib71), [72](#bib.bib72)]. For example, [[71](#bib.bib71)]
    introduces hierarchical self-attention in the encoder for information aggregation,
    and a recurrent neural network (RNN) as the decoder for point cloud reconstruction
    locally and globally. [[72](#bib.bib72)] presents MAP-VAE that introduces a half-to-half
    prediction task that first splits a point cloud object into a front half and a
    back half and then trains an RNN to predict the back half sequence from the corresponding
    front half sequence. Several studies instead formulate point cloud reconstruction
    as a point distribution learning task [[73](#bib.bib73), [74](#bib.bib74), [77](#bib.bib77)].
    For example, [[73](#bib.bib73)] presents PointFlow which generates 3D point clouds
    by modelling the distribution of shapes and that of points given shapes. [[74](#bib.bib74)]
    presents a probabilistic framework that extracts unsupervised shape descriptors
    via point distribution learning, which associates each point with a Gaussian and
    models point clouds as the distribution of points. [[77](#bib.bib77)] presents
    an autoregressive model Pointgrow that generates diverse and realistic point cloud
    samples either from scratch or conditioned on semantic contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Further, several studies learn point cloud representations from different object
    resolutions [[69](#bib.bib69), [78](#bib.bib78), [86](#bib.bib86)]. For example,
    Gadelha et al. [[69](#bib.bib69)] designed an autoencoder with a multi-resolution
    tree structure that learns point cloud representations via coarse-to-fine analysis.
    Yang et al. [[78](#bib.bib78)] proposed an autoencoder with a seed generation
    module that allows extraction of input-dependent point-wise features in multiple
    stages with gradually increasing resolution. Chen et al. [[86](#bib.bib86)] proposed
    to learn sampling-invariant features by reconstructing point cloud objects of
    different resolutions and minimizing Chamfer distances between them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d343adcec318d5acc8946106fa82c70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: An illustration of GAN which typically consists of a generator $G$
    and a discriminator $D$ that fight with each other during the training process
    (in the form of a zero-sum game, where one agent’s gain is another agent’s loss).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Learning through point cloud GAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Generative and Adversarial Network (GAN) [[87](#bib.bib87)] is a typical deep
    generative network. As demonstrated in Fig. [13](#S5.F13 "Figure 13 ‣ 5.1.1 Learning
    through point cloud self-reconstruction ‣ 5.1 Generation-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"), it consists of a generator and
    a discriminator. The generator aims to synthesize as realistic data samples as
    possible while the discriminator tries to differentiate real samples and synthesized
    samples. GAN thus learns to generate new data with the same statistics as the
    training set and the modeling can be formulated as a min-max problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{G}\max_{D}L_{GAN}=\log D(x)+\log(1-D(G(z))),$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $G$ is the generator and $D$ represents the discriminator. $x$ and $z$
    represent a real sample and a randomly sampled noise vector from a distribution
    $p(z)$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: When training GANs for URL of point clouds, the generator learns from either
    a sampled vector or a latent embedding to generate point cloud instances, while
    the discriminator tries to distinguish whether input point clouds are from real
    data distribution or generated data distribution. The two sub-networks fight with
    each other during the training process and the discriminator learns to extract
    useful feature representations for point cloud object recognition. The learning
    process involves no human annotations thus the networks can be trained in an unsupervised
    learning manner. After that, the learned discriminator is extended into various
    downstream tasks such as object classification or part segmentation by fine-tuning
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Several networks employ GAN for URL for point clouds successfully [[64](#bib.bib64),
    [68](#bib.bib68), [11](#bib.bib11), [88](#bib.bib88)]. For example, Wu et al. [[64](#bib.bib64)]
    proposed the first GAN model applying for 3D voxels. However, the voxelization
    process either sacrifices the representation accuracy or incurs huge redundancies.
    Achlioptas et al. proposed Latent-GAN [[68](#bib.bib68)] as the first GAN model
    for raw point clouds. Li et al. [[88](#bib.bib88)] further proposed a point cloud
    GAN model with a hierarchical sampling and inference network that learns a stochastic
    procedure to generate new point cloud objects. Valsesia et al. [[11](#bib.bib11)]
    designed the first graph-based GAN model to extract localized features from point
    clouds. These methods evaluated the generalization of the learned representations
    by fine-tuning them to the high-level downstream 3D tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Learning through point cloud up-sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Fig. [14](#S5.F14 "Figure 14 ‣ 5.1.3 Learning through point cloud
    up-sampling ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), given a set of points, point cloud up-sampling aims to generate a
    denser set of points with similar geometries. This task requires deep point cloud
    networks to learn underlying geometries of 3D shapes without any supervision,
    and the learned representations can be used for fine-tuning in 3D downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65f332a3f713430b0d616cd556897676.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: An illustration of point cloud up-sampling: The network DNN learns
    point cloud representations by solving a pre-text task that reproduces an object
    with the same geometry but denser point distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Li et al. [[89](#bib.bib89)] introduced GAN into the point cloud up-sampling
    task and presented PU-GAN to learn a variety of point distributions from the latent
    space by up-sampling points over patches on object surfaces. The generator aims
    to produce up-sampled point clouds while the discriminator tries to distinguish
    whether its input point cloud is produced by the generator or the real one. Similar
    to GANs introduced in Section [5.1.2](#S5.SS1.SSS2 "5.1.2 Learning through point
    cloud GAN ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point cloud representation
    learning ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), the learned discriminator can be transferred in downstream tasks.
    Remelli et al. [[90](#bib.bib90)] designed an autoencoder that can up-sample sparse
    point clouds into dense representations. The learned weight of the encoder can
    also be used as initialization weights for downstream tasks as described in Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 Learning through point cloud self-reconstruction ‣ 5.1 Generation-based
    methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey"). Though point
    cloud up-sampling is attracting increasing attention in recent years [[91](#bib.bib91),
    [92](#bib.bib92), [89](#bib.bib89), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)],
    it is largely evaluated by the quality of generated point clouds while its performance
    in transfer learning has not been well studied.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/198422e52973d56812dce34bdd4336c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: The pipeline of OcCo [[12](#bib.bib12)]. Taking occluded point cloud
    objects as input, an encoder-decoder model is trained to complete the occluded
    point clouds, where the encoder learns point cloud representations and the decoder
    learns to generate complete objects. The learned encoder weights can be used for
    network initialization for downstream tasks. The figure is from [[12](#bib.bib12)]
    with author’s permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4 Learning through point cloud completion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TABLE III: Summary of context-based methods for unsupervised representation
    learning of point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Published in | Category | Contribution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MultiTask [[96](#bib.bib96)] | ICCV 2019 | Hybrid | Learning by clustering,
    reconstruction, and self-supervised classification |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | NeurIPS 2019 | Spatial-context | Learning by
    solving 3D jigsaws |'
  prefs: []
  type: TYPE_TB
- en: '| Constrast&Cluster [[97](#bib.bib97)] | 3DV 2019 | Hybrid | Learning by contrasting
    and clustering with GNN |'
  prefs: []
  type: TYPE_TB
- en: '| GLR [[98](#bib.bib98)] | CVPR 2020 | Hybrid | Learning by global-local reasoning
    for 3D objects |'
  prefs: []
  type: TYPE_TB
- en: '| Info3D [[99](#bib.bib99)] | ECCV 2020 | Context-similarity | Learning by
    contrasting global and local parts of objects |'
  prefs: []
  type: TYPE_TB
- en: '| PointContrast [[54](#bib.bib54)] | ECCV 2020 | Context-similarity | Learning
    by contrasting different views of scene point clouds |'
  prefs: []
  type: TYPE_TB
- en: '| ACD [[100](#bib.bib100)] | ECCV 2020 | Context-similarity | Learning by contrasting
    convex components decomposed from 3D objects |'
  prefs: []
  type: TYPE_TB
- en: '| Rotation3D [[101](#bib.bib101)] | 3DV 2020 | Spatial-context | Learning by
    predicting rotation angles |'
  prefs: []
  type: TYPE_TB
- en: '| HNS [[102](#bib.bib102)] | ACM MM 2021 | Context-similarity | Learning by
    contrasting local patches of 3D objects with hard negative sampling |'
  prefs: []
  type: TYPE_TB
- en: '| CSC [[3](#bib.bib3)] | CVPR 2021 | Context-similarity | Techniques to improve
    contrasting scene point cloud views |'
  prefs: []
  type: TYPE_TB
- en: '| STRL [[1](#bib.bib1)] | ICCV 2021 | Temporal-context | Learning spatio-temporal
    data invariance from point cloud sequences |'
  prefs: []
  type: TYPE_TB
- en: '| RandomRooms [[103](#bib.bib103)] | ICCV 2021 | Context-similarity | Constructing
    pseudo scenes with synthetic objects for contrastive learning |'
  prefs: []
  type: TYPE_TB
- en: '| DepthContrast [[104](#bib.bib104)] | ICCV 2021 | Context-similarity | Joint
    contrastive learning with points and voxels |'
  prefs: []
  type: TYPE_TB
- en: '| SelfCorrection [[105](#bib.bib105)] | ICCV 2021 | Hybrid | Learning by distinguishing
    and restoring destroyed objects |'
  prefs: []
  type: TYPE_TB
- en: '| PC-FractalDB [[106](#bib.bib106)] | CVPR 2022 | Context-similarity | Leveraging
    fractal geometry to generate high-quality pre-training data |'
  prefs: []
  type: TYPE_TB
- en: '| 4dcontrast [[107](#bib.bib107)] | ECCV 2022 | Temporal-context | Learning
    by contrasting dynamic correspondences from 3D scene sequences |'
  prefs: []
  type: TYPE_TB
- en: '| DPCo [[108](#bib.bib108)] | ECCV 2022 | Context-similarity | A unified contrastive-learning
    framework for point cloud pre-training |'
  prefs: []
  type: TYPE_TB
- en: '| ProposalContrast [[109](#bib.bib109)] | ECCV 2022 | Context-similarity |
    Pre-training 3D detectors by contrasting region proposals |'
  prefs: []
  type: TYPE_TB
- en: '| MaskPoint [[110](#bib.bib110)] | ECCV 2022 | Context-similarity | Learning
    by discriminating masked object points and sampled noise points |'
  prefs: []
  type: TYPE_TB
- en: '| FAC [[111](#bib.bib111)] | CVPR 2023 | Context-similarity | Learning by contrasting
    between grouped foreground and background |'
  prefs: []
  type: TYPE_TB
- en: Point cloud completion is a task to predict arbitrary missing parts based on
    the rest of the 3D point clouds. To achieve this target, deep networks need to
    learn inner geometric structures and semantic knowledge of the 3D objects so as
    to correctly predict missing parts. On top of that, the learned representations
    can be transferred to downstream tasks. The whole process involves no human annotations
    and thus belongs to unsupervised representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Point cloud completion has been an active research area over the past decade [[112](#bib.bib112),
    [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115), [76](#bib.bib76),
    [116](#bib.bib116), [117](#bib.bib117)] with evaluation in different URL benchmarks
    [[62](#bib.bib62), [65](#bib.bib65), [76](#bib.bib76), [12](#bib.bib12)]. A pioneer
    work VConv-DAE [[62](#bib.bib62)] voxelizes point cloud objects into volumetric
    grids and learns object shape distributions with an autoencoder by predicting
    the missing voxels from the rest parts. Xie et al. [[65](#bib.bib65)] designed
    3D-DescriptorNet for probabilistic modeling of volumetric shape patterns. Achlioptas
    et al. [[68](#bib.bib68)] introduced the first DNN for raw point cloud completion
    which is a point-based network with an encoder-decoder structure. Yuan et al. [[113](#bib.bib113)]
    proposed a Point Completion Network, an autoencoder structured network for learning
    useful representations by repairing incomplete point cloud objects. Wen et al. [[76](#bib.bib76)]
    proposed SA-Net, which introduces a skip-attention mechanism in the encoder that
    selectively transfers geometric information from the local regions to the decoder
    for generating complete point cloud objects. Wang et al. [[12](#bib.bib12)] proposed
    to learn an encoder-decoder model that recovers the occluded points by different
    camera views as shown in Fig. [15](#S5.F15 "Figure 15 ‣ 5.1.3 Learning through
    point cloud up-sampling ‣ 5.1 Generation-based methods ‣ 5 Unsupervised point
    cloud representation learning ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey"). The encoder parameters are used as initialization
    for downstream tasks including classification, part segmentation, and semantic
    segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, recovering missing parts from masked input as the pre-text task of
    URL has been proved remarkably successful in NLP [[5](#bib.bib5), [6](#bib.bib6)]
    and 2D computer vision [[10](#bib.bib10)]. Such idea has also been investigated
    in 3D point cloud learning [[57](#bib.bib57), [79](#bib.bib79), [110](#bib.bib110),
    [118](#bib.bib118)]. For example, Yu et al. [[57](#bib.bib57)] proposed a Point-BERT
    paradigm that pre-trains point cloud Transformers through a masked point modeling
    task. They use a discrete variational autoencoder to generate tokens for object
    patches and randomly masked out the tokens to train the Transformer to recover
    the original complete point tokens. The representations learned by Point-BERT
    can be well transferred to new tasks and domains such as object classification
    and object part segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Context-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Context-based methods are another important category of URL of point clouds
    that has attracted increasing attention in recent years. Different from generation-based
    methods that learn representations in a generative way, these methods employ discriminative
    pre-text tasks to learn different contexts of point clouds including context similarity,
    spatial context structures, and temporal context structures. The designed pre-text
    tasks require no human annotations and Table [III](#S5.T3 "TABLE III ‣ 5.1.4 Learning
    through point cloud completion ‣ 5.1 Generation-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") lists the recent methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Learning with context similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/504beb3b6807f02d1435621d8a00d2a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: An illustration of instance contrastive learning that learns locally
    smooth representations by self-discrimination, which pulls Query (from the Anchor
    sample) close to Positive Key (from Positive Samples) and pushes it away from
    Negative Keys (from Negative Samples).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This type of method learns unsupervised representations of point clouds by
    exploring underlying context similarities between samples. A typical approach
    is contrastive learning, which has demonstrated superior performances in both
    2D vision [[7](#bib.bib7), [8](#bib.bib8), [119](#bib.bib119)] and 3D vision [[54](#bib.bib54),
    [3](#bib.bib3), [104](#bib.bib104)] in recent years. Fig. [16](#S5.F16 "Figure
    16 ‣ 5.2.1 Learning with context similarity ‣ 5.2 Context-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey") provides an illustration of instance-wise
    contrastive learning. Given one input point cloud object instance as the anchor,
    its augmented views are defined as the positive samples while other different
    instances are negative samples. The network learns representations of point clouds
    by optimizing a self-discrimination task, i.e. query (feature of the anchor) should
    be close to the positive keys (features of positive samples) and faraway from
    its negative keys (features of negative samples). This learning strategy groups
    representations of similar samples together in an unsupervised manner and helps
    networks to learn semantic structures from unlabelled data distribution. The InfoNCE
    loss [[120](#bib.bib120)] defined below and its variants are often employed as
    the objective function in training:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\mathrm{InfoNCE}}=-\log\frac{\exp{(q\cdot k_{+}/\tau)}}{\sum_{i=0}^{K}{\exp(q\cdot
    k_{i}/\tau)}},$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $q$ is encoded query, $\{k_{0},k_{1},k_{2},...\}$ are keys with $k_{+}$
    being the positive key, $\tau$ is a temperature hyper-parameter that controls
    how the distribution concentrates.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to generation-based methods, different contrastive learning methods
    [[99](#bib.bib99), [121](#bib.bib121), [100](#bib.bib100), [102](#bib.bib102),
    [122](#bib.bib122)] have been proposed to learn representations on synthetic single
    objects. For example, Sanghi et al. [[99](#bib.bib99)] proposed to learn useful
    feature representations by maximizing mutual information between synthetic objects
    and their local parts. Wang et al. [[121](#bib.bib121)] proposed a hybrid contrastive
    learning strategy that uses objects of different resolutions for instance-level
    contrast for capturing hierarchical global representations and simultaneously
    contrasted points and instances for learning local features. Gadelha et al. [[100](#bib.bib100)]
    decompose 3D objects into convex components and construct positive pairs among
    the same components and negative pairs among different components for contrastive
    learning. Du et al. [[102](#bib.bib102)] introduced a hard negative sampling strategy
    into the contrastive learning between instances and local parts. Besides, Rao
    et al. [[98](#bib.bib98)] unified contrastive learning, normal estimation, and
    self-reconstruction into the same framework and formulated a multi-task learning
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87fc7b061dd3aa400e4441d136febf82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The pipeline of PointContrast [[54](#bib.bib54)]: Two scans $x^{1}$
    and $x^{2}$ of the same scene captured from two different viewpoints are transformed
    by $T_{1}$ and $T_{2}$ for data augmentation. The correspondence mapping between
    the two views is computed to minimize the distance for matched point features
    and maximize the distance for unmatched point features for contrastive learning.
    The graph is extracted from [[54](#bib.bib54)] with authors’ permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Xie et. al proposed PointContrast [[54](#bib.bib54)], a contrastive
    learning framework that learns representations of scene point clouds as illustrated
    in Fig. [17](#S5.F17 "Figure 17 ‣ 5.2.1 Learning with context similarity ‣ 5.2
    Context-based methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey"). The
    work shows, for the first time, that network weights pre-trained on 3D scene partial
    frames can lead to performance boosts when fine-tuned on multiple 3D high-level
    tasks including object classification, semantic segmentation, and object detection.
    Firstly, dense correspondences are extracted between two aligned views of ScanNet [[18](#bib.bib18)]
    to build point pairs and point-level contrastive learning is then conducted with
    a unified backbone (SR-UNet). Finally, the learned model was transferred to multiple
    downstream 3D tasks including classification, semantic segmentation, and object
    detection with consistent performance gains.'
  prefs: []
  type: TYPE_NORMAL
- en: Since PointContrast brought new insights that the unsupervised representation
    learned from scene-level point clouds can generalize across domains and boost
    high-level scene understanding tasks, several unsupervised pre-training works
    are proposed for scene-level 3D tasks. Considering that PointContrast focuses
    on point-level alignment without capturing spatial configurations and contexts
    in scenes, Hou et al. [[3](#bib.bib3)] integrated spatial contexts into the pre-training
    objective by partitioning the space into spatially inhomogeneous cells for correspondence
    matching. Hou et al. [[123](#bib.bib123)] built a multi-modal contrastive learning
    framework that models 2D multi-view correspondences as well as 2D-3D correspondences
    with geometry-to-image alignment. While the aforementioned works [[54](#bib.bib54),
    [3](#bib.bib3), [123](#bib.bib123)] require 3D data captured from multiple camera
    views, Zhang et al. [[104](#bib.bib104)] proposed DepthContrast that can work
    with single-view data. Instead of using real point clouds as previous methods,
    Rao et al. [[103](#bib.bib103)] generated synthetic scenes and objects from ShapeNet [[14](#bib.bib14)]for
    network pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: Another unsupervised approach to learn context similarity is clustering. In
    this approach, samples are first grouped into clusters by clustering algorithms
    such as K-Means [[124](#bib.bib124)] and each sample is assigned a cluster ID
    as pseudo-label. Then networks are trained in a supervised manner to learn semantic
    structures of data distribution. The learned parameters are used for model initialization
    for fine-tuning various downstream tasks. A typical example is DeepClustering [[125](#bib.bib125)]
    which is the first unsupervised clustering method for 2D visual representation
    learning. However, no prior studies adopted a purely clustering strategy for URL
    of point clouds. Instead, hybrid approaches are proposed by integrating clustering
    with other unsupervised learning approaches (e.g., self-reconstruction [[96](#bib.bib96)]
    or contrastive learning [[97](#bib.bib97)]) for learning more robust representations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Learning with spatial context structure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Point clouds with spatial coordinates provides accurate geometric description
    of 3D shapes of objects and scenes. The rich spatial contexts in point clouds
    can be exploited in pre-text tasks for URL. For example, networks can be trained
    to sort out the relation of different object parts. Likewise, the learned parameters
    can be used for model initialization for downstream tasks. Since no human annotations
    are required in training, the key is to design effective pre-text tasks to exploit
    spatial contexts as URL objectives.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e48ac3aa0357c58b7d3a84fea99b13e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The pipeline of 3DJigsaw [[13](#bib.bib13)]: An object is split
    into voxels where each point is assigned with a voxel label. The split voxels
    are randomly rearranged via pre-processing, and a deep neural network is trained
    to predict the voxel label for each point. The graph is reproduced based on [[13](#bib.bib13)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The method Jigsaw3D [[13](#bib.bib13)] proposed by Sauder et al. is one of
    the pioneer works that use spatial context for URL of point clouds. As illustrated
    in Fig. [18](#S5.F18 "Figure 18 ‣ 5.2.2 Learning with spatial context structure
    ‣ 5.2 Context-based methods ‣ 5 Unsupervised point cloud representation learning
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), objects are first split into voxels where each point is assigned a
    voxel label. The network is then fed with randomly rearranged point clouds and
    optimized by predicting correct voxel label for each point. During the training,
    the network aims to extract spatial relations and geometric information from point
    clouds. In their following work [[126](#bib.bib126)], another pre-text task was
    designed to predict one of ten spatial relationships of two local parts from the
    same object. Inspired by the 2D method that predicts image rotations [[127](#bib.bib127)],
    Poursaeed et al. [[101](#bib.bib101)] proposed to learn representations by predicting
    rotation angles of 3D objects. Thabet et al. [[128](#bib.bib128)] designed a pre-text
    task that predicts the next point in a point sequence defined by Morton-order
    Space Filling Curve. Chen et al. [[105](#bib.bib105)] proposed to learn the spatial
    context of objects by distinguishing the distorted parts of a shape from the correct
    ones. Sun et al. [[129](#bib.bib129)] introduced a mix-and-disentangle task to
    exploit spatial context cues.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Learning with temporal context structure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Point cloud sequence is a common type of point cloud data that consists of consecutive
    point cloud frames. For example, there are indoor point cloud sequences transformed
    from RGB-D video frames  [[18](#bib.bib18)] and LiDAR sequential data [[19](#bib.bib19),
    [130](#bib.bib130), [131](#bib.bib131)] with continuous point cloud scans with
    each scan collected by one sweep of LiDAR sensors. Point cloud sequences contain
    rich temporal information that can be extracted by designing pre-text tasks and
    used as supervision signals to train DNNs. The learned representations can be
    transferred to downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5b4713a6478aa972cf346ea7f25a8990.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The pipeline of STRL [[1](#bib.bib1)]: An Online Network learns
    spatial and temporal structures from two neighbouring point cloud frames $X^{u}$
    and $X^{v}$. The figure is adopted from [[1](#bib.bib1)] with authors’ permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Huang et al. [[1](#bib.bib1)] proposed a Spatio-Temporal Representation
    Learning (STRL) framework as illustrated in Fig. [19](#S5.F19 "Figure 19 ‣ 5.2.3
    Learning with temporal context structure ‣ 5.2 Context-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"). STRL extends BYOL [[8](#bib.bib8)]
    from 2D vision to 3D vision and extracts spatial and temporal representation from
    point clouds. It treats two neighboring point cloud frames as positive pairs and
    minimizes the mean squared error between the learned feature representations of
    sample pairs. Chen et al. [[107](#bib.bib107)] exploit synthetic 3D shapes moving
    in static 3D environments to create dynamic scenarios and sample pairs in the
    temporal order. They conduct contrastive learning to learn 3D representations
    with dynamic understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised learning with temporal context structures has proved its effectiveness
    in both 2D computer vision tasks [[132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134),
    [135](#bib.bib135)] and 3D computer vision tasks [[1](#bib.bib1), [107](#bib.bib107)].
    As discussed in Section [7](#S7 "7 Future direction ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey"), this direction
    is very promising but more research is needed for better harvesting the temporal
    contextual information.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Multiple modal-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different modalities such as images [[19](#bib.bib19)] and natural language
    descriptions [[136](#bib.bib136)] can provide additional information for point-cloud
    data. Modeling relationships across modalities can be designed as pre-text tasks
    for URL which helps networks to learn more robust and comprehensive representations.
    Likewise, the learned parameters can be used as initialization weights for various
    downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4e8ec1363b5d0584672bd5e8cb4a9fb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: The pipeline CMCV [[4](#bib.bib4)]: CMCV employs a 2D CNN to extract
    2D features from rendered views of 3D objects and a 3D GCN to extract 3D features
    from point clouds. The two types of features are concatenated by a two-layer fully
    connected network (FCN) to predict cross-modality correspondences. The graph is
    reproduced based on [[4](#bib.bib4)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Several recent work [[4](#bib.bib4), [137](#bib.bib137)] exploits the correspondences
    across 3D point cloud objects and 2D images for URL. For example, Jing et al. [[4](#bib.bib4)]
    render 3D objects with different camera views into 2D images for learning from
    multi-modality data. As Fig. [20](#S5.F20 "Figure 20 ‣ 5.3 Multiple modal-based
    methods ‣ 5 Unsupervised point cloud representation learning ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey") shows, they
    employ a 2D CNN and a 3D GCN to extract image features and point cloud features,
    respectively, and then conduct contrastive learning on intra-modal correspondences
    and cross-modal correspondences. Their study shows that both pre-trained 2D CNN
    and 3D GCN achieved better classification as compared with random initialization.
    Differently, Wang et al. [[138](#bib.bib138)] project point clouds into colored
    images and then feed them into an image pre-trained model with frozen weights
    to extract representative features for downstream tasks. However, how to learn
    unsupervised point cloud representations with other modalities such as text descriptions
    and audio data remains an under-explored field. We expect more studies in this
    promising research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Local descriptor-based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aforementioned methods aim to learn semantic structures of point clouds
    for high-level understanding, while the local descriptor-based methods focus on
    learning representations for low-level tasks. For example, Deng et al. [[139](#bib.bib139)]
    introduced PPF-FoldNet that extracts rotation-invariant 3D local descriptors for
    3D matching [[140](#bib.bib140)]. Several works [[141](#bib.bib141), [142](#bib.bib142)]
    exploit non-rigid shape correspondence extraction as pre-text tasks for URL of
    point clouds, aiming to find the point-to-point correspondence of two deformable
    3D shapes. Jiang et al. [[143](#bib.bib143)] explore unsupervised 3D registration
    for finding the optimal rigid transformation that can align the source point cloud
    to the target precisely.
  prefs: []
  type: TYPE_NORMAL
- en: The performances of existing local descriptor-based methods are mainly evaluated
    on low-level tasks. However, how to adapt the learned feature representations
    toward other high-level tasks is rarely discussed. We expect more related research
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Pros and Cons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generation-based methods have been extensively studied in 3D URL, thanks to
    their ability to recover the original data distribution without assuming any downstream
    tasks. However, most existing research focuses on object-level point clouds, characterized
    by limited point numbers and data variability, restricting their applicability
    to object classification and part segmentation tasks. Additionally, these methods
    demonstrate limited effectiveness in scene-level tasks, such as 3D object detection
    and semantic segmentation, due to the difficulty of generating scene-level point
    clouds with complex distribution, rich noises and sparsity variation, and various
    occlusions. Nonetheless, generation-based methods achieve very impressive progress
    in 2D images [[10](#bib.bib10)] recently, demonstrating their great potential
    for handling 3D point-cloud data. More efforts are expected in scene-level tasks
    as well as various downstream applications.
  prefs: []
  type: TYPE_NORMAL
- en: Context-based methods have recently become a prevalent approach in scene-level
    tasks, such as 3D semantic segmentation, 3D instance segmentation, and 3D object
    detection, thanks to their ability in addressing complex real-world data. However,
    they are still facing several challenges. The first is hard-example mining which
    is crucial to effective contrastive learning. Beyond that, designing effective
    self-supervision is also challenging for context-based methods, especially while
    considering generalization across various tasks and applications.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple modal-based methods allow leveraging additional data modalities for
    enriching the distribution of point clouds. Pair-wise correspondences between
    point clouds and other data modalities also offer additional supervision, thereby
    enhancing the learned unsupervised point cloud representations. However, multi-modality
    methods are still facing several challenges. For example, acquiring large-scale
    pair-wise data is often a non-trivial task, and so does the design of effective
    cross-domain tasks. In addition, how to learn an effective homogeneous representation
    space across multiple modalities remains a very open research problem.
  prefs: []
  type: TYPE_NORMAL
- en: Local descriptor-based methods offer distinct advantages in capturing detailed
    spatial cues and exploiting low-level position information. However, these methods
    are limited in their ability of transferring learned representations to high-level
    recognition models, which restricts their application scope in more complex and
    abstract recognition tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Benchmark performances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE IV: Comparing linear shape classification on ModelNet10 and ModelNet40
    [[27](#bib.bib27)]: Linear SVM classifiers are trained with representations learned
    by different unsupervised methods. Accuracy highlighted by ^* was obtained by
    pre-training with multi-modal data. [T] denotes models with modified Transformers.
    [ST] denotes models with standard Transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Pre-text task | Backbone | Pre-train dataset | ModelNet10
    | ModelNet40 |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised learning | 2017 | N.A. | PointNet [[15](#bib.bib15)] | N.A. |
    - | 89.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | PointNet++ [[48](#bib.bib48)] | - | 90.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | DGCNN [[49](#bib.bib49)] | - | 93.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | RSCNN [[56](#bib.bib56)] | - | 93.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | [T]PointTransformer [[144](#bib.bib144)] |  | - | 93.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | [ST]Transformer [[57](#bib.bib57)] |  | - | 91.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SPH [[145](#bib.bib145)] | 2003 | Generation | - | ShapeNet | 79.8 | 68.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| LFD [[146](#bib.bib146)] | 2003 | Generation | - | ShapeNet | 79.9 | 75.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| TL-Net [[63](#bib.bib63)] | 2016 | Generation | - | ShapeNet | - | 74.4 |'
  prefs: []
  type: TYPE_TB
- en: '| VConv-DAE [[62](#bib.bib62)] | 2016 | Generation | - | ShapeNet | 80.5 |
    75.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-GAN [[64](#bib.bib64)] | 2016 | Generation | - | ShapeNet | 91.0 | 83.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3D DescriptorNet [[65](#bib.bib65)] | 2018 | Generation | - | ShapeNet |
    - | 92.4 |'
  prefs: []
  type: TYPE_TB
- en: '| FoldingNet [[66](#bib.bib66)] | 2018 | Generation | - | ModelNet40 | 91.9
    | 84.4 |'
  prefs: []
  type: TYPE_TB
- en: '| FoldingNet [[66](#bib.bib66)] | 2018 | Generation | - | ShapeNet | 94.4 |
    88.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Latent-GAN [[68](#bib.bib68)] | 2018 | Generation | - | ModelNet40 | 92.2
    | 87.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Latent-GAN [[68](#bib.bib68)] | 2018 | Generation | - | ShapeNet | 95.3 |
    85.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MRTNet [[69](#bib.bib69)] | 2018 | Generation | - | ShapeNet | 86.4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| VIP-GAN [[70](#bib.bib70)] | 2019 | Generation | - | ShapeNet | 94.1 | 92.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3DCapsuleNet [[47](#bib.bib47)] | 2019 | Generation | - | ShapeNet | - |
    88.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PC-GAN [[88](#bib.bib88)] | 2019 | Generation | - | ModelNet40 | - | 87.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| L2G-AE [[71](#bib.bib71)] | 2019 | Generation | - | ShapeNet | 95.4 | 90.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| MAP-VAE [[72](#bib.bib72)] | 2019 | Generation | - | ShapeNet | 94.8 | 90.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| PointFlow [[73](#bib.bib73)] | 2019 | Generation | - | ShapeNet | 93.7 |
    86.8 |'
  prefs: []
  type: TYPE_TB
- en: '| MultiTask [[96](#bib.bib96)] | 2019 | Hybrid | - | ShapeNet | - | 89.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | 2019 | Context | PointNet | ShapeNet | 91.6
    | 87.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | 2019 | Context | DGCNN | ShapeNet | 94.5 |
    90.6 |'
  prefs: []
  type: TYPE_TB
- en: '| ClusterNet [[97](#bib.bib97)] | 2019 | Context | DGCNN | ShapeNet | 93.8
    | 86.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CloudContext [[126](#bib.bib126)] | 2019 | Context | DGCNN | ShapeNet | 94.5
    | 89.3 |'
  prefs: []
  type: TYPE_TB
- en: '| NeuralSampler [[90](#bib.bib90)] | 2019 | Generation | - | ShapeNet | 95.3
    | 88.7 |'
  prefs: []
  type: TYPE_TB
- en: '| PointGrow [[77](#bib.bib77)] | 2020 | Generation | - | ShapeNet | 85.8 |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Info3D [[99](#bib.bib99)] | 2020 | Context | PointNet | ShapeNet | - | 89.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| Info3D [[99](#bib.bib99)] | 2020 | Context | DGCNN | ShapeNet | - | 91.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| ACD [[100](#bib.bib100)] | 2020 | Context | PointNet++ | ShapeNet | - | 89.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| PDL [[74](#bib.bib74)] | 2020 | Generation | - | ShapeNet | - | 84.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GLR [[98](#bib.bib98)] | 2020 | Hybrid | PointNet++ | ShapeNet | 94.8 | 92.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| GLR [[98](#bib.bib98)] | 2020 | Hybrid | RSCNN | ShapeNet | 94.6 | 92.2 |'
  prefs: []
  type: TYPE_TB
- en: '| SA-Net-cls [[76](#bib.bib76)] | 2020 | Generation | - | ShapeNet | - | 90.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| GraphTER [[75](#bib.bib75)] | 2020 | Generation | - | ModelNet40 | - | 89.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rotation3D [[101](#bib.bib101)] | 2020 | Context | PointNet | ShapeNet |
    - | 88.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Rotation3D [[101](#bib.bib101)] | 2020 | Context | DGCNN | ShapeNet | - |
    90.8 |'
  prefs: []
  type: TYPE_TB
- en: '| MID [[121](#bib.bib121)] | 2020 | Context | HRNet | ShapeNet | - | 90.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GTIF [[85](#bib.bib85)] | 2020 | Generation | HRNet | ShapeNet | 95.9 | 89.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| HNS [[102](#bib.bib102)] | 2021 | Context | DGCNN | ShapeNet | - | 89.6 |'
  prefs: []
  type: TYPE_TB
- en: '| ParAE [[147](#bib.bib147)] | 2021 | Generation | PointNet | ShapeNet | -
    | 90.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ParAE [[147](#bib.bib147)] | 2021 | Generation | DGCNN | ShapeNet | - | 91.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| CMCV [[4](#bib.bib4)] | 2021 | Multi-modal | DGCNN | ShapeNet | - | 89.8^*
    |'
  prefs: []
  type: TYPE_TB
- en: '| GSIR [[86](#bib.bib86)] | 2021 | Context | DGCNN | ModelNet40 | - | 90.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| STRL [[1](#bib.bib1)] | 2021 | Context | PointNet | ShapeNet | - | 88.3 |'
  prefs: []
  type: TYPE_TB
- en: '| STRL [[1](#bib.bib1)] | 2021 | Context | DGCNN | ShapeNet | - | 90.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PSG-Net [[78](#bib.bib78)] | 2021 | Generation | PointNet++ | ShapeNet |
    - | 90.9 |'
  prefs: []
  type: TYPE_TB
- en: '| SelfCorrection [[105](#bib.bib105)] | 2021 | Hybrid | PointNet | ShapeNet
    | 93.3 | 89.9 |'
  prefs: []
  type: TYPE_TB
- en: '| SelfCorrection [[105](#bib.bib105)] | 2021 | Hybrid | RSCNN | ShapeNet |
    95.0 | 92.4 |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | 2021 | Generation | [ST]Transformer | ShapeNet
    | - | 92.1 |'
  prefs: []
  type: TYPE_TB
- en: '| CrossPoint [[137](#bib.bib137)] | 2022 | Multi-modal | PointNet | ShapeNet
    | - | 89.1^* |'
  prefs: []
  type: TYPE_TB
- en: '| CrossPoint [[137](#bib.bib137)] | 2022 | Multi-modal | DGCNN | ShapeNet |
    - | 91.2^* |'
  prefs: []
  type: TYPE_TB
- en: '| Point-BERT [[57](#bib.bib57)] | 2022 | Generation | [ST]Transformer | ShapeNet
    | - | 93.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Point-MAE [[79](#bib.bib79)] | 2022 | Generation | [ST]Transformer | ShapeNet
    | - | 93.8 |'
  prefs: []
  type: TYPE_TB
- en: We benchmark representative 3D URL methods with two widely adopted evaluation
    metrics. The benchmarking is performed over public point-cloud data, where all
    performances are extracted from the corresponding papers.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Evaluation Criteria
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two metrics that have been widely adopted for evaluating the quality
    of the learned unsupervised point-cloud representations.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Linear classification first applies a pre-trained unsupervised model
    to extract features from certain labelled data. It then trains a supervised linear
    classifier with the extracted features together with the corresponding labels,
    where the quality of the pre-learned unsupervised representations is evaluated
    by the performance of the trained linear classifier over test data. Hence, the
    linear classification can be viewed as a type of representation learning metric
    which provides cluster analysis in an implicit way.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Fine-tuning optimizes a pre-trained unsupervised model using labelled
    data from downstream tasks. It can assess the quality of the pre-learned unsupervised
    representations by evaluating the performance of the fine-tuned model over downstream
    test data, i.e. how much performance gains could be obtained by unsupervised pre-training
    compared to the random initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Comparisons of unsupervised pre-training performance over the object
    classification datasets ModelNet40 and OBJ-BG split in ScanObjecNN. Performance
    numbers are presented in the format of ”A/B”, with ”A” indicating training classification
    models from scratch with random initialization and ”B” indicating fine-tuning
    classification models that are initialized with unsupervised pre-trained models.
    Performance under “A” may vary due to different implementations as reported in
    the corresponding papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone | ModelNet40 | ScanObjectNN |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | PointNet [[15](#bib.bib15)] | 89.2/89.6(+0.4)
    | 73.5/76.5(+3.0) |'
  prefs: []
  type: TYPE_TB
- en: '| Info3D [[99](#bib.bib99)] | PointNet [[15](#bib.bib15)] | 89.2/90.2(+1.0)
    | -/- |'
  prefs: []
  type: TYPE_TB
- en: '| SelfCorrection [[105](#bib.bib105)] | PointNet [[15](#bib.bib15)] | 89.1/90.0(+0.9)
    | -/- |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | PointNet [[15](#bib.bib15)] | 89.2/90.1(+0.9) |
    73.5/80.0(+6.5) |'
  prefs: []
  type: TYPE_TB
- en: '| ParAE [[147](#bib.bib147)] | PointNet [[15](#bib.bib15)] | 89.2/90.5(+1.3)
    | -/- |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | PCN [[113](#bib.bib113)] | 89.3/89.6(+0.3)
    | 78.3/78.2(-0.1) |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | PCN [[113](#bib.bib113)] | 89.3/90.3(+1.0) | 78.3/80.4(+2.1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GLR [[98](#bib.bib98)] | RSCNN [[56](#bib.bib56)] | 91.8/92.2(+0.5) | -/-
    |'
  prefs: []
  type: TYPE_TB
- en: '| SelfCorrection [[105](#bib.bib105)] | RSCNN [[56](#bib.bib56)] | 91.7/93.0(+1.3)
    | -/- |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | DGCNN [[49](#bib.bib49)] | 92.2/92.4(+0.2)
    | 82.4/82.7(+0.3) |'
  prefs: []
  type: TYPE_TB
- en: '| Info3D [[99](#bib.bib99)] | DGCNN [[49](#bib.bib49)] | 93.5/93.0(-0.5) |
    -/- |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | DGCNN [[49](#bib.bib49)] | 92.5/93.0(+0.5) | 82.4/83.9(+1.6)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ParAE [[147](#bib.bib147)] | DGCNN [[49](#bib.bib49)] | 92.2/92.9(+0.7) |
    -/- |'
  prefs: []
  type: TYPE_TB
- en: '| STRL [[1](#bib.bib1)] | DGCNN [[49](#bib.bib49)] | 92.2/93.1(+0.9) | -/-
    |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | Transformer [[57](#bib.bib57)] | 91.2/92.2(+1.0)
    | 79.9/84.9(+5.0) |'
  prefs: []
  type: TYPE_TB
- en: '| Point-BERT [[57](#bib.bib57)] | Transformer [[57](#bib.bib57)] | 91.2/93.4(+2.2)
    | 79.9/87.4(+7.5) |'
  prefs: []
  type: TYPE_TB
- en: Note URL can be evaluated with other quantitative metrics. For example, reconstruction
    error [[66](#bib.bib66)] can tell how well the learned representations encode
    the raw point clouds. Different clustering metrics such as Normalized Mutual Information [[96](#bib.bib96)]
    could complement the linear-classification metric. However, these metrics are
    mostly task-specific, e.g., the reconstruction error may not evaluate the representation
    of scene-level point clouds well due to their inherent noise, occlusion, and sparsity.
    In fact, few generic metrics can directly and explicitly evaluate the quality
    of the learned 3D unsupervised representations despite its critical importance
    to 3D URL studies. More research along this direction is needed to advance this
    research field further.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond quantitative metrics, unsupervised feature representations can be evaluated
    in a qualitative manner. For example, t-SNE (t-Distributed Stochastic Neighbor
    Embedding) [[148](#bib.bib148)] has been widely adopted to compress the dimension
    of the learned feature representations and visualize the compressed feature embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Object-level tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.2.1 Object classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Object classification is the most widely used task in evaluations since the
    majority of existing works learn point cloud representations on object-level point
    cloud datasets. As described in Section [6.1](#S6.SS1 "6.1 Evaluation Criteria
    ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey"), both two types of protocols are widely
    adopted including the linear classification protocol and the fine-tuning protocol.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Comparison of 3D URL methods for shape part segmentation over ShapeNetPart [[14](#bib.bib14)].
    ”Unsup.” denotes linear classification of the learned unsupervised point features.
    ”Trans.” is presented in a format of ”A/B”, where ”A” is obtained with segmentation
    models trained from scratch with random initialization, and ”B” is obtained by
    fine-tuning segmentation models that are initialized with unsupervised pre-trained
    models. We also provide supervised performances (”Sup.”) of different backbone
    models with random initialization (extracted from the original papers).'
  prefs: []
  type: TYPE_NORMAL
- en: '| URL Method | Type | Backbone | class mIoU | instance mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| N.A. | Sup. | PointNet | 80.4 | 83.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Sup. | PointNet++ | 81.9 | 85.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Sup. | DGCNN | 82.3 | 85.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Sup. | RSCNN | 84.0 | 86.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Sup. | Transformer | 83.4 | 85.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Latent-GAN [[68](#bib.bib68)] | Unsup. | - | 57.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MAP-VAE [[72](#bib.bib72)] | Unsup. | - | 68.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| CloudContext [[126](#bib.bib126)] | Unsup. | DGCNN | - | 81.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GraphTER [[75](#bib.bib75)] | Unsup. | - | 78.1 | 81.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MID [[121](#bib.bib121)] | Unsup. | HRNet | 83.4 | 84.6 |'
  prefs: []
  type: TYPE_TB
- en: '| HNS [[102](#bib.bib102)] | Unsup. | DGCNN | 79.9 | 82.3 |'
  prefs: []
  type: TYPE_TB
- en: '| CMCV [[4](#bib.bib4)] | Unsup. | DGCNN | 74.7 | 80.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SO-Net [[67](#bib.bib67)] | Trans. | SO-Net | -/- | 84.6/84.9(+0.3) |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | Trans. | DGCNN | 82.3/83.1(+0.8) | 85.1/85.3(+0.2)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MID [[121](#bib.bib121)] | Trans. | HRNet | 84.6/85.2(+0.6) | 85.5/85.8(+0.3)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CMCV [[4](#bib.bib4)] | Trans. | DGCNN | 77.6/79.1(+1.5) | 83.0/83.7(+0.7)
    |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | Trans. | PointNet | 82.2/83.4(+1.2) | -/- |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | Trans. | DGCNN | 84.4/85.0(+0.6) | -/- |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | Trans. | Transformer | 83.4/83.4(+0.0) | 85.1/85.1(+0.0)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Point-BERT [[57](#bib.bib57)] | Trans. | Transformer | 83.4/84.1(+0.7) |
    85.1/85.6(+0.5) |'
  prefs: []
  type: TYPE_TB
- en: 'Table [IV](#S6.T4 "TABLE IV ‣ 6 Benchmark performances ‣ Unsupervised Point
    Cloud Representation Learning with Deep Neural Networks: A Survey") summarizes
    the performance of the linear classification by existing methods. Specifically,
    linear classifiers are trained with the representations learned by different unsupervised
    methods on the ShapeNet or ModelNet40 dataset, and the classification results
    over the testing set over ModelNet10 and ModelNet40 are reported. For comparison,
    we also list supervised learning performances of the same backbone models over
    the same datasets. It can be seen that the performances of unsupervised learning
    methods keep improving and some methods have even surpassed supervised learning
    methods, demonstrating the effectiveness and great potential of URL of point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Semantic segmentation on S3DIS [[21](#bib.bib21)]: It compares supervised
    training with random weight initialization and fine-tuning with pre-trained weights
    learned from unsupervised pre-training tasks. It uses DGCNN as the segmentation
    model, which is trained on different single Areas and tested on Area 5 (upper
    part) and Area 6 (lower part).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | OA on area 5 with different train area | mIoU on area 5 with different
    train area |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Area1 | Area2 | Area3 | Area4 | Area6 | Area1 | Area2 | Area3 | Area4 | Area6
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | 82.9 | 81.2 | 82.8 | 82.8 | 83.1 | 43.6 | 34.6 | 39.9 | 39.4
    | 43.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | 83.5(+0.6) | 81.2(+0.0) | 84.0(+1.2) | 82.9(+0.1)
    | 83.3(+0.2) | 44.7(+1.1) | 34.9(+0.3) | 42.4(+2.5) | 39.9(+0.5) | 43.9(+0.0)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ParAE [[147](#bib.bib147)] | 91.8(+8.9) | 82.3(+1.1) | 89.5(+6.7) | 88.2(+5.4)
    | 86.4(+3.3) | 53.5(+9.9) | 38.5(+3.9) | 48.4(+8.5) | 45.0(+5.6) | 49.2(+5.3)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Method | OA on area 6 with different train area | mIoU on area 6 with different
    train area |'
  prefs: []
  type: TYPE_TB
- en: '| Area1 | Area2 | Area3 | Area4 | Area5 | Area1 | Area2 | Area3 | Area4 | Area5
    |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | 84.6 | 70.6 | 77.7 | 73.6 | 76.9 | 57.9 | 38.9 | 49.5 | 38.5
    | 48.6 |'
  prefs: []
  type: TYPE_TB
- en: '| STRL [[1](#bib.bib1)] | 85.3(+0.7) | 72.4(+1.8) | 79.1(+1.4) | 73.8(+0.2)
    | 77.3(+0.4) | 59.2(+1.3) | 39.2(+0.8) | 51.9(+2.4) | 39.3(+0.8) | 49.5(+0.9)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table [V](#S6.T5 "TABLE V ‣ 6.1 Evaluation Criteria ‣ 6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") lists fine-tuning performance on the ModelNet40 and ScanObjectNN datasets.
    We can see that classification models initialized with unsupervised pre-trained
    weights always achieve better classification performances as compared with random
    initialization, regardless of backbone architectures. On the other hand, the performance
    gaps are still limited, largely due to the limited size and diversity of the pre-training
    datasets (i.e., ShapeNet and ModelNet40) and the simplicity of existing backbone
    models. In comparison, thanks to the much larger pre-training datasets ImageNet [[33](#bib.bib33)]
    and the more powerful backbone network ResNet  [[37](#bib.bib37)], the state-of-the-art
    methods for unsupervised pre-training of 2D images are able to achieve more significant
    performance gains in the classification task. As discussed in Section [7](#S7
    "7 Future direction ‣ Unsupervised Point Cloud Representation Learning with Deep
    Neural Networks: A Survey"), we expect more diverse datasets and more advanced
    and generous backbone models that can set stronger foundations for this field.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Object part segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [VI](#S6.T6 "TABLE VI ‣ 6.2.1 Object classification ‣ 6.2 Object-level
    tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") presents the benchmarking of object part
    segmentation on the ShapeNetPart dataset [[14](#bib.bib14)] using the linear classification
    protocol (i.e., ”Unsup.” in Table [VI](#S6.T6 "TABLE VI ‣ 6.2.1 Object classification
    ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey")) and the fine-tuning
    protocol (i.e., ”Trans.” in Table [VI](#S6.T6 "TABLE VI ‣ 6.2.1 Object classification
    ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey")) as described in
    Section [6.1](#S6.SS1 "6.1 Evaluation Criteria ‣ 6 Benchmark performances ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey"). As
    the table shows, the performance gaps between unsupervised and supervised learning
    (i.e., ”Unsup.” vs. ”Sup.”) are decreasing. In addition, unsupervised pre-training
    achieves better performance in most cases under the fine-tuning protocol (i.e.,
    ”Trans.” vs. ”Sup.”), though the improvement is still limited.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Performances for semantic segmentation on S3DIS [[21](#bib.bib21)].
    Upper part: Models are tested on Area5 (Fold#1) and trained on the rest of the
    data. Lower part: Six-fold cross-validation over three runs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone | mACC | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | SR-UNet | 75.5 | 68.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PointConstrast [[54](#bib.bib54)] | 77.0 | 70.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DepthContrast [[104](#bib.bib104)] | - | 70.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Backbone | OA | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | PointNet | 78.2 | 47.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | 80.1 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | 82.0 | 54.9 |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | PCN | 82.9 | 51.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | 83.7 | 52.2 |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | 85.1 | 53.4 |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | DGCNN | 83.7 | 54.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Jigsaw3D [[13](#bib.bib13)] | 84.1 | 55.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OcCo [[12](#bib.bib12)] | 84.6 | 58.0 |'
  prefs: []
  type: TYPE_TB
- en: 6.3 Scene-level tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in Section [5.2](#S5.SS2 "5.2 Context-based methods ‣ 5 Unsupervised
    point cloud representation learning ‣ Unsupervised Point Cloud Representation
    Learning with Deep Neural Networks: A Survey"), unsupervised pre-training in scene-level
    tasks has recently become prevalent due to its enormous potential in various applications.
    This comes with a series of 3D URL studies that investigate the effectiveness
    of pre-training over different scene-level point cloud datasets. We provide a
    comprehensive benchmarking of these methods with respect to different 3D tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: Comparison of pre-training effects by different unsupervised learning
    methods. The benchmarking is 3D object detection task over datasets SUN RGB-D [[28](#bib.bib28)]
    and ScanNet-V2 [[18](#bib.bib18)]. “@0.25“ and “@0.5“ represent per-category results
    of average precision (AP) with IoU threshold 0.25 (mAP@0.25) and 0.5 (mAP@0.5),
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone | SUN RGB-D | ScanNet-V2 |'
  prefs: []
  type: TYPE_TB
- en: '| @0.5 | @0.25 | @0.5 | @0.25 |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | SR-UNet | 31.7 | 55.6 | 35.4 | 56.7 |'
  prefs: []
  type: TYPE_TB
- en: '| PointConstrast [[54](#bib.bib54)] | 34.8 | 57.5 | 38.0 | 58.5 |'
  prefs: []
  type: TYPE_TB
- en: '| PC-FractalDB [[106](#bib.bib106)] | 35.9 | 57.1 | 37.0 | 59.4 |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | VoteNet | 32.9 | 57.7 | 33.5 | 58.6 |'
  prefs: []
  type: TYPE_TB
- en: '| STRL [[1](#bib.bib1)] | - | 58.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RandRooms [[103](#bib.bib103)] | 35.4 | 59.2 | 36.2 | 61.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DepthContrast [[104](#bib.bib104)] | - | - | - | 62.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CSC [[3](#bib.bib3)] | 33.6 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PointContrast [[54](#bib.bib54)] | 34.0 | - | 38.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 4DContrast [[107](#bib.bib107)] | 34.4 | - | 39.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | PointNet++ | - | 57.5 | - | 58.6 |'
  prefs: []
  type: TYPE_TB
- en: '| PointContrast [[54](#bib.bib54)] | - | 57.9 | - | 58.5 |'
  prefs: []
  type: TYPE_TB
- en: '| RandRooms [[103](#bib.bib103)] | - | 59.2 | - | 61.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DepthContrast [[104](#bib.bib104)] | - | 60.7 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PC-FractalDB [[106](#bib.bib106)] | 33.9 | 59.4 | 38.3 | 61.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DPCo [[108](#bib.bib108)] | 35.6 | 59.8 | 41.5 | 64.2 |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | H3DNet | 39.0 | 60.1 | 48.1 | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: '| RandRooms [[103](#bib.bib103)] | 43.1 | 61.6 | 51.5 | 68.6 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: Object detection performance on dataset ONCE [[30](#bib.bib30)]. The
    baseline is trained from scratch. Unsupervised learning methods are used for pre-training
    models. $U_{small}$, $U_{median}$, and $U_{large}$ represent small, medium, and
    large amounts of unlabelled data that are used for unsupervised learning, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Vehicle | Pedestrian | Cyclist | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline [[149](#bib.bib149)] | 69.7 | 26.1 | 59.9 | 51.9 |'
  prefs: []
  type: TYPE_TB
- en: '| $U_{small}$ |'
  prefs: []
  type: TYPE_TB
- en: '| BYOL [[8](#bib.bib8)] | 67.6 | 17.2 | 53.4 | 46.1 (-5.8) |'
  prefs: []
  type: TYPE_TB
- en: '| PointContrast [[54](#bib.bib54)] | 71.5 | 22.7 | 58.0 | 50.8 (-0.1) |'
  prefs: []
  type: TYPE_TB
- en: '| SwAV [[150](#bib.bib150)] | 72.3 | 25.1 | 60.7 | 52.7 (+0.8) |'
  prefs: []
  type: TYPE_TB
- en: '| DeepCluster [[125](#bib.bib125)] | 72.1 | 27.6 | 50.3 | 53.3 (+1.4) |'
  prefs: []
  type: TYPE_TB
- en: '| $U_{median}$ |'
  prefs: []
  type: TYPE_TB
- en: '| BYOL [[8](#bib.bib8)] | 69.7 | 27.3 | 57.2 | 51.4 (-0.5) |'
  prefs: []
  type: TYPE_TB
- en: '| PointContrast [[54](#bib.bib54)] | 70.2 | 29.2 | 58.9 | 52.8 (+0.9) |'
  prefs: []
  type: TYPE_TB
- en: '| SwAV [[150](#bib.bib150)] | 72.1 | 28.0 | 60.2 | 53.4 (+1.5) |'
  prefs: []
  type: TYPE_TB
- en: '| DeepCluster [[125](#bib.bib125)] | 72.1 | 30.1 | 60.5 | 54.2 (+2.3) |'
  prefs: []
  type: TYPE_TB
- en: '| $U_{large}$ |'
  prefs: []
  type: TYPE_TB
- en: '| BYOL [[8](#bib.bib8)] | 72.2 | 23.6 | 60.5 | 52.1 (+0.2) |'
  prefs: []
  type: TYPE_TB
- en: '| PointContrast [[54](#bib.bib54)] | 73.2 | 27.5 | 58.3 | 53.0 (+1.1) |'
  prefs: []
  type: TYPE_TB
- en: '| SwAV [[150](#bib.bib150)] | 72.0 | 30.6 | 60.3 | 54.3 (+2.4) |'
  prefs: []
  type: TYPE_TB
- en: '| DeepCluster [[125](#bib.bib125)] | 71.9 | 30.5 | 60.4 | 54.3 (+2.4) |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XI: Performances of instance segmentation on datasets S3DIS [[21](#bib.bib21)]
    and ScanNet-V2 [[18](#bib.bib18)]. It reports the mean of average precision (mAP)
    across all semantic classes with a 3D IoU threshold of 0.25.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone | S3DIS | ScanNet |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| from scratch | SR-UNet | 59.3 | 53.4 |'
  prefs: []
  type: TYPE_TB
- en: '| PointContrast [[54](#bib.bib54)] | 60.5 | 55.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CSC [[3](#bib.bib3)] | 63.4 | 56.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 4DContrast [[107](#bib.bib107)] | - | 57.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Tables [VII](#S6.T7 "TABLE VII ‣ 6.2.1 Object classification ‣ 6.2 Object-level
    tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") and [VIII](#S6.T8 "TABLE VIII ‣ 6.2.2 Object
    part segmentation ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey") show
    the performances of semantic segmentation on the S3DIS [[21](#bib.bib21)] dataset.
    We summarized them separately since different fine-tuning setups have been used
    in prior works. In Table [VII](#S6.T7 "TABLE VII ‣ 6.2.1 Object classification
    ‣ 6.2 Object-level tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud
    Representation Learning with Deep Neural Networks: A Survey"), the unsupervised
    pre-trained DGCNN is fine-tuned on every single area of S3DIS and tested on either
    Area 5 (the upper part of table) or Area 6 (the lower part of the table). Table
    [VIII](#S6.T8 "TABLE VIII ‣ 6.2.2 Object part segmentation ‣ 6.2 Object-level
    tasks ‣ 6 Benchmark performances ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey") instead shows the performance of fine-tuning
    different segmentation networks with the whole dataset by following the one-fold
    (in the upper part of the table) and six-fold cross-validation setups (in the
    lower part of the table), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also summarize existing works that handle unsupervised pre-training for
    object detection. Tables [IX](#S6.T9 "TABLE IX ‣ 6.3 Scene-level tasks ‣ 6 Benchmark
    performances ‣ Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey") and [X](#S6.T10 "TABLE X ‣ 6.3 Scene-level tasks ‣ 6 Benchmark
    performances ‣ Unsupervised Point Cloud Representation Learning with Deep Neural
    Networks: A Survey") show their performances over indoor datasets including SUN
    RGB-D [[28](#bib.bib28)] and ScanNet-V2 [[18](#bib.bib18)] as well as outdoor
    LiDAR dataset ONCE [[30](#bib.bib30)], respectively. In addition, several works
    investigated unsupervised pre-training for instance segmentation. We summarize
    their performance over S3DIS [[21](#bib.bib21)] and ScanNet-V2 [[18](#bib.bib18)]
    in Table [XI](#S6.T11 "TABLE XI ‣ 6.3 Scene-level tasks ‣ 6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: It is inspiring to see that unsupervised learning representation can generalize
    across domains and boost performances over multiple high-level 3D tasks as compared
    with training from scratch. These experiments demonstrate the huge potential of
    URL of point clouds in saving expensive human annotations. However, the improvements
    are still limited and we expect more research in this area.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: URL of point clouds has achieved significant progress during the last decade.
    We share several potential future research directions of this research field in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unified 3D backbones are needed: One major reason of the great success of deep
    learning in 2D computer vision is the standardization of CNN architectures with
    VGG [[36](#bib.bib36)], ResNet [[37](#bib.bib37)], etc. For example, the unified
    backbone structures greatly facilitate knowledge transfer across different datasets
    and tasks. For 3D point clouds, similar development is far under-explored, despite
    a variety of 3D deep architectures that have been recently reported. This can
    be observed from the URL methods in tables in Section [6](#S6 "6 Benchmark performances
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey") most of which adopted very different backbone models. This impedes
    the development of 3D point cloud networks in scalable design and efficient deployment
    in various new tasks. Designing certain universal backbones that can be as ubiquitous
    as ResNet in 2D computer vision is crucial for the advance of 3D point cloud networks
    including unsupervised point cloud representation learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Larger datasets are needed: As described in Section [3](#S3 "3 Point cloud
    datasets ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), most existing URL datasets were originally collected for the task
    of supervised learning. Since point cloud annotation is laborious and time-consuming,
    these datasets are severely constrained in data size and data diversity and are
    not suitable for URL with point clouds which usually requires large amounts of
    point clouds of good size and diversity. This issue well explains the trivial
    improvements by URL in tables in Section [6](#S6 "6 Benchmark performances ‣ Unsupervised
    Point Cloud Representation Learning with Deep Neural Networks: A Survey"). Hence,
    it is urgent to collect large-scale and high-quality unlabelled point cloud datasets
    of sufficient diversity in terms of object-level and scene-level point clouds,
    indoor and outdoor point clouds, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised pre-training for scene-level tasks: As described in Section [5.2](#S5.SS2
    "5.2 Context-based methods ‣ 5 Unsupervised point cloud representation learning
    ‣ Unsupervised Point Cloud Representation Learning with Deep Neural Networks:
    A Survey"), most earlier research focuses on object-level point cloud processing
    though several pioneer studies [[54](#bib.bib54), [3](#bib.bib3), [123](#bib.bib123),
    [103](#bib.bib103), [1](#bib.bib1)] explored how to pre-train DNNs on scene-level
    point clouds for improving various scene-level downstream tasks such as object
    detection and instance segmentation. Prior studies show that the learned unsupervised
    representations can effectively generalize across domains and tasks. Hence, URL
    of scene-level point clouds deserves more attention as a new direction due to
    its great potential in a variety of applications. On the other hand, the research
    along this line remains at a nascent stage, largely due to the constraints in
    network architectures and datasets. We foresee that more related research will
    be conducted in the near future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning representations from multi-modal data: 3D sensors are often equipped
    with other sensors that can capture additional and complementary information to
    point clouds. For example, depth cameras are often equipped with optical sensors
    for capturing better appearance information. LiDAR sensors, optical sensors, GPU,
    and IMU are often installed together as a sensor suite to capture complementary
    information and provide certain redundancy in autonomous vehicles and mobile robot
    navigation. Unsupervised learning from such multi-modal data has attracted increasing
    attention in recent years. For example, learning correspondences among multi-modal
    data has been explored as pre-text tasks for unsupervised learning as described
    in Section [5.3](#S5.SS3 "5.3 Multiple modal-based methods ‣ 5 Unsupervised point
    cloud representation learning ‣ Unsupervised Point Cloud Representation Learning
    with Deep Neural Networks: A Survey"). However, the study along this line of research
    remains under-investigated and we expect more related research point clouds, RGB
    images, depth maps, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning Spatio-temporal representations: 3D sensors that support capturing
    sequential point clouds are becoming increasingly popular nowadays. Rich temporal
    information from point cloud streams can be extracted as useful supervision signals
    for unsupervised learning while most of the existing works still focus on static
    point clouds. We expect that more effective pretext tasks will be designed that
    can effectively learn spatio-temporal representations from unlabelled sequential
    point cloud frames.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised representation learning aims to learn effective representations
    from unannotated data, which has demonstrated impressive progress in the research
    with point cloud data. This paper presents a contemporary survey of unsupervised
    representation learning of point clouds. It first introduces the widely adopted
    datasets and deep network architectures. A comprehensive taxonomy and detailed
    review of methods are then presented. Following that, representative methods are
    discussed and benchmarked over multiple 3D point cloud tasks. Finally, we share
    our humble opinions about several potential future research directions. We hope
    that this work can lay a strong and sound foundation for future research in unsupervised
    representation learning from point cloud data.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This project is funded in part by the Ministry of Education Singapore, under
    the Tier-1 scheme with project number RG18/22\. It is also supported in part under
    the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP)
    Funding Initiative, as well as cash and in-kind contributions from Singapore Telecommunications
    Limited (Singtel), through Singtel Cognitive and Artificial Intelligence Lab for
    Enterprises (SCALE@NTU).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] S. Huang, Y. Xie, S.-C. Zhu, and Y. Zhu, “Spatio-temporal self-supervised
    representation learning for 3d point clouds,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 6535–6545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep learning
    for 3d point clouds: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] J. Hou, B. Graham, M. Nießner, and S. Xie, “Exploring data-efficient 3d
    scene understanding with contrastive scene contexts,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 15 587–15 597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] L. Jing, L. Zhang, and Y. Tian, “Self-supervised feature learning by cross-modality
    and cross-view correspondences,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2021, pp. 1581–1591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep
    bidirectional transformers for language understanding,” in *Proceedings of NAACL-HLT*,
    2019, pp. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised
    visual representation learning,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2020, pp. 9729–9738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya,
    C. Doersch, B. Pires, Z. Guo, M. Azar *et al.*, “Bootstrap your own latent: A
    new approach to self-supervised learning,” in *Neural Information Processing Systems*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with momentum
    contrastive learning,” *arXiv preprint arXiv:2003.04297*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders
    are scalable vision learners,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 16 000–16 009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] D. Valsesia, G. Fracastoro, and E. Magli, “Learning localized generative
    models for 3d point clouds via graph convolution,” in *International conference
    on learning representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] H. Wang, Q. Liu, X. Yue, J. Lasenby, and M. J. Kusner, “Unsupervised point
    cloud pre-training via occlusion completion,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision (ICCV)*, October 2021, pp. 9782–9792.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Sauder and B. Sievers, “Self-supervised deep learning on point clouds
    by reconstructing space,” *Advances in Neural Information Processing Systems*,
    vol. 32, pp. 12 962–12 972, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3d model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for 3d
    object detection in point clouds,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 9277–9286.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation
    and detection from point cloud,” in *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*, 2019, pp. 770–779.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2017, pp.
    5828–5839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The kitti dataset,” *The International Journal of Robotics Research*, vol. 32,
    no. 11, pp. 1231–1237, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for
    3d object detection from rgb-d data,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2018, pp. 918–927.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and
    S. Savarese, “3d semantic parsing of large-scale indoor spaces,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2016, pp.
    1534–1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Ioannidou, E. Chatzilari, S. Nikolopoulos, and I. Kompatsiaris, “Deep
    learning advances in computer vision with 3d data: A survey,” *ACM Computing Surveys
    (CSUR)*, vol. 50, no. 2, pp. 1–38, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Y. Xie, J. Tian, and X. X. Zhu, “Linking points with labels in 3d: A review
    of point cloud semantic segmentation,” *IEEE Geoscience and Remote Sensing Magazine*,
    vol. 8, no. 4, pp. 38–59, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, “Self-supervised
    learning: Generative or contrastive,” *IEEE Transactions on Knowledge and Data
    Engineering*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] G.-J. Qi and J. Luo, “Small data challenges in big data era: A survey
    of recent progress on unsupervised and semi-supervised methods,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
    shapenets: A deep representation for volumetric shapes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 1912–1920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2015, pp. 567–576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019, pp. 1588–1597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W. Zhang,
    Z. Li *et al.*, “One million scenes for autonomous driving: Once dataset,” *arXiv
    preprint arXiv:2106.11037*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
    synthia dataset: A large collection of synthetic images for semantic segmentation
    of urban scenes,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2016, pp. 3234–3243.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE conference on computer
    vision and pattern recognition*.   Ieee, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Hornung, K. M. Wurm, M. Bennewitz, C. Stachniss, and W. Burgard, “Octomap:
    An efficient probabilistic 3d mapping framework based on octrees,” *Autonomous
    robots*, vol. 34, no. 3, pp. 189–206, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. Nießner, M. Zollhöfer, S. Izadi, and M. Stamminger, “Real-time 3d reconstruction
    at scale using voxel hashing,” *ACM Transactions on Graphics (ToG)*, vol. 32,
    no. 6, pp. 1–11, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *International Conference on Learning Representations*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view convolutional
    neural networks for 3d shape recognition,” in *Proceedings of the IEEE international
    conference on computer vision*, 2015, pp. 945–953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] T. Yu, J. Meng, and J. Yuan, “Multi-view harmonized bilinear network for
    3d object recognition,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 186–194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] B. Yang, W. Luo, and R. Urtasun, “Pixor: Real-time 3d object detection
    from point clouds,” in *Proceedings of the IEEE conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 7652–7660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Z. Yang and L. Wang, “Learning relationships for multi-view 3d object
    recognition,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 7505–7514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] X. Wei, R. Yu, and J. Sun, “View-gcn: View-based graph convolutional network
    for 3d shape analysis,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 1850–1859.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Xiao, X. Yang, S. Lu, D. Guan, and J. Huang, “Fps-net: A convolutional
    fusion network for large-scale lidar point cloud segmentation,” *ISPRS Journal
    of Photogrammetry and Remote Sensing*, vol. 176, pp. 237–249, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Q. Huang, W. Wang, and U. Neumann, “Recurrent slice networks for 3d segmentation
    of point clouds,” in *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition*, 2018, pp. 2626–2635.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] X. Ye, J. Li, H. Huang, L. Du, and X. Zhang, “3d recurrent neural networks
    with context fusion for point cloud semantic segmentation,” in *Proceedings of
    the European conference on computer vision (ECCV)*, 2018, pp. 403–417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem, “3d-prnn: Generating
    shape primitives with recurrent neural networks,” in *Proceedings of the IEEE
    International Conference on Computer Vision*, 2017, pp. 900–909.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Y. Zhao, T. Birdal, H. Deng, and F. Tombari, “3d point capsule networks,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 1009–1018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
    “Dynamic graph cnn for learning on point clouds,” *Acm Transactions On Graphics
    (tog)*, vol. 38, no. 5, pp. 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] B. Graham, M. Engelcke, and L. Van Der Maaten, “3d semantic segmentation
    with submanifold sparse convolutional networks,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 9224–9232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets: Minkowski
    convolutional neural networks,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2019, pp. 3075–3084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. Tang, Z. Liu, X. Li, Y. Lin, and S. Han, “TorchSparse: Efficient Point
    Cloud Inference Engine,” in *Conference on Machine Learning and Systems (MLSys)*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han, “Searching
    efficient 3d architectures with sparse point-voxel convolution,” in *European
    conference on computer vision*.   Springer, 2020, pp. 685–702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany, “Pointcontrast:
    Unsupervised pre-training for 3d point cloud understanding,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 574–591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Liu, B. Fan, S. Xiang, and C. Pan, “Relation-shape convolutional neural
    network for point cloud analysis,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 8895–8904.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, “Point-bert: Pre-training
    3d point cloud transformers with masked point modeling,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 19 313–19 322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
    transformer: Hierarchical vision transformer using shifted windows,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 10 012–10 022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point transformer,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*,
    October 2021, pp. 16 259–16 268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] A. Sharma, O. Grau, and M. Fritz, “Vconv-dae: Deep volumetric shape learning
    without object labels,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 236–250.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta, “Learning a predictable
    and generative vector representation for objects,” in *European Conference on
    Computer Vision*.   Springer, 2016, pp. 484–499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenenbaum, “Learning
    a probabilistic latent space of object shapes via 3d generative-adversarial modeling,”
    in *Proceedings of the 30th International Conference on Neural Information Processing
    Systems*, 2016, pp. 82–90.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] J. Xie, Z. Zheng, R. Gao, W. Wang, S.-C. Zhu, and Y. N. Wu, “Learning
    descriptor networks for 3d shape synthesis and analysis,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2018, pp. 8629–8638.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Y. Yang, C. Feng, Y. Shen, and D. Tian, “Foldingnet: Point cloud auto-encoder
    via deep grid deformation,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018, pp. 206–215.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. Li, B. M. Chen, and G. H. Lee, “So-net: Self-organizing network for
    point cloud analysis,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 9397–9406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learning representations
    and generative models for 3d point clouds,” in *International conference on machine
    learning*.   PMLR, 2018, pp. 40–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. Gadelha, R. Wang, and S. Maji, “Multiresolution tree networks for 3d
    point cloud processing,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 103–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Z. Han, M. Shang, Y.-S. Liu, and M. Zwicker, “View inter-prediction gan:
    Unsupervised representation learning for 3d shapes by learning global shape memories
    to support local view predictions,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 33, no. 01, 2019, pp. 8376–8384.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] X. Liu, Z. Han, X. Wen, Y.-S. Liu, and M. Zwicker, “L2g auto-encoder:
    Understanding point clouds by local-to-global reconstruction with hierarchical
    self-attention,” in *Proceedings of the 27th ACM International Conference on Multimedia*,
    2019, pp. 989–997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Z. Han, X. Wang, Y.-S. Liu, and M. Zwicker, “Multi-angle point cloud-vae:
    Unsupervised feature learning for 3d point clouds from multiple angles by joint
    self-reconstruction and half-to-half prediction,” in *2019 IEEE/CVF International
    Conference on Computer Vision (ICCV)*.   IEEE, 2019, pp. 10 441–10 450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] G. Yang, X. Huang, Z. Hao, M.-Y. Liu, S. Belongie, and B. Hariharan, “Pointflow:
    3d point cloud generation with continuous normalizing flows,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2019, pp. 4541–4550.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Y. Shi, M. Xu, S. Yuan, and Y. Fang, “Unsupervised deep shape descriptor
    with point distribution learning,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 9353–9362.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] X. Gao, W. Hu, and G.-J. Qi, “Graphter: Unsupervised learning of graph
    transformation equivariant representations via auto-encoding node-wise transformations,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 7163–7172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Wen, T. Li, Z. Han, and Y.-S. Liu, “Point cloud completion by skip-attention
    network with hierarchical folding,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 1939–1948.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Y. Sun, Y. Wang, Z. Liu, J. Siegel, and S. Sarma, “Pointgrow: Autoregressively
    learned point cloud generation with self-attention,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2020, pp. 61–70.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. Yang, P. Ahn, D. Kim, H. Lee, and J. Kim, “Progressive seed generation
    auto-encoder for unsupervised point cloud learning,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 6413–6422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, “Masked autoencoders
    for point cloud self-supervised learning,” in *Computer Vision–ECCV 2022: 17th
    European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
    II*.   Springer, 2022, pp. 604–621.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] R. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, and H. Li,
    “Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training,”
    *Advances in neural information processing systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] M. A. Kramer, “Nonlinear principal component analysis using autoassociative
    neural networks,” *AIChE journal*, vol. 37, no. 2, pp. 233–243, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Autoencoder, “Autoencoder — Wikipedia, the free encyclopedia,” 2022, [Online;
    accessed 16-Feb-2022]. [Online]. Available: [https://en.wikipedia.org/wiki/Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] H. Fan, H. Su, and L. J. Guibas, “A point set generation network for 3d
    object reconstruction from a single image,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 605–613.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,”
    *Advances in neural information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Chen, C. Duan, Y. Yang, D. Li, C. Feng, and D. Tian, “Deep Unsupervised
    Learning of 3D Point Clouds via Graph Topology Inference and Filtering,” *IEEE
    Transactions on Image Processing*, vol. 29, pp. 3183–3198, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] H. Chen, S. Luo, X. Gao, and W. Hu, “Unsupervised learning of geometric
    sampling invariant representations for 3d point clouds,” in *2021 IEEE/CVF International
    Conference on Computer Vision Workshops (ICCVW)*.   IEEE Computer Society, 2021,
    pp. 893–903.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *Advances in neural
    information processing systems*, vol. 27, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, and R. Salakhutdinov, “Point
    cloud gan,” *arXiv preprint arXiv:1810.05795*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] R. Li, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, “Pu-gan: a point
    cloud upsampling adversarial network,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 7203–7212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] E. Remelli, P. Baque, and P. Fua, “Neuralsampler: Euclidean point cloud
    auto-encoder and sampler,” *arXiv preprint arXiv:1901.09394*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, and P.-A. Heng, “Pu-net: Point cloud
    upsampling network,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 2790–2799.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] W. Yifan, S. Wu, H. Huang, D. Cohen-Or, and O. Sorkine-Hornung, “Patch-based
    progressive 3d point set upsampling,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 5958–5967.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. Qian, J. Hou, S. Kwong, and Y. He, “Pugeo-net: A geometry-centric network
    for 3d point cloud upsampling,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 752–769.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] G. Qian, A. Abualshour, G. Li, A. Thabet, and B. Ghanem, “Pu-gcn: Point
    cloud upsampling using graph convolutional networks,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 11 683–11 692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] R. Li, X. Li, P.-A. Heng, and C.-W. Fu, “Point cloud upsampling via disentangled
    refinement,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 344–353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] K. Hassani and M. Haley, “Unsupervised multi-task feature learning on
    point clouds,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 8160–8171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] L. Zhang and Z. Zhu, “Unsupervised feature learning for point cloud understanding
    by contrasting and clustering using graph convolutional neural networks,” in *2019
    International Conference on 3D Vision (3DV)*.   IEEE, 2019, pp. 395–404.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Y. Rao, J. Lu, and J. Zhou, “Global-local bidirectional reasoning for
    unsupervised representation learning of 3d point clouds,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp. 5376–5385.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Sanghi, “Info3d: Representation learning on 3d objects using mutual
    information maximization and contrastive learning,” in *European Conference on
    Computer Vision*.   Springer, 2020, pp. 626–642.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] M. Gadelha, A. RoyChowdhury, G. Sharma, E. Kalogerakis, L. Cao, E. Learned-Miller,
    R. Wang, and S. Maji, “Label-efficient learning on point clouds using approximate
    convex decompositions,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 473–491.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] O. Poursaeed, T. Jiang, H. Qiao, N. Xu, and V. G. Kim, “Self-supervised
    learning of point clouds via orientation estimation,” in *2020 International Conference
    on 3D Vision (3DV)*.   IEEE, 2020, pp. 1018–1028.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] B. Du, X. Gao, W. Hu, and X. Li, “Self-contrastive learning with hard
    negative sampling for self-supervised point cloud learning,” in *Proceedings of
    the 29th ACM International Conference on Multimedia*, 2021, pp. 3133–3142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Rao, B. Liu, Y. Wei, J. Lu, C.-J. Hsieh, and J. Zhou, “Randomrooms:
    Unsupervised pre-training from synthetic shapes and randomized layouts for 3d
    object detection,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2021, pp. 3283–3292.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Z. Zhang, R. Girdhar, A. Joulin, and I. Misra, “Self-supervised pretraining
    of 3d features on any point-cloud,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, October 2021, pp. 10 252–10 263.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Chen, J. Liu, B. Ni, H. Wang, J. Yang, N. Liu, T. Li, and Q. Tian,
    “Shape self-correction for unsupervised point cloud understanding,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 8382–8391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] R. Yamada, H. Kataoka, N. Chiba, Y. Domae, and T. Ogata, “Point cloud
    pre-training with natural 3d structures,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2022, pp. 21 283–21 293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Chen, M. Nießner, and A. Dai, “4dcontrast: Contrastive learning with
    dynamic correspondences for 3d scene understanding,” in *Computer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXXII*.   Springer, 2022, pp. 543–560.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] L. Li and M. Heizmann, “A closer look at invariances in self-supervised
    pre-training for 3d vision,” in *Computer Vision–ECCV 2022: 17th European Conference,
    Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXX*.   Springer, 2022,
    pp. 656–673.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] J. Yin, D. Zhou, L. Zhang, J. Fang, C.-Z. Xu, J. Shen, and W. Wang, “Proposalcontrast:
    Unsupervised pre-training for lidar-based 3d object detection,” in *Computer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXXIX*.   Springer, 2022, pp. 17–33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] H. Liu, M. Cai, and Y. J. Lee, “Masked discrimination for self-supervised
    learning on point clouds,” in *Computer Vision–ECCV 2022: 17th European Conference,
    Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part II*.   Springer, 2022,
    pp. 657–675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] K. Liu, A. Xiao, X. Zhang, S. Lu, and L. Shao, “Fac: 3d representation
    learning via foreground aware feature contrast,” in *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry, “A papier-mâché
    approach to learning 3d surface generation,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 216–224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, “Pcn: Point completion
    network,” in *2018 International Conference on 3D Vision (3DV)*.   IEEE, 2018,
    pp. 728–737.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Z. Huang, Y. Yu, J. Xu, F. Ni, and X. Le, “Pf-net: Point fractal network
    for 3d point cloud completion,” in *2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, 2020, pp. 7659–7667.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M. Liu, L. Sheng, S. Yang, J. Shao, and S.-M. Hu, “Morphing and sampling
    network for dense point cloud completion,” in *Proceedings of the AAAI conference
    on artificial intelligence*, vol. 34, no. 07, 2020, pp. 11 596–11 603.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] W. Zhang, Q. Yan, and C. Xiao, “Detail preserved point cloud completion
    via separated feature aggregation,” in *Computer Vision–ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16*.   Springer,
    2020, pp. 512–528.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] C. Xie, C. Wang, B. Zhang, H. Yang, D. Chen, and F. Wen, “Style-based
    point generator with adversarial rendering for point cloud completion,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 4619–4628.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] K. Fu, P. Gao, S. Liu, R. Zhang, Y. Qiao, and M. Wang, “Pos-bert: Point
    cloud one-stage bert pre-training,” *arXiv preprint arXiv:2204.00989*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
    for contrastive learning of visual representations,” in *International conference
    on machine learning*.   PMLR, 2020, pp. 1597–1607.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive
    predictive coding,” *arXiv preprint arXiv:1807.03748*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] P.-S. Wang, Y.-Q. Yang, Q.-F. Zou, Z. Wu, Y. Liu, and X. Tong, “Unsupervised
    3d learning for shape analysis via multiresolution instance discrimination,” *ACM
    Trans. Graphic*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] J. Jiang, X. Lu, W. Ouyang, and M. Wang, “Unsupervised representation
    learning for 3d point cloud data,” *arXiv preprint arXiv:2110.06632*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] J. Hou, S. Xie, B. Graham, A. Dai, and M. Nießner, “Pri3d: Can 3d priors
    help 2d representation learning?” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, October 2021, pp. 5693–5702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. A. Hartigan and M. A. Wong, “Algorithm as 136: A k-means clustering
    algorithm,” *Journal of the royal statistical society. series c (applied statistics)*,
    vol. 28, no. 1, pp. 100–108, 1979.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
    unsupervised learning of visual features,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 132–149.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Sauder and B. Sievers, “Context prediction for unsupervised deep learning
    on point clouds,” *arXiv preprint arXiv:1901.08396*, vol. 2, no. 4, p. 5, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation
    learning by predicting image rotations,” in *International Conference on Learning
    Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Thabet, H. Alwassel, and B. Ghanem, “Self-supervised learning of local
    features in 3d point clouds,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops*, 2020, pp. 938–939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] C. Sun, Z. Zheng, X. Wang, M. Xu, and Y. Yang, “Point cloud pre-training
    by mixing and disentangling,” *arXiv preprint arXiv:2109.00452*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    and J. Gall, “Semantickitti: A dataset for semantic scene understanding of lidar
    sequences,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 9297–9307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] A. Xiao, J. Huang, D. Guan, F. Zhan, and S. Lu, “Transfer learning from
    synthetic to real lidar point cloud for semantic segmentation,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 36, no. 3, 2022, pp.
    2795–2803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] C. Feichtenhofer, H. Fan, B. Xiong, R. Girshick, and K. He, “A large-scale
    study on unsupervised spatiotemporal representation learning,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 3299–3309.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] X. Song, S. Zhao, J. Yang, H. Yue, P. Xu, R. Hu, and H. Chai, “Spatio-temporal
    contrastive domain adaptation for action recognition,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 9787–9795.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] K. Hu, J. Shao, Y. Liu, B. Raj, M. Savvides, and Z. Shen, “Contrast and
    order representations for video self-supervised learning,” in *Proceedings of
    the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 7939–7949.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] H. Kuang, Y. Zhu, Z. Zhang, X. Li, J. Tighe, S. Schwertfeger, C. Stachniss,
    and M. Li, “Video contrastive learning with global context,” in *Proceedings of
    the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 3195–3204.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] D. Z. Chen, A. X. Chang, and M. Nießner, “Scanrefer: 3d object localization
    in rgb-d scans using natural language,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 202–221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna,
    and R. Rodrigo, “Crosspoint: Self-supervised cross-modal contrastive learning
    for 3d point cloud understanding,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 9902–9912.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Z. Wang, X. Yu, Y. Rao, J. Zhou, and J. Lu, “P2p: Tuning pre-trained
    image models for point cloud analysis with point-to-pixel prompting,” *Advances
    in neural information processing systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] H. Deng, T. Birdal, and S. Ilic, “Ppf-foldnet: Unsupervised learning
    of rotation invariant 3d local descriptors,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 602–618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and T. Funkhouser,
    “3dmatch: Learning local geometric descriptors from rgb-d reconstructions,” in
    *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2017, pp. 1802–1811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Y. Zeng, Y. Qian, Z. Zhu, J. Hou, H. Yuan, and Y. He, “Corrnet3d: unsupervised
    end-to-end learning of dense correspondence for 3d point clouds,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 6052–6061.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] I. Lang, D. Ginzburg, S. Avidan, and D. Raviv, “Dpc: Unsupervised deep
    point correspondence via cross and self construction,” in *2021 International
    Conference on 3D Vision (3DV)*.   IEEE, 2021, pp. 1442–1451.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] H. Jiang, Y. Shen, J. Xie, J. Li, J. Qian, and J. Yang, “Sampling network
    guided cross-entropy method for unsupervised point cloud registration,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 6128–6137.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point transformer,”
    in *Proceedings of the IEEE/CVF international conference on computer vision*,
    2021, pp. 16 259–16 268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz, “Rotation invariant spherical
    harmonic representation of 3 d shape descriptors,” in *Symposium on geometry processing*,
    vol. 6, 2003, pp. 156–164.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung, “On visual similarity
    based 3d model retrieval,” in *Computer graphics forum*, vol. 22, no. 3.   Wiley
    Online Library, 2003, pp. 223–232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] B. Eckart, W. Yuan, C. Liu, and J. Kautz, “Self-supervised learning on
    3d point clouds by learning discrete generative models,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 8248–8257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” *Journal
    of machine learning research*, vol. 9, no. 11, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional detection,”
    *Sensors*, vol. 18, no. 10, p. 3337, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,
    “Unsupervised learning of visual features by contrasting cluster assignments,”
    *Advances in Neural Information Processing Systems*, vol. 33, pp. 9912–9924, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/dc667d3bd71eb0fcf6152d59da45d9fd.png) | Aoran
    Xiao received his B.Sc. and M.Sc. degree from Wuhan University, China in 2016
    and 2019, respectively. He is currently pursuing the Ph.D. degree with the school
    of computer science and engineering at Nanyang Technological University, Singapore.
    His research interests lie in point cloud processing, computer vision, and remote
    sensing. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/519ed1d04f38970f4ec322b419d2464b.png) | Jiaxing
    Huang received his B.Eng. and M.Sc. in EEE from the University of Glasgow, UK,
    and the Nanyang Technological University (NTU), Singapore, respectively. He is
    currently a Research Associate and Ph.D. student with School of Computer Science
    and Engineering, NTU, Singapore. His research interests include computer vision
    and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/32a1f6341548c7dd617e1f0326bdae09.png) | Dayan
    Guan is currently a Research Scientist at Mohamed bin Zayed University of Artificial
    Intelligence, United Arab Emirates. Before that, he had been a Research Fellow
    at Nanyang Technological University from Nov 2019 to Mar 2022\. In Sep 2019, he
    received his Ph.D. from Zhejiang University, China. His research interests include
    computer vision, pattern recognition and deep learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/08bee97017240cb66de44a95b2d61c41.png) | Xiaoqin
    Zhang is a senior member of the IEEE. He received the B.Sc. degree in electronic
    information science and technology from Central South University, China, in 2005,
    and the Ph.D. degree in pattern recognition and intelligent system from the National
    Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of
    Sciences, China, in 2010\. He is currently a Professor with Wenzhou University,
    China. He has published more than 100 papers in international and national journals,
    and international conferences, including IEEE T-PAMI, IJCV, IEEE T-IP, IEEE T-NNLS,
    IEEE T-C, ICCV, CVPR, NIPS, IJCAI, AAAI, and among others. His research interests
    include in pattern recognition, computer vision, and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/c02bcf06ed26db965b2457ebcd13290d.png) | Shijian
    Lu is an Associate Professor with the School of Computer Science and Engineering
    at the Nanyang Technological University, Singapore. He received his PhD in electrical
    and computer engineering from the National University of Singapore. His major
    research interests include image and video analytics, visual intelligence, and
    machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/a69193d2971afcd51c063197002e7771.png) | Ling Shao
    is the Chief Scientist of Terminus Group and the President of Terminus International.
    He was the founding CEO and Chief Scientist of the Inception Institute of Artificial
    Intelligence, Abu Dhabi, UAE. His research interests include computer vision,
    deep learning, medical imaging and vision and language. He is a fellow of the
    IEEE, the IAPR, the BCS and the IET. |'
  prefs: []
  type: TYPE_TB
