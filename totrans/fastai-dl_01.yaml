- en: 'Deep Learning 2: Part 1 Lesson 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197](https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Lesson 1](http://forums.fast.ai/t/wiki-lesson-1/9398/1)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting started [[0:00](https://youtu.be/IPBSB1HLNLo)]:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to train a neural network, you will most certainly need Graphics Processing
    Unit (GPU) — specifically NVIDIA GPU because it is the only one that supports
    CUDA (the language and framework that nearly all deep learning libraries and practitioners
    use).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several ways to rent GPU: Crestle [[04:06](https://youtu.be/IPBSB1HLNLo?t=4m6s)],
    Paperspace [[06:10](https://youtu.be/IPBSB1HLNLo?t=6m10s)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Jupyter Notebook and Dogs vs. Cats](https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb)
    [[12:39](https://youtu.be/IPBSB1HLNLo?t=12m39s)]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can run a cell by selecting it and hitting `shift+enter` (you can hold down
    `shift` and hit `enter` multiple times to keep going down the cells), or you can
    click on Run button at the top. A cell can contain code, text, picture, video,
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast.ai requires Python 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**First look at pictures [**[**15:39**](https://youtu.be/IPBSB1HLNLo?t=15m40s)**]**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`!` tells to use bash (shell) instead of python'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are not familiar with training set and validation set, check out Practical
    Machine Learning class (or read [Rachel’s blog](http://www.fast.ai/2017/11/13/validation-sets/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This folder structure is the most common approach for how image classification
    dataset is shared and provided. Each folder tells you the label (e.g. `dogs` or
    `cats`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`f’{PATH}valid/cats/{files[0]}’` — This is a Python 3.6\. format string which
    is a convenient to format a string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`img` is a 3 dimensional array (a.k.a. rank 3 tensor)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The three items (e.g. `[29, 20, 23]`) represents Red Green Blue pixel values
    between 0 and 255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is to take these numbers and use them to predict whether those numbers
    represent a cat or a dog based on looking at lots of pictures of cats and dogs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset comes from [Kaggle competition](https://www.kaggle.com/c/dogs-vs-cats),
    and when it was released (back in 2013) the state-of-the-art was 80% accurate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Let’s train a model [**[**20:21**](https://youtu.be/IPBSB1HLNLo?t=20m21s)**]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the three lines of code necessary to train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will do 3 **epochs** which means it is going to look at the entire set
    of images three times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last of three numbers in the output is the accuracy on the validation set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two are the value of loss function (in this case the cross-entropy
    loss) for the training set and the validation set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The start (e.g. `0.`, `1.`) is the epoch number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We achieved ~99% (which would have won the Kaggle competition back in 2013)
    in 17 seconds with 3 lines of code! [[21:49](https://youtu.be/IPBSB1HLNLo?t=21m49s)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of people assume that deep learning takes a huge amount of time, lots
    of resources, and lots of data — that, in general, is not true!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast.ai Library [[22:24](https://youtu.be/IPBSB1HLNLo?t=22m24s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The library takes all of the best practices and approaches they can find — each
    time a paper comes out that looks interesting, they test it out and if it works
    well for a variety of datasets and they can figure out how to tune it, it gets
    implement it in the library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast.ai curates all these best practices and packages up for you, and most of
    the time, figures out the best way to handle things automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast.ai sits on top of a library called PyTorch which is a really flexible deep
    learning, machine learning, and GPU computation library written by Facebook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most people are more familiar with TensorFlow than PyTorch, but most of the
    top researchers Jeremy knows nowadays have switched across to PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast.ai is flexible that you can use all these curated best practices as much
    or as little as you want. It is easy to hook in at any point and write your own
    data augmentation, loss function, network architecture, etc, and we will learn
    all that in this course.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing results [[24:21](https://youtu.be/IPBSB1HLNLo?t=24m12s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is what the validation dataset label (think of it as the correct answers)
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: What do these 0’s and 1’s represents?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`data` contains the validation and training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learn` contains the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s make predictions for the validation set (predictions are in log scale):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output represents a prediction for cats, and prediction for dogs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In PyTorch and Fast.ai, most models return the log of the predictions rather
    than the probabilities themselves (we will learn why later in the course). For
    now, just know that to get probabilities, you have to do `np.exp()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure you familiarize yourself with numpy (`np`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The number above the image is the probability of being a dog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'More interestingly, here are what the model thought it was definitely a dog
    but turns out to be a cat, or vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Why is it important to look at these images? The first thing Jeremy does after
    he builds a model is to find a way to visualize what it has built. Because if
    he wants to make the model better, then he needs to take advantage of the things
    that is doing well and fix the things that is doing badly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, we have learned something about the dataset itself which is that
    there are some images that are in here that probably should not be. But it is
    also clear that this model has room to improve (e.g. data augmentation — which
    we will learn later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now you are ready to build your own image classifier (for regular photos — maybe
    not CT scan)! For example, [here](https://towardsdatascience.com/fun-with-small-image-data-sets-8c83d95d0159)
    is what one of the students did.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out [this forum post](http://forums.fast.ai/t/understanding-softmax-probabilities-output-on-a-multi-class-classification-problem/8194)
    for different way of visualizing the results (e.g. when there are more than 2
    categories, etc)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top-down vs Bottom-up [[30:52](https://youtu.be/IPBSB1HLNLo?t=30m52s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bottom-up: learn each building block you need, and eventually put them together'
  prefs: []
  type: TYPE_NORMAL
- en: Hard to maintain motivation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard to know the “big picture”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hard to know which pieces you’ll actually need
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'fast.ai: Get students using a neural net right away, getting results ASAP'
  prefs: []
  type: TYPE_NORMAL
- en: Gradually peel back the layers, modify, look under the hood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Course Structure [[33:53](https://youtu.be/IPBSB1HLNLo?t=33m53s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image classifier with deep learning (with fewest lines of code)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multi-label classification and different kinds of images (e.g. satellite images)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Structured data (e.g. sales forecasting) — structured data is what comes from
    database or spreadsheet
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Language: NLP classifier (e.g. movie review classification)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collaborative filtering (e.g. recommendation engine)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generative language model: How to write your own Nietzsche philosophy from
    scratch character by character'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back to computer vision — not just recognize a cat photo, but find where the
    cat is in the photo (heat map) and also learn how to write our own architecture
    from scratch (ResNet)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Image Classifier Examples:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image classification algorithm is useful for lots and lots of things.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, AlphaGo [[42:20](https://youtu.be/IPBSB1HLNLo?t=42m20s)] looked
    at thousands and thousands of go boards and each one had a label saying whether
    the go board ended up being the winning or the losing player’s. So it learnt an
    image classification that was able to look at a go board and figure out whether
    it was a good or bad — which is the most important step in playing go well: to
    know which move is better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another example is an earlier student created [an image classifier of mouse
    movement images](https://www.splunk.com/blog/2017/04/18/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html)
    and detected fraudulent transactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Learning ≠Machine Learning [[44:26](https://youtu.be/IPBSB1HLNLo?t=44m26s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is a kind of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning was invented by Arthur Samuel. In the late 50s, he got an IBM
    mainframe to play checkers better than he could by inventing machine learning.
    He made the mainframe to play against itself lots of times and figure out which
    kind of things led to victories, and used that to, in a way, write its own program.
    In 1962, Arthur Samuel said one day, the vast majority of computer software would
    be written using this machine learning approach rather than written by hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C-Path (Computational Pathologist)[[45:42](https://youtu.be/IPBSB1HLNLo?t=45m42s)]
    is an example of traditional machine learning approach. He took pathology slides
    of breast cancer biopsies, consulted many pathologists on ideas about what kinds
    of patterns or features might be associated with long-term survival. Then they
    wrote specialist algorithms to calculate these features, run through logistic
    regression, and predicted the survival rate. It outperformed pathologists, but
    it took domain experts and computer experts many years of work to build.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A better way [[47:35](https://youtu.be/IPBSB1HLNLo?t=47m35s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A class of algorithm that have these three properties is Deep Learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Infinitely flexible function: Neural Network [[48:43](https://youtu.be/IPBSB1HLNLo?t=48m43s)]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Underlying function that deep learning uses is called the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: All you need to know for now is that it consists of a number of simple linear
    layers interspersed with a number of simple non-linear layers. When you intersperse
    these layers, you get something called the universal approximation theorem. What
    universal approximation theorem says is that this kind of function can solve any
    given problem to arbitrarily close accuracy as long as you add enough parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All purpose parameter fitting: Gradient Descent [[49:39](https://youtu.be/IPBSB1HLNLo?t=49m39s)]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fast and scalable: GPU [[51:05](https://youtu.be/IPBSB1HLNLo?t=51m5s)]'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The neural network example shown above has one hidden layer. Something what
    we learned in the past few years is that these kind of neural network was not
    fast or scalable unless we added multiple hidden layers — hence called “Deep”
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Putting all together [[53:40](https://youtu.be/IPBSB1HLNLo?t=53m40s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are some of the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://research.googleblog.com/2015/11/computer-respond-to-this-email.html](https://research.googleblog.com/2015/11/computer-respond-to-this-email.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.skype.com/en/features/skype-translator/](https://www.skype.com/en/features/skype-translator/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1603.01768](https://arxiv.org/abs/1603.01768)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnosing lung cancer [[56:55](https://youtu.be/IPBSB1HLNLo?t=56m55s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Other current applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network [[59:13](https://youtu.be/IPBSB1HLNLo?t=59m13s)]
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[http://setosa.io/ev/image-kernels/](http://setosa.io/ev/image-kernels/)'
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear Layer [[01:02:12](https://youtu.be/IPBSB1HLNLo?t=1h2m12s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](http://neuralnetworksanddeeplearning.com/chap4.html?source=post_page-----602f73869197--------------------------------)
    [## Neural networks and deep learning'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter I give a simple and mostly visual explanation of the universality
    theorem. We'll go step by step…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/chap4.html?source=post_page-----602f73869197--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid and ReLU
  prefs: []
  type: TYPE_NORMAL
- en: A combination of linear layer followed by an element-wise nonlinear function
    allows us to create arbitrarily complex shapes — this is the essence of the universal
    approximation theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set these parameters to solve problems [[01:04:25](https://youtu.be/IPBSB1HLNLo?t=1h4m25s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent — we take small steps down the hill. The step size
    is called **learning rate**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If learning rate is too large, it will diverge instead of converge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If learning rate is too small, it will take forever
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing and Understanding Convolutional Networks [[01:08:27](https://youtu.be/IPBSB1HLNLo?t=1h8m27s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started with something incredibly simple but if we use it as a big enough
    scale, thanks to the universal approximation theorem and the use of multiple hidden
    layers in deep learning, we actually get the very very rich capabilities. This
    is actually what we used when we used when we trained our dog vs cat recognizer.
  prefs: []
  type: TYPE_NORMAL
- en: Dog vs. Cat Revisited — Choosing a learning rate [[01:11:41](https://youtu.be/IPBSB1HLNLo?t=1h11m41s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The first number `0.01` is the learning rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *learning rate* determines how quickly or how slowly you want to update
    the *weights* (or *parameters*). Learning rate is one of the most difficult parameters
    to set, because it significantly affect model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method `learn.lr_find()` helps you find an optimal learning rate. It uses
    the technique developed in the 2015 paper [Cyclical Learning Rates for Training
    Neural Networks](http://arxiv.org/abs/1506.01186), where we simply keep increasing
    the learning rate from a very small value, until the loss stops decreasing. We
    can plot the learning rate across batches to see what this looks like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `learn` object contains an attribute `sched` that contains our learning
    rate scheduler, and has some convenient plotting functionality including this
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Jeremy is currently experimenting with increasing the learning rate exponentially
    vs. linearly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see the plot of loss versus learning rate to see where our loss stops
    decreasing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We then pick the learning rate where the loss is still clearly improving — in
    this case `1e-2` (0.01)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing number of epochs [[1:18:49](https://youtu.be/IPBSB1HLNLo?t=1h18m49s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As many as you would like, but accuracy might start getting worse if you run
    it for too long. It is something called “overfitting” and we will learn more about
    it later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another consideration is the time available to you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tips and Tricks [[1:21:40](https://youtu.be/IPBSB1HLNLo?t=1h21m40s)]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**1.**`Tab` — it will do an auto complete when you cannot remember the function
    name'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** `Shift + Tab` — it will show you the arguments of a function'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.** `Shift + Tab + Tab` — it will bring up a documentation (i.e. docstring)'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.** `Shift + Tab + Tab + Tab` — it will open a separate window with the
    same information.'
  prefs: []
  type: TYPE_NORMAL
- en: Typing `?` followed by a function name in a cell and running it will do the
    same as `shift + tab (3 times)`
  prefs: []
  type: TYPE_NORMAL
- en: '**5.** Typing two question mark will display the source code'
  prefs: []
  type: TYPE_NORMAL
- en: '**6\.** Typing `H` in Jupyter Notebook will open up a window with keyboard
    shortcuts. Try learning 4 or 5 shortcuts a day'
  prefs: []
  type: TYPE_NORMAL
- en: '**7\.** Stop Paperspace, Crestle, AWS — otherwise you’ll be charged $$'
  prefs: []
  type: TYPE_NORMAL
- en: '**8\.** Please remember about the [forum](http://forums.fast.ai/) and [http://course.fast.ai/](http://course.fast.ai/)
    (for each lesson) for up-to-date information.'
  prefs: []
  type: TYPE_NORMAL
