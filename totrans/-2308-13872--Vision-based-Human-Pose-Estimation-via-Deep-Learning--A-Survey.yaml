- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:37:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2308.13872] Vision-based Human Pose Estimation via Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.13872](https://ar5iv.labs.arxiv.org/html/2308.13872)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Vision-based Human Pose Estimation via Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gongjin Lan^∗ [<svg id="id2.2.1.pic1" class="ltx_picture" height="13.95" overflow="visible"
    version="1.1" width="17.01"><g transform="translate(0,13.95) matrix(1 0 0 -1 0
    0) translate(8.5,0) translate(0,6.98)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.89 -2.36)" fill="#FFFFFF" stroke="#FFFFFF"
    color="#A6CE39"><foreignobject width="7.78" height="4.73" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">ID</foreignobject></g></g></svg>](https://orcid.org/0000-0003-2020-8186)
    , , Yu Wu, Fei Hu, , Qi Hao^∗ [<svg id="id4.4.2.pic1" class="ltx_picture" height="13.95"
    overflow="visible" version="1.1" width="17.01"><g transform="translate(0,13.95)
    matrix(1 0 0 -1 0 0) translate(8.5,0) translate(0,6.98)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -3.89 -2.36)" fill="#FFFFFF"
    stroke="#FFFFFF" color="#A6CE39"><foreignobject width="7.78" height="4.73" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">ID</foreignobject></g></g></svg>](https://orcid.org/0000-0002-2792-5965)
    This work is partially supported by the National Natural Science Foundation of
    China (No: 61773197), the Shenzhen Fundamental Research Program (No: JCYJ20200109141622964),
    the Intel ICRI-IACV Research Fund ($CG\#52514373$). (Corresponding authors: Gongjin
    Lan; Qi Hao.)Gongjin Lan, Yu Wu, Qi Hao are with the Department of Computer Science
    and Engineering, Southern University of Science and Technology, Shenzhen, 518055,
    China (e-mail: langj@sustech.edu.cn, wuy@mail.sustech.edu.cn, hao.q@sustech.edu.cn)Fei
    Hu is with the Department of Electrical and Computer Engineering, University of
    Alabama, Tuscaloosa, AL (email: fei@eng.ua.edu)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Human Pose Estimation (HPE) has attracted a significant amount of attention
    from the computer vision community in the past decades. Moreover, HPE has been
    applied to various domains such as human-computer interaction, sports analysis,
    and human tracking via images and videos. Recently, deep learning-based approaches
    have shown state-of-the-art performance in HPE-based applications. Although deep
    learning-based approaches have achieved remarkable performance in HPE, a comprehensive
    review of deep learning-based HPE methods remains lacking in the literature. In
    this paper, we provide an up-to-date and in-depth overview of the deep learning
    approaches in vision-based HPE. We summarize these methods of 2D and 3D HPE, and
    their applications, discuss the challenges and the research trends through bibliometrics
    and provide insightful recommendations for future research. This article provides
    a meaningful overview as introductory material for beginners to deep learning-based
    HPE, as well as supplementary material for advanced researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Human pose estimation, Human performance assessment, Deep learning, Action recognition,
    Bibliometric.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human Pose Estimation (HPE) refers to estimating the positions of human joints
    and their associations in images or videos, which is a popular research topic
    in computer vision. It has been widely applied to various applications, such as
    action analysis [[1](#bib.bib1)], HCI [[2](#bib.bib2)], gaming [[3](#bib.bib3)],
    sport analysis [[4](#bib.bib4), [5](#bib.bib5)], motion capture [[6](#bib.bib6)],
    computer-generated imagery [[7](#bib.bib7), [8](#bib.bib8)]. Although HPE has
    been studied for decades, it is still an open and challenging task since the two
    main aspects of the wide diversity of the human body (such as various human poses,
    various clothing, environment or illumination conditions) and reconstruction ambiguity
    caused by occlusions (particularly the crowd) [[9](#bib.bib9), [10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The early HPE approaches often use predefined models and statistical learning
    to describe the human poses [[11](#bib.bib11), [12](#bib.bib12)]. However, those
    methods are incapable of learning from a large amount of data and suffer from
    limited model representation capability. In recent years, deep learning-based
    approaches yield great improvements in many computer vision tasks such as classification
    [[13](#bib.bib13)], object detection [[14](#bib.bib14), [15](#bib.bib15)], and
    HPE [[9](#bib.bib9)]. The success of deep learning in HPE is mainly due to the
    following three facts: the availability of big data, superior representation capability
    of deep neural networks, and high-performance hardware (e.g., GPU platform). The
    deep learning-based methods dramatically outperform the traditional approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: Although there are many promising methods in the vision-based HPE via deep learning
    models [[9](#bib.bib9), [16](#bib.bib16), [17](#bib.bib17)], a lack of articles
    with an up-to-date and in-depth review of this domain. We emphasize that a comprehensive
    overview of HPE should cover both 2D and 3D HPE studies. In this paper, we aim
    to provide a complete and solid survey, analyze the research challenges, and point
    out the research trends in HPE. In particular, we apply bibliometrics to retrieve
    scientific publications for analyzing the research trends in HPE. This paper comprehensively
    reviews HPE topics that cover both 2D and 3D HPE studies, discusses the challenges,
    observes the trends, and provides detailed bibliometrics.
  prefs: []
  type: TYPE_NORMAL
- en: I-A Related Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To date, several survey papers have discussed the related studies in HPE. Dang
    et al. [[18](#bib.bib18)] provided a survey on deep learning-based 2D HPE, including
    single- and multi-person pipelines. Poppe et al. [[19](#bib.bib19)] presented
    an overview of the literature on vision-based human action recognition. Currently,
    there are many studies that investigate monocular HPE, and several survey papers
    that discuss the studies in monocular HPE. In [[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)], the studies of monocular HPE are reviewed comprehensively,
    particularly the deep learning-based methods. Dang et al. [[23](#bib.bib23)] provided
    a comprehensive survey on sensor- or vision-based human activity recognition.
    Gadhiya et al. [[24](#bib.bib24)] analyzed and compared several prevalent HPE
    methods. The latest work [[25](#bib.bib25)] reviewed the studies of deep learning-based
    3D HPE. Although these survey papers covered HPE-related topics, they focus on
    one of the specific topics in HPE. In this paper, we aim to provide a comprehensive
    survey on the vision-based HPE based on deep learning models in terms of 2D and
    3D HPE.
  prefs: []
  type: TYPE_NORMAL
- en: I-B Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this paper, we aim to provide a detailed overview of the existing studies
    on deep learning-based human pose estimation. This review has three objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delineate the picture of this field from a ‘helicopter view’.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clarify main research streams and provide a complete overview of vision- and
    deep learning-based HPE.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss the challenges and the research trends through bibliometrics, and provide
    insightful recommendations for future research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This survey covers both 2D- and 3D-based approaches. [Figure 1](#S1.F1 "Figure
    1 ‣ I-B Contributions ‣ I Introduction ‣ Vision-based Human Pose Estimation via
    Deep Learning: A Survey") shows the taxonomy of the approaches (e.g., image-based
    or video-based, 2D HPE or 3D HPE, monocular or multi-view), applications, trends,
    and challenges in this survey. These contributions provide our survey with a more
    solid, up-to-date, and in-depth insight than the existing survey papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b32e8dd998adc469dd6c59912e995991.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Taxonomy of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this paper is organized as follows. In [section II](#S2 "II Preliminary
    Knowledge ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"),
    we present the preliminary knowledge, common datasets, and the metrics for HPE.
    Image-based 2D HPE is summarized in [section III](#S3 "III 2D Human Pose Estimation
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"). We address
    video-based 2D HPE in [subsection III-C](#S3.SS3 "III-C Video-based 2D HPE ‣ III
    2D Human Pose Estimation ‣ Vision-based Human Pose Estimation via Deep Learning:
    A Survey"). The studies of 3D HPE is addressed in [section IV](#S4 "IV 3D Human
    Pose Estimation ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").
    The applications of vision-based HPE using deep learning are presented in [section V](#S5
    "V Applications ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").
    Finally, the research trends and challenges are discussed in [section VI](#S6
    "VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey"), followed by the conclusions in [section VII](#S7 "VII Conclusions
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: II Preliminary Knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the preliminary knowledge, including essential
    concepts to guide the readers on the big picture of HPE and describe the solution
    representation and the well-known datasets with the performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '| 2D/3D | Type | Year | Dataset | URL (Open dataset) | Data Scale | #Joints
    | Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| 2D | Single- Person | 2010 | LSP [[26](#bib.bib26)] | [http://sam.johnson.io/research/lsp.html](http://sam.johnson.io/research/lsp.html)
    | 2K images | 14 | PCK&PCP |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | FLIC [[27](#bib.bib27)] | [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html)
    | 5K images | 10 | PCK&PCP |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | J-HMDB [[28](#bib.bib28)] | [http://jhmdb.is.tue.mpg.de/](http://jhmdb.is.tue.mpg.de/)
    | 928 video clips | 15 | PCK&PCP |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | PennAction [[29](#bib.bib29)] | [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)
    | 2326 video clips | 13 | PCK&PCP |'
  prefs: []
  type: TYPE_TB
- en: '| Multi- Person | 2014 | MPII [[30](#bib.bib30)] | [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)
    | 25K images & 40K persons | 16 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | COCO [[31](#bib.bib31)] | [https://cocodataset.org/#home](https://cocodataset.org/#home)
    | 330K images & 250K persons | 17 | AP & AR |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | PoseTrack [[16](#bib.bib16)] | [https://github.com/umariqb/PoseTrack-CVPR2017](https://github.com/umariqb/PoseTrack-CVPR2017)
    | 46K frames & 276K persons | 15 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CrowdPose [[32](#bib.bib32)] | [https://github.com/Jeff-sjtu/CrowdPose](https://github.com/Jeff-sjtu/CrowdPose)
    | 20K images & 80K persons | 14 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| 3D | Single- Person | 2014 | Human3.6M [[33](#bib.bib33)] | [http://vision.imar.ro/human3.6m/](http://vision.imar.ro/human3.6m/)
    | 3.6M frames & 4 camera views | 17 | MPJPE |'
  prefs: []
  type: TYPE_TB
- en: '| Multi- Person | 2017 | CMU Panoptic [[34](#bib.bib34)] | [http://domedb.perception.cs.cmu.edu/](http://domedb.perception.cs.cmu.edu/)
    | 1.5M frames & 512 camera views | 15 | MPJPE |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | 3DPW [[35](#bib.bib35)] | [https://virtualhumans.mpi-inf.mpg.de/3DPW/](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
    | 51K frames & 1 camera view | 18 | MPJPE |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2017 | MPI-INF-3DHP [[36](#bib.bib36)] | [https://vcai.mpi-inf.mpg.de/3dhp-dataset/](https://vcai.mpi-inf.mpg.de/3dhp-dataset/)
    | 1.3M frames & 16 camera views | 15 | MPJPE |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2018 | JTA [[37](#bib.bib37)] | [https://github.com/fabbrimatteo/JTA-Dataset](https://github.com/fabbrimatteo/JTA-Dataset)
    | 460K frames & 1 camera view | 14 | MPJPE |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: Illustration of the well-known datasets in 2D and 3D HPE.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Deep Learning in HPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Currently, deep learning-based approaches have become state-of-the-art methods
    in HPE. The availability of big datasets, advanced hardware like GPU, and the
    surpassing performance of deep neural networks lead to the increasing interest
    in deep learning-based HPE. In this subsection, we discuss three topical types
    of neural networks in HPE: Convolutional Neural Networks (CNNs), Recurrent Neural
    Networks (RNNs), and Graph Convolutional Networks (GCNs).'
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 Convolutional Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, a CNN for HPE tasks consists of two parts. The first part commonly
    uses off-the-shelf generic pre-trained networks such as ResNet [[38](#bib.bib38)]
    to extract features, the so-called backbone network. The second part, called the
    prediction head, predicts human poses with the extracted features.
  prefs: []
  type: TYPE_NORMAL
- en: The well-known networks like AlexNet [[13](#bib.bib13)] and ResNet [[38](#bib.bib38)]
    show remarkable classification performance on the dataset of ImageNet, and advanced
    performance in HPE [[39](#bib.bib39), [40](#bib.bib40)] as well. However, there
    is a gap between classification and HPE tasks since their targeted features and
    prediction differ from each other. Instead of directly using the backbones from
    classification tasks, HPE-specified backbones need to be improved for HPE tasks.
    For example, Hourglass [[41](#bib.bib41)], Cascaded Pyramid Network (CPN) [[42](#bib.bib42)],
    and HRNet [[10](#bib.bib10)] are proposed to be the backbone for the deep learning-based
    approaches in HPE.
  prefs: []
  type: TYPE_NORMAL
- en: For prediction heads, there are mainly two representative solutions in HPE.
    The one directly predicts joint coordinates, which is regarded as the regression
    paradigm. The other one generates an intermediate heatmap representation before
    computing joint coordinates. For the regression paradigm, fully connected layers
    are often adopted to regress concrete keypoint coordinates. For the heatmap prediction
    paradigm, the operation of upsampling [[42](#bib.bib42), [41](#bib.bib41), [43](#bib.bib43)]
    is generally used to generate higher resolution heatmaps.
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Recurrent Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recurrent neural networks stake temporal information among sequential inputs
    into consideration. They are widely used in video-based HPE by considering videos
    as sequential RGB images. We present the general pipeline of this type of networks
    in [Figure 3](#S3.F3 "Figure 3 ‣ III-C Video-based 2D HPE ‣ III 2D Human Pose
    Estimation ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").
    RNN-based methods perform robust and the state-of-the-art accuracy [[44](#bib.bib44),
    [45](#bib.bib45)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-A3 Graph Convolutional Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of taking images as input, graph convolutional networks (GCNs) take
    graphs as input. As a human skeleton can be naturally represented as a graph,
    GCNs-based methods are prevalent in many skeleton-based tasks. In HPE, GCNs are
    generally expected to better exploit the relationship among keypoints and used
    for the pose refinement [[46](#bib.bib46)], joint association [[47](#bib.bib47)],
    2D-to-3D pose lifting [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B Pose Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the natural representation of human poses (keypoint positions) uses
    coordinates in the form of ordered tuples, the existing studies have shown a significant
    improvement by representing solutions with heatmaps that can be regarded as a
    confidence map. Currently, the heatmap representation has become a prevalent solution
    representation in HPE. In this subsection, we describe how heatmap representation
    works in the single-person and multi-person HPE.
  prefs: []
  type: TYPE_NORMAL
- en: II-B1 Heatmaps in Single-person HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To enable neural networks for the heatmap prediction of human joints, the ground
    truth of a heatmap is essential. An intuitive way of producing the ground truth
    is to design a probability heatmap for each keypoint. For example, for single-person
    pose estimation tasks, the heatmaps of $\mathcal{K}$ keypoints can be defined
    as $\mathcal{K}$ matrices with the identical size to the input image $x$. The
    value of position $p$ (noted as $\mathcal{H}_{k}(p)$) can be generated by a 2D
    Gaussian distribution centered at the position of joint $k$ in $x$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\mathcal{H}_{k}(p)=e^{\frac{\&#124;p-p_{k}^{*}\&#124;_{2}^{2}}{\sigma^{2}}},\forall
    k=1,2,\ldots,\mathcal{K}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $p_{k}^{*}$ is the position of joint $k$ in $x$. In a heatmap, the predicted
    position (noted as $p_{pred}^{*}$) can be calculated by regression models as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small p_{pred}^{*}=\sum_{p\in\mathcal{P}}\mathcal{H}_{k}(p)*p,$ |  |
    (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{P}$ is the set of the possible positions of joint $k$.
  prefs: []
  type: TYPE_NORMAL
- en: II-B2 Heatmaps in Multi-person HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two primary patterns of heatmaps in multi-person scenarios. One is
    to generate $\mathcal{N}\times\mathcal{K}$ heatmaps for $\mathcal{N}$ persons
    and their $\mathcal{K}$ joints separately. The other is to generate $\mathcal{K}$
    heatmaps for $\mathcal{K}$ joint of all persons. For the former pattern ($\mathcal{N}\times\mathcal{K}$
    heatmaps), each value at the position $p$ in a heatmap (noted as $\mathcal{H}_{k,n}(p)$)
    can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\mathcal{H}_{k,n}(p)=e^{\frac{\&#124;p-p_{k,n}^{*}\&#124;_{2}^{2}}{\sigma^{2}}},~{}\forall
    k=1,2,\ldots,\mathcal{K},~{}\forall n=1,2,\ldots,\mathcal{N}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{K}$ and $\mathcal{N}$ are the numbers of joints and persons,
    respectively. For the latter pattern, a general method to produce the ground truth
    heatmaps is aggregating $\mathcal{N}$ single-person heatmaps into one heatmap
    by using a $\max$ operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\mathcal{H}_{k}(p)=\max_{n}\mathcal{H}_{k,n}(p),\forall n=1,2,\ldots,\mathcal{N}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'In summary, [Equation 3](#S2.E3 "3 ‣ II-B2 Heatmaps in Multi-person HPE ‣ II-B
    Pose Representation ‣ II Preliminary Knowledge ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey") and [Equation 4](#S2.E4 "4 ‣ II-B2 Heatmaps in Multi-person
    HPE ‣ II-B Pose Representation ‣ II Preliminary Knowledge ‣ Vision-based Human
    Pose Estimation via Deep Learning: A Survey") are the two main methods to calculate
    the heatmaps in multi-person HPE.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Datasets and Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets are critical for training and evaluating neural networks for deep learning-based
    HPE. In this subsection, we introduce the popular datasets and their applicable
    tasks, then review the evaluation metrics for image-based 2D HPE, video-based
    2D HPE, and 3D HPE.
  prefs: []
  type: TYPE_NORMAL
- en: II-C1 Datasets in 2D HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many datasets have been proposed to evaluate the performance of 2D HPE approaches.
    Here, we introduce the prevalent datasets used in 2D HPE. Early datasets like
    LSP [[26](#bib.bib26)], FLIC [[27](#bib.bib27)], Penn Action [[29](#bib.bib29)],
    and J-HMDB [[28](#bib.bib28)] mainly focus on single-person scenes with relatively
    small scales. The recent datasets such as COCO [[31](#bib.bib31)], MPII [[30](#bib.bib30)],
    CrowdPose [[32](#bib.bib32)], and PoseTrack [[16](#bib.bib16)] are used for multi-person
    HPE with larger-scale data. We summarize and provide the links of the prevalent
    datasets as well as their scales and the evaluation metrics in [Table I](#S2.T1
    "TABLE I ‣ II Preliminary Knowledge ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: II-C2 Datasets in 3D HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unlike the datasets in 2D HPE, acquiring accurate 3D annotations for human
    joints in 3D HPE often requires a motion capture system that is generally hard
    to be installed in the outdoor environment. Most 3D HPE datasets are created in
    the indoor environments or simulation, such as CMU Panoptic [[34](#bib.bib34)],
    3DPW [[35](#bib.bib35)], MPI-INF-3DHP [[36](#bib.bib36)], JTA [[37](#bib.bib37)].
    Here we introduce the well-known datasets in 3D HPE and summarize their characteristics
    in [Table I](#S2.T1 "TABLE I ‣ II Preliminary Knowledge ‣ Vision-based Human Pose
    Estimation via Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: II-C3 Metrics in 2D HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In scientific research, we usually need metrics to evaluate how well a method
    performs. Here we introduce two common metrics used in HPE. *Percentage of Correct
    Keypoints (PCK)* literally indicates the percentage of correct detected keypoints,
    which can be noted as $\mathcal{PCK}=k/\mathcal{N}$, where k is the number of
    correct predicted keypoints, $\mathcal{N}$ is the total number of keypoints. This
    metric is generally applied to the studies with the LSP dataset, MPII dataset,
    and FLIC dataset in the early 2D HPE methods (see [Table II](#S2.T2 "TABLE II
    ‣ II-C3 Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Studies | Years | Backbone | Input Size | Highlights | PCKh@0.5 | #Params
    | GFlops |'
  prefs: []
  type: TYPE_TB
- en: '| Toshev & Szegedy [[9](#bib.bib9)] | 2014 | AlexNet | $256\times 256$ | Original
    deep learning-based method for HPE | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Tompson et al. [[51](#bib.bib51)] | 2014 | AlexNet | $320\times 240$ | Utilization
    of heatmaps for solution representation | 82.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wei et al. [[52](#bib.bib52)] | 2016 | CPM | $368\times 368$ | A convolutional
    pose machine | 88.5 | 31.23M | 85.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Newell et al. [[41](#bib.bib41)] | 2016 | Hourglass | $256\times 256$ | Stacked
    Hourglass Modules | 90.9 | 23.7M | 41.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Xiao et al. [[43](#bib.bib43)] | 2018 | ResNet | $256\times 256$ | A simple
    yet effective architecture for HPE | 90.2 | 68.64 M | 17.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Chu et al. [[53](#bib.bib53)] | 2017 | Hourglass | $256\times 256$ | Attention
    mechanism in contextual representations | 91.5 | 58.1M | - |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[54](#bib.bib54)] | 2017 | Hourglass | $256\times 256$ | Pyramid
    Residual Module | 92.0 | 26.9M | 45.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Sun et al. [[10](#bib.bib10)] | 2019 | HRNet | $256\times 256$ | HRnet for
    high-resolution representations | 92.3 | 28.54M | 10.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Bulat et al. [[55](#bib.bib55)] | 2020 | Hourglass+UNet | $256\times 256$
    | A hybrid structure by combining [[41](#bib.bib41)] and U-Net [[56](#bib.bib56)]
    | 94.1 | 8.5M | 9.9 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: The state-of-the-art approaches for 2D single-person pose estimation.
    The PCKh@0.5 scores are shown in the last column and defined in [subsubsection II-C3](#S2.SS3.SSS3
    "II-C3 Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"). They are obtained
    by testing the methods on the MPII dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Object Keypoint Similarity (OKS)* was proposed originally in the COCO competition
    [[31](#bib.bib31)] as a variable to compute mean Average Precision (mAP). It can
    be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\mathcal{OKS}=\frac{\sum_{i}(-d_{i}^{2}/2s^{2}k_{i}^{2})\delta_{(v_{i}>0)}}{\sum_{i}\delta_{(v_{i}>0)}},\quad
    s.t.~{}v_{i}\in\{0,1,2\}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $i$ is a joint index, $d_{i}$ is the distance between the predicted joint
    and ground truth, $s$ (object scale) and $k_{i}$ are the keypoint constants given
    by the COCO dataset, $\delta=1$ when $v_{i}>0$, otherwise $\delta=1$, and $v_{i}$
    is the visibility flag ($v_{i}=0$: not labelled, $v_{i}=1$: labelled but not visible,
    and $v_{i}=2$: labelled and visible) of the ground truth. The average precision
    (AP) can be calculated with the $\mathcal{OKS}$ value by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small AP=\frac{\mathcal{TP}_{(\mathcal{OKS}>td)}}{\mathcal{TP}_{(\mathcal{OKS}>td)}+\mathcal{FP}_{(\mathcal{OKS}\leq
    td)}}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{TP}$ and $\mathcal{FP}$ are the numbers of true positive and
    false positive respectively, $td$ is a $\mathcal{OKS}$ threshold. The mean AP
    (mAP) is the mean of AP over ten $\mathcal{OKS}$ thresholds at (0.50, 0.55, .
    . . , 0.90, 0.95) [[31](#bib.bib31)], which is a common metric to evaluate 2D
    multi-person pose estimation (see [Table III](#S3.T3 "TABLE III ‣ III-B Image-based
    2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: II-C4 Metrics in 3D HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Generally, the metrics of PCK can be extended to evaluate 3D HPE. However,
    *Mean Per Joint Position Error (MPJPE)* is currently a popular metric in 3D HPE
    (see [Table IV](#S4.T4 "TABLE IV ‣ IV 3D Human Pose Estimation ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey")). MPJPE calculates the euclidean
    distance between the predicted joint coordinates and ground truth joint coordinates,
    which can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\text{MPJPE}=\frac{1}{\mathcal{T}\cdot\mathcal{N}}\sum\limits_{t=1}^{\mathcal{T}}\sum\limits_{i=1}^{\mathcal{N}}\left\lVert
    J_{i}^{t}-J_{root}^{t}-(\hat{J_{i}}^{t}-\hat{J}_{root}^{t})\right\rVert^{2}$ |  |
    (7) |'
  prefs: []
  type: TYPE_TB
- en: Where $i$ and $t$ are the indexes of a joint and a sample respectively; $J_{i}$
    and $\hat{J_{i}}$ refer to the predicted coordinate and ground-truth coordinate
    of $i-th$ joint. $J_{root}$ refers to the coordinate of the root joint, which
    is usually predefined as the human pelvis.
  prefs: []
  type: TYPE_NORMAL
- en: III 2D Human Pose Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The image-based 2D HPE is to estimate human joint positions in images. Early
    approaches mainly use model-based methods. Currently, deep learning-based methods
    have shown superior performance in 2D HPE [[9](#bib.bib9)]. In this section, we
    introduce the deep learning-based 2D HPE approaches from three aspects: image-based
    single-person pose estimation (SPPE), image-based multi-person pose estimation
    (MPPE), and video-based 2D HPE. Finally, we summarize the open-source codes of
    the state-of-the-art 2D HPE in [Table VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets
    ‣ VI-B1 Challenges in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research Trends
    and Challenges ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Image-based 2D Single-Person HPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SPPE task is to estimate the pose of a single person in an image. It is
    a fundamental task in HPE and is often used as a basic component of other HPE
    tasks. For example, the well-known study [[9](#bib.bib9)] proposed a cascaded
    multi-stage neural network to predict and refine human poses, in which images
    are cropped to ensure a single person in each image. To our knowledge, it is the
    original work that applies deep neural networks (DNN) for the HPE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent DNN-based approaches dramatically improve the accuracy of SPPE. Many
    generic neural networks with efficient architecture show remarkable performance
    in various applications [[13](#bib.bib13), [38](#bib.bib38)]. There are mainly
    two ways to improve SPPE performance: improving the solution representation and
    designing advanced neural networks. In [[51](#bib.bib51), [57](#bib.bib57), [58](#bib.bib58)],
    the methods with solution representation significantly improved the SPPE performance.
    In addition, well-designed neural architectures [[10](#bib.bib10), [52](#bib.bib52)]
    also performed remarkable performance in HPE. We summarize the state-of-the-art
    approaches for 2D single-person pose estimation in [Table II](#S2.T2 "TABLE II
    ‣ II-C3 Metrics in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Solution Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In general, there are mainly two types of solution representations. One is
    to represent joint positions by using coordinates. The other one is to represent
    joint positions in the form of probability distributions, i.e., heatmaps as represented
    in [Equation 1](#S2.E1 "1 ‣ II-B1 Heatmaps in Single-person HPE ‣ II-B Pose Representation
    ‣ II Preliminary Knowledge ‣ Vision-based Human Pose Estimation via Deep Learning:
    A Survey"). Early studies mainly used a DNN-based regression from the input images
    to the estimated coordinates directly. DeepPose [[9](#bib.bib9)] is an early classic
    study that applied DNNs to represent coordinate regression from RGB images and
    multi-stage refinement for the SPPE tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Solution representation of heatmaps significantly improves the performance of
    DNN-based methods in HPE [[56](#bib.bib56)]. The methods of heatmap generation
    have been widely used in HPE tasks since heatmaps were proposed by Tompson et
    al. [[51](#bib.bib51)] and Jain et al. [[57](#bib.bib57)]. Moreover, Zhang et
    al. [[58](#bib.bib58)] improved the heatmap representation by the distribution-aware
    coordinate representation of keypoints (DARK).
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Neural Network Design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: How to design the architecture of neural networks in HPE is a crucial topic.
    A generic way is to use the existing well-known neural networks (e.g., AlexNet
    [[13](#bib.bib13)], ResNet [[38](#bib.bib38)]) that have already shown superior
    performance in other computer vision tasks. For example, Toshev et al. [[9](#bib.bib9)]
    used an AlexNet-like to regress the coordinates of human joints from images. Xiao
    et al. [[43](#bib.bib43)] used ResNet as the backbone of neural networks for HPE.
  prefs: []
  type: TYPE_NORMAL
- en: The latest neural network, the transformer that adopts the attention mechanism,
    has been applied in HPE with superior performance. In general, transformers process
    the features extracted by CNNs and leverage the attention mechanism to automatically
    capture long-range relationships among features. In 2021, Mao et al.[[59](#bib.bib59)]
    used ResNet to extract features from images and a Transformer model to predict
    keypoint positions. Li et al.[[60](#bib.bib60)] utilized vision transformers to
    implement regression-based HPE, where two cascaded transformers are applied to
    predict the bounding boxes of the person as well as their keypoints positions.
    Yang et al. [[61](#bib.bib61)] proposed a new network model, TransPose which introduces
    the transformer for human pose estimation. The attention layers built-in transformers
    enable TransPose to capture long-range relationships efficiently. In addition,
    Xu et al. proposed a scalable HPE backbone, VitPose [[62](#bib.bib62)] that employs
    plain and non-hierarchical vision transformers as backbones to extract features
    for a given person instance and a lightweight decoder for pose estimation. To
    date, the existing survey papers have not discussed the studies with transformers
    in HPE.
  prefs: []
  type: TYPE_NORMAL
- en: Another generic way is to design specific networks for HPE tasks. For example,
    Newell et al. [[41](#bib.bib41)] designed a stacked hourglass network as the backbone
    for exacting multi-scale features automatically. Hourglass Networks are used in
    many HPE studies as backbone networks. Newell et al. [[63](#bib.bib63)] used the
    Hourglass Network backbone to estimate multi-person poses. BlazePose [[64](#bib.bib64)]
    utilized an Hourglass-based encoder network to accomplish real-time single-person
    pose estimation driven by the MediaPipe framework [[65](#bib.bib65)], which achieved
    a speed of 30 frames per second on the Google Pixel 2 mobile phone. Bulat et al.
    [[55](#bib.bib55)] designed a hybrid network structure with a combination of the
    hourglass network and U-Net [[56](#bib.bib56)]. Chen et al. [[42](#bib.bib42)]
    presented another remarkable study that a cascaded pyramid network is proposed
    to concatenate features among different scales. In addition, many studies enhanced
    the model performance by slightly modifying the units of a backbone. Specifically,
    Chu et al. [[53](#bib.bib53)] incorporated the attention mechanism into the hourglass
    network. Yang et al. [[54](#bib.bib54)] proposed a pyramid residual module to
    replace the residual module in the hourglass network.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Image-based 2D Multi-Person HPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, MPPE is a more challenging task than SPPE because of the high complexity
    in terms of solution space and mutual occlusions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2394166eae3e349a0eea7b9e4797ae3.png)  ![Refer to caption](img/96b54aac34ebb8fffa34bedb1c856963.png)  ![Refer
    to caption](img/eaf6b5dc55850fdea393a22a147167b9.png)  Input Image Detected Bounding
    Boxes Human Poses  ![Refer to caption](img/f2394166eae3e349a0eea7b9e4797ae3.png)  ![Refer
    to caption](img/e242ccbf4d6ca37e6920fe58d0b3f22e.png)  ![Refer to caption](img/f34b11becff0039b6b3cf8139e84e563.png)  Input
    Image Detected Keypoints Human Poses'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Illustration of the top-down and bottom-up framework. The first row
    shows a representative top-down framework with two stages: proposing a bounding
    box for each person and estimating human poses. The second row shows a representative
    bottom-up framework with two stages: detecting the joints of persons and grouping
    the joints into an associated person. The original images are from COCO dataset
    [[31](#bib.bib31)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The current solutions of MPPE generally fall into either top-down approaches
    or bottom-up approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top-down approaches apply a person detector to detect all single-persons, followed
    by estimating the joints of each single-person and calculating each single-person
    pose separately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottom-up approaches detect all joints in an image, followed by associating/grouping
    the joints into an associated person.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this subsection, we address the top-down and bottom-up approaches and discuss
    their merits and flaws. We illustrate the frameworks of both approaches in [Figure 2](#S3.F2
    "Figure 2 ‣ III-B Image-based 2D Multi-Person HPE ‣ III 2D Human Pose Estimation
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey") and summarize
    the recent well-known MPPE approaches in [Table III](#S3.T3 "TABLE III ‣ III-B
    Image-based 2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Studies/Years | Input Size | Backbone | Detector/ Grouping | Highlights
    | mAP | #Params | GFlops |'
  prefs: []
  type: TYPE_TB
- en: '| Top-down | [[66](#bib.bib66)] / 2017 | - | ResNet | Mask R-CNN | Detecting
    persons and keypoints by mask R-CNN | 63.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[39](#bib.bib39)] / 2017 | $320\times 256$ | PyraNet | Faster R-CNN | Addressing
    inaccurate bounding box of persons | 72.3 | 28.1M | 26.7 |'
  prefs: []
  type: TYPE_TB
- en: '| [[42](#bib.bib42)] / 2018 | $384\times 288$ | CPN | FPN | A two-stage cascaded
    network to refine keypoints | 72.1 | 27.0M | 6.2 |'
  prefs: []
  type: TYPE_TB
- en: '| [[43](#bib.bib43)] / 2018 | $384\times 288$ | ResNet | Faster R-CNN | A remarkable
    simple baseline | 72.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[10](#bib.bib10)] / 2019 | $384\times 288$ | HRNet | Faster R-CNN | HRnet
    for high-resolution representations | 75.5 | 63.6M | 32.9 |'
  prefs: []
  type: TYPE_TB
- en: '| [[67](#bib.bib67)] / 2019 | $384\times 288$ | MSPN | MegDet | A multi-stage
    network | 76.1 | 120M | 19.9 |'
  prefs: []
  type: TYPE_TB
- en: '| [[17](#bib.bib17)] / 2020 | $512\times 384$ | EvoPose2D | Faster R-CNN |
    Neural Architecture Search | 76.8 | 14.7M | 17.7 |'
  prefs: []
  type: TYPE_TB
- en: '| [[46](#bib.bib46)] / 2020 | $384\times 384$ | ResNet | FPN | GCN-based keypoint
    refinement | 72.9 | 25.2M | 12.9 |'
  prefs: []
  type: TYPE_TB
- en: '| [[59](#bib.bib59)] / 2021 | $384\times 288$ | ResNet+ Transformer | Faster
    R-CNN | Direct Coordinate Regression via Transformer | 72.2 | - | 20.4G |'
  prefs: []
  type: TYPE_TB
- en: '| [[61](#bib.bib61)] / 2021 | $256\times 192$ | TransPose | Faster R-CNN |
    Transformer decoders with CNNs-based extractor | 75.8 | 17.5M | 21.8G |'
  prefs: []
  type: TYPE_TB
- en: '| [[62](#bib.bib62)] / 2022 | $256\times 192$ | VitPose | Faster R-CNN | Vision
    Transformer Backbone Baseline | 79.8 | 632M | - |'
  prefs: []
  type: TYPE_TB
- en: '| Bottom-up | [[68](#bib.bib68)] / 2016 | $256\times 256$ | ResNet | Integer
    Programming | ResNet-based detectors and image-conditioned pairwise terms | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[40](#bib.bib40)] / 2017 | $256\times 256$ | VGG+CPM | Part Affinity Fields
    | Grouping by body association and Hungarian algo. | 61.8 | 25.94M | 160 |'
  prefs: []
  type: TYPE_TB
- en: '| [[63](#bib.bib63)] / 2017 | $512\times 512$ | Hourglass | Associative embedding
    | pixel-wise joint embedding for grouping | 66.3 | 277.8M | 206.9 |'
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] / 2019 | $384\times 384$ | Hourglass | $\emptyset$ | Predicting
    root and joints position | 66.9 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[47](#bib.bib47)] / 2020 | $641\times 641$ | ResNet | DGCN | Graph convolutional
    network for grouping | 68.8 | 234M | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[70](#bib.bib70)] / 2020 | $512\times 512$ | Hourglass | Associative embedding
    | Graph Clustering for grouping | 68.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[71](#bib.bib71)] / 2018 | $480\times 480$ | ResNet+FPN | Person Detection
    | Pose Residual Network assigns keypoints to instances | 69.6 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[72](#bib.bib72)] / 2020 | $640\times 640$ | HigherHRNet | Associative embedding
    | HigherHRNet for the scale variation challenge | 70.5 | 63.8M | 154.3 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: The state-of-the-art studies for image-based 2D multi-person HPE.
    The mAP scores are defined in [subsubsection II-C3](#S2.SS3.SSS3 "II-C3 Metrics
    in 2D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey"), are obtained by testing on
    the COCO dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Top-down approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The top-down method is an effective, popular method in 2D multi-person pose
    estimation tasks. By using a pretrained human detector to crop images in top-down
    approaches, the multi-person tasks can be converted into single-person tasks.
    The performance of top-down methods can be enhanced with the improvements of both
    human detector and single-person pose estimator. He et al. [[66](#bib.bib66)]
    showed that multi-person pose estimation can be implemented by extending HPE tasks
    to the detection tasks. Fang et al. [[39](#bib.bib39)] proposed a regional multi-person
    pose estimation framework to improve the performance of the human detector. Furthermore,
    the improvement of single-person pose estimators shows benefits for multi-person
    pose estimation [[58](#bib.bib58), [73](#bib.bib73), [42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: In general, the top-down approaches show advanced performance on datasets and
    can be easily implemented by combining the existing detectors and SPPE models.
    However, the computing of top-down approaches is significantly increasing over
    the number of detected persons, which limits its real-time performance for multiple-person
    scenarios. Therefore, the top-down approaches are hardly applied to the real-time
    HPE tasks in complex scenarios, particularly the crowd scenes.
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Bottom-up approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Instead of performing keypoints detection in the proposed bounding boxes, the
    bottom-up methods often consist of two parts: keypoints detection and keypoints
    grouping. For keypoints detection, body keypoints of all persons in an image are
    detected directly by the heatmaps described in [Equation 3](#S2.E3 "3 ‣ II-B2
    Heatmaps in Multi-person HPE ‣ II-B Pose Representation ‣ II Preliminary Knowledge
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"). for keypoints
    grouping, the detected keypoints need to be grouped into the single-person. The
    deep neural networks are generally applied to assign keypoints to the proposed
    bounding boxes of the detected person. Newell et al. [[63](#bib.bib63)] introduced
    associative embedding to train neural networks for assigning keypoints to each
    person. Jin et al. [[70](#bib.bib70)] applied graph neural networks to group the
    detected joints.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it is shown in [[72](#bib.bib72)] that the bottom-up approaches are
    more robust than the top-down approaches when applied to crowded scenes, which
    is crucial for practical applications. However, the top-down methods achieve better
    performance in terms of accuracy, while the computing time inevitably increases
    as the number of detected persons increases. By contrast, the bottom-up approaches
    take relatively constant computing time for multi-person HPE [[40](#bib.bib40),
    [69](#bib.bib69)], which is much less sensitive to the number of targeted persons.
    In conclusion, the bottom-up approaches are conducive to real-time multi-person
    pose estimation on the low-performance hardware platform.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Video-based 2D HPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The video-based 2D HPE is generally a more complicated task than the image-based
    2D HPE. A pipeline of the generic video-based 2D HPE is shown in [Figure 3](#S3.F3
    "Figure 3 ‣ III-C Video-based 2D HPE ‣ III 2D Human Pose Estimation ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey"). Unlike static images, video
    frames are likely to involve the problem of image degeneration such as motion
    blur and video defocus. Although the video-based 2D HPE performance may degenerate
    because of motion blur, video defocus, and temporary occlusions, these video-based
    approaches generally surpass image-based approaches by capturing temporal information.
    The correlations among video frames can be used to further improve the self-supervised
    approaches in HPE.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/91be6775104e9bf3a6263d19764e7d49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of a video-based 2D HPE pipeline where a Neural Networks-based
    (NNs-based) module is utilized to extract temporal information. The original images
    are from the Penn Action dataset [[29](#bib.bib29)].'
  prefs: []
  type: TYPE_NORMAL
- en: In video-based 2D HPE, it is costly to manually annotate human joints in each
    frame of video, which restricts the obtaining of large-scale datasets for the
    video-based HPE. To solve this issue, the temporal correlation and consistency
    among the sequences need to be fully investigated. Jain et al. [[57](#bib.bib57)]
    originally proposed a CNN-based approach to combine RGB images and motion features
    for improving both the accuracy and speed of HPE. Specially, the recurrent neural
    networks, such as the well-known long short-term memory (LSTM), perform remarkably
    to capture temporal consistency among frames [[44](#bib.bib44), [45](#bib.bib45)].
    Nie et al. [[74](#bib.bib74)] proposed a dynamic kernel distillation model to
    distil previous temporal cues among frames. Although these methods perform state-of-the-art
    accuracy, they are supervised learning-based by using large-scale and densely
    labelled data in video-based HPE. By contrast, semi-supervised learning is useful
    for video-based 2D HPE with sparsely labelled data since labelling large-scale
    and densely data is costly and labour-intensive. To this end, Bertasius et al.
    [[75](#bib.bib75)] proposed PoseWarper propagate pose information in sparsely
    labelled (each k frames) videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we collect the open-source implementations of well-known state-of-the-art
    2D HPE works, as shown in [Table VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets
    ‣ VI-B1 Challenges in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research Trends
    and Challenges ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").
    We summarize the prevalent real-time open-source implementations for practical
    applications (e.g., OpenPose, AlphaPose, a lightweight version of OpenPose, and
    BlazePose [[64](#bib.bib64)] driven by the framework of MediaPipe [[65](#bib.bib65)]
    ), which generally offer open-source codes to users.'
  prefs: []
  type: TYPE_NORMAL
- en: III-D Robustness Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Robustness is a crucial property that should be considered in deep learning-based
    methods. We review the studies of robustness analysis for HPE in this subsection.
    Currently, robustness analysis is usually applied to 2D HPE but rarely considered
    in 3D HPE. Deep learning-based methods are generally sensitive and vulnerable
    to the attack of adversarial samples.
  prefs: []
  type: TYPE_NORMAL
- en: Although the robustness of HPE is rarely studied, it is crucial to be considered
    in the design and evaluation of HPE methods. In [[41](#bib.bib41)], it is demonstrated
    that the PCK of the Hourglass network dramatically decrease from $89.4$ to $0.57$
    in the testing with adversarial samples. Wang et al. [[76](#bib.bib76)] proposed
    the new datasets COCO-C, MPIIC, and OCHuman-C, which were reconstructed on the
    basis of COCO [[31](#bib.bib31)], MPII [[30](#bib.bib30)], and OCHuman [[77](#bib.bib77)]
    for evaluating the robustness of HPE methods. Shah et al. [[78](#bib.bib78)] comprehensively
    investigated the adversarial robustness of HPE methods. The experimental results
    show that 1) heatmap-based methods perform more robust than regression-based methods,
    and 2) different body joints generally perform different robustness to the attacks
    of adversarial samples. For example, the head and neck exhibited prominent robustness,
    while the joints of the hips, knees, and ankles are sensitive to disturbances.
    These works revealed the robustness of the existing deep learning-based HPE methods.
  prefs: []
  type: TYPE_NORMAL
- en: IV 3D Human Pose Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The 3D HPE is to predict the 3D positions of human joints. It is a challenging
    task due to the large solution space and inherent ambiguity. Moreover, the lack
    of outdoor large-scale 3D HPE datasets challenges the practical performance of
    3D HPE approaches since the existing datasets 3D HPE is mostly collected by indoor
    motion capture systems [[33](#bib.bib33), [34](#bib.bib34)]. Early studies either
    implemented 3D HPE with model-based approaches [[79](#bib.bib79), [80](#bib.bib80)]
    or regarded the task as a regression problem which can be solved by optimization
    algorithms. Since the DNN-based methods [[81](#bib.bib81), [82](#bib.bib82)] outperform
    the previous works by automatically learning representations from large-scale
    data, deep learning-based methods have become the most popular methods in 3D HPE
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we review deep learning-based 3D HPE studies as shown in [Table IV](#S4.T4
    "TABLE IV ‣ IV 3D Human Pose Estimation ‣ Vision-based Human Pose Estimation via
    Deep Learning: A Survey"). According to the characteristics of inputs, we categorize
    the 3D HPE into three types: monocular 3D HPE, multi-view 3D HPE, and multimodal
    3D HPE. Finally, we present a collection of the open-source code of the state-of-the-art
    3D HPE approaches in [Table VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets ‣
    VI-B1 Challenges in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research Trends
    and Challenges ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Views | Modality | Studies | Methods | Highlights | MPJPE | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Monocular | Vision | Ching-Hang et al. [[82](#bib.bib82)] | CNN | A matching
    method to lift poses | 82.72 | Human3.6m |'
  prefs: []
  type: TYPE_TB
- en: '| Julieta et al. [[83](#bib.bib83)] | CNN | Off-the-shelf detectors & lifting
    networks | 87.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Dushyant et al. [[84](#bib.bib84)] | CNN | Occlusion-robust pose-maps | 69.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[48](#bib.bib48)] | CNN+GCN | An novel SemGCN for 2D-3D lifting.
    | 60.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Shichao et al. [[85](#bib.bib85)] | CNN | Evolutionary 2D-3D data augmentation
    & Cascaded 2D-3D lifting networks | 50.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Kehong et al. [[86](#bib.bib86)] | CNN | Differentiable pose augmentor for
    2D-3D lifting | 50.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Yujun et al. $\cite[cite]{[\@@bibref{}{cai2019exploiting}{}{}]}^{\dagger}$
    | CNN+GCN | GCN-based 2D-3D sequence lifting. | 48.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Wenbo et al. $\cite[cite]{[\@@bibref{}{hu2021conditional}{}{}]}^{\dagger}$
    | CNN+GCN | Graph for skeleton representation & GCN for 2D-3D sequence lifting
    | 41.1 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Vision & IMUs | Timo et al. [[35](#bib.bib35)] | CNN | Video Inertial
    Poser to fusing images and IMUs | 26 | 3DPW |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-view | Vision | Karim et al. [[87](#bib.bib87)] | CNN | An end-to-end
    DNN triangulation method | 17.7 | Human3.6m |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[88](#bib.bib88)] | CNN | An adaptive multi-view fusion method
    | 19.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Yihui et al. [[89](#bib.bib89)] | CNN | An epipolar transformer | 26.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Haibo et al. [[90](#bib.bib90)] | CNN | A cross-view fusion network | 31.17
    |'
  prefs: []
  type: TYPE_TB
- en: '| Size et al. [[91](#bib.bib91)] | CNN+GCN | Learnable association matching
    & graph-based 3D pose refinement | 15.84 | CMU Panoptic |'
  prefs: []
  type: TYPE_TB
- en: '| Vision & IMUs | Trumble et al. [[92](#bib.bib92)] | CNN | A two-stream network
    fuses video and IMUs | 87.3 | Human3.6m |'
  prefs: []
  type: TYPE_TB
- en: '| Gilbert et al. [[93](#bib.bib93)] | CNN | Incorporating [[92](#bib.bib92)]
    with enhanced 3D HPE | 71.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[94](#bib.bib94)] | CNN | An orientation regularized pictorial
    model | 24.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Malleson et al. [[95](#bib.bib95)] | CNN | Optimization based on IMUs and
    poses | 62 | Total Capture |'
  prefs: []
  type: TYPE_TB
- en: '|  | Huang et al. [[96](#bib.bib96)] | CNN | DeepFuse for vision-IMU data fusion
    | 28.9 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: The state-of-the-art approaches for image-based 3D HPE. The MPJPE
    scores that are defined in [subsubsection II-C4](#S2.SS3.SSS4 "II-C4 Metrics in
    3D HPE ‣ II-C Datasets and Metrics ‣ II Preliminary Knowledge ‣ Vision-based Human
    Pose Estimation via Deep Learning: A Survey"), can be obtained by testing the
    methods on the Human3.6M dataset. The symbol ${\dagger}$ denotes the methods of
    using image sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Monocular 3D HPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The monocular 3D HPE task is to predict the 3D positions of human joints for
    a monocular image, which is known to be an ill-posed problem. DNNs have shown
    remarkable performance on predicting the depth of monocular images [[97](#bib.bib97)]
    and the monocular 3D HPE tasks [[83](#bib.bib83), [98](#bib.bib98)]. The monocular
    3D HPE methods can be generally categorized into two types: the single-stage and
    the two-stage methods. The basic difference between these two types is that two-stage
    methods entail using off-the-shelf 2D predictors with the 2D HPE datasets. The
    single-stage methods predict 3D poses from images directly. By contrast, two-stage
    methods estimate 2D poses and then lift the 2D poses to 3D poses. In this subsection,
    we summarize the state-of-the-art works of both methods for monocular 3D HPE.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A1 Single-stage Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As DNN-based methods can automatically build mappings from 2D images to 3D poses,
    the single-stage 3D HPE can be generally viewed as an extension of 2D HPE. Early
    works applied the regression paradigm to estimate 3D human pose directly. Li et
    al. [[81](#bib.bib81)] originally applied an end-to-end approach with the deep
    neural network for 3D HPE by the combination of a joint detector and a joint regressor.
    Luvizon et al. [[99](#bib.bib99)] proposed a multi-task framework to perform multi-task
    learning, which predicts 3D poses directly.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the early studies, there are many current single-stage approaches
    that apply heatmap representation to 3D HPE. For example, Pavlakos et al. [[100](#bib.bib100)]
    proposed a single-stage method that predicted 3D heatmaps in voxel space and proposed
    a coarse-to-fine prediction scheme to reduce the large 3D heatmap cost. To reduce
    the computational cost of direct predicting 3D voxel heatmaps, Nibali et al. [[101](#bib.bib101)]
    predicted the marginal 2D heatmaps to produce 3D coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the single-stage methods in multi-person 3D HPE, they can be similarly
    categorized into the top-down and bottom-up methods, recalling the [Figure 2](#S3.F2
    "Figure 2 ‣ III-B Image-based 2D Multi-Person HPE ‣ III 2D Human Pose Estimation
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"). The top-down
    methods directly predict 3D poses of the detected person in each proposed bounding
    box. Bottom-up methods estimate the 3D pose by detecting all 3D joints and grouping
    all joints into person-specific sets. Typically, Fabbri et.al [[102](#bib.bib102)]
    proposed a volumetric heatmap autoencoder to estimate 3D joint locations and employed
    a distance-based heuristic strategy to associate the body joints of a person.
    In summary, the framework of the top-down and bottom-up methods in monocular multi-person
    3D HPE is similar to the frameworks in [Figure 2](#S3.F2 "Figure 2 ‣ III-B Image-based
    2D Multi-Person HPE ‣ III 2D Human Pose Estimation ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey"), while the joint points are distributed in the 3D
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 Two-stage Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the single-stage methods are efficient, the two-stage methods generally
    yield better performance as they could further benefit from 2D information and
    large-scale 2D HPE datasets. Many studies utilize off-the-shelf 2D estimators
    to produce 2D poses and followed by a 2D-3D lifting method. For example, Martinez
    et al. [[83](#bib.bib83)] used an off-the-shelf 2D estimator to produce 2D poses
    and then employed a simple lifting neural network to predict 3D poses on the basis
    of 2D poses. Zheng et al. [[103](#bib.bib103)] utilized vision transformer to
    implement the 3D lifting from 2D pose sequence. For two-stage methods, weak-supervised
    approaches are often used to leverage the 2D-to-3D constraint [[73](#bib.bib73)].
    Pavllo et al. [[104](#bib.bib104)] fuse 2D poses sequence from videos to predict
    accurate 3D poses that exploit unlabeled videos in a semi-supervised manner.
  prefs: []
  type: TYPE_NORMAL
- en: In multi-person 3D HPE, the two-stage methods also employ the top-down and bottom-up
    paradigms yet incorporate the two-stage process. For the top-down methods, Rogez
    et al. [[105](#bib.bib105)] employed a region proposal network to propose bounding
    boxes for targeted persons and developed a pose proposal network to estimate the
    human poses in these bounding boxes. Zanfir et al. [[106](#bib.bib106)] performed
    bottom-up 2D MPPE and then recovered 3D poses from 2D poses by a 3D pose decoding
    module.
  prefs: []
  type: TYPE_NORMAL
- en: Many approaches conducted the two-stage approaches of graph neural networks
    to the lifting in 3D HPE. For two-stage approaches, a GCN-based lifting network
    generally takes the graph from 2D poses as the input to produce 3D poses. Zhao
    et al. [[48](#bib.bib48)] proposed a semantic graph convolutional network to regress
    3D joint coordinates from the 2D joint coordinates. Hu et al. [[49](#bib.bib49)]
    proposed a directed graph-based skeleton representation and applied a graph convolutional
    network to exploit both spatial and temporal information of image sequences. The
    two-stage GCN-based HPE approaches show competitive performance by considering
    the graph of the prior knowledge of the human skeleton. However, GCNs-based approaches
    generally integrate other DNNs-based models to extract 2D poses in the first stage
    as the input of GCNs (the second stage), which limits the usage of GCNs in the
    HPE field.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Multi-view 3D HPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The multi-view 3D HPE tasks aim to predict 3D joints position with synchronous
    multi-view cameras. Although the classic triangulation method (the details refer
    to [[107](#bib.bib107)]) can calculate accurate 3D object locations via multi-view
    camera systems, the triangulation method performs a drawback of noticeable sensitivity
    to inaccurate 2D prediction. Alleviating the error of 2D prediction is a critical
    problem of multi-view 3D HPE. Iskakov et al. [[87](#bib.bib87)] proposed an end-to-end
    DNN-based learnable triangulation method to produce confidence weights for each
    view. Qiu et al. [[90](#bib.bib90)] proposed a multi-view fusion layer to improve
    2D pose estimation and used a recursive pictorial structure model to predict 3D
    pose. He et al. [[89](#bib.bib89)] introduced epipolar geometry to multi-view
    fusion, which significantly decreased the number of parameters in the fusion module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the multi-person HPE, the existing multi-view 3D HPE can be categorized
    into two types: the top-down and bottom-up methods. The top-down methods predict
    the 2D poses of each view and match the 2D poses to perform the 3D reconstruction.
    Dong et al. [[108](#bib.bib108)] proposed a multi-way matching algorithm that
    combined both geometric and appearance cues to match the detected 2D poses across
    views. Chen et al. [[109](#bib.bib109)] utilized the temporal consistency to match
    multi-view 2D poses to 3D poses, and retained and updated the 3D poses by the
    cross-view multi-person tracking. Huang et al. [[110](#bib.bib110)] proposed a
    dynamic matching algorithm to match corresponding multi-view 2D poses from different
    views for each person and then used point triangulation to recover 3D poses. Currently,
    there are a few studies that apply bottom-up approaches to multi-view 3D HPE.
    Elmi et al. [[111](#bib.bib111)] originally applied a bottom-up approach to the
    multi-view 3D HPE. The 2D features of each image were processed by a backbone
    network and then aggregated by an un-projection layer into a 3D input representation.
    Finally, a sub-voxel joint detection module and a skeleton decoder module were
    employed to produce a set of 3D poses.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Multimodal Learning in 3D HPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multimodal learning is a deep learning-based approach that builds models with
    multiple modalities. A modality often refers to a sensory modality such as vision,
    touch signal, or radio signal. With the fusion of multiple sensors, multimodal
    learning approaches are more robust and capable of overcoming the challenges like
    occlusions than vision methods [[112](#bib.bib112)]. Although researchers have
    shown increasing interest in the field of multimodal learning approaches for HPE,
    a few studies apply multimodal learning to implement 3D HPE. We retrieve the related
    studies of multimodel learning in the Scopus database, as shown in [4(d)](#S6.F4.sf4
    "4(d) ‣ Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣
    Vision-based Human Pose Estimation via Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: In this subsection, we review the related deep learning-based approaches that
    utilize 2D vision data, depth information, and multimodal information (e.g, IMUs
    signal). Marin et al. [[113](#bib.bib113)] proposed a deep depth pose model to
    combine RGB-D information and a set of predefined 3D poses to predict the 3D joint
    positions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the deep learning-based methods with the data of IMUs and images
    have shown remarkable results in 3D HPE (as shown in [Table IV](#S4.T4 "TABLE
    IV ‣ IV 3D Human Pose Estimation ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey")). Marcard et al. [[35](#bib.bib35)] adopted IMUs and a mobile
    camera to estimate 3D human poses, and proposed the skinned multi-person linear
    model to produce initial 3D poses and then associated the 3D poses with 2D detected
    persons. Huang et al. [[96](#bib.bib96)] used the orientation of body parts, that
    is captured by IMUs, to refine the image-based pose estimation. In summary, multimodal
    HPE generally shows superior performance compared to vision-only HPE. However,
    it’s worth noting that the lack of labeled datasets is a critical problem in multimodal
    HPE.'
  prefs: []
  type: TYPE_NORMAL
- en: V Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'HPE has been used in a variety of applications such as action analysis [[1](#bib.bib1)],
    HCI [[2](#bib.bib2)], gaming [[3](#bib.bib3)], sport analysis [[4](#bib.bib4),
    [5](#bib.bib5)], motion capture [[6](#bib.bib6)], computer-generated imagery [[7](#bib.bib7),
    [8](#bib.bib8)]. Accurate joint points are semantically informative and can be
    utilized in computer vision tasks like action recognition, and human tracking
    [[99](#bib.bib99)]. In this section, we present the main applications of HPE (see
    the summary in [Table V](#S5.T5 "TABLE V ‣ V Applications ‣ Vision-based Human
    Pose Estimation via Deep Learning: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2D/3D HPE | Applications | Methods | Remarks |'
  prefs: []
  type: TYPE_TB
- en: '| 2D HPE | Action Analysis | $\bullet$ Bottom-up MPPE (OpenPose [[40](#bib.bib40)])
    + GCN [[114](#bib.bib114)] $\bullet$ Top-down MPPE (AlphaPose [[39](#bib.bib39)])
    + GCN | HPE provides spatial joint data and implicit temporal correlationship
    for action analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Character Animation | $\bullet$ Top-down MPPE [[52](#bib.bib52)] + Motion
    Transfer [[115](#bib.bib115)] $\bullet$ Bottom-up MPPE [[40](#bib.bib40)] + Motion
    Transfer [[8](#bib.bib8)] | Animation by HPE-based motion transferring from human
    performer to animated character |'
  prefs: []
  type: TYPE_TB
- en: '| Sports Analysis | $\bullet$ Bottom-up MPPE [[40](#bib.bib40)] + Coaching
    method [[116](#bib.bib116)] $\bullet$ Video-based HPE + Coaching method [[4](#bib.bib4)]
    | HPE-based sports analysis by comparisons between players and exemplars |'
  prefs: []
  type: TYPE_TB
- en: '| Medical & Clinical | $\bullet$ Top-down MPPE + Exercise Supervision [[117](#bib.bib117)]
    $\bullet$ Bottom-up MPPE + Exercise Supervision [[118](#bib.bib118)] | Joint positions
    provide rich information for clinical applications like in-bed/sleep monitoring
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3D HPE | Action Analysis | $\bullet$ Multimodal 3D HPE (Kinect-based) + Random
    Forest [[119](#bib.bib119)] $\bullet$ Multimodal 3D HPE (Kinect-based) + CNN [[120](#bib.bib120)]
    | 3D action analysis can exploit both 2D joint position and extra depth information
    |'
  prefs: []
  type: TYPE_TB
- en: '| HCI | $\bullet$ Multimodal 3D HPE (Kinect-based) + HCI [[2](#bib.bib2)] $\bullet$
    Monocular 3D HPE + HCI [[3](#bib.bib3)] | HPE-based HCI is a natural and contactless
    way |'
  prefs: []
  type: TYPE_TB
- en: '| Character Animation | $\bullet$ Two-stage Monocular 3D HPE + Motion Transfer
    [[121](#bib.bib121), [6](#bib.bib6)] | 3D character animations benefit from 3D
    HPE and motion transfer |'
  prefs: []
  type: TYPE_TB
- en: '| Sports Analysis | $\bullet$ Multimodal 3D HPE (Kinect-based) + Coaching Method
    [[122](#bib.bib122), [5](#bib.bib5)] $\bullet$ Monocular 3D HPE [[81](#bib.bib81)]
    + Coaching Method [[123](#bib.bib123)] | 3D HPE provides more dimensionality (3D
    angles between joints) for a coaching system |'
  prefs: []
  type: TYPE_TB
- en: '| Medical & Clinical | $\bullet$ Multimodal 3D HPE (Kinect-based) + Neural
    Recording [[124](#bib.bib124)] $\bullet$ Multimodal 3D HPE (Kinect-based) + Exercise
    Supervision [[125](#bib.bib125)] | 3D HPE-based clinical monitoring and rehabilitation
    system use human joint and depth information |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Summary of the approaches in HPE-based applications.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A Action Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Action recognition/prediction is mainly a temporal task based on image sequences.
    The traditional methods might demand extensive computation and appear unstable
    to environmental variations in terms of illuminations, objects in the background
    or foreground, body scale, and motion blur [[126](#bib.bib126)]. The human skeleton
    is naturally a high-level representation which has shown benefits for action analysis
    tasks such as action recognition [[127](#bib.bib127)] and action detection [[128](#bib.bib128)].
    For example, Duan et al. [[129](#bib.bib129)] stacked 2D heatmaps from human pose
    sequences to 3D heatmap volumes and utilized ResNet layers to predict human actions
    from these volumes. Liu et al. [[1](#bib.bib1)] used graph convolutional network
    to utilize 3D pose sequence for action analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, with the development of sensors (e.g., Kinect [[125](#bib.bib125)])
    and HPE algorithms, large-scale and accurate skeleton data for action recognition
    [[127](#bib.bib127)] becomes accessible. The space correlations and temporality
    of skeleton sequences provide informative prior knowledge to yield a robust motion
    pattern. On large-scale action recognition dataset NTURGB-D [[127](#bib.bib127)],
    the state-of-the-art skeleton-based methods [[129](#bib.bib129), [130](#bib.bib130)]
    achieved more than 95% accuracy, while image-only methods [[99](#bib.bib99)] achieved
    less than 90% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the skeleton-based action analysis can be used to build smart surveillance
    systems. Hbali et al. [[119](#bib.bib119)] used HPE and action analysis to build
    up an elderly monitoring system for alerting dangerous activities. Guo et al.
    [[131](#bib.bib131)] used skeleton-based action recognition to identify unsafe
    behaviours of construction workers. In conclusion, HPE provides significant spatial
    joint information and implicit temporal correlations for skeleton-based action
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Human-Computer Interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Human-computer interaction (HCI) has been studied for several decades and plays
    an important role in our daily lives. Traditional HCI techniques allow humans
    to interact with computers via tangible devices or interfaces such as mice, keyboards,
    or touch screens. Compared with traditional interactions, HPE-based HCI provides
    a natural and contactless way that is highly suitable for the difficult pandemic
    situation of COVID-19, particularly when using public devices.
  prefs: []
  type: TYPE_NORMAL
- en: HPE-based HCI techniques are widely applied in many applications such as arts,
    gaming, and virtual reality. Most vision-only applications consider the estimation
    of 2D human poses since currently, 2D HPE approaches can offer more accurate and
    prompt predictions than 3D HPE. While 3D HPE often employs depth-aware sensors
    (like RGB-D cameras) to produce reliable and informative 3D poses. The well-known
    depth-aware distributed Microsoft Kinects [[125](#bib.bib125)] is designed to
    capture human body movements for gaming and virtual reality teleconference systems
    [[132](#bib.bib132)]. Kamel et al. [[123](#bib.bib123)], Thar et al. [[133](#bib.bib133)],
    and Park [[5](#bib.bib5)] et al. used 3D monocular HPE and a single camera to
    capture human poses for producing action evaluation and feedback for Tai Chi,
    Yoga, and golf, respectively. In summary, HPE-based HCI is a type of natural and
    contactless interface that differs from the traditional HCI.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Character Animation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creating high-quality character animation is important in animation films and
    computer games. Traditional methods rely on creating costly and time-consuming
    frame-wise animations. HPE-based performance-driven animations accomplish character
    motions by transferring motions from a human performer to an animated character,
    which is prevalent in the film and game industries conveniently and economically.
    For 2D animation applications, Willett et al. [[8](#bib.bib8)] proposed a novel
    method to jointly yield 2D animation and 2D character creation by leveraging human
    pose. In addition, the HPE technique is helpful in augmented reality. For instance,
    Weng et al. [[115](#bib.bib115)] utilized human skeleton information via HPE to
    generate 3D character animation from an image. To generate 3D animations, a commonly
    used method is to control all parts of a character via motion capture [[6](#bib.bib6)].
    In summary, DL-based HPE approaches provide an alternative to traditional motion
    capture for creating both 2D and 3D animations.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Sports Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sports performance analysis provides statistics/recording for coaches and players
    to improve their performance. The automatic sports analysis demands precise postures
    and motion of players, which requires accurate markerless motion capture techniques.
    The HPE approaches perform effectively in the analysis of many sporting activities.
    HPE-based methods can be used to compare the differences between learners and
    instructors in various sports. For example, Park et al. [[5](#bib.bib5)] used
    the HPE approach to analyze activities in golf. By comparing a user’s swing with
    the reference swing, the user can examine his/her evaluation result in head movement,
    knee alignment, swing rhythm, and balance of golf swing in their approach. Kamel
    et al. [[123](#bib.bib123)], and Thar et al. [[133](#bib.bib133)] employed HPE
    to evaluate the difference between a learner’s posture and an instructor’s posture
    in Tai Chi and Yoga, respectively. Wang et al. [[4](#bib.bib4)] built up an HPE-based
    athletic training assistance system to detect bad poses from a sequence of 2D
    users’ poses. In addition, HPE-based approach is applied to capture players’ motion
    in sports like badminton [[116](#bib.bib116)], soccer [[134](#bib.bib134), [135](#bib.bib135)],
    and tennis [[136](#bib.bib136), [137](#bib.bib137)]. In conclusion, HPE-based
    sports analysis mainly relies on a comparison between a player and an exemplar.
    2D HPE is more commonly adopted than 3D HPE as 3D HPE currently requires either
    multi-view setups or depth-aware sensors.
  prefs: []
  type: TYPE_NORMAL
- en: V-E Medical and Clinical Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Accurate joint positions provide rich information for clinical applications
    like in-bed monitoring, sleep laboratories, epilepsy monitoring, and intensive
    care units. Gabeiel et al. [[124](#bib.bib124)] combined synchronized Kinect v2
    and standard clinical electrocorticography monitors, which record neural activity
    from the cortex, to investigate the relationship between human movement behaviours
    and neural activity. In rehabilitation medicine, HPE is successfully applied to
    improve the rehabilitation of patients. Obdržálek et al. [[125](#bib.bib125)]
    employed HPE for observation and online feedback when coaching elderly patients
    with the daily physical exercise routine. Li et al. [[117](#bib.bib117)] introduced
    an in-home lower-limb rehabilitation system with an HPE-based model to help patients
    perform rehabilitation activities even without the presence of physical therapists.
    In [[118](#bib.bib118)], Rabbito used OpenPose [[40](#bib.bib40)] and motion capture
    system Vicon ¹¹1[https://www.vicon.com/](https://www.vicon.com/) to analyze patient
    gait for rehabilitation medicine.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, HPE-based clinical applications generally fall into two main types,
    in-bed monitoring and rehabilitation training. Clinical environments are suitable
    for installing multi-view cameras or depth-aware sensors, which encourages 3D
    HPE. HPE-based Rehabilitation training currently is similar to HPE-based sports
    coaching, which is mostly based on the comparison between a user and an exemplar.
    While traditional motion capture systems are available in limited environments,
    HPE-based methods have great potential for providing more extensive human motion
    data.
  prefs: []
  type: TYPE_NORMAL
- en: VI Research Trends and Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we aim to point out the observed research trends via bibliometrics,
    raise the challenging issues, and provide insightful recommendations for future
    research.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Research Trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we identify the research trends via bibliometrics which
    refers to the use of statistical methods to analyze books, articles and other
    publications. It is an effective way to measure publications in the scientific
    community. However, current survey papers rarely use bibliometrics to analyze
    the data of publications, particularly in HPE. This paper applies bibliometrics
    to retrieve the publications for discovering and demonstrating the research trends
    in HPE.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, bibliometric data can be obtained from various databases. In this
    paper, we choose the Scopus database ²²2[https://en.wikipedia.org/wiki/Scopus](https://en.wikipedia.org/wiki/Scopus)
    for literature retrieval. Scopus is the largest abstract/citation database with
    peer-reviewed literature, which is released by Elsevier. The resources of Scopus
    are more accurate and comprehensive than other alternatives such as PubMed, Web
    of Science, and Google Scholar [[138](#bib.bib138)]. Importantly, Elsevier provides
    a Python library ³³3[https://github.com/pybliometrics-dev/pybliometrics](https://github.com/pybliometrics-dev/pybliometrics)
    to retrieve the data for the expected topics from Scopus database [[139](#bib.bib139)].
    In this work, we review the articles in vision-based HPE using deep learning by
    retrieving "title, abstract, and keywords" in the Scopus database. We present
    the trends that are observed from the literature in the four aspects: multi-person
    pose estimation, 3D HPE, efficient deep learning-based HPE, and multi-modal learning.
    To observe the trends clearly, we retrieve the data for each year over ten years
    (2012 - 2022). We retrieve the number of publications in Scopus data from the
    four aspects, as shown in [Figure 4](#S6.F4 "Figure 4 ‣ VI-A Research Trends ‣
    VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey"), where the red dash lines are the fit of the number of publications
    from 2012 to 2021 (NOT 2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/663d286169155eb6cfdf39eaa1b24ee8.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/48c6dc26b282602fcf3396c3547e9284.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bf95aad6eb2b300b60700a3bf95ec62a.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9329ae4cfe06ce2ccbf0051083bae55b.png)'
  prefs: []
  type: TYPE_IMG
- en: (d)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: The number of publications on the HPE topics of a) multi-person pose
    estimation, b) 3D HPE, c) efficient deep learning-based HPE, d) multimodal learning
    HPE from Scopus. Scopus database returned 274, 769, 185, 111 results for a), b),
    c), and d) respectively until 08/09/2022\. To observe the trends clearly, we retrieve
    the data of each year over the ten years from 2012 to 2022\. The red dashed lines
    are the fit of the number of publications from 2012 to 2021 (NOT 2022). The source
    code to retrieve the data are available at [%\textcolor{red}{https://github.com/wuyuuu/elsevier-search](%%5Ctextcolor%7Bred%7D%7Bhttps://github.com/wuyuuu/elsevier-search).'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A1 Multi-Person HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multi-Person HPE has become an important research topic. We retrieve the literature
    on this topic by using the search code TITLE-ABS-KEY(("multi-person" or "crowd")
    and ("multi-person pose estimation" or "human pose estimation")). The number of
    publications over the years (from 2012 to 2022) is shown in [4(a)](#S6.F4.sf1
    "4(a) ‣ Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣
    Vision-based Human Pose Estimation via Deep Learning: A Survey"). It shows a very
    significant increasing trend in the number of Multi-Person HPE publications since
    2015, while an insignificant drop in 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, the small number of publications shows that the multi-person pose
    estimation in crowd scenes is still rarely investigated. On the other hand, the
    increasing number of publications shows the increasing trend in this topic. This
    technique has been applied in various applications such as multi-person gaming
    and sports analysis. However, as reported in [[32](#bib.bib32)], the performance
    of the existing methods degrades dramatically as the crowd level increases. In
    multi-person HPE, there are still many open issues and challenges that need to
    be further studied in the future.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A2 3D HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Accurately estimating 3D positions for human joints is an important topic in
    computer vision. Compared to 2D HPE, 3D HPE could provide extra depth information
    which brings broader applications such as markerless motion capture, video games,
    and sports analysis [[5](#bib.bib5), [92](#bib.bib92)]. We observe that 3D HPE
    attracts more and more research interest recently. We design the code of TITLE-ABS-KEY(("3D")
    and ("human pose estimation")) to retrieve the literature on the 3D HPE-related
    topics in the Scopus database. The number of publications over the years (from
    2012 to 2022) is shown in [4(b)](#S6.F4.sf2 "4(b) ‣ Figure 4 ‣ VI-A Research Trends
    ‣ VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation via Deep
    Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: We observe an increasing number of 3D HPE publications from 2014 to 2021. However,
    the existing 3D HPE methods still need to be investigated for desired performance
    because of unsolved problems such as the ill-posed property, the lack of outdoor
    environment annotations, and the incapability of performing real-time inference
    on edge devices. Therefore, the 3D HPE will be a crucial research trend in the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A3 Efficient Deep Learning-based HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To apply HPE methods to real applications, the efficiency of HPE approaches
    is a crucial research topic. In particular, although deep learning-based methods
    have achieved state-of-the-art performance in HPE, the approaches of deep neural
    networks may suffer from high computational costs and inference delay. We design
    the code of TITLE-ABS-KEY(("efficient" or "real-time") and ("human pose estimation"))
    to retrieve literature in efficient HPE. The number of publications over year
    (from 2012 to 2022) is shown in [4(c)](#S6.F4.sf3 "4(c) ‣ Figure 4 ‣ VI-A Research
    Trends ‣ VI Research Trends and Challenges ‣ Vision-based Human Pose Estimation
    via Deep Learning: A Survey"). It shows a significant increase since 2014 with
    respect to the number of publications in efficient HPE. We note that the HPE efficiency
    problem was generally tackled by designing efficient networks manually in previous
    approaches [[56](#bib.bib56), [40](#bib.bib40)], while current studies [[17](#bib.bib17),
    [140](#bib.bib140)] introduce neural architecture search (NAS) into HPE, which
    might be a typical method in solving the efficiency problem of neural network
    in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A4 Multimodal Learning for HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Multimodal learning extracts features from multiple sensing modalities, which
    alleviates the complexity of unimodal methods. Recently, the multimodal learning-based
    approaches [[94](#bib.bib94), [35](#bib.bib35)] have been validated for robustness
    and accuracy in HPE tasks. We retrieve the literature by using the code of TITLE-ABS-KEY(("multi-modal"
    or"multimodal" or "IMUs" or "radio signal") and ("human pose estimation") and
    not ("distribution")) to observe and demonstrate the research trend. The number
    of publications over years (from 2012 to 2022) is shown in [4(d)](#S6.F4.sf4 "4(d)
    ‣ Figure 4 ‣ VI-A Research Trends ‣ VI Research Trends and Challenges ‣ Vision-based
    Human Pose Estimation via Deep Learning: A Survey"). Although the total number
    of publications implies a lack of attention in multimodal learning HPE, the steady
    growth of publications implies an increasing interest in this topic. The multimodal
    learning methods offer solutions and robustness to the vision-based model in addressing
    occlusions, which are promising solutions towards the in-the-wild challenge [[35](#bib.bib35)].
    The multimodal learning methods for HPE still need to be further investigated
    in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Research Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A remarkable HPE approach should consider both high accuracy and high efficiency.
    Although many studies have investigated the HPE with prominent performance, there
    are many considerable challenges to achieve both goals. An accurate HPE approach
    is generally demanded to deal with the challenges of various occlusions and personal
    appearances, estimate the depth of monocular image, and remain robust to image
    degeneration. In addition, the current biased datasets (e.g., incomprehensive
    outdoor 3D annotations, relatively rare uncommon poses) is a prevalent challenge
    for the practical accuracy of HPE in real-world scenarios. Moreover, implementing
    efficient deep learning-based HPE on resource-limited devices is a notorious challenge.
    The current deep learning-based HPE generally take a lot of computing time because
    of the large-scale networks. In particular, the computing time significantly creases
    with the creasing number of people for the crowd scenes. Therefore, a number-robust
    HPE algorithm is critical for implementing an efficient HPE. In this subsection,
    we summarize and discuss the challenges in HPE from two aspects: accuracy and
    efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B1 Challenges in Accurate HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diverse Human Poses and Appearances
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A fundamental challenge in developing accurate HPE comes from the diversity
    of human poses [[12](#bib.bib12)]. Despite the vast range of human appearances,
    the human body entails high degree of freedom, demanding an advanced presentation
    ability for data-driven approaches in HPE. Additionally, image degeneration like
    motion blur and image defocus exists in video-based data [[44](#bib.bib44)], which
    also hinders HPE approaches from achieving remarkable performance.
  prefs: []
  type: TYPE_NORMAL
- en: Occlusions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although current HPE methods perform outstanding performance on many public
    datasets, a well-known issue is noticeable performance degeneration caused by
    occlusions and highly deformable human body [[30](#bib.bib30), [39](#bib.bib39),
    [141](#bib.bib141)]. Self-occlusions and mutual occlusions could prompt the occlusions
    and environmental truncation, while mutual occlusions can occur extensively in
    crowd scenarios [[32](#bib.bib32)] to cause the performance to decline dramatically.
    The highly deformable human body can cause ambiguity in small-scale human instances
    [[141](#bib.bib141)] or the specific human poses as well. Thus, designing the
    powerful HPE method for occlusion scenarios with the capability of utilizing global
    information and prior knowledge would be a challenging issue.
  prefs: []
  type: TYPE_NORMAL
- en: Incomprehensive Datasets
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To apply HPE approaches to practical applications, challenges could come from
    the gap between current incomprehensive datasets and real-world applications.
    For example, uncommon poses like falling down are less likely to appear in datasets,
    and the outdoors 3D HPE dataset is relatively rare. This gap leads to an imbalanced
    learning problem, which could hinder the applications of 3D HPE in the real world.
    Although current approaches can leverage semi-supervised learning [[104](#bib.bib104)]
    or synthetic dataset [[37](#bib.bib37)] to enrich the datasets, the semi-supervised
    methods still need a lot of quality training data [[98](#bib.bib98)]. Current
    datasets lack realistic simulation of lighting effects, clothing meshes, and environment
    interactions [[142](#bib.bib142)]. Therefore, it is difficult to transfer the
    trained neural networks from the simulation to real-world applications. Developing
    a remarkable HPE on current incomprehensive datasets is still challenging, particularly
    for deployment in complex environments.
  prefs: []
  type: TYPE_NORMAL
- en: '| Types | Studies | URL (Open source code) | Remarks |'
  prefs: []
  type: TYPE_TB
- en: '| 2D/3D HPE | MMPose | [https://github.com/open-mmlab/mmpose](https://github.com/open-mmlab/mmpose)
    | Well-known platform |'
  prefs: []
  type: TYPE_TB
- en: '| 2D HPE | Associative embedding [[63](#bib.bib63)] | [https://github.com/princeton-vl/pose-ae-train](https://github.com/princeton-vl/pose-ae-train)
    | A SOTA grouping for bottom-up approaches |'
  prefs: []
  type: TYPE_TB
- en: '| Hourglass [[41](#bib.bib41)] | [https://github.com/princeton-vl/pose-hg-demo](https://github.com/princeton-vl/pose-hg-demo)
    | Effective yet simple backbone |'
  prefs: []
  type: TYPE_TB
- en: '| OpenPose [[40](#bib.bib40)] | [https://github.com/CMU-Perceptual-Computing-Lab/openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
    | Real-time & bottom-up |'
  prefs: []
  type: TYPE_TB
- en: '| AlphaPose [[39](#bib.bib39)] | [https://github.com/MVIG-SJTU/AlphaPose](https://github.com/MVIG-SJTU/AlphaPose)
    | Real-time & top-down |'
  prefs: []
  type: TYPE_TB
- en: '| Higher-HRNet [[72](#bib.bib72)] | [https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation](https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation)
    | A SOTA bottom-up approach |'
  prefs: []
  type: TYPE_TB
- en: '| HRNet [[10](#bib.bib10)] | [https://github.com/HRNet/HRNet-Human-Pose-Estimation](https://github.com/HRNet/HRNet-Human-Pose-Estimation)
    | A SOTA top-down approach |'
  prefs: []
  type: TYPE_TB
- en: '| RLE [[143](#bib.bib143)] | [https://github.com/Jeff-sjtu/res-loglikelihood-regression](https://github.com/Jeff-sjtu/res-loglikelihood-regression)
    | A SOTA regression-based HPE |'
  prefs: []
  type: TYPE_TB
- en: '| UDP-POSE [[144](#bib.bib144)] | [https://github.com/HuangJunJie2017/UDP-Pose](https://github.com/HuangJunJie2017/UDP-Pose)
    | 1st in ICCV 2019 COCO keypoint challenge |'
  prefs: []
  type: TYPE_TB
- en: '| DARK [[58](#bib.bib58)] | [https://ilovepose.github.io/coco/](https://ilovepose.github.io/coco/)
    | 2nd in ICCV 2019 COCO keypoint challenge |'
  prefs: []
  type: TYPE_TB
- en: '| Lite-HRNet [[145](#bib.bib145)] | [https://github.com/HRNet/Lite-HRNet](https://github.com/HRNet/Lite-HRNet)
    | Lightweight HRNet-based model |'
  prefs: []
  type: TYPE_TB
- en: '| Lightweight OpenPose [[146](#bib.bib146)] | [https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch)
    | Real-time on CPU |'
  prefs: []
  type: TYPE_TB
- en: '| BlazePose [[64](#bib.bib64)] | [https://google.github.io/mediapipe/solutions/pose.html](https://google.github.io/mediapipe/solutions/pose.html)
    | Real-time 2D single-person HPE driven by MediaPipe [[65](#bib.bib65)] |'
  prefs: []
  type: TYPE_TB
- en: '| PRTR [[60](#bib.bib60)] | [https://github.com/mlpc-ucsd/PRTR](https://github.com/mlpc-ucsd/PRTR)
    | 2D Pose Regression transformer |'
  prefs: []
  type: TYPE_TB
- en: '|  | DCPose [[147](#bib.bib147)] | [https://github.com/Pose-Group/DCPose](https://github.com/Pose-Group/DCPose)
    | 1st in PoseTrack2017 & PoseTrack2018 |'
  prefs: []
  type: TYPE_TB
- en: '| 3D HPE | Epipolar transformer [[89](#bib.bib89)] | [https://github.com/yihui-he/epipolar-transformers](https://github.com/yihui-he/epipolar-transformers)
    | A SOTA multi-view approach |'
  prefs: []
  type: TYPE_TB
- en: '| Learnable triangulation [[87](#bib.bib87)] | [https://saic-violet.github.io/learnable-triangulation/](https://saic-violet.github.io/learnable-triangulation/)
    | A SOTA multi-view approach |'
  prefs: []
  type: TYPE_TB
- en: '| SMAP [[148](#bib.bib148)] | [https://github.com/zju3dv/SMAP](https://github.com/zju3dv/SMAP)
    | SOTA single-view multi-person |'
  prefs: []
  type: TYPE_TB
- en: '| DOPE [[149](#bib.bib149)] | [https://github.com/naver/dope](https://github.com/naver/dope)
    | Real-time whole-body 3D HPE |'
  prefs: []
  type: TYPE_TB
- en: '| VNect [[150](#bib.bib150)] | [http://gvv.mpi-inf.mpg.de/projects/VNect/](http://gvv.mpi-inf.mpg.de/projects/VNect/)
    | Real-time single-view approach |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic occlusion [[151](#bib.bib151)] | [https://github.com/isarandi/synthetic-occlusion](https://github.com/isarandi/synthetic-occlusion)
    | 1st place in ECCV2018 3D HPE Challenge |'
  prefs: []
  type: TYPE_TB
- en: '| Integral regression [[152](#bib.bib152)] | [https://github.com/JimmySuen/integral-human-pose](https://github.com/JimmySuen/integral-human-pose)
    | 2nd place in ECCV2018 3D HPE Challenge |'
  prefs: []
  type: TYPE_TB
- en: '| PoseFormer [[103](#bib.bib103)] | [https://github.com/zczcwh/PoseFormer](https://github.com/zczcwh/PoseFormer)
    | 3D pose transformer |'
  prefs: []
  type: TYPE_TB
- en: '| Top-down & bottom-up Integration [[153](#bib.bib153)] | [https://github.com/3dpose/3D-Multi-Person-Pose](https://github.com/3dpose/3D-Multi-Person-Pose)
    | A SOTA monocular multi-person 3D HPE |'
  prefs: []
  type: TYPE_TB
- en: '| Normalizing flows [[154](#bib.bib154)] | [https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows](https://github.com/twehrbein/Probabilistic-Monocular-3D-Human-Pose-Estimation-with-Normalizing-Flows)
    | A SOTA monocular 3D HPE |'
  prefs: []
  type: TYPE_TB
- en: '| PoseAug [[86](#bib.bib86)] | [https://github.com/jfzhang95/PoseAug](https://github.com/jfzhang95/PoseAug)
    | A data augmentation framework for 3D HPE |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: The open-source code of the state-of-the-art HPE methods.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B2 Challenges in Efficient HPE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Computation-intensive Neural Networks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Eventually, the HPE approach needs to be implemented for applications in the
    real world. However, the state-of-the-art neural networks [[10](#bib.bib10), [148](#bib.bib148)]
    are generally hard to be implemented on mobile devices or embedded devices as
    their enormous computational cost. Thus, it is crucial to design lightweight neural
    networks for efficient HPE. The existing methods of designing lightweight neural
    networks are mainly manual design and heuristic design (e.g., NAS [[140](#bib.bib140)]).
    However, the method of manual design is hard to balance the accuracy and network
    size [[145](#bib.bib145)]. NAS-based methods [[140](#bib.bib140), [17](#bib.bib17)]
    generally need various computational cost even weeks of CPU time. Therefore, developing
    lightweight neural networks for HPE is still a challenging task.
  prefs: []
  type: TYPE_NORMAL
- en: Time-Consuming MPPE
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Currently, MPPE algorithms consume increasing computation time over the increasing
    number of targeted persons. Top-down approaches estimate the pose of each detected
    person after the person detection stage. Bottom-up approaches [[63](#bib.bib63),
    [40](#bib.bib40)] predict similarity values between keypoints and employ a matching
    algorithm (e.g., Hungarian algorithm) for grouping keypoints. Note that top-down
    and bottom-up approaches are two-stage methods due to top-down approaches require
    an extra detection stage and bottom-up approaches require an extra grouping stage,
    besides estimating keypoint locations.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to two-stage methods, single-stage methods generally perform superior
    in terms of computational cost. A promising single-stage method [[69](#bib.bib69)]
    predicts the locations of persons and keypoints’ offsets to each location. However,
    the single-stage methods are generally not as competitive as the state-of-the-art
    two-stage methods [[72](#bib.bib72), [10](#bib.bib10)] in terms of accuracy. How
    to develop a desired efficient MPPE is still an interesting challenge, particularly
    for applications in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we presented an up-to-date and in-depth overview of the deep
    learning approaches in vision-based HPE. We systematically introduced the preliminary
    knowledge in HPE and reviewed the HPE approaches in two categories: 2D-based approaches
    and 3D-based approaches. We discussed a number of interesting applications of
    deep learning-based HPE. Finally, we pointed out the research trends via bibliometrics,
    raised the challenging issues, and provided insightful recommendations for future
    research. To help readers to reproduce the state-of-the-art methods, we summarized
    the open-source codes of the well-known studies for 2D and 3D deep learning-based
    HPE in [Table VI](#S6.T6 "TABLE VI ‣ Incomprehensive Datasets ‣ VI-B1 Challenges
    in Accurate HPE ‣ VI-B Research Challenges ‣ VI Research Trends and Challenges
    ‣ Vision-based Human Pose Estimation via Deep Learning: A Survey"), which could
    help readers easily implement their HPE tasks. This article provides a meaningful
    overview as introductory material for beginners to deep learning-based HPE, as
    well as supplementary material for advanced researchers.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Z. Liu, H. Zhang, Z. Chen, Z. Wang, and W. Ouyang, “Disentangling and unifying
    graph convolutions for skeleton-based action recognition,” in *CVPR 2020*, pp.
    143–152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman,
    and A. Blake, “Real-time human pose recognition in parts from single depth images,”
    in *CVPR 2011*, pp. 1297–1304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S.-R. Ke, L. Zhu, J.-N. Hwang, H.-I. Pai, K.-M. Lan, and C.-P. Liao, “Real-time
    3D human pose estimation from monocular view with applications to event detection
    and video gaming,” in *2010 7th IEEE International Conference on Advanced Video
    and Signal Based Surveillance*.   IEEE, 2010, pp. 489–496.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Wang, K. Qiu, H. Peng, J. Fu, and J. Zhu, “AI coach: Deep human pose
    estimation and analysis for personalized athletic training assistance,” in *Proceedings
    of the 27th ACM International Conference on Multimedia*, 2019, pp. 374–382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] S. Park, J. Yong Chang, H. Jeong, J.-H. Lee, and J.-Y. Park, “Accurate
    and efficient 3D human pose estimation algorithm using single depth images for
    pose analysis in golf,” in *CVPR Workshops 2017*, pp. 49–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Z. Yang, W. Zhu, W. Wu, C. Qian, Q. Zhou, B. Zhou, and C. C. Loy, “TransMoMo:
    Invariance-driven unsupervised video motion retargeting,” in *CVPR 2020*, pp.
    5306–5315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Hornung, E. Dekkers, and L. Kobbelt, “Character animation from 2D pictures
    and 3D motion data,” *ACM Transactions on Graphics*, vol. 26, no. 1, pp. 1–es,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] N. S. Willett, H. V. Shin, Z. Jin, W. Li, and A. Finkelstein, “Pose2Pose:
    pose selection and transfer for 2D character animation,” in *Proceedings of the
    25th International Conference on Intelligent User Interfaces*, 2020, pp. 88–99.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Toshev and C. Szegedy, “DeepPose: Human pose estimation via deep neural
    networks,” in *CVPR 2014*, pp. 1653–1660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation
    learning for human pose estimation,” in *CVPR 2019*, pp. 5693–5703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] D. Hogg, “Model-based vision: a program to see a walking person,” *Image
    and Vision computing*, vol. 1, no. 1, pp. 5–20, 1983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] T. B. Moeslund and E. Granum, “A survey of computer vision-based human
    motion capture,” *Computer vision and image understanding*, vol. 81, no. 3, pp.
    231–268, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Advances in neural information processing
    systems*, vol. 25, pp. 1097–1105, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] G. Lan, J. Benito-Picazo, D. M. Roijers, E. Domínguez, and A. Eiben, “Real-time
    robot vision on low-performance computing hardware,” in *2018 15th International
    Conference on Control, Automation, Robotics and Vision (ICARCV)*.   IEEE, 2018,
    pp. 1959–1965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] G. Lan, L. De Vries, and S. Wang, “Evolving efficient deep neural networks
    for real-time object recognition,” in *2019 IEEE Symposium Series on Computational
    Intelligence (SSCI)*.   IEEE, 2019, pp. 2571–2578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall,
    and B. Schiele, “PoseTrack: A benchmark for human pose estimation and tracking,”
    in *CVPR 2018*, pp. 5167–5176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] W. McNally, K. Vats, A. Wong, and J. McPhee, “EvoPose2D: Pushing the boundaries
    of 2d human pose estimation using accelerated neuroevolution with weight transfer,”
    *IEEE Access*, vol. 9, pp. 139 403–139 414, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Q. Dang, J. Yin, B. Wang, and W. Zheng, “Deep learning based 2D human
    pose estimation: A survey,” *Tsinghua Science and Technology*, vol. 24, no. 6,
    pp. 663–676, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] R. Poppe, “A survey on vision-based human action recognition,” *Image
    and vision computing*, vol. 28, no. 6, pp. 976–990, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Y. Chen, Y. Tian, and M. He, “Monocular human pose estimation: A survey
    of deep learning-based methods,” *Computer Vision and Image Understanding*, vol.
    192, p. 102897, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] W. Liu and T. Mei, “Recent advances of monocular 2D and 3D human pose
    estimation: A deep learning perspective,” *ACM Computing Surveys (CSUR)*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] W. Gong, X. Zhang, J. Gonzàlez, A. Sobral, T. Bouwmans, C. Tu, and E.-h.
    Zahzah, “Human pose estimation from monocular images: A comprehensive survey,”
    *Sensors*, vol. 16, no. 12, p. 1966, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] L. M. Dang, K. Min, H. Wang, M. J. Piran, C. H. Lee, and H. Moon, “Sensor-based
    and vision-based human activity recognition: A comprehensive survey,” *Pattern
    Recognition*, vol. 108, p. 107561, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. Gadhiya and N. Kalani, “Analysis of deep learning based pose estimation
    techniques for locating landmarks on human body parts,” in *2021 International
    Conference on Circuits, Controls and Communications (CCUBE)*.   IEEE, 2021, pp.
    1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] J. Wang, S. Tan, X. Zhen, S. Xu, F. Zheng, Z. He, and L. Shao, “Deep 3D
    human pose estimation: A review,” *Computer Vision and Image Understanding*, p.
    103225, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Johnson and M. Everingham, “Clustered pose and nonlinear appearance
    models for human pose estimation,” in *Proceedings of the British Machine Vision
    Conference*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] B. Sapp and B. Taskar, “MODEC: Multimodal decomposable models for human
    pose estimation,” in *CVPR 2013*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black, “Towards understanding
    action recognition,” in *ICCV 2013*, pp. 3192–3199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] W. Zhang, M. Zhu, and K. G. Derpanis, “From actemes to action: A strongly-supervised
    representation for detailed action understanding,” in *ICCV 2013*, pp. 2248–2255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, “2D human pose
    estimation: New benchmark and state of the art analysis,” in *CVPR 2014*, pp.
    3686–3693.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft COCO: Common objects in context,” in *ECCV 2014*,
    pp. 740–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Li, C. Wang, H. Zhu, Y. Mao, H.-S. Fang, and C. Lu, “CrowdPose: Efficient
    crowded scenes pose estimation and a new benchmark,” in *CVPR 2019*, pp. 10 863–10 872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6M: Large
    scale datasets and predictive methods for 3D human sensing in natural environments,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 36, no. 7,
    pp. 1325–1339, jul 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] H. Joo, T. Simon, X. Li, H. Liu, L. Tan, L. Gui, S. Banerjee, T. Godisart,
    B. Nabbe, I. Matthews *et al.*, “Panoptic studio: A massively multiview system
    for social interaction capture,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 41, no. 1, pp. 190–204, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll,
    “Recovering accurate 3D human pose in the wild using IMUs and a moving camera,”
    in *ECCV 2018*, pp. 601–617.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, and C. Theobalt,
    “Monocular 3D human pose estimation in the wild using improved cnn supervision,”
    in *2017 international conference on 3D vision*.   IEEE, 2017, pp. 506–516.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Fabbri, F. Lanzi, S. Calderara, A. Palazzi, R. Vezzani, and R. Cucchiara,
    “Learning to detect and track visible and occluded body joints in a virtual world,”
    in *ECCV 2018*, pp. 430–446.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR 2016*, June.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu, “RMPE: Regional multi-person
    pose estimation,” in *ICCV 2017*, pp. 2334–2343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2D
    pose estimation using part affinity fields,” in *CVPR 2017*, pp. 7291–7299.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for human
    pose estimation,” in *European conference on computer vision*.   Springer, 2016,
    pp. 483–499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun, “Cascaded pyramid
    network for multi-person pose estimation,” in *CVPR 2018*, pp. 7103–7112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] B. Xiao, H. Wu, and Y. Wei, “Simple baselines for human pose estimation
    and tracking,” in *Proceedings of the European conference on computer vision (ECCV)*,
    2018, pp. 466–481.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Y. Luo, J. Ren, Z. Wang, W. Sun, J. Pan, J. Liu, J. Pang, and L. Lin,
    “LSTM pose machines,” in *CVPR 2018*, pp. 5207–5215.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] B. Artacho and A. Savakis, “UniPose: Unified human pose estimation in
    single images and videos,” in *CVPR 2020*, pp. 7035–7044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Bin, Z.-M. Chen, X.-S. Wei, X. Chen, C. Gao, and N. Sang, “Structure-aware
    human pose estimation with graph convolutional networks,” *Pattern Recognition*,
    vol. 106, p. 107410, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Z. Qiu, K. Qiu, J. Fu, and D. Fu, “DGCN: Dynamic graph convolutional network
    for efficient multi-person pose estimation,” in *AAAI 2020*, vol. 34, no. 07,
    pp. 11 924–11 931.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. N. Metaxas, “Semantic graph
    convolutional networks for 3D human pose regression,” in *CVPR 2019*, pp. 3425–3435.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] W. Hu, C. Zhang, F. Zhan, L. Zhang, and T.-T. Wong, “Conditional directed
    graph convolution for 3D human pose estimation,” in *Proceedings of the 29th ACM
    International Conference on Multimedia*, 2021, pp. 602–611.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Y. Cai, L. Ge, J. Liu, J. Cai, T.-J. Cham, J. Yuan, and N. M. Thalmann,
    “Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional
    networks,” in *ICCV 2019*, pp. 2272–2281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] J. J. Tompson, A. Jain, Y. LeCun, and C. Bregler, “Joint training of a
    convolutional network and a graphical model for human pose estimation,” in *Advances
    in neural information processing systems*, 2014, pp. 1799–1807.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, “Convolutional pose
    machines,” in *CVPR 2016*, pp. 4724–4732.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang, “Multi-context
    attention for human pose estimation,” in *CVPR 2017*, pp. 1831–1840.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang, “Learning feature pyramids
    for human pose estimation,” in *ICCV 2017*, pp. 1281–1290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Bulat, J. Kossaifi, G. Tzimiropoulos, and M. Pantic, “Toward fast and
    accurate human pose estimation via soft-gated skip connections,” in *2020 15th
    IEEE International Conference on Automatic Face and Gesture Recognition*, 2020,
    pp. 101–108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] U. Rafi, B. Leibe, J. Gall, and I. Kostrikov, “An efficient convolutional
    network for human pose estimation.” in *BMVC*, vol. 1, 2016, p. 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Jain, J. Tompson, Y. LeCun, and C. Bregler, “Modeep: A deep learning
    framework using motion features for human pose estimation,” in *Asian conference
    on computer vision*.   Springer, 2014, pp. 302–315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] F. Zhang, X. Zhu, H. Dai, M. Ye, and C. Zhu, “Distribution-aware coordinate
    representation for human pose estimation,” in *CVPR 2020*, pp. 7093–7102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] W. Mao, Y. Ge, C. Shen, Z. Tian, X. Wang, and Z. Wang, “Tfpose: Direct
    human pose estimation with transformers,” *arXiv preprint arXiv:2103.15320*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] K. Li, S. Wang, X. Zhang, Y. Xu, W. Xu, and Z. Tu, “Pose recognition with
    cascade transformers,” in *CVPR 2021*, pp. 1944–1953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] S. Yang, Z. Quan, M. Nie, and W. Yang, “Transpose: Keypoint localization
    via transformer,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2021, pp. 11 802–11 812.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Y. Xu, J. Zhang, Q. Zhang, and D. Tao, “ViTPose: Simple vision transformer
    baselines for human pose estimation,” *arXiv preprint arXiv:2204.12484*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Newell, Z. Huang, and J. Deng, “Associative embedding: End-to-end learning
    for joint detection and grouping,” in *Advances in neural information processing
    systems*, 2017, pp. 2277–2287.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] V. Bazarevsky, I. Grishchenko, K. Raveendran, T. Zhu, F. Zhang, and M. Grundmann,
    “Blazepose: On-device real-time body pose tracking,” *arXiv preprint arXiv:2006.10204*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] C. Lugaresi, J. Tang, H. Nash, C. McClanahan, E. Uboweja, M. Hays, F. Zhang,
    C.-L. Chang, M. G. Yong, J. Lee *et al.*, “Mediapipe: A framework for building
    perception pipelines,” *arXiv preprint arXiv:1906.08172*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *ICCV
    2017*, pp. 2961–2969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei,
    and J. Sun, “Rethinking on multi-stage networks for human pose estimation,” *arXiv
    preprint arXiv:1901.00148*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele,
    “Deepercut: A deeper, stronger, and faster multi-person pose estimation model,”
    in *European Conference on Computer Vision*.   Springer, 2016, pp. 34–50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] X. Nie, J. Feng, J. Zhang, and S. Yan, “Single-stage multi-person pose
    machines,” in *ICCV 2019*, pp. 6951–6960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] S. Jin, W. Liu, E. Xie, W. Wang, C. Qian, W. Ouyang, and P. Luo, “Differentiable
    hierarchical graph grouping for multi-person pose estimation,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 718–734.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Kocabas, S. Karagoz, and E. Akbas, “Multiposenet: Fast multi-person
    pose estimation using pose residual network,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 417–433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang, “HigherHRNet:
    Scale-aware representation learning for bottom-up human pose estimation,” in *CVPR
    2020*, pp. 5386–5395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler,
    and K. Murphy, “Towards accurate multi-person pose estimation in the wild,” in
    *CVPR 2017*, pp. 4903–4911.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] X. Nie, Y. Li, L. Luo, N. Zhang, and J. Feng, “Dynamic kernel distillation
    for efficient pose estimation in videos,” in *ICCV 2019*, pp. 6942–6950.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] G. Bertasius, C. Feichtenhofer, D. Tran, J. Shi, and L. Torresani, “Learning
    temporal pose estimation from sparsely-labeled videos,” in *Advances in neural
    information processing systems*, 2019, pp. 3027–3038.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] J. Wang, S. Jin, W. Liu, W. Liu, C. Qian, and P. Luo, “When human pose
    estimation meets robustness: Adversarial algorithms and benchmarks,” in *CVPR
    2021*, pp. 11 855–11 864.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S.-H. Zhang, R. Li, X. Dong, P. Rosin, Z. Cai, X. Han, D. Yang, H. Huang,
    and S.-M. Hu, “Pose2Seg: Detection free human instance segmentation,” in *CVPR
    2019*, pp. 889–898.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] N. Jain, S. Shah, A. Kumar, and A. Jain, “On the robustness of human pose
    estimation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition Workshops*, 2019, pp. 29–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M. Andriluka, S. Roth, and B. Schiele, “Discriminative appearance models
    for pictorial structures,” *International journal of computer vision*, vol. 99,
    no. 3, pp. 259–280, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] M. Bergtholdt, J. Kappes, S. Schmidt, and C. Schnörr, “A study of parts-based
    object class detection using complete graphs,” *International journal of computer
    vision*, vol. 87, no. 1-2, p. 93, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] S. Li and A. B. Chan, “3D human pose estimation from monocular images
    with deep convolutional neural network,” in *Asian Conference on Computer Vision*.   Springer,
    2014, pp. 332–347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C. Chen and D. Ramanan, “3D human pose estimation = 2D pose estimation
    + matching,” in *CVPR 2017*, pp. 5759–5767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] J. Martinez, R. Hossain, J. Romero, and J. J. Little, “A simple yet effective
    baseline for 3D human pose estimation,” in *ICCV 2017*, pp. 2659–2668.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] D. Mehta, O. Sotnychenko, F. Mueller, W. Xu, S. Sridhar, G. Pons-Moll,
    and C. Theobalt, “Single-shot multi-person 3D pose estimation from monocular RGB,”
    in *2018 International Conference on 3D Vision*.   IEEE Computer Society, 2018,
    pp. 120–130.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Li, L. Ke, K. Pratama, Y.-W. Tai, C.-K. Tang, and K.-T. Cheng, “Cascaded
    deep monocular 3D human pose estimation with evolutionary training data,” in *CVPR
    2020*, pp. 6173–6183.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] K. Gong, J. Zhang, and J. Feng, “PoseAug: A differentiable pose augmentation
    framework for 3D human pose estimation,” in *CVPR 2021*, pp. 8575–8584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] K. Iskakov, E. Burkov, V. S. Lempitsky, and Y. Malkov, “Learnable triangulation
    of human pose,” in *ICCV 2019*, pp. 7717–7726.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Z. Zhang, C. Wang, W. Qiu, W. Qin, and W. Zeng, “AdaFuse: Adaptive multiview
    fusion for accurate human pose estimation in the wild,” *International Journal
    of Computer Vision*, pp. 1–16, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. He, R. Yan, K. Fragkiadaki, and S. Yu, “Epipolar transformers,” in
    *CVPR 2020*, pp. 7776–7785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] H. Qiu, C. Wang, J. Wang, N. Wang, and W. Zeng, “Cross view fusion for
    3D human pose estimation,” in *ICCV 2019*.   IEEE, pp. 4341–4350.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] S. Wu, S. Jin, W. Liu, L. Bai, C. Qian, D. Liu, and W. Ouyang, “Graph-based
    3D multi-person pose estimation using multi-view images,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 11 148–11 157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] M. Trumble, A. Gilbert, C. Malleson, A. Hilton, and J. P. Collomosse,
    “Total capture: 3D human pose estimation fusing video and inertial sensors.” in
    *BMVC*, vol. 2, no. 5, 2017, pp. 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Gilbert, M. Trumble, C. Malleson, A. Hilton, and J. Collomosse, “Fusing
    visual and inertial sensors with semantics for 3D human pose estimation,” *International
    Journal of Computer Vision*, vol. 127, no. 4, pp. 381–397, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Zhang, C. Wang, W. Qin, and W. Zeng, “Fusing wearable IMUs with multi-view
    images for human pose estimation: A geometric approach,” in *CVPR 2020*, pp. 2200–2209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] C. Malleson, A. Gilbert, M. Trumble, J. Collomosse, A. Hilton, and M. Volino,
    “Real-time full-body motion capture from video and IMUs,” in *2017 International
    Conference on 3D Vision (3DV)*.   IEEE, 2017, pp. 449–457.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] F. Huang, A. Zeng, M. Liu, Q. Lai, and Q. Xu, “DeepFuse: An imu-aware
    network for real-time 3D human pose estimation from multi-view image,” in *The
    IEEE Winter Conference on Applications of Computer Vision*, 2020, pp. 429–438.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, “Deep ordinal regression
    network for monocular depth estimation,” in *CVPR 2018*, pp. 2002–2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] H. Rhodin, M. Salzmann, and P. Fua, “Unsupervised geometry-aware representation
    for 3D human pose estimation,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 750–767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] D. C. Luvizon, D. Picard, and H. Tabia, “2D/3D pose estimation and action
    recognition using multitask deep learning,” in *CVPR 2018*, pp. 5137–5146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis, “Coarse-to-fine
    volumetric prediction for single-image 3D human pose,” in *CVPR 2017*, pp. 7025–7034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] A. Nibali, Z. He, S. Morgan, and L. Prendergast, “3D human pose estimation
    with 2D marginal heatmaps,” in *2019 IEEE Winter Conference on Applications of
    Computer Vision (WACV)*.   IEEE, 2019, pp. 1477–1485.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Fabbri, F. Lanzi, S. Calderara, S. Alletto, and R. Cucchiara, “Compressed
    volumetric heatmaps for multi-person 3D pose estimation,” in *CVPR 2020*, pp.
    7204–7213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding, “3D human
    pose estimation with spatial and temporal transformers,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 11 656–11 665.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli, “3D human pose
    estimation in video with temporal convolutions and semi-supervised training,”
    in *CVPR 2019*, pp. 7753–7762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] G. Rogez, P. Weinzaepfel, and C. Schmid, “LCR-NET++: Multi-person 2D
    and 3D pose detection in natural images,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 42, no. 5, pp. 1146–1161, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] A. Zanfir, E. Marinoiu, M. Zanfir, A.-I. Popa, and C. Sminchisescu, “Deep
    network for the integrated 3D sensing of multiple people in natural images,” *Advances
    in Neural Information Processing Systems*, vol. 31, pp. 8410–8419, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] A. Harltey and A. Zisserman, *Multiple view geometry in computer vision
    (2\. ed.)*.   Cambridge University Press, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Dong, W. Jiang, Q. Huang, H. Bao, and X. Zhou, “Fast and robust multi-person
    3D pose estimation from multiple views,” in *CVPR 2019*, pp. 7792–7801.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. Chen, H. Ai, R. Chen, Z. Zhuang, and S. Liu, “Cross-view tracking
    for multi-human 3D pose estimation at over 100 FPS,” in *CVPR 2020*, pp. 3279–3288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] C. Huang, S. Jiang, Y. Li, Z. Zhang, J. Traish, C. Deng, S. Ferguson,
    and R. Y. Da Xu, “End-to-end dynamic matching network for multi-view multi-person
    3D pose estimation,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 477–493.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] A. Elmi, D. Mazzini, and P. Tortella, “Light3DPose: Real-time multi-person
    3D pose estimation from multiple views,” in *2020 25th International Conference
    on Pattern Recognition (ICPR)*.   IEEE, 2021, pp. 2755–2762.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] M. Zhao, T. Li, M. Abu Alsheikh, Y. Tian, H. Zhao, A. Torralba, and D. Katabi,
    “Through-wall human pose estimation using radio signals,” in *CVPR 2018*, pp.
    7356–7365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. J. Marin-Jimenez, F. J. Romero-Ramirez, R. Muñoz-Salinas, and R. Medina-Carnicer,
    “3D human pose estimation from depth maps using a deep combination of poses,”
    *Journal of Visual Communication and Image Representation*, vol. 55, pp. 627–639,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] S. Kim, K. Yun, J. Park, and J. Y. Choi, “Skeleton-based action recognition
    of people handling objects,” in *2019 IEEE Winter Conference on Applications of
    Computer Vision (WACV)*.   IEEE, 2019, pp. 61–70.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] C.-Y. Weng, B. Curless, and I. Kemelmacher-Shlizerman, “Photo wake-up:
    3D character animation from a single photo,” in *CVPR 2019*, pp. 5908–5917.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Y. Su and Z. Liu, “Position detection for badminton tactical analysis
    based on multi-person pose estimation,” in *2018 14th International Conference
    on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)*.   IEEE,
    2018, pp. 379–383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Li, C. Wang, Y. Cao, B. Liu, J. Tan, and Y. Luo, “Human pose estimation
    based in-home lower body rehabilitation system,” in *2020 International Joint
    Conference on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] R. Rabbito, “Using deep learning-based pose estimation algorithms for
    markerless gait analysis in rehabilitation medicine,” Master’s thesis, Politecnico
    di Torino, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Y. Hbali, S. Hbali, L. Ballihi, and M. Sadgal, “Skeleton-based human
    activity recognition for elderly monitoring systems,” *IET Computer Vision*, vol. 12,
    no. 1, pp. 16–26, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] T.-H. Tsai and C.-W. Hsu, “Implementation of fall detection system based
    on 3D skeleton for deep learning technique,” *IEEE Access*, vol. 7, pp. 153 049–153 059,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] L. Kumarapu and P. Mukherjee, “Animepose: Multi-person 3d pose estimation
    and animation,” *Pattern Recognition Letters*, vol. 147, pp. 16–24, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] A. Elaoud, W. Barhoumi, E. Zagrouba, and B. Agrebi, “Skeleton-based comparison
    of throwing motion for handball players,” *Journal of Ambient Intelligence and
    Humanized Computing*, vol. 11, no. 1, pp. 419–431, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] A. Kamel, B. Liu, P. Li, and B. Sheng, “An investigation of 3D human
    pose estimation for learning tai chi: A human factor perspective,” *International
    Journal of Human–Computer Interaction*, vol. 35, no. 4-5, pp. 427–439, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] P. Gabriel, W. K. Doyle, O. Devinsky, D. Friedman, T. Thesen, and V. Gilja,
    “Neural correlates to automatic behavior estimations from RGB-D video in epilepsy
    unit,” in *2016 38th Annual International Conference of the IEEE Engineering in
    Medicine and Biology Society*.   IEEE, 2016, pp. 3402–3405.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Š. Obdržálek, G. Kurillo, F. Ofli, R. Bajcsy, E. Seto, H. Jimison, and
    M. Pavel, “Accuracy and robustness of kinect pose estimation in the context of
    coaching of elderly population,” in *2012 Annual International Conference of the
    IEEE Engineering in Medicine and Biology Society*.   IEEE, 2012, pp. 1188–1193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] B. Ren, M. Liu, R. Ding, and H. Liu, “A survey on 3D skeleton-based action
    recognition using learning method,” *arXiv preprint arXiv:2002.05907*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C. Kot, “NTU
    RGB+ D 120: A large-scale benchmark for 3D human activity understanding,” *IEEE
    transactions on pattern analysis and machine intelligence*, vol. 42, no. 10, pp.
    2684–2701, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] B. Li, H. Chen, Y. Chen, Y. Dai, and M. He, “Skeleton boxes: Solving
    skeleton based action detection with a single deep convolutional neural network,”
    in *2017 IEEE International Conference on Multimedia & Expo Workshops*.   IEEE,
    2017, pp. 613–616.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] H. Duan, Y. Zhao, K. Chen, D. Lin, and B. Dai, “Revisiting skeleton-based
    action recognition,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 2969–2978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. Das, S. Sharma, R. Dai, F. Bremond, and M. Thonnat, “VPN: Learning
    video-pose embedding for activities of daily living,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 72–90.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] H. Guo, Y. Yu, Q. Ding, and M. Skitmore, “Image-and-skeleton-based parameterized
    approach to real-time identification of construction workers’ unsafe behaviors,”
    *Journal of Construction Engineering and Management*, vol. 144, no. 6, p. 04018042,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] G. Lan, Z. Luo, and Q. Hao, “Development of a virtual reality teleconference
    system using distributed depth sensors,” in *2016 2nd IEEE International Conference
    on Computer and Communications (ICCC)*.   IEEE, 2016, pp. 975–978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] M. C. Thar, K. Z. N. Winn, and N. Funabiki, “A proposal of yoga pose
    assessment method using pose detection for self-learning,” in *2019 International
    Conference on Advanced Information Technologies (ICAIT)*.   IEEE, 2019, pp. 137–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] R. Afrouzian, H. Seyedarabi, and S. Kasaei, “Pose estimation of soccer
    players using multiple uncalibrated cameras,” *Multimedia Tools and Applications*,
    vol. 75, no. 12, pp. 6809–6827, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] L. Bridgeman, M. Volino, J.-Y. Guillemaut, and A. Hilton, “Multi-person
    3D pose estimation and tracking in sports,” in *CVPR Workshops 2019*, pp. 2487–2496.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] R. Kurose, M. Hayashi, T. Ishii, and Y. Aoki, “Player pose analysis in
    tennis video based on pose estimation,” in *2018 International Workshop on Advanced
    Image Technology (IWAIT)*.   IEEE, 2018, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] E. Wu and H. Koike, “FuturePong: Real-time table tennis trajectory forecasting
    using pose prediction network,” in *Extended Abstracts of the 2020 CHI Conference
    on Human Factors in Computing Systems*, 2020, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Martín-Martín, E. Orduna-Malea, M. Thelwall, and E. D. López-Cózar,
    “Google scholar, web of science, and scopus: A systematic comparison of citations
    in 252 subject categories,” *Journal of Informetrics*, vol. 12, no. 4, pp. 1160–1177,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] M. E. Rose and J. R. Kitchin, “Pybliometrics: Scriptable bibliometrics
    using a python interface to scopus,” *SoftwareX*, vol. 10, p. 100263, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] L. Xu, Y. Guan, S. Jin, W. Liu, C. Qian, P. Luo, W. Ouyang, and X. Wang,
    “ViPNAS: Efficient video pose estimation via neural architecture search,” in *CVPR
    2021*, pp. 16 072–16 081.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang, “HigherHRNet:
    Scale-aware representation learning for bottom-up human pose estimation,” in *CVPR
    2020*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C. Doersch and A. Zisserman, “Sim2real transfer learning for 3D human
    pose estimation: motion to the rescue,” in *Advances in Neural Information Processing
    Systems*, 2019, pp. 12 949–12 961.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Li, S. Bian, A. Zeng, C. Wang, B. Pang, W. Liu, and C. Lu, “Human
    pose regression with residual log-likelihood estimation,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 11 025–11 034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Huang, Z. Zhu, F. Guo, and G. Huang, “The devil is in the details:
    Delving into unbiased data processing for human pose estimation,” in *CVPR 2020*,
    pp. 5700–5709.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] C. Yu, B. Xiao, C. Gao, L. Yuan, L. Zhang, N. Sang, and J. Wang, “Lite-HRNet:
    A lightweight high-resolution network,” in *CVPR 2021*, pp. 10 440–10 450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] D. Osokin, “Real-time 2D multi-person pose estimation on CPU: Lightweight
    OpenPose,” *arXiv preprint arXiv:1811.12004*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Z. Liu, H. Chen, R. Feng, S. Wu, S. Ji, B. Yang, and X. Wang, “Deep dual
    consecutive network for human pose estimation,” in *CVPR 2021*, pp. 525–534.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Zhen, Q. Fang, J. Sun, W. Liu, W. Jiang, H. Bao, and X. Zhou, “Smap:
    Single-shot multi-person absolute 3D pose estimation,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 550–566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] P. Weinzaepfel, R. Brégier, H. Combaluzier, V. Leroy, and G. Rogez, “Dope:
    Distillation of part experts for whole-body 3D pose estimation in the wild,” in
    *European Conference on Computer Vision*.   Springer, 2020, pp. 380–397.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-P. Seidel,
    W. Xu, D. Casas, and C. Theobalt, “VNect: Real-time 3D human pose estimation with
    a single RGB camera,” *ACM Transactions on Graphics (TOG)*, vol. 36, no. 4, pp.
    1–14, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] I. Sárándi, T. Linder, K. O. Arras, and B. Leibe, “Synthetic occlusion
    augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on
    3D human pose estimation,” *arXiv preprint arXiv:1809.04987*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] X. Sun, B. Xiao, F. Wei, S. Liang, and Y. Wei, “Integral human pose regression,”
    in *ECCV 2018*, pp. 529–545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Y. Cheng, B. Wang, B. Yang, and R. T. Tan, “Monocular 3d multi-person
    pose estimation by integrating top-down and bottom-up networks,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 7649–7659.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] T. Wehrbein, M. Rudolph, B. Rosenhahn, and B. Wandt, “Probabilistic monocular
    3D human pose estimation with normalizing flows,” *arXiv preprint arXiv:2107.13788*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
