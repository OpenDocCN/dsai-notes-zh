- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 20:01:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:01:16'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2005.00935] Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2005.00935] 深度强化学习在智能交通系统中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2005.00935](https://ar5iv.labs.arxiv.org/html/2005.00935)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2005.00935](https://ar5iv.labs.arxiv.org/html/2005.00935)
- en: 'Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在智能交通系统中的应用：综述
- en: Ammar Haydari Yasin Yilmaz
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ammar Haydari Yasin Yilmaz
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Latest technological improvements increased the quality of transportation. New
    data-driven approaches bring out a new research direction for all control-based
    systems, e.g., in transportation, robotics, IoT and power systems. Combining data-driven
    applications with transportation systems plays a key role in recent transportation
    applications. In this paper, the latest deep reinforcement learning (RL) based
    traffic control applications are surveyed. Specifically, traffic signal control
    (TSC) applications based on (deep) RL, which have been studied extensively in
    the literature, are discussed in detail. Different problem formulations, RL parameters,
    and simulation environments for TSC are discussed comprehensively. In the literature,
    there are also several autonomous driving applications studied with deep RL models.
    Our survey extensively summarizes existing works in this field by categorizing
    them with respect to application types, control models and studied algorithms.
    In the end, we discuss the challenges and open questions regarding deep RL-based
    transportation applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的技术进步提升了交通质量。新的数据驱动方法为所有基于控制的系统带来了新的研究方向，例如交通、机器人、物联网和电力系统。将数据驱动应用与交通系统结合在最近的交通应用中发挥了关键作用。本文综述了基于最新深度强化学习（RL）的交通控制应用。具体而言，详细讨论了基于（深度）RL的交通信号控制（TSC）应用，这些应用在文献中得到了广泛研究。综合讨论了TSC的不同问题表述、RL参数和仿真环境。文献中也有几个使用深度RL模型研究的自动驾驶应用。我们的综述通过根据应用类型、控制模型和研究算法对现有工作进行分类，广泛总结了该领域的现有成果。最后，我们讨论了深度RL基础的交通应用面临的挑战和未解问题。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep reinforcement learning, Intelligent transportation systems, Traffic signal
    control, Autonomous driving, Multi-agent systems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习，智能交通系统，交通信号控制，自动驾驶，多智能体系统。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: With increasing urbanization and latest advances in autonomous technologies,
    transportation studies evolved to more intelligent systems, called intelligent
    transportation systems (ITS). Artificial intelligence (AI) tries to control systems
    with minimal human intervention. Combination of ITS and AI provides effective
    solutions for the 21st century transportation studies. The main goal of ITS is
    providing safe, effective and reliable transportation systems to participants.
    For this purpose, optimal traffic signal control (TSC), autonomous vehicle control,
    traffic flow control are some of the key research areas.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着城市化进程的加快和自动化技术的最新进展，交通研究发展为更智能的系统，即智能交通系统（ITS）。人工智能（AI）试图通过最小化人工干预来控制系统。ITS和AI的结合为21世纪的交通研究提供了有效的解决方案。ITS的主要目标是为参与者提供安全、有效和可靠的交通系统。为此，最优的交通信号控制（TSC）、自动驾驶控制和交通流量控制是一些关键的研究领域。
- en: The future transportation systems are expected to include full autonomy such
    as autonomous traffic management and autonomous driving. Even now, semi-autonomous
    vehicles occupy the roads and the level of autonomy is likely to increase in near
    future. There are several reasons why authorities want autonomy in ITS such as
    time saving for drivers, energy saving for environment, and safety for all participants.
    Travel time savings can be provided by coordinated and connected traffic systems
    that can be controlled more efficiently using self-autonomous systems. When vehicles
    spend more times on traffic, fuel consumption increases, which has environmental
    and economic impacts. Another reason why human intervention is tried to be minimized
    is the unpredictable nature of human behavior. It is expected that autonomous
    driving will decrease traffic accidents and increase the quality of transportation.
    For all the reasons stated above, there is a high demand on various aspects of
    autonomous controls in ITS. One popular approach is to use experience-based learning
    models, similar to human learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的交通系统预计将包括完全自主的功能，例如自主交通管理和自主驾驶。即使在现在，半自主车辆已经占据了道路，未来自主级别有可能会进一步提高。权威机构希望交通智能系统（ITS）实现自主控制的原因有很多，比如为驾驶员节省时间、为环境节能和提高所有参与者的安全性。协调和连接的交通系统可以通过自我自主系统更高效地控制，从而提供旅行时间的节省。当车辆在交通中花费更多时间时，燃油消耗增加，这对环境和经济有影响。另一个希望最小化人工干预的原因是人类行为的不可预测性。预计自主驾驶将减少交通事故，提高运输质量。基于以上所有原因，对ITS中各种自主控制的需求非常高。一种流行的方法是使用基于经验的学习模型，类似于人类学习。
- en: Growing population in urban areas causes a high volume of traffic, supported
    by the fact that the annual congestion cost for a driver in the US was 97 hours
    and $1,348 in 2018 [[1](#bib.bib1)]. Hence, controlling traffic lights with adaptive
    modules is a recent research focus in ITS. Designing an adaptive traffic management
    system through traffic signals is an effective solution for reducing the traffic
    congestion. The best approach for optimizing traffic lights is still an open question
    for researchers, but one promising approach for optimum TSC is to use learning-based
    AI techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 城市区域人口增长导致交通量大，同时2018年美国驾驶员的年拥堵成本为97小时和$1,348 [[1](#bib.bib1)]。因此，使用自适应模块控制交通信号灯是当前ITS研究的重点。通过交通信号灯设计自适应交通管理系统是减少交通拥堵的有效解决方案。优化交通信号灯的最佳方法仍然是一个未解的问题，但一种有前景的优化交通信号控制（TSC）方法是使用基于学习的人工智能技术。
- en: There are three main machine learning paradigms. Supervised learning makes decision
    based on the output labels provided in training. Unsupervised learning works based
    on pattern discovery without having the pre-knowledge of output labels. The third
    machine learning paradigm is reinforcement learning (RL), which takes sequential
    actions rooted in Markov Decision Process (MDP) with a rewarding or penalizing
    criterion. RL combined with deep learning, named deep RL, is currently accepted
    as the state-of-the art learning framework in control systems. While RL can solve
    complex control problems, deep learning helps to approximate highly nonlinear
    functions from complex dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习主要有三种范式。监督学习基于训练中提供的输出标签做出决策。无监督学习则在没有预先知道输出标签的情况下，基于模式发现进行工作。第三种机器学习范式是强化学习（RL），它在马尔可夫决策过程（MDP）中进行顺序行动，并依据奖励或惩罚标准来进行。强化学习与深度学习结合，称为深度强化学习（deep
    RL），目前被接受为控制系统中的最先进学习框架。虽然强化学习可以解决复杂的控制问题，但深度学习有助于从复杂数据集中逼近高度非线性函数。
- en: Recently, many deep RL based solution methods are presented for different ITS
    applications. There is an increasing interest on RL based control mechanisms in
    ITS applications such as traffic management systems and autonomous driving applications.
    Gathering all the data-driven ITS studies related to deep RL and discussing such
    applications together in a paper is needed for informing ITS researchers on deep
    RL, as well as deep RL researchers on ITS.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多基于深度强化学习的解决方法被提出用于不同的ITS应用。对基于RL的控制机制在ITS应用中的兴趣不断增加，例如交通管理系统和自主驾驶应用。将所有与深度RL相关的数据驱动ITS研究汇总，并在论文中讨论这些应用，对于向ITS研究人员提供深度RL的信息，以及向深度RL研究人员提供ITS的信息是必要的。
- en: In this paper, we review the deep RL applications proposed for ITS problems,
    predominantly for TSC. Different RL approaches from the literature are discussed.
    TSC solutions based on standard RL techniques have already been studied before
    the invention of deep RL. Hence, we believe standard RL techniques also have high
    importance in reviewing the deep RL solutions for ITS, in particular TSC. Since
    traffic intersection models are mainly connected and distributed, multi-agent
    dynamic control techniques, which are also extensively covered in this survey,
    play a key role in RL-based ITS applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了为ITS问题提出的深度RL应用，主要是交通信号控制（TSC）。讨论了文献中不同的RL方法。基于标准RL技术的TSC解决方案在深度RL发明之前已经有过研究。因此，我们认为标准RL技术在审视ITS，特别是TSC的深度RL解决方案时也具有重要意义。由于交通交叉口模型主要是连接的和分布式的，多智能体动态控制技术在RL基础的ITS应用中也发挥着关键作用，这些技术在本综述中也得到了广泛讨论。
- en: I-A Contributions
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 贡献
- en: This paper presents a comprehensive survey on deep RL applications for ITS by
    discussing a theoretical overview of deep RL, different problem formulations for
    TSC, various deep RL applications for TSC and other ITS topics, and finally challenges
    with future research directions. The targeted audience are the ITS researchers
    who want to have a jump start in learning deep RL techniques, and also deep RL
    researchers who are interested in ITS applications. We also believe that this
    survey will serve as “a compact handbook of deep RL in ITS” for more experienced
    researchers to review the existing methods and open challenges. Our contributions
    can be summarized as follows.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过讨论深度RL的理论概述、TSC的不同问题表述、TSC和其他ITS主题的各种深度RL应用以及未来研究方向中的挑战，提供了关于深度RL在ITS中应用的全面综述。目标读者是那些希望迅速了解深度RL技术的ITS研究人员，以及对ITS应用感兴趣的深度RL研究人员。我们还相信，这项综述将作为“深度RL在ITS中的紧凑手册”供更有经验的研究人员回顾现有方法和开放挑战。我们的贡献可以总结如下。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The first comprehensive survey of RL and deep RL based applications in ITS is
    presented.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文呈现了关于在ITS中基于RL和深度RL应用的第一次全面调查。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: From a broad concept, theoretical background of RL and deep RL models, especially
    those which are used in the ITS literature, are explained.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从广义上讲，解释了强化学习和深度强化学习模型的理论背景，特别是那些在智能交通系统（ITS）文献中使用的模型。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Existing works in TSC that use RL and deep RL are discussed and clearly summarized
    in tables for appropriate comparisons.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有的交通信号控制（TSC）领域中使用强化学习（RL）和深度强化学习（deep RL）的研究工作进行了讨论，并在表格中清晰总结，以便进行适当的比较。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Similarly, different deep RL applications in other ITS areas, such as autonomous
    driving, are presented and summarized in a table for comparison.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，其他ITS领域如自动驾驶中的不同深度RL应用也进行了展示，并在表格中总结以便比较。
- en: I-B Organization
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 组织结构
- en: The paper organized as follows.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 论文组织如下。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [II](#S2 "II Related Work ‣ Deep Reinforcement Learning for Intelligent
    Transportation Systems: A Survey"): Related Work'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[II](#S2 "II 相关工作 ‣ 深度强化学习在智能交通系统中的应用：综述")节：相关工作
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [III](#S3 "III Deep RL: An Overview ‣ Deep Reinforcement Learning for
    Intelligent Transportation Systems: A Survey"): Deep RL: An Overview'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[III](#S3 "III 深度强化学习概述 ‣ 深度强化学习在智能交通系统中的应用：综述")节：深度强化学习概述
- en: –
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [III-A](#S3.SS1 "III-A Reinforcement Learning ‣ III Deep RL: An Overview
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey"):
    Reinforcement Learning'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[III-A](#S3.SS1 "III-A 强化学习 ‣ III 深度强化学习概述 ‣ 深度强化学习在智能交通系统中的应用：综述")节：强化学习
- en: –
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [III-B](#S3.SS2 "III-B Deep Reinforcement Learning ‣ III Deep RL: An
    Overview ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey"): Deep Reinforcement Learning'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[III-B](#S3.SS2 "III-B 深度强化学习 ‣ III 深度强化学习概述 ‣ 深度强化学习在智能交通系统中的应用：综述")节：深度强化学习
- en: –
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [III-C](#S3.SS3 "III-C Summary of Deep RL ‣ III Deep RL: An Overview
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey"):
    Summary of Deep RL'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[III-C](#S3.SS3 "III-C 深度强化学习总结 ‣ III 深度强化学习概述 ‣ 深度强化学习在智能交通系统中的应用：综述")节：深度强化学习总结
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [IV](#S4 "IV Deep RL Settings for TSC ‣ Deep Reinforcement Learning
    for Intelligent Transportation Systems: A Survey"): Deep RL Settings for TSC'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第[IV](#S4 "IV TSC中的深度RL设置 ‣ 深度强化学习在智能交通系统中的应用：综述")节：TSC中的深度RL设置
- en: –
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [IV-A](#S4.SS1 "IV-A State ‣ IV Deep RL Settings for TSC ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey"): State'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [IV-A](#S4.SS1 "IV-A 状态 ‣ IV 深度 RL 设置用于 TSC ‣ 智能交通系统的深度强化学习：综述")：状态
- en: –
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [IV-B](#S4.SS2 "IV-B Action ‣ IV Deep RL Settings for TSC ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey"): Action'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [IV-B](#S4.SS2 "IV-B 行动 ‣ IV 深度 RL 设置用于 TSC ‣ 智能交通系统的深度强化学习：综述")：行动
- en: –
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [IV-C](#S4.SS3 "IV-C Reward ‣ IV Deep RL Settings for TSC ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey"): Reward'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [IV-C](#S4.SS3 "IV-C 奖励 ‣ IV 深度 RL 设置用于 TSC ‣ 智能交通系统的深度强化学习：综述")：奖励
- en: –
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [IV-D](#S4.SS4 "IV-D Neural Network Structure ‣ IV Deep RL Settings
    for TSC ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey"): Neural Network Structure'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [IV-D](#S4.SS4 "IV-D 神经网络结构 ‣ IV 深度 RL 设置用于 TSC ‣ 智能交通系统的深度强化学习：综述")：神经网络结构
- en: –
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [IV-E](#S4.SS5 "IV-E Simulation environments ‣ IV Deep RL Settings
    for TSC ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey"): Simulation Environments'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [IV-E](#S4.SS5 "IV-E 仿真环境 ‣ IV 深度 RL 设置用于 TSC ‣ 智能交通系统的深度强化学习：综述")：仿真环境
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [V](#S5 "V Deep RL Applications for TSC ‣ Deep Reinforcement Learning
    for Intelligent Transportation Systems: A Survey"): Deep RL Applications for TSC'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [V](#S5 "V 深度 RL 应用于 TSC ‣ 智能交通系统的深度强化学习：综述")：深度 RL 应用于 TSC
- en: –
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [V-A](#S5.SS1 "V-A Standard RL applications ‣ V Deep RL Applications
    for TSC ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey"): Standard RL Applications'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [V-A](#S5.SS1 "V-A 标准 RL 应用 ‣ V 深度 RL 应用于 TSC ‣ 智能交通系统的深度强化学习：综述")：标准 RL
    应用
- en: –
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [V-B](#S5.SS2 "V-B Deep RL applications ‣ V Deep RL Applications for
    TSC ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey"):
    Deep RL Applications'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [V-B](#S5.SS2 "V-B 深度 RL 应用 ‣ V 深度 RL 应用于 TSC ‣ 智能交通系统的深度强化学习：综述")：深度 RL
    应用
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [VI](#S6 "VI Deep RL for Other ITS Applications ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey"): Deep RL for Other
    ITS Applications'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [VI](#S6 "VI 深度 RL 用于其他 ITS 应用 ‣ 智能交通系统的深度强化学习：综述")：深度 RL 用于其他 ITS 应用
- en: –
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [VI-A](#S6.SS1 "VI-A Autonomous Driving ‣ VI Deep RL for Other ITS
    Applications ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey"): Autonomous Driving'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [VI-A](#S6.SS1 "VI-A 自动驾驶 ‣ VI 深度 RL 用于其他 ITS 应用 ‣ 智能交通系统的深度强化学习：综述")：自动驾驶
- en: –
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [VI-B](#S6.SS2 "VI-B Energy Management ‣ VI Deep RL for Other ITS Applications
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey"):
    Energy Management'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [VI-B](#S6.SS2 "VI-B 能源管理 ‣ VI 深度 RL 用于其他 ITS 应用 ‣ 智能交通系统的深度强化学习：综述")：能源管理
- en: –
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [VI-C](#S6.SS3 "VI-C Road Control ‣ VI Deep RL for Other ITS Applications
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey"):
    Road Control'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [VI-C](#S6.SS3 "VI-C 道路控制 ‣ VI 深度 RL 用于其他 ITS 应用 ‣ 智能交通系统的深度强化学习：综述")：道路控制
- en: –
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [VI-D](#S6.SS4 "VI-D Various ITS Applications ‣ VI Deep RL for Other
    ITS Applications ‣ Deep Reinforcement Learning for Intelligent Transportation
    Systems: A Survey"): Various ITS Applications'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [VI-D](#S6.SS4 "VI-D 各种 ITS 应用 ‣ VI 深度 RL 用于其他 ITS 应用 ‣ 智能交通系统的深度强化学习：综述")：各种
    ITS 应用
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [VII](#S7 "VII Challenges and Open Research questions ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey"): Challenges and Open
    Research Questions'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分 [VII](#S7 "VII 挑战与开放研究问题 ‣ 智能交通系统的深度强化学习：综述")：挑战与开放研究问题
- en: II Related Work
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: The earliest work summarizing AI models for TSC including RL and other approaches
    dates back to 2007 [[2](#bib.bib2)]. At that time, fuzzy logic, artificial neural
    networks and RL was three main popular AI methods researchers applied on TSC.
    Due to the connectedness of ITS components, such as intersections, multi-agent
    models provide a more complete and realistic solution than single-agent models.
    Hence, formulating the TSC problem as a multi-agent system has a high research
    potential. The opportunities and research directions of multi-agent RL for TSC
    is studied in [[3](#bib.bib3)]. [[4](#bib.bib4)] discusses the popular RL methods
    in the literature from an experimental perspective. Another comprehensive TSC
    survey for RL methods is presented in [[5](#bib.bib5)]. A recent survey presented
    in [[6](#bib.bib6)] studies AI methods in ITS from a broad perspective. It considers
    applications of supervised learning, unsupervised learning and RL for vehicle
    to everything communications.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最早总结用于交通信号控制（TSC）的AI模型，包括强化学习（RL）和其他方法的工作可以追溯到2007年[[2](#bib.bib2)]。当时，模糊逻辑、人工神经网络和强化学习是研究人员应用于交通信号控制的三种主要流行AI方法。由于智能交通系统（ITS）组件之间的关联性，例如交叉口，多智能体模型比单智能体模型提供了更完整和现实的解决方案。因此，将交通信号控制问题表述为多智能体系统具有很高的研究潜力。多智能体强化学习在交通信号控制中的机会和研究方向在[[3](#bib.bib3)]中进行了研究。[[4](#bib.bib4)]
    从实验角度讨论了文献中流行的强化学习方法。另一个全面的强化学习方法的交通信号控制调查在[[5](#bib.bib5)]中提出。最近在[[6](#bib.bib6)]中提出的调查从广泛的角度研究了智能交通系统中的AI方法。它考虑了监督学习、无监督学习和强化学习在车对车通信中的应用。
- en: 'Abduljabbar et al. summarizes the literature of AI based transportation applications
    in [[7](#bib.bib7)] with three main topics: transportation management applications,
    public transportation, and autonomous vehicles. In [[8](#bib.bib8)], authors discuss
    the TSC methods in general, including classical control methods, actuated control,
    green-wave, max-band systems, and RL based control methods. Veres et al. highlights
    the trends and challenges of deep learning applications in ITS [[9](#bib.bib9)].
    Deep learning models play a significant role in deep RL. Nonlinear neural networks
    overcome traditional challenges such as scalability in the data-driven ITS applications.
    Lately, a survey of deep RL applications for autonomous vehicles is presented
    in [[10](#bib.bib10)], where authors discuss recent works with the challenges
    of real-world deployment of such RL-based autonomous driving methods. In addition
    to autonomous driving, in this survey we discuss a broad class of ITS applications
    where deep RL is gaining popularity, together with a comprehensive overview of
    the deep RL concept.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Abduljabbar等人在[[7](#bib.bib7)]中总结了基于AI的交通应用文献，涉及三个主要主题：交通管理应用、公共交通和自动驾驶车辆。在[[8](#bib.bib8)]中，作者一般性地讨论了交通信号控制方法，包括经典控制方法、激励控制、绿波、最大带宽系统和基于强化学习的控制方法。Veres等人突出介绍了深度学习应用于智能交通系统的趋势和挑战[[9](#bib.bib9)]。深度学习模型在深度强化学习中发挥了重要作用。非线性神经网络克服了传统的数据驱动智能交通应用中的挑战，如可扩展性。最近，在[[10](#bib.bib10)]中提出了针对自动驾驶车辆的深度强化学习应用调查，作者讨论了近期的工作及其在现实世界部署中的挑战。除了自动驾驶，本调查还讨论了深度强化学习在智能交通系统中越来越受欢迎的广泛应用类，以及对深度强化学习概念的全面概述。
- en: There is no survey in the literature dedicated to the deep RL applications for
    ITS, which we believe is a very timely topic in the ITS research. Thus, this paper
    will fill an important gap for ITS researchers and deep RL researchers interested
    in ITS.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中没有专门针对智能交通系统深度强化学习应用的调查，我们认为这是智能交通系统研究中的一个非常及时的话题。因此，本文将填补智能交通系统研究人员和对智能交通系统感兴趣的深度强化学习研究人员之间的重要空白。
- en: 'III Deep RL: An Overview'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度强化学习：概述
- en: Deep RL is one of the most successful AI models and the closest machine learning
    paradigm to human learning. It combines deep neural networks and RL for more efficient
    and stabilized function approximations especially for high-dimensional and infinite-state
    problems. This section describes the theoretical background of traditional RL
    and major deep RL algorithms implemented in ITS applications.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习是最成功的AI模型之一，也是与人类学习最接近的机器学习范式。它结合了深度神经网络和强化学习，以更高效和稳定的方式进行函数逼近，特别适用于高维和无限状态问题。本节描述了传统强化学习的理论背景以及在智能交通系统应用中实施的主要深度强化学习算法。
- en: III-A Reinforcement Learning
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 强化学习
- en: RL is a general learning tool where an agent interacts with the environment
    to learn how to behave in an environment without having any prior knowledge by
    learning to maximize a numerically defined reward (or to minimize a penalty).
    After taking an action, RL agent receives a feedback from the environment at each
    time step $t$ about the performance of its action. Using this feedback (reward
    or penalty) it iteratively updates its action policy to reach to an optimum control
    policy. RL learns from experiences with the environment, exhibiting a trial-and-error
    kind of learning, similar to human learning [[11](#bib.bib11)]. The fundamental
    trade-off between exploration and exploitation in RL strikes a balance between
    new actions and learned actions. From a computational perspective, RL is a data-driven
    approach which iteratively computes an approximate solution to the optimum control
    policy. Hence, it is also known as approximate dynamic programming [[11](#bib.bib11)]
    which is one type of sequential optimization problem for dynamic programming (DP).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种通用的学习工具，其中一个智能体通过与环境互动来学习如何在环境中行为，前提是没有任何先验知识，智能体通过学习最大化一个数值定义的奖励（或最小化惩罚）来达到这一点。在采取行动后，强化学习智能体会在每个时间步
    $t$ 从环境中获得关于其行动表现的反馈。利用这些反馈（奖励或惩罚），智能体迭代地更新其行动策略，以达到最佳控制策略。强化学习通过与环境的经验学习，表现出类似于人类学习的试错学习方式
    [[11](#bib.bib11)]。强化学习中的探索与利用之间的基本权衡在于新动作和已学动作之间的平衡。从计算的角度来看，强化学习是一种数据驱动的方法，它迭代地计算接近最佳控制策略的近似解。因此，它也被称为近似动态规划
    [[11](#bib.bib11)]，这是动态规划（DP）的一种序列优化问题。
- en: 'In a general RL model, an agent controlled with an algorithm, observes the
    system state $s_{t}$ at each time step $t$ and receives a reward $r_{t}$ from
    its environment/system after taking the action $a_{t}$. After taking an action
    based on the current policy $\pi$, the system transitions to the next state $s_{t+1}$.
    After every interaction, RL agent updates its knowledge about the environment.
    Fig [1](#S3.F1 "Figure 1 ‣ III-A Reinforcement Learning ‣ III Deep RL: An Overview
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey")
    depicts the schematic of the RL process.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '在一个通用的强化学习模型中，一个受算法控制的智能体在每个时间步 $t$ 观察系统状态 $s_{t}$，并在采取行动 $a_{t}$ 后从其环境/系统中获得奖励
    $r_{t}$。在根据当前策略 $\pi$ 采取行动后，系统过渡到下一个状态 $s_{t+1}$。在每次交互后，强化学习智能体更新其关于环境的知识。图 [1](#S3.F1
    "Figure 1 ‣ III-A Reinforcement Learning ‣ III Deep RL: An Overview ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey") 描绘了强化学习过程的示意图。'
- en: '![Refer to caption](img/7bac9e0e58f709a700db1b612bf68140.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7bac9e0e58f709a700db1b612bf68140.png)'
- en: 'Figure 1: Reinforcement learning control loop.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：强化学习控制循环。
- en: III-A1 Markov Decision Process
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 马尔可夫决策过程
- en: 'RL methodology formally comes from a Markov Decision Process (MDP), which is
    a general mathematical framework sequential decision making algorithms. MDP consist
    of 5 elements in a tuple:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习方法论正式来源于马尔可夫决策过程（MDP），这是一个用于序列决策算法的通用数学框架。MDP 由一个包含 5 个元素的元组组成：
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A set of states $\mathcal{S}$,
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一组状态 $\mathcal{S}$，
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A set of actions $\mathcal{A}$,
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一组动作 $\mathcal{A}$，
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transition function $\mathcal{T}(s_{t+1}|s_{t},a_{t})$ which maps a state-action
    pair for each time $t$ to a distribution of next state $s_{t+1}$,
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移函数 $\mathcal{T}(s_{t+1}|s_{t},a_{t})$，它将每个时间点 $t$ 的状态-动作对映射到下一个状态 $s_{t+1}$
    的分布，
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reward function $\mathcal{R}(s_{t},a_{t},s_{t+1})$ which gives the reward for
    taking action $a_{t}$ from state $s_{t}$ when transitioning to the next state
    $s_{t+1}$,
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励函数 $\mathcal{R}(s_{t},a_{t},s_{t+1})$，它为从状态 $s_{t}$ 执行动作 $a_{t}$ 过渡到下一个状态
    $s_{t+1}$ 时的奖励提供值，
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discount factor $\gamma$ between 0 and 1 for future rewards.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 折扣因子 $\gamma$ 在 0 和 1 之间，用于未来奖励的计算。
- en: The essential Markov property is that given the current state $s_{t}$, the next
    state $s_{t+1}$ of system is independent from the previous states $(s_{0},s_{1},...,s_{t-1})$.
    In control systems including transportation systems, MDP models are mostly *episodic*
    in which the system has a terminal point for each episode based on the end time
    $T$ or the end state $s_{T}$. The goal of an MDP agent is to find the best policy
    $\pi^{*}$ that maximizes the expected cumulative reward $\mathbb{E}[R_{t}|s,\pi]$
    for each state $s$ and cumulative discounted reward (i.e., return)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的马尔可夫性质是，给定当前状态 $s_{t}$，系统的下一个状态 $s_{t+1}$ 与之前的状态 $(s_{0},s_{1},...,s_{t-1})$
    是独立的。在包括交通系统在内的控制系统中，MDP 模型通常是 *episodic* 的，其中系统根据结束时间 $T$ 或结束状态 $s_{T}$ 为每一集设定了终点。MDP
    代理的目标是找到最优策略 $\pi^{*}$，以最大化每个状态 $s$ 的期望累积奖励 $\mathbb{E}[R_{t}\mid s,\pi]$ 和累积折扣奖励（即回报）。
- en: '|  | $R_{t}=\sum_{i=0}^{T-1}\gamma^{i}r_{t+i},$ |  | (1) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $R_{t}=\sum_{i=0}^{T-1}\gamma^{i}r_{t+i},$ |  | (1) |'
- en: with the discount parameter $\gamma$ which reflects the importance of future
    rewards. Choosing a larger $\gamma$ value between 0 and 1 means that agent’s actions
    have higher dependency on future reward. Whereas, a smaller $\gamma$ value results
    in actions that mostly care about the instantaneous reward $r_{t}$.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用折扣参数 $\gamma$ 来反映未来奖励的重要性。选择更大的 $\gamma$ 值（介于 0 和 1 之间）意味着代理的动作对未来奖励的依赖程度更高。相反，较小的
    $\gamma$ 值会导致动作主要关注瞬时奖励 $r_{t}$。
- en: 'In general, RL agent can act in two ways: (i) by knowing/learning the transition
    probability $\mathcal{T}$ from state $s_{t}$ to $s_{t+1}$, which is called model-based
    RL, (ii) and by exploring the environment without learning a transition model,
    which is called model-free RL. Model-free RL algorithms are also divided into
    two main groups as value-based and policy-based methods. While in value-based
    RL, the agent at each iteration updates a value function that maps each state-action
    pair to a value, in policy-based methods, policy is updated at each iteration
    using policy gradient [[11](#bib.bib11)]. We next explain the value-based and
    policy-based RL methods in detail.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，强化学习（RL）代理可以通过两种方式行动：（i）通过了解/学习从状态 $s_{t}$ 到 $s_{t+1}$ 的转移概率 $\mathcal{T}$，这称为基于模型的强化学习，（ii）通过探索环境而不学习转移模型，这称为无模型强化学习。无模型强化学习算法还分为两大类：基于值的方法和基于策略的方法。在基于值的强化学习中，代理在每次迭代时更新一个将每个状态-动作对映射到一个值的值函数，而在基于策略的方法中，策略在每次迭代时使用策略梯度更新[[11](#bib.bib11)]。接下来我们将详细解释基于值和基于策略的强化学习方法。
- en: III-A2 Value-based RL
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 基于值的强化学习
- en: Value function determines how good a state is for the agent by estimating the
    value (i.e., expected return) of being in a given state $s$ under a policy $\pi$
    as
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数通过估计在给定策略 $\pi$ 下，状态 $s$ 的价值（即期望回报）来确定状态对代理的好坏。
- en: '|  | ${V}^{\pi}(s)=\mathbb{E}[R_{t}&#124;s,\pi].$ |  | (2) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | ${V}^{\pi}(s)=\mathbb{E}[R_{t}\mid s,\pi].$ |  | (2) |'
- en: 'The optimum value function ${V}^{*}(s)$ describes the maximized state value
    function over the policy for all states:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最优值函数 ${V}^{*}(s)$ 描述了在所有状态下，通过策略最大化的状态值函数：
- en: '|  | ${V}^{*}(s)=\max_{\pi}{V}^{\pi}(s),\forall{s}\;\epsilon\;\mathcal{S}.$
    |  | (3) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | ${V}^{*}(s)=\max_{\pi}{V}^{\pi}(s),\forall{s}\;\epsilon\;\mathcal{S}.$
    |  | (3) |'
- en: 'Adding the effect of action, state-action value function named as quality function
    (Q-function) is commonly used to reflect the expected return in a state-action
    pair:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑动作的影响，状态-动作值函数（即质量函数 Q 函数）通常用于反映状态-动作对的期望回报：
- en: '|  | ${Q}^{\pi}(s,a)=\mathbb{E}[R_{t}&#124;s,a,\pi].$ |  | (4) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | ${Q}^{\pi}(s,a)=\mathbb{E}[R_{t}\mid s,a,\pi].$ |  | (4) |'
- en: Optimum action value function (Q-function) is calculated similarly to the optimum
    state value function by maximizing its expected return over all states. Relation
    between the optimum state and action value functions is given by
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最优动作值函数（Q 函数）的计算方式与最优状态值函数类似，通过最大化所有状态的期望回报。最优状态值函数和动作值函数之间的关系由下式给出：
- en: '|  | ${V}^{*}(s)=\max_{a}{Q}^{*}(s,a),\forall{s}\;\epsilon\;\mathcal{S}.$ |  |
    (5) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | ${V}^{*}(s)=\max_{a}{Q}^{*}(s,a),\forall{s}\;\epsilon\;\mathcal{S}.$ |  |
    (5) |'
- en: 'Q-function $Q^{*}(s,a)$ provides the optimum policy $\pi^{*}$ by selecting
    the action $a$ that maximizes the $Q$-value for the state $s$:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Q 函数 $Q^{*}(s,a)$ 通过选择最大化状态 $s$ 的 $Q$ 值的动作 $a$ 来提供最优策略 $\pi^{*}$：
- en: '|  | $\pi^{*}(s)=\operatorname*{argmax}_{a}{Q}^{*}(s,a),\forall{s}\;\epsilon\;\mathcal{S}.$
    |  | (6) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}(s)=\operatorname*{argmax}_{a}{Q}^{*}(s,a),\forall{s}\;\epsilon\;\mathcal{S}.$
    |  | (6) |'
- en: 'Based on the definitions above, there are two main value-based RL algorithms:
    Q-learning [[12](#bib.bib12)] and SARSA [[13](#bib.bib13)], which are classified
    as off-policy RL algorithm, and on-policy RL algorithm, respectively. In both
    algorithms, the values of state-action pairs ($Q$-value) are stored in a $Q$-table,
    and are learned via the recursive nature of Bellman equations utilizing the Markov
    property:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述定义，有两个主要的基于价值的强化学习算法：Q-learning [[12](#bib.bib12)] 和 SARSA [[13](#bib.bib13)]，分别被归类为离策略强化学习算法和在策略强化学习算法。在这两个算法中，状态-动作对的值（$Q$-值）被存储在$Q$-表中，并通过利用马尔可夫性质的贝尔曼方程的递归性质来学习：
- en: '|  | $Q^{\pi}(s_{t},a_{t})\;=\;\mathbb{E}_{\pi}[r_{t}+\gamma Q^{\pi}(s_{t+1},\pi(s_{t+1})].$
    |  | (7) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{\pi}(s_{t},a_{t})\;=\;\mathbb{E}_{\pi}[r_{t}+\gamma Q^{\pi}(s_{t+1},\pi(s_{t+1})].$
    |  | (7) |'
- en: In practice, $Q^{\pi}$ estimates are updated with a learning rate $\alpha$ to
    improve the estimation as follows
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，$Q^{\pi}$估计值通过学习率$\alpha$进行更新，以改善估计，具体如下：
- en: '|  | $Q^{\pi}(s_{t},a_{t})\leftarrow Q^{\pi}(s_{t},a_{t})+\alpha(y_{t}-Q^{\pi}(s_{t},a_{t}))$
    |  | (8) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{\pi}(s_{t},a_{t})\leftarrow Q^{\pi}(s_{t},a_{t})+\alpha(y_{t}-Q^{\pi}(s_{t},a_{t}))$
    |  | (8) |'
- en: 'where $y_{t}$ is the temporal difference (TD) target for $Q^{\pi}(s_{t},a_{t})$.
    The TD step size is a user-defined parameter and determines how many experience
    steps (i.e., actions) to consider in computing $y_{t}$, the new instantaneous
    estimate for $Q^{\pi}(s_{t},a_{t})$. The rewards $R_{t}^{(n)}=\sum_{i=0}^{n-1}\gamma^{i}r_{t+i}$
    in the predefined number of $n$ TD steps, together with the Q-value $Q^{\pi}(s_{t+n},a_{t+n})$
    after $n$ steps give $y_{t}$. The difference between Q-learning and SARSA becomes
    clear in this stage. Q-learning is an off-policy model, in which actions of the
    agent are updated by maximizing Q-values over the action, whereas SARSA is an
    on-policy model, in which actions of the agent are updated according to the policy
    $\pi$ derived from the $Q$-function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$y_{t}$是$Q^{\pi}(s_{t},a_{t})$的时间差（TD）目标。TD步骤大小是一个用户定义的参数，决定了在计算$y_{t}$时考虑多少经验步骤（即动作），$y_{t}$是$Q^{\pi}(s_{t},a_{t})$的新瞬时估计。在预定义的$n$个TD步骤中，奖励$R_{t}^{(n)}=\sum_{i=0}^{n-1}\gamma^{i}r_{t+i}$，加上$n$步骤后的Q值$Q^{\pi}(s_{t+n},a_{t+n})$，给出$y_{t}$。Q-learning和SARSA之间的区别在此阶段变得清晰。Q-learning是一个离策略模型，其中代理的动作通过最大化Q值来更新，而SARSA是一个在策略模型，其中代理的动作根据从$Q$-函数推导出的策略$\pi$进行更新。
- en: '|  | $y_{t}^{Q-learning}=R_{t}^{(n)}+\gamma^{n}\max_{a_{t+n}}Q^{\pi}(s_{t+n},a_{t+n}),$
    |  | (9) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}^{Q-learning}=R_{t}^{(n)}+\gamma^{n}\max_{a_{t+n}}Q^{\pi}(s_{t+n},a_{t+n}),$
    |  | (9) |'
- en: '|  | $y_{t}^{SARSA}=R_{t}^{(n)}+\gamma^{n}Q^{\pi}(s_{t+n},a_{t+n}).$ |  | (10)
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}^{SARSA}=R_{t}^{(n)}+\gamma^{n}Q^{\pi}(s_{t+n},a_{t+n}).$ |  | (10)
    |'
- en: While Q-learning follows a greedy approach to update its Q-value estimates,
    SARSA follows the same policy for both updating Q-values and taking actions. To
    encourage exploring new states usually an $\epsilon$-greedy policy is used for
    taking actions in both Q-learning and SARSA. In the $\epsilon$-greedy policy,
    a random action is taken with probability $\epsilon$, and the best action with
    respect to the current policy defined by $Q(s,a)$ is taken with probability $1-\epsilon$.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Q-learning采用贪婪方法来更新其Q-值估计，SARSA则在更新Q-值和执行动作时遵循相同的策略。为了鼓励探索新的状态，通常在Q-learning和SARSA中使用$\epsilon$-贪婪策略来执行动作。在$\epsilon$-贪婪策略中，以概率$\epsilon$执行随机动作，以概率$1-\epsilon$执行相对于当前策略$Q(s,a)$定义的最佳动作。
- en: In both Q-learning and SARSA, the case with maximum TD steps, typically denoted
    with $n=\infty$ to express the end of episode, corresponds to a fully experience-based
    technique called Monte-Carlo RL, in which the Q-values are updated only once at
    the end of each episode. This means the same policy is used without any updates
    to take actions throughout an episode. The TD$(\lambda$) technique generalizes
    TD learning by averaging all TD targets with steps from $1$ to $\infty$ with exponentially
    decaying weights, where $\lambda$ is the decay rate [[11](#bib.bib11)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q-learning和SARSA中，最大TD步骤的情况，通常用$n=\infty$来表示回合的结束，对应于一种完全基于经验的技术，称为蒙特卡罗强化学习，其中Q值仅在每回合结束时更新一次。这意味着在整个回合中使用相同的策略而不进行任何更新来执行动作。TD$(\lambda)$技术通过对从$1$到$\infty$的所有TD目标进行加权平均来推广TD学习，其中$\lambda$是衰减率
    [[11](#bib.bib11)]。
- en: III-A3 Policy-based RL
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 基于策略的强化学习
- en: Policy-based RL algorithms treat the policy $\pi_{\theta}$ as a probability
    distribution over state-action pairs parameterized by $\theta$. Policy parameters
    $\theta$ are updated in order to maximize an objective function $J(\theta)$, such
    as the expected return $\mathbb{E}_{\pi_{\theta}}[R_{t}|\theta]=\mathbb{E}_{\pi_{\theta}}[Q^{\pi_{\theta}}(s_{t},a_{t})|\theta]$.
    The performance of policy-based methods are typically better than that of value-based
    methods on continuous control problems with infinite-dimensional action space
    or high-dimensional problems since policy does not require to explore all the
    states in a large and continuous space and store them in a table. Although there
    are some effective gradient-free approaches in the literature for optimizing policies
    in non-RL methods [[14](#bib.bib14)], gradient-based methods are known to be more
    useful for policy optimization in all types of RL algorithms.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的强化学习算法将策略$\pi_{\theta}$视为由$\theta$参数化的状态-动作对的概率分布。策略参数$\theta$会被更新以最大化目标函数$J(\theta)$，例如期望回报$\mathbb{E}_{\pi_{\theta}}[R_{t}|\theta]=\mathbb{E}_{\pi_{\theta}}[Q^{\pi_{\theta}}(s_{t},a_{t})|\theta]$。在具有无限维动作空间或高维问题的连续控制问题中，基于策略的方法的表现通常优于基于价值的方法，因为策略不需要探索大而连续空间中的所有状态并将其存储在表格中。虽然文献中有一些有效的无梯度优化策略的方法[[14](#bib.bib14)]，但基于梯度的方法在所有类型的强化学习算法中被认为对策略优化更为有用。
- en: Here, we briefly discuss the policy gradient-based RL algorithms, which select
    actions using the gradient of objective function $J(\theta)$ with respect to $\theta$,
    called the policy gradient. In the well-known policy gradient algorithm REINFORCE
    [[15](#bib.bib15)], the objective function is the expected return, and using the
    log-derivative trick $\nabla\log\pi_{\theta}=\frac{\nabla\pi_{\theta}}{\pi_{\theta}}$
    the policy gradient is written as
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简要讨论基于策略梯度的强化学习算法，这些算法使用目标函数$J(\theta)$对$\theta$的梯度来选择动作，这个梯度被称为策略梯度。在著名的策略梯度算法REINFORCE
    [[15](#bib.bib15)]中，目标函数是期望回报，通过使用对数导数技巧$\nabla\log\pi_{\theta}=\frac{\nabla\pi_{\theta}}{\pi_{\theta}}$，策略梯度可以写作
- en: '|  | $\nabla_{\theta}J(\theta)=\mathbb{E}_{\pi_{\theta}}[Q^{\pi_{\theta}}(s,a)\nabla_{\theta}\log\pi_{\theta}].$
    |  | (11) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{\theta}J(\theta)=\mathbb{E}_{\pi_{\theta}}[Q^{\pi_{\theta}}(s,a)\nabla_{\theta}\log\pi_{\theta}].$
    |  | (11) |'
- en: Since computing the entire gradient is not efficient, REINFORCE uses the popular
    stochastic gradient descent technique to approximate the gradient in updating
    the parameters $\theta$. Using the return $R_{t}$ at time $t$ as an estimator
    of $Q^{\pi_{\theta}}(s_{t},a_{t})$ in each Monte-Carlo iteration it performs the
    update
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算整个梯度效率不高，REINFORCE使用了流行的随机梯度下降技术来逼近梯度，从而更新参数$\theta$。在每次蒙特卡罗迭代中，使用时间$t$的回报$R_{t}$作为$Q^{\pi_{\theta}}(s_{t},a_{t})$的估计器来进行更新
- en: '|  | $\theta\leftarrow\theta+\alpha\nabla_{\theta}\log\pi_{\theta}R_{t},$ |  |
    (12) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha\nabla_{\theta}\log\pi_{\theta}R_{t},$ |  |
    (12) |'
- en: where $\alpha$ is the learning rate. Specifically, $\theta$ is updated in the
    $\nabla_{\theta}\log\pi_{\theta}$ direction with weight $R_{t}$. That is, if the
    approximate policy gradient corresponds to a high reward $R_{t}$, this gradient
    direction is *reinforced* by the algorithm while updating the parameters.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$是学习率。具体而言，$\theta$在$\nabla_{\theta}\log\pi_{\theta}$方向上以权重$R_{t}$进行更新。也就是说，如果近似策略梯度对应于高回报$R_{t}$，则算法在更新参数时会*强化*这一梯度方向。
- en: One problem with the Monte-Carlo policy gradient is its high variance. To reduce
    the variance in policy gradient estimates Actor-Critic algorithms use the state
    value function $V^{\pi_{\theta}(s)}$ as a baseline. Instead of $Q^{\pi_{\theta}}(s,a)$,
    the advantage function [[16](#bib.bib16)] $A^{\pi_{\theta}}(s,a)=Q^{\pi_{\theta}}(s,a)-V^{\pi_{\theta}}(s)$
    is used in the policy gradient
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗策略梯度的一个问题是其高方差。为了减少策略梯度估计中的方差，演员-评论家算法使用状态值函数$V^{\pi_{\theta}(s)}$作为基准。策略梯度中使用的是优势函数[[16](#bib.bib16)]
    $A^{\pi_{\theta}}(s,a)=Q^{\pi_{\theta}}(s,a)-V^{\pi_{\theta}}(s)$，而不是$Q^{\pi_{\theta}}(s,a)$。
- en: '|  | $\nabla_{\theta}J(\theta)=\mathbb{E}_{\pi_{\theta}}[A^{\pi_{\theta}}(s,a)\nabla_{\theta}\log\pi_{\theta}].$
    |  | (13) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{\theta}J(\theta)=\mathbb{E}_{\pi_{\theta}}[A^{\pi_{\theta}}(s,a)\nabla_{\theta}\log\pi_{\theta}].$
    |  | (13) |'
- en: 'The advantage function, being positive or negative, determines the update direction:
    go in the same/opposite direction of actions yielding higher/lower reward than
    average. Actor-Critic method is further discussed in Section [III-B3](#S3.SS2.SSS3
    "III-B3 Actor Critic Methods ‣ III-B Deep Reinforcement Learning ‣ III Deep RL:
    An Overview ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey") within the Deep RL discussion.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '优势函数，无论是正值还是负值，决定了更新方向：朝着与高于/低于平均奖励的动作相同/相反的方向前进。Actor-Critic方法将在深度强化学习讨论中的第[III-B3](#S3.SS2.SSS3
    "III-B3 Actor Critic Methods ‣ III-B Deep Reinforcement Learning ‣ III Deep RL:
    An Overview ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey")节进一步讨论。'
- en: III-A4 Multi-Agent RL
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 多智能体强化学习
- en: 'Many real world problems require interacting multiple agents to maximize the
    learning performance. Learning with multiple agents is a challenging task since
    each agent should consider other agents’ actions to reach a globally optimum solution.
    Increasing the number of agents also increases the state-action dimensions, thus
    decomposing the tasks between agents is a scalable approach for large control
    systems. There are two main issues with high-dimensional systems in multi-agent
    RL in terms of state and actions: stability and adaptation of agents to the environment
    [[17](#bib.bib17)]. When each agent optimizes its action without considering close
    agents, the optimal learning for overall system would become non-stationary. There
    are several approaches to address this problem in multi-agent RL systems such
    as distributed learning, cooperative learning and competitive learning [[17](#bib.bib17)].'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界问题需要多个智能体的互动以最大化学习性能。多智能体学习是一项具有挑战性的任务，因为每个智能体都需要考虑其他智能体的行动以达成全局最优解。增加智能体的数量也会增加状态-动作维度，因此将任务在智能体之间分解是大规模控制系统的可扩展方法。多智能体强化学习中的高维系统在状态和动作方面有两个主要问题：稳定性和智能体对环境的适应性[[17](#bib.bib17)]。当每个智能体在优化其行动时不考虑附近的智能体时，整体系统的最佳学习将变得非平稳。有几种方法可以解决多智能体强化学习系统中的这个问题，如分布式学习、合作学习和竞争学习[[17](#bib.bib17)]。
- en: III-B Deep Reinforcement Learning
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 深度强化学习
- en: In high-dimensional state spaces, standard RL algorithms cannot efficiently
    compute the value functions or policy functions for all states. Although some
    linear function approximation methods are proposed for solving the large state
    space problem in RL, their capabilities are still up to a certain point. In high-dimensional
    and complex systems, standard RL approaches cannot learn informative features
    of the environment for effective function approximation. However, this problem
    can be easily handled by deep learning based function approximators, in which
    deep neural networks are trained to learn the optimal policy or value functions.
    Different neural network structures such as convolutional neural network (CNN)
    and recurrent neural network (RNN) are used for training RL algorithms in large
    state spaces [[18](#bib.bib18)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维状态空间中，标准强化学习算法无法有效地计算所有状态的价值函数或策略函数。尽管有一些线性函数逼近方法被提出以解决强化学习中的大状态空间问题，但它们的能力仍然有限。在高维和复杂系统中，标准强化学习方法无法学习环境的有用特征以进行有效的函数逼近。然而，这个问题可以通过基于深度学习的函数逼近器轻松解决，其中深度神经网络被训练来学习最优策略或价值函数。不同的神经网络结构，如卷积神经网络（CNN）和递归神经网络（RNN），被用于训练大状态空间中的强化学习算法[[18](#bib.bib18)]。
- en: The main concept of deep learning is to extract useful patterns from data. Deep
    learning models are roughly inspired by the multi-layered structure of human neural
    system. Today, deep learning has applications in a wide spectrum of areas, including
    computer vision, speech recognition, natural language processing, and the deep
    RL applications.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的主要概念是从数据中提取有用的模式。深度学习模型大致受到人类神经系统多层结构的启发。如今，深度学习在计算机视觉、语音识别、自然语言处理和深度强化学习应用等多个领域都有应用。
- en: III-B1 Deep Q-Network
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 深度Q网络
- en: 'Since value-based RL algorithms learn the Q-function by populating a Q-table,
    it is not feasible to visit all the states and actions in a large state space
    and continuous action problems. The leading approach to this problem, called Deep
    Q-Network (DQN) [[19](#bib.bib19)], is to approximate the Q-function with deep
    neural networks. Original DQN receives raw input image as state, and estimates
    Q-values from them using CNNs. Denoting the neural network parameters with $\theta$
    the Q-function approximation is written as $Q(s,a;\theta)$. The output of neural
    network is the best action selected according to ([6](#S3.E6 "In III-A2 Value-based
    RL ‣ III-A Reinforcement Learning ‣ III Deep RL: An Overview ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey")) using a discrete
    set of approximate action values.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '由于基于值的强化学习（RL）算法通过填充Q表来学习Q函数，在大型状态空间和连续动作问题中，访问所有状态和动作是不切实际的。解决这个问题的主要方法是深度Q网络（DQN）[[19](#bib.bib19)]，它使用深度神经网络来近似Q函数。原始DQN接收原始输入图像作为状态，并使用卷积神经网络（CNN）从中估计Q值。用$\theta$表示神经网络参数，则Q函数近似表示为$Q(s,a;\theta)$。神经网络的输出是根据([6](#S3.E6
    "In III-A2 Value-based RL ‣ III-A Reinforcement Learning ‣ III Deep RL: An Overview
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey"))使用离散的近似动作值集合来选择的最佳动作。'
- en: 'The major contribution of *Mnih et al.* [[19](#bib.bib19)] was two novel techniques
    to stabilize learning with deep neural networks: target network and experience
    replay. The original DQN algorithm is shown to significantly outperform the expert
    human performance on several classic Atari video games. The complete DQN algorithm
    with experience replay and target network is given by Algorithm [1](#alg1 "Algorithm
    1 ‣ III-B1 Deep Q-Network ‣ III-B Deep Reinforcement Learning ‣ III Deep RL: An
    Overview ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mnih等人*[[19](#bib.bib19)]的主要贡献是提出了两种新技术来稳定深度神经网络的学习：目标网络和经验回放。原始DQN算法在几个经典的Atari视频游戏中明显超越了专家级人类表现。带有经验回放和目标网络的完整DQN算法由算法[1](#alg1
    "Algorithm 1 ‣ III-B1 Deep Q-Network ‣ III-B Deep Reinforcement Learning ‣ III
    Deep RL: An Overview ‣ Deep Reinforcement Learning for Intelligent Transportation
    Systems: A Survey")给出。'
- en: '*Target Network*: One of the main parts of DQN that stabilize learning is the
    target network. DQN has two separate networks denoted as the main network that
    approximates the Q-function, and the target network that gives the TD target for
    updating the main network. In the training phase, while the main network parameters
    $\theta$ are updated after every action, target network parameters $\theta^{\textendash}$
    are updated after a certain period of time. The reason why target network is not
    updated after every iteration is that it adjusts the main network updates to keep
    the value estimations in control. If both networks were updated at the same time,
    the change in the main network would be exaggerated due to the feedback loop by
    the target network, resulting in an unstable network. Similar to ([9](#S3.E9 "In
    III-A2 Value-based RL ‣ III-A Reinforcement Learning ‣ III Deep RL: An Overview
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey")),
    1-step TD target $y_{t}$ is written as'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标网络*：DQN中稳定学习的主要部分之一是目标网络。DQN有两个独立的网络，一个是主网络，它近似Q函数，另一个是目标网络，它为更新主网络提供TD目标。在训练阶段，主网络参数$\theta$在每次动作后都会更新，而目标网络参数$\theta^{\textendash}$则在一定时间间隔后更新。目标网络不在每次迭代后更新的原因是，它调整主网络的更新，以保持值估计的控制。如果两个网络同时更新，由于目标网络的反馈循环，主网络的变化将被夸大，从而导致网络不稳定。类似于([9](#S3.E9
    "In III-A2 Value-based RL ‣ III-A Reinforcement Learning ‣ III Deep RL: An Overview
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey"))，1步TD目标$y_{t}$表示为'
- en: '|  | $y_{t}^{DQN}=r_{t}+\gamma\max_{a_{t+1}}Q^{\pi}(s_{t+1},a_{t+1};\theta^{\textendash}_{t}),$
    |  | (14) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}^{DQN}=r_{t}+\gamma\max_{a_{t+1}}Q^{\pi}(s_{t+1},a_{t+1};\theta^{\textendash}_{t}),$
    |  | (14) |'
- en: where $Q^{\pi}(s_{t+1},a_{t+1};\theta^{\textendash}_{t})$ denotes the target
    network.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Q^{\pi}(s_{t+1},a_{t+1};\theta^{\textendash}_{t})$表示目标网络。
- en: '*Experience Replay*: DQN introduces another distinct feature called experience
    replay which stores recent experiences $(s_{t},a_{t},r_{t},s_{t+1})$ in replay
    memory, and samples batches uniformly from the replay memory for training neural
    network. There are two main reasons why experience replay is used in DQN. Firstly,
    it prevents the agent from getting stuck into the recent trajectories by doing
    random sampling since RL agents are prone to temporal correlations in the consecutive
    samples. Furthermore, instead of learning over full observations, DQN agent learns
    over mini-batches that increases the efficiency of the training. In a fixed-size
    memory defined for experience replay, the memory stores only recent $M$ samples
    by removing the oldest experience for allocating a space to the latest sample.
    The same technique is applied in other deep RL algorithms [[20](#bib.bib20)],
    [[21](#bib.bib21)].'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*经验回放*：DQN 引入了另一个独特的特性，即经验回放，它将近期的经历 $(s_{t},a_{t},r_{t},s_{t+1})$ 存储在回放记忆中，并从回放记忆中均匀抽取批量数据用于训练神经网络。使用经验回放在
    DQN 中有两个主要原因。首先，它通过随机抽样防止智能体陷入最近的轨迹，因为 RL 智能体容易受到连续样本的时间相关性影响。此外，DQN 智能体不是在完整观察上进行学习，而是在小批量数据上进行学习，从而提高了训练效率。在定义为经验回放的固定大小内存中，内存仅存储最近的
    $M$ 个样本，通过移除最旧的经验来为最新样本分配空间。其他深度 RL 算法也使用了相同的技术 [[20](#bib.bib20)]，[[21](#bib.bib21)]。'
- en: '*Prioritized Experience Replay*: Experience replay technique samples experiences
    uniformly from the memory, however, some experiences has more impact on learning
    than the others. A new approach prioritizing significant actions over other actions
    is proposed in [[22](#bib.bib22)] by changing the sampling distribution of DQN
    algorithm. The overall idea for prioritized experience replay is that the samples
    with higher TD error, $y_{t}^{DQN}-Q^{\pi}(s_{t},a_{t};\theta_{t}^{-})$, receives
    higher ranking in terms of probability than the other samples by applying a stochastic
    sampling with proportional prioritization or rank-based prioritization. The experiences
    are sampled based on the assigned probabilities.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*优先经验回放*：经验回放技术从内存中均匀抽样经历，但有些经历对学习的影响比其他经历更大。[[22](#bib.bib22)] 提出了一个优先考虑重要动作的新方法，通过改变
    DQN 算法的抽样分布来实现。优先经验回放的总体思路是，通过应用具有比例优先权或基于排名的优先权的随机抽样，使得具有更高 TD 误差的样本 $y_{t}^{DQN}-Q^{\pi}(s_{t},a_{t};\theta_{t}^{-})$
    在概率上获得更高的排名。根据分配的概率抽样经历。'
- en: Algorithm 1 DQN algorithm
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 DQN 算法
- en: 1:  *Input* Replay memory size $M$, batch size $d$, number of episodes $E$,
    and number of time steps $T$2:  Inititalize Main network weights $\theta$3:  Inititalize
    Target network weights $\theta^{-}$4:  Inititalize Replay memory5:  for $e=1,\ldots,E$ do6:     Inititalize
    state $s_{1}$, and action $a_{1}$7:     for $t=1,\ldots,T$ do8:        Take action
    $a_{t}=\operatorname*{argmax}_{a}Q^{\pi}(s_{t},a;\theta)$ with probability $1-\epsilon$
    or a random action with probability $\epsilon$9:        Get reward $r_{t}$ and
    observe next state $s_{t+1}$10:        if Replay capacity $M$ is full then11:           Delete
    the oldest tuple in memory12:        end if13:        Store the tuple $(s_{t},a_{t},r_{t},s_{t+1})$
    to replay memory14:        Sample random $d$ tuples from replay memory15:        <math
    alttext="y_{t}=\begin{cases}r_{t},&amp;\text{if $t=T$}.\\
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  *输入* 回放记忆大小 $M$，批量大小 $d$，回合数 $E$，和时间步数 $T$2:  初始化主网络权重 $\theta$3:  初始化目标网络权重
    $\theta^{-}$4:  初始化回放记忆5:  对于 $e=1,\ldots,E$ 执行6:     初始化状态 $s_{1}$，和动作 $a_{1}$7:
        对于 $t=1,\ldots,T$ 执行8:        以概率 $1-\epsilon$ 执行动作 $a_{t}=\operatorname*{argmax}_{a}Q^{\pi}(s_{t},a;\theta)$，或以概率
    $\epsilon$ 执行随机动作9:        获得奖励 $r_{t}$ 并观察下一个状态 $s_{t+1}$10:        如果 回放容量 $M$
    满了，则11:          删除记忆中最旧的元组12:        结束 如果13:        将元组 $(s_{t},a_{t},r_{t},s_{t+1})$
    存储到回放记忆中14:        从回放记忆中随机抽取 $d$ 个元组15:        <math alttext="y_{t}=\begin{cases}r_{t},&amp;\text{if
    $t=T$}.'
- en: r_{t}+\gamma\max_{a}Q^{\pi}(s_{t+1},a_{t+1};\theta^{\textendash}_{t}),&amp;\text{otherwise}.\end{cases}"
    display="inline"><semantics ><mrow  ><msub ><mi >y</mi><mi  >t</mi></msub><mo
    >=</mo><mrow ><mo  >{</mo><mtable columnspacing="5pt" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mrow ><msub ><mi >r</mi><mi >t</mi></msub><mo >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >if </mtext><mrow ><mi >t</mi><mo >=</mo><mi
    >T</mi></mrow></mrow><mo lspace="0em"  >.</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mrow ><msub ><mi >r</mi><mi >t</mi></msub><mo >+</mo><mrow ><mi >γ</mi><mo lspace="0.167em"
    rspace="0em" >​</mo><mrow ><msub ><mi >max</mi><mi >a</mi></msub><mo lspace="0.167em"  >⁡</mo><msup
    ><mi >Q</mi><mi >π</mi></msup></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >s</mi><mrow ><mi >t</mi><mo >+</mo><mn
    >1</mn></mrow></msub><mo >,</mo><msub ><mi >a</mi><mrow ><mi >t</mi><mo >+</mo><mn
    >1</mn></mrow></msub><mo >;</mo><msubsup ><mi >θ</mi><mi >t</mi><mi mathvariant="normal"  >–</mi></msubsup><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mtext >otherwise</mtext><mo lspace="0em" >.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑦</ci><ci >𝑡</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑟</ci><ci >𝑡</ci></apply><ci
    ><mrow ><mtext >if </mtext><mrow ><mi >t</mi><mo >=</mo><mi >T</mi></mrow></mrow></ci><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑟</ci><ci >𝑡</ci></apply><apply
    ><ci >𝛾</ci><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑄</ci><ci >𝜋</ci></apply></apply><vector
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑠</ci><apply ><ci >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑎</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝜃</ci><ci >–</ci></apply><ci >𝑡</ci></apply></vector></apply></apply><ci ><mtext
    >otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >y_{t}=\begin{cases}r_{t},&\text{if $t=T$}.\\ r_{t}+\gamma\max_{a}Q^{\pi}(s_{t+1},a_{t+1};\theta^{\textendash}_{t}),&\text{otherwise}.\end{cases}</annotation></semantics></math>16:        Perform
    policy gradient using $y_{t}$ for updating $\theta$17:        Update target network
    every $N$ step, $\theta^{\textendash}=\theta$18:     end for19:  end for
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Double Dueling DQN
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DQN is the improved version of the standard Q-Learning algorithm with a single
    estimator. Both DQN and Q-Learning overestimates some actions due to having single
    $Q$ function estimations. Authors in [[23](#bib.bib23)] proposes doubling the
    estimators for action selection with main network and action evaluation with target
    network separately in loss minimization similar to the tabular double Q-learning
    technique [[24](#bib.bib24)]. Instead of selecting the Q value that maximizes
    future reward using the target network (see Eq. ([14](#S3.E14 "In III-B1 Deep
    Q-Network ‣ III-B Deep Reinforcement Learning ‣ III Deep RL: An Overview ‣ Deep
    Reinforcement Learning for Intelligent Transportation Systems: A Survey"))), double
    DQN network selects the action using the main network and evaluates it using the
    target network. Action selection is decoupled with target network for better Q-value
    estimation:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 'DQN 是标准 Q 学习算法的改进版本，具有单一估计器。由于只有单一的 $Q$ 函数估计，DQN 和 Q 学习都会高估一些动作。[[23](#bib.bib23)]
    的作者建议将估计器加倍，主网络用于动作选择，目标网络用于动作评估，类似于表格双重 Q 学习技术 [[24](#bib.bib24)]。与使用目标网络选择最大化未来奖励的
    Q 值（见 Eq. ([14](#S3.E14 "In III-B1 Deep Q-Network ‣ III-B Deep Reinforcement Learning
    ‣ III Deep RL: An Overview ‣ Deep Reinforcement Learning for Intelligent Transportation
    Systems: A Survey"))）不同，双重 DQN 网络使用主网络选择动作，并使用目标网络评估它。动作选择与目标网络解耦，以获得更好的 Q 值估计。'
- en: '|  | $y_{t}^{DDQN}=r_{t}+\gamma Q^{\pi}(s_{t+1},\operatorname*{argmax}_{a_{t+1}}Q^{\pi}(s_{t+1},a_{t+1};\theta);\theta^{\textendash}_{t}).$
    |  | (15) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}^{DDQN}=r_{t}+\gamma Q^{\pi}(s_{t+1},\operatorname*{argmax}_{a_{t+1}}Q^{\pi}(s_{t+1},a_{t+1};\theta);\theta^{\textendash}_{t}).$
    |  | (15) |'
- en: Another improved version of DQN is a dueling network architecture which estimates
    state value function $V^{\pi}(s)$ and advantage function $A^{\pi}(s,a)$ separately
    for each action [[25](#bib.bib25)]. Output of the combination of these two networks
    is a Q-value for a discrete set of actions through an aggregation layer. This
    way dueling DQN learns the important state values without their corresponding
    effects on the actions since state value function $V^{\pi}(s)$ is an action-free
    estimation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 的另一个改进版本是对抗网络架构，它分别为每个动作估计状态价值函数 $V^{\pi}(s)$ 和优势函数 $A^{\pi}(s,a)$ [[25](#bib.bib25)]。这两个网络组合的输出是通过聚合层得到的离散动作集的
    Q 值。这样，对抗 DQN 学习了重要的状态值，而不考虑它们对动作的相应影响，因为状态价值函数 $V^{\pi}(s)$ 是一种无动作估计。
- en: These two doubling and dueling models on DQN algorithm with prioritized experience
    replay are accepted as the state-of-the-art for discrete action-based deep RL.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种双重和对抗模型基于优先经验回放的 DQN 算法被认为是离散动作深度强化学习的最先进技术。
- en: III-B3 Actor Critic Methods
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 演员-评论员方法
- en: 'Actor-critic RL models are in between policy-based algorithms and value-based
    algorithms due to having two estimators: actor using Q-value estimation and critic
    using state value function estimation (see Fig. [2](#S3.F2 "Figure 2 ‣ III-B3
    Actor Critic Methods ‣ III-B Deep Reinforcement Learning ‣ III Deep RL: An Overview
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey")).
    While actor controls the agent’s behaviors based on policy, critic evaluates the
    taken action based on value function. There are recent papers that deal with the
    variations of actor-critic models using the deep RL approach [[20](#bib.bib20)],
    [[21](#bib.bib21)], [[26](#bib.bib26)], in which function approximators for both
    actor and critic are based on deep neural networks.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'Actor-critic 强化学习模型介于基于策略的算法和基于价值的算法之间，因为它们有两个估计器：演员使用 Q 值估计，评论员使用状态价值函数估计（见图
    [2](#S3.F2 "Figure 2 ‣ III-B3 Actor Critic Methods ‣ III-B Deep Reinforcement
    Learning ‣ III Deep RL: An Overview ‣ Deep Reinforcement Learning for Intelligent
    Transportation Systems: A Survey")）。演员根据策略控制代理的行为，而评论员根据价值函数评估采取的行动。最近有一些论文处理了使用深度强化学习方法的演员-评论员模型的变体
    [[20](#bib.bib20)], [[21](#bib.bib21)], [[26](#bib.bib26)]，其中演员和评论员的函数近似器都基于深度神经网络。'
- en: '![Refer to caption](img/04687745f60a8a02b912c4eb4b804fd5.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04687745f60a8a02b912c4eb4b804fd5.png)'
- en: 'Figure 2: Actor Critic control loop.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：演员-评论员控制回路。
- en: Standard DQN techniques with single network estimator are suitable for low-dimensional
    discrete action spaces. A recent actor-critic algorithm called deep deterministic
    policy gradient (DDPG) is introduced for solving high-dimensional continuous control
    problems with deterministic policy gradient approach estimating over state space
    instead of stochastic policy gradient estimating over state and action spaces
    together [[20](#bib.bib20)]. One of the differences of DDPG from standard DQN
    is that it uses a new soft target update model doing frequent soft updates.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 标准DQN技术与单网络估计器适用于低维离散动作空间。最近引入了一种名为深度确定性策略梯度（DDPG）的演员-评论员算法，用于解决高维连续控制问题，它使用确定性策略梯度方法对状态空间进行估计，而不是对状态和动作空间一起进行随机策略梯度估计[[20](#bib.bib20)]。DDPG与标准DQN的一个区别在于它使用了一种新的软目标更新模型，进行频繁的软更新。
- en: III-B4 Asynchronous Methods
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 异步方法
- en: Improvements in hardware systems allowed RL researchers to perform parallel
    computing with multiple CPUs or GPUs, which increases the learning pace. First
    parallel models tested on DQNs advanced the agent performance in terms of lower
    training time and higher convergence results. For instance, the asynchronous multiple
    actor-learner model proposed in [[27](#bib.bib27)] achieve very high performance
    in both continuous and discrete action spaces. Multiple actor-learners enable
    the RL agent to explore the environment with different exploration rates. Furthermore,
    asynchronous updates do not require replay memory, and learners use accumulated
    multiple gradients of all experiments done in a predefined update period $T$.
    Asynchronous advantage actor-critic (A3C), a state-of-the-art deep RL algorithm,
    updates policy and value networks asynchronously over parallel processors. Each
    network is separately updated within the update period $T$, and the shared main
    network is updated with respect to the parameters $\theta^{\pi}$ and $\theta^{V}$.
    The synchronous and simpler version of A3C is known as advantage actor-critic
    (A2C).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件系统的改进使得RL研究人员能够使用多个CPU或GPU进行并行计算，从而加快学习速度。首先在DQN上测试的并行模型提高了代理的表现，体现在更低的训练时间和更高的收敛结果。例如，[[27](#bib.bib27)]中提出的异步多演员-学习者模型在连续和离散动作空间中均表现出非常高的性能。多个演员-学习者使RL代理能够以不同的探索速率探索环境。此外，异步更新不需要重放记忆，学习者在预定更新周期$T$内使用所有实验的累积多个梯度。异步优势演员-评论员（A3C）是一种最先进的深度RL算法，它在并行处理器上异步更新策略和价值网络。每个网络在更新周期$T$内单独更新，主共享网络根据参数$\theta^{\pi}$和$\theta^{V}$进行更新。A3C的同步且更简单的版本被称为优势演员-评论员（A2C）。
- en: III-C Summary of Deep RL
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 深度RL总结
- en: In this section, we discussed the background of deep RL, including policy-based
    and value-based RL models. Before discussing the details of deep RL applications
    in ITS, it is worth mentioning that certain deep RL algorithms are preferred in
    different applications depending on the specifications of application domain.
    While developing new deep RL techniques is an active research area, Q-learning
    based DQN and actor-critic based DDPG algorithms continue to dominate the RL-based
    ITS controllers. For high-dimensional state spaces, deep RL methods are preferred
    over standard RL methods. With regard to action space, policy-based deep RL methods
    are more suitable for continuous action spaces than value-based deep RL methods.
    For discrete action spaces, ITS controllers typically use DQN and its variants
    due to their simpler structures compared to policy-based methods. In general,
    we can say that Q-learning based DQN models are typically used for less complicated
    systems which have limited state and action spaces, whereas policy-based or actor-critic
    algorithms are preferred mainly for large complicated systems, including multi-agent
    control systems. We should also note here that in many cases the designer can
    choose between discrete and continuous state and action spaces while setting up
    the problem. For instance, in TSC, as discussed in the following section, some
    authors define a continuous action as how much time to extend green light while
    some other authors define a discrete action space as choosing the green light
    direction.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了深度RL的背景，包括基于策略和基于价值的RL模型。在讨论深度RL在智能交通系统中的应用细节之前，值得提及的是，某些深度RL算法在不同应用中具有不同的偏好，这取决于应用领域的规格。虽然开发新的深度RL技术是一个活跃的研究领域，但基于Q学习的DQN和基于actor-critic的DDPG算法仍然主导RL基础的智能交通系统控制器。对于高维状态空间，深度RL方法比标准RL方法更受欢迎。关于动作空间，基于策略的深度RL方法比基于价值的深度RL方法更适合连续动作空间。对于离散动作空间，智能交通系统控制器通常使用DQN及其变体，因为它们的结构比基于策略的方法更简单。一般来说，我们可以说基于Q学习的DQN模型通常用于状态和动作空间有限的较简单系统，而基于策略或actor-critic算法主要用于大型复杂系统，包括多智能体控制系统。我们还应注意，在许多情况下，设计师可以在设置问题时选择离散和连续状态及动作空间。例如，在TSC中，如下节所述，一些作者将连续动作定义为延长绿灯的时间，而其他作者将离散动作空间定义为选择绿灯方向。
- en: IV Deep RL Settings for TSC
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 深度RL在TSC中的设置
- en: Up to this point, we discussed the importance of AI in traffic systems and theoretical
    background of RL, in particular deep RL. One of the main application areas of
    deep RL in ITS is controlling signalized intersections. Since most of the existing
    works are application-oriented, proposed methods differ from each other in various
    aspects – e.g., applying deep RL to different intersection models with different
    technology to monitor traffic, characterizing the RL model with different state-action-reward
    representations, and using different neural network structures. Hence, a direct
    performance comparison between them is usually not possible.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了AI在交通系统中的重要性以及RL的理论背景，特别是深度RL。深度RL在智能交通系统中的一个主要应用领域是控制有信号的交叉口。由于现有的大多数研究都是面向应用的，所提出的方法在各个方面都不同——例如，将深度RL应用于不同的交叉口模型，使用不同的技术来监控交通，利用不同的状态-动作-奖励表示来表征RL模型，以及使用不同的神经网络结构。因此，通常无法直接比较它们的性能。
- en: In these applications, a learning algorithm (deep RL in our case) is implemented
    in the TSC center to control traffic signals adaptive to the traffic flow. First,
    the control unit collects the state information, which can be in different formats
    such as queue length, position of vehicles, speed of vehicles etc., and then control
    unit takes an action based on the current policy of proposed deep RL method. Finally,
    agent (control unit) gets a reward with respect to the taken action. By following
    these steps agent tries to find an optimal policy in order to minimize the congestion
    on intersection.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些应用中，学习算法（在我们的案例中是深度RL）被实施在TSC中心，以适应交通流量的交通信号控制。首先，控制单元收集状态信息，这些信息可以是不同格式的，如排队长度、车辆位置、车辆速度等，然后控制单元根据当前的深度RL方法的策略采取行动。最后，代理（控制单元）根据采取的行动获得奖励。通过遵循这些步骤，代理试图找到一个最优策略，以减少交叉口的拥堵。
- en: 'Dealing with the TSC problem on simulators by using RL algorithms requires
    a good problem formulation in several parts: state, action, reward definitions
    and neural network structure. In this section, we will discuss these main deep
    RL configurations together with the traffic simulators used in the literature.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟器上处理 TSC 问题时使用 RL 算法需要在多个部分进行良好的问题表述：状态、动作、奖励定义和神经网络结构。在本节中，我们将讨论这些主要的深度
    RL 配置以及文献中使用的交通模拟器。
- en: IV-A State
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 状态
- en: The learning performance is highly dependent on an accurate and concrete state
    definition. Therefore, there are many different state representations used for
    RL applications on traffic lights. Authors in [[28](#bib.bib28)] and [[29](#bib.bib29)]
    considered raw RGB images as a state representation following the same approach
    as the original DQN [[19](#bib.bib19)]. Another similar image-like state representation
    takes the snapshot of the controlled intersection for forming position and speed
    of the vehicles [[30](#bib.bib30)]. Image-like representation format, called discrete
    traffic state encoding (DTSE), is one of the most popular state definitions in
    the TSC applications [[31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [29](#bib.bib29), [38](#bib.bib38),
    [39](#bib.bib39)]. The reason why researchers prefer to use DTSE is that it acquires
    the highest available resolution and a realistic set of information from the intersection.
    Considering $n$ lanes in an intersection, each intersection is divided into cells
    whose size is on average one vehicle starting from the stopping point of intersection
    to $m$ meters back. Speed and position of vehicles, signal phases, and accelerations
    are shown in separate arrays in DTSE. Different variations of those four input
    types are selected by different researcher. For example, while some researchers
    select speed and position together [[31](#bib.bib31), [33](#bib.bib33)], some
    others select only one of the four input types for the state representation, such
    as the position of vehicles [[23](#bib.bib23), [29](#bib.bib29)]. While DTSE considers
    the lane characteristics only, [[30](#bib.bib30)] considers full camera view that
    also includes road side information in the state definition. Today, many intersections
    have high quality cameras monitoring the traffic in intersection. To enable DTSE-type
    state representation, these equipment can be easily extended for monitoring the
    roads connecting to intersections.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 学习性能高度依赖于准确而具体的状态定义。因此，对于交通信号灯的 RL 应用，有许多不同的状态表示方法。[[28](#bib.bib28)] 和 [[29](#bib.bib29)]
    的作者考虑了将原始 RGB 图像作为状态表示，采用与原始 DQN [[19](#bib.bib19)] 相同的方法。另一种类似的图像状态表示方法是拍摄受控交叉口的快照，以形成车辆的位置和速度
    [[30](#bib.bib30)]。被称为离散交通状态编码（DTSE）的图像状表示格式，是 TSC 应用中最流行的状态定义之一 [[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [29](#bib.bib29), [38](#bib.bib38), [39](#bib.bib39)]。研究人员倾向于使用
    DTSE 的原因在于它能够从交叉口获取最高的分辨率和最真实的信息。考虑到交叉口的 $n$ 个车道，每个交叉口被划分为若干单元格，每个单元格的大小平均为一个车辆，从交叉口的停车点起向
    $m$ 米后退。车辆的速度和位置、信号相位以及加速度在 DTSE 中以单独的数组显示。这四种输入类型的不同变体由不同的研究人员选择。例如，一些研究人员将速度和位置一起选择
    [[31](#bib.bib31), [33](#bib.bib33)]，而另一些则仅选择四种输入类型中的一种作为状态表示，例如车辆的位置 [[23](#bib.bib23),
    [29](#bib.bib29)]。而 DTSE 仅考虑车道特征，[[30](#bib.bib30)] 则考虑了包括道路边信息在内的完整摄像头视图。如今，许多交叉口都配备了高质量的摄像头来监控交叉口的交通。为了实现
    DTSE 类型的状态表示，这些设备可以很容易地扩展用于监控连接到交叉口的道路。
- en: '![Refer to caption](img/af0d0e5ca204603237e3ee4f8fced1b1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/af0d0e5ca204603237e3ee4f8fced1b1.png)'
- en: 'Figure 3: Two popular types of state representation: DTSE matrix (middle) and
    feature-based vector (right). Left figure shows the traffic model with the corresponding
    vehicle-based state array. In each cell, one vehicle is represented. The matrix
    in the middle shows a full matrix for one intersection with each road in different
    colors. Right figure is a feature-based state vector, where each cell represents
    a lane.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：两种流行的状态表示类型：DTSE 矩阵（中间）和基于特征的向量（右侧）。左图显示了带有相应车辆状态数组的交通模型。在每个单元格中，表示一辆车。中间的矩阵显示了一个交叉口的完整矩阵，其中每条道路以不同颜色表示。右侧图为基于特征的状态向量，每个单元格表示一个车道。
- en: 'Another common approach for state representations is forming a feature-based
    value vector. Instead of vehicle-based state representation, in feature-based
    state form, average or total value of specific information for each lane is represented
    on a vector. Queue length, cumulative waiting time in a phase cycle, average speed
    on a lane, phase duration (green, red, yellow), and number of vehicles in each
    lane are some of the common features used for state representation. Typically,
    a combination of such information is collected from intersection [[40](#bib.bib40),
    [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]. One advantage of such information
    is that they can be easily collected by road sensors or loop detectors. There
    are also some other unique traffic features that are not commonly used by researchers
    such as scoring based on the max speed on lane detectors [[41](#bib.bib41)], signal
    control threshold metrics [[44](#bib.bib44)], and left turn occupations [[45](#bib.bib45)].
    Two common forms of state representations, DTSE and feature vector, are shown
    in Fig [3](#S4.F3 "Figure 3 ‣ IV-A State ‣ IV Deep RL Settings for TSC ‣ Deep
    Reinforcement Learning for Intelligent Transportation Systems: A Survey").'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的状态表示方法是形成基于特征的价值向量。在特征基础的状态形式中，每条车道的特定信息的平均值或总值会在一个向量上表示，而不是基于车辆的状态表示。队列长度、一个相位周期内的累计等待时间、车道上的平均速度、相位持续时间（绿灯、红灯、黄灯）以及每条车道上的车辆数量是一些常用于状态表示的特征。通常，这类信息的组合是从交叉口收集的
    [[40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]。这种信息的一个优点是可以通过路面传感器或环形探测器轻松收集。还有一些其他独特的交通特征，如基于车道探测器的最大速度评分
    [[41](#bib.bib41)]、信号控制阈值指标 [[44](#bib.bib44)] 和左转占用情况 [[45](#bib.bib45)]，这些特征在研究中不常用。两种常见的状态表示形式，DTSE
    和特征向量，如图 [3](#S4.F3 "图 3 ‣ IV-A 状态 ‣ IV 深度 RL 设置用于 TSC ‣ 深度强化学习在智能交通系统中的应用：综述")
    所示。
- en: For TSC models with multiple intersections, state definitions also include neighboring
    traffic light information such as signal phase, number of vehicles, and average
    speed [[34](#bib.bib34), [44](#bib.bib44), [46](#bib.bib46)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有多个交叉口的 TSC 模型，状态定义还包括邻近交通信号灯的信息，如信号相位、车辆数量和平均速度 [[34](#bib.bib34), [44](#bib.bib44),
    [46](#bib.bib46)]。
- en: IV-B Action
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 行动
- en: The action taken by the RL algorithm from a set of possible actions after receiving
    the state has a critical impact on learning. In a single four-way intersection,
    each direction is controlled with green, red and yellow phases. There are several
    common action selections for a single intersection. The most common one is choosing
    one of the possible green phases. Another one is the binary action selection keeping
    the same phase or changing the direction. Third and relatively less common action
    model is updating the phase duration with a predefined length.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收到状态后，RL 算法从一组可能的行动中选择的行动对学习有着关键影响。在一个单一的四路交叉口，每个方向都有绿灯、红灯和黄灯相位的控制。对于单一交叉口，有几种常见的行动选择。最常见的一种是选择其中一个可能的绿灯相位。另一种是二元行动选择，即保持相同的相位或改变方向。第三种且相对较少见的行动模型是用预定义的时长更新相位持续时间。
- en: For a single intersection, mostly there are 4 possible green phases; North-South
    Green (NSG), East-West Green (EWG), North-South Advance Left Green (NSLG), East-West
    Advance Left Green (EWLG). During the green phase for a direction, vehicles proceed
    through the intersection to the allowed direction. When the action selection setting
    is to select one of the possible green phases, deep RL agent selects an action
    from these four green phases at each time $t$. After following the yellow and
    red transitions, the chosen action is performed on traffic lights. Successful
    agent learning and safety traffic also depend on right red and yellow phase definitions.
    The early applications simplify the phase definitions to two green phases only,
    North-South Green (NSG) and East-West Green (EWG) [[32](#bib.bib32), [40](#bib.bib40)]
    ignoring the left turns. Another action selection model is binary action, in which
    green phase interval length is defined beforehand, and at each time $t$, agent
    decides to either maintain the same phase or proceed to the next phase in a predefined
    cycle, e.g., NSG $\to$ EWG $\to$ NSLG $\to$ EWLG. When agent selects the action
    to change the phase, before executing the next green phase, yellow and red transition
    phases are executed first to have a safe traffic flow [[33](#bib.bib33), [42](#bib.bib42),
    [37](#bib.bib37), [38](#bib.bib38), [47](#bib.bib47)].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个交叉口，通常有4种可能的绿灯阶段：南北绿灯（NSG）、东西绿灯（EWG）、南北前进左转绿灯（NSLG）、东西前进左转绿灯（EWLG）。在某个方向的绿灯阶段，车辆通过交叉口前往允许的方向。当动作选择设置为选择可能的绿灯阶段之一时，深度强化学习代理在每个时间点$t$从这四种绿灯阶段中选择一个动作。在经历黄灯和红灯过渡后，所选动作将在交通信号灯上执行。成功的代理学习和安全交通也取决于正确的红灯和黄灯阶段定义。早期的应用将阶段定义简化为仅两个绿灯阶段，南北绿灯（NSG）和东西绿灯（EWG）[[32](#bib.bib32),
    [40](#bib.bib40)]，忽略了左转。另一种动作选择模型是二元动作，其中绿灯阶段间隔长度预先定义，并且在每个时间点$t$，代理决定是保持相同阶段还是进入预定义周期中的下一个阶段，例如，NSG
    $\to$ EWG $\to$ NSLG $\to$ EWLG。当代理选择更改阶段的动作时，在执行下一个绿灯阶段之前，首先执行黄灯和红灯过渡阶段，以确保安全的交通流[[33](#bib.bib33),
    [42](#bib.bib42), [37](#bib.bib37), [38](#bib.bib38), [47](#bib.bib47)]。
- en: Most of the applications consider discrete action selection from a set of actions,
    however there are also a few applications that consider continuous outputs [[20](#bib.bib20)],
    that only controls the duration of the next phase. This type of action definition
    mostly suitable for multiple intersections. Based on the predefined min and max
    phase duration, algorithm predicts a time length for the current phase [[41](#bib.bib41),
    [48](#bib.bib48)].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数应用考虑从一组动作中选择离散动作，但也有一些应用考虑连续输出 [[20](#bib.bib20)]，这仅控制下一阶段的持续时间。这种类型的动作定义最适合多个交叉口。根据预定义的最小和最大阶段持续时间，算法预测当前阶段的时间长度
    [[41](#bib.bib41), [48](#bib.bib48)]。
- en: IV-C Reward
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 奖励
- en: States in RL can be a feature vector or a high-dimensional matrix, and similarly
    actions can be a continuous value or a vector of discrete choices. However, reward
    is always a scalar value which is a function of the traffic data. The role of
    reward in RL is analyzing the quality of taken action with respect to the state,
    i.e., penalizing or awarding the agent for the corresponding action. Waiting time,
    cumulative delay, and queue length are the most common reward definitions in TSC.
    Waiting time is given by the sum of the times that vehicles are stopped. Delay
    is the difference between the waiting times of continuous green phases. Queue
    length is calculated for each lane in an intersection. A special congestion function
    in transportation planning defined by U.S. Bureau of Public Roads (BPR) is used
    in some works for the reward definition [[34](#bib.bib34), [47](#bib.bib47)].
    While in some works absolute value of the traffic data is used as a reward, in
    some others negative value or average value are also used.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的状态可以是特征向量或高维矩阵，类似地，动作可以是连续值或离散选择的向量。然而，奖励始终是一个标量值，它是交通数据的函数。奖励在强化学习中的作用是分析所采取动作相对于状态的质量，即对代理进行惩罚或奖励。等待时间、累计延迟和排队长度是交通信号控制中最常见的奖励定义。等待时间由车辆停滞的时间总和给出。延迟是连续绿灯阶段等待时间之间的差异。排队长度是为交叉口中的每条车道计算的。某些工作中使用美国公共道路局（BPR）定义的特殊拥堵函数作为奖励定义
    [[34](#bib.bib34), [47](#bib.bib47)]。虽然一些工作中使用了交通数据的绝对值作为奖励，但其他工作中也使用了负值或平均值。
- en: IV-D Neural Network Structure
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 神经网络结构
- en: 'The structure of deep neural networks has a high impact on learning in deep
    RL. Thus, different neural network structures are proposed for TSC in the literature.
    Multi-layer perceptron (MP), i.e., the standard fully connected neural network
    model, is a useful tool for classic data classification. An extension of multi-layer
    perceptron with kernel filters is convolutional neural network (CNN), which provide
    high performance on mapping image to an output. Standard DQN considers CNN that
    uses consecutive raw pixel frames for state definition. There are many TSC papers
    that use CNNs for DTSE state definitions (see Fig. [3](#S4.F3 "Figure 3 ‣ IV-A
    State ‣ IV Deep RL Settings for TSC ‣ Deep Reinforcement Learning for Intelligent
    Transportation Systems: A Survey")), e.g., [[31](#bib.bib31), [49](#bib.bib49),
    [33](#bib.bib33)]. Residual networks (ResNet) are used to deal with the overfitting
    problem in CNN-based deep network structures [[34](#bib.bib34)]. Another convolution-based
    network structure for operations in graphs is graph convolutional networks (GCN).
    Recurrent neural networks (RNN), e.g., Long Short-Term Memory (LSTM), are designed
    to work with sequential data. Since in TSC controlling is done sequentially, RNN
    is also used in deep RL settings [[35](#bib.bib35), [37](#bib.bib37)]. Another
    type of neural network model is autoencoder that learns an encoding for high-dimensional
    input data in a lower-dimensional subspace. The encoded input can be decoded to
    reconstruct the input, which is commonly used for clearing the noise on input
    data [[40](#bib.bib40)].'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '深度神经网络的结构对深度强化学习中的学习有着重大影响。因此，文献中提出了不同的神经网络结构用于时间序列分类（TSC）。多层感知器（MP），即标准的全连接神经网络模型，是经典数据分类的有用工具。多层感知器的扩展，具有卷积核的卷积神经网络（CNN），在将图像映射到输出上表现出色。标准DQN考虑了使用连续原始像素帧作为状态定义的CNN。有许多TSC论文使用CNN进行DTSE状态定义（见图[3](#S4.F3
    "Figure 3 ‣ IV-A State ‣ IV Deep RL Settings for TSC ‣ Deep Reinforcement Learning
    for Intelligent Transportation Systems: A Survey")），例如[[31](#bib.bib31)，[49](#bib.bib49)，[33](#bib.bib33)]。残差网络（ResNet）用于处理基于CNN的深度网络结构中的过拟合问题[[34](#bib.bib34)]。另一种用于图操作的卷积网络结构是图卷积网络（GCN）。递归神经网络（RNN），例如长短期记忆网络（LSTM），被设计用于处理序列数据。由于TSC中的控制是顺序进行的，因此RNN也被用于深度强化学习设置[[35](#bib.bib35)，[37](#bib.bib37)]。另一种神经网络模型是自编码器，它在较低维子空间中学习高维输入数据的编码。编码后的输入可以解码以重建输入，这通常用于清除输入数据上的噪声[[40](#bib.bib40)]。'
- en: IV-E Simulation environments
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 仿真环境
- en: 'RL and deep RL applications for TSC are mostly performed on traffic simulators
    due to life-threatening conditions in real-world experiments. Some authors also
    use real datasets for experimental study, but still they create a simulation environment
    based on the real data [[50](#bib.bib50)]. Microscopic individual vehicle-based
    simulators have been used throughout the years for ITS applications. The earliest
    available traffic simulator is the Java-based Green Light District (GLD) traffic
    simulator [[51](#bib.bib51)], that was initially proposed for an RL-based TSC
    problem. Many RL papers perform their experiments on the GLD simulator (see Table
    [II](#S5.T2 "TABLE II ‣ V-A2 Multi-agent RL ‣ V-A Standard RL applications ‣ V
    Deep RL Applications for TSC ‣ Deep Reinforcement Learning for Intelligent Transportation
    Systems: A Survey")), however the most popular open source traffic simulator is
    Simulation Urban Mobility (SUMO) [[52](#bib.bib52)]. Open source platforms allow
    users to modify the simulator for their purposes freely. SUMO enables users to
    interact with the environment using Python through the traffic control interface
    (TraCI) library. Different traffic models can be dynamically simulated, including
    personal vehicles, public vehicles and pedestrians. AIMSUN is a commercial traffic
    simulator designed and marketed by Transport Simulation Systems (Spain) [[53](#bib.bib53)].
    Paramics is one of the well-known traffic simulators distributed by Quadstone
    Paramics (UK) [[54](#bib.bib54)]. VISSIM [[55](#bib.bib55)] is a simulator preferred
    by researchers due to its interaction with MATLAB, similar to AIMSUN.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实世界实验中的生命威胁条件，TSC 的 RL 和深度 RL 应用主要在交通模拟器上进行。一些作者也使用真实数据集进行实验研究，但他们仍然基于真实数据创建模拟环境
    [[50](#bib.bib50)]。微观个体车辆基础的模拟器在 ITS 应用中已经使用多年。最早的交通模拟器是基于 Java 的 Green Light
    District (GLD) 交通模拟器 [[51](#bib.bib51)]，最初是为 RL 基于 TSC 问题提出的。许多 RL 论文在 GLD 模拟器上进行实验（见表
    [II](#S5.T2 "TABLE II ‣ V-A2 多智能体 RL ‣ V-A 标准 RL 应用 ‣ V 深度 RL 应用于 TSC ‣ 深度强化学习在智能交通系统中的应用：综述")），然而最受欢迎的开源交通模拟器是
    Simulation Urban Mobility (SUMO) [[52](#bib.bib52)]。开源平台允许用户自由修改模拟器以满足他们的需求。SUMO
    使用户可以通过交通控制接口 (TraCI) 库使用 Python 与环境互动。可以动态模拟不同的交通模型，包括个人车辆、公共车辆和行人。AIMSUN 是由
    Transport Simulation Systems（西班牙）设计和营销的商业交通模拟器 [[53](#bib.bib53)]。Paramics 是 Quadstone
    Paramics（英国）分发的著名交通模拟器之一 [[54](#bib.bib54)]。VISSIM [[55](#bib.bib55)] 是研究人员偏爱的模拟器，因为它与
    MATLAB 互动，类似于 AIMSUN。
- en: V Deep RL Applications for TSC
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 深度强化学习在交通信号控制中的应用
- en: 'This section focuses on (deep) RL studies for adaptive TSC. Summary of the
    works are shown in separate tables for both RL and deep RL models. We can classify
    learning-based models into two groups in terms of the number of agents: single
    agent RL which learns the optimal policy with one agent for the entire TSC network,
    and multi-agent RL which uses multiple agents in the network for acquiring optimal
    policy. For both standard RL and deep RL-based TSC works, we will discuss the
    proposed models based on their characteristic features such as state, action,
    reward definitions, and neural network structure.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点关注用于自适应 TSC 的（深度）RL 研究。RL 和深度 RL 模型的工作总结分别显示在不同的表格中。我们可以根据智能体的数量将学习型模型分为两类：单智能体
    RL，它使用一个智能体为整个 TSC 网络学习最佳策略，以及多智能体 RL，它在网络中使用多个智能体来获取最佳策略。对于标准 RL 和深度 RL 基于 TSC
    的工作，我们将根据其特征特征，如状态、动作、奖励定义和神经网络结构，讨论所提出的模型。
- en: V-A Standard RL applications
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 标准强化学习应用
- en: 'TABLE I: Outline of Single Agent RL approaches for TSC'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：单智能体 RL 方法在 TSC 中的概述
- en: '| Work | RL method | State | Action | Reward | Result comparison |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | RL 方法 | 状态 | 动作 | 奖励 | 结果比较 |'
- en: '| Thorpe et al. [[56](#bib.bib56)] | SARSA | Vehicle count Fixed vehicle distance
    Variable vehicle distance | Binary phase | Fixed penalty (-1) | Fixed-time Different
    states |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Thorpe 等人 [[56](#bib.bib56)] | SARSA | 车辆计数 固定车辆距离 可变车辆距离 | 二元阶段 | 固定惩罚（-1）
    | 固定时间 不同状态 |'
- en: '| Abudlhai et al. [[57](#bib.bib57)] | Q-learning | Queue length | Binary phase
    | Total delay | Fixed-time |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Abudlhai 等人 [[57](#bib.bib57)] | Q-学习 | 队列长度 | 二元阶段 | 总延迟 | 固定时间 |'
- en: '| Camponogara et al. [[58](#bib.bib58)] | Q-learning | Position & # vehicles
    | Green & Red phases | # waiting vehicles | Random policy Longest queue first
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Camponogara 等人 [[58](#bib.bib58)] | Q-学习 | 位置和车辆数量 | 绿色和红色阶段 | 等待车辆数量 | 随机策略
    最长队列优先 |'
- en: '| Wen et al. [[59](#bib.bib59)] | SARSA | # vehicles | Binary phase | Coefficients
    of state | Fixed-time Actuated control |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Wen 等人 [[59](#bib.bib59)] | SARSA | 车辆数量 | 二元阶段 | 状态系数 | 固定时间 激励控制 |'
- en: '| El-Tantawy et al. [[60](#bib.bib60)] | Q-learning | # vehicles / Queue length
    Queue length Cumulative delay | Green phases | Change in cum. delay | Fixed-time
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| El-Tantawy 等人 [[60](#bib.bib60)] | Q-learning | 车辆数量 / 队列长度 队列长度 累积延迟 | 绿灯相位
    | 累积延迟变化 | 固定时间 |'
- en: '| El-Tantawy et al. [[61](#bib.bib61)] | Q-learning SARSA TD error | # vehicles
    / Queue length Queue length Cumulative delay | Binary phase Green phases | Immediate
    delay Cumulative delay Queue length # stops | Fixed-time Actuated control |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| El-Tantawy 等人 [[61](#bib.bib61)] | Q-learning SARSA TD 错误 | 车辆数量 / 队列长度 队列长度
    累积延迟 | 二元相位 绿灯相位 | 即时延迟 累积延迟 队列长度 停车次数 | 固定时间 驱动控制 |'
- en: '| Shoufeng et al. [[62](#bib.bib62)] | Q-learning | Total delay | Time change
    in green phase | Total delay | Fixed-time |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Shoufeng 等人 [[62](#bib.bib62)] | Q-learning | 总延迟 | 绿灯相位时间变化 | 总延迟 | 固定时间
    |'
- en: '| Toubhi et al. [[63](#bib.bib63)] | Q-learning | Max. residual queue | Green
    phase duration | Queue length Cumulative delay Throughput | Vehicle demand |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Toubhi 等人 [[63](#bib.bib63)] | Q-learning | 最大残余队列 | 绿灯周期时长 | 队列长度 累积延迟 吞吐量
    | 车辆需求 |'
- en: V-A1 Single agent RL
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 单智能体 RL
- en: Optimizing intersections with a learning agent receives high attention from
    researchers since the second half of 1990s. The agent interacts with a simulation
    environment to learn an optimum control policy for traffic intersection using
    an RL algorithm. The ultimate goal is mostly controlling a network of coordinated
    intersections, but the initial step of this research is targeting how to control
    a single intersection with RL. Now we present some RL-based single intersection
    studies with their distinct features.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 自 1990 年代后期以来，优化交叉口的学习智能体受到研究人员的高度关注。智能体通过与模拟环境互动，学习使用 RL 算法对交通交叉口进行最佳控制策略。最终目标主要是控制协调的交叉口网络，但这项研究的初步步骤是针对如何使用
    RL 控制单个交叉口。现在我们呈现一些基于 RL 的单交叉口研究及其独特特征。
- en: 'Traffic signal control with RL-based machine learning is pioneered by the work
    [[56](#bib.bib56)], which applies the model-free SARSA algorithm on a single intersection.
    In this work, Thorpe and Anderson considered two scenarios: a four-lane intersection
    without yellow transition phase, and a $4\times 4$ grid style connected intersections
    where each intersection learns its own Q values separately. After this initial
    research, several solutions are proposed for single-intersection and multi-intersection
    traffic networks, where coordinated multi-agent and multi-objective RL dominate
    the RL research for adaptive TSC. Another SARSA-based TSC method for a single
    intersection is proposed by Wen et al. [[59](#bib.bib59)] with a stochastic control
    mechanism considering more realistic traffic situations. A specific state space
    is introduced in this work by partitioning the number of vehicles into sparse
    discrete values. Authors showed that their proposed model outperforms the fixed-time
    controller and actuated controller with respect to number of vehicles in queue.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 RL 的机器学习的交通信号控制由文献 [[56](#bib.bib56)] 开创，该文献在单个交叉口应用了无模型的 SARSA 算法。在这项工作中，Thorpe
    和 Anderson 考虑了两种场景：一个没有黄灯过渡阶段的四车道交叉口，以及一个 $4\times 4$ 网格风格的连接交叉口，其中每个交叉口分别学习自己的
    Q 值。在这项初步研究之后，提出了针对单交叉口和多交叉口交通网络的几种解决方案，其中协调的多智能体和多目标 RL 主导了适应性 TSC 的 RL 研究。Wen
    等人 [[59](#bib.bib59)] 提出了另一种基于 SARSA 的单交叉口 TSC 方法，采用了随机控制机制以考虑更现实的交通情况。该工作引入了特定的状态空间，通过将车辆数量划分为稀疏的离散值。作者展示了他们提出的模型在队列车辆数量方面优于固定时间控制器和驱动控制器。
- en: In [[57](#bib.bib57)], authors proposed a model-free Q-learning algorithm for
    a single intersection with queue length as the state representation and total
    delay between two action cycles as the reward function. This is the first paper
    that proposes a simple binary action model that switches the phase direction only.
    The results of this work are compared with the fixed-time signal controller in
    different traffic flow patterns in terms of average vehicle delay. A similar Q-learning
    based RL model is proposed by Camponogara and Kraus Jr. [[58](#bib.bib58)] based
    on a distributed Q-learning technique on two intersections by assigning separate
    Q values for each individual agent.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[57](#bib.bib57)] 中，作者提出了一种无模型的 Q-learning 算法，针对单交叉口，将队列长度作为状态表示，将两个动作周期之间的总延迟作为奖励函数。这是第一篇提出仅切换相位方向的简单二元动作模型的论文。这项工作的结果与不同交通流模式下的固定时间信号控制器进行了比较，比较的标准是平均车辆延迟。Camponogara
    和 Kraus Jr. [[58](#bib.bib58)] 提出了类似的基于 Q-learning 的 RL 模型，基于分布式 Q-learning 技术，在两个交叉口上为每个个体智能体分配了单独的
    Q 值。
- en: 'Abdulhai et al. [[60](#bib.bib60)], proposed the first RL-based real intersection
    scenario in Toronto, Canada by using Q-learning with three different state definitions.
    First state definition is a two-valued function: number of arriving vehicles to
    the current green direction and number of queued vehicles in the red direction.
    Other states are defined as queue length and cumulative delay regardless of traffic
    light. The variable-phase action model in this work selects a green phase among
    four possible phases defined for a single intersection (NSG, EWG, NSLG, EWLG)
    instead of a binary action model in a fixed cycle. The same work is extended to
    a more general concept discussing several on-policy, off-policy RL algorithms
    on various state, action, reward definitions in an experimental view [[61](#bib.bib61)].
    Along with the three state representations and the variable-phase action model
    in [[60](#bib.bib60)], authors tested their models with also the binary action
    model in a fixed green-phase cycle, and four reward functions, which are immediate
    delay, cumulative delay, queue length and the number of stops. Different RL algorithms,
    namely Q-learning, SARSA, and TD error are tested in different state, action,
    and reward settings on a single intersection. Further, two different multi-intersection
    configurations, 5 intersections in Toronto downtown and a large-scale network
    of Toronto downtown, are considered for comparison with fixed-time signal control,
    and actuated signal control models on Paramics simulator. Toubhi et al. [[63](#bib.bib63)],
    assessed three reward definitions, queue length, cumulative delay, and throughput,
    with Q-learning on a single intersection. The performance of each reward definition
    is explored on high demand and low demand traffic patterns. There are some other
    works that also deal with the single intersection control problem using the Q-learning
    approach [[62](#bib.bib62), [64](#bib.bib64)]. The summary of the presented works
    are given in Table [I](#S5.T1 "TABLE I ‣ V-A Standard RL applications ‣ V Deep
    RL Applications for TSC ‣ Deep Reinforcement Learning for Intelligent Transportation
    Systems: A Survey").'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Abdulhai 等人 [[60](#bib.bib60)] 提出了第一个基于 RL 的实际交叉口场景，该场景位于加拿大多伦多，使用了具有三种不同状态定义的
    Q-learning。第一个状态定义是一个二值函数：当前绿灯方向的到达车辆数量和红灯方向的排队车辆数量。其他状态被定义为队列长度和累积延迟，无论交通信号灯如何。该工作的可变相位动作模型在四种定义的相位中选择一个绿灯相位（NSG、EWG、NSLG、EWLG），而不是固定周期的二进制动作模型。同一工作扩展到一个更一般的概念，讨论了在实验视角下各种状态、动作、奖励定义的几种在线、离线
    RL 算法 [[61](#bib.bib61)]。除了 [[60](#bib.bib60)] 中的三种状态表示和可变相位动作模型外，作者还在固定绿灯相位周期下测试了他们的模型，以及四种奖励函数，即即时延迟、累积延迟、队列长度和停车次数。不同的
    RL 算法，包括 Q-learning、SARSA 和 TD 错误，在单一交叉口的不同状态、动作和奖励设置中进行了测试。此外，还考虑了两种不同的多交叉口配置，分别是多伦多市中心的
    5 个交叉口和多伦多市中心的大规模网络，以与 Paramics 模拟器上的固定时间信号控制和动作信号控制模型进行比较。Toubhi 等人 [[63](#bib.bib63)]
    使用 Q-learning 在单一交叉口上评估了三种奖励定义：队列长度、累积延迟和通行能力。每种奖励定义的性能在高需求和低需求交通模式下进行了探索。还有一些其他工作也使用
    Q-learning 方法处理单一交叉口控制问题 [[62](#bib.bib62), [64](#bib.bib64)]。所呈现工作的总结见表 [I](#S5.T1
    "TABLE I ‣ V-A 标准 RL 应用 ‣ V 深度 RL 应用于 TSC ‣ 深度强化学习在智能交通系统中的应用：综述")。
- en: V-A2 Multi-agent RL
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 多智能体 RL
- en: 'TABLE II: Overview of Multi-agent RL approaches for TSC'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：多智能体 RL 方法在 TSC 中的概述
- en: '| Work | RL method | Solution approach | Scenario | Simulator | Result comparison
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | RL 方法 | 解决方案方法 | 场景 | 模拟器 | 结果比较 |'
- en: '| Wiering [[65](#bib.bib65)] | Model-based RL | Waiting time sharing | 3 by
    2 grid | Not specified | Fixed-time controller Random controller Largest queue
    first |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Wiering [[65](#bib.bib65)] | 基于模型的 RL | 等待时间共享 | 3x2 网格 | 未指定 | 固定时间控制器 随机控制器
    最大队列优先 |'
- en: '| Steingrover et al. [[66](#bib.bib66)] | Model-based RL | Congestion value
    sharing | 12 mixed intersections | GDL | TC-1[[65](#bib.bib65)] |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Steingrover 等人 [[66](#bib.bib66)] | 基于模型的 RL | 拥堵值共享 | 12 个混合交叉口 | GDL |
    TC-1 [[65](#bib.bib65)] |'
- en: '| Iša et al. [[67](#bib.bib67)] | Model-based RL | Congestion & accident value
    sharing | 12 mixed intersections | GDL | TC-1 [[65](#bib.bib65)] TC-most Accident
    car removing |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Iša 等人 [[67](#bib.bib67)] | 基于模型的 RL | 拥堵与事故值共享 | 12 个混合交叉口 | GDL | TC-1
    [[65](#bib.bib65)] TC-most 事故车辆移除 |'
- en: '| Kuyer et al. [[68](#bib.bib68)] | Model-based RL | Coordination graph based
    max plus | 3 intersections 4 intersections 15 mixed intersections | GDL | TC-1
    [[65](#bib.bib65)] TC-SBC [[66](#bib.bib66)] Max-plus |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Kuyer 等人 [[68](#bib.bib68)] | 基于模型的强化学习 | 协调图最大加法 | 3 个交叉口 4 个交叉口 15 个混合交叉口
    | GDL | TC-1 [[65](#bib.bib65)] TC-SBC [[66](#bib.bib66)] 最大加法 |'
- en: '| Bakker et al. [[69](#bib.bib69)] | Model-based RL | Partially observed MDP
    | 15 mixed intersections | GDL | TC-1 [[65](#bib.bib65)] Diff. partial observation
    techniques |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Bakker 等人 [[69](#bib.bib69)] | 基于模型的强化学习 | 部分观测马尔可夫决策过程 | 15 个混合交叉口 | GDL
    | TC-1 [[65](#bib.bib65)] 不同的部分观测技术 |'
- en: '| Houli et al. [[70](#bib.bib70)] | Model-based RL | Multi-objective learning
    | Real road map in Beijing | Paramics | Fixed controller Actuated control Single
    agent RL |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Houli 等人 [[70](#bib.bib70)] | 基于模型的强化学习 | 多目标学习 | 北京的实际道路图 | Paramics | 固定控制器
    驱动控制 单代理强化学习 |'
- en: '| Brys et al. [[71](#bib.bib71)] | SARSA | Multi-objective learning Tile coding
    | 2 by 2 grid | AIM | Actuated control Distributed learning [[72](#bib.bib72)]
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Brys 等人 [[71](#bib.bib71)] | SARSA | 多目标学习 瓦片编码 | 2x2 网格 | AIM | 驱动控制 分布式学习
    [[72](#bib.bib72)] |'
- en: '| Khamis et al. [[73](#bib.bib73)] | Model-based RL with Bayesian trans. func.
    | Multi-objective learning | 12 mixed intersections | GLD | TC-1 [[65](#bib.bib65)]
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Khamis 等人 [[73](#bib.bib73)] | 基于模型的强化学习与贝叶斯转移函数 | 多目标学习 | 12 个混合交叉口 | GLD
    | TC-1 [[65](#bib.bib65)] |'
- en: '| Khamis et al. [[74](#bib.bib74)] | Model-based RL with Bayesian trans. func.
    | Multi-objective learning Agent Cooperation | 12 mixed intersections | GLD |
    TC-1 [[65](#bib.bib65)] |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Khamis 等人 [[74](#bib.bib74)] | 基于模型的强化学习与贝叶斯转移函数 | 多目标学习 代理合作 | 12 个混合交叉口
    | GLD | TC-1 [[65](#bib.bib65)] |'
- en: '| Khamis et al. [[75](#bib.bib75)] | Model-based RL with Bayesian trans. func.
    Hybrid exploration | Multi-objective learning Agent Cooperation | 22 mixed intersections
    | GLD | TC-1 [[65](#bib.bib65)] SOTL [[76](#bib.bib76)] |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Khamis 等人 [[75](#bib.bib75)] | 基于模型的强化学习与贝叶斯转移函数 混合探索 | 多目标学习 代理合作 | 22 个混合交叉口
    | GLD | TC-1 [[65](#bib.bib65)] SOTL [[76](#bib.bib76)] |'
- en: '| Jin et al. [[77](#bib.bib77)] | SARSA with function approximators | Multi-objective
    learning Threshold lexicographic ordering | 3 intersections in Stockholm | SUMO
    | Comparison between multiple function approximators |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Jin 等人 [[77](#bib.bib77)] | 带函数逼近器的SARSA | 多目标学习 阈值词典排序 | 斯德哥尔摩的 3 个交叉口 |
    SUMO | 多个函数逼近器之间的比较 |'
- en: '| Prashanth et al. [[78](#bib.bib78)] | Q-learning Actor-critic | Function
    approximation | 2 by 2 grid 5 intersectins | GLD | Fixed-time control No function
    approx. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Prashanth 等人 [[78](#bib.bib78)] | Q-learning 演员-评论家 | 函数逼近 | 2x2 网格 5 个交叉口
    | GLD | 固定时间控制 无函数逼近 |'
- en: '| Prashanth et al. [[79](#bib.bib79)] | Q-learning | Function approximation
    | 2 by 2 grid 3 by 3 grid 5 intersections 9 intersections | GLD | Fixed-time control
    No function approx. SOTL[[76](#bib.bib76)] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Prashanth 等人 [[79](#bib.bib79)] | Q-learning | 函数逼近 | 2x2 网格 3x3 网格 5 个交叉口
    9 个交叉口 | GLD | 固定时间控制 无函数逼近 SOTL [[76](#bib.bib76)] |'
- en: '| Pham et al. [[80](#bib.bib80)] | SARSA | Tile coding | 2 by 2 grid | AIM
    | Random RL Distributed learning [[72](#bib.bib72)] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Pham 等人 [[80](#bib.bib80)] | SARSA | 瓦片编码 | 2x2 网格 | AIM | 随机强化学习 分布式学习 [[72](#bib.bib72)]
    |'
- en: '| Abdoos et al. [[81](#bib.bib81)] | Q-learning | 2-level hierarchical control
    | 3 by 3 grid | AIMSUN | 1-level Q-learning |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Abdoos 等人 [[81](#bib.bib81)] | Q-learning | 两级层次控制 | 3x3 网格 | AIMSUN | 1级
    Q-learning |'
- en: '| Arel et al. [[82](#bib.bib82)] | Q-learning | Neural networks Hierarchical
    control | 5 intersections | MATLAB | Longest queue first |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Arel 等人 [[82](#bib.bib82)] | Q-learning | 神经网络 层次控制 | 5 个交叉口 | MATLAB | 最长排队优先
    |'
- en: '| El-Tantawy et al. [[83](#bib.bib83)] | Q-learning | Indirect coordination
    Direct coordination | 5 intersections | Paramics | Comp. between proposed models
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| El-Tantawy 等人 [[83](#bib.bib83)] | Q-learning | 间接协调 直接协调 | 5 个交叉口 | Paramics
    | 提议模型之间的比较 |'
- en: '| El-Tantawy et al. [[84](#bib.bib84)] | Q-learning | Indirect coordination
    Direct coordination | Real road map in downtown Toronto | Paramics | Fixed-time
    control Semi-actuated control Full actuated control |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| El-Tantawy 等人 [[84](#bib.bib84)] | Q-learning | 间接协调 直接协调 | 多伦多市中心的实际道路图
    | Paramics | 固定时间控制 半驱动控制 全驱动控制 |'
- en: '| Salkham et al. [[85](#bib.bib85)] | Q-learning | Adaptive round robin based
    collaboration | Real road map in Dublin City | UTC | Independent RL Fixed-time
    SAT-like[[86](#bib.bib86)] |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Salkham 等人 [[85](#bib.bib85)] | Q-learning | 自适应轮流协作 | 都柏林市的实际道路图 | UTC |
    独立强化学习 固定时间 SAT 类似 [[86](#bib.bib86)] |'
- en: '| Aziz et al. [[87](#bib.bib87)] | Av. expected reward | Multi-reward structure
    | 8 intersections 11 intersections | VISSIM | Q-learning SARSA Fixed-time control
    Adaptive control |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Aziz 等人 [[87](#bib.bib87)] | 平均期望回报 | 多重奖励结构 | 8 个交叉口 11 个交叉口 | VISSIM |
    Q-learning SARSA 固定时间控制 自适应控制 |'
- en: '| Aslani et al. [[88](#bib.bib88)] | Actor-critic | Tile coding Radial basis
    functions | Real road map in downtown Tehran | AIMSUN | Q-learning Fixed-time
    control Actuated control |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Aslani 等人 [[88](#bib.bib88)] | Actor-critic | 瓷砖编码 径向基函数 | 德黑兰市中心真实道路地图 |
    AIMSUN | Q-learning 固定时间控制 反馈控制 |'
- en: '| Xu et al. [[89](#bib.bib89)] | Q-learning | Non-zero sum based Markov game
    | 3 by 3 grid | MATLAB | Ind. Q-learning Fixed-time control Longest queue first
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Xu 等人 [[89](#bib.bib89)] | Q-learning | 非零和基于马尔可夫游戏 | 3x3 网格 | MATLAB | 独立
    Q-learning 固定时间控制 最长队列优先 |'
- en: '| Abdoos et al. [[90](#bib.bib90)] | Q-learning | State discretization | 50
    intersections | AIMSUN | Fixed-time control |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Abdoos 等人 [[90](#bib.bib90)] | Q-learning | 状态离散化 | 50 个交叉口 | AIMSUN | 固定时间控制
    |'
- en: '| Balaji et al. [[91](#bib.bib91)] | Q-learning | Neighboor cooperation | Real
    road map in Singapore | Paramics | Hierarchical MS [[92](#bib.bib92)] Cooperative
    ensemble Actuated control |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Balaji 等人 [[91](#bib.bib91)] | Q-learning | 邻里合作 | 新加坡真实道路地图 | Paramics |
    分层 MS [[92](#bib.bib92)] 协同集成 反馈控制 |'
- en: '| Cahill et al. [[93](#bib.bib93)] | Q-learning | CUSUM-based pattern change
    detection | Real road map in Dublin City | UTC | SAT-like[[86](#bib.bib86)] |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Cahill 等人 [[93](#bib.bib93)] | Q-learning | 基于 CUSUM 的模式变化检测 | 都柏林市真实道路地图
    | UTC | 类 SAT[[86](#bib.bib86)] |'
- en: '| Araghi et al. [[94](#bib.bib94)] | Q-learning | Distributed learning | 3
    by 3 grid | Paramics | Fixed-time control |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Araghi 等人 [[94](#bib.bib94)] | Q-learning | 分布式学习 | 3x3 网格 | Paramics | 固定时间控制
    |'
- en: Applying single agent RL algorithms individually on different intersections
    can be a good solution up to a certain point, however large intersection networks
    suffer from this approach. A cooperative learning approach is needed to reach
    an optimum policy over all network. Several multi-agent learning models are proposed
    for controlling multiple intersections cooperatively.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 单独在不同交叉口应用单一智能体 RL 算法可以在一定程度上提供良好的解决方案，但大规模交叉口网络则会受到这种方法的限制。需要一种合作学习方法，以便在整个网络中达到最优策略。已经提出了几种多智能体学习模型，用于协同控制多个交叉口。
- en: 'While most of the RL applications for single intersection consider model-free
    algorithms, such as Q-learning and SARSA, early multi-agent papers proposed model-based
    RL algorithms that form a transition probability distribution. A prominent multi-agent
    RL work for large traffic networks is [[65](#bib.bib65)] by Wiering, in which
    three algorithms were proposed, namely TC-1, TC-2, TC-3, based on the coordination
    between vehicles and intersections considering local and global information for
    the state function. States are formed based on the traffic light configuration
    of intersections, position of the vehicles and the destination of the vehicles
    at each intersection. The approach for creating a state representation in this
    early work is not realistic due to unknown destination for each vehicle. The proposed
    models iteratively update value functions to minimize the waiting times of vehicles.
    The results are compared with four standard TSC models: fixed-time control, random
    control, longest queue first, and most-car model. Several works extended the Wiering’s
    approach in different perspectives. For example, Steingrover et al. proposed an
    extension to the TC-1 method by including congestion information on other intersections
    [[66](#bib.bib66)]. Two different extensions, called TC-SBC and TC-GAC, are proposed
    by the authors. The former increases the state size by adding congestion values
    to the state space, whereas the latter uses a congestion factor while computing
    the value function instead of increasing the state space. Isa et al. [[67](#bib.bib67)]
    proposed a further improvement to the TC-1 method by including congestion and
    accident information in the state representation, which further increases the
    state representation. While the works presented until now does not consider coordination
    between agents for joint action selection, Kuyer et al. introduces a new approach
    that enables coordination between agents by using the max-plus algorithm [[68](#bib.bib68)].
    In this model, agents coordinate with each other to reach optimum joint actions
    in finite iterations. Another multi-agent RL model is proposed by Bakker et al.
    [[69](#bib.bib69)] with partial observations for the state spaces of connected
    intersections. This case is of interest when the system cannot access the full
    state information due to some reasons such as faulty sensors. All these works
    [[66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69)] use Wiering’s
    approach [[65](#bib.bib65)] as a benchmark.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数单交叉口的强化学习应用考虑的是无模型算法，如Q学习和SARSA，但早期的多智能体论文提出了基于模型的强化学习算法，这些算法形成了一个转移概率分布。Wiering的[[65](#bib.bib65)]是一个著名的大型交通网络的多智能体强化学习工作，其中提出了三种算法，即TC-1、TC-2、TC-3，这些算法基于车辆与交叉口之间的协调，考虑了状态函数的局部和全局信息。状态是基于交叉口的交通灯配置、车辆的位置以及每个交叉口的车辆目的地来形成的。由于每辆车的目的地未知，这项早期工作的状态表示方法并不现实。提出的模型通过迭代更新价值函数来最小化车辆的等待时间。结果与四种标准交通信号控制模型进行了比较：定时控制、随机控制、最长队列优先和最多车辆模型。几个研究从不同角度扩展了Wiering的方法。例如，Steingrover等人通过在其他交叉口中包含拥堵信息来扩展TC-1方法[[66](#bib.bib66)]。作者提出了两个不同的扩展，称为TC-SBC和TC-GAC。前者通过将拥堵值添加到状态空间中来增加状态大小，而后者则在计算价值函数时使用拥堵因子，而不是增加状态空间。Isa等人[[67](#bib.bib67)]进一步改进了TC-1方法，将拥堵和事故信息纳入状态表示中，进一步增加了状态表示。虽然到目前为止的工作没有考虑智能体之间的协调以进行联合行动选择，Kuyer等人引入了一种新的方法，通过使用max-plus算法来实现智能体之间的协调[[68](#bib.bib68)]。在这个模型中，智能体通过有限的迭代协调彼此，以达到最优的联合行动。Bakker等人[[69](#bib.bib69)]提出了另一种具有部分观察的多智能体强化学习模型，用于连接交叉口的状态空间。当系统由于一些原因（如传感器故障）无法访问完整的状态信息时，这种情况很有趣。所有这些工作[[66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69)]都使用了Wiering的方法[[65](#bib.bib65)]作为基准。
- en: Multi-objectivity is gaining popularity in RL [[95](#bib.bib95)] due to its
    capabilities in complex environments. When a single objective is selected for
    the overall traffic system, such as the Wiering’s objective which aims to decrease
    the waiting time of all vehicles, it may not serve well the needs of different
    traffic conditions. Authors in [[70](#bib.bib70)] consider a multi-objective approach
    in their multi-agent RL work for TSC. In particular, vehicle stops, average waiting
    time, and maximum queue length are targeted as objectives for low, medium, and
    high traffic volume, respectively. Different Q functions are updated with appropriate
    reward functions in these three traffic conditions. Taylor et al. proposes a non-RL
    based basic learning algorithm called Distributed Coordination of Exploration
    and Exploitation (DCEE) [[72](#bib.bib72)] to tackle the TSC problem. Authors
    in [[71](#bib.bib71)] and [[80](#bib.bib80)] consider a multi-agent RL-based SARSA
    algorithm with tile coding and compare it with DCEE under different traffic conditions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标性在强化学习（RL）中越来越受欢迎[[95](#bib.bib95)]，因其在复杂环境中的能力。当为整体交通系统选择单一目标时，例如旨在减少所有车辆等待时间的Wiering目标，它可能无法很好地满足不同交通条件的需求。[[70](#bib.bib70)]中的作者在他们的多智能体RL交通信号控制（TSC）工作中考虑了多目标方法。特别是，车辆停靠、平均等待时间和最大排队长度分别作为低、中和高交通量的目标。在这三种交通条件下，不同的Q函数与适当的奖励函数一起更新。Taylor等人提出了一种非RL基础的基本学习算法，称为分布式协调探索与利用（DCEE）[[72](#bib.bib72)]，以解决TSC问题。[[71](#bib.bib71)]和[[80](#bib.bib80)]中的作者考虑了一种基于多智能体RL的SARSA算法，采用了瓷砖编码，并在不同交通条件下与DCEE进行了比较。
- en: 'Khamis et al. studied multi-objective RL control for traffic signals in three
    papers [[73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75)]. In the first paper
    [[73](#bib.bib73)], authors considered Bayesian transition probability for model-based
    RL using several objectives for forming the reward function. The second paper
    followed the same approach [[74](#bib.bib74)] with more specific objectives. The
    third paper [[75](#bib.bib75)] extends the previous works to a total of seven
    objectives with a novel cooperative exploration function and experiments in several
    road conditions and vehicle demands. The paper also improves the practicality
    of GLD traffic simulator from different perspectives, e.g., continuous control,
    probabilistic travel demand. The results of these three papers are compared with
    TC-1 proposed by Wiering [[65](#bib.bib65)] and the adaptive SOTL method [[76](#bib.bib76)].
    The latest and most compact RL-based multi-objective multi-agent TSC study is
    presented in [[77](#bib.bib77)]. In this work, travel delay and fuel consumption
    are defined as learning objectives for the RL agents, and a specific technique
    called threshold lexicographic ordering is used for online multi-objective adaptation.
    SARSA is experimented in this work with several function approximators, one of
    which is based on neural networks. It is worth noting that SARSA with Q-value
    estimation is not considered a deep RL approach since it does not include experience
    replay and target network tricks discussed in [III-B1](#S3.SS2.SSS1 "III-B1 Deep
    Q-Network ‣ III-B Deep Reinforcement Learning ‣ III Deep RL: An Overview ‣ Deep
    Reinforcement Learning for Intelligent Transportation Systems: A Survey").'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 'Khamis等人研究了交通信号的多目标RL控制，发表了三篇论文[[73](#bib.bib73)、[74](#bib.bib74)、[75](#bib.bib75)]。在第一篇论文[[73](#bib.bib73)]中，作者考虑了用于模型基础RL的贝叶斯转移概率，并使用多个目标来形成奖励函数。第二篇论文[[74](#bib.bib74)]采用了相同的方法，但目标更为具体。第三篇论文[[75](#bib.bib75)]将之前的工作扩展到总共七个目标，提出了一种新颖的协作探索函数，并在几种道路条件和车辆需求下进行了实验。该论文还从不同角度提高了GLD交通模拟器的实用性，例如连续控制和概率旅行需求。这三篇论文的结果与Wiering提出的TC-1[[65](#bib.bib65)]和自适应SOTL方法[[76](#bib.bib76)]进行了比较。最新且最紧凑的基于RL的多目标多智能体TSC研究在[[77](#bib.bib77)]中提出。在这项工作中，旅行延迟和燃油消耗被定义为RL智能体的学习目标，并使用了一种称为阈值词典排序的特定技术进行在线多目标适应。本研究中的SARSA在几种函数近似器上进行了实验，其中之一基于神经网络。值得注意的是，带有Q值估计的SARSA不被视为深度RL方法，因为它不包括[III-B1](#S3.SS2.SSS1
    "III-B1 Deep Q-Network ‣ III-B Deep Reinforcement Learning ‣ III Deep RL: An Overview
    ‣ Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey")中讨论的经验重放和目标网络技巧。'
- en: 'Before DQN was introduced, function approximators were popular for Q-functions
    with large state space. For instance, authors in [[78](#bib.bib78), [79](#bib.bib79)]
    proposed two RL models for TSC using function approximation-based Q-learning and
    actor-critic policy iteration. The proposed Q-learning method outperforms standard
    Q-learning with full state representation. A novel neural network-based multi-agent
    RL for TSC is proposed in [[82](#bib.bib82)], which uses local agents and global
    agents. While local agent controls the traffic lights via the longest queue first
    algorithm, global agent controls the traffic lights with a neural network-based
    Q-learning approach, which is very similar to DQN discussed in [III-B](#S3.SS2
    "III-B Deep Reinforcement Learning ‣ III Deep RL: An Overview ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey").'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '在DQN引入之前，函数逼近器在大状态空间的Q函数中很受欢迎。例如，[[78](#bib.bib78), [79](#bib.bib79)]中的作者提出了两种使用基于函数逼近的Q学习和演员-评论员策略迭代的TSC强化学习模型。所提出的Q学习方法在全面状态表示下优于标准Q学习。[[82](#bib.bib82)]中提出了一种新型的基于神经网络的多智能体强化学习方法，用于TSC，该方法使用本地智能体和全局智能体。虽然本地智能体通过“最长队列优先”算法控制交通信号灯，但全局智能体则使用基于神经网络的Q学习方法控制交通信号灯，这与[III-B](#S3.SS2
    "III-B Deep Reinforcement Learning ‣ III Deep RL: An Overview ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey")中讨论的DQN非常相似。'
- en: Actor-critic-based multi-agent RL is an emerging field that uses continuous
    state representation. Discretizing the state space is prone to missing information
    about the state. Aslani et al. proposed a continuous space actor-critic control
    model for multiple intersections [[88](#bib.bib88)], in which tile coding and
    radial basis-based function approximators are presented. Although state space
    is continuous, action space to determine the duration of the next green phase
    is discrete. In experiments, discrete and continuous state space based actor-critic
    models are tested in the city of Tehran. In another work, two-layer hierarchical
    multi-agent RL method is studied [[81](#bib.bib81)], which implements a single
    agent for each intersection using Q-learning, and controls a wide area network
    with function approximator based on tile coding on second layer.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 基于演员-评论员的多智能体强化学习是一个新兴领域，它使用连续状态表示。离散化状态空间容易导致丢失关于状态的信息。Aslani 等人提出了一种用于多个交叉口的连续空间演员-评论员控制模型[[88](#bib.bib88)]，其中介绍了网格编码和基于径向基函数的逼近器。尽管状态空间是连续的，但确定下一个绿灯周期的时间的动作空间是离散的。在实验中，离散和连续状态空间的演员-评论员模型在德黑兰市进行了测试。在另一项研究中，探讨了两层分层多智能体强化学习方法[[81](#bib.bib81)]，该方法在每个交叉口实现一个单独的智能体，使用Q学习，并在第二层使用基于网格编码的函数逼近器控制广泛的区域网络。
- en: There are several studies offering coordination between the neighbor agents
    for reaching a joint optimum performance. To this end, Tantawy et al. proposed
    a Q-learning based multi-agent RL approach for road network coordination [[83](#bib.bib83),
    [84](#bib.bib84)]. RL agents learn the coordination directly or indirectly, called
    MARLIN-DC and MARLIN-IC. While a small-scale road network is presented in [[83](#bib.bib83)],
    in the extended paper [[84](#bib.bib84)] authors investigate a large network of
    59 intersections in downtown Toronto. [[91](#bib.bib91)] presents another coordination-based
    TSC model implementing distributed Q-learning agents for a large network where
    neighbour agents share congestion values with each other. Experiments are performed
    on the Paramics simulation environment using a real traffic network in Singapore
    with different travel demand configurations. Xu et al. [[89](#bib.bib89)] proposed
    a coordination module based on nonzero-sum Markov game for the multi-agent RL
    environment. Q-learning is used on each intersection as a single agent, and their
    coordination is controlled with a Markov game-based mathematical model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个研究提供了邻近智能体之间的协调以达到共同的最佳性能。为此，Tantawy 等人提出了一种基于Q学习的多智能体强化学习方法，用于道路网络协调[[83](#bib.bib83),
    [84](#bib.bib84)]。强化学习智能体直接或间接地学习协调，称为MARLIN-DC和MARLIN-IC。在[[83](#bib.bib83)]中介绍了一个小规模的道路网络，而在扩展的论文[[84](#bib.bib84)]中，作者研究了多伦多市中心一个有59个交叉口的大型网络。[[91](#bib.bib91)]提出了另一种基于协调的TSC模型，为大型网络实现了分布式Q学习智能体，其中邻近智能体之间共享拥堵值。实验在Paramics模拟环境中使用新加坡的实际交通网络进行，配置了不同的旅行需求。Xu
    等人[[89](#bib.bib89)]提出了一个基于非零和Markov游戏的协调模块，用于多智能体强化学习环境。Q学习在每个交叉口作为单个智能体使用，其协调通过基于Markov游戏的数学模型进行控制。
- en: 'A new technique for multiple intersection environments is proposed in [[87](#bib.bib87)]
    using the R-Markov Average Reward technique and a multi-objective reward definition
    for RL. The result of this work is compared with fixed-time controller, actuated
    controller, Q-learning and SARSA on the Paramics simulation environment by simulating
    an 18-intersection network. Chu et al. [[96](#bib.bib96)], proposed a regional
    to central multi-agent RL model for large-scale traffic networks. In low traffic
    density, authors claim that for large-scale networks collaboration is not needed
    between regions, i.e., learning the traffic model in local region is enough to
    attain a globally appropriate learning. Araghi et al., [[94](#bib.bib94)] presents
    a distributed Q-learning based multi-agent RL controller that predicts the green
    phase duration on the next phase cycle. Other multi-agent RL applications are
    studied in [[90](#bib.bib90), [93](#bib.bib93), [85](#bib.bib85)]. Table [II](#S5.T2
    "TABLE II ‣ V-A2 Multi-agent RL ‣ V-A Standard RL applications ‣ V Deep RL Applications
    for TSC ‣ Deep Reinforcement Learning for Intelligent Transportation Systems:
    A Survey") gives an overview of the multi-agent RL works.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[87](#bib.bib87)]中提出了一种用于多交叉口环境的新技术，使用了R-马尔可夫平均奖励技术和多目标奖励定义进行RL。该工作的结果与固定时间控制器、激活控制器、Q-learning和SARSA在Paramics模拟环境中进行比较，模拟了一个18个交叉口的网络。Chu等人[[96](#bib.bib96)]提出了一种区域到中央的多智能体RL模型，用于大规模交通网络。在低交通密度情况下，作者声称大规模网络中区域间的协作并不必要，即学习局部区域的交通模型足以实现全局适当学习。Araghi等人[[94](#bib.bib94)]提出了一种基于分布式Q-learning的多智能体RL控制器，该控制器预测下一个相位周期的绿灯持续时间。其他多智能体RL应用在[[90](#bib.bib90),
    [93](#bib.bib93), [85](#bib.bib85)]中进行了研究。表[II](#S5.T2 "TABLE II ‣ V-A2 多智能体RL
    ‣ V-A 标准RL应用 ‣ V 深度强化学习在TSC中的应用 ‣ 深度强化学习在智能交通系统中的调查")给出了多智能体RL工作的概述。
- en: V-B Deep RL applications
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 深度强化学习应用
- en: 'Here we discuss deep RL-based TSC applications considering. A summary of the
    discussed works is provided in Table [III](#S5.T3 "TABLE III ‣ V-B1 Single agent
    deep RL ‣ V-B Deep RL applications ‣ V Deep RL Applications for TSC ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey") considering the used
    deep RL algorithms, network structures, simulation environments, and comparison
    with benchmarks.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了基于深度强化学习的TSC应用。讨论内容的总结见表格[III](#S5.T3 "TABLE III ‣ V-B1 单智能体深度强化学习
    ‣ V-B 深度强化学习应用 ‣ V 深度强化学习在TSC中的应用 ‣ 深度强化学习在智能交通系统中的调查")，涵盖了使用的深度强化学习算法、网络结构、模拟环境及与基准的比较。
- en: V-B1 Single agent deep RL
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 单智能体深度强化学习
- en: 'TABLE III: Outline of Deep RL approaches for TSC.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：TSC的深度强化学习方法概述。
- en: '| Work | Deep RL neural network structure | Multi-agent | State - DTSE | Scenario
    | Simulator | Result comparison |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 深度强化学习神经网络结构 | 多智能体 | 状态 - DTSE | 场景 | 模拟器 | 结果比较 |'
- en: '| Genders et al. [[31](#bib.bib31)] | DQN - CNN | No | Yes | Single int. |
    SUMO | MP(64) DQN |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Genders等人 [[31](#bib.bib31)] | DQN - CNN | 否 | 是 | 单交叉口 | SUMO | MP(64) DQN
    |'
- en: '| Van der Pool et al. [[49](#bib.bib49)] | DQN - CNN | Max-plus Transfer planning
    | Yes | Single int. 2 intersections 3 intersections 2 by 2 grid | SUMO | Model
    based RL |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Van der Pool等人 [[49](#bib.bib49)] | DQN - CNN | 最大加法传输规划 | 是 | 单交叉口，2个交叉口，3个交叉口，2x2网格
    | SUMO | 基于模型的RL |'
- en: '| Van der Pool et al. [[32](#bib.bib32)] | DQN - CNN | Max-plus Transfer planning
    | Yes | 2 intersections 3 intersections 2 by 2 grid | SUMO | Model based RL |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Van der Pool等人 [[32](#bib.bib32)] | DQN - CNN | 最大加法传输规划 | 是 | 2个交叉口，3个交叉口，2x2网格
    | SUMO | 基于模型的RL |'
- en: '| Li et al.[[40](#bib.bib40)] | DQN - Autoencoder | No | No | Single int. |
    Paramics | Q-learning |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Li等人 [[40](#bib.bib40)] | DQN - 自编码器 | 否 | 否 | 单交叉口 | Paramics | Q-learning
    |'
- en: '| Gao et al. [[33](#bib.bib33)] | DQN - CNN | No | Yes | Single int. | SUMO
    | Fixed-time control Longest queue first |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Gao等人 [[33](#bib.bib33)] | DQN - CNN | 否 | 是 | 单交叉口 | SUMO | 固定时间控制 最长队列优先
    |'
- en: '| Liu et al. [[34](#bib.bib34)] | DQN-ResNeT[[97](#bib.bib97)] | Policy sharing
    | Yes | 2 by 2 grid | SUMO | SOTL DQN without CNN Q-learning |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Liu等人 [[34](#bib.bib34)] | DQN-ResNeT[[97](#bib.bib97)] | 策略共享 | 是 | 2x2网格
    | SUMO | SOTL DQN无CNN Q-learning |'
- en: '| Casas [[41](#bib.bib41)] | DDPG | Multiple actor- critic learner | No | Single
    int., 6 intersections, Real map from Barcelona | Aumsim | Q-learning Random |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Casas [[41](#bib.bib41)] | DDPG | 多演员-评论员学习者 | 否 | 单交叉口，6个交叉口，来自巴塞罗那的真实地图
    | Aumsim | Q-learning 随机 |'
- en: '| Shi et al. [[35](#bib.bib35)] | DQN-RNN | Max-plus Transfer planning | Yes
    | 2 by 2 grid | USTCMTS2.1 | Fixed-time control Q-learning |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Shi等人 [[35](#bib.bib35)] | DQN-RNN | 最大加法传输规划 | 是 | 2x2网格 | USTCMTS2.1 |
    固定时间控制 Q-learning |'
- en: '| Mousavi et al. [[28](#bib.bib28)] | DQN-CNN A2C-CNN | No | Real image | Single
    int. | SUMO | Fixed-time control |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| Mousavi et al. [[28](#bib.bib28)] | DQN-CNN A2C-CNN | 否 | 真实图像 | 单个交叉口 |
    SUMO | 固定时间控制 |'
- en: '| Lin et al. [[42](#bib.bib42)] | A2C-CNN | Multiple actor- critic learners
    | No | 3 by 3 grid | SUMO | Fixed-time control Actuated control |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Lin et al. [[42](#bib.bib42)] | A2C-CNN | 多个演员-评论家学习者 | 否 | 3x3 网格 | SUMO
    | 固定时间控制 驱动控制 |'
- en: '| Genders et al. [[98](#bib.bib98)] | A3C-MP | No | Yes | Single int. | SUMO
    | Actuated control |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Genders et al. [[98](#bib.bib98)] | A3C-MP | 否 | 是 | 单个交叉口 | SUMO | 驱动控制
    |'
- en: '| Shabestary et al. [[36](#bib.bib36)] | DQN-CNN | No | Yes | Single int. |
    Paramics | Different rewards Q-learning |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Shabestary et al. [[36](#bib.bib36)] | DQN-CNN | 否 | 是 | 单个交叉口 | Paramics
    | 不同奖励 Q-learning |'
- en: '| Choe et al. [[37](#bib.bib37)] | DQN-RNN | No | Yes | Single int. | SUMO
    | CNN-DQN |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Choe et al. [[37](#bib.bib37)] | DQN-RNN | 否 | 是 | 单个交叉口 | SUMO | CNN-DQN
    |'
- en: '| Garg et al. [[29](#bib.bib29)] | DQN-CNN (Policy based) | No | Yes | Single
    int. | Unity3d | Fixed-time control No traffic light |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Garg et al. [[29](#bib.bib29)] | DQN-CNN（基于策略） | 否 | 是 | 单个交叉口 | Unity3d
    | 固定时间控制 无交通灯 |'
- en: '| Coskun et al. [[99](#bib.bib99)] | DQN-CNN Actor-Critic | Joint learning
    | No | 4 intersections | SUMO | DQN(Policy based) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Coskun et al. [[99](#bib.bib99)] | DQN-CNN Actor-Critic | 联合学习 | 否 | 4 个交叉口
    | SUMO | DQN（基于策略） |'
- en: '| Wei et al. [[38](#bib.bib38)] | DQN-CNN | No | Yes | Single int. | SUMO Real
    dataset | Fixed-time SOTL |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Wei et al. [[38](#bib.bib38)] | DQN-CNN | 否 | 是 | 单个交叉口 | SUMO 真实数据集 | 固定时间
    SOTL |'
- en: '| Natafgi et al. [[50](#bib.bib50)] | DQN-CNN | No | No | Single int. | SUMO
    | Fixed tim |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Natafgi et al. [[50](#bib.bib50)] | DQN-CNN | 否 | 否 | 单个交叉口 | SUMO | 固定时间
    |'
- en: '| Nishi et al. [[100](#bib.bib100)] | NFQI-Graph CNN [[101](#bib.bib101)] |
    No | No | 6 intersections | SUMO | Fixed-time CNN-DQN |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Nishi et al. [[100](#bib.bib100)] | NFQI-Graph CNN [[101](#bib.bib101)] |
    否 | 否 | 6 个交叉口 | SUMO | 固定时间 CNN-DQN |'
- en: '| Wan et al. [[45](#bib.bib45)] | Modified DQN | No | No | Single int. | VISSIM
    | DQN Fixed-time control |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Wan et al. [[45](#bib.bib45)] | 修改版 DQN | 否 | 否 | 单个交叉口 | VISSIM | DQN 固定时间控制
    |'
- en: '| Calvo et al. [[39](#bib.bib39)] | DQN-CNN | Independent DQN fingerprints
    | Yes | 3 intersections | SUMO | Fixed-time control |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Calvo et al. [[39](#bib.bib39)] | DQN-CNN | 独立 DQN 指纹 | 是 | 3 个交叉口 | SUMO
    | 固定时间控制 |'
- en: '| Genders [[48](#bib.bib48)] | DDPG | Multiple learners | No | Real map from
    Luxemburg | SUMO | Fixed-time control |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Genders [[48](#bib.bib48)] | DDPG | 多个学习者 | 否 | 卢森堡的真实地图 | SUMO | 固定时间控制
    |'
- en: '| Chu et al. [[102](#bib.bib102)] | A2C-RNN | Policy sharing | No | 5 by 5
    grid Monaco city map | SUMO | Ind-Q-learning Ind-DQN Ind-A2C |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Chu et al. [[102](#bib.bib102)] | A2C-RNN | 策略共享 | 否 | 5x5 网格 摩纳哥城市地图 | SUMO
    | Ind-Q-learning Ind-DQN Ind-A2C |'
- en: '| Liang et al. [[30](#bib.bib30)] | Double Dueling DQN-CNN | No | Yes | Single
    int. | SUMO | Fixed-time control Actuated control DQN |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Liang et al. [[30](#bib.bib30)] | 双重对抗 DQN-CNN | 否 | 是 | 单个交叉口 | SUMO | 固定时间控制
    驱动控制 DQN |'
- en: '| Genders et al. [[103](#bib.bib103)] | Asynchronous n-step Q-learning | No
    | No | Single int. | SUMO | Linear learning Actuated control Random control |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Genders et al. [[103](#bib.bib103)] | 异步 n 步 Q-learning | 否 | 否 | 单个交叉口 |
    SUMO | 线性学习 驱动控制 随机控制 |'
- en: '| Zhou et al. [[44](#bib.bib44)] | DQN-MP | Threshold based | No | Real map
    from New york city | SUMO | Diff veh. demands |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Zhou et al. [[44](#bib.bib44)] | DQN-MP | 基于阈值 | 否 | 纽约市的真实地图 | SUMO | 不同车辆需求
    |'
- en: '| Xu et al. [[47](#bib.bib47)] | DQN-RNN | Critical node discovery | No | 20
    intersections 50 intersections 100 intersections | SUMO | Fixed-time SOTL Q-learning
    DQN |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. [[47](#bib.bib47)] | DQN-RNN | 关键节点发现 | 否 | 20 个交叉口 50 个交叉口 100
    个交叉口 | SUMO | 固定时间 SOTL Q-learning DQN |'
- en: '| Tan et al. [[104](#bib.bib104)] | DQN (Value based) DDPG (Wolpertinger) |
    Hierarchical cooperation | No | 6 intersections 12 intersections 24 intersections
    | SUMO | Fixed-time control Q-learning DQN |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Tan et al. [[104](#bib.bib104)] | DQN（基于价值） DDPG（Wolpertinger） | 分层合作 | 否
    | 6 个交叉口 12 个交叉口 24 个交叉口 | SUMO | 固定时间控制 Q-learning DQN |'
- en: '| Ge et al. [[46](#bib.bib46)] | DQN-CNN | Q value transfer | Yes | Heterogeneous
    4 int. 2 by 3 grid | SUMO | Dist. Q-learning DQN |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Ge et al. [[46](#bib.bib46)] | DQN-CNN | Q 值传递 | 是 | 异质 4 个交叉口 2x3 网格 | SUMO
    | 分布式 Q-learning DQN |'
- en: '| Liu et al. [[105](#bib.bib105)] | DQN-MP | No | No | Single int. 4 intersections
    | Python | No comparison |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. [[105](#bib.bib105)] | DQN-MP | 否 | 否 | 单个交叉口 4 个交叉点 | Python
    | 无对比 |'
- en: '| Zhang et al. [[106](#bib.bib106)] | DQN | No | No | Arterial topology 4 by
    4 grid | SUMO | Partially Observable states |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. [[106](#bib.bib106)] | DQN | 否 | 否 | 动脉拓扑 4x4 网格 | SUMO | 部分可观察状态
    |'
- en: In recent years, deep RL based learning tools for adaptive intersection controls
    gained a great attention from transportation researchers. After researchers proposed
    several architectures for different traffic scenarios using standard RL in the
    last two decades, invention of deep RL made a huge impact on the ITS research,
    in particular TSC. Due to its capability of dealing with large state space, a
    number of deep RL models have been proposed for controlling traffic lights. The
    deep RL paradigm is basically based on approximating Q-functions with deep neural
    networks. The earliest work using this approach is [[82](#bib.bib82)]. Although
    a neural network-based RL model is proposed in this paper, it is not a full DQN
    algorithm due to lack of experience replay and target network, which are essential
    components of DQN [[19](#bib.bib19)].
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于深度强化学习（RL）的自适应交叉口控制工具受到交通研究人员的极大关注。在过去二十年里，研究人员为不同的交通场景提出了几种标准RL架构，深度RL的发明对智能交通系统（ITS）研究产生了巨大影响，尤其是交通信号控制（TSC）。由于其处理大规模状态空间的能力，已提出了许多深度RL模型用于控制交通信号灯。深度RL范式基本上是基于使用深度神经网络近似Q函数。这种方法的最早工作是[[82](#bib.bib82)]。虽然这篇论文提出了基于神经网络的RL模型，但由于缺乏经验重放和目标网络，这些是DQN的基本组件，因此它并不是一个完整的DQN算法[[19](#bib.bib19)]。
- en: The initial work on controlling traffic signals with a deep RL approach is [[31](#bib.bib31)]
    by Genders et al.. In this work, authors use discrete traffic state encoding model,
    called DTSE, to form an image-like state representation based on detailed information
    from the traffic environment. The proposed state model is an input to CNN for
    approximating the Q-values of discrete actions. The experiments are performed
    on the SUMO simulation environment with a single intersection where 4 green phases
    are selected as actions. In order to show the power of CNN on the DTSE state form,
    the results are compared with Q-learning using a single layer neural network.
    In [[98](#bib.bib98)], the same authors studied the effects of different state
    representations for intersection optimization using the A3C algorithm. Three separate
    state definitions are experimented on a single intersection using a dynamic traffic
    environment. The first form of state definition considered in the paper is given
    by the occupancy and average speed of each lane. The second state definition is
    the queue length and vehicle density for each lane. The third state form is the
    image like representation, DTSE, with Boolean position information in which the
    existence of vehicle is represented with 1\. The results show that the resolution
    of state representation does not effect the performance of RL agent in terms of
    delay and queue length. The same authors, in a recent paper [[103](#bib.bib103)],
    studied asynchronous deep RL model for TSC. In asynchronous n-step Q-learning
    [[27](#bib.bib27)], the main job is divided to multiple processors, and each processor
    learns its local optimal parameters individually. Global parameters for the general
    network is updated after every n-step. The proposed architecture in [[103](#bib.bib103)]
    improves the performance almost 40% compared to the fixed-time and actuated traffic
    controllers.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用深度RL方法控制交通信号的初步工作是[[31](#bib.bib31)]由Genders等人完成的。在这项工作中，作者使用了一个离散交通状态编码模型，称为DTSE，以基于交通环境的详细信息形成类似图像的状态表示。所提出的状态模型作为输入用于CNN，以近似离散动作的Q值。实验在SUMO仿真环境中的单个交叉口上进行，其中选择了4个绿灯阶段作为动作。为了展示CNN在DTSE状态形式上的优势，结果与使用单层神经网络的Q学习进行比较。在[[98](#bib.bib98)]中，同一作者研究了使用A3C算法的不同状态表示对交叉口优化的影响。三个不同的状态定义在动态交通环境中的单个交叉口上进行实验。论文中考虑的第一种状态定义是每条车道的占用率和平均速度。第二种状态定义是每条车道的排队长度和车辆密度。第三种状态形式是类似图像的表示，即DTSE，包含布尔位置信息，其中车辆的存在用1表示。结果表明，状态表示的分辨率不会影响RL代理在延迟和排队长度方面的性能。近期，同一作者在[[103](#bib.bib103)]的论文中研究了用于TSC的异步深度RL模型。在异步n步Q学习[[27](#bib.bib27)]中，主要工作被分配到多个处理器上，每个处理器独立学习其局部最优参数。一般网络的全局参数在每n步后更新。[[103](#bib.bib103)]中提出的架构相比于固定时间和激活式交通控制器，性能提高了近40%。
- en: Authors in [[40](#bib.bib40)], proposed an autoencoder-based deep RL algorithm
    for a single intersection with dynamic traffic flow. Autoencoders are considered
    for action selection by mapping input queue length to a low-dimensional action
    set. Bottleneck layer, which is the output of decoding part, is used for Q-function
    approximation. The results are compared with standard Q-learning using the Paramics
    simulator. Currently, this is the only work in the literature that uses autoencoders
    to approximate action values. In [[33](#bib.bib33)], Gao et al. proposed a new
    neural network architecture in which state is a combination of the speed and position
    of vehicles based on DTSE. The output of neural network is binary action whether
    to keep the same action or change the action in a predefined phase cycle. The
    proposed model is compared with the fixed-time controller and the longest queue
    first controller.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[40](#bib.bib40)]中，作者提出了一种基于自编码器的深度RL算法，用于具有动态交通流的单个交叉口。自编码器被用于通过将输入队列长度映射到低维度动作集来选择动作。瓶颈层，即解码部分的输出，用于Q函数近似。结果与使用Paramics模拟器的标准Q学习进行了比较。目前，这项工作是文献中唯一使用自编码器来近似动作值的工作。在[[33](#bib.bib33)]中，Gao等人提出了一种新的神经网络架构，其中状态是基于DTSE的车辆速度和位置的组合。神经网络的输出是一个二进制动作，用于判断是否在预定义的周期中保持相同动作或改变动作。该模型与固定时间控制器和最长队列优先控制器进行了比较。
- en: 'The authors in [[28](#bib.bib28)], presented two deep RL algorithms for controlling
    isolated intersections: value-based DQN, and policy-based actor critic. The state
    for both agents is raw consecutive image frames following exactly the same approach
    with original DQN. As stated in the original paper [[19](#bib.bib19)], DQN algorithm
    suffers from instability issues. [[28](#bib.bib28)] shows that the policy-based
    deep RL technique solves this issue by having a smooth convergence and a stable
    trend after convergence. Shabestary et al. [[36](#bib.bib36)] proposed a DQN-based
    solution for adaptive traffic signal control on an isolated intersection using
    a new reward definition. The reward and action defined in this paper are change
    in the cumulative delay and 8 different green phases, as opposed to the commonly
    used binary action set or 4 green phases for a single intersection.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[28](#bib.bib28)]中，作者提出了两种用于控制孤立交叉口的深度强化学习（RL）算法：基于值的DQN和基于策略的actor-critic。两个智能体的状态都是原始的连续图像帧，采用与原始DQN完全相同的方法。正如原论文[[19](#bib.bib19)]所述，DQN算法存在不稳定性问题。[[28](#bib.bib28)]表明，基于策略的深度RL技术通过实现平滑的收敛和收敛后的稳定趋势来解决这一问题。Shabestary等人[[36](#bib.bib36)]提出了一种基于DQN的解决方案，用于使用新的奖励定义进行孤立交叉口的自适应交通信号控制。本文中定义的奖励和动作是累计延迟的变化和8个不同的绿灯阶段，而不是通常使用的二进制动作集或单个交叉口的4个绿灯阶段。
- en: Choe et al. proposed a RNN-based DQN model in a single intersection TSC scenario
    [[37](#bib.bib37)]. It is shown that the performance of RNN-based DQN decreased
    the travel time compared to the popular CNN structure. A policy gradient-based
    deep RL method is proposed for adaptive traffic intersection control in [[29](#bib.bib29)],
    which presents experiments on a novel realistic traffic environment called Unity3D
    by using raw pixels as an input state to policy-based DQN. The proposed model
    has similar results with the fixed-time intersection control model. An action
    value-based DQN with a novel discount factor is proposed by C. Wan et al. [[45](#bib.bib45)].
    The proposed dynamic discount factor takes execution time into account with the
    help of infinite geometric series. The proposed model is tested on a single intersection
    using the SUMO simulator by comparing it with the fixed-time controller and the
    standard DQN-based controller.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Choe等人提出了一种基于RNN的DQN模型，用于单个交叉口TSC场景[[37](#bib.bib37)]。结果表明，与流行的CNN结构相比，基于RNN的DQN减少了旅行时间。[[29](#bib.bib29)]中提出了一种基于策略梯度的深度RL方法，用于自适应交通交叉口控制，该方法在名为Unity3D的全新现实交通环境中进行实验，使用原始像素作为输入状态来进行基于策略的DQN。所提出的模型与固定时间交叉口控制模型具有类似的结果。C.
    Wan等人[[45](#bib.bib45)]提出了一种具有新型折扣因子的基于动作值的DQN。所提出的动态折扣因子通过无限几何级数考虑了执行时间。该模型在使用SUMO模拟器的单个交叉口上进行了测试，并与固定时间控制器和标准DQN基础控制器进行了比较。
- en: 'A new DQN-based controller, called IntelliLight, with a new network architecture
    is described in [[38](#bib.bib38)]. The reward function consist of multiple components:
    sum of the queue length over all lanes, sum of delay, sum of waiting time, traffic
    light state indicator, number of vehicles that passed the intersection since the
    last action, and sum of travel times since the last action. The proposed method
    is experimented on SUMO using a single intersection. A real dataset collected
    from real cameras in China is used as an input to SUMO. IntelliLight is selected
    as a benchmark in [[107](#bib.bib107)], which introduces a new transfer learning
    model with a batch learning framework. The same real-world data and a synthetic
    simulation data which generates traffic with uniform distribution is used on an
    isolated intersection for experiments. Another DQN-based study for traffic light
    control with a real dataset is presented in [[50](#bib.bib50)]. Data from a three-way
    non-homogeneous real intersection in Lebanon is used. The experimental results
    are compared with the real-world fixed-time controller that is in use at the intersection
    in terms of queue length and delay.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[38](#bib.bib38)]中描述了一种新的基于DQN的控制器，称为IntelliLight，具有新的网络架构。奖励函数由多个组件组成：所有车道的排队长度总和、延迟总和、等待时间总和、交通灯状态指示器、自上次动作以来通过交叉口的车辆数量，以及自上次动作以来的旅行时间总和。所提出的方法在SUMO上进行了实验，使用的是一个单一的交叉口。使用从中国真实摄像头收集的真实数据作为SUMO的输入。IntelliLight在[[107](#bib.bib107)]中被选为基准，该基准介绍了一种新的转移学习模型和批量学习框架。在隔离交叉口上，使用了相同的真实世界数据和生成均匀分布交通的合成模拟数据进行实验。另一项基于DQN的研究在[[50](#bib.bib50)]中提出，使用了黎巴嫩一个三路非均质真实交叉口的数据。实验结果与交叉口当前使用的固定时间控制器在排队长度和延迟方面进行了比较。
- en: A different deep RL model in terms of action set and the deep RL algorithm is
    studied by Liang et al., [[30](#bib.bib30)]. This work updates the next phase
    duration in the phase cycle instead of choosing an action from a green phase set.
    Considering a 4-phase single intersection, phase change duration is defined. The
    selected phase duration can be added or subtracted from the duration of the next
    cycle phase. In this model, for a four-way intersection the action set includes
    9 discrete actions. The proposed algorithm in this paper considers new DQN techniques,
    namely double dueling DQN and prioritized experience replay, to improve the performance.
    In another paper, Jang et al. [[43](#bib.bib43)], discusses how to integrate a
    DQN agent with a traffic simulator through the Java-based AnyLogic multipurpose
    simulator. A different approach for state definitions is proposed by Liu et al.
    [[105](#bib.bib105)] for examining the impacts of DQN on green-wave patterns in
    a linear road topology. The experiments are performed only on a Python environment
    that creates traffic data from a probability distribution without using any traffic
    simulator. Moreover, considering the Dedicated Short-Range Communication (DSRC)
    technology for vehicle-to-infrastructure (V2I) communication, Zhang et al. [[106](#bib.bib106)]
    addresses TSC under partial detection of vehicles at an intersection. Their motivation
    to study TSC with undetected vehicles comes from the case where not all vehicles
    use DSRC.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 梁等人研究了一种不同的深度强化学习模型，该模型在动作集合和深度强化学习算法方面有所不同[[30](#bib.bib30)]。该工作更新了相位周期中的下一个阶段持续时间，而不是从绿色阶段集合中选择一个动作。考虑到一个四相位单交叉口，定义了相位变更持续时间。所选的相位持续时间可以加到或减去下一个周期阶段的持续时间。在这个模型中，对于一个四路交叉口，动作集合包括9个离散动作。本文提出的算法考虑了新的DQN技术，即双重对决DQN和优先经验回放，以提高性能。在另一篇论文中，张等人[[43](#bib.bib43)]讨论了如何通过基于Java的AnyLogic多用途模拟器将DQN代理与交通模拟器集成。刘等人[[105](#bib.bib105)]提出了一种不同的状态定义方法，用于检查DQN在线性道路拓扑中对绿色波动模式的影响。实验仅在Python环境中进行，该环境从概率分布中创建交通数据，而未使用任何交通模拟器。此外，考虑到用于车辆与基础设施（V2I）通信的专用短程通信（DSRC）技术，张等人[[106](#bib.bib106)]探讨了部分检测车辆情况下的交通信号控制。他们研究未检测到车辆的交通信号控制的动机来自于并非所有车辆都使用DSRC的情况。
- en: V-B2 Multi-agent deep RL
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 多代理深度强化学习
- en: The first deep RL-based multiple intersection control mechanism is presented
    in [[32](#bib.bib32)], which defines a new reward function and proposes a coordination
    tool for multiple traffic lights. The reward definition in this paper considers
    a combination of specific traffic conditions, namely accidents or jam, emergency
    stops, and traffic light changes, and the waiting time of all vehicles. The reward
    function properly penalizes each specific traffic situation. For coordination
    of multiple intersections to have a high traffic flow rate, this paper uses a
    transfer planning technique for a smaller set of intersections and links the learning
    results to a larger set of intersections with the max-plus coordination algorithm.
    In this work, the benchmark is one of the early coordination-based RL methods
    proposed in [[65](#bib.bib65)]. As expected, the DQN-based coordination method
    outperforms the earlier standard RL-based method. This paper is expanded to a
    master thesis [[23](#bib.bib23)] by presenting the results for single agent scenario
    and different multi-agent scenarios. In [[35](#bib.bib35)], similar to [[32](#bib.bib32)],
    a multi-agent deep RL approach on a 2-by-2 intersection grid model is proposed,
    in which max-plus and transfer learning are used for reaching the global optimal
    learning with coordination. This paper differs from [[32](#bib.bib32)] mainly
    by using RNN, in particular LSTM, layers instead of fully connected layers for
    Q-function approximation. Deep RL approach with RNN structure is shown to result
    in lower average delay compared to Q-learning and fixed-time control in both low
    and high traffic demand scenarios.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 首个基于深度强化学习的多交叉口控制机制在 [[32](#bib.bib32)] 中提出，该机制定义了一个新的奖励函数，并提出了用于多个交通灯的协调工具。本文中的奖励定义考虑了特定交通条件的组合，即事故或拥堵、紧急停车和交通灯变化，以及所有车辆的等待时间。奖励函数对每种特定的交通情况进行适当的惩罚。为了使多个交叉口具有较高的交通流量，本文使用了传输规划技术针对较小的交叉口集，并将学习结果链接到较大的交叉口集，通过最大加协调算法实现。在这项工作中，基准是
    [[65](#bib.bib65)] 中提出的早期协调基础的RL方法。正如预期的那样，基于DQN的协调方法优于早期的标准RL方法。本文扩展为硕士论文 [[23](#bib.bib23)]，呈现了单代理场景和不同多代理场景的结果。在
    [[35](#bib.bib35)] 中，类似于 [[32](#bib.bib32)]，提出了一种基于多代理的深度RL方法，用于2×2交叉口网格模型，其中使用最大加和转移学习来实现具有协调性的全局最优学习。本文与
    [[32](#bib.bib32)] 主要的不同在于使用了RNN，特别是LSTM层，而不是完全连接层来进行Q函数近似。与Q学习和固定时间控制相比，基于RNN结构的深度RL方法在低流量和高流量需求场景下都显示出较低的平均延迟。
- en: 'Liu et al. [[34](#bib.bib34)] introduced a cooperative deep RL model for controlling
    multiple intersections with multiple agents. The presented algorithm is DQN with
    a ResNet structure used to form the state space. The reward function penalizes
    the system based on the driver behavior and waiting time with a BPR function (see
    Section [IV-C](#S4.SS3 "IV-C Reward ‣ IV Deep RL Settings for TSC ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey")). Cooperation between
    agents is assured by sharing the policy with other agents every n-step. Experiments
    for this study are done using a 2-by-2 intersection model on SUMO. SOTL, Q-learning
    and DQN are selected as reference points for validating the proposed model.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '刘等人 [[34](#bib.bib34)] 介绍了一种用于控制多个交叉口和多个代理的合作深度强化学习（RL）模型。提出的算法是带有ResNet结构的DQN，用于构建状态空间。奖励函数根据驾驶员行为和等待时间使用BPR函数进行惩罚（见第[IV-C节](#S4.SS3
    "IV-C Reward ‣ IV Deep RL Settings for TSC ‣ Deep Reinforcement Learning for Intelligent
    Transportation Systems: A Survey")）。通过每n步与其他代理共享策略来确保代理之间的合作。该研究的实验使用SUMO上的2×2交叉口模型进行。SOTL、Q学习和DQN被选作验证所提模型的参考点。'
- en: Multiple traffic intersections can be represented as a network graph in which
    lane connections between roads form a directed graph. Nish et al. [[100](#bib.bib100)]
    presented a GCN-based neural network structure for the RL agent. GCN is combined
    with a specific RL algorithm called k-step neural fitted Q-iteration [[101](#bib.bib101)]
    that updates the agent in a distributed manner by assigning one agent for each
    intersection considering the whole network to form the state space. The experiment
    results show that the GCN-based algorithm decreases the waiting time on all 6
    intersections compared to the fixed-time controller and standard CNN-based RL
    controller. A hierarchical control structure is presented in [[44](#bib.bib44)]
    for TSC. The lower layer optimizes the local area traffic via intersection control
    while the top layer optimizes the city-level traffic by tuning the degree of optimization
    of local areas in the lower layer. In this research, multi-intersection learning
    is built on threshold values collected from individual intersections. The action
    set of higher level controller is increasing or decreasing the threshold values
    that change the sensitivity of each intersection to the neighbor intersections.
    The learning model in this paper is different from the other deep RL-based intersection
    controllers such that the model decreases the algorithm complexity in higher level
    control through a threshold-based mechanism instead of setting the phase cycles
    or phase duration.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 多个交通交叉口可以表示为一个网络图，其中道路之间的车道连接形成一个有向图。Nish 等人 [[100](#bib.bib100)] 提出了一个基于 GCN
    的神经网络结构用于 RL 代理。GCN 与一种特定的 RL 算法 k-step 神经拟合 Q-迭代 [[101](#bib.bib101)] 结合，该算法通过为每个交叉口分配一个代理，考虑整个网络形成状态空间，从而以分布式方式更新代理。实验结果显示，与固定时间控制器和标准
    CNN 基于 RL 控制器相比，基于 GCN 的算法减少了所有 6 个交叉口的等待时间。 [[44](#bib.bib44)] 提出了一个用于 TSC 的分层控制结构。下层通过交叉口控制优化局部区域交通，而上层通过调整下层局部区域的优化程度来优化城市级交通。在本研究中，多交叉口学习建立在从各个交叉口收集的阈值基础上。高层控制器的动作集是增加或减少阈值，从而改变每个交叉口对邻近交叉口的敏感度。本文的学习模型不同于其他基于深度
    RL 的交叉口控制器，因为该模型通过基于阈值的机制来减少高层控制中的算法复杂性，而不是设置相位周期或相位持续时间。
- en: Cooperative multi-agent deep RL model is investigated in [[39](#bib.bib39)].
    Here, an agent with an independent double dueling DQN model supported with prioritized
    experience replay is assigned to each intersection. In order to improve the coordination
    performance, a special sampling technique, fingerprint, is used in experience
    replay. Fingerprint technique estimates Q-functions with neighbor agent’s policy
    via Bayesian inference [[108](#bib.bib108)]. The proposed model is tested on SUMO
    with heterogeneous multiple intersections. The results show that the proposed
    algorithm outperforms the fixed-time controller and the DQN controller without
    experience replay on several travel demand scenarios.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[[39](#bib.bib39)] 研究了合作多智能体深度 RL 模型。在这里，每个交叉口都分配了一个具有独立双重对抗 DQN 模型并支持优先经验回放的代理。为了提高协调性能，在经验回放中使用了一种特殊的采样技术，即指纹技术。指纹技术通过贝叶斯推断
    [[108](#bib.bib108)] 利用邻近代理的策略来估计 Q 函数。所提模型在 SUMO 上进行的异质多个交叉口测试结果表明，该算法在多个旅行需求场景下优于固定时间控制器和没有经验回放的
    DQN 控制器。'
- en: 'One of the approaches in multi-agent systems is updating only the critical
    edges to increase the efficiency. [[47](#bib.bib47)] first identifies important
    nodes based on multiple criteria with a specific ranking algorithm, CRRank, that
    creates a trip network using a bidirectional tripartite graph. Based on data and
    tripartite graph, system ranks the edges based on assigned scores. Once critical
    intersections are identified, RNN structured DQN agent learns the optimal policy.
    The model is tested with 20, 50 and 100 intersections on SUMO comparing its results
    with fixed-time, SOTL, Q-learning and DQN controllers. Recently, a cooperative
    deep RL method with Q-value transfer is proposed in [[46](#bib.bib46)]. At each
    intersection, a DQN agent controls the traffic light by receiving Q-values from
    other agents for learning the optimal policy. The proposed algorithm is supported
    with extensive experiments on homogeneous and heterogeneous intersections. It
    is important to have a heterogeneous traffic scenario because all the intersections
    do not have the same characteristics such as the number of roads and number of
    lanes. The authors compared their results with two benchmark papers: coordinated
    Q-learning [[32](#bib.bib32)] and distributed Q-learning [[94](#bib.bib94)] approaches.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体系统中，一种方法是仅更新关键边以提高效率。[[47](#bib.bib47)] 首先基于多个标准通过特定的排名算法 CRRank 确定重要节点，该算法利用双向三分图创建旅行网络。根据数据和三分图，系统根据分配的分数对边进行排名。一旦识别出关键交叉点，结构化的
    RNN DQN 智能体便会学习最优策略。该模型在 SUMO 上使用 20、50 和 100 个交叉点进行测试，并与固定时间、SOTL、Q 学习和 DQN 控制器的结果进行比较。最近，[[46](#bib.bib46)]
    提出了一个带有 Q 值传递的合作深度 RL 方法。在每个交叉点，DQN 智能体通过接收来自其他智能体的 Q 值来控制交通信号灯，以学习最优策略。所提出的算法在同质和异质交叉点上进行了广泛实验。拥有异质交通场景是重要的，因为所有交叉点的特性（如道路数量和车道数量）并不相同。作者将其结果与两个基准论文进行了比较：协调
    Q 学习 [[32](#bib.bib32)] 和分布式 Q 学习 [[94](#bib.bib94)] 方法。
- en: 'The work in [[41](#bib.bib41)] investigates applying the deep deterministic
    policy gradient (DDPG) algorithm to a city-scale traffic network. The author formulated
    the TSC problem with DDPG by controlling the phase duration continuously. The
    model updates phase duration of all network at once by keeping the total phase
    cycle constant in order to control the synchronization throughout the network.
    In this work, a specific information called speed score, calculated using the
    maximum speed on each detector, is considered for forming the state vector. Three
    traffic scenarios are tested from small to large networks: isolated intersection,
    2-by-3 grid intersections, and a Barcelona city-scale map with 43 intersections.
    The proposed approach achieves higher reward performance than multi-agent Q-learning
    controller. It is remarkable that actor critic models can be applied large intersection
    models without any extra multi-agent control technique. Another DDPG-based deep
    RL controller for large-scale network is studied by Genders in his PhD thesis
    [[48](#bib.bib48)]. The system model consists of a parallel architecture with
    decentralized actors for each intersection and central learners each of which
    cover a subset of the intersections. The policy determines the duration of the
    green phase in each intersection. To test the performance of the model, Luxembourg
    city map is used on SUMO with 196 intersections, which is the largest test environment
    for RL-based TSC up to now.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[[41](#bib.bib41)] 的工作研究了将深度确定性策略梯度（DDPG）算法应用于城市规模的交通网络。作者通过连续控制相位持续时间来使用 DDPG
    公式化 TSC 问题。该模型通过保持总相位周期不变来一次性更新所有网络的相位持续时间，以控制整个网络的同步。在这项工作中，考虑了使用每个检测器的最大速度计算的特定信息——速度评分，以形成状态向量。对三种从小到大的交通场景进行了测试：孤立交叉点、2x3
    网格交叉点以及具有 43 个交叉点的巴塞罗那城市规模地图。所提出的方法在奖励性能上优于多智能体 Q 学习控制器。值得注意的是，演员-评论员模型可以应用于大型交叉点模型，而无需额外的多智能体控制技术。Genders
    在其博士论文 [[48](#bib.bib48)] 中研究了另一种基于 DDPG 的大规模网络深度 RL 控制器。该系统模型由一个并行架构组成，每个交叉点配有去中心化的智能体和覆盖交叉点子集的中心学习者。策略决定每个交叉点绿色阶段的持续时间。为了测试模型的性能，使用了包含
    196 个交叉点的卢森堡城市地图，这是迄今为止 RL 基于 TSC 的最大测试环境。'
- en: A multiple actor-learner architecture considering the A2C algorithm is presented
    for multiple intersections by Lin et al. in [[42](#bib.bib42)]. Multiple actors
    observe different states, and follow different exploration policies in parallel.
    Since actor-critic approaches are built on advantage functions, authors consider
    a technique called general advantage estimation function in the learning process
    [[109](#bib.bib109)]. The presented experiments are performed on a 3-by-3 intersection
    grid on SUMO, and the results are compared with the fixed-time controller and
    actuated controller.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Lin 等人提出了一种考虑 A2C 算法的多演员-学习者架构用于多个交叉口，如 [[42](#bib.bib42)] 所示。多个演员观察不同的状态，并且并行地遵循不同的探索策略。由于演员-评论员方法是基于优势函数的，作者在学习过程中考虑了一种叫做一般优势估计函数的技术
    [[109](#bib.bib109)]。实验在 SUMO 上的 3×3 交叉口网格上进行，结果与固定时间控制器和执行控制器进行了比较。
- en: Independent Q-learning is one of the popular multi-agent RL approaches in literature.
    Chu et al. [[102](#bib.bib102)] recently expanded this approach to independent
    A2C for multi-agent TSC. The stability problem is addressed with two methods,
    fingerprints of neighbor intersections and a spatial discount factor. While the
    former provides each agent with information regarding the local policies and traffic
    distributions of neighbor agents, the latter enables each agent to focus on improving
    the local traffic. The network structure in the A2C algorithm is an LSTM-based
    RNN model. Both synthetic traffic network with a 5-by-5 grid and a real network
    from Monaco City with 30 intersections are used for performance evaluation.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 独立 Q 学习是文献中流行的多智能体 RL 方法之一。Chu 等人 [[102](#bib.bib102)] 最近将这种方法扩展到独立的 A2C 以应对多智能体
    TSC。稳定性问题通过两种方法解决，即邻近交叉口的指纹和空间折扣因子。前者为每个智能体提供有关邻近智能体的局部政策和交通分布的信息，后者使每个智能体能够专注于改善局部交通。A2C
    算法中的网络结构是基于 LSTM 的 RNN 模型。实验评估使用了一个 5×5 网格的合成交通网络和一个来自摩纳哥城市的真实网络（30 个交叉口）。
- en: 'Systematic learning for large-scale traffic networks is achieved with cooperation
    in [[104](#bib.bib104)]. A large system is divided into subsets in which each
    local region is controlled with an RL agent. Global learning is achieved by transferring
    learning policies to the global agent. For local controllers, authors investigated
    two deep RL algorithms: value based per-action DQN and actor-critic based Wolpertinger-DDPG
    [[110](#bib.bib110)]. Per-action DQN is similar to the standard DQN algorithm,
    but differs from DQN by considering state-action pair as an input and generating
    a single Q value. Wolpertinger-DDPG provides a new policy method based on the
    k-nearest-neighbor approach using DDPG for large-scale discrete action spaces.
    In experiments, three different traffic networks are used, and the results are
    compared with a decentralized Q-learning algorithm with linear function approximators,
    and two rule-based baselines (fixed-time and random-time controllers).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 [[104](#bib.bib104)] 的合作实现了大规模交通网络的系统学习。一个大系统被划分为多个子集，每个局部区域由一个 RL 代理控制。通过将学习策略转移到全局代理，实现了全局学习。对于局部控制器，作者研究了两种深度
    RL 算法：基于价值的每动作 DQN 和基于演员-评论员的 Wolpertinger-DDPG [[110](#bib.bib110)]。每动作 DQN 类似于标准的
    DQN 算法，但不同于 DQN，因为它将状态-动作对作为输入并生成一个单一的 Q 值。Wolpertinger-DDPG 提供了一种基于 k-近邻方法的新政策方法，利用
    DDPG 处理大规模离散动作空间。在实验中，使用了三种不同的交通网络，并将结果与使用线性函数逼近器的去中心化 Q 学习算法以及两种基于规则的基准（固定时间和随机时间控制器）进行了比较。
- en: Coskun et al. [[99](#bib.bib99)] expands [[28](#bib.bib28)], which use value-based
    DQN and policy-based standard actor-critic, to multiple intersections using value-based
    DQN and policy-based A2C. The results of both algorithms following deep learning
    is consistent with the results of standard RL approaches in terms of average reward
    per episode, where DQN hits higher average reward than A2C.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Coskun 等人 [[99](#bib.bib99)] 扩展了 [[28](#bib.bib28)]，该方法使用基于价值的 DQN 和基于策略的标准演员-评论员，将其扩展到使用基于价值的
    DQN 和基于策略的 A2C 的多个交叉口。两种算法的深度学习结果与标准 RL 方法在每集平均奖励方面的结果一致，其中 DQN 的平均奖励高于 A2C。
- en: VI Deep RL for Other ITS Applications
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 深度 RL 在其他 ITS 应用中的应用
- en: 'TABLE IV: Outline of deep RL approaches for other ITS applications'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：其他 ITS 应用中深度 RL 方法的概述
- en: '| Case Study | Deep RL method | Target application | Solution | Test | Result
    comparison |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 案例研究 | 深度 RL 方法 | 目标应用 | 解决方案 | 测试 | 结果比较 |'
- en: '| Sallab et al. [[111](#bib.bib111)] | DQN DDPG | Autonomous driving | Spatial
    aggregation, Recurrent temporal aggregation | TORCH game | RNN-LSTM Kalman-GRNN
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Sallab et al. [[111](#bib.bib111)] | DQN DDPG | 自主驾驶 | 空间聚合、递归时间聚合 | TORCH
    game | RNN-LSTM Kalman-GRNN |'
- en: '| Xia et al. [[112](#bib.bib112)] | DQN with filtered experience replay | Autonomous
    driving | Optimum control | TORCH game | NFQ [[101](#bib.bib101)] |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Xia et al. [[112](#bib.bib112)] | 带有过滤经验回放的 DQN | 自主驾驶 | 最优控制 | TORCH game
    | NFQ [[101](#bib.bib101)] |'
- en: '| Xiong et al. [[113](#bib.bib113)] | DDPG | Autonomous driving | Collision
    avoidance | TORCH game | - |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Xiong et al. [[113](#bib.bib113)] | DDPG | 自主驾驶 | 碰撞避免 | TORCH game | - |'
- en: '| Sharifzadeh et al. [[114](#bib.bib114)] | DQN | Autonomous driving | Lane
    changing | Personal simulator | Expert driver |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Sharifzadeh et al. [[114](#bib.bib114)] | DQN | 自主驾驶 | 车道变换 | 个人模拟器 | 专家驾驶员
    |'
- en: '| Hoel et al. [[115](#bib.bib115)] | AlphaGo Zero | Autonomous driving | Decision
    planning with Monte carlo tree search | Personal simulator | MCTS IDM/MOBIL |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Hoel et al. [[115](#bib.bib115)] | AlphaGo Zero | 自主驾驶 | 基于蒙特卡罗树搜索的决策规划 |
    个人模拟器 | MCTS IDM/MOBIL |'
- en: '| Hoel et al. [[116](#bib.bib116)] | DQN | Autonomous driving | Speed change
    Lane change | Personal simulator | CNN-FCNN IDM |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| Hoel et al. [[116](#bib.bib116)] | DQN | 自主驾驶 | 速度变化 车道变换 | 个人模拟器 | CNN-FCNN
    IDM |'
- en: '| Chae et al. [[117](#bib.bib117)] | DQN | Autonomous breaking | Pedestrian
    detection | PreScan vehicle simulator | Without Trauma memory |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| Chae et al. [[117](#bib.bib117)] | DQN | 自主制动 | 行人检测 | PreScan 车辆模拟器 | 无创伤记忆
    |'
- en: '| Shi et al. [[118](#bib.bib118)] | Hierarchical DQN | Autonomous driving |
    Safe gap adjustment Lane changing | Personal simulator | - |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Shi et al. [[118](#bib.bib118)] | 分层 DQN | 自主驾驶 | 安全间隔调整 车道变换 | 个人模拟器 | -
    |'
- en: '| Wang et al. [[119](#bib.bib119)] | Rule-based DQN | Autonomous driving |
    Lane changing | Udacity simulator | Different policy structures |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[119](#bib.bib119)] | 基于规则的 DQN | 自主驾驶 | 车道变换 | Udacity 模拟器
    | 不同的策略结构 |'
- en: '| Ye et al. [[120](#bib.bib120)] | DDPG | Autonomous driving | Lane changing
    Car following | VISSIM | IDM |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Ye et al. [[120](#bib.bib120)] | DDPG | 自主驾驶 | 车道变换 车随行 | VISSIM | IDM |'
- en: '| Makansis et al. [[121](#bib.bib121)] | DDQN | Autonomous driving | Optimum
    highway control | SUMO | DP |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Makansis et al. [[121](#bib.bib121)] | DDQN | 自主驾驶 | 最优高速公路控制 | SUMO | DP
    |'
- en: '| Yu et al. [[122](#bib.bib122)] | Multi-agent Q-learning | Autonomous driving
    | Coordination graphs | Personal simulator | Expert driver Independent Q-learning
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Yu et al. [[122](#bib.bib122)] | 多智能体 Q-learning | 自主驾驶 | 协调图 | 个人模拟器 | 专家驾驶员
    独立 Q-learning |'
- en: '| Qian et al. [[123](#bib.bib123)] | Twin delay DDPG | Autonomous driving |
    Path planning | Personal simulator | Expert driver DQN |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Qian et al. [[123](#bib.bib123)] | 双重延迟 DDPG | 自主驾驶 | 路径规划 | 个人模拟器 | 专家驾驶员
    DQN |'
- en: '| Zhou et al. [[124](#bib.bib124)] | DDPG | Autonomus driving | Optimum control
    in TSC intersections | Personal simulator | Human driver Policy gradient DQN |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Zhou et al. [[124](#bib.bib124)] | DDPG | 自主驾驶 | TSC 交叉口的最优控制 | 个人模拟器 | 人类驾驶员
    政策梯度 DQN |'
- en: '| Osinski et al. [[125](#bib.bib125)] | PPO2 [[126](#bib.bib126)] | Autonomous
    driving | Optimum control | Real world CARLA | Continuous driving model |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Osinski et al. [[125](#bib.bib125)] | PPO2 [[126](#bib.bib126)] | 自主驾驶 |
    最优控制 | 现实世界 CARLA | 连续驾驶模型 |'
- en: '| Huang et al. [[127](#bib.bib127)] | DDPG | Autonomous driving | Human in
    the loop training | IPG CarMker | Imitation learning |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. [[127](#bib.bib127)] | DDPG | 自主驾驶 | 人工介入训练 | IPG CarMker |
    模仿学习 |'
- en: '| Isele et al. [[128](#bib.bib128)] | DQN | Autonomous driving | Navigating
    in occluded intersections | SUMO | TTC [[129](#bib.bib129)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Isele et al. [[128](#bib.bib128)] | DQN | 自主驾驶 | 在遮挡交叉口导航 | SUMO | TTC [[129](#bib.bib129)]
    |'
- en: '| Kreidieh et al. [[130](#bib.bib130)] | TRPO [[131](#bib.bib131)] | Stop-and-go
    wave dissipation | Transfer learning | Flow | Human driver Random policy |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Kreidieh et al. [[130](#bib.bib130)] | TRPO [[131](#bib.bib131)] | 停走波消散
    | 迁移学习 | 流量 | 人类驾驶员 随机政策 |'
- en: '| Chalaki et al. [[132](#bib.bib132)] | TRPO | Ramp metering | Policy transfer
    to city scale map Adversarial noise injection | Scaled smart city | Human Driver
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Chalaki et al. [[132](#bib.bib132)] | TRPO | 坡道信号控制 | 政策转移到城市规模地图 对抗噪声注入
    | 扩展智能城市 | 人类驾驶员 |'
- en: '| Jang et al. [[133](#bib.bib133)] | TRPO | Ramp metering | Policy transfer
    to city scale map | Scaled smart city | IDM controller |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Jang et al. [[133](#bib.bib133)] | TRPO | 坡道信号控制 | 政策转移到城市规模地图 | 扩展智能城市 |
    IDM 控制器 |'
- en: '| Belletti et al. [[134](#bib.bib134)] | TRPO | Ramp metering | Multi-task
    conrol | Personal simulator | REINFORCE PPO [[135](#bib.bib135)] |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Belletti et al. [[134](#bib.bib134)] | TRPO | 坡道信号控制 | 多任务控制 | 个人模拟器 | REINFORCE
    PPO [[135](#bib.bib135)] |'
- en: '| Chaoui et al. [[136](#bib.bib136)] | DQN | Electric vehicle | Energy management
    with multiple batteries | Personal simulator | - |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Chaoui 等人 [[136](#bib.bib136)] | DQN | 电动汽车 | 多电池的能源管理 | 个人模拟器 | - |'
- en: '| Wu et al. [[137](#bib.bib137)] | DDPG | Hybrid electric bus | Adaptive energy
    management to road conditions | Personal simulator | DQN DP |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等人 [[137](#bib.bib137)] | DDPG | 混合动力电动公交 | 针对道路状况的自适应能源管理 | 个人模拟器 | DQN
    DP |'
- en: '| Hu et al. [[138](#bib.bib138)] | DQN | Hybrid electric vehicle | Energy management
    | MATLAB ADVISOR [[139](#bib.bib139)] | Different training models |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Hu 等人 [[138](#bib.bib138)] | DQN | 混合动力电动汽车 | 能源管理 | MATLAB ADVISOR [[139](#bib.bib139)]
    | 不同的训练模型 |'
- en: '| Wu et al. [[140](#bib.bib140)] | DDPG | Freeway control | Variable speed
    limit | SUMO | Q-learning DQN |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等人 [[140](#bib.bib140)] | DDPG | 高速公路控制 | 变速限制 | SUMO | Q-learning DQN
    |'
- en: '| Wu et al. [[141](#bib.bib141)] | ES [[142](#bib.bib142)] | Freeway control
    | Ramp meter Speed limit Lane change | SUMO | No control DQN-RM TRPO-RM DDPG-DVSL
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等人 [[141](#bib.bib141)] | ES [[142](#bib.bib142)] | 高速公路控制 | 匝道流量计 速度限制
    车道变更 | SUMO | 无控制 DQN-RM TRPO-RM DDPG-DVSL |'
- en: '| Pandey et al. [[143](#bib.bib143)] | Sparce cooperative Q-learning [[144](#bib.bib144)]
    | Toll roads | Dynamic lane management | Personal simulator | Density based Ratio
    based Random search |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Pandey 等人 [[143](#bib.bib143)] | 稀疏协作 Q-learning [[144](#bib.bib144)] | 收费公路
    | 动态车道管理 | 个人模拟器 | 基于密度 基于比例 随机搜索 |'
- en: '| Pandey et al. [[145](#bib.bib145)] | Vanilla PG Proximal PG [[126](#bib.bib126)]
    | Express lane pricing | Multi-objective opt. Transfer learning | Personal simulator
    | Feedback Control |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Pandey 等人 [[145](#bib.bib145)] | Vanilla PG Proximal PG [[126](#bib.bib126)]
    | 快车道定价 | 多目标优化 转移学习 | 个人模拟器 | 反馈控制 |'
- en: '| Gunarathna et al. [[146](#bib.bib146)] | Multi-agent Q-learning | Lane direction
    change | Dynamic coordination graphs | New york real taxi trips | Different lane
    changing models |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Gunarathna 等人 [[146](#bib.bib146)] | 多智能体 Q-learning | 车道方向变更 | 动态协调图 | 纽约实际出租车行程
    | 不同的车道变更模型 |'
- en: '| Min et al. [[147](#bib.bib147)] | QR-DQN | Driver assistant | Lane keeping
    Lane change Acceleration control | Unity [[148](#bib.bib148)] | DQN DDQN |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Min 等人 [[147](#bib.bib147)] | QR-DQN | 驾驶辅助 | 车道保持 车道变更 加速控制 | Unity [[148](#bib.bib148)]
    | DQN DDQN |'
- en: '| Schults et al. [[149](#bib.bib149)] | DQN | Traffic simulator | Calibrating
    traffic models | - | - |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Schults 等人 [[149](#bib.bib149)] | DQN | 交通模拟器 | 交通模型标定 | - | - |'
- en: '| Bacchiani et al. [[150](#bib.bib150)] | A3C | Traffic simulator | Calibrating
    traffic models | - | - |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Bacchiani 等人 [[150](#bib.bib150)] | A3C | 交通模拟器 | 交通模型标定 | - | - |'
- en: 'Several useful deep RL mechanisms have been introduced for various other applications
    in ITS. One of the major application areas of AI techniques in ITS is autonomous
    vehicles, where deep RL occupies a great place in this context. Autonomous controlling
    is studied from various aspects using deep RL approaches. Ramp metering, lane
    changing, speed acceleration/deceleration, maneuvering on intersections are some
    of the various examples studied with deep RL (see Table [IV](#S6.T4 "TABLE IV
    ‣ VI Deep RL for Other ITS Applications ‣ Deep Reinforcement Learning for Intelligent
    Transportation Systems: A Survey")).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '已介绍几种有用的深度强化学习机制用于智能交通系统中的其他应用。人工智能技术在智能交通系统中的一个主要应用领域是自动驾驶，其中深度强化学习在此背景下占据重要地位。自动控制从多个方面使用深度强化学习方法进行研究。匝道计量、车道变更、加速/减速、交叉口的操控是一些使用深度强化学习研究的各种例子（见表
    [IV](#S6.T4 "TABLE IV ‣ VI Deep RL for Other ITS Applications ‣ Deep Reinforcement
    Learning for Intelligent Transportation Systems: A Survey")）。'
- en: VI-A Autonomous Driving
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 自动驾驶
- en: Initial papers presenting deep RL-based control for autonomous vehicles experiment
    their models on the TORCS game environment [[151](#bib.bib151)]. A control framework
    proposed by Sallab et al. [[111](#bib.bib111)] uses two types of deep RL methods,
    DQN approach with RNNs for discrete action set, and actor-critic based DDPG approach
    for continuous action domain. The authors experimented the algorithms without
    using replay memory on TORCS, which led to a faster convergence. Xia et al. [[112](#bib.bib112)]
    studied a control strategy called deep Q-learning with filtered experiences (DQFE)
    for teaching autonomous vehicle how to drive. The learning performance is shown
    to outperform the neural fitted Q-learning technique on the TORCS game simulator.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 初始论文展示了基于深度强化学习（RL）的自动驾驶控制，并在TORCS游戏环境中测试了他们的模型[[151](#bib.bib151)]。Sallab等人[[111](#bib.bib111)]提出的控制框架使用了两种类型的深度RL方法：用于离散动作集的DQN方法结合RNNs，以及用于连续动作域的基于演员-评论员的DDPG方法。作者在TORCS上实验了这些算法而未使用回放记忆，这导致了更快的收敛速度。Xia等人[[112](#bib.bib112)]研究了一种称为深度Q学习与过滤经验（DQFE）的控制策略，用于教导自动驾驶汽车如何驾驶。学习性能显示出在TORCS游戏模拟器上优于神经网络拟合Q学习技术。
- en: A continuous control strategy proposed in [[113](#bib.bib113)] combines the
    DDPG algorithm for continuous actions with a safety control strategy. The combination
    is needed because only relying on past experiences does not provide a safe autonomous
    vehicle control. Hoel et al. [[115](#bib.bib115)] introduced an autonomous driving
    model including planning and learning with Monte Carlo tree search and deep RL.
    Driving planning is done with Monte Carlo tree search and learning how to drive
    is done with deep RL agent using the AlphaGO Zero algorithm [[152](#bib.bib152)].
    In that work, the proposed method is compared with a baseline called IDM/MOBIL
    agent for expert driver behaviours [[153](#bib.bib153)], [[154](#bib.bib154)].
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[[113](#bib.bib113)]中提出的一种连续控制策略将DDPG算法与安全控制策略相结合。这种组合是必要的，因为仅依赖过去的经验无法提供安全的自动驾驶控制。Hoel等人[[115](#bib.bib115)]介绍了一个包括规划和学习的自动驾驶模型，使用蒙特卡罗树搜索和深度RL。驾驶规划通过蒙特卡罗树搜索完成，而驾驶学习则通过使用AlphaGO
    Zero算法的深度RL智能体来进行[[152](#bib.bib152)]。在这项工作中，提出的方法与一个称为IDM/MOBIL智能体的基线进行比较，该智能体用于专家驾驶行为[[153](#bib.bib153)]，[
    [154](#bib.bib154)]。'
- en: Authors in [[120](#bib.bib120)] studied car following and lane changing behaviours
    of autonomous vehicles using DDDP method on VISSIM. Another RL-based autonomous
    driving policy is described by Makantasis et al. [[121](#bib.bib121)] using DDQN
    with prioritized experience replay in mixed autonomy scenarios. Proposed deep
    RL-based driving policy is compared with DP-based optimal policy in different
    traffic densities using SUMO. Deep RL autonomous driving research generally targets
    individual agents in a mixed autonomy environment or a fully autonomous environment
    for finding the best driving strategy. However, authors in [[122](#bib.bib122)]
    proposed a multi-agent deep RL approach with dynamic coordination graph. In this
    study, autonomous vehicles in coordination learn how to behave in a highway scenario.
    Two distinct coordination graph models, identity-based dynamic coordination and
    position-based dynamic coordination, are studied in that work. Qian et al. [[123](#bib.bib123)]
    described autonomous driving from a different perspective using twin delayed DDPG
    [[155](#bib.bib155)]. They proposed a two-level strategy to fill the gap between
    decision making and future planning of autonomous vehicle. Autonomous driving
    in a signalized traffic intersection using DDPG method is proposed by Zhou et
    al. [[124](#bib.bib124)]. In a recent autonomous driving study [[125](#bib.bib125)],
    RL methods are analyzed on the traffic simulator CARLA [[156](#bib.bib156)] using
    RGB image inputs collected from a camera. A different training and test strategy
    is experimented by authors in [[127](#bib.bib127)] for DDPG-based autonomous driving
    using a human-in-the-loop dynamical simulator called IPG CarMaker. While a human
    driver controls the vehicle on this software, the DDPG agent learns how to drive
    in two distinct scenarios, forward driving and stopping.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[[120](#bib.bib120)] 的作者使用 DDDP 方法在 VISSIM 上研究了自动驾驶车辆的跟车和变道行为。Makantasis 等人[[121](#bib.bib121)]
    描述了另一种基于 RL 的自动驾驶策略，使用了 DDQN 和优先经验回放，在混合自治场景下进行研究。提出的基于深度 RL 的驾驶策略与不同交通密度下的 DP
    基优化策略进行了比较，使用了 SUMO。深度 RL 自动驾驶研究通常针对混合自治环境或完全自治环境中的个体代理，以寻找最佳驾驶策略。然而，[[122](#bib.bib122)]
    的作者提出了一种具有动态协调图的多代理深度 RL 方法。在这项研究中，协调中的自动驾驶车辆学习如何在高速公路场景中行为。该工作研究了两种不同的协调图模型：基于身份的动态协调和基于位置的动态协调。Qian
    等人[[123](#bib.bib123)] 从不同的角度描述了使用双延迟 DDPG[[155](#bib.bib155)] 的自动驾驶。他们提出了一种两级策略来填补决策和未来规划之间的差距。Zhou
    等人[[124](#bib.bib124)] 提出了使用 DDPG 方法在信号化交通交叉口进行自动驾驶。在最近的自动驾驶研究[[125](#bib.bib125)]中，分析了在交通模拟器
    CARLA[[156](#bib.bib156)] 上使用 RGB 图像输入的 RL 方法。[[127](#bib.bib127)] 的作者实验了一种不同的训练和测试策略，用于基于
    DDPG 的自动驾驶，使用一种称为 IPG CarMaker 的人机交互动态模拟器。在该软件上由人类司机控制车辆，而 DDPG 代理学习如何在前进和停车两种不同场景下驾驶。'
- en: In transportation research, controlling stop-and-go waves with autonomous vehicles
    is a new approach for which a deep RL-based solution is suggested in [[130](#bib.bib130)].
    The authors implemented multiple autonomous vehicles controlled by individual
    deep RL agents to increase the flow of traffic. Isele et al. [[128](#bib.bib128)]
    using the DQN approach studied a special case for self-driving vehicles, maneuvering
    in intersections when driver have partial knowledge about the intersection. In
    this paper, three action selection modes are tested. First action mode is stop
    or go, the second mode is having sequential actions, accelerate, decelerate or
    keep constant velocity, and the last action mode is the combination of first two
    action modes, wait, move slowly or go. All three action modes are tested on 5
    different cases.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在交通研究中，通过自动驾驶车辆控制停车-起步波是一种新方法，建议使用基于深度强化学习（RL）的解决方案[[130](#bib.bib130)]。作者实现了由各自的深度
    RL 代理控制的多个自动驾驶车辆，以提高交通流量。Isele 等人[[128](#bib.bib128)] 使用 DQN 方法研究了自驾车在司机对交叉口有部分了解时在交叉口的特殊情况。本文测试了三种行动选择模式。第一种行动模式是停车或前进，第二种模式是有序行动，即加速、减速或保持恒定速度，最后一种行动模式是前两种模式的结合，即等待、缓慢移动或前进。所有三种行动模式在
    5 种不同的情况下进行了测试。
- en: The authors in [[116](#bib.bib116)] proposed a speed and lane changing framework
    for autonomous truck-trailer with surrounded vehicles using double DQN. This work
    considers several traffic situations including a highway traffic and a two-way
    traffic scenario called overtaking in order to generalize the proposed algorithm.
    Using an inverse deep RL approach, Sharifzadeh et al. [[114](#bib.bib114)] presented
    a driving model for collision-free lane changing on a self-programmed traffic
    simulator with continuous trajectories. The investigated model includes two separate
    agents. One agent controls only lane changing without speed adjustment, and the
    other agent controls lane changing actions with acceleration. Another lane changing
    application for autonomous vehicles is presented in [[118](#bib.bib118)] considering
    DQN with quadratic Q-function approximator. A hierarchical control technique is
    implemented as a lane changing module in discrete domain, and a gap adjusting
    module in continuous domain with separate deep RL agents. Similar to the other
    papers, authors in [[119](#bib.bib119)] proposed a rule-based DQN approach for
    the lane changing problem for autonomous vehicles.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[[116](#bib.bib116)] 中的作者提出了一个用于自动驾驶卡车-拖车与周围车辆的速度和车道变化框架，采用双重 DQN。这项工作考虑了包括高速公路交通和一个被称为超车的双向交通场景在内的多种交通情况，以便对提出的算法进行泛化。Sharifzadeh
    等人 [[114](#bib.bib114)] 采用逆深度 RL 方法，在自编程的交通模拟器上提出了一种无碰撞车道变换的驾驶模型，具有连续轨迹。所研究的模型包括两个独立的代理。其中一个代理仅控制车道变换而不调整速度，另一个代理则控制带有加速的车道变换动作。另一个用于自动驾驶车辆的车道变换应用在
    [[118](#bib.bib118)] 中提出，考虑了带有二次 Q 函数逼近器的 DQN。实现了一种层次控制技术，作为离散领域中的车道变换模块，并在连续领域中设置了一个间隙调整模块，使用了独立的深度
    RL 代理。与其他论文类似，[[119](#bib.bib119)] 中的作者提出了一种基于规则的 DQN 方法用于解决自动驾驶车辆的车道变换问题。'
- en: Most of the learning based control models in ITS test the proposed work on simulators
    such as autonomous vehicle control, traffic signal control, traffic flow control.
    The first learned policy transfer from simulator to real world experiments is
    studied by Chalaki et al. [[132](#bib.bib132)]. Experiment platform for this research
    is a scaled city map from University of Delaware, in which behaviors of multiple
    autonomous vehicles in a roundabout is observed with deep RL control techniques.
    In order to transfer policies efficiently, adversarial noise is injected into
    the state and action spaces. The initial results of the same work for single agents
    with Gaussian noise is studied in [[133](#bib.bib133)].
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于学习的控制模型在智能交通系统（ITS）中测试提出的工作，如自动驾驶车辆控制、交通信号控制、交通流量控制。Chalaki 等人 [[132](#bib.bib132)]
    研究了从模拟器到实际世界实验的首个学习策略转移。该研究的实验平台是特拉华大学的缩小城市地图，在此地图中观察了多个自动驾驶车辆在环形交叉口的行为，并采用了深度
    RL 控制技术。为了有效转移策略，将对状态和动作空间注入对抗性噪声。相同工作的初步结果针对带高斯噪声的单一代理在 [[133](#bib.bib133)]
    中进行了研究。
- en: VI-B Energy Management
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 能量管理
- en: Energy management systems are a crucial part of future transportation. There
    are different resource allocation schemes for electric vehicles. Power consumption
    varies in different units of vehicle that highly effects the performance of batteries.
    Chaoui et al. proposed a deep RL-based energy management solution to increase
    the life cycle of parallel batteries [[136](#bib.bib136)]. Authors in [[138](#bib.bib138)]
    suggest an optimization model for energy consumption in hybrid vehicles using
    the DQN formulation. Proposed adaptive learning model provides a better fuel consumption
    through deep RL-based energy management scheme. Wu et al. [[137](#bib.bib137)]
    proposed an energy management solution for hybrid electric buses using an actor-critic
    based DDPG algorithm. Considering two parameters, number of passengers and traffic
    information, deep RL agent can optimize the energy consumption with continuous
    controlling.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 能量管理系统是未来交通的关键组成部分。电动车有不同的资源分配方案。功耗在不同的车辆单元中变化，这会极大影响电池的性能。Chaoui 等人提出了一种基于深度强化学习（RL）的能量管理解决方案，以延长并联电池的生命周期
    [[136](#bib.bib136)]。在 [[138](#bib.bib138)] 中的作者建议使用 DQN 公式来优化混合动力车辆的能量消耗。所提的自适应学习模型通过基于深度
    RL 的能量管理方案提供了更好的燃料消耗效果。Wu 等人 [[137](#bib.bib137)] 提出了一个用于混合动力电动公交车的能量管理解决方案，采用基于
    actor-critic 的 DDPG 算法。考虑到两个参数——乘客数量和交通信息，深度 RL 代理可以通过连续控制来优化能量消耗。
- en: VI-C Road Control
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 道路控制
- en: Road controllers are an essential part of traffic control in ITS. There are
    several works which use deep RL methods for speed limit control, toll road pricing,
    ramp metering, etc.Dynamic speed limit control among lanes is a challenging task
    in transportation. We et al. [[140](#bib.bib140)] studies a dynamic solution method
    with actor-critic continuous control scheme for variable speed limits control,
    that increases the flow rate and decreases the emission rate. Deep RL-based lane
    pricing model for toll roads is proposed in [[143](#bib.bib143)] to maximize the
    total revenue with multiple entrance and exits. Another dynamic lane pricing model
    for express lanes is proposed in [[145](#bib.bib145)], where authors used multi-objective
    RL and multi-class cell transmission models to enhance the performance of deep
    RL agent.Highway connections from side roads are controlled with signalized ramp
    meters. In order to increase the efficiency of the main road flow, a new multi-agent
    deep RL technique is proposed in [[134](#bib.bib134)] for traffic models based
    on discretized partial differential equations. The control model is tested on
    a simulated highway scenario with multiple ramp meters. Wu et al. [[141](#bib.bib141)]
    proposed a freeway control model using deep RL with various agents for different
    parts of freeway. Authors’ proposal is to use an inflow ramp meter control agent,
    a dynamic lane speed limit control agent, and a dynamic lane change controller
    agent in coordination. Traditional roads have fixed number of lanes for incoming
    and outgoing directions. Lane direction change is studied for improving the traffic
    flow with multi-agent deep RL and dynamic graph configuration in [[146](#bib.bib146)].Autonomous
    braking system via DQN is proposed in [[117](#bib.bib117)], which provides traffic
    safety in cases where immediate action is required.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 道路控制器是 ITS 中交通控制的一个重要部分。有几个研究使用深度 RL 方法进行速度限制控制、收费公路定价、匝道计量等。车道间的动态速度限制控制是交通运输中的一项挑战性任务。We
    等人[[140](#bib.bib140)] 研究了一种动态解决方法，使用演员-评论家连续控制方案进行可变速度限制控制，以提高流量并减少排放。[[143](#bib.bib143)]
    中提出了一种基于深度 RL 的车道定价模型，用于收费公路，以在多个入口和出口的情况下最大化总收入。[[145](#bib.bib145)] 中提出了另一种用于快速车道的动态车道定价模型，其中作者使用了多目标
    RL 和多类别单元传输模型，以提升深度 RL 代理的性能。高速公路从侧道的连接通过信号控制的匝道计量进行控制。为了提高主干道流量的效率，[[134](#bib.bib134)]
    中提出了一种新的多智能体深度 RL 技术，用于基于离散化偏微分方程的交通模型。该控制模型在具有多个匝道计量的模拟高速公路场景中进行了测试。Wu 等人[[141](#bib.bib141)]
    提出了一个使用深度 RL 的高速公路控制模型，其中不同部分的高速公路使用了各种智能体。作者提议协调使用一个流入匝道计量控制智能体、一个动态车道速度限制控制智能体和一个动态车道变更控制智能体。传统道路的进出车道数量是固定的。车道方向变更的研究旨在通过多智能体深度
    RL 和动态图配置来改善交通流量[[146](#bib.bib146)]。[[117](#bib.bib117)] 中提出了一个通过 DQN 实现的自动刹车系统，在需要立即采取行动的情况下提供交通安全。
- en: VI-D Various ITS Applications
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 各种 ITS 应用
- en: Recently a new tool for optimizing traffic simulators is proposed by Schultz
    at al. [[149](#bib.bib149)]. The input, (traffic characteristics) and output (traffic
    congestion) of traffic simulators are correlated with an adaptive learning technique
    using DQN. Another computational interface, named Flow, enables easy integration
    of the deep RL library RLlib [[157](#bib.bib157)] with SUMO and Aimsun for various
    control problems in ITS [[158](#bib.bib158)]. Flow users can create a custom network
    via Python to test complex control problems such as ramp meter control, adaptive
    traffic signalization and flow control with autonomous vehicles. Authors in [[150](#bib.bib150)]
    introduces a traffic simulator which provides a new environment with cooperative
    multi-agent learning approach for analyzing the behaviours of autonomous vehicles.
    It is capable of testing various traffic scenarios. Min et al. [[147](#bib.bib147)]
    proposed a driver assistant system using quantile regression DQN for various controls
    such as lane keeping, lane changing, and acceleration control.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Schultz 等人提出了一种新的优化交通模拟器的工具[[149](#bib.bib149)]。交通模拟器的输入（交通特性）和输出（交通拥堵）通过使用
    DQN 的自适应学习技术进行关联。另一个计算接口 Flow，允许深度 RL 库 RLlib [[157](#bib.bib157)] 与 SUMO 和 Aimsun
    进行轻松集成，用于 ITS 中的各种控制问题[[158](#bib.bib158)]。Flow 用户可以通过 Python 创建自定义网络，以测试复杂的控制问题，例如匝道控制、自适应交通信号控制和与自动驾驶车辆的流量控制。[[150](#bib.bib150)]
    中的作者介绍了一种交通模拟器，该模拟器提供了一种新的环境，通过合作的多智能体学习方法分析自动驾驶车辆的行为。它能够测试各种交通场景。Min 等人[[147](#bib.bib147)]
    提出了一个使用分位回归 DQN 的驾驶员辅助系统，用于车道保持、车道变更和加速控制等各种控制。
- en: VII Challenges and Open Research questions
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 挑战与未解研究问题
- en: Despite the significant interest and effort, and the promising results so far
    in deep RL-based ITS solutions, there are still many major challenges to address
    before the proposed research can yield real-world products. In this section, we
    will discuss the major challenges and open research questions of deep RL for ITS.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对深度RL基础的智能交通系统（ITS）解决方案有着显著的兴趣和努力，并且迄今为止取得了有希望的结果，但在提出的研究能够产生现实世界产品之前，仍然面临许多主要挑战。在本节中，我们将讨论深度RL在ITS中的主要挑战和未解研究问题。
- en: All the research outcomes for RL-based ITS controls are experimented on simulators
    due to life threatening consequences of real-world applications. Recently, authors
    in [[132](#bib.bib132)] presented a policy transfer application from simulation
    to a city-scale test environment for autonomous driving, but still this line of
    research is in its infancy. There is a huge gap between real-world deployment
    and simulator-based applications using learning algorithms. For TSC and other
    controlling applications in ITS, a real-world deployment is needed in order to
    prove the applicability of deep RL-based automated control.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 所有基于RL的ITS控制研究结果都在模拟器上进行实验，因为实际应用可能带来生命威胁。最近，作者在[[132](#bib.bib132)]中展示了一个从模拟到城市规模测试环境的政策转移应用，用于自动驾驶，但这一研究方向仍处于起步阶段。现实世界部署与使用学习算法的模拟器应用之间存在巨大差距。对于TSC和其他ITS控制应用，需要进行现实世界部署，以证明深度RL基础的自动化控制的适用性。
- en: Specifically for TSC, simulation-based applications have two approaches in literature,
    first, simulating an artificial road network with artificial data, second, simulating
    a road network based on a real dataset. While the second one is close to a realistic
    test, it only considers the traffic demand in various times of the day without
    realistic challenges. Another point that researchers need to consider for TSC
    is increasing the realism of simulation environments, such as including the human
    intervention scenarios. In order to decrease human intervention in TSC, the control
    system should be adaptable to unstable traffic situations in the worst case scenarios.
    To do that, instead of standard traffic models, urban networks with some predictable
    extreme scenarios should be studied in order to see the consequences of deep RL
    implementations. We expect that implementing pedestrians and public transportation
    to the simulation environments will have a high impact on learning performance.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 针对TSC，文献中基于模拟的应用有两种方法：首先，模拟一个人工道路网络并使用人工数据，其次，基于真实数据集模拟道路网络。虽然第二种方法接近现实测试，但它仅考虑了不同时间段的交通需求，没有现实挑战。研究人员需要考虑的另一个点是增加模拟环境的真实性，例如包括人为干预场景。为了减少TSC中的人为干预，控制系统应能适应最坏情况下的不稳定交通状况。为此，应该研究具有一些可预测极端场景的城市网络，而不是标准的交通模型，以观察深度RL实施的后果。我们期望将行人和公共交通纳入模拟环境将对学习性能产生重大影响。
- en: There are so many proposed deep RL models in the literature for controlling
    traffic lights. While standard RL models have comparisons between each other for
    validating their proposals, deep RL models on TSC do not have satisfactory comparisons
    with existing works. For multiple intersections, researchers mostly selected DQN,
    standard RL, and fixed-time controllers as benchmark. However, they should be
    especially compared with other multi-agent approaches in the literature, such
    as distributed control, coordinated control, etc. Another challenge with results
    is that very few papers compare their performance with actuated controller, which
    is the most popular real-world TSC implementation.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了许多用于控制交通信号灯的深度强化学习（RL）模型。尽管标准的RL模型之间有比较来验证其提案，但深度RL模型在交通信号控制（TSC）中的比较却不令人满意。对于多个交叉口，研究人员大多选择了DQN、标准RL和固定时间控制器作为基准。然而，它们应该特别与文献中的其他多智能体方法进行比较，如分布式控制、协调控制等。另一个挑战是很少有论文将其性能与常用的实时控制器进行比较，而实时控制器是最受欢迎的现实世界TSC实施方案。
- en: State definition is a crucial point in deep RL applications. Thus, researchers
    pay attention to different state forms with different hardware systems such as
    cameras, loop detectors, and sensors, but still there is no clear agreement on
    the form of state in deep RL-based TSC applications. State definition highly depends
    on static devices, hence all of them should always collect data properly. A new
    research direction could be studying partially observable and noisy state definitions
    in which some of the devices do not work properly. When RL-based adaptive traffic
    signals are implemented on intersections, the system must be protected and stable
    (i.e., robust and resilient) against such kind of failures.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 状态定义是深度强化学习应用中的一个关键点。因此，研究人员关注于不同的状态形式以及不同的硬件系统，如摄像头、回路检测器和传感器，但在基于深度强化学习的交通信号控制（TSC）应用中，状态形式仍未有明确的共识。状态定义高度依赖于静态设备，因此所有设备必须始终正确收集数据。一个新的研究方向可以是研究部分可观察和噪声状态定义，其中一些设备不能正常工作。当基于强化学习的自适应交通信号在交叉口实施时，系统必须在面对这种故障时保持保护和稳定（即，鲁棒和弹性）。
- en: Regarding autonomous vehicles, researchers have been proposing solutions to
    very specific subsystems without considering the interaction between such subsystems.
    For more realistic solutions, a unified management and adaptive control strategy
    is required for several components. For example, an impactful deep RL application
    should control lane changing, breaking, flow arranging, and energy management
    components all together. Implementing different learning algorithms for different
    autonomous vehicle subsystems may cause interoperability issues.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自动驾驶汽车，研究人员一直在提出针对非常具体子系统的解决方案，但没有考虑这些子系统之间的互动。为了获得更现实的解决方案，需要对多个组件实施统一的管理和自适应控制策略。例如，一个有影响力的深度强化学习（RL）应用应该同时控制车道变换、刹车、流量安排和能量管理组件。为不同的自动驾驶汽车子系统实施不同的学习算法可能会导致互操作性问题。
- en: VIII Conclusion
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: Considering the increasing world population and urbanization, researchers have
    been conducting research on ITS applications using learning-based AI techniques.
    Dynamic nature of traffic systems does not allow a clear easy control mechanism
    for all ITS applications. Controlling transportation systems through reinforcement
    learning (RL) approaches is gaining popularity in both industry and academia.
    There are various research outcomes in recent years for solving automated control
    problems in ITS, such as traffic lights, autonomous driving, autonomous break,
    and energy management of vehicles. The most popular deep RL application in ITS
    is adaptive traffic signal control (TSC) at intersections. We presented a comprehensive
    review for the deep RL applications in ITS. Key concepts of RL and deep RL, and
    the settings in which they are applied to TSC were discussed to provide a smooth
    introduction to the literature. Characteristic details of existing works in several
    categories were compared in separate tables in order to enable clear comparison.
    Finally, we also discussed the open research directions and the gap between the
    existing works and the real-world usage. This survey showed that there are different
    single agent and multi-agent RL solutions for TSC that outperform the standard
    control methods in simulation environments. However, existing works have still
    not been tested in real-world environments except for an autonomous vehicle application
    for a specific scenario.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到世界人口和城市化的增加，研究人员一直在使用基于学习的人工智能技术进行智能交通系统（ITS）应用的研究。交通系统的动态特性不允许所有 ITS 应用有一个简单的控制机制。通过强化学习（RL）方法控制交通系统在工业和学术界越来越受欢迎。近年来，在
    ITS 中解决自动控制问题的研究成果有很多，例如交通信号灯、自动驾驶、自动刹车和车辆能量管理。最受欢迎的深度强化学习应用是在交叉口的自适应交通信号控制（TSC）。我们对
    ITS 中的深度强化学习应用进行了全面的综述。讨论了 RL 和深度 RL 的关键概念，以及它们在 TSC 中应用的环境，以便为文献提供顺畅的介绍。对现有工作的特征细节在几个类别中进行了单独的表格比较，以便进行清晰的比较。最后，我们还讨论了开放的研究方向以及现有工作与实际应用之间的差距。这项调查显示，存在不同的单一代理和多代理
    RL 解决方案用于 TSC，这些解决方案在模拟环境中超越了标准控制方法。然而，现有工作仍未在现实环境中进行测试，除了一个针对特定场景的自动驾驶汽车应用。
- en: References
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] G. Cookson, “Inrix global traffic scorecard,” *INRIX research*, 2018.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] G. Cookson, “Inrix 全球交通评分卡，” *INRIX 研究*，2018。'
- en: '[2] Z. Liu, “A survey of intelligence methods in urban traffic signal control,”
    *IJCSNS International Journal of Computer Science and Network Security*, vol. 7,
    no. 7, pp. 105–112, 2007.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Z. Liu，“城市交通信号控制中的智能方法综述，” *IJCSNS计算机科学与网络安全国际期刊*，第7卷，第7期，第105–112页，2007年。'
- en: '[3] A. L. Bazzan, “Opportunities for multiagent systems and multiagent reinforcement
    learning in traffic control,” *Autonomous Agents and Multi-Agent Systems*, vol. 18,
    no. 3, p. 342, 2009.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. L. Bazzan，“多智能体系统和多智能体强化学习在交通控制中的机会，” *自主智能体与多智能体系统*，第18卷，第3期，第342页，2009年。'
- en: '[4] P. Mannion, J. Duggan, and E. Howley, “An experimental review of reinforcement
    learning algorithms for adaptive traffic signal control,” in *Autonomic Road Transport
    Support Systems*.   Springer, 2016, pp. 47–66.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] P. Mannion, J. Duggan 和 E. Howley，“适应性交通信号控制的强化学习算法的实验回顾，” 收录于 *自主道路交通支持系统*。
    Springer，2016年，第47–66页。'
- en: '[5] K.-L. A. Yau, J. Qadir, H. L. Khoo, M. H. Ling, and P. Komisarczuk, “A
    survey on reinforcement learning models and algorithms for traffic signal control,”
    *ACM Computing Surveys (CSUR)*, vol. 50, no. 3, p. 34, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] K.-L. A. Yau, J. Qadir, H. L. Khoo, M. H. Ling 和 P. Komisarczuk，“交通信号控制的强化学习模型和算法综述，”
    *ACM计算机调查（CSUR）*，第50卷，第3期，第34页，2017年。'
- en: '[6] W. Tong, A. Hussain, W. X. Bo, and S. Maharjan, “Artificial intelligence
    for vehicle-to-everything: A survey,” *IEEE Access*, vol. 7, pp. 10 823–10 843,
    2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] W. Tong, A. Hussain, W. X. Bo 和 S. Maharjan，“车联网的人工智能：综述，” *IEEE Access*，第7卷，第10 823–10 843页，2019年。'
- en: '[7] R. Abduljabbar, H. Dia, S. Liyanage, and S. Bagloee, “Applications of artificial
    intelligence in transport: An overview,” *Sustainability*, vol. 11, no. 1, p.
    189, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] R. Abduljabbar, H. Dia, S. Liyanage 和 S. Bagloee，“人工智能在交通中的应用：概述，” *可持续性*，第11卷，第1期，第189页，2019年。'
- en: '[8] H. Wei, G. Zheng, V. Gayah, and Z. Li, “A survey on traffic signal control
    methods,” *arXiv preprint arXiv:1904.08117*, 2019.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Wei, G. Zheng, V. Gayah 和 Z. Li，“交通信号控制方法综述，” *arXiv预印本 arXiv:1904.08117*，2019年。'
- en: '[9] M. Veres and M. Moussa, “Deep learning for intelligent transportation systems:
    A survey of emerging trends,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2019.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Veres 和 M. Moussa，“智能交通系统中的深度学习：新兴趋势的综述，” *IEEE智能交通系统汇刊*，2019年。'
- en: '[10] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani,
    and P. Pérez, “Deep reinforcement learning for autonomous driving: A survey,”
    *arXiv preprint arXiv:2002.00444*, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani
    和 P. Pérez，“用于自主驾驶的深度强化学习：综述，” *arXiv预印本 arXiv:2002.00444*，2020年。'
- en: '[11] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.   MIT
    press, 2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] R. S. Sutton 和 A. G. Barto，*强化学习：导论*。 MIT出版社，2018年。'
- en: '[12] C. J. Watkins and P. Dayan, “Q-learning,” *Machine learning*, vol. 8,
    no. 3-4, pp. 279–292, 1992.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] C. J. Watkins 和 P. Dayan，“Q学习，” *机器学习*，第8卷，第3-4期，第279–292页，1992年。'
- en: '[13] G. A. Rummery and M. Niranjan, *On-line Q-learning using connectionist
    systems*.   University of Cambridge, Department of Engineering Cambridge, England,
    1994, vol. 37.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] G. A. Rummery 和 M. Niranjan，*使用连接主义系统的在线Q学习*。 剑桥大学，工程系，英国剑桥，1994年，第37卷。'
- en: '[14] L. M. Rios and N. V. Sahinidis, “Derivative-free optimization: a review
    of algorithms and comparison of software implementations,” *Journal of Global
    Optimization*, vol. 56, no. 3, pp. 1247–1293, 2013.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] L. M. Rios 和 N. V. Sahinidis，“无导数优化：算法回顾及软件实现比较，” *全球优化期刊*，第56卷，第3期，第1247–1293页，2013年。'
- en: '[15] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine learning*, vol. 8, no. 3-4, pp.
    229–256, 1992.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] R. J. Williams，“用于连接主义强化学习的简单统计梯度跟随算法，” *机器学习*，第8卷，第3-4期，第229–256页，1992年。'
- en: '[16] L. C. Baird, “Reinforcement learning in continuous time: Advantage updating,”
    in *Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN’94)*,
    vol. 4.   IEEE, 1994, pp. 2448–2453.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] L. C. Baird，“连续时间中的强化学习：优势更新，” 收录于 *1994年IEEE国际神经网络会议（ICNN''94）论文集*，第4卷。
    IEEE，1994年，第2448–2453页。'
- en: '[17] L. Busoniu, R. Babuska, and B. De Schutter, “Multi-agent reinforcement
    learning: A survey,” in *2006 9th International Conference on Control, Automation,
    Robotics and Vision*.   IEEE, 2006, pp. 1–6.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. Busoniu, R. Babuska 和 B. De Schutter，“多智能体强化学习：综述，” 收录于 *2006年第九届国际控制、自动化、机器人与视觉会议*。
    IEEE，2006年，第1–6页。'
- en: '[18] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, p. 436, 2015.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. LeCun, Y. Bengio 和 G. Hinton，“深度学习，” *自然*，第521卷，第7553期，第436页，2015年。'
- en: '[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p.
    529, 2015.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*，“通过深度强化学习实现人类水平的控制，”
    *自然*，第518卷，第7540期，p. 529，2015年。'
- en: '[20] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, 和 D. Wierstra，“基于深度强化学习的连续控制，” *arXiv 预印本 arXiv:1509.02971*，2015年。'
- en: '[21] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas,
    “Sample efficient actor-critic with experience replay,” *arXiv preprint arXiv:1611.01224*,
    2016.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, 和 N. de
    Freitas，“样本高效的演员-评论员算法与经验回放，” *arXiv 预印本 arXiv:1611.01224*，2016年。'
- en: '[22] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *arXiv preprint arXiv:1511.05952*, 2015.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] T. Schaul, J. Quan, I. Antonoglou, 和 D. Silver，“优先级经验回放，” *arXiv 预印本 arXiv:1511.05952*，2015年。'
- en: '[23] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning,” in *Thirtieth AAAI Conference on Artificial Intelligence*,
    2016.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. Van Hasselt, A. Guez, 和 D. Silver，“使用双重q学习的深度强化学习，” 收录于 *第三十届AAAI人工智能会议*，2016年。'
- en: '[24] H. V. Hasselt, “Double q-learning,” in *Advances in Neural Information
    Processing Systems*, 2010, pp. 2613–2621.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] H. V. Hasselt，“双重q学习，” 收录于 *神经信息处理系统进展*，2010年，pp. 2613–2621。'
- en: '[25] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas,
    “Dueling network architectures for deep reinforcement learning,” *arXiv preprint
    arXiv:1511.06581*, 2015.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, 和 N. De Freitas，“深度强化学习的对抗网络架构，”
    *arXiv 预印本 arXiv:1511.06581*，2015年。'
- en: '[26] B. O’Donoghue, R. Munos, K. Kavukcuoglu, and V. Mnih, “Combining policy
    gradient and q-learning,” *arXiv preprint arXiv:1611.01626*, 2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] B. O’Donoghue, R. Munos, K. Kavukcuoglu, 和 V. Mnih，“结合策略梯度和q学习，” *arXiv
    预印本 arXiv:1611.01626*，2016年。'
- en: '[27] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International conference on machine learning*, 2016, pp. 1928–1937.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver, 和 K. Kavukcuoglu，“深度强化学习的异步方法，” 收录于 *国际机器学习会议*，2016年，pp. 1928–1937。'
- en: '[28] S. S. Mousavi, M. Schukat, and E. Howley, “Traffic light control using
    deep policy-gradient and value-function-based reinforcement learning,” *IET Intelligent
    Transport Systems*, vol. 11, no. 7, pp. 417–423, 2017.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. S. Mousavi, M. Schukat, 和 E. Howley，“使用深度策略梯度和价值函数基础的强化学习进行交通灯控制，”
    *IET智能交通系统*，第11卷，第7期，pp. 417–423，2017年。'
- en: '[29] D. Garg, M. Chli, and G. Vogiatzis, “Deep reinforcement learning for autonomous
    traffic light control,” in *2018 3rd IEEE International Conference on Intelligent
    Transportation Engineering (ICITE)*.   IEEE, 2018, pp. 214–218.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] D. Garg, M. Chli, 和 G. Vogiatzis，“用于自主交通灯控制的深度强化学习，” 载于 *2018年第3届IEEE智能交通工程国际会议（ICITE）*，IEEE，2018年，pp.
    214–218。'
- en: '[30] X. Liang, X. Du, G. Wang, and Z. Han, “A deep reinforcement learning network
    for traffic light cycle control,” *IEEE Transactions on Vehicular Technology*,
    vol. 68, no. 2, pp. 1243–1253, 2019.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. Liang, X. Du, G. Wang, 和 Z. Han，“用于交通灯周期控制的深度强化学习网络，” *IEEE车辆技术学报*，第68卷，第2期，pp.
    1243–1253，2019年。'
- en: '[31] W. Genders and S. Razavi, “Using a deep reinforcement learning agent for
    traffic signal control,” *arXiv preprint arXiv:1611.01142*, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] W. Genders 和 S. Razavi，“使用深度强化学习代理进行交通信号控制，” *arXiv 预印本 arXiv:1611.01142*，2016年。'
- en: '[32] E. Van der Pol and F. A. Oliehoek, “Coordinated deep reinforcement learners
    for traffic light control,” *Proceedings of Learning, Inference and Control of
    Multi-Agent Systems (at NIPS 2016)*, 2016.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] E. Van der Pol 和 F. A. Oliehoek，“协调的深度强化学习者用于交通灯控制，” *学习、推理和多智能体系统控制会议论文集（在NIPS
    2016上）*，2016年。'
- en: '[33] J. Gao, Y. Shen, J. Liu, M. Ito, and N. Shiratori, “Adaptive traffic signal
    control: Deep reinforcement learning algorithm with experience replay and target
    network,” *arXiv preprint arXiv:1705.02755*, 2017.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Gao, Y. Shen, J. Liu, M. Ito, 和 N. Shiratori，“自适应交通信号控制：具有经验回放和目标网络的深度强化学习算法，”
    *arXiv 预印本 arXiv:1705.02755*，2017年。'
- en: '[34] M. LIU, J. DENG, M. XU, X. ZHANG, and W. WANG, “Cooperative deep reinforcement
    learning for tra ic signal control,” 2017.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. LIU, J. DENG, M. XU, X. ZHANG, 和 W. WANG，“用于交通信号控制的合作深度强化学习，” 2017年。'
- en: '[35] S. Shi and F. Chen, “Deep recurrent q-learning method for area traffic
    coordination control,” *Journal of Advances in Mathematics and Computer Science*,
    pp. 1–11, 2018.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] S. Shi 和 F. Chen，“用于区域交通协调控制的深度递归Q学习方法，”*数学与计算机科学前沿期刊*，第1–11页，2018年。'
- en: '[36] S. M. A. Shabestary and B. Abdulhai, “Deep learning vs. discrete reinforcement
    learning for adaptive traffic signal control,” in *2018 21st International Conference
    on Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 286–293.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. M. A. Shabestary 和 B. Abdulhai，“深度学习与离散强化学习在自适应交通信号控制中的对比，”在*2018年第21届智能交通系统国际会议（ITSC）*上。IEEE，2018年，第286–293页。'
- en: '[37] C.-J. Choe, S. Baek, B. Woon, and S.-H. Kong, “Deep q learning with lstm
    for traffic light control,” in *2018 24th Asia-Pacific Conference on Communications
    (APCC)*.   IEEE, 2018, pp. 331–336.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] C.-J. Choe, S. Baek, B. Woon 和 S.-H. Kong，“结合LSTM的深度Q学习用于交通灯控制，”在*2018年第24届亚太通讯会议（APCC）*上。IEEE，2018年，第331–336页。'
- en: '[38] H. Wei, G. Zheng, H. Yao, and Z. Li, “Intellilight: A reinforcement learning
    approach for intelligent traffic light control,” in *Proceedings of the 24th ACM
    SIGKDD International Conference on Knowledge Discovery & Data Mining*.   ACM,
    2018, pp. 2496–2505.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] H. Wei, G. Zheng, H. Yao 和 Z. Li，“Intellilight: 一种用于智能交通灯控制的强化学习方法，”在*第24届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*上。ACM，2018年，第2496–2505页。'
- en: '[39] J. J. A. Calvo and I. Dusparic, “Heterogeneous multi-agent deep reinforcement
    learning for traffic lights control,” in *The 26th Irish Conference on Artificial
    Intelligence and Cognitive Science*, 2018, pp. 1–12.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. J. A. Calvo 和 I. Dusparic，“异质多智能体深度强化学习用于交通灯控制，”在*第26届爱尔兰人工智能与认知科学会议*上，2018年，第1–12页。'
- en: '[40] L. Li, Y. Lv, and F.-Y. Wang, “Traffic signal timing via deep reinforcement
    learning,” *IEEE/CAA Journal of Automatica Sinica*, vol. 3, no. 3, pp. 247–254,
    2016.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] L. Li, Y. Lv 和 F.-Y. Wang，“通过深度强化学习进行交通信号定时，”*IEEE/CAA自动化学报*，第3卷，第3期，第247–254页，2016年。'
- en: '[41] N. Casas, “Deep deterministic policy gradient for urban traffic light
    control,” *arXiv preprint arXiv:1703.09035*, 2017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] N. Casas，“用于城市交通灯控制的深度确定性策略梯度，”*arXiv预印本 arXiv:1703.09035*，2017年。'
- en: '[42] Y. Lin, X. Dai, L. Li, and F.-Y. Wang, “An efficient deep reinforcement
    learning model for urban traffic control,” *arXiv preprint arXiv:1808.01876*,
    2018.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Lin, X. Dai, L. Li 和 F.-Y. Wang，“一种高效的深度强化学习模型用于城市交通控制，”*arXiv预印本 arXiv:1808.01876*，2018年。'
- en: '[43] I. Jang, D. Kim, D. Lee, and Y. Son, “An agent-based simulation modeling
    with deep reinforcement learning for smart traffic signal control,” in *2018 International
    Conference on Information and Communication Technology Convergence (ICTC)*.   IEEE,
    2018, pp. 1028–1030.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] I. Jang, D. Kim, D. Lee 和 Y. Son，“基于深度强化学习的智能交通信号控制的代理模型仿真，”在*2018年信息与通信技术融合国际会议（ICTC）*上。IEEE，2018年，第1028–1030页。'
- en: '[44] P. Zhou, T. Braud, A. Alhilal, P. Hui, and J. Kangasharju, “Erl: Edge
    based reinforcement learning for optimized urban traffic light control.”'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] P. Zhou, T. Braud, A. Alhilal, P. Hui 和 J. Kangasharju，“ERL: 基于边缘的强化学习用于优化城市交通信号控制。”'
- en: '[45] C.-H. Wan and M.-C. Hwang, “Value-based deep reinforcement learning for
    adaptive isolated intersection signal control,” *IET Intelligent Transport Systems*,
    vol. 12, no. 9, pp. 1005–1010, 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C.-H. Wan 和 M.-C. Hwang，“基于价值的深度强化学习用于自适应孤立交叉口信号控制，”*IET智能交通系统*，第12卷，第9期，第1005–1010页，2018年。'
- en: '[46] H. Ge, Y. Song, C. Wu, J. Ren, and G. Tan, “Cooperative deep q-learning
    with q-value transfer for multi-intersection signal control,” *IEEE Access*, 2019.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Ge, Y. Song, C. Wu, J. Ren 和 G. Tan，“用于多交叉口信号控制的合作深度Q学习与Q值转移，”*IEEE
    Access*，2019年。'
- en: '[47] M. Xu, J. Wu, L. Huang, R. Zhou, T. Wang, and D. Hu, “Network-wide traffic
    signal control based on the discovery of critical nodes and deep reinforcement
    learning,” *Journal of Intelligent Transportation Systems*, pp. 1–10, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Xu, J. Wu, L. Huang, R. Zhou, T. Wang 和 D. Hu，“基于关键节点发现和深度强化学习的网络范围交通信号控制，”*智能交通系统期刊*，第1–10页，2018年。'
- en: '[48] W. Genders, “Deep reinforcement learning adaptive traffic signal control,”
    Ph.D. dissertation, 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] W. Genders，“深度强化学习自适应交通信号控制，”博士论文，2018年。'
- en: '[49] E. van der Pol, “Deep reinforcement learning for coordination in traffic
    light control,” *Master’s thesis, University of Amsterdam*, 2016.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] E. van der Pol，“协调交通灯控制的深度强化学习，”*阿姆斯特丹大学硕士论文*，2016年。'
- en: '[50] M. B. Natafgi, M. Osman, A. S. Haidar, and L. Hamandi, “Smart traffic
    light system using machine learning,” in *2018 IEEE International Multidisciplinary
    Conference on Engineering Technology (IMCET)*.   IEEE, 2018, pp. 1–6.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. B. Natafgi, M. Osman, A. S. Haidar 和 L. Hamandi, “使用机器学习的智能交通灯系统，”
    在 *2018年IEEE国际多学科工程技术会议（IMCET）* 中。IEEE，2018年，页码1–6。'
- en: '[51] M. Wiering, J. Vreeken, J. Van Veenen, and A. Koopman, “Simulation and
    optimization of traffic in a city,” in *IEEE Intelligent Vehicles Symposium, 2004*.   IEEE,
    2004, pp. 453–458.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] M. Wiering, J. Vreeken, J. Van Veenen 和 A. Koopman, “城市交通的模拟与优化，” 在 *IEEE智能车辆研讨会，2004年*
    中。IEEE，2004年，页码453–458。'
- en: '[52] M. Behrisch, L. Bieker, J. Erdmann, and D. Krajzewicz, “Sumo–simulation
    of urban mobility: an overview,” in *Proceedings of SIMUL 2011, The Third International
    Conference on Advances in System Simulation*.   ThinkMind, 2011.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. Behrisch, L. Bieker, J. Erdmann 和 D. Krajzewicz, “SUMO——城市流动性的模拟：概述，”
    在 *SIMUL 2011，第三届系统仿真国际会议论文集* 中。ThinkMind，2011年。'
- en: '[53] J. Casas, J. L. Ferrer, D. Garcia, J. Perarnau, and A. Torday, “Traffic
    simulation with aimsun,” in *Fundamentals of traffic simulation*.   Springer,
    2010, pp. 173–232.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Casas, J. L. Ferrer, D. Garcia, J. Perarnau 和 A. Torday, “使用Aimsun进行交通模拟，”
    在 *交通模拟基础* 中。Springer，2010年，页码173–232。'
- en: '[54] G. D. Cameron and G. I. Duncan, “Paramics—parallel microscopic simulation
    of road traffic,” *The Journal of Supercomputing*, vol. 10, no. 1, pp. 25–53,
    1996.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] G. D. Cameron 和 G. I. Duncan, “Paramics——道路交通的并行微观模拟，” *超级计算期刊*，第10卷，第1期，页码25–53，1996年。'
- en: '[55] M. Fellendorf and P. Vortisch, “Microscopic traffic flow simulator vissim,”
    in *Fundamentals of traffic simulation*.   Springer, 2010, pp. 63–93.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M. Fellendorf 和 P. Vortisch, “微观交通流量模拟器vissim，” 在 *交通模拟基础* 中。Springer，2010年，页码63–93。'
- en: '[56] T. L. Thorpe and C. W. Anderson, “Traffic light control using sarsa with
    three state representations,” Citeseer, Tech. Rep., 1996.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] T. L. Thorpe 和 C. W. Anderson, “使用三种状态表示的sarsa进行交通灯控制，” Citeseer，技术报告，1996年。'
- en: '[57] B. Abdulhai, R. Pringle, and G. J. Karakoulas, “Reinforcement learning
    for true adaptive traffic signal control,” *Journal of Transportation Engineering*,
    vol. 129, no. 3, pp. 278–285, 2003.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] B. Abdulhai, R. Pringle 和 G. J. Karakoulas, “用于真正自适应交通信号控制的强化学习，” *交通工程期刊*，第129卷，第3期，页码278–285，2003年。'
- en: '[58] E. Camponogara and W. Kraus, “Distributed learning agents in urban traffic
    control,” in *Portuguese Conference on Artificial Intelligence*.   Springer, 2003,
    pp. 324–335.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] E. Camponogara 和 W. Kraus, “城市交通控制中的分布式学习代理，” 在 *葡萄牙人工智能会议* 中。Springer，2003年，页码324–335。'
- en: '[59] K. Wen, S. Qu, and Y. Zhang, “A stochastic adaptive control model for
    isolated intersections,” in *2007 IEEE International Conference on Robotics and
    Biomimetics (ROBIO)*.   IEEE, 2007, pp. 2256–2260.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] K. Wen, S. Qu 和 Y. Zhang, “孤立交叉口的随机自适应控制模型，” 在 *2007年IEEE国际机器人与生物仿真会议（ROBIO）*
    中。IEEE，2007年，页码2256–2260。'
- en: '[60] S. El-Tantawy and B. Abdulhai, “An agent-based learning towards decentralized
    and coordinated traffic signal control,” in *13th International IEEE Conference
    on Intelligent Transportation Systems*.   IEEE, 2010, pp. 665–670.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. El-Tantawy 和 B. Abdulhai, “基于代理的学习用于去中心化和协调的交通信号控制，” 在 *第13届国际IEEE智能交通系统会议*
    中。IEEE，2010年，页码665–670。'
- en: '[61] S. El-Tantawy, B. Abdulhai, and H. Abdelgawad, “Design of reinforcement
    learning parameters for seamless application of adaptive traffic signal control,”
    *Journal of Intelligent Transportation Systems*, vol. 18, no. 3, pp. 227–245,
    2014.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] S. El-Tantawy, B. Abdulhai 和 H. Abdelgawad, “自适应交通信号控制的强化学习参数设计，” *智能交通系统期刊*，第18卷，第3期，页码227–245，2014年。'
- en: '[62] L. Shoufeng, L. Ximin, and D. Shiqiang, “Q-learning for adaptive traffic
    signal control based on delay minimization strategy,” in *2008 IEEE International
    Conference on Networking, Sensing and Control*.   IEEE, 2008, pp. 687–691.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] L. Shoufeng, L. Ximin 和 D. Shiqiang, “基于延迟最小化策略的自适应交通信号控制中的Q学习，” 在 *2008年IEEE国际网络、传感和控制会议*
    中。IEEE，2008年，页码687–691。'
- en: '[63] S. Touhbi, M. A. Babram, T. Nguyen-Huu, N. Marilleau, M. L. Hbid, C. Cambier,
    and S. Stinckwich, “Adaptive traffic signal control: Exploring reward definition
    for reinforcement learning,” *Procedia Computer Science*, vol. 109, pp. 513–520,
    2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Touhbi, M. A. Babram, T. Nguyen-Huu, N. Marilleau, M. L. Hbid, C. Cambier
    和 S. Stinckwich, “自适应交通信号控制：探索强化学习的奖励定义，” *Procedia计算机科学*，第109卷，页码513–520，2017年。'
- en: '[64] Y. K. Chin, L. K. Lee, N. Bolong, S. S. Yang, and K. T. K. Teo, “Exploring
    q-learning optimization in traffic signal timing plan management,” in *2011 Third
    International Conference on Computational Intelligence, Communication Systems
    and Networks*.   IEEE, 2011, pp. 269–274.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. K. Chin, L. K. Lee, N. Bolong, S. S. Yang 和 K. T. K. Teo，“探索交通信号时间计划管理中的q-learning优化”，收录于
    *2011 Third International Conference on Computational Intelligence, Communication
    Systems and Networks*。IEEE, 2011，第269–274页。'
- en: '[65] M. Wiering, “Multi-agent reinforcement learning for traffic light control,”
    in *Machine Learning: Proceedings of the Seventeenth International Conference
    (ICML’2000)*, 2000, pp. 1151–1158.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] M. Wiering，“用于交通信号灯控制的多智能体强化学习”，收录于 *Machine Learning: Proceedings of
    the Seventeenth International Conference (ICML’2000)*，2000年，第1151–1158页。'
- en: '[66] M. Steingrover, R. Schouten, S. Peelen, E. Nijhuis, B. Bakker *et al.*,
    “Reinforcement learning of traffic light controllers adapting to traffic congestion.”
    in *BNAIC*.   Citeseer, 2005, pp. 216–223.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] M. Steingrover, R. Schouten, S. Peelen, E. Nijhuis, B. Bakker *等*，“适应交通拥堵的交通信号灯控制器的强化学习”，收录于
    *BNAIC*。Citeseer, 2005，第216–223页。'
- en: '[67] J. Iša, J. Kooij, R. Koppejan, and L. Kuijer, “Reinforcement learning
    of traffic light controllers adapting to accidents,” *Design and Organisation
    of Autonomous Systems*, pp. 1–14, 2006.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. Iša, J. Kooij, R. Koppejan 和 L. Kuijer，“适应事故的交通信号灯控制器的强化学习”，*Design
    and Organisation of Autonomous Systems*，第1–14页，2006年。'
- en: '[68] L. Kuyer, S. Whiteson, B. Bakker, and N. Vlassis, “Multiagent reinforcement
    learning for urban traffic control using coordination graphs,” in *Joint European
    Conference on Machine Learning and Knowledge Discovery in Databases*.   Springer,
    2008, pp. 656–671.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] L. Kuyer, S. Whiteson, B. Bakker 和 N. Vlassis，“用于城市交通控制的多智能体强化学习使用协调图”，收录于
    *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*。Springer,
    2008，第656–671页。'
- en: '[69] B. Bakker, S. Whiteson, L. Kester, and F. C. Groen, “Traffic light control
    by multiagent reinforcement learning systems,” in *Interactive Collaborative Information
    Systems*.   Springer, 2010, pp. 475–510.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] B. Bakker, S. Whiteson, L. Kester 和 F. C. Groen，“通过多智能体强化学习系统进行交通信号灯控制”，收录于
    *Interactive Collaborative Information Systems*。Springer, 2010，第475–510页。'
- en: '[70] D. Houli, L. Zhiheng, and Z. Yi, “Multiobjective reinforcement learning
    for traffic signal control using vehicular ad hoc network,” *EURASIP journal on
    advances in signal processing*, vol. 2010, no. 1, p. 724035, 2010.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] D. Houli, L. Zhiheng 和 Z. Yi，“基于车载自组网的交通信号控制的多目标强化学习”，*EURASIP journal
    on advances in signal processing*，第2010卷，第1期，第724035页，2010年。'
- en: '[71] T. Brys, T. T. Pham, and M. E. Taylor, “Distributed learning and multi-objectivity
    in traffic light control,” *Connection Science*, vol. 26, no. 1, pp. 65–83, 2014.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] T. Brys, T. T. Pham 和 M. E. Taylor，“交通信号灯控制中的分布式学习和多目标性”，*Connection Science*，第26卷，第1期，第65–83页，2014年。'
- en: '[72] M. E. Taylor, M. Jain, P. Tandon, M. Yokoo, and M. Tambe, “Distributed
    on-line multi-agent optimization under uncertainty: Balancing exploration and
    exploitation,” *Advances in Complex Systems*, vol. 14, no. 03, pp. 471–528, 2011.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. E. Taylor, M. Jain, P. Tandon, M. Yokoo 和 M. Tambe，“在不确定性下的分布式在线多智能体优化：平衡探索与开发”，*Advances
    in Complex Systems*，第14卷，第03期，第471–528页，2011年。'
- en: '[73] M. A. Khamis, W. Gomaa, and H. El-Shishiny, “Multi-objective traffic light
    control system based on bayesian probability interpretation,” in *2012 15th International
    IEEE Conference on Intelligent Transportation Systems*.   IEEE, 2012, pp. 995–1000.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. A. Khamis, W. Gomaa 和 H. El-Shishiny，“基于贝叶斯概率解释的多目标交通信号控制系统”，收录于 *2012
    15th International IEEE Conference on Intelligent Transportation Systems*。IEEE,
    2012，第995–1000页。'
- en: '[74] M. A. Khamis and W. Gomaa, “Enhanced multiagent multi-objective reinforcement
    learning for urban traffic light control,” in *2012 11th International Conference
    on Machine Learning and Applications*, vol. 1.   IEEE, 2012, pp. 586–591.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. A. Khamis 和 W. Gomaa，“用于城市交通信号灯控制的增强型多智能体多目标强化学习”，收录于 *2012 11th International
    Conference on Machine Learning and Applications*，第1卷。IEEE, 2012，第586–591页。'
- en: '[75] G. W. Khamis, Mohamed A, “Adaptive multi-objective reinforcement learning
    with hybrid exploration for traffic signal control based on cooperative multi-agent
    framework,” *Engineering Applications of Artificial Intelligence*, vol. 29, pp.
    134–151, 2014.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] G. W. Khamis, Mohamed A，“基于合作多智能体框架的混合探索的自适应多目标强化学习用于交通信号控制”，*Engineering
    Applications of Artificial Intelligence*，第29卷，第134–151页，2014年。'
- en: '[76] S.-B. Cools, C. Gershenson, and B. D’Hooghe, “Self-organizing traffic
    lights: A realistic simulation,” in *Advances in applied self-organizing systems*.   Springer,
    2013, pp. 45–55.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S.-B. Cools, C. Gershenson 和 B. D’Hooghe，“自组织交通信号灯：一种现实的仿真”，收录于 *Advances
    in applied self-organizing systems*。Springer, 2013，第45–55页。'
- en: '[77] J. Jin and X. Ma, “A multi-objective agent-based control approach with
    application in intelligent traffic signal system,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] J. Jin 和 X. Ma，“具有多目标的基于代理的控制方法在智能交通信号系统中的应用”，*IEEE智能交通系统汇刊*，2019年。'
- en: '[78] B. S. Prashanth, LA, “Reinforcement learning with average cost for adaptive
    control of traffic lights at intersections,” in *2011 14th International IEEE
    Conference on Intelligent Transportation Systems (ITSC)*.   IEEE, 2011, pp. 1640–1645.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] B. S. Prashanth, LA，“用于交叉口交通灯自适应控制的平均成本强化学习”，见于*2011年第14届国际IEEE智能交通系统会议（ITSC）*。IEEE，2011年，页码1640–1645。'
- en: '[79] L. Prashanth and S. Bhatnagar, “Reinforcement learning with function approximation
    for traffic signal control,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 12, no. 2, pp. 412–421, 2011.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] L. Prashanth 和 S. Bhatnagar，“用于交通信号控制的功能逼近强化学习”，*IEEE智能交通系统汇刊*，第12卷，第2期，页码412–421，2011年。'
- en: '[80] T. T. Pham, T. Brys, and M. E. Taylor, “Learning coordinated traffic light
    control,” in *Proceedings of the Adaptive and Learning Agents workshop (at AAMAS-13)*,
    vol. 10.   IEEE, 2013, pp. 1196–1201.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] T. T. Pham, T. Brys 和 M. E. Taylor，“协调交通灯控制的学习”，见于*自适应与学习代理研讨会（在AAMAS-13上）*，第10卷。IEEE，2013年，页码1196–1201。'
- en: '[81] M. Abdoos, N. Mozayani, and A. L. Bazzan, “Hierarchical control of traffic
    signals using q-learning with tile coding,” *Applied intelligence*, vol. 40, no. 2,
    pp. 201–213, 2014.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Abdoos, N. Mozayani 和 A. L. Bazzan，“使用带有平铺编码的q学习进行交通信号的分层控制”，*应用智能*，第40卷，第2期，页码201–213，2014年。'
- en: '[82] I. Arel, C. Liu, T. Urbanik, and A. Kohls, “Reinforcement learning-based
    multi-agent system for network traffic signal control,” *IET Intelligent Transport
    Systems*, vol. 4, no. 2, pp. 128–135, 2010.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] I. Arel, C. Liu, T. Urbanik 和 A. Kohls，“基于强化学习的多智能体系统用于网络交通信号控制”，*IET
    智能交通系统*，第4卷，第2期，页码128–135，2010年。'
- en: '[83] S. El-Tantawy and B. Abdulhai, “Multi-agent reinforcement learning for
    integrated network of adaptive traffic signal controllers (marlin-atsc),” in *2012
    15th International IEEE Conference on Intelligent Transportation Systems*.   IEEE,
    2012, pp. 319–326.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. El-Tantawy 和 B. Abdulhai，“用于自适应交通信号控制器综合网络的多智能体强化学习（marlin-atsc）”，见于*2012年第15届国际IEEE智能交通系统会议*。IEEE，2012年，页码319–326。'
- en: '[84] S. El-Tantawy, B. Abdulhai, and H. Abdelgawad, “Multiagent reinforcement
    learning for integrated network of adaptive traffic signal controllers (marlin-atsc):
    methodology and large-scale application on downtown toronto,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 14, no. 3, pp. 1140–1150, 2013.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S. El-Tantawy, B. Abdulhai 和 H. Abdelgawad，“用于自适应交通信号控制器综合网络（marlin-atsc）的多智能体强化学习：方法论及其在多伦多市区的大规模应用”，*IEEE智能交通系统汇刊*，第14卷，第3期，页码1140–1150，2013年。'
- en: '[85] A. a. Salkham, R. Cunningham, A. Garg, and V. Cahill, “A collaborative
    reinforcement learning approach to urban traffic control optimization,” in *Proceedings
    of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent
    Agent Technology-Volume 02*.   IEEE Computer Society, 2008, pp. 560–566.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. a. Salkham, R. Cunningham, A. Garg 和 V. Cahill，“一种协作的强化学习方法用于城市交通控制优化”，见于*2008
    IEEE/WIC/ACM国际会议论文集：网络智能与智能代理技术-第02卷*。IEEE计算机学会，2008年，页码560–566。'
- en: '[86] S. Richter *et al.*, “Learning road traffic control: towards practical
    traffic control using policy gradients,” 2006.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] S. Richter *等*，“学习道路交通控制：基于策略梯度的实际交通控制方法”，2006年。'
- en: '[87] H. A. Aziz, F. Zhu, and S. V. Ukkusuri, “Learning-based traffic signal
    control algorithms with neighborhood information sharing: An application for sustainable
    mobility,” *Journal of Intelligent Transportation Systems*, vol. 22, no. 1, pp.
    40–52, 2018.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. A. Aziz, F. Zhu 和 S. V. Ukkusuri，“基于学习的交通信号控制算法与邻域信息共享：可持续交通的应用”，*智能交通系统期刊*，第22卷，第1期，页码40–52，2018年。'
- en: '[88] M. Aslani, M. S. Mesgari, and M. Wiering, “Adaptive traffic signal control
    with actor-critic methods in a real-world traffic network with different traffic
    disruption events,” *Transportation Research Part C: Emerging Technologies*, vol. 85,
    pp. 732–752, 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. Aslani, M. S. Mesgari 和 M. Wiering，“在具有不同交通干扰事件的实际交通网络中使用演员-评论家方法进行自适应交通信号控制”，*运输研究C部分：新兴技术*，第85卷，页码732–752，2017年。'
- en: '[89] L.-H. Xu, X.-H. Xia, and Q. Luo, “The study of reinforcement learning
    for traffic self-adaptive control under multiagent markov game environment,” *Mathematical
    Problems in Engineering*, vol. 2013, 2013.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] L.-H. Xu, X.-H. Xia, 和 Q. Luo，“多智能体Markov游戏环境下交通自适应控制的强化学习研究，”*工程中的数学问题*，第2013卷，2013。'
- en: '[90] M. Abdoos, N. Mozayani, and A. L. Bazzan, “Traffic light control in non-stationary
    environments based on multi agent q-learning,” in *2011 14th International IEEE
    conference on intelligent transportation systems (ITSC)*.   IEEE, 2011, pp. 1580–1585.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Abdoos, N. Mozayani, 和 A. L. Bazzan，“基于多智能体Q学习的非平稳环境中的交通信号控制，”发表于*2011年第14届国际IEEE智能交通系统会议（ITSC）*。IEEE，2011，第1580–1585页。'
- en: '[91] P. Balaji, X. German, and D. Srinivasan, “Urban traffic signal control
    using reinforcement learning agents,” *IET Intelligent Transport Systems*, vol. 4,
    no. 3, pp. 177–188, 2010.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] P. Balaji, X. German, 和 D. Srinivasan，“使用强化学习代理的城市交通信号控制，”*IET智能交通系统*，第4卷，第3期，第177–188页，2010。'
- en: '[92] C. K. Keong, “The glide system—singapore’s urban traffic control system,”
    *Transport reviews*, vol. 13, no. 4, pp. 295–305, 1993.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] C. K. Keong，“滑行系统—新加坡城市交通控制系统，”*交通评论*，第13卷，第4期，第295–305页，1993。'
- en: '[93] V. Cahill *et al.*, “Soilse: A decentralized approach to optimization
    of fluctuating urban traffic using reinforcement learning,” in *13th International
    IEEE Conference on Intelligent Transportation Systems*.   IEEE, 2010, pp. 531–538.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] V. Cahill *等*，“Soilse: 使用强化学习优化波动城市交通的分散方法，”发表于*第13届国际IEEE智能交通系统会议*。IEEE，2010，第531–538页。'
- en: '[94] S. Araghi, A. Khosravi, and D. Creighton, “Distributed q-learning controller
    for a multi-intersection traffic network,” in *International Conference on Neural
    Information Processing*.   Springer, 2015, pp. 337–344.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. Araghi, A. Khosravi, 和 D. Creighton，“多交叉口交通网络的分布式Q学习控制器，”发表于*国际神经信息处理会议*。Springer，2015，第337–344页。'
- en: '[95] C. Liu, X. Xu, and D. Hu, “Multiobjective reinforcement learning: A comprehensive
    overview,” *IEEE Transactions on Systems, Man, and Cybernetics: Systems*, vol. 45,
    no. 3, pp. 385–398, 2015.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] C. Liu, X. Xu, 和 D. Hu，“多目标强化学习：全面概述，”*IEEE系统、人和控制论学报：系统*，第45卷，第3期，第385–398页，2015。'
- en: '[96] T. Chu, S. Qu, and J. Wang, “Large-scale traffic grid signal control with
    regional reinforcement learning,” in *2016 American Control Conference (ACC)*.   IEEE,
    2016, pp. 815–820.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] T. Chu, S. Qu, 和 J. Wang，“区域强化学习的大规模交通网格信号控制，”发表于*2016美国控制会议（ACC）*。IEEE，2016，第815–820页。'
- en: '[97] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，”发表于*IEEE计算机视觉与模式识别会议论文集*，2016，第770–778页。'
- en: '[98] W. Genders and S. Razavi, “Evaluating reinforcement learning state representations
    for adaptive traffic signal control,” *Procedia computer science*, vol. 130, pp.
    26–33, 2018.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] W. Genders 和 S. Razavi，“评估用于自适应交通信号控制的强化学习状态表示，”*计算机科学程序*，第130卷，第26–33页，2018。'
- en: '[99] M. Coşkun, A. Baggag, and S. Chawla, “Deep reinforcement learning for
    traffic light optimization,” in *2018 IEEE International Conference on Data Mining
    Workshops (ICDMW)*.   IEEE, 2018, pp. 564–571.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] M. Coşkun, A. Baggag, 和 S. Chawla，“用于交通信号优化的深度强化学习，”发表于*2018 IEEE国际数据挖掘会议研讨会（ICDMW）*。IEEE，2018，第564–571页。'
- en: '[100] T. Nishi, K. Otaki, K. Hayakawa, and T. Yoshimura, “Traffic signal control
    based on reinforcement learning with graph convolutional neural nets,” in *2018
    21st International Conference on Intelligent Transportation Systems (ITSC)*.   IEEE,
    2018, pp. 877–883.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] T. Nishi, K. Otaki, K. Hayakawa, 和 T. Yoshimura，“基于图卷积神经网络的强化学习交通信号控制，”发表于*2018年第21届国际智能交通系统会议（ITSC）*。IEEE，2018，第877–883页。'
- en: '[101] M. Riedmiller, “Neural fitted q iteration–first experiences with a data
    efficient neural reinforcement learning method,” in *European Conference on Machine
    Learning*.   Springer, 2005, pp. 317–328.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] M. Riedmiller，“神经拟合Q迭代—数据高效神经强化学习方法的初步经验，”发表于*欧洲机器学习会议*。Springer，2005，第317–328页。'
- en: '[102] T. Chu, J. Wang, L. Codecà, and Z. Li, “Multi-agent deep reinforcement
    learning for large-scale traffic signal control,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] T. Chu, J. Wang, L. Codecà, 和 Z. Li，“用于大规模交通信号控制的多智能体深度强化学习，”*IEEE智能交通系统学报*，2019。'
- en: '[103] W. Genders and S. Razavi, “Asynchronous n-step q-learning adaptive traffic
    signal control,” *Journal of Intelligent Transportation Systems*, vol. 23, no. 4,
    pp. 319–331, 2019.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] W. Genders 和 S. Razavi, “异步 n 步 Q 学习自适应交通信号控制，” *Journal of Intelligent
    Transportation Systems*, vol. 23, no. 4, pp. 319–331, 2019。'
- en: '[104] T. Tan, F. Bao, Y. Deng, A. Jin, Q. Dai, and J. Wang, “Cooperative deep
    reinforcement learning for large-scale traffic grid signal control,” *IEEE transactions
    on cybernetics*, 2019.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] T. Tan, F. Bao, Y. Deng, A. Jin, Q. Dai, 和 J. Wang, “大规模交通网格信号控制的合作深度强化学习，”
    *IEEE 网络系统交易*, 2019。'
- en: '[105] X.-Y. Liu, Z. Ding, S. Borst, and A. Walid, “Deep reinforcement learning
    for intelligent transportation systems,” *32nd Conference on Neural Information
    Processing Systems (NIPS 2018), Montreal, Canada.*, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] X.-Y. Liu, Z. Ding, S. Borst, 和 A. Walid, “用于智能交通系统的深度强化学习，” *第32届神经信息处理系统会议
    (NIPS 2018), 蒙特利尔, 加拿大*，2018。'
- en: '[106] R. Zhang, A. Ishikawa, W. Wang, B. Striner, and O. Tonguz, “Intelligent
    traffic signal control: Using reinforcement learning with partial detection,”
    *arXiv preprint arXiv:1807.01628*, 2018.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] R. Zhang, A. Ishikawa, W. Wang, B. Striner, 和 O. Tonguz, “智能交通信号控制：使用部分检测的强化学习，”
    *arXiv 预印本 arXiv:1807.01628*, 2018。'
- en: '[107] N. Xu, G. Zheng, K. Xu, Y. Zhu, and Z. Li, “Targeted knowledge transfer
    for learning traffic signal plans,” in *Pacific-Asia Conference on Knowledge Discovery
    and Data Mining*.   Springer, 2019, pp. 175–187.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] N. Xu, G. Zheng, K. Xu, Y. Zhu, 和 Z. Li, “针对交通信号计划的目标知识迁移，” 见于 *Pacific-Asia
    Conference on Knowledge Discovery and Data Mining*。   Springer, 2019, pp. 175–187。'
- en: '[108] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli,
    and S. Whiteson, “Stabilising experience replay for deep multi-agent reinforcement
    learning,” in *Proceedings of the 34th International Conference on Machine Learning-Volume
    70*.   JMLR. org, 2017, pp. 1146–1155.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli,
    和 S. Whiteson, “稳定深度多智能体强化学习的经验回放，” 见于 *第34届国际机器学习会议论文集-第70卷*。   JMLR.org, 2017,
    pp. 1146–1155。'
- en: '[109] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-dimensional
    continuous control using generalized advantage estimation,” *arXiv preprint arXiv:1506.02438*,
    2015.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] J. Schulman, P. Moritz, S. Levine, M. Jordan, 和 P. Abbeel, “使用广义优势估计的高维连续控制，”
    *arXiv 预印本 arXiv:1506.02438*, 2015。'
- en: '[110] G. Dulac-Arnold, R. Evans, H. van Hasselt, P. Sunehag, T. Lillicrap,
    J. Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin, “Deep reinforcement learning
    in large discrete action spaces,” *arXiv preprint arXiv:1512.07679*, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] G. Dulac-Arnold, R. Evans, H. van Hasselt, P. Sunehag, T. Lillicrap,
    J. Hunt, T. Mann, T. Weber, T. Degris, 和 B. Coppin, “在大规模离散动作空间中的深度强化学习，” *arXiv
    预印本 arXiv:1512.07679*, 2015。'
- en: '[111] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement
    learning framework for autonomous driving,” *Electronic Imaging*, vol. 2017, no. 19,
    pp. 70–76, 2017.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. E. Sallab, M. Abdou, E. Perot, 和 S. Yogamani, “用于自主驾驶的深度强化学习框架，” *Electronic
    Imaging*, vol. 2017, no. 19, pp. 70–76, 2017。'
- en: '[112] W. Xia, H. Li, and B. Li, “A control strategy of autonomous vehicles
    based on deep reinforcement learning,” in *2016 9th International Symposium on
    Computational Intelligence and Design (ISCID)*, vol. 2.   IEEE, 2016, pp. 198–201.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] W. Xia, H. Li, 和 B. Li, “基于深度强化学习的自主车辆控制策略，” 见于 *2016年第9届计算智能与设计国际研讨会
    (ISCID)*, vol. 2。   IEEE, 2016, pp. 198–201。'
- en: '[113] X. Xiong, J. Wang, F. Zhang, and K. Li, “Combining deep reinforcement
    learning and safety based control for autonomous driving,” *arXiv preprint arXiv:1612.00147*,
    2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] X. Xiong, J. Wang, F. Zhang, 和 K. Li, “将深度强化学习与基于安全的控制结合用于自主驾驶，” *arXiv
    预印本 arXiv:1612.00147*, 2016。'
- en: '[114] S. Sharifzadeh, I. Chiotellis, R. Triebel, and D. Cremers, “Learning
    to drive using inverse reinforcement learning and deep q-networks,” *arXiv preprint
    arXiv:1612.03653*, 2016.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] S. Sharifzadeh, I. Chiotellis, R. Triebel, 和 D. Cremers, “使用逆强化学习和深度
    Q 网络学习驾驶，” *arXiv 预印本 arXiv:1612.03653*, 2016。'
- en: '[115] C.-J. Hoel, K. Driggs-Campbell, K. Wolff, L. Laine, and M. J. Kochenderfer,
    “Combining planning and deep reinforcement learning in tactical decision making
    for autonomous driving,” *arXiv preprint arXiv:1905.02680*, 2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] C.-J. Hoel, K. Driggs-Campbell, K. Wolff, L. Laine, 和 M. J. Kochenderfer,
    “在自主驾驶的战术决策中结合规划和深度强化学习，” *arXiv 预印本 arXiv:1905.02680*, 2019。'
- en: '[116] C.-J. Hoel, K. Wolff, and L. Laine, “Automated speed and lane change
    decision making using deep reinforcement learning,” in *2018 21st International
    Conference on Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 2148–2155.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] C.-J. Hoel, K. Wolff, 和 L. Laine, “使用深度强化学习进行自动化速度和车道变换决策，” 见于 *2018年第21届国际智能交通系统会议
    (ITSC)*。   IEEE, 2018, pp. 2148–2155。'
- en: '[117] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, and J. W. Choi, “Autonomous
    braking system via deep reinforcement learning,” in *2017 IEEE 20th International
    Conference on Intelligent Transportation Systems (ITSC)*.   IEEE, 2017, pp. 1–6.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, 和 J. W. Choi, “通过深度强化学习实现的自动刹车系统，”
    见 *2017 IEEE第20届智能交通系统国际会议（ITSC）*。   IEEE，2017年，第1–6页。'
- en: '[118] T. Shi, P. Wang, X. Cheng, and C.-Y. Chan, “Driving decision and control
    for autonomous lane change based on deep reinforcement learning,” *arXiv preprint
    arXiv:1904.10171*, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] T. Shi, P. Wang, X. Cheng, 和 C.-Y. Chan, “基于深度强化学习的自动驾驶车道变更决策和控制，” *arXiv预印本
    arXiv:1904.10171*，2019年。'
- en: '[119] J. Wang, Q. Zhang, D. Zhao, and Y. Chen, “Lane change decision-making
    through deep reinforcement learning with rule-based constraints,” *arXiv preprint
    arXiv:1904.00231*, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] J. Wang, Q. Zhang, D. Zhao, 和 Y. Chen, “通过深度强化学习和基于规则的约束进行车道变更决策，” *arXiv预印本
    arXiv:1904.00231*，2019年。'
- en: '[120] Y. Ye, X. Zhang, and J. Sun, “Automated vehicle’s behavior decision making
    using deep reinforcement learning and high-fidelity simulation environment,” *Transportation
    Research Part C: Emerging Technologies*, vol. 107, pp. 155–170, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y. Ye, X. Zhang, 和 J. Sun, “使用深度强化学习和高保真模拟环境进行自动驾驶行为决策，” *运输研究C部分：新兴技术*，第107卷，第155–170页，2019年。'
- en: '[121] K. Makantasis, M. Kontorinaki, and I. Nikolos, “Deep reinforcement-learning-based
    driving policy for autonomous road vehicles,” *IET Intelligent Transport Systems*,
    2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] K. Makantasis, M. Kontorinaki, 和 I. Nikolos, “基于深度强化学习的自动驾驶车辆驾驶策略，” *IET智能交通系统*，2019年。'
- en: '[122] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, and
    G. Tan, “Distributed multiagent coordinated learning for autonomous driving in
    highways based on dynamic coordination graphs,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, 和 G.
    Tan, “基于动态协调图的高速公路自动驾驶分布式多智能体协调学习，” *IEEE智能交通系统汇刊*，2019年。'
- en: '[123] L. Qian, X. Xu, Y. Zeng, and J. Huang, “Deep, consistent behavioral decision
    making with planning features for autonomous vehicles,” *Electronics*, vol. 8,
    no. 12, p. 1492, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] L. Qian, X. Xu, Y. Zeng, 和 J. Huang, “具有规划特征的深度一致性行为决策用于自动驾驶车辆，” *电子学*，第8卷，第12期，第1492页，2019年。'
- en: '[124] M. Zhou, Y. Yu, and X. Qu, “Development of an efficient driving strategy
    for connected and automated vehicles at signalized intersections: A reinforcement
    learning approach,” *IEEE Transactions on Intelligent Transportation Systems*,
    2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] M. Zhou, Y. Yu, 和 X. Qu, “信号灯交叉口连接和自动化车辆的高效驾驶策略开发：一种强化学习方法，” *IEEE智能交通系统汇刊*，2019年。'
- en: '[125] B. Osiński, A. Jakubowski, P. Miłoś, P. Zikecina, C. Galias, and H. Michalewski,
    “Simulation-based reinforcement learning for real-world autonomous driving,” *arXiv
    preprint arXiv:1911.12905*, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] B. Osiński, A. Jakubowski, P. Miłoś, P. Zikecina, C. Galias, 和 H. Michalewski,
    “基于仿真的强化学习用于现实世界的自动驾驶，” *arXiv预印本 arXiv:1911.12905*，2019年。'
- en: '[126] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, 和 O. Klimov, “邻近策略优化算法，”
    *arXiv预印本 arXiv:1707.06347*，2017年。'
- en: '[127] W. Huang, F. Braghin, and S. Arrigoni, “Autonomous vehicle driving via
    deep deterministic policy gradient,” in *ASME 2019 International Design Engineering
    Technical Conferences and Computers and Information in Engineering Conference*.   American
    Society of Mechanical Engineers Digital Collection, 2019.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] W. Huang, F. Braghin, 和 S. Arrigoni, “通过深度确定性策略梯度实现自动驾驶，” 见 *ASME 2019国际设计工程技术会议与计算机和信息工程会议*。   美国机械工程师学会数字收藏，2019年。'
- en: '[128] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura, “Navigating
    occluded intersections with autonomous vehicles using deep reinforcement learning,”
    in *2018 IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE,
    2018, pp. 2034–2039.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, 和 K. Fujimura, “使用深度强化学习在遮挡交叉口导航自动驾驶车辆，”
    见 *2018 IEEE国际机器人与自动化会议（ICRA）*。   IEEE，2018年，第2034–2039页。'
- en: '[129] M. M. Minderhoud and P. H. Bovy, “Extended time-to-collision measures
    for road traffic safety assessment,” *Accident Analysis & Prevention*, vol. 33,
    no. 1, pp. 89–97, 2001.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] M. M. Minderhoud 和 P. H. Bovy, “扩展的碰撞时间测量用于道路交通安全评估，” *事故分析与预防*，第33卷，第1期，第89–97页，2001年。'
- en: '[130] A. R. Kreidieh, C. Wu, and A. M. Bayen, “Dissipating stop-and-go waves
    in closed and open networks via deep reinforcement learning,” in *2018 21st International
    Conference on Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 1475–1480.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. R. Kreidieh, C. Wu 和 A. M. Bayen，“通过深度强化学习消除闭环和开放网络中的停车和启动波，” 见 *2018年第21届智能交通系统国际会议（ITSC）*。
    IEEE，2018年，第1475–1480页。'
- en: '[131] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International Conference on Machine Learning*, 2015,
    pp. 1889–1897.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Schulman, S. Levine, P. Abbeel, M. Jordan 和 P. Moritz，“信任区域策略优化，”
    见 *国际机器学习会议*，2015年，第1889–1897页。'
- en: '[132] B. Chalaki, L. Beaver, B. Remer, K. Jang, E. Vinitsky, A. Bayen, and
    A. A. Malikopoulos, “Zero-shot autonomous vehicle policy transfer: From simulation
    to real-world via adversarial learning,” *arXiv preprint arXiv:1903.05252*, 2019.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] B. Chalaki, L. Beaver, B. Remer, K. Jang, E. Vinitsky, A. Bayen 和 A.
    A. Malikopoulos，“零样本自主车辆政策转移：通过对抗学习从模拟到现实世界，” *arXiv 预印本 arXiv:1903.05252*，2019年。'
- en: '[133] K. Jang, E. Vinitsky, B. Chalaki, B. Remer, L. Beaver, A. A. Malikopoulos,
    and A. Bayen, “Simulation to scaled city: zero-shot policy transfer for traffic
    control via autonomous vehicles,” in *Proceedings of the 10th ACM/IEEE International
    Conference on Cyber-Physical Systems*.   ACM, 2019, pp. 291–300.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] K. Jang, E. Vinitsky, B. Chalaki, B. Remer, L. Beaver, A. A. Malikopoulos
    和 A. Bayen，“从模拟到缩放城市：通过自主车辆进行零样本政策转移以控制交通，” 见 *第10届ACM/IEEE国际网络物理系统会议论文集*。 ACM，2019年，第291–300页。'
- en: '[134] F. Belletti, D. Haziza, G. Gomes, and A. M. Bayen, “Expert level control
    of ramp metering based on multi-task deep reinforcement learning,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 19, no. 4, pp. 1198–1207, 2018.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] F. Belletti, D. Haziza, G. Gomes 和 A. M. Bayen，“基于多任务深度强化学习的匝道流量控制专家级控制，”
    *IEEE 智能交通系统汇刊*，第19卷，第4期，第1198–1207页，2018年。'
- en: '[135] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Benchmarking
    deep reinforcement learning for continuous control,” in *International Conference
    on Machine Learning*, 2016, pp. 1329–1338.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Duan, X. Chen, R. Houthooft, J. Schulman 和 P. Abbeel，“连续控制的深度强化学习基准测试，”
    见 *国际机器学习会议*，2016年，第1329–1338页。'
- en: '[136] H. Chaoui, H. Gualous, L. Boulon, and S. Kelouwani, “Deep reinforcement
    learning energy management system for multiple battery based electric vehicles,”
    in *2018 IEEE Vehicle Power and Propulsion Conference (VPPC)*.   IEEE, 2018, pp.
    1–6.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. Chaoui, H. Gualous, L. Boulon 和 S. Kelouwani，“用于多电池电动车的深度强化学习能源管理系统，”
    见 *2018 IEEE 车辆动力和推进会议（VPPC）*。 IEEE，2018年，第1–6页。'
- en: '[137] Y. Wu, H. Tan, J. Peng, H. Zhang, and H. He, “Deep reinforcement learning
    of energy management with continuous control strategy and traffic information
    for a series-parallel plug-in hybrid electric bus,” *Applied Energy*, vol. 247,
    pp. 454–466, 2019.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Y. Wu, H. Tan, J. Peng, H. Zhang 和 H. He，“具有连续控制策略和交通信息的深度强化学习能源管理，用于系列并联插电式混合动力公交车，”
    *应用能源*，第247卷，第454–466页，2019年。'
- en: '[138] Y. Hu, W. Li, K. Xu, T. Zahid, F. Qin, and C. Li, “Energy management
    strategy for a hybrid electric vehicle based on deep reinforcement learning,”
    *Applied Sciences*, vol. 8, no. 2, p. 187, 2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Hu, W. Li, K. Xu, T. Zahid, F. Qin 和 C. Li，“基于深度强化学习的混合动力电动车能量管理策略，”
    *应用科学*，第8卷，第2期，第187页，2018年。'
- en: '[139] T. Markel, A. Brooker, T. Hendricks, V. Johnson, K. Kelly, B. Kramer,
    M. O’Keefe, S. Sprik, and K. Wipke, “Advisor: a systems analysis tool for advanced
    vehicle modeling,” *Journal of power sources*, vol. 110, no. 2, pp. 255–266, 2002.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] T. Markel, A. Brooker, T. Hendricks, V. Johnson, K. Kelly, B. Kramer,
    M. O’Keefe, S. Sprik 和 K. Wipke，“Advisor：一种先进车辆建模的系统分析工具，” *电源期刊*，第110卷，第2期，第255–266页，2002年。'
- en: '[140] Y. Wu, H. Tan, and B. Ran, “Differential variable speed limits control
    for freeway recurrent bottlenecks via deep reinforcement learning,” *arXiv preprint
    arXiv:1810.10952*, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Y. Wu, H. Tan 和 B. Ran，“通过深度强化学习对高速公路重复瓶颈进行差分变量限速控制，” *arXiv 预印本 arXiv:1810.10952*，2018年。'
- en: '[141] Y. Wu, H. Tan, Z. Jiang, and B. Ran, “Es-ctc: A deep neuroevolution model
    for cooperative intelligent freeway traffic control,” *arXiv preprint arXiv:1905.04083*,
    2019.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. Wu, H. Tan, Z. Jiang 和 B. Ran，“Es-ctc：一种用于合作智能高速公路交通控制的深度神经进化模型，”
    *arXiv 预印本 arXiv:1905.04083*，2019年。'
- en: '[142] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, “Evolution strategies
    as a scalable alternative to reinforcement learning,” *arXiv preprint arXiv:1703.03864*,
    2017.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] T. Salimans, J. Ho, X. Chen, S. Sidor 和 I. Sutskever，“演化策略作为一种可扩展的强化学习替代方案，”
    *arXiv 预印本 arXiv:1703.03864*，2017年。'
- en: '[143] V. Pandey and S. D. Boyles, “Multiagent reinforcement learning algorithm
    for distributed dynamic pricing of managed lanes,” in *2018 21st International
    Conference on Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 2346–2351.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] V. Pandey 和 S. D. Boyles, “用于管理车道的分布式动态定价的多智能体强化学习算法，” 见 *2018年第21届国际智能交通系统会议
    (ITSC)*。 IEEE，2018年，页码2346–2351。'
- en: '[144] J. R. Kok and N. Vlassis, “Collaborative multiagent reinforcement learning
    by payoff propagation,” *Journal of Machine Learning Research*, vol. 7, no. Sep,
    pp. 1789–1828, 2006.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. R. Kok 和 N. Vlassis, “通过收益传播的协作多智能体强化学习，” *机器学习研究杂志*，第7卷，9月号，页码1789–1828，2006年。'
- en: '[145] V. Pandey, E. Wang, and S. D. Boyles, “Deep reinforcement learning algorithm
    for dynamic pricing of express lanes with multiple access locations,” *arXiv preprint
    arXiv:1909.04760*, 2019.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] V. Pandey, E. Wang, 和 S. D. Boyles, “用于多个接入点的快速车道动态定价的深度强化学习算法，” *arXiv
    预印本 arXiv:1909.04760*，2019年。'
- en: '[146] U. Gunarathna, H. Xie, E. Tanin, S. Karunasekara, and R. Borovica-Gajic,
    “Dynamic graph configuration with reinforcement learning for connected autonomous
    vehicle trajectories,” *arXiv preprint arXiv:1910.06788*, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] U. Gunarathna, H. Xie, E. Tanin, S. Karunasekara, 和 R. Borovica-Gajic,
    “利用强化学习进行动态图配置以支持连接的自动驾驶车辆轨迹，” *arXiv 预印本 arXiv:1910.06788*，2019年。'
- en: '[147] K. Min, H. Kim, and K. Huh, “Deep distributional reinforcement learning
    based high-level driving policy determination,” *IEEE Transactions on Intelligent
    Vehicles*, vol. 4, no. 3, pp. 416–424, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] K. Min, H. Kim, 和 K. Huh, “基于深度分布式强化学习的高层驾驶策略确定，” *IEEE 智能车辆学报*，第4卷，第3期，页码416–424，2019年。'
- en: '[148] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, and
    D. Lange, “Unity: A general platform for intelligent agents,” *arXiv preprint
    arXiv:1809.02627*, 2018.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, 和 D.
    Lange, “Unity：一个通用的智能体平台，” *arXiv 预印本 arXiv:1809.02627*，2018年。'
- en: '[149] L. Schultz and V. Sokolov, “Deep reinforcement learning for dynamic urban
    transportation problems,” *arXiv preprint arXiv:1806.05310*, 2018.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] L. Schultz 和 V. Sokolov, “用于动态城市交通问题的深度强化学习，” *arXiv 预印本 arXiv:1806.05310*，2018年。'
- en: '[150] G. Bacchiani, D. Molinari, and M. Patander, “Microscopic traffic simulation
    by cooperative multi-agent deep reinforcement learning,” *arXiv preprint arXiv:1903.01365*,
    2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] G. Bacchiani, D. Molinari, 和 M. Patander, “通过合作的多智能体深度强化学习进行微观交通仿真，”
    *arXiv 预印本 arXiv:1903.01365*，2019年。'
- en: '[151] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner,
    “Torcs, the open racing car simulator,” *Software available at http://torcs. sourceforge.
    net*, vol. 4, no. 6, 2000.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, 和 A. Sumner,
    “Torcs，开放赛车模拟器，” *软件可在 http://torcs. sourceforge. net*，第4卷，第6期，2000年。'
- en: '[152] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton *et al.*, “Mastering the game of go without
    human knowledge,” *Nature*, vol. 550, no. 7676, p. 354, 2017.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A.
    Guez, T. Hubert, L. Baker, M. Lai, A. Bolton *等*，“在没有人类知识的情况下掌握围棋游戏，” *自然*，第550卷，第7676期，页码354，2017年。'
- en: '[153] M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in
    empirical observations and microscopic simulations,” *Physical review E*, vol. 62,
    no. 2, p. 1805, 2000.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] M. Treiber, A. Hennecke, 和 D. Helbing, “经验观察和微观仿真中的拥堵交通状态，” *物理评论E*，第62卷，第2期，页码1805，2000年。'
- en: '[154] A. Kesting, M. Treiber, and D. Helbing, “General lane-changing model
    mobil for car-following models,” *Transportation Research Record*, vol. 1999,
    no. 1, pp. 86–94, 2007.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] A. Kesting, M. Treiber, 和 D. Helbing, “用于车-following 模型的一般车道变换模型 mobil，”
    *运输研究记录*，第1999卷，第1期，页码86–94，2007年。'
- en: '[155] S. Fujimoto, H. Van Hoof, and D. Meger, “Addressing function approximation
    error in actor-critic methods,” *arXiv preprint arXiv:1802.09477*, 2018.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] S. Fujimoto, H. Van Hoof, 和 D. Meger, “在演员-评论家方法中解决函数逼近误差，” *arXiv 预印本
    arXiv:1802.09477*，2018年。'
- en: '[156] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla:
    An open urban driving simulator,” *arXiv preprint arXiv:1711.03938*, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, 和 V. Koltun, “Carla：一个开放的城市驾驶模拟器，”
    *arXiv 预印本 arXiv:1711.03938*，2017年。'
- en: '[157] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. E.
    Gonzalez, M. I. Jordan, and I. Stoica, “RLlib: Abstractions for distributed reinforcement
    learning,” in *International Conference on Machine Learning (ICML)*, 2018.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. E.
    Gonzalez, M. I. Jordan, 和 I. Stoica, “RLlib：分布式强化学习的抽象，” 见 *国际机器学习会议 (ICML)*，2018年。'
- en: '[158] C. Wu, A. Kreidieh, K. Parvate, E. Vinitsky, and A. M. Bayen, “Flow:
    A modular learning framework for autonomy in traffic,” *arXiv preprint arXiv:1710.05465*,
    2017.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] C. Wu, A. Kreidieh, K. Parvate, E. Vinitsky, 和 A. M. Bayen, “Flow: A
    modular learning framework for autonomy in traffic,” *arXiv预印本 arXiv:1710.05465*,
    2017.'
- en: '| ![[Uncaptioned image]](img/0415400bbb39f51ae410fd7a45d5508d.png) | Ammar
    Haydari received the B.Sc. degree in Electrical Engineering from Uludag University,
    Istanbul, Turkey, in 2014\. He is currently a M.Sc. student at the Electrical
    Engineering Department at the University of South Florida, Tampa. His research
    interests include intelligent transportation systems and machine learning. |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/0415400bbb39f51ae410fd7a45d5508d.png) | Ammar Haydari于2014年在土耳其伊斯坦布尔的Uludag大学获得电气工程学士学位。目前，他是南佛罗里达大学电气工程系的硕士研究生。他的研究兴趣包括智能交通系统和机器学习。
    |'
- en: '| ![[Uncaptioned image]](img/dbf0d55ac9dcf8b8c7bd168d36fdde7c.png) | Yasin
    Yılmaz (S’11-M’14) received the Ph.D. degree in Electrical Engineering from Columbia
    University, New York, NY, in 2014\. He is currently an Assistant Professor of
    Electrical Engineering at the University of South Florida, Tampa. He received
    the Collaborative Research Award from Columbia University in 2015\. His research
    interests include statistical signal processing, machine learning, and their applications
    to cybersecurity, cyber-physical systems, IoT networks, transportation systems,
    energy systems, and communication systems. |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/dbf0d55ac9dcf8b8c7bd168d36fdde7c.png) | Yasin Yılmaz (S’11-M’14)于2014年在纽约哥伦比亚大学获得电气工程博士学位。目前，他是南佛罗里达大学电气工程系的助理教授。他于2015年获得哥伦比亚大学的协作研究奖。他的研究兴趣包括统计信号处理、机器学习及其在网络安全、网络物理系统、物联网网络、交通系统、能源系统和通信系统中的应用。
    |'
