- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:57:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2012.08044] Deep Bayesian Active Learning, A Brief Survey on Recent Advances'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2012.08044](https://ar5iv.labs.arxiv.org/html/2012.08044)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep Bayesian Active Learning, A Brief Survey on Recent Advances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Salman Mohamadi Computer Science and Electrical Engineering
  prefs: []
  type: TYPE_NORMAL
- en: West Virginia University Morgantown, WV, USA
  prefs: []
  type: TYPE_NORMAL
- en: Sm0224@mix.wvu.edu    Hamidreza Amindavar Electrical Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Amirkabir University of Technology Tehran, Iran
  prefs: []
  type: TYPE_NORMAL
- en: hamidami@aut.ac.ir
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Active learning frameworks offer efficient data annotation without remarkable
    accuracy degradation. In other words, active learning starts training the model
    with a small size of labeled data while exploring the space of unlabeled data
    in order to select most informative samples to be labeled. Generally speaking,
    representing the uncertainty is crucial in any active learning framework, however,
    deep learning methods are not capable of either representing or manipulating model
    uncertainty. On the other hand, from the real world application perspective, uncertainty
    representation is getting more and more attention in the machine learning community.
    Deep Bayesian active learning frameworks and generally any Bayesian active learning
    settings, provide practical consideration in the model which allows training with
    small data while representing the model uncertainty for further efficient training.
    In this paper, we briefly survey recent advances in Bayesian active learning and
    in particular deep Bayesian active learning frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bayesian Active Learning, Deep learning, Posterior estimation, Bayesian inference,
    Semi-supervised learning
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real life application, while data collection may not be as costly and laborious
    as it was a few decades ago, there are still a lot of considerations that make
    the data annotation process costly and inefficient for actual deployment of many
    machine learning algorithms. Therefore, experiment design and particularly, incorporating
    active learning setting into many of machine problem domains are increasingly
    gaining attention. Active learning is a framework in the area of machine learning
    in which the model starts training by small amount of labeled data and then, in
    a sequential process asks for more data samples from a pool of unlabeled data
    to label and incorporate in the training process. In fact, the key idea behind
    this framework is to achieve desired accuracy while lowering the cost of labeling
    by efficiently asking for more labeling most informative data samples. Therefore,
    compared to many other relevant frameworks, active learning tries to incorporate
    the uncertainty representation to achieve the same or higher accuracy by using
    smaller amount of labeled data. Other than the uncertainty coming from noisy data
    samples, there are two major types of uncertainty, the uncertainty associated
    with the best model parameters, and the uncertainty associated with best network
    structure [[1](#bib.bib1), [2](#bib.bib2)]. In fact we might find multiple models
    and/or different structures which provide well representation for the data, however,
    we may face uncertainty on selecting the one with highest performance on further
    predictions or generalization. In contrast, similar frameworks such as semi-supervised
    learning addresses relatively similar problem domains, however, there are differences
    between their learning paradigm. In more detain, semi-supervised learning frameworks
    use the unlabeled data for feature representations in order to better model the
    labeled data [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]. Classical methods
    and tools in signal processing area, with an emphasis on parametric modeling,
    have been used in many areas with different type of data [[7](#bib.bib7), [9](#bib.bib9),
    [8](#bib.bib8)]. However recent advances in machine learning and in particular,
    artificial neural network, have shown that non-parametric models, are capable
    of almost modeling any type of data at the cost of higher complexity. In this
    line, deep learning could be incorporated with classical tools and frameworks
    of machine learning such as active learning in order to address a wider range
    of problems while improving the performance.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, representing the uncertainty of either embedding space or
    output probability space is challenging where we are going to use deep learning
    tools and concepts. It would show up in multiple scenarios in which we need to
    measure the model uncertainty such as problems addressed by classical active learning
    or its various versions with the similar learning paradigm. In fact, exploiting
    model uncertainty is essential for several problem domains concerning with learning
    from small amount of data. For instance, it is necessary to develop a desired
    output probability space in a typical active learning framework, which necessitates
    the model uncertainty measurement and representation[[28](#bib.bib28)]. Bayesian
    methods, here can play an important role to capture the underlying model uncertainty.
    Original idea of incorporating active learning with neural networks has been proposed
    and assessed few decades ago [[25](#bib.bib25)], however in recent years, more
    specific efforts were performed to introduce deep learning tools to active learning
    framework. In this regard, in order to better understand the addressed problem
    domain, it is crucial to ponder upon the associated difficulties. Authors of [[6](#bib.bib6)],
    in their work as a pioneered effort, discussed that bringing deep learning tools
    into active learning setting poses two major problem; uncertainty representation
    and the amount of data possibly needed to train the model. In next two sections,
    in order to have a taste of the basic concepts and some theoretical definitions,
    a brief overview of the learning paradigm of active learning and Bayesian convolutional
    and recurrent neural networks will be presented.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a4deb329459e56202f3bd2b43625929.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Learning paradigm of active learning; as it is shown, at every iteration
    the training starts from scratch on modified training data.'
  prefs: []
  type: TYPE_NORMAL
- en: II Learning Paradigm in Active Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simply put, the goal of active learning is to minimize the cost of data annotation
    or labeling by efficiently selecting the unlabeled data to be labeled. In more
    detail, in every iteration of active learning, a new labeled data sample (or even
    batch of data) will be added to the training data, and the training process starts
    from scratch. This sequential training will continue until either the accuracy
    reaches to the desired level, or the entire labeling budget be used.The overview
    of the learning cycle of active learning is shown in Fig.[1](#S1.F1 "Figure 1
    ‣ I Introduction ‣ Deep Bayesian Active Learning, A Brief Survey on Recent Advances").
  prefs: []
  type: TYPE_NORMAL
- en: In each iteration, all of the unlabeled data samples will be evaluated to select
    the most informative sample. Selection of these samples is performed by a functions
    named acquisition function which deals with classification uncertainty. The uncertainty
    presented in classification could be measured in terms of predictive entropy,
    variation ration and mutual information [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)].
    Generally speaking, we make use of acquisition function in active learning framework
    either for regression or classification purposes. In case of regression, sample
    variance is the criterion for building and acquisition function. However in case
    of classification, acquisition functions could be designed based on, maximum predictive
    entropy-based acquisition,acquisition based on maximum mutual information for
    predictions and model posterior, maximum variation ratios acquisition or simply
    random acquisition as a baseline [[28](#bib.bib28), [29](#bib.bib29)]. Hence,
    we could think of acquisition functions as a function performing uncertainty sampling,
    or diversity sampling or both of them. For the task of image classification, most
    popular functions are presented and approximately formulated in [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: III Projecting Uncertainty in the Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Convolutional Neural Networks with Bayesian Prior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nowadays, deep learning algorithms mostly rely on training convolutional neural
    networks (CNNs). In fact with the recent advancement of CNNs, one of the main
    advantages of CNNs is that they enable capturing spatial information of the image
    data [[10](#bib.bib10)]. However, Bayesian learning concepts initially were introduced
    to simple versions of neural networks. In fact first attempt on developing a special
    type of neural networks (NNs) named Bayesian NNs, dates back to a work presented
    by reference [[30](#bib.bib30)] more than three decades ago. Their goal was to
    put a prior distribution over the weights of NN in order to develop a mapping
    framework between each specific setting of weights and its corresponding sets
    of outputs. This idea gradually evolved into more complex learning frameworks
    such as Bayesian CNNs. In essence, practical implementation of Bayesian CNNs requires
    the measurement of the model predictive posterior on the test data. Theoretically
    speaking, first each kernel of CNN is set to follow a prior distribution and next,
    a Bernoulli variational distribution is applied to the resultant kernel-patch
    pair of the image; in other words the product of Bernoulli random variables and
    weight matrix is applied to each patch of the image individually. Therefore, some
    of the patches of image could be multiplied by kernels set to zero. In terms of
    practical implementation though, it is approximately implemented by performing
    dropout after every convolutional and fully-connected layer [[28](#bib.bib28),
    [11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on above discussion, authors of [[11](#bib.bib11)] proposed a new version
    of CNNs with Bayesian prior on a set of weights,i.e., Gaussian prior ${p(w);w}$
    is ${\{W_{1},W_{2},...W_{N}\}}$. In their work, Bayesian CNNs for classification
    tasks with a softmax layer is formulated with a likelihood model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(y=c&#124;x,w)=softmax(f^{w}(x)).$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'As above discussed briefly, in order to practically implement such models,
    the Gal et.al [[6](#bib.bib6)] suggest approximate inference using stochastic
    regularization techniques such as dropout or multiplicative Guassian noise. They
    performed it by utilizing dropout during the training as well as the test process
    to estimating the posterior over the test process. In more detail, such work is
    feasible by finding a distribution, namely ${q_{\theta}^{*}(w)}$ which given a
    set of training data ${D}$, minimizes the Kullback-Leibler (KL) divergence between
    estimated posterior and exact posterior ${p(w|D)}$. Finally, as it is common,
    using Mont Carlo integration for such variational inference, we will have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(y=c&#124;x,D)=\int p(y=c&#124;x,w)p(w&#124;D)dw$ |  |
    (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\approx\int p(y=c&#124;x,w)q_{\theta}^{*}(w)dw$ |  | (3)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\approx\frac{1}{T}\sum_{t=1}^{T}p(y=c&#124;x,\hat{w}_{t}),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: with ${q_{\theta}(w)}$ as dropout distribution and ${\hat{w}_{t}}$ as estimation
    of ${q_{\theta}^{*}}$ [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B Recurrent Neural Networks with Bayesian Prior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to what is performed on CNN to make it into Bayesian CNN, recurrent
    neural networks (RNNs) could be modified in terms of model uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: IV Review on Recent advances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bayesian inference methods allow the introduction of probabilistic framework
    to machine learning and deep learning. The notion behind the introduction of these
    kind of frameworks to machine learning is that learning from data would be treated
    as inferring optimal or near optimal models for data representation, such as automatic
    model discovery. In this sense, Bayesian methods and here, specifically Bayesian
    active learning methods gain attention due to their ability for uncertainty representation
    and even better generalization on small amount of data [[20](#bib.bib20)]. One
    of the main work on introduction of model uncertainty measurement and manipulation
    to active learning is done by Gal and Ghahramani [[6](#bib.bib6)]. In fact the
    major contribution of this paper is special introduction of Bayesian uncertainty
    estimation to active learning in order to form a deep active learning framework.
    In more detail, deep learning tools are data hungry while active learning tends
    to use small amount of data, moreover, generally speaking deep learning is no
    suitable for uncertainty representation while active learning relies on model
    uncertainty measurement or even manipulation. Understanding these big natural
    differences, authors of this paper found the Bayesian approach to be the solution.
    In fact they refine the active learning general framework, which usually work
    with SVM and small amount of data, to be well scaled to high dimensional data
    such as images in the case of big data. in contrast to small data. It practice,
    the authors put a Bayesian prior on the kernels of a convolutional neural network
    as the training engine of active learning framework. They refer to their previous
    work [[21](#bib.bib21)] suggesting that in order to have a practical Bayesian
    CNN, the Bayesian inference could be done through approximate inference in the
    Bayesian CNN, which makes the solution computationally tractable. The interesting
    point is that they empirically showed that dropout is a Bayesian approximation
    which can be used as a way to introduce uncertainty to deep learning [[22](#bib.bib22)].
    Here the point is that dropout is not only used in the training process, i.e.,
    they do inference applying dropout before every weight layer during training,
    and also during test to sample from the approximate posterior. This framework
    compared to other active learning methods addressing big data for image, such
    as those using RBF, performs better.
  prefs: []
  type: TYPE_NORMAL
- en: In this line, Jedoui et. al. [[23](#bib.bib23)] even go further in the level
    of uncertainty of model by assuming that the output space is no longer mutually
    exclusive, for instance we have more that one output for a single input. They
    empirically show that classical uncertainty sampling does not perform better than
    random sampling at these sort of tasks such as Visual Question Answering, therefore
    they refer to [[6](#bib.bib6), [18](#bib.bib18), [21](#bib.bib21), [19](#bib.bib19)]
    take a similar strategy by using Bayesian uncertainty in a semantically structured
    embedding space rather than modeling uncertainty of the output probability space.
    Referring to Gal and Ghahramani’s works, they mention that dropout can be interpreted
    as a variational Bayesian approximation [[21](#bib.bib21), [22](#bib.bib22)],
    where the approximating distribution is a mixture of two Gaussians with small
    variances which the mean of one of the Gaussians is zero. The prediction uncertainty
    caused by uncertainty in the weights which could be measured by approximate posterior
    using Monte Carlo integration.
  prefs: []
  type: TYPE_NORMAL
- en: Authors of reference [[24](#bib.bib24)] poses another similar problem by introducing
    deep learning with relatively very large amount of data and big network into active
    learning; and suggesting the necessity of systematic request for labeling in the
    form of batch active learning (batch rather than sample in each active learning
    iteration). They offer batch active learning in order to address the problem that
    existing greedy algorithms become computationally costly and sensitive to the
    model slightest changes. The authors propose a model aimed at efficiently scaled
    active learning by well estimating data posterior. They suggest scenarios in which
    more efficiency comes with one batch rather than one data sample at each iteration.
    In this paper, authors take multiple active learning methods, different acquisition
    functions, into account for their objective of efficient batch selection in the
    sense of sparsity, or sparse subset approximation. Moreover, they claim that based
    on their experiments, that reference [[6](#bib.bib6)], as a Bayesian approach,
    outperforms others in many problem setting. More specifically, with the same Bayesian
    active learning framework proposed by [[6](#bib.bib6)] for capturing uncertainty,
    they target the most optimum batch selection by finding data posterior, however
    as active learning setting does not provide access to the labels before querying
    the pool set, they take expectation w.r.t. the current predictive posterior distribution.
    This work represents a closed-form solution consistent with basic theoretical
    setting of reference [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: Gal and Ghahramani [[11](#bib.bib11)] suggest a Bernoulli approximate variational
    inference method, which prevents from CNNs over-fitting, i.e., by considering
    a Bayesian prior on the weights of the network, it will become capable of learning
    from small data, with no over-fitting or higher computational complexity. This
    work can be considered as one of the basics of developing deep Bayesian active
    learning. Authors of [[12](#bib.bib12)] with an emphasis on the fact that the
    nature of active learning does not allows thorough comparison of models and acquisition
    functions, explore more than 4 experiments of different models and acquisition
    functions for multiple tasks of natural language processing, and finally show
    that deep Bayesian active learning consistently provides the best performance.
    Kandasamy et. al[[13](#bib.bib13)] underscore the fact that classical methods
    for posterior estimation are query inefficient in the sense of estimating likelihood.
    They suggest that a query efficient approach would be posterior approximation
    using Bayesian active learning framework. Considering a Gaussian prior, a utility
    function as a measure of divergence between probabilities (here densities)is formed
    and at each time step, the estimated most informative query would be sent to an
    oracle. Then the posterior will be updated. Their experiment confirms the query
    efficiency of the approach. Reference [[14](#bib.bib14)] suggests that since most
    of the methods of distance metric learning are sensitive to the size of the data
    and randomly select the training pairs, they could not return satisfactory results
    in many real world problems. The authors address this problem by firstly introducing
    Bayesian approach to distance metric learning, and then developing this framework
    by uncertainty modeling, which ends up in a Bayesian framework for active distance
    metric learning. This framework enables efficient pair selection for training
    as well as posterior probability approximation. Reference [[15](#bib.bib15)] tries
    to combine the benefits of Bayesian active learning and semi-supervised learning
    by introducing a active expectation maximization framework with pseudo-labels.
    Authors use Mont Carlo dropout introduce in [[6](#bib.bib6)] to compute the probability
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[[32](#bib.bib32), [31](#bib.bib31)] present innovative strategies for addressing
    the issues of sampling and visualizing high-dimensional unbalanced datasets to
    reduce computing complexity and memory utilization while preserving accuracy.
    This could be used as an auxiliary technique for deep active learning frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: Zeng et. al [[16](#bib.bib16)] address the question that is it possible to measure
    the model uncertainty without fully Bayesian CNNs. Their results on several Bayesian
    CNNs confirm that in order to represent the model uncertainty, one needs to apply
    the Bayesian prior on only a few last layer before the output. With this setting,
    the model would enjoy the benefits of both deterministic and Bayesian CNNs. Lewenberg
    et. al [[17](#bib.bib17)] address the problem of active surveying using a Bayesian
    active learner. In fact they use dimensionality reduction in an active learning
    framework by applying Bayesian prior in order to design a system to predict the
    answers to unasked question using a limited sequential active question asking.
    Their framework outperform several state of-the-art frameworks based on enhanced
    linear regression in terms of prediction accuracy and response to missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Houlsby et. al.[[18](#bib.bib18)] propose a measurement of predictive entropy
    which later is used in a classification framework based on Gaussian Process. Their
    method performs well compared to several similar classification frameworks while
    the computational complexity is not greater than other methods. Finally they develop
    their framework to a Gaussian process preference learning by extension of binary
    preference learning to classification setting. One of the main advantages of this
    method is that it provides desired accuracy at a relatively low computational
    complexity cost.
  prefs: []
  type: TYPE_NORMAL
- en: V Future trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VI Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we surveyed recent advances in Bayesian active learning, with
    an emphasis on deep Bayesian active learning. Our main focus in on the works contributing
    to the theory of this problem domain, however some interesting works on the application
    of Bayesian active learning are surveyed. Bayesian inference approaches hold very
    important place in machine learning and recently, many attentions have shifted
    toward data, model and even network structure uncertainty representation using
    these approaches. Bayesian active learning and its intersection with deep learning
    concepts provide very interesting frameworks in terms of theroy and application.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Settles, B. (2009). Active learning literature survey. University of Wisconsin-Madison
    Department of Computer Sciences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Sener, O., & Savarese, S. (2017). Active learning for convolutional neural
    networks: A core-set approach. arXiv preprint arXiv:1708.00489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Taherkhani, Fariborz, Nasser M. Nasrabadi, and Jeremy Dawson. ”A deep face
    identification network enhanced by facial attributes prediction.” In Proceedings
    of the IEEE conference on computer vision and pattern recognition workshops, pp.
    553-560\. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Taherkhani, Fariborz, Hadi Kazemi, and Nasser M. Nasrabadi. ”Matrix completion
    for graph-based deep semi-supervised learning.” In Proceedings of the AAAI Conference
    on Artificial Intelligence, vol. 33, pp. 5058-5065\. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Taherkhani, Fariborz, Hadi Kazemi, Ali Dabouei, Jeremy Dawson, and Nasser
    M. Nasrabadi. ”A weakly supervised fine label classifier enhanced by coarse supervision.”
    In Proceedings of the IEEE International Conference on Computer Vision, pp. 6459-6468\.
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Gal, Yarin, Riashat Islam, and Zoubin Ghahramani. ”Deep bayesian active
    learning with image data.” arXiv preprint arXiv:1703.02910 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Mohamadi, Salman, Hamidreza Amindavar, and SM Ali Tayaranian Hosseini.
    ”Arima-garch modeling for epileptic seizure prediction.” In 2017 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 994-998\.
    IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Mohamadi, Salman, Farhang Yeganegi, and Nasser M. Nasrabadi. ”Detection
    and Statistical Modeling of Birth-Death Anomaly.” arXiv preprint arXiv:1906.11788
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Mohamadi, Salman, Farhang Yeganegi, and Hamidreza Amindavar. ”A New Framework
    For Spatial Modeling And Synthesis of Genome Sequence.” arXiv preprint arXiv:1908.03342
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ”Imagenet classification
    with deep convolutional neural networks.” Communications of the ACM 60, no. 6
    (2017): 84-90.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Gal, Yarin, and Zoubin Ghahramani. ”Bayesian convolutional neural networks
    with Bernoulli approximate variational inference.” arXiv preprint arXiv:1506.02158
    (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Siddhant, Aditya, and Zachary C. Lipton. ”Deep bayesian active learning
    for natural language processing: Results of a large-scale empirical study.” arXiv
    preprint arXiv:1808.05697 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Kandasamy, Kirthevasan, Jeff Schneider, and Barnabás Póczos. ”Bayesian
    active learning for posterior estimation.” In Proceedings of the 24th International
    Conference on Artificial Intelligence, pp. 3605-3611\. 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Yang, Liu, Rong Jin, and Rahul Sukthankar. ”Bayesian active distance metric
    learning.” arXiv preprint arXiv:1206.5283 (2012).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Matthias, Rottmann, Kahl Karsten, and Gottschalk Hanno. ”Deep bayesian
    active semi-supervised learning.” In 2018 17th IEEE International Conference on
    Machine Learning and Applications (ICMLA), pp. 158-164\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Zeng, Jiaming, Adam Lesnikowski, and Jose M. Alvarez. ”The relevance of
    Bayesian layer positioning to model uncertainty in deep Bayesian active learning.”
    arXiv preprint arXiv:1811.12535 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Lewenberg, Yoad, Yoram Bachrach, Ulrich Paquet, and Jeffrey S. Rosenschein.
    ”Knowing What to Ask: A Bayesian Active Learning Approach to the Surveying Problem.”
    In AAAI, pp. 1396-1402\. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Houlsby, Neil, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. ”Bayesian
    active learning for classification and preference learning.” arXiv preprint arXiv:1112.5745
    (2011).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Karpathy, Andrej, Armand Joulin, and Li F. Fei-Fei. ”Deep fragment embeddings
    for bidirectional image sentence mapping.” In Advances in neural information processing
    systems, pp. 1889-1897\. 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Ghahramani, Zoubin. ”Probabilistic machine learning and artificial intelligence.”
    Nature 521, no. 7553 (2015): 452-459.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Gal, Yarin, and Zoubin Ghahramani. ”Dropout as a bayesian approximation:
    Insights and applications.” In Deep Learning Workshop, ICML, vol. 1, p. 2\. 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Gal, Yarin, and Zoubin Ghahramani. ”Dropout as a bayesian approximation:
    Representing model uncertainty in deep learning.” In international conference
    on machine learning, pp. 1050-1059\. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Jedoui, Khaled, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. ”Deep
    Bayesian Active Learning for Multiple Correct Outputs.” arXiv preprint arXiv:1912.01119
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Pinsler, Robert, Jonathan Gordon, Eric Nalisnick, and José Miguel Hernández-Lobato.
    ”Bayesian batch active learning as sparse subset approximation.” In Advances in
    Neural Information Processing Systems, pp. 6359-6370\. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Cohn, David A., Zoubin Ghahramani, and Michael I. Jordan. Active Learning
    with Statistical Models. MASSACHUSETTS INST OF TECH CAMBRIDGE ARTIFICIAL INTELLIGENCE
    LAB, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Freeman, Linton C., and Linton C. Freeman. Elementary applied statistics:
    for students in behavioral science. New York: Wiley, 1965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Shannon, Claude E. ”A mathematical theory of communication.” The Bell
    system technical journal 27, no. 3 (1948): 379-423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Gal, Yarin. ”Uncertainty in deep learning.” University of Cambridge 1,
    no. 3 (2016): 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Houlsby, Neil, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. ”Bayesian
    active learning for classification and preference learning.” arXiv preprint arXiv:1112.5745
    (2011).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Denker, John, Daniel Schwartz, Ben Wittner, Sara Solla, Richard Howard,
    Lawrence Jackel, and John Hopfield. ”Large automatic learning, rule extraction,
    and generalization.” Complex systems 1, no. 5 (1987): 877-922.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Hajibabaee, Parisa, Farhad Pourkamali-Anaraki, and Mohammad Amin Hariri-Ardebili.
    ”Kernel matrix approximation on class-imbalanced data with an application to scientific
    simulation.” IEEE Access 9 (2021): 83579-83591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Hajibabaee, Parisa, Farhad Pourkamali-Anaraki, and Mohammad Amin Hariri-Ardebili.
    ”An empirical evaluation of the t-sne algorithm for data visualization in structural
    engineering.” In 2021 20th IEEE International Conference on Machine Learning and
    Applications (ICMLA), pp. 1674-1680\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
