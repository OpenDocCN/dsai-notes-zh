- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:35:24'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2401.00288] Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.00288](https://ar5iv.labs.arxiv.org/html/2401.00288)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \newcites
  prefs: []
  type: TYPE_NORMAL
- en: secondaryAPPENDIX REFERENCES
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yao Wan [wanyao@hust.edu.cn](mailto:wanyao@hust.edu.cn) National Engineering
    Research Center for Big Data Technology and System, Services Computing Technology
    and System Lab, Cluster and Grid Computing Lab, School of Computer Science and
    TechnologyHuazhong University of Science and TechnologyWuhanChina ,  Yang He Simon
    Fraser UniversityVancouverCanada [yha244@sfu.ca](mailto:yha244@sfu.ca) ,  Zhangqian
    Bi School of Computer Science and TechnologyHuazhong University of Science and
    TechnologyWuhanChina [zqbi@hust.edu.cn](mailto:zqbi@hust.edu.cn) ,  Jianguo Zhang
    Salesforce ResearchUSA [jianguozhang@salesforce.com](mailto:jianguozhang@salesforce.com)
    ,  Hongyu Zhang Chongqing UniversityChina [hyzhang@cqu.edu.cn](mailto:hyzhang@cqu.edu.cn)
    ,  Yulei Sui University of New South WalesAustralia [y.sui@unsw.edu.au](mailto:y.sui@unsw.edu.au)
    ,  Guandong Xu University of Technology SydneyAustralia [guandong.xu@uts.edu.au](mailto:guandong.xu@uts.edu.au)
    ,  Hai Jin [hjin@hust.edu.cn](mailto:hjin@hust.edu.cn) National Engineering Research
    Center for Big Data Technology and System, Services Computing Technology and System
    Lab, Cluster and Grid Computing Lab, School of Computer Science and TechnologyHuazhong
    University of Science and TechnologyWuhanChina  and  Philip S. Yu University of
    Illinois at ChicagoChicagoUSA [psyu@uic.edu](mailto:psyu@uic.edu)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Code intelligence leverages machine learning techniques to extract knowledge
    from extensive code corpora, with the aim of developing intelligent tools to improve
    the quality and productivity of computer programming. Currently, there is already
    a thriving research community focusing on code intelligence, with efforts ranging
    from software engineering, machine learning, data mining, natural language processing,
    and programming languages. In this paper, we conduct a comprehensive literature
    review on deep learning for code intelligence, from the aspects of code representation
    learning, deep learning techniques, and application tasks. We also benchmark several
    state-of-the-art neural models for code intelligence, and provide an open-source
    toolkit tailored for the rapid prototyping of deep-learning-based code intelligence
    models. In particular, we inspect the existing code intelligence models under
    the basis of code representation learning, and provide a comprehensive overview
    to enhance comprehension of the present state of code intelligence. Furthermore,
    we publicly release the source code and data resources to provide the community
    with a ready-to-use benchmark, which can facilitate the evaluation and comparison
    of existing and future code intelligence models ([https://xcodemind.github.io](https://xcodemind.github.io)).
    At last, we also point out several challenging and promising directions for future
    research.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Software is eating the world (Andreessen, [2011](#bib.bib15)). With the advancement
    of Artificial Intelligence (AI), it is time to expand that maxim: software ate
    the world, and AI is eating the software. As the software is primarily composed
    of code, we define the emerging concept of code intelligence as the application
    of machine learning techniques to extract knowledge from large-scale code repositories,
    with the aim of developing intelligent tools to improve the quality and productivity
    of computer programming (Lu et al., [2021](#bib.bib160)). This concept is fueled
    by the ever-expanding reservoir of source code, often referred to as “Big Code” (Allamanis
    et al., [2018a](#bib.bib7)), which is harvested from platforms such as GitHub (git,
    [2019](#bib.bib2)) and StackOverflow (sta, [2019](#bib.bib3)). In this paper,
    our research scope is confined to code intelligence, with a particular focus on
    the application of deep learning techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Achieving code intelligence necessitates a collaborative synergy in research
    across the domains of software engineering, machine learning, Natural Language
    Processing (NLP), programming language, and security. From our investigation,
    precise and reliable code representation learning or code embedding, which aims
    to efficiently and effectively encode the semantics of source code into distributed
    vector representations, is the foundation for code intelligence. Such embedding
    vectors are then used in many downstream tasks, such as code completion (Raychev
    et al., [2014](#bib.bib198); Svyatkovskiy et al., [2019](#bib.bib223); Kim et al.,
    [2021](#bib.bib118); Liu et al., [2020b](#bib.bib150)), code search (Gu et al.,
    [2018](#bib.bib78); Wan et al., [2019](#bib.bib235); Husain et al., [2019](#bib.bib107)),
    code summarization (Allamanis et al., [2016](#bib.bib11); Iyer et al., [2016](#bib.bib108);
    Wan et al., [2018](#bib.bib238); Hu et al., [2018b](#bib.bib104); Zhang et al.,
    [2020c](#bib.bib285)), type inference (Hellendoorn et al., [2018](#bib.bib99);
    Wei et al., [2020a](#bib.bib254); Pradel et al., [2020](#bib.bib189); Allamanis
    et al., [2020](#bib.bib8)), etc. In terms of code embedding, notable advancements
    have been achieved by employing deep learning and NLP techniques to encode source
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Analogous to word2vec (Mikolov et al., [2013](#bib.bib167)) in NLP, Alon et al.
    ([2019](#bib.bib14)) proposed code2vec, a distributed representation of code,
    based on a collection of paths extracted from the Abstract Syntax Tree (AST) of
    code. In recent years, a multitude of neural networks tailored for specific tasks
    have been proposed and trained using supervised methods. As pre-trained language
    models (e.g., BERT (Devlin et al., [2019](#bib.bib61)) and GPT-3 (Brown et al.,
    [2020](#bib.bib27))) have been widely applied to NLP, many pre-trained language
    models for code have been proposed (Kanade et al., [2020](#bib.bib116); Feng et al.,
    [2020](#bib.bib67); Guo et al., [2021](#bib.bib84)) to better represent the semantics
    of code. More recently, the emergence of Large Language Models (LLMs), exemplified
    by ChatGPT, has illuminated the pathway for further advancement of pre-trained
    language models, with a notable trend of increasing model sizes. This trend has
    extended to the domain of code intelligence, resulting in the development of various
    LLMs tailored for code, including but not limited to CodeT5 (Wang et al., [2023](#bib.bib247)),
    StarCoder (Li et al., [2023a](#bib.bib131)), and Code Llama (Roziere et al., [2023](#bib.bib199)).
    In this paper, we examine code intelligence through the lenses of code representation
    learning, deep learning methods, and their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Comparison of the current work with previous survey efforts.
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Artifact | Technique | Survey | Benchmark | Toolkit |'
  prefs: []
  type: TYPE_TB
- en: '| Allamanis et al. ([2018a](#bib.bib7)) | Software | Machine Learning | ✓ |
    $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Watson et al. ([2020](#bib.bib250)) | Software | Deep Learning | ✓ | $\times$
    | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2020a](#bib.bib242)) |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([2022c](#bib.bib270)) |'
  prefs: []
  type: TYPE_TB
- en: '| Devanbu et al. ([2020](#bib.bib60)) |'
  prefs: []
  type: TYPE_TB
- en: '| Lu et al. ([2021](#bib.bib160)) | Software | Deep Learning | $\times$ | ✓
    | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | Code | Deep Learning | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: Related Surveys and Differences. Within our literature review, we identified
    several surveys related to ours. Notably, Allamanis et al. ([2018a](#bib.bib7))
    conducted an exhaustive examination of machine learning approaches for modeling
    the naturalness of programming language. They primarily emphasize machine learning
    algorithms, with a specific focus on probabilistic models, as opposed to those
    based on deep learning. Recently, Watson et al. ([2020](#bib.bib250)), Wang et al.
    ([2020a](#bib.bib242)) and Yang et al. ([2022c](#bib.bib270)) conducted a thorough
    review of the literature on applications of deep learning in software engineering
    research. They investigated mostly software engineering and artificial intelligence
    conferences and journals, focusing on various software engineering tasks (not
    limited to the source code) that are based on deep learning. (Devanbu et al.,
    [2020](#bib.bib60)) is a report that summarizes the current status of research
    on the subject of the intersection between deep learning and software engineering,
    as well as suggests several future directions. In (Lu et al., [2021](#bib.bib160)),
    the authors established a benchmark dataset called CodeXGLUE for code representation
    and generation. In addition, several benchmark results especially based on pre-trained
    language models (i.e., CodeBERT) are presented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") summarizes the differences between our paper when
    compared with several related surveys in code intelligence. In contrast to (Allamanis
    et al., [2018a](#bib.bib7)) that focuses on traditional machine learning approaches,
    this paper places greater emphasis on leveraging deep learning techniques for
    code intelligence. In contrast to (Watson et al., [2020](#bib.bib250)), (Wang
    et al., [2020a](#bib.bib242)), (Yang et al., [2022c](#bib.bib270)), and (Devanbu
    et al., [2020](#bib.bib60)) that cover various tasks in broad software engineering,
    our study narrows its focus to tasks associated with source code, examining them
    specifically from the perspective of deep learning. In addition, we survey papers
    from various fields including software engineering, programming languages, machine
    learning, NLP, and security. Note that, as code intelligence based on deep learning
    is an emerging and active research topic, we also include several high-quality
    unpublished papers that have been made available on arXiv. This is because these
    unpublished works in arXiv can be seen as an indicator of future research. Furthermore,
    existing surveys do not provide comprehensive benchmark evaluation results, nor
    do they develop an open-source toolkit to facilitate further research. This paper
    addresses this gap by presenting an open-source toolkit, referred to as NaturalCC
    (standards for Natural Code Comprehension) (Wan et al., [2022a](#bib.bib234)).
    The toolkit is designed to streamline the prototyping of code intelligence models
    and to serve as a benchmarking platform for evaluating various state-of-the-art
    models. In complement to CodeXGLUE (Lu et al., [2021](#bib.bib160)), which aims
    to establish a benchmark dataset for code understanding and generation, particularly
    leveraging pre-trained code models, our focus lies in the construction of infrastructures
    that support diverse model implementations and provide users with the ability
    to conduct rapid prototyping. Compared to CodeXGLUE, our toolkit contains a more
    extensive array of tools designed for the entire pipeline involved in constructing
    code intelligence models, offering heightened flexibility.'
  prefs: []
  type: TYPE_NORMAL
- en: Our Contributions. This paper is targeted at researchers and practitioners intrigued
    by the convergence of code intelligence and deep learning, with a specific emphasis
    on intelligent software engineering, NLP, and programming languages. In this paper,
    we begin by providing a thorough review of existing research on deep learning
    for code intelligence. Subsequently, we advance our contribution by developing
    an open-source toolkit, referred to as NaturalCC, that incorporates state-of-the-art
    models across various downstream tasks. Employing NaturalCC, we conduct a comprehensive
    performance benchmark of each model across 4 downstream tasks, including code
    summarization, code search, code completion, and type inference. The major contributions
    of this paper are summarized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct a comprehensive review on deep learning for code intelligence. Specifically,
    we have collected 269 papers from various top-tier venues and arXiv, covering
    multiple domains including software engineering, artificial intelligence, NLP,
    programming languages, and security.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We benchmark the performance of 13 leading models across four different tasks
    (i.e., code summarization, code search, code completion, and type inference).
    All the resources, datasets and source code are publicly available.¹¹1[http://xcodemind.github.io](http://xcodemind.github.io)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce NaturalCC, an open-source toolkit that has integrated many state-of-the-art
    baselines on different tasks, in order to facilitate research on code intelligence.
    Researchers in the fields of software engineering, NLP, and other fields can benefit
    from the toolkit for quick prototyping and replication.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Survey Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. A Unified View from Code Representation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81860a9045e46ac2b6dc8c5dd6881057.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. Code intelligence tasks based on code representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We propose to summarize existing deep-learning-based approaches to code intelligence
    from the lens of code representation learning in this paper. As shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2.1\. A Unified View from Code Representation Learning ‣ 2\. Survey
    Methodology ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit"),
    for code representation learning, researchers first extract features that potentially
    describe the semantics of code, and then design various neural networks to encode
    them into distributed vectors. Code representation learning can be viewed as the
    foundation for different downstream applications. Based on the characteristics
    of each application, the downstream applications can be divided into three groups:
    (1) Classification-based. In these tasks (e.g., code classification, vulnerability
    detection, and type inference), a classifier layer (e.g., softmax) is used to
    map the code embeddings to labels/classes. (2) Similarity-based. In these tasks
    (e.g., code search and code clone detection), Siamese neural network structure (Chicco,
    [2021](#bib.bib48)) is often adopted, where dual encoders are used to encode the
    source code and natural-language query into embedding vectors. Based on the two
    embeddings of code and query, a constraint (such as a triplet loss function) is
    always used to regularize the similarity between them. Note that, in several approaches
    to code search and code clone detection, the two embeddings of code and query
    are also concatenated, and the task is reformulated as a classification task to
    determine whether the code and query are related (Feng et al., [2020](#bib.bib67)).
    (3) Generation-based. In these tasks (e.g., code completion, code summarization,
    program translation, program synthesis, and program repair), the objective is
    to generate source code, natural language descriptions, or programs in another
    programming language from a given code snippet. These tasks usually follow the
    encoder-decoder paradigm, where an encoder network is used to represent the semantics
    of code, and a decoder network (e.g., RNN) is designed to generate sequences,
    e.g., natural-language descriptions or source code. Additionally, we categorize
    the learning paradigms into four groups: supervised learning, unsupervised learning,
    self-supervised learning, and reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Paper Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning for code intelligence has been studied in many related research
    communities. In this paper, we review high-quality papers selected from top-tier
    conferences and journals, ranging from software engineering, programming languages,
    NLP, and artificial intelligence, to security. Overall, we have identified 32
    publication venues, as shown in the Supplementary Materials. We first manually
    check the publication list of the venues and obtain an initial collection of papers.
    Particularly, we systematically query the aforementioned venue names within the
    DBLP database²²2[https://dblp.uni-trier.de](https://dblp.uni-trier.de) and examine
    the associated proceedings. Subsequently, two authors, both possessing over five
    years of expertise in deep learning for code intelligence, collaboratively undertake
    the task of manually refining the results. This involves meticulous scrutiny of
    titles and a brief review of abstracts to identify and filter out papers that
    are potentially relevant to code intelligence. For those large conferences (e.g.,
    AAAI and IJCAI) that accept thousands of papers per year, we first filter out
    those papers whose titles contain the keywords of “code” or “program”, and then
    manually check them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this initial collection of papers, we start to augment it through
    keyword searching. We systematically search DBLP and Google Scholar using the
    following keywords: “code representation”, “program comprehension”, “code embedding”,
    “code classification”, “vulnerability detection”, “bug finding”, “code completion”,
    “type inference”, “code search/retrieval”, “code clone detection”, “code summarization”,
    “program translation”, “program synthesis”, and “program repair”, with a combination
    of “deep”, “learning”, “neural”, and “network”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that, in addition to accepted papers from the aforementioned
    venues, we also consider some recent publications from the pre-print archive,
    as they reflect the most current research outputs. We choose publications from
    arXiv based on two criteria: paper quality, author reputation. The quality of
    a pre-printed paper can be assessed based on the number of citations it has garnered
    in recent months. The reputations of authors can be indicated by their Google
    Scholar citations. If a paper satisfies either of these selection criteria, we
    include it for consideration. Having obtained this collection of papers, we then
    filter out the irrelevant papers by manual checking. Finally, we obtained a collection
    of 269 papers. To ensure transparency and accessibility, a comprehensive table
    of the surveyed papers and the source of papers is maintained online.³³3[https://github.com/CGCL-codes/awesome-code-intelligence](https://github.com/CGCL-codes/awesome-code-intelligence)'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Publication Trends of Code Intelligence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.3\. Publication Trends of Code Intelligence
    ‣ 2\. Survey Methodology ‣ Deep Learning for Code Intelligence: Survey, Benchmark
    and Toolkit") provides statistics of the surveyed papers to reveal the publication
    trend and research topic trend. Figure [2(a)](#S2.F2.sf1 "In Figure 2 ‣ 2.3\.
    Publication Trends of Code Intelligence ‣ 2\. Survey Methodology ‣ Deep Learning
    for Code Intelligence: Survey, Benchmark and Toolkit") shows the collected papers
    on deep learning for code intelligence, from January 2014 to December 2022. It
    is noteworthy that, 11 papers about LLMs for code intelligence have been included,
    all published in 2023. Each year, we analyze the publication trends across various
    communities and venues, including NLP, artificial intelligence, software engineering,
    programming language, security, preprint, and others. The surveyed venues for
    each community are detailed in Table 1 of the Supplementary Materials. According
    to our statistical findings, the top five most popular publication venues are
    ICSE, ASE, ACL, ICLR, and FSE, from the software engineering, NLP, and AI communities.
    Notably, we can also observe that code intelligence has garnered increasing attention
    from the AI and NLP communities since 2018. Although deep learning was first proposed
    in 2006 (Hinton et al., [2006](#bib.bib101)), it is initially used for source
    code modeling in 2014. From Figure [2(a)](#S2.F2.sf1 "In Figure 2 ‣ 2.3\. Publication
    Trends of Code Intelligence ‣ 2\. Survey Methodology ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit"), we can also see that the number
    of relevant papers for code intelligence has increased significantly since 2018,
    indicating that deep learning has significantly advanced code intelligence research
    since then. This development can be attributed to the widespread use of deep learning
    in NLP since 2018, which has sparked a lot of studies on using NLP methods for
    tasks involving source code.'
  prefs: []
  type: TYPE_NORMAL
- en: '][c].42 ![Refer to caption](img/3ca3c2bddfab1355c6772b95a32aa3b6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Number of publications in different years.
  prefs: []
  type: TYPE_NORMAL
- en: '][c].56 ![Refer to caption](img/e004f71258840ebe566afbf156c50cb8.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (b) Publication in each application
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2. Statistics of the surveyed papers to reveal the publication trend
    and research topic trend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2.3\. Publication Trends of Code Intelligence
    ‣ 2\. Survey Methodology ‣ Deep Learning for Code Intelligence: Survey, Benchmark
    and Toolkit") shows the distribution of papers across applications, including
    code classification, vulnerability detection, type inference, code search, code
    clone detection, code completion, code summarization, program translation, program
    synthesis, and program repair. This figure shows a burgeoning interest in recent
    years surrounding topics of code summarization, program synthesis, program repair,
    vulnerability detection, and code search.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Literature Review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. Taxonomy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <svg   height="973.33" overflow="visible"
    version="1.1" width="756.98"><g transform="translate(0,973.33) matrix(1 0 0 -1
    0 0) translate(46.4,0) translate(0,541.42)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -41.51 1.3)" fill="#000000" stroke="#000000"><foreignobject
    width="83.02" height="21.97" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Code
    Intelligence <g fill="#FFFFE6"><path d="M 165.12 13.84 L 83.95 13.84 C 80.89 13.84
    78.41 11.36 78.41 8.3 L 78.41 -8.3 C 78.41 -11.36 80.89 -13.84 83.95 -13.84 L
    165.12 -13.84 C 168.18 -13.84 170.66 -11.36 170.66 -8.3 L 170.66 8.3 C 170.66
    11.36 168.18 13.84 165.12 13.84 Z M 78.41 -13.84"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 83.02 -4.84)" fill="#000000" stroke="#000000"><foreignobject width="83.02"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Code Features</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 165.12 282.28 L 83.95 282.28 C 80.89 282.28 78.41
    279.8 78.41 276.74 L 78.41 260.14 C 78.41 257.08 80.89 254.6 83.95 254.6 L 165.12
    254.6 C 168.18 254.6 170.66 257.08 170.66 260.14 L 170.66 276.74 C 170.66 279.8
    168.18 282.28 165.12 282.28 Z M 78.41 254.6"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 83.02 264.22)" fill="#000000" stroke="#000000"><foreignobject width="83.02"
    height="10.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Deep Learning</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 165.12 -260.14 L 83.95 -260.14 C 80.89 -260.14 78.41
    -262.61 78.41 -265.67 L 78.41 -282.28 C 78.41 -285.33 80.89 -287.81 83.95 -287.81
    L 165.12 -287.81 C 168.18 -287.81 170.66 -285.33 170.66 -282.28 L 170.66 -265.67
    C 170.66 -262.61 168.18 -260.14 165.12 -260.14 Z M 78.41 -287.81"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 83.02 -278.12)" fill="#000000" stroke="#000000"><foreignobject
    width="83.02" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Application</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 209.49 361.58 L 142.15 361.58 C 139.1 361.58 136.62
    359.11 136.62 356.05 L 136.62 332.64 C 136.62 329.59 139.1 327.11 142.15 327.11
    L 209.49 327.11 C 212.55 327.11 215.03 329.59 215.03 332.64 L 215.03 356.05 C
    215.03 359.11 212.55 361.58 209.49 361.58 Z M 136.62 327.11"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 141.23 347.29)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="25.25" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Neural Networks</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 209.49 226.37 L 142.15 226.37 C 139.1 226.37 136.62
    223.9 136.62 220.84 L 136.62 195.01 C 136.62 191.95 139.1 189.48 142.15 189.48
    L 209.49 189.48 C 212.55 189.48 215.03 191.95 215.03 195.01 L 215.03 220.84 C
    215.03 223.9 212.55 226.37 209.49 226.37 Z M 136.62 189.48"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 141.23 212.08)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="27.67" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Learning
    Paradigms</foreignobject></g> <g fill="#D9FFFF"><path d="M 371.48 358.18 L 248.79
    358.18 C 245.73 358.18 243.25 355.7 243.25 352.65 L 243.25 336.04 C 243.25 332.99
    245.73 330.51 248.79 330.51 L 371.48 330.51 C 374.53 330.51 377.01 332.99 377.01
    336.04 L 377.01 352.65 C 377.01 355.7 374.53 358.18 371.48 358.18 Z M 243.25 330.51"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 247.87 339.5)" fill="#000000" stroke="#000000"><foreignobject
    width="124.53" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GNN</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 371.48 327.74 L 248.79 327.74 C 245.73 327.74 243.25
    325.26 243.25 322.21 L 243.25 305.6 C 243.25 302.54 245.73 300.07 248.79 300.07
    L 371.48 300.07 C 374.53 300.07 377.01 302.54 377.01 305.6 L 377.01 322.21 C 377.01
    325.26 374.53 327.74 371.48 327.74 Z M 243.25 300.07"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 247.87 309.06)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transformer</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 371.48 388.62 L 248.79 388.62 C 245.73 388.62 243.25
    386.15 243.25 383.09 L 243.25 366.48 C 243.25 363.43 245.73 360.95 248.79 360.95
    L 371.48 360.95 C 374.53 360.95 377.01 363.43 377.01 366.48 L 377.01 383.09 C
    377.01 386.15 374.53 388.62 371.48 388.62 Z M 243.25 360.95"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 247.87 369.94)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CNN</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 371.48 419.07 L 248.79 419.07 C 245.73 419.07 243.25
    416.59 243.25 413.53 L 243.25 396.93 C 243.25 393.87 245.73 391.39 248.79 391.39
    L 371.48 391.39 C 374.53 391.39 377.01 393.87 377.01 396.93 L 377.01 413.53 C
    377.01 416.59 374.53 419.07 371.48 419.07 Z M 243.25 391.39"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 247.87 400.39)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RNN</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 371.48 221.76 L 248.79 221.76 C 245.73 221.76 243.25
    219.28 243.25 216.23 L 243.25 199.62 C 243.25 196.57 245.73 194.09 248.79 194.09
    L 371.48 194.09 C 374.53 194.09 377.01 196.57 377.01 199.62 L 377.01 216.23 C
    377.01 219.28 374.53 221.76 371.48 221.76 Z M 243.25 194.09"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 247.87 203.77)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Unsupervised
    Learning</foreignobject></g> <g fill="#D9FFFF"><path d="M 371.48 272.96 L 248.79
    272.96 C 245.73 272.96 243.25 270.48 243.25 267.42 L 243.25 250.82 C 243.25 247.76
    245.73 245.28 248.79 245.28 L 371.48 245.28 C 374.53 245.28 377.01 247.76 377.01
    250.82 L 377.01 267.42 C 377.01 270.48 374.53 272.96 371.48 272.96 Z M 243.25
    245.28"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 247.87 254.97)" fill="#000000"
    stroke="#000000"><foreignobject width="124.53" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Supervised Learning</foreignobject></g> <g
    fill="#D9FFFF"><path d="M 371.48 183.22 L 248.79 183.22 C 245.73 183.22 243.25
    180.74 243.25 177.68 L 243.25 155.14 C 243.25 152.09 245.73 149.61 248.79 149.61
    L 371.48 149.61 C 374.53 149.61 377.01 152.09 377.01 155.14 L 377.01 177.68 C
    377.01 180.74 374.53 183.22 371.48 183.22 Z M 243.25 149.61"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 247.87 168.92)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="24.39" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Self-Supervised
    Learning</foreignobject></g> <g fill="#D9FFFF"><path d="M 371.48 138.74 L 248.79
    138.74 C 245.73 138.74 243.25 136.26 243.25 133.2 L 243.25 116.6 C 243.25 113.54
    245.73 111.07 248.79 111.07 L 371.48 111.07 C 374.53 111.07 377.01 113.54 377.01
    116.6 L 377.01 133.2 C 377.01 136.26 374.53 138.74 371.48 138.74 Z M 243.25 111.07"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 247.87 120.75)" fill="#000000" stroke="#000000"><foreignobject
    width="124.53" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Reinforcement
    Learning</foreignobject></g> <g fill="#FFFFE6"><path d="M 372.68 13.84 L 249.99
    13.84 C 246.93 13.84 244.45 11.36 244.45 8.3 L 244.45 -8.3 C 244.45 -11.36 246.93
    -13.84 249.99 -13.84 L 372.68 -13.84 C 375.73 -13.84 378.21 -11.36 378.21 -8.3
    L 378.21 8.3 C 378.21 11.36 375.73 13.84 372.68 13.84 Z M 244.45 -13.84"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.07 -4.84)" fill="#000000" stroke="#000000"><foreignobject
    width="124.53" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">IR</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 372.68 44.28 L 249.99 44.28 C 246.93 44.28 244.45
    41.8 244.45 38.74 L 244.45 22.14 C 244.45 19.08 246.93 16.6 249.99 16.6 L 372.68
    16.6 C 375.73 16.6 378.21 19.08 378.21 22.14 L 378.21 38.74 C 378.21 41.8 375.73
    44.28 372.68 44.28 Z M 244.45 16.6"></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 249.07 25.6)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">AST</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 372.68 74.72 L 249.99 74.72 C 246.93 74.72 244.45
    72.24 244.45 69.19 L 244.45 52.58 C 244.45 49.52 246.93 47.05 249.99 47.05 L 372.68
    47.05 C 375.73 47.05 378.21 49.52 378.21 52.58 L 378.21 69.19 C 378.21 72.24 375.73
    74.72 372.68 74.72 Z M 244.45 47.05"></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 249.07 56.04)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">API</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 372.68 105.16 L 249.99 105.16 C 246.93 105.16 244.45
    102.68 244.45 99.63 L 244.45 83.02 C 244.45 79.97 246.93 77.49 249.99 77.49 L
    372.68 77.49 C 375.73 77.49 378.21 79.97 378.21 83.02 L 378.21 99.63 C 378.21
    102.68 375.73 105.16 372.68 105.16 Z M 244.45 77.49"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 249.07 86.48)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Token</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 372.68 -16.6 L 249.99 -16.6 C 246.93 -16.6 244.45
    -19.08 244.45 -22.14 L 244.45 -38.74 C 244.45 -41.8 246.93 -44.28 249.99 -44.28
    L 372.68 -44.28 C 375.73 -44.28 378.21 -41.8 378.21 -38.74 L 378.21 -22.14 C 378.21
    -19.08 375.73 -16.6 372.68 -16.6 Z M 244.45 -44.28"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 249.07 -34.59)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Code Graph</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 372.68 -47.05 L 249.99 -47.05 C 246.93 -47.05 244.45
    -49.52 244.45 -52.58 L 244.45 -69.19 C 244.45 -72.24 246.93 -74.72 249.99 -74.72
    L 372.68 -74.72 C 375.73 -74.72 378.21 -72.24 378.21 -69.19 L 378.21 -52.58 C
    378.21 -49.52 375.73 -47.05 372.68 -47.05 Z M 244.45 -74.72"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 249.07 -65.73)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Other Features</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 372.68 -77.49 L 249.99 -77.49 C 246.93 -77.49 244.45
    -79.97 244.45 -83.02 L 244.45 -99.63 C 244.45 -102.68 246.93 -105.16 249.99 -105.16
    L 372.68 -105.16 C 375.73 -105.16 378.21 -102.68 378.21 -99.63 L 378.21 -83.02
    C 378.21 -79.97 375.73 -77.49 372.68 -77.49 Z M 244.45 -105.16"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.07 -95.48)" fill="#000000" stroke="#000000"><foreignobject
    width="124.53" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Hybrid</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 372.68 -260.14 L 249.99 -260.14 C 246.93 -260.14
    244.45 -262.61 244.45 -265.67 L 244.45 -282.28 C 244.45 -285.33 246.93 -287.81
    249.99 -287.81 L 372.68 -287.81 C 375.73 -287.81 378.21 -285.33 378.21 -282.28
    L 378.21 -265.67 C 378.21 -262.61 375.73 -260.14 372.68 -260.14 Z M 244.45 -287.81"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.07 -278.82)" fill="#000000" stroke="#000000"><foreignobject
    width="124.53" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Code
    Clone Detection</foreignobject></g> <g fill="#EBFFEB"><path d="M 372.68 -218.62
    L 249.99 -218.62 C 246.93 -218.62 244.45 -221.1 244.45 -224.16 L 244.45 -240.76
    C 244.45 -243.82 246.93 -246.3 249.99 -246.3 L 372.68 -246.3 C 375.73 -246.3 378.21
    -243.82 378.21 -240.76 L 378.21 -224.16 C 378.21 -221.1 375.73 -218.62 372.68
    -218.62 Z M 244.45 -246.3"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 249.07
    -237.3)" fill="#000000" stroke="#000000"><foreignobject width="124.53" height="9.69"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Code Search</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 372.68 -185.42 L 249.99 -185.42 C 246.93 -185.42
    244.45 -187.89 244.45 -190.95 L 244.45 -207.56 C 244.45 -210.61 246.93 -213.09
    249.99 -213.09 L 372.68 -213.09 C 375.73 -213.09 378.21 -210.61 378.21 -207.56
    L 378.21 -190.95 C 378.21 -187.89 375.73 -185.42 372.68 -185.42 Z M 244.45 -213.09"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.07 -203.4)" fill="#000000" stroke="#000000"><foreignobject
    width="124.53" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Type
    Inference</foreignobject></g> <g fill="#EBFFEB"><path d="M 372.68 -144.83 L 249.99
    -144.83 C 246.93 -144.83 244.45 -147.31 244.45 -150.36 L 244.45 -176.19 C 244.45
    -179.25 246.93 -181.73 249.99 -181.73 L 372.68 -181.73 C 375.73 -181.73 378.21
    -179.25 378.21 -176.19 L 378.21 -150.36 C 378.21 -147.31 375.73 -144.83 372.68
    -144.83 Z M 244.45 -181.73"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 249.07
    -159.13)" fill="#000000" stroke="#000000"><foreignobject width="124.53" height="27.67"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Vulnerability Detection
    and Bug Finding</foreignobject></g> <g fill="#EBFFEB"><path d="M 372.68 -113.46
    L 249.99 -113.46 C 246.93 -113.46 244.45 -115.94 244.45 -119 L 244.45 -135.6 C
    244.45 -138.66 246.93 -141.14 249.99 -141.14 L 372.68 -141.14 C 375.73 -141.14
    378.21 -138.66 378.21 -135.6 L 378.21 -119 C 378.21 -115.94 375.73 -113.46 372.68
    -113.46 Z M 244.45 -141.14"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 249.07
    -132.14)" fill="#000000" stroke="#000000"><foreignobject width="124.53" height="9.69"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Code Classification</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 372.68 -298.88 L 249.99 -298.88 C 246.93 -298.88
    244.45 -301.36 244.45 -304.41 L 244.45 -321.02 C 244.45 -324.08 246.93 -326.55
    249.99 -326.55 L 372.68 -326.55 C 375.73 -326.55 378.21 -324.08 378.21 -321.02
    L 378.21 -304.41 C 378.21 -301.36 375.73 -298.88 372.68 -298.88 Z M 244.45 -326.55"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.07 -316.87)" fill="#000000" stroke="#000000"><foreignobject
    width="124.53" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Code
    Completion</foreignobject></g> <g fill="#EBFFEB"><path d="M 372.68 -351.46 L 249.99
    -351.46 C 246.93 -351.46 244.45 -353.94 244.45 -357 L 244.45 -373.6 C 244.45 -376.66
    246.93 -379.13 249.99 -379.13 L 372.68 -379.13 C 375.73 -379.13 378.21 -376.66
    378.21 -373.6 L 378.21 -357 C 378.21 -353.94 375.73 -351.46 372.68 -351.46 Z M
    244.45 -379.13"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 249.07 -370.14)"
    fill="#000000" stroke="#000000"><foreignobject width="124.53" height="9.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Code Summarization</foreignobject></g> <g fill="#EBFFEB"><path
    d="M 372.68 -401.27 L 249.99 -401.27 C 246.93 -401.27 244.45 -403.75 244.45 -406.81
    L 244.45 -423.41 C 244.45 -426.47 246.93 -428.95 249.99 -428.95 L 372.68 -428.95
    C 375.73 -428.95 378.21 -426.47 378.21 -423.41 L 378.21 -406.81 C 378.21 -403.75
    375.73 -401.27 372.68 -401.27 Z M 244.45 -428.95"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 249.07 -419.26)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Program
    Translation</foreignobject></g> <g fill="#EBFFEB"><path d="M 372.68 -437.25 L
    249.99 -437.25 C 246.93 -437.25 244.45 -439.73 244.45 -442.78 L 244.45 -459.39
    C 244.45 -462.45 246.93 -464.92 249.99 -464.92 L 372.68 -464.92 C 375.73 -464.92
    378.21 -462.45 378.21 -459.39 L 378.21 -442.78 C 378.21 -439.73 375.73 -437.25
    372.68 -437.25 Z M 244.45 -464.92"></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 249.07 -455.24)" fill="#000000" stroke="#000000"><foreignobject width="124.53"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Program
    Synthesis</foreignobject></g> <g fill="#EBFFEB"><path d="M 372.68 -475.99 L 249.99
    -475.99 C 246.93 -475.99 244.45 -478.47 244.45 -481.53 L 244.45 -498.13 C 244.45
    -501.19 246.93 -503.67 249.99 -503.67 L 372.68 -503.67 C 375.73 -503.67 378.21
    -501.19 378.21 -498.13 L 378.21 -481.53 C 378.21 -478.47 375.73 -475.99 372.68
    -475.99 Z M 244.45 -503.67"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 249.07
    -494.05)" fill="#000000" stroke="#000000"><foreignobject width="124.53" height="10.93"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Program Repair</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 703.56 431.63 L 401 431.63 C 397.94 431.63 395.46
    429.16 395.46 426.1 L 395.46 384.36 C 395.46 381.3 397.94 378.82 401 378.82 L
    703.56 378.82 C 706.62 378.82 709.1 381.3 709.1 384.36 L 709.1 426.1 C 709.1 429.16
    706.62 431.63 703.56 431.63 Z M 395.46 378.82"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 400.07 417.34)" fill="#000000" stroke="#000000"><foreignobject width="304.41"
    height="43.59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Raychev
    et al., [2014](#bib.bib198); Liu et al., [2016b](#bib.bib149); Gu et al., [2018](#bib.bib78);
    Alon et al., [2018](#bib.bib12); Hellendoorn et al., [2018](#bib.bib99); Malik
    et al., [2019](#bib.bib163); Svyatkovskiy et al., [2019](#bib.bib223); White et al.,
    [2016](#bib.bib257); Hu et al., [2018a](#bib.bib103); Zhang et al., [2020c](#bib.bib285);
    Gupta et al., [2017](#bib.bib90))</foreignobject></g> <g fill="#D9FFFF"><path
    d="M 703.56 388.62 L 401 388.62 C 397.94 388.62 395.46 386.15 395.46 383.09 L
    395.46 366.48 C 395.46 363.43 397.94 360.95 401 360.95 L 703.56 360.95 C 706.62
    360.95 709.1 363.43 709.1 366.48 L 709.1 383.09 C 709.1 386.15 706.62 388.62 703.56
    388.62 Z M 395.46 360.95"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 400.07
    373.4)" fill="#000000" stroke="#000000"><foreignobject width="304.41" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Allamanis et al., [2016](#bib.bib11);
    Sun et al., [2019](#bib.bib219))</foreignobject></g> <g fill="#D9FFFF"><path d="M
    703.56 370.75 L 401 370.75 C 397.94 370.75 395.46 368.27 395.46 365.22 L 395.46
    323.47 C 395.46 320.42 397.94 317.94 401 317.94 L 703.56 317.94 C 706.62 317.94
    709.1 320.42 709.1 323.47 L 709.1 365.22 C 709.1 368.27 706.62 370.75 703.56 370.75
    Z M 395.46 317.94"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 400.07 356.45)"
    fill="#000000" stroke="#000000"><foreignobject width="304.41" height="43.59" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">(Wan et al., [2018](#bib.bib238); Mou et al.,
    [2016](#bib.bib170); Chen et al., [2018](#bib.bib44); Allamanis et al., [2018b](#bib.bib10);
    Zhou et al., [2019](#bib.bib294); Wang and Li, [2021](#bib.bib248); Brockschmidt
    et al., [2019](#bib.bib25); Wei et al., [2020a](#bib.bib254); Allamanis et al.,
    [2020](#bib.bib8); Liu et al., [2020a](#bib.bib155))</foreignobject></g> <g fill="#D9FFFF"><path
    d="M 703.56 327.74 L 401 327.74 C 397.94 327.74 395.46 325.26 395.46 322.21 L
    395.46 305.6 C 395.46 302.54 397.94 300.07 401 300.07 L 703.56 300.07 C 706.62
    300.07 709.1 302.54 709.1 305.6 L 709.1 322.21 C 709.1 325.26 706.62 327.74 703.56
    327.74 Z M 395.46 300.07"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 400.07
    312.52)" fill="#000000" stroke="#000000"><foreignobject width="304.41" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Kim et al., [2021](#bib.bib118);
    Svyatkovskiy et al., [2020](#bib.bib221); Chirkova and Troshin, [2021a](#bib.bib49))</foreignobject></g>
    <g fill="#D9FFFF"><path d="M 703.56 335.34 L 401 335.34 C 397.94 335.34 395.46
    332.86 395.46 329.81 L 395.46 188.44 C 395.46 185.38 397.94 182.9 401 182.9 L
    703.56 182.9 C 706.62 182.9 709.1 185.38 709.1 188.44 L 709.1 329.81 C 709.1 332.86
    706.62 335.34 703.56 335.34 Z M 395.46 182.9"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 400.07 321.04)" fill="#000000" stroke="#000000"><foreignobject width="304.41"
    height="143.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Mou et al.,
    [2016](#bib.bib170); Bui et al., [2019a](#bib.bib29); Li et al., [2018b](#bib.bib142);
    Zhou et al., [2019](#bib.bib294); Cheng et al., [2021](#bib.bib47); Li et al.,
    [2019](#bib.bib139); Raychev et al., [2014](#bib.bib198); Liu et al., [2016b](#bib.bib149);
    Li et al., [2018a](#bib.bib129); Svyatkovskiy et al., [2019](#bib.bib223); Alon
    et al., [2020](#bib.bib13); Svyatkovskiy et al., [2020](#bib.bib221); Hellendoorn
    et al., [2018](#bib.bib99); Malik et al., [2019](#bib.bib163); Wei et al., [2020a](#bib.bib254);
    Allamanis et al., [2020](#bib.bib8); Gu et al., [2018](#bib.bib78); Wan et al.,
    [2019](#bib.bib235); Haldar et al., [2020](#bib.bib92); White et al., [2016](#bib.bib257);
    Wei and Li, [2017](#bib.bib253); Zhao and Huang, [2018](#bib.bib290); Wu et al.,
    [2020](#bib.bib262); Zhang et al., [2019](#bib.bib286); Allamanis et al., [2016](#bib.bib11);
    Iyer et al., [2016](#bib.bib108); Hu et al., [2018a](#bib.bib103); Alon et al.,
    [2018](#bib.bib12); Wan et al., [2018](#bib.bib238); Chen et al., [2018](#bib.bib44);
    Gu et al., [2017](#bib.bib80); Dong and Lapata, [2016](#bib.bib65); Liu et al.,
    [2016a](#bib.bib148); Sun et al., [2019](#bib.bib219); Zhong et al., [2017](#bib.bib292);
    Cai et al., [2018](#bib.bib33); Gupta et al., [2017](#bib.bib90); Vasic et al.,
    [2018](#bib.bib231); Dinella et al., [2020](#bib.bib63); Chakraborty et al., [2020](#bib.bib40);
    Zhu et al., [2021](#bib.bib297); Tufano et al., [2018a](#bib.bib229); Li et al.,
    [2020](#bib.bib135))</foreignobject></g> <g fill="#D9FFFF"><path d="M 703.56 221.76
    L 401 221.76 C 397.94 221.76 395.46 219.28 395.46 216.23 L 395.46 199.62 C 395.46
    196.57 397.94 194.09 401 194.09 L 703.56 194.09 C 706.62 194.09 709.1 196.57 709.1
    199.62 L 709.1 216.23 C 709.1 219.28 706.62 221.76 703.56 221.76 Z M 395.46 194.09"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 400.07 204.47)" fill="#000000" stroke="#000000"><foreignobject
    width="304.41" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Rozière
    et al., [2020](#bib.bib200))</foreignobject></g> <g fill="#D9FFFF"><path d="M
    703.56 234.33 L 401 234.33 C 397.94 234.33 395.46 231.85 395.46 228.8 L 395.46
    104.03 C 395.46 100.98 397.94 98.5 401 98.5 L 703.56 98.5 C 706.62 98.5 709.1
    100.98 709.1 104.03 L 709.1 228.8 C 709.1 231.85 706.62 234.33 703.56 234.33 Z
    M 395.46 98.5"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 400.07 220.03)"
    fill="#000000" stroke="#000000"><foreignobject width="304.41" height="126.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Kanade et al., [2020](#bib.bib116);
    Feng et al., [2020](#bib.bib67); Guo et al., [2021](#bib.bib84); Niu et al., [2022](#bib.bib179);
    Jiang et al., [2021b](#bib.bib114); Lachaux et al., [2021](#bib.bib119); Ahmad
    et al., [2021](#bib.bib4); Zhang et al., [2022a](#bib.bib284); Wang et al., [2021b](#bib.bib245);
    Guo et al., [2022b](#bib.bib83); Mastropaolo et al., [2021](#bib.bib164); Wang
    et al., [2021a](#bib.bib249); Bui et al., [2021a](#bib.bib31); Jain et al., [2021](#bib.bib110);
    Wan et al., [2022c](#bib.bib237); Zhang et al., [2022b](#bib.bib289); Shi et al.,
    [2022b](#bib.bib209); Zhou et al., [2021](#bib.bib293); Wang et al., [2022b](#bib.bib240),
    [c](#bib.bib239); Chen et al., [2021b](#bib.bib43); Li et al., [2022a](#bib.bib133);
    Nijkamp et al., [2022](#bib.bib178); Chowdhery et al., [2022](#bib.bib51); Fried
    et al., [2022](#bib.bib69); Christopoulou et al., [2022](#bib.bib52); Zheng et al.,
    [2023](#bib.bib291); Chai et al., [2022a](#bib.bib38); Li et al., [2023a](#bib.bib131);
    Roziere et al., [2023](#bib.bib199); Wang et al., [2023](#bib.bib247); Gunasekar
    et al., [2023](#bib.bib82); Liu et al., [2023](#bib.bib147); Geng et al., [2024](#bib.bib74);
    Xia et al., [2023](#bib.bib264); Li et al., [2023c](#bib.bib127), [b](#bib.bib126);
    Luo et al., [2023](#bib.bib161))</foreignobject></g> <g fill="#D9FFFF"><path d="M
    703.56 138.74 L 401 138.74 C 397.94 138.74 395.46 136.26 395.46 133.2 L 395.46
    116.6 C 395.46 113.54 397.94 111.07 401 111.07 L 703.56 111.07 C 706.62 111.07
    709.1 113.54 709.1 116.6 L 709.1 133.2 C 709.1 136.26 706.62 138.74 703.56 138.74
    Z M 395.46 111.07"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 400.07 123.52)"
    fill="#000000" stroke="#000000"><foreignobject width="304.41" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">(Wan et al., [2018](#bib.bib238); Yao et al.,
    [2019](#bib.bib273); Gupta et al., [2018](#bib.bib88); Zhong et al., [2017](#bib.bib292))</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 704.77 117.73 L 402.2 117.73 C 399.14 117.73 396.66
    115.25 396.66 112.19 L 396.66 70.45 C 396.66 67.4 399.14 64.92 402.2 64.92 L 704.77
    64.92 C 707.82 64.92 710.3 67.4 710.3 70.45 L 710.3 112.19 C 710.3 115.25 707.82
    117.73 704.77 117.73 Z M 396.66 64.92"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 401.27 103.43)" fill="#000000" stroke="#000000"><foreignobject width="304.41"
    height="43.59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Cummins
    et al., [2017](#bib.bib56); White et al., [2015](#bib.bib258); Iyer et al., [2016](#bib.bib108);
    Allamanis et al., [2016](#bib.bib11); Cvitkovic et al., [2019](#bib.bib57); Chirkova
    and Troshin, [2021b](#bib.bib50); Kanade et al., [2020](#bib.bib116); Feng et al.,
    [2020](#bib.bib67); Karampatsis et al., [2020](#bib.bib117))</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 704.77 87.29 L 402.2 87.29 C 399.14 87.29 396.66
    84.81 396.66 81.75 L 396.66 40.01 C 396.66 36.96 399.14 34.48 402.2 34.48 L 704.77
    34.48 C 707.82 34.48 710.3 36.96 710.3 40.01 L 710.3 81.75 C 710.3 84.81 707.82
    87.29 704.77 87.29 Z M 396.66 34.48"></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 401.27 72.99)" fill="#000000" stroke="#000000"><foreignobject width="304.41"
    height="43.59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Moreno
    et al., [2015](#bib.bib169); Jiang et al., [2017](#bib.bib111); Gu et al., [2016](#bib.bib79);
    Nguyen et al., [2017](#bib.bib177); Bui et al., [2019b](#bib.bib30); Hu et al.,
    [2018b](#bib.bib104); Wei et al., [2022](#bib.bib255); Hadi et al., [2022](#bib.bib91))</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 704.77 56.85 L 402.2 56.85 C 399.14 56.85 396.66
    54.37 396.66 51.31 L 396.66 9.57 C 396.66 6.51 399.14 4.04 402.2 4.04 L 704.77
    4.04 C 707.82 4.04 710.3 6.51 710.3 9.57 L 710.3 51.31 C 710.3 54.37 707.82 56.85
    704.77 56.85 Z M 396.66 4.04"></path></g><g transform="matrix(1.0 0.0 0.0 1.0
    401.27 42.55)" fill="#000000" stroke="#000000"><foreignobject width="304.41" height="43.59"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Mou et al., [2016](#bib.bib170);
    Liu et al., [2020e](#bib.bib152); Zhang et al., [2019](#bib.bib286); Kim et al.,
    [2021](#bib.bib118); Niu et al., [2022](#bib.bib179); Hu et al., [2018a](#bib.bib103);
    Alon et al., [2019](#bib.bib14), [2018](#bib.bib12), [2020](#bib.bib13); Yin and
    Neubig, [2017](#bib.bib277); Chen et al., [2018](#bib.bib44))</foreignobject></g>
    <g fill="#FFFFE6"><path d="M 704.77 26.41 L 402.2 26.41 C 399.14 26.41 396.66
    23.93 396.66 20.87 L 396.66 -20.87 C 396.66 -23.93 399.14 -26.41 402.2 -26.41
    L 704.77 -26.41 C 707.82 -26.41 710.3 -23.93 710.3 -20.87 L 710.3 20.87 C 710.3
    23.93 707.82 26.41 704.77 26.41 Z M 396.66 -26.41"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 401.27 12.11)" fill="#000000" stroke="#000000"><foreignobject width="304.41"
    height="43.59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Li et al.,
    [2022c](#bib.bib140); Ben-Nun et al., [2018](#bib.bib21); VenkataKeerthy et al.,
    [2020](#bib.bib233); Cummins et al., [2021](#bib.bib55); Peng et al., [2021](#bib.bib186);
    Gui et al., [2022](#bib.bib81))</foreignobject></g> <g fill="#FFFFE6"><path d="M
    704.77 -4.04 L 402.2 -4.04 C 399.14 -4.04 396.66 -6.51 396.66 -9.57 L 396.66 -51.31
    C 396.66 -54.37 399.14 -56.85 402.2 -56.85 L 704.77 -56.85 C 707.82 -56.85 710.3
    -54.37 710.3 -51.31 L 710.3 -9.57 C 710.3 -6.51 707.82 -4.04 704.77 -4.04 Z M
    396.66 -56.85"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 401.27 -18.33)"
    fill="#000000" stroke="#000000"><foreignobject width="304.41" height="43.59" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">(Cummins et al., [2021](#bib.bib55); Allamanis
    et al., [2018b](#bib.bib10); Allamanis and Brockschmidt, [2017](#bib.bib9); Brockschmidt
    et al., [2019](#bib.bib25); Sui et al., [2020](#bib.bib214); Shi et al., [2022c](#bib.bib211);
    Chen et al., [2021a](#bib.bib45))</foreignobject></g> <g fill="#FFFFE6"><path
    d="M 704.77 -47.05 L 402.2 -47.05 C 399.14 -47.05 396.66 -49.52 396.66 -52.58
    L 396.66 -69.19 C 396.66 -72.24 399.14 -74.72 402.2 -74.72 L 704.77 -74.72 C 707.82
    -74.72 710.3 -72.24 710.3 -69.19 L 710.3 -52.58 C 710.3 -49.52 707.82 -47.05 704.77
    -47.05 Z M 396.66 -74.72"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 401.27
    -62.27)" fill="#000000" stroke="#000000"><foreignobject width="304.41" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Henkel et al., [2018](#bib.bib100);
    Hoang et al., [2020](#bib.bib102); Tufano et al., [2019](#bib.bib228); Brody et al.,
    [2020](#bib.bib26))</foreignobject></g> <g fill="#FFFFE6"><path d="M 704.77 -64.92
    L 402.2 -64.92 C 399.14 -64.92 396.66 -67.4 396.66 -70.45 L 396.66 -112.19 C 396.66
    -115.25 399.14 -117.73 402.2 -117.73 L 704.77 -117.73 C 707.82 -117.73 710.3 -115.25
    710.3 -112.19 L 710.3 -70.45 C 710.3 -67.4 707.82 -64.92 704.77 -64.92 Z M 396.66
    -117.73"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 401.27 -79.22)" fill="#000000"
    stroke="#000000"><foreignobject width="304.41" height="43.59" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">(Gu et al., [2018](#bib.bib78); White et al.,
    [2016](#bib.bib257); Zhao and Huang, [2018](#bib.bib290); Wan et al., [2018](#bib.bib238),
    [2019](#bib.bib235); Chakraborty and Ray, [2021](#bib.bib41))</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 704.77 -113.46 L 402.2 -113.46 C 399.14 -113.46 396.66
    -115.94 396.66 -119 L 396.66 -135.6 C 396.66 -138.66 399.14 -141.14 402.2 -141.14
    L 704.77 -141.14 C 707.82 -141.14 710.3 -138.66 710.3 -135.6 L 710.3 -119 C 710.3
    -115.94 707.82 -113.46 704.77 -113.46 Z M 396.66 -141.14"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 401.27 -128.68)" fill="#000000" stroke="#000000"><foreignobject width="304.41"
    height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Mou et al.,
    [2016](#bib.bib170); LeClair et al., [2018](#bib.bib122); Bui et al., [2019a](#bib.bib29))</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 704.77 -120.27 L 402.2 -120.27 C 399.14 -120.27 396.66
    -122.75 396.66 -125.8 L 396.66 -200.75 C 396.66 -203.81 399.14 -206.29 402.2 -206.29
    L 704.77 -206.29 C 707.82 -206.29 710.3 -203.81 710.3 -200.75 L 710.3 -125.8 C
    710.3 -122.75 707.82 -120.27 704.77 -120.27 Z M 396.66 -206.29"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 401.27 -134.57)" fill="#000000" stroke="#000000"><foreignobject
    width="304.41" height="76.8" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Wang
    et al., [2016](#bib.bib243); Dam et al., [2018](#bib.bib58); Li et al., [2018b](#bib.bib142);
    Zou et al., [2019](#bib.bib299); Li et al., [2021d](#bib.bib141); Le et al., [2018](#bib.bib121);
    Zhou et al., [2019](#bib.bib294); Wang et al., [2020c](#bib.bib241); Cao et al.,
    [2022](#bib.bib35); Cheng et al., [2021](#bib.bib47); Liu et al., [2021b](#bib.bib156);
    Wu et al., [2022b](#bib.bib263); Li et al., [2021c](#bib.bib137); Zou et al.,
    [2021](#bib.bib300); Pradel and Sen, [2018](#bib.bib190); Li et al., [2019](#bib.bib139);
    Gupta et al., [2019](#bib.bib89); Li et al., [2021b](#bib.bib136))</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 704.77 -172.85 L 402.2 -172.85 C 399.14 -172.85 396.66
    -175.33 396.66 -178.38 L 396.66 -220.12 C 396.66 -223.18 399.14 -225.66 402.2
    -225.66 L 704.77 -225.66 C 707.82 -225.66 710.3 -223.18 710.3 -220.12 L 710.3
    -178.38 C 710.3 -175.33 707.82 -172.85 704.77 -172.85 Z M 396.66 -225.66"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 401.27 -187.15)" fill="#000000" stroke="#000000"><foreignobject
    width="304.41" height="43.59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Hellendoorn
    et al., [2018](#bib.bib99); Malik et al., [2019](#bib.bib163); Pradel et al.,
    [2020](#bib.bib189); Wei et al., [2020a](#bib.bib254); Pandi et al., [2020](#bib.bib183);
    Allamanis et al., [2020](#bib.bib8); Mir et al., [2022](#bib.bib168); Huang et al.,
    [2022](#bib.bib106))</foreignobject></g> <g fill="#EBFFEB"><path d="M 704.77 -197.75
    L 402.2 -197.75 C 399.14 -197.75 396.66 -200.23 396.66 -203.29 L 396.66 -261.63
    C 396.66 -264.69 399.14 -267.17 402.2 -267.17 L 704.77 -267.17 C 707.82 -267.17
    710.3 -264.69 710.3 -261.63 L 710.3 -203.29 C 710.3 -200.23 707.82 -197.75 704.77
    -197.75 Z M 396.66 -267.17"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 401.27
    -212.05)" fill="#000000" stroke="#000000"><foreignobject width="304.41" height="60.19"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Gu et al., [2018](#bib.bib78);
    Wan et al., [2019](#bib.bib235); Deng et al., [2022](#bib.bib59); Ling et al.,
    [2021a](#bib.bib146); Shi et al., [2022c](#bib.bib211); Haldar et al., [2020](#bib.bib92);
    Cambronero et al., [2019](#bib.bib34); Bui et al., [2021b](#bib.bib32); Li et al.,
    [2022b](#bib.bib132); Yao et al., [2019](#bib.bib273); Sun et al., [2022a](#bib.bib216);
    Zhu et al., [2020](#bib.bib296); Chai et al., [2022b](#bib.bib39); Wan et al.,
    [2022b](#bib.bib236); Gu et al., [2022](#bib.bib77))</foreignobject></g> <g fill="#EBFFEB"><path
    d="M 704.77 -239.27 L 402.2 -239.27 C 399.14 -239.27 396.66 -241.74 396.66 -244.8
    L 396.66 -303.15 C 396.66 -306.2 399.14 -308.68 402.2 -308.68 L 704.77 -308.68
    C 707.82 -308.68 710.3 -306.2 710.3 -303.15 L 710.3 -244.8 C 710.3 -241.74 707.82
    -239.27 704.77 -239.27 Z M 396.66 -308.68"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 401.27 -253.56)" fill="#000000" stroke="#000000"><foreignobject width="304.41"
    height="60.19" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(White et al.,
    [2016](#bib.bib257); Wei and Li, [2017](#bib.bib253); Zhao and Huang, [2018](#bib.bib290);
    Zhang et al., [2019](#bib.bib286); Büch and Andrzejak, [2019](#bib.bib28); Wang
    et al., [2020b](#bib.bib244); Nair et al., [2020](#bib.bib173); Wu et al., [2020](#bib.bib262);
    Hu et al., [2022](#bib.bib105); Wu et al., [2022a](#bib.bib260); Mehrotra et al.,
    [2021](#bib.bib165); Tufano et al., [2018b](#bib.bib230); Ding et al., [2022](#bib.bib64);
    Tao et al., [2022](#bib.bib225); Gui et al., [2022](#bib.bib81))</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 704.77 -278.01 L 402.2 -278.01 C 399.14 -278.01 396.66
    -280.49 396.66 -283.54 L 396.66 -341.89 C 396.66 -344.95 399.14 -347.42 402.2
    -347.42 L 704.77 -347.42 C 707.82 -347.42 710.3 -344.95 710.3 -341.89 L 710.3
    -283.54 C 710.3 -280.49 707.82 -278.01 704.77 -278.01 Z M 396.66 -347.42"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 401.27 -292.31)" fill="#000000" stroke="#000000"><foreignobject
    width="304.41" height="60.19" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Raychev
    et al., [2014](#bib.bib198); Liu et al., [2016b](#bib.bib149); Li et al., [2018a](#bib.bib129);
    Svyatkovskiy et al., [2019](#bib.bib223); Kim et al., [2021](#bib.bib118); Alon
    et al., [2020](#bib.bib13); Wang and Li, [2021](#bib.bib248); Brockschmidt et al.,
    [2019](#bib.bib25); Svyatkovskiy et al., [2020](#bib.bib221); Liu et al., [2020b](#bib.bib150),
    [c](#bib.bib151); Lu et al., [2022](#bib.bib159); Guo et al., [2022c](#bib.bib85);
    Svyatkovskiy et al., [2021](#bib.bib222); Shrivastava et al., [2020](#bib.bib212))</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 704.77 -289.08 L 402.2 -289.08 C 399.14 -289.08 396.66
    -291.56 396.66 -294.61 L 396.66 -435.98 C 396.66 -439.04 399.14 -441.52 402.2
    -441.52 L 704.77 -441.52 C 707.82 -441.52 710.3 -439.04 710.3 -435.98 L 710.3
    -294.61 C 710.3 -291.56 707.82 -289.08 704.77 -289.08 Z M 396.66 -441.52"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 401.27 -303.38)" fill="#000000" stroke="#000000"><foreignobject
    width="304.41" height="143.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Allamanis
    et al., [2016](#bib.bib11); Iyer et al., [2016](#bib.bib108); Hu et al., [2018a](#bib.bib103);
    Alon et al., [2018](#bib.bib12); LeClair et al., [2019](#bib.bib124); Fernandes
    et al., [2019](#bib.bib68); LeClair et al., [2020](#bib.bib123); Jin et al., [2022](#bib.bib115);
    Guo et al., [2022a](#bib.bib86); Ahmad et al., [2020](#bib.bib5); Wu et al., [2021](#bib.bib259);
    Gong et al., [2022](#bib.bib75); Tang et al., [2022](#bib.bib224); Wan et al.,
    [2018](#bib.bib238); Shi et al., [2021](#bib.bib208); Yang et al., [2021](#bib.bib271);
    Gao and Lyu, [2022](#bib.bib71); Wang et al., [2022a](#bib.bib246); Haque et al.,
    [2020](#bib.bib94); Bansal et al., [2021](#bib.bib17); Shahbazi et al., [2021](#bib.bib206);
    Ciurumelea et al., [2020](#bib.bib53); Lin et al., [2021](#bib.bib143); Zhang
    et al., [2020c](#bib.bib285); Wei et al., [2020b](#bib.bib252); Liu et al., [2020a](#bib.bib155);
    Li et al., [2021a](#bib.bib128); Zhu et al., [2022](#bib.bib298); Hu et al., [2018b](#bib.bib104);
    Xie et al., [2022](#bib.bib265); Wei et al., [2019](#bib.bib251); Yang et al.,
    [2022a](#bib.bib269); Ye et al., [2020](#bib.bib275); Mu et al., [2022](#bib.bib171);
    Xie et al., [2021](#bib.bib266); Haque et al., [2021](#bib.bib93); Liu et al.,
    [2019](#bib.bib153); Panthaplackel et al., [2021](#bib.bib184); Nguyen et al.,
    [2020](#bib.bib176); Panthaplackel et al., [2020](#bib.bib185); Liu et al., [2020d](#bib.bib157);
    Gao et al., [2021](#bib.bib72); Li et al., [2022e](#bib.bib130))</foreignobject></g>
    <g fill="#EBFFEB"><path d="M 704.77 -401.27 L 402.2 -401.27 C 399.14 -401.27 396.66
    -403.75 396.66 -406.81 L 396.66 -423.41 C 396.66 -426.47 399.14 -428.95 402.2
    -428.95 L 704.77 -428.95 C 707.82 -428.95 710.3 -426.47 710.3 -423.41 L 710.3
    -406.81 C 710.3 -403.75 707.82 -401.27 704.77 -401.27 Z M 396.66 -428.95"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 401.27 -416.49)" fill="#000000" stroke="#000000"><foreignobject
    width="304.41" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Chen
    et al., [2018](#bib.bib44); Gu et al., [2017](#bib.bib80); Rozière et al., [2020](#bib.bib200),
    [2022](#bib.bib201))</foreignobject></g> <g fill="#EBFFEB"><path d="M 704.77 -391.47
    L 402.2 -391.47 C 399.14 -391.47 396.66 -393.95 396.66 -397.01 L 396.66 -505.17
    C 396.66 -508.22 399.14 -510.7 402.2 -510.7 L 704.77 -510.7 C 707.82 -510.7 710.3
    -508.22 710.3 -505.17 L 710.3 -397.01 C 710.3 -393.95 707.82 -391.47 704.77 -391.47
    Z M 396.66 -510.7"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 401.27 -405.77)"
    fill="#000000" stroke="#000000"><foreignobject width="304.41" height="110" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">(Dong and Lapata, [2016](#bib.bib65); Liu et al.,
    [2016a](#bib.bib148); Beltagy and Quirk, [2016](#bib.bib20); Yin and Neubig, [2017](#bib.bib277);
    Maddison and Tarlow, [2014](#bib.bib162); Ling et al., [2016](#bib.bib145); Rabinovich
    et al., [2017](#bib.bib193); Sun et al., [2019](#bib.bib219), [2020](#bib.bib220);
    Hayati et al., [2018](#bib.bib98); Iyer et al., [2018](#bib.bib109); Nan et al.,
    [2020](#bib.bib174); Lu et al., [2021](#bib.bib160); Raffel et al., [2020](#bib.bib194);
    Xu et al., [2020](#bib.bib267); Chen et al., [2021b](#bib.bib43); Li et al., [2022a](#bib.bib133);
    Poesia et al., [2022](#bib.bib187); Shu and Zhang, [2017](#bib.bib213); Balog
    et al., [2017](#bib.bib16); Devlin et al., [2017](#bib.bib62); Nye et al., [2019](#bib.bib180);
    Bavishi et al., [2019](#bib.bib19); Zhong et al., [2017](#bib.bib292); Yu et al.,
    [2018b](#bib.bib280), [2019b](#bib.bib281), [2019a](#bib.bib279); Cai et al.,
    [2018](#bib.bib33); Yu et al., [2018a](#bib.bib278))</foreignobject></g> <g fill="#EBFFEB"><path
    d="M 704.77 -438.52 L 402.2 -438.52 C 399.14 -438.52 396.66 -441 396.66 -444.05
    L 396.66 -535.61 C 396.66 -538.66 399.14 -541.14 402.2 -541.14 L 704.77 -541.14
    C 707.82 -541.14 710.3 -538.66 710.3 -535.61 L 710.3 -444.05 C 710.3 -441 707.82
    -438.52 704.77 -438.52 Z M 396.66 -541.14"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 401.27 -452.82)" fill="#000000" stroke="#000000"><foreignobject width="304.41"
    height="93.4" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Bhatia and
    Singh, [2016](#bib.bib23); Santos et al., [2018](#bib.bib203); Gupta et al., [2017](#bib.bib90);
    Chen et al., [2019](#bib.bib46); Gupta et al., [2018](#bib.bib88); Vasic et al.,
    [2018](#bib.bib231); Dinella et al., [2020](#bib.bib63); Graves et al., [2014](#bib.bib76);
    Tarlow et al., [2020](#bib.bib226); Mesbah et al., [2019](#bib.bib166); Chakraborty
    et al., [2020](#bib.bib40); Zhu et al., [2021](#bib.bib297); Yasunaga and Liang,
    [2020](#bib.bib274); Li et al., [2022d](#bib.bib138); Berabi et al., [2021](#bib.bib22);
    Fu et al., [2022](#bib.bib70); Raffel et al., [2020](#bib.bib194); Jiang et al.,
    [2021a](#bib.bib113); Tufano et al., [2018a](#bib.bib229); Hata et al., [2018](#bib.bib97);
    Harer et al., [2018](#bib.bib95); Gupta et al., [2020](#bib.bib87); Li et al.,
    [2020](#bib.bib135); White et al., [2019](#bib.bib256); Tian et al., [2020](#bib.bib227))</foreignobject></g><path
    d="M 46.4 0 L 78.13 0" style="fill:none"><path d="M 46.4 0 L 60.24 0 L 60.24 0
    L 60.24 268.44 L 78.13 268.44" style="fill:none"><path d="M 46.4 0 L 60.24 0 L
    60.24 0 L 60.24 -273.97 L 78.13 -273.97" style="fill:none"><path d="M 124.53 282.55
    L 124.53 344.35 L 136.34 344.35" style="fill:none"><path d="M 124.53 254.32 L
    124.53 207.92 L 136.34 207.92" style="fill:none"><path d="M 215.3 344.35 L 229.14
    344.35 L 229.14 344.35 L 229.14 405.23 L 242.98 405.23" style="fill:none"><path
    d="M 215.3 344.35 L 229.14 344.35 L 229.14 344.35 L 229.14 374.79 L 242.98 374.79"
    style="fill:none"><path d="M 215.3 344.35 L 229.14 344.35 L 229.14 344.35 L 229.14
    344.35 L 242.98 344.35" style="fill:none"><path d="M 215.3 344.35 L 229.14 344.35
    L 229.14 344.35 L 229.14 313.9 L 242.98 313.9" style="fill:none"><path d="M 215.3
    207.92 L 229.14 207.92 L 229.14 207.92 L 229.14 259.12 L 242.98 259.12" style="fill:none"><path
    d="M 215.3 207.92 L 229.14 207.92 L 229.14 207.92 L 229.14 207.92 L 242.98 207.92"
    style="fill:none"><path d="M 215.3 207.92 L 229.14 207.92 L 229.14 207.92 L 229.14
    166.41 L 242.98 166.41" style="fill:none"><path d="M 215.3 207.92 L 229.14 207.92
    L 229.14 207.92 L 229.14 124.9 L 242.98 124.9" style="fill:none"><path d="M 170.93
    0 L 198.61 0 L 198.61 0 L 198.61 91.32 L 244.18 91.32" style="fill:none"><path
    d="M 170.93 0 L 198.61 0 L 198.61 0 L 198.61 60.88 L 244.18 60.88" style="fill:none"><path
    d="M 170.93 0 L 198.61 0 L 198.61 0 L 198.61 30.44 L 244.18 30.44" style="fill:none"><path
    d="M 170.93 0 L 244.18 0" style="fill:none"><path d="M 170.93 0 L 198.61 0 L 198.61
    0 L 198.61 -30.44 L 244.18 -30.44" style="fill:none"><path d="M 170.93 0 L 198.61
    0 L 198.61 0 L 198.61 -60.88 L 244.18 -60.88" style="fill:none"><path d="M 170.93
    0 L 198.61 0 L 198.61 0 L 198.61 -91.32 L 244.18 -91.32" style="fill:none"><path
    d="M 170.93 -273.97 L 198.61 -273.97 L 198.61 -273.97 L 198.61 -127.3 L 244.18
    -127.3" style="fill:none"><path d="M 170.93 -273.97 L 198.61 -273.97 L 198.61
    -273.97 L 198.61 -163.28 L 244.18 -163.28" style="fill:none"><path d="M 170.93
    -273.97 L 198.61 -273.97 L 198.61 -273.97 L 198.61 -199.25 L 244.18 -199.25" style="fill:none"><path
    d="M 170.93 -273.97 L 198.61 -273.97 L 198.61 -273.97 L 198.61 -232.46 L 244.18
    -232.46" style="fill:none"><path d="M 170.93 -273.97 L 244.18 -273.97" style="fill:none"><path
    d="M 170.93 -273.97 L 198.61 -273.97 L 198.61 -273.97 L 198.61 -312.72 L 244.18
    -312.72" style="fill:none"><path d="M 170.93 -273.97 L 198.61 -273.97 L 198.61
    -273.97 L 198.61 -365.3 L 244.18 -365.3" style="fill:none"><path d="M 170.93 -273.97
    L 198.61 -273.97 L 198.61 -273.97 L 198.61 -415.11 L 244.18 -415.11" style="fill:none"><path
    d="M 170.93 -273.97 L 198.61 -273.97 L 198.61 -273.97 L 198.61 -451.09 L 244.18
    -451.09" style="fill:none"><path d="M 170.93 -273.97 L 198.61 -273.97 L 198.61
    -273.97 L 198.61 -489.83 L 244.18 -489.83" style="fill:none"><path d="M 377.29
    405.23 L 395.18 405.23" style="fill:none"><path d="M 377.29 374.79 L 395.18 374.79"
    style="fill:none"><path d="M 377.29 344.35 L 395.18 344.35" style="fill:none"><path
    d="M 377.29 313.9 L 395.18 313.9" style="fill:none"><path d="M 377.29 259.12 L
    395.18 259.12" style="fill:none"><path d="M 377.29 207.92 L 395.18 207.92" style="fill:none"><path
    d="M 377.29 166.41 L 395.18 166.41" style="fill:none"><path d="M 377.29 124.9
    L 395.18 124.9" style="fill:none"><path d="M 378.49 91.32 L 396.39 91.32" style="fill:none"><path
    d="M 378.49 60.88 L 396.39 60.88" style="fill:none"><path d="M 378.49 30.44 L
    396.39 30.44" style="fill:none"><path d="M 378.49 0 L 396.39 0" style="fill:none"><path
    d="M 378.49 -30.44 L 396.39 -30.44" style="fill:none"><path d="M 378.49 -60.88
    L 396.39 -60.88" style="fill:none"><path d="M 378.49 -91.32 L 396.39 -91.32" style="fill:none"><path
    d="M 378.49 -127.3 L 396.39 -127.3" style="fill:none"><path d="M 378.49 -163.28
    L 396.39 -163.28" style="fill:none"><path d="M 378.49 -199.25 L 396.39 -199.25"
    style="fill:none"><path d="M 378.49 -232.46 L 396.39 -232.46" style="fill:none"><path
    d="M 378.49 -273.97 L 396.39 -273.97" style="fill:none"><path d="M 378.49 -312.72
    L 396.39 -312.72" style="fill:none"><path d="M 378.49 -365.3 L 396.39 -365.3"
    style="fill:none"><path d="M 378.49 -415.11 L 396.39 -415.11" style="fill:none"><path
    d="M 378.49 -451.09 L 396.39 -451.09" style="fill:none"><path d="M 378.49 -489.83
    L 396.39 -489.83" style="fill:none"></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path></path>
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3. The taxonomy of deep learning for code intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ 3.1\. Taxonomy ‣ 3\. Literature Review ‣ Deep
    Learning for Code Intelligence: Survey, Benchmark and Toolkit") illustrates the
    taxonomy of current studies on deep learning for code intelligence that we have
    surveyed in this paper. From our observation, the research in this field can be
    broken down into three distinct aspects: i.e., code features, deep learning techniques,
    and applications. (1) Code Features. As the foundation of deep-learning-based
    code intelligence, code representation seeks to represent source code as distributed
    vectors. We categorize the current code representation approaches by the features
    of input code that they use, such as code tokens, Intermediate Representations
    (IRs), Application Programming Interfaces (APIs), Abstract Syntax Trees (ASTs)
    and code graphs (e.g., graphs that illustrate control flow and data flow). (2)
    In the realm of deep learning techniques, we initially delve into various types
    of neural networks, i.e., RNNs, CNNs, Transformers, and GNNs. Subsequently, we
    examine the diverse learning paradigms employed for modeling source code, i.e.,
    supervised learning, unsupervised learning, self-supervised learning, and reinforcement
    learning. (3) We investigate multiple downstream applications that are based on
    code representation and deep learning techniques, including code classification,
    vulnerability detection and bug finding, type inference, code search, code clone
    detection, code completion, code summarization, program translation, program synthesis,
    and program repair.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Code Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/083c580783c3d1da625dab26cb03aa3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4. A detailed C code snippet with its corresponding tokens, IR, AST,
    and IR-based ﬂow graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To represent source code, we need to first determine what to represent. Numerous
    works have proposed to extract the code features from diverse perspectives, including
    code tokens, IRs, ASTs, and various types of code graphs. Figure [4](#S3.F4 "Figure
    4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") shows a detailed code snippet written in C, with
    its corresponding code tokens, IR, AST, control-flow graph, data-flow graph, code
    property graph, and IR-based flow graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Code Tokens
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Code tokens, shaping the textual appearance of source code, are composed of
    function name, keywords, and various variable identifiers. These tokens are simple
    yet effective in representing the semantics of programs. The majority of approaches
    for processing code involve breaking the program down into a sequence of tokens
    based on specific delimiters, such as spaces or the capitalization patterns in
    identifiers (for identifiers like SortList and intArray). Cummins et al. ([2017](#bib.bib56))
    introduced a character-level LSTM network to represent the sequence of code characters
    for program synthesis. Since the set of characters to form a program is always
    a limited size, the character-level code representation does not have the problem
    of out-of-vocabulary. However, this tokenization process at the character level
    breaks down the meaning of the original words and also increases the length of
    the code sequence, which can make it challenging to understand the overall semantics
    of the program.
  prefs: []
  type: TYPE_NORMAL
- en: More coarsely, many word-level approaches are proposed to tokenize source code
    into words by separators. For example, White et al. ([2015](#bib.bib258)) and
    Iyer et al. ([2016](#bib.bib108)) proposed to tokenize the program into words
    by whitespace, and designed RNNs to represent them for code summarization and
    code completion. Allamanis et al. ([2016](#bib.bib11)) designed a CNN with an
    attention mechanism to better represent the hierarchical structure of code over
    the subtokens that are simply tokenized by camel cases, to predict the function
    name.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-Vocabulary (OOV) Issue. Since the variables and function names are always
    defined by developers without constraints, the size of vocabulary will explosively
    increase with the increasing training data, resulting in the out-of-vocabulary
    issue, which is more severe than that in NLP. To mitigate this issue, Cvitkovic
    et al. ([2019](#bib.bib57)) proposed a graph–structured cache, which introduces
    additional nodes for the encountered new words, and connects those nodes with
    edges based on where they occur in the code. Recently, Chirkova and Troshin ([2021b](#bib.bib50))
    offered a straightforward yet effective solution to mitigate the OOV issue by
    using identifier anonymization, and observed promising performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another effective approach is to tokenize the source code at a sub-word level,
    such as using techniques like Byte Pair Encoding (BPE), which aims to construct
    a set of sub-words that can be combined to represent the entire code corpus. Figure [4](#S3.F4
    "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code
    Intelligence: Survey, Benchmark and Toolkit") (b) shows the source tokens obtained
    by the strategy of word tokenization and BPE tokenization. For the input variable
    number, the word tokenization will maintain the original word and consider it
    as a rare word, while the BPE tokenization will split it into two common sub-words,
    i.e., num and ber. In the recent pre-trained language models of source code, e.g.,
    CuBERT (Kanade et al., [2020](#bib.bib116)) and CodeBERT (Feng et al., [2020](#bib.bib67)),
    BPE has commonly been adopted for reducing the vocabulary size. Karampatsis et al.
    ([2020](#bib.bib117)) conducted an empirical study on the granularity of word
    segmentation, and showed that tokenizing code by BPE can significantly reduce
    the vocabulary size.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Application Programming Interfaces (API)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There have been multiple methods proposed to analyze the API sequences in programs.
    One line of work is about mining API usage patterns from a large code corpus to
    demonstrate how to use an API. For example, Moreno et al. ([2015](#bib.bib169))
    proposed a novel approach, named Muse, to demonstrate API usage by mining and
    ranking the code examples in usage. Another line of work is API recommendation,
    which aims to recommend or generate a sequence of APIs for users. Jiang et al.
    ([2017](#bib.bib111)) proposed to discover relevant tutorial fragments for APIs
    by calculating the correlation score based on PageRank and topic relevance. Gu
    et al. ([2016](#bib.bib79)) proposed a language model named DeepAPI, under the
    framework of sequence-to-sequence learning, to produce API sequences in response
    to a given natural language description. Different from DeepAPI, Nguyen et al.
    ([2017](#bib.bib177)) proposed API2Vec to represent the contextual information
    of API elements within an API sequence. Likewise, they also developed a tool called
    API2API based on API2Vec to migrate the APIs across different programming languages,
    i.e., from Java to C#, to validate the learned API embedding. Ling et al. ([2021b](#bib.bib144))
    introduced a method that integrated API call interactions and project structure
    into a single graph, and used this graph to design a graph-based collaborative
    filtering for making API usage recommendations. Bui et al. ([2019b](#bib.bib30))
    proposed a cross-language API mapping approach to map APIs from Java to C# with
    much less prior knowledge, through transfer learning across multiple domains.
    Hu et al. ([2018b](#bib.bib104)) suggested that incorporating API information
    as supplementary knowledge could improve code summarization. To improve the representation
    of semantics in natural-language queries and API sequences, Wei et al. ([2022](#bib.bib255))
    proposed a contrastive learning approach for API recommendation, and Hadi et al.
    ([2022](#bib.bib91)) investigated the effectiveness of pre-trained models for
    generating API sequences from natural language queries.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. Abstract Syntax Tree (AST)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The AST is a tree-structured intermediate representation of code that describes
    the syntactic structure of a program. As shown in Figure [4](#S3.F4 "Figure 4
    ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") (d), in an AST, the leaf nodes (e.g., number,
    Fib) typically correspond to the tokens of variables and method names in the source
    code, while the non-leaf nodes (e.g., FuncName, SwitchStmt) represent the syntactic
    structure of code, like function definition, branch functions. As a result, this
    representation allows ASTs to be useful for both capturing the lexical information
    (e.g., variable number) and the syntactic structure of the source code. In practice,
    we can extract ASTs using several open source tools, e.g., ANTLR⁴⁴4[https://www.antlr.org](https://www.antlr.org)
    parser, tree-sitter⁵⁵5[https://tree-sitter.github.io/tree-sitter](https://tree-sitter.github.io/tree-sitter)
    parser, and LLVM Clang⁶⁶6[https://clang.llvm.org](https://clang.llvm.org). To
    represent the ASTs, Mou et al. ([2016](#bib.bib170)) proposed a tree structure-based
    CNN, and verified it in a code classification task. In order to handle long-distance
    dependencies between nodes in an AST, Liu et al. ([2020e](#bib.bib152)) proposed
    an improved LSTM by introducing operations such as PUSH and POP, and verified
    it in the tasks of code completion, code classification, and code summarization.
    To better process an AST, Zhang et al. ([2019](#bib.bib286)) divided an AST into
    sentence-based subtrees and represented them using a two-way loop network. Recently,
    Kim et al. ([2021](#bib.bib118)) proposed using a relative position embedding
    for code completion to feed the AST to Transformers. Niu et al. ([2022](#bib.bib179))
    introduced a pre-trained model of source code by integrating AST information.'
  prefs: []
  type: TYPE_NORMAL
- en: Another line of work (Hu et al., [2018a](#bib.bib103); Alon et al., [2019](#bib.bib14),
    [2018](#bib.bib12)) is to represent ASTs indirectly by traversing or path sampling.
    Hu et al. ([2018a](#bib.bib103)) suggested traversing an AST to transform it into
    a linear series of nodes, and then using RNNs to represent the AST sequences for
    the task of code summarization. Alon et al. ([2019](#bib.bib14)) performed path
    sampling on the ASTs, and then used word2vec to represent the semantics of a program.
    Furthermore, Alon et al. ([2018](#bib.bib12)) also applied a similar idea to the
    task of code summarization. Similarly, Alon et al. ([2020](#bib.bib13)) proposed
    a structured code language model for code completion, by sampling paths from an
    incomplete AST.
  prefs: []
  type: TYPE_NORMAL
- en: In program synthesis, an AST is also incorporated to guide the synthesis of
    programs. Yin and Neubig ([2017](#bib.bib277)) proposed an encoder-decoder framework
    for code generation, in which the encoder first encodes the natural language,
    then the decoder generates an AST of code, and finally, the AST is converted into
    source code. Chen et al. ([2018](#bib.bib44)) proposed a Tree2Tree model for program
    translation, which first uses a TreeLSTM to represent the source program, and
    another TreeLSTM to generate the target program written in another programming
    language.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4\. Intermediate Representation (IR)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The IR is a well-formed structure that is independent of programming languages
    and machine architectures. It is used by compilers to accurately represent the
    source code during the translation process from the source code to low-level machine
    code. The IR can express the operations of the target machine. It is natural to
    enhance the code embeddings via utilizing IRs (Li et al., [2022c](#bib.bib140)),
    with the benefit of limited vocabulary to significantly alleviate the OOV issue.
    In this paper, we employ LLVM-IR, which is used in the LLVM infrastructure (Lattner
    and Adve, [2004](#bib.bib120)), as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2\.
    Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit") (c). To represent IRs, Ben-Nun et al. ([2018](#bib.bib21))
    proposed inst2vec, which first compiles a program using LLVM Clang to obtain the
    LLVM intermediate representation, and then adopts skip-gram to represent the instructions.
    VenkataKeerthy et al. ([2020](#bib.bib233)) proposed IR2Vec, which regards the
    intermediate code representation as triples in the knowledge graph, and then explores
    several knowledge graph representation methods. Cummins et al. ([2021](#bib.bib55))
    introduced ProGraML, a novel graph-based code representation based on IR. This
    code graph provides new opportunities to represent the semantics of source code
    at a low level using machine learning techniques (e.g., GNNs), for complex downstream
    tasks such as program optimization and analysis. Peng et al. ([2021](#bib.bib186))
    proposed to represent the augmented IR of source code based on pre-training and
    contrastive learning techniques, guided by compiler optimization. Interestingly,
    Gui et al. ([2022](#bib.bib81)) studied a new problem of matching binary code
    and source code across languages by transforming both of them into LLVM-IRs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5\. Code Graphs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Currently, many approaches have been proposed to convert programs into graphs
    to better represent the rich structural information within the programs, including
    Control-Flow Graph (CFG), Data-Flow Graph (DFG) and Code Property Graph (CPG).
    As shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2\. Code Features ‣ 3\. Literature
    Review ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit")
    (e), the CFG represents the computation and control flow of a program. In this
    representation, each node represents a basic block and each edge represents the
    transitions of control flow in the program. As shown in Figure [4](#S3.F4 "Figure
    4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") (f), the DFG is a directed graph that illustrates
    data relationships among various functions. Each node in the DFG has input and
    output data ports, and each edge links an output port to an input port on another
    node. To represent multiple structural information of code using a joint data
    structure, Yamaguchi et al. ([2014](#bib.bib268)) proposed an innovative CPG to
    merge the structural information of code, including AST, CFG and Program Dependence
    Graph (PDG), into a single graph, as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2\.
    Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit") (g). In practice, we can build CFGs and DFGs using LLVM
    Clang, and build CPGs using Plume⁷⁷7[https://plume-oss.github.io/plume-docs](https://plume-oss.github.io/plume-docs).
    Recently, Cummins et al. ([2021](#bib.bib55)) built a unified graph, termed ProGraML,
    which includes the CFG, DFG and call-graph, as shown in Figure [4](#S3.F4 "Figure
    4 ‣ 3.2\. Code Features ‣ 3\. Literature Review ‣ Deep Learning for Code Intelligence:
    Survey, Benchmark and Toolkit") (h).'
  prefs: []
  type: TYPE_NORMAL
- en: To represent these code graphs, Allamanis et al. ([2018b](#bib.bib10)) introduced
    the data flow on the top of ASTs and formed a code graph. Then, a Gated Graph
    Neural Network (GGNN) (Li et al., [2016](#bib.bib134)) was developed to learn
    the data dependencies among this code graph. Allamanis and Brockschmidt ([2017](#bib.bib9))
    built the data flow among variables and considered the contextual information
    of variables for the task of automated pasting in programming. Brockschmidt et al.
    ([2019](#bib.bib25)) expanded the incomplete code into a graph, and then proposed
    a graph neural network for code completion. Sui et al. ([2020](#bib.bib214)) made
    the code representation more accurate by using the value-flow graph of a program.
    Shi et al. ([2022c](#bib.bib211)) resorted to converting the code graphs (e.g.,
    CFG and DFG) into sequences through traversing for the task of code search. Chen
    et al. ([2021a](#bib.bib45)) introduced a general method for transforming a code
    graph into a sequence of tokens and pointers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.6\. Other Features of Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the aforementioned features of code that have already been widely
    explored, there also exist several kinds of features that are used in some specific
    scenarios. For example, Henkel et al. ([2018](#bib.bib100)) introduced a novel
    feature for code representation learning based on abstractions of traces collected
    from the symbolic execution of a program. Hoang et al. ([2020](#bib.bib102)) proposed
    using deep learning to learn distributed representations of code changes/edits
    that may be used to generate software patches. In terms of code changes, several
    related works are also proposed to represent or predict them. Tufano et al. ([2019](#bib.bib228))
    proposed to automate code editing through sequence-to-sequence-based neural machine
    translation. Brody et al. ([2020](#bib.bib26)) proposed to represent the code
    edits first, and then iteratively generate tree edits over the AST.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.7\. Hybrid Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To leverage multiple code features, several approaches to representing source
    code in a hybrid fashion have been developed. For instance, Gu et al. ([2018](#bib.bib78))
    explored using three separate RNNs for representing function names, code tokens,
    as well as API sequences of code, respectively. It has also been evaluated in
    the code search task. White et al. ([2016](#bib.bib257)) considered both the code
    tokens and AST node sequences, and used two different RNNs to represent these
    two sequences respectively, for the task of code cloning detection. Zhao and Huang
    ([2018](#bib.bib290)) proposed to represent the source code by incorporating the
    flow graphs of code into a semantic matrix. They also developed a neural network
    model to assess the functional similarity between the representations of two code
    snippets. Similarly, Wan et al. ([2018](#bib.bib238)) and Wan et al. ([2019](#bib.bib235))
    developed a hybrid network consisting of an LSTM representing the code tokens,
    a GGNN representing the CFG of code, and a TreeLSTM representing the AST of code,
    for the task of code summarization and code search. Chakraborty and Ray ([2021](#bib.bib41))
    suggested leveraging three modalities of information (e.g., edit location, edit
    code context, and commit messages) to represent the context of programming and
    generate code patches automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Deep Learning Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We investigate the types of neural networks and classify the learning paradigms
    into four groups: supervised learning, unsupervised learning, self-supervised
    learning, and reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is natural to model source code as sequential text, and directly apply NLP
    techniques to represent it. Simply, RNN (Raychev et al., [2014](#bib.bib198);
    Liu et al., [2016b](#bib.bib149); Gu et al., [2018](#bib.bib78); Alon et al.,
    [2018](#bib.bib12); Hellendoorn et al., [2018](#bib.bib99); Malik et al., [2019](#bib.bib163);
    Svyatkovskiy et al., [2019](#bib.bib223); White et al., [2016](#bib.bib257); Hu
    et al., [2018a](#bib.bib103); Zhang et al., [2020c](#bib.bib285); Gupta et al.,
    [2017](#bib.bib90)) and CNN (Allamanis et al., [2016](#bib.bib11); Sun et al.,
    [2019](#bib.bib219)) neural networks can be easily applied to represent the sequential
    structure of source code. In order to capture the syntax structure, especially
    the AST of source code, many tree-structured neural networks (Wan et al., [2018](#bib.bib238);
    Mou et al., [2016](#bib.bib170); Chen et al., [2018](#bib.bib44)) have also been
    designed. Furthermore, to represent the semantic structures (e.g., CFG and DFG)
    of source code, GNNs (Allamanis et al., [2018b](#bib.bib10); Zhou et al., [2019](#bib.bib294);
    Wang and Li, [2021](#bib.bib248); Brockschmidt et al., [2019](#bib.bib25); Wei
    et al., [2020a](#bib.bib254); Allamanis et al., [2020](#bib.bib8); Liu et al.,
    [2020a](#bib.bib155)) have been introduced to represent the source code. Recently,
    the Transformer architecture has been utilized to represent the source code (Kim
    et al., [2021](#bib.bib118); Svyatkovskiy et al., [2020](#bib.bib221)). Chirkova
    and Troshin ([2021a](#bib.bib49)) conducted a comprehensive empirical study of
    how well Transformers can leverage syntactic information in source code for various
    tasks. More preliminaries about the mentioned neural networks are referred to
    the Supplementary Materials.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Supervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Supervised learning aims to learn a function that maps an input to an output
    based on a set of input-output pairs as training data. It is a widely used learning
    paradigm in deep learning. From our investigation, current deep learning approaches
    for code intelligence are mainly based on supervised learning. For each specific
    code intelligence task, such as code classification (Mou et al., [2016](#bib.bib170);
    Bui et al., [2019a](#bib.bib29)), vulnerability detection and bug finding (Li
    et al., [2018b](#bib.bib142); Zhou et al., [2019](#bib.bib294); Cheng et al.,
    [2021](#bib.bib47); Li et al., [2019](#bib.bib139)), code completion (Raychev
    et al., [2014](#bib.bib198); Liu et al., [2016b](#bib.bib149); Li et al., [2018a](#bib.bib129);
    Svyatkovskiy et al., [2019](#bib.bib223); Alon et al., [2020](#bib.bib13); Svyatkovskiy
    et al., [2020](#bib.bib221)), type inference (Hellendoorn et al., [2018](#bib.bib99);
    Malik et al., [2019](#bib.bib163); Wei et al., [2020a](#bib.bib254); Allamanis
    et al., [2020](#bib.bib8)), code search (Gu et al., [2018](#bib.bib78); Wan et al.,
    [2019](#bib.bib235); Haldar et al., [2020](#bib.bib92)), code clone detection (White
    et al., [2016](#bib.bib257); Wei and Li, [2017](#bib.bib253); Zhao and Huang,
    [2018](#bib.bib290); Wu et al., [2020](#bib.bib262); Zhang et al., [2019](#bib.bib286)),
    code summarization (Allamanis et al., [2016](#bib.bib11); Iyer et al., [2016](#bib.bib108);
    Hu et al., [2018a](#bib.bib103); Alon et al., [2018](#bib.bib12); Wan et al.,
    [2018](#bib.bib238)), program translation (Chen et al., [2018](#bib.bib44); Gu
    et al., [2017](#bib.bib80)), program synthesis (Dong and Lapata, [2016](#bib.bib65);
    Liu et al., [2016a](#bib.bib148); Sun et al., [2019](#bib.bib219); Zhong et al.,
    [2017](#bib.bib292); Cai et al., [2018](#bib.bib33)), and program repair (Gupta
    et al., [2017](#bib.bib90); Vasic et al., [2018](#bib.bib231); Dinella et al.,
    [2020](#bib.bib63); Chakraborty et al., [2020](#bib.bib40); Zhu et al., [2021](#bib.bib297);
    Tufano et al., [2018a](#bib.bib229); Li et al., [2020](#bib.bib135)), a set of
    paired input-output data is collected first. For each task, supervised learning
    is guided by a specific loss function. One limitation of this kind of approach
    is that it relies on lots of well-labeled input-output pairs, which are always
    expensive to collect in some scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Unsupervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As opposed to supervised learning, unsupervised learning seeks to identify patterns
    from a dataset without labels. One representative work is TransCoder (Rozière
    et al., [2020](#bib.bib200)), in which a fully unsupervised neural source-to-source
    translator is trained based on unsupervised machine translation. This kind of
    learning paradigm is challenging for code intelligence and more research work
    is still required.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4\. Self-Supervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Self-supervised learning can be thought of as a blend of supervised learning
    and unsupervised learning. Different from supervised learning where data labels
    are available for training, self-supervised learning obtains the supervisory signals
    directly from the data itself, usually the underlying structure of the data. One
    common practice used by self-supervised learning is to predict any unobserved
    (or masked) part of input from the part that can be observed. As a representative
    technique of self-supervised learning, language model pre-training has been widely
    studied in source code (Kanade et al., [2020](#bib.bib116); Feng et al., [2020](#bib.bib67);
    Guo et al., [2021](#bib.bib84)). Kanade et al. ([2020](#bib.bib116)) proposed
    to train a CuBERT on the Python code corpus, and verified the pre-trained model
    on multiple downstream tasks such as variable misuse, operator classification,
    and function-document matching. CodeBERT (Feng et al., [2020](#bib.bib67)) is
    yet another pre-trained model that deals with the two different modalities of
    source code and natural language descriptions. It is based on masked language
    modeling, and has achieved promising results in tasks such as code search and
    code completion. Based on CodeBERT, GraphCodeBERT (Guo et al., [2021](#bib.bib84)),
    SPT-Code (Niu et al., [2022](#bib.bib179)), and TreeBERT (Jiang et al., [2021b](#bib.bib114))
    are proposed to digest the structural information from source code. Lachaux et al.
    ([2021](#bib.bib119)) presented a pre-training objective based on deobfuscation
    as an alternative criterion. Inspired by BART (Lewis et al., [2020](#bib.bib125))
    which is a pre-trained deep model specially designed towards natural language
    understanding and generation, Ahmad et al. ([2021](#bib.bib4)) trained a similar
    pre-trained model PLBART for tasks that are related to code generation as well
    as code understanding. Zhang et al. ([2022a](#bib.bib284)) trained a model named
    CoditT5 on large amounts of source code and natural-language comments, for software-related
    editing tasks, e.g., comment updating, bug fixing, and automated code review.
    Wang et al. ([2021b](#bib.bib245)) and Guo et al. ([2022b](#bib.bib83)) proposed
    to train a model by unifying the modality of source code and natural language
    with contrastive learning, to improve the representation of the semantics of source
    code. Mastropaolo et al. ([2021](#bib.bib164)) and Wang et al. ([2021a](#bib.bib249))
    explored building pre-trained models based on the T5 (Text-To-Text Transfer Transformer)
    architecture, which has attained state-of-the-art results in NLP tasks. Bui et al.
    ([2021a](#bib.bib31)) proposed InferCode, a self-supervised learning method through
    predicting subtrees that are identified from the context of ASTs. Jain et al.
    ([2021](#bib.bib110)) proposed a contrastive learning approach for task-agnostic
    code representation based on program transformations in the compiler. ERNIE-Code (Chai
    et al., [2022a](#bib.bib38)) is a unified pre-trained model based on 116 natural
    languages and 6 programming languages, with the aim of bridging the gap between
    multilingual natural languages and multilingual programming languages. Given that
    LLMs are pre-trained on a dataset that may have a different distribution from
    the testing dataset, Wang et al. ([2022b](#bib.bib240)) explored the fine-tuning
    of pre-trained code models to facilitate adaptation to downstream tasks through
    curriculum learning.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of improving the capability of code embedding, Wan et al. ([2022c](#bib.bib237))
    investigated the explainability of pre-trained models for code intelligence, i.e.,
    what kind of information do these models capture, through structural analysis.
    Zhang et al. ([2022b](#bib.bib289)) and Shi et al. ([2022b](#bib.bib209)) suggested
    compressing pre-trained models of code, as to accelerate their efficiency in practice.
    Zhou et al. ([2021](#bib.bib293)) carried out an empirical study to assess the
    generalizability of CodeBERT when applied to various datasets and downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) of Code. The aforementioned pre-trained code models
    have demonstrated promising capabilities in comprehending the semantics of source
    code. More recently, with the remarkable performance achievements of LLMs in text
    generation and conversational dialogs, exemplified by the success of ChatGPT,
    a diverse array of LLMs has been specifically trained for code-related tasks,
    notably, code generation. Nijkamp et al. ([2022](#bib.bib178)) introduced a novel
    code generation task that enables users to progressively express their intentions
    through multi-turn interactions, and further trained a family of LLMs with up
    to 16.1 billion parameters, called CodeGen, for this task. PaLM (Chowdhery et al.,
    [2022](#bib.bib51)) is a general-purpose LLM developed by Google, which is pre-trained
    on a substantial dataset comprising both text and code corpora, boasting a vast
    parameter size of up to 540 billion. Derived from PaLM, PaLM-Coder is a model
    specifically fine-tuned for code-related tasks, such as code generation and program
    translation. InCoder (Fried et al., [2022](#bib.bib69)) is a LLM developed by
    Meta, which employs a causal masking objective for the purpose of infilling code
    blocks based on arbitrary left and right contexts. Pangu-Coder (Christopoulou
    et al., [2022](#bib.bib52)), introduced by Huawei, is a LLM specifically developed
    for code generation. Its training follows a two-stage strategy: in the first stage,
    it undergoes pre-training using Causal Language Modeling (CLM) on raw code corpora.
    In the second stage, it employs a combination of CLM and Masked Language Modeling
    (MLM) training objectives, with a focus on the downstream task of code generation
    from text. CodeGeeX (Zheng et al., [2023](#bib.bib291)) is a multilingual model
    with 13 billion parameters for code generation, pre-trained on 850 billion tokens
    of 23 programming languages. StarCoder (Li et al., [2023a](#bib.bib131)) is a
    LLM for code, up to 15.5 billion parameters, which is trained on an extensive
    dataset consisting of 1 trillion tokens sourced from a vast collection of permissively
    licensed GitHub repositories with inspection tools and an opt-out process. Code
    Llama (Roziere et al., [2023](#bib.bib199)) is a family of large language models
    for code, released by Meta, built upon the foundation of Llama 2. These models
    are distinguished by their advanced infilling capabilities, extensive long-context
    fine-tuning, and precise instruction fine-tuning. CodeT5+ (Wang et al., [2023](#bib.bib247)),
    released by Salesforce, represents a novel family of encoder-decoder-based LLMs
    explicitly tailored for a broad spectrum of tasks related to both code comprehension
    and code generation. This model introduces innovative pre-training objectives,
    including text-code contrastive learning, matching, and CLM tasks on text-code
    data. phi-1 (Gunasekar et al., [2023](#bib.bib82)) is a comparatively smaller
    LLM for code, consisting of 1.3 billion parameters, achieved through data set
    refinement, while maintaining competitive performance in code generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Different from traditional pre-trained code models that are designed for specific
    tasks, the LLMs for code are distinguished by their strong capabilities in zero-shot
    learning. To unleash the zero-shot capabilities of LLMs, many techniques such
    as prompt tuning, in-context learning, chain-of-thought, and instruction tuning,
    have been developed. Recently, numerous studies have explored the potential of
    LLMs in tasks such as code generation (Liu et al., [2023](#bib.bib147)), code
    summarization (Geng et al., [2024](#bib.bib74)), and code repair (Xia et al.,
    [2023](#bib.bib264)), all achieved through the design of textual prompts. As a
    specific prompting, in-context learning seeks to bolster the capabilities of LLMs
    by furnishing them with contextual information or illustrative examples. Li et al.
    ([2023c](#bib.bib127)) explored in-context learning for better code generation
    based on LLMs. The chain-of-thought is designed to ensure the outputs of LLMs
    follow a logical chain. Li et al. ([2023b](#bib.bib126)) explored chain-of-thought
    for better code generation based on LLMs. The instruction tuning is initially
    designed to enhance the generalization capabilities of LLMs across different tasks.
    WizardCoder (Luo et al., [2023](#bib.bib161)) is crafted to augment the capabilities
    of StarCoder by creating sophisticated code instruction data via the code-specific
    Evol-Instruct approach.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.5\. Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reinforcement learning aims to learn an agent through interacting with the environment
    without input-output pairs. This kind of learning paradigm has been used in code
    summarization (Wan et al., [2018](#bib.bib238)), code search (Yao et al., [2019](#bib.bib273)),
    program repair (Gupta et al., [2018](#bib.bib88)), and program synthesis (Zhong
    et al., [2017](#bib.bib292)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Classification-based Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classification-based applications, such as code categorization, vulnerability
    detection, and type inference, seek to train a classifier with the objective of
    mapping the source code to specific labels or classes, such as identifying vulnerability
    status or variable types.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1\. Code Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Classifying source code into different classes (e.g., different functionalities
    and programming languages), is important for many tasks such as code categorization,
    programming language identification, code prediction, and vulnerability detection.
    Various studies have been conducted to classify code snippets into categories
    based on their functionalities. To represent programs in the form of ASTs, Mou
    et al. ([2016](#bib.bib170)) developed a Tree-Based Convolutional Neural Network
    (TBCNN), which was then verified on code classification. In the wider realm of
    software categorization, LeClair et al. ([2018](#bib.bib122)) devised a series
    of adaptations, incorporating techniques such as word embedding and neural architectures,
    to tailor NLP methods for text classification specifically to the domain of source
    code. Bui et al. ([2019a](#bib.bib29)) presented a bilateral neural network for
    the cross-language algorithm classification task, where each sub-network is used
    to encode the semantics of code in a specific language, and an additional classification
    module is designed to model the connection of those bilateral programs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2\. Vulnerability Detection and Bug Finding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Detecting vulnerabilities or bugs in programs is essential for assuring the
    quality of software, as well as saves much effort and time for software development.
    Although many tools have been developed for vulnerability detection, e.g., Clang
    Static Analyzer⁸⁸8[https://clang-analyzer.llvm.org/scan-build.html](https://clang-analyzer.llvm.org/scan-build.html),
    Coverity⁹⁹9[https://scan.coverity.com](https://scan.coverity.com), Fortify^(10)^(10)10[https://www.hpfod.com](https://www.hpfod.com),
    Flawfinder^(11)^(11)11[https://dwheeler.com/flawfinder](https://dwheeler.com/flawfinder),
    Infer^(12)^(12)12[https://fbinfer.com](https://fbinfer.com), and SVF (Sui and
    Xue, [2016](#bib.bib215)), most of them are based on static analysis. Recently,
    a growing number of works employ deep learning to discover vulnerabilities. Wang
    et al. ([2016](#bib.bib243)) made an early attempt at applying deep learning,
    specifically deep belief network, to predict the defects of software, which learns
    the semantic features of programs based on AST. Dam et al. ([2018](#bib.bib58))
    proposed an LSTM-based method to exploit both the syntactic and semantic aspects
    of source code, and apply the embeddings for both within-project and cross-project
    vulnerability detection. VulDeePecker (Li et al., [2018b](#bib.bib142)), $\mu$VulDeePecker (Zou
    et al., [2019](#bib.bib299)) and SySeVR (Li et al., [2021d](#bib.bib141)) are
    a series of works that preserve the semantics of program by extracting API function
    calls and program slices for vulnerability detection. Le et al. ([2018](#bib.bib121))
    presented a maximal divergence sequential auto-encoder network to find vulnerabilities
    in binary files. The network is designed so that the embeddings of vulnerable
    code and invulnerable code are encouraged to be maximally divergent. Zhou et al.
    ([2019](#bib.bib294)) proposed Devign for vulnerability detection, which first
    represents a program by fusing its AST, CFG and DFG into a unified CPG, and then
    designs a graph neural network to represent the CPG of code. Similarly, Wang et al.
    ([2020c](#bib.bib241)) and Cao et al. ([2022](#bib.bib35)) proposed a flow-sensitive
    framework for vulnerability detection, which leverages a GNN to represent the
    control, data, and call dependencies of a program. Cheng et al. ([2021](#bib.bib47))
    introduced DeepWukong, a GNN-based model for vulnerability detection of C/C++
    programs, in which the flow information of programs is preserved. Liu et al. ([2021b](#bib.bib156))
    introduced a GNN model with expert knowledge for detecting vulnerabilities in
    smart contracts, which incorporates the flow information of programs. Inspired
    by image processing, Wu et al. ([2022b](#bib.bib263)) proposed a method to enhance
    the scalability of vulnerability detection by transforming code into an image
    with semantics preserved, and implementing a CNN to capture them effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, several works have attempted to explain the results of deep learning
    models for vulnerability detection. Li et al. ([2021c](#bib.bib137)) introduced
    a GNN model for vulnerability detection that allows for interpretability, by providing
    users with parts of the Program Dependency Graph (PDG) that may contain the vulnerability.
    Additionally, Zou et al. ([2021](#bib.bib300)) proposed an interpretable deep-learning-based
    model based on heuristic searching for vulnerability detection.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to vulnerability detection which only classifies a program as vulnerable
    or non-vulnerable, another line of work is bug finding, which aims to pinpoint
    the buggy location. DeepBugs (Pradel and Sen, [2018](#bib.bib190)) is an approach
    for name-based bug detection, which trains a classifier to distinguish buggy or
    non-buggy code, based on deep learning. To enhance the accuracy of bug detection,
    Li et al. ([2019](#bib.bib139)) suggested a fusion method by exploiting both the
    PDG and DFG for better representation. Larger weights are assigned to the buggy
    paths using the attention mechanism to identify the possible vulnerability. Gupta
    et al. ([2019](#bib.bib89)) developed a tree-structured CNN to identify the vulnerabilities
    or faults in a flawed program with respect to a failed test. Li et al. ([2021b](#bib.bib136))
    defined the fault localization problem as image recognition, and provided a deep-learning-based
    approach that integrates code coverage, data dependencies between statements,
    and source code representations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3\. Type Inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Programming languages with dynamic typing, like Python and JavaScript, allow
    for rapid prototyping for developers and can save the time of software development
    dramatically. However, without the type information, unexpected run-time errors
    are prone to occur, which may introduce bugs and produce low-quality code. Current
    works on type inference, with the aim of automatically inferring variable types,
    mainly fall into two categories: static-analysis-based and learning-based. Traditional
    static-analysis approaches (Hassan et al., [2018](#bib.bib96); Salib, [2004](#bib.bib202))
    are often imprecise since the behavior of programs is always over-approximated.
    In addition, static-analysis-based approaches typically analyze the dependencies
    of an entire program, resulting in relatively low efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, many deep learning techniques have been introduced for type inference.
    To the best of our knowledge, Hellendoorn et al. ([2018](#bib.bib99)) was the
    first to employ deep learning for type inference. They proposed a neural network
    based on sequence-to-sequence architecture, named DeepTyper, which uses GRUs to
    represent the program context and predict the type annotations for TypeScript.
    Furthermore, Malik et al. ([2019](#bib.bib163)) proposed NL2Type to predict type
    annotations by leveraging the natural-language information of programs. Based
    on NL2Type, Pradel et al. ([2020](#bib.bib189)) further proposed TypeWriter, which
    utilizes both the natural-language information and programming context (e.g.,
    arguments usage a function). Wei et al. ([2020a](#bib.bib254)) proposed LambdaNet
    for type inference based on GNNs, which first represents the code in the form
    of a type dependency graph, where typed variables and logical constraints among
    them are preserved. Then a GNN is proposed to propagate and aggregate features
    along related type variables, and eventually, predict the type annotations. Pandi
    et al. ([2020](#bib.bib183)) presented OptTyper, which first extracts relevant
    logical constraints, and shapes type inference as an optimization problem. Allamanis
    et al. ([2020](#bib.bib8)) proposed Typilus for type inference in Python, which
    expands ASTs into a graph structure and predicts type annotations over this graph
    using GNNs. To cope with large-scale type vocabulary, Mir et al. ([2022](#bib.bib168))
    presented Type4Py, a similarity-based deep learning model with type clusters,
    which can support the inference of rare types and user-defined classes. Recently,
    Huang et al. ([2022](#bib.bib106)) formulated the type inference task as a cloze-style
    fill-in-blank problem and then trained a CodeBERT model based on prompt tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Similarity-based Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similarity-based applications, such as code search and code clone detection,
    aim to assess the likeness between a query (in either natural language or programming
    language) and a candidate code snippet. It is important to note that several approaches
    propose to reframe these tasks as a classification problem, where both the code
    and query are concatenated, and the goal is to determine their relatedness (Feng
    et al., [2020](#bib.bib67)). In this paper, we differentiate between similarity-based
    and classification-based applications by the objects they address, namely, the
    query and candidate code snippet. Specifically, similarity-based applications
    center on tasks involving two objects.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1\. Code Search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Code search aims to retrieve a code snippet by a natural-language query (nl-to-code)
    or code query (code-to-code). The nl-to-code search refers to searching code fragments
    that have similar semantics to the natural-language query from a codebase. As
    the first solution for code search using deep learning, Gu et al. ([2018](#bib.bib78))
    proposed DeepCS, which simultaneously learns the source code representation (e.g.,
    function name, parameters and API usage) and the natural-language query in a shared
    feature vector space, with triplet criterion as the objective function. On the
    basis of DeepCS, Wan et al. ([2019](#bib.bib235)) and Deng et al. ([2022](#bib.bib59))
    included more structural information of source code, including the ASTs and CFGs,
    under a multi-modal neural network equipped with an attention mechanism for better
    explainability. Ling et al. ([2021a](#bib.bib146)) first converted code fragments
    and natural-language descriptions into two different graphs, and presented a matching
    technique for better source code and natural-language description matching. Furthermore,
    Shi et al. ([2022c](#bib.bib211)) suggested an improved code search method by
    converting code graphs (e.g., CFGs and PDGs) into sequences through traversing.
    Haldar et al. ([2020](#bib.bib92)) proposed a multi-perspective matching method
    to calculate the similarities among source code and natural-language query from
    multiple perspectives. Cambronero et al. ([2019](#bib.bib34)) empirically evaluated
    the architectures and training techniques when applying deep learning to code
    search. Bui et al. ([2021b](#bib.bib32)) and Li et al. ([2022b](#bib.bib132))
    leveraged contrastive learning with semantics-preserving code transformations
    for better code representation in code search.
  prefs: []
  type: TYPE_NORMAL
- en: Similar but different to the DeepCS framework, several more works have been
    proposed as complements for code search. Yao et al. ([2019](#bib.bib273)) proposed
    using reinforcement learning to first generate the summary of code snippet and
    then use the summary for better code search. Sun et al. ([2022a](#bib.bib216))
    suggested parsing source code to machine instructions, then mapping them into
    natural-language descriptions based on several predefined rules, followed by an
    LSTM-based code search model like DeepCS. Zhu et al. ([2020](#bib.bib296)) considered
    the overlapped substrings between natural-language query and source code, and
    developed a neural network component to represent the overlap matrix for code
    search.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Chai et al. ([2022b](#bib.bib39)) suggested a transfer learning method
    for domain-specific code search, with the aim of transferring knowledge from Python
    to SQL. Wan et al. ([2022b](#bib.bib236)) examined the robustness of different
    neural code search models, and showed that some of them are vulnerable to data-poisoning-based
    backdoor attacks. Gu et al. ([2022](#bib.bib77)) proposed to optimize code search
    by deep hashing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to nl-to-code search, the input of code-to-code search is source
    code, rather than natural-language description. The objective of the code-to-code
    search is to find code snippets that are semantically related to an input code
    from a codebase. The core technique of code-to-code search is to measure the similarity
    index between two code snippets, which is identical to the process of identifying
    code clones. More related work will be investigated in the code clone detection
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2\. Code Clone Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Numerous software engineering activities, including code reuse, vulnerability
    detection, and code search, rely on detecting similar code snippets (or code clones).
    There are basically four main types of code clones: Type-1 code clones are ones
    that are identical except for spaces, blanks, and comments. Type-2 code clones
    denote identical code snippets except for the variable, type, literal, and function
    names. Type-3 code clones denote two code snippets that are almost identical except
    for a few statements that have been added or removed. Type-4 code clones denote
    heterogeneous code snippets with similar functionality but differing code structures
    or syntax. To handle different types of code clones, various works have been proposed.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, several deep-learning-based approaches have been designed for the
    semantics representation of a pair of code snippets for the task of clone detection.
    The core of these approaches lies in representing the source code as distributed
    vectors, in which the semantics are preserved. As an example, White et al. ([2016](#bib.bib257))
    proposed DLC, which comprehends the semantics of source code by considering its
    lexical and syntactic information, and then designs RNNs for representation. To
    improve the representation of the syntactic structure of code, Wei and Li ([2017](#bib.bib253))
    applied TreeLSTM to incorporate AST information of source code. Zhao and Huang
    ([2018](#bib.bib290)) proposed encoding the CFG and DFG of code into a semantic
    matrix, and introduced a deep learning model to match similar code representations.
    Zhang et al. ([2019](#bib.bib286)) and Büch and Andrzejak ([2019](#bib.bib28))
    designed approaches to better represent the ASTs of the program, and applied them
    for code clone detection task. Furthermore, Wang et al. ([2020b](#bib.bib244)),
    Nair et al. ([2020](#bib.bib173)) and Mehrotra et al. ([2021](#bib.bib165)) proposed
    to convert source code into graphs (e.g., CFG), represent the code graphs via
    GNN, and then measure the similarities between them. Instead of using GNN, Wu
    et al. ([2020](#bib.bib262))and Hu et al. ([2022](#bib.bib105)) introduced a centrality
    analysis approach on the flow graph (e.g., CFG) of code for clone detection, inspired
    by social network analysis. Wu et al. ([2022a](#bib.bib260)) considered the nodes
    of an AST as distinct states and constructed a model based on Markov chain to
    convert the tree structure into Markov state transitions. Then, for code clone
    detection, a classifier model is trained on the state transitions. Tufano et al.
    ([2018b](#bib.bib230)) empirically evaluated the effectiveness of learning representation
    from diverse perspectives for code clone detection, including identifiers, ASTs,
    CFGs, and bytecode. Recently, Ding et al. ([2022](#bib.bib64)) and Tao et al.
    ([2022](#bib.bib225)) utilized program transformation techniques to augment the
    training data, and then applied pre-training and contrastive learning techniques
    for clone detection. Gui et al. ([2022](#bib.bib81)) studied a new problem of
    cross-language binary-source code matching by transforming both source and binary
    into LLVM-IRs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6\. Generation-based Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generation-based applications, including code completion, code summarization,
    program translation, program synthesis, and program repair, are designed to produce
    source code, natural-language descriptions, or programs in an alternative programming
    language, in response to specific requirements presented in either natural language
    or (partial) code.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1\. Code Completion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Code completion is a core feature of most modern IDEs. It offers the developers
    a list of possible code hints based on available information. Raychev et al. ([2014](#bib.bib198))
    made the first attempt to combine the program analysis with neural language models
    for better code completion. It first extracts the abstract histories of programs
    through program analysis, and then learns the probabilities of histories via an
    RNN-based neural language model. Similarly, various works (Liu et al., [2016b](#bib.bib149);
    Li et al., [2018a](#bib.bib129); Svyatkovskiy et al., [2019](#bib.bib223)) resort
    to inferring the next code token over the partial AST, by first traversing the
    AST in a depth-first order, and then introducing an RNN-based neural language
    model. To better represent the structure of code, Kim et al. ([2021](#bib.bib118))
    suggested predicting the missing partial code by feeding the ASTs to Transformers.
    Alon et al. ([2020](#bib.bib13)) presented a structural model for code completion,
    which represents code by sampling paths from an incomplete AST. Furthermore, Wang
    and Li ([2021](#bib.bib248)) suggested a GNN-based approach for code completion,
    which parses the flattened sequence of an AST into a graph, and represents it
    using Gated Graph Neural Networks (GGNNs) (Li et al., [2016](#bib.bib134)). Guo
    et al. ([2022c](#bib.bib85)) modeled the problem of code completion as filling
    in a hole, and developed a Transformer model guided by the grammar file of a specified
    programming language. Brockschmidt et al. ([2019](#bib.bib25)) expanded incomplete
    code into a graph representation, and then proposed a GNN for code completion.
    Svyatkovskiy et al. ([2020](#bib.bib221)) proposed IntelliCode Compose, a pre-trained
    language model of code based on GPT-2, providing instant code completion across
    different programming languages. Liu et al. ([2020b](#bib.bib150), [c](#bib.bib151))
    proposed a multi-task learning framework that unifies the code completion and
    type inference tasks into one overall framework. Lu et al. ([2022](#bib.bib159))
    suggested a retrieval-augmented code completion method that retrieves similar
    code snippets from a code corpus and then uses them as external context.
  prefs: []
  type: TYPE_NORMAL
- en: Since instant code completion is desired, several studies aim to improve the
    efficiency and flexibility of code completion. Svyatkovskiy et al. ([2021](#bib.bib222))
    suggested improving the efficiency of neural network models for code completion
    by reshaping the problem from generation to ranking the candidates from static
    analysis. Additionally, Shrivastava et al. ([2020](#bib.bib212)) proposed a code
    completion approach that supports fast adaption to an unseen file based on meta-learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2\. Code Summarization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by the text generation work in NLP, many approaches have been put forward
    to systematically generate a description or function name to summarize the semantics
    of source code. To the best of our knowledge, Allamanis et al. ([2016](#bib.bib11))
    were the first to use deep learning for code summarization. They designed a CNN
    to represent the code and applied a hybrid breath-first search and beam search
    to predict the tokens of function name. Concurrently, Iyer et al. ([2016](#bib.bib108))
    proposed an LSTM-based sequence-to-sequence network with an attention mechanism
    for generating descriptions for source code. The sequence-to-sequence network (Iyer
    et al., [2016](#bib.bib108)) inspired a line of works for code summarization,
    distinguished in code representation learning. To represent the AST information,
    Hu et al. ([2018a](#bib.bib103)), Alon et al. ([2018](#bib.bib12)), and LeClair
    et al. ([2019](#bib.bib124)) proposed to linearize the ASTs via traversing or
    path sampling, and used RNNs to represent the sequential AST traversals/paths
    for code summarization. Likewise, Fernandes et al. ([2019](#bib.bib68)), LeClair
    et al. ([2020](#bib.bib123)) and Jin et al. ([2022](#bib.bib115)) investigated
    representing the structure of source code via a GNN, and verified it in code summarization.
    Guo et al. ([2022a](#bib.bib86)) designed the triplet position to model hierarchies
    in the syntax structure of source code for better code summarization. Recently,
    several works (Ahmad et al., [2020](#bib.bib5); Wu et al., [2021](#bib.bib259);
    Gong et al., [2022](#bib.bib75); Tang et al., [2022](#bib.bib224)) proposed to
    improve code summarization by designing enhanced Transformers to better capture
    the structural information of code (i.e., ASTs). Wan et al. ([2018](#bib.bib238)),
    Shi et al. ([2021](#bib.bib208)), Yang et al. ([2021](#bib.bib271)), Gao and Lyu
    ([2022](#bib.bib71)), and Wang et al. ([2022a](#bib.bib246)) proposed a hybrid
    representation approach by combining the embeddings of sequential code tokens
    and structured ASTs, and feeding them into a decoder network to generate summaries.
    As a complement, Haque et al. ([2020](#bib.bib94)) and Bansal et al. ([2021](#bib.bib17))
    advanced the performance of code summarization by integrating the context of summarized
    code, which contains important hints for comprehending subroutines of code. Shahbazi
    et al. ([2021](#bib.bib206)) leveraged the API documentation as a knowledge resource
    for better code summarization. Instead of generating a sequence of summary tokens
    at once, Ciurumelea et al. ([2020](#bib.bib53)) resorted to suggesting code comment
    completions based on neural language modeling. Lin et al. ([2021](#bib.bib143))
    proposed to improve the code summarization by splitting the AST under the guidance
    of CFG, which can decrease the AST size and make model training easier.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of work aims to utilize code search to enhance the quality of code
    summaries generated by deep learning models. For example, Zhang et al. ([2020c](#bib.bib285)),
    Wei et al. ([2020b](#bib.bib252)), Liu et al. ([2020a](#bib.bib155)) and Li et al.
    ([2021a](#bib.bib128)) suggested augmenting the provided code snippet by searching
    similar source code snippets together with their comments, for better code summarization.
    Instead of acquiring the retrieved samples in advance, Zhu et al. ([2022](#bib.bib298))
    suggested a simple retrieval-based method for the task of code summarization,
    which estimates a probability distribution for generating each token given the
    current translation context.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the above approaches, several works (Hu et al., [2018b](#bib.bib104);
    Xie et al., [2022](#bib.bib265); Wei et al., [2019](#bib.bib251); Yang et al.,
    [2022a](#bib.bib269); Ye et al., [2020](#bib.bib275)) are also worthy to be mentioned.
    Hu et al. ([2018b](#bib.bib104)) transferred the code API information as additional
    knowledge to the code summarization task. Xie et al. ([2022](#bib.bib265)) studied
    a new task of project-specific code summarization with limited historical code
    summaries via meta-transfer learning. Wei et al. ([2019](#bib.bib251)) and Yang
    et al. ([2022a](#bib.bib269)) viewed the code generation task as a dual of code
    summarization, and incorporated dual learning for a better summary generation.
    Similarly, Ye et al. ([2020](#bib.bib275)) leveraged code generation for code
    search and code summarization through dual learning as well. Mu et al. ([2022](#bib.bib171))
    introduced a multi-pass deliberation framework for code summarization, inspired
    by human cognitive processes. Xie et al. ([2021](#bib.bib266)) proposed a multi-task
    learning framework by leveraging method name suggestion as an auxiliary task to
    improve code summarization. Haque et al. ([2021](#bib.bib93)) emphasized that
    predicting the action word (always the first word) is an important intermediate
    problem in order to generate improved code summaries. Recently, the consistency
    between source code and comments has also attracted much attention, which is critical
    to ensure the quality of software. Liu et al. ([2019](#bib.bib153)), Panthaplackel
    et al. ([2021](#bib.bib184)), and Nguyen et al. ([2020](#bib.bib176)) trained
    a deep-learning-based classifier to determine whether or not the function body
    and function name are consistent. Panthaplackel et al. ([2020](#bib.bib185)) and
    Liu et al. ([2020d](#bib.bib157)) proposed automatically updating an existing
    comment when the related code is modified, as revealed in the commit histories.
    Gao et al. ([2021](#bib.bib72)) proposed to automate the removal of obsolete TODO
    comments by representing the semantic features of TODO comments, code changes,
    and commit messages using neural networks. Li et al. ([2022e](#bib.bib130)) proposed
    to generate review comments automatically based on pre-trained code models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.3\. Program Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Translating programs from a deprecated programming language to a modern one
    is important for software maintenance. Many neural machine translation-based methods
    have been proposed for program translation. In order to utilize the AST structure
    of code, Chen et al. ([2018](#bib.bib44)) proposed Tree2Tree, a neural network
    with structural information preserved. It first converts ASTs into binary trees
    following the left-child right-sibling rule, and then feeds them into an encoder-decoder
    model equipped with TreeLSTM. Gu et al. ([2017](#bib.bib80)) presented DeepAM,
    which can extract API mappings among programming languages without the need of
    bilingual projects. Recently, Rozière et al. ([2020](#bib.bib200)) proposed TransCoder,
    a neural program translator based on unsupervised machine translation. Furthermore,
    Rozière et al. ([2022](#bib.bib201)) leveraged the automated unit tests to filter
    out invalid translations for unsupervised program translation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.4\. Program Synthesis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Program synthesis is a task for generating source code using high-level specifications
    (e.g., program descriptions or input-output samples). Given the natural-language
    inputs, current approaches resort to generating programs through machine translation.
    For semantic parsing, Dong and Lapata ([2016](#bib.bib65)) proposed an attention-based
    encoder-decoder model, which first encodes input natural language into a vector
    representation using an RNN, and then incorporates another tree-based RNN to generate
    programs. Liu et al. ([2016a](#bib.bib148)) proposed latent attention for the
    If-Then program synthesis, which can effectively learn the importance of words
    in natural-language descriptions. Beltagy and Quirk ([2016](#bib.bib20)) modeled
    the generation of If-Then programs from natural-language descriptions as a structure
    prediction problem, and investigated both neural network and logistic regression
    models for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike synthesizing simple If-Then programs, Yin and Neubig ([2017](#bib.bib277))
    proposed a syntax-preserving model for general-purpose programming languages,
    which generates Python code from pseudo code, powered by a grammar model that
    explicitly captures the compilation rules. Maddison and Tarlow ([2014](#bib.bib162))
    proposed a probabilistic model based on probabilistic context-free grammars (PCFGs)
    for capturing the structure of code for code generation. Ling et al. ([2016](#bib.bib145))
    collected two datasets (i.e., Hearthstone and Magic the Gathering) for code generation
    in trading card games, and proposed a probabilistic neural network with multiple
    predictors. On the basis of (Ling et al., [2016](#bib.bib145)), Rabinovich et al.
    ([2017](#bib.bib193)) proposed to incorporate the structural constraints on outputs
    into a decoder network for executable code generation. Similarly, Sun et al. ([2019](#bib.bib219))
    and Sun et al. ([2020](#bib.bib220)) designed a tree-based CNN and Transformer,
    respectively, for code generation and semantic parsing tasks based on the sequence-to-sequence
    framework. Hayati et al. ([2018](#bib.bib98)) suggested using a neural code generation
    model to retrieve action subtrees at test time.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of synthesizing programs from natural-language descriptions, several
    works resort to generating programs from the (pseudo) program in another format
    or language. Iyer et al. ([2018](#bib.bib109)) proposed to synthesize the AST
    derivation of source code given descriptions as well as the programmatic contexts.
    The above approaches are driven by well-labeled training examples, while Nan et al.
    ([2020](#bib.bib174)) proposed a novel approach to program synthesis without using
    any training example, inspired by how humans learn to program.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, various pre-trained code models also achieved significant progress
    in code generation. CodeGPT (Lu et al., [2021](#bib.bib160)) is a Transformer-based
    model that is trained using corpus for program synthesis, following the same architecture
    of GPT-2. CodeT5 (Wang et al., [2021a](#bib.bib249)) is a pre-trained code model
    in eight programming languages based on T5 (Raffel et al., [2020](#bib.bib194)),
    which incorporates an identifier-aware objective during its pre-training phase.
    Xu et al. ([2020](#bib.bib267)) endeavored to integrate external knowledge into
    the pre-training phase to enhance code generation from natural-language input.
    Codex (Chen et al., [2021b](#bib.bib43)) is a GPT model trained on a code corpus
    sourced from GitHub. This model has played a pivotal role as the underpinning
    framework for Copilot^(13)^(13)13[https://github.com/features/copilot](https://github.com/features/copilot).
    Li et al. ([2022a](#bib.bib133)) introduced AlphaCode, a code generation system
    designed to produce distinctive solutions for complex problems that demand profound
    cognitive engagement. Poesia et al. ([2022](#bib.bib187)) introduced a constrained
    semantic decoding mechanism into a pre-trained model, as to constrain outputs
    of the model in a set of valid programs. More recently, the code generation has
    been dominated by the LLMs, including CodeGen (Nijkamp et al., [2022](#bib.bib178)),
    CodeT5+ (Wang et al., [2023](#bib.bib247)), InCoder (Fried et al., [2022](#bib.bib69)),
    GPT-3.5 (OpenAI, [2022](#bib.bib181)), StarCoder (Li et al., [2023a](#bib.bib131)),
    Code Llama (Roziere et al., [2023](#bib.bib199)), and WizardCoder (Luo et al.,
    [2023](#bib.bib161)).
  prefs: []
  type: TYPE_NORMAL
- en: Programming by example is another flourishing direction for program synthesis.
    Shu and Zhang ([2017](#bib.bib213)) proposed a Neural Programming By Example (NPBE)
    model, which learns to solve string manipulation problems through inducting from
    input-output strings. Balog et al. ([2017](#bib.bib16)) proposed DeepCoder, which
    trains a model to predict possible functions useful in the program space, as to
    guide the conventional search-based synthesizer. Devlin et al. ([2017](#bib.bib62))
    proposed RobustFill, which is an end-to-end neural network for synthesising programs
    from input-output examples. Nye et al. ([2019](#bib.bib180)) developed a neuro-symbolic
    program synthesis system called SketchAdapt, which can build programs from input-output
    samples and code descriptions by intermediate sketch. Bavishi et al. ([2019](#bib.bib19))
    proposed a program candidate generator, backed by GNNs, for program synthesis
    in large real-world API.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that there are many works on generating code from natural
    language for specific domain-specific programming languages, e.g., Bash and SQL.
    WikiSQL (Zhong et al., [2017](#bib.bib292)), Spider (Yu et al., [2018b](#bib.bib280)),
    SparC (Yu et al., [2019b](#bib.bib281)), and CoSQL (Yu et al., [2019a](#bib.bib279))
    are four datasets with human annotations for the task of text-to-SQL. Based on
    these datasets, many works (Yu et al., [2018a](#bib.bib278), [2019b](#bib.bib281),
    [2019a](#bib.bib279)) have been proposed. For example, Seq2SQL (Zhong et al.,
    [2017](#bib.bib292)) is a neural machine translation model to generate SQL queries
    from natural-language descriptions with reinforcement learning. Cai et al. ([2018](#bib.bib33))
    further proposed an encoder-decoder framework to translate natural language into
    SQL queries, which integrates the grammar structure of SQL for better generation.
    Yu et al. ([2018a](#bib.bib278)) proposed a neural network SyntaxSQLNet, with
    syntax tree preserved, for the task of text-to-SQL translation across different
    domains, which takes the syntax tree of SQL into account during generation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.5\. Program Repair
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Automatically localizing and repairing bugs in programs can save much manual
    effort in software development (Jiang et al., [2019](#bib.bib112)). One line of
    work is to learn the patterns of how programmers edit the source code, which can
    be used to check syntax errors while compiling. Bhatia and Singh ([2016](#bib.bib23))
    and Santos et al. ([2018](#bib.bib203)) proposed RNN-based language models for
    correcting syntax errors in programs. DeepFix (Gupta et al., [2017](#bib.bib90))
    and SequenceR (Chen et al., [2019](#bib.bib46)) are two sequence-to-sequence models
    for syntax error correction, by translating the erroneous programs into fixed
    ones. Furthermore, Gupta et al. ([2018](#bib.bib88)) improved program repair by
    reinforcement learning. Vasic et al. ([2018](#bib.bib231)) proposed multi-headed
    pointer networks (one head each for localization and repair) for jointly localizing
    and repairing misused variables in code. Dinella et al. ([2020](#bib.bib63)) presented
    Hoppity to jointly detect and fix bugs based on neural Turing machine (Graves
    et al., [2014](#bib.bib76)), where a GNN-based memory unit is designed for buggy
    program representation, and an LSTM-based central controller is designed to predict
    the operations of bug fixing, e.g., patch generation and type prediction. Tarlow
    et al. ([2020](#bib.bib226)) proposed Graph2Diff, which designs a GNN for representing
    the graph structure of programs, and a pointer network to localize the initial
    AST to be edited. Mesbah et al. ([2019](#bib.bib166)) and Chakraborty et al. ([2020](#bib.bib40))
    proposed to model the modifications of ASTs, and designed a neural machine translation
    model to generate correct patches. Zhu et al. ([2021](#bib.bib297)) presented
    a syntax-directed decoder network with placeholder generation for program repair,
    which aims to generate program modifications rather than the target code. Yasunaga
    and Liang ([2020](#bib.bib274)) proposed DrRepair, which first builds a program-feedback
    graph to align the corresponding symbols and diagnostic feedback, and then designs
    a GNN to generate repaired code. Li et al. ([2022d](#bib.bib138)) introduced a
    novel deep learning-based method for fixing general bugs, which combines spectrum-based
    fault localization with deep learning and flow analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Benefiting from the pre-training techniques in NLP, TFix (Berabi et al., [2021](#bib.bib22))
    and VulRepair (Fu et al., [2022](#bib.bib70)) directly posed program repair as
    a text-to-text problem and utilized a model named T5 (Raffel et al., [2020](#bib.bib194)).
    Specifically, it digests the error message and directly outputs the correct code.
    Jiang et al. ([2021a](#bib.bib113)) proposed CURE for program repair, which is
    composed of a pre-trained language model, a code-aware search method, and a sub-word
    tokenization technique.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of work is focusing on repairing programs by generating patches.
    Tufano et al. ([2018a](#bib.bib229)) carried out an empirical study to evaluate
    the viability of applying machine translation to generate patches for program
    repair in real-world scenarios. Different from (Tufano et al., [2018a](#bib.bib229))
    which targets at function-level small code snippets, Hata et al. ([2018](#bib.bib97))
    trained a neural machine translation model, targeting at statements, by learning
    from the corresponding pre- and post-correction code in previous commits. Harer
    et al. ([2018](#bib.bib95)) proposed to generate the input buggy code via generative
    adversarial networks so that the correction model can be trained without labeled
    pairs. Gupta et al. ([2020](#bib.bib87)) embedded execution traces in order to
    predict a sequence of edits for repairing Karel programs. Li et al. ([2020](#bib.bib135))
    treated the program repair as code transformation and introduced two neural networks,
    a tree-based RNN for learning the context of a bug patch, and another one designed
    to learn the code transformation of fixing bugs. White et al. ([2019](#bib.bib256))
    introduced a novel approach for selecting and transforming program repair patches
    using deep-learning-based code similarities. Empirically, Tian et al. ([2020](#bib.bib227))
    studied the practicality of patch generation through representation learning of
    code changes.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though significant progress has been made in code intelligence with deep
    learning, two limitations remain obstacles to the development of this field. (1)
    Lack of standardized implementation for reproducing the results. It has become
    a common issue that deep-learning-based models are difficult to reproduce due
    to the sensitivity to data and hyperparameter tuning. From our investigation,
    most of them are implemented independently using different toolkits (i.e., PyTorch
    and TensorFlow). There is a need for a unified framework that enables developers
    to easily evaluate their models by utilizing some shared components. Actually,
    in the artificial intelligence area (e.g., NLP and computer vision), many toolkits
    such as Fairseq (Ott et al., [2019](#bib.bib182)), AllenNLP (Gardner et al., [2018](#bib.bib73)),
    Detectron2 (Wu et al., [2019](#bib.bib261)) have been developed, which significantly
    advance the progress of their corresponding research areas. (2) Lack of benchmarks
    for fair comparisons. Currently, many approaches have been proposed and each of
    them claims that the proposed approach has outperformed other ones. To identify
    where the performance improvements come from, it is essential to create a benchmark
    for fair comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these motivations, we propose NaturalCC (standards for Natural Code
    Comprehension), a thorough platform for evaluating source code models using deep
    learning techniques. Under this platform, we also benchmark four specific application
    tasks, including code summarization, code search, code completion, and type inference.
    The implementation and usage of NaturalCC will be introduced in Section [5](#S5
    "5\. Toolkit and Demonstration ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Code Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 2. Performance of our model and baseline methods for code summarization
    over Python-Doc dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BLEU | METEOR | ROUGE-L | Time Cost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Seq2Seq+Attn | 25.57 | 14.40 | 39.41 | 0.09s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| Tree2Seq+Attn | 23.35 | 12.59 | 36.49 | 0.48s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer | 30.64 | 17.65 | 44.59 | 0.26s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| PLBART | 32.71 | 18.13 | 46.05 | 0.26s/Batch |'
  prefs: []
  type: TYPE_TB
- en: 4.1.1\. Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Currently, most deep-learning-based code summarization methods use the encoder-decoder
    architecture. An encoder network is used to convert the input source code into
    an embedding vector, and the decoder network is used to generate output summaries
    from the encoded vector. In this paper, we benchmark the following representative
    methods for code summarization, including three different encoders (i.e., LSTM,
    TreeLSTM, and Transformer) as well as a pre-training-based model.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seq2Seq+Attn (Iyer et al., [2016](#bib.bib108); Wan et al., [2018](#bib.bib238))
    is a vanilla model following sequence-to-sequence architecture with attention
    mechanism. It is a famous method for neural machine translation. Unlike works
    that only represent the source code as token embedding (Iyer et al., [2016](#bib.bib108)),
    we represent the source code via an LSTM network and generate the summary via
    another LSTM network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree2Seq+Attn (Wan et al., [2018](#bib.bib238)) also follows the structure of
    Seq2Seq. The difference is that it uses TreeLSTM as the encoder network for syntax-aware
    modeling of code. Moreover, an attention module is also designed to attend to
    different nodes of the syntax tree of code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer (Ahmad et al., [2020](#bib.bib5)) is currently considered the leading
    approach for code summarization, which has also achieved significant improvement
    in neural machine translation. In Transformer, a relative position embedding,
    rather than absolute position embedding, is introduced for modeling the positions
    of code tokens.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PLBART (Ahmad et al., [2021](#bib.bib4)) is built on the top of BART (Lewis
    et al., [2020](#bib.bib125)), which is originally designed for text understanding
    and generation. PLBART can be seen as a specific BART model pre-trained on code
    corpus.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.1.2\. Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the performance of each model on the Python-Doc (Barone and Sennrich,
    [2017](#bib.bib18); Wan et al., [2018](#bib.bib238)) dataset using the BLEU, METEOR,
    and ROUGE metrics as in (Wan et al., [2018](#bib.bib238)). The overall performance
    is summarized in Table [2](#S4.T2 "Table 2 ‣ 4.1\. Code Summarization ‣ 4\. Benchmark
    ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit"). This table
    shows that PLBART, which utilizes the Transformer architecture and pre-training
    techniques, achieves the highest performance. It is interesting to see that the
    simple Seq2Seq+Attn outperforms the Tree2Seq+Attn that considers the AST of code.
    For Transformer, we find that the relative position embedding can indeed represent
    the relative relationships among code tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Code Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 3. MRR of our model and baseline methods for code search over CodeSearchNet
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Go | Java | JavaScript | PHP | Python | Ruby | Time Cost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| NBOW | 66.59 | 59.92 | 47.15 | 54.75 | 63.33 | 42.86 | 0.16s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| 1D-CNN | 70.87 | 60.49 | 38.81 | 61.92 | 67.29 | 36.53 | 0.30s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| biRNN | 65.80 | 48.60 | 23.23 | 51.36 | 48.28 | 19.35 | 0.74s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| SelfAtt | 78.45 | 66.55 | 50.38 | 65.78 | 79.09 | 47.96 | 0.25s/Batch |'
  prefs: []
  type: TYPE_TB
- en: 4.2.1\. Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CodeSearchNet Challenge (Husain et al., [2019](#bib.bib107)) is an open challenge
    designed to assess the current state of code search. In (Husain et al., [2019](#bib.bib107)),
    the authors have benchmarked four code search methods. The fundamental idea of (Husain
    et al., [2019](#bib.bib107)) is to learn a joint embedding of code and natural-language
    query in a shared vector space. That is, two encoders are used for representing
    the source code and query, respectively. A loss function is then designed to maximize
    the weighted sum for paired embeddings of source code and natural-language query.
    Based on different encoder networks, we have implemented the following four variant
    models.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural Bag of Words (NBOW) (Husain et al., [2019](#bib.bib107)) is a naive approach
    by representing the input sequences by a bag of words. For a given code snippet
    or some specified query written in natural language, it represents tokens into
    a collection of word embeddings before feeding them into a max pooling layer for
    creating a sentence-level representation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional RNN models (biRNN) (Husain et al., [2019](#bib.bib107)) proposes
    to represent the semantics of source code and query via RNN models. Specially,
    we adopt the two-layer bidirectional LSTM network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1D Convolutional Neural Network (1D-CNN) (Husain et al., [2019](#bib.bib107))
    employs convolutional neural layers for code and query representation, and builds
    a residual connection at each layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-Attention (SelfAtt) (Husain et al., [2019](#bib.bib107)) adopts self-attention
    layers to capture the semantic information of sequential source code and query.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2.2\. Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We employ word-level BPE to tokenize both code snippets and natural-language
    descriptions in the considered methods. Subsequently, a shared vocabulary of size
    $50,000$ is constructed based on the sorted token frequency. All models undergo
    training on a singular Nvidia RTX V100 GPU, utilizing a learning rate of $5\times
    10^{-4}$. The gradient norm is maintained at $1.0$, and a batch size of $1,000$
    is specified to expedite training. The optimization process for all models is
    executed using the Adam optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3\. Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the performance of each model on the CodeSearchNet corpus using
    the MRR metric, as described in (Husain et al., [2019](#bib.bib107)). The overall
    performance of each model is summarized in Table [3](#S4.T3 "Table 3 ‣ 4.2\. Code
    Search ‣ 4\. Benchmark ‣ Deep Learning for Code Intelligence: Survey, Benchmark
    and Toolkit"). As shown in the table, it is clear that the NBOW model with the
    simplest architecture achieves a comparable performance, at the lowest cost. Moreover,
    we can also observe that the performance of biRNN is poor, in both effectiveness
    and efficiency. The recurrent characteristic of RNN makes it time-consuming. The
    SelfAttn model obtains the best results, which may be attributed to its use of
    the self-attention mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Code Completion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 4. MRR of our model and baseline methods for code completion over Py150
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Attribute | Number | Identifier | Parameter | All Tokens | Time Cost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM | 51.67 | 47.45 | 46.52 | 66.06 | 73.73 | 0.31s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 | 70.37 | 62.20 | 63.84 | 73.54 | 82.17 | 0.43s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| TravTrans | 72.08 | 68.55 | 76.33 | 71.08 | 83.17 | 0.43s/Batch |'
  prefs: []
  type: TYPE_TB
- en: 4.3.1\. Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code completion task aims to generate the completion text based on the given
    partial code. In this paper, we investigate three representative approaches.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM (Kim et al., [2021](#bib.bib118)) denotes the model that represents the
    partial code by LSTM, and then predicts the missing token via a softmax layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-2 (Kim et al., [2021](#bib.bib118)) is a pre-trained language model based
    on Transformer. It refers to the Transformer model that is trained by iteratively
    predicting the next code token.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TravTrans (Kim et al., [2021](#bib.bib118)) is designed to preserve the syntax
    structure of source code while predicting the missing token. It first linearizes
    the code ASTs into a sequence of tokens using depth-first traversing, and afterward
    feeds the traversal into Transformer for representation. It also uses a softmax
    layer to predict the missing token.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3.2\. Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For acquiring high-quality code tokens, we perform preprocessing on the code
    snippets by parsing them into ASTs and extracting their leaf nodes as code tokens.
    We establish a unified vocabulary comprising $50,000$ tokens, organized based
    on token frequency. All models undergo training utilizing four Nvidia RTX V100
    GPUs, employing a learning rate of $1\times 10^{-3}$, and a batch size of $32$.
    The optimization of all models is executed using the Adam optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate each model on the Py150 (Raychev et al., [2016](#bib.bib197)) dataset
    using the MRR metric as used in (Kim et al., [2021](#bib.bib118)). We divide the
    prediction tokens into five categories, namely attributes, numeric constants,
    identifier names, function parameters and all tokens. We summarize the performance
    of each model in Table [4](#S4.T4 "Table 4 ‣ 4.3\. Code Completion ‣ 4\. Benchmark
    ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit"). From this
    table, when comparing GPT-2 with LSTM, we can observe that the Transformer architecture
    outperforms other models in representing the semantics of code, thus, resulting
    in better performance for code completion. Furthermore, when comparing TravTrans
    with GPT-2, we can see that the TravTrans that incorporates the syntax structure
    information achieves better performance, showing that the syntax information is
    useful for code completion.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Type Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 5. Accuracy of our model and baseline methods for type inference over
    Py150 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Accuracy@1 | Accuracy@5 | Accuracy@1 | Accuracy@5 | Time Cost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | All types | Any types |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DeepTyper | 0.52 | 0.67 | 0.43 | 0.67 | 0.42s/Batch |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer | 0.34 | 0.64 | 0.37 | 0.75 | 0.85s/Batch |'
  prefs: []
  type: TYPE_TB
- en: 4.4.1\. Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to code completion, the type inference task aims to predict the types
    of variables based on contextual information. It first represents the contextual
    code into a vector, and then predicts the missing types by a softmax layer. In
    our work, we employ two state-of-the-art methods for this task.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepTyper (Hellendoorn et al., [2018](#bib.bib99)) proposes to represent the
    contextual code by a two-layer biGRU, and then predicts the missing variable types
    via a softmax layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer (Ahmad et al., [2020](#bib.bib5)) proposes to represent the contextual
    code by a Transformer encoder network, and then predicts the missing variable
    types via a softmax layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4.2\. Implementation Details
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We initially tokenize both the code snippets and natural-language descriptions.
    Subsequently, we establish a common vocabulary comprising $40,000$ tokens, determined
    by sorting them based on frequency. The hardware configuration for training and
    the optimizer employed remains consistent with the aforementioned specifications.
    A batch size of $16$ and a learning rate of $1\times 10^{-4}$ are utilized.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3\. Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate each model on the Py150 (Raychev et al., [2016](#bib.bib197)),
    by using the Accuracy metric as in (Jain et al., [2021](#bib.bib110)). In particular,
    we measure the performance under the settings of all types and any types. The
    performance of different models is summarized in Table [5](#S4.T5 "Table 5 ‣ 4.4\.
    Type Inference ‣ 4\. Benchmark ‣ Deep Learning for Code Intelligence: Survey,
    Benchmark and Toolkit"). From this table, it is interesting to see that the simple
    LSTM-based DeepTyper outperforms the Transformer-based approach, especially under
    the all types setting, at a lower time cost.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Toolkit and Demonstration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section introduces the design of NaturalCC and its user interface. Figure [5](#S5.F5
    "Figure 5 ‣ 5.1\. Data Preprocessing Module ‣ 5\. Toolkit and Demonstration ‣
    Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit") (left) shows
    the code structure of NaturalCC. The dataset folder contains data preprocessing
    code. The ncc folder is the core module. The third_party folder holds model evaluation
    packages. The gui folder contains graphical user interface files and assets. As
    shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.1\. Data Preprocessing Module ‣ 5\. Toolkit
    and Demonstration ‣ Deep Learning for Code Intelligence: Survey, Benchmark and
    Toolkit") (right), NaturalCC is composed of four components, i.e., data preprocessing,
    code representation, downstream tasks, and their corresponding evaluations. At
    the stage of data preprocessing, we process the source code with a series of steps,
    including word tokenization, building vocabulary, and feature extraction. Additionally,
    a data loader is used to iteratively yield batches of code samples with their
    features. The resulting batches are then sent into the code representation models,
    which facilitate a variety of downstream tasks, including code summarization,
    code search, code completion, and type inference. To evaluate the performance
    of each task, we also implement several corresponding metrics that have been widely
    adopted previously.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Data Preprocessing Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In NaturalCC, we have collected and processed four datasets including CodeSearchNet (Husain
    et al., [2019](#bib.bib107)), Python-Doc (Wan et al., [2018](#bib.bib238)), Py150 (Raychev
    et al., [2016](#bib.bib197)), and DeepTyper (Hellendoorn et al., [2018](#bib.bib99)).
    First, we tokenize the input source code, and then build a vocabulary to map the
    code tokens into indexes. Currently, we support two types of tokenizations: space
    tokenizer and BPE tokenizer (Karampatsis et al., [2020](#bib.bib117)). Along with
    code tokens, we also explore different features of code, such as AST, IR, CFGs,
    and DFGs. All the related scripts for data preprocessing have been put in the
    data and dataset folders.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95a5275b210c31db91f96e05e2b62c5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5. The source code hierarchy and pipeline of NaturalCC.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Code Representation Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the core component of NaturalCC, we have implemented several encoders that
    are widely used in state-of-the-art approaches for source code representation,
    including RNN, GNN, and Transformer. For example, we have implemented LSTM, TreeLSTM
    and Transformer networks for sequential tokens and (linearized) ASTs. We have
    also implemented a GNN, i.e., GGNN, to represent the control-flow graph of source
    code. It is worth mentioning that in NaturalCC, we have also incorporated the
    pre-training approaches for source code. We have implemented several state-of-the-art
    pre-trained code models, including CodeBERT (Feng et al., [2020](#bib.bib67)),
    PLBART (Ahmad et al., [2021](#bib.bib4)), and GPT-2 (Lu et al., [2021](#bib.bib160)).
    The models and modules folders contain all the implemented networks for code representation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Tool Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NaturalCC is mainly implemented by PyTorch, and builds upon other successful
    open-source toolkits in NLP, such as Fairseq, and AllenNLP.
  prefs: []
  type: TYPE_NORMAL
- en: Registry Mechanism. To be flexible, NaturalCC is expected to be easily extended
    to different tasks and model implementations, with minimum modification. Similar
    to Fairseq, we design a register decorator on instantiating a new task or model,
    the implementation of which is in the corresponding __init__.py in each folder.
    The registry mechanism is to create a global variable to store all the available
    tasks, models, and objects at the initialization stage, so that users can easily
    access them throughout the whole project.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Training. NaturalCC supports efficient training of models in a distributed
    way through torch.distributed. It can utilize multiple GPUs across different servers.
    Furthermore, NaturalCC can support calculation in mixed precision to further increase
    the training speed, including both FP32 and FP16 training. Typically, the gradients
    are updated in FP16 while the parameters are saved in FP32.
  prefs: []
  type: TYPE_NORMAL
- en: Flexible Configuration. Instead of employing argparse for managing command-line
    options within Fairseq, we advocate the adoption of individual yaml configuration
    files for each model’s configuration. We contend that the flexibility offered
    by modifying these yaml configuration files is better suited for model exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Graphical User Interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also design a Web system as a graphical user interface to help users explore
    the results of trained models. The design is based on the open-source demonstration
    of AllenNLP (Gardner et al., [2018](#bib.bib73)). Figure [6(a)](#S5.F6.sf1 "In
    Figure 6 ‣ 5.5\. Leaderboard ‣ 5\. Toolkit and Demonstration ‣ Deep Learning for
    Code Intelligence: Survey, Benchmark and Toolkit") shows the screenshot of our
    demonstration system. Currently, we have implemented three tasks that are related
    to code intelligence, i.e., code summarization, code search, and code completion.
    We leave the integration of other related tasks to our future work.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5\. Leaderboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also develop a leaderboard so that researchers can report the results of
    their own models and compete with others, as shown in Figure [6(b)](#S5.F6.sf2
    "In Figure 6 ‣ 5.5\. Leaderboard ‣ 5\. Toolkit and Demonstration ‣ Deep Learning
    for Code Intelligence: Survey, Benchmark and Toolkit"). Currently, we only support
    researchers and developers who use NaturalCC to implement their approach and update
    the experimental results via pull requests in GitHub. In our future work, we will
    build a web-based service, which allows users to upload their predicted results
    and evaluate the model performance automatically using the ground-truth labels
    as a reference.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/19eccc44d99d16eb9b63a0e4af30d705.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Demonstration
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26b412cf3018d1a551daf07d7f231a41.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Leaderboard
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6. Screenshots of GUI and leaderboard of NaturalCC.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Challenges and Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although much effort has been made into deep learning for code intelligence,
    this area of research is still in its infancy with many open challenges and opportunities.
    To inspire future research, this section suggests several potential directions
    that are worth pursuing.
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensive Code Representation. Designing a representation approach to effectively
    and efficiently preserve the semantics of programs has always been a fundamental
    problem in code intelligence. Despite much effort on code representation, as mentioned
    in this paper, there are still three main obstacles to be overcome. (a) Open Vocabulary.
    Building a vocabulary to index the textual tokens of code is the first step toward
    applying deep learning models for code intelligence. Since the unambiguous characteristic
    of code, the vocabulary in code is much more open and complicated than the vocabulary
    in natural languages. The vocabulary of programming languages often consists of
    keywords, identifiers, customized method names, and variable names. The large
    vocabulary contains much “noise”, making it difficult to comprehend the code.
    Although many attempts (Cvitkovic et al., [2019](#bib.bib57); Karampatsis et al.,
    [2020](#bib.bib117); Chirkova and Troshin, [2021b](#bib.bib50)) have been made
    towards mitigating the OOV issue, it remains a challenge to design a simple yet
    effective approach to map the source code into indexes while preserving the semantics.
    (b) Complex Structure of Program. Unlike natural language, code is written with
    strict grammar. The computations described by code can be executed in an order
    that is different from the order in which the code was written. This is often
    seen in operations such as loops, recursions, and pointer manipulation. Although
    many attempts to capture the structure of code from different modalities, as we
    surveyed in this paper, we believe that the structures of code are not sufficiently
    preserved, and more effort is needed here. Inspired by the GNNs, there is potential
    to design specific GNNs to better represent the structure of programs. For example,
    from our analysis, ASTs, CFGs, DFGs and CPGs all have high heterogeneity. It is
    desirable to design some heterogeneous-information-network-based approaches (Sun
    et al., [2022b](#bib.bib217)) to represent the heterogeneous code graph. (c) Big
    Models of Code. Despite the significant progress made by pre-trained code models
    in code intelligence, pre-training on a large-scale code corpus is still computationally
    expensive and very costly. Recently, Zhang et al. ([2022b](#bib.bib289)) and Shi
    et al. ([2022b](#bib.bib209)) proposed to improve the efficiency of the training
    process by model compressing. It is a promising research direction to reduce the
    computational resources of pre-trained code models.
  prefs: []
  type: TYPE_NORMAL
- en: Data Hungry and Data Quality. Despite much progress achieved in deep-learning-based
    approaches for code intelligence, we argue that existing approaches still suffer
    from the data-hungry issue. In other words, the effectiveness of cutting-edge
    techniques significantly depends on the availability of vast quantities of expensive
    and labor-intensive well-labeled training data. Training the model on a small
    qualified dataset will result in far less imprecise results, especially for new
    programming languages or languages with an inadequate number of labeled samples.
    Therefore, it is important to design approaches to reduce the reliance on a large
    quantity of labeled data. A similar problem exists in the field of machine learning.
    One promising solution for this dilemma is transfer learning, which has achieved
    great success in transferring knowledge to alleviate the data-hungry issue in
    computer vision and NLP. Similarly, to model an emerging programming language
    with limited data, it is desirable to mitigate the data-hungry issue by leveraging
    models trained in programming languages with sufficient labeled training data (Chai
    et al., [2022b](#bib.bib39); Cui et al., [2022](#bib.bib54); Chen et al., [2022](#bib.bib42)).
    Data quality is also a crucial issue for code intelligence, which may exacerbate
    the data-hungry problem. From our analysis, the collected datasets from online
    resources, like GitHub and StackOverflow, are not quality ensured. Sun et al.
    ([2022c](#bib.bib218)) and Shi et al. ([2022a](#bib.bib210)) investigated the
    importance of data quality and verified it on the tasks of code search and code
    summarization, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Lingual and Cross-Language. The codebase written in multiple programming
    languages is can be considered a multi-lingual corpus, as in NLP. However, the
    multi-lingual problem in programming languages has not been well investigated.
    Different from the multi-lingual problems studied in NLP, the corpus of multiple
    programming languages will bring more opportunities and challenges to future research.
    Recently, several attempts have been made to learn the common knowledge shared
    among multiple programming languages, and transfer the knowledge across different
    programming languages. For example, Zhang et al. ([2021b](#bib.bib283)) proposed
    obtaining better interpretability and generalizability by disentangling the semantics
    of source code from multiple programming languages based on variational autoencoders.
    Zügner et al. ([2021](#bib.bib301)) introduced a language-agnostic code representation
    based on the features directly extracted from the AST. Ahmed and Devanbu ([2022](#bib.bib6))
    conducted an exploratory study and revealed the evidence that multilingual property
    indeed exists in the source code corpora. For example, it is more likely that
    programs that solve the same problem in different languages make use of the same
    or similar identifier names. They also investigate the effect of multilingual
    (pre-)training for code summarization and code search. Nafi et al. ([2019](#bib.bib172))
    proposed CLCDSA, a cross-language clone detector with syntactical features and
    API documentation. Bui et al. ([2019a](#bib.bib29)) proposed a bilateral neural
    network for the task of cross-language algorithm classification. Bui et al. ([2019b](#bib.bib30))
    proposed SAR, which can learn cross-language API mappings with minimal knowledge.
    Recently, Chai et al. ([2022b](#bib.bib39)) proposed a novel approach termed CDCS
    for domain-specific code search through transfer learning across programming languages.
    Gui et al. ([2022](#bib.bib81)) proposed an approach that matches source code
    and binary code across different languages based on intermediate representation.
  prefs: []
  type: TYPE_NORMAL
- en: Model Interpretability. Lack of interpretability is a common challenge for most
    deep learning-based techniques for code intelligence, as deep learning is a black-box
    method. New methods and studies on interpreting the working mechanisms of deep
    neural networks should be a potential research direction. Recently, several efforts
    have been made toward increasing the interpretability of deep-learning-based models.
    As an example, Li et al. ([2021c](#bib.bib137)) presented a novel approach to
    explain predicted results for GNN-based vulnerability detection by extracting
    sub-graphs in the program dependency graph. In addition, Zou et al. ([2021](#bib.bib300))
    proposed interpreting a deep-learning-based model for vulnerability detection
    by identifying a limited number of tokens that play a significant role in the
    final prediction of the detectors. Zhang et al. ([2021a](#bib.bib287)) proposed
    interpretable program synthesis that allows users to see the synthesis process
    and have control over the synthesizer. Pornprasit et al. ([2021](#bib.bib188))
    proposed a local rule-based model-agnostic approach, termed PyExplainer, to explain
    the predictions of just-in-time defect models. Rabin et al. ([2021](#bib.bib192))
    proposed a model-agnostic explainer based on program simplification, inspired
    by the delta debugging algorithms. Wan et al. ([2022c](#bib.bib237)), López et al.
    ([2022](#bib.bib158)), and Sharma et al. ([2022](#bib.bib207)) investigated the
    explainability of pre-trained code models through probing the code attention and
    hidden representations. We believe that it is essential to enhance the interpretability
    of current deep-learning-based approaches for code intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness and Security. Despite significant progress being made in the training
    of accurate models for code intelligence, the robustness and security of these
    models have rarely been explored. As seen in the fields of NLP and CV, deep neural
    networks are frequently not robust (Carlini and Wagner, [2017](#bib.bib37)). Specifically,
    current deep learning models can be easily deceived by adversarial examples, which
    are created by making small changes to the inputs of the model that it would consider
    as benign. There are many different ways to produce adversarial samples in the
    computer vision and NLP communities, particularly for image classification (Eykholt
    et al., [2018](#bib.bib66); Carlini and Wagner, [2017](#bib.bib37); Carlini et al.,
    [2019](#bib.bib36)) and sentiment classification (Zhang et al., [2020b](#bib.bib288)).
    Similarly, for source code models, the adversarial attack also exists. Recently,
    there have been several efforts to investigate the robustness and security of
    deep-learning-based models for code intelligence. For example, Ramakrishnan et al.
    ([2020](#bib.bib196)) and Yefet et al. ([2020](#bib.bib276)) investigated how
    to improve the robustness of source code models through adversarial training.
    Nguyen et al. ([2021](#bib.bib175)) empirically investigated the use of adversarial
    learning techniques for API recommendation. Bielik and Vechev ([2020](#bib.bib24))
    introduced a novel method that incorporates adversarial training and representation
    refinement to create precise and robust models of source code. Zhou et al. ([2022](#bib.bib295)),
    Yang et al. ([2022b](#bib.bib272)) and Zhang et al. ([2020a](#bib.bib282)) proposed
    a black-box attack for neural code models by generating adversarial examples while
    preserving the semantics of source code. Based on semantics-preserving code transformations,
    Quiring et al. ([2019](#bib.bib191)) and Liu et al. ([2021a](#bib.bib154)) developed
    a novel attack against authorship attribution of source code. Ramakrishnan and
    Albarghouthi ([2022](#bib.bib195)) investigated the possibility of injecting a
    number of common backdoors into deep-learning-based models, and developed a protection
    approach based on spectral signatures. Schuster et al. ([2021](#bib.bib204)) and
    Wan et al. ([2022b](#bib.bib236)) proposed attacking the neural code models through
    data poisoning, and verified it in code completion and code search, respectively.
    Severi et al. ([2021](#bib.bib205)) suggested an explanation-guided backdoor approach
    to attack the malware classifiers. Overall, exploring the robustness and security
    of code intelligence models is an interesting and important research direction.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we study deep learning for code intelligence by conducting a
    comprehensive survey, establishing a benchmark, as well as developing an open-source
    toolkit. We begin by providing a thorough literature review on deep learning for
    code intelligence, from the perspectives of code representations, deep learning
    techniques, application tasks, and public datasets. We then present an open-source
    toolkit for code intelligence, termed NaturalCC. On top of NaturalCC, we have
    benchmarked four popular application tasks about code intelligence, i.e., code
    summarization, code search, code completion, and type inference. We hope that
    our study contributes to a better understanding of the current status of code
    intelligence. We also hope that our toolkit and benchmark will contribute to the
    development of better code intelligence models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: git (2019) 2019. GitHub. [https://www.github.com](https://www.github.com). [Online;
    accessed 1-May-2019].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sta (2019) 2019. StackOverflow. [https://www.stackoverflow.com](https://www.stackoverflow.com).
    [Online; accessed 1-May-2019].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmad et al. (2021) Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei
    Chang. 2021. Unified Pre-training for Program Understanding and Generation. In
    *NAACL*. 2655–2668.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmad et al. (2020) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
    Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization.
    In *ACL*. 4998–5007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmed and Devanbu (2022) Toufique Ahmed and Premkumar Devanbu. 2022. Multilingual
    training for Software Engineering. In *ICSE*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allamanis et al. (2018a) Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,
    and Charles Sutton. 2018a. A survey of machine learning for big code and naturalness.
    *ACM Computing Surveys (CSUR)* 51, 4 (2018), 1–37.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Allamanis et al. (2020) Miltiadis Allamanis, Earl T Barr, Soline Ducousso,
    and Zheng Gao. 2020. Typilus: neural type hints. In *PLDI*. 91–105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Allamanis and Brockschmidt (2017) Miltiadis Allamanis and Marc Brockschmidt.
    2017. Smartpaste: Learning to adapt source code. *arXiv:1705.07867* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allamanis et al. (2018b) Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud
    Khademi. 2018b. Learning to Represent Programs with Graphs. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allamanis et al. (2016) Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016.
    A convolutional attention network for extreme summarization of source code. In
    *ICML*. 2091–2100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alon et al. (2018) Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018.
    code2seq: Generating Sequences from Structured Representations of Code. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alon et al. (2020) Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural
    language models of code. In *ICML*. 245–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alon et al. (2019) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2019. code2vec: Learning distributed representations of code. *POPL* 3 (2019),
    1–29.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andreessen (2011) Marc Andreessen. 2011. Why software is eating the world. *Wall
    Street Journal* 20, 2011 (2011), C2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balog et al. (2017) Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian
    Nowozin, and Daniel Tarlow. 2017. DeepCoder: Learning to Write Programs. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bansal et al. (2021) Aakash Bansal, Sakib Haque, and Collin McMillan. 2021.
    Project-Level Encoding for Neural Source Code Summarization of Subroutines. In
    *ICPC*. IEEE, 253–264.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barone and Sennrich (2017) Antonio Valerio Miceli Barone and Rico Sennrich.
    2017. A Parallel Corpus of Python Functions and Documentation Strings for Automated
    Code Documentation and Code Generation. In *IJCNLP*. 314–319.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bavishi et al. (2019) Rohan Bavishi, Caroline Lemieux, Roy Fox, Koushik Sen,
    and Ion Stoica. 2019. AutoPandas: neural-backed generators for program synthesis.
    *OOPSLA* 3 (2019), 1–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beltagy and Quirk (2016) Islam Beltagy and Chris Quirk. 2016. Improved semantic
    parsers for if-then statements. In *ACL*. 726–736.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ben-Nun et al. (2018) Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler.
    2018. Neural Code Comprehension: A Learnable Representation of Code Semantics.
    In *NeurIPS*. 3589–3601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Berabi et al. (2021) Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin
    Vechev. 2021. TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer.
    In *ICML*. 780–791.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhatia and Singh (2016) Sahil Bhatia and Rishabh Singh. 2016. Automated correction
    for syntax errors in programming assignments using recurrent neural networks.
    *arXiv:1603.06129* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bielik and Vechev (2020) Pavol Bielik and Martin Vechev. 2020. Adversarial robustness
    for code. In *ICML*. 896–907.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brockschmidt et al. (2019) Marc Brockschmidt, Miltiadis Allamanis, Alexander L.
    Gaunt, and Oleksandr Polozov. 2019. Generative Code Modeling with Graphs. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brody et al. (2020) Shaked Brody, Uri Alon, and Eran Yahav. 2020. A structural
    model for contextual code changes. *OOPSLA* 4 (2020), 1–28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, et al. 2020. Language models are few-shot learners.
    *NeurIPS* 33 (2020), 1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Büch and Andrzejak (2019) Lutz Büch and Artur Andrzejak. 2019. Learning-based
    recursive aggregation of abstract syntax trees for code clone detection. In *SANER*.
    95–104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bui et al. (2019a) Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2019a. Bilateral
    dependency neural networks for cross-language algorithm classification. In *SANER*.
    422–433.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bui et al. (2019b) Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2019b. SAR: learning
    cross-language API mappings with little knowledge. In *ESEC/FSE*. 796–806.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bui et al. (2021a) Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2021a. InferCode:
    Self-Supervised Learning of Code Representations by Predicting Subtrees. In *ICSE*.
    1186–1197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bui et al. (2021b) Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021b. Self-Supervised
    Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving
    Transformations. In *SIGIR*. ACM, 511–521.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2018) Ruichu Cai, Boyan Xu, Zhenjie Zhang, Xiaoyan Yang, Zijian
    Li, and Zhihao Liang. 2018. An Encoder-Decoder Framework Translating Natural Language
    to Database Queries. In *IJCAI*. 3977–3983.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cambronero et al. (2019) Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen,
    and Satish Chandra. 2019. When deep learning met code search. In *ESEC/FSE*. 964–974.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2022) Sicong Cao, Xiaobing Sun, Lili Bo, Rongxin Wu, Bin Li, and
    Chuanqi Tao. 2022. MVD: Memory-Related Vulnerability Detection Based on Flow-Sensitive
    Graph Neural Networks. In *ICSE*. 1456–1468.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini et al. (2019) Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland
    Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and
    Alexey Kurakin. 2019. On evaluating adversarial robustness. *arXiv:1902.06705*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini and Wagner (2017) Nicholas Carlini and David Wagner. 2017. Towards evaluating
    the robustness of neural networks. In *S&P*. 39–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chai et al. (2022a) Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian,
    and Hua Wu. 2022a. ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining
    for Programming Languages. *arXiv preprint arXiv:2212.06742* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chai et al. (2022b) Yitian Chai, Hongyu Zhang, Beijun Shen, and Xiaodong Gu.
    2022b. Cross-Domain Deep Code Search with Meta Learning. In *ICSE*. 487–498.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chakraborty et al. (2020) Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis,
    and Baishakhi Ray. 2020. Codit: Code editing with tree-based neural models. *TSE*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chakraborty and Ray (2021) Saikat Chakraborty and Baishakhi Ray. 2021. On Multi-Modal
    Learning of Editing Source Code. In *ASE*. IEEE, 443–455.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022) Fuxiang Chen, Fatemeh H. Fard, David Lo, and Timofey Bryksin.
    2022. On the transferability of pre-trained language models for low-resource programming
    languages. In *ICPC*. ACM, 401–412.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, et al. 2021b. Evaluating
    large language models trained on code. *arXiv preprint arXiv:2107.03374* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Xinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree
    Neural Networks for Program Translation. In *NeurIPS*. 2552–2562.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021a) Zimin Chen, Vincent Hellendoorn, Pascal Lamblin, Petros
    Maniatis, Pierre-Antoine Manzagol, et al. 2021a. PLUR: A Unifying, Graph-Based
    View of Program Learning, Understanding, and Repair. *NeurIPS* 34 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019) Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël
    Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2019. Sequencer: Sequence-to-sequence
    learning for end-to-end program repair. *TSE* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2021) Xiao Cheng, Haoyu Wang, Jiayi Hua, Guoai Xu, and Yulei
    Sui. 2021. DeepWukong: Statically detecting software vulnerabilities using deep
    graph neural network. *TOSEM* 30, 3 (2021), 1–33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chicco (2021) Davide Chicco. 2021. Siamese neural networks: An overview. *Artificial
    Neural Networks* (2021), 73–94.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chirkova and Troshin (2021a) Nadezhda Chirkova and Sergey Troshin. 2021a. Empirical
    study of transformers for source code. In *ESEC/FSE*. 703–715.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chirkova and Troshin (2021b) Nadezhda Chirkova and Sergey Troshin. 2021b. A
    Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for
    Source Code. In *NAACL*. 278–288.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Christopoulou et al. (2022) Fenia Christopoulou, Gerasimos Lampouras, Milan
    Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin
    Li, et al. 2022. Pangu-coder: Program synthesis with function-level language modeling.
    *arXiv preprint arXiv:2207.11280* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ciurumelea et al. (2020) Adelina Ciurumelea, Sebastian Proksch, and Harald C
    Gall. 2020. Suggesting comment completions for python using neural language models.
    In *SANER*. 456–467.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui et al. (2022) Nan Cui, Yuze Jiang, Xiaodong Gu, and Beijun Shen. 2022. Zero-shot
    program representation learning. In *ICPC*. ACM, 60–70.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cummins et al. (2021) Chris Cummins, Zacharias Fisches, Tal Ben-Nun, Torsten
    Hoefler, Michael O’Boyle, and Hugh Leather. 2021. ProGraML: A Graph-based Program
    Representation for Data Flow Analysis and Compiler Optimizations. In *ICML*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cummins et al. (2017) Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh
    Leather. 2017. Synthesizing benchmarks for predictive modeling. In *CGO*. 86–99.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cvitkovic et al. (2019) Milan Cvitkovic, Badal Singh, and Animashree Anandkumar.
    2019. Open vocabulary learning on source code with a graph-structured cache. In
    *ICML*. 1475–1485.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dam et al. (2018) Hoa Khanh Dam, Truyen Tran, Trang Thi Minh Pham, Shien Wee
    Ng, John Grundy, and Aditya Ghose. 2018. Automatic feature learning for predicting
    vulnerable software components. *TSE* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2022) Zhongyang Deng, Ling Xu, Chao Liu, Meng Yan, Zhou Xu, and
    Yan Lei. 2022. Fine-grained Co-Attentive Representation Learning for Semantic
    Code Search. In *SANER*. 396–407.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devanbu et al. (2020) Prem Devanbu, Matthew B. Dwyer, Sebastian G. Elbaum,
    Michael Lowry, Kevin Moran, et al. 2020. Deep Learning & Software Engineering:
    State of Research and Future Directions. *CoRR* abs/2009.08525 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *NAACL*. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2017) Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh
    Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017. Robustfill: Neural program
    learning under noisy i/o. In *ICML*. 990–998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dinella et al. (2020) Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik,
    Le Song, and Ke Wang. 2020. Hoppity: Learning graph transformations to detect
    and fix bugs in programs. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2022) Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari,
    Baishakhi Ray, and Saikat Chakraborty. 2022. Towards Learning (Dis)-Similarity
    of Source Code from Program Contrasts. In *ACL*. 6300–6312.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong and Lapata (2016) Li Dong and Mirella Lapata. 2016. Language to Logical
    Form with Neural Attention. In *ACL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eykholt et al. (2018) Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li,
    Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018.
    Robust physical-world attacks on deep learning visual classification. In *CVPR*.
    1625–1634.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
    Feng, et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages.
    In *Findings of EMNLP*. 1536–1547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernandes et al. (2019) Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt.
    2019. Structured Neural Summarization. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
    2022. Incoder: A generative model for code infilling and synthesis. *arXiv preprint
    arXiv:2204.05999* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2022) Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen,
    and Dinh Q. Phung. 2022. VulRepair: a T5-based automated software vulnerability
    repair. In *ESEC/FSE*. 935–947.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao and Lyu (2022) Yuexiu Gao and Chen Lyu. 2022. M2TS: multi-scale multi-modal
    approach based on transformer for source code summarization. In *ICPC*. ACM, 24–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021) Zhipeng Gao, Xin Xia, David Lo, John Grundy, and Thomas Zimmermann.
    2021. Automating the removal of obsolete TODO comments. In *ESEC/FSE*. 218–229.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gardner et al. (2018) Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord,
    et al. 2018. AllenNLP: A Deep Semantic Natural Language Processing Platform. In
    *Proceedings of Workshop for NLP Open Source Software (NLP-OSS)*. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng et al. (2024) Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang,
    Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao. 2024. Large Language Models are
    Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning.
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2022) Zi Gong, Cuiyun Gao, Yasheng Wang, Wenchao Gu, Yun Peng,
    and Zenglin Xu. 2022. Source Code Summarization with Structural Relative Position
    Guided Transformer. In *SANER*. 13–24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves et al. (2014) Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural
    turing machines. *arXiv:1410.5401* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2022) Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei
    Zhang, and Michael R. Lyu. 2022. Accelerating Code Search with Deep Hashing and
    Code Classification. In *ACL*. 2534–2544.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2018) Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code
    search. In *ICSE*. 933–944.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2016) Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim.
    2016. Deep API learning. In *Proceedings of the 2016 24th ACM SIGSOFT International
    Symposium on Foundations of Software Engineering*. 631–642.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2017) Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim.
    2017. DeepAM: Migrate APIs with Multi-Modal Sequence to Sequence Learning. In
    *IJCAI*. 3675–3681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gui et al. (2022) Yi Gui, Yao Wan, Hongyu Zhang, Huifang Huang, Yulei Sui, Guandong
    Xu, Zhiyuan Shao, and Hai Jin. 2022. Cross-Language Binary-Source Code Matching
    with Intermediate Representations. In *SANER*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. *arXiv preprint
    arXiv:2306.11644* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2022b) Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and
    Jian Yin. 2022b. UniXcoder: Unified Cross-Modal Pre-training for Code Representation.
    In *ACL*. 7212–7225.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2021) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    Liu, Long Zhou, et al. 2021. GraphCodeBERT: Pre-training Code Representations
    with Data Flow. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2022c) Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc Brockschmidt,
    and Miltiadis Allamanis. 2022c. Learning to Complete Code with Sketches. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2022a) Juncai Guo, Jin Liu, Yao Wan, Li Li, and Pingyi Zhou. 2022a.
    Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization.
    In *ACL*. 486–500.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2020) Kavi Gupta, Peter Ebert Christensen, Xinyun Chen, and Dawn
    Song. 2020. Synthesize, Execute and Debug: Learning to Repair for Neural Program
    Synthesis. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2018) Rahul Gupta, Aditya Kanade, and Shirish Shevade. 2018. Deep
    reinforcement learning for programming language correction. *arXiv:1801.10467*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2019) R Gupta, A Kanade, and S Shevade. 2019. Neural attribution
    for semantic bug-localization in student programs. *NeurIPS* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2017) Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade.
    2017. Deepfix: Fixing common c language errors by deep learning. In *AAAI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadi et al. (2022) Mohammad Abdul Hadi, Imam Nur Bani Yusuf, Ferdian Thung,
    Kien Gia Luong, Lingxiao Jiang, Fatemeh H. Fard, and David Lo. 2022. On the effectiveness
    of pretrained models for API learning. In *ICPC*. ACM, 309–320.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haldar et al. (2020) Rajarshi Haldar, Lingfei Wu, JinJun Xiong, and Julia Hockenmaier.
    2020. A Multi-Perspective Architecture for Semantic Code Search. In *ACL*. 8563–8568.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haque et al. (2021) Sakib Haque, Aakash Bansal, Lingfei Wu, and Collin McMillan.
    2021. Action Word Prediction for Neural Source Code Summarization. In *SANER*.
    IEEE, 330–341.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haque et al. (2020) Sakib Haque, Alexander LeClair, Lingfei Wu, and Collin McMillan.
    2020. Improved automatic summarization of subroutines via attention to file context.
    In *MSR*. 300–310.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harer et al. (2018) Jacob Harer, Onur Ozdemir, Tomo Lazovich, Christopher P.
    Reale, Rebecca L. Russell, Louis Y. Kim, and Sang Peter Chin. 2018. Learning to
    Repair Software Vulnerabilities with Generative Adversarial Networks. In *NeurIPS*.
    7944–7954.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassan et al. (2018) Mostafa Hassan, Caterina Urban, Marco Eilers, and Peter
    Müller. 2018. MaxSMT-based type inference for Python 3\. In *International Conference
    on Computer Aided Verification*. 12–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hata et al. (2018) Hideaki Hata, Emad Shihab, and Graham Neubig. 2018. Learning
    to generate corrective patches using neural machine translation. *arXiv:1812.07170*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hayati et al. (2018) Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru,
    Pengcheng Yin, Anthony Tomasic, and Graham Neubig. 2018. Retrieval-Based Neural
    Code Generation. In *EMNLP*. 925–930.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hellendoorn et al. (2018) Vincent J Hellendoorn, Christian Bird, Earl T Barr,
    and Miltiadis Allamanis. 2018. Deep learning type inference. In *ESEC/FSE*. 152–162.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henkel et al. (2018) Jordan Henkel, Shuvendu K Lahiri, Ben Liblit, and Thomas
    Reps. 2018. Code vectors: Understanding programs through embedded abstracted symbolic
    traces. In *ESEC/FSE*. 163–174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2006) Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006.
    A fast learning algorithm for deep belief nets. *Neural computation* 18, 7 (2006),
    1527–1554.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoang et al. (2020) Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall.
    2020. Cc2vec: Distributed representations of code changes. In *ICSE*. 518–529.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2018a) Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018a. Deep
    code comment generation. In *ICPC*. 200–20010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2018b) Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin.
    2018b. Summarizing source code with transferred api knowledge.(2018). In *IJCAI*,
    Vol. 19\. 2269–2275.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Yutao Hu, Deqing Zou, Junru Peng, Yueming Wu, Junjie Shan,
    and Hai Jin. 2022. TreeCen: Building Tree Graph for Scalable Semantic Code Clone
    Detection. In *ASE*. ACM, 109:1–109:12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming
    Zhu, and Qinghua Lu. 2022. Prompt-tuned Code Language Model as a Neural Knowledge
    Base for Type Inference in Statically-Typed Partial Code. In *ASE*. ACM, 79:1–79:13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of
    semantic code search. *arXiv:1909.09436* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iyer et al. (2016) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2016. Summarizing source code using a neural attention model. In
    *ACL*. 2073–2083.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iyer et al. (2018) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2018. Mapping Language to Code in Programmatic Context. In *EMNLP*.
    1643–1652.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2021) Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph
    Gonzalez, and Ion Stoica. 2021. Contrastive Code Representation Learning. In *EMNLP*.
    5954–5971.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2017) He Jiang, Jingxuan Zhang, Zhilei Ren, and Tao Zhang. 2017.
    An unsupervised approach for discovering relevant tutorial fragments for APIs.
    In *ICSE*. 38–48.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2019) Jiajun Jiang, Yingfei Xiong, and Xin Xia. 2019. A manual
    inspection of Defects4J bugs and its implications for automatic program repair.
    *Sci. China Inf. Sci.* 62, 10 (2019), 200102:1–200102:16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2021a) Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021a. CURE:
    Code-Aware Neural Machine Translation for Automatic Program Repair. In *ICSE*.
    1161–1173.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2021b) Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei
    Lyu. 2021b. TreeBERT: A tree-based pre-trained model for programming language.
    In *Uncertainty in Artificial Intelligence*. 54–63.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2022) Dun Jin, Peiyu Liu, and Zhenfang Zhu. 2022. Automatically
    Generating Code Comment Using Heterogeneous Graph Neural Networks. In *SANER*.
    1078–1088.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kanade et al. (2020) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. 2020. Learning and Evaluating Contextual Embedding of Source Code.
    In *ICML*. 5110–5121.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karampatsis et al. (2020) Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes,
    Charles Sutton, and Andrea Janes. 2020. Big code!= big vocabulary: Open-vocabulary
    models for source code. In *ICSE*. 1073–1085.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021) Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra.
    2021. Code prediction by feeding trees to transformers. In *ICSE*. 150–162.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lachaux et al. (2021) Marie-Anne Lachaux, Baptiste Rozière, Marc Szafraniec,
    and Guillaume Lample. 2021. DOBF: A Deobfuscation Pre-Training Objective for Programming
    Languages. In *NeurIPS*. 14967–14979.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lattner and Adve (2004) Chris Lattner and Vikram Adve. 2004. LLVM: A compilation
    framework for lifelong program analysis & transformation. In *CGO*. 75–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le et al. (2018) Tue Le, Tuan Nguyen, Trung Le, Dinh Phung, Paul Montague, Olivier
    De Vel, and Lizhen Qu. 2018. Maximal divergence sequential autoencoder for binary
    software vulnerability detection. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeClair et al. (2018) Alexander LeClair, Zachary Eberhart, and Collin McMillan.
    2018. Adapting neural text classification for improved software categorization.
    In *ICSME*. 461–472.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeClair et al. (2020) Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin
    McMillan. 2020. Improved code summarization via a graph neural network. In *ICPC*.
    184–195.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeClair et al. (2019) Alexander LeClair, Siyuan Jiang, and Collin McMillan.
    2019. A neural model for generating natural language summaries of program subroutines.
    In *ICSE*. 795–806.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    et al. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
    Generation, Translation, and Comprehension. In *ACL*. 7871–7880.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023b. Enabling Programming
    Thinking in Large Language Models Toward Code Generation. *arXiv preprint arXiv:2305.06599*
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023c) Jia Li, Ge Li, Chongyang Tao, Huangzhao Zhang, Fang Liu, and
    Zhi Jin. 2023c. Large Language Model-Aware In-Context Learning for Code Generation.
    *arXiv preprint arXiv:2310.09748* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021a) Jia Li, Yongmin Li, Ge Li, Xing Hu, Xin Xia, and Zhi Jin.
    2021a. EditSum: A Retrieve-and-Edit Framework for Source Code Summarization. In
    *ASE*. 155–166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018a) Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2018a.
    Code Completion with Neural Attention and Pointer Networks. In *IJCAI*. 4159–4165.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022e) Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan
    Hua, Geng Liang, and Chun Zuo. 2022e. AUGER: automatically generating review comments
    with pre-training models. In *ESEC/FSE*. 1009–1021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, et al. 2023a. StarCoder:
    may the source be with you! *arXiv preprint arXiv:2305.06161* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) Xiaonan Li, Yeyun Gong, Yelong Shen, et al. 2022b. CodeRetriever:
    Unimodal and Bimodal Contrastive Learning. In *EMNLP*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022a) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, et al. 2022a. Competition-Level Code
    Generation with AlphaCode. *Science* 378, 6624 (2022), 1092–1097.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S.
    Zemel. 2016. Gated Graph Sequence Neural Networks. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Yi Li, Shaohua Wang, and Tien N Nguyen. 2020. Dlfix: Context-based
    code transformation learning for automated program repair. In *ICSE*. 602–614.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021b) Yi Li, Shaohua Wang, and Tien N Nguyen. 2021b. Fault Localization
    with Code Coverage Representation Learning. In *ICSE*. 661–673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021c) Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021c. Vulnerability
    detection with fine-grained interpretations. In *ESEC/FSE*. 292–303.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022d) Yi Li, Shaohua Wang, and Tien N. Nguyen. 2022d. DEAR: A Novel
    Deep Learning-based Approach for Automated Program Repair. In *ICSE*. 511–523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Yi Li, Shaohua Wang, Tien N Nguyen, and Son Van Nguyen. 2019.
    Improving bug detection via context-based code representation learning and attention-based
    neural networks. *OOPSLA* 3 (2019), 1–30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022c) Zongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang,
    Sen Nie, and Shi Wu. 2022c. Unleashing the Power of Compiler Intermediate Representation
    to Enhance Neural Program Embeddings. In *ICSE*. 2253–2265.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021d) Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and
    Zhaoxuan Chen. 2021d. SySeVR: A framework for using deep learning to detect software
    vulnerabilities. *TDSC* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018b) Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan
    Wang, Zhijun Deng, and Yuyi Zhong. 2018b. VulDeePecker: A Deep Learning-Based
    System for Vulnerability Detection. In *NDSS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2021) Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen,
    Hui Li, and Rongxin Wu. 2021. Improving Code Summarization with Block-wise Abstract
    Syntax Tree Splitting. In *ICPC*. IEEE, 184–195.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling et al. (2021b) Chunyang Ling, Yanzhen Zou, and Bing Xie. 2021b. Graph Neural
    Network Based Collaborative Filtering for API Usage Recommendation. In *SANER*.
    IEEE, 36–47.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling et al. (2016) Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz
    Hermann, Tomáš Kočiskỳ, Fumin Wang, and Andrew Senior. 2016. Latent Predictor
    Networks for Code Generation. In *ACL*. 599–609.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling et al. (2021a) Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei
    Ma, Fangli Xu, Alex X. Liu, Chunming Wu, and Shouling Ji. 2021a. Deep Graph Matching
    and Searching for Semantic Code Retrieval. *TKDD* 15, 5 (2021), 88:1–88:21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Chao Liu, Xuanlin Bao, Hongyu Zhang, Neng Zhang, Haibo Hu,
    Xiaohong Zhang, and Meng Yan. 2023. Improving ChatGPT Prompt for Code Generation.
    *arXiv preprint arXiv:2305.08360* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2016a) Chang Liu, Xinyun Chen, Eui Chul Shin, Mingcheng Chen, and
    Dawn Song. 2016a. Latent attention for if-then program synthesis. *NeurIPS* 29
    (2016), 4574–4582.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2016b) Chang Liu, Xin Wang, Richard Shin, Joseph E Gonzalez, and
    Dawn Song. 2016b. Neural code completion. (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin.
    2020b. A Self-Attentional Neural Architecture for Code Completion with Multi-Task
    Learning. In *ICPC*. 37–47.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020c) Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020c. Multi-task
    learning based pre-trained language model for code completion. In *ASE*. 473–485.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020e) Fang Liu, Lu Zhang, and Zhi Jin. 2020e. Modeling programs
    hierarchically with stack-augmented LSTM. *Journal of Systems and Software* 164
    (2020), 110547.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Kui Liu, Dongsun Kim, Tegawendé F Bissyandé, Taeyoung Kim,
    Kisub Kim, Anil Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning to spot
    and refactor inconsistent method names. In *ICSE*. 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021a) Qianjun Liu, Shouling Ji, Changchang Liu, and Chunming Wu.
    2021a. A Practical Black-box Attack on Source Code Authorship Identification Classifiers.
    *TIFS* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, and Yang
    Liu. 2020a. Retrieval-Augmented Generation for Code Summarization via Hybrid GNN.
    In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin
    Qiu, and Xun Wang. 2021b. Combining Graph Neural Networks with Expert Knowledge
    for Smart Contract Vulnerability Detection. *TKDE* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020d) Zhongxin Liu, Xin Xia, Meng Yan, and Shanping Li. 2020d.
    Automating just-in-time comment updating. In *ASE*. 585–597.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'López et al. (2022) José Antonio Hernández López, Martin Weyssow, Jesús Sánchez
    Cuadrado, and Houari A. Sahraoui. 2022. AST-Probe: Recovering abstract syntax
    trees from hidden representations of pre-trained language models. In *ASE*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2022) Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang,
    and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval-Augmented Code Completion Framework.
    In *ACL*. 6227–6240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    et al. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding
    and Generation. In *NeurIPS Datasets and Benchmarks*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder: Empowering
    Code Large Language Models with Evol-Instruct. *arXiv preprint arXiv:2306.08568*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maddison and Tarlow (2014) Chris Maddison and Daniel Tarlow. 2014. Structured
    generative models of natural source code. In *ICML*. 649–657.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Malik et al. (2019) Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. 2019.
    NL2Type: inferring JavaScript function types from natural language information.
    In *ICSE*. 304–315.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mastropaolo et al. (2021) Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper,
    David Nader Palacio, Denys Poshyvanyk, et al. 2021. Studying the usage of text-to-text
    transfer transformer to support code-related tasks. In *ICSE*. 336–347.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mehrotra et al. (2021) Nikita Mehrotra, Navdha Agarwal, Piyush Gupta, Saket
    Anand, David Lo, and Rahul Purandare. 2021. Modeling Functional Similarity in
    Source Code with Graph-Based Siamese Networks. *TSE* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mesbah et al. (2019) Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso,
    and Edward Aftandilian. 2019. DeepDelta: learning to repair compilation errors.
    In *ESEC/FSE*. 925–936.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. Efficient Estimation of Word Representations in Vector Space. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mir et al. (2022) Amir M. Mir, Evaldas Latoskinas, Sebastian Proksch, and Georgios
    Gousios. 2022. Type4Py: Practical Deep Similarity Learning-Based Type Inference
    for Python. In *ICSE*. 2241–2252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreno et al. (2015) Laura Moreno, Gabriele Bavota, Massimiliano Di Penta, Rocco
    Oliveto, and Andrian Marcus. 2015. How can I use this method?. In *ICSE*, Vol. 1\.
    880–890.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mou et al. (2016) Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional
    neural networks over tree structures for programming language processing. In *AAAI*,
    Vol. 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mu et al. (2022) Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang. 2022.
    Automatic Comment Generation via Multi-Pass Deliberation. In *ASE*. ACM, 14:1–14:12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nafi et al. (2019) Kawser Wazed Nafi, Tonny Shekha Kar, Banani Roy, Chanchal K
    Roy, and Kevin A Schneider. 2019. Clcdsa: cross language code clone detection
    using syntactical features and api documentation. In *ASE*. 1026–1037.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nair et al. (2020) Aravind Nair, Avijit Roy, and Karl Meinke. 2020. funcGNN:
    A Graph Neural Network Approach to Program Similarity. In *ESEM*. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nan et al. (2020) Zifan Nan, Hui Guan, and Xipeng Shen. 2020. HISyn: human
    learning-inspired natural language programming. In *ESEC/FSE*. 75–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2021) Phuong T Nguyen, Claudio Di Sipio, Juri Di Rocco, Massimiliano
    Di Penta, and Davide Di Ruscio. 2021. Adversarial Attacks to API Recommender Systems:
    Time to Wake Up and Smell the Coffee?. In *ASE*. 253–265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2020) Son Nguyen, Hung Phan, Trinh Le, and Tien N Nguyen. 2020.
    Suggesting natural method names to check name consistencies. In *ICSE*. 1372–1384.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2017) Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and
    Tien N Nguyen. 2017. Exploring API embedding for API usages and applications.
    In *ICSE*. 438–449.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niu et al. (2022) Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang,
    and Bin Luo. 2022. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source
    Code Representations. In *ICSE*. 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nye et al. (2019) Maxwell Nye, Luke Hewitt, Joshua Tenenbaum, and Armando Solar-Lezama.
    2019. Learning to infer program sketches. In *ICML*. 4861–4870.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI. 2022. ChatGPT. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam
    Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A Fast, Extensible
    Toolkit for Sequence Modeling. In *NAACL-HLT: Demonstrations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pandi et al. (2020) Irene Vlassi Pandi, Earl T Barr, Andrew D Gordon, and Charles
    Sutton. 2020. OptTyper: Probabilistic Type Inference by Optimising Logical and
    Natural Constraints. *arXiv:2004.00348* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panthaplackel et al. (2021) Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric,
    and Raymond J Mooney. 2021. Deep Just-In-Time Inconsistency Detection Between
    Comments and Source Code. In *AAAI*, Vol. 35\. 427–435.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panthaplackel et al. (2020) Sheena Panthaplackel, Pengyu Nie, Milos Gligoric,
    Junyi Jessy Li, and Raymond Mooney. 2020. Learning to Update Natural Language
    Comments Based on Code Changes. In *ACL*. 1853–1868.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2021) Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and
    Tie-Yan Liu. 2021. How could Neural Networks understand Programs?. In *ICML*,
    Vol. 139\. 8476–8486.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poesia et al. (2022) Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo
    Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable Code
    Generation from Pre-trained Language Models. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pornprasit et al. (2021) Chanathip Pornprasit, Chakkrit Tantithamthavorn, Jirayus
    Jiarpakdee, Michael Fu, and Patanamon Thongtanunam. 2021. PyExplainer: Explaining
    the Predictions of Just-In-Time Defect Models. In *ASE*. 407–418.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pradel et al. (2020) Michael Pradel, Georgios Gousios, Jason Liu, and Satish
    Chandra. 2020. Typewriter: Neural type prediction with search-based validation.
    In *ESEC/FSE*. 209–220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pradel and Sen (2018) Michael Pradel and Koushik Sen. 2018. Deepbugs: A learning
    approach to name-based bug detection. *OOPSLA* 2 (2018), 1–25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quiring et al. (2019) Erwin Quiring, Alwin Maier, and Konrad Rieck. 2019. Misleading
    authorship attribution of source code using adversarial learning. In *USENIX Security
    19*. 479–496.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabin et al. (2021) Md. Rafiqul Islam Rabin, Vincent J. Hellendoorn, and Mohammad Amin
    Alipour. 2021. Understanding neural code intelligence through program simplification.
    In *ESEC/FSE*. ACM, 441–452.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabinovich et al. (2017) Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017.
    Abstract Syntax Networks for Code Generation and Semantic Parsing. In *ACL*. 1139–1149.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, et al. 2020. Exploring the Limits of Transfer Learning
    with a Unified Text-to-Text Transformer. *JMLR* 21 (2020), 1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramakrishnan and Albarghouthi (2022) Goutham Ramakrishnan and Aws Albarghouthi.
    2022. Backdoors in Neural Models of Source Code. In *ICPR*. IEEE, 2892–2899.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramakrishnan et al. (2020) Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws
    Albarghouthi, Somesh Jha, and Thomas Reps. 2020. Semantic robustness of models
    of source code. *arXiv:2002.03043* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raychev et al. (2016) Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016.
    Probabilistic model for code with decision trees. *ACM SIGPLAN Notices* 51, 10
    (2016), 731–747.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raychev et al. (2014) Veselin Raychev, Martin Vechev, and Eran Yahav. 2014.
    Code completion with statistical language models. In *ICPC*. 419–428.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rozière et al. (2020) Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot,
    and Guillaume Lample. 2020. Unsupervised Translation of Programming Languages.
    In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rozière et al. (2022) Baptiste Rozière, Jie Zhang, François Charton, Mark Harman,
    Gabriel Synnaeve, and Guillaume Lample. 2022. Leveraging Automated Unit Tests
    for Unsupervised Code Translation. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salib (2004) Michael Salib. 2004. Faster than C: Static type inference with
    Starkiller. *PyCon Proceedings, Washington DC* 3 (2004).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Santos et al. (2018) Eddie Antonio Santos, Joshua Charles Campbell, Dhvani
    Patel, Abram Hindle, and José Nelson Amaral. 2018. Syntax and sensibility: Using
    language models to detect and correct syntax errors. In *SANER*. 311–322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schuster et al. (2021) Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly
    Shmatikov. 2021. You autocomplete me: Poisoning vulnerabilities in neural code
    completion. In *USENIX Security*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Severi et al. (2021) Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea.
    2021. Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers.
    In *USENIX Security*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shahbazi et al. (2021) Ramin Shahbazi, Rishab Sharma, and Fatemeh H. Fard.
    2021. API2Com: On the Improvement of Automatically Generated Code Comments Using
    API Documentations. In *ICPC*. IEEE, 411–421.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2022) Rishab Sharma, Fuxiang Chen, Fatemeh H. Fard, and David
    Lo. 2022. An exploratory study on code attention in BERT. In *ICPC*. ACM, 437–448.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2021) Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han,
    et al. 2021. CAST: Enhancing Code Summarization with Hierarchical Splitting and
    Reconstruction of Abstract Syntax Trees. In *EMNLP*. 4053–4062.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2022b) Jieke Shi, Zhou Yang, Bowen Xu, Hong Jin Kang, and David
    Lo. 2022b. Compressing Pre-trained Models of Code into 3 MB. In *ASE*. ACM, 24:1–24:12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2022a) Lin Shi, Fangwen Mu, Xiao Chen, Song Wang, Junjie Wang, Ye
    Yang, Ge Li, Xin Xia, and Qing Wang. 2022a. Are we building on the rock? on the
    importance of data preprocessing for code summarization. In *ESEC/FSE*. ACM, 107–119.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2022c) Yucen Shi, Ying Yin, Zhengkui Wang, David Lo, Tao Zhang,
    Xin Xia, Yuhai Zhao, and Bowen Xu. 2022c. How to better utilize code graphs in
    semantic code search?. In *ESEC/FSE*. 722–733.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shrivastava et al. (2020) Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow.
    2020. On-the-Fly Adaptation of Source Code Models using Meta-Learning. *arXiv:2003.11768*
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu and Zhang (2017) Chengxun Shu and Hongyu Zhang. 2017. Neural Programming
    by Example. In *AAAI*. 1539–1545.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sui et al. (2020) Yulei Sui, Xiao Cheng, Guanqin Zhang, and Haoyu Wang. 2020.
    Flow2Vec: value-flow-based precise code embedding. *OOPSLA* 4 (2020), 1–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sui and Xue (2016) Yulei Sui and Jingling Xue. 2016. SVF: interprocedural static
    value-flow analysis in LLVM. In *Proceedings of the 25th international conference
    on compiler construction*. 265–266.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022a) Weisong Sun, Chunrong Fang, Yuchen Chen, Guanhong Tao, Tingxu
    Han, and Quanjun Zhang. 2022a. Code Search based on Context-aware Code Translation.
    In *ICSE*. 388–400.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2022b) Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi
    Wu. 2022b. Heterogeneous Information Networks: the Past, the Present, and the
    Future. *Proc. VLDB Endow.* 15, 12 (2022), 3807–3811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022c) Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022c.
    On the Importance of Building High-quality Training Datasets for Neural Code Search.
    In *ICSE*. ACM, 1609–1620.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Zeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li, and Lu
    Zhang. 2019. A grammar-based structural cnn decoder for code generation. In *AAAI*,
    Vol. 33\. 7055–7062.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou,
    and Lu Zhang. 2020. TreeGen: A Tree-Based Transformer Architecture for Code Generation.
    In *AAAI*. 8984–8991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Svyatkovskiy et al. (2020) Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
    and Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer.
    In *ESEC/FSE*. 1433–1443.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Svyatkovskiy et al. (2021) Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi,
    Maik Riechert, Juliana Vicente Franco, and Miltiadis Allamanis. 2021. Fast and
    memory-efficient neural code completion. In *MSR*. 329–340.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Svyatkovskiy et al. (2019) Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and
    Neel Sundaresan. 2019. Pythia: Ai-assisted code completion system. In *SIGKDD*.
    2727–2735.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2022) Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang,
    Zheling Zhu, and Bin Luo. 2022. AST-Trans: Code Summarization with Efficient Tree-Structured
    Attention. In *ICSE*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2022) Chenning Tao, Qi Zhan, Xing Hu, and Xin Xia. 2022. C4: contrastive
    cross-language code clone detection. In *ICPC*. ACM, 413–424.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarlow et al. (2020) Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen,
    Pierre-Antoine Manzagol, Charles Sutton, and Edward Aftandilian. 2020. Learning
    to fix build errors with graph2diff neural networks. In *ICSE Workshops*. 19–20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2020) Haoye Tian, Kui Liu, Abdoul Kader Kaboré, Anil Koyuncu, Li
    Li, et al. 2020. Evaluating representation learning of code changes for predicting
    patch correctness in program repair. In *ASE*. 981–992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tufano et al. (2019) Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele
    Bavota, and Denys Poshyvanyk. 2019. On learning meaningful code changes via neural
    machine translation. In *ICSE*. 25–36.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tufano et al. (2018a) Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
    Di Penta, et al. 2018a. An empirical investigation into learning bug-fixing patches
    in the wild via neural machine translation. In *ASE*. 832–837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tufano et al. (2018b) Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano
    Di Penta, Martin White, and Denys Poshyvanyk. 2018b. Deep learning similarities
    from different representations of source code. In *MSR*. 542–553.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vasic et al. (2018) Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber,
    and Rishabh Singh. 2018. Neural Program Repair by Jointly Learning to Localize
    and Repair. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *NeurIPS*. 5998–6008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VenkataKeerthy et al. (2020) S VenkataKeerthy, Rohit Aggarwal, Shalini Jain,
    Maunendra Sankar Desarkar, Ramakrishna Upadrasta, and YN Srikant. 2020. Ir2vec:
    Llvm ir based scalable program embeddings. *TACO* 17, 4 (2020), 1–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan et al. (2022a) Yao Wan, Yang He, Zhangqian Bi, Jianguo Zhang, Yulei Sui,
    Hongyu Zhang, et al. 2022a. NaturalCC: An Open-Source Toolkit for Code Intelligence.
    In *ICSE, Companion Volume*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2019) Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao,
    Jian Wu, and Philip S. Yu. 2019. Multi-modal Attention Network Learning for Semantic
    Source Code Retrieval. In *ASE*. 13–25.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan et al. (2022b) Yao Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, Guandong
    Xu, Dezhong Yao, Hai Jin, and Lichao Sun. 2022b. You see what I want you to see:
    poisoning vulnerabilities in neural code search. In *ESEC/FSE*. 1233–1245.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2022c) Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu,
    and Hai Jin. 2022c. What Do They Capture? - A Structural Analysis of Pre-Trained
    Language Models for Source Code. In *ICSE*. 2377–2388.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2018) Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian
    Wu, and Philip S Yu. 2018. Improving automatic source code summarization via deep
    reinforcement learning. In *ASE*. 397–407.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022c) Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu
    Zhang, and Michael R. Lyu. 2022c. No more fine-tuning? an experimental evaluation
    of prompt tuning in code intelligence. In *ESEC/FSE*. 382–394.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong,
    Wei Dong, and Xiangke Liao. 2022b. Bridging Pre-trained Models and Downstream
    Tasks for Source Code Understanding. In *ICSE*. 287–298.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Huanting Wang, Guixin Ye, Zhanyong Tang, Shin Hwei Tan,
    et al. 2020c. Combining graph-based learning with automated data collection for
    code vulnerability detection. *TIFS* 16 (2020), 1943–1958.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Simin Wang, Liguo Huang, Jidong Ge, Tengfei Zhang, Haitao
    Feng, Ming Li, He Zhang, and Vincent Ng. 2020a. Synergy between Machine/Deep Learning
    and Software Engineering: How Far Are We? *arXiv:2008.05515* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2016) Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning
    semantic features for defect prediction. In *ICSE*. 297–308.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020b.
    Detecting code clones with graph neural network and flow-augmented abstract syntax
    tree. In *SANER*. 261–271.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao
    Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang. 2021b. SynCoBERT: Syntax-Guided Multi-Modal
    Contrastive Pre-Training for Code Representation. *arXiv:2108.04556* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) Yu Wang, Yu Dong, Xuesong Lu, and Aoying Zhou. 2022a. GypSum:
    learning hybrid representations for code summarization. In *ICPC*. ACM, 12–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui,
    Junnan Li, and Steven CH Hoi. 2023. Codet5+: Open code large language models for
    code understanding and generation. *arXiv preprint arXiv:2305.07922* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Li (2021) Yanlin Wang and Hui Li. 2021. Code completion by modeling
    flattened abstract syntax trees as graphs. In *AAAI*, Vol. 35\. 14015–14023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H.
    Hoi. 2021a. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models
    for Code Understanding and Generation. In *EMNLP*. 8696–8708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watson et al. (2020) Cody Watson, Ncthan Cooper, David Nader Palacio, Kevin
    Moran, and Denys Poshyvanyk. 2020. A Systematic Literature Review on the Use of
    Deep Learning in Software Engineering Research. *arXiv:2009.06520* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2019) Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code
    Generation as a Dual Task of Code Summarization. In *NeurIPS*. 6559–6569.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2020b) Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2020b.
    Retrieve and refine: exemplar-based neural comment generation. In *ASE*. 349–360.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei and Li (2017) Huihui Wei and Ming Li. 2017. Supervised Deep Features for
    Software Functional Clone Detection by Exploiting Lexical and Syntactical Information
    in Source Code.. In *IJCAI*. 3034–3040.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2020a) Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig.
    2020a. LambdaNet: Probabilistic Type Inference using Graph Neural Networks. In
    *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2022) Moshi Wei, Nima Shiri Harzevili, Yuchao Huang, Junjie Wang,
    and Song Wang. 2022. CLEAR: contrastive learning for API recommendation. In *ICSE*.
    376–387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White et al. (2019) Martin White, Michele Tufano, Matias Martinez, Martin Monperrus,
    and Denys Poshyvanyk. 2019. Sorting and transforming program repair ingredients
    via deep learning code similarities. In *SANER*. 479–490.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White et al. (2016) Martin White, Michele Tufano, Christopher Vendome, and Denys
    Poshyvanyk. 2016. Deep learning code fragments for code clone detection. In *ASE*.
    87–98.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White et al. (2015) Martin White, Christopher Vendome, Mario Linares-Vásquez,
    and Denys Poshyvanyk. 2015. Toward deep learning software repositories. In *MSR*.
    334–345.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021) Hongqiu Wu, Hai Zhao, and Min Zhang. 2021. Code Summarization
    with Structure-induced Transformer. In *Findings of ACL*. 1078–1090.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022a) Yueming Wu, Siyue Feng, Deqing Zou, and Hai Jin. 2022a. Detecting
    Semantic Code Clones by Building AST-based Markov Chains Model. In *ASE*. ACM,
    34:1–34:13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019) Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo,
    and Ross Girshick. 2019. Detectron2. [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Yueming Wu, Deqing Zou, Shihan Dou, Siru Yang, Wei Yang, Feng
    Cheng, Hong Liang, and Hai Jin. 2020. SCDetector: Software Functional Clone Detection
    Based on Semantic Tokens Analysis. In *ASE*. 821–833.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022b) Yueming Wu, Deqing Zou, Shihan Dou, Wei Yang, Duo Xu, and
    Hai Jin. 2022b. VulCNN: An Image-inspired Scalable Vulnerability Detection System.
    In *ICSE*. 2365–2376.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia et al. (2023) Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023.
    Automated program repair in the era of large pre-trained language models. In *Proceedings
    of the 45th International Conference on Software Engineering (ICSE 2023). Association
    for Computing Machinery*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2022) Rui Xie, Tianxiang Hu, Wei Ye, and Shikun Zhang. 2022. Low-Resources
    Project-Specific Code Summarization. In *ASE*. ACM, 68:1–68:12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2021) Rui Xie, Wei Ye, Jinan Sun, and Shikun Zhang. 2021. Exploiting
    Method Names to Improve Code Summarization: A Deliberation Multi-Task Learning
    Approach. In *ICPC*. IEEE, 138–148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu,
    and Graham Neubig. 2020. Incorporating External Knowledge through Pre-training
    for Natural Language to Code Generation. In *ACL*. 6045–6052.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamaguchi et al. (2014) Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad
    Rieck. 2014. Modeling and discovering vulnerabilities with code property graphs.
    In *S&P*. 590–604.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022a) Guang Yang, Xiang Chen, Yanlin Zhou, and Chi Yu. 2022a.
    DualSC: Automatic Generation and Summarization of Shellcode via Transformer and
    Dual Learning. In *SANER*. 361–372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022c) Yanming Yang, Xin Xia, David Lo, and John Grundy. 2022c.
    A Survey on Deep Learning for Software Engineering. *ACM Comput. Surv.* 54, 10s,
    Article 206 (sep 2022), 73 pages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021) Zhen Yang, Jacky Keung, Xiao Yu, Xiaodong Gu, Zhengyuan Wei,
    Xiaoxue Ma, and Miao Zhang. 2021. A Multi-Modal Transformer-based Code Summarization
    Approach for Smart Contracts. In *ICPC*. IEEE, 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022b) Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022b. Natural
    Attack for Pre-trained Models of Code. In *ICSE*. ACM, 1482–1493.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2019) Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019.
    Coacor: Code annotation for code retrieval with reinforcement learning. In *The
    World Wide Web Conference*. 2203–2214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yasunaga and Liang (2020) Michihiro Yasunaga and Percy Liang. 2020. Graph-based,
    self-supervised program repair from diagnostic feedback. In *ICML*. 10799–10808.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. (2020) Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang,
    and Shikun Zhang. 2020. Leveraging code generation to improve code retrieval and
    summarization via dual learning. In *Proceedings of The Web Conference 2020*.
    2309–2319.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yefet et al. (2020) Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial
    examples for models of code. *OOPSLA* 4 (2020), 1–30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin and Neubig (2017) Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural
    Model for General-Purpose Code Generation. In *ACL*. 440–450.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018a) Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang,
    Zifan Li, and Dragomir R. Radev. 2018a. SyntaxSQLNet: Syntax Tree Networks for
    Complex and Cross-Domain Text-to-SQL Task. In *EMNLP*. 1653–1663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019a) Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang,
    Xi Victoria Lin, et al. 2019a. CoSQL: A Conversational Text-to-SQL Challenge Towards
    Cross-Domain Natural Language Interfaces to Databases. In *EMNLP*. 1962–1979.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018b) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, et al. 2018b. Spider: A Large-Scale Human-Labeled Dataset for Complex
    and Cross-Domain Semantic Parsing and Text-to-SQL Task. In *EMNLP*. 3911–3921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019b) Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria
    Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, et al. 2019b. SParC: Cross-Domain
    Semantic Parsing in Context. In *ACL*. 4511–4523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Huangzhao Zhang, Zhuo Li, Ge Li, Lei Ma, Yang Liu, and
    Zhi Jin. 2020a. Generating adversarial examples for holding robustness of source
    code processing models. In *AAAI*, Vol. 34\. 1169–1176.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021b) Jingfeng Zhang, Haiwen Hong, Yin Zhang, Yao Wan, Ye Liu,
    and Yulei Sui. 2021b. Disentangled Code Representation Learning for Multiple Programming
    Languages. In *Findings of ACL*. 4454–4466.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy
    Li, and Milos Gligoric. 2022a. CoditT5: Pretraining for Source Code and Natural
    Language Editing. In *37th IEEE/ACM International Conference on Automated Software
    Engineering, ASE 2022, Rochester, MI, USA, October 10-14, 2022*. ACM, 22:1–22:12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020c) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2020c. Retrieval-based neural source code summarization. In *ICSE*. 1385–1397.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan
    Wang, and Xudong Liu. 2019. A novel neural source code representation based on
    abstract syntax tree. In *ICSE*. 783–794.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021a) Tianyi Zhang, Zhiyang Chen, Yuanli Zhu, Priyan Vaithilingam,
    Xinyu Wang, and Elena L Glassman. 2021a. Interpretable Program Synthesis. In *Proceedings
    of the 2021 CHI Conference on Human Factors in Computing Systems*. 1–16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang
    Li. 2020b. Adversarial attacks on deep-learning models in natural language processing:
    A survey. *TIST* 11, 3 (2020), 1–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022b) Zhaowei Zhang, Hongyu Zhang, Beijun Shen, and Xiaodong
    Gu. 2022b. Diet code is healthy: simplifying programs for pre-trained models of
    code. In *ESEC/FSE*. 1073–1084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao and Huang (2018) Gang Zhao and Jeff Huang. 2018. Deepsim: deep learning
    code functional similarity. In *ESEC/FSE*. 141–151.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A
    pre-trained model for code generation with multilingual evaluations on humaneval-x.
    *arXiv preprint arXiv:2303.17568* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017.
    Seq2sql: Generating structured queries from natural language using reinforcement
    learning. *arXiv:1709.00103* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2021) Xin Zhou, DongGyun Han, and David Lo. 2021. Assessing Generalizability
    of CodeBERT. In *ICSME*. IEEE, 425–436.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2019) Yaqin Zhou, Shangqing Liu, Jing Kai Siow, Xiaoning Du, and
    Yang Liu. 2019. Devign: Effective Vulnerability Identification by Learning Comprehensive
    Program Semantics via Graph Neural Networks. In *NeurIPS*. 10197–10207.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue
    Chen, and Harald C. Gall. 2022. Adversarial Robustness of Deep Code Comment Generation.
    *TOSEM* 31, 4 (2022), 60:1–60:30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) Qihao Zhu, Zeyu Sun, Xiran Liang, Yingfei Xiong, and Lu Zhang.
    2020. OCoR: an overlapping-aware code retriever. In *ASE*. 883–894.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2021) Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan,
    Yingfei Xiong, and Lu Zhang. 2021. A syntax-guided edit decoder for neural program
    repair. In *ESEC/FSE*. 341–353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2022) Xiaoning Zhu, Chaofeng Sha, and Junyu Niu. 2022. A Simple
    Retrieval-based Method for Code Comment Generation. In *SANER*. 1089–1100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. (2019) Deqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li, and Hai Jin.
    2019. $\mu$VulDeePecker: A deep learning-based system for multiclass vulnerability
    detection. *TDSC* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2021) Deqing Zou, Yawei Zhu, Shouhuai Xu, Zhen Li, Hai Jin, and
    Hengkai Ye. 2021. Interpreting deep learning-based vulnerability detector predictions
    based on heuristic searching. *TOSEM* 30, 2 (2021), 1–31.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zügner et al. (2021) Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure
    Leskovec, and Stephan Günnemann. 2021. Language-Agnostic Representation Learning
    of Source Code from Structure and Context. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplementary Materials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Surveyed Venues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [6](#A2.T6 "Table 6 ‣ Appendix B Neural Network Modules and Techniques
    ‣ Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit") summarizes
    a list of top-tier conferences and journals surveyed in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Neural Network Modules and Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benefiting from the powerful representation ability of deep learning, recently
    deep neural networks have been widely used to represent source code as distributed
    vector representations. Here we quickly review several major neural network modules
    and techniques, e.g., RNN, CNN, attention mechanism, Transformer, graph neural
    network, and model pre-training. Let $\mathcal{C}=\{c_{1},c_{2},\ldots,c_{n}\}$
    denote a code corpus, which is composed of a collection of code snippets. For
    each code snippet $c_{i}$, it is composed of a sequence of code tokens, i.e.,
    $c_{i}=\{x_{1},x_{2},\ldots,x_{|c_{i}|}\}$, where $|\cdot|$ denotes the length
    of the sequence. Let $\mathbf{x}_{i}=w(x_{i})$ denote the word embedding corresponding
    to the $i$-th token in the code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNN. Recurrent Neural Networks (RNNs) are neural networks designed to handle
    the sequential inputs with variable lengths. At time step $t$, the hidden state
    $\mathbf{h}_{t}$ of RNNs is updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\mathbf{h}_{t}=f(\mathbf{x}_{t},\mathbf{h}_{t-1})\,,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $f$ is the non-linear mapping function, which is usually a hyperbolic
    tangent, i.e., $f(\mathbf{x}_{t},\mathbf{h}_{t})=\operatorname{tanh}(\mathbf{W}_{x}\mathbf{x}_{t}+\mathbf{W}_{h}\mathbf{h}_{t-1}+\mathbf{b})$.
  prefs: []
  type: TYPE_NORMAL
- en: There have been many variants of RNNs. To alleviate the gradient vanishing issue
    of RNNs, the Long Short-Term Memory (LSTM) \citesecondaryhochreiter1997long2 technology
    with a gate mechanism is proposed to determine the information accumulation, where
    the input gate, forget gate and output gate control the input, output and forget
    part of the entire network through weights and activation function. GRU (Gate
    Recurrent Unit) \citesecondarychung2015gated2 is a simplified version of LSTM
    with fewer parameters by combining the forget and input gates into a single update
    gate by merging the cell state and hidden state. Furthermore, bi-directional RNNs
    are used to represent the sequential contents in both forward and backward directions,
    with two parallel RNNs and combined outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Although RNNs are effective in representing sequential texts, it is difficult
    to be parallelized since sequential computation inhibits parallelization. That
    means, the computation of the current time step is dependent on the output of
    the previous time step. Furthermore, the RNNs are also difficult to handle long-range
    dependency when processing long sequences.
  prefs: []
  type: TYPE_NORMAL
- en: CNN. Convolutional Neural Networks (CNNs) which are originally designed to process
    the pixels of images, have also been introduced to model the sequential texts.
    A CNN is composed of convolutional layers and pooling layers. The convolutional
    layer uses convolution operation to extract meaningful local patterns of inputs,
    and the pooling layer reduces the parameters and computation to make the networks
    deeper. A CNN first concatenates the word embedding of each code token as $\mathbf{x}_{1:n}=\mathbf{x}_{1}\oplus\mathbf{x}_{2}\oplus\cdots\oplus\mathbf{x}_{n}$,
    where $\oplus$ is the concatenation operator. For a window of words $\mathbf{x}_{i:i+h-1}$,
    a feature $\mathbf{h}_{i}$ is calculated by the filter $f$ as $\mathbf{h}_{i}=f(\mathbf{W}\mathbf{x}_{i:i+h-1}+\mathbf{b})$,
    where $\mathbf{b}$ is a bias term and $f$ is a non-linear function like hyperbolic
    tangent. After applying the filter to all possible windows of words, a feature
    map will be obtained, i.e., $\mathbf{h}=[\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{n-h+1}]$.
    Then a pooling layer can be applied to obtain the final representation of code,
    i.e., $\hat{\mathbf{h}}=\operatorname{Pooling}(\mathbf{h})$.
  prefs: []
  type: TYPE_NORMAL
- en: The CNNs that exploit local dependencies using convolution operations are easy
    to parallelize. However, they require many layers to model the long-distance dependencies
    when handling long sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Mechanism. The attention mechanism is a simple yet effective module
    in deep neural networks. It was first introduced by \citesecondarybahdanau2014neural2
    in Neural Machine Translation (NMT) to mitigate the issue of information loss
    in compressing long sentences into a fixed-length vector, as well as the alignment
    between input and output sequences. Since then, the attention mechanism has become
    a widely used component to improve the performance of various models in NLP and
    computer vision. In NMT, the attention mechanism is designed to include an additional
    context vector that allows the decoder to access the entire encoded input sequence
    $\{\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{m}\}$. Especially, it aims
    to learn the attention weights $\alpha_{ij}$, which captures the relevance between
    the encoder hidden state $\mathbf{h}_{i}$ and decoder hidden state $\mathbf{s}_{j}$.
    Consequently, the context vector can be formulated as $c_{j}=\sum_{i=1}^{T}\alpha_{ij}\mathbf{h}_{i}$.
    In general, the attention can also be seen as a mapping of keys $\mathbf{K}$ to
    an attention distribution $\alpha$ according to query $\mathbf{q}$, i.e., $\alpha(\mathbf{q},\mathbf{K})=\operatorname{softmax}(g(\mathbf{q},\mathbf{K}))$,
    where $g$ is the attention score function which measures the similarity between
    the query and key. In some cases, there is also an additional input of values
    $V$ on which the attention distribution is applied. Hence a generalized attention
    with a set of key-value pairs $(\mathbf{K},\mathbf{V})$ and query $\mathbf{Q}$
    is formulated as $\alpha(\mathbf{Q},\mathbf{K},\mathbf{V})=\operatorname{softmax}(g(\mathbf{Q},\mathbf{K}))\cdot\mathbf{V}$.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6. A list of top-tier conferences and journals surveyed in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | No. | Venue | Venue (Full name) |'
  prefs: []
  type: TYPE_TB
- en: '| SE | 1 | ICSE | ACM/IEEE International Conference on Software Engineering
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | ASE | IEEE/ACM International Conference Automated Software Engineering
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | FSE/ESEC | ACM SIGSOFT Symposium on the Foundation of Software Engineering/European
    Software Engineering Conference |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | SANER | IEEE International Conference on Software Analysis, Evolution
    and Reengineering |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | ICSME | IEEE International Conference on Software Maintenance and Evolution
    |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | ICPC | IEEE International Conference on Program Comprehension |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | ESEM | International Symposium on Empirical Software Engineering and
    Measurement |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | MSR | International Conference on Mining Software Repositories |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | TSE | IEEE Transactions on Software Engineering |'
  prefs: []
  type: TYPE_TB
- en: '|  | 10 | TOSEM | ACM Transactions on Software Engineering and Methodology
    |'
  prefs: []
  type: TYPE_TB
- en: '| PL/Compiler | 1 | POPL | ACM SIGPLAN-SIGACT Symposium on Principles of Programming
    Languages |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | PLDI | ACM SIGPLAN Conference on Programming Language Design & Implementation
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | OOPSLA | Conference on Object-Oriented Programming Systems, Languages,
    and Applications |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | CGO | Code Generation and Optimization |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | PACT | International Conference on Parallel Architectures and Compilation
    Techniques |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | TOPLAS | ACM Transactions on Programming Languages & Systems |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | TACO | ACM Transactions on Architecture and Code Optimization |'
  prefs: []
  type: TYPE_TB
- en: '| AI | 1 | NeurIPS | Annual Conference on Neural Information Processing Systems
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | ICML | International Conference on Machine Learning |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | ICLR | International Conference on Learning Representations |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | AAAI | AAAI Conference on Artificial Intelligence |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | IJCAI | International Joint Conference on Artificial Intelligence |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | SIGKDD | ACM Knowledge Discovery and Data Mining |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | WWW | The Web Conference |'
  prefs: []
  type: TYPE_TB
- en: '| NLP | 1 | ACL | Annual Meeting of the Association for Computational Linguistics
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | EMNLP | Conference on Empirical Methods in Natural Language Processing
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | NAACL | The Annual Conference of the North American Chapter of the Association
    for Computational Linguistics |'
  prefs: []
  type: TYPE_TB
- en: '| Security | 1 | CCS | ACM Conference on Computer and Communications Security
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | S&P | IEEE Symposium on Security and Privacy |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | USENIX Security | Usenix Security Symposium |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | NDSS | ISOC Network and Distributed System Security Symposium |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | TIFS | IEEE Transactions on Information Forensics and Security |'
  prefs: []
  type: TYPE_TB
- en: 'Transformer. Since sequential computation in RNNs inhibits parallelization,
    recently, a new state-of-the-art network Transformer has been designed for parallel
    processing of sequences (Vaswani et al., [2017](#bib.bib232)). The Transformer
    is mainly composed of multiple self-attention blocks. When feeding a sequence
    of code tokens $c_{i}=\{x_{1},x_{2},\ldots,x_{|c_{i}|}\}$ into Transformer, the
    Transformer block at layer $l$ will produce a sequence of hidden states for each
    code token, i.e., $\mathbf{H}^{l}=[\mathbf{h}_{1}^{l},\mathbf{h}_{2}^{l},\ldots,\mathbf{h}_{|c_{i}|}^{l}]$.
    For each layer, the layer representation $\mathbf{H}^{l}$ is computed by the $l$-th
    layer Transformer block $\mathbf{H}^{l}=\mathrm{Transformer}_{l}(\mathbf{H}^{l-1})$,
    where $l\in\{1,2,\ldots,L\}$. In each Transformer block, multiple self-attention
    heads are used to aggregate the output vectors of the previous layer. A general
    attention mechanism can be formulated as the weighted sum of the value vector
    $\mathbf{V}$, using the query vector $\mathbf{Q}$ and the key vector $\mathbf{K}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\alpha(\mathbf{Q},\mathbf{K},\mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d_{\text{model}}}}\right)\cdot\mathbf{V}\,,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $d_{\rm model}$ represents the dimension of hidden representations. For
    self-attention, $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are mappings of previous
    hidden representations by different linear functions, i.e., $\mathbf{Q}=\mathbf{H}^{l-1}\mathbf{W}_{Q}^{l}$,
    $\mathbf{K}=\mathbf{H}^{l-1}\mathbf{W}_{K}^{l}$, and $\mathbf{V}=\mathbf{H}^{l-1}\mathbf{W}_{V}^{l}$,
    respectively. At last, the encoder produces a final contextual representation
    $\mathbf{H}^{L}=[\mathbf{h}^{L}_{1},\ldots,\mathbf{h}^{L}_{n}]$, which is obtained
    from the last Transformer block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph Neural Network. The Graph Neural Networks (GNNs) take a graph as input
    and the goal is to learn the representations of each node by recursively updating
    until convergence, which aggregates the information of each neighborhood node.
    Let $\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{A})$ denote a graph, where $\mathcal{V}$
    is a set of nodes, $\mathcal{E}$ is a set of edges, and $\mathbf{A}$ is a adjacency
    matrix. In a graph, let $v_{i}\in\mathcal{V}$ denote a node and $e_{ij}=(v_{i},v_{j})\in\mathcal{E}$
    denote an edge. For graph $\mathcal{G}$, the hidden representation $\mathbf{h}_{v}$
    for each node $v$ can be updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\mathbf{h}_{v,t+1}=f(\ell_{v},\ell_{e},\mathbf{h}_{v^{\prime},t},\ell_{v^{\prime}})\,,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $f$ is a local transition function (shared among all nodes), which updates
    the node state according to the input neighborhood. $\ell_{v}$ denotes the label
    attributes of node $v$, $\ell_{e}$ denotes the label attributes of the corresponding
    edges of node $v$, $\mathbf{h}_{v^{\prime},t}$ denotes the hidden representations
    of node $v$’s neighbors at time step $t$, and $\ell_{v^{\prime}}$ denotes the
    label attributes of node $v$’s neighbors. For the options of $f$, there are several
    approaches attempting to use the gate mechanism like GRU \citesecondarychung2015gated
    or LSTM \citesecondaryhochreiter1997long in the propagation step to diminish the
    restrictions in the former GNN models, thereby improving the long-term propagation
    of information across the graph structure. If the local transition function $f$
    is GRU, the model is called Gated Graph Neural Network (GGNN) \citesecondaryli2015gated.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Public Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table 7. A summary of datasets that have been used for code intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Language | Train | Valid | Test | Doc. | Granularity | AST |'
  prefs: []
  type: TYPE_TB
- en: '| CSN-Java | Java | 454,451 | 15,328 | 26,909 | Y | Function | Y |'
  prefs: []
  type: TYPE_TB
- en: '| CSN-JavaScript | JavaScript | 123,889 | 8,253 | 6,483 | Y | Function | Y
    |'
  prefs: []
  type: TYPE_TB
- en: '| CSN-Python | Python | 412,178 | 23,107 | 22,176 | Y | Function | Y |'
  prefs: []
  type: TYPE_TB
- en: '| CSN-PHP | PHP | 523,712 | 26,015 | 28,391 | Y | Function | Y |'
  prefs: []
  type: TYPE_TB
- en: '| CSN-Go | Go | 317,832 | 14,242 | 14,291 | Y | Function | Y |'
  prefs: []
  type: TYPE_TB
- en: '| CSN-Ruby | Ruby | 48,791 | 2,209 | 2279 | Y | Function | Y |'
  prefs: []
  type: TYPE_TB
- en: '| CoSQA-QA | Java, Python | 20,000 | 604 | - | Y | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CoSQA-Search | Java, Python | 19,604 | 500 | 500 | Y | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| Py150 | Python | 100,000 | - | 50,000 | N | File | Y |'
  prefs: []
  type: TYPE_TB
- en: '| python-code-docstring | Python | 55,538 | 18,505 | 18,502 | Y | Function
    | Y |'
  prefs: []
  type: TYPE_TB
- en: '| DeepCom-Java | Java | 69,708 | 8,714 | 8,714 | Y | Function | Y |'
  prefs: []
  type: TYPE_TB
- en: '| CCSD | C | 1,275,937 | 425,312 | 425,312 | Y | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| Typescript | Typescript | 49,850 | 7,854 | 4,650 | N | File | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeNet | mainly C++ | 1,235,000 | - | - | N | File | N |'
  prefs: []
  type: TYPE_TB
- en: '| JuICe | Python | 1,518,049 | 1,744 | 1,981 | Y | File | N |'
  prefs: []
  type: TYPE_TB
- en: '| ProGraML | C, C++, Fortran, Haskell, Swift | 250,428 | - | - | N | File |
    N |'
  prefs: []
  type: TYPE_TB
- en: '| FunCom | Java | 1,937,136 | 106,153 | 105,832 | Y | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CoDesc | Java | 3,369,218 | 421,149 | 421,149 | Y | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| APPS | Python | 117,232 | - | 115,212 | N | File | N |'
  prefs: []
  type: TYPE_TB
- en: '| AVATAR | Python, Java | 40,423 | 848 | 1,699 | N | File | N |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorrent | Python | 2,273,157 | 284,145 | 284,144 | Y | Function | Y |'
  prefs: []
  type: TYPE_TB
- en: '| StaQC | Python, SQL | 267,065 | - | - | N | File | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeQA-Python | Python | 56,084 | 6,999 | 6,999 | N | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeQA-Java | Java | 95,777 | 11,999 | 11,999 | N | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeXGLUE-BCB | Java | 901,028 | 415,416 | 415,416 | N | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeXGLUE-POJ104 | C/C++ | 32,000 | 8,000 | 12,000 | N | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeXGLUE-Devign | C | 21,854 | 2,732 | 2,732 | N | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeXGLUE-Bugs2Fix | Java | 52,364 | 6,545 | 6,545 | N | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeXGLUE-CodeTrans | Java/C# | 10,295 | 499 | 1,000 | N | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeXGLUE-WebQuery | Python | 251,820 | 9,604 | - | Y | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeXGLUE-Concode | Java | 100,000 | 2,000 | 2,000 | Y | Function | N |'
  prefs: []
  type: TYPE_TB
- en: '| CodeXGLUE-MicoDocs | mainly English | 155,926 | 4,000 | 4,00 | Y | Docs |
    N |'
  prefs: []
  type: TYPE_TB
- en: Many datasets have been collected for different tasks of code intelligence.
    In the literature, many pre-processing steps have been conducted before using
    the dataset for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodeSearchNet (CSN)^(14)^(14)14https://github.com/github/CodeSearchNet \citesecondaryhusain2019codesearchnet2
    is a dataset collected from GitHub, originally for code search. It contains about
    6 million functions, where 2 millions of them are annotated with natural language
    descriptions, covering six programming languages (i.e., Go, Java, JavaScript,
    PHP, Python, and Ruby). Since the code snippets in this dataset are paired with
    natural language descriptions and have the characteristic of multi-linguality,
    this dataset can also be used for code summarization and cross-language-related
    tasks. In addition, this dataset has been used for large-scale pre-training, attributed
    to its large scale.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoSQA^(15)^(15)15https://github.com/Jun-jie-Huang/CoCLR \citesecondaryhuang2021cosqa2
    is a dataset of paired natural-language queries and code snippets designed for
    code search and question answering, where the queries are from the real search
    logs of Microsoft Bing and the code snippets are from GitHub. It consists of 20,604
    pairs of data samples and each pair is annotated by at least 3 crowd-sourced workers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Py150^(16)^(16)16https://eth-sri.github.io/py150 \citesecondaryraychev2016probabilistic2
    is a collection of 150$k$ Python source code files from GitHub, which has been
    widely used for evaluating code completion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: python-code-docstring^(17)^(17)17https://github.com/wanyao1992/code_summarization_public \citesecondarybarone2017parallel2,wan2018improving2
    is a dataset of parallel Python code snippets with corresponding descriptions,
    which has been widely adopted for code summarization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeepCom-Java^(18)^(18)18https://github.com/xing-hu/DeepCom \citesecondaryhu2018deep2
    is a dataset collected from the open source repositories in GitHub. In this dataset,
    only repositories are implemented in Java, and those with at least 10 stars are
    considered. Finally, 588,108 pairs of Java methods and their corresponding Javadoc
    are obtained.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CCSD-benchmark-for-code-summarization^(19)^(19)19https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization \citesecondaryliu2020retrieval2
    is a C benchmark dataset for code summarization, which is crawled from GitHub,
    and contains 95$k$+ functions as well as natural-language descriptions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typescript^(20)^(20)20https://github.com/DeepTyper/DeepTyper \citesecondaryhellendoorn2018deep2
    is a dataset of Typescript code snippets, which contain the type information of
    variables and can be used for type inference evaluation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodeNet^(21)^(21)21https://github.com/IBM/Project_CodeNet \citesecondarypuri2021project2
    is a large-scale dataset released by IBM for code intelligence, consisting of
    14 million code samples and about 500 million lines of code in 55 different programming
    languages.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JuICe^(22)^(22)22https://github.com/rajasagashe/juice \citesecondaryagashe2019juice2
    is large-scale dataset for open-domain, context-based code generation, which consists
    of 1.5 million examples with a curated test set of 3.7$k$ instances from online
    programming assignments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ProGraML^(23)^(23)23https://github.com/ChrisCummins/ProGraML \citesecondarycummins2020programl2
    is a benchmark dataset that converts programs into graphs based on LLVM-IRs, consisting
    of 250$k$ LLVM-IR files covering six programming languages. It can be used for
    evaluating GNNs for program optimization and analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FunCom^(24)^(24)24http://leclair.tech/data/funcom/ \citesecondaryleclair2019recommendations2
    provides a curated dataset for code summarization, which excludes auto-generated
    source code files, as well all functions that are lack of associated comments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoDesc^(25)^(25)25https://github.com/code-desc/CoDesc \citesecondaryhasan2021codesc2
    is a large-scale parallel dataset composed of 4.2 million Java methods and natural
    language descriptions, based on CodeSearchNet, DeepCom, CONCODE and FunCom, with
    comprehensive data cleaning processes and manually check.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APPS^(26)^(26)26https://github.com/hendrycks/apps \citesecondaryhendrycks2021measuring
    is a benchmark dataset for code generation in Python based on natural language
    specification, consisting of 10,000 problems at various levels of difficulty,
    covering simple introductory problems, interview-level problems, and coding competition
    challenges.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AVATAR^(27)^(27)27https://github.com/wasiahmad/AVATAR \citesecondaryahmad2021avatar2
    is a parallel corpus for Java-Python program translation. It is composed of 8,475
    programming problems and their solutions implemented in Java and Python, collected
    from open source programming contest sites (e.g., AtCoder, Google Code Jam, and
    Codeforces), and online platforms (e.g., GeeksforGeeks, LeetCode, and Project
    Euler).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorrent \citesecondarybahrami2021pytorrent2 is a Python dataset of paired
    source code and natural-language comment, which is similar to the CodeSearchNet
    dataset. The difference is that it is collected from Python libraries such as
    PyPI and Anaconda packages, rather than GitHub projects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StaQC \citesecondaryyao2018staqc2 is a collection of about 148$k$ Python and
    120$k$ SQL question-code pairs, mined from Stack Overflow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodeQA^(28)^(28)28https://github.com/jadecxliu/CodeQA \citesecondaryliu2021codeqa2
    is a free-form question-answering dataset for code comprehension, in which a code
    snippet and a question are given, and a textual answer is required to be generated.
    It is composed of a Java dataset with 119,778 question-answer pairs and a Python
    dataset with 70,085 question-answer pairs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodeXGLUE^(29)^(29)29https://github.com/microsoft/CodeXGLUE \citesecondarylu2021codexglue2
    is a benchmark dataset for code understanding and generation, which can support
    multiple code intelligence tasks, such as code summarization, code completion,
    code search, and program translation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix D Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have evaluated the performance of our proposed model based on four widely-used
    evaluation criteria in the area of neural machine translation, i.e., BLEU \citesecondarypapineni2002bleu,
    METEOR \citesecondarybanerjee2005meteor, and ROUGE \citesecondarylin2004rouge.
    BLEU measures the average n-gram precision on a set of reference sentences, with
    a penalty for short sentences. METEOR is recall-oriented and measures how well
    our model captures the contents from the references in our output. ROUGE considers
    sentence-level structure similarity and identifies the longest co-occurring in
    sequence n-grams automatically.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU (Bilingual Evaluation Understudy) is a classical evaluation metric in neural
    machine translation, which measures the average n-gram precision on a set of reference
    sentences, with a penalty for short sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $p_{n}=\frac{\sum_{w_{n}\in a}\min\left(c_{a}(w_{n}),\underset{j=1,\cdots,&#124;n&#124;}{\max}c_{b_{j}}(w_{n})\right)}{\sum_{w_{n}\in
    a}c_{a}(w_{n})},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $a$ and $b$ represent a candidate sentence and its corresponding reference,
    respectively. $w_{n}$ represents the n-gram word, $c_{a}(w_{n})$ represents the
    count of n-gram $w_{n}$ in sentence $a$. Since $p_{n}$ encourages to prediction
    of short sentences, thus, we introduce a Brevity Penalty (BP) defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\operatorname{BP}=\left\{\begin{matrix}1&amp;if\ c>r\\ e^{(1-\frac{r}{c})}&amp;if\
    c\leq r,\end{matrix}\right.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $r$ is the reference sentence length, $c$ the length of the candidate
    sentence. Finally, the BLEU is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\operatorname{BLEU}=\operatorname{BP}*\exp\left(\sum_{n=1}^{N}\alpha_{n}\log
    p_{n}\right),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $N=1,2,3,4$, $p_{n}$ is the n-gram precision of n-grams up to $N$, $\alpha_{n}$
    is positive weight for each gram.
  prefs: []
  type: TYPE_NORMAL
- en: 'METEOR is recall-oriented and measures how well our model captures content
    from the references in our output. METEOR matches candidate sentences and reference
    sentence words one by one, and then calculates the harmonic average of their accuracy
    and recall rates between the corresponding candidate sentence and the reference
    sentence. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\operatorname{METEOR}=\underset{j=1,\cdots,&#124;b&#124;}{\max}\left(\frac{10PR}{R+9P}\right)\left(1-\frac{1}{2}\left(\frac{\#\text{chunks}}{\#\text{matched\
    uni-gram}}\right)\right),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $P$ = uni-gram precision, $R$ =uni-gram recall, $\#chunks$ is the number
    of matched chunks between two sentences. $\#\operatorname{matched\ unigram}$ is
    the number of matched uni-grams between the candidate sentence and the reference
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'ROUGE is one of the general metrics in the automatic text summarization, it
    is a measurement based on the longest common sub-sequence (Longest Common Sub-sequence,
    LCS). ROUGE takes into account sentence-level structure similarity naturally and
    identifies the longest co-occurring in sequence n-grams automatically. It is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\operatorname{ROUGE}=\frac{\sum_{j=1}^{&#124;b&#124;}\sum_{w_{n}\in
    b_{j}}\min\left(c_{a}(w_{n}),c_{b_{j}}(w_{n})\right)}{\sum_{j=1}^{&#124;b&#124;}\sum_{w_{n}\in
    b_{j}}c_{b_{j}}(w_{n})},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'MRR (Mean Reciprocal Rank) serves as a metric for assessing processes that
    generate lists of potential responses to a set of queries, arranged based on their
    likelihood of correctness. This metric computes the average reciprocal ranks for
    the outcomes associated with a given set of queries. The reciprocal rank of a
    response to a query corresponds to the inverse of the rank assigned to the first
    correct answer within the list. The MRR is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\operatorname{MRR}=\frac{1}{Q}\sum_{i=1}^{Q}\frac{1}{rank_{i}}\,,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $Q$ is the number of queries, and $rank_{i}$ is the rank position of the
    first correct answer for the $i$-th query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy stands as a pivotal metric for evaluating the efficacy of a classification
    model. This metric is delineated by the ratio of correct predictions to the total
    number of predictions generated by the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\operatorname{Accuracy}=\frac{\text{Number of Correct Predictions}}{\text{Total
    Number of Predictions Made}}\,.$ |  |'
  prefs: []
  type: TYPE_TB
- en: \bibliographystylesecondary
  prefs: []
  type: TYPE_NORMAL
- en: unsrt \bibliographysecondaryrefappendix
  prefs: []
  type: TYPE_NORMAL
