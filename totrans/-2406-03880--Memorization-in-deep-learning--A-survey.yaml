- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:31:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2406.03880] Memorization in deep learning: A survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.03880](https://ar5iv.labs.arxiv.org/html/2406.03880)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Memorization in deep learning: A survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jiaheng Wei, Yanjun Zhang, Leo Yu Zhang,  Ming Ding,  Chao Chen,  Kok-Leong
    Ong, Jun Zhang,  Yang Xiang Manuscript received June 7, 2024.Jiaheng Wei, Chao
    Chen, and Kok-Leong Ong are with the School of Accounting, Information System
    and Supply Chain, RMIT University, Melbourne, VIC 3001, Australia (e-mail: s3986349@student.rmit.edu.au;
    chao.chen@rmit.edu.au; kok-leong.ong2@rmit.edu.au).Yanjun Zhang is with the School
    of Computer Science, University of Technology Sydney, Sydney, NSW 2007, Australia
    (e-mail: Yanjun.Zhang@uts.edu.au).Leo Yu Zhang is with the School of Information
    and Communication Technology, Griffith University, Brisbane, QLD 4215, Australia
    (e-mail: leo.zhang@griffith.edu.au).Ming Ding is with Data61, CSIRO, Sydney, NSW
    2015, Australia (e-mail: ming.ding@data61.csiro.au).Jun Zhang and Yang Xiang are
    with the School of Science, Computing and Engineering Technologies, Swinburne
    University of Technology, Melbourne, VIC 3122, Australia (e-mail: junzhang@swin.edu.au;
    yxiang@swin.edu.au). [0009-0003-7180-4268](https://orcid.org/0009-0003-7180-4268
    "ORCID identifier") [0000-0001-5611-3483](https://orcid.org/0000-0001-5611-3483
    "ORCID identifier") [0000-0001-9330-2662](https://orcid.org/0000-0001-9330-2662
    "ORCID identifier") [0000-0002-3690-0321](https://orcid.org/0000-0002-3690-0321
    "ORCID identifier") [0000-0003-1355-3870](https://orcid.org/0000-0003-1355-3870
    "ORCID identifier") [0000-0003-4688-7674](https://orcid.org/0000-0003-4688-7674%0A
    "ORCID identifier") [0000-0002-2189-7801](https://orcid.org/0000-0002-2189-7801
    "ORCID identifier") [0000-0001-5252-0831](https://orcid.org/0000-0001-5252-0831
    "ORCID identifier")'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Learning (DL) powered by Deep Neural Networks (DNNs) has revolutionized
    various domains, yet understanding the intricacies of DNN decision-making and
    learning processes remains a significant challenge. Recent investigations have
    uncovered an interesting memorization phenomenon in which DNNs tend to memorize
    specific details from examples rather than learning general patterns, affecting
    model generalization, security, and privacy. This raises critical questions about
    the nature of generalization in DNNs and their susceptibility to security breaches.
    In this survey, we present a systematic framework to organize memorization definitions
    based on the generalization and security/privacy domains and summarize memorization
    evaluation methods at both the example and model levels. Through a comprehensive
    literature review, we explore DNN memorization behaviors and their impacts on
    security and privacy. We also introduce privacy vulnerabilities caused by memorization
    and the phenomenon of forgetting and explore its connection with memorization.
    Furthermore, we spotlight various applications leveraging memorization and forgetting
    mechanisms, including noisy label learning, privacy preservation, and model enhancement.
    This survey offers the first-in-kind understanding of memorization in DNNs, providing
    insights into its challenges and opportunities for enhancing AI development while
    addressing critical ethical concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, deep neural networks, memorization phenomenon, forgetting phenomenon,
    privacy.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the development of artificial intelligence (AI), deep learning (DL) has emerged
    as an effective solution for various complex tasks like text generation [[1](#bib.bib1)],
    speech translation [[2](#bib.bib2)], etc. Deep neural network (DNN) as the main
    model architecture has been widely used in numerous innovative applications such
    as autonomous vehicles [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)] and medical
    diagnosis [[6](#bib.bib6), [7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ad7d875d03906d68abb8d3672cf3711.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd0d52b1f91031c4f2b4367e98b3d312.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: The Direct Memorization Effect. In (a), we use an image generator
    to describe memorization. The upper part demonstrates the memorization effect
    and the lower part represents the common generation. For (b), the memorization
    effect has two different levels: Example Memorization and Model Memorization.'
  prefs: []
  type: TYPE_NORMAL
- en: However, it is still challenging to understand how DNNs make decisions and what
    they learn from the training data. Though researchers believe DNNs can learn patterns
    in the training data to achieve success in assigned tasks, a recent study found
    that DNNs are able to memorize the entire randomly labeled training dataset [[8](#bib.bib8)],
    which illustrates that properties of the model family, or the regularization techniques
    fail to explain why large neural networks generalize well. DNNs may memorize particular
    features from training data instead of learning patterns to perform specific tasks.
    This attracts the community to explore the memorization mechanism and prompts
    researchers to rethink the generalization in DNNs. Additionally, this memorization
    phenomenon raises concerns about the security of AI because of potential privacy
    leakage risks and vulnerability against malicious attacks. Furthermore, the training
    dataset collected from the real world may contain significant noise and bias,
    and memorized data in DNNs may keep the noise and bias, impairing the usability
    and fairness of the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, numerous papers have found the memorization effects that neural networks
    may memorize some training data in training with gradient descent [[9](#bib.bib9),
    [10](#bib.bib10), [8](#bib.bib8), [11](#bib.bib11), [12](#bib.bib12)]. Current
    memorization studies mainly focus on two domains: the behaviors in standard training
    and the security/privacy risks. We summarize explicit memorization definitions
    in literature based on generalization and security/privacy. However, there is
    a lack of a widely adopted definition for memorization, making describing and
    discussing the memorization concept challenging. Many relevant works provide inconsistent,
    sometimes contradictory, definitions of memorization. Especially, many works directly
    apply the word "memorization" as the synonymous words of "learning" and "fitting".
    Thus, we adopt the following terms for facilitating discussion: Memorization Learning
    refers to DNNs learning specific details or particular features of examples, while
    common Pattern Learning indicates DNNs learning the common patterns or generalized
    features of the data distribution.In Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ I
    Introduction ‣ Memorization in deep learning: A survey"), we use a large language
    model to illustrate memorization learning and pattern learning. We use the word
    "generalization" to define the model performance on the new, unseen data. Suppose
    there is no extra explanation, all terms like "memorization", "memorization effect",
    and "memorization phenomenon" point to memorization learning. Moreover, we think
    pattern learning and memorization learning together constitute the learning path
    of DNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b81d839cb98b9b1107badd97640856bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Paper Structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, memorization is a complex concept that requires us to consider it
    at various levels. In our opinion, memorization learning and pattern learning
    operate at a feature level. However, understanding the features of neural networks
    directly is exceedingly difficult for humans. Hence, we mainly study memorization
    at the example level and model level as illustrated in Figure  [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ I Introduction ‣ Memorization in deep learning: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, example memorization and model memorization indicate the objects
    of study are examples and models. Consequently, memorization concepts at different
    levels inspire distinct memorization evaluation methods. Example memorization
    evaluation tries to ensure if an example is memorized including differential evaluation
    and probabilistic evaluation. On the other hand, model memorization evaluation
    measures how much models memorize or the memorization ability of models. We summarize
    various approaches to three main methods: noisy label evaluation, recurrence evaluation,
    and extraction evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: After the definitions and evaluation methods, we systematically review related
    literature. For memorization behaviors in standard training, existing studies
    investigate the relationships between the memorization effect and training data,
    training stages, model architecture, overfitting, regularization, and other factors.
    One study [[13](#bib.bib13), [11](#bib.bib11)] provides an interesting conclusion
    that memorization learning improves the generalization of models because the memorization
    of rare and atypical examples actually contributes to the generalization performance
    of similar rare subgroups, which is adverse to some early opinions. Additionally,
    some evidence [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)] shows overfitting
    is not responsible for memorization. Memorization is a persistent process in training.
    For security/privacy risks, the memorized particular features become multiple
    risk sources like membership inference risks and extraction risks, enabling attackers
    to exploit the memorization mechanism to invade privacy and violate the security
    rules of DNNs. In contrast, some risks like adversary attack risks are not obviously
    related to the memorization mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: On a related aspect, the forgetting phenomenon is closely connected to the memorization
    effect. Thus, we also discuss and review the forgetting effect. We explore useful
    forgetting definitions and evaluation methods and summarize relevant forgetting
    phenomenon studies.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we also review numerous applications utilizing the memorization
    and forgetting mechanisms. These applications like noisy label learning, example
    enhancement, privacy audit and protection, memorization architecture, and model
    editing, take advantage of different properties of memorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we attempt to organize the memorization definitions and evaluation
    methods and review relevant literature, aiming to build a scientific and effective
    framework and help the readers understand the memorization mechanism and its influence
    on model training and system security. Additionally, we also explore the forgetting
    phenomenon and illustrate some potential applications of the memorization and
    forgetting mechanisms. We hope this survey can help the research community have
    a general understanding of the memorization phenomenon. The key contributions
    of this survey are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing definitions. We propose a framework to organize all existing memorization
    definitions and evaluation methods. We also explain the scope and limitations
    of these definitions and evaluation methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comprehensive review. We review relevant memorization studies on its behaviors
    in the standard training and security/privacy risks. Moreover, we also investigate
    its connection with the forgetting studies and some possible applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussion. In this survey, we thoroughly discuss the memorization mechanism
    and how memorization effects can boost other relevant technologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The survey is organized into the following sections, each focusing on a different
    aspect of memorization in deep learning as we present in Figure [2](#S1.F2 "Figure
    2 ‣ I Introduction ‣ Memorization in deep learning: A survey"). Section [II](#S2
    "II Memorization Definition ‣ Memorization in deep learning: A survey") provides
    existing memorization definitions and Section [III](#S3 "III Memorization Evaluations
    ‣ Memorization in deep learning: A survey") lists the memorization evaluation
    methods based on various levels. Section [IV](#S4 "IV Memorization in DNN Training
    ‣ Memorization in deep learning: A survey") delves into the memorization behaviors,
    presenting how memorization affects each training component and its relationship
    with overfitting, data augmentation, and regularization technology. Section [V](#S5
    "V Underlying Risks of Memorization Learning ‣ Memorization in deep learning:
    A survey") presents a review of memorization-associated risks that memorized particular
    features enhance privacy risks. Section [VI](#S6 "VI Forgetting Research ‣ Memorization
    in deep learning: A survey") explores the forgetting phenomenon, which is the
    opposite of memorization. Section [VII](#S7 "VII Application ‣ Memorization in
    deep learning: A survey") demonstrates the underlying application of the memorization
    effects including the noisy label learning, example enhancement technology, privacy
    audit and protection, and memorization architecture. Section [VIII](#S8 "VIII
    Discussion and Future Research ‣ Memorization in deep learning: A survey") comprehensively
    discusses the memorization phenomenon’s influence on standard training and security/privacy
    risks and how it enlightens and explains other technologies or phenomena.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4b281e9f4039711192e303a3b24c5ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Memorization Definitions and Evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Main Memorization Definitions'
  prefs: []
  type: TYPE_NORMAL
- en: '| Domain | Name | Reference | Research Question | Description |'
  prefs: []
  type: TYPE_TB
- en: '| Generalization | Label Memorization | Feldman et al. 2020 [[13](#bib.bib13),
    [11](#bib.bib11)] | Studying memorization effect of long-tailed examples. | This
    definition provides a universal understanding of memorization and distinguishes
    memorization learning and pattern learning effectively. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Exact Memorization | Tirumala et al. 2022 [[16](#bib.bib16)] | Studying
    underlying training and memorization dynamics of very large language models. |
    The exact memorization actually represents accuracy that cannot identify memorization
    learning in the language model. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Counterfactual Memorization | Zhang et al. 2021 [[17](#bib.bib17)] | Studying
    counterfactual memorization in language models. | This concept extends label memorization
    to unsupervised tasks. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Benign Memorization | Anagnostidis et al. 2023 [[18](#bib.bib18)] | Studying
    learned features with data augmentation. | Benign memorization describes DNNs
    can learn useful features on the randomly labeled dataset with data augmentation
    technology. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Corrupt Label Memorization based on Neural Collapse | Nguyen et al. 2023 [[19](#bib.bib19)]
    | Studying how corrupt label data impacts neural collapse. | The definition attempts
    to explain the influence of corrupt label data in neural collapse. |'
  prefs: []
  type: TYPE_TB
- en: '| Security and Privacy | Unintended Memorization | Carlini et al. 2019 [[14](#bib.bib14)]
    | Studying unintended memorization in training. | Unintended memorization definites
    the features that are irrelevant to the main task but memorized by DNNs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | $k$-Eidetic Memorization | Carlini et al. 2021 [[9](#bib.bib9)] | Studying
    privacy leakage in language models. | This memorization definition helps analyze
    the possibly memorized data based on repetition times. |'
  prefs: []
  type: TYPE_TB
- en: II Memorization Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Memorization is a vague and abstract concept, and difficult to obverse during
    the training of neural networks. Thus, previous studies did not provide a clear
    and uniform definition. Based on relevant research, we find that the motivations
    for studying the memorization phenomenon are its impact on generalization and
    the concerns about privacy and security risks. In this section, we outline existing
    definitions of memorization within the contexts of the generalization domain and
    security domain as Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Memorization in
    deep learning: A survey") and Figure [3](#S1.F3 "Figure 3 ‣ I Introduction ‣ Memorization
    in deep learning: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Memorization Definitions in Generalization Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: II-A1 Label Memorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Intuitively, there would exist an obvious disparity when evaluating a data point
    on a model between the model memorizing the data point and not. Feldman [[13](#bib.bib13)]
    introduces the label memorization concept to describe the disparity in supervised
    learning tasks. Label memorization differentially defines what memorizing a label
    of a point in the dataset means.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Label Memorization for Supervised Tasks. Given a training algorithm $A$ that
    maps a training dataset $D$ to a trained model $f$, the amount of memorization
    by $A$ on example $(x_{i},y_{i})\in D$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $D^{\backslash i}$ denotes the dataset $D$ with $(x_{i},y_{i})$ removed.
  prefs: []
  type: TYPE_NORMAL
- en: This definition provides a universal understanding of memorization and distinguishes
    generalized examples effectively. The definition actually approaches the nature
    of memorization that memorized examples cannot rely on generalized features.
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Exact Memorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exact memorization proposed by Tirumala et al. [[16](#bib.bib16)], is used to
    perform a large-scale study of the dynamics of memorization over training. Additionally,
    this definition is only applied to the language models.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exact Memorization. Let $V$ denote the vocabulary size. Let $C$ denote a set
    of contexts, which can be thought of as a list of tuples $(s,y)$ where $s$ is
    an input context (incomplete block of text) and $y$ is the index of the ground
    truth token in the vocabulary that completes the block of text. Let $S$ denote
    the set of input contexts, and let $f:S\rightarrow\mathbb{R}^{V}$ denote a language
    model. A context $c=(s,y)\in C$ is memorized if $\arg\max(f(s))=y$. For a given
    set of contexts $C$ (i.e., a given training dataset), the proportion of memorized
    contexts can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{mem(f)}=\frac{\sum_{(s,y)\in C}1\{\arg\max(f(s))=y\}}{&#124;C&#124;}.$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Based on the formula, we know that the exact memorization actually represents
    accuracy since it just measures the average number that predicted token matches
    the ground truth token in the contexts. Thus, this definition is not related to
    the memorization phenomenon and cannot describe the memorization.
  prefs: []
  type: TYPE_NORMAL
- en: II-A3 Counterfactual Memorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Counterfactual memorization is extended from label memorization to unsupervised
    tasks. Zhang et al. [[17](#bib.bib17)] introduce the definition, applying it to
    quantify the episodic memorization in language models.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Counterfactual Memorization. Given a training algorithm $A$ that maps a training
    dataset $D$ to a trained model $f$, and a measure $M$ which measures the performance
    of $x_{i}$ on $f$, the amount of memorization by $A$ on example $x_{i}\in D$ measured
    with $M$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $M$ can be per-token accuracy that $f$ predicts the next token based on
    the preceding tokens, then measures the 0-1 loss.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, counterfactual memorization is a universal version of label memorization
    and this differential memorization definition can empirically evaluate memorization
    in various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: II-A4 Benign Memorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Benign memorization describes the phenomenon that neural networks can learn
    useful features on the randomly labeled dataset with data augmentation technology [[18](#bib.bib18)].
    This work regards the general neural network structure as an encoder-projector
    pair and trains the pair on an augmented noisy dataset. If the accuracy of $k$NN
    probing at the embedding vectors of the encoder increases over probing at initialization,
    this is benign memorization.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Benign Memorization. Here are two datasets, $D:={(x_{i},y_{i})}^{n}_{i=1}$ denotes
    the original clean dataset and $\tilde{D}={(x_{i},\tilde{y}_{i})}^{n}_{i=1}$ its
    randomly labeled version. We call an encoder-projector pair $(h_{\phi*},g_{\psi*})$
    a memorization of $\tilde{D}$, if $f_{*}$ perfectly fits $\tilde{D}$. Moreover,
    we call $(h_{\phi*},g_{\psi*})$ a malign memorization if additionally, probing
    of $h_{\phi*}$ on $D$ does not improve over probing at initialization. On the
    contrary, we call $(h_{\phi*},g_{\psi*})$ a benign memorization of $\tilde{D}$
    if probing of $h_{\phi*}$ on $D$ outperforms probing at initialization.
  prefs: []
  type: TYPE_NORMAL
- en: This definition focuses on the generalization performance when training on randomly
    labeled datasets. Benign memorization occurs if the encoder learns generalized
    features. Therefore, this memorization definition is auxiliary to explain noisy
    label learning rather than a general memorization definition.
  prefs: []
  type: TYPE_NORMAL
- en: II-A5 Corrupt Label Memorization based on Neural Collapse
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Empirical evidence indicates that the memorization of noisy data points may
    lead to degradation (dilation) of the neural collapse. Nguyen et al. [[19](#bib.bib19)]
    purpose memorization-dilation model and define memorization based on neural collapse
    under corrupt label training data.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Corrupt Label Memorization based on Neural Collapse. For a given and labeled
    dataset $D$ with label noise $\eta$ and $K$ categories, if $f$ is a feature extractor,
    denoting the feature representations $f(x_{i}^{k})$ of the example $x_{i}^{k}$
    by $h_{i}^{k}$. Under neural collapse, any $h_{i}^{k}$ will collapse to a single
    feature representation $h^{k}$. We denote the set of corrupted instances of class
    $k$ by $[\tilde{I}^{k}]$. Memorization can be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{mem}:=\sum\limits_{k=1}^{K}\sum\limits_{i\in[\tilde{I}^{k}]}&#124;&#124;h_{i}^{k}-h_{*}^{k}&#124;&#124;$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $h_{*}^{k}$ denotes the mean of (unseen) test instances belonging to class
    $k$.
  prefs: []
  type: TYPE_NORMAL
- en: The DNN under neural collapse intends to map examples with the same ground truth
    label to a single representation due to the similarity in input features. Therefore,
    instances of the same ground truth but with randomly corrupted labels lack predictable
    characteristics, making it challenging for the network to distinguish and separate
    them in a manner that can be generalized effectively. Thus, when the network is
    still able to successfully separate such instances, it indicates that the network
    has memorized the feature representations of the corrupted instances present in
    the training set. This definition expresses the memorization of noisy examples
    but only applies to the problem domain of neural collapse. The scope of the definition
    is limited.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Memorization Definitions in Security Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: II-B1 Unintended Memorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unintended memorization mainly serves to privacy concerns. The concept was first
    proposed by Carlini et al. [[14](#bib.bib14)] when they found that LLMs may memorize
    some sensitive information like social-security numbers unintentionally. Generally,
    such memorization is unnecessary for achieving generalization and they give a
    simple unintended memorization definition.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unintended Memorization. Unintended memorization occurs when trained neural
    networks may expose the presence of out-of-distribution training data and the
    training data is irrelevant to the learning task and definitely does not contribute
    to improving model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the differential memorization definitions, the unintended definition
    focuses specifically on the memorization of out-of-distribution and sensitive
    data. These data can also be considered as secrets, as they should not be revealed
    or disclosed by the trained neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: II-B2 $k$-Eidetic Memorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Carlini et al. [[9](#bib.bib9)] introduces the concept of $k$-Eidetic Memorization
    for language tasks. The parameter $k$ represents the count of distinct training
    examples that contain a specific string.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: $k$-Eidetic Memorization. A string $s$ is $k$-eidetic memorized (for $k\geq
    1$) by an LM $f_{\theta}$ if $s$ appears in at most $k$ examples in the training
    data $X:|{x\in X:s\subseteq x}|\leq k$ and the $s$ is extractable from the LM
    $f_{\theta}$ with a prefix $c$ which satisfies
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle s\leftarrow\arg\max\limits_{s^{\prime}:&#124;s^{\prime}&#124;=N}f_{\theta}(s^{\prime}&#124;c)$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{\theta}(s^{\prime}|c)$ is the likelihood of an entire sequence $s^{\prime}$
    with length $N$.
  prefs: []
  type: TYPE_NORMAL
- en: This memorization definition helps figure out the possibly memorized strings
    based on repetition times in LM. If $k$ is large, the memorized string may be
    common knowledge like the zip code of a particular city. But when $k$ is very
    small, the memorized string could be harmful like accidentally exposing a personal
    phone number. The $k$-Eidetic Memorization is also concerned with privacy but
    utilizes the repetition times as a parameter to identify common knowledge memorization
    and harmful unintended memorization in language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: III Memorization Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Memorization evaluation basically follows different memorization levels to
    identify the existence of memorization and its influence. Figure [3](#S1.F3 "Figure
    3 ‣ I Introduction ‣ Memorization in deep learning: A survey") demonstrates the
    methods of memorization evaluation and associated memorization definitions.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Example Memorization Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Example memorization focuses on individual example memorization, which means
    checking if any example has been memorized by neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Differential Memorization Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When example memorization happens, the model’s outputs on the memorized data
    between the model training with it and without it will produce a large gap. This
    is ’leave-one-out’ memorization or differential memorization that we present in
    Defintion [1](#Thmdefinition1 "Definition 1 ‣ II-A1 Label Memorization ‣ II-A
    Memorization Definitions in Generalization Domain ‣ II Memorization Definition
    ‣ Memorization in deep learning: A survey") and Definition [3](#Thmdefinition3
    "Definition 3 ‣ II-A3 Counterfactual Memorization ‣ II-A Memorization Definitions
    in Generalization Domain ‣ II Memorization Definition ‣ Memorization in deep learning:
    A survey"). The measurement can be called the memorization score and the formula
    is the same as the definitions.'
  prefs: []
  type: TYPE_NORMAL
- en: The memorization score for supervised tasks is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: and the memorization score for unsupervised tasks is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |'
  prefs: []
  type: TYPE_TB
- en: The memorization score quantifies the performance gap on a single example when
    the example is included and excluded from the training dataset. A notably large
    performance gap indicates that other examples cannot provide useful features for
    the data example and the model has to memorize it. Additionally, this measurement
    may require more computation resources. Jiang et al. [[20](#bib.bib20)] provide
    a simplified method to use multiple large subsets to replace the held-out dataset
    which saves the evaluation cost.
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Probabilistic Memorization Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Probabilistic memorization depends on the differences in model outputs of memorized
    and generalized examples. There may exist multiple techniques to capture the differences
    but the most relevant method is the membership inference attack.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of attack aims to determine whether a data point belongs to the training
    dataset. The success of the attack cannot rely on the generalized feature of examples
    because these features are common for the entire data distribution. Therefore,
    the membership inference attack focuses on the particular or unique features that
    models memorize. In other words, data points that the model has memorized during
    training are more likely to be correctly identified as belonging to the training
    dataset in membership inference attacks. Though there is no obvious quantitative
    research to prove such a relationship and no formal definition, some works [[21](#bib.bib21),
    [22](#bib.bib22)] tacitly approve the relationship and adopt membership inference
    attack to measure memorization. Thus, it is possible to use membership inference
    attacks to evaluate model memorization indirectly. It is noted that some membership
    inference methods have high false positive rates which cannot exactly measure
    memorization [[23](#bib.bib23), [24](#bib.bib24)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical work is Likelihood Ratio Attack (LiRA) [[24](#bib.bib24)]. The
    core idea behind LiRA is similar to Definition [1](#Thmdefinition1 "Definition
    1 ‣ II-A1 Label Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey"),
    which involves evaluating membership inference risks by leveraging likelihood
    ratios. LiRA aims to assess whether a given data point is a member of the training
    dataset by computing the likelihood ratio based on the model’s predictions of
    the data point when the training dataset includes and excludes it. The original
    formula can be demonstrated as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Lambda(f,(x_{i},y_{i}))=\frac{p(f&#124;\mathbb{Q}_{in}(x_{i},y_{i}))}{p(f&#124;\mathbb{Q}_{out}(x_{i},y_{i}))}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{Q}_{in}(x_{i},y_{i})$ and $\mathbb{Q}_{out}(x_{i},y_{i})$ represents
    the distribution of models trained on the training dataset with and without the
    data point $(x_{i},y_{i})$ and $p$ is the probability density function over $f$
    under the distribution of model parameters $\mathbb{Q}$. The similarity in the
    core idea highlights the connection between membership inference risks and memorization
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, there may exist other techniques based on probability that can be
    used to estimate memorization. Nevertheless, relevant techniques need careful
    validation and confirmation that they can really reflect the memorization effect.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Example Memorization Influence Score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The influence score represents how an individual memorized example impacts model
    generalization performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we know some methods to evaluate example memorization but we are also curious
    about how memorized examples influence model generalization. To measure the impact
    of an individual memorized example on generalization, the influence score based
    on the memorization score has been proposed [[11](#bib.bib11)]. Generally, the
    influence score of a training example $(x_{i},y_{i})$ against a test example $(x_{j}^{\prime},y_{j}^{\prime})$
    under a supervised task can be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Similarly, the influence score for unsupervised tasks [[17](#bib.bib17)] can
    be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: In the corresponding work [[11](#bib.bib11), [17](#bib.bib17)], they find a
    direct positive correlation between memorization scores and influence scores,
    and these examples are almost atypical. This observation proves rare and memorized
    examples provide particular features for their subpopulation generalization. Moreover,
    it’s worth noting that not all memorized examples contribute high influence scores
    because some memorized examples can be regarded as noisy examples and some are
    so rare that even no test examples belong to the corresponding subpopulation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2b5f86fb96c48fd43a2fb4c4545ba21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Demonstration of the Long-tailed Examples.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Model Memorization Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model memorization cares about the role of neural networks, concerning how much
    memorization exists in models and the memorization capacity and ability of models.
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Noisy Label Memorization Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Noisy label memorization evaluation actually is not used to measure model memorization
    but is a valuable method to build memorization baselines compared to other properties
    of the model. Depending on the fact that noisy label examples have no shared class-level
    features and patterns, the model has to memorize all of these noisy label examples.
    Thus, many works utilize noisy label examples as known memorization. Arpit et
    al. [[25](#bib.bib25)] mix the noisy label examples with normal examples to study
    the learning dynamics during training. They use the ratio of noisy label examples
    in the training dataset to represent memorization. Another work [[26](#bib.bib26)]
    is studying the memorization effect in adversarial training, utilizing the randomly
    labeled adversarial examples. Additionally, Maini et al. [[15](#bib.bib15)] attempt
    to employ noisy label examples to localize the memorization in the neural network.
    Hence, noisy label memorization evaluation is a common method to investigate the
    relationships between memorization and other factors.
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 Recurrence Memorization Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recurrence memorization evaluation refers to the probability that neural networks
    can generate or extract specific marked examples put in the training dataset to
    measure the memorization tendency and ability of the model. Obviously, the selection
    of marked examples mainly affects the memorization evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Carlini et al. [[14](#bib.bib14)] employ random sequences to evaluate unintended
    memorization (Definition [6](#Thmdefinition6 "Definition 6 ‣ II-B1 Unintended
    Memorization ‣ II-B Memorization Definitions in Security Domain ‣ II Memorization
    Definition ‣ Memorization in deep learning: A survey")) in language models depending
    on this evaluation method. Specifically, they build the canary sequences which
    consist of two parts. The first part is like "the random number is" and the second
    part is just random numbers. Consequently, they create a metric called exposure
    index based on the log-perplexity,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{Px}_{f}(x_{1},\cdots,x_{n})=\sum_{i=1}^{N}(-\log_{2}\mathbf{Pr}(x_{i}&#124;f(x_{1},\cdots,x_{i-1}))),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $f$ is the language model, and $x_{1},\cdots,x_{n}$ represents the input
    sequence. The perplexity measures how “surprised” the model is to see a given
    value. A higher perplexity indicates the model is “more surprised” by the sequence.
    Therefore, the exposure index measures the likelihood of data sequences. The evaluation
    follows confirming the canary sequence inserted into the training dataset, training,
    and then applying the exposure index to gain the probability of the canary sequence
    reproduction. The exposure index of the canary sequence may represent the model
    memorization ability.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, employing other types of examples like atypical examples instead
    of random examples may disclose other properties of model memorization. This requires
    further studies.
  prefs: []
  type: TYPE_NORMAL
- en: III-C3 Extraction Memorization Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Extraction memorization evaluation applies the extraction or inversion approaches
    to empirically evaluate the model memorization by producing all extractable examples
    and identifying those in the training dataset. This method attempts to provide
    a lower bound of model memorization. However, it is noted that not all extracted
    examples are identical to corresponding training examples because memorization
    works on the feature scale. Some extractable examples can be regarded as the representatives
    of generalized examples. It requires good metrics to ensure extractable examples
    are really memorized.
  prefs: []
  type: TYPE_NORMAL
- en: 'An effective work [[9](#bib.bib9)] applying this method attempts to extract
    training data from large language models and find examples with small $k$ in $k$-Eidetic
    Memorization (Definition [7](#Thmdefinition7 "Definition 7 ‣ II-B2 𝑘-Eidetic Memorization
    ‣ II-B Memorization Definitions in Security Domain ‣ II Memorization Definition
    ‣ Memorization in deep learning: A survey")). They generate a lot of text with
    GPT-2 and pick text with the highest memorization probability, validating the
    memorization on picked text manually via Internet search.'
  prefs: []
  type: TYPE_NORMAL
- en: This evaluation method may require more resources but perhaps additionally assist
    researchers in understanding what models memorize.
  prefs: []
  type: TYPE_NORMAL
- en: IV Memorization in DNN Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A primary motivation behind research on memorization is to explore its impact
    and what role it plays in DNN training. In this section, we will provide a comprehensive
    review of memorization research in the DNN training framework. Table [II](#S4.T2
    "TABLE II ‣ IV Memorization in DNN Training ‣ Memorization in deep learning: A
    survey") demonstrates the main relevant works in this area.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Main Works about Memorization in DNN Training'
  prefs: []
  type: TYPE_NORMAL
- en: '| Main Topic | Reference | Background Task | Main Eval. Method | Main Findings
    |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization Mechanism | Zhang et al., 2017 [[8](#bib.bib8)] | Supervised
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Task | Noisy Label Memorization Evaluation | DNNs can memorize
    randomly labeled datasets that traditional approaches fail to explain generalization.
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization about Data | Feldmen et al., 2020 [[13](#bib.bib13), [11](#bib.bib11)]
    | Supervised |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Task | Differential Memorization Evaluation | Propose the
    long tail theory that memorization of long-tailed examples is crucial for achieving
    close-to-optimal generalization error. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hacohen et al., 2020 [[27](#bib.bib27)] | Various Tasks | / | Different
    neural networks memorize data in different orders. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zhang et al., 2021 [[17](#bib.bib17)] | Unsupervised Language |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Task | Differential emorization Evaluation | High memorization
    examples are generally unconventional texts. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lee et al., 2022 [[28](#bib.bib28)] | Unsupervised Language |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Task | Probabilistic Memorization Evaluation | Deduplicated datasets
    make less memorization. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Carlini et al., 2023 [[29](#bib.bib29)] | Unsupervised Language |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Task | Extraction Memorization Evaluation | Repeated examples
    have a high probability of being extracted. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Memorzation about Training Stage | Arpit et al., 2018 [[25](#bib.bib25)]
    | Supervised |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Task | Noisy Label Memorization Evaluation | Learning simple
    patterns is prior to remembering noise data in the early training stage. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Maennel et al., 2020 [[30](#bib.bib30)] | Supervised |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Task | Noisy Label Memorization Evaluation | An alignment
    between the principal components of network parameters and data takes place when
    training with random labels in the early training stage. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization about Architecture | Stephenson et al., 2021 [[31](#bib.bib31)]
    | Supervised |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Task | Mean Field |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Theoretic Geometric Analysis | Memorization predominately occurs in the deeper
    layers. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Maini et al., 2023 [[15](#bib.bib15)] | Supervised |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Task | Noisy Label Memorization Evaluation | Memorization
    exists in a small set of neurons in various layers of the model. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Geva et al., 2021 [[32](#bib.bib32)] | Unsupervised Language Generative
    Task | / | Feed-forward layers in Transformer are key-value memories. |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization about Overfitting | Tirumala et al., 2022 [[16](#bib.bib16)]
    | Unsupervised Language |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Generative Task | Exact Memorization | Larger models can memorize a larger
    portion of the data before over-fitting |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization about DA and Regularization | Anagnostidis et al., 2023 [[18](#bib.bib18)]
    | Supervised |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Task | $k$NN Probe | Even randomly labeled datasets with DA
    could lead to highly useful features. |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Li et al., 2023 [[22](#bib.bib22)] | Supervised |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Task | Probabilistic Memorization Evaluation | Trivial data
    augmentation technologies can mitigate memorization. |  |  |'
  prefs: []
  type: TYPE_TB
- en: IV-A Exploring Memorization Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In some early studies [[33](#bib.bib33)], researchers believed that the memorization
    effect was not necessary for learning. Generally, the generalization of DNNs means
    networks can learn and recognize common patterns hidden within the input data.
    These common patterns consist of shared features among similar data examples.
    When DNNs learn these common patterns, they exhibit the ability to generalize,
    thereby demonstrating their capacity to perform well on new, unseen data beyond
    the training dataset. In contrast, memorization means networks memorize input
    examples rather than patterns which results in overfitting. However, as Zhang
    et al. [[8](#bib.bib8)] find that DNNs can easily fit the random labeled dataset,
    the traditional statistical learning theory like VC dimension [[34](#bib.bib34)],
    Rademacher complexity [[35](#bib.bib35)], and uniform stability [[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38)] cannot explain the generalization of DNNs.
    It is well known that DNNs cannot correctly classify the randomly labeled dataset
    based on the common patterns of the data distribution, thus DNNs have to memorize
    the entire dataset. However, the memorization mechanism in the DNNs remains unclear
    and vague. Therefore, two important and interesting questions arise: why DNNs
    memorize data in the standard training process, and how the memorization mechanism
    operates. This attracts the machine learning community to explore the memorization
    effect.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Memorization and Data Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In studying overfitting [[39](#bib.bib39)], researchers have found that DNNs
    may memorize data. In exploring the memorization phenomenon, understanding the
    relationship between data distribution and memorization tendency, orders, as well
    as how the memorization mechanism affects training performance, is an important
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real-world natural data distributions are generally long-tailed [[40](#bib.bib40)]
    and almost all practical datasets are sampled from the real world. Considering
    this, Feldman et al. [[13](#bib.bib13), [11](#bib.bib11)] propose the long tail
    theory. This theory suggests that long-tailed examples as illustrated in Figure [4](#S3.F4
    "Figure 4 ‣ III-B Example Memorization Influence Score ‣ III Memorization Evaluations
    ‣ Memorization in deep learning: A survey") are prone to be memorized. Moreover,
    memorizing these long-tailed examples is crucial for achieving close-to-optimal
    generalization errors in long-tailed data distributions because rare and atypical
    instances can provide necessary generalization. They further validate the theory
    by evaluating examples based on the memorization score (Definition [1](#Thmdefinition1
    "Definition 1 ‣ II-A1 Label Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")).
    The results illustrate that examples with high memorization scores are more atypical.
    Thus, compared to typical examples, atypical examples are more likely to be memorized
    by DNNs. When removing examples with high memorization scores, the generalization
    errors increase. This theory also has empirical evidence in language tasks [[12](#bib.bib12)].
    Jiang et al. [[20](#bib.bib20)] develop the consistency score (C-score) based
    on memorization score (Definition [1](#Thmdefinition1 "Definition 1 ‣ II-A1 Label
    Memorization ‣ II-A Memorization Definitions in Generalization Domain ‣ II Memorization
    Definition ‣ Memorization in deep learning: A survey")), which can be applied
    in larger datasets. The C-score aims to measure the per-instance generalization.
    Their result demonstrates that a more atypical example has a lower C-score which
    provides convincing evidence for the long tail theory. Zhang et al. [[17](#bib.bib17)]
    extend the memorization score to unsupervised learning and propose counterfactual
    memorization (Definition [3](#Thmdefinition3 "Definition 3 ‣ II-A3 Counterfactual
    Memorization ‣ II-A Memorization Definitions in Generalization Domain ‣ II Memorization
    Definition ‣ Memorization in deep learning: A survey")) to evaluate text datasets.
    They discover that high memorization examples are generally unconventional texts
    such as all-capital letters, structured formats, and multilingual texts. In contrast,
    low memorization examples are generally templated documents with many near-duplicate
    copies in the training data. The tendency also corresponds to the long tail theory [[11](#bib.bib11)].'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, a recent research [[15](#bib.bib15)] indicates that the DNNs cannot
    identify noisy examples from atypical examples empirically. When removing memorization-associated
    neurons, DNNs cannot classify the noisy examples and the generalization performance
    also reduces on the noise-mixed dataset. Additionally, for the same memorized
    example, the memorization path is distinct in repeated independent training experiments [[41](#bib.bib41)].
    This may represent that DNNs can select various particular features to uniquely
    identify the same example. Furthermore, the learning order of clean examples has
    observable consistency in similar architectures [[27](#bib.bib27)]. However, when
    training DNNs on the same dataset with randomly shuffled labels, they find that
    different models memorize the data in a different order. This finding also suggests
    that memorization learning has various possible paths.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. Atypical and noisy examples as long-tailed examples lack representativity
    in datasets, thus, DNNs are prone to memorize these long-tailed examples to minimize
    the training loss. This explains why and what DNNs memorize.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Memorization and Repetition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intuitively, DNNs tend to memorize duplicated examples. Zhang et al. [[17](#bib.bib17)]
    believe most memorization criteria strongly correlate with the number of example
    occurrences in the training, and language models will capture common memorization
    such as familiar phrases, public knowledge, or templated texts. Moreover, deduplicated
    datasets reduce the memorization frequency and improve generalization [[28](#bib.bib28)].
    From the perspective of the extraction task, repeated examples have a high probability
    of being extracted [[29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: 'One study [[9](#bib.bib9)] links repetition times and memorization, proposing
    $k$-Eidetic Memorization (Definition [7](#Thmdefinition7 "Definition 7 ‣ II-B2
    𝑘-Eidetic Memorization ‣ II-B Memorization Definitions in Security Domain ‣ II
    Memorization Definition ‣ Memorization in deep learning: A survey")), where $k$
    relates to the number of occurrences for one example. They apply this definition
    to the language model extraction task and investigate GPT-2\. For extractable
    large $k$ examples, they include common knowledge like city names or high-frequency
    words, and complex text such as the entire text of the MIT public license because
    the license may occur thousands of times in the training dataset. However, GPT-2
    also memorizes some low-frequent examples with small $k$ like contact information
    and valid URLs.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in practical environments, example repetition is an influence factor
    of memorization. Nevertheless, the long tail theory [[13](#bib.bib13), [11](#bib.bib11)]
    tells us that the long-tailed examples are prone to be memorized and these long-tailed
    examples are low-frequent in the distribution. This requires systematically evaluating
    memorization factors.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. DNNs tend to prioritize the memorization of repeated data. However,
    memorization learning is limited by multiple factors. There is currently no unified
    framework to describe the impact of data on memorization.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Memorization and Training Stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Researchers have discovered that DNN training has a critical early learning
    stage [[42](#bib.bib42), [43](#bib.bib43)], during which model performance increases
    rapidly. Then with the truth that DNNs typically minimize loss in the final stage
    of training [[8](#bib.bib8), [44](#bib.bib44)], it is reasonable to believe that
    pattern learning and memorization learning dominate different training stages.
    Therefore, investigating and explaining the memorization dynamic during training
    stages constitutes a valuable research topic.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the difficulty in separating the memorization learning from the generalization,
    it is possible to explore how DNN learns by using a training dataset containing
    a mixture of normal and noisy examples in certain proportions. Arpit et al. [[25](#bib.bib25)]
    utilize the method and find that DNNs tend to prioritize learning patterns even
    in noisy datasets, as evidenced by high validation accuracy in the early training
    stage. Subsequently, DNNs begin to directly memorize noisy examples, leading to
    a rapid drop in validation accuracy. Maennel et al. [[30](#bib.bib30)] obverse
    an alignment between the principal components of network parameters and data takes
    place when training with random labels in the early training stage. However, the
    misalignment scores gradually increase during the later training stage.
  prefs: []
  type: TYPE_NORMAL
- en: Another perspective [[31](#bib.bib31), [41](#bib.bib41)] based on analyzing
    the gradient variation explains the phenomenon. In the early learning phase, the
    gradients from noisy examples contribute minimally to the total gradient because
    inconsistent gradient information may counteract each other, and those shared
    patterns of the same class are consistent, facilitating quick updates and promoting
    pattern learning. Similarly, applying a new detection method called ’variance
    of gradients’ (VoG) [[45](#bib.bib45)], the examples with lower VoG in the early
    training stage are more typical compared to the examples in the later training
    stage. Combining this with the long tail theory [[13](#bib.bib13)], it may be
    inferred that pattern learning dominates the early training stage.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. There is sufficient evidence to demonstrate that pattern learning dominates
    the early training stage, while memorization learning mainly occupies the relatively
    later training stage. One reasonable explanation is the subpopulation with the
    same pattern can contribute effective gradients during the early learning stage.
    Conversely, atypical examples and noisy examples contain conflicting gradient
    information that cannot be effectively learned during early training.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Memorization and Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different network layers experience diverse learning dynamics. In exploring
    the functions of DNN layers, a study on transferability [[46](#bib.bib46)] suggests
    that shallow layers’ features appear to be general and applicable to other tasks
    or datasets. However, deep layers tend to learn task-correlated features. This
    illustrates that different layers learn distinct features, with shallow layers
    in DNNs being more prone to learn patterns while deep layers specialize.
  prefs: []
  type: TYPE_NORMAL
- en: Some subsequent studies have contributed to enhancing the reliability of this
    viewpoint. Using Singular Vector Canonical Correlation Analysis (SVCCA), Raghu
    et al. [[47](#bib.bib47)] compare layers across time and observe their convergence
    starting from the shallow layers. Morcos et al. [[48](#bib.bib48)] develop projection-weighted
    Canonical Correlation Analysis (CCA) based on SVCCA. With multiple networks, they
    demonstrate that generalized networks converge to more similar representations.
    Specifically, they note that at shallow layers, all networks converged to equally
    similar solutions. Intuitively, this indicates that the shallow layers learn common
    patterns. However, at deep layers, groups of generalizing networks converged to
    substantially more similar solutions compared to groups of memorizing networks.
    This indicates that each memorizing network memorizes the training data using
    different strategies. Ansuini et al. [[49](#bib.bib49)] employ the Intrinsic Dimensionality
    (ID) of data representations, i.e. the minimal number of parameters needed to
    describe a representation. In their study on the utility of different layers,
    they find across layers, the ID initially increases and then progressively decreases
    in the final layers. Remarkably, they observed that the ID of the last hidden
    layer could predict classification on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, the specialization of deep layers connects to memorization learning.
    Therefore, a reasonable inference is that the memorization of training examples
    occurs in the last (or final few) layers of a deep network. Stephenson et al. [[31](#bib.bib31)],
    employing replica-based mean field theoretic geometric analysis method, believe
    memorization mainly occurs in the deeper layers due to decreasing object manifolds’
    radius and dimension, whereas previous layers are minimally affected. In the experiment,
    if rewinding the parameters of the final convolutional layer to an earlier epoch,
    the generalization of the model can achieve a similar performance to the early-stopped
    model. Moreover, Anagnostidis et al. [[18](#bib.bib18)] employ the $k$NN probing
    to evaluate feature learning of each network layer, utilizing embedding vectors
    of each training example from each layer and correct labels to build $k$NN models.
    They find that embedding vectors of test examples produced from shallow layers
    achieve non-trivial $k$NN accuracy with the randomly labeled training dataset
    and data augmentation. They term this phenomenon benign memorization (Definition [4](#Thmdefinition4
    "Definition 4 ‣ II-A4 Benign Memorization ‣ II-A Memorization Definitions in Generalization
    Domain ‣ II Memorization Definition ‣ Memorization in deep learning: A survey")).
    However, the $k$NN probing accuracies drop significantly in the deep layers, indicating
    that DNN fits the random labels. This suggests that only the very last layers
    are used for memorization, while previous layers encode generalized features that
    remain largely unaffected by the label noise. Furthermore, another work [[30](#bib.bib30)]
    explains the training data will align the principal components of network parameters
    at the earlier layers when trained with random labels and later layers become
    specialized.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the latest work conducted by Maini et al. [[15](#bib.bib15)] reveals
    that memorization of a classification task exists in a small set of neurons in
    various layers of the model, and the layers that contribute to example memorization
    are, not the final layers. In their experiment, they use a noisy dataset to train
    DNNs. Subsequently, they apply technologies known as layer retraining and layer
    rewinding to eliminate memorization within individual layers. Finally, they validate
    the memorization effect (i.e. accuracy of noisy examples in the training dataset)
    on modified models. The unexpected finding is that the memorization effect still
    persists in the model, which proves various layers contribute to the memorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, researchers also investigate the inherent functions of layers
    regarding memorization. An interesting work [[50](#bib.bib50)] attempts to demonstrate
    if only the memorization function can provide generalization. The author builds
    a network of lookup tables and finds deep look-up table network exhibits generalization
    in the Binary-MNIST task. Moreover, this model reproduces a crucial finding with
    neural networks: memorization can provide generalization with depth, though it
    is doubtful that the model can work on more complex tasks. Additionally, Zhang
    et al. [[51](#bib.bib51)] investigate whether different trained networks tend
    to demonstrate the constant function (memorization) or the identity function (generalization)
    and they empirically find that different architectures exhibit strikingly different
    complex biases.'
  prefs: []
  type: TYPE_NORMAL
- en: As Transformers [[52](#bib.bib52)] achieve a big success in various tasks, people
    are also interested in memorization of Transformers and large language models.
    Sukhbaatar et al. [[53](#bib.bib53)] augment the self-attention layers with persistent
    memory vectors and find this plays a similar role as the feed-forward layer. Moreover,
    Geva et al. [[32](#bib.bib32)] directly point out that feed-forward layers are
    key-value memories, where each key correlates with textual patterns in the training
    examples, and each value induces a distribution over the output vocabulary. It
    should be noted here that this kind of key-value memory combines pattern learning
    and memorization learning. They show that feed-forward layers act as pattern detectors
    over the input across all layers and learned patterns are human-interpretable.
    Additionally, Dai et al. [[54](#bib.bib54)] introduce the concept of knowledge
    neurons that express factual knowledge and propose a knowledge attribution method
    to identify the neurons via a fill-in-the-blank cloze task in BERT. They find
    that the activation of such knowledge neurons is positively correlated to the
    expression of their corresponding facts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. Functions of different layers in neural networks vary significantly.
    While many works prove deep layers specialize, memorization location still requires
    more exploration. For Transformers, researchers have found that feed-forward layers
    are key-value memories but the memory is not only the result of memorization learning.
    Among these key-value memories, we cannot ensure they are all task-correlated.
    Some irrelevant and unexpected details could also be stored in them. Further research
    is needed to investigate the memorization mechanism associated with the model
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Memorization and Overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overfitting is a common phenomenon in deep learning which represents that a
    model learns the training data so well that it captures not only the underlying
    patterns but also the particular features in the data. This causes the model to
    fail in generalizing effectively to new, unseen data. Thus, early research commonly
    held the opinion that overfitting was responsible for memorization. However, contemporary
    studies [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)] provide evidence
    supporting the persistence of memorization throughout the training process. Memorization
    does not necessarily lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the long tail theory [[13](#bib.bib13), [11](#bib.bib11)], we understand
    that memorizing atypical examples contributes to generalization. In contrast,
    overfitting will enlarge the generalization error while training loss decreases.
    Some recent works suggest that even for DNNs without a significant train-test
    gap, memorization still exists [[9](#bib.bib9), [16](#bib.bib16)]. Additionally,
    the privacy risks (Evaluation [III-A2](#S3.SS1.SSS2 "III-A2 Probabilistic Memorization
    Evaluation ‣ III-A Example Memorization Evaluation ‣ III Memorization Evaluations
    ‣ Memorization in deep learning: A survey")) also imply the underlying memorization.
    It is known that overfitting is not necessary for successful membership inference
    attacks [[55](#bib.bib55), [56](#bib.bib56)]. Furthermore, when a neural network
    is trained in a training dataset mixed with clean examples and less noisy examples,
    both the accuracy of noisy examples and that of clean examples exhibit concurrent
    improvement [[15](#bib.bib15)]. Overfitting is a phenomenon of training observed
    in the later stages of training. For individual training examples, DNNs may memorize
    them while learning patterns in the early training stage. Thus, memorization does
    not necessarily require overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting phenomenon is benign overfitting. The phenomenon means that
    even after overlearning training data, DNNs still can generalize well [[57](#bib.bib57),
    [58](#bib.bib58)]. This theory believes overparameterized DNNs can generalize
    to the majority of the data distribution using simple paths, and memorize mislabeled
    and irregular data using complex paths. These components do not interfere, making
    such overfitting benign. One explanation [[59](#bib.bib59)] believes that overfitting
    becomes benign when the signal-to-noise ratio satisfies a certain condition. In
    simple terms, benign overfitting requires sufficient signals in the dataset. Thus,
    benign overfitting may involve less memorization, yet there is insufficient evidence
    to illustrate their relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. Overfitting as a training phenomenon does not have a strong relationship
    with memorization. In the context of overfitting, memorization is necessary but
    not sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: IV-G Memorization and Data Augmentation, Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data augmentation and regularization are widespread techniques used in training
    neural networks. Therefore, it is necessary to study the impact of these practices
    on memorization.
  prefs: []
  type: TYPE_NORMAL
- en: General Data Augmentation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Data Augmentation is a pivotal strategy used to expand the original training
    dataset by introducing a variety of artificially generated examples. Generally,
    the primary objective of this technique is to enrich example representations based
    on corresponding semantic features. Multiple representations can help DNNs perform
    well on unseen examples, thereby improving the generalization. Presently, various
    technologies exist to implement data augmentation, but here we focus on trivial
    data augmentation, which means fundamental transformations applied to the original
    training dataset. Trivial data augmentation depends on specific data formats.
    For instance, in image processing, these transformations could include rotations,
    flips, zooms, and color variations. In natural language processing, techniques
    might encompass synonym replacement or back-translation.
  prefs: []
  type: TYPE_NORMAL
- en: In related works, one early study [[60](#bib.bib60)] demonstrates trivial data
    augmentation can reduce the risks of membership inference attacks, thereby diminishing
    memorization. Utilizing recent memorization evaluation methods, Li et al. [[22](#bib.bib22)]
    study the memorization effect of multiple data augmentation. They measure the
    memorization evaluation results by membership inference and demonstrate trivial
    data augmentation technologies significantly mitigate memorization. However, for
    advanced data augmentation technologies, further research on the memorization
    effect is still required. Another work [[18](#bib.bib18)] measures memorization
    based on $k$NN probing and they find that $k$NN probing accuracy of the embedding
    vectors increases with data augmentation under random label training datasets
    and clean training datasets. Moreover, they observe that learning under complete
    label noise with data augmentation still leads to highly useful features in the
    shallow layers, explaining it as augmented datasets increasing the effective size
    of the dataset beyond the capacity of networks. This supports that data augmentation
    mitigates memorization. However, the mechanism of how data augmentation impacts
    memorization still needs to be explored and systematically evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Regularizers like weight decay and dropout are the standard tools in theory
    and practice to mitigate overfitting in the training of neural networks. We know
    that regularizers help constrain the learning process to a specific subset of
    the hypothesis space with manageable complexity. In the work of Zhang et al. [[8](#bib.bib8)],
    explicit regularizers can prevent model memorization under random label learning,
    and help the model improve generalization. However, regularization is neither
    necessary nor by itself sufficient for controlling generalization errors. Then
    the research conducted by Arpit et al. [[25](#bib.bib25)] reproduces a similar
    result as Zhang et al. [[8](#bib.bib8)] and finds dropout is best at hindering
    memorization without reducing the model’s ability to learn. This also responds
    to the work of location memorization [[15](#bib.bib15)], in which finds memorization
    exists in a small set of neurons in various layers of the model. It seems under
    random label training, explicit regularizers can mitigate memorization by dropping
    or constraining neurons, but it is not clear to understand how regularizers influence
    atypical example memorization in standard training.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. Data augmentation and regularization are standard tools in training
    neural networks and help improve model generalization. In related works, both
    tools mitigate memorization under random label training, but we do not know if
    they hinder learning long-tailed examples in standard training. This could be
    a research direction in the future.
  prefs: []
  type: TYPE_NORMAL
- en: IV-H Memorization and Other Factors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Capacity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The model capacity is related to model memorization. Generally, models with
    larger sizes can memorize more data than smaller ones [[29](#bib.bib29)]. Additionally,
    early work has shown that overparameterized neural networks can directly memorize
    randomly labeled modern datasets [[8](#bib.bib8)]. However, we cannot easily think
    larger capacity leads to more memorization because training data plays an important
    role. The effective capacity of networks cannot directly explain memorization
    and generalization [[25](#bib.bib25)]. Naturally, a question arises: what happens
    if the training dataset size far exceeds the model’s capacity? Data augmentation
    can create this condition, and Anagnostidis et al. [[18](#bib.bib18)] find that
    even the randomly labeled dataset with data augmentation exceeding the model capacity
    can produce effective patterns in the model. Nevertheless, related topics still
    require further research.'
  prefs: []
  type: TYPE_NORMAL
- en: Loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The loss function is an important component of neural network training. Thus,
    it must influence the memorization dynamics of models. Patel et al. [[61](#bib.bib61)]
    propose robust log loss (RLL) which can prevent model overfitting on the randomly
    labeled data. However, no further studies have explored how different types of
    loss functions affect model memorization.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Rate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Learning rate is an essential hyperparameter in neural network training. Li
    et al. [[62](#bib.bib62)] believe that a small learning rate model easily learns
    details, while a large learning rate helps capture patterns. They demonstrate
    this by adding a small patch to CIFAR10 images that are immediately memorizable
    by a model with a small initial learning rate but ignored by the model with a
    large learning rate until after annealing.
  prefs: []
  type: TYPE_NORMAL
- en: Data Format
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The data format may affect memorization during training, particularly for language
    tasks. Kharitonov et al. [[63](#bib.bib63)] find the size of the subword vocabulary
    learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency
    of standard Transformer models to memorize training data. Larger subword vocabulary
    and shorter input sequences result in strong memorization of Transformer models.
    The underlying reason could be that complex subwords weaken the patterns in the
    data distribution. Thus, the input data format likewise impacts memorization.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. Many other factors may facilitate or hinder memorization. Specifically,
    large models tend to memorize due to an excessive number of parameters. Moreover,
    a small learning rate can promote memorization. Additionally, loss functions and
    data format have impacts on model memorization but we still require further studies.
  prefs: []
  type: TYPE_NORMAL
- en: V Underlying Risks of Memorization Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In previous sections, DNNs have been shown the feature of memorizing training
    data, and this property may cause various security risks. This section undertakes
    exploration and synthesis of the impact of memorization on typical threats and
    defenses in DNNs. We summarize the main literature related to the risks of memorization
    learning in Table [III](#S5.T3 "TABLE III ‣ V Underlying Risks of Memorization
    Learning ‣ Memorization in deep learning: A survey") and plot Figure [5](#S5.F5
    "Figure 5 ‣ V Underlying Risks of Memorization Learning ‣ Memorization in deep
    learning: A survey") to demonstrate these risks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/82f74e31176b70b1f31dc3a395996a23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Underlying Risks of Memorization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Main Works about Underlying Risks of Memorization Learning'
  prefs: []
  type: TYPE_NORMAL
- en: '| Main Topic | Reference | Background Task | Main Eval. Method | Main Findings
    |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization about Membership Inference Risks | Leino et al., 2020 [[64](#bib.bib64)]
    | Supervised Classification Task | / | Propose a new membership inference attack
    based on memorized features. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Carlini et al., 2022 [[24](#bib.bib24)] | Various Tasks | / | Propose
    a new membership inference attack ’LiRA’ utilizing the memorization effect. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Carlini et al., 2022 [[21](#bib.bib21)] | Supervised Classification Task
    | Probabilistic Memorization Evaluation | Removing the vulnerable outlier points
    may threaten inner previously-safe points on the same attack. |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization about Extraction Risks | Carlini et al., 2019 [[14](#bib.bib14)]
    | Unsupervised Language Generative Task | Recurrence Memorization Evaluation |
    Introduce a memorization exposure metric to measure unintended memorization. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Carlini et al., 2021 [[9](#bib.bib9)] | Unsupervised Language Generative
    Task | Extraction Memorization Evaluation | An adversary can perform a training
    data extraction attack to recover individual training examples by querying the
    large language model. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Carlini et al., 2023 [[29](#bib.bib29)] | Unsupervised Language Generative
    Task | Extraction Memorization Evaluation | Describe three log-linear relationships
    that how model capacity, duplication times, and the number of tokens impact memorization
    of LMs. |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization about Poisoning Risks | Zhang et al., 2017 [[8](#bib.bib8)]
    | Supervised Classification Task | Noisy Label Memorization Evaluation | DNNs
    can memorize randomly labeled datasets that traditional approaches fail to explain
    generalization. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Nguyen et al., 2023 [[19](#bib.bib19)] | Supervised Classification Task
    | Noisy Label Memorization Evaluation | Mislabeled examples may degrade the neural
    collapse and damage model generalization |'
  prefs: []
  type: TYPE_TB
- en: '|  | Maini et al., 2023 [[15](#bib.bib15)] | Supervised Classification Task
    | Noisy Label Memorization Evaluation | Drop memorization-associated neurons,
    mislabeled examples cannot be effectively classified. |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization about Adversarial Risks | Li et al., 2023 [[22](#bib.bib22)]
    | Supervised Classification Task | Probabilistic Memorization Evaluation | In
    adversarial training, adversarial examples are very atypical and prone to be memorized.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Xu et al., 2023 [[65](#bib.bib65)] | Supervised Classification Task |
    Differential Memorization Evaluation | Memorizing atypical samples hardly improve
    their adversarial robustness. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dong et al., 2022 [[26](#bib.bib26)] | Supervised Classification Task
    | Noisy Label Memorization Evaluation | Memorization in adversarial training could
    result in robust overfitting. |'
  prefs: []
  type: TYPE_TB
- en: V-A Memorization and Membership Inference Risks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A membership inference attack is a representative privacy inference attack and
    seeks to address the query if a specific instance belongs to the training dataset [[66](#bib.bib66)].
    In the machine learning setting, the membership inference adversary is typically
    given access to a model’s predictions with varying granularity, ranging from the
    complete confidence vector to the label corresponding to the highest confidence
    score. It is established that the memorization phenomenon entails the memorization
    of training data points by DNNs, thereby implying that memorized data bears substantial
    risks for membership inference.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, prior work has demonstrated that the membership inference risks associated
    with training data exhibit significant non-uniformity. According to empirical
    results, typical data points have lower membership inference risks than those
    atypical data examples and outliers [[24](#bib.bib24), [11](#bib.bib11), [64](#bib.bib64),
    [67](#bib.bib67)]. These findings imply memorization is highly corresponding or
    even results in high membership inference risks. However, no direct quantitative
    investigation has been identified to establish the precise relationship between
    memorization and membership inference risks. In practice, this relationship has
    been implicitly approved [[21](#bib.bib21), [24](#bib.bib24), [64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the relationship between memorization and membership inference
    attack, Carlini et al. [[21](#bib.bib21)] find the privacy onion effect. The effect
    can be defined: when removing the most vulnerable data under a specific privacy
    attack and retraining a model on only the previously safe data, a new set of examples
    in turn becomes vulnerable to the same privacy attack. This phenomenon may indicate
    that even after removing those memorized data points, the model still memorizes
    relatively atypical data examples in the remaining training dataset. This observation
    intuitively underscores membership inference risks are highly associated with
    memorization and proves that memorization is relative.'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the memorization effect, researchers investigate more threatening
    membership inference attacks. Leino et al. [[64](#bib.bib64)] attempt to exploit
    features of memorized examples that are predictive only for the training data
    but not the sampling distribution. They capture differences in memorization learning
    data and pattern learning data and build a confident binary logistic classifier
    to infer membership. Another work is Likelihood Ratio Attack (LiRA) [[24](#bib.bib24)],
    it depends on the leave-one-out method as memorization score definition and utilizes
    differences of model outputs that training with and without the training example
    to do membership inference. These attacks directly exhibit that memorized examples
    have higher privacy risks than generalized examples.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. While no direct quantitative research has yet proven the relationship
    between memorization and membership inference attacks, nearly all relevant existing
    studies suggest a strong correspondence between memorization and membership inference
    risks and even indicate a causal relationship. Additionally, it is imperative
    to explore novel inference risks and mitigation strategies that are contingent
    upon the memorization effect.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Memorization and Inversion/Extraction Risks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversary in an inversion/extraction attack attempts to rebuild or extract
    training examples by leveraging gradients or models. The attack obviously threatens
    the privacy of machine learning as the acquired training examples inherently unveil
    sensitive information. Based on current knowledge, the generalized features embedded
    in the gradient or model parameters cannot facilitate the precise reconstruction
    or extraction of training examples because these features are common. Consequently,
    the underlying reasons for the inversion/extraction risk potentially come from
    the memorization phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: Related work mainly analyses the memorization effect concerning the extraction
    risk of language tasks [[14](#bib.bib14), [9](#bib.bib9), [29](#bib.bib29)]. Carlini
    et al. [[14](#bib.bib14)] introduce a memorization exposure metric utilizing canary
    sequences and log-perplexity. Subsequently, they establish that successful extraction
    becomes feasible when the level of memorization exposure surpasses a threshold.
    Conversely, extraction remains unsuccessful below this threshold. Consequently,
    it can be inferred that memorized examples carry a substantial risk of extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Another work [[9](#bib.bib9)] directly demonstrates the performance of their
    proposed extraction attack applied to GPT-2\. The researchers generate an extensive
    dataset through unconditional sampling from the model and employ diverse metrics
    to identify examples exhibiting high memorization likelihood. Consequently, they
    find the extraction result actually consists of trivial memorization and atypical
    information. We can explain the outcome originating from two distinct inversion/extraction
    mechanisms. The first mechanism rebuilds representations of common knowledge based
    on patterns. Another mechanism extracts atypical and individual examples exactly
    depending on memorized data. If we consider the two through the lens of privacy,
    the former mechanism supports the main task of DNNs while the latter apparently
    breaks privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if we acknowledge the robust correlation between memorization and
    the inversion/extraction risk, the measurement outcomes of inversion/extraction
    risk can be regarded as an empirical lower-bound of memorization [[29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary. Previous studies indicate the vulnerability of inversion/extraction
    risks is highly relevant to the memorization effect within models. Despite the
    absence of direct experimental evidence, we attempt to reveal two inversion/extraction
    mechanisms: one where the attack reconstructs representations of common knowledge
    through generalized patterns, and another where it precisely extracts exceptional
    and individual examples using memorized data. Consequently, we deduce that the
    memorization effect constitutes the foundational cause of privacy risk in inversion/extraction
    attacks. Furthermore, the outcomes of such attacks can serve as a lower-bound
    approximation of model memorization.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Memorization and Poisoning Risks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Poisoning attacks target breaking model availability. Specifically, adversaries
    attempt to degrade model performance on all examples (i.e. untargeted poisoning
    attack) or specific classes or examples (i.e. targeted poisoning attack), even
    examples with particular features (i.e. backdoor attack). Common poisoning techniques
    include data manipulation, called data poisoning, and model corruption, called
    model poisoning. As model poisoning is generally used in distribution machine
    learning systems, we mainly discuss data poisoning including label manipulation
    and input noise corruption.
  prefs: []
  type: TYPE_NORMAL
- en: Randomly labeled examples cannot be classified to the assigned noisy class based
    on pattern learning due to the absence of highly shared features. However, based
    on empirical results [[8](#bib.bib8)], we know that DNNs can minimize the training
    loss of randomly labeled datasets and achieve almost perfect accuracy. Therefore,
    randomly labeled examples are memorized and we can infer that data poisoning via
    label manipulation depends on the memorization effect. Arpit et al. [[25](#bib.bib25)]
    provides more effective evidence that model performance reduction is quantitatively
    corresponding to the proportion of mislabeled examples. Additionally, a recent
    study [[15](#bib.bib15)] finds if drop memorization-associated neurons, mislabeled
    examples cannot be effectively classified. However, they also demonstrate atypical
    examples and noisy examples are hard to identify for DNNs and generalization may
    be damaged by dropping memorization-associated neurons. Memorization-dilation [[19](#bib.bib19)]
    based on neural collapse provides an explanation that mislabeled examples may
    degrade the neural collapse and damage model generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Another technology of data poisoning is adding random noise to the input. As
    noise increases, we can infer the inputs may gradually transform from typical
    examples to atypical examples then full noise. Concurrently, DNNs have been forced
    to minimize the loss so models may boost the memorization of such inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. When an adversary launches a poisoning attack with mislabeled data,
    neural networks would memorize these data to minimize the loss. Thus, memorization
    is an adaptive process and the vulnerability to poisoning attack comes from the
    DNNs training framework.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Memorization and Adversarial Risks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The adversarial attack employs adversarial noise on inputs to drive examples
    approaching the decision boundary and achieving the maximum loss. Generally, the
    adversarial noise is generated by gradient ascending [[68](#bib.bib68), [69](#bib.bib69)].
    An effective defense strategy is adversarial training [[68](#bib.bib68)] which
    means directly training on the adversarial examples and this method provides a
    lower-bound robustness guarantee.
  prefs: []
  type: TYPE_NORMAL
- en: In spite of the absence of relevant studies on memorization and adversarial
    attacks, we can infer that the memorization effect is not the source of adversarial
    vulnerability because existing work [[70](#bib.bib70)] believes adversarial vulnerability
    derives from non-robust features.
  prefs: []
  type: TYPE_NORMAL
- en: Many works [[65](#bib.bib65), [22](#bib.bib22), [26](#bib.bib26)] investigate
    memorization in adversarial training. Li et al. [[22](#bib.bib22)] find adversarial
    examples are very atypical and Schmidt et al. [[71](#bib.bib71)] demonstrate the
    complexity of adversarial examples can be significantly larger than normal examples
    in standard learning, thus DNNs memorize them during adversarial training making
    DNNs more vulnerable to privacy attacks. Another work [[65](#bib.bib65)] exhibits
    that memorizing atypical examples hardly helps adversarial robustness and when
    memorizing some harmful atypical examples that share similar features with a “wrong"
    class, the boundary becomes blurred, and this damages robustness. This phenomenon
    may be explained by robust overfitting that one-hot labels can be inappropriate
    and some adversarial examples should be given low confidence [[72](#bib.bib72)].
    Researchers [[26](#bib.bib26)] are also curious about randomly labeled dataset
    performance in adversarial training and they find PGD-AT [[68](#bib.bib68)] fails
    to converge while TRADES [[73](#bib.bib73)] still reaches nearly 100% training
    accuracy. However, they believe DNNs have sufficient capacity to memorize adversarial
    examples of training data with completely random labels, but the convergence depends
    on the AT algorithms. Next, they analyze the gradients of the two different adversarial
    training methods and recognize the gradient of PGD-AT performs large variance,
    making it fail to converge. Moreover, they study robust overfitting and put it
    down to excessive memorization of one-hot labels breaking the robust decision
    boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. It seems that the memorization effect is not responsible for adversarial
    vulnerability. In the adversarial training, due to the hardness of examples, most
    adversarial examples will be memorized increasing privacy leakage. Particularly,
    memorizing examples sharing similar features with a “wrong" class or excessive
    memorization of one-hot labels may corrupt the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: V-E Memorization and Differential Privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Differential Privacy [[74](#bib.bib74)] (DP) is a commonly employed strategy
    for defending against privacy attacks, which aims to guarantee indistinguishability
    between various data points. In particular, DP ensures that the trained model
    remains largely unchanged when any single example is removed from the training
    set. Within the framework of $(\epsilon,\delta)$-DP setting, DP comprises two
    key components: gradient clipping and the application of noise. Gradient clipping
    restricts the gradients of each example to a predefined boundary, reducing disparities
    in their gradient magnitudes. This facilitates the standardization of gradients
    in terms of magnitude and mitigates the memorization effect. The phenomenon has
    been observed in some cases [[75](#bib.bib75), [24](#bib.bib24)]. Additionally,
    the random noise application also promotes example memorization reduction. When
    models are trained on examples mixed with random noise, the features carried by
    long-tailed examples or atypical features will be diluted, leading the models
    to mainly learn typical patterns. Moreover, neural networks may memorize artificial
    random noise [[13](#bib.bib13), [11](#bib.bib11), [9](#bib.bib9)]. Because we
    always observe that effective DP measures hurt model generalization. Corresponding,
    from a privacy standpoint, we can understand that DP operates by safeguarding
    privacy through the prevention of atypical feature memorization. While a comprehensive
    analysis of DP from a memory perspective is currently lacking, some researchers
    agree that DP can limit example memorization supported by empirical results [[24](#bib.bib24),
    [64](#bib.bib64), [14](#bib.bib14)].'
  prefs: []
  type: TYPE_NORMAL
- en: Summary. DP serves as an effective measure to alleviate the issue of example
    memorization in neural networks, achieved through gradient clipping and the introduction
    of noise. Nevertheless, the efficacy of DP may be guaranteed by reducing the memorization
    of atypical features and increasing the learning of random noise.
  prefs: []
  type: TYPE_NORMAL
- en: V-F Memorization and Other Risks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As DNNs have been widely applied in various social scenarios, the public gradually
    shifted its focus from system performance to additional attributes such as model
    fairness [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)], interpretability [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [84](#bib.bib84), [85](#bib.bib85)], and others. These additional attributes potentially
    exhibit a strong relationship with the phenomenon of memorization, consequently
    leading to additional risks. In terms of fairness, DNNs may inadvertently learn
    societal biases present in the training data from the real world [[86](#bib.bib86)].
    Such biases come from unbalanced subgroups or sensitive attributes, which correlates
    with the long-tailed theory [[13](#bib.bib13)]. This indicates the significant
    role of memorization in the risk of unfairness. Furthermore, certain approaches [[87](#bib.bib87),
    [88](#bib.bib88)] aimed at promoting fairness employ techniques like data augmentation
    and weight reassignment, which may amplify privacy leakage via memorization learning.
    Regarding interpretability, the public views uninterpretable models and predictions
    as uncontrolled risks. Memorization study can help mitigate this risk. Additionally,
    the phenomenon of memorization is also associated with certain risks of violation
    of intellectual property rights or copyrights.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. We believe that the memorization effect has strong relationships with
    fairness, interpretability, and other risks. However, this area of research remains
    largely unexplored.
  prefs: []
  type: TYPE_NORMAL
- en: VI Forgetting Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Forgetting is the opposite of memorization. Generally, neural networks may
    encounter difficulties in continual learning because the learning capacity of
    networks is not infinite. During iterative training, as networks train on new
    examples, they tend to forget learned features or information from previous examples,
    as shown in Figure [6](#S6.F6 "Figure 6 ‣ VI Forgetting Research ‣ Memorization
    in deep learning: A survey"). This phenomenon is known as catastrophic forgetting [[89](#bib.bib89),
    [90](#bib.bib90)]. Variations in data distributions cause models to converge to
    different optimal points. Although there are some methods to overcome this phenomenon [[91](#bib.bib91),
    [90](#bib.bib90), [89](#bib.bib89), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94),
    [91](#bib.bib91)], we still lack an understanding of forgetting especially as
    an opposite of memorization. We may be curious about what information will be
    forgotten, how the forgetting effect impacts model performance and privacy, and
    its relationship with memorization. The main works are presented in Table [IV](#S6.T4
    "TABLE IV ‣ VI Forgetting Research ‣ Memorization in deep learning: A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Main Works about Forgetting'
  prefs: []
  type: TYPE_NORMAL
- en: '| Main Topic | Reference | Background Task | Main Eval. Method | Main Findings
    |'
  prefs: []
  type: TYPE_TB
- en: '| Forgetting about Data | Toneva et al. 2019 [[95](#bib.bib95)] | Supervised
    Classification Task | Forgetting and Learning Event | Atypical examples and noisy
    examples are prone to be forgotten. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Maini et al. 2022 [[96](#bib.bib96)] | Supervised Classification Task
    | Second-Split Forgetting Time | In fine-tune, noisy examples are forgotten quickly
    and seemingly atypical examples are forgotten slowly. |'
  prefs: []
  type: TYPE_TB
- en: '| Forgetting about Privacy | Jagielski et al. 2022 [[67](#bib.bib67)] | Various
    Tasks | Probabilistic Memorization Evaluation | Standard image, speech, and language
    models empirically do forget examples over time. |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/cf09ca8180cfc702b948943444b36390.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Demonstration of the Forgetting Phenomenon.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Forgetting Definition and Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VI-A1 Forgetting Definition based on Accuracy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Toneva et al. [[95](#bib.bib95)] propose the forgetting and learning event:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Forgetting and Learning Event. For supervised classification task, given an
    example $\mathbf{x}_{i}$, the predicted label for example $\mathbf{x}_{i}$ obtained
    after $t$ steps of SGD is $\hat{y}_{i}^{t}=\arg\max_{k}p(y_{ik}|\mathbf{x}_{i};\theta^{t})$
    and accuracy is $acc_{i}^{t}=\vmathbb{1}_{\hat{y}^{t}_{i}=y_{i}}$. Therefore,
    the forgetting event is that example $i$ is misclassified at step $t+1$ after
    having been correctly classified at step $t$ (i.e. $acc^{t}_{i}>acc^{t+1}_{i}$).
    Conversely, a learning event has occurred if $acc^{t}_{i}<acc^{t+1}_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Following the forgetting and learning event definitions, Maini et al. [[96](#bib.bib96)]
    definite First-Split Learning Time (FSLT) to demonstrate the first epoch that
    model learns an example and Second-Split Forgetting Time (SSFT) to describe the
    forgetting time in the fine-tuned stage.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: First-Split Learning Time. For $\{\mathbf{x_{i}},y_{i}\}\in D_{A}$, learning
    time is defined as the earliest epoch during the training of a classifier $f$
    on $D_{A}$ after which it is always classified correctly, i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle FSLT_{i}=\arg\min_{t^{*}}(\hat{y}_{i,(A)}^{t}=y_{i},\forall
    t\geq t^{*})\ \forall\{\mathbf{x_{i}},y_{i}\}\in D_{A},$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $t$ denotes epoch, $A$ is the pre-training stage and $D_{A}$ is the training
    dataset. The $f_{A}$ represents the trained model with 100% training accuracy
    on the $D_{A}$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Second-Split Forgetting Time. Let $\hat{y}_{i,(A\rightarrow B)}^{t}$ to denote
    the prediction of example $\{\mathbf{x_{i}},y_{i}\}\in D_{A}$ after training $f_{(A\rightarrow
    B)}$ for $t$ epochs on $D_{B}$. Then, for $\{\mathbf{x_{i}},y_{i}\}\in D_{A}$
    forgetting time is defined as the earliest epoch after which it is never classified
    correctly, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle SSFT_{i}=\arg\min_{t^{*}}(\hat{y}_{i,(A\rightarrow B)}^{t}\neq
    y_{i},\forall t\geq t^{*})\ \forall\{\mathbf{x_{i}},y_{i}\}\in D_{A},$ |  | (9)
    |'
  prefs: []
  type: TYPE_TB
- en: where $D_{B}$ is a held-out split dataset (without $\{\mathbf{x_{i}},y_{i}\}$)
    of $D_{A}$, $f_{(A\rightarrow B)}$ is initialized by $f_{A}$.
  prefs: []
  type: TYPE_NORMAL
- en: These definitions can be used to measure forgetting in supervised classification
    tasks. Based on the nature of forgetting, i.e. learned features have been lost,
    it is reasonable to obverse forgetting depending on the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A2 Forgetting Definition based on Membership Inference Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Jagielski et al. [[67](#bib.bib67)] measure the ratio of forgetting based on
    a membership inference attack.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Rate of Forgetting. A training algorithm $\mathcal{T}$ is said to $(\mathcal{A},\alpha,k)$-forget
    a training example $z$ if, $k$ steps after $z$ is last used in $\mathcal{T}$,
    a privacy attack $\mathcal{A}$ achieves no higher than success rate $\alpha$.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the membership inference attack relies on particular features,
    thus, the reduced risk could be regarded as forgetting. This is also an effective
    definition to describe forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Forgetting Phenomenon
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some existing studies provide interesting evidence to understand the forgetting
    phenomenon. Toneva et al. [[95](#bib.bib95)] study example forgetting during DNN
    learning and use accuracy as the metric to define the example forgetting event,
    i.e. an example that has been correctly classified becomes misclassified. They
    find that generalized examples are unforgettable (always correctly classified)
    but atypical examples and noisy examples are prone to be forgotten. This is connected
    with the related memorization findings [[13](#bib.bib13), [11](#bib.bib11)] that
    forgetting occurs on those memorized examples. Even with some intermediate forgetting
    events, DNNs still can correctly classify all training examples and finally achieve
    100% training accuracy, indicating memorization is a challenging but forced learning
    stage. Another finding is removing a part of these generalized/unforgettable examples
    will not damage the model generalization performance. This may be explained as
    DNNs do not need to repeatedly learn common patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this research, Maini et al. [[96](#bib.bib96)] propose second-split
    forgetting time (SSFT) to track the epoch (if any) after which an original training
    example is forgotten as the network is fine-tuned on a randomly held-out partition
    of the data. In the fine-tuned stage, they demonstrate that noisy examples are
    forgotten quickly and seemingly atypical examples are forgotten slowly, while
    typical examples are never forgotten. Tirumala et al. [[16](#bib.bib16)] employ
    Definition [2](#Thmdefinition2 "Definition 2 ‣ II-A2 Exact Memorization ‣ II-A
    Memorization Definitions in Generalization Domain ‣ II Memorization Definition
    ‣ Memorization in deep learning: A survey") to measure the single-injected validation
    dataset forgetting dynamics and find the exact memorization from a higher point
    gradually drops to the forgetting baseline as the number of epoch increases. The
    forgetting baseline may represent generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: Jagielski et al. [[67](#bib.bib67)] focus on the privacy risk associated with
    forgetting. They utilize the membership inference probability to evaluate forgetting
    and believe that the size of the training dataset, repetitions, and hardness mainly
    influence forgetting. Examples used early in model training may be more robust
    to privacy attacks, while repeated examples are harder to forget. During the forgetting
    phase, the membership inference probability of typical examples is still around
    50% which is lower than the inference risk of atypical examples. This finding
    corresponds to previous studies. They also illustrate that non-convexity and deterministic
    SGD can prevent forgetting. Another piece of evidence is the variation in local
    data distribution of every batch of data leads optimization techniques to converge
    to different local optimal points. Therefore, some lack-of-representativeness
    information may be lost.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. In the single-task learning scenario, memorized examples based on the
    view of performance metrics are easy to forget. However, the low performance does
    not mean all features of these memorized examples have been forgotten because
    they still pose high inference risks compared to generalized examples.
  prefs: []
  type: TYPE_NORMAL
- en: VII Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Utilizing the memorization and forgetting effects of neural networks, researchers
    have developed various applications in several scenarios. We organize these applications
    in Tabel [V](#S7.T5 "TABLE V ‣ VII Application ‣ Memorization in deep learning:
    A survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Related Application about Memorization and Forgetting'
  prefs: []
  type: TYPE_NORMAL
- en: '| Application | Associated Memorization or Forgetting Effect | Technology |
    Reference |'
  prefs: []
  type: TYPE_TB
- en: '| Noisy Label Learning | DNNs are prior to learning patterns in the early training
    stage. | Noise Control Scheduler | Han et al., 2018 [[97](#bib.bib97)]; Yao et
    al., 2020 [[98](#bib.bib98)] |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Regularization | Liu et al., 2020 [[99](#bib.bib99)]; Xia et al., 2020 [[100](#bib.bib100)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pre-training on random labels can learn label-irrelevant generalized features.
    | Random Labels Pre-training | Pondenkandath et al., 2018 [[101](#bib.bib101)];
    Maennel et al., 2020 [[30](#bib.bib30)] |'
  prefs: []
  type: TYPE_TB
- en: '| Example Enhancement | Long-tailed examples are prone to be memorized. | Example
    Reweighting | Zhou et al., 2022 [[102](#bib.bib102)]; Xu et al., 2023 [[65](#bib.bib65)];
    Zhang et al., 2023 [[103](#bib.bib103)] |'
  prefs: []
  type: TYPE_TB
- en: '| Privacy Audit and Protection | Memorization cause inference and extraction
    risks | Membership Inference and Extraction Attacks | Carlini et al., 2021 [[9](#bib.bib9)];
    Carlini et al., 2022 [[24](#bib.bib24)] |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Memorization Suppression | Maini et al., 2023 [[15](#bib.bib15)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | The privacy of forgotten examples are relatively guaranteed. | Unlearning
    Technology | Zhu et al., 2020 [[104](#bib.bib104)] |'
  prefs: []
  type: TYPE_TB
- en: '| Memorization Architecture | Explicit memorization helps specific task performance.
    | Explicit Memorization Structure | Khandelwal et al., 2019 [[105](#bib.bib105)];
    Yogatama et al., 2021 [[106](#bib.bib106)]; Guu et al., 2020 [[107](#bib.bib107)];
    Lewis et al., 2020 [[108](#bib.bib108)]; Lewis et al., 2020 [[109](#bib.bib109)];
    Wu et al., 2022 [[110](#bib.bib110)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | DNNs could be viewed as databases or knowledge-bases | DNN Database |
    Tay et al., 2022 [[111](#bib.bib111)] |'
  prefs: []
  type: TYPE_TB
- en: '| Model Editing | Language models memorize a lot of facts. | Memorization Neurons
    Modification | Dai et al., 2021 [[112](#bib.bib112)]; De Cao et al., 2021 [[113](#bib.bib113)];
    Mitchell et al., 2021 [[114](#bib.bib114)]; Meng et al., 2022 [[115](#bib.bib115),
    [116](#bib.bib116)]; Gupta et al., 2024 [[117](#bib.bib117)]. |'
  prefs: []
  type: TYPE_TB
- en: VII-A Noisy Label Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning with noisy labels is challenging, as neural networks have powerful
    memorization abilities that can completely memorize all noisy labels in the later
    training stage. However, this memorization phenomenon is utilizable.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, one effective method for learning with noisy labels involves selecting
    potentially clean learning examples in each iteration for training [[118](#bib.bib118),
    [119](#bib.bib119), [120](#bib.bib120), [98](#bib.bib98)]. In example selection,
    it is challenging to choose a reasonable threshold based on example loss to drop
    underlying noisy examples because dropping too many examples may lose some useful
    features and lead to lower accuracy [[119](#bib.bib119)]. According to previous
    studies [[25](#bib.bib25)], we know that DNNs usually learn easy patterns before
    overfitting the noisy examples, and the pattern learning period can be regarded
    as the early learning stage. Thus, in the early stage of noisy label learning,
    it is not necessary to drop a large number of examples and the threshold could
    be loose. As training epochs increase, the dropping rate will also increase to
    avoid noisy label memorization. Therefore, it requires to define a scheduler used
    to control the example selection. Han et al. [[119](#bib.bib119)] propose a novel
    pre-defined scheduler. The scheduler is a non-increasing function and is controlled
    by noise level and current epoch. At the start of training, the scheduler would
    not drop any examples, but as the training continues, the dropping rate would
    increase. However, this pre-defined scheduler may not be "optimal" and is limited
    to specific tasks and datasets. Yao et al. [[98](#bib.bib98)] improve the scheduler
    as an AutoML problem to conduct a search, which outperforms previous works.
  prefs: []
  type: TYPE_NORMAL
- en: Loss modification and regularization technology represent another approach to
    learning with noisy labels. Existing works [[99](#bib.bib99), [100](#bib.bib100)]
    attempt to control noise during the early-learning stage. Liu et al. [[99](#bib.bib99)]
    develop early-learning regularization (ELR). This regularization item can facilitate
    learning from clean examples by prioritizing pattern learning and restraining
    noise in the training dataset by maximizing the inner product between the model
    output and the targets. Specifically, the regularization item can diminish noisy
    examples’ effect on the gradient, implicitly preventing the memorization of wrong
    labels. Modifying the gradient update based on the differences between pattern-associated
    neurons and memorization-associated neurons can provide a similar regularization
    effect. Xia et al. [[100](#bib.bib100)] find parameters have different tendencies
    that some respond for fitting clean labels as critical parameters and some for
    memorization as non-critical parameters. It is possible to assess the parameter
    tendency in each iteration based on the inner product between the value of the
    parameters and the gradient with respect to the parameters. Then, performing positive
    normal gradient updates on critical parameters and applying weight decay only
    to non-critical parameters mitigates noise learning during the early learning
    stage. If the minimum classification error is achieved on the validation dataset,
    the training should be stopped early.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to utilize the memorization effect in noisy label learning is pre-training,
    expecting that models can learn label-irrelevant useful features. Basically, the
    data distribution always has some low-level and label-irrelevant features. For
    instance, in colorful image classification, the data distribution contains colorful
    low-level features compared to the gray in random noise. One previous study [[101](#bib.bib101)]
    empirically proves that performing unsupervised pre-training in the form of training
    for classification with random labels may boost initialized training speed and
    improve the generalization performance. However, these label-irrelevant features
    are all low-level generalized features and can be easily learned in the normal
    training process. Maennel et al. [[30](#bib.bib30)] demonstrate that the pre-training
    on random labels may pose a positive effect or negative effect on downstream tasks.
    They show that aligned shallow layers improve the performance of downstream tasks.
    However, the neural activations at the deep layers may drop abruptly and permanently
    on downstream tasks due to specialization, which may impair downstream task performance.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Example Enhancement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data distribution is not uniform in the real environment. Atypical examples
    with low frequencies in the data distribution always exist and cause models to
    memorize them instead of learning patterns based on the long tail theory [[11](#bib.bib11),
    [13](#bib.bib13)]. Some existing research applies example reweighting technology
    to handle these long-tailed examples.
  prefs: []
  type: TYPE_NORMAL
- en: In self-supervised long-tailed representation learning, learning long-tailed
    examples is generally challenging. Zhou et al. [[102](#bib.bib102)] employ the
    memorization effect to boost the performance of contrastive learning on tail examples.
    Specifically, they attempt to identify tail examples and apply heavier augmentation,
    consistently improving the performance of these examples. Due to the high computational
    cost associated with tracking memorization using memorization scores [[11](#bib.bib11)].
    They tend to trace the historical losses of each example as memorization clues.
    Then, they construct the normalization of momentum losses which indicates the
    memorization level of examples. Based on the memorization level, a stronger information
    discrepancy between views will be constructed to emphasize the importance of tail
    examples. Their results demonstrate that the method is effective.
  prefs: []
  type: TYPE_NORMAL
- en: For adversarial training, Xu et al. [[65](#bib.bib65)] discover that memorizing
    atypical examples is only effective in improving DNN’s accuracy on clean atypical
    examples but hardly improves their adversarial robustness. Moreover, fitting some
    atypical adversarial examples even damages the model’s robustness. They believe
    some atypical examples share similar features with a wrong class and become harmful
    in the context of adversarial training. This may uncover the key differences between
    traditional standard training and adversarial training. Motivated by their findings,
    they propose an algorithm called Benign Adversarial Training (BAT), which can
    mitigate the negative influence of memorizing those harmful atypical examples,
    simultaneously preserving the model’s ability to learn those useful/benign atypical
    examples. In BAT, the core is the example reweighting technology that assigns
    the harmful atypical examples a small weight to reduce the negative effect. Additionally,
    harmful atypical examples can be detected by the high memorization score and high
    misclassification confidence in the standard training. Another related work [[103](#bib.bib103)]
    also agrees the atypical examples may hurt DNN’s robustness. They similarly employ
    example reweighting technology to reassign example update weights. Their method
    actually builds a $k$NN structure to measure the example typicalness. This structure
    has been called the codebook and trained concurrently with the classifier. The
    codebook training is actually clustering based on distance, finding the central
    points approaching the nearest embedding vectors. These central points could be
    regarded as the most typical embedding vectors. Then, depending on the distance
    between input batch embedding and central vectors in the codebook, it is effective
    in identifying the atypical examples. Subsequently, the distances can be utilized
    as weights to enhance atypical example learning in standard training and reduce
    atypical example learning in adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Privacy Audit and Protection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a deeper understanding of the memorization phenomenon, some works [[75](#bib.bib75),
    [9](#bib.bib9), [24](#bib.bib24), [29](#bib.bib29), [14](#bib.bib14)] have demonstrated
    that memorized particular features become sources of risk for several attacks.
    Thus, researchers can apply the memorization effects to audit security risks and
    develop novel defense strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In section [V](#S5 "V Underlying Risks of Memorization Learning ‣ Memorization
    in deep learning: A survey"), we have introduced memorization-associated risks.
    For membership inference attacks, it is possible to utilize the memorized particular
    features of examples to achieve high true-positive rates at low false-positive
    rates [[24](#bib.bib24)]. Moreover, extractable training examples also should
    depend on those particular features [[9](#bib.bib9)]. However, adversely, we actually
    can employ these attacks as memorization audit tools to help model compliance.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, mitigating the memorization effect may reduce the corresponding
    risks. Maini et al. [[15](#bib.bib15)] propose a novel dropout technology. This
    technology utilizes the memorization effect to guide the specific neurons to memorize
    the specific details of examples, while other neurons work for pattern learning.
    When the neural network drops these memorization neurons, those memorized particular
    features are dropped along with the neurons. Another promising strategy is applying
    the forgetting mechanism to unlearning technology. Private features of long-tailed
    examples could be forgotten under continuous learning with tail-changing data
    distribution. This means the new data distribution after removing the target unlearning
    example could not provide similar features for the target unlearning example,
    and features of the example will be gradually forgotten because parameters related
    to the example may be updated. Zhu et al. [[104](#bib.bib104)] apply the forgetting
    phenomenon without retraining to update a Transformer on the new integrated dataset
    without stale knowledge. However, it is still unsure if we can control the forgetting
    process and achieve the expected unlearning effect.
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Memorization Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although memorization leads to privacy risks, we may require the memorization
    ability of networks. Specifically, memorizing or caching common input and output
    can improve inference speed. Networks or large language models (LLMs) also can
    function as an information retrieval system. Despite this, some external memory
    structures like $k$NN and key-value memory could serve as additional components
    to improve task performance.
  prefs: []
  type: TYPE_NORMAL
- en: Khandelwal et al. [[105](#bib.bib105)] attempt to combine the $k$NN and language
    model. They find that applying $k$NN as an external memorization structure can
    improve performance. Qualitatively, this kind of model is particularly helpful
    in predicting rare patterns that allow rare features to be memorized explicitly
    rather than implicitly in model parameters. Search based on similarity is better
    than predicting the next word in the long tail. Moreover, they also extend this
    method to the machine translation [[121](#bib.bib121)] and achieve non-trivial
    performance. Yogatama et al. [[106](#bib.bib106)] modify this approach by a gating
    mechanism and context compression retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the explicit key-value memory can also improve the effectiveness
    of inference. In the context of Transformers, Févry et al. [[122](#bib.bib122)]
    and Verga et al. [[123](#bib.bib123)] apply similar methods to install an external
    key-value memory to store entities and facts. With this explicit memorization,
    models can achieve higher performance in question-answering tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For information retrieval, REALM [[107](#bib.bib107)], MARGE [[108](#bib.bib108)],
    and RAG [[109](#bib.bib109)] apply knowledge retrieval in pre-training. REALM [[107](#bib.bib107)]
    augments language model pretraining with a knowledge retriever, which allows the
    model to retrieve documents from a large corpus, used during pre-training, fine-tuning,
    and inference. Next, MARGE [[108](#bib.bib108)] is trained by self-supervising
    the reconstruction of the target text. This process first involves retrieving
    a set of related texts (in many languages) and then maximizing their likelihood
    of generating the original. RAG [[109](#bib.bib109)] combines parametric memory
    and non-memory for language generation. The parametric memory is a pre-trained
    seq2seq model and the non-parametric memory is a dense vector index of Wikipedia.
    These pre-training models all gain non-trivial results in the downstream tasks
    like question-answering tasks. Regarding the latest studies, Wu et al. [[110](#bib.bib110)]
    envision language models that can simply read and memorize new data at inference
    time, thus acquiring new knowledge immediately. By using attention, a model can
    simply memorize facts (e.g. function definitions) by storing them as key-value
    pairs in long-term memory. Then, it can retrieve those facts later by creating
    a query that attends to them. In this case, attention acts as a form of information
    retrieval, allowing the model to look up facts that it has seen previously. Thus,
    they propose a Transformer model with $k$NN-augmented attention that unifies attention
    and retrieval. Their experiment demonstrates that an approximate $k$NN lookup
    into a non-differentiable memory of recent key-value pairs improves language modeling
    across various benchmarks and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A more interesting work directly regards Transformer memory as a differentiable
    search index (DSI) [[111](#bib.bib111)]. All information about the corpus is encoded
    in the parameters of a Transformer. In other words, a DSI model answers queries
    directly using only its parameters, dramatically simplifying the whole retrieval
    process. At inference time, the trained model takes as input a text query and
    outputs the id of the correlated document.
  prefs: []
  type: TYPE_NORMAL
- en: VII-E Model Editing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the computational burden of training large language models (LLMs) and
    the requirement of updating information in LLMs, researchers attempt to directly
    edit LLMs neurons to update facts [[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117)]. LLMs learn a variety
    of facts about the world during pre-training and these facts are stored in model
    weights [[124](#bib.bib124)]. Specifically, the MLP weights actually serve as
    key-value memories [[125](#bib.bib125), [126](#bib.bib126), [115](#bib.bib115)].
    Thus, editing these neurons to update facts becomes a practical approach.
  prefs: []
  type: TYPE_NORMAL
- en: Dai et al. [[112](#bib.bib112)] first identify knowledge-containing neurons
    in a model using integrated gradients [[127](#bib.bib127)] and then modify the
    selected neurons to edit facts in a model. Specifically, they focus on evaluating
    BERT’s performance on the fill-in-the-blank cloze task. In this task, they introduce
    a technique called knowledge attribution, aiming to find the neurons in BERT that
    represent specific facts. Their analysis demonstrates a positive correlation between
    the activation of these identified ’knowledge neurons’ and the accurate expression
    of the corresponding facts. De Cao et al. [[113](#bib.bib113)] and Mitchell et
    al. [[114](#bib.bib114)] train a hypernetwork that predicts the new weights of
    the model being edited. This method modifies a fact rapidly without affecting
    the rest of the knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meng et al. [[115](#bib.bib115), [116](#bib.bib116)] develop two "locating
    and editing" technologies: Rank-One Model Editing (ROME) and Mass-Editing Memory
    In a Transformer (MEMIT). They create a method for causal intervention to identify
    the activation of neurons crucial for a model’s factual predictions. Then, they
    directly update particular "knowledge-containing" components of the model without
    requiring to train additional models. Additionally, this approach is applicable
    to any transformer-based LLM. Subsequently, Gupta et al. [[117](#bib.bib117)]
    build a unifying conceptual framework for ROME and MEMIT following the preservation-memorization
    objective of model editing. During the editing process, this approach preserves
    the representations of certain selected vectors while memorizing the representations
    of new factual information. In summary, model editing demonstrates a good prospect
    of memorization utilization.'
  prefs: []
  type: TYPE_NORMAL
- en: VIII Discussion and Future Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The memorization effect of DNN is an ongoing field with significant implications
    for the interpretability, generalization, and security of AI. In this section,
    we will discuss existing research findings and possible future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-A Memorization and Forgetting Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The memorization and forgetting mechanism remains unclear and confusing. However,
    based on existing studies, we have known some memorization and forgetting truths
    on the classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard training framework always leads DNNs to the minimal loss [[44](#bib.bib44)];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNNs can memorize common modern training datasets, even when the dataset is
    randomly labeled [[8](#bib.bib8)];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long-tailed examples that lack representation in the data distribution like
    atypical examples and noisy examples are prone to be memorized [[13](#bib.bib13),
    [11](#bib.bib11)];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNNs cannot identify atypical examples and noisy examples in training [[15](#bib.bib15)];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNNs tend to prioritize the memorization of repeated data [[17](#bib.bib17),
    [28](#bib.bib28), [29](#bib.bib29)];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNNs have a critical early learning stage where pattern learning takes domination [[25](#bib.bib25)];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memorization appears to be confined to a limited set of neurons across various
    layers in DNNs [[15](#bib.bib15)];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memorization is not responsible for overfitting [[56](#bib.bib56), [55](#bib.bib55),
    [15](#bib.bib15)];
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long-tailed examples are prioritized to be forgotten, and noisy examples are
    forgotten more quickly than atypical examples [[96](#bib.bib96), [95](#bib.bib95)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: According to these observations, it is reasonable to infer that, at least in
    the context of classification tasks, the memorization phenomenon is a property
    of the standard DNN gradient descent training framework. Specifically, minimizing
    the loss leads to the memorization effect. For instance, when a neural network
    has been fed some data during training, some parameters are activated and updated.
    The network may gradually build several mapping paths of common patterns, and
    each input example passes these mapping paths and then gains a probability score
    for every class. These probability scores constitute a probability vector and
    the predicted label will be the class with the maximum score. However, long-tailed
    examples cannot gain reasonable scores on all of these mapping paths because they
    have no strong patterns compared to more generalized examples. Consequently, the
    network may build a unique mapping path for each long-tailed example to minimize
    the loss, and this is the memorization phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering training with the batch stochastic gradient descent method, in
    the beginning, the network may randomly create unique mapping paths for each example
    to fit the labels. Subsequently, because of inherent patterns of data distribution,
    some of these mapping paths gradually align. The alignment can be regarded as
    the pattern extraction and may correlate with the early learning phenomenon [[43](#bib.bib43),
    [42](#bib.bib42), [25](#bib.bib25)] because the alignment effect decreases the
    loss of representative examples in the data distribution. This alignment precedes
    memorization because early gradients represent the direction that can most effectively
    reduce the loss. Additionally, long-tailed examples may also participate in the
    alignment but not align well, which let these examples experience forgetting events [[95](#bib.bib95)]
    (Definion [8](#Thmdefinition8 "Definition 8 ‣ VI-A1 Forgetting Definition based
    on Accuracy ‣ VI-A Forgetting Definition and Evaluation ‣ VI Forgetting Research
    ‣ Memorization in deep learning: A survey")), i.e. some long-tailed examples could
    be correctly classified at previous steps but misclassified at later steps. Simultaneously,
    the network could employ some extra capacity or parameters to memorize some particular
    features of long-tailed examples that are not aligned well in the early learning
    stage to reduce the loss. This is evidenced by concurrent improvements in accuracy
    for randomly labeled examples and clean examples [[15](#bib.bib15)]. This also
    explains why memorization does not depend on overfitting. Moreover, this indicates
    memorization learning and pattern learning are not totally discrete and contrary.
    They imply the difficulty of pattern extraction on examples. The more challenging
    the pattern extraction, the more apparent the tendency to be memorized. After
    the alignment phase, the network will prioritize memorizing long-tailed examples,
    which can lead to close-to-optimal generalization error based on the long tail
    theory [[13](#bib.bib13), [11](#bib.bib11)]. After memorizing of long-tailed examples,
    the network may exhibit the best generalization performance. However, if the training
    continues, diminishing the loss becomes challenging. The network may develop unique
    paths for all examples, causing the predicted vector to closely approach the label
    vector, even resulting in zero training error. This phenomenon is referred to
    as neural collapse [[44](#bib.bib44)], where each example in the same class collapses
    to the same representation. For architecture, unique memorization mapping paths
    may require only a few parameters across layers because these paths are not based
    on pattern recognition. Therefore, it is reasonable to observe that memorization
    appears to be confined to a limited set of neurons across various layers in DNNs [[15](#bib.bib15)].
    Furthermore, if we take into account forgetting, these memorized examples become
    highly unstable. This instability arises because even if a small part of the associated
    parameters has been updated, these examples are likely to be misclassified. This
    phenomenon explains why long-tailed examples are particularly prone to being forgotten [[96](#bib.bib96)].
    Meanwhile, there may remain some unchanged associated parameters that pose privacy
    risks [[67](#bib.bib67)].'
  prefs: []
  type: TYPE_NORMAL
- en: Certainly, our theoretic model of memorization and forgetting in the classification
    task is an assumption. Further experimentation and empirical evidence are required
    to fully explain the memorization phenomenon. Understanding the memorization mechanism
    carries significant implications for enhancing the interpretability of DNNs. To
    effectively understand this mechanism, it is crucial to describe the spatiotemporal
    memorization process. In terms of training periods, the primary objectives include
    characterizing memorization in different stages and investigating whether the
    memorization phenomenon constitutes a form of overlearning. Concerning neural
    network components, it becomes essential to quantitatively explain the distribution
    of memorization across layers or components and evaluate whether certain neurons
    exhibit a tendency for memorizing examples. Additionally, it is important to differentiate
    between memorization learning and pattern learning neurons. Moreover, different
    training frameworks may have distinct memorization phenomena, particularly for
    unsupervised tasks and multiple-task scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-B Memorization and Forgetting for Training Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: DNNs offer significant advantages in processing complex real-world data such
    as images and text compared to traditional machine learning methods [[128](#bib.bib128),
    [52](#bib.bib52)]. A notable observation is that DNNs can effectively extract
    common features or patterns from the data distribution. However, controlling this
    extraction process is challenging, and some uncommon yet useful features may not
    be learned well [[13](#bib.bib13), [11](#bib.bib11)]. At the feature level, out-of-distribution
    features and rare but useful features are both less representative. This may explain
    why memorization learning cannot identify atypical examples and noisy examples.
    Moreover, we may rethink how to describe complex data in reality to keep features
    balanced. This encourages us to contemplate aspects such as data dimension, granularity [[129](#bib.bib129)],
    and distribution [[130](#bib.bib130), [131](#bib.bib131)] to enhance the performance
    of DNNs. Additionally, the size of the training dataset probably does not serve
    as the sole determining factor for task performance [[132](#bib.bib132)].
  prefs: []
  type: TYPE_NORMAL
- en: Training Framework
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It is understood that the stochastic gradient descent method will aggressively
    minimize the loss function until reaching extreme mathematical conditions such
    as neural collapse [[44](#bib.bib44)]. However, the extreme conditions may not
    meet our requirements, and even potentially introduce further challenges. From
    this perspective, the loss function really matters and decides the learning direction.
    The challenge lies in the fact that loss functions may not always accurately measure
    the true loss associated with the assigned task. For instance, in a classification
    task, a model with minimal loss may have poor generalization performance due to
    overfitting. Therefore, the memorization and forgetting effect may serve as adaptive
    solutions to address this conflict.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The impact of neural network architecture details on the memorization phenomenon
    remains unclear. Different layers within the architecture may assume distinct
    roles, with certain layers potentially exhibiting a preference for memorization.
    Viewing the architecture in terms of layer depth, deeper layers may tend to learn
    more specialized features [[46](#bib.bib46)] although these features are not completely
    for memorization. Specialized features still retain patterns, whereas memorized
    features may lack patterns and serve primarily to mark data. Therefore, deeper
    layers do not function for memorization [[15](#bib.bib15)]. Regarding the size
    of networks, the memorization tendency also correlates with the size of the training
    dataset. A larger model trained on a small dataset may lead to significant overfitting
    and a strong inclination toward memorization. Conversely, larger datasets, which
    contain more diverse patterns, tend to reduce the preference for memorization
    but may increase the probability of underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Memorization and forgetting manifest differently across various tasks. Presently,
    most memorization studies focus on the classification task, where the memorization
    phenomenon entails the utilization of a small set of parameters to uniquely mark
    examples. The classification task, being a dimension reduction task, can apply
    this way to minimize the loss. However, for other tasks such as generative tasks,
    the dynamics differ. Obviously, the generative task could be a dimension increment
    task like GAN [[133](#bib.bib133)], Diffusion model [[134](#bib.bib134)], and
    GPT [[135](#bib.bib135)], which aim to learn a target data distribution. Thus,
    the generalization of generative models refers to the model’s ability to produce
    accurate, relevant, and coherent outputs to cover the target data distribution.
    From this viewpoint, the memorization phenomenon in the generative task could
    be significantly different from the classification task. For instance, generative
    models may use enormous parameters to memorize almost all features of those long-tailed
    examples. This phenomenon has been demonstrated in some works [[9](#bib.bib9),
    [10](#bib.bib10)]. Furthermore, multiple-task learning and continuous learning
    are also distinct. A famous phenomenon called catastrophic forgetting [[90](#bib.bib90),
    [91](#bib.bib91), [89](#bib.bib89)] means that neural networks are hard to retain
    learned knowledge on multiple and dynamic data distributions. As the data distribution
    shifts based on sub-tasks, and the model learning capacity is limited, separating
    learned features becomes challenging. In such scenarios, memorization may have
    a very beneficial effect in preserving learned features.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-C Memorization and Forgetting for Privacy and Security Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Privacy Leakage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Privacy leakage is a common problem in neural networks. Direct privacy leakage
    includes the inference attack and the inversion/extraction attack we mentioned
    before. According to some empirical evidence [[24](#bib.bib24), [67](#bib.bib67)],
    the vulnerability of privacy leakage often arises from memorization. For membership
    inference attacks, unique features of examples are heavily exploited rather than
    representative features that generally improve false-positive rates [[24](#bib.bib24)].
    It is known that examples sharing patterns but not in the training set are very
    easily inferred as membership. This protects the privacy of those representative
    examples. Conversely, memorized long-tailed examples are more vulnerable. In inversion/extraction
    attacks, the adversary can generally produce representative data based on patterns.
    However, the representative data could be common knowledge and not private. The
    real and particular data belonging to the training dataset is more valuable for
    the adversary. Due to varied behaviors of memorization in various tasks, generative
    models are more vulnerable from the perspective of memorization because the generative
    models may be required to memorize more long-tailed examples to fit the target
    data distribution. The memorization process likely contains most features of certain
    examples, allowing these examples to be reconstructed in a lossless manner under
    inversion/extraction attacks. Some empirical results have illustrated that memorization
    is the source of extraction [[10](#bib.bib10), [9](#bib.bib9)]. Qualitatively,
    the memorization effect indeed poses risks of privacy leakage. Therefore, mitigating
    the memorization effect may reduce risks associated with inference and inversion
    attacks. Nonetheless, we acknowledge that memorization somehow contributes to
    task performance [[13](#bib.bib13), [11](#bib.bib11)]. Considering a trade-off
    framework could be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Malicious Attacks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Poisoning, backdoor, and adversarial attacks are typical malicious attacks.
    These attacks can directly disable networks or embed malicious triggers to mislead
    models. Due to the absence of specific threat evaluation, we only conduct some
    hypothetical discussions. Suppose the attack just modifies the training data without
    optimization and targets disabling networks like label flip and random noise.
    In such cases, the model fails because it cannot learn correct patterns following
    the gradient direction and has to memorize them. Therefore, the memorization phenomenon
    is an adaptive process. For induced attacks without optimization [[136](#bib.bib136)],
    these attacks can install malicious triggers in networks, the triggers could be
    learned via pattern learning or memorization learning. Specifically, this depends
    on the feature distribution of triggers. If the backdoor feature is long-tailed,
    here applying memorization, otherwise it is pattern learning. Finally, some malicious
    attacks are based on optimization, the adversary can submit artificial features
    or gradients to the networks [[68](#bib.bib68)]. The synthetic features or gradients
    are out of the standard training framework, so it is challenging to discuss the
    memorization effect under this condition. This requires further studies.
  prefs: []
  type: TYPE_NORMAL
- en: Forgetting Guarantee
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: From the perspective of privacy, the forgetting phenomenon actually provides
    the privacy guarantee by ensuring that previously learned particular features
    of examples will be forgotten in later stages of training without prompt repetition [[16](#bib.bib16)].
    Therefore, the input order of examples may also impact the example privacy. Forgetting
    also corresponds to machine unlearning technology. At the example level, typical
    examples may only provide generalized patterns. Unlearning these examples is not
    valid because other examples also provide similar features. However, for long-tailed
    examples, after removing these examples from the training dataset, the model will
    gradually forget particular features of them during constant training and the
    corresponding privacy risk is simultaneously reducing [[67](#bib.bib67)]. However,
    the privacy onion effect [[21](#bib.bib21)] is another problem. This effect indicates
    that the removal of some most vulnerable data could improve the threat of other
    vulnerable data. Consequently, it is uncertain how the forgetting phenomenon and
    relevant unlearning technologies quantitatively reduce privacy risks.
  prefs: []
  type: TYPE_NORMAL
- en: Memorization Inhibition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Considering the underlying privacy risks of memorization, we could apply some
    technologies or strategies to inhibit memorization. In relevant technologies,
    data augmentation and regularization can help inhibit memorization and improve
    generalization. Data augmentation enhances the patterns within the data, particularly
    for those long-tailed examples. Regarding regularizers, weight decay technology
    restrains the feature space and prevents extreme parameters used in memorization,
    while the dropout strategy could randomly drop memorization activation. Moreover,
    it’s feasible to guide memorization to specific neurons and drop them when testing [[15](#bib.bib15)].
    However, this technology may drop some features of atypical examples. Despite
    this, we can also follow the memorization mechanism to propose some new regularizers
    to mitigate the memorization effect. Differential privacy is another effective
    tool. The added noise would cover some features that tend to be memorized, potentially
    causing the networks to memorize the added noise rather than unique features.
    However, this may result in performance degradation. Thus, while memorization
    inhibition can protect privacy, it may simultaneously impair generalization performance.
  prefs: []
  type: TYPE_NORMAL
- en: Threat Evaluation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The perspective of memorization and forgetting offers a novel and interesting
    entry point to re-evaluate existing threat and defense strategies. The memorization
    and forgetting mechanism can enhance the interpretability of certain threats,
    thereby deeply understanding existing threats. From this perspective, we could
    propose new solutions or enhance existing defense methods. Additionally, this
    viewpoint can assist in uncovering previously unknown threats. It is essential
    to systematically assess how memorization and forgetting influence these threats
    and defense methods.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-D Application Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As memorization is an innate feature of DNNs, we can apply the advantages of
    the memorization effect to assist in some specific tasks and mitigate the disadvantages
    in some scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In positive terms, large language models directly benefit from memorization
    abilities in tasks like question-answer. Memorization can serve as an additional
    structure to cache representations [[105](#bib.bib105), [121](#bib.bib121)] or
    key-value pairs, improving network speed and performance [[122](#bib.bib122),
    [123](#bib.bib123)]. Furthermore, networks can function as databases or knowledge
    bases with the memorization effect [[111](#bib.bib111)]. Additionally, the memorization
    of LLMs can be modified directly to update facts. LLMs learn a variety of facts
    about the world during pretraining and these facts are stored in model weights [[124](#bib.bib124)],
    suggesting that MLP weights act as key-value memories [[125](#bib.bib125), [126](#bib.bib126),
    [115](#bib.bib115)]. Thus, editing memorization neurons for injecting new information
    has been a popular technology called model editing to update facts which sidesteps
    the computational burden associated with training a wholly new model [[137](#bib.bib137),
    [113](#bib.bib113), [112](#bib.bib112), [115](#bib.bib115), [116](#bib.bib116),
    [114](#bib.bib114), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [139](#bib.bib139), [141](#bib.bib141)].
  prefs: []
  type: TYPE_NORMAL
- en: Indirectly, the memorization phenomenon can be employed to filter noisy examples
    and atypical examples, which benefits example enhancement [[102](#bib.bib102),
    [65](#bib.bib65), [103](#bib.bib103)] and noisy data learning [[97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: However, the memorization effect also raises privacy concerns. We may utilize
    the memorization effect to audit privacy [[29](#bib.bib29), [9](#bib.bib9)] or
    mitigate memorization to ensure compliance [[15](#bib.bib15)]. Additionally, the
    forgetting effect may provide some privacy guarantee and correlate with unlearning
    technology.
  prefs: []
  type: TYPE_NORMAL
- en: IX Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This survey based on the memorization effect provides a detailed exploration
    of a pivotal concept in DNNs. It begins by discussing the memorization definitions
    in the generalization domain and security and privacy domain. The survey then
    provides relevant measurements of memorization at different levels. Next, we discuss
    how memorization influences DNN training including data distribution, training
    stage, model structure, and other factors. After that, we review related studies
    on underlying privacy and security risks that correlate with the memorization
    effect. Following this, we also review the studies about the forgetting effect
    because forgetting is the opposite of memorization. Subsequently, this survey
    discusses related applications to the memorization effect or are highly associated
    with memorization functions. Lastly, we discuss possible memorization and forgetting
    mechanisms, attempt to understand memorization and forgetting impacts, and suggest
    further research. In this survey, to the furthest extent, we collect and present
    the main literature about the memorization and forgetting effect of DNNs, organizing
    relevant works in a comprehensive framework.
  prefs: []
  type: TYPE_NORMAL
- en: In this review, we highlight that memorization and forgetting effects are features
    of DNNs. These effects have deep impacts on the performance, fairness, explainability,
    accountability, and privacy of DNNs. Therefore, we should develop the ability
    to control, manage, and utilize the effects, leading to highly usable and trustworthy
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “Gpt-4 technical report,” *arXiv
    preprint arXiv:2303.08774*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. S. Dhanjal and W. Singh, “A comprehensive survey on automatic speech
    recognition using neural networks,” *Multimedia Tools and Applications*, vol. 83,
    no. 8, pp. 23 367–23 412, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing of deep-neural-network-driven
    autonomous cars,” in *Proceedings of the 40th international conference on software
    engineering*, 2018, pp. 303–314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin,
    W. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao, and H. Li, “Planning-oriented
    autonomous driving,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2023, pp. 17 853–17 862.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin,
    W. Wang *et al.*, “Planning-oriented autonomous driving,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023, pp. 17 853–17 862.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. Li, B. Lin, Z. Chen, H. Lin, X. Liang, and X. Chang, “Dynamic graph
    enhanced contrastive learning for chest x-ray report generation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 3334–3343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] T. Tu, A. Palepu, M. Schaekermann, K. Saab, J. Freyberg, R. Tanno, A. Wang,
    B. Li, M. Amin, N. Tomasev *et al.*, “Towards conversational diagnostic ai,” *arXiv
    preprint arXiv:2401.05654*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding
    deep learning requires rethinking generalization,” in *International Conference
    on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] N. Carlini, F. Tramèr, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee,
    A. Roberts, T. Brown, D. Song, Ú. Erlingsson, A. Oprea, and C. Raffel, “Extracting
    Training Data from Large Language Models,” in *30th USENIX Security Symposium
    (USENIX Security 21)*, 2021, pp. 2633–2650.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramèr, B. Balle,
    D. Ippolito, and E. Wallace, “Extracting Training Data from Diffusion Models,”
    in *32nd USENIX Security Symposium (USENIX Security 23)*, 2023, pp. 5253–5270.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] V. Feldman and C. Zhang, “What Neural Networks Memorize and Why: Discovering
    the Long Tail via Influence Estimation,” in *Advances in Neural Information Processing
    Systems*, vol. 33.   Curran Associates, Inc., 2020, pp. 2881–2891.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] X. Zheng and J. Jiang. An Empirical Study of Memorization in NLP. [Online].
    Available: http://arxiv.org/abs/2203.12171'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] V. Feldman, “Does learning require memorization? a short tale about a
    long tail,” in *Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory
    of Computing*, ser. STOC 2020.   New York, NY, USA: Association for Computing
    Machinery, Jun. 2020, pp. 954–959.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song, “The Secret Sharer:
    Evaluating and Testing Unintended Memorization in Neural Networks,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] P. Maini, M. C. Mozer, H. Sedghi, Z. C. Lipton, J. Z. Kolter, and C. Zhang,
    “Can Neural Network Memorization Be Localized?” Jul. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] K. Tirumala, A. Markosyan, L. Zettlemoyer, and A. Aghajanyan, “Memorization
    Without Overfitting: Analyzing the Training Dynamics of Large Language Models,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 38 274–38 290,
    Dec. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] C. Zhang, D. Ippolito, K. Lee, M. Jagielski, F. Tramèr, and N. Carlini,
    “Counterfactual Memorization in Neural Language Models,” Dec. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Anagnostidis, G. Bachmann, L. Noci, and T. Hofmann, “The Curious Case
    of Benign Memorization,” in *The Eleventh International Conference on Learning
    Representations*, Sep. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. A. Nguyen, R. Levie, J. Lienen, G. Kutyniok, and E. Hüllermeier, “Memorization-Dilation:
    Modeling Neural Collapse Under Label Noise,” Apr. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Z. Jiang, C. Zhang, K. Talwar, and M. C. Mozer, “Characterizing Structural
    Regularities of Labeled Data in Overparameterized Models,” in *Proceedings of
    the 38th International Conference on Machine Learning*.   PMLR, Jul. 2021, pp.
    5034–5044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] N. Carlini, M. Jagielski, C. Zhang, N. Papernot, A. Terzis, and F. Tramer,
    “The Privacy Onion Effect: Memorization is Relative,” *Advances in Neural Information
    Processing Systems*, vol. 35, pp. 13 263–13 276, Dec. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Li, Q. Li, Z. Hu, and X. Hu, “On the Privacy Effect of Data Enhancement
    via the Lens of Memorization,” Feb. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Rezaei and X. Liu, “On the Difficulty of Membership Inference Attacks,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2021, pp. 7892–7900.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer, “Membership
    Inference Attacks From First Principles,” Apr. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] D. Arpit, S. Jastrzębski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
    T. Maharaj, A. Fischer, A. Courville, Y. Bengio, and S. Lacoste-Julien, “A Closer
    Look at Memorization in Deep Networks,” in *Proceedings of the 34th International
    Conference on Machine Learning*.   PMLR, Jul. 2017, pp. 233–242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. Dong, K. Xu, X. Yang, T. Pang, Z. Deng, H. Su, and J. Zhu, “Exploring
    Memorization in Adversarial Training,” Mar. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] G. Hacohen, L. Choshen, and D. Weinshall, “Let’s Agree to Agree: Neural
    Networks Share Classification Order on Real Datasets,” in *Proceedings of the
    37th International Conference on Machine Learning*.   PMLR, Nov. 2020, pp. 3950–3960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch,
    and N. Carlini, “Deduplicating Training Data Makes Language Models Better,” in
    *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, S. Muresan, P. Nakov, and A. Villavicencio, Eds.   Dublin,
    Ireland: Association for Computational Linguistics, May 2022, pp. 8424–8445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang,
    “Quantifying Memorization Across Neural Language Models,” Mar. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. Maennel, I. M. Alabdulmohsin, I. O. Tolstikhin, R. Baldock, O. Bousquet,
    S. Gelly, and D. Keysers, “What Do Neural Networks Learn When Trained With Random
    Labels?” in *Advances in Neural Information Processing Systems*, vol. 33.   Curran
    Associates, Inc., 2020, pp. 19 693–19 704.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] C. Stephenson, S. Padhy, A. Ganesh, Y. Hui, H. Tang, and S. Chung, “On
    the geometry of generalization and memorization in deep neural networks,” May
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer Feed-Forward
    Layers Are Key-Value Memories. [Online]. Available: http://arxiv.org/abs/2012.14913'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] D. Krueger*, N. Ballas*, S. Jastrzebski*, D. Arpit*, M. S. Kanwal, T. Maharaj,
    E. Bengio, A. Fischer, and A. Courville, “Deep Nets Don’t Learn via Memorization,”
    Feb. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] V. N. Vapnik, “Adaptive and learning systems for signal processing communications,
    and control,” *Statistical learning theory*, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] P. L. Bartlett and S. Mendelson, “Rademacher and gaussian complexities:
    Risk bounds and structural results,” in *International Conference on Computational
    Learning Theory*.   Springer, 2001, pp. 224–240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Mukherjee, P. Niyogi, T. Poggio, and R. Rifkin, “Statistical learning:
    Stability is sufficient for generalization and necessary and sufficient for consistency
    of empirical risk minimization,” 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] O. Bousquet and A. Elisseeff, “Stability and generalization,” *The Journal
    of Machine Learning Research*, vol. 2, pp. 499–526, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi, “General conditions
    for predictivity in learning theory,” *Nature*, vol. 428, no. 6981, pp. 419–422,
    2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] W. J. Reed, “The Pareto, Zipf and other power laws,” *Economics Letters*,
    vol. 74, no. 1, pp. 15–19, Dec. 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Gu and V. Tresp, “Neural Network Memorization Dissection,” Nov. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Frankle, D. J. Schwab, and A. S. Morcos, “The Early Phase of Neural
    Network Training,” in *International Conference on Learning Representations*,
    Sep. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Achille, M. Rovere, and S. Soatto, “Critical learning periods in deep
    networks,” in *International Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] V. Papyan, X. Han, and D. L. Donoho, “Prevalence of neural collapse during
    the terminal phase of deep learning training,” *Proceedings of the National Academy
    of Sciences*, vol. 117, no. 40, pp. 24 652–24 663, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C. Agarwal, D. D’souza, and S. Hooker, “Estimating Example Difficulty
    Using Variance of Gradients,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 10 368–10 378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
    features in deep neural networks?” in *Advances in Neural Information Processing
    Systems*, vol. 27.   Curran Associates, Inc., 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein, “SVCCA: Singular
    Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability,”
    in *Advances in Neural Information Processing Systems*, vol. 30.   Curran Associates,
    Inc., 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Morcos, M. Raghu, and S. Bengio, “Insights on representational similarity
    in neural networks with canonical correlation,” in *Advances in Neural Information
    Processing Systems*, vol. 31.   Curran Associates, Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. Ansuini, A. Laio, J. H. Macke, and D. Zoccolan, “Intrinsic dimension
    of data representations in deep neural networks,” in *Advances in Neural Information
    Processing Systems*, vol. 32.   Curran Associates, Inc., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Chatterjee, “Learning and Memorization,” in *Proceedings of the 35th
    International Conference on Machine Learning*.   PMLR, Jul. 2018, pp. 755–763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Zhang, S. Bengio, M. Hardt, M. C. Mozer, and Y. Singer, “Identity Crisis:
    Memorization and Generalization under Extreme Overparameterization,” Jan. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is All you Need,” in *Advances in Neural
    Information Processing Systems*, vol. 30.   Curran Associates, Inc., 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Sukhbaatar, E. Grave, G. Lample, H. Jegou, and A. Joulin. Augmenting
    Self-attention with Persistent Memory. [Online]. Available: http://arxiv.org/abs/1907.01470'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei. Knowledge Neurons
    in Pretrained Transformers. [Online]. Available: http://arxiv.org/abs/2104.08696'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy Risk in Machine
    Learning: Analyzing the Connection to Overfitting,” in *2018 IEEE 31st Computer
    Security Foundations Symposium (CSF)*, Jul. 2018, pp. 268–282.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A. Gunter,
    and K. Chen, “Understanding Membership Inferences on Well-Generalized Learning
    Models,” Feb. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler, “Benign overfitting
    in linear regression,” *Proceedings of the National Academy of Sciences*, vol.
    117, no. 48, pp. 30 063–30 070, Dec. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Z. Li, Z.-H. Zhou, and A. Gretton, “Towards an Understanding of Benign
    Overfitting in Neural Networks,” Jun. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y. Cao, Z. Chen, M. Belkin, and Q. Gu, “Benign Overfitting in Two-layer
    Convolutional Neural Networks,” *Advances in Neural Information Processing Systems*,
    vol. 35, pp. 25 237–25 250, Dec. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] A. Sablayrolles, M. Douze, C. Schmid, and H. Jégou, “D\’ej\‘a Vu: An empirical
    evaluation of the memorization properties of ConvNets,” Sep. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] D. Patel and P. S. Sastry, “Memorization in Deep Neural Networks: Does
    the Loss Function Matter?” in *Advances in Knowledge Discovery and Data Mining*,
    ser. Lecture Notes in Computer Science, K. Karlapalem, H. Cheng, N. Ramakrishnan,
    R. K. Agrawal, P. K. Reddy, J. Srivastava, and T. Chakraborty, Eds.   Cham: Springer
    International Publishing, 2021, pp. 131–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Y. Li, C. Wei, and T. Ma, “Towards Explaining the Regularization Effect
    of Initial Large Learning Rate in Training Neural Networks,” in *Advances in Neural
    Information Processing Systems*, vol. 32.   Curran Associates, Inc., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] E. Kharitonov, M. Baroni, and D. Hupkes, “How BPE Affects Memorization
    in Transformers,” Dec. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] K. Leino and M. Fredrikson, “Stolen Memories: Leveraging Model Memorization
    for Calibrated {}White-Box{} Membership Inference,” in *29th USENIX Security Symposium
    (USENIX Security 20)*, 2020, pp. 1605–1622.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. Xu, X. Liu, W. Wang, Z. Liu, A. K. Jain, and J. Tang, “How does the
    Memorization of Neural Networks Impact Adversarial Robust Models?” in *Proceedings
    of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, ser.
    KDD ’23.   New York, NY, USA: Association for Computing Machinery, Aug. 2023,
    pp. 2801–2812.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership Inference
    Attacks Against Machine Learning Models,” in *2017 IEEE Symposium on Security
    and Privacy (SP)*, pp. 3–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini,
    E. Wallace, S. Song, A. G. Thakurta, N. Papernot, and C. Zhang, “Measuring Forgetting
    of Memorized Training Examples,” in *The Eleventh International Conference on
    Learning Representations*, Sep. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards Deep
    Learning Models Resistant to Adversarial Attacks.” [Online]. Available: https://arxiv.org/abs/1706.06083v4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing
    Adversarial Examples.” [Online]. Available: https://arxiv.org/abs/1412.6572v3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry,
    “Adversarial Examples Are Not Bugs, They Are Features.” [Online]. Available: http://arxiv.org/abs/1905.02175'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry, “Adversarially
    Robust Generalization Requires More Data,” in *Advances in Neural Information
    Processing Systems*, vol. 31.   Curran Associates, Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] L. Rice, E. Wong, and Z. Kolter, “Overfitting in adversarially robust
    deep learning,” in *Proceedings of the 37th International Conference on Machine
    Learning*.   PMLR, Nov. 2020, pp. 8093–8104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. E. Ghaoui, and M. Jordan, “Theoretically
    Principled Trade-off between Robustness and Accuracy,” in *Proceedings of the
    36th International Conference on Machine Learning*.   PMLR, pp. 7472–7482. [Online].
    Available: https://proceedings.mlr.press/v97/zhang19p.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar,
    and L. Zhang, “Deep Learning with Differential Privacy,” in *Proceedings of the
    2016 ACM SIGSAC Conference on Computer and Communications Security*, pp. 308–318\.
    [Online]. Available: http://arxiv.org/abs/1607.00133'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] O. Thakkar, S. Ramaswamy, R. Mathews, and F. Beaufays, “Understanding
    Unintended Memorization in Federated Learning,” Jun. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. Caton and C. Haas, “Fairness in machine learning: A survey,” *ACM Computing
    Surveys*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness through
    awareness,” in *Proceedings of the 3rd innovations in theoretical computer science
    conference*, 2012, pp. 214–226.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian,
    “Certifying and removing disparate impact,” in *proceedings of the 21th ACM SIGKDD
    international conference on knowledge discovery and data mining*, 2015, pp. 259–268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M. Hardt, E. Price, and N. Srebro, “Equality of opportunity in supervised
    learning,” *Advances in neural information processing systems*, vol. 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, “Learning fair
    representations,” in *International conference on machine learning*.   PMLR, 2013,
    pp. 325–333.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    in *Proceedings of the IEEE international conference on computer vision*, 2017,
    pp. 618–626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional
    networks: Visualising image classification models and saliency maps,” *arXiv preprint
    arXiv:1312.6034*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] R. C. Fong and A. Vedaldi, “Interpretable explanations of black boxes
    by meaningful perturbation,” in *Proceedings of the IEEE international conference
    on computer vision*, 2017, pp. 3429–3437.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] R. Dwivedi, D. Dave, H. Naik, S. Singhal, R. Omer, P. Patel, B. Qian,
    Z. Wen, T. Shah, G. Morgan *et al.*, “Explainable ai (xai): Core ideas, techniques,
    and solutions,” *ACM Computing Surveys*, vol. 55, no. 9, pp. 1–33, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi,
    “A survey of methods for explaining black box models,” *ACM computing surveys
    (CSUR)*, vol. 51, no. 5, pp. 1–42, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] F. Tramer, V. Atlidakis, R. Geambasu, D. Hsu, J.-P. Hubaux, M. Humbert,
    A. Juels, and H. Lin, “Fairtest: Discovering unwarranted associations in data-driven
    applications,” in *2017 IEEE European Symposium on Security and Privacy (EuroS&P)*.   IEEE,
    2017, pp. 401–416.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] X. Gao, J. Zhai, S. Ma, C. Shen, Y. Chen, and Q. Wang, “FairNeuron: Improving
    deep neural network fairness with adversary games on selective neurons,” in *Proceedings
    of the 44th International Conference on Software Engineering*, ser. ICSE ’22.   Association
    for Computing Machinery, pp. 921–933\. [Online]. Available: https://dl.acm.org/doi/10.1145/3510003.3510087'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] G. Barone, A. Cunchala, and R. Nunez. Increasing Fairness in Classification
    of Out of Distribution Data for Facial Recognition. [Online]. Available: http://arxiv.org/abs/2404.03876'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] H. Ritter, A. Botev, and D. Barber, “Online Structured Laplace Approximations
    For Overcoming Catastrophic Forgetting,” May 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A.
    Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,
    D. Kumaran, and R. Hadsell, “Overcoming catastrophic forgetting in neural networks,”
    *Proceedings of the National Academy of Sciences*, vol. 114, no. 13, pp. 3521–3526,
    Mar. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] C. Shao and Y. Feng, “Overcoming Catastrophic Forgetting beyond Continual
    Learning: Balanced Training for Neural Machine Translation,” in *Proceedings of
    the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, S. Muresan, P. Nakov, and A. Villavicencio, Eds.   Dublin, Ireland:
    Association for Computational Linguistics, May 2022, pp. 2023–2036.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. L. Hayes, K. Kafle, R. Shrestha, M. Acharya, and C. Kanan, “REMIND
    Your Neural Network to Prevent Catastrophic Forgetting,” in *Computer Vision –
    ECCV 2020*, ser. Lecture Notes in Computer Science, A. Vedaldi, H. Bischof, T. Brox,
    and J.-M. Frahm, Eds.   Cham: Springer International Publishing, 2020, pp. 466–483.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] H. Liu, Y. Yang, and X. Wang, “Overcoming Catastrophic Forgetting in Graph
    Neural Networks,” *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 35, no. 10, pp. 8653–8661, May 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Li and D. Hoiem, “Learning without Forgetting,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, vol. 40, no. 12, pp. 2935–2947,
    Dec. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] M. Toneva, A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio, and
    G. J. Gordon, “An Empirical Study of Example Forgetting during Deep Neural Network
    Learning,” Nov. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] P. Maini, S. Garg, Z. Lipton, and J. Z. Kolter, “Characterizing Datapoints
    via Second-Split Forgetting,” *Advances in Neural Information Processing Systems*,
    vol. 35, pp. 30 044–30 057, Dec. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, and M. Sugiyama,
    “Co-teaching: Robust training of deep neural networks with extremely noisy labels,”
    in *Proceedings of the 32nd International Conference on Neural Information Processing
    Systems*, ser. NIPS’18.   Red Hook, NY, USA: Curran Associates Inc., Dec. 2018,
    pp. 8536–8546.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Q. Yao, H. Yang, B. Han, G. Niu, and J. T.-Y. Kwok, “Searching to Exploit
    Memorization Effect in Learning with Noisy Labels,” in *Proceedings of the 37th
    International Conference on Machine Learning*.   PMLR, Nov. 2020, pp. 10 789–10 798.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Liu, J. Niles-Weed, N. Razavian, and C. Fernandez-Granda, “Early-Learning
    Regularization Prevents Memorization of Noisy Labels,” in *Advances in Neural
    Information Processing Systems*, vol. 33.   Curran Associates, Inc., 2020, pp.
    20 331–20 342.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Xia, T. Liu, B. Han, C. Gong, N. Wang, Z. Ge, and Y. Chang, “Robust
    early-learning: Hindering the memorization of noisy labels,” in *International
    Conference on Learning Representations*, Oct. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] V. Pondenkandath, M. Alberti, S. Puran, R. Ingold, and M. Liwicki, “Leveraging
    Random Label Memorization for Unsupervised Pre-Training,” Nov. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Z. Zhou, J. Yao, Y.-F. Wang, B. Han, and Y. Zhang, “Contrastive Learning
    with Boosted Memorization,” in *Proceedings of the 39th International Conference
    on Machine Learning*.   PMLR, Jun. 2022, pp. 27 367–27 377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. Zhang, Y. Hong, and Q. Zhao, “Memorization Weights for Instance Reweighting
    in Adversarial Training,” *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 37, no. 9, pp. 11 228–11 236, Jun. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] C. Zhu, A. S. Rawat, M. Zaheer, S. Bhojanapalli, D. Li, F. Yu, and S. Kumar,
    “Modifying Memories in Transformer Models,” Dec. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis, “Generalization
    through Memorization: Nearest Neighbor Language Models,” in *International Conference
    on Learning Representations*, Sep. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] D. Yogatama, C. de Masson d’Autume, and L. Kong, “Adaptive Semiparametric
    Language Models,” *Transactions of the Association for Computational Linguistics*,
    vol. 9, pp. 362–373, Apr. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval Augmented
    Language Model Pre-Training,” in *Proceedings of the 37th International Conference
    on Machine Learning*.   PMLR, Nov. 2020, pp. 3929–3938.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] M. Lewis, M. Ghazvininejad, G. Ghosh, A. Aghajanyan, S. Wang, and L. Zettlemoyer,
    “Pre-training via Paraphrasing,” in *Advances in Neural Information Processing
    Systems*, vol. 33.   Curran Associates, Inc., 2020, pp. 18 470–18 481.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler,
    M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-Augmented
    Generation for Knowledge-Intensive NLP Tasks,” in *Advances in Neural Information
    Processing Systems*, vol. 33.   Curran Associates, Inc., 2020, pp. 9459–9474.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Y. Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, “Memorizing Transformers,”
    Mar. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Tay, V. Tran, M. Dehghani, J. Ni, D. Bahri, H. Mehta, Z. Qin, K. Hui,
    Z. Zhao, J. Gupta, T. Schuster, W. W. Cohen, and D. Metzler, “Transformer Memory
    as a Differentiable Search Index,” *Advances in Neural Information Processing
    Systems*, vol. 35, pp. 21 831–21 843, Dec. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, “Knowledge neurons
    in pretrained transformers,” *arXiv preprint arXiv:2104.08696*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] N. De Cao, W. Aziz, and I. Titov, “Editing factual knowledge in language
    models,” *arXiv preprint arXiv:2104.08164*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning, “Fast model
    editing at scale,” *arXiv preprint arXiv:2110.11309*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] K. Meng, D. Bau, A. Andonian, and Y. Belinkov, “Locating and editing
    factual associations in gpt,” *Advances in Neural Information Processing Systems*,
    vol. 35, pp. 17 359–17 372, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] K. Meng, A. S. Sharma, A. Andonian, Y. Belinkov, and D. Bau, “Mass-editing
    memory in a transformer,” *arXiv preprint arXiv:2210.07229*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. Gupta, D. Sajnani, and G. Anumanchipalli. A unified framework for
    model editing. [Online]. Available: http://arxiv.org/abs/2403.14236'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] X. Yu, B. Han, J. Yao, G. Niu, I. Tsang, and M. Sugiyama, “How does disagreement
    help generalization against label corruption?” in *Proceedings of the 36th International
    Conference on Machine Learning*, ser. Proceedings of Machine Learning Research,
    K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97.   PMLR, 09–15 Jun 2019, pp.
    7164–7173\. [Online]. Available: https://proceedings.mlr.press/v97/yu19b.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, and M. Sugiyama,
    “Co-teaching: Robust training of deep neural networks with extremely noisy labels,”
    in *Proceedings of the 32nd International Conference on Neural Information Processing
    Systems*, ser. NIPS’18.   Red Hook, NY, USA: Curran Associates Inc., Dec. 2018,
    pp. 8536–8546.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei, “MentorNet: Learning
    Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels,” in
    *Proceedings of the 35th International Conference on Machine Learning*.   PMLR,
    Jul. 2018, pp. 2304–2313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] U. Khandelwal, A. Fan, D. Jurafsky, L. Zettlemoyer, and M. Lewis, “Nearest
    Neighbor Machine Translation,” in *International Conference on Learning Representations*,
    Oct. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] T. Févry, L. Baldini Soares, N. FitzGerald, E. Choi, and T. Kwiatkowski,
    “Entities as Experts: Sparse Memory Access with Entity Supervision,” in *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    B. Webber, T. Cohn, Y. He, and Y. Liu, Eds.   Online: Association for Computational
    Linguistics, Nov. 2020, pp. 4937–4951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] P. Verga, H. Sun, L. B. Soares, and W. W. Cohen, “Facts as Experts: Adaptable
    and Interpretable Neural Memory over Symbolic Knowledge,” Jul. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller,
    and S. Riedel, “Language models as knowledge bases?” *arXiv preprint arXiv:1909.01066*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Geva, R. Schuster, J. Berant, and O. Levy, “Transformer feed-forward
    layers are key-value memories,” *arXiv preprint arXiv:2012.14913*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Geva, A. Caciularu, K. R. Wang, and Y. Goldberg, “Transformer feed-forward
    layers build predictions by promoting concepts in the vocabulary space,” *arXiv
    preprint arXiv:2203.14680*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep
    networks,” in *International conference on machine learning*.   PMLR, 2017, pp.
    3319–3328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification
    with Deep Convolutional Neural Networks,” in *Advances in Neural Information Processing
    Systems*, vol. 25.   Curran Associates, Inc., 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Y. Cui, Z. Gu, D. Mahajan, L. van der Maaten, S. Belongie, and S.-N.
    Lim, “Measuring Dataset Granularity,” Dec. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] P. Chu, X. Bian, S. Liu, and H. Ling, “Feature Space Augmentation for
    Long-Tailed Data,” in *Computer Vision – ECCV 2020*, ser. Lecture Notes in Computer
    Science, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds.   Cham: Springer
    International Publishing, 2020, pp. 694–710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] S. Zhang, Z. Li, S. Yan, X. He, and J. Sun, “Distribution Alignment:
    A Unified Framework for Long-Tail Visual Recognition,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 2361–2370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] A. Althnian, D. AlSaeed, H. Al-Baity, A. Samha, A. B. Dris, N. Alzakari,
    A. Abou Elwafa, and H. Kurdi, “Impact of Dataset Size on Classification Performance:
    An Empirical Evaluation in the Medical Domain,” *Applied Sciences*, vol. 11, no. 2,
    p. 796, Jan. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” *Communications
    of the ACM*, vol. 63, no. 11, pp. 139–144, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] P. Dhariwal and A. Nichol, “Diffusion Models Beat GANs on Image Synthesis,”
    in *Advances in Neural Information Processing Systems*, vol. 34.   Curran Associates,
    Inc., 2021, pp. 8780–8794.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving
    Language Understanding by Generative Pre-Training.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted Backdoor Attacks
    on Deep Learning Systems Using Data Poisoning,” Dec. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] A. Sinitsin, V. Plokhotnyuk, D. Pyrkin, S. Popov, and A. Babenko, “Editable
    neural networks,” *arXiv preprint arXiv:2004.00345*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] E. Mitchell, C. Lin, A. Bosselut, C. D. Manning, and C. Finn, “Memory-based
    model editing at scale,” in *International Conference on Machine Learning*.   PMLR,
    2022, pp. 15 817–15 831.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. Gupta, S. Baskaran, and G. Anumanchipalli. Rebuilding rome: Resolving
    model collapse during sequential model editing. [Online]. Available: http://arxiv.org/abs/2403.07175'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] P. Hase, M. Bansal, B. Kim, and A. Ghandeharioun, “Does localization
    inform editing? surprising differences in causality-based localization vs. knowledge
    editing in language models,” *Advances in Neural Information Processing Systems*,
    vol. 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang.
    Editing large language models: Problems, methods, and opportunities. [Online].
    Available: http://arxiv.org/abs/2305.13172'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
