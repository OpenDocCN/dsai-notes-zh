- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:44:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:44:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2208.11296] Semi-Supervised and Unsupervised Deep Visual Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2208.11296] 半监督和无监督深度视觉学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.11296](https://ar5iv.labs.arxiv.org/html/2208.11296)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2208.11296](https://ar5iv.labs.arxiv.org/html/2208.11296)
- en: Semi-Supervised and Unsupervised
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督和无监督
- en: 'Deep Visual Learning: A Survey'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度视觉学习：综述
- en: Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, and Zeynep Akata This work was
    done when Y.Chen was with the University of Tübingen.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, 和 Zeynep Akata 本文在Y.Chen任职于图宾根大学期间完成。
- en: 'E-mail: yanbeic@gmail.com M. Mancini is with the University of Tübingen.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：yanbeic@gmail.com M. Mancini 现就职于图宾根大学。
- en: 'E-mail: massimiliano.mancini@uni-tuebingen.de X. Zhu is with the University
    of Surrey. E-mail: xiatian.zhu@surrey.ac.uk Z. Akata is with the University of
    Tübingen, MPI for Informatics and MPI for Intelligent Systems. E-mail: zeynep.akata@uni-tuebingen.de'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：massimiliano.mancini@uni-tuebingen.de X. Zhu 现就职于萨里大学。电子邮件：xiatian.zhu@surrey.ac.uk
    Z. Akata 现就职于图宾根大学、MPI计算机科学所和MPI智能系统所。电子邮件：zeynep.akata@uni-tuebingen.de
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: State-of-the-art deep learning models are often trained with a large amount
    of costly labeled training data. However, requiring exhaustive manual annotations
    may degrade the model’s generalizability in the limited-label regime. Semi-supervised
    learning and unsupervised learning offer promising paradigms to learn from an
    abundance of unlabeled visual data. Recent progress in these paradigms has indicated
    the strong benefits of leveraging unlabeled data to improve model generalization
    and provide better model initialization. In this survey, we review the recent
    advanced deep learning algorithms on semi-supervised learning (SSL) and unsupervised
    learning (UL) for visual recognition from a unified perspective. To offer a holistic
    understanding of the state-of-the-art in these areas, we propose a unified taxonomy.
    We categorize existing representative SSL and UL with comprehensive and insightful
    analysis to highlight their design rationales in different learning scenarios
    and applications in different computer vision tasks. Lastly, we discuss the emerging
    trends and open challenges in SSL and UL to shed light on future critical research
    directions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的深度学习模型通常需要大量昂贵的标注训练数据进行训练。然而，要求全面的手动标注可能会降低模型在有限标签情况下的泛化能力。半监督学习和无监督学习提供了从大量未标注视觉数据中学习的有希望的范式。最近在这些范式中的进展表明，利用未标注数据可以显著提高模型的泛化能力，并提供更好的模型初始化。在这篇综述中，我们从统一的角度回顾了半监督学习（SSL）和无监督学习（UL）在视觉识别中的最新先进深度学习算法。为了提供对这些领域前沿的全面理解，我们提出了一个统一的分类法。我们对现有的代表性SSL和UL进行全面而深入的分析，以突出它们在不同学习场景和计算机视觉任务中的设计原理。最后，我们讨论了SSL和UL中的新兴趋势和开放挑战，以指明未来关键研究方向。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Semi-Supervised, Unsupervised, Self-Supervised, Visual Representation Learning,
    Survey
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督、无监督、自监督、视觉表示学习、综述
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Over the last decade, deep learning algorithms and architectures [[1](#bib.bib1),
    [2](#bib.bib2)] have been pushing the state of the art in a wide variety of computer
    vision tasks, ranging from object recognition [[3](#bib.bib3)], retrieval [[4](#bib.bib4)],
    detection [[5](#bib.bib5)], to segmentation [[6](#bib.bib6)]. To achieve human-level
    performance, deep learning models are typically built by supervised training upon
    a tremendous amount of labeled training data.However, collecting large-scale labeled
    training sets manually is not only expensive and time-consuming, but may also
    be legally prohibited due to privacy, security, and ethics restrictions. Moreover,
    supervised deep learning models tend to memorize the labeled data and incorporate
    the annotator’s bias, which weakens their generalization to new scenarios with
    unseen data distributions in practice.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，深度学习算法和架构 [[1](#bib.bib1), [2](#bib.bib2)] 一直在推动各种计算机视觉任务的最前沿，包括对象识别 [[3](#bib.bib3)],
    检索 [[4](#bib.bib4)], 检测 [[5](#bib.bib5)], 和分割 [[6](#bib.bib6)]。为了达到人类水平的性能，深度学习模型通常需要在大量标注训练数据上进行监督训练。然而，手动收集大规模标注训练集不仅成本高且耗时，而且由于隐私、安全和伦理限制，可能还会受到法律禁止。此外，监督深度学习模型往往会记忆标注数据并引入标注者的偏差，这会削弱它们在面对新的数据分布时的泛化能力。
- en: Cheaper imaging technologies and more convenient access to web data, makes obtaining
    large unlabeled visual data no longer challenging. Learning from unlabeled data
    thus becomes a natural and promising way to scale models towards practical scenarios
    where it is infeasible to collect a large labeled training set that covers all
    types of visual variations in illumination, viewpoint, resolution, occlusion,
    and background clutter induced by different scenes, camera positions, times of
    the day, and weather conditions.Semi-supervised learning [[7](#bib.bib7), [8](#bib.bib8)]
    and unsupervised learning [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)] stand out as two most representative paradigms for leveraging
    unlabeled data. Built upon different assumptions, these paradigms are often developed
    independently, whilst sharing the same aim to learn more powerful representations
    and models using unlabeled data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 更便宜的成像技术和更便捷的网络数据访问，使得获取大量未标记的视觉数据不再具有挑战性。因此，从未标记的数据中学习成为了在实际场景中扩展模型的自然和有前景的方法，在这些场景中，收集覆盖所有类型视觉变化（如光照、视角、分辨率、遮挡和背景杂乱）的标记训练集是不切实际的。这些场景包括不同的场景、相机位置、时间和天气条件。半监督学习
    [[7](#bib.bib7), [8](#bib.bib8)] 和无监督学习 [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)] 被认为是利用未标记数据的两个最具代表性的范式。这些范式基于不同的假设，通常独立发展，但共享相同的目标，即使用未标记的数据学习更强大的表征和模型。
- en: '![Refer to caption](img/e786a84692e6fffbe5cc9e79a3e6ec5b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e786a84692e6fffbe5cc9e79a3e6ec5b.png)'
- en: 'Figure 1: An overview of semi-supervised and unsupervised learning paradigms
    – both aim to learn from unlabeled data.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：半监督和无监督学习范式的概述——这两者都旨在从未标记的数据中学习。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey") summarizes the two paradigms covered in this
    survey, which both utilize unlabeled data for visual representation learning.
    According to whether label annotations are given for a small portion or none of
    the training data, we categorize the paradigms as semi-supervised learning, and
    unsupervised learning as defined explicitly in the following.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 半监督与无监督深度视觉学习：综述") 总结了本综述中涉及的两种范式，这两种范式都利用未标记的数据进行视觉表征学习。根据是否对小部分或没有训练数据提供标签注释，我们将这些范式分类为半监督学习和无监督学习，具体定义如下。
- en: (a)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (a)
- en: Semi-Supervised Learning (SSL) aims to jointly learn from sparsely labeled data
    and a large amount of auxiliary unlabeled data often drawn from the same underlying
    data distribution as the labeled data. In standard closed-set SSL [[13](#bib.bib13),
    [8](#bib.bib8)], the labeled and unlabeled data belong to the same set of classes
    from the same domain. In open-set SSL [[14](#bib.bib14), [15](#bib.bib15)], they
    may not lie in the same label space, i.e., the unlabeled data may contain unknown
    and/or mislabeled classes.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半监督学习（SSL）旨在从稀疏标记的数据和大量辅助未标记数据中共同学习，这些未标记数据通常来自与标记数据相同的基础数据分布。在标准的闭集SSL [[13](#bib.bib13),
    [8](#bib.bib8)]中，标记数据和未标记数据属于同一领域中的相同类别。在开放集SSL [[14](#bib.bib14), [15](#bib.bib15)]中，它们可能不在相同的标签空间中，即未标记数据可能包含未知和/或标记错误的类别。
- en: (b)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (b)
- en: Unsupervised Learning (UL) aims to learn from only unlabeled data without utilizing
    any task-relevant label supervision. Once trained, the model can be fine-tuned
    using labeled data to achieve better model generalization in a downstream task [[16](#bib.bib16)].
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无监督学习（UL）旨在仅从未标记的数据中学习，而不利用任何与任务相关的标签监督。一旦训练完成，模型可以使用标记数据进行微调，以在下游任务中实现更好的模型泛化[[16](#bib.bib16)]。
- en: 'Following the above definitions, let the sets of labeled data and unlabeled
    data be denoted as $\mathcal{D}_{l}$ and $\mathcal{D}_{u}$. The overall unified
    learning objective for SSL and UL is:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述定义，设标记数据集和未标记数据集分别为 $\mathcal{D}_{l}$ 和 $\mathcal{D}_{u}$。SSL 和 UL 的总体统一学习目标为：
- en: '|  | $\underset{{\theta}}{\text{min}}\ \lambda_{l}\sum_{(\textbf{x},y)\in\mathcal{D}_{L}}\mathcal{L}_{\text{sup}}(\textbf{x},y,\theta)+\lambda_{u}\sum_{\textbf{x}\in\mathcal{D}_{U}}\mathcal{L}_{\text{unsup}}(\textbf{x},\theta),$
    |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{{\theta}}{\text{最小化}}\ \lambda_{l}\sum_{(\textbf{x},y)\in\mathcal{D}_{L}}\mathcal{L}_{\text{sup}}(\textbf{x},y,\theta)+\lambda_{u}\sum_{\textbf{x}\in\mathcal{D}_{U}}\mathcal{L}_{\text{unsup}}(\textbf{x},\theta),$
    |  | (1) |'
- en: where $\theta$ refers to the model parameters of a deep neural network (DNN);
    x is an input image and $y$ is the corresponding label; $\mathcal{L}_{\text{sup}}$
    and $\mathcal{L}_{\text{unsup}}$ are the supervised and unsupervised loss terms;
    $\lambda_{l}$ and $\lambda_{u}$ are balancing hyperparameters. In SSL, both loss
    terms are jointly optimized. In UL, only the unsupervised loss term is used for
    unsupervised model pre-training (i.e., $\lambda_{l}=0$). Although SSL and UL share
    the same rationale of learning with an unsupervised objective, they differ in
    the learning setup, leading to different unique challenges. Specifically, SSL
    assumes the availability of limited labeled data, and its core challenge is to
    expand the labeled set with abundant unlabeled data. UL assumes no labeled data
    for the main learning task and its key challenge is to learn task-generic representations
    from unlabeled data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta$指的是深度神经网络（DNN）的模型参数；x是输入图像，$y$是相应的标签；$\mathcal{L}_{\text{sup}}$和$\mathcal{L}_{\text{unsup}}$分别是监督和无监督的损失项；$\lambda_{l}$和$\lambda_{u}$是平衡超参数。在SSL中，这两个损失项是联合优化的。在UL中，仅使用无监督损失项进行无监督模型预训练（即，$\lambda_{l}=0$）。虽然SSL和UL共享使用无监督目标进行学习的相同原理，但它们在学习设置上有所不同，导致了不同的独特挑战。具体来说，SSL假设有有限的标记数据，其核心挑战是用大量的未标记数据扩展标记集。UL假设主要学习任务没有标记数据，其关键挑战是从未标记数据中学习任务通用的表示。
- en: We focus on providing a timely and comprehensive review of the advances in leveraging
    unlabeled data to improve model generalization, covering the representative state-of-the-art
    methods in SSL and UL, their application domains, to the emerging trends in self-supervised
    learning. Importantly, we propose a unified taxonomy of the advanced deep learning
    methods to offer researchers a systematic overview that helps to understand the
    current state of the art and identify open challenges for future research.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于提供有关利用未标记数据提高模型泛化能力的进展的及时而全面的综述，涵盖了**自监督学习**和**无监督学习**中的代表性前沿方法及其应用领域，以及自监督学习中的新兴趋势。重要的是，我们提出了一个统一的高级深度学习方法分类法，为研究人员提供系统的概述，帮助理解当前的最前沿技术并识别未来研究的开放挑战。
- en: Comparison with previous surveys. Our survey is related to other surveys on
    semi-supervised learning [[13](#bib.bib13), [8](#bib.bib8), [17](#bib.bib17)],
    self-supervised learning [[18](#bib.bib18), [19](#bib.bib19)], or both topics [[20](#bib.bib20)].
    While these surveys mostly focus on a single particular learning setup [[13](#bib.bib13),
    [8](#bib.bib8), [17](#bib.bib17), [18](#bib.bib18)], non-deep learning methods [[13](#bib.bib13),
    [8](#bib.bib8)], or lacking a comprehensive taxonomy on methods and discussion
    on applications [[20](#bib.bib20)], our work covers a wider review of representative
    SSL and UL algorithms involving unlabeled visual data. Importantly, we categorize
    the state-of-the-art SSL and UL algorithms with novel taxonomies and draw connections
    among different methods. Beyond intrinsic challenges with each learning paradigm,
    we distill their underlying connections from the problem and algorithmic perspectives,
    discuss unique insights into different existing techniques, and their practical
    applicability.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的综述相比。我们的综述与其他关于半监督学习[[13](#bib.bib13), [8](#bib.bib8), [17](#bib.bib17)]、自监督学习[[18](#bib.bib18),
    [19](#bib.bib19)]或两者的综述[[20](#bib.bib20)]相关。虽然这些综述大多专注于单一的学习设置[[13](#bib.bib13),
    [8](#bib.bib8), [17](#bib.bib17), [18](#bib.bib18)]、非深度学习方法[[13](#bib.bib13),
    [8](#bib.bib8)]，或缺乏对方法的全面分类和应用讨论[[20](#bib.bib20)]，但我们的工作则涵盖了代表性的SSL和UL算法的更广泛的综述，涉及未标记的视觉数据。重要的是，我们用新的分类法对最前沿的SSL和UL算法进行分类，并在不同方法之间建立联系。除了每种学习范式的内在挑战之外，我们还从问题和算法的角度提炼它们之间的内在联系，讨论不同现有技术的独特见解及其实际应用。
- en: 'Survey organization and contributions. Our contributions are three fold. First,
    to our knowledge, this is the first deep learning survey of its kind to provide
    a comprehensive review of three prevalent machine learning paradigms in exploiting
    unlabeled data for visual recognition, including semi-supervised learning (SSL,
    §[2](#S2 "2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")), unsupervised learning (UL, §[3](#S3 "3 Unsupervised
    Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")),
    and a further discussion on SSL and UL (§[4](#S4 "4 Discussion on SSL and UL ‣
    Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")). Second, we
    provide a unified, insightful taxonomy and analysis of the existing methods in
    both the learning setup and model formulation to uncover their underlying algorithmic
    connections. Finally, we outlook the emerging trends and future research directions
    in §[5](#S5 "5 Emerging Trends and Open Challenges ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey") to shed light on those under-explored and potentially
    critical open avenues.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '调查组织和贡献。我们的贡献有三方面。首先，据我们所知，这是第一份深度学习调查，全面回顾了三种流行的机器学习范式，利用未标记数据进行视觉识别，包括半监督学习（SSL，§[2](#S2
    "2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")）、无监督学习（UL，§[3](#S3 "3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")）以及对 SSL 和 UL 的进一步讨论（§[4](#S4
    "4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey")）。其次，我们提供了一个统一的、深刻的分类法和现有方法的分析，以揭示它们的基础算法联系。最后，我们在 §[5](#S5 "5 Emerging
    Trends and Open Challenges ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey") 中展望了新兴趋势和未来的研究方向，以揭示那些尚未充分探讨的潜在关键领域。'
- en: 2 Semi-Supervised Learning (SSL)
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 半监督学习（SSL）
- en: Semi-Supervised Learning (SSL) [[13](#bib.bib13), [8](#bib.bib8)] aims at exploiting
    large unlabeled data together with sparsely labeled data. SSL is explored in various
    application domains, such as image search [[21](#bib.bib21)], medical data analysis [[22](#bib.bib22)],
    web-page classification [[23](#bib.bib23)], document retrieval [[24](#bib.bib24)],
    genetics and genomics [[25](#bib.bib25)]. More recently, SSL has been used for
    learning generic visual representations to facilitate many computer vision tasks
    such as image classification [[26](#bib.bib26), [27](#bib.bib27)], image retrieval [[28](#bib.bib28)],
    object detection [[29](#bib.bib29), [30](#bib.bib30)], semantic segmentation [[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33)], and pose estimation [[34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36)]. While our review mainly covers generic semi-supervised learners
    for image classification  [[37](#bib.bib37), [26](#bib.bib26), [27](#bib.bib27),
    [38](#bib.bib38)], the ideas behind thembe generalized to solve other vision recognition
    tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习（SSL）[[13](#bib.bib13)、[8](#bib.bib8)] 旨在利用大量未标记数据和稀疏标记数据。SSL 被应用于多个领域，如图像检索
    [[21](#bib.bib21)]、医学数据分析 [[22](#bib.bib22)]、网页分类 [[23](#bib.bib23)]、文档检索 [[24](#bib.bib24)]、遗传学和基因组学
    [[25](#bib.bib25)]。近年来，SSL 被用于学习通用视觉表示，以促进许多计算机视觉任务，如图像分类 [[26](#bib.bib26)、[27](#bib.bib27)]、图像检索
    [[28](#bib.bib28)]、目标检测 [[29](#bib.bib29)、[30](#bib.bib30)]、语义分割 [[31](#bib.bib31)、[32](#bib.bib32)、[33](#bib.bib33)]
    和姿势估计 [[34](#bib.bib34)、[35](#bib.bib35)、[36](#bib.bib36)]。虽然我们的综述主要涵盖了图像分类的通用半监督学习者
    [[37](#bib.bib37)、[26](#bib.bib26)、[27](#bib.bib27)、[38](#bib.bib38)]，但其背后的思想也可推广到解决其他视觉识别任务。
- en: 'We define the SSL problem setup and discuss its assumptions in §[2.1](#S2.SS1
    "2.1 The Problem Setting of SSL ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"). We provide a taxonomy and analysis
    of the existing semi-supervised deep learning methods in §[2.2](#S2.SS2 "2.2 Taxonomy
    on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey").'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 §[2.1](#S2.SS1 "2.1 The Problem Setting of SSL ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey") 中定义了
    SSL 问题设置并讨论了其假设。我们在 §[2.2](#S2.SS2 "2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")
    中提供了现有半监督深度学习方法的分类和分析。'
- en: 2.1 The Problem Setting of SSL
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 SSL 的问题设置
- en: 'Problem Definition. In SSL, we often have access to a limited amount of labeled
    samples $\mathcal{D}_{l}=\{\textbf{x}_{i,l},y_{i}\}_{i=1}^{N_{l}}$ and a large
    amount of unlabeled samples $\mathcal{D}_{u}=\{\textbf{x}_{i,u}\}_{i=1}^{N_{u}}$.
    Each labeled sample $\textbf{x}_{i,l}$ belongs to one of $K$ class labels $\mathcal{Y}=\{y_{k}\}_{k=1}^{K}$.
    For training, the SSL loss function $\mathcal{L}$ for a deep neural network (DNN)
    $\theta$ can generally be expressed as Eq. ([1](#S1.E1 "In 1 Introduction ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")), i.e., $\mathcal{L}=\lambda_{l}\mathcal{L}_{\text{sup}}+\lambda_{u}\mathcal{L}_{\text{unsup}}$.
    In many SSL methods, the hyperparameters $\lambda_{u}$ in Eq. ([1](#S1.E1 "In
    1 Introduction ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))
    is often a ramp-up weighting function (i.e., $\lambda=w(t)$ and $t$ is training
    iteration), which gradually increases the importance of the unsupervised loss
    term during training [[37](#bib.bib37), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [14](#bib.bib14)]. At test time, the model is deployed to recognize the $K$ known
    classes. See Figure [2](#S2.F2 "Figure 2 ‣ 2.1 The Problem Setting of SSL ‣ 2
    Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey") for an illustration of SSL.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '问题定义。在SSL中，我们通常访问到有限量的标记样本 $\mathcal{D}_{l}=\{\textbf{x}_{i,l},y_{i}\}_{i=1}^{N_{l}}$
    和大量的未标记样本 $\mathcal{D}_{u}=\{\textbf{x}_{i,u}\}_{i=1}^{N_{u}}$。每个标记样本 $\textbf{x}_{i,l}$
    属于 $K$ 个类别标签中的一个 $\mathcal{Y}=\{y_{k}\}_{k=1}^{K}$。对于训练，深度神经网络（DNN） $\theta$ 的SSL损失函数
    $\mathcal{L}$ 通常可以表示为 Eq. ([1](#S1.E1 "In 1 Introduction ‣ Semi-Supervised and
    Unsupervised Deep Visual Learning: A Survey"))，即 $\mathcal{L}=\lambda_{l}\mathcal{L}_{\text{sup}}+\lambda_{u}\mathcal{L}_{\text{unsup}}$。在许多SSL方法中，Eq. ([1](#S1.E1
    "In 1 Introduction ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A
    Survey")) 中的超参数 $\lambda_{u}$ 通常是一个逐步加重的函数（即 $\lambda=w(t)$ 和 $t$ 是训练迭代），它在训练过程中逐渐增加无监督损失项的重要性 [[37](#bib.bib37),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [14](#bib.bib14)]。在测试时，模型被部署以识别
    $K$ 个已知类别。有关SSL的说明，请参见图 [2](#S2.F2 "Figure 2 ‣ 2.1 The Problem Setting of SSL
    ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")。'
- en: '![Refer to caption](img/25a98a05fc09f720e106ef50bdf9d6e5.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/25a98a05fc09f720e106ef50bdf9d6e5.png)'
- en: 'Figure 2: Semi-supervised learning (SSL) aims to learn jointly from a small
    set of labeled and a large set of unlabeled data.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：半监督学习（SSL）旨在从一小部分标记数据和大量未标记数据中共同学习。
- en: Evaluation Protocol. To test whether an SSL model utilizes the unlabeled data
    effectively, two evaluation criteria are commonly adopted. First, the model needs
    to outperform its supervised baseline that learns from merely the labeled data.
    Second, when increasing the proportion of unlabeled samples in the training set,
    the improved margins upon the supervised baseline are expected to increase accordingly.
    Overall, these improved margins indicate the effectiveness and robustness of an
    SSL method.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 评估协议。为了测试SSL模型是否有效利用了未标记数据，通常采用两个评估标准。首先，模型需要超越其仅从标记数据中学习的监督基线。其次，当训练集中未标记样本的比例增加时，期望相对于监督基线的改进幅度也会相应增加。总体而言，这些改进幅度表示了SSL方法的有效性和鲁棒性。
- en: Assumptions. The main assumptions for SSL include the smoothness assumption [[42](#bib.bib42)]
    and manifold assumption [[42](#bib.bib42), [8](#bib.bib8)] – the latter is also
    known as cluster assumption [[43](#bib.bib43)], structure assumption [[44](#bib.bib44)],
    and low-density separation assumption [[45](#bib.bib45)]. Specifically, the smoothness
    assumption considers that the nearby data points are likely to share the same
    class label. The manifold assumption considers data points lying within the same
    structure (i.e., the same cluster or manifold) should share the same class label.
    In other words, the former assumption is imposed locally for nearby data points,
    while the latter is imposed globally based on the underlying data structure formed
    by clusters or graphs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设。SSL的主要假设包括平滑性假设 [[42](#bib.bib42)] 和流形假设 [[42](#bib.bib42), [8](#bib.bib8)]
    —— 后者也被称为集群假设 [[43](#bib.bib43)], 结构假设 [[44](#bib.bib44)] 和低密度分离假设 [[45](#bib.bib45)]。具体而言，平滑性假设认为相邻的数据点可能具有相同的类别标签。流形假设认为处于相同结构（即相同的集群或流形）中的数据点应具有相同的类别标签。换句话说，前者的假设是对相邻数据点局部施加的，而后者是基于由集群或图构成的底层数据结构的全球性假设。
- en: 'TABLE I: A taxonomy on semi-supervised deep learning methods, including five
    representative families in §[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency Regularization
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey") – §[2.2.5](#S2.SS2.SSS5 "2.2.5
    Self-Supervised Learning ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 半监督深度学习方法的分类，包括 §[2.2.1](#S2.SS2.SSS1 "2.2.1 一致性正则化 ‣ 2.2 SSL算法的分类 ‣ 2
    半监督学习 (SSL) ‣ 半监督和无监督深度视觉学习：一项调查") – §[2.2.5](#S2.SS2.SSS5 "2.2.5 自监督学习 ‣ 2.2
    SSL算法的分类 ‣ 2 半监督学习 (SSL) ‣ 半监督和无监督深度视觉学习：一项调查") 中介绍了五个代表性家族。'
- en: '| Families of Models | Model Rationale | Representative Strategies and Methods
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 模型家族 | 模型理由 | 代表性策略和方法 |'
- en: '| Consistency regularization | Random augmentation | $\Pi$-model [[46](#bib.bib46),
    [37](#bib.bib37)], ensemble transformations [[47](#bib.bib47)] |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 一致性正则化 | 随机增强 | $\Pi$-model [[46](#bib.bib46), [37](#bib.bib37)], 集成变换 [[47](#bib.bib47)]
    |'
- en: '| Adversarial perturbation | Virtual Adversarial Training (VAT) [[48](#bib.bib48),
    [49](#bib.bib49)] |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 对抗扰动 | 虚拟对抗训练（VAT） [[48](#bib.bib48), [49](#bib.bib49)] |'
- en: '| MixUp | MixMatch [[26](#bib.bib26)], ICT [[50](#bib.bib50)] |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| MixUp | MixMatch [[26](#bib.bib26)], ICT [[50](#bib.bib50)] |'
- en: '| Automated augmentation | ReMixMatch [[27](#bib.bib27)], UDA [[51](#bib.bib51)],
    FixMatch [[38](#bib.bib38)] |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 自动增强 | ReMixMatch [[27](#bib.bib27)], UDA [[51](#bib.bib51)], FixMatch [[38](#bib.bib38)]
    |'
- en: '| Stochastic perturbation | Pseudo-Ensembles [[52](#bib.bib52)], Ladder Network [[53](#bib.bib53)],
    Virtual Adversarial Dropout [[54](#bib.bib54)], WCP [[55](#bib.bib55)] |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 随机扰动 | 伪集成 [[52](#bib.bib52)], 阶梯网络 [[53](#bib.bib53)], 虚拟对抗性dropout [[54](#bib.bib54)],
    WCP [[55](#bib.bib55)] |'
- en: '| Ensembling | Temporal Ensembling [[37](#bib.bib37)], Mean Teacher [[39](#bib.bib39)],
    SWA [[41](#bib.bib41)], UASD [[14](#bib.bib14)] |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 集成方法 | 时间集成 [[37](#bib.bib37)], Mean Teacher [[39](#bib.bib39)], SWA [[41](#bib.bib41)],
    UASD [[14](#bib.bib14)] |'
- en: '| Self-training | Entropy minimization | Pseudo-Label [[56](#bib.bib56)], MixMatch [[26](#bib.bib26)],
    ReMixMatch [[27](#bib.bib27)], Memory [[57](#bib.bib57)] |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 自我训练 | 熵最小化 | 伪标签 [[56](#bib.bib56)], MixMatch [[26](#bib.bib26)], ReMixMatch
    [[27](#bib.bib27)], Memory [[57](#bib.bib57)] |'
- en: '| Co-training | Deep Co-training [[58](#bib.bib58)], Tri-training [[59](#bib.bib59)]
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 协同训练 | 深度协同训练 [[58](#bib.bib58)], 三重训练 [[59](#bib.bib59)] |'
- en: '| Distillation | model distillation (Noisy Student Training [[60](#bib.bib60)],
    UASD [[14](#bib.bib14)]), data distillation [[35](#bib.bib35)] |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 蒸馏 | 模型蒸馏（噪声学生训练 [[60](#bib.bib60)], UASD [[14](#bib.bib14)]），数据蒸馏 [[35](#bib.bib35)]
    |'
- en: '| Graph-based regularization | Graph-based feature regularizer | EmbedNN [[44](#bib.bib44)],
    Teacher Graph [[61](#bib.bib61)], Graph Convolutional Networks [[62](#bib.bib62)]
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的正则化 | 基于图的特征正则化器 | EmbedNN [[44](#bib.bib44)], Teacher Graph [[61](#bib.bib61)],
    图卷积网络 [[62](#bib.bib62)] |'
- en: '| Graph-based prediction regularizer | Label Propagation [[63](#bib.bib63)]
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 基于图的预测正则化器 | 标签传播 [[63](#bib.bib63)] |'
- en: '| Deep generative models | Variational auto-encoders | Class-conditional VAE [[64](#bib.bib64)],
    ADGM [[65](#bib.bib65)] |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 深度生成模型 | 变分自编码器 | 类条件VAE [[64](#bib.bib64)], ADGM [[65](#bib.bib65)] |'
- en: '| Generative adversarial networks | CatGAN [[66](#bib.bib66)], FM-GAN [[67](#bib.bib67)],
    ALI [[68](#bib.bib68)], BadGAN [[69](#bib.bib69)], Localized GAN [[70](#bib.bib70)]
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 生成对抗网络 | CatGAN [[66](#bib.bib66)], FM-GAN [[67](#bib.bib67)], ALI [[68](#bib.bib68)],
    BadGAN [[69](#bib.bib69)], Localized GAN [[70](#bib.bib70)] |'
- en: '| Self-supervised learning | Self-supervision | S4L [[71](#bib.bib71)], SimCLR [[12](#bib.bib12)],
    SimCLRv2 [[72](#bib.bib72)] |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 自监督学习 | 自监督 | S4L [[71](#bib.bib71)], SimCLR [[12](#bib.bib12)], SimCLRv2
    [[72](#bib.bib72)] |'
- en: 2.2 Taxonomy on SSL Algorithms
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 SSL算法的分类
- en: 'Existing SSL methods generally assume that the unlabeled data is closed-set
    and task-specific, i.e., all unlabeled training samples belong to a pre-defined
    set of classes. The idea shared by most existing works is to assign each unlabeled
    sample with a class label based on a certain underlying data structure, e.g.,
    manifold structure [[42](#bib.bib42), [44](#bib.bib44)], and graph structure [[73](#bib.bib73)].
    We divide the most representative semi-supervised deep learning methods into five
    categories: consistency regularization, self-training, graph-based regularization,
    deep generative models, and self-supervised learning (Table [I](#S2.T1 "TABLE
    I ‣ 2.1 The Problem Setting of SSL ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")), and provide their general
    model formulations in §[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency Regularization
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"), §[2.2.2](#S2.SS2.SSS2 "2.2.2
    Self-Training ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"), §[2.2.3](#S2.SS2.SSS3
    "2.2.3 Graph-based Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"),
    §[2.2.4](#S2.SS2.SSS4 "2.2.4 Deep Generative Models ‣ 2.2 Taxonomy on SSL Algorithms
    ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey") and §[2.2.5](#S2.SS2.SSS5 "2.2.5 Self-Supervised Learning
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的SSL方法通常假设未标记的数据是封闭集和任务特定的，即所有未标记的训练样本都属于预定义的类别集合。大多数现有工作的共同理念是基于某种潜在的数据结构（例如，流形结构 [[42](#bib.bib42),
    [44](#bib.bib44)]，图结构 [[73](#bib.bib73)]）为每个未标记样本分配一个类别标签。我们将最具代表性的半监督深度学习方法分为五类：一致性正则化、自训练、基于图的正则化、深度生成模型和自监督学习（表
    [I](#S2.T1 "TABLE I ‣ 2.1 The Problem Setting of SSL ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")），并在
    §[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency Regularization ‣ 2.2 Taxonomy on SSL
    Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")，§[2.2.2](#S2.SS2.SSS2 "2.2.2 Self-Training ‣
    2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")，§[2.2.3](#S2.SS2.SSS3 "2.2.3
    Graph-based Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")，§[2.2.4](#S2.SS2.SSS4
    "2.2.4 Deep Generative Models ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")和
    §[2.2.5](#S2.SS2.SSS5 "2.2.5 Self-Supervised Learning ‣ 2.2 Taxonomy on SSL Algorithms
    ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey") 中提供它们的一般模型公式。'
- en: 2.2.1 Consistency Regularization
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 一致性正则化
- en: 'Consistency regularization includes a number of successful and prevalent methods
    [[46](#bib.bib46), [37](#bib.bib37), [39](#bib.bib39), [49](#bib.bib49), [50](#bib.bib50),
    [26](#bib.bib26), [27](#bib.bib27), [74](#bib.bib74), [51](#bib.bib51)]. The basic
    rationale is to enforce consistent model outputs under variations in the input
    space and (or) model space. The variations are often implemented by adding noise,
    perturbations or forming variants of the same input or model. Formally, the objective
    in case of input variation is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性正则化包括一些成功且广泛使用的方法 [[46](#bib.bib46), [37](#bib.bib37), [39](#bib.bib39),
    [49](#bib.bib49), [50](#bib.bib50), [26](#bib.bib26), [27](#bib.bib27), [74](#bib.bib74),
    [51](#bib.bib51)]。其基本原理是强制模型输出在输入空间和（或）模型空间的变化下保持一致。这些变化通常通过添加噪声、扰动或形成相同输入或模型的变体来实现。形式上，输入变化情况下的目标是：
- en: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}d(p(y&#124;x;\theta),\hat{p}(y&#124;\hat{x};\theta)),$
    |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}d(p(y&#124;x;\theta),\hat{p}(y&#124;\hat{x};\theta)),$
    |  | (2) |'
- en: 'and in case of model variation is:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型变化情况下的目标是：
- en: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}d(p(y&#124;x;\theta),\hat{p}(y&#124;x;\hat{\theta})).$
    |  | (3) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}d(p(y&#124;x;\theta),\hat{p}(y&#124;x;\hat{\theta})).$
    |  | (3) |'
- en: 'In Eq. ([2](#S2.E2 "In 2.2.1 Consistency Regularization ‣ 2.2 Taxonomy on SSL
    Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")), $\hat{x}=q_{x}(x;\epsilon)$ is a variant of
    the original input $x$, which is derived through a data transformation operation
    $q_{x}(\cdot,\epsilon)$ with $\epsilon$ being the noise added via data augmentation
    and stochastic perturbation. Similarly, in Eq. ([3](#S2.E3 "In 2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")), $\hat{\theta}=f_{\theta}(\theta;\eta)$
    is a variant of the model $\theta$ derived via a transformation function $f_{\theta}(\cdot;\eta)$
    with $\eta$ being the randomness added via stochastic perturbation on model weights
    and model ensembling strategies. In both equations, the consistency is measured
    as the discrepancy $d(\cdot,\cdot)$ between two network outputs $p(y|\cdot,\cdot)$
    and $\hat{p}(y|\cdot,\cdot)$, typically quantified by divergence or distance metrics
    such as Kullback-Leibler (KL) divergence [[49](#bib.bib49)], cross-entropy [[51](#bib.bib51)],
    and mean square error (MSE) [[37](#bib.bib37)]. See Figure [3](#S2.F3 "Figure
    3 ‣ 2.2.1.1 Consistency regularization under input variations ‣ 2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey") for an illustration
    of consistency regularization.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式 ([2](#S2.E2 "在 2.2.1 一致性正则化 ‣ 2.2 SSL 算法分类 ‣ 2 半监督学习 (SSL) ‣ 半监督和无监督深度视觉学习：综述"))中，$\hat{x}=q_{x}(x;\epsilon)$
    是原始输入 $x$ 的一个变体，该变体通过数据变换操作 $q_{x}(\cdot,\epsilon)$ 获得，其中 $\epsilon$ 是通过数据增强和随机扰动添加的噪声。类似地，在公式 ([3](#S2.E3
    "在 2.2.1 一致性正则化 ‣ 2.2 SSL 算法分类 ‣ 2 半监督学习 (SSL) ‣ 半监督和无监督深度视觉学习：综述"))中，$\hat{\theta}=f_{\theta}(\theta;\eta)$
    是模型 $\theta$ 的一个变体，通过变换函数 $f_{\theta}(\cdot;\eta)$ 获得，其中 $\eta$ 是通过对模型权重和模型集成策略的随机扰动添加的随机性。在这两个公式中，一致性通过两个网络输出
    $p(y|\cdot,\cdot)$ 和 $\hat{p}(y|\cdot,\cdot)$ 之间的差异 $d(\cdot,\cdot)$ 来衡量，通常通过诸如
    Kullback-Leibler (KL) 散度 [[49](#bib.bib49)]、交叉熵 [[51](#bib.bib51)] 和均方误差 (MSE) [[37](#bib.bib37)]
    等散度或距离度量来量化。有关一致性正则化的示意图，请参见图 [3](#S2.F3 "图 3 ‣ 2.2.1.1 输入变化下的一致性正则化 ‣ 2.2.1 一致性正则化
    ‣ 2.2 SSL 算法分类 ‣ 2 半监督学习 (SSL) ‣ 半监督和无监督深度视觉学习：综述")。
- en: 2.2.1.1   Consistency regularization under input variations
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.2.1.1   输入变化下的一致性正则化
- en: '![Refer to caption](img/efd8149785c42a49f34686ab3e654067.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/efd8149785c42a49f34686ab3e654067.png)'
- en: 'Figure 3: In consistency regularization (§[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")) (a) input
    variations vs (b) model variations, where variations can be induced by transformation
    on input data or model weights.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：一致性正则化 (§[2.2.1](#S2.SS2.SSS1 "2.2.1 一致性正则化 ‣ 2.2 SSL 算法分类 ‣ 2 半监督学习 (SSL)
    ‣ 半监督和无监督深度视觉学习：综述")) (a) 输入变化与 (b) 模型变化，其中变化可以通过对输入数据或模型权重的变换引起。
- en: 'Various strategies aim to generate different versions of the same input ($\hat{x}$
    in Eq. ([2](#S2.E2 "In 2.2.1 Consistency Regularization ‣ 2.2 Taxonomy on SSL
    Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey"))) enforcing consistency (distributional smoothness)
    under input variations as depicted in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2.1.1 Consistency
    regularization under input variations ‣ 2.2.1 Consistency Regularization ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey") (a). Techniques range from simple
    random augmentation [[46](#bib.bib46), [37](#bib.bib37)], to more advanced transformations
    such as adversarial perturbation [[49](#bib.bib49)], MixUp [[75](#bib.bib75),
    [26](#bib.bib26)], as well as stronger automated augmentation such as AutoAugment [[76](#bib.bib76)],
    RandAugment [[77](#bib.bib77)], CTAugment [[27](#bib.bib27)] and Cutout [[78](#bib.bib78)].
    Below we review these four streams of models.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '各种策略旨在生成相同输入的不同版本（如公式中的$\hat{x}$ ([2](#S2.E2 "In 2.2.1 Consistency Regularization
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"))) 强调在输入变化下的一致性（分布平滑性），如图[3](#S2.F3
    "Figure 3 ‣ 2.2.1.1 Consistency regularization under input variations ‣ 2.2.1
    Consistency Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")
    (a)所示。技术范围从简单的随机增强[[46](#bib.bib46), [37](#bib.bib37)]，到更先进的变换，如对抗性扰动[[49](#bib.bib49)]，MixUp
    [[75](#bib.bib75), [26](#bib.bib26)]，以及更强的自动化增强，如AutoAugment [[76](#bib.bib76)]，RandAugment
    [[77](#bib.bib77)]，CTAugment [[27](#bib.bib27)] 和 Cutout [[78](#bib.bib78)]。下面我们将回顾这四种模型流派。'
- en: Random augmentation is a standard data transformation strategy widely adopted [[46](#bib.bib46),
    [37](#bib.bib37), [39](#bib.bib39)] via adding Gaussian noise and applying simple
    domain-specific jittering such as flipping and cropping on image data. For instance,
    the $\Pi$-model [[46](#bib.bib46), [37](#bib.bib37)], applies random data augmentation
    on the same input and minimizes a consistency regularization term (MSE) between
    two network outputs. Ensemble transformations [[47](#bib.bib47)] introduces more
    diverse data augmentation on input images, including spatial transformations (i.e.,
    projective, affine, similarity, euclidean transformations) to modify the spatial
    aspect ratio, as well as non-spatial transformations to change the color, contrast,
    brightness, and sharpness. This way, the model learns representations invariant
    to various transformations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 随机增强是一种标准的数据变换策略，广泛采用[[46](#bib.bib46), [37](#bib.bib37), [39](#bib.bib39)]，通过添加高斯噪声以及对图像数据应用简单的特定领域抖动，如翻转和裁剪。例如，$\Pi$-model
    [[46](#bib.bib46), [37](#bib.bib37)] 在相同输入上应用随机数据增强，并最小化两个网络输出之间的一致性正则化项（MSE）。集成变换
    [[47](#bib.bib47)] 引入了更多样化的数据增强，包括空间变换（即投影、仿射、相似、欧几里得变换）以修改空间纵横比，以及非空间变换以改变颜色、对比度、亮度和清晰度。通过这种方式，模型学习对各种变换不变的表示。
- en: Adversarial perturbation augments the input data by adding adversarial noise
    aiming to alter the model predictions, e.g., reducing predictive confidence or
    changing the predicted correct label [[79](#bib.bib79), [80](#bib.bib80)]. Adversarial
    noise is introduced for SSL to augment data and learn from the unlabeled data
    with adversarial transformations [[48](#bib.bib48), [49](#bib.bib49), [81](#bib.bib81),
    [74](#bib.bib74)]. Virtual Adversarial Training (VAT) [[48](#bib.bib48), [49](#bib.bib49)]
    is the first representative SSL method that perturbs input data adversarially.
    In VAT, a small adversarial perturbation is added to each input and a consistency
    regularization term (i.e., KL divergence) is imposed to encourage distributional
    robustness of the model against the virtual adversarial direction. Notably, it
    has been discovered that semi-supervised learning with adversarial perturbed unlabeled
    data does not only improve model generalization, but it also enhances robustness
    to adversarial attacks [[81](#bib.bib81), [82](#bib.bib82)].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性扰动通过添加对抗性噪声来增强输入数据，旨在改变模型预测，例如，减少预测信心或更改预测的正确标签[[79](#bib.bib79), [80](#bib.bib80)]。对抗性噪声被引入SSL以增强数据，并从具有对抗性变换的未标记数据中学习[[48](#bib.bib48),
    [49](#bib.bib49), [81](#bib.bib81), [74](#bib.bib74)]。虚拟对抗训练（VAT）[[48](#bib.bib48),
    [49](#bib.bib49)] 是第一个代表性的SSL方法，它对输入数据进行对抗性扰动。在VAT中，对每个输入添加一个小的对抗性扰动，并施加一致性正则化项（即KL散度），以鼓励模型对虚拟对抗方向的分布鲁棒性。值得注意的是，发现具有对抗性扰动未标记数据的半监督学习不仅提高了模型的泛化能力，而且增强了对抗攻击的鲁棒性[[81](#bib.bib81),
    [82](#bib.bib82)]。
- en: MixUp is a simple and data-agnostic augmentation strategy by performing linear
    interpolations on two inputs and their corresponding labels [[75](#bib.bib75)].
    It is also introduced as an effective regularizer for SSL [[50](#bib.bib50), [26](#bib.bib26)].
    The Interpolation Consistency Training (ICT) [[50](#bib.bib50)] interpolates two
    unlabeled samples and their network outputs. MixMatch [[26](#bib.bib26)] further
    considers to mix a labeled sample and unlabeled sample as the input, and the groundtruth
    label (of labeled data) and the predicted label (of unlabeled data) as the output
    targets. Both methods impose consistency regularization to guide the learning
    of a mapping between the interpolated input and interpolated output to learn from
    unlabeled data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: MixUp 是一种简单且与数据无关的增强策略，通过对两个输入及其对应标签进行线性插值来实现[[75](#bib.bib75)]。它还被引入作为一种有效的正则化方法，用于半监督学习（SSL）[[50](#bib.bib50),
    [26](#bib.bib26)]。插值一致性训练（ICT）[[50](#bib.bib50)]对两个未标记样本及其网络输出进行插值。MixMatch [[26](#bib.bib26)]
    进一步考虑将标记样本和未标记样本混合作为输入，将真实标签（标记数据的标签）和预测标签（未标记数据的标签）作为输出目标。这两种方法都施加一致性正则化，以指导学习插值输入和插值输出之间的映射，从而从未标记数据中学习。
- en: Automated augmentation learns augmentation strategies from data to produce strong
    samples, alleviating the need to manually design domain-specific data augmentation [[76](#bib.bib76),
    [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85), [77](#bib.bib77)]. It is
    introduced for SSL by enforcing that the predicted labels of a weakly-augmented
    or clean sample and its strongly augmented versions derived from automated augmentation [[27](#bib.bib27),
    [51](#bib.bib51)] are consistent. Inspired by the advances of AutoAugment [[76](#bib.bib76)],
    ReMixMatch [[27](#bib.bib27)] introduces CTAugment to learn an automated augmentation
    policy. Unsupervised Data Augmentation (UDA) [[51](#bib.bib51)] adopts RandAugment [[77](#bib.bib77)]
    to produce more diverse and strongly augmented samples by uniformly sampling a
    set of standard transformations based on the Python Image Library. Later on, FixMatch [[38](#bib.bib38)]
    unifies multiple augmentation strategies including Cutout [[78](#bib.bib78)],
    CTAugment [[27](#bib.bib27)], and RandAugment [[77](#bib.bib77)] and produces
    even more strongly augmented samples as input.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化增强从数据中学习增强策略，以生成强样本，减少了手动设计特定领域数据增强的需要[[76](#bib.bib76), [83](#bib.bib83),
    [84](#bib.bib84), [85](#bib.bib85)]。它通过强制要求弱增强或清洁样本及其由自动化增强[[27](#bib.bib27),
    [51](#bib.bib51)] 派生的强增强版本的预测标签保持一致来引入到SSL中。受到 AutoAugment [[76](#bib.bib76)]
    进展的启发，ReMixMatch [[27](#bib.bib27)] 引入了 CTAugment 以学习自动化增强策略。无监督数据增强（UDA）[[51](#bib.bib51)]
    采用 RandAugment [[77](#bib.bib77)] 通过基于 Python 图像库的标准变换集进行均匀采样，以生成更多样化且强增强的样本。随后，FixMatch
    [[38](#bib.bib38)] 统一了包括 Cutout [[78](#bib.bib78)], CTAugment [[27](#bib.bib27)],
    和 RandAugment [[77](#bib.bib77)] 在内的多种增强策略，并生成了更强的增强样本作为输入。
- en: 2.2.1.2   Consistency regularization under model variations
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2.2.1.2   模型变化下的一致性正则化
- en: 'To impose the predictive consistency under model variations (i.e., variations
    made in the model’s parameter space) as in Eq. ([3](#S2.E3 "In 2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")), stochastic
    perturbation [[52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)] and ensembling [[39](#bib.bib39),
    [86](#bib.bib86), [37](#bib.bib37)] are proposed. Via non-identical models they
    produce different outputs for the same input – a new model variant is denoted
    by $\hat{\theta}$ in Eq. ([3](#S2.E3 "In 2.2.1 Consistency Regularization ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")). Below we review these two
    streams of works as depicted in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2.1.1 Consistency
    regularization under input variations ‣ 2.2.1 Consistency Regularization ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey") (b).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在模型变化下强加预测一致性（即，模型参数空间中的变化），如 Eq. ([3](#S2.E3 "In 2.2.1 Consistency Regularization
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")），提出了随机扰动[[52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54)]和集成方法[[39](#bib.bib39), [86](#bib.bib86), [37](#bib.bib37)]。通过非同质模型，它们对相同输入产生不同输出——一个新的模型变体在
    Eq. ([3](#S2.E3 "In 2.2.1 Consistency Regularization ‣ 2.2 Taxonomy on SSL Algorithms
    ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")) 中表示为 $\hat{\theta}$。下面我们将回顾这两种方法，如图 [3](#S2.F3 "Figure 3
    ‣ 2.2.1.1 Consistency regularization under input variations ‣ 2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey") (b) 所示。'
- en: Stochastic perturbation introduces slight modifications on the model weights
    by adding Gaussian noise, dropout, or adversarial noise in a class-agnostic manner
    [[52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)]. For example, Ladder Network
    injects layer-wise Gaussian noises into the network and minimizes a denoising
    L2 loss between outputs from the original network and the noisy-corrupted network [[53](#bib.bib53)].
    Pseudo-Ensemble applies dropout on the model’s parameters to obtain a collection
    of models (a pseudo-ensemble), while minimizing the disagreements (KL divergence)
    between the pseudo-ensemble and the model [[52](#bib.bib52)]. Similarly, Virtual
    Adversarial Dropout introduces adversarial dropout to selectively deactivates
    network neurons and minimizes the discrepancy between outputs from the original
    model and the perturbed model [[54](#bib.bib54)]. Worst-Case Perturbations (WCP)
    introduces both addictive perturbations and drop connections on model parameters,
    where drop connections set certain model weights to zero to further change the
    network structure [[55](#bib.bib55)]. Notably, these perturbation mechanisms promote
    the model robustness against noise in network parameters or structure.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 随机扰动通过添加高斯噪声、丢弃法或对抗噪声对模型权重进行轻微修改，这些修改与类别无关[[52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54)]。例如，Ladder Network向网络中注入层级高斯噪声，并最小化原始网络与噪声污染网络输出之间的去噪L2损失[[53](#bib.bib53)]。Pseudo-Ensemble在模型参数上应用丢弃法，以获得一组模型（伪集成），同时最小化伪集成与模型之间的不一致（KL散度）[[52](#bib.bib52)]。类似地，Virtual
    Adversarial Dropout引入对抗丢弃法，选择性地停用网络神经元，并最小化原始模型与扰动模型之间的差异[[54](#bib.bib54)]。Worst-Case
    Perturbations (WCP)引入了添加扰动和丢弃连接到模型参数，其中丢弃连接将某些模型权重设置为零，从而进一步改变网络结构[[55](#bib.bib55)]。值得注意的是，这些扰动机制增强了模型对网络参数或结构中噪声的鲁棒性。
- en: Ensembling learns a set of models covering different regions of the version
    space [[87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89)]. As demonstrated
    by seminal machine learning models such as boosting [[90](#bib.bib90)] and random
    forest [[89](#bib.bib89)], a set of different models can often provide more reliable
    predictions than a single model. Moreover, ensembling offers a rich inference
    uncertainty to mitigate the overconfidence issue in deep neural networks  [[91](#bib.bib91)].
    For SSL, an ensemble model is typically derived by computing an exponential moving
    average (EMA) or equal average in the prediction space or weight space [[39](#bib.bib39),
    [37](#bib.bib37), [41](#bib.bib41), [14](#bib.bib14)]. Temporal Ensembling [[37](#bib.bib37)]
    and Mean Teacher [[39](#bib.bib39)] are two representatives that first propose
    to ensemble all the networks produced during training by maintaining an EMA in
    the weight space [[39](#bib.bib39)] or prediction space [[37](#bib.bib37)]. Stochastic
    Weight Averaging (SWA) [[41](#bib.bib41)] applies an equal average of the model
    parameters in the weight space to provide a more stable target for deriving the
    consistency cost. Later on, Uncertainty-Aware Self-Distillation (UASD) [[14](#bib.bib14)]
    computes an equal average of all the preceding model predictions during training
    to derive soft targets as the regularizer.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习学习了一组覆盖版本空间不同区域的模型[[87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89)]。如提升[[90](#bib.bib90)]和随机森林[[89](#bib.bib89)]等开创性机器学习模型所示，一组不同的模型通常能提供比单个模型更可靠的预测。此外，集成学习提供了丰富的推理不确定性，以减轻深度神经网络中的过度自信问题[[91](#bib.bib91)]。对于SSL，集成模型通常通过计算预测空间或权重空间中的指数移动平均（EMA）或等均值来获得[[39](#bib.bib39),
    [37](#bib.bib37), [41](#bib.bib41), [14](#bib.bib14)]。时序集成[[37](#bib.bib37)]和均值教师[[39](#bib.bib39)]是两个代表性方法，它们首次提出通过在权重空间[[39](#bib.bib39)]或预测空间[[37](#bib.bib37)]中保持EMA来集成训练过程中产生的所有网络。随机权重平均（SWA）[[41](#bib.bib41)]在权重空间中应用模型参数的等均值，以提供一个更稳定的目标来推导一致性成本。后来，带不确定性感知的自蒸馏（UASD）[[14](#bib.bib14)]计算训练期间所有前序模型预测的等均值，以推导软目标作为正则化器。
- en: Remarks. Consistency regularization can be treated as an auxiliary task where
    the model learns from the unlabeled data to minimize its predictive variance towards
    the variations in the input space or weight space. The predictive variance is
    generally quantified as the discrepancy between two predictive probability distributions
    or network outputs. By minimizing the consistency regularization loss, the model
    is encouraged to learn more powerful representations invariant towards variations
    added on each sample, without utilizing any additional label annotation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：一致性正则化可以视为一个辅助任务，其中模型从未标记数据中学习，以最小化其对输入空间或权重空间变化的预测方差。预测方差通常被量化为两个预测概率分布或网络输出之间的差异。通过最小化一致性正则化损失，模型被鼓励学习对每个样本添加的变化不变的更强表示，而无需使用任何额外的标签注释。
- en: '![Refer to caption](img/f6730591df05c366101b7bb7a7e708c8.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f6730591df05c366101b7bb7a7e708c8.png)'
- en: 'Figure 4: In self-training, (a) the model prediction is enforced to have low
    entropy, (b) two models learn from each other and (c) the student model learns
    from the teacher model.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在自训练中，（a）模型预测被强制保持低熵，（b）两个模型互相学习，（c）学生模型从教师模型中学习。
- en: 2.2.2 Self-Training
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 自训练
- en: 'Self-training methods learn from unlabeled data by imputing the labels for
    samples predicted with high confidence [[92](#bib.bib92), [23](#bib.bib23), [24](#bib.bib24)].
    It is originally proposed for conventional machine learning models such as logistic
    regression [[92](#bib.bib92)], bipartite graph [[23](#bib.bib23)] and Naive Bayes
    classifier [[24](#bib.bib24)]. It is re-visited in deep neural networks to learn
    from massive unlabeled data along with limited labeled data. We review three representative
    lines of works in self-training, including entropy minimization, co-training and
    distillation as follows. See Figure [4](#S2.F4 "Figure 4 ‣ 2.2.1.2 Consistency
    regularization under model variations ‣ 2.2.1 Consistency Regularization ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey") for an illustration of self-training.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy minimization regularizes the model training based on the low density
    separation assumption  [[92](#bib.bib92), [45](#bib.bib45)], to enforce that the
    class decision boundary is placed in the low density regions. This is also in
    line with the cluster assumption and manifold assumption [[42](#bib.bib42), [44](#bib.bib44)],
    which hypothesizes that data points from the same class are likely to share the
    same cluster or manifold. Formally, the entropy minimization objective can be
    formulated as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}\Big{(}-\sum{{}_{j=1}^{K}}p(y_{j}&#124;x;\theta)\log\
    p(y_{j}&#124;x;\theta)\Big{)},$ |  | (4) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: 'where $K$ refers to the number of classes. $p(y_{j}|x;\theta)$ is the probability
    of assigning the sample $x$ to the class $y_{j}$. This measures the class overlap.
    As a lower entropy indicates a higher confidence in model prediction, minimizing
    Eq. ([4](#S2.E4 "In 2.2.2 Self-Training ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))
    enforces each unlabeled sample to be assigned to the class predicted with the
    highest probability. Although entropy minimization is originally proposed for
    logistic regression to impute the labels of samples classified with high confidence [[92](#bib.bib92)],
    it is later extended to train deep neural networks in SSL setting by minimizing
    the entropy of the class assignments either derived in the prediction space [[56](#bib.bib56),
    [93](#bib.bib93), [49](#bib.bib49), [26](#bib.bib26), [27](#bib.bib27), [38](#bib.bib38)]
    or the feature space [[57](#bib.bib57)], as detailed next.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Entropy minimization can be imposed in the prediction space, e.g., Pseudo-Label [[56](#bib.bib56)]
    directly assigns each sample to the class label predicted with the maximum probability,
    which implicitly minimizes the entropy of model predictions. When pseudo labels
    are one-hot vectors, they could easily cause error propagation due to the wrong
    label assignments. To alleviate this risk, MixMatch [[26](#bib.bib26)] uses an
    ensemble of predictions over different input augmentations, and softly sharpens
    the one-hot pseudo labels with a temperature hyperparameter. Similarly, FixMatch [[38](#bib.bib38)]
    assigns the one-hot labels only when the confidence scores of the model predictions
    are higher than a certain threshold.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 熵最小化可以在预测空间中强加，例如，伪标签 [[56](#bib.bib56)] 直接将每个样本分配给预测概率最大化的类别标签，这隐含地最小化了模型预测的熵。当伪标签是独热编码向量时，它们可能由于错误的标签分配而容易导致错误传播。为了减轻这种风险，MixMatch [[26](#bib.bib26)]
    使用了不同输入增强的预测集成，并通过一个温度超参数对独热伪标签进行平滑处理。类似地，FixMatch [[38](#bib.bib38)] 仅在模型预测的置信度分数高于某个阈值时才分配独热标签。
- en: Entropy minimization can also be imposed in the feature space, as it is feasible
    to derive the class assignments based on proximities to class-level prototypes
    (e.g., cluster centers) in the feature space [[94](#bib.bib94), [57](#bib.bib57)].
    In [[57](#bib.bib57)], a Memory module learns a center per class that is derived
    based on proximities to all the cluster centers. Each unlabeled sample is assigned
    to the nearest cluster center by minimizing the entropy.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 熵最小化也可以在特征空间中强加，因为基于特征空间中与类级原型（例如，聚类中心）的接近度来推导类别分配是可行的 [[94](#bib.bib94), [57](#bib.bib57)]。在 [[57](#bib.bib57)]
    中，一个记忆模块学习每个类的中心，该中心是基于与所有聚类中心的接近度推导的。通过最小化熵，将每个未标记样本分配给最近的聚类中心。
- en: 'Co-training learns two or more classifiers on more than one view of the same
    sample coming from different sources [[23](#bib.bib23), [7](#bib.bib7), [24](#bib.bib24),
    [58](#bib.bib58), [59](#bib.bib59)]. Conceptually, a co-training framework [[23](#bib.bib23),
    [24](#bib.bib24)] trains two independent classifier models on two different but
    complementary data views and imputes the predicted labels in a cross-model manner.
    It is later extended for deep visual learning [[58](#bib.bib58), [59](#bib.bib59),
    [95](#bib.bib95)], e.g., Deep Co-training (DCT) [[58](#bib.bib58)] trains a network
    with two or more classification layers, and passes different views (e.g., the
    original view and the adversarial view [[96](#bib.bib96)]) to individual classifiers
    for co-training, while an unsupervised loss is imposed to minimize the similarity
    of predictions from different views. The basic idea of co-training can be extended
    from dual-view [[58](#bib.bib58)] to triple [[59](#bib.bib59)] or multi-view [[58](#bib.bib58)]
    – e.g., in Tri-training [[59](#bib.bib59)], three classifiers are trained together,
    with labels assigned to the unlabeled data when two of the classifiers agree on
    the predictions and the confidence scores are higher than a threshold. Formally,
    the deep co-training objective can be written as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 协同训练在来自不同来源的同一样本的多个视图上学习两个或更多分类器 [[23](#bib.bib23), [7](#bib.bib7), [24](#bib.bib24),
    [58](#bib.bib58), [59](#bib.bib59)]。从概念上讲，协同训练框架 [[23](#bib.bib23), [24](#bib.bib24)]
    在两个不同但互补的数据视图上训练两个独立的分类器模型，并以交叉模型的方式推断预测标签。它后来被扩展到深度视觉学习 [[58](#bib.bib58), [59](#bib.bib59),
    [95](#bib.bib95)]，例如，Deep Co-training (DCT) [[58](#bib.bib58)] 训练一个具有两个或更多分类层的网络，并将不同的视图（例如，原始视图和对抗视图 [[96](#bib.bib96)]）传递给各个分类器进行协同训练，同时施加无监督损失以最小化来自不同视图的预测相似性。协同训练的基本思想可以从双视图 [[58](#bib.bib58)]
    扩展到三视图 [[59](#bib.bib59)] 或多视图 [[58](#bib.bib58)] – 例如，在 Tri-training [[59](#bib.bib59)]
    中，三个分类器一起训练，当两个分类器对预测达成一致且置信度分数高于阈值时，将标签分配给未标记的数据。形式上，深度协同训练目标可以写作：
- en: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}d(p_{1}(y&#124;x;\theta_{1}),z_{2})+d(p_{2}(y&#124;x;\theta_{2}),z_{1}),$
    |  | (5) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}d(p_{1}(y\mid
    x;\theta_{1}),z_{2})+d(p_{2}(y\mid x;\theta_{2}),z_{1}),$ |  | (5) |'
- en: where $p_{1},p_{2}$ are predictions of two independent classifiers $\theta_{1},\theta_{2}$
    trained on different data views. $d(\cdot,\cdot)$ introduces the similarity metric
    to learn from the imputed targets $z_{1},z_{2}$ from each other, e.g., cross-entropy
    on one-hot targets [[59](#bib.bib59)], or Jensen-Shannon divergence between output
    targets [[58](#bib.bib58)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{1},p_{2}$ 是在不同数据视图上训练的两个独立分类器 $\theta_{1},\theta_{2}$ 的预测。$d(\cdot,\cdot)$
    引入了相似性度量来从彼此填补的目标 $z_{1},z_{2}$ 中学习，例如，独热目标上的交叉熵 [[59](#bib.bib59)]，或输出目标之间的 Jensen-Shannon
    散度 [[58](#bib.bib58)]。
- en: 'Distillation is originally proposed to transfer the knowledge learned by a
    teacher model to a student model, where the soft targets from the teacher model
    (e.g., an ensemble of networks or a larger network) can serve as an effective
    regularizer or a model compression strategy to train a student model [[97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99)]. Recent works in SSL use distillation to impute
    learning targets on the unlabeled data for training the student network [[35](#bib.bib35),
    [60](#bib.bib60), [100](#bib.bib100), [14](#bib.bib14)]. Formally, an unsupervised
    distillation objective is introduced on a student model $\theta_{s}$ to learn
    from the unlabeled data as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏最初是为了将教师模型所学的知识转移到学生模型中，其中教师模型（例如，一个网络集合或更大的网络）产生的软目标可以作为有效的正则化器或模型压缩策略来训练学生模型 [[97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99)]。最近的半监督学习（SSL）研究使用蒸馏来填补未标记数据上的学习目标，以训练学生网络 [[35](#bib.bib35),
    [60](#bib.bib60), [100](#bib.bib100), [14](#bib.bib14)]。形式上，引入了一个无监督的蒸馏目标到学生模型
    $\theta_{s}$ 中，以从未标记的数据中学习，如下所示：
- en: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}d(p_{s}(y&#124;x;\theta_{s}),z_{t}),$
    |  | (6) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\theta}{\text{min}}\ \ \sum_{x\in\mathcal{D}}d(p_{s}(y&#124;x;\theta_{s}),z_{t}),$
    |  | (6) |'
- en: 'where the student prediction $p_{s}$ is enforced to align with the targets
    $z_{t}$ produced by a teacher model $\theta_{t}$ on either the unlabeled data
    or all the data. Compared to co-training (Eq. ([5](#S2.E5 "In 2.2.2 Self-Training
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"))), distillation in SSL (Eq. ([6](#S2.E6
    "In 2.2.2 Self-Training ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))) does
    not optimize multiple networks simultaneously, but instead trains more than one
    network in different stages. In distillation, the existing works can be further
    grouped into model distillation and data distillation, which generate learning
    targets for unlabeled data using the teacher model output or multiple forward
    passes of the same input data, as detailed next.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '学生预测 $p_{s}$ 被要求与教师模型 $\theta_{t}$ 在未标记数据或所有数据上生成的目标 $z_{t}$ 对齐。与共同训练（Eq. ([5](#S2.E5
    "In 2.2.2 Self-Training ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))）相比，SSL中的蒸馏（Eq. ([6](#S2.E6
    "In 2.2.2 Self-Training ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))) 并不同时优化多个网络，而是分阶段训练多个网络。在蒸馏中，现有的工作可以进一步分为模型蒸馏和数据蒸馏，它们通过教师模型的输出或相同输入数据的多次前向传播来生成未标记数据的学习目标，具体如下。'
- en: In model distillation, labels from a teacher are assigned to a student [[60](#bib.bib60),
    [100](#bib.bib100), [14](#bib.bib14)]. The teacher model can be formed, e.g.,
    via a pre-trained model or an ensemble of models. In Noisy Student Training [[60](#bib.bib60)],
    an iterative self-training process iterates the teacher-student training by first
    training a teacher to impute labels on unlabeled data for the student, and reuses
    the student as the teacher in the next iteration. In Uncertainty-Aware Self-Distillation
    (USAD) [[14](#bib.bib14)], the teacher averages all the preceding network predictions
    to impute labels on unlabeled data for updating the student network itself. In
    model distillation, both soft targets and one-hot labels from the teacher model
    can serve as the learning targets on the unlabeled data [[60](#bib.bib60), [14](#bib.bib14)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型蒸馏中，来自教师的标签被分配给学生 [[60](#bib.bib60), [100](#bib.bib100), [14](#bib.bib14)]。教师模型可以通过例如预训练模型或模型集成来构建。在噪声学生训练（Noisy
    Student Training） [[60](#bib.bib60)]中，迭代自训练过程通过首先训练一个教师模型来为学生模型的未标记数据填补标签，然后在下一次迭代中将学生模型重新用作教师模型，从而迭代教师-学生训练。在不确定性感知自蒸馏（Uncertainty-Aware
    Self-Distillation, USAD）[[14](#bib.bib14)]中，教师模型通过平均所有之前的网络预测来填补未标记数据上的标签，从而更新学生网络自身。在模型蒸馏中，来自教师模型的软目标和独热标签都可以作为未标记数据上的学习目标 [[60](#bib.bib60),
    [14](#bib.bib14)]。
- en: In data distillation, the teacher model predicts learning targets on unlabeled
    data by ensembling the outputs of the same input under different data transformations [[35](#bib.bib35)].
    Specifically, the ensembled teacher predictions (i.e., soft targets) are derived
    by averaging the outputs of the same inputs under multiple data transformations;
    while the student model is then trained with the soft targets. Data distillation
    transforms the input data multiple times rather than training multiple networks
    to impute the ensembled predictions on unlabeled data. This is similar to consistency
    regularization with random data augmentation; however, in data distillation, two
    training stages are involved – the first stage involves pre-training the teacher
    model; while the second stage involves training the student network to mimic the
    teacher model by distillation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据蒸馏中，教师模型通过对相同输入进行不同数据转换的输出进行集成，从而预测无标签数据上的学习目标[[35](#bib.bib35)]。具体而言，集成的教师预测（即软目标）是通过对相同输入在多个数据转换下的输出进行平均得到的；然后，学生模型利用这些软目标进行训练。数据蒸馏对输入数据进行多次转换，而不是训练多个网络来推断无标签数据上的集成预测。这类似于具有随机数据增强的一致性正则化；然而，在数据蒸馏中涉及两个训练阶段——第一阶段是预训练教师模型；第二阶段是训练学生网络，通过蒸馏模仿教师模型。
- en: Remarks. Similar to consistency regularization, self-training can be considered
    as an unsupervised auxiliary task learned along with the supervised learning task.
    In general, it also enforces the predictive invariance towards instance-wise variations
    or the teacher’s predictions. However, self-training differs in design. While
    consistency regularization generally trains one model, self-training may require
    more than one model to be trained, e.g., co-training requires at least two models
    trained in parallel while distillation requires to train a teacher and a student
    model sequentially.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。类似于一致性正则化，自我训练可以被视为一种与监督学习任务同时学习的无监督辅助任务。一般来说，它也会强制实施预测对实例级变化或教师预测的稳定性。然而，自我训练在设计上有所不同。尽管一致性正则化通常训练一个模型，自我训练可能需要训练多个模型，例如，共训练需要至少两个模型并行训练，而蒸馏则需要依次训练教师模型和学生模型。
- en: 2.2.3 Graph-based Regularization
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 基于图的正则化
- en: '![Refer to caption](img/9752b71c97cb3464b17aa5151a9c3f4d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9752b71c97cb3464b17aa5151a9c3f4d.png)'
- en: 'Figure 5: In graph-based regularization (§[2.2.3](#S2.SS2.SSS3 "2.2.3 Graph-based
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")) pseudo labels
    are propagated over the Nearest Neighbor graph based on neighbourhood consistency
    and an unsupervised regularization term is imposed on the feature or prediction
    space.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在基于图的正则化（§[2.2.3](#S2.SS2.SSS3 "2.2.3 基于图的正则化 ‣ 2.2 SSL 算法分类 ‣ 2 半监督学习 (SSL)
    ‣ 半监督和无监督深度视觉学习：综述")）中，伪标签在最近邻图上基于邻域一致性进行传播，并在特征或预测空间上施加无监督正则化项。
- en: 'Graph-based regularization is a family of transductive learning methods originally
    proposed for non-deep semi-supervised learning algorithms [[73](#bib.bib73), [101](#bib.bib101),
    [42](#bib.bib42), [102](#bib.bib102), [103](#bib.bib103)], such as transductive
    Support Vector Machine [[42](#bib.bib42), [102](#bib.bib102)] and Gaussian random
    field model [[101](#bib.bib101)]. Most algorithms from this family build a weighted
    graph to exploit relationships among the data samples. Specifically, both labeled
    and unlabeled samples are represented as nodes, while the edge weights encode
    the similarities between different samples. The labels can be propagated over
    the graph based on the smoothness assumption [[42](#bib.bib42)], i.e., neighboring
    data points should share the same class label as shown in Figure [5](#S2.F5 "Figure
    5 ‣ 2.2.3 Graph-based Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的正则化是一类最初为非深度半监督学习算法提出的传递学习方法[[73](#bib.bib73), [101](#bib.bib101), [42](#bib.bib42),
    [102](#bib.bib102), [103](#bib.bib103)]，例如传递支持向量机[[42](#bib.bib42), [102](#bib.bib102)]和高斯随机场模型[[101](#bib.bib101)]。该家族中的大多数算法构建一个加权图以利用数据样本之间的关系。具体而言，有标签样本和无标签样本都被表示为节点，而边的权重编码不同样本之间的相似性。基于平滑性假设[[42](#bib.bib42)]，即邻近的数据点应共享相同的类别标签，如图[5](#S2.F5
    "图 5 ‣ 2.2.3 基于图的正则化 ‣ 2.2 SSL 算法分类 ‣ 2 半监督学习 (SSL) ‣ 半监督和无监督深度视觉学习：综述")所示，可以在图上传播标签。
- en: A graph-based regularization term is used in model optimization by imposing
    various forms of smoothness constraints to minimize the pairwise similarities
    between nearby data points. Graph-based regularization is later reformulated for
    semi-supervised learning with deep neural networks, such as EmbedNN [[44](#bib.bib44)],
    Graph Convolutional Network [[62](#bib.bib62), [104](#bib.bib104)], Teacher Graph [[61](#bib.bib61)],
    and Label Propagation [[63](#bib.bib63)]. Although this line of works share the
    same smoothness assumption for model optimization, graph-based regularization
    can be imposed differently in either the feature space or prediction space, detailed
    as follows.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型优化中使用基于图的正则化项，通过施加各种形式的平滑约束来最小化附近数据点之间的成对相似度。基于图的正则化随后被重新制定用于带有深度神经网络的半监督学习，例如
    EmbedNN [[44](#bib.bib44)]、图卷积网络 [[62](#bib.bib62), [104](#bib.bib104)]、教师图 [[61](#bib.bib61)]
    和标签传播 [[63](#bib.bib63)]。尽管这些工作在模型优化中共享相同的平滑假设，但基于图的正则化可以在特征空间或预测空间中以不同方式施加，具体如下。
- en: 'Graph-based feature regularization is typically done by building a learnable
    nearest neighbor (NN) graph that augments the original DNN to encode the affinity
    between data points in the feature space, as represented by EmbedCNN [[44](#bib.bib44)]
    and Teacher Graph [[61](#bib.bib61)]. Each node in the graph is encoded by the
    visual feature extracted from the intermediate network layer or the output from
    the last layer; while an affinity matrix $W_{ij}$ is computed to encode the pairwise
    similarities between all the nodes. To exploit unlabeled data, a graph-based regularization
    term can be formed as a metric learning loss, such as the margin-based contrastive
    loss for Siamese networks [[105](#bib.bib105), [106](#bib.bib106)] which constrains
    feature learning by enforcing the local smoothness:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的特征正则化通常通过构建一个可学习的最近邻（NN）图来完成，这个图增强了原始DNN，以编码特征空间中数据点之间的亲和性，如 EmbedCNN [[44](#bib.bib44)]
    和教师图 [[61](#bib.bib61)] 所示。图中的每个节点由从中间网络层提取的视觉特征或最后一层的输出编码；同时计算亲和矩阵 $W_{ij}$ 以编码所有节点之间的成对相似度。为了利用未标记数据，可以形成基于图的正则化项作为度量学习损失，例如用于孪生网络的基于边际的对比损失 [[105](#bib.bib105),
    [106](#bib.bib106)]，通过强制局部平滑来约束特征学习。
- en: '|  | $\underset{\theta}{\text{min}}\sum_{x_{i},x_{j}\in\mathcal{D}}\left\{\begin{aligned}
    &amp;\&#124;h(x_{i})-h(x_{j})\&#124;^{2},&amp;\text{if}\ W_{ij}{=}1\\ &amp;\text{max}(0,m-&#124;&#124;h(x_{i})-h(x_{j})&#124;&#124;)^{2},&amp;\text{if}\
    W_{ij}{=}0\end{aligned}\right.$ |  | (7) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\theta}{\text{min}}\sum_{x_{i},x_{j}\in\mathcal{D}}\left\{\begin{aligned}
    &amp;\&#124;h(x_{i})-h(x_{j})\&#124;^{2},&amp;\text{如果}\ W_{ij}{=}1\\ &amp;\text{max}(0,m-&#124;&#124;h(x_{i})-h(x_{j})&#124;&#124;)^{2},&amp;\text{如果}\
    W_{ij}{=}0\end{aligned}\right.$ |  | (7) |'
- en: ensuring that features $h(x_{i}),h(x_{j})$ of nearest neighbors (i.e., $W_{ij}{=}1$)
    are close to and dissimilar pairs (i.e., $W_{ij}{=}0$) are away from each other
    with a distance margin $m$.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 确保最近邻的特征 $h(x_{i}),h(x_{j})$（即 $W_{ij}{=}1$）彼此接近，而不相似的对（即 $W_{ij}{=}0$）则相距一定距离边际
    $m$。
- en: Beyond augmenting a DNN with a graph, a more flexible way is to use graph convolutions,
    i.e., Graph Convolutional Networks (GCN) [[62](#bib.bib62)], which derive new
    feature representations for each node subject to the graph structure [[104](#bib.bib104),
    [107](#bib.bib107)]. Specifically, a GCN takes the data and affinity matrix as
    input, and learns to estimate the class labels of unlabeled data under a supervised
    cross-entropy loss on labeled data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用图增强DNN之外，更灵活的方式是使用图卷积，即图卷积网络（GCN） [[62](#bib.bib62)]，它为每个节点推导新的特征表示，受限于图结构
    [[104](#bib.bib104), [107](#bib.bib107)]。具体来说，GCN 以数据和亲和矩阵作为输入，并学习在标记数据上通过监督交叉熵损失来估计未标记数据的类别标签。
- en: Graph-based prediction regularization operates in the prediction space [[63](#bib.bib63),
    [108](#bib.bib108)], as in Label Propagation [[63](#bib.bib63)]. Driven by the
    same rationale of building a learnable NN-graph as above, in label propagation,
    an NN-graph encidong the similarity between data points is used to propagate the
    labels from the labeled data to the unlabeled data based on transitivity via with
    a cross-entropy loss. While being similar to the approach Pseudo-Labels [[56](#bib.bib56)],
    the propagated labels are derived with an external NN-graph that encodes the global
    manifold structure. Further, label propagation on the graph and the update of
    DNN are performed alternatively to propagate more reliable labels.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Remarks. Graph-based regularization shares several similarities with consistency
    regularization and self-training in SSL. First, it introduces an unsupervised
    auxiliary task to train a DNN with propagated learning targets (e.g., pseudo labels)
    on the unlabeled data. Second, its learning objective can be formulated as a cross-entropy
    loss or metric learning loss. Notably, while consistency regularization and self-training
    are inductive approaches that estimate a learning target per instance, graph-based
    regularization methods are transductive approaches that propagate learning targets
    based on a graph constructed on the dataset. Beyond concrete details, however,
    the three techniques all share the same fundamental idea of seeking for unsupervised
    targets.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Deep Generative Models
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9a94fc29103e358ab6261342a0a83ad.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: In GAN-based deep generative models (§[2.2.4](#S2.SS2.SSS4 "2.2.4
    Deep Generative Models ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")), the
    discriminator assigns the labeled samples to the $K$ classes and the generated
    unlabeled data to an auxiliary class ($K+1$). At test time, the discriminator
    acts as the classifier.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep generative models are a class of unsupervised learning models that learn
    to approximate the data distributions without labels [[109](#bib.bib109), [110](#bib.bib110)].
    By integrating the generative unsupervised learning concept into a supervised
    model, a semi-supervised learning framework can be formulated to unify the merits
    of supervised and unsupervised learning. Two main streams of deep generative models
    are Variational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs),
    as detailed below. See Figure [6](#S2.F6 "Figure 6 ‣ 2.2.4 Deep Generative Models
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey") for an illustration of a GAN
    framework for SSL.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Variational auto-encoders (VAEs) are probabilistic models based on variational
    inference for unsupervised learning of a complex data distribution [[109](#bib.bib109),
    [111](#bib.bib111)]. A standard VAE model contains a network that encodes an input
    sample to a latent variable and a network that decodes the latent variable to
    reconstruct the input; maximizing a variational lower bound. In semi-supervised
    learning [[64](#bib.bib64), [65](#bib.bib65), [112](#bib.bib112)], an unsupervised
    VAE model is generally combined with a supervised classifier. For instance, to
    predict task-specific class information required in SSL, Class-conditional VAE [[64](#bib.bib64)]
    and ADGM [[65](#bib.bib65)] introduce the class label as an extra latent variable
    in the latent feature space to explicitly disentangle the class information (content)
    and the stochastic information (style), and impose an explicit classification
    loss on the labeled data along with the vanilla VAE loss.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks (GANs) [[110](#bib.bib110)] learn to capture
    the data distribution by an adversarial minimax game. Specifically, a generator
    is trained to generate as realistic images as possible while a discriminator is
    trained to discriminate between real and generated samples. When re-formulated
    as a semi-supervised representation learner, GANs can leverage the benefits of
    both unsupervised generative modeling and supervised discriminative learning [[66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68), [113](#bib.bib113), [114](#bib.bib114), [69](#bib.bib69),
    [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The generic idea is to augment the standard GAN framework with supervised learning
    on the labeled real samples (i.e., discriminative) and unsupervised learning on
    the generated samples. Formally, this enhances the original discriminator with
    an extra supervised learning capability. For example, Categorical GAN (CatGAN) [[66](#bib.bib66)]
    introduces a $K$-class discriminator, and minimizes a supervised cross-entropy
    loss on the real labeled samples, while imposing a uniform distribution constraint
    on the generated samples by maximizing the prediction’s entropy. Similarly, feature
    matching GAN (FM-GAN) [[67](#bib.bib67)], ALI [[68](#bib.bib68)], BadGAN [[69](#bib.bib69)]
    and Localized GAN [[70](#bib.bib70)] formulate a $(K{+}1)$-class discriminator
    for SSL, whereby a real labeled sample $x_{l}$ is considered as one of the $K$
    classes and a generated sample $x_{G}$ as the $(K+1)_{\text{th}}$ class. The supervised
    and unsupervised learning objective for the $(K{+}1)$-class discriminator is formulated
    as;
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\theta}{\text{max}}\ \sum_{x\in\mathcal{D}}\text{log}\
    p(y&#124;x_{l},y{<}K{+}1),$ |  | (8) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\underset{\theta}{\text{max}}\ \sum_{x\in\mathcal{D}}\text{log}\
    (1{-}p(y{=}K{+}1&#124;x_{l}))-\text{log}\ p(y{=}K{+}1&#124;x_{G}),$ |  | (9) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: 'where Eq. ([8](#S2.E8 "In 2.2.4 Deep Generative Models ‣ 2.2 Taxonomy on SSL
    Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")) is the supervised classification loss on the
    labeled samples $x_{l}$; Eq. ([9](#S2.E9 "In 2.2.4 Deep Generative Models ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) is an unsupervised GAN loss
    that discriminates between the real labeled samples $x_{l}$ and the generated
    fake samples $x_{G}$ from the image generator. To constrain the generated samples,
    Localized GAN [[70](#bib.bib70)] introduces a regularizer on the generator to
    ensure the generated samples lie in the neighborhood of an original sample on
    the manifold, thus allowing to train a locally consistent classifier based on
    the generated samples in a semi-supervised fashion.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Remarks. Unlike previously discussed discriminative SSL techniques DGMs can
    naturally learn from unlabeled data without the need to estimate their labels.
    In other words, DGMs are native unsupervised representation learners. To enable
    SSL in DGMs, the key in model reformulation is thus to integrate the label supervision
    into training, e.g., adding a class label latent variable in VAEs or an extra
    class discriminator in GANs. Further, one also needs to tackle more difficult
    model optimization in a GAN framework.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 Self-Supervised Learning
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Self-supervised learning is a class of unsupervised representation learners
    designed based on unsupervised surrogate (pretext) tasks [[119](#bib.bib119),
    [120](#bib.bib120), [11](#bib.bib11), [121](#bib.bib121), [122](#bib.bib122),
    [123](#bib.bib123)]. Self-supervision differs from self-training algorithms in
    §[2.2.2](#S2.SS2.SSS2 "2.2.2 Self-Training ‣ 2.2 Taxonomy on SSL Algorithms ‣
    2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey"), as self-supervised learning objectives are task-agnostic
    and could be trained without any label supervision. The former is originally proposed
    to learn from only unlabeled data with task-agnostic unsupervised learning objectives,
    but it is also explored for SSL [[71](#bib.bib71), [72](#bib.bib72), [12](#bib.bib12)].
    In SSL, task-agnostic self-supervision signals on all training data are often
    integrated with a supervised learning objective on labeled data. For instance,
    S4L [[71](#bib.bib71)] uses self-supervision for SSL based on multiple self-supervision
    signals such as predicting rotation degree [[123](#bib.bib123)] and enforcing
    invariance to exemplar transformation [[119](#bib.bib119)] to train the model
    along with supervised learning. SimCLR [[12](#bib.bib12)] and SimCLRv2 [[72](#bib.bib72)]
    are follow-up works introducing self-supervised contrastive learning for task-agnostic
    unsupervised pre-training, followed by supervised or semi-supervised fine-tuning
    with label supervision as the downstream task.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks. A unique advantage of self-supervision for SSL is that task-specific
    label supervision is not required during training. While the aforementioned semi-supervised
    learners typically solve a supervised task and an auxiliary unsupervised task
    jointly, self-supervised semi-supervised learners can be trained in a fully task-agnostic
    fashion. This suggests the great flexibility of self-supervision for SSL. Thus,
    the self-supervised training can be introduced as unsupervised pre-training or
    as an auxiliary unsupervised task solved along with supervised learning. Although
    self-supervision is relatively new for SSL, it has been more widely explored for
    unsupervised learning, which is detailed more extensively in §[3.2.1](#S3.SS2.SSS1
    "3.2.1 Pretext Tasks ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning
    (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey") and §[3.2.2](#S3.SS2.SSS2
    "3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised
    Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey").'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: A taxonomy of unsupervised deep learning methods, including three
    representative families in §[3.2.1](#S3.SS2.SSS1 "3.2.1 Pretext Tasks ‣ 3.2 Taxonomy
    on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey") – §[3.2.3](#S3.SS2.SSS3 "3.2.3 Deep Generative
    Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '| Families of Models | Model Rationale | Representative Strategies and Methods
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| Pretext tasks | Pixel-level | reconstruction [[124](#bib.bib124), [125](#bib.bib125)],
    inpainting [[126](#bib.bib126)], MAE [[127](#bib.bib127)], denoising [[128](#bib.bib128)],
    colorization [[129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131)] |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Instance-level | predict image rotations [[123](#bib.bib123)], scaling and
    tiling [[122](#bib.bib122)], patch ordering [[11](#bib.bib11)], patch re-ordering [[121](#bib.bib121)]
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| Discriminative models | Instance discrimination | negative sampling | large
    batch size (SimLR [[12](#bib.bib12)]), memory bank (InstDis [[132](#bib.bib132)]),
    queue (MoCo [[16](#bib.bib16)]) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| input transformation | data augmentation (PIRL [[133](#bib.bib133)]), multi-view
    augmentation (CMC [[134](#bib.bib134)]) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| negative-sample-free | simple siamese (SimSiam [[135](#bib.bib135)]), Bootstrap
    (BYOL [[136](#bib.bib136)]), DirectPred [[137](#bib.bib137)] |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| Deep clustering | offline clustering | DeepCluster [[138](#bib.bib138)],
    JULE [[139](#bib.bib139)], SeLa [[140](#bib.bib140)] |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| online clustering | IIC [[141](#bib.bib141)], PICA [[142](#bib.bib142)],
    AssociativeCluster [[143](#bib.bib143)], SwAV [[144](#bib.bib144)] |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| Deep generative models | Discriminator-level | DCGAN [[145](#bib.bib145)],
    Self-supervised GAN [[146](#bib.bib146)], Transformation GAN [[147](#bib.bib147)]
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| Generator-level | BiGAN [[148](#bib.bib148)], BigBiGAN [[149](#bib.bib149)]
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 生成器级别 | BiGAN [[148](#bib.bib148)]，BigBiGAN [[149](#bib.bib149)] |'
- en: 3 Unsupervised Learning (UL)
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 无监督学习（UL）
- en: Unsupervised Learning (UL) aims to learn representations without utilizing any
    label supervision. The learned representation is not only expected to capture
    the underlying semantic information, but also be transferable to tackle unseen
    downstream tasks such as visual recognition, detection, and segmentation [[16](#bib.bib16)],
    visual retrieval [[150](#bib.bib150)], and tracking [[151](#bib.bib151)].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习（UL）旨在在不利用任何标签监督的情况下学习表示。学到的表示不仅期望捕捉到潜在的语义信息，还应能转移到应对未见的下游任务，如视觉识别、检测和分割 [[16](#bib.bib16)]、视觉检索 [[150](#bib.bib150)]
    和跟踪 [[151](#bib.bib151)]。
- en: UL is attractive in computer vision for multiple reasons. First, due to costly
    label annotations, large labeled datasets may not be available in many application
    scenarios, e.g., medical imaging [[152](#bib.bib152)]. Second, as there are often
    data/label distribution drifts (or gaps) across tasks and application scenarios,
    pre-training on a large labeled dataset cannot always guarantee good model initialization
    for unseen situations [[153](#bib.bib153)]. Third, UL could supply strong pre-trained
    models that may perform on par with or even outperform supervised pre-training [[16](#bib.bib16),
    [12](#bib.bib12), [154](#bib.bib154)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: UL 在计算机视觉中因多种原因而具有吸引力。首先，由于标签注释成本高昂，在许多应用场景中可能没有大规模标注数据集，例如医学成像 [[152](#bib.bib152)]。其次，由于任务和应用场景中经常存在数据/标签分布漂移（或差距），在大型标注数据集上进行的预训练无法总是保证未见情况的良好模型初始化 [[153](#bib.bib153)]。第三，UL
    可以提供强大的预训练模型，这些模型可能与监督预训练相当，甚至超越其表现 [[16](#bib.bib16), [12](#bib.bib12), [154](#bib.bib154)]。
- en: Remarks. UL and SSL share the same aim to learn from unlabeled data, and leverage
    similar modeling principles to formulate unsupervised surrogate supervision signals
    without any label annotation. However, instead of assuming the availability of
    task-specific information (i.e., class labels) as in SSL, UL considers model learning
    from purely task-agnostic unlabeled data. Given that unlabeled data are abundantly
    available in different scenarios (e.g., Internet), UL offers an appealing strategy
    to provide good pre-trained models that could facilitate various downstream tasks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。UL 和 SSL 共享从未标记数据中学习的相同目标，并利用类似的建模原理来制定无监督的替代监督信号，而无需任何标签注释。然而，与 SSL 假定任务特定信息（即类别标签）可用不同，UL
    考虑从纯粹任务无关的未标记数据中进行模型学习。鉴于不同场景（如互联网）中未标记数据的丰富性，UL 提供了一种有吸引力的策略来提供良好的预训练模型，从而促进各种下游任务。
- en: 'Focusing on unsupervised visual learners trained on image classification datasets,
    we define the UL problem setup in §[3.1](#S3.SS1 "3.1 The Problem Setting of UL
    ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey"), and provide a taxonomy and analysis of the existing representative
    unsupervised deep learning methods in §[3.2](#S3.SS2 "3.2 Taxonomy on UL Algorithms
    ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 专注于在图像分类数据集上训练的无监督视觉学习者，我们在 §[3.1](#S3.SS1 "3.1 UL 的问题设置 ‣ 3 无监督学习（UL） ‣ 半监督和无监督深度视觉学习：综述")
    中定义 UL 问题设置，并在 §[3.2](#S3.SS2 "3.2 UL 算法的分类 ‣ 3 无监督学习（UL） ‣ 半监督和无监督深度视觉学习：综述")
    中提供现有代表性无监督深度学习方法的分类和分析。
- en: 3.1 The Problem Setting of UL
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 UL 的问题设置
- en: 'Problem Definition. In UL, we have access to an unlabeled dataset $\mathcal{D}_{u}=\{\textbf{x}_{i}\}_{i=1}^{N_{u}}$.
    As label information is unknown, the UL loss function $\mathcal{L}$ for training
    a DNN $\theta$ can generally be expressed as Eq. ([1](#S1.E1 "In 1 Introduction
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")), i.e., $\mathcal{L}=\lambda_{l}\mathcal{L}_{\text{sup}}+\lambda_{u}\mathcal{L}_{\text{unsup}}$
    with $\lambda_{l}=0$. In discriminative models, the unsupervised objective $\mathcal{L}_{\text{unsup}}$
    requires certain pseudo/proxy targets to learn semantically meaningful and generalizable
    representations. In generative models, $\mathcal{L}_{\text{unsup}}$ is imposed
    to explicitly model the data distribution. See Figure [7](#S3.F7 "Figure 7 ‣ 3.1
    The Problem Setting of UL ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and
    Unsupervised Deep Visual Learning: A Survey") for an illustration of UL.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/67cbebae6057aca1423938fd7348489e.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Unsupervised learning trains a generalizable model using purely unlabeled
    data. The model can later be fine-tuned with labeled data and tested on a downstream
    task.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Protocol. The performance of UL methods are often evaluated via two
    protocols, commonly known as the (1) linear classification protocol, and (2) fine-tuning
    on downstream tasks. In (1), the pre-trained DNN is frozen to extract the features
    for an image dataset, while a linear classifier (e.g., a fully-connected layer
    or a kNN classifier) is trained to classify the extracted features. In (2), the
    pre-trained DNN is used to initialize a model for any downstream task, followed
    by fine-tuning with a task-specific objective, such as fine-tuning an object detector
    initialized from an unsupervised pre-trained backbone (e.g., FasterR-CNN [[155](#bib.bib155)])
    on object detection datasets (e.g., PASCAL VOC [[156](#bib.bib156)]), or fine-tuning
    a segmentation model (e.g., Mask R-CNN [[157](#bib.bib157)]) with a pre-trained
    backbone on segmentation datasets (e.g., COCO [[158](#bib.bib158)]).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Taxonomy on UL Algorithms
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Existing unsupervised deep learning models can be mainly grouped into three
    families: pretext tasks, discriminative models and generative models (Table [II](#S2.T2
    "TABLE II ‣ 2.2.5 Self-Supervised Learning ‣ 2.2 Taxonomy on SSL Algorithms ‣
    2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")). Pretext tasks and discriminative models are also known
    as self-supervised learning, which drive model learning by a proxy protocol/task
    and construct pseudo label supervision to formulate unsupervised surrogate losses.
    Generative models is inherently unsupervised and explicitly models the data distribution
    to learn representations without label supervision. We review these models in
    §[3.2.1](#S3.SS2.SSS1 "3.2.1 Pretext Tasks ‣ 3.2 Taxonomy on UL Algorithms ‣ 3
    Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey"), §[3.2.2](#S3.SS2.SSS2 "3.2.2 Discriminative Models ‣ 3.2 Taxonomy
    on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey") and §[3.2.3](#S3.SS2.SSS3 "3.2.3 Deep Generative
    Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey").'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Pretext Tasks
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pretext Tasks refer to hand-crafted proxy tasks manually designed to predict
    certain task-agnostic properties of the input data, which do not require any label
    supervision for training. By formulating self-supervised learning objectives with
    free labels, meaningful visual representations can be learned in a fully unsupervised
    manner. In the following, we review two classes of pretext tasks, which introduce
    the self-supervision signals at the pixel-level (illustrated in Figure [8](#S3.F8
    "Figure 8 ‣ 3.2.1 Pretext Tasks ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised
    Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))
    or instance-level (illustrated in Figure [9](#S3.F9 "Figure 9 ‣ 3.2.1 Pretext
    Tasks ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8a3145f9632d65f65cd916b848b0b8b.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: In pixel-level pretext tasks (§[3.2.1](#S3.SS2.SSS1 "3.2.1 Pretext
    Tasks ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")), the aim is to reconstruct
    the original image $\hat{x}$ from a corrupted input $x$.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Pixel-level pretext task is generally designed as a dense prediction task that
    aims to predict the expected pixel values of an output image as a self-supervision
    signal [[124](#bib.bib124), [128](#bib.bib128), [125](#bib.bib125), [126](#bib.bib126),
    [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131), [127](#bib.bib127)].
    Auto-Encoder [[124](#bib.bib124), [125](#bib.bib125)] is one of the most representative
    and primitive unsupervised models that learn representations by reconstructing
    input images. In addition to standard reconstruction, pixel-level pretext tasks
    introduce more advanced image generation tasks to hallucinate the pixel colour
    values of the corrupted input images, as represented by three standard low-level
    image processing tasks: (1) image inpainting [[126](#bib.bib126), [127](#bib.bib127)]
    learns by inpainting the masked-out missing regions in the input images, which
    is also known as masked auto-encoders (MAE) [[127](#bib.bib127)]; (2) denoising [[128](#bib.bib128)]
    learns to denoise the partial destructed input; and (3) colorization [[129](#bib.bib129),
    [130](#bib.bib130), [131](#bib.bib131)] aims to predict the colour values of the
    grayscale images. These self-supervised models are trained with an image generation
    task objective (e.g., a mean square error) to enforce predicting the expected
    pixel values:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{{\theta}}{\text{min}}\ \sum_{x\in\mathcal{D}}&#124;&#124;G_{\theta}(x)-\hat{x}&#124;&#124;^{2},$
    |  | (10) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: where $G_{\theta}(\cdot)$ is an image generation network (typically implemented
    as an encoder-decoder network architecture) trained to predict the expected output
    image $\hat{x}$ per pixel. Once trained, part of the network $G_{\theta}(\cdot)$
    (e.g., encoder) can be used to initialize the model weights or extract the intermediate
    features for solving the downstream task.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dbe9f314f45d895370ca1a938c239a33.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: In instance-level pretext tasks (§[3.2.1](#S3.SS2.SSS1 "3.2.1 Pretext
    Tasks ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) the aim is to predict the transformation
    on the input.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance-level pretext tasks introduce sparse semantic labels for each image
    sample by designing a surrogate proxy task that can be solved per instance without
    any label annotations [[11](#bib.bib11), [121](#bib.bib121), [122](#bib.bib122),
    [159](#bib.bib159), [123](#bib.bib123), [160](#bib.bib160), [161](#bib.bib161),
    [162](#bib.bib162)]. In general, these pretext tasks involve applying different
    image transformations to generate diverse input variations, whereby an artificial
    supervision signal is imposed to predict the applied transformation on each image
    instance. Among this line of works, the representative ones consider mainly two
    classes of instance-wise transformations on input images. The first one is classifying
    global transformations, such as rotations [[123](#bib.bib123)], scaling and tiling [[122](#bib.bib122)],
    where the learning objective is to recognize the geometric transformation applied
    on an image. The second one is predicting local transformations, such as patch
    orderings [[11](#bib.bib11)] and patch re-orderings [[121](#bib.bib121), [159](#bib.bib159),
    [161](#bib.bib161)], which cut each image into multiple local patches. The goal
    of patch orderings is to recognize the order of a given cut-out patch, while patch
    re-orderings, also known as the jigsaw puzzles, permute the cut-out patches randomly
    and the goal is to predict the permuted configurations. In summary, the objective
    of an instance-level pretext task can be written as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{{\theta}}{\text{min}}\ \sum_{{x}\in\mathcal{D}}\mathcal{L}_{\text{unsup}}(\Phi_{z}({x}),z,\theta),$
    |  | (11) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{\text{unsup}}(\cdot)$ can be various loss functions (e.g.,
    cross-entropy loss [[123](#bib.bib123)]) that learn a mapping from a transformed
    input image $\Phi_{z}({x})$ to a discrete category or a configuration of the applied
    transformation $z$. Once trained, the representations are covariant with the transformations
    $\Phi_{z}(\cdot)$, thus being aware of the spatial context information, e.g.,
    how an image is rotated or how the local patches are permuted.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Remarks. Although self-supervised learning objectives of pixel-level or instance-level
    pretext tasks are generally not explicitly related to the downstream task objectives
    (e.g., image classification, detection and segmentation), they permit to learn
    from unlabeled data by predicting the spatial context or structured correlation
    in images, such as inpainting missing regions, and predicting the applied rotations.
    As these self-supervision signals can implicitly uncover the semantic content
    (e.g. human interpretable concepts [[163](#bib.bib163)]) or spatial context in
    images, they often yield a meaningful pre-trained model for initialization in
    unseen downstream tasks, or even serve as a flexible and effective regularizer
    to facilitate other machine learning setups, such as semi-supervised learning [[71](#bib.bib71)]
    and domain generalization [[164](#bib.bib164)].
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Discriminative Models
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Discriminative models hereby refer to the class of unsupervised discriminative
    models that learn visual representations from the unlabeled data by enforcing
    invariance towards various task-irrelevant visual variations at either instance-level,
    neighbor-level or group-level. These visual variations can be intra-instance variations
    such as different views of the same instance [[165](#bib.bib165), [134](#bib.bib134),
    [166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)], or inter-instance
    variations between neighbor instances [[169](#bib.bib169), [170](#bib.bib170)]
    or across a group of instances [[138](#bib.bib138), [171](#bib.bib171), [144](#bib.bib144)].
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following, we review two representative classes of unsupervised discriminative
    models that offer the state of the art in unsupervised visual feature learning,
    including instance discrimination (see Figure [10](#S3.F10 "Figure 10 ‣ 3.2.2
    Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning
    (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")) and
    deep clustering (see Figure [11](#S3.F11 "Figure 11 ‣ 3.2.2 Discriminative Models
    ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")). The former imposes self-supervision
    per instance by treating each instance as a class, while the latter introduces
    supervision per group by considering a group of similar instances as a class.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e22658e0b1ecfc2ae8b691a33942242c.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The unsupervised discriminative model using contrastive learning
    (§[3.2.2](#S3.SS2.SSS2 "3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms
    ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")) aims to pull together the positive pairs and push away the
    negative ones.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Instance discrimination models learn discriminative representations by enforcing
    invariance towards different viewing conditions, data augmentations or various
    parts of the same image instance [[119](#bib.bib119), [120](#bib.bib120), [172](#bib.bib172),
    [165](#bib.bib165), [132](#bib.bib132), [167](#bib.bib167), [166](#bib.bib166),
    [173](#bib.bib173), [133](#bib.bib133), [16](#bib.bib16), [12](#bib.bib12), [134](#bib.bib134),
    [168](#bib.bib168), [174](#bib.bib174), [72](#bib.bib72)] – also known as exemplar
    learning [[119](#bib.bib119), [120](#bib.bib120)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The most prevalent scheme in instance discrimination is contrastive learning,
    which was initially proposed to learn invariant representations by mapping similar
    inputs to nearby points in the latent space [[106](#bib.bib106), [105](#bib.bib105)].
    The state-of-the-art contrastive learning models for self-supervised learning
    generally aim to obtain an invariance property by optimizing a contrastive loss
    formulated upon the noise contrastive estimation (NCE) principle [[175](#bib.bib175)],
    which maximizes the mutual information across different views. The multi-view
    information bottleneck model [[176](#bib.bib176)] extends the original information
    bottleneck principle to unsupervised learning and trains an encoder to retain
    all the relevant information for predicting the label while minimizing the excess
    information in the representation. Formally, contrastive learners such as SimLR [[12](#bib.bib12)]
    and MoCo [[16](#bib.bib16)] are generally optimized by an instance-wise contrastive
    loss (i.e., infoNCE loss) [[106](#bib.bib106), [177](#bib.bib177)]:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{{\theta}}{\text{min}}\ \sum_{{x_{i}}\in\mathcal{D}}-\text{log}\
    \frac{\text{exp}(f_{\theta}(x_{i})\cdot f_{\theta}({x}^{+}_{i})/\tau)}{\sum_{j=1}^{M}\text{exp}(f_{\theta}(x_{i})\cdot
    f_{\theta}({x}_{j})/\tau)},$ |  | (12) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: 'where $\tau$ is a temperature, $f_{\theta}$ is the feature encoder, i.e., a
    DNN; $f_{\theta}(x_{i}),f_{\theta}({x}^{+}_{i})$ are the feature embeddings of
    two different augmentations, or views of the same image; $\{x_{j}\}_{j=1}^{M}$
    includes $(M{-}1)$ negative samples and $1$ positive (i.e., $x_{i}^{+}$) sample.
    Eq. ([12](#S3.E12 "In 3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms
    ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")) optimizes the network by enforcing the positive pairs (i.e.,
    embeddings of the same instance) to lie closer, while pushing apart the negative
    pairs (i.e., embeddings of different instances). Minimizing the InfoNCE loss is
    equivalent to maximizing a lower bound on the mutual information between $f_{\theta}(x_{i})$
    and $f_{\theta}({x}^{+}_{i})$ [[165](#bib.bib165)].'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'To derive a tractable yet meaningful contrastive distribution in Eq. ([12](#S3.E12
    "In 3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised
    Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")),
    a large amount of negative pairs are often required per training batch. To this
    aim, existing state-of-the-art methods are typically featured with different negative
    sampling strategies to collect more negative pairs. For instance, a large batch
    size of 4096 is adopted in SimCLR [[12](#bib.bib12)]. In InstDis [[132](#bib.bib132)],
    MoCo [[16](#bib.bib16)], PIRL [[133](#bib.bib133)], and CMC [[134](#bib.bib134)],
    a memory bank is used to maintain all the instance prototypes by keeping moving
    average of their feature representations over training iterations. Finally, running
    queue enqueues the features of samples in the latest batches and dequeues the
    old mini-batches of samples to store a fraction of sample’s features from the
    preceding mini-batches [[16](#bib.bib16), [174](#bib.bib174), [133](#bib.bib133)].'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by deep metric learning, various training strategies further boost
    contrastive learning. For instance, a hard negative sampling strategy [[178](#bib.bib178)]
    mines the negative pairs that are similar to the samples but likely belong to
    different classes. To train negative pairs and (or) positive pairs by adversarial
    training [[179](#bib.bib179), [180](#bib.bib180)] learn a set of “adversarial
    negatives” confused with the given samples, or “cooperative positives” similar
    to the given samples. These strategies are designed to find the better negative
    and positive pairs for improving contrastive learning.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: In addition to negative sampling, it is essential to apply various image transformations
    for generating multiple diverse variants (i.e., views) of the same instance to
    construct the positive pairs. The most typical way is to apply common data augmentation
    such as random cropping and color jittering [[132](#bib.bib132), [166](#bib.bib166),
    [173](#bib.bib173), [16](#bib.bib16), [133](#bib.bib133), [12](#bib.bib12)], or
    pretext transformation [[133](#bib.bib133)] like patch re-ordering [[121](#bib.bib121)]
    and rotation [[123](#bib.bib123)]. An alternative way is to artificially construct
    multiple views of a single image by using different image channels like luminance
    and chrominance [[134](#bib.bib134)], or by extracting the local and global patches
    of the same image [[165](#bib.bib165)]. In a nutshell, although there are different
    strategies in negative sampling and image transformations to construct the negative
    and positive pairs for contrastive learning, these strategies share the same aim
    to learn visual representations invariant to diverse input transformations [[172](#bib.bib172),
    [133](#bib.bib133)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'While contrastive learning approaches rely on obtaining a sufficient amount
    of negative pairs to derive the contrastive loss (Eq. ([12](#S3.E12 "In 3.2.2
    Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning
    (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))), another
    alternative non-contrastive scheme for instance discrimintation operates in a
    negative-sample-free manner [[136](#bib.bib136), [135](#bib.bib135), [137](#bib.bib137),
    [181](#bib.bib181)], as exemplified by bootstrap (BYOL) [[136](#bib.bib136)] and
    simple siamese networks (SimSiam) [[135](#bib.bib135)]). In particular, in BYOL
    and SimSiam, two views (obtained from data augmentation) of the same images are
    passed towards the networks and the mean squared error is minimized between the
    representations of two views to enforce invariances. Importantly, a stop gradient
    scheme is adopted to prevent representational collapse, i.e. avoid mapping all
    the samples to the same representations. Another related method is Barlow Twins [[181](#bib.bib181)],
    which computes a cross-correlation matrix between the distorted versions of a
    batch of training samples and enforce the matrix to be an identity matrix, thus
    learning self-supervised representations invariant to different distortions. Although
    these non-contrastive methods adopt other loss formulations, they all share the
    similar spirit as contrastive learning given that meaningful representations are
    learned by enforcing invariances to different views of the same instance.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6240b547c69a394ba34ae749d601a218.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: In unsupervised discriminative models using deep clustering (§[3.2.2](#S3.SS2.SSS2
    "3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised
    Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")),
    unlabeled samples are assigned to a set of clusters by online or offline clustering,
    while the cluster memberships are utilized as pseudo labels for training.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep clustering models learn discriminative representations by grouping similar
    instances from the same cluster together [[182](#bib.bib182), [139](#bib.bib139),
    [183](#bib.bib183), [138](#bib.bib138), [184](#bib.bib184), [171](#bib.bib171),
    [140](#bib.bib140), [142](#bib.bib142), [185](#bib.bib185), [186](#bib.bib186),
    [144](#bib.bib144), [187](#bib.bib187), [170](#bib.bib170)]. In training, the
    entire dataset is generally divided into groups by associating each instance to
    a certain cluster centroid based on pairwise similarities. Although clustering
    algorithms are longstanding machine learning techniques  [[188](#bib.bib188),
    [189](#bib.bib189), [190](#bib.bib190)], they have been re-designed to be seamlessly
    integrated with DNNs to learn discriminative representations without label supervision.
    Conceptually, the cluster memberships can be considered as some pseudo labels
    to supervise the model training, as written in Eq. ([13](#S3.E13 "In 3.2.2 Discriminative
    Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{{\theta}}{\text{min}}\ \sum_{x\in\mathcal{D}}\mathcal{L}_{\text{unsup}}(x,\hat{y},\theta),$
    |  | (13) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: where $\hat{y}$ is the cluster membership of sample $x$, $\mathcal{L}_{\text{unsup}}(\cdot,\cdot,\theta)$
    is the loss function that constrains the mapping from $x$ to $y$, such as a classification
    loss. Deep clustering algorithms can be further grouped into two categories according
    to whether the assignments of cluster memberships are derived in an offline or
    online manner, as detailed in the following.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: In offline clustering, unsupervised training is alternated between a cluster
    assignment step and a network training step [[182](#bib.bib182), [139](#bib.bib139),
    [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193), [140](#bib.bib140),
    [171](#bib.bib171), [170](#bib.bib170)]. While the former step estimates the cluster
    memberships of all the training samples, the latter uses the assigned cluster
    memberships as pseudo labels to train the network. Representative offline clustering
    models include DeepCluster [[138](#bib.bib138)], JULE [[139](#bib.bib139)] and
    SeLa [[140](#bib.bib140)], which mainly differ in the clustering algorithms. Specifically,
    DeepCluster [[138](#bib.bib138), [171](#bib.bib171)] groups visual features using
    k-means clustering [[189](#bib.bib189)]. JULE [[139](#bib.bib139)] uses agglomerative
    clustering [[194](#bib.bib194)] that merges similar clusters to iteratively derive
    new cluster memberships. SeLa [[140](#bib.bib140)] casts clustering as an optimal
    transport problem solved by Sinkhorn-Knopp algorithm [[195](#bib.bib195)] to obtain
    the cluster memberships as pseudo labels.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'In online clustering, the cluster assignment step and network training step
    are coupled in an end-to-end training framework, as represented by IIC [[141](#bib.bib141)],
    AssociativeCluster [[143](#bib.bib143)], PICA [[142](#bib.bib142)], and SwAV [[144](#bib.bib144)].
    Compared to offline clustering, online clustering could better scale to large-scale
    datasets, as it does not require clustering the entire dataset iteratively. This
    is typically achieved in two ways: (1) training a classifier that parameterizes
    the cluster memberships (e.g., IIC and PICA); (2) learning a set of cluster centroids/prototypes
    (e.g., AssociativeCluster and SwAV). For instance, IIC [[141](#bib.bib141)] learns
    the cluster memberships by maximizing the mutual information between predictions
    of an original instance and a randomly perturbed instance obtained from data augmentation.
    SwAV [[144](#bib.bib144)] learns a set of prototypes (i.e., cluster centroids)
    in the feature space and assigns each sample to the closest prototype.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks. Recent advances of discriminative unsupervised models include both
    contrastive learning and deep clustering, which have set the new state of the
    art. On one side, contrastive learning discriminates individual instances by imposing
    transformation invariance at the instance-level. Interestingly, this opposes some
    instance-level pretext tasks that instead learn by predicting the applied transformations.
    Contrastive learning also closely relates to consistency regularization in SSL
    in the sense of enforcing invariance to transformations, although different loss
    functions are often used. However, as shown in [[135](#bib.bib135)], a pairwise
    loss objective – often used for consistency regularization in SSL – can be also
    effective as contrastive loss (Eq. ([12](#S3.E12 "In 3.2.2 Discriminative Models
    ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"))). This suggests that the essential
    idea behind them is identical – imposing transformation invariance at instance
    level. Deep clustering, on the other hand, discriminates between groups of instances
    for discovering the underlying semantic boundaries, and enforces group-level invariance.
    The idea of consistency regularization is also adopted by several deep clustering
    methods [[142](#bib.bib142), [141](#bib.bib141)], conforming its more generic
    efficacy beyond SSL. Lastly, discriminative unsupervised learning can also be
    conducted at both instance-level and group-level to learn more powerful representations
    [[186](#bib.bib186), [196](#bib.bib196)].'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2148d3a5c52962ff836bc32eea83f8f2.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: In GANs (§[3.2.3](#S3.SS2.SSS3 "3.2.3 Deep Generative Models ‣ 3.2
    Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and
    Unsupervised Deep Visual Learning: A Survey")), a generator and a discriminator
    are trained with a minimax game (Eq. ([14](#S3.E14 "In 3.2.3 Deep Generative Models
    ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"))) in an unsupervised manner,
    whilst their intermediate features lead to discriminative visual representations.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Deep Generative Models
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep generative models (DGMs), as introduced in §[2.2.4](#S2.SS2.SSS4 "2.2.4
    Deep Generative Models ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"), are
    inherent unsupervised learners that explicitly model the data distribution [[197](#bib.bib197),
    [109](#bib.bib109), [110](#bib.bib110), [198](#bib.bib198)]. DGMs are applicable
    for both semi-supervised and unsupervised learning. A typical Generative Adversarial
    Network (GAN) [[66](#bib.bib66), [199](#bib.bib199), [149](#bib.bib149), [146](#bib.bib146),
    [147](#bib.bib147)] contains a discriminator $D$ to differentiate real and fake
    samples, and a generator $G$ that can serve as an image encoder to capture the
    semantics in latent space, as trained by a min-max game:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{G}{\text{min}}\ \underset{D}{\text{max}}\ \mathds{E}_{x\sim
    p_{\text{data}}(x)}[\text{log}D(x)]{+}\mathds{E}_{z\sim p_{z}(z)}[\text{log}(1{-}D(G(z)))],$
    |  | (14) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: 'where $z$ is sampled from an input noise distribution $p_{z}(z)$. GANs can
    learn representations at both the discriminator and the generator level. See Figure
    [12](#S3.F12 "Figure 12 ‣ 3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms
    ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey") for an illustration of deep generative model based on a GAN.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: To learn representations at the discriminator-level, Deep Convolutional Generative
    Adversarial Network (DCGAN) [[145](#bib.bib145)] adopts a pre-trained convolutional
    discriminator to extract features for tackling a downstream image classification
    task. Later on, Self-supervised GAN [[146](#bib.bib146)] and Transformation GAN [[147](#bib.bib147)]
    further imbue the discriminator with a self-supervised pretext task to predict
    the applied image transformation, thus enabling the representations to capture
    the latent visual structures.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: To learn representations at the generator-level, Bidirectional Generative Adversarial
    Networks (BiGAN) [[199](#bib.bib199)] introduces an image encoder coupled with
    the generator, which is trained with a joint discriminator loss to tie the data
    distribution and the latent feature distribution together. This allows the image
    encoder to capture the semantic variations in its latent representation, and offer
    discriminative visual representations for one nearest neighbor (1NN) classification.
    To further improve BiGAN, BigBiGAN [[149](#bib.bib149)] adopts more powerful discriminator
    and generator architectures than BigGAN [[148](#bib.bib148)], together with an
    additional unary discriminator loss to constrain the data or latent distribution
    independently, therefore enabling more expressive unsupervised representation
    learning at the generator-level.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Remarks. Although most state-of-the-art UL methods are self-supervised models
    that solve pretext tasks or perform unsupervised discriminative learning (as reviewed
    in §[3.2.1](#S3.SS2.SSS1 "3.2.1 Pretext Tasks ‣ 3.2 Taxonomy on UL Algorithms
    ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey") and §[3.2.2](#S3.SS2.SSS2 "3.2.2 Discriminative Models ‣
    3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")), deep generative models are
    still an important class of unsupervised learners owing to their native unsupervised
    nature to learn expressive data representations in a probabilistic manner. Further,
    they do not require manual design of a meaningful discriminative learning objective,
    while offering a unique ability to generate abundant data.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 4 Discussion on SSL and UL
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we connect SSL and UL via further discussion on their common
    learning assumptions (§[4.1](#S4.SS1 "4.1 The learning assumptions shared by SSL
    and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")), and their applications in different computer vision tasks
    (§[4.2](#S4.SS2 "4.2 Applied SSL and UL in Visual Recognition ‣ 4 Discussion on
    SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 The learning assumptions shared by SSL and UL
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0459eee95b05414965c1248428f1c8dc.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: SSL and UL share (a) local and (b) global smoothness assumptions.
    Unlabeled samples (grey dots) are assigned to class labels depending on the decision
    boundaries derived from the local or global smoothness assumptions.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in §[2.1](#S2.SS1 "2.1 The Problem Setting of SSL ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"),
    the unsupervised learning objectives in SSL are often formulated based on the
    smoothness assumption [[42](#bib.bib42)]. Broadly speaking, the learning assumptions
    of various discriminative SSL and UL algorithms can be grouped into two types
    of smoothness assumptions, i.e. local smoothness and global smoothness – as visually
    illustrated in Figure [13](#S4.F13 "Figure 13 ‣ 4.1 The learning assumptions shared
    by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey"). In the following, we further elaborate these
    assumptions and discuss the different SSL and UL algorithms that are built upon
    these assumptions.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Local Smoothness
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two flavors of local smoothness assumption. First, a sample $x_{i}$
    is assumed to share the same class label as its transformed variant $\hat{x}_{i}$
    (Eq. ([15](#S4.E15 "In 4.1.1 Local Smoothness ‣ 4.1 The learning assumptions shared
    by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey"))). Second, a sample $x_{i}$ is assumed to belong
    to the same class as its nearby sample $x_{j}$ in the latent representation space
    (Eq. ([16](#S4.E16 "In 4.1.1 Local Smoothness ‣ 4.1 The learning assumptions shared
    by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey"))). An unsupervised loss term enforces local smoothness
    on an unlabeled sample $x_{i}$ via:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{{\theta}}{\text{min}}\ \sum_{{x_{i}}\in\mathcal{D}}\mathcal{L}_{\text{unsup}}(f({x_{i}}),f(\hat{x}_{i}))$
    |  | (15) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '|  | $\underset{{\theta}}{\text{min}}\ \sum_{{x_{i}}\in\mathcal{D}}\mathcal{L}_{\text{unsup}}(f({x_{i}}),f(x_{j}))$
    |  | (16) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: where $f(\cdot)$ is the model to be trained and gives the model output (such
    as features or predictions). $\mathcal{L}_{\text{unsup}}(\cdot)$ could be any
    similarity metric that quantifies the divergence or inconsistency between two
    model outputs, such as a mean square error, or contrastive loss.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Local smoothness among different views of the same sample (Eq. ([15](#S4.E15
    "In 4.1.1 Local Smoothness ‣ 4.1 The learning assumptions shared by SSL and UL
    ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey"))) can be achieved via the consistency regularization techniques in
    SSL (§[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency Regularization ‣ 2.2 Taxonomy on
    SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey"), Figure [3](#S2.F3 "Figure 3 ‣ 2.2.1.1 Consistency
    regularization under input variations ‣ 2.2.1 Consistency Regularization ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")). They enforce predictive smoothness
    to the same samples under different variations imposed at the input space and
    (or) model space, given that the different transformed versions of the same sample
    should lie in its own local neighborhood. Similarly, the instance discrimination
    algorithms in UL also implicitly enforce the same samples under different views
    or transformations to have locally consistent representations, as represented
    by contrastive learning which encourages local invariances on each sample (§[10](#S3.F10
    "Figure 10 ‣ 3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised
    Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"),
    Figure [10](#S3.F10 "Figure 10 ‣ 3.2.2 Discriminative Models ‣ 3.2 Taxonomy on
    UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Local smoothness among the nearby samples (Eq. ([16](#S4.E16 "In 4.1.1 Local
    Smoothness ‣ 4.1 The learning assumptions shared by SSL and UL ‣ 4 Discussion
    on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")))
    can be imposed via the graph-based regularization techniques in SSL. They often
    propagate the class labels to the unlabeled samples using the labels of their
    neighbours on the graph, as the nearby samples should likely share the same class
    (§[2.2.3](#S2.SS2.SSS3 "2.2.3 Graph-based Regularization ‣ 2.2 Taxonomy on SSL
    Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey"), Figure [5](#S2.F5 "Figure 5 ‣ 2.2.3 Graph-based
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")). Similarly,
    neighbourhood consistency is also explored in UL [[169](#bib.bib169), [170](#bib.bib170)],
    which forms the semantic training labels by mining the nearest neighbors of each
    sample based on feature similarity, given that nearest neighbors are likely to
    belong to the same semantic class.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Global Smoothness
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The global smoothness assumption indicates that a sample $x_{i}$ could be assigned
    to a certain class (or target) $z_{i}$ based on the underlying global structures
    captured by the model:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{{\theta}}{\text{min}}\ \sum_{{x_{i}}\in\mathcal{D}}\mathcal{L}_{\text{unsup}}(f({x_{i}}),z_{i})$
    |  | (17) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: 'where $z_{i}$ is the learning target (e.g. the cluster membership or the most
    confident predicted class), which is derived from the global class decision boundaries
    discovered during training (Figure [13](#S4.F13 "Figure 13 ‣ 4.1 The learning
    assumptions shared by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) whilst the decision boundaries
    are supposed to lie in low density regions. Similar to Eq. ([15](#S4.E15 "In 4.1.1
    Local Smoothness ‣ 4.1 The learning assumptions shared by SSL and UL ‣ 4 Discussion
    on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))
    and Eq. ([16](#S4.E16 "In 4.1.1 Local Smoothness ‣ 4.1 The learning assumptions
    shared by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")), $\mathcal{L}_{\text{unsup}}(\cdot)$ is a similarity
    metric that quantifies the inconsistency between the model output and the training
    target, such as a cross-entropy loss. The global smoothness assumption is also
    widely adopted in various SSL and UL techniques to learn from the unlabeled samples
    with pseudo learning targets, as detailed in the following.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'The self-training techniques in SSL (§[2.2.2](#S2.SS2.SSS2 "2.2.2 Self-Training
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"), Figure [4](#S2.F4 "Figure 4
    ‣ 2.2.1.2 Consistency regularization under model variations ‣ 2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")) are generally
    formulated based on global smoothness, as the learning targets for unlabeled data
    are derived based on the class decision boundaries discovered by the models. For
    instance, in entropy minimization (Eq. ([4](#S2.E4 "In 2.2.2 Self-Training ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")), Figure [4](#S2.F4 "Figure
    4 ‣ 2.2.1.2 Consistency regularization under model variations ‣ 2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey") (a)), the
    pseudo label is obtained as the class predicted with the highest confidence. In
    co-training and distillation (Eq. ([5](#S2.E5 "In 2.2.2 Self-Training ‣ 2.2 Taxonomy
    on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")), Eq. ([6](#S2.E6 "In 2.2.2 Self-Training ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")), Figure [4](#S2.F4 "Figure
    4 ‣ 2.2.1.2 Consistency regularization under model variations ‣ 2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey") (b)(c)), the
    learning targets come from the model co-trained in parallel or pre-trained beforehand.
    Similarly, the deep clustering algorithms in UL (§[11](#S3.F11 "Figure 11 ‣ 3.2.2
    Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning
    (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"), Figure
    [11](#S3.F11 "Figure 11 ‣ 3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms
    ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")) are also proposed upon global smoothness, given that the
    cluster memberships for unlabeled samples are acquired from an online or offline
    clustering algorithm which uncovers the latent class decision boundaries in the
    feature space.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Connections between SSL and UL
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TABLE III: A common taxonomy on SSL and UL methods based on their learning
    assumptions.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '| Assumption | Objective | Corresponding SSL & UL methods |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| local smoothness | Eq. ([15](#S4.E15 "In 4.1.1 Local Smoothness ‣ 4.1 The
    learning assumptions shared by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) | consistency regularization
    in SSL (§[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency Regularization ‣ 2.2 Taxonomy
    on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| instance discrimination in UL (§[10](#S3.F10 "Figure 10 ‣ 3.2.2 Discriminative
    Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| Eq. ([16](#S4.E16 "In 4.1.1 Local Smoothness ‣ 4.1 The learning assumptions
    shared by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")) | graph-based regularization in SSL (§[2.2.3](#S2.SS2.SSS3
    "2.2.3 Graph-based Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| neighbourhood consistency in UL (§[3.2.2](#S3.SS2.SSS2 "3.2.2 Discriminative
    Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| global smoothness | Eq. ([17](#S4.E17 "In 4.1.2 Global Smoothness ‣ 4.1 The
    learning assumptions shared by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) | self-training in SSL (§[2.2.2](#S2.SS2.SSS2
    "2.2.2 Self-Training ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| deep clustering in UL (§[11](#S3.F11 "Figure 11 ‣ 3.2.2 Discriminative Models
    ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: 'The learning rationales common in SSL and UL. As analyzed in §[4.1.1](#S4.SS1.SSS1
    "4.1.1 Local Smoothness ‣ 4.1 The learning assumptions shared by SSL and UL ‣
    4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey") and §[4.1.2](#S4.SS1.SSS2 "4.1.2 Global Smoothness ‣ 4.1 The learning
    assumptions shared by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"), most SSL and UL algorithms
    are formulated based on the same local smoothness or global smoothness assumption
    – as summarized in Table [III](#S4.T3 "TABLE III ‣ 4.1.3 Connections between SSL
    and UL ‣ 4.1 The learning assumptions shared by SSL and UL ‣ 4 Discussion on SSL
    and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"). A
    common aspect of these SSL and UL algorithms is to design visual learning objectives
    that enforce invariance or equivariance towards different transformations applied
    on the input data, as represented by consistency regularization in SSL (§[2.2.1](#S2.SS2.SSS1
    "2.2.1 Consistency Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))
    and instance discrimination in UL (§[10](#S3.F10 "Figure 10 ‣ 3.2.2 Discriminative
    Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")). Typical transformation strategies
    can range from simple data augmentation [[46](#bib.bib46), [37](#bib.bib37), [39](#bib.bib39)],
    to more complex transformations such as adversarial perturbations [[48](#bib.bib48),
    [49](#bib.bib49), [81](#bib.bib81), [74](#bib.bib74)], rotations [[123](#bib.bib123)]
    and patch reordering [[121](#bib.bib121)], autoencoding transformations [[200](#bib.bib200),
    [201](#bib.bib201)] and automated augmentation [[27](#bib.bib27), [51](#bib.bib51),
    [38](#bib.bib38)]. On one side, most of these SSL and UL methods hinge on learning
    representations invariant to data augmentation and perturbations by assigning
    the same underlying labels to the augmented and perturbed data samples. On the
    other side, other SSL and UL methods consider learning representations that are
    equivalent to different transformations such as rotations and patch re-ordering
    by learning to predict the type of transformations.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Many state-of-the-art SSL and UL methods can be well related with the same
    underlying learning assumptions, given that they introduce similar objectives
    to learn from the unlabeled samples. In essence, the learning rationales of these
    SSL and UL methods could be broadly categorized as: (1) impose the consistency
    among different transformed versions of the same sample (Eq. ([15](#S4.E15 "In
    4.1.1 Local Smoothness ‣ 4.1 The learning assumptions shared by SSL and UL ‣ 4
    Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey"))), (2) enforce the smoothness between a sample and its neighbouring
    one (Eq. ([16](#S4.E16 "In 4.1.1 Local Smoothness ‣ 4.1 The learning assumptions
    shared by SSL and UL ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey"))), and (3) derive learning targets for the unlabeled
    samples based on global decision boundaries (Eq. ([17](#S4.E17 "In 4.1.2 Global
    Smoothness ‣ 4.1 The learning assumptions shared by SSL and UL ‣ 4 Discussion
    on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'The similarities and differences between problem setups. In the problems setups,
    SSL and UL are similar in the sense that both labeled and unlabeled data are often
    involved in their training protocols before evaluating their generalized model
    performance on the test set. In particular, the SSL paradigm adopts one-stage
    training and uses both labeled and unlabeled data during training (Figure [2](#S2.F2
    "Figure 2 ‣ 2.1 The Problem Setting of SSL ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")); while most
    existing UL protocols consider two-stage training (Figure [7](#S3.F7 "Figure 7
    ‣ 3.1 The Problem Setting of UL ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) – one stage for pre-training
    with unlabeled data and another stage for fine-tuning with labeled data on a downstream
    task.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'In brief, when it comes to training protocols, UL differs from SSL in several
    ways: (1) the labeled data and unlabeled data are not given together at once;
    (2) unlabeled and labeled datasets may have different distributions. These properties
    make UL a more generic learning paradigm to leverage different unlabeled datasets.
    Nevertheless, how unsupervised pre-training upon different forms of unlabeled
    data benefits the model generalization on specific downstream tasks remains an
    open research question. For instance, it remains unclear how an unsupervised model
    pre-trained on natural colour images could generalize to a downstream task that
    has a different data distribution such as grayscale images in medical imaging.
    In this regard, SSL provides a more reliable learning paradigm to utilize the
    unlabeled data, given that the label set offers the prior knowledge for the models
    and (or) the model designers to select the useful set of unlabeled samples that
    are similar to the labeled data distribution.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Applied SSL and UL in Visual Recognition
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In §[2](#S2 "2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey") and §[3](#S3 "3 Unsupervised Learning (UL) ‣
    Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"), we mainly present
    the SSL and UL methods for standard image classification. However, their underlying
    learning rationales can be generalized to other challenging computer vision tasks,
    e.g., semantic segmentation [[32](#bib.bib32), [202](#bib.bib202)], object detection [[203](#bib.bib203),
    [30](#bib.bib30)], unsupervised domain adaptation [[204](#bib.bib204), [205](#bib.bib205)],
    pose estimation [[34](#bib.bib34), [206](#bib.bib206)], 3D scene understanding [[207](#bib.bib207)],
    video recognition [[150](#bib.bib150), [208](#bib.bib208)], etc. In the following,
    we review three core visual recognition tasks that widely benefit from SSL and
    UL methods to exploit unlabeled data: semantic segmentation (§[4.2.1](#S4.SS2.SSS1
    "4.2.1 Semantic Segmentation ‣ 4.2 Applied SSL and UL in Visual Recognition ‣
    4 Discussion on SSL and UL ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey")), object detection (§[4.2.2](#S4.SS2.SSS2 "4.2.2 Object Detection ‣
    4.2 Applied SSL and UL in Visual Recognition ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")), and unsupervised domain adaptation
    (§[4.2.3](#S4.SS2.SSS3 "4.2.3 Unsupervised Domain Adaptation ‣ 4.2 Applied SSL
    and UL in Visual Recognition ‣ 4 Discussion on SSL and UL ‣ Semi-Supervised and
    Unsupervised Deep Visual Learning: A Survey")).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Semantic Segmentation
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Semantic segmentation aims to assign a semantic class label for each pixel in
    an input image. It is a core computer vision task that could be beneficial to
    various real-world applications such as medical image analysis [[209](#bib.bib209),
    [210](#bib.bib210), [211](#bib.bib211), [212](#bib.bib212)] and autonomous driving [[213](#bib.bib213),
    [214](#bib.bib214), [215](#bib.bib215)]. Supervised semantic segmentation requires
    tedious and expensive pixel-wise label annotations, e.g. manually annotating one
    single natural image in Cityscapes needs 1.5 hours [[213](#bib.bib213)].
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the annotation costs in semantic segmentation, a group of works consider
    only a small set of the training data annotated with per-pixel semantic labels
    while the rest of the training data being unlabeled – known as semi-supervised
    semantic segmentation. These works generally inherit similar learning rationales
    as SSL or UL for image classification, and adapt techniques such as consistency
    regularization [[216](#bib.bib216), [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)],
    self-training [[220](#bib.bib220), [221](#bib.bib221), [222](#bib.bib222), [210](#bib.bib210),
    [202](#bib.bib202), [223](#bib.bib223), [224](#bib.bib224)], GAN frameworks [[225](#bib.bib225),
    [226](#bib.bib226), [227](#bib.bib227)] in SSL, or contrastive learning [[228](#bib.bib228),
    [229](#bib.bib229), [230](#bib.bib230), [231](#bib.bib231)] in UL to learn from
    unlabeled images. Nevertheless, unsupervised loss terms in semantic segmentation
    are often required to impose in a per-pixel manner to align with the pixel-wise
    learning objective in semantic segmentation. In the following, we discuss the
    three most representative lines of state-of-the-art methods driven by recent advances
    in SSL and UL for semi-supervised semantic segmentation.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Consistency regularization (§[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency Regularization
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) can be generalized for pixel-wise
    tasks by formulating the consistency loss (Eq. ([2](#S2.E2 "In 2.2.1 Consistency
    Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL)
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")), Eq. ([3](#S2.E3
    "In 2.2.1 Consistency Regularization ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")))
    at the pixel level. In a similar spirit as the standard consistency regularization
    in SSL, recent works in semi-supervised semantic segmentation [[216](#bib.bib216),
    [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)] resort to enforcing
    pixel consistency among the images before and after perturbations, whilst perturbations
    being introduced at the input space [[216](#bib.bib216)] or feature space [[217](#bib.bib217)].
    For instance, the first consistency regularization method in semantic segmentation [[216](#bib.bib216)]
    applies CutOut [[78](#bib.bib78)] and CutMix [[232](#bib.bib232)] augmentation
    techniques to perturb the input images with partial corruption, and imposes pixel-level
    loss terms to ensure the uncorrupted regions in perturbed images should have consistent
    pixel-wise predictions as the same regions in original images. A cross-consistency
    training [[217](#bib.bib217)] instead applies feature perturbations by injecting
    noise into network’s activations and enforces pixel consistency between the clean
    and perturbed outputs.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-training algorithms (§[2.2.2](#S2.SS2.SSS2 "2.2.2 Self-Training ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) are adapted and shown effective
    for semi-supervised semantic segmentation [[220](#bib.bib220), [221](#bib.bib221),
    [222](#bib.bib222), [210](#bib.bib210), [202](#bib.bib202), [223](#bib.bib223),
    [224](#bib.bib224)], where pseudo segmentation maps on unlabeled images are propagated
    using a pre-trained teacher model [[223](#bib.bib223)], or a co-trained model [[202](#bib.bib202)].
    For example, a self-training method [[223](#bib.bib223)] propagates pseudo segmentation
    labels with two steps – (1) assigning pixel-wise pseudo labels on unlabeled data
    with a pre-trained teacher model; and (2) re-training a student model with the
    re-labeled dataset – until no more performance gain is achieved. Another self-training
    approach [[202](#bib.bib202)] adopts a co-training scheme by training two models
    to learn the per-pixel segmentation predictions from each others.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrastive learning is widely used in UL and adapted to learn from unlabeled
    data in semantic segmentation [[228](#bib.bib228), [229](#bib.bib229), [230](#bib.bib230),
    [231](#bib.bib231)]. To formulate the contrastive loss (Eq. ([12](#S3.E12 "In
    3.2.2 Discriminative Models ‣ 3.2 Taxonomy on UL Algorithms ‣ 3 Unsupervised Learning
    (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))) per
    pixel, one needs select meaningful positive and negative pairs with consideration
    of pixel spatial locations. For this aim, a directional context-aware contrastive
    loss [[228](#bib.bib228)] is proposed to crop two patches from one image, and
    take features at the same location as a positive pair and the rest as negative
    pairs. Another pixel contrastive loss [[230](#bib.bib230)] is introduced to align
    the features before and after a random color augmentation by taking features at
    the same location as a positive pair, while sampling a fixed amount of negative
    pairs from different images.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Object Detection
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Object detection aims to predict a set of bounding boxes and the corresponding
    class labels for the objects of interest in an image. An object detector needs
    to unify classification and localization into one model by jointly training a
    classifier to predict class labels and a regression head to generate the bounding
    boxes [[5](#bib.bib5), [233](#bib.bib233)]. It is an important computer vision
    task that widely impacts different applications such as person search [[234](#bib.bib234)],
    vehicle detection [[235](#bib.bib235)], logo detection [[236](#bib.bib236)], text
    detection [[237](#bib.bib237)], etc. Supervised object detection requires costly
    annotation efforts – annotating the bounding box of a single object takes up to
    42 seconds [[238](#bib.bib238)].
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: To exploit the unlabeled data without bounding box or class label information,
    a group of works in object detection exploit unlabeled data to boost model generalization
    by training on a small set of labeled data and a set of completely unlabeled images
    – known as semi-supervised object detection. These works mainly reformulate two
    streams of SSL techniques, including consistency regularization [[203](#bib.bib203),
    [239](#bib.bib239), [240](#bib.bib240), [30](#bib.bib30), [241](#bib.bib241),
    [242](#bib.bib242)] and self-training [[35](#bib.bib35), [243](#bib.bib243), [244](#bib.bib244),
    [245](#bib.bib245), [246](#bib.bib246)], both of which introduce the learning
    targets for both bounding boxes and class labels to learn from the completely
    unlabeled data, as detailed next.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Consistency regularization (§[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency Regularization
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) is introduced for semi-supervised
    object detection to propagate the soft label and bounding boxes assignment on
    unlabeled images based on dual consistency constraints on classification and regression [[203](#bib.bib203),
    [239](#bib.bib239), [240](#bib.bib240), [30](#bib.bib30), [241](#bib.bib241),
    [242](#bib.bib242)]. One line of works apply data augmentation such as random
    flipping [[203](#bib.bib203)] and MixUp [[75](#bib.bib75)] to generate augmented
    views of unlabeled images and encourage the predicted bounding boxes and its class
    labels remain consistent for the different views. Compared to standard consistency
    regularization, these methods especially need re-estimating the bounding box location
    in an augmented image, such as flip the bounding box [[203](#bib.bib203)], or
    calculate the overlapped bounding boxes of two mixed images in MixUp [[75](#bib.bib75)].
    Another line of works follow a teacher-student training framework and impose teacher-student
    consistency [[240](#bib.bib240), [30](#bib.bib30), [241](#bib.bib241), [242](#bib.bib242)]
    similar to Mean Teacher [[39](#bib.bib39)]. The teacher model is derived either
    from the student model via exponential mean average (EMA) [[240](#bib.bib240),
    [30](#bib.bib30), [242](#bib.bib242)], or by applying non-maximum suppression
    (NMS, a filtering technique for refining the detected bounding boxes) on the instant
    model outputs [[241](#bib.bib241)] to obtain the pseudo bounding boxes and label
    annotations for training.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-training algorithms (§[2.2.2](#S2.SS2.SSS2 "2.2.2 Self-Training ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) are also introduced to annotated
    unlabeled images for object detection [[35](#bib.bib35), [243](#bib.bib243), [244](#bib.bib244),
    [245](#bib.bib245), [246](#bib.bib246)]. A simple self-training paradigm is to
    annotate the unlabeled images with bounding boxes and their class labels using
    a pre-trained teacher model and use these data for re-training [[243](#bib.bib243)].
    However, such pseudo annotations may be rather noisy. To improve the quality of
    pseudo labels, recent works propose interactive self-training to progressively
    refine the pseudo labels with NMS [[244](#bib.bib244)], or quantify model uncertainty
    to select or derive more reliable pseudo labels [[245](#bib.bib245), [246](#bib.bib246)]
    to learn from unlabeled data.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Unsupervised Domain Adaptation
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unsupervised domain adaptation (UDA) is a special case of SSL where the labeled
    (source) and unlabeled (target) data lie in different distributions, a.k.a. different
    domains. UDA is essential for visual recognition [[247](#bib.bib247)], as the
    statistical properties of visual data are sensitive to a wider variety of factors,
    e.g., illumination, viewpoint, resolution, occlusion, times of the day, and weather
    conditions. While most UDA methods focus on tackling the domain gap between the
    labeled and unlabeled data, SSL and UL algorithms can also be adapted to learn
    from unlabeled data in UDA, as follows.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Consistency regularization (§[2.2.1](#S2.SS2.SSS1 "2.2.1 Consistency Regularization
    ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")) is shown to be effective in
    UDA. In the same spirit of encouraging consistent outputs under perturbations,
    various UDA approaches apply input transformations or model ensembling to simulate
    variations in input or model space [[248](#bib.bib248), [249](#bib.bib249), [39](#bib.bib39),
    [250](#bib.bib250)]. To generate input variations, a dual MixUp regularization
    integrates category-level MixUp and domain-level MixUp to regularize the model
    with consistency constraints, thus learning from unlabeled data to enhance domain-invariance [[248](#bib.bib248)].
    To generate model variations, self-ensembling [[249](#bib.bib249)] utilizes the
    Mean Teacher [[39](#bib.bib39)] to impute unlabeled training targets in target
    domain.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-training (§[2.2.2](#S2.SS2.SSS2 "2.2.2 Self-Training ‣ 2.2 Taxonomy on
    SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey")) has been also useful for UDA. Similar to SSL,
    self-training for UDA include three streams of techniques to impute pseudo labels
    on the unlabeled target samples, including entropy minimization, pseudo-label
    and co-training. To ensure the effectiveness, self-training methods are often
    coupled with domain distribution alignment for reducing the domain shift. For
    instance, entropy minimization (Eq. ([4](#S2.E4 "In 2.2.2 Self-Training ‣ 2.2
    Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey"))) is adopted for UDA  [[251](#bib.bib251),
    [252](#bib.bib252), [253](#bib.bib253)], in combination with distribution alignment
    techniques such as domain-specific batch normalization layers [[251](#bib.bib251)],
    aligning second-order statistics of features [[252](#bib.bib252)], or adversarial
    training and gradient synchronization [[253](#bib.bib253)]. Co-training (Eq. ([5](#S2.E5
    "In 2.2.2 Self-Training ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised Learning
    (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"))) is
    also introduced for UDA, which imputes training targets from multiple co-trained
    classifiers to learn from unlabeled data and match cross-domain distributions [[254](#bib.bib254)].'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep generative models (DGMs), as a class of models for SSL and UL (§[2.2.4](#S2.SS2.SSS4
    "2.2.4 Deep Generative Models ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"),
    §[3.2.3](#S3.SS2.SSS3 "3.2.3 Deep Generative Models ‣ 3.2 Taxonomy on UL Algorithms
    ‣ 3 Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual
    Learning: A Survey")), are widely adopted for UDA. In contrast to other UDA methods
    that reduce the domain shift at the feature level, DGMs provide an alternative
    and complementary solution to mitigate the domain discrepancy at pixel level by
    cross-domain image-to-image translation. The majority of these frameworks are
    based on GANs, such as PixelDA [[255](#bib.bib255)], generate to adapt [[256](#bib.bib256)],
    and GANs with cycle-consistency like CyCADA[[257](#bib.bib257)], SBADA-GAN [[258](#bib.bib258)],
    I2I Adapt [[259](#bib.bib259)] and CrDoCo [[260](#bib.bib260)]. These models typically
    learn a real-to-real [[261](#bib.bib261), [257](#bib.bib257), [260](#bib.bib260),
    [258](#bib.bib258)] or synthetic-to-real [[262](#bib.bib262), [255](#bib.bib255),
    [256](#bib.bib256)] mapping to render the image style from the labeled source
    to the unlabeled target domain, thus offering synthetic training data with pseudo
    labels.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-supervised learning popularized in SSL and UL (§[2.2.5](#S2.SS2.SSS5 "2.2.5
    Self-Supervised Learning ‣ 2.2 Taxonomy on SSL Algorithms ‣ 2 Semi-Supervised
    Learning (SSL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey"),
    §[3.2.1](#S3.SS2.SSS1 "3.2.1 Pretext Tasks ‣ 3.2 Taxonomy on UL Algorithms ‣ 3
    Unsupervised Learning (UL) ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey")), is also introduced in UDA to construct auxiliary self-supervised
    learning objectives on unlabeled data. Self-supervised models often address the
    UDA problem by self-supervision coupled with a supervised objective on the labeled
    source data [[164](#bib.bib164), [204](#bib.bib204), [263](#bib.bib263), [264](#bib.bib264)].
    The pioneer work in this direction is JiGen [[164](#bib.bib164)], which learns
    jointly to classify objects and solve the jigsaw puzzles [[121](#bib.bib121)]
    pretext task to achieve better generalization in new domains. Recent works [[204](#bib.bib204),
    [263](#bib.bib263), [264](#bib.bib264)] explored other self-supervised pretext
    tasks such as predicting rotation [[204](#bib.bib204), [263](#bib.bib263), [264](#bib.bib264)],
    flipping [[204](#bib.bib204)] and patch ordering [[204](#bib.bib204)]. Besides
    pretext tasks, recent UDA methods also explored discriminative self-supervision
    signals based on clustering or contrastive learning. For instance, DANCE [[205](#bib.bib205)]
    performs neighborhood clustering by assigning the target samples to a “known”
    class prototype in the source domain or its neighbor in the target domain. Gradient
    regularized contrastive learning [[265](#bib.bib265)] leverages the contrastive
    loss to push unlabeled target samples towards the most similar labeled source
    samples. Similarly, [[266](#bib.bib266)] aligns target domain features to class
    prototypes in the source domain through contrastive loss, minimizing the distances
    between the cross-domain samples that likely belong to the same class.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 5 Emerging Trends and Open Challenges
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we discuss the emerging trends in SSL and UL from unlabeled
    data, covering three directions, namely open-set learning (§[5.1](#S5.SS1 "5.1
    Open-Set Learning from Unlabeled Data ‣ 5 Emerging Trends and Open Challenges
    ‣ Semi-Supervised and Unsupervised Deep Visual Learning: A Survey")), incremental
    learning (§[5.2](#S5.SS2 "5.2 Incremental Learning from Unlabeled Data ‣ 5 Emerging
    Trends and Open Challenges ‣ Semi-Supervised and Unsupervised Deep Visual Learning:
    A Survey")) and multi-modal learning (§[5.3](#S5.SS3 "5.3 Multi-Modal Learning
    from Unlabeled Data ‣ 5 Emerging Trends and Open Challenges ‣ Semi-Supervised
    and Unsupervised Deep Visual Learning: A Survey")). We detail both recent developments
    and open challenges.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Open-Set Learning from Unlabeled Data
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In §[2](#S2 "2 Semi-Supervised Learning (SSL) ‣ Semi-Supervised and Unsupervised
    Deep Visual Learning: A Survey"), we review works addressing the relatively simple
    closed-set learning in SSL, which assume that unlabeled data share the same label
    space as the labeled one. However, this closed-set assumption may greatly hinder
    the effectiveness of SSL in leveraging real-world uncurated unlabeled data that
    contains unseen classes, i.e., out-of-distribution (OOD) samples (also known as
    outliers) [[40](#bib.bib40)]. When applying most existing SSL methods to open-set
    learning with noisy unlabeled data, their model performance may degrade significantly,
    as the OOD samples could induce catastrophic error propagation.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: A line of works propose to address a more complex open-set SSL scenario [[14](#bib.bib14),
    [15](#bib.bib15), [267](#bib.bib267), [268](#bib.bib268), [269](#bib.bib269),
    [270](#bib.bib270), [271](#bib.bib271), [272](#bib.bib272)], where the unlabeled
    set contains task-irrelevant OOD data. In this setup (so-called open-world SSL),
    unlabeled samples are not all beneficial. To prevent possible performance hazards
    caused by unlabeled OOD samples, recent advances in SSL propose various sample-specific
    selection strategies to discount their importance or usage [[14](#bib.bib14),
    [15](#bib.bib15), [267](#bib.bib267), [268](#bib.bib268)]. The pioneer works including
    UASD [[14](#bib.bib14)] and DS³L [[15](#bib.bib15)] propose to impose a dynamic
    weighting function to down-weight the unsupervised regularization loss term proportional
    to the likelihood that an unlabeled sample belongs to an unseen class. Follow-up
    works resort to curriculum learning [[267](#bib.bib267)] and iterative self-training [[268](#bib.bib268)]
    by training an OOD classifier to detect and discard the potentially detrimental
    samples. More recently, OpenMatch [[270](#bib.bib270)] propose to train a set
    of one-vs-all classifiers for detecting inliers and outliers and regularize the
    model with a consistency constraint on only the unlabeled inliers.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Open Challenges. The open-set SSL calls for integrating OOD detection [[273](#bib.bib273)]
    or novel class discovery [[274](#bib.bib274)] with semi-supervised learning in
    a unified model to advance selective exploitation of noisy unlabeled data. Moreover,
    a more recent work propose a universal SSL benchmark [[271](#bib.bib271)] which
    further extends the distribution mismatch problem in open-set setup as subset
    or intersectional class mismatch, and feature distribution mismatch. These more
    realistic setups pose multiple new challenges, including confidence calibration
    of DNN for OOD detection [[273](#bib.bib273), [273](#bib.bib273), [275](#bib.bib275),
    [276](#bib.bib276), [277](#bib.bib277)], imbalanced class distribution caused
    by real-world long-tailed distributed unlabeled data [[278](#bib.bib278), [279](#bib.bib279)],
    and discovery of unseen classes in unlabeled data [[280](#bib.bib280), [281](#bib.bib281),
    [274](#bib.bib274)]. Although recent advances in open-set SSL have explored OOD
    detection, the other challenges remain to be resolved to exploit real-world unlabeled
    data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Incremental Learning from Unlabeled Data
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing works on SSL and UL often assume all unlabeled training data is available
    at once, which however may not always hold in practice due to privacy concerns
    or computational constraints. In many realistic scenarios, we need to perform
    incremental learning (IL) with new data to update the model incrementally without
    access to past training data. Here we review research directions on IL from unlabeled
    data [[282](#bib.bib282), [283](#bib.bib283)] and discuss its open challenges.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Incremental learning (IL) from unlabeled data has been investigated in a semi-supervised
    fashion [[282](#bib.bib282)]. IL (also known as continual learning and lifelong
    learning [[284](#bib.bib284)]) aims to extend an existing model’s knowledge without
    accessing the previous training data. Most existing IL approaches use regularization
    objectives to not forget old knowledge, i.e., reducing catastrophic forgetting [[285](#bib.bib285),
    [286](#bib.bib286), [287](#bib.bib287), [288](#bib.bib288)]. To this aim, unlabeled
    data is often used in IL to prevent catastrophic forgetting by estimating the
    importance weights of model parameters for old tasks [[289](#bib.bib289)], or
    formulating a knowledge distillation objective [[282](#bib.bib282), [290](#bib.bib290)]
    to consolidate the knowledge learned from old data. Recently, multiple works explore
    IL from unlabeled data that comes as a non-stationary stream [[283](#bib.bib283),
    [291](#bib.bib291)], with the class label space possibly varying over time [[292](#bib.bib292)].
    In this setting, the goal is to learn a salient representation from continuous
    incoming unlabeled data stream. To expand the representations for novel classes
    and unlabeled data, several strategies are adopted to dynamically update representations
    in the latent space, such as creating new cluster centroids by online clustering [[292](#bib.bib292)]
    and updating mixture-of-Gaussians [[283](#bib.bib283)]. Some recent works apply
    self-supervised techniques on the unlabeled test-data [[293](#bib.bib293), [294](#bib.bib294),
    [295](#bib.bib295)], which is useful to overcome possible shifts in the data distribution [[296](#bib.bib296)].
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Open Challenges. Incremental learning from unlabeled data requires solving multiple
    challenges, ranging from catastrophic forgetting [[282](#bib.bib282), [297](#bib.bib297)],
    modeling new concepts [[283](#bib.bib283), [292](#bib.bib292)] to predicting the
    evolution of data streams [[296](#bib.bib296)]. Due to lacking the access to all
    the unlabeled training data at once, addressing these challenges is nontrivial
    as directly applying many existing SSL and UL methods could not guarantee good
    generalization performance. As an example, pseudo labels may suffer the confirmation
    bias problem [[298](#bib.bib298)] when classifying unseen unlabeled data. Thus,
    incremental learning from a stream of potentially non-i.i.d. unlabeled data remains
    an open challenge.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Multi-Modal Learning from Unlabeled Data
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A growing number of works combine visual and non-visual modalities (e.g., text,
    audio) to form discriminative self-supervision signals that enable learning from
    multi-modal unlabeled data. To bring vision and language for unsupervised learning,
    variants of vision and language BERT models (e.g., ViLBERT [[299](#bib.bib299)],
    LXMERT [[300](#bib.bib300)], VL-BERT [[301](#bib.bib301)], Uniter [[302](#bib.bib302)]
    and Unicoder-VL [[303](#bib.bib303)]) are built upon the transformer blocks [[304](#bib.bib304)]
    to jointly model images and natural language in an unsupervised way. Specifically,
    the visual, linguistic or their joint representations can be learned in an unsupervised
    manner by solving the Cloze task in natural language processing which predicts
    the masked words in the input sentences [[305](#bib.bib305)], or by optimizing
    a linguistic-visual alignment objective [[300](#bib.bib300), [306](#bib.bib306)].
    Another line of works utilize the language supervision (e.g., from web data [[307](#bib.bib307)]
    or narrated materials [[308](#bib.bib308), [309](#bib.bib309), [310](#bib.bib310),
    [311](#bib.bib311), [312](#bib.bib312), [313](#bib.bib313)]) to guide unsupervised
    representation learning by aligning images and languages in the shared latent
    space, as exemplified by CLIP [[312](#bib.bib312)] and ALIGN [[313](#bib.bib313)].
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, to combine audio and visual modalities for unsupervised learning,
    existing works exploit the natural audio-visual correspondence in videos to formulate
    various self-supervised signals, which predict the cross-modal correspondence [[314](#bib.bib314),
    [315](#bib.bib315)], align the temporally corresponding representations [[316](#bib.bib316),
    [317](#bib.bib317), [318](#bib.bib318), [309](#bib.bib309)], or cluster their
    representations in a shared audio-visual latent space [[319](#bib.bib319), [208](#bib.bib208)].
    Several works further explore audio, vision and language together for unsupervised
    representation learning by aligning different modalities in a shared multi-modal
    latent space [[310](#bib.bib310), [320](#bib.bib320)] or in a hierarchical latent
    space for audio-vision and vision-language [[308](#bib.bib308)].
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Open Challenges. The success of multi-modal learning from unlabeled data often
    relies on an assumption that different modalities are semantically correlated.
    For instance, when clustering audio and video data for unsupervised representation
    learning [[208](#bib.bib208)], or transferring text knowledge to the unlabeled
    image data [[321](#bib.bib321)], the two data modalities are assumed to share
    similar semantics. However, this assumption may not hold in real-world data, leading
    to degraded model performance [[322](#bib.bib322), [309](#bib.bib309)]. Thus,
    it remains an open challenge to learn from the multi-modal unlabeled data that
    contains a semantic gap across modalities.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning visual representations with limited or no manual supervision is critical
    for scalable computer vision applications. Semi-supervised learning (SSL) and
    unsupervised learning (UL) models provide feasible and promising solutions to
    learn from unlabeled visual data. In this comprehensive survey, we have introduced
    unified problem definitions and taxonomies to summarize and correlate a wide variety
    of recent advanced and popularized SSL and UL deep learning methodologies for
    building superior visual classification models. We believe that our concise taxonomies
    of existing algorithms and extensive discussions of emerging trends help to better
    understand the status quo of research in visual representation learning with unlabeled
    data, as well as to inspire new learning solutions for major unresolved challenges
    involved in the limited-label regime.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work has been partially funded by the ERC (853489-DEXIM) and the DFG (2064/1–Project
    number 390727645).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, 2015.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] I. Goodfellow, Y. Bengio, and A. Courville, “Deep learning,” *MIT press*,
    2016.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified embedding
    for face recognition and clustering,” in *CVPR*, 2015.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in *NeurIPS*, 2015.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE TPAMI*, 2017.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Z.-H. Zhou and M. Li, “Semi-supervised regression with co-training.” in
    *IJCAI*, 2005.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] O. Chapelle, B. Scholkopf, and A. Zien, “Semi-supervised learning,” *IEEE
    TNNLS*, 2009.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] K. Q. Weinberger and L. K. Saul, “Unsupervised learning of image manifolds
    by semidefinite programming,” *IJCV*, 2006.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review
    and new perspectives,” *IEEE TPAMI*, 2013.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] C. Doersch, A. Gupta, and A. A. Efros, “Unsupervised visual representation
    learning by context prediction,” in *ICCV*, 2015.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework
    for contrastive learning of visual representations,” in *ICML*, 2020.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] X. J. Zhu, “Semi-supervised learning literature survey,” University of
    Wisconsin-Madison Department of Computer Sciences, Tech. Rep., 2005.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Y. Chen, X. Zhu, W. Li, and S. Gong, “Semi-supervised learning under class
    distribution mismatch.” in *AAAI*, 2020.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] L.-Z. Guo, Z.-Y. Zhang, Y. Jiang, Y.-F. Li, and Z.-H. Zhou, “Safe deep
    semi-supervised learning for unseen-class unlabeled data,” in *ICML*, 2020.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
    unsupervised visual representation learning,” in *CVPR*, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised learning,”
    *ML*, 2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE TPAMI*, 2020.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L. Schmarje, M. Santarossa, S.-M. Schröder, and R. Koch, “A survey on
    semi-, self-and unsupervised learning for image classification,” *arXiv:2002.08721*,
    2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] G.-J. Qi and J. Luo, “Small data challenges in big data era: A survey
    of recent progress on unsupervised and semi-supervised methods,” *IEEE TPAMI*,
    2020.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] R. Fergus, Y. Weiss, and A. Torralba, “Semi-supervised learning in gigantic
    image collections,” in *NeurIPS*, 2009.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] N. Papernot, M. Abadi, Ú. Erlingsson, I. Goodfellow, and K. Talwar, “Semi-supervised
    knowledge transfer for deep learning from private training data,” in *ICLR*, 2017.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Blum and T. Mitchell, “Combining labeled and unlabeled data with co-training,”
    in *COLT*, 1998.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] K. Nigam and R. Ghani, “Analyzing the effectiveness and applicability
    of co-training,” in *CIKM*, 2000.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] M. W. Libbrecht and W. S. Noble, “Machine learning applications in genetics
    and genomics,” *Nature Reviews Genetics*, 2015.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. Raffel,
    “Mixmatch: A holistic approach to semi-supervised learning,” in *NeurIPS*, 2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang,
    and C. Raffel, “Remixmatch: Semi-supervised learning with distribution alignment
    and augmentation anchoring,” in *ICLR*, 2020.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. K. Jang and N. I. Cho, “Generalized product quantization network for
    semi-supervised image retrieval,” in *CVPR*, 2020.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Gao, J. Wang, S. Dai, L.-J. Li, and R. Nevatia, “Note-rcnn: Noise tolerant
    ensemble rcnn for semi-supervised object detection,” in *ICCV*, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Y. Tang, W. Chen, Y. Luo, and Y. Zhang, “Humble teachers teach better
    students for semi-supervised object detection,” in *CVPR*, 2021.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] T. Kalluri, G. Varma, M. Chandraker, and C. Jawahar, “Universal semi-supervised
    semantic segmentation,” in *ICCV*, 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Ouali, C. Hudelot, and M. Tami, “Semi-supervised semantic segmentation
    with cross-consistency training,” in *CVPR*, 2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] M. S. Ibrahim, A. Vahdat, M. Ranjbar, and W. G. Macready, “Semi-supervised
    semantic image segmentation with self-correcting networks,” in *CVPR*, 2020.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Y. Chen, Z. Tu, L. Ge, D. Zhang, R. Chen, and J. Yuan, “So-handnet: Self-organizing
    network for 3d hand pose estimation with semi-supervised learning,” in *ICCV*,
    2019.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] I. Radosavovic, P. Dollár, R. Girshick, G. Gkioxari, and K. He, “Data
    distillation: Towards omni-supervised learning,” in *CVPR*, 2018.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] R. Mitra, N. B. Gundavarapu, A. Sharma, and A. Jain, “Multiview-consistent
    semi-supervised learning for 3d human pose estimation,” in *CVPR*, 2020.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Laine and T. Aila, “Temporal ensembling for semi-supervised learning,”
    in *ICLR*, 2017.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] K. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D. Cubuk, A. Kurakin,
    H. Zhang, and C. Raffel, “Fixmatch: Simplifying semi-supervised learning with
    consistency and confidence,” in *NeurIPS*, 2020.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results,” in *NeurIPS*,
    2017.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Oliver, A. Odena, C. A. Raffel, E. D. Cubuk, and I. Goodfellow, “Realistic
    evaluation of deep semi-supervised learning algorithms,” in *NeurIPS*, 2018.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] B. Athiwaratkun, M. Finzi, P. Izmailov, and A. G. Wilson, “There are many
    consistent explanations of unlabeled data: Why you should average,” in *ICLR*,
    2019.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf, “Learning
    with local and global consistency,” in *NeurIPS*, 2004.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] O. Chapelle, J. Weston, and B. Schölkopf, “Cluster kernels for semi-supervised
    learning,” in *NeurIPS*, 2002.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] J. Weston, F. Ratle, H. Mobahi, and R. Collobert, “Deep learning via semi-supervised
    embedding,” in *ICML*, 2008.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] O. Chapelle, A. Zien, C. Z. Ghahramani *et al.*, “Semi-supervised classification
    by low density separation,” in *AISTATSW*, 2005.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M. Sajjadi, M. Javanmardi, and T. Tasdizen, “Regularization with stochastic
    transformations and perturbations for deep semi-supervised learning,” in *NeurIPS*,
    2016.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] X. Wang, D. Kihara, J. Luo, and G.-J. Qi, “Enaet: A self-trained framework
    for semi-supervised and supervised learning with ensemble transformations,” *IEEE
    TIP*, 2020.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] T. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and S. Ishii, “Distributional
    smoothing with virtual adversarial training,” in *ICLR*, 2016.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, “Virtual adversarial
    training: a regularization method for supervised and semi-supervised learning,”
    *IEEE TPAMI*, 2018.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] V. Verma, A. Lamb, J. Kannala, Y. Bengio, and D. Lopez-Paz, “Interpolation
    consistency training for semi-supervised learning,” in *IJCAI*, 2019.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le, “Unsupervised data augmentation
    for consistency training,” in *NeurIPS*, 2020.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] P. Bachman, O. Alsharif, and D. Precup, “Learning with pseudo-ensembles,”
    in *NeurIPS*, 2014.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko, “Semi-supervised
    learning with ladder networks,” in *NeurIPS*, 2015.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Park, J.-K. Park, S.-J. Shin, and I.-C. Moon, “Adversarial dropout
    for supervised and semi-supervised learning,” in *AAAI*, 2018.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] L. Zhang and G.-J. Qi, “Wcp: Worst-case perturbations for semi-supervised
    deep learning,” in *CVPR*, 2020.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] D.-H. Lee, “Pseudo-label: The simple and efficient semi-supervised learning
    method for deep neural networks,” in *ICMLW*, 2013.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Y. Chen, X. Zhu, and S. Gong, “Semi-supervised deep learning with memory,”
    in *ECCV*, 2018.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S. Qiao, W. Shen, Z. Zhang, B. Wang, and A. Yuille, “Deep co-training
    for semi-supervised image recognition,” in *ECCV*, 2018.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] W. Dong-DongChen and Z.-H. WeiGao, “Tri-net for semi-supervised deep learning,”
    in *IJCAI*, 2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, “Self-training with noisy
    student improves imagenet classification,” in *CVPR*, 2020.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Y. Luo, J. Zhu, M. Li, Y. Ren, and B. Zhang, “Smooth neighbors on teacher
    graphs for semi-supervised learning,” in *CVPR*, 2018.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] A. Iscen, G. Tolias, Y. Avrithis, and O. Chum, “Label propagation for
    deep semi-supervised learning,” in *CVPR*, 2019.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semi-supervised
    learning with deep generative models,” in *NeurIPS*, 2014.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] L. Maaløe, C. K. Sønderby, S. K. Sønderby, and O. Winther, “Auxiliary
    deep generative models,” in *ICML*, 2016.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. T. Springenberg, “Unsupervised and semi-supervised learning with categorical
    generative adversarial networks,” in *ICLR*, 2016.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen,
    “Improved techniques for training gans,” in *NeurIPS*, 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro,
    and A. Courville, “Adversarially learned inference,” in *ICLR*, 2017.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Z. Dai, Z. Yang, F. Yang, W. W. Cohen, and R. R. Salakhutdinov, “Good
    semi-supervised learning that requires a bad gan,” in *NeurIPS*, 2017.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] G.-J. Qi, L. Zhang, H. Hu, M. Edraki, J. Wang, and X.-S. Hua, “Global
    versus localized generative adversarial nets,” in *CVPR*, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] X. Zhai, A. Oliver, A. Kolesnikov, and L. Beyer, “S4l: Self-supervised
    semi-supervised learning,” in *ICCV*, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, “Big
    self-supervised models are strong semi-supervised learners,” in *NeurIPS*, 2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data with
    label propagation,” *Technical Report, Carnegie Mellon University*, 2002.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] T. Suzuki and I. Sato, “Adversarial transformations for semi-supervised
    learning.” in *AAAI*, 2020.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical
    risk minimization,” in *ICLR*, 2018.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “Autoaugment:
    Learning augmentation policies from data,” *arXiv:1805.09501*, 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: Practical
    automated data augmentation with a reduced search space,” in *CVPRW*, 2020.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] T. DeVries and G. W. Taylor, “Improved regularization of convolutional
    neural networks with cutout,” *arXiv:1708.04552*, 2017.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” in *ICLR*, 2014.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the
    physical world,” in *ICLR*, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] A. Najafi, S.-i. Maeda, M. Koyama, and T. Miyato, “Robustness to adversarial
    perturbations in learning from incomplete data,” in *NeurIPS*, 2019.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Carmon, A. Raghunathan, L. Schmidt, J. C. Duchi, and P. S. Liang, “Unlabeled
    data improves adversarial robustness,” in *NeurIPS*, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Lim, I. Kim, T. Kim, C. Kim, and S. Kim, “Fast autoaugment,” in *NeurIPS*,
    2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] D. Ho, E. Liang, X. Chen, I. Stoica, and P. Abbeel, “Population based
    augmentation: Efficient learning of augmentation policy schedules,” in *ICML*,
    2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] X. Zhang, Q. Wang, J. Zhang, and Z. Zhong, “Adversarial autoaugment,”
    in *ICLR*, 2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson,
    “Averaging weights leads to wider optima and better generalization,” in *UAI*,
    2018.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] T. M. Mitchell, “Generalization as search,” *AI*, 1982.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] R. E. Schapire, “The strength of weak learnability,” *ML*, 1990.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] L. Breiman, “Random forests,” *ML*, 2001.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Y. Freund, R. Schapire, and N. Abe, “A short introduction to boosting,”
    *Journal-Japanese Society For Artificial Intelligence*, 1999.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable
    predictive uncertainty estimation using deep ensembles,” in *NeurIPS*, 2017.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy minimization,”
    in *NeurIPS*, 2005.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] M. Sajjadi, M. Javanmardi, and T. Tasdizen, “Mutual exclusivity loss for
    semi-supervised deep learning,” in *ICIP*, 2016.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for few-shot
    learning,” in *NeurIPS*, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Z. Ke, D. Wang, Q. Yan, J. Ren, and R. W. Lau, “Dual student: Breaking
    the limits of the teacher in semi-supervised learning,” in *CVPR*, 2019.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” in *ICLR*, 2015.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv:1503.02531*, 2015.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] C. Buciluǎ, R. Caruana, and A. Niculescu-Mizil, “Model compression,” in
    *ACM SIGKDD*, 2006.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] J. Ba and R. Caruana, “Do deep nets really need to be deep?” *NeurIPS*,
    2014.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] I. Z. Yalniz, H. Jégou, K. Chen, M. Paluri, and D. Mahajan, “Billion-scale
    semi-supervised learning for image classification,” *arXiv:1905.00546*, 2019.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Zhu, Z. Ghahramani, and J. D. Lafferty, “Semi-supervised learning
    using gaussian fields and harmonic functions,” in *ICML*, 2003.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization: A geometric
    framework for learning from labeled and unlabeled examples,” *JMLR*, 2006.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] B. Wang, Z. Tu, and J. K. Tsotsos, “Dynamic label propagation for semi-supervised
    multi-class multi-label classification,” in *CVPR*, 2013.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] B. Jiang, Z. Zhang, D. Lin, J. Tang, and B. Luo, “Semi-supervised learning
    with graph learning-convolutional networks,” in *CVPR*, 2019.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Säckinger,
    and R. Shah, “Signature verification using a “siamese” time delay neural network,”
    *IJPRAI*, 1993.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by learning
    an invariant mapping,” in *CVPR*, 2006.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] W. Lin, Z. Gao, and B. Li, “Shoestring: Graph-based semi-supervised classification
    with severely limited labeled data,” in *CVPR*, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] S. Li, B. Liu, D. Chen, Q. Chu, L. Yuan, and N. Yu, “Density-aware graph
    for deep semi-supervised visual recognition,” in *CVPR*, 2020.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv:1312.6114*,
    2013.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NeurIPS*, 2014.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] C. Doersch, “Tutorial on variational autoencoders,” *arXiv:1606.05908*,
    2016.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] M. Ehsan Abbasnejad, A. Dick, and A. van den Hengel, “Infinite variational
    autoencoder for semi-supervised learning,” in *CVPR*, 2017.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Kumar, P. Sattigeri, and T. Fletcher, “Semi-supervised learning with
    gans: Manifold invariance with improved inference,” in *NeurIPS*, 2017.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] C. Li, T. Xu, J. Zhu, and B. Zhang, “Triple generative adversarial nets,”
    in *NeurIPS*, 2017.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, “Generative
    adversarial text to image synthesis.” in *ICML*, 2016.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. Reed, Z. Akata, S. Mohan, S. Tenka, B. Schiele, and H. Lee, “Learning
    what and where to draw.” in *NIPS*, 2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Xian, T. Lorenz, B. Schiele, and Z. Akata, “Feature generating networks
    for zero-shot learning,” in *CVPR*, 2018.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Y. Xian, S. Sharma, B. Schiele, and Z. Akata, “F-vaegan-d2: A feature
    generating framework for any-shot learning,” in *CVPR*, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox, “Discriminative
    unsupervised feature learning with convolutional neural networks,” in *NeurIPS*,
    2014.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Riedmiller, and T. Brox,
    “Discriminative unsupervised feature learning with exemplar convolutional neural
    networks,” *IEEE TPAMI*, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Noroozi and P. Favaro, “Unsupervised learning of visual representations
    by solving jigsaw puzzles,” in *ECCV*, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] M. Noroozi, H. Pirsiavash, and P. Favaro, “Representation learning by
    learning to count,” in *ICCV*, 2017.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation
    learning by predicting image rotations,” in *ICLR*, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
    data with neural networks,” *Science*, 2006.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] J. Masci, U. Meier, D. Cireşan, and J. Schmidhuber, “Stacked convolutional
    auto-encoders for hierarchical feature extraction,” in *ICANN*, 2011.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, “Context
    encoders: Feature learning by inpainting,” in *CVPR*, 2016.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders
    are scalable vision learners,” in *CVPR*, 2022.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extracting
    and composing robust features with denoising autoencoders,” in *ICML*, 2008.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in
    *ECCV*, 2016.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] ——, “Split-brain autoencoders: Unsupervised learning by cross-channel
    prediction,” in *CVPR*, 2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] G. Larsson, M. Maire, and G. Shakhnarovich, “Colorization as a proxy
    task for visual understanding,” in *CVPR*, 2017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning
    via non-parametric instance discrimination,” in *CVPR*, 2018.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] I. Misra and L. v. d. Maaten, “Self-supervised learning of pretext-invariant
    representations,” in *CVPR*, 2020.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,” in
    *ECCV*, 2020.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] X. Chen and K. He, “Exploring simple siamese representation learning,”
    in *CVPR*, 2021.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya,
    C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar *et al.*, “Bootstrap your own latent:
    A new approach to self-supervised learning,” in *NeurIPS*, 2020.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Y. Tian, X. Chen, and S. Ganguli, “Understanding self-supervised learning
    dynamics without contrastive pairs,” in *ICML*, 2021.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
    unsupervised learning of visual features,” in *ECCV*, 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Yang, D. Parikh, and D. Batra, “Joint unsupervised learning of deep
    representations and image clusters,” in *CVPR*, 2016.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Y. Asano, C. Rupprecht, and A. Vedaldi, “Self-labelling via simultaneous
    clustering and representation learning,” in *ICLR*, 2020.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] X. Ji, J. F. Henriques, and A. Vedaldi, “Invariant information clustering
    for unsupervised image classification and segmentation,” in *CVPR*, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] J. Huang, S. Gong, and X. Zhu, “Deep semantic clustering by partition
    confidence maximisation,” in *CVPR*, 2020.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] P. Haeusser, J. Plapp, V. Golkov, E. Aljalbout, and D. Cremers, “Associative
    deep clustering: Training a classification network with no labels,” in *GCPR*,
    2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,
    “Unsupervised learning of visual features by contrasting cluster assignments,”
    in *NeurIPS*, 2020.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” *arXiv:1511.06434*,
    2015.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] T. Chen, X. Zhai, M. Ritter, M. Lucic, and N. Houlsby, “Self-supervised
    gans via auxiliary rotation loss,” in *CVPR*, 2019.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] J. Wang, W. Zhou, G.-J. Qi, Z. Fu, Q. Tian, and H. Li, “Transformation
    gan for unsupervised image synthesis and representation learning,” in *CVPR*,
    2020.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training for
    high fidelity natural image synthesis,” in *ICLR*, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] J. Donahue and K. Simonyan, “Large scale adversarial representation learning,”
    in *NeurIPS*, 2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] X. Wang and A. Gupta, “Unsupervised learning of visual representations
    using videos,” in *ICCV*, 2015.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy,
    “Tracking emerges by colorizing videos,” in *ECCV*, 2018.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] I. Aganj, M. G. Harisinghani, R. Weissleder, and B. Fischl, “Unsupervised
    medical image segmentation based on the local center of mass,” *Nature*, 2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] K. He, R. Girshick, and P. Dollár, “Rethinking imagenet pre-training,”
    in *CVPR*, 2019.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] C. Feichtenhofer, H. Fan, B. Xiong, R. Girshick, and K. He, “A large-scale
    study on unsupervised spatiotemporal representation learning,” in *CVPR*, 2021.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” *IEEE TPAMI*, 2016.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *IJCV*, 2010.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *CVPR*,
    2017.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *ECCV*, 2014.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] R. Santa Cruz, B. Fernando, A. Cherian, and S. Gould, “Visual permutation
    learning,” *IEEE TPAMI*, 2018.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] T. Nathan Mundhenk, D. Ho, and B. Y. Chen, “Improvements to context based
    self-supervised learning,” in *CVPR*, 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] C. Wei, L. Xie, X. Ren, Y. Xia, C. Su, J. Liu, Q. Tian, and A. L. Yuille,
    “Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw
    puzzles for unsupervised representation learning,” in *CVPR*, 2019.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] P. Goyal, D. Mahajan, A. Gupta, and I. Misra, “Scaling and benchmarking
    self-supervised visual representation learning,” in *ICCV*, 2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] G. Goh, N. Cammarata, C. Voss, S. Carter, M. Petrov, L. Schubert, A. Radford,
    and C. Olah, “Multimodal neurons in artificial neural networks,” *Distill*, 2021.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] F. M. Carlucci, A. D’Innocente, S. Bucci, B. Caputo, and T. Tommasi,
    “Domain generalization by solving jigsaw puzzles,” in *CVPR*, 2019.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman,
    A. Trischler, and Y. Bengio, “Learning deep representations by mutual information
    estimation and maximization,” in *ICLR*, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] P. Bachman, R. D. Hjelm, and W. Buchwalter, “Learning representations
    by maximizing mutual information across views,” in *NeurIPS*, 2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] M. Tschannen, J. Djolonga, P. K. Rubenstein, S. Gelly, and M. Lucic,
    “On mutual information maximization for representation learning,” in *ICLR*, 2019.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola, “What
    makes for good views for contrastive learning,” in *NeurIPS*, 2020.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] J. Huang, Q. Dong, S. Gong, and X. Zhu, “Unsupervised deep learning by
    neighbourhood discovery,” in *ICML*, 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] W. Van Gansbeke, S. Vandenhende, S. Georgoulis, M. Proesmans, and L. Van Gool,
    “Learning to classify images without labels,” in *ECCV*, 2020.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] M. Caron, P. Bojanowski, J. Mairal, and A. Joulin, “Unsupervised pre-training
    of image features on non-curated data,” in *ICCV*, 2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] D. Novotny, S. Albanie, D. Larlus, and A. Vedaldi, “Self-supervised learning
    of geometrically stable features through probabilistic introspection,” in *CVPR*,
    2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] M. Ye, X. Zhang, P. C. Yuen, and S.-F. Chang, “Unsupervised embedding
    learning via invariant and spreading instance feature,” in *CVPR*, 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with momentum
    contrastive learning,” *arXiv:2003.04297*, 2020.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] M. Gutmann and A. Hyvärinen, “Noise-contrastive estimation: A new estimation
    principle for unnormalized statistical models,” in *AISTATS*, 2010.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] M. Federici, A. Dutta, P. Forré, N. Kushman, and Z. Akata, “Learning
    robust representations via multi-view information bottleneck,” in *ICLR*, 2020.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive
    predictive coding,” *arXiv:1807.03748*, 2018.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] J. Robinson, C.-Y. Chuang, S. Sra, and S. Jegelka, “Contrastive learning
    with hard negative samples,” in *ICLR*, 2021.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Q. Hu, X. Wang, W. Hu, and G.-J. Qi, “Adco: Adversarial contrast for
    efficient learning of unsupervised representations from self-trained negative
    adversaries,” in *CVPR*, 2021.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] X. Wang, Y. Huang, D. Zeng, and G.-J. Qi, “Caco: Both positive and negative
    samples are directly learnable via cooperative-adversarial contrastive learning,”
    *arXiv:2203.14370*, 2022.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] J. Zbontar, L. Jing, I. Misra, Y. LeCun, and S. Deny, “Barlow twins:
    Self-supervised learning via redundancy reduction,” in *ICML*, 2021.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding for
    clustering analysis,” in *ICML*, 2016.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] W. Hu, T. Miyato, S. Tokui, E. Matsumoto, and M. Sugiyama, “Learning
    discrete representations via information maximizing self-augmented training,”
    in *ICML*, 2017.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] C. Zhuang, A. L. Zhai, and D. Yamins, “Local aggregation for unsupervised
    learning of visual embeddings,” in *CVPR*, 2019.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] X. Yan, I. Misra, A. Gupta, D. Ghadiyaram, and D. Mahajan, “Clusterfit:
    Improving generalization of visual representations,” in *CVPR*, 2020.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] X. Wang, Z. Liu, and S. X. Yu, “Unsupervised feature learning by cross-level
    discrimination between instances and groups,” in *CVPR*, 2021.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] S. Gidaris, A. Bursuc, N. Komodakis, P. Pérez, and M. Cord, “Learning
    representations by predicting bags of visual words,” in *CVPR*, 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] A. K. Jain, M. N. Murty, and P. J. Flynn, “Data clustering: a review,”
    *CSUR*, 1999.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A. Coates and A. Y. Ng, “Learning feature representations with k-means,”
    in *Neural networks: Tricks of the trade*, 2012.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] U. Von Luxburg, “A tutorial on spectral clustering,” *Statistics and
    computing*, 2007.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] J. Chang, L. Wang, G. Meng, S. Xiang, and C. Pan, “Deep adaptive image
    clustering,” in *CVPR*, 2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] X. Guo, L. Gao, X. Liu, and J. Yin, “Improved deep embedded clustering
    with local structure preservation,” in *IJCAI*, 2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong, “Towards k-means-friendly
    spaces: Simultaneous deep learning and clustering,” in *ICML*, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] K. C. Gowda and G. Krishna, “Agglomerative clustering using the concept
    of mutual nearest neighbourhood,” *PR*, 1978.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] M. Cuturi, “Sinkhorn distances: Lightspeed computation of optimal transport,”
    in *NeurIPS*, 2013.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Y. Li, P. Hu, Z. Liu, D. Peng, J. T. Zhou, and X. Peng, “Contrastive
    clustering,” in *AAAI*, 2020.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm
    for deep belief nets,” *Neural computation*, 2006.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. Gabbay and Y. Hoshen, “Demystifying inter-class disentanglement,”
    in *ICLR*, 2020.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Donahue, P. Krähenbühl, and T. Darrell, “Adversarial feature learning,”
    in *ICLR*, 2017.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] G.-J. Qi, L. Zhang, C. W. Chen, and Q. Tian, “Avt: Unsupervised learning
    of transformation equivariant representations by autoencoding variational transformations,”
    in *ICCV*, 2019.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] L. Zhang, G.-J. Qi, L. Wang, and J. Luo, “Aet vs. aed: Unsupervised representation
    learning by auto-encoding transformations rather than data,” in *CVPR*, 2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] X. Chen, Y. Yuan, G. Zeng, and J. Wang, “Semi-supervised semantic segmentation
    with cross pseudo supervision,” in *CVPR*, 2021.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] J. Jeong, S. Lee, J. Kim, and N. Kwak, “Consistency-based semi-supervised
    learning for object detection,” in *NeurIPS*, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Y. Sun, E. Tzeng, T. Darrell, and A. A. Efros, “Unsupervised domain adaptation
    through self-supervision,” *arXiv:1909.11825*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] K. Saito, D. Kim, S. Sclaroff, and K. Saenko, “Universal domain adaptation
    through self supervision,” in *NeurIPS*, 2020.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] L. Yang, S. Chen, and A. Yao, “Semihand: Semi-supervised hand pose estimation
    with consistency,” in *ICCV*, 2021.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] T. Kim, J. Choi, S. Choi, D. Jung, and C. Kim, “Just a few points are
    all you need for multi-view stereo: A novel semi-supervised learning method for
    multi-view stereo,” in *ICCV*, 2021.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] H. Alwassel, D. Mahajan, B. Korbar, L. Torresani, B. Ghanem, and D. Tran,
    “Self-supervised learning by cross-modal audio-video clustering,” in *NeurIPS*,
    2020.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *MICCAI*, 2015.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] X. Huo, L. Xie, J. He, Z. Yang, W. Zhou, H. Li, and Q. Tian, “Atso: Asynchronous
    teacher-student optimization for semi-supervised image segmentation,” in *CVPR*,
    2021.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] H. Wu, G. Chen, Z. Wen, and J. Qin, “Collaborative and adversarial learning
    of focused and dispersive representations for semi-supervised polyp segmentation,”
    in *ICCV*, 2021.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] H. Huang, L. Lin, Y. Zhang, Y. Xu, J. Zheng, X. Mao, X. Qian, Z. Peng,
    J. Zhou, Y.-W. Chen *et al.*, “Graph-bas3net: Boundary-aware semi-supervised segmentation
    network with bilateral graph convolution,” in *ICCV*, 2021.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *CVPR*, 2016.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, “Denseaspp for semantic
    segmentation in street scenes,” in *CVPR*, 2018.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    and J. Gall, “Semantickitti: A dataset for semantic scene understanding of lidar
    sequences,” in *ICCV*, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] G. French, T. Aila, S. Laine, M. Mackiewicz, and G. Finlayson, “Semi-supervised
    semantic segmentation needs strong, high-dimensional perturbations,” in *BMVC*,
    2020.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Y. Ouali, C. Hudelot, and M. Tami, “Semi-supervised semantic segmentation
    with cross-consistency training,” in *CVPR*, 2020.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Z. Ke, D. Qiu, K. Li, Q. Yan, and R. W. Lau, “Guided collaborative training
    for pixel-wise semi-supervised learning,” in *ECCV*, 2020.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] H. Hu, F. Wei, H. Hu, Q. Ye, J. Cui, and L. Wang, “Semi-supervised semantic
    segmentation via adaptive equalization learning,” in *NeurIPS*, 2021.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] R. Mendel, L. A. De Souza, D. Rauber, J. P. Papa, and C. Palm, “Semi-supervised
    segmentation based on error-correcting supervision,” in *ECCV*, 2020.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Y. Zou, Z. Zhang, H. Zhang, C.-L. Li, X. Bian, J.-B. Huang, and T. Pfister,
    “Pseudoseg: Designing pseudo labels for semantic segmentation,” in *ICLR*, 2021.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] M. S. Ibrahim, A. Vahdat, M. Ranjbar, and W. G. Macready, “Semi-supervised
    semantic image segmentation with self-correcting networks,” in *CVPR*, 2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] R. He, J. Yang, and X. Qi, “Re-distributing biased pseudo labels for
    semi-supervised semantic segmentation: A baseline investigation,” in *ICCV*, 2021.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] J. Yuan, Y. Liu, C. Shen, Z. Wang, and H. Li, “A simple baseline for
    semi-supervised semantic segmentation with strong data augmentation,” in *ICCV*,
    2021.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] N. Souly, C. Spampinato, and M. Shah, “Semi supervised semantic segmentation
    using generative adversarial network,” in *ICCV*, 2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] W.-C. Hung, Y.-H. Tsai, Y.-T. Liou, Y.-Y. Lin, and M.-H. Yang, “Adversarial
    learning for semi-supervised semantic segmentation,” in *BMVC*, 2018.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] S. Mittal, M. Tatarchenko, and T. Brox, “Semi-supervised semantic segmentation
    with high-and low-level consistency,” *IEEE TPAMI*, 2021.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] X. Lai, Z. Tian, L. Jiang, S. Liu, H. Zhao, L. Wang, and J. Jia, “Semi-supervised
    semantic segmentation with directional context-aware consistency,” in *CVPR*,
    2021.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] I. Alonso, A. Sabater, D. Ferstl, L. Montesano, and A. C. Murillo, “Semi-supervised
    semantic segmentation with pixel-level contrastive learning from a class-wise
    memory bank,” in *ICCV*, 2021.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Y. Zhong, B. Yuan, H. Wu, Z. Yuan, J. Peng, and Y.-X. Wang, “Pixel contrastive-consistent
    semi-supervised semantic segmentation,” in *ICCV*, 2021.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Y. Zhou, H. Xu, W. Zhang, B. Gao, and P.-A. Heng, “C3-semiseg: Contrastive
    semi-supervised segmentation via cross-set learning and dynamic class-balancing,”
    in *ICCV*, 2021.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, “Cutmix: Regularization
    strategy to train strong classifiers with localizable features,” in *ICCV*, 2019.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
    “End-to-end object detection with transformers,” in *ECCV*, 2020.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, “Joint detection and identification
    feature learning for person search,” in *CVPR*, 2017.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] K. Qian, S. Zhu, X. Zhang, and L. E. Li, “Robust multimodal vehicle detection
    in foggy weather using complementary lidar and radar signals,” in *CVPR*, 2021.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] H. Su, S. Gong, and X. Zhu, “Multi-perspective cross-class domain adaptation
    for open logo detection,” *CVIU*, 2021.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] W. Feng, F. Yin, X.-Y. Zhang, and C.-L. Liu, “Semantic-aware video text
    detection,” in *CVPR*, 2021.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] O. Russakovsky, L.-J. Li, and L. Fei-Fei, “Best of both worlds: human-machine
    collaboration for object annotation,” in *CVPR*, 2015.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] J. Jeong, V. Verma, M. Hyun, J. Kannala, and N. Kwak, “Interpolation-based
    semi-supervised learning for object detection,” in *CVPR*, 2021.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Y.-C. Liu, C.-Y. Ma, Z. He, C.-W. Kuo, K. Chen, P. Zhang, B. Wu, Z. Kira,
    and P. Vajda, “Unbiased teacher for semi-supervised object detection,” in *ICLR*,
    2021.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Q. Zhou, C. Yu, Z. Wang, Q. Qian, and H. Li, “Instant-teaching: An end-to-end
    semi-supervised object detection framework,” in *CVPR*, 2021.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] M. Xu, Z. Zhang, H. Hu, J. Wang, L. Wang, F. Wei, X. Bai, and Z. Liu,
    “End-to-end semi-supervised object detection with soft teacher,” in *ICCV*, 2021.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] K. Sohn, Z. Zhang, C.-L. Li, H. Zhang, C.-Y. Lee, and T. Pfister, “A
    simple semi-supervised learning framework for object detection,” *arXiv:2005.04757*,
    2020.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Q. Yang, X. Wei, B. Wang, X.-S. Hua, and L. Zhang, “Interactive self-training
    with mean teachers for semi-supervised object detection,” in *CVPR*, 2021.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Z. Wang, Y. Li, Y. Guo, L. Fang, and S. Wang, “Data-uncertainty guided
    multi-phase learning for semi-supervised object detection,” in *CVPR*, 2021.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] Z. Wang, Y. Li, Y. Guo, and S. Wang, “Combating noise: Semi-supervised
    learning by region uncertainty quantification,” in *NeurIPS*, 2021.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual category
    models to new domains,” in *ECCV*, 2010.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Y. Wu, D. Inkpen, and A. El-Roby, “Dual mixup regularized learning for
    adversarial domain adaptation,” in *ECCV*, 2020.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] G. French, M. Mackiewicz, and M. Fisher, “Self-ensembling for visual
    domain adaptation,” in *ICLR*, 2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Z. Deng, Y. Luo, and J. Zhu, “Cluster alignment with a teacher for unsupervised
    domain adaptation,” in *ICCV*, 2019.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] F. M. Carlucci, L. Porzi, B. Caputo, E. Ricci, and S. R. Bulo, “Autodial:
    Automatic domain alignment layers,” in *ICCV*, 2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] P. Morerio, J. Cavazza, and V. Murino, “Minimal-entropy correlation alignment
    for unsupervised deep domain adaptation,” in *ICLR*, 2018.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] L. Hu, M. Kan, S. Shan, and X. Chen, “Unsupervised domain adaptation
    with hierarchical gradient synchronization,” in *CVPR*, 2020.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] K. Saito, Y. Ushiku, and T. Harada, “Asymmetric tri-training for unsupervised
    domain adaptation,” in *ICML*, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, “Unsupervised
    pixel-level domain adaptation with generative adversarial networks,” in *CVPR*,
    2017.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] S. Sankaranarayanan, Y. Balaji, A. Jain, S. Nam Lim, and R. Chellappa,
    “Learning from synthetic data: Addressing domain shift for semantic segmentation,”
    in *CVPR*, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    and T. Darrell, “CyCADA: Cycle-consistent adversarial domain adaptation,” in *ICML*,
    2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] P. Russo, F. M. Carlucci, T. Tommasi, and B. Caputo, “From source to
    target and back: symmetric bi-directional adaptive gan,” in *CVPR*, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim, “Image
    to image translation for domain adaptation,” in *CVPR*, 2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] Y.-C. Chen, Y.-Y. Lin, M.-H. Yang, and J.-B. Huang, “Crdoco: Pixel-level
    domain transfer with cross-domain consistency,” in *CVPR*, 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon, “Pixel-level domain
    transfer,” in *ECCV*, 2016.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb,
    “Learning from simulated and unsupervised images through adversarial training.”
    in *CVPR*, 2017.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] J. Xu, L. Xiao, and A. M. López, “Self-supervised domain adaptation for
    computer vision tasks,” *IEEE Access*, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] S. Bucci, A. D’Innocente, Y. Liao, F. M. Carlucci, B. Caputo, and T. Tommasi,
    “Self-supervised learning across domains,” *IEEE TPAMI*, 2020.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] P. Su, S. Tang, P. Gao, D. Qiu, N. Zhao, and X. Wang, “Gradient regularized
    contrastive learning for continual domain adaptation,” in *AAAI*, 2021.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] R. Wang, Z. Wu, Z. Weng, J. Chen, G.-J. Qi, and Y.-G. Jiang, “Cross-domain
    contrastive learning for unsupervised domain adaptation,” *IEEE Transactions on
    Multimedia*, 2022.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Q. Yu, D. Ikami, G. Irie, and K. Aizawa, “Multi-task curriculum framework
    for open-set semi-supervised learning,” in *ECCV*, 2020.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] M. Augustin and M. Hein, “Out-distribution aware self-training in an
    open world setting,” *arXiv:2012.12372*, 2020.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] J. Huang, C. Fang, W. Chen, Z. Chai, X. Wei, P. Wei, L. Lin, and G. Li,
    “Trash to treasure: Harvesting ood data with cross-modal matching for open-set
    semi-supervised learning,” in *ICCV*, 2021.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] K. Saito, D. Kim, and K. Saenko, “Openmatch: Open-set consistency regularization
    for semi-supervised learning with outliers,” in *NeurIPS*, 2021.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Z. Huang, C. Xue, B. Han, J. Yang, and C. Gong, “Universal semi-supervised
    learning,” in *Thirty-Fifth Conference on Neural Information Processing Systems*,
    2021.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] K. Cao, M. Brbic, and J. Leskovec, “Open-world semi-supervised learning,”
    in *ICLR*, 2022.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] D. Hendrycks and K. Gimpel, “A baseline for detecting misclassified and
    out-of-distribution examples in neural networks,” in *ICLR*, 2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] Z. Zhong, L. Zhu, Z. Luo, S. Li, Y. Yang, and N. Sebe, “Openmix: Reviving
    known knowledge for discovering novel visual categories in an open world,” in
    *CVPR*, 2021.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] D. Hendrycks, M. Mazeika, and T. Dietterich, “Deep anomaly detection
    with outlier exposure,” in *ICLR*, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] K. Lee, K. Lee, H. Lee, and J. Shin, “A simple unified framework for
    detecting out-of-distribution samples and adversarial attacks,” in *NeurIPS*,
    2018.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] M. Hein, M. Andriushchenko, and J. Bitterwolf, “Why relu networks yield
    high-confidence predictions far away from the training data and how to mitigate
    the problem,” in *CVPR*, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] J. Kim, Y. Hur, S. Park, E. Yang, S. J. Hwang, and J. Shin, “Distribution
    aligning refinery of pseudo-label for imbalanced semi-supervised learning,” in
    *NeurIPS*, 2020.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] H. Lee, S. Shin, and H. Kim, “Abc: Auxiliary balanced classifier for
    class-imbalanced semi-supervised learning,” in *NeurIPS*, 2021.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] K. Han, A. Vedaldi, and A. Zisserman, “Learning to discover novel visual
    categories via deep transfer clustering,” in *ICCV*, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] K. Han, S.-A. Rebuffi, S. Ehrhardt, A. Vedaldi, and A. Zisserman, “Automatically
    discovering and learning new visual categories with ranking statistics,” in *ICLR*,
    2020.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] K. Lee, K. Lee, J. Shin, and H. Lee, “Overcoming catastrophic forgetting
    with unlabeled data in the wild,” in *ICCV*, 2019.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] D. Rao, F. Visin, A. A. Rusu, R. Pascanu, Y. W. Teh, and R. Hadsell,
    “Continual unsupervised representation learning,” in *NeurIPS*, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,
    G. Slabaugh, and T. Tuytelaars, “A continual learning survey: Defying forgetting
    in classification tasks,” *IEEE TPAMI*, 2021.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] M. McCloskey and N. J. Cohen, “Catastrophic interference in connectionist
    networks: The sequential learning problem,” in *Psychology of learning and motivation*,
    1989.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl: Incremental
    classifier and representation learning,” in *CVPR*, 2017.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
    A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska *et al.*, “Overcoming
    catastrophic forgetting in neural networks,” *PNAS*, 2017.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr, “Riemannian
    walk for incremental learning: Understanding forgetting and intransigence,” in
    *ECCV*, 2018.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars,
    “Memory aware synapses: Learning what (not) to forget,” in *ECCV*, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] J. Zhang, J. Zhang, S. Ghosh, D. Li, S. Tasci, L. Heck, H. Zhang, and
    C.-C. J. Kuo, “Class-incremental learning via deep model consolidation,” in *WACV*,
    2020.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Y. Li, Y. Wang, Q. Liu, C. Bi, X. Jiang, and S. Sun, “Incremental semi-supervised
    learning on streaming data,” *PR*, 2019.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] J. Smith, S. Baer, Z. Kira, and C. Dovrolis, “Unsupervised continual
    learning and self-taught associative memory hierarchies,” in *ICLRW*, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt, “Test-time
    training with self-supervision for generalization under distribution shifts,”
    in *ICML*, 2020.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] T. Varsavsky, M. Orbes-Arteaga, C. H. Sudre, M. S. Graham, P. Nachev,
    and M. J. Cardoso, “Test-time unsupervised domain adaptation,” in *MICCAI*, 2020.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell, “Tent: Fully
    test-time adaptation by entropy minimiaztion,” in *ICLR*, 2021.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] J. Hoffman, T. Darrell, and K. Saenko, “Continuous manifold based adaptation
    for evolving visual domains,” in *CVPR*, 2014.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] A. Bobu, E. Tzeng, J. Hoffman, and T. Darrell, “Adapting to continuously
    shifting domains,” in *ICLRW*, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] E. Arazo, D. Ortego, P. Albert, N. E. O’Connor, and K. McGuinness, “Pseudo-labeling
    and confirmation bias in deep semi-supervised learning,” in *IJCNN*, 2020.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-agnostic
    visiolinguistic representations for vision-and-language tasks,” in *NeurIPS*,
    2019.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder representations
    from transformers,” in *ACL*, 2019.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “Vl-bert: Pre-training
    of generic visual-linguistic representations,” in *ICLR*, 2019.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and
    J. Liu, “Uniter: Universal image-text representation learning,” in *ECCV*, 2020.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] G. Li, N. Duan, Y. Fang, M. Gong, D. Jiang, and M. Zhou, “Unicoder-vl:
    A universal encoder for vision and language by cross-modal pre-training.” in *AAAI*,
    2020.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, 2017.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *ACL*, 2019.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, “Videobert:
    A joint model for video and language representation learning,” in *ICCV*, 2019.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] J. C. Stroud, D. A. Ross, C. Sun, J. Deng, R. Sukthankar, and C. Schmid,
    “Learning video representations from textual web supervision,” *arXiv:2007.14937*,
    2020.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] J.-B. Alayrac, A. Recasens, R. Schneider, R. Arandjelović, J. Ramapuram,
    J. De Fauw, L. Smaira, S. Dieleman, and A. Zisserman, “Self-supervised multimodal
    versatile networks,” in *NeurIPS*, 2020.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zisserman,
    “End-to-end learning of visual representations from uncurated instructional videos,”
    in *CVPR*, 2020.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] A. Rouditchenko, A. Boggust, D. Harwath, D. Joshi, S. Thomas, K. Audhkhasi,
    R. Feris, B. Kingsbury, M. Picheny, A. Torralba *et al.*, “Avlnet: Learning audio-visual
    language representations from instructional videos,” *arXiv:2006.09199*, 2020.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic,
    “Howto100m: Learning a text-video embedding by watching hundred million narrated
    video clips,” in *ICCV*, 2019.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *ICML*, 2021.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung,
    Z. Li, and T. Duerig, “Scaling up visual and vision-language representation learning
    with noisy text supervision,” in *ICML*, 2021.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] R. Arandjelovic and A. Zisserman, “Look, listen and learn,” in *CVPR*,
    2017.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] A. Owens and A. A. Efros, “Audio-visual scene analysis with self-supervised
    multisensory features,” in *ECCV*, 2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] B. Korbar, D. Tran, and L. Torresani, “Cooperative learning of audio
    and video models from self-supervised synchronization,” in *NeurIPS*, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] P. Morgado, N. Vasconcelos, and I. Misra, “Audio-visual instance discrimination
    with cross-modal agreement,” in *CVPR*, 2021.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] M. Patrick, Y. M. Asano, R. Fong, J. F. Henriques, G. Zweig, and A. Vedaldi,
    “Multi-modal self-supervision from generalized data transformations,” *arXiv:2003.04298*,
    2020.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and A. Torralba, “Ambient
    sound provides supervision for visual learning,” in *ECCV*, 2016.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] P. Hu, H. Zhu, X. Peng, and J. Lin, “Semi-supervised multi-modal learning
    with balanced spectral decomposition,” in *AAAI*, 2020.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] S. Li, B. Xie, J. Wu, Y. Zhao, C. H. Liu, and Z. Ding, “Simultaneous
    semantic alignment network for heterogeneous domain adaptation,” in *ACM MM*,
    2020.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] Y. Chen, Y. Xian, A. S. Koepke, Y. Shan, and Z. Akata, “Distilling audio-visual
    knowledge by compositional contrastive learning,” in *CVPR*, 2021.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
