- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 20:09:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 20:09:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1702.00764] Symbolic, Distributed and Distributional Representations for Natural
    Language Processing in the Era of Deep Learning: a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1702.00764] ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒå¼è¡¨ç¤ºåœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è°ƒæŸ¥'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/1702.00764](https://ar5iv.labs.arxiv.org/html/1702.00764)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/1702.00764](https://ar5iv.labs.arxiv.org/html/1702.00764)
- en: \correspondance\extraAuth
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \correspondance\extraAuth
- en: 'Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒå¼è¡¨ç¤ºåœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è°ƒæŸ¥
- en: Lorenzo Ferroneâ€‰Â¹ and Fabio Massimo Zanzottoâ€‰^(1,âˆ—)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ´›ä¼¦ä½Â·è´¹ç½—å†…Â¹ å’Œ æ³•æ¯”å¥¥Â·é©¬è¥¿è«Â·èµä½æ‰˜^(1,âˆ—)
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: '1'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '1'
- en: 'Natural language is inherently a discrete symbolic representation of human
    knowledge. Recent advances in machine learning (ML) and in natural language processing
    (NLP) seem to contradict the above intuition: discrete symbols are fading away,
    erased by vectors or tensors called *distributed* and *distributional representations*.
    However, there is a strict link between distributed/distributional representations
    and discrete symbols, being the first an approximation of the second. A clearer
    understanding of the strict link between distributed/distributional representations
    and symbols may certainly lead to radically new deep learning networks. In this
    paper we make a survey that aims to renew the link between symbolic representations
    and distributed/distributional representations. This is the right time to revitalize
    the area of interpreting how discrete symbols are represented inside neural networks.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€æœ¬è´¨ä¸Šæ˜¯äººç±»çŸ¥è¯†çš„ç¦»æ•£ç¬¦å·è¡¨ç¤ºã€‚æœ€è¿‘åœ¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ–¹é¢çš„è¿›å±•ä¼¼ä¹ä¸ä¸Šè¿°ç›´è§‰ç›¸çŸ›ç›¾ï¼šç¦»æ•£ç¬¦å·æ­£åœ¨é€æ¸æ¶ˆå¤±ï¼Œè¢«ç§°ä¸º*åˆ†å¸ƒå¼*å’Œ*åˆ†å¸ƒå¼è¡¨ç¤º*çš„å‘é‡æˆ–å¼ é‡æ‰€å–ä»£ã€‚ç„¶è€Œï¼Œåˆ†å¸ƒå¼/åˆ†å¸ƒå¼è¡¨ç¤ºå’Œç¦»æ•£ç¬¦å·ä¹‹é—´å­˜åœ¨ä¸¥æ ¼çš„è”ç³»ï¼Œå‰è€…æ˜¯åè€…çš„è¿‘ä¼¼ã€‚å¯¹åˆ†å¸ƒå¼/åˆ†å¸ƒå¼è¡¨ç¤ºå’Œç¬¦å·ä¹‹é—´ä¸¥æ ¼è”ç³»çš„æ›´æ¸…æ™°ç†è§£ï¼Œå¯èƒ½ä¼šå¯¼è‡´å…¨æ–°æ·±åº¦å­¦ä¹ ç½‘ç»œçš„è¯ç”Ÿã€‚æœ¬æ–‡å¯¹ç¬¦å·è¡¨ç¤ºå’Œåˆ†å¸ƒå¼/åˆ†å¸ƒå¼è¡¨ç¤ºä¹‹é—´çš„è”ç³»è¿›è¡Œäº†ç»¼è¿°ã€‚ç°åœ¨æ­£æ˜¯é‡æŒ¯è§£é‡Šç¦»æ•£ç¬¦å·å¦‚ä½•åœ¨ç¥ç»ç½‘ç»œå†…éƒ¨è¡¨ç¤ºçš„é¢†åŸŸçš„æœ€ä½³æ—¶æœºã€‚
- en: \helveticabold
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \helveticabold
- en: '2 Keywords:'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 å…³é”®å­—ï¼š
- en: keyword, keyword, keyword, keyword, keyword, keyword, keyword, keyword
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®å­—ï¼Œå…³é”®å­—ï¼Œå…³é”®å­—ï¼Œå…³é”®å­—ï¼Œå…³é”®å­—ï¼Œå…³é”®å­—ï¼Œå…³é”®å­—ï¼Œå…³é”®å­—
- en: 3 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 å¼•è¨€
- en: Natural language is inherently a discrete symbolic representation of human knowledge.
    Sounds are transformed in letters or ideograms and these discrete symbols are
    composed to obtain words. Words then form sentences and sentences form texts,
    discourses, dialogs, which ultimately convey knowledge, emotions, and so on. This
    composition of symbols in words and of words in sentences follow rules that both
    the hearer and the speaker know (Chomsky, [1957](#bib.bib13)). Hence, thinking
    to natural language understanding systems, which are not based on discrete symbols,
    seems to be extremely odd.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€æœ¬è´¨ä¸Šæ˜¯äººç±»çŸ¥è¯†çš„ç¦»æ•£ç¬¦å·è¡¨ç¤ºã€‚å£°éŸ³è¢«è½¬åŒ–ä¸ºå­—æ¯æˆ–è¡¨æ„ç¬¦å·ï¼Œè¿™äº›ç¦»æ•£ç¬¦å·è¢«ç»„åˆä»¥å½¢æˆå•è¯ã€‚å•è¯ç„¶åç»„æˆå¥å­ï¼Œå¥å­ç»„æˆæ–‡æœ¬ã€æ¼”è®²ã€å¯¹è¯ï¼Œæœ€ç»ˆä¼ è¾¾çŸ¥è¯†ã€æƒ…æ„Ÿç­‰ã€‚è¿™äº›ç¬¦å·åœ¨å•è¯ä¸­çš„ç»„åˆä»¥åŠå•è¯åœ¨å¥å­ä¸­çš„ç»„åˆéµå¾ªäº†å¬è€…å’Œè¯´è¯è€…éƒ½çŸ¥é“çš„è§„åˆ™ï¼ˆä¹”å§†æ–¯åŸºï¼Œ[1957](#bib.bib13)ï¼‰ã€‚å› æ­¤ï¼Œè€ƒè™‘é‚£äº›ä¸åŸºäºç¦»æ•£ç¬¦å·çš„è‡ªç„¶è¯­è¨€ç†è§£ç³»ç»Ÿä¼¼ä¹æå…¶å¥‡æ€ªã€‚
- en: 'Recent advances in machine learning (ML) applied to natural language processing
    (NLP) seem to contradict the above intuition: discrete symbols are fading away,
    erased by vectors or tensors called *distributed* and *distributional representations*.
    In ML applied to NLP, *distributed representations* are pushing deep learning
    models (LeCun etÂ al., [2015](#bib.bib44); Schmidhuber, [2015](#bib.bib62)) towards
    amazing results in many high-level tasks such as image generation (Goodfellow
    etÂ al., [2014](#bib.bib29)), image captioning (Vinyals etÂ al., [2015b](#bib.bib73);
    Xu etÂ al., [2015](#bib.bib76)), machine translation (Bahdanau etÂ al., [2014](#bib.bib3);
    Zou etÂ al., [2013](#bib.bib82)), syntactic parsing (Vinyals etÂ al., [2015a](#bib.bib72);
    Weiss etÂ al., [2015](#bib.bib74)) and in a variety of other NLP tasks Devlin etÂ al.
    ([2018](#bib.bib19)). In a more traditional NLP, *distributional representations*
    are pursued as a more flexible way to represent semantics of natural language,
    the so-called *distributional semantics* (see (Turney and Pantel, [2010](#bib.bib68))).
    Words as well as sentences are represented as vectors or tensors of real numbersVectors
    for words are obtained observing how rhese words co-occur with other words in
    document collections. Moreover, as in traditional compositional representations,
    vectors for phrases (Mitchell and Lapata, [2008](#bib.bib50); Baroni and Zamparelli,
    [2010](#bib.bib6); Clark etÂ al., [2008](#bib.bib14); Grefenstette and Sadrzadeh,
    [2011](#bib.bib31); Zanzotto etÂ al., [2010](#bib.bib79)) and sentences (Socher
    etÂ al., [2011](#bib.bib64), [2012](#bib.bib65); Kalchbrenner and Blunsom, [2013](#bib.bib40))
    are obtained by composing vectors for words.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘æœŸåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­åº”ç”¨çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰è¿›å±•ä¼¼ä¹ä¸ä¸Šè¿°ç›´è§‰ç›¸çŸ›ç›¾ï¼šç¦»æ•£ç¬¦å·æ­£åœ¨æ¶ˆé€€ï¼Œè¢«ç§°ä¸º*åˆ†å¸ƒå¼*å’Œ*åˆ†å¸ƒè¡¨å¾*çš„å‘é‡æˆ–å¼ é‡æ‰€å–ä»£ã€‚åœ¨åº”ç”¨äºNLPçš„MLä¸­ï¼Œ*åˆ†å¸ƒå¼è¡¨å¾*æ¨åŠ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆLeCunç­‰ï¼Œ[2015](#bib.bib44)ï¼›Schmidhuberï¼Œ[2015](#bib.bib62)ï¼‰åœ¨è®¸å¤šé«˜çº§ä»»åŠ¡ä¸­å–å¾—äº†æƒŠäººçš„ç»“æœï¼Œå¦‚å›¾åƒç”Ÿæˆï¼ˆGoodfellowç­‰ï¼Œ[2014](#bib.bib29)ï¼‰ï¼Œå›¾åƒæè¿°ï¼ˆVinyalsç­‰ï¼Œ[2015b](#bib.bib73)ï¼›Xuç­‰ï¼Œ[2015](#bib.bib76)ï¼‰ï¼Œæœºå™¨ç¿»è¯‘ï¼ˆBahdanauç­‰ï¼Œ[2014](#bib.bib3)ï¼›Zouç­‰ï¼Œ[2013](#bib.bib82)ï¼‰ï¼Œå¥æ³•åˆ†æï¼ˆVinyalsç­‰ï¼Œ[2015a](#bib.bib72)ï¼›Weissç­‰ï¼Œ[2015](#bib.bib74)ï¼‰ä»¥åŠå…¶ä»–å„ç§NLPä»»åŠ¡ï¼ˆDevlinç­‰ï¼Œ[2018](#bib.bib19)ï¼‰ã€‚åœ¨æ›´ä¼ ç»Ÿçš„NLPä¸­ï¼Œ*åˆ†å¸ƒè¡¨å¾*è¢«è§†ä¸ºè¡¨ç¤ºè‡ªç„¶è¯­è¨€è¯­ä¹‰çš„æ›´çµæ´»æ–¹å¼ï¼Œå³æ‰€è°“çš„*åˆ†å¸ƒè¯­ä¹‰*ï¼ˆè§ï¼ˆTurneyå’ŒPantelï¼Œ[2010](#bib.bib68)ï¼‰ï¼‰ã€‚å•è¯å’Œå¥å­éƒ½è¢«è¡¨ç¤ºä¸ºå®æ•°çš„å‘é‡æˆ–å¼ é‡ã€‚å•è¯çš„å‘é‡é€šè¿‡è§‚å¯Ÿè¿™äº›å•è¯åœ¨æ–‡æ¡£é›†åˆä¸­ä¸å…¶ä»–å•è¯çš„å…±ç°æƒ…å†µæ¥è·å¾—ã€‚æ­¤å¤–ï¼Œåƒä¼ ç»Ÿç»„åˆè¡¨å¾ä¸€æ ·ï¼ŒçŸ­è¯­ï¼ˆMitchellå’ŒLapataï¼Œ[2008](#bib.bib50)ï¼›Baroniå’ŒZamparelliï¼Œ[2010](#bib.bib6)ï¼›Clarkç­‰ï¼Œ[2008](#bib.bib14)ï¼›Grefenstetteå’ŒSadrzadehï¼Œ[2011](#bib.bib31)ï¼›Zanzottoç­‰ï¼Œ[2010](#bib.bib79)ï¼‰å’Œå¥å­çš„å‘é‡ï¼ˆSocherç­‰ï¼Œ[2011](#bib.bib64)ï¼Œ[2012](#bib.bib65)ï¼›Kalchbrennerå’ŒBlunsomï¼Œ[2013](#bib.bib40)ï¼‰æ˜¯é€šè¿‡ç»„åˆå•è¯çš„å‘é‡æ¥è·å¾—çš„ã€‚
- en: The success of distributed and distributional representations over symbolic
    approaches is mainly due to the advent of new parallel paradigms that pushed neural
    networks (Rosenblatt, [1958](#bib.bib58); Werbos, [1974](#bib.bib75)) towards
    deep learning (LeCun etÂ al., [2015](#bib.bib44); Schmidhuber, [2015](#bib.bib62)).
    Massively parallel algorithms running on Graphic Processing Units (GPUs) (Chetlur
    etÂ al., [2014](#bib.bib12); Cui etÂ al., [2015](#bib.bib16)) crunch vectors, matrices
    and tensors faster than decades ago. The back-propagation algorithm can be now
    computed for complex and large neural networks. Symbols are not needed any more
    during â€œresoningâ€, that is, the neural network learning and its application. Hence,
    discrete symbols only survive as inputs and outputs of these wonderful learning
    machines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼å’Œåˆ†å¸ƒè¡¨å¾åœ¨ç¬¦å·æ–¹æ³•ä¸Šçš„æˆåŠŸä¸»è¦å½’å› äºæ–°å¹¶è¡ŒèŒƒå¼çš„å‡ºç°ï¼Œè¿™æ¨åŠ¨äº†ç¥ç»ç½‘ç»œï¼ˆRosenblattï¼Œ[1958](#bib.bib58)ï¼›Werbosï¼Œ[1974](#bib.bib75)ï¼‰å‘æ·±åº¦å­¦ä¹ ï¼ˆLeCunç­‰ï¼Œ[2015](#bib.bib44)ï¼›Schmidhuberï¼Œ[2015](#bib.bib62)ï¼‰çš„å‘å±•ã€‚è¿è¡Œåœ¨å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰ï¼ˆChetlurç­‰ï¼Œ[2014](#bib.bib12)ï¼›Cuiç­‰ï¼Œ[2015](#bib.bib16)ï¼‰ä¸Šçš„å¤§è§„æ¨¡å¹¶è¡Œç®—æ³•æ¯”å‡ åå¹´å‰æ›´å¿«åœ°å¤„ç†å‘é‡ã€çŸ©é˜µå’Œå¼ é‡ã€‚ç°åœ¨å¯ä»¥ä¸ºå¤æ‚çš„å¤§å‹ç¥ç»ç½‘ç»œè®¡ç®—åå‘ä¼ æ’­ç®—æ³•ã€‚åœ¨â€œæ¨ç†â€è¿‡ç¨‹ä¸­ï¼Œå³ç¥ç»ç½‘ç»œçš„å­¦ä¹ å’Œåº”ç”¨ä¸­ï¼Œå·²ä¸å†éœ€è¦ç¬¦å·ã€‚å› æ­¤ï¼Œç¦»æ•£ç¬¦å·ä»…ä½œä¸ºè¿™äº›ç¥å¥‡å­¦ä¹ æœºå™¨çš„è¾“å…¥å’Œè¾“å‡ºè€Œå­˜åœ¨ã€‚
- en: However, there is a strict link between distributed/distributional representations
    and symbols, being the first an approximation of the second (Fodor and Pylyshyn,
    [1988](#bib.bib24); Plate, [1994](#bib.bib56), [1995](#bib.bib57); Ferrone etÂ al.,
    [2015](#bib.bib21)). The representation of the input and the output of these networks
    is not that far from their internal representation. The similarity and the interpretation
    of the internal representation is clearer in image processing (Zeiler and Fergus,
    [2014a](#bib.bib80)). In fact, networks are generally interpreted visualizing
    how subparts represent salient subparts of target images. Both input images and
    subparts are tensors of real number. Hence, these networks can be examined and
    understood. The same does not apply to natural language processing with its discrete
    symbols.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåˆ†å¸ƒå¼/åˆ†é…å¼è¡¨ç¤ºä¸ç¬¦å·ä¹‹é—´å­˜åœ¨ä¸¥æ ¼çš„è”ç³»ï¼Œç¬¬ä¸€ä¸ªæ˜¯å¯¹ç¬¬äºŒä¸ªçš„è¿‘ä¼¼ï¼ˆFodor å’Œ Pylyshynï¼Œ[1988](#bib.bib24)ï¼›Plateï¼Œ[1994](#bib.bib56)ï¼Œ[1995](#bib.bib57)ï¼›Ferrone
    ç­‰ï¼Œ[2015](#bib.bib21)ï¼‰ã€‚è¿™äº›ç½‘ç»œçš„è¾“å…¥å’Œè¾“å‡ºè¡¨ç¤ºä¸å…¶å†…éƒ¨è¡¨ç¤ºå¹¶æ²¡æœ‰å¤ªå¤§å·®åˆ«ã€‚å›¾åƒå¤„ç†ä¸­çš„å†…éƒ¨è¡¨ç¤ºçš„ç›¸ä¼¼æ€§å’Œè§£é‡Šæ›´ä¸ºæ¸…æ™°ï¼ˆZeiler å’Œ
    Fergusï¼Œ[2014a](#bib.bib80)ï¼‰ã€‚äº‹å®ä¸Šï¼Œç½‘ç»œé€šå¸¸é€šè¿‡å¯è§†åŒ–å­éƒ¨åˆ†å¦‚ä½•ä»£è¡¨ç›®æ ‡å›¾åƒçš„æ˜¾è‘—å­éƒ¨åˆ†æ¥è¿›è¡Œè§£é‡Šã€‚è¾“å…¥å›¾åƒå’Œå­éƒ¨åˆ†éƒ½æ˜¯å®æ•°å¼ é‡ã€‚å› æ­¤ï¼Œè¿™äº›ç½‘ç»œå¯ä»¥è¢«æ£€æŸ¥å’Œç†è§£ã€‚è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ç¦»æ•£ç¬¦å·åˆ™ä¸é€‚ç”¨è¿™ç§æƒ…å†µã€‚
- en: 'A clearer understanding of the strict link between distributed/distributional
    representations and discrete symbols is needed (Jang etÂ al., [2018](#bib.bib38);
    Jacovi etÂ al., [2018](#bib.bib37)) to understand how neural networks treat information
    and to propose novel deep learning architectures. Model interpretability is becoming
    an important topic in machine learning in general (Lipton, [2016](#bib.bib46)).
    This clearer understanding is then the dawn of a new range of possibilities: understanding
    what part of the current symbolic techniques for natural language processing have
    a sufficient representation in deep neural networks; and, ultimately, understanding
    whether a more brain-like model â€“ the neural networks â€“ is compatible with methods
    for syntactic parsing or semantic processing that have been defined in these decades
    of studies in computational linguistics and natural language processing. There
    is thus a tremendous opportunity to understand whether and how symbolic representations
    are used and emitted in a brain model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦å¯¹åˆ†å¸ƒå¼/åˆ†é…å¼è¡¨ç¤ºä¸ç¦»æ•£ç¬¦å·ä¹‹é—´çš„ä¸¥æ ¼è”ç³»æœ‰æ›´æ¸…æ™°çš„ç†è§£ï¼ˆJang ç­‰ï¼Œ[2018](#bib.bib38)ï¼›Jacovi ç­‰ï¼Œ[2018](#bib.bib37)ï¼‰ï¼Œä»¥ç†è§£ç¥ç»ç½‘ç»œå¦‚ä½•å¤„ç†ä¿¡æ¯ï¼Œå¹¶æå‡ºæ–°çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚æ¨¡å‹å¯è§£é‡Šæ€§æ­£æˆä¸ºæœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦è¯é¢˜ï¼ˆLiptonï¼Œ[2016](#bib.bib46)ï¼‰ã€‚è¿™ç§æ›´æ¸…æ™°çš„ç†è§£æ ‡å¿—ç€æ–°å¯èƒ½æ€§çš„æ›™å…‰ï¼šç†è§£å½“å‰è‡ªç„¶è¯­è¨€å¤„ç†ç¬¦å·æŠ€æœ¯çš„å“ªäº›éƒ¨åˆ†åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­æœ‰è¶³å¤Ÿçš„è¡¨ç¤ºï¼›ä»¥åŠï¼Œ*æœ€ç»ˆ*ï¼Œç†è§£æ›´ç±»ä¼¼å¤§è„‘çš„æ¨¡å‹â€”â€”ç¥ç»ç½‘ç»œâ€”â€”æ˜¯å¦ä¸è¿™äº›å‡ åå¹´æ¥åœ¨è®¡ç®—è¯­è¨€å­¦å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå®šä¹‰çš„å¥æ³•è§£ææˆ–è¯­ä¹‰å¤„ç†æ–¹æ³•å…¼å®¹ã€‚å› æ­¤ï¼Œäº†è§£ç¬¦å·è¡¨ç¤ºæ˜¯å¦ä»¥åŠå¦‚ä½•åœ¨å¤§è„‘æ¨¡å‹ä¸­ä½¿ç”¨å’Œå‘å°„æ˜¯ä¸€ä¸ªå·¨å¤§çš„æœºä¼šã€‚
- en: In this paper we make a survey that aims to draw the link between symbolic representations
    and distributed/distributional representations. This is the right time to revitalize
    the area of interpreting how symbols are represented inside neural networks. In
    our opinion, this survey will help to devise new deep neural networks that can
    exploit existing and novel symbolic models of classical natural language processing
    tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹è°ƒæŸ¥ï¼Œæ—¨åœ¨å»ºç«‹ç¬¦å·è¡¨ç¤ºä¸åˆ†å¸ƒå¼/åˆ†é…å¼è¡¨ç¤ºä¹‹é—´çš„è”ç³»ã€‚ç°åœ¨æ­£æ˜¯æŒ¯å…´è§£é‡Šç¬¦å·åœ¨ç¥ç»ç½‘ç»œå†…éƒ¨å¦‚ä½•è¡¨ç¤ºçš„é¢†åŸŸçš„æœ€ä½³æ—¶æœºã€‚åœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œè¿™é¡¹è°ƒæŸ¥å°†æœ‰åŠ©äºè®¾è®¡èƒ½å¤Ÿåˆ©ç”¨ç°æœ‰å’Œæ–°å‹ç¬¦å·æ¨¡å‹çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç”¨äºç»å…¸è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚
- en: 'The paper is structured as follow: first we give an introduction to the very
    general concept of representations and the difference between *local* and *distributed*
    representations (Plate, [1995](#bib.bib57)). After that we present each techniques
    in detail. Afterwards, we focus on distributional representations (Turney and
    Pantel, [2010](#bib.bib68)), which we treat as a specific example of a distributed
    representation. Finally we discuss more in depth the general issue of compositionality,
    analyzing three different approaches to the problem: compositional distributional
    semantics (Clark etÂ al., [2008](#bib.bib14); Baroni etÂ al., [2014](#bib.bib4)),
    holographic reduced representations (Plate, [1994](#bib.bib56); Neumann, [2001](#bib.bib53)),
    and recurrent neural networks (Kalchbrenner and Blunsom, [2013](#bib.bib40); Socher
    etÂ al., [2012](#bib.bib65)).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡çš„ç»“æ„å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»è¡¨ç¤ºçš„éå¸¸ä¸€èˆ¬çš„æ¦‚å¿µä»¥åŠ *å±€éƒ¨* å’Œ *åˆ†å¸ƒå¼* è¡¨ç¤ºä¹‹é—´çš„åŒºåˆ«ï¼ˆPlateï¼Œ[1995](#bib.bib57)ï¼‰ã€‚ç„¶åæˆ‘ä»¬è¯¦ç»†ä»‹ç»æ¯ç§æŠ€æœ¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡ç‚¹è®¨è®ºåˆ†å¸ƒå¼è¡¨ç¤ºï¼ˆTurney
    å’Œ Pantelï¼Œ[2010](#bib.bib68)ï¼‰ï¼Œæˆ‘ä»¬å°†å…¶è§†ä¸ºåˆ†å¸ƒå¼è¡¨ç¤ºçš„ä¸€ä¸ªç‰¹ä¾‹ã€‚æœ€åï¼Œæˆ‘ä»¬æ·±å…¥è®¨è®ºç»„åˆæ€§çš„æ€»ä½“é—®é¢˜ï¼Œåˆ†æäº†ä¸‰ç§ä¸åŒçš„è§£å†³æ–¹æ³•ï¼šç»„åˆåˆ†å¸ƒå¼è¯­ä¹‰ï¼ˆClark
    ç­‰ï¼Œ[2008](#bib.bib14)ï¼›Baroni ç­‰ï¼Œ[2014](#bib.bib4)ï¼‰ï¼Œå…¨æ¯å‡å°‘è¡¨ç¤ºï¼ˆPlateï¼Œ[1994](#bib.bib56)ï¼›Neumannï¼Œ[2001](#bib.bib53)ï¼‰ï¼Œä»¥åŠé€’å½’ç¥ç»ç½‘ç»œï¼ˆKalchbrenner
    å’Œ Blunsomï¼Œ[2013](#bib.bib40)ï¼›Socher ç­‰ï¼Œ[2012](#bib.bib65)ï¼‰ã€‚
- en: '4 Symbolic and Distributed Representations: Interpretability and *Concatenative*
    Compositionality'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¦å·ä¸åˆ†å¸ƒå¼è¡¨ç¤ºï¼šå¯è§£é‡Šæ€§å’Œ *è¿æ¥* ç»„åˆæ€§
- en: '*Distributed representations* put symbolic expressions in metric spaces where
    similarity among examples is used to learn regularities for specific tasks by
    using neural networks or other machine learning models. Given two symbolic expressions,
    their distributed representation should capture their similarity along specific
    features useful for the final task. For example, two sentences such as $s_{1}$=*â€œa
    mouse eats some cheeseâ€* and $s_{2}$=*â€œa cat swallows a mouseâ€* can be considered
    similar in many different ways: (1) number of words in common; (2) realization
    of the pattern â€œANIMAL EATS FOODâ€. The key point is to decide or to let an algorithm
    decide which is the best representation for a specific task.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ†å¸ƒå¼è¡¨ç¤º* å°†ç¬¦å·è¡¨è¾¾æ”¾ç½®åœ¨åº¦é‡ç©ºé—´ä¸­ï¼Œé€šè¿‡ä½¿ç”¨ç¥ç»ç½‘ç»œæˆ–å…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œåˆ©ç”¨ç¤ºä¾‹ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„è§„å¾‹ã€‚ç»™å®šä¸¤ä¸ªç¬¦å·è¡¨è¾¾ï¼Œå®ƒä»¬çš„åˆ†å¸ƒå¼è¡¨ç¤ºåº”æ•æ‰å®ƒä»¬åœ¨å¯¹æœ€ç»ˆä»»åŠ¡æœ‰ç”¨çš„ç‰¹å®šç‰¹å¾ä¸Šçš„ç›¸ä¼¼æ€§ã€‚ä¾‹å¦‚ï¼Œä¸¤ä¸ªå¥å­å¦‚
    $s_{1}$=*â€œä¸€åªè€é¼ åƒäº†ä¸€äº›å¥¶é…ªâ€* å’Œ $s_{2}$=*â€œä¸€åªçŒ«åä¸‹äº†ä¸€åªè€é¼ â€* å¯ä»¥ä»è®¸å¤šä¸åŒçš„æ–¹é¢çœ‹ä½œæ˜¯ç›¸ä¼¼çš„ï¼šï¼ˆ1ï¼‰å…±æœ‰çš„å•è¯æ•°é‡ï¼›ï¼ˆ2ï¼‰æ¨¡å¼
    â€œåŠ¨ç‰© åƒ é£Ÿç‰©â€ çš„å®ç°ã€‚å…³é”®åœ¨äºå†³å®šæˆ–è®©ç®—æ³•å†³å®šå“ªä¸ªæ˜¯ç‰¹å®šä»»åŠ¡çš„æœ€ä½³è¡¨ç¤ºã€‚'
- en: '*Distributed representations* are then replacing long-lasting, successful *discrete
    symbolic representations* in representing knowledge for learning machines but
    these representations are less human *interpretable*. Hence, discussing about
    basic, obvious properties of *discrete symbolic representations* is not useless
    as these properties may guarantee success to distributed representations similar
    to the one of discrete symbolic representations.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ†å¸ƒå¼è¡¨ç¤º* æ­£åœ¨å–ä»£é•¿æœŸå­˜åœ¨ä¸”æˆåŠŸçš„ *ç¦»æ•£ç¬¦å·è¡¨ç¤º*ï¼Œç”¨äºå­¦ä¹ æœºå™¨çš„çŸ¥è¯†è¡¨ç¤ºï¼Œä½†è¿™äº›è¡¨ç¤ºå¯¹äººç±»çš„ *å¯è§£é‡Šæ€§* è¾ƒå·®ã€‚å› æ­¤ï¼Œè®¨è®º *ç¦»æ•£ç¬¦å·è¡¨ç¤º*
    çš„åŸºæœ¬æ˜æ˜¾å±æ€§å¹¶éæ¯«æ— æ„ä¹‰ï¼Œå› ä¸ºè¿™äº›å±æ€§å¯èƒ½ä¼šå¯¹åˆ†å¸ƒå¼è¡¨ç¤ºçš„æˆåŠŸæä¾›ä¿è¯ï¼Œç±»ä¼¼äºç¦»æ•£ç¬¦å·è¡¨ç¤ºã€‚'
- en: Discrete symbolic representations are human *interpretable* as *symbols are
    not altered in expressions*. This is one of the most important, obvious feature
    of these representations. Infinite sets of expressions, which are sequences of
    symbols, can be *interpreted* as these expressions are obtained by concatenating
    a finite set of basic symbols according to some concatenative rules. During concatenation,
    symbols are not altered and, then, can be recognized. By using the principle of
    *semantic compositionality*, the meaning of expressions can be obtained by combining
    the meaning of the parts and, hence, recursively, by combining the meaning of
    the finite set of basic symbols. For example, given the set of basic symbols $\mathcal{D}$
    = {*mouse*,*cat*,*a*,*swallows*,*(*,*)*}, expressions like $s_{1}$=*â€œa cat swallows
    a mouseâ€* or $t_{1}$=*((a cat) (swallows (a mouse)))* are totally plausible and
    interpretable given rules for producing natural language utterances or for producing
    tree structured representations in parenthetical form, respectively. This strongly
    depends on the fact that individual symbols can be recognized.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦»æ•£ç¬¦å·è¡¨ç¤ºå¯¹äººç±»æ¥è¯´*æ˜“äºè§£é‡Š*ï¼Œå› ä¸º*ç¬¦å·åœ¨è¡¨è¾¾å¼ä¸­æœªè¢«æ”¹å˜*ã€‚è¿™æ˜¯è¿™äº›è¡¨ç¤ºçš„ä¸€ä¸ªæœ€é‡è¦ã€æœ€æ˜æ˜¾çš„ç‰¹å¾ã€‚æ— é™çš„è¡¨è¾¾å¼é›†åˆï¼Œå³ç¬¦å·çš„åºåˆ—ï¼Œå¯ä»¥*è¢«è§£é‡Š*ä¸ºè¿™äº›è¡¨è¾¾å¼æ˜¯é€šè¿‡æŒ‰ç…§æŸäº›ä¸²è”è§„åˆ™å°†æœ‰é™çš„åŸºæœ¬ç¬¦å·é›†åˆè¿æ¥è€Œæˆçš„ã€‚åœ¨ä¸²è”è¿‡ç¨‹ä¸­ï¼Œç¬¦å·ä¸ä¼šè¢«æ”¹å˜ï¼Œä»è€Œå¯ä»¥è¢«è¯†åˆ«ã€‚åˆ©ç”¨*è¯­ä¹‰ç»„åˆæ€§*åŸç†ï¼Œå¯ä»¥é€šè¿‡ç»„åˆéƒ¨åˆ†çš„æ„ä¹‰æ¥è·å¾—è¡¨è¾¾å¼çš„æ„ä¹‰ï¼Œä»è€Œé€’å½’åœ°é€šè¿‡ç»„åˆæœ‰é™çš„åŸºæœ¬ç¬¦å·çš„æ„ä¹‰æ¥è·å¾—ã€‚ä¾‹å¦‚ï¼Œç»™å®šåŸºæœ¬ç¬¦å·é›†$\mathcal{D}$
    = {*mouse*,*cat*,*a*,*swallows*,*(*,*)*}ï¼Œè¡¨è¾¾å¼å¦‚$s_{1}$=*â€œä¸€åªçŒ«åé£Ÿäº†ä¸€åªè€é¼ â€*æˆ–$t_{1}$=*((ä¸€åªçŒ«)
    (åé£Ÿ (ä¸€åªè€é¼ )))*æ˜¯å®Œå…¨åˆç†ä¸”å¯è§£é‡Šçš„ï¼Œå‰ææ˜¯æœ‰ç”¨äºç”Ÿæˆè‡ªç„¶è¯­è¨€å‘è¨€æˆ–ç”Ÿæˆæ‹¬å·å½¢å¼çš„æ ‘ç»“æ„è¡¨ç¤ºçš„è§„åˆ™ã€‚è¿™å¼ºçƒˆä¾èµ–äºä¸ªåˆ«ç¬¦å·èƒ½å¤Ÿè¢«è¯†åˆ«çš„äº‹å®ã€‚
- en: 'Distributed representations instead seem to *alter symbols* when applied to
    symbolic inputs and, thus, are less interpretable. In fact, symbols as well as
    expressions are represented as vectors in these metric spaces. Observing distributed
    representations, symbols and expressions do not immediately emerge. Moreover,
    these distributed representations may be transformed by using matrix multiplication
    or by using non-linear functions. Hence, it is generally unclear: (1) what is
    the relation between the initial symbols or expressions and their distributed
    representations and (2) how these expressions are manipulated during matrix multiplication
    or when applying non-linear functions. In other words, it is unclear whether symbols
    can be recognized in distributed representations.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è¡¨ç¤ºåœ¨åº”ç”¨äºç¬¦å·è¾“å…¥æ—¶ä¼¼ä¹ä¼š*æ”¹å˜ç¬¦å·*ï¼Œå› æ­¤ï¼Œè¿™äº›è¡¨ç¤ºè¾ƒéš¾è§£é‡Šã€‚äº‹å®ä¸Šï¼Œç¬¦å·ä»¥åŠè¡¨è¾¾å¼åœ¨è¿™äº›åº¦é‡ç©ºé—´ä¸­è¢«è¡¨ç¤ºä¸ºå‘é‡ã€‚è§‚å¯Ÿåˆ†å¸ƒå¼è¡¨ç¤ºæ—¶ï¼Œç¬¦å·å’Œè¡¨è¾¾å¼ä¸ä¼šç«‹å³æ˜¾ç°ã€‚æ­¤å¤–ï¼Œè¿™äº›åˆ†å¸ƒå¼è¡¨ç¤ºå¯èƒ½é€šè¿‡çŸ©é˜µä¹˜æ³•æˆ–éçº¿æ€§å‡½æ•°è¢«è½¬æ¢ã€‚å› æ­¤ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œä»¥ä¸‹é—®é¢˜ä¸æ˜ç¡®ï¼šï¼ˆ1ï¼‰åˆå§‹ç¬¦å·æˆ–è¡¨è¾¾å¼ä¸å…¶åˆ†å¸ƒå¼è¡¨ç¤ºä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Œä»¥åŠï¼ˆ2ï¼‰è¿™äº›è¡¨è¾¾å¼åœ¨çŸ©é˜µä¹˜æ³•æˆ–åº”ç”¨éçº¿æ€§å‡½æ•°è¿‡ç¨‹ä¸­å¦‚ä½•è¢«æ“ä½œã€‚æ¢å¥è¯è¯´ï¼Œå°šä¸æ¸…æ¥šç¬¦å·æ˜¯å¦å¯ä»¥åœ¨åˆ†å¸ƒå¼è¡¨ç¤ºä¸­è¢«è¯†åˆ«ã€‚
- en: Hence, a debated question is whether discrete symbolic representations and distributed
    representations are two very different ways of encoding knowledge because of the
    difference in *alterning symbols*. The debate dates back in the late 80s. For
    Fodor and Pylyshyn ([1988](#bib.bib24)), distributed representations in Neural
    Network architectures are *â€œonly an implementation of the Classical approachâ€*
    where classical approach is related to discrete symbolic representations. Whereas,
    for Chalmers ([1992](#bib.bib11)), distributed representations give the important
    opportunity to reason *â€œholisticallyâ€* about encoded knowledge. This means that
    decisions over some specific part of the stored knowledge can be taken without
    retrieving the specific part but acting on the whole representation. However,
    this does not solve the debated question as it is still unclear what is in a distributed
    representation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸€ä¸ªæœ‰äº‰è®®çš„é—®é¢˜æ˜¯ç¦»æ•£ç¬¦å·è¡¨ç¤ºå’Œåˆ†å¸ƒå¼è¡¨ç¤ºæ˜¯å¦ç”±äº*ç¬¦å·çš„æ”¹å˜*çš„ä¸åŒè€Œæ˜¯ä¸¤ç§æˆªç„¶ä¸åŒçš„çŸ¥è¯†ç¼–ç æ–¹å¼ã€‚è¿™åœºè¾©è®ºå¯ä»¥è¿½æº¯åˆ°80å¹´ä»£æœ«ã€‚å¯¹äºFodorå’ŒPylyshynï¼ˆ[1988](#bib.bib24)ï¼‰ï¼Œç¥ç»ç½‘ç»œæ¶æ„ä¸­çš„åˆ†å¸ƒå¼è¡¨ç¤ºæ˜¯*â€œç»å…¸æ–¹æ³•çš„ä»…ä»…ä¸€ç§å®ç°â€*ï¼Œè€Œç»å…¸æ–¹æ³•ä¸ç¦»æ•£ç¬¦å·è¡¨ç¤ºç›¸å…³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¯¹äºChalmersï¼ˆ[1992](#bib.bib11)ï¼‰ï¼Œåˆ†å¸ƒå¼è¡¨ç¤ºæä¾›äº†*â€œæ•´ä½“æ€§â€*åœ°æ¨ç†ç¼–ç çŸ¥è¯†çš„é‡è¦æœºä¼šã€‚è¿™æ„å‘³ç€å¯ä»¥åœ¨ä¸æ£€ç´¢ç‰¹å®šéƒ¨åˆ†çš„æƒ…å†µä¸‹å¯¹å­˜å‚¨çŸ¥è¯†çš„æŸäº›ç‰¹å®šéƒ¨åˆ†åšå‡ºå†³å®šï¼Œè€Œæ˜¯å¯¹æ•´ä¸ªè¡¨ç¤ºè¿›è¡Œæ“ä½œã€‚ç„¶è€Œï¼Œè¿™å¹¶æ²¡æœ‰è§£å†³äº‰è®®çš„é—®é¢˜ï¼Œå› ä¸ºåœ¨åˆ†å¸ƒå¼è¡¨ç¤ºä¸­åˆ°åº•åŒ…å«äº†ä»€ä¹ˆä»ä¸æ¸…æ¥šã€‚
- en: 'To contribute to the above debated question, Gelder ([1990](#bib.bib27)) has
    formalized the property of *altering symbols in expressions* by defining two different
    notions of compositionality: *concatentative* compositionality and *functional*
    compositionality. *Concatenative compositionality* explains how discrete symbolic
    representations compose symbols to obtain expressions. In fact, the mode of combination
    is an extended concept of juxtaposition that provides a way of linking successive
    symbols without altering them as these form expressions. Concatenative compositionality
    explains discrete symbolic representations no matter the means is used to store
    expressions: a piece of paper or a computer memory. Concatenation is sometime
    expressed with an operator like $\circ$, which can be used in a infix or prefix
    notation, that is a sort of function with arguments $\circ(w_{1},...,w_{n})$.
    By using the operator for concatenation, the two above examples $s_{1}$ and $t_{1}$
    can be represented as the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¯¹ä¸Šè¿°äº‰è®ºé—®é¢˜åšå‡ºè´¡çŒ®ï¼ŒGelder ([1990](#bib.bib27)) é€šè¿‡å®šä¹‰ä¸¤ç§ä¸åŒçš„ç»„åˆæ€§æ¦‚å¿µï¼š*è¿æ¥æ€§*ç»„åˆæ€§å’Œ*å‡½æ•°æ€§*ç»„åˆæ€§ï¼Œå½¢å¼åŒ–äº†*æ”¹å˜è¡¨è¾¾å¼ä¸­çš„ç¬¦å·*çš„å±æ€§ã€‚*è¿æ¥æ€§ç»„åˆæ€§*è§£é‡Šäº†ç¦»æ•£ç¬¦å·è¡¨ç¤ºå¦‚ä½•ç»„åˆç¬¦å·ä»¥è·å¾—è¡¨è¾¾å¼ã€‚å®é™…ä¸Šï¼Œè¿™ç§ç»„åˆæ¨¡å¼æ˜¯å¹¶æ’çš„æ‰©å±•æ¦‚å¿µï¼Œå®ƒæä¾›äº†ä¸€ç§åœ¨ä¸æ”¹å˜ç¬¦å·çš„æƒ…å†µä¸‹å°†è¿ç»­ç¬¦å·è¿æ¥åœ¨ä¸€èµ·çš„æ–¹å¼ã€‚è¿æ¥æ€§ç»„åˆæ€§è§£é‡Šäº†ç¦»æ•£ç¬¦å·è¡¨ç¤ºï¼Œæ— è®ºå­˜å‚¨è¡¨è¾¾å¼çš„æ–¹å¼æ˜¯çº¸å¼ è¿˜æ˜¯è®¡ç®—æœºå†…å­˜ã€‚è¿æ¥æœ‰æ—¶ç”¨ä¸€ä¸ªå¦‚
    $\circ$ çš„è¿ç®—ç¬¦è¡¨ç¤ºï¼Œè¯¥è¿ç®—ç¬¦å¯ä»¥ç”¨äºä¸­ç¼€æˆ–å‰ç¼€è¡¨ç¤ºæ³•ï¼Œå³ä¸€ç§å¸¦æœ‰å‚æ•°çš„å‡½æ•° $\circ(w_{1},...,w_{n})$ã€‚ä½¿ç”¨è¿æ¥è¿ç®—ç¬¦ï¼Œä¸Šè¿°ä¸¤ä¸ªç¤ºä¾‹
    $s_{1}$ å’Œ $t_{1}$ å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: '|  | $a\circ cat\circ swallows\circ a\circ mouse$ |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $a\circ cat\circ swallows\circ a\circ mouse$ |  |'
- en: that represents a sequence with the infix notation and
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»£è¡¨äº†ä¸­ç¼€è¡¨ç¤ºæ³•ä¸­çš„ä¸€ä¸ªåºåˆ—å’Œ
- en: '|  | $\circ(\circ(a,cat),\circ(swallows,\circ(a,mouse)))$ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\circ(\circ(a,cat),\circ(swallows,\circ(a,mouse)))$ |  |'
- en: that represents a tree with the prefix notation. *Functional compositionality*
    explains distributed representations. In functional compositionality, the mode
    of combination is a function $\Phi$ that gives a reliable, general process for
    producing expressions given its constituents. Within this perspective, semantic
    compositionality is a special case of functional compositionality where the target
    of the composition is a way to represent meaning (Blutner etÂ al., [2003](#bib.bib10)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»£è¡¨äº†å‰ç¼€è¡¨ç¤ºæ³•ä¸­çš„ä¸€ä¸ªæ ‘ã€‚*å‡½æ•°æ€§ç»„åˆæ€§*è§£é‡Šäº†åˆ†å¸ƒå¼è¡¨ç¤ºã€‚åœ¨å‡½æ•°æ€§ç»„åˆæ€§ä¸­ï¼Œç»„åˆæ¨¡å¼æ˜¯ä¸€ä¸ªå‡½æ•° $\Phi$ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªå¯é çš„ã€é€šç”¨çš„è¿‡ç¨‹æ¥ç”Ÿæˆç»™å®šç»„æˆéƒ¨åˆ†çš„è¡¨è¾¾å¼ã€‚åœ¨è¿™ç§è§†è§’ä¸‹ï¼Œè¯­ä¹‰ç»„åˆæ€§æ˜¯å‡½æ•°æ€§ç»„åˆæ€§çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå…¶ä¸­ç»„åˆçš„ç›®æ ‡æ˜¯è¡¨ç¤ºæ„ä¹‰çš„æ–¹å¼
    (Blutner et al., [2003](#bib.bib10))ã€‚
- en: '*Local distributed representations* (as referred in (Plate, [1995](#bib.bib57)))
    or *one-hot encodings* are the easiest way to visualize how *functional compositionality*
    act on *distributed representations*. Local distributed representations give a
    first, simple encoding of discrete symbolic representations in a metric space.
    Given a set of symbols $\mathcal{D}$, a local distributed epresentation maps the
    $i$-th symbol in $\mathcal{D}$ to the $i$-th base unit vector $\mathbf{e}_{i}$
    in $\mathbb{R}^{n}$, where $n$ is the cardinality of $\mathcal{D}$. Hence, the
    $i$-th unit vector represents the $i$-th symbol. In *functional compositionality*,
    expressions $s=w_{1}\ldots w_{k}$ are represented by vectors $\mathbf{s}$ obtained
    with an eventually recursive function $\Phi$ applied to vectors $\mathbf{e}_{w_{1}}\ldots\mathbf{e}_{w_{k}}$.
    The function $f$ may be very simple as the sum or more complex. In case the function
    $\Phi$ is the sum, that is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*å±€éƒ¨åˆ†å¸ƒå¼è¡¨ç¤º*ï¼ˆå¦‚ï¼ˆPlate, [1995](#bib.bib57)) æ‰€æåˆ°çš„ï¼‰æˆ– *ç‹¬çƒ­ç¼–ç * æ˜¯å¯è§†åŒ–*å‡½æ•°æ€§ç»„åˆæ€§*å¦‚ä½•ä½œç”¨äº*åˆ†å¸ƒå¼è¡¨ç¤º*çš„æœ€ç®€å•æ–¹å¼ã€‚å±€éƒ¨åˆ†å¸ƒå¼è¡¨ç¤ºåœ¨åº¦é‡ç©ºé—´ä¸­å¯¹ç¦»æ•£ç¬¦å·è¡¨ç¤ºè¿›è¡Œåˆæ­¥ç®€å•ç¼–ç ã€‚ç»™å®šä¸€ä¸ªç¬¦å·é›†åˆ
    $\mathcal{D}$ï¼Œå±€éƒ¨åˆ†å¸ƒå¼è¡¨ç¤ºå°† $\mathcal{D}$ ä¸­ç¬¬ $i$ ä¸ªç¬¦å·æ˜ å°„åˆ° $\mathbb{R}^{n}$ ä¸­ç¬¬ $i$ ä¸ªåŸºå•ä½å‘é‡
    $\mathbf{e}_{i}$ï¼Œå…¶ä¸­ $n$ æ˜¯ $\mathcal{D}$ çš„åŸºæ•°ã€‚å› æ­¤ï¼Œç¬¬ $i$ ä¸ªå•ä½å‘é‡è¡¨ç¤ºç¬¬ $i$ ä¸ªç¬¦å·ã€‚åœ¨*å‡½æ•°æ€§ç»„åˆæ€§*ä¸­ï¼Œè¡¨è¾¾å¼
    $s=w_{1}\ldots w_{k}$ ç”±å‘é‡ $\mathbf{s}$ è¡¨ç¤ºï¼Œè¯¥å‘é‡æ˜¯åº”ç”¨äºå‘é‡ $\mathbf{e}_{w_{1}}\ldots\mathbf{e}_{w_{k}}$
    çš„ä¸€ä¸ªé€’å½’å‡½æ•° $\Phi$ è·å¾—çš„ã€‚å‡½æ•° $f$ å¯ä»¥éå¸¸ç®€å•ï¼Œå¦‚æ±‚å’Œï¼Œæˆ–è€…æ›´å¤æ‚ã€‚å¦‚æœå‡½æ•° $\Phi$ æ˜¯æ±‚å’Œï¼Œå³ï¼š'
- en: '|  | $\mathbf{func}_{\Sigma}(s)=\sum_{j=1}^{k}\mathbf{e}_{w_{j}}$ |  | (1)
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{func}_{\Sigma}(s)=\sum_{j=1}^{k}\mathbf{e}_{w_{j}}$ |  | (1)
    |'
- en: 'the derived vector is the classical bag-of-word vector space model (Salton,
    [1989](#bib.bib61)). Whereas, more complex functions $f$ can range from different
    vector-to-vector operations like circular convolution in Holographic Reduced Representations
    (Plate, [1995](#bib.bib57)) to matrix multiplications plus non linear operations
    in models such as in recurrent neural networks (Schuster and Paliwal, [1997](#bib.bib63);
    Hochreiter and Schmidhuber, [1997](#bib.bib36)) or in neural networks with attention
    (Vaswani etÂ al., [2017](#bib.bib69); Devlin etÂ al., [2018](#bib.bib19)). The above
    example can be useful to describe *concatenative* and *functional* compositionality.
    The set $\mathcal{D}$= {*mouse*,*cat*,*a*,*swallows*,*eats*,*some*,*cheese*,*(*,*)*}
    may be represented with the base vectors $\mathbf{e}_{i}\in\mathbb{R}^{9}$ where
    $\mathbf{e}_{1}$ is the base vector for *mouse*, $\mathbf{e}_{2}$ for *cat*, $\mathbf{e}_{3}$
    for *a*, $\mathbf{e}_{4}$ for *swallaws*, $\mathbf{e}_{5}$ for *eats*, $\mathbf{e}_{6}$
    for *some*, $\mathbf{e}_{7}$ for *cheese*, $\mathbf{e}_{8}$ for *(*, and $\mathbf{e}_{9}$
    for *)*. The additive functional composition of the expression $s_{1}$=*a cat
    swallows a mouse* is then:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€å¾—åˆ°çš„å‘é‡æ˜¯ç»å…¸çš„è¯è¢‹å‘é‡ç©ºé—´æ¨¡å‹ï¼ˆSalton, [1989](#bib.bib61)ï¼‰ã€‚è€Œæ›´å¤æ‚çš„å‡½æ•° $f$ å¯ä»¥æ¶‰åŠä»ä¸åŒçš„å‘é‡åˆ°å‘é‡æ“ä½œï¼Œæ¯”å¦‚å…¨æ¯å‹ç¼©è¡¨ç¤ºä¸­çš„å¾ªç¯å·ç§¯ï¼ˆPlate,
    [1995](#bib.bib57)ï¼‰ï¼Œåˆ°åœ¨å¦‚é€’å½’ç¥ç»ç½‘ç»œï¼ˆSchuster å’Œ Paliwal, [1997](#bib.bib63)ï¼›Hochreiter
    å’Œ Schmidhuber, [1997](#bib.bib36)ï¼‰ä¸­çš„çŸ©é˜µä¹˜æ³•åŠ éçº¿æ€§æ“ä½œï¼Œæˆ–åœ¨å…·æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œä¸­ï¼ˆVaswani ç­‰, [2017](#bib.bib69)ï¼›Devlin
    ç­‰, [2018](#bib.bib19)ï¼‰ã€‚ä¸Šè¿°ç¤ºä¾‹å¯ä»¥ç”¨äºæè¿° *è¿æ¥æ€§* å’Œ *åŠŸèƒ½æ€§* ç»„åˆæ€§ã€‚é›†åˆ $\mathcal{D}$= {*mouse*,*cat*,*a*,*swallows*,*eats*,*some*,*cheese*,*(*,*)*}
    å¯ä»¥ç”¨åŸºå‘é‡ $\mathbf{e}_{i}\in\mathbb{R}^{9}$ è¡¨ç¤ºï¼Œå…¶ä¸­ $\mathbf{e}_{1}$ æ˜¯ *mouse* çš„åŸºå‘é‡ï¼Œ$\mathbf{e}_{2}$
    æ˜¯ *cat* çš„åŸºå‘é‡ï¼Œ$\mathbf{e}_{3}$ æ˜¯ *a* çš„åŸºå‘é‡ï¼Œ$\mathbf{e}_{4}$ æ˜¯ *swallows* çš„åŸºå‘é‡ï¼Œ$\mathbf{e}_{5}$
    æ˜¯ *eats* çš„åŸºå‘é‡ï¼Œ$\mathbf{e}_{6}$ æ˜¯ *some* çš„åŸºå‘é‡ï¼Œ$\mathbf{e}_{7}$ æ˜¯ *cheese* çš„åŸºå‘é‡ï¼Œ$\mathbf{e}_{8}$
    æ˜¯ *(* çš„åŸºå‘é‡ï¼Œ$\mathbf{e}_{9}$ æ˜¯ *)* çš„åŸºå‘é‡ã€‚è¡¨è¾¾å¼ $s_{1}$=*a cat swallows a mouse* çš„åŠ æ€§åŠŸèƒ½ç»„åˆä¸ºï¼š
- en: '| *expression in $e_{i}$* | *additive functional composition* |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| *åœ¨ $e_{i}$ ä¸­çš„è¡¨è¾¾å¼* | *åŠ æ€§åŠŸèƒ½ç»„åˆ* |'
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; a &#124; cat &#124; swallows &#124; a &#124; mouse &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; a &#124; cat &#124; swallows &#124; a &#124; mouse &#124; '
- en: '&#124; <math   alttext="\begin{pmatrix}0\\ 0\\'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; <math   alttext="\begin{pmatrix}0\\ 0\\'
- en: 1\\
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 1\\
- en: 0\\
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}0\\ 0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124; <math   alttext="\begin{pmatrix}0\\
    1\\
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.31.2.2.1.1.1.
- en: 0\\
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >1</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124; <math   alttext="\begin{pmatrix}0\\
    0\\
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.32.3.3.2.2.2.
- en: 0\\
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 1\\
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 1\\
- en: 0\\
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >1</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}0\\ 0\\ 0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124; <math   alttext="\begin{pmatrix}0\\
    0\\
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.33.4.4.3.3.3.
- en: 1\\
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 1\\
- en: 0\\
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}0\\ 0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124; <math   alttext="\begin{pmatrix}1\\
    0\\
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.34.5.5.4.4.4.
- en: 0\\
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow ><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124;
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.35.6.6.5.5.5.
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathbf{e_{3}}+\mathbf{e_{2}}+\mathbf{e_{4}}+\mathbf{e_{3}}+\mathbf{e_{1}}$
    &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{e_{3}}+\mathbf{e_{2}}+\mathbf{e_{4}}+\mathbf{e_{3}}+\mathbf{e_{1}}$
    &#124;'
- en: '&#124; <math   alttext="\mathbf{func_{\Sigma}(s_{1})}=\begin{pmatrix}1\\ 1\\'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; <math   alttext="\mathbf{func_{\Sigma}(s_{1})}=\begin{pmatrix}1\\ 1\\'
- en: 2\\
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2\\
- en: 1\\
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 1\\
- en: 0\\
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\end{pmatrix}" display="inline"><semantics ><mrow ><mrow ><msub ><mi mathsize="50%"  >ğŸğ®ğ§ğœ</mi><mi
    mathsize="50%"  >ğšº</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo
    maxsize="50%" minsize="50%"  >(</mo><msub ><mi mathsize="50%"  >ğ¬</mi><mn mathsize="50%"  >ğŸ</mn></msub><mo
    maxsize="50%" minsize="50%"  >)</mo></mrow></mrow><mo mathsize="50%"  >=</mo><mrow
    ><mo >(</mo><mtable rowspacing="0pt"  ><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >2</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğŸğ®ğ§ğœ</ci><ci >ğšº</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ¬</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="latexml" >matrix</csymbol><matrix ><matrixrow ><cn type="integer"
    >1</cn></matrixrow><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow
    ><cn type="integer" >2</cn></matrixrow><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{func_{\Sigma}(s_{1})}=\begin{pmatrix}1\\
    1\\ 2\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\end{pmatrix}</annotation></semantics></math> &#124;
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 0\end{pmatrix}" display="inline"><semantics ><mrow ><mrow ><msub ><mi mathsize="50%"  >ğŸğ®ğ§ğœ</mi><mi
    mathsize="50%"  >ğšº</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo
    maxsize="50%" minsize="50%" >(</mo><msub ><mi mathsize="50%" >ğ¬</mi><mn mathsize="50%"
    >ğŸ</mn></msub><mo maxsize="50%" minsize="50%" >)</mo></mrow></mrow><mo mathsize="50%"  >=</mo><mrow
    ><mo >(</mo><mtable rowspacing="0pt"  ><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.37.8.8.7.2.2.1.m1.1.1.1.1
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: where the concatenative operator $\circ$ has been substituted with the sum $+$.
    Just to observe, in the additive functional composition $\mathbf{func_{\Sigma}(s_{1})}$,
    symbols are still visible but the sequence is lost. Hence, it is difficult to
    reproduce the initial discrete symbolic expression. However, for example, the
    additive composition function gives the possibility to compare two expressions.
    Given the expression $s_{1}$ and $s_{2}$=*a mouse eats some cheese*, the dot product
    between $\mathbf{func_{\Sigma}(s_{1})}$ and $\mathbf{func_{\Sigma}(s_{2})}=\begin{pmatrix}1&amp;0&amp;1&amp;0&amp;1&amp;1&amp;1&amp;0&amp;0\end{pmatrix}^{T}$
    counts the common words between the two expressions. In a functional composition
    with a function $\Phi$, the expression $s_{1}$ may become $\mathbf{func_{\Phi}(s_{1})}=\Phi(\Phi(\Phi(\Phi(\mathbf{e_{3}},\mathbf{e_{2}}),\mathbf{e_{4}}),\mathbf{e_{3}}),\mathbf{e_{1}})$
    by following the concatenative compositionality of the discrete symbolic expression.
    The same functional compositional principle can be applied to discrete symbolic
    trees as $t_{1}$ by producing this distributed representation $\Phi(\Phi(\mathbf{e_{3}},\mathbf{e_{2}}),\Phi(\mathbf{e_{4}},\Phi(\mathbf{e_{3}},\mathbf{e_{1}})))$.
    Finally, in the functional composition with a generic recursive function $\mathbf{func_{\Phi}(s_{1})}$,
    the function $\Phi$ will be crucial to determine whether symbols can be recognized
    and sequence is preserved.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­è¿æ¥æ“ä½œç¬¦ $\circ$ è¢«åŠ æ³• $+$ æ›¿ä»£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨åŠ æ³•åŠŸèƒ½ç»„åˆ $\mathbf{func_{\Sigma}(s_{1})}$ ä¸­ï¼Œç¬¦å·ä»ç„¶å¯è§ä½†åºåˆ—ä¸¢å¤±ã€‚å› æ­¤ï¼Œé‡ç°åˆå§‹çš„ç¦»æ•£ç¬¦å·è¡¨è¾¾å¼å˜å¾—å›°éš¾ã€‚ç„¶è€Œï¼Œä¾‹å¦‚ï¼ŒåŠ æ³•ç»„åˆå‡½æ•°æä¾›äº†æ¯”è¾ƒä¸¤ä¸ªè¡¨è¾¾å¼çš„å¯èƒ½æ€§ã€‚ç»™å®šè¡¨è¾¾å¼
    $s_{1}$ å’Œ $s_{2}$=*ä¸€åªè€é¼ åƒäº†ä¸€äº›å¥¶é…ª*ï¼Œ$\mathbf{func_{\Sigma}(s_{1})}$ å’Œ $\mathbf{func_{\Sigma}(s_{2})}=\begin{pmatrix}1&amp;0&amp;1&amp;0&amp;1&amp;1&amp;1&amp;0&amp;0\end{pmatrix}^{T}$
    ä¹‹é—´çš„ç‚¹ç§¯è®¡ç®—äº†ä¸¤ä¸ªè¡¨è¾¾å¼ä¹‹é—´çš„å…±åŒå•è¯ã€‚åœ¨å¸¦æœ‰å‡½æ•° $\Phi$ çš„åŠŸèƒ½ç»„åˆä¸­ï¼Œè¡¨è¾¾å¼ $s_{1}$ å¯èƒ½ä¼šå˜æˆ $\mathbf{func_{\Phi}(s_{1})}=\Phi(\Phi(\Phi(\Phi(\mathbf{e_{3}},\mathbf{e_{2}}),\mathbf{e_{4}}),\mathbf{e_{3}}),\mathbf{e_{1}})$ï¼Œè¿™æ˜¯é€šè¿‡éµå¾ªç¦»æ•£ç¬¦å·è¡¨è¾¾å¼çš„è¿æ¥æ€§ç»„åˆåŸç†å®ç°çš„ã€‚ç›¸åŒçš„åŠŸèƒ½ç»„åˆåŸç†å¯ä»¥åº”ç”¨äºç¦»æ•£ç¬¦å·æ ‘ï¼Œå¦‚
    $t_{1}$ï¼Œé€šè¿‡ç”Ÿæˆè¿™ä¸ªåˆ†å¸ƒå¼è¡¨ç¤º $\Phi(\Phi(\mathbf{e_{3}},\mathbf{e_{2}}),\Phi(\mathbf{e_{4}},\Phi(\mathbf{e_{3}},\mathbf{e_{1}})))$ã€‚æœ€åï¼Œåœ¨å¸¦æœ‰é€šç”¨é€’å½’å‡½æ•°
    $\mathbf{func_{\Phi}(s_{1})}$ çš„åŠŸèƒ½ç»„åˆä¸­ï¼Œå‡½æ•° $\Phi$ å°†å¯¹ç¡®å®šç¬¦å·æ˜¯å¦å¯ä»¥è¢«è¯†åˆ«ä»¥åŠåºåˆ—æ˜¯å¦å¾—ä»¥ä¿ç•™è‡³å…³é‡è¦ã€‚
- en: '*Distributed representations* in their general form are more ambitious than
    distributed *local* representations and tend to encode basic symbols of $\mathcal{D}$
    in vectors in $\mathbb{R}^{d}$ where $d<<n$. These vectors generally alter symbols
    as there is not a direct link between symbols and dimensions of the space. Given
    a distributed local representation $\mathbf{e}_{w}$ of a symbol $w$, the encoder
    for a distributed representation is a matrix $\mathbf{W_{d\times n}}$ that transforms
    $\mathbf{x}_{w}$ in $\mathbf{y}_{w}=\mathbf{W_{d\times n}}\mathbf{e}_{w}$. As
    an example, the encoding matrix $\mathbf{W_{d\times n}}$ can be build by modeling
    words in $\mathcal{D}$ around three dimensions: number of vowels, number of consonants
    and, finally, number of non-alphabetic symbols. Given these dimensions, the matrix
    $\mathbf{W_{3\times 9}}$ for the example is :'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ†å¸ƒå¼è¡¨ç¤º* åœ¨å…¶ä¸€èˆ¬å½¢å¼ä¸Šæ¯”åˆ†å¸ƒå¼*å±€éƒ¨*è¡¨ç¤ºæ›´å…·é›„å¿ƒï¼Œå¾€å¾€å°† $\mathcal{D}$ çš„åŸºæœ¬ç¬¦å·ç¼–ç åœ¨ $\mathbb{R}^{d}$ ä¸­çš„å‘é‡ä¸­ï¼Œå…¶ä¸­
    $d<<n$ã€‚è¿™äº›å‘é‡é€šå¸¸ä¼šæ”¹å˜ç¬¦å·ï¼Œå› ä¸ºç¬¦å·ä¸ç©ºé—´ç»´åº¦ä¹‹é—´æ²¡æœ‰ç›´æ¥çš„è”ç³»ã€‚ç»™å®šç¬¦å· $w$ çš„åˆ†å¸ƒå¼å±€éƒ¨è¡¨ç¤º $\mathbf{e}_{w}$ï¼Œåˆ†å¸ƒå¼è¡¨ç¤ºçš„ç¼–ç å™¨æ˜¯ä¸€ä¸ªçŸ©é˜µ
    $\mathbf{W_{d\times n}}$ï¼Œå®ƒå°† $\mathbf{x}_{w}$ è½¬æ¢ä¸º $\mathbf{y}_{w}=\mathbf{W_{d\times
    n}}\mathbf{e}_{w}$ã€‚ä¾‹å¦‚ï¼Œç¼–ç çŸ©é˜µ $\mathbf{W_{d\times n}}$ å¯ä»¥é€šè¿‡åœ¨ä¸‰ä¸ªç»´åº¦ä¸Šå»ºæ¨¡ $\mathcal{D}$
    ä¸­çš„å•è¯æ¥æ„å»ºï¼šå…ƒéŸ³æ•°ã€è¾…éŸ³æ•°ï¼Œæœ€åæ˜¯éå­—æ¯ç¬¦å·æ•°ã€‚ç»™å®šè¿™äº›ç»´åº¦ï¼Œç¤ºä¾‹ä¸­çš„çŸ©é˜µ $\mathbf{W_{3\times 9}}$ ä¸ºï¼š'
- en: '|  | <math   alttext="\mathbf{W_{3\times 9}}=\begin{pmatrix}3&amp;1&amp;1&amp;2&amp;2&amp;2&amp;3&amp;0&amp;0\\
    2&amp;2&amp;0&amp;6&amp;2&amp;2&amp;3&amp;0&amp;0\\'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{W_{3\times 9}}=\begin{pmatrix}3&amp;1&amp;1&amp;2&amp;2&amp;2&amp;3&amp;0&amp;0\\
    2&amp;2&amp;0&amp;6&amp;2&amp;2&amp;3&amp;0&amp;0\\'
- en: 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1\\
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1\\
- en: \end{pmatrix}" display="block"><semantics ><mrow ><msub  ><mi >ğ–</mi><mrow ><mn  >ğŸ‘</mn><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mn >ğŸ—</mn></mrow></msub><mo >=</mo><mrow
    ><mo  >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mn >3</mn></mtd><mtd  ><mn >1</mn></mtd><mtd ><mn >1</mn></mtd><mtd  ><mn
    >2</mn></mtd><mtd ><mn >2</mn></mtd><mtd  ><mn >2</mn></mtd><mtd ><mn >3</mn></mtd><mtd  ><mn
    >0</mn></mtd><mtd ><mn >0</mn></mtd></mtr><mtr ><mtd  ><mn >2</mn></mtd><mtd ><mn
    >2</mn></mtd><mtd  ><mn >0</mn></mtd><mtd ><mn >6</mn></mtd><mtd  ><mn >2</mn></mtd><mtd
    ><mn >2</mn></mtd><mtd  ><mn >3</mn></mtd><mtd ><mn >0</mn></mtd><mtd  ><mn >0</mn></mtd></mtr><mtr
    ><mtd ><mn  >0</mn></mtd><mtd ><mn >0</mn></mtd><mtd  ><mn >0</mn></mtd><mtd ><mn
    >0</mn></mtd><mtd  ><mn >0</mn></mtd><mtd ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd
    ><mn >1</mn></mtd><mtd ><mn  >1</mn></mtd></mtr></mtable><mo >)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >ğ–</ci><apply
    ><cn type="integer" >3</cn><cn type="integer" >9</cn></apply></apply><apply ><csymbol
    cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><cn type="integer" >3</cn><cn
    type="integer" >1</cn><cn type="integer" >1</cn><cn type="integer" >2</cn><cn
    type="integer" >2</cn><cn type="integer" >2</cn><cn type="integer" >3</cn><cn
    type="integer" >0</cn><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >2</cn><cn type="integer" >2</cn><cn type="integer" >0</cn><cn type="integer"
    >6</cn><cn type="integer" >2</cn><cn type="integer" >2</cn><cn type="integer"
    >3</cn><cn type="integer" >0</cn><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn><cn type="integer" >0</cn><cn type="integer" >0</cn><cn
    type="integer" >0</cn><cn type="integer" >0</cn><cn type="integer" >0</cn><cn
    type="integer" >0</cn><cn type="integer" >1</cn><cn type="integer" >1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{W_{3\times 9}}=\begin{pmatrix}3&1&1&2&2&2&3&0&0\\
    2&2&0&6&2&2&3&0&0\\ 0&0&0&0&0&0&0&1&1\\ \end{pmatrix}</annotation></semantics></math>
    |  |
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="block"><semantics ><mrow ><msub  ><mi >ğ–</mi><mrow ><mn  >ğŸ‘</mn><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mn >ğŸ—</mn></mrow></msub><mo >=</mo><mrow
    ><mo  >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mn >3</mn></mtd><mtd  ><mn >1</mn></mtd><mtd ><mn >1</mn></mtd><mtd  ><mn
    >2</mn></mtd><mtd ><mn >2</mn></mtd><mtd  ><mn >2</mn></mtd><mtd ><mn >3</mn></mtd><mtd  ><mn
    >0</mn></mtd><mtd ><mn >0</mn></mtd></mtr><mtr ><mtd  ><mn >2</mn></mtd><mtd ><mn
    >2</mn></mtd><mtd  ><mn >0</mn></mtd><mtd ><mn >6</mn></mtd><mtd  ><mn >2</mn></mtd><mtd
    ><mn >2</mn></mtd><m
- en: 'This is a simple example of a *distributed* representation. In a distributed
    representation (Plate, [1995](#bib.bib57); Hinton etÂ al., [1986](#bib.bib35))
    the informational content is distributed (hence the name) among multiple units,
    and at the same time each unit can contribute to the representation of multiple
    elements. Distributed representation has two evident advantages with respect to
    a distributed local representation: it is more efficient (in the example, the
    representation uses only 3 numbers instead of 9) and it does not treat each element
    as being equally different to any other. In fact, *mouse* and *cat* in this representation
    are more similar than *mouse* and *a*. In other words, this representation captures
    by construction something interesting about the set of symbols. The drawback is
    that symbols are altered and, hence, it may be difficult to interpret which symbol
    is given its distributed representation. In the example, the distributed representations
    for *eats* and *some* are exactly the same vector $\mathbf{W_{3\times 9}}\mathbf{e_{5}}=\mathbf{W_{3\times
    9}}\mathbf{e_{6}}$.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ª*åˆ†å¸ƒå¼*è¡¨ç¤ºçš„ç®€å•ç¤ºä¾‹ã€‚åœ¨åˆ†å¸ƒå¼è¡¨ç¤ºï¼ˆPlate, [1995](#bib.bib57); Hinton ç­‰, [1986](#bib.bib35)ï¼‰ä¸­ï¼Œä¿¡æ¯å†…å®¹åœ¨å¤šä¸ªå•å…ƒä¹‹é—´åˆ†å¸ƒï¼ˆå› æ­¤å¾—åï¼‰ï¼ŒåŒæ—¶æ¯ä¸ªå•å…ƒå¯ä»¥å¯¹å¤šä¸ªå…ƒç´ çš„è¡¨ç¤ºåšå‡ºè´¡çŒ®ã€‚ä¸åˆ†å¸ƒå¼å±€éƒ¨è¡¨ç¤ºç›¸æ¯”ï¼Œåˆ†å¸ƒå¼è¡¨ç¤ºæœ‰ä¸¤ä¸ªæ˜æ˜¾çš„ä¼˜åŠ¿ï¼šå®ƒæ›´é«˜æ•ˆï¼ˆåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œè¡¨ç¤ºåªä½¿ç”¨äº†3ä¸ªæ•°å­—è€Œä¸æ˜¯9ä¸ªï¼‰ä¸”å®ƒä¸ä¼šå°†æ¯ä¸ªå…ƒç´ è§†ä¸ºä¸å…¶ä»–å…ƒç´ åŒæ ·ä¸åŒã€‚å®é™…ä¸Šï¼Œåœ¨è¿™ç§è¡¨ç¤ºä¸­ï¼Œ*é¼ æ ‡*å’Œ*çŒ«*æ¯”*é¼ æ ‡*å’Œ*a*æ›´ç›¸ä¼¼ã€‚æ¢å¥è¯è¯´ï¼Œè¿™ç§è¡¨ç¤ºé€šè¿‡æ„é€ æ•æ‰äº†å…³äºç¬¦å·é›†åˆçš„ä¸€äº›æœ‰è¶£çš„ä¸œè¥¿ã€‚ç¼ºç‚¹æ˜¯ç¬¦å·è¢«æ”¹å˜ï¼Œå› æ­¤ï¼Œå¯èƒ½å¾ˆéš¾è§£é‡Šå“ªäº›ç¬¦å·æ˜¯ç»™å®šçš„åˆ†å¸ƒå¼è¡¨ç¤ºã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ*åƒ*å’Œ*ä¸€äº›*çš„åˆ†å¸ƒå¼è¡¨ç¤ºæ˜¯å®Œå…¨ç›¸åŒçš„å‘é‡
    $\mathbf{W_{3\times 9}}\mathbf{e_{5}}=\mathbf{W_{3\times 9}}\mathbf{e_{6}}$ã€‚
- en: 'Even for distributed representations in the general form, it is possible to
    define *concatenative composition* and *functional composition* to represent expressions.
    Vectors $\mathbf{W_{d\times n}}\mathbf{e_{i}}$ should be replaced to vectors $\mathbf{e_{i}}$
    in the definition of the concatenative compositionality and the functional compositionality.
    Equation (LABEL:conc) is translated to:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ˜¯ä¸€èˆ¬å½¢å¼çš„åˆ†å¸ƒå¼è¡¨ç¤ºï¼Œä¹Ÿå¯ä»¥å®šä¹‰*è¿æ¥æ€§ç»„åˆ*å’Œ*åŠŸèƒ½ç»„åˆ*æ¥è¡¨ç¤ºè¡¨è¾¾å¼ã€‚åœ¨è¿æ¥æ€§ç»„åˆæ€§å’ŒåŠŸèƒ½ç»„åˆæ€§çš„å®šä¹‰ä¸­ï¼Œå‘é‡ $\mathbf{W_{d\times
    n}}\mathbf{e_{i}}$ åº”æ›¿æ¢ä¸ºå‘é‡ $\mathbf{e_{i}}$ã€‚æ–¹ç¨‹ (LABEL:conc) ç¿»è¯‘ä¸ºï¼š
- en: '|  | $\mathbf{Y_{s}}=\mathbf{W_{d\times n}}\mathbf{conc(s)}=[\mathbf{W_{d\times
    n}}\mathbf{e}_{w_{1}}\ldots\mathbf{W_{d\times n}}\mathbf{e}_{w_{k}}]$ |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Y_{s}}=\mathbf{W_{d\times n}}\mathbf{conc(s)}=[\mathbf{W_{d\times
    n}}\mathbf{e}_{w_{1}}\ldots\mathbf{W_{d\times n}}\mathbf{e}_{w_{k}}]$ |  |'
- en: 'and Equation ([1](#S4.E1 "In 4 Symbolic and Distributed Representations: Interpretability
    and Concatenative Compositionality â€£ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey")) for additive functional compositionality becomes:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ ([1](#S4.E1 "åœ¨4ä¸­ç¬¦å·å’Œåˆ†å¸ƒå¼è¡¨ç¤ºï¼šå¯è§£é‡Šæ€§å’Œè¿æ¥æ€§ç»„åˆ â€£ ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒå¼è¡¨ç¤ºåœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼šä¸€é¡¹ç»¼è¿°"))
    å¯¹äºåŠ æ³•åŠŸèƒ½ç»„åˆæ€§å˜ä¸ºï¼š
- en: '|  | $\mathbf{y_{s}}=\mathbf{W_{d\times n}}\mathbf{func}_{\Sigma}(s)=\sum_{j=1}^{k}\mathbf{W_{d\times
    n}}\mathbf{e}_{j}$ |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{y_{s}}=\mathbf{W_{d\times n}}\mathbf{func}_{\Sigma}(s)=\sum_{j=1}^{k}\mathbf{W_{d\times
    n}}\mathbf{e}_{j}$ |  |'
- en: 'In the running example, the additive functional compositionality of sentence
    $s_{1}$ is:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œç¤ºä¾‹ä¸­ï¼Œå¥å­ $s_{1}$ çš„åŠ æ³•åŠŸèƒ½ç»„åˆæ€§æ˜¯ï¼š
- en: '|  | <math   alttext="\mathbf{y_{s_{1}}}=\mathbf{W_{3\times 9}}\mathbf{func}_{\Sigma}(s_{1})=\begin{pmatrix}8\\
    12\\'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{y_{s_{1}}}=\mathbf{W_{3\times 9}}\mathbf{func}_{\Sigma}(s_{1})=\begin{pmatrix}8\\
    12\\'
- en: 0\end{pmatrix}" display="block"><semantics ><mrow ><msub  ><mi >ğ²</mi><msub
    ><mi  >ğ¬</mi><mn >ğŸ</mn></msub></msub><mo >=</mo><mrow ><msub  ><mi >ğ–</mi><mrow
    ><mn  >ğŸ‘</mn><mo lspace="0.222em" rspace="0.222em"  >Ã—</mo><mn >ğŸ—</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi >ğŸğ®ğ§ğœ</mi><mi mathvariant="normal"
    >Î£</mi></msub><mo lspace="0em" rspace="0em" >â€‹</mo><mrow  ><mo stretchy="false"  >(</mo><msub
    ><mi >s</mi><mn  >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo  >(</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><mn  >8</mn></mtd></mtr><mtr ><mtd ><mn  >12</mn></mtd></mtr><mtr ><mtd
    ><mn  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ²</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ¬</ci><cn type="integer" >1</cn></apply></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ–</ci><apply ><cn
    type="integer" >3</cn><cn type="integer" >9</cn></apply></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğŸğ®ğ§ğœ</ci><ci  >Î£</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><cn type="integer" >1</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><cn type="integer"
    >8</cn></matrixrow><matrixrow ><cn type="integer" >12</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{y_{s_{1}}}=\mathbf{W_{3\times 9}}\mathbf{func}_{\Sigma}(s_{1})=\begin{pmatrix}8\\
    12\\ 0\end{pmatrix}</annotation></semantics></math> |  |
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`\mathbf{y_{s_{1}}}` = `\mathbf{W_{3\times 9}}` `\mathbf{func}_{\Sigma}(s_{1})`
    = `\begin{pmatrix}8\\ 12\\ 0\end{pmatrix}`'
- en: Clearly, in this case, it is extremely difficult to derive back the discrete
    symbolic sequence $s_{1}$ that has generated the final distributed representation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»æœ€ç»ˆçš„åˆ†å¸ƒå¼è¡¨ç¤ºä¸­æ¢å¤ç”Ÿæˆçš„ç¦»æ•£ç¬¦å·åºåˆ— $s_{1}$ æ˜¯æå…¶å›°éš¾çš„ã€‚
- en: 'Summing up, a distributed representation $y_{s}$ of an discrete symbolic expression
    $s$ is obtained by using an encoder that acts in two ways:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œç¦»æ•£ç¬¦å·è¡¨è¾¾ $s$ çš„åˆ†å¸ƒå¼è¡¨ç¤º $y_{s}$ æ˜¯é€šè¿‡ä½¿ç”¨ä¸€ç§åŒé‡ä½œç”¨çš„ç¼–ç å™¨è·å¾—çš„ï¼š
- en: â€¢
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: transforms symbols $w_{i}$ in vectors by using an embedding matrix $\mathbf{W_{d\times
    n}}$ and the local distributed representation $\mathbf{e_{i}}$ of $w_{i}$;
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨åµŒå…¥çŸ©é˜µ $\mathbf{W_{d\times n}}$ å’Œ $w_{i}$ çš„å±€éƒ¨åˆ†å¸ƒè¡¨ç¤º $\mathbf{e_{i}}$ï¼Œå°†ç¬¦å· $w_{i}$
    è½¬æ¢ä¸ºå‘é‡ï¼›
- en: â€¢
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: transposes the concatenative compositionality of the discrete symbolic expression
    $s$ in a functional compositionality by defining the used composition function
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å°†ç¦»æ•£ç¬¦å·è¡¨è¾¾ $s$ çš„ä¸²è”ç»„åˆæ€§é€šè¿‡å®šä¹‰æ‰€ä½¿ç”¨çš„ç»„åˆå‡½æ•°è½¬ç½®ä¸ºåŠŸèƒ½ç»„åˆæ€§
- en: 'When defining a distributed representation, we need to define two elements:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®šä¹‰åˆ†å¸ƒå¼è¡¨ç¤ºæ—¶ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸¤ä¸ªè¦ç´ ï¼š
- en: â€¢
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'an embedding matrix $\mathbf{W}$ that should balance these two different aims:
    (1) *maximize* interpretability, that is, inversion; (2) *maximize* similarity
    among different symbols for specific purposes.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåµŒå…¥çŸ©é˜µ $\mathbf{W}$ åº”è¯¥å¹³è¡¡è¿™ä¸¤ä¸ªä¸åŒçš„ç›®æ ‡ï¼šï¼ˆ1ï¼‰*æœ€å¤§åŒ–* å¯è§£é‡Šæ€§ï¼Œå³é€†è½¬ï¼›ï¼ˆ2ï¼‰*æœ€å¤§åŒ–* åœ¨ç‰¹å®šç›®çš„ä¸‹ä¸åŒç¬¦å·ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
- en: â€¢
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'the functional composition model: additive, holographic reduced representations
    (Plate, [1995](#bib.bib57)), recursive neural networks Schuster and Paliwal ([1997](#bib.bib63));
    Hochreiter and Schmidhuber ([1997](#bib.bib36)) or with attention Vaswani etÂ al.
    ([2017](#bib.bib69)); Devlin etÂ al. ([2018](#bib.bib19))'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŠŸèƒ½ç»„åˆæ¨¡å‹ï¼šåŠ æ³•çš„ï¼Œå…¨æ¯ç¼©å‡è¡¨ç¤ºï¼ˆPlateï¼Œ[1995](#bib.bib57)ï¼‰ï¼Œé€’å½’ç¥ç»ç½‘ç»œ Schuster å’Œ Paliwalï¼ˆ[1997](#bib.bib63)ï¼‰ï¼›Hochreiter
    å’Œ Schmidhuberï¼ˆ[1997](#bib.bib36)ï¼‰æˆ–å¸¦æœ‰æ³¨æ„åŠ›çš„ Vaswani ç­‰ï¼ˆ[2017](#bib.bib69)ï¼‰ï¼›Devlin
    ç­‰ï¼ˆ[2018](#bib.bib19)ï¼‰
- en: 'And, the final questions are: Whatâ€™s inside the distributed representation?
    Whatâ€™s exactly encoded? How this information is used to take decisions? Hence,
    the debated question become how concatenative is the functional compositionality
    in distributed representations behind neural networks? Can we retrieve discrete
    symbols and rebuild sequences?'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„é—®é¢˜æ˜¯ï¼šåˆ†å¸ƒå¼è¡¨ç¤ºä¸­åŒ…å«ä»€ä¹ˆï¼Ÿç©¶ç«Ÿç¼–ç äº†ä»€ä¹ˆï¼Ÿè¿™äº›ä¿¡æ¯å¦‚ä½•ç”¨äºåšå‡ºå†³ç­–ï¼Ÿå› æ­¤ï¼Œäº‰è®ºçš„é—®é¢˜å˜æˆäº†ç¥ç»ç½‘ç»œä¸­çš„åˆ†å¸ƒå¼è¡¨ç¤ºåœ¨åŠŸèƒ½ç»„åˆæ€§ä¸Šçš„ä¸²è”ç¨‹åº¦å¦‚ä½•ï¼Ÿæˆ‘ä»¬èƒ½å¦æ£€ç´¢ç¦»æ•£ç¬¦å·å¹¶é‡å»ºåºåˆ—ï¼Ÿ
- en: To answer the above questions, we then describe the two properties *Interpretability*
    and *concatenative compositionality* for distributed representations. These two
    properties want to measure how far are distributed representations from symbolic
    representations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å›ç­”ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æ¥ç€æè¿°åˆ†å¸ƒå¼è¡¨ç¤ºçš„ä¸¤ä¸ªå±æ€§ *å¯è§£é‡Šæ€§* å’Œ *ä¸²è”ç»„åˆæ€§*ã€‚è¿™ä¸¤ä¸ªå±æ€§æ—¨åœ¨è¡¡é‡åˆ†å¸ƒå¼è¡¨ç¤ºä¸ç¬¦å·è¡¨ç¤ºä¹‹é—´çš„è·ç¦»ã€‚
- en: 'Interpretability is the possibility of decoding distributed representations,
    that is, extracting the embedded symbolic representations. This is an important
    characteristic but it must be noted that itâ€™s not a simple yes-or-no classification.
    It is more a degree associated to specific representations. In fact, even if each
    component of a vector representation does not have a specific meaning, this does
    not mean that the representation is not interpretable as a whole, or that symbolic
    information cannot be recovered from it. For this reason, we can categorize the
    degree of interpretability of a representation as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§£é‡Šæ€§æ˜¯è§£ç åˆ†å¸ƒå¼è¡¨ç¤ºçš„å¯èƒ½æ€§ï¼Œå³æå–åµŒå…¥çš„ç¬¦å·è¡¨ç¤ºã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦ç‰¹æ€§ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªç®€å•çš„â€œæ˜¯â€æˆ–â€œå¦â€çš„åˆ†ç±»é—®é¢˜ã€‚å®ƒæ›´åƒæ˜¯ä¸ç‰¹å®šè¡¨ç¤ºç›¸å…³çš„ä¸€ä¸ªç¨‹åº¦ã€‚å®é™…ä¸Šï¼Œå³ä½¿ä¸€ä¸ªå‘é‡è¡¨ç¤ºçš„æ¯ä¸ªç»„ä»¶æ²¡æœ‰ç‰¹å®šçš„æ„ä¹‰ï¼Œè¿™å¹¶ä¸æ„å‘³ç€è¯¥è¡¨ç¤ºæ•´ä½“ä¸Šä¸å¯è§£é‡Šï¼Œæˆ–è€…æ— æ³•ä»ä¸­æ¢å¤ç¬¦å·ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼å¯¹è¡¨ç¤ºçš„å¯è§£é‡Šæ€§ç¨‹åº¦è¿›è¡Œåˆ†ç±»ï¼š
- en: '*human-interpretable* â€“ each dimension of a representation has a specific meaning;'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*äººç±»å¯è§£é‡Šçš„* â€” è¡¨ç¤ºçš„æ¯ä¸ªç»´åº¦å…·æœ‰ç‰¹å®šçš„æ„ä¹‰ï¼›'
- en: '*decodable* â€“ the representation may be obscure, but it can be decoded into
    an interpretable, symbolic representation.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å¯è§£ç * â€” è¡¨ç¤ºå¯èƒ½å¾ˆæ¨¡ç³Šï¼Œä½†å¯ä»¥è§£ç ä¸ºå¯è§£é‡Šçš„ç¬¦å·è¡¨ç¤ºã€‚'
- en: Concatenative Compositionality for distributed representations is the possibility
    of composing basic distributed representations with strong rules and of decomposing
    back composed representations with inverse rules. Generally, in NLP, basic distributed
    representations refer to basic symbols.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è¡¨ç¤ºçš„ä¸²è”ç»„åˆæ€§æ˜¯å°†åŸºæœ¬åˆ†å¸ƒå¼è¡¨ç¤ºæŒ‰ç…§å¼ºè§„åˆ™ç»„åˆï¼Œå¹¶æ ¹æ®é€†è§„åˆ™åˆ†è§£å›ç»„åˆè¡¨ç¤ºçš„å¯èƒ½æ€§ã€‚é€šå¸¸ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼ŒåŸºæœ¬åˆ†å¸ƒå¼è¡¨ç¤ºæŒ‡çš„æ˜¯åŸºæœ¬ç¬¦å·ã€‚
- en: The two axes of *Interpretability* and *Concatenative Compositionality for distributed
    representations* will be used to describe the presented distributed representations
    as we are interested in understanding whether or not a representation can be used
    to represent structures or sequences and whether it is possible to extract back
    the underlying structure or sequence given a distributed representation. It is
    clear that a local distributed representation is more interpretable than a distributed
    representation. Yet, both representations lack in concatenative compositionality
    when sequences or structures are collapsed in vectors or tensors that do not depend
    on the length of represented sequences or structures. For example, the bag-of-word
    local representation does not take into consideration the order of the symbols
    in the sequence.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä½¿ç”¨*å¯è§£é‡Šæ€§*å’Œ*åˆ†å¸ƒå¼è¡¨ç¤ºçš„è¿æ¥ç»„åˆæ€§*è¿™ä¸¤ä¸ªè½´æ¥æè¿°æ‰€å‘ˆç°çš„åˆ†å¸ƒå¼è¡¨ç¤ºï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›æ·±å…¥äº†è§£ä¸€ä¸ªè¡¨ç¤ºæ˜¯å¦å¯ä»¥ç”¨äºè¡¨ç¤ºç»“æ„æˆ–åºåˆ—ï¼Œä»¥åŠæ˜¯å¦å¯ä»¥åœ¨ç»™å®šåˆ†å¸ƒå¼è¡¨ç¤ºçš„æƒ…å†µä¸‹æå–å‡ºæ½œåœ¨çš„ç»“æ„æˆ–åºåˆ—ã€‚æ˜¾ç„¶ï¼Œæœ¬åœ°åˆ†å¸ƒå¼è¡¨ç¤ºæ¯”åˆ†å¸ƒå¼è¡¨ç¤ºæ›´å…·å¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå½“åºåˆ—æˆ–ç»“æ„è¢«å‹ç¼©æˆä¸ä¾èµ–äºè¡¨ç¤ºåºåˆ—æˆ–ç»“æ„é•¿åº¦çš„å‘é‡æˆ–å¼ é‡æ—¶ï¼Œä¸¤ç§è¡¨ç¤ºéƒ½ç¼ºä¹è¿æ¥ç»„åˆæ€§ã€‚ä¾‹å¦‚ï¼Œè¯è¢‹å±€éƒ¨è¡¨ç¤ºæ²¡æœ‰è€ƒè™‘ç¬¦å·åœ¨åºåˆ—ä¸­çš„é¡ºåºã€‚
- en: 5 Strategies to obtain distributed representations from symbols
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»ç¬¦å·ä¸­è·å–åˆ†å¸ƒå¼è¡¨ç¤ºçš„5ç§ç­–ç•¥
- en: 'There is a wide range of techniques to transform symbolic representations in
    distributed representations. When combining natural language processing and machine
    learning, this is a major issue: transforming symbols, sequences of symbols or
    symbolic structures in vectors or tensors that can be used in learning machines.
    These techniques generally propose a function $\eta$ to transform a *local representation*
    with a large number of dimensions in a *distributed representation* with a lower
    number of dimensions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬æ¢ç¬¦å·è¡¨ç¤ºä¸ºåˆ†å¸ƒå¼è¡¨ç¤ºçš„æŠ€æœ¯ç§ç±»ç¹å¤šã€‚å½“ç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸»è¦é—®é¢˜ï¼šå°†ç¬¦å·ã€ç¬¦å·åºåˆ—æˆ–ç¬¦å·ç»“æ„è½¬æ¢ä¸ºå¯ä»¥åœ¨å­¦ä¹ æœºå™¨ä¸­ä½¿ç”¨çš„å‘é‡æˆ–å¼ é‡ã€‚è¿™äº›æŠ€æœ¯é€šå¸¸æå‡ºä¸€ä¸ªå‡½æ•°$\eta$ï¼Œä»¥å°†å…·æœ‰å¤§é‡ç»´åº¦çš„*å±€éƒ¨è¡¨ç¤º*è½¬æ¢ä¸ºå…·æœ‰è¾ƒå°‘ç»´åº¦çš„*åˆ†å¸ƒå¼è¡¨ç¤º*ï¼š
- en: '|  | $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ |  |'
- en: This function is often called *encoder*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡½æ•°é€šå¸¸è¢«ç§°ä¸º*ç¼–ç å™¨*ã€‚
- en: 'We propose to categorize techniques to obtain distributed representations in
    two broad categories, showing some degree of overlapping:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®å°†è·å–åˆ†å¸ƒå¼è¡¨ç¤ºçš„æŠ€æœ¯åˆ†ä¸ºä¸¤ä¸ªå¹¿æ³›çš„ç±»åˆ«ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¸€å®šç¨‹åº¦çš„é‡å ï¼š
- en: â€¢
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: representations derived from dimensionality reduction techniques;
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»é™ç»´æŠ€æœ¯ä¸­è·å¾—çš„è¡¨ç¤ºï¼›
- en: â€¢
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: learned representations
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å­¦åˆ°çš„è¡¨ç¤º
- en: 'In the rest of the section, we will introduce the different strategies according
    to the proposed categorization. Moreover, we will emphasize its degree of interpretability
    for each representation and its related function $\eta$ by answering to two questions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ ¹æ®æå‡ºçš„åˆ†ç±»ä»‹ç»ä¸åŒçš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†é€šè¿‡å›ç­”ä¸¤ä¸ªé—®é¢˜æ¥å¼ºè°ƒæ¯ä¸ªè¡¨ç¤ºåŠå…¶ç›¸å…³å‡½æ•°$\eta$çš„è§£é‡Šç¨‹åº¦ï¼š
- en: â€¢
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Has a specific dimension in $\mathbb{R}^{d}$ a clear meaning?
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨$\mathbb{R}^{d}$ä¸­å…·æœ‰ç‰¹å®šç»´åº¦æ˜¯å¦æœ‰æ˜ç¡®å«ä¹‰ï¼Ÿ
- en: â€¢
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Can we decode an encoded symbolic representation? In other words, assuming a
    decoding function $\delta\colon\mathbb{R}^{d}\to\mathbb{R}^{n}$, how far is $v\in\mathbb{R}^{n}$,
    which represents a symbolic representation, from $v^{\prime}=\delta(\eta(v))$?
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è§£ç ä¸€ä¸ªç¼–ç çš„ç¬¦å·è¡¨ç¤ºå—ï¼Ÿæ¢å¥è¯è¯´ï¼Œå‡è®¾æœ‰ä¸€ä¸ªè§£ç å‡½æ•°$\delta\colon\mathbb{R}^{d}\to\mathbb{R}^{n}$ï¼Œé‚£ä¹ˆè¡¨ç¤ºç¬¦å·è¡¨ç¤ºçš„$v\in\mathbb{R}^{n}$è·ç¦»$v^{\prime}=\delta(\eta(v))$æœ‰å¤šè¿œï¼Ÿ
- en: 'Instead, composability of the resulting representations will be analyzed in
    Sec. [7](#S7 "7 Composing distributed representations â€£ Symbolic, Distributed
    and Distributional Representations for Natural Language Processing in the Era
    of Deep Learning: a Survey").'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œç»“æœè¡¨ç¤ºçš„å¯ç»„åˆæ€§å°†åœ¨ç¬¬[7](#S7 "7 ç»„æˆåˆ†å¸ƒå¼è¡¨ç¤º â€£ ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒåŒ–è¡¨ç¤ºåœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è°ƒæŸ¥")èŠ‚ä¸­è¿›è¡Œåˆ†æã€‚
- en: 5.1 Dimensionality reductio with Random Projections
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 éšæœºæŠ•å½±çš„é™ç»´
- en: '*Random projection* (RP) (Bingham and Mannila, [2001](#bib.bib9); Fodor, [2002](#bib.bib23))
    is a technique based on random matrices $W_{d}\in\mathbb{R}^{d\times n}$. Generally,
    the rows of the matrix $W_{d}$ are sampled from a Gaussian distribution with zero
    mean, and normalized as to have unit length (Johnson and Lindenstrauss, [1984](#bib.bib39))
    or even less complex random vectors (Achlioptas, [2003](#bib.bib1)). Random projections
    from Gaussian distributions approximately preserves pairwise distance between
    points (see the *Johnsonn-Lindenstrauss Lemma* (Johnson and Lindenstrauss, [1984](#bib.bib39))),
    that is, for any vector $x,y\in X$:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*éšæœºæŠ•å½±* (RP) (Bingham and Mannila, [2001](#bib.bib9); Fodor, [2002](#bib.bib23))
    æ˜¯ä¸€ç§åŸºäºéšæœºçŸ©é˜µ $W_{d}\in\mathbb{R}^{d\times n}$ çš„æŠ€æœ¯ã€‚é€šå¸¸ï¼ŒçŸ©é˜µ $W_{d}$ çš„è¡Œä»å‡å€¼ä¸ºé›¶çš„é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œå¹¶å½’ä¸€åŒ–ä¸ºå•ä½é•¿åº¦ï¼ˆJohnson
    and Lindenstrauss, [1984](#bib.bib39)ï¼‰ï¼Œæˆ–è€…ç”šè‡³ä»æ›´ç®€å•çš„éšæœºå‘é‡ä¸­é‡‡æ ·ï¼ˆAchlioptas, [2003](#bib.bib1)ï¼‰ã€‚é«˜æ–¯åˆ†å¸ƒä¸­çš„éšæœºæŠ•å½±å¤§è‡´ä¿ç•™ç‚¹ä¹‹é—´çš„æˆå¯¹è·ç¦»ï¼ˆå‚è§
    *Johnson-Lindenstrauss å¼•ç†*ï¼ˆJohnson and Lindenstrauss, [1984](#bib.bib39)ï¼‰ï¼Œå³ï¼Œå¯¹äºä»»ä½•å‘é‡
    $x,y\in X$ï¼š'
- en: '|  | $(1-\varepsilon)\ \&#124;\mathbf{x}-\mathbf{y}\&#124;^{2}\leq\&#124;W\mathbf{x}-W\mathbf{y}\&#124;^{2}\leq(1+\varepsilon)\
    \&#124;\mathbf{x}-\mathbf{y}\&#124;^{2}$ |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $(1-\varepsilon)\ \&#124;\mathbf{x}-\mathbf{y}\&#124;^{2}\leq\&#124;W\mathbf{x}-W\mathbf{y}\&#124;^{2}\leq(1+\varepsilon)\
    \&#124;\mathbf{x}-\mathbf{y}\&#124;^{2}$ |  |'
- en: 'where the approximation factor $\varepsilon$ depends on the dimension of the
    projection, namely, to assure that the approximation factor is $\varepsilon$,
    the dimension $k$ must be chosen such that:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­è¿‘ä¼¼å› å­ $\varepsilon$ å–å†³äºæŠ•å½±çš„ç»´åº¦ï¼Œå³ï¼Œä¸ºäº†ç¡®ä¿è¿‘ä¼¼å› å­ä¸º $\varepsilon$ï¼Œç»´åº¦ $k$ å¿…é¡»é€‰æ‹©å¦‚ä¸‹ï¼š
- en: '|  | $k\geq\frac{8\log(m)}{\varepsilon^{2}}$ |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $k\geq\frac{8\log(m)}{\varepsilon^{2}}$ |  |'
- en: 'Constraints for building the matrix $W$ can be significantly relaxed to less
    complex random vectors (Achlioptas, [2003](#bib.bib1)). Rows of the matrix can
    be sampled from very simple zero-mean distributions such as:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºçŸ©é˜µ $W$ çš„çº¦æŸå¯ä»¥æ˜¾è‘—æ”¾å®½åˆ°æ›´ç®€å•çš„éšæœºå‘é‡ï¼ˆAchlioptasï¼Œ[2003](#bib.bib1)ï¼‰ã€‚çŸ©é˜µçš„è¡Œå¯ä»¥ä»éå¸¸ç®€å•çš„é›¶å‡å€¼åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œä¾‹å¦‚ï¼š
- en: '|  | <math   alttext="W_{ij}=\sqrt{3}\begin{cases}+1\ \text{ with probability
    }\frac{1}{6}\\ -1\ \text{ with probability }\frac{1}{6}\\'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="W_{ij}=\sqrt{3}\begin{cases}+1\ \text{ with probability
    }\frac{1}{6}\\ -1\ \text{ with probability }\frac{1}{6}\\'
- en: 0\ \ \ \text{ with probability }\frac{2}{3}\\
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 0\ \ \ \text{ with probability }\frac{2}{3}\\
- en: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >W</mi><mrow ><mi  >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >j</mi></mrow></msub><mo >=</mo><mrow ><msqrt  ><mn
    >3</mn></msqrt><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  columnalign="left"
    ><mrow ><mo  >+</mo><mrow ><mn >1</mn><mo lspace="0.500em" rspace="0em" >â€‹</mo><mtext
    >Â with probabilityÂ </mtext><mo lspace="0em" rspace="0em" >â€‹</mo><mstyle displaystyle="false"
    ><mfrac ><mn  >1</mn><mn >6</mn></mfrac></mstyle></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >âˆ’</mo><mrow  ><mn >1</mn><mo lspace="0.500em"
    rspace="0em"  >â€‹</mo><mtext >Â with probabilityÂ </mtext><mo lspace="0em" rspace="0em"
    >â€‹</mo><mstyle displaystyle="false" ><mfrac ><mn  >1</mn><mn >6</mn></mfrac></mstyle></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mn >0</mn><mrow ><mtext >Â with probabilityÂ </mtext><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mstyle displaystyle="false"  ><mfrac ><mn >2</mn><mn
    >3</mn></mfrac></mstyle></mrow></mrow></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >ğ‘Š</ci><apply
    ><ci >ğ‘–</ci><ci  >ğ‘—</ci></apply></apply><apply ><apply ><cn type="integer"  >3</cn></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><apply ><apply  ><cn type="integer"  >1</cn><ci
    ><mtext >Â with probabilityÂ </mtext></ci><apply ><cn type="integer" >1</cn><cn
    type="integer"  >6</cn></apply></apply></apply><ci ><mtext >otherwise</mtext></ci><apply
    ><apply ><cn type="integer" >1</cn><ci ><mtext >Â with probabilityÂ </mtext></ci><apply
    ><cn type="integer" >1</cn><cn type="integer" >6</cn></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><list ><cn type="integer" >0</cn><apply ><ci ><mtext
    >Â with probabilityÂ </mtext></ci><apply ><cn type="integer" >2</cn><cn type="integer"
    >3</cn></apply></apply></list><ci ><mtext >otherwise</mtext></ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >W_{ij}=\sqrt{3}\begin{cases}+1\ \text{ with probability
    }\frac{1}{6}\\ -1\ \text{ with probability }\frac{1}{6}\\ 0\ \ \ \text{ with probability
    }\frac{2}{3}\\ \end{cases}</annotation></semantics></math> |  |
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`W_{ij}=\sqrt{3}\begin{cases}+1\ \text{ with probability }\frac{1}{6}\\ -1\
    \text{ with probability }\frac{1}{6}\\ 0\ \ \ \text{ with probability }\frac{2}{3}\\
    \end{cases}`'
- en: without the need to manually ensure unit-length of the rows, and at the same
    time providing a significant speed up in computation due to the sparsity of the
    projection.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ— éœ€æ‰‹åŠ¨ç¡®ä¿è¡Œçš„å•ä½é•¿åº¦ï¼ŒåŒæ—¶ç”±äºæŠ•å½±çš„ç¨€ç–æ€§ï¼Œè®¡ç®—é€Ÿåº¦æ˜¾è‘—åŠ å¿«ã€‚
- en: Unfortunately, vectors $\eta(\mathbf{v})$ are not *human-interpretable* as,
    even if their dimensions represent linear combinations of dimensions in the original
    local distribution, these dimensions have not an interpretation or particular
    properties.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œå‘é‡ $\eta(\mathbf{v})$ å¹¶ä¸æ˜¯*äººç±»å¯è§£é‡Šçš„*ï¼Œå³ä½¿å®ƒä»¬çš„ç»´åº¦è¡¨ç¤ºåŸå§‹å±€éƒ¨åˆ†å¸ƒä¸­çš„çº¿æ€§ç»„åˆï¼Œè¿™äº›ç»´åº¦ä¹Ÿæ²¡æœ‰æ˜ç¡®çš„è§£é‡Šæˆ–ç‰¹å®šå±æ€§ã€‚
- en: 'On the contrary, vectors $\eta(\mathbf{v})$ are *decodable*. The decoding function
    is:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œå‘é‡ $\eta(\mathbf{v})$ æ˜¯*å¯è§£ç çš„*ã€‚è§£ç å‡½æ•°æ˜¯ï¼š
- en: '|  | $\delta(\mathbf{v^{\prime}})=W_{d}^{T}\mathbf{v^{\prime}}$ |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta(\mathbf{v^{\prime}})=W_{d}^{T}\mathbf{v^{\prime}}$ |  |'
- en: and $W_{d}^{T}W_{d}\approx I$ when $W_{d}$ is derived using Gaussian random
    vectors. Hence, distributed vectors in $\mathbb{R}^{d}$ can be approximately decoded
    back in the original symbolic representation with a degree of approximation that
    depends on the distance between $d$ .
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ $W_{d}$ æ˜¯ä½¿ç”¨é«˜æ–¯éšæœºå‘é‡å¾—å‡ºçš„æ—¶ï¼Œ$W_{d}^{T}W_{d}\approx I$ã€‚å› æ­¤ï¼Œåˆ†å¸ƒå‘é‡åœ¨ $\mathbb{R}^{d}$
    ä¸­å¯ä»¥å¤§è‡´è§£ç å›åŸå§‹ç¬¦å·è¡¨ç¤ºï¼Œè¿‘ä¼¼åº¦å–å†³äº $d$ ä¹‹é—´çš„è·ç¦»ã€‚
- en: The major advantage of RP with respect to PCA is that the matrix $X$ of all
    the data points is not needed to derive the matrix $W_{d}$. Moreover, the matrix
    $W_{d}$ can be produced *Ã -la-carte* starting from the symbols encountered so
    far in the encoding procedure. In fact, it is sufficient to generate new Gaussian
    vectors for new symbols when they appear.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ PCA ç›¸æ¯”ï¼ŒRP çš„ä¸»è¦ä¼˜ç‚¹åœ¨äºï¼Œä¸éœ€è¦æ‰€æœ‰æ•°æ®ç‚¹çš„çŸ©é˜µ $X$ æ¥æ¨å¯¼çŸ©é˜µ $W_{d}$ã€‚æ­¤å¤–ï¼ŒçŸ©é˜µ $W_{d}$ å¯ä»¥*æŒ‰éœ€*ä»ç¼–ç è¿‡ç¨‹ä¸­é‡åˆ°çš„ç¬¦å·å¼€å§‹ç”Ÿæˆã€‚å®é™…ä¸Šï¼Œå½“å‡ºç°æ–°ç¬¦å·æ—¶ï¼Œåªéœ€ç”Ÿæˆæ–°çš„é«˜æ–¯å‘é‡å³å¯ã€‚
- en: 5.2 Learned representation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 å­¦ä¹ è¡¨ç¤º
- en: 'Learned representations differ from the dimensionality reduction techniques
    by the fact that: (1) encoding/decoding functions may not be linear; (2) learning
    can optimize functions that are different with respect to the target of PCA; and,
    (3) solutions are not derived in a closed form but are obtained using optimization
    techniques such as *stochastic gradient decent*.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„è¡¨ç¤ºä¸é™ç»´æŠ€æœ¯çš„ä¸åŒåœ¨äºï¼šï¼ˆ1ï¼‰ç¼–ç /è§£ç å‡½æ•°å¯èƒ½ä¸æ˜¯çº¿æ€§çš„ï¼›ï¼ˆ2ï¼‰å­¦ä¹ å¯ä»¥ä¼˜åŒ–ä¸ PCA ç›®æ ‡ä¸åŒçš„å‡½æ•°ï¼›ä»¥åŠï¼Œï¼ˆ3ï¼‰è§£å†³æ–¹æ¡ˆä¸æ˜¯ä»¥å°é—­å½¢å¼å¾—å‡ºçš„ï¼Œè€Œæ˜¯ä½¿ç”¨ä¼˜åŒ–æŠ€æœ¯å¦‚*éšæœºæ¢¯åº¦ä¸‹é™*è·å¾—çš„ã€‚
- en: 'Learned representation can be further classified into:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„è¡¨ç¤ºå¯ä»¥è¿›ä¸€æ­¥åˆ†ç±»ä¸ºï¼š
- en: â€¢
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*task-independent representations* learned with a standalone algorithm (as
    in *autoencoders* (Socher etÂ al., [2011](#bib.bib64); Liou etÂ al., [2014](#bib.bib45)))
    which is independent from any task, and which learns a representation that only
    depends from the dataset used;'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ä»»åŠ¡æ— å…³çš„è¡¨ç¤º*ï¼Œé€šè¿‡ç‹¬ç«‹ç®—æ³•ï¼ˆå¦‚*è‡ªç¼–ç å™¨*ï¼ˆSocher ç­‰ï¼Œ[2011](#bib.bib64); Liou ç­‰ï¼Œ[2014](#bib.bib45)ï¼‰ï¼‰å­¦ä¹ ï¼Œè¿™ä¸ä»»ä½•ä»»åŠ¡æ— å…³ï¼Œåªå­¦ä¹ ä¾èµ–äºä½¿ç”¨çš„æ•°æ®é›†çš„è¡¨ç¤ºï¼›'
- en: â€¢
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '*task-dependent representations* learned as the first step of another algorithm
    (this is called *end-to-end training*), usually the first layer of a deep neural
    network. In this case the new representation is driven by the task.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ä»»åŠ¡ç›¸å…³çš„è¡¨ç¤º* ä½œä¸ºå¦ä¸€ç®—æ³•çš„ç¬¬ä¸€æ­¥å­¦ä¹ ï¼ˆè¿™ç§°ä¸º*ç«¯åˆ°ç«¯è®­ç»ƒ*ï¼‰ï¼Œé€šå¸¸æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€å±‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–°è¡¨ç¤ºæ˜¯ç”±ä»»åŠ¡é©±åŠ¨çš„ã€‚'
- en: 5.2.1 Autoencoder
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 è‡ªç¼–ç å™¨
- en: Autoencoders are a task independent technique to learn a distributed representation
    encoder $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ by using local representations
    of a set of examples (Socher etÂ al., [2011](#bib.bib64); Liou etÂ al., [2014](#bib.bib45)).
    The distributed representation encoder $\eta$ is half of an autoencoder.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨æ˜¯ä¸€ç§ä»»åŠ¡æ— å…³çš„æŠ€æœ¯ï¼Œé€šè¿‡ä½¿ç”¨ä¸€ç»„ç¤ºä¾‹çš„å±€éƒ¨è¡¨ç¤ºæ¥å­¦ä¹ åˆ†å¸ƒè¡¨ç¤ºç¼–ç å™¨ $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ï¼ˆSocher
    ç­‰ï¼Œ[2011](#bib.bib64); Liou ç­‰ï¼Œ[2014](#bib.bib45)ï¼‰ã€‚åˆ†å¸ƒè¡¨ç¤ºç¼–ç å™¨ $\eta$ æ˜¯è‡ªç¼–ç å™¨çš„ä¸€åŠã€‚
- en: 'An autoencoder is a neural network that aims to reproduce an input vector in
    $\mathbb{R}^{n}$ as output by passing in a hidden layer(s) that are in $\mathbb{R}^{d}$.
    Given $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ and $\delta\colon\mathbb{R}^{d}\to\mathbb{R}^{n}$
    as the encoder and the decoder, respectively, an autoencoder aims to maximize
    the following function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨é€šè¿‡å°†è¾“å…¥å‘é‡ $\mathbb{R}^{n}$ é€šè¿‡éšè—å±‚ä¼ é€’ï¼Œé‡å»ºä¸ºè¾“å‡ºã€‚ç»™å®šç¼–ç å™¨ $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$
    å’Œè§£ç å™¨ $\delta\colon\mathbb{R}^{d}\to\mathbb{R}^{n}$ï¼Œè‡ªç¼–ç å™¨æ—¨åœ¨æœ€å¤§åŒ–ä»¥ä¸‹å‡½æ•°ï¼š
- en: '|  | $\mathcal{L}(\mathbf{x},\mathbf{x}^{\prime})=\&#124;\mathbf{x}-\mathbf{x}^{\prime}\&#124;^{2}$
    |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\mathbf{x},\mathbf{x}^{\prime})=\&#124;\mathbf{x}-\mathbf{x}^{\prime}\&#124;^{2}$
    |  |'
- en: where
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '|  | $\mathbf{x^{\prime}}=\delta(\eta(\mathbf{x}))$ |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x^{\prime}}=\delta(\eta(\mathbf{x}))$ |  |'
- en: The encoding and decoding module are two neural networks, which means that they
    are functions depending on a set of parameters $\theta$ of the form
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å’Œè§£ç æ¨¡å—æ˜¯ä¸¤ä¸ªç¥ç»ç½‘ç»œï¼Œè¿™æ„å‘³ç€å®ƒä»¬æ˜¯ä¾èµ–äºä¸€ç»„å‚æ•° $\theta$ çš„å‡½æ•°ï¼Œå…¶å½¢å¼ä¸º
- en: '|  | $\displaystyle\eta_{\theta}(x)=s(Wx+b)$ |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\eta_{\theta}(x)=s(Wx+b)$ |  |'
- en: '|  | $\displaystyle\delta_{\theta^{\prime}}(y)=s(W^{\prime}y+b^{\prime})$ |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\delta_{\theta^{\prime}}(y)=s(W^{\prime}y+b^{\prime})$ |  |'
- en: where the parameters of the entire model are $\theta,\theta^{\prime}=\left\{W,b,W^{\prime},b^{\prime}\right\}$
    with $W,W^{\prime}$ matrices, $b,b^{\prime}$ vectors and $s$ is a function that
    can be either a non-linearity sigmoid shaped function, or in some cases the identity
    function. In some variants the matrices $W$ and $W^{\prime}$ are constrained to
    $W^{T}=W^{\prime}$. This model is different with respect to PCA due to the target
    loss function and the use of non-linear functions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­æ•´ä¸ªæ¨¡å‹çš„å‚æ•°ä¸º $\theta,\theta^{\prime}=\left\{W,b,W^{\prime},b^{\prime}\right\}$ï¼Œ$W$
    å’Œ $W^{\prime}$ æ˜¯çŸ©é˜µï¼Œ$b$ å’Œ $b^{\prime}$ æ˜¯å‘é‡ï¼Œè€Œ $s$ æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå¯ä»¥æ˜¯éçº¿æ€§ sigmoid å½¢çŠ¶çš„å‡½æ•°ï¼Œæˆ–è€…åœ¨æŸäº›æƒ…å†µä¸‹æ˜¯æ’ç­‰å‡½æ•°ã€‚åœ¨æŸäº›å˜ä½“ä¸­ï¼ŒçŸ©é˜µ
    $W$ å’Œ $W^{\prime}$ è¢«é™åˆ¶ä¸º $W^{T}=W^{\prime}$ã€‚ç”±äºç›®æ ‡æŸå¤±å‡½æ•°å’Œéçº¿æ€§å‡½æ•°çš„ä½¿ç”¨ï¼Œè¿™ä¸ªæ¨¡å‹ä¸ PCA æœ‰æ‰€ä¸åŒã€‚
- en: 'Autoencoders have been further improved with *denoising autoencoders* (Vincent
    etÂ al., [2010](#bib.bib71), [2008](#bib.bib70); Masci etÂ al., [2011](#bib.bib48))
    that are a variant of autoencoders where the goal is to reconstruct the input
    from a corrupted version. The intuition is that higher level features should be
    robust with regard to small noise in the input. In particular, the input $\mathbf{x}$
    gets corrupted via a stochastic function:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨é€šè¿‡ *å»å™ªè‡ªç¼–ç å™¨*ï¼ˆVincent et al., [2010](#bib.bib71), [2008](#bib.bib70); Masci
    et al., [2011](#bib.bib48)ï¼‰å¾—åˆ°äº†è¿›ä¸€æ­¥çš„æ”¹è¿›ï¼Œè¿™æ˜¯è‡ªç¼–ç å™¨çš„ä¸€ç§å˜ä½“ï¼Œå…¶ç›®æ ‡æ˜¯ä»æŸåçš„ç‰ˆæœ¬ä¸­é‡æ„è¾“å…¥ã€‚ç›´è§‰æ˜¯è¾ƒé«˜å±‚æ¬¡çš„ç‰¹å¾åº”è¯¥å¯¹è¾“å…¥ä¸­çš„å°å™ªå£°å…·æœ‰é²æ£’æ€§ã€‚ç‰¹åˆ«åœ°ï¼Œè¾“å…¥
    $\mathbf{x}$ é€šè¿‡éšæœºå‡½æ•°è¿›è¡ŒæŸåï¼š
- en: '|  | $\tilde{\mathbf{x}}=g(\mathbf{x})$ |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\mathbf{x}}=g(\mathbf{x})$ |  |'
- en: 'and then one minimizes again the reconstruction error, but with regard to the
    *original* (uncorrupted) input:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå†æ¬¡æœ€å°åŒ–é‡æ„è¯¯å·®ï¼Œä½†ç›¸å¯¹äº *åŸå§‹*ï¼ˆæœªæŸåï¼‰è¾“å…¥ï¼š
- en: '|  | $\mathcal{L}(\mathbf{x},\mathbf{x}^{\prime})=\&#124;\mathbf{x}-\delta(\eta(g(\mathbf{x})))\&#124;^{2}$
    |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\mathbf{x},\mathbf{x}^{\prime})=\&#124;\mathbf{x}-\delta(\eta(g(\mathbf{x})))\&#124;^{2}$
    |  |'
- en: 'Usually $g$ can be either:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ $g$ å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š
- en: â€¢
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'adding gaussian noise: $g(\mathbf{x})=\mathbf{x}+\varepsilon$, where $\varepsilon\sim\mathcal{N}(0,\sigma\mathbb{I})$;'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ·»åŠ é«˜æ–¯å™ªå£°ï¼š$g(\mathbf{x})=\mathbf{x}+\varepsilon$ï¼Œå…¶ä¸­ $\varepsilon\sim\mathcal{N}(0,\sigma\mathbb{I})$ï¼›
- en: â€¢
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: 'masking noise: where a given a fraction $\nu$ of the components of the input
    gets set to $0$'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ©è”½å™ªå£°ï¼šå°†è¾“å…¥çš„ç»™å®šåˆ†é‡ $\nu$ è®¾ä¸º $0$
- en: For what concerns *intepretability*, as for random projection, distributed representations
    $\eta(\mathbf{v})$ obtained with encoders from autoencoders and denoising autoencoders
    are not *human-interpretable* but are *decodable* as this is the nature of autoencoders.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äº *å¯è§£é‡Šæ€§*ï¼Œä¸éšæœºæŠ•å½±ä¸€æ ·ï¼Œé€šè¿‡è‡ªç¼–ç å™¨å’Œå»å™ªè‡ªç¼–ç å™¨è·å¾—çš„åˆ†å¸ƒå¼è¡¨ç¤º $\eta(\mathbf{v})$ å¹¶ä¸ *äººç±»å¯è§£é‡Š*ï¼Œä½†å¯ä»¥ *è§£ç *ï¼Œå› ä¸ºè¿™æ­£æ˜¯è‡ªç¼–ç å™¨çš„ç‰¹æ€§ã€‚
- en: Moreover, *composability* is not covered by this formulation of autoencoders.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œ*ç»„åˆæ€§* ä¸åŒ…å«åœ¨è¿™ä¸ªè‡ªç¼–ç å™¨çš„è¡¨è¿°ä¸­ã€‚
- en: 5.2.2 Embedding layers
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 åµŒå…¥å±‚
- en: Embedding layers are generally the first layers of more complex neural networks
    which are responsible to transform an initial local representation in the first
    internal distributed representation. The main difference with autoencoders is
    that these layers are shaped by the entire overall learning process. The learning
    process is generally task dependent. Hence, these first embedding layers depend
    on the final task.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥å±‚é€šå¸¸æ˜¯æ›´å¤æ‚ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€å±‚ï¼Œè´Ÿè´£å°†åˆå§‹çš„å±€éƒ¨è¡¨ç¤ºè½¬æ¢ä¸ºç¬¬ä¸€ä¸ªå†…éƒ¨åˆ†å¸ƒå¼è¡¨ç¤ºã€‚ä¸è‡ªç¼–ç å™¨çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œè¿™äº›å±‚ç”±æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹å¡‘é€ ã€‚å­¦ä¹ è¿‡ç¨‹é€šå¸¸ä¾èµ–äºä»»åŠ¡ã€‚å› æ­¤ï¼Œè¿™äº›é¦–å±‚åµŒå…¥å±‚å–å†³äºæœ€ç»ˆä»»åŠ¡ã€‚
- en: It is argued that each layers learn a higher-level representation of its input.
    This is particularly visible with convolutional network (Krizhevsky etÂ al., [2012](#bib.bib42))
    applied to computer vision tasks. In these suggestive visualizations (Zeiler and
    Fergus, [2014b](#bib.bib81)), the hidden layers are seen to correspond to abstract
    feature of the image, starting from simple edges (in lower layers) up to faces
    in the higher ones.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äººè®¤ä¸ºï¼Œæ¯ä¸€å±‚å­¦ä¹ å…¶è¾“å…¥çš„æ›´é«˜çº§åˆ«çš„è¡¨ç¤ºã€‚è¿™åœ¨åº”ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å·ç§¯ç½‘ç»œï¼ˆKrizhevsky ç­‰ï¼Œ[2012](#bib.bib42)ï¼‰ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚åœ¨è¿™äº›å…·æœ‰å¯å‘æ€§çš„å¯è§†åŒ–ï¼ˆZeiler
    å’Œ Fergusï¼Œ[2014b](#bib.bib81)ï¼‰ä¸­ï¼Œéšè—å±‚è¢«è§†ä¸ºå¯¹åº”äºå›¾åƒçš„æŠ½è±¡ç‰¹å¾ï¼Œä»ç®€å•çš„è¾¹ç¼˜ï¼ˆåœ¨è¾ƒä½å±‚ï¼‰åˆ°æ›´é«˜å±‚çš„é¢å­”ã€‚
- en: However, these embedding layers produce encoding functions and, thus, distributed
    representations that are not interpretable when applied to symbols. In fact, these
    distributed representations are not human-interpretable as dimensions are not
    clearly related to specific aggregations of symbols. Moreover, these embedding
    layers do not naturally provide decoders. Hence, this distributed representation
    is not decodable.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™äº›åµŒå…¥å±‚äº§ç”Ÿçš„ç¼–ç å‡½æ•°ï¼Œå› æ­¤ï¼Œå½“åº”ç”¨äºç¬¦å·æ—¶ï¼Œè¿™äº›åˆ†å¸ƒå¼è¡¨ç¤ºæ˜¯ä¸å¯è§£é‡Šçš„ã€‚äº‹å®ä¸Šï¼Œè¿™äº›åˆ†å¸ƒå¼è¡¨ç¤ºåœ¨ç»´åº¦ä¸ç¬¦å·çš„å…·ä½“èšåˆå…³ç³»ä¸æ˜ç¡®æ—¶ï¼Œæ— æ³•è¢«äººç±»è§£é‡Šã€‚æ­¤å¤–ï¼Œè¿™äº›åµŒå…¥å±‚è‡ªç„¶ä¸æä¾›è§£ç å™¨ã€‚å› æ­¤ï¼Œè¿™ç§åˆ†å¸ƒå¼è¡¨ç¤ºæ˜¯ä¸å¯è§£ç çš„ã€‚
- en: 6 *Distributional* Representations as another side of the coin
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 *åˆ†å¸ƒå¼* è¡¨ç¤ºçš„å¦ä¸€é¢
- en: '*Distributional* semantics is an important area of research in natural language
    processing that aims to describe meaning of words and sentences with vectorial
    representations (see (Turney and Pantel, [2010](#bib.bib68)) for a survey). These
    representations are called *distributional representations*.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*åˆ†å¸ƒå¼* è¯­ä¹‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªé‡è¦ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡å‘é‡è¡¨ç¤ºæè¿°è¯æ±‡å’Œå¥å­çš„æ„ä¹‰ï¼ˆå‚è§ï¼ˆTurney å’Œ Pantelï¼Œ[2010](#bib.bib68)ï¼‰çš„ç»¼è¿°ï¼‰ã€‚è¿™äº›è¡¨ç¤ºè¢«ç§°ä¸º*åˆ†å¸ƒå¼è¡¨ç¤º*ã€‚'
- en: It is a strange historical accident that two similar sounding names â€“ *distributed*
    and *distributional* â€“ have been given to two concepts that should not be confused
    for many. Maybe, this has happened because the two concepts are definitely related.
    We argue that distributional representation are nothing more than a subset of
    distributed representations, and in fact can be categorized neatly into the divisions
    presented in the previous section
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¥‡æ€ªçš„å†å²å·§åˆæ˜¯ï¼Œä¸¤ä¸ªå‘éŸ³ç›¸ä¼¼çš„åç§°â€”â€”*åˆ†å¸ƒå¼* å’Œ *åˆ†å¸ƒ*â€”â€”è¢«èµ‹äºˆäº†ä¸¤ä¸ªä¸åº”æ··æ·†çš„æ¦‚å¿µã€‚ä¹Ÿè®¸ï¼Œè¿™ç§æƒ…å†µå‘ç”Ÿæ˜¯å› ä¸ºè¿™ä¸¤ä¸ªæ¦‚å¿µç¡®å®ç›¸å…³ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåˆ†å¸ƒå¼è¡¨ç¤ºä¸è¿‡æ˜¯åˆ†å¸ƒè¡¨ç¤ºçš„ä¸€ä¸ªå­é›†ï¼Œå®é™…ä¸Šå¯ä»¥æ•´é½åœ°å½’å…¥å‰ä¸€èŠ‚ä¸­æå‡ºçš„åˆ†ç±»ã€‚
- en: Distributional semantics is based on a famous slogan â€“ *â€œyou shall judge a word
    by the company it keepsâ€* (Firth, [1957](#bib.bib22)) â€“ and on the *distributional
    hypothesis* (Harris, [1964](#bib.bib33)) â€“ words have similar meaning if used
    in similar contexts, that is, words with the same or similar *distribution*. Hence,
    the name distributional as well as the core hypothesis comes from a linguistic
    rather than computer science background.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è¯­ä¹‰åŸºäºä¸€ä¸ªè‘—åçš„å£å·â€”â€”*â€œä½ å¯ä»¥é€šè¿‡ä¸€ä¸ªè¯æ‰€å¤„çš„ç¯å¢ƒæ¥åˆ¤æ–­å®ƒâ€*ï¼ˆFirthï¼Œ[1957](#bib.bib22)ï¼‰â€”â€”ä»¥åŠ*åˆ†å¸ƒå‡è®¾*ï¼ˆHarrisï¼Œ[1964](#bib.bib33)ï¼‰â€”â€”å¦‚æœè¯æ±‡åœ¨ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨ï¼Œåˆ™å…·æœ‰ç›¸ä¼¼çš„æ„ä¹‰ï¼Œå³å…·æœ‰ç›¸åŒæˆ–ç›¸ä¼¼çš„*åˆ†å¸ƒ*ã€‚å› æ­¤ï¼Œåˆ†å¸ƒå¼è¿™ä¸€åç§°ä»¥åŠæ ¸å¿ƒå‡è®¾æºäºè¯­è¨€å­¦è€Œéè®¡ç®—æœºç§‘å­¦èƒŒæ™¯ã€‚
- en: Distributional vectors represent words by describing information related to
    the contexts in which they appear. Put in this way it is apparent that a distributional
    representation *is* a specific case of a distributed representation, and the different
    name is only an indicator of the context in which this techniques originated.
    Representations for sentences are generally obtained combining vectors representing
    words.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼å‘é‡é€šè¿‡æè¿°è¯æ±‡å‡ºç°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥è¡¨ç¤ºè¯æ±‡ã€‚è¿™æ ·çœ‹æ¥ï¼Œåˆ†å¸ƒå¼è¡¨ç¤º*æ˜¯*åˆ†å¸ƒè¡¨ç¤ºçš„ä¸€ä¸ªç‰¹å®šæ¡ˆä¾‹ï¼Œä¸åŒçš„åç§°åªæ˜¯æŒ‡ç¤ºäº†è¿™äº›æŠ€æœ¯èµ·æºçš„ä¸Šä¸‹æ–‡ã€‚å¥å­çš„è¡¨ç¤ºé€šå¸¸æ˜¯é€šè¿‡ç»“åˆè¡¨ç¤ºè¯æ±‡çš„å‘é‡æ¥è·å¾—çš„ã€‚
- en: 'Hence, distributional semantics is a special case of distributed representations
    with a restriction on what can be used as features in vector spaces: features
    represent a bit of contextual information. Then, the largest body of research
    is on what should be used to represent contexts and how it should be taken into
    account. Once this is decided, large matrices $X$ representing words in context
    are collected and, then, dimensionality reduction techniques are applied to have
    treatable and more discriminative vectors.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåˆ†å¸ƒè¯­ä¹‰å­¦æ˜¯åˆ†å¸ƒå¼è¡¨ç¤ºçš„ä¸€ç§ç‰¹æ®Šæƒ…å†µï¼Œå…¶é™åˆ¶äº†å¯ä»¥åœ¨å‘é‡ç©ºé—´ä¸­ä½œä¸ºç‰¹å¾ä½¿ç”¨çš„å†…å®¹ï¼šç‰¹å¾ä»£è¡¨äº†ä¸€äº›ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç„¶åï¼Œæœ€å¤§çš„ç ”ç©¶é¢†åŸŸé›†ä¸­åœ¨åº”ä½¿ç”¨ä»€ä¹ˆæ¥è¡¨ç¤ºä¸Šä¸‹æ–‡ä»¥åŠå¦‚ä½•è€ƒè™‘è¿™äº›ä¸Šä¸‹æ–‡ã€‚ä¸€æ—¦å†³å®šäº†è¿™äº›å†…å®¹ï¼Œå°±ä¼šæ”¶é›†å¤§é‡çš„çŸ©é˜µ$X$æ¥è¡¨ç¤ºä¸Šä¸‹æ–‡ä¸­çš„è¯æ±‡ï¼Œç„¶ååº”ç”¨é™ç»´æŠ€æœ¯ä»¥è·å¾—å¯å¤„ç†å’Œæ›´å…·è¾¨åˆ«æ€§çš„å‘é‡ã€‚
- en: In the rest of the section, we present how to build matrices representing words
    in context, we will shortly recap on how dimensionality reduction techniques have
    been used in distributional semantics, and, finally, we report on word2vec (Mikolov
    etÂ al., [2013](#bib.bib49)), which is a novel distributional semantic techniques
    based on deep learning.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•æ„å»ºè¡¨ç¤ºä¸Šä¸‹æ–‡ä¸­çš„è¯æ±‡çš„çŸ©é˜µï¼Œæˆ‘ä»¬å°†ç®€è¦å›é¡¾ä¸€ä¸‹é™ç»´æŠ€æœ¯åœ¨åˆ†å¸ƒè¯­ä¹‰å­¦ä¸­çš„åº”ç”¨ï¼Œæœ€åï¼Œæˆ‘ä»¬å°†ä»‹ç»word2vecï¼ˆMikolov
    et al., [2013](#bib.bib49)ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ–°å‹åˆ†å¸ƒè¯­ä¹‰å­¦æŠ€æœ¯ã€‚
- en: 6.1 Building distributional representations for words from a corpus
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 ä»è¯­æ–™åº“ä¸­æ„å»ºè¯æ±‡çš„åˆ†å¸ƒå¼è¡¨ç¤º
- en: 'The major issue in distributional semantics is how to build distributional
    representations for words by observing word contexts in a collection of documents.
    In this section, we will describe these techniques using the example of the corpus
    in Table [1](#S6.T1 "Table 1 â€£ 6.1 Building distributional representations for
    words from a corpus â€£ 6 Distributional Representations as another side of the
    coin â€£ Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey").'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 'åˆ†å¸ƒè¯­ä¹‰å­¦ä¸­çš„ä¸»è¦é—®é¢˜æ˜¯å¦‚ä½•é€šè¿‡è§‚å¯Ÿæ–‡æ¡£é›†åˆä¸­çš„è¯æ±‡ä¸Šä¸‹æ–‡æ¥æ„å»ºè¯æ±‡çš„åˆ†å¸ƒå¼è¡¨ç¤ºã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥è¡¨[1](#S6.T1 "Table 1 â€£ 6.1
    Building distributional representations for words from a corpus â€£ 6 Distributional
    Representations as another side of the coin â€£ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey")ä¸­çš„è¯­æ–™åº“ä¸ºä¾‹æ¥æè¿°è¿™äº›æŠ€æœ¯ã€‚'
- en: '| $s_{1}$ | *a cat catches a mouse* |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| $s_{1}$ | *ä¸€åªçŒ«æŠ“ä½äº†ä¸€åªè€é¼ * |'
- en: '| $s_{2}$ | *a dog eats a mouse* |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| $s_{2}$ | *ä¸€åªç‹—åƒäº†ä¸€åªè€é¼ * |'
- en: '| $s_{3}$ | *a dog catches a cat* |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| $s_{3}$ | *ä¸€åªç‹—æŠ“ä½äº†ä¸€åªçŒ«* |'
- en: 'Table 1: A very small corpus'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨1ï¼šä¸€ä¸ªéå¸¸å°çš„è¯­æ–™åº“
- en: A first and simple distributional semantic representations of words is given
    by word vs. document matrices as those typical in information retrieval (Salton,
    [1989](#bib.bib61)). Word context are represented by document indexes. Then, words
    are similar if these words similarly appear in documents. This is generally referred
    as *topical similarity* (Landauer and Dumais, [1997](#bib.bib43)) as words belonging
    to the same topic tend to be more similar. An example of this approach is given
    by the matrix in Eq. LABEL:first_distributional_representation. In fact, this
    matrix is already a distributional and distributed representation for words which
    are represented as vectors in rows.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡çš„é¦–ä¸ªç®€å•çš„åˆ†å¸ƒè¯­ä¹‰è¡¨ç¤ºæ˜¯é€šè¿‡è¯æ±‡ä¸æ–‡æ¡£çŸ©é˜µæ¥å®ç°çš„ï¼Œè¿™äº›çŸ©é˜µåœ¨ä¿¡æ¯æ£€ç´¢ä¸­å¾ˆå¸¸è§ï¼ˆSalton, [1989](#bib.bib61)ï¼‰ã€‚è¯æ±‡ä¸Šä¸‹æ–‡é€šè¿‡æ–‡æ¡£ç´¢å¼•æ¥è¡¨ç¤ºã€‚ç„¶åï¼Œå¦‚æœè¿™äº›è¯æ±‡åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ–¹å¼ç›¸ä¼¼ï¼Œé‚£ä¹ˆè¿™äº›è¯æ±‡å°±è¢«è®¤ä¸ºæ˜¯ç›¸ä¼¼çš„ã€‚è¿™é€šå¸¸è¢«ç§°ä¸º*ä¸»é¢˜ç›¸ä¼¼æ€§*ï¼ˆLandauer
    and Dumais, [1997](#bib.bib43)ï¼‰ï¼Œå› ä¸ºå±äºåŒä¸€ä¸»é¢˜çš„è¯æ±‡å¾€å¾€æ›´ç›¸ä¼¼ã€‚è¯¥æ–¹æ³•çš„ä¸€ä¸ªç¤ºä¾‹ç”±å…¬å¼ LABEL:first_distributional_representation
    ä¸­çš„çŸ©é˜µç»™å‡ºã€‚å®é™…ä¸Šï¼Œè¿™ä¸ªçŸ©é˜µå·²ç»æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼å’Œåˆ†å¸ƒå¼çš„è¯æ±‡è¡¨ç¤ºï¼Œå…¶ä¸­è¯æ±‡åœ¨è¡Œä¸­ä½œä¸ºå‘é‡è¡¨ç¤ºã€‚
- en: A second strategy to build distributional representations for words is to build
    word vs. contextual feature matrices. These contextual features represent *proxies*
    for semantic attributes of modeled words (Baroni and Lenci, [2010](#bib.bib5)).
    For example, contexts of the word *dog* will somehow have relation with the fact
    that a dog has four legs, barks, eats, and so on. In this case, these vectors
    capture a similarity that is more related to a co-hyponymy, that is, words sharing
    similar attributes are similar. For example, *dog* is more similar to *cat* than
    to *car* as *dog* and *cat* share more attributes than *dog* and *car*. This is
    often referred as *attributional similarity* (Turney, [2006](#bib.bib67)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºè¯çš„åˆ†å¸ƒå¼è¡¨ç¤ºçš„ç¬¬äºŒç§ç­–ç•¥æ˜¯å»ºç«‹è¯ä¸ä¸Šä¸‹æ–‡ç‰¹å¾çŸ©é˜µã€‚è¿™äº›ä¸Šä¸‹æ–‡ç‰¹å¾ä»£è¡¨äº†å»ºæ¨¡è¯çš„*ä»£ç†*è¯­ä¹‰å±æ€§ï¼ˆBaroniå’ŒLenciï¼Œ[2010](#bib.bib5)ï¼‰ã€‚ä¾‹å¦‚ï¼Œ*dog*ï¼ˆç‹—ï¼‰çš„ä¸Šä¸‹æ–‡å°†ä¸ç‹—æœ‰å››æ¡è…¿ã€å å«ã€åƒä¸œè¥¿ç­‰äº‹å®æœ‰å…³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™äº›å‘é‡æ•æ‰åˆ°ä¸€ç§æ›´ç›¸å…³çš„ç›¸ä¼¼æ€§ï¼Œå³å…±ä¸Šä½è¯æ€§ï¼Œå³å…±äº«ç›¸ä¼¼å±æ€§çš„è¯æ˜¯ç›¸ä¼¼çš„ã€‚ä¾‹å¦‚ï¼Œ*dog*ï¼ˆç‹—ï¼‰ä¸*cat*ï¼ˆçŒ«ï¼‰çš„ç›¸ä¼¼æ€§è¦é«˜äºä¸*car*ï¼ˆè½¦ï¼‰çš„ç›¸ä¼¼æ€§ï¼Œå› ä¸º*dog*å’Œ*cat*å…±äº«çš„å±æ€§æ¯”*dog*å’Œ*car*æ›´å¤šã€‚è¿™é€šå¸¸è¢«ç§°ä¸º*å±æ€§ç›¸ä¼¼æ€§*ï¼ˆTurneyï¼Œ[2006](#bib.bib67)ï¼‰ã€‚
- en: 'A simple example of this second strategy are word-to-word matrices obtained
    by observing n-word windows of target words. For example, a word-to-word matrix
    obtained for the corpus in Table [1](#S6.T1 "Table 1 â€£ 6.1 Building distributional
    representations for words from a corpus â€£ 6 Distributional Representations as
    another side of the coin â€£ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey") by considering
    a 1-word window is the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¬¬äºŒç§ç­–ç•¥çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹æ˜¯é€šè¿‡è§‚å¯Ÿç›®æ ‡è¯çš„nè¯çª—å£è·å¾—çš„è¯å¯¹è¯çŸ©é˜µã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä¸€ä¸ª1è¯çª—å£ï¼Œå¯¹è¡¨æ ¼[1](#S6.T1 "è¡¨1 â€£ 6.1 ä»è¯­æ–™åº“æ„å»ºè¯çš„åˆ†å¸ƒå¼è¡¨ç¤º
    â€£ 6 åˆ†å¸ƒå¼è¡¨ç¤ºä½œä¸ºå¦ä¸€é¢ â€£ æ·±åº¦å­¦ä¹ æ—¶ä»£è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒå¼è¡¨ç¤ºï¼šç»¼è¿°")ä¸­çš„è¯­æ–™åº“è·å¾—çš„è¯å¯¹è¯çŸ©é˜µå¦‚ä¸‹ï¼š
- en: '|  | <math   alttext="X=\hbox{}\vbox{\kern 0.86108pt\hbox{$\kern 0.0pt\kern
    2.5pt\kern-5.0pt\left(\kern 0.0pt\kern-2.5pt\kern-6.66669pt\vbox{\kern-0.86108pt\vbox{\vbox{
    \halign{\kern\arraycolsep\hfil\@arstrut$\kbcolstyle#$\hfil\kern\arraycolsep&amp;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="X=\hbox{}\vbox{\kern 0.86108pt\hbox{$\kern 0.0pt\kern
    2.5pt\kern-5.0pt\left(\kern 0.0pt\kern-2.5pt\kern-6.66669pt\vbox{\kern-0.86108pt\vbox{\vbox{
    \halign{\kern\arraycolsep\hfil\@arstrut$\kbcolstyle#$\hfil\kern\arraycolsep&amp;'
- en: \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep&amp;&amp;
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep&amp;&amp;
- en: \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep\cr
    5.0pt\hfil\@arstrut$\scriptstyle$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    a$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle cat$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    dog$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle mouse$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    catches$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle eats\\a$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 2\\cat$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\dog$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1\\mouse$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\catches$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\eats$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\$\hfil\kern 5.0pt\crcr}}}}\right)$}}"
    display="block"><semantics ><mrow  ><mi >X</mi><mo  >=</mo> <mrow  ><mtext  ><xmath
    xmlns="http://dlmf.nist.gov/LaTeXML"  frag ><xmdual  frag ><xmref idref="S6.E2.m1.1.1.1.1.m1.43.43nest"
    frag ><xmwrap frag ><xmtok role="OPEN" stretchy="true"  frag >(</xmtok><xmarray
    vattach="bottom" frag ><xmrow frag ><xmcell align="left"   frag ><xmtext  frag
    ><xmtext  frag >\@arstrut</xmtext></xmtext></xmcell><xmcell align="left" frag
    ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >a</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >d</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag >o</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >g</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >m</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >o</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >u</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >s</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >e</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >c</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >a</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >t</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag
    >c</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >h</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic"  frag >e</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >e</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%"  frag >s</xmtok>\\<xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >a</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%"
    frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%"
    frag >2</xmtok>\\<xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag >c</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >t</xmtok></xmtext></xmcell><xmcell align="left"
    class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER"
    fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\<xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >d</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >o</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >g</xmtok></xmtext></xmcell><xmcell align="left" class="ltx_nopad_l ltx_nopad_r"  frag
    ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="1" role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok>\\<xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >m</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >o</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >u</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >s</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >e</xmtok></xmtext></xmcell><xmcell align="left" class="ltx_nopad_l ltx_nopad_r"  frag
    ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok>\\<xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >c</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%" frag >t</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >h</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >e</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="2"
    role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\<xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >e</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >t</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="1"
    role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\</xmtext></xmcell></xmrow></xmarray><xmtok
    role="CLOSE" stretchy="true"  frag >)</xmtok></xmwrap></xmref></xmdual></xmath></mtext></mrow></mrow>
    <annotation-xml encoding="MathML-Content" ><apply ><ci  >ğ‘‹</ci> <ci  ><mrow ><mtext  ><xmath
    xmlns="http://dlmf.nist.gov/LaTeXML"  frag ><xmdual  frag ><xmref idref="S6.E2.m1.1.1.1.1.m1.43.43anest"
    frag ><xmwrap frag ><xmtok role="OPEN" stretchy="true"  frag >(</xmtok><xmarray
    vattach="bottom" frag ><xmrow frag ><xmcell align="left"   frag ><xmtext  frag
    ><xmtext  frag >\@arstrut</xmtext></xmtext></xmcell><xmcell align="left" frag
    ><xmtext frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic"  frag >a</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >t</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >d</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >o</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >g</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >m</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >o</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >u</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag
    >s</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >e</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >c</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >h</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag
    >e</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >e</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >s</xmtok>\\<xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >a</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%"
    frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%"
    frag >2</xmtok>\\<xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag >c</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >t</xmtok></xmtext></xmcell><xmcell align="left"
    class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER"
    fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\<xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >d</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >o</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >g</xmtok></xmtext></xmcell><xmcell align="left" class="ltx_nopad_l ltx_nopad_r"  frag
    ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="1" role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok>\\<xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >m</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >o</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >u</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >s</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >e</xmtok></xmtext></xmcell><xmcell align="left" class="ltx_nopad_l ltx_nopad_r"  frag
    ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok>\\<xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >c</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%" frag >t</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >h</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >e</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="2"
    role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\<xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >e</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >t</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="1"
    role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\</xmtext></xmcell></xmrow></xmarray><xmtok
    role="CLOSE" stretchy="true"  frag >)</xmtok></xmwrap></xmref></xmdual></xmath></mtext></mrow></ci></apply></annotation-xml>
    <annotation encoding="application/x-tex" >X=\hbox{}\vbox{\kern 0.86108pt\hbox{$\kern
    0.0pt\kern 2.5pt\kern-5.0pt\left(\kern 0.0pt\kern-2.5pt\kern-6.66669pt\vbox{\kern-0.86108pt\vbox{\vbox{
    \halign{\kern\arraycolsep\hfil\@arstrut$\kbcolstyle#$\hfil\kern\arraycolsep& \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep&&
    \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep\cr
    5.0pt\hfil\@arstrut$\scriptstyle$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle a$\hfil\kern
    5.0pt&5.0pt\hfil$\scriptstyle cat$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle dog$\hfil\kern
    5.0pt&5.0pt\hfil$\scriptstyle mouse$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle catches$\hfil\kern
    5.0pt&5.0pt\hfil$\scriptstyle eats\\a$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 2\\cat$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0\\dog$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 1\\mouse$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0\\catches$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0\\eats$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0\\$\hfil\kern 5.0pt\crcr}}}}\right)$}}</annotation></semantics></math>
    |  | (2) |
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep\cr
    5.0pt\hfil\@arstrut$\scriptstyle$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    a$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle cat$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    dog$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle mouse$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    catches$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle eats\\a$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 2\\cat$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\dog$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1\\mouse$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\catches$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\eats$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\$\hfil\kern 5.0pt\crcr}}}}\right)$}}"
    display="block"><semantics ><mrow  ><mi >X</mi><mo  >=</mo> <mrow  ><mtext  ><xmath
    xmlns="http://dlmf.nist.gov/LaTeXML"  frag ><xmdual  frag ><xmref idref="S6.E2.m1.1.1.1.1.m1.43.43nest"
    frag ><xmwrap frag ><xmtok role="OPEN" stretchy="true"  frag >(</xmtok><xmarray
    vattach="bottom" frag ><xmrow frag ><xmcell align="left"   frag ><xmtext  frag
    ><xmtext  frag >\@arstrut</xmtext></xmtext></xmcell><xmcell align="left" frag
    ><xmtext frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic"  frag >a</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >t</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >d</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >o</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >g</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >m</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >o</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >u</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag
    >s</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >e</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >h</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag
    >e</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >e</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >s</xmtok>\\<xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >a</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
- en: Hence, the word *cat* is represented by the vector $\mathbf{cat}=\begin{pmatrix}2&amp;0&amp;0&amp;0&amp;1&amp;0\end{pmatrix}$
    and the similarity between *cat* and *dog* is higher than the similarity between
    *cat* and *mouse* as the cosine similarity $cos(\mathbf{cat},\mathbf{dog})$ is
    higher than the cosine similarity $cos(\mathbf{cat},\mathbf{mouse})$.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå•è¯*cat* è¢«è¡¨ç¤ºä¸ºå‘é‡ $\mathbf{cat}=\begin{pmatrix}2&amp;0&amp;0&amp;0&amp;1&amp;0\end{pmatrix}$ï¼Œå¹¶ä¸”*cat*
    ä¸*dog* ä¹‹é—´çš„ç›¸ä¼¼æ€§é«˜äº*cat* ä¸*mouse* ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå› ä¸ºä½™å¼¦ç›¸ä¼¼æ€§ $cos(\mathbf{cat},\mathbf{dog})$
    é«˜äºä½™å¼¦ç›¸ä¼¼æ€§ $cos(\mathbf{cat},\mathbf{mouse})$ã€‚
- en: 'The research on distributional semantics focuses on two aspects: (1) the best
    features to represent contexts; (2) the best correlation measure among target
    words and features.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹åˆ†å¸ƒå¼è¯­ä¹‰çš„ç ”ç©¶é›†ä¸­åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰è¡¨ç¤ºä¸Šä¸‹æ–‡çš„æœ€ä½³ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ç›®æ ‡å•è¯ä¸ç‰¹å¾ä¹‹é—´çš„æœ€ä½³ç›¸å…³æ€§åº¦é‡ã€‚
- en: How to represent contexts is a crucial problem in distributional semantics.
    This problem is strictly correlated to the classical question of feature definition
    and feature selection in machine learning. A wide variety of features have been
    tried. Contexts have been represented as set of relevant words, sets of relevant
    syntactic triples involving target words (Pado and Lapata, [2007](#bib.bib54);
    RothenhÃ¤usler and SchÃ¼tze, [2009](#bib.bib59)) and sets of labeled lexical triples
    (Baroni and Lenci, [2010](#bib.bib5)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•è¡¨ç¤ºä¸Šä¸‹æ–‡æ˜¯åˆ†å¸ƒå¼è¯­ä¹‰å­¦ä¸­çš„ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜ä¸æœºå™¨å­¦ä¹ ä¸­çš„ç‰¹å¾å®šä¹‰å’Œç‰¹å¾é€‰æ‹©çš„ç»å…¸é—®é¢˜å¯†åˆ‡ç›¸å…³ã€‚å·²ç»å°è¯•äº†å„ç§å„æ ·çš„ç‰¹å¾ã€‚ä¸Šä¸‹æ–‡è¢«è¡¨ç¤ºä¸ºç›¸å…³å•è¯çš„é›†åˆã€æ¶‰åŠç›®æ ‡å•è¯çš„ç›¸å…³å¥æ³•ä¸‰å…ƒç»„é›†åˆï¼ˆPado
    å’Œ Lapataï¼Œ[2007](#bib.bib54)ï¼›RothenhÃ¤usler å’Œ SchÃ¼tzeï¼Œ[2009](#bib.bib59)ï¼‰ä»¥åŠæ ‡è®°çš„è¯æ±‡ä¸‰å…ƒç»„é›†åˆï¼ˆBaroni
    å’Œ Lenciï¼Œ[2010](#bib.bib5)ï¼‰ã€‚
- en: Finding the best correlation measure among target words and their contextual
    features is the other issue. Many correlation measures have been tried. The classical
    measures are *term frequency-inverse document frequency* (*tf-idf*) (Salton, [1989](#bib.bib61))
    and *point-wise mutual information* ($pmi$). These, among other measures, are
    used to better capture the importance of contextual features for representing
    distributional semantic of words.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé—®é¢˜æ˜¯æ‰¾åˆ°ç›®æ ‡å•è¯åŠå…¶ä¸Šä¸‹æ–‡ç‰¹å¾ä¹‹é—´çš„æœ€ä½³ç›¸å…³æ€§åº¦é‡ã€‚å·²ç»å°è¯•äº†è®¸å¤šç›¸å…³æ€§åº¦é‡ã€‚ç»å…¸åº¦é‡åŒ…æ‹¬*è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡*ï¼ˆ*tf-idf*ï¼‰ï¼ˆSaltonï¼Œ[1989](#bib.bib61)ï¼‰å’Œ*ç‚¹å¯¹ç‚¹äº’ä¿¡æ¯*ï¼ˆ$pmi$ï¼‰ã€‚è¿™äº›åº¦é‡ï¼ŒåŠå…¶ä»–ä¸€äº›åº¦é‡ï¼Œç”¨äºæ›´å¥½åœ°æ•æ‰ä¸Šä¸‹æ–‡ç‰¹å¾åœ¨è¡¨ç¤ºå•è¯çš„åˆ†å¸ƒå¼è¯­ä¹‰ä¸­çš„é‡è¦æ€§ã€‚
- en: This first formulation of distributional semantics is a distributed representation
    that is *interpretable*. In fact, features represent contextual information which
    is a proxy for semantic attributes of target words (Baroni and Lenci, [2010](#bib.bib5)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼è¯­ä¹‰çš„é¦–æ¬¡è¡¨è¿°æ˜¯ä¸€ç§*å¯è§£é‡Š*çš„åˆ†å¸ƒå¼è¡¨ç¤ºã€‚äº‹å®ä¸Šï¼Œç‰¹å¾è¡¨ç¤ºçš„æ˜¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®ƒæ˜¯ç›®æ ‡å•è¯è¯­ä¹‰å±æ€§çš„ä»£ç†ï¼ˆBaroni å’Œ Lenciï¼Œ[2010](#bib.bib5)ï¼‰ã€‚
- en: 6.2 Compacting distributional representations
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 å‹ç¼©åˆ†å¸ƒå¼è¡¨ç¤º
- en: As distributed representations, *distributional representations* can undergo
    the process of dimensionality reduction with Principal Component Analysis and
    Random Indexing. This process is used for two issues. The first is the classical
    problem of reducing the dimensions of the representation to obtain more compact
    representations. The second instead want to help the representation to focus on
    more discriminative dimensions. This latter issue focuses on the feature selection
    and merging which is an important task in making these representations more effective
    on the final task of similarity detection.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºåˆ†å¸ƒå¼è¡¨ç¤ºï¼Œ*åˆ†å¸ƒå¼è¡¨ç¤º* å¯ä»¥é€šè¿‡ä¸»æˆåˆ†åˆ†æå’Œéšæœºç´¢å¼•è¿›è¡Œé™ç»´å¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹ç”¨äºä¸¤ä¸ªé—®é¢˜ã€‚ç¬¬ä¸€ä¸ªæ˜¯ç»å…¸çš„å°†è¡¨ç¤ºç»´åº¦å‡å°‘ä»¥è·å¾—æ›´ç´§å‡‘è¡¨ç¤ºçš„é—®é¢˜ã€‚ç¬¬äºŒä¸ªåˆ™æ˜¯å¸®åŠ©è¡¨ç¤ºé›†ä¸­äºæ›´å…·åŒºåˆ†æ€§çš„ç»´åº¦ã€‚åè€…é—®é¢˜å…³æ³¨ç‰¹å¾é€‰æ‹©å’Œåˆå¹¶ï¼Œè¿™æ˜¯ä½¿è¿™äº›è¡¨ç¤ºåœ¨æœ€ç»ˆç›¸ä¼¼æ€§æ£€æµ‹ä»»åŠ¡ä¸­æ›´æœ‰æ•ˆçš„é‡è¦ä»»åŠ¡ã€‚
- en: 'Principal Component Analysis (PCA) is largely applied in compacting distributional
    representations: Latent Semantic Analysis (LSA) is a prominent example (Landauer
    and Dumais, [1997](#bib.bib43)). LSA were born in Information Retrieval with the
    idea of reducing word-to-document matrices. Hence, in this compact representation,
    word context are documents and distributional vectors of words report on the documents
    where words appear. This or similar matrix reduction techniques have been then
    applied to word-to-word matrices.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åœ¨å‹ç¼©åˆ†å¸ƒå¼è¡¨ç¤ºæ–¹é¢å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼šæ½œåœ¨è¯­ä¹‰åˆ†æï¼ˆLSAï¼‰æ˜¯ä¸€ä¸ªçªå‡ºçš„ä¾‹å­ï¼ˆLandauer å’Œ Dumaisï¼Œ[1997](#bib.bib43)ï¼‰ã€‚LSAè¯ç”Ÿäºä¿¡æ¯æ£€ç´¢é¢†åŸŸï¼Œæ—¨åœ¨å‡å°‘å•è¯ä¸æ–‡æ¡£çš„çŸ©é˜µã€‚å› æ­¤ï¼Œåœ¨è¿™ç§ç´§å‡‘è¡¨ç¤ºä¸­ï¼Œå•è¯ä¸Šä¸‹æ–‡æ˜¯æ–‡æ¡£ï¼Œè€Œå•è¯çš„åˆ†å¸ƒå‘é‡æŠ¥å‘Šäº†å•è¯å‡ºç°çš„æ–‡æ¡£ã€‚è¿™ç§æˆ–ç±»ä¼¼çš„çŸ©é˜µé™ç»´æŠ€æœ¯éšåä¹Ÿè¢«åº”ç”¨äºå•è¯ä¸å•è¯çš„çŸ©é˜µã€‚
- en: Principal Component Analysis (PCA) (Markovsky, [2012](#bib.bib47); Pearson,
    [1901](#bib.bib55)) is a linear method which reduces the number of dimensions
    by projecting $\mathbb{R}^{n}$ into the *â€œbestâ€* linear subspace of a given dimension
    $d$ by using the a set of data points. The *â€œbestâ€* linear subspace is a subspace
    where dimensions maximize the variance of the data points in the set. PCA can
    be interpreted either as a probabilistic method or as a matrix approximation and
    is then usually known as *truncated singular value decomposition*. We are here
    interested in describing PCA as probabilistic method as it related to the *interpretability*
    of the related *distributed representation*.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼ˆMarkovsky, [2012](#bib.bib47); Pearson, [1901](#bib.bib55)ï¼‰æ˜¯ä¸€ç§çº¿æ€§æ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†$\mathbb{R}^{n}$æŠ•å½±åˆ°ä¸€ä¸ªç»™å®šç»´åº¦$d$çš„*â€œæœ€ä½³â€*çº¿æ€§å­ç©ºé—´ä¸­ï¼Œä»è€Œå‡å°‘ç»´åº¦æ•°é‡ï¼Œä½¿ç”¨ä¸€ç»„æ•°æ®ç‚¹ã€‚*â€œæœ€ä½³â€*çº¿æ€§å­ç©ºé—´æ˜¯ä¸€ä¸ªä½¿å¾—æ•°æ®ç‚¹çš„æ–¹å·®æœ€å¤§åŒ–çš„å­ç©ºé—´ã€‚PCAå¯ä»¥è¢«è§£é‡Šä¸ºä¸€ç§æ¦‚ç‡æ–¹æ³•ï¼Œä¹Ÿå¯ä»¥è¢«è§†ä¸ºçŸ©é˜µè¿‘ä¼¼ï¼Œé€šå¸¸è¢«ç§°ä¸º*æˆªæ–­å¥‡å¼‚å€¼åˆ†è§£*ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ„Ÿå…´è¶£çš„æ˜¯å°†PCAæè¿°ä¸ºæ¦‚ç‡æ–¹æ³•ï¼Œå› ä¸ºå®ƒä¸*å¯è§£é‡Šæ€§*å’Œç›¸å…³çš„*åˆ†å¸ƒè¡¨ç¤º*æœ‰å…³ã€‚
- en: As a probabilistic method, PCA finds an orthogonal projection matrix $W_{d}\in\mathbb{R}^{n\times
    d}$ such that the variance of the projected set of data points is maximized. The
    set of data points is referred as a matrix $X\in\mathbb{R}^{m\times n}$ where
    each row $\mathbf{x}_{i}^{T}\in\mathbb{R}^{n}$ is a single observation. Hence,
    the variance that is maximized is $\widehat{X}_{d}=XW_{d}^{T}\in\mathbb{R}^{m\times
    d}$.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§æ¦‚ç‡æ–¹æ³•ï¼ŒPCAå¯»æ‰¾ä¸€ä¸ªæ­£äº¤æŠ•å½±çŸ©é˜µ$W_{d}\in\mathbb{R}^{n\times d}$ï¼Œä½¿å¾—æŠ•å½±åçš„æ•°æ®ç‚¹é›†çš„æ–¹å·®æœ€å¤§åŒ–ã€‚æ•°æ®ç‚¹é›†è¢«ç§°ä¸ºçŸ©é˜µ$X\in\mathbb{R}^{m\times
    n}$ï¼Œå…¶ä¸­æ¯ä¸€è¡Œ$\mathbf{x}_{i}^{T}\in\mathbb{R}^{n}$æ˜¯ä¸€ä¸ªå•ç‹¬çš„è§‚æµ‹å€¼ã€‚å› æ­¤ï¼Œæœ€å¤§åŒ–çš„æ–¹å·®æ˜¯$\widehat{X}_{d}=XW_{d}^{T}\in\mathbb{R}^{m\times
    d}$ã€‚
- en: 'More specifically, letâ€™s consider the first weight vector $\mathbf{w_{1}}$,
    which maps an element of the dataset $\mathbf{x}$ into a single number $\langle\mathbf{x},\mathbf{w_{1}}\rangle$.
    Maximizing the variance means that $\mathbf{w}$ is such that:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°è¯´ï¼Œè€ƒè™‘ç¬¬ä¸€ä¸ªæƒé‡å‘é‡$\mathbf{w_{1}}$ï¼Œå®ƒå°†æ•°æ®é›†ä¸­çš„ä¸€ä¸ªå…ƒç´ $\mathbf{x}$æ˜ å°„ä¸ºä¸€ä¸ªå•ä¸€çš„æ•°å­—$\langle\mathbf{x},\mathbf{w_{1}}\rangle$ã€‚æœ€å¤§åŒ–æ–¹å·®æ„å‘³ç€$\mathbf{w}$æ˜¯è¿™æ ·çš„ï¼š
- en: '|  | $\mathbf{w_{1}}=\operatorname*{arg\,max}_{\&#124;\mathbf{w}\&#124;=1}\sum_{i}\left(\langle\mathbf{x_{i}},\mathbf{w}\rangle\right)^{2}$
    |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{w_{1}}=\operatorname*{arg\,max}_{\&#124;\mathbf{w}\&#124;=1}\sum_{i}\left(\langle\mathbf{x_{i}},\mathbf{w}\rangle\right)^{2}$
    |  |'
- en: 'and it can be shown that the optimal value is achieved when $\mathbf{w}$ is
    the eigenvector of $X^{T}X$ with largest eigenvalue. This then produces a projected
    dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è¯æ˜ï¼Œå½“$\mathbf{w}$æ˜¯çŸ©é˜µ$X^{T}X$ä¸­å…·æœ‰æœ€å¤§ç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡æ—¶ï¼Œè¾¾åˆ°æœ€ä¼˜å€¼ã€‚è¿™å°†äº§ç”Ÿä¸€ä¸ªæŠ•å½±æ•°æ®é›†ï¼š
- en: '|  | $\widehat{X}_{1}=X^{T}W_{1}=X^{T}\mathbf{w_{1}}$ |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{X}_{1}=X^{T}W_{1}=X^{T}\mathbf{w_{1}}$ |  |'
- en: 'The algorithm can then compute iteratively the second and further components
    by first subtracting the components already computed from $X$:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œç®—æ³•å¯ä»¥é€šè¿‡é¦–å…ˆä»$X$ä¸­å‡å»å·²ç»è®¡ç®—çš„ç»„ä»¶ï¼Œè¿­ä»£è®¡ç®—ç¬¬äºŒä¸ªåŠæ›´å¤šç»„ä»¶ï¼š
- en: '|  | $X-X\mathbf{w_{1}}\mathbf{w_{1}}^{T}$ |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | $X-X\mathbf{w_{1}}\mathbf{w_{1}}^{T}$ |  |'
- en: and then proceed as before. However, it turns out that all subsequent components
    are related to the eigenvectors of the matrix $X^{T}X$, that is, the $d$-th weight
    vector is the eigenvector of $X^{T}X$ with the $d$-th largest corresponding eigenvalue.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååƒä¹‹å‰ä¸€æ ·ç»§ç»­ã€‚ç„¶è€Œï¼Œäº‹å®è¯æ˜ï¼Œæ‰€æœ‰åç»­ç»„ä»¶éƒ½ä¸çŸ©é˜µ$X^{T}X$çš„ç‰¹å¾å‘é‡æœ‰å…³ï¼Œå³ï¼Œç¬¬$d$ä¸ªæƒé‡å‘é‡æ˜¯çŸ©é˜µ$X^{T}X$çš„ç¬¬$d$å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚
- en: 'The encoding matrix for distributed representations derived with a PCA method
    is the matrix:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PCAæ–¹æ³•å¾—åˆ°çš„åˆ†å¸ƒè¡¨ç¤ºçš„ç¼–ç çŸ©é˜µæ˜¯çŸ©é˜µï¼š
- en: '|  | <math   alttext="W_{d}=\left[\begin{array}[]{c}\mathbf{w}_{1}\\ \mathbf{w}_{2}\\'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="W_{d}=\left[\begin{array}[]{c}\mathbf{w}_{1}\\ \mathbf{w}_{2}\\'
- en: \ldots\\
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: \ldots\\
- en: \mathbf{w}_{d}\end{array}\right]\in\mathbb{R}^{d\times n}" display="block"><semantics
    ><mrow  ><msub ><mi >W</mi><mi  >d</mi></msub><mo >=</mo><mrow ><mo  >[</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><msub ><mi >ğ°</mi><mn  >1</mn></msub></mtd></mtr><mtr
    ><mtd  ><msub ><mi >ğ°</mi><mn  >2</mn></msub></mtd></mtr><mtr ><mtd  ><mi mathvariant="normal"  >â€¦</mi></mtd></mtr><mtr
    ><mtd  ><msub ><mi >ğ°</mi><mi  >d</mi></msub></mtd></mtr></mtable><mo >]</mo></mrow><mo
    >âˆˆ</mo><msup  ><mi >â„</mi><mrow ><mi  >d</mi><mo lspace="0.222em" rspace="0.222em"  >Ã—</mo><mi
    >n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘Š</ci><ci >ğ‘‘</ci></apply><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ°</ci><cn type="integer" >1</cn></apply></matrixrow><matrixrow ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ°</ci><cn type="integer" >2</cn></apply></matrixrow><matrixrow
    ><ci  >â€¦</ci></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ°</ci><ci  >ğ‘‘</ci></apply></matrixrow></matrix></apply></apply><apply ><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >â„</ci><apply ><ci  >ğ‘‘</ci><ci
    >ğ‘›</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >W_{d}=\left[\begin{array}[]{c}\mathbf{w}_{1}\\ \mathbf{w}_{2}\\ \ldots\\ \mathbf{w}_{d}\end{array}\right]\in\mathbb{R}^{d\times
    n}</annotation></semantics></math> |  |
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{w}_{d}\end{array}\right]\in\mathbb{R}^{d\times n}" display="block"><semantics
    ><mrow  ><msub ><mi >W</mi><mi  >d</mi></msub><mo >=</mo><mrow ><mo  >[</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><msub ><mi >ğ°</mi><mn  >1</mn></msub></mtd></mtr><mtr
    ><mtd  ><msub ><mi >ğ°</mi><mn  >2</mn></msub></mtd></mtr><mtr ><mtd  ><mi mathvariant="normal"  >â€¦</mi></mtd></mtr><mtr
    ><mtd  ><msub ><mi >ğ°</mi><mi  >d</mi></msub></mtd></mtr></mtable><mo >]</mo></mrow><mo
    >âˆˆ</mo><msup  ><mi >â„</mi><mrow ><mi  >d</mi><mo lspace="0.222em" rspace="0.222em"  >Ã—</mo><mi
    >n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘Š</ci><ci >ğ‘‘</ci></apply><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    xref="S6.Ex21.m1
- en: 'where $\mathbf{w}_{i}$ are eigenvectors with eigenvalues decreasing with $i$.
    Hence, local representations $\mathbf{v}\in\mathbb{R}^{n}$ are represented in
    distributed representations in $\mathbb{R}^{d}$ as:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{w}_{i}$ æ˜¯ç‰¹å¾å‘é‡ï¼Œå…¶ç‰¹å¾å€¼éš $i$ é€’å‡ã€‚å› æ­¤ï¼Œå±€éƒ¨è¡¨ç¤º $\mathbf{v}\in\mathbb{R}^{n}$
    è¡¨ç¤ºä¸ºåˆ†å¸ƒå¼è¡¨ç¤º $\mathbb{R}^{d}$ï¼š
- en: '|  | $\eta(\mathbf{v})=W_{d}\mathbf{v}$ |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eta(\mathbf{v})=W_{d}\mathbf{v}$ |  |'
- en: Hence, vectors $\eta(\mathbf{v})$ are *human-interpretable* as their dimensions
    represent linear combinations of dimensions in the original local representation
    and these dimensions are ordered according to their importance in the dataset,
    that is, their variance. Moreover, each dimension is a linear combination of the
    original symbols. Then, the matrix $W_{d}$ reports on which combination of the
    original symbols is more important to distinguish data points in the set.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå‘é‡ $\eta(\mathbf{v})$ æ˜¯*äººç±»å¯è§£é‡Šçš„*ï¼Œå› ä¸ºå®ƒä»¬çš„ç»´åº¦ä»£è¡¨äº†åŸå§‹å±€éƒ¨è¡¨ç¤ºä¸­ç»´åº¦çš„çº¿æ€§ç»„åˆï¼Œè¿™äº›ç»´åº¦æ ¹æ®åœ¨æ•°æ®é›†ä¸­çš„é‡è¦æ€§è¿›è¡Œæ’åºï¼Œå³å®ƒä»¬çš„æ–¹å·®ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªç»´åº¦æ˜¯åŸå§‹ç¬¦å·çš„çº¿æ€§ç»„åˆã€‚å› æ­¤ï¼ŒçŸ©é˜µ
    $W_{d}$ æŠ¥å‘Šäº†å“ªäº›åŸå§‹ç¬¦å·çš„ç»„åˆåœ¨åŒºåˆ†æ•°æ®ç‚¹æ—¶æ›´ä¸ºé‡è¦ã€‚
- en: 'Moreover, vectors $\eta(\mathbf{v})$ are *decodable*. The decoding function
    is:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå‘é‡ $\eta(\mathbf{v})$ æ˜¯*å¯è§£ç çš„*ã€‚è§£ç å‡½æ•°ä¸ºï¼š
- en: '|  | $\delta(\mathbf{v^{\prime}})=W_{d}^{T}\mathbf{v^{\prime}}$ |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta(\mathbf{v^{\prime}})=W_{d}^{T}\mathbf{v^{\prime}}$ |  |'
- en: and $W_{d}^{T}W_{d}=I$ if $d$ is the rank of the matrix $X$, otherwise it is
    a degraded approximation (for more details refer to (Fodor, [2002](#bib.bib23);
    Sorzano etÂ al., [2014](#bib.bib66))). Hence, distributed vectors in $\mathbb{R}^{d}$
    can be decoded back in the original symbolic representation with a degree of approximation
    that depends on the distance between $d$ and the rank of the matrix $X$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸” $W_{d}^{T}W_{d}=I$ å¦‚æœ $d$ æ˜¯çŸ©é˜µ $X$ çš„ç§©ï¼Œå¦åˆ™å®ƒæ˜¯é€€åŒ–çš„è¿‘ä¼¼ï¼ˆæœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ (Fodor, [2002](#bib.bib23);
    Sorzano et al., [2014](#bib.bib66) )ï¼‰ã€‚å› æ­¤ï¼Œåˆ†å¸ƒå¼å‘é‡åœ¨ $\mathbb{R}^{d}$ ä¸­å¯ä»¥ä»¥ä¸€å®šç¨‹åº¦çš„è¿‘ä¼¼è§£ç å›åŸå§‹ç¬¦å·è¡¨ç¤ºï¼Œè¿™å–å†³äº
    $d$ ä¸çŸ©é˜µ $X$ çš„ç§©ä¹‹é—´çš„è·ç¦»ã€‚
- en: The compelling limit of PCA is that all the data points have to be used in order
    to obtain the encoding/decoding matrices. This is not feasible in two cases. First,
    when the model has to deal with big data. Second, when the set of symbols to be
    encoded in extremely large. In this latter case, local representations cannot
    be used to produce matrices $X$ for applying PCA.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: PCAçš„ä¸€ä¸ªé‡è¦é™åˆ¶æ˜¯å¿…é¡»ä½¿ç”¨æ‰€æœ‰æ•°æ®ç‚¹æ¥è·å¾—ç¼–ç /è§£ç çŸ©é˜µã€‚è¿™åœ¨ä¸¤ç§æƒ…å†µä¸‹ä¸å¯è¡Œã€‚é¦–å…ˆï¼Œå½“æ¨¡å‹éœ€è¦å¤„ç†å¤§æ•°æ®æ—¶ã€‚å…¶æ¬¡ï¼Œå½“éœ€è¦ç¼–ç çš„ç¬¦å·é›†åˆæå…¶åºå¤§æ—¶ã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œå±€éƒ¨è¡¨ç¤ºä¸èƒ½ç”¨æ¥ç”ŸæˆçŸ©é˜µ
    $X$ ä»¥åº”ç”¨ PCAã€‚
- en: 'In Distributional Semantics, *random indexing* has been used to solve some
    issues that arise naturally with PCA when working with large vocabularies and
    large corpora. PCA has some scalability problems:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼è¯­ä¹‰ä¸­ï¼Œ*éšæœºç´¢å¼•* å·²è¢«ç”¨æ¥è§£å†³åœ¨å¤„ç†å¤§è¯æ±‡è¡¨å’Œå¤§è¯­æ–™åº“æ—¶è‡ªç„¶å‡ºç°çš„ä¸€äº›é—®é¢˜ã€‚PCA å­˜åœ¨ä¸€äº›å¯æ‰©å±•æ€§é—®é¢˜ï¼š
- en: â€¢
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: The original co-occurrence matrix is very costly to obtain and store, moreover,
    it is only needed to be later transformed;
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸå§‹å…±ç°çŸ©é˜µçš„è·å–å’Œå­˜å‚¨æˆæœ¬å¾ˆé«˜ï¼Œè€Œä¸”ï¼Œåæ¥ä»…éœ€è½¬æ¢ï¼›
- en: â€¢
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: Dimensionality reduction is also very costly, moreover, with the dimensions
    at hand it can only be done with iterative methods;
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é™ç»´ä¹Ÿæ˜¯éå¸¸æ˜‚è´µçš„ï¼Œè€Œä¸”ï¼Œè€ƒè™‘åˆ°å½“å‰çš„ç»´åº¦ï¼Œåªèƒ½é€šè¿‡è¿­ä»£æ–¹æ³•æ¥å®ç°ï¼›
- en: â€¢
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: The entire method is not incremental, if we want to add new words to our corpus
    we have to recompute the entire co-occurrence matrix and then re-perform the PCA
    step.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ•´ä¸ªæ–¹æ³•ä¸æ˜¯å¢é‡çš„ï¼Œå¦‚æœæˆ‘ä»¬è¦å°†æ–°è¯æ·»åŠ åˆ°è¯­æ–™åº“ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»é‡æ–°è®¡ç®—æ•´ä¸ªå…±ç°çŸ©é˜µï¼Œç„¶åé‡æ–°æ‰§è¡Œ PCA æ­¥éª¤ã€‚
- en: 'Random Indexing (Sahlgren, [2005](#bib.bib60)) solves these problems: it is
    an incremental method (new words can be easily added any time at low computational
    cost) which creates word vector of reduced dimension without the need to create
    the full dimensional matrix.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºç´¢å¼•ï¼ˆSahlgren, [2005](#bib.bib60)ï¼‰è§£å†³äº†è¿™äº›é—®é¢˜ï¼šå®ƒæ˜¯ä¸€ç§å¢é‡æ–¹æ³•ï¼ˆå¯ä»¥åœ¨ä»»ä½•æ—¶é—´ä»¥ä½è®¡ç®—æˆæœ¬è½»æ¾æ·»åŠ æ–°è¯ï¼‰ï¼Œç”Ÿæˆå‡å°‘ç»´åº¦çš„è¯å‘é‡ï¼Œè€Œæ— éœ€åˆ›å»ºå®Œæ•´ç»´åº¦çš„çŸ©é˜µã€‚
- en: Interpretability of compacted distributional semantic vectors is comparable
    to the interpretability of distributed representations obtained with the same
    techniques.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: å‹ç¼©çš„åˆ†å¸ƒå¼è¯­ä¹‰å‘é‡çš„å¯è§£é‡Šæ€§ä¸ä½¿ç”¨ç›¸åŒæŠ€æœ¯è·å¾—çš„åˆ†å¸ƒå¼è¡¨ç¤ºçš„å¯è§£é‡Šæ€§ç›¸å½“ã€‚
- en: '6.3 Learning representations: word2vec'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 å­¦ä¹ è¡¨ç¤ºï¼šword2vec
- en: '![Refer to caption](img/45c0fc96cdbd9e87392d359d3b4dd9f1.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/45c0fc96cdbd9e87392d359d3b4dd9f1.png)'
- en: 'Figure 1: word2vec: CBOW model'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 1ï¼šword2vec: CBOW æ¨¡å‹'
- en: 'Recently, *distributional hypothesis* has invaded neural networks: *word2vec*
    (Mikolov etÂ al., [2013](#bib.bib49)) uses contextual information to learn word
    vectors. Hence, we discuss this technique in the section devoted to *distributional
    semantics*.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œ*åˆ†å¸ƒå‡è®¾*å·²ä¾µå…¥ç¥ç»ç½‘ç»œï¼š*word2vec*ï¼ˆMikolovç­‰ï¼Œ[2013](#bib.bib49)ï¼‰ä½¿ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å­¦ä¹ å•è¯å‘é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨*åˆ†å¸ƒè¯­ä¹‰*éƒ¨åˆ†è®¨è®ºè¿™ä¸€æŠ€æœ¯ã€‚
- en: The name word2Vec comprises two similar techniques, called *skip grams* and
    *continuous bag of words* (CBOW). Both methods are neural networks, the former
    takes input a word and try to predict its context, while the latter does the reverse
    process, predicting a word from the words surrounding it. With this technique
    there is no explicitly computed co-occurrence matrix, and neither there is an
    explicit association feature between pairs of words, instead, the regularities
    and distribution of the words are learned implicitly by the network.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: word2Vecè¿™ä¸ªåå­—åŒ…æ‹¬ä¸¤ç§ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œç§°ä¸º*skip grams*å’Œ*è¿ç»­è¯è¢‹*ï¼ˆCBOWï¼‰ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½æ˜¯ç¥ç»ç½‘ç»œï¼Œå‰è€…ä»¥ä¸€ä¸ªå•è¯ä¸ºè¾“å…¥å¹¶å°è¯•é¢„æµ‹å…¶ä¸Šä¸‹æ–‡ï¼Œè€Œåè€…åˆ™åå‘å¤„ç†ï¼Œä»å‘¨å›´å•è¯é¢„æµ‹ä¸€ä¸ªå•è¯ã€‚ä½¿ç”¨è¿™ç§æŠ€æœ¯ï¼Œæ²¡æœ‰æ˜¾å¼è®¡ç®—çš„å…±ç°çŸ©é˜µï¼Œä¹Ÿæ²¡æœ‰æ˜¾å¼çš„å•è¯å¯¹å…³è”ç‰¹å¾ï¼Œç›¸åï¼Œå•è¯çš„è§„å¾‹æ€§å’Œåˆ†å¸ƒæ˜¯ç”±ç½‘ç»œéšå¼å­¦ä¹ çš„ã€‚
- en: 'We describe only CBOW because it is conceptually simpler and because the core
    ideas are the same in both cases. The full network is generally realized with
    two layers $W1_{n\times k}$ and $W2_{k\times n}$ plus a softmax layer to reconstruct
    the final vector representing the word. In the learning phase, the input and the
    output of the network are local representation for words. In CBOW, the network
    aims to predict a target word given context words. For example, given the sentence
    $s_{1}$ of the corpus in Table [1](#S6.T1 "Table 1 â€£ 6.1 Building distributional
    representations for words from a corpus â€£ 6 Distributional Representations as
    another side of the coin â€£ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey"), the network
    has to predict *catches* given its context (see Figure [1](#S6.F1 "Figure 1 â€£
    6.3 Learning representations: word2vec â€£ 6 Distributional Representations as another
    side of the coin â€£ Symbolic, Distributed and Distributional Representations for
    Natural Language Processing in the Era of Deep Learning: a Survey")).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬ä»…æè¿°CBOWï¼Œå› ä¸ºå®ƒåœ¨æ¦‚å¿µä¸Šæ›´ç®€å•ï¼Œè€Œä¸”ä¸¤è€…çš„æ ¸å¿ƒæ€æƒ³æ˜¯ç›¸åŒçš„ã€‚å®Œæ•´çš„ç½‘ç»œé€šå¸¸ç”±ä¸¤ä¸ªå±‚$W1_{n\times k}$å’Œ$W2_{k\times
    n}$ä»¥åŠä¸€ä¸ªsoftmaxå±‚æ¥é‡å»ºè¡¨ç¤ºå•è¯çš„æœ€ç»ˆå‘é‡ã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼Œç½‘ç»œçš„è¾“å…¥å’Œè¾“å‡ºæ˜¯å•è¯çš„å±€éƒ¨è¡¨ç¤ºã€‚åœ¨CBOWä¸­ï¼Œç½‘ç»œæ—¨åœ¨æ ¹æ®ä¸Šä¸‹æ–‡å•è¯é¢„æµ‹ç›®æ ‡å•è¯ã€‚ä¾‹å¦‚ï¼Œç»™å®šè¡¨æ ¼[1](#S6.T1
    "Table 1 â€£ 6.1 Building distributional representations for words from a corpus
    â€£ 6 Distributional Representations as another side of the coin â€£ Symbolic, Distributed
    and Distributional Representations for Natural Language Processing in the Era
    of Deep Learning: a Survey")ä¸­çš„å¥å­$s_{1}$ï¼Œç½‘ç»œå¿…é¡»æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹*catches*ï¼ˆè§å›¾[1](#S6.F1 "Figure
    1 â€£ 6.3 Learning representations: word2vec â€£ 6 Distributional Representations
    as another side of the coin â€£ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey")ï¼‰ã€‚'
- en: Hence, CBOW offers an encoder $W1_{n\times k}$, that is, a linear word encoder
    from data where $n$ is the size of the vocabulary and $k$ is the size of the distributional
    vector. This encoder models contextual information learned by maximizing the prediction
    capability of the network. A nice description on how this approach is related
    to previous techniques is given in (Goldberg and Levy, [2014](#bib.bib28)).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒCBOWæä¾›äº†ä¸€ä¸ªç¼–ç å™¨$W1_{n\times k}$ï¼Œå³ä»æ•°æ®ä¸­çº¿æ€§ç¼–ç å•è¯ï¼Œå…¶ä¸­$n$æ˜¯è¯æ±‡è¡¨çš„å¤§å°ï¼Œ$k$æ˜¯åˆ†å¸ƒå¼å‘é‡çš„å¤§å°ã€‚è¯¥ç¼–ç å™¨é€šè¿‡æœ€å¤§åŒ–ç½‘ç»œçš„é¢„æµ‹èƒ½åŠ›æ¥å»ºæ¨¡ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…³äºè¿™ç§æ–¹æ³•ä¸ä»¥å¾€æŠ€æœ¯çš„å…³ç³»çš„è¯¦ç»†æè¿°è§ï¼ˆGoldbergå’ŒLevyï¼Œ[2014](#bib.bib28)ï¼‰ã€‚
- en: 'Clearly, CBOW distributional vectors are not easily human and machine *interpretable*.
    In fact, specific dimensions of vectors have not a particular meaning and, differently
    from what happens for auto-encoders (see Sec. [5.2.1](#S5.SS2.SSS1 "5.2.1 Autoencoder
    â€£ 5.2 Learned representation â€£ 5 Strategies to obtain distributed representations
    from symbols â€£ Symbolic, Distributed and Distributional Representations for Natural
    Language Processing in the Era of Deep Learning: a Survey")), these networks are
    not trained to be invertible.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ˜¾ç„¶ï¼ŒCBOWåˆ†å¸ƒå¼å‘é‡æ—¢ä¸å®¹æ˜“è¢«äººç±»ä¹Ÿä¸å®¹æ˜“è¢«æœºå™¨*è§£é‡Š*ã€‚å®é™…ä¸Šï¼Œå‘é‡çš„ç‰¹å®šç»´åº¦æ²¡æœ‰ç‰¹å®šçš„å«ä¹‰ï¼Œå¹¶ä¸”ä¸è‡ªåŠ¨ç¼–ç å™¨ä¸åŒï¼ˆè§ç¬¬[5.2.1èŠ‚](#S5.SS2.SSS1
    "5.2.1 Autoencoder â€£ 5.2 Learned representation â€£ 5 Strategies to obtain distributed
    representations from symbols â€£ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey")ï¼‰ï¼Œè¿™äº›ç½‘ç»œå¹¶æœªç»è¿‡å¯é€†æ€§è®­ç»ƒã€‚'
- en: 7 Composing distributed representations
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 ç»„åˆåˆ†å¸ƒå¼è¡¨ç¤º
- en: In the previous sections, we described how one symbol or a bag-of-symbols can
    be transformed in distributed representations focusing on whether these distributed
    representations are *interpretable*. In this section, we want to investigate a
    second and important aspect of these representations, that is, have these representations
    *Concatenative Compositionality* as symbolic representations? And, if these representations
    are *composed*, are still *interpretable*?
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ä¸ªç¬¦å·æˆ–ç¬¦å·åŒ…å¦‚ä½•è¢«è½¬åŒ–ä¸ºåˆ†å¸ƒå¼è¡¨ç¤ºï¼Œé‡ç‚¹æ˜¯è¿™äº›åˆ†å¸ƒå¼è¡¨ç¤ºæ˜¯å¦æ˜¯*å¯è§£é‡Šçš„*ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æƒ³è¦æ¢è®¨è¿™äº›è¡¨ç¤ºçš„ç¬¬äºŒä¸ªé‡è¦æ–¹é¢ï¼Œå³è¿™äº›è¡¨ç¤ºæ˜¯å¦å…·æœ‰ä½œä¸ºç¬¦å·è¡¨ç¤ºçš„*è¿æ¥æ€§ç»„åˆæ€§*ï¼Ÿå¦‚æœè¿™äº›è¡¨ç¤ºæ˜¯*ç»„åˆçš„*ï¼Œå®ƒä»¬æ˜¯å¦ä»ç„¶æ˜¯*å¯è§£é‡Šçš„*ï¼Ÿ
- en: '*Concatenative Compositionality* is the ability of a symbolic representation
    to describe sequences or structures by composing symbols with specific rules.
    In this process, symbols remain distinct and composing rules are clear. Hence,
    final sequences and structures can be used for subsequent steps as knowledge repositories.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿æ¥æ€§ç»„åˆæ€§*æ˜¯æŒ‡ç¬¦å·è¡¨ç¤ºé€šè¿‡æŒ‰ç…§ç‰¹å®šè§„åˆ™ç»„åˆç¬¦å·æ¥æè¿°åºåˆ—æˆ–ç»“æ„çš„èƒ½åŠ›ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œç¬¦å·ä¿æŒç‹¬ç«‹ï¼Œç»„åˆè§„åˆ™ä¹Ÿå¾ˆæ˜ç¡®ã€‚å› æ­¤ï¼Œæœ€ç»ˆçš„åºåˆ—å’Œç»“æ„å¯ä»¥ä½œä¸ºçŸ¥è¯†åº“ç”¨äºåç»­æ­¥éª¤ã€‚'
- en: '*Concatenative Compositionality* is an important aspect for any representation
    and, then, for a distributed representation. Understanding to what extent a distributed
    representation has *concatenative compositionality* and how information can be
    recovered is then a critical issue. In fact, this issue has been strongly posed
    by Plate (Plate, [1995](#bib.bib57), [1994](#bib.bib56)) who analyzed how same
    specific distributed representations encode structural information and how this
    structural information can be recovered back.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿æ¥æ€§ç»„åˆæ€§*æ˜¯ä»»ä½•è¡¨ç¤ºä¸­çš„ä¸€ä¸ªé‡è¦æ–¹é¢ï¼Œå› æ­¤ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¡¨ç¤ºä¸­çš„ä¸€ä¸ªé‡è¦æ–¹é¢ã€‚ç†è§£ä¸€ä¸ªåˆ†å¸ƒå¼è¡¨ç¤ºåœ¨å¤šå¤§ç¨‹åº¦ä¸Šå…·å¤‡*è¿æ¥æ€§ç»„åˆæ€§*ä»¥åŠå¦‚ä½•æ¢å¤ä¿¡æ¯æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚äº‹å®ä¸Šï¼ŒPlateï¼ˆPlate,
    [1995](#bib.bib57), [1994](#bib.bib56)ï¼‰å·²ç»å¼ºçƒˆæå‡ºäº†è¿™ä¸ªé—®é¢˜ï¼Œä»–åˆ†æäº†ç›¸åŒçš„ç‰¹å®šåˆ†å¸ƒå¼è¡¨ç¤ºå¦‚ä½•ç¼–ç ç»“æ„ä¿¡æ¯ä»¥åŠè¿™äº›ç»“æ„ä¿¡æ¯å¦‚ä½•è¢«æ¢å¤ã€‚'
- en: 'Current approaches for treating distributed/distributional representation of
    sequences and structures mix two aspects in one model: a *â€œsemanticâ€* aspect and
    a *representational* aspect. Generally, the semantic aspect is the predominant
    and the representational aspect is left aside. For *â€œsemanticâ€* aspect, we refer
    to the reason why distributed symbols are composed: a final task in neural network
    applications or the need to give a *distributional semantic vector* for sequences
    of words. This latter is the case for *compositional distributional semantics*
    (Clark etÂ al., [2008](#bib.bib14); Baroni etÂ al., [2014](#bib.bib4)). For the
    *representational* aspect, we refer to the fact that composed distributed representations
    are in fact representing structures and these representations can be decoded back
    in order to extract what is in these structures.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰å¤„ç†åºåˆ—å’Œç»“æ„çš„åˆ†å¸ƒå¼/åˆ†å¸ƒè¡¨ç¤ºçš„æ–¹æ³•å°†*â€œè¯­ä¹‰â€*æ–¹é¢å’Œ*è¡¨ç¤º*æ–¹é¢æ··åˆåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ã€‚é€šå¸¸ï¼Œè¯­ä¹‰æ–¹é¢å æ®ä¸»å¯¼åœ°ä½ï¼Œè€Œè¡¨ç¤ºæ–¹é¢åˆ™è¢«å¿½ç•¥ã€‚å¯¹äº*â€œè¯­ä¹‰â€*æ–¹é¢ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯ä¸ºä»€ä¹ˆåˆ†å¸ƒå¼ç¬¦å·ä¼šè¢«ç»„åˆï¼šè¿™æ˜¯ç¥ç»ç½‘ç»œåº”ç”¨ä¸­çš„æœ€ç»ˆä»»åŠ¡ï¼Œæˆ–éœ€è¦ä¸ºè¯åºåˆ—æä¾›*åˆ†å¸ƒè¯­ä¹‰å‘é‡*ã€‚åè€…æ˜¯*ç»„åˆåˆ†å¸ƒè¯­ä¹‰*ï¼ˆClark
    et al., [2008](#bib.bib14); Baroni et al., [2014](#bib.bib4)ï¼‰çš„æƒ…å†µã€‚å¯¹äº*è¡¨ç¤º*æ–¹é¢ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯ç»„åˆçš„åˆ†å¸ƒå¼è¡¨ç¤ºå®é™…ä¸Šè¡¨ç¤ºäº†ç»“æ„ï¼Œå¹¶ä¸”è¿™äº›è¡¨ç¤ºå¯ä»¥è¢«è§£ç å›æ¥ï¼Œä»¥æå–è¿™äº›ç»“æ„ä¸­çš„å†…å®¹ã€‚
- en: Although the *â€œsemanticâ€* aspect seems to be predominant in *models-that-compose*,
    the *convolution conjecture* (Zanzotto etÂ al., [2015](#bib.bib78)) hypothesizes
    that the two aspects coexist and the *representational* aspect plays always a
    crucial role. According to this conjecture, structural information is preserved
    in any model that composes and structural information emerges back when comparing
    two distributed representations with dot product to determine their similarity.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡*â€œè¯­ä¹‰â€*æ–¹é¢åœ¨*ç»„åˆæ¨¡å‹*ä¸­ä¼¼ä¹å æ®ä¸»å¯¼åœ°ä½ï¼Œ*å·ç§¯çŒœæƒ³*ï¼ˆZanzotto et al., [2015](#bib.bib78)ï¼‰å´å‡è®¾è¿™ä¸¤ä¸ªæ–¹é¢æ˜¯å…±å­˜çš„ï¼Œå¹¶ä¸”*è¡¨ç¤º*æ–¹é¢å§‹ç»ˆå‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ ¹æ®è¿™ä¸€çŒœæƒ³ï¼Œåœ¨ä»»ä½•ç»„åˆæ¨¡å‹ä¸­ï¼Œç»“æ„ä¿¡æ¯éƒ½è¢«ä¿ç•™ï¼Œå½“é€šè¿‡ç‚¹ç§¯æ¯”è¾ƒä¸¤ä¸ªåˆ†å¸ƒå¼è¡¨ç¤ºä»¥ç¡®å®šå®ƒä»¬çš„ç›¸ä¼¼æ€§æ—¶ï¼Œç»“æ„ä¿¡æ¯ä¼šé‡æ–°æ˜¾ç°å‡ºæ¥ã€‚
- en: Hence, given the *convolution conjecture*, *models-that-compose* produce distributed
    representations for structures that can be interpreted back. *Interpretability*
    is a very important feature in these *models-that-compose* which will drive our
    analysis.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè€ƒè™‘åˆ°*å·ç§¯çŒœæƒ³*ï¼Œ*ç»„åˆæ¨¡å‹*äº§ç”Ÿçš„åˆ†å¸ƒå¼è¡¨ç¤ºå¯ä»¥è¢«è§£é‡Šå›æ¥ã€‚*å¯è§£é‡Šæ€§*æ˜¯è¿™äº›*ç»„åˆæ¨¡å‹*ä¸­çš„ä¸€ä¸ªéå¸¸é‡è¦çš„ç‰¹å¾ï¼Œå°†å¼•å¯¼æˆ‘ä»¬çš„åˆ†æã€‚
- en: In this section we will explore the issues faced with the compositionality of
    representations, and the main â€œtrendsâ€, which correspond somewhat to the categories
    already presented. In particular we will start from the work on compositional
    distributional semantics, then we revise the work on holographic reduced representations
    (Plate, [1995](#bib.bib57); Neumann, [2001](#bib.bib53)) and, finally, we analyze
    the recent approaches with recurrent and recursive neural networks. Again, these
    categories are not entirely disjoint, and methods presented in one class can be
    often interpreted to belonging into another class.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚å°†æ¢è®¨è¡¨ç¤ºçš„ç»„åˆæ€§æ‰€é¢ä¸´çš„é—®é¢˜ï¼Œä»¥åŠä¸»è¦çš„â€œè¶‹åŠ¿â€ï¼Œè¿™äº›è¶‹åŠ¿åœ¨æŸç§ç¨‹åº¦ä¸Šå¯¹åº”äºå·²ç»å‘ˆç°çš„ç±»åˆ«ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†ä»ç»„åˆåˆ†å¸ƒè¯­ä¹‰çš„å·¥ä½œå¼€å§‹ï¼Œç„¶åå›é¡¾å…¨æ¯é™ç»´è¡¨ç¤ºï¼ˆPlate,
    [1995](#bib.bib57); Neumann, [2001](#bib.bib53)ï¼‰çš„å·¥ä½œï¼Œæœ€ååˆ†æä½¿ç”¨é€’å½’å’Œé€’å½’ç¥ç»ç½‘ç»œçš„æœ€æ–°æ–¹æ³•ã€‚å†æ¬¡å¼ºè°ƒï¼Œè¿™äº›ç±»åˆ«å¹¶éå®Œå…¨ç‹¬ç«‹ï¼Œä¸€ä¸ªç±»åˆ«ä¸­ä»‹ç»çš„æ–¹æ³•é€šå¸¸å¯ä»¥è§£é‡Šä¸ºå±äºå¦ä¸€ä¸ªç±»åˆ«ã€‚
- en: 7.1 Compositional Distributional Semantics
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 ç»„åˆåˆ†å¸ƒè¯­ä¹‰å­¦
- en: In distributional semantics, *models-that-compose* have the name of *compositional
    distributional semantics models* (CDSMs) (Baroni etÂ al., [2014](#bib.bib4); Mitchell
    and Lapata, [2010](#bib.bib51)) and aim to apply the principle of compositionality
    (Frege, [1884](#bib.bib25); Montague, [1974](#bib.bib52)) to compute distributional
    semantic vectors for phrases. These CDSMs produce distributional semantic vectors
    of phrases by composing distributional vectors of words in these phrases. These
    models generally exploit *structured or syntactic representations* of phrases
    to derive their distributional meaning. Hence, CDSMs aim to give a complete semantic
    model for distributional semantics.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒè¯­ä¹‰å­¦ä¸­ï¼Œ*ç»„åˆæ¨¡å‹* è¢«ç§°ä¸º *ç»„åˆåˆ†å¸ƒè¯­ä¹‰æ¨¡å‹*ï¼ˆCDSMsï¼‰ï¼ˆBaroni et al., [2014](#bib.bib4); Mitchell
    and Lapata, [2010](#bib.bib51)ï¼‰ï¼Œå…¶ç›®æ ‡æ˜¯åº”ç”¨ç»„åˆæ€§åŸåˆ™ï¼ˆFrege, [1884](#bib.bib25); Montague,
    [1974](#bib.bib52)ï¼‰æ¥è®¡ç®—çŸ­è¯­çš„åˆ†å¸ƒè¯­ä¹‰å‘é‡ã€‚è¿™äº› CDSMs é€šè¿‡ç»„åˆçŸ­è¯­ä¸­è¯æ±‡çš„åˆ†å¸ƒå‘é‡æ¥ç”ŸæˆçŸ­è¯­çš„åˆ†å¸ƒè¯­ä¹‰å‘é‡ã€‚è¿™äº›æ¨¡å‹é€šå¸¸åˆ©ç”¨çŸ­è¯­çš„*ç»“æ„æˆ–å¥æ³•è¡¨ç¤º*æ¥æ¨å¯¼å®ƒä»¬çš„åˆ†å¸ƒè¯­ä¹‰ã€‚å› æ­¤ï¼ŒCDSMs
    æ—¨åœ¨ä¸ºåˆ†å¸ƒè¯­ä¹‰å­¦æä¾›å®Œæ•´çš„è¯­ä¹‰æ¨¡å‹ã€‚
- en: As in distributional semantics for words, the aim of CDSMs is to produce similar
    vectors for semantically similar sentences regardless their lengths or structures.
    For example, words and word definitions in dictionaries should have similar vectors
    as discussed in (Zanzotto etÂ al., [2010](#bib.bib79)). As usual in distributional
    semantics, similarity is captured with dot products (or similar metrics) among
    distributional vectors.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚åŒè¯æ±‡çš„åˆ†å¸ƒè¯­ä¹‰å­¦ä¸€æ ·ï¼ŒCDSMs çš„ç›®æ ‡æ˜¯ä¸ºè¯­ä¹‰ç›¸ä¼¼çš„å¥å­ç”Ÿæˆç›¸ä¼¼çš„å‘é‡ï¼Œè€Œä¸è€ƒè™‘å®ƒä»¬çš„é•¿åº¦æˆ–ç»“æ„ã€‚ä¾‹å¦‚ï¼Œè¯æ±‡å’Œè¯å…¸ä¸­çš„è¯ä¹‰åº”è¯¥å…·æœ‰ç±»ä¼¼çš„å‘é‡ï¼Œæ­£å¦‚ï¼ˆZanzotto
    et al., [2010](#bib.bib79)ï¼‰ä¸­æ‰€è®¨è®ºçš„é‚£æ ·ã€‚ä¸åˆ†å¸ƒè¯­ä¹‰å­¦ä¸­çš„å¸¸è§„åšæ³•ä¸€æ ·ï¼Œç›¸ä¼¼æ€§æ˜¯é€šè¿‡åˆ†å¸ƒå‘é‡ä¹‹é—´çš„ç‚¹ç§¯ï¼ˆæˆ–ç±»ä¼¼åº¦é‡ï¼‰æ¥æ•æ‰çš„ã€‚
- en: The applications of these CDSMs encompass multi-document summarization, recognizing
    textual entailment (Dagan etÂ al., [2013](#bib.bib17)) and, obviously, semantic
    textual similarity detection (Agirre etÂ al., [2013](#bib.bib2)).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº› CDSMs çš„åº”ç”¨åŒ…æ‹¬å¤šæ–‡æ¡£æ€»ç»“ã€è¯†åˆ«æ–‡æœ¬è•´æ¶µï¼ˆDagan et al., [2013](#bib.bib17)ï¼‰ä»¥åŠæ˜¾ç„¶çš„è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§æ£€æµ‹ï¼ˆAgirre
    et al., [2013](#bib.bib2)ï¼‰ã€‚
- en: Apparently, these CDSMs are far from having *concatenative compositionality*
    , since these distributed representations that can be *interpreted* back. In some
    sense, their nature wants that resulting vectors forget how these are obtained
    and focus on the final distributional meaning of phrases. There is some evidence
    that this is not exactly the case.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œè¿™äº› CDSMs è¿œæœªå…·æœ‰*ä¸²è”ç»„åˆæ€§*ï¼Œå› ä¸ºè¿™äº›åˆ†å¸ƒå¼è¡¨ç¤ºå¯ä»¥*è¢«è§£é‡Š*å›å»ã€‚åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œå®ƒä»¬çš„æœ¬è´¨è¦æ±‚ç»“æœå‘é‡å¿˜è®°å®ƒä»¬æ˜¯å¦‚ä½•è·å¾—çš„ï¼Œä¸“æ³¨äºçŸ­è¯­çš„æœ€ç»ˆåˆ†å¸ƒè¯­ä¹‰ã€‚å·²æœ‰ä¸€äº›è¯æ®è¡¨æ˜ï¼Œè¿™å¹¶éå®Œå…¨å¦‚æ­¤ã€‚
- en: The *convolution conjecture* (Zanzotto etÂ al., [2015](#bib.bib78)) suggests
    that many CDSMs produce distributional vectors where structural information and
    vectors for individual words can be still *interpreted*. Hence, many CDSMs have
    the *concatenative compositionality* property and *interpretable*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*å·ç§¯çŒœæƒ³*ï¼ˆZanzotto et al., [2015](#bib.bib78)ï¼‰æå‡ºï¼Œè®¸å¤š CDSMs ç”Ÿæˆçš„åˆ†å¸ƒå‘é‡ä¸­ï¼Œç»“æ„ä¿¡æ¯å’Œå•è¯çš„å‘é‡ä»ç„¶å¯ä»¥*è¢«è§£é‡Š*ã€‚å› æ­¤ï¼Œè®¸å¤š
    CDSMs å…·æœ‰*ä¸²è”ç»„åˆæ€§*å±æ€§å’Œ*å¯è§£é‡Šæ€§*ã€‚'
- en: In the rest of this section, we will show some classes of these CDSMs and we
    focus on describing how these morels are interpretable.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å±•ç¤ºè¿™äº› CDSMs çš„ä¸€äº›ç±»åˆ«ï¼Œå¹¶é‡ç‚¹æè¿°è¿™äº›æ¨¡å‹å¦‚ä½•å…·æœ‰è§£é‡Šæ€§ã€‚
- en: 7.1.1 Additive Models
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1 åŠ æ€§æ¨¡å‹
- en: '*Additive models* for compositional distributional semantics are important
    examples of *models-that-composes* where *semantic* and *representational* aspects
    is clearly separated. Hence, these models can be highly *interpretable*.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*åŠ æ³•æ¨¡å‹* å¯¹äºç»„åˆåˆ†å¸ƒè¯­ä¹‰æ˜¯ *ç»„åˆæ¨¡å‹* çš„é‡è¦ä¾‹å­ï¼Œå…¶ä¸­ *è¯­ä¹‰* å’Œ *è¡¨å¾* æ–¹é¢è¢«æ˜ç¡®åˆ†å¼€ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥å…·æœ‰å¾ˆé«˜çš„ *å¯è§£é‡Šæ€§*ã€‚'
- en: 'These additive models have been formally captured in the general framework
    for two words sequences proposed by Mitchell&Lapata (Mitchell and Lapata, [2008](#bib.bib50)).
    The general framework for composing distributional vectors of two word sequences
    *â€œu vâ€* is the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŠ æ³•æ¨¡å‹å·²è¢«æ­£å¼çº³å…¥ Mitchell&Lapata æå‡ºçš„ä¸¤è¯åºåˆ—çš„é€šç”¨æ¡†æ¶ï¼ˆMitchell å’Œ Lapataï¼Œ[2008](#bib.bib50)ï¼‰ã€‚ç»„åˆåˆ†å¸ƒå‘é‡çš„é€šç”¨æ¡†æ¶å¦‚ä¸‹ï¼š
- en: '|  | $\mathbf{p}=f(\mathbf{u},\mathbf{v};R;K)$ |  | (3) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{p}=f(\mathbf{u},\mathbf{v};R;K)$ |  | (3) |'
- en: 'where $\mathbf{p}\in\mathbb{R}^{n}$ is the composition vector, $\mathbf{u}$
    and $\mathbf{v}$ are the vectors for the two words *u* and *v*, $R$ is the grammatical
    relation linking the two words and $K$ is any other additional knowledge used
    in the composition operation. In the additive model, this equation has the following
    form:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\mathbf{p}\in\mathbb{R}^{n}$ æ˜¯ç»„åˆå‘é‡ï¼Œ$\mathbf{u}$ å’Œ $\mathbf{v}$ æ˜¯ä¸¤ä¸ªè¯ *u*
    å’Œ *v* çš„å‘é‡ï¼Œ$R$ æ˜¯è¿æ¥è¿™ä¸¤ä¸ªè¯çš„è¯­æ³•å…³ç³»ï¼Œ$K$ æ˜¯åœ¨ç»„åˆæ“ä½œä¸­ä½¿ç”¨çš„ä»»ä½•å…¶ä»–é™„åŠ çŸ¥è¯†ã€‚åœ¨åŠ æ³•æ¨¡å‹ä¸­ï¼Œè¿™ä¸ªæ–¹ç¨‹å…·æœ‰ä»¥ä¸‹å½¢å¼ï¼š
- en: '|  | $\mathbf{p}=f(\mathbf{u},\mathbf{v};R;K)=A_{R}\mathbf{u}+B_{R}\mathbf{v}$
    |  | (4) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{p}=f(\mathbf{u},\mathbf{v};R;K)=A_{R}\mathbf{u}+B_{R}\mathbf{v}$
    |  | (4) |'
- en: where $A_{R}$ and $B_{R}$ are two square matrices depending on the grammatical
    relation $R$ which may be learned from data (Zanzotto etÂ al., [2010](#bib.bib79);
    Guevara, [2010](#bib.bib32)).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $A_{R}$ å’Œ $B_{R}$ æ˜¯ä¸¤ä¸ªä¾èµ–äºè¯­æ³•å…³ç³» $R$ çš„æ–¹é˜µï¼Œè¿™äº›æ–¹é˜µå¯ä»¥é€šè¿‡æ•°æ®è¿›è¡Œå­¦ä¹ ï¼ˆZanzotto ç­‰ï¼Œ[2010](#bib.bib79);
    Guevaraï¼Œ[2010](#bib.bib32)ï¼‰ã€‚
- en: 'Before investigating if these models are interpretable, let introduce a recursive
    formulation of additive models which can be applied to structural representations
    of sentences. For this purpose, we use dependency trees. A dependency tree can
    be defined as a tree whose nodes are words and the typed links are the relations
    between two words. The root of the tree represents the word that governs the meaning
    of the sentence. A dependency tree $T$ is then a word if it is a final node or
    it has a root $r_{T}$ and links $(r_{T},R,C_{i})$ where $C_{i}$ is the i-th subtree
    of the node $r_{T}$ and $R$ is the relation that links the node $r_{T}$ with $C_{i}$.
    The dependency trees of two example sentences are reported in Figure [2](#S7.F2
    "Figure 2 â€£ 7.1.1 Additive Models â€£ 7.1 Compositional Distributional Semantics
    â€£ 7 Composing distributed representations â€£ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey"). The recursive formulation is then the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¢è®¨è¿™äº›æ¨¡å‹æ˜¯å¦å…·æœ‰å¯è§£é‡Šæ€§ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå¼•å…¥ä¸€ä¸ªå¯ä»¥åº”ç”¨äºå¥å­ç»“æ„è¡¨ç¤ºçš„åŠ æ³•æ¨¡å‹çš„é€’å½’è¡¨è¿°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¾èµ–æ ‘ã€‚ä¾èµ–æ ‘å¯ä»¥å®šä¹‰ä¸ºä¸€ä¸ªèŠ‚ç‚¹ä¸ºè¯çš„æ ‘ï¼Œç±»å‹åŒ–çš„é“¾æ¥æ˜¯ä¸¤ä¸ªè¯ä¹‹é—´çš„å…³ç³»ã€‚æ ‘çš„æ ¹ä»£è¡¨æ”¯é…å¥å­æ„ä¹‰çš„è¯ã€‚ä¸€ä¸ªä¾èµ–æ ‘
    $T$ æ˜¯ä¸€ä¸ªè¯ï¼Œå¦‚æœå®ƒæ˜¯æœ€ç»ˆèŠ‚ç‚¹æˆ–è€…å®ƒå…·æœ‰æ ¹ $r_{T}$ å’Œé“¾æ¥ $(r_{T},R,C_{i})$ï¼Œå…¶ä¸­ $C_{i}$ æ˜¯èŠ‚ç‚¹ $r_{T}$ çš„ç¬¬
    $i$ ä¸ªå­æ ‘ï¼Œ$R$ æ˜¯å°†èŠ‚ç‚¹ $r_{T}$ ä¸ $C_{i}$ è¿æ¥çš„å…³ç³»ã€‚ä¸¤ä¸ªç¤ºä¾‹å¥å­çš„ä¾èµ–æ ‘å¦‚å›¾ [2](#S7.F2 "å›¾ 2 â€£ 7.1.1
    åŠ æ³•æ¨¡å‹ â€£ 7.1 ç»„åˆåˆ†å¸ƒè¯­ä¹‰ â€£ 7 ç»„åˆåˆ†å¸ƒè¡¨ç¤º â€£ æ·±åº¦å­¦ä¹ æ—¶ä»£è‡ªç„¶è¯­è¨€å¤„ç†çš„ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒè¡¨ç¤ºï¼šç»¼è¿°") ä¸­æ‰€ç¤ºã€‚é€’å½’è¡¨è¿°å¦‚ä¸‹ï¼š
- en: '|  | $f_{r}(T)=\sum_{i}(A_{R}\mathbf{r_{T}}+B_{R}f_{r}(C_{i}))$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{r}(T)=\sum_{i}(A_{R}\mathbf{r_{T}}+B_{R}f_{r}(C_{i}))$ |  |'
- en: 'According to the recursive definition of the additive model, the function $f_{r}(T)$
    results in a linear combination of elements $M_{s}\mathbf{w}_{s}$ where $M_{s}$
    is a product of matrices that *represents the structure* and $\mathbf{w}_{s}$
    is the *distributional meaning* of one word in this structure, that is:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®åŠ æ³•æ¨¡å‹çš„é€’å½’å®šä¹‰ï¼Œå‡½æ•° $f_{r}(T)$ ç»“æœæ˜¯å…ƒç´  $M_{s}\mathbf{w}_{s}$ çš„çº¿æ€§ç»„åˆï¼Œå…¶ä¸­ $M_{s}$ æ˜¯ *è¡¨ç¤ºç»“æ„*
    çš„çŸ©é˜µä¹˜ç§¯ï¼Œè€Œ $\mathbf{w}_{s}$ æ˜¯è¯¥ç»“æ„ä¸­ä¸€ä¸ªè¯çš„ *åˆ†å¸ƒè¯­ä¹‰*ï¼Œå³ï¼š
- en: '|  | $f_{r}(T)=\sum_{s\in S(T)}\mathbf{M}_{s}\mathbf{w}_{s}$ |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{r}(T)=\sum_{s\in S(T)}\mathbf{M}_{s}\mathbf{w}_{s}$ |  |'
- en: 'where $S(T)$ are the relevant substructures of $T$. In this case, $S(T)$ contains
    the link chains. For example, the first sentence in Fig. [2](#S7.F2 "Figure 2
    â€£ 7.1.1 Additive Models â€£ 7.1 Compositional Distributional Semantics â€£ 7 Composing
    distributed representations â€£ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey") has a
    distributed vector defined in this way:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '$S(T)$ æ˜¯ $T$ çš„ç›¸å…³å­ç»“æ„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$S(T)$ åŒ…å«é“¾å¼é“¾æ¥ã€‚ä¾‹å¦‚ï¼Œå›¾ä¸­çš„ç¬¬ä¸€å¥è¯ [2](#S7.F2 "Figure 2 â€£
    7.1.1 Additive Models â€£ 7.1 Compositional Distributional Semantics â€£ 7 Composing
    distributed representations â€£ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey") ä¸­çš„åˆ†å¸ƒå¼å‘é‡æ˜¯è¿™æ ·å®šä¹‰çš„ï¼š'
- en: '|  | $\displaystyle f_{r}(\text{cows eat animal extracts})=$ |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{r}(\text{cows eat animal extracts})=$ |  |'
- en: '|  | $\displaystyle=A_{VN}\mathbf{eat}+B_{VN}\mathbf{cows}+A_{VN}\mathbf{eat}+$
    |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=A_{VN}\mathbf{eat}+B_{VN}\mathbf{cows}+A_{VN}\mathbf{eat}+$
    |  |'
- en: '|  | $\displaystyle+B_{VN}f_{r}(\text{animal extracts})=$ |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+B_{VN}f_{r}(\text{animal extracts})=$ |  |'
- en: '|  | $\displaystyle=A_{VN}\mathbf{eat}+B_{VN}\mathbf{cows}+A_{VN}\mathbf{eat}+$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=A_{VN}\mathbf{eat}+B_{VN}\mathbf{cows}+A_{VN}\mathbf{eat}+$
    |  |'
- en: '|  | $\displaystyle+B_{VN}A_{NN}\mathbf{extracts}+B_{VN}B_{NN}\mathbf{animal}$
    |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+B_{VN}A_{NN}\mathbf{extracts}+B_{VN}B_{NN}\mathbf{animal}$
    |  |'
- en: 'Each term of the sum has a part that represents the structure and a part that
    represents the meaning, for example:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œçš„æ¯ä¸€é¡¹éƒ½æœ‰ä¸€éƒ¨åˆ†è¡¨ç¤ºç»“æ„ï¼Œä¸€éƒ¨åˆ†è¡¨ç¤ºæ„ä¹‰ï¼Œä¾‹å¦‚ï¼š
- en: '|  | $\overbrace{B_{VN}B_{NN}}^{structure}\underbrace{\mathbf{beef}}_{meaning}$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\overbrace{B_{VN}B_{NN}}^{structure}\underbrace{\mathbf{beef}}_{meaning}$
    |  |'
- en: 'Hence, this recursive additive model for compositional semantics is a *model-that-composes*
    which, in principle, can be highly *interpretable*. By selecting matrices $\mathbf{M}_{s}$
    such that:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™ç§ç”¨äºç»„åˆè¯­ä¹‰çš„é€’å½’åŠ æ³•æ¨¡å‹æ˜¯ä¸€ä¸ª *ç»„åˆæ¨¡å‹*ï¼ŒåŸåˆ™ä¸Šå¯ä»¥é«˜åº¦ *å¯è§£é‡Š*ã€‚é€šè¿‡é€‰æ‹©çŸ©é˜µ $\mathbf{M}_{s}$ï¼Œä½¿å¾—ï¼š
- en: '|  | <math   alttext="\mathbf{M}_{s_{1}}^{T}\mathbf{M}_{s_{2}}\approx\begin{cases}\mathbf{I}&amp;s_{1}=s_{2}\\
    \mathbf{0}&amp;s_{1}\neq s_{2}\\'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{M}_{s_{1}}^{T}\mathbf{M}_{s_{2}}\approx\begin{cases}\mathbf{I}&amp;s_{1}=s_{2}\\
    \mathbf{0}&amp;s_{1}\neq s_{2}\\'
- en: \end{cases}" display="block"><semantics ><mrow ><mrow  ><msubsup ><mi >ğŒ</mi><msub  ><mi
    >s</mi><mn >1</mn></msub><mi  >T</mi></msubsup><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >ğŒ</mi><msub  ><mi >s</mi><mn >2</mn></msub></msub></mrow><mo >â‰ˆ</mo><mrow  ><mo
    >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mi >ğˆ</mi></mtd><mtd  columnalign="left" ><mrow ><msub  ><mi
    >s</mi><mn >1</mn></msub><mo >=</mo><msub  ><mi >s</mi><mn >2</mn></msub></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mn  >ğŸ</mn></mtd><mtd columnalign="left"  ><mrow ><msub
    ><mi  >s</mi><mn >1</mn></msub><mo >â‰ </mo><msub ><mi  >s</mi><mn >2</mn></msub></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğŒ</ci><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><cn type="integer" >1</cn></apply></apply><ci >ğ‘‡</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >ğŒ</ci><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘ </ci><cn type="integer" >2</cn></apply></apply></apply><apply
    ><csymbol cd="latexml"  >cases</csymbol><ci >ğˆ</ci><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘ </ci><cn type="integer" >1</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘ </ci><cn type="integer" >2</cn></apply></apply><cn type="integer"  >0</cn><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><cn type="integer"
    >1</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘ </ci><cn
    type="integer" >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{M}_{s_{1}}^{T}\mathbf{M}_{s_{2}}\approx\begin{cases}\mathbf{I}&s_{1}=s_{2}\\
    \mathbf{0}&s_{1}\neq s_{2}\\ \end{cases}</annotation></semantics></math> |  |
    (5) |
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><mrow  ><msubsup ><mi >ğŒ</mi><msub  ><mi
    >s</mi><mn >1</mn></msub><mi  >T</mi></msubsup><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi >ğŒ</mi><msub  ><mi >s</mi><mn >2</mn></msub></msub></mrow><mo >â‰ˆ</mo><mrow  ><mo
    >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mi >ğˆ</mi></mtd><mtd  columnalign="left" ><mrow ><msub  ><mi
    >s</mi><mn >1</mn></msub><mo >=</mo><msub  ><mi >s</mi><mn >2</mn></msub></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mn  >ğŸ</mn></mtd><mtd columnalign="left"  ><mrow ><msub
    ><mi  >s</mi><mn >1</mn></msub><mo >â‰ </mo><msub ><mi  xref="S7.E5.m1.4.4
- en: 'it is possible to recover distributional semantic vectors related to words
    that are in specific parts of the structure. For example, the main verb of the
    sample sentence in Fig. [2](#S7.F2 "Figure 2 â€£ 7.1.1 Additive Models â€£ 7.1 Compositional
    Distributional Semantics â€£ 7 Composing distributed representations â€£ Symbolic,
    Distributed and Distributional Representations for Natural Language Processing
    in the Era of Deep Learning: a Survey") with a matrix $A_{VN}^{T}$, that is:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯ä»¥æ¢å¤ä¸ç»“æ„ä¸­ç‰¹å®šéƒ¨åˆ†çš„å•è¯ç›¸å…³çš„åˆ†å¸ƒå¼è¯­ä¹‰å‘é‡ã€‚ä¾‹å¦‚ï¼Œå›¾[2](#S7.F2 "Figure 2 â€£ 7.1.1 Additive Models
    â€£ 7.1 Compositional Distributional Semantics â€£ 7 Composing distributed representations
    â€£ Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey")ä¸­æ ·æœ¬å¥å­çš„ä¸»è¦åŠ¨è¯ä¸çŸ©é˜µ$A_{VN}^{T}$ï¼Œå³ï¼š'
- en: '|  | $A_{VN}^{T}f_{r}(\text{cows eat animal extracts})\approx 2\mathbf{eat}$
    |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{VN}^{T}f_{r}(\text{cows eat animal extracts})\approx 2\mathbf{eat}$
    |  |'
- en: In general, matrices derived for compositional distributional semantic models
    (Guevara, [2010](#bib.bib32); Zanzotto etÂ al., [2010](#bib.bib79)) do not have
    this property but it is possible to obtain matrices with this property by applying
    thee Jonson-Linderstrauss Tranform (Johnson and Lindenstrauss, [1984](#bib.bib39))
    or similar techniques as discussed also in (Zanzotto etÂ al., [2015](#bib.bib78)).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œä¸ºç»„åˆåˆ†å¸ƒè¯­ä¹‰æ¨¡å‹ï¼ˆGuevara, [2010](#bib.bib32); Zanzotto et al., [2010](#bib.bib79)ï¼‰æ¨å¯¼å‡ºçš„çŸ©é˜µå¹¶ä¸å…·å¤‡è¿™ç§å±æ€§ï¼Œä½†å¯ä»¥é€šè¿‡åº”ç”¨ä¹”æ©é€Š-æ—ç™»æ–½ç‰¹åŠ³æ–¯å˜æ¢ï¼ˆJohnson
    and Lindenstrauss, [1984](#bib.bib39)ï¼‰æˆ–ç±»ä¼¼æŠ€æœ¯æ¥è·å¾—å…·å¤‡è¿™ç§å±æ€§çš„çŸ©é˜µï¼Œç›¸å…³è®¨è®ºä¹Ÿå¯ä»¥å‚è€ƒï¼ˆZanzotto et
    al., [2015](#bib.bib78)ï¼‰ã€‚
- en: '| ![Refer to caption](img/e4d6edc6dc130666802976a787b8cbf9.png) |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| ![å‚è§è¯´æ˜](img/e4d6edc6dc130666802976a787b8cbf9.png) |  |'
- en: 'Figure 2: A sentence and its dependency graph'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 2: ä¸€ä¸ªå¥å­åŠå…¶ä¾èµ–å›¾'
- en: 7.1.2 Lexical Functional Compositional Distributional Semantic Models
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2 è¯æ±‡åŠŸèƒ½ç»„åˆåˆ†å¸ƒè¯­ä¹‰æ¨¡å‹
- en: Lexical Functional Models are compositional distributional semantic models where
    words are tensors and each type of word is represented by tensors of different
    order. Composing meaning is then composing these tensors to obtain vectors. These
    models have solid mathematical background linking Lambek pregroup theory, formal
    semantics and distributional semantics (Coecke etÂ al., [2010](#bib.bib15)). Lexical
    Function models are concatenative compositional, yet, in the following, we will
    examine whether these models produce vectors that my be *interpreted*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡åŠŸèƒ½æ¨¡å‹æ˜¯ç»„åˆåˆ†å¸ƒè¯­ä¹‰æ¨¡å‹ï¼Œå…¶ä¸­å•è¯æ˜¯å¼ é‡ï¼Œæ¯ç§å•è¯ç±»å‹ç”±ä¸åŒé˜¶çš„å¼ é‡è¡¨ç¤ºã€‚ç»„åˆæ„ä¹‰å³æ˜¯ç»„åˆè¿™äº›å¼ é‡ä»¥è·å¾—å‘é‡ã€‚è¿™äº›æ¨¡å‹å…·æœ‰åšå®çš„æ•°å­¦åŸºç¡€ï¼Œå°†Lambekå‰ç¾¤ç†è®ºã€å½¢å¼è¯­ä¹‰å­¦å’Œåˆ†å¸ƒå¼è¯­ä¹‰å­¦ï¼ˆCoecke
    et al., [2010](#bib.bib15)ï¼‰è”ç³»èµ·æ¥ã€‚è¯æ±‡åŠŸèƒ½æ¨¡å‹æ˜¯ä¸²æ¥ç»„åˆçš„ï¼Œç„¶è€Œï¼Œåœ¨ä»¥ä¸‹å†…å®¹ä¸­ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥è¿™äº›æ¨¡å‹æ˜¯å¦äº§ç”Ÿå¯ä»¥*è§£é‡Š*çš„å‘é‡ã€‚
- en: To determine whether these models produce *interpretable* vectors, we start
    from a simple Lexical Function model applied to two word sequences. This model
    has been largely analyzed in (Baroni and Zamparelli, [2010](#bib.bib6)) as matrices
    were considered better linear models to encode *adjectives*.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®å®šè¿™äº›æ¨¡å‹æ˜¯å¦äº§ç”Ÿ*å¯è§£é‡Š*çš„å‘é‡ï¼Œæˆ‘ä»¬ä»åº”ç”¨äºä¸¤ä¸ªå•è¯åºåˆ—çš„ç®€å•è¯æ±‡åŠŸèƒ½æ¨¡å‹å¼€å§‹ã€‚è¯¥æ¨¡å‹åœ¨ï¼ˆBaroni and Zamparelli, [2010](#bib.bib6)ï¼‰ä¸­å·²ç»è¿›è¡Œäº†å¹¿æ³›åˆ†æï¼Œå› ä¸ºçŸ©é˜µè¢«è®¤ä¸ºæ˜¯ç¼–ç *å½¢å®¹è¯*çš„æ›´å¥½çº¿æ€§æ¨¡å‹ã€‚
- en: 'In Lexical Functional models over two words sequences, there is one of the
    two words which as a tensor of order 2 (that is, a matrix) and one word that is
    represented by a vector. For example, *adjectives* are matrices and nouns are
    vectors (Baroni and Zamparelli, [2010](#bib.bib6)) in adjective-noun sequences.
    Hence, adjective-noun sequences like *â€œblack catâ€* or *â€œwhite dogâ€* are represented
    as:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸¤ä¸ªå•è¯åºåˆ—çš„è¯æ±‡åŠŸèƒ½æ¨¡å‹ä¸­ï¼Œä¸¤ä¸ªå•è¯ä¸­çš„ä¸€ä¸ªæœ‰ä¸€ä¸ª2é˜¶çš„å¼ é‡ï¼ˆå³çŸ©é˜µï¼‰ï¼Œå¦ä¸€ä¸ªå•è¯åˆ™ç”±ä¸€ä¸ªå‘é‡è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œåœ¨å½¢å®¹è¯-åè¯åºåˆ—ä¸­ï¼Œ*å½¢å®¹è¯*æ˜¯çŸ©é˜µï¼Œè€Œåè¯æ˜¯å‘é‡ï¼ˆBaroni
    and Zamparelli, [2010](#bib.bib6)ï¼‰ã€‚å› æ­¤ï¼Œåƒ*â€œé»‘çŒ«â€*æˆ–*â€œç™½ç‹—â€*è¿™æ ·çš„å½¢å®¹è¯-åè¯åºåˆ—è¢«è¡¨ç¤ºä¸ºï¼š
- en: '|  | $f(\text{black cat})=\mathbf{BLACK}\mathbf{cat}$ |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(\text{black cat})=\mathbf{BLACK}\mathbf{cat}$ |  |'
- en: '|  | $f(\text{white dog})=\mathbf{WHITE}\mathbf{dog}$ |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(\text{white dog})=\mathbf{WHITE}\mathbf{dog}$ |  |'
- en: where $\mathbf{BLACK}$ and $\mathbf{WHITE}$ are matrices representing the two
    adjectives and $\mathbf{cat}$ and $\mathbf{dog}$ are the two vectors representing
    the two nouns.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\mathbf{BLACK}$å’Œ$\mathbf{WHITE}$æ˜¯è¡¨ç¤ºä¸¤ä¸ªå½¢å®¹è¯çš„çŸ©é˜µï¼Œè€Œ$\mathbf{cat}$å’Œ$\mathbf{dog}$æ˜¯è¡¨ç¤ºä¸¤ä¸ªåè¯çš„ä¸¤ä¸ªå‘é‡ã€‚
- en: 'These two words models are *partially interpretable*: knowing the adjective
    it is possible to extract the noun but not vice-versa. In fact, if matrices for
    adjectives are invertible, there is the possibility of extracting which nouns
    has been related to particular adjectives. For example, if $\mathbf{BLACK}$ is
    invertible, the inverse matrix $\mathbf{BLACK}^{-1}$ can be used to extract the
    vector of *cat* from the vector $f(\text{black cat})$:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªè¯æ¨¡å‹æ˜¯*éƒ¨åˆ†å¯è§£é‡Š*çš„ï¼šçŸ¥é“å½¢å®¹è¯å¯ä»¥æå–åè¯ï¼Œä½†åä¹‹ä¸è¡Œã€‚å®é™…ä¸Šï¼Œå¦‚æœå½¢å®¹è¯çš„çŸ©é˜µæ˜¯å¯é€†çš„ï¼Œåˆ™å¯ä»¥æå–ä¸ç‰¹å®šå½¢å®¹è¯ç›¸å…³çš„åè¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ$\mathbf{BLACK}$æ˜¯å¯é€†çš„ï¼Œåˆ™å¯ä»¥ä½¿ç”¨é€†çŸ©é˜µ$\mathbf{BLACK}^{-1}$ä»å‘é‡$f(\text{black
    cat})$ä¸­æå–*cat*çš„å‘é‡ï¼š
- en: '|  | $\mathbf{cat}=\mathbf{BLACK}^{-1}f(\text{black cat})$ |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{cat}=\mathbf{BLACK}^{-1}f(\text{black cat})$ |  |'
- en: 'This contributes to the *interpretability* of this model. Moreover, if matrices
    for adjectives are built using Jonson-Lindestrauss Transforms (Johnson and Lindenstrauss,
    [1984](#bib.bib39)), that is matrices with the property in Eq. [5](#S7.E5 "In
    7.1.1 Additive Models â€£ 7.1 Compositional Distributional Semantics â€£ 7 Composing
    distributed representations â€£ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey"), it is
    possible to pack different pieces of sentences in a single vector and, then, select
    only relevant information, for example:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ‰åŠ©äºè¯¥æ¨¡å‹çš„*å¯è§£é‡Šæ€§*ã€‚æ­¤å¤–ï¼Œå¦‚æœå½¢å®¹è¯çš„çŸ©é˜µæ˜¯ä½¿ç”¨Jonson-Lindestrausså˜æ¢ï¼ˆJohnsonå’ŒLindenstraussï¼Œ[1984](#bib.bib39)ï¼‰æ„å»ºçš„ï¼Œå³å…·æœ‰Eq.
    [5](#S7.E5 "åœ¨7.1.1 åŠ æ³•æ¨¡å‹ â€£ 7.1 ç»„åˆåˆ†å¸ƒè¯­ä¹‰ â€£ 7 ç»„åˆåˆ†å¸ƒå¼è¡¨ç¤º â€£ æ·±åº¦å­¦ä¹ æ—¶ä»£è‡ªç„¶è¯­è¨€å¤„ç†çš„ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒè¡¨ç¤ºï¼šç»¼è¿°")ä¸­å±æ€§çš„çŸ©é˜µï¼Œåˆ™å¯ä»¥å°†å¥å­çš„ä¸åŒç‰‡æ®µæ‰“åŒ…åˆ°ä¸€ä¸ªå‘é‡ä¸­ï¼Œç„¶åä»…é€‰æ‹©ç›¸å…³ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š
- en: '|  | $\mathbf{cat}\approx\mathbf{BLACK}^{T}(f(\text{black cat})+f(\text{white
    dog}))$ |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{cat}\approx\mathbf{BLACK}^{T}(f(\text{black cat})+f(\text{white
    dog}))$ |  |'
- en: On the contrary, knowing noun vectors, it is not possible to extract back adjective
    matrices. This is a strong limitation in term of interpretability.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼ŒçŸ¥é“åè¯å‘é‡ï¼Œä¸å¯èƒ½æå–å›å½¢å®¹è¯çŸ©é˜µã€‚è¿™æ˜¯è§£é‡Šæ€§æ–¹é¢çš„ä¸€ä¸ªé‡å¤§é™åˆ¶ã€‚
- en: Lexical Functional models for larger structures are concatenative compositional
    but not interpretable at all. In fact, in general these models have tensors in
    the middle and these tensors are the only parts that can be inverted. Hence, in
    general these models are not interpretable. However, using the *convolution conjecture*
    (Zanzotto etÂ al., [2015](#bib.bib78)), it is possible to know whether subparts
    are contained in some final vectors obtained with these models.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºè¾ƒå¤§ç»“æ„çš„è¯æ±‡åŠŸèƒ½æ¨¡å‹æ˜¯è¿æ¥ç»„åˆçš„ï¼Œä½†å®Œå…¨ä¸å¯è§£é‡Šã€‚å®é™…ä¸Šï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åœ¨ä¸­é—´æœ‰å¼ é‡ï¼Œè¿™äº›å¼ é‡æ˜¯å”¯ä¸€å¯ä»¥è¢«åè½¬çš„éƒ¨åˆ†ã€‚å› æ­¤ï¼Œé€šå¸¸è¿™äº›æ¨¡å‹æ˜¯ä¸å¯è§£é‡Šçš„ã€‚ç„¶è€Œï¼Œé€šè¿‡ä½¿ç”¨*å·ç§¯çŒœæƒ³*ï¼ˆZanzottoç­‰ï¼Œ[2015](#bib.bib78)ï¼‰ï¼Œå¯ä»¥çŸ¥é“æ˜¯å¦æŸäº›å­éƒ¨åˆ†åŒ…å«åœ¨è¿™äº›æ¨¡å‹è·å¾—çš„æœ€ç»ˆå‘é‡ä¸­ã€‚
- en: 7.2 Holographic Representations
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 å…¨æ¯è¡¨ç¤º
- en: Holographic reduced representations (HRRs) are *models-that-compose* expressly
    designed to be *interpretable* (Plate, [1995](#bib.bib57); Neumann, [2001](#bib.bib53)).
    In fact, these models to encode flat structures representing assertions and these
    assertions should be then searched in oder to recover pieces of knowledge that
    is in. For example, these representations have been used to encode logical propositions
    such as $eat(John,apple)$. In this case, each atomic element has an associated
    vector and the vector for the compound is obtained by combining these vectors.
    The major concern here is to build encoding functions that can be decoded, that
    is, it should be possible to retrieve composing elements from final distributed
    vectors such as the vector of $eat(John,apple)$.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨æ¯ç¼©å‡è¡¨ç¤ºï¼ˆHRRsï¼‰æ˜¯*æ¨¡å‹-ç»„åˆ*ï¼Œä¸“é—¨è®¾è®¡ä¸º*å¯è§£é‡Š*çš„ï¼ˆPlateï¼Œ[1995](#bib.bib57)ï¼›Neumannï¼Œ[2001](#bib.bib53)ï¼‰ã€‚å®é™…ä¸Šï¼Œè¿™äº›æ¨¡å‹ç”¨äºç¼–ç è¡¨ç¤ºæ–­è¨€çš„å¹³é¢ç»“æ„ï¼Œå¹¶ä¸”è¿™äº›æ–­è¨€åº”è¢«æœç´¢ä»¥æ¢å¤å…¶ä¸­çš„çŸ¥è¯†ã€‚ä¾‹å¦‚ï¼Œè¿™äº›è¡¨ç¤ºè¢«ç”¨æ¥ç¼–ç é€»è¾‘å‘½é¢˜ï¼Œå¦‚$eat(John,apple)$ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªåŸå­å…ƒç´ éƒ½æœ‰ä¸€ä¸ªç›¸å…³çš„å‘é‡ï¼Œå¤åˆä½“çš„å‘é‡é€šè¿‡ç»„åˆè¿™äº›å‘é‡è·å¾—ã€‚è¿™é‡Œä¸»è¦çš„å…³æ³¨ç‚¹æ˜¯æ„å»ºå¯ä»¥è§£ç çš„ç¼–ç å‡½æ•°ï¼Œå³åº”è¯¥èƒ½å¤Ÿä»æœ€ç»ˆçš„åˆ†å¸ƒå¼å‘é‡ä¸­æ£€ç´¢ç»„æˆå…ƒç´ ï¼Œä¾‹å¦‚$eat(John,apple)$çš„å‘é‡ã€‚
- en: 'In HRRs, *nearly orthogonal unit vectors* (Johnson and Lindenstrauss, [1984](#bib.bib39))
    for basic symbols, *circular convolution* $\otimes$ and *circular correlation*
    $\oplus$ guarantees *composability* and *intepretability*. HRRs are the extension
    of Random Indexing (see Sec. [5.1](#S5.SS1 "5.1 Dimensionality reductio with Random
    Projections â€£ 5 Strategies to obtain distributed representations from symbols
    â€£ Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey")) to structures. Hence, symbols
    are represented with vectors sampled from a multivariate normal distribution $N(0,\frac{1}{d}I_{d})$.
    The composition function is the circular convolution indicated as $\otimes$ and
    defined as:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ HRRs ä¸­ï¼Œ*å‡ ä¹æ­£äº¤çš„å•ä½å‘é‡*ï¼ˆJohnson å’Œ Lindenstraussï¼Œ[1984](#bib.bib39)ï¼‰ç”¨äºåŸºæœ¬ç¬¦å·ï¼Œ*å¾ªç¯å·ç§¯*
    $\otimes$ å’Œ *å¾ªç¯ç›¸å…³* $\oplus$ ç¡®ä¿äº† *å¯ç»„åˆæ€§* å’Œ *å¯è§£é‡Šæ€§*ã€‚HRRs æ˜¯å¯¹éšæœºç´¢å¼•çš„æ‰©å±•ï¼ˆè§ Sec. [5.1](#S5.SS1
    "5.1 Dimensionality reductio with Random Projections â€£ 5 Strategies to obtain
    distributed representations from symbols â€£ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey")ï¼‰ã€‚å› æ­¤ï¼Œç¬¦å·é€šè¿‡ä»å¤šå…ƒæ­£æ€åˆ†å¸ƒ $N(0,\frac{1}{d}I_{d})$ ä¸­é‡‡æ ·çš„å‘é‡è¿›è¡Œè¡¨ç¤ºã€‚ç»„åˆå‡½æ•°æ˜¯å¾ªç¯å·ç§¯ï¼Œè¡¨ç¤ºä¸º $\otimes$
    å¹¶å®šä¹‰ä¸ºï¼š'
- en: '|  | $z_{j}=(\mathbf{a}\otimes\mathbf{b})_{j}=\sum_{k=0}^{d-1}a_{k}b_{j-k}$
    |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{j}=(\mathbf{a}\otimes\mathbf{b})_{j}=\sum_{k=0}^{d-1}a_{k}b_{j-k}$
    |  |'
- en: 'where subscripts are modulo $d$. Circular convolution is commutative and bilinear.
    This operation can be also computed using *circulant matrices*:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸‹æ ‡å–æ¨¡ $d$ã€‚å¾ªç¯å·ç§¯æ˜¯å¯äº¤æ¢ä¸”åŒçº¿æ€§çš„ã€‚è¿™ä¸€æ“ä½œä¹Ÿå¯ä»¥é€šè¿‡*circular matrices*è®¡ç®—ï¼š
- en: '|  | $\mathbf{z}=(\mathbf{a}\otimes\mathbf{b})=\mathbf{A}_{\circ}\mathbf{b}=\mathbf{B}_{\circ}\mathbf{a}$
    |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{z}=(\mathbf{a}\otimes\mathbf{b})=\mathbf{A}_{\circ}\mathbf{b}=\mathbf{B}_{\circ}\mathbf{a}$
    |  |'
- en: 'where $\mathbf{A}_{\circ}$ and $\mathbf{B}_{\circ}$ are circulant matrices
    of the vectors $\mathbf{a}$ and $\mathbf{b}$. Given the properties of vectors
    $\mathbf{a}$ and $\mathbf{b}$, matrices $\mathbf{A}_{\circ}$ and $\mathbf{B}_{\circ}$
    have the property in Eq. [5](#S7.E5 "In 7.1.1 Additive Models â€£ 7.1 Compositional
    Distributional Semantics â€£ 7 Composing distributed representations â€£ Symbolic,
    Distributed and Distributional Representations for Natural Language Processing
    in the Era of Deep Learning: a Survey"). Hence, *circular convolution* is approximately
    invertible with the *circular correlation* function ($\oplus$) defined as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 'å…¶ä¸­ $\mathbf{A}_{\circ}$ å’Œ $\mathbf{B}_{\circ}$ æ˜¯å‘é‡ $\mathbf{a}$ å’Œ $\mathbf{b}$
    çš„å¾ªç¯çŸ©é˜µã€‚è€ƒè™‘åˆ°å‘é‡ $\mathbf{a}$ å’Œ $\mathbf{b}$ çš„æ€§è´¨ï¼ŒçŸ©é˜µ $\mathbf{A}_{\circ}$ å’Œ $\mathbf{B}_{\circ}$
    å…·æœ‰ç­‰å¼ [5](#S7.E5 "In 7.1.1 Additive Models â€£ 7.1 Compositional Distributional Semantics
    â€£ 7 Composing distributed representations â€£ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey") ä¸­çš„æ€§è´¨ã€‚å› æ­¤ï¼Œ*å¾ªç¯å·ç§¯* ä¸ *å¾ªç¯ç›¸å…³* å‡½æ•° ($\oplus$) çš„é€†æ€§è¿‘ä¼¼å¦‚ä¸‹ï¼š'
- en: '|  | $c_{j}=(\mathbf{z}\oplus\mathbf{b})_{j}=\sum_{k=0}^{d-1}z_{k}b_{j+k}$
    |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{j}=(\mathbf{z}\oplus\mathbf{b})_{j}=\sum_{k=0}^{d-1}z_{k}b_{j+k}$
    |  |'
- en: 'where again subscripts are modulo $d$. Circular correlation is related to inverse
    matrices of circulant matrices, that is $\mathbf{B}_{\circ}^{T}$. In the decoding
    with $\oplus$, parts of the structures can be derived in an approximated way,
    that is:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸‹æ ‡å†æ¬¡å–æ¨¡ $d$ã€‚å¾ªç¯ç›¸å…³ä¸å¾ªç¯çŸ©é˜µçš„é€†çŸ©é˜µç›¸å…³ï¼Œå³ $\mathbf{B}_{\circ}^{T}$ã€‚åœ¨ç”¨ $\oplus$ è¿›è¡Œè§£ç æ—¶ï¼Œéƒ¨åˆ†ç»“æ„å¯ä»¥ä»¥è¿‘ä¼¼çš„æ–¹å¼æ¨å¯¼ï¼Œå³ï¼š
- en: '|  | $(\mathbf{a}\otimes\mathbf{b})\oplus\mathbf{b}\approx\mathbf{a}$ |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\mathbf{a}\otimes\mathbf{b})\oplus\mathbf{b}\approx\mathbf{a}$ |  |'
- en: 'Hence, circular convolution $\otimes$ and circular correlation $\oplus$ allow
    to build interpretable representations. For example, having the vectors $\mathbf{e}$,
    $\mathbf{J}$, and $\mathbf{a}$ for $eat$, $John$ and $apple$, respectively, the
    following encoding and decoding produces a vector that approximates the original
    vector for $John$:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¾ªç¯å·ç§¯ $\otimes$ å’Œå¾ªç¯ç›¸å…³ $\oplus$ å…è®¸å»ºç«‹å¯è§£é‡Šçš„è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œå…·æœ‰å‘é‡ $\mathbf{e}$ã€$\mathbf{J}$
    å’Œ $\mathbf{a}$ å¯¹åº”äº $eat$ã€$John$ å’Œ $apple$ï¼Œä»¥ä¸‹ç¼–ç å’Œè§£ç äº§ç”Ÿäº†ä¸€ä¸ªè¿‘ä¼¼äº $John$ çš„åŸå§‹å‘é‡çš„å‘é‡ï¼š
- en: '|  | $\mathbf{J}\approx(\mathbf{J}\otimes\mathbf{e}\otimes\mathbf{a})\oplus(\mathbf{e}\otimes\mathbf{a})$
    |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{J}\approx(\mathbf{J}\otimes\mathbf{e}\otimes\mathbf{a})\oplus(\mathbf{e}\otimes\mathbf{a})$
    |  |'
- en: The â€œinvertibilityâ€ of these representations is important because it allow us
    not to consider these representations as black boxes.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è¡¨ç¤ºçš„â€œå¯é€†æ€§â€å¾ˆé‡è¦ï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘ä»¬ä¸å°†è¿™äº›è¡¨ç¤ºè§†ä¸ºé»‘ç®±ã€‚
- en: However, holographic representations have severe limitations as these can encode
    and decode simple, flat structures. In fact, these representations are based on
    the circular convolution, which is a commutative function; this implies that the
    representation cannot keep track of composition of objects where the order matters
    and this phenomenon is particularly important when encoding nested structures.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå…¨æ¯è¡¨ç¤ºæœ‰ä¸¥é‡çš„å±€é™æ€§ï¼Œå› ä¸ºè¿™äº›è¡¨ç¤ºåªèƒ½ç¼–ç å’Œè§£ç ç®€å•ã€å¹³å¦çš„ç»“æ„ã€‚å®é™…ä¸Šï¼Œè¿™äº›è¡¨ç¤ºåŸºäºå¾ªç¯å·ç§¯ï¼Œå®ƒæ˜¯ä¸€ä¸ªäº¤æ¢å‡½æ•°ï¼›è¿™æ„å‘³ç€è¡¨ç¤ºæ— æ³•è·Ÿè¸ªå¯¹è±¡çš„ç»„æˆé¡ºåºï¼Œè€Œè¿™ä¸€ç°è±¡åœ¨ç¼–ç åµŒå¥—ç»“æ„æ—¶å°¤å…¶é‡è¦ã€‚
- en: Distributed trees (Zanzotto and Dellâ€™Arciprete, [2012](#bib.bib77)) have shown
    that the principles expressed in holographic representation can be applied to
    encode larger structures, overcoming the problem of reliably encoding the order
    in which elements are composed using the *shuffled circular convolution* function
    as the composition operator. Distributed trees are encoding functions that transform
    trees into low-dimensional vectors that also contain the encoding of every substructures
    of the tree. Thus, these distributed trees are particularly attractive as they
    can be used to represent structures in linear learning machines which are computationally
    efficient.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼æ ‘ï¼ˆZanzotto and Dellâ€™Arciprete, [2012](#bib.bib77)ï¼‰å·²ç»å±•ç¤ºäº†å…¨æ¯è¡¨ç¤ºä¸­è¡¨è¾¾çš„åŸåˆ™å¯ä»¥ç”¨äºç¼–ç æ›´å¤§ç»“æ„ï¼Œå…‹æœäº†ä½¿ç”¨*æ´—ç‰Œå¾ªç¯å·ç§¯*å‡½æ•°ä½œä¸ºç»„æˆç®—å­æ—¶å¯é ç¼–ç å…ƒç´ ç»„æˆé¡ºåºçš„é—®é¢˜ã€‚åˆ†å¸ƒå¼æ ‘æ˜¯å°†æ ‘è½¬æ¢ä¸ºä½ç»´å‘é‡çš„ç¼–ç å‡½æ•°ï¼Œè¿™äº›å‘é‡è¿˜åŒ…å«æ ‘ä¸­æ¯ä¸ªå­ç»“æ„çš„ç¼–ç ã€‚å› æ­¤ï¼Œè¿™äº›åˆ†å¸ƒå¼æ ‘ç‰¹åˆ«æœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ç”¨æ¥è¡¨ç¤ºçº¿æ€§å­¦ä¹ æœºå™¨ä¸­çš„ç»“æ„ï¼Œè¿™åœ¨è®¡ç®—ä¸Šæ˜¯é«˜æ•ˆçš„ã€‚
- en: Distributed trees and, in particular, distributed smoothed trees (Ferrone and
    Zanzotto, [2014](#bib.bib20)) represent an interesting middle way between compositional
    distributional semantic models and holographic representation.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼æ ‘ï¼Œç‰¹åˆ«æ˜¯åˆ†å¸ƒå¼å¹³æ»‘æ ‘ï¼ˆFerrone and Zanzotto, [2014](#bib.bib20)ï¼‰ï¼Œä»£è¡¨äº†ç»„æˆåˆ†å¸ƒå¼è¯­ä¹‰æ¨¡å‹å’Œå…¨æ¯è¡¨ç¤ºä¹‹é—´çš„æœ‰è¶£æŠ˜è¡·ã€‚
- en: 7.3 Compositional Models in Neural Networks
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 ç¥ç»ç½‘ç»œä¸­çš„ç»„æˆæ¨¡å‹
- en: 'When neural networks are applied to sequences or structured data, these networks
    are in fact *models-that-compose*. However, these models result in *models-that-compose*
    which are not interpretable. In fact, composition functions are trained on specific
    tasks and not on the possibility of reconstructing the structured input, unless
    in some rare cases (Socher etÂ al., [2011](#bib.bib64)). The input of these networks
    are sequences or structured data where basic symbols are embedded in *local* representations
    or *distributed* representations obtained with word embedding (see Sec. [6.3](#S6.SS3
    "6.3 Learning representations: word2vec â€£ 6 Distributional Representations as
    another side of the coin â€£ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey")). The
    output are distributed vectors derived for specific tasks. Hence, these *models-that-compose*
    are not interpretable in our sense for their final aim and for the fact that *non
    linear* functions are adopted in the specification of the neural networks.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç¥ç»ç½‘ç»œåº”ç”¨äºåºåˆ—æˆ–ç»“æ„åŒ–æ•°æ®æ—¶ï¼Œè¿™äº›ç½‘ç»œå®é™…ä¸Šæ˜¯*ç»„æˆæ¨¡å‹*ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹äº§ç”Ÿçš„*ç»„æˆæ¨¡å‹*æ˜¯ä¸å¯è§£é‡Šçš„ã€‚äº‹å®ä¸Šï¼Œç»„æˆå‡½æ•°æ˜¯åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè®­ç»ƒçš„ï¼Œè€Œä¸æ˜¯ä¸ºäº†é‡å»ºç»“æ„åŒ–è¾“å…¥ï¼Œé™¤éåœ¨ä¸€äº›å°‘è§çš„æƒ…å†µä¸‹ï¼ˆSocher
    et al., [2011](#bib.bib64)ï¼‰ã€‚è¿™äº›ç½‘ç»œçš„è¾“å…¥æ˜¯åºåˆ—æˆ–ç»“æ„åŒ–æ•°æ®ï¼Œå…¶ä¸­åŸºæœ¬ç¬¦å·åµŒå…¥åœ¨*å±€éƒ¨*è¡¨ç¤ºæˆ–é€šè¿‡è¯åµŒå…¥è·å¾—çš„*åˆ†å¸ƒå¼*è¡¨ç¤ºä¸­ï¼ˆå‚è§ç¬¬
    [6.3](#S6.SS3 "6.3 å­¦ä¹ è¡¨ç¤ºï¼šword2vec â€£ 6 åˆ†å¸ƒå¼è¡¨ç¤ºä½œä¸ºå¦ä¸€é¢ â€£ ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒå¼è¡¨ç¤ºåœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è°ƒæŸ¥")èŠ‚ï¼‰ã€‚è¾“å‡ºæ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„åˆ†å¸ƒå¼å‘é‡ã€‚å› æ­¤ï¼Œè¿™äº›*ç»„æˆæ¨¡å‹*åœ¨æˆ‘ä»¬çœ‹æ¥ä¸å¯è§£é‡Šï¼Œæ—¢å› ä¸ºå®ƒä»¬çš„æœ€ç»ˆç›®æ ‡ï¼Œä¹Ÿå› ä¸ºåœ¨ç¥ç»ç½‘ç»œçš„è§„èŒƒä¸­é‡‡ç”¨äº†*éçº¿æ€§*å‡½æ•°ã€‚
- en: 'In this section, we revise some prominent neural network architectures that
    can be interpreted as *models-that-compose*: the *recurrent neural networks* (Krizhevsky
    etÂ al., [2012](#bib.bib42); He etÂ al., [2016](#bib.bib34); Vinyals etÂ al., [2015a](#bib.bib72);
    Graves, [2013](#bib.bib30)) and the *recursive neural networks* (Socher etÂ al.,
    [2012](#bib.bib65)).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†ä¸€äº›å¯ä»¥è¢«è§£é‡Šä¸º*ç»„æˆæ¨¡å‹*çš„æ˜¾è‘—ç¥ç»ç½‘ç»œæ¶æ„ï¼š*é€’å½’ç¥ç»ç½‘ç»œ*ï¼ˆKrizhevsky et al., [2012](#bib.bib42)ï¼›He
    et al., [2016](#bib.bib34)ï¼›Vinyals et al., [2015a](#bib.bib72)ï¼›Graves, [2013](#bib.bib30)ï¼‰å’Œ*é€’å½’ç¥ç»ç½‘ç»œ*ï¼ˆSocher
    et al., [2012](#bib.bib65)ï¼‰ã€‚
- en: 7.3.1 Recurrent Neural Networks
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1 é€’å½’ç¥ç»ç½‘ç»œ
- en: Recurrent neural networks form a very broad family of neural networks architectures
    that deal with the representation (and processing) of complex objects. At its
    core a recurrent neural network (RNN) is a network which takes in input the current
    element in the sequence and processes it based on an internal state which depends
    on previous inputs. At the moment the most powerful network architectures are
    convolutional neural networks (Krizhevsky etÂ al., [2012](#bib.bib42); He etÂ al.,
    [2016](#bib.bib34)) for vision related tasks and LSTM-type network for language
    related task (Vinyals etÂ al., [2015a](#bib.bib72); Graves, [2013](#bib.bib30)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œæ„æˆäº†ä¸€ä¸ªéå¸¸å¹¿æ³›çš„ç¥ç»ç½‘ç»œæ¶æ„å®¶æ—ï¼Œå¤„ç†å¤æ‚å¯¹è±¡çš„è¡¨ç¤ºï¼ˆå’Œå¤„ç†ï¼‰ã€‚åœ¨å…¶æ ¸å¿ƒï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ˜¯ä¸€ä¸ªç½‘ç»œï¼Œå®ƒæ¥æ”¶åºåˆ—ä¸­çš„å½“å‰å…ƒç´ ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ ¹æ®ä¾èµ–äºå…ˆå‰è¾“å…¥çš„å†…éƒ¨çŠ¶æ€è¿›è¡Œå¤„ç†ã€‚ç›®å‰ï¼Œæœ€å¼ºå¤§çš„ç½‘ç»œæ¶æ„æ˜¯ç”¨äºè§†è§‰ç›¸å…³ä»»åŠ¡çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆKrizhevsky
    et al., [2012](#bib.bib42); He et al., [2016](#bib.bib34)ï¼‰å’Œç”¨äºè¯­è¨€ç›¸å…³ä»»åŠ¡çš„LSTMç±»å‹ç½‘ç»œï¼ˆVinyals
    et al., [2015a](#bib.bib72); Graves, [2013](#bib.bib30)ï¼‰ã€‚
- en: A recurrent neural network takes as input a sequence $\mathbf{x}=\left(\mathbf{x_{1}}\
    \ldots\ \mathbf{x_{n}}\right)$ and produce as output a single vector $\mathbf{y}\in\mathbb{R}^{n}$
    which is a representation of the entire sequence. At each step Â¹Â¹1we can usually
    think of this as a timestep, but not all applications of recurrent neural network
    have a temporal interpretation $t$ the network takes as input the current element
    $\mathbf{x_{t}}$, the previous output $\mathbf{h_{t-1}}$ and performs the following
    operation to produce the current output $\mathbf{h_{t}}$
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ªç¯ç¥ç»ç½‘ç»œå°†åºåˆ—$\mathbf{x}=\left(\mathbf{x_{1}}\ \ldots\ \mathbf{x_{n}}\right)$ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªå•ä¸€çš„å‘é‡$\mathbf{y}\in\mathbb{R}^{n}$ä½œä¸ºè¾“å‡ºï¼Œè¡¨ç¤ºæ•´ä¸ªåºåˆ—ã€‚åœ¨æ¯ä¸€æ­¥Â¹Â¹1ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥å°†å…¶è§†ä¸ºä¸€ä¸ªæ—¶é—´æ­¥ï¼Œä½†å¹¶éæ‰€æœ‰å¾ªç¯ç¥ç»ç½‘ç»œçš„åº”ç”¨éƒ½æœ‰æ—¶é—´è§£é‡Š$t$ã€‚ç½‘ç»œä»¥å½“å‰å…ƒç´ $\mathbf{x_{t}}$ã€å…ˆå‰è¾“å‡º$\mathbf{h_{t-1}}$ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹æ“ä½œä»¥ç”Ÿæˆå½“å‰è¾“å‡º$\mathbf{h_{t}}$ã€‚
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b)$ |  | (6) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b)$ |  | (6) |'
- en: where $\sigma$ is a non-linear function such as the logistic function or the
    hyperbolic tangent and $\left[\mathbf{h_{t-1}}\ \mathbf{x_{t}}\right]$ denotes
    the concatenation of the vectors $\mathbf{h_{t-1}}$ and $\mathbf{x_{t}}$. The
    parameters of the model are the matrix $W$ and the bias vector $b$.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$\sigma$æ˜¯éçº¿æ€§å‡½æ•°ï¼Œå¦‚é€»è¾‘å‡½æ•°æˆ–åŒæ›²æ­£åˆ‡å‡½æ•°ï¼Œ$\left[\mathbf{h_{t-1}}\ \mathbf{x_{t}}\right]$è¡¨ç¤ºå‘é‡$\mathbf{h_{t-1}}$å’Œ$\mathbf{x_{t}}$çš„æ‹¼æ¥ã€‚æ¨¡å‹çš„å‚æ•°æ˜¯çŸ©é˜µ$W$å’Œåç½®å‘é‡$b$ã€‚
- en: 'Hence, a recurrent neural network is effectively a learned composition function,
    which dynamically depends on its current input, all of its previous inputs and
    also on the dataset on which is trained. However, this learned composition function
    is basically impossible to analyze or interpret in any way. Sometime an â€œintuitiveâ€
    explanation is given about what the learned weights represent: with some weights
    representing information that must be remembered or forgotten.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œå®é™…ä¸Šæ˜¯ä¸€ä¸ªå­¦ä¹ åˆ°çš„ç»„åˆå‡½æ•°ï¼Œå®ƒåŠ¨æ€ä¾èµ–äºå½“å‰è¾“å…¥ã€æ‰€æœ‰å…ˆå‰è¾“å…¥ä»¥åŠè®­ç»ƒæ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™ä¸ªå­¦ä¹ åˆ°çš„ç»„åˆå‡½æ•°åŸºæœ¬ä¸Šæ— æ³•ä»¥ä»»ä½•æ–¹å¼è¿›è¡Œåˆ†ææˆ–è§£é‡Šã€‚æœ‰æ—¶ä¼šæä¾›ä¸€ç§â€œç›´è§‚â€çš„è§£é‡Šï¼Œå…³äºå­¦ä¹ åˆ°çš„æƒé‡è¡¨ç¤ºçš„å†…å®¹ï¼šä¸€äº›æƒé‡è¡¨ç¤ºå¿…é¡»è®°ä½æˆ–é—å¿˜çš„ä¿¡æ¯ã€‚
- en: Even more complex recurrent neural networks as long-short term memory (LSTM)
    (Hochreiter and Schmidhuber, [1997](#bib.bib36)) have the same problem of interpretability.
    LSTM are a recent and successful way for neural network to deal with longer sequences
    of inputs, overcoming some difficulty that RNN face in the training phase. As
    with RNN, LSTM network takes as input a sequence $\mathbf{x}=\left(\mathbf{x_{1}}\
    \ldots\ \mathbf{x_{n}}\right)$ and produce as output a single vector $\mathbf{y}\in\mathbb{R}^{n}$
    which is a representation of the entire sequence. At each step $t$ the network
    takes as input the current element $\mathbf{x_{t}}$, the previous output $\mathbf{h_{t-1}}$
    and performs the following operation to produce the current output $\mathbf{h_{t}}$
    and update the internal state $\mathbf{c_{t}}$.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä¾¿æ˜¯æ›´å¤æ‚çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå¦‚é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ï¼ˆHochreiter å’Œ Schmidhuber, [1997](#bib.bib36)ï¼‰ä¹Ÿå­˜åœ¨ç›¸åŒçš„å¯è§£é‡Šæ€§é—®é¢˜ã€‚LSTM
    æ˜¯ä¸€ç§æœ€è¿‘ä¸”æˆåŠŸçš„æ–¹æ³•ï¼Œä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è¾“å…¥åºåˆ—ï¼Œå…‹æœäº†RNNåœ¨è®­ç»ƒé˜¶æ®µé¢ä¸´çš„ä¸€äº›å›°éš¾ã€‚ä¸RNNä¸€æ ·ï¼ŒLSTMç½‘ç»œå°†åºåˆ—$\mathbf{x}=\left(\mathbf{x_{1}}\
    \ldots\ \mathbf{x_{n}}\right)$ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªå•ä¸€çš„å‘é‡$\mathbf{y}\in\mathbb{R}^{n}$ä½œä¸ºè¾“å‡ºï¼Œè¡¨ç¤ºæ•´ä¸ªåºåˆ—ã€‚åœ¨æ¯ä¸€æ­¥$t$ï¼Œç½‘ç»œä»¥å½“å‰å…ƒç´ $\mathbf{x_{t}}$ã€å…ˆå‰è¾“å‡º$\mathbf{h_{t-1}}$ä½œä¸ºè¾“å…¥ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹æ“ä½œä»¥ç”Ÿæˆå½“å‰è¾“å‡º$\mathbf{h_{t}}$å¹¶æ›´æ–°å†…éƒ¨çŠ¶æ€$\mathbf{c_{t}}$ã€‚
- en: '|  | $\displaystyle f_{t}$ | $\displaystyle=\sigma(W_{f}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{f})$ |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{t}$ | $\displaystyle=\sigma(W_{f}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{f})$ |  |'
- en: '|  | $\displaystyle i_{t}$ | $\displaystyle=\sigma(W_{i}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{i})$ |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle i_{t}$ | $\displaystyle=\sigma(W_{i}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{i})$ |  |'
- en: '|  | $\displaystyle o_{t}$ | $\displaystyle=\sigma(W_{o}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{o})$ |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o_{t}$ | $\displaystyle=\sigma(W_{o}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{o})$ |  |'
- en: '|  | $\displaystyle\mathbf{\tilde{c_{t}}}$ | $\displaystyle=\tanh(W_{c}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{c})$ |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{\tilde{c_{t}}}$ | $\displaystyle=\tanh(W_{c}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{c})$ |  |'
- en: '|  | $\displaystyle\mathbf{c_{t}}$ | $\displaystyle=f_{t}\odot\mathbf{c_{t-i}}+i_{t}\odot\mathbf{\tilde{c_{t}}}$
    |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{c_{t}}$ | $\displaystyle=f_{t}\odot\mathbf{c_{t-i}}+i_{t}\odot\mathbf{\tilde{c_{t}}}$
    |  |'
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\tanh(\mathbf{c_{t}})$
    |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\tanh(\mathbf{c_{t}})$
    |  |'
- en: where $\odot$ stands for element-wise multiplication, and the parameters of
    the model are the matrices $W_{f},W_{i},W_{o},W_{c}$ and the bias vectors $b_{f},b_{i},b_{o},b_{c}$.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼Œæ¨¡å‹çš„å‚æ•°æ˜¯çŸ©é˜µ $W_{f},W_{i},W_{o},W_{c}$ å’Œåç½®å‘é‡ $b_{f},b_{i},b_{o},b_{c}$ã€‚
- en: 'Generally, the interpretation offered for recursive neural networks is *functional*
    or *â€œpsychologicalâ€* and not on the content of intermediate vectors. For example,
    an interpretation of the parameters of LSTM is the following:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œå¯¹äºé€’å½’ç¥ç»ç½‘ç»œçš„è§£é‡Šæ˜¯ *åŠŸèƒ½æ€§* æˆ– *â€œå¿ƒç†å­¦â€* çš„ï¼Œè€Œä¸æ˜¯é’ˆå¯¹ä¸­é—´å‘é‡çš„å†…å®¹ã€‚ä¾‹å¦‚ï¼Œå¯¹ LSTM å‚æ•°çš„è§£é‡Šå¦‚ä¸‹ï¼š
- en: â€¢
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$f_{t}$ is the *forget gate*: at each step takes in consideration the new input
    and output computed so far to decide which information in the internal state must
    be *forgotten* (that is, set to $0$);'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f_{t}$ æ˜¯ *é—å¿˜é—¨*ï¼šåœ¨æ¯ä¸€æ­¥ä¸­ï¼Œå®ƒä¼šè€ƒè™‘æ–°çš„è¾“å…¥å’Œå½“å‰è®¡ç®—å‡ºçš„è¾“å‡ºï¼Œä»¥å†³å®šå†…éƒ¨çŠ¶æ€ä¸­å“ªäº›ä¿¡æ¯å¿…é¡»è¢« *é—å¿˜*ï¼ˆå³ï¼Œè®¾ç½®ä¸º $0$ï¼‰ï¼›
- en: â€¢
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$i_{t}$ is the *input gate*: it decides which position in the internal state
    will be updated, and by how much;'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $i_{t}$ æ˜¯ *è¾“å…¥é—¨*ï¼šå®ƒå†³å®šäº†å†…éƒ¨çŠ¶æ€ä¸­çš„å“ªä¸ªä½ç½®å°†è¢«æ›´æ–°ï¼Œä»¥åŠæ›´æ–°çš„é‡ï¼›
- en: â€¢
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: $\tilde{c_{t}}$ is the proposed new internal state, which will then be updated
    effectively combining the previous gate;
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\tilde{c_{t}}$ æ˜¯æè®®çš„æ–°å†…éƒ¨çŠ¶æ€ï¼Œç„¶åå°†é€šè¿‡ç»„åˆå‰ä¸€ä¸ªé—¨æ¥æœ‰æ•ˆæ›´æ–°ï¼›
- en: â€¢
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€¢
- en: '$o_{t}$ is the *output gate*: it decides how to modulate the internal state
    to produce the output'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $o_{t}$ æ˜¯ *è¾“å‡ºé—¨*ï¼šå®ƒå†³å®šå¦‚ä½•è°ƒèŠ‚å†…éƒ¨çŠ¶æ€ä»¥äº§ç”Ÿè¾“å‡º
- en: These *models-that-compose* have high performance on final tasks but are definitely
    not interpretable.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº› *ç»„åˆæ¨¡å‹* åœ¨æœ€ç»ˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç¡®å®ä¸æ˜“è§£é‡Šã€‚
- en: 7.3.2 Recursive Neural Network
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2 é€’å½’ç¥ç»ç½‘ç»œ
- en: \Tree
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: \Tree
- en: '[.S [.cows ] [.VP [.eat ] [.NP [.animal ] [.extracts ] ] ] ]'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[.S [.cows ] [.VP [.eat ] [.NP [.animal ] [.extracts ] ] ] ]'
- en: 'Figure 3: A simple binary tree'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šä¸€ä¸ªç®€å•çš„äºŒå‰æ ‘
- en: '![Refer to caption](img/59cbb7affc4acb0b6326db80824d1c4a.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/59cbb7affc4acb0b6326db80824d1c4a.png)'
- en: 'Figure 4: Recursive Neural Networks'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šé€’å½’ç¥ç»ç½‘ç»œ
- en: The last class of *models-that-compose* that we present is the class of *recursive
    neural networks* (Socher etÂ al., [2012](#bib.bib65)). These networks are applied
    to data structures as trees and are in fact applied recursively on the structure.
    Generally, the aim of the network is a final task as *sentiment analysis* or *paraphrase
    detection*.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å±•ç¤ºçš„æœ€åä¸€ç±» *ç»„åˆæ¨¡å‹* æ˜¯ *é€’å½’ç¥ç»ç½‘ç»œ*ï¼ˆSocher ç­‰äººï¼Œ[2012](#bib.bib65)ï¼‰ã€‚è¿™äº›ç½‘ç»œåº”ç”¨äºæ ‘çŠ¶æ•°æ®ç»“æ„ï¼Œå¹¶ä¸”å®é™…ä¸Šæ˜¯åœ¨ç»“æ„ä¸Šé€’å½’åº”ç”¨ã€‚é€šå¸¸ï¼Œç½‘ç»œçš„ç›®æ ‡æ˜¯ä¸€ä¸ªæœ€ç»ˆä»»åŠ¡ï¼Œå¦‚
    *æƒ…æ„Ÿåˆ†æ* æˆ– *åŒä¹‰å¥æ£€æµ‹*ã€‚
- en: 'Recursive neural networks is then a basic block (see Fig. [4](#S7.F4 "Figure
    4 â€£ 7.3.2 Recursive Neural Network â€£ 7.3 Compositional Models in Neural Networks
    â€£ 7 Composing distributed representations â€£ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey")) which is recursively applied on trees like the one in Fig. [3](#S7.F3
    "Figure 3 â€£ 7.3.2 Recursive Neural Network â€£ 7.3 Compositional Models in Neural
    Networks â€£ 7 Composing distributed representations â€£ Symbolic, Distributed and
    Distributional Representations for Natural Language Processing in the Era of Deep
    Learning: a Survey"). The formal definition is the following:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªåŸºæœ¬æ¨¡å—ï¼ˆè§å›¾ [4](#S7.F4 "å›¾ 4 â€£ 7.3.2 é€’å½’ç¥ç»ç½‘ç»œ â€£ 7.3 ç¥ç»ç½‘ç»œä¸­çš„ç»„åˆæ¨¡å‹ â€£ 7 ç»„åˆåˆ†å¸ƒå¼è¡¨ç¤º
    â€£ ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒæ€§è¡¨ç¤ºåœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨ï¼šç»¼è¿°")ï¼‰ï¼Œå®ƒåœ¨æ ‘çŠ¶ç»“æ„ä¸Šé€’å½’åº”ç”¨ï¼Œå¦‚å›¾ [3](#S7.F3 "å›¾ 3 â€£ 7.3.2
    é€’å½’ç¥ç»ç½‘ç»œ â€£ 7.3 ç¥ç»ç½‘ç»œä¸­çš„ç»„åˆæ¨¡å‹ â€£ 7 ç»„åˆåˆ†å¸ƒå¼è¡¨ç¤º â€£ ç¬¦å·ã€åˆ†å¸ƒå¼å’Œåˆ†å¸ƒæ€§è¡¨ç¤ºåœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£çš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨ï¼šç»¼è¿°") æ‰€ç¤ºã€‚å…¶æ­£å¼å®šä¹‰å¦‚ä¸‹ï¼š
- en: '|  | $\mathbf{p}=f_{U,V}(\mathbf{u},\mathbf{v})=f(V\mathbf{u},U\mathbf{v})=g(W\begin{pmatrix}V\mathbf{u}\\
    U\mathbf{v}\end{pmatrix})$ |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{p}=f_{U,V}(\mathbf{u},\mathbf{v})=f(V\mathbf{u},U\mathbf{v})=g(W\begin{pmatrix}V\mathbf{u}\\
    U\mathbf{v}\end{pmatrix})$ |  |'
- en: where $g$ is a component-wise sigmoid function or $\mathrm{tanh}$, and $W$ is
    a matrix that maps the concatenation vector<math alttext="\begin{pmatrix}V\mathbf{u}\\
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $g$ æ˜¯é€å…ƒç´ çš„ sigmoid å‡½æ•°æˆ– $\mathrm{tanh}$ï¼Œ$W$ æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå®ƒå°†æ‹¼æ¥å‘é‡æ˜ å°„åˆ°<math alttext="\begin{pmatrix}V\mathbf{u}\\
- en: U\mathbf{v}\end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable
    rowspacing="0pt" ><mtr ><mtd ><mrow ><mi >V</mi><mo lspace="0em" rspace="0em"
    >â€‹</mo><mi >ğ®</mi></mrow></mtd></mtr><mtr ><mtd ><mrow ><mi >U</mi><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mi >ğ¯</mi></mrow></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><apply ><ci >ğ‘‰</ci><ci >ğ®</ci></apply></matrixrow><matrixrow ><apply
    ><ci >ğ‘ˆ</ci><ci >ğ¯</ci></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}V\mathbf{u}\\ U\mathbf{v}\end{pmatrix}</annotation></semantics></math>
    to have the same dimension.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿å…¶å…·æœ‰ç›¸åŒçš„ç»´åº¦ã€‚
- en: 'This method deals naturally with recursion: given a binary parse tree of a
    sentence $s$, the algorithm creates vectors and matrices representation for each
    node, starting from the terminal nodes. Words are represented by distributed representations
    or local representations. For example, the tree in Fig. [3](#S7.F3 "Figure 3 â€£
    7.3.2 Recursive Neural Network â€£ 7.3 Compositional Models in Neural Networks â€£
    7 Composing distributed representations â€£ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey") is processed by the recursive network in the following way. First, the
    network in Fig. [4](#S7.F4 "Figure 4 â€£ 7.3.2 Recursive Neural Network â€£ 7.3 Compositional
    Models in Neural Networks â€£ 7 Composing distributed representations â€£ Symbolic,
    Distributed and Distributional Representations for Natural Language Processing
    in the Era of Deep Learning: a Survey") is applied to the pair *(animal,extracts)*
    and $f_{UV}(\mathbf{animal},\mathbf{extract})$ is obtained. Then, the network
    is applied to the result and *eat* and $f_{UV}(\mathbf{eat},f_{UV}(\mathbf{animal},\mathbf{extract}))$
    is obtained and so on.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¯¥æ–¹æ³•è‡ªç„¶åœ°å¤„ç†é€’å½’ï¼šç»™å®šä¸€ä¸ªå¥å­$s$çš„äºŒå‰è§£ææ ‘ï¼Œç®—æ³•ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºå‘é‡å’ŒçŸ©é˜µè¡¨ç¤ºï¼Œä»ç»ˆç«¯èŠ‚ç‚¹å¼€å§‹ã€‚å•è¯ç”±åˆ†å¸ƒå¼è¡¨ç¤ºæˆ–å±€éƒ¨è¡¨ç¤ºè¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œå›¾[3](#S7.F3
    "Figure 3 â€£ 7.3.2 Recursive Neural Network â€£ 7.3 Compositional Models in Neural
    Networks â€£ 7 Composing distributed representations â€£ Symbolic, Distributed and
    Distributional Representations for Natural Language Processing in the Era of Deep
    Learning: a Survey")ä¸­çš„æ ‘é€šè¿‡é€’å½’ç½‘ç»œä»¥ä»¥ä¸‹æ–¹å¼å¤„ç†ã€‚é¦–å…ˆï¼Œå°†å›¾[4](#S7.F4 "Figure 4 â€£ 7.3.2 Recursive
    Neural Network â€£ 7.3 Compositional Models in Neural Networks â€£ 7 Composing distributed
    representations â€£ Symbolic, Distributed and Distributional Representations for
    Natural Language Processing in the Era of Deep Learning: a Survey")ä¸­çš„ç½‘ç»œåº”ç”¨äº*(animal,extracts)*å¯¹ï¼Œå¾—åˆ°$f_{UV}(\mathbf{animal},\mathbf{extract})$ã€‚ç„¶åï¼Œå°†ç½‘ç»œåº”ç”¨äºç»“æœï¼Œå¾—åˆ°*eat*å’Œ$f_{UV}(\mathbf{eat},f_{UV}(\mathbf{animal},\mathbf{extract}))$ï¼Œä¾æ­¤ç±»æ¨ã€‚'
- en: 'Recursive neural networks are not easily interpretable even if quite similar
    to the additive *compositional distributional semantic models* as those presented
    in Sec. [7.1.1](#S7.SS1.SSS1 "7.1.1 Additive Models â€£ 7.1 Compositional Distributional
    Semantics â€£ 7 Composing distributed representations â€£ Symbolic, Distributed and
    Distributional Representations for Natural Language Processing in the Era of Deep
    Learning: a Survey"). In fact, the non-linear function $g$ is the one that makes
    final vectors less interpretable.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 'é€’å½’ç¥ç»ç½‘ç»œå³ä½¿ä¸åŠ æ€§*ç»„åˆåˆ†å¸ƒæ€§è¯­ä¹‰æ¨¡å‹*ç›¸ä¼¼ï¼Œä¹Ÿä¸å®¹æ˜“è¢«è§£é‡Šï¼Œå¦‚ç¬¬[7.1.1èŠ‚](#S7.SS1.SSS1 "7.1.1 Additive Models
    â€£ 7.1 Compositional Distributional Semantics â€£ 7 Composing distributed representations
    â€£ Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey")æ‰€è¿°ã€‚å®é™…ä¸Šï¼Œéçº¿æ€§å‡½æ•°$g$ä½¿æœ€ç»ˆå‘é‡ä¸æ˜“è§£é‡Šã€‚'
- en: 8 Conclusions
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 ç»“è®º
- en: 'Natural language is symbolic representation. Thinking to natural language understanding
    systems which are not based on symbols seems to be extremely odd. However, recent
    advances in machine learning (ML) and in natural language processing (NLP) seem
    to contradict the above intuition: symbols are fading away, erased by vectors
    or tensors called *distributed* and *distributional representations*.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€æ˜¯ä¸€ç§ç¬¦å·è¡¨ç¤ºã€‚è€ƒè™‘åˆ°éåŸºäºç¬¦å·çš„è‡ªç„¶è¯­è¨€ç†è§£ç³»ç»Ÿä¼¼ä¹éå¸¸å¥‡æ€ªã€‚ç„¶è€Œï¼Œæœ€è¿‘åœ¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„è¿›å±•ä¼¼ä¹ä¸ä¸Šè¿°ç›´è§‰ç›¸çŸ›ç›¾ï¼šç¬¦å·æ­£åœ¨æ¶ˆé€€ï¼Œè¢«ç§°ä¸º*åˆ†å¸ƒå¼*å’Œ*åˆ†å¸ƒæ€§è¡¨ç¤º*çš„å‘é‡æˆ–å¼ é‡æ‰€å–ä»£ã€‚
- en: We made this survey to show the not-surprising link between symbolic representations
    and distributed/distributional representations. This is the right time to revitalize
    the area of interpreting how symbols are represented inside neural networks. In
    our opinion, this survey will help to devise new deep neural networks that can
    exploit existing and novel symbolic models of classical natural language processing
    tasks. We believe that a clearer understanding of the strict link between distributed/distributional
    representations and symbols may lead to radically new deep learning networks.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åšè¿™é¡¹è°ƒæŸ¥æ˜¯ä¸ºäº†å±•ç¤ºç¬¦å·è¡¨ç¤ºä¸åˆ†å¸ƒå¼/åˆ†å¸ƒæ€§è¡¨ç¤ºä¹‹é—´çš„ä¸æƒŠè®¶çš„è”ç³»ã€‚ç°åœ¨æ­£æ˜¯é‡æŒ¯è§£é‡Šç¬¦å·å¦‚ä½•åœ¨ç¥ç»ç½‘ç»œå†…éƒ¨è¡¨ç¤ºçš„é¢†åŸŸçš„åˆé€‚æ—¶æœºã€‚åœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œè¿™é¡¹è°ƒæŸ¥å°†æœ‰åŠ©äºè®¾è®¡æ–°çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œè¿™äº›ç½‘ç»œå¯ä»¥åˆ©ç”¨ç°æœ‰çš„å’Œæ–°çš„ç¬¦å·æ¨¡å‹æ¥å¤„ç†ç»å…¸çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œå¯¹åˆ†å¸ƒå¼/åˆ†å¸ƒæ€§è¡¨ç¤ºä¸ç¬¦å·ä¹‹é—´ä¸¥æ ¼è”ç³»çš„æ›´æ¸…æ™°ç†è§£ï¼Œå¯èƒ½ä¼šå¯¼è‡´å…¨æ–°çš„æ·±åº¦å­¦ä¹ ç½‘ç»œã€‚
- en: References
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'Achlioptas (2003) Achlioptas, D. (2003). Database-friendly random projections:
    Johnson-lindenstrauss with binary coins. *Journal of computer and System Sciences*
    66, 671â€“687'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achlioptas (2003) Achlioptas, D. (2003). æ•°æ®åº“å‹å¥½çš„éšæœºæŠ•å½±ï¼šå¸¦æœ‰äºŒè¿›åˆ¶ç¡¬å¸çš„Johnson-Lindenstraussã€‚*è®¡ç®—æœºä¸ç³»ç»Ÿç§‘å­¦æ‚å¿—*
    66, 671â€“687
- en: 'Agirre etÂ al. (2013) Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A., and
    Guo, W. (2013). *sem 2013 shared task: Semantic textual similarity. In *Second
    Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings
    of the Main Conference and the Shared Task: Semantic Textual Similarity* (Atlanta,
    Georgia, USA: Association for Computational Linguistics), 32â€“43'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agirre ç­‰ (2013) Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A., å’Œ Guo, W.
    (2013). *sem 2013 å…±äº«ä»»åŠ¡ï¼šè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ã€‚åœ¨ *ç¬¬äºŒå±Šè¯æ±‡ä¸è®¡ç®—è¯­ä¹‰å­¦è”åˆä¼šè®®* (*SEM)ï¼Œç¬¬ 1 å·ï¼šä¸»ä¼šè®®å’Œå…±äº«ä»»åŠ¡ï¼šè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§è®ºæ–‡é›†*
    ï¼ˆç¾å›½ä¹”æ²»äºšå·äºšç‰¹å…°å¤§ï¼šè®¡ç®—è¯­è¨€å­¦åä¼šï¼‰ï¼Œ32â€“43
- en: Bahdanau etÂ al. (2014) Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural
    machine translation by jointly learning to align and translate. *arXiv preprint
    arXiv:1409.0473*
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau ç­‰ (2014) Bahdanau, D., Cho, K., å’Œ Bengio, Y. (2014). é€šè¿‡è”åˆå­¦ä¹ å¯¹é½å’Œç¿»è¯‘çš„ç¥ç»æœºå™¨ç¿»è¯‘ã€‚
    *arXiv é¢„å°æœ¬ arXiv:1409.0473*
- en: 'Baroni etÂ al. (2014) Baroni, M., Bernardi, R., and Zamparelli, R. (2014). Frege
    in space: A program of compositional distributional semantics. *LiLT (Linguistic
    Issues in Language Technology)* 9'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baroni ç­‰ (2014) Baroni, M., Bernardi, R., å’Œ Zamparelli, R. (2014). ç©ºé—´ä¸­çš„å¼—é›·æ ¼ï¼šç»„åˆåˆ†å¸ƒè¯­ä¹‰çš„ä¸€ä¸ªç¨‹åºã€‚
    *LiLT (è¯­è¨€æŠ€æœ¯ä¸­çš„è¯­è¨€å­¦é—®é¢˜)* 9
- en: 'Baroni and Lenci (2010) Baroni, M. and Lenci, A. (2010). Distributional memory:
    A general framework for corpus-based semantics. *Comput. Linguist.* 36, 673â€“721.
    [10.1162/coli_a_00016](https:/doi.org/10.1162/coli_a_00016)'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baroni å’Œ Lenci (2010) Baroni, M. å’Œ Lenci, A. (2010). åˆ†å¸ƒå¼è®°å¿†ï¼šåŸºäºè¯­æ–™åº“è¯­ä¹‰çš„é€šç”¨æ¡†æ¶ã€‚ *è®¡ç®—è¯­è¨€å­¦*
    36, 673â€“721ã€‚ [10.1162/coli_a_00016](https:/doi.org/10.1162/coli_a_00016)
- en: 'Baroni and Zamparelli (2010) Baroni, M. and Zamparelli, R. (2010). Nouns are
    vectors, adjectives are matrices: Representing adjective-noun constructions in
    semantic space. In *Proceedings of the 2010 Conference on Empirical Methods in
    Natural Language Processing* (Cambridge, MA: Association for Computational Linguistics),
    1183â€“1193'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baroni å’Œ Zamparelli (2010) Baroni, M. å’Œ Zamparelli, R. (2010). åè¯æ˜¯å‘é‡ï¼Œå½¢å®¹è¯æ˜¯çŸ©é˜µï¼šåœ¨è¯­ä¹‰ç©ºé—´ä¸­è¡¨ç¤ºå½¢å®¹è¯-åè¯æ„é€ ã€‚åœ¨
    *2010 å¹´è‡ªç„¶è¯­è¨€å¤„ç†å®è¯æ–¹æ³•ä¼šè®®è®ºæ–‡é›†* ï¼ˆå‰‘æ¡¥ï¼ŒMAï¼šè®¡ç®—è¯­è¨€å­¦åä¼šï¼‰ï¼Œ1183â€“1193
- en: Belkin and Niyogi (2001) Belkin, M. and Niyogi, P. (2001). Laplacian eigenmaps
    and spectral techniques for embedding and clustering. In *NIPS*. vol.Â 14, 585â€“591
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belkin å’Œ Niyogi (2001) Belkin, M. å’Œ Niyogi, P. (2001). æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾æ˜ å°„å’Œè°±æŠ€æœ¯åœ¨åµŒå…¥å’Œèšç±»ä¸­çš„åº”ç”¨ã€‚åœ¨
    *NIPS*ã€‚ç¬¬ 14 å·ï¼Œ585â€“591
- en: Bellman and Corporation (1957) Bellman, R. and Corporation, R. (1957). *Dynamic
    Programming*. Rand Corporation research study (Princeton University Press)
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman å’Œ Corporation (1957) Bellman, R. å’Œ Corporation, R. (1957). *åŠ¨æ€è§„åˆ’*ã€‚Rand
    Corporation ç ”ç©¶æŠ¥å‘Šï¼ˆPrinceton University Pressï¼‰
- en: 'Bingham and Mannila (2001) Bingham, E. and Mannila, H. (2001). Random projection
    in dimensionality reduction: applications to image and text data. In *Proceedings
    of the seventh ACM SIGKDD international conference on Knowledge discovery and
    data mining* (ACM), 245â€“250'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bingham å’Œ Mannila (2001) Bingham, E. å’Œ Mannila, H. (2001). ç»´åº¦ç¼©å‡ä¸­çš„éšæœºæŠ•å½±ï¼šå›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„åº”ç”¨ã€‚åœ¨
    *ç¬¬ä¸ƒå±Š ACM SIGKDD å›½é™…çŸ¥è¯†å‘ç°ä¸æ•°æ®æŒ–æ˜ä¼šè®®è®ºæ–‡é›†* ï¼ˆACMï¼‰ï¼Œ245â€“250
- en: Blutner etÂ al. (2003) Blutner, R., Hendriks, P., and deÂ Hoop, H. (2003). A new
    hypothesis on compositionality. In *Proceedings of the joint international conference
    on cognitive science*
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blutner ç­‰ (2003) Blutner, R., Hendriks, P., å’Œ de Hoop, H. (2003). å…³äºç»„åˆæ€§çš„ä¸€ä¸ªæ–°å‡è®¾ã€‚åœ¨
    *è®¤çŸ¥ç§‘å­¦è”åˆå›½é™…ä¼šè®®è®ºæ–‡é›†*
- en: 'Chalmers (1992) Chalmers, D.Â J. (1992). *Syntactic Transformations on Distributed
    Representations* (Dordrecht: Springer Netherlands). 46â€“55. [10.1007/978-94-011-2624-3_3](https:/doi.org/10.1007/978-94-011-2624-3_3)'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chalmers (1992) Chalmers, D. J. (1992). *åˆ†å¸ƒå¼è¡¨ç¤ºä¸Šçš„å¥æ³•å˜æ¢* ï¼ˆDordrecht: Springer
    Netherlandsï¼‰ã€‚46â€“55ã€‚ [10.1007/978-94-011-2624-3_3](https:/doi.org/10.1007/978-94-011-2624-3_3)'
- en: 'Chetlur etÂ al. (2014) Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J.,
    Tran, J., Catanzaro, B., etÂ al. (2014). cudnn: Efficient primitives for deep learning.
    *arXiv preprint arXiv:1410.0759*'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chetlur ç­‰ (2014) Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran,
    J., Catanzaro, B., ç­‰ (2014). cudnnï¼šé«˜æ•ˆçš„æ·±åº¦å­¦ä¹ åŸè¯­ã€‚ *arXiv é¢„å°æœ¬ arXiv:1410.0759*
- en: 'Chomsky (1957) Chomsky, N. (1957). *Aspect of Syntax Theory* (Cambridge, Massachussetts:
    MIT Press)'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chomsky (1957) Chomsky, N. (1957). *å¥æ³•ç†è®ºçš„å±‚é¢* ï¼ˆå‰‘æ¡¥ï¼Œé©¬è¨è¯¸å¡å·ï¼šMIT Pressï¼‰
- en: Clark etÂ al. (2008) Clark, S., Coecke, B., and Sadrzadeh, M. (2008). A compositional
    distributional model of meaning. *Proceedings of the Second Symposium on Quantum
    Interaction (QI-2008)* , 133â€“140
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark ç­‰ (2008) Clark, S., Coecke, B., å’Œ Sadrzadeh, M. (2008). è¯­ä¹‰çš„ç»„åˆåˆ†å¸ƒæ¨¡å‹ã€‚ *ç¬¬äºŒå±Šé‡å­äº’åŠ¨ç ”è®¨ä¼š
    (QI-2008) è®ºæ–‡é›†* ï¼Œ133â€“140
- en: Coecke etÂ al. (2010) Coecke, B., Sadrzadeh, M., and Clark, S. (2010). Mathematical
    foundations for a compositional distributional model of meaning. *CoRR* abs/1003.4394
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coecke ç­‰ (2010) Coecke, B., Sadrzadeh, M., å’Œ Clark, S. (2010). è¯­ä¹‰çš„ç»„åˆåˆ†å¸ƒæ¨¡å‹çš„æ•°å­¦åŸºç¡€ã€‚
    *CoRR* abs/1003.4394
- en: Cui etÂ al. (2015) Cui, H., Ganger, G.Â R., and Gibbons, P.Â B. (2015). *Scalable
    deep learning on distributed GPUs with a GPU-specialized parameter server*. Tech.
    rep., CMU PDL Technical Report (CMU-PDL-15-107)
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui ç­‰ (2015) Cui, H., Ganger, G.Â R., å’Œ Gibbons, P.Â B. (2015). *åœ¨åˆ†å¸ƒå¼GPUä¸Šå¯æ‰©å±•çš„æ·±åº¦å­¦ä¹ ï¼Œé…æœ‰GPUä¸“ç”¨å‚æ•°æœåŠ¡å™¨*ã€‚æŠ€æœ¯æŠ¥å‘Šï¼ŒCMU
    PDL æŠ€æœ¯æŠ¥å‘Š (CMU-PDL-15-107)
- en: 'Dagan etÂ al. (2013) Dagan, I., Roth, D., Sammons, M., and Zanzotto, F.Â M. (2013).
    *Recognizing Textual Entailment: Models and Applications*. Synthesis Lectures
    on Human Language Technologies (Morgan & Claypool Publishers)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dagan ç­‰ (2013) Dagan, I., Roth, D., Sammons, M., å’Œ Zanzotto, F.Â M. (2013).
    *è¯†åˆ«æ–‡æœ¬è•´å«: æ¨¡å‹ä¸åº”ç”¨*ã€‚äººç±»è¯­è¨€æŠ€æœ¯åˆæˆè®²åº§ï¼ˆMorgan & Claypool Publishersï¼‰'
- en: Daum and Huang (2003) Daum, F. and Huang, J. (2003). Curse of dimensionality
    and particle filters. In *Aerospace Conference, 2003\. Proceedings. 2003 IEEE*
    (IEEE), vol.Â 4, 4_1979â€“4_1993
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daum å’Œ Huang (2003) Daum, F. å’Œ Huang, J. (2003). ç»´åº¦ç¾éš¾ä¸ç²’å­æ»¤æ³¢å™¨ã€‚è§ *èˆªå¤©ä¼šè®®ï¼Œ2003å¹´\.
    è®ºæ–‡é›†ã€‚2003 IEEE*ï¼ˆIEEEï¼‰ï¼Œç¬¬4å·ï¼Œ4_1979â€“4_1993
- en: 'Devlin etÂ al. (2018) Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018).
    BERT: pre-training of deep bidirectional transformers for language understanding.
    *CoRR* abs/1810.04805'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin ç­‰ (2018) Devlin, J., Chang, M., Lee, K., å’Œ Toutanova, K. (2018). BERT:
    æ·±åº¦åŒå‘å˜æ¢å™¨çš„é¢„è®­ç»ƒç”¨äºè¯­è¨€ç†è§£ã€‚*CoRR* abs/1810.04805'
- en: 'Ferrone and Zanzotto (2014) Ferrone, L. and Zanzotto, F.Â M. (2014). Towards
    syntax-aware compositional distributional semantic models. In *Proceedings of
    COLING 2014, the 25th International Conference on Computational Linguistics: Technical
    Papers* (Dublin, Ireland: Dublin City University and Association for Computational
    Linguistics), 721â€“730'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ferrone å’Œ Zanzotto (2014) Ferrone, L. å’Œ Zanzotto, F.Â M. (2014). æœç€è¯­æ³•æ„ŸçŸ¥çš„ç»„åˆåˆ†å¸ƒè¯­ä¹‰æ¨¡å‹è¿ˆè¿›ã€‚è§
    *COLING 2014, ç¬¬25å±Šè®¡ç®—è¯­è¨€å­¦å›½é™…ä¼šè®®: æŠ€æœ¯è®ºæ–‡é›†*ï¼ˆéƒ½æŸæ—ï¼Œçˆ±å°”å…°: éƒ½æŸæ—åŸå¸‚å¤§å­¦å’Œè®¡ç®—è¯­è¨€å­¦åä¼šï¼‰ï¼Œ721â€“730'
- en: Ferrone etÂ al. (2015) Ferrone, L., Zanzotto, F.Â M., and Carreras, X. (2015).
    Decoding distributed tree structures. In *Statistical Language and Speech Processing
    - Third International Conference, SLSP 2015, Budapest, Hungary, November 24-26,
    2015, Proceedings*. 73â€“83. [10.1007/978-3-319-25789-1_8](https:/doi.org/10.1007/978-3-319-25789-1_8)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrone ç­‰ (2015) Ferrone, L., Zanzotto, F.Â M., å’Œ Carreras, X. (2015). è§£ç åˆ†å¸ƒå¼æ ‘ç»“æ„ã€‚è§
    *ç»Ÿè®¡è¯­è¨€ä¸è¯­éŸ³å¤„ç† - ç¬¬ä¸‰å±Šå›½é™…ä¼šè®®ï¼ŒSLSP 2015ï¼ŒåŒˆç‰™åˆ©å¸ƒè¾¾ä½©æ–¯ï¼Œ2015å¹´11æœˆ24-26æ—¥ï¼Œä¼šè®®è®ºæ–‡é›†*ã€‚73â€“83. [10.1007/978-3-319-25789-1_8](https:/doi.org/10.1007/978-3-319-25789-1_8)
- en: 'Firth (1957) Firth, J.Â R. (1957). *Papers in Linguistics.* (London: Oxford
    University Press.)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Firth (1957) Firth, J.Â R. (1957). *è¯­è¨€å­¦è®ºæ–‡é›†*ã€‚ï¼ˆä¼¦æ•¦: ç‰›æ´¥å¤§å­¦å‡ºç‰ˆç¤¾ã€‚ï¼‰'
- en: Fodor (2002) Fodor, I. (2002). *A Survey of Dimension Reduction Techniques*.
    Tech. rep.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fodor (2002) Fodor, I. (2002). *é™ç»´æŠ€æœ¯æ¦‚è¿°*ã€‚æŠ€æœ¯æŠ¥å‘Šã€‚
- en: 'Fodor and Pylyshyn (1988) Fodor, J.Â A. and Pylyshyn, Z.Â W. (1988). Connectionism
    and cognitive architecture: A critical analysis. *Cognition* 28, 3 â€“ 71. [https://doi.org/10.1016/0010-0277(88)90031-5](https:/doi.org/https://doi.org/10.1016/0010-0277(88)90031-5)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fodor å’Œ Pylyshyn (1988) Fodor, J.Â A. å’Œ Pylyshyn, Z.Â W. (1988). è¿æ¥ä¸»ä¹‰ä¸è®¤çŸ¥æ¶æ„: æ‰¹åˆ¤æ€§åˆ†æã€‚*è®¤çŸ¥*
    28, 3 â€“ 71. [https://doi.org/10.1016/0010-0277(88)90031-5](https:/doi.org/https://doi.org/10.1016/0010-0277(88)90031-5)'
- en: 'Frege (1884) Frege, G. (1884). *Die Grundlagen der Arithmetik (The Foundations
    of Arithmetic): eine logisch-mathematische Untersuchung Ã¼ber den Begriff der Zahl*
    (Breslau)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frege (1884) Frege, G. (1884). *ç®—æœ¯åŸºç¡€ï¼ˆThe Foundations of Arithmeticï¼‰ï¼šå…³äºæ•°å­—æ¦‚å¿µçš„é€»è¾‘-æ•°å­¦ç ”ç©¶*ï¼ˆå¸ƒé›·æ–¯åŠ³ï¼‰
- en: Friedman (1997) Friedman, J.Â H. (1997). On bias, variance, 0/1â€”loss, and the
    curse-of-dimensionality. *Data mining and knowledge discovery* 1, 55â€“77
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Friedman (1997) Friedman, J.Â H. (1997). å…³äºåå·®ã€æ–¹å·®ã€0/1â€”æŸå¤±å’Œç»´åº¦ç¾éš¾ã€‚*æ•°æ®æŒ–æ˜ä¸çŸ¥è¯†å‘ç°* 1, 55â€“77
- en: 'Gelder (1990) Gelder, T.Â V. (1990). Compositionality: A connectionist variation
    on a classical theme. *Cognitive Science* 384, 355â€“384. [10.1207/s15516709cog1403_2](https:/doi.org/10.1207/s15516709cog1403_2)'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gelder (1990) Gelder, T.Â V. (1990). ç»„åˆæ€§: ä¸€ç§è¿æ¥ä¸»ä¹‰çš„å¤å…¸ä¸»é¢˜å˜ä½“ã€‚*è®¤çŸ¥ç§‘å­¦* 384, 355â€“384.
    [10.1207/s15516709cog1403_2](https:/doi.org/10.1207/s15516709cog1403_2)'
- en: 'Goldberg and Levy (2014) Goldberg, Y. and Levy, O. (2014). word2vec explained:
    deriving mikolov et al.â€™s negative-sampling word-embedding method. *arXiv preprint
    arXiv:1402.3722*'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goldberg å’Œ Levy (2014) Goldberg, Y. å’Œ Levy, O. (2014). word2vec è§£é‡Š: æ¨å¯¼ Mikolov
    ç­‰çš„è´Ÿé‡‡æ ·è¯åµŒå…¥æ–¹æ³•ã€‚*arXiv é¢„å°æœ¬ arXiv:1402.3722*'
- en: Goodfellow etÂ al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., etÂ al. (2014). Generative adversarial nets. In *Advances
    in Neural Information Processing Systems*. 2672â€“2680
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow ç­‰ (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
    D., Ozair, S., ç­‰ (2014). ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚è§ *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ã€‚2672â€“2680
- en: Graves (2013) Graves, A. (2013). Generating sequences with recurrent neural
    networks. *CoRR* abs/1308.0850
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves (2013) Graves, A. (2013). ä½¿ç”¨é€’å½’ç¥ç»ç½‘ç»œç”Ÿæˆåºåˆ—ã€‚*CoRR* abs/1308.0850
- en: 'Grefenstette and Sadrzadeh (2011) Grefenstette, E. and Sadrzadeh, M. (2011).
    Experimental support for a categorical compositional distributional model of meaning.
    In *Proceedings of the Conference on Empirical Methods in Natural Language Processing*
    (Stroudsburg, PA, USA: Association for Computational Linguistics), EMNLP â€™11,
    1394â€“1404'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grefenstette å’Œ Sadrzadeh (2011) Grefenstette, E. å’Œ Sadrzadeh, M. (2011). å¯¹ç±»åˆ«ç»„åˆåˆ†å¸ƒæ¨¡å‹çš„å®éªŒæ”¯æŒã€‚åœ¨
    *è‡ªç„¶è¯­è¨€å¤„ç†ç»éªŒæ–¹æ³•ä¼šè®®è®ºæ–‡é›†*ï¼ˆå®¾å¤•æ³•å°¼äºšå·æ–¯ç‰¹åŠ³å…¹å ¡ï¼šè®¡ç®—è¯­è¨€å­¦åä¼šï¼‰ï¼ŒEMNLP â€™11ï¼Œ1394â€“1404
- en: 'Guevara (2010) Guevara, E. (2010). A regression model of adjective-noun compositionality
    in distributional semantics. In *Proceedings of the 2010 Workshop on GEometrical
    Models of Natural Language Semantics* (Uppsala, Sweden: Association for Computational
    Linguistics), 33â€“37'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guevara (2010) Guevara, E. (2010). åˆ†å¸ƒè¯­ä¹‰ä¸­å½¢å®¹è¯-åè¯ç»„åˆæ€§çš„å›å½’æ¨¡å‹ã€‚åœ¨ *2010 å¹´å‡ ä½•æ¨¡å‹è‡ªç„¶è¯­è¨€è¯­ä¹‰ç ”è®¨ä¼šè®ºæ–‡é›†*ï¼ˆç‘å…¸ä¹Œæ™®è¨æ‹‰ï¼šè®¡ç®—è¯­è¨€å­¦åä¼šï¼‰ï¼Œ33â€“37
- en: 'Harris (1964) Harris, Z. (1964). Distributional structure. In *The Philosophy
    of Linguistics*, eds. J.Â J. Katz and J.Â A. Fodor (New York: Oxford University
    Press)'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harris (1964) Harris, Z. (1964). åˆ†å¸ƒç»“æ„ã€‚åœ¨ *è¯­è¨€å­¦å“²å­¦*ï¼Œç¼–è¾‘ J. J. Katz å’Œ J. A. Fodorï¼ˆçº½çº¦ï¼šç‰›æ´¥å¤§å­¦å‡ºç‰ˆç¤¾ï¼‰
- en: He etÂ al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. (2016). Identity mappings
    in deep residual networks. *arXiv preprint arXiv:1603.05027*
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He ç­‰ (2016) He, K., Zhang, X., Ren, S., å’Œ Sun, J. (2016). æ·±åº¦æ®‹å·®ç½‘ç»œä¸­çš„èº«ä»½æ˜ å°„ã€‚*arXiv
    é¢„å°æœ¬ arXiv:1603.05027*
- en: 'Hinton etÂ al. (1986) Hinton, G.Â E., McClelland, J.Â L., and Rumelhart, D.Â E.
    (1986). Distributed representations. In *Parallel Distributed Processing: Explorations
    in the Microstructure of Cognition. Volume 1: Foundations*, eds. D.Â E. Rumelhart
    and J.Â L. McClelland (MIT Press, Cambridge, MA.)'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton ç­‰ (1986) Hinton, G. E., McClelland, J. L., å’Œ Rumelhart, D. E. (1986).
    åˆ†å¸ƒå¼è¡¨ç¤ºã€‚åœ¨ *å¹¶è¡Œåˆ†å¸ƒå¤„ç†ï¼šè®¤çŸ¥å¾®ç»“æ„æ¢ç´¢ã€‚ç¬¬ 1 å·ï¼šåŸºç¡€*ï¼Œç¼–è¾‘ D. E. Rumelhart å’Œ J. L. McClellandï¼ˆMIT Press,
    Cambridge, MAï¼‰
- en: Hochreiter and Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. (1997).
    Long short-term memory. *Neural computation* 9, 1735â€“1780
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter å’Œ Schmidhuber (1997) Hochreiter, S. å’Œ Schmidhuber, J. (1997). é•¿çŸ­æœŸè®°å¿†ã€‚*ç¥ç»è®¡ç®—*
    9ï¼Œ1735â€“1780
- en: Jacovi etÂ al. (2018) Jacovi, A., Shalom, O.Â S., and Goldberg, Y. (2018). Understanding
    Convolutional Neural Networks for Text Classification , 56â€“65[doi:10.1046/j.1365-3040.2003.01027.x](https:/doi.org/doi:10.1046/j.1365-3040.2003.01027.x)
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacovi ç­‰ (2018) Jacovi, A., Shalom, O. S., å’Œ Goldberg, Y. (2018). ç†è§£å·ç§¯ç¥ç»ç½‘ç»œç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œ56â€“65
    [doi:10.1046/j.1365-3040.2003.01027.x](https:/doi.org/doi:10.1046/j.1365-3040.2003.01027.x)
- en: Jang etÂ al. (2018) Jang, K.-r., Kim, S.-b., and Corp, N. (2018). Interpretable
    Word Embedding Contextualization , 341â€“343
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang ç­‰ (2018) Jang, K.-r., Kim, S.-b., å’Œ Corp, N. (2018). å¯è§£é‡Šçš„è¯åµŒå…¥ä¸Šä¸‹æ–‡åŒ–ï¼Œ341â€“343
- en: Johnson and Lindenstrauss (1984) Johnson, W. and Lindenstrauss, J. (1984). Extensions
    of lipschitz mappings into a hilbert space. *Contemp. Math.* 26, 189â€“206
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson å’Œ Lindenstrauss (1984) Johnson, W. å’Œ Lindenstrauss, J. (1984). å‘å¸Œå°”ä¼¯ç‰¹ç©ºé—´çš„
    Lipschitz æ˜ å°„çš„æ‰©å±•ã€‚ *Contemp. Math.* 26ï¼Œ189â€“206
- en: Kalchbrenner and Blunsom (2013) Kalchbrenner, N. and Blunsom, P. (2013). Recurrent
    convolutional neural networks for discourse compositionality. *Proceedings of
    the 2013 Workshop on Continuous Vector Space Models and their Compositionality*
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalchbrenner å’Œ Blunsom (2013) Kalchbrenner, N. å’Œ Blunsom, P. (2013). ç”¨äºè¯è¯­ç»„åˆæ€§çš„é€’å½’å·ç§¯ç¥ç»ç½‘ç»œã€‚*2013
    å¹´è¿ç»­å‘é‡ç©ºé—´æ¨¡å‹åŠå…¶ç»„åˆæ€§ç ”è®¨ä¼šè®ºæ–‡é›†*
- en: Keogh and Mueen (2011) Keogh, E. and Mueen, A. (2011). Curse of dimensionality.
    In *Encyclopedia of Machine Learning* (Springer). 257â€“258
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keogh å’Œ Mueen (2011) Keogh, E. å’Œ Mueen, A. (2011). ç»´åº¦ç¾éš¾ã€‚åœ¨ *æœºå™¨å­¦ä¹ ç™¾ç§‘å…¨ä¹¦*ï¼ˆSpringerï¼‰ã€‚257â€“258
- en: Krizhevsky etÂ al. (2012) Krizhevsky, A., Sutskever, I., and Hinton, G.Â E. (2012).
    Imagenet classification with deep convolutional neural networks. In *Advances
    in neural information processing systems*. 1097â€“1105
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky ç­‰ (2012) Krizhevsky, A., Sutskever, I., å’Œ Hinton, G. E. (2012). ä½¿ç”¨æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œçš„
    ImageNet åˆ†ç±»ã€‚åœ¨ *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ã€‚1097â€“1105
- en: 'Landauer and Dumais (1997) Landauer, T.Â K. and Dumais, S.Â T. (1997). A solution
    to platoâ€™s problem: The latent semantic analysis theory of acquisition, induction,
    and representation of knowledge. *Psychological Review* 104, 211â€“240'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landauer å’Œ Dumais (1997) Landauer, T. K. å’Œ Dumais, S. T. (1997). è§£å†³æŸæ‹‰å›¾é—®é¢˜ï¼šæ½œåœ¨è¯­ä¹‰åˆ†æç†è®ºçš„çŸ¥è¯†è·å–ã€å½’çº³å’Œè¡¨ç¤ºã€‚*å¿ƒç†å­¦è¯„è®º*
    104ï¼Œ211â€“240
- en: LeCun etÂ al. (2015) LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning.
    *Nature* 521, 436â€“444
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun ç­‰ (2015) LeCun, Y., Bengio, Y., å’Œ Hinton, G. (2015). æ·±åº¦å­¦ä¹ ã€‚*è‡ªç„¶* 521ï¼Œ436â€“444
- en: Liou etÂ al. (2014) Liou, C.-Y., Cheng, W.-C., Liou, J.-W., and Liou, D.-R. (2014).
    Autoencoder for words. *Neurocomputing* 139, 84 â€“ 96. [http://dx.doi.org/10.1016/j.neucom.2013.09.055](https:/doi.org/http://dx.doi.org/10.1016/j.neucom.2013.09.055)
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liou ç­‰ (2014) Liou, C.-Y., Cheng, W.-C., Liou, J.-W., å’Œ Liou, D.-R. (2014).
    è¯çš„è‡ªç¼–ç å™¨ã€‚*ç¥ç»è®¡ç®—* 139, 84 â€“ 96. [http://dx.doi.org/10.1016/j.neucom.2013.09.055](https:/doi.org/http://dx.doi.org/10.1016/j.neucom.2013.09.055)
- en: Lipton (2016) Lipton, Z.Â C. (2016). The Mythos of Model Interpretability [10.1145/3233231](https:/doi.org/10.1145/3233231)
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lipton (2016) Lipton, Z. C. (2016). æ¨¡å‹å¯è§£é‡Šæ€§çš„ç¥è¯ [10.1145/3233231](https:/doi.org/10.1145/3233231)
- en: 'Markovsky (2012) Markovsky, I. (2012). Low rank approximation: Algorithms,
    implementation, applications'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Markovsky (2012) Markovsky, I. (2012). ä½ç§©é€¼è¿‘: ç®—æ³•ã€å®ç°ã€åº”ç”¨'
- en: Masci etÂ al. (2011) Masci, J., Meier, U., CireÅŸan, D., and Schmidhuber, J. (2011).
    Stacked convolutional auto-encoders for hierarchical feature extraction. In *International
    Conference on Artificial Neural Networks* (Springer), 52â€“59
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masci ç­‰ (2011) Masci, J., Meier, U., CireÅŸan, D., å’Œ Schmidhuber, J. (2011).
    ç”¨äºåˆ†å±‚ç‰¹å¾æå–çš„å †å å·ç§¯è‡ªç¼–ç å™¨ã€‚è§ *å›½é™…äººå·¥ç¥ç»ç½‘ç»œå¤§ä¼š* (Springer), 52â€“59
- en: Mikolov etÂ al. (2013) Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013).
    Efficient estimation of word representations in vector space. *CoRR* abs/1301.3781
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov ç­‰ (2013) Mikolov, T., Chen, K., Corrado, G., å’Œ Dean, J. (2013). å‘é‡ç©ºé—´ä¸­è¯è¡¨ç¤ºçš„é«˜æ•ˆä¼°è®¡ã€‚*CoRR*
    abs/1301.3781
- en: 'Mitchell and Lapata (2008) Mitchell, J. and Lapata, M. (2008). Vector-based
    models of semantic composition. In *Proceedings of ACL-08: HLT* (Columbus, Ohio:
    Association for Computational Linguistics), 236â€“244'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mitchell å’Œ Lapata (2008) Mitchell, J. å’Œ Lapata, M. (2008). åŸºäºå‘é‡çš„è¯­ä¹‰ç»„åˆæ¨¡å‹ã€‚è§ *ACL-08:
    HLT ä¼šè®®è®ºæ–‡é›†* (ä¿„äº¥ä¿„å·å“¥ä¼¦å¸ƒå¸‚: è®¡ç®—è¯­è¨€å­¦åä¼š), 236â€“244'
- en: Mitchell and Lapata (2010) Mitchell, J. and Lapata, M. (2010). Composition in
    distributional models of semantics. *Cognitive Science* [10.1111/j.1551-6709.2010.01106.x](https:/doi.org/10.1111/j.1551-6709.2010.01106.x)
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell å’Œ Lapata (2010) Mitchell, J. å’Œ Lapata, M. (2010). è¯­ä¹‰åˆ†å¸ƒæ¨¡å‹ä¸­çš„ç»„åˆã€‚*è®¤çŸ¥ç§‘å­¦*
    [10.1111/j.1551-6709.2010.01106.x](https:/doi.org/10.1111/j.1551-6709.2010.01106.x)
- en: 'Montague (1974) Montague, R. (1974). English as a formal language. In *Formal
    Philosophy: Selected Papers of Richard Montague*, ed. R.Â Thomason (New Haven:
    Yale University Press). 188â€“221'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Montague (1974) Montague, R. (1974). è‹±è¯­ä½œä¸ºå½¢å¼è¯­è¨€ã€‚è§ *å½¢å¼å“²å­¦: ç†æŸ¥å¾·Â·è’™å¡”å¤ç²¾é€‰è®ºæ–‡*, ç¼–è¾‘ R.
    Thomason (çº½é»‘æ–‡: è€¶é²å¤§å­¦å‡ºç‰ˆç¤¾). 188â€“221'
- en: Neumann (2001) Neumann, J. (2001). *Holistic processing of hierarchical structures
    in connectionist networks*. Ph.D. thesis, University of Edinburgh
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neumann (2001) Neumann, J. (2001). *è¿æ¥ä¸»ä¹‰ç½‘ç»œä¸­çš„å±‚æ¬¡ç»“æ„çš„æ•´ä½“å¤„ç†*. åšå£«è®ºæ–‡, çˆ±ä¸å ¡å¤§å­¦
- en: Pado and Lapata (2007) Pado, S. and Lapata, M. (2007). Dependency-based construction
    of semantic space models. *Computational Linguistics* 33, 161â€“199
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pado å’Œ Lapata (2007) Pado, S. å’Œ Lapata, M. (2007). åŸºäºä¾èµ–çš„è¯­ä¹‰ç©ºé—´æ¨¡å‹æ„å»ºã€‚*è®¡ç®—è¯­è¨€å­¦* 33,
    161â€“199
- en: Pearson (1901) Pearson, K. (1901). Principal components analysis. *The London,
    Edinburgh and Dublin Philosophical Magazine and Journal* 6, 566
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearson (1901) Pearson, K. (1901). ä¸»æˆåˆ†åˆ†æã€‚*ä¼¦æ•¦ã€çˆ±ä¸å ¡å’Œéƒ½æŸæ—å“²å­¦æ‚å¿—* 6, 566
- en: Plate (1994) Plate, T.Â A. (1994). *Distributed Representations and Nested Compositional
    Structure*. Ph.D. thesis
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plate (1994) Plate, T. A. (1994). *åˆ†å¸ƒå¼è¡¨ç¤ºå’ŒåµŒå¥—ç»„åˆç»“æ„*. åšå£«è®ºæ–‡
- en: Plate (1995) Plate, T.Â A. (1995). Holographic reduced representations. *IEEE
    Transactions on Neural Networks* 6, 623â€“641. [10.1109/72.377968](https:/doi.org/10.1109/72.377968)
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plate (1995) Plate, T. A. (1995). å…¨æ¯é™ç»´è¡¨ç¤ºã€‚*IEEE ç¥ç»ç½‘ç»œäº¤æ˜“* 6, 623â€“641. [10.1109/72.377968](https:/doi.org/10.1109/72.377968)
- en: 'Rosenblatt (1958) Rosenblatt, F. (1958). The perceptron: a probabilistic model
    for information storage and organization in the brain. *Psychological Reviews*
    65, 386â€“408'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rosenblatt (1958) Rosenblatt, F. (1958). æ„ŸçŸ¥å™¨: ä¸€ç§ç”¨äºä¿¡æ¯å­˜å‚¨å’Œå¤§è„‘ç»„ç»‡çš„æ¦‚ç‡æ¨¡å‹ã€‚*å¿ƒç†å­¦è¯„è®º* 65,
    386â€“408'
- en: 'RothenhÃ¤usler and SchÃ¼tze (2009) RothenhÃ¤usler, K. and SchÃ¼tze, H. (2009).
    Unsupervised classification with dependency based word spaces. In *Proceedings
    of the Workshop on Geometrical Models of Natural Language Semantics* (Stroudsburg,
    PA, USA: Association for Computational Linguistics), GEMS â€™09, 17â€“24'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RothenhÃ¤usler å’Œ SchÃ¼tze (2009) RothenhÃ¤usler, K. å’Œ SchÃ¼tze, H. (2009). åŸºäºä¾èµ–çš„æ— ç›‘ç£åˆ†ç±»è¯ç©ºé—´ã€‚è§
    *è‡ªç„¶è¯­è¨€è¯­ä¹‰å‡ ä½•æ¨¡å‹ç ”è®¨ä¼šè®ºæ–‡é›†* (å®¾å¤•æ³•å°¼äºšå·æ–¯ç‰¹åŠ³å…¹å ¡å¸‚: è®¡ç®—è¯­è¨€å­¦åä¼š), GEMS â€™09, 17â€“24'
- en: Sahlgren (2005) Sahlgren, M. (2005). An introduction to random indexing. In
    *Proceedings of the Methods and Applications of Semantic Indexing Workshop at
    the 7th International Conference on Terminology and Knowledge Engineering TKE*
    (Copenhagen, Denmark)
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahlgren (2005) Sahlgren, M. (2005). éšæœºç´¢å¼•ç®€ä»‹ã€‚è§ *ç¬¬ä¸ƒå±Šæœ¯è¯­å’ŒçŸ¥è¯†å·¥ç¨‹å›½é™…ä¼šè®® TKE ç ”è®¨ä¼šæ–¹æ³•ä¸åº”ç”¨è®ºæ–‡é›†*
    (ä¸¹éº¦å“¥æœ¬å“ˆæ ¹)
- en: 'Salton (1989) Salton, G. (1989). *Automatic text processing: the transformation,
    analysis and retrieval of information by computer* (Addison-Wesley)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salton (1989) Salton, G. (1989). *è‡ªåŠ¨æ–‡æœ¬å¤„ç†ï¼šè®¡ç®—æœºçš„ä¿¡æ¯è½¬åŒ–ã€åˆ†æä¸æ£€ç´¢* (Addison-Wesley)
- en: 'Schmidhuber (2015) Schmidhuber, J. (2015). Deep learning in neural networks:
    An overview. *Neural Networks* 61, 85â€“117'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidhuber (2015) Schmidhuber, J. (2015). ç¥ç»ç½‘ç»œä¸­çš„æ·±åº¦å­¦ä¹ ï¼šæ¦‚è¿°ã€‚*Neural Networks* 61,
    85â€“117
- en: Schuster and Paliwal (1997) Schuster, M. and Paliwal, K. (1997). Bidirectional
    recurrent neural networks. *Trans. Sig. Proc.* 45, 2673â€“2681. [10.1109/78.650093](https:/doi.org/10.1109/78.650093)
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuster and Paliwal (1997) Schuster, M. and Paliwal, K. (1997). åŒå‘é€’å½’ç¥ç»ç½‘ç»œã€‚*Trans.
    Sig. Proc.* 45, 2673â€“2681. [10.1109/78.650093](https:/doi.org/10.1109/78.650093)
- en: Socher etÂ al. (2011) Socher, R., Huang, E.Â H., Pennington, J., Ng, A.Â Y., and
    Manning, C.Â D. (2011). Dynamic pooling and unfolding recursive autoencoders for
    paraphrase detection. In *Advances in Neural Information Processing Systems 24*
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. (2011) Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and
    Manning, C. D. (2011). åŠ¨æ€æ± åŒ–å’Œå±•å¼€é€’å½’è‡ªç¼–ç å™¨ç”¨äºåŒä¹‰å¥æ£€æµ‹ã€‚è½½äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±• 24*
- en: Socher etÂ al. (2012) Socher, R., Huval, B., Manning, C.Â D., and Ng, A.Â Y. (2012).
    Semantic Compositionality Through Recursive Matrix-Vector Spaces. In *Proceedings
    of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)*
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. (2012) Socher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012).
    é€šè¿‡é€’å½’çŸ©é˜µ-å‘é‡ç©ºé—´çš„è¯­ä¹‰ç»„åˆæ€§ã€‚è½½äº *2012å¹´è‡ªç„¶è¯­è¨€å¤„ç†ç»éªŒæ–¹æ³•ä¼šè®®è®ºæ–‡é›†ï¼ˆEMNLPï¼‰*
- en: Sorzano etÂ al. (2014) Sorzano, C. O.Â S., Vargas, J., and Montano, A.Â P. (2014).
    A survey of dimensionality reduction techniques. *arXiv preprint arXiv:1403.2877*
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorzano et al. (2014) Sorzano, C. O. S., Vargas, J., and Montano, A. P. (2014).
    ç»´åº¦ç¼©å‡æŠ€æœ¯ç»¼è¿°ã€‚*arXivé¢„å°æœ¬ arXiv:1403.2877*
- en: Turney (2006) Turney, P.Â D. (2006). Similarity of semantic relations. *Comput.
    Linguist.* 32, 379â€“416. [http://dx.doi.org/10.1162/coli.2006.32.3.379](https:/doi.org/http://dx.doi.org/10.1162/coli.2006.32.3.379)
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turney (2006) Turney, P. D. (2006). è¯­ä¹‰å…³ç³»çš„ç›¸ä¼¼æ€§ã€‚*Comput. Linguist.* 32, 379â€“416.
    [http://dx.doi.org/10.1162/coli.2006.32.3.379](https:/doi.org/http://dx.doi.org/10.1162/coli.2006.32.3.379)
- en: 'Turney and Pantel (2010) Turney, P.Â D. and Pantel, P. (2010). From frequency
    to meaning: Vector space models of semantics. *J. Artif. Intell. Res. (JAIR)*
    37, 141â€“188'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turney and Pantel (2010) Turney, P. D. and Pantel, P. (2010). ä»é¢‘ç‡åˆ°æ„ä¹‰ï¼šè¯­ä¹‰çš„å‘é‡ç©ºé—´æ¨¡å‹ã€‚*J.
    Artif. Intell. Res. (JAIR)* 37, 141â€“188
- en: Vaswani etÂ al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A.Â N., etÂ al. (2017). Attention is all you need. In *Advances in Neural
    Information Processing Systems 30*, eds. I.Â Guyon, U.Â V. Luxburg, S.Â Bengio, H.Â Wallach,
    R.Â Fergus, S.Â Vishwanathan, and R.Â Garnett (Curran Associates, Inc.). 5998â€“6008
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., et al. (2017). æ³¨æ„åŠ›æœºåˆ¶æ˜¯ä½ æ‰€éœ€çš„ä¸€åˆ‡ã€‚è½½äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±• 30*ï¼Œç¼–è¾‘ I. Guyon,
    U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, å’Œ R. Garnett
    (Curran Associates, Inc.)ã€‚5998â€“6008
- en: Vincent etÂ al. (2008) Vincent, P., Larochelle, H., Bengio, Y., and Manzagol,
    P.-A. (2008). Extracting and composing robust features with denoising autoencoders.
    In *Proceedings of the 25th international conference on Machine learning* (ACM),
    1096â€“1103
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent et al. (2008) Vincent, P., Larochelle, H., Bengio, Y., and Manzagol,
    P.-A. (2008). ä½¿ç”¨å»å™ªè‡ªç¼–ç å™¨æå–å’Œç»„åˆé²æ£’ç‰¹å¾ã€‚è½½äº *ç¬¬25å±Šå›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šè®ºæ–‡é›†* (ACM)ï¼Œ1096â€“1103
- en: 'Vincent etÂ al. (2010) Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y.,
    and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations
    in a deep network with a local denoising criterion. *J. Mach. Learn. Res.* 11,
    3371â€“3408'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent et al. (2010) Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and
    Manzagol, P.-A. (2010). å †å å»å™ªè‡ªç¼–ç å™¨ï¼šé€šè¿‡å±€éƒ¨å»å™ªå‡†åˆ™åœ¨æ·±åº¦ç½‘ç»œä¸­å­¦ä¹ æœ‰ç”¨è¡¨ç¤ºã€‚*J. Mach. Learn. Res.* 11,
    3371â€“3408
- en: Vinyals etÂ al. (2015a) Vinyals, O., Kaiser, L.Â u., Koo, T., Petrov, S., Sutskever,
    I., and Hinton, G. (2015a). Grammar as a foreign language. In *Advances in Neural
    Information Processing Systems 28*, eds. C.Â Cortes, N.Â D. Lawrence, D.Â D. Lee,
    M.Â Sugiyama, and R.Â Garnett (Curran Associates, Inc.). 2755â€“2763
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2015a) Vinyals, O., Kaiser, L. u., Koo, T., Petrov, S., Sutskever,
    I., and Hinton, G. (2015a). è¯­æ³•ä½œä¸ºå¤–è¯­ã€‚è½½äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±• 28*ï¼Œç¼–è¾‘ C. Cortes, N. D. Lawrence,
    D. D. Lee, M. Sugiyama, å’Œ R. Garnett (Curran Associates, Inc.)ã€‚2755â€“2763
- en: 'Vinyals etÂ al. (2015b) Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015b).
    Show and tell: A neural image caption generator. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*. 3156â€“3164'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2015b) Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015b).
    å±•ç¤ºä¸è®²è¿°ï¼šä¸€ä¸ªç¥ç»å›¾åƒæè¿°ç”Ÿæˆå™¨ã€‚è½½äº *IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ã€‚3156â€“3164
- en: Weiss etÂ al. (2015) Weiss, D., Alberti, C., Collins, M., and Petrov, S. (2015).
    Structured training for neural network transition-based parsing. *arXiv preprint
    arXiv:1506.06158*
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weiss ç­‰äººï¼ˆ2015ï¼‰Weiss, D., Alberti, C., Collins, M., å’Œ Petrov, S.ï¼ˆ2015ï¼‰ã€‚é’ˆå¯¹ç¥ç»ç½‘ç»œè¿‡æ¸¡å¼è§£æçš„ç»“æ„åŒ–è®­ç»ƒã€‚*arXiv
    é¢„å°æœ¬ arXiv:1506.06158*
- en: 'Werbos (1974) Werbos, P. (1974). Beyond regression: New tools for prediction
    and analysis in the behavioral sciences'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Werbosï¼ˆ1974ï¼‰Werbos, P.ï¼ˆ1974ï¼‰ã€‚è¶…è¶Šå›å½’ï¼šè¡Œä¸ºç§‘å­¦ä¸­çš„æ–°é¢„æµ‹ä¸åˆ†æå·¥å…·
- en: 'Xu etÂ al. (2015) Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov,
    R., etÂ al. (2015). Show, attend and tell: Neural image caption generation with
    visual attention. *arXiv preprint arXiv:1502.03044* 2, 5'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu ç­‰äººï¼ˆ2015ï¼‰Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov,
    R., ç­‰äººï¼ˆ2015ï¼‰ã€‚å±•ç¤ºã€å…³æ³¨å’Œè®²è¿°ï¼šå¸¦æœ‰è§†è§‰æ³¨æ„åŠ›çš„ç¥ç»å›¾åƒæè¿°ç”Ÿæˆã€‚*arXiv é¢„å°æœ¬ arXiv:1502.03044* 2, 5
- en: Zanzotto and Dellâ€™Arciprete (2012) Zanzotto, F.Â M. and Dellâ€™Arciprete, L. (2012).
    Distributed tree kernels. In *Proceedings of International Conference on Machine
    Learning*. â€“
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanzotto å’Œ Dellâ€™Arcipreteï¼ˆ2012ï¼‰Zanzotto, F. M. å’Œ Dellâ€™Arciprete, L.ï¼ˆ2012ï¼‰ã€‚åˆ†å¸ƒå¼æ ‘æ ¸ã€‚è§
    *Proceedings of International Conference on Machine Learning*ã€‚
- en: 'Zanzotto etÂ al. (2015) Zanzotto, F.Â M., Ferrone, L., and Baroni, M. (2015).
    When the whole is not greater than the combination of its parts: A â€decompositionalâ€
    look at compositional distributional semantics. *Comput. Linguist.* 41, 165â€“173.
    [10.1162/COLI_a_00215](https:/doi.org/10.1162/COLI_a_00215)'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanzotto ç­‰äººï¼ˆ2015ï¼‰Zanzotto, F. M., Ferrone, L., å’Œ Baroni, M.ï¼ˆ2015ï¼‰ã€‚å½“æ•´ä½“ä¸å¤§äºå…¶éƒ¨åˆ†çš„ç»„åˆæ—¶ï¼šå¯¹ç»„åˆåˆ†å¸ƒè¯­ä¹‰çš„â€œåˆ†è§£â€è§†è§’ã€‚*Comput.
    Linguist.* 41, 165â€“173. [10.1162/COLI_a_00215](https:/doi.org/10.1162/COLI_a_00215)
- en: Zanzotto etÂ al. (2010) Zanzotto, F.Â M., Korkontzelos, I., Fallucchi, F., and
    Manandhar, S. (2010). Estimating linear models for compositional distributional
    semantics. In *Proceedings of the 23rd International Conference on Computational
    Linguistics (COLING)*
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanzotto ç­‰äººï¼ˆ2010ï¼‰Zanzotto, F. M., Korkontzelos, I., Fallucchi, F., å’Œ Manandhar,
    S.ï¼ˆ2010ï¼‰ã€‚ä¼°è®¡ç”¨äºç»„åˆåˆ†å¸ƒè¯­ä¹‰çš„çº¿æ€§æ¨¡å‹ã€‚è§ *Proceedings of the 23rd International Conference on
    Computational Linguistics (COLING)*
- en: 'Zeiler and Fergus (2014a) Zeiler, M.Â D. and Fergus, R. (2014a). Visualizing
    and understanding convolutional networks. In *Computer Vision â€“ ECCV 2014*, eds.
    D.Â Fleet, T.Â Pajdla, B.Â Schiele, and T.Â Tuytelaars (Cham: Springer International
    Publishing), 818â€“833'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeiler å’Œ Fergusï¼ˆ2014aï¼‰Zeiler, M. D. å’Œ Fergus, R.ï¼ˆ2014aï¼‰ã€‚å¯è§†åŒ–ä¸ç†è§£å·ç§¯ç½‘ç»œã€‚è§ *Computer
    Vision â€“ ECCV 2014*ï¼Œç¼–è€… D. Fleet, T. Pajdla, B. Schiele, å’Œ T. Tuytelaarsï¼ˆCham:
    Springer International Publishingï¼‰ï¼Œ818â€“833'
- en: Zeiler and Fergus (2014b) Zeiler, M.Â D. and Fergus, R. (2014b). Visualizing
    and understanding convolutional networks. In *European Conference on Computer
    Vision* (Springer), 818â€“833
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler å’Œ Fergusï¼ˆ2014bï¼‰Zeiler, M. D. å’Œ Fergus, R.ï¼ˆ2014bï¼‰ã€‚å¯è§†åŒ–ä¸ç†è§£å·ç§¯ç½‘ç»œã€‚è§ *European
    Conference on Computer Vision*ï¼ˆSpringerï¼‰ï¼Œ818â€“833
- en: Zou etÂ al. (2013) Zou, W.Â Y., Socher, R., Cer, D.Â M., and Manning, C.Â D. (2013).
    Bilingual word embeddings for phrase-based machine translation. In *EMNLP*. 1393â€“1398
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou ç­‰äººï¼ˆ2013ï¼‰Zou, W. Y., Socher, R., Cer, D. M., å’Œ Manning, C. D.ï¼ˆ2013ï¼‰ã€‚åŒè¯­è¯åµŒå…¥ç”¨äºåŸºäºçŸ­è¯­çš„æœºå™¨ç¿»è¯‘ã€‚è§
    *EMNLP*ï¼Œ1393â€“1398
