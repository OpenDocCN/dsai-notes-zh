- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:09:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:09:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1702.00764] Symbolic, Distributed and Distributional Representations for Natural
    Language Processing in the Era of Deep Learning: a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1702.00764] 符号、分布式和分布式表示在深度学习时代的自然语言处理中的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1702.00764](https://ar5iv.labs.arxiv.org/html/1702.00764)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1702.00764](https://ar5iv.labs.arxiv.org/html/1702.00764)
- en: \correspondance\extraAuth
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \correspondance\extraAuth
- en: 'Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 符号、分布式和分布式表示在深度学习时代的自然语言处理中的调查
- en: Lorenzo Ferrone ¹ and Fabio Massimo Zanzotto ^(1,∗)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 洛伦佐·费罗内¹ 和 法比奥·马西莫·赞佐托^(1,∗)
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: '1'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '1'
- en: 'Natural language is inherently a discrete symbolic representation of human
    knowledge. Recent advances in machine learning (ML) and in natural language processing
    (NLP) seem to contradict the above intuition: discrete symbols are fading away,
    erased by vectors or tensors called *distributed* and *distributional representations*.
    However, there is a strict link between distributed/distributional representations
    and discrete symbols, being the first an approximation of the second. A clearer
    understanding of the strict link between distributed/distributional representations
    and symbols may certainly lead to radically new deep learning networks. In this
    paper we make a survey that aims to renew the link between symbolic representations
    and distributed/distributional representations. This is the right time to revitalize
    the area of interpreting how discrete symbols are represented inside neural networks.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言本质上是人类知识的离散符号表示。最近在机器学习（ML）和自然语言处理（NLP）方面的进展似乎与上述直觉相矛盾：离散符号正在逐渐消失，被称为*分布式*和*分布式表示*的向量或张量所取代。然而，分布式/分布式表示和离散符号之间存在严格的联系，前者是后者的近似。对分布式/分布式表示和符号之间严格联系的更清晰理解，可能会导致全新深度学习网络的诞生。本文对符号表示和分布式/分布式表示之间的联系进行了综述。现在正是重振解释离散符号如何在神经网络内部表示的领域的最佳时机。
- en: \helveticabold
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \helveticabold
- en: '2 Keywords:'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 关键字：
- en: keyword, keyword, keyword, keyword, keyword, keyword, keyword, keyword
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关键字，关键字，关键字，关键字，关键字，关键字，关键字，关键字
- en: 3 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 引言
- en: Natural language is inherently a discrete symbolic representation of human knowledge.
    Sounds are transformed in letters or ideograms and these discrete symbols are
    composed to obtain words. Words then form sentences and sentences form texts,
    discourses, dialogs, which ultimately convey knowledge, emotions, and so on. This
    composition of symbols in words and of words in sentences follow rules that both
    the hearer and the speaker know (Chomsky, [1957](#bib.bib13)). Hence, thinking
    to natural language understanding systems, which are not based on discrete symbols,
    seems to be extremely odd.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言本质上是人类知识的离散符号表示。声音被转化为字母或表意符号，这些离散符号被组合以形成单词。单词然后组成句子，句子组成文本、演讲、对话，最终传达知识、情感等。这些符号在单词中的组合以及单词在句子中的组合遵循了听者和说话者都知道的规则（乔姆斯基，[1957](#bib.bib13)）。因此，考虑那些不基于离散符号的自然语言理解系统似乎极其奇怪。
- en: 'Recent advances in machine learning (ML) applied to natural language processing
    (NLP) seem to contradict the above intuition: discrete symbols are fading away,
    erased by vectors or tensors called *distributed* and *distributional representations*.
    In ML applied to NLP, *distributed representations* are pushing deep learning
    models (LeCun et al., [2015](#bib.bib44); Schmidhuber, [2015](#bib.bib62)) towards
    amazing results in many high-level tasks such as image generation (Goodfellow
    et al., [2014](#bib.bib29)), image captioning (Vinyals et al., [2015b](#bib.bib73);
    Xu et al., [2015](#bib.bib76)), machine translation (Bahdanau et al., [2014](#bib.bib3);
    Zou et al., [2013](#bib.bib82)), syntactic parsing (Vinyals et al., [2015a](#bib.bib72);
    Weiss et al., [2015](#bib.bib74)) and in a variety of other NLP tasks Devlin et al.
    ([2018](#bib.bib19)). In a more traditional NLP, *distributional representations*
    are pursued as a more flexible way to represent semantics of natural language,
    the so-called *distributional semantics* (see (Turney and Pantel, [2010](#bib.bib68))).
    Words as well as sentences are represented as vectors or tensors of real numbersVectors
    for words are obtained observing how rhese words co-occur with other words in
    document collections. Moreover, as in traditional compositional representations,
    vectors for phrases (Mitchell and Lapata, [2008](#bib.bib50); Baroni and Zamparelli,
    [2010](#bib.bib6); Clark et al., [2008](#bib.bib14); Grefenstette and Sadrzadeh,
    [2011](#bib.bib31); Zanzotto et al., [2010](#bib.bib79)) and sentences (Socher
    et al., [2011](#bib.bib64), [2012](#bib.bib65); Kalchbrenner and Blunsom, [2013](#bib.bib40))
    are obtained by composing vectors for words.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 近期在自然语言处理（NLP）中应用的机器学习（ML）进展似乎与上述直觉相矛盾：离散符号正在消退，被称为*分布式*和*分布表征*的向量或张量所取代。在应用于NLP的ML中，*分布式表征*推动深度学习模型（LeCun等，[2015](#bib.bib44)；Schmidhuber，[2015](#bib.bib62)）在许多高级任务中取得了惊人的结果，如图像生成（Goodfellow等，[2014](#bib.bib29)），图像描述（Vinyals等，[2015b](#bib.bib73)；Xu等，[2015](#bib.bib76)），机器翻译（Bahdanau等，[2014](#bib.bib3)；Zou等，[2013](#bib.bib82)），句法分析（Vinyals等，[2015a](#bib.bib72)；Weiss等，[2015](#bib.bib74)）以及其他各种NLP任务（Devlin等，[2018](#bib.bib19)）。在更传统的NLP中，*分布表征*被视为表示自然语言语义的更灵活方式，即所谓的*分布语义*（见（Turney和Pantel，[2010](#bib.bib68)））。单词和句子都被表示为实数的向量或张量。单词的向量通过观察这些单词在文档集合中与其他单词的共现情况来获得。此外，像传统组合表征一样，短语（Mitchell和Lapata，[2008](#bib.bib50)；Baroni和Zamparelli，[2010](#bib.bib6)；Clark等，[2008](#bib.bib14)；Grefenstette和Sadrzadeh，[2011](#bib.bib31)；Zanzotto等，[2010](#bib.bib79)）和句子的向量（Socher等，[2011](#bib.bib64)，[2012](#bib.bib65)；Kalchbrenner和Blunsom，[2013](#bib.bib40)）是通过组合单词的向量来获得的。
- en: The success of distributed and distributional representations over symbolic
    approaches is mainly due to the advent of new parallel paradigms that pushed neural
    networks (Rosenblatt, [1958](#bib.bib58); Werbos, [1974](#bib.bib75)) towards
    deep learning (LeCun et al., [2015](#bib.bib44); Schmidhuber, [2015](#bib.bib62)).
    Massively parallel algorithms running on Graphic Processing Units (GPUs) (Chetlur
    et al., [2014](#bib.bib12); Cui et al., [2015](#bib.bib16)) crunch vectors, matrices
    and tensors faster than decades ago. The back-propagation algorithm can be now
    computed for complex and large neural networks. Symbols are not needed any more
    during “resoning”, that is, the neural network learning and its application. Hence,
    discrete symbols only survive as inputs and outputs of these wonderful learning
    machines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式和分布表征在符号方法上的成功主要归因于新并行范式的出现，这推动了神经网络（Rosenblatt，[1958](#bib.bib58)；Werbos，[1974](#bib.bib75)）向深度学习（LeCun等，[2015](#bib.bib44)；Schmidhuber，[2015](#bib.bib62)）的发展。运行在图形处理单元（GPU）（Chetlur等，[2014](#bib.bib12)；Cui等，[2015](#bib.bib16)）上的大规模并行算法比几十年前更快地处理向量、矩阵和张量。现在可以为复杂的大型神经网络计算反向传播算法。在“推理”过程中，即神经网络的学习和应用中，已不再需要符号。因此，离散符号仅作为这些神奇学习机器的输入和输出而存在。
- en: However, there is a strict link between distributed/distributional representations
    and symbols, being the first an approximation of the second (Fodor and Pylyshyn,
    [1988](#bib.bib24); Plate, [1994](#bib.bib56), [1995](#bib.bib57); Ferrone et al.,
    [2015](#bib.bib21)). The representation of the input and the output of these networks
    is not that far from their internal representation. The similarity and the interpretation
    of the internal representation is clearer in image processing (Zeiler and Fergus,
    [2014a](#bib.bib80)). In fact, networks are generally interpreted visualizing
    how subparts represent salient subparts of target images. Both input images and
    subparts are tensors of real number. Hence, these networks can be examined and
    understood. The same does not apply to natural language processing with its discrete
    symbols.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分布式/分配式表示与符号之间存在严格的联系，第一个是对第二个的近似（Fodor 和 Pylyshyn，[1988](#bib.bib24)；Plate，[1994](#bib.bib56)，[1995](#bib.bib57)；Ferrone
    等，[2015](#bib.bib21)）。这些网络的输入和输出表示与其内部表示并没有太大差别。图像处理中的内部表示的相似性和解释更为清晰（Zeiler 和
    Fergus，[2014a](#bib.bib80)）。事实上，网络通常通过可视化子部分如何代表目标图像的显著子部分来进行解释。输入图像和子部分都是实数张量。因此，这些网络可以被检查和理解。自然语言处理中的离散符号则不适用这种情况。
- en: 'A clearer understanding of the strict link between distributed/distributional
    representations and discrete symbols is needed (Jang et al., [2018](#bib.bib38);
    Jacovi et al., [2018](#bib.bib37)) to understand how neural networks treat information
    and to propose novel deep learning architectures. Model interpretability is becoming
    an important topic in machine learning in general (Lipton, [2016](#bib.bib46)).
    This clearer understanding is then the dawn of a new range of possibilities: understanding
    what part of the current symbolic techniques for natural language processing have
    a sufficient representation in deep neural networks; and, ultimately, understanding
    whether a more brain-like model – the neural networks – is compatible with methods
    for syntactic parsing or semantic processing that have been defined in these decades
    of studies in computational linguistics and natural language processing. There
    is thus a tremendous opportunity to understand whether and how symbolic representations
    are used and emitted in a brain model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 需要对分布式/分配式表示与离散符号之间的严格联系有更清晰的理解（Jang 等，[2018](#bib.bib38)；Jacovi 等，[2018](#bib.bib37)），以理解神经网络如何处理信息，并提出新的深度学习架构。模型可解释性正成为机器学习中的一个重要话题（Lipton，[2016](#bib.bib46)）。这种更清晰的理解标志着新可能性的曙光：理解当前自然语言处理符号技术的哪些部分在深度神经网络中有足够的表示；以及，*最终*，理解更类似大脑的模型——神经网络——是否与这些几十年来在计算语言学和自然语言处理领域定义的句法解析或语义处理方法兼容。因此，了解符号表示是否以及如何在大脑模型中使用和发射是一个巨大的机会。
- en: In this paper we make a survey that aims to draw the link between symbolic representations
    and distributed/distributional representations. This is the right time to revitalize
    the area of interpreting how symbols are represented inside neural networks. In
    our opinion, this survey will help to devise new deep neural networks that can
    exploit existing and novel symbolic models of classical natural language processing
    tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们进行了一项调查，旨在建立符号表示与分布式/分配式表示之间的联系。现在正是振兴解释符号在神经网络内部如何表示的领域的最佳时机。在我们看来，这项调查将有助于设计能够利用现有和新型符号模型的深度神经网络，用于经典自然语言处理任务。
- en: 'The paper is structured as follow: first we give an introduction to the very
    general concept of representations and the difference between *local* and *distributed*
    representations (Plate, [1995](#bib.bib57)). After that we present each techniques
    in detail. Afterwards, we focus on distributional representations (Turney and
    Pantel, [2010](#bib.bib68)), which we treat as a specific example of a distributed
    representation. Finally we discuss more in depth the general issue of compositionality,
    analyzing three different approaches to the problem: compositional distributional
    semantics (Clark et al., [2008](#bib.bib14); Baroni et al., [2014](#bib.bib4)),
    holographic reduced representations (Plate, [1994](#bib.bib56); Neumann, [2001](#bib.bib53)),
    and recurrent neural networks (Kalchbrenner and Blunsom, [2013](#bib.bib40); Socher
    et al., [2012](#bib.bib65)).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的结构如下：首先，我们介绍表示的非常一般的概念以及 *局部* 和 *分布式* 表示之间的区别（Plate，[1995](#bib.bib57)）。然后我们详细介绍每种技术。接下来，我们重点讨论分布式表示（Turney
    和 Pantel，[2010](#bib.bib68)），我们将其视为分布式表示的一个特例。最后，我们深入讨论组合性的总体问题，分析了三种不同的解决方法：组合分布式语义（Clark
    等，[2008](#bib.bib14)；Baroni 等，[2014](#bib.bib4)），全息减少表示（Plate，[1994](#bib.bib56)；Neumann，[2001](#bib.bib53)），以及递归神经网络（Kalchbrenner
    和 Blunsom，[2013](#bib.bib40)；Socher 等，[2012](#bib.bib65)）。
- en: '4 Symbolic and Distributed Representations: Interpretability and *Concatenative*
    Compositionality'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号与分布式表示：可解释性和 *连接* 组合性
- en: '*Distributed representations* put symbolic expressions in metric spaces where
    similarity among examples is used to learn regularities for specific tasks by
    using neural networks or other machine learning models. Given two symbolic expressions,
    their distributed representation should capture their similarity along specific
    features useful for the final task. For example, two sentences such as $s_{1}$=*“a
    mouse eats some cheese”* and $s_{2}$=*“a cat swallows a mouse”* can be considered
    similar in many different ways: (1) number of words in common; (2) realization
    of the pattern “ANIMAL EATS FOOD”. The key point is to decide or to let an algorithm
    decide which is the best representation for a specific task.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式表示* 将符号表达放置在度量空间中，通过使用神经网络或其他机器学习模型，利用示例之间的相似性来学习特定任务的规律。给定两个符号表达，它们的分布式表示应捕捉它们在对最终任务有用的特定特征上的相似性。例如，两个句子如
    $s_{1}$=*“一只老鼠吃了一些奶酪”* 和 $s_{2}$=*“一只猫吞下了一只老鼠”* 可以从许多不同的方面看作是相似的：（1）共有的单词数量；（2）模式
    “动物 吃 食物” 的实现。关键在于决定或让算法决定哪个是特定任务的最佳表示。'
- en: '*Distributed representations* are then replacing long-lasting, successful *discrete
    symbolic representations* in representing knowledge for learning machines but
    these representations are less human *interpretable*. Hence, discussing about
    basic, obvious properties of *discrete symbolic representations* is not useless
    as these properties may guarantee success to distributed representations similar
    to the one of discrete symbolic representations.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式表示* 正在取代长期存在且成功的 *离散符号表示*，用于学习机器的知识表示，但这些表示对人类的 *可解释性* 较差。因此，讨论 *离散符号表示*
    的基本明显属性并非毫无意义，因为这些属性可能会对分布式表示的成功提供保证，类似于离散符号表示。'
- en: Discrete symbolic representations are human *interpretable* as *symbols are
    not altered in expressions*. This is one of the most important, obvious feature
    of these representations. Infinite sets of expressions, which are sequences of
    symbols, can be *interpreted* as these expressions are obtained by concatenating
    a finite set of basic symbols according to some concatenative rules. During concatenation,
    symbols are not altered and, then, can be recognized. By using the principle of
    *semantic compositionality*, the meaning of expressions can be obtained by combining
    the meaning of the parts and, hence, recursively, by combining the meaning of
    the finite set of basic symbols. For example, given the set of basic symbols $\mathcal{D}$
    = {*mouse*,*cat*,*a*,*swallows*,*(*,*)*}, expressions like $s_{1}$=*“a cat swallows
    a mouse”* or $t_{1}$=*((a cat) (swallows (a mouse)))* are totally plausible and
    interpretable given rules for producing natural language utterances or for producing
    tree structured representations in parenthetical form, respectively. This strongly
    depends on the fact that individual symbols can be recognized.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 离散符号表示对人类来说*易于解释*，因为*符号在表达式中未被改变*。这是这些表示的一个最重要、最明显的特征。无限的表达式集合，即符号的序列，可以*被解释*为这些表达式是通过按照某些串联规则将有限的基本符号集合连接而成的。在串联过程中，符号不会被改变，从而可以被识别。利用*语义组合性*原理，可以通过组合部分的意义来获得表达式的意义，从而递归地通过组合有限的基本符号的意义来获得。例如，给定基本符号集$\mathcal{D}$
    = {*mouse*,*cat*,*a*,*swallows*,*(*,*)*}，表达式如$s_{1}$=*“一只猫吞食了一只老鼠”*或$t_{1}$=*((一只猫)
    (吞食 (一只老鼠)))*是完全合理且可解释的，前提是有用于生成自然语言发言或生成括号形式的树结构表示的规则。这强烈依赖于个别符号能够被识别的事实。
- en: 'Distributed representations instead seem to *alter symbols* when applied to
    symbolic inputs and, thus, are less interpretable. In fact, symbols as well as
    expressions are represented as vectors in these metric spaces. Observing distributed
    representations, symbols and expressions do not immediately emerge. Moreover,
    these distributed representations may be transformed by using matrix multiplication
    or by using non-linear functions. Hence, it is generally unclear: (1) what is
    the relation between the initial symbols or expressions and their distributed
    representations and (2) how these expressions are manipulated during matrix multiplication
    or when applying non-linear functions. In other words, it is unclear whether symbols
    can be recognized in distributed representations.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式表示在应用于符号输入时似乎会*改变符号*，因此，这些表示较难解释。事实上，符号以及表达式在这些度量空间中被表示为向量。观察分布式表示时，符号和表达式不会立即显现。此外，这些分布式表示可能通过矩阵乘法或非线性函数被转换。因此，一般来说，以下问题不明确：（1）初始符号或表达式与其分布式表示之间的关系是什么，以及（2）这些表达式在矩阵乘法或应用非线性函数过程中如何被操作。换句话说，尚不清楚符号是否可以在分布式表示中被识别。
- en: Hence, a debated question is whether discrete symbolic representations and distributed
    representations are two very different ways of encoding knowledge because of the
    difference in *alterning symbols*. The debate dates back in the late 80s. For
    Fodor and Pylyshyn ([1988](#bib.bib24)), distributed representations in Neural
    Network architectures are *“only an implementation of the Classical approach”*
    where classical approach is related to discrete symbolic representations. Whereas,
    for Chalmers ([1992](#bib.bib11)), distributed representations give the important
    opportunity to reason *“holistically”* about encoded knowledge. This means that
    decisions over some specific part of the stored knowledge can be taken without
    retrieving the specific part but acting on the whole representation. However,
    this does not solve the debated question as it is still unclear what is in a distributed
    representation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个有争议的问题是离散符号表示和分布式表示是否由于*符号的改变*的不同而是两种截然不同的知识编码方式。这场辩论可以追溯到80年代末。对于Fodor和Pylyshyn（[1988](#bib.bib24)），神经网络架构中的分布式表示是*“经典方法的仅仅一种实现”*，而经典方法与离散符号表示相关。相比之下，对于Chalmers（[1992](#bib.bib11)），分布式表示提供了*“整体性”*地推理编码知识的重要机会。这意味着可以在不检索特定部分的情况下对存储知识的某些特定部分做出决定，而是对整个表示进行操作。然而，这并没有解决争议的问题，因为在分布式表示中到底包含了什么仍不清楚。
- en: 'To contribute to the above debated question, Gelder ([1990](#bib.bib27)) has
    formalized the property of *altering symbols in expressions* by defining two different
    notions of compositionality: *concatentative* compositionality and *functional*
    compositionality. *Concatenative compositionality* explains how discrete symbolic
    representations compose symbols to obtain expressions. In fact, the mode of combination
    is an extended concept of juxtaposition that provides a way of linking successive
    symbols without altering them as these form expressions. Concatenative compositionality
    explains discrete symbolic representations no matter the means is used to store
    expressions: a piece of paper or a computer memory. Concatenation is sometime
    expressed with an operator like $\circ$, which can be used in a infix or prefix
    notation, that is a sort of function with arguments $\circ(w_{1},...,w_{n})$.
    By using the operator for concatenation, the two above examples $s_{1}$ and $t_{1}$
    can be represented as the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对上述争论问题做出贡献，Gelder ([1990](#bib.bib27)) 通过定义两种不同的组合性概念：*连接性*组合性和*函数性*组合性，形式化了*改变表达式中的符号*的属性。*连接性组合性*解释了离散符号表示如何组合符号以获得表达式。实际上，这种组合模式是并排的扩展概念，它提供了一种在不改变符号的情况下将连续符号连接在一起的方式。连接性组合性解释了离散符号表示，无论存储表达式的方式是纸张还是计算机内存。连接有时用一个如
    $\circ$ 的运算符表示，该运算符可以用于中缀或前缀表示法，即一种带有参数的函数 $\circ(w_{1},...,w_{n})$。使用连接运算符，上述两个示例
    $s_{1}$ 和 $t_{1}$ 可以表示为：
- en: '|  | $a\circ cat\circ swallows\circ a\circ mouse$ |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $a\circ cat\circ swallows\circ a\circ mouse$ |  |'
- en: that represents a sequence with the infix notation and
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了中缀表示法中的一个序列和
- en: '|  | $\circ(\circ(a,cat),\circ(swallows,\circ(a,mouse)))$ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\circ(\circ(a,cat),\circ(swallows,\circ(a,mouse)))$ |  |'
- en: that represents a tree with the prefix notation. *Functional compositionality*
    explains distributed representations. In functional compositionality, the mode
    of combination is a function $\Phi$ that gives a reliable, general process for
    producing expressions given its constituents. Within this perspective, semantic
    compositionality is a special case of functional compositionality where the target
    of the composition is a way to represent meaning (Blutner et al., [2003](#bib.bib10)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了前缀表示法中的一个树。*函数性组合性*解释了分布式表示。在函数性组合性中，组合模式是一个函数 $\Phi$，它提供了一个可靠的、通用的过程来生成给定组成部分的表达式。在这种视角下，语义组合性是函数性组合性的一个特例，其中组合的目标是表示意义的方式
    (Blutner et al., [2003](#bib.bib10))。
- en: '*Local distributed representations* (as referred in (Plate, [1995](#bib.bib57)))
    or *one-hot encodings* are the easiest way to visualize how *functional compositionality*
    act on *distributed representations*. Local distributed representations give a
    first, simple encoding of discrete symbolic representations in a metric space.
    Given a set of symbols $\mathcal{D}$, a local distributed epresentation maps the
    $i$-th symbol in $\mathcal{D}$ to the $i$-th base unit vector $\mathbf{e}_{i}$
    in $\mathbb{R}^{n}$, where $n$ is the cardinality of $\mathcal{D}$. Hence, the
    $i$-th unit vector represents the $i$-th symbol. In *functional compositionality*,
    expressions $s=w_{1}\ldots w_{k}$ are represented by vectors $\mathbf{s}$ obtained
    with an eventually recursive function $\Phi$ applied to vectors $\mathbf{e}_{w_{1}}\ldots\mathbf{e}_{w_{k}}$.
    The function $f$ may be very simple as the sum or more complex. In case the function
    $\Phi$ is the sum, that is:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*局部分布式表示*（如（Plate, [1995](#bib.bib57)) 所提到的）或 *独热编码* 是可视化*函数性组合性*如何作用于*分布式表示*的最简单方式。局部分布式表示在度量空间中对离散符号表示进行初步简单编码。给定一个符号集合
    $\mathcal{D}$，局部分布式表示将 $\mathcal{D}$ 中第 $i$ 个符号映射到 $\mathbb{R}^{n}$ 中第 $i$ 个基单位向量
    $\mathbf{e}_{i}$，其中 $n$ 是 $\mathcal{D}$ 的基数。因此，第 $i$ 个单位向量表示第 $i$ 个符号。在*函数性组合性*中，表达式
    $s=w_{1}\ldots w_{k}$ 由向量 $\mathbf{s}$ 表示，该向量是应用于向量 $\mathbf{e}_{w_{1}}\ldots\mathbf{e}_{w_{k}}$
    的一个递归函数 $\Phi$ 获得的。函数 $f$ 可以非常简单，如求和，或者更复杂。如果函数 $\Phi$ 是求和，即：'
- en: '|  | $\mathbf{func}_{\Sigma}(s)=\sum_{j=1}^{k}\mathbf{e}_{w_{j}}$ |  | (1)
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{func}_{\Sigma}(s)=\sum_{j=1}^{k}\mathbf{e}_{w_{j}}$ |  | (1)
    |'
- en: 'the derived vector is the classical bag-of-word vector space model (Salton,
    [1989](#bib.bib61)). Whereas, more complex functions $f$ can range from different
    vector-to-vector operations like circular convolution in Holographic Reduced Representations
    (Plate, [1995](#bib.bib57)) to matrix multiplications plus non linear operations
    in models such as in recurrent neural networks (Schuster and Paliwal, [1997](#bib.bib63);
    Hochreiter and Schmidhuber, [1997](#bib.bib36)) or in neural networks with attention
    (Vaswani et al., [2017](#bib.bib69); Devlin et al., [2018](#bib.bib19)). The above
    example can be useful to describe *concatenative* and *functional* compositionality.
    The set $\mathcal{D}$= {*mouse*,*cat*,*a*,*swallows*,*eats*,*some*,*cheese*,*(*,*)*}
    may be represented with the base vectors $\mathbf{e}_{i}\in\mathbb{R}^{9}$ where
    $\mathbf{e}_{1}$ is the base vector for *mouse*, $\mathbf{e}_{2}$ for *cat*, $\mathbf{e}_{3}$
    for *a*, $\mathbf{e}_{4}$ for *swallaws*, $\mathbf{e}_{5}$ for *eats*, $\mathbf{e}_{6}$
    for *some*, $\mathbf{e}_{7}$ for *cheese*, $\mathbf{e}_{8}$ for *(*, and $\mathbf{e}_{9}$
    for *)*. The additive functional composition of the expression $s_{1}$=*a cat
    swallows a mouse* is then:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所得到的向量是经典的词袋向量空间模型（Salton, [1989](#bib.bib61)）。而更复杂的函数 $f$ 可以涉及从不同的向量到向量操作，比如全息压缩表示中的循环卷积（Plate,
    [1995](#bib.bib57)），到在如递归神经网络（Schuster 和 Paliwal, [1997](#bib.bib63)；Hochreiter
    和 Schmidhuber, [1997](#bib.bib36)）中的矩阵乘法加非线性操作，或在具有注意力机制的神经网络中（Vaswani 等, [2017](#bib.bib69)；Devlin
    等, [2018](#bib.bib19)）。上述示例可以用于描述 *连接性* 和 *功能性* 组合性。集合 $\mathcal{D}$= {*mouse*,*cat*,*a*,*swallows*,*eats*,*some*,*cheese*,*(*,*)*}
    可以用基向量 $\mathbf{e}_{i}\in\mathbb{R}^{9}$ 表示，其中 $\mathbf{e}_{1}$ 是 *mouse* 的基向量，$\mathbf{e}_{2}$
    是 *cat* 的基向量，$\mathbf{e}_{3}$ 是 *a* 的基向量，$\mathbf{e}_{4}$ 是 *swallows* 的基向量，$\mathbf{e}_{5}$
    是 *eats* 的基向量，$\mathbf{e}_{6}$ 是 *some* 的基向量，$\mathbf{e}_{7}$ 是 *cheese* 的基向量，$\mathbf{e}_{8}$
    是 *(* 的基向量，$\mathbf{e}_{9}$ 是 *)* 的基向量。表达式 $s_{1}$=*a cat swallows a mouse* 的加性功能组合为：
- en: '| *expression in $e_{i}$* | *additive functional composition* |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| *在 $e_{i}$ 中的表达式* | *加性功能组合* |'
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; a &#124; cat &#124; swallows &#124; a &#124; mouse &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; a &#124; cat &#124; swallows &#124; a &#124; mouse &#124; '
- en: '&#124; <math   alttext="\begin{pmatrix}0\\ 0\\'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; <math   alttext="\begin{pmatrix}0\\ 0\\'
- en: 1\\
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 1\\
- en: 0\\
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}0\\ 0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124; <math   alttext="\begin{pmatrix}0\\
    1\\
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.31.2.2.1.1.1.
- en: 0\\
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >1</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124; <math   alttext="\begin{pmatrix}0\\
    0\\
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.32.3.3.2.2.2.
- en: 0\\
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 1\\
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 1\\
- en: 0\\
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >1</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}0\\ 0\\ 0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124; <math   alttext="\begin{pmatrix}0\\
    0\\
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.33.4.4.3.3.3.
- en: 1\\
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 1\\
- en: 0\\
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}0\\ 0\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124; <math   alttext="\begin{pmatrix}1\\
    0\\
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.34.5.5.4.4.4.
- en: 0\\
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow ><cn type="integer"
    >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}1\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\ 0\\
    \end{pmatrix}</annotation></semantics></math> &#124;
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.35.6.6.5.5.5.
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathbf{e_{3}}+\mathbf{e_{2}}+\mathbf{e_{4}}+\mathbf{e_{3}}+\mathbf{e_{1}}$
    &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathbf{e_{3}}+\mathbf{e_{2}}+\mathbf{e_{4}}+\mathbf{e_{3}}+\mathbf{e_{1}}$
    &#124;'
- en: '&#124; <math   alttext="\mathbf{func_{\Sigma}(s_{1})}=\begin{pmatrix}1\\ 1\\'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; <math   alttext="\mathbf{func_{\Sigma}(s_{1})}=\begin{pmatrix}1\\ 1\\'
- en: 2\\
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2\\
- en: 1\\
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 1\\
- en: 0\\
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\\
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 0\\
- en: 0\end{pmatrix}" display="inline"><semantics ><mrow ><mrow ><msub ><mi mathsize="50%"  >𝐟𝐮𝐧𝐜</mi><mi
    mathsize="50%"  >𝚺</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    maxsize="50%" minsize="50%"  >(</mo><msub ><mi mathsize="50%"  >𝐬</mi><mn mathsize="50%"  >𝟏</mn></msub><mo
    maxsize="50%" minsize="50%"  >)</mo></mrow></mrow><mo mathsize="50%"  >=</mo><mrow
    ><mo >(</mo><mtable rowspacing="0pt"  ><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >2</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  >0</mn></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐟𝐮𝐧𝐜</ci><ci >𝚺</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐬</ci><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="latexml" >matrix</csymbol><matrix ><matrixrow ><cn type="integer"
    >1</cn></matrixrow><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow
    ><cn type="integer" >2</cn></matrixrow><matrixrow ><cn type="integer" >1</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{func_{\Sigma}(s_{1})}=\begin{pmatrix}1\\
    1\\ 2\\ 1\\ 0\\ 0\\ 0\\ 0\\ 0\end{pmatrix}</annotation></semantics></math> &#124;
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 0\end{pmatrix}" display="inline"><semantics ><mrow ><mrow ><msub ><mi mathsize="50%"  >𝐟𝐮𝐧𝐜</mi><mi
    mathsize="50%"  >𝚺</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    maxsize="50%" minsize="50%" >(</mo><msub ><mi mathsize="50%" >𝐬</mi><mn mathsize="50%"
    >𝟏</mn></msub><mo maxsize="50%" minsize="50%" >)</mo></mrow></mrow><mo mathsize="50%"  >=</mo><mrow
    ><mo >(</mo><mtable rowspacing="0pt"  ><mtr ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr
    ><mtd ><mn mathsize="50%"  >1</mn></mtd></mtr><mtr ><mtd ><mn mathsize="50%"  xref="S4.p7.37.8.8.7.2.2.1.m1.1.1.1.1
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: where the concatenative operator $\circ$ has been substituted with the sum $+$.
    Just to observe, in the additive functional composition $\mathbf{func_{\Sigma}(s_{1})}$,
    symbols are still visible but the sequence is lost. Hence, it is difficult to
    reproduce the initial discrete symbolic expression. However, for example, the
    additive composition function gives the possibility to compare two expressions.
    Given the expression $s_{1}$ and $s_{2}$=*a mouse eats some cheese*, the dot product
    between $\mathbf{func_{\Sigma}(s_{1})}$ and $\mathbf{func_{\Sigma}(s_{2})}=\begin{pmatrix}1&amp;0&amp;1&amp;0&amp;1&amp;1&amp;1&amp;0&amp;0\end{pmatrix}^{T}$
    counts the common words between the two expressions. In a functional composition
    with a function $\Phi$, the expression $s_{1}$ may become $\mathbf{func_{\Phi}(s_{1})}=\Phi(\Phi(\Phi(\Phi(\mathbf{e_{3}},\mathbf{e_{2}}),\mathbf{e_{4}}),\mathbf{e_{3}}),\mathbf{e_{1}})$
    by following the concatenative compositionality of the discrete symbolic expression.
    The same functional compositional principle can be applied to discrete symbolic
    trees as $t_{1}$ by producing this distributed representation $\Phi(\Phi(\mathbf{e_{3}},\mathbf{e_{2}}),\Phi(\mathbf{e_{4}},\Phi(\mathbf{e_{3}},\mathbf{e_{1}})))$.
    Finally, in the functional composition with a generic recursive function $\mathbf{func_{\Phi}(s_{1})}$,
    the function $\Phi$ will be crucial to determine whether symbols can be recognized
    and sequence is preserved.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中连接操作符 $\circ$ 被加法 $+$ 替代。值得注意的是，在加法功能组合 $\mathbf{func_{\Sigma}(s_{1})}$ 中，符号仍然可见但序列丢失。因此，重现初始的离散符号表达式变得困难。然而，例如，加法组合函数提供了比较两个表达式的可能性。给定表达式
    $s_{1}$ 和 $s_{2}$=*一只老鼠吃了一些奶酪*，$\mathbf{func_{\Sigma}(s_{1})}$ 和 $\mathbf{func_{\Sigma}(s_{2})}=\begin{pmatrix}1&amp;0&amp;1&amp;0&amp;1&amp;1&amp;1&amp;0&amp;0\end{pmatrix}^{T}$
    之间的点积计算了两个表达式之间的共同单词。在带有函数 $\Phi$ 的功能组合中，表达式 $s_{1}$ 可能会变成 $\mathbf{func_{\Phi}(s_{1})}=\Phi(\Phi(\Phi(\Phi(\mathbf{e_{3}},\mathbf{e_{2}}),\mathbf{e_{4}}),\mathbf{e_{3}}),\mathbf{e_{1}})$，这是通过遵循离散符号表达式的连接性组合原理实现的。相同的功能组合原理可以应用于离散符号树，如
    $t_{1}$，通过生成这个分布式表示 $\Phi(\Phi(\mathbf{e_{3}},\mathbf{e_{2}}),\Phi(\mathbf{e_{4}},\Phi(\mathbf{e_{3}},\mathbf{e_{1}})))$。最后，在带有通用递归函数
    $\mathbf{func_{\Phi}(s_{1})}$ 的功能组合中，函数 $\Phi$ 将对确定符号是否可以被识别以及序列是否得以保留至关重要。
- en: '*Distributed representations* in their general form are more ambitious than
    distributed *local* representations and tend to encode basic symbols of $\mathcal{D}$
    in vectors in $\mathbb{R}^{d}$ where $d<<n$. These vectors generally alter symbols
    as there is not a direct link between symbols and dimensions of the space. Given
    a distributed local representation $\mathbf{e}_{w}$ of a symbol $w$, the encoder
    for a distributed representation is a matrix $\mathbf{W_{d\times n}}$ that transforms
    $\mathbf{x}_{w}$ in $\mathbf{y}_{w}=\mathbf{W_{d\times n}}\mathbf{e}_{w}$. As
    an example, the encoding matrix $\mathbf{W_{d\times n}}$ can be build by modeling
    words in $\mathcal{D}$ around three dimensions: number of vowels, number of consonants
    and, finally, number of non-alphabetic symbols. Given these dimensions, the matrix
    $\mathbf{W_{3\times 9}}$ for the example is :'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式表示* 在其一般形式上比分布式*局部*表示更具雄心，往往将 $\mathcal{D}$ 的基本符号编码在 $\mathbb{R}^{d}$ 中的向量中，其中
    $d<<n$。这些向量通常会改变符号，因为符号与空间维度之间没有直接的联系。给定符号 $w$ 的分布式局部表示 $\mathbf{e}_{w}$，分布式表示的编码器是一个矩阵
    $\mathbf{W_{d\times n}}$，它将 $\mathbf{x}_{w}$ 转换为 $\mathbf{y}_{w}=\mathbf{W_{d\times
    n}}\mathbf{e}_{w}$。例如，编码矩阵 $\mathbf{W_{d\times n}}$ 可以通过在三个维度上建模 $\mathcal{D}$
    中的单词来构建：元音数、辅音数，最后是非字母符号数。给定这些维度，示例中的矩阵 $\mathbf{W_{3\times 9}}$ 为：'
- en: '|  | <math   alttext="\mathbf{W_{3\times 9}}=\begin{pmatrix}3&amp;1&amp;1&amp;2&amp;2&amp;2&amp;3&amp;0&amp;0\\
    2&amp;2&amp;0&amp;6&amp;2&amp;2&amp;3&amp;0&amp;0\\'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{W_{3\times 9}}=\begin{pmatrix}3&amp;1&amp;1&amp;2&amp;2&amp;2&amp;3&amp;0&amp;0\\
    2&amp;2&amp;0&amp;6&amp;2&amp;2&amp;3&amp;0&amp;0\\'
- en: 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1\\
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;1\\
- en: \end{pmatrix}" display="block"><semantics ><mrow ><msub  ><mi >𝐖</mi><mrow ><mn  >𝟑</mn><mo
    lspace="0.222em" rspace="0.222em"  >×</mo><mn >𝟗</mn></mrow></msub><mo >=</mo><mrow
    ><mo  >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mn >3</mn></mtd><mtd  ><mn >1</mn></mtd><mtd ><mn >1</mn></mtd><mtd  ><mn
    >2</mn></mtd><mtd ><mn >2</mn></mtd><mtd  ><mn >2</mn></mtd><mtd ><mn >3</mn></mtd><mtd  ><mn
    >0</mn></mtd><mtd ><mn >0</mn></mtd></mtr><mtr ><mtd  ><mn >2</mn></mtd><mtd ><mn
    >2</mn></mtd><mtd  ><mn >0</mn></mtd><mtd ><mn >6</mn></mtd><mtd  ><mn >2</mn></mtd><mtd
    ><mn >2</mn></mtd><mtd  ><mn >3</mn></mtd><mtd ><mn >0</mn></mtd><mtd  ><mn >0</mn></mtd></mtr><mtr
    ><mtd ><mn  >0</mn></mtd><mtd ><mn >0</mn></mtd><mtd  ><mn >0</mn></mtd><mtd ><mn
    >0</mn></mtd><mtd  ><mn >0</mn></mtd><mtd ><mn >0</mn></mtd><mtd ><mn  >0</mn></mtd><mtd
    ><mn >1</mn></mtd><mtd ><mn  >1</mn></mtd></mtr></mtable><mo >)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝐖</ci><apply
    ><cn type="integer" >3</cn><cn type="integer" >9</cn></apply></apply><apply ><csymbol
    cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><cn type="integer" >3</cn><cn
    type="integer" >1</cn><cn type="integer" >1</cn><cn type="integer" >2</cn><cn
    type="integer" >2</cn><cn type="integer" >2</cn><cn type="integer" >3</cn><cn
    type="integer" >0</cn><cn type="integer" >0</cn></matrixrow><matrixrow ><cn type="integer"
    >2</cn><cn type="integer" >2</cn><cn type="integer" >0</cn><cn type="integer"
    >6</cn><cn type="integer" >2</cn><cn type="integer" >2</cn><cn type="integer"
    >3</cn><cn type="integer" >0</cn><cn type="integer" >0</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn><cn type="integer" >0</cn><cn type="integer" >0</cn><cn
    type="integer" >0</cn><cn type="integer" >0</cn><cn type="integer" >0</cn><cn
    type="integer" >0</cn><cn type="integer" >1</cn><cn type="integer" >1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{W_{3\times 9}}=\begin{pmatrix}3&1&1&2&2&2&3&0&0\\
    2&2&0&6&2&2&3&0&0\\ 0&0&0&0&0&0&0&1&1\\ \end{pmatrix}</annotation></semantics></math>
    |  |
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}" display="block"><semantics ><mrow ><msub  ><mi >𝐖</mi><mrow ><mn  >𝟑</mn><mo
    lspace="0.222em" rspace="0.222em"  >×</mo><mn >𝟗</mn></mrow></msub><mo >=</mo><mrow
    ><mo  >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mn >3</mn></mtd><mtd  ><mn >1</mn></mtd><mtd ><mn >1</mn></mtd><mtd  ><mn
    >2</mn></mtd><mtd ><mn >2</mn></mtd><mtd  ><mn >2</mn></mtd><mtd ><mn >3</mn></mtd><mtd  ><mn
    >0</mn></mtd><mtd ><mn >0</mn></mtd></mtr><mtr ><mtd  ><mn >2</mn></mtd><mtd ><mn
    >2</mn></mtd><mtd  ><mn >0</mn></mtd><mtd ><mn >6</mn></mtd><mtd  ><mn >2</mn></mtd><mtd
    ><mn >2</mn></mtd><m
- en: 'This is a simple example of a *distributed* representation. In a distributed
    representation (Plate, [1995](#bib.bib57); Hinton et al., [1986](#bib.bib35))
    the informational content is distributed (hence the name) among multiple units,
    and at the same time each unit can contribute to the representation of multiple
    elements. Distributed representation has two evident advantages with respect to
    a distributed local representation: it is more efficient (in the example, the
    representation uses only 3 numbers instead of 9) and it does not treat each element
    as being equally different to any other. In fact, *mouse* and *cat* in this representation
    are more similar than *mouse* and *a*. In other words, this representation captures
    by construction something interesting about the set of symbols. The drawback is
    that symbols are altered and, hence, it may be difficult to interpret which symbol
    is given its distributed representation. In the example, the distributed representations
    for *eats* and *some* are exactly the same vector $\mathbf{W_{3\times 9}}\mathbf{e_{5}}=\mathbf{W_{3\times
    9}}\mathbf{e_{6}}$.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个*分布式*表示的简单示例。在分布式表示（Plate, [1995](#bib.bib57); Hinton 等, [1986](#bib.bib35)）中，信息内容在多个单元之间分布（因此得名），同时每个单元可以对多个元素的表示做出贡献。与分布式局部表示相比，分布式表示有两个明显的优势：它更高效（在这个示例中，表示只使用了3个数字而不是9个）且它不会将每个元素视为与其他元素同样不同。实际上，在这种表示中，*鼠标*和*猫*比*鼠标*和*a*更相似。换句话说，这种表示通过构造捕捉了关于符号集合的一些有趣的东西。缺点是符号被改变，因此，可能很难解释哪些符号是给定的分布式表示。在这个例子中，*吃*和*一些*的分布式表示是完全相同的向量
    $\mathbf{W_{3\times 9}}\mathbf{e_{5}}=\mathbf{W_{3\times 9}}\mathbf{e_{6}}$。
- en: 'Even for distributed representations in the general form, it is possible to
    define *concatenative composition* and *functional composition* to represent expressions.
    Vectors $\mathbf{W_{d\times n}}\mathbf{e_{i}}$ should be replaced to vectors $\mathbf{e_{i}}$
    in the definition of the concatenative compositionality and the functional compositionality.
    Equation (LABEL:conc) is translated to:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是一般形式的分布式表示，也可以定义*连接性组合*和*功能组合*来表示表达式。在连接性组合性和功能组合性的定义中，向量 $\mathbf{W_{d\times
    n}}\mathbf{e_{i}}$ 应替换为向量 $\mathbf{e_{i}}$。方程 (LABEL:conc) 翻译为：
- en: '|  | $\mathbf{Y_{s}}=\mathbf{W_{d\times n}}\mathbf{conc(s)}=[\mathbf{W_{d\times
    n}}\mathbf{e}_{w_{1}}\ldots\mathbf{W_{d\times n}}\mathbf{e}_{w_{k}}]$ |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Y_{s}}=\mathbf{W_{d\times n}}\mathbf{conc(s)}=[\mathbf{W_{d\times
    n}}\mathbf{e}_{w_{1}}\ldots\mathbf{W_{d\times n}}\mathbf{e}_{w_{k}}]$ |  |'
- en: 'and Equation ([1](#S4.E1 "In 4 Symbolic and Distributed Representations: Interpretability
    and Concatenative Compositionality ‣ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey")) for additive functional compositionality becomes:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 ([1](#S4.E1 "在4中符号和分布式表示：可解释性和连接性组合 ‣ 符号、分布式和分布式表示在深度学习时代的自然语言处理：一项综述"))
    对于加法功能组合性变为：
- en: '|  | $\mathbf{y_{s}}=\mathbf{W_{d\times n}}\mathbf{func}_{\Sigma}(s)=\sum_{j=1}^{k}\mathbf{W_{d\times
    n}}\mathbf{e}_{j}$ |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{y_{s}}=\mathbf{W_{d\times n}}\mathbf{func}_{\Sigma}(s)=\sum_{j=1}^{k}\mathbf{W_{d\times
    n}}\mathbf{e}_{j}$ |  |'
- en: 'In the running example, the additive functional compositionality of sentence
    $s_{1}$ is:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行示例中，句子 $s_{1}$ 的加法功能组合性是：
- en: '|  | <math   alttext="\mathbf{y_{s_{1}}}=\mathbf{W_{3\times 9}}\mathbf{func}_{\Sigma}(s_{1})=\begin{pmatrix}8\\
    12\\'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{y_{s_{1}}}=\mathbf{W_{3\times 9}}\mathbf{func}_{\Sigma}(s_{1})=\begin{pmatrix}8\\
    12\\'
- en: 0\end{pmatrix}" display="block"><semantics ><mrow ><msub  ><mi >𝐲</mi><msub
    ><mi  >𝐬</mi><mn >𝟏</mn></msub></msub><mo >=</mo><mrow ><msub  ><mi >𝐖</mi><mrow
    ><mn  >𝟑</mn><mo lspace="0.222em" rspace="0.222em"  >×</mo><mn >𝟗</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >𝐟𝐮𝐧𝐜</mi><mi mathvariant="normal"
    >Σ</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow  ><mo stretchy="false"  >(</mo><msub
    ><mi >s</mi><mn  >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo  >(</mo><mtable displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd ><mn  >8</mn></mtd></mtr><mtr ><mtd ><mn  >12</mn></mtd></mtr><mtr ><mtd
    ><mn  >0</mn></mtd></mtr></mtable><mo >)</mo></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐲</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐬</ci><cn type="integer" >1</cn></apply></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐖</ci><apply ><cn
    type="integer" >3</cn><cn type="integer" >9</cn></apply></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐟𝐮𝐧𝐜</ci><ci  >Σ</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><cn type="integer" >1</cn></apply></apply></apply><apply
    ><apply ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow ><cn type="integer"
    >8</cn></matrixrow><matrixrow ><cn type="integer" >12</cn></matrixrow><matrixrow
    ><cn type="integer" >0</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{y_{s_{1}}}=\mathbf{W_{3\times 9}}\mathbf{func}_{\Sigma}(s_{1})=\begin{pmatrix}8\\
    12\\ 0\end{pmatrix}</annotation></semantics></math> |  |
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`\mathbf{y_{s_{1}}}` = `\mathbf{W_{3\times 9}}` `\mathbf{func}_{\Sigma}(s_{1})`
    = `\begin{pmatrix}8\\ 12\\ 0\end{pmatrix}`'
- en: Clearly, in this case, it is extremely difficult to derive back the discrete
    symbolic sequence $s_{1}$ that has generated the final distributed representation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在这种情况下，从最终的分布式表示中恢复生成的离散符号序列 $s_{1}$ 是极其困难的。
- en: 'Summing up, a distributed representation $y_{s}$ of an discrete symbolic expression
    $s$ is obtained by using an encoder that acts in two ways:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，离散符号表达 $s$ 的分布式表示 $y_{s}$ 是通过使用一种双重作用的编码器获得的：
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: transforms symbols $w_{i}$ in vectors by using an embedding matrix $\mathbf{W_{d\times
    n}}$ and the local distributed representation $\mathbf{e_{i}}$ of $w_{i}$;
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用嵌入矩阵 $\mathbf{W_{d\times n}}$ 和 $w_{i}$ 的局部分布表示 $\mathbf{e_{i}}$，将符号 $w_{i}$
    转换为向量；
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: transposes the concatenative compositionality of the discrete symbolic expression
    $s$ in a functional compositionality by defining the used composition function
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将离散符号表达 $s$ 的串联组合性通过定义所使用的组合函数转置为功能组合性
- en: 'When defining a distributed representation, we need to define two elements:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义分布式表示时，我们需要定义两个要素：
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'an embedding matrix $\mathbf{W}$ that should balance these two different aims:
    (1) *maximize* interpretability, that is, inversion; (2) *maximize* similarity
    among different symbols for specific purposes.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个嵌入矩阵 $\mathbf{W}$ 应该平衡这两个不同的目标：（1）*最大化* 可解释性，即逆转；（2）*最大化* 在特定目的下不同符号之间的相似性。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'the functional composition model: additive, holographic reduced representations
    (Plate, [1995](#bib.bib57)), recursive neural networks Schuster and Paliwal ([1997](#bib.bib63));
    Hochreiter and Schmidhuber ([1997](#bib.bib36)) or with attention Vaswani et al.
    ([2017](#bib.bib69)); Devlin et al. ([2018](#bib.bib19))'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 功能组合模型：加法的，全息缩减表示（Plate，[1995](#bib.bib57)），递归神经网络 Schuster 和 Paliwal（[1997](#bib.bib63)）；Hochreiter
    和 Schmidhuber（[1997](#bib.bib36)）或带有注意力的 Vaswani 等（[2017](#bib.bib69)）；Devlin
    等（[2018](#bib.bib19)）
- en: 'And, the final questions are: What’s inside the distributed representation?
    What’s exactly encoded? How this information is used to take decisions? Hence,
    the debated question become how concatenative is the functional compositionality
    in distributed representations behind neural networks? Can we retrieve discrete
    symbols and rebuild sequences?'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的问题是：分布式表示中包含什么？究竟编码了什么？这些信息如何用于做出决策？因此，争论的问题变成了神经网络中的分布式表示在功能组合性上的串联程度如何？我们能否检索离散符号并重建序列？
- en: To answer the above questions, we then describe the two properties *Interpretability*
    and *concatenative compositionality* for distributed representations. These two
    properties want to measure how far are distributed representations from symbolic
    representations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答上述问题，我们接着描述分布式表示的两个属性 *可解释性* 和 *串联组合性*。这两个属性旨在衡量分布式表示与符号表示之间的距离。
- en: 'Interpretability is the possibility of decoding distributed representations,
    that is, extracting the embedded symbolic representations. This is an important
    characteristic but it must be noted that it’s not a simple yes-or-no classification.
    It is more a degree associated to specific representations. In fact, even if each
    component of a vector representation does not have a specific meaning, this does
    not mean that the representation is not interpretable as a whole, or that symbolic
    information cannot be recovered from it. For this reason, we can categorize the
    degree of interpretability of a representation as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是解码分布式表示的可能性，即提取嵌入的符号表示。这是一个重要特性，但需要注意的是，这不是一个简单的“是”或“否”的分类问题。它更像是与特定表示相关的一个程度。实际上，即使一个向量表示的每个组件没有特定的意义，这并不意味着该表示整体上不可解释，或者无法从中恢复符号信息。因此，我们可以按如下方式对表示的可解释性程度进行分类：
- en: '*human-interpretable* – each dimension of a representation has a specific meaning;'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人类可解释的* — 表示的每个维度具有特定的意义；'
- en: '*decodable* – the representation may be obscure, but it can be decoded into
    an interpretable, symbolic representation.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解码* — 表示可能很模糊，但可以解码为可解释的符号表示。'
- en: Concatenative Compositionality for distributed representations is the possibility
    of composing basic distributed representations with strong rules and of decomposing
    back composed representations with inverse rules. Generally, in NLP, basic distributed
    representations refer to basic symbols.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式表示的串联组合性是将基本分布式表示按照强规则组合，并根据逆规则分解回组合表示的可能性。通常，在自然语言处理中，基本分布式表示指的是基本符号。
- en: The two axes of *Interpretability* and *Concatenative Compositionality for distributed
    representations* will be used to describe the presented distributed representations
    as we are interested in understanding whether or not a representation can be used
    to represent structures or sequences and whether it is possible to extract back
    the underlying structure or sequence given a distributed representation. It is
    clear that a local distributed representation is more interpretable than a distributed
    representation. Yet, both representations lack in concatenative compositionality
    when sequences or structures are collapsed in vectors or tensors that do not depend
    on the length of represented sequences or structures. For example, the bag-of-word
    local representation does not take into consideration the order of the symbols
    in the sequence.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用*可解释性*和*分布式表示的连接组合性*这两个轴来描述所呈现的分布式表示，因为我们希望深入了解一个表示是否可以用于表示结构或序列，以及是否可以在给定分布式表示的情况下提取出潜在的结构或序列。显然，本地分布式表示比分布式表示更具可解释性。然而，当序列或结构被压缩成不依赖于表示序列或结构长度的向量或张量时，两种表示都缺乏连接组合性。例如，词袋局部表示没有考虑符号在序列中的顺序。
- en: 5 Strategies to obtain distributed representations from symbols
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从符号中获取分布式表示的5种策略
- en: 'There is a wide range of techniques to transform symbolic representations in
    distributed representations. When combining natural language processing and machine
    learning, this is a major issue: transforming symbols, sequences of symbols or
    symbolic structures in vectors or tensors that can be used in learning machines.
    These techniques generally propose a function $\eta$ to transform a *local representation*
    with a large number of dimensions in a *distributed representation* with a lower
    number of dimensions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 转换符号表示为分布式表示的技术种类繁多。当结合自然语言处理和机器学习时，这是一个主要问题：将符号、符号序列或符号结构转换为可以在学习机器中使用的向量或张量。这些技术通常提出一个函数$\eta$，以将具有大量维度的*局部表示*转换为具有较少维度的*分布式表示*：
- en: '|  | $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ |  |'
- en: This function is often called *encoder*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数通常被称为*编码器*。
- en: 'We propose to categorize techniques to obtain distributed representations in
    two broad categories, showing some degree of overlapping:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议将获取分布式表示的技术分为两个广泛的类别，并显示出一定程度的重叠：
- en: •
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: representations derived from dimensionality reduction techniques;
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从降维技术中获得的表示；
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: learned representations
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学到的表示
- en: 'In the rest of the section, we will introduce the different strategies according
    to the proposed categorization. Moreover, we will emphasize its degree of interpretability
    for each representation and its related function $\eta$ by answering to two questions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将根据提出的分类介绍不同的策略。此外，我们将通过回答两个问题来强调每个表示及其相关函数$\eta$的解释程度：
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Has a specific dimension in $\mathbb{R}^{d}$ a clear meaning?
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在$\mathbb{R}^{d}$中具有特定维度是否有明确含义？
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can we decode an encoded symbolic representation? In other words, assuming a
    decoding function $\delta\colon\mathbb{R}^{d}\to\mathbb{R}^{n}$, how far is $v\in\mathbb{R}^{n}$,
    which represents a symbolic representation, from $v^{\prime}=\delta(\eta(v))$?
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以解码一个编码的符号表示吗？换句话说，假设有一个解码函数$\delta\colon\mathbb{R}^{d}\to\mathbb{R}^{n}$，那么表示符号表示的$v\in\mathbb{R}^{n}$距离$v^{\prime}=\delta(\eta(v))$有多远？
- en: 'Instead, composability of the resulting representations will be analyzed in
    Sec. [7](#S7 "7 Composing distributed representations ‣ Symbolic, Distributed
    and Distributional Representations for Natural Language Processing in the Era
    of Deep Learning: a Survey").'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，结果表示的可组合性将在第[7](#S7 "7 组成分布式表示 ‣ 符号、分布式和分布化表示在深度学习时代的自然语言处理中的调查")节中进行分析。
- en: 5.1 Dimensionality reductio with Random Projections
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 随机投影的降维
- en: '*Random projection* (RP) (Bingham and Mannila, [2001](#bib.bib9); Fodor, [2002](#bib.bib23))
    is a technique based on random matrices $W_{d}\in\mathbb{R}^{d\times n}$. Generally,
    the rows of the matrix $W_{d}$ are sampled from a Gaussian distribution with zero
    mean, and normalized as to have unit length (Johnson and Lindenstrauss, [1984](#bib.bib39))
    or even less complex random vectors (Achlioptas, [2003](#bib.bib1)). Random projections
    from Gaussian distributions approximately preserves pairwise distance between
    points (see the *Johnsonn-Lindenstrauss Lemma* (Johnson and Lindenstrauss, [1984](#bib.bib39))),
    that is, for any vector $x,y\in X$:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机投影* (RP) (Bingham and Mannila, [2001](#bib.bib9); Fodor, [2002](#bib.bib23))
    是一种基于随机矩阵 $W_{d}\in\mathbb{R}^{d\times n}$ 的技术。通常，矩阵 $W_{d}$ 的行从均值为零的高斯分布中采样，并归一化为单位长度（Johnson
    and Lindenstrauss, [1984](#bib.bib39)），或者甚至从更简单的随机向量中采样（Achlioptas, [2003](#bib.bib1)）。高斯分布中的随机投影大致保留点之间的成对距离（参见
    *Johnson-Lindenstrauss 引理*（Johnson and Lindenstrauss, [1984](#bib.bib39)），即，对于任何向量
    $x,y\in X$：'
- en: '|  | $(1-\varepsilon)\ \&#124;\mathbf{x}-\mathbf{y}\&#124;^{2}\leq\&#124;W\mathbf{x}-W\mathbf{y}\&#124;^{2}\leq(1+\varepsilon)\
    \&#124;\mathbf{x}-\mathbf{y}\&#124;^{2}$ |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $(1-\varepsilon)\ \&#124;\mathbf{x}-\mathbf{y}\&#124;^{2}\leq\&#124;W\mathbf{x}-W\mathbf{y}\&#124;^{2}\leq(1+\varepsilon)\
    \&#124;\mathbf{x}-\mathbf{y}\&#124;^{2}$ |  |'
- en: 'where the approximation factor $\varepsilon$ depends on the dimension of the
    projection, namely, to assure that the approximation factor is $\varepsilon$,
    the dimension $k$ must be chosen such that:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中近似因子 $\varepsilon$ 取决于投影的维度，即，为了确保近似因子为 $\varepsilon$，维度 $k$ 必须选择如下：
- en: '|  | $k\geq\frac{8\log(m)}{\varepsilon^{2}}$ |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $k\geq\frac{8\log(m)}{\varepsilon^{2}}$ |  |'
- en: 'Constraints for building the matrix $W$ can be significantly relaxed to less
    complex random vectors (Achlioptas, [2003](#bib.bib1)). Rows of the matrix can
    be sampled from very simple zero-mean distributions such as:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 构建矩阵 $W$ 的约束可以显著放宽到更简单的随机向量（Achlioptas，[2003](#bib.bib1)）。矩阵的行可以从非常简单的零均值分布中采样，例如：
- en: '|  | <math   alttext="W_{ij}=\sqrt{3}\begin{cases}+1\ \text{ with probability
    }\frac{1}{6}\\ -1\ \text{ with probability }\frac{1}{6}\\'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="W_{ij}=\sqrt{3}\begin{cases}+1\ \text{ with probability
    }\frac{1}{6}\\ -1\ \text{ with probability }\frac{1}{6}\\'
- en: 0\ \ \ \text{ with probability }\frac{2}{3}\\
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 0\ \ \ \text{ with probability }\frac{2}{3}\\
- en: \end{cases}" display="block"><semantics ><mrow ><msub  ><mi >W</mi><mrow ><mi  >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow></msub><mo >=</mo><mrow ><msqrt  ><mn
    >3</mn></msqrt><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  columnalign="left"
    ><mrow ><mo  >+</mo><mrow ><mn >1</mn><mo lspace="0.500em" rspace="0em" >​</mo><mtext
    > with probability </mtext><mo lspace="0em" rspace="0em" >​</mo><mstyle displaystyle="false"
    ><mfrac ><mn  >1</mn><mn >6</mn></mfrac></mstyle></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mo >−</mo><mrow  ><mn >1</mn><mo lspace="0.500em"
    rspace="0em"  >​</mo><mtext > with probability </mtext><mo lspace="0em" rspace="0em"
    >​</mo><mstyle displaystyle="false" ><mfrac ><mn  >1</mn><mn >6</mn></mfrac></mstyle></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mn >0</mn><mrow ><mtext > with probability </mtext><mo
    lspace="0em" rspace="0em"  >​</mo><mstyle displaystyle="false"  ><mfrac ><mn >2</mn><mn
    >3</mn></mfrac></mstyle></mrow></mrow></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci  >𝑊</ci><apply
    ><ci >𝑖</ci><ci  >𝑗</ci></apply></apply><apply ><apply ><cn type="integer"  >3</cn></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><apply ><apply  ><cn type="integer"  >1</cn><ci
    ><mtext > with probability </mtext></ci><apply ><cn type="integer" >1</cn><cn
    type="integer"  >6</cn></apply></apply></apply><ci ><mtext >otherwise</mtext></ci><apply
    ><apply ><cn type="integer" >1</cn><ci ><mtext > with probability </mtext></ci><apply
    ><cn type="integer" >1</cn><cn type="integer" >6</cn></apply></apply></apply><ci
    ><mtext >otherwise</mtext></ci><list ><cn type="integer" >0</cn><apply ><ci ><mtext
    > with probability </mtext></ci><apply ><cn type="integer" >2</cn><cn type="integer"
    >3</cn></apply></apply></list><ci ><mtext >otherwise</mtext></ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >W_{ij}=\sqrt{3}\begin{cases}+1\ \text{ with probability
    }\frac{1}{6}\\ -1\ \text{ with probability }\frac{1}{6}\\ 0\ \ \ \text{ with probability
    }\frac{2}{3}\\ \end{cases}</annotation></semantics></math> |  |
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`W_{ij}=\sqrt{3}\begin{cases}+1\ \text{ with probability }\frac{1}{6}\\ -1\
    \text{ with probability }\frac{1}{6}\\ 0\ \ \ \text{ with probability }\frac{2}{3}\\
    \end{cases}`'
- en: without the need to manually ensure unit-length of the rows, and at the same
    time providing a significant speed up in computation due to the sparsity of the
    projection.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 无需手动确保行的单位长度，同时由于投影的稀疏性，计算速度显著加快。
- en: Unfortunately, vectors $\eta(\mathbf{v})$ are not *human-interpretable* as,
    even if their dimensions represent linear combinations of dimensions in the original
    local distribution, these dimensions have not an interpretation or particular
    properties.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，向量 $\eta(\mathbf{v})$ 并不是*人类可解释的*，即使它们的维度表示原始局部分布中的线性组合，这些维度也没有明确的解释或特定属性。
- en: 'On the contrary, vectors $\eta(\mathbf{v})$ are *decodable*. The decoding function
    is:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，向量 $\eta(\mathbf{v})$ 是*可解码的*。解码函数是：
- en: '|  | $\delta(\mathbf{v^{\prime}})=W_{d}^{T}\mathbf{v^{\prime}}$ |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta(\mathbf{v^{\prime}})=W_{d}^{T}\mathbf{v^{\prime}}$ |  |'
- en: and $W_{d}^{T}W_{d}\approx I$ when $W_{d}$ is derived using Gaussian random
    vectors. Hence, distributed vectors in $\mathbb{R}^{d}$ can be approximately decoded
    back in the original symbolic representation with a degree of approximation that
    depends on the distance between $d$ .
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $W_{d}$ 是使用高斯随机向量得出的时，$W_{d}^{T}W_{d}\approx I$。因此，分布向量在 $\mathbb{R}^{d}$
    中可以大致解码回原始符号表示，近似度取决于 $d$ 之间的距离。
- en: The major advantage of RP with respect to PCA is that the matrix $X$ of all
    the data points is not needed to derive the matrix $W_{d}$. Moreover, the matrix
    $W_{d}$ can be produced *à-la-carte* starting from the symbols encountered so
    far in the encoding procedure. In fact, it is sufficient to generate new Gaussian
    vectors for new symbols when they appear.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与 PCA 相比，RP 的主要优点在于，不需要所有数据点的矩阵 $X$ 来推导矩阵 $W_{d}$。此外，矩阵 $W_{d}$ 可以*按需*从编码过程中遇到的符号开始生成。实际上，当出现新符号时，只需生成新的高斯向量即可。
- en: 5.2 Learned representation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 学习表示
- en: 'Learned representations differ from the dimensionality reduction techniques
    by the fact that: (1) encoding/decoding functions may not be linear; (2) learning
    can optimize functions that are different with respect to the target of PCA; and,
    (3) solutions are not derived in a closed form but are obtained using optimization
    techniques such as *stochastic gradient decent*.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的表示与降维技术的不同在于：（1）编码/解码函数可能不是线性的；（2）学习可以优化与 PCA 目标不同的函数；以及，（3）解决方案不是以封闭形式得出的，而是使用优化技术如*随机梯度下降*获得的。
- en: 'Learned representation can be further classified into:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的表示可以进一步分类为：
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*task-independent representations* learned with a standalone algorithm (as
    in *autoencoders* (Socher et al., [2011](#bib.bib64); Liou et al., [2014](#bib.bib45)))
    which is independent from any task, and which learns a representation that only
    depends from the dataset used;'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*任务无关的表示*，通过独立算法（如*自编码器*（Socher 等，[2011](#bib.bib64); Liou 等，[2014](#bib.bib45)））学习，这与任何任务无关，只学习依赖于使用的数据集的表示；'
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*task-dependent representations* learned as the first step of another algorithm
    (this is called *end-to-end training*), usually the first layer of a deep neural
    network. In this case the new representation is driven by the task.'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*任务相关的表示* 作为另一算法的第一步学习（这称为*端到端训练*），通常是深度神经网络的第一层。在这种情况下，新表示是由任务驱动的。'
- en: 5.2.1 Autoencoder
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 自编码器
- en: Autoencoders are a task independent technique to learn a distributed representation
    encoder $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ by using local representations
    of a set of examples (Socher et al., [2011](#bib.bib64); Liou et al., [2014](#bib.bib45)).
    The distributed representation encoder $\eta$ is half of an autoencoder.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种任务无关的技术，通过使用一组示例的局部表示来学习分布表示编码器 $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$（Socher
    等，[2011](#bib.bib64); Liou 等，[2014](#bib.bib45)）。分布表示编码器 $\eta$ 是自编码器的一半。
- en: 'An autoencoder is a neural network that aims to reproduce an input vector in
    $\mathbb{R}^{n}$ as output by passing in a hidden layer(s) that are in $\mathbb{R}^{d}$.
    Given $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$ and $\delta\colon\mathbb{R}^{d}\to\mathbb{R}^{n}$
    as the encoder and the decoder, respectively, an autoencoder aims to maximize
    the following function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一个神经网络，旨在通过将输入向量 $\mathbb{R}^{n}$ 通过隐藏层传递，重建为输出。给定编码器 $\eta\colon\mathbb{R}^{n}\to\mathbb{R}^{d}$
    和解码器 $\delta\colon\mathbb{R}^{d}\to\mathbb{R}^{n}$，自编码器旨在最大化以下函数：
- en: '|  | $\mathcal{L}(\mathbf{x},\mathbf{x}^{\prime})=\&#124;\mathbf{x}-\mathbf{x}^{\prime}\&#124;^{2}$
    |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\mathbf{x},\mathbf{x}^{\prime})=\&#124;\mathbf{x}-\mathbf{x}^{\prime}\&#124;^{2}$
    |  |'
- en: where
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\mathbf{x^{\prime}}=\delta(\eta(\mathbf{x}))$ |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x^{\prime}}=\delta(\eta(\mathbf{x}))$ |  |'
- en: The encoding and decoding module are two neural networks, which means that they
    are functions depending on a set of parameters $\theta$ of the form
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 编码和解码模块是两个神经网络，这意味着它们是依赖于一组参数 $\theta$ 的函数，其形式为
- en: '|  | $\displaystyle\eta_{\theta}(x)=s(Wx+b)$ |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\eta_{\theta}(x)=s(Wx+b)$ |  |'
- en: '|  | $\displaystyle\delta_{\theta^{\prime}}(y)=s(W^{\prime}y+b^{\prime})$ |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\delta_{\theta^{\prime}}(y)=s(W^{\prime}y+b^{\prime})$ |  |'
- en: where the parameters of the entire model are $\theta,\theta^{\prime}=\left\{W,b,W^{\prime},b^{\prime}\right\}$
    with $W,W^{\prime}$ matrices, $b,b^{\prime}$ vectors and $s$ is a function that
    can be either a non-linearity sigmoid shaped function, or in some cases the identity
    function. In some variants the matrices $W$ and $W^{\prime}$ are constrained to
    $W^{T}=W^{\prime}$. This model is different with respect to PCA due to the target
    loss function and the use of non-linear functions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中整个模型的参数为 $\theta,\theta^{\prime}=\left\{W,b,W^{\prime},b^{\prime}\right\}$，$W$
    和 $W^{\prime}$ 是矩阵，$b$ 和 $b^{\prime}$ 是向量，而 $s$ 是一个函数，可以是非线性 sigmoid 形状的函数，或者在某些情况下是恒等函数。在某些变体中，矩阵
    $W$ 和 $W^{\prime}$ 被限制为 $W^{T}=W^{\prime}$。由于目标损失函数和非线性函数的使用，这个模型与 PCA 有所不同。
- en: 'Autoencoders have been further improved with *denoising autoencoders* (Vincent
    et al., [2010](#bib.bib71), [2008](#bib.bib70); Masci et al., [2011](#bib.bib48))
    that are a variant of autoencoders where the goal is to reconstruct the input
    from a corrupted version. The intuition is that higher level features should be
    robust with regard to small noise in the input. In particular, the input $\mathbf{x}$
    gets corrupted via a stochastic function:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器通过 *去噪自编码器*（Vincent et al., [2010](#bib.bib71), [2008](#bib.bib70); Masci
    et al., [2011](#bib.bib48)）得到了进一步的改进，这是自编码器的一种变体，其目标是从损坏的版本中重构输入。直觉是较高层次的特征应该对输入中的小噪声具有鲁棒性。特别地，输入
    $\mathbf{x}$ 通过随机函数进行损坏：
- en: '|  | $\tilde{\mathbf{x}}=g(\mathbf{x})$ |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\mathbf{x}}=g(\mathbf{x})$ |  |'
- en: 'and then one minimizes again the reconstruction error, but with regard to the
    *original* (uncorrupted) input:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次最小化重构误差，但相对于 *原始*（未损坏）输入：
- en: '|  | $\mathcal{L}(\mathbf{x},\mathbf{x}^{\prime})=\&#124;\mathbf{x}-\delta(\eta(g(\mathbf{x})))\&#124;^{2}$
    |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\mathbf{x},\mathbf{x}^{\prime})=\&#124;\mathbf{x}-\delta(\eta(g(\mathbf{x})))\&#124;^{2}$
    |  |'
- en: 'Usually $g$ can be either:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常 $g$ 可以是以下之一：
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'adding gaussian noise: $g(\mathbf{x})=\mathbf{x}+\varepsilon$, where $\varepsilon\sim\mathcal{N}(0,\sigma\mathbb{I})$;'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加高斯噪声：$g(\mathbf{x})=\mathbf{x}+\varepsilon$，其中 $\varepsilon\sim\mathcal{N}(0,\sigma\mathbb{I})$；
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'masking noise: where a given a fraction $\nu$ of the components of the input
    gets set to $0$'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 掩蔽噪声：将输入的给定分量 $\nu$ 设为 $0$
- en: For what concerns *intepretability*, as for random projection, distributed representations
    $\eta(\mathbf{v})$ obtained with encoders from autoencoders and denoising autoencoders
    are not *human-interpretable* but are *decodable* as this is the nature of autoencoders.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 *可解释性*，与随机投影一样，通过自编码器和去噪自编码器获得的分布式表示 $\eta(\mathbf{v})$ 并不 *人类可解释*，但可以 *解码*，因为这正是自编码器的特性。
- en: Moreover, *composability* is not covered by this formulation of autoencoders.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*组合性* 不包含在这个自编码器的表述中。
- en: 5.2.2 Embedding layers
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 嵌入层
- en: Embedding layers are generally the first layers of more complex neural networks
    which are responsible to transform an initial local representation in the first
    internal distributed representation. The main difference with autoencoders is
    that these layers are shaped by the entire overall learning process. The learning
    process is generally task dependent. Hence, these first embedding layers depend
    on the final task.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层通常是更复杂神经网络的第一层，负责将初始的局部表示转换为第一个内部分布式表示。与自编码器的主要区别在于，这些层由整个学习过程塑造。学习过程通常依赖于任务。因此，这些首层嵌入层取决于最终任务。
- en: It is argued that each layers learn a higher-level representation of its input.
    This is particularly visible with convolutional network (Krizhevsky et al., [2012](#bib.bib42))
    applied to computer vision tasks. In these suggestive visualizations (Zeiler and
    Fergus, [2014b](#bib.bib81)), the hidden layers are seen to correspond to abstract
    feature of the image, starting from simple edges (in lower layers) up to faces
    in the higher ones.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为，每一层学习其输入的更高级别的表示。这在应用于计算机视觉任务的卷积网络（Krizhevsky 等，[2012](#bib.bib42)）中尤为明显。在这些具有启发性的可视化（Zeiler
    和 Fergus，[2014b](#bib.bib81)）中，隐藏层被视为对应于图像的抽象特征，从简单的边缘（在较低层）到更高层的面孔。
- en: However, these embedding layers produce encoding functions and, thus, distributed
    representations that are not interpretable when applied to symbols. In fact, these
    distributed representations are not human-interpretable as dimensions are not
    clearly related to specific aggregations of symbols. Moreover, these embedding
    layers do not naturally provide decoders. Hence, this distributed representation
    is not decodable.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些嵌入层产生的编码函数，因此，当应用于符号时，这些分布式表示是不可解释的。事实上，这些分布式表示在维度与符号的具体聚合关系不明确时，无法被人类解释。此外，这些嵌入层自然不提供解码器。因此，这种分布式表示是不可解码的。
- en: 6 *Distributional* Representations as another side of the coin
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 *分布式* 表示的另一面
- en: '*Distributional* semantics is an important area of research in natural language
    processing that aims to describe meaning of words and sentences with vectorial
    representations (see (Turney and Pantel, [2010](#bib.bib68)) for a survey). These
    representations are called *distributional representations*.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式* 语义是自然语言处理中的一个重要研究领域，旨在通过向量表示描述词汇和句子的意义（参见（Turney 和 Pantel，[2010](#bib.bib68)）的综述）。这些表示被称为*分布式表示*。'
- en: It is a strange historical accident that two similar sounding names – *distributed*
    and *distributional* – have been given to two concepts that should not be confused
    for many. Maybe, this has happened because the two concepts are definitely related.
    We argue that distributional representation are nothing more than a subset of
    distributed representations, and in fact can be categorized neatly into the divisions
    presented in the previous section
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个奇怪的历史巧合是，两个发音相似的名称——*分布式* 和 *分布*——被赋予了两个不应混淆的概念。也许，这种情况发生是因为这两个概念确实相关。我们认为，分布式表示不过是分布表示的一个子集，实际上可以整齐地归入前一节中提出的分类。
- en: Distributional semantics is based on a famous slogan – *“you shall judge a word
    by the company it keeps”* (Firth, [1957](#bib.bib22)) – and on the *distributional
    hypothesis* (Harris, [1964](#bib.bib33)) – words have similar meaning if used
    in similar contexts, that is, words with the same or similar *distribution*. Hence,
    the name distributional as well as the core hypothesis comes from a linguistic
    rather than computer science background.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式语义基于一个著名的口号——*“你可以通过一个词所处的环境来判断它”*（Firth，[1957](#bib.bib22)）——以及*分布假设*（Harris，[1964](#bib.bib33)）——如果词汇在相似的上下文中使用，则具有相似的意义，即具有相同或相似的*分布*。因此，分布式这一名称以及核心假设源于语言学而非计算机科学背景。
- en: Distributional vectors represent words by describing information related to
    the contexts in which they appear. Put in this way it is apparent that a distributional
    representation *is* a specific case of a distributed representation, and the different
    name is only an indicator of the context in which this techniques originated.
    Representations for sentences are generally obtained combining vectors representing
    words.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式向量通过描述词汇出现的上下文信息来表示词汇。这样看来，分布式表示*是*分布表示的一个特定案例，不同的名称只是指示了这些技术起源的上下文。句子的表示通常是通过结合表示词汇的向量来获得的。
- en: 'Hence, distributional semantics is a special case of distributed representations
    with a restriction on what can be used as features in vector spaces: features
    represent a bit of contextual information. Then, the largest body of research
    is on what should be used to represent contexts and how it should be taken into
    account. Once this is decided, large matrices $X$ representing words in context
    are collected and, then, dimensionality reduction techniques are applied to have
    treatable and more discriminative vectors.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分布语义学是分布式表示的一种特殊情况，其限制了可以在向量空间中作为特征使用的内容：特征代表了一些上下文信息。然后，最大的研究领域集中在应使用什么来表示上下文以及如何考虑这些上下文。一旦决定了这些内容，就会收集大量的矩阵$X$来表示上下文中的词汇，然后应用降维技术以获得可处理和更具辨别性的向量。
- en: In the rest of the section, we present how to build matrices representing words
    in context, we will shortly recap on how dimensionality reduction techniques have
    been used in distributional semantics, and, finally, we report on word2vec (Mikolov
    et al., [2013](#bib.bib49)), which is a novel distributional semantic techniques
    based on deep learning.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们展示了如何构建表示上下文中的词汇的矩阵，我们将简要回顾一下降维技术在分布语义学中的应用，最后，我们将介绍word2vec（Mikolov
    et al., [2013](#bib.bib49)），这是一种基于深度学习的新型分布语义学技术。
- en: 6.1 Building distributional representations for words from a corpus
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 从语料库中构建词汇的分布式表示
- en: 'The major issue in distributional semantics is how to build distributional
    representations for words by observing word contexts in a collection of documents.
    In this section, we will describe these techniques using the example of the corpus
    in Table [1](#S6.T1 "Table 1 ‣ 6.1 Building distributional representations for
    words from a corpus ‣ 6 Distributional Representations as another side of the
    coin ‣ Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey").'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '分布语义学中的主要问题是如何通过观察文档集合中的词汇上下文来构建词汇的分布式表示。在本节中，我们将以表[1](#S6.T1 "Table 1 ‣ 6.1
    Building distributional representations for words from a corpus ‣ 6 Distributional
    Representations as another side of the coin ‣ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey")中的语料库为例来描述这些技术。'
- en: '| $s_{1}$ | *a cat catches a mouse* |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| $s_{1}$ | *一只猫抓住了一只老鼠* |'
- en: '| $s_{2}$ | *a dog eats a mouse* |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| $s_{2}$ | *一只狗吃了一只老鼠* |'
- en: '| $s_{3}$ | *a dog catches a cat* |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| $s_{3}$ | *一只狗抓住了一只猫* |'
- en: 'Table 1: A very small corpus'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：一个非常小的语料库
- en: A first and simple distributional semantic representations of words is given
    by word vs. document matrices as those typical in information retrieval (Salton,
    [1989](#bib.bib61)). Word context are represented by document indexes. Then, words
    are similar if these words similarly appear in documents. This is generally referred
    as *topical similarity* (Landauer and Dumais, [1997](#bib.bib43)) as words belonging
    to the same topic tend to be more similar. An example of this approach is given
    by the matrix in Eq. LABEL:first_distributional_representation. In fact, this
    matrix is already a distributional and distributed representation for words which
    are represented as vectors in rows.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇的首个简单的分布语义表示是通过词汇与文档矩阵来实现的，这些矩阵在信息检索中很常见（Salton, [1989](#bib.bib61)）。词汇上下文通过文档索引来表示。然后，如果这些词汇在文档中出现的方式相似，那么这些词汇就被认为是相似的。这通常被称为*主题相似性*（Landauer
    and Dumais, [1997](#bib.bib43)），因为属于同一主题的词汇往往更相似。该方法的一个示例由公式 LABEL:first_distributional_representation
    中的矩阵给出。实际上，这个矩阵已经是一个分布式和分布式的词汇表示，其中词汇在行中作为向量表示。
- en: A second strategy to build distributional representations for words is to build
    word vs. contextual feature matrices. These contextual features represent *proxies*
    for semantic attributes of modeled words (Baroni and Lenci, [2010](#bib.bib5)).
    For example, contexts of the word *dog* will somehow have relation with the fact
    that a dog has four legs, barks, eats, and so on. In this case, these vectors
    capture a similarity that is more related to a co-hyponymy, that is, words sharing
    similar attributes are similar. For example, *dog* is more similar to *cat* than
    to *car* as *dog* and *cat* share more attributes than *dog* and *car*. This is
    often referred as *attributional similarity* (Turney, [2006](#bib.bib67)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 构建词的分布式表示的第二种策略是建立词与上下文特征矩阵。这些上下文特征代表了建模词的*代理*语义属性（Baroni和Lenci，[2010](#bib.bib5)）。例如，*dog*（狗）的上下文将与狗有四条腿、吠叫、吃东西等事实有关。在这种情况下，这些向量捕捉到一种更相关的相似性，即共上位词性，即共享相似属性的词是相似的。例如，*dog*（狗）与*cat*（猫）的相似性要高于与*car*（车）的相似性，因为*dog*和*cat*共享的属性比*dog*和*car*更多。这通常被称为*属性相似性*（Turney，[2006](#bib.bib67)）。
- en: 'A simple example of this second strategy are word-to-word matrices obtained
    by observing n-word windows of target words. For example, a word-to-word matrix
    obtained for the corpus in Table [1](#S6.T1 "Table 1 ‣ 6.1 Building distributional
    representations for words from a corpus ‣ 6 Distributional Representations as
    another side of the coin ‣ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey") by considering
    a 1-word window is the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这第二种策略的一个简单示例是通过观察目标词的n词窗口获得的词对词矩阵。例如，考虑一个1词窗口，对表格[1](#S6.T1 "表1 ‣ 6.1 从语料库构建词的分布式表示
    ‣ 6 分布式表示作为另一面 ‣ 深度学习时代自然语言处理中的符号、分布式和分布式表示：综述")中的语料库获得的词对词矩阵如下：
- en: '|  | <math   alttext="X=\hbox{}\vbox{\kern 0.86108pt\hbox{$\kern 0.0pt\kern
    2.5pt\kern-5.0pt\left(\kern 0.0pt\kern-2.5pt\kern-6.66669pt\vbox{\kern-0.86108pt\vbox{\vbox{
    \halign{\kern\arraycolsep\hfil\@arstrut$\kbcolstyle#$\hfil\kern\arraycolsep&amp;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="X=\hbox{}\vbox{\kern 0.86108pt\hbox{$\kern 0.0pt\kern
    2.5pt\kern-5.0pt\left(\kern 0.0pt\kern-2.5pt\kern-6.66669pt\vbox{\kern-0.86108pt\vbox{\vbox{
    \halign{\kern\arraycolsep\hfil\@arstrut$\kbcolstyle#$\hfil\kern\arraycolsep&amp;'
- en: \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep&amp;&amp;
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep&amp;&amp;
- en: \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep\cr
    5.0pt\hfil\@arstrut$\scriptstyle$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    a$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle cat$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    dog$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle mouse$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    catches$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle eats\\a$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 2\\cat$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\dog$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1\\mouse$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\catches$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\eats$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\$\hfil\kern 5.0pt\crcr}}}}\right)$}}"
    display="block"><semantics ><mrow  ><mi >X</mi><mo  >=</mo> <mrow  ><mtext  ><xmath
    xmlns="http://dlmf.nist.gov/LaTeXML"  frag ><xmdual  frag ><xmref idref="S6.E2.m1.1.1.1.1.m1.43.43nest"
    frag ><xmwrap frag ><xmtok role="OPEN" stretchy="true"  frag >(</xmtok><xmarray
    vattach="bottom" frag ><xmrow frag ><xmcell align="left"   frag ><xmtext  frag
    ><xmtext  frag >\@arstrut</xmtext></xmtext></xmcell><xmcell align="left" frag
    ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >a</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >d</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag >o</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >g</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >m</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >o</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >u</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >s</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >e</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >c</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >a</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >t</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag
    >c</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >h</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic"  frag >e</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >e</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%"  frag >s</xmtok>\\<xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >a</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%"
    frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%"
    frag >2</xmtok>\\<xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag >c</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >t</xmtok></xmtext></xmcell><xmcell align="left"
    class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER"
    fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\<xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >d</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >o</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >g</xmtok></xmtext></xmcell><xmcell align="left" class="ltx_nopad_l ltx_nopad_r"  frag
    ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="1" role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok>\\<xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >m</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >o</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >u</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >s</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >e</xmtok></xmtext></xmcell><xmcell align="left" class="ltx_nopad_l ltx_nopad_r"  frag
    ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok>\\<xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >c</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%" frag >t</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >h</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >e</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="2"
    role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\<xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >e</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >t</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="1"
    role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\</xmtext></xmcell></xmrow></xmarray><xmtok
    role="CLOSE" stretchy="true"  frag >)</xmtok></xmwrap></xmref></xmdual></xmath></mtext></mrow></mrow>
    <annotation-xml encoding="MathML-Content" ><apply ><ci  >𝑋</ci> <ci  ><mrow ><mtext  ><xmath
    xmlns="http://dlmf.nist.gov/LaTeXML"  frag ><xmdual  frag ><xmref idref="S6.E2.m1.1.1.1.1.m1.43.43anest"
    frag ><xmwrap frag ><xmtok role="OPEN" stretchy="true"  frag >(</xmtok><xmarray
    vattach="bottom" frag ><xmrow frag ><xmcell align="left"   frag ><xmtext  frag
    ><xmtext  frag >\@arstrut</xmtext></xmtext></xmcell><xmcell align="left" frag
    ><xmtext frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic"  frag >a</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >t</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >d</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >o</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >g</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >m</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >o</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >u</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag
    >s</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >e</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >c</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >h</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag
    >e</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >e</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >s</xmtok>\\<xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >a</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%"
    frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%"
    frag >2</xmtok>\\<xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag >c</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >t</xmtok></xmtext></xmcell><xmcell align="left"
    class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="2" role="NUMBER"
    fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\<xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >d</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >o</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >g</xmtok></xmtext></xmcell><xmcell align="left" class="ltx_nopad_l ltx_nopad_r"  frag
    ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="1" role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok>\\<xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >m</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >o</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >u</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >s</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >e</xmtok></xmtext></xmcell><xmcell align="left" class="ltx_nopad_l ltx_nopad_r"  frag
    ><xmtext  frag ><xmtok meaning="2" role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok>\\<xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >c</xmtok><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%" frag >t</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >h</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >e</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="2"
    role="NUMBER" fontsize="70%" frag >2</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\<xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >e</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >a</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >t</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok meaning="1"
    role="NUMBER" fontsize="70%" frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag
    ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="0" role="NUMBER" fontsize="70%"
    frag >0</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok>\\</xmtext></xmcell></xmrow></xmarray><xmtok
    role="CLOSE" stretchy="true"  frag >)</xmtok></xmwrap></xmref></xmdual></xmath></mtext></mrow></ci></apply></annotation-xml>
    <annotation encoding="application/x-tex" >X=\hbox{}\vbox{\kern 0.86108pt\hbox{$\kern
    0.0pt\kern 2.5pt\kern-5.0pt\left(\kern 0.0pt\kern-2.5pt\kern-6.66669pt\vbox{\kern-0.86108pt\vbox{\vbox{
    \halign{\kern\arraycolsep\hfil\@arstrut$\kbcolstyle#$\hfil\kern\arraycolsep& \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep&&
    \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep\cr
    5.0pt\hfil\@arstrut$\scriptstyle$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle a$\hfil\kern
    5.0pt&5.0pt\hfil$\scriptstyle cat$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle dog$\hfil\kern
    5.0pt&5.0pt\hfil$\scriptstyle mouse$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle catches$\hfil\kern
    5.0pt&5.0pt\hfil$\scriptstyle eats\\a$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 2\\cat$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0\\dog$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 1\\mouse$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0\\catches$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0\\eats$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&5.0pt\hfil$\scriptstyle 0\\$\hfil\kern 5.0pt\crcr}}}}\right)$}}</annotation></semantics></math>
    |  | (2) |
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: \kern\arraycolsep\hfil$\@kbrowstyle#$\ifkbalignright\relax\else\hfil\fi\kern\arraycolsep\cr
    5.0pt\hfil\@arstrut$\scriptstyle$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    a$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle cat$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    dog$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle mouse$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    catches$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle eats\\a$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 2\\cat$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\dog$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1\\mouse$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\catches$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    2$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\eats$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    1$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle
    0$\hfil\kern 5.0pt&amp;5.0pt\hfil$\scriptstyle 0\\$\hfil\kern 5.0pt\crcr}}}}\right)$}}"
    display="block"><semantics ><mrow  ><mi >X</mi><mo  >=</mo> <mrow  ><mtext  ><xmath
    xmlns="http://dlmf.nist.gov/LaTeXML"  frag ><xmdual  frag ><xmref idref="S6.E2.m1.1.1.1.1.m1.43.43nest"
    frag ><xmwrap frag ><xmtok role="OPEN" stretchy="true"  frag >(</xmtok><xmarray
    vattach="bottom" frag ><xmrow frag ><xmcell align="left"   frag ><xmtext  frag
    ><xmtext  frag >\@arstrut</xmtext></xmtext></xmcell><xmcell align="left" frag
    ><xmtext frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok></xmtext></xmcell><xmcell
    align="left" class="ltx_nopad_l ltx_nopad_r"  frag ><xmtext  frag ><xmtok role="UNKNOWN"
    fontsize="70%" font="italic" frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic"  frag >a</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >t</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
    role="UNKNOWN" font="italic" fontsize="70%" frag >d</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >o</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >g</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >m</xmtok><xmtok role="UNKNOWN"
    fontsize="70%" font="italic"  frag >o</xmtok><xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >u</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%"  frag
    >s</xmtok><xmtok role="UNKNOWN" font="italic" fontsize="70%" frag >e</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" font="italic" fontsize="70%"
    frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >c</xmtok><xmtok role="UNKNOWN" fontsize="70%"
    font="italic" frag >h</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag
    >e</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic" frag >s</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok role="UNKNOWN" fontsize="70%" font="italic"
    frag >e</xmtok><xmtok role="UNKNOWN" fontsize="70%" font="italic"  frag >a</xmtok><xmtok
    role="UNKNOWN" fontsize="70%" font="italic" frag >t</xmtok><xmtok role="UNKNOWN"
    font="italic" fontsize="70%"  frag >s</xmtok>\\<xmtok role="UNKNOWN" font="italic"
    fontsize="70%" frag >a</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag
    ><xmtok meaning="0" role="NUMBER" fontsize="70%" frag >0</xmtok></xmtext></xmcell><xmcell
    align="left"   frag ><xmtext  frag ><xmtok meaning="1" role="NUMBER" fontsize="70%"
    frag >1</xmtok></xmtext></xmcell><xmcell align="left"   frag ><xmtext  frag ><xmtok
- en: Hence, the word *cat* is represented by the vector $\mathbf{cat}=\begin{pmatrix}2&amp;0&amp;0&amp;0&amp;1&amp;0\end{pmatrix}$
    and the similarity between *cat* and *dog* is higher than the similarity between
    *cat* and *mouse* as the cosine similarity $cos(\mathbf{cat},\mathbf{dog})$ is
    higher than the cosine similarity $cos(\mathbf{cat},\mathbf{mouse})$.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单词*cat* 被表示为向量 $\mathbf{cat}=\begin{pmatrix}2&amp;0&amp;0&amp;0&amp;1&amp;0\end{pmatrix}$，并且*cat*
    与*dog* 之间的相似性高于*cat* 与*mouse* 之间的相似性，因为余弦相似性 $cos(\mathbf{cat},\mathbf{dog})$
    高于余弦相似性 $cos(\mathbf{cat},\mathbf{mouse})$。
- en: 'The research on distributional semantics focuses on two aspects: (1) the best
    features to represent contexts; (2) the best correlation measure among target
    words and features.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对分布式语义的研究集中在两个方面：（1）表示上下文的最佳特征；（2）目标单词与特征之间的最佳相关性度量。
- en: How to represent contexts is a crucial problem in distributional semantics.
    This problem is strictly correlated to the classical question of feature definition
    and feature selection in machine learning. A wide variety of features have been
    tried. Contexts have been represented as set of relevant words, sets of relevant
    syntactic triples involving target words (Pado and Lapata, [2007](#bib.bib54);
    Rothenhäusler and Schütze, [2009](#bib.bib59)) and sets of labeled lexical triples
    (Baroni and Lenci, [2010](#bib.bib5)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如何表示上下文是分布式语义学中的一个关键问题。这个问题与机器学习中的特征定义和特征选择的经典问题密切相关。已经尝试了各种各样的特征。上下文被表示为相关单词的集合、涉及目标单词的相关句法三元组集合（Pado
    和 Lapata，[2007](#bib.bib54)；Rothenhäusler 和 Schütze，[2009](#bib.bib59)）以及标记的词汇三元组集合（Baroni
    和 Lenci，[2010](#bib.bib5)）。
- en: Finding the best correlation measure among target words and their contextual
    features is the other issue. Many correlation measures have been tried. The classical
    measures are *term frequency-inverse document frequency* (*tf-idf*) (Salton, [1989](#bib.bib61))
    and *point-wise mutual information* ($pmi$). These, among other measures, are
    used to better capture the importance of contextual features for representing
    distributional semantic of words.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是找到目标单词及其上下文特征之间的最佳相关性度量。已经尝试了许多相关性度量。经典度量包括*词频-逆文档频率*（*tf-idf*）（Salton，[1989](#bib.bib61)）和*点对点互信息*（$pmi$）。这些度量，及其他一些度量，用于更好地捕捉上下文特征在表示单词的分布式语义中的重要性。
- en: This first formulation of distributional semantics is a distributed representation
    that is *interpretable*. In fact, features represent contextual information which
    is a proxy for semantic attributes of target words (Baroni and Lenci, [2010](#bib.bib5)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式语义的首次表述是一种*可解释*的分布式表示。事实上，特征表示的是上下文信息，它是目标单词语义属性的代理（Baroni 和 Lenci，[2010](#bib.bib5)）。
- en: 6.2 Compacting distributional representations
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 压缩分布式表示
- en: As distributed representations, *distributional representations* can undergo
    the process of dimensionality reduction with Principal Component Analysis and
    Random Indexing. This process is used for two issues. The first is the classical
    problem of reducing the dimensions of the representation to obtain more compact
    representations. The second instead want to help the representation to focus on
    more discriminative dimensions. This latter issue focuses on the feature selection
    and merging which is an important task in making these representations more effective
    on the final task of similarity detection.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分布式表示，*分布式表示* 可以通过主成分分析和随机索引进行降维处理。这个过程用于两个问题。第一个是经典的将表示维度减少以获得更紧凑表示的问题。第二个则是帮助表示集中于更具区分性的维度。后者问题关注特征选择和合并，这是使这些表示在最终相似性检测任务中更有效的重要任务。
- en: 'Principal Component Analysis (PCA) is largely applied in compacting distributional
    representations: Latent Semantic Analysis (LSA) is a prominent example (Landauer
    and Dumais, [1997](#bib.bib43)). LSA were born in Information Retrieval with the
    idea of reducing word-to-document matrices. Hence, in this compact representation,
    word context are documents and distributional vectors of words report on the documents
    where words appear. This or similar matrix reduction techniques have been then
    applied to word-to-word matrices.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）在压缩分布式表示方面得到了广泛应用：潜在语义分析（LSA）是一个突出的例子（Landauer 和 Dumais，[1997](#bib.bib43)）。LSA诞生于信息检索领域，旨在减少单词与文档的矩阵。因此，在这种紧凑表示中，单词上下文是文档，而单词的分布向量报告了单词出现的文档。这种或类似的矩阵降维技术随后也被应用于单词与单词的矩阵。
- en: Principal Component Analysis (PCA) (Markovsky, [2012](#bib.bib47); Pearson,
    [1901](#bib.bib55)) is a linear method which reduces the number of dimensions
    by projecting $\mathbb{R}^{n}$ into the *“best”* linear subspace of a given dimension
    $d$ by using the a set of data points. The *“best”* linear subspace is a subspace
    where dimensions maximize the variance of the data points in the set. PCA can
    be interpreted either as a probabilistic method or as a matrix approximation and
    is then usually known as *truncated singular value decomposition*. We are here
    interested in describing PCA as probabilistic method as it related to the *interpretability*
    of the related *distributed representation*.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）（Markovsky, [2012](#bib.bib47); Pearson, [1901](#bib.bib55)）是一种线性方法，它通过将$\mathbb{R}^{n}$投影到一个给定维度$d$的*“最佳”*线性子空间中，从而减少维度数量，使用一组数据点。*“最佳”*线性子空间是一个使得数据点的方差最大化的子空间。PCA可以被解释为一种概率方法，也可以被视为矩阵近似，通常被称为*截断奇异值分解*。我们在这里感兴趣的是将PCA描述为概率方法，因为它与*可解释性*和相关的*分布表示*有关。
- en: As a probabilistic method, PCA finds an orthogonal projection matrix $W_{d}\in\mathbb{R}^{n\times
    d}$ such that the variance of the projected set of data points is maximized. The
    set of data points is referred as a matrix $X\in\mathbb{R}^{m\times n}$ where
    each row $\mathbf{x}_{i}^{T}\in\mathbb{R}^{n}$ is a single observation. Hence,
    the variance that is maximized is $\widehat{X}_{d}=XW_{d}^{T}\in\mathbb{R}^{m\times
    d}$.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种概率方法，PCA寻找一个正交投影矩阵$W_{d}\in\mathbb{R}^{n\times d}$，使得投影后的数据点集的方差最大化。数据点集被称为矩阵$X\in\mathbb{R}^{m\times
    n}$，其中每一行$\mathbf{x}_{i}^{T}\in\mathbb{R}^{n}$是一个单独的观测值。因此，最大化的方差是$\widehat{X}_{d}=XW_{d}^{T}\in\mathbb{R}^{m\times
    d}$。
- en: 'More specifically, let’s consider the first weight vector $\mathbf{w_{1}}$,
    which maps an element of the dataset $\mathbf{x}$ into a single number $\langle\mathbf{x},\mathbf{w_{1}}\rangle$.
    Maximizing the variance means that $\mathbf{w}$ is such that:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，考虑第一个权重向量$\mathbf{w_{1}}$，它将数据集中的一个元素$\mathbf{x}$映射为一个单一的数字$\langle\mathbf{x},\mathbf{w_{1}}\rangle$。最大化方差意味着$\mathbf{w}$是这样的：
- en: '|  | $\mathbf{w_{1}}=\operatorname*{arg\,max}_{\&#124;\mathbf{w}\&#124;=1}\sum_{i}\left(\langle\mathbf{x_{i}},\mathbf{w}\rangle\right)^{2}$
    |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{w_{1}}=\operatorname*{arg\,max}_{\&#124;\mathbf{w}\&#124;=1}\sum_{i}\left(\langle\mathbf{x_{i}},\mathbf{w}\rangle\right)^{2}$
    |  |'
- en: 'and it can be shown that the optimal value is achieved when $\mathbf{w}$ is
    the eigenvector of $X^{T}X$ with largest eigenvalue. This then produces a projected
    dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，当$\mathbf{w}$是矩阵$X^{T}X$中具有最大特征值的特征向量时，达到最优值。这将产生一个投影数据集：
- en: '|  | $\widehat{X}_{1}=X^{T}W_{1}=X^{T}\mathbf{w_{1}}$ |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{X}_{1}=X^{T}W_{1}=X^{T}\mathbf{w_{1}}$ |  |'
- en: 'The algorithm can then compute iteratively the second and further components
    by first subtracting the components already computed from $X$:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，算法可以通过首先从$X$中减去已经计算的组件，迭代计算第二个及更多组件：
- en: '|  | $X-X\mathbf{w_{1}}\mathbf{w_{1}}^{T}$ |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | $X-X\mathbf{w_{1}}\mathbf{w_{1}}^{T}$ |  |'
- en: and then proceed as before. However, it turns out that all subsequent components
    are related to the eigenvectors of the matrix $X^{T}X$, that is, the $d$-th weight
    vector is the eigenvector of $X^{T}X$ with the $d$-th largest corresponding eigenvalue.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后像之前一样继续。然而，事实证明，所有后续组件都与矩阵$X^{T}X$的特征向量有关，即，第$d$个权重向量是矩阵$X^{T}X$的第$d$大特征值对应的特征向量。
- en: 'The encoding matrix for distributed representations derived with a PCA method
    is the matrix:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA方法得到的分布表示的编码矩阵是矩阵：
- en: '|  | <math   alttext="W_{d}=\left[\begin{array}[]{c}\mathbf{w}_{1}\\ \mathbf{w}_{2}\\'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="W_{d}=\left[\begin{array}[]{c}\mathbf{w}_{1}\\ \mathbf{w}_{2}\\'
- en: \ldots\\
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: \ldots\\
- en: \mathbf{w}_{d}\end{array}\right]\in\mathbb{R}^{d\times n}" display="block"><semantics
    ><mrow  ><msub ><mi >W</mi><mi  >d</mi></msub><mo >=</mo><mrow ><mo  >[</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><msub ><mi >𝐰</mi><mn  >1</mn></msub></mtd></mtr><mtr
    ><mtd  ><msub ><mi >𝐰</mi><mn  >2</mn></msub></mtd></mtr><mtr ><mtd  ><mi mathvariant="normal"  >…</mi></mtd></mtr><mtr
    ><mtd  ><msub ><mi >𝐰</mi><mi  >d</mi></msub></mtd></mtr></mtable><mo >]</mo></mrow><mo
    >∈</mo><msup  ><mi >ℝ</mi><mrow ><mi  >d</mi><mo lspace="0.222em" rspace="0.222em"  >×</mo><mi
    >n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑊</ci><ci >𝑑</ci></apply><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐰</ci><cn type="integer" >1</cn></apply></matrixrow><matrixrow ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐰</ci><cn type="integer" >2</cn></apply></matrixrow><matrixrow
    ><ci  >…</ci></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐰</ci><ci  >𝑑</ci></apply></matrixrow></matrix></apply></apply><apply ><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ℝ</ci><apply ><ci  >𝑑</ci><ci
    >𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >W_{d}=\left[\begin{array}[]{c}\mathbf{w}_{1}\\ \mathbf{w}_{2}\\ \ldots\\ \mathbf{w}_{d}\end{array}\right]\in\mathbb{R}^{d\times
    n}</annotation></semantics></math> |  |
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{w}_{d}\end{array}\right]\in\mathbb{R}^{d\times n}" display="block"><semantics
    ><mrow  ><msub ><mi >W</mi><mi  >d</mi></msub><mo >=</mo><mrow ><mo  >[</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><msub ><mi >𝐰</mi><mn  >1</mn></msub></mtd></mtr><mtr
    ><mtd  ><msub ><mi >𝐰</mi><mn  >2</mn></msub></mtd></mtr><mtr ><mtd  ><mi mathvariant="normal"  >…</mi></mtd></mtr><mtr
    ><mtd  ><msub ><mi >𝐰</mi><mi  >d</mi></msub></mtd></mtr></mtable><mo >]</mo></mrow><mo
    >∈</mo><msup  ><mi >ℝ</mi><mrow ><mi  >d</mi><mo lspace="0.222em" rspace="0.222em"  >×</mo><mi
    >n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑊</ci><ci >𝑑</ci></apply><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix ><matrixrow ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    xref="S6.Ex21.m1
- en: 'where $\mathbf{w}_{i}$ are eigenvectors with eigenvalues decreasing with $i$.
    Hence, local representations $\mathbf{v}\in\mathbb{R}^{n}$ are represented in
    distributed representations in $\mathbb{R}^{d}$ as:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{w}_{i}$ 是特征向量，其特征值随 $i$ 递减。因此，局部表示 $\mathbf{v}\in\mathbb{R}^{n}$
    表示为分布式表示 $\mathbb{R}^{d}$：
- en: '|  | $\eta(\mathbf{v})=W_{d}\mathbf{v}$ |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eta(\mathbf{v})=W_{d}\mathbf{v}$ |  |'
- en: Hence, vectors $\eta(\mathbf{v})$ are *human-interpretable* as their dimensions
    represent linear combinations of dimensions in the original local representation
    and these dimensions are ordered according to their importance in the dataset,
    that is, their variance. Moreover, each dimension is a linear combination of the
    original symbols. Then, the matrix $W_{d}$ reports on which combination of the
    original symbols is more important to distinguish data points in the set.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，向量 $\eta(\mathbf{v})$ 是*人类可解释的*，因为它们的维度代表了原始局部表示中维度的线性组合，这些维度根据在数据集中的重要性进行排序，即它们的方差。此外，每个维度是原始符号的线性组合。因此，矩阵
    $W_{d}$ 报告了哪些原始符号的组合在区分数据点时更为重要。
- en: 'Moreover, vectors $\eta(\mathbf{v})$ are *decodable*. The decoding function
    is:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，向量 $\eta(\mathbf{v})$ 是*可解码的*。解码函数为：
- en: '|  | $\delta(\mathbf{v^{\prime}})=W_{d}^{T}\mathbf{v^{\prime}}$ |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta(\mathbf{v^{\prime}})=W_{d}^{T}\mathbf{v^{\prime}}$ |  |'
- en: and $W_{d}^{T}W_{d}=I$ if $d$ is the rank of the matrix $X$, otherwise it is
    a degraded approximation (for more details refer to (Fodor, [2002](#bib.bib23);
    Sorzano et al., [2014](#bib.bib66))). Hence, distributed vectors in $\mathbb{R}^{d}$
    can be decoded back in the original symbolic representation with a degree of approximation
    that depends on the distance between $d$ and the rank of the matrix $X$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 $W_{d}^{T}W_{d}=I$ 如果 $d$ 是矩阵 $X$ 的秩，否则它是退化的近似（有关更多详细信息，请参见 (Fodor, [2002](#bib.bib23);
    Sorzano et al., [2014](#bib.bib66) )）。因此，分布式向量在 $\mathbb{R}^{d}$ 中可以以一定程度的近似解码回原始符号表示，这取决于
    $d$ 与矩阵 $X$ 的秩之间的距离。
- en: The compelling limit of PCA is that all the data points have to be used in order
    to obtain the encoding/decoding matrices. This is not feasible in two cases. First,
    when the model has to deal with big data. Second, when the set of symbols to be
    encoded in extremely large. In this latter case, local representations cannot
    be used to produce matrices $X$ for applying PCA.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的一个重要限制是必须使用所有数据点来获得编码/解码矩阵。这在两种情况下不可行。首先，当模型需要处理大数据时。其次，当需要编码的符号集合极其庞大时。在后一种情况下，局部表示不能用来生成矩阵
    $X$ 以应用 PCA。
- en: 'In Distributional Semantics, *random indexing* has been used to solve some
    issues that arise naturally with PCA when working with large vocabularies and
    large corpora. PCA has some scalability problems:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式语义中，*随机索引* 已被用来解决在处理大词汇表和大语料库时自然出现的一些问题。PCA 存在一些可扩展性问题：
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The original co-occurrence matrix is very costly to obtain and store, moreover,
    it is only needed to be later transformed;
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始共现矩阵的获取和存储成本很高，而且，后来仅需转换；
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dimensionality reduction is also very costly, moreover, with the dimensions
    at hand it can only be done with iterative methods;
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 降维也是非常昂贵的，而且，考虑到当前的维度，只能通过迭代方法来实现；
- en: •
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The entire method is not incremental, if we want to add new words to our corpus
    we have to recompute the entire co-occurrence matrix and then re-perform the PCA
    step.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 整个方法不是增量的，如果我们要将新词添加到语料库中，我们必须重新计算整个共现矩阵，然后重新执行 PCA 步骤。
- en: 'Random Indexing (Sahlgren, [2005](#bib.bib60)) solves these problems: it is
    an incremental method (new words can be easily added any time at low computational
    cost) which creates word vector of reduced dimension without the need to create
    the full dimensional matrix.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 随机索引（Sahlgren, [2005](#bib.bib60)）解决了这些问题：它是一种增量方法（可以在任何时间以低计算成本轻松添加新词），生成减少维度的词向量，而无需创建完整维度的矩阵。
- en: Interpretability of compacted distributional semantic vectors is comparable
    to the interpretability of distributed representations obtained with the same
    techniques.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩的分布式语义向量的可解释性与使用相同技术获得的分布式表示的可解释性相当。
- en: '6.3 Learning representations: word2vec'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 学习表示：word2vec
- en: '![Refer to caption](img/45c0fc96cdbd9e87392d359d3b4dd9f1.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/45c0fc96cdbd9e87392d359d3b4dd9f1.png)'
- en: 'Figure 1: word2vec: CBOW model'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1：word2vec: CBOW 模型'
- en: 'Recently, *distributional hypothesis* has invaded neural networks: *word2vec*
    (Mikolov et al., [2013](#bib.bib49)) uses contextual information to learn word
    vectors. Hence, we discuss this technique in the section devoted to *distributional
    semantics*.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，*分布假设*已侵入神经网络：*word2vec*（Mikolov等，[2013](#bib.bib49)）使用上下文信息来学习单词向量。因此，我们在*分布语义*部分讨论这一技术。
- en: The name word2Vec comprises two similar techniques, called *skip grams* and
    *continuous bag of words* (CBOW). Both methods are neural networks, the former
    takes input a word and try to predict its context, while the latter does the reverse
    process, predicting a word from the words surrounding it. With this technique
    there is no explicitly computed co-occurrence matrix, and neither there is an
    explicit association feature between pairs of words, instead, the regularities
    and distribution of the words are learned implicitly by the network.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: word2Vec这个名字包括两种相似的技术，称为*skip grams*和*连续词袋*（CBOW）。这两种方法都是神经网络，前者以一个单词为输入并尝试预测其上下文，而后者则反向处理，从周围单词预测一个单词。使用这种技术，没有显式计算的共现矩阵，也没有显式的单词对关联特征，相反，单词的规律性和分布是由网络隐式学习的。
- en: 'We describe only CBOW because it is conceptually simpler and because the core
    ideas are the same in both cases. The full network is generally realized with
    two layers $W1_{n\times k}$ and $W2_{k\times n}$ plus a softmax layer to reconstruct
    the final vector representing the word. In the learning phase, the input and the
    output of the network are local representation for words. In CBOW, the network
    aims to predict a target word given context words. For example, given the sentence
    $s_{1}$ of the corpus in Table [1](#S6.T1 "Table 1 ‣ 6.1 Building distributional
    representations for words from a corpus ‣ 6 Distributional Representations as
    another side of the coin ‣ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey"), the network
    has to predict *catches* given its context (see Figure [1](#S6.F1 "Figure 1 ‣
    6.3 Learning representations: word2vec ‣ 6 Distributional Representations as another
    side of the coin ‣ Symbolic, Distributed and Distributional Representations for
    Natural Language Processing in the Era of Deep Learning: a Survey")).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '我们仅描述CBOW，因为它在概念上更简单，而且两者的核心思想是相同的。完整的网络通常由两个层$W1_{n\times k}$和$W2_{k\times
    n}$以及一个softmax层来重建表示单词的最终向量。在学习阶段，网络的输入和输出是单词的局部表示。在CBOW中，网络旨在根据上下文单词预测目标单词。例如，给定表格[1](#S6.T1
    "Table 1 ‣ 6.1 Building distributional representations for words from a corpus
    ‣ 6 Distributional Representations as another side of the coin ‣ Symbolic, Distributed
    and Distributional Representations for Natural Language Processing in the Era
    of Deep Learning: a Survey")中的句子$s_{1}$，网络必须根据上下文预测*catches*（见图[1](#S6.F1 "Figure
    1 ‣ 6.3 Learning representations: word2vec ‣ 6 Distributional Representations
    as another side of the coin ‣ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey")）。'
- en: Hence, CBOW offers an encoder $W1_{n\times k}$, that is, a linear word encoder
    from data where $n$ is the size of the vocabulary and $k$ is the size of the distributional
    vector. This encoder models contextual information learned by maximizing the prediction
    capability of the network. A nice description on how this approach is related
    to previous techniques is given in (Goldberg and Levy, [2014](#bib.bib28)).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CBOW提供了一个编码器$W1_{n\times k}$，即从数据中线性编码单词，其中$n$是词汇表的大小，$k$是分布式向量的大小。该编码器通过最大化网络的预测能力来建模上下文信息。关于这种方法与以往技术的关系的详细描述见（Goldberg和Levy，[2014](#bib.bib28)）。
- en: 'Clearly, CBOW distributional vectors are not easily human and machine *interpretable*.
    In fact, specific dimensions of vectors have not a particular meaning and, differently
    from what happens for auto-encoders (see Sec. [5.2.1](#S5.SS2.SSS1 "5.2.1 Autoencoder
    ‣ 5.2 Learned representation ‣ 5 Strategies to obtain distributed representations
    from symbols ‣ Symbolic, Distributed and Distributional Representations for Natural
    Language Processing in the Era of Deep Learning: a Survey")), these networks are
    not trained to be invertible.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '显然，CBOW分布式向量既不容易被人类也不容易被机器*解释*。实际上，向量的特定维度没有特定的含义，并且与自动编码器不同（见第[5.2.1节](#S5.SS2.SSS1
    "5.2.1 Autoencoder ‣ 5.2 Learned representation ‣ 5 Strategies to obtain distributed
    representations from symbols ‣ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey")），这些网络并未经过可逆性训练。'
- en: 7 Composing distributed representations
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 组合分布式表示
- en: In the previous sections, we described how one symbol or a bag-of-symbols can
    be transformed in distributed representations focusing on whether these distributed
    representations are *interpretable*. In this section, we want to investigate a
    second and important aspect of these representations, that is, have these representations
    *Concatenative Compositionality* as symbolic representations? And, if these representations
    are *composed*, are still *interpretable*?
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们描述了一个符号或符号包如何被转化为分布式表示，重点是这些分布式表示是否是*可解释的*。在这一部分，我们想要探讨这些表示的第二个重要方面，即这些表示是否具有作为符号表示的*连接性组合性*？如果这些表示是*组合的*，它们是否仍然是*可解释的*？
- en: '*Concatenative Compositionality* is the ability of a symbolic representation
    to describe sequences or structures by composing symbols with specific rules.
    In this process, symbols remain distinct and composing rules are clear. Hence,
    final sequences and structures can be used for subsequent steps as knowledge repositories.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*连接性组合性*是指符号表示通过按照特定规则组合符号来描述序列或结构的能力。在这个过程中，符号保持独立，组合规则也很明确。因此，最终的序列和结构可以作为知识库用于后续步骤。'
- en: '*Concatenative Compositionality* is an important aspect for any representation
    and, then, for a distributed representation. Understanding to what extent a distributed
    representation has *concatenative compositionality* and how information can be
    recovered is then a critical issue. In fact, this issue has been strongly posed
    by Plate (Plate, [1995](#bib.bib57), [1994](#bib.bib56)) who analyzed how same
    specific distributed representations encode structural information and how this
    structural information can be recovered back.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*连接性组合性*是任何表示中的一个重要方面，因此也是分布式表示中的一个重要方面。理解一个分布式表示在多大程度上具备*连接性组合性*以及如何恢复信息是一个关键问题。事实上，Plate（Plate,
    [1995](#bib.bib57), [1994](#bib.bib56)）已经强烈提出了这个问题，他分析了相同的特定分布式表示如何编码结构信息以及这些结构信息如何被恢复。'
- en: 'Current approaches for treating distributed/distributional representation of
    sequences and structures mix two aspects in one model: a *“semantic”* aspect and
    a *representational* aspect. Generally, the semantic aspect is the predominant
    and the representational aspect is left aside. For *“semantic”* aspect, we refer
    to the reason why distributed symbols are composed: a final task in neural network
    applications or the need to give a *distributional semantic vector* for sequences
    of words. This latter is the case for *compositional distributional semantics*
    (Clark et al., [2008](#bib.bib14); Baroni et al., [2014](#bib.bib4)). For the
    *representational* aspect, we refer to the fact that composed distributed representations
    are in fact representing structures and these representations can be decoded back
    in order to extract what is in these structures.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当前处理序列和结构的分布式/分布表示的方法将*“语义”*方面和*表示*方面混合在一个模型中。通常，语义方面占据主导地位，而表示方面则被忽略。对于*“语义”*方面，我们指的是为什么分布式符号会被组合：这是神经网络应用中的最终任务，或需要为词序列提供*分布语义向量*。后者是*组合分布语义*（Clark
    et al., [2008](#bib.bib14); Baroni et al., [2014](#bib.bib4)）的情况。对于*表示*方面，我们指的是组合的分布式表示实际上表示了结构，并且这些表示可以被解码回来，以提取这些结构中的内容。
- en: Although the *“semantic”* aspect seems to be predominant in *models-that-compose*,
    the *convolution conjecture* (Zanzotto et al., [2015](#bib.bib78)) hypothesizes
    that the two aspects coexist and the *representational* aspect plays always a
    crucial role. According to this conjecture, structural information is preserved
    in any model that composes and structural information emerges back when comparing
    two distributed representations with dot product to determine their similarity.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*“语义”*方面在*组合模型*中似乎占据主导地位，*卷积猜想*（Zanzotto et al., [2015](#bib.bib78)）却假设这两个方面是共存的，并且*表示*方面始终发挥着至关重要的作用。根据这一猜想，在任何组合模型中，结构信息都被保留，当通过点积比较两个分布式表示以确定它们的相似性时，结构信息会重新显现出来。
- en: Hence, given the *convolution conjecture*, *models-that-compose* produce distributed
    representations for structures that can be interpreted back. *Interpretability*
    is a very important feature in these *models-that-compose* which will drive our
    analysis.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到*卷积猜想*，*组合模型*产生的分布式表示可以被解释回来。*可解释性*是这些*组合模型*中的一个非常重要的特征，将引导我们的分析。
- en: In this section we will explore the issues faced with the compositionality of
    representations, and the main “trends”, which correspond somewhat to the categories
    already presented. In particular we will start from the work on compositional
    distributional semantics, then we revise the work on holographic reduced representations
    (Plate, [1995](#bib.bib57); Neumann, [2001](#bib.bib53)) and, finally, we analyze
    the recent approaches with recurrent and recursive neural networks. Again, these
    categories are not entirely disjoint, and methods presented in one class can be
    often interpreted to belonging into another class.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨表示的组合性所面临的问题，以及主要的“趋势”，这些趋势在某种程度上对应于已经呈现的类别。特别是，我们将从组合分布语义的工作开始，然后回顾全息降维表示（Plate,
    [1995](#bib.bib57); Neumann, [2001](#bib.bib53)）的工作，最后分析使用递归和递归神经网络的最新方法。再次强调，这些类别并非完全独立，一个类别中介绍的方法通常可以解释为属于另一个类别。
- en: 7.1 Compositional Distributional Semantics
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 组合分布语义学
- en: In distributional semantics, *models-that-compose* have the name of *compositional
    distributional semantics models* (CDSMs) (Baroni et al., [2014](#bib.bib4); Mitchell
    and Lapata, [2010](#bib.bib51)) and aim to apply the principle of compositionality
    (Frege, [1884](#bib.bib25); Montague, [1974](#bib.bib52)) to compute distributional
    semantic vectors for phrases. These CDSMs produce distributional semantic vectors
    of phrases by composing distributional vectors of words in these phrases. These
    models generally exploit *structured or syntactic representations* of phrases
    to derive their distributional meaning. Hence, CDSMs aim to give a complete semantic
    model for distributional semantics.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布语义学中，*组合模型* 被称为 *组合分布语义模型*（CDSMs）（Baroni et al., [2014](#bib.bib4); Mitchell
    and Lapata, [2010](#bib.bib51)），其目标是应用组合性原则（Frege, [1884](#bib.bib25); Montague,
    [1974](#bib.bib52)）来计算短语的分布语义向量。这些 CDSMs 通过组合短语中词汇的分布向量来生成短语的分布语义向量。这些模型通常利用短语的*结构或句法表示*来推导它们的分布语义。因此，CDSMs
    旨在为分布语义学提供完整的语义模型。
- en: As in distributional semantics for words, the aim of CDSMs is to produce similar
    vectors for semantically similar sentences regardless their lengths or structures.
    For example, words and word definitions in dictionaries should have similar vectors
    as discussed in (Zanzotto et al., [2010](#bib.bib79)). As usual in distributional
    semantics, similarity is captured with dot products (or similar metrics) among
    distributional vectors.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如同词汇的分布语义学一样，CDSMs 的目标是为语义相似的句子生成相似的向量，而不考虑它们的长度或结构。例如，词汇和词典中的词义应该具有类似的向量，正如（Zanzotto
    et al., [2010](#bib.bib79)）中所讨论的那样。与分布语义学中的常规做法一样，相似性是通过分布向量之间的点积（或类似度量）来捕捉的。
- en: The applications of these CDSMs encompass multi-document summarization, recognizing
    textual entailment (Dagan et al., [2013](#bib.bib17)) and, obviously, semantic
    textual similarity detection (Agirre et al., [2013](#bib.bib2)).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 CDSMs 的应用包括多文档总结、识别文本蕴涵（Dagan et al., [2013](#bib.bib17)）以及显然的语义文本相似性检测（Agirre
    et al., [2013](#bib.bib2)）。
- en: Apparently, these CDSMs are far from having *concatenative compositionality*
    , since these distributed representations that can be *interpreted* back. In some
    sense, their nature wants that resulting vectors forget how these are obtained
    and focus on the final distributional meaning of phrases. There is some evidence
    that this is not exactly the case.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些 CDSMs 远未具有*串联组合性*，因为这些分布式表示可以*被解释*回去。在某种意义上，它们的本质要求结果向量忘记它们是如何获得的，专注于短语的最终分布语义。已有一些证据表明，这并非完全如此。
- en: The *convolution conjecture* (Zanzotto et al., [2015](#bib.bib78)) suggests
    that many CDSMs produce distributional vectors where structural information and
    vectors for individual words can be still *interpreted*. Hence, many CDSMs have
    the *concatenative compositionality* property and *interpretable*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积猜想*（Zanzotto et al., [2015](#bib.bib78)）提出，许多 CDSMs 生成的分布向量中，结构信息和单词的向量仍然可以*被解释*。因此，许多
    CDSMs 具有*串联组合性*属性和*可解释性*。'
- en: In the rest of this section, we will show some classes of these CDSMs and we
    focus on describing how these morels are interpretable.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将展示这些 CDSMs 的一些类别，并重点描述这些模型如何具有解释性。
- en: 7.1.1 Additive Models
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1 加性模型
- en: '*Additive models* for compositional distributional semantics are important
    examples of *models-that-composes* where *semantic* and *representational* aspects
    is clearly separated. Hence, these models can be highly *interpretable*.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*加法模型* 对于组合分布语义是 *组合模型* 的重要例子，其中 *语义* 和 *表征* 方面被明确分开。因此，这些模型可以具有很高的 *可解释性*。'
- en: 'These additive models have been formally captured in the general framework
    for two words sequences proposed by Mitchell&Lapata (Mitchell and Lapata, [2008](#bib.bib50)).
    The general framework for composing distributional vectors of two word sequences
    *“u v”* is the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这些加法模型已被正式纳入 Mitchell&Lapata 提出的两词序列的通用框架（Mitchell 和 Lapata，[2008](#bib.bib50)）。组合分布向量的通用框架如下：
- en: '|  | $\mathbf{p}=f(\mathbf{u},\mathbf{v};R;K)$ |  | (3) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{p}=f(\mathbf{u},\mathbf{v};R;K)$ |  | (3) |'
- en: 'where $\mathbf{p}\in\mathbb{R}^{n}$ is the composition vector, $\mathbf{u}$
    and $\mathbf{v}$ are the vectors for the two words *u* and *v*, $R$ is the grammatical
    relation linking the two words and $K$ is any other additional knowledge used
    in the composition operation. In the additive model, this equation has the following
    form:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{p}\in\mathbb{R}^{n}$ 是组合向量，$\mathbf{u}$ 和 $\mathbf{v}$ 是两个词 *u*
    和 *v* 的向量，$R$ 是连接这两个词的语法关系，$K$ 是在组合操作中使用的任何其他附加知识。在加法模型中，这个方程具有以下形式：
- en: '|  | $\mathbf{p}=f(\mathbf{u},\mathbf{v};R;K)=A_{R}\mathbf{u}+B_{R}\mathbf{v}$
    |  | (4) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{p}=f(\mathbf{u},\mathbf{v};R;K)=A_{R}\mathbf{u}+B_{R}\mathbf{v}$
    |  | (4) |'
- en: where $A_{R}$ and $B_{R}$ are two square matrices depending on the grammatical
    relation $R$ which may be learned from data (Zanzotto et al., [2010](#bib.bib79);
    Guevara, [2010](#bib.bib32)).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A_{R}$ 和 $B_{R}$ 是两个依赖于语法关系 $R$ 的方阵，这些方阵可以通过数据进行学习（Zanzotto 等，[2010](#bib.bib79);
    Guevara，[2010](#bib.bib32)）。
- en: 'Before investigating if these models are interpretable, let introduce a recursive
    formulation of additive models which can be applied to structural representations
    of sentences. For this purpose, we use dependency trees. A dependency tree can
    be defined as a tree whose nodes are words and the typed links are the relations
    between two words. The root of the tree represents the word that governs the meaning
    of the sentence. A dependency tree $T$ is then a word if it is a final node or
    it has a root $r_{T}$ and links $(r_{T},R,C_{i})$ where $C_{i}$ is the i-th subtree
    of the node $r_{T}$ and $R$ is the relation that links the node $r_{T}$ with $C_{i}$.
    The dependency trees of two example sentences are reported in Figure [2](#S7.F2
    "Figure 2 ‣ 7.1.1 Additive Models ‣ 7.1 Compositional Distributional Semantics
    ‣ 7 Composing distributed representations ‣ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey"). The recursive formulation is then the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨这些模型是否具有可解释性之前，我们先引入一个可以应用于句子结构表示的加法模型的递归表述。为此，我们使用依赖树。依赖树可以定义为一个节点为词的树，类型化的链接是两个词之间的关系。树的根代表支配句子意义的词。一个依赖树
    $T$ 是一个词，如果它是最终节点或者它具有根 $r_{T}$ 和链接 $(r_{T},R,C_{i})$，其中 $C_{i}$ 是节点 $r_{T}$ 的第
    $i$ 个子树，$R$ 是将节点 $r_{T}$ 与 $C_{i}$ 连接的关系。两个示例句子的依赖树如图 [2](#S7.F2 "图 2 ‣ 7.1.1
    加法模型 ‣ 7.1 组合分布语义 ‣ 7 组合分布表示 ‣ 深度学习时代自然语言处理的符号、分布式和分布表示：综述") 中所示。递归表述如下：
- en: '|  | $f_{r}(T)=\sum_{i}(A_{R}\mathbf{r_{T}}+B_{R}f_{r}(C_{i}))$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{r}(T)=\sum_{i}(A_{R}\mathbf{r_{T}}+B_{R}f_{r}(C_{i}))$ |  |'
- en: 'According to the recursive definition of the additive model, the function $f_{r}(T)$
    results in a linear combination of elements $M_{s}\mathbf{w}_{s}$ where $M_{s}$
    is a product of matrices that *represents the structure* and $\mathbf{w}_{s}$
    is the *distributional meaning* of one word in this structure, that is:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 根据加法模型的递归定义，函数 $f_{r}(T)$ 结果是元素 $M_{s}\mathbf{w}_{s}$ 的线性组合，其中 $M_{s}$ 是 *表示结构*
    的矩阵乘积，而 $\mathbf{w}_{s}$ 是该结构中一个词的 *分布语义*，即：
- en: '|  | $f_{r}(T)=\sum_{s\in S(T)}\mathbf{M}_{s}\mathbf{w}_{s}$ |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{r}(T)=\sum_{s\in S(T)}\mathbf{M}_{s}\mathbf{w}_{s}$ |  |'
- en: 'where $S(T)$ are the relevant substructures of $T$. In this case, $S(T)$ contains
    the link chains. For example, the first sentence in Fig. [2](#S7.F2 "Figure 2
    ‣ 7.1.1 Additive Models ‣ 7.1 Compositional Distributional Semantics ‣ 7 Composing
    distributed representations ‣ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey") has a
    distributed vector defined in this way:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '$S(T)$ 是 $T$ 的相关子结构。在这种情况下，$S(T)$ 包含链式链接。例如，图中的第一句话 [2](#S7.F2 "Figure 2 ‣
    7.1.1 Additive Models ‣ 7.1 Compositional Distributional Semantics ‣ 7 Composing
    distributed representations ‣ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey") 中的分布式向量是这样定义的：'
- en: '|  | $\displaystyle f_{r}(\text{cows eat animal extracts})=$ |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{r}(\text{cows eat animal extracts})=$ |  |'
- en: '|  | $\displaystyle=A_{VN}\mathbf{eat}+B_{VN}\mathbf{cows}+A_{VN}\mathbf{eat}+$
    |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=A_{VN}\mathbf{eat}+B_{VN}\mathbf{cows}+A_{VN}\mathbf{eat}+$
    |  |'
- en: '|  | $\displaystyle+B_{VN}f_{r}(\text{animal extracts})=$ |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+B_{VN}f_{r}(\text{animal extracts})=$ |  |'
- en: '|  | $\displaystyle=A_{VN}\mathbf{eat}+B_{VN}\mathbf{cows}+A_{VN}\mathbf{eat}+$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=A_{VN}\mathbf{eat}+B_{VN}\mathbf{cows}+A_{VN}\mathbf{eat}+$
    |  |'
- en: '|  | $\displaystyle+B_{VN}A_{NN}\mathbf{extracts}+B_{VN}B_{NN}\mathbf{animal}$
    |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+B_{VN}A_{NN}\mathbf{extracts}+B_{VN}B_{NN}\mathbf{animal}$
    |  |'
- en: 'Each term of the sum has a part that represents the structure and a part that
    represents the meaning, for example:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 和的每一项都有一部分表示结构，一部分表示意义，例如：
- en: '|  | $\overbrace{B_{VN}B_{NN}}^{structure}\underbrace{\mathbf{beef}}_{meaning}$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\overbrace{B_{VN}B_{NN}}^{structure}\underbrace{\mathbf{beef}}_{meaning}$
    |  |'
- en: 'Hence, this recursive additive model for compositional semantics is a *model-that-composes*
    which, in principle, can be highly *interpretable*. By selecting matrices $\mathbf{M}_{s}$
    such that:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种用于组合语义的递归加法模型是一个 *组合模型*，原则上可以高度 *可解释*。通过选择矩阵 $\mathbf{M}_{s}$，使得：
- en: '|  | <math   alttext="\mathbf{M}_{s_{1}}^{T}\mathbf{M}_{s_{2}}\approx\begin{cases}\mathbf{I}&amp;s_{1}=s_{2}\\
    \mathbf{0}&amp;s_{1}\neq s_{2}\\'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathbf{M}_{s_{1}}^{T}\mathbf{M}_{s_{2}}\approx\begin{cases}\mathbf{I}&amp;s_{1}=s_{2}\\
    \mathbf{0}&amp;s_{1}\neq s_{2}\\'
- en: \end{cases}" display="block"><semantics ><mrow ><mrow  ><msubsup ><mi >𝐌</mi><msub  ><mi
    >s</mi><mn >1</mn></msub><mi  >T</mi></msubsup><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >𝐌</mi><msub  ><mi >s</mi><mn >2</mn></msub></msub></mrow><mo >≈</mo><mrow  ><mo
    >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mi >𝐈</mi></mtd><mtd  columnalign="left" ><mrow ><msub  ><mi
    >s</mi><mn >1</mn></msub><mo >=</mo><msub  ><mi >s</mi><mn >2</mn></msub></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mn  >𝟎</mn></mtd><mtd columnalign="left"  ><mrow ><msub
    ><mi  >s</mi><mn >1</mn></msub><mo >≠</mo><msub ><mi  >s</mi><mn >2</mn></msub></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐌</ci><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><cn type="integer" >1</cn></apply></apply><ci >𝑇</ci></apply><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐌</ci><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑠</ci><cn type="integer" >2</cn></apply></apply></apply><apply
    ><csymbol cd="latexml"  >cases</csymbol><ci >𝐈</ci><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑠</ci><cn type="integer" >1</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑠</ci><cn type="integer" >2</cn></apply></apply><cn type="integer"  >0</cn><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><cn type="integer"
    >1</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑠</ci><cn
    type="integer" >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{M}_{s_{1}}^{T}\mathbf{M}_{s_{2}}\approx\begin{cases}\mathbf{I}&s_{1}=s_{2}\\
    \mathbf{0}&s_{1}\neq s_{2}\\ \end{cases}</annotation></semantics></math> |  |
    (5) |
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: \end{cases}" display="block"><semantics ><mrow ><mrow  ><msubsup ><mi >𝐌</mi><msub  ><mi
    >s</mi><mn >1</mn></msub><mi  >T</mi></msubsup><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >𝐌</mi><msub  ><mi >s</mi><mn >2</mn></msub></msub></mrow><mo >≈</mo><mrow  ><mo
    >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mi >𝐈</mi></mtd><mtd  columnalign="left" ><mrow ><msub  ><mi
    >s</mi><mn >1</mn></msub><mo >=</mo><msub  ><mi >s</mi><mn >2</mn></msub></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mn  >𝟎</mn></mtd><mtd columnalign="left"  ><mrow ><msub
    ><mi  >s</mi><mn >1</mn></msub><mo >≠</mo><msub ><mi  xref="S7.E5.m1.4.4
- en: 'it is possible to recover distributional semantic vectors related to words
    that are in specific parts of the structure. For example, the main verb of the
    sample sentence in Fig. [2](#S7.F2 "Figure 2 ‣ 7.1.1 Additive Models ‣ 7.1 Compositional
    Distributional Semantics ‣ 7 Composing distributed representations ‣ Symbolic,
    Distributed and Distributional Representations for Natural Language Processing
    in the Era of Deep Learning: a Survey") with a matrix $A_{VN}^{T}$, that is:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '可以恢复与结构中特定部分的单词相关的分布式语义向量。例如，图[2](#S7.F2 "Figure 2 ‣ 7.1.1 Additive Models
    ‣ 7.1 Compositional Distributional Semantics ‣ 7 Composing distributed representations
    ‣ Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey")中样本句子的主要动词与矩阵$A_{VN}^{T}$，即：'
- en: '|  | $A_{VN}^{T}f_{r}(\text{cows eat animal extracts})\approx 2\mathbf{eat}$
    |  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{VN}^{T}f_{r}(\text{cows eat animal extracts})\approx 2\mathbf{eat}$
    |  |'
- en: In general, matrices derived for compositional distributional semantic models
    (Guevara, [2010](#bib.bib32); Zanzotto et al., [2010](#bib.bib79)) do not have
    this property but it is possible to obtain matrices with this property by applying
    thee Jonson-Linderstrauss Tranform (Johnson and Lindenstrauss, [1984](#bib.bib39))
    or similar techniques as discussed also in (Zanzotto et al., [2015](#bib.bib78)).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，为组合分布语义模型（Guevara, [2010](#bib.bib32); Zanzotto et al., [2010](#bib.bib79)）推导出的矩阵并不具备这种属性，但可以通过应用乔恩逊-林登施特劳斯变换（Johnson
    and Lindenstrauss, [1984](#bib.bib39)）或类似技术来获得具备这种属性的矩阵，相关讨论也可以参考（Zanzotto et
    al., [2015](#bib.bib78)）。
- en: '| ![Refer to caption](img/e4d6edc6dc130666802976a787b8cbf9.png) |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/e4d6edc6dc130666802976a787b8cbf9.png) |  |'
- en: 'Figure 2: A sentence and its dependency graph'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 一个句子及其依赖图'
- en: 7.1.2 Lexical Functional Compositional Distributional Semantic Models
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2 词汇功能组合分布语义模型
- en: Lexical Functional Models are compositional distributional semantic models where
    words are tensors and each type of word is represented by tensors of different
    order. Composing meaning is then composing these tensors to obtain vectors. These
    models have solid mathematical background linking Lambek pregroup theory, formal
    semantics and distributional semantics (Coecke et al., [2010](#bib.bib15)). Lexical
    Function models are concatenative compositional, yet, in the following, we will
    examine whether these models produce vectors that my be *interpreted*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇功能模型是组合分布语义模型，其中单词是张量，每种单词类型由不同阶的张量表示。组合意义即是组合这些张量以获得向量。这些模型具有坚实的数学基础，将Lambek前群理论、形式语义学和分布式语义学（Coecke
    et al., [2010](#bib.bib15)）联系起来。词汇功能模型是串接组合的，然而，在以下内容中，我们将检查这些模型是否产生可以*解释*的向量。
- en: To determine whether these models produce *interpretable* vectors, we start
    from a simple Lexical Function model applied to two word sequences. This model
    has been largely analyzed in (Baroni and Zamparelli, [2010](#bib.bib6)) as matrices
    were considered better linear models to encode *adjectives*.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定这些模型是否产生*可解释*的向量，我们从应用于两个单词序列的简单词汇功能模型开始。该模型在（Baroni and Zamparelli, [2010](#bib.bib6)）中已经进行了广泛分析，因为矩阵被认为是编码*形容词*的更好线性模型。
- en: 'In Lexical Functional models over two words sequences, there is one of the
    two words which as a tensor of order 2 (that is, a matrix) and one word that is
    represented by a vector. For example, *adjectives* are matrices and nouns are
    vectors (Baroni and Zamparelli, [2010](#bib.bib6)) in adjective-noun sequences.
    Hence, adjective-noun sequences like *“black cat”* or *“white dog”* are represented
    as:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个单词序列的词汇功能模型中，两个单词中的一个有一个2阶的张量（即矩阵），另一个单词则由一个向量表示。例如，在形容词-名词序列中，*形容词*是矩阵，而名词是向量（Baroni
    and Zamparelli, [2010](#bib.bib6)）。因此，像*“黑猫”*或*“白狗”*这样的形容词-名词序列被表示为：
- en: '|  | $f(\text{black cat})=\mathbf{BLACK}\mathbf{cat}$ |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(\text{black cat})=\mathbf{BLACK}\mathbf{cat}$ |  |'
- en: '|  | $f(\text{white dog})=\mathbf{WHITE}\mathbf{dog}$ |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(\text{white dog})=\mathbf{WHITE}\mathbf{dog}$ |  |'
- en: where $\mathbf{BLACK}$ and $\mathbf{WHITE}$ are matrices representing the two
    adjectives and $\mathbf{cat}$ and $\mathbf{dog}$ are the two vectors representing
    the two nouns.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{BLACK}$和$\mathbf{WHITE}$是表示两个形容词的矩阵，而$\mathbf{cat}$和$\mathbf{dog}$是表示两个名词的两个向量。
- en: 'These two words models are *partially interpretable*: knowing the adjective
    it is possible to extract the noun but not vice-versa. In fact, if matrices for
    adjectives are invertible, there is the possibility of extracting which nouns
    has been related to particular adjectives. For example, if $\mathbf{BLACK}$ is
    invertible, the inverse matrix $\mathbf{BLACK}^{-1}$ can be used to extract the
    vector of *cat* from the vector $f(\text{black cat})$:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个词模型是*部分可解释*的：知道形容词可以提取名词，但反之不行。实际上，如果形容词的矩阵是可逆的，则可以提取与特定形容词相关的名词。例如，如果$\mathbf{BLACK}$是可逆的，则可以使用逆矩阵$\mathbf{BLACK}^{-1}$从向量$f(\text{black
    cat})$中提取*cat*的向量：
- en: '|  | $\mathbf{cat}=\mathbf{BLACK}^{-1}f(\text{black cat})$ |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{cat}=\mathbf{BLACK}^{-1}f(\text{black cat})$ |  |'
- en: 'This contributes to the *interpretability* of this model. Moreover, if matrices
    for adjectives are built using Jonson-Lindestrauss Transforms (Johnson and Lindenstrauss,
    [1984](#bib.bib39)), that is matrices with the property in Eq. [5](#S7.E5 "In
    7.1.1 Additive Models ‣ 7.1 Compositional Distributional Semantics ‣ 7 Composing
    distributed representations ‣ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey"), it is
    possible to pack different pieces of sentences in a single vector and, then, select
    only relevant information, for example:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于该模型的*可解释性*。此外，如果形容词的矩阵是使用Jonson-Lindestrauss变换（Johnson和Lindenstrauss，[1984](#bib.bib39)）构建的，即具有Eq.
    [5](#S7.E5 "在7.1.1 加法模型 ‣ 7.1 组合分布语义 ‣ 7 组合分布式表示 ‣ 深度学习时代自然语言处理的符号、分布式和分布表示：综述")中属性的矩阵，则可以将句子的不同片段打包到一个向量中，然后仅选择相关信息，例如：
- en: '|  | $\mathbf{cat}\approx\mathbf{BLACK}^{T}(f(\text{black cat})+f(\text{white
    dog}))$ |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{cat}\approx\mathbf{BLACK}^{T}(f(\text{black cat})+f(\text{white
    dog}))$ |  |'
- en: On the contrary, knowing noun vectors, it is not possible to extract back adjective
    matrices. This is a strong limitation in term of interpretability.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，知道名词向量，不可能提取回形容词矩阵。这是解释性方面的一个重大限制。
- en: Lexical Functional models for larger structures are concatenative compositional
    but not interpretable at all. In fact, in general these models have tensors in
    the middle and these tensors are the only parts that can be inverted. Hence, in
    general these models are not interpretable. However, using the *convolution conjecture*
    (Zanzotto et al., [2015](#bib.bib78)), it is possible to know whether subparts
    are contained in some final vectors obtained with these models.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 用于较大结构的词汇功能模型是连接组合的，但完全不可解释。实际上，这些模型通常在中间有张量，这些张量是唯一可以被反转的部分。因此，通常这些模型是不可解释的。然而，通过使用*卷积猜想*（Zanzotto等，[2015](#bib.bib78)），可以知道是否某些子部分包含在这些模型获得的最终向量中。
- en: 7.2 Holographic Representations
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 全息表示
- en: Holographic reduced representations (HRRs) are *models-that-compose* expressly
    designed to be *interpretable* (Plate, [1995](#bib.bib57); Neumann, [2001](#bib.bib53)).
    In fact, these models to encode flat structures representing assertions and these
    assertions should be then searched in oder to recover pieces of knowledge that
    is in. For example, these representations have been used to encode logical propositions
    such as $eat(John,apple)$. In this case, each atomic element has an associated
    vector and the vector for the compound is obtained by combining these vectors.
    The major concern here is to build encoding functions that can be decoded, that
    is, it should be possible to retrieve composing elements from final distributed
    vectors such as the vector of $eat(John,apple)$.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 全息缩减表示（HRRs）是*模型-组合*，专门设计为*可解释*的（Plate，[1995](#bib.bib57)；Neumann，[2001](#bib.bib53)）。实际上，这些模型用于编码表示断言的平面结构，并且这些断言应被搜索以恢复其中的知识。例如，这些表示被用来编码逻辑命题，如$eat(John,apple)$。在这种情况下，每个原子元素都有一个相关的向量，复合体的向量通过组合这些向量获得。这里主要的关注点是构建可以解码的编码函数，即应该能够从最终的分布式向量中检索组成元素，例如$eat(John,apple)$的向量。
- en: 'In HRRs, *nearly orthogonal unit vectors* (Johnson and Lindenstrauss, [1984](#bib.bib39))
    for basic symbols, *circular convolution* $\otimes$ and *circular correlation*
    $\oplus$ guarantees *composability* and *intepretability*. HRRs are the extension
    of Random Indexing (see Sec. [5.1](#S5.SS1 "5.1 Dimensionality reductio with Random
    Projections ‣ 5 Strategies to obtain distributed representations from symbols
    ‣ Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey")) to structures. Hence, symbols
    are represented with vectors sampled from a multivariate normal distribution $N(0,\frac{1}{d}I_{d})$.
    The composition function is the circular convolution indicated as $\otimes$ and
    defined as:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '在 HRRs 中，*几乎正交的单位向量*（Johnson 和 Lindenstrauss，[1984](#bib.bib39)）用于基本符号，*循环卷积*
    $\otimes$ 和 *循环相关* $\oplus$ 确保了 *可组合性* 和 *可解释性*。HRRs 是对随机索引的扩展（见 Sec. [5.1](#S5.SS1
    "5.1 Dimensionality reductio with Random Projections ‣ 5 Strategies to obtain
    distributed representations from symbols ‣ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey")）。因此，符号通过从多元正态分布 $N(0,\frac{1}{d}I_{d})$ 中采样的向量进行表示。组合函数是循环卷积，表示为 $\otimes$
    并定义为：'
- en: '|  | $z_{j}=(\mathbf{a}\otimes\mathbf{b})_{j}=\sum_{k=0}^{d-1}a_{k}b_{j-k}$
    |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{j}=(\mathbf{a}\otimes\mathbf{b})_{j}=\sum_{k=0}^{d-1}a_{k}b_{j-k}$
    |  |'
- en: 'where subscripts are modulo $d$. Circular convolution is commutative and bilinear.
    This operation can be also computed using *circulant matrices*:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 其中下标取模 $d$。循环卷积是可交换且双线性的。这一操作也可以通过*circular matrices*计算：
- en: '|  | $\mathbf{z}=(\mathbf{a}\otimes\mathbf{b})=\mathbf{A}_{\circ}\mathbf{b}=\mathbf{B}_{\circ}\mathbf{a}$
    |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{z}=(\mathbf{a}\otimes\mathbf{b})=\mathbf{A}_{\circ}\mathbf{b}=\mathbf{B}_{\circ}\mathbf{a}$
    |  |'
- en: 'where $\mathbf{A}_{\circ}$ and $\mathbf{B}_{\circ}$ are circulant matrices
    of the vectors $\mathbf{a}$ and $\mathbf{b}$. Given the properties of vectors
    $\mathbf{a}$ and $\mathbf{b}$, matrices $\mathbf{A}_{\circ}$ and $\mathbf{B}_{\circ}$
    have the property in Eq. [5](#S7.E5 "In 7.1.1 Additive Models ‣ 7.1 Compositional
    Distributional Semantics ‣ 7 Composing distributed representations ‣ Symbolic,
    Distributed and Distributional Representations for Natural Language Processing
    in the Era of Deep Learning: a Survey"). Hence, *circular convolution* is approximately
    invertible with the *circular correlation* function ($\oplus$) defined as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{A}_{\circ}$ 和 $\mathbf{B}_{\circ}$ 是向量 $\mathbf{a}$ 和 $\mathbf{b}$
    的循环矩阵。考虑到向量 $\mathbf{a}$ 和 $\mathbf{b}$ 的性质，矩阵 $\mathbf{A}_{\circ}$ 和 $\mathbf{B}_{\circ}$
    具有等式 [5](#S7.E5 "In 7.1.1 Additive Models ‣ 7.1 Compositional Distributional Semantics
    ‣ 7 Composing distributed representations ‣ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey") 中的性质。因此，*循环卷积* 与 *循环相关* 函数 ($\oplus$) 的逆性近似如下：'
- en: '|  | $c_{j}=(\mathbf{z}\oplus\mathbf{b})_{j}=\sum_{k=0}^{d-1}z_{k}b_{j+k}$
    |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{j}=(\mathbf{z}\oplus\mathbf{b})_{j}=\sum_{k=0}^{d-1}z_{k}b_{j+k}$
    |  |'
- en: 'where again subscripts are modulo $d$. Circular correlation is related to inverse
    matrices of circulant matrices, that is $\mathbf{B}_{\circ}^{T}$. In the decoding
    with $\oplus$, parts of the structures can be derived in an approximated way,
    that is:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 其中下标再次取模 $d$。循环相关与循环矩阵的逆矩阵相关，即 $\mathbf{B}_{\circ}^{T}$。在用 $\oplus$ 进行解码时，部分结构可以以近似的方式推导，即：
- en: '|  | $(\mathbf{a}\otimes\mathbf{b})\oplus\mathbf{b}\approx\mathbf{a}$ |  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\mathbf{a}\otimes\mathbf{b})\oplus\mathbf{b}\approx\mathbf{a}$ |  |'
- en: 'Hence, circular convolution $\otimes$ and circular correlation $\oplus$ allow
    to build interpretable representations. For example, having the vectors $\mathbf{e}$,
    $\mathbf{J}$, and $\mathbf{a}$ for $eat$, $John$ and $apple$, respectively, the
    following encoding and decoding produces a vector that approximates the original
    vector for $John$:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，循环卷积 $\otimes$ 和循环相关 $\oplus$ 允许建立可解释的表示。例如，具有向量 $\mathbf{e}$、$\mathbf{J}$
    和 $\mathbf{a}$ 对应于 $eat$、$John$ 和 $apple$，以下编码和解码产生了一个近似于 $John$ 的原始向量的向量：
- en: '|  | $\mathbf{J}\approx(\mathbf{J}\otimes\mathbf{e}\otimes\mathbf{a})\oplus(\mathbf{e}\otimes\mathbf{a})$
    |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{J}\approx(\mathbf{J}\otimes\mathbf{e}\otimes\mathbf{a})\oplus(\mathbf{e}\otimes\mathbf{a})$
    |  |'
- en: The “invertibility” of these representations is important because it allow us
    not to consider these representations as black boxes.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示的“可逆性”很重要，因为它允许我们不将这些表示视为黑箱。
- en: However, holographic representations have severe limitations as these can encode
    and decode simple, flat structures. In fact, these representations are based on
    the circular convolution, which is a commutative function; this implies that the
    representation cannot keep track of composition of objects where the order matters
    and this phenomenon is particularly important when encoding nested structures.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，全息表示有严重的局限性，因为这些表示只能编码和解码简单、平坦的结构。实际上，这些表示基于循环卷积，它是一个交换函数；这意味着表示无法跟踪对象的组成顺序，而这一现象在编码嵌套结构时尤其重要。
- en: Distributed trees (Zanzotto and Dell’Arciprete, [2012](#bib.bib77)) have shown
    that the principles expressed in holographic representation can be applied to
    encode larger structures, overcoming the problem of reliably encoding the order
    in which elements are composed using the *shuffled circular convolution* function
    as the composition operator. Distributed trees are encoding functions that transform
    trees into low-dimensional vectors that also contain the encoding of every substructures
    of the tree. Thus, these distributed trees are particularly attractive as they
    can be used to represent structures in linear learning machines which are computationally
    efficient.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式树（Zanzotto and Dell’Arciprete, [2012](#bib.bib77)）已经展示了全息表示中表达的原则可以用于编码更大结构，克服了使用*洗牌循环卷积*函数作为组成算子时可靠编码元素组成顺序的问题。分布式树是将树转换为低维向量的编码函数，这些向量还包含树中每个子结构的编码。因此，这些分布式树特别有吸引力，因为它们可以用来表示线性学习机器中的结构，这在计算上是高效的。
- en: Distributed trees and, in particular, distributed smoothed trees (Ferrone and
    Zanzotto, [2014](#bib.bib20)) represent an interesting middle way between compositional
    distributional semantic models and holographic representation.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式树，特别是分布式平滑树（Ferrone and Zanzotto, [2014](#bib.bib20)），代表了组成分布式语义模型和全息表示之间的有趣折衷。
- en: 7.3 Compositional Models in Neural Networks
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 神经网络中的组成模型
- en: 'When neural networks are applied to sequences or structured data, these networks
    are in fact *models-that-compose*. However, these models result in *models-that-compose*
    which are not interpretable. In fact, composition functions are trained on specific
    tasks and not on the possibility of reconstructing the structured input, unless
    in some rare cases (Socher et al., [2011](#bib.bib64)). The input of these networks
    are sequences or structured data where basic symbols are embedded in *local* representations
    or *distributed* representations obtained with word embedding (see Sec. [6.3](#S6.SS3
    "6.3 Learning representations: word2vec ‣ 6 Distributional Representations as
    another side of the coin ‣ Symbolic, Distributed and Distributional Representations
    for Natural Language Processing in the Era of Deep Learning: a Survey")). The
    output are distributed vectors derived for specific tasks. Hence, these *models-that-compose*
    are not interpretable in our sense for their final aim and for the fact that *non
    linear* functions are adopted in the specification of the neural networks.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络应用于序列或结构化数据时，这些网络实际上是*组成模型*。然而，这些模型产生的*组成模型*是不可解释的。事实上，组成函数是在特定任务上训练的，而不是为了重建结构化输入，除非在一些少见的情况下（Socher
    et al., [2011](#bib.bib64)）。这些网络的输入是序列或结构化数据，其中基本符号嵌入在*局部*表示或通过词嵌入获得的*分布式*表示中（参见第
    [6.3](#S6.SS3 "6.3 学习表示：word2vec ‣ 6 分布式表示作为另一面 ‣ 符号、分布式和分布式表示在深度学习时代的自然语言处理中的调查")节）。输出是针对特定任务的分布式向量。因此，这些*组成模型*在我们看来不可解释，既因为它们的最终目标，也因为在神经网络的规范中采用了*非线性*函数。
- en: 'In this section, we revise some prominent neural network architectures that
    can be interpreted as *models-that-compose*: the *recurrent neural networks* (Krizhevsky
    et al., [2012](#bib.bib42); He et al., [2016](#bib.bib34); Vinyals et al., [2015a](#bib.bib72);
    Graves, [2013](#bib.bib30)) and the *recursive neural networks* (Socher et al.,
    [2012](#bib.bib65)).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了一些可以被解释为*组成模型*的显著神经网络架构：*递归神经网络*（Krizhevsky et al., [2012](#bib.bib42)；He
    et al., [2016](#bib.bib34)；Vinyals et al., [2015a](#bib.bib72)；Graves, [2013](#bib.bib30)）和*递归神经网络*（Socher
    et al., [2012](#bib.bib65)）。
- en: 7.3.1 Recurrent Neural Networks
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1 递归神经网络
- en: Recurrent neural networks form a very broad family of neural networks architectures
    that deal with the representation (and processing) of complex objects. At its
    core a recurrent neural network (RNN) is a network which takes in input the current
    element in the sequence and processes it based on an internal state which depends
    on previous inputs. At the moment the most powerful network architectures are
    convolutional neural networks (Krizhevsky et al., [2012](#bib.bib42); He et al.,
    [2016](#bib.bib34)) for vision related tasks and LSTM-type network for language
    related task (Vinyals et al., [2015a](#bib.bib72); Graves, [2013](#bib.bib30)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络构成了一个非常广泛的神经网络架构家族，处理复杂对象的表示（和处理）。在其核心，循环神经网络（RNN）是一个网络，它接收序列中的当前元素作为输入，并根据依赖于先前输入的内部状态进行处理。目前，最强大的网络架构是用于视觉相关任务的卷积神经网络（Krizhevsky
    et al., [2012](#bib.bib42); He et al., [2016](#bib.bib34)）和用于语言相关任务的LSTM类型网络（Vinyals
    et al., [2015a](#bib.bib72); Graves, [2013](#bib.bib30)）。
- en: A recurrent neural network takes as input a sequence $\mathbf{x}=\left(\mathbf{x_{1}}\
    \ldots\ \mathbf{x_{n}}\right)$ and produce as output a single vector $\mathbf{y}\in\mathbb{R}^{n}$
    which is a representation of the entire sequence. At each step ¹¹1we can usually
    think of this as a timestep, but not all applications of recurrent neural network
    have a temporal interpretation $t$ the network takes as input the current element
    $\mathbf{x_{t}}$, the previous output $\mathbf{h_{t-1}}$ and performs the following
    operation to produce the current output $\mathbf{h_{t}}$
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络将序列$\mathbf{x}=\left(\mathbf{x_{1}}\ \ldots\ \mathbf{x_{n}}\right)$作为输入，并生成一个单一的向量$\mathbf{y}\in\mathbb{R}^{n}$作为输出，表示整个序列。在每一步¹¹1，我们通常可以将其视为一个时间步，但并非所有循环神经网络的应用都有时间解释$t$。网络以当前元素$\mathbf{x_{t}}$、先前输出$\mathbf{h_{t-1}}$作为输入，并执行以下操作以生成当前输出$\mathbf{h_{t}}$。
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b)$ |  | (6) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t}$ | $\displaystyle=$ | $\displaystyle\sigma(W\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b)$ |  | (6) |'
- en: where $\sigma$ is a non-linear function such as the logistic function or the
    hyperbolic tangent and $\left[\mathbf{h_{t-1}}\ \mathbf{x_{t}}\right]$ denotes
    the concatenation of the vectors $\mathbf{h_{t-1}}$ and $\mathbf{x_{t}}$. The
    parameters of the model are the matrix $W$ and the bias vector $b$.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\sigma$是非线性函数，如逻辑函数或双曲正切函数，$\left[\mathbf{h_{t-1}}\ \mathbf{x_{t}}\right]$表示向量$\mathbf{h_{t-1}}$和$\mathbf{x_{t}}$的拼接。模型的参数是矩阵$W$和偏置向量$b$。
- en: 'Hence, a recurrent neural network is effectively a learned composition function,
    which dynamically depends on its current input, all of its previous inputs and
    also on the dataset on which is trained. However, this learned composition function
    is basically impossible to analyze or interpret in any way. Sometime an “intuitive”
    explanation is given about what the learned weights represent: with some weights
    representing information that must be remembered or forgotten.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，循环神经网络实际上是一个学习到的组合函数，它动态依赖于当前输入、所有先前输入以及训练数据集。然而，这个学习到的组合函数基本上无法以任何方式进行分析或解释。有时会提供一种“直观”的解释，关于学习到的权重表示的内容：一些权重表示必须记住或遗忘的信息。
- en: Even more complex recurrent neural networks as long-short term memory (LSTM)
    (Hochreiter and Schmidhuber, [1997](#bib.bib36)) have the same problem of interpretability.
    LSTM are a recent and successful way for neural network to deal with longer sequences
    of inputs, overcoming some difficulty that RNN face in the training phase. As
    with RNN, LSTM network takes as input a sequence $\mathbf{x}=\left(\mathbf{x_{1}}\
    \ldots\ \mathbf{x_{n}}\right)$ and produce as output a single vector $\mathbf{y}\in\mathbb{R}^{n}$
    which is a representation of the entire sequence. At each step $t$ the network
    takes as input the current element $\mathbf{x_{t}}$, the previous output $\mathbf{h_{t-1}}$
    and performs the following operation to produce the current output $\mathbf{h_{t}}$
    and update the internal state $\mathbf{c_{t}}$.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 即便是更复杂的循环神经网络，如长短期记忆网络（LSTM）（Hochreiter 和 Schmidhuber, [1997](#bib.bib36)）也存在相同的可解释性问题。LSTM
    是一种最近且成功的方法，使神经网络能够处理更长的输入序列，克服了RNN在训练阶段面临的一些困难。与RNN一样，LSTM网络将序列$\mathbf{x}=\left(\mathbf{x_{1}}\
    \ldots\ \mathbf{x_{n}}\right)$作为输入，并生成一个单一的向量$\mathbf{y}\in\mathbb{R}^{n}$作为输出，表示整个序列。在每一步$t$，网络以当前元素$\mathbf{x_{t}}$、先前输出$\mathbf{h_{t-1}}$作为输入，并执行以下操作以生成当前输出$\mathbf{h_{t}}$并更新内部状态$\mathbf{c_{t}}$。
- en: '|  | $\displaystyle f_{t}$ | $\displaystyle=\sigma(W_{f}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{f})$ |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f_{t}$ | $\displaystyle=\sigma(W_{f}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{f})$ |  |'
- en: '|  | $\displaystyle i_{t}$ | $\displaystyle=\sigma(W_{i}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{i})$ |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle i_{t}$ | $\displaystyle=\sigma(W_{i}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{i})$ |  |'
- en: '|  | $\displaystyle o_{t}$ | $\displaystyle=\sigma(W_{o}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{o})$ |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle o_{t}$ | $\displaystyle=\sigma(W_{o}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{o})$ |  |'
- en: '|  | $\displaystyle\mathbf{\tilde{c_{t}}}$ | $\displaystyle=\tanh(W_{c}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{c})$ |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{\tilde{c_{t}}}$ | $\displaystyle=\tanh(W_{c}\left[\mathbf{h_{t-1}}\
    \mathbf{x_{t}}\right]+b_{c})$ |  |'
- en: '|  | $\displaystyle\mathbf{c_{t}}$ | $\displaystyle=f_{t}\odot\mathbf{c_{t-i}}+i_{t}\odot\mathbf{\tilde{c_{t}}}$
    |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{c_{t}}$ | $\displaystyle=f_{t}\odot\mathbf{c_{t-i}}+i_{t}\odot\mathbf{\tilde{c_{t}}}$
    |  |'
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\tanh(\mathbf{c_{t}})$
    |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\tanh(\mathbf{c_{t}})$
    |  |'
- en: where $\odot$ stands for element-wise multiplication, and the parameters of
    the model are the matrices $W_{f},W_{i},W_{o},W_{c}$ and the bias vectors $b_{f},b_{i},b_{o},b_{c}$.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$ 表示逐元素乘法，模型的参数是矩阵 $W_{f},W_{i},W_{o},W_{c}$ 和偏置向量 $b_{f},b_{i},b_{o},b_{c}$。
- en: 'Generally, the interpretation offered for recursive neural networks is *functional*
    or *“psychological”* and not on the content of intermediate vectors. For example,
    an interpretation of the parameters of LSTM is the following:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于递归神经网络的解释是 *功能性* 或 *“心理学”* 的，而不是针对中间向量的内容。例如，对 LSTM 参数的解释如下：
- en: •
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$f_{t}$ is the *forget gate*: at each step takes in consideration the new input
    and output computed so far to decide which information in the internal state must
    be *forgotten* (that is, set to $0$);'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f_{t}$ 是 *遗忘门*：在每一步中，它会考虑新的输入和当前计算出的输出，以决定内部状态中哪些信息必须被 *遗忘*（即，设置为 $0$）；
- en: •
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$i_{t}$ is the *input gate*: it decides which position in the internal state
    will be updated, and by how much;'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $i_{t}$ 是 *输入门*：它决定了内部状态中的哪个位置将被更新，以及更新的量；
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\tilde{c_{t}}$ is the proposed new internal state, which will then be updated
    effectively combining the previous gate;
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\tilde{c_{t}}$ 是提议的新内部状态，然后将通过组合前一个门来有效更新；
- en: •
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$o_{t}$ is the *output gate*: it decides how to modulate the internal state
    to produce the output'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $o_{t}$ 是 *输出门*：它决定如何调节内部状态以产生输出
- en: These *models-that-compose* have high performance on final tasks but are definitely
    not interpretable.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 *组合模型* 在最终任务上表现出色，但确实不易解释。
- en: 7.3.2 Recursive Neural Network
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2 递归神经网络
- en: \Tree
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: \Tree
- en: '[.S [.cows ] [.VP [.eat ] [.NP [.animal ] [.extracts ] ] ] ]'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[.S [.cows ] [.VP [.eat ] [.NP [.animal ] [.extracts ] ] ] ]'
- en: 'Figure 3: A simple binary tree'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：一个简单的二叉树
- en: '![Refer to caption](img/59cbb7affc4acb0b6326db80824d1c4a.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/59cbb7affc4acb0b6326db80824d1c4a.png)'
- en: 'Figure 4: Recursive Neural Networks'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：递归神经网络
- en: The last class of *models-that-compose* that we present is the class of *recursive
    neural networks* (Socher et al., [2012](#bib.bib65)). These networks are applied
    to data structures as trees and are in fact applied recursively on the structure.
    Generally, the aim of the network is a final task as *sentiment analysis* or *paraphrase
    detection*.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示的最后一类 *组合模型* 是 *递归神经网络*（Socher 等人，[2012](#bib.bib65)）。这些网络应用于树状数据结构，并且实际上是在结构上递归应用。通常，网络的目标是一个最终任务，如
    *情感分析* 或 *同义句检测*。
- en: 'Recursive neural networks is then a basic block (see Fig. [4](#S7.F4 "Figure
    4 ‣ 7.3.2 Recursive Neural Network ‣ 7.3 Compositional Models in Neural Networks
    ‣ 7 Composing distributed representations ‣ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey")) which is recursively applied on trees like the one in Fig. [3](#S7.F3
    "Figure 3 ‣ 7.3.2 Recursive Neural Network ‣ 7.3 Compositional Models in Neural
    Networks ‣ 7 Composing distributed representations ‣ Symbolic, Distributed and
    Distributional Representations for Natural Language Processing in the Era of Deep
    Learning: a Survey"). The formal definition is the following:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络是一个基本模块（见图 [4](#S7.F4 "图 4 ‣ 7.3.2 递归神经网络 ‣ 7.3 神经网络中的组合模型 ‣ 7 组合分布式表示
    ‣ 符号、分布式和分布性表示在深度学习时代的自然语言处理中的应用：综述")），它在树状结构上递归应用，如图 [3](#S7.F3 "图 3 ‣ 7.3.2
    递归神经网络 ‣ 7.3 神经网络中的组合模型 ‣ 7 组合分布式表示 ‣ 符号、分布式和分布性表示在深度学习时代的自然语言处理中的应用：综述") 所示。其正式定义如下：
- en: '|  | $\mathbf{p}=f_{U,V}(\mathbf{u},\mathbf{v})=f(V\mathbf{u},U\mathbf{v})=g(W\begin{pmatrix}V\mathbf{u}\\
    U\mathbf{v}\end{pmatrix})$ |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{p}=f_{U,V}(\mathbf{u},\mathbf{v})=f(V\mathbf{u},U\mathbf{v})=g(W\begin{pmatrix}V\mathbf{u}\\
    U\mathbf{v}\end{pmatrix})$ |  |'
- en: where $g$ is a component-wise sigmoid function or $\mathrm{tanh}$, and $W$ is
    a matrix that maps the concatenation vector<math alttext="\begin{pmatrix}V\mathbf{u}\\
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g$ 是逐元素的 sigmoid 函数或 $\mathrm{tanh}$，$W$ 是一个矩阵，它将拼接向量映射到<math alttext="\begin{pmatrix}V\mathbf{u}\\
- en: U\mathbf{v}\end{pmatrix}" display="inline"><semantics ><mrow ><mo >(</mo><mtable
    rowspacing="0pt" ><mtr ><mtd ><mrow ><mi >V</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >𝐮</mi></mrow></mtd></mtr><mtr ><mtd ><mrow ><mi >U</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >𝐯</mi></mrow></mtd></mtr></mtable><mo >)</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="latexml" >matrix</csymbol><matrix
    ><matrixrow ><apply ><ci >𝑉</ci><ci >𝐮</ci></apply></matrixrow><matrixrow ><apply
    ><ci >𝑈</ci><ci >𝐯</ci></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{pmatrix}V\mathbf{u}\\ U\mathbf{v}\end{pmatrix}</annotation></semantics></math>
    to have the same dimension.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 使其具有相同的维度。
- en: 'This method deals naturally with recursion: given a binary parse tree of a
    sentence $s$, the algorithm creates vectors and matrices representation for each
    node, starting from the terminal nodes. Words are represented by distributed representations
    or local representations. For example, the tree in Fig. [3](#S7.F3 "Figure 3 ‣
    7.3.2 Recursive Neural Network ‣ 7.3 Compositional Models in Neural Networks ‣
    7 Composing distributed representations ‣ Symbolic, Distributed and Distributional
    Representations for Natural Language Processing in the Era of Deep Learning: a
    Survey") is processed by the recursive network in the following way. First, the
    network in Fig. [4](#S7.F4 "Figure 4 ‣ 7.3.2 Recursive Neural Network ‣ 7.3 Compositional
    Models in Neural Networks ‣ 7 Composing distributed representations ‣ Symbolic,
    Distributed and Distributional Representations for Natural Language Processing
    in the Era of Deep Learning: a Survey") is applied to the pair *(animal,extracts)*
    and $f_{UV}(\mathbf{animal},\mathbf{extract})$ is obtained. Then, the network
    is applied to the result and *eat* and $f_{UV}(\mathbf{eat},f_{UV}(\mathbf{animal},\mathbf{extract}))$
    is obtained and so on.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '该方法自然地处理递归：给定一个句子$s$的二叉解析树，算法为每个节点创建向量和矩阵表示，从终端节点开始。单词由分布式表示或局部表示表示。例如，图[3](#S7.F3
    "Figure 3 ‣ 7.3.2 Recursive Neural Network ‣ 7.3 Compositional Models in Neural
    Networks ‣ 7 Composing distributed representations ‣ Symbolic, Distributed and
    Distributional Representations for Natural Language Processing in the Era of Deep
    Learning: a Survey")中的树通过递归网络以以下方式处理。首先，将图[4](#S7.F4 "Figure 4 ‣ 7.3.2 Recursive
    Neural Network ‣ 7.3 Compositional Models in Neural Networks ‣ 7 Composing distributed
    representations ‣ Symbolic, Distributed and Distributional Representations for
    Natural Language Processing in the Era of Deep Learning: a Survey")中的网络应用于*(animal,extracts)*对，得到$f_{UV}(\mathbf{animal},\mathbf{extract})$。然后，将网络应用于结果，得到*eat*和$f_{UV}(\mathbf{eat},f_{UV}(\mathbf{animal},\mathbf{extract}))$，依此类推。'
- en: 'Recursive neural networks are not easily interpretable even if quite similar
    to the additive *compositional distributional semantic models* as those presented
    in Sec. [7.1.1](#S7.SS1.SSS1 "7.1.1 Additive Models ‣ 7.1 Compositional Distributional
    Semantics ‣ 7 Composing distributed representations ‣ Symbolic, Distributed and
    Distributional Representations for Natural Language Processing in the Era of Deep
    Learning: a Survey"). In fact, the non-linear function $g$ is the one that makes
    final vectors less interpretable.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '递归神经网络即使与加性*组合分布性语义模型*相似，也不容易被解释，如第[7.1.1节](#S7.SS1.SSS1 "7.1.1 Additive Models
    ‣ 7.1 Compositional Distributional Semantics ‣ 7 Composing distributed representations
    ‣ Symbolic, Distributed and Distributional Representations for Natural Language
    Processing in the Era of Deep Learning: a Survey")所述。实际上，非线性函数$g$使最终向量不易解释。'
- en: 8 Conclusions
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: 'Natural language is symbolic representation. Thinking to natural language understanding
    systems which are not based on symbols seems to be extremely odd. However, recent
    advances in machine learning (ML) and in natural language processing (NLP) seem
    to contradict the above intuition: symbols are fading away, erased by vectors
    or tensors called *distributed* and *distributional representations*.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言是一种符号表示。考虑到非基于符号的自然语言理解系统似乎非常奇怪。然而，最近在机器学习（ML）和自然语言处理（NLP）领域的进展似乎与上述直觉相矛盾：符号正在消退，被称为*分布式*和*分布性表示*的向量或张量所取代。
- en: We made this survey to show the not-surprising link between symbolic representations
    and distributed/distributional representations. This is the right time to revitalize
    the area of interpreting how symbols are represented inside neural networks. In
    our opinion, this survey will help to devise new deep neural networks that can
    exploit existing and novel symbolic models of classical natural language processing
    tasks. We believe that a clearer understanding of the strict link between distributed/distributional
    representations and symbols may lead to radically new deep learning networks.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做这项调查是为了展示符号表示与分布式/分布性表示之间的不惊讶的联系。现在正是重振解释符号如何在神经网络内部表示的领域的合适时机。在我们看来，这项调查将有助于设计新的深度神经网络，这些网络可以利用现有的和新的符号模型来处理经典的自然语言处理任务。我们相信，对分布式/分布性表示与符号之间严格联系的更清晰理解，可能会导致全新的深度学习网络。
- en: References
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Achlioptas (2003) Achlioptas, D. (2003). Database-friendly random projections:
    Johnson-lindenstrauss with binary coins. *Journal of computer and System Sciences*
    66, 671–687'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achlioptas (2003) Achlioptas, D. (2003). 数据库友好的随机投影：带有二进制硬币的Johnson-Lindenstrauss。*计算机与系统科学杂志*
    66, 671–687
- en: 'Agirre et al. (2013) Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A., and
    Guo, W. (2013). *sem 2013 shared task: Semantic textual similarity. In *Second
    Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings
    of the Main Conference and the Shared Task: Semantic Textual Similarity* (Atlanta,
    Georgia, USA: Association for Computational Linguistics), 32–43'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agirre 等 (2013) Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A., 和 Guo, W.
    (2013). *sem 2013 共享任务：语义文本相似性。在 *第二届词汇与计算语义学联合会议* (*SEM)，第 1 卷：主会议和共享任务：语义文本相似性论文集*
    （美国乔治亚州亚特兰大：计算语言学协会），32–43
- en: Bahdanau et al. (2014) Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural
    machine translation by jointly learning to align and translate. *arXiv preprint
    arXiv:1409.0473*
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等 (2014) Bahdanau, D., Cho, K., 和 Bengio, Y. (2014). 通过联合学习对齐和翻译的神经机器翻译。
    *arXiv 预印本 arXiv:1409.0473*
- en: 'Baroni et al. (2014) Baroni, M., Bernardi, R., and Zamparelli, R. (2014). Frege
    in space: A program of compositional distributional semantics. *LiLT (Linguistic
    Issues in Language Technology)* 9'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baroni 等 (2014) Baroni, M., Bernardi, R., 和 Zamparelli, R. (2014). 空间中的弗雷格：组合分布语义的一个程序。
    *LiLT (语言技术中的语言学问题)* 9
- en: 'Baroni and Lenci (2010) Baroni, M. and Lenci, A. (2010). Distributional memory:
    A general framework for corpus-based semantics. *Comput. Linguist.* 36, 673–721.
    [10.1162/coli_a_00016](https:/doi.org/10.1162/coli_a_00016)'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baroni 和 Lenci (2010) Baroni, M. 和 Lenci, A. (2010). 分布式记忆：基于语料库语义的通用框架。 *计算语言学*
    36, 673–721。 [10.1162/coli_a_00016](https:/doi.org/10.1162/coli_a_00016)
- en: 'Baroni and Zamparelli (2010) Baroni, M. and Zamparelli, R. (2010). Nouns are
    vectors, adjectives are matrices: Representing adjective-noun constructions in
    semantic space. In *Proceedings of the 2010 Conference on Empirical Methods in
    Natural Language Processing* (Cambridge, MA: Association for Computational Linguistics),
    1183–1193'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baroni 和 Zamparelli (2010) Baroni, M. 和 Zamparelli, R. (2010). 名词是向量，形容词是矩阵：在语义空间中表示形容词-名词构造。在
    *2010 年自然语言处理实证方法会议论文集* （剑桥，MA：计算语言学协会），1183–1193
- en: Belkin and Niyogi (2001) Belkin, M. and Niyogi, P. (2001). Laplacian eigenmaps
    and spectral techniques for embedding and clustering. In *NIPS*. vol. 14, 585–591
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belkin 和 Niyogi (2001) Belkin, M. 和 Niyogi, P. (2001). 拉普拉斯特征映射和谱技术在嵌入和聚类中的应用。在
    *NIPS*。第 14 卷，585–591
- en: Bellman and Corporation (1957) Bellman, R. and Corporation, R. (1957). *Dynamic
    Programming*. Rand Corporation research study (Princeton University Press)
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman 和 Corporation (1957) Bellman, R. 和 Corporation, R. (1957). *动态规划*。Rand
    Corporation 研究报告（Princeton University Press）
- en: 'Bingham and Mannila (2001) Bingham, E. and Mannila, H. (2001). Random projection
    in dimensionality reduction: applications to image and text data. In *Proceedings
    of the seventh ACM SIGKDD international conference on Knowledge discovery and
    data mining* (ACM), 245–250'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bingham 和 Mannila (2001) Bingham, E. 和 Mannila, H. (2001). 维度缩减中的随机投影：图像和文本数据的应用。在
    *第七届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集* （ACM），245–250
- en: Blutner et al. (2003) Blutner, R., Hendriks, P., and de Hoop, H. (2003). A new
    hypothesis on compositionality. In *Proceedings of the joint international conference
    on cognitive science*
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blutner 等 (2003) Blutner, R., Hendriks, P., 和 de Hoop, H. (2003). 关于组合性的一个新假设。在
    *认知科学联合国际会议论文集*
- en: 'Chalmers (1992) Chalmers, D. J. (1992). *Syntactic Transformations on Distributed
    Representations* (Dordrecht: Springer Netherlands). 46–55. [10.1007/978-94-011-2624-3_3](https:/doi.org/10.1007/978-94-011-2624-3_3)'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chalmers (1992) Chalmers, D. J. (1992). *分布式表示上的句法变换* （Dordrecht: Springer
    Netherlands）。46–55。 [10.1007/978-94-011-2624-3_3](https:/doi.org/10.1007/978-94-011-2624-3_3)'
- en: 'Chetlur et al. (2014) Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J.,
    Tran, J., Catanzaro, B., et al. (2014). cudnn: Efficient primitives for deep learning.
    *arXiv preprint arXiv:1410.0759*'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chetlur 等 (2014) Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran,
    J., Catanzaro, B., 等 (2014). cudnn：高效的深度学习原语。 *arXiv 预印本 arXiv:1410.0759*
- en: 'Chomsky (1957) Chomsky, N. (1957). *Aspect of Syntax Theory* (Cambridge, Massachussetts:
    MIT Press)'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chomsky (1957) Chomsky, N. (1957). *句法理论的层面* （剑桥，马萨诸塞州：MIT Press）
- en: Clark et al. (2008) Clark, S., Coecke, B., and Sadrzadeh, M. (2008). A compositional
    distributional model of meaning. *Proceedings of the Second Symposium on Quantum
    Interaction (QI-2008)* , 133–140
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 (2008) Clark, S., Coecke, B., 和 Sadrzadeh, M. (2008). 语义的组合分布模型。 *第二届量子互动研讨会
    (QI-2008) 论文集* ，133–140
- en: Coecke et al. (2010) Coecke, B., Sadrzadeh, M., and Clark, S. (2010). Mathematical
    foundations for a compositional distributional model of meaning. *CoRR* abs/1003.4394
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coecke 等 (2010) Coecke, B., Sadrzadeh, M., 和 Clark, S. (2010). 语义的组合分布模型的数学基础。
    *CoRR* abs/1003.4394
- en: Cui et al. (2015) Cui, H., Ganger, G. R., and Gibbons, P. B. (2015). *Scalable
    deep learning on distributed GPUs with a GPU-specialized parameter server*. Tech.
    rep., CMU PDL Technical Report (CMU-PDL-15-107)
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等 (2015) Cui, H., Ganger, G. R., 和 Gibbons, P. B. (2015). *在分布式GPU上可扩展的深度学习，配有GPU专用参数服务器*。技术报告，CMU
    PDL 技术报告 (CMU-PDL-15-107)
- en: 'Dagan et al. (2013) Dagan, I., Roth, D., Sammons, M., and Zanzotto, F. M. (2013).
    *Recognizing Textual Entailment: Models and Applications*. Synthesis Lectures
    on Human Language Technologies (Morgan & Claypool Publishers)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dagan 等 (2013) Dagan, I., Roth, D., Sammons, M., 和 Zanzotto, F. M. (2013).
    *识别文本蕴含: 模型与应用*。人类语言技术合成讲座（Morgan & Claypool Publishers）'
- en: Daum and Huang (2003) Daum, F. and Huang, J. (2003). Curse of dimensionality
    and particle filters. In *Aerospace Conference, 2003\. Proceedings. 2003 IEEE*
    (IEEE), vol. 4, 4_1979–4_1993
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daum 和 Huang (2003) Daum, F. 和 Huang, J. (2003). 维度灾难与粒子滤波器。见 *航天会议，2003年\.
    论文集。2003 IEEE*（IEEE），第4卷，4_1979–4_1993
- en: 'Devlin et al. (2018) Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018).
    BERT: pre-training of deep bidirectional transformers for language understanding.
    *CoRR* abs/1810.04805'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等 (2018) Devlin, J., Chang, M., Lee, K., 和 Toutanova, K. (2018). BERT:
    深度双向变换器的预训练用于语言理解。*CoRR* abs/1810.04805'
- en: 'Ferrone and Zanzotto (2014) Ferrone, L. and Zanzotto, F. M. (2014). Towards
    syntax-aware compositional distributional semantic models. In *Proceedings of
    COLING 2014, the 25th International Conference on Computational Linguistics: Technical
    Papers* (Dublin, Ireland: Dublin City University and Association for Computational
    Linguistics), 721–730'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ferrone 和 Zanzotto (2014) Ferrone, L. 和 Zanzotto, F. M. (2014). 朝着语法感知的组合分布语义模型迈进。见
    *COLING 2014, 第25届计算语言学国际会议: 技术论文集*（都柏林，爱尔兰: 都柏林城市大学和计算语言学协会），721–730'
- en: Ferrone et al. (2015) Ferrone, L., Zanzotto, F. M., and Carreras, X. (2015).
    Decoding distributed tree structures. In *Statistical Language and Speech Processing
    - Third International Conference, SLSP 2015, Budapest, Hungary, November 24-26,
    2015, Proceedings*. 73–83. [10.1007/978-3-319-25789-1_8](https:/doi.org/10.1007/978-3-319-25789-1_8)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrone 等 (2015) Ferrone, L., Zanzotto, F. M., 和 Carreras, X. (2015). 解码分布式树结构。见
    *统计语言与语音处理 - 第三届国际会议，SLSP 2015，匈牙利布达佩斯，2015年11月24-26日，会议论文集*。73–83. [10.1007/978-3-319-25789-1_8](https:/doi.org/10.1007/978-3-319-25789-1_8)
- en: 'Firth (1957) Firth, J. R. (1957). *Papers in Linguistics.* (London: Oxford
    University Press.)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Firth (1957) Firth, J. R. (1957). *语言学论文集*。（伦敦: 牛津大学出版社。）'
- en: Fodor (2002) Fodor, I. (2002). *A Survey of Dimension Reduction Techniques*.
    Tech. rep.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fodor (2002) Fodor, I. (2002). *降维技术概述*。技术报告。
- en: 'Fodor and Pylyshyn (1988) Fodor, J. A. and Pylyshyn, Z. W. (1988). Connectionism
    and cognitive architecture: A critical analysis. *Cognition* 28, 3 – 71. [https://doi.org/10.1016/0010-0277(88)90031-5](https:/doi.org/https://doi.org/10.1016/0010-0277(88)90031-5)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fodor 和 Pylyshyn (1988) Fodor, J. A. 和 Pylyshyn, Z. W. (1988). 连接主义与认知架构: 批判性分析。*认知*
    28, 3 – 71. [https://doi.org/10.1016/0010-0277(88)90031-5](https:/doi.org/https://doi.org/10.1016/0010-0277(88)90031-5)'
- en: 'Frege (1884) Frege, G. (1884). *Die Grundlagen der Arithmetik (The Foundations
    of Arithmetic): eine logisch-mathematische Untersuchung über den Begriff der Zahl*
    (Breslau)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frege (1884) Frege, G. (1884). *算术基础（The Foundations of Arithmetic）：关于数字概念的逻辑-数学研究*（布雷斯劳）
- en: Friedman (1997) Friedman, J. H. (1997). On bias, variance, 0/1—loss, and the
    curse-of-dimensionality. *Data mining and knowledge discovery* 1, 55–77
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Friedman (1997) Friedman, J. H. (1997). 关于偏差、方差、0/1—损失和维度灾难。*数据挖掘与知识发现* 1, 55–77
- en: 'Gelder (1990) Gelder, T. V. (1990). Compositionality: A connectionist variation
    on a classical theme. *Cognitive Science* 384, 355–384. [10.1207/s15516709cog1403_2](https:/doi.org/10.1207/s15516709cog1403_2)'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gelder (1990) Gelder, T. V. (1990). 组合性: 一种连接主义的古典主题变体。*认知科学* 384, 355–384.
    [10.1207/s15516709cog1403_2](https:/doi.org/10.1207/s15516709cog1403_2)'
- en: 'Goldberg and Levy (2014) Goldberg, Y. and Levy, O. (2014). word2vec explained:
    deriving mikolov et al.’s negative-sampling word-embedding method. *arXiv preprint
    arXiv:1402.3722*'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goldberg 和 Levy (2014) Goldberg, Y. 和 Levy, O. (2014). word2vec 解释: 推导 Mikolov
    等的负采样词嵌入方法。*arXiv 预印本 arXiv:1402.3722*'
- en: Goodfellow et al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., et al. (2014). Generative adversarial nets. In *Advances
    in Neural Information Processing Systems*. 2672–2680
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
    D., Ozair, S., 等 (2014). 生成对抗网络。见 *神经信息处理系统进展*。2672–2680
- en: Graves (2013) Graves, A. (2013). Generating sequences with recurrent neural
    networks. *CoRR* abs/1308.0850
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves (2013) Graves, A. (2013). 使用递归神经网络生成序列。*CoRR* abs/1308.0850
- en: 'Grefenstette and Sadrzadeh (2011) Grefenstette, E. and Sadrzadeh, M. (2011).
    Experimental support for a categorical compositional distributional model of meaning.
    In *Proceedings of the Conference on Empirical Methods in Natural Language Processing*
    (Stroudsburg, PA, USA: Association for Computational Linguistics), EMNLP ’11,
    1394–1404'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grefenstette 和 Sadrzadeh (2011) Grefenstette, E. 和 Sadrzadeh, M. (2011). 对类别组合分布模型的实验支持。在
    *自然语言处理经验方法会议论文集*（宾夕法尼亚州斯特劳兹堡：计算语言学协会），EMNLP ’11，1394–1404
- en: 'Guevara (2010) Guevara, E. (2010). A regression model of adjective-noun compositionality
    in distributional semantics. In *Proceedings of the 2010 Workshop on GEometrical
    Models of Natural Language Semantics* (Uppsala, Sweden: Association for Computational
    Linguistics), 33–37'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guevara (2010) Guevara, E. (2010). 分布语义中形容词-名词组合性的回归模型。在 *2010 年几何模型自然语言语义研讨会论文集*（瑞典乌普萨拉：计算语言学协会），33–37
- en: 'Harris (1964) Harris, Z. (1964). Distributional structure. In *The Philosophy
    of Linguistics*, eds. J. J. Katz and J. A. Fodor (New York: Oxford University
    Press)'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harris (1964) Harris, Z. (1964). 分布结构。在 *语言学哲学*，编辑 J. J. Katz 和 J. A. Fodor（纽约：牛津大学出版社）
- en: He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. (2016). Identity mappings
    in deep residual networks. *arXiv preprint arXiv:1603.05027*
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2016) He, K., Zhang, X., Ren, S., 和 Sun, J. (2016). 深度残差网络中的身份映射。*arXiv
    预印本 arXiv:1603.05027*
- en: 'Hinton et al. (1986) Hinton, G. E., McClelland, J. L., and Rumelhart, D. E.
    (1986). Distributed representations. In *Parallel Distributed Processing: Explorations
    in the Microstructure of Cognition. Volume 1: Foundations*, eds. D. E. Rumelhart
    and J. L. McClelland (MIT Press, Cambridge, MA.)'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等 (1986) Hinton, G. E., McClelland, J. L., 和 Rumelhart, D. E. (1986).
    分布式表示。在 *并行分布处理：认知微结构探索。第 1 卷：基础*，编辑 D. E. Rumelhart 和 J. L. McClelland（MIT Press,
    Cambridge, MA）
- en: Hochreiter and Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. (1997).
    Long short-term memory. *Neural computation* 9, 1735–1780
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber (1997) Hochreiter, S. 和 Schmidhuber, J. (1997). 长短期记忆。*神经计算*
    9，1735–1780
- en: Jacovi et al. (2018) Jacovi, A., Shalom, O. S., and Goldberg, Y. (2018). Understanding
    Convolutional Neural Networks for Text Classification , 56–65[doi:10.1046/j.1365-3040.2003.01027.x](https:/doi.org/doi:10.1046/j.1365-3040.2003.01027.x)
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacovi 等 (2018) Jacovi, A., Shalom, O. S., 和 Goldberg, Y. (2018). 理解卷积神经网络用于文本分类，56–65
    [doi:10.1046/j.1365-3040.2003.01027.x](https:/doi.org/doi:10.1046/j.1365-3040.2003.01027.x)
- en: Jang et al. (2018) Jang, K.-r., Kim, S.-b., and Corp, N. (2018). Interpretable
    Word Embedding Contextualization , 341–343
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等 (2018) Jang, K.-r., Kim, S.-b., 和 Corp, N. (2018). 可解释的词嵌入上下文化，341–343
- en: Johnson and Lindenstrauss (1984) Johnson, W. and Lindenstrauss, J. (1984). Extensions
    of lipschitz mappings into a hilbert space. *Contemp. Math.* 26, 189–206
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 和 Lindenstrauss (1984) Johnson, W. 和 Lindenstrauss, J. (1984). 向希尔伯特空间的
    Lipschitz 映射的扩展。 *Contemp. Math.* 26，189–206
- en: Kalchbrenner and Blunsom (2013) Kalchbrenner, N. and Blunsom, P. (2013). Recurrent
    convolutional neural networks for discourse compositionality. *Proceedings of
    the 2013 Workshop on Continuous Vector Space Models and their Compositionality*
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalchbrenner 和 Blunsom (2013) Kalchbrenner, N. 和 Blunsom, P. (2013). 用于话语组合性的递归卷积神经网络。*2013
    年连续向量空间模型及其组合性研讨会论文集*
- en: Keogh and Mueen (2011) Keogh, E. and Mueen, A. (2011). Curse of dimensionality.
    In *Encyclopedia of Machine Learning* (Springer). 257–258
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keogh 和 Mueen (2011) Keogh, E. 和 Mueen, A. (2011). 维度灾难。在 *机器学习百科全书*（Springer）。257–258
- en: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012).
    Imagenet classification with deep convolutional neural networks. In *Advances
    in neural information processing systems*. 1097–1105
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2012) Krizhevsky, A., Sutskever, I., 和 Hinton, G. E. (2012). 使用深度卷积神经网络的
    ImageNet 分类。在 *神经信息处理系统进展*。1097–1105
- en: 'Landauer and Dumais (1997) Landauer, T. K. and Dumais, S. T. (1997). A solution
    to plato’s problem: The latent semantic analysis theory of acquisition, induction,
    and representation of knowledge. *Psychological Review* 104, 211–240'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landauer 和 Dumais (1997) Landauer, T. K. 和 Dumais, S. T. (1997). 解决柏拉图问题：潜在语义分析理论的知识获取、归纳和表示。*心理学评论*
    104，211–240
- en: LeCun et al. (2015) LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning.
    *Nature* 521, 436–444
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (2015) LeCun, Y., Bengio, Y., 和 Hinton, G. (2015). 深度学习。*自然* 521，436–444
- en: Liou et al. (2014) Liou, C.-Y., Cheng, W.-C., Liou, J.-W., and Liou, D.-R. (2014).
    Autoencoder for words. *Neurocomputing* 139, 84 – 96. [http://dx.doi.org/10.1016/j.neucom.2013.09.055](https:/doi.org/http://dx.doi.org/10.1016/j.neucom.2013.09.055)
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liou 等 (2014) Liou, C.-Y., Cheng, W.-C., Liou, J.-W., 和 Liou, D.-R. (2014).
    词的自编码器。*神经计算* 139, 84 – 96. [http://dx.doi.org/10.1016/j.neucom.2013.09.055](https:/doi.org/http://dx.doi.org/10.1016/j.neucom.2013.09.055)
- en: Lipton (2016) Lipton, Z. C. (2016). The Mythos of Model Interpretability [10.1145/3233231](https:/doi.org/10.1145/3233231)
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lipton (2016) Lipton, Z. C. (2016). 模型可解释性的神话 [10.1145/3233231](https:/doi.org/10.1145/3233231)
- en: 'Markovsky (2012) Markovsky, I. (2012). Low rank approximation: Algorithms,
    implementation, applications'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Markovsky (2012) Markovsky, I. (2012). 低秩逼近: 算法、实现、应用'
- en: Masci et al. (2011) Masci, J., Meier, U., Cireşan, D., and Schmidhuber, J. (2011).
    Stacked convolutional auto-encoders for hierarchical feature extraction. In *International
    Conference on Artificial Neural Networks* (Springer), 52–59
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masci 等 (2011) Masci, J., Meier, U., Cireşan, D., 和 Schmidhuber, J. (2011).
    用于分层特征提取的堆叠卷积自编码器。见 *国际人工神经网络大会* (Springer), 52–59
- en: Mikolov et al. (2013) Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013).
    Efficient estimation of word representations in vector space. *CoRR* abs/1301.3781
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等 (2013) Mikolov, T., Chen, K., Corrado, G., 和 Dean, J. (2013). 向量空间中词表示的高效估计。*CoRR*
    abs/1301.3781
- en: 'Mitchell and Lapata (2008) Mitchell, J. and Lapata, M. (2008). Vector-based
    models of semantic composition. In *Proceedings of ACL-08: HLT* (Columbus, Ohio:
    Association for Computational Linguistics), 236–244'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mitchell 和 Lapata (2008) Mitchell, J. 和 Lapata, M. (2008). 基于向量的语义组合模型。见 *ACL-08:
    HLT 会议论文集* (俄亥俄州哥伦布市: 计算语言学协会), 236–244'
- en: Mitchell and Lapata (2010) Mitchell, J. and Lapata, M. (2010). Composition in
    distributional models of semantics. *Cognitive Science* [10.1111/j.1551-6709.2010.01106.x](https:/doi.org/10.1111/j.1551-6709.2010.01106.x)
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell 和 Lapata (2010) Mitchell, J. 和 Lapata, M. (2010). 语义分布模型中的组合。*认知科学*
    [10.1111/j.1551-6709.2010.01106.x](https:/doi.org/10.1111/j.1551-6709.2010.01106.x)
- en: 'Montague (1974) Montague, R. (1974). English as a formal language. In *Formal
    Philosophy: Selected Papers of Richard Montague*, ed. R. Thomason (New Haven:
    Yale University Press). 188–221'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Montague (1974) Montague, R. (1974). 英语作为形式语言。见 *形式哲学: 理查德·蒙塔古精选论文*, 编辑 R.
    Thomason (纽黑文: 耶鲁大学出版社). 188–221'
- en: Neumann (2001) Neumann, J. (2001). *Holistic processing of hierarchical structures
    in connectionist networks*. Ph.D. thesis, University of Edinburgh
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neumann (2001) Neumann, J. (2001). *连接主义网络中的层次结构的整体处理*. 博士论文, 爱丁堡大学
- en: Pado and Lapata (2007) Pado, S. and Lapata, M. (2007). Dependency-based construction
    of semantic space models. *Computational Linguistics* 33, 161–199
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pado 和 Lapata (2007) Pado, S. 和 Lapata, M. (2007). 基于依赖的语义空间模型构建。*计算语言学* 33,
    161–199
- en: Pearson (1901) Pearson, K. (1901). Principal components analysis. *The London,
    Edinburgh and Dublin Philosophical Magazine and Journal* 6, 566
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearson (1901) Pearson, K. (1901). 主成分分析。*伦敦、爱丁堡和都柏林哲学杂志* 6, 566
- en: Plate (1994) Plate, T. A. (1994). *Distributed Representations and Nested Compositional
    Structure*. Ph.D. thesis
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plate (1994) Plate, T. A. (1994). *分布式表示和嵌套组合结构*. 博士论文
- en: Plate (1995) Plate, T. A. (1995). Holographic reduced representations. *IEEE
    Transactions on Neural Networks* 6, 623–641. [10.1109/72.377968](https:/doi.org/10.1109/72.377968)
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plate (1995) Plate, T. A. (1995). 全息降维表示。*IEEE 神经网络交易* 6, 623–641. [10.1109/72.377968](https:/doi.org/10.1109/72.377968)
- en: 'Rosenblatt (1958) Rosenblatt, F. (1958). The perceptron: a probabilistic model
    for information storage and organization in the brain. *Psychological Reviews*
    65, 386–408'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rosenblatt (1958) Rosenblatt, F. (1958). 感知器: 一种用于信息存储和大脑组织的概率模型。*心理学评论* 65,
    386–408'
- en: 'Rothenhäusler and Schütze (2009) Rothenhäusler, K. and Schütze, H. (2009).
    Unsupervised classification with dependency based word spaces. In *Proceedings
    of the Workshop on Geometrical Models of Natural Language Semantics* (Stroudsburg,
    PA, USA: Association for Computational Linguistics), GEMS ’09, 17–24'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rothenhäusler 和 Schütze (2009) Rothenhäusler, K. 和 Schütze, H. (2009). 基于依赖的无监督分类词空间。见
    *自然语言语义几何模型研讨会论文集* (宾夕法尼亚州斯特劳兹堡市: 计算语言学协会), GEMS ’09, 17–24'
- en: Sahlgren (2005) Sahlgren, M. (2005). An introduction to random indexing. In
    *Proceedings of the Methods and Applications of Semantic Indexing Workshop at
    the 7th International Conference on Terminology and Knowledge Engineering TKE*
    (Copenhagen, Denmark)
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahlgren (2005) Sahlgren, M. (2005). 随机索引简介。见 *第七届术语和知识工程国际会议 TKE 研讨会方法与应用论文集*
    (丹麦哥本哈根)
- en: 'Salton (1989) Salton, G. (1989). *Automatic text processing: the transformation,
    analysis and retrieval of information by computer* (Addison-Wesley)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salton (1989) Salton, G. (1989). *自动文本处理：计算机的信息转化、分析与检索* (Addison-Wesley)
- en: 'Schmidhuber (2015) Schmidhuber, J. (2015). Deep learning in neural networks:
    An overview. *Neural Networks* 61, 85–117'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidhuber (2015) Schmidhuber, J. (2015). 神经网络中的深度学习：概述。*Neural Networks* 61,
    85–117
- en: Schuster and Paliwal (1997) Schuster, M. and Paliwal, K. (1997). Bidirectional
    recurrent neural networks. *Trans. Sig. Proc.* 45, 2673–2681. [10.1109/78.650093](https:/doi.org/10.1109/78.650093)
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuster and Paliwal (1997) Schuster, M. and Paliwal, K. (1997). 双向递归神经网络。*Trans.
    Sig. Proc.* 45, 2673–2681. [10.1109/78.650093](https:/doi.org/10.1109/78.650093)
- en: Socher et al. (2011) Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and
    Manning, C. D. (2011). Dynamic pooling and unfolding recursive autoencoders for
    paraphrase detection. In *Advances in Neural Information Processing Systems 24*
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. (2011) Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and
    Manning, C. D. (2011). 动态池化和展开递归自编码器用于同义句检测。载于 *神经信息处理系统进展 24*
- en: Socher et al. (2012) Socher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012).
    Semantic Compositionality Through Recursive Matrix-Vector Spaces. In *Proceedings
    of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP)*
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. (2012) Socher, R., Huval, B., Manning, C. D., and Ng, A. Y. (2012).
    通过递归矩阵-向量空间的语义组合性。载于 *2012年自然语言处理经验方法会议论文集（EMNLP）*
- en: Sorzano et al. (2014) Sorzano, C. O. S., Vargas, J., and Montano, A. P. (2014).
    A survey of dimensionality reduction techniques. *arXiv preprint arXiv:1403.2877*
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorzano et al. (2014) Sorzano, C. O. S., Vargas, J., and Montano, A. P. (2014).
    维度缩减技术综述。*arXiv预印本 arXiv:1403.2877*
- en: Turney (2006) Turney, P. D. (2006). Similarity of semantic relations. *Comput.
    Linguist.* 32, 379–416. [http://dx.doi.org/10.1162/coli.2006.32.3.379](https:/doi.org/http://dx.doi.org/10.1162/coli.2006.32.3.379)
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turney (2006) Turney, P. D. (2006). 语义关系的相似性。*Comput. Linguist.* 32, 379–416.
    [http://dx.doi.org/10.1162/coli.2006.32.3.379](https:/doi.org/http://dx.doi.org/10.1162/coli.2006.32.3.379)
- en: 'Turney and Pantel (2010) Turney, P. D. and Pantel, P. (2010). From frequency
    to meaning: Vector space models of semantics. *J. Artif. Intell. Res. (JAIR)*
    37, 141–188'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turney and Pantel (2010) Turney, P. D. and Pantel, P. (2010). 从频率到意义：语义的向量空间模型。*J.
    Artif. Intell. Res. (JAIR)* 37, 141–188
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., et al. (2017). Attention is all you need. In *Advances in Neural
    Information Processing Systems 30*, eds. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
    R. Fergus, S. Vishwanathan, and R. Garnett (Curran Associates, Inc.). 5998–6008
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., et al. (2017). 注意力机制是你所需的一切。载于 *神经信息处理系统进展 30*，编辑 I. Guyon,
    U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, 和 R. Garnett
    (Curran Associates, Inc.)。5998–6008
- en: Vincent et al. (2008) Vincent, P., Larochelle, H., Bengio, Y., and Manzagol,
    P.-A. (2008). Extracting and composing robust features with denoising autoencoders.
    In *Proceedings of the 25th international conference on Machine learning* (ACM),
    1096–1103
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent et al. (2008) Vincent, P., Larochelle, H., Bengio, Y., and Manzagol,
    P.-A. (2008). 使用去噪自编码器提取和组合鲁棒特征。载于 *第25届国际机器学习大会论文集* (ACM)，1096–1103
- en: 'Vincent et al. (2010) Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y.,
    and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations
    in a deep network with a local denoising criterion. *J. Mach. Learn. Res.* 11,
    3371–3408'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent et al. (2010) Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and
    Manzagol, P.-A. (2010). 堆叠去噪自编码器：通过局部去噪准则在深度网络中学习有用表示。*J. Mach. Learn. Res.* 11,
    3371–3408
- en: Vinyals et al. (2015a) Vinyals, O., Kaiser, L. u., Koo, T., Petrov, S., Sutskever,
    I., and Hinton, G. (2015a). Grammar as a foreign language. In *Advances in Neural
    Information Processing Systems 28*, eds. C. Cortes, N. D. Lawrence, D. D. Lee,
    M. Sugiyama, and R. Garnett (Curran Associates, Inc.). 2755–2763
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2015a) Vinyals, O., Kaiser, L. u., Koo, T., Petrov, S., Sutskever,
    I., and Hinton, G. (2015a). 语法作为外语。载于 *神经信息处理系统进展 28*，编辑 C. Cortes, N. D. Lawrence,
    D. D. Lee, M. Sugiyama, 和 R. Garnett (Curran Associates, Inc.)。2755–2763
- en: 'Vinyals et al. (2015b) Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015b).
    Show and tell: A neural image caption generator. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*. 3156–3164'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2015b) Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015b).
    展示与讲述：一个神经图像描述生成器。载于 *IEEE计算机视觉与模式识别会议论文集*。3156–3164
- en: Weiss et al. (2015) Weiss, D., Alberti, C., Collins, M., and Petrov, S. (2015).
    Structured training for neural network transition-based parsing. *arXiv preprint
    arXiv:1506.06158*
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weiss 等人（2015）Weiss, D., Alberti, C., Collins, M., 和 Petrov, S.（2015）。针对神经网络过渡式解析的结构化训练。*arXiv
    预印本 arXiv:1506.06158*
- en: 'Werbos (1974) Werbos, P. (1974). Beyond regression: New tools for prediction
    and analysis in the behavioral sciences'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Werbos（1974）Werbos, P.（1974）。超越回归：行为科学中的新预测与分析工具
- en: 'Xu et al. (2015) Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov,
    R., et al. (2015). Show, attend and tell: Neural image caption generation with
    visual attention. *arXiv preprint arXiv:1502.03044* 2, 5'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2015）Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov,
    R., 等人（2015）。展示、关注和讲述：带有视觉注意力的神经图像描述生成。*arXiv 预印本 arXiv:1502.03044* 2, 5
- en: Zanzotto and Dell’Arciprete (2012) Zanzotto, F. M. and Dell’Arciprete, L. (2012).
    Distributed tree kernels. In *Proceedings of International Conference on Machine
    Learning*. –
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanzotto 和 Dell’Arciprete（2012）Zanzotto, F. M. 和 Dell’Arciprete, L.（2012）。分布式树核。见
    *Proceedings of International Conference on Machine Learning*。
- en: 'Zanzotto et al. (2015) Zanzotto, F. M., Ferrone, L., and Baroni, M. (2015).
    When the whole is not greater than the combination of its parts: A ”decompositional”
    look at compositional distributional semantics. *Comput. Linguist.* 41, 165–173.
    [10.1162/COLI_a_00215](https:/doi.org/10.1162/COLI_a_00215)'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanzotto 等人（2015）Zanzotto, F. M., Ferrone, L., 和 Baroni, M.（2015）。当整体不大于其部分的组合时：对组合分布语义的“分解”视角。*Comput.
    Linguist.* 41, 165–173. [10.1162/COLI_a_00215](https:/doi.org/10.1162/COLI_a_00215)
- en: Zanzotto et al. (2010) Zanzotto, F. M., Korkontzelos, I., Fallucchi, F., and
    Manandhar, S. (2010). Estimating linear models for compositional distributional
    semantics. In *Proceedings of the 23rd International Conference on Computational
    Linguistics (COLING)*
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanzotto 等人（2010）Zanzotto, F. M., Korkontzelos, I., Fallucchi, F., 和 Manandhar,
    S.（2010）。估计用于组合分布语义的线性模型。见 *Proceedings of the 23rd International Conference on
    Computational Linguistics (COLING)*
- en: 'Zeiler and Fergus (2014a) Zeiler, M. D. and Fergus, R. (2014a). Visualizing
    and understanding convolutional networks. In *Computer Vision – ECCV 2014*, eds.
    D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars (Cham: Springer International
    Publishing), 818–833'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeiler 和 Fergus（2014a）Zeiler, M. D. 和 Fergus, R.（2014a）。可视化与理解卷积网络。见 *Computer
    Vision – ECCV 2014*，编者 D. Fleet, T. Pajdla, B. Schiele, 和 T. Tuytelaars（Cham:
    Springer International Publishing），818–833'
- en: Zeiler and Fergus (2014b) Zeiler, M. D. and Fergus, R. (2014b). Visualizing
    and understanding convolutional networks. In *European Conference on Computer
    Vision* (Springer), 818–833
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler 和 Fergus（2014b）Zeiler, M. D. 和 Fergus, R.（2014b）。可视化与理解卷积网络。见 *European
    Conference on Computer Vision*（Springer），818–833
- en: Zou et al. (2013) Zou, W. Y., Socher, R., Cer, D. M., and Manning, C. D. (2013).
    Bilingual word embeddings for phrase-based machine translation. In *EMNLP*. 1393–1398
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人（2013）Zou, W. Y., Socher, R., Cer, D. M., 和 Manning, C. D.（2013）。双语词嵌入用于基于短语的机器翻译。见
    *EMNLP*，1393–1398
