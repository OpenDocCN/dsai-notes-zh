- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:55:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:55:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2104.11892] A Survey of Modern Deep Learning based Object Detection Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2104.11892] 现代深度学习基础的目标检测模型调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2104.11892](https://ar5iv.labs.arxiv.org/html/2104.11892)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2104.11892](https://ar5iv.labs.arxiv.org/html/2104.11892)
- en: A Survey of Modern Deep Learning based Object Detection Models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代深度学习基础的目标检测模型调查
- en: Syed Sahil Abbas Zaidi, Mohammad Samar Ansari, Asra Aslam,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Syed Sahil Abbas Zaidi, Mohammad Samar Ansari, Asra Aslam,
- en: 'Nadia Kanwal, Mamoona Asghar, and Brian Lee S.S.A. Zaidi, N. Kanwal, M Asghar
    and B. Lee are with the Athlone Institute of Technology, Ireland. M.S. Ansari
    is with the Aligarh Muslim University, India. A. Aslam is with the Insight Center
    for Data Analytics, National University of Ireland, Galway. (Emails: sahilzaidi78@gmail.com,
    samar.ansari@zhect.ac.in, asra.aslam@insight-centre.org, nkanwal@ait.ie, masghar@ait.ie,
    blee@ait.ie)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Nadia Kanwal, Mamoona Asghar 和 Brian Lee S.S.A. Zaidi, N. Kanwal, M Asghar 和
    B. Lee 均在爱尔兰的阿斯隆技术学院工作。M.S. Ansari 在印度的阿里格尔穆斯林大学工作。A. Aslam 在爱尔兰国立大学戈尔韦分校的数据分析洞察中心工作。（电子邮件：sahilzaidi78@gmail.com,
    samar.ansari@zhect.ac.in, asra.aslam@insight-centre.org, nkanwal@ait.ie, masghar@ait.ie,
    blee@ait.ie）
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Object Detection is the task of classification and localization of objects in
    an image or video. It has gained prominence in recent years due to its widespread
    applications. This article surveys recent developments in deep learning based
    object detectors. Concise overview of benchmark datasets and evaluation metrics
    used in detection is also provided along with some of the prominent backbone architectures
    used in recognition tasks. It also covers contemporary lightweight classification
    models used on edge devices. Lastly, we compare the performances of these architectures
    on multiple metrics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是图像或视频中物体的分类和定位任务。由于其广泛的应用，这一领域近年来得到了显著关注。本文调查了基于深度学习的目标检测器的最新发展，并简要概述了检测中使用的基准数据集和评估指标，以及一些用于识别任务的显著骨干架构。它还涵盖了在边缘设备上使用的当代轻量级分类模型。最后，我们比较了这些架构在多个指标上的性能。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引术语：
- en: 'Object detection and recognition, convolutional neural networks (CNN), lightweight
    networks, deep learning^†^†publicationid: pubid: Preprint submitted to IET Computer
    Vision.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '目标检测与识别，卷积神经网络（CNN），轻量级网络，深度学习^†^†publicationid: pubid: 预印本提交至 IET 计算机视觉。'
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Object detection is a trivial task for humans. A few months old child can start
    recognizing common objects, however teaching it to the computer has been an uphill
    task until the turn of the last decade. It entails identifying and localizing
    all instances of an object (like cars, humans, street signs, etc.) within the
    field of view. Similarly, other tasks like classification, segmentation, motion
    estimation, scene understanding, etc, have been the fundamental problems in computer
    vision.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对人类来说，目标检测是一项微不足道的任务。几个月大的婴儿就能开始识别常见物体，但直到上个十年的转折点，将其教给计算机一直是个艰巨的任务。这涉及到在视野范围内识别和定位所有物体实例（如汽车、人类、街道标志等）。类似地，分类、分割、运动估计、场景理解等其他任务一直是计算机视觉中的基本问题。
- en: Early object detection models were built as an ensemble of hand-crafted feature
    extractors such as Viola-Jones detector [viola_rapid_2001], Histogram of Oriented
    Gradients (HOG) [dalal_histograms_2005] etc. These models were slow, inaccurate
    and performed poorly on unfamiliar datasets. The re-introduction of convolutional
    neural network (CNNs) and deep learning for image classification changed the landscape
    of visual perception. Its use in the ImageNet Large Scale Visual Recognition Challenge
    (ILSVRC) 2012 challenge by AlexNet [NIPS2012_c399862d] inspired further research
    of its application in computer vision. Today, object detection finds application
    from self-driving cars and identity detection to security and medical uses. In
    recent years, it has seen exponential growth with rapid development of new tools
    and techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的目标检测模型是通过手工设计的特征提取器（如 Viola-Jones 检测器 [viola_rapid_2001]、方向梯度直方图 (HOG) [dalal_histograms_2005]
    等）构建的。这些模型较慢、不准确，并且在不熟悉的数据集上表现不佳。卷积神经网络 (CNNs) 和深度学习的重新引入改变了视觉感知的格局。AlexNet [NIPS2012_c399862d]
    在 ImageNet 大规模视觉识别挑战 (ILSVRC) 2012 挑战中的应用激发了对其在计算机视觉中应用的进一步研究。今天，目标检测从自动驾驶汽车和身份识别到安全和医疗用途都有应用。近年来，它随着新工具和技术的快速发展而呈指数增长。
- en: 'This survey provides a comprehensive review of deep learning based object detectors
    and lightweight classification architectures. While existing reviews are quite
    thorough [zou_object_2019, liu_deep_2018, chahal_survey_2018, jiao_survey_2019],
    most of them lack new developments in the domain. The main contributions of this
    paper are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述提供了基于深度学习的目标检测器和轻量级分类架构的全面回顾。虽然现有的综述已经相当详尽 [zou_object_2019, liu_deep_2018,
    chahal_survey_2018, jiao_survey_2019]，但它们大多数缺乏领域内的新进展。本文的主要贡献如下：
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: This paper provides an in-depth analysis of major object detectors in both categories
    – single and two stage detectors. Furthermore, we take historic look at the evolution
    of these methods.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文对两个类别的主要目标检测器进行了深入分析——单阶段检测器和双阶段检测器。此外，我们还回顾了这些方法的发展历程。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We present a detailed evaluation of the landmark backbone architectures and
    lightweight models. We could not find any paper which provides a broad overview
    of both these topics.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对标志性的骨干架构和轻量级模型进行了详细评估。我们未能找到任何一篇论文对这两个主题提供广泛的概述。
- en: In this paper, we have systematically reviewed various object detection architectures
    and its associated technologies, as illustrated in figure [1](#S1.F1 "Figure 1
    ‣ I Introduction ‣ A Survey of Modern Deep Learning based Object Detection Models").
    Rest of this paper is organized as follows. In section [II](#S2 "II Background
    ‣ A Survey of Modern Deep Learning based Object Detection Models"), the problem
    of object detection and its associated challenges are discussed. Various benchmark
    datasets and evaluation metrics are listed in Section [III](#S3 "III Datasets
    and Evaluation Metrics ‣ A Survey of Modern Deep Learning based Object Detection
    Models"). In Section [IV](#S4 "IV Backbone architectures ‣ A Survey of Modern
    Deep Learning based Object Detection Models"), several milestone backbone architectures
    used in modern object detectors are examined. Section [V](#S5 "V Object Detectors
    ‣ A Survey of Modern Deep Learning based Object Detection Models") is divided
    into three major sub-section, each studying a different category of object detectors.
    This is followed by the analysis of a special classification of object detectors,
    called lightweight networks in section [VI](#S6 "VI Lightweight Networks ‣ A Survey
    of Modern Deep Learning based Object Detection Models") and a comparative analysis
    in Section [VII](#S7 "VII Comparative Results ‣ A Survey of Modern Deep Learning
    based Object Detection Models"). The future trends are mentioned in Section LABEL:FuT
    while the paper is concluded in Section LABEL:Con.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们系统地回顾了各种目标检测架构及其相关技术，如图 [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Survey
    of Modern Deep Learning based Object Detection Models") 所示。本文其余部分的组织结构如下。在 [II](#S2
    "II Background ‣ A Survey of Modern Deep Learning based Object Detection Models")
    节中，讨论了目标检测问题及其相关挑战。各种基准数据集和评估指标列在 [III](#S3 "III Datasets and Evaluation Metrics
    ‣ A Survey of Modern Deep Learning based Object Detection Models") 节中。在 [IV](#S4
    "IV Backbone architectures ‣ A Survey of Modern Deep Learning based Object Detection
    Models") 节中，检查了现代目标检测器中使用的几个里程碑式的骨干架构。 [V](#S5 "V Object Detectors ‣ A Survey
    of Modern Deep Learning based Object Detection Models") 节分为三个主要子节，每个子节研究不同类别的目标检测器。接着在
    [VI](#S6 "VI Lightweight Networks ‣ A Survey of Modern Deep Learning based Object
    Detection Models") 节中分析了一种特殊分类的目标检测器，称为轻量级网络，并在 [VII](#S7 "VII Comparative Results
    ‣ A Survey of Modern Deep Learning based Object Detection Models") 节中进行比较分析。未来趋势在
    LABEL:FuT 节中提及，论文在 LABEL:Con 节中总结。
- en: '![Refer to caption](img/13cf4f352e1933238bcf6cb24a85f675.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/13cf4f352e1933238bcf6cb24a85f675.png)'
- en: 'Figure 1: Structure of the paper.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：论文结构。
- en: '![Refer to caption](img/dde13522b1ecb0584652542ec808d393.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/dde13522b1ecb0584652542ec808d393.png)'
- en: (a) PASCAL VOC 12
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PASCAL VOC 12
- en: '![Refer to caption](img/a1177e3b3c15f18fd4eb53d91c451b34.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/a1177e3b3c15f18fd4eb53d91c451b34.png)'
- en: (b) MS-COCO
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MS-COCO
- en: '![Refer to caption](img/30b73c5eaabf4d81320e270affe2f725.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/30b73c5eaabf4d81320e270affe2f725.png)'
- en: (c) ILSVRC
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (c) ILSVRC
- en: '![Refer to caption](img/039d0cca93685b9ab247c80e3ac46048.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/039d0cca93685b9ab247c80e3ac46048.png)'
- en: (d) OpenImage
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (d) OpenImage
- en: 'Figure 2: Sample images from different datasets.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：来自不同数据集的示例图像。
- en: II Background
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: II-A Problem Statement
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 问题陈述
- en: The object detection is the natural extension of object classification, which
    aims only at recognizing the object in the image. The goal of the object detection
    is to detect all instances of the predefined classes and provide its coarse localization
    in the image by axis-aligned boxes. The detector should be able to identify all
    instances of the object classes and draw bounding box around it. It is generally
    seen as a supervised learning problem. Modern object detection models have access
    to large sets of labelled images for training and are evaluated on various canonical
    benchmarks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是目标分类的自然延伸，目标分类仅旨在识别图像中的对象。目标检测的目标是检测所有预定义类别的实例，并通过轴对齐的框在图像中提供粗略定位。检测器应能够识别所有对象类别的实例，并绘制边界框。它通常被视为一个监督学习问题。现代目标检测模型可以访问大量标注图像进行训练，并在各种经典基准上进行评估。
- en: II-B Key challenges in Object Detection
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 目标检测中的关键挑战
- en: 'Computer vision has come a long way in the past decade, however it still has
    some major challenges to overcome. Some of these key challenges faced by the networks
    in real life applications are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉在过去十年中取得了长足进展，但仍面临一些主要挑战。这些挑战中一些在实际应用中面临的关键问题包括：
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intra class variation : Intra class variation between the instances of same
    object is relatively common in nature. This variation could be due to various
    reasons like occlusion, illumination, pose, viewpoint, etc. These unconstrained
    external can have dramatic effect of the object appearance [liu_deep_2018]. It
    is expected that the objects could have non-rigid deformation or be rotated, scaled
    or blurry. Some objects could have inconspicuous surroundings, making the extraction
    difficult.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类内变异：同一对象的实例之间的类内变异在自然界中相对常见。这种变异可能由于遮挡、光照、姿态、视角等各种原因造成。这些不受限制的外部因素会对对象的外观产生显著影响[liu_deep_2018]。预计对象可能会发生非刚性变形或出现旋转、缩放或模糊现象。有些对象可能具有不显眼的周围环境，使得提取变得困难。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of categories: The sheer number of object classes available to classify
    makes it a challenging problem to solve. It also requires more high-quality annotated
    data, which is hard to come by. Using fewer examples for training a detector is
    an open research question.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类别数量：可供分类的对象类别数量庞大，这使得问题解决起来具有挑战性。这也需要更多高质量的标注数据，而这些数据难以获得。使用较少的示例训练检测器仍是一个开放的研究问题。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Efficiency: Present day models need high computation resources to generate
    accurate detection results. With mobile and edge devices becoming common place,
    efficient object detectors are crucial for further development in the field of
    computer vision.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 效率：现代模型需要高计算资源才能生成准确的检测结果。随着移动设备和边缘设备的普及，高效的目标检测器对计算机视觉领域的进一步发展至关重要。
- en: III Datasets and Evaluation Metrics
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 数据集和评估指标
- en: III-A Datasets
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 数据集
- en: This section presents an overview of the datasets that are available, and have
    been most commonly used for object detection tasks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了可用的数据集，并且这些数据集在目标检测任务中被最常使用。
- en: III-A1 PASCAL VOC 07/12
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 PASCAL VOC 07/12
- en: The Pascal Visual Object Classes (VOC) challenge was a multiyear effort to accelerate
    the development in the field of visual perception. It started in 2005 with classification
    and detection tasks on four object classes [everingham_pascal_2010], but two versions
    of this challenges are mostly used as a standard benchmark. While the VOC07 challenge
    had 5k training images and more than 12k labelled objects [pascal-voc-2007], the
    VOC12 challenge increased them to 11k training images and more than 27k labelled
    objects [everingham_pascal_2012]. Object classes was expanded to 20 categories
    and the tasks like segmentation and action detection were included as well. Pascal
    VOC introduced the mean Average Precision (mAP) at 0.5 IoU (Intersection over
    Union) to evaluate the performance of the models. Figure [3](#S3.F3 "Figure 3
    ‣ III-A4 Open Image ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣ A
    Survey of Modern Deep Learning based Object Detection Models") depicts the distribution
    of the number of images w.r.t. to the different classes in the Pascal VOC dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Pascal视觉对象类别（VOC）挑战是促进视觉感知领域发展的多年努力。它始于2005年，包括四个物体类别的分类和检测任务[everingham_pascal_2010]，但是两个版本的这个挑战赛被多数人用作标准基准。虽然VOC07挑战有5千张训练图片和超过12千个标记物体[pascal-voc-2007]，VOC12挑战增加到了11千张训练图片和超过27千个标记物体[everingham_pascal_2012]。物体类别扩展到20个类别，并且还包括了分割和动作检测等任务。Pascal
    VOC引入了0.5 IoU（交并比）的平均精度均值（mAP）来评估模型的性能。图[3](#S3.F3 "图3 ‣ III-A4 开放图像 ‣ III-A 数据集
    ‣ III 数据集和评估指标 ‣ 基于现代深度学习的目标检测模型调查")描述了Pascal VOC数据集中不同类别图片数量的分布。
- en: III-A2 ILSVRC
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 ILSVRC
- en: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [russakovsky_imagenet_2015]
    was an annual challenge running from 2010 to 2017 and became a benchmark for evaluating
    algorithm performance. The dataset size was scaled up to more than a million images
    consisting of 1000 object classification classes. 200 of these classes were hand-picked
    for object detection task, constitute of more than 500k images. Various sources
    including ImageNet [deng_imagenet_2009] and Flikr, were used to construct detection
    dataset. ILSVRC also updated the evaluation metric by loosening the IoU threshold
    to help include smaller object detection. Figure [4](#S3.F4 "Figure 4 ‣ III-A4
    Open Image ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣ A Survey of
    Modern Deep Learning based Object Detection Models") depicts the distribution
    of the number of images w.r.t. to the different classes in the ImageNet dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet大规模视觉识别挑战（ILSVRC）[russakovsky_imagenet_2015]是一个从2010年到2017年每年举办的挑战赛，成为了评估算法性能的基准。数据集的规模扩大到了一百多万张图片，包括1000个目标分类类别。其中有200个类别被挑选出来用于目标检测任务，包括超过50万张图片。数据集的构建使用了包括ImageNet
    [deng_imagenet_2009]和Flikr在内的各种来源。ILSVRC还通过放宽IoU阈值来更新了评估指标，以帮助包含更小的目标检测。图[4](#S3.F4
    "图4 ‣ III-A4 开放图像 ‣ III-A 数据集 ‣ III 数据集和评估指标 ‣ 基于现代深度学习的目标检测模型调查")描述了ImageNet数据集中不同类别图片数量的分布。
- en: III-A3 MS-COCO
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 MS-COCO
- en: The Microsoft Common Objects in Context (MS-COCO) [Lin_ms_coco_2014] is one
    of the most challenging datasets available. It has 91 common objects found in
    their natural context which a 4-year-old human can easily recognize. It was launched
    in 2015 and its popularity has only increased since then. It has more than two
    million instances and an average of 3.5 categories per images. Furthermore, it
    contains 7.7 instances per image, comfortably more than other popular datasets.
    MS COCO comprises of images from varied viewpoints as well. It also introduced
    a more stringent method to measure the performance of the detector. Unlike the
    Pascal VOC and ILSVCR, it calculates the IoU from 0.5 to 0.95 in steps of 0.5,
    then using a combination of these 10 values as final metric, called Average Precision
    (AP). Apart from this, it also utilizes AP for small, medium and large objects
    separately to compare performance at different scales. Figure [5](#S3.F5 "Figure
    5 ‣ III-A4 Open Image ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣
    A Survey of Modern Deep Learning based Object Detection Models") depicts the distribution
    of the number of images w.r.t. to the different classes in the MS-COCO dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微软常见对象数据集 (MS-COCO) [Lin_ms_coco_2014] 是最具挑战性的数据集之一。它包含 91 个在自然环境中常见的对象，这些对象是
    4 岁的儿童可以轻松识别的。该数据集于 2015 年推出，自那时以来其受欢迎程度不断增加。它包含超过两百万个实例，平均每张图像 3.5 个类别。此外，它每张图像包含
    7.7 个实例，明显多于其他流行的数据集。MS COCO 还包含来自不同视角的图像。它还引入了一种更严格的方法来衡量检测器的性能。与 Pascal VOC
    和 ILSVCR 不同，它计算 IoU 从 0.5 到 0.95，每步 0.05，然后使用这 10 个值的组合作为最终度量，称为平均精度 (AP)。除此之外，它还利用
    AP 分别衡量小型、中型和大型对象，以比较不同尺度的性能。图 [5](#S3.F5 "Figure 5 ‣ III-A4 Open Image ‣ III-A
    Datasets ‣ III Datasets and Evaluation Metrics ‣ A Survey of Modern Deep Learning
    based Object Detection Models") 描述了 MS-COCO 数据集中不同类别的图像数量分布。
- en: 'TABLE I: Comparison of various object detection datasets.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：各种对象检测数据集的比较。
- en: '| Dataset | Classes | Train | Validation | Test |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类别 | 训练 | 验证 | 测试 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  | Images | Objects | Objects/Image | Images | Objects | Objects/Image
    |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 图像 | 对象 | 对象/图像 | 图像 | 对象 | 对象/图像 |  |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| PASCAL VOC 12 | 20 | 5,717 | 13,609 | 2.38 | 5,823 | 13,841 | 2.37 | 10,991
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL VOC 12 | 20 | 5,717 | 13,609 | 2.38 | 5,823 | 13,841 | 2.37 | 10,991
    |'
- en: '| MS-COCO | 80 | 118,287 | 860,001 | 7.27 | 5,000 | 36,781 | 7.35 | 40,670
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| MS-COCO | 80 | 118,287 | 860,001 | 7.27 | 5,000 | 36,781 | 7.35 | 40,670
    |'
- en: '| ILSVRC | 200 | 456,567 | 478,807 | 1.05 | 20,121 | 55,501 | 2.76 | 40,152
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ILSVRC | 200 | 456,567 | 478,807 | 1.05 | 20,121 | 55,501 | 2.76 | 40,152
    |'
- en: '| OpenImage | 600 | 1,743,042 | 14,610,229 | 8.38 | 41,620 | 204,621 | 4.92
    | 125,436 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| OpenImage | 600 | 1,743,042 | 14,610,229 | 8.38 | 41,620 | 204,621 | 4.92
    | 125,436 |'
- en: III-A4 Open Image
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 开放图像
- en: Google’s Open Images [kuznetsova_open_2020] dataset is composed of 9.2 million
    images, annotated with image-level labels, object bounding boxes, and segmentation
    masks, among others. It was launched in 2017 and has received six updates. For
    object detection, Open Images has 16 million bounding boxes for 600 categories
    on 1.9 million images, which makes it the largest dataset of object localization.
    Its creators took extra care to choose interesting, complex and diverse images,
    having 8.3 object categories per image. Several changes were made to the AP introduced
    in Pascal VOC like ignoring un-annotated class, detection requirement for class
    and its subclass, etc. Figure [6](#S3.F6 "Figure 6 ‣ III-A4 Open Image ‣ III-A
    Datasets ‣ III Datasets and Evaluation Metrics ‣ A Survey of Modern Deep Learning
    based Object Detection Models") depicts the distribution of the number of images
    w.r.t. to the different classes in the Open Images dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的开放图像数据集 [kuznetsova_open_2020] 由 920 万张图像组成，附有图像级标签、对象边界框和分割掩码等注释。该数据集于 2017
    年推出，并已经接收了六次更新。对于对象检测，开放图像数据集在 190 万张图像上有 1600 万个边界框，涵盖 600 个类别，这使得它成为最大的对象定位数据集。其创建者特别挑选了有趣、复杂和多样化的图像，每张图像包含
    8.3 个对象类别。对 AP 进行了几项更改，例如忽略未标注的类别、对类别及其子类的检测要求等。图 [6](#S3.F6 "Figure 6 ‣ III-A4
    Open Image ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣ A Survey of
    Modern Deep Learning based Object Detection Models") 描述了开放图像数据集中不同类别的图像数量分布。
- en: '![Refer to caption](img/6f6ddf6cfbe58ced27cd2cfd1177f891.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6f6ddf6cfbe58ced27cd2cfd1177f891.png)'
- en: 'Figure 3: (This image is best viewed in PDF form with magnification) Number
    of images for different classes annotated in the PascalVOC dataset [aslam2021survey]'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：（此图像最佳查看方式为放大 PDF 格式）PascalVOC 数据集中不同类别的图像数量注释 [aslam2021survey]
- en: '![Refer to caption](img/1af99794286f5936723b931744b76cc6.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1af99794286f5936723b931744b76cc6.png)'
- en: 'Figure 4: (This image is best viewed in PDF form with magnification) Number
    of images for different classes annotated in the ImageNet dataset [aslam2021survey]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: （此图像在放大状态下以 PDF 形式查看效果最佳）ImageNet 数据集中不同类别的图像数量 [aslam2021survey]'
- en: '![Refer to caption](img/f06501b78639dedd0d7f6a7621e41d71.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f06501b78639dedd0d7f6a7621e41d71.png)'
- en: 'Figure 5: (This image is best viewed in PDF form with magnification) Number
    of images for different classes annotated in the MS-COCO dataset [aslam2021survey]'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: （此图像在放大状态下以 PDF 形式查看效果最佳）MS-COCO 数据集中不同类别的图像数量 [aslam2021survey]'
- en: '![Refer to caption](img/58ebafb04acd48c9ac2452f938689004.png)![Refer to caption](img/a2d835e29ff6ebd56ac2a7a36a48a1be.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/58ebafb04acd48c9ac2452f938689004.png)![参见说明](img/a2d835e29ff6ebd56ac2a7a36a48a1be.png)'
- en: 'Figure 6: (This image is best viewed in PDF form with magnification) Number
    of images for different classes annotated in the Open Images dataset [aslam2021survey]'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: （此图像在放大状态下以 PDF 形式查看效果最佳）Open Images 数据集中不同类别的图像数量 [aslam2021survey]'
- en: III-A5 Issues of Data Skew/Bias
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A5 数据偏斜/偏差问题
- en: 'While observing Fig. [3](#S3.F3 "Figure 3 ‣ III-A4 Open Image ‣ III-A Datasets
    ‣ III Datasets and Evaluation Metrics ‣ A Survey of Modern Deep Learning based
    Object Detection Models") through Fig. [6](#S3.F6 "Figure 6 ‣ III-A4 Open Image
    ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣ A Survey of Modern Deep
    Learning based Object Detection Models"), an alert reader would certainly notice
    that the number of images for difference classes vary significantly in all the
    datasets [aslam2021survey]. Three (Pascal VOC, MS-COCO, and Open Images Dataset)
    of the four datasets discussed above have a very significant drop in the number
    of images beyond the top-5 most frequent classes. As can be readily observed for
    Fig. [3](#S3.F3 "Figure 3 ‣ III-A4 Open Image ‣ III-A Datasets ‣ III Datasets
    and Evaluation Metrics ‣ A Survey of Modern Deep Learning based Object Detection
    Models"), there are 13775 images which contain a ‘person’ and then 2829 images
    which contain a ‘car’. The number of images for the remaining 18 classes in this
    dataset almost fall linearly to the 55 images of ‘sheep’. Similarly, for the MS-COCO
    dataset, the class ‘person’ has 262465 images, and the next most-frequent class
    ‘car’ has 43867 images. The downward trend continues till there are only 198 images
    for the class ‘hair drier’. A similar phenomenon is also observed in the Open
    Images Dataset, wherein the class ‘Man’ is the most frequent with 378077 images,
    and the class ‘Paper Cutter’ has only 3 images. This clearly represents a skew
    in the datasets and is bound to create a bias in the training process of any object
    detection model. Therefore, an object detection model trained on these skewed
    datasets will in all probability show better detection performance for the classes
    with more number of images in the training data. Although still present, this
    issue is slightly less pronounced in the ImageNet dataset, as can be observed
    from Fig. [4](#S3.F4 "Figure 4 ‣ III-A4 Open Image ‣ III-A Datasets ‣ III Datasets
    and Evaluation Metrics ‣ A Survey of Modern Deep Learning based Object Detection
    Models") from where it can be seen that the most frequent class i.e. ‘koala’ has
    2469 images, and the least frequent class i.e. ‘cart’ has 624 images. However,
    this leads to another point of concern in the ImageNet dataset: the most frequent
    class is for ‘koala’ and the next most-appearing class is ‘computer keyboard’,
    which are clearly not the most sought after objects in a real-world object detection
    scenario (where person, cars, traffic signs, etc. are of higher concern).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察图[3](#S3.F3 "图 3 ‣ III-A4 打开图像 ‣ III-A 数据集 ‣ III 数据集和评估指标 ‣ 现代深度学习基础的目标检测模型概述")到图[6](#S3.F6
    "图 6 ‣ III-A4 打开图像 ‣ III-A 数据集 ‣ III 数据集和评估指标 ‣ 现代深度学习基础的目标检测模型概述")时，一个警觉的读者肯定会注意到，不同类别的图像数量在所有数据集中差异显著[aslam2021survey]。上述四个数据集中有三个（Pascal
    VOC、MS-COCO 和 Open Images 数据集）在前五个最频繁的类别之外，图像数量显著减少。正如图[3](#S3.F3 "图 3 ‣ III-A4
    打开图像 ‣ III-A 数据集 ‣ III 数据集和评估指标 ‣ 现代深度学习基础的目标检测模型概述")中可以清楚地观察到，包含‘人’的图像有13775张，而包含‘车’的图像有2829张。这个数据集中其余18个类别的图像数量几乎线性减少到‘羊’的55张图像。同样，在
    MS-COCO 数据集中，类别‘人’有262465张图像，接下来最频繁的类别‘车’有43867张图像。下降趋势持续，直到‘吹风机’类别的图像数量只有198张。Open
    Images 数据集中也观察到了类似现象，其中‘人’类别最为频繁，有378077张图像，而‘纸刀’类别仅有3张图像。这明显代表了数据集的偏差，并且必然会在任何目标检测模型的训练过程中产生偏倚。因此，在这些偏斜的数据集上训练的目标检测模型很可能会对训练数据中图像数量更多的类别表现出更好的检测性能。尽管仍然存在，但这个问题在
    ImageNet 数据集中略微减少，可以从图[4](#S3.F4 "图 4 ‣ III-A4 打开图像 ‣ III-A 数据集 ‣ III 数据集和评估指标
    ‣ 现代深度学习基础的目标检测模型概述")中观察到，其中最频繁的类别‘考拉’有2469张图像，而最不频繁的类别‘手推车’有624张图像。然而，这引出了 ImageNet
    数据集的另一个问题：最频繁的类别是‘考拉’，其次是‘电脑键盘’，这些显然不是现实世界目标检测场景中最受关注的对象（如人、车、交通标志等）。
- en: III-B Metrics
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 指标
- en: Object detectors use multiple criteria to measure the performance of the detectors
    viz., frames per second (FPS), precision and recall. However, mean Average Precision
    (mAP) is the most common evaluation metric. Precision is derived from Intersection
    over Union (IoU), which is the ratio of the area of overlap and the area of union
    between the ground truth and the predicted bounding box. A threshold is set to
    determine if the detection is correct. If the IoU is more than the threshold,
    it is classified as True Positive while an IoU below it is classified as False
    Positive. If the model fails to detect an object present in the ground truth,
    it is termed as False Negative. Precision measures the percentage of correct predictions
    while the recall measure the correct predictions with respect to the ground truth.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测器使用多个标准来衡量检测器的性能，例如每秒帧数（FPS）、精确度和召回率。然而，平均精度均值（mAP）是最常用的评估指标。精确度源自交并比（IoU），即真实框和预测边界框之间重叠区域与联合区域的比率。设置一个阈值来确定检测是否正确。如果IoU大于阈值，则分类为真正例（True
    Positive），而IoU低于阈值则分类为假正例（False Positive）。如果模型未能检测到真实框中存在的物体，则称为假负例（False Negative）。精确度测量的是正确预测的百分比，而召回率测量的是相对于真实框的正确预测。
- en: '|  | $\begin{split}Precision&amp;=\frac{True\ Positive}{True\ Positive+False\
    Positive}\\ &amp;=\frac{True\ Positive}{All\ Observations}\end{split}$ |  | (1)
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}Precision&amp;=\frac{True\ Positive}{True\ Positive+False\
    Positive}\\ &amp;=\frac{True\ Positive}{All\ Observations}\end{split}$ |  | (1)
    |'
- en: '|  | $\begin{split}Recall&amp;=\frac{True\ Positive}{True\ Positive+False\
    Negative}\\ &amp;=\frac{True\ Positive}{All\ Ground\ Truth}\end{split}$ |  | (2)
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}Recall&amp;=\frac{True\ Positive}{True\ Positive+False\
    Negative}\\ &amp;=\frac{True\ Positive}{All\ Ground\ Truth}\end{split}$ |  | (2)
    |'
- en: Based on the above equation, average precision is computed separately for each
    class. To compare performance between the detectors, the mean of average precision
    of all classes, called mean average precision (mAP) is used, which acts as a single
    metric for final evaluation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述公式，平均精度是针对每个类别单独计算的。为了比较检测器之间的性能，使用所有类别的平均精度的均值，称为平均精度均值（mAP），它作为最终评估的单一指标。
- en: IV Backbone architectures
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 主干网络架构
- en: 'Backbone architectures are one of the most important component of the object
    detector. These networks extract feature from the input image used by the model.
    Here, we have discussed some milestone backbone architectures used in modern detectors:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 主干网络架构是目标检测器最重要的组成部分之一。这些网络从模型使用的输入图像中提取特征。这里，我们讨论了一些在现代检测器中使用的里程碑式主干网络架构：
- en: IV-A AlexNet
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A AlexNet
- en: 'Krizhevsky et al. proposed AlexNet [NIPS2012_c399862d], a convolutional neural
    network based architecture for image classification, and won the ImageNet Large-Scale
    Visual Recognition Challenge (ILSVRC) 2012 challenge. It achieved a considerably
    higher accuracy (more than 26%) than the contemporary models. AlexNet is composed
    of eight learnable layers - five convolutional and three fully connected layers.
    The last layer of the fully connected layer is connected to an N-way (N: number
    of classes) softmax classifier. It uses multiple convolutional kernels throughout
    the network to obtain features from the image. It also uses dropout and ReLU for
    regularization and faster training convergence respectively. The convolutional
    neural networks were given a new life by its reintroduction in AlexNet and it
    soon became the go-to technique in processing imaging data.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'Krizhevsky等人提出了AlexNet [NIPS2012_c399862d]，这是一种基于卷积神经网络的图像分类架构，并赢得了2012年ImageNet大规模视觉识别挑战（ILSVRC）。它的准确性比当时的模型高出相当多（超过26%）。AlexNet由八个可学习的层组成——五个卷积层和三个全连接层。全连接层的最后一层连接到一个N类（N:
    类别数）的softmax分类器。它在整个网络中使用多个卷积核来从图像中提取特征。它还使用dropout和ReLU分别用于正则化和更快的训练收敛。卷积神经网络通过在AlexNet中的重新引入获得了新的生命，并迅速成为处理图像数据的首选技术。'
- en: IV-B VGG
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B VGG
- en: While AlexNet [NIPS2012_c399862d] and its successors like [zeiler_visualizing_2014]
    focused on smaller receptive window size to improve accuracy, Simonyan and Zisserman
    investigated the effects of network depth on it. They proposed VGG [simonyan_very_2015],
    which used small convolution filters to construct networks of varying depths.
    While a larger receptive field can be captured by a set of smaller convolutional
    filters, it drastically reduces network parameters and converges sooner. The paper
    demonstrated how deep network architecture (16-19 layers) can be used to perform
    classification and localization with superior accuracy. VGG was created by adding
    a stack of convolutional layers with three fully connected layers, followed by
    a softmax layer. The number of convolutional layers, according to the authors,
    can vary from 8 to 16\. VGG is trained in multiple iterations; first, the smallest
    11-layer architecture is trained with random initialization whose weights are
    then used to train larger networks to prevent gradient instability. VGG outperformed
    ILSVRC 2014 winner GoogLeNet [szegedy_going_2014] in the single network performance
    category. It soon became one of the most used network backbones for object classification
    and detection models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AlexNet [NIPS2012_c399862d] 及其后续模型如 [zeiler_visualizing_2014] 关注于通过较小的感受野来提高准确性，但
    Simonyan 和 Zisserman 研究了网络深度对其影响。他们提出了 VGG [simonyan_very_2015]，该网络使用小卷积滤波器构建不同深度的网络。虽然一组较小的卷积滤波器可以捕捉到更大的感受野，但它会显著减少网络参数并更快地收敛。论文展示了深度网络架构（16-19
    层）如何用于执行分类和定位，并提供了更高的准确性。VGG 是通过添加一系列卷积层、三个全连接层以及一个 softmax 层而创建的。根据作者的说法，卷积层的数量可以从
    8 层到 16 层不等。VGG 经过多次迭代训练；首先，用随机初始化训练最小的 11 层架构，然后使用其权重训练更大的网络，以防止梯度不稳定。VGG 在单网络性能类别中超越了
    ILSVRC 2014 的获胜者 GoogLeNet [szegedy_going_2014]。它很快成为对象分类和检测模型中最常用的网络骨干之一。
- en: IV-C GoogLeNet/Inception
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C GoogLeNet/Inception
- en: Even though classification networks were making inroads towards faster and more
    accurate networks, deploying them in real-world applications was still a long
    way off as they were resource-intensive. As networks are scaled for better performance,
    the computation cost increases exponentially. Szegedy et al. in [szegedy_going_2014]
    postulated the wastage of computations in the network as a major reason for it.
    Bigger models also have a large number of parameters and tend to overfit the data.
    They proposed using locally sparse connected architecture instead of a fully connected
    one to solve these issues. GoogLeNet is thus a 22 layer deep network, made up
    by stacking multiple Inception modules on top of each other. Inception modules
    are networks that have multiple sized filters at the same level. Input feature
    maps pass through these filters and are concatenated and forwarded to the next
    layer. The network also has auxiliary classifiers in the intermediate layers to
    help regularize and propagate gradient. GoogLeNet showed how efficient use of
    computation blocks can perform at par with other parameter-heavy networks. It
    achieved 93.3% top-5 accuracy on ImageNet [russakovsky_imagenet_2015] dataset
    without external data, while being faster than other contemporary models. Updated
    versions of Inception like [szegedy_rethinking_2016],[szegedy_inception-v4_2016]
    were also published in the following years which further improved its performance
    and gave further evidence of the applications of refined sparsely connected architectures.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分类网络在向更快、更准确的网络发展，但将它们部署到实际应用中仍然遥不可及，因为这些网络资源消耗很大。随着网络规模的扩大，计算成本呈指数级增加。Szegedy
    等人在 [szegedy_going_2014] 中提出，网络计算的浪费是其主要原因之一。更大的模型也有大量的参数，容易导致数据过拟合。他们提出使用局部稀疏连接架构来替代完全连接的架构，以解决这些问题。因此，GoogLeNet
    是一个 22 层深的网络，由多个 Inception 模块堆叠而成。Inception 模块是在同一层级上具有不同尺寸滤波器的网络。输入特征图通过这些滤波器，并被拼接后传递到下一层。网络还在中间层中具有辅助分类器，以帮助正则化和传播梯度。GoogLeNet
    展示了如何通过有效利用计算块来与其他参数密集型网络相媲美。它在 ImageNet [russakovsky_imagenet_2015] 数据集上实现了 93.3%
    的 top-5 准确率，并且比其他当代模型更快。更新版本的 Inception 如 [szegedy_rethinking_2016]、[szegedy_inception-v4_2016]
    也在随后的几年中发布，进一步提升了其性能，并提供了精细稀疏连接架构应用的进一步证据。
- en: IV-D ResNets
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D ResNets
- en: As convolutional neural networks become deeper and deeper, Kaiming He et al.
    in [kaiming_resnet_2016] showed how their accuracy first saturates and then degrades
    rapidly. They proposed the use of residual learning to the stacked layers to mitigate
    the performance decay. It is realized by addition of a skip connection between
    the layers. This connection is an element-wise addition between input and output
    of the block and does not add extra parameter or computational complexity to the
    network. A typical 34 layer ResNet [kaiming_resnet_2016] is basically a large
    (7x7) convolution filter followed by 16 bottleneck modules (pair of small 3x3
    filters with identity shortcut across them) and ultimately a fully connected layer.
    The bottleneck architecture can be adapted for deeper networks by stacking 3 convolutional
    layers (1x1,3x3,1x3) instead of 2\. Kaiming He et al. also demonstrated how the
    16-layer VGG net had higher complexity than their considerably deeper 101 and
    152 layer ResNet architectures while having lower accuracy. In subsequent paper,
    the authors proposed Resnetv2 [he_identity_2016] which used batch normalization
    and ReLU layer in the blocks. It is more generalized and easier to train. ResNets
    are widely used in classification and detection backbones, and its core principles
    have inspired many networks ([huang_densely_2018, xie_aggregated_2017, szegedy_inception-v4_2016]).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 随着卷积神经网络变得越来越深，Kaiming He 等人在 [kaiming_resnet_2016] 中展示了其准确性如何先饱和后迅速下降。他们提出使用残差学习来解决性能衰退问题。这是通过在层之间添加跳跃连接来实现的。这个连接是输入和块输出之间的逐元素加法，不会给网络增加额外的参数或计算复杂度。一个典型的
    34 层 ResNet [kaiming_resnet_2016] 基本上是一个大型 (7x7) 卷积滤波器，后跟 16 个瓶颈模块（成对的小 3x3 滤波器，中间有身份捷径），最终是一个全连接层。瓶颈架构可以通过堆叠
    3 个卷积层（1x1,3x3,1x1）来适应更深的网络，而不是 2 个。Kaiming He 等人还展示了 16 层 VGG 网络的复杂性高于他们深得多的
    101 层和 152 层 ResNet 架构，同时准确性较低。在随后的论文中，作者提出了 ResNetv2 [he_identity_2016]，该架构在块中使用了批量归一化和
    ReLU 层。它更加通用，训练更容易。ResNet 被广泛用于分类和检测主干网络，其核心原则也启发了许多网络（[huang_densely_2018, xie_aggregated_2017,
    szegedy_inception-v4_2016]）。
- en: IV-E ResNeXt
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E ResNeXt
- en: The existing conventional methods of improving the accuracy of a model were
    by either increasing the depth or the width of the model. However, increasing
    any of these leads to higher model complexity and number of parameters while the
    gain margins diminish rapidly. Xie et al. introduced ResNeXt [xie_aggregated_2017]
    architecture which is simpler and more efficient than other existing models. ResNeXt
    was inspired by the stacking of similar blocks in VGG/ResNet[kaiming_resnet_2016,
    NIPS2012_c399862d] and “split-transform-merge” behavior of Inception module[szegedy_going_2014].
    It is essentially a ResNet where each ResNet block is replaced by an inception-like
    ResNeXt module. The complicated, tailored transformation modules from the Inception
    is replaced by topologically same modules in the ResNeXt blocks, making the network
    easier to scale and generalize. Xie et al. also emphasize that the cardinality
    (topological paths in the ResNeXt block) can be considered as a third dimension,
    along with depth and width, to improve model accuracy. ResNeXt is elegant and
    more concise. It achieved higher accuracy while having considerably fewer hyperparameters
    than a similar depth ResNet architecture. It was also the first runner up to the
    ILSVRC 2016 challenge.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现有传统方法通过增加模型的深度或宽度来提高模型的准确性。然而，增加这些参数会导致模型复杂性和参数数量增加，同时收益边际迅速减少。Xie 等人引入了 ResNeXt
    [xie_aggregated_2017] 架构，它比其他现有模型更简单、更高效。ResNeXt 的灵感来源于 VGG/ResNet[kaiming_resnet_2016,
    NIPS2012_c399862d] 中相似块的堆叠以及 Inception 模块[szegedy_going_2014] 的“分裂-变换-合并”行为。它本质上是一个
    ResNet，每个 ResNet 块都被类似 Inception 的 ResNeXt 模块所替代。Inception 中复杂的、量身定制的变换模块被 ResNeXt
    块中的拓扑结构相同的模块所替代，使得网络更容易扩展和泛化。Xie 等人还强调，基数（ResNeXt 块中的拓扑路径）可以被视为第三个维度，与深度和宽度一起用于提高模型准确性。ResNeXt
    优雅且更加简洁。它在具有明显较少的超参数的情况下达到了更高的准确性。它也是 ILSVRC 2016 挑战的第一名亚军。
- en: 'TABLE II: Comparison of Backbone architectures.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：主干网络架构的比较。
- en: '| Model | Year | Layers | Parameters (Million) | Top-1 acc% | FLOPs (Billion)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 层数 | 参数（百万） | Top-1 精度% | FLOPs（十亿） |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| AlexNet | 2012 | 7 | 62.4 | 63.3 | 1.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet | 2012 | 7 | 62.4 | 63.3 | 1.5 |'
- en: '| VGG-16 | 2014 | 16 | 138.4 | 73 | 15.5 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| VGG-16 | 2014 | 16 | 138.4 | 73 | 15.5 |'
- en: '| GoogLeNet | 2014 | 22 | 6.7 | - | 1.6 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| GoogLeNet | 2014 | 22 | 6.7 | - | 1.6 |'
- en: '| ResNet-50 | 2015 | 50 | 25.6 | 76 | 3.8 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 | 2015 | 50 | 25.6 | 76 | 3.8 |'
- en: '| ResNeXt-50 | 2016 | 50 | 25 | 77.8 | 4.2 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ResNeXt-50 | 2016 | 50 | 25 | 77.8 | 4.2 |'
- en: '| CSPResNeXt-50 | 2019 | 59 | 20.5 | 78.2 | 7.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| CSPResNeXt-50 | 2019 | 59 | 20.5 | 78.2 | 7.9 |'
- en: '| EfficientNet-B4 | 2019 | 160 | 19 | 83 | 4.2 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet-B4 | 2019 | 160 | 19 | 83 | 4.2 |'
- en: IV-F CSPNet
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F CSPNet
- en: Existing neural networks have shown incredible results in achieving high accuracy
    in computer vision tasks; however, they rely on excessive computational resources.
    Wang et al. believe that heavy inference computations can be reduced by cutting
    down the duplicate gradient information in the network. They proposed CSPNet [wang_cspnet_2019]
    which creates different paths for the gradient flow within the network. CSPNet
    separates feature maps at the base layer into two parts. One part is passed through
    the partial convolution network block (e.g., Dense and Transition block in DenseNet
    [huang_densely_2018] or Res(X) block in ResNeXt [xie_aggregated_2017]) while the
    other part is combined with its outputs at a later stage. This reduces the number
    of parameters, increases the utilization of computation units and eases memory
    footprint. It is easy to implement and general enough to be applicable on other
    architectures like ResNet [kaiming_resnet_2016], ResNeXt [xie_aggregated_2017],
    DenseNet [huang_densely_2018], Scaled-YOLOv4 [wang_scaled-yolov4_2020] etc. Applying
    CSPNet on these networks reduced computations from 10% to 20%, while the accuracy
    remained constant or improved. Memory cost and computational bottleneck is also
    reduced significantly with this method. It is leveraged in many state of the art
    detector models, while also being used for mobile and edge devices.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的神经网络在计算机视觉任务中取得了令人惊叹的高准确率；然而，它们依赖于过多的计算资源。王等人认为，通过减少网络中的重复梯度信息，可以降低推理计算的复杂度。他们提出了CSPNet
    [wang_cspnet_2019]，该网络为梯度流创建了不同的路径。CSPNet在基础层将特征图分为两个部分。一部分通过部分卷积网络块（例如，DenseNet
    [huang_densely_2018] 中的 Dense 和 Transition 块或 ResNeXt [xie_aggregated_2017] 中的
    Res(X) 块），而另一部分则在后续阶段与其输出进行结合。这减少了参数的数量，提高了计算单元的利用率，并减轻了内存占用。它易于实现且足够通用，适用于其他架构，如
    ResNet [kaiming_resnet_2016]、ResNeXt [xie_aggregated_2017]、DenseNet [huang_densely_2018]、Scaled-YOLOv4
    [wang_scaled-yolov4_2020] 等。将 CSPNet 应用于这些网络将计算量减少了 10% 到 20%，同时准确率保持不变或有所提高。内存成本和计算瓶颈也通过这种方法显著减少。它在许多最先进的检测器模型中得到了应用，同时也被用于移动设备和边缘设备。
- en: '![Refer to caption](img/af1d5b84b389ad9d65a1e6c841befc99.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/af1d5b84b389ad9d65a1e6c841befc99.png)'
- en: 'Figure 7: Visualization of CNN Architectures²²2Tool Used: https://netron.app/.
    Left to Right: AlexNet, VGG$-$16, GoogLeNet, ResNet$-$50, CSPResNeXt$-$50, EfficientNet$-$B4.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：CNN 架构的可视化²²2使用的工具： https://netron.app/。从左到右：AlexNet、VGG$-$16、GoogLeNet、ResNet$-$50、CSPResNeXt$-$50、EfficientNet$-$B4。
- en: IV-G EfficientNet
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-G EfficientNet
- en: Tan et al. systematically studied network scaling and its effects on the model
    performance. They summarized how altering network parameters like depth, width
    and resolution influence its accuracy. Scaling any parameter individually comes
    with an associated cost. Increasing depth of a network can help in capturing richer
    and more complex features, but they are difficult to train due to vanishing gradient
    problem. Similarly, scaling network width will make it easier to capture fine
    grained features but have difficulty in obtaining high level features. Gains from
    increasing the image resolution, like depth and width, saturate as model scales.
    In the paper [tan_efficientnet_2020], Tan et al. proposed the use of a compound
    coefficient that can uniformly scale all three dimensions. Each model parameter
    has an associated constant, which is found by fixing the coefficient as $1$ and
    performing a grid search on a baseline network. The baseline architecture, inspired
    by their previous work [tan_mnasnet_2018], is developed by neural architecture
    search on a search target while optimizing accuracy and computations. EfficientNet
    is a simple and efficient architecture. It outperformed existing models in accuracy
    and speed while being considerably smaller. By providing a monumental increase
    in efficiency, it could potentially open a new era in the field of efficient networks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Tan 等人系统地研究了网络扩展及其对模型性能的影响。他们总结了改变网络参数如深度、宽度和分辨率如何影响其准确性。单独扩展任何参数都会带来相关的成本。增加网络的深度可以帮助捕捉更丰富和复杂的特征，但由于梯度消失问题，这些特征难以训练。同样，扩展网络宽度将使捕捉细粒度特征变得更容易，但在获取高级特征方面会有困难。提高图像分辨率带来的收益，像深度和宽度一样，会随着模型的扩展而饱和。在论文
    [tan_efficientnet_2020] 中，Tan 等人提出使用复合系数来均匀扩展所有三个维度。每个模型参数都有一个相关的常数，这个常数通过将系数固定为
    $1$ 并对基准网络进行网格搜索来找到。基准架构受到他们之前工作 [tan_mnasnet_2018] 的启发，通过在优化准确性和计算的搜索目标上进行神经架构搜索来开发。EfficientNet
    是一个简单而高效的架构。它在准确性和速度上超过了现有模型，同时体积显著更小。通过提供显著的效率提升，它有可能在高效网络领域开辟一个新时代。
- en: V Object Detectors
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 目标检测器
- en: We have divided this review based on the two types of detectors — two-stage
    and single-stage detectors. However, we also discussed the pioneer work, where
    we briefly examine a few traditional object detectors. A network which has a separate
    module to generate region proposals is termed as a two-stage detector. These models
    try to find an arbitrary number of objects proposals in an image during the first
    stage and then classify and localize them in the second. As these systems have
    two separate steps, they generally take longer to generate proposals, have complicated
    architecture and lacks global context. Single-stage detectors classify and localize
    semantic objects in a single shot using dense sampling. They use predefined boxes/keypoints
    of various scale and aspect ratio to localize objects. It edges two-stage detectors
    in real-time performance and simpler design.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据两种检测器类型——两阶段检测器和单阶段检测器，划分了这篇综述。然而，我们也讨论了开创性工作，简要考察了一些传统的目标检测器。一个具有单独模块来生成区域提议的网络被称为两阶段检测器。这些模型在第一阶段试图在图像中找到任意数量的目标提议，然后在第二阶段对它们进行分类和定位。由于这些系统有两个独立的步骤，它们通常需要更长的时间生成提议，具有复杂的架构并缺乏全局上下文。单阶段检测器通过密集采样在一次操作中分类和定位语义对象。它们使用预定义的框/关键点来定位对象，并具有比两阶段检测器更好的实时性能和更简单的设计。
- en: V-A Pioneer Work
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 开创性工作
- en: V-A1 Viola-Jones
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 Viola-Jones
- en: Primarily designed for face detection, Viola-Jones object detector [viola_rapid_2001],
    proposed in 2001, was an accurate and powerful detector. It combined multiple
    techniques like Haar-like features, integral image, Adaboost and cascading classifier.
    First step is to search for Haar-like features by sliding a window on the input
    image and uses integral image to calculate. It then uses a trained Adaboost to
    find the classifier of each haar feature and cascades them. Viola Jones algorithm
    is still used in small devices as it is very efficient and fast.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Viola-Jones 目标检测器 [viola_rapid_2001] 主要用于人脸检测，2001 年提出，是一个准确且强大的检测器。它结合了多种技术，如
    Haar 类似特征、积分图像、Adaboost 和级联分类器。第一步是通过在输入图像上滑动窗口来搜索 Haar 类似特征，并使用积分图像进行计算。接着，它使用训练过的
    Adaboost 找到每个 Haar 特征的分类器并将其级联。Viola-Jones 算法仍在小型设备中使用，因为它非常高效和快速。
- en: V-A2 HOG Detector
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 HOG 检测器
- en: In 2005, Dalal and Triggs proposed the Histogram of Oriented Gradients (HOG)
    [dalal_histograms_2005] feature descriptor used to extract features for object
    detection. It was an improvement over other detectors like [lowe_distinctive_2004,
    lowe_object_1999, mohan_example-based_2001, yan_ke_pca-sift_2004]. HOG extracts
    gradient and its orientation of the edges to create a feature table. The image
    is divided into grids and the feature table is then used to create histogram for
    each cell in the grid. HOG features are generated for the region of interest and
    fed into a linear SVM classifier for detection. The detector was proposed for
    pedestrian detection; however, it could be trained to detect various classes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在2005年，Dalal和Triggs提出了方向梯度直方图（HOG）[dalal_histograms_2005] 特征描述符，用于提取对象检测的特征。它相较于其他检测器如
    [lowe_distinctive_2004, lowe_object_1999, mohan_example-based_2001, yan_ke_pca-sift_2004]
    有了改进。HOG提取边缘的梯度及其方向来创建特征表。图像被划分为网格，然后使用特征表为网格中的每个单元创建直方图。HOG特征生成区域感兴趣的部分，并输入线性SVM分类器进行检测。该检测器最初是为行人检测提出的，但也可以训练检测各种类别。
- en: V-A3 DPM
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A3 DPM
- en: Deformable Parts Model (DPM) [felzenszwalb_discriminatively_2008] was introduced
    by Felzenszwalb et al. and was the winner Pascal VOC challenge in 2009\. It used
    individual “part” of the object for detection and achieved higher accuracy than
    HOG. It follows the philosophy of divide and rule; parts of the object are individually
    detected during inference time and a probable arrangement of them is marked as
    detection. For example, a human body can be considered as a collection of parts
    like head, arms, legs and torso. One model will be assigned to capture one of
    the parts in the whole image and the process is repeated for all such parts. A
    model then removes improbable configurations of the combination of these parts
    to produce detection. DPM based models [felzenszwalb_object_2010, felzenszwalb_cascade_2010]
    were one of the most successful algorithms before the era of deep learning.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 可变形部件模型（DPM）[felzenszwalb_discriminatively_2008] 由Felzenszwalb等人提出，并在2009年赢得了Pascal
    VOC挑战赛。该模型利用对象的单个“部件”进行检测，准确性高于HOG。它遵循分而治之的理念；在推理时单独检测对象的各个部件，并标记它们的可能排列为检测结果。例如，人类身体可以看作是头部、手臂、腿部和躯干等部件的集合。一个模型会被分配来捕捉整个图像中的一个部件，这个过程对所有这样的部件重复进行。然后，模型会去除这些部件组合的可能性配置，以生成检测结果。DPM基础的模型
    [felzenszwalb_object_2010, felzenszwalb_cascade_2010] 是深度学习时代之前最成功的算法之一。
- en: V-B Two-Stage Detectors
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 两阶段检测器
- en: V-B1 R-CNN
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 R-CNN
- en: The Region-based Convolutional Neural Network (R-CNN) [girshick_rich_2014] was
    the first paper in the R-CNN family, and demonstrated how CNNs can be used to
    immensely improve the detection performance. R-CNN use a class agnostic region
    proposal module with CNNs to convert detection into classification and localization
    problem. A mean-subtracted input image is first passed through the region proposal
    module, which produces 2000 object candidates. This module find parts of the image
    which has a higher probability of finding an object using Selective Search [uijlings_selective_2013].
    These candidates are then warped and propagated through a CNN network, which extracts
    a 4096-dimension feature vector for each proposal. Girshick et al. used AlexNet
    [NIPS2012_c399862d] as the backbone architecture of the detector. The feature
    vectors are then passed to the trained, class-specific Support Vector Machines
    (SVMs) to obtain confidence scores. Non-maximum suppression (NMS) is later applied
    to the scored regions, based on its IoU and class. Once the class has been identified,
    the algorithm predicts its bounding box using a trained bounding-box regressor,
    which predicts four parameters i.e., center coordinates of box along with its
    width and height.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基于区域的卷积神经网络（R-CNN）[girshick_rich_2014] 是R-CNN系列中的首篇论文，展示了卷积神经网络（CNN）如何极大地提升检测性能。R-CNN
    使用与CNN结合的类无关区域提议模块，将检测问题转换为分类和定位问题。首先，将均值减去的输入图像传递通过区域提议模块，该模块生成2000个对象候选框。该模块使用选择性搜索
    [uijlings_selective_2013] 找到图像中具有较高物体发现概率的部分。这些候选框随后经过CNN网络的变形和传播，为每个提议提取一个4096维的特征向量。Girshick等人使用AlexNet
    [NIPS2012_c399862d] 作为检测器的骨干网络。特征向量随后传递给训练过的类特定支持向量机（SVM）以获得置信度分数。基于其IoU和类别，之后对评分区域应用非极大值抑制（NMS）。一旦识别出类别，算法使用训练好的边界框回归器预测其边界框，回归器预测四个参数，即框的中心坐标及其宽度和高度。
- en: R-CNN has a complicated multistage training process. The first stage is pre-training
    the CNN with a large classification dataset. It is then fine-tuned for detection
    using domain-specific images (mean-subtracted, warped proposals) by replacing
    of the classification layer with a randomly initialized N+1-way classifier, N
    being the number of classes, using stochastic gradient descent (SGD) [lecun_backpropagation_1989].
    One liner SVM and bounding box regressor is trained for each class.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN 具有复杂的多阶段训练过程。第一阶段是用大型分类数据集对 CNN 进行预训练。接着，通过用一个随机初始化的 N+1 类分类器替换分类层，并使用领域特定的图像（均值减去，扭曲的提案），对检测进行微调，其中
    N 为类别数量，使用随机梯度下降（SGD）[lecun_backpropagation_1989]。每个类别训练一个一行 SVM 和边界框回归器。
- en: R-CNN ushered a new wave in the field of object detection, but it was slow (47
    sec per image) and expensive in time and space [girshick_fast_2015]. It had complex
    training process and took days to train on small datasets even when some of the
    computations were shared.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN 带来了对象检测领域的新潮流，但其处理速度较慢（每张图像 47 秒）且在时间和空间上都很昂贵 [girshick_fast_2015]。其训练过程复杂，即使在一些计算共享的情况下，也需要几天的时间来训练小型数据集。
- en: V-B2 SPP-Net
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 SPP-Net
- en: He et al. proposed the use of Spatial Pyramid Pooling (SPP) layer [grauman_pyramid_2005]
    to process image of arbitrary size or aspect ratio. They realized that only the
    fully connected part of the CNN required a fixed input. SPP-net [he_spatial_2015]
    merely shifted the convolution layers of CNN before the region proposal module
    and added a pooling layer, thereby making the network independent of size/aspect
    ratio and reducing the computations. The selective search [uijlings_selective_2013]
    algorithm is used to generate candidate windows. Feature maps are obtained by
    passing the input image through the convolution layers of a ZF-5 [zeiler_visualizing_2014]
    network. The candidate windows are then mapped on to the feature maps, which are
    subsequently converted into fixed length representations by spatial bins of a
    pyramidal pooling layer. This vector is passed to the fully connected layer and
    ultimately, to SVM classifiers to predict class and score. Similar to R-CNN [girshick_rich_2014],
    SPP-net has as post processing layer to improve localization by bounding box regression.
    It also uses the same multistage training process, except that the fine tuning
    is done only on the fully connected layers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: He 等人提出了使用空间金字塔池化（SPP）层 [grauman_pyramid_2005] 来处理任意大小或纵横比的图像。他们意识到，只有 CNN 的全连接部分需要固定输入。SPP-net
    [he_spatial_2015] 仅将 CNN 的卷积层移到区域提议模块之前，并添加了一个池化层，从而使网络不受大小/纵横比的影响并减少计算量。选择性搜索
    [uijlings_selective_2013] 算法用于生成候选窗口。通过将输入图像传递通过 ZF-5 [zeiler_visualizing_2014]
    网络的卷积层来获得特征图。然后将候选窗口映射到特征图上，这些特征图随后通过金字塔池化层的空间区块转换为固定长度的表示。这个向量传递到全连接层，最终传递到 SVM
    分类器以预测类别和得分。类似于 R-CNN [girshick_rich_2014]，SPP-net 具有后处理层，通过边界框回归来改善定位。它也使用相同的多阶段训练过程，不同之处在于，仅对全连接层进行微调。
- en: SPP-Net is considerably faster than the R-CNN model with comparable accuracy.
    It can process images of any shape/aspect ratio and thus, avoid object deformation
    due to input warping. However, as its architecture is analogous to R-CNN, it shared
    R-CNN’s disadvantages too like multistage training, computationally expensive
    and training time as well.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: SPP-Net 在准确性相当的情况下，比 R-CNN 模型快得多。它可以处理任意形状/纵横比的图像，从而避免由于输入变形而导致的物体形变。然而，由于其架构类似于
    R-CNN，它也继承了 R-CNN 的缺点，如多阶段训练、计算开销大以及训练时间长等。
- en: V-B3 Fast R-CNN
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3 快速 R-CNN
- en: One of the major issues with R-CNN/SPP-Net was the need to train multiple systems
    separately. Fast R-CNN [girshick_fast_2015] solved this by creating a single end-to-end
    trainable system. The network takes as input an image and its object proposals.
    The image is passed through a set of convolution layers and the object proposals
    are mapped to the obtained feature maps. Girshick replaced pyramidal structure
    of pooling layers from SPP-net [he_spatial_2015] with a single spatial bin, called
    RoI pooling layer. This layer is connected to 2 fully connected layer and then
    branches out into a N+1-class SoftMax layer and a bounding box regressor layer,
    which has a fully connected layer as well. The model also changed the loss function
    of bounding box regressor from L2 to smooth L1 to better performance, while introducing
    a multi-task loss to train the network.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN/SPP-Net 的主要问题之一是需要分别训练多个系统。Fast R-CNN [girshick_fast_2015] 通过创建一个单一的端到端可训练系统解决了这个问题。网络以图像及其对象建议作为输入。图像通过一系列卷积层处理，对象建议则映射到获得的特征图上。Girshick
    用一个称为 RoI pooling 层的单一空间箱替代了 SPP-net [he_spatial_2015] 的金字塔结构的池化层。这个层连接到两个全连接层，然后分支成一个
    N+1 类 SoftMax 层和一个边界框回归层，后者也有一个全连接层。该模型还将边界框回归器的损失函数从 L2 改为平滑 L1，以提高性能，同时引入了多任务损失来训练网络。
- en: The authors used modified version of existing state-of-art pre-trained models
    like [NIPS2012_c399862d], [simonyan_very_2015] and [jia_caffe_2014] as backbone.
    The network was trained in a single step by stochastic gradient descent (SGD)
    and a mini-batch of 2 images. This helped the network converge faster as the back-propagation
    shared computations among the RoIs from the two images.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了修改版的现有最先进的预训练模型，如 [NIPS2012_c399862d]、[simonyan_very_2015] 和 [jia_caffe_2014]
    作为骨干。网络通过随机梯度下降（SGD）和一个包含 2 张图像的迷你批量进行了一步训练。这帮助网络更快地收敛，因为反向传播在两个图像的 RoIs 之间共享了计算。
- en: Fast R-CNN was introduced as an improvement in speed (146x on R-CNN) while the
    increase in accuracy was supplementary. It simplified training procedure, removed
    pyramidal pooling and introduces a new loss function. The object detector, without
    the region proposal network, reported near real time speed with considerable accuracy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN 作为一种在速度上（比 R-CNN 快 146 倍）的改进被引入，而准确性的提高是附加的。它简化了训练过程，移除了金字塔池化，并引入了新的损失函数。该对象检测器在没有区域提议网络的情况下，以接近实时的速度报告了相当的准确性。
- en: '![Refer to caption](img/f3a555dcb1949744a5af9446876199ec.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f3a555dcb1949744a5af9446876199ec.png)'
- en: 'Figure 8: Illustration of the internal architecture of different two stage
    object detectors⁴⁴4Features created using: https://poloclub.github.io/cnn-explainer/.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 不同两阶段对象检测器的内部架构示意图⁴⁴4特征由：https://poloclub.github.io/cnn-explainer/ 创建。'
- en: V-B4 Faster R-CNN
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B4 Faster R-CNN
- en: Even though Fast R-CNN inched closer to real time object detection, its region
    proposal generation was still an order of magnitude slower (2 sec per image compared
    to 0.2 sec per image). Ren et al. suggested a fully convoluted network [long_fully_2015]
    as a region proposal network (RPN) in [ren_faster_2015] that takes an arbitrary
    input image and outputs a set of candidate windows. Each such window has an associated
    objectness score which determines likelihood of an object. Unlike its predecessors
    like [felzenszwalb_object_2010, girshick_fast_2015, kaiming_resnet_2016] which
    used image pyramids to solve size variance of objects, RPN introduces Anchor boxes.
    It used multiple bounding boxes of different aspect ratios and regressed over
    them to localize object. The input image is first passed through the CNN to obtain
    a set of feature maps. These are forwarded to the RPN, which produces bounding
    boxes and their classification. Selected proposals are then mapped back to the
    feature maps obtained from previous CNN layer in RoI pooling layer, and ultimately
    fed to fully connected layer, which is sent to classifier and bounding box regressor.
    Faster R-CNN is essentially Fast R-CNN with RPN as region proposal module.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Fast R-CNN 更接近实时目标检测，但其区域提议生成仍然慢了一个数量级（每张图片 2 秒，而不是 0.2 秒）。Ren 等人建议了一种全卷积网络
    [long_fully_2015] 作为区域提议网络（RPN），在 [ren_faster_2015] 中使用，它接收任意输入图像并输出一组候选窗口。每个这样的窗口都有一个关联的物体评分，用于确定物体的可能性。与之前的
    [felzenszwalb_object_2010, girshick_fast_2015, kaiming_resnet_2016] 使用图像金字塔解决物体尺寸变异不同，RPN
    引入了 Anchor boxes。它使用了多种不同纵横比的边界框，并对其进行回归以定位物体。输入图像首先通过 CNN 以获得一组特征图。这些特征图被传递到
    RPN，RPN 产生边界框及其分类。选择的提议随后被映射回从之前 CNN 层获得的特征图中，在 RoI 池化层中，最终输入到全连接层，再送入分类器和边界框回归器。Faster
    R-CNN 本质上是带有 RPN 作为区域提议模块的 Fast R-CNN。
- en: Training of Faster R-CNN is more convoluted, due to the presence of shared layers
    between two models which perform very different tasks. Firstly, RPN is pre-trained
    on ImageNet dataset [deng_imagenet_2009] and fine-tuned on PASCAL VOC dataset
    [everingham_pascal_2010]. A Fast R-CNN is trained from the region proposals of
    RPN from first step. Till this point, the networks do not have shared convolution
    layer. Now, we fix the convolution layers of the detector and fine-tune the unique
    layers in RPN. And finally, Fast R-CNN is fine-tuned from the updated RPN.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN 的训练过程更为复杂，因为两个模型之间存在共享层，而它们执行的任务截然不同。首先，RPN 在 ImageNet 数据集 [deng_imagenet_2009]
    上进行预训练，并在 PASCAL VOC 数据集 [everingham_pascal_2010] 上进行微调。然后，从 RPN 提出的区域开始训练 Fast
    R-CNN。在这之前，网络没有共享卷积层。现在，我们固定检测器的卷积层，并微调 RPN 中的独特层。最后，Fast R-CNN 从更新后的 RPN 进行微调。
- en: Faster R-CNN improved the detection accuracy over the previous state-of-art
    [girshick_fast_2015] by more than 3% and decreased inference time by an order
    of magnitude. It fixed the bottleneck of slow region proposal and ran in near
    real time at 5 frames per second. Another advantage of having a CNN in region
    proposal was that it could learn to produce better proposals and thereby increase
    accuracy.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN 将检测精度提高了超过 3%，超越了之前的最先进技术 [girshick_fast_2015]，并将推理时间缩短了一个数量级。它解决了缓慢区域提议的瓶颈，并以每秒
    5 帧的接近实时速度运行。使用 CNN 进行区域提议的另一个优点是它可以学习生成更好的提议，从而提高准确性。
- en: V-B5 FPN
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B5 FPN
- en: Use of image pyramid to obtain feature pyramid (or featurized image pyramids)
    at multiple levels is a common method to increase detection of small objects.
    Even though it increases Average Precision of the detector, the increase in the
    inference time is substantial. Lin et al. proposed the Feature Pyramid Network
    (FPN) [lin2017feature], which has a top-down architecture with lateral connections
    to build high-level semantic features at different scales. The FPN has two pathways,
    a bottom-up pathway which is a ConvNet computing feature hierarchy at several
    scales and a top-down pathway which upsamples coarse feature maps from higher
    level into high-resolution features. These pathways are connected by lateral connection
    by a 1x1 convolution operation to enhance the semantic information in the features.
    FPN is used as a region proposal network (RPN) of a ResNet-101 [kaiming_resnet_2016]
    based Faster R-CNN here.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图像金字塔来获取多级特征金字塔（或特征化图像金字塔）是一种常见的方法，用于增加对小物体的检测。尽管它提高了检测器的平均精度，但推断时间的增加却是显著的。Lin
    等人提出了特征金字塔网络（FPN）[lin2017feature]，它具有自上而下的架构，并通过侧向连接来构建不同尺度的高级语义特征。FPN 具有两条路径，一条是自下而上的路径，它是一个在多个尺度上计算特征层次的
    ConvNet，另一条是自上而下的路径，它将来自高层的粗糙特征图上采样成高分辨率特征。这些路径通过 1x1 卷积操作通过侧向连接相连，以增强特征中的语义信息。在这里，FPN
    被用作基于 ResNet-101 [kaiming_resnet_2016] 的 Faster R-CNN 的区域提议网络（RPN）。
- en: FPN could provide high-level semantics at all scales, which reduced the error
    rate in detection. It became a standard building block in future detections models
    and improved accuracy their accuracy across the table. It also lead to development
    of other improved networks like PANet [liu_path_2018], NAS-FPN [ghiasi_nas-fpn_2019]
    and EfficientNet [tan_efficientnet_2020], which is current state of art detector.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: FPN 可以在所有尺度上提供高级语义，这减少了检测中的错误率。它成为了未来检测模型中的标准构建块，并提高了模型的准确性。它还导致了其他改进网络的开发，如
    PANet [liu_path_2018]、NAS-FPN [ghiasi_nas-fpn_2019] 和 EfficientNet [tan_efficientnet_2020]，这些都是当前的最先进检测器。
- en: V-B6 R-FCN
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B6 R-FCN
- en: Dai et al. proposed Region-based Fully Convolutional Network (R-FCN) [dai_r-fcn_2016]
    that shared almost all computations within the network, unlike previous two stage
    detectors which applied resource intensive techniques on each proposal. They argued
    against the use of fully connected layers and instead used convolutional layers.
    However, deeper layers in the convolutional network are translation-invariant,
    making them ineffective for localization tasks. The authors proposed the use of
    position-sensitive score maps to remedy it. These sensitive score maps encode
    relative spatial information of the subject and are later pooled to identify exact
    localization. R-FCN does it by dividing the region of interest into k x k grid
    and scoring the likeliness of each cell with the detection class feature map.
    These scores are later averaged and used to predict the object class. R-FCN detector
    is a combination of four convolutional networks. The input image is first passed
    through the ResNet-101[kaiming_resnet_2016] to get feature maps. An intermediate
    output (Conv4 layer) is passed to a Region Proposal Network (RPN) to identify
    RoI proposals while the final output is further processed through a convolutional
    layer and is input to classifier and regressor. The classification layer combines
    the generated the position-sensitive map with the RoI proposals to generate predictions
    while the regression network outputs the bounding box details. R-FCN is trained
    in a similar 4 step fashion as Faster-RCNN [ren_faster_2015] whilst using a combined
    cross-entropy and box regression loss. It also adopts online hard example mining
    (OHEM) [shrivastava_training_2016] during the training.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Dai 等人提出了基于区域的全卷积网络（R-FCN）[dai_r-fcn_2016]，该网络几乎共享了所有的计算，与之前的两阶段检测器不同，后者在每个提议上应用了资源密集型技术。他们反对使用全连接层，转而使用卷积层。然而，卷积网络中的深层对平移不变，因此在定位任务中效果不佳。作者提出使用位置敏感的得分图来解决这个问题。这些敏感得分图编码了对象的相对空间信息，并在之后进行池化以识别准确的定位。R-FCN
    通过将感兴趣区域划分为 k x k 网格，并与检测类别特征图评分每个单元的可能性来实现这一点。这些得分随后被平均，并用于预测对象类别。R-FCN 检测器是四个卷积网络的组合。输入图像首先通过
    ResNet-101[kaiming_resnet_2016] 生成特征图。一个中间输出（Conv4 层）传递给区域提议网络（RPN）以识别 RoI 提议，而最终输出则通过卷积层进一步处理，并输入分类器和回归器。分类层将生成的位置敏感图与
    RoI 提议结合起来生成预测，而回归网络则输出边界框细节。R-FCN 以类似 Faster-RCNN [ren_faster_2015] 的 4 步方式进行训练，同时使用了结合交叉熵和框回归损失的方法。在训练过程中，它还采用了在线困难样本挖掘（OHEM）[shrivastava_training_2016]。
- en: Dai et al. offered a novel method to solve the problem of translation invariance
    in convolutional neural networks. R-FCN combines Faster R-CNN and FCN to achieve
    a fast, more accurate detector. Even though it did not improve accuracy by much,
    but it was 2.5-20 times faster than its counterpart.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Dai 等人提出了一种新方法来解决卷积神经网络中的平移不变性问题。R-FCN 结合了 Faster R-CNN 和 FCN，实现了一个更快、更准确的检测器。尽管它没有显著提高准确性，但比其对应方法快了
    2.5 到 20 倍。
- en: V-B7 Mask R-CNN
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B7 Mask R-CNN
- en: Mask R-CNN [he2018mask] extends on the Faster R-CNN by adding another branch
    in parallel for pixel-level object instance segmentation. The branch is a fully
    connected network applied on RoIs to classify each pixel into segments with little
    overall computation cost. It uses similar basic Faster R-CNN architecture for
    object proposal, but adds a mask head parallel to classification and bounding
    box regressor head. One major difference was the use of RoIAlign layer, instead
    of RoIPool layer, to avoid pixel level misalignment due to spatial quantization.
    The authors chose the ResNeXt-101 [xie_aggregated_2017] as its backbone along
    with the feature Pyramid Network (FPN) for better accuracy and speed. The loss
    function of Faster R-CNN is updated with the mask loss and as in FPN, it uses
    5 anchor boxes with 3 aspect ratio. Overall training of Mask R-CNN is similar
    to faster R-CNN.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN [he2018mask] 在 Faster R-CNN 的基础上进行扩展，通过增加另一个并行分支来进行像素级对象实例分割。该分支是一个全连接网络，应用于
    RoI 上，以将每个像素分类到不同的分段中，总体计算成本较低。它使用与 Faster R-CNN 相似的基本架构进行对象提议，但在分类和边界框回归头旁边增加了一个掩码头。一个主要的区别是使用
    RoIAlign 层，而不是 RoIPool 层，以避免由于空间量化而导致的像素级对齐误差。作者选择了 ResNeXt-101 [xie_aggregated_2017]
    作为其骨干网络，并配合特征金字塔网络（FPN）以提高准确性和速度。Faster R-CNN 的损失函数被更新为掩码损失，并且与 FPN 一样，使用了 5 个锚框和
    3 种长宽比。整体上，Mask R-CNN 的训练与 Faster R-CNN 相似。
- en: Mask R-CNN performed better than the existing state of the art single-model
    architectures, added an extra functionality of instance segmentation with little
    overhead computations. It is simple to train, flexible and generalizes well in
    applications like keypoint detection, human pose estimation, etc. However, it
    was still below the real time performance ($>$30 fps).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN在现有的单模型架构中表现更好，增加了实例分割的额外功能，计算开销很小。它易于训练，灵活，并且在关键点检测、人类姿态估计等应用中具有很好的泛化能力。然而，它的实时性能仍低于$>$30
    fps。
- en: V-B8 DetectoRS
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B8 DetectoRS
- en: Many contemporary two stage detectors like [ren_faster_2015, chen_hybrid_2019,
    cai_cascade_2017] use the mechanism of looking and thinking twice i.e. calculating
    object proposals first and using them to extract features to detect objects. DetectoRS
    [qiao_detectors_2020] applies this mechanism at both macro and micro level of
    the network. At macro level, they propose Recursive Feature Pyramid (RFP), formed
    by stacking multiple feature pyramid network (FPN) with extra feedback connection
    from the top-down level path in FPN to the bottom-up layer. The output of the
    FPN is processed by the Atrous Spatial Pyramid Pooling layer (ASPP) [chen_deeplab_2017]
    before passing it to the next FPN layer. A Fusion module is used to combine FPN
    outputs from different modules by creating an attention map. At micro level, Qiao
    et al. presented the Switchable Atrous Convolution (SAC) to regulate the dilation
    rate of convolution. An average pooling layer with 5x5 filter and a 1x1 convolution
    is used as a switch function to decide the rate of atrous convolution [holschneider_real-time_1990],
    helping the backbone detect objects at various scale on the fly. They also packed
    the SAC in between two global context modules [hu_squeeze-and-excitation_2019]
    as it helps in making more stable switching. The combination of these two techniques,
    Recursive Feature Pyramid and Switchable Atrous Convolution results in DetectoRS.
    The authors incorporated the above techniques with the Hybrid Task Cascade (HTC)
    [chen_hybrid_2019] as the baseline model and a ResNext-101 backbone.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代的两阶段检测器如[ren_faster_2015, chen_hybrid_2019, cai_cascade_2017]使用了“看两次再想”的机制，即先计算目标提议，然后用它们提取特征以检测目标。DetectoRS
    [qiao_detectors_2020]在网络的宏观和微观层面上应用了这一机制。在宏观层面上，他们提出了递归特征金字塔（RFP），由多个特征金字塔网络（FPN）堆叠而成，并且FPN的自上而下路径中的顶部到底部层有额外的反馈连接。FPN的输出在传递到下一个FPN层之前经过了Atrous
    Spatial Pyramid Pooling层（ASPP）[chen_deeplab_2017]处理。一个融合模块用于通过创建注意力图来结合来自不同模块的FPN输出。在微观层面上，Qiao等人提出了可切换的扩张卷积（SAC）来调节卷积的膨胀率。使用带有5x5滤波器的平均池化层和1x1卷积作为开关函数，以决定扩张卷积[holschneider_real-time_1990]的膨胀率，帮助骨干网络实时检测各种尺度的目标。他们还将SAC打包在两个全局上下文模块[hu_squeeze-and-excitation_2019]之间，因为这有助于实现更稳定的切换。这两种技术，递归特征金字塔和可切换扩张卷积的结合形成了DetectoRS。作者将上述技术与Hybrid
    Task Cascade（HTC）[chen_hybrid_2019]作为基线模型，并使用了ResNext-101骨干网络。
- en: DetectoRS combined multiple systems to improve performance of the detector and
    sets the state-of-the-art for the two stage detectors. Its RFP and SAC modules
    are well generalized and can be used in other detection models. However, it is
    not suitable for real time detections as it can only process about 4 frames per
    second.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: DetectoRS结合了多个系统来提高检测器的性能，并为两阶段检测器设定了最新水平。其RFP和SAC模块具有良好的泛化能力，可以用于其他检测模型。然而，它不适合实时检测，因为它每秒只能处理约4帧。
- en: V-C Single Stage Detectors
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 单阶段检测器
- en: V-C1 YOLO
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C1 YOLO
- en: 'Two stage detectors solve the object detection as a classification problem,
    a module presents some candidates which the network classifies as either an object
    or background. However, YOLO or You Only Look Once [redmon_you_2016] reframed
    it as a regression problem, directly predicting the image pixels as objects and
    its bounding box attributes. In YOLO, the input image is divided into a S x S
    grid and the cell where the object’s center falls is responsible for detecting
    it. A grid cell predicts multiple bounding boxes, and each prediction array consists
    of 5 elements: center of bounding box – x and y, dimensions of the box – w and
    h, and the confidence score.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段检测器将目标检测视为一个分类问题，一个模块展示一些候选框，网络将其分类为目标或背景。然而，YOLO 或“你只看一次”[redmon_you_2016]将其重新定义为回归问题，直接预测图像像素作为目标及其边界框属性。在YOLO中，输入图像被划分为S
    x S网格，目标中心所在的单元负责检测目标。一个网格单元预测多个边界框，每个预测数组由5个元素组成：边界框中心的x和y，框的尺寸w和h，以及置信度分数。
- en: YOLO was inspired from the GoogLeNet model for image classification [szegedy_going_2014],
    which uses cascaded modules of smaller convolution networks [lin_network_2014].
    It is pre-trained on ImageNet data [deng_imagenet_2009] till the model achieves
    high accuracy and then modified by adding randomly initialized convolution and
    fully connected layers. At training time, grid cells predict only one class as
    it converges better, but it is be increased during the inference time. Multitask
    loss, combined loss of all predicted components, is used to optimize the model.
    Non maximum suppression (NMS) removes class-specific multiple detections.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO的灵感来源于用于图像分类的GoogLeNet模型 [szegedy_going_2014]，该模型使用了较小卷积网络的级联模块 [lin_network_2014]。它在ImageNet数据
    [deng_imagenet_2009] 上进行预训练，直到模型达到高准确率，然后通过添加随机初始化的卷积和全连接层进行修改。在训练时，网格单元仅预测一个类别，因为这样收敛效果更好，但在推理时可以增加。多任务损失，即所有预测组件的综合损失，用于优化模型。非最大抑制（NMS）去除特定类别的多重检测。
- en: YOLO surpassed its contemporary single stage real time models by a huge margin
    in both accuracy and speed. However, it had significant shortcomings as well.
    Localization accuracy for small or clustered objects and limitation to number
    of objects per cell were its major drawbacks. These issues were fixed in later
    versions of YOLO [redmon_yolo9000_2016, redmon_yolov3_2018, bochkovskiy_yolov4_2020].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO在准确度和速度上大幅超越了当代的单阶段实时模型。然而，它也存在显著的缺陷。小物体或聚集物体的定位准确性以及每个单元的物体数量限制是其主要缺点。这些问题在YOLO的后续版本中得到解决
    [redmon_yolo9000_2016, redmon_yolov3_2018, bochkovskiy_yolov4_2020]。
- en: V-C2 SSD
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C2 SSD
- en: Single Shot MultiBox Detector (SSD) [liu_ssd_2016] was the first single stage
    detector that matched accuracy of contemporary two stage detectors like Faster
    R-CNN [ren_faster_2015], while maintaining real time speed. SSD was built on VGG-16
    [simonyan_very_2015], with additional auxiliary structures to improve performance.
    These auxiliary convolution layers, added to the end of the model, decrease progressively
    in size. SSD detects smaller objects earlier in the network when the image features
    are not too crude, while the deeper layers were responsible for offset of the
    default boxes and aspect ratios [erhan_scalable_2014].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 单阶段多框检测器（SSD） [liu_ssd_2016] 是第一个在保持实时速度的同时达到与当代两阶段检测器如Faster R-CNN [ren_faster_2015]
    相同准确度的单阶段检测器。SSD构建在VGG-16 [simonyan_very_2015]上，并添加了额外的辅助结构以提高性能。这些附加的卷积层被添加到模型的末端，大小逐渐减小。SSD在网络的早期阶段检测到较小的物体，当图像特征还不太粗糙时，而深层则负责默认框的偏移和纵横比
    [erhan_scalable_2014]。
- en: During training, SSD match each ground truth box with the default boxes with
    the best jaccard overlap and train the network accordingly, similar to Multibox
    [erhan_scalable_2014]. They also used hard negative mining and heavy data augmentation.
    Similar to DPM [felzenszwalb_discriminatively_2008], it utilized weighted sum
    of the localization and confidence loss to train the model. Final output is obtained
    by performing non maximum suppression.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，SSD将每个真实框与具有最佳Jaccard重叠度的默认框匹配，并相应地训练网络，这与Multibox [erhan_scalable_2014]类似。他们还使用了困难负样本挖掘和大量的数据增强。类似于DPM
    [felzenszwalb_discriminatively_2008]，它利用定位和置信度损失的加权和来训练模型。最终输出通过执行非最大抑制来获得。
- en: Even though SSD was significantly faster and more accurate than both state-of-art
    networks like YOLO and Faster R-CNN, it had difficulty in detecting small objects.
    This issue was later solved by using better backbone architectures like ResNet
    and other small fixes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SSD在速度和准确度上显著优于YOLO和Faster R-CNN等最先进的网络，但它在检测小物体方面存在困难。这个问题后来通过使用更好的主干架构如ResNet和其他小修复得到解决。
- en: '![Refer to caption](img/e1c44d95f15ba2fa5865008f52ac407e.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e1c44d95f15ba2fa5865008f52ac407e.png)'
- en: 'Figure 9: Illustration of the internal architecture of different two and single
    stage object detectors²²footnotemark: 2.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 不同两阶段和单阶段目标检测器的内部架构示意图²²脚注标记: 2。'
- en: V-C3 YOLOv2 and YOLO9000
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C3 YOLOv2 和 YOLO9000
- en: YOLOv2 [redmon_yolo9000_2016], an improvement on the YOLO [redmon_you_2016],
    offered an easy tradeoff between speed and accuracy while the YOLO9000 model could
    predict 9000 object classes in real time. They replaced the backbone architecture
    of GoogLeNet [szegedy_going_2014] with DarkNet-19 [redmon_darknet_2016]. It incorporated
    many impressive techniques like Batch Normalization [he_delving_2015] to improve
    convergence, joint training of classification and detection systems to increase
    detection classes, removing fully connected layers to increase speed and using
    learnt anchor boxes to improve recall and have better priors. Redmon et al. also
    combined the classification and detection datasets in hierarchical structure using
    WordNet [miller_introduction_1991]. This WordTree can be used to predict a higher
    conditional probability of hypernym, even when the hyponym is not classified correctly,
    thereby increasing the overall performance of the system.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv2 [redmon_yolo9000_2016]，作为 YOLO [redmon_you_2016] 的改进，提供了速度和准确性之间的简单权衡，同时
    YOLO9000 模型可以实时预测 9000 个物体类别。他们用 DarkNet-19 [redmon_darknet_2016] 替换了 GoogLeNet
    [szegedy_going_2014] 的主干架构。它结合了许多令人印象深刻的技术，如批归一化 [he_delving_2015] 来提高收敛性、分类和检测系统的联合训练以增加检测类别、去除全连接层以提高速度，并使用学习到的锚框来提高召回率并有更好的先验。Redmon
    等人还结合了分类和检测数据集，使用 WordNet [miller_introduction_1991] 进行层次结构处理。这个 WordTree 可以用于预测更高条件概率的上位词，即使下位词没有正确分类，也能提高系统的整体性能。
- en: YOLOv2 provided better flexibility to choose the model on speed and accuracy,
    and the new architecture had fewer parameters. As the title of the paper suggests,
    it was “better, faster and stronger” [redmon_yolo9000_2016].
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv2 提供了更好的灵活性来选择速度和准确性上的模型，并且新架构的参数更少。正如论文标题所示，它“更好、更快、更强” [redmon_yolo9000_2016]。
- en: V-C4 RetinaNet
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C4 RetinaNet
- en: Given the difference between the accuracies of single and two stage detectors,
    Lin et al. suggested that the reason single stage detectors lag is the “extreme
    foreground-background class imbalance” [lin_focal_2017]. They proposed a reshaped
    cross entropy loss, called Focal loss as the means to remedy the imbalance. Focal
    loss parameter reduces the loss contribution from easy examples. The authors demonstrate
    its efficacy with the help of a simple, single stage detector, called RetinaNet
    [lin_focal_2017], which predicts objects by dense sampling of the input image
    in location, scale and aspect ratio. It uses ResNet [kaiming_resnet_2016] augmented
    by Feature Pyramid Network (FPN) [lin2017feature] as the backbone and two similar
    subnets - classification and bounding box regressor. Each layer from the FPN is
    passed to the subnets, enabling it to detect objects as various scales. The classification
    subnet predicts the object score for each location while the box regression subnet
    regresses the offset for each anchor to the ground truth. Both subnets are small
    FCN and share parameters across the individual networks. Unlike most previous
    works, the authors employ a class-agnostic bounding box regressor and found them
    to be equally effective.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于单阶段和两阶段检测器的准确性差异，Lin 等人提出单阶段检测器滞后的原因是“极端的前景-背景类别不平衡” [lin_focal_2017]。他们提出了一种重新调整的交叉熵损失，称为
    Focal 损失，以解决这种不平衡。Focal 损失参数减少了来自简单示例的损失贡献。作者通过一个简单的单阶段检测器 RetinaNet [lin_focal_2017]
    展示了其有效性，该检测器通过在位置、尺度和长宽比上对输入图像进行密集采样来预测物体。它使用由特征金字塔网络（FPN）[lin2017feature] 增强的
    ResNet [kaiming_resnet_2016] 作为主干网，并且有两个类似的子网络——分类和边界框回归器。每一层来自 FPN 的输出都传递给子网络，使其能够检测不同尺度的物体。分类子网络为每个位置预测物体分数，而边界框回归子网络将每个锚点的偏移回归到真实值。两个子网络都是小型全卷积网络（FCN），并在各自网络之间共享参数。与大多数先前的工作不同，作者使用了无类别的边界框回归器，并发现它们同样有效。
- en: RetinaNet is simple to train, converges faster and easy to implement. It achieved
    better performance in accuracy and run time than the two stage detectors. RetinaNet
    also pushed the envelope in advancing the ways object detectors are optimized
    by the introduction of a new loss function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet 训练简单，收敛速度快且易于实现。它在准确性和运行时间上比两阶段检测器表现更好。RetinaNet 还通过引入新的损失函数，推动了物体检测器优化方式的发展。
- en: V-C5 YOLOv3
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C5 YOLOv3
- en: YOLOv3 had “incremental improvements” from the previous YOLO versions [redmon_you_2016,
    redmon_yolo9000_2016]. Redmon et al. replaced the feature extractor network with
    a larger Darknet-53 network[redmon_darknet_2016]. They also incorporated various
    techniques like data augmentation, multi-scale training, batch normalization,
    among others. Softmax in classifier layer was replaced by a logistical classifier.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv3 相较于之前的 YOLO 版本 [redmon_you_2016, redmon_yolo9000_2016] 有了“逐步改进”。Redmon
    等人用更大的 Darknet-53 网络 [redmon_darknet_2016] 替换了特征提取网络。他们还融合了数据增强、多尺度训练、批量归一化等多种技术。分类器层中的
    Softmax 被替换为逻辑分类器。
- en: Even though YOLOv3 was faster than YOLOv2 [redmon_yolo9000_2016], it lacked
    any ground breaking change from its predecessor. It even had lesser accuracy than
    an year old state-of-the-art detector [lin_focal_2017].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 YOLOv3 比 YOLOv2 [redmon_yolo9000_2016] 更快，但它并没有带来颠覆性的变化，甚至比一年前的最先进检测器 [lin_focal_2017]
    精度还低。
- en: V-C6 CenterNet
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C6 CenterNet
- en: Zhou et al. in [zhou_objects_2019] takes a very different approach of modelling
    objects as points, instead of the conventional bounding box representation. CenterNet
    predicts the object as a single point at the center of the bounding box. The input
    image is passed through the FCN that generates a heatmap, whose peaks correspond
    to center of detected object. It uses a ImageNet pretrained stacked Hourglass-101
    [newell_stacked_2016] as the feature extractor network and has 3 heads – heatmap
    head to determine the object center, dimension head to estimate size of object
    and offset head to correct offset of object point. Multitask loss of all three
    heads is back propagated to feature extractor while training. During inference,
    the output from offset head is used to determine the object point and finally
    a box is generated. As the predictions, not the result, are points and not bounding
    boxes, non-maximum suppression (NMS) is not required for post-processing.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou 等人在 [zhou_objects_2019] 中采取了非常不同的建模方式，将物体建模为点，而不是传统的边界框表示。CenterNet 将物体预测为边界框中心的一个点。输入图像经过
    FCN 生成热图，其峰值对应检测物体的中心。它使用 ImageNet 预训练的堆叠 Hourglass-101 [newell_stacked_2016]
    作为特征提取网络，并具有三个头部——热图头用于确定物体中心，尺寸头用于估计物体大小，偏移头用于修正物体点的偏移。三个头部的多任务损失在训练过程中被反向传播到特征提取网络。在推理时，使用偏移头的输出确定物体点，最终生成一个框。由于预测的是点而不是边界框，因此后处理不需要非极大值抑制
    (NMS)。
- en: CenterNet brings a fresh perspective and set aside years of progress in the
    field of object detection. It is more accurate and has lesser inference time than
    its predecessors. It has high precision for multiple tasks like 3D object detection,
    keypoint estimation, pose, instance segmentation, orientation detection and others.
    However, it requires different backbone architectures as general architectures
    that work well with other detectors give poor performance with it and vice-versa.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: CenterNet 带来了新的视角，抛弃了多年的物体检测领域进展。它比其前身更准确，推理时间更短。它在 3D 物体检测、关键点估计、姿态、实例分割、方向检测等多个任务中具有高精度。然而，它需要不同的骨干网络，因为与其他检测器兼容的通用网络在这里表现较差，反之亦然。
- en: V-C7 EfficientDet
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C7 EfficientDet
- en: EfficientDet [tan_efficientdet_2020] builds towards the idea of scalable detector
    with higher accuracy and efficiency. It introduces efficient multi-scale features,
    BiFPN and model scaling. BiFPN is bi-directional feature pyramid network with
    learnable weights for cross connection of input features at different scales.
    It improves on NAS-FPN [ghiasi_nas-fpn_2019], which required heavy training and
    had complex network, by removing one-input nodes and adding an extra lateral connection.
    This eliminates less efficient nodes and enhances high-level feature fusion. Unlike
    existing detectors which scale up with bigger, deeper backbone or stacking FPN
    layers, EfficientDet introduces a compounding coefficient which can be used to
    “jointly scale up all dimensions of backbone network, BiFPN network, class/box
    network and resolution” [tan_efficientdet_2020]. EfficientDet utilizes EfficientNet
    [tan_efficientnet_2020] as the backbone network with multiple sets of BiFPN layers
    stacked in series as feature extraction network. Each output from the final BiFPN
    layer is sent to class and box prediction network. The model is trained using
    SGD optimizer along with synchronized batch normalization and uses swish activation
    [ramachandran_searching_2017], instead of the standard ReLU activation, which
    is differentiable, more efficient and has better performance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: EfficientDet [tan_efficientdet_2020] 构建了一个具有更高准确性和效率的可扩展检测器。它引入了高效的多尺度特征、BiFPN
    和模型缩放。BiFPN 是一个双向特征金字塔网络，具有可学习的权重，用于不同尺度输入特征的交叉连接。它改进了 NAS-FPN [ghiasi_nas-fpn_2019]，后者需要大量训练且网络复杂，通过去除单输入节点和添加额外的侧向连接来优化。这消除了效率较低的节点，并增强了高层特征融合。与现有检测器通过增加更大的、更深的主干或堆叠
    FPN 层来扩大规模不同，EfficientDet 引入了一个复合系数，可以用来“联合缩放主干网络、BiFPN 网络、类别/框网络和分辨率” [tan_efficientdet_2020]。EfficientDet
    采用 EfficientNet [tan_efficientnet_2020] 作为主干网络，并将多组 BiFPN 层串联作为特征提取网络。来自最终 BiFPN
    层的每个输出都发送到类别和框预测网络。该模型使用 SGD 优化器以及同步批量归一化进行训练，并使用 swish 激活 [ramachandran_searching_2017]，而不是标准的
    ReLU 激活，这种激活函数可微分、效率更高、性能更佳。
- en: EfficientDet achieves better efficiency and accuracy than previous detectors
    while being smaller and computationally cheaper. It is easy to scale, generalizes
    well for other tasks and is the current state-of-the-art model for single-stage
    object detection.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: EfficientDet 在保持较小和计算成本较低的同时，比之前的检测器实现了更好的效率和准确性。它易于扩展，对其他任务具有良好的泛化能力，是当前单阶段目标检测的**最先进**模型。
- en: V-C8 YOLOv4
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C8 YOLOv4
- en: YOLOv4 [bochkovskiy_yolov4_2020] incorporated a lot of exciting ideas to design
    a fast and easy to train object detector that could work in existing production
    systems. It utilizes “bag of freebies” i.e., methods that only increase training
    time and do not affect the inference time. YOLOv4 utilizes data augmentation techniques,
    regularization methods, class label smoothing, CIoU-loss [zheng_distance-iou_2019],
    Cross mini-Batch Normalization (CmBN) , Self-adversarial training, Cosine annealing
    scheduler [loshchilov_sgdr_2017] and other tricks to improve training. Methods
    that only affect the inference time, called “Bag of Specials”, are also added
    to the network, including Mish activation [misra_mish_2020], Cross-stage partial
    connections (CSP) [wang_cspnet_2019], SPP-Block [he_spatial_2015], PAN path aggregated
    block [liu_path_2018] , Multi input weighted residual connections (MiWRC), etc.
    It also used genetic algorithm for searching hyper-parameter. It has an ImageNet
    pre-trained CSPNetDarknet-53 backbone, SPP and PAN block neck and YOLOv3 as detection
    head.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv4 [bochkovskiy_yolov4_2020] 融入了许多令人兴奋的想法，设计了一种快速且易于训练的目标检测器，可以在现有的生产系统中运行。它利用了“免费赠品”，即只增加训练时间而不影响推理时间的方法。YOLOv4
    采用了数据增强技术、正则化方法、类别标签平滑、CIoU-loss [zheng_distance-iou_2019]、Cross mini-Batch Normalization
    (CmBN)、自对抗训练、Cosine annealing scheduler [loshchilov_sgdr_2017] 和其他技巧来改进训练。也添加了只影响推理时间的方法，称为“特别赠品”，包括
    Mish 激活 [misra_mish_2020]、Cross-stage partial connections (CSP) [wang_cspnet_2019]、SPP-Block
    [he_spatial_2015]、PAN path aggregated block [liu_path_2018]、多输入加权残差连接 (MiWRC)
    等。它还使用了遗传算法来搜索超参数。它具有一个在 ImageNet 上预训练的 CSPNetDarknet-53 主干、SPP 和 PAN block neck，以及
    YOLOv3 作为检测头。
- en: Most existing detection algorithms require multiple GPUs to train model, but
    YOLOv4 can be easily trained on a single GPU. It is twice as fast as EfficientDet
    with comparable performance. It is the state-of-the-art for real time single stage
    detectors.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数检测算法需要多个 GPU 来训练模型，但 YOLOv4 可以轻松地在单个 GPU 上进行训练。它的速度是 EfficientDet 的两倍，同时性能相当。它是实时单阶段检测器的**最先进**技术。
- en: V-C9 Swin Transformer
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C9 Swin Transformer
- en: Transformers [vaswani2017attention] have had a profound impact in the Natural
    Language Processing (NLP) domain since its inception. Its application in language
    models like BERT (Bidirectional Encoder Representation from Transformers) [devlin2019bert],
    GPT (Generative Pre-trained Transformer) [radford2018improving], T5 (Text-To-Text
    Transfer Transformer) [2020t5] etc. have pushed the state of the art in the field.
    Transformers [vaswani2017attention] uses the attention model to establish dependencies
    among the elements of the sequence and can a attend to longer context than other
    sequential architectures. The success of transformers in NLP sparked interest
    in its application in computer vision. While CNNs have been the backbone on advancement
    in vision, they have some inherent shortcomings like the lack of importance of
    global context, fixed post-training weights [khan2021transformers] etc.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers [vaswani2017attention] 自问世以来对自然语言处理（NLP）领域产生了深远的影响。在语言模型中，如 BERT（双向编码器表示
    Transformer） [devlin2019bert]、GPT（生成预训练 Transformer） [radford2018improving]、T5（文本到文本转换
    Transformer） [2020t5] 等应用，推动了该领域的技术进步。Transformers [vaswani2017attention] 使用注意力模型来建立序列元素之间的依赖关系，并能处理比其他顺序架构更长的上下文。Transformers
    在 NLP 领域的成功引发了对其在计算机视觉中应用的兴趣。尽管 CNN 一直是视觉领域进展的骨干，但它们也存在一些固有的缺点，比如对全局上下文的重要性不足、固定的后训练权重
    [khan2021transformers] 等。
- en: Swin Transformer [liu2021swin] seeks to provide a transformer based backbone
    for computer vision tasks. It splits the input images in multiple, non-overlapping
    patches and converts them into embeddings. Numerous Swin Transformer blocks are
    then applied to the patches in 4 stages, with each successive stage reducing the
    number of patches to maintain hierarchical representation. The Swin Transformer
    block is composed of local multi-headed self-attention (MSA) modules, based on
    alternating shifted patch window in successive blocks. Computation complexity
    becomes linear with image size in local self-attention while shifted window enables
    cross-window connection. [liu2021swin] also shows how shifted windows increase
    detection accuracy with little overhead.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Swin Transformer [liu2021swin] 旨在为计算机视觉任务提供一个基于 Transformer 的骨干网。它将输入图像分割成多个不重叠的块，并将这些块转换成嵌入。然后，在四个阶段中对这些块应用大量的
    Swin Transformer 模块，每个后续阶段减少块的数量，以保持层次表示。Swin Transformer 模块由局部多头自注意力（MSA）模块组成，基于交替的移动块窗口。在局部自注意力中，计算复杂度随着图像大小线性增长，而移动窗口使得跨窗口连接成为可能。[liu2021swin]
    还展示了移动窗口如何以较小的开销提高检测准确率。
- en: Transformers present a paradigm shift from the CNN based neural networks. While
    its application in vision is still in a nascent stage, its potential to replace
    convolution from these tasks is very real. Swin Transformer achieved the state-of-the-art
    on MS COCO dataset, but utilises comparatively higher parameters than convolutional
    models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 展示了一种从基于 CNN 的神经网络到新范式的转变。虽然其在视觉领域的应用仍处于初期阶段，但它替代卷积的潜力非常真实。Swin
    Transformer 在 MS COCO 数据集上达到了最先进的水平，但相比卷积模型使用了更高的参数量。
- en: VI Lightweight Networks
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 轻量级网络
- en: A new branch of research has shaped up in recent years, aimed at designing small
    and efficient networks for resource constrained environments as is common in Internet
    of Things (IoT) deployments [abbas2021lightweight, karakanis2021lightweight, jadon2020low,
    jadon2019firenet]. This trend has percolated to the design of potent object detectors
    too. It is seen that although a large number of object detectors achieve excellent
    accuracy and perform inference in real-time, a majority of these models require
    excessive computing resources and therefore cannot be deployed on edge devices.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，出现了一个新的研究方向，旨在为资源受限的环境设计小型高效的网络，这在物联网（IoT）部署中很常见 [abbas2021lightweight,
    karakanis2021lightweight, jadon2020low, jadon2019firenet]。这一趋势也影响到了高效物体检测器的设计。尽管大量物体检测器在准确性上表现出色并能实时推断，但这些模型中的大多数需要过多的计算资源，因此无法在边缘设备上部署。
- en: Many different approaches have shown exciting results in the past. Utilization
    of efficient components and compression techniques like pruning ([lecun_optimal_1990,
    hassibi_advances_1993]), quantization ([han_deep_2016, courbariaux_binaryconnect_2016]),
    hashing [chen_compressing_2015], etc. have improved the efficiency of deep learning
    models. Use of trained large network to train smaller models, called distillation
    [hinton_distilling_2015], has also shown interesting results. However in this
    section, we explore some prominent examples of efficient neural network design
    for achieving high performance on edge devices.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，许多不同的方法已显示出令人兴奋的结果。利用高效组件和压缩技术，如剪枝（[lecun_optimal_1990, hassibi_advances_1993]）、量化（[han_deep_2016,
    courbariaux_binaryconnect_2016]）、哈希（[chen_compressing_2015]）等，已经提高了深度学习模型的效率。使用训练好的大型网络来训练较小模型，称为蒸馏（[hinton_distilling_2015]），也显示了有趣的结果。然而，在本节中，我们将深入探讨一些高效神经网络设计的突出示例，以在边缘设备上实现高性能。
- en: VI-A SqueezeNet
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A SqueezeNet
- en: 'Recent advances in the field of CNNs had mostly focused on improving the state-of-the-art
    accuracy on the benchmark datasets, which led to an explosion of model size and
    their parameters. But in 2016, Iandola et al. proposed a smaller, smarter network
    called SqueezeNet [iandola_squeezenet_2016], which reduced the parameters while
    maintaining the performance. They achieved it by employing three main design strategies
    viz. using smaller filters, decreasing the number of input channels to 3x3 filters
    and placing downsampling layers later in the network. The first two strategies
    decrease the number of parameters while attempting to preserve the accuracy and
    the third strategy increases the accuracy of the network. The building block of
    SqueezeNet is called a fire module, which consist of two layers: a squeeze layer
    and an expand layer, each with a ReLU activation. The squeeze layer is made up
    of multiple 1x1 filters while the expand layer is a mix of 1x1 and 3x3 filters,
    thereby limiting the number of input channels. The SqueezeNet architecture is
    composed of a stack of 8 Fire modules squashed in between the convolution layers.
    Inspired by ResNet [kaiming_resnet_2016], SqueezeNet with residual connections
    was also proposed which increased the accuracy over the vanilla model. The authors
    also experimented with Deep Compression [han_deep_2016] and achieved 510$\times$
    reduction in model size compared to AlexNet, while maintaining the baseline accuracy.
    SqueezeNet presented a good candidate for improving the hardware efficiency of
    the neural network architectures.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在卷积神经网络（CNN）领域的进展大多集中在提高基准数据集上的最先进精度，这导致了模型大小和参数的激增。但在2016年，Iandola等人提出了一种更小、更智能的网络，称为SqueezeNet（[iandola_squeezenet_2016]），它在保持性能的同时减少了参数。他们通过采用三种主要设计策略来实现这一点，即使用更小的滤波器，将输入通道数减少到3x3滤波器，并将下采样层放置在网络的后面。前两种策略减少了参数数量，同时试图保持精度，而第三种策略则提高了网络的精度。SqueezeNet的基本模块称为火模块，包含两个层次：一个挤压层和一个扩展层，每个层都具有ReLU激活。挤压层由多个1x1滤波器组成，而扩展层则是1x1和3x3滤波器的混合，从而限制了输入通道的数量。SqueezeNet架构由8个火模块堆叠在卷积层之间组成。受到ResNet（[kaiming_resnet_2016]）的启发，还提出了带有残差连接的SqueezeNet，这提高了相对于基础模型的精度。作者还实验了深度压缩（[han_deep_2016]），在保持基线精度的同时，实现了与AlexNet相比的510$\times$模型大小减少。SqueezeNet成为提升神经网络架构硬件效率的一个不错候选。
- en: VI-B MobileNets
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B MobileNets
- en: 'MobileNet [howard_mobilenets_2017] moved away from the conventional methods
    of small models like shrinking, pruning, quantization or compressing, and instead
    used efficient network architecture. The network used depthwise separable convolution,
    which factorizes a standard convolution into a depthwise convolution and a 1x1
    pointwise convolution. A standard convolution uses kernels on all input channels
    and combines them in one step while the depthwise convolution uses different kernels
    for each input channel and uses pointwise convolution to combine inputs. This
    separation of filtering and combining of features reduces the computation cost
    and model size. MobileNet consists of 28 separate convolutional layers, each followed
    by batch normalization and ReLU activation function. Howard et al. also introduced
    the two model shrinking hyperparameters: width and resolution multiplier, in order
    to further improve speed and reduce size of the model. The width multiplier manipulates
    the width of the network uniformly by reducing the input and output channels while
    the resolution multiplier influences the size of the input image and its representations
    throughout the network. MobileNet achieves comparable accuracy to some full-fledged
    models while being a fraction of their size. Howard et al. also showed how it
    could generalize over various applications like face attribution, geolocalization
    and object detection. However, it was too simple and linear like VGG and therefore
    had fewer avenues for gradient flow. These were fixed in later iterations of this
    model [sandler_mobilenetv2_2019, howard_searching_2019].'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: VI-C ShuffleNet
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In 2017, Zhang et al. introduced ShuffleNet [zhang_shufflenet_2018], an extremely
    computationally efficient neural network architecture, specifically designed for
    mobile devices. They recognized that many efficient networks become less effective
    as they scale down and purported it to be caused by expensive 1x1 convolutions.
    In conjunction with channel shuffle, they proposed the use of group convolution
    to circumvent its drawback of limited information flow. ShuffleNet consists mainly
    of a standard convolution followed by stacks of ShuffleNet units grouped in three
    stages. The ShuffleNet unit is similar to the ResNet block where they use depthwise
    convolution in the 3x3 layer and replace the 1x1 layer with pointwise group convolution.
    The depthwise convolution layer is preceded by a channel shuffle operation. The
    computation cost of the ShuffleNet can be administered by two hyperparameters:
    group number to control the connection sparsity and scaling factor to manipulate
    the model size. As group numbers become large, the error rate saturates as the
    input channels to each group decreases and therefore may reduce the representational
    capabilities. ShuffleNet outperformed contemporary models ([NIPS2012_c399862d,
    szegedy_going_2014, iandola_squeezenet_2016, howard_mobilenets_2017]) while having
    considerably smaller size. As the only advancement in ShuffleNet was channel shuffle,
    there isn’t any improvement in inference speed of the model.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，张等人引入了ShuffleNet [zhang_shufflenet_2018]，这是一种极其计算高效的神经网络架构，专为移动设备设计。他们认识到许多高效网络在缩小规模时效果会减弱，并认为这是由于昂贵的1x1卷积所致。为了克服信息流受限的缺点，他们提出结合通道洗牌和使用组卷积。ShuffleNet主要由标准卷积组成，后面跟随三个阶段分组的ShuffleNet单元。ShuffleNet单元类似于ResNet块，其中在3x3层使用深度卷积，并用点卷积组卷积替代1x1层。深度卷积层之前进行了通道洗牌操作。ShuffleNet的计算成本由两个超参数管理：组数用于控制连接稀疏性，缩放因子用于调整模型大小。随着组数增大，错误率趋于饱和，因为每组的输入通道减少，从而可能降低表示能力。ShuffleNet在体积明显较小的情况下超越了同时期的模型（[NIPS2012_c399862d,
    szegedy_going_2014, iandola_squeezenet_2016, howard_mobilenets_2017]）。由于ShuffleNet唯一的改进是通道洗牌，因此模型的推理速度没有提高。
- en: VI-D MobileNetv2
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D MobileNetv2
- en: Improving on MobileNetv1 [howard_mobilenets_2017], Sandler et al. proposed MobileNetv2
    [sandler_mobilenetv2_2019] in 2018\. It introduced the inverted residual with
    linear bottleneck, a novel layer module to reduce computation and improve accuracy.
    The module expands a low-dimensional representation of the input into high dimension,
    filters with a depthwise convolution and then projects it back to low dimension,
    unlike the common residual block which performs compression, convolution and then
    expansion operations. The MobileNetv2 contains a convolution layer followed by
    19 residual bottleneck modules and subsequently two convolutional layers. The
    residual bottleneck module has a shortcut connection only when the stride is 1\.
    For higher stride, the shortcut is not used because of the difference in dimensions.
    They also employed ReLU6 as the non-linearity function, instead of simple ReLU,
    to limit computations. For object detection, the authors used MobileNetv2 as the
    feature extractor of a computationally efficient variant of the SSD [liu_ssd_2016].
    This model, called SSDLite, claimed to have 8x fewer parameters than the original
    SSD while achieving competitive accuracy. It generalizes well over on other datasets,
    is easy to implement and hence, was well-received by the community.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 改进了MobileNetv1 [howard_mobilenets_2017]，Sandler等人于2018年提出了MobileNetv2 [sandler_mobilenetv2_2019]。它引入了倒置残差与线性瓶颈，这是一种新颖的层模块，用于减少计算并提高准确性。该模块将输入的低维表示扩展到高维，使用深度卷积进行过滤，然后再将其投影回低维，与常见的残差块不同，后者执行压缩、卷积然后扩展操作。MobileNetv2包含一个卷积层，随后是19个残差瓶颈模块，然后是两个卷积层。残差瓶颈模块仅在步幅为1时具有捷径连接。对于更高的步幅，由于尺寸差异，不使用捷径。他们还采用了ReLU6作为非线性函数，而不是简单的ReLU，以限制计算。对于目标检测，作者将MobileNetv2用作SSD
    [liu_ssd_2016]的计算高效变体的特征提取器。这个模型称为SSDLite，声称相比于原始SSD具有8倍更少的参数，同时实现了竞争力的准确性。它在其他数据集上表现良好，易于实现，因此受到社区的广泛欢迎。
- en: VI-E PeleeNet
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E PeleeNet
- en: Existing lightweight deep learning models like [howard_mobilenets_2017, zhang_shufflenet_2018,
    sandler_mobilenetv2_2019] relied heavily on depthwise separable convolution, which
    lacked efficient implementation. Wang et al. proposed a novel efficient architecture
    based on conventional convolution, named PeleeNet [wang_pelee_2018], using an
    assortment of computation conserving techniques. PeleeNet was centered around
    the DenseNet [huang_densely_2018] but looked at many other models for inspiration.
    It introduced two-way dense layers, stem block, dynamic number of channels in
    a bottleneck, transition layer compression and conventional post activation to
    reduce computation cost and increase speed. Inspired from [szegedy_going_2014],
    the two-way dense layer helps in getting different scales of the receptive field,
    making it easier to identify larger objects. To reduce information loss, a stem
    block was used in the same way to [szegedy_inception-v4_2016, shen_dsod_2018].
    They also parted way with the compression factor used in [huang_densely_2018]
    as it hurts the feature expression and reduces accuracy. PeleeNet consists of
    a stem block, four stages of modified dense and transition layers, and ultimately
    the classification layer. The authors also proposed a real-time object detection
    system, called Pelee, which was based on PeleeNet and a variant of SSD [liu_ssd_2016].
    Its performance against the contemporary object detectors on mobile and edge devices
    was incremental but showed how simple design choices can make a huge difference
    in overall performance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的轻量级深度学习模型如[howard_mobilenets_2017, zhang_shufflenet_2018, sandler_mobilenetv2_2019]严重依赖深度可分离卷积，但这种方法的实现效率不高。Wang等人提出了一种基于传统卷积的新型高效架构，名为PeleeNet
    [wang_pelee_2018]，使用了一系列节省计算资源的技术。PeleeNet以DenseNet [huang_densely_2018]为核心，同时参考了许多其他模型。它引入了双向密集层、stem块、瓶颈中动态通道数、过渡层压缩以及传统的后激活，以减少计算成本并提高速度。受到[szegedy_going_2014]的启发，双向密集层有助于获得不同尺度的感受野，使得识别较大物体变得更容易。为了减少信息损失，stem块的使用方式类似于[szegedy_inception-v4_2016,
    shen_dsod_2018]。他们还与[huang_densely_2018]中使用的压缩因子不同，因为它会影响特征表达并降低准确性。PeleeNet由一个stem块、四个阶段的修改过的密集和过渡层，最终是分类层组成。作者还提出了一个实时目标检测系统，称为Pelee，该系统基于PeleeNet并且是SSD
    [liu_ssd_2016]的变体。它在移动和边缘设备上的表现相较于当代目标检测器有了进步，但展示了简单的设计选择如何在整体性能上产生巨大差异。
- en: VI-F ShuffleNetv2
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-F ShuffleNetv2
- en: In 2018, Ningning Ma et al. present a set of comprehensive guidelines for designing
    efficient network architectures in ShuffleNetv2 [ma_shufflenet_2018]. They argued
    for the use of direct metrics like speed or latency to measure computational complexity,
    instead of indirect metrics like FLOPs. ShuffleNetv2 is built on four guiding
    principles – 1) equal width for input and output channels to minimize memory access
    cost, 2) carefully choosing group convolution based on the target platform and
    task, 3) multi-path structures achieve higher accuracy at the cost of efficiency
    and 4) element-wise operations like add and ReLU are computationally non-negligible.
    Following the above principles, they designed a new building block. It split the
    input into two parts by a channel split layer, followed by three convolutional
    layers which are then concatenated with the residual connection and passed through
    a channel shuffle layer. For the downsampling model, channel split is removed
    and residual connection has depthwise separable convolution layers. An ensemble
    of these blocks slotted in between a couple of convolutional layers results in
    ShuffleNetv2\. The authors also experimented with larger models (50/162 layers)
    and obtained superior accuracy with considerably fewer FLOPs. ShuffleNetv2 punched
    above its weight and outperformed other state-of-the-art models at comparable
    complexity.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，Ningning Ma等人提出了一套设计高效网络架构的全面指南，在ShuffleNetv2 [ma_shufflenet_2018]中。他们主张使用直接指标如速度或延迟来衡量计算复杂度，而不是使用间接指标如FLOPs。ShuffleNetv2基于四个指导原则：1)
    输入和输出通道宽度相等，以最小化内存访问成本；2) 根据目标平台和任务精心选择组卷积；3) 多路径结构以更高的准确性换取效率；4) 元素级操作如加法和ReLU在计算上不可忽视。遵循上述原则，他们设计了一种新的构建块。它通过通道分割层将输入分为两部分，然后经过三层卷积层，再与残差连接拼接，并通过通道混洗层传递。对于下采样模型，通道分割被移除，残差连接使用深度可分离卷积层。将这些块组合在几个卷积层之间，即得到ShuffleNetv2。作者还尝试了更大的模型（50/162层），并获得了更高的准确性，且FLOPs大大减少。ShuffleNetv2超越了其重量级，并在相似复杂度下超越了其他最先进的模型。
- en: VI-G MnasNet
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-G MnasNet
- en: With the increasing need for accurate, fast and low latency models for various
    edge devices, designing such a neural network is becoming more challenging than
    ever. In 2018, Tan et al. proposed Mnasnet [tan_mnasnet_2018] designed from an
    automated neural architecture search (NAS) approach. They formulate the search
    problem as multi-object optimization aimed at both high accuracy and low latency.
    It also factorized the search space by partitioning the CNN into unique blocks
    and subsequently searching for operations and connections in those blocks separately,
    thereby reducing the search space. This also allowed each block to have a distinctive
    design, unlike the earlier models [zoph_neural_2016, liu_progressive_2017, real_regularized_2019]
    which stacked the same blocks. The authors used RNN-based reinforcement learning
    agent as controller along with a trainer to measure accuracy and mobile devices
    for latency. Each sampled model is trained on a task to get its accuracy and run
    on the real devices for latency. This is used to achieve a soft reward target
    and the controller is updated. The process is repeated until the maximum iterations
    or a suitable candidate is derived. It is composed of 16 diverse blocks, some
    with residual connections. MnasNet was almost twice as fast as MobileNetv2 while
    having higher accuracy. However, like other reinforcement learning based neural
    architecture search models, the search time of MnasNet requires astronomical computational
    resources.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对各种边缘设备上精确、快速和低延迟模型需求的增加，设计这样的神经网络变得比以往任何时候都更加具有挑战性。2018年，Tan等人提出了Mnasnet
    [tan_mnasnet_2018]，它是基于自动化神经网络架构搜索（NAS）方法设计的。他们将搜索问题公式化为多目标优化，旨在同时达到高精度和低延迟。它还通过将CNN分解为独特的块，并随后在这些块中分别搜索操作和连接，从而减少了搜索空间。这也使得每个块具有独特的设计，与早期模型
    [zoph_neural_2016, liu_progressive_2017, real_regularized_2019] 不同，后者堆叠了相同的块。作者使用基于RNN的强化学习代理作为控制器，同时使用训练器来测量精度，并使用移动设备来测试延迟。每个采样模型都在任务上进行训练以获得其精度，并在真实设备上运行以测量延迟。这用于实现软奖励目标，控制器会进行更新。这个过程会重复，直到达到最大迭代次数或得出合适的候选模型。它由16个多样化的块组成，其中一些带有残差连接。MnasNet的速度几乎是MobileNetv2的两倍，同时具有更高的精度。然而，与其他基于强化学习的神经网络架构搜索模型一样，MnasNet的搜索时间需要巨大的计算资源。
- en: VI-H MobileNetv3
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-H MobileNetv3
- en: At the heart of MobileNetv3 [howard_searching_2019] is the same method used
    to create MnasNet [tan_mnasnet_2018] with some modifications. A platform aware
    automated neural architecture search is performed in a factorized hierarchical
    search space and consequently optimized by NetAdapt [yang_netadapt_2018], which
    removes the underutilized components of the network in multiple iterations. Once
    an architecture proposal is obtained, it trims the channels, randomly initialize
    the weights and then fine-tunes it to improve the target metrics. The model was
    further modified to remove some expensive layer in the architecture and gain additional
    latency improvement. Howard et al. argued that the filters in the architecture
    are often mirrored images of each other, and that accuracy can be maintained even
    after dropping half of these filters. Using this technique reduced the computations.
    MobileNetv3 used a blend of ReLU and hard swish as activation filters, the latter
    is mostly employed towards the end of the model. Hard swish has no noticeable
    difference from the swish function but is computationally cheaper while retaining
    the accuracy. For different resource use cases, [howard_searching_2019] introduced
    two models – MobileNetv3-Large and MobileNetv3-Small. MobileNetv3-Large is composed
    of 15 bottleneck blocks while MobileNetv3-Small has 11\. It also included squeeze
    and excitation layer [hu_squeeze-and-excitation_2019] on its building blocks.
    Similar to [sandler_mobilenetv2_2019], these model act as a feature detector in
    SSDLite and is 35% faster than earlier iterations [tan_mnasnet_2018, sandler_mobilenetv2_2019],
    whilst achieving higher mAP.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetv3 [howard_searching_2019] 的核心方法与 MnasNet [tan_mnasnet_2018] 相同，但进行了部分修改。在一个分解的层次化搜索空间中执行平台感知自动神经架构搜索，并由
    NetAdapt [yang_netadapt_2018] 进行优化，该过程通过多个迭代移除网络中未充分利用的组件。一旦获得了架构提案，就会修剪通道，随机初始化权重，然后对其进行微调以提高目标指标。该模型进一步修改以去除架构中的一些昂贵层，从而获得额外的延迟改进。Howard
    等人认为架构中的滤波器通常是彼此的镜像，即使丢弃其中一半，这种准确性也能保持。使用这种技术减少了计算量。MobileNetv3 使用 ReLU 和硬 swish
    的混合作为激活滤波器，后者主要用于模型的末尾。硬 swish 与 swish 函数没有明显区别，但计算上更便宜，同时保持了准确性。对于不同的资源使用场景，[howard_searching_2019]
    引入了两个模型——MobileNetv3-Large 和 MobileNetv3-Small。MobileNetv3-Large 由 15 个瓶颈块组成，而
    MobileNetv3-Small 有 11 个。它还在构建块上包含了 squeeze 和 excitation 层 [hu_squeeze-and-excitation_2019]。与
    [sandler_mobilenetv2_2019] 类似，这些模型在 SSDLite 中充当特征检测器，比早期版本 [tan_mnasnet_2018,
    sandler_mobilenetv2_2019] 快 35%，同时实现了更高的 mAP。
- en: 'TABLE III: Performance comparison of various object detectors on MS COCO and
    PASCAL VOC 2012 datasets at similar input image size.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：在相似输入图像大小下，MS COCO 和 PASCAL VOC 2012 数据集上各种目标检测器的性能比较。
- en: '| Model | Year | Backbone | Size | AP[[0.5:0.95]] | AP[0.5] | FPS |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Model | Year | Backbone | Size | AP[[0.5:0.95]] | AP[0.5] | FPS |'
- en: '| R-CNN* | 2014 | AlexNet | 224 | - | 58.50% | $\sim$0.02 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| R-CNN* | 2014 | AlexNet | 224 | - | 58.50% | $\sim$0.02 |'
- en: '| SPP-Net* | 2015 | ZF-5 | Variable | - | 59.20% | $\sim$0.23 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| SPP-Net* | 2015 | ZF-5 | Variable | - | 59.20% | $\sim$0.23 |'
- en: '| Fast R-CNN* | 2015 | VGG-16 | Variable | - | 65.70% | $\sim$0.43 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Fast R-CNN* | 2015 | VGG-16 | Variable | - | 65.70% | $\sim$0.43 |'
- en: '| Faster R-CNN* | 2016 | VGG-16 | 600 | - | 67.00% | 5 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN* | 2016 | VGG-16 | 600 | - | 67.00% | 5 |'
- en: '| R-FCN | 2016 | ResNet-101 | 600 | 31.50% | 53.20% | $\sim$3 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| R-FCN | 2016 | ResNet-101 | 600 | 31.50% | 53.20% | $\sim$3 |'
- en: '| FPN | 2017 | ResNet-101 | 800 | 36.20% | 59.10% | 5 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| FPN | 2017 | ResNet-101 | 800 | 36.20% | 59.10% | 5 |'
- en: '| Mask R-CNN | 2018 | ResNeXt-101-FPN | 800 | 39.80% | 62.30% | 5 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Mask R-CNN | 2018 | ResNeXt-101-FPN | 800 | 39.80% | 62.30% | 5 |'
- en: '| DetectoRS | 2020 | ResNeXt-101 | 1333 | 53.30% | 71.60% | $\sim$4 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| DetectoRS | 2020 | ResNeXt-101 | 1333 | 53.30% | 71.60% | $\sim$4 |'
- en: '| YOLO* | 2015 | (Modified) GoogLeNet | 448 | - | 57.90% | 45 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| YOLO* | 2015 | (Modified) GoogLeNet | 448 | - | 57.90% | 45 |'
- en: '| SSD | 2016 | VGG-16 | 300 | 23.20% | 41.20% | 46 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SSD | 2016 | VGG-16 | 300 | 23.20% | 41.20% | 46 |'
- en: '| YOLOv2 | 2016 | DarkNet-19 | 352 | 21.60% | 44.00% | 81 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| YOLOv2 | 2016 | DarkNet-19 | 352 | 21.60% | 44.00% | 81 |'
- en: '| RetinaNet | 2018 | ResNet-101-FPN | 400 | 31.90% | 49.50% | 12 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| RetinaNet | 2018 | ResNet-101-FPN | 400 | 31.90% | 49.50% | 12 |'
- en: '| YOLOv3 | 2018 | DarkNet-53 | 320 | 28.20% | 51.50% | 45 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| YOLOv3 | 2018 | DarkNet-53 | 320 | 28.20% | 51.50% | 45 |'
- en: '| CenterNet | 2019 | Hourglass-104 | 512 | 42.10% | 61.10% | 7.8 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| CenterNet | 2019 | Hourglass-104 | 512 | 42.10% | 61.10% | 7.8 |'
- en: '| EfficientDet-D2 | 2020 | Efficient-B2 | 768 | 43.00% | 62.30% | 41.7 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| EfficientDet-D2 | 2020 | Efficient-B2 | 768 | 43.00% | 62.30% | 41.7 |'
- en: '| YOLOv4 | 2020 | CSPDarkNet-53 | 512 | 43.00% | 64.90% | 31 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| YOLOv4 | 2020 | CSPDarkNet-53 | 512 | 43.00% | 64.90% | 31 |'
- en: '| Swin-L | 2021 | HTC++ | - | 57.70% | - | - |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Swin-L | 2021 | HTC++ | - | 57.70% | - | - |'
- en: '| ^aModels marked with * are compared on PASCAL VOC 2012, while others on MS
    COCO.Rows colored gray are real-time detectors ($>$30 FPS). |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ^a带有 * 标记的模型在 PASCAL VOC 2012 上进行比较，而其他模型在 MS COCO 上进行比较。灰色行表示实时检测器（$>$30
    FPS）。|'
- en: 'TABLE IV: Comparison of Lightweight models.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：轻量级模型的比较。
- en: '| Model | Year | Top-1 Acc% | Latency (ms) | Parameters (Million) | FLOPs (Million)
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | Top-1 准确率% | 延迟 (毫秒) | 参数 (百万) | FLOPs (百万) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SqueezeNet | 2016 | 60.5 | - | 3.2 | 833 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeNet | 2016 | 60.5 | - | 3.2 | 833 |'
- en: '| MobileNet | 2017 | 70.6 | 113 | 4.2 | 569 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet | 2017 | 70.6 | 113 | 4.2 | 569 |'
- en: '| ShuffleNet | 2017 | 73.3 | 108 | 5.4 | 524 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ShuffleNet | 2017 | 73.3 | 108 | 5.4 | 524 |'
- en: '| MobileNetv2 | 2018 | 74.7 | 143 | 6.9 | 300 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| MobileNetv2 | 2018 | 74.7 | 143 | 6.9 | 300 |'
- en: '| PeleeNet | 2018 | 72.6 | - | 2.8 | 508 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| PeleeNet | 2018 | 72.6 | - | 2.8 | 508 |'
- en: '| ShuffleNetv2 | 2018 | 75.4 | 178 | 7.4 | 597 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| ShuffleNetv2 | 2018 | 75.4 | 178 | 7.4 | 597 |'
- en: '| MnasNet | 2018 | 76.7 | 103 | 5.2 | 403 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| MnasNet | 2018 | 76.7 | 103 | 5.2 | 403 |'
- en: '| MobileNetv3 | 2019 | 75.2 | 58 | 5.4 | 219 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| MobileNetv3 | 2019 | 75.2 | 58 | 5.4 | 219 |'
- en: '| OFA | 2020 | 80.0 | 58 | 7.7 | 595 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| OFA | 2020 | 80.0 | 58 | 7.7 | 595 |'
- en: VI-I Once-For-All (OFA)
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-I 一次性所有 (OFA)
- en: The use of neural architecture search (NAS) for architecture design has produced
    state-of-the-art models in the past few years, however, they are compute expensive
    because of the sampled model training. Cai et al. in [cai2020onceforall] proposed
    a novel method of decoupling model training stage and the neural architecture
    search stage. The model is trained only once and sub-networks can be distilled
    from it as per the requirements. Once-for-all (OFA) network provides flexibility
    for such sub-networks in four important dimension of a convolutional neural network
    – depth, width, kernel size and dimension. As they are nested within the OFA network
    and interfere with the training, progressive shrinking was introduced. First,
    the largest network is trained with all parameters set to maximum. Subsequently,
    network is fine-tuned by gradually reducing the parameter dimensions like kernel
    size, depth and width. For elastic kernel, a center of the large kernel is used
    as the small kernel. As the center is shared, a kernel transformation matrix is
    used to maintain performance. To vary depth, the first few layers are used and
    the rest are skipped from the large network. Elastic width employs a channel sorting
    operation to reorganize channels and uses the most important ones in smaller models.
    OFA achieved state-of-the-art of 80% in ImageNet top-1 accuracy percentage and
    also won the 4^(th) Low Power Computer Vision Challenge (LPCVC) while reducing
    many order of magnitude of GPU training hours. It shows a new paradigm of designing
    lightweight models for a variety of hardware requirements.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，神经架构搜索（NAS）用于架构设计，产生了最先进的模型。然而，由于样本模型训练，它们的计算开销很大。Cai 等人在 [cai2020onceforall]
    中提出了一种新颖的方法，将模型训练阶段与神经架构搜索阶段解耦。模型只训练一次，并且可以根据需要从中提取子网络。一次性所有（OFA）网络在卷积神经网络的四个重要维度——深度、宽度、卷积核大小和维度——提供了灵活性。由于这些子网络嵌套在OFA网络中并且会干扰训练，因此引入了渐进收缩策略。首先，训练最大的网络，将所有参数设置为最大。随后，通过逐渐减少参数维度（如卷积核大小、深度和宽度）对网络进行微调。对于弹性卷积核，使用大卷积核的中心作为小卷积核。由于中心是共享的，使用卷积核变换矩阵来保持性能。为了改变深度，使用前几层，并从大网络中跳过其余层。弹性宽度采用通道排序操作来重新组织通道，并在较小模型中使用最重要的通道。OFA
    在 ImageNet top-1 准确率中达到了 80% 的最先进水平，并赢得了第 4 届低功耗计算机视觉挑战（LPCVC），同时减少了多个数量级的 GPU
    训练时间。这展示了针对各种硬件需求设计轻量级模型的新范式。
- en: VII Comparative Results
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 比较结果
- en: We compare the performance of both single and two stage detectors on PASCAL
    VOC 2012 [everingham_pascal_2012] and Microsoft COCO [Lin_ms_coco_2014] datasets.
    Performance of object detectors is influenced by a number of factors like input
    image size and scale, feature extractor, GPU architecture, number of proposals,
    training methodology, loss function etc., which makes it difficult to compare
    various models without a common benchmark environment. Here in table [III](#S6.T3
    "TABLE III ‣ VI-H MobileNetv3 ‣ VI Lightweight Networks ‣ A Survey of Modern Deep
    Learning based Object Detection Models"), we evaluate performance of models based
    on the results from their papers. Models are compared on average precision (AP)
    and processed frames per second (FPS) at inference time. AP[0.5] is the average
    precision of all classes when predicted bounding box has an IoU $>$ 0.5 with ground
    truth. COCO dataset introduced another performance metric AP[[0.5:0.95]], or simply
    AP, which is the average AP for IoU from 0.5 to 0.95 in step size of 0.5\. We
    intentionally compare the performances of detectors on similarly size input image,
    where possible, to provide a reasonable account, as authors often introduce an
    array of models to provide flexibility between accuracy and inference time. In
    fig. LABEL:fig:chart, we use only the state-of-the-art model from the possible
    array of object detector family of models. Lightweight models are compared in
    table [IV](#S6.T4 "TABLE IV ‣ VI-H MobileNetv3 ‣ VI Lightweight Networks ‣ A Survey
    of Modern Deep Learning based Object Detection Models") where we compare them
    on ImageNet Top-1 classification accuracy, latency, number of parameters and complexity
    in MFLOPs. Models with MFLOPs lesser than 600 are expected to perform adequately
    on mobile devices.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
