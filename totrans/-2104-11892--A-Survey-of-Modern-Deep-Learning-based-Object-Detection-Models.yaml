- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:55:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2104.11892] A Survey of Modern Deep Learning based Object Detection Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2104.11892](https://ar5iv.labs.arxiv.org/html/2104.11892)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Modern Deep Learning based Object Detection Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Syed Sahil Abbas Zaidi, Mohammad Samar Ansari, Asra Aslam,
  prefs: []
  type: TYPE_NORMAL
- en: 'Nadia Kanwal, Mamoona Asghar, and Brian Lee S.S.A. Zaidi, N. Kanwal, M Asghar
    and B. Lee are with the Athlone Institute of Technology, Ireland. M.S. Ansari
    is with the Aligarh Muslim University, India. A. Aslam is with the Insight Center
    for Data Analytics, National University of Ireland, Galway. (Emails: sahilzaidi78@gmail.com,
    samar.ansari@zhect.ac.in, asra.aslam@insight-centre.org, nkanwal@ait.ie, masghar@ait.ie,
    blee@ait.ie)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Object Detection is the task of classification and localization of objects in
    an image or video. It has gained prominence in recent years due to its widespread
    applications. This article surveys recent developments in deep learning based
    object detectors. Concise overview of benchmark datasets and evaluation metrics
    used in detection is also provided along with some of the prominent backbone architectures
    used in recognition tasks. It also covers contemporary lightweight classification
    models used on edge devices. Lastly, we compare the performances of these architectures
    on multiple metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Object detection and recognition, convolutional neural networks (CNN), lightweight
    networks, deep learning^†^†publicationid: pubid: Preprint submitted to IET Computer
    Vision.'
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object detection is a trivial task for humans. A few months old child can start
    recognizing common objects, however teaching it to the computer has been an uphill
    task until the turn of the last decade. It entails identifying and localizing
    all instances of an object (like cars, humans, street signs, etc.) within the
    field of view. Similarly, other tasks like classification, segmentation, motion
    estimation, scene understanding, etc, have been the fundamental problems in computer
    vision.
  prefs: []
  type: TYPE_NORMAL
- en: Early object detection models were built as an ensemble of hand-crafted feature
    extractors such as Viola-Jones detector [viola_rapid_2001], Histogram of Oriented
    Gradients (HOG) [dalal_histograms_2005] etc. These models were slow, inaccurate
    and performed poorly on unfamiliar datasets. The re-introduction of convolutional
    neural network (CNNs) and deep learning for image classification changed the landscape
    of visual perception. Its use in the ImageNet Large Scale Visual Recognition Challenge
    (ILSVRC) 2012 challenge by AlexNet [NIPS2012_c399862d] inspired further research
    of its application in computer vision. Today, object detection finds application
    from self-driving cars and identity detection to security and medical uses. In
    recent years, it has seen exponential growth with rapid development of new tools
    and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey provides a comprehensive review of deep learning based object detectors
    and lightweight classification architectures. While existing reviews are quite
    thorough [zou_object_2019, liu_deep_2018, chahal_survey_2018, jiao_survey_2019],
    most of them lack new developments in the domain. The main contributions of this
    paper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This paper provides an in-depth analysis of major object detectors in both categories
    – single and two stage detectors. Furthermore, we take historic look at the evolution
    of these methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We present a detailed evaluation of the landmark backbone architectures and
    lightweight models. We could not find any paper which provides a broad overview
    of both these topics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this paper, we have systematically reviewed various object detection architectures
    and its associated technologies, as illustrated in figure [1](#S1.F1 "Figure 1
    ‣ I Introduction ‣ A Survey of Modern Deep Learning based Object Detection Models").
    Rest of this paper is organized as follows. In section [II](#S2 "II Background
    ‣ A Survey of Modern Deep Learning based Object Detection Models"), the problem
    of object detection and its associated challenges are discussed. Various benchmark
    datasets and evaluation metrics are listed in Section [III](#S3 "III Datasets
    and Evaluation Metrics ‣ A Survey of Modern Deep Learning based Object Detection
    Models"). In Section [IV](#S4 "IV Backbone architectures ‣ A Survey of Modern
    Deep Learning based Object Detection Models"), several milestone backbone architectures
    used in modern object detectors are examined. Section [V](#S5 "V Object Detectors
    ‣ A Survey of Modern Deep Learning based Object Detection Models") is divided
    into three major sub-section, each studying a different category of object detectors.
    This is followed by the analysis of a special classification of object detectors,
    called lightweight networks in section [VI](#S6 "VI Lightweight Networks ‣ A Survey
    of Modern Deep Learning based Object Detection Models") and a comparative analysis
    in Section [VII](#S7 "VII Comparative Results ‣ A Survey of Modern Deep Learning
    based Object Detection Models"). The future trends are mentioned in Section LABEL:FuT
    while the paper is concluded in Section LABEL:Con.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13cf4f352e1933238bcf6cb24a85f675.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Structure of the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dde13522b1ecb0584652542ec808d393.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) PASCAL VOC 12
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1177e3b3c15f18fd4eb53d91c451b34.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) MS-COCO
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30b73c5eaabf4d81320e270affe2f725.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) ILSVRC
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/039d0cca93685b9ab247c80e3ac46048.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) OpenImage
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Sample images from different datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: II Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Problem Statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The object detection is the natural extension of object classification, which
    aims only at recognizing the object in the image. The goal of the object detection
    is to detect all instances of the predefined classes and provide its coarse localization
    in the image by axis-aligned boxes. The detector should be able to identify all
    instances of the object classes and draw bounding box around it. It is generally
    seen as a supervised learning problem. Modern object detection models have access
    to large sets of labelled images for training and are evaluated on various canonical
    benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Key challenges in Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Computer vision has come a long way in the past decade, however it still has
    some major challenges to overcome. Some of these key challenges faced by the networks
    in real life applications are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intra class variation : Intra class variation between the instances of same
    object is relatively common in nature. This variation could be due to various
    reasons like occlusion, illumination, pose, viewpoint, etc. These unconstrained
    external can have dramatic effect of the object appearance [liu_deep_2018]. It
    is expected that the objects could have non-rigid deformation or be rotated, scaled
    or blurry. Some objects could have inconspicuous surroundings, making the extraction
    difficult.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of categories: The sheer number of object classes available to classify
    makes it a challenging problem to solve. It also requires more high-quality annotated
    data, which is hard to come by. Using fewer examples for training a detector is
    an open research question.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Efficiency: Present day models need high computation resources to generate
    accurate detection results. With mobile and edge devices becoming common place,
    efficient object detectors are crucial for further development in the field of
    computer vision.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: III Datasets and Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section presents an overview of the datasets that are available, and have
    been most commonly used for object detection tasks.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 PASCAL VOC 07/12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Pascal Visual Object Classes (VOC) challenge was a multiyear effort to accelerate
    the development in the field of visual perception. It started in 2005 with classification
    and detection tasks on four object classes [everingham_pascal_2010], but two versions
    of this challenges are mostly used as a standard benchmark. While the VOC07 challenge
    had 5k training images and more than 12k labelled objects [pascal-voc-2007], the
    VOC12 challenge increased them to 11k training images and more than 27k labelled
    objects [everingham_pascal_2012]. Object classes was expanded to 20 categories
    and the tasks like segmentation and action detection were included as well. Pascal
    VOC introduced the mean Average Precision (mAP) at 0.5 IoU (Intersection over
    Union) to evaluate the performance of the models. Figure [3](#S3.F3 "Figure 3
    ‣ III-A4 Open Image ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣ A
    Survey of Modern Deep Learning based Object Detection Models") depicts the distribution
    of the number of images w.r.t. to the different classes in the Pascal VOC dataset.
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 ILSVRC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [russakovsky_imagenet_2015]
    was an annual challenge running from 2010 to 2017 and became a benchmark for evaluating
    algorithm performance. The dataset size was scaled up to more than a million images
    consisting of 1000 object classification classes. 200 of these classes were hand-picked
    for object detection task, constitute of more than 500k images. Various sources
    including ImageNet [deng_imagenet_2009] and Flikr, were used to construct detection
    dataset. ILSVRC also updated the evaluation metric by loosening the IoU threshold
    to help include smaller object detection. Figure [4](#S3.F4 "Figure 4 ‣ III-A4
    Open Image ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣ A Survey of
    Modern Deep Learning based Object Detection Models") depicts the distribution
    of the number of images w.r.t. to the different classes in the ImageNet dataset.
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 MS-COCO
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Microsoft Common Objects in Context (MS-COCO) [Lin_ms_coco_2014] is one
    of the most challenging datasets available. It has 91 common objects found in
    their natural context which a 4-year-old human can easily recognize. It was launched
    in 2015 and its popularity has only increased since then. It has more than two
    million instances and an average of 3.5 categories per images. Furthermore, it
    contains 7.7 instances per image, comfortably more than other popular datasets.
    MS COCO comprises of images from varied viewpoints as well. It also introduced
    a more stringent method to measure the performance of the detector. Unlike the
    Pascal VOC and ILSVCR, it calculates the IoU from 0.5 to 0.95 in steps of 0.5,
    then using a combination of these 10 values as final metric, called Average Precision
    (AP). Apart from this, it also utilizes AP for small, medium and large objects
    separately to compare performance at different scales. Figure [5](#S3.F5 "Figure
    5 ‣ III-A4 Open Image ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣
    A Survey of Modern Deep Learning based Object Detection Models") depicts the distribution
    of the number of images w.r.t. to the different classes in the MS-COCO dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Comparison of various object detection datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Classes | Train | Validation | Test |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Images | Objects | Objects/Image | Images | Objects | Objects/Image
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PASCAL VOC 12 | 20 | 5,717 | 13,609 | 2.38 | 5,823 | 13,841 | 2.37 | 10,991
    |'
  prefs: []
  type: TYPE_TB
- en: '| MS-COCO | 80 | 118,287 | 860,001 | 7.27 | 5,000 | 36,781 | 7.35 | 40,670
    |'
  prefs: []
  type: TYPE_TB
- en: '| ILSVRC | 200 | 456,567 | 478,807 | 1.05 | 20,121 | 55,501 | 2.76 | 40,152
    |'
  prefs: []
  type: TYPE_TB
- en: '| OpenImage | 600 | 1,743,042 | 14,610,229 | 8.38 | 41,620 | 204,621 | 4.92
    | 125,436 |'
  prefs: []
  type: TYPE_TB
- en: III-A4 Open Image
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Google’s Open Images [kuznetsova_open_2020] dataset is composed of 9.2 million
    images, annotated with image-level labels, object bounding boxes, and segmentation
    masks, among others. It was launched in 2017 and has received six updates. For
    object detection, Open Images has 16 million bounding boxes for 600 categories
    on 1.9 million images, which makes it the largest dataset of object localization.
    Its creators took extra care to choose interesting, complex and diverse images,
    having 8.3 object categories per image. Several changes were made to the AP introduced
    in Pascal VOC like ignoring un-annotated class, detection requirement for class
    and its subclass, etc. Figure [6](#S3.F6 "Figure 6 ‣ III-A4 Open Image ‣ III-A
    Datasets ‣ III Datasets and Evaluation Metrics ‣ A Survey of Modern Deep Learning
    based Object Detection Models") depicts the distribution of the number of images
    w.r.t. to the different classes in the Open Images dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f6ddf6cfbe58ced27cd2cfd1177f891.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: (This image is best viewed in PDF form with magnification) Number
    of images for different classes annotated in the PascalVOC dataset [aslam2021survey]'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1af99794286f5936723b931744b76cc6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: (This image is best viewed in PDF form with magnification) Number
    of images for different classes annotated in the ImageNet dataset [aslam2021survey]'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f06501b78639dedd0d7f6a7621e41d71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: (This image is best viewed in PDF form with magnification) Number
    of images for different classes annotated in the MS-COCO dataset [aslam2021survey]'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58ebafb04acd48c9ac2452f938689004.png)![Refer to caption](img/a2d835e29ff6ebd56ac2a7a36a48a1be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: (This image is best viewed in PDF form with magnification) Number
    of images for different classes annotated in the Open Images dataset [aslam2021survey]'
  prefs: []
  type: TYPE_NORMAL
- en: III-A5 Issues of Data Skew/Bias
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While observing Fig. [3](#S3.F3 "Figure 3 ‣ III-A4 Open Image ‣ III-A Datasets
    ‣ III Datasets and Evaluation Metrics ‣ A Survey of Modern Deep Learning based
    Object Detection Models") through Fig. [6](#S3.F6 "Figure 6 ‣ III-A4 Open Image
    ‣ III-A Datasets ‣ III Datasets and Evaluation Metrics ‣ A Survey of Modern Deep
    Learning based Object Detection Models"), an alert reader would certainly notice
    that the number of images for difference classes vary significantly in all the
    datasets [aslam2021survey]. Three (Pascal VOC, MS-COCO, and Open Images Dataset)
    of the four datasets discussed above have a very significant drop in the number
    of images beyond the top-5 most frequent classes. As can be readily observed for
    Fig. [3](#S3.F3 "Figure 3 ‣ III-A4 Open Image ‣ III-A Datasets ‣ III Datasets
    and Evaluation Metrics ‣ A Survey of Modern Deep Learning based Object Detection
    Models"), there are 13775 images which contain a ‘person’ and then 2829 images
    which contain a ‘car’. The number of images for the remaining 18 classes in this
    dataset almost fall linearly to the 55 images of ‘sheep’. Similarly, for the MS-COCO
    dataset, the class ‘person’ has 262465 images, and the next most-frequent class
    ‘car’ has 43867 images. The downward trend continues till there are only 198 images
    for the class ‘hair drier’. A similar phenomenon is also observed in the Open
    Images Dataset, wherein the class ‘Man’ is the most frequent with 378077 images,
    and the class ‘Paper Cutter’ has only 3 images. This clearly represents a skew
    in the datasets and is bound to create a bias in the training process of any object
    detection model. Therefore, an object detection model trained on these skewed
    datasets will in all probability show better detection performance for the classes
    with more number of images in the training data. Although still present, this
    issue is slightly less pronounced in the ImageNet dataset, as can be observed
    from Fig. [4](#S3.F4 "Figure 4 ‣ III-A4 Open Image ‣ III-A Datasets ‣ III Datasets
    and Evaluation Metrics ‣ A Survey of Modern Deep Learning based Object Detection
    Models") from where it can be seen that the most frequent class i.e. ‘koala’ has
    2469 images, and the least frequent class i.e. ‘cart’ has 624 images. However,
    this leads to another point of concern in the ImageNet dataset: the most frequent
    class is for ‘koala’ and the next most-appearing class is ‘computer keyboard’,
    which are clearly not the most sought after objects in a real-world object detection
    scenario (where person, cars, traffic signs, etc. are of higher concern).'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object detectors use multiple criteria to measure the performance of the detectors
    viz., frames per second (FPS), precision and recall. However, mean Average Precision
    (mAP) is the most common evaluation metric. Precision is derived from Intersection
    over Union (IoU), which is the ratio of the area of overlap and the area of union
    between the ground truth and the predicted bounding box. A threshold is set to
    determine if the detection is correct. If the IoU is more than the threshold,
    it is classified as True Positive while an IoU below it is classified as False
    Positive. If the model fails to detect an object present in the ground truth,
    it is termed as False Negative. Precision measures the percentage of correct predictions
    while the recall measure the correct predictions with respect to the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}Precision&amp;=\frac{True\ Positive}{True\ Positive+False\
    Positive}\\ &amp;=\frac{True\ Positive}{All\ Observations}\end{split}$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\begin{split}Recall&amp;=\frac{True\ Positive}{True\ Positive+False\
    Negative}\\ &amp;=\frac{True\ Positive}{All\ Ground\ Truth}\end{split}$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: Based on the above equation, average precision is computed separately for each
    class. To compare performance between the detectors, the mean of average precision
    of all classes, called mean average precision (mAP) is used, which acts as a single
    metric for final evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: IV Backbone architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Backbone architectures are one of the most important component of the object
    detector. These networks extract feature from the input image used by the model.
    Here, we have discussed some milestone backbone architectures used in modern detectors:'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A AlexNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Krizhevsky et al. proposed AlexNet [NIPS2012_c399862d], a convolutional neural
    network based architecture for image classification, and won the ImageNet Large-Scale
    Visual Recognition Challenge (ILSVRC) 2012 challenge. It achieved a considerably
    higher accuracy (more than 26%) than the contemporary models. AlexNet is composed
    of eight learnable layers - five convolutional and three fully connected layers.
    The last layer of the fully connected layer is connected to an N-way (N: number
    of classes) softmax classifier. It uses multiple convolutional kernels throughout
    the network to obtain features from the image. It also uses dropout and ReLU for
    regularization and faster training convergence respectively. The convolutional
    neural networks were given a new life by its reintroduction in AlexNet and it
    soon became the go-to technique in processing imaging data.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B VGG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While AlexNet [NIPS2012_c399862d] and its successors like [zeiler_visualizing_2014]
    focused on smaller receptive window size to improve accuracy, Simonyan and Zisserman
    investigated the effects of network depth on it. They proposed VGG [simonyan_very_2015],
    which used small convolution filters to construct networks of varying depths.
    While a larger receptive field can be captured by a set of smaller convolutional
    filters, it drastically reduces network parameters and converges sooner. The paper
    demonstrated how deep network architecture (16-19 layers) can be used to perform
    classification and localization with superior accuracy. VGG was created by adding
    a stack of convolutional layers with three fully connected layers, followed by
    a softmax layer. The number of convolutional layers, according to the authors,
    can vary from 8 to 16\. VGG is trained in multiple iterations; first, the smallest
    11-layer architecture is trained with random initialization whose weights are
    then used to train larger networks to prevent gradient instability. VGG outperformed
    ILSVRC 2014 winner GoogLeNet [szegedy_going_2014] in the single network performance
    category. It soon became one of the most used network backbones for object classification
    and detection models.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C GoogLeNet/Inception
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though classification networks were making inroads towards faster and more
    accurate networks, deploying them in real-world applications was still a long
    way off as they were resource-intensive. As networks are scaled for better performance,
    the computation cost increases exponentially. Szegedy et al. in [szegedy_going_2014]
    postulated the wastage of computations in the network as a major reason for it.
    Bigger models also have a large number of parameters and tend to overfit the data.
    They proposed using locally sparse connected architecture instead of a fully connected
    one to solve these issues. GoogLeNet is thus a 22 layer deep network, made up
    by stacking multiple Inception modules on top of each other. Inception modules
    are networks that have multiple sized filters at the same level. Input feature
    maps pass through these filters and are concatenated and forwarded to the next
    layer. The network also has auxiliary classifiers in the intermediate layers to
    help regularize and propagate gradient. GoogLeNet showed how efficient use of
    computation blocks can perform at par with other parameter-heavy networks. It
    achieved 93.3% top-5 accuracy on ImageNet [russakovsky_imagenet_2015] dataset
    without external data, while being faster than other contemporary models. Updated
    versions of Inception like [szegedy_rethinking_2016],[szegedy_inception-v4_2016]
    were also published in the following years which further improved its performance
    and gave further evidence of the applications of refined sparsely connected architectures.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D ResNets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As convolutional neural networks become deeper and deeper, Kaiming He et al.
    in [kaiming_resnet_2016] showed how their accuracy first saturates and then degrades
    rapidly. They proposed the use of residual learning to the stacked layers to mitigate
    the performance decay. It is realized by addition of a skip connection between
    the layers. This connection is an element-wise addition between input and output
    of the block and does not add extra parameter or computational complexity to the
    network. A typical 34 layer ResNet [kaiming_resnet_2016] is basically a large
    (7x7) convolution filter followed by 16 bottleneck modules (pair of small 3x3
    filters with identity shortcut across them) and ultimately a fully connected layer.
    The bottleneck architecture can be adapted for deeper networks by stacking 3 convolutional
    layers (1x1,3x3,1x3) instead of 2\. Kaiming He et al. also demonstrated how the
    16-layer VGG net had higher complexity than their considerably deeper 101 and
    152 layer ResNet architectures while having lower accuracy. In subsequent paper,
    the authors proposed Resnetv2 [he_identity_2016] which used batch normalization
    and ReLU layer in the blocks. It is more generalized and easier to train. ResNets
    are widely used in classification and detection backbones, and its core principles
    have inspired many networks ([huang_densely_2018, xie_aggregated_2017, szegedy_inception-v4_2016]).
  prefs: []
  type: TYPE_NORMAL
- en: IV-E ResNeXt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The existing conventional methods of improving the accuracy of a model were
    by either increasing the depth or the width of the model. However, increasing
    any of these leads to higher model complexity and number of parameters while the
    gain margins diminish rapidly. Xie et al. introduced ResNeXt [xie_aggregated_2017]
    architecture which is simpler and more efficient than other existing models. ResNeXt
    was inspired by the stacking of similar blocks in VGG/ResNet[kaiming_resnet_2016,
    NIPS2012_c399862d] and “split-transform-merge” behavior of Inception module[szegedy_going_2014].
    It is essentially a ResNet where each ResNet block is replaced by an inception-like
    ResNeXt module. The complicated, tailored transformation modules from the Inception
    is replaced by topologically same modules in the ResNeXt blocks, making the network
    easier to scale and generalize. Xie et al. also emphasize that the cardinality
    (topological paths in the ResNeXt block) can be considered as a third dimension,
    along with depth and width, to improve model accuracy. ResNeXt is elegant and
    more concise. It achieved higher accuracy while having considerably fewer hyperparameters
    than a similar depth ResNet architecture. It was also the first runner up to the
    ILSVRC 2016 challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Comparison of Backbone architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Layers | Parameters (Million) | Top-1 acc% | FLOPs (Billion)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AlexNet | 2012 | 7 | 62.4 | 63.3 | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| VGG-16 | 2014 | 16 | 138.4 | 73 | 15.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GoogLeNet | 2014 | 22 | 6.7 | - | 1.6 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-50 | 2015 | 50 | 25.6 | 76 | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNeXt-50 | 2016 | 50 | 25 | 77.8 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CSPResNeXt-50 | 2019 | 59 | 20.5 | 78.2 | 7.9 |'
  prefs: []
  type: TYPE_TB
- en: '| EfficientNet-B4 | 2019 | 160 | 19 | 83 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: IV-F CSPNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing neural networks have shown incredible results in achieving high accuracy
    in computer vision tasks; however, they rely on excessive computational resources.
    Wang et al. believe that heavy inference computations can be reduced by cutting
    down the duplicate gradient information in the network. They proposed CSPNet [wang_cspnet_2019]
    which creates different paths for the gradient flow within the network. CSPNet
    separates feature maps at the base layer into two parts. One part is passed through
    the partial convolution network block (e.g., Dense and Transition block in DenseNet
    [huang_densely_2018] or Res(X) block in ResNeXt [xie_aggregated_2017]) while the
    other part is combined with its outputs at a later stage. This reduces the number
    of parameters, increases the utilization of computation units and eases memory
    footprint. It is easy to implement and general enough to be applicable on other
    architectures like ResNet [kaiming_resnet_2016], ResNeXt [xie_aggregated_2017],
    DenseNet [huang_densely_2018], Scaled-YOLOv4 [wang_scaled-yolov4_2020] etc. Applying
    CSPNet on these networks reduced computations from 10% to 20%, while the accuracy
    remained constant or improved. Memory cost and computational bottleneck is also
    reduced significantly with this method. It is leveraged in many state of the art
    detector models, while also being used for mobile and edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af1d5b84b389ad9d65a1e6c841befc99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Visualization of CNN Architectures²²2Tool Used: https://netron.app/.
    Left to Right: AlexNet, VGG$-$16, GoogLeNet, ResNet$-$50, CSPResNeXt$-$50, EfficientNet$-$B4.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-G EfficientNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tan et al. systematically studied network scaling and its effects on the model
    performance. They summarized how altering network parameters like depth, width
    and resolution influence its accuracy. Scaling any parameter individually comes
    with an associated cost. Increasing depth of a network can help in capturing richer
    and more complex features, but they are difficult to train due to vanishing gradient
    problem. Similarly, scaling network width will make it easier to capture fine
    grained features but have difficulty in obtaining high level features. Gains from
    increasing the image resolution, like depth and width, saturate as model scales.
    In the paper [tan_efficientnet_2020], Tan et al. proposed the use of a compound
    coefficient that can uniformly scale all three dimensions. Each model parameter
    has an associated constant, which is found by fixing the coefficient as $1$ and
    performing a grid search on a baseline network. The baseline architecture, inspired
    by their previous work [tan_mnasnet_2018], is developed by neural architecture
    search on a search target while optimizing accuracy and computations. EfficientNet
    is a simple and efficient architecture. It outperformed existing models in accuracy
    and speed while being considerably smaller. By providing a monumental increase
    in efficiency, it could potentially open a new era in the field of efficient networks.
  prefs: []
  type: TYPE_NORMAL
- en: V Object Detectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have divided this review based on the two types of detectors — two-stage
    and single-stage detectors. However, we also discussed the pioneer work, where
    we briefly examine a few traditional object detectors. A network which has a separate
    module to generate region proposals is termed as a two-stage detector. These models
    try to find an arbitrary number of objects proposals in an image during the first
    stage and then classify and localize them in the second. As these systems have
    two separate steps, they generally take longer to generate proposals, have complicated
    architecture and lacks global context. Single-stage detectors classify and localize
    semantic objects in a single shot using dense sampling. They use predefined boxes/keypoints
    of various scale and aspect ratio to localize objects. It edges two-stage detectors
    in real-time performance and simpler design.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Pioneer Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-A1 Viola-Jones
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Primarily designed for face detection, Viola-Jones object detector [viola_rapid_2001],
    proposed in 2001, was an accurate and powerful detector. It combined multiple
    techniques like Haar-like features, integral image, Adaboost and cascading classifier.
    First step is to search for Haar-like features by sliding a window on the input
    image and uses integral image to calculate. It then uses a trained Adaboost to
    find the classifier of each haar feature and cascades them. Viola Jones algorithm
    is still used in small devices as it is very efficient and fast.
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 HOG Detector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2005, Dalal and Triggs proposed the Histogram of Oriented Gradients (HOG)
    [dalal_histograms_2005] feature descriptor used to extract features for object
    detection. It was an improvement over other detectors like [lowe_distinctive_2004,
    lowe_object_1999, mohan_example-based_2001, yan_ke_pca-sift_2004]. HOG extracts
    gradient and its orientation of the edges to create a feature table. The image
    is divided into grids and the feature table is then used to create histogram for
    each cell in the grid. HOG features are generated for the region of interest and
    fed into a linear SVM classifier for detection. The detector was proposed for
    pedestrian detection; however, it could be trained to detect various classes.
  prefs: []
  type: TYPE_NORMAL
- en: V-A3 DPM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deformable Parts Model (DPM) [felzenszwalb_discriminatively_2008] was introduced
    by Felzenszwalb et al. and was the winner Pascal VOC challenge in 2009\. It used
    individual “part” of the object for detection and achieved higher accuracy than
    HOG. It follows the philosophy of divide and rule; parts of the object are individually
    detected during inference time and a probable arrangement of them is marked as
    detection. For example, a human body can be considered as a collection of parts
    like head, arms, legs and torso. One model will be assigned to capture one of
    the parts in the whole image and the process is repeated for all such parts. A
    model then removes improbable configurations of the combination of these parts
    to produce detection. DPM based models [felzenszwalb_object_2010, felzenszwalb_cascade_2010]
    were one of the most successful algorithms before the era of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Two-Stage Detectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-B1 R-CNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Region-based Convolutional Neural Network (R-CNN) [girshick_rich_2014] was
    the first paper in the R-CNN family, and demonstrated how CNNs can be used to
    immensely improve the detection performance. R-CNN use a class agnostic region
    proposal module with CNNs to convert detection into classification and localization
    problem. A mean-subtracted input image is first passed through the region proposal
    module, which produces 2000 object candidates. This module find parts of the image
    which has a higher probability of finding an object using Selective Search [uijlings_selective_2013].
    These candidates are then warped and propagated through a CNN network, which extracts
    a 4096-dimension feature vector for each proposal. Girshick et al. used AlexNet
    [NIPS2012_c399862d] as the backbone architecture of the detector. The feature
    vectors are then passed to the trained, class-specific Support Vector Machines
    (SVMs) to obtain confidence scores. Non-maximum suppression (NMS) is later applied
    to the scored regions, based on its IoU and class. Once the class has been identified,
    the algorithm predicts its bounding box using a trained bounding-box regressor,
    which predicts four parameters i.e., center coordinates of box along with its
    width and height.
  prefs: []
  type: TYPE_NORMAL
- en: R-CNN has a complicated multistage training process. The first stage is pre-training
    the CNN with a large classification dataset. It is then fine-tuned for detection
    using domain-specific images (mean-subtracted, warped proposals) by replacing
    of the classification layer with a randomly initialized N+1-way classifier, N
    being the number of classes, using stochastic gradient descent (SGD) [lecun_backpropagation_1989].
    One liner SVM and bounding box regressor is trained for each class.
  prefs: []
  type: TYPE_NORMAL
- en: R-CNN ushered a new wave in the field of object detection, but it was slow (47
    sec per image) and expensive in time and space [girshick_fast_2015]. It had complex
    training process and took days to train on small datasets even when some of the
    computations were shared.
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 SPP-Net
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: He et al. proposed the use of Spatial Pyramid Pooling (SPP) layer [grauman_pyramid_2005]
    to process image of arbitrary size or aspect ratio. They realized that only the
    fully connected part of the CNN required a fixed input. SPP-net [he_spatial_2015]
    merely shifted the convolution layers of CNN before the region proposal module
    and added a pooling layer, thereby making the network independent of size/aspect
    ratio and reducing the computations. The selective search [uijlings_selective_2013]
    algorithm is used to generate candidate windows. Feature maps are obtained by
    passing the input image through the convolution layers of a ZF-5 [zeiler_visualizing_2014]
    network. The candidate windows are then mapped on to the feature maps, which are
    subsequently converted into fixed length representations by spatial bins of a
    pyramidal pooling layer. This vector is passed to the fully connected layer and
    ultimately, to SVM classifiers to predict class and score. Similar to R-CNN [girshick_rich_2014],
    SPP-net has as post processing layer to improve localization by bounding box regression.
    It also uses the same multistage training process, except that the fine tuning
    is done only on the fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: SPP-Net is considerably faster than the R-CNN model with comparable accuracy.
    It can process images of any shape/aspect ratio and thus, avoid object deformation
    due to input warping. However, as its architecture is analogous to R-CNN, it shared
    R-CNN’s disadvantages too like multistage training, computationally expensive
    and training time as well.
  prefs: []
  type: TYPE_NORMAL
- en: V-B3 Fast R-CNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the major issues with R-CNN/SPP-Net was the need to train multiple systems
    separately. Fast R-CNN [girshick_fast_2015] solved this by creating a single end-to-end
    trainable system. The network takes as input an image and its object proposals.
    The image is passed through a set of convolution layers and the object proposals
    are mapped to the obtained feature maps. Girshick replaced pyramidal structure
    of pooling layers from SPP-net [he_spatial_2015] with a single spatial bin, called
    RoI pooling layer. This layer is connected to 2 fully connected layer and then
    branches out into a N+1-class SoftMax layer and a bounding box regressor layer,
    which has a fully connected layer as well. The model also changed the loss function
    of bounding box regressor from L2 to smooth L1 to better performance, while introducing
    a multi-task loss to train the network.
  prefs: []
  type: TYPE_NORMAL
- en: The authors used modified version of existing state-of-art pre-trained models
    like [NIPS2012_c399862d], [simonyan_very_2015] and [jia_caffe_2014] as backbone.
    The network was trained in a single step by stochastic gradient descent (SGD)
    and a mini-batch of 2 images. This helped the network converge faster as the back-propagation
    shared computations among the RoIs from the two images.
  prefs: []
  type: TYPE_NORMAL
- en: Fast R-CNN was introduced as an improvement in speed (146x on R-CNN) while the
    increase in accuracy was supplementary. It simplified training procedure, removed
    pyramidal pooling and introduces a new loss function. The object detector, without
    the region proposal network, reported near real time speed with considerable accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3a555dcb1949744a5af9446876199ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Illustration of the internal architecture of different two stage
    object detectors⁴⁴4Features created using: https://poloclub.github.io/cnn-explainer/.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B4 Faster R-CNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even though Fast R-CNN inched closer to real time object detection, its region
    proposal generation was still an order of magnitude slower (2 sec per image compared
    to 0.2 sec per image). Ren et al. suggested a fully convoluted network [long_fully_2015]
    as a region proposal network (RPN) in [ren_faster_2015] that takes an arbitrary
    input image and outputs a set of candidate windows. Each such window has an associated
    objectness score which determines likelihood of an object. Unlike its predecessors
    like [felzenszwalb_object_2010, girshick_fast_2015, kaiming_resnet_2016] which
    used image pyramids to solve size variance of objects, RPN introduces Anchor boxes.
    It used multiple bounding boxes of different aspect ratios and regressed over
    them to localize object. The input image is first passed through the CNN to obtain
    a set of feature maps. These are forwarded to the RPN, which produces bounding
    boxes and their classification. Selected proposals are then mapped back to the
    feature maps obtained from previous CNN layer in RoI pooling layer, and ultimately
    fed to fully connected layer, which is sent to classifier and bounding box regressor.
    Faster R-CNN is essentially Fast R-CNN with RPN as region proposal module.
  prefs: []
  type: TYPE_NORMAL
- en: Training of Faster R-CNN is more convoluted, due to the presence of shared layers
    between two models which perform very different tasks. Firstly, RPN is pre-trained
    on ImageNet dataset [deng_imagenet_2009] and fine-tuned on PASCAL VOC dataset
    [everingham_pascal_2010]. A Fast R-CNN is trained from the region proposals of
    RPN from first step. Till this point, the networks do not have shared convolution
    layer. Now, we fix the convolution layers of the detector and fine-tune the unique
    layers in RPN. And finally, Fast R-CNN is fine-tuned from the updated RPN.
  prefs: []
  type: TYPE_NORMAL
- en: Faster R-CNN improved the detection accuracy over the previous state-of-art
    [girshick_fast_2015] by more than 3% and decreased inference time by an order
    of magnitude. It fixed the bottleneck of slow region proposal and ran in near
    real time at 5 frames per second. Another advantage of having a CNN in region
    proposal was that it could learn to produce better proposals and thereby increase
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: V-B5 FPN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use of image pyramid to obtain feature pyramid (or featurized image pyramids)
    at multiple levels is a common method to increase detection of small objects.
    Even though it increases Average Precision of the detector, the increase in the
    inference time is substantial. Lin et al. proposed the Feature Pyramid Network
    (FPN) [lin2017feature], which has a top-down architecture with lateral connections
    to build high-level semantic features at different scales. The FPN has two pathways,
    a bottom-up pathway which is a ConvNet computing feature hierarchy at several
    scales and a top-down pathway which upsamples coarse feature maps from higher
    level into high-resolution features. These pathways are connected by lateral connection
    by a 1x1 convolution operation to enhance the semantic information in the features.
    FPN is used as a region proposal network (RPN) of a ResNet-101 [kaiming_resnet_2016]
    based Faster R-CNN here.
  prefs: []
  type: TYPE_NORMAL
- en: FPN could provide high-level semantics at all scales, which reduced the error
    rate in detection. It became a standard building block in future detections models
    and improved accuracy their accuracy across the table. It also lead to development
    of other improved networks like PANet [liu_path_2018], NAS-FPN [ghiasi_nas-fpn_2019]
    and EfficientNet [tan_efficientnet_2020], which is current state of art detector.
  prefs: []
  type: TYPE_NORMAL
- en: V-B6 R-FCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dai et al. proposed Region-based Fully Convolutional Network (R-FCN) [dai_r-fcn_2016]
    that shared almost all computations within the network, unlike previous two stage
    detectors which applied resource intensive techniques on each proposal. They argued
    against the use of fully connected layers and instead used convolutional layers.
    However, deeper layers in the convolutional network are translation-invariant,
    making them ineffective for localization tasks. The authors proposed the use of
    position-sensitive score maps to remedy it. These sensitive score maps encode
    relative spatial information of the subject and are later pooled to identify exact
    localization. R-FCN does it by dividing the region of interest into k x k grid
    and scoring the likeliness of each cell with the detection class feature map.
    These scores are later averaged and used to predict the object class. R-FCN detector
    is a combination of four convolutional networks. The input image is first passed
    through the ResNet-101[kaiming_resnet_2016] to get feature maps. An intermediate
    output (Conv4 layer) is passed to a Region Proposal Network (RPN) to identify
    RoI proposals while the final output is further processed through a convolutional
    layer and is input to classifier and regressor. The classification layer combines
    the generated the position-sensitive map with the RoI proposals to generate predictions
    while the regression network outputs the bounding box details. R-FCN is trained
    in a similar 4 step fashion as Faster-RCNN [ren_faster_2015] whilst using a combined
    cross-entropy and box regression loss. It also adopts online hard example mining
    (OHEM) [shrivastava_training_2016] during the training.
  prefs: []
  type: TYPE_NORMAL
- en: Dai et al. offered a novel method to solve the problem of translation invariance
    in convolutional neural networks. R-FCN combines Faster R-CNN and FCN to achieve
    a fast, more accurate detector. Even though it did not improve accuracy by much,
    but it was 2.5-20 times faster than its counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: V-B7 Mask R-CNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mask R-CNN [he2018mask] extends on the Faster R-CNN by adding another branch
    in parallel for pixel-level object instance segmentation. The branch is a fully
    connected network applied on RoIs to classify each pixel into segments with little
    overall computation cost. It uses similar basic Faster R-CNN architecture for
    object proposal, but adds a mask head parallel to classification and bounding
    box regressor head. One major difference was the use of RoIAlign layer, instead
    of RoIPool layer, to avoid pixel level misalignment due to spatial quantization.
    The authors chose the ResNeXt-101 [xie_aggregated_2017] as its backbone along
    with the feature Pyramid Network (FPN) for better accuracy and speed. The loss
    function of Faster R-CNN is updated with the mask loss and as in FPN, it uses
    5 anchor boxes with 3 aspect ratio. Overall training of Mask R-CNN is similar
    to faster R-CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Mask R-CNN performed better than the existing state of the art single-model
    architectures, added an extra functionality of instance segmentation with little
    overhead computations. It is simple to train, flexible and generalizes well in
    applications like keypoint detection, human pose estimation, etc. However, it
    was still below the real time performance ($>$30 fps).
  prefs: []
  type: TYPE_NORMAL
- en: V-B8 DetectoRS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many contemporary two stage detectors like [ren_faster_2015, chen_hybrid_2019,
    cai_cascade_2017] use the mechanism of looking and thinking twice i.e. calculating
    object proposals first and using them to extract features to detect objects. DetectoRS
    [qiao_detectors_2020] applies this mechanism at both macro and micro level of
    the network. At macro level, they propose Recursive Feature Pyramid (RFP), formed
    by stacking multiple feature pyramid network (FPN) with extra feedback connection
    from the top-down level path in FPN to the bottom-up layer. The output of the
    FPN is processed by the Atrous Spatial Pyramid Pooling layer (ASPP) [chen_deeplab_2017]
    before passing it to the next FPN layer. A Fusion module is used to combine FPN
    outputs from different modules by creating an attention map. At micro level, Qiao
    et al. presented the Switchable Atrous Convolution (SAC) to regulate the dilation
    rate of convolution. An average pooling layer with 5x5 filter and a 1x1 convolution
    is used as a switch function to decide the rate of atrous convolution [holschneider_real-time_1990],
    helping the backbone detect objects at various scale on the fly. They also packed
    the SAC in between two global context modules [hu_squeeze-and-excitation_2019]
    as it helps in making more stable switching. The combination of these two techniques,
    Recursive Feature Pyramid and Switchable Atrous Convolution results in DetectoRS.
    The authors incorporated the above techniques with the Hybrid Task Cascade (HTC)
    [chen_hybrid_2019] as the baseline model and a ResNext-101 backbone.
  prefs: []
  type: TYPE_NORMAL
- en: DetectoRS combined multiple systems to improve performance of the detector and
    sets the state-of-the-art for the two stage detectors. Its RFP and SAC modules
    are well generalized and can be used in other detection models. However, it is
    not suitable for real time detections as it can only process about 4 frames per
    second.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Single Stage Detectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-C1 YOLO
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two stage detectors solve the object detection as a classification problem,
    a module presents some candidates which the network classifies as either an object
    or background. However, YOLO or You Only Look Once [redmon_you_2016] reframed
    it as a regression problem, directly predicting the image pixels as objects and
    its bounding box attributes. In YOLO, the input image is divided into a S x S
    grid and the cell where the object’s center falls is responsible for detecting
    it. A grid cell predicts multiple bounding boxes, and each prediction array consists
    of 5 elements: center of bounding box – x and y, dimensions of the box – w and
    h, and the confidence score.'
  prefs: []
  type: TYPE_NORMAL
- en: YOLO was inspired from the GoogLeNet model for image classification [szegedy_going_2014],
    which uses cascaded modules of smaller convolution networks [lin_network_2014].
    It is pre-trained on ImageNet data [deng_imagenet_2009] till the model achieves
    high accuracy and then modified by adding randomly initialized convolution and
    fully connected layers. At training time, grid cells predict only one class as
    it converges better, but it is be increased during the inference time. Multitask
    loss, combined loss of all predicted components, is used to optimize the model.
    Non maximum suppression (NMS) removes class-specific multiple detections.
  prefs: []
  type: TYPE_NORMAL
- en: YOLO surpassed its contemporary single stage real time models by a huge margin
    in both accuracy and speed. However, it had significant shortcomings as well.
    Localization accuracy for small or clustered objects and limitation to number
    of objects per cell were its major drawbacks. These issues were fixed in later
    versions of YOLO [redmon_yolo9000_2016, redmon_yolov3_2018, bochkovskiy_yolov4_2020].
  prefs: []
  type: TYPE_NORMAL
- en: V-C2 SSD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Single Shot MultiBox Detector (SSD) [liu_ssd_2016] was the first single stage
    detector that matched accuracy of contemporary two stage detectors like Faster
    R-CNN [ren_faster_2015], while maintaining real time speed. SSD was built on VGG-16
    [simonyan_very_2015], with additional auxiliary structures to improve performance.
    These auxiliary convolution layers, added to the end of the model, decrease progressively
    in size. SSD detects smaller objects earlier in the network when the image features
    are not too crude, while the deeper layers were responsible for offset of the
    default boxes and aspect ratios [erhan_scalable_2014].
  prefs: []
  type: TYPE_NORMAL
- en: During training, SSD match each ground truth box with the default boxes with
    the best jaccard overlap and train the network accordingly, similar to Multibox
    [erhan_scalable_2014]. They also used hard negative mining and heavy data augmentation.
    Similar to DPM [felzenszwalb_discriminatively_2008], it utilized weighted sum
    of the localization and confidence loss to train the model. Final output is obtained
    by performing non maximum suppression.
  prefs: []
  type: TYPE_NORMAL
- en: Even though SSD was significantly faster and more accurate than both state-of-art
    networks like YOLO and Faster R-CNN, it had difficulty in detecting small objects.
    This issue was later solved by using better backbone architectures like ResNet
    and other small fixes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1c44d95f15ba2fa5865008f52ac407e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Illustration of the internal architecture of different two and single
    stage object detectors²²footnotemark: 2.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C3 YOLOv2 and YOLO9000
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: YOLOv2 [redmon_yolo9000_2016], an improvement on the YOLO [redmon_you_2016],
    offered an easy tradeoff between speed and accuracy while the YOLO9000 model could
    predict 9000 object classes in real time. They replaced the backbone architecture
    of GoogLeNet [szegedy_going_2014] with DarkNet-19 [redmon_darknet_2016]. It incorporated
    many impressive techniques like Batch Normalization [he_delving_2015] to improve
    convergence, joint training of classification and detection systems to increase
    detection classes, removing fully connected layers to increase speed and using
    learnt anchor boxes to improve recall and have better priors. Redmon et al. also
    combined the classification and detection datasets in hierarchical structure using
    WordNet [miller_introduction_1991]. This WordTree can be used to predict a higher
    conditional probability of hypernym, even when the hyponym is not classified correctly,
    thereby increasing the overall performance of the system.
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv2 provided better flexibility to choose the model on speed and accuracy,
    and the new architecture had fewer parameters. As the title of the paper suggests,
    it was “better, faster and stronger” [redmon_yolo9000_2016].
  prefs: []
  type: TYPE_NORMAL
- en: V-C4 RetinaNet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the difference between the accuracies of single and two stage detectors,
    Lin et al. suggested that the reason single stage detectors lag is the “extreme
    foreground-background class imbalance” [lin_focal_2017]. They proposed a reshaped
    cross entropy loss, called Focal loss as the means to remedy the imbalance. Focal
    loss parameter reduces the loss contribution from easy examples. The authors demonstrate
    its efficacy with the help of a simple, single stage detector, called RetinaNet
    [lin_focal_2017], which predicts objects by dense sampling of the input image
    in location, scale and aspect ratio. It uses ResNet [kaiming_resnet_2016] augmented
    by Feature Pyramid Network (FPN) [lin2017feature] as the backbone and two similar
    subnets - classification and bounding box regressor. Each layer from the FPN is
    passed to the subnets, enabling it to detect objects as various scales. The classification
    subnet predicts the object score for each location while the box regression subnet
    regresses the offset for each anchor to the ground truth. Both subnets are small
    FCN and share parameters across the individual networks. Unlike most previous
    works, the authors employ a class-agnostic bounding box regressor and found them
    to be equally effective.
  prefs: []
  type: TYPE_NORMAL
- en: RetinaNet is simple to train, converges faster and easy to implement. It achieved
    better performance in accuracy and run time than the two stage detectors. RetinaNet
    also pushed the envelope in advancing the ways object detectors are optimized
    by the introduction of a new loss function.
  prefs: []
  type: TYPE_NORMAL
- en: V-C5 YOLOv3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: YOLOv3 had “incremental improvements” from the previous YOLO versions [redmon_you_2016,
    redmon_yolo9000_2016]. Redmon et al. replaced the feature extractor network with
    a larger Darknet-53 network[redmon_darknet_2016]. They also incorporated various
    techniques like data augmentation, multi-scale training, batch normalization,
    among others. Softmax in classifier layer was replaced by a logistical classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Even though YOLOv3 was faster than YOLOv2 [redmon_yolo9000_2016], it lacked
    any ground breaking change from its predecessor. It even had lesser accuracy than
    an year old state-of-the-art detector [lin_focal_2017].
  prefs: []
  type: TYPE_NORMAL
- en: V-C6 CenterNet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zhou et al. in [zhou_objects_2019] takes a very different approach of modelling
    objects as points, instead of the conventional bounding box representation. CenterNet
    predicts the object as a single point at the center of the bounding box. The input
    image is passed through the FCN that generates a heatmap, whose peaks correspond
    to center of detected object. It uses a ImageNet pretrained stacked Hourglass-101
    [newell_stacked_2016] as the feature extractor network and has 3 heads – heatmap
    head to determine the object center, dimension head to estimate size of object
    and offset head to correct offset of object point. Multitask loss of all three
    heads is back propagated to feature extractor while training. During inference,
    the output from offset head is used to determine the object point and finally
    a box is generated. As the predictions, not the result, are points and not bounding
    boxes, non-maximum suppression (NMS) is not required for post-processing.
  prefs: []
  type: TYPE_NORMAL
- en: CenterNet brings a fresh perspective and set aside years of progress in the
    field of object detection. It is more accurate and has lesser inference time than
    its predecessors. It has high precision for multiple tasks like 3D object detection,
    keypoint estimation, pose, instance segmentation, orientation detection and others.
    However, it requires different backbone architectures as general architectures
    that work well with other detectors give poor performance with it and vice-versa.
  prefs: []
  type: TYPE_NORMAL
- en: V-C7 EfficientDet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EfficientDet [tan_efficientdet_2020] builds towards the idea of scalable detector
    with higher accuracy and efficiency. It introduces efficient multi-scale features,
    BiFPN and model scaling. BiFPN is bi-directional feature pyramid network with
    learnable weights for cross connection of input features at different scales.
    It improves on NAS-FPN [ghiasi_nas-fpn_2019], which required heavy training and
    had complex network, by removing one-input nodes and adding an extra lateral connection.
    This eliminates less efficient nodes and enhances high-level feature fusion. Unlike
    existing detectors which scale up with bigger, deeper backbone or stacking FPN
    layers, EfficientDet introduces a compounding coefficient which can be used to
    “jointly scale up all dimensions of backbone network, BiFPN network, class/box
    network and resolution” [tan_efficientdet_2020]. EfficientDet utilizes EfficientNet
    [tan_efficientnet_2020] as the backbone network with multiple sets of BiFPN layers
    stacked in series as feature extraction network. Each output from the final BiFPN
    layer is sent to class and box prediction network. The model is trained using
    SGD optimizer along with synchronized batch normalization and uses swish activation
    [ramachandran_searching_2017], instead of the standard ReLU activation, which
    is differentiable, more efficient and has better performance.
  prefs: []
  type: TYPE_NORMAL
- en: EfficientDet achieves better efficiency and accuracy than previous detectors
    while being smaller and computationally cheaper. It is easy to scale, generalizes
    well for other tasks and is the current state-of-the-art model for single-stage
    object detection.
  prefs: []
  type: TYPE_NORMAL
- en: V-C8 YOLOv4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: YOLOv4 [bochkovskiy_yolov4_2020] incorporated a lot of exciting ideas to design
    a fast and easy to train object detector that could work in existing production
    systems. It utilizes “bag of freebies” i.e., methods that only increase training
    time and do not affect the inference time. YOLOv4 utilizes data augmentation techniques,
    regularization methods, class label smoothing, CIoU-loss [zheng_distance-iou_2019],
    Cross mini-Batch Normalization (CmBN) , Self-adversarial training, Cosine annealing
    scheduler [loshchilov_sgdr_2017] and other tricks to improve training. Methods
    that only affect the inference time, called “Bag of Specials”, are also added
    to the network, including Mish activation [misra_mish_2020], Cross-stage partial
    connections (CSP) [wang_cspnet_2019], SPP-Block [he_spatial_2015], PAN path aggregated
    block [liu_path_2018] , Multi input weighted residual connections (MiWRC), etc.
    It also used genetic algorithm for searching hyper-parameter. It has an ImageNet
    pre-trained CSPNetDarknet-53 backbone, SPP and PAN block neck and YOLOv3 as detection
    head.
  prefs: []
  type: TYPE_NORMAL
- en: Most existing detection algorithms require multiple GPUs to train model, but
    YOLOv4 can be easily trained on a single GPU. It is twice as fast as EfficientDet
    with comparable performance. It is the state-of-the-art for real time single stage
    detectors.
  prefs: []
  type: TYPE_NORMAL
- en: V-C9 Swin Transformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transformers [vaswani2017attention] have had a profound impact in the Natural
    Language Processing (NLP) domain since its inception. Its application in language
    models like BERT (Bidirectional Encoder Representation from Transformers) [devlin2019bert],
    GPT (Generative Pre-trained Transformer) [radford2018improving], T5 (Text-To-Text
    Transfer Transformer) [2020t5] etc. have pushed the state of the art in the field.
    Transformers [vaswani2017attention] uses the attention model to establish dependencies
    among the elements of the sequence and can a attend to longer context than other
    sequential architectures. The success of transformers in NLP sparked interest
    in its application in computer vision. While CNNs have been the backbone on advancement
    in vision, they have some inherent shortcomings like the lack of importance of
    global context, fixed post-training weights [khan2021transformers] etc.
  prefs: []
  type: TYPE_NORMAL
- en: Swin Transformer [liu2021swin] seeks to provide a transformer based backbone
    for computer vision tasks. It splits the input images in multiple, non-overlapping
    patches and converts them into embeddings. Numerous Swin Transformer blocks are
    then applied to the patches in 4 stages, with each successive stage reducing the
    number of patches to maintain hierarchical representation. The Swin Transformer
    block is composed of local multi-headed self-attention (MSA) modules, based on
    alternating shifted patch window in successive blocks. Computation complexity
    becomes linear with image size in local self-attention while shifted window enables
    cross-window connection. [liu2021swin] also shows how shifted windows increase
    detection accuracy with little overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers present a paradigm shift from the CNN based neural networks. While
    its application in vision is still in a nascent stage, its potential to replace
    convolution from these tasks is very real. Swin Transformer achieved the state-of-the-art
    on MS COCO dataset, but utilises comparatively higher parameters than convolutional
    models.
  prefs: []
  type: TYPE_NORMAL
- en: VI Lightweight Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A new branch of research has shaped up in recent years, aimed at designing small
    and efficient networks for resource constrained environments as is common in Internet
    of Things (IoT) deployments [abbas2021lightweight, karakanis2021lightweight, jadon2020low,
    jadon2019firenet]. This trend has percolated to the design of potent object detectors
    too. It is seen that although a large number of object detectors achieve excellent
    accuracy and perform inference in real-time, a majority of these models require
    excessive computing resources and therefore cannot be deployed on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: Many different approaches have shown exciting results in the past. Utilization
    of efficient components and compression techniques like pruning ([lecun_optimal_1990,
    hassibi_advances_1993]), quantization ([han_deep_2016, courbariaux_binaryconnect_2016]),
    hashing [chen_compressing_2015], etc. have improved the efficiency of deep learning
    models. Use of trained large network to train smaller models, called distillation
    [hinton_distilling_2015], has also shown interesting results. However in this
    section, we explore some prominent examples of efficient neural network design
    for achieving high performance on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A SqueezeNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recent advances in the field of CNNs had mostly focused on improving the state-of-the-art
    accuracy on the benchmark datasets, which led to an explosion of model size and
    their parameters. But in 2016, Iandola et al. proposed a smaller, smarter network
    called SqueezeNet [iandola_squeezenet_2016], which reduced the parameters while
    maintaining the performance. They achieved it by employing three main design strategies
    viz. using smaller filters, decreasing the number of input channels to 3x3 filters
    and placing downsampling layers later in the network. The first two strategies
    decrease the number of parameters while attempting to preserve the accuracy and
    the third strategy increases the accuracy of the network. The building block of
    SqueezeNet is called a fire module, which consist of two layers: a squeeze layer
    and an expand layer, each with a ReLU activation. The squeeze layer is made up
    of multiple 1x1 filters while the expand layer is a mix of 1x1 and 3x3 filters,
    thereby limiting the number of input channels. The SqueezeNet architecture is
    composed of a stack of 8 Fire modules squashed in between the convolution layers.
    Inspired by ResNet [kaiming_resnet_2016], SqueezeNet with residual connections
    was also proposed which increased the accuracy over the vanilla model. The authors
    also experimented with Deep Compression [han_deep_2016] and achieved 510$\times$
    reduction in model size compared to AlexNet, while maintaining the baseline accuracy.
    SqueezeNet presented a good candidate for improving the hardware efficiency of
    the neural network architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B MobileNets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MobileNet [howard_mobilenets_2017] moved away from the conventional methods
    of small models like shrinking, pruning, quantization or compressing, and instead
    used efficient network architecture. The network used depthwise separable convolution,
    which factorizes a standard convolution into a depthwise convolution and a 1x1
    pointwise convolution. A standard convolution uses kernels on all input channels
    and combines them in one step while the depthwise convolution uses different kernels
    for each input channel and uses pointwise convolution to combine inputs. This
    separation of filtering and combining of features reduces the computation cost
    and model size. MobileNet consists of 28 separate convolutional layers, each followed
    by batch normalization and ReLU activation function. Howard et al. also introduced
    the two model shrinking hyperparameters: width and resolution multiplier, in order
    to further improve speed and reduce size of the model. The width multiplier manipulates
    the width of the network uniformly by reducing the input and output channels while
    the resolution multiplier influences the size of the input image and its representations
    throughout the network. MobileNet achieves comparable accuracy to some full-fledged
    models while being a fraction of their size. Howard et al. also showed how it
    could generalize over various applications like face attribution, geolocalization
    and object detection. However, it was too simple and linear like VGG and therefore
    had fewer avenues for gradient flow. These were fixed in later iterations of this
    model [sandler_mobilenetv2_2019, howard_searching_2019].'
  prefs: []
  type: TYPE_NORMAL
- en: VI-C ShuffleNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In 2017, Zhang et al. introduced ShuffleNet [zhang_shufflenet_2018], an extremely
    computationally efficient neural network architecture, specifically designed for
    mobile devices. They recognized that many efficient networks become less effective
    as they scale down and purported it to be caused by expensive 1x1 convolutions.
    In conjunction with channel shuffle, they proposed the use of group convolution
    to circumvent its drawback of limited information flow. ShuffleNet consists mainly
    of a standard convolution followed by stacks of ShuffleNet units grouped in three
    stages. The ShuffleNet unit is similar to the ResNet block where they use depthwise
    convolution in the 3x3 layer and replace the 1x1 layer with pointwise group convolution.
    The depthwise convolution layer is preceded by a channel shuffle operation. The
    computation cost of the ShuffleNet can be administered by two hyperparameters:
    group number to control the connection sparsity and scaling factor to manipulate
    the model size. As group numbers become large, the error rate saturates as the
    input channels to each group decreases and therefore may reduce the representational
    capabilities. ShuffleNet outperformed contemporary models ([NIPS2012_c399862d,
    szegedy_going_2014, iandola_squeezenet_2016, howard_mobilenets_2017]) while having
    considerably smaller size. As the only advancement in ShuffleNet was channel shuffle,
    there isn’t any improvement in inference speed of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-D MobileNetv2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Improving on MobileNetv1 [howard_mobilenets_2017], Sandler et al. proposed MobileNetv2
    [sandler_mobilenetv2_2019] in 2018\. It introduced the inverted residual with
    linear bottleneck, a novel layer module to reduce computation and improve accuracy.
    The module expands a low-dimensional representation of the input into high dimension,
    filters with a depthwise convolution and then projects it back to low dimension,
    unlike the common residual block which performs compression, convolution and then
    expansion operations. The MobileNetv2 contains a convolution layer followed by
    19 residual bottleneck modules and subsequently two convolutional layers. The
    residual bottleneck module has a shortcut connection only when the stride is 1\.
    For higher stride, the shortcut is not used because of the difference in dimensions.
    They also employed ReLU6 as the non-linearity function, instead of simple ReLU,
    to limit computations. For object detection, the authors used MobileNetv2 as the
    feature extractor of a computationally efficient variant of the SSD [liu_ssd_2016].
    This model, called SSDLite, claimed to have 8x fewer parameters than the original
    SSD while achieving competitive accuracy. It generalizes well over on other datasets,
    is easy to implement and hence, was well-received by the community.
  prefs: []
  type: TYPE_NORMAL
- en: VI-E PeleeNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing lightweight deep learning models like [howard_mobilenets_2017, zhang_shufflenet_2018,
    sandler_mobilenetv2_2019] relied heavily on depthwise separable convolution, which
    lacked efficient implementation. Wang et al. proposed a novel efficient architecture
    based on conventional convolution, named PeleeNet [wang_pelee_2018], using an
    assortment of computation conserving techniques. PeleeNet was centered around
    the DenseNet [huang_densely_2018] but looked at many other models for inspiration.
    It introduced two-way dense layers, stem block, dynamic number of channels in
    a bottleneck, transition layer compression and conventional post activation to
    reduce computation cost and increase speed. Inspired from [szegedy_going_2014],
    the two-way dense layer helps in getting different scales of the receptive field,
    making it easier to identify larger objects. To reduce information loss, a stem
    block was used in the same way to [szegedy_inception-v4_2016, shen_dsod_2018].
    They also parted way with the compression factor used in [huang_densely_2018]
    as it hurts the feature expression and reduces accuracy. PeleeNet consists of
    a stem block, four stages of modified dense and transition layers, and ultimately
    the classification layer. The authors also proposed a real-time object detection
    system, called Pelee, which was based on PeleeNet and a variant of SSD [liu_ssd_2016].
    Its performance against the contemporary object detectors on mobile and edge devices
    was incremental but showed how simple design choices can make a huge difference
    in overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: VI-F ShuffleNetv2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2018, Ningning Ma et al. present a set of comprehensive guidelines for designing
    efficient network architectures in ShuffleNetv2 [ma_shufflenet_2018]. They argued
    for the use of direct metrics like speed or latency to measure computational complexity,
    instead of indirect metrics like FLOPs. ShuffleNetv2 is built on four guiding
    principles – 1) equal width for input and output channels to minimize memory access
    cost, 2) carefully choosing group convolution based on the target platform and
    task, 3) multi-path structures achieve higher accuracy at the cost of efficiency
    and 4) element-wise operations like add and ReLU are computationally non-negligible.
    Following the above principles, they designed a new building block. It split the
    input into two parts by a channel split layer, followed by three convolutional
    layers which are then concatenated with the residual connection and passed through
    a channel shuffle layer. For the downsampling model, channel split is removed
    and residual connection has depthwise separable convolution layers. An ensemble
    of these blocks slotted in between a couple of convolutional layers results in
    ShuffleNetv2\. The authors also experimented with larger models (50/162 layers)
    and obtained superior accuracy with considerably fewer FLOPs. ShuffleNetv2 punched
    above its weight and outperformed other state-of-the-art models at comparable
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: VI-G MnasNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the increasing need for accurate, fast and low latency models for various
    edge devices, designing such a neural network is becoming more challenging than
    ever. In 2018, Tan et al. proposed Mnasnet [tan_mnasnet_2018] designed from an
    automated neural architecture search (NAS) approach. They formulate the search
    problem as multi-object optimization aimed at both high accuracy and low latency.
    It also factorized the search space by partitioning the CNN into unique blocks
    and subsequently searching for operations and connections in those blocks separately,
    thereby reducing the search space. This also allowed each block to have a distinctive
    design, unlike the earlier models [zoph_neural_2016, liu_progressive_2017, real_regularized_2019]
    which stacked the same blocks. The authors used RNN-based reinforcement learning
    agent as controller along with a trainer to measure accuracy and mobile devices
    for latency. Each sampled model is trained on a task to get its accuracy and run
    on the real devices for latency. This is used to achieve a soft reward target
    and the controller is updated. The process is repeated until the maximum iterations
    or a suitable candidate is derived. It is composed of 16 diverse blocks, some
    with residual connections. MnasNet was almost twice as fast as MobileNetv2 while
    having higher accuracy. However, like other reinforcement learning based neural
    architecture search models, the search time of MnasNet requires astronomical computational
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: VI-H MobileNetv3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the heart of MobileNetv3 [howard_searching_2019] is the same method used
    to create MnasNet [tan_mnasnet_2018] with some modifications. A platform aware
    automated neural architecture search is performed in a factorized hierarchical
    search space and consequently optimized by NetAdapt [yang_netadapt_2018], which
    removes the underutilized components of the network in multiple iterations. Once
    an architecture proposal is obtained, it trims the channels, randomly initialize
    the weights and then fine-tunes it to improve the target metrics. The model was
    further modified to remove some expensive layer in the architecture and gain additional
    latency improvement. Howard et al. argued that the filters in the architecture
    are often mirrored images of each other, and that accuracy can be maintained even
    after dropping half of these filters. Using this technique reduced the computations.
    MobileNetv3 used a blend of ReLU and hard swish as activation filters, the latter
    is mostly employed towards the end of the model. Hard swish has no noticeable
    difference from the swish function but is computationally cheaper while retaining
    the accuracy. For different resource use cases, [howard_searching_2019] introduced
    two models – MobileNetv3-Large and MobileNetv3-Small. MobileNetv3-Large is composed
    of 15 bottleneck blocks while MobileNetv3-Small has 11\. It also included squeeze
    and excitation layer [hu_squeeze-and-excitation_2019] on its building blocks.
    Similar to [sandler_mobilenetv2_2019], these model act as a feature detector in
    SSDLite and is 35% faster than earlier iterations [tan_mnasnet_2018, sandler_mobilenetv2_2019],
    whilst achieving higher mAP.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Performance comparison of various object detectors on MS COCO and
    PASCAL VOC 2012 datasets at similar input image size.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Backbone | Size | AP[[0.5:0.95]] | AP[0.5] | FPS |'
  prefs: []
  type: TYPE_TB
- en: '| R-CNN* | 2014 | AlexNet | 224 | - | 58.50% | $\sim$0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| SPP-Net* | 2015 | ZF-5 | Variable | - | 59.20% | $\sim$0.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Fast R-CNN* | 2015 | VGG-16 | Variable | - | 65.70% | $\sim$0.43 |'
  prefs: []
  type: TYPE_TB
- en: '| Faster R-CNN* | 2016 | VGG-16 | 600 | - | 67.00% | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| R-FCN | 2016 | ResNet-101 | 600 | 31.50% | 53.20% | $\sim$3 |'
  prefs: []
  type: TYPE_TB
- en: '| FPN | 2017 | ResNet-101 | 800 | 36.20% | 59.10% | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| Mask R-CNN | 2018 | ResNeXt-101-FPN | 800 | 39.80% | 62.30% | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| DetectoRS | 2020 | ResNeXt-101 | 1333 | 53.30% | 71.60% | $\sim$4 |'
  prefs: []
  type: TYPE_TB
- en: '| YOLO* | 2015 | (Modified) GoogLeNet | 448 | - | 57.90% | 45 |'
  prefs: []
  type: TYPE_TB
- en: '| SSD | 2016 | VGG-16 | 300 | 23.20% | 41.20% | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv2 | 2016 | DarkNet-19 | 352 | 21.60% | 44.00% | 81 |'
  prefs: []
  type: TYPE_TB
- en: '| RetinaNet | 2018 | ResNet-101-FPN | 400 | 31.90% | 49.50% | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv3 | 2018 | DarkNet-53 | 320 | 28.20% | 51.50% | 45 |'
  prefs: []
  type: TYPE_TB
- en: '| CenterNet | 2019 | Hourglass-104 | 512 | 42.10% | 61.10% | 7.8 |'
  prefs: []
  type: TYPE_TB
- en: '| EfficientDet-D2 | 2020 | Efficient-B2 | 768 | 43.00% | 62.30% | 41.7 |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv4 | 2020 | CSPDarkNet-53 | 512 | 43.00% | 64.90% | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| Swin-L | 2021 | HTC++ | - | 57.70% | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ^aModels marked with * are compared on PASCAL VOC 2012, while others on MS
    COCO.Rows colored gray are real-time detectors ($>$30 FPS). |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Comparison of Lightweight models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Year | Top-1 Acc% | Latency (ms) | Parameters (Million) | FLOPs (Million)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SqueezeNet | 2016 | 60.5 | - | 3.2 | 833 |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNet | 2017 | 70.6 | 113 | 4.2 | 569 |'
  prefs: []
  type: TYPE_TB
- en: '| ShuffleNet | 2017 | 73.3 | 108 | 5.4 | 524 |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNetv2 | 2018 | 74.7 | 143 | 6.9 | 300 |'
  prefs: []
  type: TYPE_TB
- en: '| PeleeNet | 2018 | 72.6 | - | 2.8 | 508 |'
  prefs: []
  type: TYPE_TB
- en: '| ShuffleNetv2 | 2018 | 75.4 | 178 | 7.4 | 597 |'
  prefs: []
  type: TYPE_TB
- en: '| MnasNet | 2018 | 76.7 | 103 | 5.2 | 403 |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNetv3 | 2019 | 75.2 | 58 | 5.4 | 219 |'
  prefs: []
  type: TYPE_TB
- en: '| OFA | 2020 | 80.0 | 58 | 7.7 | 595 |'
  prefs: []
  type: TYPE_TB
- en: VI-I Once-For-All (OFA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The use of neural architecture search (NAS) for architecture design has produced
    state-of-the-art models in the past few years, however, they are compute expensive
    because of the sampled model training. Cai et al. in [cai2020onceforall] proposed
    a novel method of decoupling model training stage and the neural architecture
    search stage. The model is trained only once and sub-networks can be distilled
    from it as per the requirements. Once-for-all (OFA) network provides flexibility
    for such sub-networks in four important dimension of a convolutional neural network
    – depth, width, kernel size and dimension. As they are nested within the OFA network
    and interfere with the training, progressive shrinking was introduced. First,
    the largest network is trained with all parameters set to maximum. Subsequently,
    network is fine-tuned by gradually reducing the parameter dimensions like kernel
    size, depth and width. For elastic kernel, a center of the large kernel is used
    as the small kernel. As the center is shared, a kernel transformation matrix is
    used to maintain performance. To vary depth, the first few layers are used and
    the rest are skipped from the large network. Elastic width employs a channel sorting
    operation to reorganize channels and uses the most important ones in smaller models.
    OFA achieved state-of-the-art of 80% in ImageNet top-1 accuracy percentage and
    also won the 4^(th) Low Power Computer Vision Challenge (LPCVC) while reducing
    many order of magnitude of GPU training hours. It shows a new paradigm of designing
    lightweight models for a variety of hardware requirements.
  prefs: []
  type: TYPE_NORMAL
- en: VII Comparative Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We compare the performance of both single and two stage detectors on PASCAL
    VOC 2012 [everingham_pascal_2012] and Microsoft COCO [Lin_ms_coco_2014] datasets.
    Performance of object detectors is influenced by a number of factors like input
    image size and scale, feature extractor, GPU architecture, number of proposals,
    training methodology, loss function etc., which makes it difficult to compare
    various models without a common benchmark environment. Here in table [III](#S6.T3
    "TABLE III ‣ VI-H MobileNetv3 ‣ VI Lightweight Networks ‣ A Survey of Modern Deep
    Learning based Object Detection Models"), we evaluate performance of models based
    on the results from their papers. Models are compared on average precision (AP)
    and processed frames per second (FPS) at inference time. AP[0.5] is the average
    precision of all classes when predicted bounding box has an IoU $>$ 0.5 with ground
    truth. COCO dataset introduced another performance metric AP[[0.5:0.95]], or simply
    AP, which is the average AP for IoU from 0.5 to 0.95 in step size of 0.5\. We
    intentionally compare the performances of detectors on similarly size input image,
    where possible, to provide a reasonable account, as authors often introduce an
    array of models to provide flexibility between accuracy and inference time. In
    fig. LABEL:fig:chart, we use only the state-of-the-art model from the possible
    array of object detector family of models. Lightweight models are compared in
    table [IV](#S6.T4 "TABLE IV ‣ VI-H MobileNetv3 ‣ VI Lightweight Networks ‣ A Survey
    of Modern Deep Learning based Object Detection Models") where we compare them
    on ImageNet Top-1 classification accuracy, latency, number of parameters and complexity
    in MFLOPs. Models with MFLOPs lesser than 600 are expected to perform adequately
    on mobile devices.
  prefs: []
  type: TYPE_NORMAL
