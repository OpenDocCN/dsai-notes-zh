- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:52:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:52:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2107.06912] From Show to Tell: A Survey on Deep Learning-based Image Captioning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2107.06912] 从展示到叙述：关于基于深度学习的图像描述的调研'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.06912](https://ar5iv.labs.arxiv.org/html/2107.06912)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2107.06912](https://ar5iv.labs.arxiv.org/html/2107.06912)
- en: 'From Show to Tell: A Survey on'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从展示到叙述：关于
- en: Deep Learning-based Image Captioning
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的图像描述
- en: Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Matteo Stefanini、Marcella Cornia、Lorenzo Baraldi、Silvia Cascianelli
- en: Giuseppe Fiameni, and Rita Cucchiara M. Stefanini, M. Cornia, L. Baraldi, S.
    Cascianelli, and R. Cucchiara are with the Department of Engineering “Enzo Ferrari”,
    University of Modena and Reggio Emilia, Modena, Italy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Giuseppe Fiameni 和 Rita Cucchiara、M. Stefanini、M. Cornia、L. Baraldi、S. Cascianelli
    以及 R. Cucchiara 现为意大利摩德纳大学“恩佐·法拉利”工程系的成员。
- en: 'E-mail: {matteo.stefanini, marcella.cornia, lorenzo.baraldi, silvia.cascianelli,
    rita.cucchiara}@unimore.it. G. Fiameni is with NVIDIA AI Technology Centre, Italy.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：{matteo.stefanini, marcella.cornia, lorenzo.baraldi, silvia.cascianelli,
    rita.cucchiara}@unimore.it。G. Fiameni 现为 NVIDIA AI 技术中心的成员，意大利。
- en: 'E-mail: gfiameni@nvidia.com'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：gfiameni@nvidia.com
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Connecting Vision and Language plays an essential role in Generative Intelligence.
    For this reason, large research efforts have been devoted to image captioning,
    *i.e.* describing images with syntactically and semantically meaningful sentences.
    Starting from 2015 the task has generally been addressed with pipelines composed
    of a visual encoder and a language model for text generation. During these years,
    both components have evolved considerably through the exploitation of object regions,
    attributes, the introduction of multi-modal connections, fully-attentive approaches,
    and BERT-like early-fusion strategies. However, regardless of the impressive results,
    research in image captioning has not reached a conclusive answer yet. This work
    aims at providing a comprehensive overview of image captioning approaches, from
    visual encoding and text generation to training strategies, datasets, and evaluation
    metrics. In this respect, we quantitatively compare many relevant state-of-the-art
    approaches to identify the most impactful technical innovations in architectures
    and training strategies. Moreover, many variants of the problem and its open challenges
    are discussed. The final goal of this work is to serve as a tool for understanding
    the existing literature and highlighting the future directions for a research
    area where Computer Vision and Natural Language Processing can find an optimal
    synergy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 连接视觉和语言在生成智能中发挥着至关重要的作用。因此，许多研究工作已经投入到图像描述中，*即* 用语法和语义上有意义的句子描述图像。从 2015 年开始，这一任务通常通过由视觉编码器和用于文本生成的语言模型组成的管道来处理。在这些年里，通过利用物体区域、属性、引入多模态连接、完全注意力方法以及类似
    BERT 的早期融合策略，这两个组件都发生了显著演变。然而，尽管取得了令人印象深刻的结果，图像描述领域的研究尚未得出最终结论。本研究旨在提供关于图像描述方法的全面概述，从视觉编码和文本生成到训练策略、数据集和评估指标。在这方面，我们定量比较了许多相关的最先进方法，以识别架构和训练策略中的最具影响力的技术创新。此外，还讨论了问题的许多变体及其开放挑战。本研究的最终目标是作为理解现有文献的工具，并突出计算机视觉和自然语言处理可以找到最佳协同效应的研究领域的未来方向。
- en: 'Index Terms:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Image Captioning, Vision-and-Language, Deep Learning, Survey.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述，视觉与语言，深度学习，调研。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Image captioning is the task of describing the visual content of an image in
    natural language, employing a visual understanding system and a language model
    capable of generating meaningful and syntactically correct sentences. Neuroscience
    research has clarified the link between human vision and language generation only
    in the last few years [[1](#bib.bib1)]. Similarly, in Artificial Intelligence,
    the design of architectures capable of processing images and generating language
    is a very recent matter. The goal of these research efforts is to find the most
    effective pipeline to process an input image, represent its content, and transform
    that into a sequence of words by generating connections between visual and textual
    elements while maintaining the fluency of language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像标题生成是描述图像视觉内容的任务，使用视觉理解系统和能够生成有意义且语法正确句子的语言模型。神经科学研究在最近几年才阐明了人类视觉与语言生成之间的联系[[1](#bib.bib1)]。同样地，在人工智能领域，设计能够处理图像并生成语言的架构也是非常新的事物。这些研究工作的目标是找到处理输入图像、表示其内容并将其转换为一系列单词的最有效流程，通过在视觉和文本元素之间生成连接，同时保持语言的流畅性。
- en: The early-proposed approaches to image captioning have entailed description
    retrieval [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7)] or template filling and hand-crafted natural language generation
    techniques [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)]. While
    these have been treated in other surveys [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18)], image captioning is currently based on the usage of deep learning-based
    generative models. In its standard configuration, the task is an image-to-sequence
    problem whose inputs are pixels. These inputs are encoded as one or multiple feature
    vectors in the visual encoding step, which prepares the input for a second generative
    step, called the language model. This produces a sequence of words or sub-words
    decoded according to a given vocabulary.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 早期提出的图像标题生成方法包括描述检索[[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)]或模板填充和手工制作的自然语言生成技术[[8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15)]。虽然这些方法已在其他综述中讨论过[[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]，但目前图像标题生成基于深度学习生成模型的使用。在其标准配置下，该任务是一个图像到序列的问题，其输入是像素。这些输入在视觉编码步骤中被编码为一个或多个特征向量，为第二个生成步骤——语言模型——做准备。该模型生成的单词或子词序列根据给定的词汇表进行解码。
- en: 'In these few years, the research community has improved model design considerably:
    from the first deep learning-based proposals adopting Recurrent Neural Networks
    (RNNs) fed with global image descriptors, methods have been enriched with attentive
    approaches and reinforcement learning up to the breakthroughs of Transformers
    and self-attention and single-stream BERT-like approaches. At the same time, the
    Computer Vision and Natural Language Processing (NLP) communities have addressed
    the challenge of building proper evaluation protocols and metrics to compare results
    with human-generated ground-truths. However, despite the investigation and improvements
    achieved in these years, image captioning is still far from being considered a
    solved task.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这几年里，研究社区在模型设计上有了显著改进：从最初的基于深度学习的提议采用全局图像描述符的递归神经网络（RNNs），方法不断丰富，增加了注意力机制和强化学习，直到突破性的变压器和自注意力以及单流BERT类似方法。同时，计算机视觉和自然语言处理（NLP）社区也解决了构建适当评估协议和指标的挑战，以便将结果与人工生成的基准进行比较。然而，尽管在这些年中取得了调查和改进，图像标题生成仍远未被视为一个解决的任务。
- en: Several domain-specific proposals and variants of the task have also been investigated
    to accommodate for different user needs and descriptions styles. According to [[19](#bib.bib19),
    [20](#bib.bib20)], indeed, image captions can be perceptual, when focusing on
    low-level visual attributes; non-visual, when reporting implicit and contextual
    information; conceptual, when describing the actual visual content (*e.g.* visual
    entities and their relations). While the latter is commonly recognized as the
    target of the image captioning task, this definition encompasses descriptions
    focusing on different aspects and at various levels of detail (*e.g.* including
    attributes or not, mentioning named entities or high-level concepts only, describing
    salient parts only, or also finer details).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特定领域的提议和任务变体也得到了研究，以满足不同用户需求和描述风格。根据[[19](#bib.bib19), [20](#bib.bib20)]，图像描述可以是感知性的，当关注于低级视觉属性时；非视觉的，当报告隐含和上下文信息时；概念性的，当描述实际视觉内容时（*例如*
    视觉实体及其关系）。虽然后者通常被认为是图像描述任务的目标，但该定义涵盖了关注不同方面和不同细节水平的描述（*例如* 包括属性或不包括属性，仅提及命名实体或仅提及高层次概念，仅描述显著部分，或包括更细致的细节）。
- en: With the aim of providing a testament to the journey that captioning has taken
    so far, and with that of encouraging novel ideas, we trace a holistic overview
    of techniques, models, and task variants developed in the last years. Furthermore,
    we review datasets and evaluation metrics and perform quantitative comparisons
    of the main approaches. Finally, we discuss open challenges and future directions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示描述技术的发展历程，并鼓励新的想法，我们追踪了近年来开发的技术、模型和任务变体的整体概述。此外，我们回顾了数据集和评价指标，并对主要方法进行了定量比较。最后，我们讨论了开放的挑战和未来方向。
- en: 'Contributions. To sum up, the contributions of this survey are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献。总之，本次调查的贡献如下：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Following the inherent dual nature of captioning models, we develop taxonomies
    for visual encoding and language modeling approaches and describe their key aspects
    and limitations.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据描述模型固有的双重特性，我们为视觉编码和语言建模方法制定了分类法，并描述了它们的关键方面和局限性。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review the training strategies adopted in the literature over the past years
    and the recent advancement obtained by the pre-training paradigm and masked language
    model losses.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了过去几年文献中采用的训练策略，以及通过预训练范式和掩蔽语言模型损失获得的最新进展。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review the main datasets used to explore image captioning, both domain-generic
    benchmarks and domain-specific datasets collected to investigate specific aspects.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了用于探索图像描述的主要数据集，包括领域通用基准和为调查特定方面而收集的领域特定数据集。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyze both standard and non-standard metrics adopted for performance evaluation
    and the characteristics of the caption they highlight.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了用于性能评估的标准和非标准指标及其所突出的描述特点。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present a quantitative comparison of the main image captioning methods considering
    both standard and non-standard metrics and a discussion on their relationships,
    which sheds light on performance, differences, and characteristics of the most
    important models.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对主要图像描述方法进行了定量比较，考虑了标准和非标准指标，并讨论了它们之间的关系，这有助于揭示最重要模型的性能、差异和特点。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We give an overview of many variants of the task and discuss open challenges
    and future directions.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们概述了任务的多种变体，并讨论了开放的挑战和未来的方向。
- en: Compared to previous surveys on image captioning [[18](#bib.bib18), [17](#bib.bib17),
    [21](#bib.bib21), [22](#bib.bib22), [16](#bib.bib16)], we provide a comprehensive
    and updated view on deep learning-based generative captioning models. We perform
    a deeper analysis of proposed approaches and survey a considerably larger number
    of papers on the topic. Also, we cover non-standard evaluation metrics, which
    are disregarded by other works, discuss their characteristics, and employ them
    in a quantitative evaluation of state-of-the-art methods. Moreover, we tackle
    emerging variants of the task and a broader set of available datasets.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于之前关于图像描述的调查[[18](#bib.bib18), [17](#bib.bib17), [21](#bib.bib21), [22](#bib.bib22),
    [16](#bib.bib16)]，我们提供了一个全面且更新的关于基于深度学习的生成描述模型的观点。我们对提出的方法进行了更深入的分析，并调查了更多的相关论文。此外，我们涵盖了其他研究忽视的非标准评价指标，讨论了它们的特点，并将其用于对最先进方法的定量评估。此外，我们还处理了任务的最新变体和更广泛的可用数据集。
- en: '![Refer to caption](img/1182d4c98cd348d893bd6e069d54b076.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1182d4c98cd348d893bd6e069d54b076.png)'
- en: 'Figure 1: Overview of the image captioning task and taxonomy of the most relevant
    approaches.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 图像描述任务概述及最相关方法的分类。'
- en: 2 Visual Encoding
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 视觉编码
- en: 'Providing an effective representation of the visual content is the first challenge
    of an image captioning pipeline. The current approaches for visual encoding can
    be classified as belonging to four main categories: 1. *non-attentive methods*
    based on global CNN features; 2. *additive attentive methods* that embed the visual
    content using either grids or regions; 3. *graph-based methods* adding visual
    relationships between visual regions; and 4. *self-attentive methods* that employ
    Transformer-based paradigms, either by using region-based, patch-based, or image-text
    early fusion solutions. This taxonomy is visually summarized in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '提供有效的视觉内容表示是图像描述生成流程中的首要挑战。当前的视觉编码方法可以归类为四大类：1. 基于全局CNN特征的*非注意力方法*；2. 通过网格或区域嵌入视觉内容的*加性注意力方法*；3.
    增加视觉区域间关系的*基于图的方法*；4. 使用基于Transformer的范式的*自注意力方法*，包括区域基础、补丁基础或图像-文本早期融合解决方案。这种分类在图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")中以图示方式总结。'
- en: '![Refer to caption](img/cd2286ba240977f732fa99f8740ac3e2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cd2286ba240977f732fa99f8740ac3e2.png)'
- en: (a)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/9da0e966afb4cc81b5ec8fdbae0b3400.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9da0e966afb4cc81b5ec8fdbae0b3400.png)'
- en: (b)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/1e77237cc8b4da05134aaa60133d5965.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1e77237cc8b4da05134aaa60133d5965.png)'
- en: (c)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 2: Three of the most relevant visual encoding strategies for image captioning:
    (a) global CNN features; (b) fine-grained features extracted from the activation
    of a convolutional layer, together with an attention mechanism guided by the language
    model; (c) image region features coming from a detector, together with an attention
    mechanism.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 三种与图像描述生成最相关的视觉编码策略：(a) 全局CNN特征；(b) 从卷积层激活中提取的细粒度特征，结合由语言模型引导的注意力机制；(c)
    来自检测器的图像区域特征，结合注意力机制。'
- en: 2.1 Global CNN Features
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 全局CNN特征
- en: 'With the advent of CNNs, all models consuming visual inputs have been improved
    in terms of performance. The visual encoding step of image captioning is no exception.
    In the most simple recipe, the activation of one of the last layers of a CNN is
    employed to extract high-level representations, which are then used as a conditioning
    element for the language model (Fig. [2(a)](#S2.F2.sf1 "In Figure 2 ‣ 2 Visual
    Encoding ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")).
    This is the approach employed in the seminal “Show and Tell” paper [[23](#bib.bib23)]¹¹1The
    title of this survey is a tribute of this pioneering work., where the output of
    GoogleNet [[24](#bib.bib24)] is fed to the initial hidden state of the language
    model. In the same year, Karpathy *et al.* [[25](#bib.bib25)] used global features
    extracted from AlexNet [[26](#bib.bib26)] as the input for a language model. Further,
    Mao *et al.* [[27](#bib.bib27)] and Donahue *et al.* [[28](#bib.bib28)] injected
    global features extracted from the VGG network [[29](#bib.bib29)] at each time-step
    of the language model.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '随着CNN的出现，所有处理视觉输入的模型在性能上都有所提升。图像描述生成的视觉编码步骤也不例外。在最简单的方案中，使用CNN的最后几层的激活来提取高级表示，然后将其用作语言模型的条件元素（图 [2(a)](#S2.F2.sf1
    "In Figure 2 ‣ 2 Visual Encoding ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")）。这是在开创性论文“Show and Tell”中使用的方法 [[23](#bib.bib23)]¹¹1该调查的标题是对这一开创性工作的致敬。中，GoogleNet [[24](#bib.bib24)]的输出被送入语言模型的初始隐藏状态。同年，Karpathy *et
    al.* [[25](#bib.bib25)]使用从AlexNet [[26](#bib.bib26)]提取的全局特征作为语言模型的输入。此外，Mao *et
    al.* [[27](#bib.bib27)]和Donahue *et al.* [[28](#bib.bib28)]在语言模型的每个时间步都注入了从VGG网络 [[29](#bib.bib29)]提取的全局特征。'
- en: Global CNN features were then employed in a large variety of image captioning
    models [[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]. Notably,
    Rennie *et al.* [[38](#bib.bib38)] introduced the FC model, in which images are
    encoded using a ResNet-101 [[39](#bib.bib39)], preserving their original dimensions.
    Other approaches [[40](#bib.bib40), [41](#bib.bib41)] integrated high-level attributes
    or tags, represented as a probability distribution over the most common words
    of the training captions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 全局CNN特征随后被应用于各种图像描述模型 [[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]。特别是，Rennie
    *et al.* [[38](#bib.bib38)] 引入了FC模型，其中图像使用ResNet-101 [[39](#bib.bib39)] 进行编码，保持其原始维度。其他方法
    [[40](#bib.bib40), [41](#bib.bib41)] 结合了高级属性或标签，这些标签表示为训练描述中最常见单词的概率分布。
- en: The main advantage of employing global CNN features resides in their simplicity
    and compactness of representation, which embraces the capacity to extract and
    condense information from the whole input and to consider the overall context
    of an image. However, this paradigm also leads to excessive compression of information
    and lacks granularity, making it hard for a captioning model to produce specific
    and fine-grained descriptions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用全局CNN特征的主要优点在于其简单性和紧凑性，这种表示方式能够从整个输入中提取和浓缩信息，并考虑图像的整体背景。然而，这种范式也导致了信息的过度压缩，缺乏粒度，使得描述模型难以生成具体而精细的描述。
- en: 2.2 Attention Over Grid of CNN Features
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 CNN特征网格上的注意力
- en: 'Motivated by the drawbacks of global representations, most of the following
    approaches have increased the granularity level of visual encoding [[42](#bib.bib42),
    [38](#bib.bib38), [43](#bib.bib43)]. For instance, Dai *et al.*[[44](#bib.bib44)]
    have employed 2D activation maps in place of 1D global feature vectors to bring
    spatial structure directly in the language model. Drawing from machine translation
    literature, a big portion of the captioning community has instead employed the
    additive attention mechanism (Fig. [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2 Visual Encoding
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")), which
    has endowed image captioning architectures with time-varying visual features encoding,
    enabling greater flexibility and finer granularity.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 受到全局表示不足的启发，以下大多数方法增加了视觉编码的粒度水平 [[42](#bib.bib42), [38](#bib.bib38), [43](#bib.bib43)]。例如，Dai
    *et al.*[[44](#bib.bib44)] 使用2D激活图代替1D全局特征向量，将空间结构直接引入语言模型。从机器翻译文献中汲取灵感，大片的图像描述社区则采用了加性注意机制（图 [2(b)](#S2.F2.sf2
    "在图2 ‣ 2 视觉编码 ‣ 从展示到讲述：深度学习图像描述的调查")），这使得图像描述架构具有时间变化的视觉特征编码，从而实现了更大的灵活性和更精细的粒度。
- en: 'Definition of additive attention. The intuition behind attention boils down
    to weighted averaging. In the first formulation proposed for sequence alignment
    by Bahdanau *et al.* [[45](#bib.bib45)] (also known as additive attention), a
    single-layer feed-forward neural network with a hyperbolic tangent non-linearity
    is used to compute attention weights. Formally, given two generic sets of vectors
    $\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}$ and $\{\mathbf{h}_{1},\ldots,\mathbf{h}_{m}\}$,
    the additive attention score between $\mathbf{h}_{i}$ and $\mathbf{x}_{j}$ is
    computed as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 加性注意的定义。注意机制的直观理解是加权平均。Bahdanau *et al.* [[45](#bib.bib45)] 提出的序列对齐的首个公式（也称为加性注意），使用了一个具有双曲正切非线性的单层前馈神经网络来计算注意权重。正式地，给定两个通用的向量集
    $\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}$ 和 $\{\mathbf{h}_{1},\ldots,\mathbf{h}_{m}\}$，$\mathbf{h}_{i}$
    和 $\mathbf{x}_{j}$ 之间的加性注意得分计算如下：
- en: '|  | $f_{\mathrm{att}}\left(\mathbf{h}_{i},\mathbf{x}_{j}\right)=\mathbf{W}_{3}^{\top}\tanh\left(\mathbf{W}_{1}\mathbf{h}_{i}+\mathbf{W}_{2}\mathbf{x}_{j}\right),$
    |  | (1) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{\mathrm{att}}\left(\mathbf{h}_{i},\mathbf{x}_{j}\right)=\mathbf{W}_{3}^{\top}\tanh\left(\mathbf{W}_{1}\mathbf{h}_{i}+\mathbf{W}_{2}\mathbf{x}_{j}\right),$
    |  | (1) |'
- en: where $\mathbf{W}_{1}$ and $\mathbf{W}_{2}$ are weight matrices, and $\mathbf{W}_{3}$
    is a weight vector that performs a linear combination. A softmax function is then
    applied to obtain a probability distribution $p\left(\mathbf{x}_{j}\mid\mathbf{h}_{i}\right)$,
    representing how much the element encoded by $\mathbf{x}_{j}$ is relevant for
    $\mathbf{h}_{i}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}_{1}$ 和 $\mathbf{W}_{2}$ 是权重矩阵，$\mathbf{W}_{3}$ 是执行线性组合的权重向量。然后应用softmax函数来获得概率分布
    $p\left(\mathbf{x}_{j}\mid\mathbf{h}_{i}\right)$，表示 $\mathbf{x}_{j}$ 编码的元素对 $\mathbf{h}_{i}$
    的相关性。
- en: Although the attention mechanism was initially devised for modeling the relationships
    between two sequences of elements (*i.e.* hidden states from a recurrent encoder
    and a decoder), it can be adapted to connect a set of visual representations with
    the hidden states of a language model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力机制最初是为了建模两个元素序列之间的关系（*即* 来自递归编码器和解码器的隐藏状态），但它可以适应于将一组视觉表示与语言模型的隐藏状态连接起来。
- en: Attending convolutional activations. Xu *et al.* [[42](#bib.bib42)] introduced
    the first method leveraging the additive attention over the spatial output grid
    of a convolutional layer. This allows the model to selectively focus on certain
    elements of the grid by selecting a subset of features for each generated word.
    Specifically, the model first extracts the activation of the last convolutional
    layer of a VGG network [[29](#bib.bib29)], then uses additive attention to compute
    a weight for each grid element, interpreted as the relative importance of that
    element for generating the next word.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关注卷积激活。徐*等*[[42](#bib.bib42)] 介绍了首个利用加性注意力对卷积层的空间输出网格进行操作的方法。这使得模型能够通过为每个生成的单词选择特征子集，选择性地关注网格中的某些元素。具体而言，模型首先提取
    VGG 网络的最后一个卷积层的激活值[[29](#bib.bib29)]，然后使用加性注意力为每个网格元素计算一个权重，该权重被解释为该元素对生成下一个单词的相对重要性。
- en: Other approaches. The solution based on additive attention over a grid of features
    has been widely adopted by several following works with minor improvements in
    terms of visual encoding [[40](#bib.bib40), [46](#bib.bib46), [43](#bib.bib43),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法。基于网格特征的加性注意力的解决方案已被若干后续工作广泛采用，并在视觉编码方面做了些许改进[[40](#bib.bib40), [46](#bib.bib46),
    [43](#bib.bib43), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]。
- en: Review networks – For instance, Yang *et al.* [[50](#bib.bib50)] supplemented
    the encoder-decoder framework with a recurrent review network. This performs a
    given number of review steps with attention on the encoder hidden states and outputs
    a “thought vector” after each step, which is then used by the attention mechanism
    in the decoder.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 复习网络——例如，杨*等*[[50](#bib.bib50)] 在编码器-解码器框架中补充了一个递归复习网络。该网络执行指定数量的复习步骤，关注编码器隐藏状态，并在每一步后输出一个“思维向量”，该向量随后被解码器中的注意力机制使用。
- en: Multi-level features – Chen *et al.* [[51](#bib.bib51)] proposed to employ channel-wise
    attention over convolutional activations, followed by a more classical spatial
    attention. They also experimented with using more than one convolutional layer
    to exploit multi-level features. On the same line, Jiang *et al.* [[52](#bib.bib52)]
    proposed to use multiple CNNs in order to exploit their complementary information,
    then fused their representations with a recurrent procedure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 多层次特征——陈*等*[[51](#bib.bib51)] 提出了在卷积激活上使用通道级注意力，然后是更经典的空间注意力。他们还尝试使用多个卷积层以利用多层次特征。同样，姜*等*[[52](#bib.bib52)]
    提出了使用多个 CNN 以利用其互补信息，然后通过递归过程融合它们的表示。
- en: Exploiting human attention – Some works also integrated saliency information
    (*i.e.* what do humans pay more attention to in a scene) to guide caption generation
    with stimulus-based attention. This idea was first explored by Sugano and Bulling [[53](#bib.bib53)]
    who exploited human eye fixations for image captioning by including normalized
    fixation histograms over the image as an input to the soft-attention module of [[42](#bib.bib42)]
    and weighing the attended image regions based on whether these are fixated or
    not. Subsequent works on this line [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57)] employed saliency maps as a form of additional attention source.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 利用人类注意力——一些工作还集成了显著性信息（*即* 人类在场景中更关注什么），以通过基于刺激的注意力指导字幕生成。这个想法最早由 Sugano 和 Bulling
    [[53](#bib.bib53)] 探索，他们通过将标准化的注视直方图作为输入包含到[[42](#bib.bib42)]的软注意力模块中，并根据是否被注视来加权关注的图像区域。随后的研究[[54](#bib.bib54),
    [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57)] 将显著性图作为额外注意力源的一种形式。
- en: '![Refer to caption](img/ae2e209e6f5a8dd86a552acd439d465f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ae2e209e6f5a8dd86a552acd439d465f.png)'
- en: (a)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/0670cfe3f326af220e80d22fcb023052.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0670cfe3f326af220e80d22fcb023052.png)'
- en: (b)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 3: Summary of the two most recent visual encoding strategies for image
    captioning: (a) graph-based encoding of visual regions; (b) self-attention-based
    encoding over image region features.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：图像字幕生成的两种最新视觉编码策略总结：(a) 基于图的视觉区域编码；(b) 基于自注意力的图像区域特征编码。
- en: 2.3 Attention Over Visual Regions
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 对视觉区域的注意力
- en: The intuition of using saliency boils down to neuroscience, which suggests that
    our brain integrates a top-down reasoning process with a bottom-up flow of visual
    signals. The top-down path consists of predicting the upcoming sensory input by
    leveraging our knowledge and inductive bias, while the bottom-up flow provides
    visual stimuli adjusting the previous predictions. Additive attention can be thought
    of as a top-down system. In this mechanism, the language model predicts the next
    word while attending a feature grid, whose geometry is irrespective of the image
    content.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用显著性的直觉源于神经科学，这表明我们的脑部整合了自上而下的推理过程与自下而上的视觉信号流。自上而下的路径由利用我们的知识和归纳偏差预测即将到来的感觉输入组成，而自下而上的流动则提供调整先前预测的视觉刺激。附加注意力可以被视为一种自上而下的系统。在这种机制中，语言模型在关注特征网格的同时预测下一个词，而网格的几何形状与图像内容无关。
- en: 'Bottom-up and top-down attention. Differently from saliency-based approaches [[57](#bib.bib57)],
    in the solution proposed by Anderson *et al.* [[58](#bib.bib58)] the bottom-up
    path is defined by an object detector in charge of proposing image regions. This
    is then coupled with a top-down mechanism that learns to weigh each region for
    each word prediction (see Fig. [2(c)](#S2.F2.sf3 "In Figure 2 ‣ 2 Visual Encoding
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")). In this
    approach, Faster R-CNN [[59](#bib.bib59), [60](#bib.bib60)] is adopted to detect
    objects, obtaining a pooled feature vector for each region proposal. One of the
    key elements of this approach resides in its pre-training strategy, where an auxiliary
    training loss is added for learning to predict attribute classes alongside object
    classes on the Visual Genome [[61](#bib.bib61)] dataset. This allows the model
    to predict a dense and rich set of detections, including both salient object and
    contextual regions, and favors the learning of better feature representations.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 自下而上和自上而下的注意力。与基于显著性的方法[[57](#bib.bib57)]不同，Anderson *等人* [[58](#bib.bib58)]
    提出的解决方案通过一个物体检测器定义了自下而上的路径，该检测器负责提出图像区域。然后，这与一个自上而下的机制相结合，该机制学习为每个词预测加权每个区域（见图
    [2(c)](#S2.F2.sf3 "在图 2 ‣ 2 视觉编码 ‣ 从展示到讲述：深度学习图像字幕生成的调查")）。在这种方法中，采用 Faster R-CNN
    [[59](#bib.bib59), [60](#bib.bib60)] 来检测物体，为每个区域提议获得一个池化特征向量。该方法的一个关键要素在于其预训练策略，其中添加了辅助训练损失，用于在
    Visual Genome [[61](#bib.bib61)] 数据集上学习预测属性类别和物体类别。这使得模型能够预测密集且丰富的检测结果，包括显著物体和背景区域，并有助于学习更好的特征表示。
- en: Other approaches. Employing image region features has demonstrated its advantages
    when dealing with the raw visual input and has been the standard de-facto in image
    captioning for years. As a result, many of the following works have based the
    visual encoding phase on this strategy [[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65)]. Among them, we point out two remarkable variants.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法。采用图像区域特征在处理原始视觉输入时展示了其优势，多年来已成为图像字幕生成的事实标准。因此，许多后续工作都基于这种策略进行视觉编码阶段[[62](#bib.bib62),
    [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)]。在这些工作中，我们指出了两个显著的变体。
- en: Visual Policy – While typical visual attention points to a single image region
    at every step, the approach proposed by Zha *et al.* [[66](#bib.bib66)] introduces
    a sub-policy network that interprets also the visual part sequentially by encoding
    historical visual actions (*e.g.* previously attended regions) via an LSTM to
    serve as context for the next visual action.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉策略——虽然典型的视觉注意力在每一步指向单一的图像区域，但 Zha *等人* [[66](#bib.bib66)] 提出的办法引入了一个子策略网络，该网络还通过
    LSTM 顺序地编码历史视觉动作（*例如* 以前关注过的区域），以作为下一步视觉动作的上下文。
- en: Geometric Transforms – Pedersoli *et al.* [[67](#bib.bib67)] proposed to use
    spatial transformers for generating image-specific attention areas by regressing
    region proposals in a weakly-supervised fashion. Specifically, a localization
    network learns an affine transformation or each location of the feature map, and
    then a bilinear interpolation is used to regress a feature vector for each region
    with respect to anchor boxes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 几何变换 – Pedersoli *et al.* [[67](#bib.bib67)] 提议使用空间变换器，通过在弱监督的方式下回归区域提议来生成特定于图像的注意区域。具体而言，定位网络学习特征图每个位置的仿射变换，然后使用双线性插值来回归每个区域的特征向量，相对于锚框。
- en: 2.4 Graph-based Encoding
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 基于图的编码
- en: 'To further improve the encoding of image regions and their relationships, some
    studies consider using graphs built over image regions (see Fig. [3(a)](#S2.F3.sf1
    "In Figure 3 ‣ 2.2 Attention Over Grid of CNN Features ‣ 2 Visual Encoding ‣ From
    Show to Tell: A Survey on Deep Learning-based Image Captioning")) to enrich the
    representation by including semantic and spatial connections.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步改进图像区域及其关系的编码，一些研究考虑使用构建在图像区域上的图（见图[3(a)](#S2.F3.sf1 "在图3 ‣ 2.2 CNN特征网格上的注意
    ‣ 2 视觉编码 ‣ 从展示到讲述：基于深度学习的图像字幕生成调查")）来通过包含语义和空间连接来丰富表示。
- en: Spatial and semantic graphs. The first attempt in this sense is due to Yao *et
    al.* [[68](#bib.bib68)], followed by Guo *et al.* [[69](#bib.bib69)], who proposed
    the use of a graph convolutional network (GCN) [[70](#bib.bib70)] to integrate
    both semantic and spatial relationships between objects. The semantic relationships
    graph is obtained by applying a classifier pre-trained on Visual Genome [[61](#bib.bib61)]
    that predicts an action or an interaction between object pairs. The spatial relationships
    graph is instead inferred through geometry measures (*i.e.* intersection over
    union, relative distance, and angle) between bounding boxes of object pairs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 空间和语义图。在这方面的第一次尝试是由 Yao *et al.* [[68](#bib.bib68)] 进行的，随后 Guo *et al.* [[69](#bib.bib69)]
    提出了使用图卷积网络（GCN） [[70](#bib.bib70)] 来集成对象之间的语义和空间关系。语义关系图是通过应用在 Visual Genome [[61](#bib.bib61)]
    上预训练的分类器来预测对象对之间的动作或交互获得的。空间关系图则是通过对象对的边界框之间的几何度量（*即* 交并比、相对距离和角度）推断得到的。
- en: Scene graphs. With a focus on modeling semantic relations, Yang *et al.* [[71](#bib.bib71)]
    proposed to integrate semantic priors learned from text in the image encoding
    by exploiting a graph-based representation of both images and sentences. The representation
    used is the scene graph, *i.e.* a directed graph connecting the objects, their
    attributes, and their relations. On the same line, Shi *et al.* [[72](#bib.bib72)]
    represented the image as a semantic relationship graph but proposed to train the
    module in charge of predicting the predicate nodes directly on the ground-truth
    captions rather than on external datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 场景图。Yang *et al.* [[71](#bib.bib71)] 重点建模语义关系，提出通过利用图形表示来整合从文本中学到的语义先验，将其应用于图像编码。所使用的表示是场景图，*即*
    一个连接对象、属性及其关系的有向图。在同一方向上，Shi *et al.* [[72](#bib.bib72)] 将图像表示为一个语义关系图，但建议直接在真实的字幕上训练预测谓词节点的模块，而不是在外部数据集上。
- en: Hierarchical trees. As a special case of a graph-based encoding, Yao *et al.* [[73](#bib.bib73)]
    employed a tree to represent the image as a hierarchical structure. The root represents
    the image as a whole, intermediate nodes represent image regions and their contained
    sub-regions, and the leaves represent segmented objects in the regions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 层次树。作为基于图编码的一个特殊情况，Yao *et al.* [[73](#bib.bib73)] 使用树来将图像表示为层次结构。根表示整个图像，中间节点表示图像区域及其包含的子区域，叶子表示区域中的分割对象。
- en: Graph encodings brought a mechanism to leverage relationships between detected
    objects, which allows the exchange of information in adjacent nodes and thus in
    a local manner. Further, it seamlessly allows the integration of external semantic
    information. On the other hand, manually building the graph structure can limit
    the interactions between visual features. This is where self-attention proved
    to be more successful by connecting all the elements with each other in a complete
    graph representation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图编码引入了一种机制来利用检测到的对象之间的关系，这允许在相邻节点之间交换信息，从而实现局部信息传递。此外，它还无缝地允许集成外部语义信息。另一方面，手动构建图结构可能会限制视觉特征之间的交互。这时，自注意力机制通过将所有元素在完整图表示中连接在一起，证明了其更成功。
- en: 2.5 Self-Attention Encoding
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 自注意力编码
- en: 'Self-attention is an attentive mechanism where each element of a set is connected
    with all the others, and that can be adopted to compute a refined representation
    of the same set of elements through residual connections (Fig. [3(b)](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Attention Over Grid of CNN Features ‣ 2 Visual Encoding ‣ From
    Show to Tell: A Survey on Deep Learning-based Image Captioning")). It was first
    introduced by Vaswani *et al.* [[74](#bib.bib74)] for machine translation and
    language understanding tasks, giving birth to the Transformer architecture and
    its variants, which have dominated the NLP field and later also Computer Vision.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是一种关注机制，其中集合中的每个元素与所有其他元素连接，并可以通过残差连接（图 [3(b)](#S2.F3.sf2 "在图3 ‣ 2.2 CNN特征网格上的注意力
    ‣ 2 视觉编码 ‣ 从展示到讲述：深度学习图像描述的调查")）计算同一元素集合的精炼表示。它最早由Vaswani *等人* [[74](#bib.bib74)]
    引入用于机器翻译和语言理解任务，孕育了Transformer架构及其变体，这些变体主导了NLP领域，后来也在计算机视觉中占据了重要地位。
- en: 'Definition of self-attention. Formally, self-attention makes use of the scaled
    dot-product mechanism, *i.e.* a multiplicative attention operator that handles
    three sets of vectors: a set of $n_{q}$ query vectors $\bm{Q}$, a set of key vectors
    $\bm{K}$, and a set of value vectors $\bm{V}$, both containing $n_{k}$ elements.
    The operator takes a weighted sum of value vectors according to a similarity distribution
    between query and key vectors:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的定义。形式上，自注意力使用缩放点积机制，即一个处理三组向量的乘法注意力算子：一组 $n_{q}$ 查询向量 $\bm{Q}$，一组键向量 $\bm{K}$，和一组值向量
    $\bm{V}$，这两组都包含 $n_{k}$ 元素。该算子根据查询向量和键向量之间的相似性分布，对值向量进行加权求和：
- en: '|  | $\displaystyle\mathsf{Attention}(\bm{Q},\bm{K},\bm{V})=\operatorname{softmax}\left(\frac{\bm{Q}\bm{K}^{T}}{\sqrt{d_{k}}}\right)\bm{V},$
    |  | (2) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathsf{Attention}(\bm{Q},\bm{K},\bm{V})=\operatorname{softmax}\left(\frac{\bm{Q}\bm{K}^{T}}{\sqrt{d_{k}}}\right)\bm{V},$
    |  | (2) |'
- en: where $d_{k}$ is a scaling factor. In the case of self-attention, the three
    sets of vectors are obtained as linear projections of the same input set of elements.
    The success of the Transformer demonstrates that leveraging self-attention allows
    achieving superior performances compared to attentive RNNs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{k}$ 是一个缩放因子。在自注意力的情况下，这三组向量是通过对相同输入元素集合的线性投影获得的。Transformer的成功表明，利用自注意力可以获得比关注型RNN更优越的性能。
- en: Early self-attention approaches. Among the first image captioning models leveraging
    this approach, Yang *et al.* [[75](#bib.bib75)] used a self-attentive module to
    encode relationships between features coming from an object detector. Later, Li *et
    al.* [[76](#bib.bib76)] proposed a Transformer model with a visual encoder for
    the region features coupled with a semantic encoder that exploits knowledge from
    an external tagger. Both encoders are based on self-attention and feed-forward
    layers. Their output is then fused through a gating mechanism governing the propagation
    of visual and semantic information.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 早期自注意力方法。在利用这种方法的首批图像描述模型中，Yang *等人* [[75](#bib.bib75)] 使用了一个自注意力模块来编码来自目标检测器的特征之间的关系。后来，Li
    *等人* [[76](#bib.bib76)] 提出了一个Transformer模型，该模型具有一个用于区域特征的视觉编码器，以及一个利用外部标注器知识的语义编码器。这两个编码器都基于自注意力和前馈层。然后通过一个门控机制融合它们的输出，该机制控制视觉和语义信息的传播。
- en: Variants of the self-attention operator. Other works proposed variants or modifications
    of the self-attention operator tailored for image captioning [[77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力算子的变体。其他研究提出了针对图像描述任务量身定制的自注意力算子的变体或修改 [[77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)]。
- en: Geometry-aware encoding – Herdade *et al.* [[77](#bib.bib77)] introduced a modified
    version of self-attention that takes into account the spatial relationships between
    regions. In particular, an additional geometric weight is computed between object
    pairs and is used to scale the attention weights. On a similar line, Guo *et al.* [[78](#bib.bib78)]
    proposed a normalized and geometry-aware version of self-attention that makes
    use of the relative geometry relationships between input objects. Further, He *et
    al.* [[82](#bib.bib82)] introduced a spatial graph transformer, which considers
    different categories of spatial relationship between detections (*e.g.*, parent,
    neighbor, child) when performing attention.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Geometry-aware encoding – Herdade *等人* [[77](#bib.bib77)] 引入了一种修改版的自注意力，考虑了区域之间的空间关系。具体而言，在对象对之间计算额外的几何权重，并用于缩放注意力权重。类似地，Guo
    *等人* [[78](#bib.bib78)] 提出了一个标准化且几何感知的自注意力版本，利用输入对象之间的相对几何关系。此外，He *等人* [[82](#bib.bib82)]
    引入了一个空间图变换器，在执行注意力时考虑了检测之间的不同类别的空间关系（*例如*，父、邻居、子）。
- en: Attention on Attention – Huang *et al.* [[79](#bib.bib79)] proposed an extension
    of the attention operator in which the final attended information is weighted
    by a gate guided by the context. Specifically, the output of the self-attention
    is concatenated with the queries, then an information and a gate vector are computed
    and finally multiplied together. In their encoder, they employed this mechanism
    to refine the visual features. This method is then adopted by later models such
    as [[83](#bib.bib83)].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Attention on Attention – Huang *等人* [[79](#bib.bib79)] 提出了一个注意力操作的扩展，其中最终的注意信息由上下文引导的门加权。具体而言，自注意力的输出与查询连接，然后计算信息向量和门向量，最后将它们相乘。在他们的编码器中，他们使用这种机制来细化视觉特征。随后，类似的方法也被[[83](#bib.bib83)]等后续模型采用。
- en: X-Linear Attention – Pan *et al.* [[80](#bib.bib80)] proposed to use bilinear
    pooling techniques to strengthen the representative capacity of the output attended
    feature. Notably, this mechanism encodes the region-level features with higher-order
    interaction, leading to a set of enhanced region-level and image-level features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: X-Linear Attention – Pan *等人* [[80](#bib.bib80)] 提出了使用双线性池化技术来增强输出注意特征的代表能力。值得注意的是，这种机制通过高阶交互编码区域级特征，导致一组增强的区域级和图像级特征。
- en: Memory-augmented Attention – Cornia *et al.* [[81](#bib.bib81), [84](#bib.bib84)]
    proposed a Transformer-based architecture where the self-attention operator of
    each encoder layer is augmented with a set of memory vectors. Specifically, the
    set of keys and values is extended with additional “slots” learned during training,
    which can encode multi-level visual relationships.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Memory-augmented Attention – Cornia *等人* [[81](#bib.bib81), [84](#bib.bib84)]
    提出了一个基于Transformer的架构，其中每个编码层的自注意力操作通过一组记忆向量进行增强。具体来说，键和值的集合通过在训练过程中学习的额外“槽”进行扩展，从而能够编码多级视觉关系。
- en: Other self-attention-based approaches. Ji *et al.* [[85](#bib.bib85)] proposed
    to improve self-attention by adding to the sequence of feature vectors a global
    vector computed as their average. A global vector is computed for each layer,
    and the resulting global vectors are combined via an LSTM, thus obtaining an inter-layer
    representation. Luo *et al.* [[86](#bib.bib86)] proposed a hybrid approach that
    combines region and grid features to exploit their complementary advantages. Two
    self-attention modules are applied independently to each kind of features, and
    a cross-attention module locally fuses their interactions. On a different line,
    the architecture proposed by Liu *et al.* [[87](#bib.bib87)] is based on an attention
    module to align grid or detection features with visual words extracted from a
    concept extractor and to obtain semantic-grounded encodings.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其他基于自注意力的方法。Ji *等人* [[85](#bib.bib85)] 提出了通过在特征向量序列中添加一个作为其平均值计算的全局向量来改进自注意力。为每一层计算全局向量，并通过LSTM组合得到的全局向量，从而获得层间表示。Luo
    *等人* [[86](#bib.bib86)] 提出了一个混合方法，将区域特征和网格特征结合起来，以利用它们的互补优势。对每种特征应用两个独立的自注意力模块，并通过交叉注意力模块局部融合它们的交互。另一方面，Liu
    *等人* [[87](#bib.bib87)] 提出的架构基于一个注意力模块，将网格或检测特征与从概念提取器中提取的视觉词对齐，从而获得语义基础编码。
- en: '![Refer to caption](img/3a1d3d5d3d89e48cb53a7e4a4d0127f6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3a1d3d5d3d89e48cb53a7e4a4d0127f6.png)'
- en: 'Figure 4: Vision Transformer encoding. The image is split into fixed-size patches,
    linearly embedded, added to position embeddings, and fed to a standard Transformer
    encoder.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：视觉 Transformer 编码。图像被分割成固定大小的补丁，线性嵌入，添加位置嵌入，然后输入到标准 Transformer 编码器。
- en: 'Attention on grid features and patches. Other than applying the attention operator
    on detections, the role of grid features has been recently re-evaluated [[88](#bib.bib88)].
    For instance, the approach proposed by Zhang *et al.* [[89](#bib.bib89)] applies
    self-attention directly to grid features, incorporating their relative geometry
    relationships into self-attention computation. Transformer-like architectures
    can also be applied directly on image patches, thus excluding the usage of the
    convolutional operator [[90](#bib.bib90), [91](#bib.bib91)] (Fig. [4](#S2.F4 "Figure
    4 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual Encoding ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning")). On this line, Liu *et al.* [[92](#bib.bib92)]
    devised the first convolution-free architecture for image captioning. Specifically,
    a pre-trained Vision Transformer network (*i.e.* ViT [[90](#bib.bib90)]) is adopted
    as encoder, and a standard Transformer decoder is employed to generate captions.
    Interestingly, the same visual encoding approach has been adopted in CLIP [[93](#bib.bib93)]
    and SimVLM [[94](#bib.bib94)], with the difference that the visual encoder is
    trained from scratch on large-scale noisy data. CLIP-based features have then
    been used by subsequent captioning approaches [[95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97)].'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意网格特征和补丁。除了在检测上应用注意力算子外，网格特征的角色最近已被重新评估 [[88](#bib.bib88)]。例如，Zhang *et al.* [[89](#bib.bib89)]
    提出的办法将自注意力直接应用于网格特征，将它们的相对几何关系纳入自注意力计算中。类似 Transformer 的架构也可以直接应用于图像补丁，从而排除卷积算子的使用 [[90](#bib.bib90),
    [91](#bib.bib91)]（见图 [4](#S2.F4 "图 4 ‣ 2.5 自注意力编码 ‣ 2 视觉编码 ‣ 从展示到讲述：基于深度学习的图像描述调查")）。在这一方向上，Liu
    *et al.* [[92](#bib.bib92)] 设计了第一个无卷积架构用于图像描述。具体来说，采用预训练的视觉 Transformer 网络（*即*
    ViT [[90](#bib.bib90)]）作为编码器，使用标准 Transformer 解码器生成描述。有趣的是，相同的视觉编码方法已被应用于 CLIP [[93](#bib.bib93)]
    和 SimVLM [[94](#bib.bib94)]，不同之处在于视觉编码器是从头开始在大规模噪声数据上训练的。随后，CLIP 基于的特征已被后续描述方法使用 [[95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97)]。
- en: Early fusion and vision-and-language pre-training. Other works using self-attention
    to encode visual features achieved remarkable performance also thanks to vision-and-language
    pre-training [[98](#bib.bib98), [99](#bib.bib99)] and early-fusion strategies [[100](#bib.bib100),
    [101](#bib.bib101)]. For example, following the BERT architecture [[102](#bib.bib102)],
    Zhou *et al.* [[101](#bib.bib101)] combined encoder and decoder into a single
    stream of Transformer layers, where region and word tokens are early fused together
    into a unique flow. This unified model is first pre-trained on large amounts of
    image-caption pairs to perform both bidirectional and sequence-to-sequence prediction
    tasks and then fine-tuned.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 早期融合和视觉-语言预训练。其他使用自注意力编码视觉特征的研究也取得了显著成绩，这得益于视觉-语言预训练 [[98](#bib.bib98), [99](#bib.bib99)]
    和早期融合策略 [[100](#bib.bib100), [101](#bib.bib101)]。例如，遵循 BERT 架构 [[102](#bib.bib102)]，Zhou
    *et al.* [[101](#bib.bib101)] 将编码器和解码器合并为一个单一的 Transformer 层流，其中区域和词令牌被早期融合成一个唯一的流。这个统一模型首先在大量图像-描述对上进行预训练，以执行双向和序列到序列的预测任务，然后进行微调。
- en: On the same line, Li *et al.* [[100](#bib.bib100)] proposed OSCAR, a BERT-like
    architecture that includes object tags as anchor points to ease the semantic alignment
    between images and text. They also performed a large-scale pre-train with $6.5$
    million image-text pairs, with a masked token loss similar to the BERT mask language
    loss and a contrastive loss for distinguishing aligned words-tags-regions triples
    from polluted ones. Later, Zhang *et al.* [[103](#bib.bib103)] proposed VinVL,
    built on top of OSCAR, introducing a new object detector capable of extracting
    better visual features and a modified version of the vision-and-language pre-training
    objectives. On this line, Hu *et al.* [[104](#bib.bib104)] improved the VinVL
    model by scaling up its size and using larger scale noisy data to pre-train.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一行中，李*等*[[100](#bib.bib100)] 提出了 OSCAR，一种类似于 BERT 的架构，该架构包括对象标签作为锚点，以简化图像与文本之间的语义对齐。他们还进行了大规模的预训练，使用了
    $6.5$ 百万对图像-文本对，采用了类似于 BERT 掩码语言损失的掩码标记损失和对比损失，用于区分对齐的单词-标签-区域三元组与污染的三元组。随后，张*等*[[103](#bib.bib103)]
    提出了基于 OSCAR 的 VinVL，介绍了一种新型对象检测器，能够提取更好的视觉特征，并修改了视觉与语言预训练目标。在这一方向上，胡*等*[[104](#bib.bib104)]
    通过扩大 VinVL 模型的规模并使用大规模的噪声数据进行预训练，改进了该模型。
- en: '![Refer to caption](img/8cda9db270d0a64b2e8bfbd31fd4118f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8cda9db270d0a64b2e8bfbd31fd4118f.png)'
- en: (a)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/ef8d4853073f358de43616679ea1c662.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ef8d4853073f358de43616679ea1c662.png)'
- en: (b)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/693ee75f721b6f927375b4cd894b04e9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/693ee75f721b6f927375b4cd894b04e9.png)'
- en: (c)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: '![Refer to caption](img/b349a9d3ea14d7fc268a2a41bd0fb09c.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b349a9d3ea14d7fc268a2a41bd0fb09c.png)'
- en: (d)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (d)
- en: 'Figure 5: LSTM-based language modeling strategies: (a) Single-Layer LSTM model
    conditioned on the visual feature; (b) LSTM with attention, as proposed in the
    Show, Attend and Tell model [[42](#bib.bib42)]; (c) LSTM with attention, in the
    variant proposed in [[43](#bib.bib43)]; (d) two-layer LSTM with attention, in
    the style of the bottom-up top-down approach by Anderson *et al.* [[58](#bib.bib58)].
    In all figures, $\bm{X}$ represents either a grid of CNN features or image region
    features extracted by an object detector.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于 LSTM 的语言建模策略：（a）基于视觉特征的单层 LSTM 模型；（b）具有注意力机制的 LSTM，正如 Show, Attend and
    Tell 模型[[42](#bib.bib42)] 中所提出的；（c）具有注意力机制的 LSTM，正如[[43](#bib.bib43)] 中提出的变体；（d）具有注意力机制的双层
    LSTM，采用 Anderson*等*[[58](#bib.bib58)] 提出的自下而上的方法。在所有图中，$\bm{X}$ 表示 CNN 特征网格或由对象检测器提取的图像区域特征。
- en: 2.6 Discussion
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 讨论
- en: After the emergence of global features and grid features, region-based features
    have been the state-of-the-art choice in image captioning for years thanks to
    their compelling performances. Recently, however, different factors are reopening
    the discussion on which feature model is most appropriate for image captioning,
    ranging from the performance of better-trained grid features [[88](#bib.bib88)]
    to the emergence of self-attentive visual encoders [[90](#bib.bib90)] and large-scale
    multi-modal models like CLIP [[93](#bib.bib93)]. Recent strategies encompass training
    better object detectors on large-scale data [[103](#bib.bib103)] or employing
    end-to-end visual models trained from scratch [[94](#bib.bib94)]. Moreover, the
    success of BERT-like solutions performing image and text early-fusion indicates
    the suitability of visual representations that also integrate textual information.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在全局特征和网格特征出现之后，区域基础特征由于其出色的性能，多年来一直是图像描述中的最先进选择。然而，最近不同的因素正在重新开启关于哪种特征模型最适合图像描述的讨论，这些因素包括更好训练的网格特征[[88](#bib.bib88)]的性能、具有自注意力的视觉编码器[[90](#bib.bib90)]的出现以及像
    CLIP[[93](#bib.bib93)] 这样的大规模多模态模型。近期的策略包括在大规模数据上训练更好的对象检测器[[103](#bib.bib103)]或使用从头开始训练的端到端视觉模型[[94](#bib.bib94)]。此外，BERT
    类解决方案在图像和文本的早期融合中取得的成功表明，视觉表示也适合整合文本信息。
- en: 3 Language Models
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 语言模型
- en: The goal of a language model is to predict the probability of a given sequence
    of words to occur in a sentence. As such, it is a crucial component in image captioning,
    as it gives the ability to deal with natural language as a stochastic process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的目标是预测给定单词序列在句子中出现的概率。因此，它在图像描述中是一个关键组件，因为它使得处理自然语言成为一个随机过程。
- en: 'Formally, given a sequence of $n$ words, the language model component of an
    image captioning algorithm assigns a probability $P\left(y_{1},y_{2},\ldots,y_{n}\mid\bm{X}\right)$
    to the sequence as:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，给定一个长度为 $n$ 的单词序列，图像描述算法的语言模型组件为该序列分配一个概率 $P\left(y_{1},y_{2},\ldots,y_{n}\mid\bm{X}\right)$
    如下：
- en: '|  | $P\left(y_{1},y_{2},\ldots y_{n}\mid\bm{X}\right)=\prod_{i=1}^{n}P\left(y_{i}\mid
    y_{1},y_{2},\ldots,y_{i-1},\bm{X}\right),$ |  | (3) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $P\left(y_{1},y_{2},\ldots y_{n}\mid\bm{X}\right)=\prod_{i=1}^{n}P\left(y_{i}\mid
    y_{1},y_{2},\ldots,y_{i-1},\bm{X}\right),$ |  | (3) |'
- en: where $\bm{X}$ represents the visual encoding on which the language model is
    specifically conditioned. Notably, when predicting the next word given the previous
    ones, the language model is auto-regressive, which means that each predicted word
    is conditioned on the previous ones. The language model usually also decides when
    to stop generating caption words by outputting a special end-of-sequence token.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{X}$表示视觉编码，语言模型则特别以此为条件。值得注意的是，在给定先前词的情况下预测下一个词时，语言模型是自回归的，这意味着每个预测的词都以之前的词为条件。语言模型通常还通过输出一个特殊的序列结束标记来决定何时停止生成字幕词。
- en: 'The main language modeling strategies applied to image captioning can be categorized
    as: 1. *LSTM-based* approaches, which can be either single-layer or two-layer;
    2. *CNN-based* methods that constitute a first attempt in surpassing the fully
    recurrent paradigm; 3. *Transformer-based* fully-attentive approaches; 4. *image-text
    early-fusion* (BERT-like) strategies that directly connect the visual and textual
    inputs. This taxonomy is visually summarized in Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于图像字幕生成的主要语言建模策略可以分为：1. *基于LSTM*的方法，可以是单层或双层的；2. *基于CNN*的方法，这是超越完全递归范式的初步尝试；3.
    *基于Transformer*的全注意力方法；4. *图像-文本早期融合*（类似BERT）的策略，直接连接视觉和文本输入。这一分类在图[1](#S1.F1
    "图1 ‣ 1 引言 ‣ 从展示到描述：深度学习图像字幕生成综述")中得到了可视化总结。
- en: 3.1 LSTM-based Models
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于LSTM的模型
- en: As language has a sequential structure, RNNs are naturally suited to deal with
    the generation of sentences. Among RNN variants, LSTM [[105](#bib.bib105)] has
    been the predominant option for language modeling.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语言具有顺序结构，RNN自然适合处理句子的生成。在RNN变体中，LSTM[[105](#bib.bib105)]已成为语言建模的主要选择。
- en: 3.1.1 Single-layer LSTM
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 单层LSTM
- en: 'The most simple LSTM-based captioning architecture is based on a single-layer
    LSTM and was proposed by Vinyals *et al.* [[23](#bib.bib23)]. As shown in Fig. [5(a)](#S2.F5.sf1
    "In Figure 5 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual Encoding ‣ From Show to
    Tell: A Survey on Deep Learning-based Image Captioning"), the visual encoding
    is used as the initial hidden state of the LSTM, which then generates the output
    caption. At each time step, a word is predicted by applying a softmax activation
    function over the projection of the hidden state into a vector of the same size
    as the vocabulary. During training, input words are taken from the ground-truth
    sentence, while during inference, input words are those generated at the previous
    step.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的基于LSTM的字幕生成架构是基于单层LSTM的，由Vinyals*等*[[23](#bib.bib23)]提出。如图[5(a)](#S2.F5.sf1
    "图5 ‣ 2.5 自注意编码 ‣ 2 视觉编码 ‣ 从展示到描述：深度学习图像字幕生成综述")所示，视觉编码被用作LSTM的初始隐藏状态，LSTM随后生成输出字幕。在每个时间步，通过对隐藏状态投影到与词汇表大小相同的向量上应用softmax激活函数来预测一个词。在训练过程中，输入词取自真实句子，而在推断过程中，输入词是先前步骤生成的词。
- en: 'Shortly after, Xu *et al.* [[42](#bib.bib42)] introduced the additive attention
    mechanism. As depicted in Fig. [5(b)](#S2.F5.sf2 "In Figure 5 ‣ 2.5 Self-Attention
    Encoding ‣ 2 Visual Encoding ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), in this case, the previous hidden state guides the attention
    mechanism over the visual features $\bm{X}$, computing a context vector which
    is then fed to the MLP in charge of predicting the output word.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之后，徐*等*[[42](#bib.bib42)]引入了加性注意机制。如图[5(b)](#S2.F5.sf2 "图5 ‣ 2.5 自注意编码 ‣ 2
    视觉编码 ‣ 从展示到描述：深度学习图像字幕生成综述")所示，在这种情况下，先前的隐藏状态指导注意机制对视觉特征$\bm{X}$进行操作，计算出一个上下文向量，然后将其输入到负责预测输出词的MLP中。
- en: Other approaches. Many subsequent works have adopted a decoder based on a single-layer
    LSTM, mostly without any architectural changes [[50](#bib.bib50), [51](#bib.bib51),
    [67](#bib.bib67)], while others have proposed significant modifications, summarized
    below.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法。许多后续的工作采用了基于单层LSTM的解码器，大多数没有进行任何架构上的变化[[50](#bib.bib50), [51](#bib.bib51),
    [67](#bib.bib67)]，而其他工作则提出了显著的修改，概述如下。
- en: 'Visual sentinel – Lu *et al.* [[43](#bib.bib43)] augmented the spatial image
    features with an additional learnable vector, called visual sentinel, which can
    be attended by the decoder in place of visual features while generating “non-visual”
    words (*e.g.* “the”, “of”, and “on”), for which visual features are not needed
    (Fig. [5(c)](#S2.F5.sf3 "In Figure 5 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual
    Encoding ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")).
    At each time step, the visual sentinel is computed from the previous hidden state
    and generated word. Then, the model generates a context vector as a combination
    of attended image features and visual sentinel, whose importance is weighted by
    a learnable gate.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉哨兵 – Lu *et al.* [[43](#bib.bib43)] 通过添加一个可学习的向量——视觉哨兵，来增强空间图像特征。生成“非视觉”单词（*例如*
    “the”，“of”，和“on”）时，解码器可以关注该视觉哨兵，而不是视觉特征（见图[5(c)](#S2.F5.sf3 "在图5 ‣ 2.5 自注意力编码
    ‣ 2 视觉编码 ‣ 从展示到讲述：基于深度学习的图像描述调查")）。在每个时间步，视觉哨兵是从之前的隐藏状态和生成的单词中计算出来的。然后，模型生成一个上下文向量，作为关注的图像特征和视觉哨兵的组合，其重要性由一个可学习的门控加权。
- en: Hidden state reconstruction – Chen *et al.* [[46](#bib.bib46)] proposed to regularize
    the transition dynamics of the language model by using a second LSTM for reconstructing
    the previous hidden state based on the current one. Ge *et al.* [[48](#bib.bib48)]
    enhance context modeling by by using a bidirectional LSTM with an auxiliary module.
    The auxiliary module in a direction approximates the hidden state of the LSTM
    in the other direction. Finally, a cross-modal attention mechanism combines grid
    visual features with the two sentences from the bidirectional LSTM to obtain the
    final caption.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态重建 – Chen *et al.* [[46](#bib.bib46)] 提出了通过使用第二个 LSTM 来重建基于当前隐藏状态的前一个隐藏状态，从而对语言模型的转移动态进行正则化。Ge
    *et al.* [[48](#bib.bib48)] 通过使用具有辅助模块的双向 LSTM 来增强上下文建模。辅助模块在一个方向上近似 LSTM 在另一个方向上的隐藏状态。最后，跨模态注意力机制将网格视觉特征与双向
    LSTM 的两个句子结合，以获得最终的描述。
- en: 'Multi-stage generation – Wang *et al.* [[47](#bib.bib47)] proposed to generate
    a caption from coarse central aspects to finer attributes by decomposing the caption
    generation process into two phases: skeleton sentence generation and attributes
    enriching, both implemented with single-layer LSTMs. On the same line, Gu *et
    al.* [[49](#bib.bib49)] devised a coarse-to-fine multi-stage framework using a
    sequence of LSTM decoders, each operating on the output of the previous one to
    produce increasingly refined captions.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 多阶段生成 – Wang *et al.* [[47](#bib.bib47)] 提出了通过将描述生成过程分解为两个阶段：骨架句子生成和属性丰富，从粗略的中央方面生成描述到更精细的属性，两者均由单层
    LSTM 实现。类似地，Gu *et al.* [[49](#bib.bib49)] 设计了一种粗到细的多阶段框架，使用一系列 LSTM 解码器，每个解码器在前一个解码器的输出上操作，以生成越来越精细的描述。
- en: Semantic-guided LSTM – Jia *et al.* [[32](#bib.bib32)] proposed an extension
    of LSTM that includes semantic information extracted from the image to guide the
    generation. Specifically, the semantic information is used as an extra input to
    each gate in the LSTM block.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 语义引导 LSTM – Jia *et al.* [[32](#bib.bib32)] 提出了包括从图像中提取的语义信息以指导生成的 LSTM 扩展。具体来说，语义信息被用作
    LSTM 块中每个门的额外输入。
- en: 3.1.2 Two-layer LSTM
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 两层 LSTM
- en: LSTMs can be expanded to multi-layer structures to augment their capability
    of capturing higher-order relations. Donahue *et al.* [[28](#bib.bib28)] firstly
    proposed a two-layer LSTM as a language model for captioning, stacking two layers,
    where the hidden states of the first are the input to the second.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 可以扩展为多层结构，以增强捕捉高阶关系的能力。Donahue *et al.* [[28](#bib.bib28)] 首次提出了作为描述语言模型的两层
    LSTM，将两层堆叠，其中第一层的隐藏状态是第二层的输入。
- en: 'Two-layers and additive attention. Anderson *et al.* [[58](#bib.bib58)] went
    further and proposed to specialize the two layers to perform visual attention
    and the actual language modeling. As shown in Fig. [5(d)](#S2.F5.sf4 "In Figure
    5 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual Encoding ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning"), the first LSTM layer acts as a top-down
    visual attention model which takes the previously generated word, the previous
    hidden state, and the mean-pooled image features. Then, the current hidden state
    is used to compute a probability distribution over image regions with an additive
    attention mechanism. The so-obtained attended image feature vector is fed to the
    second LSTM layer, which combines it with the hidden state of the first layer
    to generate a probability distribution over the vocabulary.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '两层和加性注意力。Anderson *et al.* [[58](#bib.bib58)] 更进一步，提出将两层专门化以执行视觉注意和实际语言建模。如图
    [5(d)](#S2.F5.sf4 "In Figure 5 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual Encoding
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning") 所示，第一层
    LSTM 充当自上而下的视觉注意模型，它接受先前生成的词、前一个隐藏状态和均值池化的图像特征。然后，当前隐藏状态用于通过加性注意机制计算图像区域的概率分布。所得到的注意图像特征向量被送入第二层
    LSTM，它将其与第一层的隐藏状态结合，以生成词汇表上的概率分布。'
- en: Variants of two-layers LSTM. Because of their representation power, LSTMs with
    two-layers and internal attention mechanisms represent the most employed language
    model approach before the advent of Transformer-based architectures [[68](#bib.bib68),
    [71](#bib.bib71), [73](#bib.bib73), [72](#bib.bib72)]. As such, many other variants
    have been proposed to improve the performance of this approach.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 两层 LSTM 的变体。由于其表示能力，具有两层和内部注意机制的 LSTM 是在 Transformer 架构出现之前最常用的语言模型方法[[68](#bib.bib68),
    [71](#bib.bib71), [73](#bib.bib73), [72](#bib.bib72)]。因此，已经提出了许多其他变体以提高这种方法的性能。
- en: Neural Baby Talk – To ground words into image regions, Lu *et al.* [[106](#bib.bib106)]
    incorporated a pointing network that modulates the content-based attention mechanism.
    In particular, during the generation process, the network predicts slots in the
    caption, which are then filled with the image region classes. For non-visual words,
    a visual sentinel is used as dummy grounding. This approach leverages the object
    detector both as a feature region extractor and as a visual word prompter for
    the language model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 神经婴儿语言 – 为了将词语与图像区域关联起来，Lu *et al.* [[106](#bib.bib106)] 融入了一个指向网络，该网络调节基于内容的注意机制。特别是在生成过程中，网络预测字幕中的槽位，然后用图像区域类别填充这些槽位。对于非视觉词语，使用视觉哨兵作为虚拟基础。该方法将对象检测器既用作特征区域提取器，也用作语言模型的视觉词语提示器。
- en: 'Reflective attention – Ke *et al.* [[62](#bib.bib62)] introduced two reflective
    modules: while the first computes the relevance between hidden states from all
    the past predicted words and the current one, the second improves the syntactic
    structure of the sentence by guiding the generation process with words common
    position information.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 反射注意力 – Ke *et al.* [[62](#bib.bib62)] 引入了两个反射模块：第一个模块计算所有过去预测词和当前词之间的相关性，第二个模块通过使用常见位置的信息指导生成过程，从而改善句子的句法结构。
- en: 'Look back and predict forward – On a similar line, Qin *et al.* [[63](#bib.bib63)]
    used two modules: the look back module that takes into account the previous attended
    vector to compute the next one, and the predict forward module that predicts the
    new two words at once, thus alleviating the accumulated errors problem that may
    occur at inference time.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾并预测前进 – 类似地，Qin *et al.* [[63](#bib.bib63)] 使用了两个模块：回顾模块考虑之前的注意向量来计算下一个向量，预测前进模块同时预测两个新词，从而减轻推理时可能发生的累积错误问题。
- en: Adaptive attention time – Huang *et al.* [[64](#bib.bib64)] proposed an adaptive
    attention time mechanism, in which the decoder can take an arbitrary number of
    attention steps for each generated word, determined by a confidence network on
    top of the second-layer LSTM.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应注意时间 – Huang *et al.* [[64](#bib.bib64)] 提出了自适应注意时间机制，在该机制中，解码器可以为每个生成的词进行任意数量的注意步骤，由第二层
    LSTM 上方的置信网络确定。
- en: 3.1.3 Boosting LSTM with Self-Attention
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 使用自注意力机制增强 LSTM
- en: Some works adopted the self-attention operator in place of the additive attention
    one in LSTM-based language models [[79](#bib.bib79), [80](#bib.bib80), [83](#bib.bib83),
    [107](#bib.bib107)]. In particular, Huang *et al.* [[79](#bib.bib79)] augmented
    the LSTM with the Attention on Attention operator, which computes another step
    of attention on top of visual self-attention. Pan *et al.* [[80](#bib.bib80)]
    introduced the X-Linear attention block, which enhances self-attention with second-order
    interactions and improves both the visual encoding and the language model. On
    a different line, Zhu *et al.* [[107](#bib.bib107)] applied the neural architecture
    search paradigm to select the connections between layers and the operations within
    gates of RNN-based image captioning language models, using a decoder enriched
    with self-attention [[80](#bib.bib80)].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究采用了自注意力操作来替代 LSTM 基于语言模型中的加法注意力操作 [[79](#bib.bib79), [80](#bib.bib80), [83](#bib.bib83),
    [107](#bib.bib107)]。特别地，黄*等人* [[79](#bib.bib79)] 将 Attention on Attention 操作符扩展到
    LSTM 上，在视觉自注意力的基础上计算另一层注意力。潘*等人* [[80](#bib.bib80)] 引入了 X-Linear 注意力块，该块通过二阶交互增强自注意力，从而改进了视觉编码和语言模型。另一方面，朱*等人* [[107](#bib.bib107)]
    应用了神经架构搜索范式来选择层之间的连接和 RNN 基于图像描述语言模型中门内的操作，使用了一个增强自注意力的解码器 [[80](#bib.bib80)]。
- en: 3.2 Convolutional Language Models
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 卷积语言模型
- en: A worth-to-mention approach is that proposed by Aneya *et al.* [[108](#bib.bib108)],
    which uses convolutions as a language model. In particular, a global image feature
    vector is combined with word embeddings and fed to a CNN, operating on all words
    in parallel during training and sequentially in inference. Convolutions are right-masked
    to prevent the model from using the information of future word tokens. Despite
    the clear advantage of parallel training, the usage of the convolutional operator
    in language models has not gained popularity due to the poor performance and the
    advent of Transformer architectures.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得提及的方法是 Aneya*等人* [[108](#bib.bib108)] 提出的，该方法使用卷积作为语言模型。具体来说，将全局图像特征向量与词嵌入结合，并输入
    CNN，在训练期间对所有词进行并行处理，在推断过程中进行顺序处理。卷积操作右侧掩码以防止模型使用未来词标记的信息。尽管并行训练有明显的优势，但由于性能较差和
    Transformer 架构的出现，卷积操作在语言模型中的使用并未获得广泛关注。
- en: 3.3 Transformer-based Architectures
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于 Transformer 的架构
- en: '![Refer to caption](img/b77e90263311a0e160fb52258d5b695c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b77e90263311a0e160fb52258d5b695c.png)'
- en: 'Figure 6: Schema of the Transformer-based language model. The caption generation
    is performed via masked self-attention over previously generated tokens and cross-attention
    with encoded visual features.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于 Transformer 的语言模型的示意图。图像生成通过对先前生成的标记进行掩码自注意力和与编码视觉特征进行交叉注意力来完成。
- en: 'The fully-attentive paradigm proposed by Vaswani *et al.* [[74](#bib.bib74)]
    has completely changed the perspective of language generation. Shortly after,
    the Transformer model became the building block of other breakthroughs in NLP,
    such as BERT [[102](#bib.bib102)] and GPT [[109](#bib.bib109)], and the standard
    de-facto architecture for many language understanding tasks. As image captioning
    can be cast as a sequence-to-sequence problem, the Transformer architecture has
    been employed also for this task. The standard Transformer decoder performs a
    masked self-attention operation, which is applied to words, followed by a cross-attention
    operation, where words act as queries and the outputs of the last encoder layer
    act as keys and values, plus a final feed-forward network (Fig. [6](#S3.F6 "Figure
    6 ‣ 3.3 Transformer-based Architectures ‣ 3 Language Models ‣ From Show to Tell:
    A Survey on Deep Learning-based Image Captioning")). During training, a masking
    mechanism is applied to the previous words to constrain a unidirectional generation
    process. The original Transformer decoder has been employed in some image captioning
    models without significant architectural modifications [[77](#bib.bib77), [78](#bib.bib78),
    [86](#bib.bib86), [94](#bib.bib94)]. Besides, some variants have been proposed
    to improve language generation and visual feature encoding.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '由Vaswani *等人* 提出的完全注意力范式[[74](#bib.bib74)] 完全改变了语言生成的视角。不久之后，Transformer模型成为了NLP领域其他突破的基础，如BERT
    [[102](#bib.bib102)] 和GPT [[109](#bib.bib109)]，并且成为许多语言理解任务的标准架构。由于图像描述可以被视作一个序列到序列的问题，Transformer架构也被用于这一任务。标准的Transformer解码器执行一个掩蔽自注意力操作，应用于词语，然后是一个交叉注意力操作，其中词语作为查询，最后编码器层的输出作为键和值，以及一个最终的前馈网络（图[6](#S3.F6
    "Figure 6 ‣ 3.3 Transformer-based Architectures ‣ 3 Language Models ‣ From Show
    to Tell: A Survey on Deep Learning-based Image Captioning")）。在训练过程中，掩蔽机制被应用于前面的词语，以限制单向生成过程。原始的Transformer解码器已经在一些图像描述模型中使用，没有显著的架构修改[[77](#bib.bib77),
    [78](#bib.bib78), [86](#bib.bib86), [94](#bib.bib94)]。此外，还提出了一些变体来改进语言生成和视觉特征编码。'
- en: Gating mechanisms. Li *et al.* [[76](#bib.bib76)] proposed a gating mechanism
    for the cross-attention operator, which controls the flow of visual and semantic
    information by combining and modulating image regions representations with semantic
    attributes coming from an external tagger. On the same line, Ji *et al.* [[85](#bib.bib85)]
    integrated a context gating mechanism to modulate the influence of the global
    image representation on each generated word, modeled via multi-head attention.
    Cornia *et al.* [[81](#bib.bib81)] proposed to take into account all encoding
    layers in place of performing cross-attention only on the last one. To this end,
    they devised the meshed decoder, which contains a mesh operator that modulates
    the contribution of all the encoding layers independently and a gate that weights
    these contributions guided by the text query. In [[94](#bib.bib94), [97](#bib.bib97)],
    the decoder architecture is again employed in conjunction with textual prefixes,
    also extracted from pre-trained visual-semantic models and employed as visual
    tags.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 门控机制。Li *等人* [[76](#bib.bib76)] 提出了一个用于交叉注意力操作的门控机制，通过结合和调节来自外部标注器的语义属性与图像区域表示，来控制视觉和语义信息的流动。同样，Ji
    *等人* [[85](#bib.bib85)] 集成了一个上下文门控机制，以调节全局图像表示对每个生成词的影响，通过多头注意力建模。Cornia *等人*
    [[81](#bib.bib81)] 提议考虑所有编码层，而不是仅在最后一层上执行交叉注意力。为此，他们设计了网格解码器，该解码器包含一个网格操作符，独立调节所有编码层的贡献，以及一个通过文本查询引导的门来加权这些贡献。在[[94](#bib.bib94),
    [97](#bib.bib97)]中，解码器架构再次与文本前缀一起使用，这些前缀也从预训练的视觉-语义模型中提取，并用作视觉标签。
- en: 3.4 BERT-like Architectures
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 BERT-like架构
- en: 'Despite the encoder-decoder paradigm being a common approach to image captioning,
    some works have revisited captioning architectures to exploit a BERT-like [[102](#bib.bib102)]
    structure in which the visual and textual modalities are fused together in the
    early stages (Fig. [7](#S3.F7 "Figure 7 ‣ 3.4 BERT-like Architectures ‣ 3 Language
    Models ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")).
    The main advantage of this architecture is that layers dealing with text can be
    initialized with pre-trained parameters learned from massive textual corpora.
    Therefore, the BERT paradigm has been widely adopted in works that exploit pre-training [[100](#bib.bib100),
    [101](#bib.bib101), [103](#bib.bib103)]. The first example is due to Zhou *et
    al.* [[101](#bib.bib101)], who developed a unified model that fuses visual and
    textual modalities into a BERT-like architecture for image captioning. The model
    consists of a shared multi-layer Transformer encoder network for both encoding
    and decoding, pre-trained on a large corpus of image-caption pairs and then fine-tuned
    for image captioning by right-masking the tokens sequence to simulate the unidirectional
    generation process. Further, Li *et al.* [[100](#bib.bib100)] introduced the usage
    of object tags detected in the image as anchors points for learning a better alignment
    in vision-and-language joint representations. To this end, their model represents
    an input image-text pair as a word tokens-object tags-region features triple,
    where the object tags are the textual classes proposed by the object detector.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管编码器-解码器范式是图像字幕生成的常见方法，但一些研究重新审视了字幕生成架构，利用类似 BERT 的 [[102](#bib.bib102)] 结构，在早期阶段将视觉和文本模态融合在一起（图 [7](#S3.F7
    "Figure 7 ‣ 3.4 BERT-like Architectures ‣ 3 Language Models ‣ From Show to Tell:
    A Survey on Deep Learning-based Image Captioning")）。这种架构的主要优点是处理文本的层可以用从大量文本语料中学到的预训练参数进行初始化。因此，BERT
    范式已被广泛应用于利用预训练的研究中 [[100](#bib.bib100), [101](#bib.bib101), [103](#bib.bib103)]。第一个例子是
    Zhou *et al.* [[101](#bib.bib101)]，他们开发了一个统一的模型，将视觉和文本模态融合成一个类似 BERT 的架构用于图像字幕生成。该模型由一个共享的多层
    Transformer 编码器网络组成，用于编码和解码，先在大量图像-字幕对的语料库上进行预训练，然后通过对标记序列进行右掩蔽来模拟单向生成过程，从而进行微调。进一步地，Li
    *et al.* [[100](#bib.bib100)] 引入了在图像中检测到的物体标签作为锚点，以学习更好的视觉与语言联合表示对齐。为此，他们的模型将输入的图像-文本对表示为一个由词标记-物体标签-区域特征组成的三重，其中物体标签是物体检测器提出的文本类。'
- en: '![Refer to caption](img/d1768dba41b1098ce4a1a7c07dcdc0c9.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d1768dba41b1098ce4a1a7c07dcdc0c9.png)'
- en: 'Figure 7: Schema of a BERT-like language model. A single stream of attentive
    layers processes both image regions and word tokens and generates the output caption.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 类似 BERT 的语言模型的示意图。单一的关注层流处理图像区域和词标记，并生成输出字幕。'
- en: 3.5 Non-autoregressive Language Models
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 非自回归语言模型
- en: Thanks to the parallelism offered by Transformers, non-autoregressive language
    models have been proposed in machine translation to reduce the inference time
    by generating all words in parallel. Some efforts have been made to apply this
    paradigm to image captioning [[110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112),
    [113](#bib.bib113)]. The first approaches towards a non-autoregressive generation
    were composed of a number of different generation stages, where all words were
    predicted in parallel and refined at each stage. Subsequent methods, instead,
    employ reinforcement learning techniques to improve the final results. Specifically,
    these approaches treat the generation process as a cooperative multi-agent reinforcement
    system, where the positions in of the words in the target sequence are viewed
    as agents that learn to cooperatively maximize a sentence-level reward [[111](#bib.bib111),
    [113](#bib.bib113)]. These works also leverage knowledge distillation on unlabeled
    data and a post-processing step to remove identical consecutive tokens.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Transformers 提供的并行性，非自回归语言模型已经在机器翻译中被提出，以通过并行生成所有单词来减少推理时间。一些努力也被用来将这种范式应用于图像字幕生成 [[110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)]。最早的非自回归生成方法由多个不同的生成阶段组成，其中所有单词并行预测，并在每个阶段进行精细调整。随后的方法则采用强化学习技术来改进最终结果。具体而言，这些方法将生成过程视为一个合作的多智能体强化系统，其中目标序列中单词的位置被视为学习协作以最大化句子级奖励的智能体 [[111](#bib.bib111),
    [113](#bib.bib113)]。这些工作还利用了在未标记数据上的知识蒸馏和一个后处理步骤，以去除相同的连续标记。
- en: 3.6 Discussion
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 讨论
- en: 'Recurrent models have been the standard for many years, and their application
    brought to the development of clever and successful ideas that can be integrated
    also into non-recurrent solutions. However, they are slow to train and struggle
    to maintain long-term dependencies: these drawbacks are alleviated by autoregressive
    and Transformer-based solutions that recently gained popularity. Inspired by the
    success of pre-training on large, unsupervised corpora for NLP tasks, massive
    pre-training has been applied also for image captioning by employing either encoder-decoder
    or BERT-like architectures, often in conjunction with textual tags. This strategy
    led to impressive performance, suggesting that visual and textual semantic relations
    can be inferred and learned also from not well-curated data [[100](#bib.bib100),
    [94](#bib.bib94), [104](#bib.bib104)]. BERT-like architectures are suitable for
    such a massive pre-training but are not generative architectures by design. Massive
    pre-training on generative-oriented architectures [[94](#bib.bib94), [97](#bib.bib97)]
    is currently a worth-exploring direction, which leads to performances that are
    at least on-pair with the early-fusion counterparts.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 循环模型多年来一直是标准，其应用带来了聪明和成功的想法，这些想法也可以整合到非循环解决方案中。然而，它们的训练速度较慢，并且难以保持长期依赖关系：这些缺点通过自回归和基于Transformer的解决方案得到缓解，这些解决方案最近获得了普及。受到在大规模无监督语料库上预训练成功的启发，图像描述领域也采用了大规模预训练，使用编码器-解码器或类似BERT的架构，通常结合文本标签。这种策略取得了令人印象深刻的表现，表明视觉和文本的语义关系也可以从不良策划的数据中推断和学习
    [[100](#bib.bib100), [94](#bib.bib94), [104](#bib.bib104)]。类似BERT的架构适合于这样的庞大预训练，但在设计上并不是生成架构。对生成导向架构的庞大预训练
    [[94](#bib.bib94), [97](#bib.bib97)] 目前是值得探索的方向，这导致的性能至少与早期融合的对手持平。
- en: 4 Training Strategies
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 训练策略
- en: An image captioning model is commonly expected to generate a caption word by
    word by taking into account the previous words and the image. At each step, the
    output word is sampled from a learned distribution over the vocabulary words.
    In the most simple scenario, *i.e.* the greedy decoding mechanism, the word with
    the highest probability is output. The main drawback of this setting is that possible
    prediction errors quickly accumulate along the way. To alleviate this drawback,
    one effective strategy is to use the beam search algorithm [[114](#bib.bib114)]
    that, instead of outputting the word with maximum probability at each time step,
    maintains $k$ sequence candidates (those with the highest probability at each
    step) and finally outputs the most probable one.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述模型通常期望通过考虑先前的单词和图像逐字生成描述。在每一步，输出单词是从词汇表单词的学习分布中采样的。在最简单的场景中，即*贪婪解码机制*，概率最高的单词被输出。这种设置的主要缺点是可能的预测错误会迅速累积。为了解决这个缺点，一种有效的策略是使用束搜索算法
    [[114](#bib.bib114)]，它不在每个时间步输出概率最大的单词，而是维护$k$个序列候选（每一步具有最高概率的序列），最后输出最可能的一个。
- en: 'During training, the captioning model must learn to properly predict the probabilities
    of the words to appear in the caption. To this end, the most common training strategies
    are based on 1. *cross-entropy loss*; 2. *masked language model*; 3. *reinforcement
    learning* that allows directly optimizing for captioning-specific non-differentiable
    metrics; 4. *vision-and-language pre-training* objectives (see Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '在训练过程中，描述模型必须学会正确预测描述中单词的出现概率。为此，最常见的训练策略基于 1. *交叉熵损失*；2. *掩蔽语言模型*；3. *强化学习*，允许直接优化针对描述特定的非可微度量；4.
    *视觉与语言预训练* 目标（见图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ From Show to Tell: A
    Survey on Deep Learning-based Image Captioning")）。'
- en: 4.1 Cross-Entropy Loss
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 交叉熵损失
- en: 'The cross-entropy loss is the first proposed and most used objective for image
    captioning models. With this loss, the goal of the training, at each timestep,
    is to minimize the negative log-likelihood of the current word given the previous
    ground-truth words. Given a sequence of target words $y_{1:T}$, the loss is formally
    defined as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失是最早提出的并且最常用的图像描述模型目标函数。通过这种损失，训练的目标是在每个时间步最小化当前单词在给定先前真实单词的条件下的负对数似然。给定目标词序列$y_{1:T}$，损失的正式定义为：
- en: '|  | $L_{XE}(\theta)=-\sum_{i=1}^{n}\log\left(P\left(y_{i}\mid y_{1:i-1},\bm{X}\right)\right),$
    |  | (4) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{XE}(\theta)=-\sum_{i=1}^{n}\log\left(P\left(y_{i}\mid y_{1:i-1},\bm{X}\right)\right),$
    |  | (4) |'
- en: where $P$ is the probability distribution induced by the language model, $y_{i}$
    the ground-truth word at time $i$, $y_{1:i-1}$ indicate the previous ground-truth
    words, and $\bm{X}$ the visual encoding. The cross-entropy loss is designed to
    operate at word level and optimize the probability of each word in the ground-truth
    sequence without considering longer range dependencies between generated words.
    The traditional training setting with cross-entropy also suffers from the exposure
    bias problem [[115](#bib.bib115)] caused by the discrepancy between the training
    data distribution as opposed to the distribution of its own predicted words.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P$ 是语言模型引起的概率分布，$y_{i}$ 是时间 $i$ 的真实词，$y_{1:i-1}$ 表示之前的真实词，$\bm{X}$ 是视觉编码。交叉熵损失旨在在词级别操作，优化真实词序列中每个词的概率，而不考虑生成词之间的长程依赖关系。传统的交叉熵训练设置也受到暴露偏差问题的困扰 [[115](#bib.bib115)]，这是由于训练数据分布与自身预测词的分布之间的差异造成的。
- en: 4.2 Masked Language Model (MLM)
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 掩码语言模型 (MLM)
- en: The first masked language model has been proposed for training the BERT [[102](#bib.bib102)]
    architecture. The main idea behind this optimization function consists in randomly
    masking out a small subset of the input tokens sequence and training the model
    to predict masked tokens while relying on the rest of the sequence, *i.e.* both
    previous and subsequent tokens. As a consequence, the model learns to employ contextual
    information to infer missing tokens, which allows building a robust sentence representation
    where the context plays an essential role. Since this strategy considers only
    the prediction of the masked tokens and ignores the prediction of the non-masked
    ones, training with it is much slower than training for complete left-to-right
    or right-to-left generation. Notably, some works have employed this strategy as
    a pre-training objective, sometimes completely avoiding the combination with the
    cross-entropy [[100](#bib.bib100), [103](#bib.bib103)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首个掩码语言模型被提出用于训练 BERT [[102](#bib.bib102)] 架构。这个优化函数的主要思想是随机掩盖输入令牌序列中的一小部分，并训练模型在依赖于序列其余部分的情况下预测掩盖的令牌，*即*
    既包括前面的也包括后面的令牌。因此，模型学会利用上下文信息来推断缺失的令牌，从而构建一个稳健的句子表示，其中上下文起着至关重要的作用。由于这种策略仅考虑掩盖令牌的预测，而忽略了非掩盖令牌的预测，因此其训练速度比完整的从左到右或从右到左生成训练要慢得多。值得注意的是，一些研究将这种策略作为预训练目标，有时完全避免与交叉熵 [[100](#bib.bib100),
    [103](#bib.bib103)] 结合。
- en: 4.3 Reinforcement Learning
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 强化学习
- en: Given the limitations of word-level training strategies observed when using
    limited amounts of data, a significant improvement was achieved by applying the
    reinforcement learning paradigm for training image captioning models. Within this
    framework, the image captioning model is considered as an agent whose parameters
    determine a policy. At each time step, the agent executes the policy to choose
    an action, *i.e.* the prediction of the next word in the generated sentence. Once
    the end-of-sequence is reached, the agent receives a reward, and the aim of the
    training is to optimize the agent parameters to maximize the expected reward.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到在使用有限数据时观察到的词级训练策略的局限性，通过应用强化学习范式进行图像描述模型的训练取得了显著的改进。在这个框架下，图像描述模型被视为一个代理，其参数决定了一个策略。在每个时间步骤，代理执行该策略以选择一个动作，*即*
    生成句子中下一个词的预测。一旦达到序列结束，代理会获得奖励，训练的目标是优化代理参数以最大化期望奖励。
- en: 'Many works harnessed this paradigm and explored different sequence-level metrics
    as rewards. The first proposal is due to Ranzato *et al.* [[115](#bib.bib115)],
    which introduced the usage of the REINFORCE algorithm [[116](#bib.bib116)] adopting
    BLEU [[117](#bib.bib117)] and ROUGE [[118](#bib.bib118)] as reward signals. Ren *et
    al.* [[119](#bib.bib119)] experimented using visual-semantic embeddings obtained
    from a network that encodes the image and the so far generated caption in order
    to compute a similarity score to be used as reward. Liu *et al.* [[120](#bib.bib120)]
    proposed to use as reward a linear combination of SPICE [[121](#bib.bib121)] and
    CIDEr [[122](#bib.bib122)], called SPIDEr. Finally, the most widely adopted strategy [[123](#bib.bib123),
    [124](#bib.bib124), [81](#bib.bib81)], introduced by Rennie *et al.* [[38](#bib.bib38)],
    entails using the CIDEr score, as it correlates better with human judgment [[122](#bib.bib122)].
    The reward is normalized with respect to a baseline value to reduce variance.
    Formally, to compute the loss gradient, beam search and greedy decoding are leveraged
    as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究利用了这种范式并探索了不同的序列级度量作为奖励。第一个提案来自 Ranzato *et al.* [[115](#bib.bib115)]，该提案介绍了使用
    REINFORCE 算法[[116](#bib.bib116)]，并采用 BLEU[[117](#bib.bib117)] 和 ROUGE[[118](#bib.bib118)]
    作为奖励信号。Ren *et al.* [[119](#bib.bib119)] 实验使用从网络中获得的视觉-语义嵌入，该网络对图像和迄今为止生成的字幕进行编码，以计算用作奖励的相似度分数。Liu
    *et al.* [[120](#bib.bib120)] 提议将 SPICE[[121](#bib.bib121)] 和 CIDEr[[122](#bib.bib122)]
    的线性组合作为奖励，称为 SPIDEr。最后，最广泛采用的策略[[123](#bib.bib123), [124](#bib.bib124), [81](#bib.bib81)]，由
    Rennie *et al.* [[38](#bib.bib38)] 提出，采用 CIDEr 分数，因为它与人类判断的相关性更好[[122](#bib.bib122)]。奖励是相对于基线值进行归一化的，以减少方差。正式地，为了计算损失梯度，使用光束搜索和贪婪解码，如下所示：
- en: '|  | $\nabla_{\theta}L(\theta)=-\frac{1}{k}\sum_{i=1}^{k}\left((r(\bm{w}^{i})-b)\nabla_{\theta}\log
    P(\bm{w}^{i})\right),$ |  | (5) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{\theta}L(\theta)=-\frac{1}{k}\sum_{i=1}^{k}\left((r(\bm{w}^{i})-b)\nabla_{\theta}\log
    P(\bm{w}^{i})\right),$ |  | (5) |'
- en: where $\bm{w}^{i}$ is the $i$-th sentence in the beam or a sampled collection,
    $r(\cdot)$ is the reward function, *i.e.* the CIDEr computation, and $b$ is the
    baseline, computed as the reward of the sentence obtained via greedy decoding [[38](#bib.bib38)],
    or as the average reward of the beam candidates [[81](#bib.bib81)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\bm{w}^{i}$ 是光束中的第 $i$ 个句子或采样集合，$r(\cdot)$ 是奖励函数，即 CIDEr 计算，$b$ 是基线，通过贪婪解码获得的句子的奖励计算得出[[38](#bib.bib38)]，或作为光束候选句子的平均奖励[[81](#bib.bib81)]。
- en: Note that, since it would be difficult for a random policy to improve in an
    acceptable amount of time, the usual procedure entails pre-training with cross-entropy
    or masked language model first, and then fine-tuning stage with reinforcement
    learning by employing a sequence level metric as reward. This ensures the initial
    reinforcement learning policy to be more suitable than the random one.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于随机策略在可接受的时间内难以改进，通常的程序是首先进行交叉熵或掩码语言模型的预训练，然后通过采用序列级别度量作为奖励的强化学习进行微调。这确保了初始强化学习策略比随机策略更合适。
- en: '![Refer to caption](img/518c765dbacb58a6066363ff8fc7035a.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/518c765dbacb58a6066363ff8fc7035a.png)'
- en: (a)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/9368d4f168abe0744a0fa1c43dcf4d2b.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9368d4f168abe0744a0fa1c43dcf4d2b.png)'
- en: (b)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 8: Qualitative examples from some of the most common image captioning
    datasets: (a) image-caption pairs; (b) word clouds of the captions most common
    visual words.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：来自一些最常见的图像字幕数据集的定性示例：（a）图像-字幕对；（b）最常见视觉词的字幕词云。
- en: 4.4 Large-scale Pre-Training
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 大规模预训练
- en: In the context of vision-and-language pre-training in early-fusion architectures,
    one of the most common pre-training objectives is the masked contextual token
    loss, where tokens of each modality (visual and textual) are randomly masked following
    the BERT strategy [[102](#bib.bib102)], and the model has to predict the masked
    input based on the context of both modalities, thus connecting their joint representation.
    Another largely adopted strategy entails using a contrastive loss, where the inputs
    are organized as image regions-captions words-object tags triples, and the model
    is asked to discriminate correct triples from polluted ones, in which tags are
    randomly replaced [[100](#bib.bib100), [103](#bib.bib103)]. Other objectives take
    into account the text-image alignment at a word-region level and entail predicting
    the original word sequence given a corrupted one [[125](#bib.bib125)].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期融合架构中的视觉和语言预训练背景下，最常见的预训练目标之一是掩码上下文令牌损失，其中每种模态（视觉和文本）的令牌按照 BERT 策略[[102](#bib.bib102)]
    随机掩码，模型必须基于两种模态的上下文预测掩码输入，从而连接它们的联合表示。另一个被广泛采用的策略涉及使用对比损失，其中输入被组织为图像区域-描述词-对象标签三元组，模型被要求区分正确的三元组和被污染的三元组，其中标签被随机替换[[100](#bib.bib100),
    [103](#bib.bib103)]。其他目标考虑到在单词-区域级别的文本-图像对齐，并涉及在给定损坏的序列时预测原始单词序列[[125](#bib.bib125)]。
- en: On the other hand, cross-entropy has also been used when pre-training on noisy
    captions [[97](#bib.bib97), [94](#bib.bib94)], sometimes also employing prefixes.
    PrefixLM [[94](#bib.bib94)] has indeed proved to be a valuable strategy that enables
    bidirectional attention within the prefix sequence, and thus, it is applicable
    for both decoder-only and encoder-decoder sequence-to-sequence language models.
    Noticeably, some large-scale models pre-trained on noisy data under this setting
    can achieve state-of-the-art performance without requiring a fine-tuning stage
    with Reinforcement [[94](#bib.bib94)].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，交叉熵在噪声标签的预训练中也被使用[[97](#bib.bib97), [94](#bib.bib94)]，有时也使用前缀。PrefixLM
    [[94](#bib.bib94)] 确实证明了它是一种有价值的策略，它在前缀序列中启用了双向注意力，因此适用于仅解码器和编码器-解码器序列到序列语言模型。值得注意的是，一些在这种设置下在噪声数据上预训练的大规模模型能够在不需要强化[[94](#bib.bib94)]微调阶段的情况下实现最先进的性能。
- en: Finally, we notice that image captioning can be used as a pre-training task
    to efficiently learn visual representations, which can benefit downstream tasks
    such as image classification, object detection, and instance segmentation [[126](#bib.bib126)].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到图像描述可以作为一种预训练任务来有效地学习视觉表示，这可以有助于下游任务，如图像分类、目标检测和实例分割[[126](#bib.bib126)]。
- en: 5 Evaluation Protocol
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估协议
- en: As for any data-driven task, the development of image captioning has been enabled
    by the collection of large datasets and the definition of quantitative scores
    to evaluate the performance and monitor the advancement of the field.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何数据驱动的任务，图像描述的发展得益于大量数据集的收集以及定义定量评分来评估性能和监控领域的进展。
- en: 5.1 Datasets
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集
- en: 'Image captioning datasets contain images and one or multiple captions associated
    with them. Having multiple ground-truth captions for each image helps to capture
    the variability of human descriptions. Other than the number of available captions,
    also their characteristics (*e.g.* average caption length and vocabulary size)
    highly influence the design and the performance of image captioning algorithms.
    Note that the distribution of the terms in the datasets captions is usually long-tailed,
    thus, when using word-level dictionaries, the common practice is to include in
    the vocabulary only those terms whose frequency is above a pre-defined threshold.
    Recently, however, using subword-based tokenization approaches like BPE [[127](#bib.bib127)]
    is a popular choice that allows avoiding dataset pre-processing. The available
    datasets differ both on the images contained (for their domain and visual quality)
    and on the captions associated with the images (for their length, number, relevance,
    and style). A summary of the most used public datasets is reported in Table [I](#S5.T1
    "TABLE I ‣ 5.1.1 Standard captioning datasets ‣ 5.1 Datasets ‣ 5 Evaluation Protocol
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning"), and some
    sample image-caption pairs are reported in Fig. [8](#S4.F8 "Figure 8 ‣ 4.3 Reinforcement
    Learning ‣ 4 Training Strategies ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), along with some word clouds obtained from the 50 most used
    visual words in the captions.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '图像描述数据集包含图像以及与之相关的一项或多项描述。每张图像有多个真实描述有助于捕捉人类描述的多样性。除了可用描述的数量，其特征（*例如* 平均描述长度和词汇量）也对图像描述算法的设计和性能有很大影响。请注意，数据集中描述的术语分布通常是长尾分布，因此，在使用词级字典时，常见做法是将词汇表中仅包括那些频率高于预定义阈值的术语。然而，最近，使用基于子词的分词方法如BPE [[127](#bib.bib127)]成为一种流行的选择，允许避免数据集预处理。现有的数据集在包含的图像（无论是领域还是视觉质量）和与图像相关的描述（长度、数量、相关性和风格）上有所不同。最常用公共数据集的总结见表 [I](#S5.T1
    "TABLE I ‣ 5.1.1 Standard captioning datasets ‣ 5.1 Datasets ‣ 5 Evaluation Protocol
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")，一些样本图像-描述对见图 [8](#S4.F8
    "Figure 8 ‣ 4.3 Reinforcement Learning ‣ 4 Training Strategies ‣ From Show to
    Tell: A Survey on Deep Learning-based Image Captioning")，以及从描述中最常用的50个视觉词获得的一些词云。'
- en: 5.1.1 Standard captioning datasets
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 标准描述数据集
- en: Standard benchmark datasets are used by the community to compare their approaches
    on a common test-bed, a procedure that guides the development of image captioning
    strategies by allowing to identify suitable directions. Datasets used as benchmarks
    should be representative of the task at hand, both in terms of the challenges
    and ideal expected results (*i.e.* achievable human performance). Further, they
    should contain a large number of generic-domain images, each associated with multiple
    captions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 标准基准数据集被社区用于在共同测试平台上比较他们的方法，这一过程指导了图像描述策略的发展，使其能够确定合适的方向。用作基准的数据集应代表当前任务，包括挑战和理想的预期结果（*即* 可实现的人类表现）。此外，它们应包含大量通用领域的图像，每张图像关联多个描述。
- en: 'TABLE I: Overview of the main image captioning datasets.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：主要图像描述数据集概述。
- en: '|  |  | Domain | Nb. Images | Nb. Caps | Vocab Size | Nb. Words |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 领域 | 图像数 | 描述数 | 词汇量 | 词数 |'
- en: '|  |  | (per Image) | (per Cap.) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  |  | （每图像） | （每描述） |'
- en: '| COCO [[128](#bib.bib128)] |  | Generic | $132$K | $5$ | $27$K ($10$K) | $10.5$
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| COCO [[128](#bib.bib128)] |  | 通用 | $132$K | $5$ | $27$K ($10$K) | $10.5$
    |'
- en: '| Flickr30K [[129](#bib.bib129)] |  | Generic | $31$K | $5$ | $18$K ($7$K)
    | $12.4$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Flickr30K [[129](#bib.bib129)] |  | 通用 | $31$K | $5$ | $18$K ($7$K) | $12.4$
    |'
- en: '| Flickr8K [[19](#bib.bib19)] |  | Generic | $8$K | $5$ | $8$K ($3$K) | $10.9$
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Flickr8K [[19](#bib.bib19)] |  | 通用 | $8$K | $5$ | $8$K ($3$K) | $10.9$ |'
- en: '| CC3M [[130](#bib.bib130)] |  | Generic | $3.3$M | $1$ | $48$K ($25$K) | $10.3$
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| CC3M [[130](#bib.bib130)] |  | 通用 | $3.3$M | $1$ | $48$K ($25$K) | $10.3$
    |'
- en: '| CC12M [[131](#bib.bib131)] |  | Generic | $12.4$M | $1$ | $523$K ($163$K)
    | $20.0$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| CC12M [[131](#bib.bib131)] |  | 通用 | $12.4$M | $1$ | $523$K ($163$K) | $20.0$
    |'
- en: '| SBU Captions [[4](#bib.bib4)] |  | Generic | $1$M | $1$ | $238$K ($46$K)
    | $12.1$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| SBU Captions [[4](#bib.bib4)] |  | 通用 | $1$M | $1$ | $238$K ($46$K) | $12.1$
    |'
- en: '| VizWiz [[132](#bib.bib132)] |  | Assistive | $70$K | $5$ | $20$K ($8$K) |
    $13.0$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| VizWiz [[132](#bib.bib132)] |  | 辅助 | $70$K | $5$ | $20$K ($8$K) | $13.0$
    |'
- en: '| CUB-200 [[133](#bib.bib133)] |  | Birds | $12$K | $10$ | $6$K ($2$K) | $15.2$
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| CUB-200 [[133](#bib.bib133)] |  | 鸟类 | $12$K | $10$ | $6$K ($2$K) | $15.2$
    |'
- en: '| Oxford-102 [[133](#bib.bib133)] |  | Flowers | $8$K | $10$ | $5$K ($2$K)
    | $14.1$ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Oxford-102 [[133](#bib.bib133)] |  | 花卉 | $8$K | $10$ | $5$K ($2$K) | $14.1$
    |'
- en: '| Fashion Cap. [[134](#bib.bib134)] |  | Fashion | $130$K | $1$ | $17$K ($16$K)
    | $21.0$ |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 时尚标签 [[134](#bib.bib134)] |  | 时尚 | $130$K | $1$ | $17$K ($16$K) | $21.0$
    |'
- en: '| BreakingNews [[135](#bib.bib135)] |  | News | $115$K | $1$ | $85$K ($10$K)
    | $28.1$ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 头条新闻 [[135](#bib.bib135)] |  | 新闻 | $115$K | $1$ | $85$K ($10$K) | $28.1$
    |'
- en: '| GoodNews [[136](#bib.bib136)] |  | News | $466$K | $1$ | $192$K ($54$K) |
    $18.2$ |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 好消息 [[136](#bib.bib136)] |  | 新闻 | $466$K | $1$ | $192$K ($54$K) | $18.2$
    |'
- en: '| TextCaps [[137](#bib.bib137)] |  | OCR | $28$K | $5/6$ | $44$K ($13$K) |
    $12.4$ |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 文本字符 [[137](#bib.bib137)] |  | OCR | $28$K | $5/6$ | $44$K ($13$K) | $12.4$
    |'
- en: '| Loc. Narratives [[138](#bib.bib138)] |  | Generic | $849$K | $1/5$ | $16$K
    ($7$K) | $41.8$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 位置叙述 [[138](#bib.bib138)] |  | 通用 | $849$K | $1/5$ | $16$K ($7$K) | $41.8$
    |'
- en: Early image captioning architectures [[27](#bib.bib27), [28](#bib.bib28), [25](#bib.bib25)]
    were commonly trained and tested on the Flickr30K [[129](#bib.bib129)] and Flickr8K [[19](#bib.bib19)]
    datasets, consisting of pictures collected from the Flickr website, containing
    everyday activities, events, and scenes, paired with five captions each. Currently,
    the most commonly used dataset is Microsoft COCO [[128](#bib.bib128)], which consists
    of images of complex scenes with people, animals, and common everyday objects
    in their context. It contains more than 120,000 images, each annotated with five
    captions, divided into 82,783 images for training and 40,504 for validation. For
    ease of evaluation, most of the literature follows the splits defined by Karpathy *et
    al.* [[25](#bib.bib25)], where 5,000 images of the original validation set are
    used for validation, 5,000 for test, and the rest for training. The dataset has
    also an official test set, composed of 40,775 images paired with 40 private captions
    each, and a public evaluation server²²2[https://competitions.codalab.org/competitions/3221](https://competitions.codalab.org/competitions/3221).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 早期图像描述架构 [[27](#bib.bib27), [28](#bib.bib28), [25](#bib.bib25)] 通常在 Flickr30K [[129](#bib.bib129)]
    和 Flickr8K [[19](#bib.bib19)] 数据集上进行训练和测试，这些数据集包含从 Flickr 网站收集的图片，展示日常活动、事件和场景，每张图片配有五个描述。目前，最常用的数据集是
    Microsoft COCO [[128](#bib.bib128)]，该数据集包含有复杂场景的图像，包括人物、动物和常见的日常物品，且有上下文信息。它包含超过
    120,000 张图像，每张图像都标注有五个描述，分为 82,783 张用于训练和 40,504 张用于验证。为了便于评估，大多数文献遵循 Karpathy
    *et al.* [[25](#bib.bib25)] 定义的分割，其中 5,000 张原始验证集图像用于验证，5,000 张用于测试，其余用于训练。该数据集还有一个官方测试集，由
    40,775 张图像组成，每张图像配有 40 个私有描述，并且有一个公共评估服务器²²2[https://competitions.codalab.org/competitions/3221](https://competitions.codalab.org/competitions/3221)。
- en: 5.1.2 Pre-training datasets
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 预训练数据集
- en: Although training on large well-curated datasets is a sound approach, some works [[99](#bib.bib99),
    [100](#bib.bib100), [94](#bib.bib94), [104](#bib.bib104)] have demonstrated the
    benefits of pre-training on even bigger vision-and-language datasets, which can
    be either image captioning datasets of lower-quality captions or datasets collected
    for other tasks (*e.g.* visual question answering [[100](#bib.bib100), [101](#bib.bib101)],
    text-to-image generation [[139](#bib.bib139)], image-caption association [[93](#bib.bib93)]).
    Among the datasets used for pre-training, that have been specifically collected
    for image captioning, it is worth mentioning SBU Captions [[4](#bib.bib4)], originally
    used for tackling image captioning as a retrieval task [[19](#bib.bib19)], which
    contains around 1 million image-text pairs, collected from the Flickr website.
    Similarly, YFCC100M [[140](#bib.bib140)] is composed of 100 million media objects
    in which 14.8 million images are available with automatically-collected textual
    descriptions. Later, the Conceptual Captions [[130](#bib.bib130), [131](#bib.bib131)]
    datasets have been proposed, which are collections of around 3.3 million (CC3M)
    and 12 million (CC12M) images paired with one weakly-associated description automatically
    collected from the web with a relaxed filtering procedure. Differently from previous
    datasets, Wikipedia-based Image Text (WIT) [[141](#bib.bib141)] provides images
    coming from Wikipedia together with various metadata extracted from the original
    pages, with approximately 5.3 million images available with the corresponding
    descriptions in English. Although the large scale and variety in caption style
    make all these datasets particularly interesting for pre-training, the contained
    captions can be noisy, and the availability of images is not always guaranteed
    since most of them are provided as URLs.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在大型精心策划的数据集上进行训练是一种可靠的方法，但一些研究 [[99](#bib.bib99), [100](#bib.bib100), [94](#bib.bib94),
    [104](#bib.bib104)] 已经展示了在更大规模的视觉-语言数据集上进行预训练的好处，这些数据集可以是质量较低的图像描述数据集或为其他任务（*例如*
    视觉问答 [[100](#bib.bib100), [101](#bib.bib101)], 文本到图像生成 [[139](#bib.bib139)], 图像-描述关联 [[93](#bib.bib93)]）收集的数据集。在用于预训练的特定为图像描述收集的数据集中，值得一提的是SBU
    Captions [[4](#bib.bib4)]，最初用于将图像描述作为检索任务 [[19](#bib.bib19)]，该数据集包含约100万个图像-文本对，收集自Flickr网站。类似地，YFCC100M [[140](#bib.bib140)]由1亿个媒体对象组成，其中1480万张图像配有自动收集的文本描述。随后，提出了Conceptual
    Captions [[130](#bib.bib130), [131](#bib.bib131)] 数据集，这些数据集包含约330万（CC3M）和1200万（CC12M）张图像，每张图像配有一个从网络上自动收集的弱关联描述，并采用了较宽松的过滤程序。不同于之前的数据集，基于维基百科的图像文本（WIT） [[141](#bib.bib141)]
    提供了来自维基百科的图像及其原始页面提取的各种元数据，共有约530万张图像及相应的英文描述。尽管大规模和描述风格的多样性使所有这些数据集对于预训练特别有趣，但包含的描述可能是噪声，并且图像的可用性并不总是有保障，因为大多数图像是通过URL提供的。
- en: Pre-training on such datasets requires significant computational resources and
    effort to collect the data needed. Nevertheless, this strategy represents an asset
    to obtain state-of-the-art performances. Accordingly, some pre-training datasets
    are currently not publicly available, such as ALIGN [[142](#bib.bib142), [94](#bib.bib94)]
    and ALT-200 [[104](#bib.bib104)], respectively containing 1.8 billion and 200
    million noisy image-text pairs, or the datasets used to train DALL-E [[139](#bib.bib139)]
    and CLIP [[93](#bib.bib93)] consisting of 250 and 400 million pairs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在此类数据集上进行预训练需要大量的计算资源和收集所需数据的努力。然而，这一策略代表了获得尖端性能的一个重要资产。因此，一些预训练数据集目前并未公开，例如ALIGN [[142](#bib.bib142),
    [94](#bib.bib94)]和ALT-200 [[104](#bib.bib104)]，分别包含18亿和2亿对噪声图像-文本对，或用于训练DALL-E [[139](#bib.bib139)]和CLIP [[93](#bib.bib93)]的数据集，分别由2.5亿和4亿对组成。
- en: 5.1.3 Domain-specific datasets
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 特定领域数据集
- en: While domain-generic benchmark datasets are important to capture the main aspects
    of the image captioning task, domain-specific datasets are also important to highlight
    and target specific challenges. These may relate to the visual domain (*e.g.* type
    and style of the images) and the semantic domain. In particular, the distribution
    of the terms used to describe domain-specific images can be significantly different
    from that of the terms used for domain-generic images.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管领域通用基准数据集对于捕捉图像描述任务的主要方面很重要，但领域特定的数据集同样重要，以突出和应对特定挑战。这些挑战可能涉及视觉领域（*例如* 图像的类型和风格）和语义领域。特别是，用于描述领域特定图像的术语分布可能与用于描述领域通用图像的术语分布有显著不同。
- en: An example of dataset-specific in terms of the visual domain is the VizWiz Captions [[132](#bib.bib132)]
    dataset, collected to favor the image captioning research towards assistive technologies.
    The images in this dataset have been taken by visually-impaired people with their
    phones, thus, they can be of low quality and concern a wide variety of everyday
    activities, most of which entail reading some text.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉领域，数据集特定的一个例子是 VizWiz Captions [[132](#bib.bib132)] 数据集，该数据集的收集旨在促进图像描述研究以支持辅助技术。该数据集中的图像由视觉障碍人士使用手机拍摄，因此它们可能质量较低，并且涉及各种日常活动，大多数活动都包括阅读一些文字。
- en: Some examples of specific semantic domain are the CUB-200 [[143](#bib.bib143)]
    and the Oxford-102 [[144](#bib.bib144)] datasets, which contain images of birds
    and flowers, respectively, that have been paired with ten captions each by Reed *et
    al.* [[133](#bib.bib133)]. Given the specificity of these datasets, rather than
    for standard image captioning, they are usually adopted for different related
    tasks such as cross-domain captioning [[145](#bib.bib145)], visual explanation
    generation [[146](#bib.bib146), [147](#bib.bib147)], and text-to-image synthesis [[148](#bib.bib148)].
    Another domain-specific dataset is Fashion Captioning [[134](#bib.bib134)] that
    contains images of clothing items in different poses and colors that may share
    the same caption. The vocabulary for describing these images is somewhat smaller
    and more specific than for generic datasets. Differently, datasets as BreakingNews [[135](#bib.bib135)]
    and GoodNews [[136](#bib.bib136)] enforce using a richer vocabulary since their
    images, taken from news articles, have long associated captions written by expert
    journalists. The same applies to the TextCaps [[137](#bib.bib137)] dataset, which
    contains images with text, that must be “read” and included in the caption, and
    to Localized Narratives [[138](#bib.bib138)], whose captions have been collected
    by recording people freely narrating what they see in the images.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特定语义领域的例子包括 CUB-200 [[143](#bib.bib143)] 和 Oxford-102 [[144](#bib.bib144)]
    数据集，这些数据集分别包含鸟类和花卉的图像，每个图像都有 Reed *et al.* [[133](#bib.bib133)] 组合的十个描述。鉴于这些数据集的特定性，它们通常被用于不同的相关任务，如跨领域描述 [[145](#bib.bib145)]、视觉解释生成 [[146](#bib.bib146),
    [147](#bib.bib147)] 和文本到图像合成 [[148](#bib.bib148)]，而非标准图像描述。另一个领域特定的数据集是 Fashion
    Captioning [[134](#bib.bib134)]，它包含不同姿势和颜色的服装图像，这些图像可能共享相同的描述。描述这些图像的词汇比通用数据集要小且更具体。不同的是，像
    BreakingNews [[135](#bib.bib135)] 和 GoodNews [[136](#bib.bib136)] 这样的数据集要求使用更丰富的词汇，因为这些图像来源于新闻文章，附有由专业记者撰写的长描述。TextCaps [[137](#bib.bib137)]
    数据集也是如此，它包含含有文字的图像，这些文字必须被“读取”并包含在描述中，以及 Localized Narratives [[138](#bib.bib138)]，其描述是通过录音记录人们自由叙述他们在图像中看到的内容收集的。
- en: Collecting domain-specific datasets and developing solutions to tackle the challenges
    they pose is crucial to extend the applicability of image captioning algorithms.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 收集特定领域的数据集并开发解决方案来应对它们带来的挑战，对于扩展图像描述算法的适用性至关重要。
- en: 5.2 Evaluation Metrics
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评估指标
- en: 'Evaluating the quality of a generated caption is a tricky and subjective task [[122](#bib.bib122),
    [121](#bib.bib121)], complicated by the fact that captions cannot only be grammatical
    and fluent but need to properly refer to the input image. Arguably, the best way
    to measure the quality of the caption for an image is still carefully designing
    a human evaluation campaign in which multiple users score the produced sentences [[149](#bib.bib149)].
    However, human evaluation is costly and not reproducible – which prevents a fair
    comparison between different approaches. Automatic scoring methods exist that
    are used to assess the quality of system-produced captions, usually by comparing
    them with human-produced reference sentences, although some metrics do not rely
    on reference captions. Table [III](#A1.T3 "TABLE III ‣ Appendix A Further analysis
    of the evaluation metrics ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '评估生成的描述的质量是一项棘手且主观的任务 [[122](#bib.bib122), [121](#bib.bib121)]，其复杂性在于描述不仅需要语法正确且流畅，还需准确指代输入图像。可以说，衡量图像描述质量的最佳方法仍然是精心设计一个人类评估活动，其中多个用户对生成的句子进行评分 [[149](#bib.bib149)]。然而，人类评估费用高且不可重复，这使得不同方法之间的公平比较变得困难。存在自动评分方法，用于评估系统生成的描述的质量，通常通过与人类生成的参考句子进行比较，尽管有些指标不依赖于参考描述。表 [III](#A1.T3
    "TABLE III ‣ Appendix A Further analysis of the evaluation metrics ‣ From Show
    to Tell: A Survey on Deep Learning-based Image Captioning")。'
- en: 5.2.1 Standard evaluation metrics
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 标准评估指标
- en: The first strategy adopted to evaluate image captioning performance consists
    of exploiting metrics designed for NLP tasks. For example, the BLEU score [[117](#bib.bib117)]
    and the METEOR [[150](#bib.bib150)] score were introduced for machine translation.
    The former is based on *n*-gram precision considering *n*-grams up to length four;
    the latter favors the recall of matching unigrams from the candidate and reference
    sentences in their exact form stemmed form and meaning. Moreover, the ROUGE score [[118](#bib.bib118)]
    was designed for summarization and applied also for image captioning in its variant
    considering the longest subsequence of tokens in the same relative order, possibly
    with other tokens in-between, that appears in both candidate and reference caption.
    Later, specific image captioning metrics have been proposed [[122](#bib.bib122),
    [121](#bib.bib121)]. The reference CIDEr score [[122](#bib.bib122)] is based on
    the cosine similarity between the Term Frequency-Inverse Document Frequency weighted
    *n*-grams in the candidate caption and in the set of reference captions associated
    with the image, thus taking into account both precision and recall. The SPICE
    score [[121](#bib.bib121)] considers matching tuples extracted from the candidate
    and the reference (or possibly directly the image) scene graphs, thus favoring
    the semantic content rather than the fluency.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个用于评估图像描述性能的策略是利用为自然语言处理任务设计的度量标准。例如，BLEU分数 [[117](#bib.bib117)] 和 METEOR分数 [[150](#bib.bib150)]
    最初是为机器翻译引入的。前者基于* n *-gram精确度，考虑到长度为四的*n*-gram；后者则偏向于匹配单词在候选句子和参考句子中的精确形式、词干形式和含义的召回。此外，ROUGE分数 [[118](#bib.bib118)]
    是为摘要生成设计的，也被应用于图像描述，其变体考虑了在候选描述和参考描述中都出现的最长子序列，可能有其他令牌在其间。后来，还提出了特定的图像描述度量标准 [[122](#bib.bib122),
    [121](#bib.bib121)]。参考CIDEr分数 [[122](#bib.bib122)] 基于候选描述和与图像相关的参考描述集合中的术语频率-逆文档频率加权*n*-gram之间的余弦相似度，从而考虑了精确度和召回率。SPICE分数 [[121](#bib.bib121)]
    考虑了从候选描述和参考（或可能直接从图像）场景图中提取的匹配元组，从而更加关注语义内容而非流畅性。
- en: 'TABLE II: Performance analysis of representative image captioning approaches
    in terms of different evaluation metrics. The $\dagger$ marker indicates models
    trained by us with ResNet-152 features, while the $\ddagger$ marker indicates
    unofficial implementations. For all the metrics, the higher the value, the better
    ($\uparrow$).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 各种评估度量下的代表性图像描述方法的性能分析。$\dagger$ 标记表示由我们用ResNet-152特征训练的模型，而$\ddagger$标记表示非官方实现。对于所有度量标准，值越高越好（$\uparrow$）。'
- en: '|  |  |  |  | Standard Metrics |  |  | Diversity Metrics |  |  | Embedding-based
    Metrics |  |  | Learning-based Metrics |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 标准度量 |  |  | 多样性度量 |  |  | 基于嵌入的度量 |  |  | 基于学习的度量 |'
- en: '|  |  | #Params (M) |  | B-1 | B-4 | M | R | C | S |  |  | Div-1 | Div-2 |
    Vocab | %Novel |  |  | WMD | Alignment | Coverage |  |  | TIGEr | BERT-S | CLIP-S
    | CLIP-S${}^{\text{Ref}}$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  |  | #参数 (M) |  | B-1 | B-4 | M | R | C | S |  |  | Div-1 | Div-2 | 词汇 |
    %新颖 |  |  | WMD | 对齐 | 覆盖率 |  |  | TIGEr | BERT-S | CLIP-S | CLIP-S${}^{\text{Ref}}$
    |'
- en: '| Show and Tell^† [[23](#bib.bib23)] |  | 13.6 |  | 72.4 | 31.4 | 25.0 | 53.1
    | 97.2 | 18.1 |  |  | 0.014 | 0.045 | 635 | 36.1 |  |  | 16.5 | 0.199 | 71.7 |  |  |
    71.8 | 93.4 | 0.697 | 0.762 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 展示与讲述^† [[23](#bib.bib23)] |  | 13.6 |  | 72.4 | 31.4 | 25.0 | 53.1 | 97.2
    | 18.1 |  |  | 0.014 | 0.045 | 635 | 36.1 |  |  | 16.5 | 0.199 | 71.7 |  |  |
    71.8 | 93.4 | 0.697 | 0.762 |'
- en: '| SCST (FC)^‡ [[38](#bib.bib38)] |  | 13.4 |  | 74.7 | 31.7 | 25.2 | 54.0 |
    104.5 | 18.4 |  |  | 0.008 | 0.023 | 376 | 60.7 |  |  | 16.8 | 0.218 | 74.7 |  |  |
    71.9 | 89.0 | 0.691 | 0.758 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| SCST (FC)^‡ [[38](#bib.bib38)] |  | 13.4 |  | 74.7 | 31.7 | 25.2 | 54.0 |
    104.5 | 18.4 |  |  | 0.008 | 0.023 | 376 | 60.7 |  |  | 16.8 | 0.218 | 74.7 |  |  |
    71.9 | 89.0 | 0.691 | 0.758 |'
- en: '| Show, Attend and Tell^† [[42](#bib.bib42)] |  | 18.1 |  | 74.1 | 33.4 | 26.2
    | 54.6 | 104.6 | 19.3 |  |  | 0.017 | 0.060 | 771 | 47.0 |  |  | 17.6 | 0.209
    | 72.1 |  |  | 73.2 | 93.6 | 0.710 | 0.773 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 展示、关注和讲述^† [[42](#bib.bib42)] |  | 18.1 |  | 74.1 | 33.4 | 26.2 | 54.6 |
    104.6 | 19.3 |  |  | 0.017 | 0.060 | 771 | 47.0 |  |  | 17.6 | 0.209 | 72.1 |  |  |
    73.2 | 93.6 | 0.710 | 0.773 |'
- en: '| SCST (Att2in)^‡ [[38](#bib.bib38)] |  | 14.5 |  | 78.0 | 35.3 | 27.1 | 56.7
    | 117.4 | 20.5 |  |  | 0.010 | 0.031 | 445 | 64.9 |  |  | 18.5 | 0.238 | 76.0
    |  |  | 73.9 | 88.9 | 0.712 | 0.779 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| SCST (Att2in)^‡ [[38](#bib.bib38)] |  | 14.5 |  | 78.0 | 35.3 | 27.1 | 56.7
    | 117.4 | 20.5 |  |  | 0.010 | 0.031 | 445 | 64.9 |  |  | 18.5 | 0.238 | 76.0
    |  |  | 73.9 | 88.9 | 0.712 | 0.779 |'
- en: '| Up-Down^‡ [[58](#bib.bib58)] |  | 52.1 |  | 79.4 | 36.7 | 27.9 | 57.6 | 122.7
    | 21.5 |  |  | 0.012 | 0.044 | 577 | 67.6 |  |  | 19.1 | 0.248 | 76.7 |  |  |
    74.6 | 88.8 | 0.723 | 0.787 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Up-Down^‡ [[58](#bib.bib58)] |  | 52.1 |  | 79.4 | 36.7 | 27.9 | 57.6 | 122.7
    | 21.5 |  |  | 0.012 | 0.044 | 577 | 67.6 |  |  | 19.1 | 0.248 | 76.7 |  |  |
    74.6 | 88.8 | 0.723 | 0.787 |'
- en: '| SGAE [[71](#bib.bib71)] |  | 125.7 |  | 81.0 | 39.0 | 28.4 | 58.9 | 129.1
    | 22.2 |  |  | 0.014 | 0.054 | 647 | 71.4 |  |  | 20.0 | 0.255 | 76.9 |  |  |
    74.6 | 94.1 | 0.734 | 0.796 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| SGAE [[71](#bib.bib71)] |  | 125.7 |  | 81.0 | 39.0 | 28.4 | 58.9 | 129.1
    | 22.2 |  |  | 0.014 | 0.054 | 647 | 71.4 |  |  | 20.0 | 0.255 | 76.9 |  |  |
    74.6 | 94.1 | 0.734 | 0.796 |'
- en: '| MT [[72](#bib.bib72)] |  | 63.2 |  | 80.8 | 38.9 | 28.8 | 58.7 | 129.6 |
    22.3 |  |  | 0.011 | 0.048 | 530 | 70.4 |  |  | 20.2 | 0.253 | 77.0 |  |  | 74.8
    | 88.8 | 0.726 | 0.791 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| MT [[72](#bib.bib72)] |  | 63.2 |  | 80.8 | 38.9 | 28.8 | 58.7 | 129.6 |
    22.3 |  |  | 0.011 | 0.048 | 530 | 70.4 |  |  | 20.2 | 0.253 | 77.0 |  |  | 74.8
    | 88.8 | 0.726 | 0.791 |'
- en: '| AoANet [[79](#bib.bib79)] |  | 87.4 |  | 80.2 | 38.9 | 29.2 | 58.8 | 129.8
    | 22.4 |  |  | 0.016 | 0.062 | 740 | 69.3 |  |  | 20.0 | 0.254 | 77.3 |  |  |
    75.1 | 94.3 | 0.737 | 0.797 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| AoANet [[79](#bib.bib79)] |  | 87.4 |  | 80.2 | 38.9 | 29.2 | 58.8 | 129.8
    | 22.4 |  |  | 0.016 | 0.062 | 740 | 69.3 |  |  | 20.0 | 0.254 | 77.3 |  |  |
    75.1 | 94.3 | 0.737 | 0.797 |'
- en: '| X-LAN [[80](#bib.bib80)] |  | 75.2 |  | 80.8 | 39.5 | 29.5 | 59.2 | 132.0
    | 23.4 |  |  | 0.018 | 0.078 | 858 | 73.9 |  |  | 20.6 | 0.261 | 77.9 |  |  |
    75.4 | 94.3 | 0.746 | 0.803 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| X-LAN [[80](#bib.bib80)] |  | 75.2 |  | 80.8 | 39.5 | 29.5 | 59.2 | 132.0
    | 23.4 |  |  | 0.018 | 0.078 | 858 | 73.9 |  |  | 20.6 | 0.261 | 77.9 |  |  |
    75.4 | 94.3 | 0.746 | 0.803 |'
- en: '| DPA [[83](#bib.bib83)] |  | 111.8 |  | 80.3 | 40.5 | 29.6 | 59.2 | 133.4
    | 23.3 |  |  | 0.019 | 0.079 | 937 | 65.9 |  |  | 20.5 | 0.261 | 77.3 |  |  |
    75.0 | 94.3 | 0.738 | 0.802 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| DPA [[83](#bib.bib83)] |  | 111.8 |  | 80.3 | 40.5 | 29.6 | 59.2 | 133.4
    | 23.3 |  |  | 0.019 | 0.079 | 937 | 65.9 |  |  | 20.5 | 0.261 | 77.3 |  |  |
    75.0 | 94.3 | 0.738 | 0.802 |'
- en: '| AutoCaption [[107](#bib.bib107)] |  | - |  | 81.5 | 40.2 | 29.9 | 59.5 |
    135.8 | 23.8 |  |  | 0.022 | 0.096 | 1064 | 75.8 |  |  | 20.9 | 0.262 | 77.7 |  |  |
    75.4 | 94.3 | 0.752 | 0.808 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| AutoCaption [[107](#bib.bib107)] |  | - |  | 81.5 | 40.2 | 29.9 | 59.5 |
    135.8 | 23.8 |  |  | 0.022 | 0.096 | 1064 | 75.8 |  |  | 20.9 | 0.262 | 77.7 |  |  |
    75.4 | 94.3 | 0.752 | 0.808 |'
- en: '| ORT [[77](#bib.bib77)] |  | 54.9 |  | 80.5 | 38.6 | 28.7 | 58.4 | 128.3 |
    22.6 |  |  | 0.021 | 0.072 | 1002 | 73.8 |  |  | 19.8 | 0.255 | 76.9 |  |  | 75.1
    | 94.1 | 0.736 | 0.796 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| ORT [[77](#bib.bib77)] |  | 54.9 |  | 80.5 | 38.6 | 28.7 | 58.4 | 128.3 |
    22.6 |  |  | 0.021 | 0.072 | 1002 | 73.8 |  |  | 19.8 | 0.255 | 76.9 |  |  | 75.1
    | 94.1 | 0.736 | 0.796 |'
- en: '| CPTR [[92](#bib.bib92)] |  | 138.5 |  | 81.7 | 40.0 | 29.1 | 59.4 | 129.4
    | - |  |  | 0.014 | 0.068 | 667 | 75.6 |  |  | 20.2 | 0.261 | 77.0 |  |  | 74.8
    | 94.3 | 0.745 | 0.802 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| CPTR [[92](#bib.bib92)] |  | 138.5 |  | 81.7 | 40.0 | 29.1 | 59.4 | 129.4
    | - |  |  | 0.014 | 0.068 | 667 | 75.6 |  |  | 20.2 | 0.261 | 77.0 |  |  | 74.8
    | 94.3 | 0.745 | 0.802 |'
- en: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  | 38.4 |  | 80.8 | 39.1
    | 29.2 | 58.6 | 131.2 | 22.6 |  |  | 0.017 | 0.079 | 847 | 78.9 |  |  | 20.3 |
    0.256 | 76.0 |  |  | 75.3 | 93.7 | 0.734 | 0.792 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  | 38.4 |  | 80.8 | 39.1
    | 29.2 | 58.6 | 131.2 | 22.6 |  |  | 0.017 | 0.079 | 847 | 78.9 |  |  | 20.3 |
    0.256 | 76.0 |  |  | 75.3 | 93.7 | 0.734 | 0.792 |'
- en: '| X-Transformer [[80](#bib.bib80)] |  | 137.5 |  | 80.9 | 39.7 | 29.5 | 59.1
    | 132.8 | 23.4 |  |  | 0.018 | 0.081 | 878 | 74.3 |  |  | 20.6 | 0.257 | 77.7
    |  |  | 75.5 | 94.3 | 0.747 | 0.803 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| X-Transformer [[80](#bib.bib80)] |  | 137.5 |  | 80.9 | 39.7 | 29.5 | 59.1
    | 132.8 | 23.4 |  |  | 0.018 | 0.081 | 878 | 74.3 |  |  | 20.6 | 0.257 | 77.7
    |  |  | 75.5 | 94.3 | 0.747 | 0.803 |'
- en: '| Unified VLP [[101](#bib.bib101)] |  | 138.2 |  | 80.9 | 39.5 | 29.3 | 59.6
    | 129.3 | 23.2 |  |  | 0.019 | 0.081 | 898 | 74.1 |  |  | 26.6 | 0.258 | 77.1
    |  |  | 75.1 | 94.4 | 0.750 | 0.807 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Unified VLP [[101](#bib.bib101)] |  | 138.2 |  | 80.9 | 39.5 | 29.3 | 59.6
    | 129.3 | 23.2 |  |  | 0.019 | 0.081 | 898 | 74.1 |  |  | 26.6 | 0.258 | 77.1
    |  |  | 75.1 | 94.4 | 0.750 | 0.807 |'
- en: '| VinVL [[103](#bib.bib103)] |  | 369.6 |  | 82.0 | 41.0 | 31.1 | 60.9 | 140.9
    | 25.2 |  |  | 0.023 | 0.099 | 1125 | 77.9 |  |  | 20.5 | 0.265 | 79.6 |  |  |
    75.7 | 88.5 | 0.766 | 0.820 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| VinVL [[103](#bib.bib103)] |  | 369.6 |  | 82.0 | 41.0 | 31.1 | 60.9 | 140.9
    | 25.2 |  |  | 0.023 | 0.099 | 1125 | 77.9 |  |  | 20.5 | 0.265 | 79.6 |  |  |
    75.7 | 88.5 | 0.766 | 0.820 |'
- en: As expected, metrics designed for image captioning usually correlate better
    with human judgment than those borrowed from other NLP tasks (with the exception
    of METEOR [[150](#bib.bib150)]), both at corpus-level and caption-level [[121](#bib.bib121),
    [151](#bib.bib151), [152](#bib.bib152)]. Correlation with human judgment is measured
    via statistical correlation coefficients (such as Pearson’s, Kendall’s, and Spearman’s
    correlation coefficients) and via the agreement with humans’ preferred caption
    in a pair of candidates, all evaluated on sample captioned images.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，专为图像描述设计的指标通常比从其他自然语言处理任务借用的指标（METEOR [[150](#bib.bib150)]除外）更能与人工判断相符，这在语料库级别和描述级别上都表现得很明显 [[121](#bib.bib121),
    [151](#bib.bib151), [152](#bib.bib152)]。与人工判断的相关性通过统计相关系数（如皮尔逊相关系数、肯德尔相关系数和斯皮尔曼相关系数）以及通过与人类在一对候选描述中偏好的描述的一致性来衡量，所有这些都是在样本描述图像上评估的。
- en: 5.2.2 Diversity metrics
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 多样性指标
- en: To better assess the performance of a captioning system, it is common practice
    to consider a set of the above-mentioned standard metrics. Nevertheless, these
    are somehow gameable because they favor word similarity rather than meaning correctness [[153](#bib.bib153)].
    Another drawback of the standard metrics is that they do not capture (but rather
    disfavor) the desirable capability of the system to produce novel and diverse
    captions, which is more in line with the variability with which humans describe
    complex images. This consideration brought to the development of diversity metrics [[154](#bib.bib154),
    [155](#bib.bib155), [156](#bib.bib156), [157](#bib.bib157)]. Most of these metrics
    can potentially be calculated even when no ground-truth captions are available
    at test time. However, since they overlook the syntactic correctness of the captions
    and their relatedness with the image, it is advisable to combine them with other
    metrics.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地评估字幕系统的性能，通常会考虑上述标准指标的集合。然而，这些指标在某种程度上是可操控的，因为它们偏向词汇相似性而不是意义正确性[[153](#bib.bib153)]。标准指标的另一个缺点是它们未能捕捉（而是偏向于排斥）系统生成新颖且多样化字幕的期望能力，这更符合人类描述复杂图像的变异性。这种考虑导致了多样性指标的发展[[154](#bib.bib154),
    [155](#bib.bib155), [156](#bib.bib156), [157](#bib.bib157)]。这些指标中的大多数即使在测试时没有真实字幕也可能被计算。然而，由于它们忽略了字幕的句法正确性及其与图像的相关性，因此建议将它们与其他指标结合使用。
- en: The overall performance of a captioning system can be evaluated in terms of
    corpus-level diversity or, when the system can output multiple captions for the
    same image, single image-level diversity (termed as *global diversity* and *local
    diversity*, respectively, in [[155](#bib.bib155)]). To quantify the former, it
    can be considered the number of unique words used in all the generated captions
    (Vocab) and the percentage of generated captions that were not present in the
    training set (%Novel). For the latter, it can be used the ratio of unique captions
    unigrams or bigrams to the total number of captions unigrams (Div-1 and Div-2).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 评估字幕系统的整体性能可以从语料库级别的多样性或者当系统能够为同一图像输出多个字幕时的单图像级别多样性（在[[155](#bib.bib155)]中分别称为*全局多样性*和*局部多样性*）来进行。前者可以通过生成的所有字幕中使用的独特词汇数量（词汇表）和生成字幕中不在训练集中的百分比（%Novel）来量化。后者可以通过独特字幕单词或双词组的比例与总字幕单词的比例（Div-1和Div-2）来衡量。
- en: 5.2.3 Embedding-based metrics
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 基于嵌入的指标
- en: An alternative approach to captioning evaluation consists in relying on captions
    semantic similarity or other specific aspects of caption quality, which are estimated
    via embedding-based metrics [[158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)].
    For example, the WMD score [[161](#bib.bib161)], originally introduced to evaluate
    document semantic dissimilarity, can also be applied to captioning evaluation
    by considering generated captions and ground-truth captions as the compared documents [[162](#bib.bib162)].
    Moreover, the Alignment score [[163](#bib.bib163)] is based on the alignment between
    the sequences of nouns in the candidate and reference sentence and captures whether
    concepts are mentioned in a human-like order. Finally, the Coverage score [[84](#bib.bib84),
    [164](#bib.bib164)] expresses the completeness of a caption, which is evaluated
    by considering the mentioned scene visual entities. Since this score considers
    visual objects directly, it can be applied even when no ground-truth caption is
    available.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种字幕评估方法是依赖字幕的语义相似性或其他特定的字幕质量方面，这些方面通过基于嵌入的指标来估计[[158](#bib.bib158), [159](#bib.bib159),
    [160](#bib.bib160)]。例如，WMD分数[[161](#bib.bib161)]，最初用于评估文档语义差异，也可以应用于字幕评估，通过将生成的字幕和真实字幕视为比较的文档[[162](#bib.bib162)]。此外，Alignment分数[[163](#bib.bib163)]基于候选句子和参考句子中名词序列的对齐，并捕捉概念是否以类人方式顺序提及。最后，Coverage分数[[84](#bib.bib84),
    [164](#bib.bib164)]表达了字幕的完整性，通过考虑提到的场景视觉实体来评估。由于该分数直接考虑视觉对象，因此即使没有真实字幕，也可以应用。
- en: 5.2.4 Learning-based evaluation
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 基于学习的评估
- en: As a further development towards captions quality assessment, learning-based
    evaluation strategies [[151](#bib.bib151), [152](#bib.bib152), [165](#bib.bib165),
    [166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)] are being investigated.
    To this end, it can be exploited a component of a complete captioning approach,
    in charge to evaluate the produced caption completeness [[169](#bib.bib169)] or
    how human-like it is [[170](#bib.bib170)]. Alternatively, learning-based evaluation
    is usually based on a pre-trained model. For example, the BERT-S score [[171](#bib.bib171)],
    which is used to evaluate various language generation tasks [[172](#bib.bib172)],
    exploits pre-trained BERT embeddings [[102](#bib.bib102)] to represent and match
    the tokens in the reference and candidate sentences via cosine similarity. Moreover,
    the TIGEr score [[173](#bib.bib173)] represents the reference and candidate captions
    as grounding score vectors obtained from a pre-trained model [[174](#bib.bib174)]
    that grounds their words on the image regions and scores the candidate caption
    based on the similarity of the grounding vectors. Further, the CLIP-S score [[175](#bib.bib175)]
    is a direct application of the CLIP [[93](#bib.bib93)] model to image captioning
    evaluation and consists of an adjusted cosine similarity between image and candidate
    caption representation. Thus, CLIP-S is designed to work without reference captions,
    although the CLIP-S${}^{\text{Ref}}$ variant can exploit also the reference captions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对描述质量评估的进一步发展，正在研究基于学习的评估策略 [[151](#bib.bib151), [152](#bib.bib152), [165](#bib.bib165),
    [166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)]。为此，它可以被利用作为完整描述方法的一个组成部分，负责评估生成描述的完整性 [[169](#bib.bib169)]
    或其类人性 [[170](#bib.bib170)]。此外，基于学习的评估通常基于预训练模型。例如，BERT-S 分数 [[171](#bib.bib171)]
    用于评估各种语言生成任务 [[172](#bib.bib172)]，利用预训练的 BERT 嵌入 [[102](#bib.bib102)] 通过余弦相似度表示和匹配参考和候选句子中的标记。此外，TIGEr
    分数 [[173](#bib.bib173)] 将参考和候选描述表示为从预训练模型 [[174](#bib.bib174)] 获得的基础分数向量，并根据基础向量的相似性对候选描述进行评分。此外，CLIP-S
    分数 [[175](#bib.bib175)] 是对图像描述评估的 CLIP [[93](#bib.bib93)] 模型的直接应用，由图像和候选描述表示之间的调整余弦相似度组成。因此，CLIP-S
    被设计为在没有参考描述的情况下工作，尽管 CLIP-S${}^{\text{Ref}}$ 变体也可以利用参考描述。
- en: 'We refer the reader to Appendix [A](#A1 "Appendix A Further analysis of the
    evaluation metrics ‣ From Show to Tell: A Survey on Deep Learning-based Image
    Captioning") for a deeper discussion on diversity, embedding-based, and learning-based
    metrics.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议读者参考附录 [A](#A1 "附录 A 对评估指标的进一步分析 ‣ 从展示到讲述：基于深度学习的图像描述研究")，以便更深入地讨论多样性、基于嵌入的和基于学习的指标。
- en: '![Refer to caption](img/fb6002dc1d6f9933e588a934bd352b2e.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb6002dc1d6f9933e588a934bd352b2e.png)'
- en: 'Figure 9: Relationship between CIDEr, number of parameters and other scores.
    Values of Div-1 and CLIP-S are multiplied by powers of 10 for readability.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：CIDEr、参数数量和其他分数之间的关系。为了可读性，Div-1 和 CLIP-S 的值被乘以 10 的幂。
- en: 6 Experimental Evaluation
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验评估
- en: 'In Table [II](#S5.T2 "TABLE II ‣ 5.2.1 Standard evaluation metrics ‣ 5.2 Evaluation
    Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), we analyze the performance of some of the main approaches
    in terms of all the evaluation scores presented in Section [5.2](#S5.SS2 "5.2
    Evaluation Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep
    Learning-based Image Captioning") to take into account the different aspects of
    caption quality these express and report their number of parameters to give an
    idea of the computational complexity and memory occupancy of the models. The data
    in the table have been obtained either from the model weights and captions files
    provided by the original authors or from our best implementation. Given its large
    use as a benchmark in the field, we consider the domain-generic COCO dataset also
    for this analysis. In the table, methods are clustered based on the information
    included in the visual encoding and ordered by CIDEr score. It can be observed
    that standard and embedding-based metrics all had a substantial improvement with
    the introduction of region-based visual encodings. Further improvement was due
    to the integration of information on inter-objects relations, either expressed
    via graphs or self-attention. Notably, CIDEr, SPICE, and Coverage most reflect
    the benefit of vision-and-language pre-training. Moreover, as expected, it emerges
    that the diversity-based scores are correlated, especially Div-1 and Div-2 and
    the Vocab Size. The correlation of this family of scores and the others is almost
    linear, except for early approaches, which perform averagely well in terms of
    Diversity despite lower values for standard metrics. From the trend of learning-based
    scores, it emerges that exploiting models trained on textual data only (BERT-S,
    reported in the table as its F1-score variant) does not help discriminating among
    image captioning approaches. On the other hand, considering as reference only
    the visual information and disregarding the ground-truth captions is possible
    with the appropriate vision-and-language pre-trained model (consider that CLIP-S
    and CLIP-S${}^{\text{Ref}}$ are linearly correlated). This is a desirable property
    for an image captioning evaluation score since it allows estimating the performance
    of a model without relying on reference captions that can be limited in number
    and somehow subjective.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [II](#S5.T2 "TABLE II ‣ 5.2.1 Standard evaluation metrics ‣ 5.2 Evaluation
    Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning") 中，我们分析了某些主要方法在第 [5.2](#S5.SS2 "5.2 Evaluation Metrics ‣ 5 Evaluation
    Protocol ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")
    节中所列出的所有评估得分方面的表现，以考虑这些得分所表达的字幕质量的不同方面，并报告它们的参数数量，以提供模型的计算复杂性和内存占用的概念。表格中的数据要么来自原作者提供的模型权重和字幕文件，要么来自我们最佳的实现。鉴于其作为领域基准的广泛使用，我们也将领域通用的
    COCO 数据集考虑在此分析中。在表格中，方法根据包含的视觉编码信息进行聚类，并按 CIDEr 得分排序。可以观察到，标准和基于嵌入的度量在引入基于区域的视觉编码后都有了显著改进。进一步的改进是由于信息的整合，包括对象间关系，无论是通过图形还是自注意力表达。特别地，CIDEr、SPICE
    和 Coverage 最能反映视觉与语言预训练的好处。此外，正如预期的那样，基于多样性的得分之间存在相关性，特别是 Div-1 和 Div-2 以及词汇大小。这类得分与其他得分的相关性几乎是线性的，除了早期的方法，这些方法在多样性方面表现中等，但标准度量值较低。从基于学习的得分趋势来看，仅利用文本数据训练的模型（表中以其
    F1 得分变体表示的 BERT-S）并未能帮助区分图像字幕方法。另一方面，只考虑视觉信息而忽略真实字幕可以通过适当的视觉与语言预训练模型实现（考虑 CLIP-S
    和 CLIP-S${}^{\text{Ref}}$ 是线性相关的）。这对于图像字幕评估得分是一种理想属性，因为它允许在不依赖于可能数量有限且在某种程度上主观的参考字幕的情况下估计模型的性能。'
- en: 'For readability, in Fig. [9](#S5.F9 "Figure 9 ‣ 5.2.4 Learning-based evaluation
    ‣ 5.2 Evaluation Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning") we highlight the relation between the
    CIDEr score and other characteristics from Table [II](#S5.T2 "TABLE II ‣ 5.2.1
    Standard evaluation metrics ‣ 5.2 Evaluation Metrics ‣ 5 Evaluation Protocol ‣
    From Show to Tell: A Survey on Deep Learning-based Image Captioning"). We chose
    CIDEr as this score is commonly regarded as one of the most relevant indicators
    of image captioning systems performance. The first plot, depicting the relation
    between model complexity and performance, shows that more complex models do not
    necessarily bring to better performance. The other plots describe an almost-linear
    relation between CIDEr and the other scores, with some flattening for high CIDEr
    values. These trends confirm the suitability of the CIDEr score as an indicator
    of the overall performance of an image captioning algorithm, whose specific characteristics
    in terms of the produced captions would still be expressed more precisely in terms
    of non-standard metrics.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可读性，在图[9](#S5.F9 "图 9 ‣ 5.2.4 基于学习的评估 ‣ 5.2 评估指标 ‣ 5 评估协议 ‣ 从展示到讲述：深度学习图像描述的调查")中，我们突出了CIDEr评分与表[II](#S5.T2
    "表 II ‣ 5.2.1 标准评估指标 ‣ 5.2 评估指标 ‣ 5 评估协议 ‣ 从展示到讲述：深度学习图像描述的调查")中其他特征之间的关系。我们选择CIDEr是因为这一评分被普遍认为是图像描述系统性能最相关的指标之一。第一个图表显示了模型复杂性与性能之间的关系，表明更复杂的模型并不一定带来更好的性能。其他图表描述了CIDEr与其他评分之间几乎线性的关系，尽管在高CIDEr值时有所平缓。这些趋势证实了CIDEr评分作为图像描述算法总体性能指标的适用性，而其在生成的描述方面的具体特征仍需通过非标准指标更准确地表达。
- en: 'We refer the reader to Appendix [B](#A2 "Appendix B Further Performance Analysis
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning") for additional
    performance analyses and qualitative results.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议读者参阅附录[B](#A2 "附录 B 进一步性能分析 ‣ 从展示到讲述：深度学习图像描述的调查")以获取更多的性能分析和定性结果。
- en: 7 Image Captioning Variants
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 图像描述变体
- en: 'Beyond general-purpose image captioning, several specific sub-tasks have been
    explored in the literature. These can be classified into four categories according
    to their scope: 1\. *dealing with the lack of training data*; 2\. *focusing on
    the visual input*; 3\. *focusing on the textual output*; 4\. *addressing user
    requirements*.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一般图像描述外，文献中还探索了若干具体的子任务。这些子任务可以根据其范围分为四类：1. *处理训练数据不足*；2. *专注于视觉输入*；3. *专注于文本输出*；4.
    *解决用户需求*。
- en: 7.1 Dealing with the lack of training data
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 处理训练数据不足
- en: Paired image-caption datasets are very expensive to obtain. Thus, some image
    captioning variants are being explored that limit the need for full supervision
    information.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 配对的图像-描述数据集获取成本非常高。因此，正在探索一些限制完全监督信息需求的图像描述变体。
- en: Novel Object Captioning. Novel object captioning focuses on describing objects
    not appearing in the training set, thus enabling a zero-shot learning setting
    that can increase the applicability of the models in the real world. Early approaches
    to this task [[176](#bib.bib176), [177](#bib.bib177)] tried to transfer knowledge
    from out-domain images by conditioning the model on external unpaired visual and
    textual data at training time. To explore this strategy, Hendricks *et al.* [[176](#bib.bib176)]
    introduced a variant of the COCO dataset [[128](#bib.bib128)], called *held-out
    COCO*, in which image-caption pairs containing one of eight pre-selected object
    classes were removed from the training set but not from the test set. To further
    encourage research on this task, the more challenging *nocaps* dataset, with nearly
    400 novel objects, has been introduced [[178](#bib.bib178)]. Some approaches to
    this variant [[179](#bib.bib179), [180](#bib.bib180)] integrate copying mechanisms
    in the language model to select novel objects predicted from a tagger or generate
    a caption template with placeholders to be filled with novel objects [[181](#bib.bib181),
    [106](#bib.bib106)]. On a different line, Anderson *et al.* [[182](#bib.bib182)]
    devised the Constrained Beam Search algorithm to force the inclusion of selected
    tag words in the output caption, following the predictions of a tagger. Moreover,
    following the pre-training trend with BERT-like architectures, Hu *et al.* [[183](#bib.bib183)]
    proposed a multi-layer Transformer model pre-trained by randomly masking one or
    more tags from image-tag pairs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖物体描述。新颖物体描述关注于描述训练集中未出现的物体，从而实现零样本学习设置，这可以增加模型在实际应用中的适用性。早期的研究[[176](#bib.bib176),
    [177](#bib.bib177)]尝试通过在训练时将模型条件化于外部未配对的视觉和文本数据，来从域外图像中转移知识。为了探索这种策略，Hendricks
    *等人* [[176](#bib.bib176)] 引入了COCO数据集的一个变体[[128](#bib.bib128)]，称为*held-out COCO*，其中包含八个预选物体类别之一的图像-标题对被从训练集中移除，但未从测试集中移除。为了进一步鼓励对这一任务的研究，已经引入了更具挑战性的*nocaps*
    数据集，该数据集包含近400种新颖物体[[178](#bib.bib178)]。一些针对这种变体的方法[[179](#bib.bib179), [180](#bib.bib180)]在语言模型中集成了复制机制，以选择从标记器预测的新物体，或生成带有占位符的标题模板以填充新物体[[181](#bib.bib181),
    [106](#bib.bib106)]。另一方面，Anderson *等人* [[182](#bib.bib182)] 设计了受限束搜索算法，以强制在输出标题中包含选择的标签词，按照标记器的预测。此外，跟随BERT类似架构的预训练趋势，Hu
    *等人* [[183](#bib.bib183)] 提出了一个多层Transformer模型，通过随机遮盖一个或多个图像-标签对中的标签进行预训练。
- en: Unpaired Image Captioning. Unpaired Image Captioning approaches can be either
    unsupervised or semi-supervised. Unsupervised captioning aims at understanding
    and describing images without paired image-text training data. Following unpaired
    machine translation approaches, the early work [[184](#bib.bib184)] proposes to
    generate captions in a pivot language and then translate predicted captions to
    the target language. After this work, the most common approach focuses on adversarial
    learning by training an LSTM-based discriminator to distinguish whether a caption
    is real or generated [[185](#bib.bib185), [186](#bib.bib186)]. As alternative
    approaches, it is worth mentioning [[187](#bib.bib187)] that generates a caption
    from the image scene-graph and [[188](#bib.bib188)] that leverages a memory-based
    network. Moreover, semi-supervised approaches have been proposed, such as [[189](#bib.bib189)],
    which uses both paired and unpaired data with adversarial learning, and [[190](#bib.bib190)],
    which performs iterative self-learning.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 无配对图像描述。无配对图像描述的方法可以是无监督的或半监督的。无监督描述旨在在没有配对图像-文本训练数据的情况下理解和描述图像。继无配对机器翻译方法之后，早期的研究[[184](#bib.bib184)]建议生成一种中介语言的标题，然后将预测的标题翻译成目标语言。在这项工作之后，最常见的方法集中在对抗学习上，通过训练基于LSTM的判别器来区分标题是真实的还是生成的[[185](#bib.bib185),
    [186](#bib.bib186)]。作为替代方法，值得提及的是[[187](#bib.bib187)]，该方法从图像场景图中生成标题，以及[[188](#bib.bib188)]，它利用了基于记忆的网络。此外，还提出了半监督的方法，例如[[189](#bib.bib189)]，该方法使用配对和无配对数据进行对抗学习，以及[[190](#bib.bib190)]，该方法进行迭代自学习。
- en: Continual Captioning. Continual captioning aims to deal with partially unavailable
    data by following the continual learning paradigm to incrementally learn new tasks
    without forgetting what has been learned before. In this respect, new tasks can
    be represented as sequences of captioning tasks with different vocabularies, as
    proposed in [[191](#bib.bib191)], and the model should be able to transfer visual
    concepts from one to the other while enlarging its vocabulary.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 连续描述。连续描述旨在通过遵循连续学习范式来处理部分不可用的数据，以增量学习新任务而不忘记之前学到的内容。在这方面，新任务可以表示为具有不同词汇的描述任务序列，如[[191](#bib.bib191)]中所提议的，模型应能够在扩展词汇的同时将视觉概念从一个任务转移到另一个任务。
- en: 7.2 Focusing on the visual input
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 关注视觉输入
- en: Some sub-tasks focus on making the textual description more correlated with
    visual data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一些子任务专注于使文本描述与视觉数据更相关。
- en: Dense Captioning. Dense captioning was proposed by Johnson *et al.* [[192](#bib.bib192)]
    and consists of concurrently localizing and describing salient image regions with
    short natural language sentences. In this respect, the task can be conceived as
    a generalization of object detection, where caption replaces object tags, or image
    captioning, where single regions replace the full image. To address this task,
    contextual and global features [[193](#bib.bib193), [194](#bib.bib194)] and attribute
    generators [[195](#bib.bib195), [196](#bib.bib196)] can be exploited. Related
    to this variant, an important line of works [[197](#bib.bib197), [198](#bib.bib198),
    [199](#bib.bib199), [200](#bib.bib200), [66](#bib.bib66), [201](#bib.bib201)]
    focuses on the generation of textual paragraphs that densely describe the visual
    content as a coherent story.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 密集描述。密集描述由Johnson *等人* [[192](#bib.bib192)] 提出，旨在同时定位和描述显著的图像区域，并用简短的自然语言句子进行描述。在这方面，该任务可以被视为对象检测的推广，其中描述替代对象标签，或图像描述，其中单一区域替代整个图像。为了解决这一任务，可以利用上下文和全局特征[[193](#bib.bib193),
    [194](#bib.bib194)]以及属性生成器[[195](#bib.bib195), [196](#bib.bib196)]。与这一变体相关的重要工作[[197](#bib.bib197),
    [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200), [66](#bib.bib66),
    [201](#bib.bib201)] 专注于生成密集描述视觉内容的文本段落，作为一个连贯的故事。
- en: Text-based Image Captioning. Text-based image captioning, also known as OCR-based
    image captioning or image captioning with reading comprehension, aims at reading
    and including the text appearing in images in the generated descriptions. The
    task was introduced by Sidorov *et al.* [[137](#bib.bib137)] with the TextCaps
    dataset. Another dataset designed for pre-training for this variant is *OCR-CC* [[202](#bib.bib202)],
    which is a subset of images containing meaningful text taken from the CC3M dataset [[130](#bib.bib130)]
    and automatically annotated through a commercial OCR system. The common approach
    to this variant entails combining image regions and text tokens, *i.e.* groups
    of characters from an OCR, possibly enriched with mutual spatial information [[203](#bib.bib203),
    [204](#bib.bib204)], in the visual encoding [[137](#bib.bib137), [205](#bib.bib205)].
    Another direction entails generating multiple captions describing different parts
    of the image, including the contained text [[206](#bib.bib206)].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本的图像描述。基于文本的图像描述，也称为OCR基础的图像描述或带有阅读理解的图像描述，旨在读取并包含图像中出现的文本于生成的描述中。该任务由Sidorov
    *等人* [[137](#bib.bib137)] 引入，使用了TextCaps数据集。另一个为这种变体进行预训练的数据集是*OCR-CC* [[202](#bib.bib202)]，它是从CC3M数据集[[130](#bib.bib130)]中提取的包含有意义文本的图像子集，并通过商业OCR系统自动注释。该变体的常见方法包括将图像区域和文本标记，即OCR中的字符组，可能通过相互空间信息[[203](#bib.bib203),
    [204](#bib.bib204)]丰富，结合到视觉编码中[[137](#bib.bib137), [205](#bib.bib205)]。另一个方向是生成描述图像不同部分的多个标题，包括包含的文本[[206](#bib.bib206)]。
- en: Change Captioning. Change captioning targets changes that occurred in a scene,
    thus requiring both accurate change detection and effective natural language description.
    The task was first presented in [[207](#bib.bib207)] with the *Spot-the-Diff*
    dataset, composed of pairs of frames extracted from video surveillance footages
    and the corresponding textual descriptions of visual changes. To further explore
    this variant, the *CLEVR-Change* dataset [[208](#bib.bib208)] has been introduced,
    which contains five scene change types on almost 80K image pairs. The proposed
    approaches for this variant apply attention mechanisms to focus on semantically
    relevant aspects without being deceived by distractors such as viewpoint changes [[209](#bib.bib209),
    [210](#bib.bib210), [211](#bib.bib211)] or perform multi-task learning with image
    retrieval as an auxiliary task [[212](#bib.bib212)], where an image must be retrieved
    from its paired image and the description of the occurred changes.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 变更描述。变更描述针对场景中发生的变化，因此需要准确的变化检测和有效的自然语言描述。该任务首次在 [[207](#bib.bib207)]中提出，使用了*Spot-the-Diff*数据集，由从视频监控画面中提取的帧对和相应的视觉变化文本描述组成。为了进一步探索这一变体，*CLEVR-Change*数据集 [[208](#bib.bib208)]被引入，包含近80K图像对的五种场景变化类型。为这一变体提出的方法应用注意力机制，专注于语义相关方面，而不被如视角变化 [[209](#bib.bib209),
    [210](#bib.bib210), [211](#bib.bib211)]等干扰因素所欺骗，或者执行多任务学习，将图像检索作为辅助任务 [[212](#bib.bib212)]，其中需要从配对图像中检索图像及描述发生的变化。
- en: 7.3 Focusing on the textual output
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 专注于文本输出
- en: Since every image captures a wide variety of entities with complex interactions,
    human descriptions tend to be diverse and grounded to different objects and details.
    Some image captioning variants explicitly focus on these aspects.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每张图像捕捉了多种复杂互动的实体，人类描述往往多样且与不同对象和细节相关联。一些图像描述变体明确关注这些方面。
- en: Diverse Captioning. Diverse image captioning tries to replicate the quality
    and variability of the sentences produced by humans. The most common technique
    to achieve diversity is based on variants of the beam search algorithm [[213](#bib.bib213)]
    that entail dividing the beams into similar groups and encouraging diversity between
    groups. Other solutions have been investigated, such as contrastive learning [[214](#bib.bib214)],
    conditional GANs [[170](#bib.bib170), [154](#bib.bib154)], and paraphrasing [[215](#bib.bib215)].
    However, these solutions tend to underperform in terms of caption quality, which
    is partially recovered by using variational auto-encoders [[216](#bib.bib216),
    [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)]. Another approach
    is exploiting multiple part-of-speech tags sequences predicted from image region
    classes [[220](#bib.bib220)] and forcing the model to produce different captions
    based on these sequences.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 多样化描述。多样化图像描述试图复制人类生成句子的质量和多样性。实现多样性的最常用技术基于 [[213](#bib.bib213)]的束搜索算法变体，这些算法将束划分为相似的组，并鼓励组之间的多样性。还研究了其他解决方案，如对比学习 [[214](#bib.bib214)]，条件生成对抗网络（GANs） [[170](#bib.bib170),
    [154](#bib.bib154)]，以及释义 [[215](#bib.bib215)]。然而，这些解决方案在描述质量方面表现较差，部分通过使用变分自编码器 [[216](#bib.bib216),
    [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)]得到改善。另一种方法是利用从图像区域类别预测的多个词性标记序列 [[220](#bib.bib220)]，并迫使模型基于这些序列生成不同的描述。
- en: Multilingual Captioning. Since image captioning is commonly performed in English,
    multilingual captioning [[221](#bib.bib221)] aims to extend the applicability
    of captioning systems to other languages. The two main strategies entail collecting
    captions in different languages for commonly used datasets (*e.g.* Chinese and
    Japanese captions for COCO images [[222](#bib.bib222), [223](#bib.bib223)], German
    captions for Flick30K [[224](#bib.bib224)]), or directly training multilingual
    captioning systems with unpaired captions [[221](#bib.bib221), [225](#bib.bib225),
    [184](#bib.bib184), [226](#bib.bib226)].
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 多语言描述。由于图像描述通常使用英语，多语言描述 [[221](#bib.bib221)]旨在将描述系统的适用性扩展到其他语言。两种主要策略包括为常用数据集收集不同语言的描述（*例如*，COCO图像的中文和日文描述 [[222](#bib.bib222),
    [223](#bib.bib223)]，Flick30K的德文描述 [[224](#bib.bib224)]），或直接用未配对的描述训练多语言描述系统 [[221](#bib.bib221),
    [225](#bib.bib225), [184](#bib.bib184), [226](#bib.bib226)]。
- en: Application-specific Captioning. Image captioning can be applied to ease and
    automate activities involving text generation from images. For example, captioning
    systems can be applied for medical report generation, for which they need to predict
    disease tags and try to imitate the style of real medical reports [[227](#bib.bib227),
    [228](#bib.bib228), [229](#bib.bib229)]. Another interesting application is art
    description generation, which entails describing not only factual aspects of the
    artworks, but also their context and style, and conveyed message art description [[230](#bib.bib230)].
    To this end, captioning systems could also rely on external knowledge, *e.g.* metadata.
    A similar application is automatic caption generation for news articles [[135](#bib.bib135),
    [136](#bib.bib136)], for which named entities from the article should be described [[231](#bib.bib231),
    [232](#bib.bib232)], and the rich journalistic style should be maintained [[233](#bib.bib233),
    [234](#bib.bib234)]. Another important application domain is assistive technology
    for the visually impaired [[235](#bib.bib235)], where image captioning approaches
    must be able to provide informative descriptions even for low-quality visual inputs [[132](#bib.bib132)].
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 应用特定字幕。图像字幕可以应用于简化和自动化涉及从图像生成文本的活动。例如，字幕系统可以用于医学报告生成，为此它们需要预测疾病标签，并尝试模仿真实医学报告的风格[[227](#bib.bib227),
    [228](#bib.bib228), [229](#bib.bib229)]。另一个有趣的应用是艺术描述生成，它不仅涉及描述艺术作品的事实性方面，还包括其背景和风格，以及传达的信息艺术描述[[230](#bib.bib230)]。为此，字幕系统还可以依赖外部知识，*例如*元数据。类似的应用还有新闻文章的自动字幕生成[[135](#bib.bib135),
    [136](#bib.bib136)]，其中需要描述文章中的命名实体[[231](#bib.bib231), [232](#bib.bib232)]，并且应保持丰富的新闻风格[[233](#bib.bib233),
    [234](#bib.bib234)]。另一个重要的应用领域是针对视力障碍人士的辅助技术[[235](#bib.bib235)]，其中图像字幕方法必须能够为低质量视觉输入提供信息丰富的描述[[132](#bib.bib132)]。
- en: 7.4 Addressing user requirements
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 满足用户需求
- en: Regular image captioning models generate factual captions with a neutral tone
    and no interaction with end-users. Instead, some image captioning sub-tasks are
    devoted to coping with user requests.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 常规图像字幕模型生成中立语调的事实性字幕，并且不与最终用户互动。而一些图像字幕子任务则致力于应对用户请求。
- en: Personalized Captioning. Humans consider more effective the captions that avoid
    stating the obvious and that are written in a style that catches their interest.
    Personalized image captioning aims at fulfilling this requirement by generating
    descriptions that take into account the user’s prior knowledge, active vocabulary,
    and writing style. To this end, early approaches exploit a memory block as a repository
    for this contextual information [[236](#bib.bib236), [237](#bib.bib237)]. On another
    line, Zhang *et al.* [[238](#bib.bib238)] proposed a multi-modal Transformer network
    that personalizes captions conditioned on the user’s recent captions and a learned
    user representation. Other works have instead focused on the style of captions
    as an additional controllable input and proposed to solve this task by exploiting
    unpaired stylized textual corpus [[239](#bib.bib239), [240](#bib.bib240), [241](#bib.bib241),
    [242](#bib.bib242)]. Some datasets have been collected to explore this variant,
    such as *InstaPIC* [[236](#bib.bib236)], which is composed of multiple Instagram
    posts from the same users, *FlickrStyle10K* [[239](#bib.bib239)], which contains
    images and textual sentences with two different styles, and *Personality-Captions* [[243](#bib.bib243)],
    which contains triples of images, captions, and one among 215 personality traits
    to be used to condition the caption generation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化字幕。人们认为避免陈述显而易见的内容并以引人兴趣的风格书写的字幕更有效。个性化图像字幕旨在通过生成考虑到用户先前知识、活跃词汇和写作风格的描述来满足这一要求。为此，早期的方法利用一个记忆块作为此上下文信息的存储库[[236](#bib.bib236),
    [237](#bib.bib237)]。另一方面，张*等人*[[238](#bib.bib238)]提出了一个多模态Transformer网络，该网络根据用户最近的字幕和学习到的用户表示来个性化字幕。其他研究则专注于字幕的风格作为额外的可控输入，并提议通过利用未配对的风格化文本语料库来解决这一任务[[239](#bib.bib239),
    [240](#bib.bib240), [241](#bib.bib241), [242](#bib.bib242)]。一些数据集已被收集以探索这种变体，例如*InstaPIC*[[236](#bib.bib236)]，该数据集由来自同一用户的多个Instagram帖子组成，*FlickrStyle10K*[[239](#bib.bib239)]，其中包含两种不同风格的图像和文本句子，以及*Personality-Captions*[[243](#bib.bib243)]，其中包含图像、字幕和215种个性特质之一的三元组，用于条件生成字幕。
- en: Controllable Captioning. Controllable captioning puts the users in the loop
    by asking them to select and give priorities to what should be described in an
    image. This information is exploited as a guiding signal for the generation process.
    The signal can be sparse, as selected image regions [[244](#bib.bib244), [163](#bib.bib163)]
    and user-provided visual words [[220](#bib.bib220)], or dense, as mouse traces [[138](#bib.bib138),
    [245](#bib.bib245)]. Eventually, the guiding signal can incorporate some form
    of structure, such as sequences that encode the mentioning order of concepts (part-of-speech
    tag as in [[220](#bib.bib220)]) or visual objects [[163](#bib.bib163)]. Guiding
    inputs can also encode the relation between objects that is most of interest for
    the user, as done for example in [[246](#bib.bib246)] via verbs and semantic roles
    (verbs represent activities in the image and semantic roles determine how objects
    engage in these activities) and in [[247](#bib.bib247), [248](#bib.bib248)] via
    user-generated or user-selected scene graphs. A different control signal is introduced
    by [[249](#bib.bib249)], which consist of a length-level embedding added as an
    additional token to each textual word, providing existing models the ability to
    generate length-controllable image captions.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 可控字幕生成。可控字幕生成通过让用户选择和优先描述图像中的内容来将用户纳入流程。这些信息被用作生成过程的指导信号。信号可以是稀疏的，例如选定的图像区域[[244](#bib.bib244),
    [163](#bib.bib163)]和用户提供的视觉词汇[[220](#bib.bib220)]，也可以是密集的，如鼠标轨迹[[138](#bib.bib138),
    [245](#bib.bib245)]。最终，指导信号可以包含某种形式的结构，例如编码概念提及顺序的序列（如[[220](#bib.bib220)]中的词性标记）或视觉对象[[163](#bib.bib163)]。指导输入还可以编码用户最感兴趣的对象之间的关系，例如[[246](#bib.bib246)]中通过动词和语义角色（动词表示图像中的活动，语义角色决定对象如何参与这些活动）以及[[247](#bib.bib247),
    [248](#bib.bib248)]中通过用户生成或用户选择的场景图来实现。另一种控制信号由[[249](#bib.bib249)]引入，它在每个文本词中添加了一个长度级嵌入作为附加标记，为现有模型提供生成长度可控的图像字幕的能力。
- en: Image Captioning Editing. Image captioning editing was proposed by Sammani *et
    al.* [[250](#bib.bib250)], following the consideration that generated captions
    may have repetitions and inconsistencies. This variant focuses on decoupling the
    decoding stage in a caption generation step and a caption polishing one to correct
    syntactic errors.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图像字幕编辑。图像字幕编辑由Sammani *et al.* [[250](#bib.bib250)] 提出，考虑到生成的字幕可能存在重复和不一致。这种变体专注于将解码阶段拆分为字幕生成步骤和字幕润色步骤，以纠正句法错误。
- en: 8 Conclusions and Future Directions
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论与未来方向
- en: Image captioning is an intrinsically complex challenge for machine intelligence
    as it integrates difficulties from both Computer Vision and NLP. Further, as mentioned
    in the Introduction, the task itself is vaguely defined and captions can, in principle,
    be generated with many different styles and objectives. The presented literature
    review and experimental comparison show the performance improvement over the last
    few years on standard datasets. However, many open challenges remain since accuracy,
    robustness, and generalization results are far from satisfactory. Similarly, requirements
    of fidelity, naturalness, and diversity are not yet met. Based on the analysis
    presented, we can trace three main developmental directions for the image captioning
    field, which are discussed in the following.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图像字幕生成对机器智能而言是一个内在复杂的挑战，因为它融合了计算机视觉和自然语言处理的困难。此外，正如引言中提到的，这项任务本身定义模糊，字幕原则上可以用多种不同风格和目标生成。所展示的文献综述和实验比较显示了在标准数据集上近几年性能的提升。然而，许多挑战仍然存在，因为准确性、鲁棒性和泛化结果远未令人满意。同样，忠实性、自然性和多样性的要求尚未满足。基于所呈现的分析，我们可以追溯图像字幕生成领域的三个主要发展方向，这些方向将在下文中讨论。
- en: Procedural and architectural challenges. Since image captioning models are data
    greedy, pre-training on large-scale datasets, even if not well-curated, is becoming
    a solid strategy, as demonstrated in [[100](#bib.bib100), [101](#bib.bib101),
    [103](#bib.bib103), [97](#bib.bib97)]. In this regard, promoting the public release
    of such datasets will be fundamental to fostering reproducibility and allowing
    fair comparisons. The growing size of pre-training models is also a concern, and
    the community will probably need to investigate less computationally-intensive
    alternatives to promote equality in the community. In architectural terms, instead,
    the growing dichotomy between early-fusion strategies and the encoder-decoder
    paradigm is still to be solved and is currently one of the main open issues. On
    the other side, the supremacy of detection features is leaving space to a variety
    of visual encoding strategies (pre-training from scratch, using detections, using
    features from multi-modal models) which all appear to be on pair in terms of performance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 程序性和结构性挑战。由于图像描述模型对数据的需求很大，尽管数据集未经过良好整理，但在大规模数据集上进行预训练已成为一种可靠的策略，这一点在[[100](#bib.bib100),
    [101](#bib.bib101), [103](#bib.bib103), [97](#bib.bib97)]中得到了证明。在这方面，推动这些数据集的公开发布对促进可重复性和允许公平比较至关重要。预训练模型的规模不断增长也是一个问题，社区可能需要研究计算资源消耗较少的替代方案，以促进社区的公平性。在结构性方面，早期融合策略与编码器-解码器范式之间的日益二分仍然是未解决的主要问题之一。另一方面，检测特征的主导地位正在让位于各种视觉编码策略（从头开始预训练、使用检测、使用多模态模型的特征），这些策略在性能方面似乎处于同一水平。
- en: Generalization, diversity, long-tail concepts. While pre-training on web-scale
    datasets provides a promising direction to increase generalization and promoting
    long-tail concepts [[97](#bib.bib97)], specializing in particular domains and
    generating captions with different styles and aims is still among the main open
    challenges for image captioning. Although we discussed some attempts to encourage
    naturalness and diversity [[170](#bib.bib170), [214](#bib.bib214), [216](#bib.bib216)],
    further research is needed to design models that are suitable for real-world applications.
    In this sense, the emergence of models which can deal with long-tail concepts [[97](#bib.bib97),
    [104](#bib.bib104)] offers a valuable promise of modeling real-life scenarios
    and generalizing to different contexts. Additionally, developments in image captioning
    variants such as novel objects captioning or controllable captioning could help
    to tackle this open issue. Notably, the emergence of subword-based tokenization
    techniques has made it possible to handle and generate rare words.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化、多样性、长尾概念。虽然在网络规模的数据集上进行预训练提供了增加泛化和推动长尾概念的有前途的方向[[97](#bib.bib97)]，但专注于特定领域并生成具有不同风格和目标的描述仍然是图像描述中的主要挑战之一。虽然我们讨论了一些鼓励自然性和多样性的尝试[[170](#bib.bib170),
    [214](#bib.bib214), [216](#bib.bib216)]，但仍需进一步研究设计适合实际应用的模型。在这方面，能够处理长尾概念的模型[[97](#bib.bib97),
    [104](#bib.bib104)]的出现为模拟现实生活场景和泛化到不同背景提供了宝贵的希望。此外，图像描述变体的发展，如新颖对象描述或可控描述，可能有助于解决这一开放问题。值得注意的是，基于子词的分词技术的出现使得处理和生成稀有词汇成为可能。
- en: 'Design of trustworthy AI solutions. Due to its potential in human-machine interaction,
    image captioning needs solutions that are transparent and acceptable for end-users,
    framed as overcome bias, and interpretable. Since most vision-and-language datasets
    share common patterns and regularities, datasets bias and overrepresented visual
    concepts are major issues for any vision-and-language task. In this sense, some
    effort should be devoted to the study of fairness and bias: two possible directions
    entail designing specific evaluation metrics and focusing on the robustness to
    unwanted correlations. Further, despite the promising performance on the benchmark
    datasets, state-of-the-art approaches are not yet satisfactory when applied in
    the wild. A possible reason for this is the evaluation procedures used and their
    impact on the training approaches currently adopted. In this sense, the design
    of appropriate and reproducible evaluation protocols [[251](#bib.bib251), [252](#bib.bib252),
    [253](#bib.bib253)] and insightful metrics remains an open challenge in image
    captioning. Moreover, since the task is currently defined as a supervised one
    and thus is strongly influenced by the training data, the development of scores
    that do not need reference captions for assessing the performance would be key
    for a shift towards unsupervised image captioning. Finally, since existing image
    captioning algorithms lack reliable and interpretable means for determining the
    cause of a particular output, further research is needed to shed more light on
    model explainability, focusing on how these deal with different modalities or
    novel concepts.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 设计值得信赖的人工智能解决方案。由于其在人与机器互动中的潜力，图像描述需要透明且可接受的解决方案，这些方案需要克服偏见并具备可解释性。由于大多数视觉与语言数据集共享共同的模式和规律，数据集偏见和过度表示的视觉概念是任何视觉与语言任务中的主要问题。从这个角度来看，应投入一些精力来研究公平性和偏见：两个可能的方向包括设计特定的评估指标和关注对不必要关联的鲁棒性。此外，尽管在基准数据集上的表现令人鼓舞，但现有的最先进方法在实际应用中仍不令人满意。一个可能的原因是使用的评估程序及其对当前采用的训练方法的影响。从这个意义上说，设计适当且可重复的评估协议[[251](#bib.bib251)、[252](#bib.bib252)、[253](#bib.bib253)]和有洞察力的指标仍然是图像描述中的一个未解挑战。此外，由于任务目前被定义为监督任务，因此受到训练数据的强烈影响，开发不需要参考描述来评估性能的分数将是向无监督图像描述转变的关键。最后，由于现有图像描述算法缺乏可靠且可解释的手段来确定特定输出的原因，需要进一步研究以揭示模型的可解释性，关注这些模型如何处理不同的模态或新概念。
- en: Acknowledgments
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank CINECA for providing computational resources. This work has been supported
    by “Fondazione di Modena”, by the “Artificial Intelligence for Cultural Heritage
    (AI4CH)” project, co-funded by the Italian Ministry of Foreign Affairs and International
    Cooperation, and by the H2020 ICT-48-2020 HumanE-AI-NET project. We also want
    to thank the authors who provided us with the captions and model weights for some
    of the surveyed approaches.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 CINECA 提供计算资源。此项工作得到“Modena 基金会”、由意大利外交部和国际合作部共同资助的“文化遗产的人工智能（AI4CH）”项目，以及
    H2020 ICT-48-2020 HumanE-AI-NET 项目的支持。我们还要感谢那些为我们提供了一些调查方法的描述和模型权重的作者。
- en: References
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Ardila, B. Bernal, and M. Rosselli, “Language and visual perception
    associations: meta-analytic connectivity modeling of Brodmann Area 37,” *Behavioural
    Neurology*, 2015.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Ardila、B. Bernal 和 M. Rosselli，“语言与视觉感知关联：Brodmann 37 区的元分析连接建模，” *Behavioural
    Neurology*，2015年。'
- en: '[2] J.-Y. Pan, H.-J. Yang, P. Duygulu, and C. Faloutsos, “Automatic image captioning,”
    in *ICME*, 2004.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J.-Y. Pan、H.-J. Yang、P. Duygulu 和 C. Faloutsos，“自动图像描述，” 发表在*ICME*，2004年。'
- en: '[3] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier,
    and D. Forsyth, “Every picture tells a story: Generating sentences from images,”
    in *ECCV*, 2010.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Farhadi、M. Hejrati、M. A. Sadeghi、P. Young、C. Rashtchian、J. Hockenmaier
    和 D. Forsyth，“每张图片讲述一个故事：从图像生成句子，” 发表在*ECCV*，2010年。'
- en: '[4] V. Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing images using
    1 million captioned photographs,” in *NeurIPS*, 2011.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] V. Ordonez、G. Kulkarni 和 T. Berg， “Im2text: 使用 100 万张带注释的照片描述图像，” 发表在*NeurIPS*，2011年。'
- en: '[5] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and
    T. Mikolov, “DeViSE: a deep visual-semantic embedding model,” in *NeurIPS*, 2013.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Frome、G. S. Corrado、J. Shlens、S. Bengio、J. Dean、M. Ranzato 和 T. Mikolov，“DeViSE:
    深度视觉-语义嵌入模型，” 发表在*NeurIPS*，2013年。'
- en: '[6] R. Kiros, R. Salakhutdinov, and R. S. Zemel, “Unifying visual-semantic
    embeddings with multimodal neural language models,” in *NeurIPS Workshops*, 2014.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] R. Kiros, R. Salakhutdinov 和 R. S. Zemel，"通过多模态神经语言模型统一视觉-语义嵌入"，发表于*NeurIPS
    Workshops*，2014年。'
- en: '[7] A. Karpathy, A. Joulin, and L. Fei-Fei, “Deep fragment embeddings for bidirectional
    image sentence mapping,” in *NeurIPS*, 2014.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Karpathy, A. Joulin 和 L. Fei-Fei，"用于双向图像句子映射的深度片段嵌入"，发表于*NeurIPS*，2014年。'
- en: '[8] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu, “I2T: Image parsing
    to text description,” *Proceedings of the IEEE*, 2010.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] B. Z. Yao, X. Yang, L. Lin, M. W. Lee 和 S.-C. Zhu，"I2T：图像解析到文本描述"，*IEEE会议录*，2010年。'
- en: '[9] A. Aker and R. Gaizauskas, “Generating image descriptions using dependency
    relational patterns,” in *ACL*, 2010.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Aker 和 R. Gaizauskas，"使用依赖关系模式生成图像描述"，发表于*ACL*，2010年。'
- en: '[10] Y. Yang, C. Teo, H. Daumé III, and Y. Aloimonos, “Corpus-guided sentence
    generation of natural images,” in *EMNLP*, 2011.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Yang, C. Teo, H. Daumé III 和 Y. Aloimonos，"基于语料库的自然图像句子生成"，发表于*EMNLP*，2011年。'
- en: '[11] S. Li, G. Kulkarni, T. Berg, A. Berg, and Y. Choi, “Composing simple image
    descriptions using web-scale n-grams,” in *CoNLL*, 2011.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Li, G. Kulkarni, T. Berg, A. Berg 和 Y. Choi，"使用网络规模的n-gram组成简单图像描述"，发表于*CoNLL*，2011年。'
- en: '[12] A. Gupta, Y. Verma, and C. Jawahar, “Choosing linguistics over vision
    to describe images,” in *AAAI*, 2012.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Gupta, Y. Verma 和 C. Jawahar，"选择语言学而非视觉来描述图像"，发表于*AAAI*，2012年。'
- en: '[13] M. Mitchell, J. Dodge, A. Goyal, K. Yamaguchi, K. Stratos, X. Han, A. Mensch,
    A. Berg, T. Berg, and H. Daumé III, “Midge: Generating image descriptions from
    computer vision detections,” in *ACL*, 2012.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Mitchell, J. Dodge, A. Goyal, K. Yamaguchi, K. Stratos, X. Han, A.
    Mensch, A. Berg, T. Berg 和 H. Daumé III，"Midge：从计算机视觉检测生成图像描述"，发表于*ACL*，2012年。'
- en: '[14] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi, A. C. Berg,
    and T. L. Berg, “BabyTalk: Understanding and generating simple image descriptions,”
    *IEEE Trans. PAMI*, 2013.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi, A. C. Berg
    和 T. L. Berg，"BabyTalk：理解和生成简单的图像描述"，*IEEE PAMI*，2013年。'
- en: '[15] P. Kuznetsova, V. Ordonez, T. L. Berg, and Y. Choi, “Treetalk: Composition
    and compression of trees for image descriptions,” *TACL*, vol. 2, pp. 351–362,
    2014.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] P. Kuznetsova, V. Ordonez, T. L. Berg 和 Y. Choi，"Treetalk：图像描述的树的组成与压缩"，*TACL*，第2卷，第351-362页，2014年。'
- en: '[16] R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. Ikizler-Cinbis,
    F. Keller, A. Muscat, and B. Plank, “Automatic description generation from images:
    A survey of models, datasets, and evaluation measures,” *JAIR*, vol. 55, pp. 409–442,
    2016.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. Ikizler-Cinbis,
    F. Keller, A. Muscat 和 B. Plank，"从图像自动生成描述：模型、数据集和评估指标的调查"，*JAIR*，第55卷，第409-442页，2016年。'
- en: '[17] S. Bai and S. An, “A survey on automatic image caption generation,” *Neurocomputing*,
    vol. 311, pp. 291–304, 2018.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Bai 和 S. An，"自动图像描述生成调查"，*神经计算*，第311卷，第291-304页，2018年。'
- en: '[18] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A Comprehensive
    Survey of Deep Learning for Image Captioning,” *ACM Computing Surveys*, vol. 51,
    no. 6, pp. 1–36, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Z. Hossain, F. Sohel, M. F. Shiratuddin 和 H. Laga，"深度学习在图像描述中的全面调查"，*ACM计算机调查*，第51卷，第6期，第1-36页，2019年。'
- en: '[19] M. Hodosh, P. Young, and J. Hockenmaier, “Framing image description as
    a ranking task: Data, models and evaluation metrics,” *JAIR*, 2013.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. Hodosh, P. Young 和 J. Hockenmaier，"将图像描述框架化为排名任务：数据、模型和评估指标"，*JAIR*，2013年。'
- en: '[20] N. Sharif, U. Nadeem, S. A. A. Shah, M. Bennamoun, and W. Liu, “Vision
    to Language: Methods, Metrics and Datasets,” in *Machine Learning Paradigms*,
    2020, pp. 9–62.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] N. Sharif, U. Nadeem, S. A. A. Shah, M. Bennamoun 和 W. Liu，"视觉到语言：方法、度量和数据集"，发表于*Machine
    Learning Paradigms*，2020年，第9-62页。'
- en: '[21] X. Liu, Q. Xu, and N. Wang, “A survey on deep neural network-based image
    captioning,” *The Visual Computer*, vol. 35, no. 3, pp. 445–470, 2019.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] X. Liu, Q. Xu 和 N. Wang，"基于深度神经网络的图像描述调查"，*视觉计算*，第35卷，第3期，第445-470页，2019年。'
- en: '[22] H. Sharma, M. Agrahari, S. K. Singh, M. Firoj, and R. K. Mishra, “Image
    captioning: a comprehensive survey,” in *PARC*, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Sharma, M. Agrahari, S. K. Singh, M. Firoj 和 R. K. Mishra，"图像描述：全面调查"，发表于*PARC*，2020年。'
- en: '[23] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural
    image caption generator,” in *CVPR*, 2015.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] O. Vinyals, A. Toshev, S. Bengio 和 D. Erhan，"展示与讲述：一个神经图像描述生成器"，发表于*CVPR*，2015年。'
- en: '[24] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke 和 A. Rabinovich，"通过卷积深入探讨"，发表于*CVPR*，2015年。'
- en: '[25] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating
    image descriptions,” in *CVPR*, 2015.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Karpathy 和 L. Fei-Fei, “生成图像描述的深度视觉-语义对齐”，发表于 *CVPR*, 2015。'
- en: '[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “深度卷积神经网络的 Imagenet 分类”，发表于
    *NeurIPS*, 2012。'
- en: '[27] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille, “Deep Captioning
    with Multimodal Recurrent Neural Networks (m-RNN),” in *ICLR*, 2015.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, 和 A. Yuille, “基于多模态递归神经网络（m-RNN）的深度描述”，发表于
    *ICLR*, 2015。'
- en: '[28] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
    K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for visual
    recognition and description,” in *CVPR*, 2015.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
    K. Saenko, 和 T. Darrell, “用于视觉识别和描述的长期递归卷积网络”，发表于 *CVPR*, 2015。'
- en: '[29] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络”，发表于 *ICLR*, 2015。'
- en: '[30] X. Chen and C. Lawrence Zitnick, “Mind’s Eye: A Recurrent Visual Representation
    for Image Caption Generation,” in *CVPR*, 2015.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. Chen 和 C. Lawrence Zitnick, “Mind’s Eye: 用于图像描述生成的递归视觉表示”，发表于 *CVPR*,
    2015。'
- en: '[31] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollár, J. Gao,
    X. He, M. Mitchell, J. C. Platt *et al.*, “From captions to visual concepts and
    back,” in *CVPR*, 2015.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollár, J.
    Gao, X. He, M. Mitchell, J. C. Platt *等*，“从描述到视觉概念再到描述”，发表于 *CVPR*, 2015。'
- en: '[32] X. Jia, E. Gavves, B. Fernando, and T. Tuytelaars, “Guiding the Long-Short
    Term Memory model for Image Caption Generation,” in *ICCV*, 2015.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] X. Jia, E. Gavves, B. Fernando, 和 T. Tuytelaars, “引导长短期记忆模型生成图像描述”，发表于
    *ICCV*, 2015。'
- en: '[33] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, “Image captioning with semantic
    attention,” in *CVPR*, 2016.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Q. You, H. Jin, Z. Wang, C. Fang, 和 J. Luo, “具有语义注意力的图像描述”，发表于 *CVPR*,
    2016。'
- en: '[34] Q. Wu, C. Shen, L. Liu, A. Dick, and A. Van Den Hengel, “What Value Do
    Explicit High Level Concepts Have in Vision to Language Problems?” in *CVPR*,
    2016.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Q. Wu, C. Shen, L. Liu, A. Dick, 和 A. Van Den Hengel, “显式高层次概念在视觉到语言问题中的价值是什么？”发表于
    *CVPR*, 2016。'
- en: '[35] J. Gu, G. Wang, J. Cai, and T. Chen, “An Empirical Study of Language CNN
    for Image Captioning,” in *ICCV*, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Gu, G. Wang, J. Cai, 和 T. Chen, “语言 CNN 在图像描述中的实证研究”，发表于 *ICCV*, 2017。'
- en: '[36] F. Chen, R. Ji, J. Su, Y. Wu, and Y. Wu, “StructCap: Structured Semantic
    Embedding for Image Captioning,” in *ACM Multimedia*, 2017.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] F. Chen, R. Ji, J. Su, Y. Wu, 和 Y. Wu, “StructCap: 用于图像描述的结构化语义嵌入”，发表于
    *ACM Multimedia*, 2017。'
- en: '[37] F. Chen, R. Ji, X. Sun, Y. Wu, and J. Su, “GroupCap: Group-based Image
    Captioning with Structured Relevance and Diversity Constraints,” in *CVPR*, 2018.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] F. Chen, R. Ji, X. Sun, Y. Wu, 和 J. Su, “GroupCap: 基于结构化相关性和多样性约束的图像描述”，发表于
    *CVPR*, 2018。'
- en: '[38] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel, “Self-critical
    sequence training for image captioning,” in *CVPR*, 2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, 和 V. Goel, “图像描述的自我批判序列训练”，发表于
    *CVPR*, 2017。'
- en: '[39] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习”，发表于 *CVPR*, 2016。'
- en: '[40] T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei, “Boosting image captioning
    with attributes,” in *ICCV*, 2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] T. Yao, Y. Pan, Y. Li, Z. Qiu, 和 T. Mei, “通过属性提升图像描述”，发表于 *ICCV*, 2017。'
- en: '[41] Z. Gan, C. Gan, X. He, Y. Pu, K. Tran, J. Gao, L. Carin, and L. Deng,
    “Semantic Compositional Networks for Visual Captioning,” in *CVPR*, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Z. Gan, C. Gan, X. He, Y. Pu, K. Tran, J. Gao, L. Carin, 和 L. Deng, “用于视觉描述的语义组合网络”，发表于
    *CVPR*, 2017。'
- en: '[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S.
    Zemel, and Y. Bengio, “Show, attend and tell: Neural image caption generation
    with visual attention,” in *ICML*, 2015.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S.
    Zemel, 和 Y. Bengio, “展示、关注并描述：具有视觉注意力的神经图像描述生成”，发表于 *ICML*, 2015。'
- en: '[43] J. Lu, C. Xiong, D. Parikh, and R. Socher, “Knowing when to look: Adaptive
    attention via a visual sentinel for image captioning,” in *CVPR*, 2017.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Lu, C. Xiong, D. Parikh, 和 R. Socher, “知道何时查看：通过视觉哨兵进行自适应注意力以进行图像描述”，发表于
    *CVPR*, 2017。'
- en: '[44] B. Dai, D. Ye, and D. Lin, “Rethinking the form of latent states in image
    captioning,” in *ECCV*, 2018.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] B. Dai, D. Ye, 和 D. Lin, “重新思考图像描述中潜在状态的形式”，发表于 *ECCV*, 2018。'
- en: '[45] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *ICLR*, 2014.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] D. Bahdanau, K. Cho, 和 Y. Bengio, “通过联合学习对齐和翻译的神经机器翻译”，发表于 *ICLR*, 2014。'
- en: '[46] X. Chen, L. Ma, W. Jiang, J. Yao, and W. Liu, “Regularizing RNNs for Caption
    Generation by Reconstructing The Past with The Present,” in *CVPR*, 2018.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] X. Chen, L. Ma, W. Jiang, J. Yao, 和 W. Liu，“通过用现在重建过去来正则化 RNNs 以生成描述”，发表于
    *CVPR*，2018。'
- en: '[47] Y. Wang, Z. Lin, X. Shen, S. Cohen, and G. W. Cottrell, “Skeleton Key:
    Image Captioning by Skeleton-Attribute Decomposition,” in *CVPR*, 2017.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. Wang, Z. Lin, X. Shen, S. Cohen, 和 G. W. Cottrell，“Skeleton Key：通过骨架-属性分解进行图像描述”，发表于
    *CVPR*，2017。'
- en: '[48] H. Ge, Z. Yan, K. Zhang, M. Zhao, and L. Sun, “Exploring Overall Contextual
    Information for Image Captioning in Human-Like Cognitive Style,” in *ICCV*, 2019.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] H. Ge, Z. Yan, K. Zhang, M. Zhao, 和 L. Sun，“以类似人类的认知风格探索图像描述中的整体上下文信息”，发表于
    *ICCV*，2019。'
- en: '[49] J. Gu, J. Cai, G. Wang, and T. Chen, “Stack-Captioning: Coarse-to-Fine
    Learning for Image Captioning,” in *AAAI*, 2018.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Gu, J. Cai, G. Wang, 和 T. Chen，“堆叠描述：图像描述的粗到细学习”，发表于 *AAAI*，2018。'
- en: '[50] Z. Yang, Y. Yuan, Y. Wu, W. W. Cohen, and R. R. Salakhutdinov, “Review
    Networks for Caption Generation,” in *NeurIPS*, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. Yang, Y. Yuan, Y. Wu, W. W. Cohen, 和 R. R. Salakhutdinov，“用于描述生成的回顾网络”，发表于
    *NeurIPS*，2016。'
- en: '[51] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T.-S. Chua, “SCA-CNN:
    Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning,”
    in *CVPR*, 2017.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, 和 T.-S. Chua，“SCA-CNN：用于图像描述的卷积网络中的空间和通道注意力”，发表于
    *CVPR*，2017。'
- en: '[52] W. Jiang, L. Ma, Y.-G. Jiang, W. Liu, and T. Zhang, “Recurrent Fusion
    Network for Image Captioning,” in *ECCV*, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] W. Jiang, L. Ma, Y.-G. Jiang, W. Liu, 和 T. Zhang，“用于图像描述的递归融合网络”，发表于 *ECCV*，2018。'
- en: '[53] Y. Sugano and A. Bulling, “Seeing with Humans: Gaze-Assisted Neural Image
    Captioning,” *arXiv preprint arXiv:1608.05203*, 2016.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Sugano 和 A. Bulling，“与人类一起观察：注视辅助的神经图像描述”，*arXiv preprint arXiv:1608.05203*，2016。'
- en: '[54] H. R. Tavakoli, R. Shetty, A. Borji, and J. Laaksonen, “Paying attention
    to descriptions generated by image captioning models,” in *ICCV*, 2017.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. R. Tavakoli, R. Shetty, A. Borji, 和 J. Laaksonen，“关注图像描述模型生成的描述”，发表于
    *ICCV*，2017。'
- en: '[55] V. Ramanishka, A. Das, J. Zhang, and K. Saenko, “Top-down visual saliency
    guided by captions,” in *CVPR*, 2017.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] V. Ramanishka, A. Das, J. Zhang, 和 K. Saenko，“由描述引导的自上而下的视觉显著性”，发表于 *CVPR*，2017。'
- en: '[56] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “Paying More Attention
    to Saliency: Image Captioning with Saliency and Context Attention,” *ACM TOMM*,
    vol. 14, no. 2, pp. 1–21, 2018.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M. Cornia, L. Baraldi, G. Serra, 和 R. Cucchiara，“更多关注显著性：带有显著性和上下文注意力的图像描述”，*ACM
    TOMM*，第14卷，第2期，第1–21页，2018。'
- en: '[57] S. Chen and Q. Zhao, “Boosted attention: Leveraging human attention for
    image captioning,” in *ECCV*, 2018.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Chen 和 Q. Zhao，“增强的注意力：利用人类注意力进行图像描述”，发表于 *ECCV*，2018。'
- en: '[58] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
    “Bottom-up and top-down attention for image captioning and visual question answering,”
    in *CVPR*, 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, 和 L. Zhang，“自下而上和自上而下的注意力用于图像描述和视觉问答”，发表于
    *CVPR*，2018。'
- en: '[59] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
    object detection with region proposal networks,” in *NeurIPS*, 2015.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] S. Ren, K. He, R. Girshick, 和 J. Sun，“Faster R-CNN：朝着实时对象检测的区域提议网络”，发表于
    *NeurIPS*，2015。'
- en: '[60] ——, “Faster R-CNN: towards real-time object detection with region proposal
    networks,” *IEEE Trans. PAMI*, vol. 39, no. 6, pp. 1137–1149, 2017.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] ——，“Faster R-CNN：朝着实时对象检测的区域提议网络”，*IEEE Trans. PAMI*，第39卷，第6期，第1137–1149页，2017。'
- en: '[61] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
    Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei, “Visual Genome:
    Connecting Language and Vision Using Crowdsourced Dense Image Annotations,” *IJCV*,
    vol. 123, no. 1, pp. 32–73, 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
    Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, 和 L. Fei-Fei，“Visual Genome：使用众包密集图像注释连接语言和视觉”，*IJCV*，第123卷，第1期，第32–73页，2017。'
- en: '[62] L. Ke, W. Pei, R. Li, X. Shen, and Y.-W. Tai, “Reflective Decoding Network
    for Image Captioning,” in *ICCV*, 2019.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] L. Ke, W. Pei, R. Li, X. Shen, 和 Y.-W. Tai，“用于图像描述的反射解码网络”，发表于 *ICCV*，2019。'
- en: '[63] Y. Qin, J. Du, Y. Zhang, and H. Lu, “Look Back and Predict Forward in
    Image Captioning,” in *CVPR*, 2019.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. Qin, J. Du, Y. Zhang, 和 H. Lu，“回顾并预测图像描述中的前景”，发表于 *CVPR*，2019。'
- en: '[64] L. Huang, W. Wang, Y. Xia, and J. Chen, “Adaptively Aligned Image Captioning
    via Adaptive Attention Time,” in *NeurIPS*, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. Huang, W. Wang, Y. Xia, 和 J. Chen，“通过自适应注意时间进行自适应对齐的图像描述”，发表于 *NeurIPS*，2019。'
- en: '[65] L. Wang, Z. Bai, Y. Zhang, and H. Lu, “Show, Recall, and Tell: Image Captioning
    with Recall Mechanism,” in *AAAI*, 2020.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] L. Wang, Z. Bai, Y. Zhang, 和 H. Lu，“展示、回顾和讲述：带有回顾机制的图像描述”，发表于 *AAAI*，2020。'
- en: '[66] Z.-J. Zha, D. Liu, H. Zhang, Y. Zhang, and F. Wu, “Context-aware visual
    policy network for fine-grained image captioning,” *IEEE Trans. PAMI*, 2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Z.-J. Zha, D. Liu, H. Zhang, Y. Zhang, 和 F. Wu, “基于上下文的视觉策略网络用于细粒度图像描述，”
    *IEEE Trans. PAMI*，2019年。'
- en: '[67] M. Pedersoli, T. Lucas, C. Schmid, and J. Verbeek, “Areas of Attention
    for Image Captioning,” in *ICCV*, 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] M. Pedersoli, T. Lucas, C. Schmid, 和 J. Verbeek, “图像描述中的注意力区域，”在 *ICCV*，2017年。'
- en: '[68] T. Yao, Y. Pan, Y. Li, and T. Mei, “Exploring Visual Relationship for
    Image Captioning,” in *ECCV*, 2018.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] T. Yao, Y. Pan, Y. Li, 和 T. Mei, “探索图像描述中的视觉关系，”在 *ECCV*，2018年。'
- en: '[69] L. Guo, J. Liu, J. Tang, J. Li, W. Luo, and H. Lu, “Aligning linguistic
    words and visual semantic units for image captioning,” in *ACM Multimedia*, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] L. Guo, J. Liu, J. Tang, J. Li, W. Luo, 和 H. Lu, “对齐语言单词和视觉语义单元用于图像描述，”在
    *ACM Multimedia*，2019年。'
- en: '[70] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] T. N. Kipf 和 M. Welling, “基于图卷积网络的半监督分类，”在 *ICLR*，2017年。'
- en: '[71] X. Yang, K. Tang, H. Zhang, and J. Cai, “Auto-Encoding Scene Graphs for
    Image Captioning,” in *CVPR*, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] X. Yang, K. Tang, H. Zhang, 和 J. Cai, “自编码场景图用于图像描述，”在 *CVPR*，2019年。'
- en: '[72] Z. Shi, X. Zhou, X. Qiu, and X. Zhu, “Improving Image Captioning with
    Better Use of Captions,” in *ACL*, 2020.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Z. Shi, X. Zhou, X. Qiu, 和 X. Zhu, “通过更好地利用标题改进图像描述，”在 *ACL*，2020年。'
- en: '[73] T. Yao, Y. Pan, Y. Li, and T. Mei, “Hierarchy Parsing for Image Captioning,”
    in *ICCV*, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] T. Yao, Y. Pan, Y. Li, 和 T. Mei, “图像描述的层次解析，”在 *ICCV*，2019年。'
- en: '[74] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, 2017.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, “注意力机制就是你所需要的，”在 *NeurIPS*，2017年。'
- en: '[75] X. Yang, H. Zhang, and J. Cai, “Learning to Collocate Neural Modules for
    Image Captioning,” in *ICCV*, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] X. Yang, H. Zhang, 和 J. Cai, “学习协作神经模块用于图像描述，”在 *ICCV*，2019年。'
- en: '[76] G. Li, L. Zhu, P. Liu, and Y. Yang, “Entangled Transformer for Image Captioning,”
    in *ICCV*, 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] G. Li, L. Zhu, P. Liu, 和 Y. Yang, “缠结变换器用于图像描述，”在 *ICCV*，2019年。'
- en: '[77] S. Herdade, A. Kappeler, K. Boakye, and J. Soares, “Image Captioning:
    Transforming Objects into Words,” in *NeurIPS*, 2019.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Herdade, A. Kappeler, K. Boakye, 和 J. Soares, “图像描述：将对象转换为文字，”在 *NeurIPS*，2019年。'
- en: '[78] L. Guo, J. Liu, X. Zhu, P. Yao, S. Lu, and H. Lu, “Normalized and Geometry-Aware
    Self-Attention Network for Image Captioning,” in *CVPR*, 2020.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] L. Guo, J. Liu, X. Zhu, P. Yao, S. Lu, 和 H. Lu, “归一化和几何感知自注意力网络用于图像描述，”在
    *CVPR*，2020年。'
- en: '[79] L. Huang, W. Wang, J. Chen, and X.-Y. Wei, “Attention on Attention for
    Image Captioning,” in *ICCV*, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] L. Huang, W. Wang, J. Chen, 和 X.-Y. Wei, “图像描述中的注意力上的注意力，”在 *ICCV*，2019年。'
- en: '[80] Y. Pan, T. Yao, Y. Li, and T. Mei, “X-Linear Attention Networks for Image
    Captioning,” in *CVPR*, 2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Pan, T. Yao, Y. Li, 和 T. Mei, “X-线性注意网络用于图像描述，”在 *CVPR*，2020年。'
- en: '[81] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, “Meshed-Memory
    Transformer for Image Captioning,” in *CVPR*, 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Cornia, M. Stefanini, L. Baraldi, 和 R. Cucchiara, “网状记忆变换器用于图像描述，”在
    *CVPR*，2020年。'
- en: '[82] S. He, W. Liao, H. R. Tavakoli, M. Yang, B. Rosenhahn, and N. Pugeault,
    “Image captioning through image transformer,” in *ACCV*, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] S. He, W. Liao, H. R. Tavakoli, M. Yang, B. Rosenhahn, 和 N. Pugeault,
    “通过图像变换器进行图像描述，”在 *ACCV*，2020年。'
- en: '[83] F. Liu, X. Ren, X. Wu, S. Ge, W. Fan, Y. Zou, and X. Sun, “Prophet Attention:
    Predicting Attention with Future Attention,” in *NeurIPS*, 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] F. Liu, X. Ren, X. Wu, S. Ge, W. Fan, Y. Zou, 和 X. Sun, “预言者注意力：预测未来注意力，”在
    *NeurIPS*，2020年。'
- en: '[84] M. Cornia, L. Baraldi, and R. Cucchiara, “SMArT: Training Shallow Memory-aware
    Transformers for Robotic Explainability,” in *ICRA*, 2020.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] M. Cornia, L. Baraldi, 和 R. Cucchiara, “SMArT：训练浅层记忆感知变换器以实现机器人可解释性，”在
    *ICRA*，2020年。'
- en: '[85] J. Ji, Y. Luo, X. Sun, F. Chen, G. Luo, Y. Wu, Y. Gao, and R. Ji, “Improving
    Image Captioning by Leveraging Intra- and Inter-layer Global Representation in
    Transformer Network,” in *AAAI*, 2021.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Ji, Y. Luo, X. Sun, F. Chen, G. Luo, Y. Wu, Y. Gao, 和 R. Ji, “通过利用变换器网络中的层内和层间全局表示改进图像描述，”在
    *AAAI*，2021年。'
- en: '[86] Y. Luo, J. Ji, X. Sun, L. Cao, Y. Wu, F. Huang, C.-W. Lin, and R. Ji,
    “Dual-Level Collaborative Transformer for Image Captioning,” in *AAAI*, 2021.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Y. Luo, J. Ji, X. Sun, L. Cao, Y. Wu, F. Huang, C.-W. Lin, 和 R. Ji, “双层协作变换器用于图像描述，”在
    *AAAI*，2021年。'
- en: '[87] F. Liu, Y. Liu, X. Ren, X. He, and X. Sun, “Aligning visual regions and
    textual concepts for semantic-grounded image representations,” in *NeurIPS*, 2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] F. Liu, Y. Liu, X. Ren, X. He, 和 X. Sun, “对齐视觉区域和文本概念用于语义驱动的图像表示，”在 *NeurIPS*，2019年。'
- en: '[88] H. Jiang, I. Misra, M. Rohrbach, E. Learned-Miller, and X. Chen, “In defense
    of grid features for visual question answering,” in *CVPR*, 2020.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] H. Jiang, I. Misra, M. Rohrbach, E. Learned-Miller, 和 X. Chen，“为视觉问答防御网格特征，”在
    *CVPR*，2020 年。'
- en: '[89] X. Zhang, X. Sun, Y. Luo, J. Ji, Y. Zhou, Y. Wu, F. Huang, and R. Ji,
    “RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words,” in
    *CVPR*, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] X. Zhang, X. Sun, Y. Luo, J. Ji, Y. Zhou, Y. Wu, F. Huang, 和 R. Ji，“RSTNet：对视觉和非视觉词的自适应注意力图像描述，”在
    *CVPR*，2021 年。'
- en: '[90] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An Image is Worth 16x16
    Words: Transformers for Image Recognition at Scale,” *ICLR*, 2021.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“图像的价值相当于 16x16 个词：用于大规模图像识别的变压器，”
    *ICLR*，2021 年。'
- en: '[91] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,
    “Training data-efficient image transformers & distillation through attention,”
    in *ICML*, 2021.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, 和 H. Jégou，“通过注意力进行数据高效的图像变压器训练和蒸馏，”在
    *ICML*，2021 年。'
- en: '[92] W. Liu, S. Chen, L. Guo, X. Zhu, and J. Liu, “CPTR: Full Transformer Network
    for Image Captioning,” *arXiv preprint arXiv:2101.10804*, 2021.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] W. Liu, S. Chen, L. Guo, X. Zhu, 和 J. Liu，“CPTR：用于图像描述的全变压器网络，” *arXiv
    预印本 arXiv:2101.10804*，2021 年。'
- en: '[93] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning Transferable
    Visual Models From Natural Language Supervision,” *arXiv preprint arXiv:2103.00020*,
    2021.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, 和 I. Sutskever，“从自然语言监督中学习可转移的视觉模型，”
    *arXiv 预印本 arXiv:2103.00020*，2021 年。'
- en: '[94] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao, “SimVLM: Simple
    visual language model pretraining with weak supervision,” *arXiv preprint arXiv:2108.10904*,
    2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, 和 Y. Cao，“SimVLM：带有弱监督的简单视觉语言模型预训练，”
    *arXiv 预印本 arXiv:2108.10904*，2021 年。'
- en: '[95] S. Shen, L. H. Li, H. Tan, M. Bansal, A. Rohrbach, K.-W. Chang, Z. Yao,
    and K. Keutzer, “How Much Can CLIP Benefit Vision-and-Language Tasks?” *arXiv
    preprint arXiv:2107.06383*, 2021.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. Shen, L. H. Li, H. Tan, M. Bansal, A. Rohrbach, K.-W. Chang, Z. Yao,
    和 K. Keutzer，“CLIP 对视觉语言任务的益处有多大？” *arXiv 预印本 arXiv:2107.06383*，2021 年。'
- en: '[96] R. Mokady, A. Hertz, and A. H. Bermano, “ClipCap: CLIP Prefix for Image
    Captioning,” *arXiv preprint arXiv:2111.09734*, 2021.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] R. Mokady, A. Hertz, 和 A. H. Bermano，“ClipCap：用于图像描述的 CLIP 前缀，” *arXiv
    预印本 arXiv:2111.09734*，2021 年。'
- en: '[97] M. Cornia, L. Baraldi, G. Fiameni, and R. Cucchiara, “Universal Captioner:
    Long-Tail Vision-and-Language Model Training through Content-Style Separation,”
    *arXiv preprint arXiv:2111.12727*, 2021.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] M. Cornia, L. Baraldi, G. Fiameni, 和 R. Cucchiara，“Universal Captioner：通过内容-风格分离进行长尾视觉语言模型训练，”
    *arXiv 预印本 arXiv:2111.12727*，2021 年。'
- en: '[98] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder representations
    from transformers,” *EMNLP*, 2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] H. Tan 和 M. Bansal，“Lxmert：从变压器中学习跨模态编码器表示，” *EMNLP*，2019 年。'
- en: '[99] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-agnostic
    visiolinguistic representations for vision-and-language tasks,” in *NeurIPS*,
    2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] J. Lu, D. Batra, D. Parikh, 和 S. Lee，“Vilbert：用于视觉和语言任务的任务无关的视觉语言表示的预训练，”在
    *NeurIPS*，2019 年。'
- en: '[100] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong,
    F. Wei *et al.*, “Oscar: Object-semantics aligned pre-training for vision-language
    tasks,” in *ECCV*, 2020.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong,
    F. Wei *等*，“Oscar：用于视觉语言任务的对象语义对齐预训练，”在 *ECCV*，2020 年。'
- en: '[101] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. J. Corso, and J. Gao, “Unified
    Vision-Language Pre-Training for Image Captioning and VQA,” in *AAAI*, 2020.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. J. Corso, 和 J. Gao，“统一的视觉语言预训练用于图像描述和
    VQA，”在 *AAAI*，2020 年。'
- en: '[102] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” *NAACL*, 2018.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“BERT：深度双向变压器的预训练用于语言理解，”
    *NAACL*，2018 年。'
- en: '[103] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao,
    “VinVL: Revisiting visual representations in vision-language models,” in *CVPR*,
    2021.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, 和 J. Gao，“VinVL：重新审视视觉语言模型中的视觉表示，”在
    *CVPR*，2021 年。'
- en: '[104] X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang, “Scaling
    Up Vision-Language Pre-training for Image Captioning,” *arXiv preprint arXiv:2111.12233*,
    2021.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, 和 L. Wang，“用于图像描述的视觉语言预训练扩展，”
    *arXiv 预印本 arXiv:2111.12233*，2021 年。'
- en: '[105] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] S. Hochreiter 和 J. Schmidhuber，“长短期记忆”，*神经计算*，第9卷，第8期，第1735–1780页，1997年。'
- en: '[106] J. Lu, J. Yang, D. Batra, and D. Parikh, “Neural Baby Talk,” in *CVPR*,
    2018.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. Lu, J. Yang, D. Batra, 和 D. Parikh，“神经宝宝对话”，发表于*CVPR*，2018年。'
- en: '[107] X. Zhu, W. Wang, L. Guo, and J. Liu, “AutoCaption: Image Captioning with
    Neural Architecture Search,” *arXiv preprint arXiv:2012.09742*, 2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] X. Zhu, W. Wang, L. Guo, 和 J. Liu，“AutoCaption：基于神经架构搜索的图像描述”，*arXiv
    预印本 arXiv:2012.09742*，2020年。'
- en: '[108] J. Aneja, A. Deshpande, and A. G. Schwing, “Convolutional image captioning,”
    in *CVPR*, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J. Aneja, A. Deshpande, 和 A. G. Schwing，“卷积图像描述”，发表于*CVPR*，2018年。'
- en: '[109] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving
    language understanding by generative pre-training,” 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] A. Radford, K. Narasimhan, T. Salimans, 和 I. Sutskever，“通过生成预训练提升语言理解”，2018年。'
- en: '[110] Z.-c. Fei, “Fast Image Caption Generation with Position Alignment,” *AAAI
    Workshops*, 2019.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Z.-c. Fei，“带位置对齐的快速图像描述生成”，*AAAI Workshops*，2019年。'
- en: '[111] L. Guo, J. Liu, X. Zhu, X. He, J. Jiang, and H. Lu, “Non-autoregressive
    image captioning with counterfactuals-critical multi-agent learning,” *IJCAI*,
    2020.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] L. Guo, J. Liu, X. Zhu, X. He, J. Jiang, 和 H. Lu，“基于反事实的非自回归图像描述与关键多智能体学习”，*IJCAI*，2020年。'
- en: '[112] Z. Fei, “Iterative Back Modification for Faster Image Captioning,” in
    *ACM Multimedia*, 2020.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Z. Fei，“快速图像描述的迭代回改”，发表于*ACM Multimedia*，2020年。'
- en: '[113] L. Guo, J. Liu, X. Zhu, and H. Lu, “Fast Sequence Generation with Multi-Agent
    Reinforcement Learning,” *arXiv preprint arXiv:2101.09698*, 2021.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] L. Guo, J. Liu, X. Zhu, 和 H. Lu，“利用多智能体强化学习进行快速序列生成”，*arXiv 预印本 arXiv:2101.09698*，2021年。'
- en: '[114] P. Koehn, *Statistical Machine Translation*.   Cambridge University Press,
    2009.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] P. Koehn, *统计机器翻译*。剑桥大学出版社，2009年。'
- en: '[115] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence level training
    with recurrent neural networks,” in *ICLR*, 2016.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] M. Ranzato, S. Chopra, M. Auli, 和 W. Zaremba，“使用递归神经网络的序列级训练”，发表于*ICLR*，2016年。'
- en: '[116] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine Learning*, 1992.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] R. J. Williams，“用于连接主义强化学习的简单统计梯度跟随算法”，*机器学习*，1992年。'
- en: '[117] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method for automatic
    evaluation of machine translation,” in *ACL*, 2002.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] K. Papineni, S. Roukos, T. Ward, 和 W.-J. Zhu，“BLEU：一种自动评估机器翻译的方法”，发表于*ACL*，2002年。'
- en: '[118] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,”
    in *ACL Workshops*, 2004.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] C.-Y. Lin，“Rouge：自动摘要评估包”，发表于*ACL Workshops*，2004年。'
- en: '[119] Z. Ren, X. Wang, N. Zhang, X. Lv, and L.-J. Li, “Deep reinforcement learning-based
    image captioning with embedding reward,” in *CVPR*, 2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. Ren, X. Wang, N. Zhang, X. Lv, 和 L.-J. Li，“基于深度强化学习的图像描述与嵌入奖励”，发表于*CVPR*，2017年。'
- en: '[120] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy, “Improved Image
    Captioning via Policy Gradient Optimization of SPIDEr,” in *ICCV*, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, 和 K. Murphy，“通过SPIDEr的策略梯度优化改进图像描述”，发表于*ICCV*，2017年。'
- en: '[121] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “SPICE: Semantic
    Propositional Image Caption Evaluation,” in *ECCV*, 2016.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] P. Anderson, B. Fernando, M. Johnson, 和 S. Gould，“SPICE：语义命题图像描述评估”，发表于*ECCV*，2016年。'
- en: '[122] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “CIDEr: Consensus-based
    Image Description Evaluation,” in *CVPR*, 2015.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] R. Vedantam, C. Lawrence Zitnick, 和 D. Parikh，“CIDEr：基于共识的图像描述评估”，发表于*CVPR*，2015年。'
- en: '[123] L. Zhang, F. Sung, F. Liu, T. Xiang, S. Gong, Y. Yang, and T. M. Hospedales,
    “Actor-Critic Sequence Training for Image Captioning,” in *NeurIPS*, 2017.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] L. Zhang, F. Sung, F. Liu, T. Xiang, S. Gong, Y. Yang, 和 T. M. Hospedales，“用于图像描述的演员-评论家序列训练”，发表于*NeurIPS*，2017年。'
- en: '[124] J. Gao, S. Wang, S. Wang, S. Ma, and W. Gao, “Self-critical n-step Training
    for Image Captioning,” in *CVPR*, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. Gao, S. Wang, S. Wang, S. Ma, 和 W. Gao，“图像描述的自我批判n步训练”，发表于*CVPR*，2019年。'
- en: '[125] Q. Xia, H. Huang, N. Duan, D. Zhang, L. Ji, Z. Sui, E. Cui, T. Bharti,
    and M. Zhou, “XGPT: Cross-modal Generative Pre-Training for Image Captioning,”
    *arXiv preprint arXiv:2003.01473*, 2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Q. Xia, H. Huang, N. Duan, D. Zhang, L. Ji, Z. Sui, E. Cui, T. Bharti,
    和 M. Zhou，“XGPT：用于图像描述的跨模态生成预训练”，*arXiv 预印本 arXiv:2003.01473*，2020年。'
- en: '[126] K. Desai and J. Johnson, “VirTex: Learning Visual Representations From
    Textual Annotations,” in *CVPR*, 2021.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] K. Desai 和 J. Johnson，“VirTex：从文本注释中学习视觉表示”，发表于*CVPR*，2021年。'
- en: '[127] R. Sennrich, B. Haddow, and A. Birch, “Neural Machine Translation of
    Rare Words with Subword Units,” in *ACL*, 2016.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] R. Sennrich, B. Haddow, 和 A. Birch，“使用子词单元的稀有词神经机器翻译”，发表于*ACL*，2016年。'
- en: '[128] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft COCO: Common Objects in Context,” in *ECCV*, 2014.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P.
    Dollár 和 C. L. Zitnick，“Microsoft COCO: 上下文中的常见物体”，发表于 *ECCV*，2014年。'
- en: '[129] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions
    to visual denotations: New similarity metrics for semantic inference over event
    descriptions,” *TACL*, 2014.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] P. Young, A. Lai, M. Hodosh 和 J. Hockenmaier，“从图像描述到视觉指代：用于事件描述语义推理的新相似性度量”，
    *TACL*，2014年。'
- en: '[130] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions:
    A cleaned, hypernymed, image alt-text dataset for automatic image captioning,”
    in *ACL*, 2018.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] P. Sharma, N. Ding, S. Goodman 和 R. Soricut，“概念性标题：用于自动图像标题生成的清理过的、超义词化的图像备用文本数据集”，发表于
    *ACL*，2018年。'
- en: '[131] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12M: Pushing
    Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts,” in
    *CVPR*, 2021.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] S. Changpinyo, P. Sharma, N. Ding 和 R. Soricut，“Conceptual 12M: 推动Web规模的图像-文本预训练以识别长尾视觉概念”，发表于
    *CVPR*，2021年。'
- en: '[132] D. Gurari, Y. Zhao, M. Zhang, and N. Bhattacharya, “Captioning Images
    Taken by People Who Are Blind,” in *ECCV*, 2020.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] D. Gurari, Y. Zhao, M. Zhang 和 N. Bhattacharya，“为盲人拍摄的图像生成标题”，发表于 *ECCV*，2020年。'
- en: '[133] S. Reed, Z. Akata, H. Lee, and B. Schiele, “Learning deep representations
    of fine-grained visual descriptions,” in *CVPR*, 2016.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] S. Reed, Z. Akata, H. Lee 和 B. Schiele，“学习细粒度视觉描述的深度表示”，发表于 *CVPR*，2016年。'
- en: '[134] X. Yang, H. Zhang, D. Jin, Y. Liu, C.-H. Wu, J. Tan, D. Xie, J. Wang,
    and X. Wang, “Fashion Captioning: Towards Generating Accurate Descriptions with
    Semantic Rewards,” in *ECCV*, 2020.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] X. Yang, H. Zhang, D. Jin, Y. Liu, C.-H. Wu, J. Tan, D. Xie, J. Wang
    和 X. Wang，“时尚标题生成：朝着利用语义奖励生成准确描述的方向前进”，发表于 *ECCV*，2020年。'
- en: '[135] A. Ramisa, F. Yan, F. Moreno-Noguer, and K. Mikolajczyk, “BreakingNews:
    Article Annotation by Image and Text Processing,” *IEEE Trans. PAMI*, vol. 40,
    no. 5, pp. 1072–1085, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Ramisa, F. Yan, F. Moreno-Noguer 和 K. Mikolajczyk，“BreakingNews: 通过图像和文本处理的文章注释”，
    *IEEE Trans. PAMI*，第40卷，第5期，页码1072–1085，2017年。'
- en: '[136] A. F. Biten, L. Gomez, M. Rusinol, and D. Karatzas, “Good news, everyone!
    context driven entity-aware captioning for news images,” in *CVPR*, 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. F. Biten, L. Gomez, M. Rusinol 和 D. Karatzas，“大家好！基于上下文驱动的实体感知新闻图像标题生成”，发表于
    *CVPR*，2019年。'
- en: '[137] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh, “TextCaps: a Dataset for
    Image Captioning with Reading Comprehension,” in *ECCV*, 2020.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] O. Sidorov, R. Hu, M. Rohrbach 和 A. Singh，“TextCaps: 一种用于图像标题生成的阅读理解数据集”，发表于
    *ECCV*，2020年。'
- en: '[138] J. Pont-Tuset, J. Uijlings, S. Changpinyo, R. Soricut, and V. Ferrari,
    “Connecting vision and language with localized narratives,” in *ECCV*, 2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] J. Pont-Tuset, J. Uijlings, S. Changpinyo, R. Soricut 和 V. Ferrari，“通过本地化叙事连接视觉和语言”，发表于
    *ECCV*，2020年。'
- en: '[139] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,
    and I. Sutskever, “Zero-Shot Text-to-Image Generation,” *arXiv preprint arXiv:2102.12092*,
    2021.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen 和
    I. Sutskever，“零样本文本到图像生成”， *arXiv preprint arXiv:2102.12092*，2021年。'
- en: '[140] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland,
    D. Borth, and L.-J. Li, “YFCC100M: The new data in multimedia research,” *Communications
    of the ACM*, vol. 59, no. 2, pp. 64–73, 2016.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland,
    D. Borth 和 L.-J. Li，“YFCC100M: 多媒体研究中的新数据”， *Communications of the ACM*，第59卷，第2期，页码64–73，2016年。'
- en: '[141] K. Srinivasan, K. Raman, J. Chen, M. Bendersky, and M. Najork, “WIT:
    Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning,”
    *arXiv preprint arXiv:2103.01913*, 2021.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] K. Srinivasan, K. Raman, J. Chen, M. Bendersky 和 M. Najork，“WIT: 基于Wikipedia的图像文本数据集用于多模态多语言机器学习”，
    *arXiv preprint arXiv:2103.01913*，2021年。'
- en: '[142] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung,
    Z. Li, and T. Duerig, “Scaling up visual and vision-language representation learning
    with noisy text supervision,” in *ICML*, 2021.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y.
    Sung, Z. Li 和 T. Duerig，“通过噪声文本监督扩展视觉和视觉语言表示学习”，发表于 *ICML*，2021年。'
- en: '[143] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and
    P. Perona, “Caltech-UCSD Birds 200,” California Institute of Technology, Tech.
    Rep., 2010.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie 和 P.
    Perona，“Caltech-UCSD 鸟类200”，加州理工学院，技术报告，2010年。'
- en: '[144] M.-E. Nilsback and A. Zisserman, “Automated flower classification over
    a large number of classes,” in *ICVGIP*, 2008.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] M.-E. Nilsback 和 A. Zisserman， “在大量类别上的自动化花卉分类”，发表于 *ICVGIP*，2008年。'
- en: '[145] T.-H. Chen, Y.-H. Liao, C.-Y. Chuang, W.-T. Hsu, J. Fu, and M. Sun, “Show,
    adapt and tell: Adversarial training of cross-domain image captioner,” in *ICCV*,
    2017.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] T.-H. Chen, Y.-H. Liao, C.-Y. Chuang, W.-T. Hsu, J. Fu, 和 M. Sun，“展示、适应与讲述：跨领域图像描述生成器的对抗训练”，发表于
    *ICCV*，2017年。'
- en: '[146] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, and T. Darrell,
    “Generating visual explanations,” in *ECCV*, 2016.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, 和 T.
    Darrell，“生成视觉解释”，发表于 *ECCV*，2016年。'
- en: '[147] L. A. Hendricks, R. Hu, T. Darrell, and Z. Akata, “Grounding visual explanations,”
    in *ECCV*, 2018.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] L. A. Hendricks, R. Hu, T. Darrell, 和 Z. Akata，“视觉解释的基础”，发表于 *ECCV*，2018年。'
- en: '[148] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, “Generative
    adversarial text to image synthesis,” in *ICML*, 2016.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, 和 H. Lee，“生成对抗文本到图像合成”，发表于
    *ICML*，2016年。'
- en: '[149] J. Kasai, K. Sakaguchi, L. Dunagan, J. Morrison, R. L. Bras, Y. Choi,
    and N. A. Smith, “Transparent human evaluation for image captioning,” *arXiv preprint
    arXiv:2111.08940*, 2021.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] J. Kasai, K. Sakaguchi, L. Dunagan, J. Morrison, R. L. Bras, Y. Choi,
    和 N. A. Smith，“图像描述的透明人类评估”，*arXiv preprint arXiv:2111.08940*，2021年。'
- en: '[150] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT evaluation
    with improved correlation with human judgments,” in *ACL Workshops*, 2005.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Banerjee 和 A. Lavie，“METEOR：一种自动机器翻译评估指标，改进了与人工判断的相关性”，发表于 *ACL Workshops*，2005年。'
- en: '[151] N. Sharif, L. White, M. Bennamoun, and S. A. A. Shah, “NNEval: Neural
    network based evaluation metric for image captioning,” in *ECCV*, 2018.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] N. Sharif, L. White, M. Bennamoun, 和 S. A. A. Shah，“NNEval：基于神经网络的图像描述评估指标”，发表于
    *ECCV*，2018年。'
- en: '[152] Y. Cui, G. Yang, A. Veit, X. Huang, and S. Belongie, “Learning to evaluate
    image captioning,” in *CVPR*, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Y. Cui, G. Yang, A. Veit, X. Huang, 和 S. Belongie，“学习评估图像描述”，发表于 *CVPR*，2018年。'
- en: '[153] O. Caglayan, P. Madhyastha, and L. Specia, “Curious Case of Language
    Generation Evaluation Metrics: A Cautionary Tale,” in *COLING*, 2020.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] O. Caglayan, P. Madhyastha, 和 L. Specia，“语言生成评估指标的奇特案例：一个警示故事”，发表于 *COLING*，2020年。'
- en: '[154] R. Shetty, M. Rohrbach, L. Anne Hendricks, M. Fritz, and B. Schiele,
    “Speaking the same language: Matching machine to human captions by adversarial
    training,” in *ICCV*, 2017.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] R. Shetty, M. Rohrbach, L. Anne Hendricks, M. Fritz, 和 B. Schiele，“说相同的语言：通过对抗训练将机器描述与人类描述匹配”，发表于
    *ICCV*，2017年。'
- en: '[155] E. Van Miltenburg, D. Elliott, and P. Vossen, “Measuring the diversity
    of automatic image descriptions,” in *COLING*, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] E. Van Miltenburg, D. Elliott, 和 P. Vossen，“自动图像描述的多样性测量”，发表于 *COLING*，2018年。'
- en: '[156] Q. Wang and A. B. Chan, “Describing like humans: on diversity in image
    captioning,” in *CVPR*, 2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Q. Wang 和 A. B. Chan，“像人类一样描述：图像描述中的多样性”，发表于 *CVPR*，2019年。'
- en: '[157] Q. Wang, J. Wan, and A. B. Chan, “On Diversity in Image Captioning: Metrics
    and Methods,” *IEEE Trans. PAMI*, 2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Q. Wang, J. Wan, 和 A. B. Chan，“图像描述中的多样性：指标与方法”，*IEEE Trans. PAMI*，2020年。'
- en: '[158] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko, “Object
    Hallucination in Image Captioning,” in *EMNLP*, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, 和 K. Saenko，“图像描述中的对象幻觉”，发表于
    *EMNLP*，2018年。'
- en: '[159] M. Jiang, J. Hu, Q. Huang, L. Zhang, J. Diesner, and J. Gao, “REO-Relevance,
    Extraness, Omission: A Fine-grained Evaluation for Image Captioning,” in *EMNLP-IJCNLP*,
    2019.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] M. Jiang, J. Hu, Q. Huang, L. Zhang, J. Diesner, 和 J. Gao，“REO-相关性、额外性、遗漏：图像描述的细粒度评估”，发表于
    *EMNLP-IJCNLP*，2019年。'
- en: '[160] Z. Wang, B. Feng, K. Narasimhan, and O. Russakovsky, “Towards unique
    and informative captioning of images,” in *ECCV*, 2020.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Wang, B. Feng, K. Narasimhan, 和 O. Russakovsky，“朝着独特且信息丰富的图像描述迈进”，发表于
    *ECCV*，2020年。'
- en: '[161] M. Kusner, Y. Sun, N. Kolkin, and K. Weinberger, “From word embeddings
    to document distances,” in *ICML*, 2015.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] M. Kusner, Y. Sun, N. Kolkin, 和 K. Weinberger，“从词嵌入到文档距离”，发表于 *ICML*，2015年。'
- en: '[162] M. Kilickaya, A. Erdem, N. Ikizler-Cinbis, and E. Erdem, “Re-evaluating
    automatic metrics for image captioning,” in *ACL*, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] M. Kilickaya, A. Erdem, N. Ikizler-Cinbis, 和 E. Erdem，“重新评估图像描述的自动指标”，发表于
    *ACL*，2017年。'
- en: '[163] M. Cornia, L. Baraldi, and R. Cucchiara, “Show, Control and Tell: A Framework
    for Generating Controllable and Grounded Captions,” in *CVPR*, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] M. Cornia, L. Baraldi, 和 R. Cucchiara，“展示、控制与讲述：生成可控且有依据的描述的框架”，发表于 *CVPR*，2019年。'
- en: '[164] R. Bigazzi, F. Landi, M. Cornia, S. Cascianelli, L. Baraldi, and R. Cucchiara,
    “Explore and Explain: Self-supervised Navigation and Recounting,” in *ICPR*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] R. Bigazzi, F. Landi, M. Cornia, S. Cascianelli, L. Baraldi, 和 R. Cucchiara，“探索与解释：自监督导航与叙述”，发表于
    *ICPR*，2020年。'
- en: '[165] H. Lee, S. Yoon, F. Dernoncourt, D. S. Kim, T. Bui, and K. Jung, “ViLBERTScore:
    Evaluating Image Caption Using Vision-and-Language BERT,” in *EMNLP Workshops*,
    2020.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] H. Lee, S. Yoon, F. Dernoncourt, D. S. Kim, T. Bui, 和 K. Jung，“ViLBERTScore:
    使用视觉-语言BERT评估图像描述，”发表于 *EMNLP Workshops*，2020。'
- en: '[166] Y. Yi, H. Deng, and J. Hu, “Improving image captioning evaluation by
    considering inter references variance,” in *ACL*, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Y. Yi, H. Deng, 和 J. Hu，“通过考虑参考间方差来改进图像描述评估，”发表于 *ACL*，2020。'
- en: '[167] S. Wang, Z. Yao, R. Wang, Z. Wu, and X. Chen, “FAIEr: Fidelity and Adequacy
    Ensured Image Caption Evaluation,” in *CVPR*, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] S. Wang, Z. Yao, R. Wang, Z. Wu, 和 X. Chen，“FAIEr: 确保保真度和充分性的图像描述评估，”发表于
    *CVPR*，2021。'
- en: '[168] H. Lee, S. Yoon, F. Dernoncourt, T. Bui, and K. Jung, “UMIC: An Unreferenced
    Metric for Image Captioning via Contrastive Learning,” in *ACL*, 2021.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] H. Lee, S. Yoon, F. Dernoncourt, T. Bui, 和 K. Jung，“UMIC: 一种通过对比学习的无参考图像描述指标，”发表于
    *ACL*，2021。'
- en: '[169] B. Dai, S. Fidler, and D. Lin, “A neural compositional paradigm for image
    captioning,” in *NeurIPS*, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] B. Dai, S. Fidler, 和 D. Lin，“一种神经组合范式用于图像描述，”发表于 *NeurIPS*，2018。'
- en: '[170] B. Dai, S. Fidler, R. Urtasun, and D. Lin, “Towards Diverse and Natural
    Image Descriptions via a Conditional GAN,” in *ICCV*, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] B. Dai, S. Fidler, R. Urtasun, 和 D. Lin，“通过条件GAN实现多样和自然的图像描述，”发表于 *ICCV*，2017。'
- en: '[171] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “BERTScore:
    Evaluating Text Generation with BERT,” in *ICLR*, 2020.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, 和 Y. Artzi，“BERTScore:
    使用BERT评估文本生成，”发表于 *ICLR*，2020。'
- en: '[172] I. J. Unanue, J. Parnell, and M. Piccardi, “BERTTune: Fine-Tuning Neural
    Machine Translation with BERTScore,” *ACL*, 2021.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] I. J. Unanue, J. Parnell, 和 M. Piccardi，“BERTTune: 使用BERTScore对神经机器翻译进行微调，”
    *ACL*，2021。'
- en: '[173] M. Jiang, Q. Huang, L. Zhang, X. Wang, P. Zhang, Z. Gan, J. Diesner,
    and J. Gao, “TIGEr: text-to-image grounding for image caption evaluation,” in
    *ACL*, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] M. Jiang, Q. Huang, L. Zhang, X. Wang, P. Zhang, Z. Gan, J. Diesner,
    和 J. Gao，“TIGEr: 图像描述评估的文本到图像对齐，”发表于 *ACL*，2019。'
- en: '[174] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked Cross Attention
    for Image-Text Matching,” in *ECCV*, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] K.-H. Lee, X. Chen, G. Hua, H. Hu, 和 X. He，“图像-文本匹配的堆叠交叉注意力，”发表于 *ECCV*，2018。'
- en: '[175] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi, “CLIPScore:
    A Reference-free Evaluation Metric for Image Captioning,” *arXiv preprint arXiv:2104.08718*,
    2021.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, 和 Y. Choi，“CLIPScore:
    一种无参考的图像描述评估指标，” *arXiv 预印本 arXiv:2104.08718*，2021。'
- en: '[176] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, and
    T. Darrell, “Deep Compositional Captioning: Describing Novel Object Categories
    without Paired Training Data,” in *CVPR*, 2016.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, 和
    T. Darrell，“深度组合描述：无需配对训练数据描述新颖对象类别，”发表于 *CVPR*，2016。'
- en: '[177] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney, T. Darrell,
    and K. Saenko, “Captioning Images with Diverse Objects,” in *CVPR*, 2017.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney, T. Darrell,
    和 K. Saenko，“具有多样对象的图像描述，”发表于 *CVPR*，2017。'
- en: '[178] H. Agrawal, K. Desai, X. Chen, R. Jain, D. Batra, D. Parikh, S. Lee,
    and P. Anderson, “nocaps: novel object captioning at scale,” in *ICCV*, 2019.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] H. Agrawal, K. Desai, X. Chen, R. Jain, D. Batra, D. Parikh, S. Lee,
    和 P. Anderson，“nocaps: 大规模新颖对象描述，”发表于 *ICCV*，2019。'
- en: '[179] T. Yao, Y. Pan, Y. Li, and T. Mei, “Incorporating Copying Mechanism in
    Image Captioning for Learning Novel Objects,” in *CVPR*, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] T. Yao, Y. Pan, Y. Li, 和 T. Mei，“在图像描述中融入复制机制以学习新颖对象，”发表于 *CVPR*，2017。'
- en: '[180] Y. Li, T. Yao, Y. Pan, H. Chao, and T. Mei, “Pointing Novel Objects in
    Image Captioning,” in *CVPR*, 2019.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Y. Li, T. Yao, Y. Pan, H. Chao, 和 T. Mei，“图像描述中的指向新颖对象，”发表于 *CVPR*，2019。'
- en: '[181] Y. Wu, L. Zhu, L. Jiang, and Y. Yang, “Decoupled Novel Object Captioner,”
    in *ACM Multimedia*, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Y. Wu, L. Zhu, L. Jiang, 和 Y. Yang，“解耦的新颖对象描述器，”发表于 *ACM Multimedia*，2018。'
- en: '[182] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “Guided Open Vocabulary
    Image Captioning with Constrained Beam Search,” in *EMNLP*, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] P. Anderson, B. Fernando, M. Johnson, 和 S. Gould，“使用约束束搜索的引导开放词汇图像描述，”发表于
    *EMNLP*，2017。'
- en: '[183] X. Hu, X. Yin, K. Lin, L. Wang, L. Zhang, J. Gao, and Z. Liu, “VIVO:
    Visual Vocabulary Pre-Training for Novel Object Captioning,” *AAAI*, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] X. Hu, X. Yin, K. Lin, L. Wang, L. Zhang, J. Gao, 和 Z. Liu，“VIVO: 用于新颖对象描述的视觉词汇预训练，”
    *AAAI*，2020。'
- en: '[184] J. Gu, S. Joty, J. Cai, and G. Wang, “Unpaired image captioning by language
    pivoting,” in *ECCV*, 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Gu, S. Joty, J. Cai, 和 G. Wang，“通过语言中介实现无配对图像描述，”发表于 *ECCV*，2018。'
- en: '[185] Y. Feng, L. Ma, W. Liu, and J. Luo, “Unsupervised image captioning,”
    in *CVPR*, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Y. Feng, L. Ma, W. Liu, 和 J. Luo，“无监督图像描述，”发表于 *CVPR*，2019。'
- en: '[186] I. Laina, C. Rupprecht, and N. Navab, “Towards unsupervised image captioning
    with shared multimodal embeddings,” in *ICCV*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Laina, I. Rupprecht, 和 N. Navab, “朝着具有共享多模态嵌入的无监督图像描述”，发表于 *ICCV*，2019年。'
- en: '[187] J. Gu, S. Joty, J. Cai, H. Zhao, X. Yang, and G. Wang, “Unpaired image
    captioning via scene graph alignments,” in *ICCV*, 2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Gu, S. Joty, J. Cai, H. Zhao, X. Yang, 和 G. Wang, “通过场景图对齐进行无配对图像描述”，发表于
    *ICCV*，2019年。'
- en: '[188] D. Guo, Y. Wang, P. Song, and M. Wang, “Recurrent relational memory network
    for unsupervised image captioning,” *IJCAI*, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Guo, Y. Wang, P. Song, 和 M. Wang, “用于无监督图像描述的递归关系记忆网络”，发表于 *IJCAI*，2020年。'
- en: '[189] D.-J. Kim, J. Choi, T.-H. Oh, and I. S. Kweon, “Image Captioning with
    Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach,” in
    *EMNLP*, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Kim, J. Choi, T.-H. Oh, 和 I. S. Kweon, “使用极少监督数据的图像描述：对抗性半监督学习方法”，发表于
    *EMNLP*，2019年。'
- en: '[190] H. Ben, Y. Pan, Y. Li, T. Yao, R. Hong, M. Wang, and T. Mei, “Unpaired
    Image Captioning with Semantic-Constrained Self-Learning,” *IEEE Trans. Multimedia*,
    2021.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Ben, H. Pan, Y. Li, T. Yao, R. Hong, M. Wang, 和 T. Mei, “具有语义约束的自学习的无配对图像描述”，发表于
    *IEEE Trans. Multimedia*，2021年。'
- en: '[191] R. Del Chiaro, B. Twardowski, A. D. Bagdanov, and J. van de Weijer, “RATT:
    Recurrent Attention to Transient Tasks for Continual Image Captioning,” in *NeurIPS*,
    2020.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Del Chiaro, B. Twardowski, A. D. Bagdanov, 和 J. van de Weijer, “RATT：递归注意力用于持续图像描述任务”，发表于
    *NeurIPS*，2020年。'
- en: '[192] J. Johnson, A. Karpathy, and L. Fei-Fei, “DenseCap: Fully convolutional
    Localization Networks for Dense Captioning,” in *CVPR*, 2016.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Johnson, A. Karpathy, 和 L. Fei-Fei, “DenseCap：用于密集描述的全卷积定位网络”，发表于 *CVPR*，2016年。'
- en: '[193] L. Yang, K. Tang, J. Yang, and L.-J. Li, “Dense captioning with joint
    inference and visual context,” in *CVPR*, 2017.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Yang, K. Tang, J. Yang, 和 L.-J. Li, “具有联合推理和视觉上下文的密集描述”，发表于 *CVPR*，2017年。'
- en: '[194] X. Li, S. Jiang, and J. Han, “Learning object context for dense captioning,”
    in *AAAI*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Li, S. Jiang, 和 J. Han, “为密集描述学习对象上下文”，发表于 *AAAI*，2019年。'
- en: '[195] G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, and J. Shao, “Context and attribute
    grounded dense captioning,” in *CVPR*, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Yin, G. Sheng, L. Liu, N. Yu, X. Wang, 和 J. Shao, “上下文和属性驱动的密集描述”，发表于
    *CVPR*，2019年。'
- en: '[196] D.-J. Kim, J. Choi, T.-H. Oh, and I. S. Kweon, “Dense Relational Captioning:
    Triple-Stream Networks for Relationship-Based Captioning,” in *CVPR*, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Kim, D.-J. Choi, T.-H. Oh, 和 I. S. Kweon, “密集关系描述：基于关系的三流网络”，发表于 *CVPR*，2019年。'
- en: '[197] J. Krause, J. Johnson, R. Krishna, and L. Fei-Fei, “A hierarchical approach
    for generating descriptive image paragraphs,” in *CVPR*, 2017.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Krause, J. Johnson, R. Krishna, 和 L. Fei-Fei, “生成描述性图像段落的层次方法”，发表于 *CVPR*，2017年。'
- en: '[198] X. Liang, Z. Hu, H. Zhang, C. Gan, and E. P. Xing, “Recurrent topic-transition
    GAN for visual paragraph generation,” in *ICCV*, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Liang, X. Hu, H. Zhang, C. Gan, 和 E. P. Xing, “用于视觉段落生成的递归主题过渡GAN”，发表于
    *ICCV*，2017年。'
- en: '[199] Y. Mao, C. Zhou, X. Wang, and R. Li, “Show and Tell More: Topic-Oriented
    Multi-Sentence Image Captioning,” in *IJCAI*, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Mao, C. Zhou, X. Wang, 和 R. Li, “展示更多：面向主题的多句图像描述”，发表于 *IJCAI*，2018年。'
- en: '[200] M. Chatterjee and A. G. Schwing, “Diverse and coherent paragraph generation
    from images,” in *ECCV*, 2018.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] Chatterjee, M. 和 A. G. Schwing, “从图像生成多样且连贯的段落”，发表于 *ECCV*，2018年。'
- en: '[201] Y. Luo, Z. Huang, Z. Zhang, Z. Wang, J. Li, and Y. Yang, “Curiosity-driven
    reinforcement learning for diverse visual paragraph generation,” in *ACM Multimedia*,
    2019.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Luo, Y. Huang, Z. Zhang, Z. Wang, J. Li, 和 Y. Yang, “基于好奇心驱动的强化学习生成多样的视觉段落”，发表于
    *ACM Multimedia*，2019年。'
- en: '[202] Z. Yang, Y. Lu, J. Wang, X. Yin, D. Florencio, L. Wang, C. Zhang, L. Zhang,
    and J. Luo, “TAP: Text-Aware Pre-training for Text-VQA and Text-Caption,” in *CVPR*,
    2021.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Yang, Y. Lu, J. Wang, X. Yin, D. Florencio, L. Wang, C. Zhang, L. Zhang,
    和 J. Luo, “TAP：用于文本VQA和文本描述的文本感知预训练”，发表于 *CVPR*，2021年。'
- en: '[203] J. Wang, J. Tang, and J. Luo, “Multimodal Attention with Image Text Spatial
    Relationship for OCR-Based Image Captioning,” in *ACM Multimedia*, 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Wang, J. Tang, 和 J. Luo, “结合图像文本空间关系的多模态注意力用于基于OCR的图像描述”，发表于 *ACM Multimedia*，2020年。'
- en: '[204] J. Wang, J. Tang, M. Yang, X. Bai, and J. Luo, “Improving OCR-based Image
    Captioning by Incorporating Geometrical Relationship,” in *CVPR*, 2021.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Wang, J. Tang, M. Yang, X. Bai, 和 J. Luo, “通过结合几何关系改善基于OCR的图像描述”，发表于
    *CVPR*，2021年。'
- en: '[205] Q. Zhu, C. Gao, P. Wang, and Q. Wu, “Simple is not Easy: A Simple Strong
    Baseline for TextVQA and TextCaps,” in *AAAI*, 2021.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Zhu, C. Gao, P. Wang, 和 Q. Wu, “简单并不容易：一个简单且强大的文本VQA和TextCaps基线”，发表于
    *AAAI*，2021年。'
- en: '[206] G. Xu, S. Niu, M. Tan, Y. Luo, Q. Du, and Q. Wu, “Towards Accurate Text-based
    Image Captioning with Content Diversity Exploration,” in *CVPR*, 2021.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] G. Xu, S. Niu, M. Tan, Y. Luo, Q. Du, 和 Q. Wu, “朝着准确的基于文本的图像标题生成与内容多样性探索，”发表于*CVPR*，2021年。'
- en: '[207] H. Jhamtani and T. Berg-Kirkpatrick, “Learning to describe differences
    between pairs of similar images,” in *EMNLP*, 2018.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] H. Jhamtani 和 T. Berg-Kirkpatrick, “学习描述相似图像对之间的差异，”发表于*EMNLP*，2018年。'
- en: '[208] D. H. Park, T. Darrell, and A. Rohrbach, “Robust Change Captioning,”
    in *CVPR*, 2019.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] D. H. Park, T. Darrell, 和 A. Rohrbach, “鲁棒的变化标题生成，”发表于*CVPR*，2019年。'
- en: '[209] X. Shi, X. Yang, J. Gu, S. Joty, and J. Cai, “Finding It at Another Side:
    A Viewpoint-Adapted Matching Encoder for Change Captioning,” in *ECCV*, 2020.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] X. Shi, X. Yang, J. Gu, S. Joty, 和 J. Cai, “在另一侧找到它：用于变化标题生成的视角自适应匹配编码器，”发表于*ECCV*，2020年。'
- en: '[210] Q. Huang, Y. Liang, J. Wei, C. Yi, H. Liang, H.-f. Leung, and Q. Li,
    “Image Difference Captioning with Instance-Level Fine-Grained Feature Representation,”
    *IEEE Trans. Multimedia*, 2021.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Q. Huang, Y. Liang, J. Wei, C. Yi, H. Liang, H.-f. Leung, 和 Q. Li, “实例级细粒度特征表示的图像差异标题生成，”*IEEE
    Trans. Multimedia*，2021年。'
- en: '[211] H. Kim, J. Kim, H. Lee, H. Park, and G. Kim, “Viewpoint-Agnostic Change
    Captioning With Cycle Consistency,” in *ICCV*, 2021.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] H. Kim, J. Kim, H. Lee, H. Park, 和 G. Kim, “视角无关的变化标题生成与循环一致性，”发表于*ICCV*，2021年。'
- en: '[212] M. Hosseinzadeh and Y. Wang, “Image Change Captioning by Learning from
    an Auxiliary Task,” in *CVPR*, 2021.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] M. Hosseinzadeh 和 Y. Wang, “通过学习辅助任务进行图像变化标题生成，”发表于*CVPR*，2021年。'
- en: '[213] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun, S. Lee, D. J.
    Crandall, and D. Batra, “Diverse Beam Search for Improved Description of Complex
    Scenes,” in *AAAI*, 2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun, S. Lee, D. J.
    Crandall, 和 D. Batra, “改进复杂场景描述的多样化束搜索，”发表于*AAAI*，2018年。'
- en: '[214] B. Dai and D. Lin, “Contrastive learning for image captioning,” in *NeurIPS*,
    2017.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] B. Dai 和 D. Lin, “图像标题生成的对比学习，”发表于*NeurIPS*，2017年。'
- en: '[215] L. Liu, J. Tang, X. Wan, and Z. Guo, “Generating diverse and descriptive
    image captions using visual paraphrases,” in *ICCV*, 2019.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] L. Liu, J. Tang, X. Wan, 和 Z. Guo, “使用视觉同义句生成多样且描述性的图像标题，”发表于*ICCV*，2019年。'
- en: '[216] L. Wang, A. G. Schwing, and S. Lazebnik, “Diverse and accurate image
    description using a variational auto-encoder with an additive gaussian encoding
    space,” in *NeurIPS*, 2017.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] L. Wang, A. G. Schwing, 和 S. Lazebnik, “使用具有加性高斯编码空间的变分自编码器进行多样且准确的图像描述，”发表于*NeurIPS*，2017年。'
- en: '[217] J. Aneja, H. Agrawal, D. Batra, and A. Schwing, “Sequential latent spaces
    for modeling the intention during diverse image captioning,” in *ICCV*, 2019.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] J. Aneja, H. Agrawal, D. Batra, 和 A. Schwing, “用于建模多样化图像标题生成中的意图的序列潜空间，”发表于*ICCV*，2019年。'
- en: '[218] F. Chen, R. Ji, J. Ji, X. Sun, B. Zhang, X. Ge, Y. Wu, F. Huang, and
    Y. Wang, “Variational structured semantic inference for diverse image captioning,”
    in *NeurIPS*, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] F. Chen, R. Ji, J. Ji, X. Sun, B. Zhang, X. Ge, Y. Wu, F. Huang, 和 Y.
    Wang, “用于多样化图像标题生成的变分结构语义推理，”发表于*NeurIPS*，2019年。'
- en: '[219] S. Mahajan and S. Roth, “Diverse image captioning with context-object
    split latent spaces,” in *NeurIPS*, 2020.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] S. Mahajan 和 S. Roth, “具有上下文-对象分裂潜空间的多样化图像标题生成，”发表于*NeurIPS*，2020年。'
- en: '[220] A. Deshpande, J. Aneja, L. Wang, A. G. Schwing, and D. Forsyth, “Fast,
    diverse and accurate image captioning guided by part-of-speech,” in *CVPR*, 2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] A. Deshpande, J. Aneja, L. Wang, A. G. Schwing, 和 D. Forsyth, “通过词性引导的快速、多样且准确的图像标题生成，”发表于*CVPR*，2019年。'
- en: '[221] D. Elliott, S. Frank, and E. Hasler, “Multilingual image description
    with neural sequence models,” *ICLR*, 2015.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] D. Elliott, S. Frank, 和 E. Hasler, “使用神经序列模型的多语言图像描述，”*ICLR*，2015年。'
- en: '[222] X. Li, C. Xu, X. Wang, W. Lan, Z. Jia, G. Yang, and J. Xu, “COCO-CN for
    cross-lingual image tagging, captioning, and retrieval,” *IEEE Trans. Multimedia*,
    vol. 21, no. 9, pp. 2347–2360, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] X. Li, C. Xu, X. Wang, W. Lan, Z. Jia, G. Yang, 和 J. Xu, “用于跨语言图像标记、标题生成和检索的COCO-CN，”*IEEE
    Trans. Multimedia*，第21卷，第9期，页码2347–2360，2019年。'
- en: '[223] T. Miyazaki and N. Shimizu, “Cross-lingual image caption generation,”
    in *ACM Multimedia*, 2016.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] T. Miyazaki 和 N. Shimizu, “跨语言图像标题生成，”发表于*ACM Multimedia*，2016年。'
- en: '[224] D. Elliott, S. Frank, K. Sima’an, and L. Specia, “Multi30K: Multilingual
    English-German Image Descriptions,” in *ACL Workshops*, 2016.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] D. Elliott, S. Frank, K. Sima’an, 和 L. Specia, “Multi30K：多语言英语-德语图像描述，”发表于*ACL
    Workshops*，2016年。'
- en: '[225] W. Lan, X. Li, and J. Dong, “Fluency-guided cross-lingual image captioning,”
    in *ACM Multimedia*, 2017.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] W. Lan, X. Li, 和 J. Dong, “流畅性引导的跨语言图像标题生成，”发表于*ACM Multimedia*，2017年。'
- en: '[226] Y. Song, S. Chen, Y. Zhao, and Q. Jin, “Unpaired cross-lingual image
    caption generation with self-supervised rewards,” in *ACM Multimedia*, 2019.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Y. Song, S. Chen, Y. Zhao, 和 Q. Jin, “基于自监督奖励的无配对跨语言图像标题生成，” 见于 *ACM
    Multimedia*，2019年。'
- en: '[227] B. Jing, P. Xie, and E. Xing, “On the Automatic Generation of Medical
    Imaging Reports,” in *ACL*, 2018.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] B. Jing, P. Xie, 和 E. Xing, “自动生成医学影像报告，” 见于 *ACL*，2018年。'
- en: '[228] F. Liu, X. Wu, S. Ge, W. Fan, and Y. Zou, “Exploring and Distilling Posterior
    and Prior Knowledge for Radiology Report Generation,” in *CVPR*, 2021.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] F. Liu, X. Wu, S. Ge, W. Fan, 和 Y. Zou, “探索和提炼放射学报告生成中的后验和先验知识，” 见于 *CVPR*，2021年。'
- en: '[229] X. Yang, M. Ye, Q. You, and F. Ma, “Writing by Memorizing: Hierarchical
    Retrieval-based Medical Report Generation,” in *ACL-IJCNLP*, 2021.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] X. Yang, M. Ye, Q. You, 和 F. Ma, “通过记忆写作：基于层次检索的医学报告生成，” 见于 *ACL-IJCNLP*，2021年。'
- en: '[230] Z. Bai, Y. Nakashima, and N. Garcia, “Explain Me the Painting: Multi-Topic
    Knowledgeable Art Description Generation,” in *ICCV*, 2021.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Z. Bai, Y. Nakashima, 和 N. Garcia, “给我解释画作：多主题知识性艺术描述生成，” 见于 *ICCV*，2021年。'
- en: '[231] Y. Feng and M. Lapata, “Automatic Caption Generation for News Images,”
    *IEEE Trans. PAMI*, vol. 35, no. 4, pp. 797–812, 2012.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Y. Feng 和 M. Lapata, “新闻图像的自动标题生成，” *IEEE Trans. PAMI*，第35卷，第4期，页码 797–812，2012年。'
- en: '[232] A. Tran, A. Mathews, and L. Xie, “Transform and Tell: Entity-Aware News
    Image Captioning,” in *CVPR*, 2020.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] A. Tran, A. Mathews, 和 L. Xie, “变换与讲述：实体感知新闻图像标题生成，” 见于 *CVPR*，2020年。'
- en: '[233] F. Liu, Y. Wang, T. Wang, and V. Ordonez, “Visual News: Benchmark and
    Challenges in News Image Captioning,” in *EMNLP*, 2021.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] F. Liu, Y. Wang, T. Wang, 和 V. Ordonez, “视觉新闻：新闻图像描述的基准和挑战，” 见于 *EMNLP*，2021年。'
- en: '[234] X. Yang, S. Karaman, J. Tetreault, and A. Jaimes, “Journalistic Guidelines
    Aware News Image Captioning,” *arXiv preprint arXiv:2109.02865*, 2021.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] X. Yang, S. Karaman, J. Tetreault, 和 A. Jaimes, “关注新闻图像标题中的新闻指导方针，” *arXiv
    preprint arXiv:2109.02865*，2021年。'
- en: '[235] S. Wu, J. Wieland, O. Farivar, and J. Schiller, “Automatic alt-text:
    Computer-generated image descriptions for blind users on a social network service,”
    in *CSCW*, 2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] S. Wu, J. Wieland, O. Farivar, 和 J. Schiller, “自动化替代文本：为社交网络服务中的盲人用户生成计算机生成的图像描述，”
    见于 *CSCW*，2017年。'
- en: '[236] C. Chunseong Park, B. Kim, and G. Kim, “Attend to you: Personalized image
    captioning with context sequence memory networks,” in *CVPR*, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] C. Chunseong Park, B. Kim, 和 G. Kim, “关注你：通过上下文序列记忆网络实现个性化图像标题生成，” 见于
    *CVPR*，2017年。'
- en: '[237] C. C. Park, B. Kim, and G. Kim, “Towards personalized image captioning
    via multimodal memory networks,” *IEEE Trans. PAMI*, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] C. C. Park, B. Kim, 和 G. Kim, “通过多模态记忆网络实现个性化图像标题生成，” *IEEE Trans. PAMI*，2018年。'
- en: '[238] W. Zhang, Y. Ying, P. Lu, and H. Zha, “Learning Long-and Short-Term User
    Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized
    Image Caption,” in *AAAI*, 2020.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] W. Zhang, Y. Ying, P. Lu, 和 H. Zha, “使用多模态层次变换网络学习用户长期和短期的字面偏好以实现个性化图像标题，”
    见于 *AAAI*，2020年。'
- en: '[239] C. Gan, Z. Gan, X. He, J. Gao, and L. Deng, “StyleNet: Generating Attractive
    Visual Captions with Styles,” in *CVPR*, 2017.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] C. Gan, Z. Gan, X. He, J. Gao, 和 L. Deng, “StyleNet: 使用风格生成吸引人的视觉标题，”
    见于 *CVPR*，2017年。'
- en: '[240] A. Mathews, L. Xie, and X. He, “Semstyle: Learning to generate stylised
    image captions using unaligned text,” in *CVPR*, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] A. Mathews, L. Xie, 和 X. He, “Semstyle: 使用未对齐文本生成风格化图像标题的学习，” 见于 *CVPR*，2018年。'
- en: '[241] L. Guo, J. Liu, P. Yao, J. Li, and H. Lu, “Mscap: Multi-style image captioning
    with unpaired stylized text,” in *CVPR*, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] L. Guo, J. Liu, P. Yao, J. Li, 和 H. Lu, “Mscap: 具有未配对风格文本的多风格图像描述，” 见于
    *CVPR*，2019年。'
- en: '[242] W. Zhao, X. Wu, and X. Zhang, “MemCap: Memorizing style knowledge for
    image captioning,” in *AAAI*, 2020.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] W. Zhao, X. Wu, 和 X. Zhang, “MemCap: 记忆风格知识用于图像标题生成，” 见于 *AAAI*，2020年。'
- en: '[243] K. Shuster, S. Humeau, H. Hu, A. Bordes, and J. Weston, “Engaging image
    captioning via personality,” in *CVPR*, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] K. Shuster, S. Humeau, H. Hu, A. Bordes, 和 J. Weston, “通过个性化的图像描述提高参与度，”
    见于 *CVPR*，2019年。'
- en: '[244] Y. Zheng, Y. Li, and S. Wang, “Intention oriented image captions with
    guiding objects,” in *CVPR*, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Y. Zheng, Y. Li, 和 S. Wang, “以意图为导向的图像标题与指导对象，” 见于 *CVPR*，2019年。'
- en: '[245] Z. Meng, L. Yu, N. Zhang, T. Berg, B. Damavandi, V. Singh, and A. Bearman,
    “Connecting What to Say With Where to Look by Modeling Human Attention Traces,”
    in *CVPR*, 2021.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] Z. Meng, L. Yu, N. Zhang, T. Berg, B. Damavandi, V. Singh, 和 A. Bearman,
    “通过建模人类注意力轨迹将‘说什么’与‘看哪里’连接起来，” 见于 *CVPR*，2021年。'
- en: '[246] L. Chen, Z. Jiang, J. Xiao, and W. Liu, “Human-like Controllable Image
    Captioning with Verb-specific Semantic Roles,” in *CVPR*, 2021.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] L. Chen, Z. Jiang, J. Xiao, 和 W. Liu, “类人可控图像描述与动词特定语义角色，” 见于 *CVPR*，2021。'
- en: '[247] S. Chen, Q. Jin, P. Wang, and Q. Wu, “Say as you wish: Fine-grained control
    of image caption generation with abstract scene graphs,” in *CVPR*, 2020.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] S. Chen, Q. Jin, P. Wang, 和 Q. Wu，“随心所欲：通过抽象场景图进行图像描述生成的细粒度控制”，发表于*CVPR*，2020年。'
- en: '[248] Y. Zhong, L. Wang, J. Chen, D. Yu, and Y. Li, “Comprehensive image captioning
    via scene graph decomposition,” in *ECCV*, 2020.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] Y. Zhong, L. Wang, J. Chen, D. Yu, 和 Y. Li，“通过场景图分解实现全面的图像描述”，发表于*ECCV*，2020年。'
- en: '[249] C. Deng, N. Ding, M. Tan, and Q. Wu, “Length-controllable image captioning,”
    in *ECCV*, 2020.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] C. Deng, N. Ding, M. Tan, 和 Q. Wu，“可控长度的图像描述”，发表于*ECCV*，2020年。'
- en: '[250] F. Sammani and L. Melas-Kyriazi, “Show, edit and tell: A framework for
    editing image captions,” in *CVPR*, 2020.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] F. Sammani 和 L. Melas-Kyriazi，“展示、编辑和讲述：一个编辑图像描述的框架”，发表于*CVPR*，2020年。'
- en: '[251] M. Hodosh and J. Hockenmaier, “Focused evaluation for image description
    with binary forced-choice tasks,” in *ACL Workshops*, 2016.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] M. Hodosh 和 J. Hockenmaier，“针对图像描述的二元强制选择任务的重点评估”，发表于*ACL Workshops*，2016年。'
- en: '[252] H. Xie, T. Sherborne, A. Kuhnle, and A. Copestake, “Going beneath the
    surface: Evaluating image captioning for grammaticality, truthfulness and diversity,”
    *arXiv preprint arXiv:1912.08960*, 2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] H. Xie, T. Sherborne, A. Kuhnle, 和 A. Copestake，“深入探讨：评估图像描述的语法性、真实性和多样性”，*arXiv预印本
    arXiv:1912.08960*，2019年。'
- en: '[253] M. Alikhani, P. Sharma, S. Li, R. Soricut, and M. Stone, “Cross-modal
    Coherence Modeling for Caption Generation,” in *ACL*, 2020.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] M. Alikhani, P. Sharma, S. Li, R. Soricut, 和 M. Stone，“用于描述生成的跨模态一致性建模”，发表于*ACL*，2020年。'
- en: '[254] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and
    J. Liu, “UNITER: UNiversal Image-TExt Representation Learning,” in *ECCV*, 2020.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, 和
    J. Liu，“UNITER：通用图像-文本表示学习”，发表于*ECCV*，2020年。'
- en: '[255] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep
    networks,” in *ICML*, 2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] M. Sundararajan, A. Taly, 和 Q. Yan，“深度网络的公理化归因”，发表于*ICML*，2017年。'
- en: '| ![[Uncaptioned image]](img/f7c1091f8d857fd0453bf5934777cf29.png) | Matteo
    Stefanini received the M.Sc. degree in Computer Engineering cum laude from the
    University of Modena and Reggio Emilia, in 2018\. He is currently pursuing a PhD
    degree in Information and Communication Technologies at the Department of Engineering
    “Enzo Ferrari”, University of Modena and Reggio Emilia. His research activities
    involve the integration of vision and language modalities, focusing on image captioning
    and Transformer-based architectures. |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/f7c1091f8d857fd0453bf5934777cf29.png) | 马泰奥·斯特法尼尼于2018年获得摩德纳和雷焦艾米利亚大学计算机工程硕士学位（荣誉）。他目前在摩德纳和雷焦艾米利亚大学“恩佐·费拉里”工程系攻读信息与通信技术博士学位。他的研究活动涉及视觉和语言模式的整合，重点关注图像描述和基于变换器的架构。
    |'
- en: '| ![[Uncaptioned image]](img/5a51e3e0f8a1c3f72c9fd9970e0fa74b.png) | Marcella
    Cornia received the M.Sc. degree in Computer Engineering and the Ph.D. degree
    cum laude in Information and Communication Technologies from the University of
    Modena and Reggio Emilia, in 2016 and 2020, respectively. She is currently a Postdoctoral
    Researcher with the University of Modena and Reggio Emilia. She has authored or
    coauthored more than 30 publications in scientific journals and international
    conference proceedings. |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/5a51e3e0f8a1c3f72c9fd9970e0fa74b.png) | 马塞拉·科尼亚于2016年和2020年分别获得摩德纳和雷焦艾米利亚大学计算机工程硕士学位和信息与通信技术博士学位（荣誉）。她目前是摩德纳和雷焦艾米利亚大学的博士后研究员。她已在科学期刊和国际会议论文集中发表或合著了30多篇文章。
    |'
- en: '| ![[Uncaptioned image]](img/2d5e1859fc24b5d41972939ce78065af.png) | Lorenzo
    Baraldi received the M.Sc. degree in Computer Engineering and the Ph.D. degree
    cum laude in Information and Communication Technologies from the University of
    Modena and Reggio Emilia, in 2014 and 2018\. He is currently Tenure Track Assistant
    Professor with the University of Modena and Reggio Emilia. He was a Research Intern
    at Facebook AI Research (FAIR) in 2017\. He has authored or coauthored more than
    70 publications in scientific journals and international conference proceedings.
    |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/2d5e1859fc24b5d41972939ce78065af.png) | 洛伦佐·巴拉尔迪于2014年和2018年分别获得摩德纳和雷焦艾米利亚大学计算机工程硕士学位和信息与通信技术博士学位（荣誉）。他目前是摩德纳和雷焦艾米利亚大学的终身教职助理教授。他曾在2017年担任Facebook
    AI Research (FAIR)的研究实习生。他已在科学期刊和国际会议论文集中发表或合著了70多篇文章。'
- en: '| ![[Uncaptioned image]](img/56e3a89c52c9911ae11a531802ec8275.png) | Silvia
    Cascianelli received the M.Sc. degree in Information and Automation Engineering
    and the Ph.D. degree cum laude in Information and Industrial Engineering from
    the University of Perugia, in 2015 and 2019, respectively. She is an Assistant
    Professor with the University of Modena and Reggio Emilia. She was a Visitor Researcher
    at the Queen Mary University of London in 2018\. She has authored or coauthored
    more than 30 publications in scientific journals and international conference
    proceedings. |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/56e3a89c52c9911ae11a531802ec8275.png) | Silvia Cascianelli
    获得了佩鲁贾大学的信息与自动化工程硕士学位和信息与工业工程博士学位（优等），分别于 2015 年和 2019 年。她是摩德纳与雷焦艾米利亚大学的助理教授。她在
    2018 年曾在伦敦玛丽女王大学担任访问研究员。她在科学期刊和国际会议论文集上发表了 30 多篇文章。 |'
- en: '| ![[Uncaptioned image]](img/a57415e3de38de64f8bd3a8371108825.png) | Giuseppe
    Fiameni is a Data Scientist at NVIDIA where he oversees the NVIDIA AI Technology
    Centre in Italy, a collaboration among NVIDIA, CINI and CINECA to accelerate academic
    research in the field of AI. He has been working as HPC specialist at CINECA,
    the largest HPC facility in Italy, for more than 14 years providing support for
    large-scale data analytics workloads. Research interests include large scale deep
    learning models, system architectures, massive data engineering, video action
    detection. |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/a57415e3de38de64f8bd3a8371108825.png) | Giuseppe Fiameni 是
    NVIDIA 的数据科学家，他负责监督位于意大利的 NVIDIA AI 技术中心，这是 NVIDIA、CINI 和 CINECA 之间的合作，旨在加速人工智能领域的学术研究。他在意大利最大的高性能计算设施
    CINECA 工作了超过 14 年，为大规模数据分析工作负载提供支持。研究兴趣包括大规模深度学习模型、系统架构、大数据工程和视频动作检测。 |'
- en: '| ![[Uncaptioned image]](img/eb0b46f0394bbe0815488621cb90d107.png) | Rita Cucchiara
    received the M.Sc. degree in Electronics Engineering and the Ph.D. degree in Computer
    Engineering from the University of Bologna, in 1989 and 1992\. She is currently
    Full Professor with the University of Modena and Reggio Emilia, where she heads
    the AImageLab Laboratory. She has authored or coauthored more than 400 papers
    in journals and international proceedings, and has been a coordinator of several
    projects in computer vision. She is Member of the Advisory Board of the Computer
    Vision Foundation, and Director of the ELLIS Unit of Modena. |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/eb0b46f0394bbe0815488621cb90d107.png) | Rita Cucchiara 获得了博洛尼亚大学的电子工程硕士学位和计算机工程博士学位，分别于
    1989 年和 1992 年。她目前是摩德纳与雷焦艾米利亚大学的全职教授，并且领导 AImageLab 实验室。她在期刊和国际会议上发表了超过 400 篇论文，并且担任过多个计算机视觉项目的协调员。她是计算机视觉基金会顾问委员会的成员，以及摩德纳
    ELLIS 单元的主任。 |'
- en: Appendix A Further analysis of the evaluation metrics
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 评估指标的进一步分析
- en: 'In this section, we extend our analysis of the evaluation metrics for image
    captioning. In particular, in Table [III](#A1.T3 "TABLE III ‣ Appendix A Further
    analysis of the evaluation metrics ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), we provide a taxonomy and summarize the main characteristics
    of the metrics presented. Moreover, in the following, we describe in more detail
    additional diversity metrics, embedding-based metrics, and learning-based metrics,
    which were only mentioned in the main paper.'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们扩展了对图像描述评估指标的分析。特别是在表格[III](#A1.T3 "TABLE III ‣ Appendix A Further
    analysis of the evaluation metrics ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")中，我们提供了一个分类法，并总结了所提出指标的主要特征。此外，接下来我们将更详细地描述额外的多样性指标、基于嵌入的指标以及基于学习的指标，这些在主要论文中只是简单提及。'
- en: 'Diversity metrics. Local Diversity can be quantified via the average BLEU score
    between each caption and the others (mBLEU: the lower the mBLEU, the more diverse
    the produced caption set is) [[154](#bib.bib154)]. Another approach to quantify
    captions diversity is to focus on the mention of all the relevant words in the
    produced caption, despite their rareness in the training set. In this respect,
    in [[155](#bib.bib155)], two recall-based diversity metrics have been proposed,
    namely Global Recall and Local Recall. The former is computed as the fraction
    of generated words with respect to the words appearing in both the training and
    validation set. The latter is computed for each test image as the fraction of
    generated words with respect to the words in the reference captions. Moreover,
    when the system can produce multiple captions for the same image, diversity can
    be quantified at the topics level by using the Latent Semantic Analysis-based
    metric LSA and the kernelized version of the CIDEr score Self-CIDEr, proposed
    in [[156](#bib.bib156), [157](#bib.bib157)].'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性指标。本地多样性可以通过每个标题与其他标题之间的平均BLEU分数来量化（mBLEU：mBLEU越低，生成的标题集越多样化）[[154](#bib.bib154)]。量化标题多样性的另一种方法是关注生成的标题中所有相关词汇的提及，尽管这些词在训练集中较为稀有。在这方面，[[155](#bib.bib155)]中提出了两种基于召回率的多样性指标，即全局召回率和局部召回率。前者是生成的词与训练集和验证集中都出现的词的比例。后者则是对于每张测试图像，生成的词与参考标题中的词的比例。此外，当系统可以为同一图像生成多个标题时，可以通过使用基于潜在语义分析的指标LSA和CIDEr分数的核化版本Self-CIDEr来在主题层面量化多样性，这些方法在[[156](#bib.bib156),
    [157](#bib.bib157)]中提出。
- en: Embedding-based metrics. Among the specific aspects of a produced caption that
    can e evaluated via embedding-based metrics, the hallucination rate is measured
    via the CHAIR score [[158](#bib.bib158)], which is expressed as the fraction of
    hallucinated objects among those mentioned in a caption (in the CHAIR${}_{\text{i}}$
    variant) or the fraction of captions with at least a hallucinated object among
    all the produced captions (in the CHAIR${}_{\text{s}}$ variant). Other aspects
    that can be measured by exploiting embedding-based representation of candidate
    caption, reference captions, and image are Relevance (via cosine similarity),
    Extraness, and Omission (via orthogonal projections), as proposed in [[159](#bib.bib159)].
    Moreover, to take into account the uniqueness of the generated captions, in [[160](#bib.bib160)]
    the SPICE-U score is proposed as the harmonic mean of the SPICE score and a measure
    of the caption uniqueness, which considers the fraction of images in the training
    set not containing the mentioned concepts.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 基于嵌入的指标。在可以通过基于嵌入的指标评估的生成标题的特定方面中，幻觉率通过CHAIR分数来测量[[158](#bib.bib158)]，该分数表示为在标题中提到的幻觉对象的比例（在CHAIR${}_{\text{i}}$变体中）或包含至少一个幻觉对象的标题所占的比例（在CHAIR${}_{\text{s}}$变体中）。其他可以通过利用候选标题、参考标题和图像的嵌入表示来测量的方面包括相关性（通过余弦相似度）、额外性和遗漏（通过正交投影），这些方法在[[159](#bib.bib159)]中提出。此外，为了考虑生成标题的独特性，[[160](#bib.bib160)]中提出了SPICE-U分数，该分数作为SPICE分数和标题独特性度量的调和平均值，考虑了训练集中不包含提到概念的图像比例。
- en: Learning-based metrics. With respect to learning-based evaluation scores, NNEval [[151](#bib.bib151)]
    was proposed as the first learning-based image captioning evaluation approach.
    It considers classical metrics (BLEU, METEOR, CIDEr, SPICE, and WMD) as features
    describing the candidate caption when compared to reference captions and predicts
    its probability of being human-generated. Another early-proposed learning-based
    evaluation strategy is LEIC [[152](#bib.bib152)], which directly scores the probability
    of a caption being human-generated, conditioned on the image and eventually on
    a reference caption, by using a binary classifier fed with pre-trained ResNet
    image features and an LSTM-based encoding of the caption. As a refinement of BERT-S
    specific for image captioning, the ViLBERT-S [[165](#bib.bib165)] exploits the
    image-conditioned embedding obtained from the vision-and-language representation
    model ViLBERT [[99](#bib.bib99)]. Similar to the BERT-S, the matching between
    these tokens is expressed via the cosine similarity of their embeddings, and the
    best matching token pairs are used for computing precision, recall, and F1-score.
    Another variant of the BERT-S, to which we here refer to as BERT-S${}^{\text{IRV}}$ [[166](#bib.bib166)],
    takes into account the variability of the reference captions associated with the
    same image by combining them in a unique embedding vector that contains all the
    mentioned concepts and against which the candidate caption is compared. To evaluate
    the candidate caption fidelity and adequacy, the FAIEr [[167](#bib.bib167)] score
    exploits scene graphs matching. The references and the image scene graphs are
    fused in a unique scene graph, whose more relevant nodes (representing the concepts
    more often mentioned in the references) get more weight, and the score is obtained
    based on the similarity between the candidate scene graph and the unique scene
    graph. On a similar line, the UMIC score [[168](#bib.bib168)] exploits the pre-trained
    vision-and-language representation model UNITER [[254](#bib.bib254)], fine-tuned
    on carefully-designed negative samples, to score a candidate caption without the
    need for reference captions.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的指标。关于基于学习的评价分数，NNEval [[151](#bib.bib151)] 被提出作为首个基于学习的图像描述评价方法。它将经典指标（BLEU、METEOR、CIDEr、SPICE
    和 WMD）作为描述候选描述与参考描述比较时的特征，并预测其为人类生成的概率。另一个早期提出的基于学习的评价策略是 LEIC [[152](#bib.bib152)]，该策略通过使用预训练的
    ResNet 图像特征和基于 LSTM 的描述编码的二分类器，直接评分描述为人类生成的概率，条件是图像和最终参考描述。作为针对图像描述的 BERT-S 的改进，ViLBERT-S [[165](#bib.bib165)]
    利用从视觉与语言表示模型 ViLBERT [[99](#bib.bib99)] 获得的图像条件嵌入。类似于 BERT-S，这些令牌之间的匹配通过其嵌入的余弦相似性表示，最佳匹配的令牌对用于计算精确度、召回率和
    F1 分数。另一种 BERT-S 的变体，我们在这里称之为 BERT-S${}^{\text{IRV}}$ [[166](#bib.bib166)]，通过将与同一图像相关的参考描述组合成一个唯一的嵌入向量来考虑参考描述的变异性，该向量包含所有提到的概念，并与候选描述进行比较。为了评估候选描述的忠实性和适应性，FAIEr [[167](#bib.bib167)]
    分数利用场景图匹配。参考和图像场景图被融合成一个唯一的场景图，其中更相关的节点（表示在参考中更频繁提到的概念）得到更多的权重，分数基于候选场景图和唯一场景图之间的相似性获得。类似地，UMIC
    分数 [[168](#bib.bib168)] 利用经过精心设计的负样本进行微调的预训练视觉与语言表示模型 UNITER [[254](#bib.bib254)]，在没有参考描述的情况下为候选描述打分。
- en: 'TABLE III: Taxonomy and main characteristics of image captioning metrics.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：图像描述指标的分类及主要特征。
- en: '|  |  |  |  | Inputs |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 输入 |'
- en: '|  |  |  | Original Task | Pred | Refs | Image |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 原始任务 | 预测 | 参考 | 图片 |'
- en: '|  |  | BLEU [[117](#bib.bib117)] | Translation | ✓ | ✓ |  |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BLEU [[117](#bib.bib117)] | 翻译 | ✓ | ✓ |  |'
- en: '|  |  | METEOR [[150](#bib.bib150)] | Translation | ✓ | ✓ |  |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '|  |  | METEOR [[150](#bib.bib150)] | 翻译 | ✓ | ✓ |  |'
- en: '| Standard |  | ROUGE [[118](#bib.bib118)] | Summarization | ✓ | ✓ |  |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| 标准 |  | ROUGE [[118](#bib.bib118)] | 总结 | ✓ | ✓ |  |'
- en: '|  |  | CIDEr [[122](#bib.bib122)] | Captioning | ✓ | ✓ |  |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CIDEr [[122](#bib.bib122)] | 描述 | ✓ | ✓ |  |'
- en: '|  |  | SPICE [[121](#bib.bib121)] | Captioning | ✓ | (✓) | (✓) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SPICE [[121](#bib.bib121)] | 描述 | ✓ | (✓) | (✓) |'
- en: '|  |  | Div [[154](#bib.bib154)] | Captioning | ✓ |  |  |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Div [[154](#bib.bib154)] | 描述 | ✓ |  |  |'
- en: '|  |  | Vocab [[154](#bib.bib154)] | Captioning | ✓ |  |  |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Vocab [[154](#bib.bib154)] | 描述 | ✓ |  |  |'
- en: '|  |  | %Novel [[154](#bib.bib154)] | Captioning | ✓ |  |  |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  |  | %Novel [[154](#bib.bib154)] | 描述 | ✓ |  |  |'
- en: '| Diversity |  | mBLEU [[154](#bib.bib154)] | Captioning | ✓ |  |  |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 多样性 |  | mBLEU [[154](#bib.bib154)] | 描述 | ✓ |  |  |'
- en: '|  |  | LSA [[156](#bib.bib156), [157](#bib.bib157)] | Captioning | ✓ |  |  |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LSA [[156](#bib.bib156), [157](#bib.bib157)] | 描述 | ✓ |  |  |'
- en: '|  |  | Self-CIDEr [[156](#bib.bib156), [157](#bib.bib157)] | Captioning |
    ✓ |  |  |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Self-CIDEr [[156](#bib.bib156), [157](#bib.bib157)] | 图像描述 | ✓ |  |  |'
- en: '|  |  | Recall [[155](#bib.bib155)] | Captioning | ✓ | (✓) |  |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Recall [[155](#bib.bib155)] | 图像描述 | ✓ | (✓) |  |'
- en: '|  |  | WMD [[161](#bib.bib161)] | Doc. Dissimilarity | ✓ | ✓ |  |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|  |  | WMD [[161](#bib.bib161)] | 文档相似性 | ✓ | ✓ |  |'
- en: '|  |  | Alignment [[163](#bib.bib163)] | Captioning | ✓ | ✓ |  |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Alignment [[163](#bib.bib163)] | 图像描述 | ✓ | ✓ |  |'
- en: '|  |  | Coverage [[163](#bib.bib163), [164](#bib.bib164)] | Captioning | ✓
    | (✓) | (✓) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Coverage [[163](#bib.bib163), [164](#bib.bib164)] | 图像描述 | ✓ | (✓)
    | (✓) |'
- en: '|  |  | Relevance [[159](#bib.bib159)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Relevance [[159](#bib.bib159)] | 图像描述 | ✓ | (✓) | ✓ |'
- en: '|  |  | Extraness [[159](#bib.bib159)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Extraness [[159](#bib.bib159)] | 图像描述 | ✓ | (✓) | ✓ |'
- en: '|  |  | Omission [[159](#bib.bib159)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Omission [[159](#bib.bib159)] | 图像描述 | ✓ | (✓) | ✓ |'
- en: '|  |  | SPICE-U [[160](#bib.bib160)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SPICE-U [[160](#bib.bib160)] | 图像描述 | ✓ | (✓) | ✓ |'
- en: '| Embedding-based |  | CHAIR [[158](#bib.bib158)] | Captioning | ✓ | ✓ | ✓
    |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| 基于嵌入的 |  | CHAIR [[158](#bib.bib158)] | 图像描述 | ✓ | ✓ | ✓ |'
- en: '|  |  | NNEval [[151](#bib.bib151)] | Captioning | ✓ | ✓ |  |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '|  |  | NNEval [[151](#bib.bib151)] | 图像描述 | ✓ | ✓ |  |'
- en: '|  |  | BERT-S [[171](#bib.bib171)] | Text Similarity | ✓ | ✓ |  |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BERT-S [[171](#bib.bib171)] | 文本相似性 | ✓ | ✓ |  |'
- en: '|  |  | BERT-S${}^{\text{IRV}}$ [[166](#bib.bib166)] | Captioning | ✓ | ✓ |  |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BERT-S${}^{\text{IRV}}$ [[166](#bib.bib166)] | 图像描述 | ✓ | ✓ |  |'
- en: '|  |  | UMIC [[168](#bib.bib168)] | Captioning | ✓ |  | ✓ |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '|  |  | UMIC [[168](#bib.bib168)] | 图像描述 | ✓ |  | ✓ |'
- en: '| Learning-based |  | LEIC [[152](#bib.bib152)] | Captioning | ✓ | (✓) | ✓
    |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| 基于学习的 |  | LEIC [[152](#bib.bib152)] | 图像描述 | ✓ | (✓) | ✓ |'
- en: '|  |  | CLIP-S [[175](#bib.bib175)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CLIP-S [[175](#bib.bib175)] | 图像描述 | ✓ | (✓) | ✓ |'
- en: '|  |  | TIGEr [[173](#bib.bib173)] | Captioning | ✓ | ✓ | ✓ |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '|  |  | TIGEr [[173](#bib.bib173)] | 图像描述 | ✓ | ✓ | ✓ |'
- en: '|  |  | ViLBERT-S [[165](#bib.bib165)] | Captioning | ✓ | ✓ | ✓ |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ViLBERT-S [[165](#bib.bib165)] | 图像描述 | ✓ | ✓ | ✓ |'
- en: '|  |  | FAIEr [[167](#bib.bib167)] | Captioning | ✓ | ✓ | ✓ |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '|  |  | FAIEr [[167](#bib.bib167)] | 图像描述 | ✓ | ✓ | ✓ |'
- en: Appendix B Further Performance Analysis
  id: totrans-571
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 进一步性能分析
- en: 'According to the taxonomies proposed in Sections [2](#S2 "2 Visual Encoding
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning"), [3](#S3
    "3 Language Models ‣ From Show to Tell: A Survey on Deep Learning-based Image
    Captioning"), and [4](#S4 "4 Training Strategies ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning"), in Table [IV](#A2.T4 "TABLE IV ‣ Appendix
    B Further Performance Analysis ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), we overview the most relevant surveyed methods. We report
    their performance in terms of BLEU-4, METEOR, and CIDEr on the COCO Karpathy test
    set and their main features in terms of visual encoding, language modeling, and
    training strategies. In the table, methods are clustered based primarily on their
    visual encoding strategy and ordered based on the obtained scores. Methods exploiting
    vision-and-language pre-training are further separated from the others. Image
    captioning models have reached impressive performance in just a few years: from
    an average BLEU-4 of 25.1 for the methods using global CNN features to an average
    BLEU-4 of 35.3 and 40.0 for those exploiting the attention and self-attention
    mechanisms, peaking at 42.6 in the case of vision-and-language pre-training. By
    looking at the performance in terms of the CIDEr score, we can notice that, as
    for the visual encoding, the more complete and structured information about semantic
    visual concepts and their mutual relation is included, the better is the performance
    (consider that methods applying attention over a grid of features reach an average
    CIDEr score of 105.8, while those performing attention over visual regions 121.8,
    further increased for graph-based approaches and methods using self-attention,
    which reach 133.2 on average). As for the language model, LSTM-based approaches
    combined with strong visual encoders are still competitive with subsequent fully-attentive
    methods in terms of performance. These methods are slower to train but are generally
    smaller than Transformer-based ones. As for the training strategy, sentence-level
    fine-tuning with reinforcement learning leads to significant performance improvement
    (consider that methods relying only on the cross-entropy loss obtain an average
    CIDEr score of 92.3, while those combining it with reinforcement learning fine-tuning
    reach 125.1 on average). Moreover, it emerges that vision-and-language pre-training
    on large datasets allows boosting the performance and deserves further investigation
    (with an average CIDEr score of 140.4).'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '根据[2](#S2 "2 Visual Encoding ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")、[3](#S3 "3 Language Models ‣ From Show to Tell: A Survey on
    Deep Learning-based Image Captioning")和[4](#S4 "4 Training Strategies ‣ From Show
    to Tell: A Survey on Deep Learning-based Image Captioning")节中提出的分类法，以及[IV](#A2.T4
    "TABLE IV ‣ Appendix B Further Performance Analysis ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning")表，我们概述了最相关的调查方法。我们报告了这些方法在 COCO Karpathy
    测试集上的 BLEU-4、METEOR 和 CIDEr 性能，以及它们在视觉编码、语言建模和训练策略方面的主要特征。在表中，方法主要根据其视觉编码策略进行分组，并根据获得的分数进行排序。利用视觉与语言预训练的方法与其他方法进一步区分。图像描述模型在短短几年内取得了令人印象深刻的成绩：从使用全局
    CNN 特征的方法的平均 BLEU-4 25.1 到利用注意力和自注意力机制的 35.3 和 40.0，再到视觉与语言预训练情况下的 42.6。通过查看 CIDEr
    分数的表现，我们可以注意到，就视觉编码而言，包含关于语义视觉概念及其相互关系的更完整和结构化的信息，性能越好（考虑到对特征网格应用注意力的方法的平均 CIDEr
    分数为 105.8，而对视觉区域执行注意力的方法为 121.8，图形方法和使用自注意力的方法进一步增加，平均达到 133.2）。至于语言模型，基于 LSTM
    的方法与强大的视觉编码器结合，仍然在性能上与后续的完全注意力方法具有竞争力。这些方法训练较慢，但通常比基于 Transformer 的方法规模更小。至于训练策略，句子级的强化学习微调可以显著提高性能（考虑到仅依赖交叉熵损失的方法的平均
    CIDEr 分数为 92.3，而结合强化学习微调的方法平均达到 125.1）。此外，视觉与语言预训练在大数据集上的应用可以提升性能，值得进一步研究（平均 CIDEr
    分数为 140.4）。'
- en: 'For completeness, in Fig. [10](#A2.F10 "Figure 10 ‣ Appendix B Further Performance
    Analysis ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")
    we report the relation between the CIDEr score and all the other characteristics
    from Table [II](#S5.T2 "TABLE II ‣ 5.2.1 Standard evaluation metrics ‣ 5.2 Evaluation
    Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"). The almost-linear relation with the CIDEr score is observable
    also for the scores not reported in Fig. [9](#S5.F9 "Figure 9 ‣ 5.2.4 Learning-based
    evaluation ‣ 5.2 Evaluation Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell:
    A Survey on Deep Learning-based Image Captioning") in the main paper, with the
    only exception of the BERT-S score.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Overview of deep learning-based image captioning models. Scores are
    taken from the respective papers. For all the metrics, the higher the value, the
    better ($\uparrow$).'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | Visual Encoding |  |  | Language Model |  |  | Training Strategies
    |  |  | Main Results |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| Model |  |  | Global | Grid | Regions | Graph | Self-Attention |  |  | RNN/LSTM
    | Transformer | BERT |  |  | XE | MLM | Reinforce | VL Pre-Training |  |  | BLEU-4
    | METEOR | CIDEr |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
- en: '| LEMON [[104](#bib.bib104)] |  |  |  |  | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |  |
    ✓ | ✓ | ✓ |  |  | 42.6 | 31.4 | 145.5 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
- en: '| UniversalCap [[97](#bib.bib97)] |  |  |  | ✓ |  |  | ✓ |  |  |  | ✓ |  |  |  |
    ✓ |  | ✓ | ✓ |  |  | 40.8 | 30.4 | 143.4 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
- en: '| SimVLM [[94](#bib.bib94)] |  |  |  |  |  |  | ✓ |  |  |  | ✓ |  |  |  | ✓
    |  |  | ✓ |  |  | 40.6 | 33.7 | 143.3 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
- en: '| VinVL [[103](#bib.bib103)] |  |  |  |  | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |  |
    ✓ | ✓ | ✓ |  |  | 41.0 | 31.1 | 140.9 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
- en: '| Oscar [[100](#bib.bib100)] |  |  |  |  | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |  |
    ✓ | ✓ | ✓ |  |  | 41.7 | 30.6 | 140.0 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
- en: '| Unified VLP [[101](#bib.bib101)] |  |  |  |  | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |
    ✓ |  | ✓ | ✓ |  |  | 39.5 | 29.3 | 129.3 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
- en: '| AutoCaption [[107](#bib.bib107)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 40.2 | 29.9 | 135.8 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
- en: '| RSTNet [[89](#bib.bib89)] |  |  |  | ✓ |  |  | ✓ |  |  |  | ✓ |  |  |  |
    ✓ |  | ✓ |  |  |  | 40.1 | 29.8 | 135.6 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
- en: '| DLCT [[86](#bib.bib86)] |  |  |  | ✓ | ✓ |  | ✓ |  |  |  | ✓ |  |  |  | ✓
    |  | ✓ |  |  |  | 39.8 | 29.5 | 133.8 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
- en: '| DPA [[83](#bib.bib83)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 40.5 | 29.6 | 133.4 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
- en: '| X-Transformer [[80](#bib.bib80)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  |
    ✓ |  | ✓ |  |  |  | 39.7 | 29.5 | 132.8 |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
- en: '| NG-SAN [[78](#bib.bib78)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  |
    ✓ |  | ✓ |  |  |  | 39.9 | 29.3 | 132.1 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
- en: '| X-LAN [[80](#bib.bib80)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 39.5 | 29.5 | 132.0 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
- en: '| GET [[85](#bib.bib85)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |  | 39.5 | 29.3 | 131.6 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  |  |  |  | ✓ |  | ✓ |  |  |  |
    ✓ |  |  |  | ✓ |  | ✓ |  |  |  | 39.1 | 29.2 | 131.2 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
- en: '| AoANet [[79](#bib.bib79)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 38.9 | 29.2 | 129.8 |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
- en: '| CPTR [[92](#bib.bib92)] |  |  |  |  |  |  | ✓ |  |  |  | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |  | 40.0 | 29.1 | 129.4 |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
- en: '| ORT [[77](#bib.bib77)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.6 | 28.7 | 128.3 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
- en: '| CNM [[75](#bib.bib75)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.9 | 28.4 | 127.9 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
- en: '| ETA [[76](#bib.bib76)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |  | 39.9 | 28.9 | 127.6 |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
- en: '| GCN-LSTM+HIP [[73](#bib.bib73)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 39.1 | 28.9 | 130.6 |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
- en: '| MT [[72](#bib.bib72)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.9 | 28.8 | 129.6 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
- en: '| SGAE [[71](#bib.bib71)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 39.0 | 28.4 | 129.1 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
- en: '| GCN-LSTM [[68](#bib.bib68)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 38.3 | 28.6 | 128.7 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
- en: '| VSUA [[69](#bib.bib69)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 38.4 | 28.5 | 128.6 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
- en: '| SG-RWS [[65](#bib.bib65)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 38.5 | 28.7 | 129.1 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
- en: '| LBPF [[63](#bib.bib63)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.3 | 28.5 | 127.6 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
- en: '| AAT [[64](#bib.bib64)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.2 | 28.3 | 126.7 |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
- en: '| CAVP [[66](#bib.bib66)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.6 | 28.3 | 126.3 |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 36.3 | 27.7 | 120.1 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
- en: '| RDN [[62](#bib.bib62)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |  |
    36.8 | 27.2 | 115.3 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
- en: '| Neural Baby Talk [[106](#bib.bib106)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 34.7 | 27.1 | 107.2 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
- en: '| Stack-Cap [[49](#bib.bib49)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 36.1 | 27.4 | 120.4 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
- en: '| MaBi-LSTM [[48](#bib.bib48)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 36.8 | 28.1 | 116.6 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
- en: '| RFNet [[52](#bib.bib52)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 35.8 | 27.4 | 112.5 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
- en: '| SCST (Att2in) [[38](#bib.bib38)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 33.3 | 26.3 | 111.4 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
- en: '| Adaptive Attention [[43](#bib.bib43)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 33.2 | 26.6 | 108.5 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
- en: '| Skeleton [[47](#bib.bib47)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 33.6 | 26.8 | 107.3 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
- en: '| ARNet [[46](#bib.bib46)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 33.5 | 26.1 | 103.4 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
- en: '| SCA-CNN [[51](#bib.bib51)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 31.1 | 25.0 | 95.2 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
- en: '| Areas of Attention [[67](#bib.bib67)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 30.7 | 24.5 | 93.8 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
- en: '| Review Net [[50](#bib.bib50)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 29.0 | 23.7 | 88.6 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
- en: '| Show, Attend and Tell [[42](#bib.bib42)] |  |  |  | ✓ |  |  |  |  |  | ✓
    |  |  |  |  | ✓ |  |  |  |  |  | 24.3 | 23.9 | - |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
- en: '| SCST (FC) [[38](#bib.bib38)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 31.9 | 25.5 | 106.3 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
- en: '| PG-SPIDEr [[120](#bib.bib120)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 33.2 | 25.7 | 101.3 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
- en: '| SCN-LSTM [[41](#bib.bib41)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 33.0 | 25.7 | 101.2 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
- en: '| LSTM-A [[40](#bib.bib40)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 32.6 | 25.4 | 100.2 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
- en: '| CNN[L]+RNH [[35](#bib.bib35)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 30.6 | 25.2 | 98.9 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: '| Att-CNN+LSTM [[34](#bib.bib34)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 31.0 | 26.0 | 94.0 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
- en: '| GroupCap [[37](#bib.bib37)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 33.0 | 26.0 | - |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
- en: '| StructCap [[36](#bib.bib36)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 32.9 | 25.4 | - |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
- en: '| Embedding Reward [[119](#bib.bib119)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 30.4 | 25.1 | 93.7 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
- en: '| ATT-FCN [[33](#bib.bib33)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 30.4 | 24.3 | - |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
- en: '| MIXER [[115](#bib.bib115)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 29.0 | - | - |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
- en: '| MSR [[31](#bib.bib31)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |  |
    25.7 | 23.6 | - |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
- en: '| gLSTM [[32](#bib.bib32)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 26.4 | 22.7 | 81.3 |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
- en: '| m-RNN [[27](#bib.bib27)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 25.0 | - | - |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
- en: '| Show and Tell [[23](#bib.bib23)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 24.6 | - | - |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
- en: '| Mind’s Eye [[30](#bib.bib30)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 19.0 | 20.4 | - |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
- en: '| DeepVS [[25](#bib.bib25)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 23.0 | 19.5 | 66.0 |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
- en: '| LRCN [[28](#bib.bib28)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |  |
    21.0 | - | - | ![Refer to caption](img/db0988b53691211608e49129ec9bd823.png)'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Relationship between CIDEr, number of parameters and other scores.
    Values of Div-1, Div-2, Alignment, CLIP-S, and CLIP-S${}^{\text{Ref}}$ are multiplied
    by powers of 10 for readability.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: 'We deepen our analysis by considering also the Flickr30K dataset, and evaluate
    the performance of methods trained on COCO, and tested on the test set of Flickr30K,
    and of methods developed for COCO, but trained and tested on Flickr30K. The results
    of this study are reported in Table [V](#A2.T5 "TABLE V ‣ Appendix B Further Performance
    Analysis ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning").
    Compared to what is obtained by the same models when trained and tested on COCO
    (in Table [II](#S5.T2 "TABLE II ‣ 5.2.1 Standard evaluation metrics ‣ 5.2 Evaluation
    Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")), the standard metrics and the embedding-based metrics significantly
    drop while diversity metrics increase. This can be imputed to the smaller size
    of Flickr30K compared to COCO. Learning-based metrics, especially BERT-S and CLIP-S,
    are more stable. As expected, training on COCO and directly testing on Flickr30K
    results in a performance drop under all the metrics when compared with the case
    in which both training and test are performed on data from Flickr30K. This confirms
    that generalization is still an open issue for image captioning approaches. Interestingly,
    VinVL surpasses the other considered approaches under all metrics in the case
    of the test on Flickr30K, suggesting the benefits of pre-training in terms of
    generalization capabilities of the resulting model.'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过考虑Flickr30K数据集来深入分析，并评估在COCO上训练、在Flickr30K测试集上测试的方法的表现，以及在COCO上开发但在Flickr30K上训练和测试的方法的表现。这项研究的结果报告在表格 [V](#A2.T5
    "TABLE V ‣ Appendix B Further Performance Analysis ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning")中。与在COCO上训练和测试的相同模型的结果相比（在表格 [II](#S5.T2
    "TABLE II ‣ 5.2.1 Standard evaluation metrics ‣ 5.2 Evaluation Metrics ‣ 5 Evaluation
    Protocol ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")中），标准指标和基于嵌入的指标显著下降，而多样性指标则增加。这可以归因于Flickr30K相较于COCO的规模较小。基于学习的指标，特别是BERT-S和CLIP-S，表现得更为稳定。正如预期的那样，与在Flickr30K上同时进行训练和测试的情况相比，在COCO上训练并直接在Flickr30K上测试会导致所有指标下性能的下降。这证实了图像描述方法中的泛化问题仍然存在。有趣的是，VinVL在所有Flickr30K测试指标中都超越了其他考虑的方法，这表明预训练在模型泛化能力方面的优势。'
- en: 'For a deeper qualitative analysis, in Fig. [11](#A2.F11 "Figure 11 ‣ Appendix
    B Further Performance Analysis ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")-[12](#A2.F12 "Figure 12 ‣ Appendix B Further Performance Analysis
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning"), we report
    examples of captions generated by eight popular approaches on images from the
    COCO dataset. It can be observed that the produced captions have similar length
    and structure (*i.e.* the main subject is mentioned first, then the main action,
    and finally additional details concerning other visual entities in the scene).
    This mimics the characteristics of the majority of the ground-truth captions in
    COCO. Another aspect that emerges is the lack of counting capabilities (consider
    *e.g.* the first example in the second row and the second example in the bottom
    row of Fig. [12](#A2.F12 "Figure 12 ‣ Appendix B Further Performance Analysis
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")). Current
    approaches struggle in mentioning the right amount of instances of the same entities
    and generally refer to multiple instances as a group of. Finally, it is worth
    mentioning the difficulty in describing unusual concepts, both situations and
    visual entities (consider *e.g.* the first two examples in Fig. [11](#A2.F11 "Figure
    11 ‣ Appendix B Further Performance Analysis ‣ From Show to Tell: A Survey on
    Deep Learning-based Image Captioning")), which is a symptom of the lack of generalization
    capability. In fact, in such cases, unusual concepts are described as more represented
    concepts in the training set. For example, the ferret in the top-right of Fig. [11](#A2.F11
    "Figure 11 ‣ Appendix B Further Performance Analysis ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning") is described as a cat or a mouse, and
    the historically-dressed man in the last example of the second row of Fig. [12](#A2.F12
    "Figure 12 ‣ Appendix B Further Performance Analysis ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning") is described as a woman. This issue
    is less evident for VinVL, enforcing the role of pre-training to achieve better
    generalization capabilities.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更深入的定性分析，如图[11](#A2.F11 "图 11 ‣ 附录 B 进一步性能分析 ‣ 从展示到叙述：基于深度学习的图像描述调查")-[12](#A2.F12
    "图 12 ‣ 附录 B 进一步性能分析 ‣ 从展示到叙述：基于深度学习的图像描述调查")所示，我们报告了八种流行方法在COCO数据集中的图像上生成的描述示例。可以观察到，生成的描述在长度和结构上相似（*即*，主要主题首先提及，然后是主要动作，最后是关于场景中其他视觉实体的额外细节）。这模仿了COCO中大多数真实描述的特点。另一个显著方面是缺乏计数能力（考虑*例如*图[12](#A2.F12
    "图 12 ‣ 附录 B 进一步性能分析 ‣ 从展示到叙述：基于深度学习的图像描述调查")第二行的第一个示例和底部行的第二个示例）。当前的方法在提及同一实体的正确数量时遇到困难，并且通常将多个实例称为一组。最后，值得提及的是描述不寻常概念的困难，包括情境和视觉实体（考虑*例如*图[11](#A2.F11
    "图 11 ‣ 附录 B 进一步性能分析 ‣ 从展示到叙述：基于深度学习的图像描述调查")中的前两个示例），这反映了缺乏泛化能力的症状。实际上，在这种情况下，不寻常的概念被描述为训练集中更常见的概念。例如，图[11](#A2.F11
    "图 11 ‣ 附录 B 进一步性能分析 ‣ 从展示到叙述：基于深度学习的图像描述调查")右上角的雪貂被描述为猫或鼠，而图[12](#A2.F12 "图 12
    ‣ 附录 B 进一步性能分析 ‣ 从展示到叙述：基于深度学习的图像描述调查")第二行最后一个示例中的历史装扮男子被描述为女性。对于VinVL而言，这一问题不那么明显，这突显了预训练在实现更好泛化能力中的作用。
- en: 'Finally, in Fig. [13](#A2.F13 "Figure 13 ‣ Appendix B Further Performance Analysis
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning"), we report
    a visualization of the attention states corresponding to the captions generated
    by two methods based on image regions, *i.e.* Up-Down [[58](#bib.bib58)], which
    performs attention over image regions, and $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)],
    which performs self-attention encoding. For visualization, we use the normalized
    attribution scores obtained for each image region via the Integrated Gradients
    approach [[255](#bib.bib255)] and projected between 0 and 1 by applying a contrast
    stretching function. In particular, we show the attended regions for each generated
    word and outline the region with the highest attribution score. With a focus on
    visual words, it can be observed that, for both approaches, the regions with the
    highest scores are coherent with the produced word. However, thanks to the more
    complex relations modeled via self-attention, $\mathcal{M}^{2}$ Transformer generally
    pays more attention to fewer, more precise regions compared to Up-Down (consider
    *e.g.* the region contributing the most to outputting tracks in the third example,
    or skateboard in the last one).'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在图 [13](#A2.F13 "图 13 ‣ 附录 B 进一步性能分析 ‣ 从展示到描述：基于深度学习的图像字幕生成综述")中，我们展示了基于图像区域的两种方法生成的字幕对应的注意力状态的可视化，*即* Up-Down [[58](#bib.bib58)]，它对图像区域进行注意力处理，以及
    $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)]，它进行自注意力编码。为了进行可视化，我们使用通过集成梯度方法
    [[255](#bib.bib255)] 获得的每个图像区域的归一化归因分数，并通过应用对比度拉伸函数将其投影到0到1之间。特别地，我们展示了每个生成词的注意区域，并突出显示了具有最高归因分数的区域。重点关注视觉词汇时，可以观察到，对于这两种方法，得分最高的区域与生成的词汇一致。然而，得益于通过自注意力建模的更复杂的关系，$\mathcal{M}^{2}$
    Transformer 通常相比于 Up-Down 更加关注较少的、更加精确的区域（例如*例如*第三个例子中对输出轨道贡献最大区域，或最后一个例子中的滑板）。
- en: 'TABLE V: Performance analysis of representative image captioning approaches
    in terms of different evaluation metrics on the Flickr30K datatset. The $\dagger$
    marker indicates models trained by us with ResNet-152 features, while the $\ddagger$
    marker indicates unofficial implementations. For all the metrics, the higher the
    value, the better ($\uparrow$).'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：Flickr30K 数据集上代表性图像字幕生成方法在不同评估指标下的性能分析。$\dagger$ 标记表示我们使用 ResNet-152 特征训练的模型，而
    $\ddagger$ 标记表示非官方实现。对于所有指标，值越高越好（$\uparrow$）。
- en: '|  |  |  |  | Standard Metrics |  |  | Diversity Metrics |  |  | Embedding-based
    Metrics |  |  | Learning-based Metrics |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 标准指标 |  |  | 多样性指标 |  |  | 基于嵌入的指标 |  |  | 基于学习的指标 |'
- en: '|  |  | Trained on |  | B-1 | B-4 | M | R | C | S |  |  | Div-1 | Div-2 | Vocab
    | %Novel |  |  | WMD | Alignment | Coverage |  |  | TIGEr | BERT-S | CLIP-S |
    CLIP-S${}^{\text{Ref}}$ |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 训练于 |  | B-1 | B-4 | M | R | C | S |  |  | Div-1 | Div-2 | Vocab |
    %Novel |  |  | WMD | 对齐 | 覆盖率 |  |  | TIGEr | BERT-S | CLIP-S | CLIP-S${}^{\text{Ref}}$
    |'
- en: '| Show and Tell^† [[23](#bib.bib23)] |  | COCO |  | 51.0 | 11.4 | 13.1 | 34.8
    | 22.8 | 7.6 |  |  | 0.037 | 0.093 | 331 | 94.8 |  |  | 8.6 | 0.019 | 61.9 |  |  |
    52.9 | 90.6 | 0.604 | 0.656 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| Show and Tell^† [[23](#bib.bib23)] |  | COCO |  | 51.0 | 11.4 | 13.1 | 34.8
    | 22.8 | 7.6 |  |  | 0.037 | 0.093 | 331 | 94.8 |  |  | 8.6 | 0.019 | 61.9 |  |  |
    52.9 | 90.6 | 0.604 | 0.656 |'
- en: '| Show, Attend and Tell^† [[42](#bib.bib42)] |  | COCO |  | 57.3 | 14.7 | 15.1
    | 38.8 | 29.4 | 9.4 |  |  | 0.044 | 0.124 | 402 | 96.3 |  |  | 9.5 | 0.053 | 63.7
    |  |  | 53.0 | 91.1 | 0.638 | 0.686 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| Show, Attend and Tell^† [[42](#bib.bib42)] |  | COCO |  | 57.3 | 14.7 | 15.1
    | 38.8 | 29.4 | 9.4 |  |  | 0.044 | 0.124 | 402 | 96.3 |  |  | 9.5 | 0.053 | 63.7
    |  |  | 53.0 | 91.1 | 0.638 | 0.686 |'
- en: '| Up-Down^‡ [[58](#bib.bib58)] |  | COCO |  | 65.5 | 19.5 | 18.6 | 44.0 | 42.6
    | 12.5 |  |  | 0.047 | 0.131 | 463 | 98.1 |  |  | 11.1 | 0.105 | 68.9 |  |  |
    53.6 | 91.9 | 0.682 | 0.719 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| Up-Down^‡ [[58](#bib.bib58)] |  | COCO |  | 65.5 | 19.5 | 18.6 | 44.0 | 42.6
    | 12.5 |  |  | 0.047 | 0.131 | 463 | 98.1 |  |  | 11.1 | 0.105 | 68.9 |  |  |
    53.6 | 91.9 | 0.682 | 0.719 |'
- en: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  | COCO |  | 67.9 | 21.0
    | 19.4 | 45.3 | 47.4 | 13.0 |  |  | 0.048 | 0.150 | 470 | 98.9 |  |  | 11.7 |
    0.106 | 67.0 |  |  | 53.7 | 91.8 | 0.680 | 0.721 |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  | COCO |  | 67.9 | 21.0
    | 19.4 | 45.3 | 47.4 | 13.0 |  |  | 0.048 | 0.150 | 470 | 98.9 |  |  | 11.7 |
    0.106 | 67.0 |  |  | 53.7 | 91.8 | 0.680 | 0.721 |'
- en: '| VinVL [[103](#bib.bib103)] |  | COCO |  | 74.3 | 28.4 | 23.5 | 51.1 | 75.2
    | 16.8 |  |  | 0.066 | 0.188 | 651 | 98.8 |  |  | 15.0 | 0.147 | 72.2 |  |  |
    53.6 | 93.1 | 0.754 | 0.787 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| VinVL [[103](#bib.bib103)] |  | COCO |  | 74.3 | 28.4 | 23.5 | 51.1 | 75.2
    | 16.8 |  |  | 0.066 | 0.188 | 651 | 98.8 |  |  | 15.0 | 0.147 | 72.2 |  |  |
    53.6 | 93.1 | 0.754 | 0.787 |'
- en: '| Show and Tell^† [[23](#bib.bib23)] |  | Flickr30K |  | 64.1 | 21.5 | 18.3
    | 44.4 | 41.7 | 12.2 |  |  | 0.037 | 0.075 | 373 | 84.5 |  |  | 11.2 | 0.090 |
    64.2 |  |  | 53.5 | 92.1 | 0.658 | 0.701 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
- en: '| Show, Attend and Tell^† [[42](#bib.bib42)] |  | Flickr30K |  | 65.6 | 23.6
    | 19.2 | 45.4 | 49.1 | 13.3 |  |  | 0.045 | 0.096 | 454 | 90.1 |  |  | 11.8 |
    0.089 | 64.1 |  |  | 53.4 | 92.1 | 0.679 | 0.717 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
- en: '| Up-Down^‡ [[58](#bib.bib58)] |  | Flickr30K |  | 72.4 | 28.3 | 21.6 | 49.5
    | 63.3 | 15.9 |  |  | 0.061 | 0.155 | 587 | 95.6 |  |  | 13.1 | 0.119 | 65.5 |  |  |
    53.5 | 92.7 | 0.720 | 0.755 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
- en: '| ORT [[77](#bib.bib77)] |  | Flickr30K |  | 72.2 | 30.1 | 22.8 | 50.4 | 68.8
    | 16.9 |  |  | 0.072 | 0.171 | 738 | 96.1 |  |  | 13.7 | 0.129 | 67.2 |  |  |
    53.5 | 92.7 | 0.728 | 0.760 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  | Flickr30K |  | 72.4
    | 29.8 | 22.4 | 50.6 | 68.4 | 16.2 |  |  | 0.079 | 0.196 | 728 | 93.8 |  |  |
    13.6 | 0.120 | 65.0 |  |  | 53.6 | 92.9 | 0.724 | 0.763 |  | ![Refer to caption](img/ed00a182cb129d9e124d7b9c96017455.png)
    |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/9bbd181eec893a0f164e5aaf4abea649.png) |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/6b15613efa7e851545893e1c02aa5c9f.png) |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
- en: 'Figure 11: Additional qualitative examples from eight popular captioning models
    on COCO test images.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/fbf9005e6823e993a64922abba6e6f6c.png) |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/535215756c59a41fd863801c4dffc9b1.png) |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/659d95a3ea0101fcc8569f8069cd0d1b.png) |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
- en: 'Figure 12: Additional qualitative examples from eight popular captioning models
    on COCO test images.'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/c7fa9ce5db800d51584cb543df85dd53.png) | ![Refer to
    caption](img/e400654d5e9d23176d3642d428b826b6.png) | ![Refer to caption](img/c831120ff2413cbdb0c31ea6634cabf8.png)
    | ![Refer to caption](img/739879922d4f6349ddc7a976ac9a9617.png) | ![Refer to caption](img/a9e484b17e063e59337a4cb1552f86c3.png)
    | ![Refer to caption](img/ebf924b0b873c17872c55c349b90bc15.png) | ![Refer to caption](img/fea81ffea1ac777286a454aab3f818a0.png)
    | ![Refer to caption](img/f848d6f72107fa3619514d29057623f6.png) |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/2c38e04c8d737b0d3ce5cab205c39bd4.png) | ![Refer to
    caption](img/fb483a68e03624a00fd505dbbee529f9.png) | ![Refer to caption](img/dfdff53c973a2d1e8c9a58956db2f16c.png)
    | ![Refer to caption](img/cb5b3e1919f00befd01ea83432e305cd.png) | ![Refer to caption](img/021a8ed94a7c4d04266c7497fbb7c7c0.png)
    | ![Refer to caption](img/c2be0a7385e2aa3fe0480b556c4ec9e4.png) | ![Refer to caption](img/0d08df5adf8c2af3f9fcda63dd379e66.png)
    | ![Refer to caption](img/df5713a89f0bc30ccbe3fd29ef428f04.png) |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/405354936bb2459ce0de07fc9e6b6c6c.png) | ![Refer to
    caption](img/a8a59af0603a4431e7cc26789fd4aeac.png) | ![Refer to caption](img/69b8dc60f47b38ee53d1871a2b6cc3b6.png)
    | ![Refer to caption](img/34e59b84634a083373f7c6c10f9ce0fd.png) | ![Refer to caption](img/3151f99d6833d4e6a9f932b2bc4ac8d3.png)
    | ![Refer to caption](img/8f5b2e3962bcf7ee761922417c35331b.png) | ![Refer to caption](img/f84878d99a2cff3455f106bd14a4c2af.png)
    | ![Refer to caption](img/4d9f52b9d25947539a7a5007ba67dcc0.png) |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/4fa3858c1a50b7393a16b751f6f78a59.png) | ![Refer to
    caption](img/36c0e3128d1d41805eb7af606f9a80e7.png) | ![Refer to caption](img/411b81e26cb40e4cc576ccc89eefa313.png)
    | ![Refer to caption](img/6b5fdcc586ef179d6122291d9b4461f8.png) | ![Refer to caption](img/0445a791ce92a6f917e535e94a39bda4.png)
    | ![Refer to caption](img/c0040e66a8e090507a6403ac352d2626.png) | ![Refer to caption](img/9f18e7bc3750a76bfdfbf792e05ae338.png)
    | ![Refer to caption](img/d774c5b04d07e33b60205eb016efa20a.png) |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/7a498e0d184a07525434c12d5bbf7937.png) | ![Refer to
    caption](img/6797e0fbd1c90afd6e59ad0e5476575e.png) | ![Refer to caption](img/272955280f29879fbd757b464d634b34.png)
    | ![Refer to caption](img/97347f62805bbf804955b39a037cf48d.png) | ![Refer to caption](img/5813006be150f20be1e007c08bdecc4a.png)
    | ![Refer to caption](img/f6035e2d1bb1d938351eeba06fb437de.png) | ![Refer to caption](img/4c715c7b54290b92030b6151c2f89a09.png)
    | ![Refer to caption](img/a561c3a1094cb3cf89f8c7489d3f120e.png) |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/b55e8f14e3701a6d02a125c595a4dfd6.png) | ![Refer to
    caption](img/1e7469d65e4e74efb5184e256599e1e5.png) | ![Refer to caption](img/8d6dc6b42d8c8f661756958ea4ff43fd.png)
    | ![Refer to caption](img/0f3953be61456b3d18858b62bd8f2806.png) | ![Refer to caption](img/664c6f4fe4dbb0d500792d473616d34b.png)
    | ![Refer to caption](img/5a4f3483e75755713678ff9a703eb23b.png) | ![Refer to caption](img/736b5f824ad516140040d99870d18422.png)
    | ![Refer to caption](img/fb9ea405cbf8ee093d20778996e8f3b4.png) |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/a25f13b723aff1f9962b25cfaf0b8d3c.png) | ![Refer to
    caption](img/71c63faf102858cf37d08979979604bc.png) | ![Refer to caption](img/844e6324a83883662ae09aa5f316ff63.png)
    | ![Refer to caption](img/c41a4dc61e5a8f7bd7de3a788e0908ac.png) | ![Refer to caption](img/2bbb9a2c76fd04ce46297fe987ae9740.png)
    | ![Refer to caption](img/6e92cf8aaa89e4c7bb1588586bbb0fb1.png) | ![Refer to caption](img/e5c34c69046f4dc70441aea25575b129.png)
    | ![Refer to caption](img/4180b0d1cb82a5c27d9d159e830525c2.png) |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/8610e30d9a218317375a614675415e81.png) | ![Refer to
    caption](img/a2e854b7544a8eae79e9c16e92ab675b.png) | ![Refer to caption](img/5241838e6979fbfc3382a99bf594d655.png)
    | ![Refer to caption](img/4445fc2c60da808e4e1ae157ed48d889.png) | ![Refer to caption](img/6d2480dcc33367274a2f86d4da803a1b.png)
    | ![Refer to caption](img/6f18b3d914899ff733c3ad31d3459d38.png) | ![Refer to caption](img/bfbb35ca971bf5def1de9cdaec727a8f.png)
    | ![Refer to caption](img/4ae471c51f2b6169965d60700924ffa8.png) |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/0cb0cb5640f2a3dde73db95850ffe9f1.png) | ![Refer to
    caption](img/14cad532146e889afef62c2c7f41fe4e.png) | ![Refer to caption](img/c302d520e15c1bb33538735e85712c5d.png)
    | ![Refer to caption](img/9029035755fd48f0607b94dc7377de63.png) | ![Refer to caption](img/5c3dd1e0ee20524f1c386fe40f28bc02.png)
    | ![Refer to caption](img/3004af1e4e7ba028e483ce753c36ee72.png) | ![Refer to caption](img/318af55aa4fcf44c2abe8c5c11eb02e4.png)
    | ![Refer to caption](img/9a99880ff9320a524dd3e6a5d2abf3b2.png) |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/777bb9ab0c681035ab8f1afdd13488b8.png) | ![Refer to
    caption](img/0a9fa5422774b6233fd98f31007f289f.png) | ![Refer to caption](img/c5cca027fdeb0164f5306a108d685710.png)
    | ![Refer to caption](img/bdde6429844aa52107356e8b23f63702.png) | ![Refer to caption](img/1278a059d15a6fdb344e12a7c9efd3b1.png)
    | ![Refer to caption](img/2a82a45e1cc9f4a9f2b4b27133bad068.png) | ![Refer to caption](img/3338131ddd2d664af1c3c31c84dd70de.png)
    | ![Refer to caption](img/58d17ed396eaebb3ce0d944eeafaecf1.png) |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
- en: 'Figure 13: Visualization of attention states for sample captions generated
    by Up-Down [[58](#bib.bib58)] and $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)].
    For each generated word, we show the attended image regions, outlining the region
    with the maximum output attribution in green and blue, respectively.'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
