- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:52:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[2107.06912] From Show to Tell: A Survey on Deep Learning-based Image Captioning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.06912](https://ar5iv.labs.arxiv.org/html/2107.06912)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'From Show to Tell: A Survey on'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning-based Image Captioning
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Giuseppe Fiameni, and Rita Cucchiara M. Stefanini, M. Cornia, L. Baraldi, S.
    Cascianelli, and R. Cucchiara are with the Department of Engineering “Enzo Ferrari”,
    University of Modena and Reggio Emilia, Modena, Italy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: {matteo.stefanini, marcella.cornia, lorenzo.baraldi, silvia.cascianelli,
    rita.cucchiara}@unimore.it. G. Fiameni is with NVIDIA AI Technology Centre, Italy.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: gfiameni@nvidia.com'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Connecting Vision and Language plays an essential role in Generative Intelligence.
    For this reason, large research efforts have been devoted to image captioning,
    *i.e.* describing images with syntactically and semantically meaningful sentences.
    Starting from 2015 the task has generally been addressed with pipelines composed
    of a visual encoder and a language model for text generation. During these years,
    both components have evolved considerably through the exploitation of object regions,
    attributes, the introduction of multi-modal connections, fully-attentive approaches,
    and BERT-like early-fusion strategies. However, regardless of the impressive results,
    research in image captioning has not reached a conclusive answer yet. This work
    aims at providing a comprehensive overview of image captioning approaches, from
    visual encoding and text generation to training strategies, datasets, and evaluation
    metrics. In this respect, we quantitatively compare many relevant state-of-the-art
    approaches to identify the most impactful technical innovations in architectures
    and training strategies. Moreover, many variants of the problem and its open challenges
    are discussed. The final goal of this work is to serve as a tool for understanding
    the existing literature and highlighting the future directions for a research
    area where Computer Vision and Natural Language Processing can find an optimal
    synergy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Image Captioning, Vision-and-Language, Deep Learning, Survey.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image captioning is the task of describing the visual content of an image in
    natural language, employing a visual understanding system and a language model
    capable of generating meaningful and syntactically correct sentences. Neuroscience
    research has clarified the link between human vision and language generation only
    in the last few years [[1](#bib.bib1)]. Similarly, in Artificial Intelligence,
    the design of architectures capable of processing images and generating language
    is a very recent matter. The goal of these research efforts is to find the most
    effective pipeline to process an input image, represent its content, and transform
    that into a sequence of words by generating connections between visual and textual
    elements while maintaining the fluency of language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The early-proposed approaches to image captioning have entailed description
    retrieval [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7)] or template filling and hand-crafted natural language generation
    techniques [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)]. While
    these have been treated in other surveys [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18)], image captioning is currently based on the usage of deep learning-based
    generative models. In its standard configuration, the task is an image-to-sequence
    problem whose inputs are pixels. These inputs are encoded as one or multiple feature
    vectors in the visual encoding step, which prepares the input for a second generative
    step, called the language model. This produces a sequence of words or sub-words
    decoded according to a given vocabulary.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'In these few years, the research community has improved model design considerably:
    from the first deep learning-based proposals adopting Recurrent Neural Networks
    (RNNs) fed with global image descriptors, methods have been enriched with attentive
    approaches and reinforcement learning up to the breakthroughs of Transformers
    and self-attention and single-stream BERT-like approaches. At the same time, the
    Computer Vision and Natural Language Processing (NLP) communities have addressed
    the challenge of building proper evaluation protocols and metrics to compare results
    with human-generated ground-truths. However, despite the investigation and improvements
    achieved in these years, image captioning is still far from being considered a
    solved task.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Several domain-specific proposals and variants of the task have also been investigated
    to accommodate for different user needs and descriptions styles. According to [[19](#bib.bib19),
    [20](#bib.bib20)], indeed, image captions can be perceptual, when focusing on
    low-level visual attributes; non-visual, when reporting implicit and contextual
    information; conceptual, when describing the actual visual content (*e.g.* visual
    entities and their relations). While the latter is commonly recognized as the
    target of the image captioning task, this definition encompasses descriptions
    focusing on different aspects and at various levels of detail (*e.g.* including
    attributes or not, mentioning named entities or high-level concepts only, describing
    salient parts only, or also finer details).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: With the aim of providing a testament to the journey that captioning has taken
    so far, and with that of encouraging novel ideas, we trace a holistic overview
    of techniques, models, and task variants developed in the last years. Furthermore,
    we review datasets and evaluation metrics and perform quantitative comparisons
    of the main approaches. Finally, we discuss open challenges and future directions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributions. To sum up, the contributions of this survey are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the inherent dual nature of captioning models, we develop taxonomies
    for visual encoding and language modeling approaches and describe their key aspects
    and limitations.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We review the training strategies adopted in the literature over the past years
    and the recent advancement obtained by the pre-training paradigm and masked language
    model losses.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We review the main datasets used to explore image captioning, both domain-generic
    benchmarks and domain-specific datasets collected to investigate specific aspects.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyze both standard and non-standard metrics adopted for performance evaluation
    and the characteristics of the caption they highlight.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present a quantitative comparison of the main image captioning methods considering
    both standard and non-standard metrics and a discussion on their relationships,
    which sheds light on performance, differences, and characteristics of the most
    important models.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We give an overview of many variants of the task and discuss open challenges
    and future directions.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compared to previous surveys on image captioning [[18](#bib.bib18), [17](#bib.bib17),
    [21](#bib.bib21), [22](#bib.bib22), [16](#bib.bib16)], we provide a comprehensive
    and updated view on deep learning-based generative captioning models. We perform
    a deeper analysis of proposed approaches and survey a considerably larger number
    of papers on the topic. Also, we cover non-standard evaluation metrics, which
    are disregarded by other works, discuss their characteristics, and employ them
    in a quantitative evaluation of state-of-the-art methods. Moreover, we tackle
    emerging variants of the task and a broader set of available datasets.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1182d4c98cd348d893bd6e069d54b076.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of the image captioning task and taxonomy of the most relevant
    approaches.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 2 Visual Encoding
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Providing an effective representation of the visual content is the first challenge
    of an image captioning pipeline. The current approaches for visual encoding can
    be classified as belonging to four main categories: 1. *non-attentive methods*
    based on global CNN features; 2. *additive attentive methods* that embed the visual
    content using either grids or regions; 3. *graph-based methods* adding visual
    relationships between visual regions; and 4. *self-attentive methods* that employ
    Transformer-based paradigms, either by using region-based, patch-based, or image-text
    early fusion solutions. This taxonomy is visually summarized in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cd2286ba240977f732fa99f8740ac3e2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: (a)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9da0e966afb4cc81b5ec8fdbae0b3400.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: (b)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1e77237cc8b4da05134aaa60133d5965.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: (c)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Three of the most relevant visual encoding strategies for image captioning:
    (a) global CNN features; (b) fine-grained features extracted from the activation
    of a convolutional layer, together with an attention mechanism guided by the language
    model; (c) image region features coming from a detector, together with an attention
    mechanism.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Global CNN Features
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the advent of CNNs, all models consuming visual inputs have been improved
    in terms of performance. The visual encoding step of image captioning is no exception.
    In the most simple recipe, the activation of one of the last layers of a CNN is
    employed to extract high-level representations, which are then used as a conditioning
    element for the language model (Fig. [2(a)](#S2.F2.sf1 "In Figure 2 ‣ 2 Visual
    Encoding ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")).
    This is the approach employed in the seminal “Show and Tell” paper [[23](#bib.bib23)]¹¹1The
    title of this survey is a tribute of this pioneering work., where the output of
    GoogleNet [[24](#bib.bib24)] is fed to the initial hidden state of the language
    model. In the same year, Karpathy *et al.* [[25](#bib.bib25)] used global features
    extracted from AlexNet [[26](#bib.bib26)] as the input for a language model. Further,
    Mao *et al.* [[27](#bib.bib27)] and Donahue *et al.* [[28](#bib.bib28)] injected
    global features extracted from the VGG network [[29](#bib.bib29)] at each time-step
    of the language model.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Global CNN features were then employed in a large variety of image captioning
    models [[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]. Notably,
    Rennie *et al.* [[38](#bib.bib38)] introduced the FC model, in which images are
    encoded using a ResNet-101 [[39](#bib.bib39)], preserving their original dimensions.
    Other approaches [[40](#bib.bib40), [41](#bib.bib41)] integrated high-level attributes
    or tags, represented as a probability distribution over the most common words
    of the training captions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of employing global CNN features resides in their simplicity
    and compactness of representation, which embraces the capacity to extract and
    condense information from the whole input and to consider the overall context
    of an image. However, this paradigm also leads to excessive compression of information
    and lacks granularity, making it hard for a captioning model to produce specific
    and fine-grained descriptions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Attention Over Grid of CNN Features
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Motivated by the drawbacks of global representations, most of the following
    approaches have increased the granularity level of visual encoding [[42](#bib.bib42),
    [38](#bib.bib38), [43](#bib.bib43)]. For instance, Dai *et al.*[[44](#bib.bib44)]
    have employed 2D activation maps in place of 1D global feature vectors to bring
    spatial structure directly in the language model. Drawing from machine translation
    literature, a big portion of the captioning community has instead employed the
    additive attention mechanism (Fig. [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2 Visual Encoding
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")), which
    has endowed image captioning architectures with time-varying visual features encoding,
    enabling greater flexibility and finer granularity.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition of additive attention. The intuition behind attention boils down
    to weighted averaging. In the first formulation proposed for sequence alignment
    by Bahdanau *et al.* [[45](#bib.bib45)] (also known as additive attention), a
    single-layer feed-forward neural network with a hyperbolic tangent non-linearity
    is used to compute attention weights. Formally, given two generic sets of vectors
    $\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}$ and $\{\mathbf{h}_{1},\ldots,\mathbf{h}_{m}\}$,
    the additive attention score between $\mathbf{h}_{i}$ and $\mathbf{x}_{j}$ is
    computed as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{\mathrm{att}}\left(\mathbf{h}_{i},\mathbf{x}_{j}\right)=\mathbf{W}_{3}^{\top}\tanh\left(\mathbf{W}_{1}\mathbf{h}_{i}+\mathbf{W}_{2}\mathbf{x}_{j}\right),$
    |  | (1) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{W}_{1}$ and $\mathbf{W}_{2}$ are weight matrices, and $\mathbf{W}_{3}$
    is a weight vector that performs a linear combination. A softmax function is then
    applied to obtain a probability distribution $p\left(\mathbf{x}_{j}\mid\mathbf{h}_{i}\right)$,
    representing how much the element encoded by $\mathbf{x}_{j}$ is relevant for
    $\mathbf{h}_{i}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Although the attention mechanism was initially devised for modeling the relationships
    between two sequences of elements (*i.e.* hidden states from a recurrent encoder
    and a decoder), it can be adapted to connect a set of visual representations with
    the hidden states of a language model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Attending convolutional activations. Xu *et al.* [[42](#bib.bib42)] introduced
    the first method leveraging the additive attention over the spatial output grid
    of a convolutional layer. This allows the model to selectively focus on certain
    elements of the grid by selecting a subset of features for each generated word.
    Specifically, the model first extracts the activation of the last convolutional
    layer of a VGG network [[29](#bib.bib29)], then uses additive attention to compute
    a weight for each grid element, interpreted as the relative importance of that
    element for generating the next word.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches. The solution based on additive attention over a grid of features
    has been widely adopted by several following works with minor improvements in
    terms of visual encoding [[40](#bib.bib40), [46](#bib.bib46), [43](#bib.bib43),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Review networks – For instance, Yang *et al.* [[50](#bib.bib50)] supplemented
    the encoder-decoder framework with a recurrent review network. This performs a
    given number of review steps with attention on the encoder hidden states and outputs
    a “thought vector” after each step, which is then used by the attention mechanism
    in the decoder.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Multi-level features – Chen *et al.* [[51](#bib.bib51)] proposed to employ channel-wise
    attention over convolutional activations, followed by a more classical spatial
    attention. They also experimented with using more than one convolutional layer
    to exploit multi-level features. On the same line, Jiang *et al.* [[52](#bib.bib52)]
    proposed to use multiple CNNs in order to exploit their complementary information,
    then fused their representations with a recurrent procedure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Exploiting human attention – Some works also integrated saliency information
    (*i.e.* what do humans pay more attention to in a scene) to guide caption generation
    with stimulus-based attention. This idea was first explored by Sugano and Bulling [[53](#bib.bib53)]
    who exploited human eye fixations for image captioning by including normalized
    fixation histograms over the image as an input to the soft-attention module of [[42](#bib.bib42)]
    and weighing the attended image regions based on whether these are fixated or
    not. Subsequent works on this line [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57)] employed saliency maps as a form of additional attention source.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae2e209e6f5a8dd86a552acd439d465f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: (a)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0670cfe3f326af220e80d22fcb023052.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: (b)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Summary of the two most recent visual encoding strategies for image
    captioning: (a) graph-based encoding of visual regions; (b) self-attention-based
    encoding over image region features.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Attention Over Visual Regions
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The intuition of using saliency boils down to neuroscience, which suggests that
    our brain integrates a top-down reasoning process with a bottom-up flow of visual
    signals. The top-down path consists of predicting the upcoming sensory input by
    leveraging our knowledge and inductive bias, while the bottom-up flow provides
    visual stimuli adjusting the previous predictions. Additive attention can be thought
    of as a top-down system. In this mechanism, the language model predicts the next
    word while attending a feature grid, whose geometry is irrespective of the image
    content.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Bottom-up and top-down attention. Differently from saliency-based approaches [[57](#bib.bib57)],
    in the solution proposed by Anderson *et al.* [[58](#bib.bib58)] the bottom-up
    path is defined by an object detector in charge of proposing image regions. This
    is then coupled with a top-down mechanism that learns to weigh each region for
    each word prediction (see Fig. [2(c)](#S2.F2.sf3 "In Figure 2 ‣ 2 Visual Encoding
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")). In this
    approach, Faster R-CNN [[59](#bib.bib59), [60](#bib.bib60)] is adopted to detect
    objects, obtaining a pooled feature vector for each region proposal. One of the
    key elements of this approach resides in its pre-training strategy, where an auxiliary
    training loss is added for learning to predict attribute classes alongside object
    classes on the Visual Genome [[61](#bib.bib61)] dataset. This allows the model
    to predict a dense and rich set of detections, including both salient object and
    contextual regions, and favors the learning of better feature representations.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches. Employing image region features has demonstrated its advantages
    when dealing with the raw visual input and has been the standard de-facto in image
    captioning for years. As a result, many of the following works have based the
    visual encoding phase on this strategy [[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65)]. Among them, we point out two remarkable variants.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Visual Policy – While typical visual attention points to a single image region
    at every step, the approach proposed by Zha *et al.* [[66](#bib.bib66)] introduces
    a sub-policy network that interprets also the visual part sequentially by encoding
    historical visual actions (*e.g.* previously attended regions) via an LSTM to
    serve as context for the next visual action.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Transforms – Pedersoli *et al.* [[67](#bib.bib67)] proposed to use
    spatial transformers for generating image-specific attention areas by regressing
    region proposals in a weakly-supervised fashion. Specifically, a localization
    network learns an affine transformation or each location of the feature map, and
    then a bilinear interpolation is used to regress a feature vector for each region
    with respect to anchor boxes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Graph-based Encoding
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further improve the encoding of image regions and their relationships, some
    studies consider using graphs built over image regions (see Fig. [3(a)](#S2.F3.sf1
    "In Figure 3 ‣ 2.2 Attention Over Grid of CNN Features ‣ 2 Visual Encoding ‣ From
    Show to Tell: A Survey on Deep Learning-based Image Captioning")) to enrich the
    representation by including semantic and spatial connections.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Spatial and semantic graphs. The first attempt in this sense is due to Yao *et
    al.* [[68](#bib.bib68)], followed by Guo *et al.* [[69](#bib.bib69)], who proposed
    the use of a graph convolutional network (GCN) [[70](#bib.bib70)] to integrate
    both semantic and spatial relationships between objects. The semantic relationships
    graph is obtained by applying a classifier pre-trained on Visual Genome [[61](#bib.bib61)]
    that predicts an action or an interaction between object pairs. The spatial relationships
    graph is instead inferred through geometry measures (*i.e.* intersection over
    union, relative distance, and angle) between bounding boxes of object pairs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Scene graphs. With a focus on modeling semantic relations, Yang *et al.* [[71](#bib.bib71)]
    proposed to integrate semantic priors learned from text in the image encoding
    by exploiting a graph-based representation of both images and sentences. The representation
    used is the scene graph, *i.e.* a directed graph connecting the objects, their
    attributes, and their relations. On the same line, Shi *et al.* [[72](#bib.bib72)]
    represented the image as a semantic relationship graph but proposed to train the
    module in charge of predicting the predicate nodes directly on the ground-truth
    captions rather than on external datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical trees. As a special case of a graph-based encoding, Yao *et al.* [[73](#bib.bib73)]
    employed a tree to represent the image as a hierarchical structure. The root represents
    the image as a whole, intermediate nodes represent image regions and their contained
    sub-regions, and the leaves represent segmented objects in the regions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Graph encodings brought a mechanism to leverage relationships between detected
    objects, which allows the exchange of information in adjacent nodes and thus in
    a local manner. Further, it seamlessly allows the integration of external semantic
    information. On the other hand, manually building the graph structure can limit
    the interactions between visual features. This is where self-attention proved
    to be more successful by connecting all the elements with each other in a complete
    graph representation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Self-Attention Encoding
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Self-attention is an attentive mechanism where each element of a set is connected
    with all the others, and that can be adopted to compute a refined representation
    of the same set of elements through residual connections (Fig. [3(b)](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Attention Over Grid of CNN Features ‣ 2 Visual Encoding ‣ From
    Show to Tell: A Survey on Deep Learning-based Image Captioning")). It was first
    introduced by Vaswani *et al.* [[74](#bib.bib74)] for machine translation and
    language understanding tasks, giving birth to the Transformer architecture and
    its variants, which have dominated the NLP field and later also Computer Vision.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition of self-attention. Formally, self-attention makes use of the scaled
    dot-product mechanism, *i.e.* a multiplicative attention operator that handles
    three sets of vectors: a set of $n_{q}$ query vectors $\bm{Q}$, a set of key vectors
    $\bm{K}$, and a set of value vectors $\bm{V}$, both containing $n_{k}$ elements.
    The operator takes a weighted sum of value vectors according to a similarity distribution
    between query and key vectors:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathsf{Attention}(\bm{Q},\bm{K},\bm{V})=\operatorname{softmax}\left(\frac{\bm{Q}\bm{K}^{T}}{\sqrt{d_{k}}}\right)\bm{V},$
    |  | (2) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: where $d_{k}$ is a scaling factor. In the case of self-attention, the three
    sets of vectors are obtained as linear projections of the same input set of elements.
    The success of the Transformer demonstrates that leveraging self-attention allows
    achieving superior performances compared to attentive RNNs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Early self-attention approaches. Among the first image captioning models leveraging
    this approach, Yang *et al.* [[75](#bib.bib75)] used a self-attentive module to
    encode relationships between features coming from an object detector. Later, Li *et
    al.* [[76](#bib.bib76)] proposed a Transformer model with a visual encoder for
    the region features coupled with a semantic encoder that exploits knowledge from
    an external tagger. Both encoders are based on self-attention and feed-forward
    layers. Their output is then fused through a gating mechanism governing the propagation
    of visual and semantic information.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Variants of the self-attention operator. Other works proposed variants or modifications
    of the self-attention operator tailored for image captioning [[77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Geometry-aware encoding – Herdade *et al.* [[77](#bib.bib77)] introduced a modified
    version of self-attention that takes into account the spatial relationships between
    regions. In particular, an additional geometric weight is computed between object
    pairs and is used to scale the attention weights. On a similar line, Guo *et al.* [[78](#bib.bib78)]
    proposed a normalized and geometry-aware version of self-attention that makes
    use of the relative geometry relationships between input objects. Further, He *et
    al.* [[82](#bib.bib82)] introduced a spatial graph transformer, which considers
    different categories of spatial relationship between detections (*e.g.*, parent,
    neighbor, child) when performing attention.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Attention on Attention – Huang *et al.* [[79](#bib.bib79)] proposed an extension
    of the attention operator in which the final attended information is weighted
    by a gate guided by the context. Specifically, the output of the self-attention
    is concatenated with the queries, then an information and a gate vector are computed
    and finally multiplied together. In their encoder, they employed this mechanism
    to refine the visual features. This method is then adopted by later models such
    as [[83](#bib.bib83)].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: X-Linear Attention – Pan *et al.* [[80](#bib.bib80)] proposed to use bilinear
    pooling techniques to strengthen the representative capacity of the output attended
    feature. Notably, this mechanism encodes the region-level features with higher-order
    interaction, leading to a set of enhanced region-level and image-level features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Memory-augmented Attention – Cornia *et al.* [[81](#bib.bib81), [84](#bib.bib84)]
    proposed a Transformer-based architecture where the self-attention operator of
    each encoder layer is augmented with a set of memory vectors. Specifically, the
    set of keys and values is extended with additional “slots” learned during training,
    which can encode multi-level visual relationships.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Other self-attention-based approaches. Ji *et al.* [[85](#bib.bib85)] proposed
    to improve self-attention by adding to the sequence of feature vectors a global
    vector computed as their average. A global vector is computed for each layer,
    and the resulting global vectors are combined via an LSTM, thus obtaining an inter-layer
    representation. Luo *et al.* [[86](#bib.bib86)] proposed a hybrid approach that
    combines region and grid features to exploit their complementary advantages. Two
    self-attention modules are applied independently to each kind of features, and
    a cross-attention module locally fuses their interactions. On a different line,
    the architecture proposed by Liu *et al.* [[87](#bib.bib87)] is based on an attention
    module to align grid or detection features with visual words extracted from a
    concept extractor and to obtain semantic-grounded encodings.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3a1d3d5d3d89e48cb53a7e4a4d0127f6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Vision Transformer encoding. The image is split into fixed-size patches,
    linearly embedded, added to position embeddings, and fed to a standard Transformer
    encoder.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention on grid features and patches. Other than applying the attention operator
    on detections, the role of grid features has been recently re-evaluated [[88](#bib.bib88)].
    For instance, the approach proposed by Zhang *et al.* [[89](#bib.bib89)] applies
    self-attention directly to grid features, incorporating their relative geometry
    relationships into self-attention computation. Transformer-like architectures
    can also be applied directly on image patches, thus excluding the usage of the
    convolutional operator [[90](#bib.bib90), [91](#bib.bib91)] (Fig. [4](#S2.F4 "Figure
    4 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual Encoding ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning")). On this line, Liu *et al.* [[92](#bib.bib92)]
    devised the first convolution-free architecture for image captioning. Specifically,
    a pre-trained Vision Transformer network (*i.e.* ViT [[90](#bib.bib90)]) is adopted
    as encoder, and a standard Transformer decoder is employed to generate captions.
    Interestingly, the same visual encoding approach has been adopted in CLIP [[93](#bib.bib93)]
    and SimVLM [[94](#bib.bib94)], with the difference that the visual encoder is
    trained from scratch on large-scale noisy data. CLIP-based features have then
    been used by subsequent captioning approaches [[95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97)].'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Early fusion and vision-and-language pre-training. Other works using self-attention
    to encode visual features achieved remarkable performance also thanks to vision-and-language
    pre-training [[98](#bib.bib98), [99](#bib.bib99)] and early-fusion strategies [[100](#bib.bib100),
    [101](#bib.bib101)]. For example, following the BERT architecture [[102](#bib.bib102)],
    Zhou *et al.* [[101](#bib.bib101)] combined encoder and decoder into a single
    stream of Transformer layers, where region and word tokens are early fused together
    into a unique flow. This unified model is first pre-trained on large amounts of
    image-caption pairs to perform both bidirectional and sequence-to-sequence prediction
    tasks and then fine-tuned.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: On the same line, Li *et al.* [[100](#bib.bib100)] proposed OSCAR, a BERT-like
    architecture that includes object tags as anchor points to ease the semantic alignment
    between images and text. They also performed a large-scale pre-train with $6.5$
    million image-text pairs, with a masked token loss similar to the BERT mask language
    loss and a contrastive loss for distinguishing aligned words-tags-regions triples
    from polluted ones. Later, Zhang *et al.* [[103](#bib.bib103)] proposed VinVL,
    built on top of OSCAR, introducing a new object detector capable of extracting
    better visual features and a modified version of the vision-and-language pre-training
    objectives. On this line, Hu *et al.* [[104](#bib.bib104)] improved the VinVL
    model by scaling up its size and using larger scale noisy data to pre-train.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8cda9db270d0a64b2e8bfbd31fd4118f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: (a)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef8d4853073f358de43616679ea1c662.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: (b)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/693ee75f721b6f927375b4cd894b04e9.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: (c)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b349a9d3ea14d7fc268a2a41bd0fb09c.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: (d)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: LSTM-based language modeling strategies: (a) Single-Layer LSTM model
    conditioned on the visual feature; (b) LSTM with attention, as proposed in the
    Show, Attend and Tell model [[42](#bib.bib42)]; (c) LSTM with attention, in the
    variant proposed in [[43](#bib.bib43)]; (d) two-layer LSTM with attention, in
    the style of the bottom-up top-down approach by Anderson *et al.* [[58](#bib.bib58)].
    In all figures, $\bm{X}$ represents either a grid of CNN features or image region
    features extracted by an object detector.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Discussion
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the emergence of global features and grid features, region-based features
    have been the state-of-the-art choice in image captioning for years thanks to
    their compelling performances. Recently, however, different factors are reopening
    the discussion on which feature model is most appropriate for image captioning,
    ranging from the performance of better-trained grid features [[88](#bib.bib88)]
    to the emergence of self-attentive visual encoders [[90](#bib.bib90)] and large-scale
    multi-modal models like CLIP [[93](#bib.bib93)]. Recent strategies encompass training
    better object detectors on large-scale data [[103](#bib.bib103)] or employing
    end-to-end visual models trained from scratch [[94](#bib.bib94)]. Moreover, the
    success of BERT-like solutions performing image and text early-fusion indicates
    the suitability of visual representations that also integrate textual information.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 3 Language Models
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of a language model is to predict the probability of a given sequence
    of words to occur in a sentence. As such, it is a crucial component in image captioning,
    as it gives the ability to deal with natural language as a stochastic process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, given a sequence of $n$ words, the language model component of an
    image captioning algorithm assigns a probability $P\left(y_{1},y_{2},\ldots,y_{n}\mid\bm{X}\right)$
    to the sequence as:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P\left(y_{1},y_{2},\ldots y_{n}\mid\bm{X}\right)=\prod_{i=1}^{n}P\left(y_{i}\mid
    y_{1},y_{2},\ldots,y_{i-1},\bm{X}\right),$ |  | (3) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: where $\bm{X}$ represents the visual encoding on which the language model is
    specifically conditioned. Notably, when predicting the next word given the previous
    ones, the language model is auto-regressive, which means that each predicted word
    is conditioned on the previous ones. The language model usually also decides when
    to stop generating caption words by outputting a special end-of-sequence token.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'The main language modeling strategies applied to image captioning can be categorized
    as: 1. *LSTM-based* approaches, which can be either single-layer or two-layer;
    2. *CNN-based* methods that constitute a first attempt in surpassing the fully
    recurrent paradigm; 3. *Transformer-based* fully-attentive approaches; 4. *image-text
    early-fusion* (BERT-like) strategies that directly connect the visual and textual
    inputs. This taxonomy is visually summarized in Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 LSTM-based Models
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As language has a sequential structure, RNNs are naturally suited to deal with
    the generation of sentences. Among RNN variants, LSTM [[105](#bib.bib105)] has
    been the predominant option for language modeling.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Single-layer LSTM
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most simple LSTM-based captioning architecture is based on a single-layer
    LSTM and was proposed by Vinyals *et al.* [[23](#bib.bib23)]. As shown in Fig. [5(a)](#S2.F5.sf1
    "In Figure 5 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual Encoding ‣ From Show to
    Tell: A Survey on Deep Learning-based Image Captioning"), the visual encoding
    is used as the initial hidden state of the LSTM, which then generates the output
    caption. At each time step, a word is predicted by applying a softmax activation
    function over the projection of the hidden state into a vector of the same size
    as the vocabulary. During training, input words are taken from the ground-truth
    sentence, while during inference, input words are those generated at the previous
    step.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Shortly after, Xu *et al.* [[42](#bib.bib42)] introduced the additive attention
    mechanism. As depicted in Fig. [5(b)](#S2.F5.sf2 "In Figure 5 ‣ 2.5 Self-Attention
    Encoding ‣ 2 Visual Encoding ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), in this case, the previous hidden state guides the attention
    mechanism over the visual features $\bm{X}$, computing a context vector which
    is then fed to the MLP in charge of predicting the output word.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches. Many subsequent works have adopted a decoder based on a single-layer
    LSTM, mostly without any architectural changes [[50](#bib.bib50), [51](#bib.bib51),
    [67](#bib.bib67)], while others have proposed significant modifications, summarized
    below.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual sentinel – Lu *et al.* [[43](#bib.bib43)] augmented the spatial image
    features with an additional learnable vector, called visual sentinel, which can
    be attended by the decoder in place of visual features while generating “non-visual”
    words (*e.g.* “the”, “of”, and “on”), for which visual features are not needed
    (Fig. [5(c)](#S2.F5.sf3 "In Figure 5 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual
    Encoding ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")).
    At each time step, the visual sentinel is computed from the previous hidden state
    and generated word. Then, the model generates a context vector as a combination
    of attended image features and visual sentinel, whose importance is weighted by
    a learnable gate.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Hidden state reconstruction – Chen *et al.* [[46](#bib.bib46)] proposed to regularize
    the transition dynamics of the language model by using a second LSTM for reconstructing
    the previous hidden state based on the current one. Ge *et al.* [[48](#bib.bib48)]
    enhance context modeling by by using a bidirectional LSTM with an auxiliary module.
    The auxiliary module in a direction approximates the hidden state of the LSTM
    in the other direction. Finally, a cross-modal attention mechanism combines grid
    visual features with the two sentences from the bidirectional LSTM to obtain the
    final caption.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-stage generation – Wang *et al.* [[47](#bib.bib47)] proposed to generate
    a caption from coarse central aspects to finer attributes by decomposing the caption
    generation process into two phases: skeleton sentence generation and attributes
    enriching, both implemented with single-layer LSTMs. On the same line, Gu *et
    al.* [[49](#bib.bib49)] devised a coarse-to-fine multi-stage framework using a
    sequence of LSTM decoders, each operating on the output of the previous one to
    produce increasingly refined captions.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Semantic-guided LSTM – Jia *et al.* [[32](#bib.bib32)] proposed an extension
    of LSTM that includes semantic information extracted from the image to guide the
    generation. Specifically, the semantic information is used as an extra input to
    each gate in the LSTM block.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Two-layer LSTM
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LSTMs can be expanded to multi-layer structures to augment their capability
    of capturing higher-order relations. Donahue *et al.* [[28](#bib.bib28)] firstly
    proposed a two-layer LSTM as a language model for captioning, stacking two layers,
    where the hidden states of the first are the input to the second.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Two-layers and additive attention. Anderson *et al.* [[58](#bib.bib58)] went
    further and proposed to specialize the two layers to perform visual attention
    and the actual language modeling. As shown in Fig. [5(d)](#S2.F5.sf4 "In Figure
    5 ‣ 2.5 Self-Attention Encoding ‣ 2 Visual Encoding ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning"), the first LSTM layer acts as a top-down
    visual attention model which takes the previously generated word, the previous
    hidden state, and the mean-pooled image features. Then, the current hidden state
    is used to compute a probability distribution over image regions with an additive
    attention mechanism. The so-obtained attended image feature vector is fed to the
    second LSTM layer, which combines it with the hidden state of the first layer
    to generate a probability distribution over the vocabulary.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Variants of two-layers LSTM. Because of their representation power, LSTMs with
    two-layers and internal attention mechanisms represent the most employed language
    model approach before the advent of Transformer-based architectures [[68](#bib.bib68),
    [71](#bib.bib71), [73](#bib.bib73), [72](#bib.bib72)]. As such, many other variants
    have been proposed to improve the performance of this approach.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Neural Baby Talk – To ground words into image regions, Lu *et al.* [[106](#bib.bib106)]
    incorporated a pointing network that modulates the content-based attention mechanism.
    In particular, during the generation process, the network predicts slots in the
    caption, which are then filled with the image region classes. For non-visual words,
    a visual sentinel is used as dummy grounding. This approach leverages the object
    detector both as a feature region extractor and as a visual word prompter for
    the language model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Reflective attention – Ke *et al.* [[62](#bib.bib62)] introduced two reflective
    modules: while the first computes the relevance between hidden states from all
    the past predicted words and the current one, the second improves the syntactic
    structure of the sentence by guiding the generation process with words common
    position information.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Look back and predict forward – On a similar line, Qin *et al.* [[63](#bib.bib63)]
    used two modules: the look back module that takes into account the previous attended
    vector to compute the next one, and the predict forward module that predicts the
    new two words at once, thus alleviating the accumulated errors problem that may
    occur at inference time.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive attention time – Huang *et al.* [[64](#bib.bib64)] proposed an adaptive
    attention time mechanism, in which the decoder can take an arbitrary number of
    attention steps for each generated word, determined by a confidence network on
    top of the second-layer LSTM.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Boosting LSTM with Self-Attention
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some works adopted the self-attention operator in place of the additive attention
    one in LSTM-based language models [[79](#bib.bib79), [80](#bib.bib80), [83](#bib.bib83),
    [107](#bib.bib107)]. In particular, Huang *et al.* [[79](#bib.bib79)] augmented
    the LSTM with the Attention on Attention operator, which computes another step
    of attention on top of visual self-attention. Pan *et al.* [[80](#bib.bib80)]
    introduced the X-Linear attention block, which enhances self-attention with second-order
    interactions and improves both the visual encoding and the language model. On
    a different line, Zhu *et al.* [[107](#bib.bib107)] applied the neural architecture
    search paradigm to select the connections between layers and the operations within
    gates of RNN-based image captioning language models, using a decoder enriched
    with self-attention [[80](#bib.bib80)].
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究采用了自注意力操作来替代 LSTM 基于语言模型中的加法注意力操作 [[79](#bib.bib79), [80](#bib.bib80), [83](#bib.bib83),
    [107](#bib.bib107)]。特别地，黄*等人* [[79](#bib.bib79)] 将 Attention on Attention 操作符扩展到
    LSTM 上，在视觉自注意力的基础上计算另一层注意力。潘*等人* [[80](#bib.bib80)] 引入了 X-Linear 注意力块，该块通过二阶交互增强自注意力，从而改进了视觉编码和语言模型。另一方面，朱*等人* [[107](#bib.bib107)]
    应用了神经架构搜索范式来选择层之间的连接和 RNN 基于图像描述语言模型中门内的操作，使用了一个增强自注意力的解码器 [[80](#bib.bib80)]。
- en: 3.2 Convolutional Language Models
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 卷积语言模型
- en: A worth-to-mention approach is that proposed by Aneya *et al.* [[108](#bib.bib108)],
    which uses convolutions as a language model. In particular, a global image feature
    vector is combined with word embeddings and fed to a CNN, operating on all words
    in parallel during training and sequentially in inference. Convolutions are right-masked
    to prevent the model from using the information of future word tokens. Despite
    the clear advantage of parallel training, the usage of the convolutional operator
    in language models has not gained popularity due to the poor performance and the
    advent of Transformer architectures.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得提及的方法是 Aneya*等人* [[108](#bib.bib108)] 提出的，该方法使用卷积作为语言模型。具体来说，将全局图像特征向量与词嵌入结合，并输入
    CNN，在训练期间对所有词进行并行处理，在推断过程中进行顺序处理。卷积操作右侧掩码以防止模型使用未来词标记的信息。尽管并行训练有明显的优势，但由于性能较差和
    Transformer 架构的出现，卷积操作在语言模型中的使用并未获得广泛关注。
- en: 3.3 Transformer-based Architectures
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于 Transformer 的架构
- en: '![Refer to caption](img/b77e90263311a0e160fb52258d5b695c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b77e90263311a0e160fb52258d5b695c.png)'
- en: 'Figure 6: Schema of the Transformer-based language model. The caption generation
    is performed via masked self-attention over previously generated tokens and cross-attention
    with encoded visual features.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于 Transformer 的语言模型的示意图。图像生成通过对先前生成的标记进行掩码自注意力和与编码视觉特征进行交叉注意力来完成。
- en: 'The fully-attentive paradigm proposed by Vaswani *et al.* [[74](#bib.bib74)]
    has completely changed the perspective of language generation. Shortly after,
    the Transformer model became the building block of other breakthroughs in NLP,
    such as BERT [[102](#bib.bib102)] and GPT [[109](#bib.bib109)], and the standard
    de-facto architecture for many language understanding tasks. As image captioning
    can be cast as a sequence-to-sequence problem, the Transformer architecture has
    been employed also for this task. The standard Transformer decoder performs a
    masked self-attention operation, which is applied to words, followed by a cross-attention
    operation, where words act as queries and the outputs of the last encoder layer
    act as keys and values, plus a final feed-forward network (Fig. [6](#S3.F6 "Figure
    6 ‣ 3.3 Transformer-based Architectures ‣ 3 Language Models ‣ From Show to Tell:
    A Survey on Deep Learning-based Image Captioning")). During training, a masking
    mechanism is applied to the previous words to constrain a unidirectional generation
    process. The original Transformer decoder has been employed in some image captioning
    models without significant architectural modifications [[77](#bib.bib77), [78](#bib.bib78),
    [86](#bib.bib86), [94](#bib.bib94)]. Besides, some variants have been proposed
    to improve language generation and visual feature encoding.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Gating mechanisms. Li *et al.* [[76](#bib.bib76)] proposed a gating mechanism
    for the cross-attention operator, which controls the flow of visual and semantic
    information by combining and modulating image regions representations with semantic
    attributes coming from an external tagger. On the same line, Ji *et al.* [[85](#bib.bib85)]
    integrated a context gating mechanism to modulate the influence of the global
    image representation on each generated word, modeled via multi-head attention.
    Cornia *et al.* [[81](#bib.bib81)] proposed to take into account all encoding
    layers in place of performing cross-attention only on the last one. To this end,
    they devised the meshed decoder, which contains a mesh operator that modulates
    the contribution of all the encoding layers independently and a gate that weights
    these contributions guided by the text query. In [[94](#bib.bib94), [97](#bib.bib97)],
    the decoder architecture is again employed in conjunction with textual prefixes,
    also extracted from pre-trained visual-semantic models and employed as visual
    tags.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 BERT-like Architectures
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite the encoder-decoder paradigm being a common approach to image captioning,
    some works have revisited captioning architectures to exploit a BERT-like [[102](#bib.bib102)]
    structure in which the visual and textual modalities are fused together in the
    early stages (Fig. [7](#S3.F7 "Figure 7 ‣ 3.4 BERT-like Architectures ‣ 3 Language
    Models ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")).
    The main advantage of this architecture is that layers dealing with text can be
    initialized with pre-trained parameters learned from massive textual corpora.
    Therefore, the BERT paradigm has been widely adopted in works that exploit pre-training [[100](#bib.bib100),
    [101](#bib.bib101), [103](#bib.bib103)]. The first example is due to Zhou *et
    al.* [[101](#bib.bib101)], who developed a unified model that fuses visual and
    textual modalities into a BERT-like architecture for image captioning. The model
    consists of a shared multi-layer Transformer encoder network for both encoding
    and decoding, pre-trained on a large corpus of image-caption pairs and then fine-tuned
    for image captioning by right-masking the tokens sequence to simulate the unidirectional
    generation process. Further, Li *et al.* [[100](#bib.bib100)] introduced the usage
    of object tags detected in the image as anchors points for learning a better alignment
    in vision-and-language joint representations. To this end, their model represents
    an input image-text pair as a word tokens-object tags-region features triple,
    where the object tags are the textual classes proposed by the object detector.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1768dba41b1098ce4a1a7c07dcdc0c9.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Schema of a BERT-like language model. A single stream of attentive
    layers processes both image regions and word tokens and generates the output caption.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Non-autoregressive Language Models
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thanks to the parallelism offered by Transformers, non-autoregressive language
    models have been proposed in machine translation to reduce the inference time
    by generating all words in parallel. Some efforts have been made to apply this
    paradigm to image captioning [[110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112),
    [113](#bib.bib113)]. The first approaches towards a non-autoregressive generation
    were composed of a number of different generation stages, where all words were
    predicted in parallel and refined at each stage. Subsequent methods, instead,
    employ reinforcement learning techniques to improve the final results. Specifically,
    these approaches treat the generation process as a cooperative multi-agent reinforcement
    system, where the positions in of the words in the target sequence are viewed
    as agents that learn to cooperatively maximize a sentence-level reward [[111](#bib.bib111),
    [113](#bib.bib113)]. These works also leverage knowledge distillation on unlabeled
    data and a post-processing step to remove identical consecutive tokens.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Discussion
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recurrent models have been the standard for many years, and their application
    brought to the development of clever and successful ideas that can be integrated
    also into non-recurrent solutions. However, they are slow to train and struggle
    to maintain long-term dependencies: these drawbacks are alleviated by autoregressive
    and Transformer-based solutions that recently gained popularity. Inspired by the
    success of pre-training on large, unsupervised corpora for NLP tasks, massive
    pre-training has been applied also for image captioning by employing either encoder-decoder
    or BERT-like architectures, often in conjunction with textual tags. This strategy
    led to impressive performance, suggesting that visual and textual semantic relations
    can be inferred and learned also from not well-curated data [[100](#bib.bib100),
    [94](#bib.bib94), [104](#bib.bib104)]. BERT-like architectures are suitable for
    such a massive pre-training but are not generative architectures by design. Massive
    pre-training on generative-oriented architectures [[94](#bib.bib94), [97](#bib.bib97)]
    is currently a worth-exploring direction, which leads to performances that are
    at least on-pair with the early-fusion counterparts.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 4 Training Strategies
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An image captioning model is commonly expected to generate a caption word by
    word by taking into account the previous words and the image. At each step, the
    output word is sampled from a learned distribution over the vocabulary words.
    In the most simple scenario, *i.e.* the greedy decoding mechanism, the word with
    the highest probability is output. The main drawback of this setting is that possible
    prediction errors quickly accumulate along the way. To alleviate this drawback,
    one effective strategy is to use the beam search algorithm [[114](#bib.bib114)]
    that, instead of outputting the word with maximum probability at each time step,
    maintains $k$ sequence candidates (those with the highest probability at each
    step) and finally outputs the most probable one.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, the captioning model must learn to properly predict the probabilities
    of the words to appear in the caption. To this end, the most common training strategies
    are based on 1. *cross-entropy loss*; 2. *masked language model*; 3. *reinforcement
    learning* that allows directly optimizing for captioning-specific non-differentiable
    metrics; 4. *vision-and-language pre-training* objectives (see Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Cross-Entropy Loss
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The cross-entropy loss is the first proposed and most used objective for image
    captioning models. With this loss, the goal of the training, at each timestep,
    is to minimize the negative log-likelihood of the current word given the previous
    ground-truth words. Given a sequence of target words $y_{1:T}$, the loss is formally
    defined as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{XE}(\theta)=-\sum_{i=1}^{n}\log\left(P\left(y_{i}\mid y_{1:i-1},\bm{X}\right)\right),$
    |  | (4) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: where $P$ is the probability distribution induced by the language model, $y_{i}$
    the ground-truth word at time $i$, $y_{1:i-1}$ indicate the previous ground-truth
    words, and $\bm{X}$ the visual encoding. The cross-entropy loss is designed to
    operate at word level and optimize the probability of each word in the ground-truth
    sequence without considering longer range dependencies between generated words.
    The traditional training setting with cross-entropy also suffers from the exposure
    bias problem [[115](#bib.bib115)] caused by the discrepancy between the training
    data distribution as opposed to the distribution of its own predicted words.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Masked Language Model (MLM)
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first masked language model has been proposed for training the BERT [[102](#bib.bib102)]
    architecture. The main idea behind this optimization function consists in randomly
    masking out a small subset of the input tokens sequence and training the model
    to predict masked tokens while relying on the rest of the sequence, *i.e.* both
    previous and subsequent tokens. As a consequence, the model learns to employ contextual
    information to infer missing tokens, which allows building a robust sentence representation
    where the context plays an essential role. Since this strategy considers only
    the prediction of the masked tokens and ignores the prediction of the non-masked
    ones, training with it is much slower than training for complete left-to-right
    or right-to-left generation. Notably, some works have employed this strategy as
    a pre-training objective, sometimes completely avoiding the combination with the
    cross-entropy [[100](#bib.bib100), [103](#bib.bib103)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Reinforcement Learning
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the limitations of word-level training strategies observed when using
    limited amounts of data, a significant improvement was achieved by applying the
    reinforcement learning paradigm for training image captioning models. Within this
    framework, the image captioning model is considered as an agent whose parameters
    determine a policy. At each time step, the agent executes the policy to choose
    an action, *i.e.* the prediction of the next word in the generated sentence. Once
    the end-of-sequence is reached, the agent receives a reward, and the aim of the
    training is to optimize the agent parameters to maximize the expected reward.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Many works harnessed this paradigm and explored different sequence-level metrics
    as rewards. The first proposal is due to Ranzato *et al.* [[115](#bib.bib115)],
    which introduced the usage of the REINFORCE algorithm [[116](#bib.bib116)] adopting
    BLEU [[117](#bib.bib117)] and ROUGE [[118](#bib.bib118)] as reward signals. Ren *et
    al.* [[119](#bib.bib119)] experimented using visual-semantic embeddings obtained
    from a network that encodes the image and the so far generated caption in order
    to compute a similarity score to be used as reward. Liu *et al.* [[120](#bib.bib120)]
    proposed to use as reward a linear combination of SPICE [[121](#bib.bib121)] and
    CIDEr [[122](#bib.bib122)], called SPIDEr. Finally, the most widely adopted strategy [[123](#bib.bib123),
    [124](#bib.bib124), [81](#bib.bib81)], introduced by Rennie *et al.* [[38](#bib.bib38)],
    entails using the CIDEr score, as it correlates better with human judgment [[122](#bib.bib122)].
    The reward is normalized with respect to a baseline value to reduce variance.
    Formally, to compute the loss gradient, beam search and greedy decoding are leveraged
    as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\nabla_{\theta}L(\theta)=-\frac{1}{k}\sum_{i=1}^{k}\left((r(\bm{w}^{i})-b)\nabla_{\theta}\log
    P(\bm{w}^{i})\right),$ |  | (5) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: where $\bm{w}^{i}$ is the $i$-th sentence in the beam or a sampled collection,
    $r(\cdot)$ is the reward function, *i.e.* the CIDEr computation, and $b$ is the
    baseline, computed as the reward of the sentence obtained via greedy decoding [[38](#bib.bib38)],
    or as the average reward of the beam candidates [[81](#bib.bib81)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Note that, since it would be difficult for a random policy to improve in an
    acceptable amount of time, the usual procedure entails pre-training with cross-entropy
    or masked language model first, and then fine-tuning stage with reinforcement
    learning by employing a sequence level metric as reward. This ensures the initial
    reinforcement learning policy to be more suitable than the random one.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/518c765dbacb58a6066363ff8fc7035a.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: (a)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9368d4f168abe0744a0fa1c43dcf4d2b.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: (b)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Qualitative examples from some of the most common image captioning
    datasets: (a) image-caption pairs; (b) word clouds of the captions most common
    visual words.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Large-scale Pre-Training
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of vision-and-language pre-training in early-fusion architectures,
    one of the most common pre-training objectives is the masked contextual token
    loss, where tokens of each modality (visual and textual) are randomly masked following
    the BERT strategy [[102](#bib.bib102)], and the model has to predict the masked
    input based on the context of both modalities, thus connecting their joint representation.
    Another largely adopted strategy entails using a contrastive loss, where the inputs
    are organized as image regions-captions words-object tags triples, and the model
    is asked to discriminate correct triples from polluted ones, in which tags are
    randomly replaced [[100](#bib.bib100), [103](#bib.bib103)]. Other objectives take
    into account the text-image alignment at a word-region level and entail predicting
    the original word sequence given a corrupted one [[125](#bib.bib125)].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, cross-entropy has also been used when pre-training on noisy
    captions [[97](#bib.bib97), [94](#bib.bib94)], sometimes also employing prefixes.
    PrefixLM [[94](#bib.bib94)] has indeed proved to be a valuable strategy that enables
    bidirectional attention within the prefix sequence, and thus, it is applicable
    for both decoder-only and encoder-decoder sequence-to-sequence language models.
    Noticeably, some large-scale models pre-trained on noisy data under this setting
    can achieve state-of-the-art performance without requiring a fine-tuning stage
    with Reinforcement [[94](#bib.bib94)].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we notice that image captioning can be used as a pre-training task
    to efficiently learn visual representations, which can benefit downstream tasks
    such as image classification, object detection, and instance segmentation [[126](#bib.bib126)].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evaluation Protocol
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As for any data-driven task, the development of image captioning has been enabled
    by the collection of large datasets and the definition of quantitative scores
    to evaluate the performance and monitor the advancement of the field.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Datasets
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Image captioning datasets contain images and one or multiple captions associated
    with them. Having multiple ground-truth captions for each image helps to capture
    the variability of human descriptions. Other than the number of available captions,
    also their characteristics (*e.g.* average caption length and vocabulary size)
    highly influence the design and the performance of image captioning algorithms.
    Note that the distribution of the terms in the datasets captions is usually long-tailed,
    thus, when using word-level dictionaries, the common practice is to include in
    the vocabulary only those terms whose frequency is above a pre-defined threshold.
    Recently, however, using subword-based tokenization approaches like BPE [[127](#bib.bib127)]
    is a popular choice that allows avoiding dataset pre-processing. The available
    datasets differ both on the images contained (for their domain and visual quality)
    and on the captions associated with the images (for their length, number, relevance,
    and style). A summary of the most used public datasets is reported in Table [I](#S5.T1
    "TABLE I ‣ 5.1.1 Standard captioning datasets ‣ 5.1 Datasets ‣ 5 Evaluation Protocol
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning"), and some
    sample image-caption pairs are reported in Fig. [8](#S4.F8 "Figure 8 ‣ 4.3 Reinforcement
    Learning ‣ 4 Training Strategies ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), along with some word clouds obtained from the 50 most used
    visual words in the captions.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Standard captioning datasets
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Standard benchmark datasets are used by the community to compare their approaches
    on a common test-bed, a procedure that guides the development of image captioning
    strategies by allowing to identify suitable directions. Datasets used as benchmarks
    should be representative of the task at hand, both in terms of the challenges
    and ideal expected results (*i.e.* achievable human performance). Further, they
    should contain a large number of generic-domain images, each associated with multiple
    captions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Overview of the main image captioning datasets.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Domain | Nb. Images | Nb. Caps | Vocab Size | Nb. Words |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '|  |  | (per Image) | (per Cap.) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| COCO [[128](#bib.bib128)] |  | Generic | $132$K | $5$ | $27$K ($10$K) | $10.5$
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| Flickr30K [[129](#bib.bib129)] |  | Generic | $31$K | $5$ | $18$K ($7$K)
    | $12.4$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| Flickr8K [[19](#bib.bib19)] |  | Generic | $8$K | $5$ | $8$K ($3$K) | $10.9$
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| CC3M [[130](#bib.bib130)] |  | Generic | $3.3$M | $1$ | $48$K ($25$K) | $10.3$
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| CC12M [[131](#bib.bib131)] |  | Generic | $12.4$M | $1$ | $523$K ($163$K)
    | $20.0$ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| SBU Captions [[4](#bib.bib4)] |  | Generic | $1$M | $1$ | $238$K ($46$K)
    | $12.1$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| VizWiz [[132](#bib.bib132)] |  | Assistive | $70$K | $5$ | $20$K ($8$K) |
    $13.0$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| CUB-200 [[133](#bib.bib133)] |  | Birds | $12$K | $10$ | $6$K ($2$K) | $15.2$
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| Oxford-102 [[133](#bib.bib133)] |  | Flowers | $8$K | $10$ | $5$K ($2$K)
    | $14.1$ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| Fashion Cap. [[134](#bib.bib134)] |  | Fashion | $130$K | $1$ | $17$K ($16$K)
    | $21.0$ |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| BreakingNews [[135](#bib.bib135)] |  | News | $115$K | $1$ | $85$K ($10$K)
    | $28.1$ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| GoodNews [[136](#bib.bib136)] |  | News | $466$K | $1$ | $192$K ($54$K) |
    $18.2$ |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| TextCaps [[137](#bib.bib137)] |  | OCR | $28$K | $5/6$ | $44$K ($13$K) |
    $12.4$ |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| Loc. Narratives [[138](#bib.bib138)] |  | Generic | $849$K | $1/5$ | $16$K
    ($7$K) | $41.8$ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: Early image captioning architectures [[27](#bib.bib27), [28](#bib.bib28), [25](#bib.bib25)]
    were commonly trained and tested on the Flickr30K [[129](#bib.bib129)] and Flickr8K [[19](#bib.bib19)]
    datasets, consisting of pictures collected from the Flickr website, containing
    everyday activities, events, and scenes, paired with five captions each. Currently,
    the most commonly used dataset is Microsoft COCO [[128](#bib.bib128)], which consists
    of images of complex scenes with people, animals, and common everyday objects
    in their context. It contains more than 120,000 images, each annotated with five
    captions, divided into 82,783 images for training and 40,504 for validation. For
    ease of evaluation, most of the literature follows the splits defined by Karpathy *et
    al.* [[25](#bib.bib25)], where 5,000 images of the original validation set are
    used for validation, 5,000 for test, and the rest for training. The dataset has
    also an official test set, composed of 40,775 images paired with 40 private captions
    each, and a public evaluation server²²2[https://competitions.codalab.org/competitions/3221](https://competitions.codalab.org/competitions/3221).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Pre-training datasets
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although training on large well-curated datasets is a sound approach, some works [[99](#bib.bib99),
    [100](#bib.bib100), [94](#bib.bib94), [104](#bib.bib104)] have demonstrated the
    benefits of pre-training on even bigger vision-and-language datasets, which can
    be either image captioning datasets of lower-quality captions or datasets collected
    for other tasks (*e.g.* visual question answering [[100](#bib.bib100), [101](#bib.bib101)],
    text-to-image generation [[139](#bib.bib139)], image-caption association [[93](#bib.bib93)]).
    Among the datasets used for pre-training, that have been specifically collected
    for image captioning, it is worth mentioning SBU Captions [[4](#bib.bib4)], originally
    used for tackling image captioning as a retrieval task [[19](#bib.bib19)], which
    contains around 1 million image-text pairs, collected from the Flickr website.
    Similarly, YFCC100M [[140](#bib.bib140)] is composed of 100 million media objects
    in which 14.8 million images are available with automatically-collected textual
    descriptions. Later, the Conceptual Captions [[130](#bib.bib130), [131](#bib.bib131)]
    datasets have been proposed, which are collections of around 3.3 million (CC3M)
    and 12 million (CC12M) images paired with one weakly-associated description automatically
    collected from the web with a relaxed filtering procedure. Differently from previous
    datasets, Wikipedia-based Image Text (WIT) [[141](#bib.bib141)] provides images
    coming from Wikipedia together with various metadata extracted from the original
    pages, with approximately 5.3 million images available with the corresponding
    descriptions in English. Although the large scale and variety in caption style
    make all these datasets particularly interesting for pre-training, the contained
    captions can be noisy, and the availability of images is not always guaranteed
    since most of them are provided as URLs.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training on such datasets requires significant computational resources and
    effort to collect the data needed. Nevertheless, this strategy represents an asset
    to obtain state-of-the-art performances. Accordingly, some pre-training datasets
    are currently not publicly available, such as ALIGN [[142](#bib.bib142), [94](#bib.bib94)]
    and ALT-200 [[104](#bib.bib104)], respectively containing 1.8 billion and 200
    million noisy image-text pairs, or the datasets used to train DALL-E [[139](#bib.bib139)]
    and CLIP [[93](#bib.bib93)] consisting of 250 and 400 million pairs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Domain-specific datasets
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While domain-generic benchmark datasets are important to capture the main aspects
    of the image captioning task, domain-specific datasets are also important to highlight
    and target specific challenges. These may relate to the visual domain (*e.g.* type
    and style of the images) and the semantic domain. In particular, the distribution
    of the terms used to describe domain-specific images can be significantly different
    from that of the terms used for domain-generic images.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: An example of dataset-specific in terms of the visual domain is the VizWiz Captions [[132](#bib.bib132)]
    dataset, collected to favor the image captioning research towards assistive technologies.
    The images in this dataset have been taken by visually-impaired people with their
    phones, thus, they can be of low quality and concern a wide variety of everyday
    activities, most of which entail reading some text.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Some examples of specific semantic domain are the CUB-200 [[143](#bib.bib143)]
    and the Oxford-102 [[144](#bib.bib144)] datasets, which contain images of birds
    and flowers, respectively, that have been paired with ten captions each by Reed *et
    al.* [[133](#bib.bib133)]. Given the specificity of these datasets, rather than
    for standard image captioning, they are usually adopted for different related
    tasks such as cross-domain captioning [[145](#bib.bib145)], visual explanation
    generation [[146](#bib.bib146), [147](#bib.bib147)], and text-to-image synthesis [[148](#bib.bib148)].
    Another domain-specific dataset is Fashion Captioning [[134](#bib.bib134)] that
    contains images of clothing items in different poses and colors that may share
    the same caption. The vocabulary for describing these images is somewhat smaller
    and more specific than for generic datasets. Differently, datasets as BreakingNews [[135](#bib.bib135)]
    and GoodNews [[136](#bib.bib136)] enforce using a richer vocabulary since their
    images, taken from news articles, have long associated captions written by expert
    journalists. The same applies to the TextCaps [[137](#bib.bib137)] dataset, which
    contains images with text, that must be “read” and included in the caption, and
    to Localized Narratives [[138](#bib.bib138)], whose captions have been collected
    by recording people freely narrating what they see in the images.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Collecting domain-specific datasets and developing solutions to tackle the challenges
    they pose is crucial to extend the applicability of image captioning algorithms.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Evaluation Metrics
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Evaluating the quality of a generated caption is a tricky and subjective task [[122](#bib.bib122),
    [121](#bib.bib121)], complicated by the fact that captions cannot only be grammatical
    and fluent but need to properly refer to the input image. Arguably, the best way
    to measure the quality of the caption for an image is still carefully designing
    a human evaluation campaign in which multiple users score the produced sentences [[149](#bib.bib149)].
    However, human evaluation is costly and not reproducible – which prevents a fair
    comparison between different approaches. Automatic scoring methods exist that
    are used to assess the quality of system-produced captions, usually by comparing
    them with human-produced reference sentences, although some metrics do not rely
    on reference captions. Table [III](#A1.T3 "TABLE III ‣ Appendix A Further analysis
    of the evaluation metrics ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning").'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Standard evaluation metrics
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first strategy adopted to evaluate image captioning performance consists
    of exploiting metrics designed for NLP tasks. For example, the BLEU score [[117](#bib.bib117)]
    and the METEOR [[150](#bib.bib150)] score were introduced for machine translation.
    The former is based on *n*-gram precision considering *n*-grams up to length four;
    the latter favors the recall of matching unigrams from the candidate and reference
    sentences in their exact form stemmed form and meaning. Moreover, the ROUGE score [[118](#bib.bib118)]
    was designed for summarization and applied also for image captioning in its variant
    considering the longest subsequence of tokens in the same relative order, possibly
    with other tokens in-between, that appears in both candidate and reference caption.
    Later, specific image captioning metrics have been proposed [[122](#bib.bib122),
    [121](#bib.bib121)]. The reference CIDEr score [[122](#bib.bib122)] is based on
    the cosine similarity between the Term Frequency-Inverse Document Frequency weighted
    *n*-grams in the candidate caption and in the set of reference captions associated
    with the image, thus taking into account both precision and recall. The SPICE
    score [[121](#bib.bib121)] considers matching tuples extracted from the candidate
    and the reference (or possibly directly the image) scene graphs, thus favoring
    the semantic content rather than the fluency.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Performance analysis of representative image captioning approaches
    in terms of different evaluation metrics. The $\dagger$ marker indicates models
    trained by us with ResNet-152 features, while the $\ddagger$ marker indicates
    unofficial implementations. For all the metrics, the higher the value, the better
    ($\uparrow$).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  | Standard Metrics |  |  | Diversity Metrics |  |  | Embedding-based
    Metrics |  |  | Learning-based Metrics |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '|  |  | #Params (M) |  | B-1 | B-4 | M | R | C | S |  |  | Div-1 | Div-2 |
    Vocab | %Novel |  |  | WMD | Alignment | Coverage |  |  | TIGEr | BERT-S | CLIP-S
    | CLIP-S${}^{\text{Ref}}$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Show and Tell^† [[23](#bib.bib23)] |  | 13.6 |  | 72.4 | 31.4 | 25.0 | 53.1
    | 97.2 | 18.1 |  |  | 0.014 | 0.045 | 635 | 36.1 |  |  | 16.5 | 0.199 | 71.7 |  |  |
    71.8 | 93.4 | 0.697 | 0.762 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| SCST (FC)^‡ [[38](#bib.bib38)] |  | 13.4 |  | 74.7 | 31.7 | 25.2 | 54.0 |
    104.5 | 18.4 |  |  | 0.008 | 0.023 | 376 | 60.7 |  |  | 16.8 | 0.218 | 74.7 |  |  |
    71.9 | 89.0 | 0.691 | 0.758 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| Show, Attend and Tell^† [[42](#bib.bib42)] |  | 18.1 |  | 74.1 | 33.4 | 26.2
    | 54.6 | 104.6 | 19.3 |  |  | 0.017 | 0.060 | 771 | 47.0 |  |  | 17.6 | 0.209
    | 72.1 |  |  | 73.2 | 93.6 | 0.710 | 0.773 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| SCST (Att2in)^‡ [[38](#bib.bib38)] |  | 14.5 |  | 78.0 | 35.3 | 27.1 | 56.7
    | 117.4 | 20.5 |  |  | 0.010 | 0.031 | 445 | 64.9 |  |  | 18.5 | 0.238 | 76.0
    |  |  | 73.9 | 88.9 | 0.712 | 0.779 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| Up-Down^‡ [[58](#bib.bib58)] |  | 52.1 |  | 79.4 | 36.7 | 27.9 | 57.6 | 122.7
    | 21.5 |  |  | 0.012 | 0.044 | 577 | 67.6 |  |  | 19.1 | 0.248 | 76.7 |  |  |
    74.6 | 88.8 | 0.723 | 0.787 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| SGAE [[71](#bib.bib71)] |  | 125.7 |  | 81.0 | 39.0 | 28.4 | 58.9 | 129.1
    | 22.2 |  |  | 0.014 | 0.054 | 647 | 71.4 |  |  | 20.0 | 0.255 | 76.9 |  |  |
    74.6 | 94.1 | 0.734 | 0.796 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| MT [[72](#bib.bib72)] |  | 63.2 |  | 80.8 | 38.9 | 28.8 | 58.7 | 129.6 |
    22.3 |  |  | 0.011 | 0.048 | 530 | 70.4 |  |  | 20.2 | 0.253 | 77.0 |  |  | 74.8
    | 88.8 | 0.726 | 0.791 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| AoANet [[79](#bib.bib79)] |  | 87.4 |  | 80.2 | 38.9 | 29.2 | 58.8 | 129.8
    | 22.4 |  |  | 0.016 | 0.062 | 740 | 69.3 |  |  | 20.0 | 0.254 | 77.3 |  |  |
    75.1 | 94.3 | 0.737 | 0.797 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| X-LAN [[80](#bib.bib80)] |  | 75.2 |  | 80.8 | 39.5 | 29.5 | 59.2 | 132.0
    | 23.4 |  |  | 0.018 | 0.078 | 858 | 73.9 |  |  | 20.6 | 0.261 | 77.9 |  |  |
    75.4 | 94.3 | 0.746 | 0.803 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| DPA [[83](#bib.bib83)] |  | 111.8 |  | 80.3 | 40.5 | 29.6 | 59.2 | 133.4
    | 23.3 |  |  | 0.019 | 0.079 | 937 | 65.9 |  |  | 20.5 | 0.261 | 77.3 |  |  |
    75.0 | 94.3 | 0.738 | 0.802 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| AutoCaption [[107](#bib.bib107)] |  | - |  | 81.5 | 40.2 | 29.9 | 59.5 |
    135.8 | 23.8 |  |  | 0.022 | 0.096 | 1064 | 75.8 |  |  | 20.9 | 0.262 | 77.7 |  |  |
    75.4 | 94.3 | 0.752 | 0.808 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| ORT [[77](#bib.bib77)] |  | 54.9 |  | 80.5 | 38.6 | 28.7 | 58.4 | 128.3 |
    22.6 |  |  | 0.021 | 0.072 | 1002 | 73.8 |  |  | 19.8 | 0.255 | 76.9 |  |  | 75.1
    | 94.1 | 0.736 | 0.796 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| CPTR [[92](#bib.bib92)] |  | 138.5 |  | 81.7 | 40.0 | 29.1 | 59.4 | 129.4
    | - |  |  | 0.014 | 0.068 | 667 | 75.6 |  |  | 20.2 | 0.261 | 77.0 |  |  | 74.8
    | 94.3 | 0.745 | 0.802 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  | 38.4 |  | 80.8 | 39.1
    | 29.2 | 58.6 | 131.2 | 22.6 |  |  | 0.017 | 0.079 | 847 | 78.9 |  |  | 20.3 |
    0.256 | 76.0 |  |  | 75.3 | 93.7 | 0.734 | 0.792 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| X-Transformer [[80](#bib.bib80)] |  | 137.5 |  | 80.9 | 39.7 | 29.5 | 59.1
    | 132.8 | 23.4 |  |  | 0.018 | 0.081 | 878 | 74.3 |  |  | 20.6 | 0.257 | 77.7
    |  |  | 75.5 | 94.3 | 0.747 | 0.803 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| Unified VLP [[101](#bib.bib101)] |  | 138.2 |  | 80.9 | 39.5 | 29.3 | 59.6
    | 129.3 | 23.2 |  |  | 0.019 | 0.081 | 898 | 74.1 |  |  | 26.6 | 0.258 | 77.1
    |  |  | 75.1 | 94.4 | 0.750 | 0.807 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| VinVL [[103](#bib.bib103)] |  | 369.6 |  | 82.0 | 41.0 | 31.1 | 60.9 | 140.9
    | 25.2 |  |  | 0.023 | 0.099 | 1125 | 77.9 |  |  | 20.5 | 0.265 | 79.6 |  |  |
    75.7 | 88.5 | 0.766 | 0.820 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: As expected, metrics designed for image captioning usually correlate better
    with human judgment than those borrowed from other NLP tasks (with the exception
    of METEOR [[150](#bib.bib150)]), both at corpus-level and caption-level [[121](#bib.bib121),
    [151](#bib.bib151), [152](#bib.bib152)]. Correlation with human judgment is measured
    via statistical correlation coefficients (such as Pearson’s, Kendall’s, and Spearman’s
    correlation coefficients) and via the agreement with humans’ preferred caption
    in a pair of candidates, all evaluated on sample captioned images.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Diversity metrics
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To better assess the performance of a captioning system, it is common practice
    to consider a set of the above-mentioned standard metrics. Nevertheless, these
    are somehow gameable because they favor word similarity rather than meaning correctness [[153](#bib.bib153)].
    Another drawback of the standard metrics is that they do not capture (but rather
    disfavor) the desirable capability of the system to produce novel and diverse
    captions, which is more in line with the variability with which humans describe
    complex images. This consideration brought to the development of diversity metrics [[154](#bib.bib154),
    [155](#bib.bib155), [156](#bib.bib156), [157](#bib.bib157)]. Most of these metrics
    can potentially be calculated even when no ground-truth captions are available
    at test time. However, since they overlook the syntactic correctness of the captions
    and their relatedness with the image, it is advisable to combine them with other
    metrics.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The overall performance of a captioning system can be evaluated in terms of
    corpus-level diversity or, when the system can output multiple captions for the
    same image, single image-level diversity (termed as *global diversity* and *local
    diversity*, respectively, in [[155](#bib.bib155)]). To quantify the former, it
    can be considered the number of unique words used in all the generated captions
    (Vocab) and the percentage of generated captions that were not present in the
    training set (%Novel). For the latter, it can be used the ratio of unique captions
    unigrams or bigrams to the total number of captions unigrams (Div-1 and Div-2).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Embedding-based metrics
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An alternative approach to captioning evaluation consists in relying on captions
    semantic similarity or other specific aspects of caption quality, which are estimated
    via embedding-based metrics [[158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160)].
    For example, the WMD score [[161](#bib.bib161)], originally introduced to evaluate
    document semantic dissimilarity, can also be applied to captioning evaluation
    by considering generated captions and ground-truth captions as the compared documents [[162](#bib.bib162)].
    Moreover, the Alignment score [[163](#bib.bib163)] is based on the alignment between
    the sequences of nouns in the candidate and reference sentence and captures whether
    concepts are mentioned in a human-like order. Finally, the Coverage score [[84](#bib.bib84),
    [164](#bib.bib164)] expresses the completeness of a caption, which is evaluated
    by considering the mentioned scene visual entities. Since this score considers
    visual objects directly, it can be applied even when no ground-truth caption is
    available.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Learning-based evaluation
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a further development towards captions quality assessment, learning-based
    evaluation strategies [[151](#bib.bib151), [152](#bib.bib152), [165](#bib.bib165),
    [166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)] are being investigated.
    To this end, it can be exploited a component of a complete captioning approach,
    in charge to evaluate the produced caption completeness [[169](#bib.bib169)] or
    how human-like it is [[170](#bib.bib170)]. Alternatively, learning-based evaluation
    is usually based on a pre-trained model. For example, the BERT-S score [[171](#bib.bib171)],
    which is used to evaluate various language generation tasks [[172](#bib.bib172)],
    exploits pre-trained BERT embeddings [[102](#bib.bib102)] to represent and match
    the tokens in the reference and candidate sentences via cosine similarity. Moreover,
    the TIGEr score [[173](#bib.bib173)] represents the reference and candidate captions
    as grounding score vectors obtained from a pre-trained model [[174](#bib.bib174)]
    that grounds their words on the image regions and scores the candidate caption
    based on the similarity of the grounding vectors. Further, the CLIP-S score [[175](#bib.bib175)]
    is a direct application of the CLIP [[93](#bib.bib93)] model to image captioning
    evaluation and consists of an adjusted cosine similarity between image and candidate
    caption representation. Thus, CLIP-S is designed to work without reference captions,
    although the CLIP-S${}^{\text{Ref}}$ variant can exploit also the reference captions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'We refer the reader to Appendix [A](#A1 "Appendix A Further analysis of the
    evaluation metrics ‣ From Show to Tell: A Survey on Deep Learning-based Image
    Captioning") for a deeper discussion on diversity, embedding-based, and learning-based
    metrics.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb6002dc1d6f9933e588a934bd352b2e.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Relationship between CIDEr, number of parameters and other scores.
    Values of Div-1 and CLIP-S are multiplied by powers of 10 for readability.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 6 Experimental Evaluation
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Table [II](#S5.T2 "TABLE II ‣ 5.2.1 Standard evaluation metrics ‣ 5.2 Evaluation
    Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), we analyze the performance of some of the main approaches
    in terms of all the evaluation scores presented in Section [5.2](#S5.SS2 "5.2
    Evaluation Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep
    Learning-based Image Captioning") to take into account the different aspects of
    caption quality these express and report their number of parameters to give an
    idea of the computational complexity and memory occupancy of the models. The data
    in the table have been obtained either from the model weights and captions files
    provided by the original authors or from our best implementation. Given its large
    use as a benchmark in the field, we consider the domain-generic COCO dataset also
    for this analysis. In the table, methods are clustered based on the information
    included in the visual encoding and ordered by CIDEr score. It can be observed
    that standard and embedding-based metrics all had a substantial improvement with
    the introduction of region-based visual encodings. Further improvement was due
    to the integration of information on inter-objects relations, either expressed
    via graphs or self-attention. Notably, CIDEr, SPICE, and Coverage most reflect
    the benefit of vision-and-language pre-training. Moreover, as expected, it emerges
    that the diversity-based scores are correlated, especially Div-1 and Div-2 and
    the Vocab Size. The correlation of this family of scores and the others is almost
    linear, except for early approaches, which perform averagely well in terms of
    Diversity despite lower values for standard metrics. From the trend of learning-based
    scores, it emerges that exploiting models trained on textual data only (BERT-S,
    reported in the table as its F1-score variant) does not help discriminating among
    image captioning approaches. On the other hand, considering as reference only
    the visual information and disregarding the ground-truth captions is possible
    with the appropriate vision-and-language pre-trained model (consider that CLIP-S
    and CLIP-S${}^{\text{Ref}}$ are linearly correlated). This is a desirable property
    for an image captioning evaluation score since it allows estimating the performance
    of a model without relying on reference captions that can be limited in number
    and somehow subjective.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'For readability, in Fig. [9](#S5.F9 "Figure 9 ‣ 5.2.4 Learning-based evaluation
    ‣ 5.2 Evaluation Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning") we highlight the relation between the
    CIDEr score and other characteristics from Table [II](#S5.T2 "TABLE II ‣ 5.2.1
    Standard evaluation metrics ‣ 5.2 Evaluation Metrics ‣ 5 Evaluation Protocol ‣
    From Show to Tell: A Survey on Deep Learning-based Image Captioning"). We chose
    CIDEr as this score is commonly regarded as one of the most relevant indicators
    of image captioning systems performance. The first plot, depicting the relation
    between model complexity and performance, shows that more complex models do not
    necessarily bring to better performance. The other plots describe an almost-linear
    relation between CIDEr and the other scores, with some flattening for high CIDEr
    values. These trends confirm the suitability of the CIDEr score as an indicator
    of the overall performance of an image captioning algorithm, whose specific characteristics
    in terms of the produced captions would still be expressed more precisely in terms
    of non-standard metrics.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'We refer the reader to Appendix [B](#A2 "Appendix B Further Performance Analysis
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning") for additional
    performance analyses and qualitative results.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 7 Image Captioning Variants
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beyond general-purpose image captioning, several specific sub-tasks have been
    explored in the literature. These can be classified into four categories according
    to their scope: 1\. *dealing with the lack of training data*; 2\. *focusing on
    the visual input*; 3\. *focusing on the textual output*; 4\. *addressing user
    requirements*.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Dealing with the lack of training data
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Paired image-caption datasets are very expensive to obtain. Thus, some image
    captioning variants are being explored that limit the need for full supervision
    information.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Novel Object Captioning. Novel object captioning focuses on describing objects
    not appearing in the training set, thus enabling a zero-shot learning setting
    that can increase the applicability of the models in the real world. Early approaches
    to this task [[176](#bib.bib176), [177](#bib.bib177)] tried to transfer knowledge
    from out-domain images by conditioning the model on external unpaired visual and
    textual data at training time. To explore this strategy, Hendricks *et al.* [[176](#bib.bib176)]
    introduced a variant of the COCO dataset [[128](#bib.bib128)], called *held-out
    COCO*, in which image-caption pairs containing one of eight pre-selected object
    classes were removed from the training set but not from the test set. To further
    encourage research on this task, the more challenging *nocaps* dataset, with nearly
    400 novel objects, has been introduced [[178](#bib.bib178)]. Some approaches to
    this variant [[179](#bib.bib179), [180](#bib.bib180)] integrate copying mechanisms
    in the language model to select novel objects predicted from a tagger or generate
    a caption template with placeholders to be filled with novel objects [[181](#bib.bib181),
    [106](#bib.bib106)]. On a different line, Anderson *et al.* [[182](#bib.bib182)]
    devised the Constrained Beam Search algorithm to force the inclusion of selected
    tag words in the output caption, following the predictions of a tagger. Moreover,
    following the pre-training trend with BERT-like architectures, Hu *et al.* [[183](#bib.bib183)]
    proposed a multi-layer Transformer model pre-trained by randomly masking one or
    more tags from image-tag pairs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Unpaired Image Captioning. Unpaired Image Captioning approaches can be either
    unsupervised or semi-supervised. Unsupervised captioning aims at understanding
    and describing images without paired image-text training data. Following unpaired
    machine translation approaches, the early work [[184](#bib.bib184)] proposes to
    generate captions in a pivot language and then translate predicted captions to
    the target language. After this work, the most common approach focuses on adversarial
    learning by training an LSTM-based discriminator to distinguish whether a caption
    is real or generated [[185](#bib.bib185), [186](#bib.bib186)]. As alternative
    approaches, it is worth mentioning [[187](#bib.bib187)] that generates a caption
    from the image scene-graph and [[188](#bib.bib188)] that leverages a memory-based
    network. Moreover, semi-supervised approaches have been proposed, such as [[189](#bib.bib189)],
    which uses both paired and unpaired data with adversarial learning, and [[190](#bib.bib190)],
    which performs iterative self-learning.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Continual Captioning. Continual captioning aims to deal with partially unavailable
    data by following the continual learning paradigm to incrementally learn new tasks
    without forgetting what has been learned before. In this respect, new tasks can
    be represented as sequences of captioning tasks with different vocabularies, as
    proposed in [[191](#bib.bib191)], and the model should be able to transfer visual
    concepts from one to the other while enlarging its vocabulary.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Focusing on the visual input
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some sub-tasks focus on making the textual description more correlated with
    visual data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Dense Captioning. Dense captioning was proposed by Johnson *et al.* [[192](#bib.bib192)]
    and consists of concurrently localizing and describing salient image regions with
    short natural language sentences. In this respect, the task can be conceived as
    a generalization of object detection, where caption replaces object tags, or image
    captioning, where single regions replace the full image. To address this task,
    contextual and global features [[193](#bib.bib193), [194](#bib.bib194)] and attribute
    generators [[195](#bib.bib195), [196](#bib.bib196)] can be exploited. Related
    to this variant, an important line of works [[197](#bib.bib197), [198](#bib.bib198),
    [199](#bib.bib199), [200](#bib.bib200), [66](#bib.bib66), [201](#bib.bib201)]
    focuses on the generation of textual paragraphs that densely describe the visual
    content as a coherent story.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Text-based Image Captioning. Text-based image captioning, also known as OCR-based
    image captioning or image captioning with reading comprehension, aims at reading
    and including the text appearing in images in the generated descriptions. The
    task was introduced by Sidorov *et al.* [[137](#bib.bib137)] with the TextCaps
    dataset. Another dataset designed for pre-training for this variant is *OCR-CC* [[202](#bib.bib202)],
    which is a subset of images containing meaningful text taken from the CC3M dataset [[130](#bib.bib130)]
    and automatically annotated through a commercial OCR system. The common approach
    to this variant entails combining image regions and text tokens, *i.e.* groups
    of characters from an OCR, possibly enriched with mutual spatial information [[203](#bib.bib203),
    [204](#bib.bib204)], in the visual encoding [[137](#bib.bib137), [205](#bib.bib205)].
    Another direction entails generating multiple captions describing different parts
    of the image, including the contained text [[206](#bib.bib206)].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Change Captioning. Change captioning targets changes that occurred in a scene,
    thus requiring both accurate change detection and effective natural language description.
    The task was first presented in [[207](#bib.bib207)] with the *Spot-the-Diff*
    dataset, composed of pairs of frames extracted from video surveillance footages
    and the corresponding textual descriptions of visual changes. To further explore
    this variant, the *CLEVR-Change* dataset [[208](#bib.bib208)] has been introduced,
    which contains five scene change types on almost 80K image pairs. The proposed
    approaches for this variant apply attention mechanisms to focus on semantically
    relevant aspects without being deceived by distractors such as viewpoint changes [[209](#bib.bib209),
    [210](#bib.bib210), [211](#bib.bib211)] or perform multi-task learning with image
    retrieval as an auxiliary task [[212](#bib.bib212)], where an image must be retrieved
    from its paired image and the description of the occurred changes.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Focusing on the textual output
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since every image captures a wide variety of entities with complex interactions,
    human descriptions tend to be diverse and grounded to different objects and details.
    Some image captioning variants explicitly focus on these aspects.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Diverse Captioning. Diverse image captioning tries to replicate the quality
    and variability of the sentences produced by humans. The most common technique
    to achieve diversity is based on variants of the beam search algorithm [[213](#bib.bib213)]
    that entail dividing the beams into similar groups and encouraging diversity between
    groups. Other solutions have been investigated, such as contrastive learning [[214](#bib.bib214)],
    conditional GANs [[170](#bib.bib170), [154](#bib.bib154)], and paraphrasing [[215](#bib.bib215)].
    However, these solutions tend to underperform in terms of caption quality, which
    is partially recovered by using variational auto-encoders [[216](#bib.bib216),
    [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)]. Another approach
    is exploiting multiple part-of-speech tags sequences predicted from image region
    classes [[220](#bib.bib220)] and forcing the model to produce different captions
    based on these sequences.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual Captioning. Since image captioning is commonly performed in English,
    multilingual captioning [[221](#bib.bib221)] aims to extend the applicability
    of captioning systems to other languages. The two main strategies entail collecting
    captions in different languages for commonly used datasets (*e.g.* Chinese and
    Japanese captions for COCO images [[222](#bib.bib222), [223](#bib.bib223)], German
    captions for Flick30K [[224](#bib.bib224)]), or directly training multilingual
    captioning systems with unpaired captions [[221](#bib.bib221), [225](#bib.bib225),
    [184](#bib.bib184), [226](#bib.bib226)].
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Application-specific Captioning. Image captioning can be applied to ease and
    automate activities involving text generation from images. For example, captioning
    systems can be applied for medical report generation, for which they need to predict
    disease tags and try to imitate the style of real medical reports [[227](#bib.bib227),
    [228](#bib.bib228), [229](#bib.bib229)]. Another interesting application is art
    description generation, which entails describing not only factual aspects of the
    artworks, but also their context and style, and conveyed message art description [[230](#bib.bib230)].
    To this end, captioning systems could also rely on external knowledge, *e.g.* metadata.
    A similar application is automatic caption generation for news articles [[135](#bib.bib135),
    [136](#bib.bib136)], for which named entities from the article should be described [[231](#bib.bib231),
    [232](#bib.bib232)], and the rich journalistic style should be maintained [[233](#bib.bib233),
    [234](#bib.bib234)]. Another important application domain is assistive technology
    for the visually impaired [[235](#bib.bib235)], where image captioning approaches
    must be able to provide informative descriptions even for low-quality visual inputs [[132](#bib.bib132)].
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Addressing user requirements
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regular image captioning models generate factual captions with a neutral tone
    and no interaction with end-users. Instead, some image captioning sub-tasks are
    devoted to coping with user requests.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Personalized Captioning. Humans consider more effective the captions that avoid
    stating the obvious and that are written in a style that catches their interest.
    Personalized image captioning aims at fulfilling this requirement by generating
    descriptions that take into account the user’s prior knowledge, active vocabulary,
    and writing style. To this end, early approaches exploit a memory block as a repository
    for this contextual information [[236](#bib.bib236), [237](#bib.bib237)]. On another
    line, Zhang *et al.* [[238](#bib.bib238)] proposed a multi-modal Transformer network
    that personalizes captions conditioned on the user’s recent captions and a learned
    user representation. Other works have instead focused on the style of captions
    as an additional controllable input and proposed to solve this task by exploiting
    unpaired stylized textual corpus [[239](#bib.bib239), [240](#bib.bib240), [241](#bib.bib241),
    [242](#bib.bib242)]. Some datasets have been collected to explore this variant,
    such as *InstaPIC* [[236](#bib.bib236)], which is composed of multiple Instagram
    posts from the same users, *FlickrStyle10K* [[239](#bib.bib239)], which contains
    images and textual sentences with two different styles, and *Personality-Captions* [[243](#bib.bib243)],
    which contains triples of images, captions, and one among 215 personality traits
    to be used to condition the caption generation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Controllable Captioning. Controllable captioning puts the users in the loop
    by asking them to select and give priorities to what should be described in an
    image. This information is exploited as a guiding signal for the generation process.
    The signal can be sparse, as selected image regions [[244](#bib.bib244), [163](#bib.bib163)]
    and user-provided visual words [[220](#bib.bib220)], or dense, as mouse traces [[138](#bib.bib138),
    [245](#bib.bib245)]. Eventually, the guiding signal can incorporate some form
    of structure, such as sequences that encode the mentioning order of concepts (part-of-speech
    tag as in [[220](#bib.bib220)]) or visual objects [[163](#bib.bib163)]. Guiding
    inputs can also encode the relation between objects that is most of interest for
    the user, as done for example in [[246](#bib.bib246)] via verbs and semantic roles
    (verbs represent activities in the image and semantic roles determine how objects
    engage in these activities) and in [[247](#bib.bib247), [248](#bib.bib248)] via
    user-generated or user-selected scene graphs. A different control signal is introduced
    by [[249](#bib.bib249)], which consist of a length-level embedding added as an
    additional token to each textual word, providing existing models the ability to
    generate length-controllable image captions.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Image Captioning Editing. Image captioning editing was proposed by Sammani *et
    al.* [[250](#bib.bib250)], following the consideration that generated captions
    may have repetitions and inconsistencies. This variant focuses on decoupling the
    decoding stage in a caption generation step and a caption polishing one to correct
    syntactic errors.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions and Future Directions
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image captioning is an intrinsically complex challenge for machine intelligence
    as it integrates difficulties from both Computer Vision and NLP. Further, as mentioned
    in the Introduction, the task itself is vaguely defined and captions can, in principle,
    be generated with many different styles and objectives. The presented literature
    review and experimental comparison show the performance improvement over the last
    few years on standard datasets. However, many open challenges remain since accuracy,
    robustness, and generalization results are far from satisfactory. Similarly, requirements
    of fidelity, naturalness, and diversity are not yet met. Based on the analysis
    presented, we can trace three main developmental directions for the image captioning
    field, which are discussed in the following.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Procedural and architectural challenges. Since image captioning models are data
    greedy, pre-training on large-scale datasets, even if not well-curated, is becoming
    a solid strategy, as demonstrated in [[100](#bib.bib100), [101](#bib.bib101),
    [103](#bib.bib103), [97](#bib.bib97)]. In this regard, promoting the public release
    of such datasets will be fundamental to fostering reproducibility and allowing
    fair comparisons. The growing size of pre-training models is also a concern, and
    the community will probably need to investigate less computationally-intensive
    alternatives to promote equality in the community. In architectural terms, instead,
    the growing dichotomy between early-fusion strategies and the encoder-decoder
    paradigm is still to be solved and is currently one of the main open issues. On
    the other side, the supremacy of detection features is leaving space to a variety
    of visual encoding strategies (pre-training from scratch, using detections, using
    features from multi-modal models) which all appear to be on pair in terms of performance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Generalization, diversity, long-tail concepts. While pre-training on web-scale
    datasets provides a promising direction to increase generalization and promoting
    long-tail concepts [[97](#bib.bib97)], specializing in particular domains and
    generating captions with different styles and aims is still among the main open
    challenges for image captioning. Although we discussed some attempts to encourage
    naturalness and diversity [[170](#bib.bib170), [214](#bib.bib214), [216](#bib.bib216)],
    further research is needed to design models that are suitable for real-world applications.
    In this sense, the emergence of models which can deal with long-tail concepts [[97](#bib.bib97),
    [104](#bib.bib104)] offers a valuable promise of modeling real-life scenarios
    and generalizing to different contexts. Additionally, developments in image captioning
    variants such as novel objects captioning or controllable captioning could help
    to tackle this open issue. Notably, the emergence of subword-based tokenization
    techniques has made it possible to handle and generate rare words.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Design of trustworthy AI solutions. Due to its potential in human-machine interaction,
    image captioning needs solutions that are transparent and acceptable for end-users,
    framed as overcome bias, and interpretable. Since most vision-and-language datasets
    share common patterns and regularities, datasets bias and overrepresented visual
    concepts are major issues for any vision-and-language task. In this sense, some
    effort should be devoted to the study of fairness and bias: two possible directions
    entail designing specific evaluation metrics and focusing on the robustness to
    unwanted correlations. Further, despite the promising performance on the benchmark
    datasets, state-of-the-art approaches are not yet satisfactory when applied in
    the wild. A possible reason for this is the evaluation procedures used and their
    impact on the training approaches currently adopted. In this sense, the design
    of appropriate and reproducible evaluation protocols [[251](#bib.bib251), [252](#bib.bib252),
    [253](#bib.bib253)] and insightful metrics remains an open challenge in image
    captioning. Moreover, since the task is currently defined as a supervised one
    and thus is strongly influenced by the training data, the development of scores
    that do not need reference captions for assessing the performance would be key
    for a shift towards unsupervised image captioning. Finally, since existing image
    captioning algorithms lack reliable and interpretable means for determining the
    cause of a particular output, further research is needed to shed more light on
    model explainability, focusing on how these deal with different modalities or
    novel concepts.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank CINECA for providing computational resources. This work has been supported
    by “Fondazione di Modena”, by the “Artificial Intelligence for Cultural Heritage
    (AI4CH)” project, co-funded by the Italian Ministry of Foreign Affairs and International
    Cooperation, and by the H2020 ICT-48-2020 HumanE-AI-NET project. We also want
    to thank the authors who provided us with the captions and model weights for some
    of the surveyed approaches.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Ardila, B. Bernal, and M. Rosselli, “Language and visual perception
    associations: meta-analytic connectivity modeling of Brodmann Area 37,” *Behavioural
    Neurology*, 2015.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J.-Y. Pan, H.-J. Yang, P. Duygulu, and C. Faloutsos, “Automatic image captioning,”
    in *ICME*, 2004.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier,
    and D. Forsyth, “Every picture tells a story: Generating sentences from images,”
    in *ECCV*, 2010.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] V. Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing images using
    1 million captioned photographs,” in *NeurIPS*, 2011.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and
    T. Mikolov, “DeViSE: a deep visual-semantic embedding model,” in *NeurIPS*, 2013.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] R. Kiros, R. Salakhutdinov, and R. S. Zemel, “Unifying visual-semantic
    embeddings with multimodal neural language models,” in *NeurIPS Workshops*, 2014.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Karpathy, A. Joulin, and L. Fei-Fei, “Deep fragment embeddings for bidirectional
    image sentence mapping,” in *NeurIPS*, 2014.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu, “I2T: Image parsing
    to text description,” *Proceedings of the IEEE*, 2010.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Aker and R. Gaizauskas, “Generating image descriptions using dependency
    relational patterns,” in *ACL*, 2010.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Yang, C. Teo, H. Daumé III, and Y. Aloimonos, “Corpus-guided sentence
    generation of natural images,” in *EMNLP*, 2011.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Li, G. Kulkarni, T. Berg, A. Berg, and Y. Choi, “Composing simple image
    descriptions using web-scale n-grams,” in *CoNLL*, 2011.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Gupta, Y. Verma, and C. Jawahar, “Choosing linguistics over vision
    to describe images,” in *AAAI*, 2012.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Mitchell, J. Dodge, A. Goyal, K. Yamaguchi, K. Stratos, X. Han, A. Mensch,
    A. Berg, T. Berg, and H. Daumé III, “Midge: Generating image descriptions from
    computer vision detections,” in *ACL*, 2012.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi, A. C. Berg,
    and T. L. Berg, “BabyTalk: Understanding and generating simple image descriptions,”
    *IEEE Trans. PAMI*, 2013.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] P. Kuznetsova, V. Ordonez, T. L. Berg, and Y. Choi, “Treetalk: Composition
    and compression of trees for image descriptions,” *TACL*, vol. 2, pp. 351–362,
    2014.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. Ikizler-Cinbis,
    F. Keller, A. Muscat, and B. Plank, “Automatic description generation from images:
    A survey of models, datasets, and evaluation measures,” *JAIR*, vol. 55, pp. 409–442,
    2016.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Bai and S. An, “A survey on automatic image caption generation,” *Neurocomputing*,
    vol. 311, pp. 291–304, 2018.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A Comprehensive
    Survey of Deep Learning for Image Captioning,” *ACM Computing Surveys*, vol. 51,
    no. 6, pp. 1–36, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] M. Hodosh, P. Young, and J. Hockenmaier, “Framing image description as
    a ranking task: Data, models and evaluation metrics,” *JAIR*, 2013.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] N. Sharif, U. Nadeem, S. A. A. Shah, M. Bennamoun, and W. Liu, “Vision
    to Language: Methods, Metrics and Datasets,” in *Machine Learning Paradigms*,
    2020, pp. 9–62.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] X. Liu, Q. Xu, and N. Wang, “A survey on deep neural network-based image
    captioning,” *The Visual Computer*, vol. 35, no. 3, pp. 445–470, 2019.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Sharma, M. Agrahari, S. K. Singh, M. Firoj, and R. K. Mishra, “Image
    captioning: a comprehensive survey,” in *PARC*, 2020.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural
    image caption generator,” in *CVPR*, 2015.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating
    image descriptions,” in *CVPR*, 2015.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille, “Deep Captioning
    with Multimodal Recurrent Neural Networks (m-RNN),” in *ICLR*, 2015.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
    K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for visual
    recognition and description,” in *CVPR*, 2015.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] X. Chen and C. Lawrence Zitnick, “Mind’s Eye: A Recurrent Visual Representation
    for Image Caption Generation,” in *CVPR*, 2015.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollár, J. Gao,
    X. He, M. Mitchell, J. C. Platt *et al.*, “From captions to visual concepts and
    back,” in *CVPR*, 2015.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] X. Jia, E. Gavves, B. Fernando, and T. Tuytelaars, “Guiding the Long-Short
    Term Memory model for Image Caption Generation,” in *ICCV*, 2015.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, “Image captioning with semantic
    attention,” in *CVPR*, 2016.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Q. Wu, C. Shen, L. Liu, A. Dick, and A. Van Den Hengel, “What Value Do
    Explicit High Level Concepts Have in Vision to Language Problems?” in *CVPR*,
    2016.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Gu, G. Wang, J. Cai, and T. Chen, “An Empirical Study of Language CNN
    for Image Captioning,” in *ICCV*, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] F. Chen, R. Ji, J. Su, Y. Wu, and Y. Wu, “StructCap: Structured Semantic
    Embedding for Image Captioning,” in *ACM Multimedia*, 2017.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] F. Chen, R. Ji, X. Sun, Y. Wu, and J. Su, “GroupCap: Group-based Image
    Captioning with Structured Relevance and Diversity Constraints,” in *CVPR*, 2018.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel, “Self-critical
    sequence training for image captioning,” in *CVPR*, 2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei, “Boosting image captioning
    with attributes,” in *ICCV*, 2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Z. Gan, C. Gan, X. He, Y. Pu, K. Tran, J. Gao, L. Carin, and L. Deng,
    “Semantic Compositional Networks for Visual Captioning,” in *CVPR*, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S.
    Zemel, and Y. Bengio, “Show, attend and tell: Neural image caption generation
    with visual attention,” in *ICML*, 2015.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Lu, C. Xiong, D. Parikh, and R. Socher, “Knowing when to look: Adaptive
    attention via a visual sentinel for image captioning,” in *CVPR*, 2017.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] B. Dai, D. Ye, and D. Lin, “Rethinking the form of latent states in image
    captioning,” in *ECCV*, 2018.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *ICLR*, 2014.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] X. Chen, L. Ma, W. Jiang, J. Yao, and W. Liu, “Regularizing RNNs for Caption
    Generation by Reconstructing The Past with The Present,” in *CVPR*, 2018.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Y. Wang, Z. Lin, X. Shen, S. Cohen, and G. W. Cottrell, “Skeleton Key:
    Image Captioning by Skeleton-Attribute Decomposition,” in *CVPR*, 2017.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] H. Ge, Z. Yan, K. Zhang, M. Zhao, and L. Sun, “Exploring Overall Contextual
    Information for Image Captioning in Human-Like Cognitive Style,” in *ICCV*, 2019.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Gu, J. Cai, G. Wang, and T. Chen, “Stack-Captioning: Coarse-to-Fine
    Learning for Image Captioning,” in *AAAI*, 2018.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. Yang, Y. Yuan, Y. Wu, W. W. Cohen, and R. R. Salakhutdinov, “Review
    Networks for Caption Generation,” in *NeurIPS*, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T.-S. Chua, “SCA-CNN:
    Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning,”
    in *CVPR*, 2017.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] W. Jiang, L. Ma, Y.-G. Jiang, W. Liu, and T. Zhang, “Recurrent Fusion
    Network for Image Captioning,” in *ECCV*, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Sugano and A. Bulling, “Seeing with Humans: Gaze-Assisted Neural Image
    Captioning,” *arXiv preprint arXiv:1608.05203*, 2016.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] H. R. Tavakoli, R. Shetty, A. Borji, and J. Laaksonen, “Paying attention
    to descriptions generated by image captioning models,” in *ICCV*, 2017.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] V. Ramanishka, A. Das, J. Zhang, and K. Saenko, “Top-down visual saliency
    guided by captions,” in *CVPR*, 2017.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, “Paying More Attention
    to Saliency: Image Captioning with Saliency and Context Attention,” *ACM TOMM*,
    vol. 14, no. 2, pp. 1–21, 2018.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. Chen and Q. Zhao, “Boosted attention: Leveraging human attention for
    image captioning,” in *ECCV*, 2018.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
    “Bottom-up and top-down attention for image captioning and visual question answering,”
    in *CVPR*, 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
    object detection with region proposal networks,” in *NeurIPS*, 2015.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] ——, “Faster R-CNN: towards real-time object detection with region proposal
    networks,” *IEEE Trans. PAMI*, vol. 39, no. 6, pp. 1137–1149, 2017.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
    Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei, “Visual Genome:
    Connecting Language and Vision Using Crowdsourced Dense Image Annotations,” *IJCV*,
    vol. 123, no. 1, pp. 32–73, 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] L. Ke, W. Pei, R. Li, X. Shen, and Y.-W. Tai, “Reflective Decoding Network
    for Image Captioning,” in *ICCV*, 2019.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Qin, J. Du, Y. Zhang, and H. Lu, “Look Back and Predict Forward in
    Image Captioning,” in *CVPR*, 2019.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] L. Huang, W. Wang, Y. Xia, and J. Chen, “Adaptively Aligned Image Captioning
    via Adaptive Attention Time,” in *NeurIPS*, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] L. Wang, Z. Bai, Y. Zhang, and H. Lu, “Show, Recall, and Tell: Image Captioning
    with Recall Mechanism,” in *AAAI*, 2020.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Z.-J. Zha, D. Liu, H. Zhang, Y. Zhang, and F. Wu, “Context-aware visual
    policy network for fine-grained image captioning,” *IEEE Trans. PAMI*, 2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] M. Pedersoli, T. Lucas, C. Schmid, and J. Verbeek, “Areas of Attention
    for Image Captioning,” in *ICCV*, 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] T. Yao, Y. Pan, Y. Li, and T. Mei, “Exploring Visual Relationship for
    Image Captioning,” in *ECCV*, 2018.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] L. Guo, J. Liu, J. Tang, J. Li, W. Luo, and H. Lu, “Aligning linguistic
    words and visual semantic units for image captioning,” in *ACM Multimedia*, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] X. Yang, K. Tang, H. Zhang, and J. Cai, “Auto-Encoding Scene Graphs for
    Image Captioning,” in *CVPR*, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Z. Shi, X. Zhou, X. Qiu, and X. Zhu, “Improving Image Captioning with
    Better Use of Captions,” in *ACL*, 2020.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] T. Yao, Y. Pan, Y. Li, and T. Mei, “Hierarchy Parsing for Image Captioning,”
    in *ICCV*, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, 2017.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] X. Yang, H. Zhang, and J. Cai, “Learning to Collocate Neural Modules for
    Image Captioning,” in *ICCV*, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] G. Li, L. Zhu, P. Liu, and Y. Yang, “Entangled Transformer for Image Captioning,”
    in *ICCV*, 2019.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Herdade, A. Kappeler, K. Boakye, and J. Soares, “Image Captioning:
    Transforming Objects into Words,” in *NeurIPS*, 2019.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] L. Guo, J. Liu, X. Zhu, P. Yao, S. Lu, and H. Lu, “Normalized and Geometry-Aware
    Self-Attention Network for Image Captioning,” in *CVPR*, 2020.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] L. Huang, W. Wang, J. Chen, and X.-Y. Wei, “Attention on Attention for
    Image Captioning,” in *ICCV*, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Y. Pan, T. Yao, Y. Li, and T. Mei, “X-Linear Attention Networks for Image
    Captioning,” in *CVPR*, 2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, “Meshed-Memory
    Transformer for Image Captioning,” in *CVPR*, 2020.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. He, W. Liao, H. R. Tavakoli, M. Yang, B. Rosenhahn, and N. Pugeault,
    “Image captioning through image transformer,” in *ACCV*, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] F. Liu, X. Ren, X. Wu, S. Ge, W. Fan, Y. Zou, and X. Sun, “Prophet Attention:
    Predicting Attention with Future Attention,” in *NeurIPS*, 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] M. Cornia, L. Baraldi, and R. Cucchiara, “SMArT: Training Shallow Memory-aware
    Transformers for Robotic Explainability,” in *ICRA*, 2020.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Ji, Y. Luo, X. Sun, F. Chen, G. Luo, Y. Wu, Y. Gao, and R. Ji, “Improving
    Image Captioning by Leveraging Intra- and Inter-layer Global Representation in
    Transformer Network,” in *AAAI*, 2021.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y. Luo, J. Ji, X. Sun, L. Cao, Y. Wu, F. Huang, C.-W. Lin, and R. Ji,
    “Dual-Level Collaborative Transformer for Image Captioning,” in *AAAI*, 2021.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] F. Liu, Y. Liu, X. Ren, X. He, and X. Sun, “Aligning visual regions and
    textual concepts for semantic-grounded image representations,” in *NeurIPS*, 2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] H. Jiang, I. Misra, M. Rohrbach, E. Learned-Miller, and X. Chen, “In defense
    of grid features for visual question answering,” in *CVPR*, 2020.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] X. Zhang, X. Sun, Y. Luo, J. Ji, Y. Zhou, Y. Wu, F. Huang, and R. Ji,
    “RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words,” in
    *CVPR*, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An Image is Worth 16x16
    Words: Transformers for Image Recognition at Scale,” *ICLR*, 2021.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,
    “Training data-efficient image transformers & distillation through attention,”
    in *ICML*, 2021.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] W. Liu, S. Chen, L. Guo, X. Zhu, and J. Liu, “CPTR: Full Transformer Network
    for Image Captioning,” *arXiv preprint arXiv:2101.10804*, 2021.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning Transferable
    Visual Models From Natural Language Supervision,” *arXiv preprint arXiv:2103.00020*,
    2021.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao, “SimVLM: Simple
    visual language model pretraining with weak supervision,” *arXiv preprint arXiv:2108.10904*,
    2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. Shen, L. H. Li, H. Tan, M. Bansal, A. Rohrbach, K.-W. Chang, Z. Yao,
    and K. Keutzer, “How Much Can CLIP Benefit Vision-and-Language Tasks?” *arXiv
    preprint arXiv:2107.06383*, 2021.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] R. Mokady, A. Hertz, and A. H. Bermano, “ClipCap: CLIP Prefix for Image
    Captioning,” *arXiv preprint arXiv:2111.09734*, 2021.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] M. Cornia, L. Baraldi, G. Fiameni, and R. Cucchiara, “Universal Captioner:
    Long-Tail Vision-and-Language Model Training through Content-Style Separation,”
    *arXiv preprint arXiv:2111.12727*, 2021.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder representations
    from transformers,” *EMNLP*, 2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-agnostic
    visiolinguistic representations for vision-and-language tasks,” in *NeurIPS*,
    2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong,
    F. Wei *et al.*, “Oscar: Object-semantics aligned pre-training for vision-language
    tasks,” in *ECCV*, 2020.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. J. Corso, and J. Gao, “Unified
    Vision-Language Pre-Training for Image Captioning and VQA,” in *AAAI*, 2020.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” *NAACL*, 2018.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao,
    “VinVL: Revisiting visual representations in vision-language models,” in *CVPR*,
    2021.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang, “Scaling
    Up Vision-Language Pre-training for Image Captioning,” *arXiv preprint arXiv:2111.12233*,
    2021.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. Lu, J. Yang, D. Batra, and D. Parikh, “Neural Baby Talk,” in *CVPR*,
    2018.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] X. Zhu, W. Wang, L. Guo, and J. Liu, “AutoCaption: Image Captioning with
    Neural Architecture Search,” *arXiv preprint arXiv:2012.09742*, 2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J. Aneja, A. Deshpande, and A. G. Schwing, “Convolutional image captioning,”
    in *CVPR*, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving
    language understanding by generative pre-training,” 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Z.-c. Fei, “Fast Image Caption Generation with Position Alignment,” *AAAI
    Workshops*, 2019.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] L. Guo, J. Liu, X. Zhu, X. He, J. Jiang, and H. Lu, “Non-autoregressive
    image captioning with counterfactuals-critical multi-agent learning,” *IJCAI*,
    2020.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Z. Fei, “Iterative Back Modification for Faster Image Captioning,” in
    *ACM Multimedia*, 2020.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] L. Guo, J. Liu, X. Zhu, and H. Lu, “Fast Sequence Generation with Multi-Agent
    Reinforcement Learning,” *arXiv preprint arXiv:2101.09698*, 2021.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] P. Koehn, *Statistical Machine Translation*.   Cambridge University Press,
    2009.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence level training
    with recurrent neural networks,” in *ICLR*, 2016.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine Learning*, 1992.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method for automatic
    evaluation of machine translation,” in *ACL*, 2002.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,”
    in *ACL Workshops*, 2004.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Z. Ren, X. Wang, N. Zhang, X. Lv, and L.-J. Li, “Deep reinforcement learning-based
    image captioning with embedding reward,” in *CVPR*, 2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy, “Improved Image
    Captioning via Policy Gradient Optimization of SPIDEr,” in *ICCV*, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “SPICE: Semantic
    Propositional Image Caption Evaluation,” in *ECCV*, 2016.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “CIDEr: Consensus-based
    Image Description Evaluation,” in *CVPR*, 2015.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] L. Zhang, F. Sung, F. Liu, T. Xiang, S. Gong, Y. Yang, and T. M. Hospedales,
    “Actor-Critic Sequence Training for Image Captioning,” in *NeurIPS*, 2017.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Gao, S. Wang, S. Wang, S. Ma, and W. Gao, “Self-critical n-step Training
    for Image Captioning,” in *CVPR*, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Q. Xia, H. Huang, N. Duan, D. Zhang, L. Ji, Z. Sui, E. Cui, T. Bharti,
    and M. Zhou, “XGPT: Cross-modal Generative Pre-Training for Image Captioning,”
    *arXiv preprint arXiv:2003.01473*, 2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] K. Desai and J. Johnson, “VirTex: Learning Visual Representations From
    Textual Annotations,” in *CVPR*, 2021.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] R. Sennrich, B. Haddow, and A. Birch, “Neural Machine Translation of
    Rare Words with Subword Units,” in *ACL*, 2016.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft COCO: Common Objects in Context,” in *ECCV*, 2014.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions
    to visual denotations: New similarity metrics for semantic inference over event
    descriptions,” *TACL*, 2014.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions:
    A cleaned, hypernymed, image alt-text dataset for automatic image captioning,”
    in *ACL*, 2018.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12M: Pushing
    Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts,” in
    *CVPR*, 2021.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] D. Gurari, Y. Zhao, M. Zhang, and N. Bhattacharya, “Captioning Images
    Taken by People Who Are Blind,” in *ECCV*, 2020.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] S. Reed, Z. Akata, H. Lee, and B. Schiele, “Learning deep representations
    of fine-grained visual descriptions,” in *CVPR*, 2016.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] X. Yang, H. Zhang, D. Jin, Y. Liu, C.-H. Wu, J. Tan, D. Xie, J. Wang,
    and X. Wang, “Fashion Captioning: Towards Generating Accurate Descriptions with
    Semantic Rewards,” in *ECCV*, 2020.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Ramisa, F. Yan, F. Moreno-Noguer, and K. Mikolajczyk, “BreakingNews:
    Article Annotation by Image and Text Processing,” *IEEE Trans. PAMI*, vol. 40,
    no. 5, pp. 1072–1085, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. F. Biten, L. Gomez, M. Rusinol, and D. Karatzas, “Good news, everyone!
    context driven entity-aware captioning for news images,” in *CVPR*, 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh, “TextCaps: a Dataset for
    Image Captioning with Reading Comprehension,” in *ECCV*, 2020.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] J. Pont-Tuset, J. Uijlings, S. Changpinyo, R. Soricut, and V. Ferrari,
    “Connecting vision and language with localized narratives,” in *ECCV*, 2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,
    and I. Sutskever, “Zero-Shot Text-to-Image Generation,” *arXiv preprint arXiv:2102.12092*,
    2021.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland,
    D. Borth, and L.-J. Li, “YFCC100M: The new data in multimedia research,” *Communications
    of the ACM*, vol. 59, no. 2, pp. 64–73, 2016.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] K. Srinivasan, K. Raman, J. Chen, M. Bendersky, and M. Najork, “WIT:
    Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning,”
    *arXiv preprint arXiv:2103.01913*, 2021.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung,
    Z. Li, and T. Duerig, “Scaling up visual and vision-language representation learning
    with noisy text supervision,” in *ICML*, 2021.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and
    P. Perona, “Caltech-UCSD Birds 200,” California Institute of Technology, Tech.
    Rep., 2010.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] M.-E. Nilsback and A. Zisserman, “Automated flower classification over
    a large number of classes,” in *ICVGIP*, 2008.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] T.-H. Chen, Y.-H. Liao, C.-Y. Chuang, W.-T. Hsu, J. Fu, and M. Sun, “Show,
    adapt and tell: Adversarial training of cross-domain image captioner,” in *ICCV*,
    2017.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, and T. Darrell,
    “Generating visual explanations,” in *ECCV*, 2016.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] L. A. Hendricks, R. Hu, T. Darrell, and Z. Akata, “Grounding visual explanations,”
    in *ECCV*, 2018.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, “Generative
    adversarial text to image synthesis,” in *ICML*, 2016.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] J. Kasai, K. Sakaguchi, L. Dunagan, J. Morrison, R. L. Bras, Y. Choi,
    and N. A. Smith, “Transparent human evaluation for image captioning,” *arXiv preprint
    arXiv:2111.08940*, 2021.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT evaluation
    with improved correlation with human judgments,” in *ACL Workshops*, 2005.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] N. Sharif, L. White, M. Bennamoun, and S. A. A. Shah, “NNEval: Neural
    network based evaluation metric for image captioning,” in *ECCV*, 2018.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Y. Cui, G. Yang, A. Veit, X. Huang, and S. Belongie, “Learning to evaluate
    image captioning,” in *CVPR*, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] O. Caglayan, P. Madhyastha, and L. Specia, “Curious Case of Language
    Generation Evaluation Metrics: A Cautionary Tale,” in *COLING*, 2020.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] R. Shetty, M. Rohrbach, L. Anne Hendricks, M. Fritz, and B. Schiele,
    “Speaking the same language: Matching machine to human captions by adversarial
    training,” in *ICCV*, 2017.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] E. Van Miltenburg, D. Elliott, and P. Vossen, “Measuring the diversity
    of automatic image descriptions,” in *COLING*, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Q. Wang and A. B. Chan, “Describing like humans: on diversity in image
    captioning,” in *CVPR*, 2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Q. Wang, J. Wan, and A. B. Chan, “On Diversity in Image Captioning: Metrics
    and Methods,” *IEEE Trans. PAMI*, 2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko, “Object
    Hallucination in Image Captioning,” in *EMNLP*, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] M. Jiang, J. Hu, Q. Huang, L. Zhang, J. Diesner, and J. Gao, “REO-Relevance,
    Extraness, Omission: A Fine-grained Evaluation for Image Captioning,” in *EMNLP-IJCNLP*,
    2019.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Z. Wang, B. Feng, K. Narasimhan, and O. Russakovsky, “Towards unique
    and informative captioning of images,” in *ECCV*, 2020.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] M. Kusner, Y. Sun, N. Kolkin, and K. Weinberger, “From word embeddings
    to document distances,” in *ICML*, 2015.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] M. Kilickaya, A. Erdem, N. Ikizler-Cinbis, and E. Erdem, “Re-evaluating
    automatic metrics for image captioning,” in *ACL*, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] M. Cornia, L. Baraldi, and R. Cucchiara, “Show, Control and Tell: A Framework
    for Generating Controllable and Grounded Captions,” in *CVPR*, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] R. Bigazzi, F. Landi, M. Cornia, S. Cascianelli, L. Baraldi, and R. Cucchiara,
    “Explore and Explain: Self-supervised Navigation and Recounting,” in *ICPR*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] H. Lee, S. Yoon, F. Dernoncourt, D. S. Kim, T. Bui, and K. Jung, “ViLBERTScore:
    Evaluating Image Caption Using Vision-and-Language BERT,” in *EMNLP Workshops*,
    2020.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Y. Yi, H. Deng, and J. Hu, “Improving image captioning evaluation by
    considering inter references variance,” in *ACL*, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] S. Wang, Z. Yao, R. Wang, Z. Wu, and X. Chen, “FAIEr: Fidelity and Adequacy
    Ensured Image Caption Evaluation,” in *CVPR*, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] H. Lee, S. Yoon, F. Dernoncourt, T. Bui, and K. Jung, “UMIC: An Unreferenced
    Metric for Image Captioning via Contrastive Learning,” in *ACL*, 2021.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] B. Dai, S. Fidler, and D. Lin, “A neural compositional paradigm for image
    captioning,” in *NeurIPS*, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] B. Dai, S. Fidler, R. Urtasun, and D. Lin, “Towards Diverse and Natural
    Image Descriptions via a Conditional GAN,” in *ICCV*, 2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “BERTScore:
    Evaluating Text Generation with BERT,” in *ICLR*, 2020.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] I. J. Unanue, J. Parnell, and M. Piccardi, “BERTTune: Fine-Tuning Neural
    Machine Translation with BERTScore,” *ACL*, 2021.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] M. Jiang, Q. Huang, L. Zhang, X. Wang, P. Zhang, Z. Gan, J. Diesner,
    and J. Gao, “TIGEr: text-to-image grounding for image caption evaluation,” in
    *ACL*, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked Cross Attention
    for Image-Text Matching,” in *ECCV*, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi, “CLIPScore:
    A Reference-free Evaluation Metric for Image Captioning,” *arXiv preprint arXiv:2104.08718*,
    2021.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, and
    T. Darrell, “Deep Compositional Captioning: Describing Novel Object Categories
    without Paired Training Data,” in *CVPR*, 2016.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney, T. Darrell,
    and K. Saenko, “Captioning Images with Diverse Objects,” in *CVPR*, 2017.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] H. Agrawal, K. Desai, X. Chen, R. Jain, D. Batra, D. Parikh, S. Lee,
    and P. Anderson, “nocaps: novel object captioning at scale,” in *ICCV*, 2019.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] T. Yao, Y. Pan, Y. Li, and T. Mei, “Incorporating Copying Mechanism in
    Image Captioning for Learning Novel Objects,” in *CVPR*, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Y. Li, T. Yao, Y. Pan, H. Chao, and T. Mei, “Pointing Novel Objects in
    Image Captioning,” in *CVPR*, 2019.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Y. Wu, L. Zhu, L. Jiang, and Y. Yang, “Decoupled Novel Object Captioner,”
    in *ACM Multimedia*, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “Guided Open Vocabulary
    Image Captioning with Constrained Beam Search,” in *EMNLP*, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] X. Hu, X. Yin, K. Lin, L. Wang, L. Zhang, J. Gao, and Z. Liu, “VIVO:
    Visual Vocabulary Pre-Training for Novel Object Captioning,” *AAAI*, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] J. Gu, S. Joty, J. Cai, and G. Wang, “Unpaired image captioning by language
    pivoting,” in *ECCV*, 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Y. Feng, L. Ma, W. Liu, and J. Luo, “Unsupervised image captioning,”
    in *CVPR*, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] I. Laina, C. Rupprecht, and N. Navab, “Towards unsupervised image captioning
    with shared multimodal embeddings,” in *ICCV*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] J. Gu, S. Joty, J. Cai, H. Zhao, X. Yang, and G. Wang, “Unpaired image
    captioning via scene graph alignments,” in *ICCV*, 2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] D. Guo, Y. Wang, P. Song, and M. Wang, “Recurrent relational memory network
    for unsupervised image captioning,” *IJCAI*, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] D.-J. Kim, J. Choi, T.-H. Oh, and I. S. Kweon, “Image Captioning with
    Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach,” in
    *EMNLP*, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] H. Ben, Y. Pan, Y. Li, T. Yao, R. Hong, M. Wang, and T. Mei, “Unpaired
    Image Captioning with Semantic-Constrained Self-Learning,” *IEEE Trans. Multimedia*,
    2021.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] R. Del Chiaro, B. Twardowski, A. D. Bagdanov, and J. van de Weijer, “RATT:
    Recurrent Attention to Transient Tasks for Continual Image Captioning,” in *NeurIPS*,
    2020.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] J. Johnson, A. Karpathy, and L. Fei-Fei, “DenseCap: Fully convolutional
    Localization Networks for Dense Captioning,” in *CVPR*, 2016.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] L. Yang, K. Tang, J. Yang, and L.-J. Li, “Dense captioning with joint
    inference and visual context,” in *CVPR*, 2017.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] X. Li, S. Jiang, and J. Han, “Learning object context for dense captioning,”
    in *AAAI*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] G. Yin, L. Sheng, B. Liu, N. Yu, X. Wang, and J. Shao, “Context and attribute
    grounded dense captioning,” in *CVPR*, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] D.-J. Kim, J. Choi, T.-H. Oh, and I. S. Kweon, “Dense Relational Captioning:
    Triple-Stream Networks for Relationship-Based Captioning,” in *CVPR*, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] J. Krause, J. Johnson, R. Krishna, and L. Fei-Fei, “A hierarchical approach
    for generating descriptive image paragraphs,” in *CVPR*, 2017.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] X. Liang, Z. Hu, H. Zhang, C. Gan, and E. P. Xing, “Recurrent topic-transition
    GAN for visual paragraph generation,” in *ICCV*, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Y. Mao, C. Zhou, X. Wang, and R. Li, “Show and Tell More: Topic-Oriented
    Multi-Sentence Image Captioning,” in *IJCAI*, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] M. Chatterjee and A. G. Schwing, “Diverse and coherent paragraph generation
    from images,” in *ECCV*, 2018.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Y. Luo, Z. Huang, Z. Zhang, Z. Wang, J. Li, and Y. Yang, “Curiosity-driven
    reinforcement learning for diverse visual paragraph generation,” in *ACM Multimedia*,
    2019.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Z. Yang, Y. Lu, J. Wang, X. Yin, D. Florencio, L. Wang, C. Zhang, L. Zhang,
    and J. Luo, “TAP: Text-Aware Pre-training for Text-VQA and Text-Caption,” in *CVPR*,
    2021.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] J. Wang, J. Tang, and J. Luo, “Multimodal Attention with Image Text Spatial
    Relationship for OCR-Based Image Captioning,” in *ACM Multimedia*, 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] J. Wang, J. Tang, M. Yang, X. Bai, and J. Luo, “Improving OCR-based Image
    Captioning by Incorporating Geometrical Relationship,” in *CVPR*, 2021.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Q. Zhu, C. Gao, P. Wang, and Q. Wu, “Simple is not Easy: A Simple Strong
    Baseline for TextVQA and TextCaps,” in *AAAI*, 2021.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] G. Xu, S. Niu, M. Tan, Y. Luo, Q. Du, and Q. Wu, “Towards Accurate Text-based
    Image Captioning with Content Diversity Exploration,” in *CVPR*, 2021.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] H. Jhamtani and T. Berg-Kirkpatrick, “Learning to describe differences
    between pairs of similar images,” in *EMNLP*, 2018.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] D. H. Park, T. Darrell, and A. Rohrbach, “Robust Change Captioning,”
    in *CVPR*, 2019.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] X. Shi, X. Yang, J. Gu, S. Joty, and J. Cai, “Finding It at Another Side:
    A Viewpoint-Adapted Matching Encoder for Change Captioning,” in *ECCV*, 2020.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Q. Huang, Y. Liang, J. Wei, C. Yi, H. Liang, H.-f. Leung, and Q. Li,
    “Image Difference Captioning with Instance-Level Fine-Grained Feature Representation,”
    *IEEE Trans. Multimedia*, 2021.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] H. Kim, J. Kim, H. Lee, H. Park, and G. Kim, “Viewpoint-Agnostic Change
    Captioning With Cycle Consistency,” in *ICCV*, 2021.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] M. Hosseinzadeh and Y. Wang, “Image Change Captioning by Learning from
    an Auxiliary Task,” in *CVPR*, 2021.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun, S. Lee, D. J.
    Crandall, and D. Batra, “Diverse Beam Search for Improved Description of Complex
    Scenes,” in *AAAI*, 2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] B. Dai and D. Lin, “Contrastive learning for image captioning,” in *NeurIPS*,
    2017.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] L. Liu, J. Tang, X. Wan, and Z. Guo, “Generating diverse and descriptive
    image captions using visual paraphrases,” in *ICCV*, 2019.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] L. Wang, A. G. Schwing, and S. Lazebnik, “Diverse and accurate image
    description using a variational auto-encoder with an additive gaussian encoding
    space,” in *NeurIPS*, 2017.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] J. Aneja, H. Agrawal, D. Batra, and A. Schwing, “Sequential latent spaces
    for modeling the intention during diverse image captioning,” in *ICCV*, 2019.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] F. Chen, R. Ji, J. Ji, X. Sun, B. Zhang, X. Ge, Y. Wu, F. Huang, and
    Y. Wang, “Variational structured semantic inference for diverse image captioning,”
    in *NeurIPS*, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] S. Mahajan and S. Roth, “Diverse image captioning with context-object
    split latent spaces,” in *NeurIPS*, 2020.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] A. Deshpande, J. Aneja, L. Wang, A. G. Schwing, and D. Forsyth, “Fast,
    diverse and accurate image captioning guided by part-of-speech,” in *CVPR*, 2019.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] D. Elliott, S. Frank, and E. Hasler, “Multilingual image description
    with neural sequence models,” *ICLR*, 2015.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] X. Li, C. Xu, X. Wang, W. Lan, Z. Jia, G. Yang, and J. Xu, “COCO-CN for
    cross-lingual image tagging, captioning, and retrieval,” *IEEE Trans. Multimedia*,
    vol. 21, no. 9, pp. 2347–2360, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] T. Miyazaki and N. Shimizu, “Cross-lingual image caption generation,”
    in *ACM Multimedia*, 2016.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] D. Elliott, S. Frank, K. Sima’an, and L. Specia, “Multi30K: Multilingual
    English-German Image Descriptions,” in *ACL Workshops*, 2016.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] W. Lan, X. Li, and J. Dong, “Fluency-guided cross-lingual image captioning,”
    in *ACM Multimedia*, 2017.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Y. Song, S. Chen, Y. Zhao, and Q. Jin, “Unpaired cross-lingual image
    caption generation with self-supervised rewards,” in *ACM Multimedia*, 2019.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] B. Jing, P. Xie, and E. Xing, “On the Automatic Generation of Medical
    Imaging Reports,” in *ACL*, 2018.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] F. Liu, X. Wu, S. Ge, W. Fan, and Y. Zou, “Exploring and Distilling Posterior
    and Prior Knowledge for Radiology Report Generation,” in *CVPR*, 2021.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] X. Yang, M. Ye, Q. You, and F. Ma, “Writing by Memorizing: Hierarchical
    Retrieval-based Medical Report Generation,” in *ACL-IJCNLP*, 2021.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Z. Bai, Y. Nakashima, and N. Garcia, “Explain Me the Painting: Multi-Topic
    Knowledgeable Art Description Generation,” in *ICCV*, 2021.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Y. Feng and M. Lapata, “Automatic Caption Generation for News Images,”
    *IEEE Trans. PAMI*, vol. 35, no. 4, pp. 797–812, 2012.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] A. Tran, A. Mathews, and L. Xie, “Transform and Tell: Entity-Aware News
    Image Captioning,” in *CVPR*, 2020.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] F. Liu, Y. Wang, T. Wang, and V. Ordonez, “Visual News: Benchmark and
    Challenges in News Image Captioning,” in *EMNLP*, 2021.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] X. Yang, S. Karaman, J. Tetreault, and A. Jaimes, “Journalistic Guidelines
    Aware News Image Captioning,” *arXiv preprint arXiv:2109.02865*, 2021.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] S. Wu, J. Wieland, O. Farivar, and J. Schiller, “Automatic alt-text:
    Computer-generated image descriptions for blind users on a social network service,”
    in *CSCW*, 2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] C. Chunseong Park, B. Kim, and G. Kim, “Attend to you: Personalized image
    captioning with context sequence memory networks,” in *CVPR*, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] C. C. Park, B. Kim, and G. Kim, “Towards personalized image captioning
    via multimodal memory networks,” *IEEE Trans. PAMI*, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] W. Zhang, Y. Ying, P. Lu, and H. Zha, “Learning Long-and Short-Term User
    Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized
    Image Caption,” in *AAAI*, 2020.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] C. Gan, Z. Gan, X. He, J. Gao, and L. Deng, “StyleNet: Generating Attractive
    Visual Captions with Styles,” in *CVPR*, 2017.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] A. Mathews, L. Xie, and X. He, “Semstyle: Learning to generate stylised
    image captions using unaligned text,” in *CVPR*, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] L. Guo, J. Liu, P. Yao, J. Li, and H. Lu, “Mscap: Multi-style image captioning
    with unpaired stylized text,” in *CVPR*, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] W. Zhao, X. Wu, and X. Zhang, “MemCap: Memorizing style knowledge for
    image captioning,” in *AAAI*, 2020.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] K. Shuster, S. Humeau, H. Hu, A. Bordes, and J. Weston, “Engaging image
    captioning via personality,” in *CVPR*, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Y. Zheng, Y. Li, and S. Wang, “Intention oriented image captions with
    guiding objects,” in *CVPR*, 2019.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Z. Meng, L. Yu, N. Zhang, T. Berg, B. Damavandi, V. Singh, and A. Bearman,
    “Connecting What to Say With Where to Look by Modeling Human Attention Traces,”
    in *CVPR*, 2021.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] L. Chen, Z. Jiang, J. Xiao, and W. Liu, “Human-like Controllable Image
    Captioning with Verb-specific Semantic Roles,” in *CVPR*, 2021.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] S. Chen, Q. Jin, P. Wang, and Q. Wu, “Say as you wish: Fine-grained control
    of image caption generation with abstract scene graphs,” in *CVPR*, 2020.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Y. Zhong, L. Wang, J. Chen, D. Yu, and Y. Li, “Comprehensive image captioning
    via scene graph decomposition,” in *ECCV*, 2020.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] C. Deng, N. Ding, M. Tan, and Q. Wu, “Length-controllable image captioning,”
    in *ECCV*, 2020.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] F. Sammani and L. Melas-Kyriazi, “Show, edit and tell: A framework for
    editing image captions,” in *CVPR*, 2020.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] M. Hodosh and J. Hockenmaier, “Focused evaluation for image description
    with binary forced-choice tasks,” in *ACL Workshops*, 2016.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] H. Xie, T. Sherborne, A. Kuhnle, and A. Copestake, “Going beneath the
    surface: Evaluating image captioning for grammaticality, truthfulness and diversity,”
    *arXiv preprint arXiv:1912.08960*, 2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] M. Alikhani, P. Sharma, S. Li, R. Soricut, and M. Stone, “Cross-modal
    Coherence Modeling for Caption Generation,” in *ACL*, 2020.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and
    J. Liu, “UNITER: UNiversal Image-TExt Representation Learning,” in *ECCV*, 2020.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep
    networks,” in *ICML*, 2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/f7c1091f8d857fd0453bf5934777cf29.png) | Matteo
    Stefanini received the M.Sc. degree in Computer Engineering cum laude from the
    University of Modena and Reggio Emilia, in 2018\. He is currently pursuing a PhD
    degree in Information and Communication Technologies at the Department of Engineering
    “Enzo Ferrari”, University of Modena and Reggio Emilia. His research activities
    involve the integration of vision and language modalities, focusing on image captioning
    and Transformer-based architectures. |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/5a51e3e0f8a1c3f72c9fd9970e0fa74b.png) | Marcella
    Cornia received the M.Sc. degree in Computer Engineering and the Ph.D. degree
    cum laude in Information and Communication Technologies from the University of
    Modena and Reggio Emilia, in 2016 and 2020, respectively. She is currently a Postdoctoral
    Researcher with the University of Modena and Reggio Emilia. She has authored or
    coauthored more than 30 publications in scientific journals and international
    conference proceedings. |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2d5e1859fc24b5d41972939ce78065af.png) | Lorenzo
    Baraldi received the M.Sc. degree in Computer Engineering and the Ph.D. degree
    cum laude in Information and Communication Technologies from the University of
    Modena and Reggio Emilia, in 2014 and 2018\. He is currently Tenure Track Assistant
    Professor with the University of Modena and Reggio Emilia. He was a Research Intern
    at Facebook AI Research (FAIR) in 2017\. He has authored or coauthored more than
    70 publications in scientific journals and international conference proceedings.
    |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/56e3a89c52c9911ae11a531802ec8275.png) | Silvia
    Cascianelli received the M.Sc. degree in Information and Automation Engineering
    and the Ph.D. degree cum laude in Information and Industrial Engineering from
    the University of Perugia, in 2015 and 2019, respectively. She is an Assistant
    Professor with the University of Modena and Reggio Emilia. She was a Visitor Researcher
    at the Queen Mary University of London in 2018\. She has authored or coauthored
    more than 30 publications in scientific journals and international conference
    proceedings. |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/a57415e3de38de64f8bd3a8371108825.png) | Giuseppe
    Fiameni is a Data Scientist at NVIDIA where he oversees the NVIDIA AI Technology
    Centre in Italy, a collaboration among NVIDIA, CINI and CINECA to accelerate academic
    research in the field of AI. He has been working as HPC specialist at CINECA,
    the largest HPC facility in Italy, for more than 14 years providing support for
    large-scale data analytics workloads. Research interests include large scale deep
    learning models, system architectures, massive data engineering, video action
    detection. |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/eb0b46f0394bbe0815488621cb90d107.png) | Rita Cucchiara
    received the M.Sc. degree in Electronics Engineering and the Ph.D. degree in Computer
    Engineering from the University of Bologna, in 1989 and 1992\. She is currently
    Full Professor with the University of Modena and Reggio Emilia, where she heads
    the AImageLab Laboratory. She has authored or coauthored more than 400 papers
    in journals and international proceedings, and has been a coordinator of several
    projects in computer vision. She is Member of the Advisory Board of the Computer
    Vision Foundation, and Director of the ELLIS Unit of Modena. |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
- en: Appendix A Further analysis of the evaluation metrics
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we extend our analysis of the evaluation metrics for image
    captioning. In particular, in Table [III](#A1.T3 "TABLE III ‣ Appendix A Further
    analysis of the evaluation metrics ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), we provide a taxonomy and summarize the main characteristics
    of the metrics presented. Moreover, in the following, we describe in more detail
    additional diversity metrics, embedding-based metrics, and learning-based metrics,
    which were only mentioned in the main paper.'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: 'Diversity metrics. Local Diversity can be quantified via the average BLEU score
    between each caption and the others (mBLEU: the lower the mBLEU, the more diverse
    the produced caption set is) [[154](#bib.bib154)]. Another approach to quantify
    captions diversity is to focus on the mention of all the relevant words in the
    produced caption, despite their rareness in the training set. In this respect,
    in [[155](#bib.bib155)], two recall-based diversity metrics have been proposed,
    namely Global Recall and Local Recall. The former is computed as the fraction
    of generated words with respect to the words appearing in both the training and
    validation set. The latter is computed for each test image as the fraction of
    generated words with respect to the words in the reference captions. Moreover,
    when the system can produce multiple captions for the same image, diversity can
    be quantified at the topics level by using the Latent Semantic Analysis-based
    metric LSA and the kernelized version of the CIDEr score Self-CIDEr, proposed
    in [[156](#bib.bib156), [157](#bib.bib157)].'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: Embedding-based metrics. Among the specific aspects of a produced caption that
    can e evaluated via embedding-based metrics, the hallucination rate is measured
    via the CHAIR score [[158](#bib.bib158)], which is expressed as the fraction of
    hallucinated objects among those mentioned in a caption (in the CHAIR${}_{\text{i}}$
    variant) or the fraction of captions with at least a hallucinated object among
    all the produced captions (in the CHAIR${}_{\text{s}}$ variant). Other aspects
    that can be measured by exploiting embedding-based representation of candidate
    caption, reference captions, and image are Relevance (via cosine similarity),
    Extraness, and Omission (via orthogonal projections), as proposed in [[159](#bib.bib159)].
    Moreover, to take into account the uniqueness of the generated captions, in [[160](#bib.bib160)]
    the SPICE-U score is proposed as the harmonic mean of the SPICE score and a measure
    of the caption uniqueness, which considers the fraction of images in the training
    set not containing the mentioned concepts.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: Learning-based metrics. With respect to learning-based evaluation scores, NNEval [[151](#bib.bib151)]
    was proposed as the first learning-based image captioning evaluation approach.
    It considers classical metrics (BLEU, METEOR, CIDEr, SPICE, and WMD) as features
    describing the candidate caption when compared to reference captions and predicts
    its probability of being human-generated. Another early-proposed learning-based
    evaluation strategy is LEIC [[152](#bib.bib152)], which directly scores the probability
    of a caption being human-generated, conditioned on the image and eventually on
    a reference caption, by using a binary classifier fed with pre-trained ResNet
    image features and an LSTM-based encoding of the caption. As a refinement of BERT-S
    specific for image captioning, the ViLBERT-S [[165](#bib.bib165)] exploits the
    image-conditioned embedding obtained from the vision-and-language representation
    model ViLBERT [[99](#bib.bib99)]. Similar to the BERT-S, the matching between
    these tokens is expressed via the cosine similarity of their embeddings, and the
    best matching token pairs are used for computing precision, recall, and F1-score.
    Another variant of the BERT-S, to which we here refer to as BERT-S${}^{\text{IRV}}$ [[166](#bib.bib166)],
    takes into account the variability of the reference captions associated with the
    same image by combining them in a unique embedding vector that contains all the
    mentioned concepts and against which the candidate caption is compared. To evaluate
    the candidate caption fidelity and adequacy, the FAIEr [[167](#bib.bib167)] score
    exploits scene graphs matching. The references and the image scene graphs are
    fused in a unique scene graph, whose more relevant nodes (representing the concepts
    more often mentioned in the references) get more weight, and the score is obtained
    based on the similarity between the candidate scene graph and the unique scene
    graph. On a similar line, the UMIC score [[168](#bib.bib168)] exploits the pre-trained
    vision-and-language representation model UNITER [[254](#bib.bib254)], fine-tuned
    on carefully-designed negative samples, to score a candidate caption without the
    need for reference captions.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Taxonomy and main characteristics of image captioning metrics.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  | Inputs |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | Original Task | Pred | Refs | Image |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
- en: '|  |  | BLEU [[117](#bib.bib117)] | Translation | ✓ | ✓ |  |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
- en: '|  |  | METEOR [[150](#bib.bib150)] | Translation | ✓ | ✓ |  |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
- en: '| Standard |  | ROUGE [[118](#bib.bib118)] | Summarization | ✓ | ✓ |  |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
- en: '|  |  | CIDEr [[122](#bib.bib122)] | Captioning | ✓ | ✓ |  |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
- en: '|  |  | SPICE [[121](#bib.bib121)] | Captioning | ✓ | (✓) | (✓) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
- en: '|  |  | Div [[154](#bib.bib154)] | Captioning | ✓ |  |  |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
- en: '|  |  | Vocab [[154](#bib.bib154)] | Captioning | ✓ |  |  |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
- en: '|  |  | %Novel [[154](#bib.bib154)] | Captioning | ✓ |  |  |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
- en: '| Diversity |  | mBLEU [[154](#bib.bib154)] | Captioning | ✓ |  |  |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
- en: '|  |  | LSA [[156](#bib.bib156), [157](#bib.bib157)] | Captioning | ✓ |  |  |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
- en: '|  |  | Self-CIDEr [[156](#bib.bib156), [157](#bib.bib157)] | Captioning |
    ✓ |  |  |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
- en: '|  |  | Recall [[155](#bib.bib155)] | Captioning | ✓ | (✓) |  |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
- en: '|  |  | WMD [[161](#bib.bib161)] | Doc. Dissimilarity | ✓ | ✓ |  |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
- en: '|  |  | Alignment [[163](#bib.bib163)] | Captioning | ✓ | ✓ |  |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
- en: '|  |  | Coverage [[163](#bib.bib163), [164](#bib.bib164)] | Captioning | ✓
    | (✓) | (✓) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
- en: '|  |  | Relevance [[159](#bib.bib159)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
- en: '|  |  | Extraness [[159](#bib.bib159)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: '|  |  | Omission [[159](#bib.bib159)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
- en: '|  |  | SPICE-U [[160](#bib.bib160)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
- en: '| Embedding-based |  | CHAIR [[158](#bib.bib158)] | Captioning | ✓ | ✓ | ✓
    |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '|  |  | NNEval [[151](#bib.bib151)] | Captioning | ✓ | ✓ |  |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '|  |  | BERT-S [[171](#bib.bib171)] | Text Similarity | ✓ | ✓ |  |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
- en: '|  |  | BERT-S${}^{\text{IRV}}$ [[166](#bib.bib166)] | Captioning | ✓ | ✓ |  |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
- en: '|  |  | UMIC [[168](#bib.bib168)] | Captioning | ✓ |  | ✓ |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
- en: '| Learning-based |  | LEIC [[152](#bib.bib152)] | Captioning | ✓ | (✓) | ✓
    |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
- en: '|  |  | CLIP-S [[175](#bib.bib175)] | Captioning | ✓ | (✓) | ✓ |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
- en: '|  |  | TIGEr [[173](#bib.bib173)] | Captioning | ✓ | ✓ | ✓ |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
- en: '|  |  | ViLBERT-S [[165](#bib.bib165)] | Captioning | ✓ | ✓ | ✓ |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '|  |  | FAIEr [[167](#bib.bib167)] | Captioning | ✓ | ✓ | ✓ |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: Appendix B Further Performance Analysis
  id: totrans-571
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'According to the taxonomies proposed in Sections [2](#S2 "2 Visual Encoding
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning"), [3](#S3
    "3 Language Models ‣ From Show to Tell: A Survey on Deep Learning-based Image
    Captioning"), and [4](#S4 "4 Training Strategies ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning"), in Table [IV](#A2.T4 "TABLE IV ‣ Appendix
    B Further Performance Analysis ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"), we overview the most relevant surveyed methods. We report
    their performance in terms of BLEU-4, METEOR, and CIDEr on the COCO Karpathy test
    set and their main features in terms of visual encoding, language modeling, and
    training strategies. In the table, methods are clustered based primarily on their
    visual encoding strategy and ordered based on the obtained scores. Methods exploiting
    vision-and-language pre-training are further separated from the others. Image
    captioning models have reached impressive performance in just a few years: from
    an average BLEU-4 of 25.1 for the methods using global CNN features to an average
    BLEU-4 of 35.3 and 40.0 for those exploiting the attention and self-attention
    mechanisms, peaking at 42.6 in the case of vision-and-language pre-training. By
    looking at the performance in terms of the CIDEr score, we can notice that, as
    for the visual encoding, the more complete and structured information about semantic
    visual concepts and their mutual relation is included, the better is the performance
    (consider that methods applying attention over a grid of features reach an average
    CIDEr score of 105.8, while those performing attention over visual regions 121.8,
    further increased for graph-based approaches and methods using self-attention,
    which reach 133.2 on average). As for the language model, LSTM-based approaches
    combined with strong visual encoders are still competitive with subsequent fully-attentive
    methods in terms of performance. These methods are slower to train but are generally
    smaller than Transformer-based ones. As for the training strategy, sentence-level
    fine-tuning with reinforcement learning leads to significant performance improvement
    (consider that methods relying only on the cross-entropy loss obtain an average
    CIDEr score of 92.3, while those combining it with reinforcement learning fine-tuning
    reach 125.1 on average). Moreover, it emerges that vision-and-language pre-training
    on large datasets allows boosting the performance and deserves further investigation
    (with an average CIDEr score of 140.4).'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness, in Fig. [10](#A2.F10 "Figure 10 ‣ Appendix B Further Performance
    Analysis ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")
    we report the relation between the CIDEr score and all the other characteristics
    from Table [II](#S5.T2 "TABLE II ‣ 5.2.1 Standard evaluation metrics ‣ 5.2 Evaluation
    Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning"). The almost-linear relation with the CIDEr score is observable
    also for the scores not reported in Fig. [9](#S5.F9 "Figure 9 ‣ 5.2.4 Learning-based
    evaluation ‣ 5.2 Evaluation Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell:
    A Survey on Deep Learning-based Image Captioning") in the main paper, with the
    only exception of the BERT-S score.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Overview of deep learning-based image captioning models. Scores are
    taken from the respective papers. For all the metrics, the higher the value, the
    better ($\uparrow$).'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | Visual Encoding |  |  | Language Model |  |  | Training Strategies
    |  |  | Main Results |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| Model |  |  | Global | Grid | Regions | Graph | Self-Attention |  |  | RNN/LSTM
    | Transformer | BERT |  |  | XE | MLM | Reinforce | VL Pre-Training |  |  | BLEU-4
    | METEOR | CIDEr |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
- en: '| LEMON [[104](#bib.bib104)] |  |  |  |  | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |  |
    ✓ | ✓ | ✓ |  |  | 42.6 | 31.4 | 145.5 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
- en: '| UniversalCap [[97](#bib.bib97)] |  |  |  | ✓ |  |  | ✓ |  |  |  | ✓ |  |  |  |
    ✓ |  | ✓ | ✓ |  |  | 40.8 | 30.4 | 143.4 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
- en: '| SimVLM [[94](#bib.bib94)] |  |  |  |  |  |  | ✓ |  |  |  | ✓ |  |  |  | ✓
    |  |  | ✓ |  |  | 40.6 | 33.7 | 143.3 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
- en: '| VinVL [[103](#bib.bib103)] |  |  |  |  | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |  |
    ✓ | ✓ | ✓ |  |  | 41.0 | 31.1 | 140.9 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
- en: '| Oscar [[100](#bib.bib100)] |  |  |  |  | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |  |
    ✓ | ✓ | ✓ |  |  | 41.7 | 30.6 | 140.0 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
- en: '| Unified VLP [[101](#bib.bib101)] |  |  |  |  | ✓ |  | ✓ |  |  |  |  | ✓ |  |  |
    ✓ |  | ✓ | ✓ |  |  | 39.5 | 29.3 | 129.3 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
- en: '| AutoCaption [[107](#bib.bib107)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 40.2 | 29.9 | 135.8 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
- en: '| RSTNet [[89](#bib.bib89)] |  |  |  | ✓ |  |  | ✓ |  |  |  | ✓ |  |  |  |
    ✓ |  | ✓ |  |  |  | 40.1 | 29.8 | 135.6 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
- en: '| DLCT [[86](#bib.bib86)] |  |  |  | ✓ | ✓ |  | ✓ |  |  |  | ✓ |  |  |  | ✓
    |  | ✓ |  |  |  | 39.8 | 29.5 | 133.8 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
- en: '| DPA [[83](#bib.bib83)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 40.5 | 29.6 | 133.4 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
- en: '| X-Transformer [[80](#bib.bib80)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  |
    ✓ |  | ✓ |  |  |  | 39.7 | 29.5 | 132.8 |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
- en: '| NG-SAN [[78](#bib.bib78)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  |
    ✓ |  | ✓ |  |  |  | 39.9 | 29.3 | 132.1 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
- en: '| X-LAN [[80](#bib.bib80)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 39.5 | 29.5 | 132.0 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
- en: '| GET [[85](#bib.bib85)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |  | 39.5 | 29.3 | 131.6 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  |  |  |  | ✓ |  | ✓ |  |  |  |
    ✓ |  |  |  | ✓ |  | ✓ |  |  |  | 39.1 | 29.2 | 131.2 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
- en: '| AoANet [[79](#bib.bib79)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 38.9 | 29.2 | 129.8 |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
- en: '| CPTR [[92](#bib.bib92)] |  |  |  |  |  |  | ✓ |  |  |  | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |  | 40.0 | 29.1 | 129.4 |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
- en: '| ORT [[77](#bib.bib77)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.6 | 28.7 | 128.3 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
- en: '| CNM [[75](#bib.bib75)] |  |  |  |  | ✓ |  | ✓ |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.9 | 28.4 | 127.9 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
- en: '| ETA [[76](#bib.bib76)] |  |  |  |  | ✓ |  | ✓ |  |  |  | ✓ |  |  |  | ✓ |  |
    ✓ |  |  |  | 39.9 | 28.9 | 127.6 |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
- en: '| GCN-LSTM+HIP [[73](#bib.bib73)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 39.1 | 28.9 | 130.6 |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
- en: '| MT [[72](#bib.bib72)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.9 | 28.8 | 129.6 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
- en: '| SGAE [[71](#bib.bib71)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 39.0 | 28.4 | 129.1 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
- en: '| GCN-LSTM [[68](#bib.bib68)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 38.3 | 28.6 | 128.7 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
- en: '| VSUA [[69](#bib.bib69)] |  |  |  |  | ✓ | ✓ |  |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 38.4 | 28.5 | 128.6 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
- en: '| SG-RWS [[65](#bib.bib65)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 38.5 | 28.7 | 129.1 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
- en: '| LBPF [[63](#bib.bib63)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.3 | 28.5 | 127.6 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
- en: '| AAT [[64](#bib.bib64)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.2 | 28.3 | 126.7 |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
- en: '| CAVP [[66](#bib.bib66)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓ |  |
    ✓ |  |  |  | 38.6 | 28.3 | 126.3 |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 36.3 | 27.7 | 120.1 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
- en: '| RDN [[62](#bib.bib62)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |  |
    36.8 | 27.2 | 115.3 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
- en: '| Neural Baby Talk [[106](#bib.bib106)] |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 34.7 | 27.1 | 107.2 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
- en: '| Stack-Cap [[49](#bib.bib49)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 36.1 | 27.4 | 120.4 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
- en: '| MaBi-LSTM [[48](#bib.bib48)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 36.8 | 28.1 | 116.6 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
- en: '| RFNet [[52](#bib.bib52)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  | ✓ |  |  |  | 35.8 | 27.4 | 112.5 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
- en: '| SCST (Att2in) [[38](#bib.bib38)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 33.3 | 26.3 | 111.4 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
- en: '| Adaptive Attention [[43](#bib.bib43)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 33.2 | 26.6 | 108.5 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
- en: '| Skeleton [[47](#bib.bib47)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 33.6 | 26.8 | 107.3 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
- en: '| ARNet [[46](#bib.bib46)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 33.5 | 26.1 | 103.4 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
- en: '| SCA-CNN [[51](#bib.bib51)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 31.1 | 25.0 | 95.2 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
- en: '| Areas of Attention [[67](#bib.bib67)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 30.7 | 24.5 | 93.8 |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
- en: '| Review Net [[50](#bib.bib50)] |  |  |  | ✓ |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 29.0 | 23.7 | 88.6 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
- en: '| Show, Attend and Tell [[42](#bib.bib42)] |  |  |  | ✓ |  |  |  |  |  | ✓
    |  |  |  |  | ✓ |  |  |  |  |  | 24.3 | 23.9 | - |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
- en: '| SCST (FC) [[38](#bib.bib38)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 31.9 | 25.5 | 106.3 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
- en: '| PG-SPIDEr [[120](#bib.bib120)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 33.2 | 25.7 | 101.3 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
- en: '| SCN-LSTM [[41](#bib.bib41)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 33.0 | 25.7 | 101.2 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
- en: '| LSTM-A [[40](#bib.bib40)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 32.6 | 25.4 | 100.2 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
- en: '| CNN[L]+RNH [[35](#bib.bib35)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 30.6 | 25.2 | 98.9 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: '| Att-CNN+LSTM [[34](#bib.bib34)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 31.0 | 26.0 | 94.0 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
- en: '| GroupCap [[37](#bib.bib37)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 33.0 | 26.0 | - |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
- en: '| StructCap [[36](#bib.bib36)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 32.9 | 25.4 | - |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
- en: '| Embedding Reward [[119](#bib.bib119)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 30.4 | 25.1 | 93.7 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
- en: '| ATT-FCN [[33](#bib.bib33)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 30.4 | 24.3 | - |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
- en: '| MIXER [[115](#bib.bib115)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  | ✓ |  |  |  | 29.0 | - | - |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
- en: '| MSR [[31](#bib.bib31)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |  |
    25.7 | 23.6 | - |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
- en: '| gLSTM [[32](#bib.bib32)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 26.4 | 22.7 | 81.3 |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
- en: '| m-RNN [[27](#bib.bib27)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 25.0 | - | - |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
- en: '| Show and Tell [[23](#bib.bib23)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 24.6 | - | - |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
- en: '| Mind’s Eye [[30](#bib.bib30)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  |
    ✓ |  |  |  |  |  | 19.0 | 20.4 | - |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
- en: '| DeepVS [[25](#bib.bib25)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓
    |  |  |  |  |  | 23.0 | 19.5 | 66.0 |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
- en: '| LRCN [[28](#bib.bib28)] |  |  | ✓ |  |  |  |  |  |  | ✓ |  |  |  |  | ✓ |  |  |  |  |  |
    21.0 | - | - | ![Refer to caption](img/db0988b53691211608e49129ec9bd823.png)'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Relationship between CIDEr, number of parameters and other scores.
    Values of Div-1, Div-2, Alignment, CLIP-S, and CLIP-S${}^{\text{Ref}}$ are multiplied
    by powers of 10 for readability.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: 'We deepen our analysis by considering also the Flickr30K dataset, and evaluate
    the performance of methods trained on COCO, and tested on the test set of Flickr30K,
    and of methods developed for COCO, but trained and tested on Flickr30K. The results
    of this study are reported in Table [V](#A2.T5 "TABLE V ‣ Appendix B Further Performance
    Analysis ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning").
    Compared to what is obtained by the same models when trained and tested on COCO
    (in Table [II](#S5.T2 "TABLE II ‣ 5.2.1 Standard evaluation metrics ‣ 5.2 Evaluation
    Metrics ‣ 5 Evaluation Protocol ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")), the standard metrics and the embedding-based metrics significantly
    drop while diversity metrics increase. This can be imputed to the smaller size
    of Flickr30K compared to COCO. Learning-based metrics, especially BERT-S and CLIP-S,
    are more stable. As expected, training on COCO and directly testing on Flickr30K
    results in a performance drop under all the metrics when compared with the case
    in which both training and test are performed on data from Flickr30K. This confirms
    that generalization is still an open issue for image captioning approaches. Interestingly,
    VinVL surpasses the other considered approaches under all metrics in the case
    of the test on Flickr30K, suggesting the benefits of pre-training in terms of
    generalization capabilities of the resulting model.'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: 'For a deeper qualitative analysis, in Fig. [11](#A2.F11 "Figure 11 ‣ Appendix
    B Further Performance Analysis ‣ From Show to Tell: A Survey on Deep Learning-based
    Image Captioning")-[12](#A2.F12 "Figure 12 ‣ Appendix B Further Performance Analysis
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning"), we report
    examples of captions generated by eight popular approaches on images from the
    COCO dataset. It can be observed that the produced captions have similar length
    and structure (*i.e.* the main subject is mentioned first, then the main action,
    and finally additional details concerning other visual entities in the scene).
    This mimics the characteristics of the majority of the ground-truth captions in
    COCO. Another aspect that emerges is the lack of counting capabilities (consider
    *e.g.* the first example in the second row and the second example in the bottom
    row of Fig. [12](#A2.F12 "Figure 12 ‣ Appendix B Further Performance Analysis
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning")). Current
    approaches struggle in mentioning the right amount of instances of the same entities
    and generally refer to multiple instances as a group of. Finally, it is worth
    mentioning the difficulty in describing unusual concepts, both situations and
    visual entities (consider *e.g.* the first two examples in Fig. [11](#A2.F11 "Figure
    11 ‣ Appendix B Further Performance Analysis ‣ From Show to Tell: A Survey on
    Deep Learning-based Image Captioning")), which is a symptom of the lack of generalization
    capability. In fact, in such cases, unusual concepts are described as more represented
    concepts in the training set. For example, the ferret in the top-right of Fig. [11](#A2.F11
    "Figure 11 ‣ Appendix B Further Performance Analysis ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning") is described as a cat or a mouse, and
    the historically-dressed man in the last example of the second row of Fig. [12](#A2.F12
    "Figure 12 ‣ Appendix B Further Performance Analysis ‣ From Show to Tell: A Survey
    on Deep Learning-based Image Captioning") is described as a woman. This issue
    is less evident for VinVL, enforcing the role of pre-training to achieve better
    generalization capabilities.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in Fig. [13](#A2.F13 "Figure 13 ‣ Appendix B Further Performance Analysis
    ‣ From Show to Tell: A Survey on Deep Learning-based Image Captioning"), we report
    a visualization of the attention states corresponding to the captions generated
    by two methods based on image regions, *i.e.* Up-Down [[58](#bib.bib58)], which
    performs attention over image regions, and $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)],
    which performs self-attention encoding. For visualization, we use the normalized
    attribution scores obtained for each image region via the Integrated Gradients
    approach [[255](#bib.bib255)] and projected between 0 and 1 by applying a contrast
    stretching function. In particular, we show the attended regions for each generated
    word and outline the region with the highest attribution score. With a focus on
    visual words, it can be observed that, for both approaches, the regions with the
    highest scores are coherent with the produced word. However, thanks to the more
    complex relations modeled via self-attention, $\mathcal{M}^{2}$ Transformer generally
    pays more attention to fewer, more precise regions compared to Up-Down (consider
    *e.g.* the region contributing the most to outputting tracks in the third example,
    or skateboard in the last one).'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Performance analysis of representative image captioning approaches
    in terms of different evaluation metrics on the Flickr30K datatset. The $\dagger$
    marker indicates models trained by us with ResNet-152 features, while the $\ddagger$
    marker indicates unofficial implementations. For all the metrics, the higher the
    value, the better ($\uparrow$).'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  | Standard Metrics |  |  | Diversity Metrics |  |  | Embedding-based
    Metrics |  |  | Learning-based Metrics |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
- en: '|  |  | Trained on |  | B-1 | B-4 | M | R | C | S |  |  | Div-1 | Div-2 | Vocab
    | %Novel |  |  | WMD | Alignment | Coverage |  |  | TIGEr | BERT-S | CLIP-S |
    CLIP-S${}^{\text{Ref}}$ |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
- en: '| Show and Tell^† [[23](#bib.bib23)] |  | COCO |  | 51.0 | 11.4 | 13.1 | 34.8
    | 22.8 | 7.6 |  |  | 0.037 | 0.093 | 331 | 94.8 |  |  | 8.6 | 0.019 | 61.9 |  |  |
    52.9 | 90.6 | 0.604 | 0.656 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
- en: '| Show, Attend and Tell^† [[42](#bib.bib42)] |  | COCO |  | 57.3 | 14.7 | 15.1
    | 38.8 | 29.4 | 9.4 |  |  | 0.044 | 0.124 | 402 | 96.3 |  |  | 9.5 | 0.053 | 63.7
    |  |  | 53.0 | 91.1 | 0.638 | 0.686 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
- en: '| Up-Down^‡ [[58](#bib.bib58)] |  | COCO |  | 65.5 | 19.5 | 18.6 | 44.0 | 42.6
    | 12.5 |  |  | 0.047 | 0.131 | 463 | 98.1 |  |  | 11.1 | 0.105 | 68.9 |  |  |
    53.6 | 91.9 | 0.682 | 0.719 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  | COCO |  | 67.9 | 21.0
    | 19.4 | 45.3 | 47.4 | 13.0 |  |  | 0.048 | 0.150 | 470 | 98.9 |  |  | 11.7 |
    0.106 | 67.0 |  |  | 53.7 | 91.8 | 0.680 | 0.721 |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
- en: '| VinVL [[103](#bib.bib103)] |  | COCO |  | 74.3 | 28.4 | 23.5 | 51.1 | 75.2
    | 16.8 |  |  | 0.066 | 0.188 | 651 | 98.8 |  |  | 15.0 | 0.147 | 72.2 |  |  |
    53.6 | 93.1 | 0.754 | 0.787 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
- en: '| Show and Tell^† [[23](#bib.bib23)] |  | Flickr30K |  | 64.1 | 21.5 | 18.3
    | 44.4 | 41.7 | 12.2 |  |  | 0.037 | 0.075 | 373 | 84.5 |  |  | 11.2 | 0.090 |
    64.2 |  |  | 53.5 | 92.1 | 0.658 | 0.701 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
- en: '| Show, Attend and Tell^† [[42](#bib.bib42)] |  | Flickr30K |  | 65.6 | 23.6
    | 19.2 | 45.4 | 49.1 | 13.3 |  |  | 0.045 | 0.096 | 454 | 90.1 |  |  | 11.8 |
    0.089 | 64.1 |  |  | 53.4 | 92.1 | 0.679 | 0.717 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
- en: '| Up-Down^‡ [[58](#bib.bib58)] |  | Flickr30K |  | 72.4 | 28.3 | 21.6 | 49.5
    | 63.3 | 15.9 |  |  | 0.061 | 0.155 | 587 | 95.6 |  |  | 13.1 | 0.119 | 65.5 |  |  |
    53.5 | 92.7 | 0.720 | 0.755 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
- en: '| ORT [[77](#bib.bib77)] |  | Flickr30K |  | 72.2 | 30.1 | 22.8 | 50.4 | 68.8
    | 16.9 |  |  | 0.072 | 0.171 | 738 | 96.1 |  |  | 13.7 | 0.129 | 67.2 |  |  |
    53.5 | 92.7 | 0.728 | 0.760 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)] |  | Flickr30K |  | 72.4
    | 29.8 | 22.4 | 50.6 | 68.4 | 16.2 |  |  | 0.079 | 0.196 | 728 | 93.8 |  |  |
    13.6 | 0.120 | 65.0 |  |  | 53.6 | 92.9 | 0.724 | 0.763 |  | ![Refer to caption](img/ed00a182cb129d9e124d7b9c96017455.png)
    |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/9bbd181eec893a0f164e5aaf4abea649.png) |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/6b15613efa7e851545893e1c02aa5c9f.png) |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
- en: 'Figure 11: Additional qualitative examples from eight popular captioning models
    on COCO test images.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/fbf9005e6823e993a64922abba6e6f6c.png) |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/535215756c59a41fd863801c4dffc9b1.png) |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/659d95a3ea0101fcc8569f8069cd0d1b.png) |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
- en: 'Figure 12: Additional qualitative examples from eight popular captioning models
    on COCO test images.'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/c7fa9ce5db800d51584cb543df85dd53.png) | ![Refer to
    caption](img/e400654d5e9d23176d3642d428b826b6.png) | ![Refer to caption](img/c831120ff2413cbdb0c31ea6634cabf8.png)
    | ![Refer to caption](img/739879922d4f6349ddc7a976ac9a9617.png) | ![Refer to caption](img/a9e484b17e063e59337a4cb1552f86c3.png)
    | ![Refer to caption](img/ebf924b0b873c17872c55c349b90bc15.png) | ![Refer to caption](img/fea81ffea1ac777286a454aab3f818a0.png)
    | ![Refer to caption](img/f848d6f72107fa3619514d29057623f6.png) |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/2c38e04c8d737b0d3ce5cab205c39bd4.png) | ![Refer to
    caption](img/fb483a68e03624a00fd505dbbee529f9.png) | ![Refer to caption](img/dfdff53c973a2d1e8c9a58956db2f16c.png)
    | ![Refer to caption](img/cb5b3e1919f00befd01ea83432e305cd.png) | ![Refer to caption](img/021a8ed94a7c4d04266c7497fbb7c7c0.png)
    | ![Refer to caption](img/c2be0a7385e2aa3fe0480b556c4ec9e4.png) | ![Refer to caption](img/0d08df5adf8c2af3f9fcda63dd379e66.png)
    | ![Refer to caption](img/df5713a89f0bc30ccbe3fd29ef428f04.png) |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/405354936bb2459ce0de07fc9e6b6c6c.png) | ![Refer to
    caption](img/a8a59af0603a4431e7cc26789fd4aeac.png) | ![Refer to caption](img/69b8dc60f47b38ee53d1871a2b6cc3b6.png)
    | ![Refer to caption](img/34e59b84634a083373f7c6c10f9ce0fd.png) | ![Refer to caption](img/3151f99d6833d4e6a9f932b2bc4ac8d3.png)
    | ![Refer to caption](img/8f5b2e3962bcf7ee761922417c35331b.png) | ![Refer to caption](img/f84878d99a2cff3455f106bd14a4c2af.png)
    | ![Refer to caption](img/4d9f52b9d25947539a7a5007ba67dcc0.png) |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/4fa3858c1a50b7393a16b751f6f78a59.png) | ![Refer to
    caption](img/36c0e3128d1d41805eb7af606f9a80e7.png) | ![Refer to caption](img/411b81e26cb40e4cc576ccc89eefa313.png)
    | ![Refer to caption](img/6b5fdcc586ef179d6122291d9b4461f8.png) | ![Refer to caption](img/0445a791ce92a6f917e535e94a39bda4.png)
    | ![Refer to caption](img/c0040e66a8e090507a6403ac352d2626.png) | ![Refer to caption](img/9f18e7bc3750a76bfdfbf792e05ae338.png)
    | ![Refer to caption](img/d774c5b04d07e33b60205eb016efa20a.png) |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/7a498e0d184a07525434c12d5bbf7937.png) | ![Refer to
    caption](img/6797e0fbd1c90afd6e59ad0e5476575e.png) | ![Refer to caption](img/272955280f29879fbd757b464d634b34.png)
    | ![Refer to caption](img/97347f62805bbf804955b39a037cf48d.png) | ![Refer to caption](img/5813006be150f20be1e007c08bdecc4a.png)
    | ![Refer to caption](img/f6035e2d1bb1d938351eeba06fb437de.png) | ![Refer to caption](img/4c715c7b54290b92030b6151c2f89a09.png)
    | ![Refer to caption](img/a561c3a1094cb3cf89f8c7489d3f120e.png) |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/b55e8f14e3701a6d02a125c595a4dfd6.png) | ![Refer to
    caption](img/1e7469d65e4e74efb5184e256599e1e5.png) | ![Refer to caption](img/8d6dc6b42d8c8f661756958ea4ff43fd.png)
    | ![Refer to caption](img/0f3953be61456b3d18858b62bd8f2806.png) | ![Refer to caption](img/664c6f4fe4dbb0d500792d473616d34b.png)
    | ![Refer to caption](img/5a4f3483e75755713678ff9a703eb23b.png) | ![Refer to caption](img/736b5f824ad516140040d99870d18422.png)
    | ![Refer to caption](img/fb9ea405cbf8ee093d20778996e8f3b4.png) |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/a25f13b723aff1f9962b25cfaf0b8d3c.png) | ![Refer to
    caption](img/71c63faf102858cf37d08979979604bc.png) | ![Refer to caption](img/844e6324a83883662ae09aa5f316ff63.png)
    | ![Refer to caption](img/c41a4dc61e5a8f7bd7de3a788e0908ac.png) | ![Refer to caption](img/2bbb9a2c76fd04ce46297fe987ae9740.png)
    | ![Refer to caption](img/6e92cf8aaa89e4c7bb1588586bbb0fb1.png) | ![Refer to caption](img/e5c34c69046f4dc70441aea25575b129.png)
    | ![Refer to caption](img/4180b0d1cb82a5c27d9d159e830525c2.png) |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/8610e30d9a218317375a614675415e81.png) | ![Refer to
    caption](img/a2e854b7544a8eae79e9c16e92ab675b.png) | ![Refer to caption](img/5241838e6979fbfc3382a99bf594d655.png)
    | ![Refer to caption](img/4445fc2c60da808e4e1ae157ed48d889.png) | ![Refer to caption](img/6d2480dcc33367274a2f86d4da803a1b.png)
    | ![Refer to caption](img/6f18b3d914899ff733c3ad31d3459d38.png) | ![Refer to caption](img/bfbb35ca971bf5def1de9cdaec727a8f.png)
    | ![Refer to caption](img/4ae471c51f2b6169965d60700924ffa8.png) |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
- en: '| Up-Down [[58](#bib.bib58)] |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/0cb0cb5640f2a3dde73db95850ffe9f1.png) | ![Refer to
    caption](img/14cad532146e889afef62c2c7f41fe4e.png) | ![Refer to caption](img/c302d520e15c1bb33538735e85712c5d.png)
    | ![Refer to caption](img/9029035755fd48f0607b94dc7377de63.png) | ![Refer to caption](img/5c3dd1e0ee20524f1c386fe40f28bc02.png)
    | ![Refer to caption](img/3004af1e4e7ba028e483ce753c36ee72.png) | ![Refer to caption](img/318af55aa4fcf44c2abe8c5c11eb02e4.png)
    | ![Refer to caption](img/9a99880ff9320a524dd3e6a5d2abf3b2.png) |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
- en: '| $\mathbf{\mathcal{M}^{2}}$ Transformer [[81](#bib.bib81)] |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/777bb9ab0c681035ab8f1afdd13488b8.png) | ![Refer to
    caption](img/0a9fa5422774b6233fd98f31007f289f.png) | ![Refer to caption](img/c5cca027fdeb0164f5306a108d685710.png)
    | ![Refer to caption](img/bdde6429844aa52107356e8b23f63702.png) | ![Refer to caption](img/1278a059d15a6fdb344e12a7c9efd3b1.png)
    | ![Refer to caption](img/2a82a45e1cc9f4a9f2b4b27133bad068.png) | ![Refer to caption](img/3338131ddd2d664af1c3c31c84dd70de.png)
    | ![Refer to caption](img/58d17ed396eaebb3ce0d944eeafaecf1.png) |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
- en: 'Figure 13: Visualization of attention states for sample captions generated
    by Up-Down [[58](#bib.bib58)] and $\mathcal{M}^{2}$ Transformer [[81](#bib.bib81)].
    For each generated word, we show the attended image regions, outlining the region
    with the maximum output attribution in green and blue, respectively.'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
