- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:37:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2308.01618] 1 Introduction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2308.01618] 1 引言'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.01618](https://ar5iv.labs.arxiv.org/html/2308.01618)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.01618](https://ar5iv.labs.arxiv.org/html/2308.01618)
- en: \CVMsetup
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \CVMsetup
- en: type = Review Article, doi = s41095-0xx-xxxx-x, title = A Survey on Deep Learning-based
    Spatio-temporal Action Detection, author = Peng Wang¹, Fanwei Zeng², and Yuntao
    Qian¹\cor
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 = 综述文章，doi = s41095-0xx-xxxx-x，标题 = 基于深度学习的时空动作检测综述，作者 = Peng Wang¹, Fanwei
    Zeng², 和 Yuntao Qian¹\cor
- en: ', runauthor = F. A. Author, S. B. Author, T. C. Author, abstract = Spatio-temporal
    action detection (STAD) aims to classify the actions present in a video and localize
    them in space and time. It has become a particularly active area of research in
    computer vision because of its explosively emerging real-world applications, such
    as autonomous driving, visual surveillance, entertainment, etc. Many efforts have
    been devoted in recent years to building a robust and effective framework for
    STAD. This paper provides a comprehensive review of the state-of-the-art deep
    learning-based methods for STAD. Firstly, a taxonomy is developed to organize
    these methods. Next, the linking algorithms, which aim to associate the frame-
    or clip-level detection results together to form action tubes, are reviewed. Then,
    the commonly used benchmark datasets and evaluation metrics are introduced, and
    the performance of state-of-the-art models is compared. At last, this paper is
    concluded, and a set of potential research directions of STAD are discussed. ,
    keywords = Computer vision; deep learning; spatio-temporal action detection, copyright
    = The Author(s),'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ', runauthor = F. A. Author, S. B. Author, T. C. Author，摘要 = 时空动作检测（STAD）旨在分类视频中存在的动作并在空间和时间上进行定位。由于其在自动驾驶、视觉监控、娱乐等实际应用中的快速发展，已成为计算机视觉研究的一个特别活跃的领域。近年来，许多努力致力于构建一个稳健而有效的STAD框架。本文提供了对最先进的基于深度学习的STAD方法的全面综述。首先，开发了一个分类系统来组织这些方法。接下来，回顾了旨在将帧级或片段级检测结果关联在一起以形成动作管道的链接算法。然后，介绍了常用的基准数据集和评估指标，并比较了最先进模型的性能。最后，总结了本文并讨论了STAD的潜在研究方向。
    ，关键词 = 计算机视觉；深度学习；时空动作检测，版权 = 作者（们），'
- en: '| $1\quad$ | College of Computer Science, Zhejiang University, Hangzhou, Zhejiang
    310007, China. E-mail: P. Wang, pengwang18@zju.edu.cn; Y.-T. Qian, ytqian@zju.edu.cn\cor.
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| $1\quad$ | 计算机科学学院，浙江大学，浙江省杭州市 310007，中国。电子邮件：P. Wang，pengwang18@zju.edu.cn；Y.-T.
    Qian，ytqian@zju.edu.cn\cor。 |'
- en: '| $2\quad$ | Ant Group, Hangzhou, Zhejiang 310007, China. E-mail: fanwei.zfw@antgroup.com.
    |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| $2\quad$ | 蚂蚁集团，浙江省杭州市 310007，中国。电子邮件：fanwei.zfw@antgroup.com。 |'
- en: '|  | Manuscript received: 2022-01-01; accepted: 2022-01-01 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '|  | 手稿收到日期：2022-01-01；接受日期：2022-01-01 |'
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 1.1 Motivation
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 动机
- en: Spatio-temporal action detection (STAD), which aims to localize actions in space
    and time in long untrimmed videos as well as predict action categories (see Figure [1](#S1.F1
    "Figure 1 ‣ 1.1 Motivation ‣ 1 Introduction")), is an essential and challenging
    task in video understanding. Because of its fundamental role in many applications,
    such as surveillance, sports analysis, robotics and self-driving cars, it has
    attracted a lot of attention and been actively researched in computer vision communities.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 时空动作检测（STAD），旨在定位长时间未修剪视频中的动作空间和时间以及预测动作类别（见图 [1](#S1.F1 "图 1 ‣ 1.1 动机 ‣ 1 引言")），是视频理解中的一个重要且具有挑战性的任务。由于其在监控、体育分析、机器人技术和自动驾驶等许多应用中的基础性作用，它吸引了大量关注，并在计算机视觉领域得到了积极研究。
- en: Before the prevalence of deep learning, traditional STAD usually involves a
    sliding window approach [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)], such as deformable part models [[1](#bib.bib1)], branch and bound
    approach [[2](#bib.bib2)], etc. Recently, with the significant development of
    deep neural networks, especially the convolutional neural network [[6](#bib.bib6)]
    and transformer [[7](#bib.bib7)], many significant advances in STAD have been
    achieved. These deep learning-based methods outperform the traditional algorithms
    by large margins and continue to improve the state-of-the-art.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习普及之前，传统 STAD 通常涉及滑动窗口方法 [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]，如可变形部件模型 [[1](#bib.bib1)]、分支界限方法 [[2](#bib.bib2)]
    等。最近，随着深度神经网络，特别是卷积神经网络 [[6](#bib.bib6)] 和 Transformer [[7](#bib.bib7)] 的显著发展，STAD
    取得了许多重要进展。这些基于深度学习的方法在很大程度上超越了传统算法，并继续改进最先进的技术。
- en: With the rapid progress in deep learning-based STAD, a multitude of literature
    is being produced in this field. However, the survey of this area is scarce. Vahdani
    et al. [[8](#bib.bib8)] reviewed the action detection task in untrimmed videos
    with the emphasis on temporal action detection that aims to detect the start and
    end of action instances, and only a brief introduction was given to STAD. Bhoi
    et al. [[9](#bib.bib9)] conducted a survey of STAD, but they focused on introducing
    traditional methods, and only three early deep learning-based approaches were
    reviewed. Thus, the recent development of this area is missing in their survey.
    Overall, there is a lack of comprehensive and in-depth survey in deep learning-based
    STAD, though it is highly needed for further research in this field.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于深度学习的 STAD 的快速进展，该领域产生了大量文献。然而，对这一领域的综述仍然稀缺。Vahdani 等人 [[8](#bib.bib8)]
    综述了未经修剪视频中的动作检测任务，重点强调了旨在检测动作实例的开始和结束的时间动作检测，仅对 STAD 进行了简要介绍。Bhoi 等人 [[9](#bib.bib9)]
    对 STAD 进行了综述，但他们侧重于介绍传统方法，仅审查了三种早期的基于深度学习的方法。因此，他们的综述中缺乏这一领域的最新发展。总体而言，基于深度学习的
    STAD 还缺乏全面且深入的综述，尽管这一综述对该领域的进一步研究非常必要。
- en: In response, we provide the first review that systematically introduces the
    most recent advances in deep learning-based STAD for interested researchers who
    would like to enter this ever-changing field and experts who want to compare STAD
    models and datasets. We collect abundant resources in this field, including state-of-the-art
    models, linking algorithms, benchmark datasets, performance comparison, etc. We
    hope this survey will facilitate the advancement of STAD.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们提供了首个系统介绍基于深度学习的 STAD 最新进展的综述，旨在为希望进入这一不断变化领域的研究人员以及希望比较 STAD 模型和数据集的专家提供帮助。我们收集了该领域的大量资源，包括最先进的模型、关联算法、基准数据集、性能比较等。我们希望这份综述能够促进
    STAD 的进步。
- en: '![Refer to caption](img/c735baf28bc1338fdb96392d327a827d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c735baf28bc1338fdb96392d327a827d.png)'
- en: 'Figure 1: Spatio-temporal action detection classifies and localizes actions
    in space and time. Images are from MultiSports [[10](#bib.bib10)] dataset.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 时空动作检测在空间和时间上对动作进行分类和定位。图像来源于 MultiSports [[10](#bib.bib10)] 数据集。'
- en: 1.2 Organization
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 组织结构
- en: The rest of this survey is organized as follows. Section [2](#S2 "2 Taxonomy
    and Methods") clarifies the categorization of deep learning-based STAD methods.
    Section [3](#S3 "3 Linking up the Detection Results") reviews how the detection
    results in each frame or clip are linked together. Section [4](#S4 "4 Datasets
    and Evaluation") summarizes the benchmark datasets and evaluation metrics and
    compares the performance among state-of-the-art methods. Section [5](#S5 "5 Future
    Directions") points out a set of future directions. Finally, Section [6](#S6 "6
    Conclusion") concludes this survey.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述的其余部分组织如下。第[2](#S2 "2 Taxonomy and Methods")节阐明了基于深度学习的 STAD 方法的分类。第[3](#S3
    "3 Linking up the Detection Results")节回顾了如何将每一帧或剪辑中的检测结果关联起来。第[4](#S4 "4 Datasets
    and Evaluation")节总结了基准数据集和评估指标，并比较了最先进方法之间的性能。第[5](#S5 "5 Future Directions")节指出了一系列未来方向。最后，第[6](#S6
    "6 Conclusion")节总结了本综述。
- en: 2 Taxonomy and Methods
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 分类法与方法
- en: In this section, we first describe the STAD problem formulation. Then we illustrate
    the taxonomy of deep learning-based STAD methods. Finally, we review these methods
    in detail in Subsection [2.3](#S2.SS3 "2.3 Frame-level Methods ‣ 2 Taxonomy and
    Methods") and [2.4](#S2.SS4 "2.4 Clip-level Methods ‣ 2 Taxonomy and Methods").
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先描述STAD问题的定义。然后，我们说明基于深度学习的STAD方法的分类。最后，我们在子节 [2.3](#S2.SS3 "2.3 Frame-level
    Methods ‣ 2 Taxonomy and Methods")和[2.4](#S2.SS4 "2.4 Clip-level Methods ‣ 2 Taxonomy
    and Methods")中详细回顾这些方法。
- en: 2.1 Problem Formulation
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题定义
- en: Given a video of $T$ frames $\{I_{t}\}_{t=1..T}$, the spatio-temporal action
    detection task, sometimes termed action localization or event detection, determines
    what actions occur in this video, and when and where they occur. That is to say,
    STAD models should output the action label $c_{i}\in\mathcal{C}$ ($\mathcal{C}$
    is the set of action classes), as well as a set of bounding boxes (or regions)
    $\{R^{i}_{t}\}_{t=t_{b}..t_{e}}$, where $t_{b}$ is the beginning and $t_{e}$ is
    the end of the predicted action $c_{i}$, and $R^{i}_{t}$ is the detected region
    in frame $I_{t}$. Notably, action recognition and temporal action detection are
    closely related to STAD, but they only determine what or when action occurs (see
    Table [1](#S2.T1 "Table 1 ‣ 2.1 Problem Formulation ‣ 2 Taxonomy and Methods")).
    Thus, they are not as challenging as STAD.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含$T$帧的视频$\{I_{t}\}_{t=1..T}$，时空动作检测任务，有时称为动作定位或事件检测，确定视频中发生了什么动作，以及它们何时何地发生。也就是说，STAD模型应输出动作标签$c_{i}\in\mathcal{C}$（$\mathcal{C}$是动作类别的集合），以及一组边界框（或区域）$\{R^{i}_{t}\}_{t=t_{b}..t_{e}}$，其中$t_{b}$是预测动作$c_{i}$的开始，$t_{e}$是结束，$R^{i}_{t}$是帧$I_{t}$中检测到的区域。值得注意的是，动作识别和时间动作检测与STAD密切相关，但它们仅确定动作发生的时间或何时发生（参见表 [1](#S2.T1
    "Table 1 ‣ 2.1 Problem Formulation ‣ 2 Taxonomy and Methods")）。因此，它们没有STAD那么具有挑战性。
- en: 'Table 1: Comparison of action recognition, temporal action detection (TAD),
    and spatio-temporal action detection (STAD)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：动作识别、时间动作检测（TAD）和时空动作检测（STAD）的比较
- en: '|  | Action class | The start and end of action | Actor localization |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | 动作类别 | 动作的开始和结束 | 演员定位 |'
- en: '| Action recognition | ✓ | ✗ | ✗ |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 动作识别 | ✓ | ✗ | ✗ |'
- en: '| TAD | ✓ | ✓ | ✗ |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| TAD | ✓ | ✓ | ✗ |'
- en: '| STAD | ✓ | ✓ | ✓ |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| STAD | ✓ | ✓ | ✓ |'
- en: '| ”✓” means ”need to be determined”. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| ”✓”表示”需要确定”。 |'
- en: '| ”✗” means ”need not to be determined”. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ”✗”表示”无需确定”。 |'
- en: '![Refer to caption](img/215d5bd00463d98326aeca75ad8cf878.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/215d5bd00463d98326aeca75ad8cf878.png)'
- en: 'Figure 2: An illustration of the output of the frame-level model and clip-level
    model'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：帧级模型和片段级模型输出的示意图
- en: 2.2 Taxonomy of Deep Learning-based STAD Methods
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基于深度学习的STAD方法分类
- en: 'A wide variety of deep learning-based STAD methods have been proposed so far.
    These models can be mainly divided into two categories: frame-level and clip-level.
    Whereas frame-level models predict 2D bounding boxes for a frame, the clip-level
    models predict 3D spatio-temporal tubelets for a clip. Fig. [2](#S2.F2 "Figure
    2 ‣ 2.1 Problem Formulation ‣ 2 Taxonomy and Methods") illustrates the output
    paradigms of these two categories of models. To provide an in-depth review of
    deep learning-based STAD models, we subdivide the frame-level and clip-level models
    according to the motivation of model design. Since the motivation behind a model
    often reflects researchers’ insights in the STAD field, we hope this categorization
    can provide a clear way to sort out the methods in this field and give researchers
    inspiration. Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Taxonomy of Deep Learning-based STAD
    Methods ‣ 2 Taxonomy and Methods") illustrates our taxonomy and some representative
    models.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，已经提出了各种各样的基于深度学习的STAD方法。这些模型主要分为两类：帧级和片段级。帧级模型为每一帧预测2D边界框，而片段级模型为每个片段预测3D时空管道。图 [2](#S2.F2
    "Figure 2 ‣ 2.1 Problem Formulation ‣ 2 Taxonomy and Methods")展示了这两类模型的输出范式。为了对基于深度学习的STAD模型进行深入评审，我们根据模型设计的动机细分了帧级和片段级模型。由于模型背后的动机通常反映了研究人员在STAD领域的见解，我们希望这种分类可以提供一种清晰的方式来整理该领域的方法，并给予研究人员启发。图 [3](#S2.F3
    "Figure 3 ‣ 2.2 Taxonomy of Deep Learning-based STAD Methods ‣ 2 Taxonomy and
    Methods")展示了我们的分类和一些代表性模型。
- en: '![Refer to caption](img/c393ae1d1f3245f073f6fa58b617efd7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c393ae1d1f3245f073f6fa58b617efd7.png)'
- en: 'Figure 3: Taxonomy of deep learning-based STAD models'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于深度学习的STAD模型分类
- en: '![Refer to caption](img/b8ea5510e4dc9f5ccef3d2c6a81be767.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b8ea5510e4dc9f5ccef3d2c6a81be767.png)'
- en: 'Figure 4: Flowchart of the method proposed by Saha et al. (2016) [[11](#bib.bib11)].'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Saha等人（2016年）提出的方法的流程图 [[11](#bib.bib11)]。
- en: 2.3 Frame-level Methods
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 帧级方法
- en: The past decade has witnessed the dramatic development of object detection,
    with numerous models being proposed, such as R-CNN [[12](#bib.bib12)], Faster
    R-CNN [[13](#bib.bib13)], YOLO [[14](#bib.bib14)], SSD [[15](#bib.bib15)], etc.
    Inspired by these advancements, many researchers seek to generalize the object
    detection models to the STAD field. A straightforward generalization is to regard
    the STAD in the video as a set of 2D image detections. Concretely, one applies
    an action detector at each frame independently to produce frame-level 2D bounding
    boxes. Then, 3D action proposals (i.e., action tubes) are generated by associating
    these frame-level detection results using linking or tracking algorithms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年见证了目标检测的戏剧性发展，提出了许多模型，如R-CNN [[12](#bib.bib12)]、Faster R-CNN [[13](#bib.bib13)]、YOLO [[14](#bib.bib14)]、SSD [[15](#bib.bib15)]等。受这些进展的启发，许多研究人员寻求将目标检测模型推广到STAD领域。一种直接的推广方式是将视频中的STAD视为一组2D图像检测。具体来说，针对每一帧独立应用动作检测器，以生成帧级2D边界框。然后，通过使用连接或跟踪算法将这些帧级检测结果关联起来，生成3D动作提议（即动作管道）。
- en: Along this research line, Gkioxari et al. [[16](#bib.bib16)] proposed a model
    based on R-CNN [[12](#bib.bib12)]. To incorporate both appearance and motion cues,
    they adopted two-stream architecture, with a spatial stream operating on the RGB
    frames and a temporal stream on the optical flow. Weinzaepfel et al. [[17](#bib.bib17)]
    presented a method that extracts a set of candidate regions at the frame level
    via EdgeBoxes [[18](#bib.bib18)] and then tracks high-scoring proposals throughout
    the video using a tracking-by-detection approach. They determined the temporal
    extension of actions by using a sliding-window approach. Driven by the huge success
    of Faster R-CNN [[13](#bib.bib13)], Saha et al. [[11](#bib.bib11)] introduced
    the first model that replaces the unsupervised region proposal algorithms with
    the region proposal network (RPN) for STAD, as shown in Fig [4](#S2.F4 "Figure
    4 ‣ 2.2 Taxonomy of Deep Learning-based STAD Methods ‣ 2 Taxonomy and Methods").
    They passed RGB and optical-flow images to two separate region proposal networks
    to output detection boxes and action class scores. These appearance and motion-based
    detections were fused and linked up to generate class-specific action tubes. The
    effectiveness of RPN on STAD was further verified by Peng et al. [[19](#bib.bib19)],
    where they found that compared to other proposal generalization methods, RPN achieves
    consistently better results with higher inter-section-over-union (IoU) score.
    Moreover, they experimentally found that stacking optical flow over several frames
    can improve frame-level action detection. Besides these proposal-based action
    detection methods, Wang et al. [[20](#bib.bib20)] introduced H-FCN, a method that
    detects actions based on the estimated actionness maps, where actionness means
    the likelihood of containing a generic action instance at a specific location
    of an image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一研究方向上，Gkioxari等人[[16](#bib.bib16)]提出了一个基于R-CNN [[12](#bib.bib12)]的模型。为了融合外观和运动线索，他们采用了双流架构，其中空间流处理RGB帧，时间流处理光流。Weinzaepfel等人[[17](#bib.bib17)]提出了一种方法，通过EdgeBoxes [[18](#bib.bib18)]在帧级提取一组候选区域，然后使用基于检测的跟踪方法跟踪高评分提议。他们通过滑动窗口方法确定了动作的时间扩展。受到Faster
    R-CNN [[13](#bib.bib13)]巨大成功的驱动，Saha等人[[11](#bib.bib11)]首次引入了将无监督区域提议算法替换为区域提议网络（RPN）用于STAD的模型，如图 [4](#S2.F4
    "Figure 4 ‣ 2.2 Taxonomy of Deep Learning-based STAD Methods ‣ 2 Taxonomy and
    Methods")所示。他们将RGB和光流图像传递给两个独立的区域提议网络，以输出检测框和动作类别分数。这些基于外观和运动的检测被融合和连接以生成类别特定的动作管道。Peng等人[[19](#bib.bib19)]进一步验证了RPN在STAD上的有效性，他们发现与其他提议推广方法相比，RPN在交并比（IoU）评分上始终取得了更好的结果。此外，他们还实验证明，将光流堆叠在几帧上可以提高帧级动作检测。除了这些基于提议的动作检测方法外，Wang等人[[20](#bib.bib20)]介绍了H-FCN，这是一种基于估计的动作性图检测动作的方法，其中动作性意味着在图像特定位置包含通用动作实例的可能性。
- en: Temporal context. The above STAD methods treat each frame independently, ignoring
    the temporal contextual relationships. To overcome this problem, Yang et al. [[21](#bib.bib21)]
    proposed a cascade proposal and location anticipation model, named CPLA, which
    is capable of inferring the movement trend of action occurrences between two frames.
    It uses detected bounding boxes on frame $I_{t}$ to infer the corresponding boxes
    on frame $I_{t+k}$, where $k$ is the anticipation gap. As a follow-up work, Li
    et al. [[22](#bib.bib22)] presented an approach that models the temporal correlations
    of proposals between two consecutive frames to predict movements. Whereas [[21](#bib.bib21)]
    requires running RPN at each frame, [[22](#bib.bib22)] only runs RPN at the first
    frame of a video. In another work, Alwando et al. [[23](#bib.bib23)] proposed
    a video localization refinement scheme to iteratively rectify the potentially
    inaccurate bounding boxes by exploiting the temporal consistency between adjacent
    frames. Wu et al. [[24](#bib.bib24)] presented an insight that actions can become
    clear when relating the target frame to the long-range context. Therefore, they
    proposed a long-term feature bank (LFB) that provides long-term supportive information
    to video models, enabling them to better understand the present.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 时间上下文。上述 STAD 方法将每一帧独立对待，忽略了时间上的上下文关系。为了解决这个问题，Yang 等人 [[21](#bib.bib21)] 提出了一个级联提议和位置预测模型，命名为
    CPLA，该模型能够推断两个帧之间动作出现的运动趋势。它利用帧 $I_{t}$ 上检测到的边界框来推断帧 $I_{t+k}$ 上的对应框，其中 $k$ 是预测间隔。作为后续工作，Li
    等人 [[22](#bib.bib22)] 提出了一个方法，通过建模连续帧之间提议的时间相关性来预测运动。而 [[21](#bib.bib21)] 需要在每一帧上运行
    RPN，[[22](#bib.bib22)] 只在视频的第一帧上运行 RPN。在另一项工作中，Alwando 等人 [[23](#bib.bib23)] 提出了一个视频定位精化方案，通过利用相邻帧之间的时间一致性来迭代修正可能不准确的边界框。Wu
    等人 [[24](#bib.bib24)] 提出了一个观点，即将目标帧与长程上下文关联时，动作会变得更清晰。因此，他们提出了一个长期特征库（LFB），为视频模型提供长期支持信息，使其能够更好地理解当前内容。
- en: '![Refer to caption](img/dbe5cc2da34b5c1a3c11732095cb88b2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dbe5cc2da34b5c1a3c11732095cb88b2.png)'
- en: 'Figure 5: Architecture of the model proposed by Girdhar et al. (2018) [[25](#bib.bib25)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Girdhar 等人（2018年）提出的模型架构 [[25](#bib.bib25)]。
- en: '3D CNN. Apart from the methods mentioned above that capture the motion characteristics
    in videos via optical flow, another group of work adopts 3D convolutional neural
    networks (3D CNNs) [[26](#bib.bib26)] to extract the motion information encoded
    in multiple adjacent frames. Gu et al. [[27](#bib.bib27)] proposed to combine
    inflated 3D convolutional network (I3D) [[28](#bib.bib28)] with Faster R-CNN [[13](#bib.bib13)].
    They first fed input frames to the I3D model to extract 3D feature maps and used
    ResNet-50 model on the keyframe to generate region proposals. Then, they extended
    ROI Pooling to 3D by applying the 2D ROI Pooling at the same spatial location
    over all time steps so that the spatio-temporal feature for each proposal was
    obtained, which was then fed into the classifier for action labeling. Girdhar
    et al. [[25](#bib.bib25)] improved the performance of [[27](#bib.bib27)] by applying
    the I3D features for both proposal generation and classification, see Fig. [5](#S2.F5
    "Figure 5 ‣ 2.3 Frame-level Methods ‣ 2 Taxonomy and Methods"). They passed a
    video clip through the first few blocks of I3D to get a video representation.
    Then, the center frame representation was used to predict potential ‘person’ regions
    using an RPN. The proposals were extended in time by replicating and used to extract
    a feature map for the region using ROI pooling. The feature map was then classified
    into different actions using the two last I3D blocks. Zolfaghari et al. [[29](#bib.bib29)]
    proposed a network architecture that integrates three visual cues for action recognition
    and detection: pose, motion, and raw images. They introduced a Markov chain model
    that adds cues successively to integrate the features. Feichtenhofer et al. [[30](#bib.bib30)]
    presented a SlowFast network involving two pathways: a slow pathway and a fast
    pathway. Whereas the former operates at a low frame rate to capture spatial semantics,
    the latter operates at a high frame rate to extract motion information at fine
    temporal resolution. The fast pathway is intentionally made very lightweight by
    reducing its channel capacity. With these innovative designs, SlowFast achieved
    strong performance for STAD in videos.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 3D CNN。除了上述通过光流捕捉视频中运动特征的方法外，另一类工作采用了 3D 卷积神经网络（3D CNNs）[[26](#bib.bib26)] 来提取编码在多个相邻帧中的运动信息。Gu
    等人[[27](#bib.bib27)] 提出了将膨胀 3D 卷积网络（I3D）[[28](#bib.bib28)] 与 Faster R-CNN[[13](#bib.bib13)]
    结合的方案。他们首先将输入帧传递给 I3D 模型以提取 3D 特征图，并在关键帧上使用 ResNet-50 模型生成区域建议。然后，他们通过在所有时间步上在相同空间位置应用
    2D ROI Pooling，将 ROI Pooling 扩展到 3D，从而获得每个建议的时空特征，然后将其输入分类器进行动作标记。Girdhar 等人[[25](#bib.bib25)]
    通过将 I3D 特征应用于建议生成和分类，改进了 [[27](#bib.bib27)] 的性能，见图 [5](#S2.F5 "Figure 5 ‣ 2.3
    Frame-level Methods ‣ 2 Taxonomy and Methods")。他们通过 I3D 的前几个块处理视频片段，以获取视频表示。然后，使用中心帧表示来预测潜在的“人”区域，使用
    RPN 扩展这些建议的时间，并利用 ROI pooling 提取区域的特征图。然后，使用最后两个 I3D 块将特征图分类为不同的动作。Zolfaghari
    等人[[29](#bib.bib29)] 提出了一个网络架构，集成了用于动作识别和检测的三种视觉线索：姿态、运动和原始图像。他们引入了一种马尔可夫链模型，逐步添加线索以集成功能。Feichtenhofer
    等人[[30](#bib.bib30)] 提出了一个 SlowFast 网络，涉及两条路径：一条慢路径和一条快路径。前者以较低的帧率操作以捕捉空间语义，而后者以较高的帧率操作以提取细微时间分辨率的运动信息。快路径通过减少其通道容量被故意设计得非常轻量。通过这些创新设计，SlowFast
    在视频的 STAD 中实现了强大的性能。
- en: High efficiency and real-time speed. Current stage-of-the-art STAD models usually
    push performance by larger backbone network and more complex architecture and
    training procedures, thereby suffering from heavy computational burden and low
    efficiency. To move beyond such limitations, some researchers strived to design
    efficient and real-time models. Feichtenhofer et al. [[31](#bib.bib31)] presented
    an X3D network that was constructed by progressively expanding a tiny 2D architecture
    along multiple network axes, including space, time, width, and depth. X3D achieved
    state-of-the-art performance while requiring 5.5$\times$ fewer parameters for
    similar accuracy as previous work. Liu et al. [[32](#bib.bib32)] introduced ACDnet
    that performs feature approximation at most frames by exploiting the temporal
    coherence between successive video frames, instead of conducting the time-consuming
    CNN feature extraction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 高效和实时速度。当前最先进的 STAD 模型通常通过更大的骨干网络和更复杂的架构以及训练过程来提高性能，因此面临着沉重的计算负担和低效率。为了超越这些限制，一些研究人员致力于设计高效且实时的模型。Feichtenhofer
    等人[[31](#bib.bib31)] 提出了一个 X3D 网络，该网络通过沿多个网络轴（包括空间、时间、宽度和深度）逐步扩展一个微小的 2D 架构来构建。X3D
    在达到与之前工作相似的准确度的同时，所需的参数减少了 5.5$\times$。Liu 等人[[32](#bib.bib32)] 介绍了 ACDnet，该网络通过利用连续视频帧之间的时间一致性，在大多数帧上进行特征近似，而不是进行耗时的
    CNN 特征提取。
- en: '![Refer to caption](img/953e4192d0683c75f10962c2dcac8f8a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/953e4192d0683c75f10962c2dcac8f8a.png)'
- en: 'Figure 6: Network architecture of YOWO [[33](#bib.bib33)].'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：YOWO 网络架构[[33](#bib.bib33)]。
- en: Zhao et al. [[34](#bib.bib34)] argued that the two-stream detection network
    based on RGB and flow provides state-of-the-art accuracy at the expense of a large
    model size and heavy computation. Hence, they proposed embedding RGB and optical
    flow into a single stream network. They introduced a motion modulation layer to
    leverage optical flow to modulate RGB features. Their network, called 2in1, has
    half the computation and parameters of a two-stream equivalent while obtaining
    better action detection accuracy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人[[34](#bib.bib34)] 认为，基于 RGB 和光流的双流检测网络在提供最先进的准确度的同时，牺牲了模型的大小和计算量。因此，他们提议将
    RGB 和光流嵌入到一个单流网络中。他们引入了一个运动调制层，以利用光流调制 RGB 特征。他们的网络称为 2in1，计算量和参数数量仅为双流网络的一半，同时获得了更好的动作检测准确度。
- en: 'Singh et al. [[35](#bib.bib35)] proposed a method that can perform STAD in
    an online setting and at real-time speed, owing to two major developments: 1)
    they adopted real-time SSD [[15](#bib.bib15)] CNNs to detect actors; 2) they designed
    an online algorithm to incrementally construct action tubes from the frame-level
    detections (see Section [3.1](#S3.SS1 "3.1 Linking up Frame-level Detection Boxes
    ‣ 3 Linking up the Detection Results")).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Singh 等人[[35](#bib.bib35)] 提出了一种可以在在线环境中以实时速度执行 STAD 的方法，得益于两个主要的发展：1）他们采用了实时
    SSD [[15](#bib.bib15)] CNN 来检测演员；2）他们设计了一个在线算法，从帧级检测结果中逐步构建动作管道（见第[3.1节](#S3.SS1
    "3.1 Linking up Frame-level Detection Boxes ‣ 3 Linking up the Detection Results")）。
- en: Inspired by the outstanding real-time performance of YOLO [[14](#bib.bib14)],
    Kopuklu et al. put forward a STAD counterpart network, i.e., YOWO [[33](#bib.bib33)],
    which replaces the detect-then-classify two-stage paradigm with the end-to-end
    one-stage paradigm, as shown in Fig. [6](#S2.F6 "Figure 6 ‣ 2.3 Frame-level Methods
    ‣ 2 Taxonomy and Methods"). It outputs the regions of interest (ROIs) and the
    corresponding class prediction simultaneously. Albeit efficient, YOWO adopts two
    backbones, i.e., a 3D CNN for extracting spatio-temporal information and a 2D
    model for extracting spatial features. To combat this limitation, Chen et al. [[36](#bib.bib36)]
    presented a single unified network (i.e., WOO) that only adopts one backbone for
    both actor localization and action classification. Compared to two-backbone STAD
    models, WOO reduces the model complexity by over 50%. However, it suffers from
    a performance drop. To improve the performance, they designed an extra attention-based
    embedding interaction module to obtain more discriminative features. As a follow-up
    work, SE-STAD [[37](#bib.bib37)] achieves stronger performances than WOO [[36](#bib.bib36)]
    with better training strategies and FCOS [[38](#bib.bib38)] as object detector.
    Most recently, Chen et al. proposed a framework for efficient video action detection
    (EVAD) [[39](#bib.bib39)] based on vanilla visual transformers (ViTs). They reduced
    computational costs by dropping out the non-keframe tokens and enhanced the model
    performance by refining scene context.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 受到YOLO[[14](#bib.bib14)]出色的实时性能启发，Kopuklu等人提出了一种STAD对应网络，即YOWO[[33](#bib.bib33)]，它用端到端的一阶段范式取代了检测-分类两阶段范式，如图[6](#S2.F6
    "Figure 6 ‣ 2.3 Frame-level Methods ‣ 2 Taxonomy and Methods")所示。它同时输出感兴趣区域（ROIs）和相应的类别预测。尽管高效，YOWO采用了两个骨干网络，即一个用于提取时空信息的3D
    CNN和一个用于提取空间特征的2D模型。为了克服这一限制，Chen等人[[36](#bib.bib36)]提出了一个统一的单一网络（即WOO），只采用一个骨干网络来进行演员定位和动作分类。与两个骨干网络的STAD模型相比，WOO将模型复杂度降低了50%以上。然而，它的性能有所下降。为了提高性能，他们设计了一个额外的基于注意力的嵌入交互模块，以获取更多区分性特征。作为后续工作，SE-STAD[[37](#bib.bib37)]在训练策略和对象检测器FCOS[[38](#bib.bib38)]方面优于WOO[[36](#bib.bib36)]。最近，Chen等人提出了一种基于原始视觉变换器（ViTs）的高效视频动作检测（EVAD）框架[[39](#bib.bib39)]。他们通过丢弃非关键帧的tokens来降低计算成本，并通过优化场景上下文来提升模型性能。
- en: '![Refer to caption](img/ef0c9095bef3734f72d82320088237ff.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ef0c9095bef3734f72d82320088237ff.png)'
- en: 'Figure 7: The framework of ACRN [[40](#bib.bib40)].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：ACRN框架[[40](#bib.bib40)]。
- en: Visual Relation Modeling. Detecting and classifying the action of an actor usually
    depend on its relationships with other actors and objects. Thus, many works strived
    to model the relationship among actors, objects, and scenes conveyed in the video.
    For example, [[40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)] observed that the surrounding
    context provides essential information for understanding actions. Accordingly,
    they proposed a variety of actor-scene interaction models. Concretely, Sun et
    al. [[40](#bib.bib40)] proposed an actor-centric relation network (ACRN) to extract
    pairwise relations from cropped actor features and a global scene feature, see
    Fig. [7](#S2.F7 "Figure 7 ‣ 2.3 Frame-level Methods ‣ 2 Taxonomy and Methods").
    These relation features are then used for action classification. Wu et al. [[45](#bib.bib45)]
    empirically found that action recognition accuracy is highly correlated with the
    resolution of actors. Hence, they cropped and resized image patches around actors
    and fed them into a 3D CNN to extract actor features that were used to interact
    with scene features and long-term features.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉关系建模。检测和分类演员的动作通常依赖于其与其他演员和物体的关系。因此，许多研究致力于建模视频中传达的演员、物体和场景之间的关系。例如，[[40](#bib.bib40)、[41](#bib.bib41)、[42](#bib.bib42)、[43](#bib.bib43)、[44](#bib.bib44)、[45](#bib.bib45)、[46](#bib.bib46)]观察到周围环境为理解动作提供了重要信息。因此，他们提出了多种演员-场景互动模型。具体而言，Sun等人[[40](#bib.bib40)]提出了一个以演员为中心的关系网络（ACRN），从裁剪后的演员特征和全局场景特征中提取成对的关系，见图[7](#S2.F7
    "Figure 7 ‣ 2.3 Frame-level Methods ‣ 2 Taxonomy and Methods")。这些关系特征随后用于动作分类。Wu等人[[45](#bib.bib45)]通过实验证明，动作识别的准确性与演员的分辨率高度相关。因此，他们裁剪并调整了演员周围的图像块，并将其输入到3D
    CNN中，以提取演员特征，这些特征用于与场景特征和长期特征进行交互。
- en: The attention-based learning strategies are widely adopted in visual relation
    modeling. For example, non-local network [[44](#bib.bib44)] leverages self-attention
    mechanisms to capture long-range dependencies between different entities. PCCA [[43](#bib.bib43)]
    utilizes a cross-attention mechanism to model relations between person and context
    for action detection. Ultan et al. [[41](#bib.bib41)] proposed an actor-conditioned
    attention maps (ACAM) method that explicitly models the surrounding context and
    generates features from the complete scene by conditioning them on detected actors.
    Similarly, Jiang et al. [[42](#bib.bib42)] designed an actor-target relation network
    to capture the human-context relationships, which was achieved with a non-local
    operation between the ROI and its surrounding regions. Girdhar et al. [[46](#bib.bib46)]
    proposed an action transformer network that can learn spatio-temporal context
    from other human actions and objects in a video clip to localize and classify
    target actions. Their resulting feature embeddings and attention maps were experimentally
    demonstrated to have a semantic meaning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的学习策略在视觉关系建模中被广泛采用。例如，非局部网络[[44](#bib.bib44)]利用自注意力机制捕捉不同实体之间的长程依赖关系。PCCA[[43](#bib.bib43)]利用交叉注意力机制建模人物与上下文之间的关系以进行动作检测。乌尔坦等人[[41](#bib.bib41)]提出了一种演员条件注意力图（ACAM）方法，该方法明确建模周围上下文，并通过条件化检测到的演员生成来自完整场景的特征。类似地，姜等人[[42](#bib.bib42)]设计了一种演员-目标关系网络，以捕捉人-上下文关系，这通过在ROI及其周围区域之间进行非局部操作来实现。吉尔达尔等人[[46](#bib.bib46)]提出了一种动作变换器网络，该网络能够从视频剪辑中的其他人类动作和物体中学习时空上下文，以定位和分类目标动作。他们的特征嵌入和注意力图经过实验验证具有语义意义。
- en: There are some works considering multiple interactions. For example, Tang et
    al. [[47](#bib.bib47)] introduced an asynchronous interaction aggregation (AIA)
    network to explore three kinds of interactions, i.e., person-person, person-object,
    and temporal interaction. They made them work cooperatively in a hierarchical
    structure to capture meaningful features. Zheng et al. proposed a network called
    multi-relation support network (MRSN) [[48](#bib.bib48)], which contains an actor-context
    relation encoder and an actor-actor relation encoder to model the actor-context
    and actor-actor relation separately. Then an relation support encoder was deployed
    to compute the supports between the two relations and performs relation-level
    interactions. Faure et al. [[49](#bib.bib49)] proposed a bi-modal holistic interaction
    transformer (HIT) network that comprises an RGB stream and a pose stream. Each
    stream separately models person, object, and hand interactions. The resulting
    features from each stream are then glued using an attentive fusion mechanism,
    and the glued feature is used for action classification. Pramono et al. proposed
    a model called HISAN [[50](#bib.bib50)] that combines the two-stream CNNs with
    hierarchical bidirectional self-attention mechanism to learn the structure relationship
    among key actors and spatial context to improve the localization accuracy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些研究考虑了多重交互。例如，唐等人[[47](#bib.bib47)]引入了一种异步交互聚合（AIA）网络来探索三种交互，即人-人、人与物体以及时间交互。他们在分层结构中使这些交互协同工作，以捕捉有意义的特征。郑等人提出了一种叫做多关系支持网络（MRSN）的网络[[48](#bib.bib48)]，该网络包含一个演员-上下文关系编码器和一个演员-演员关系编码器，分别建模演员-上下文和演员-演员关系。然后，部署了一个关系支持编码器来计算这两种关系之间的支持，并进行关系级交互。法雷等人[[49](#bib.bib49)]提出了一种双模态整体交互变换器（HIT）网络，该网络包含一个RGB流和一个姿势流。每个流分别建模人物、物体和手部交互。然后，使用注意力融合机制将每个流的结果特征进行融合，融合后的特征用于动作分类。普拉莫诺等人提出了一种叫做HISAN[[50](#bib.bib50)]的模型，该模型结合了双流CNN和层次双向自注意力机制，以学习关键演员和空间上下文之间的结构关系，从而提高定位精度。
- en: Different from the above methods that only consider direct relations between
    pairs, Pan et al. [[51](#bib.bib51)] proposed an actor-context-actor relation
    (ACAR) network that takes into account indirect higher-order relations established
    upon multiple elements. They designed a high-order relation reasoning operator
    and an actor-context feature bank to fulfil indirect relation reasoning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述仅考虑成对直接关系的方法不同，潘等人[[51](#bib.bib51)]提出了一种考虑基于多个元素建立的间接高阶关系的演员-上下文-演员关系（ACAR）网络。他们设计了一个高阶关系推理算子和一个演员-上下文特征库，以实现间接关系推理。
- en: Graph neural networks (GNNs) can naturally pass information among entity nodes
    and model their relations. Thus, GNN is often adopted for visual relation modeling.
    For instance, Tomei et al. proposed a model called STAGE [[52](#bib.bib52)] that
    explores the spatio-temporal relationships through self-attention on a multi-layer
    graph structure that can connect entities from consecutive clips. Ni et al. [[53](#bib.bib53)]
    proposed an identity-aware graph memory network (IGMN) that highlights the identity
    information of the actors in terms of both long-term and short-term context, so
    that the consistency and distinctness between actors are considered for action
    detection.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）可以自然地在实体节点之间传递信息并建模它们的关系。因此，GNN常用于视觉关系建模。例如，Tomei等人提出了一种名为STAGE[[52](#bib.bib52)]的模型，该模型通过在多层图结构上的自注意力探索时空关系，可以连接来自连续片段的实体。Ni等人[[53](#bib.bib53)]提出了一种身份感知图记忆网络（IGMN），它在长短期上下文中突出演员的身份信息，从而考虑演员之间的一致性和独特性，以进行动作检测。
- en: Although these frame-level methods have achieved significant results, they do
    not fully explore temporal continuity as they treat video frames as a set of independent
    images. Thus, their results can be sub-optimal. To address this issue, clip-level
    STAD approaches have been proposed, which take as input a sequence of frames and
    directly output detected tubelet proposals (i.e., a short sequence of bounding
    boxes).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些帧级方法已取得显著成果，但由于它们将视频帧视为一组独立的图像，未能充分探索时间连续性，因此其结果可能次优。为解决此问题，提出了片段级STAD方法，这些方法以帧序列作为输入，并直接输出检测到的管道提议（即短序列的边界框）。
- en: 2.4 Clip-level Methods
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 片段级方法
- en: Taking as input a video clip (i.e., a short video snippet), clip-level models
    directly output 3D spatio-temporal tubelet proposals in this clip. The 3D tubelet
    proposals are formed by a sequence of bounding boxes that tightly bound the actions
    of interest. Then, these tubelet proposals in the successive clips are linked
    together to form complete action tubes. Since being proposed by Jain et al. [[54](#bib.bib54)],
    clip-level approaches have become popular among STAD research communities.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以视频片段（即短视频片段）作为输入，片段级模型直接输出该片段中的3D时空管道提议。这些3D管道提议由一系列紧密包围感兴趣动作的边界框组成。然后，将这些在连续片段中的管道提议连接起来，形成完整的动作管道。自Jain等人提出[[54](#bib.bib54)]以来，片段级方法在STAD研究社区中变得非常流行。
- en: Hou et al. [[55](#bib.bib55)] presented a tube convolutional neural network
    (T-CNN) which explores generalizing faster R-CNN [[13](#bib.bib13)] from 2D image
    regions to 3D video volumes. Concerning the RPN and ROI pooling layer of R-CNN,
    they proposed tube proposal network (TPN) and tube-of-interest (ToI) pooling layer
    in T-CNN for tubelet generation and spatio-temporal feature pooling, respectively.
    Kalogeiton et al. [[56](#bib.bib56)] introduced an action tubelet detector (ACT)
    based on SSD framework [[15](#bib.bib15)], which takes as input a sequence of
    frames and then extracts features from each frame with VGG backbone [[57](#bib.bib57)].
    These features are stacked to predict scores and regress coordinates for the anchor
    cuboids to output final action tubelets. Song et al. [[58](#bib.bib58)] proposed
    a transition-aware context network (TACNet) to distinguish transitional states
    between action and non-action frames so that they could localize the temporal
    boundaries for the target actions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Hou等人[[55](#bib.bib55)]提出了一种管道卷积神经网络（T-CNN），它探索了从2D图像区域到3D视频体积的快速R-CNN[[13](#bib.bib13)]的泛化。关于R-CNN的RPN和ROI池化层，他们在T-CNN中提出了管道提议网络（TPN）和感兴趣的管道（ToI）池化层，分别用于管道生成和时空特征池化。Kalogeiton等人[[56](#bib.bib56)]基于SSD框架[[15](#bib.bib15)]介绍了一种动作管道检测器（ACT），它以帧序列作为输入，然后使用VGG骨干网络[[57](#bib.bib57)]从每帧中提取特征。这些特征被堆叠以预测分数并回归锚立方体的坐标，从而输出最终的动作管道。Song等人[[58](#bib.bib58)]提出了一种过渡感知上下文网络（TACNet），以区分动作帧和非动作帧之间的过渡状态，从而定位目标动作的时间边界。
- en: 'Large motion. The aforementioned clip-level methods are all based on an underlying
    hypothesis that the 3D anchor proposals are cuboid, i.e., having fixed spatial
    extent across time. Unfortunately, these 3D anchor cuboids can stray far from
    the flexible ground truth action tubes due to large actor displacement, dramatic
    actor body shape deformation, large camera motion, etc., particularly for the
    anchors spanning over a long time. To surpass this limitation, Saha et al. [[59](#bib.bib59)]
    proposed a framework that generates two-frame micro-tubes and then links these
    micro-tubes up into proper action tubes. A year later, they took a further step
    and explored the anchor micro-tube proposal search space via an approximate transition
    matrix estimated based on a hidden Markov model (HMM) formulation [[60](#bib.bib60)].
    Their micro-tube hypothesis generation framework can handle large spatial movements
    in dynamic actors. Four years later, they put forward a new solution to the large-motion
    case: they proposed to track the actors over time and perform temporal feature
    aggregation along the respective tracks to enhance actor feature representation [[61](#bib.bib61)].'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大范围运动。前述的剪辑级方法都基于一个潜在假设，即3D锚点建议是长方体，即在时间上具有固定的空间范围。不幸的是，由于演员的位移剧烈、演员身体形状的变形、相机的剧烈运动等，特别是对于跨越长时间的锚点，这些3D锚点长方体可能与灵活的真实动作管道相距甚远。为了超越这一限制，Saha等人[[59](#bib.bib59)]
    提出了一个生成两帧微型管道的框架，然后将这些微型管道链接成适当的动作管道。一年后，他们进一步探索了通过基于隐马尔可夫模型（HMM）公式估算的近似转移矩阵的锚点微型管道建议空间[[60](#bib.bib60)]。他们的微型管道假设生成框架可以处理动态演员中的大范围空间运动。四年后，他们提出了对大范围运动情况的新解决方案：他们建议跟踪演员的时间，并在各自的轨迹上执行时间特征聚合，以增强演员特征表示[[61](#bib.bib61)]。
- en: 'He et al. [[62](#bib.bib62)] proposed a method that avoids the 3D cuboid anchor
    hypothesis by performing frame-level actor detection and then linking the detected
    bounding boxes to form class-independent action tubelets, which are fed into the
    temporal understanding module for action classification. Li et al. [[62](#bib.bib62)]
    proposed a similar framework to [[62](#bib.bib62)], but they performed STAD in
    a sparse-to-dense manner: they first generated box proposals at sparsely sampled
    frames, then they obtained the dense tube by interpolating the sparse proposals
    across a given detected time interval.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: He等人[[62](#bib.bib62)] 提出了一种避免3D长方体锚点假设的方法，通过执行帧级演员检测，然后将检测到的边界框链接成与类别无关的动作管段，这些管段被输入到时间理解模块中进行动作分类。Li等人[[62](#bib.bib62)]
    提出了类似于[[62](#bib.bib62)]的框架，但他们以稀疏到密集的方式执行STAD：他们首先在稀疏采样的帧中生成框建议，然后通过在给定的检测时间间隔内插值稀疏建议来获得密集管道。
- en: '![Refer to caption](img/fa36be2281c0ff6bd8e6bb5bcd2ddce9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa36be2281c0ff6bd8e6bb5bcd2ddce9.png)'
- en: 'Figure 8: Overview of CFAD framework [[63](#bib.bib63)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：CFAD框架概述[[63](#bib.bib63)]。
- en: Progressive learning. Unlike the models that perform action detection in one
    run, progressive learning approaches iteratively refine the proposals towards
    actions over a few steps. For example, Su et al. [[64](#bib.bib64)] proposed a
    progressive cross-stream cooperation (PCSC) framework to use region proposals
    and features from one stream (i.e., Flow/RGB) to help another stream (i.e., RGB/Flow)
    to improve action localization results in an iterative fashion. Yang et al. [[65](#bib.bib65)]
    proposed a model, named STEP, which progressively refines the pre-defined proposal
    cuboids across time, i.e., the proposals obtained in current step are fed into
    the next step for further refinement. Besides, at each step, the 3D proposals
    are extended in the time dimension to incorporate more adjacent temporal context.
    Similarly, Li et al. [[63](#bib.bib63)] proposed a coarse-to-fine action detector
    (CFAD), which first estimates coarse spatio-temporal action tubes from video streams,
    and then refines the tubes’ location based on key timestamps. As shown in Fig. [8](#S2.F8
    "Figure 8 ‣ 2.4 Clip-level Methods ‣ 2 Taxonomy and Methods"), CFAD contains Coarse
    Module and Refine Module. While the Coarse Module produces initial action tube
    estimation, the Refine Module selectively adjusts the tube location under the
    guidance of key timestamps.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 进阶学习。与那些在一次运行中进行动作检测的模型不同，进阶学习方法通过几个步骤迭代地优化动作提议。例如，Su 等人[[64](#bib.bib64)] 提出了一个进阶交叉流合作（PCSC）框架，利用一个流（即
    Flow/RGB）的区域提议和特征来帮助另一个流（即 RGB/Flow）在迭代过程中改进动作定位结果。Yang 等人[[65](#bib.bib65)] 提出了一个名为
    STEP 的模型，它逐步优化跨时间的预定义提议立方体，即当前步骤获得的提议被输入到下一步骤进行进一步优化。此外，在每一步中，3D 提议在时间维度上进行扩展，以结合更多相邻的时间上下文。类似地，Li
    等人[[63](#bib.bib63)] 提出了一个粗到细的动作检测器（CFAD），该检测器首先从视频流中估计粗略的时空动作管，然后基于关键时间戳优化管的位置。如图[8](#S2.F8
    "Figure 8 ‣ 2.4 Clip-level Methods ‣ 2 Taxonomy and Methods")所示，CFAD 包含粗略模块和优化模块。粗略模块生成初步的动作管估计，而优化模块在关键时间戳的指导下选择性地调整管的位置。
- en: 'Anchor free. The current state-of-the-art methods usually depend on heuristic
    anchor design and operate on a vast number of pre-defined anchor boxes or cuboids.
    Although these anchor-based methods have achieved remarkable success, they suffer
    from critical issues such as huge proposal search space and low efficiency. For
    example, considering Faster R-CNN [[13](#bib.bib13)] for 2D object detection,
    it requires $KH^{\prime}W^{\prime}$ anchors for a feature map with spatial size
    $H^{\prime}\times W^{\prime}$, where usually $K=9$. While for a tubelet across
    $T^{\prime}$ frames, the number of 3D anchor proposals soars to $(KH^{\prime}W^{\prime})^{T^{\prime}}$
    to maintain the same sampling in space-time. This is a huge number even for a
    small value of $T^{\prime}$. To avoid this problem, anchor-free methods for STAD
    are introduced. For instance, Li et al. [[66](#bib.bib66)] viewed each action
    instance as a trajectory of moving points and presented a model termed as MovingCenter
    Detector (MOC-detector). It comprises three branches: the center branch for instance
    center detection and action recognition; the movement branch for movement estimation;
    the box branch for spatial extent detection. Based on MOC framework, Liu et al. [[67](#bib.bib67)]
    proposed a lightweight online action detector which encodes short-term action
    dynamics as accumulated micro-motion. Duarte et al. [[68](#bib.bib68)] proposed
    a capsule network for videos, called VideoCapsuleNet, which uses 3D convolutions
    along with capsules to learn semantic information necessary for action detection
    and recognition. It has a localization component that utilizes the action representation
    captured by the capsules for a pixel-wise localization of actions. Recently, inspired
    by DETR [[69](#bib.bib69)], Zhao et al. [[70](#bib.bib70)] proposed a transformer-based
    framework (termed as TubeR) that directly detects an action tubelet in a video
    by simultaneously performing action localization and recognition from a single
    representation. In TubeR, a tubelet-attention module is devised to model the dynamic
    spatio-temporal nature of a video clip. TubeR learns a set of tubelet queries
    and outputs action tubelets.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 无锚点方法。当前最先进的方法通常依赖于启发式的锚点设计，并在大量预定义的锚框或立方体上操作。尽管这些基于锚点的方法取得了显著的成功，但它们存在关键问题，例如巨大的提议搜索空间和低效率。例如，考虑到
    Faster R-CNN [[13](#bib.bib13)] 用于2D物体检测，它需要为空间尺寸为 $H^{\prime}\times W^{\prime}$
    的特征图提供 $KH^{\prime}W^{\prime}$ 个锚点，其中通常 $K=9$。而对于跨越 $T^{\prime}$ 帧的 tubelet，为了在时空中保持相同的采样，3D
    锚点提议的数量急剧上升至 $(KH^{\prime}W^{\prime})^{T^{\prime}}$。即使对于较小的 $T^{\prime}$ 值，这也是一个巨大的数字。为了避免这个问题，引入了用于
    STAD 的无锚点方法。例如，Li 等人 [[66](#bib.bib66)] 将每个动作实例视为移动点的轨迹，并提出了一种名为 MovingCenter
    Detector（MOC-detector）的模型。它包括三个分支：用于实例中心检测和动作识别的中心分支；用于运动估计的运动分支；用于空间范围检测的框分支。基于
    MOC 框架，Liu 等人 [[67](#bib.bib67)] 提出了一个轻量级的在线动作检测器，该检测器将短期动作动态编码为累积微动作。Duarte 等人 [[68](#bib.bib68)]
    提出了一个用于视频的胶囊网络，称为 VideoCapsuleNet，它结合了 3D 卷积和胶囊来学习动作检测和识别所需的语义信息。它具有一个定位组件，利用胶囊捕获的动作表示进行像素级动作定位。最近，受到
    DETR [[69](#bib.bib69)] 的启发，Zhao 等人 [[70](#bib.bib70)] 提出了一个基于变压器的框架（称为 TubeR），该框架通过从单一表示中同时执行动作定位和识别来直接检测视频中的动作
    tubelet。在 TubeR 中，设计了一个 tubelet-attention 模块以建模视频剪辑的动态时空特性。TubeR 学习了一组 tubelet
    查询并输出动作 tubelets。
- en: Visual Relation Modeling. Recently, the clip-level (or tubelet-level) visual
    relations have been explored to enhance STAD models. Li et al. introduced a long
    short-term relation network, dubbed as LSTR [[71](#bib.bib71)], which captures
    both short-term and long-term relations in a video. To be specific, LSTR first
    produces 3D bounding boxes, i.e., tubelets, in each video clip, and then models
    short-term human-context interactions within each clip through spatio-temporal
    attention mechanism and reasons long-term temporal dynamics across video clips
    via graph convolutional networks in a cascaded manner. In a concurrent work to [[71](#bib.bib71)],
    Zhang et al. [[72](#bib.bib72)] proposed an approach where the actors across the
    video are associated to generate actor tubelets for learning long temporal dependency.
    The features from actor tubelets and object proposals are then used to construct
    a relation graph to model human-object manipulation and human-human interaction
    actions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉关系建模。最近，剪辑级（或管道级）的视觉关系被用来增强 STAD 模型。Li 等人介绍了一种长短期关系网络，称为 LSTR [[71](#bib.bib71)]，它捕捉视频中的短期和长期关系。具体而言，LSTR
    首先在每个视频剪辑中生成 3D 边界框，即管道，然后通过时空注意机制建模每个剪辑中的短期人际上下文交互，并通过图卷积网络以级联的方式推理视频剪辑之间的长期时间动态。在与 [[71](#bib.bib71)]
    同期的工作中，Zhang 等人 [[72](#bib.bib72)] 提出了一个方法，将视频中的演员进行关联，以生成演员管道用于学习长期时间依赖。然后，将来自演员管道和对象提议的特征用于构建关系图，以建模人类-对象操作和人类-人类交互动作。
- en: 3 Linking up the Detection Results
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 链接检测结果
- en: Actions are being performed over a period of time, which usually span many frames
    and clips. After the frame-level or clip-level detection results are obtained,
    most methods adopt a linking algorithm to link the detections across frames or
    clips to produce video-level action tubes. In this section, we briefly review
    the evolution of linking algorithms.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 动作在一段时间内执行，这通常跨越多个帧和剪辑。在获得帧级或剪辑级检测结果后，大多数方法采用链接算法将检测结果跨帧或剪辑进行关联，以生成视频级动作管道。在这一部分，我们简要回顾了链接算法的演变。
- en: 3.1 Linking up Frame-level Detection Boxes
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 链接帧级检测框
- en: The first frame-level action detection linking algorithm is introduced by Gkioxari
    et al. [[16](#bib.bib16)]. They assumed that if the spatial extent of two region
    proposals (i.e., bounding boxes) in adjacent frames have significant overlap and
    their scores are high, they are highly likely to be linked. Formally, suppose
    we have two region proposals $R_{t}$ and $R_{t+1}$ that are located at frame $t$
    and frame $t+1$, respectively. For an action class $c$, Gkioxari et al. [[16](#bib.bib16)]
    defined the linking score between these two region proposals to be
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个帧级动作检测链接算法是由 Gkioxari 等人提出的 [[16](#bib.bib16)]。他们假设，如果相邻帧中两个区域提议（即边界框）的空间范围有显著重叠且它们的得分很高，它们很可能被链接。形式上，假设我们有两个区域提议
    $R_{t}$ 和 $R_{t+1}$，分别位于帧 $t$ 和帧 $t+1$。对于一个动作类别 $c$，Gkioxari 等人 [[16](#bib.bib16)]
    定义这两个区域提议之间的链接得分为
- en: '|  | $s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdot ov(R_{t},R_{t+1}),$
    |  | (1) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdot ov(R_{t},R_{t+1}),$
    |  | (1) |'
- en: where $s_{c}(R_{i})$ is the class specific score of $R_{i}$. $ov(R_{i},R_{j})$
    is the IoU of two region proposals $R_{i}$ and $R_{j}$. $\lambda$ is a scalar
    parameter weighting the relative importance of the IoU term. Notably, some models
    output bounding boxes with associated actionness scores [[22](#bib.bib22)], and
    the class-specific scores in Eq. ([1](#S3.E1 "In 3.1 Linking up Frame-level Detection
    Boxes ‣ 3 Linking up the Detection Results")) are replaced by actionness scores
    in these works.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{c}(R_{i})$ 是 $R_{i}$ 的类别特定得分。$ov(R_{i},R_{j})$ 是两个区域提议 $R_{i}$ 和 $R_{j}$
    的 IoU。$\lambda$ 是一个标量参数，权衡 IoU 项的相对重要性。值得注意的是，一些模型输出带有相关动作性得分的边界框 [[22](#bib.bib22)]，在这些工作中，公式中的类别特定得分被动作性得分所替代。
- en: After all the linking scores are computed, the optimal path is searched by
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算所有链接得分后，通过下述方式搜索最佳路径
- en: '|  | $\bar{R}_{c}^{*}=\underset{\bar{R}}{\operatorname{argmax}}\frac{1}{T}\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1}),$
    |  | (2) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{R}_{c}^{*}=\underset{\bar{R}}{\operatorname{argmax}}\frac{1}{T}\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1}),$
    |  | (2) |'
- en: where $\bar{R}_{c}=[R_{1},R_{2},\ldots,R_{T}]$ is the sequence of linked regions
    for action class $c$. This optimization problem is solved via the Viterbi algorithm.
    After the optimal path is found, the region proposals in $\bar{R}_{c}^{*}$ are
    removed from the set of region proposals and Eq. ([2](#S3.E2 "In 3.1 Linking up
    Frame-level Detection Boxes ‣ 3 Linking up the Detection Results")) is solved
    again [[16](#bib.bib16)]. This process is repeated until the set of region proposals
    is empty. Each path from Eq. ([2](#S3.E2 "In 3.1 Linking up Frame-level Detection
    Boxes ‣ 3 Linking up the Detection Results")) is called an action tube. The score
    of an action tube $\bar{R_{c}}$ is defined as $S_{c}(\bar{R}_{c})=\frac{1}{T}\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1})$.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{R}_{c}=[R_{1},R_{2},\ldots,R_{T}]$ 是动作类别 $c$ 的链接区域序列。这个优化问题通过 Viterbi
    算法解决。找到最佳路径后，将 $\bar{R}_{c}^{*}$ 中的区域提议从区域提议集合中移除，并重新解决公式 ([2](#S3.E2 "In 3.1
    Linking up Frame-level Detection Boxes ‣ 3 Linking up the Detection Results"))
    [[16](#bib.bib16)]。这个过程重复进行，直到区域提议集合为空。公式 ([2](#S3.E2 "In 3.1 Linking up Frame-level
    Detection Boxes ‣ 3 Linking up the Detection Results")) 中的每条路径称为动作管道。动作管道 $\bar{R_{c}}$
    的分数定义为 $S_{c}(\bar{R}_{c})=\frac{1}{T}\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1})$。
- en: 'Based on the thought of Gkioxari et al. [[16](#bib.bib16)], Peng et al. [[19](#bib.bib19)]
    added a threshold function to Eq. ([1](#S3.E1 "In 3.1 Linking up Frame-level Detection
    Boxes ‣ 3 Linking up the Detection Results")) so that the linking score between
    two region proposals becomes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Gkioxari 等人的思路[[16](#bib.bib16)]，Peng 等人[[19](#bib.bib19)] 在公式 ([1](#S3.E1
    "In 3.1 Linking up Frame-level Detection Boxes ‣ 3 Linking up the Detection Results"))
    中添加了一个阈值函数，使得两个区域提议之间的链接分数变为：
- en: '|  | $s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdot ov(R_{t},R_{t+1})\cdot\psi(ov),$
    |  | (3) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdot ov(R_{t},R_{t+1})\cdot\psi(ov),$
    |  | (3) |'
- en: where $\psi(ov)$ is a threshold function defined by $\psi(ov)=1$ if $ov$ is
    larger than $\tau$, $\psi(ov)=0$ otherwise. Peng et al. [[19](#bib.bib19)] experimentally
    observed that, with this threshold function, the linking score is better than
    the one in [[16](#bib.bib16)] and more robust due to the additional overlap constraint.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\psi(ov)$ 是一个阈值函数，当 $ov$ 大于 $\tau$ 时，$\psi(ov)=1$，否则 $\psi(ov)=0$。Peng 等人[[19](#bib.bib19)]
    实验观察到，使用这个阈值函数，链接分数优于[[16](#bib.bib16)]中的分数，并且由于额外的重叠约束更具鲁棒性。
- en: 'Kopuklu et al. [[33](#bib.bib33)] further extended the linking score definition
    as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Kopuklu 等人[[33](#bib.bib33)] 进一步扩展了链接分数的定义如下：
- en: '|  | $\displaystyle s_{c}\left(R_{t},R_{t+1}\right)=$ | $\displaystyle\psi(ov)\cdot\left[s_{c}\left(R_{t}\right)+s_{c}\left(R_{t+1}\right)\right.$
    |  | (4) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s_{c}\left(R_{t},R_{t+1}\right)=$ | $\displaystyle\psi(ov)\cdot\left[s_{c}\left(R_{t}\right)+s_{c}\left(R_{t+1}\right)\right.$
    |  | (4) |'
- en: '|  |  | $\displaystyle+\alpha\cdot s_{c}\left(R_{t}\right)\cdot s_{c}\left(R_{t+1}\right)$
    |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\alpha\cdot s_{c}\left(R_{t}\right)\cdot s_{c}\left(R_{t+1}\right)$
    |  |'
- en: '|  |  | $\displaystyle\left.+\beta\cdot ov\left(R_{t},R_{t+1}\right)\right],$
    |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\left.+\beta\cdot ov\left(R_{t},R_{t+1}\right)\right],$
    |  |'
- en: where $\alpha$, $\beta$ are scalar parameters. The new term $\alpha\cdot s_{c}\left(R_{t}\right)\cdot
    s_{c}\left(R_{t+1}\right)$ takes the dramatic change of scores between two successive
    frames into account and is supposed to improve the performance of video detection
    in experiments [[33](#bib.bib33)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 和 $\beta$ 是标量参数。新项 $\alpha\cdot s_{c}\left(R_{t}\right)\cdot s_{c}\left(R_{t+1}\right)$
    考虑了两个连续帧之间分数的剧烈变化，预计能够在实验中提高视频检测的性能[[33](#bib.bib33)]。
- en: 'Temporal trimming. The above linking algorithms produce action tubes spanning
    the whole video duration. However, human actions usually occupy only a fraction
    of it. In order to determine the temporal extent of an action instance, some works [[11](#bib.bib11),
    [19](#bib.bib19)] proposed temporal trimming methods. Saha et al. [[11](#bib.bib11)]
    restrained the consecutive proposals to have smooth actionness scores. They solved
    an energy maximization problem via dynamic programming. Peng et al. [[19](#bib.bib19)]
    relied on an efficient maximum subarray method: given a video-level action tube
    $\bar{R}$, its ideal temporal extent is from frame $s$ to frame $e$ that satisfies
    the following objective:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 时间修剪。上述链接算法生成的动作管道覆盖了整个视频时长。然而，人类动作通常只占其中的一部分。为了确定动作实例的时间范围，一些研究[[11](#bib.bib11),
    [19](#bib.bib19)] 提出了时间修剪方法。Saha 等人[[11](#bib.bib11)] 限制了连续提议的动作性分数保持平滑。他们通过动态规划解决了一个能量最大化问题。Peng
    等人[[19](#bib.bib19)] 依赖于一种高效的最大子数组方法：给定一个视频级别的动作管道 $\bar{R}$，其理想时间范围是从帧 $s$ 到帧
    $e$，满足以下目标：
- en: '|  | $s_{c}(\bar{R}_{(s,e)}^{\star})=\underset{(s,e)}{\operatorname{argmax}}\{\frac{1}{L_{(s,e)}}\sum_{i=s}^{e}s_{c}(R_{i})-\lambda\frac{&#124;L_{(s,e)}-L_{c}&#124;}{L_{c}}\},$
    |  | (5) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{c}(\bar{R}_{(s,e)}^{\star})=\underset{(s,e)}{\operatorname{argmax}}\{\frac{1}{L_{(s,e)}}\sum_{i=s}^{e}s_{c}(R_{i})-\lambda\frac{&#124;L_{(s,e)}-L_{c}&#124;}{L_{c}}\},$
    |  | (5) |'
- en: where $L_{(s,e)}$ is the action tube length and $L_{c}$ is the average duration
    of class $c$ on the training set.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{(s,e)}$ 是动作管道的长度，$L_{c}$ 是训练集中类别 $c$ 的平均持续时间。
- en: Online action tube generation. Singh et al. [[35](#bib.bib35)] introduced an
    online action tube generation algorithm that incrementally (frame by frame) builds
    multiple action tubes for each action class in parallel. Specifically, for each
    frame $t$ in a video, the per-class non-maximum suppression (NMS) is conducted
    to obtain the top $n$ class-specific detection boxes. At the first frame of the
    video, $n$ action tubes for each class $c$ are initialized by using $n$ detected
    bounding boxes in this frame. Then, the algorithm grows these action tubes over
    time by adding one box at a frame or terminates if no matching boxes are found
    for $k$ consecutive frames [[35](#bib.bib35)]. Finally, each newly updated tube
    is temporally trimmed by performing binary labeling using an online Viterbi algorithm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在线动作管道生成。Singh 等人 [[35](#bib.bib35)] 提出了一个在线动作管道生成算法，该算法逐帧地（逐帧）为每个动作类别并行构建多个动作管道。具体来说，对于视频中的每一帧
    $t$，进行每类的非极大值抑制（NMS）以获得前 $n$ 个类别特定的检测框。在视频的第一帧中，通过使用此帧中的 $n$ 个检测框来初始化每个类别 $c$
    的 $n$ 个动作管道。然后，算法通过在每一帧中添加一个框来扩展这些动作管道，或者如果 $k$ 连续帧没有找到匹配的框，则终止 [[35](#bib.bib35)]。最后，通过使用在线维特比算法对每个新更新的管道进行时间裁剪。
- en: 3.2 Linking up Clip-level Detection Tubelets
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 剪辑级别检测 tubelets 的链接
- en: 'The clip-level tubelet linking algorithms aim to associate a sequence of clip-level
    tubelets into video-level action tubes. They are often derived from frame-level
    box linking algorithms mentioned in the last subsection. For instance, the tubelet
    linking algorithms in [[55](#bib.bib55)] stems from [[16](#bib.bib16)], and their
    intuition is that the content within a tubelet should capture an action and the
    connected tubelets in any two consecutive clips should have a large temporal overlap.
    Therefore, they defined a tubelet’s linking score as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 剪辑级别的 tubelet 链接算法旨在将一系列剪辑级别的 tubelets 关联成视频级别的动作管道。它们通常源于上一小节提到的帧级别框链接算法。例如，[[55](#bib.bib55)]
    中的 tubelet 链接算法源自 [[16](#bib.bib16)]，其直观的理解是 tubelet 中的内容应捕捉一个动作，并且任何两个连续剪辑中的连接
    tubelets 应具有较大的时间重叠。因此，他们定义了 tubelet 的链接评分如下：
- en: '|  | $S=\frac{1}{m}\sum_{i=1}^{m}{Actionness}_{i}+\frac{1}{m-1}\sum_{j=1}^{m-1}{Overlap}_{j,j+1}$
    |  | (6) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $S=\frac{1}{m}\sum_{i=1}^{m}{Actionness}_{i}+\frac{1}{m-1}\sum_{j=1}^{m-1}{Overlap}_{j,j+1}$
    |  | (6) |'
- en: where ${Actionness}_{i}$ denotes the actionness score of the tubelet from the
    $i$-th clip. ${Overlap}_{j,j+1}$ measures the overlap between the linked two proposals
    respectively from the $j$-th and ($j+1$)-th clips, and $m$ is the total number
    of video clips. The overlap between two tubelets is calculated based on the IoU
    of the last frame of the $j$-th tubelet and the first frame of the $(j+1)$-th
    tubelet. After computing the tubelets’ score, [[55](#bib.bib55)] chose a number
    of linked action tubelets with the highest scores in a video.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${Actionness}_{i}$ 表示第 $i$ 个剪辑中 tubelet 的动作评分。${Overlap}_{j,j+1}$ 衡量了第 $j$
    个剪辑和第 ($j+1$) 个剪辑中连接的两个提议之间的重叠情况，而 $m$ 是视频剪辑的总数。两个 tubelets 之间的重叠是基于第 $j$ 个 tubelet
    的最后一帧和第 $(j+1)$ 个 tubelet 的第一帧的 IoU 来计算的。在计算 tubelets 的评分后，[[55](#bib.bib55)]
    选择了视频中具有最高评分的一些连接动作 tubelets。
- en: 'In another work, [[56](#bib.bib56)] extended the linking algorithm of [[35](#bib.bib35)]
    from frame linking to tubelet linking to build action tubes. Its core idea can
    be summarized as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项工作中，[[56](#bib.bib56)] 将 [[35](#bib.bib35)] 的链接算法从帧链接扩展到 tubelet 链接，以构建动作管道。其核心思想可以总结如下：
- en: '1.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Initialization: in the first frame of a video, [[56](#bib.bib56)] started a
    new link for each tubelet. Here, a link refers to a sequence of linked tubelets.'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化：在视频的第一帧中，[[56](#bib.bib56)] 为每个 tubelet 启动了一个新的链接。在这里，链接指的是一系列连接的 tubelets。
- en: '2.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Linking: given a new frame $f$, they extended the existing links with one of
    the tubelet candidates starting at this frame. They selected the tubelet candidate
    that met the following criteria: (a) is not already picked by other links, (b)
    has the highest action score, and (c) its overlap with the link to be extended
    is higher than a given threshold.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 链接：给定一帧新图像$f$，他们使用从该帧开始的管道候选者扩展现有链接。他们选择了符合以下标准的管道候选者：（a）尚未被其他链接选取，（b）具有最高的动作分数，（c）与要扩展的链接的重叠高于给定阈值。
- en: '3.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Termination: for an existing link, if the criteria are not met for more than
    $K$ consecutive frames, the link terminates. $K$ is a given hyperparameter.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 终止：对于现有链接，如果连续$K$帧未满足标准，则链接终止。$K$是给定的超参数。
- en: Due to its simplicity and efficiency, the tubelet linking algorithm of [[56](#bib.bib56)]
    was adopted by later works, such as [[66](#bib.bib66), [70](#bib.bib70)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简洁性和高效性，[[56](#bib.bib56)]的管道链接算法被后续工作所采用，例如[[66](#bib.bib66), [70](#bib.bib70)]。
- en: Temporal trimming. In the tubelet linking algorithm of [[56](#bib.bib56)], the
    initialization and termination steps determine action tubes’ temporal extents,
    but Song et al. [[58](#bib.bib58)] found that it can not thoroughly address the
    temporal location error induced by transitional state, which is defined as ambiguous
    states around but not belong to the target actions. To address this issue, Song
    et al. [[58](#bib.bib58)] proposed a transition-aware classifier that can distinguish
    between transitional states and real actions and thus alleviate the temporal error
    of spatio-temporal action detection. In follow-up work, Zhao et al. [[70](#bib.bib70)]
    tried to avoid misclassification for the transitional states by introducing an
    action switch regression head, which decides whether a box prediction depicts
    the actor performing the action(s). This regression head gives each bounding box
    of a tubelet an action switch score. If the score is higher than a given threshold,
    the box contains an action. Experiments in  [[70](#bib.bib70)] showed that the
    action switch regression head could remarkably reduce the misclassification for
    the transitional states.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 时间修剪。在[[56](#bib.bib56)]的管道链接算法中，初始化和终止步骤决定了动作管道的时间范围，但Song等人[[58](#bib.bib58)]发现这不能彻底解决过渡状态引起的时间位置误差，这定义为周围但不属于目标动作的模糊状态。为了解决这个问题，Song等人[[58](#bib.bib58)]提出了一种过渡感知分类器，可以区分过渡状态和真实动作，从而减轻时空动作检测的时间误差。在后续工作中，Zhao等人[[70](#bib.bib70)]尝试通过引入一个动作切换回归头来避免对过渡状态的误分类，该回归头决定一个框的预测是否描述了演员执行动作。这个回归头给每个管道的边界框一个动作切换分数。如果分数高于给定的阈值，则框包含一个动作。[[70](#bib.bib70)]中的实验表明，动作切换回归头可以显著减少对过渡状态的误分类。
- en: 4 Datasets and Evaluation
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集与评估
- en: 4.1 Benchmark Datasets
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基准数据集
- en: Benchmark datasets play significant role in understanding the comparative and
    absolute strengths and weaknesses of each method. In this section, we first review
    the commonly used video datasets for STAD, and then we conclude this section by
    presenting the summary and comparison of these datasets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集在理解每种方法的相对和绝对优缺点方面发挥着重要作用。在本节中，我们首先回顾了STAD中常用的视频数据集，然后通过总结和比较这些数据集来结束本节。
- en: 4.1.1 Weizmann
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 Weizmann
- en: Weizmann dataset [[73](#bib.bib73)] was recorded using a static camera on a
    uniform background. The actors move horizontally across the frame, maintaining
    consistency in the actor’s size as they perform each action. This dataset comprises
    a total of 90 video clips grouped 10 action classes, such as “walking”, “jogging”,
    and “waving”, performed by 9 different subjects. Each video clip contains multiple
    instances of a single action. The spatial resolution of these videos is 180$\times$144
    pixels, and each clip ranges from 1 second to 5 seconds.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Weizmann数据集[[73](#bib.bib73)]使用静态摄像机在统一背景下录制。演员在画面中水平移动，保持演员的大小一致，进行每个动作。该数据集包括总共90个视频片段，分为10个动作类别，如“走路”、“慢跑”和“挥手”，由9名不同的受试者完成。每个视频片段包含多个单一动作的实例。这些视频的空间分辨率为180$\times$144像素，每个片段的长度从1秒到5秒不等。
- en: 4.1.2 CMU Crowded Videos
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 CMU Crowded Videos
- en: 'The CMU Crowded Videos [[5](#bib.bib5)] contains five actions: pick-up, one-hand
    wave, two-hand wave, push button, and jumping jack. There are 5 training videos
    for each action and 48 test videos. All videos had been scaled such that the spatial
    resolution of each video is 120 $\times$ 160\. The test videos range from 5 to
    37 seconds (166 to 1115 frames). This dataset was recorded with cluttered and
    dynamic backgrounds so that action detection on this dataset is more challenging
    than on Weizmann dataset [[73](#bib.bib73)]. The CMU Crowded Videos dataset was
    densely annotated, providing the spatial and temporal coordinates (x, y, height,
    width, start, and end frames) for specified actions as ground truth.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: CMU Crowded Videos [[5](#bib.bib5)] 包含五种动作：捡起、单手挥手、双手挥手、按按钮和开合跳。每个动作有 5 个训练视频和
    48 个测试视频。所有视频的空间分辨率均为 120 $\times$ 160。测试视频的时长从 5 秒到 37 秒（166 到 1115 帧）。这个数据集录制了背景杂乱和动态的环境，因此在这个数据集上的动作检测比在
    Weizmann 数据集 [[73](#bib.bib73)] 上更具挑战性。CMU Crowded Videos 数据集进行了密集标注，提供了指定动作的空间和时间坐标（x，y，高度，宽度，起始和结束帧）作为真实值。
- en: 4.1.3 MSR Action I and II
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 MSR Action I 和 II
- en: 'The MSR Action dataset I [[74](#bib.bib74)] and II [[75](#bib.bib75)] were
    created by Microsoft Research Group (MSR), and were made publicly available in
    2009 and 2010, respectively. The MSR Action dataset II is an extension of I. Whereas
    the MSR Action dataset I contains 62 action instances in 16 video sequences, the
    MSR Action dataset II contains 203 instances in 54 videos. Each video contains
    multiple actions performed by different individuals. All videos range from 32
    to 76 seconds. Each action instance’s spatial and temporal coordinates are provided,
    allowing the dataset to be used for action detection and recognition. Both datasets
    consist of three action classes: clap, hand wave, and boxing. Similar to the CMU
    Crowded dataset [[5](#bib.bib5)], the MSR Action datasets were created with cluttered
    and dynamic backgrounds.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MSR Action 数据集 I [[74](#bib.bib74)] 和 II [[75](#bib.bib75)] 是由微软研究集团（MSR）创建的，并分别于
    2009 年和 2010 年公开发布。MSR Action 数据集 II 是 I 的扩展。MSR Action 数据集 I 包含 62 个动作实例，分布在
    16 个视频序列中，而 MSR Action 数据集 II 包含 203 个实例，分布在 54 个视频中。每个视频包含多个不同个体执行的动作。所有视频的时长从
    32 秒到 76 秒。每个动作实例的空间和时间坐标都有提供，这使得数据集可以用于动作检测和识别。两个数据集都包含三个动作类别：拍手、挥手和拳击。与 CMU
    Crowded 数据集 [[5](#bib.bib5)] 类似，MSR Action 数据集也是在背景杂乱和动态的环境中创建的。
- en: 4.1.4 J-HMDB
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 J-HMDB
- en: J-HMDB [[76](#bib.bib76)] is a joint-annotated HMDB [[77](#bib.bib77)] dataset
    to better understand and analyze the limitations and identify components of algorithms
    for improvement on overall accuracy on the HMDB dataset. To clarify the description,
    we first briefly introduce the HMDB dataset and then review the J-HMDB dataset.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: J-HMDB [[76](#bib.bib76)] 是一个联合标注的 HMDB [[77](#bib.bib77)] 数据集，用于更好地理解和分析 HMDB
    数据集中的限制，并识别算法的组件以改进 HMDB 数据集的整体准确性。为澄清描述，我们首先简要介绍 HMDB 数据集，然后回顾 J-HMDB 数据集。
- en: HMDB dataset [[77](#bib.bib77)] contain 5 action categories, which are general
    facial actions (e.g.smile, chew), facial actions with object manipulation (e.g.smoke,
    eat), general body movements (e.g.cartwheel, clap hands), body movements with
    object interaction (e.g.brush hair, catch), body movements for human interaction
    (e.g.fencing, hug). Each category contains a minimum of 101 video clips, and the
    dataset contains a total of 6849 video clips distributed in 51 action categories.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: HMDB 数据集 [[77](#bib.bib77)] 包含 5 种动作类别，分别是一般的面部动作（例如：微笑、咀嚼），带物体操作的面部动作（例如：吸烟、吃东西），一般的身体运动（例如：空翻、拍手），带物体互动的身体运动（例如：梳头、抓取），用于人与人互动的身体运动（例如：击剑、拥抱）。每个类别包含至少
    101 个视频片段，整个数据集总共包含 6849 个视频片段，分布在 51 个动作类别中。
- en: 'Table 2: Summary of major STAD datasets.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：主要 STAD 数据集的总结。
- en: '| Dataset | No. Actions | No. Actors | No. Videos | No. Instances | No. Bbox
    | Year | Resource |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 动作数量 | 演员数量 | 视频数量 | 实例数量 | 边框数量 | 年份 | 资源 |'
- en: '| Weizmann [[73](#bib.bib73)] | 10 | 9 | 90 | - | - | 2005 | Actor Staged |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Weizmann [[73](#bib.bib73)] | 10 | 9 | 90 | - | - | 2005 | 演员舞台 |'
- en: '| CMU Crowded Videos [[5](#bib.bib5)] | 5 | 6 | 98 | - | - | 2007 | Actor Staged
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| CMU Crowded Videos [[5](#bib.bib5)] | 5 | 6 | 98 | - | - | 2007 | 演员舞台 |'
- en: '| MSR Action I [[74](#bib.bib74)] | 3 | 10 | 16 | 62 | - | 2009 | Actor Staged
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| MSR Action I [[74](#bib.bib74)] | 3 | 10 | 16 | 62 | - | 2009 | 演员舞台 |'
- en: '| MSR Action II [[75](#bib.bib75)] | 3 | ¿10 | 54 | 203 | - | 2010 | Actor
    Staged |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MSR Action II [[75](#bib.bib75)] | 3 | ¿10 | 54 | 203 | - | 2010 | 演员舞台 |'
- en: '| UCF Sports [[78](#bib.bib78)] | 10 | - | 150 | - | - | 2009 | TV |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| UCF Sports [[78](#bib.bib78)] | 10 | - | 150 | - | - | 2009 | 电视 |'
- en: '| J-HMDB [[76](#bib.bib76)] | 21 | - | 928 | 928 | 32k | 2013 | Movies, YouTube
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| J-HMDB [[76](#bib.bib76)] | 21 | - | 928 | 928 | 32k | 2013 | 电影，YouTube
    |'
- en: '| UCF101-24 [[79](#bib.bib79)] | 24 | - | 3207 | 4458 | 574k | 2015 | YouTube
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| UCF101-24 [[79](#bib.bib79)] | 24 | - | 3207 | 4458 | 574k | 2015 | YouTube
    |'
- en: '| MultiTHUMOS [[80](#bib.bib80)] | 65 | - | 400 | - | - | 2017 | YouTube |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| MultiTHUMOS [[80](#bib.bib80)] | 65 | - | 400 | - | - | 2017 | YouTube |'
- en: '| AVA [[27](#bib.bib27)] | 80 | - | 430 | 386k | 426k | 2018 | Movies, YouTube
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| AVA [[27](#bib.bib27)] | 80 | - | 430 | 386k | 426k | 2018 | 电影，YouTube |'
- en: '| MultiSports [[10](#bib.bib10)] | 66 | - | 3200 | 37701 | 902k | 2021 | YouTube
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| MultiSports [[10](#bib.bib10)] | 66 | - | 3200 | 37701 | 902k | 2021 | YouTube
    |'
- en: J-HMDB [[76](#bib.bib76)] consists of the selected 21 classes of videos from
    the HMDB dataset. These selected videos involve a single individual performing
    the action like brushing hair, jumping, running, etc. There are 36 to 55 clips
    per action class, with each clip containing about 15-40 frames, summing to a total
    of 928 clips in the dataset. Each clip is trimmed such that the first and last
    frames correspond to the beginning and end of an action. The frame resolution
    is 320$\times$240, and the frame rate is 30 fps.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: J-HMDB [[76](#bib.bib76)] 包含从 HMDB 数据集中选取的 21 类视频。这些选定的视频涉及单个人执行如梳头、跳跃、跑步等动作。每个动作类别有
    36 到 55 个片段，每个片段包含大约 15-40 帧，总共有 928 个片段。每个片段被剪辑，使得第一帧和最后一帧对应于动作的开始和结束。帧分辨率为 320$\times$240，帧率为
    30 fps。
- en: 4.1.5 UCF Sports
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 UCF Sports
- en: The UCF Sports [[78](#bib.bib78)] contains 10 actions in the sports domain,
    which are diving, golf swing, kicking, lifting, horseback riding, running, skateboarding,
    swinging on a pommel horse and floor, swinging on parallel bars and walking. All
    the videos contain camera motion and complex backgrounds gathered from the broadcast
    television channels, such as BBC and ESPN video corpus. The UCF Sports dataset
    contains 150 clips and each clip has a frame rate of 10 fps. The spatial resolution
    of the videos ranges from 480$\times$360 to 720$\times$576 and are 2.20 to 14.40
    seconds in duration, averaging 6.39 seconds.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: UCF Sports [[78](#bib.bib78)] 包含体育领域的 10 种动作，分别是跳水、高尔夫挥杆、踢球、举重、骑马、跑步、滑板、在马鞍和地面上摆动、在双杠上摆动和步行。所有视频均包含摄像机运动和复杂的背景，采集自广播电视台，如
    BBC 和 ESPN 视频库。UCF Sports 数据集包含 150 个片段，每个片段的帧率为 10 fps。视频的空间分辨率范围从 480$\times$360
    到 720$\times$576，时长为 2.20 到 14.40 秒，平均为 6.39 秒。
- en: 4.1.6 UCF101-24
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6 UCF101-24
- en: UCF101 dataset [[79](#bib.bib79)] has been widely used in action recognition
    research. It comprises realistic videos collected from Youtube containing 101
    action categories, with 13320 videos in total. UCF101 gives significant diversity
    in actions with large variations in camera motion, object appearance, viewpoint,
    cluttered background and illumination conditions, etc. For the action detection
    task, a subset of 24 action classes and 3207 videos are provided with dense annotations.
    This subset is therefore called UCF101-24. Different from UCF Sports [[78](#bib.bib78)]
    and J-HMDB [[76](#bib.bib76)], in which videos are truncated to actions, videos
    in UCF101-24 are untrimmed.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: UCF101 数据集 [[79](#bib.bib79)] 已广泛用于动作识别研究。它包含从 YouTube 收集的现实视频，涵盖 101 种动作类别，总共有
    13320 个视频。UCF101 在动作方面具有显著的多样性，摄像机运动、物体外观、视角、背景杂乱和光照条件等方面存在较大变化。在动作检测任务中，提供了一个包含
    24 个动作类别和 3207 个视频的子集，并带有详细标注。该子集因此被称为 UCF101-24。不同于 UCF Sports [[78](#bib.bib78)]
    和 J-HMDB [[76](#bib.bib76)] 中被截断为动作的视频，UCF101-24 的视频为未剪辑的。
- en: 4.1.7 THUMOS and MultiTHUMOS
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.7 THUMOS 和 MultiTHUMOS
- en: 'The THMOS series dataset consists of four datasets: THUMOS’13, THUMOS’14, THUMOS’
    15, and MultiTHUMOS. The first three are from THUMOS Challenge took place annually
    from 2013 to 2015 in conjunction with various major conferences in computer vision
    [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)]. All
    of the videos are from UCF101 [[79](#bib.bib79)]. THUMOS datasets consist of 24
    action classes. The length of actions varies significantly, i.e., from less than
    a second to minutes. These datasets contain 13,000 temporally trimmed videos,
    over 1000 temporally untrimmed videos, and over 2500 negative sample videos. These
    videos might contain none, one, or multiple instances of a single or multiple
    action(s). MultiTHUMOS [[80](#bib.bib80)] is an enhanced version of THUMOS. It
    is a dense, multi-class, frame-wise labeled video dataset with 400 videos of 30
    hours and 38,690 annotations of 65 classes. Averagely, it has 1.5 labels per frame
    and 10.5 action classes per video.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: THMOS 系列数据集包括四个数据集：THUMOS’13、THUMOS’14、THUMOS’15 和 MultiTHUMOS。前三个数据集来自于 THUMOS
    Challenge，该挑战每年从 2013 年到 2015 年与计算机视觉领域的多个主要会议同时进行 [[81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [84](#bib.bib84)]。所有视频均来自 UCF101 [[79](#bib.bib79)]。THUMOS 数据集包含
    24 个动作类别。动作长度差异很大，从不到一秒到几分钟。这些数据集包含 13,000 个时间裁剪视频、超过 1000 个未裁剪时间视频和超过 2500 个负样本视频。这些视频可能包含零个、一个或多个单一或多个动作的实例。MultiTHUMOS
    [[80](#bib.bib80)] 是 THUMOS 的增强版。它是一个密集的、多类别、逐帧标注的视频数据集，包含 400 个 30 小时的视频和 38,690
    个 65 类的标注。平均每帧有 1.5 个标签，每个视频有 10.5 个动作类别。
- en: 4.1.8 AVA
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.8 AVA
- en: The Atomic Visual Actions (AVA) dataset is sourced from 430 movies on YouTube.
    Each movie contributes a clip ranging from the 15th to the 30th minute to the
    dataset. Each clip is then partitioned into 897 overlapping 3s segments with a
    stride of 1 second. For each segment, the middle frame is selected as the keyframe.
    In each keyframe, every person is annotated with a bounding box and (possibly
    multiple) actions. The 430 movies are split into 235 training, 64 validation,
    and 131 test movies, roughly a 55:15:30 split, resulting in 211k training, 57k
    validation, and 118k test segments. AVA dataset contains 80 atomic actions covering
    pose actions, person-person interactions, and person-object interactions. 60 actions
    that have at least 25 instances are often adopted for evaluation. This dataset
    has two annotation versions, i.e., v2.1 and v2.2\. The annotation v2.2 is more
    consistent than v2.1.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Atomic Visual Actions (AVA) 数据集来源于 YouTube 上的 430 部电影。每部电影为数据集提供了从第 15 分钟到第
    30 分钟的一个片段。每个片段被划分为 897 个重叠的 3 秒段，步长为 1 秒。每个段中，中间的帧被选为关键帧。在每个关键帧中，每个人都用一个边界框和（可能有多个）动作进行标注。430
    部电影被分为 235 部训练集、64 部验证集和 131 部测试集，大致按 55:15:30 的比例分配，结果是 211k 训练段、57k 验证段和 118k
    测试段。AVA 数据集包含 80 种原子动作，涵盖姿势动作、人际互动和人-物互动。经常用于评估的 60 种动作至少有 25 个实例。此数据集有两个注释版本，即
    v2.1 和 v2.2。注释 v2.2 比 v2.1 更一致。
- en: 4.1.9 MultiSports
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.9 MultiSports
- en: MultiSports dataset is a new STAD dataset recently released by Li et al. [[10](#bib.bib10)].
    The raw video content of this dataset comes from Olympics and World Cup competitions
    on YouTube. It consists of 4 sports (i.e., basketball, volleyball, football, and
    aerobic gymnastics) and 66 action categories. There are 800 clips for each sport,
    3200 clips in total. It consists of 37701 action instances with 902k bounding
    boxes. The instance number of each action category ranges from 3 to 3,477, showing
    the natural long-tailed distribution. Each video is annotated with multiple instances
    of multiple action classes. The average video length is 750 frames. Due to the
    fine granularity of the action labels, the length of each action segment is short,
    with an average length of 24 frames [[61](#bib.bib61)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: MultiSports 数据集是 Li 等人最近发布的一个新的 STAD 数据集 [[10](#bib.bib10)]。该数据集的原始视频内容来自 YouTube
    上的奥运会和世界杯比赛。它包括 4 项运动（即篮球、排球、足球和健美操）和 66 个动作类别。每项运动有 800 个片段，总共 3200 个片段。它包含 37701
    个动作实例和 902k 个边界框。每个动作类别的实例数量范围从 3 到 3,477，显示出自然的长尾分布。每个视频都标注了多个动作类别的多个实例。平均视频长度为
    750 帧。由于动作标签的细粒度，每个动作段的长度较短，平均长度为 24 帧 [[61](#bib.bib61)]。
- en: 4.1.10 Summary and Comparison of STAD Datasets
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.10 STAD 数据集的总结与比较
- en: We summarize the STAD datasets in Table [2](#S4.T2 "Table 2 ‣ 4.1.4 J-HMDB ‣
    4.1 Benchmark Datasets ‣ 4 Datasets and Evaluation"), which shows that as research
    progresses, the datasets supporting STAD studies become increasingly complex in
    terms of the number of action classes, human subjects, annotation boxes, etc.
    For example, the UCF Sports proposed in 2009 has only 10 action classes and 150
    videos, while the MultiSports proposed in 2021 contains 66 actions and 3200 videos.
    Moreover, in order to keep pace with the growing capability of STAD models, recent
    datasets are intentionally made increasingly challenging for action detection
    by adding camera motion, dynamic background, and frequent occlusions. The most
    recent STAD datasets, such as UCF101-24, AVA, and MultiSports, directly collect
    videos from YouTube and hence contain videos we would encounter in the real world.
    Thus, models that perform well in these datasets have great potential for use
    in real-life scenarios.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格[2](#S4.T2 "表 2 ‣ 4.1.4 J-HMDB ‣ 4.1 基准数据集 ‣ 4 数据集和评估")中总结了STAD数据集，这表明随着研究的进展，支持STAD研究的数据集在动作类别、人类主体、注释框等方面变得越来越复杂。例如，2009年提出的UCF
    Sports只有10个动作类别和150个视频，而2021年提出的MultiSports包含66个动作和3200个视频。此外，为了跟上STAD模型不断增长的能力，最近的数据集通过增加相机运动、动态背景和频繁的遮挡故意增加了动作检测的挑战。最近的STAD数据集，如UCF101-24、AVA和MultiSports，直接从YouTube收集视频，因此包含了我们在现实世界中会遇到的视频。因此，在这些数据集中表现良好的模型具有在现实场景中使用的巨大潜力。
- en: 'Table 3: Performance comparison on J-HMDB and UCF101-24 datasets. ‘f.mAP’ and
    ‘v.mAP’ denotes frame-mAP and video-mAP respectively. ‘-’ means that the result
    is not available. We report the detection accuracy in percentage. The model abbreviations
    used here refer to the following. I3D: Inflated 3D convolutions [[28](#bib.bib28)].
    S3D(+G): Separable 3D convolutions (with gating) [[85](#bib.bib85)]. NL: Non-local
    networks [[44](#bib.bib44)]. 3D-ResNeXt: [[86](#bib.bib86)]. SF: SlowFast [[30](#bib.bib30)].
    DLA34: [[87](#bib.bib87)].'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在J-HMDB和UCF101-24数据集上的性能比较。‘f.mAP’和‘v.mAP’分别表示帧mAP和视频mAP。‘-’表示结果不可用。我们报告检测准确率的百分比。这里使用的模型缩写指代以下内容：I3D：膨胀3D卷积[[28](#bib.bib28)]。S3D(+G)：可分离3D卷积（带门控）[[85](#bib.bib85)]。NL：非本地网络[[44](#bib.bib44)]。3D-ResNeXt：[[86](#bib.bib86)]。SF：SlowFast[[30](#bib.bib30)]。DLA34：[[87](#bib.bib87)]。
- en: '| Year | Method | Backbone | J-HMDB | UCF101-24 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 方法 | 骨干网络 | J-HMDB | UCF101-24 |'
- en: '|  |  | f.-mAP | v.-mAP | f.-mAP | v.-mAP |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  |  | f.-mAP | v.-mAP | f.-mAP | v.-mAP |'
- en: '|  |  |  | @0.2 | @0.5 | @0.75 | 0.5:0.95 |  | @0.2 | @0.5 | @0.75 | 0.5:0.95
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | @0.2 | @0.5 | @0.75 | 0.5:0.95 |  | @0.2 | @0.5 | @0.75 | 0.5:0.95
    |'
- en: '| 2015 | Gkioxari et al.[[16](#bib.bib16)] | AlexNet | 36.2 | - | 53.3 | -
    | - | - | - | - | - | - |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | Gkioxari et al.[[16](#bib.bib16)] | AlexNet | 36.2 | - | 53.3 | -
    | - | - | - | - | - | - |'
- en: '| 2015 | Weinzaepfel et al.[[17](#bib.bib17)] | AlexNet | 45.8 | 63.1 | 60.7
    | - | - | 35.8 | 51.7 | - | - | - |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | Weinzaepfel et al.[[17](#bib.bib17)] | AlexNet | 45.8 | 63.1 | 60.7
    | - | - | 35.8 | 51.7 | - | - | - |'
- en: '| 2016 | MR-TS R-CNN[[19](#bib.bib19)] | VGG | 58.5 | 74.3 | 73.1 | - | - |
    65.7 | 72.9 | - | - | - |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | MR-TS R-CNN[[19](#bib.bib19)] | VGG | 58.5 | 74.3 | 73.1 | - | - |
    65.7 | 72.9 | - | - | - |'
- en: '| 2016 | H-FCN [[20](#bib.bib20)] | H-FCN | 39.9 | - | 56.4 | - | - | - | -
    | - | - | - |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | H-FCN [[20](#bib.bib20)] | H-FCN | 39.9 | - | 56.4 | - | - | - | -
    | - | - | - |'
- en: '| 2016 | Saha et al. [[11](#bib.bib11)] | VGG | - | 72.6 | 71.5 | 43.3 | 40.0
    | - | 66.7 | 35.9 | 7.9 | 14.4 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Saha et al. [[11](#bib.bib11)] | VGG | - | 72.6 | 71.5 | 43.3 | 40.0
    | - | 66.7 | 35.9 | 7.9 | 14.4 |'
- en: '| 2017 | ACT [[56](#bib.bib56)] | VGG | 65.7 | 74.2 | 73.7 | 52.1 | 44.8 |
    67.1 | 77.2 | 51.4 | 22.7 | 25.0 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | ACT [[56](#bib.bib56)] | VGG | 65.7 | 74.2 | 73.7 | 52.1 | 44.8 |
    67.1 | 77.2 | 51.4 | 22.7 | 25.0 |'
- en: '| 2017 | Singh et al.[[35](#bib.bib35)] | VGG | - | 73.8 | 72.0 | 44.5 | 41.6
    | - | 73.5 | 46.3 | 15.0 | 20.4 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Singh et al.[[35](#bib.bib35)] | VGG | - | 73.8 | 72.0 | 44.5 | 41.6
    | - | 73.5 | 46.3 | 15.0 | 20.4 |'
- en: '| 2017 | Zolfaghari et al. [[29](#bib.bib29)] | C3D | - | 78.2 | 73.5 | - |
    - | - | 47.6 | 26.8 | - | - |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Zolfaghari et al. [[29](#bib.bib29)] | C3D | - | 78.2 | 73.5 | - |
    - | - | 47.6 | 26.8 | - | - |'
- en: '| 2017 | AMTnet [[59](#bib.bib59)] | VGG | - | - | - | - | - | - | 79.4 | 51.2
    | 19.0 | 23.4 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | AMTnet [[59](#bib.bib59)] | VGG | - | - | - | - | - | - | 79.4 | 51.2
    | 19.0 | 23.4 |'
- en: '| 2017 | CPLA [[21](#bib.bib21)] | VGG | - | - | - | - | - | - | 73.5 | 37.8
    | - | - |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | CPLA [[21](#bib.bib21)] | VGG | - | - | - | - | - | - | 73.5 | 37.8
    | - | - |'
- en: '| 2017 | T-CNN [[55](#bib.bib55)] | C3D | 61.3 | 78.4 | 76.9 | - | - | 41.4
    | 47.1 | - | - | - |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | T-CNN [[55](#bib.bib55)] | C3D | 61.3 | 78.4 | 76.9 | - | - | 41.4
    | 47.1 | - | - | - |'
- en: '| 2018 | Gu et al.[[27](#bib.bib27)] | I3D | 73.3 | - | 78.6 | - | - | 76.3
    | - | 59.9 | - | - |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Gu et al.[[27](#bib.bib27)] | I3D | 73.3 | - | 78.6 | - | - | 76.3
    | - | 59.9 | - | - |'
- en: '| 2018 | ACRN [[40](#bib.bib40)] | S3D-G | 77.9 | - | 80.1 | - | - | - | -
    | - | - | - |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | ACRN [[40](#bib.bib40)] | S3D-G | 77.9 | - | 80.1 | - | - | - | -
    | - | - | - |'
- en: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | - | 82.7 | 81.3 | - | - | - | 77.9
    | - | - | - |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | - | 82.7 | 81.3 | - | - | - | 77.9
    | - | - | - |'
- en: '| 2018 | TraMNet [[60](#bib.bib60)] | VGG | - | - | - | - | - | - | 79.0 |
    50.9 | 20.1 | 23.9 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | TraMNet [[60](#bib.bib60)] | VGG | - | - | - | - | - | - | 79.0 |
    50.9 | 20.1 | 23.9 |'
- en: '| 2018 | TPN [[62](#bib.bib62)] | VGG | - | 79.7 | 77.0 | - | - | - | 71.7
    | - | - | - |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | TPN [[62](#bib.bib62)] | VGG | - | 79.7 | 77.0 | - | - | - | 71.7
    | - | - | - |'
- en: '| 2018 | Alwando et al.[[23](#bib.bib23)] | VGG | - | 79.9 | 78.3 | - | - |
    - | 72.9 | 41.1 | - | - |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Alwando et al.[[23](#bib.bib23)] | VGG | - | 79.9 | 78.3 | - | - |
    - | 72.9 | 41.1 | - | - |'
- en: '| 2018 | VideoCapsuleNet [[68](#bib.bib68)] | - | 64.6 | 95.1 | - | - | - |
    78.6 | 97.1 | 80.3 | - | - |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | VideoCapsuleNet [[68](#bib.bib68)] | - | 64.6 | 95.1 | - | - | - |
    78.6 | 97.1 | 80.3 | - | - |'
- en: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | - | 86.9 | 85.5 | - | - | - | 83.0
    | 64.4 | - | - |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | - | 86.9 | 85.5 | - | - | - | 83.0
    | 64.4 | - | - |'
- en: '| 2019 | HISAN [[50](#bib.bib50)] | Res101 | - | 87.6 | 86.5 | 53.8 | 51.3
    | - | 82.3 | 51.5 | 23.5 | 24.9 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | HISAN [[50](#bib.bib50)] | Res101 | - | 87.6 | 86.5 | 53.8 | 51.3
    | - | 82.3 | 51.5 | 23.5 | 24.9 |'
- en: '| 2019 | STEP [[65](#bib.bib65)] | VGG | - | - | - | - | - | 75.0 | 76.6 |
    - | - | - |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | STEP [[65](#bib.bib65)] | VGG | - | - | - | - | - | 75.0 | 76.6 |
    - | - | - |'
- en: '| 2019 | TACNet [[58](#bib.bib58)] | VGG | 65.5 | 74.1 | 73.4 | 52.5 | 44.8
    | 72.1 | 77.5 | 52.9 | 21.8 | 24.1 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | TACNet [[58](#bib.bib58)] | VGG | 65.5 | 74.1 | 73.4 | 52.5 | 44.8
    | 72.1 | 77.5 | 52.9 | 21.8 | 24.1 |'
- en: '| 2019 | 2in1 [[34](#bib.bib34)] | VGG | - | - | 74.7 | 53.3 | 45.0 | - | 78.5
    | 50.3 | 22.2 | 24.5 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 2in1 [[34](#bib.bib34)] | VGG | - | - | 74.7 | 53.3 | 45.0 | - | 78.5
    | 50.3 | 22.2 | 24.5 |'
- en: '| 2019 | PCSC [[64](#bib.bib64)] | I3D | 74.8 | 82.6 | 82.2 | 63.1 | 52.8 |
    79.2 | 84.3 | 61.0 | 23.0 | 27.8 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | PCSC [[64](#bib.bib64)] | I3D | 74.8 | 82.6 | 82.2 | 63.1 | 52.8 |
    79.2 | 84.3 | 61.0 | 23.0 | 27.8 |'
- en: '| 2019 | YOWO [[33](#bib.bib33)] | 3D-ResNeXt | 74.4 | 87.8 | 85.7 | 58.1 |
    - | 87.2 | 75.8 | 48.8 | - | - |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | YOWO [[33](#bib.bib33)] | 3D-ResNeXt | 74.4 | 87.8 | 85.7 | 58.1 |
    - | 87.2 | 75.8 | 48.8 | - | - |'
- en: '| 2020 | MOC [[66](#bib.bib66)] | DLA34 | 70.8 | 77.3 | 77.2 | 71.7 | 59.1
    | 78.0 | 82.8 | 53.8 | 29.6 | 28.3 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | MOC [[66](#bib.bib66)] | DLA34 | 70.8 | 77.3 | 77.2 | 71.7 | 59.1
    | 78.0 | 82.8 | 53.8 | 29.6 | 28.3 |'
- en: '| 2020 | AIA [[47](#bib.bib47)] | SF101 | - | - | - | - | - | 78.8 | - | -
    | - | - |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | AIA [[47](#bib.bib47)] | SF101 | - | - | - | - | - | 78.8 | - | -
    | - | - |'
- en: '| 2020 | C-RCNN [[45](#bib.bib45)] | Res50-I3D | 79.2 | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | C-RCNN [[45](#bib.bib45)] | Res50-I3D | 79.2 | - | - | - | - | - |
    - | - | - | - |'
- en: '| 2020 | CFAD [[63](#bib.bib63)] | I3D | - | 86.8 | 85.3 | - | - | 72.5 | 81.6
    | 64.6 | - | 26.7 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | CFAD [[63](#bib.bib63)] | I3D | - | 86.8 | 85.3 | - | - | 72.5 | 81.6
    | 64.6 | - | 26.7 |'
- en: '| 2020 | ACAM [[41](#bib.bib41)] | I3D | 78.9 | - | 83.9 | - | - | - | - |
    - | - | - |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | ACAM [[41](#bib.bib41)] | I3D | 78.9 | - | 83.9 | - | - | - | - |
    - | - | - |'
- en: '| 2020 | Li et al.[[88](#bib.bib88)] | I3D | - | 76.1 | 74.3 | 56.4 | - | -
    | 71.1 | 54.0 | 21.8 | - |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Li et al.[[88](#bib.bib88)] | I3D | - | 76.1 | 74.3 | 56.4 | - | -
    | 71.1 | 54.0 | 21.8 | - |'
- en: '| 2021 | ACDnet [[89](#bib.bib89)] | VGG | - | - | - | - | - | 70.9 | - | -
    | - | - |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | ACDnet [[89](#bib.bib89)] | VGG | - | - | - | - | - | 70.9 | - | -
    | - | - |'
- en: '| 2021 | ACAR-Net [[51](#bib.bib51)] | SF101 | - | - | - | - | - | 84.3 | -
    | - | - | - |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | ACAR-Net [[51](#bib.bib51)] | SF101 | - | - | - | - | - | 84.3 | -
    | - | - | - |'
- en: '| 2021 | SAMOC [[90](#bib.bib90)] | DLA34 | 73.1 | 79.2 | 78.3 | 70.5 | 58.7
    | 79.3 | 80.5 | 53.5 | 30.3 | 28.7 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | SAMOC [[90](#bib.bib90)] | DLA34 | 73.1 | 79.2 | 78.3 | 70.5 | 58.7
    | 79.3 | 80.5 | 53.5 | 30.3 | 28.7 |'
- en: '| 2021 | WOO [[36](#bib.bib36)] | SF101 | 80.5 | - | - | - | - | - | - | -
    | - | - |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | WOO [[36](#bib.bib36)] | SF101 | 80.5 | - | - | - | - | - | - | -
    | - | - |'
- en: '| 2022 | SE-STAD [[37](#bib.bib37)] | SF101-NL | 82.5 | - | - | - | - | - |
    - | - | - | - |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | SE-STAD [[37](#bib.bib37)] | SF101-NL | 82.5 | - | - | - | - | - |
    - | - | - | - |'
- en: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | - | - | - | - | - | 81.5 | - | -
    | - | - |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | - | - | - | - | - | 81.5 | - | -
    | - | - |'
- en: '| 2023 | MRSN [[48](#bib.bib48)] | SF50 | - | - | - | - | - | 80.3 | - | -
    | - | - |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | MRSN [[48](#bib.bib48)] | SF50 | - | - | - | - | - | 80.3 | - | -
    | - | - |'
- en: '| 2022 | TubeR [[70](#bib.bib70)] | I3D | - | 81.8 | 80.7 | - | - | 81.3 |
    85.3 | 60.2 | - | 29.7 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | TubeR [[70](#bib.bib70)] | I3D | - | 81.8 | 80.7 | - | - | 81.3 |
    85.3 | 60.2 | - | 29.7 |'
- en: '| 2023 | HIT [[49](#bib.bib49)] | SF50 | 83.8 | 89.7 | 88.1 | - | - | 84.8
    | 88.8 | 74.3 | - | - |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | HIT [[49](#bib.bib49)] | SF50 | 83.8 | 89.7 | 88.1 | - | - | 84.8
    | 88.8 | 74.3 | - | - |'
- en: '| 2023 | EVAD [[39](#bib.bib39)] | ViT-B | 90.2 | - | - | - | - | 85.1 | -
    | - | - | - |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | EVAD [[39](#bib.bib39)] | ViT-B | 90.2 | - | - | - | - | 85.1 | -
    | - | - | - |'
- en: 'Table 4: Performance comparison on UCF Sports and MultiSports datasets.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: UCF Sports 和 MultiSports 数据集的性能比较。'
- en: '| Year | Method | Backbone | UCF Sports | MultiSports |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Year | Method | Backbone | UCF Sports | MultiSports |'
- en: '| f.-mAP | v.-mAP | f.-mAP | v.-mAP |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| f.-mAP | v.-mAP | f.-mAP | v.-mAP |'
- en: '| @0.2 | @0.5 | @0.75 | 0.5:0.95 | @0.2 | @0.5 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| @0.2 | @0.5 | @0.75 | 0.5:0.95 | @0.2 | @0.5 |'
- en: '| 2015 | Gkioxari et al.[[16](#bib.bib16)] | AlexNet | 68.1 | - | 75.8 | -
    | - | - | - |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | Gkioxari 等人[[16](#bib.bib16)] | AlexNet | 68.1 | - | 75.8 | - | -
    | - | - |  |'
- en: '| 2015 | Weinzaepfel et al.[[17](#bib.bib17)] | AlexNet | 71.9 | - | 90.5 |
    - | - | - | - |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | Weinzaepfel 等人[[17](#bib.bib17)] | AlexNet | 71.9 | - | 90.5 | - |
    - | - | - |  |'
- en: '| 2016 | MR-TS R-CNN[[19](#bib.bib19)] | VGG | 84.5 | 94.8 | 94.7 | - | - |
    - | - |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | MR-TS R-CNN[[19](#bib.bib19)] | VGG | 84.5 | 94.8 | 94.7 | - | - |
    - | - |  |'
- en: '| 2016 | H-FCN [[20](#bib.bib20)] | H-FCN | 82.7 | - | - | - | - | - | - |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | H-FCN [[20](#bib.bib20)] | H-FCN | 82.7 | - | - | - | - | - | - |  |'
- en: '| 2017 | ACT [[56](#bib.bib56)] | VGG | 87.7 | 92.7 | 92.7 | 78.4 | 58.5 |
    - | - |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | ACT [[56](#bib.bib56)] | VGG | 87.7 | 92.7 | 92.7 | 78.4 | 58.5 |
    - | - |  |'
- en: '| 2017 | T-CNN [[55](#bib.bib55)] | C3D | 86.7 | 95.2 | - | - | - | - | - |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | T-CNN [[55](#bib.bib55)] | C3D | 86.7 | 95.2 | - | - | - | - | - |  |'
- en: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | - | 98.6 | 98.6 | - | - | - | -
    |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | - | 98.6 | 98.6 | - | - | - | -
    |  |'
- en: '| 2018 | TPN [[62](#bib.bib62)] | VGG | - | 96.0 | 95.7 | - | - | - | - |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | TPN [[62](#bib.bib62)] | VGG | - | 96.0 | 95.7 | - | - | - | - |  |'
- en: '| 2018 | Alwando et al.[[23](#bib.bib23)] | VGG | - | 94.7 | 94.7 | 89.6 |
    67.5 | - | - |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Alwando 等人[[23](#bib.bib23)] | VGG | - | 94.7 | 94.7 | 89.6 | 67.5
    | - | - |  |'
- en: '| 2018 | VideoCapsuleNet [[68](#bib.bib68)] | - | 83.9 | 97.1 | - | - | - |
    - | - |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | VideoCapsuleNet [[68](#bib.bib68)] | - | 83.9 | 97.1 | - | - | - |
    - | - |  |'
- en: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | - | 98.9 | 98.9 | - | - | - | -
    |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | - | 98.9 | 98.9 | - | - | - | -
    |  |'
- en: '| 2019 | SlowFast [[30](#bib.bib30)] | SF101 | - | - | - | - | - | 27.2 | 24.2
    | 9.7 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | SlowFast [[30](#bib.bib30)] | SF101 | - | - | - | - | - | 27.2 | 24.2
    | 9.7 |'
- en: '| 2019 | 2in1 [[34](#bib.bib34)] | VGG |  |  | 92.7 | 83.4 | - |  |  |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 2in1 [[34](#bib.bib34)] | VGG |  |  | 92.7 | 83.4 | - |  |  |  |'
- en: '| 2019 | YOWO [[33](#bib.bib33)] | 3D-ResNeXt-101 | - | - | - | - | - | 9.3
    | 10.8 | 0.9 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | YOWO [[33](#bib.bib33)] | 3D-ResNeXt-101 | - | - | - | - | - | 9.3
    | 10.8 | 0.9 |'
- en: '| 2020 | MOC [[66](#bib.bib66)] | DLA34 | - | - | - | - | - | 25.2 | 12.9 |
    0.6 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | MOC [[66](#bib.bib66)] | DLA34 | - | - | - | - | - | 25.2 | 12.9 |
    0.6 |'
- en: '| 2020 | CFAD [[63](#bib.bib63)] | I3D | - | 94.5 | 92.7 | - | - | - | - |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | CFAD [[63](#bib.bib63)] | I3D | - | 94.5 | 92.7 | - | - | - | - |  |'
- en: '| 2020 | Li et al.[[88](#bib.bib88)] | I3D | - | 94.3 | 93.8 | 79.5 | - | -
    | - |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Li 等人[[88](#bib.bib88)] | I3D | - | 94.3 | 93.8 | 79.5 | - | - | -
    |  |'
- en: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | - | - | - | - | - | 55.3 | 60.6 |
    37.0 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | - | - | - | - | - | 55.3 | 60.6 |
    37.0 |'
- en: '| 2023 | HIT [[49](#bib.bib49)] | SF50 | - | - | - | - | - | 33.3 | 27.8 |
    8.8 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | HIT [[49](#bib.bib49)] | SF50 | - | - | - | - | - | 33.3 | 27.8 |
    8.8 |'
- en: 4.2 Evaluation Metrics
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标
- en: The performances of STAD methods are often evaluated by two popular metrics,
    i.e., frame and video mean Average Precision (mAP), which are usually denoted
    as frame-mAP and video-mAP.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: STAD 方法的性能通常通过两种常用指标进行评估，即帧平均精度（frame-mAP）和视频平均精度（video-mAP），通常表示为帧-mAP 和视频-mAP。
- en: 4.2.1 Frame-mAP
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 帧-mAP
- en: Frame-mAP measures the area under the precision-recall curve of the bounding
    box detections at each frame. A detection is correct if its IoU with the ground
    truth bounding box is larger than a given threshold and the action label is correctly
    predicted [[16](#bib.bib16)]. The threshold is often set as 0.5\. Frame-mAP allows
    researchers to compare the detection accuracy independently of the linking strategy.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 帧-mAP 衡量的是每帧边界框检测的精确度-召回曲线下的面积。如果检测框与真实边界框的 IoU 大于给定阈值，并且动作标签预测正确，则检测为正确[[16](#bib.bib16)]。阈值通常设为
    0.5。帧-mAP 允许研究人员独立于链接策略比较检测准确性。
- en: 4.2.2 Video-mAP
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 视频-mAP
- en: Video-mAP measures the area under precision-recall curve of the action tube
    predictions. A tube detection is correct if its IoU with the ground truth tube
    is larger than a given threshold and the action label is correctly predicted [[16](#bib.bib16)].
    The IoU between two tubes is defined as the IoU over the temporal domain, multiplied
    by the average of the IoU between boxes averaged over all overlapping frames [[17](#bib.bib17)].
    The threshold for video-mAP is often set as 0.2, 0.5, 0.75, and 0.5:0.95, corresponding
    to the average video-mAP for thresholds with step 0.05 in this range. Whereas
    frame-mAP measures the ability of classification and spatial detection in a single
    frame, video-mAP can further evaluate the performance of temporal detection.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 视频 mAP 衡量动作管道预测的精度-召回曲线下的面积。如果一个管道检测的 IoU 大于给定阈值，并且动作标签被正确预测，则该检测是正确的 [[16](#bib.bib16)]。两个管道之间的
    IoU 被定义为时间域上的 IoU 乘以在所有重叠帧上平均框的 IoU 的平均值 [[17](#bib.bib17)]。视频 mAP 的阈值通常设置为 0.2、0.5、0.75
    和 0.5:0.95，对应于该范围内步长为 0.05 的阈值的平均视频 mAP。虽然帧 mAP 衡量单帧中的分类和空间检测能力，但视频 mAP 可以进一步评估时间检测的性能。
- en: 4.3 Performance Analysis
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 性能分析
- en: In this section, we analyze the performance of the state-of-the-art methods
    for STAD. Since J-HMDB, UCF101-24, UCF Sports, MultiSports, and AVA datasets are
    the most commonly used in deep learning-based STAD, we report the performance
    of state-of-the-art methods on these five datasets.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了最先进的 STAD 方法的性能。由于 J-HMDB、UCF101-24、UCF Sports、MultiSports 和 AVA 数据集是深度学习基础的
    STAD 中最常用的数据集，我们报告了这些最先进方法在这五个数据集上的性能。
- en: Table [3](#S4.T3 "Table 3 ‣ 4.1.10 Summary and Comparison of STAD Datasets ‣
    4.1 Benchmark Datasets ‣ 4 Datasets and Evaluation") presents the performance
    of STAD methods on the J-HMDB and UCF101-24 datasets. VideoCapsuleNet [[68](#bib.bib68)]
    achieved the best results in terms of video-mAP. VideoCapsuleNet is different
    from the mainstream STAD methods. Whereas the mainstream STAD methods usually
    involve box or tubelet proposal generation and linking these proposals, VideoCapsuleNet
    is a 3D capsule network that performs pixel-wise action segmentation along with
    action classification [[68](#bib.bib68)]. This is probably why VideoCapsuleNet
    is excluded by most state-of-the-art methods for performance comparison. Apart
    from VideoCapsuleNet, the transformer-based frameworks, e.g., HIT [[49](#bib.bib49)],
    TubeR [[70](#bib.bib70)] and EVAD [[39](#bib.bib39)], achieved the best performance,
    demonstrating the capability of transformers in STAD. This is further verified
    by the results in Table [4](#S4.T4 "Table 4 ‣ 4.1.10 Summary and Comparison of
    STAD Datasets ‣ 4.1 Benchmark Datasets ‣ 4 Datasets and Evaluation") and Table [5](#S5.T5
    "Table 5 ‣ 5 Future Directions").
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S4.T3 "Table 3 ‣ 4.1.10 Summary and Comparison of STAD Datasets ‣ 4.1
    Benchmark Datasets ‣ 4 Datasets and Evaluation") 展示了 STAD 方法在 J-HMDB 和 UCF101-24
    数据集上的性能。VideoCapsuleNet [[68](#bib.bib68)] 在视频 mAP 方面取得了最佳结果。VideoCapsuleNet 与主流
    STAD 方法不同。主流 STAD 方法通常涉及框或管状体提议生成以及连接这些提议，而 VideoCapsuleNet 是一个 3D 胶囊网络，它执行像素级动作分割和动作分类 [[68](#bib.bib68)]。这可能是为什么
    VideoCapsuleNet 被大多数最先进方法排除在性能比较之外的原因。除了 VideoCapsuleNet，基于变换器的框架，如 HIT [[49](#bib.bib49)]、TubeR [[70](#bib.bib70)]
    和 EVAD [[39](#bib.bib39)]，表现最佳，展示了变换器在 STAD 中的能力。这一点在表 [4](#S4.T4 "Table 4 ‣ 4.1.10
    Summary and Comparison of STAD Datasets ‣ 4.1 Benchmark Datasets ‣ 4 Datasets
    and Evaluation") 和表 [5](#S5.T5 "Table 5 ‣ 5 Future Directions") 的结果中得到了进一步验证。
- en: Table [4](#S4.T4 "Table 4 ‣ 4.1.10 Summary and Comparison of STAD Datasets ‣
    4.1 Benchmark Datasets ‣ 4 Datasets and Evaluation") provides the performance
    of STAD methods on the UCF Sports and MultiSports datasets. The MultiSports dataset
    was constructed very recently, so the reported results on this dataset are fewer
    than other datasets. Besides, the performance on this dataset is relatively low.
    This is probably because MultiSports action instances have large actor displacement,
    and there are often multiple actions in one video, making the detection difficult.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#S4.T4 "Table 4 ‣ 4.1.10 Summary and Comparison of STAD Datasets ‣ 4.1
    Benchmark Datasets ‣ 4 Datasets and Evaluation") 提供了 STAD 方法在 UCF Sports 和 MultiSports
    数据集上的性能。MultiSports 数据集是最近构建的，因此在该数据集上的报告结果少于其他数据集。此外，该数据集上的性能相对较低。这可能是因为 MultiSports
    动作实例具有较大的演员位移，而且一个视频中经常有多个动作，使得检测变得困难。
- en: Table [5](#S5.T5 "Table 5 ‣ 5 Future Directions") shows the results of STAD
    methods on the AVA (v2.1 and v2.2) dataset. The remarkable performance of AIA [[47](#bib.bib47)],
    ACAR-Net [[51](#bib.bib51)], IGMN [[53](#bib.bib53)] and HIT [[49](#bib.bib49)]
    indicates that visual relation and long-term temporal context is critical for
    the model to learn discriminative representation and can improve the detection
    accuracy.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#S5.T5 "Table 5 ‣ 5 Future Directions") 显示了 STAD 方法在 AVA (v2.1 和 v2.2)
    数据集上的结果。AIA [[47](#bib.bib47)]、ACAR-Net [[51](#bib.bib51)]、IGMN [[53](#bib.bib53)]
    和 HIT [[49](#bib.bib49)] 的卓越表现表明，视觉关系和长期时间上下文对于模型学习区分性表示至关重要，并且可以提高检测精度。
- en: 5 Future Directions
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 未来方向
- en: In this section, we discuss the future directions in STAD that might be interesting
    to explore.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了 STAD 可能感兴趣的未来方向。
- en: 'Table 5: Performance comparison on AVA dataset under the metric of frame-mAP
    with threshold of 0.5\. The column ‘Flow’ denotes whether optical flow is used
    as input.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在框架 mAP 度量下，阈值为 0.5 的 AVA 数据集性能比较。列“Flow”表示是否使用光流作为输入。
- en: '| Year | Method | Backbone | Flow | Detector | Pretrained | AVA (v2.1) | AVA
    (v2.2) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 方法 | 骨干网络 | 流 | 检测器 | 预训练 | AVA (v2.1) | AVA (v2.2) |'
- en: '| 2018 | Gu et al.[[27](#bib.bib27)] | I3D | ✓ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 15.8 | - |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Gu et al. [[27](#bib.bib27)] | I3D | ✓ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 15.8 | - |'
- en: '| 2018 | ACRN [[40](#bib.bib40)] | S3D-G | ✓ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 17.4 | - |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | ACRN [[40](#bib.bib40)] | S3D-G | ✓ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 17.4 | - |'
- en: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | ✓ | - | - | 22.3 |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | ✓ | - | - | 22.3 |  |'
- en: '| 2018 | Girdhar et al. [[25](#bib.bib25)] | I3D | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 21.9 | - |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | Girdhar et al. [[25](#bib.bib25)] | I3D | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 21.9 | - |'
- en: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | ✗ | - | - | 25.3 | - |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | ✗ | - | - | 25.3 | - |'
- en: '| 2019 | SlowFast [[30](#bib.bib30)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 28.2 |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | SlowFast [[30](#bib.bib30)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 28.2 |  |'
- en: '| 2019 | STEP [[65](#bib.bib65)] | I3D | ✗ | - | Kinetics-400 | 18.6 | - |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | STEP [[65](#bib.bib65)] | I3D | ✗ | - | Kinetics-400 | 18.6 | - |'
- en: '| 2019 | Girdhar et al.[[46](#bib.bib46)] | I3D | ✗ | - | Kinetics-400 | 24.9
    |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Girdhar et al. [[46](#bib.bib46)] | I3D | ✗ | - | Kinetics-400 | 24.9
    |  |'
- en: '| 2019 | LFB [[24](#bib.bib24)] | Res101-I3D-NL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 27.7 |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | LFB [[24](#bib.bib24)] | Res101-I3D-NL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 27.7 |  |'
- en: '| 2019 | Zhang et al.[[72](#bib.bib72)] | I3D | ✗ | Dave et al. [[91](#bib.bib91)]
    | Kinetics-400 | 22.2 | - |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Zhang et al. [[72](#bib.bib72)] | I3D | ✗ | Dave et al. [[91](#bib.bib91)]
    | Kinetics-400 | 22.2 | - |'
- en: '| 2020 | AIA [[47](#bib.bib47)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | 31.2 | 34.4 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | AIA [[47](#bib.bib47)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | 31.2 | 34.4 |'
- en: '| 2020 | C-RCNN [[45](#bib.bib45)] | Res50-I3D-NL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 28.0 |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | C-RCNN [[45](#bib.bib45)] | Res50-I3D-NL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 28.0 |  |'
- en: '| 2020 | X3D [[31](#bib.bib31)] | X3D-XL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 27.4 |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | X3D [[31](#bib.bib31)] | X3D-XL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 27.4 |  |'
- en: '| 2020 | ACAM [[41](#bib.bib41)] | I3D | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 24.4 |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | ACAM [[41](#bib.bib41)] | I3D | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 24.4 |  |'
- en: '| 2021 | ACAR-Net [[51](#bib.bib51)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 30.0 | - |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | ACAR-Net [[51](#bib.bib51)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 30.0 | - |'
- en: '| 2021 | IGMN [[53](#bib.bib53)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | 30.2 | 33.0 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | IGMN [[53](#bib.bib53)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | 30.2 | 33.0 |'
- en: '| 2021 | WOO [[36](#bib.bib36)] | SF101 | ✗ | Sparse R-CNN [[92](#bib.bib92)]
    | Kinetics-600 | 28.0 | 28.3 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | WOO [[36](#bib.bib36)] | SF101 | ✗ | Sparse R-CNN [[92](#bib.bib92)]
    | Kinetics-600 | 28.0 | 28.3 |'
- en: '| 2021 | STAGE [[52](#bib.bib52)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 29.8 |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | STAGE [[52](#bib.bib52)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 29.8 |  |'
- en: '| 2022 | SE-STAD [[37](#bib.bib37)] | SF101-NL | ✗ | FCOS [[38](#bib.bib38)]
    | Kinetics-600 | 28.8 | 29.3 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | SE-STAD [[37](#bib.bib37)] | SF101-NL | ✗ | FCOS [[38](#bib.bib38)]
    | Kinetics-600 | 28.8 | 29.3 |'
- en: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | ✗ | YOLOv5 [[93](#bib.bib93)] | Kinetics-700
    | 31.8 | - |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | ✗ | YOLOv5 [[93](#bib.bib93)] | Kinetics-700
    | 31.8 | - |'
- en: '| 2022 | TubeR [[70](#bib.bib70)] | CSN-152 | ✗ | - | IG+Kinetics-400 | 32.0
    | 33.6 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 2022 | TubeR [[70](#bib.bib70)] | CSN-152 | ✗ | - | IG+Kinetics-400 | 32.0
    | 33.6 |'
- en: '| 2023 | MRSN [[48](#bib.bib48)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | - | 33.5 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | MRSN [[48](#bib.bib48)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | - | 33.5 |'
- en: '| 2023 | HIT [[49](#bib.bib49)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | - | 32.6 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | HIT [[49](#bib.bib49)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | - | 32.6 |'
- en: '| 2023 | EVAD [[39](#bib.bib39)] | ViT-B | ✗ | - | Kinetics-400 | 31.1 | 32.2
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | EVAD [[39](#bib.bib39)] | ViT-B | ✗ | - | Kinetics-400 | 31.1 | 32.2
    |'
- en: Label-efficient learning for STAD. The annotation for STAD contains not only
    the action class but also the bounding boxes of the actors and the start and end
    of action instances, making the collection and annotation of data expensive and
    time-consuming. To alleviate this burden, some pioneering works, such as [[94](#bib.bib94),
    [95](#bib.bib95)], have been introduced. While [[94](#bib.bib94)] proposed a semi-supervised
    framework to utilize the unlabeled video actions, [[95](#bib.bib95)] presented
    a co-finetuning method to leverage the large-scale action classification datasets
    like Kinetics [[96](#bib.bib96)] and Something-Something v2 [[97](#bib.bib97)].
    Nevertheless, much more effort is needed to explore the label-efficient STAD algorithm.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: STAD的标签高效学习。STAD的标注不仅包括动作类别，还包括演员的边界框以及动作实例的开始和结束，使得数据的收集和标注变得昂贵且耗时。为缓解这一负担，一些开创性工作，如[[94](#bib.bib94),
    [95](#bib.bib95)]，已经提出。虽然[[94](#bib.bib94)]提出了一个半监督框架以利用未标记的视频动作，[[95](#bib.bib95)]则提出了一种共同微调方法，以利用大规模动作分类数据集如Kinetics
    [[96](#bib.bib96)]和Something-Something v2 [[97](#bib.bib97)]。然而，还需要更多努力来探索标签高效的STAD算法。
- en: Online real-time STAD. STAD has numerous online applications, such as surveillance
    and autonomous driving. In these applications, the system must process the newly
    captured frame or clip only based on history and the current data and report the
    detection result as soon as possible. To reach this goal, the model is required
    to be lightweight and efficient. This is a challenging task. Although [[98](#bib.bib98),
    [35](#bib.bib35)] have taken the first step to undertake this task, it is still
    far from fully explored, and more effort is needed.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在线实时STAD。STAD有许多在线应用，例如监控和自动驾驶。在这些应用中，系统必须仅基于历史和当前数据处理新捕获的帧或剪辑，并尽快报告检测结果。为了实现这一目标，模型需要轻量且高效。这是一个具有挑战性的任务。尽管[[98](#bib.bib98),
    [35](#bib.bib35)]已经迈出了第一步，但仍远未完全探索，还需要更多努力。
- en: STAD under large motion. In real scenarios, many actions are with large motion
    because of fast actor displacement, rapid camera motion, etc. STAD under large
    motion is an interesting direction worth exploring.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 大运动下的STAD。在实际场景中，由于演员快速位移、相机快速运动等原因，许多动作涉及大幅度运动。大运动下的STAD是一个值得探索的有趣方向。
- en: Multimodal learning for STAD. On the one hand, action video naturally contains
    multiple modalities, including visual, acoustic, and even linguistic messages
    (e.g., caption). Thus, fully utilizing these modalities via multimodal learning
    for STAD has the potential to achieve better detection accuracy than single-modality
    learning. On the other hand, actions can be captured by various sensors, such
    as depth camera, infrared camera, inertial sensor, and LiDAR. STAD might benefit
    from the fusion of the representations learned from these multimodal data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: STAD的多模态学习。一方面，动作视频自然包含多种模态，包括视觉、声学，甚至语言信息（例如，字幕）。因此，通过多模态学习充分利用这些模态，有潜力比单模态学习实现更好的检测精度。另一方面，动作可以通过各种传感器捕捉，例如深度相机、红外相机、惯性传感器和激光雷达。STAD可能从这些多模态数据学习的表示融合中受益。
- en: Diffusion models for STAD. Diffusion models [[99](#bib.bib99), [100](#bib.bib100),
    [101](#bib.bib101), [102](#bib.bib102)] are a class of generative models that
    starts from the sample in random distribution and recover the data sample via
    a gradual denoising process. They have recently become one of the hottest topics
    in computer vision. Even though they belong to generative model, they are freshly
    demonstrated effective for representative perception tasks, such as object detection [[103](#bib.bib103)]
    and temporal action location [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)].
    Taking as input random spatial boxes (or temporal proposals), the diffusion-based
    models can yield object boxes (or action proposals) accurately. Since STAD can
    be regarded as the combination of object detection and temporal action location,
    these pioneering works [[103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105),
    [106](#bib.bib106)] shed light on leveraging diffusion models for solving STAD
    task.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型用于STAD。扩散模型[[99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)]是一类生成模型，从随机分布的样本开始，通过逐步去噪过程恢复数据样本。最近，它们已成为计算机视觉中的热门话题。尽管它们属于生成模型，但新近证明在代表性感知任务中，如对象检测[[103](#bib.bib103)]和时间动作定位[[104](#bib.bib104),
    [105](#bib.bib105), [106](#bib.bib106)]，有效性显著。以随机空间框（或时间提议）为输入，基于扩散的模型能够准确生成对象框（或动作提议）。由于STAD可以视为对象检测和时间动作定位的结合，这些开创性的工作[[103](#bib.bib103),
    [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)]为利用扩散模型解决STAD任务提供了启示。
- en: 6 Conclusion
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Expedited by the rapid advances of deep neural networks, spatio-temporal action
    detection has made significant progress in recent years. This survey has extensively
    reviewed deep learning-based spatio-temporal action detection methods from different
    aspects, including models, datasets, linking algorithms, performance comparison,
    and future directions. The comparative summary of methods, datasets, and performance
    in pictorial and tabular forms clearly shows their attributes which will benefit
    the interested researchers. We hope this comprehensive survey will foster further
    research in spatio-temporal action detection.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度神经网络的迅速发展，空间时间动作检测在近年来取得了显著进展。本综述从模型、数据集、链接算法、性能比较和未来方向等不同方面广泛回顾了基于深度学习的空间时间动作检测方法。方法、数据集和性能的比较总结以图示和表格形式清晰展示了它们的属性，将对感兴趣的研究者有所帮助。我们希望这篇全面的综述能够促进空间时间动作检测领域的进一步研究。
- en: References
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Tian Y, Sukthankar R, Shah M. Spatiotemporal Deformable Part Models for
    Action Detection. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013,
    2642–2649.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Tian Y, Sukthankar R, Shah M. 用于动作检测的空间时间可变形部件模型。在 *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*，2013年，2642–2649。'
- en: '[2] Yuan J, Liu Z, Wu Y. Discriminative Video Pattern Search for Efficient
    Action Detection. *IEEE Trans. Pattern Anal. Mach. Intell.*, 2011, 33: 1728–1743.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yuan J, Liu Z, Wu Y. 用于高效动作检测的判别视频模式搜索。*IEEE Trans. Pattern Anal. Mach.
    Intell.*，2011年，33: 1728–1743。'
- en: '[3] Siva P, Xiang T. Weakly Supervised Action Detection. In *Proc. Brit. Mach.
    Vis. Conf.*, 2011, 1–14.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Siva P, Xiang T. 弱监督动作检测。在 *Proc. Brit. Mach. Vis. Conf.*，2011年，1–14。'
- en: '[4] Lan T, Wang Y, Mori G. Discriminative figure-centric models for joint action
    localization and recognition. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2011, 2003–2010.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Lan T, Wang Y, Mori G. 用于联合动作定位和识别的判别式图形中心模型。在 *Proc. IEEE Int. Conf. Comput.
    Vis.*，2011年，2003–2010。'
- en: '[5] Ke Y, Sukthankar R, Hebert M. Event detection in crowded videos. In *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2007, 1–8.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ke Y, Sukthankar R, Hebert M. 拥挤视频中的事件检测。在 *Proc. IEEE Int. Conf. Comput.
    Vis.*，2007年，1–8。'
- en: '[6] LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied
    to document recognition. In *Proc. IEEE*, volume 86, 1998, 2278–2324.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] LeCun Y, Bottou L, Bengio Y, Haffner P. 基于梯度的学习应用于文档识别。在 *Proc. IEEE*，第86卷，1998年，2278–2324。'
- en: '[7] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser
    Ł, Polosukhin I. Attention is all you need. In *Proc. Adv. Neural Inf. Process.
    Syst.*, volume 30, 2017, 9662–9673.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser
    Ł, Polosukhin I. 注意力即你所需。在 *Proc. Adv. Neural Inf. Process. Syst.*，第30卷，2017年，9662–9673。'
- en: '[8] Vahdani E, Tian Y. Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey. *IEEE Trans. Pattern Anal. Mach. Intell.*, 2021, 1: 1–20.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Vahdani E, Tian Y. 基于深度学习的未剪辑视频中的动作检测：综述。*IEEE Trans. Pattern Anal. Mach.
    Intell.*，2021年，1: 1–20。'
- en: '[9] Bhoi A. Spatio-temporal action recognition: A survey. *arXiv preprint arXiv:1901.09403*,
    2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Bhoi A. 空间时间动作识别：综述。*arXiv 预印本 arXiv:1901.09403*，2019。'
- en: '[10] Li Y, Chen L, He R, Wang Z, Wu G, Wang L. Multisports: A multi-person
    video dataset of spatio-temporally localized sports actions. In *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2021, 13536–13545.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Li Y, Chen L, He R, Wang Z, Wu G, Wang L. Multisports：一个多人的时空定位运动动作视频数据集。发表于*Proc.
    IEEE Int. Conf. Comput. Vis.*，2021年，第13536–13545页。'
- en: '[11] Saha S, Singh G, Sapienza M, Torr PHS, Cuzzolin F. Deep Learning for Detecting
    Multiple Space-Time Action Tubes in Videos. In *Proc. Brit. Mach. Vis. Conf.*,
    2016, 1–13.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Saha S, Singh G, Sapienza M, Torr PHS, Cuzzolin F. 用于检测视频中多个时空动作管的深度学习。发表于*Proc.
    Brit. Mach. Vis. Conf.*，2016年，第1–13页。'
- en: '[12] Girshick R, Donahue J, Darrell T, Malik J. Rich feature hierarchies for
    accurate object detection and semantic segmentation. In *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2014, 580–587.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Girshick R, Donahue J, Darrell T, Malik J. 用于准确物体检测和语义分割的丰富特征层次结构。发表于*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2014年，第580–587页。'
- en: '[13] Ren S, He K, Girshick R, Sun J. Faster r-cnn: Towards real-time object
    detection with region proposal networks. In *Proc. Adv. Neural Inf. Process. Syst.*,
    2015, 1–14.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ren S, He K, Girshick R, Sun J. Faster r-cnn: 通过区域提议网络实现实时物体检测。发表于*Proc.
    Adv. Neural Inf. Process. Syst.*，2015年，第1–14页。'
- en: '[14] Redmon J, Divvala S, Girshick R, Farhadi A. You only look once: Unified,
    real-time object detection. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2016, 779–788.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Redmon J, Divvala S, Girshick R, Farhadi A. 你只需看一次：统一的实时物体检测。发表于*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2016年，第779–788页。'
- en: '[15] Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC. Ssd: Single
    shot multibox detector. In *Proc. Eur. Conf. Comput. Vis.*, 2016, 21–37.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC. SSD：单次多框检测器。发表于*Proc.
    Eur. Conf. Comput. Vis.*，2016年，第21–37页。'
- en: '[16] Gkioxari G, Malik J. Finding action tubes. In *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2014, 759–768.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Gkioxari G, Malik J. 寻找动作管。发表于*Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*，2014年，第759–768页。'
- en: '[17] Weinzaepfel P, Harchaoui Z, Schmid C. Learning to Track for Spatio-Temporal
    Action Localization. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2015, 3164–3172.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Weinzaepfel P, Harchaoui Z, Schmid C. 学习跟踪以进行时空动作定位。发表于*Proc. IEEE Int.
    Conf. Comput. Vis.*，2015年，第3164–3172页。'
- en: '[18] Zitnick CL, Dollár P. Edge boxes: Locating object proposals from edges.
    In *Proc. Eur. Conf. Comput. Vis.*, 2014, 391–405.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Zitnick CL, Dollár P. 边缘框：从边缘定位物体提议。发表于*Proc. Eur. Conf. Comput. Vis.*，2014年，第391–405页。'
- en: '[19] Peng X, Schmid C. Multi-region Two-Stream R-CNN for Action Detection.
    In *Proc. Eur. Conf. Comput. Vis.*, 2016, 744–759.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Peng X, Schmid C. 多区域双流R-CNN用于动作检测。发表于*Proc. Eur. Conf. Comput. Vis.*，2016年，第744–759页。'
- en: '[20] Wang L, Qiao Y, Tang X, Gool LV. Actionness Estimation Using Hybrid Fully
    Convolutional Networks. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2016, 2708–2717.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Wang L, Qiao Y, Tang X, Gool LV. 使用混合全卷积网络的动作性估计。发表于*Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*，2016年，第2708–2717页。'
- en: '[21] Yang Z, Gao J, Nevatia R. Spatio-Temporal Action Detection with Cascade
    Proposal and Location Anticipation. In *Proc. Brit. Mach. Vis. Conf.*, volume
    1-12, 2017.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Yang Z, Gao J, Nevatia R. 带有级联提议和位置预期的时空动作检测。发表于*Proc. Brit. Mach. Vis.
    Conf.*，第1-12卷，2017年。'
- en: '[22] Li D, Qiu Z, Dai Q, Yao T, Mei T. Recurrent Tubelet Proposal and Recognition
    Networks for Action Detection. In *Proc. Eur. Conf. Comput. Vis.*, 2018, 18–33.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Li D, Qiu Z, Dai Q, Yao T, Mei T. 循环管状提议和识别网络用于动作检测。发表于*Proc. Eur. Conf.
    Comput. Vis.*，2018年，第18–33页。'
- en: '[23] Alwando EHP, Chen YT, Fang WH. CNN-based multiple path search for action
    tube detection in videos. *IEEE Trans. Circuits Syst. Video Technol.*, 2018, 30(1):
    104–116.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Alwando EHP, Chen YT, Fang WH. 基于CNN的多路径搜索用于视频中的动作管检测。*IEEE Trans. Circuits
    Syst. Video Technol.*，2018年，第30卷，第1期：104–116页。'
- en: '[24] Wu CY, Feichtenhofer C, Fan H, He K, Krahenbuhl P, Girshick R. Long-term
    feature banks for detailed video understanding. In *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2019, 284–293.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Wu CY, Feichtenhofer C, Fan H, He K, Krahenbuhl P, Girshick R. 长期特征库用于详细的视频理解。发表于*Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2019年，第284–293页。'
- en: '[25] Girdhar R, Carreira J, Doersch C, Zisserman A. A better baseline for ava.
    *arXiv preprint arXiv:1807.10066*, 2018.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Girdhar R, Carreira J, Doersch C, Zisserman A. 一个更好的AVA基线。*arXiv preprint
    arXiv:1807.10066*，2018年。'
- en: '[26] Ji S, Xu W, Yang M, Yu K. 3D convolutional neural networks for human action
    recognition. *IEEE Trans. Pattern Anal. Mach. Intell.*, 2012, 35(1): 221–231.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ji S, Xu W, Yang M, Yu K. 用于人体动作识别的3D卷积神经网络。*IEEE Trans. Pattern Anal.
    Mach. Intell.*，2012年，第35卷，第1期：221–231页。'
- en: '[27] Gu C, Sun C, Vijayanarasimhan S, Pantofaru C, Ross DA, Toderici G, Li
    Y, Ricco S, Sukthankar R, Schmid C, Malik J. AVA: A Video Dataset of Spatio-Temporally
    Localized Atomic Visual Actions. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, 6047–6056.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Gu C, Sun C, Vijayanarasimhan S, Pantofaru C, Ross DA, Toderici G, Li
    Y, Ricco S, Sukthankar R, Schmid C, Malik J. AVA：一个空间-时间定位的原子视觉动作视频数据集。在 *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*，2018，6047–6056。'
- en: '[28] Carreira J, Zisserman A. Quo vadis, action recognition? a new model and
    the kinetics dataset. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    6299–6308.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Carreira J, Zisserman A. Quo vadis, action recognition? 一个新模型和 Kinetics
    数据集。在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*，2017，6299–6308。'
- en: '[29] Zolfaghari M, Oliveira GL, Sedaghat N, Brox T. Chained multi-stream networks
    exploiting pose, motion, and appearance for action classification and detection.
    In *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, 2904–2913.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Zolfaghari M, Oliveira GL, Sedaghat N, Brox T. 链式多流网络利用姿态、运动和外观进行动作分类和检测。在
    *Proc. IEEE Int. Conf. Comput. Vis.*，2017，2904–2913。'
- en: '[30] Feichtenhofer C, Fan H, Malik J, He K. Slowfast networks for video recognition.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, 6202–6211.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Feichtenhofer C, Fan H, Malik J, He K. SlowFast 网络用于视频识别。在 *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*，2019，6202–6211。'
- en: '[31] Feichtenhofer C. X3d: Expanding architectures for efficient video recognition.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, 203–213.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Feichtenhofer C. X3d：扩展架构以提高视频识别效率。在 *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*，2020，203–213。'
- en: '[32] Liu Y, Yang F, Ginhac D. ACDnet: An Action Detection network for real-time
    edge computing based on flow-guided feature approximation and memory aggregation.
    *Pattern Recognit. Lett.*, 2021, 145: 118–126.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Liu Y, Yang F, Ginhac D. ACDnet：一种基于流引导特征逼近和记忆聚合的实时边缘计算动作检测网络。*Pattern
    Recognit. Lett.*，2021，145: 118–126。'
- en: '[33] Köpüklü O, Wei X, Rigoll G. You only watch once: A unified cnn architecture
    for real-time spatiotemporal action localization. *arXiv preprint arXiv:1911.06644*,
    2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Köpüklü O, Wei X, Rigoll G. 你只看一次：一种统一的 CNN 架构用于实时时空动作定位。*arXiv preprint
    arXiv:1911.06644*，2019。'
- en: '[34] Zhao J, Snoek CG. Dance with flow: Two-in-one stream action detection.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, 9935–9944.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Zhao J, Snoek CG. 随流而舞：双流动作检测。在 *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*，2019，9935–9944。'
- en: '[35] Singh G, Saha S, Sapienza M, Torr PHS, Cuzzolin F. Online Real-Time Multiple
    Spatiotemporal Action Localisation and Prediction. In *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2016, 3657–3666.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Singh G, Saha S, Sapienza M, Torr PHS, Cuzzolin F. 在线实时多时空动作定位与预测。在 *Proc.
    IEEE Int. Conf. Comput. Vis.*，2016，3657–3666。'
- en: '[36] Chen S, Sun P, Xie E, Ge C, Wu J, Ma L, Shen J, Luo P. Watch Only Once:
    An End-to-End Video Action Detection Framework. In *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2021, 8158–8167.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Chen S, Sun P, Xie E, Ge C, Wu J, Ma L, Shen J, Luo P. 仅看一次：一个端到端的视频动作检测框架。在
    *Proc. IEEE Int. Conf. Comput. Vis.*，2021，8158–8167。'
- en: '[37] Sui L, Zhang CL, Gu L, Han F. A Simple and Efficient Pipeline to Build
    an End-to-End Spatial-Temporal Action Detector. *arXiv preprint arXiv:2206.03064*,
    2022.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Sui L, Zhang CL, Gu L, Han F. 一种简单高效的端到端时空动作检测管道。*arXiv preprint arXiv:2206.03064*，2022。'
- en: '[38] Tian Z, Shen C, Chen H, He T. Fcos: Fully convolutional one-stage object
    detection. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, 9627–9636.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Tian Z, Shen C, Chen H, He T. FCOS：全卷积单阶段目标检测。在 *Proc. IEEE Int. Conf.
    Comput. Vis.*，2019，9627–9636。'
- en: '[39] Chen L, Tong Z, Song Y, Wu G, Wang L. Efficient Video Action Detection
    with Token Dropout and Context Refinement. *arXiv preprint arXiv:2304.08451*,
    2023.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Chen L, Tong Z, Song Y, Wu G, Wang L. 使用 Token Dropout 和上下文优化的高效视频动作检测。*arXiv
    preprint arXiv:2304.08451*，2023。'
- en: '[40] Sun C, Shrivastava A, Vondrick C, Murphy KP, Sukthankar R, Schmid C. Actor-Centric
    Relation Network. In *Proc. Eur. Conf. Comput. Vis.*, 2018, 1–17.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Sun C, Shrivastava A, Vondrick C, Murphy KP, Sukthankar R, Schmid C. 以演员为中心的关系网络。在
    *Proc. Eur. Conf. Comput. Vis.*，2018，1–17。'
- en: '[41] Ulutan O, Rallapalli S, Srivatsa M, Torres C, Manjunath B. Actor conditioned
    attention maps for video action detection. In *Proc. IEEE Winter Conf. Appl. Comput.
    Vis.*, 2020, 527–536.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Ulutan O, Rallapalli S, Srivatsa M, Torres C, Manjunath B. 基于演员条件的注意力图用于视频动作检测。在
    *Proc. IEEE Winter Conf. Appl. Comput. Vis.*，2020，527–536。'
- en: '[42] Jiang J, Cao Y, Song L, Zhang S, Li Y, Xu Z, Wu Q, Gan C, Zhang C, Yu
    G. Human centric spatio-temporal action localization. In *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit. Workshop*, 2018.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Jiang J, Cao Y, Song L, Zhang S, Li Y, Xu Z, Wu Q, Gan C, Zhang C, Yu
    G. 以人为中心的时空动作定位。在 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshop*，2018。'
- en: '[43] Ning Z, Xie Q, Zhou W, Wang L, Li H. Person-Context Cross Attention for
    Spatio-Temporal Action Detection. Technical report, Technical report, Huawei Noah’s
    Ark Lab, and University of Science and Technology of China, 2021.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 宁哲, 谢琪, 周伟, 王磊, 李辉。用于时空动作检测的人物-上下文交叉注意力。技术报告，华为诺亚方舟实验室和中国科技大学，2021年。'
- en: '[44] Wang X, Girshick R, Gupta A, He K. Non-local neural networks. In *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 7794–7803.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] 王鑫, Girshick R, Gupta A, 何凯。非局部神经网络。在*IEEE计算机视觉与模式识别会议论文集*，2018年，7794–7803。'
- en: '[45] Wu J, Kuang Z, Wang L, Zhang W, Wu G. Context-aware rcnn: A baseline for
    action detection in videos. In *Proc. Eur. Conf. Comput. Vis.*, 2020, 440–456.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] 吴杰, 邝忠, 王磊, 张伟, 吴刚。上下文感知rcnn：视频动作检测的基线。在*欧洲计算机视觉会议论文集*，2020年，440–456。'
- en: '[46] Girdhar R, Carreira J, Doersch C, Zisserman A. Video Action Transformer
    Network. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, 244–253.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Girdhar R, Carreira J, Doersch C, Zisserman A。视频动作变压器网络。在*IEEE计算机视觉与模式识别会议论文集*，2019年，244–253。'
- en: '[47] Tang J, Xia J, Mu X, Pang B, Lu C. Asynchronous Interaction Aggregation
    for Action Detection. In *Proc. Eur. Conf. Comput. Vis.*, 2020, 22–39.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Tang J, Xia J, Mu X, Pang B, Lu C。异步交互聚合用于动作检测。在*欧洲计算机视觉会议论文集*，2020年，22–39。'
- en: '[48] Zheng YD, Chen G, Yuan M, Lu T. MRSN: Multi-Relation Support Network for
    Video Action Detection. *arXiv preprint arXiv:2304.11975*, 2023.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 郑耀东, 陈光, 袁敏, 陆涛。MRSN：用于视频动作检测的多关系支持网络。*arXiv预印本 arXiv:2304.11975*，2023年。'
- en: '[49] Faure GJ, Chen MH, Lai SH. Holistic Interaction Transformer Network for
    Action Detection. In *Proc. IEEE Winter Conf. Appl. Comput. Vis.*, 2023.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Faure GJ, 陈美华, 赖胜华。全局交互变压器网络用于动作检测。在*IEEE冬季计算机视觉应用会议论文集*，2023年。'
- en: '[50] Pramono RRA, Chen YT, Fang WH. Hierarchical self-attention network for
    action localization in videos. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2019,
    61–70.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Pramono RRA, 陈跃腾, 方伟华。用于视频中动作定位的层次自注意力网络。在*IEEE国际计算机视觉会议论文集*，2019年，61–70。'
- en: '[51] Pan J, Chen S, Shou Z, Shao J, Li H. Actor-Context-Actor Relation Network
    for Spatio-Temporal Action Localization. In *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, 464–474.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 潘俊, 陈晟, 收志勇, 邵杰, 李辉。演员-上下文-演员关系网络用于时空动作定位。在*IEEE计算机视觉与模式识别会议论文集*，2021年，464–474。'
- en: '[52] Tomei M, Baraldi L, Calderara S, Bronzin S, Cucchiara R. Video action
    detection by learning graph-based spatio-temporal interactions. *Comput. Vis.
    Image Understand.*, 2021, 206: 103187–103197.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Tomei M, Baraldi L, Calderara S, Bronzin S, Cucchiara R。通过学习基于图的时空交互进行视频动作检测。*计算机视觉与图像理解*，2021年，206:
    103187–103197。'
- en: '[53] Ni J, Qin J, Huang D. Identity-aware Graph Memory Network for Action Detection.
    In *Proc. ACM Multimedia Conf.*, 2021, 1–9.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 倪建波, 秦峥, 黄达。身份感知图记忆网络用于动作检测。在*ACM多媒体会议论文集*，2021年，1–9。'
- en: '[54] Jain M, Van Gemert J, Jégou H, Bouthemy P, Snoek CG. Action localization
    with tubelets from motion. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, 740–747.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Jain M, Van Gemert J, Jégou H, Bouthemy P, Snoek CG。基于运动的管道动作定位。在*IEEE计算机视觉与模式识别会议论文集*，2014年，740–747。'
- en: '[55] Hou R, Chen C, Shah M. Tube Convolutional Neural Network (T-CNN) for Action
    Detection in Videos. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, 5823–5832.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Hou R, 陈聪, Shah M。用于视频中动作检测的管道卷积神经网络（T-CNN）。在*IEEE国际计算机视觉会议论文集*，2017年，5823–5832。'
- en: '[56] Kalogeiton VS, Weinzaepfel P, Ferrari V, Schmid C. Action Tubelet Detector
    for Spatio-Temporal Action Localization. In *Proc. IEEE Int. Conf. Comput. Vis.*,
    2017, 4415–4423.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Kalogeiton VS, Weinzaepfel P, Ferrari V, Schmid C。动作管道检测器用于时空动作定位。在*IEEE国际计算机视觉会议论文集*，2017年，4415–4423。'
- en: '[57] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale
    image recognition. In *Proc. Int. Conf. Learn. Represent.*, 2015.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Simonyan K, Zisserman A。用于大规模图像识别的非常深的卷积网络。在*国际学习表示会议论文集*，2015年。'
- en: '[58] Song L, Zhang S, Yu G, Sun H. TACNet: Transition-Aware Context Network
    for Spatio-Temporal Action Detection. In *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2019, 11979–11987.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 宋亮, 张帅, 于刚, 孙宏。TACNet：用于时空动作检测的过渡感知上下文网络。在*IEEE计算机视觉与模式识别会议论文集*，2019年，11979–11987。'
- en: '[59] Saha S, Singh G, Cuzzolin F. AMTnet: Action-Micro-Tube Regression by End-to-end
    Trainable Deep Architecture. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, 4424–4433.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Saha S, Singh G, Cuzzolin F。AMTnet：通过端到端可训练的深度架构进行动作-微管回归。在*IEEE国际计算机视觉会议论文集*，2017年，4424–4433。'
- en: '[60] Singh G, Saha S, Cuzzolin F. TraMNet - Transition Matrix Network for Efficient
    Action Tube Proposals. In *Proc. Asian Conf. Comput. Vis.*, 2018, 1–18.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Singh G, Saha S, Cuzzolin F。TraMNet - 用于高效动作管道提议的过渡矩阵网络。在*亚洲计算机视觉会议论文集*，2018年，1–18。'
- en: '[61] Singh G, Choutas V, Saha S, Yu F, Van Gool L. Spatio-Temporal Action Detection
    Under Large Motion. *arXiv preprint arXiv:2209.02250*, 2022.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Singh G, Choutas V, Saha S, Yu F, Van Gool L. 大运动下的时空动作检测。*arXiv preprint
    arXiv:2209.02250*，2022。'
- en: '[62] He J, Deng Z, Ibrahim MS, Mori G. Generic tubelet proposals for action
    localization. In *Proc. IEEE Winter Conf. Appl. Comput. Vis.*, 2018, 343–351.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] He J, Deng Z, Ibrahim MS, Mori G. 动作定位的通用 tubelet 提案。见于 *Proc. IEEE Winter
    Conf. Appl. Comput. Vis.*，2018，343–351。'
- en: '[63] Li Y, Lin W, See J, Xu N, Xu S, Yan K, Yang C. CFAD: Coarse-to-Fine Action
    Detector for Spatiotemporal Action Localization. In *Proc. Eur. Conf. Comput.
    Vis.*, 2020, 40–56.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Li Y, Lin W, See J, Xu N, Xu S, Yan K, Yang C. CFAD：粗到细动作检测器用于时空动作定位。见于
    *Proc. Eur. Conf. Comput. Vis.*，2020，40–56。'
- en: '[64] Su R, Ouyang W, Zhou L, Xu D. Improving action localization by progressive
    cross-stream cooperation. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2019, 12016–12025.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Su R, Ouyang W, Zhou L, Xu D. 通过渐进的跨流合作提高动作定位。见于 *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*，2019，12016–12025。'
- en: '[65] Yang X, Yang X, Liu MY, Xiao F, Davis LS, Kautz J. STEP: Spatio-Temporal
    Progressive Learning for Video Action Detection. In *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, 264–272.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Yang X, Yang X, Liu MY, Xiao F, Davis LS, Kautz J. STEP：空间-时间渐进学习用于视频动作检测。见于
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*，2019，264–272。'
- en: '[66] Li Y, Wang Z, Wang L, Wu G. Actions as Moving Points. In *Proc. Eur. Conf.
    Comput. Vis.*, 2020, 1–21.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Li Y, Wang Z, Wang L, Wu G. 动作作为移动点。见于 *Proc. Eur. Conf. Comput. Vis.*，2020，1–21。'
- en: '[67] Liu Y, Yang F, Ginhac D. Accumulated micro-motion representations for
    lightweight online action detection in real-time. *Journal of Visual Communication
    and Image Representation*, 2023: 103879.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Liu Y, Yang F, Ginhac D. 用于实时轻量级在线动作检测的累积微动作表示。*Journal of Visual Communication
    and Image Representation*，2023：103879。'
- en: '[68] Duarte K, Rawat Y, Shah M. Videocapsulenet: A simplified network for action
    detection. *Proc. Adv. Neural Inf. Process. Syst.*, 2018, 31.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Duarte K, Rawat Y, Shah M. Videocapsulenet：一种简化的动作检测网络。*Proc. Adv. Neural
    Inf. Process. Syst.*，2018，31。'
- en: '[69] Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. End-to-end
    object detection with transformers. In *Proc. Eur. Conf. Comput. Vis.*, 2020,
    213–229.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. 端到端目标检测与
    transformers。见于 *Proc. Eur. Conf. Comput. Vis.*，2020，213–229。'
- en: '[70] Zhao J, Zhang Y, Li X, Chen H, Shuai B, Xu M, Liu C, Kundu K, Xiong Y,
    Modolo D, et al.. TubeR: Tubelet transformer for video action detection. In *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2022, 13598–13607.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Zhao J, Zhang Y, Li X, Chen H, Shuai B, Xu M, Liu C, Kundu K, Xiong Y,
    Modolo D 等。TubeR：用于视频动作检测的 Tubelet transformer。见于 *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*，2022，13598–13607。'
- en: '[71] Li D, Yao T, Qiu Z, Li H, Mei T. Long Short-Term Relation Networks for
    Video Action Detection. In *Proc. ACM Multimedia Conf.*, 2019, 629–637.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Li D, Yao T, Qiu Z, Li H, Mei T. 长短期关系网络用于视频动作检测。见于 *Proc. ACM Multimedia
    Conf.*，2019，629–637。'
- en: '[72] Zhang Y, Tokmakov P, Hebert M, Schmid C. A structured model for action
    detection. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, 9975–9984.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Zhang Y, Tokmakov P, Hebert M, Schmid C. 一个用于动作检测的结构化模型。见于 *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*，2019，9975–9984。'
- en: '[73] Blank M, Gorelick L, Shechtman E, Irani M, Basri R. Actions as space-time
    shapes. In *Proc. IEEE Int. Conf. Comput. Vis.*, volume 2, 2005, 1395–1402.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Blank M, Gorelick L, Shechtman E, Irani M, Basri R. 动作作为时空形状。见于 *Proc.
    IEEE Int. Conf. Comput. Vis.*，第2卷，2005，1395–1402。'
- en: '[74] Yuan J, Liu Z, Wu Y. Discriminative subvolume search for efficient action
    detection. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2009, 2442–2449.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Yuan J, Liu Z, Wu Y. 高效动作检测的区分子体积搜索。见于 *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*，2009，2442–2449。'
- en: '[75] Cao L, Liu Z, Huang TS. Cross-dataset action detection. In *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2010, 1998–2005.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Cao L, Liu Z, Huang TS. 跨数据集动作检测。见于 *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*，2010，1998–2005。'
- en: '[76] Jhuang H, Gall J, Zuffi S, Schmid C, Black MJ. Towards understanding action
    recognition. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2013, 3192–3199.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Jhuang H, Gall J, Zuffi S, Schmid C, Black MJ. 迈向理解动作识别。见于 *Proc. IEEE
    Int. Conf. Comput. Vis.*，2013，3192–3199。'
- en: '[77] Kuehne H, Jhuang H, Garrote E, Poggio T, Serre T. HMDB: a large video
    database for human motion recognition. In *Proc. IEEE Int. Conf. Comput. Vis.*,
    2011, 2556–2563.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Kuehne H, Jhuang H, Garrote E, Poggio T, Serre T. HMDB：一个用于人类动作识别的大型视频数据库。见于
    *Proc. IEEE Int. Conf. Comput. Vis.*，2011，2556–2563。'
- en: '[78] Rodriguez MD, Ahmed J, Shah M. Action mach: a spatio-temporal maximum
    average correlation height filter for action recognition. In *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2008, 1–8.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Rodriguez MD, Ahmed J, Shah M. Action mach：一种用于动作识别的空间-时间最大平均相关高度滤波器。见于
    *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*，2008，1–8。'
- en: '[79] Soomro K, Zamir AR, Shah M. UCF101: A dataset of 101 human actions classes
    from videos in the wild. *arXiv preprint arXiv:1212.0402*, 2012.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Soomro K, Zamir AR, Shah M. UCF101: 来自野外视频的101个人类动作类别数据集。*arXiv preprint
    arXiv:1212.0402*, 2012。'
- en: '[80] Yeung S, Russakovsky O, Jin N, Andriluka M, Mori G, Fei-Fei L. Every Moment
    Counts: Dense Detailed Labeling of Actions in Complex Videos. *Int. J. of Comput.
    Vis.*, 2017.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Yeung S, Russakovsky O, Jin N, Andriluka M, Mori G, Fei-Fei L. 每一刻都很重要：复杂视频中的详细动作标记。*Int.
    J. of Comput. Vis.*, 2017。'
- en: '[81] Idrees H, Zamir AR, Jiang YG, Gorban A, Laptev I, Sukthankar R, Shah M.
    The THUMOS challenge on action recognition for videos “in the wild”. *Comput.
    Vis. Image Understand.*, 2017, 155: 1–23.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Idrees H, Zamir AR, Jiang YG, Gorban A, Laptev I, Sukthankar R, Shah M.
    THUMOS挑战：野外视频中的动作识别。*Comput. Vis. Image Understand.*, 2017, 155: 1–23。'
- en: '[82] Jiang YG, Liu J, Zamir AR, Toderici G, Laptev I, Shah M, Sukthankar R.
    THUMOS challenge: Action recognition with a large number of classes, 2014.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Jiang YG, Liu J, Zamir AR, Toderici G, Laptev I, Shah M, Sukthankar R.
    THUMOS挑战：具有大量类别的动作识别，2014。'
- en: '[83] Gorban A, Idrees H, Jiang YG, Zamir AR, Laptev I, Shah M, Sukthankar R.
    THUMOS challenge: Action recognition with a large number of classes, 2015.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Gorban A, Idrees H, Jiang YG, Zamir AR, Laptev I, Shah M, Sukthankar R.
    THUMOS挑战：具有大量类别的动作识别，2015。'
- en: '[84] Caba Heilbron F, Escorcia V, Ghanem B, Carlos Niebles J. Activitynet:
    A large-scale video benchmark for human activity understanding. In *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2015, 961–970.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Caba Heilbron F, Escorcia V, Ghanem B, Carlos Niebles J. Activitynet:
    大规模视频基准用于人类活动理解。见 *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, 961–970。'
- en: '[85] Xie S, Sun C, Huang J, Tu Z, Murphy K. Rethinking spatiotemporal feature
    learning for video understanding. In *Proc. Eur. Conf. Comput. Vis.*, 2018.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Xie S, Sun C, Huang J, Tu Z, Murphy K. 重新思考视频理解的时空特征学习。见 *Proc. Eur. Conf.
    Comput. Vis.*, 2018。'
- en: '[86] Xie S, Girshick R, Dollár P, Tu Z, He K. Aggregated residual transformations
    for deep neural networks. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2017, 1492–1500.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Xie S, Girshick R, Dollár P, Tu Z, He K. 深度神经网络的聚合残差变换。见 *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, 1492–1500。'
- en: '[87] Yu F, Wang D, Darrell T. Deep Layer Aggregation. In *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, 2403–2412.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Yu F, Wang D, Darrell T. 深层聚合。见 *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2017, 2403–2412。'
- en: '[88] Li Y, Lin W, Wang T, See J, Qian R, Xu N, Wang L, Xu S. Finding action
    tubes with a sparse-to-dense framework. In *Proc. AAAI Conf. Artif. Intell.*,
    volume 34, 2020, 11466–11473.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Li Y, Lin W, Wang T, See J, Qian R, Xu N, Wang L, Xu S. 通过稀疏到密集框架寻找动作管道。见
    *Proc. AAAI Conf. Artif. Intell.*, volume 34, 2020, 11466–11473。'
- en: '[89] Liu Y, Yang F, Ginhac D. ACDnet: An action detection network for real-time
    edge computing based on flow-guided feature approximation and memory aggregation.
    *Pattern Recognit. Lett.*, 2021, 145: 118–126.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Liu Y, Yang F, Ginhac D. ACDnet: 基于流引导特征逼近和记忆聚合的实时边缘计算动作检测网络。*Pattern
    Recognit. Lett.*, 2021, 145: 118–126。'
- en: '[90] Ma XY, Luo Z, Zhang X, Liao Q, Shen X, Wang M. Spatio-Temporal Action
    Detector with Self-Attention. In *Int. Joint Conf. Neural Netw.*, 2021, 1–8.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Ma XY, Luo Z, Zhang X, Liao Q, Shen X, Wang M. 带有自注意力的时空动作检测器。见 *Int.
    Joint Conf. Neural Netw.*, 2021, 1–8。'
- en: '[91] Dave A, Tokmakov P, Ramanan D. Towards segmenting anything that moves.
    In *Proc. IEEE Int. Conf. Comput. Vis. Workshops*, 2019, 0–10.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Dave A, Tokmakov P, Ramanan D. 迈向分割所有移动的目标。见 *Proc. IEEE Int. Conf. Comput.
    Vis. Workshops*, 2019, 0–10。'
- en: '[92] Sun P, Zhang R, Jiang Y, Kong T, Xu C, Zhan W, Tomizuka M, Li L, Yuan
    Z, Wang C, et al.. Sparse r-cnn: End-to-end object detection with learnable proposals.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, 14454–14463.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Sun P, Zhang R, Jiang Y, Kong T, Xu C, Zhan W, Tomizuka M, Li L, Yuan
    Z, Wang C, et al.. 稀疏 R-CNN: 端到端可学习提议的目标检测。见 *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, 14454–14463。'
- en: '[93] Brostom M. Real-time multi-object tracker using yolov5 and deep sort.
    *[https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch](https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch)*,
    2020.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Brostom M. 使用 yolov5 和 deep sort 的实时多目标跟踪器。*[https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch](https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch)*,
    2020。'
- en: '[94] Zhang H, Zhao X, Wang D. Semi-supervised Learning for Multi-label Video
    Action Detection. In *Proc. ACM Multimedia Conf.*, 2022, 2124–2134.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Zhang H, Zhao X, Wang D. 用于多标签视频动作检测的半监督学习。见 *Proc. ACM Multimedia Conf.*,
    2022, 2124–2134。'
- en: '[95] Arnab A, Xiong X, Gritsenko A, Romijnders R, Djolonga J, Dehghani M, Sun
    C, Lučić M, Schmid C. Beyond Transfer Learning: Co-finetuning for Action Localisation.
    *arXiv preprint arXiv:2207.03807*, 2022.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Arnab A, Xiong X, Gritsenko A, Romijnders R, Djolonga J, Dehghani M, Sun
    C, Lučić M, Schmid C. 超越迁移学习：动作定位的协同微调。*arXiv preprint arXiv:2207.03807*, 2022。'
- en: '[96] Kay W, Carreira J, Simonyan K, Zhang B, Hillier C, Vijayanarasimhan S,
    Viola F, Green T, Back T, Natsev P, et al.. The kinetics human action video dataset.
    *arXiv preprint arXiv:1705.06950*, 2017.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Kay W, Carreira J, Simonyan K, Zhang B, Hillier C, Vijayanarasimhan S,
    Viola F, Green T, Back T, Natsev P, 等等。Kinetics 人体动作视频数据集。*arXiv 预印本 arXiv:1705.06950*，2017。'
- en: '[97] Goyal R, Ebrahimi Kahou S, Michalski V, Materzynska J, Westphal S, Kim
    H, Haenel V, Fruend I, Yianilos P, Mueller-Freitag M, et al.. The” something something”
    video database for learning and evaluating visual common sense. In *Proc. IEEE
    Int. Conf. Comput. Vis.*, 2017, 5842–5850.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Goyal R, Ebrahimi Kahou S, Michalski V, Materzynska J, Westphal S, Kim
    H, Haenel V, Fruend I, Yianilos P, Mueller-Freitag M, 等等。用于学习和评估视觉常识的“something
    something”视频数据库。见 *Proc. IEEE Int. Conf. Comput. Vis.*，2017，5842–5850。'
- en: '[98] Dave I, Scheffer Z, Kumar A, Shiraz S, Rawat YS, Shah M. GabriellaV2:
    Towards better generalization in surveillance videos for Action Detection. In
    *Proc. IEEE Winter Conf. Appl. Comput. Vis. Workshop*, 2022, 122–132.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Dave I, Scheffer Z, Kumar A, Shiraz S, Rawat YS, Shah M. GabriellaV2：在监控视频中实现更好的泛化以进行动作检测。见
    *Proc. IEEE Winter Conf. Appl. Comput. Vis. Workshop*，2022，122–132。'
- en: '[99] Song J, Meng C, Ermon S. Denoising diffusion implicit models. In *Proc.
    Int. Conf. Learn. Represent.*, 2021.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Song J, Meng C, Ermon S. 去噪扩散隐式模型。见 *Proc. Int. Conf. Learn. Represent.*，2021。'
- en: '[100] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models. volume 33,
    2020, 6840–6851.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Ho J, Jain A, Abbeel P. 去噪扩散概率模型。卷33，2020，6840–6851。'
- en: '[101] Song Y, Ermon S. Generative modeling by estimating gradients of the data
    distribution. volume 32, 2019.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Song Y, Ermon S. 通过估计数据分布的梯度进行生成建模。卷32，2019。'
- en: '[102] Song Y, Sohl-Dickstein J, Kingma DP, Kumar A, Ermon S, Poole B. Score-based
    generative modeling through stochastic differential equations. 2021.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Song Y, Sohl-Dickstein J, Kingma DP, Kumar A, Ermon S, Poole B. 通过随机微分方程的基于评分的生成建模。2021。'
- en: '[103] Chen S, Sun P, Song Y, Luo P. Diffusiondet: Diffusion model for object
    detection. *arXiv preprint arXiv:2211.09788*, 2022.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Chen S, Sun P, Song Y, Luo P. Diffusiondet：用于物体检测的扩散模型。*arXiv 预印本 arXiv:2211.09788*，2022。'
- en: '[104] Nag S, Zhu X, Deng J, Song YZ, Xiang T. DiffTAD: Temporal Action Detection
    with Proposal Denoising Diffusion. *arXiv preprint arXiv:2303.14863*, 2023.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Nag S, Zhu X, Deng J, Song YZ, Xiang T. DiffTAD：基于提议去噪扩散的时间动作检测。*arXiv
    预印本 arXiv:2303.14863*，2023。'
- en: '[105] Liu D, Li Q, Dinh A, Jiang T, Shah M, Xu C. Diffusion Action Segmentation.
    *arXiv preprint arXiv:2303.17959*, 2023.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Liu D, Li Q, Dinh A, Jiang T, Shah M, Xu C. 扩散动作分割。*arXiv 预印本 arXiv:2303.17959*，2023。'
- en: '[106] Xu M, Soldan M, Gao J, Liu S, Pérez-Rúa JM, Ghanem B. Boundary-Denoising
    for Video Activity Localization. *arXiv preprint arXiv:2304.02934*, 2023.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Xu M, Soldan M, Gao J, Liu S, Pérez-Rúa JM, Ghanem B. 边界去噪用于视频活动定位。*arXiv
    预印本 arXiv:2304.02934*，2023。'
