- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:37:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2308.01618] 1 Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.01618](https://ar5iv.labs.arxiv.org/html/2308.01618)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \CVMsetup
  prefs: []
  type: TYPE_NORMAL
- en: type = Review Article, doi = s41095-0xx-xxxx-x, title = A Survey on Deep Learning-based
    Spatio-temporal Action Detection, author = Peng Wang¹, Fanwei Zeng², and Yuntao
    Qian¹\cor
  prefs: []
  type: TYPE_NORMAL
- en: ', runauthor = F. A. Author, S. B. Author, T. C. Author, abstract = Spatio-temporal
    action detection (STAD) aims to classify the actions present in a video and localize
    them in space and time. It has become a particularly active area of research in
    computer vision because of its explosively emerging real-world applications, such
    as autonomous driving, visual surveillance, entertainment, etc. Many efforts have
    been devoted in recent years to building a robust and effective framework for
    STAD. This paper provides a comprehensive review of the state-of-the-art deep
    learning-based methods for STAD. Firstly, a taxonomy is developed to organize
    these methods. Next, the linking algorithms, which aim to associate the frame-
    or clip-level detection results together to form action tubes, are reviewed. Then,
    the commonly used benchmark datasets and evaluation metrics are introduced, and
    the performance of state-of-the-art models is compared. At last, this paper is
    concluded, and a set of potential research directions of STAD are discussed. ,
    keywords = Computer vision; deep learning; spatio-temporal action detection, copyright
    = The Author(s),'
  prefs: []
  type: TYPE_NORMAL
- en: '| $1\quad$ | College of Computer Science, Zhejiang University, Hangzhou, Zhejiang
    310007, China. E-mail: P. Wang, pengwang18@zju.edu.cn; Y.-T. Qian, ytqian@zju.edu.cn\cor.
    |'
  prefs: []
  type: TYPE_TB
- en: '| $2\quad$ | Ant Group, Hangzhou, Zhejiang 310007, China. E-mail: fanwei.zfw@antgroup.com.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Manuscript received: 2022-01-01; accepted: 2022-01-01 |'
  prefs: []
  type: TYPE_TB
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1.1 Motivation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spatio-temporal action detection (STAD), which aims to localize actions in space
    and time in long untrimmed videos as well as predict action categories (see Figure [1](#S1.F1
    "Figure 1 ‣ 1.1 Motivation ‣ 1 Introduction")), is an essential and challenging
    task in video understanding. Because of its fundamental role in many applications,
    such as surveillance, sports analysis, robotics and self-driving cars, it has
    attracted a lot of attention and been actively researched in computer vision communities.
  prefs: []
  type: TYPE_NORMAL
- en: Before the prevalence of deep learning, traditional STAD usually involves a
    sliding window approach [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)], such as deformable part models [[1](#bib.bib1)], branch and bound
    approach [[2](#bib.bib2)], etc. Recently, with the significant development of
    deep neural networks, especially the convolutional neural network [[6](#bib.bib6)]
    and transformer [[7](#bib.bib7)], many significant advances in STAD have been
    achieved. These deep learning-based methods outperform the traditional algorithms
    by large margins and continue to improve the state-of-the-art.
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid progress in deep learning-based STAD, a multitude of literature
    is being produced in this field. However, the survey of this area is scarce. Vahdani
    et al. [[8](#bib.bib8)] reviewed the action detection task in untrimmed videos
    with the emphasis on temporal action detection that aims to detect the start and
    end of action instances, and only a brief introduction was given to STAD. Bhoi
    et al. [[9](#bib.bib9)] conducted a survey of STAD, but they focused on introducing
    traditional methods, and only three early deep learning-based approaches were
    reviewed. Thus, the recent development of this area is missing in their survey.
    Overall, there is a lack of comprehensive and in-depth survey in deep learning-based
    STAD, though it is highly needed for further research in this field.
  prefs: []
  type: TYPE_NORMAL
- en: In response, we provide the first review that systematically introduces the
    most recent advances in deep learning-based STAD for interested researchers who
    would like to enter this ever-changing field and experts who want to compare STAD
    models and datasets. We collect abundant resources in this field, including state-of-the-art
    models, linking algorithms, benchmark datasets, performance comparison, etc. We
    hope this survey will facilitate the advancement of STAD.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c735baf28bc1338fdb96392d327a827d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Spatio-temporal action detection classifies and localizes actions
    in space and time. Images are from MultiSports [[10](#bib.bib10)] dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The rest of this survey is organized as follows. Section [2](#S2 "2 Taxonomy
    and Methods") clarifies the categorization of deep learning-based STAD methods.
    Section [3](#S3 "3 Linking up the Detection Results") reviews how the detection
    results in each frame or clip are linked together. Section [4](#S4 "4 Datasets
    and Evaluation") summarizes the benchmark datasets and evaluation metrics and
    compares the performance among state-of-the-art methods. Section [5](#S5 "5 Future
    Directions") points out a set of future directions. Finally, Section [6](#S6 "6
    Conclusion") concludes this survey.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Taxonomy and Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first describe the STAD problem formulation. Then we illustrate
    the taxonomy of deep learning-based STAD methods. Finally, we review these methods
    in detail in Subsection [2.3](#S2.SS3 "2.3 Frame-level Methods ‣ 2 Taxonomy and
    Methods") and [2.4](#S2.SS4 "2.4 Clip-level Methods ‣ 2 Taxonomy and Methods").
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a video of $T$ frames $\{I_{t}\}_{t=1..T}$, the spatio-temporal action
    detection task, sometimes termed action localization or event detection, determines
    what actions occur in this video, and when and where they occur. That is to say,
    STAD models should output the action label $c_{i}\in\mathcal{C}$ ($\mathcal{C}$
    is the set of action classes), as well as a set of bounding boxes (or regions)
    $\{R^{i}_{t}\}_{t=t_{b}..t_{e}}$, where $t_{b}$ is the beginning and $t_{e}$ is
    the end of the predicted action $c_{i}$, and $R^{i}_{t}$ is the detected region
    in frame $I_{t}$. Notably, action recognition and temporal action detection are
    closely related to STAD, but they only determine what or when action occurs (see
    Table [1](#S2.T1 "Table 1 ‣ 2.1 Problem Formulation ‣ 2 Taxonomy and Methods")).
    Thus, they are not as challenging as STAD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison of action recognition, temporal action detection (TAD),
    and spatio-temporal action detection (STAD)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Action class | The start and end of action | Actor localization |'
  prefs: []
  type: TYPE_TB
- en: '| Action recognition | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TAD | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| STAD | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ”✓” means ”need to be determined”. |'
  prefs: []
  type: TYPE_TB
- en: '| ”✗” means ”need not to be determined”. |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/215d5bd00463d98326aeca75ad8cf878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An illustration of the output of the frame-level model and clip-level
    model'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Taxonomy of Deep Learning-based STAD Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A wide variety of deep learning-based STAD methods have been proposed so far.
    These models can be mainly divided into two categories: frame-level and clip-level.
    Whereas frame-level models predict 2D bounding boxes for a frame, the clip-level
    models predict 3D spatio-temporal tubelets for a clip. Fig. [2](#S2.F2 "Figure
    2 ‣ 2.1 Problem Formulation ‣ 2 Taxonomy and Methods") illustrates the output
    paradigms of these two categories of models. To provide an in-depth review of
    deep learning-based STAD models, we subdivide the frame-level and clip-level models
    according to the motivation of model design. Since the motivation behind a model
    often reflects researchers’ insights in the STAD field, we hope this categorization
    can provide a clear way to sort out the methods in this field and give researchers
    inspiration. Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Taxonomy of Deep Learning-based STAD
    Methods ‣ 2 Taxonomy and Methods") illustrates our taxonomy and some representative
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c393ae1d1f3245f073f6fa58b617efd7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Taxonomy of deep learning-based STAD models'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b8ea5510e4dc9f5ccef3d2c6a81be767.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Flowchart of the method proposed by Saha et al. (2016) [[11](#bib.bib11)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Frame-level Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The past decade has witnessed the dramatic development of object detection,
    with numerous models being proposed, such as R-CNN [[12](#bib.bib12)], Faster
    R-CNN [[13](#bib.bib13)], YOLO [[14](#bib.bib14)], SSD [[15](#bib.bib15)], etc.
    Inspired by these advancements, many researchers seek to generalize the object
    detection models to the STAD field. A straightforward generalization is to regard
    the STAD in the video as a set of 2D image detections. Concretely, one applies
    an action detector at each frame independently to produce frame-level 2D bounding
    boxes. Then, 3D action proposals (i.e., action tubes) are generated by associating
    these frame-level detection results using linking or tracking algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Along this research line, Gkioxari et al. [[16](#bib.bib16)] proposed a model
    based on R-CNN [[12](#bib.bib12)]. To incorporate both appearance and motion cues,
    they adopted two-stream architecture, with a spatial stream operating on the RGB
    frames and a temporal stream on the optical flow. Weinzaepfel et al. [[17](#bib.bib17)]
    presented a method that extracts a set of candidate regions at the frame level
    via EdgeBoxes [[18](#bib.bib18)] and then tracks high-scoring proposals throughout
    the video using a tracking-by-detection approach. They determined the temporal
    extension of actions by using a sliding-window approach. Driven by the huge success
    of Faster R-CNN [[13](#bib.bib13)], Saha et al. [[11](#bib.bib11)] introduced
    the first model that replaces the unsupervised region proposal algorithms with
    the region proposal network (RPN) for STAD, as shown in Fig [4](#S2.F4 "Figure
    4 ‣ 2.2 Taxonomy of Deep Learning-based STAD Methods ‣ 2 Taxonomy and Methods").
    They passed RGB and optical-flow images to two separate region proposal networks
    to output detection boxes and action class scores. These appearance and motion-based
    detections were fused and linked up to generate class-specific action tubes. The
    effectiveness of RPN on STAD was further verified by Peng et al. [[19](#bib.bib19)],
    where they found that compared to other proposal generalization methods, RPN achieves
    consistently better results with higher inter-section-over-union (IoU) score.
    Moreover, they experimentally found that stacking optical flow over several frames
    can improve frame-level action detection. Besides these proposal-based action
    detection methods, Wang et al. [[20](#bib.bib20)] introduced H-FCN, a method that
    detects actions based on the estimated actionness maps, where actionness means
    the likelihood of containing a generic action instance at a specific location
    of an image.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal context. The above STAD methods treat each frame independently, ignoring
    the temporal contextual relationships. To overcome this problem, Yang et al. [[21](#bib.bib21)]
    proposed a cascade proposal and location anticipation model, named CPLA, which
    is capable of inferring the movement trend of action occurrences between two frames.
    It uses detected bounding boxes on frame $I_{t}$ to infer the corresponding boxes
    on frame $I_{t+k}$, where $k$ is the anticipation gap. As a follow-up work, Li
    et al. [[22](#bib.bib22)] presented an approach that models the temporal correlations
    of proposals between two consecutive frames to predict movements. Whereas [[21](#bib.bib21)]
    requires running RPN at each frame, [[22](#bib.bib22)] only runs RPN at the first
    frame of a video. In another work, Alwando et al. [[23](#bib.bib23)] proposed
    a video localization refinement scheme to iteratively rectify the potentially
    inaccurate bounding boxes by exploiting the temporal consistency between adjacent
    frames. Wu et al. [[24](#bib.bib24)] presented an insight that actions can become
    clear when relating the target frame to the long-range context. Therefore, they
    proposed a long-term feature bank (LFB) that provides long-term supportive information
    to video models, enabling them to better understand the present.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dbe5cc2da34b5c1a3c11732095cb88b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Architecture of the model proposed by Girdhar et al. (2018) [[25](#bib.bib25)].'
  prefs: []
  type: TYPE_NORMAL
- en: '3D CNN. Apart from the methods mentioned above that capture the motion characteristics
    in videos via optical flow, another group of work adopts 3D convolutional neural
    networks (3D CNNs) [[26](#bib.bib26)] to extract the motion information encoded
    in multiple adjacent frames. Gu et al. [[27](#bib.bib27)] proposed to combine
    inflated 3D convolutional network (I3D) [[28](#bib.bib28)] with Faster R-CNN [[13](#bib.bib13)].
    They first fed input frames to the I3D model to extract 3D feature maps and used
    ResNet-50 model on the keyframe to generate region proposals. Then, they extended
    ROI Pooling to 3D by applying the 2D ROI Pooling at the same spatial location
    over all time steps so that the spatio-temporal feature for each proposal was
    obtained, which was then fed into the classifier for action labeling. Girdhar
    et al. [[25](#bib.bib25)] improved the performance of [[27](#bib.bib27)] by applying
    the I3D features for both proposal generation and classification, see Fig. [5](#S2.F5
    "Figure 5 ‣ 2.3 Frame-level Methods ‣ 2 Taxonomy and Methods"). They passed a
    video clip through the first few blocks of I3D to get a video representation.
    Then, the center frame representation was used to predict potential ‘person’ regions
    using an RPN. The proposals were extended in time by replicating and used to extract
    a feature map for the region using ROI pooling. The feature map was then classified
    into different actions using the two last I3D blocks. Zolfaghari et al. [[29](#bib.bib29)]
    proposed a network architecture that integrates three visual cues for action recognition
    and detection: pose, motion, and raw images. They introduced a Markov chain model
    that adds cues successively to integrate the features. Feichtenhofer et al. [[30](#bib.bib30)]
    presented a SlowFast network involving two pathways: a slow pathway and a fast
    pathway. Whereas the former operates at a low frame rate to capture spatial semantics,
    the latter operates at a high frame rate to extract motion information at fine
    temporal resolution. The fast pathway is intentionally made very lightweight by
    reducing its channel capacity. With these innovative designs, SlowFast achieved
    strong performance for STAD in videos.'
  prefs: []
  type: TYPE_NORMAL
- en: High efficiency and real-time speed. Current stage-of-the-art STAD models usually
    push performance by larger backbone network and more complex architecture and
    training procedures, thereby suffering from heavy computational burden and low
    efficiency. To move beyond such limitations, some researchers strived to design
    efficient and real-time models. Feichtenhofer et al. [[31](#bib.bib31)] presented
    an X3D network that was constructed by progressively expanding a tiny 2D architecture
    along multiple network axes, including space, time, width, and depth. X3D achieved
    state-of-the-art performance while requiring 5.5$\times$ fewer parameters for
    similar accuracy as previous work. Liu et al. [[32](#bib.bib32)] introduced ACDnet
    that performs feature approximation at most frames by exploiting the temporal
    coherence between successive video frames, instead of conducting the time-consuming
    CNN feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/953e4192d0683c75f10962c2dcac8f8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Network architecture of YOWO [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et al. [[34](#bib.bib34)] argued that the two-stream detection network
    based on RGB and flow provides state-of-the-art accuracy at the expense of a large
    model size and heavy computation. Hence, they proposed embedding RGB and optical
    flow into a single stream network. They introduced a motion modulation layer to
    leverage optical flow to modulate RGB features. Their network, called 2in1, has
    half the computation and parameters of a two-stream equivalent while obtaining
    better action detection accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Singh et al. [[35](#bib.bib35)] proposed a method that can perform STAD in
    an online setting and at real-time speed, owing to two major developments: 1)
    they adopted real-time SSD [[15](#bib.bib15)] CNNs to detect actors; 2) they designed
    an online algorithm to incrementally construct action tubes from the frame-level
    detections (see Section [3.1](#S3.SS1 "3.1 Linking up Frame-level Detection Boxes
    ‣ 3 Linking up the Detection Results")).'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the outstanding real-time performance of YOLO [[14](#bib.bib14)],
    Kopuklu et al. put forward a STAD counterpart network, i.e., YOWO [[33](#bib.bib33)],
    which replaces the detect-then-classify two-stage paradigm with the end-to-end
    one-stage paradigm, as shown in Fig. [6](#S2.F6 "Figure 6 ‣ 2.3 Frame-level Methods
    ‣ 2 Taxonomy and Methods"). It outputs the regions of interest (ROIs) and the
    corresponding class prediction simultaneously. Albeit efficient, YOWO adopts two
    backbones, i.e., a 3D CNN for extracting spatio-temporal information and a 2D
    model for extracting spatial features. To combat this limitation, Chen et al. [[36](#bib.bib36)]
    presented a single unified network (i.e., WOO) that only adopts one backbone for
    both actor localization and action classification. Compared to two-backbone STAD
    models, WOO reduces the model complexity by over 50%. However, it suffers from
    a performance drop. To improve the performance, they designed an extra attention-based
    embedding interaction module to obtain more discriminative features. As a follow-up
    work, SE-STAD [[37](#bib.bib37)] achieves stronger performances than WOO [[36](#bib.bib36)]
    with better training strategies and FCOS [[38](#bib.bib38)] as object detector.
    Most recently, Chen et al. proposed a framework for efficient video action detection
    (EVAD) [[39](#bib.bib39)] based on vanilla visual transformers (ViTs). They reduced
    computational costs by dropping out the non-keframe tokens and enhanced the model
    performance by refining scene context.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef0c9095bef3734f72d82320088237ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The framework of ACRN [[40](#bib.bib40)].'
  prefs: []
  type: TYPE_NORMAL
- en: Visual Relation Modeling. Detecting and classifying the action of an actor usually
    depend on its relationships with other actors and objects. Thus, many works strived
    to model the relationship among actors, objects, and scenes conveyed in the video.
    For example, [[40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)] observed that the surrounding
    context provides essential information for understanding actions. Accordingly,
    they proposed a variety of actor-scene interaction models. Concretely, Sun et
    al. [[40](#bib.bib40)] proposed an actor-centric relation network (ACRN) to extract
    pairwise relations from cropped actor features and a global scene feature, see
    Fig. [7](#S2.F7 "Figure 7 ‣ 2.3 Frame-level Methods ‣ 2 Taxonomy and Methods").
    These relation features are then used for action classification. Wu et al. [[45](#bib.bib45)]
    empirically found that action recognition accuracy is highly correlated with the
    resolution of actors. Hence, they cropped and resized image patches around actors
    and fed them into a 3D CNN to extract actor features that were used to interact
    with scene features and long-term features.
  prefs: []
  type: TYPE_NORMAL
- en: The attention-based learning strategies are widely adopted in visual relation
    modeling. For example, non-local network [[44](#bib.bib44)] leverages self-attention
    mechanisms to capture long-range dependencies between different entities. PCCA [[43](#bib.bib43)]
    utilizes a cross-attention mechanism to model relations between person and context
    for action detection. Ultan et al. [[41](#bib.bib41)] proposed an actor-conditioned
    attention maps (ACAM) method that explicitly models the surrounding context and
    generates features from the complete scene by conditioning them on detected actors.
    Similarly, Jiang et al. [[42](#bib.bib42)] designed an actor-target relation network
    to capture the human-context relationships, which was achieved with a non-local
    operation between the ROI and its surrounding regions. Girdhar et al. [[46](#bib.bib46)]
    proposed an action transformer network that can learn spatio-temporal context
    from other human actions and objects in a video clip to localize and classify
    target actions. Their resulting feature embeddings and attention maps were experimentally
    demonstrated to have a semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: There are some works considering multiple interactions. For example, Tang et
    al. [[47](#bib.bib47)] introduced an asynchronous interaction aggregation (AIA)
    network to explore three kinds of interactions, i.e., person-person, person-object,
    and temporal interaction. They made them work cooperatively in a hierarchical
    structure to capture meaningful features. Zheng et al. proposed a network called
    multi-relation support network (MRSN) [[48](#bib.bib48)], which contains an actor-context
    relation encoder and an actor-actor relation encoder to model the actor-context
    and actor-actor relation separately. Then an relation support encoder was deployed
    to compute the supports between the two relations and performs relation-level
    interactions. Faure et al. [[49](#bib.bib49)] proposed a bi-modal holistic interaction
    transformer (HIT) network that comprises an RGB stream and a pose stream. Each
    stream separately models person, object, and hand interactions. The resulting
    features from each stream are then glued using an attentive fusion mechanism,
    and the glued feature is used for action classification. Pramono et al. proposed
    a model called HISAN [[50](#bib.bib50)] that combines the two-stream CNNs with
    hierarchical bidirectional self-attention mechanism to learn the structure relationship
    among key actors and spatial context to improve the localization accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Different from the above methods that only consider direct relations between
    pairs, Pan et al. [[51](#bib.bib51)] proposed an actor-context-actor relation
    (ACAR) network that takes into account indirect higher-order relations established
    upon multiple elements. They designed a high-order relation reasoning operator
    and an actor-context feature bank to fulfil indirect relation reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Graph neural networks (GNNs) can naturally pass information among entity nodes
    and model their relations. Thus, GNN is often adopted for visual relation modeling.
    For instance, Tomei et al. proposed a model called STAGE [[52](#bib.bib52)] that
    explores the spatio-temporal relationships through self-attention on a multi-layer
    graph structure that can connect entities from consecutive clips. Ni et al. [[53](#bib.bib53)]
    proposed an identity-aware graph memory network (IGMN) that highlights the identity
    information of the actors in terms of both long-term and short-term context, so
    that the consistency and distinctness between actors are considered for action
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: Although these frame-level methods have achieved significant results, they do
    not fully explore temporal continuity as they treat video frames as a set of independent
    images. Thus, their results can be sub-optimal. To address this issue, clip-level
    STAD approaches have been proposed, which take as input a sequence of frames and
    directly output detected tubelet proposals (i.e., a short sequence of bounding
    boxes).
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Clip-level Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Taking as input a video clip (i.e., a short video snippet), clip-level models
    directly output 3D spatio-temporal tubelet proposals in this clip. The 3D tubelet
    proposals are formed by a sequence of bounding boxes that tightly bound the actions
    of interest. Then, these tubelet proposals in the successive clips are linked
    together to form complete action tubes. Since being proposed by Jain et al. [[54](#bib.bib54)],
    clip-level approaches have become popular among STAD research communities.
  prefs: []
  type: TYPE_NORMAL
- en: Hou et al. [[55](#bib.bib55)] presented a tube convolutional neural network
    (T-CNN) which explores generalizing faster R-CNN [[13](#bib.bib13)] from 2D image
    regions to 3D video volumes. Concerning the RPN and ROI pooling layer of R-CNN,
    they proposed tube proposal network (TPN) and tube-of-interest (ToI) pooling layer
    in T-CNN for tubelet generation and spatio-temporal feature pooling, respectively.
    Kalogeiton et al. [[56](#bib.bib56)] introduced an action tubelet detector (ACT)
    based on SSD framework [[15](#bib.bib15)], which takes as input a sequence of
    frames and then extracts features from each frame with VGG backbone [[57](#bib.bib57)].
    These features are stacked to predict scores and regress coordinates for the anchor
    cuboids to output final action tubelets. Song et al. [[58](#bib.bib58)] proposed
    a transition-aware context network (TACNet) to distinguish transitional states
    between action and non-action frames so that they could localize the temporal
    boundaries for the target actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large motion. The aforementioned clip-level methods are all based on an underlying
    hypothesis that the 3D anchor proposals are cuboid, i.e., having fixed spatial
    extent across time. Unfortunately, these 3D anchor cuboids can stray far from
    the flexible ground truth action tubes due to large actor displacement, dramatic
    actor body shape deformation, large camera motion, etc., particularly for the
    anchors spanning over a long time. To surpass this limitation, Saha et al. [[59](#bib.bib59)]
    proposed a framework that generates two-frame micro-tubes and then links these
    micro-tubes up into proper action tubes. A year later, they took a further step
    and explored the anchor micro-tube proposal search space via an approximate transition
    matrix estimated based on a hidden Markov model (HMM) formulation [[60](#bib.bib60)].
    Their micro-tube hypothesis generation framework can handle large spatial movements
    in dynamic actors. Four years later, they put forward a new solution to the large-motion
    case: they proposed to track the actors over time and perform temporal feature
    aggregation along the respective tracks to enhance actor feature representation [[61](#bib.bib61)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'He et al. [[62](#bib.bib62)] proposed a method that avoids the 3D cuboid anchor
    hypothesis by performing frame-level actor detection and then linking the detected
    bounding boxes to form class-independent action tubelets, which are fed into the
    temporal understanding module for action classification. Li et al. [[62](#bib.bib62)]
    proposed a similar framework to [[62](#bib.bib62)], but they performed STAD in
    a sparse-to-dense manner: they first generated box proposals at sparsely sampled
    frames, then they obtained the dense tube by interpolating the sparse proposals
    across a given detected time interval.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa36be2281c0ff6bd8e6bb5bcd2ddce9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Overview of CFAD framework [[63](#bib.bib63)].'
  prefs: []
  type: TYPE_NORMAL
- en: Progressive learning. Unlike the models that perform action detection in one
    run, progressive learning approaches iteratively refine the proposals towards
    actions over a few steps. For example, Su et al. [[64](#bib.bib64)] proposed a
    progressive cross-stream cooperation (PCSC) framework to use region proposals
    and features from one stream (i.e., Flow/RGB) to help another stream (i.e., RGB/Flow)
    to improve action localization results in an iterative fashion. Yang et al. [[65](#bib.bib65)]
    proposed a model, named STEP, which progressively refines the pre-defined proposal
    cuboids across time, i.e., the proposals obtained in current step are fed into
    the next step for further refinement. Besides, at each step, the 3D proposals
    are extended in the time dimension to incorporate more adjacent temporal context.
    Similarly, Li et al. [[63](#bib.bib63)] proposed a coarse-to-fine action detector
    (CFAD), which first estimates coarse spatio-temporal action tubes from video streams,
    and then refines the tubes’ location based on key timestamps. As shown in Fig. [8](#S2.F8
    "Figure 8 ‣ 2.4 Clip-level Methods ‣ 2 Taxonomy and Methods"), CFAD contains Coarse
    Module and Refine Module. While the Coarse Module produces initial action tube
    estimation, the Refine Module selectively adjusts the tube location under the
    guidance of key timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anchor free. The current state-of-the-art methods usually depend on heuristic
    anchor design and operate on a vast number of pre-defined anchor boxes or cuboids.
    Although these anchor-based methods have achieved remarkable success, they suffer
    from critical issues such as huge proposal search space and low efficiency. For
    example, considering Faster R-CNN [[13](#bib.bib13)] for 2D object detection,
    it requires $KH^{\prime}W^{\prime}$ anchors for a feature map with spatial size
    $H^{\prime}\times W^{\prime}$, where usually $K=9$. While for a tubelet across
    $T^{\prime}$ frames, the number of 3D anchor proposals soars to $(KH^{\prime}W^{\prime})^{T^{\prime}}$
    to maintain the same sampling in space-time. This is a huge number even for a
    small value of $T^{\prime}$. To avoid this problem, anchor-free methods for STAD
    are introduced. For instance, Li et al. [[66](#bib.bib66)] viewed each action
    instance as a trajectory of moving points and presented a model termed as MovingCenter
    Detector (MOC-detector). It comprises three branches: the center branch for instance
    center detection and action recognition; the movement branch for movement estimation;
    the box branch for spatial extent detection. Based on MOC framework, Liu et al. [[67](#bib.bib67)]
    proposed a lightweight online action detector which encodes short-term action
    dynamics as accumulated micro-motion. Duarte et al. [[68](#bib.bib68)] proposed
    a capsule network for videos, called VideoCapsuleNet, which uses 3D convolutions
    along with capsules to learn semantic information necessary for action detection
    and recognition. It has a localization component that utilizes the action representation
    captured by the capsules for a pixel-wise localization of actions. Recently, inspired
    by DETR [[69](#bib.bib69)], Zhao et al. [[70](#bib.bib70)] proposed a transformer-based
    framework (termed as TubeR) that directly detects an action tubelet in a video
    by simultaneously performing action localization and recognition from a single
    representation. In TubeR, a tubelet-attention module is devised to model the dynamic
    spatio-temporal nature of a video clip. TubeR learns a set of tubelet queries
    and outputs action tubelets.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual Relation Modeling. Recently, the clip-level (or tubelet-level) visual
    relations have been explored to enhance STAD models. Li et al. introduced a long
    short-term relation network, dubbed as LSTR [[71](#bib.bib71)], which captures
    both short-term and long-term relations in a video. To be specific, LSTR first
    produces 3D bounding boxes, i.e., tubelets, in each video clip, and then models
    short-term human-context interactions within each clip through spatio-temporal
    attention mechanism and reasons long-term temporal dynamics across video clips
    via graph convolutional networks in a cascaded manner. In a concurrent work to [[71](#bib.bib71)],
    Zhang et al. [[72](#bib.bib72)] proposed an approach where the actors across the
    video are associated to generate actor tubelets for learning long temporal dependency.
    The features from actor tubelets and object proposals are then used to construct
    a relation graph to model human-object manipulation and human-human interaction
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Linking up the Detection Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Actions are being performed over a period of time, which usually span many frames
    and clips. After the frame-level or clip-level detection results are obtained,
    most methods adopt a linking algorithm to link the detections across frames or
    clips to produce video-level action tubes. In this section, we briefly review
    the evolution of linking algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Linking up Frame-level Detection Boxes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first frame-level action detection linking algorithm is introduced by Gkioxari
    et al. [[16](#bib.bib16)]. They assumed that if the spatial extent of two region
    proposals (i.e., bounding boxes) in adjacent frames have significant overlap and
    their scores are high, they are highly likely to be linked. Formally, suppose
    we have two region proposals $R_{t}$ and $R_{t+1}$ that are located at frame $t$
    and frame $t+1$, respectively. For an action class $c$, Gkioxari et al. [[16](#bib.bib16)]
    defined the linking score between these two region proposals to be
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdot ov(R_{t},R_{t+1}),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $s_{c}(R_{i})$ is the class specific score of $R_{i}$. $ov(R_{i},R_{j})$
    is the IoU of two region proposals $R_{i}$ and $R_{j}$. $\lambda$ is a scalar
    parameter weighting the relative importance of the IoU term. Notably, some models
    output bounding boxes with associated actionness scores [[22](#bib.bib22)], and
    the class-specific scores in Eq. ([1](#S3.E1 "In 3.1 Linking up Frame-level Detection
    Boxes ‣ 3 Linking up the Detection Results")) are replaced by actionness scores
    in these works.
  prefs: []
  type: TYPE_NORMAL
- en: After all the linking scores are computed, the optimal path is searched by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bar{R}_{c}^{*}=\underset{\bar{R}}{\operatorname{argmax}}\frac{1}{T}\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1}),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\bar{R}_{c}=[R_{1},R_{2},\ldots,R_{T}]$ is the sequence of linked regions
    for action class $c$. This optimization problem is solved via the Viterbi algorithm.
    After the optimal path is found, the region proposals in $\bar{R}_{c}^{*}$ are
    removed from the set of region proposals and Eq. ([2](#S3.E2 "In 3.1 Linking up
    Frame-level Detection Boxes ‣ 3 Linking up the Detection Results")) is solved
    again [[16](#bib.bib16)]. This process is repeated until the set of region proposals
    is empty. Each path from Eq. ([2](#S3.E2 "In 3.1 Linking up Frame-level Detection
    Boxes ‣ 3 Linking up the Detection Results")) is called an action tube. The score
    of an action tube $\bar{R_{c}}$ is defined as $S_{c}(\bar{R}_{c})=\frac{1}{T}\sum_{t=1}^{T-1}s_{c}(R_{t},R_{t+1})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the thought of Gkioxari et al. [[16](#bib.bib16)], Peng et al. [[19](#bib.bib19)]
    added a threshold function to Eq. ([1](#S3.E1 "In 3.1 Linking up Frame-level Detection
    Boxes ‣ 3 Linking up the Detection Results")) so that the linking score between
    two region proposals becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{c}(R_{t},R_{t+1})=s_{c}(R_{t})+s_{c}(R_{t+1})+\lambda\cdot ov(R_{t},R_{t+1})\cdot\psi(ov),$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\psi(ov)$ is a threshold function defined by $\psi(ov)=1$ if $ov$ is
    larger than $\tau$, $\psi(ov)=0$ otherwise. Peng et al. [[19](#bib.bib19)] experimentally
    observed that, with this threshold function, the linking score is better than
    the one in [[16](#bib.bib16)] and more robust due to the additional overlap constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kopuklu et al. [[33](#bib.bib33)] further extended the linking score definition
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle s_{c}\left(R_{t},R_{t+1}\right)=$ | $\displaystyle\psi(ov)\cdot\left[s_{c}\left(R_{t}\right)+s_{c}\left(R_{t+1}\right)\right.$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\alpha\cdot s_{c}\left(R_{t}\right)\cdot s_{c}\left(R_{t+1}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\left.+\beta\cdot ov\left(R_{t},R_{t+1}\right)\right],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$, $\beta$ are scalar parameters. The new term $\alpha\cdot s_{c}\left(R_{t}\right)\cdot
    s_{c}\left(R_{t+1}\right)$ takes the dramatic change of scores between two successive
    frames into account and is supposed to improve the performance of video detection
    in experiments [[33](#bib.bib33)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Temporal trimming. The above linking algorithms produce action tubes spanning
    the whole video duration. However, human actions usually occupy only a fraction
    of it. In order to determine the temporal extent of an action instance, some works [[11](#bib.bib11),
    [19](#bib.bib19)] proposed temporal trimming methods. Saha et al. [[11](#bib.bib11)]
    restrained the consecutive proposals to have smooth actionness scores. They solved
    an energy maximization problem via dynamic programming. Peng et al. [[19](#bib.bib19)]
    relied on an efficient maximum subarray method: given a video-level action tube
    $\bar{R}$, its ideal temporal extent is from frame $s$ to frame $e$ that satisfies
    the following objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{c}(\bar{R}_{(s,e)}^{\star})=\underset{(s,e)}{\operatorname{argmax}}\{\frac{1}{L_{(s,e)}}\sum_{i=s}^{e}s_{c}(R_{i})-\lambda\frac{&#124;L_{(s,e)}-L_{c}&#124;}{L_{c}}\},$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $L_{(s,e)}$ is the action tube length and $L_{c}$ is the average duration
    of class $c$ on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Online action tube generation. Singh et al. [[35](#bib.bib35)] introduced an
    online action tube generation algorithm that incrementally (frame by frame) builds
    multiple action tubes for each action class in parallel. Specifically, for each
    frame $t$ in a video, the per-class non-maximum suppression (NMS) is conducted
    to obtain the top $n$ class-specific detection boxes. At the first frame of the
    video, $n$ action tubes for each class $c$ are initialized by using $n$ detected
    bounding boxes in this frame. Then, the algorithm grows these action tubes over
    time by adding one box at a frame or terminates if no matching boxes are found
    for $k$ consecutive frames [[35](#bib.bib35)]. Finally, each newly updated tube
    is temporally trimmed by performing binary labeling using an online Viterbi algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Linking up Clip-level Detection Tubelets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The clip-level tubelet linking algorithms aim to associate a sequence of clip-level
    tubelets into video-level action tubes. They are often derived from frame-level
    box linking algorithms mentioned in the last subsection. For instance, the tubelet
    linking algorithms in [[55](#bib.bib55)] stems from [[16](#bib.bib16)], and their
    intuition is that the content within a tubelet should capture an action and the
    connected tubelets in any two consecutive clips should have a large temporal overlap.
    Therefore, they defined a tubelet’s linking score as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S=\frac{1}{m}\sum_{i=1}^{m}{Actionness}_{i}+\frac{1}{m-1}\sum_{j=1}^{m-1}{Overlap}_{j,j+1}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where ${Actionness}_{i}$ denotes the actionness score of the tubelet from the
    $i$-th clip. ${Overlap}_{j,j+1}$ measures the overlap between the linked two proposals
    respectively from the $j$-th and ($j+1$)-th clips, and $m$ is the total number
    of video clips. The overlap between two tubelets is calculated based on the IoU
    of the last frame of the $j$-th tubelet and the first frame of the $(j+1)$-th
    tubelet. After computing the tubelets’ score, [[55](#bib.bib55)] chose a number
    of linked action tubelets with the highest scores in a video.
  prefs: []
  type: TYPE_NORMAL
- en: 'In another work, [[56](#bib.bib56)] extended the linking algorithm of [[35](#bib.bib35)]
    from frame linking to tubelet linking to build action tubes. Its core idea can
    be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialization: in the first frame of a video, [[56](#bib.bib56)] started a
    new link for each tubelet. Here, a link refers to a sequence of linked tubelets.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linking: given a new frame $f$, they extended the existing links with one of
    the tubelet candidates starting at this frame. They selected the tubelet candidate
    that met the following criteria: (a) is not already picked by other links, (b)
    has the highest action score, and (c) its overlap with the link to be extended
    is higher than a given threshold.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Termination: for an existing link, if the criteria are not met for more than
    $K$ consecutive frames, the link terminates. $K$ is a given hyperparameter.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Due to its simplicity and efficiency, the tubelet linking algorithm of [[56](#bib.bib56)]
    was adopted by later works, such as [[66](#bib.bib66), [70](#bib.bib70)].
  prefs: []
  type: TYPE_NORMAL
- en: Temporal trimming. In the tubelet linking algorithm of [[56](#bib.bib56)], the
    initialization and termination steps determine action tubes’ temporal extents,
    but Song et al. [[58](#bib.bib58)] found that it can not thoroughly address the
    temporal location error induced by transitional state, which is defined as ambiguous
    states around but not belong to the target actions. To address this issue, Song
    et al. [[58](#bib.bib58)] proposed a transition-aware classifier that can distinguish
    between transitional states and real actions and thus alleviate the temporal error
    of spatio-temporal action detection. In follow-up work, Zhao et al. [[70](#bib.bib70)]
    tried to avoid misclassification for the transitional states by introducing an
    action switch regression head, which decides whether a box prediction depicts
    the actor performing the action(s). This regression head gives each bounding box
    of a tubelet an action switch score. If the score is higher than a given threshold,
    the box contains an action. Experiments in  [[70](#bib.bib70)] showed that the
    action switch regression head could remarkably reduce the misclassification for
    the transitional states.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Datasets and Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Benchmark Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Benchmark datasets play significant role in understanding the comparative and
    absolute strengths and weaknesses of each method. In this section, we first review
    the commonly used video datasets for STAD, and then we conclude this section by
    presenting the summary and comparison of these datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Weizmann
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Weizmann dataset [[73](#bib.bib73)] was recorded using a static camera on a
    uniform background. The actors move horizontally across the frame, maintaining
    consistency in the actor’s size as they perform each action. This dataset comprises
    a total of 90 video clips grouped 10 action classes, such as “walking”, “jogging”,
    and “waving”, performed by 9 different subjects. Each video clip contains multiple
    instances of a single action. The spatial resolution of these videos is 180$\times$144
    pixels, and each clip ranges from 1 second to 5 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 CMU Crowded Videos
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The CMU Crowded Videos [[5](#bib.bib5)] contains five actions: pick-up, one-hand
    wave, two-hand wave, push button, and jumping jack. There are 5 training videos
    for each action and 48 test videos. All videos had been scaled such that the spatial
    resolution of each video is 120 $\times$ 160\. The test videos range from 5 to
    37 seconds (166 to 1115 frames). This dataset was recorded with cluttered and
    dynamic backgrounds so that action detection on this dataset is more challenging
    than on Weizmann dataset [[73](#bib.bib73)]. The CMU Crowded Videos dataset was
    densely annotated, providing the spatial and temporal coordinates (x, y, height,
    width, start, and end frames) for specified actions as ground truth.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 MSR Action I and II
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The MSR Action dataset I [[74](#bib.bib74)] and II [[75](#bib.bib75)] were
    created by Microsoft Research Group (MSR), and were made publicly available in
    2009 and 2010, respectively. The MSR Action dataset II is an extension of I. Whereas
    the MSR Action dataset I contains 62 action instances in 16 video sequences, the
    MSR Action dataset II contains 203 instances in 54 videos. Each video contains
    multiple actions performed by different individuals. All videos range from 32
    to 76 seconds. Each action instance’s spatial and temporal coordinates are provided,
    allowing the dataset to be used for action detection and recognition. Both datasets
    consist of three action classes: clap, hand wave, and boxing. Similar to the CMU
    Crowded dataset [[5](#bib.bib5)], the MSR Action datasets were created with cluttered
    and dynamic backgrounds.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 J-HMDB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: J-HMDB [[76](#bib.bib76)] is a joint-annotated HMDB [[77](#bib.bib77)] dataset
    to better understand and analyze the limitations and identify components of algorithms
    for improvement on overall accuracy on the HMDB dataset. To clarify the description,
    we first briefly introduce the HMDB dataset and then review the J-HMDB dataset.
  prefs: []
  type: TYPE_NORMAL
- en: HMDB dataset [[77](#bib.bib77)] contain 5 action categories, which are general
    facial actions (e.g.smile, chew), facial actions with object manipulation (e.g.smoke,
    eat), general body movements (e.g.cartwheel, clap hands), body movements with
    object interaction (e.g.brush hair, catch), body movements for human interaction
    (e.g.fencing, hug). Each category contains a minimum of 101 video clips, and the
    dataset contains a total of 6849 video clips distributed in 51 action categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of major STAD datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | No. Actions | No. Actors | No. Videos | No. Instances | No. Bbox
    | Year | Resource |'
  prefs: []
  type: TYPE_TB
- en: '| Weizmann [[73](#bib.bib73)] | 10 | 9 | 90 | - | - | 2005 | Actor Staged |'
  prefs: []
  type: TYPE_TB
- en: '| CMU Crowded Videos [[5](#bib.bib5)] | 5 | 6 | 98 | - | - | 2007 | Actor Staged
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSR Action I [[74](#bib.bib74)] | 3 | 10 | 16 | 62 | - | 2009 | Actor Staged
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSR Action II [[75](#bib.bib75)] | 3 | ¿10 | 54 | 203 | - | 2010 | Actor
    Staged |'
  prefs: []
  type: TYPE_TB
- en: '| UCF Sports [[78](#bib.bib78)] | 10 | - | 150 | - | - | 2009 | TV |'
  prefs: []
  type: TYPE_TB
- en: '| J-HMDB [[76](#bib.bib76)] | 21 | - | 928 | 928 | 32k | 2013 | Movies, YouTube
    |'
  prefs: []
  type: TYPE_TB
- en: '| UCF101-24 [[79](#bib.bib79)] | 24 | - | 3207 | 4458 | 574k | 2015 | YouTube
    |'
  prefs: []
  type: TYPE_TB
- en: '| MultiTHUMOS [[80](#bib.bib80)] | 65 | - | 400 | - | - | 2017 | YouTube |'
  prefs: []
  type: TYPE_TB
- en: '| AVA [[27](#bib.bib27)] | 80 | - | 430 | 386k | 426k | 2018 | Movies, YouTube
    |'
  prefs: []
  type: TYPE_TB
- en: '| MultiSports [[10](#bib.bib10)] | 66 | - | 3200 | 37701 | 902k | 2021 | YouTube
    |'
  prefs: []
  type: TYPE_TB
- en: J-HMDB [[76](#bib.bib76)] consists of the selected 21 classes of videos from
    the HMDB dataset. These selected videos involve a single individual performing
    the action like brushing hair, jumping, running, etc. There are 36 to 55 clips
    per action class, with each clip containing about 15-40 frames, summing to a total
    of 928 clips in the dataset. Each clip is trimmed such that the first and last
    frames correspond to the beginning and end of an action. The frame resolution
    is 320$\times$240, and the frame rate is 30 fps.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 UCF Sports
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The UCF Sports [[78](#bib.bib78)] contains 10 actions in the sports domain,
    which are diving, golf swing, kicking, lifting, horseback riding, running, skateboarding,
    swinging on a pommel horse and floor, swinging on parallel bars and walking. All
    the videos contain camera motion and complex backgrounds gathered from the broadcast
    television channels, such as BBC and ESPN video corpus. The UCF Sports dataset
    contains 150 clips and each clip has a frame rate of 10 fps. The spatial resolution
    of the videos ranges from 480$\times$360 to 720$\times$576 and are 2.20 to 14.40
    seconds in duration, averaging 6.39 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.6 UCF101-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: UCF101 dataset [[79](#bib.bib79)] has been widely used in action recognition
    research. It comprises realistic videos collected from Youtube containing 101
    action categories, with 13320 videos in total. UCF101 gives significant diversity
    in actions with large variations in camera motion, object appearance, viewpoint,
    cluttered background and illumination conditions, etc. For the action detection
    task, a subset of 24 action classes and 3207 videos are provided with dense annotations.
    This subset is therefore called UCF101-24. Different from UCF Sports [[78](#bib.bib78)]
    and J-HMDB [[76](#bib.bib76)], in which videos are truncated to actions, videos
    in UCF101-24 are untrimmed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.7 THUMOS and MultiTHUMOS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The THMOS series dataset consists of four datasets: THUMOS’13, THUMOS’14, THUMOS’
    15, and MultiTHUMOS. The first three are from THUMOS Challenge took place annually
    from 2013 to 2015 in conjunction with various major conferences in computer vision
    [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)]. All
    of the videos are from UCF101 [[79](#bib.bib79)]. THUMOS datasets consist of 24
    action classes. The length of actions varies significantly, i.e., from less than
    a second to minutes. These datasets contain 13,000 temporally trimmed videos,
    over 1000 temporally untrimmed videos, and over 2500 negative sample videos. These
    videos might contain none, one, or multiple instances of a single or multiple
    action(s). MultiTHUMOS [[80](#bib.bib80)] is an enhanced version of THUMOS. It
    is a dense, multi-class, frame-wise labeled video dataset with 400 videos of 30
    hours and 38,690 annotations of 65 classes. Averagely, it has 1.5 labels per frame
    and 10.5 action classes per video.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.8 AVA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Atomic Visual Actions (AVA) dataset is sourced from 430 movies on YouTube.
    Each movie contributes a clip ranging from the 15th to the 30th minute to the
    dataset. Each clip is then partitioned into 897 overlapping 3s segments with a
    stride of 1 second. For each segment, the middle frame is selected as the keyframe.
    In each keyframe, every person is annotated with a bounding box and (possibly
    multiple) actions. The 430 movies are split into 235 training, 64 validation,
    and 131 test movies, roughly a 55:15:30 split, resulting in 211k training, 57k
    validation, and 118k test segments. AVA dataset contains 80 atomic actions covering
    pose actions, person-person interactions, and person-object interactions. 60 actions
    that have at least 25 instances are often adopted for evaluation. This dataset
    has two annotation versions, i.e., v2.1 and v2.2\. The annotation v2.2 is more
    consistent than v2.1.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.9 MultiSports
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MultiSports dataset is a new STAD dataset recently released by Li et al. [[10](#bib.bib10)].
    The raw video content of this dataset comes from Olympics and World Cup competitions
    on YouTube. It consists of 4 sports (i.e., basketball, volleyball, football, and
    aerobic gymnastics) and 66 action categories. There are 800 clips for each sport,
    3200 clips in total. It consists of 37701 action instances with 902k bounding
    boxes. The instance number of each action category ranges from 3 to 3,477, showing
    the natural long-tailed distribution. Each video is annotated with multiple instances
    of multiple action classes. The average video length is 750 frames. Due to the
    fine granularity of the action labels, the length of each action segment is short,
    with an average length of 24 frames [[61](#bib.bib61)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.10 Summary and Comparison of STAD Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We summarize the STAD datasets in Table [2](#S4.T2 "Table 2 ‣ 4.1.4 J-HMDB ‣
    4.1 Benchmark Datasets ‣ 4 Datasets and Evaluation"), which shows that as research
    progresses, the datasets supporting STAD studies become increasingly complex in
    terms of the number of action classes, human subjects, annotation boxes, etc.
    For example, the UCF Sports proposed in 2009 has only 10 action classes and 150
    videos, while the MultiSports proposed in 2021 contains 66 actions and 3200 videos.
    Moreover, in order to keep pace with the growing capability of STAD models, recent
    datasets are intentionally made increasingly challenging for action detection
    by adding camera motion, dynamic background, and frequent occlusions. The most
    recent STAD datasets, such as UCF101-24, AVA, and MultiSports, directly collect
    videos from YouTube and hence contain videos we would encounter in the real world.
    Thus, models that perform well in these datasets have great potential for use
    in real-life scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance comparison on J-HMDB and UCF101-24 datasets. ‘f.mAP’ and
    ‘v.mAP’ denotes frame-mAP and video-mAP respectively. ‘-’ means that the result
    is not available. We report the detection accuracy in percentage. The model abbreviations
    used here refer to the following. I3D: Inflated 3D convolutions [[28](#bib.bib28)].
    S3D(+G): Separable 3D convolutions (with gating) [[85](#bib.bib85)]. NL: Non-local
    networks [[44](#bib.bib44)]. 3D-ResNeXt: [[86](#bib.bib86)]. SF: SlowFast [[30](#bib.bib30)].
    DLA34: [[87](#bib.bib87)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Method | Backbone | J-HMDB | UCF101-24 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | f.-mAP | v.-mAP | f.-mAP | v.-mAP |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | @0.2 | @0.5 | @0.75 | 0.5:0.95 |  | @0.2 | @0.5 | @0.75 | 0.5:0.95
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Gkioxari et al.[[16](#bib.bib16)] | AlexNet | 36.2 | - | 53.3 | -
    | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Weinzaepfel et al.[[17](#bib.bib17)] | AlexNet | 45.8 | 63.1 | 60.7
    | - | - | 35.8 | 51.7 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | MR-TS R-CNN[[19](#bib.bib19)] | VGG | 58.5 | 74.3 | 73.1 | - | - |
    65.7 | 72.9 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | H-FCN [[20](#bib.bib20)] | H-FCN | 39.9 | - | 56.4 | - | - | - | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | Saha et al. [[11](#bib.bib11)] | VGG | - | 72.6 | 71.5 | 43.3 | 40.0
    | - | 66.7 | 35.9 | 7.9 | 14.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | ACT [[56](#bib.bib56)] | VGG | 65.7 | 74.2 | 73.7 | 52.1 | 44.8 |
    67.1 | 77.2 | 51.4 | 22.7 | 25.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Singh et al.[[35](#bib.bib35)] | VGG | - | 73.8 | 72.0 | 44.5 | 41.6
    | - | 73.5 | 46.3 | 15.0 | 20.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Zolfaghari et al. [[29](#bib.bib29)] | C3D | - | 78.2 | 73.5 | - |
    - | - | 47.6 | 26.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | AMTnet [[59](#bib.bib59)] | VGG | - | - | - | - | - | - | 79.4 | 51.2
    | 19.0 | 23.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | CPLA [[21](#bib.bib21)] | VGG | - | - | - | - | - | - | 73.5 | 37.8
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | T-CNN [[55](#bib.bib55)] | C3D | 61.3 | 78.4 | 76.9 | - | - | 41.4
    | 47.1 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Gu et al.[[27](#bib.bib27)] | I3D | 73.3 | - | 78.6 | - | - | 76.3
    | - | 59.9 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | ACRN [[40](#bib.bib40)] | S3D-G | 77.9 | - | 80.1 | - | - | - | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | - | 82.7 | 81.3 | - | - | - | 77.9
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | TraMNet [[60](#bib.bib60)] | VGG | - | - | - | - | - | - | 79.0 |
    50.9 | 20.1 | 23.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | TPN [[62](#bib.bib62)] | VGG | - | 79.7 | 77.0 | - | - | - | 71.7
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Alwando et al.[[23](#bib.bib23)] | VGG | - | 79.9 | 78.3 | - | - |
    - | 72.9 | 41.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | VideoCapsuleNet [[68](#bib.bib68)] | - | 64.6 | 95.1 | - | - | - |
    78.6 | 97.1 | 80.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | - | 86.9 | 85.5 | - | - | - | 83.0
    | 64.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | HISAN [[50](#bib.bib50)] | Res101 | - | 87.6 | 86.5 | 53.8 | 51.3
    | - | 82.3 | 51.5 | 23.5 | 24.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | STEP [[65](#bib.bib65)] | VGG | - | - | - | - | - | 75.0 | 76.6 |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | TACNet [[58](#bib.bib58)] | VGG | 65.5 | 74.1 | 73.4 | 52.5 | 44.8
    | 72.1 | 77.5 | 52.9 | 21.8 | 24.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | 2in1 [[34](#bib.bib34)] | VGG | - | - | 74.7 | 53.3 | 45.0 | - | 78.5
    | 50.3 | 22.2 | 24.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | PCSC [[64](#bib.bib64)] | I3D | 74.8 | 82.6 | 82.2 | 63.1 | 52.8 |
    79.2 | 84.3 | 61.0 | 23.0 | 27.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | YOWO [[33](#bib.bib33)] | 3D-ResNeXt | 74.4 | 87.8 | 85.7 | 58.1 |
    - | 87.2 | 75.8 | 48.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | MOC [[66](#bib.bib66)] | DLA34 | 70.8 | 77.3 | 77.2 | 71.7 | 59.1
    | 78.0 | 82.8 | 53.8 | 29.6 | 28.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AIA [[47](#bib.bib47)] | SF101 | - | - | - | - | - | 78.8 | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | C-RCNN [[45](#bib.bib45)] | Res50-I3D | 79.2 | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CFAD [[63](#bib.bib63)] | I3D | - | 86.8 | 85.3 | - | - | 72.5 | 81.6
    | 64.6 | - | 26.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ACAM [[41](#bib.bib41)] | I3D | 78.9 | - | 83.9 | - | - | - | - |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Li et al.[[88](#bib.bib88)] | I3D | - | 76.1 | 74.3 | 56.4 | - | -
    | 71.1 | 54.0 | 21.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | ACDnet [[89](#bib.bib89)] | VGG | - | - | - | - | - | 70.9 | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | ACAR-Net [[51](#bib.bib51)] | SF101 | - | - | - | - | - | 84.3 | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | SAMOC [[90](#bib.bib90)] | DLA34 | 73.1 | 79.2 | 78.3 | 70.5 | 58.7
    | 79.3 | 80.5 | 53.5 | 30.3 | 28.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | WOO [[36](#bib.bib36)] | SF101 | 80.5 | - | - | - | - | - | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | SE-STAD [[37](#bib.bib37)] | SF101-NL | 82.5 | - | - | - | - | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | - | - | - | - | - | 81.5 | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | MRSN [[48](#bib.bib48)] | SF50 | - | - | - | - | - | 80.3 | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TubeR [[70](#bib.bib70)] | I3D | - | 81.8 | 80.7 | - | - | 81.3 |
    85.3 | 60.2 | - | 29.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | HIT [[49](#bib.bib49)] | SF50 | 83.8 | 89.7 | 88.1 | - | - | 84.8
    | 88.8 | 74.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | EVAD [[39](#bib.bib39)] | ViT-B | 90.2 | - | - | - | - | 85.1 | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Performance comparison on UCF Sports and MultiSports datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Method | Backbone | UCF Sports | MultiSports |'
  prefs: []
  type: TYPE_TB
- en: '| f.-mAP | v.-mAP | f.-mAP | v.-mAP |'
  prefs: []
  type: TYPE_TB
- en: '| @0.2 | @0.5 | @0.75 | 0.5:0.95 | @0.2 | @0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Gkioxari et al.[[16](#bib.bib16)] | AlexNet | 68.1 | - | 75.8 | -
    | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Weinzaepfel et al.[[17](#bib.bib17)] | AlexNet | 71.9 | - | 90.5 |
    - | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | MR-TS R-CNN[[19](#bib.bib19)] | VGG | 84.5 | 94.8 | 94.7 | - | - |
    - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | H-FCN [[20](#bib.bib20)] | H-FCN | 82.7 | - | - | - | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | ACT [[56](#bib.bib56)] | VGG | 87.7 | 92.7 | 92.7 | 78.4 | 58.5 |
    - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | T-CNN [[55](#bib.bib55)] | C3D | 86.7 | 95.2 | - | - | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | - | 98.6 | 98.6 | - | - | - | -
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | TPN [[62](#bib.bib62)] | VGG | - | 96.0 | 95.7 | - | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Alwando et al.[[23](#bib.bib23)] | VGG | - | 94.7 | 94.7 | 89.6 |
    67.5 | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | VideoCapsuleNet [[68](#bib.bib68)] | - | 83.9 | 97.1 | - | - | - |
    - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | - | 98.9 | 98.9 | - | - | - | -
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | SlowFast [[30](#bib.bib30)] | SF101 | - | - | - | - | - | 27.2 | 24.2
    | 9.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | 2in1 [[34](#bib.bib34)] | VGG |  |  | 92.7 | 83.4 | - |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | YOWO [[33](#bib.bib33)] | 3D-ResNeXt-101 | - | - | - | - | - | 9.3
    | 10.8 | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | MOC [[66](#bib.bib66)] | DLA34 | - | - | - | - | - | 25.2 | 12.9 |
    0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CFAD [[63](#bib.bib63)] | I3D | - | 94.5 | 92.7 | - | - | - | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Li et al.[[88](#bib.bib88)] | I3D | - | 94.3 | 93.8 | 79.5 | - | -
    | - |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | - | - | - | - | - | 55.3 | 60.6 |
    37.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | HIT [[49](#bib.bib49)] | SF50 | - | - | - | - | - | 33.3 | 27.8 |
    8.8 |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The performances of STAD methods are often evaluated by two popular metrics,
    i.e., frame and video mean Average Precision (mAP), which are usually denoted
    as frame-mAP and video-mAP.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Frame-mAP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Frame-mAP measures the area under the precision-recall curve of the bounding
    box detections at each frame. A detection is correct if its IoU with the ground
    truth bounding box is larger than a given threshold and the action label is correctly
    predicted [[16](#bib.bib16)]. The threshold is often set as 0.5\. Frame-mAP allows
    researchers to compare the detection accuracy independently of the linking strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Video-mAP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Video-mAP measures the area under precision-recall curve of the action tube
    predictions. A tube detection is correct if its IoU with the ground truth tube
    is larger than a given threshold and the action label is correctly predicted [[16](#bib.bib16)].
    The IoU between two tubes is defined as the IoU over the temporal domain, multiplied
    by the average of the IoU between boxes averaged over all overlapping frames [[17](#bib.bib17)].
    The threshold for video-mAP is often set as 0.2, 0.5, 0.75, and 0.5:0.95, corresponding
    to the average video-mAP for thresholds with step 0.05 in this range. Whereas
    frame-mAP measures the ability of classification and spatial detection in a single
    frame, video-mAP can further evaluate the performance of temporal detection.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we analyze the performance of the state-of-the-art methods
    for STAD. Since J-HMDB, UCF101-24, UCF Sports, MultiSports, and AVA datasets are
    the most commonly used in deep learning-based STAD, we report the performance
    of state-of-the-art methods on these five datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Table [3](#S4.T3 "Table 3 ‣ 4.1.10 Summary and Comparison of STAD Datasets ‣
    4.1 Benchmark Datasets ‣ 4 Datasets and Evaluation") presents the performance
    of STAD methods on the J-HMDB and UCF101-24 datasets. VideoCapsuleNet [[68](#bib.bib68)]
    achieved the best results in terms of video-mAP. VideoCapsuleNet is different
    from the mainstream STAD methods. Whereas the mainstream STAD methods usually
    involve box or tubelet proposal generation and linking these proposals, VideoCapsuleNet
    is a 3D capsule network that performs pixel-wise action segmentation along with
    action classification [[68](#bib.bib68)]. This is probably why VideoCapsuleNet
    is excluded by most state-of-the-art methods for performance comparison. Apart
    from VideoCapsuleNet, the transformer-based frameworks, e.g., HIT [[49](#bib.bib49)],
    TubeR [[70](#bib.bib70)] and EVAD [[39](#bib.bib39)], achieved the best performance,
    demonstrating the capability of transformers in STAD. This is further verified
    by the results in Table [4](#S4.T4 "Table 4 ‣ 4.1.10 Summary and Comparison of
    STAD Datasets ‣ 4.1 Benchmark Datasets ‣ 4 Datasets and Evaluation") and Table [5](#S5.T5
    "Table 5 ‣ 5 Future Directions").
  prefs: []
  type: TYPE_NORMAL
- en: Table [4](#S4.T4 "Table 4 ‣ 4.1.10 Summary and Comparison of STAD Datasets ‣
    4.1 Benchmark Datasets ‣ 4 Datasets and Evaluation") provides the performance
    of STAD methods on the UCF Sports and MultiSports datasets. The MultiSports dataset
    was constructed very recently, so the reported results on this dataset are fewer
    than other datasets. Besides, the performance on this dataset is relatively low.
    This is probably because MultiSports action instances have large actor displacement,
    and there are often multiple actions in one video, making the detection difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Table [5](#S5.T5 "Table 5 ‣ 5 Future Directions") shows the results of STAD
    methods on the AVA (v2.1 and v2.2) dataset. The remarkable performance of AIA [[47](#bib.bib47)],
    ACAR-Net [[51](#bib.bib51)], IGMN [[53](#bib.bib53)] and HIT [[49](#bib.bib49)]
    indicates that visual relation and long-term temporal context is critical for
    the model to learn discriminative representation and can improve the detection
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the future directions in STAD that might be interesting
    to explore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Performance comparison on AVA dataset under the metric of frame-mAP
    with threshold of 0.5\. The column ‘Flow’ denotes whether optical flow is used
    as input.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Method | Backbone | Flow | Detector | Pretrained | AVA (v2.1) | AVA
    (v2.2) |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Gu et al.[[27](#bib.bib27)] | I3D | ✓ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 15.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | ACRN [[40](#bib.bib40)] | S3D-G | ✓ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 17.4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | RTPR [[22](#bib.bib22)] | Res101 | ✓ | - | - | 22.3 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Girdhar et al. [[25](#bib.bib25)] | I3D | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 21.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | LSTR [[71](#bib.bib71)] | Res101 | ✗ | - | - | 25.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | SlowFast [[30](#bib.bib30)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 28.2 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | STEP [[65](#bib.bib65)] | I3D | ✗ | - | Kinetics-400 | 18.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Girdhar et al.[[46](#bib.bib46)] | I3D | ✗ | - | Kinetics-400 | 24.9
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | LFB [[24](#bib.bib24)] | Res101-I3D-NL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 27.7 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Zhang et al.[[72](#bib.bib72)] | I3D | ✗ | Dave et al. [[91](#bib.bib91)]
    | Kinetics-400 | 22.2 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AIA [[47](#bib.bib47)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | 31.2 | 34.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | C-RCNN [[45](#bib.bib45)] | Res50-I3D-NL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 28.0 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | X3D [[31](#bib.bib31)] | X3D-XL | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 27.4 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ACAM [[41](#bib.bib41)] | I3D | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 24.4 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | ACAR-Net [[51](#bib.bib51)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | 30.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | IGMN [[53](#bib.bib53)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | 30.2 | 33.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | WOO [[36](#bib.bib36)] | SF101 | ✗ | Sparse R-CNN [[92](#bib.bib92)]
    | Kinetics-600 | 28.0 | 28.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | STAGE [[52](#bib.bib52)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-600 | 29.8 |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | SE-STAD [[37](#bib.bib37)] | SF101-NL | ✗ | FCOS [[38](#bib.bib38)]
    | Kinetics-600 | 28.8 | 29.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TAAD [[61](#bib.bib61)] | SF50 | ✗ | YOLOv5 [[93](#bib.bib93)] | Kinetics-700
    | 31.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TubeR [[70](#bib.bib70)] | CSN-152 | ✗ | - | IG+Kinetics-400 | 32.0
    | 33.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | MRSN [[48](#bib.bib48)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-400 | - | 33.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | HIT [[49](#bib.bib49)] | SF101 | ✗ | Faster R-CNN [[13](#bib.bib13)]
    | Kinetics-700 | - | 32.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | EVAD [[39](#bib.bib39)] | ViT-B | ✗ | - | Kinetics-400 | 31.1 | 32.2
    |'
  prefs: []
  type: TYPE_TB
- en: Label-efficient learning for STAD. The annotation for STAD contains not only
    the action class but also the bounding boxes of the actors and the start and end
    of action instances, making the collection and annotation of data expensive and
    time-consuming. To alleviate this burden, some pioneering works, such as [[94](#bib.bib94),
    [95](#bib.bib95)], have been introduced. While [[94](#bib.bib94)] proposed a semi-supervised
    framework to utilize the unlabeled video actions, [[95](#bib.bib95)] presented
    a co-finetuning method to leverage the large-scale action classification datasets
    like Kinetics [[96](#bib.bib96)] and Something-Something v2 [[97](#bib.bib97)].
    Nevertheless, much more effort is needed to explore the label-efficient STAD algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Online real-time STAD. STAD has numerous online applications, such as surveillance
    and autonomous driving. In these applications, the system must process the newly
    captured frame or clip only based on history and the current data and report the
    detection result as soon as possible. To reach this goal, the model is required
    to be lightweight and efficient. This is a challenging task. Although [[98](#bib.bib98),
    [35](#bib.bib35)] have taken the first step to undertake this task, it is still
    far from fully explored, and more effort is needed.
  prefs: []
  type: TYPE_NORMAL
- en: STAD under large motion. In real scenarios, many actions are with large motion
    because of fast actor displacement, rapid camera motion, etc. STAD under large
    motion is an interesting direction worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal learning for STAD. On the one hand, action video naturally contains
    multiple modalities, including visual, acoustic, and even linguistic messages
    (e.g., caption). Thus, fully utilizing these modalities via multimodal learning
    for STAD has the potential to achieve better detection accuracy than single-modality
    learning. On the other hand, actions can be captured by various sensors, such
    as depth camera, infrared camera, inertial sensor, and LiDAR. STAD might benefit
    from the fusion of the representations learned from these multimodal data.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models for STAD. Diffusion models [[99](#bib.bib99), [100](#bib.bib100),
    [101](#bib.bib101), [102](#bib.bib102)] are a class of generative models that
    starts from the sample in random distribution and recover the data sample via
    a gradual denoising process. They have recently become one of the hottest topics
    in computer vision. Even though they belong to generative model, they are freshly
    demonstrated effective for representative perception tasks, such as object detection [[103](#bib.bib103)]
    and temporal action location [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)].
    Taking as input random spatial boxes (or temporal proposals), the diffusion-based
    models can yield object boxes (or action proposals) accurately. Since STAD can
    be regarded as the combination of object detection and temporal action location,
    these pioneering works [[103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105),
    [106](#bib.bib106)] shed light on leveraging diffusion models for solving STAD
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Expedited by the rapid advances of deep neural networks, spatio-temporal action
    detection has made significant progress in recent years. This survey has extensively
    reviewed deep learning-based spatio-temporal action detection methods from different
    aspects, including models, datasets, linking algorithms, performance comparison,
    and future directions. The comparative summary of methods, datasets, and performance
    in pictorial and tabular forms clearly shows their attributes which will benefit
    the interested researchers. We hope this comprehensive survey will foster further
    research in spatio-temporal action detection.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Tian Y, Sukthankar R, Shah M. Spatiotemporal Deformable Part Models for
    Action Detection. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013,
    2642–2649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Yuan J, Liu Z, Wu Y. Discriminative Video Pattern Search for Efficient
    Action Detection. *IEEE Trans. Pattern Anal. Mach. Intell.*, 2011, 33: 1728–1743.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Siva P, Xiang T. Weakly Supervised Action Detection. In *Proc. Brit. Mach.
    Vis. Conf.*, 2011, 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Lan T, Wang Y, Mori G. Discriminative figure-centric models for joint action
    localization and recognition. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2011, 2003–2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Ke Y, Sukthankar R, Hebert M. Event detection in crowded videos. In *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2007, 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied
    to document recognition. In *Proc. IEEE*, volume 86, 1998, 2278–2324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser
    Ł, Polosukhin I. Attention is all you need. In *Proc. Adv. Neural Inf. Process.
    Syst.*, volume 30, 2017, 9662–9673.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Vahdani E, Tian Y. Deep Learning-based Action Detection in Untrimmed Videos:
    A Survey. *IEEE Trans. Pattern Anal. Mach. Intell.*, 2021, 1: 1–20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Bhoi A. Spatio-temporal action recognition: A survey. *arXiv preprint arXiv:1901.09403*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Li Y, Chen L, He R, Wang Z, Wu G, Wang L. Multisports: A multi-person
    video dataset of spatio-temporally localized sports actions. In *Proc. IEEE Int.
    Conf. Comput. Vis.*, 2021, 13536–13545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Saha S, Singh G, Sapienza M, Torr PHS, Cuzzolin F. Deep Learning for Detecting
    Multiple Space-Time Action Tubes in Videos. In *Proc. Brit. Mach. Vis. Conf.*,
    2016, 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Girshick R, Donahue J, Darrell T, Malik J. Rich feature hierarchies for
    accurate object detection and semantic segmentation. In *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2014, 580–587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Ren S, He K, Girshick R, Sun J. Faster r-cnn: Towards real-time object
    detection with region proposal networks. In *Proc. Adv. Neural Inf. Process. Syst.*,
    2015, 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Redmon J, Divvala S, Girshick R, Farhadi A. You only look once: Unified,
    real-time object detection. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2016, 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY, Berg AC. Ssd: Single
    shot multibox detector. In *Proc. Eur. Conf. Comput. Vis.*, 2016, 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Gkioxari G, Malik J. Finding action tubes. In *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2014, 759–768.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Weinzaepfel P, Harchaoui Z, Schmid C. Learning to Track for Spatio-Temporal
    Action Localization. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2015, 3164–3172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Zitnick CL, Dollár P. Edge boxes: Locating object proposals from edges.
    In *Proc. Eur. Conf. Comput. Vis.*, 2014, 391–405.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Peng X, Schmid C. Multi-region Two-Stream R-CNN for Action Detection.
    In *Proc. Eur. Conf. Comput. Vis.*, 2016, 744–759.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Wang L, Qiao Y, Tang X, Gool LV. Actionness Estimation Using Hybrid Fully
    Convolutional Networks. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2016, 2708–2717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Yang Z, Gao J, Nevatia R. Spatio-Temporal Action Detection with Cascade
    Proposal and Location Anticipation. In *Proc. Brit. Mach. Vis. Conf.*, volume
    1-12, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Li D, Qiu Z, Dai Q, Yao T, Mei T. Recurrent Tubelet Proposal and Recognition
    Networks for Action Detection. In *Proc. Eur. Conf. Comput. Vis.*, 2018, 18–33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Alwando EHP, Chen YT, Fang WH. CNN-based multiple path search for action
    tube detection in videos. *IEEE Trans. Circuits Syst. Video Technol.*, 2018, 30(1):
    104–116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Wu CY, Feichtenhofer C, Fan H, He K, Krahenbuhl P, Girshick R. Long-term
    feature banks for detailed video understanding. In *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2019, 284–293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Girdhar R, Carreira J, Doersch C, Zisserman A. A better baseline for ava.
    *arXiv preprint arXiv:1807.10066*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Ji S, Xu W, Yang M, Yu K. 3D convolutional neural networks for human action
    recognition. *IEEE Trans. Pattern Anal. Mach. Intell.*, 2012, 35(1): 221–231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Gu C, Sun C, Vijayanarasimhan S, Pantofaru C, Ross DA, Toderici G, Li
    Y, Ricco S, Sukthankar R, Schmid C, Malik J. AVA: A Video Dataset of Spatio-Temporally
    Localized Atomic Visual Actions. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, 6047–6056.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Carreira J, Zisserman A. Quo vadis, action recognition? a new model and
    the kinetics dataset. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017,
    6299–6308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Zolfaghari M, Oliveira GL, Sedaghat N, Brox T. Chained multi-stream networks
    exploiting pose, motion, and appearance for action classification and detection.
    In *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, 2904–2913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Feichtenhofer C, Fan H, Malik J, He K. Slowfast networks for video recognition.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, 6202–6211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Feichtenhofer C. X3d: Expanding architectures for efficient video recognition.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2020, 203–213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Liu Y, Yang F, Ginhac D. ACDnet: An Action Detection network for real-time
    edge computing based on flow-guided feature approximation and memory aggregation.
    *Pattern Recognit. Lett.*, 2021, 145: 118–126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Köpüklü O, Wei X, Rigoll G. You only watch once: A unified cnn architecture
    for real-time spatiotemporal action localization. *arXiv preprint arXiv:1911.06644*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Zhao J, Snoek CG. Dance with flow: Two-in-one stream action detection.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, 9935–9944.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Singh G, Saha S, Sapienza M, Torr PHS, Cuzzolin F. Online Real-Time Multiple
    Spatiotemporal Action Localisation and Prediction. In *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2016, 3657–3666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Chen S, Sun P, Xie E, Ge C, Wu J, Ma L, Shen J, Luo P. Watch Only Once:
    An End-to-End Video Action Detection Framework. In *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2021, 8158–8167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Sui L, Zhang CL, Gu L, Han F. A Simple and Efficient Pipeline to Build
    an End-to-End Spatial-Temporal Action Detector. *arXiv preprint arXiv:2206.03064*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Tian Z, Shen C, Chen H, He T. Fcos: Fully convolutional one-stage object
    detection. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, 9627–9636.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Chen L, Tong Z, Song Y, Wu G, Wang L. Efficient Video Action Detection
    with Token Dropout and Context Refinement. *arXiv preprint arXiv:2304.08451*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Sun C, Shrivastava A, Vondrick C, Murphy KP, Sukthankar R, Schmid C. Actor-Centric
    Relation Network. In *Proc. Eur. Conf. Comput. Vis.*, 2018, 1–17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Ulutan O, Rallapalli S, Srivatsa M, Torres C, Manjunath B. Actor conditioned
    attention maps for video action detection. In *Proc. IEEE Winter Conf. Appl. Comput.
    Vis.*, 2020, 527–536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Jiang J, Cao Y, Song L, Zhang S, Li Y, Xu Z, Wu Q, Gan C, Zhang C, Yu
    G. Human centric spatio-temporal action localization. In *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit. Workshop*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Ning Z, Xie Q, Zhou W, Wang L, Li H. Person-Context Cross Attention for
    Spatio-Temporal Action Detection. Technical report, Technical report, Huawei Noah’s
    Ark Lab, and University of Science and Technology of China, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Wang X, Girshick R, Gupta A, He K. Non-local neural networks. In *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018, 7794–7803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Wu J, Kuang Z, Wang L, Zhang W, Wu G. Context-aware rcnn: A baseline for
    action detection in videos. In *Proc. Eur. Conf. Comput. Vis.*, 2020, 440–456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Girdhar R, Carreira J, Doersch C, Zisserman A. Video Action Transformer
    Network. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, 244–253.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Tang J, Xia J, Mu X, Pang B, Lu C. Asynchronous Interaction Aggregation
    for Action Detection. In *Proc. Eur. Conf. Comput. Vis.*, 2020, 22–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Zheng YD, Chen G, Yuan M, Lu T. MRSN: Multi-Relation Support Network for
    Video Action Detection. *arXiv preprint arXiv:2304.11975*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Faure GJ, Chen MH, Lai SH. Holistic Interaction Transformer Network for
    Action Detection. In *Proc. IEEE Winter Conf. Appl. Comput. Vis.*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Pramono RRA, Chen YT, Fang WH. Hierarchical self-attention network for
    action localization in videos. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2019,
    61–70.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Pan J, Chen S, Shou Z, Shao J, Li H. Actor-Context-Actor Relation Network
    for Spatio-Temporal Action Localization. In *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2021, 464–474.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Tomei M, Baraldi L, Calderara S, Bronzin S, Cucchiara R. Video action
    detection by learning graph-based spatio-temporal interactions. *Comput. Vis.
    Image Understand.*, 2021, 206: 103187–103197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Ni J, Qin J, Huang D. Identity-aware Graph Memory Network for Action Detection.
    In *Proc. ACM Multimedia Conf.*, 2021, 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Jain M, Van Gemert J, Jégou H, Bouthemy P, Snoek CG. Action localization
    with tubelets from motion. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, 740–747.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Hou R, Chen C, Shah M. Tube Convolutional Neural Network (T-CNN) for Action
    Detection in Videos. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, 5823–5832.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Kalogeiton VS, Weinzaepfel P, Ferrari V, Schmid C. Action Tubelet Detector
    for Spatio-Temporal Action Localization. In *Proc. IEEE Int. Conf. Comput. Vis.*,
    2017, 4415–4423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale
    image recognition. In *Proc. Int. Conf. Learn. Represent.*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Song L, Zhang S, Yu G, Sun H. TACNet: Transition-Aware Context Network
    for Spatio-Temporal Action Detection. In *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2019, 11979–11987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Saha S, Singh G, Cuzzolin F. AMTnet: Action-Micro-Tube Regression by End-to-end
    Trainable Deep Architecture. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2017, 4424–4433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Singh G, Saha S, Cuzzolin F. TraMNet - Transition Matrix Network for Efficient
    Action Tube Proposals. In *Proc. Asian Conf. Comput. Vis.*, 2018, 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Singh G, Choutas V, Saha S, Yu F, Van Gool L. Spatio-Temporal Action Detection
    Under Large Motion. *arXiv preprint arXiv:2209.02250*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] He J, Deng Z, Ibrahim MS, Mori G. Generic tubelet proposals for action
    localization. In *Proc. IEEE Winter Conf. Appl. Comput. Vis.*, 2018, 343–351.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Li Y, Lin W, See J, Xu N, Xu S, Yan K, Yang C. CFAD: Coarse-to-Fine Action
    Detector for Spatiotemporal Action Localization. In *Proc. Eur. Conf. Comput.
    Vis.*, 2020, 40–56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Su R, Ouyang W, Zhou L, Xu D. Improving action localization by progressive
    cross-stream cooperation. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2019, 12016–12025.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Yang X, Yang X, Liu MY, Xiao F, Davis LS, Kautz J. STEP: Spatio-Temporal
    Progressive Learning for Video Action Detection. In *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, 264–272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Li Y, Wang Z, Wang L, Wu G. Actions as Moving Points. In *Proc. Eur. Conf.
    Comput. Vis.*, 2020, 1–21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Liu Y, Yang F, Ginhac D. Accumulated micro-motion representations for
    lightweight online action detection in real-time. *Journal of Visual Communication
    and Image Representation*, 2023: 103879.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Duarte K, Rawat Y, Shah M. Videocapsulenet: A simplified network for action
    detection. *Proc. Adv. Neural Inf. Process. Syst.*, 2018, 31.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. End-to-end
    object detection with transformers. In *Proc. Eur. Conf. Comput. Vis.*, 2020,
    213–229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Zhao J, Zhang Y, Li X, Chen H, Shuai B, Xu M, Liu C, Kundu K, Xiong Y,
    Modolo D, et al.. TubeR: Tubelet transformer for video action detection. In *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2022, 13598–13607.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Li D, Yao T, Qiu Z, Li H, Mei T. Long Short-Term Relation Networks for
    Video Action Detection. In *Proc. ACM Multimedia Conf.*, 2019, 629–637.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Zhang Y, Tokmakov P, Hebert M, Schmid C. A structured model for action
    detection. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, 9975–9984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Blank M, Gorelick L, Shechtman E, Irani M, Basri R. Actions as space-time
    shapes. In *Proc. IEEE Int. Conf. Comput. Vis.*, volume 2, 2005, 1395–1402.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Yuan J, Liu Z, Wu Y. Discriminative subvolume search for efficient action
    detection. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2009, 2442–2449.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Cao L, Liu Z, Huang TS. Cross-dataset action detection. In *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2010, 1998–2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Jhuang H, Gall J, Zuffi S, Schmid C, Black MJ. Towards understanding action
    recognition. In *Proc. IEEE Int. Conf. Comput. Vis.*, 2013, 3192–3199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Kuehne H, Jhuang H, Garrote E, Poggio T, Serre T. HMDB: a large video
    database for human motion recognition. In *Proc. IEEE Int. Conf. Comput. Vis.*,
    2011, 2556–2563.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Rodriguez MD, Ahmed J, Shah M. Action mach: a spatio-temporal maximum
    average correlation height filter for action recognition. In *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2008, 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Soomro K, Zamir AR, Shah M. UCF101: A dataset of 101 human actions classes
    from videos in the wild. *arXiv preprint arXiv:1212.0402*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Yeung S, Russakovsky O, Jin N, Andriluka M, Mori G, Fei-Fei L. Every Moment
    Counts: Dense Detailed Labeling of Actions in Complex Videos. *Int. J. of Comput.
    Vis.*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Idrees H, Zamir AR, Jiang YG, Gorban A, Laptev I, Sukthankar R, Shah M.
    The THUMOS challenge on action recognition for videos “in the wild”. *Comput.
    Vis. Image Understand.*, 2017, 155: 1–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Jiang YG, Liu J, Zamir AR, Toderici G, Laptev I, Shah M, Sukthankar R.
    THUMOS challenge: Action recognition with a large number of classes, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Gorban A, Idrees H, Jiang YG, Zamir AR, Laptev I, Shah M, Sukthankar R.
    THUMOS challenge: Action recognition with a large number of classes, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Caba Heilbron F, Escorcia V, Ghanem B, Carlos Niebles J. Activitynet:
    A large-scale video benchmark for human activity understanding. In *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2015, 961–970.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Xie S, Sun C, Huang J, Tu Z, Murphy K. Rethinking spatiotemporal feature
    learning for video understanding. In *Proc. Eur. Conf. Comput. Vis.*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Xie S, Girshick R, Dollár P, Tu Z, He K. Aggregated residual transformations
    for deep neural networks. In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2017, 1492–1500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Yu F, Wang D, Darrell T. Deep Layer Aggregation. In *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, 2403–2412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Li Y, Lin W, Wang T, See J, Qian R, Xu N, Wang L, Xu S. Finding action
    tubes with a sparse-to-dense framework. In *Proc. AAAI Conf. Artif. Intell.*,
    volume 34, 2020, 11466–11473.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Liu Y, Yang F, Ginhac D. ACDnet: An action detection network for real-time
    edge computing based on flow-guided feature approximation and memory aggregation.
    *Pattern Recognit. Lett.*, 2021, 145: 118–126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Ma XY, Luo Z, Zhang X, Liao Q, Shen X, Wang M. Spatio-Temporal Action
    Detector with Self-Attention. In *Int. Joint Conf. Neural Netw.*, 2021, 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Dave A, Tokmakov P, Ramanan D. Towards segmenting anything that moves.
    In *Proc. IEEE Int. Conf. Comput. Vis. Workshops*, 2019, 0–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Sun P, Zhang R, Jiang Y, Kong T, Xu C, Zhan W, Tomizuka M, Li L, Yuan
    Z, Wang C, et al.. Sparse r-cnn: End-to-end object detection with learnable proposals.
    In *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2021, 14454–14463.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Brostom M. Real-time multi-object tracker using yolov5 and deep sort.
    *[https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch](https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch)*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Zhang H, Zhao X, Wang D. Semi-supervised Learning for Multi-label Video
    Action Detection. In *Proc. ACM Multimedia Conf.*, 2022, 2124–2134.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Arnab A, Xiong X, Gritsenko A, Romijnders R, Djolonga J, Dehghani M, Sun
    C, Lučić M, Schmid C. Beyond Transfer Learning: Co-finetuning for Action Localisation.
    *arXiv preprint arXiv:2207.03807*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Kay W, Carreira J, Simonyan K, Zhang B, Hillier C, Vijayanarasimhan S,
    Viola F, Green T, Back T, Natsev P, et al.. The kinetics human action video dataset.
    *arXiv preprint arXiv:1705.06950*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Goyal R, Ebrahimi Kahou S, Michalski V, Materzynska J, Westphal S, Kim
    H, Haenel V, Fruend I, Yianilos P, Mueller-Freitag M, et al.. The” something something”
    video database for learning and evaluating visual common sense. In *Proc. IEEE
    Int. Conf. Comput. Vis.*, 2017, 5842–5850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Dave I, Scheffer Z, Kumar A, Shiraz S, Rawat YS, Shah M. GabriellaV2:
    Towards better generalization in surveillance videos for Action Detection. In
    *Proc. IEEE Winter Conf. Appl. Comput. Vis. Workshop*, 2022, 122–132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Song J, Meng C, Ermon S. Denoising diffusion implicit models. In *Proc.
    Int. Conf. Learn. Represent.*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models. volume 33,
    2020, 6840–6851.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Song Y, Ermon S. Generative modeling by estimating gradients of the data
    distribution. volume 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Song Y, Sohl-Dickstein J, Kingma DP, Kumar A, Ermon S, Poole B. Score-based
    generative modeling through stochastic differential equations. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Chen S, Sun P, Song Y, Luo P. Diffusiondet: Diffusion model for object
    detection. *arXiv preprint arXiv:2211.09788*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Nag S, Zhu X, Deng J, Song YZ, Xiang T. DiffTAD: Temporal Action Detection
    with Proposal Denoising Diffusion. *arXiv preprint arXiv:2303.14863*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Liu D, Li Q, Dinh A, Jiang T, Shah M, Xu C. Diffusion Action Segmentation.
    *arXiv preprint arXiv:2303.17959*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Xu M, Soldan M, Gao J, Liu S, Pérez-Rúa JM, Ghanem B. Boundary-Denoising
    for Video Activity Localization. *arXiv preprint arXiv:2304.02934*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
