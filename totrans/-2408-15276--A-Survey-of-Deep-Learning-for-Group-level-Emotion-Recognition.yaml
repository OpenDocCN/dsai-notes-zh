- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:30:28'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2408.15276] A Survey of Deep Learning for Group-level Emotion Recognition'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.15276](https://ar5iv.labs.arxiv.org/html/2408.15276)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning for Group-level Emotion Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Xiaohua Huang, Jinke Xu, Wenming Zheng, , Qirong Mao,Abhinav Dhall X. Huang
    is with the School of Computer Engineering, Nanjing Institute of Technology, China,
    the Key Laboratory of Child Development and Learning Science (Southeast University),
    Ministry of Education, Southeast University, Nanjing 210096, China, and also with
    Research Center for Learning Science, Southeast University, China. (email: xiaohuahwang@gmail.com)J.
    Xu is with the School of Computer Engineering, Nanjing Institute of Technology,
    China. (email: y00450220246@njit.edu.cn)W. Zheng is with the Key Laboratory of
    Child Development and Learning Science (Southeast University), Ministry of Education,
    Southeast University, Nanjing 210096, China and also with the School of Biological
    Science and Medical Engineering, Southeast University, Nanjing 210096, Jiangsu,
    China. (E-mail: wenming_zheng@seu.edu.cn)Q. Mao is with the School of Computer
    Science and Communication Engineering, Jiangsu University, Zhenjiang, Jiangsu,
    China. (E-mail: mao_qr@ujs.edu.cn)A. Dhall is with College of Science & Engineering,
    Flinders University, Adelaide, Australia and Indian Institute of Technology Ropar,
    Hussainpur, Rupnagar 140001, Punjab. (E-mail: abhinav.dhall@flinders.edu.au)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the advancement of artificial intelligence (AI) technology, group-level
    emotion recognition (GER) has emerged as an important area in analyzing human
    behavior. Early GER methods are primarily relied on handcrafted features. However,
    with the proliferation of Deep Learning (DL) techniques and their remarkable success
    in diverse tasks, neural networks have garnered increasing interest in GER. Unlike
    individual’s emotion, group emotions exhibit diversity and dynamics. Presently,
    several DL approaches have been proposed to effectively leverage the rich information
    inherent in group-level image and enhance GER performance significantly. In this
    survey, we present a comprehensive review of DL techniques applied to GER, proposing
    a new taxonomy for the field cover all aspects of GER based on DL. The survey
    overviews datasets, the deep GER pipeline, and performance comparisons of the
    state-of-the-art methods past decade. Moreover, it summarizes and discuss the
    fundamental approaches and advanced developments for each aspect. Furthermore,
    we identify outstanding challenges and suggest potential avenues for the design
    of robust GER systems. To the best of our knowledge, thus survey represents the
    first comprehensive review of deep GER methods, serving as a pivotal references
    for future GER research endeavors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Group-level Emotion Recognition, Deep Learning, Feature representation learning,
    Attention mechanism, Fusion scheme.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Emotio exhibits a profound influence on human perception, attention, memory,
    and decision-making, directly impacting both physical and mental well-being [[1](#bib.bib1)].
    Consequently, the comprehension and perception of emotions not only enhance interpersonal
    communication but also implicitly contribute to the regulation of human physical
    health. With the evolution of big data technology, the broadening of application
    scenarios, and the progress in artificial intelligence technology, group-level
    emotion recognition (GER) has become a focal point for researchers [[2](#bib.bib2),
    [3](#bib.bib3)]. Unlike individual-level emotion recognition, which analyzes the
    emotions of individuals through facial expressions, speech, and postures, GER
    focuses on identifying the collective emotions of a group of people. Existing
    GER technology integrates diverse information, encompassing facial expressions,
    postures, social interactions, etc., to predict the emotions of a group. Taking
    facial expression data as an example, GER typically involves three key steps:
    first, given an image, detecting and extracting information such as faces and
    postures from images or videos; secondly, employing handcrafted descriptor or
    neural networks to extract facial features and other relevant information; finally,
    inputting these features as sequential data into another model like recurrent
    neural network to classify the group-level emotion. However, GER presents three
    key challenges. Firstly, the groups and contexts involved in GER may exhibit diversity
    and complexity. Secondly, the labeling of group-level emotion demands more nuanced
    considerations for the emotions expressed by the primary group, introducing additional
    complexities compared to individual-level emotion labeling. Lastly, due to the
    involvement of multiple individuals and diverse contexts, the recognition process
    becomes more complex than individual-level emotion recognition. Despite these
    challenges, GER remains an essential and formidable research area. It would be
    utilized to analyze emotion changes of a group of people, detecting abnormal behaviors
    and potential dangers in a timely manner in surveillance videos, or understanding
    students’ learning status in collaborative learning [[4](#bib.bib4)].'
  prefs: []
  type: TYPE_NORMAL
- en: This article provides a comprehensive survey of deep learning approaches for
    GER. Different from existing review [[5](#bib.bib5)], this paper elaborates on
    the methods of GER with a unique focus on deep learning architecture, providing
    insights into the current state and technical challenges associated with deep
    learning method in GER. In the beginning,we commence by providing an in-depth
    analysis of groups and emotions from a social perspective, offering a concise
    concept for GER. Subsequently, we present a thorough description of the image-based
    and video-based group-level emotion databases currently available for GER. Moreover,
    we explore recently developed deep learning methods for GER and scrutinize their
    technical challenges. In conclusion, we discuss the development trends of GER,
    offering valuable insights and guidance for future research and applications.
  prefs: []
  type: TYPE_NORMAL
- en: II Group-level emotion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Emotions, traditionally viewed as individual-level phenomena in common parlance,
    have increasingly received attention in the field of social psychology as being
    potentially group-level [[6](#bib.bib6)]. Niedenthal and Brauer offer a broad
    definition of group-level emotions, characterizing them as emotions experienced
    by individuals on behalf of a group with which they identify [[7](#bib.bib7)].
    This perspective suggests that group-level emotions are inherently more complex
    than their individual-level counterparts. Adopting both sociological and psychological
    perspectives, we will proceed by elucidating the concepts of groups and emotions
    before providing a precise definition of group emotions.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental disparity between group-level emotion and individual-level emotion
    arises from the distinction between the concepts of a group and an individual.
    While an individual pertains to an singular, independent entity, a group transcends
    mere aggregation, constituting a social phenomenon that amalgamates individuals
    into a collective entity. Various perspectives and definitions of the concept
    of a group have been proposed in scholarly literature [[8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)]. Shaw et al. [[8](#bib.bib8)] defines a group
    as comprising two or more individuals engaged in mutual interaction and influence,
    emphasizing the significance of interaction among group members. Szilagi and Wallance [[9](#bib.bib9)]
    characterize a group as a collection of two or more individuals who interact and
    depend on each other to achieve a common goal. Additionally, Barron et al. [[10](#bib.bib10)]
    further elaborate on this definition by conceptualizing a group as individuals
    connected by some kind of bond, exhibiting varying degrees of cohesion. Moreover,
    Dasgupta et al. [[11](#bib.bib11)] describe a group as a group of people closely
    interconnected in some manner. According to these studies, a group consists of
    multiple individuals with a shared objective and direct or indirect interactions
    between them during a certain period of time. Thus, only when these conditions
    are met can a combination of multiple individuals be deemed a group.
  prefs: []
  type: TYPE_NORMAL
- en: Numerous scholars have investigated the concept of emotion within the sociological
    and psychological field. Schachter et al. [[12](#bib.bib12)] proposed that emotion
    encompasses both a physiological arousal state and a cognitive state adapted to
    this physiological state. Kleinginna et al. [[13](#bib.bib13)] conducted a comprehensive
    review categorizing 92 different definitions of emotion from various literature
    sources. Ekman in [[14](#bib.bib14)] suggested that “emotions evolve from the
    adaptive value of human beings in handling basic life tasks”, indicating that
    emotions arise as adaptive responses to different situations encountered during
    social activities. Averill regards emotion as a complex of impulsive motivation
    in higher cognition, involving various psychological processes and physiological
    responses [[15](#bib.bib15)]. Cabanac defines emotion as any psychological experience
    with high intensity and high pleasure content, emphasizing the relationship between
    emotion and psychological experience [[16](#bib.bib16)]. Barrett posits that emotions
    are constructs of the brain’s interpretation of external and internal stimuli [[17](#bib.bib17)].
    Scherer views emotion as a biopsychological phenomenon resulting from the interaction
    of specific neural systems and physiological responses, cognitive assessments,
    and social-cultural factors [[18](#bib.bib18)]. Therefore, emotions can be understood
    as physiological and psychological responses to stimuli in our environment.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the foregoing elucidation of group and emotion, group-level emotion
    denotes the physiological and psychological responses elicited by multiple interacting
    individuals over a defined period. Scholars have proffered diverse definitions
    of group-level emotion in their studies. In the beginning, Hatfield et al. [[19](#bib.bib19)]
    characterize group-level emotion as the process whereby group members reciprocally
    influence each other through emotional contagion and empathy, culminating in emotional
    synchronization and consistency. Essentially, group-level emotion entails the
    reciprocal transmission and influence of emotions among individuals, leading to
    emotional consistency. Furthermore, group emotion is perceived as the emotional
    state propagated among members of a social group, cultivated and diffused through
    social interaction and shared experiences [[20](#bib.bib20)]. Barsäde and Gibson
    stressed the importance for researchers in the social science community to approach
    group-level emotions from both a “top-down approach” and a “bottom-up approach” [[21](#bib.bib21)].
    A “top-down approach” suggests that emotion exhibited by group is represented
    at the group level and is felt by individual members, while the ”bottom-up approach”
    highlights the unique compositional effects of individual-level group member emotions.
    According to the framework [[21](#bib.bib21)], Kelly and Barsäde further proposed
    that group-level emotion comprises affective compositional effects and affective
    context [[22](#bib.bib22)]. In essence, group-level emotion emerges from a combination
    of individual-level affective factors posed by group members and group-level factors
    that shape the emotional experience of the group. Additionally, Barsäde and Gibson
    further explore group-level emotion as the emotional state disseminated and shared
    among members of an organization [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: III Group-level emotion dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rise of social media platforms has led to a surge in the volume of uploaded
    photos and videos, driving advancements in big data technologies and affective
    computing domains, especially GER. In recent years, numerous group-level emotion
    datasets are established, attracting attention from researchers in the domains
    of affective computing and computer vision. However, the quality of annotation
    on images and videos plays an critical role in determining the efficacy of GER
    models. Thus, this section aims to provide an exhaustive examination of existing
    group-level emotion datasets, classifying them into two main types: image-based
    and video-based types.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Image-based datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contrast to individual-level emotion datasets, group-level emotion datasets
    necessitate annotation for a group. Image-based datasets began to emerge since
    2013, with notable representations including MultiEmoVA [[24](#bib.bib24)], Happiness
    Image database (HAPPEI) [[25](#bib.bib25)], Group-level Affect database (GAF) [[2](#bib.bib2),
    [26](#bib.bib26), [27](#bib.bib27)], Group cohesion dataset [[28](#bib.bib28)],
    GroupEmoW [[29](#bib.bib29)], and SiteGroEmo [[3](#bib.bib3)]. Among these database,
    HAPPEI, GAF, and the Group cohesion dataset, which were utilized in the EmotiW
    sub-challenge, have received significant attention and adoption by researchers
    in the fields of computer vision and affective computing.
  prefs: []
  type: TYPE_NORMAL
- en: Mou et al. [[24](#bib.bib24)] introduced a multi-dimensional group emotion image
    dataset, namely MultiEmoVA. This dataset was primarily constructed by scouring
    various social media platforms for real-life photos. Initially, 400 color images
    were collected, and after manually filtering images with ambiguous emotional expressions,
    250 images meeting the criteria were remained. Furthermore, these images were
    categorized into positive, neutral, and negative. Additionally, Mou et al. also
    incorporated arousal-level annotation into the dataset, providing more explicit
    representations of the intensity for each emotion category.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HAPPEI database contains images captured from social media platform and
    categorizes group-level emotions into neutral, small smile, big smile, smile,
    big laugh, and thrilled, totaling 2,638 images. In contrast, the GAF datbase delineates
    three emotion categories along the valence dimension: positive, neutral, and negative.
    The data collection process involved web search using keywords corresponding to
    various scenarios, such as weddings, birthday parties, and sports events, etc.,
    to obtain images depicting group-level emotions in those contexts. Subsequently,
    these images were annotated by 2 to 3 experts in affective computing domain. The
    GAF database has undergone three iterations, namely GAF [[26](#bib.bib26)], GAF2.0 [[2](#bib.bib2)],
    and GAF3.0 [[27](#bib.bib27)]. The initial version, GAF includes 504 images [[26](#bib.bib26)],
    which is relatively small in scale and has been limited usage by researcher for
    evaluating the performance of deep GER models. However, the subsequent iterations,
    GAF2.0 and GAF3.0, witnessed a substantial increase in data size, featuring 6,467 [[2](#bib.bib2)]
    and 17,172 images [[27](#bib.bib27)], respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to emotion category, the cohesiveness of a group serves as a crucial
    indicator of the emotional state, structure and success of a group of individuals.
    Group Cohesion database was derived from GAF3.0 by adding cohesion labels [[28](#bib.bib28)].
    Each image in this database was annotated with a cohesion score ranging from 0
    to 3 by five annotators, where 0 represents no cohesion and 3 means strong cohesion.
    This database was utilized in the Emotiw2019 challenge [[30](#bib.bib30)], with
    9,300 images allocated for training, 4,244 images for validation and 2,899 images
    for testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the above-mentioned databases, another databases have recently proposed
    by Guo et al. [[29](#bib.bib29)] and Wang et al. [[3](#bib.bib3)], that is GroupEmoW
    and SiteGroEmo. GroupEmoW was created with a stringent criterion mandating that
    each image contains 2 to 9 individuals engaged in a specific activity, thereby
    forming distinct groups. According to this criterion, they collected 15,894 images
    from the internet, creating a diverse dataset with varying image resolutions and
    in-the-wild. Subsequently, these images were categorized into positive, neutral,
    and negative. On the other hand, the SiteGroEmo dataset diverges from existing
    database by capturing image across tourism scenes worldwide. This dataset not
    only contains rich geographic information and scene variations but also randomly
    captures the facial and body movements of individuals at a specific moment. Comprising
    a total of 10,034 images, the dataset is labeled with valence to denote emotions,
    specifically categorized as negative, neutral, and positive.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the aforementioned image-based datasets predominantly
    rely on internet keywords searches for data collection. While this method may
    expedite the establishment of satisfactory datasets, the existing datasets, with
    the exception of the MultiEmoVA database, solely employ valence-level annotations
    for images, lacking arousal-level annotations and more specific emotion categories
    such as anger and surprise. This limitation may arise from the diverse expression
    exhibited by participants in a group, making it challenging to annotate group-level
    images with a comprehensive emotion taxonomy. Additionally, manual annotation
    may introduce discrepancies in labeling due to cultural differences. Moreover,
    many images in these datasets may suffer from quality issues such as poor lighting,
    incomplete capture of facial expressions, or obstructions obscuring certain individuals.
    Finally, given their static nature, these images lack information about emotional
    dynamics. In the domain of emotion recognition research, scholars have emphasized
    that dynamic changes in facial expressions can offer crucial clues for both humans
    and computers to discern emotions or emotional processes [[31](#bib.bib31)]. Therefore,
    these uncontrollable circumstances and the absence of dynamic information may
    have a discernible impact on the accuracy of GER based on image.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Video-based datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In contrast to image-based datasets, video-based datasets not only capture
    the temporal dynamics of emotional expressions but also provide additional contextual
    information, facilitating a more comprehensive and accurate portrayal of changes
    in emotional states. However, due to the demanding collection and annotation process,
    only two video-based datasets have emerged recently since 2019: the VGAF dataset
    introduced by Sharma et al. [[32](#bib.bib32)] and the GECV dataset by Quach et
    al. [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The VAGF dataset comprises videos sourced from the YouTube platform, each featuring
    a varying number of individuals forming groups of different sizes. The dataset
    is partitioned into training, validation, and test sets, encompassing 2,661, 766,
    and 756 samples, respectively. Alongside valence annotation, the dataset includes
    cohesion metrics among individuals in the group. The corresponding dataset has
    also been used in EmotiW2023 [[34](#bib.bib34)]. Conversely, the GECV dataset
    contains videos captured in leisure and crowded scenes, amounting to a total of
    627 videos. This dataset is further subdivided into three subsets: GEVC-SingleImg,
    GEVC-GroupImg, and GEVC-GroupVid. The latter two subsets are tailored to capture
    group emotions more effectively by showcasing various various group behaviors
    across different scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: While video-based databases offer richer semantic and contextual information
    compared to image-based databases, facilitating more discriminant criteria for
    data annotation, they are sourced from media platforms featuring complex scenes
    that often depict real-life scenarios. Nonetheless, they present limitations such
    as the absence of physiological signals akin to multi-modal emotion recognition,
    thereby posing challenges in data collection.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Dataset summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The specific comparisons of the existing group emotion datasets are presented
    in Table [I](#S3.T1 "TABLE I ‣ III-C Dataset summary ‣ III Group-level emotion
    dataset ‣ A Survey of Deep Learning for Group-level Emotion Recognition"). Despite
    all datasets collecting images or videos from diverse real-world scenarios, their
    sample size is comparably small when compare with comprehensive datasets like
    AffectNet [[35](#bib.bib35)]. This limited data size impedes the robust learning
    of group-level features. Recently, in the domain of micro-expression recognition,
    a composite dataset consolidating various micro-expression databases has become
    popular [[36](#bib.bib36)]. This approach corroborates the generalization capacity
    of the method across datasets with disparate characteristics, mitigating the issue
    of data scarcity. Such a strategy holds promise for GER by potentially augmenting
    data volumes, particularly for video-based datasets, and enhancing the generalization
    ability of GER methods.
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art approaches, particularly those showcased in the EmotiW challenge,
    primarily undergo evaluation using image-based databases. All databases adopt
    an annotation strategy categorizing images into three valence-level categories,
    as certain nuanced emotions such as fear and contempt pose challenges in data
    collection, resulting in limited samples for these categories, which are insufficient
    for robust learning. As seen from Table [I](#S3.T1 "TABLE I ‣ III-C Dataset summary
    ‣ III Group-level emotion dataset ‣ A Survey of Deep Learning for Group-level
    Emotion Recognition"), except SiteGroEmo, each database has a balanced distribution
    of data across each class. Furthermore, in practical experiments, apart from the
    HAPPEI and MultiEmoVA databases, other databases follow an official protocol where
    the train, validation, and test sets remain strictly fixed without random validation.
    Such rigid dataset partitioning may not be facilitate accurate assessments of
    GER method performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Group-level emotion database.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Type | Sample size | Category | Task | Protocol |'
  prefs: []
  type: TYPE_TB
- en: '| HAPPEI [[26](#bib.bib26)]* | Image | 2,638 | Neural (92), Small smile (147),
    Large smile (774), Small laugh (1256), Large laugh (331), Thrilled (38) | Regression
    | 4-fold cross validation train (1500), val (1138), test (496) |'
  prefs: []
  type: TYPE_TB
- en: '| MultiEmoVA [[24](#bib.bib24)] | Image | 250 | High-pos (46), Medium-pos (64),
    High-neg (31), medium-neg (27), low-neg (10), neu (72) | Classification | 5-fold
    cross validation |'
  prefs: []
  type: TYPE_TB
- en: '| GAF2.0 [[2](#bib.bib2)] | Image | 6,467 | pos (2,356), neu (2,092), neg (2,019)
    | Classification | train (3,630), val (2,068), test (772) |'
  prefs: []
  type: TYPE_TB
- en: '| GAF3.0 [[27](#bib.bib27)] | Image | 17,172 | pos (6,553), neu (5,364), neg
    (5,256) | Classification | train (9,836), val (4346), test (3011) |'
  prefs: []
  type: TYPE_TB
- en: '| Group Cohesion [[30](#bib.bib30), [28](#bib.bib28), [37](#bib.bib37)] | Image
    | 16,433 | [0, 3] | Regression | train (9,300), val (4,244), test (2,899) |'
  prefs: []
  type: TYPE_TB
- en: '| SiteGroEmo [[3](#bib.bib3)] | Image | 10,034 | pos (4,660), neu (4,355),
    neg (1,019) | Classification | train (6,096), val (1,972), test (1,966) |'
  prefs: []
  type: TYPE_TB
- en: '| GroupEmoW [[29](#bib.bib29)] | Image | 15,894 | pos (6,636), neu (4,947),
    neg (4,311) | Classification | train (11,127), val (3,178), test (1,589) |'
  prefs: []
  type: TYPE_TB
- en: '| GECV [[33](#bib.bib33)] | Video | 627 | pos (204), neu (221), neg (202) |
    Classification | train (90%), test (10%) |'
  prefs: []
  type: TYPE_TB
- en: '| VGAF [[38](#bib.bib38)] | Video | 4,183 | pos (1,104), neu (1,203), neg (1,120)
    | Classification | train (2,661), val (766), test (756) |'
  prefs: []
  type: TYPE_TB
- en: '| pos: Positive; neg: Negative; neu: Neutral |  |'
  prefs: []
  type: TYPE_TB
- en: '| val: validation. |  |'
  prefs: []
  type: TYPE_TB
- en: '| *For HAPPEI database, we only offer the database volume of each class for
    train and validation sets. |  |'
  prefs: []
  type: TYPE_TB
- en: IV Input modality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recognizing group-level emotions poses significant challenges due to the diversity
    in group dynamic, individual emotion expressions, and limited data availability,
    deep learning (DL) methods, while promising, exhibit varied performance depending
    on the various modalities utilized. In this section, we describe the diverse information
    cues based on static image or video sequence utilized for GER and outline their
    respectively strengths and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Static image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the abundant availability of facial images online such as AffectNet database [[35](#bib.bib35)],
    numerous existing studies in FER are conducted on static images. Leveraging the
    efficiency demonstrated by FER with static images, numerous researchers have extended
    their efforts to GER, incorporating various additional cues such as face, scene,
    pose, even objects. Convolutional Neural Networks (CNNs), notably VGG, ResNet
    and their variants, are commonly employed to analyze static images in GER research.
  prefs: []
  type: TYPE_NORMAL
- en: Cue aggregation as input. Given the flexible nature of group sizes, GER faces
    the challenge of effectively aggregating features from multiple individuals within
    a group to derive an overall emotional state. Existing methods are broadly categorized
    into two approaches. The first category involves averaging or weighted sum all
    individuals’ emotion scores [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)], which are typically outputted by a classifier such as Support
    Vector Machine (SVM). For example, Rassadin et al. [[39](#bib.bib39)] normalized
    detected faces and fed them into VGGFace and ImageNet. They constructed an ensemble
    of four Random Forest classifier trained on the features outputted by VGGFace,
    ImageNet, and landmark features. Finally, they employed a weighted sum approach
    to fuse the score outputted by the random forest classifiers. The second approach
    employs some machine learning algorithms such as bag-of-words and clustering to
    aggregate all individuals’ features into a single feature vector. Balaji et al. [[43](#bib.bib43)],
    for example, utilized CNNs to extract face information. Subsequently, Fisher vector
    and VLAD encoding techniques were applied to compress all individual features
    in the image, yielding bottom-up features capable of representing group emotions.
    This method effectively compresses features, reduces computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodality as input. Group are often considered as “emotional entities and
    a rich source of varied manifestations of affect”. Earlier discussions by Barsäde
    and Gibson in [[23](#bib.bib23)] emphasized the necessity for GER researchers
    to adopt both a “top-down approach” and a “bottom-up approach”. Consequently,
    GER research has concentrated on integrating both bottom-up and top-down components.
    Typically, bottom-up refers to individual emotions, while top-down encompasses
    contextual factors such as background information in an image. Recognizing the
    benefits of incorporating both bottom-up and top-down components, several studies [[44](#bib.bib44),
    [43](#bib.bib43), [45](#bib.bib45), [46](#bib.bib46), [24](#bib.bib24), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)] have explored to fuse features
    from various cues in group-level images. For example, in Garg’s work [[46](#bib.bib46)],
    a deep convolutional neural network is utilized to identify facial expressions
    within an image, while a Bayesian network leverages scene descriptors to extract
    visual features of the image content, thereby inferring the overall emotion of
    the image. In addition to these modalities, body information [[24](#bib.bib24),
    [51](#bib.bib51)] and object [[49](#bib.bib49), [50](#bib.bib50)] have also been
    incorporated into certain studies. In summary, GER research has commonly explored
    one or more cues extracted from face, pose/skeleton information, object, and scene
    context. Cues combination offers the advantage of enabling successful recognition
    of group-level emotions in the challenging environment when one cue is lacking.
    However, even employing multiple cues, a key concern arises regarding how to effectively
    establish connection among these cues to enhance the robustness of GER in real-world
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Dynamic image sequence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As emotions are temporal in nature, such automatic tools in environments like
    factories, companies, and offices can help identifying interventions to maintain
    a healthy work culture. Facial expressions, being dynamic cues, evolve and change,
    revealing their effective signals over time. The visual information captured in
    videos plays a pivotal role in discerning the emotions depicted within them [[52](#bib.bib52)].
    The temporal variations across video frames provides additional information to
    be exploited, albeit encoding these variations introduces complexity to emotion
    recognition. Training a network to comprehend the overall affect of a group of
    people shown across frames poses challenges. In this subsection, we delineate
    various dynamic inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal information as input. Temporal information encapsulate the dynamics
    of an entire video sequence in a single instance. The temporal information, modeled
    by using algorithms like LSTM, has been successfully employed in GER to model
    scene dynamics and appearance in a video [[32](#bib.bib32)]. Similarly, active
    image encapsulated spatial and temporal information from video sequences into
    a single instance by estimating and accumulating changes in each pixel component.
    Sun et al. [[53](#bib.bib53)] utilized temporal segment networks to extract RGB
    information, optical flow frame, and warped optical flow frame for each video,
    incorporating a temporal shift module to model dynamic scene information. Quach et
    al. [[33](#bib.bib33)] introduced a fusion mechanism called Non Volume Preserving
    Fusion (NVPF) to better model spatial relationships between facial emotions in
    each frame, effectively addressing emotional ambiguity caused by insufficient
    facial resolution or undetectable emotions.
  prefs: []
  type: TYPE_NORMAL
- en: Frame aggregation as input. Dynamic image sequence collected in the wild often
    include complex scene background. Petrova et al. [[54](#bib.bib54)] designed a
    method based on VGG-19 framework for each frame, capturing global emotions, followed
    by using score averaging and accumulation across all frames. Li et al. [[55](#bib.bib55)]
    proposed leveraging multi-task learning theory to aggregate frame features, while
    Liu et al. [[56](#bib.bib56)] employed four aggregation methods including maximum,
    minimum, average, and standard deviation to consolidate all individual’s face
    feature in a group image.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodality as input. In analyzing group affect, audio features play a crucial
    role alongside facial image, as relying solely on facial expressions may lead
    to inaccuracies in estimating overall group affect. Pitch, speech rate, and duration,
    etc. have been found relevant to affect analysis. In group settings, these features
    are vital for distinguishing between situations like arguments and discussions,
    where the visual model may falter. Visual-audio fusion models have been proposed
    to enhance visual-based models [[32](#bib.bib32), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [55](#bib.bib55)]. For example, Wang et al. [[57](#bib.bib57)]
    introduced a network called K-injection audiovisual network, which employs a multi-head
    cross-attention mechanism to jointly model audio and video data, integrating previous
    emotion knowledge to improve the model’s generalization ability. Recent study
    indicate that human gesture can convery emotion [[59](#bib.bib59)]. Several researchers
    have incorporated human gesture features fused with scene and face cues for video-level
    GER [[53](#bib.bib53), [56](#bib.bib56)]. For example, Sun et al. [[53](#bib.bib53)]
    utilized CenterNet for human detection and pose estimation, followed by ResNetSt
    for extracting body feature. For prediction, the average probability of each frame’s
    prediction was calculated, yielding the video’s class prediction.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primary challenge faced by these methods lies in establishing relationships
    between modalities and effectively fuse them. As evident from the discussed methods,
    GER has increasingly emphasized mining the dynamic information present in videos.
    This trend is expected to drive further advancements in Recurrent Neural Networks
    (RNN) and its derivatives in group-level emotion recognition. Additionally, these
    studies have ventured beyong single-modal information, paving the way for research
    to better comprehend and leverage multimodal information. This broader perspective
    holds promise for enrich the understanding and utilization of diverse sources
    of information in group-level emotion recognition tasks.
  prefs: []
  type: TYPE_NORMAL
- en: V Deep Networks for GER
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the inception of artificial neurons in the 1940s, deep learning has been
    undergone extensive exploration and implementation. Evolving from single-layer
    perceptrons to multi-layer neural networks, Convolutional neural networks (CNNs),
    Recurrent neural networks (RNNs), Cascade networks, Graph convolutional networks
    (GCNs), attention mechanisms, and beyond, deep learning has witnessed rapid evolution.
    So far, various deep learning approaches have emerged to discern the collective
    emotions of a group of people, leveraging diverse information such as facial expression,
    gestures, social interactions, and more. Figure [1](#S5.F1 "Figure 1 ‣ V Deep
    Networks for GER ‣ A Survey of Deep Learning for Group-level Emotion Recognition")
    illustrates literature spanning database, emotion competition, method, and survey
    categories of academic papers employing deep learning based methods for GER over
    the past decade. It is noteworthy that the publication trend has exhibited a noticeable
    increase, especially attributed to the EmotiW competition. In this section, we
    delve into the approaches from the perspectives of specialized blocks, network
    architecture, fusion stage and scheme, and loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f799ea049767d520e0608ac165231e40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The overview of deep learning based technical papers and survey papers
    for group-level emotion recognition. Viewed in color is BEST.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A Basic Network Block
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/69cdf7089073e36dbf3f80f233f0403c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) CNN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b77a06569ffba9074756be0244aa6fea.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) RNN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d3993262c559d6a6ce711b1ae213f9db.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The cascade network
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/54627f60d7ea00ffd34f027f779f5ece.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) GCN
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Basic blocks for GER: (1) Convolution block; (b) Recurrent neural
    network (RNN); (c) Cascade network; (d) Graph Convolutional Network (GCN).'
  prefs: []
  type: TYPE_NORMAL
- en: Before describing the network architecture, Figure [2](#S5.F2 "Figure 2 ‣ V-A
    Basic Network Block ‣ V Deep Networks for GER ‣ A Survey of Deep Learning for
    Group-level Emotion Recognition") first introduce basic network block widely used
    in GER, including CNN, RNN, GCN.
  prefs: []
  type: TYPE_NORMAL
- en: CNN. Due to the limitations of traditional machine learning in addressing complex
    environments, many researchers have explored more in-depth research methods. LeCun et
    al. [[60](#bib.bib60)] first proposed a convolutional neural network model based
    on the backpropagation algorithm. However, due to hardware limitations at the
    time, the research progress of CNN was relatively slow. It was not until 2012
    when Krizhevsky et al. [[61](#bib.bib61)] proposed the AlexNet CNN model in the
    ImageNet Large Scale Visual Recognition Challenge, which significantly surpassed
    traditional machine learning methods in accuracy and promoted the development
    of deep learning in the field of computer vision. Since then, various models based
    on CNN have been proposed, such as VGGNet, GoogLeNet, ResNet, DenseNet, and MobileNet.
    Meanwhile, CNN models have also shown their superiority in GER.
  prefs: []
  type: TYPE_NORMAL
- en: RNN. In Convolutional Neural Networks (CNNs), each input and output are independent
    of each other, but this ignores the relationship between them. Although CNNs can
    extract good features when processing image datasets, they are not ideal for datasets
    with time series data, such as speech, audio, and video. Rumelhart et al. [[62](#bib.bib62)]
    proposed a method of Recurrent Neural Networks (RNN) that learns and trains through
    the backpropagation algorithm, and applied it to process data with time series.
    RNN has the characteristic of introducing a recurrent structure, allowing the
    network to remember and utilize previous information, thereby extracting better
    time series features. Various improved network architectures based on RNN have
    also been widely used, such as Simple Recurrent Neural Network (SRNN), Bidirectional
    Recurrent Neural Network (BRNN), Long Short-Term Memory Network (LSTM), and Gated
    Recurrent Unit (GRU), which have achieved good results based on RNN. Normally,
    RNN is always combined with CNN, leading to the cascade network for GER, as shown
    in Figure [2(c)](#S5.F2.sf3 "In Figure 2 ‣ V-A Basic Network Block ‣ V Deep Networks
    for GER ‣ A Survey of Deep Learning for Group-level Emotion Recognition").
  prefs: []
  type: TYPE_NORMAL
- en: GCN. Graph data has a wide presence in the real world, such as social networks,
    biological networks, recommendation networks, and chemical molecules. However,
    previous deep learning models based on CNN and RNN mainly deal with vector and
    matrix data, which neglects the topological structure of graphs and the relationships
    between nodes, leading to possible information loss and performance degradation.
    In order to solve this problem, Scarselli et al. [[63](#bib.bib63)] first proposed
    a new neural network model, namely, the graph neural network model, which extends
    existing neural network methods to handle data represented in the graph domain.
    Recent studies demonstrate that the effectiveness of graph convolutional networks
    (GCNs) in modeling semantic relationships, making them valuable for facial expression
    recognition (FER) tasks [[64](#bib.bib64)], as depicted in Figure [2(d)](#S5.F2.sf4
    "In Figure 2 ‣ V-A Basic Network Block ‣ V Deep Networks for GER ‣ A Survey of
    Deep Learning for Group-level Emotion Recognition"). With the success of GCNs
    in FER, [[65](#bib.bib65)] introduced GCNs for GER, aiming to enhance performance
    by capturing inter-individual relationships.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Network architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The efficacy of FER neural units depends on how multiple networks are integrated.
    GER methods typically adopt one of five network architectures: single-stream,
    multi-stream, cascade, graph convolutional network, and attention mechanism. In
    this section, we delve into the specifics of each architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B1 Single-stream networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typical deep GER methods adopt single CNN with individual input. In single-stream
    2D CNNs, the primary input is facial images, while single-stream 3D CNNs directly
    extract spatial and temporal features from video sequences. Many studies [[66](#bib.bib66),
    [39](#bib.bib39), [40](#bib.bib40)] employ transfer learning strategy on deep
    networks pretrained on large-scale face datasets to mitigate the overfitting issues.
    For example, Rassadin et al. [[39](#bib.bib39)] employ a pre-trained VGGFace model
    on detected faces, followed by a weighted sum to obtain the final result using
    a random forest classifier. Similarly, Lu et al. [[40](#bib.bib40)] utilize a
    VGG model pretrained on the VGGFace dataset to extract facial features for GER.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to transfer learning methods, several works design cascade network [[67](#bib.bib67),
    [68](#bib.bib68)] or kernel methods [[69](#bib.bib69)] on single-stream shallow
    CNNs. Sun et al. [[67](#bib.bib67)] explore various handcrafted feature (LBP),
    AlexNet, Reduced AlexNet and ResNet, followed by group-expression model or LSTM
    for group-level happiness intensity estimation. Building on this, Wei et al. [[68](#bib.bib68)]
    extend the group-level intensity estimation with VGGFace pretrained VGG-Face dataset.
    Furthermore, GER with ResNet18, ResNet34, MobileNet, DenseNet, Resnet50, Inception,
    GoogleNet, and VGG19 pretrained on Imagenet for scene-level information is explored
    in [[54](#bib.bib54), [41](#bib.bib41)]. The results demonstrate that VGG surpasses
    other architectures in GER and excels at distinguishing the complex hidden information
    in data.
  prefs: []
  type: TYPE_NORMAL
- en: While the aforementioned works are based on 2D CNN with image input, several
    works employ 3D CNN variants [[58](#bib.bib58), [53](#bib.bib53), [53](#bib.bib53)]
    or cascade network [[70](#bib.bib70)] to directly extract spatial and temporal
    features from video sequences. Inflated ResNet-3D [[58](#bib.bib58)], Temporal
    shift module (TSM) [[53](#bib.bib53)], and Temporal Binding Network (TBN) [[53](#bib.bib53)]
    are introduced in GER. Additionally, a end-to-end cross-attention cascade network [[70](#bib.bib70)]
    combined the idea of ClipBERT [[71](#bib.bib71)] modules with the temporal sequence
    to enhance the representation in spatial and temporal dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 Multi-stream Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Single-stream model represent a basic structure in GER, extracting features
    solely from a single viewpoint such as the face or scene within group-level image.
    However, since group-level images encompass diverse and rich information, a single
    view may not provide sufficient insight. As we discussed in Section [IV](#S4 "IV
    Input modality ‣ A Survey of Deep Learning for Group-level Emotion Recognition"),
    employing various inputs from different perspectives can effectively explore spatial
    and temporal information. Hence, multi-stream networks have been adopted in GER
    to extract features through multiple inputs. Generally, multi-stream networks
    can be categorized into networks with two inputs, more than two inputs, and handcrafted
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-stream networks with two inputs. According to [[72](#bib.bib72)], the
    face plays a crucial role in expressing the emotion. Therefore, among multi-stream
    networks with local and global inputs, the face remains a primary input. Several
    studies incorporated the face as local information and the scene as global information.
    They combined a convolution neural network for face while another neural network
    on scene descriptors for GER [[42](#bib.bib42), [44](#bib.bib44), [66](#bib.bib66),
    [73](#bib.bib73), [45](#bib.bib45)]. Experiment results of these studies demonstrate
    that models based on face outperforms those on other methods and other modality
    is still competitive and useful to GER. Moreover, the multi-stream networks have
    shown improvement in GER. Furthermore, Zhang et al. [[74](#bib.bib74)] presented
    a semi-supervised group-level emotion recognition (SSGER) framework based on contrastive
    learning, learning efficient features from both labeled and unlabeled images,
    where face images and scene images are utilized. In addition to visual features,
    audio feature are also being considered in dynamic GER. Augusma et al. [[75](#bib.bib75)]
    proposed branches for both video and audio, with cross-attention between modalities
    for GER. In their work, the video branch is based on a fine-tuned ViT architecture,
    while the audio branch extracts Mel-spectrograms and feeds them through CNN blocks
    into a transformer encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-stream networks with more than two inputs. As mentioned in the previous
    section, bottom-up components involve individual emotions, while top-down components
    consider contextual factors such as the background of an image [[23](#bib.bib23)].
    Therefore, to enhance group feature representation, some works [[51](#bib.bib51),
    [56](#bib.bib56), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78), [48](#bib.bib48),
    [53](#bib.bib53), [79](#bib.bib79), [65](#bib.bib65)] have investigated combinations
    of more than one top-down component and bottom-up components. Guo et al. [[51](#bib.bib51)]
    designed a hybrid network that incorporates scene features, skeleton features,
    and local facial features with deep convolutional neural networks. Additionally,
    they [[76](#bib.bib76)] further used visual attention attention mechanism to fuse
    face, scene, skeletons, and salient regions. Different from the aforementioned
    studies, Fujii et al. [[78](#bib.bib78), [48](#bib.bib48)] proposed a two-stage
    architecture for GER. The first stage performs binary classification based on
    facial expression to distinguish “Positive” labels, including discriminative facial
    expressions from others. For second stage, they considered exploiting object-wise
    semantic information and scene background for the second classification. Recently,
    several researchers [[53](#bib.bib53), [56](#bib.bib56)] investigated multi-stream
    network for dynamic GER. Spatio-temporal features and static features were exploited
    by Sun et al. [[53](#bib.bib53)]. The fusion of several spatio-temporal modality
    adopted RGB, RGB difference, optical flow, warped optical flow, and audio, while
    image-level CNNs were designed based on face and body images. Moreover, a hybrid
    network fusing audio stream, facial emotion stream, environmental object statistics
    stream (EOS), and video stream are designed with temporal shift module and SVM
    for GER [[56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: Multi-stream networks with handcafted features. According to the analysis, the
    facial emotion or movements of group-level emotion are highly related to face
    textures, while scene information contains more abundant information for GER,
    the handcrafted features for low-level representation also plays an important
    role in GER. Multiple works [[68](#bib.bib68), [43](#bib.bib43)] combined deep
    features for face-level and handcrafted features for scene-level to leverage the
    low-level and high-level information for robust GER.
  prefs: []
  type: TYPE_NORMAL
- en: V-B3 Cascade network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For GER, dealing with varying numbers of face between group-level images poses
    a significant challenge. As discussed in Section [IV-A](#S4.SS1 "IV-A Static image
    ‣ IV Input modality ‣ A Survey of Deep Learning for Group-level Emotion Recognition"),
    current cascade networks can be categorized to two types. The first category utilizes
    variants of CNN such as ResNet, VGG, followed by decision-level score fusion.
    In contrast, the second category combines various CNN and RNNs to address the
    inconsistency in the number of faces between two group-level image. In this section,
    we will discuss the details of the second category [[67](#bib.bib67), [76](#bib.bib76),
    [80](#bib.bib80), [70](#bib.bib70), [81](#bib.bib81)].
  prefs: []
  type: TYPE_NORMAL
- en: In GER, a critical issue is how to effectively model the variability in the
    number of faces. LSTM is a primary method. Sun et al. [[67](#bib.bib67)] were
    the first to investigate the combination of CNNs and LSTM. They explored the use
    of AlexNet, Reduced AlexNet, and ResNet to extract the individual faces in a group.
    Subsequently, a weighted LSTM was used to assign different weights to each facial
    feature based on factors such as the size of the face and the distance between
    them. Furthermore, visual attention mechanisms were incorporated in LSTM for GER [[76](#bib.bib76),
    [70](#bib.bib70)]. In [[70](#bib.bib70)], a cascade network containing CNN and
    LSTM was employed to extract image-level and audio-level features, respectively.
    An attention mechanism was then introduced to compute important features at each
    time step. Additionally, Li et al. [[82](#bib.bib82)] employed a similar architecture
    at the face-level for GER. However, in contrast to [[67](#bib.bib67)], skeletons
    and scene features were directly fused at the end. Moreover, skeletons information
    extracted by OpenPose toolkit was considered in cascade network proposed by Slogrove et
    al. [[80](#bib.bib80)]. In their approach, the coordinates and confidence information
    of all individual key points were fed into an LSTM as a sequence for modeling,
    resulting in a group-level emotion classification. Additionally, speech signals
    were investigated for GER in [[81](#bib.bib81)]. They proposed a method incorporating
    a cascade network with multi-task learning for GER, using deep spectral features
    on speech signal. The cascade network based on CNN and RNN, was designed to extract
    discriminative deep spectral features, while multi-task learning combines emotion
    recognition and speaker identification tasks during the model training process.
  prefs: []
  type: TYPE_NORMAL
- en: V-B4 GCN based network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Individuals within a group often exhibit diverse social relationships with others.
    To highlight this social aspect, several works [[29](#bib.bib29), [3](#bib.bib3)]
    have utilized graph convolutional networks (GCNs) to model both visual features
    and social context within group-level images. In these approaches, the emotional
    states of individuals are treated as node features, while the interactions between
    individuals are represented as graph edges, thus forming a graph structure. Guo et
    al. [[29](#bib.bib29)] introduced a group-level emotion recognition method based
    on four cues, where faces, bodies, objects, and the entire image are transformed
    into a graph structure. This graph represents the relationships within the group
    based on these four cues, facilitating group-level emotion recognition. Additionally,
    Wang et al. [[3](#bib.bib3)] proposed a context-consistent cross-graph neural
    network to mitigate emotional biases resulting from different cues in multi-cue
    emotion recognition.
  prefs: []
  type: TYPE_NORMAL
- en: V-B5 Attention based network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to prioritize important character or object features that play a pivotal
    role in group emotions, five studies have incorporated attention mechanisms. Gupta et
    al. [[83](#bib.bib83)] detect local facial emotions by employing an attention
    mechanism to concentrate on more pertinent local information on the face, generating
    probability attention weights through the Softmax function. The weighted sum of
    facial features is then calculated based on these attention weights to produce
    a single facial feature vector representation. Additionall, Guo et al. [[76](#bib.bib76)]
    and Khan et al. [[79](#bib.bib79)] also integrated attention mechanisms. They
    introduced a novel region attention network (RAN) to detect and extract crucial
    features in facial regions. The region attention mechanism comprises an attention
    generator and an attention applier, where the former generates adaptive region
    attention weights to emphasize important facial features in different regions,
    while the latter applies these generated region attention weights to the output
    of the feature extractor to enhance the distinction of facial features. Furthermore,
    Wang et al. [[84](#bib.bib84)] proposed a cascaded attention network, which leverages
    the importance of each face in the image to generate a global representation based
    on all faces, effectively focusing on the feature information of the most important
    face.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Transformer-based architectures have found extensive applications
    in NLP [[85](#bib.bib85)] and computer vision [[86](#bib.bib86)] tasks. Inspired
    by the significant success of Transformer architecture in various tasks, two recent
    studies [[75](#bib.bib75), [87](#bib.bib87)] have explored the application of
    transformers to the GER task. Augusma et al. [[75](#bib.bib75)] initially utilized
    the visual Transformer mechanism and the BERT framework to extract features from
    global images and speech, respectively. They employed a cross-attention mechanism
    to learn the weights of the two modes and subsequently performed weight fusion.
    Moreover, a dual-branch cross-patch attention Transformer (DCAT) was proposed
    to incorporate the psychological concept of the Most Important Person (MIP) and
    the global image.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, GER network architectures can be broadly categorized into single-stream,
    multi-stream, cascade networks, GNN-based networks, and attention-based networks.
    While single-stream serves as the fundamental model, it only considers a single
    view of the group-level image. To leverage more information, multi-stream networks
    learn features from multiple perspectives for robust GER. Additionally, as the
    group size fluctuates, cascade networks sequentially incorporate various modules
    like RNNs and LSTMs to construct an end-to-end GER network. GNN effectively models
    interactions between individuals based on social relationships. Conversely, the
    attention mechanism, inspired by the psychological concept of the most important
    person, focuses on extracting key features from all individuals. In the future,
    combining more effective modules in multi-stream, cascade, and attention-based
    approaches could further enhance GER performance.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Fusion stage and scheme
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In GER, the fusion stage plays a crucial role in integrating information from
    multiple cues to improve the accuracy of emotion recognition. It encompass various
    methods for combining features extracted from different modalities, such as facial
    expressions, scene, skeleton etc. One common fusion approach is score-level, as
    used in studies like  [[76](#bib.bib76), [49](#bib.bib49), [47](#bib.bib47)].
    In their approaches, the output scores from individual modalities are combined
    using techniques like averaging or mean voting.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, feature-level fusion used in studies like those by [[78](#bib.bib78),
    [48](#bib.bib48), [33](#bib.bib33)], integrates raw feature representations extracted
    from each modality before feeding them into a classifier. This approach allows
    the model to learn more complex relationships between different modalities but
    may be more computationally intensive. Besides score-level and feature-level fusion,
    kernel-based [[88](#bib.bib88), [69](#bib.bib69)] and loss function-based [[50](#bib.bib50),
    [74](#bib.bib74)] fusion are two alternative ways to fuse multi-modality. For
    example, [[74](#bib.bib74)] proposed a weight cross-entropy loss function on Scene-Face
    network by combining face and scene information.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different from classical methods, where the feature extraction and classification
    are independent, deep networks can perform end-to-end classification through loss
    functions by penalizing the deviation between predicted and true labels during
    training. Most GER works directly apply the commonly used softmax cross-entropy
    loss [[39](#bib.bib39)]. The softmax loss is typically effective at correctly
    classifying known categories. However, in practical classification tasks, the
    classification of unknown samples is also essential. Therefore, to achieve better
    generalization ability, it is crucial to further enhance inter-class difference
    and reduce intra-class variation, especailly for data scarcity. Metric learning
    techniques, such as contrastive loss [[89](#bib.bib89)], have been developed to
    ensure intra-class compactness and inter-class separability by measuring the relative
    distances between inputs. Wang et al. [[90](#bib.bib90)] proposed a contrastive
    learning-based self-attentive network. In this approach, different features are
    embedded into a vector space, and the similarity and difference of features are
    learned by enhancing the similarity between samples of the same class and reducing
    the similarity between samples of different classes. Then, adaptive weight calculation
    and weighted average fusion are performed to adaptively fuse features at different
    levels. Although the above two methods have achieved good performance, they are
    still limited to static images. Additionally, metric learning loss often requires
    effective sample mining strategies for robust recognition performance. Metric
    learning alone may not suffice for learning a discriminative metric space for
    GER. To address these challenges, Zhang et al. [[74](#bib.bib74)] proposed a semi-supervised
    group-level emotion recognition framework based on contrastive learning to learn
    efficient features from both labeled and unlabeled images. To alleviate the uncertainty
    of given pseudo-labels, they introduce Weight Cross-Entropy Loss (WCE-Loss) to
    suppress the influence of samples with unreliable pseudo-labels in the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, although most current GER approaches utilize the standard softmax
    cross-entropy loss, only a limited number of studies have explored alternative
    loss functions such as contrastive learning loss or introduced novel loss functions
    to enhance inter-class separability, intra-class compactness, and achieve well-balanced
    learning. Looking ahead, investigating more effective loss functions targeting
    discriminative representations for group-level emotion features holds significant
    promise as a direction for future research in GER.
  prefs: []
  type: TYPE_NORMAL
- en: VI Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VI-A Performance metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The standard evaluation metric for GER typically involves using accuracy for
    group-level emotion recognition and mean square error for group happiness estimation.
    Accuracy assesses the proportion of correct predictions relative to the total
    number of evaluated samples, providing a measure of the model’s overall performance
    in correctly identifying group-level emotion. Conversely, mean square error quantifies
    the average squared difference between the predicted and true happiness values,
    offering a measure of the model’s accuracy in estimating the happiness levels
    of a group.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Model Evaluation Protocols
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cross-validation stands as a widely utilized protocol for evaluating GER performance.
    This protocol involves dividing the dataset into train, validation, and test sets,
    ensuring fair verification of deep learning architectures on group emotion datasets.
    Cross-validation in the GER contains fixed partition validation and K-fold cross
    validation. The first kind of cross-validation is commonly used as the official
    evaluation method in the competitions such as the Emotion Recognition in the wild
    challenge (EmotiW) [[27](#bib.bib27)]. Here, the train, validation, and test sets
    are pre-determined and remain fixed throughout the competition, eliminating randomization.
    Participants receive the train and validation sets at the competition’s outset
    for model development, while the test set is revealed later for final performance
    assessment and ranking. On the other hand, K-fold cross-validation protocol is
    also prevalent in GER research. This protocol involves randomly dividing the dataset
    into $k$ equally sized parts, with each part serving as a test set in turn while
    the remaining portions constitute the training data. The process repeats $k$ times,
    with each partition serving as the test set once. The choice for $k$, typically
    4 or 5 in GER, can significantly impact evaluating time while ensuring robust
    performance assessment.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Performance analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [II](#S6.T2 "TABLE II ‣ VI-C Performance analysis ‣ VI Experiments ‣ A
    Survey of Deep Learning for Group-level Emotion Recognition") reports the performance
    of various deep learning models for GER as reported in EmotiW since 2016\. With
    the exceptions of [[54](#bib.bib54), [56](#bib.bib56)], most methods proposed
    in these competition aim to fuse more than two cues such as face, scene, and skeleton,
    among others. Notably, established neural network architectures such as VGG have
    been commonly employed for extracting facial expression feature. Additionally,
    to accommodate the variable number of faces/objects, variations of RNN have been
    prevalent in many algorithms. Furthermore, since 2019, attention modules, facilitated
    by the successful application of transformer architectures, have gained widespread
    adoption in Emotion Recognition in the Wild competitions. Regarding performance
    metrics, in the HAPPEI dataset, the lowest reported RMSE is 0.822\. In the GAF2.0
    competition, the highest reported accuracy is 80.9%, while in GAF3.0, it is 68.08%.
    This suggests that GAF3.0 introduced more competitive samples and increased the
    challenge level. Additionally, a new track called Group Cohesion, derived from
    GAF3.0, was introduced to evaluate group cohesion. Despite improvements in team
    performance between EmotionW2016 and EmotiW2019, there is still significant room
    for enhancing overall performance. Since 2020, the utilization of video-based
    datasets has become prevalent, indicating a growing consideration for temporal
    information in algorithm development. Moreover, traditional feature extraction
    techniques have been gradually supplanted by deep learning methods in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond competition, many researchers have also explored GER using group-level
    emotion databases like HAPPEI and GAF. Tables [III](#S6.T3 "TABLE III ‣ VI-C Performance
    analysis ‣ VI Experiments ‣ A Survey of Deep Learning for Group-level Emotion
    Recognition") and [IV](#S6.T4 "TABLE IV ‣ VI-C Performance analysis ‣ VI Experiments
    ‣ A Survey of Deep Learning for Group-level Emotion Recognition") offer detailed
    comparison of method or model accuracies proposed by researchers over the last
    decade, excluding those participating in EmotionW competition. Notably, except
    for [[91](#bib.bib91), [69](#bib.bib69), [88](#bib.bib88), [33](#bib.bib33)],
    all methods fused more than two cues. Among these, face, pose/skeleton, and object
    are regarded as local component, while scene information serves as global information.
    This approach aligns with the concept of bottom-up and top-down components mentioned
    in group emotion theory. Furthermore, widely recognized network blocks such as
    VGG, Xception, ResNet and AlexNet have found extensive use in GER due to their
    promising performance in image and face recognition tasks. Additionally, the fusion
    of various networks enables better exploitation of complementary information between
    them. Common fusion schemes include Average and Feature concatenation, which provide
    straightforward solutions to the fusion problem. However, some researchers have
    proposed novel approaches to fuse multi-modality features. For example, Guo et
    al. [[29](#bib.bib29)] introduced a graph convolutional network to facilitate
    information exchange among features extracted from different models. Zhu et al. [[50](#bib.bib50)]
    proposed a uncertain-aware learning to extract more robust representation from
    face, object, and scene modalities for GER. Moreover, unlike competitions, recently
    proposed GER methods have been evaluated across various databases, such as GroupEmoW,
    GAF, and GECV-GroupImg to access their generalization ability. With the introduction
    of databases like VGAF and GECV-GroupVid, researchers have started exploring the
    spatiotemporal GER.
  prefs: []
  type: TYPE_NORMAL
- en: In general, modality fusion can yield promising results across all datasets.
    Different modalities contribute diverse information, allowing for more comprehensive
    exploration of limited GER samples. Since the combined inputs offer robust GER
    solutions, multi-stream networks are recommended to effectively learn representations
    from available modalities. In contrast, single-modality approaches perform worse
    due to limited information and redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: From Tables [III](#S6.T3 "TABLE III ‣ VI-C Performance analysis ‣ VI Experiments
    ‣ A Survey of Deep Learning for Group-level Emotion Recognition") and [IV](#S6.T4
    "TABLE IV ‣ VI-C Performance analysis ‣ VI Experiments ‣ A Survey of Deep Learning
    for Group-level Emotion Recognition"), it is clear that fusion of scores and features
    is a common approach in integrating multiple modalities. Additionally, there is
    a growing trend towards using loss functions for multi-modality fusion. Fusion
    schemes like cross-attention, GCN, ECL, and NVPF have demonstrated state-of-the-art
    results across all databases. This is likely due to the challenges posed bythat
    the limited number of GER samples and group sizes, making leveraging additional
    data sources as reasonable and effective solution.
  prefs: []
  type: TYPE_NORMAL
- en: Presently, scene information based on LSTM and averaging features across all
    faces are commonly employed in video-based GER studies. This is likely because
    flexible group sizes pose the main challenge for video-based GER. Recently, Quach et
    al. [[33](#bib.bib33)] proposed a Non-Volume Preserving Fusion (NVPF) mechanism
    with LSTM to model spatial representation between groups of multiple faces and
    temporal relationships between multiple video frames. However, the small-sample
    GE dataset limits the ability to model group-level features for video-based GER.
    The combination of transfer learning and graph-based methods is anticipated to
    be a promising direction for future GER studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Performance comparison of remarkable deep learning techniques published
    in ACM Library in Group emotion competition in Emotion Recognition in the Wild
    (EmotiW) challenge [[92](#bib.bib92), [2](#bib.bib2), [27](#bib.bib27), [30](#bib.bib30),
    [38](#bib.bib38), [34](#bib.bib34)]. The best performance for specific database
    on the test set is bold and red color.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset (Year) | Cate. | Ref. | Modality | Network architecture | Fusion
    scheme | Fusion stage | Pre-train | Prot | Perf. |'
  prefs: []
  type: TYPE_TB
- en: '| F | S | P | A | T |'
  prefs: []
  type: TYPE_TB
- en: '| HAPPEI* (2016) | 6 | [[42](#bib.bib42)] | ✓ | ✓ |  |  |  | ResNet for F CENTRIST+PCA
    for S | LSTM | Feature | FER2013 | val | 0.494 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.822 |'
  prefs: []
  type: TYPE_TB
- en: '| [[67](#bib.bib67)] | ✓ |  |  |  |  | AlexNet | LSTM | Feature | FER2013 |
    val | 0.4942 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.836 |'
  prefs: []
  type: TYPE_TB
- en: '| [[93](#bib.bib93)] | ✓ | ✓ |  |  |  | ResNet+LSTM for F CENTRIST/VGG for
    S | Concat | Feature | - | val | 0.55 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.865 |'
  prefs: []
  type: TYPE_TB
- en: '| GAF2.0 (2017) | 3 | [[39](#bib.bib39)] | ✓ | ✓ |  |  |  | VGG for F ImageNet
    for S | Soft aggregation &weighting | Score | VGG face ImageNet | val | 75.39%
    |'
  prefs: []
  type: TYPE_TB
- en: '| test | 78.53% |'
  prefs: []
  type: TYPE_TB
- en: '| [[43](#bib.bib43)] | ✓ | ✓ |  |  |  | HOG+FV for S VGG+VLAD for F | Cont
    | Feature | - | val | 65% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 75.10% |'
  prefs: []
  type: TYPE_TB
- en: '| [[44](#bib.bib44)] | ✓ | ✓ |  |  |  | AlexNet for F Context for S | Bayesian
    Network | Score | - | val | 67.75% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 64.68% |'
  prefs: []
  type: TYPE_TB
- en: '| [[51](#bib.bib51)] | ✓ | ✓ | ✓ |  |  | VGG for F Inception&ResNet for P Inception&VGG
    for S | SVM | Score | FER2013 GENKI-4K | val | 80.05% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 80.61% |'
  prefs: []
  type: TYPE_TB
- en: '| [[66](#bib.bib66)] | ✓ | ✓ |  |  |  | Xception for F VGG for S | Cont | Feature
    | FER2013 | val | 72.38% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 63.43% |'
  prefs: []
  type: TYPE_TB
- en: '| [[68](#bib.bib68)] | ✓ | ✓ |  |  |  | VGG+DCNN for F CENTRIST+VGG for S |
    LSTM/SVM | Feature | - | val | - |'
  prefs: []
  type: TYPE_TB
- en: '| test | 79.78% |'
  prefs: []
  type: TYPE_TB
- en: '| [[73](#bib.bib73)] | ✓ | ✓ |  |  |  | 4-layer CNN for F ResNet for S | Average
    | Score | FERPlus Places | val | 83.7% |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | test | 80.9% |'
  prefs: []
  type: TYPE_TB
- en: '| GAF3.0 (2018) | 3 | [[76](#bib.bib76)] | ✓ | ✓ | ✓ |  |  | VGG for F Inception/SE-ResNet
    for S ResNet for P | Weight Average | Score | FER2013 GENKI-4K | val | 78.98%
    |'
  prefs: []
  type: TYPE_TB
- en: '| test | 68.08% |'
  prefs: []
  type: TYPE_TB
- en: '| [[77](#bib.bib77)] | ✓ | ✓ |  |  |  | ResNet for F VGG for S | Weight average
    | Score | FER2013 RAF-DB | val | 78.39% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 65.59% |'
  prefs: []
  type: TYPE_TB
- en: '| [[83](#bib.bib83)] | ✓ | ✓ | ✓ |  |  | DenseNet for S SphereFace for F |
    Cont | Feature | ImageNet CASIA-Webface | val | 80.98% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 64.83% |'
  prefs: []
  type: TYPE_TB
- en: '| [[84](#bib.bib84)] | ✓ | ✓ |  |  |  | CAN for F, ResNet for S, SE-net for
    P | Average | Score | FERPlus | val | 86.7% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 67.48% |'
  prefs: []
  type: TYPE_TB
- en: '| Group Cohesion* (2019) | 4 | [[94](#bib.bib94)] | ✓ | ✓ | ✓ |  |  | CAN for
    F SE-Net for S/P | Average | Score | FERPlus ImageNet | val | 0.5588 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.4382 |'
  prefs: []
  type: TYPE_TB
- en: '| [[95](#bib.bib95)] | ✓ | ✓ | ✓ |  |  | DensePose for S ResNet/Inception/NasNet
    for S ResNet for F | Average | Score | VGG Face2 RAF-DB | val | 0.517 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.416 |'
  prefs: []
  type: TYPE_TB
- en: '| [[96](#bib.bib96)] | ✓ | ✓ | ✓ |  |  | VGG+SVR for F Efficient+SVR for P
    Densenet+SVR for S | Grid Search | Score | FER2013 Emotic | val | 0.672 |'
  prefs: []
  type: TYPE_TB
- en: '| test | 0.444 |'
  prefs: []
  type: TYPE_TB
- en: '| VGAF (2020) | 3 | [[54](#bib.bib54)] |  | ✓ |  |  |  | VGG+ML | - | - | ImageNet
    | val | 57.18% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 59.13% |'
  prefs: []
  type: TYPE_TB
- en: '| [[81](#bib.bib81)] |  |  | ✓ |  |  | DeepSpectrum+AlexNet +VGG+DenseNet |
    Mean | Score | - | val | 58.09% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 62.70% |'
  prefs: []
  type: TYPE_TB
- en: '| [[56](#bib.bib56)] | ✓ | ✓ | ✓ | ✓ | ✓ | TSM for T Dense for F, OpenSmile
    for A | Average | Score | FER2013 | val | 74.28% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 76.85% |'
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] |  | ✓ |  | ✓ | ✓ | K-injection | Cont | Feature | - |
    val | 66.19% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 66.40% |'
  prefs: []
  type: TYPE_TB
- en: '| [[53](#bib.bib53)] | ✓ | ✓ | ✓ |  | ✓ | TSM TBN | Weight sum | Score | ImageNet
    | val | 71.93% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 70.77% |'
  prefs: []
  type: TYPE_TB
- en: '| VGAF (2023) | 3 | [[55](#bib.bib55)] | ✓ | ✓ |  | ✓ | ✓ | ResNet for F, SeNet
    for S Hubert large for A | Cont | Feature | FER2013 ImageNet | val | 68.41% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 72% |'
  prefs: []
  type: TYPE_TB
- en: '| [[75](#bib.bib75)] |  | ✓ |  | ✓ | ✓ | ViT-large for V CNN+Transformer for
    A | Average | Feature | ImageNet | val | 78.72% |'
  prefs: []
  type: TYPE_TB
- en: '| test | 75.13% |'
  prefs: []
  type: TYPE_TB
- en: '| Prot.: Protocol; Cate.: Category. |'
  prefs: []
  type: TYPE_TB
- en: '| TSM: Temporal Shift Module; TBN: Temporal Binding Network; CAN: Cascade Attention
    Network. |'
  prefs: []
  type: TYPE_TB
- en: '| F: Face, S: Scene, P: Pose/skeleton, A: Audio, T: Temporal. |'
  prefs: []
  type: TYPE_TB
- en: '| Concat: Concatenation. |'
  prefs: []
  type: TYPE_TB
- en: '| * For HAPPEI and Group Coheison, RMSE is used as performance metric, while
    for other databases, accuracy is used. |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Performance comparison of remarkable deep learning techniques in
    image-based group-level emotion databases.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Database | Modality | Network | Fusion Scheme | Fusion Stage | Prot.
    | Cate. | Perf. |'
  prefs: []
  type: TYPE_TB
- en: '| F | S | P | O |'
  prefs: []
  type: TYPE_TB
- en: '| [[91](#bib.bib91)] | GAF2.0 | ✓ |  |  |  | AlexNet | HeatMap | Feature |
    val | 3 | 55.23% |'
  prefs: []
  type: TYPE_TB
- en: '| [[45](#bib.bib45)] | ✓ | ✓ |  |  | Xception for face VGG for scene | Concat
    | Feature | val | 3 | 71.83% |'
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | ✓ |  |  |  | CNN, RVLBP | DMKL | Kernel | val | 3 |
    79.49% |'
  prefs: []
  type: TYPE_TB
- en: '| [[49](#bib.bib49)] | ✓ | ✓ |  |  | MobileNet for scene LSTM for face | Average
    | Score | 1-fold | 3 | 78% |'
  prefs: []
  type: TYPE_TB
- en: '| [[47](#bib.bib47)] | GAF3.0 | ✓ |  |  |  | Inception/VGG for scene VGG for
    face | SVM | Score | val | 3 | 70.1% |'
  prefs: []
  type: TYPE_TB
- en: '| [[82](#bib.bib82)] | ✓ | ✓ | ✓ |  | VGG+LSTM for face Dense for Skeleton
    Attention for Scene | Concat | Feature | val | 3 | 62.90% |'
  prefs: []
  type: TYPE_TB
- en: '| [[88](#bib.bib88)] | MultiEmoVA HAPPEI* GAF2.0 | ✓ |  |  |  | RVLBP, VGG
    | SVM-CGAK | Kernel | 5-fold 4-fold val | 5 6 3 | 54.40% 0.4920 72.17% |'
  prefs: []
  type: TYPE_TB
- en: '| [[48](#bib.bib48)] | GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | VGG, attention for face
    VGG, attention for object VGG for scene | Hierarchical | Feature | val | 3 | 80.41%
    76.61% |'
  prefs: []
  type: TYPE_TB
- en: '| [[50](#bib.bib50)] | MultiEmoVA GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | ResNet for
    face VGG for object VGG for scene | UAL | Loss | 5-fold val val | 5 3 3 | 61.22%
    79.19% 77.10% |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] | GroupEmoW GAF2.0 SocEID | ✓ | ✓ | ✓ | ✓ | VGG for face
    SE-ResNet for skeleton SENet for object Inception for scene | GNN | Score | test
    val test | 3 3 8 | 89.14% 78.16% 91.61% |'
  prefs: []
  type: TYPE_TB
- en: '| [[79](#bib.bib79)] | GAF2.0 GroupEmoW | ✓ | ✓ |  | ✓ | Resnet for all modalities
    Attention module | CARAN | Loss | val test | 3 | 67.61% 90.18% |'
  prefs: []
  type: TYPE_TB
- en: '| [[74](#bib.bib74)] | GroupEmoW GAF2.0 GAF3.0 | ✓ | ✓ |  |  | ResNet for all
    modalities | FusionNet | Loss | test val val | 3 | 88.67% 78.51% 77.01% |'
  prefs: []
  type: TYPE_TB
- en: '| [[3](#bib.bib3)] | GroupEmoW GAF2.0 GAF3.0 | ✓ | ✓ |  | ✓ | ResNet+LSTM+GNN
    for face SE-ResNet+GNN for object SE-ResNet+GNN for scene | ECL | Loss | test
    val val | 3 | 90.06% 79.45% 79.95% |'
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] | GAF3.0 GroupEmoW | ✓ | ✓ |  |  | Multi-scale Transformer
    | DCAT | Feature | val test | 3 | 79.20% 90.47% |'
  prefs: []
  type: TYPE_TB
- en: '| [[33](#bib.bib33)] | GECV-GroupImg GAF3.0 | ✓ |  |  |  | EmoNet | NVPF |
    Feature | val | 3 | 77.02% 76.12% |'
  prefs: []
  type: TYPE_TB
- en: '| Prot.: Protocol; Cate.: Category. |'
  prefs: []
  type: TYPE_TB
- en: '| NVPF: Non-volume Preserving-based Fusion; DMKL: Deep Multiple Kernel Learning;
    GNN: Graph Neural Network. |'
  prefs: []
  type: TYPE_TB
- en: '| UAL: Uncertain-aware Learning; CARAN: Context-aware Regional Attention Network;
    Concat: Concatenation. |'
  prefs: []
  type: TYPE_TB
- en: '| ECL: Emotion context-consistent learning; DCAT: Dual-branch Cross-Patch Attention
    Transformer. |'
  prefs: []
  type: TYPE_TB
- en: '| F: Face, S: Scene, P: Pose/skeleton, O: Object. |'
  prefs: []
  type: TYPE_TB
- en: '| * For HAPPEI, RMSE is used as performance metric, while for other databases,
    accuracy is used. |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Performance comparison of remarkable deep learning techniques in
    video-based group-level emotion (VGAF) database, where F, S, P, and A mean face,
    scene, pose, and audio modality, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Dataset | Modality | Network | Fusion Scheme | Fusion Stage | Prot.
    | Cate. | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| F | S | P | A |'
  prefs: []
  type: TYPE_TB
- en: '| Sharma+[[32](#bib.bib32)] | VGAF |  | ✓ |  | ✓ | LSTM for scene OpenSMILE
    for audio | Concat | Feature | val | 3 | 47.50% |'
  prefs: []
  type: TYPE_TB
- en: '| Pinto+[[58](#bib.bib58)] |  | ✓ |  | ✓ | ResNet for scene Bi-LSTM for audio
    | SVM | Score | val | 3 | 65.74% |'
  prefs: []
  type: TYPE_TB
- en: '| Evtodienko+[[70](#bib.bib70)] |  | ✓ |  | ✓ | Hubert+Attention for audio
    ResNet+Attention for scene | Concat | Feature | val | 3 | 60.37% |'
  prefs: []
  type: TYPE_TB
- en: '| Quach+[[33](#bib.bib33)] | GECV -GroupVid | ✓ |  |  |  | EmoNet | TNVPF |
    Feature | test | 3 | 70.97% |'
  prefs: []
  type: TYPE_TB
- en: '| Prot.: Protocol; Cate.: Category. |'
  prefs: []
  type: TYPE_TB
- en: '| TNVPF: Temporal Non-volume Preserving-based Fusion. |'
  prefs: []
  type: TYPE_TB
- en: '| Concat: Concatenation. |'
  prefs: []
  type: TYPE_TB
- en: '| F: Face, S: Scene, P: Pose/skeleton, A: Audio. |'
  prefs: []
  type: TYPE_TB
- en: VII Challenges and future direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Group-level emotion recognition in unconstrained environments has become significant
    attention within the computer vision community, offering substantial implications
    for social public security and education. This article delves into the intricate
    concepts of group dynamics and emotion, along with methodologies for recognizing
    group-level emotion, and pertient datasets, aiming to provide a comprehensive
    analysis of the current landscape and future trajectories of GER. This endeavor
    furnishes a robust theoretical foundation for potential applications of GER in
    domains such as social psychology, human-computer interaction, and smart cities.
    In this section, we summarize and discuss the future prospects of GER from three
    pivotal dimensions: database-level, technique-level, multimodality, and evaluation
    metric.'
  prefs: []
  type: TYPE_NORMAL
- en: GER encounters three primary technical challenges. Firstly, the accurate discernment
    of emotions inherently presents complex, compounded by the potential bias introduced
    by human subjective labeling in annotating datasets. Secondly, the fluctuating
    number of individuals and diverse scenes within a group necessitates enhanced
    generalization and robustness of features. Furthermore, different extracted features
    may convey inconsistent emotions. Lastly, the cross-fusion of diverse features
    in a multimodal model and achieving end-to-end feature learning pose significant
    challenges. Despite these obstacles hindering GER advancement, its potential applications
    span a wide spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A GER Database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While currently available group emotion databases primarily collect images and
    videos from websites and media platforms, their sample size remain relatively
    small. On the other hand, Smith et al. [[97](#bib.bib97)] emphasize the significance
    of the changes in group emotions along time, noting that individuals may react
    differently to external group members based on their prevailing emotion states.
    Pantic et al. [[31](#bib.bib31)] found that dynamic videos provide more discriminative
    information to extract temporal changes in emotions. Therefore, the collection
    of richer and more realistic video data holds promise for furnishing contextual
    semantic insights into group interactions and dynamic processes, facilitating
    a more nuanced observation of emotional dynamics within team.
  prefs: []
  type: TYPE_NORMAL
- en: Presently, database annotations are typically derived from independent observers
    rating the images, lacking subjective evaluation like self-reporting measurements.
    This limitation may lead to the challenges in acquiring such measurements from
    online media platforms. However, the absence of subjective evaluation may potentially
    yield considerable performance in group-level emotion recognition. By integrating
    subjective evaluation and other auxiliary information, computer scientists can
    devise advanced methodologies for analyzing data, bridging the gap between computational
    methods and social science research. Moreover, existing databases primarily reply
    on methods such as multi-observer cross-calibration to categorize images or videos,
    which may introduce calibration biases due to cultural disparities and overlook
    the fundamental tenets of group emotion posited by social psychologists [[23](#bib.bib23)].
    Therefore, it is crucial to expanding the repertoire of basic emotion categories
    to encompass more generalized emotion states in affective computing is imperative.
    Additionally, involving social psychologists in the data collection and annotation
    process can furnish more rational and valuable calibration information.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B GER Technique
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The methods ranging from Convolutional Neural Networks (CNN) to Recurrent Neural
    Networks (RNN), Hybrid Networks combining CNN and RNN, and Graph Convolutional
    Networks (GCN), have already demonstrated remarkable success in GER. However,
    the challenge of limited sample sizes necessitates the exploration of unsupervised
    and self-supervised learning techniques. Unsupervised learning technique like
    Generative adversarial networks [[98](#bib.bib98)] and generative AI [[99](#bib.bib99)]
    can be valuable for learning meaningful representations from unannotated data,
    enabling the development of robust deep GER models. These techniques help in extracting
    rich features from data, even when labeled examples are scarce. Similarly, self-supervised
    learning paradigms, such as contrastive learning [[100](#bib.bib100)], offer a
    way to learn representations from auxiliary tasks, enhancing the generalization
    capabilities of GER models across diverse group contexts.
  prefs: []
  type: TYPE_NORMAL
- en: As GER systems move towards real-world deployment, the demand for continual
    learning and adaptive capabilities become increasingly critical. Deep learning
    architectures will need to evolve to accommodate dynamic changes in group compositions,
    social contexts, and environmental conditions over time. Incremental learning
    strategies, lifelong learning approaches, and adaptive neural networks will play
    crucial roles in enabling models to adapt and refine their representations based
    on incoming data streams. Moreover, techniques for mitigating catastrophic forgetting
    and domain adaptation will be essential for ensuring the long-term stability and
    effectiveness of GER systems. These methods help models retain previously learned
    knowledge while adapting to new information, thereby enhancing their robustness
    in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: By embracing unsupervised and self-supervised learning techniques, as well as
    continual learning and adaptive methodologies, future deep learning systems will
    be better equipped to capture the nuanced dynamics of group-level emotions across
    diverse scenarios and domains. This holistic approach holds the potential to significantly
    advance the field of Group Emotion Recognition and its applications in various
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Multi-modal architecture for GER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the research process of GER, we have observed a continuous evolution towards
    richer and more diverse in feature representations. Initially, the focus was predominantly
    on a single facial feature. However, as research progressed, there was a shift
    towards utilizing multiple features concurrently, including facial features, local
    object features, and scene features. This enrichment of feature diversity has
    contributed to an enhanced accuracy of group-level emotion recognition to a certain
    extent. Moreover, researcher have begun to explore the integration of different
    types of data sources. For example, studies by Sharma et al. [[32](#bib.bib32)],
    Wang et al. [[57](#bib.bib57)], Liu et al. [[101](#bib.bib101)], and Pinto et
    al. [[58](#bib.bib58)] have incorporated both video frame features and audio features,
    while Liu et al. [[56](#bib.bib56)] and Sun et al. [[53](#bib.bib53)] have combined
    both facial and audio features. These approaches, known as multimodal methods,
    offer advantages such as robustness against interference, high interpretability,
    and broad applicability compared to single-modal methods.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, GER relied primarily on single-modal information. However, as
    the field has progressed and dataset forms have diversified, researchers have
    increasingly delved into multi-modal methods. These methods leverage various data
    forms including images, videos, and sounds. The adoption of multimodal fusion
    techniques in group-level emotion recognition not only facilitates a more comprehensive
    understanding of emotions but also enhances the accuracy and robustness of the
    recognition process.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the application of multi-modal fusion in GER poses several challenges.
    Obtaining and annotating datasets for group-level emotion recognition, particularly
    those encompassing different modalities, can be arduous. Furthermore, variations
    in feature extraction and fusion methods across modalities present additional
    complexities, making it challenging to harmonize features between different modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, while multi-modal fusion encounters challenges in GER, its potential
    and advantages are undeniable. With advancements in technology and ongoing research
    efforts, it is anticipated that these challenges will be gradually addressed,
    and multimodal methods will emerge as the primary research direction in the field
    of GER. Future research endeavors will likely focus on effectively integrating
    information from diverse modalities and constructing larger and more diverse datasets
    for GER, thereby presenting both challenges and opportunities in this dynamic
    field.
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Evaluation metric of GER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is worth noting that while accuracy is a common metric in GER, it can be
    influenced by biased data. To address this issue, F1-score offers a more comprehensive
    evaluation by considering True Positives (TP), False Positives (FP), and False
    Negatives (FN). This metric provides a balanced assessment of the true classification
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Although the data may not exhibit severe imbalance, as indicated in Table [I](#S3.T1
    "TABLE I ‣ III-C Dataset summary ‣ III Group-level emotion dataset ‣ A Survey
    of Deep Learning for Group-level Emotion Recognition"), it may be more suitable
    to employ metrics such as Unweighted F1-score (UF1) and Unweighted Average Recall
    (UAR) to assess method performance. UF1, also known as macro-averaged F1-score,
    calculates the average F1-score across all classes, offering equal weighting to
    each class in multi-class scenarios. Conversely, UAR computes the average accuracy
    per class, normalized by the total number of classes. UAR helps mitigate bias
    arising from class imbalance existing in some databases,e.g., SiteGroEmo and is
    often referred to as balanced accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. Brosch, K. Scherer, D. Grandjean, and D. Sander, “The impact of emotion
    on perception, attention, memory, and decision-making,” *Swiss medical weekly*,
    vol. 143, no. 1920, pp. w13 786–w13 786, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Dhall, R. Goecke, S. Ghosh, J. Joshi, J. Hoey, and T. Gedeon, “From
    individual to group-level emotion recognition: Emotiw 5.0,” in *Proceedings of
    the 19th ACM international conference on multimodal interaction*, 2017, pp. 524–528.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Wang, S. Zhou, Y. Liu, K. Wang, F. Fang, and H. Qian, “Congnn: Context-consistent
    cross-graph neural network for group emotion recognition in the wild,” *Information
    Sciences*, vol. 610, pp. 707–724, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. Dindar, S. Järvelä, S. Ahola, X. Huang, and G. Zhao, “Leaders and followers
    identified by emotional mimicry during collaborative learning: A facial expression
    recognition study on emotional valence,” *IEEE Transactions on Affective Computing*,
    vol. 13, no. 3, pp. 1390–1400, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] E. A. Veltmeijer, C. Gerritsen, and K. V. Hindriks, “Automatic emotion
    recognition for groups: a review,” *IEEE Transactions on Affective Computing*,
    vol. 14, no. 1, pp. 89–107, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] E. R. Smith and D. M. Mackie, “Group-level emotions,” *Current Opinion
    in Psychology*, vol. 11, pp. 15–19, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] P. M. Niedenthal and M. Brauer, “Social functionality of human emotion,”
    *Annual review of psychology*, vol. 63, pp. 259–285, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] M. E. Shaw, “Group dynamics,” *Annual Review of Psychology*, vol. 12, no. 1,
    pp. 129–156, 1961.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. D. Szilagyi and M. J. Wallace, *Organizational Behavior and Performance*.   JAI
    Press, 1983.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] D. N. Barron, E. West, and M. T. Hannan, “A time to grow and a time to
    die: Growth and mortality of credit unions in new york city, 1914-1990,” *American
    Journal of Sociology*, vol. 100, no. 2, pp. 381–421, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] N. Dasgupta, M. R. Banaji, and R. P. Abelson, “Group entitativity and
    group perception: Associations between physical features and psychological judgment.”
    *Journal of personality and social psychology*, vol. 77, no. 5, p. 991, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. Schachter and J. Singer, “Cognitive, social, and physiological determinants
    of emotional state.” *Psychological review*, vol. 69, no. 5, p. 379, 1962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P. R. Kleinginna Jr and A. M. Kleinginna, “A categorized list of emotion
    definitions, with suggestions for a consensual definition,” *Motivation and emotion*,
    vol. 5, no. 4, pp. 345–379, 1981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] P. Ekman, “An argument for basic emotions,” *Cognition & emotion*, vol. 6,
    no. 3-4, pp. 169–200, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. R. Averill, “What are emotions, really?” *Cognition & Emotion*, vol. 12,
    no. 6, pp. 849–855, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Cabanac, “What is emotion?” *Behavioural processes*, vol. 60, no. 2,
    pp. 69–83, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] L. F. Barrett, “Are emotions natural kinds?” *Perspectives on psychological
    science*, vol. 1, no. 1, pp. 28–58, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] B. W. Schuller, B. Vlasenko, F. Eyben, M. Wöllmer, A. Stuhlsatz, A. Wendemuth,
    and G. Rigoll, “Cross-corpus acoustic emotion recognition: Variances and strategies,”
    *IEEE Trans. Affect. Comput.*, vol. 1, no. 2, pp. 119–131, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] E. Hatfield, J. T. Cacioppo, and R. L. Rapson, “Emotional contagion,”
    *Current directions in psychological science*, vol. 2, no. 3, pp. 96–100, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. P. Bagozzi and U. M. Dholakia, “Antecedents and purchase consequences
    of customer participation in small group brand communities,” *International Journal
    of research in Marketing*, vol. 23, no. 1, pp. 45–61, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. G. Barsade and D. E. Gibson, “Group emotion: A view from top and bottom.”
    *Res. Manag. Group Teamss*, vol. 41, pp. 81–102, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. R. Kelly and S. G. Barsade, “Mood and emotions in small groups and
    work teams,” *Organizational behavior and human decision processes*, vol. 86,
    no. 1, pp. 99–130, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. G. Barsade and D. E. Gibson, “Why does affect matter in organizations?”
    *Academy of management perspectives*, vol. 21, no. 1, pp. 36–59, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] W. Mou, O. Celiktutan, and H. Gunes, “Group-level arousal and valence
    recognition in static images: Face, body and context,” in *2015 11th IEEE International
    Conference and Workshops on Automatic Face and Gesture Recognition (FG)*, vol. 5.   IEEE,
    2015, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Dhall, J. Joshi, I. Radwan, and R. Goecke, “Finding happiest moments
    in a social context,” in *Computer Vision–ACCV 2012: 11th Asian Conference on
    Computer Vision, Daejeon, Korea, November 5-9, 2012, Revised Selected Papers,
    Part II 11*.   Springer, 2013, pp. 613–626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Dhall, J. Joshi, K. Sikka, R. Goecke, and N. Sebe, “The more the merrier:
    Analysing the affect of a group of people in images,” in *2015 11th IEEE international
    conference and workshops on automatic face and gesture recognition (FG)*, vol. 1.   IEEE,
    2015, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Dhall, A. Kaur, R. Goecke, and T. Gedeon, “Emotiw 2018: Audio-video,
    student engagement and group-level affect prediction,” in *Proceedings of the
    20th ACM International Conference on Multimodal Interaction*, 2018, pp. 653–656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Ghosh, A. Dhall, N. Sebe, and T. Gedeon, “Predicting group cohesiveness
    in images,” in *2019 International Joint Conference on Neural Networks (IJCNN)*.   IEEE,
    2019, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] X. Guo, L. Polania, B. Zhu, C. Boncelet, and K. Barner, “Graph neural
    networks for image understanding based on multiple cues: Group emotion recognition
    and event recognition as use cases,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2020, pp. 2921–2930.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] A. Dhall, “Emotiw 2019: Automatic emotion, engagement and cohesion prediction
    tasks,” in *2019 International Conference on Multimodal Interaction*, 2019, pp.
    546–550.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] M. Pantic and I. Patras, “Dynamics of facial expression: recognition of
    facial actions and their temporal segments from face profile image sequences,”
    *IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)*, vol. 36,
    no. 2, pp. 433–449, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] G. Sharma, S. Ghosh, and A. Dhall, “Automatic group level affect and cohesion
    prediction in videos,” in *2019 8th International Conference on Affective Computing
    and Intelligent Interaction Workshops and Demos (ACIIW)*.   IEEE, 2019, pp. 161–167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] K. G. Quach, N. Le, C. N. Duong, I. Jalata, K. Roy, and K. Luu, “Non-volume
    preserving-based fusion to group-level emotion recognition on crowd videos,” *Pattern
    Recognition*, vol. 128, p. 108646, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Dhall, M. Singh, R. Goecke, T. Gedeon, D. Zeng, Y. Wang, and K. Ikeda,
    “Emotiw 2023: Emotion recognition in the wild challenge,” in *Proceedings of the
    25th International Conference on Multimodal Interaction*, 2023, pp. 746–749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database
    for facial expression, valence, and arousal computing in the wild,” *IEEE Transactions
    on Affective Computing*, vol. 10, no. 1, pp. 18–31, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. See, M. H. Yap, J. Li, X. Hong, and S.-J. Wang, “Megc 2019–the second
    facial micro-expressions grand challenge,” in *2019 14th IEEE International Conference
    on Automatic Face & Gesture Recognition (FG 2019)*.   IEEE, 2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Ghosh, A. Dhall, N. Sebe, and T. Gedeon, “Automatic prediction of group
    cohesiveness in images,” *IEEE Transactions on Affective Computing*, vol. 13,
    no. 3, pp. 1677–1690, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. Dhall, G. Sharma, R. Goecke, and T. Gedeon, “Emotiw 2020: Driver gaze,
    group emotion, student engagement and physiological signal based challenges,”
    in *Proceedings of the 2020 International Conference on Multimodal Interaction*,
    2020, pp. 784–789.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. Rassadin, A. Gruzdev, and A. Savchenko, “Group-level emotion recognition
    using transfer learning from face identification,” in *Proceedings of the 19th
    ACM international conference on multimodal interaction*, 2017, pp. 544–548.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] G. Lu and W. Zhang, “Happiness intensity estimation for a group of people
    in images using convolutional neural networks,” in *2019 3rd international conference
    on electronic information technology and computer engineering (EITCE)*.   IEEE,
    2019, pp. 1707–1710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] C. Pan, D. Yu, L. Sijiang, G. Zhen, and Y. Lei, “Group emotion recognition
    based on multilayer hybrid network,” in *2018 IEEE 3rd International Conference
    on Image, Vision and Computing (ICIVC)*.   IEEE, 2018, pp. 173–177.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Li, S. Roy, J. Feng, and T. Sim, “Happiness level prediction with sequential
    inputs via multiple regressions,” in *Proceedings of the 18th ACM international
    conference on multimodal interaction*, 2016, pp. 487–493.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] B. Balaji and V. R. M. Oruganti, “Multi-level feature fusion for group-level
    emotion recognition,” in *Proceedings of the 19th ACM international conference
    on multimodal interaction*, 2017, pp. 583–586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] L. Surace, M. Patacchiola, E. Battini Sönmez, W. Spataro, and A. Cangelosi,
    “Emotion recognition in the wild using deep neural networks and bayesian classifiers,”
    in *Proceedings of the 19th ACM international conference on multimodal interaction*,
    2017, pp. 593–597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] N. Liu, Y. Fang, and Y. Guo, “Enhancing feature correlation for bi-modal
    group emotion recognition,” in *Advances in Multimedia Information Processing–PCM
    2018: 19th Pacific-Rim Conference on Multimedia, Hefei, China, September 21-22,
    2018, Proceedings, Part II 19*.   Springer, 2018, pp. 24–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Garg, “Group emotion recognition using machine learning,” *arXiv preprint
    arXiv:1905.01118*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] B. Nagarajan and V. R. M. Oruganti, “Group emotion recognition in adverse
    face detection,” in *2019 14th IEEE International Conference on Automatic Face
    & Gesture Recognition (FG 2019)*.   IEEE, 2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] K. Fujii, D. Sugimura, and T. Hamamoto, “Hierarchical group-level emotion
    recognition,” *IEEE Transactions on Multimedia*, vol. 23, pp. 3892–3906, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] D. Yu, L. Xingyu, D. Shuzhan, and Y. Lei, “Group emotion recognition based
    on global and local features,” *IEEE Access*, vol. 7, pp. 111 617–111 624, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Q. Zhu, Q. Mao, J. Zhang, X. Huang, and W. Zheng, “Towards a robust group-level
    emotion recognition via uncertainty-aware learning,” *arXiv preprint arXiv:2310.04306*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] X. Guo, L. F. Polanía, and K. E. Barner, “Group-level emotion recognition
    using deep models on image scene, faces, and skeletons,” in *Proceedings of the
    19th ACM International Conference on Multimodal Interaction*, 2017, pp. 603–608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] E. G. Krumhuber, L. I. Skora, H. C. Hill, and K. Lander, “The role of
    facial movements in emotion recognition,” *Nature Reviews Psychology*, vol. 2,
    no. 5, pp. 283–296, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. Sun, J. Li, H. Feng, W. Gou, H. Shen, J. Tang, Y. Yang, and J. Ye,
    “Multi-modal fusion using spatio-temporal and static features for group emotion
    recognition,” in *Proceedings of the 2020 International Conference on Multimodal
    Interaction*, 2020, pp. 835–840.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Petrova, D. Vaufreydaz, and P. Dessus, “Group-level emotion recognition
    using a unimodal privacy-safe non-individual approach,” in *Proceedings of the
    2020 International Conference on Multimodal Interaction*, 2020, pp. 813–820.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] S. Li, H. Lian, C. Lu, Y. Zhao, C. Tang, Y. Zong, and W. Zheng, “Audio-visual
    group-based emotion recognition using local and global feature aggregation based
    multi-task learning,” in *Proceedings of the 25th International Conference on
    Multimodal Interaction*, 2023, pp. 741–745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] C. Liu, W. Jiang, M. Wang, and T. Tang, “Group level audio-video emotion
    recognition using hybrid networks,” in *Proceedings of the 2020 International
    Conference on Multimodal Interaction*, 2020, pp. 807–812.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Y. Wang, J. Wu, P. Heracleous, S. Wada, R. Kimura, and S. Kurihara, “Implicit
    knowledge injectable cross attention audiovisual model for group emotion recognition,”
    in *Proceedings of the 2020 international conference on multimodal interaction*,
    2020, pp. 827–834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] J. R. Pinto, T. Gonçalves, C. Pinto, L. Sanhudo, J. Fonseca, F. Gonçalves,
    P. Carvalho, and J. S. Cardoso, “Audiovisual classification of group emotion valence
    using activity recognition networks,” in *2020 IEEE 4th International Conference
    on Image Processing, Applications and Systems (IPAS)*.   IEEE, 2020, pp. 114–119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] F. Noroozi, C. A. Corneanu, D. Kamińska, T. Sapiński, S. Escalera, and
    G. Anbarjafari, “Survey on emotional body gesture recognition,” *IEEE transactions
    on affective computing*, vol. 12, no. 2, pp. 505–523, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    *Neural computation*, vol. 1, no. 4, pp. 541–551, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Proceedings of NeurIPS*, 2012, pp.
    1106–1114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
    by back-propagating errors,” *nature*, vol. 323, no. 6088, pp. 533–536, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE transactions on neural networks*, vol. 20,
    no. 1, pp. 61–80, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Z. Liu, J. Dong, C. Zhang, L. Wang, and J. Dang, “Relation modeling with
    graph convolutional networks for facial action unit detection,” in *Proceedings
    of the International Conference MultiMedia Modeling*, 2020, pp. 489–501.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] X. Wang, D. Zhang, and D.-J. Lee, “Implementing the affective mechanism
    for group emotion recognition with a new graph convolutional network architecture,”
    *IEEE Transactions on Affective Computing*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. Abbas and S. K. Chalup, “Group emotion recognition in the wild by combining
    deep neural networks for facial expression classification and scene-context analysis,”
    in *Proceedings of the 19th ACM international conference on multimodal interaction*,
    2017, pp. 561–568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] B. Sun, Q. Wei, L. Li, Q. Xu, J. He, and L. Yu, “Lstm for dynamic emotion
    and group emotion recognition in the wild,” in *Proceedings of the 18th ACM international
    conference on multimodal interaction*, 2016, pp. 451–457.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Q. Wei, Y. Zhao, Q. Xu, L. Li, J. He, L. Yu, and B. Sun, “A new deep-learning
    framework for group emotion recognition,” in *Proceedings of the 19th ACM International
    Conference on Multimodal Interaction*, 2017, pp. 587–592.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] X. Huang, “Group-level human affect recognition with multiple graph kernel
    fusion,” in *INFORMS International Conference on Service Science*.   Springer,
    2022, pp. 127–140.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] L. Evtodienko, “Multimodal end-to-end group emotion recognition using
    cross-modal attention,” *arXiv preprint arXiv:2111.05890*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, “Less
    is more: Clipbert for video-and-language learning via sparse sampling,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2021,
    pp. 7331–7341.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] D. Bombari, P. C. Schmid, M. Schmid Mast, S. Birri, F. W. Mast, and J. S.
    Lobmaier, “Emotion recognition: The role of featural and configural face information,”
    *Quarterly Journal of Experimental Psychology*, vol. 66, no. 12, pp. 2426–2442,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] L. Tan, K. Zhang, K. Wang, X. Zeng, X. Peng, and Y. Qiao, “Group emotion
    recognition with individual facial emotion cnns and global image based cnns,”
    in *Proceedings of the 19th ACM international conference on multimodal interaction*,
    2017, pp. 549–552.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Zhang, X. Wang, D. Zhang, and D.-J. Lee, “Semi-supervised group emotion
    recognition based on contrastive learning,” *Electronics*, vol. 11, no. 23, p.
    3990, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A. Augusma, D. Vaufreydaz, and F. Letué, “Multimodal group emotion recognition
    in-the-wild using privacy-compliant features,” in *Proceedings of the 25th International
    Conference on Multimodal Interaction*, 2023, pp. 750–754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Guo, B. Zhu, L. F. Polanía, C. Boncelet, and K. E. Barner, “Group-level
    emotion recognition using hybrid deep models based on faces, scenes, skeletons
    and visual attentions,” in *Proceedings of the 20th ACM International Conference
    on Multimodal Interaction*, 2018, pp. 635–639.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] A.-S. Khan, Z. Li, J. Cai, Z. Meng, J. O’Reilly, and Y. Tong, “Group-level
    emotion recognition using deep models with a four-stream hybrid network.” in *ICMI*,
    2018, pp. 623–629.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] K. Fujii, D. Sugimura, and T. Hamamoto, “Hierarchical group-level emotion
    recognition in the wild,” in *2019 14th IEEE International Conference on Automatic
    Face & Gesture Recognition (FG 2019)*.   IEEE, 2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. S. Khan, Z. Li, J. Cai, and Y. Tong, “Regional attention networks with
    context-aware fusion for group emotion recognition,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2021, pp. 1150–1159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] K. Slogrove and D. van der Haar, “Group emotion recognition in the wild
    using pose estimation and lstm neural networks,” in *2022 International Conference
    on Artificial Intelligence, Big Data, Computing and Data Communication Systems
    (icABCD)*.   IEEE, 2022, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] S. Ottl, S. Amiriparian, M. Gerczuk, V. Karas, and B. Schuller, “Group-level
    speech emotion recognition utilising deep spectrum features,” in *Proceedings
    of the 2020 International Conference on Multimodal Interaction*, 2020, pp. 821–826.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] D. Li, R. Luo, and S. Sun, “Group-level emotion recognition based on faces,
    scenes, skeletons features,” in *Eleventh International Conference on Graphics
    and Image Processing (ICGIP 2019)*, vol. 11373.   SPIE, 2020, pp. 46–51.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Gupta, D. Agrawal, H. Chauhan, J. Dolz, and M. Pedersoli, “An attention
    model for group-level emotion recognition,” in *Proceedings of the 20th ACM International
    Conference on Multimodal Interaction*, 2018, pp. 611–615.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] K. Wang, X. Zeng, J. Yang, D. Meng, K. Zhang, X. Peng, and Y. Qiao, “Cascade
    attention networks for group emotion recognition with face, body and image cues,”
    in *Proceedings of the 20th ACM international conference on multimodal interaction*,
    2018, pp. 640–645.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu,
    Y. Xu *et al.*, “A survey on vision transformer,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 45, no. 1, pp. 87–110, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. Xie, M.-X. Lee, T.-J. Chen, H.-J. Chen, H.-I. Liu, H.-H. Shuai, and
    W.-H. Cheng, “Most important person-guided dual-branch cross-patch attention for
    group affect recognition,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2023, pp. 20 598–20 608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] X. Huang, A. Dhall, R. Goecke, M. Pietikäinen, and G. Zhao, “Analyzing
    group-level emotion with global alignment kernel based approach,” *IEEE Transactions
    on Affective Computing*, vol. 13, no. 2, pp. 713–728, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] F. Wang and H. Liu, “Understanding the behaviour of contrastive loss,”
    in *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    2021, pp. 2495–2504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] X. Wang, D. Zhang, H.-Z. Tan, and D.-J. Lee, “A self-fusion network based
    on contrastive learning for group emotion recognition,” *IEEE Transactions on
    Computational Social Systems*, vol. 10, no. 2, pp. 458–469, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] S. N. Shamsi, B. P. Singh, and M. Wadhwa, “Group affect prediction using
    multimodal distributions,” in *2018 IEEE Winter Applications of Computer Vision
    Workshops (WACVW)*.   IEEE, 2018, pp. 77–83.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] A. Dhall, R. Goecke, J. Joshi, J. Hoey, and T. Gedeon, “Emotiw 2016: Video
    and group-level emotion recognition challenges,” in *Proceedings of the 18th ACM
    international conference on multimodal interaction*, 2016, pp. 427–432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Cerekovic, “A deep look into group happiness prediction from images,”
    in *Proceedings of the 18th ACM international conference on multimodal interaction*,
    2016, pp. 437–444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] D. Guo, K. Wang, J. Yang, K. Zhang, X. Peng, and Y. Qiao, “Exploring regularizations
    with face, body and image cues for group cohesion prediction,” in *2019 International
    Conference on Multimodal Interaction*, 2019, pp. 557–561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] T. Xuan Dang, S.-H. Kim, H.-J. Yang, G.-S. Lee, and T.-H. Vo, “Group-level
    cohesion prediction using deep learning models with a multi-stream hybrid network,”
    in *2019 International Conference on Multimodal Interaction*, 2019, pp. 572–576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] B. Zhu, X. Guo, K. Barner, and C. Boncelet, “Automatic group cohesiveness
    detection with multi-modal features,” in *2019 International Conference on Multimodal
    Interaction*, 2019, pp. 577–581.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] E. R. Smith and D. M. Mackie, “Dynamics of group-based emotions: Insights
    from intergroup emotions theory,” *Emotion Review*, vol. 7, no. 4, pp. 349–354,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Y. Xia, W. Zheng, Y. Wang, H. Yu, J. Dong, and F.-Y. Wang, “Local and
    global perception generative adversarial network for facial expression synthesis,”
    *IEEE Transactions on Circuits and Systems for Video Technology*, vol. 32, no. 3,
    pp. 1443–1452, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Latif, A. Shahid, and J. Qadir, “Generative emotional ai for speech
    emotion recognition: The case for synthetic emotional speech augmentation,” *Applied
    Acoustics*, vol. 210, p. 109425, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] D. Kim and B. C. Song, “Emotion-aware multi-view contrastive learning
    for facial emotion recognition,” in *European Conference on Computer Vision*.   Springer,
    2022, pp. 178–195.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] J. Liu, S. Chen, L. Wang, Z. Liu, Y. Fu, L. Guo, and J. Dang, “Multimodal
    emotion recognition with capsule graph convolutional based representation fusion,”
    in *ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2021, pp. 6339–6343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
