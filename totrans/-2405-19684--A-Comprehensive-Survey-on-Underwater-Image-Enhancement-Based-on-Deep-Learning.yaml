- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:32:07'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2405.19684] A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.19684](https://ar5iv.labs.arxiv.org/html/2405.19684)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Xiaofeng Cong, Yu Zhao, Jie Gui, Junming Hou, Dacheng Tao (X. Cong and Y. Zhao
    contributed equally to this work.) (Corresponding author: J. Gui.) X. Cong and
    Y. Zhao are with the School of Cyber Science and Engineering, Southeast University,
    Nanjing 210000, China (e-mail: cxf_svip@163.com, zyzzustc@gmail.com). J. Gui is
    with the School of Cyber Science and Engineering, Southeast University and with
    Purple Mountain Laboratories, Nanjing 210000, China (e-mail: guijie@seu.edu.cn).
    J. Hou is with the State Key Laboratory of Millimeter Waves, School of Information
    Science and Engineering, Southeast University, Nanjing 210096, China (e-mails:
    junming_hou@seu.edu.cn). D. Tao is with the College of Computing and Data Science,
    Nanyang Technological University, Singapore 639798 (e-mail: dacheng.tao@gmail.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Underwater image enhancement (UIE) is a challenging research task in the field
    of computer vision. Although hundreds of UIE algorithms have been proposed, a
    comprehensive and systematic review is still lacking. To promote future research,
    we summarize the UIE task from multiple perspectives. First, the physical models,
    data construction processes, evaluation metrics, and loss functions are introduced.
    Second, according to the contributions brought by different literatures, recent
    proposed algorithms are discussed and classified from six perspectives, namely
    network architecture, learning strategy, learning stage, assistance task, domain
    perspective and disentanglement fusion, respectively. Third, considering the inconsistencies
    in experimental settings in different literatures, a comprehensive and fair comparison
    does not yet exist. To this end, we quantitatively and qualitatively evaluate
    state-of-the-art algorithms on multiple benchmark datasets. Finally, issues worthy
    of further research in the UIE task are raised. A collection of useful materials
    is available at https://github.com/YuZhao1999/UIE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Underwater Image Enhancement, Quality Degradation, Color Distortion, Light Attenuation.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Underwater imaging is an important task in the field of computer vision [[6](#bib.bib6),
    [123](#bib.bib123)]. High-quality underwater images are necessary for underwater
    resource exploration, film shooting, personal entertainment, etc. However, due
    to the absorption and scattering effects of light in underwater scenes, the quality
    of underwater images may be degraded to varying degrees [[17](#bib.bib17)]. As
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Comprehensive Survey on
    Underwater Image Enhancement Based on Deep Learning")-(a), as the depth of water
    increases, the red, orange, yellow and green light components disappear in sequence.
    Meanwhile, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(b), diverse distortions
    may occur in underwater environments. Common types of distortion include
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Color distortion and contrast reduction: Due to the different attenuation degrees
    of light with different wavelengths, the color of images tends to be blue-green
    [[33](#bib.bib33)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hazy, noisy and blurry effect: Light may be significantly attenuated in water,
    which usually caused by suspended particles or muddy water. The image obtained
    in such environment may appear hazy, noisy or blurry [[124](#bib.bib124)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low illumination: When the depth of water exceeds a certain value, the environment
    lighting approaches a low-light state, which requires an auxiliary light source.
    [[43](#bib.bib43)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Aiming at improving the quality of degraded underwater images collected in complex
    underwater environments, various underwater images enhancement (UIE) methods have
    been proposed. Existing algorithms can be divided into non-deep learning-based
    UIE (NDL-UIE) and deep learning-based UIE (DL-UIE). Various prior assumptions,
    physical models and non-data-driven classical image processing methods are widely
    utilized by NDL-UIE [[114](#bib.bib114), [24](#bib.bib24), [115](#bib.bib115),
    [159](#bib.bib159), [175](#bib.bib175)]. However, due to the complexity of the
    underwater environment, the strategy adopted by NDL-UIE may be inaccurate in certain
    scenarios. Problems include (i) perfect physical modeling of underwater scenarios
    does not exist, (ii) inherent errors exist in the estimation of physical parameters,
    (iii) specific prior assumptions may not applicable in every scene, and (iv) scene-specific
    representations not held by classical image processing methods [[144](#bib.bib144),
    [7](#bib.bib7), [109](#bib.bib109), [58](#bib.bib58)].
  prefs: []
  type: TYPE_NORMAL
- en: Considering the impressive performance of data-driven algorithms in the field
    of computer vision and image processing, learning-based solutions have received
    more attention from researchers. Aiming at further improving the performance of
    the UIE task, various DL-UIE algorithms have been proposed and verified [[141](#bib.bib141),
    [53](#bib.bib53), [15](#bib.bib15), [30](#bib.bib30), [99](#bib.bib99), [122](#bib.bib122),
    [28](#bib.bib28), [83](#bib.bib83), [74](#bib.bib74), [105](#bib.bib105), [47](#bib.bib47)].
    Focusing on the problems faced by the UIE task, various efforts are expended by
    researchers. To promote future research, we summarize the UIE task from multiple
    perspectives include
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solutions have been proposed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results achieved by existing solutions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues worthy of further exploration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Solutions have been proposed. On the one hand, we summarize the physical models,
    data generation methods, and evaluation metrics which constructed to serve as
    the basis for research. On the other hand, numerous network structures and training
    strategies are classified and discussed. In order to know which methods have been
    widely explored, we need to understand the similarities and differences between
    them. Within our perspective, existing research can be subjectively viewed as
    being carried out from six different aspects, namely network architecture, learning
    strategy, learning stage, assistance task, domain perspective and disentanglement
    $\&amp;$ fusion. To facilitate future research that can easily build on existing
    work. According to the main contributions of different algorithms, we divide the
    existing work into 6 first-level categories. Then, for each first-level category,
    we give the corresponding second-level category. The taxonomy of algorithms is
    shown in Table [I](#S1.T1 "TABLE I ‣ I-B A guide for reading this survey ‣ I Introduction
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: The results achieved by existing solutions. In existing papers, state-of-the-art
    (SOTA) algorithms have been verified on benchmark datasets. Meanwhile, their performance
    have been compared with other algorithms. However, the experimental settings adopted
    in different papers may be inconsistent. A comprehensive and fair comparative
    experiment for the UIE task has not yet been conducted. Here, we provide a fair
    comparison by unifying various experimental settings with the aim of obtaining
    answers on the following two questions.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under fair settings, how do SOTA algorithms perform?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which algorithms achieves the best performance?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Issues worthy of further exploration. Based on our systematic review of existing
    algorithms and comprehensive experimental evaluation, we further discuss issues
    worthy of future research. Specifically, high-quality data synthesis, cooperation
    with text-image multi-modal models, non-uniform illumination, reliable evaluation
    metrics, and combination with other image restoration tasks are topics that still
    need to be studied in depth. In a word, the UIE is in an emerging stage, rather
    than a task that has been almost perfectly solved.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f670b4d3f1f0a1b2a5d8e272c8cbdbe.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Underwater distortion [[157](#bib.bib157), [170](#bib.bib170), [128](#bib.bib128)]
                                                (b) Various distortions
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Underwater degradation.'
  prefs: []
  type: TYPE_NORMAL
- en: I-A The difference between this survey and others
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [[157](#bib.bib157), [144](#bib.bib144), [109](#bib.bib109), [58](#bib.bib58),
    [128](#bib.bib128)] mainly introduce the traditional UIE algorithms, while the
    latest DL-UIE algorithms are rarely mentioned. [[7](#bib.bib7)] classifies DL
    methods from the perspective of network architecture. But the latest progress
    in the research of the UIE task such as Fourier operation [[131](#bib.bib131)],
    contrastive learning [[86](#bib.bib86)], and rank learning [[37](#bib.bib37)]
    are not mentioned. The perspective of [[170](#bib.bib170)]’s research is the difference
    between hardware-based and software-based algorithms, so a comprehensive discussion
    of DL-based algorithms is not the goal of [[170](#bib.bib170)]. In this survey,
    we provide a comprehensive discussion of recent DL-based advancements.
  prefs: []
  type: TYPE_NORMAL
- en: I-B A guide for reading this survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The terminology in the existing literature may not be consistent. To facilitate
    the reading of this paper, especially the figures, we summarize the important
    terminologies and their meanings as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Degradation: Quality degradation during underwater imaging due to absorption
    and scattering phenomena.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distortion $x$: The degraded quality underwater image taken in different types
    of water.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reference $y$: The underwater image with subjectively higher quality. There
    is no such thing as a perfect, distortion-free underwater image.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prediction $\hat{y}$: A visual quality enhanced underwater image obtained by
    an UIE algorithm.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paired images: A distortion image $x$ with the corresponding reference image
    $y$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In-air images: Images taken in a terrestrial scene, which may be indoors or
    outdoors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The terminology used for the task of improving underwater image quality has
    certain differences in existing literatures, which includes underwater image enhancement,
    underwater image restoration and underwater image dehazing. Since certain similarities
    exist between the task we studied and the image dehazing task, such as similar
    imaging principles [[11](#bib.bib11)], some literature uses “underwater image
    dehazing” as a definition. This definition has been used less frequently in recent
    literature [[7](#bib.bib7)]. Furthermore, “enhancement” and “restoration” are
    often used in different literatures. There is no significant difference between
    these two terms in the task of improving underwater image quality. Therefore,
    in this survey, we use “enhancement” uniformly to define the task we discuss.
  prefs: []
  type: TYPE_NORMAL
- en: For ease of reading, abbreviations are used to refer to each UIE algorithm mentioned
    in the paper. Many papers provide abbreviations for their algorithms, such as
    URanker [[37](#bib.bib37)]. For papers that did not provide an algorithm abbreviation,
    we constructed the abbreviation using the first letters of the words in the title
    that illustrate their main contributions.
  prefs: []
  type: TYPE_NORMAL
- en: The commonly used physical models, data generation methods, evaluation metrics
    and loss functions are summarized in Section [II](#S2 "II Related Work ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning"). The taxonomy
    and analysis of existing algorithms are placed in Section [III](#S3 "III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning").
    Section [IV](#S4 "IV Experiments ‣ A Comprehensive Survey on Underwater Image
    Enhancement Based on Deep Learning") provides comprehensive experiments and summarizes
    the conclusions. Challenging and valuable issues that have not yet been solved
    are discussed in Section [V](#S5 "V Future Work ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning"). A summary of this paper is in Section
    [VI](#S6 "VI Conclusion ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A taxonomy of UIE algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Key Idea | Methods |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Network Architecture | Convolution Operation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; UWCNN [[72](#bib.bib72)], UWNet [[99](#bib.bib99)], UResNet [[83](#bib.bib83)],
    DUIR [[25](#bib.bib25)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UIR-Net [[95](#bib.bib95)], FloodNet [[32](#bib.bib32)], LAFFNet [[143](#bib.bib143)],
    UICoE-Net [[106](#bib.bib106)], PUIE-Net [[31](#bib.bib31)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attention Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '&#124; WaveNet [[111](#bib.bib111)], ADMNNet [[141](#bib.bib141)], LightEnhanceNet
    [[156](#bib.bib156)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNMS [[152](#bib.bib152)], HAAM-GAN [[151](#bib.bib151)], MFEF [[169](#bib.bib169)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformer Module |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PTT [[10](#bib.bib10)], U-Trans [[102](#bib.bib102)], UWAGA [[48](#bib.bib48)],
    WaterFormer [[132](#bib.bib132)], Spectroformer [[66](#bib.bib66)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fourier Transformation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; UHD [[131](#bib.bib131)], WFI2-Net [[161](#bib.bib161)], TANet [[150](#bib.bib150)],
    UIE-INN [[18](#bib.bib18)], SFGNet [[162](#bib.bib162)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Wavelet Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '&#124; UIE-WD [[94](#bib.bib94)], EUIE [[56](#bib.bib56)], PRWNet [[49](#bib.bib49)],
    MWEN [[127](#bib.bib127)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Neural Architecture Search |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AutoEnhancer [[116](#bib.bib116)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning Strategy | Adversarial Learning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DGD-cGAN [[34](#bib.bib34)], FGAN [[76](#bib.bib76)], UIE-cGAN [[145](#bib.bib145)],
    TOPAL [[63](#bib.bib63)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FUnIEGAN [[53](#bib.bib53)], EUIGAN [[28](#bib.bib28)], CE-CGAN [[1](#bib.bib1)],
    RUIG [[22](#bib.bib22)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank Learning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; URanker [[37](#bib.bib37)], PDD-Net [[61](#bib.bib61)], CLUIE-Net [[79](#bib.bib79)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Contrastive Learning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TACL [[86](#bib.bib86)], Semi-UIR [[47](#bib.bib47)], CWR [[40](#bib.bib40)],
    DRDCL [[148](#bib.bib148)], TFUIE [[149](#bib.bib149)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RUIESR [[80](#bib.bib80)], CL-UIE [[118](#bib.bib118)], HCLR-Net [[168](#bib.bib168)],
    UIE-CWR [[39](#bib.bib39)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reinforcement Learning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; HPUIE-RL [[113](#bib.bib113)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning Stage | Single Stage | UWCNN [[72](#bib.bib72)], UWNet [[99](#bib.bib99)],
    UResNet [[83](#bib.bib83)] |'
  prefs: []
  type: TYPE_TB
- en: '| Coarse-to-fine |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MBANet [[138](#bib.bib138)], GSL [[81](#bib.bib81)], CUIE [[67](#bib.bib67)],
    DMML [[27](#bib.bib27)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Diffusion Learning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; UIE-DM [[117](#bib.bib117)], SU-DDPM [[92](#bib.bib92)], CPDM [[112](#bib.bib112)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Assistance Task | Semantic Assistance | SGUIE [[105](#bib.bib105)], SATS
    [[122](#bib.bib122)], DAL-UIE [[64](#bib.bib64)], WaterFlow [[160](#bib.bib160)],
    HDGAN [[13](#bib.bib13)] |'
  prefs: []
  type: TYPE_TB
- en: '| Depth Assistance |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Joint-ID [[142](#bib.bib142)], DAUT [[8](#bib.bib8)], DepthCue [[20](#bib.bib20)],
    HybrUR [[140](#bib.bib140)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Domain Perspective | Knowledge Transfer |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TUDA [[130](#bib.bib130)], TSDA [[62](#bib.bib62)], DAAL [[171](#bib.bib171)],
    IUIE [[9](#bib.bib9)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WSDS-GAN [[84](#bib.bib84)], TRUDGCR [[55](#bib.bib55)], UW-CycleGAN
    [[139](#bib.bib139)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Domain Translation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; URD-UIE [[173](#bib.bib173)], TACL [[86](#bib.bib86)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Diversified Output |'
  prefs: []
  type: TYPE_TB
- en: '&#124; UIESS [[16](#bib.bib16)], UMRD [[174](#bib.bib174)], PWAE [[69](#bib.bib69)],
    CECF [[19](#bib.bib19)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Disentanglement $\&amp;$ Fusion | Physical Embedding |'
  prefs: []
  type: TYPE_TB
- en: '&#124; IPMGAN [[87](#bib.bib87)], ACPAB [[82](#bib.bib82)], USUIR [[29](#bib.bib29)],
    AquaGAN [[21](#bib.bib21)], PhysicalNN [[15](#bib.bib15)], GUPDM [[98](#bib.bib98)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Retinex Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CCMSR-Net [[104](#bib.bib104)], ReX-Net [[153](#bib.bib153)], ASSU-Net
    [[65](#bib.bib65)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Color Space Fusion |'
  prefs: []
  type: TYPE_TB
- en: '&#124; UIEC²-Net [[125](#bib.bib125)], UGIF-Net [[166](#bib.bib166)], TCTL-Net
    [[78](#bib.bib78)], MTNet [[97](#bib.bib97)], UDNet [[110](#bib.bib110)], &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; P2CNet [[108](#bib.bib108)], JLCL-Net [[137](#bib.bib137)], TBDNN [[45](#bib.bib45)],
    MSTAF [[57](#bib.bib57)], UColor [[71](#bib.bib71)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Water Type Focus |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SCNet [[30](#bib.bib30)], DAL [[119](#bib.bib119)], IACC [[165](#bib.bib165)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multi-input Fusion |'
  prefs: []
  type: TYPE_TB
- en: '&#124; WaterNet [[74](#bib.bib74)], MFEF [[169](#bib.bib169)], F2UIE [[121](#bib.bib121)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: II Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, the six aspects involved in the UIE task are introduced. The
    definition of UIE research and the scope of this paper are given in [II-A](#S2.SS1
    "II-A Problem Definition and Research Scope ‣ II Related Work ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning"). Similar problems
    faced by the UIE task and other image restoration tasks are discussed in [II-B](#S2.SS2
    "II-B The connections between the UIE task and other image restoration tasks ‣
    II Related Work ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning"). Physical models that are considered reliable are presented
    in Section [II-C](#S2.SS3 "II-C Physical Models ‣ II Related Work ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning"). Section [II-D](#S2.SS4
    "II-D Datasets for Model Training and Performance Evaluation ‣ II Related Work
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")
    summarizes common data generation and collection methods. The widely used evaluation
    metrics and loss functions are given in Section [II-E](#S2.SS5 "II-E Evaluation
    Metrics ‣ II Related Work ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning") and Section [II-F](#S2.SS6 "II-F Loss Functions ‣ II
    Related Work ‣ A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning"), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Problem Definition and Research Scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the way shown in [[73](#bib.bib73)], we give a commonly used definition
    of the DL-UIE. For a given distorted underwater image $x$, the enhancement process
    can be regarded as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}=\Phi(x;\theta),$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\Phi$ denotes the neural network with the learnable parameters $\theta$.
    Both $x$ and $\hat{y}$ belong to $\mathcal{R}^{H\times W\times 3}$. For UIE models,
    a common optimization purpose is to minimize the error
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\theta}=\arg\min\mathcal{L}(\hat{y},y),$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}(\hat{y},y)$ denote the loss function for obtaining the optimal
    parameters $\hat{\theta}$. $\mathcal{L}(\hat{y},y)$ may be supervised or unsupervised
    loss functions with any form.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the above definition, it is worth clearly pointing out that our survey
    is aimed at the investigation of DL-UIE algorithms with single-frame underwater
    images as input. Therefore, algorithms based on multi-frame images [[135](#bib.bib135)]
    or that do not utilize deep learning at all [[75](#bib.bib75), [103](#bib.bib103),
    [11](#bib.bib11)] are outside the scope of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: II-B The connections between the UIE task and other image restoration tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The UIE task is regarded as a sub-research field of low-level image restoration
    tasks. Similar problems arise in the UIE task and other image restoration tasks.
    The common faced issues include, (i) hazy effect caused by absorption and scattering,
    like image dehazing [[41](#bib.bib41)], (ii) blurry details caused by camera shake,
    light scattering, or fast target motion, like deblurring [[155](#bib.bib155)],
    (iii) noise caused by suspended particles [[58](#bib.bib58), [59](#bib.bib59)],
    like image denoising [[12](#bib.bib12)], (iv) low-illumination due to poor light,
    like low-light image enhancement [[136](#bib.bib136)], (v) non-uniform illumination
    caused by artificial light sources, like nighttime dehazing [[89](#bib.bib89)].
    In general, the problems faced by underwater imaging may be regarded as a complex
    combination of multiple image restoration tasks.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Physical Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To the best of our knowledge, there is currently no model that perfectly describes
    the underwater imaging process [[7](#bib.bib7)], which is totally suitable for
    algorithmic solution. Here, two widely used and proven effective imaging models
    are introduced, namely the atmospheric scattering model [[23](#bib.bib23)] and
    revised underwater image formation model [[2](#bib.bib2)]. The atmospheric scattering
    model [[7](#bib.bib7)] is widely adopted in the research of image dehazing and
    underwater image enhancement [[147](#bib.bib147)], which can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x=y\cdot e^{-\beta d}+B^{\infty}\cdot(1-e^{-\beta d}),$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $d$, $B^{\infty}$ and $\beta$ denote the scene depth, background light
    and attenuation coefficient, respectively. The revised underwater image formation
    model [[2](#bib.bib2)], which is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x=ye^{-\beta^{D}(v_{D})d}+B^{\infty}\left(1-e^{\beta^{B}(v_{B})d}\right),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\beta^{B}$ and $\beta^{D}$ represent the backscatter and direct transmission
    attenuation coefficients [[7](#bib.bib7)]. The $v_{B}$ and $v_{D}$ are the dependent
    coefficients for $\beta^{B}$ and $\beta^{D}$. The atmospheric scattering model
    and revised underwater image formation model can be used for synthesize of data
    and design of algorithms. Since an in-depth discussion of the imaging mechanism
    may be beyond the scope of this paper, more details on the physical model can
    be found in [[2](#bib.bib2), [4](#bib.bib4), [3](#bib.bib3)].
  prefs: []
  type: TYPE_NORMAL
- en: II-D Datasets for Model Training and Performance Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pairs of real-world distorted underwater images and reference images are difficult
    to obtain [[38](#bib.bib38), [76](#bib.bib76)]. The data used for the training
    and evaluation processes need to be collected by various reliable ways. We group
    existing ways of building datasets into the following categories.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voted by SOTA UIE algorithms. 12 algorithms are used by UIEB [[74](#bib.bib74)]
    to generate diverse reference images. The best enhancement effect for each scene
    is then voted on by 50 volunteers as the final reference image. Meanwhile, 18
    UIE algorithms and 20 volunteers are involved in the voting process of LSUI’s
    [[102](#bib.bib102)] reference images. SAUD [[60](#bib.bib60)] proposes a subjectively
    annotated benchmark that contains both real-world raw underwater images and available
    ranking scores under 10 different UIE algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generated by domain transformation algorithms. EUVP [[53](#bib.bib53)] and UFO-120
    [[51](#bib.bib51)] treat the unpaired distorted image and the reference image
    as two domains. Pairs of images in EUVP and UFO-120 are generated via an unsupervised
    domain transformation algorithm [[172](#bib.bib172)] with cycle consistency constraint.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rendered by light field. UWNR [[147](#bib.bib147)] designs a light field retention
    mechanism to transfer the style from natural existing underwater images to objective
    generated images. UWNR adopts a data-driven strategy, which means that it may
    avoid the accuracy limitations caused by the complexity of the physical model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collected by the undersea image capturing system. By setting up a multi-view
    imaging system under seawater, RUIE [[85](#bib.bib85)] constructs an underwater
    benchmark under natural light. The scenes in RUIE show a natural marine ecosystem
    containing various sea life, such as fish, sea urchins, sea cucumbers, scallops,
    etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesized by physical model. Physical models are used by SUID [[44](#bib.bib44)],
    RUIG [[22](#bib.bib22)] and WaterGAN [[77](#bib.bib77)] to estimate the parameters
    that describe the imaging process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition to the methods introduced above, neural radiance field is used by
    a recent work to synthesize both degraded and clear multi-view images [[167](#bib.bib167)].
    Currently, there is no reliable evidence on which way of constructing a dataset
    is optimal. Despite the impressive advances that have been achieved, existing
    methods of constructing datasets still have limitations in various aspects. The
    quality of the label images produced by the voting method is inherently limited,
    that is, the best performance of the model being evaluated may not exceed the
    various algorithms used in the voting process. The diversity of attenuation patterns
    of images obtained by attenuation synthesis processes inspired by generative models
    may be limited. Since the generation process with cycle consistency is limited
    by the one-to-one training mode, which results in cross-domain information not
    being fully taken into account [[5](#bib.bib5)]. The complexity of underwater
    imaging prevents physical models from accurately controlling diverse degradation
    parameters [[147](#bib.bib147)]. Effective data acquisition strategies remain
    a pressing challenge.
  prefs: []
  type: TYPE_NORMAL
- en: II-E Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The way to evaluate the performance of UIE models is a challenging topic under
    exploration. Currently, full-reference and no-reference evaluation metrics are
    the most widely used metrics. In addition, human subjective evaluation, downstream
    task evaluation and model efficiency are also adopted by various literatures.
    Details are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full-reference metrics. The Peak Signal-to-Noise Ratio (PSNR) [[36](#bib.bib36)]
    and Structural Similarity (SSIM) [[129](#bib.bib129)] are full-reference evaluation
    metrics that are widely used for datasets with paired images. They can accurately
    describe the distance between prediction and reference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No-reference metrics. For the evaluation of real-world enhancement performance
    without reference images, no-reference metrics are needed. The Underwater Image
    Quality Measures (UIQM) [[101](#bib.bib101)] and Underwater Color Image Quality
    Evaluation (UCIQE) [[146](#bib.bib146)] are widely adopted as no-reference metrics
    in literatures. Efforts [[164](#bib.bib164)], [[60](#bib.bib60)] are still being
    made to develop more reliable metrics for no-reference assessment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human subjective evaluation. Human vision-friendly enhanced underwater images
    are one of the purposes of UIE models. On the one hand, UIE-related papers [[141](#bib.bib141),
    [53](#bib.bib53), [15](#bib.bib15), [30](#bib.bib30)] usually illustrate the superiority
    of their proposed methods in color correction, detail restoration, and edge sharpening
    from the perspective of quantitative analysis. Such an evaluation is usually performed
    by the authors of the paper themselves. On the other hand, multiple people with
    or without image processing experience are involved in the evaluation of some
    works [[74](#bib.bib74), [37](#bib.bib37)]. They can give subjective ratings to
    different algorithms. The best enhancement result may be selected [[74](#bib.bib74)],
    or the enhancement results obtained by different algorithms may be ranked [[37](#bib.bib37)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation by downstream tasks. The UIE task can be used as an upstream task
    for other image understanding tasks. Due to the inherent difficulties in UIE models
    evaluation, that is, the inability to obtain perfect ground-truth, the use of
    downstream tasks to evaluate UIE models has been widely used in the existing literature.
    Representative downstream tasks include
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: object detection [[163](#bib.bib163)] used in [[134](#bib.bib134), [166](#bib.bib166)],
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: feature matching [[91](#bib.bib91)] used in [[142](#bib.bib142), [169](#bib.bib169)],
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: saliency detection [[52](#bib.bib52)] used in [[142](#bib.bib142), [168](#bib.bib168)],
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: semantic segmentation [[50](#bib.bib50)] used in [[111](#bib.bib111), [149](#bib.bib149)].
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model efficiency. An UIE model may be used in underwater devices that do not
    have significant computing power or storage space. Therefore, the computing and
    storage resources required by the model are important factors for evaluating an
    UIE model. The commonly adopted metrics include Flops (G), Params (M) and Inference
    Time (seconds per image) [[156](#bib.bib156), [104](#bib.bib104), [148](#bib.bib148)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II-F Loss Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although different UIE models may adopt different training strategies. Effective
    loss functions are commonly used by UIE models, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 loss [[34](#bib.bib34), [169](#bib.bib169)], Smooth L1 loss [[153](#bib.bib153),
    [166](#bib.bib166)] and L2 loss [[20](#bib.bib20), [111](#bib.bib111)] provide
    the pixel-level constraint, which is used as fidelity loss.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perceptual loss [[51](#bib.bib51), [153](#bib.bib153), [169](#bib.bib169), [168](#bib.bib168)],
    also known as content loss, is used to measure the distance between two images
    in feature space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSIM is the evaluation metric for UIE tasks, which is differentiable. Therefore,
    SSIM loss [[165](#bib.bib165), [20](#bib.bib20), [111](#bib.bib111)] is widely
    used for structural constraints.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The attenuation of underwater images may be accompanied by the loss of edge
    information. So, the edge loss [[51](#bib.bib51), [38](#bib.bib38), [97](#bib.bib97)]
    calculated by the gradient operator is exploited.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial losses [[22](#bib.bib22), [25](#bib.bib25), [84](#bib.bib84)] can
    provide domain discriminative capabilities, which may improve the visual quality
    of the restored image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: III UIE Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to main contributions of each paper, we divide UIE algorithms into
    six categories, namely (i) Section [III-A](#S3.SS1 "III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning") Network Architecture, (ii) Section [III-B](#S3.SS2 "III-B Learning
    Strategy ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning") Learning Strategy, (iii) Section [III-C](#S3.SS3 "III-C
    Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image
    Enhancement Based on Deep Learning") Learning Stage, (iv) Section [III-D](#S3.SS4
    "III-D Assistance Task ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning") Assistance Task, (v) Section [III-E](#S3.SS5
    "III-E Domain Perspective ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning") Domain Perspective and (vi) Section
    [III-F](#S3.SS6 "III-F Disentanglement & Fusion ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning") Disentanglement
    $\&amp;$ Fusion, respectively. Details are as
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network Architecture. Network architecture is crucial to the performance of
    UIE models. The construction of UIE models mainly uses convolution, attention
    [[100](#bib.bib100)], Transformer [[90](#bib.bib90)], Fourier [[26](#bib.bib26)]
    and Wavelet [[46](#bib.bib46)] operations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Strategy. Beyond conventional end-to-end supervised training, due to
    the complexity of UIE scenarios, diverse learning processes are adopted by the
    UIE algorithm. The typical learning process includes adversarial learning [[54](#bib.bib54)],
    rank learning [[158](#bib.bib158)], contrastive learning [[68](#bib.bib68)] and
    reinforcement learning [[133](#bib.bib133)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Stage. The mapping from distortion to prediction may be implemented
    by the single stage, coarse-to-fine, or stepwise diffusion process [[42](#bib.bib42)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assistance Task. Semantic segmentation, object detection, or depth estimation
    tasks are common auxiliary tasks for the UIE task. The training of joint models
    for different tasks is beneficial to each other.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain Perspective. In-air clear images, underwater distorted images, and underwater
    clear images can all be regarded as independent domains. From a domain perspective,
    knowledge transfer, degradation conversion, and enhancement effect adjustment
    can all be achieved through domain-aware training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disentanglement $\&amp;$ Fusion. Considering the degradation properties of underwater
    images, disentanglement and fusion are used as two effective ways to improve the
    interpretability of the model. Physical models and Retinex models are usually
    used as the theoretical basis for disentanglement. Color space fusion, water type
    fusion and multi-input fusion are common solutions for fusion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we introduce the algorithms in each category, schematic diagrams are used
    to illustrate the differences between the different algorithms. It is worth pointing
    out that what is expressed in the schematic diagram is not the details of a specific
    algorithm, but the main idea of a type of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: III-A Network Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afdc58bd105f62bf51d58b6ef4422b50.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Window-based Transformer                     (b) Fourier Transformation
                                  (c) Wavelet Decomposition [[127](#bib.bib127)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Building feature maps with high-quality representation by window-based
    Transformer, Fourier Transformation and Wavelet Decomposition, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7444f1c68a493996018a8f6f54d870fc.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Single Stage                     (b) Coarse-to-fine [[67](#bib.bib67)]                                       
    (c) Diffusion Process [[117](#bib.bib117)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Generating high-quality enhanced images with varying numbers of stages.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Convolution Operation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolution is one of the most commonly used operations in UIE models. Network
    block constructed methods by convolution operation include naive convolution (UWCNN
    [[72](#bib.bib72)], UWNet [[99](#bib.bib99)]), residual connection (UResNet [[83](#bib.bib83)]),
    residual-dense connection (DUIR [[25](#bib.bib25)], FloodNet [[32](#bib.bib32)]),
    and multi-scale fusion (LAFFNet [[143](#bib.bib143)]). The computational consumption
    of UIE models based on convolutional architecture is usually less.
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Attention Mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the UIE task, the attention mechanism is mainly used to help the model focus
    on different spatial locations or weight different channel information. By analyzing
    the wavelength-driven contextual size relationship of the underwater scene, WaveNet
    [[111](#bib.bib111)] assigns operations with different receptive fields to different
    color channels. Then, attention operations along spatial and channel dimensions
    are used to enhance convolutional paths with different receptive fields. Based
    on the selective kernel mechanism, a nonlinear strategy is proposed by ADMNNet
    [[141](#bib.bib141)] to change receptive field sizes reasonably by adopting soft
    attention. Meanwhile, a module with the ability to dynamically aggregate channel
    information is designed by ADMNNet. Aiming at calibrating the detailed information
    of the distorted input, MFEF [[169](#bib.bib169)] constructs a pixel-weighted
    channel attention calculation flow.
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Transformer Module
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Transformer architecture [[120](#bib.bib120), [90](#bib.bib90)] has been
    widely used in natural language and computer vision research. UWAGA [[48](#bib.bib48)]
    points out that an automatic selection mechanism for grouping the input channels
    is beneficial for mining relationships between channels. Based on this view, an
    adaptive group attention operation embedded in Swin-Transformer [[90](#bib.bib90)]
    is designed by UWAGA. As shown in Fig. [2](#S3.F2 "Figure 2 ‣ III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning")-(a), under the principle of Swin-Transformer, the image (or
    feature map) can be split into window-based patches. Image Transformers pretrained
    on ImageNet are used by PTT [[10](#bib.bib10)] to fine-tune on the UIE task, which
    shows that features with long-distance dependencies are effective for handling
    the underwater distortion. From both the channel and spatial perspectives, a multi-scale
    feature fusion transformer block and a global feature modeling transformer block
    are proposed by U-Trans [[102](#bib.bib102)]. A multi-domain query cascaded Transformer
    architecture is designed by Spectroformer [[66](#bib.bib66)], where localized
    transmission representation and global illumination information are considered.
    Although these Transformer-based architectures have proven beneficial for the
    UIE task, they often incur computational costs that exceed conventional convolution
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: III-A4 Fourier Transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Fourier transform [[18](#bib.bib18)] provides a perspective for analyzing
    features in the frequency domain. In the UIE task, operations in the Fourier domain
    and the spatial domain are often used jointly. The Fourier operation for a 2D
    image signal $x\in\mathcal{R}^{H\times W}$ is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{F}(x)(u,v)=\frac{1}{\sqrt{HW}}\sum_{h=0}^{H-1}\sum_{w=0}^{W-1}x(h,w)e^{-j2{\pi}(\frac{h}{H}u+\frac{w}{W}v)},$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $(h,w)$ and $(u,v)$ denote coordinates in spatial and Fourier domain,
    respectively. The process of decomposing an underwater image into an amplitude
    spectrum and a phase spectrum is visualized in Fig. [2](#S3.F2 "Figure 2 ‣ III-A
    Network Architecture ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(b). A dual-path enhancement module
    that jointly performs feature processing in the spatial and Fourier domains was
    designed by UHD [[131](#bib.bib131)]. In the Fourier domain, channel mixers are
    used by UHD to extract the frequency domain features of the distorted image. Specifically,
    the real part and the imaginary part are the objects processed by UHD feature
    extraction. In the spatial domain, a contracting path and an expansive path are
    adopted to construct the spatial feature descriptor. Finally, the features in
    the frequency domain and spatial domain are fused. TANet [[150](#bib.bib150)]
    designs an atmospheric light removal Fourier module by utilizing the feature information
    of the real and imaginary parts. WFI2-Net [[161](#bib.bib161)] and SFGNet [[162](#bib.bib162)]
    also adopt the feature extraction and fusion method in the space-frequency domain.
    Unlike UHD and TANet, which process the real and imaginary parts, WFI2-Net and
    SFGNet dynamically filter the amplitude spectrum and phase spectrum. Although
    Fourier operations have been widely used by the UIE task, to the best of our knowledge,
    in-depth observations of degradation information in the frequency domain have
    not been given.
  prefs: []
  type: TYPE_NORMAL
- en: III-A5 Wavelet Decomposition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The description of different frequency sub-bands can be obtained by wavelet
    decomposition. Four frequency bands can be obtained from the original 2D signal
    $x$ by Discrete Wavelet Transform (DWT) as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I_{LL},I_{LH},I_{HL},I_{HH}=DWT(x),$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $I_{LL},I_{LH},I_{HL},I_{HH}$ denote the low-low, low-high, high-low and
    high-high sub-bands, respectively. The process of Wavelet decomposition of underwater
    images is visualized in Fig. [2](#S3.F2 "Figure 2 ‣ III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning")-(c). UIE-WD [[94](#bib.bib94)] uses a dual-branch network to
    process images of different frequency sub-bands separately, in which each branch
    handles color distortion and detail blur problems respectively. By analyzing the
    limitations of a single pooling operation, wavelet pooling and unpooling layers
    based on Haar wavelets are proposed by EUIE [[56](#bib.bib56)]. PRWNet [[49](#bib.bib49)]
    uses wavelet boost learning to obtain low-frequency and high-frequency features.
    It proposes that high-frequency sub-bands represent texture and edge information,
    while low-frequency sub-bands contain color and lighting information. By using
    normalization and attention, the information of different sub-bands can be refined.
    A frequency subband-aware multi-level interactive wavelet enhancement module is
    designed by MWEN [[127](#bib.bib127)], aiming at building a feature extraction
    branch with more expressive capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: III-A6 Neural Architecture Search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The overall network architecture and the way modules are connected are important
    components of the UIE algorithm. Effective network design solutions require designers
    to have extensive experience and knowledge. Not only that, a large number of tentative
    experiments need to be performed repeatedly. Therefore, neural architecture search
    is explored by AutoEnhancer [[116](#bib.bib116)] for building UIE network structures
    with impressive performance. An encoder-decoder architecture is chosen by AutoEnhancer
    as the supernet. Three processes are used as the basis of AutoEnhancer, namely
    supernet training, subnet searching and subnet retraining. The first step is to
    obtain the parameter $W^{*}$
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W^{*}=\underset{W}{\arg\min}L_{train}(N(S,W)),$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $S$ and $W$ denotes the searching space and network parameters for the
    supernet $N(S,W)$. The $L_{train}$ means the training objective function. Then
    the optimal subnets $S^{*}$ are searched by the trained supernet as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s^{*}=\underset{s\in S}{\arg\max}Acc_{val}(N(s,W^{*}(s))),$ |  | (8)
    |'
  prefs: []
  type: TYPE_TB
- en: where $Acc_{val}$ denotes the accuracy of the validation dataset. The finally
    optimal network parameters $W^{{}^{\prime}}$ is obtain by re-training the optimal
    subnet $M(\cdot)$ under data $X$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W^{{}^{\prime}}=\underset{W}{\arg\min}L_{train}(M(X;W)).$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: There is currently little discussion on the automation of UIE network design
    and it deserves further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec2aebcafa38d4b5acaa925208ea104d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Adversarial Learning                     (b) Rank Learning                                   
    (c) Contrastive Learning
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Schematic diagrams of the Adversarial Learning, Rank Learning and
    Contrastive Learning in the UIE task.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Learning Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-B1 Adversarial Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The adversarial training used in the generative adversarial network [[35](#bib.bib35)]
    can provide additional supervision signals for the image quality improvement process
    of the UIE task. The general form of adversarial training is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}(G,D)=\mathbb{E}_{y\sim p(y)}\log{D(y)}+\mathbb{E}_{x\sim
    p(x)}[\log{(1-D(G(x)))}],$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $G$ and $D$ denote the generator and discriminator, respectively. For
    the UIE task, the generator $G$ is usually used to generate enhanced images or
    physical parameters. The process of distinguishing enhanced and reference underwater
    images through adversarial training is visualized in Fig. [4](#S3.F4 "Figure 4
    ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")-(a).
    An architecture with a dual generator and a single discriminator is designed by
    DGD-cGAN [[34](#bib.bib34)]. One generator is responsible for learning the mapping
    of distorted images to enhanced images, while another generator has the ability
    to simulate the imaging process through transmission information. In order to
    integrate information at different scales simultaneously, UIE-cGAN [[145](#bib.bib145)]
    and TOPAL [[63](#bib.bib63)] adopt a single generator and dual discriminator architecture.
    One discriminator is responsible for optimizing local detailed features, while
    the other discriminator is designed to distinguish global semantic information.
    CE-CGAN [[1](#bib.bib1)] and RUIG [[22](#bib.bib22)] utilize the discriminative
    process provided by conditional generative adversarial networks as an auxiliary
    loss. By utilizing the adversarial training, enhanced images obtained by generator
    may be more consistent with the distribution of reference underwater high-quality
    images perceptually.
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Rank Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The optimization goal of supervised loss is to make the enhanced image as close
    as possible to the reference image. However, model performance may be limited
    by imperfect reference images. Therefore, ranking learning is explored to provide
    “better” guidance to UIE models. From a ranking perspective, the UIE model does
    not need to know a priori which reference image is the best choice but looks for
    which reference image is the better choice. The process of selecting the best
    enhanced image through the ranking strategy is visualized in Fig. [4](#S3.F4 "Figure
    4 ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")-(b).
    URanker [[37](#bib.bib37)] proposes that quality evaluation metrics designed for
    the UIE task can be used to guide the optimization of UIE models. To this end,
    a ranker loss is designed by URanker to rank the order of underwater images under
    the same scene content in terms of visual qualities. For $(x_{n},x_{m})$ selected
    from the dataset $\{x_{0},x_{1},...,x_{N}\}$ that contains $N$ images, URanker
    can predict the corresponding score $(s_{n},s_{m})$ that conforms to the ranking
    relationship by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\mathcal{L}(s_{n},s_{m})=\begin{cases}\max{(0,(s_{m}-s_{n})+\epsilon)},q_{n}>q_{m},\\
    \max{(0,(s_{n}-s_{m})+\epsilon)},q_{n}<q_{m},\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics ><mrow 
    ><mrow  ><mi
      >ℒ</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><msub 
    ><mi  >s</mi><mi
     >n</mi></msub><mo
     >,</mo><msub 
    ><mi  >s</mi><mi
     >m</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >=</mo><mrow 
    ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
     columnalign="left"  ><mrow
     ><mrow 
    ><mrow 
    ><mrow 
    ><mi 
    >max</mi><mo 
    >⁡</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mn 
    >0</mn><mo 
    >,</mo><mrow 
    ><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >m</mi></msub><mo 
    >−</mo><msub 
    ><mi 
    >s</mi><mi 
    >n</mi></msub></mrow><mo
    stretchy="false"  >)</mo></mrow><mo
     >+</mo><mi
     >ϵ</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >,</mo><msub
     ><mi
     >q</mi><mi
     >n</mi></msub></mrow><mo
     >></mo><msub
     ><mi 
    >q</mi><mi 
    >m</mi></msub></mrow><mo 
    >,</mo></mrow></mtd></mtr><mtr 
    ><mtd  columnalign="left" 
    ><mrow  ><mrow
     ><mrow 
    ><mrow 
    ><mi 
    >max</mi><mo 
    >⁡</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mn 
    >0</mn><mo 
    >,</mo><mrow 
    ><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><msub 
    ><mi 
    >s</mi><mi 
    >n</mi></msub><mo 
    >−</mo><msub 
    ><mi 
    >s</mi><mi 
    >m</mi></msub></mrow><mo
    stretchy="false"  >)</mo></mrow><mo
     >+</mo><mi
     >ϵ</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >,</mo><msub
     ><mi
     >q</mi><mi
     >n</mi></msub></mrow><mo
     ><</mo><msub
     ><mi 
    >q</mi><mi 
    >m</mi></msub></mrow><mo 
    >,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><ci 
    >ℒ</ci><interval closure="open" 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci 
    >𝑛</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci  >𝑚</ci></apply></interval></apply><apply
     ><csymbol cd="latexml" 
    >cases</csymbol><apply 
    ><list  ><apply
     ><cn
    type="integer"  >0</cn><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑚</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑛</ci></apply></apply><ci
     >italic-ϵ</ci></apply></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑛</ci></apply></list><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑚</ci></apply></apply><ci
     ><mtext 
     >otherwise</mtext></ci><apply
     ><list 
    ><apply 
    ><cn type="integer" 
    >0</cn><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑛</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑚</ci></apply></apply><ci
     >italic-ϵ</ci></apply></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑛</ci></apply></list><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑞</ci><ci
     >𝑚</ci></apply></apply><ci
     ><mtext 
     >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathcal{L}(s_{n},s_{m})=\begin{cases}\max{(0,(s_{m}-s_{n})+\epsilon)},q_{n}>q_{m},\\
    \max{(0,(s_{n}-s_{m})+\epsilon)},q_{n}<q_{m},\\ \end{cases}</annotation></semantics></math>
    |  | (11) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\mathcal{L}(s_{n},s_{m})$ is regarded as the margin-ranking loss. The
    $q_{n}$ and $q_{m}$ denote the quality of $x_{n}$ and $x_{m}$, respectively. Moreover,
    PDD-Net [[61](#bib.bib61)] designs a pairwise quality ranking loss that is used
    in the form of a pre-trained model to guide the enhanced images toward the higher
    visual quality. The comparative mechanism is utilized by CLUIE-Net [[79](#bib.bib79)]
    to learn from multiple candidates of the reference. The main advantage of comparative
    learning is to make the image quality generated by the network better than the
    current enhancement candidates.
  prefs: []
  type: TYPE_NORMAL
- en: III-B3 Contrastive Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Real-world pairs of distorted images and reference images are difficult to obtain.
    Therefore, reference images that are not accurate enough may provide imperfect
    supervision signals for the UIE network. Contrastive learning, a method that can
    alleviate the problem of missing labeled data, is used by UIE models to provide
    additional supervision signals. The optimization goals of UIE algorithms inspired
    by contrastive learning are generally consistent, which is to increase the distance
    between the anchor and the negative sample while reducing the distance between
    the anchor and the positive sample. The process of optimizing the distance between
    different types of samples through contrastive learning is visualized in Fig.
    [4](#S3.F4 "Figure 4 ‣ III-A6 Neural Architecture Search ‣ III-A Network Architecture
    ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning")-(c). Research on contrast patterns in the UIE task focuses
    on two perspectives. The first is how to construct positive samples, negative
    samples and anchors. The second is how to optimize the distance between samples
    or features. TACL [[86](#bib.bib86)] treats the observed distorted underwater
    image as the negative sample and the clear image as the positive sample. To construct
    a reliable representation space, TACL adopts a pre-trained feature extractor to
    obtain the perceptual feature for the computing of the contrastive loss. A hybrid
    contrastive learning regularization $\mathcal{L}_{hclr}$ is proposed by HCLR-Net
    [[168](#bib.bib168)] as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{hclr}=\sum_{i=1}^{5}w_{i}\frac{D(G_{i}(y),G_{i}(\phi(x)))}{D(G_{i}(x_{r},G_{i}(\phi(x))))},$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $D(\cdot,\cdot)$ denotes the L1 loss. $G_{i}$ and $w_{i}$ represent the
    $i$-th layer of the pretrained feature extraction model and weight factor, respectively.
    $\phi(\cdot)$ is combined by feature and detail network branches. $x_{r}$ means
    a randomly selected distorted underwater image. The reliable bank and augment
    methods are used by Semi-UIR [[47](#bib.bib47)] to generate positive and negative
    samples. No-reference evaluation metrics are used in the triplet construction
    process of Semi-UIR as a guide for sample selection. A closed-loop approach is
    adopted by TFUIE [[149](#bib.bib149)], which uses two paths to simultaneously
    enhance and synthesize distorted images. Correspondingly, a triplet contrastive
    loss is applied to both the enhancement and synthesis processes.
  prefs: []
  type: TYPE_NORMAL
- en: III-B4 Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep learning has been widely proven to be effective for the UIE task. Beyond
    conventional research on neural networks that use fidelity losses for optimization,
    HPUIE-RL [[113](#bib.bib113)] explores how to take advantage of both deep learning
    and reinforcement learning. A two-stage pipeline with pre-training and fine-tuning
    is adopted by HPUIE-RL. The update method of model parameters during pre-training
    is consistent with that of conventional deep neural networks. For the fine-tuning
    phase of the model, a reinforcement learning strategy based on the reward function
    is designed. The reward function consists of three no-reference metrics that evaluate
    underwater image quality from different perspectives, which is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{R}=\beta_{1}\times&#124;\mathcal{R}_{a}-r^{max}_{a}&#124;+\beta_{2}\times\mathcal{R}_{b}+\beta_{3}\times&#124;\mathcal{R}_{c}-r^{max}_{c}&#124;,$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where $a$, $b$ and $c$ represent the metric UCIQE [[146](#bib.bib146)], NIQE
    [[96](#bib.bib96)] and URanker [[37](#bib.bib37)], respectively. The $\beta_{1}$,
    $\beta_{2}$ and $\beta_{3}$ denote weight factors of the reward $\mathcal{R}_{a}$,
    $\mathcal{R}_{b}$ and $\mathcal{R}_{c}$, respectively. The $r^{max}_{a}$ and $r^{max}_{c}$
    are the upper bounds to constrain the value range. The performance of HPUIE-RL
    can be continuously improved through iterative refinement optimization.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Learning Stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-C1 Single Stage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the help of the data fitting capabilities of neural networks, the mapping
    from the distortion to the prediction can be learned in a single-stage manner
    as shown in Fig. [3](#S3.F3 "Figure 3 ‣ III-A Network Architecture ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning").
    Representative single-stage enhancement methods include UWCNN [[72](#bib.bib72)],
    UWNet [[99](#bib.bib99)] and UResNet [[83](#bib.bib83)], etc.
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 Coarse-to-fine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Considering the difficulty of obtaining optimal enhancement results at one time,
    the coarse-to-fine model with different purposes at different stages is utilized.
    By analyzing the color shifts and veil phenomena, a multi-branch multi-variable
    network for obtaining coarse results and attenuation factors is designed by MBANet
    [[138](#bib.bib138)]. Then, the attenuation factors and coarse results are put
    into a physically inspired model to obtain refined enhancement results. A way
    to fuse two intermediate results through soft attention is proposed by GSL [[81](#bib.bib81)].
    One intermediate result contains structure and color information estimated by
    the global flow, while the other intermediate result handles overexposure and
    artifacts by the local flow. CUIE [[67](#bib.bib67)] builds a two-stage framework,
    in which the first stage obtains preliminary enhancement results through the global-local
    path, while the second stage uses histogram equalization and neural networks to
    improve the contrast and brightness of the image. The process of enhancing image
    quality by refinement in a two-stage manner proposed by CUIE is visualized in
    Fig. [3](#S3.F3 "Figure 3 ‣ III-A Network Architecture ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(b). A three-stage
    pipeline is designed by DMML [[27](#bib.bib27)], namely supervised training, adversarial
    training and fusion training. The goal of supervised training is to achieve high
    full-reference evaluation metric values, which usually represent better performance
    on synthetic data. Adversarial training is responsible for improving the no-reference
    evaluation metric values, which may make the model more friendly to real-world
    distorted images. The final fusion training takes advantage of both.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3bfa8cd336c12bab65ffea0fabd4cf94.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Semantic Assistance                                    (b) Guided by Depth
                                  (c) Taking Depth as Input
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Schematic diagrams of improving the UIE model performance by different
    auxiliary tasks. The gray and green arrows represent forward and backward propagation,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd7a661add8f2f8f37d4df2e5a100fe2.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Knowledge Transfer                               (b) Domain Translation
                                  (c) Diversified Output
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Schematic diagrams of UIE models designed from a domain perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C3 Diffusion Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The diffusion model is a powerful generative model that has been widely studied
    recently. The forward process of the diffusion model is a Markov process that
    continuously adds noise to the image [[42](#bib.bib42)]. High-quality images can
    be generated by the reverse process. The process of enhancing underwater image
    quality through the diffusion strategy is visualized in Fig. [3](#S3.F3 "Figure
    3 ‣ III-A Network Architecture ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(c). The generative process cannot
    produce a specific enhanced underwater image in the desired form. Therefore, conditional
    information $c$ is added by UIE-DM [[117](#bib.bib117)] to the diffusion process
    for the UIE task. The training loss of UIE-DM is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{s}=&#124;&#124;\epsilon_{t}-\epsilon_{\theta}(x_{t},c,t)&#124;&#124;_{1},$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $x_{t}$ and $t$ denote the noisy image and time step, respectively. The
    $\epsilon_{t}$ and $\epsilon_{\theta}$ represent the noise image and predicted
    noise image, respectively. Therefore, the mean value $\mu_{\theta}$ can be defined
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu_{\theta}(x_{t},c,t)=\frac{1}{\sqrt{1-\beta_{t}}}\left(x_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(x_{t},c,t)\right).$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: According to the above definition, the distorted underwater image can be used
    as condition information, which makes the image generated by the diffusion process
    deterministic. The relatively high computational cost of diffusion processes is
    an unsolved problem. SU-DDPM [[92](#bib.bib92)] proposes that the computational
    cost can be reduced by using different initial distributions. Although the diffusion
    model has been proven effective for the UIE task, its powerful data generation
    capabilities have not been fully explored.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Assistance Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-D1 Semantic Assistance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: High-level semantic information has not been widely explored in conventional
    UIE algorithms. In order to generate diverse features to UIE models, classification,
    segmentation and detection tasks has been embedded into UIE models by recent research.
    A schematic diagram of the UIE network training assisted by semantic information
    is visualized in Fig. [5](#S3.F5 "Figure 5 ‣ III-C2 Coarse-to-fine ‣ III-C Learning
    Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning")-(a). Regional information related to semantic segmentation
    maps is embedded into the UIE model by SGUIE [[105](#bib.bib105)] to provide richer
    semantic information. A pre-trained classification model trained on a large-scale
    dataset is used as the feature extraction network by SATS [[122](#bib.bib122)].
    DAL-UIE [[64](#bib.bib64)] embeds a classifier that can constrain the latent space
    between the encoder and decoder. The classification loss $\mathcal{L}_{N}$ used
    for distinguish difference water types is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{N}=-(1-p_{t})^{\gamma}\log(p_{t}),$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $p_{t}$ is the probability of classification for category $t$. The $\gamma$
    denote hyperparameter for the weight factor. Meanwhile, by minimizing the maximum
    mean divergence between the encoder and the classifier, robust features can be
    learned by DAL-UIE. A detection perception module is designed by WaterFlow [[160](#bib.bib160)]
    to extract the local position information of the object. Through two-stage training
    of the UIE model and detection model from independent to joint, the network can
    obtain features that represent position-related semantic information. Exploring
    how to combine low-level and high-level features can be beneficial for both tasks.
    The goal of the UIE task is not only to obtain images with good human subjective
    perception, but also to promote the performance of downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: III-D2 Depth Assistance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The degree of attenuation of underwater images is related to the depth of the
    scene. Therefore, depth maps are used by UIE algorithms to assist in the training
    of the network. As shown in Fig. [5](#S3.F5 "Figure 5 ‣ III-C2 Coarse-to-fine
    ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(b) and Fig. [5](#S3.F5 "Figure 5 ‣
    III-C2 Coarse-to-fine ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive
    Survey on Underwater Image Enhancement Based on Deep Learning")-(c), there are
    two ways to use depth information to assist the training of UIE models. The first
    one uses depth maps as the prediction targets, while the second one uses depth
    maps as the fusion inputs. Joint-ID [[142](#bib.bib142)] treats the UIE task and
    the depth estimation task as a unified task. It uses a joint training method to
    enable the decoder to output enhanced images and depth images simultaneously.
    For the predicted depth map $\hat{d}$ and ground-truth depth map $d$, the depth
    loss $\mathcal{L}_{depth}$ can be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $g_{j}=\log{\hat{d}_{j}}-\log{d_{j}},$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathcal{L}_{depth}(\hat{d},d)=\lambda_{1}\sqrt{\frac{1}{T}\sum_{j}{g_{j}^{2}-\frac{\lambda_{2}}{T^{2}}\left(\sum_{i}{g_{j}^{2}}\right)^{2}}},$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{1}$ and $\lambda_{2}$ are two weight factors. The $j$ represents
    the index of the total number of pixels $T$. A combination of depth estimation
    loss $\mathcal{L}_{depth}$ and UIE loss is used for the training of Joint-ID.
    A two-stage architecture is designed by DAUT [[8](#bib.bib8)], whose first and
    second stages are depth estimation and enhancement, respectively. In the enhancement
    stage, namely the second stage, the depth map and the distortion map are simultaneously
    used as inputs to the enhancement network to provide richer prior information.
    DepthCue [[20](#bib.bib20)] utilizes the depth map obtained by a pre-trained depth
    estimation network as auxiliary information for the decoder of the enhancement
    network. HybrUR [[140](#bib.bib140)] trains a depth estimation network from scratch
    that provides depth information which fits the degeneration factors. Although
    these studies show that depth maps can bring positive effects to the UIE task,
    it is worth pointing out that the monocular depth estimation task itself has inherent
    experimental errors. Since it is almost impossible to obtain perfect depth information
    from a monocular image.
  prefs: []
  type: TYPE_NORMAL
- en: III-E Domain Perspective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-E1 Knowledge Transfer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Synthetic data cannot accurately reflect the complex underwater attenuation
    of the real world, which results in a model trained on synthetic data that may
    not achieve satisfactory performance in the real-world UIE task. Domain discrepancy
    between synthetic and real-world data are inherent. Therefore, domain adaptation
    strategies are explored to reduce this domain bias. A triple-alignment network
    with translation path and a task-oriented enhancement path is designed by TUDA
    [[130](#bib.bib130)]. Domain gap is handled at image-level ($\mathcal{L}^{img}$),
    output-level ($\mathcal{L}^{out}$), feature-level ($\mathcal{L}^{feat}$) by discriminators
    $D^{img}$, $D^{out}$ and $D^{feat}$, respectively. The inter-domain adaptation
    at image-level and output-level involves the real-world underwater image $x_{r}$
    and the corresponding enhanced version $y_{r}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{L}^{img}&amp;=\mathbb{E}_{x_{st}}[D^{img}(x_{st})]-\mathbb{E}_{x_{r}}[D^{img}(x_{r})]\\
    &amp;=+\lambda_{img}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{img}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\begin{split}\mathcal{L}^{out}&amp;=\mathbb{E}_{y_{st}}[D^{out}(y_{st})]-\mathbb{E}_{y_{r}}[D^{out}(y_{r})]\\
    &amp;+\lambda_{out}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{out}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: where $x_{st}$ is translated from an in-air image $x_{s}$, while the $y_{st}$
    is an enhanced version of $x_{st}$. The $\hat{I}$ is sampled from $\{x_{r},x_{st}\}$.
    The domain adaptation at feature-level is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{L}^{feat}&amp;=\mathbb{E}_{x_{st}}[D^{feat}(\mathcal{G}_{r}(x_{st}))]-\mathbb{E}_{x_{r}}[D^{feat}(\mathcal{G}_{r}(x_{r}))]\\
    &amp;+\lambda_{feat}\mathbb{E}_{\hat{I}}(&#124;&#124;\nabla_{\hat{I}}D^{feat}(\hat{I})&#124;&#124;_{2}-1)^{2},\end{split}$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{G}_{r}$ is the inter-class encoder. $\lambda_{img}$, $\lambda_{out}$
    and $\lambda_{feat}$ are weight factors. The schematic diagram of knowledge transfer
    is shown in Fig. [6](#S3.F6 "Figure 6 ‣ III-C2 Coarse-to-fine ‣ III-C Learning
    Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning")-(a). By utilizing domain transformation algorithms, in-air
    images and underwater images are simultaneously used by TSDA [[62](#bib.bib62)]
    to construct intermediate domains. Then, the domain discrepancy between the intermediate
    domain and in-air domain is reduced by a carefully designed enhancement network.
    Unpaired underwater images and paired in-air images are used by IUIE [[9](#bib.bib9)]
    for semi-supervised training. Through weight sharing strategy and channel prior
    loss, in-air images and prior knowledge are used to enhance the quality of underwater
    distorted images. WSDS-GAN [[84](#bib.bib84)] designs a weak-strong two-stage
    process, in which weak learning and strong learning are implemented in unsupervised
    and supervised ways, respectively. Weak learning uses in-air and underwater image
    domains to learn information such as the content and brightness, while the goal
    of strong learning is to reduce the blurring of details caused by training with
    domain differences.
  prefs: []
  type: TYPE_NORMAL
- en: III-E2 Domain Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Distorted and clear underwater images can be viewed as two different domains.
    Through unsupervised domain transformation, the dependence on pairs of underwater
    images can be alleviated. The unsupervised disentanglement of content information
    and style information is designed by URD-UIE [[173](#bib.bib173)]. URD-UIE proposes
    that in the UIE task, content information usually represents the texture and semantics,
    while style information can represent degradation such as noise or blur. By leveraging
    an adversarial process to learn content and style encoding, the degradation information
    can be manipulated in the latent space. Aiming at achieving cross-domain translation,
    URD-UIE employs a domain bidirectional loop reconstruction process, that is, $x\rightarrow
    x_{x-y}\rightarrow\dot{x}$ and $y\rightarrow y_{y-x}\rightarrow\dot{y}$, where
    $\dot{x}$ and $\dot{y}$ represent reconstructed image after two-time domain translations.
    The reconstruction is adopted on the image ($\mathcal{L}^{img}$), style ($\mathcal{L}^{sty}$)
    and content ($\mathcal{L}^{con}$) spaces as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{cases}\mathcal{L}^{img}=&#124;&#124;x-\dot{x}&#124;&#124;_{1}+&#124;&#124;y-\dot{y}&#124;&#124;_{1},\\
    \mathcal{L}^{sty}=&#124;&#124;s_{x}-\dot{s_{x}}&#124;&#124;_{1}+&#124;&#124;s_{y}-\dot{s_{y}}&#124;&#124;_{1},\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathcal{L}^{con}=&#124;&#124;c_{x}-\dot{c_{x}}&#124;&#124;_{1}+&#124;&#124;c_{y}-\dot{c_{y}}&#124;&#124;_{1},\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics ><mrow 
    ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
     columnalign="left"  ><mrow
     ><mrow 
    ><msup  ><mi
      >ℒ</mi><mrow
     ><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >m</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >g</mi></mrow></msup><mo
     >=</mo><mrow
     ><msub
     ><mrow
     ><mo
    stretchy="false"  >‖</mo><mrow
     ><mi
     >x</mi><mo
     >−</mo><mover
    accent="true"  ><mi
     >x</mi><mo
     >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn
     >1</mn></msub><mo
     >+</mo><msub
     ><mrow
     ><mo
    stretchy="false"  >‖</mo><mrow
     ><mi
     >y</mi><mo
     >−</mo><mover
    accent="true"  ><mi
     >y</mi><mo
     >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn
     >1</mn></msub></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow 
    ><mrow  ><msup
     ><mi 
     >ℒ</mi><mrow
     ><mi
     >s</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >t</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >y</mi></mrow></msup><mo
     >=</mo><mrow
     ><msub
     ><mrow
     ><mo
    stretchy="false"  >‖</mo><mrow
     ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
     >−</mo><mover
    accent="true"  ><msub
     ><mi
     >s</mi><mi
     >x</mi></msub><mo
     >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn
     >1</mn></msub><mo
     >+</mo><msub
     ><mrow
     ><mo
    stretchy="false"  >‖</mo><mrow
     ><msub
     ><mi
     >s</mi><mi
     >y</mi></msub><mo
     >−</mo><mover
    accent="true"  ><msub
     ><mi
     >s</mi><mi
     >y</mi></msub><mo
     >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn
     >1</mn></msub></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow 
    ><mrow  ><msup
     ><mi 
     >ℒ</mi><mrow
     ><mi
     >c</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >n</mi></mrow></msup><mo
     >=</mo><mrow
     ><msub
     ><mrow
     ><mo
    stretchy="false"  >‖</mo><mrow
     ><msub
     ><mi
     >c</mi><mi
     >x</mi></msub><mo
     >−</mo><mover
    accent="true"  ><msub
     ><mi
     >c</mi><mi
     >x</mi></msub><mo
     >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn
     >1</mn></msub><mo
     >+</mo><msub
     ><mrow
     ><mo
    stretchy="false"  >‖</mo><mrow
     ><msub
     ><mi
     >c</mi><mi
     >y</mi></msub><mo
     >−</mo><mover
    accent="true"  ><msub
     ><mi
     >c</mi><mi
     >y</mi></msub><mo
     >˙</mo></mover></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn
     >1</mn></msub></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol
    cd="latexml"  >cases</csymbol><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >ℒ</ci><apply 
    ><ci 
    >𝑖</ci><ci 
    >𝑚</ci><ci 
    >𝑔</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="latexml" 
    >norm</csymbol><apply 
    ><ci 
    >𝑥</ci><apply 
    ><ci 
    >˙</ci><ci 
    >𝑥</ci></apply></apply></apply><cn
    type="integer"  >1</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><ci
     >𝑦</ci><apply
     ><ci
     >˙</ci><ci
     >𝑦</ci></apply></apply></apply><cn
    type="integer"  >1</cn></apply></apply></apply><ci
     ><mtext 
     >otherwise</mtext></ci><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >ℒ</ci><apply 
    ><ci 
    >𝑠</ci><ci 
    >𝑡</ci><ci 
    >𝑦</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="latexml" 
    >norm</csymbol><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑥</ci></apply><apply 
    ><ci 
    >˙</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑠</ci><ci 
    >𝑥</ci></apply></apply></apply></apply><cn
    type="integer"  >1</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑦</ci></apply><apply
     ><ci
     >˙</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑠</ci><ci
     >𝑦</ci></apply></apply></apply></apply><cn
    type="integer"  >1</cn></apply></apply></apply><ci
     ><mtext 
     >otherwise</mtext></ci><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >ℒ</ci><apply 
    ><ci 
    >𝑐</ci><ci 
    >𝑜</ci><ci 
    >𝑛</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="latexml" 
    >norm</csymbol><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑐</ci><ci 
    >𝑥</ci></apply><apply 
    ><ci 
    >˙</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑐</ci><ci 
    >𝑥</ci></apply></apply></apply></apply><cn
    type="integer"  >1</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑐</ci><ci
     >𝑦</ci></apply><apply
     ><ci
     >˙</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑐</ci><ci
     >𝑦</ci></apply></apply></apply></apply><cn
    type="integer"  >1</cn></apply></apply></apply><ci
     ><mtext 
     >otherwise</mtext></ci></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{cases}\mathcal{L}^{img}=&#124;&#124;x-\dot{x}&#124;&#124;_{1}+&#124;&#124;y-\dot{y}&#124;&#124;_{1},\\
    \mathcal{L}^{sty}=&#124;&#124;s_{x}-\dot{s_{x}}&#124;&#124;_{1}+&#124;&#124;s_{y}-\dot{s_{y}}&#124;&#124;_{1},\\
    \mathcal{L}^{con}=&#124;&#124;c_{x}-\dot{c_{x}}&#124;&#124;_{1}+&#124;&#124;c_{y}-\dot{c_{y}}&#124;&#124;_{1},\\
    \end{cases}</annotation></semantics></math> |  | (22) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\{s_{x},s_{y}\}$ and $\{c_{x},c_{y}\}$ denote style and content codes
    for $x$ and $y$, respectively. The $\dot{s_{x}},\dot{s_{y}},\dot{c_{x}}$ and $\dot{c_{y}}$
    mean the corresponding reconstructed versions. TACL [[86](#bib.bib86)] constructs
    a closed-loop path to learn bidirectional mappings simultaneously. The forward
    branch is responsible for learning distortion to clean images, while the optimization
    goal of the reverse network is to learn the attenuation process. The process of
    domain translation is shown in Fig. [6](#S3.F6 "Figure 6 ‣ III-C2 Coarse-to-fine
    ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning")-(b).
  prefs: []
  type: TYPE_NORMAL
- en: III-E3 Diversified Output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For an image enhancement task which no perfect solution exists, it may be beneficial
    to provide the user with selectable and diverse outputs. The pixel-wise wasserstein
    autoencoder architecture is designed by PWAE [[69](#bib.bib69)], which has a two-dimensional
    latent tensor representation. The tensor $z_{h}$ and $z_{s}$ are used to represent
    enhancement space and style space. The space translation can be implemented by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z_{h\rightarrow s}=\sigma(z_{s})\left(\frac{z_{h}-\mu{(z_{h})}}{\sigma(z_{h})}\right)+\mu{(z_{s})},$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: where $\mu(\cdot)$ and $\sigma(\cdot)$ denote the mean and standard deviation,
    respectively. The degree of the fused tensor $z^{s}_{h}$ can be controlled by
    the parameter $\alpha$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z^{s}_{h}=\alpha z_{h\rightarrow s}+(1-\alpha)z_{h}.$ |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: By fusing the latent code provided by the style image, PWAE is able to change
    the illumination and style of the enhanced result. Inspired by the multi-domain
    image-to-image algorithm, UIESS [[16](#bib.bib16)] decomposes the UIE process
    into content and style learning flows. By manipulating the encoding values in
    the latent space, images with different degrees of enhancement can be produced
    by the decoder of UIESS. Beyond the content and style codes studied by UIESS,
    CECF [[19](#bib.bib19)] proposes the concept of color code. CECF assumes that
    when there exists images with less local distortion in the dataset, these images
    may have colors with long wavelengths that are approximately invariant during
    the enhancement process. By learning a color representation with such invariance,
    CECF can obtain color-adjustable underwater creatures from guidance images. The
    diverse output provided by CECF is shown in Fig. [6](#S3.F6 "Figure 6 ‣ III-C2
    Coarse-to-fine ‣ III-C Learning Stage ‣ III UIE Methods ‣ A Comprehensive Survey
    on Underwater Image Enhancement Based on Deep Learning")-(c).
  prefs: []
  type: TYPE_NORMAL
- en: III-F Disentanglement $\&amp;$ Fusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-F1 Physical Embedding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural network-based UIE methods can often achieve impressive performance on
    specific datasets, but the generalization capabilities and interpretability they
    exhibit may be limited. Therefore, physical models are integrated into the data-driven
    training process by various algorithms. IPMGAN [[87](#bib.bib87)] embeds the Akkaynak-Treibitz
    model [[2](#bib.bib2)] introduced in Section [II-C](#S2.SS3 "II-C Physical Models
    ‣ II Related Work ‣ A Comprehensive Survey on Underwater Image Enhancement Based
    on Deep Learning") into a generative adversarial network. The desired reference
    image can be refered as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=\frac{x-B^{\infty}\left(1-e^{-\beta^{B}(v_{B})d}\right)}{e^{-\beta^{D}(v_{D})d}}.$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: This physical representation can be reformulated for obtaining enhanced image
    $\hat{y}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}=\frac{x-\widehat{B}^{\infty}(1-\widehat{S})}{\widehat{T}},$ |  |
    (26) |'
  prefs: []
  type: TYPE_TB
- en: where $\widehat{T}$ and $\widehat{S}$ are the estimated $e^{-\beta^{D}(v_{D})z}$
    and $e^{-\beta^{B}(v_{B})d}$, respectively. The $\widehat{B}^{\infty}$ denotes
    the estimated veiling light. Through weight-sharing physical parameters ($\widehat{T}$,
    $\widehat{S}$ and $\widehat{B}^{\infty}$) estimation, the enhanced image can be
    derived from the inverse imaging model. The distortion process of the underwater
    image is considered as a horizontal and vertical process by ACPAB [[82](#bib.bib82)].
    From a horizontal perspective, the parameters required by the physical model,
    namely attenuation coefficient, transmission map and background light, are estimated
    by the neural network. From a vertical perspective, an attenuation coefficient
    prior is embedded into the enhancement network. USUIR [[29](#bib.bib29)] builds
    a physically decoupled closed-loop system. Three physical parameters are estimated
    by the network and priori assumptions. Then, the original attenuation image is
    synthesized inversely through the physical parameters. Through such a self-feedback
    mechanism, the training of USUIR can be carried out in an unsupervised manner.
    It is worth pointing out that the modeling of the imaging process of the underwater
    environment is still a task under exploration, and no perfect model has yet been
    proposed.
  prefs: []
  type: TYPE_NORMAL
- en: III-F2 Retinex Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'According to the Retinex model [[104](#bib.bib104)], the image can be decomposed
    into the Hadamard product of two components: reflection and illumination. The
    form of the Retinex model is simpler than the physical models commonly used in
    the UIE task since it requires fewer parameters to be estimated. The multi-scale
    Retinex is employed by CCMSR-Net [[104](#bib.bib104)], which is'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R=\sum_{n=1}^{N}w_{n}[\log{x}-\log{x\ast G_{n}}],$ |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: where $n$ and $w_{n}$ represent the scale index and corresponding weight factor,
    respectively. $G_{n}$ stand for the Gaussian kernel under scale $n$. The $R$ denote
    the reflectance. CCMSR-Net decomposes the enhancement task into two procedures.
    A subnetwork for color correction and a multiscale Retinex subnetwork for estimating
    the illumination, which take into account two consecutive procedures, are integrated
    by CCMSR-Net. ReX-Net [[153](#bib.bib153)] uses two encoders, namely the original
    image encoder and the Retinex-based reflectance encoder, to extract complementary
    content and color information. Based on the self-information extracted which utilizes
    the Retinex decomposition consistency, ASSU-Net [[65](#bib.bib65)] designs a pipeline
    to improve the contrast of the weak and backlit areas.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b79d54f8f3a9044806df01dcd7a41f3a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Physical Embedding                                    (b) Color Space Fusion
                                  (c) Multi-inputs Fusion
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Schematic diagrams of disentanglement and fusion.'
  prefs: []
  type: TYPE_NORMAL
- en: III-F3 Color Space Fusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different attributes are held by different color spaces. RGB representation
    is the most commonly used color space for the UIE task. The three color channels
    of RGB space are highly correlated. This property makes it possible for the RGB
    space to be easily affected by the changes in luminance, occlusion, and shadow
    [[125](#bib.bib125)]. Therefore, color spaces with other properties are explored
    by UIE algorithms. A HSV global-adjust module is designed in UIEC²-Net [[125](#bib.bib125)],
    which can be used for adjusting the luminance, color and saturation by utilizing
    a piece-wise linear scaling curve layer. Meanwhile, a HSV loss $\mathcal{L}_{hsv}$
    is adopted by UIEC²-Net as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{hsv}=&#124;&#124;\widehat{S}\widehat{V}\cos{\widehat{H}}-{SV}\cos{H}&#124;&#124;_{1},$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where $\widehat{H}$, $\widehat{S}$ and $\widehat{V}$ denote the predictions
    of $H\in[0,2\pi)$, $S\in[0,1]$ and $C\in[0,1]$, respectively. MTNet [[97](#bib.bib97)]
    proposes a loss function measured from HSV space to better restore contrast and
    saturation information. An RGB-HSV dual-color space-guided color estimation block
    is proposed by UGIF-Net [[166](#bib.bib166)] to generate comprehensive color information.
    The LAB space is explored by TCTL-Net [[78](#bib.bib78)] and P2CNet [[108](#bib.bib108)]
    to improve color recovery performance. JLCL-Net [[137](#bib.bib137)] converts
    the distorted image and reference image into YCbCr space to improve the sensitivity
    of luminance and chrominance. RGB, HSV and LAB are simultaneously embedded into
    the encoding path by UColor [[71](#bib.bib71)] to construct features with a unified
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: III-F4 Water Type Focus
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the complexity of underwater scene imaging, underwater distorted images
    may exhibit different properties. Common imaging environments include shallow
    coastal waters, deep oceanic waters, and muddy waters. The lighting conditions
    of different underwater environments are diverse. Therefore, a challenging topic
    is how to handle different water types using a single UIE model. Aiming at learning
    water type’s desensitized features, SCNet [[30](#bib.bib30)] performs normalization
    operations in both spatial and channel dimensions. An instance whitening is designed
    in the encoding-encoding structure of SCNet. For the $n$-th example $x_{n}\in\mathbb{R}^{C\times
    HW}$ in a mini-batch, the designed instance whitening $\Gamma(\cdot)$ can be expressed
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Gamma(x_{n})=\Sigma^{-1/2}(x_{n}-\mu)\gamma+\beta,$ |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: where $\gamma$ and $\beta$ denote scale and shift which can be dynamically learned.
    The mean vector $\mu$ and covariance matrix $\Sigma$ are computed with each individual
    sample by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{cases}\mu=\frac{1}{HW}x_{n},\\
    \Sigma=\frac{1}{HW}(x_{n}-\mu)(x_{n}-\mu)^{T}+\alpha{I},\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{cases}" display="block"><semantics ><mrow 
    ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
     columnalign="left"  ><mrow
     ><mrow 
    ><mi  >μ</mi><mo
     >=</mo><mrow
     ><mstyle
    displaystyle="false"  ><mfrac
     ><mn
     >1</mn><mrow
     ><mi
     >H</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >W</mi></mrow></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >x</mi><mi
     >n</mi></msub></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow 
    ><mrow  ><mi
    mathvariant="normal"  >Σ</mi><mo
     >=</mo><mrow
     ><mrow
     ><mstyle
    displaystyle="false"  ><mfrac
     ><mn
     >1</mn><mrow
     ><mi
     >H</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >W</mi></mrow></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mrow
     ><msub
     ><mi
     >x</mi><mi
     >n</mi></msub><mo
     >−</mo><mi
     >μ</mi></mrow><mo
    stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><msup
     ><mrow
     ><mo
    stretchy="false"  >(</mo><mrow
     ><msub
     ><mi
     >x</mi><mi
     >n</mi></msub><mo
     >−</mo><mi
     >μ</mi></mrow><mo
    stretchy="false"  >)</mo></mrow><mi
     >T</mi></msup></mrow><mo
     >+</mo><mrow
     ><mi
     >α</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >I</mi></mrow></mrow></mrow><mo
     >,</mo></mrow></mtd></mtr></mtable></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><csymbol
    cd="latexml"  >cases</csymbol><apply
     ><ci 
    >𝜇</ci><apply 
    ><apply 
    ><cn type="integer" 
    >1</cn><apply 
    ><ci 
    >𝐻</ci><ci 
    >𝑊</ci></apply></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑥</ci><ci 
    >𝑛</ci></apply></apply></apply><ci 
    ><mtext  
    >otherwise</mtext></ci><apply 
    ><ci  >Σ</ci><apply
     ><apply
     ><apply
     ><cn
    type="integer"  >1</cn><apply
     ><ci
     >𝐻</ci><ci
     >𝑊</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑥</ci><ci
     >𝑛</ci></apply><ci
     >𝜇</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑥</ci><ci
     >𝑛</ci></apply><ci
     >𝜇</ci></apply><ci
     >𝑇</ci></apply></apply><apply
     ><ci
     >𝛼</ci><ci
     >𝐼</ci></apply></apply></apply><ci
     ><mtext 
     >otherwise</mtext></ci></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{cases}\mu=\frac{1}{HW}x_{n},\\
    \Sigma=\frac{1}{HW}(x_{n}-\mu)(x_{n}-\mu)^{T}+\alpha{I},\\ \end{cases}</annotation></semantics></math>
    |  | (30) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\alpha$ and $I$ denote a small positive number and identity matrix, respectively.
    By using the instance whitening operation, the influence of diverse water types
    can be reduced. An adversarial process involving latent variable analysis is used
    by DAL [[119](#bib.bib119)] to disentangle the unwanted nuisances corresponding
    to water types. A nuisance classifier is designed by DAL, which classifies the
    water type of the distorted image according to its latent vector. IACC [[165](#bib.bib165)]
    designs an underwater convolution module that can learn channel-specific features
    and adapt to diverse underwater environments by leveraging the mini-batch insensitivity
    of instance normalization.
  prefs: []
  type: TYPE_NORMAL
- en: III-F5 Multi-input Fusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Research [[74](#bib.bib74)] shows that preprocessing input can be beneficial
    for the UIE task. WaterNet [[74](#bib.bib74)] applies White Balance (WB), Histogram
    Equalization (HE) and Gamma Correction (GC) algorithms to distorted underwater
    images. The WB may adjust the color distortion. The HE and GC can increase contrast
    and optimize dark regions. A gated fusion approach is used by WaterNet to obtain
    an integrated output. The gated enhanced result is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}=R_{WB}\odot C_{WB}+R_{HE}\odot C_{HE}+R_{GC}\odot C_{GC},$ |  |
    (31) |'
  prefs: []
  type: TYPE_TB
- en: where $R_{WB}$, $R_{HE}$ and $R_{GC}$ mean the refined images obtained by the
    corresponding pre-process methods. The $C_{WB}$, $C_{HE}$ and $C_{GC}$ denote
    the learned confidence maps. The $\odot$ is element-wise production. MFEF [[169](#bib.bib169)]
    and F2UIE [[121](#bib.bib121)] utilize the WB and Contrast-limited Adaptive Histogram
    Equalization (CLAHE) to obtain high-quality input with better contrast.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0915934e88086c9db7864c81d081797d.png)![Refer to caption](img/b470d6e564d4271aa258092caf46918f.png)![Refer
    to caption](img/2828e8ed817a7544be826309ebcdad15.png)![Refer to caption](img/7e7e48c0bc247e674ecc3aaf54c0cb35.png)![Refer
    to caption](img/e50236e40b9ca689f243eb5dadae4e1c.png)![Refer to caption](img/1cf4feca048b8a66201c9eb2801b0987.png)![Refer
    to caption](img/8a8008684e8361a024afb244a6ea9abc.png)![Refer to caption](img/2358aedba316db7c5fad377debe42ace.png)![Refer
    to caption](img/be37f24d4f38e99211adb1862ee9d3a8.png)![Refer to caption](img/b96e3a5bcc8e00412d5585f7df770993.png)![Refer
    to caption](img/0b5bfa5a4896aadf23e48c22a0a7f863.png)![Refer to caption](img/7175da679ba0e41e2bfc639ca31206e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Distortion              UWNet              FUnIEGAN            WaterNet            
    ADMNNet             UColor
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7671d258b13a7f6919e1d35be6bb58a6.png)![Refer to caption](img/a1a1d86c583bcd4e450299f79d0a0f17.png)![Refer
    to caption](img/5806af330dbf636987885bcaf84f3195.png)![Refer to caption](img/1a6cf0389ffd9e94bc9fd919a84ad003.png)![Refer
    to caption](img/afbaa027582e8654db14fd3ebd9706ee.png)![Refer to caption](img/43a352dfb715334da44b3a0c86660a0a.png)![Refer
    to caption](img/db833ddb00063ad2899f7d30086cbbc7.png)![Refer to caption](img/670097c2f037fe17bac3f8be0192ec9e.png)![Refer
    to caption](img/2d13f0040c2bc6f296f4382e1515f21a.png)![Refer to caption](img/f93cc42282253a9eff0d9db7fd8baec8.png)![Refer
    to caption](img/38b913c61e618508c16a5d89a793cc81.png)![Refer to caption](img/afb5c52bb02b8d2445efd23af44ea212.png)'
  prefs: []
  type: TYPE_IMG
- en: UGAN              SGUIE                 UIE-WD               SCNet                
    STSC                  U-Trans
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f7a2512cbb5fc4670a1d0a72d1ec6bb5.png)![Refer to caption](img/af67abe19e6813b733f44718a1b809ff.png)![Refer
    to caption](img/4576b303e1ec02fd5f35bd2c1a98f7cc.png)![Refer to caption](img/fa2c836da44827c030948d9d08764382.png)![Refer
    to caption](img/86f3192d6a53797542c1c1b91c540884.png)![Refer to caption](img/21cb3438a1a186bd88b11c6521199294.png)![Refer
    to caption](img/14259c1102f91f097928a13d79f4f61b.png)![Refer to caption](img/cc247e39584e9c3b05eeb575a6a051df.png)![Refer
    to caption](img/db8bfa3faeb4618059977593156b8d27.png)![Refer to caption](img/8e94a1f2a69522f89452341efd2f7bac.png)![Refer
    to caption](img/a97c8c2a1f5f2d658e205c33ac606395.png)![Refer to caption](img/a4f66d45fe397f9103e01499957151d6.png)'
  prefs: []
  type: TYPE_IMG
- en: CECF              Semi-UIR              UIE-DM              HCLR-Net            
    GUPDM              Reference
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Visual results obtained on UIEB full-reference test set. The curve
    figure denotes the pixel histogram. The bar figure gives the average pixel value
    of each channel.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7610932a375b2bf8838826bd87d2fe5c.png)![Refer to caption](img/1b5b381c7f3b5131fa3b2228250d5fe9.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Challenge-60                                                               
    (b) U45
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc013fd57c087bfb27927fb64e3ba1e2.png)![Refer to caption](img/fed1ba509e715c915281e740c727f5a0.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) UCCS                                                                    
    (d) EUVP-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: URANKER values on no-reference benchmark datasets. The horizontal
    dashed line represents the mean of the quantitative results obtained by all algorithms.
    The correspondence between the letters on the horizontal axis and the algorithm
    is: A (FUnIEGAN), B (WaterNet), C (ADMNNet), D(UColor), E (UGAN), F (SGUIE), G
    (UIE-WD), H (SCNet), I (STSC), J (U-Trans), K (CECF), L (Semi-UIR), M (UIE-DM),
    N (HCLR-Net) and O (GUPDM). Best results are in red font and second best results
    are with blue font.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/044ffddce626c178f2d3cfdf246120ff.png)![Refer to caption](img/1c45fbcc18253eec2fd2dfb40c6e7a23.png)![Refer
    to caption](img/363e00d3bd98615dde6e06e7e6fe82f0.png)![Refer to caption](img/2d4bc6dbccff270c7a63c7ea578d1fab.png)![Refer
    to caption](img/9777089af4ac16a610b3bc3633f94955.png)![Refer to caption](img/42ad8d93c8b829b210deb3c615f47d80.png)![Refer
    to caption](img/9aeb866eeab6e4d017f283fd684861b3.png)![Refer to caption](img/de0bb070b09bfe8928ac6a20c4e2712d.png)![Refer
    to caption](img/50e9bd42a8ff11a2e7ecf8e2830ca765.png)![Refer to caption](img/4c3a6ea50cd565c897f416a563686195.png)![Refer
    to caption](img/633c2ded4b6c623efba91379b9723ff6.png)![Refer to caption](img/914a55d519100847b3f660205b184e4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Distortion              UWNet              PhysicalNN           FUnIEGAN          
    WaterNet             ADMNNet
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5963076e6ddc95e80cb1b7039690bc74.png)![Refer to caption](img/0d5ac138ad567254996a6e48ba028f24.png)![Refer
    to caption](img/efa18d17b0da8a00a8736a4ce1799759.png)![Refer to caption](img/3b1aa67038e23fb323558dbba3ece45e.png)![Refer
    to caption](img/c0fc43be8b25ae15c3ff89dd2a510a2f.png)![Refer to caption](img/2ed2dbc3568347882663e5bee5931516.png)![Refer
    to caption](img/c7c65beef56c1c41a4ca2814a2dd9030.png)![Refer to caption](img/517c0573381c15adfac19254177abc7e.png)![Refer
    to caption](img/9279e499654ff97154ab319b6b2197d3.png)![Refer to caption](img/b9804cc7f4b87cf1578cbdb2b3d5f17b.png)![Refer
    to caption](img/ca047d79938409d38ada405c8dae2664.png)![Refer to caption](img/6d14cf974b4ade3fc50bd23ee4ba7f87.png)'
  prefs: []
  type: TYPE_IMG
- en: UColor               UGAN               SGUIE                 UIE-WD               
    SCNet                 STSC
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e638cdd94ce65ddb819000bbbcca7893.png)![Refer to caption](img/7bbaee969af052fc72d0a2103f3b0f20.png)![Refer
    to caption](img/489112061d2fc2bbf713e190f0893625.png)![Refer to caption](img/509d5866c1c6786ee319da3ad3dcb98c.png)![Refer
    to caption](img/690a14f72dfd7c293f6b0de64e3ab999.png)![Refer to caption](img/245eb71f4b252e180a3bda2f018f7dea.png)![Refer
    to caption](img/5b9659a9e6e4664a782eeefc67016870.png)![Refer to caption](img/416be6abfb66d35ff580735a239a6fee.png)![Refer
    to caption](img/6e354b8c031bdb25730c30830574a4e8.png)![Refer to caption](img/e2e23a5c471bba644eccf2323038b674.png)![Refer
    to caption](img/3a7d3cbe548573375af4e5f425a999f5.png)![Refer to caption](img/efe5ef3f9891a7c30bdb51f53d23fe53.png)'
  prefs: []
  type: TYPE_IMG
- en: U-Trans              CECF               Semi-UIR              UIE-DM             
    HCLR-Net             GUPDM
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Visual results obtained on challenge-60\. The curve figure denotes
    the pixel histogram. The bar figure gives the average pixel value of each channel.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To promote research on the UIE task, we provide a fair evaluation of UIE algorithms
    that have been proven to be effective on benchmark underwater datasets.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Algorithms. The DL-UIE algorithms include UWNet [[99](#bib.bib99)], PhysicalNN
    [[15](#bib.bib15)], FUnIEGAN [[53](#bib.bib53)], WaterNet [[74](#bib.bib74)],
    ADMNNet [[141](#bib.bib141)], UColor [[71](#bib.bib71)], UGAN [[28](#bib.bib28)],
    SGUIE [[105](#bib.bib105)], UIE-WD [[94](#bib.bib94)], SCNet [[30](#bib.bib30)],
    STSC [[122](#bib.bib122)], U-Trans [[102](#bib.bib102)], CECF [[19](#bib.bib19)],
    Semi-UIR [[47](#bib.bib47)], UIE-DM [[117](#bib.bib117)], HCLR-Net [[168](#bib.bib168)]
    and GUPDM [[98](#bib.bib98)]. They are all open source code implemented in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics. The full-reference metrics include PSNR and SSIM. The no-reference
    metrics include UIQM [[101](#bib.bib101)], UCIQE [[146](#bib.bib146)] and URANKER
    [[37](#bib.bib37)]. For all experimental analyzes and discussions below, we assume
    that the quantitative evaluation metrics that widely used in existing literatures
    are reliable. Otherwise, we can not draw any conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets. The benchmark datasets with paired images include UIEB [[74](#bib.bib74)],
    LSUI [[102](#bib.bib102)], {EUVP-D/EUVP-I/EUVP-S} from EUVP [[53](#bib.bib53)]
    and UFO-120 [[51](#bib.bib51)]. The benchmark datasets without references include
    Challenge-60 from UIEB [[74](#bib.bib74)], U45 [[76](#bib.bib76)], UCCS from RUIE
    [[85](#bib.bib85)], and EUVP-330 from EUVP [[53](#bib.bib53)]. Different semi-supervised
    algorithms require different types of external data. In order to ensure fairness
    as much as possible, we use the pretrained models on the UIEB (full-reference
    training set) dataset for the performance test of no-reference datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Factor Settings. In order to do our best to keep the comparative experiments
    fair, we have unified the following factors that may affect the experimental results.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch size is set to 8.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image size in both training and testing phases is set to $256\times 256$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three data augmentation methods are used, namely horizontal random flipping,
    vertical random flipping and random cropping.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature extraction part of any model does not use pre-trained patterns guided
    by external data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The partition ratio of training and testing data may be different in existing
    literature. Here we use a uniform ratio to divide the training and test data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different models may require different initial learning rates, learning rate
    decay strategies, and optimizers. We follow the settings given in their respective
    papers. We did not conduct additional hyperparameter searches for either model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to prevent differences in metric values caused by different toolbox
    or implementation [[14](#bib.bib14)], we unified the code for metric calculations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV-B Comparison among DL-UIE Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evaluation of Fitting Ability on Full-Reference Benchmark Datasets. Table LABEL:tab:full_reference_results
    shows the results obtained by various DL-UIE algorithms on datasets with paired
    data. Overall, the quantitative evaluation metrics obtained by UIE-DM are the
    best. However, on most datasets, there is no significant difference in the quantitative
    evaluation results (i.e., PSNR and SSIM) obtained by the top 5 algorithms. What
    is particularly noteworthy is that for EUVP-D, EUVP-I and EUVP-S, the PSNR and
    SSIM of the top 10 algorithms are approaching the same level. The visualization
    results in Fig. [8](#S3.F8 "Figure 8 ‣ III-F5 Multi-input Fusion ‣ III-F Disentanglement
    & Fusion ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement
    Based on Deep Learning") also illustrate that multiple DL-UIE algorithms achieve
    similar visual effects. Not only that, the color histogram also shows that the
    results obtained by different algorithms are close in terms of pixel statistics.
    For these UIE algorithms, the representation capabilities of the neural networks
    they designed with different architectures may be very close. This phenomenon
    implies that the pursuit of higher PSNR and SSIM may be difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of Generalization Ability on No-Reference Benchmark Datasets. We
    tested two kinds no-reference metrics. The first is the manually calculated metrics
    UIQM and UCIQE, which are shown in Table LABEL:tab:non_reference_results. The
    UIQM and UCIQE values of different DL-UIE algorithms given in Table LABEL:tab:non_reference_results
    show that UGAN achieves the overall best performance. Meanwhile, in terms of the
    evaluation of generalization ability, the UIQM and UCIQE values obtained by different
    algorithms are close. The second is URANKER obtained by data-driven training,
    which is given in Fig. [9](#S3.F9 "Figure 9 ‣ III-F5 Multi-input Fusion ‣ III-F
    Disentanglement & Fusion ‣ III UIE Methods ‣ A Comprehensive Survey on Underwater
    Image Enhancement Based on Deep Learning"). The best URANKER values are obtained
    by M (UIE-DM), K (CECF), E (UGAN), and I (STSC), respectively. The second best
    URANKER values are obtained by K (CECF), I (STSC), H (SCNet), M (UIE-DM) respectively.
    For the evaluation of URANKER, UIE-DM has the best overall evaluation effect.
    Considering Table LABEL:tab:non_reference_results and Fig. [9](#S3.F9 "Figure
    9 ‣ III-F5 Multi-input Fusion ‣ III-F Disentanglement & Fusion ‣ III UIE Methods
    ‣ A Comprehensive Survey on Underwater Image Enhancement Based on Deep Learning")
    simultaneously, the generalization ability of UGAN is the best. Fig. [10](#S3.F10
    "Figure 10 ‣ III-F5 Multi-input Fusion ‣ III-F Disentanglement & Fusion ‣ III
    UIE Methods ‣ A Comprehensive Survey on Underwater Image Enhancement Based on
    Deep Learning") shows the visual enhancement results of the evaluation of generalization
    ability. There are obvious differences in the visual results and pixel histograms
    of images obtained by different algorithms. Some enhanced images still show blue-green
    of green effect.
  prefs: []
  type: TYPE_NORMAL
- en: Overall Conclusions. Under our experimental settings, UIE-DM and UGAN achieved
    the best performance in fitting ability on full-reference datasets and generalization
    ability on no-reference datasets, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: V Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: According to our discussion of existing progress and experiments, there are
    still challenging problems that remain unsolved. We raise the following issues
    worthy of study.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Towards high-quality pairwise data synthesis by game engines. Constructing a
    large-scale database contains real-world paired underwater images is almost impossible.
    Existing methods that synthesizing data by algorithms are difficult to accurately
    simulate different influencing factors, such as illumination intensity, number
    of suspended particles, water depth, scene content, etc. For game engines that
    can customize extended functions, controlling influencing factors is an inherent
    advantage. For example, Liu et al. [[88](#bib.bib88)] use the UNREAL game engine
    to simulate non-uniform lighting, low-illumination, multiple light sources, diverse
    scene content and hazy effects at nighttime. It is worth exploring how game engines
    can be used to build datasets for the UIE task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effects on downstream vision tasks. High-level vision tasks, or be called as
    downstream tasks of the UIE task, have been used by existing research as a strategy
    to evaluate the performance of the UIE model itself. An intuitive and widely adopted
    hypothesis is that the UIE model’s ability to enhance the distorted images is
    positively related to its ability to facilitate downstream tasks. However, a recent
    study [[126](#bib.bib126)] on the object detection task and the UIE task discovered
    a surprising phenomenon, and here we directly quote their conclusion, “One of
    the most significant findings is that underwater image enhancement suppresses
    the performance of object detection.”. A comprehensive research on the correlation
    between the performance of the UIE and downstream tasks may provides a reliable
    conclusion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cooperation with large-scale pretrained vision-language models. The texture
    and local semantics contained in underwater images have been extensively studied.
    However, the global semantics that may be provided by language models have not
    been embedded into UIE models by existing research. Large-scale pretrained vision-language
    models, such as CLIP [[107](#bib.bib107), [154](#bib.bib154)], can provide human-level
    high-level semantic information. Text-image multi-modal restoration models have
    been partially studied in rain and haze removal [[93](#bib.bib93)]. Language features
    may facilitate the performance of UIE models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-uniform illumination. Artificial light sources are used by underwater equipment
    when the depth of the water exceeds the range that can obtain a properly illuminated
    image. However, unlike the smoothness of natural light, the illumination of images
    captured under artificial light sources may be non-uniform [[43](#bib.bib43)].
    The enhancement task under non-uniform lighting is worth digging into.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliable evaluation metrics. The difference between the enhanced result and
    the reference can be reliably evaluated by using full-reference evaluation metrics.
    However, obtaining ground-truth in the underwater imaging process is extremely
    challenging. Therefore, numerous literatures use no-reference evaluation metrics
    to evaluate the effectiveness of their proposed algorithms. The reliability of
    no-reference metrics needs to be consistent with human subjective aesthetics.
    Currently, in-depth research on related topics is still scarce. More research
    about subjective and objective evaluations are worthy of conduct.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combination with other image restoration tasks. As we discuss in this paper,
    there are inherent differences between the UIE task and other image restoration
    tasks. Preliminary attempts [[70](#bib.bib70)] have been made to combine tasks
    such as haze, rain and noise removal. The low-level features learned by different
    in-air image restoration models may be beneficial to each other. Utilizing auxiliary
    knowledge provided by other restoration tasks may improve the performance of UIE
    models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: VI Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we attempt to conduct a systematic review of the research of
    the underwater image enhancement task. We first investigated the research background
    and related work, which include physical models describing the degradation process,
    data construction strategies for model training, evaluation metrics implemented
    from different perspectives, and commonly used loss functions. We then provide
    a comprehensive taxonomy of existing UIE algorithms. According to the main contributions
    of each algorithm, state-of-the-art algorithms are discussed and analyzed from
    different perspectives. Further, we perform quantitative and qualitative evaluations
    on multiple benchmark datasets containing synthetic and real-world distorted underwater
    images. Finally, based on our summary of algorithms and analysis of the experiments,
    open issues and challenging topics are raised.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Agarwal, S. Gupta, and M. Vashishath. Contrast enhancement of underwater
    images using conditional generative adversarial network. Multimedia Tools and
    Applications, pages 1–30, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] D. Akkaynak and T. Treibitz. A revised underwater image formation model.
    In IEEE Conference on Computer Vision and Pattern Recognition, pages 6723–6732,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. Akkaynak and T. Treibitz. Sea-thru: A method for removing water from
    underwater images. In IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1682–1691, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. Akkaynak, T. Treibitz, T. Shlesinger, Y. Loya, R. Tamir, and D. Iluz.
    What is the space of attenuation coefficients in underwater computer vision? In
    IEEE Conference on Computer Vision and Pattern Recognition, pages 4931–4940, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Almahairi, S. Rajeshwar, A. Sordoni, P. Bachman, and A. Courville. Augmented
    cyclegan: Learning many-to-many mappings from unpaired data. In International
    conference on machine learning, pages 195–204, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] C. O. Ancuti, C. Ancuti, C. De Vleeschouwer, and P. Bekaert. Color balance
    and fusion for underwater image enhancement. IEEE Transactions on Image Processing,
    27(1):379–393, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. Anwar and C. Li. Diving deeper into underwater image enhancement: A
    survey. Signal Processing: Image Communication, 89:115978, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] M. Badran and M. Torki. Daut: Underwater image enhancement using depth
    aware u-shape transformer. In IEEE International Conference on Image Processing,
    pages 1830–1834, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Bing, W. Ren, Y. Tang, G. G. Yen, and Q. Sun. Domain adaptation for
    in-air to underwater image enhancement via deep learning. IEEE Transactions on
    Emerging Topics in Computational Intelligence, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Boudiaf, Y. Guo, A. Ghimire, N. Werghi, G. De Masi, S. Javed, and J. Dias.
    Underwater image enhancement using pre-trained transformer. In International Conference
    on Image Analysis and Processing, pages 480–488, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] N. Carlevaris-Bianco, A. Mohan, and R. M. Eustice. Initial results in
    underwater single image dehazing. In Oceans 2010 Mts/IEEE Seattle, pages 1–8,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] H. Chen, J. Gu, Y. Liu, S. A. Magid, C. Dong, Q. Wang, H. Pfister, and
    L. Zhu. Masked image training for generalizable deep image denoising. In IEEE
    Conference on Computer Vision and Pattern Recognition, pages 1692–1703, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] L. Chen, Z. Jiang, L. Tong, Z. Liu, A. Zhao, Q. Zhang, J. Dong, and H. Zhou.
    Perceptual underwater image enhancement with deep learning and physical priors.
    IEEE Transactions on Circuits and Systems for Video Technology, 31(8):3078–3092,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] X. Chen, J. Pan, J. Dong, and J. Tang. Towards unified deep image deraining:
    A survey and a new benchmark. arXiv preprint arXiv:2310.03535, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] X. Chen, P. Zhang, L. Quan, C. Yi, and C. Lu. Underwater image enhancement
    based on deep learning and image formation model. arXiv preprint arXiv:2101.00991,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Y.-W. Chen and S.-C. Pei. Domain adaptation for underwater image enhancement
    via content and style separation. IEEE Access, 10:90523–90534, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. Y. Chiang and Y.-C. Chen. Underwater image enhancement by wavelength
    compensation and dehazing. IEEE Transactions on Image Processing, 21(4):1756–1769,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Chu, Z. Fu, S. Yu, X. Tu, Y. Huang, and X. Ding. Underwater image enhancement
    and super-resolution using implicit neural networks. In IEEE International Conference
    on Image Processing, pages 1295–1299, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] X. Cong, J. Gui, and J. Hou. Underwater organism color fine-tuning via
    decomposition and guidance. In AAAI Conference on Artificial Intelligence, volume 38,
    pages 1389–1398, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] C. Desai, S. Benur, R. A. Tabib, U. Patil, and U. Mudenagudi. Depthcue:
    Restoration of underwater images using monocular depth as a clue. In IEEE Winter
    Conference on Applications of Computer Vision Workshops, pages 196–205, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] C. Desai, B. S. S. Reddy, R. A. Tabib, U. Patil, and U. Mudenagudi. Aquagan:
    Restoration of underwater images. In IEEE Conference on Computer Vision and Pattern
    Recognition Workshops, pages 296–304, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] C. Desai, R. A. Tabib, S. S. Reddy, U. Patil, and U. Mudenagudi. Ruig:
    Realistic underwater image generation towards restoration. In IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, pages 2181–2189, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] P. Drews, E. Nascimento, F. Moraes, S. Botelho, and M. Campos. Transmission
    estimation in underwater single images. In IEEE International Conference on Computer
    Vision Workshops, pages 825–830, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] P. L. Drews, E. R. Nascimento, S. S. Botelho, and M. F. M. Campos. Underwater
    depth estimation and image restoration based on single images. IEEE computer graphics
    and applications, 36(2):24–35, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Dudhane, P. Hambarde, P. Patil, and S. Murala. Deep underwater image
    restoration and beyond. IEEE Signal Processing Letters, 27:675–679, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] P. Duhamel and M. Vetterli. Fast fourier transforms: a tutorial review
    and a state of the art. Signal processing, 19(4):259–299, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Esmaeilzehi, Y. Ou, M. O. Ahmad, and M. Swamy. Dmml: Deep multi-prior
    and multi-discriminator learning for underwater image enhancement. IEEE Transactions
    on Broadcasting, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] C. Fabbri, M. J. Islam, and J. Sattar. Enhancing underwater imagery using
    generative adversarial networks. In International Conference on Robotics and Automation,
    pages 7159–7165, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Z. Fu, H. Lin, Y. Yang, S. Chai, L. Sun, Y. Huang, and X. Ding. Unsupervised
    underwater image restoration: From a homology perspective. In AAAI Conference
    on Artificial Intelligence, pages 643–651, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Z. Fu, X. Lin, W. Wang, Y. Huang, and X. Ding. Underwater image enhancement
    via learning water type desensitized representations. In IEEE International Conference
    on Acoustics, Speech and Signal Processing, pages 2764–2768, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Z. Fu, W. Wang, Y. Huang, X. Ding, and K.-K. Ma. Uncertainty inspired
    underwater image enhancement. In European Conference on Computer Vision, pages
    465–482, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. Gangisetty and R. R. Rai. Floodnet: Underwater image restoration based
    on residual dense learning. Signal Processing: Image Communication, 104:116647,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S.-B. Gao, M. Zhang, Q. Zhao, X.-S. Zhang, and Y.-J. Li. Underwater image
    enhancement using adaptive retinal mechanisms. IEEE Transactions on Image Processing,
    28(11):5580–5595, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Gonzalez-Sabbagh, A. Robles-Kelly, and S. Gao. Dgd-cgan: A dual generator
    for image dewatering and restoration. Pattern Recognition, 148:110159, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio. Generative adversarial networks. Communications of
    the ACM, 63(11):139–144, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Gui, X. Cong, Y. Cao, W. Ren, J. Zhang, J. Zhang, J. Cao, and D. Tao.
    A comprehensive survey and taxonomy on single image dehazing based on deep learning.
    ACM Computing Surveys, 55(13s):1–37, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] C. Guo, R. Wu, X. Jin, L. Han, W. Zhang, Z. Chai, and C. Li. Underwater
    ranker: Learn which is better and how to be better. In AAAI Conference on Artificial
    Intelligence, pages 702–709, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] P. Hambarde, S. Murala, and A. Dhall. Uw-gan: Single-image depth estimation
    and image enhancement for underwater images. IEEE Transactions on Instrumentation
    and Measurement, 70:1–12, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] J. Han, M. Shoeiby, T. Malthus, E. Botha, J. Anstee, S. Anwar, R. Wei,
    M. A. Armin, H. Li, and L. Petersson. Underwater image restoration via contrastive
    learning and a real-world dataset. Remote Sensing, 14(17):4297, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Han, M. Shoeiby, T. Malthus, E. Botha, J. Anstee, S. Anwar, R. Wei,
    L. Petersson, and M. A. Armin. Single underwater image restoration by contrastive
    learning. In IEEE International Geoscience and Remote Sensing Symposium, pages
    2385–2388, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. Han, Z. Lyu, T. Qiu, and M. Xu. A review on intelligence dehazing and
    color restoration for underwater images. IEEE Transactions on Systems, Man, and
    Cybernetics: Systems, 50(5):1820–1832, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models.
    Advances in Neural Information Processing Systems, 33:6840–6851, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] G. Hou, N. Li, P. Zhuang, K. Li, H. Sun, and C. Li. Non-uniform illumination
    underwater image restoration via illumination channel sparsity prior. IEEE Transactions
    on Circuits and Systems for Video Technology, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] G. Hou, X. Zhao, Z. Pan, H. Yang, L. Tan, and J. Li. Benchmarking underwater
    image enhancement and restoration, and beyond. IEEE Access, 8:122078–122091, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Hu, Q. Jiang, R. Cong, W. Gao, and F. Shao. Two-branch deep neural
    network for underwater image enhancement in hsv color space. IEEE Signal Processing
    Letters, 28:2152–2156, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] H. Huang, R. He, Z. Sun, and T. Tan. Wavelet-srnet: A wavelet-based cnn
    for multi-scale face super resolution. In IEEE International Conference on Computer
    Vision, pages 1689–1697, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] S. Huang, K. Wang, H. Liu, J. Chen, and Y. Li. Contrastive semi-supervised
    learning for underwater image restoration via reliable bank. In IEEE Conference
    on Computer Vision and Pattern Recognition, pages 18145–18155, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Z. Huang, J. Li, Z. Hua, and L. Fan. Underwater image enhancement via
    adaptive group attention-based multiscale cascade transformer. IEEE Transactions
    on Instrumentation and Measurement, 71:1–18, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] F. Huo, B. Li, and X. Zhu. Efficient wavelet boost learning-based multi-stage
    progressive refinement network for underwater image enhancement. In IEEE International
    Conference on Computer Vision Workshops, pages 1944–1952, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. J. Islam, C. Edge, Y. Xiao, P. Luo, M. Mehtaz, C. Morse, S. S. Enan,
    and J. Sattar. Semantic segmentation of underwater imagery: Dataset and benchmark.
    In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages
    1769–1776\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] M. J. Islam, P. Luo, and J. Sattar. Simultaneous enhancement and super-resolution
    of underwater imagery for improved visual perception. In 16th Robotics: Science
    and Systems, RSS 2020, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] M. J. Islam, R. Wang, and J. Sattar. Svam: Saliency-guided visual attention
    modeling by autonomous underwater robots. In Robotics: Science and Systems, NY,
    USA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. J. Islam, Y. Xia, and J. Sattar. Fast underwater image enhancement
    for improved visual perception. IEEE Robotics and Automation Letters, 5(2):3227–3234,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Jabbar, X. Li, and B. Omar. A survey on generative adversarial networks:
    Variants, applications, and training. ACM Computing Surveys, 54(8):1–49, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] N. Jain, G. R. Matta, and K. Mitra. Towards realistic underwater dataset
    generation and color restoration. In Proceedings of the Thirteenth Indian Conference
    on Computer Vision, Graphics and Image Processing, pages 1–9, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Jamadandi and U. Mudenagudi. Exemplar-based underwater image enhancement
    augmented by wavelet corrected transforms. In IEEE Conference on Computer Vision
    and Pattern Recognition Workshops, pages 11–17, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] K. Ji, W. Lei, and W. Zhang. A real-world underwater image enhancement
    method based on multi-color space and two-stage adaptive fusion. Signal, Image
    and Video Processing, pages 1–15, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] M. Jian, X. Liu, H. Luo, X. Lu, H. Yu, and J. Dong. Underwater image processing
    and analysis: A review. Signal Processing: Image Communication, 91:116088, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Q. Jiang, Y. Chen, G. Wang, and T. Ji. A novel deep neural network for
    noise removal from underwater image. Signal Processing: Image Communication, 87:115921,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Q. Jiang, Y. Gu, C. Li, R. Cong, and F. Shao. Underwater image enhancement
    quality evaluation: Benchmark dataset and objective metric. IEEE Transactions
    on Circuits and Systems for Video Technology, 32(9):5959–5974, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Q. Jiang, Y. Kang, Z. Wang, W. Ren, and C. Li. Perception-driven deep
    underwater image enhancement without paired supervision. IEEE Transactions on
    Multimedia, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Q. Jiang, Y. Zhang, F. Bao, X. Zhao, C. Zhang, and P. Liu. Two-step domain
    adaptation for underwater image enhancement. Pattern Recognition, 122:108324,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Z. Jiang, Z. Li, S. Yang, X. Fan, and R. Liu. Target oriented perceptual
    adversarial fusion network for underwater image enhancement. IEEE Transactions
    on Circuits and Systems for Video Technology, 32(10):6584–6598, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. Kapoor, R. Baghel, B. N. Subudhi, V. Jakhetiya, and A. Bansal. Domain
    adversarial learning towards underwater image enhancement. In IEEE International
    Conference on Computer Vision Workshops, pages 2241–2251, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] R. Khan, A. Mehmood, S. Akbar, and Z. Zheng. Underwater image enhancement
    with an adaptive self supervised network. In IEEE International Conference on
    Multimedia and Expo, pages 1355–1360, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] R. Khan, P. Mishra, N. Mehta, S. S. Phutke, S. K. Vipparthi, S. Nandi,
    and S. Murala. Spectroformer: Multi-domain query cascaded transformer network
    for underwater image enhancement. In IEEE Winter Conference on Applications of
    Computer Vision, pages 1454–1463, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] A. Khandouzi and M. Ezoji. Coarse-to-fine underwater image enhancement
    with lightweight cnn and attention-based refinement. Journal of Visual Communication
    and Image Representation, page 104068, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot,
    C. Liu, and D. Krishnan. Supervised contrastive learning. Advances in neural information
    processing systems, 33:18661–18673, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] G. Kim, S. W. Park, and J. Kwon. Pixel-wise wasserstein autoencoder for
    highly generative dehazing. IEEE Transactions on Image Processing, 30:5452–5462,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] B. Li, X. Liu, P. Hu, Z. Wu, J. Lv, and X. Peng. All-in-one image restoration
    for unknown corruption. In IEEE Conference on Computer Vision and Pattern Recognition,
    pages 17452–17462, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C. Li, S. Anwar, J. Hou, R. Cong, C. Guo, and W. Ren. Underwater image
    enhancement via medium transmission-guided multi-color space embedding. IEEE Transactions
    on Image Processing, 30:4985–5000, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] C. Li, S. Anwar, and F. Porikli. Underwater scene prior inspired deep
    underwater image and video enhancement. Pattern Recognition, 98:107038, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] C. Li, C. Guo, L. Han, J. Jiang, M.-M. Cheng, J. Gu, and C. C. Loy. Low-light
    image and video enhancement using deep learning: A survey. IEEE transactions on
    pattern analysis and machine intelligence, 44(12):9396–9416, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] C. Li, C. Guo, W. Ren, R. Cong, J. Hou, S. Kwong, and D. Tao. An underwater
    image enhancement benchmark dataset and beyond. IEEE Transactions on Image Processing,
    29:4376–4389, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] C. Li, J. Quo, Y. Pang, S. Chen, and J. Wang. Single underwater image
    restoration by blue-green channels dehazing and red channel correction. In IEEE
    International Conference on Acoustics, Speech and Signal Processing, pages 1731–1735,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] H. Li, J. Li, and W. Wang. A fusion adversarial underwater image enhancement
    network with a public test dataset. arXiv preprint arXiv:1906.06819, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] J. Li, K. A. Skinner, R. M. Eustice, and M. Johnson-Roberson. Watergan:
    Unsupervised generative network to enable real-time color correction of monocular
    underwater images. IEEE Robotics and Automation letters, 3(1):387–394, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] K. Li, H. Fan, Q. Qi, C. Yan, K. Sun, and Q. J. Wu. Tctl-net: Template-free
    color transfer learning for self-attention driven underwater image enhancement.
    IEEE Transactions on Circuits and Systems for Video Technology, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] K. Li, L. Wu, Q. Qi, W. Liu, X. Gao, L. Zhou, and D. Song. Beyond single
    reference for training: underwater image enhancement via comparative learning.
    IEEE Transactions on Circuits and Systems for Video Technology, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Y. Li, L. Shen, M. Li, Z. Wang, and L. Zhuang. Ruiesr: Realistic underwater
    image enhancement and super resolution. IEEE Transactions on Circuits and Systems
    for Video Technology, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] R. Lin, J. Liu, R. Liu, and X. Fan. Global structure-guided learning framework
    for underwater image enhancement. The Visual Computer, 38(12):4419–4434, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Lin, L. Shen, Z. Wang, K. Wang, and X. Zhang. Attenuation coefficient
    guided two-stage network for underwater image restoration. IEEE Signal Processing
    Letters, 28:199–203, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] P. Liu, G. Wang, H. Qi, C. Zhang, H. Zheng, and Z. Yu. Underwater image
    enhancement with a deep residual framework. IEEE Access, 7:94614–94629, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Q. Liu, Q. Zhang, W. Liu, W. Chen, X. Liu, and X. Wang. Wsds-gan: A weak-strong
    dual supervised learning method for underwater image enhancement. Pattern Recognition,
    page 109774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] R. Liu, X. Fan, M. Zhu, M. Hou, and Z. Luo. Real-world underwater enhancement:
    Challenges, benchmarks, and solutions under natural light. IEEE transactions on
    circuits and systems for video technology, 30(12):4861–4875, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] R. Liu, Z. Jiang, S. Yang, and X. Fan. Twin adversarial contrastive learning
    for underwater image enhancement and beyond. IEEE Transactions on Image Processing,
    31:4922–4936, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] X. Liu, Z. Gao, and B. M. Chen. Ipmgan: Integrating physical model and
    generative adversarial network for underwater image enhancement. Neurocomputing,
    453:538–551, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Liu, Z. Yan, S. Chen, T. Ye, W. Ren, and E. Chen. Nighthazeformer:
    Single nighttime haze removal using prior query transformer. In ACM International
    Conference on Multimedia, pages 4119–4128, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Liu, Z. Yan, J. Tan, and Y. Li. Multi-purpose oriented single nighttime
    image haze removal based on unified variational retinex model. IEEE Transactions
    on Circuits and Systems for Video Technology, 33(4):1643–1657, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin
    transformer: Hierarchical vision transformer using shifted windows. In IEEE International
    Conference on Computer Vision, pages 10012–10022, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] D. G. Lowe. Distinctive image features from scale-invariant keypoints.
    International journal of computer vision, 60:91–110, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. Lu, F. Guan, H. Zhang, and H. Lai. Speed-up ddpm for real-time underwater
    image enhancement. IEEE Transactions on Circuits and Systems for Video Technology,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Z. Luo, F. K. Gustafsson, Z. Zhao, J. Sjölund, and T. B. Schön. Controlling
    vision-language models for universal image restoration. In International Conference
    on Learning Representations, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Ma and C. Oh. A wavelet-based dual-stream network for underwater image
    enhancement. In IEEE International Conference on Acoustics, Speech and Signal
    Processing, pages 2769–2773, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Mei, X. Ye, X. Zhang, Y. Liu, J. Wang, J. Hou, and X. Wang. Uir-net:
    A simple and effective baseline for underwater image restoration and enhancement.
    Remote Sensing, 15(1):39, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Mittal, R. Soundararajan, and A. C. Bovik. Making a completely blind
    image quality analyzer. IEEE Signal processing letters, 20(3):209–212, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Moran and H. Qing. Mtnet: A multi-task cascaded network for underwater
    image enhancement. Multimedia Tools and Applications, pages 1–15, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] P. Mu, H. Xu, Z. Liu, Z. Wang, S. Chan, and C. Bai. A generalized physical-knowledge-guided
    dynamic model for underwater image enhancement. In ACM International Conference
    on Multimedia, pages 7111–7120, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Naik, A. Swarnakar, and K. Mittal. Shallow-uwnet: Compressed model
    for underwater image enhancement (student abstract). In AAAI Conference on Artificial
    Intelligence, pages 15853–15854, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Z. Niu, G. Zhong, and H. Yu. A review on the attention mechanism of deep
    learning. Neurocomputing, 452:48–62, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] K. Panetta, C. Gao, and S. Agaian. Human-visual-system-inspired underwater
    image quality measures. IEEE Journal of Oceanic Engineering, 41(3):541–551, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] L. Peng, C. Zhu, and L. Bian. U-shape transformer for underwater image
    enhancement. IEEE Transactions on Image Processing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y.-T. Peng and P. C. Cosman. Underwater image restoration based on image
    blurriness and light absorption. IEEE transactions on image processing, 26(4):1579–1594,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] H. Qi, H. Zhou, J. Dong, and X. Dong. Deep color-corrected multi-scale
    retinex network for underwater image enhancement. IEEE Transactions on Geoscience
    and Remote Sensing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Q. Qi, K. Li, H. Zheng, X. Gao, G. Hou, and K. Sun. Sguie-net: Semantic
    attention guided underwater image enhancement with multi-scale perception. IEEE
    Transactions on Image Processing, 31:6816–6830, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Q. Qi, Y. Zhang, F. Tian, Q. J. Wu, K. Li, X. Luan, and D. Song. Underwater
    image co-enhancement with correlation feature matching and joint learning. IEEE
    Transactions on Circuits and Systems for Video Technology, 32(3):1133–1147, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from
    natural language supervision. In International Conference on Machine Learning,
    pages 8748–8763\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Rao, W. Liu, K. Li, H. Fan, S. Wang, and J. Dong. Deep color compensation
    for generalized underwater image enhancement. IEEE Transactions on Circuits and
    Systems for Video Technology, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] S. Raveendran, M. D. Patil, and G. K. Birajdar. Underwater image enhancement:
    a comprehensive review, recent trends, challenges and applications. Artificial
    Intelligence Review, 54:5413–5467, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. Saleh, M. Sheaves, D. Jerry, and M. R. Azghadi. Adaptive uncertainty
    distribution in deep learning for unsupervised underwater image enhancement. arXiv
    preprint arXiv:2212.08983, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] P. Sharma, I. Bisht, and A. Sur. Wavelength-based attributed deep neural
    network for underwater image restoration. ACM Transactions on Multimedia Computing,
    Communications and Applications, 19(1):1–23, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] X. Shi and Y.-G. Wang. Cpdm: Content-preserving diffusion model for underwater
    image enhancement. arXiv preprint arXiv:2401.15649, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] W. Song, Z. Shen, M. Zhang, Y. Wang, and A. Liotta. A hierarchical probabilistic
    underwater image enhancement model with reinforcement tuning. Journal of Visual
    Communication and Image Representation, page 104052, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] W. Song, Y. Wang, D. Huang, A. Liotta, and C. Perra. Enhancement of underwater
    images with statistical model of background light and optimization of transmission
    map. IEEE Transactions on Broadcasting, 66(1):153–169, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] W. Song, Y. Wang, D. Huang, and D. Tjondronegoro. A rapid scene depth
    estimation model based on underwater light attenuation prior for underwater image
    restoration. In Advances in Multimedia Information Processing Pacific-Rim Conference
    on Multimedia, pages 678–688, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Y. Tang, T. Iwaguchi, H. Kawasaki, R. Sagawa, and R. Furukawa. Autoenhancer:
    Transformer on u-net architecture search for underwater image enhancement. In
    Proceedings of the Asian Conference on Computer Vision, pages 1403–1420, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Tang, H. Kawasaki, and T. Iwaguchi. Underwater image enhancement by
    transformer-based diffusion model with non-uniform sampling for skip strategy.
    In ACM International Conference on Multimedia, pages 5419–5427, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] J. Tian, X. Guo, W. Liu, D. Tao, and B. Liu. Deformable convolutional
    network constrained by contrastive learning for underwater image enhancement.
    IEEE Geoscience and Remote Sensing Letters, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] P. M. Uplavikar, Z. Wu, and Z. Wang. All-in-one underwater image enhancement
    using domain-adversarial learning. In IEEE Conference on Computer Vision and Pattern
    Recognition Workshops, pages 1–8, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information
    processing systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] G. Verma, M. Kumar, and S. Raikwar. F2uie: feature transfer-based underwater
    image enhancement using multi-stackcnn. Multimedia Tools and Applications, pages
    1–22, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] D. Wang, L. Ma, R. Liu, and X. Fan. Semantic-aware texture-structure
    feature collaboration for underwater image enhancement. In International Conference
    on Robotics and Automation, pages 4592–4598, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] H. Wang, W. Zhang, L. Bai, and P. Ren. Metalantis: A comprehensive underwater
    image enhancement framework. IEEE Transactions on Geoscience and Remote Sensing,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Y. Wang, Y. Cao, J. Zhang, F. Wu, and Z.-J. Zha. Leveraging deep statistics
    for underwater image enhancement. ACM Transactions on Multimedia Computing, Communications,
    and Applications, 17(3s):1–20, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Y. Wang, J. Guo, H. Gao, and H. Yue. Uiec2̂-net: Cnn-based underwater
    image enhancement using two color space. Signal Processing: Image Communication,
    96:116250, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Y. Wang, J. Guo, W. He, H. Gao, H. Yue, Z. Zhang, and C. Li. Is underwater
    image enhancement all object detectors need? IEEE Journal of Oceanic Engineering,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Y. Wang, S. Hu, S. Yin, Z. Deng, and Y.-H. Yang. A multi-level wavelet-based
    underwater image enhancement network with color compensation prior. Expert Systems
    with Applications, 242:122710, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Y. Wang, W. Song, G. Fortino, L.-Z. Qi, W. Zhang, and A. Liotta. An experimental-based
    review of image enhancement and image restoration methods for underwater imaging.
    IEEE access, 7:140233–140251, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality
    assessment: from error visibility to structural similarity. IEEE transactions
    on image processing, 13(4):600–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Z. Wang, L. Shen, M. Xu, M. Yu, K. Wang, and Y. Lin. Domain adaptation
    for underwater image enhancement. IEEE Transactions on Image Processing, 32:1442–1457,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Y. Wei, Z. Zheng, and X. Jia. Uhd underwater image enhancement via frequency-spatial
    domain aware network. In Proceedings of the Asian Conference on Computer Vision,
    pages 299–314, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] J. Wen, J. Cui, G. Yang, B. Zhao, Y. Zhai, Z. Gao, L. Dou, and B. M.
    Chen. Waterformer: A global–local transformer for underwater image enhancement
    with environment adaptor. IEEE Robotics & Automation Magazine, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] R. J. Williams. Simple statistical gradient-following algorithms for
    connectionist reinforcement learning. Machine learning, 8:229–256, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Z. Wu, Z. Wu, X. Chen, Y. Lu, and J. Yu. Self-supervised underwater image
    generation for underwater domain pre-training. IEEE Transactions on Instrumentation
    and Measurement, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Y. Xie, L. Kong, K. Chen, Z. Zheng, X. Yu, Z. Yu, and B. Zheng. Uveb:
    A large-scale benchmark and baseline towards real-world underwater video enhancement.
    In IEEE Conference on Computer Vision and Pattern Recognition, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] X. Xu, R. Wang, and J. Lu. Low-light image enhancement via structure
    modeling and guidance. In IEEE Conference on Computer Vision and Pattern Recognition,
    pages 9893–9903, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] X. Xue, Z. Hao, L. Ma, Y. Wang, and R. Liu. Joint luminance and chrominance
    learning for underwater image enhancement. IEEE Signal Processing Letters, 28:818–822,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] X. Xue, Z. Li, L. Ma, Q. Jia, R. Liu, and X. Fan. Investigating intrinsic
    degradation factors by multi-branch aggregation for real-world underwater image
    enhancement. Pattern Recognition, 133:109041, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] H. Yan, Z. Zhang, J. Xu, T. Wang, P. An, A. Wang, and Y. Duan. Uw-cyclegan:
    Model-driven cyclegan for underwater image restoration. IEEE Transactions on Geoscience
    and Remote Sensing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] S. Yan, X. Chen, Z. Wu, M. Tan, and J. Yu. Hybrur: A hybrid physical-neural
    solution for unsupervised underwater image restoration. IEEE Transactions on Image
    Processing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] X. Yan, W. Qin, Y. Wang, G. Wang, and X. Fu. Attention-guided dynamic
    multi-branch neural network for underwater image enhancement. Knowledge-Based
    Systems, 258:110041, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] G. Yang, G. Kang, J. Lee, and Y. Cho. Joint-id: Transformer-based joint
    image enhancement and depth estimation for underwater environments. IEEE Sensors
    Journal, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] H.-H. Yang, K.-C. Huang, and W.-T. Chen. Laffnet: A lightweight adaptive
    feature fusion network for underwater image enhancement. In IEEE International
    Conference on Robotics and Automation, pages 685–692, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] M. Yang, J. Hu, C. Li, G. Rohde, Y. Du, and K. Hu. An in-depth survey
    of underwater image enhancement and restoration. IEEE Access, 7:123638–123657,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M. Yang, K. Hu, Y. Du, Z. Wei, Z. Sheng, and J. Hu. Underwater image
    enhancement based on conditional generative adversarial network. Signal Processing:
    Image Communication, 81:115723, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] M. Yang and A. Sowmya. An underwater color image quality evaluation metric.
    IEEE Transactions on Image Processing, 24(12):6062–6071, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] T. Ye, S. Chen, Y. Liu, Y. Ye, E. Chen, and Y. Li. Underwater light field
    retention: Neural rendering for underwater imaging. In IEEE Conference on Computer
    Vision and Pattern Recognition Workshops, pages 488–497, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Yin, Y. Wang, B. Guan, X. Zeng, and L. Guo. Unsupervised underwater
    image enhancement based on disentangled representations via double-order contrastive
    loss. IEEE Transactions on Geoscience and Remote Sensing, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M. Yu, L. Shen, Z. Wang, and X. Hua. Task-friendly underwater image enhancement
    for machine vision applications. IEEE Transactions on Geoscience and Remote Sensing,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] D. Zhang, Y. Guo, J. Zhou, W. Zhang, Z. Lin, K. Polat, F. Alenezi, and
    A. Alhudhaif. Tanet: Transmission and atmospheric light driven enhancement of
    underwater images. Expert Systems with Applications, 242:122693, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] D. Zhang, C. Wu, J. Zhou, W. Zhang, C. Li, and Z. Lin. Hierarchical attention
    aggregation with multi-resolution feature learning for gan-based underwater image
    enhancement. Engineering Applications of Artificial Intelligence, 125:106743,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] D. Zhang, C. Wu, J. Zhou, W. Zhang, Z. Lin, K. Polat, and F. Alenezi.
    Robust underwater image enhancement with cascaded multi-level sub-networks and
    triple attention mechanism. Neural Networks, 169:685–697, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] D. Zhang, J. Zhou, W. Zhang, Z. Lin, J. Yao, K. Polat, F. Alenezi, and
    A. Alhudhaif. Rex-net: A reflectance-guided underwater image enhancement network
    for extreme scenarios. Expert Systems with Applications, page 120842, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] F. Zhang, S. You, Y. Li, and Y. Fu. Atlantis: Enabling underwater depth
    estimation with stable diffusion. In IEEE Conference on Computer Vision and Pattern
    Recognition, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] K. Zhang, W. Ren, W. Luo, W.-S. Lai, B. Stenger, M.-H. Yang, and H. Li.
    Deep image deblurring: A survey. International Journal of Computer Vision, 130(9):2103–2130,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Zhang, S. Zhao, D. An, D. Li, and R. Zhao. Liteenhancenet: A lightweight
    network for real-time single underwater image enhancement. Expert Systems with
    Applications, 240:122546, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] W. Zhang, L. Dong, X. Pan, P. Zou, L. Qin, and W. Xu. A survey of restoration
    and enhancement for underwater images. IEEE Access, 7:182259–182279, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] W. Zhang, Y. Liu, C. Dong, and Y. Qiao. Ranksrgan: Super resolution generative
    adversarial networks with learning to rank. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 44(10):7149–7166, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] W. Zhang, P. Zhuang, H.-H. Sun, G. Li, S. Kwong, and C. Li. Underwater
    image enhancement via minimal color loss and locally adaptive contrast enhancement.
    IEEE Transactions on Image Processing, 31:3997–4010, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Z. Zhang, Z. Jiang, J. Liu, X. Fan, and R. Liu. Waterflow: heuristic
    normalizing flow for underwater image enhancement and beyond. In ACM International
    Conference on Multimedia, pages 7314–7323, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] C. Zhao, W. Cai, C. Dong, and C. Hu. Wavelet-based fourier information
    interaction with frequency diffusion adjustment for underwater image restoration.
    arXiv preprint arXiv:2311.16845, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] C. Zhao, W. Cai, C. Dong, and Z. Zeng. Toward sufficient spatial-frequency
    interaction for gradient-aware underwater image enhancement. arXiv preprint arXiv:2309.04089,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu. Object detection with deep
    learning: A review. IEEE transactions on neural networks and learning systems,
    30(11):3212–3232, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Y. Zheng, W. Chen, R. Lin, T. Zhao, and P. Le Callet. Uif: An objective
    quality assessment for underwater image enhancement. IEEE Transactions on Image
    Processing, 31:5456–5468, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] J. Zhou, Q. Gai, D. Zhang, K.-M. Lam, W. Zhang, and X. Fu. Iacc: Cross-illumination
    awareness and color correction for underwater images under mixed natural and artificial
    lighting. IEEE Transactions on Geoscience and Remote Sensing, 62:1–15, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] J. Zhou, B. Li, D. Zhang, J. Yuan, W. Zhang, Z. Cai, and J. Shi. Ugif-net:
    An efficient fully guided information flow network for underwater image enhancement.
    IEEE Transactions on Geoscience and Remote Sensing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Zhou, T. Liang, Z. He, D. Zhang, W. Zhang, X. Fu, and C. Li. Waterhe-nerf:
    Water-ray tracing neural radiance fields for underwater scene reconstruction.
    arXiv preprint arXiv:2312.06946, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] J. Zhou, J. Sun, C. Li, Q. Jiang, M. Zhou, K.-M. Lam, W. Zhang, and X. Fu.
    Hclr-net: Hybrid contrastive learning regularization with locally randomized perturbation
    for underwater image enhancement. International Journal of Computer Vision, pages
    1–25, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] J. Zhou, J. Sun, W. Zhang, and Z. Lin. Multi-view underwater image enhancement
    method via embedded fusion mechanism. Engineering Applications of Artificial Intelligence,
    121:105946, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] J. Zhou, T. Yang, and W. Zhang. Underwater vision enhancement technologies:
    a comprehensive review, challenges, and recent trends. Applied Intelligence, 53(3):3594–3621,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Y. Zhou and K. Yan. Domain adaptive adversarial learning based on physics
    model feedback for underwater image enhancement. arXiv preprint arXiv:2002.09315,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image
    translation using cycle-consistent adversarial networks. In IEEE International
    Conference on Computer Vision, pages 2223–2232, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] P. Zhu, Y. Liu, Y. Wen, M. Xu, X. Fu, and S. Liu. Unsupervised underwater
    image enhancement via content-style representation disentanglement. Engineering
    Applications of Artificial Intelligence, 126:106866, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] P. Zhu, Y. Liu, M. Xu, X. Fu, N. Wang, and S. Liu. Unsupervised multiple
    representation disentanglement framework for improved underwater visual perception.
    IEEE Journal of Oceanic Engineering, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] P. Zhuang, J. Wu, F. Porikli, and C. Li. Underwater image enhancement
    with hyper-laplacian reflectance priors. IEEE Transactions on Image Processing,
    31:5442–5455, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
