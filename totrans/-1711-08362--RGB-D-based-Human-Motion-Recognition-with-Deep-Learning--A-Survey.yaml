- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:08:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:08:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1711.08362] RGB-D-based Human Motion Recognition with Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1711.08362] 基于RGB-D的人体运动识别与深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1711.08362](https://ar5iv.labs.arxiv.org/html/1711.08362)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1711.08362](https://ar5iv.labs.arxiv.org/html/1711.08362)
- en: 'RGB-D-based Human Motion Recognition with Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于RGB-D的人体运动识别与深度学习：综述
- en: Pichao Wang [pw212@uowmail.edu.au](mailto:pw212@uowmail.edu.au) Wanqing Li [wanqing@uow.edu.au](mailto:wanqing@uow.edu.au)
    Philip Ogunbona [philipo@uow.edu.au](mailto:philipo@uow.edu.au) Jun Wan [jun.wan@nlpr.ia.ac.cn](mailto:jun.wan@nlpr.ia.ac.cn)
    Sergio Escalera [sergio@maia.ub.es](mailto:sergio@maia.ub.es) Advanced Multimedia
    Research Lab, University of Wollongong, Australia Motovis Inc., Adelaide, Australia
    Center for Biometrics and Security Research (CBSR)& National Laboratory of Pattern
    Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA),
    Beijing, China University of Barcelona and Computer Vision Center, Campus UAB,
    Barcelona, Spain
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pichao Wang [pw212@uowmail.edu.au](mailto:pw212@uowmail.edu.au) Wanqing Li [wanqing@uow.edu.au](mailto:wanqing@uow.edu.au)
    Philip Ogunbona [philipo@uow.edu.au](mailto:philipo@uow.edu.au) Jun Wan [jun.wan@nlpr.ia.ac.cn](mailto:jun.wan@nlpr.ia.ac.cn)
    Sergio Escalera [sergio@maia.ub.es](mailto:sergio@maia.ub.es) 高级多媒体研究实验室，伍伦贡大学，澳大利亚
    Motovis Inc.，阿德莱德，澳大利亚 生物特征识别与安全研究中心（CBSR）& 自动化研究所（NLPR），中国科学院（CASIA），北京， 中国 巴萨罗那大学及计算机视觉中心，UAB校园，巴萨罗那，西班牙
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Human motion recognition is one of the most important branches of human-centered
    research activities. In recent years, motion recognition based on RGB-D data has
    attracted much attention. Along with the development in artificial intelligence,
    deep learning techniques have gained remarkable success in computer vision. In
    particular, convolutional neural networks (CNN) have achieved great success for
    image-based tasks, and recurrent neural networks (RNN) are renowned for sequence-based
    problems. Specifically, deep learning methods based on the CNN and RNN architectures
    have been adopted for motion recognition using RGB-D data. In this paper, a detailed
    overview of recent advances in RGB-D-based motion recognition is presented. The
    reviewed methods are broadly categorized into four groups, depending on the modality
    adopted for recognition: RGB-based, depth-based, skeleton-based and RGB+D-based.
    As a survey focused on the application of deep learning to RGB-D-based motion
    recognition, we explicitly discuss the advantages and limitations of existing
    techniques. Particularly, we highlighted the methods of encoding spatial-temporal-structural
    information inherent in video sequence, and discuss potential directions for future
    research.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人体运动识别是以人为中心的研究活动中最重要的分支之一。近年来，基于RGB-D数据的运动识别引起了广泛关注。随着人工智能的发展，深度学习技术在计算机视觉领域取得了显著成功。特别是，卷积神经网络（CNN）在基于图像的任务中取得了巨大成功，而递归神经网络（RNN）则以其在序列问题上的卓越表现而闻名。具体而言，基于CNN和RNN架构的深度学习方法已被用于RGB-D数据的运动识别。本文详细概述了基于RGB-D的运动识别的最新进展。所述方法大致分为四类，具体取决于用于识别的模态：基于RGB、基于深度、基于骨架和基于RGB+D。作为一项专注于深度学习在RGB-D运动识别中的应用的综述，我们明确讨论了现有技术的优缺点。特别是，我们强调了编码视频序列中固有的时空结构信息的方法，并讨论了未来研究的潜在方向。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Human Motion Recognition, RGB-D Data, Deep Learning, Survey^†^†journal: Computer
    Vision and Image Understanding'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人体运动识别，RGB-D数据，深度学习，综述^†^†期刊：计算机视觉与图像理解
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Among the several human-centered research activities (e.g. human detection,
    tracking, pose estimation and motion recognition) in computer vision, human motion
    recognition is particularly important due to its potential application in video
    surveillance, human computer interfaces, ambient assisted living, human-robot
    interaction, intelligent driving, etc. A human motion recognition task can be
    summarised as the automatic identification of human behaviours from images or
    video sequences. The complexity and duration of the motion involved can be used
    as basis for broad categorization into four kinds namely gesture, action, interaction
    and group activity. A gesture can be defined as the basic movement or positioning
    of the hand, arm, body, or head that communicates an idea, emotion, etc. “Hand
    waving” and “nodding” are some typical examples of gestures. Usually, a gesture
    has relatively short duration. An action is considered as a type of motion performed
    by a single person during short time period and involves multiple body parts,
    in contrast with the few body parts that involved in gesture. An activity is composed
    by a sequence of actions. An interaction is a type of motion performed by two
    actors; one actor is human while the other may be human or an object. This implies
    that the interaction category will include human-human or human-object interaction.
    “Hugging each other” and “playing guitar” are examples of these two kinds of interaction,
    respectively. Group activity is the most complex type of activity, and it may
    be a combination of gestures, actions and interactions. Necessarily, it involves
    more than two humans and from zero to multiple objects. Examples of group activities
    would include “two teams playing basketball” and “group meeting”.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中的几个人体中心研究活动（例如，人类检测、跟踪、姿态估计和运动识别）中，人类运动识别尤其重要，因为它在视频监控、人机接口、环境辅助生活、人机交互、智能驾驶等领域具有潜在应用。人类运动识别任务可以总结为从图像或视频序列中自动识别人的行为。涉及的运动的复杂性和持续时间可以作为广泛分类的基础，分为手势、动作、交互和群体活动四类。手势可以定义为传达思想、情感等的手、手臂、身体或头部的基本运动或定位。“挥手”和“点头”是手势的一些典型例子。通常，手势的持续时间相对较短。动作被认为是一种由单个人在短时间内执行的运动，并涉及多个身体部位，相比之下，手势只涉及少量身体部位。活动由一系列动作组成。交互是一种由两个参与者执行的运动；一个参与者是人类，而另一个可以是人类或物体。这意味着交互类别将包括人类与人类或人类与物体的交互。“互相拥抱”和“弹吉他”分别是这两种交互的例子。群体活动是最复杂的活动类型，它可能是手势、动作和交互的组合。它必然涉及两个以上的人类以及零到多个物体。群体活动的例子包括“两个队伍打篮球”和“团队会议”。
- en: Early research on human motion recognition was dominated by the analysis of
    still images or videos [[2](#bib.bib2), [144](#bib.bib144), [132](#bib.bib132),
    [99](#bib.bib99), [44](#bib.bib44), [176](#bib.bib176)]. Most of these efforts
    used color and texture cues in 2D images for recognition. However, the task remains
    challenging due to problems posed by background clutter, partial occlusion, view-point,
    lighting changes, execution rate and biometric variation. This challenge remains
    even with current deep learning approaches [[49](#bib.bib49), [4](#bib.bib4)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对人类运动识别的早期研究主要集中在静态图像或视频的分析[[2](#bib.bib2), [144](#bib.bib144), [132](#bib.bib132),
    [99](#bib.bib99), [44](#bib.bib44), [176](#bib.bib176)]。这些研究大多使用了2D图像中的颜色和纹理线索进行识别。然而，由于背景杂乱、部分遮挡、视角、光照变化、执行速度和生物特征变化等问题，这项任务仍然具有挑战性。即使使用当前的深度学习方法，这一挑战依然存在[[49](#bib.bib49),
    [4](#bib.bib4)]。
- en: With the recent development of cost-effective RGB-D sensors, such as Microsoft
    Kinect ™and Asus Xtion ™, RGB-D-based motion recognition has attracted much attention.
    This is largely because the extra dimension (depth) is insensitive to illumination
    changes and includes rich 3D structural information of the scene. Additionally,
    3D positions of body joints can be estimated from depth maps [[114](#bib.bib114)].
    As a consequence, several methods based on RGB-D data have been proposed and the
    approach has proven to be a promising direction for human motion analysis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微软 Kinect™ 和华硕 Xtion™ 等成本效益高的RGB-D传感器的近期发展，基于RGB-D的运动识别引起了广泛关注。这主要是因为额外的维度（深度）对光照变化不敏感，并且包含了场景的丰富3D结构信息。此外，可以从深度图中估计身体关节的3D位置[[114](#bib.bib114)]。因此，已经提出了几种基于RGB-D数据的方法，这一方法已被证明是人类运动分析的一个有前途的方向。
- en: 'Several survey papers have summarized the research on human motion recognition
    using RGB-D data [[14](#bib.bib14), [166](#bib.bib166), [3](#bib.bib3), [16](#bib.bib16),
    [172](#bib.bib172), [28](#bib.bib28), [100](#bib.bib100), [46](#bib.bib46)]. Specifically,
    Chen et al. [[14](#bib.bib14)] focused on depth sensors, pre-processing of depth
    data, depth-based action recognition methods and datasets. In their work, Ye et
    al. [[166](#bib.bib166)] presented an overview of approaches using depth and skeleton
    modalities for tasks including activity recognition, head/hand pose estimation,
    facial feature detection and gesture recognition. The survey presented by Aggarwal
    and Xia [[3](#bib.bib3)] summarized five categories of representations based on
    3D silhouettes, skeletal joints/body part location, local spatial-temporal features,
    scene flow features and local occupancy features. The work of Cheng et al. [[16](#bib.bib16)]
    focused on RGB-D-based hand gesture recognition datasets and summarized corresponding
    methods from three perspectives: static hand gesture recognition, hand trajectory
    gesture recognition and continuous hand gesture recognition. In another effort
    Escalera et al. [[28](#bib.bib28)] reviewed the challenges and methods for gesture
    recognition using multimodal data. Some of the surveys have focused on available
    datasets for RGB-D research. For example, the work of Zhang et al. [[172](#bib.bib172)]
    described available benchmark RGB-D datasets for action/activity recognition and
    included 27 single-view datasets, 10 multi-view datasets and 7 multi-person datasets.
    Other works as Presti and La Cascia [[100](#bib.bib100)] and Han et al. [[46](#bib.bib46)]
    mainly reviewed skeleton-based representation and approaches for action recognition.
    A short survey on RGB-D action recognition using deep learning was recently presented
    in [[4](#bib.bib4)], analysing RGB and depth cues in terms of 2DCNN, 3DCNN, and
    Deep temporal approaches'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 几篇综述论文总结了使用RGB-D数据进行人类运动识别的研究[[14](#bib.bib14), [166](#bib.bib166), [3](#bib.bib3),
    [16](#bib.bib16), [172](#bib.bib172), [28](#bib.bib28), [100](#bib.bib100), [46](#bib.bib46)]。具体而言，陈等人[[14](#bib.bib14)]
    关注于深度传感器、深度数据的预处理、基于深度的动作识别方法和数据集。在他们的工作中，叶等人[[166](#bib.bib166)] 提供了使用深度和骨架模态的方法概述，涵盖了活动识别、头部/手部姿势估计、面部特征检测和手势识别等任务。Aggarwal和Xia[[3](#bib.bib3)]
    提出的综述总结了五类基于3D轮廓、骨架关节/身体部位位置、局部时空特征、场景流特征和局部占用特征的表示方法。程等人[[16](#bib.bib16)] 的工作重点是基于RGB-D的手势识别数据集，并从三个角度总结了相应的方法：静态手势识别、手部轨迹手势识别和连续手势识别。在另一个研究中，Escalera等人[[28](#bib.bib28)]
    综述了使用多模态数据进行手势识别的挑战和方法。一些综述专注于RGB-D研究中的可用数据集。例如，张等人[[172](#bib.bib172)] 描述了用于动作/活动识别的可用基准RGB-D数据集，包括27个单视图数据集、10个多视图数据集和7个多人数据集。其他工作如Presti和La
    Cascia[[100](#bib.bib100)] 以及Han等人[[46](#bib.bib46)] 主要回顾了基于骨架的动作识别表示和方法。最近在[[4](#bib.bib4)]
    中提出了一项关于使用深度学习进行RGB-D动作识别的简短综述，分析了RGB和深度线索在2DCNN、3DCNN和深度时间方法中的应用。
- en: '![Refer to caption](img/8cac053f00decb67ced6380783a8912c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8cac053f00decb67ced6380783a8912c.png)'
- en: 'Figure 1: Categorisation of the methods for RGB-D-based motion recognition
    using deep learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用深度学习进行RGB-D基础的运动识别方法的分类。
- en: 'All above surveys mainly focused on the analysis of handcrafted features. Here,
    we provide a comprehensive review of RGB-D-based human motion recognition using
    deep learning approaches. Even while focusing on deep learning approaches, the
    nature of the input data is still important. RGB-D data for human motion analysis
    comprises three modalities: RGB, depth and skeleton. The main characteristic of
    RGB data is its shape, color and texture which brings the benefits of extracting
    interesting points and optical flow. Compared to RGB videos, the depth modality
    is insensitive to illumination variations, invariant to color and texture changes,
    reliable for estimating body silhouette and skeleton, and provides rich 3D structural
    information of the scene. Differently from RGB and depth, skeleton data containing
    the positions of human joints, is a relatively high-level feature for motion recognition.
    The different properties of the three modalities have inspired the various methods
    found in the literature. For example, optical flow-based methods with Convolutional
    Neural Networks (CNN) is very effective for RGB channel [[27](#bib.bib27)]; depth
    rank pooling based-method with CNN is a good choice for depth modality [[152](#bib.bib152)];
    sequence based method with Recurrent Neural Networks (RNN) [[82](#bib.bib82)]
    and image-based method with CNN [[155](#bib.bib155)] are effective for skeleton;
    and scene flow-based method using CNN are promising for RGB+D channels [[151](#bib.bib151)].
    These methods are very effective for specific modalities, but not always the case
    for all the modalities. Given these observations, this survey identified four
    broad categories of methods based on the modality adopted for human motion recognition.
    The categories include RGB-based, depth-based, skeleton-based and RGB+D-based.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上述调查主要集中在手工特征分析上。在这里，我们提供了基于RGB-D的深度学习方法的人体动作识别的全面综述。即使专注于深度学习方法，输入数据的性质仍然很重要。用于人体动作分析的RGB-D数据包括三种模态：RGB、深度和骨架。RGB数据的主要特征是其形状、颜色和纹理，这带来了提取有趣点和光流的好处。与RGB视频相比，深度模态对光照变化不敏感，对颜色和纹理变化不变，可靠地估计身体轮廓和骨架，并提供场景的丰富3D结构信息。与RGB和深度不同，包含人体关节位置的骨架数据是用于动作识别的相对高级特征。这三种模态的不同特性激发了文献中出现的各种方法。例如，基于光流的卷积神经网络（CNN）方法对RGB通道非常有效[[27](#bib.bib27)];
    基于深度排名池化的CNN方法是深度模态的一个好选择[[152](#bib.bib152)]; 基于序列的递归神经网络（RNN）方法[[82](#bib.bib82)]和基于图像的CNN方法[[155](#bib.bib155)]对骨架有效;
    使用CNN的场景流方法对RGB+D通道有前景[[151](#bib.bib151)]。这些方法对特定模态非常有效，但并非总适用于所有模态。鉴于这些观察结果，本综述根据用于人体动作识别的模态确定了四个广泛的类别。这些类别包括基于RGB的、基于深度的、基于骨架的和基于RGB+D的。
- en: In each category, two sub-divisions are further identified, namely segmented
    human motion recognition and continuous/online motion recognition. For segmented
    motion recognition, the scenario of the problem can be simply described as classifying
    a well delineated sequence of video frames as one of a set of motion types. This
    is in contrast to continuous/online human motion recognition where there are no
    a priori given boundaries of motion execution. The online situation is compounded
    by the fact that the video sequence is not recorded and the algorithm must deal
    with frames as they are being captured, save for possibly a small data cache.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个类别中，进一步识别了两个子类别，即分段人体动作识别和连续/在线动作识别。对于分段动作识别，问题的场景可以简单描述为将一段明确划分的视频帧序列分类为一组动作类型之一。这与连续/在线人体动作识别形成对比，后者没有给定的动作执行边界。在线情况还受到视频序列未被记录的事实的影响，算法必须处理在捕获过程中出现的帧，可能仅保存一个小的数据缓存。
- en: During the performance of a specified motion spatial information which refers
    to the spatial configuration of human body at an instant of time (e.g. relative
    positions of the human body parts) can be identified. Similarly, there is the
    temporal information which characterizes the spatial configuration of the body
    over time (i.e. the dynamics of the body). Lastly, the structural information
    encodes the coordination and synchronization of body parts over the period in
    which the action is being performed. It describes the relationship of the spatial
    configurations of human body across different time slots.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行特定动作时，可以识别出空间信息，即在某一时刻人体的空间配置（例如，人体部位的相对位置）。类似地，还有时间信息，它描述了身体随时间变化的空间配置（即身体的动态）。最后，结构信息编码了在执行动作期间身体部位的协调与同步。它描述了在不同时间段内人体空间配置的关系。
- en: In reviewing the various methods, consideration has been given to the manner
    in which the spatial, temporal and structural information have been exploited.
    Hence, the survey discusses the advantages and limitations of the reviewed methods
    from the spatial-temporal-structural encoding viewpoint, and suggests potential
    directions for future research.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在评审各种方法时，考虑了如何利用空间、时间和结构信息。因此，调查从空间-时间-结构编码的角度讨论了所评审方法的优点和局限性，并提出了未来研究的潜在方向。
- en: 'A key novelty of this survey is the focus on three architectures of neural
    networks used in the various deep learning methods reviewed namely CNN-based,
    RNN-based and other structured networks. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey") illustrates
    the taxonomy underpinning this survey.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的一个关键创新点是关注于所评审的各种深度学习方法中使用的三种神经网络架构，即基于CNN、基于RNN和其他结构化网络。图[1](#S1.F1 "图
    1 ‣ 1 引言 ‣ 基于RGB-D的数据的人体运动识别与深度学习：一项调查")展示了支撑本调查的分类法。
- en: 'This is one of the first surveys dedicated to RGB-D-based human motion recognition
    using deep learning. Apart from this claim, this survey distinguishes itself from
    other surveys through the following contributions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是首批专注于基于RGB-D的深度学习人体运动识别的调查之一。除了这一声明外，本调查还通过以下贡献与其他调查区分开来：
- en: '1.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Comprehensive coverage of the most recent and advanced deep learning-based methods
    developed in the last five years, thereby providing readers with a complete overview
    of recent research results and state-of-the-art methods.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对过去五年中开发的最新和最先进的基于深度学习的方法进行了全面覆盖，从而为读者提供了对近期研究成果和最先进方法的完整概述。
- en: '2.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: Insightful categorization and analysis of methods based on the different properties
    of the modalities; highlight of the pros and cons of the methods described in
    the reviewed papers from the viewpoint of spatial-temporal-structural encoding.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据模态的不同属性，对方法进行了有见地的分类和分析；从空间-时间-结构编码的角度突出了所述方法的优缺点。
- en: '3.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: Discussion of the challenges of RGB-D-based motion recognition; analysis of
    the limitations of available methods and discussion of potential research directions.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 讨论了基于RGB-D的运动识别的挑战；分析了现有方法的局限性并讨论了潜在的研究方向。
- en: Additionally, several recently released or commonly used RGB-D-based benchmark
    datasets associated with deep learning are surveyed. The main application domain
    of interest in this survey paper is human motion recognition based on RGB-D data,
    including gesture recognition, action/activity recognition and interaction recognition.
    The lack of datasets focused on RGB-D-based group activity recognition has led
    to paucity of research on this topic and thus this survey does not cover this
    topic. Other RGB-D-based human-centered applications, such as human detection,
    tracking and pose estimation, are also not the focus of this paper. For surveys
    on RGB-D data acquisition readers are referred to [[14](#bib.bib14), [16](#bib.bib16),
    [46](#bib.bib46)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还调查了与深度学习相关的几种最近发布或常用的RGB-D基准数据集。本调查论文主要关注基于RGB-D数据的人体运动识别，包括手势识别、动作/活动识别和交互识别。由于缺乏专注于基于RGB-D的群体活动识别的数据集，因此该主题的研究相对稀缺，本调查未涵盖此主题。其他基于RGB-D的以人为中心的应用，如人体检测、跟踪和姿态估计，也不是本文的重点。关于RGB-D数据获取的调查，请参考[[14](#bib.bib14)、[16](#bib.bib16)、[46](#bib.bib46)]。
- en: 'Subsequent sections of the this survey are organized as follows. Commonly used
    RGB-D-based benchmark datasets are described in Section [2](#S2 "2 Benchmark Datasets
    ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey"). Sections [3](#S3
    "3 RGB-based Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion
    Recognition with Deep Learning: A Survey") to [6](#S6 "6 RGB+D-based Motion Recognition
    with Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning:
    A Survey") discuss methods of RGB-D-based motion recognition using deep learning
    from four perspectives: RGB-based motion recognition, depth-based motion recognition,
    skeleton-based motion recognition and RGB+D-based motion recognition. Challenges
    of RGB-D-based motion recognition and pointers to future directions are presented
    in Section [7](#S7 "7 Discussion ‣ RGB-D-based Human Motion Recognition with Deep
    Learning: A Survey"). The survey provides concluding remarks in Section [8](#S8
    "8 Conclusion ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的后续部分组织如下。常用的基于RGB-D的基准数据集在第[2](#S2 "2 Benchmark Datasets ‣ RGB-D-based
    Human Motion Recognition with Deep Learning: A Survey")节中描述。第[3](#S3 "3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")至[6](#S6 "6 RGB+D-based Motion Recognition with Deep
    Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")节讨论了从四个角度：基于RGB的运动识别、基于深度的运动识别、基于骨骼的运动识别和基于RGB+D的运动识别，使用深度学习进行RGB-D基于的运动识别的方法。RGB-D基于的运动识别的挑战和未来方向的指引在第[7](#S7
    "7 Discussion ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")节中介绍。调查在第[8](#S8
    "8 Conclusion ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")节中提供了结论性评论。'
- en: 2 Benchmark Datasets
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基准数据集
- en: 'Over the last decade, a number of RGB-D benchmark datasets have been collected
    and made publicly available for the research community. The sources of the datasets
    are mainly of three categories [[14](#bib.bib14), [16](#bib.bib16), [46](#bib.bib46)]:
    Motion capture (Mocap) system, structured-light cameras (e.g. Kinect v1) and time-of-flight
    (ToF) cameras (e.g. Kinect v2). Hence the modalities of the datasets cover RGB,
    depth, skeleton and their combinations. With the advance of deep learning, deep
    methods have been developed for estimating skeletons directly from single images
    or video sequences, such as DeepPose [[129](#bib.bib129)], Deepercut [[58](#bib.bib58)]
    and Adversarial PoseNet [[15](#bib.bib15)]. A comprehensive survey of these datasets
    have appeared in the literature (see e.g. [[16](#bib.bib16)] for hand gestures
    and [[172](#bib.bib172)] for action recognition). In the present survey only 15
    large-scale datasets that have been commonly adopted for evaluating deep learning-based
    methods are described. The reader is referred to Table LABEL:performance for a
    sample of works (publications) that have used these datasets. For the purpose
    of this survey, the datasets have been divided into two groups: segmented datasets
    and continuous/online datasets.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，已经收集了许多RGB-D基准数据集，并公开提供给研究社区。这些数据集的来源主要分为三类[[14](#bib.bib14), [16](#bib.bib16),
    [46](#bib.bib46)]：运动捕捉系统（Mocap）、结构光相机（例如Kinect v1）和飞行时间（ToF）相机（例如Kinect v2）。因此，这些数据集的模式涵盖了RGB、深度、骨骼及其组合。随着深度学习的发展，已经开发出用于直接从单幅图像或视频序列中估计骨骼的深度方法，如DeepPose[[129](#bib.bib129)]、Deepercut[[58](#bib.bib58)]和Adversarial
    PoseNet[[15](#bib.bib15)]。文献中已经出现了这些数据集的综合调查（例如，参见[[16](#bib.bib16)]的手势和[[172](#bib.bib172)]的动作识别）。在本调查中，仅描述了15个被广泛采用用于评估基于深度学习的方法的大规模数据集。读者可以参考表LABEL:performance，查看使用这些数据集的部分工作（出版物）。为了本调查的目的，这些数据集被分为两组：分段数据集和连续/在线数据集。
- en: 2.1 Segmented Datasets
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 分段数据集
- en: By segmented datasets we refer to those datasets where samples correspond to
    a whole begin-end action/gestures, with one segment for one action. They are mainly
    used for classification purposes. The following are several segmented datasets
    commonly used for the evaluation of methods based on deep learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓分段数据集，是指样本对应于整个开始-结束动作/手势，每个分段代表一个动作。它们主要用于分类目的。以下是几个常用的分段数据集，用于评估基于深度学习的方法。
- en: 2.1.1 CMU Mocap
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 CMU Mocap
- en: CMU Graphics Lab Motion Capture Database (CMU Mocap) [[1](#bib.bib1)]([http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/))
    is one of the earliest source of data that covers a wide range of human actions,
    including interactions between two subjects, human locomotion, interactions with
    uneven terrain, sports, and other human actions. This dataset consists of RGB
    and skeleton modalities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: CMU Graphics Lab Motion Capture Database (CMU Mocap) [[1](#bib.bib1)]([http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/))
    是最早的数据源之一，涵盖了广泛的人类动作，包括两个主体之间的互动、人类运动、与不平坦地形的互动、体育运动及其他人类动作。该数据集包括RGB和骨架模态。
- en: 2.1.2 HDM05
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 HDM05
- en: Motion Capture Database HDM05 [[94](#bib.bib94)] ([http://resources.mpi-inf.mpg.de/HDM05/](http://resources.mpi-inf.mpg.de/HDM05/))
    was captured by an optical marker-based technology with the frequency of 120 Hz,
    which contains 2337 sequences for 130 actions performed by 5 non-professional
    actors, and 31 joints in each frame. Besides skeleton data, this dataset also
    provides RGB data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 动作捕捉数据库 HDM05 [[94](#bib.bib94)] ([http://resources.mpi-inf.mpg.de/HDM05/](http://resources.mpi-inf.mpg.de/HDM05/))
    是通过光学标记技术以120 Hz的频率捕捉的，包含了2337个序列，记录了5名非专业演员执行的130个动作，每帧包含31个关节。除了骨架数据，该数据集还提供了RGB数据。
- en: 2.1.3 MSR-Action3D
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 MSR-Action3D
- en: 'MSR-Action3D [[77](#bib.bib77)] ([http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets](http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets))
    is the first public benchmark RGB-D action dataset collected using Kinect ™sensor
    by Microsoft Research, Redmond and University of Wollongong in 2010. The dataset
    contains 20 actions: high arm wave, horizontal arm wave, hammer, hand catch, forward
    punch, high throw, draw x, draw tick, draw circle, hand clap, two hand wave, side-boxing,
    bend, forward kick, side kick, jogging, tennis serve, golf swing, pickup and throw.
    Ten subjects performed these actions three times. All the videos were recorded
    from a fixed point of view and the subjects were facing the camera while performing
    the actions. The background of the dataset was removed by some post-processing.
    Specifically, if an action needs to be performed with one arm or one leg, the
    actors were required to perform it using right arm or leg.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MSR-Action3D [[77](#bib.bib77)] ([http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets](http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets))
    是第一个由微软研究院、雷德蒙德和卧龙岗大学在2010年收集的公共基准RGB-D动作数据集。该数据集包含20个动作：高臂挥动、水平臂挥动、锤子、手抓、前拳、高抛、画X、画勾、画圈、拍手、双手挥动、侧击、弯腰、前踢、侧踢、慢跑、网球发球、高尔夫挥杆、捡起和抛掷。十名受试者重复执行这些动作三次。所有视频都从一个固定的视角录制，受试者在执行动作时面向摄像头。数据集的背景经过一些后处理移除。具体而言，如果一个动作需要用一只手或一条腿来完成，演员被要求使用右手或右腿进行表演。
- en: 2.1.4 MSRC-12
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 MSRC-12
- en: 'MSRC-12 dataset [[36](#bib.bib36)]([%****␣CVIU-arxiv.tex␣Line␣400␣****http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/](%****%20CVIU-arxiv.tex%20Line%20400%20****http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/))
    was collected by Microsoft Research Cambridge and University of Cambridge in 2012.
    The authors provided three familiar and easy to prepare instruction modalities
    and their combinations to the participants. The modalities are (1) descriptive
    text breaking down the performance kinematics, (2) an ordered series of static
    images of a person performing the gesture with arrows annotating as appropriate,
    and (3) video (dynamic images) of a person performing the gesture. There are 30
    participants in total and for each gesture, the data were collected as: Text (10
    people), Images (10 people), Video (10 people), Video with text (10 people), Images
    with text (10 people). The dataset was captured using one Kinect ™sensor and only
    the skeleton data are made available.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MSRC-12数据集 [[36](#bib.bib36)]([%****␣CVIU-arxiv.tex␣Line␣400␣****http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/](%****%20CVIU-arxiv.tex%20Line%20400%20****http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/))
    是由微软研究院剑桥分部和剑桥大学于2012年收集的。作者为参与者提供了三种熟悉且易于准备的指令模态及其组合。这些模态包括（1）分解动作运动学的描述性文本，（2）一个人执行手势的静态图像序列，并用箭头适当标注，以及（3）一个人执行手势的视频（动态图像）。共有30名参与者，对于每个手势，数据被收集为：文本（10人）、图像（10人）、视频（10人）、带文本的视频（10人）、带文本的图像（10人）。数据集使用一个Kinect™传感器捕捉，仅提供骨架数据。
- en: 2.1.5 MSRDailyActivity3D
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5 MSRDailyActivity3D
- en: MSRDailyActivity3D Dataset [[142](#bib.bib142)]([http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets](http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets))
    was collected by Microsoft and the Northwestern University in 2012 and focused
    on daily activities. The motivation was to cover human daily activities in the
    living room. The actions were performed by 10 actors while sitting on the sofa
    or standing close to the sofa. The camera was fixed in front of the sofa. In addition
    to depth data, skeleton data are also recorded, but the joint positions extracted
    by the tracker are very noisy due to the actors being either sitting on or standing
    close to the sofa.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: MSRDailyActivity3D 数据集 [[142](#bib.bib142)]([http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets](http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets))
    由微软和西北大学于2012年收集，专注于日常活动。其动机是涵盖客厅中的人类日常活动。10名演员在沙发上坐着或靠近沙发站立时执行动作。相机固定在沙发前方。除了深度数据外，还记录了骨架数据，但由于演员坐在沙发上或靠近沙发，跟踪器提取的关节位置非常嘈杂。
- en: 2.1.6 UTKinect
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.6 UTKinect
- en: UTKinect dataset [[162](#bib.bib162)]([http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html))
    was collected by the University of Texas at Austin in 2012\. Ten types of human
    actions were performed twice by 10 subjects. The subjects performed the actions
    from a variety of views. One challenge of the dataset is due to the actions being
    performed with high actor-dependent variability. Furthermore, human-object occlusions
    and body parts being out of the field of view have further increased the difficulty
    of the dataset. Ground truth in terms of action labels and segmentation of sequences
    are provided.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: UTKinect 数据集 [[162](#bib.bib162)]([http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html))
    由德克萨斯大学奥斯汀分校于2012年收集。10名受试者执行了10种类型的人类动作，每种动作重复执行两次。受试者从不同的视角执行这些动作。该数据集的一个挑战在于动作执行中存在较大的演员依赖性变异。此外，人类与物体的遮挡以及身体部位超出视野范围也进一步增加了数据集的难度。提供了关于动作标签和序列分割的真实数据。
- en: 2.1.7 G3D
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.7 G3D
- en: 'Gaming 3D dataset (G3D) [[7](#bib.bib7)]([http://dipersec.king.ac.uk/G3D/](http://dipersec.king.ac.uk/G3D/))
    captured by Kingston University in 2012 focuses on real-time action recognition
    in gaming scenario. It contains 10 subjects performing 20 gaming actions. Each
    subject performed these actions thrice. Two kinds of labels were provided as ground
    truth: the onset and offset of each action and the peak frame of each action.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Gaming 3D 数据集 (G3D) [[7](#bib.bib7)]([http://dipersec.king.ac.uk/G3D/](http://dipersec.king.ac.uk/G3D/))
    由金斯顿大学于2012年收集，专注于游戏场景中的实时动作识别。数据集包含10名受试者执行20种游戏动作。每名受试者执行这些动作三次。提供了两种标签作为真实数据：每个动作的开始和结束时间以及每个动作的峰值帧。
- en: 2.1.8 SBU Kinect Interaction Dataset
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.8 SBU Kinect Interaction Dataset
- en: 'SBU Kinect Interaction Dataset [[169](#bib.bib169)]([http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html](http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html)
    ) was collected by Stony Brook University in 2012\. It contains eight types of
    interactions. All videos were recorded with the same indoor background. Seven
    participants were involved in performing the activities which have interactions
    between two actors. The dataset is segmented into 21 sets and each set contains
    one or two sequences of each action category. Two kinds of ground truth information
    are provided: action labels of each segmented video and identification of “active”
    actor and “inactive” actor.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: SBU Kinect Interaction 数据集 [[169](#bib.bib169)]([http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html](http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html))
    由石溪大学于2012年收集。该数据集包含八种类型的互动。所有视频都在相同的室内背景下录制。七名参与者执行了两名演员之间的互动活动。数据集被分割成21组，每组包含一个或两个每个动作类别的序列。提供了两种真实数据：每个分段视频的动作标签和“主动”演员与“非主动”演员的识别。
- en: 2.1.9 Berkeley MHAD
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.9 Berkeley MHAD
- en: 'Berkeley Multimodal Human Action Database (Berkeley MHAD) [[96](#bib.bib96)]([http://tele-immersion.citris-uc.org/berkeley_mhad#dl](http://tele-immersion.citris-uc.org/berkeley_mhad#dl)),
    collected by University of California at Berkeley and Johns Hopkins University
    in 2013, was captured in five different modalities to expand the fields of application.
    The modalities are derived from: optical mocap system, four multi-view stereo
    vision cameras, two Microsoft Kinect v1 cameras, six wireless accelerometers and
    four microphones. Twelve subjects performed 11 actions, five times each. Three
    categories of actions are included: (1) actions with movement in full body parts,
    e.g., jumping in place, jumping jacks, throwing, etc., (2) actions with high dynamics
    in upper extremities, e.g., waving hands, clapping hands, etc. and (3) actions
    with high dynamics in lower extremities, e.g., sit down, stand up. The actions
    were executed with style and speed variations.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Berkeley Multimodal Human Action Database (Berkeley MHAD) [[96](http://tele-immersion.citris-uc.org/berkeley_mhad#dl)]，由加州大学伯克利分校和约翰斯·霍普金斯大学在2013年收集，使用五种不同的模式进行捕捉，以扩展应用领域。这些模式包括：光学动作捕捉系统、四台多视角立体视觉摄像头、两台微软Kinect
    v1摄像头、六个无线加速度计和四个麦克风。十二名受试者执行了11个动作，每个动作5次。包含三类动作：(1) 全身部位运动的动作，如原地跳跃、开合跳、投掷等，(2)
    上肢高动态动作，如挥手、拍手等，以及(3) 下肢高动态动作，如坐下、站立。动作执行时具有风格和速度的变化。
- en: 2.1.10 Northwestern-UCLA Multiview Action 3D
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.10 Northwestern-UCLA Multiview Action 3D
- en: Northwestern-UCLA Multiview Action 3D [[143](#bib.bib143)]([http://users.eecs.northwestern.edu/~jwa368/my_data.html](http://users.eecs.northwestern.edu/~jwa368/my_data.html))
    was collected by Northwestern University and University of California at Los Angles
    in 2014\. This dataset contains data taken from a variety of viewpoints. The actions
    were performed by 10 actors and captured by three simultaneous Kinect™v1 cameras.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Northwestern-UCLA Multiview Action 3D [[143](http://users.eecs.northwestern.edu/~jwa368/my_data.html)]是由西北大学和加州大学洛杉矶分校在2014年收集的。该数据集包含从各种视角获取的数据。动作由10名演员表演，并由三台同时工作的Kinect™v1摄像头捕捉。
- en: 2.1.11 ChaLearn LAP IsoGD
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.11 ChaLearn LAP IsoGD
- en: ChaLearn LAP IsoGD Dataset [[139](#bib.bib139)] ([http://www.cbsr.ia.ac.cn/users/jwan/database/isogd.html](http://www.cbsr.ia.ac.cn/users/jwan/database/isogd.html))
    is a large RGB-D dataset for segmented gesture recognition, and it was collected
    by Kinect v1 camera. It includes 47933 RGB-D depth sequences, each RGB-D video
    representing one gesture instance. There are 249 gestures performed by 21 different
    individuals. The dataset is divided into training, validation and test sets. All
    three sets consist of samples of different subjects to ensure that the gestures
    of one subject in the validation and test sets will not appear in the training
    set.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ChaLearn LAP IsoGD 数据集 [[139](http://www.cbsr.ia.ac.cn/users/jwan/database/isogd.html)]是一个用于分段手势识别的大型RGB-D数据集，由Kinect
    v1摄像头收集。它包括47933个RGB-D深度序列，每个RGB-D视频表示一个手势实例。数据集中包含21个不同个体执行的249个手势。数据集分为训练集、验证集和测试集。所有三部分集由不同受试者的样本组成，以确保验证集和测试集中的一个受试者的手势不会出现在训练集中。
- en: 2.1.12 NTU RGB+D
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.12 NTU RGB+D
- en: NTU RGB+D Dataset [[109](#bib.bib109)]([https://github.com/shahroudy/NTURGB-D](https://github.com/shahroudy/NTURGB-D))
    is currently the largest action recognition dataset in terms of the number of
    samples per action. The RGB-D data is captured by Kinect v2 cameras. The dataset
    has more than 56 thousand sequences and 4 million frames, containing 60 actions
    performed by 40 subjects aging between 10 and 35\. It consists of front view,
    two side views and left, right 45 degree views.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: NTU RGB+D 数据集 [[109](https://github.com/shahroudy/NTURGB-D)]目前是按每个动作样本数量计算的最大动作识别数据集。RGB-D数据是由Kinect
    v2摄像头捕捉的。数据集包含超过56,000个序列和400万帧，包含40名年龄在10到35岁之间的受试者执行的60个动作。数据集包括正面视图、两个侧面视图和左、右45度视图。
- en: 2.2 Continuous/Online Datasets
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 连续/在线数据集
- en: Continuous/online datasets refer to those datasets where each video sequence
    may contain one or more actions/gestures, and the segmented position between different
    motion classes are unknown. These datasets are mainly used for action detection,
    localization and online prediction. There are few datasets for this type.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 连续/在线数据集指的是那些每个视频序列可能包含一个或多个动作/手势，并且不同运动类别之间的分段位置是未知的。这些数据集主要用于动作检测、定位和在线预测。此类型的数据集较少。
- en: 2.2.1 ChaLearn2014 Multimodal Gesture Recognition
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 ChaLearn2014 多模态手势识别
- en: ChaLearn2014 Multimodal Gesture Recognition [[29](#bib.bib29)] ([http://gesture.chalearn.org/2014-looking-at-people-challenge](http://gesture.chalearn.org/2014-looking-at-people-challenge))
    is multi-modal dataset collected by Kinect v1 sensor, including RGB, depth, skeleton
    and audio modalities. In all sequences, a single user is recorded in front of
    the camera, performing natural communicative Italian gestures. The starting and
    ending frames for each gesture are annotated along with the gesture class label.
    It contains nearly 14K manually labeled (beginning and ending frame) gesture performances
    in continuous video sequences, with a vocabulary of 20 Italian gesture categories.
    There are 1, 720, 800 labeled frames across 13, 858 video fragments of about 1
    to 2 minutes sampled at 20 Hz. The gestures are performed by 27 different individuals
    under diverse conditions; these include varying clothes, positions, backgrounds
    and lighting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ChaLearn2014 多模态手势识别 [[29](#bib.bib29)] ([http://gesture.chalearn.org/2014-looking-at-people-challenge](http://gesture.chalearn.org/2014-looking-at-people-challenge))
    是一个由Kinect v1传感器收集的多模态数据集，包括RGB、深度、骨架和音频模态。在所有序列中，一个用户在摄像头前进行自然的意大利手势。每个手势的开始和结束帧都被标注，并附有手势类别标签。该数据集包含近14K手动标注的（开始和结束帧）手势表演，视频序列连续，词汇量为20种意大利手势类别。数据集共有1,720,800标注帧，跨13,858个约1到2分钟的视频片段，采样频率为20
    Hz。手势由27个不同个体在多种条件下执行，包括不同的衣物、位置、背景和光照。
- en: 'Table 1: Statistics of the public available benchmark datasets that are commonly
    used for evaluation with deep learning. Notation for the header: Seg: Segmented,
    Con:Continuous, D: Depth, S:Skeleton, Au:Audio, Ac:Accelerometer, IR:IR videos,
    #:number of, JI:Jaccard Index.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '表1：用于深度学习评估的公共基准数据集的统计信息。标题的符号说明：Seg: 分割，Con: 连续，D: 深度，S: 骨架，Au: 音频，Ac: 加速度计，IR:
    红外视频，#: 数量，JI: 杰卡德指数。'
- en: '| Dataset | year |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 |'
- en: '&#124; Acquisition &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 获取 &#124;'
- en: '&#124; device &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设备 &#124;'
- en: '| Seg/Con | Modality | #Class | #Subjects | #Samples | #Views | Metric |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 分割/连续 | 模态 | #类别 | #受试者 | #样本 | #视角 | 量度 |'
- en: '| CMU Mocap | 2001 | Mocap | Seg | RGB,S | 45 | 144 | 2,235 | 1 | Accuracy
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| CMU 动作捕捉 | 2001 | 动作捕捉 | 分割 | RGB,S | 45 | 144 | 2,235 | 1 | 准确度 |'
- en: '| HDM05 | 2007 | Mocap | Seg | RGB,S | 130 | 5 | 2337 | 1 | Accuracy |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| HDM05 | 2007 | 动作捕捉 | 分割 | RGB,S | 130 | 5 | 2337 | 1 | 准确度 |'
- en: '| MSR-Action3D | 2010 | Kinect v1 | Seg | S,D | 20 | 10 | 567 | 1 | Accuracy
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| MSR-Action3D | 2010 | Kinect v1 | 分割 | S,D | 20 | 10 | 567 | 1 | 准确度 |'
- en: '| MSRC-12 | 2012 | Kinect v1 | Seg | S | 12 | 30 | 594 | 1 | Accuracy |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| MSRC-12 | 2012 | Kinect v1 | 分割 | S | 12 | 30 | 594 | 1 | 准确度 |'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MSR &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSR &#124;'
- en: '&#124; DailyActivity3D &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DailyActivity3D &#124;'
- en: '| 2012 | Kinect v1 | Seg | RGB,D,S | 16 | 10 | 320 | 1 | Accuracy |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | Kinect v1 | 分割 | RGB,D,S | 16 | 10 | 320 | 1 | 准确度 |'
- en: '| UTKinect | 2012 | Kinect v1 | Seg | RGB,D,S | 10 | 10 | 200 | 1 | Accuracy
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| UTKinect | 2012 | Kinect v1 | 分割 | RGB,D,S | 10 | 10 | 200 | 1 | 准确度 |'
- en: '| G3D | 2012 | Kinect v1 | Seg | RGB,D,S | 5 | 5 | 200 | 1 | Accuracy |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| G3D | 2012 | Kinect v1 | 分割 | RGB,D,S | 5 | 5 | 200 | 1 | 准确度 |'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SBU Kinect &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SBU Kinect &#124;'
- en: '&#124; Interaction &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互动 &#124;'
- en: '| 2012 | Kinect v1 | Seg | RGB,D,S | 7 | 8 | 300 | 1 | Accuracy |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 2012 | Kinect v1 | 分割 | RGB,D,S | 7 | 8 | 300 | 1 | 准确度 |'
- en: '| Berkeley MHAD | 2013 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 伯克利MHAD | 2013 |'
- en: '&#124; Mocap &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 动作捕捉 &#124;'
- en: '&#124; Kinect v1 &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Kinect v1 &#124;'
- en: '| Seg | RGB,D,S,Au,Ac | 12 | 12 | 660 | 4 | Accuracy |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 分割 | RGB,D,S,Au,Ac | 12 | 12 | 660 | 4 | 准确度 |'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multiview &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多视角 &#124;'
- en: '&#124; Action3D &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Action3D &#124;'
- en: '| 2014 | Kinect v1 | Seg | RGB,D,S | 10 | 10 | 1475 | 3 | Accuracy |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | Kinect v1 | 分割 | RGB,D,S | 10 | 10 | 1475 | 3 | 准确度 |'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ChaLearn LAP &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ChaLearn LAP &#124;'
- en: '&#124; IsoGD &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IsoGD &#124;'
- en: '| 2016 | Kinect v1 | Seg | RGB,D | 249 | 21 | 47933 | 1 | Accuracy |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Kinect v1 | 分割 | RGB,D | 249 | 21 | 47933 | 1 | 准确度 |'
- en: '| NTU RGB+D | 2016 | Kinect v2 | Seg | RGB,D,S,IR | 60 | 40 | 56880 | 80 |
    Accuracy |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| NTU RGB+D | 2016 | Kinect v2 | 分割 | RGB,D,S,IR | 60 | 40 | 56880 | 80 | 准确度
    |'
- en: '| ChaLearn2014 | 2014 | Kinect v1 | Con | RGB,D,S,Au | 20 | 27 | 13858 | 1
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ChaLearn2014 | 2014 | Kinect v1 | 连续 | RGB,D,S,Au | 20 | 27 | 13858 | 1 |'
- en: '&#124; Accuracy &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确度 &#124;'
- en: '&#124; JI etc. &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; JI等 &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ChaLearn LAP &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ChaLearn LAP &#124;'
- en: '&#124; ConGD &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ConGD &#124;'
- en: '| 2016 | Kinect v1 | Con | RGB,D | 249 | 21 | 22535 | 1 | JI |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | Kinect v1 | 连续 | RGB,D | 249 | 21 | 22535 | 1 | JI |'
- en: '| PKU-MMD | 2017 | Kinect v2 | Con | RGB,D,S,IR | 51 | 66 | 1076 | 3 | JI etc.
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| PKU-MMD | 2017 | Kinect v2 | 连续 | RGB,D,S,IR | 51 | 66 | 1076 | 3 | JI等 |'
- en: 2.2.2 ChaLearn LAP ConGD
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 ChaLearn LAP ConGD
- en: The ChaLearn LAP ConGD Dataset [[139](#bib.bib139)] ([http://www.cbsr.ia.ac.cn/users/jwan/database/congd.html](http://www.cbsr.ia.ac.cn/users/jwan/database/congd.html))
    is a large RGB-D dataset for continuous gesture recognition. It was collected
    by Kinect v1 sensor and includes 47933 RGB-D gesture instances in 22535 RGB-D
    gesture videos. Each RGB-D video may contain one or more gestures. There are 249
    gestures performed by 21 different individuals. The dataset is divided into training,
    validation and test sets. All three sets consist of samples of different subjects
    to ensure that the gestures of one subject in the validation and test sets will
    not appear in the training set.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ChaLearn LAP ConGD 数据集 [[139](#bib.bib139)] ([http://www.cbsr.ia.ac.cn/users/jwan/database/congd.html](http://www.cbsr.ia.ac.cn/users/jwan/database/congd.html))
    是一个用于连续手势识别的大型 RGB-D 数据集。该数据集由 Kinect v1 传感器收集，包含 47933 个 RGB-D 手势实例，分布在 22535
    个 RGB-D 手势视频中。每个 RGB-D 视频可能包含一个或多个手势，共有 249 种手势，由 21 个不同的个体执行。数据集分为训练集、验证集和测试集。三个数据集中的样本均来自不同的受试者，以确保验证集和测试集中的某一受试者的手势不会出现在训练集中。
- en: 2.2.3 PKU-MMD
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 PKU-MMD
- en: PKU-MMD [[19](#bib.bib19)] ([http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html))
    is a large scale dataset for continuous multi-modality 3D human action understanding
    and covers a wide range of complex human activities with well annotated information.
    It was captured via the Kinect v2 sensor. PKU-MMD contains 1076 long video sequences
    in 51 action categories, performed by 66 subjects in three camera views. It contains
    almost 20,000 action instances and 5.4 million frames in total. It provides multi-modality
    data sources, including RGB, depth, Infrared Radiation and Skeleton.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PKU-MMD [[19](#bib.bib19)] ([http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html))
    是一个大规模的连续多模态 3D 人体动作理解数据集，涵盖了广泛的复杂人类活动，并附有详细的注释信息。该数据集通过 Kinect v2 传感器捕获，包含 1076
    个长视频序列，分为 51 个动作类别，由 66 名受试者在三个摄像机视角下执行。它包含近 20,000 个动作实例和总计 540 万帧。数据集提供了多模态数据源，包括
    RGB、深度、红外辐射和骨架。
- en: 'Table [2](#S2 "2 Benchmark Datasets ‣ RGB-D-based Human Motion Recognition
    with Deep Learning: A Survey") shows the statistics of publicly available benchmark
    datasets that are commonly used for evaluation of deep learning-based algorithms.
    It can be seen that the surveyed datasets cover a wide range of different types
    of actions including gestures, simple actions, daily activities, human-object
    interactions, human-human interactions. It also covers both segmented and continuous/online
    datasets, with different acquisition devices, modalities, and views. Sample images
    from different datasets are shown in Fig. [2](#S2.F2 "Figure 2 ‣ 2.2.3 PKU-MMD
    ‣ 2.2 Continuous/Online Datasets ‣ 2 Benchmark Datasets ‣ RGB-D-based Human Motion
    Recognition with Deep Learning: A Survey").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S2 "2 基准数据集 ‣ 基于 RGB-D 的深度学习人类运动识别综述") 显示了常用于深度学习算法评估的公开基准数据集的统计数据。可以看到，调查的数据集涵盖了多种不同类型的动作，包括手势、简单动作、日常活动、人机交互和人际交互。它还涵盖了分段和连续/在线数据集，使用不同的采集设备、模态和视角。来自不同数据集的示例图像见图
    [2](#S2.F2 "图 2 ‣ 2.2.3 PKU-MMD ‣ 2.2 连续/在线数据集 ‣ 2 基准数据集 ‣ 基于 RGB-D 的深度学习人类运动识别综述")。
- en: '![Refer to caption](img/88f9e23c47dc1a73a814d5b7b37667a3.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/88f9e23c47dc1a73a814d5b7b37667a3.png)'
- en: 'Figure 2: Sample images from different datasets.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：来自不同数据集的示例图像。
- en: In this section, we introduce the deep learning concepts and architectures that
    are relevant or have been applied to RGB-D-based motion recognition. Readers who
    are interested in more background and techniques are referred to the book by [[41](#bib.bib41)].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了与基于 RGB-D 的运动识别相关或已应用的深度学习概念和架构。对于更详细的背景和技术，感兴趣的读者可以参考 [[41](#bib.bib41)]
    的书籍。
- en: 3 RGB-based Motion Recognition with Deep Learning
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 RGB 的深度学习运动识别
- en: RGB is one important channel of RGB-D data with characteristics including shape,
    color and texture that bear rich features. These properties also make it effective
    to directly use networks, such as 2D CNNs [[68](#bib.bib68), [117](#bib.bib117),
    [47](#bib.bib47)], to extract frame-level features. Even though most of the surveyed
    methods for this section are not adapted to RGB-D-based datasets, we argue that
    the following methods could be directly adapted to RGB modality of RGB-D datasets.
    We define three categories namely, CNN-based, RNN-based and other-architecture-based
    approaches for segmented motion recognition; the first two categories are for
    continuous/online motion recognition.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: RGB 是 RGB-D 数据中的一个重要通道，其特性包括形状、颜色和纹理，包含丰富的特征。这些特性也使得直接使用网络（如 2D CNNs [[68](#bib.bib68),
    [117](#bib.bib117), [47](#bib.bib47)]）提取帧级特征变得有效。尽管本节中大多数调查的方法并未针对 RGB-D 数据集进行调整，但我们认为以下方法可以直接适应
    RGB-D 数据集的 RGB 模态。我们定义了三类方法，即基于 CNN、基于 RNN 和其他架构的分段运动识别方法；前两类方法用于连续/在线运动识别。
- en: 3.1 Segmented Motion Recognition
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 分段运动识别
- en: 3.1.1 CNN-based Approach
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 基于 CNN 的方法
- en: 'For this group of methods, currently there are mainly four approaches to encode
    spatial-temporal-structural information. The first approach applies CNN to extract
    features from individual frames and later, fuse the temporal information. For
    example [[64](#bib.bib64)] investigated four temporal fusion methods, and proposed
    the concept of slow fusion where higher layers get access to progressively more
    global information in both spatial and temporal dimensions (see Fig. [4](#S3.F4
    "Figure 4 ‣ 3.1.1 CNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")). Furthermore, several temporal pooling methods have
    been explored and the suggestion is that max pooling in the temporal domain is
    preferable [[95](#bib.bib95)].'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一组方法，目前主要有四种编码空间-时间-结构信息的方法。第一种方法应用 CNN 从单个帧中提取特征，然后融合时间信息。例如 [[64](#bib.bib64)]
    研究了四种时间融合方法，并提出了慢融合的概念，其中较高的层逐步获取更多的空间和时间维度的全局信息（见图 [4](#S3.F4 "图 4 ‣ 3.1.1 基于
    CNN 的方法 ‣ 3.1 分段运动识别 ‣ 3 基于 RGB 的深度学习运动识别 ‣ 基于 RGB-D 的深度学习人体运动识别：综述")）。此外，还探讨了几种时间池化方法，建议在时间域中使用最大池化较为可取 [[95](#bib.bib95)]。
- en: '![Refer to caption](img/2dc2c82a891e1c15171c023c9aaa5cf1.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2dc2c82a891e1c15171c023c9aaa5cf1.png)'
- en: 'Figure 3: C3D net has 8 convolutions, 5 max-poolings, and 2 fully connected
    layers, followed by a softmax output layer. All 3D convolution kernels are 3$\times$3$\times$3
    with stride 1 in both spatial and temporal dimensions. Number of filters are indicated
    in each box. The 3D pooling layers are as indicated from pool1 to pool5. All pooling
    kernels are 2$\times$2$\times$2, except for pool1 which is 1$\times$2$\times$2.
    Each fully connected layer has 4096 output units. Figure from [[130](#bib.bib130)].'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：C3D 网络包含 8 个卷积层、5 个最大池化层和 2 个全连接层，最后是一个 softmax 输出层。所有 3D 卷积核都是 3$\times$3$\times$3，在空间和时间维度上步幅均为
    1。每个框中标示了滤波器的数量。3D 池化层如从 pool1 到 pool5 所示。所有池化核为 2$\times$2$\times$2，除了 pool1
    为 1$\times$2$\times$2。每个全连接层有 4096 个输出单元。图源 [[130](#bib.bib130)]。
- en: '![Refer to caption](img/67817e269a71caf68c8c5f976610b3ee.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/67817e269a71caf68c8c5f976610b3ee.png)'
- en: 'Figure 4: Different approaches for fusing information over temporal dimension
    through the network. Red, green and blue boxes indicate convolutional, normalization
    and pooling layers respectively. In the Slow Fusion model, the depicted columns
    share parameters. Figure from [[64](#bib.bib64)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：通过网络在时间维度上融合信息的不同方法。红色、绿色和蓝色框分别表示卷积层、归一化层和池化层。在 Slow Fusion 模型中，所示的列共享参数。图源 [[64](#bib.bib64)]。
- en: 'The second approach extends convolutional operation into temporal domain. In
    one such implementation, Ji et al. [[61](#bib.bib61)] proposed 3D-convolutional
    networks using 3D kernels (filters extended along the time axis) to extract features
    from both spatial and temporal dimensions. This work empirically showed that the
    3D-convolutional networks outperform their 2D frame-based counterparts. With modern
    deep architectures, such as VGG [[117](#bib.bib117)], and large-scale supervised
    training datasets, such as Sports-1M [[64](#bib.bib64)], Tran et al [[130](#bib.bib130)]
    extended the work presented in [[61](#bib.bib61)] by including 3D pooling layers,
    and proposed a generic descriptor named C3D by averaging the outputs of the first
    fully connected layer of the networks (see Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 CNN-based
    Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion Recognition with
    Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")).
    However, both of these works break the video sequence into short clips and aggregate
    video-level information by late score fusion. This is likely to be suboptimal
    when considering some long action sequence, such as walking or swimming that lasts
    several seconds and spans tens or hundreds of video frames. To handle this problem,
    Varol et al. [[133](#bib.bib133)] investigated the learning of long-term video
    representations and proposed the Long-term Temporal Convolutions (LTC) at the
    expense of decreasing spatial resolution to keep the complexity of networks tractable.
    Despite the fact that this is straightforward and mainstream, extending spatial
    kernels to 3D spatio-temporal derivative inevitably increases the number of parameters
    of the network. To relieve the drawbacks of 3D kernels, Sun et al. [[125](#bib.bib125)]
    factorized a 3D filter into a combination of 2D and 1D filters.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '第二种方法将卷积操作扩展到时间域。在一种实现中，Ji 等人[[61](#bib.bib61)] 提出了使用沿时间轴扩展的 3D 核（滤波器）来提取空间和时间维度的特征。这项工作通过实验证明，3D
    卷积网络优于其 2D 帧基的对应物。结合现代深度架构，如 VGG[[117](#bib.bib117)]，以及大规模监督训练数据集，如 Sports-1M[[64](#bib.bib64)]，Tran
    等人[[130](#bib.bib130)] 在[[61](#bib.bib61)]的基础上扩展了工作，加入了 3D 池化层，并提出了一个名为 C3D 的通用描述符，通过平均网络的第一个全连接层的输出（见图[3](#S3.F3
    "Figure 3 ‣ 3.1.1 CNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")）。然而，这些工作都将视频序列分解为短片段，并通过后期得分融合来聚合视频级信息。在考虑到一些长时间动作序列时，比如持续几秒钟的行走或游泳，这种方法可能是次优的。为了处理这个问题，Varol
    等人[[133](#bib.bib133)] 研究了长期视频表示的学习，并提出了长时间卷积（LTC），以降低空间分辨率，从而保持网络复杂度的可处理性。尽管这种方法直接且主流，但将空间核扩展到
    3D 时空导数不可避免地增加了网络的参数数量。为了缓解 3D 核的缺点，Sun 等人[[125](#bib.bib125)] 将 3D 滤波器分解为 2D
    和 1D 滤波器的组合。'
- en: 'The third approach is to encode the video into dynamic images that contain
    the spatio-temporal information and then apply CNN for image-based recognition.
    Bilen et al. [[6](#bib.bib6)] proposed to adopt rank pooling [[34](#bib.bib34)]
    to encode the video into one dynamic set of images and used pre-trained models
    over ImageNet [[68](#bib.bib68)] for fine-tuning (see Figure. [6](#S3.F6 "Figure
    6 ‣ 3.1.1 CNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")). The end-to-end learning methods with rank pooling
    has also been proposed in [[6](#bib.bib6), [35](#bib.bib35)]. Hierarchical rank
    pooling [[33](#bib.bib33)] is proposed to learn higher order and non-linear representations
    compared to the original work. Generalized rank pooling [[17](#bib.bib17)] is
    introduced to improve the original method via a quadratic ranking function which
    jointly provides a low-rank approximation to the input data and preserves their
    temporal order in a subspace.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '第三种方法是将视频编码为包含时空信息的动态图像，然后应用 CNN 进行基于图像的识别。Bilen 等人[[6](#bib.bib6)] 提出了采用排名池化[[34](#bib.bib34)]
    将视频编码为一组动态图像，并使用在 ImageNet[[68](#bib.bib68)] 上预训练的模型进行微调（见图[6](#S3.F6 "Figure
    6 ‣ 3.1.1 CNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")）。采用排名池化的端到端学习方法也在[[6](#bib.bib6), [35](#bib.bib35)]中提出。与原始工作相比，分层排名池化[[33](#bib.bib33)]
    被提出用于学习更高阶和非线性的表示。广义排名池化[[17](#bib.bib17)] 被引入以通过二次排名函数改进原始方法，该函数同时提供输入数据的低秩近似并保留其在子空间中的时间顺序。'
- en: '![Refer to caption](img/52d2121334e65c1eeb4cc54f349a75ac.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/52d2121334e65c1eeb4cc54f349a75ac.png)'
- en: 'Figure 5: Two-stream architecture for RGB-based motion recognition. Figure
    from [[116](#bib.bib116)].'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于RGB的运动识别的双流架构。图源自 [[116](#bib.bib116)]。
- en: '![Refer to caption](img/b9374e1639e6afae68b3f24b9156a8cc.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9374e1639e6afae68b3f24b9156a8cc.png)'
- en: 'Figure 6: Rank pooling encodes the RGB video into one dynamic image and CNN
    is adopted for feature extraction and classification. Figure from [[6](#bib.bib6)].'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：排名池化将RGB视频编码成一个动态图像，并采用CNN进行特征提取和分类。图源自 [[6](#bib.bib6)]。
- en: 'Besides the above works that aim to adopt one network to exploit both spatio-temporal
    information contained in the video, the fourth approach separates the two factors
    and adopt multiple stream networks. Simonyan et al. [[116](#bib.bib116)] proposed
    one spatial stream network fed with raw video frames, and one temporal stream
    network accepting optical flow fields as input. The two streams are fused together
    using the softmax scores (see Figure. [5](#S3.F5 "Figure 5 ‣ 3.1.1 CNN-based Approach
    ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion Recognition with Deep
    Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")
    for the two-stream architecture). Wang et al. [[145](#bib.bib145)] extended the
    two-stream networks by integrating improved trajectories [[141](#bib.bib141)],
    where trajectory-constrained sampling and pooling are used to encode deep features
    learned from deep CNN architecture, into effective descriptors. To incorporate
    long-range temporal structure using the two-stream networks, Wang et al. [[147](#bib.bib147)]
    devised a temporal segment network (TSN) that uses a sparse sampling scheme to
    extract short snippets over a long video sequence. With the removal of redundancy
    from consecutive frames and a segmental structure, aggregated information is obtained
    from the sampled snippets. To reduce the expensive calculation of optical flow,
    Zhang et al. [[170](#bib.bib170)] accelerated this two stream structure by replacing
    optical flow with motion vector which can be obtained directly from compressed
    videos without extra calculation. Wang et al. [[158](#bib.bib158)] leveraged semantic
    cues in video by using a two-stream semantic region-based CNNs (SR-CNNs) to incorporate
    human/object detection results into the framework. In their work, Chéron et al. [[18](#bib.bib18)]
    exploit spatial structure of the human pose and extract a pose-based convolutional
    neural network (P-CNN) feature from both RGB frames and optical flow for fine-grained
    action recognition. The work presented in [[157](#bib.bib157)] formulated the
    problem of action recognition from a new perspective and model an action as a
    transformation which changes the state of the environment before the action to
    the state after the action. They designed a Siamese network which models the action
    as a transformation on a high-level feature space based on the two-stream model.
    Based on the two-stream framework, [[180](#bib.bib180)] proposed a key volume
    mining deep framework for action recognition, where they identified key volumes
    and conducted classification simultaneously. Inspired by the success of Residual
    Networks (ResNets) [[47](#bib.bib47)], Feichtenhofer et al. [[31](#bib.bib31)]
    injected residual connections between the two streams to allow spatial-temporal
    interaction between them. Instead of using optical flow for temporal stream, [[72](#bib.bib72)]
    adopted Motion History Image (MHI) [[8](#bib.bib8)] as the motion clue. The MHI
    was combined with RGB frames in a spatio-temporal CNN for fined grained action
    recognition. However, all the methods reviewed above incorporated the two streams
    from separate training regimes; any registration of the two streams was neglected.
    In order to address this gap and propose a new architecture for spatial-temporal
    fusion of the two streams Feichtenhofer et al. [[32](#bib.bib32)] investigated
    three aspects of fusion for the two streams: (i) how to fuse the two networks
    with consideration for spatial registration, (ii) where to fuse the two networks
    and, (iii) how to fuse the networks temporally. One of their conclusions was that
    the results suggest the importance of learning correspondences between highly
    abstract ConvNet features both spatially and temporally.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '除了上述旨在采用单一网络来利用视频中包含的时空信息的工作，第四种方法将这两个因素分开，并采用多个流网络。Simonyan 等人[[116](#bib.bib116)]
    提出了一个空间流网络，该网络以原始视频帧作为输入，还有一个时间流网络，该网络接受光流场作为输入。这两个流通过 softmax 分数融合在一起（见图。[5](#S3.F5
    "Figure 5 ‣ 3.1.1 CNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey") 以实现双流架构）。Wang 等人[[145](#bib.bib145)] 通过将改进的轨迹[[141](#bib.bib141)]
    集成到双流网络中扩展了该网络，其中使用轨迹约束采样和池化来将从深度 CNN 架构中学习到的深度特征编码为有效描述符。为了使用双流网络整合长范围时间结构，Wang
    等人[[147](#bib.bib147)] 设计了一个时间段网络（TSN），该网络使用稀疏采样方案从长视频序列中提取短片段。通过去除连续帧中的冗余和段落结构，从采样的片段中获得聚合信息。为了减少光流的昂贵计算，Zhang
    等人[[170](#bib.bib170)] 通过用可以直接从压缩视频中获得的运动矢量替代光流来加速这一双流结构，从而避免了额外的计算。Wang 等人[[158](#bib.bib158)]
    通过使用双流语义区域基础 CNNs（SR-CNNs）利用视频中的语义线索，将人类/物体检测结果融入框架。在他们的工作中，Chéron 等人[[18](#bib.bib18)]
    利用人体姿态的空间结构，并从 RGB 帧和光流中提取基于姿态的卷积神经网络（P-CNN）特征，以进行细粒度的动作识别。[[157](#bib.bib157)]
    中提出的工作从新视角构建了动作识别问题，并将动作建模为一种转变，即在动作之前环境的状态变化为动作后的状态。他们设计了一个 Siamese 网络，该网络基于双流模型在高级特征空间上建模动作的转变。基于双流框架，[[180](#bib.bib180)]
    提出了一个关键体积挖掘深度框架用于动作识别，在其中识别关键体积并同时进行分类。受到残差网络（ResNets）[[47](#bib.bib47)] 成功的启发，Feichtenhofer
    等人[[31](#bib.bib31)] 在两个流之间注入了残差连接，以允许它们之间的时空交互。[[72](#bib.bib72)] 采用了运动历史图像（MHI）[[8](#bib.bib8)]
    作为运动线索，替代了时间流中的光流。MHI 与 RGB 帧结合在一个时空 CNN 中用于细粒度动作识别。然而，以上所有回顾的方法都将两个流合并于不同的训练阶段；对两个流的任何配准都被忽视了。为了填补这一空白并提出一种新的空间-时间融合架构，Feichtenhofer
    等人[[32](#bib.bib32)] 调查了双流融合的三个方面：（i）如何融合两个网络以考虑空间配准，（ii）在哪里融合这两个网络，以及（iii）如何在时间上融合这两个网络。他们的结论之一是结果表明学习高度抽象的
    ConvNet 特征之间的空间和时间对应的重要性。'
- en: 3.1.2 RNN-based Approach
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 基于 RNN 的方法
- en: 'For RNN-based approach, Baccouche et al. [[5](#bib.bib5)] tackled the problem
    of action recognition through a cascade of 3D CNN and LSTM, in which the two networks
    were trained separately. Differently from the separate training, Donahue et al. [[22](#bib.bib22)]
    proposed one Long-term Recurrent Convolutional Network (LRCN) to exploit end-to-end
    training of the two networks( see illustration in Figure. [7](#S3.F7 "Figure 7
    ‣ 3.1.2 RNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion
    Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep
    Learning: A Survey")). To take full advantage of both CNN and RNN, Ng et al. [[95](#bib.bib95)]
    aggregated CNN features with both temporal pooling and LSTM for temporal exploitation,
    and fused the output scores from the feature pooling and LSTM network to conduct
    final action recognition. Pigou et al. [[98](#bib.bib98)] proposed an end-to-end
    trainable neural network architecture incorporating temporal convolutions and
    bidirectional LSTM for gesture recognition. This provided opportunity to mine
    temporal information that is much discriminative for gesture recognition. In their
    work, Sharma et al. [[111](#bib.bib111)] proposed a soft attention model for action
    recognition based on LSTM (see Figure. [8](#S3.F8 "Figure 8 ‣ 3.1.2 RNN-based
    Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion Recognition with
    Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")).
    The attention model learns the parts in the frames that are relevant for the task
    at hand and attaches higher importance to them. Previous attention-based methods
    have only utilized video-level category as supervision to train LSTM. This strategy
    may lack a detailed and dynamical guidance and consequently restrict their capacity
    for modelling complex motions in videos. Du et al. [[23](#bib.bib23)] address
    this problem by proposing a recurrent pose-attention network (RPAN) for action
    recognition in videos, which can adaptively learn a highly discriminative pose-related
    feature for every-step action prediction of LSTM. To take advantage of both Fisher
    Vector [[108](#bib.bib108)] and RNN, Lev et al. [[74](#bib.bib74)] introduced
    a Recurrent Neural Network Fisher Vector (RNN-FV) where the GMM probabilistic
    model in the fisher vector is replaced by a RNN and thus avoids the need for the
    assumptions of data distribution in the GMM. Even though RNN is remarkably capable
    of modeling temporal dependences, it lacks an intuitive high-level spatial-temporal
    structure. The spatio-temporal-structural information has been mined by Jain et
    al. [[59](#bib.bib59)] through a combination of the powers of spatio-temporal
    graphs and RNN for action recognition. Recently, Sun et al. [[124](#bib.bib124)]
    proposed a Lattice-LSTM (L2STM) network, which extends LSTM by learning independent
    hidden state transitions of memory cells for individual spatial locations. This
    method effectively enhances the ability to model dynamics across time and addresses
    the non-stationary issue of long-term motion dynamics without significantly increasing
    the model complexity. Differently from previous methods that using only feedforward
    connections, Shi et al. [[112](#bib.bib112)] proposed a biologically-inspired
    deep network, called ShuttleNet1\. Unlike traditional RNNs, all processors inside
    ShuttleNet are connected in a loop to mimic the human brain’s feedforward and
    feedback connections. In this manner, the processors are shared across multiple
    pathways in the loop connection. Attention mechanism is then employed to select
    the best information flow pathway.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '对于基于 RNN 的方法，Baccouche 等人 [[5](#bib.bib5)] 通过级联的 3D CNN 和 LSTM 解决了动作识别的问题，其中这两个网络是分别训练的。与单独训练不同，Donahue
    等人 [[22](#bib.bib22)] 提出了一个长短期递归卷积网络（LRCN），以利用这两个网络的端到端训练（参见图示 [7](#S3.F7 "Figure
    7 ‣ 3.1.2 RNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")）。为了充分利用 CNN 和 RNN，Ng 等人 [[95](#bib.bib95)] 聚合了 CNN 特征，并结合了时间池化和
    LSTM 进行时间利用，同时融合了特征池化和 LSTM 网络的输出得分以进行最终的动作识别。Pigou 等人 [[98](#bib.bib98)] 提出了一个端到端可训练的神经网络架构，该架构结合了时间卷积和双向
    LSTM 用于手势识别。这提供了挖掘时间信息的机会，这对于手势识别具有很大的区分性。在他们的工作中，Sharma 等人 [[111](#bib.bib111)]
    提出了一个基于 LSTM 的软注意力模型用于动作识别（见图 [8](#S3.F8 "Figure 8 ‣ 3.1.2 RNN-based Approach
    ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion Recognition with Deep
    Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")）。该注意力模型学习帧中与当前任务相关的部分，并赋予它们更高的权重。以往基于注意力的方法仅利用视频级类别作为监督来训练
    LSTM。这种策略可能缺乏详细和动态的指导，从而限制了其在视频中建模复杂动作的能力。Du 等人 [[23](#bib.bib23)] 通过提出一个递归姿态注意力网络（RPAN）来解决这个问题，用于视频中的动作识别，该网络可以自适应地学习每一步动作预测的高度区分性姿态相关特征。为了利用
    Fisher Vector [[108](#bib.bib108)] 和 RNN，Lev 等人 [[74](#bib.bib74)] 引入了一个递归神经网络
    Fisher Vector（RNN-FV），在这里 Fisher Vector 中的 GMM 概率模型被 RNN 替代，从而避免了对 GMM 数据分布的假设。尽管
    RNN 在建模时间依赖性方面表现出色，但它缺乏直观的高层次时空结构。Jain 等人 [[59](#bib.bib59)] 通过结合时空图和 RNN 的优势挖掘了时空结构信息用于动作识别。最近，Sun
    等人 [[124](#bib.bib124)] 提出了一个 Lattice-LSTM (L2STM) 网络，该网络通过学习独立的记忆单元隐状态转移来扩展 LSTM，以处理个体空间位置。该方法有效地增强了跨时间建模动态的能力，并解决了长期运动动态的非平稳问题，而不显著增加模型复杂性。与之前仅使用前馈连接的方法不同，Shi
    等人 [[112](#bib.bib112)] 提出了一个生物学启发的深度网络，称为 ShuttleNet1。与传统的 RNN 不同，ShuttleNet
    中的所有处理器都以循环方式连接，以模拟人脑的前馈和反馈连接。通过这种方式，处理器在循环连接中共享多个路径。然后使用注意力机制来选择最佳的信息流路径。'
- en: '![Refer to caption](img/f4d1a980d6e11dd10559a72a5b0750e6.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f4d1a980d6e11dd10559a72a5b0750e6.png)'
- en: 'Figure 7: LRCN processes the variable-length visual input with a CNN, whose
    outputs are fed into a stack of recurrent sequence models. The output is a variable-length
    prediction. Figure from [[22](#bib.bib22)].'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：LRCN 使用 CNN 处理可变长度的视觉输入，其输出被输入到一组递归序列模型中。输出是可变长度的预测。图来自 [[22](#bib.bib22)]。
- en: '![Refer to caption](img/6098f5de806d60b51842c79223ca8741.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6098f5de806d60b51842c79223ca8741.png)'
- en: 'Figure 8: The CNN takes the video frames as its input and produces a feature
    tube. The model computes the current input $\boldsymbol{x}_{t}$ as an average
    of the feature slices weighted according to the location softmax $\boldsymbol{I}_{t}$.
    At each time-step $t$, the recurrent network takes a feature slice $\boldsymbol{x}_{t}$
    as input. It then propagates $\boldsymbol{x}_{t}$ through three layers of LSTMs
    and predicts the next location probabilities $\boldsymbol{I}_{t+1}$ and the class
    label $\boldsymbol{y}_{t}$. Figure from [[111](#bib.bib111)].'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：CNN 以视频帧作为输入，并生成特征管道。模型将当前输入 $\boldsymbol{x}_{t}$ 计算为按位置 softmax $\boldsymbol{I}_{t}$
    权重加权的特征切片的平均值。在每个时间步 $t$，递归网络以特征切片 $\boldsymbol{x}_{t}$ 作为输入。然后，它通过三层 LSTM 传播
    $\boldsymbol{x}_{t}$ 并预测下一个位置概率 $\boldsymbol{I}_{t+1}$ 和类别标签 $\boldsymbol{y}_{t}$。图来自 [[111](#bib.bib111)]。
- en: '![Refer to caption](img/12443cb4f930c439be39bcb3896dad5f.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/12443cb4f930c439be39bcb3896dad5f.png)'
- en: 'Figure 9: The Deep Action Proposals algorithm can localize segments of varied
    duration around actions occurring along a video without exhaustively exploring
    multiple temporal scales. Figure from [[30](#bib.bib30)].'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：深度动作提议算法可以在视频中定位不同持续时间的片段，而无需对多个时间尺度进行详尽的探索。图来自 [[30](#bib.bib30)]。
- en: '![Refer to caption](img/12619eba7925a48a94ce9508aea4408e.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/12619eba7925a48a94ce9508aea4408e.png)'
- en: 'Figure 10: The LSTM autoencoder model and LSTM future predictor model. Figure
    from [[122](#bib.bib122)].'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：LSTM 自编码器模型和 LSTM 未来预测模型。图来自 [[122](#bib.bib122)]。
- en: 3.1.3 Other-architecture-based Approach
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 基于其他架构的方法
- en: 'Besides the commonly used CNN- and RNN-based methods for motion recognition
    from RGB modality, there are several other structures that have been adopted for
    this task. Jhuang et al. [[60](#bib.bib60)] used a feedforward hierarchical template
    matching architecture for action recognition with pre-defined spatio-temporal
    filters in the first layer. In his thesis, Chen [[13](#bib.bib13)] adopted the
    convolutional RBM (CRBM) as the basic processing unit and proposed the so-called
    space-time Deep Belief Network (ST-DBN) that alternates the aggregation of spatial
    and temporal information so that higher layers capture longer range statistical
    dependencies in both space and time. Taylor et al. [[126](#bib.bib126)] extended
    the Gated RBM (GRBM) [[91](#bib.bib91)] to convolutional GRBM (convGRBM) that
    shares weights at all locations in an image and inference is performed through
    convolution. Le et al. [[70](#bib.bib70)] presented an extension of the independent
    subspace analysis algorithm [[128](#bib.bib128)] to learn invariant spatio-temporal
    features from unlabeled video data. They scale up the original ISA to larger input
    data by employing two important ideas from convolutional neural networks: convolution
    and stacking. This convolutional stacking idea enables the algorithm to learn
    a hierarchical representation of the data suitable for recognition. Yan et al. [[164](#bib.bib164)]
    proposed Dynencoder, a three layer auto-encoder, to capture video dynamics. Dynencoder
    is shown to be successful in synthesizing dynamic textures, and one can think
    of a Dynencoder as a compact way of representing the spatio-temporal information
    of a video. Similarly, Srivastava et al. [[122](#bib.bib122)] introduced a LSTM
    autoencoder model. The LSTM autoencoder model consists of two RNNs, namely, the
    encoder LSTM and the decoder LSTM. The encoder LSTM accepts a sequence as input
    and learns the corresponding compact representation. The states of the encoder
    LSTM contain the appearance and dynamics of the sequence. The decoder LSTM receives
    the learned representation to reconstruct the input sequence. Inspired by the
    Generative Adversarial Networks (GAN) [[42](#bib.bib42)], Mathieu et al. [[90](#bib.bib90)]
    adopted the adversarial mechanism to train a multi-scale convolutional network
    to generate future frames given an input sequence. To deal with the inherently
    blurry predictions obtained from the standard Mean Squared Error (MSE) loss function,
    they proposed three different and complementary feature learning strategies: a
    multi-scale architecture, an adversarial training method, and an image gradient
    difference loss function.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于 CNN 和 RNN 的 RGB 模态运动识别常用方法外，还有一些其他结构被采用于这一任务。Jhuang 等人[[60](#bib.bib60)]
    使用了一种前馈层次模板匹配架构进行动作识别，在第一层中使用预定义的时空滤波器。在他的论文中，Chen [[13](#bib.bib13)] 采用了卷积式 RBM
    (CRBM) 作为基本处理单元，并提出了所谓的时空深度置信网络 (ST-DBN)，该网络交替聚合空间和时间信息，使得更高层次能够捕捉空间和时间上的长程统计依赖。Taylor
    等人[[126](#bib.bib126)] 扩展了门控 RBM (GRBM) [[91](#bib.bib91)] 为卷积式 GRBM (convGRBM)，它在图像的所有位置共享权重，并通过卷积进行推理。Le
    等人[[70](#bib.bib70)] 提出了独立子空间分析算法[[128](#bib.bib128)] 的扩展，以从未标记的视频数据中学习不变的时空特征。他们通过采用卷积神经网络的两个重要思想：卷积和堆叠，将原始的
    ISA 扩展到更大的输入数据。这种卷积堆叠思想使算法能够学习适合识别的分层数据表示。Yan 等人[[164](#bib.bib164)] 提出了 Dynencoder，一种三层自编码器，用于捕捉视频动态。Dynencoder
    在合成动态纹理方面表现成功，可以将 Dynencoder 视为表示视频时空信息的紧凑方式。类似地，Srivastava 等人[[122](#bib.bib122)]
    介绍了一种 LSTM 自编码器模型。LSTM 自编码器模型由两个 RNN 组成，即编码器 LSTM 和解码器 LSTM。编码器 LSTM 接受序列作为输入，并学习相应的紧凑表示。编码器
    LSTM 的状态包含序列的外观和动态。解码器 LSTM 接收学习到的表示来重建输入序列。受生成对抗网络 (GAN) [[42](#bib.bib42)] 启发，Mathieu
    等人[[90](#bib.bib90)] 采用了对抗机制来训练多尺度卷积网络，以在给定输入序列的情况下生成未来帧。为了解决标准均方误差 (MSE) 损失函数固有的模糊预测，他们提出了三种不同且互补的特征学习策略：多尺度架构、对抗训练方法和图像梯度差异损失函数。
- en: 3.2 Continuous/Online Motion Recognition
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 连续/在线运动识别
- en: Most of the action recognition methods reviewed above rely heavily on segmented
    videos for model training. However, it is very expensive and time-consuming to
    acquire a large-scale trimmed video dataset. The availability of untrimmed video
    datasets (e.g. [[48](#bib.bib48), [167](#bib.bib167), [21](#bib.bib21), [139](#bib.bib139),
    [19](#bib.bib19)]) have encouraged research and challenges/contests in motion
    recognition in this domain.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上述大多数动作识别方法在模型训练中都严重依赖于分段视频。然而，获取大规模的裁剪视频数据集非常昂贵且耗时。未裁剪视频数据集（例如 [[48](#bib.bib48),
    [167](#bib.bib167), [21](#bib.bib21), [139](#bib.bib139), [19](#bib.bib19)]）的可用性促进了该领域动作识别的研究以及挑战/竞赛。
- en: 3.2.1 CNN-based Approach
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 基于 CNN 的方法
- en: Inspired by the success of region-proposal-based object detection using R-CNN [[39](#bib.bib39),
    [38](#bib.bib38), [103](#bib.bib103)], several proposal-based action recognition
    methods from untrimmed video are proposed. These methods first generate a reduced
    number of candidate temporal windows, and then an action classifier discriminates
    each proposal independently into one of the actions of interest. For instance,
    based on the two-stream concept [[116](#bib.bib116)], [[40](#bib.bib40)] classified
    frame-based region proposals of interest using static and motion cues. The regions
    are then linked across frames based on the predictions and their spatial overlap;
    thus producing action tubes respectively for each action and video. Weinzaepfel
    et al. [[159](#bib.bib159)] also started from the frame-level proposals, selected
    the highest scoring ones, tracked them throughout the video, and adopted a multi-scale
    sliding window approach over tracks to detect the temporal content of an action.
    Shou et al. [[115](#bib.bib115)] proposed a multi-stage segment-based 3D CNNs
    to generate candidate segments, that are used to recognize actions and localize
    temporal boundaries. Peng and Schimd [[97](#bib.bib97)] generated rich proposals
    from both RGB and optical flow data by using region proposal networks for frame-level
    action detection, and stacked optical flows to enhance the discriminative power
    of motion R-CNN. Wang et al. [[146](#bib.bib146)] proposed an UntrimmedNet to
    generate clip proposals that may contain action instances for untrimmed action
    recognition. Based on these clip-level representations, the classification module
    aims to predict the scores for each clip proposal and the selection module tries
    to select or rank those clip proposals. Similarly in the same direction, Zhao
    et al. [[175](#bib.bib175)] adopted explicit structural modeling in the temporal
    dimension. In their model, each complete activity instance is considered as a
    composition of three major stages, namely starting, course, and ending, and they
    introduced structured temporal pyramid pooling to produce a global representation
    of the entire proposal. Differently from previous methods, Zhu et al. [[179](#bib.bib179)]
    proposed a framework that integrates the complementary spatial and temporal information
    into an end-to-end trainable system for video action proposal, and a novel and
    efficient path trimming method is proposed to handle untrimmed video by examining
    actionness and background score pattern without using extra detectors. To generalize
    R-CNN from 2D to 3D, Hou et al. [[52](#bib.bib52)] proposed an end-to-end 3D CNN-based
    approach for action detection in videos. A Tube Proposal Network was introduced
    to leverage skip pooling in temporal domain to preserve temporal information for
    action localization in 3D volumes, and Tube-of-Interest pooling layer was proposed
    to effectively alleviate the problem with variable spatial and temporal sizes
    of tube proposals. Saha et al. [[106](#bib.bib106)] proposed a deep net framework
    capable of regressing and classifying 3D region proposals spanning two successive
    video frames. The core of the framework is an evolution of classical region proposal
    networks (RPNs) to 3D RPNs. Similarly, Kalogeiton et al. [[63](#bib.bib63)] extended
    the Single Shot MultiBox Detector (SSD) [[84](#bib.bib84)] framework from 2D to
    3D by proposing an Action Tubelet detector. In order to quickly and accurately
    generate temporal action proposals, Gao et al. [[37](#bib.bib37)] proposed a Temporal
    Unit Regression Network (TURN) model, that jointly predicts action proposals and
    refines the temporal boundaries by temporal coordinate regression using CNN. Similarly,
    Singh et al. [[119](#bib.bib119)] designed an efficient online algorithm to incrementally
    construct and label ’action tubes’ from the SSD frame level detections, making
    it the first real-time (up to 40fps) system able to perform online S/T action
    localisation on the untrimmed videos. Besides the proposal-based methods discussed
    above, Lea et al. [[71](#bib.bib71)] introduced a new class of temporal models,
    called Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal
    convolutions to perform fine-grained action segmentation or detection.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 受到基于区域提议的对象检测成功的启发，例如使用R-CNN[[39](#bib.bib39), [38](#bib.bib38), [103](#bib.bib103)]，提出了几种基于提议的动作识别方法，这些方法来源于未修剪的视频。这些方法首先生成较少的候选时间窗口，然后一个动作分类器将每个提议独立地区分为感兴趣的动作之一。例如，基于双流概念[[116](#bib.bib116)],
    [[40](#bib.bib40)]，通过静态和运动线索分类了基于帧的区域提议。然后根据预测和空间重叠将区域在帧之间进行连接，从而为每个动作和视频分别生成动作管道。Weinzaepfel等[[159](#bib.bib159)]也从帧级别的提议开始，选择了得分最高的提议，跟踪了整个视频，并采用多尺度滑动窗口方法对轨迹进行操作以检测动作的时间内容。Shou等[[115](#bib.bib115)]提出了一种多阶段基于段的3D
    CNN来生成候选段，用于识别动作和定位时间边界。Peng和Schimd[[97](#bib.bib97)]通过使用区域提议网络进行帧级动作检测，生成了来自RGB和光流数据的丰富提议，并堆叠了光流以增强运动R-CNN的辨别能力。Wang等[[146](#bib.bib146)]提出了一种UntrimmedNet来生成可能包含动作实例的剪辑提议，用于未修剪动作识别。基于这些剪辑级别的表示，分类模块旨在预测每个剪辑提议的得分，选择模块则尝试选择或排名这些剪辑提议。同样，在相同方向上，Zhao等[[175](#bib.bib175)]在时间维度上采用了明确的结构建模。在他们的模型中，每个完整的活动实例被视为由三个主要阶段组成，即开始、过程和结束，并引入了结构化的时间金字塔池化来生成整个提议的全局表示。与以前的方法不同，Zhu等[[179](#bib.bib179)]提出了一个将互补的空间和时间信息集成到端到端可训练的视频动作提议系统中的框架，并提出了一种新颖而高效的路径修剪方法，通过检查动作性和背景分数模式来处理未修剪的视频，而无需使用额外的检测器。为了将R-CNN从2D推广到3D，Hou等[[52](#bib.bib52)]提出了一种基于3D
    CNN的端到端动作检测方法。引入了Tube Proposal Network以利用时间域的跳跃池化来保留3D体积中的时间信息，并提出了Tube-of-Interest池化层，以有效缓解提议的空间和时间尺寸变异问题。Saha等[[106](#bib.bib106)]提出了一个深度网络框架，能够回归和分类跨越两个连续视频帧的3D区域提议。该框架的核心是将经典区域提议网络（RPNs）演化为3D
    RPNs。类似地，Kalogeiton等[[63](#bib.bib63)]通过提出一个动作管道检测器，将Single Shot MultiBox Detector
    (SSD)[[84](#bib.bib84)]框架从2D扩展到3D。为了快速准确地生成时间动作提议，Gao等[[37](#bib.bib37)]提出了一种Temporal
    Unit Regression Network (TURN)模型，该模型通过CNN联合预测动作提议并通过时间坐标回归来细化时间边界。类似地，Singh等[[119](#bib.bib119)]设计了一种高效的在线算法，以增量方式构建和标记来自SSD帧级检测的‘动作管道’，使其成为第一个能够在未修剪的视频上实时（高达40fps）执行在线S/T动作定位的系统。除了上述基于提议的方法外，Lea等[[71](#bib.bib71)]引入了一类新的时间模型，称为时间卷积网络（TCNs），它使用时间卷积的层次结构来执行细粒度动作分割或检测。
- en: '![Refer to caption](img/4a0588c64e353b3dff63da6441a92cdd.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4a0588c64e353b3dff63da6441a92cdd.png)'
- en: 'Figure 11: The problem of weakly supervised action labelling is tackled where
    only the order of the occurring actions in given during training. The temporal
    model is trained by maximizing the probability of all possible frame-to-label
    alignments. At testing time, no annotation is given. The learned model encodes
    the temporal structure of videos and could predict the actions without further
    information. Figure from [[55](#bib.bib55)].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：解决了弱监督动作标注的问题，其中仅在训练期间给出了发生动作的顺序。通过最大化所有可能的帧到标签对齐的概率来训练时间模型。在测试时，没有给出注释。学习到的模型编码了视频的时间结构，并可以在没有进一步信息的情况下预测动作。图来源于[[55](#bib.bib55)]。
- en: 3.2.2 RNN-based Approach
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 基于RNN的方法
- en: 'Apart from the proposal-based methods that use CNN, there are several proposal-based
    methods using RNN for temporal modeling. Escorcia et al. [[30](#bib.bib30)] introduced
    the Deep Action Proposals (DAPs) that generate temporal action proposals from
    long untrimmed videos for action detection and classification (see Figure. [9](#S3.F9
    "Figure 9 ‣ 3.1.2 RNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")). They adopted C3D network [[130](#bib.bib130)] for
    visual encoder and LSTM for sequence encoder. However, all of these methods generated
    proposals by a sliding window approach, dividing the video into short overlapping
    temporal window, which is computationally expensive. To reduce the number of proposals,
    Buch et al. [[9](#bib.bib9)] proposed a single-stream temporal action proposal
    generation method that does not the need to divide input into short overlapping
    clips or temporal windows for batch processing.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '除了使用CNN的基于提案的方法外，还有几种使用RNN进行时间建模的基于提案的方法。Escorcia等人[[30](#bib.bib30)]提出了深度动作提案（DAPs），它从长时间未修剪的视频中生成时间动作提案，用于动作检测和分类（见图[9](#S3.F9
    "Figure 9 ‣ 3.1.2 RNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")）。他们采用了C3D网络[[130](#bib.bib130)]作为视觉编码器，并采用LSTM作为序列编码器。然而，所有这些方法都是通过滑动窗口方法生成提案的，将视频划分为短的重叠时间窗口，这在计算上很昂贵。为了减少提案的数量，Buch等人[[9](#bib.bib9)]提出了一种单流时间动作提案生成方法，该方法不需要将输入划分为短的重叠剪辑或时间窗口进行批处理。'
- en: 'Besides the proposal-based methods, there are several methods that are proposal-free.
    Yeung et al. [[168](#bib.bib168)] proposed an end-to-end training model that is
    formulated as a recurrent neural network-based agent. This agent learns a policy
    for sequentially forming and refining hypotheses about action instances based
    on the intuition that the process of detecting actions is naturally one of observation
    and refinement. They adopted two networks namely, observation network and recurrent
    network, for this purpose. Singh et al. [[118](#bib.bib118)] presented a multi-stream
    bi-directional recurrent neural network for fine-grained action detection. They
    adopted a tracking algorithm to locate a bounding box around the person and trained
    two streams on motion and appearance cropped to the tracked bounding box. The
    video sequence was split into fixed long chunks for the input of two-stream networks,
    and bi-directional LSTM was used to model long-term temporal dynamics within and
    between actions. Ma et al. [[87](#bib.bib87)] introduced a novel ranking loss
    within the RNN objective so that the trained model better captures progression
    of activities. The ranking loss constrains the detection score of the correct
    category to be monotonically non-decreasing as the activity progresses. The same
    time, the detection score margin between the correct activity category and all
    other categories is monotonically non-decreasing. Huang et al. [[55](#bib.bib55)]
    proposed a weakly-supervised framework for action labeling in video ( see Figure [11](#S3.F11
    "Figure 11 ‣ 3.2.1 CNN-based Approach ‣ 3.2 Continuous/Online Motion Recognition
    ‣ 3 RGB-based Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion
    Recognition with Deep Learning: A Survey")), where only the order of occurring
    actions is required during training. They proposed an Extended Connectionist Temporal
    Classification (ECTC) framework to efficiently evaluate all possible alignments
    between the input and label sequences via dynamic programming and explicitly enforce
    their consistency with frame-to-frame visual similarities. Taking inspiration
    from classical linear dynamic systems theory for modeling time series, Dave et
    al. [[20](#bib.bib20)] derived a series of recurrent neural networks that sequentially
    make top-down predictions about the future and then correct those predictions
    with bottom-up observations. Their predictive-corrective architecture allows the
    incorporation of insights from time-series analysis: adaptively focus computation
    on “surprising” frames where predictions require large corrections; simplify learning
    in that only “residual-like” corrective terms need to be learned over time and
    naturally decorrelate an input stream in a hierarchical fashion, producing a more
    reliable signal for learning at each layer of a network.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '除了基于提议的方法，还有几种不依赖提议的方法。Yeung 等人 [[168](#bib.bib168)] 提出了一个端到端训练模型，该模型被构建为基于递归神经网络的代理。该代理学习一个策略，以根据对动作实例的直觉形成和改进假设，因为检测动作的过程本质上是观察和改进的过程。他们为此采用了两个网络，即观察网络和递归网络。Singh
    等人 [[118](#bib.bib118)] 提出了一个多流双向递归神经网络，用于细粒度动作检测。他们采用了一种跟踪算法来定位围绕人的边界框，并在剪裁到跟踪边界框的运动和外观上训练两个流。视频序列被分割成固定长度的片段，以供双流网络输入，使用双向
    LSTM 来建模动作内和动作间的长期时间动态。Ma 等人 [[87](#bib.bib87)] 在 RNN 目标中引入了一种新颖的排名损失，使训练模型能够更好地捕捉活动的进展。排名损失约束正确类别的检测分数随着活动的进行单调不减。与此同时，正确活动类别与所有其他类别之间的检测分数差距也单调不减。Huang
    等人 [[55](#bib.bib55)] 提出了一个弱监督框架，用于视频中的动作标注（见图 [11](#S3.F11 "图 11 ‣ 3.2.1 CNN-based
    Approach ‣ 3.2 Continuous/Online Motion Recognition ‣ 3 RGB-based Motion Recognition
    with Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning:
    A Survey")），其中在训练过程中仅要求动作的发生顺序。他们提出了一个扩展的连接主义时间分类（ECTC）框架，通过动态编程有效评估输入和标签序列之间的所有可能对齐，并明确强制它们与帧间视觉相似性的一致性。受到经典线性动态系统理论对时间序列建模的启发，Dave
    等人 [[20](#bib.bib20)] 推导出一系列递归神经网络，这些网络依次对未来做出自上而下的预测，然后通过自下而上的观察来修正这些预测。他们的预测-修正架构允许从时间序列分析中获得见解：自适应地将计算重点放在需要大幅修正的“意外”帧上；简化学习，使得只需学习“残差型”修正项，并以分层方式自然解相关输入流，为每一层网络的学习提供更可靠的信号。'
- en: 4 Depth-based Motion Recognition with Deep Learning
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**基于深度学习的深度运动识别**'
- en: Compared with RGB videos, the depth modality is insensitive to illumination
    variations, invariant to color and texture changes, reliable for estimating body
    silhouette and skeleton, and provides rich 3D structural information of the scene.
    However, there are only few published results on depth based action recognition
    using deep learning methods. Two reasons can be adduced for this situation. First,
    the absence of color and texture in depth maps weakens the discriminative representation
    power of CNN models [[85](#bib.bib85)]. Second, existing depth data is relatively
    small-scale. The conventional pipelines are purely data-driven and learn representation
    directly from the pixels. Such model is likely to be at risk of overfitting when
    the network is optimized on limited training data. Currently, there are only CNN-based
    methods for depth-based motion recognition.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与RGB视频相比，深度模态对光照变化不敏感，对颜色和纹理变化具有不变性，可靠地估计身体轮廓和骨架，并提供场景的丰富3D结构信息。然而，关于基于深度的动作识别的深度学习方法的已发布结果很少。造成这种情况的原因有两个。首先，深度图中缺乏颜色和纹理削弱了CNN模型的辨别表现力[[85](#bib.bib85)]。其次，现有的深度数据相对较小规模。传统的处理流程完全是数据驱动的，直接从像素中学习表征。这样的模型在有限的训练数据上优化时容易出现过拟合。目前，仅有基于CNN的方法用于深度动作识别。
- en: 4.1 Segmented Motion Recognition
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 分段动作识别
- en: 4.1.1 CNN-based Approach
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 基于CNN的方法
- en: 'Wang et al. [[149](#bib.bib149), [150](#bib.bib150)] proposed a method called
    Weighted Hierarchical Depth Motion Maps (WHDMM) + 3ConvNet, for human action recognition
    from depth maps on small training datasets. Three strategies were developed to
    leverage the capability of ConvNets in mining discriminative features for recognition.
    Firstly, different viewpoints are mimicked by rotating the 3D points of the captured
    depth maps. This not only auguments the data, but also makes the trained ConvNets
    view-tolerant. Secondly, WHDMMs at several temporal scales were constructed to
    encode the spatio-temporal motion patterns of actions into 2D spatial structures.
    The 2D spatial structures are further enhanced for recognition by converting the
    WHDMMs into pseudo-color images. Lastly, the three ConvNets were initialized with
    the models obtained from ImageNet and fine-tuned independently on the color-coded
    WHDMMs constructed in three orthogonal planes. Inspired by the promising results
    achieved by rank pooling method [[6](#bib.bib6)] on RGB data, Wang et al. [[152](#bib.bib152)]
    encoded the depth map sequences into three kinds of dynamic images with rank pooling:
    Dynamic Depth Images (DDI), Dynamic Depth Normal Images (DDNI) and Dynamic Depth
    Motion Normal Images (DDMNI). These three representations capture the posture
    and motion information from three different levels for gesture recognition. Specifically,
    DDI exploits the dynamics of postures over time and DDNI and DDMNI exploit the
    3D structural information captured by depth maps. Wang et al. [[148](#bib.bib148)]
    replaced the bidirectional rank pooling in the method of [[152](#bib.bib152)]
    with hierarchical and bidirectional rank pooling to capture both high order and
    non-linear dynamics effectively for both gesture and action recognition. Recently,
    Wang et al. [[156](#bib.bib156)] proposed to represent a depth map sequence into
    three pairs of structured dynamic images at body, part and joint levels respectively
    through bidirectional rank pooling. Different from previous works that applied
    one ConvNet for each part/joint separately, one pair of structured dynamic images
    is constructed from depth maps at each granularity level and serves as the input
    of a ConvNet. The structured dynamic image not only preserves the spatial-temporal
    information but also enhances the structure information across both body parts/joints
    and at different temporal scales. In addition, it requires low computational cost
    and memory to construct. This new representation, referred to as Spatially Structured
    Dynamic Depth Images (S²DDI), aggregates from global to fine-grained motion and
    structure information in a depth sequence, and enables us to fine-tune the existing
    ConvNet models trained on image data for classification of depth sequences, without
    a need for training the models afresh. Similarly, Hou et al. [[54](#bib.bib54)]
    extended S²DDI to Spatially and Temporally Structured Dynamic Depth Images (STSDDI),
    where a hierarchical bidirectional rank pooling method was adopted to exploit
    the spatio-temporal-structural information contained in the depth sequence and
    it is applied to interactions of two subjects. Differently from the above texture
    image encoding method, Rahmani et al. [[102](#bib.bib102)] proposed a cross-view
    action recognition based on depth sequence. Their method comprises two steps:
    (i) learning a general view-invariant human pose model from synthetic depth images
    and, (ii) modeling the temporal action variations. To enlarge the training data
    for CNN, they generated the training data synthetically by fitting realistic synthetic
    3D human models to real mocap data and then rendering each pose from a large number
    of viewpoints. For spatio-temporal representation, they used group sparse Fourier
    Temporal Pyramid which encodes the action-specific discriminative output features
    of the proposed human pose model.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wang 等人[[149](#bib.bib149), [150](#bib.bib150)] 提出了一种称为加权层次深度运动地图（WHDMM）+ 3ConvNet
    的方法，用于在小型训练数据集上从深度图中识别人类动作。他们开发了三种策略来利用 ConvNet 在识别中挖掘辨识特征的能力。首先，通过旋转捕获的深度图的三维点来模仿不同视角。这不仅增加了数据的多样性，还使训练的
    ConvNet 具有视角容忍性。其次，构建了多个时间尺度的WHDMM，以将动作的时空运动模式编码为二维空间结构。通过将WHDMM转换为伪彩色图像，进一步增强了这些二维空间结构以进行识别。最后，三个ConvNet使用从ImageNet获取的模型进行初始化，并在三个正交平面上构建的彩色编码WHDMM上独立进行微调。受rank
    pooling方法在RGB数据上取得的有前途的结果[[6](#bib.bib6)]的启发，Wang 等人[[152](#bib.bib152)]使用rank
    pooling将深度图序列编码为三种动态图像: 动态深度图像（DDI）、动态深度正常图像（DDNI）和动态深度运动正常图像（DDMNI）。这三种表示分别从三个不同的层次捕获姿势和运动信息。特别是，DDI利用时间内的姿势动态，而DDNI和DDMNI则利用深度图捕获的三维结构信息。Wang
    等人[[148](#bib.bib148)]将[[152](#bib.bib152)]中的双向rank pooling替换为层次化和双向rank pooling，以有效捕获姿势和动作识别中的高阶和非线性动态。最近，Wang
    等人[[156](#bib.bib156)]提出了通过双向rank pooling将深度图序列表示为身体、部分和关节级别的三对结构化动态图像。与以往将一个ConvNet分别应用于每个部分/关节的方法不同，每个粒度级别都从深度图中构建一对结构化动态图像，并作为ConvNet的输入。这种结构化动态图像不仅保留了时空信息，还增强了身体部位/关节和不同时间尺度上的结构信息。此外，构建它所需的计算成本和内存开销低。这种新的表示称为空间结构化动态深度图像（S²DDI），从全局到细粒度的运动和结构信息在深度序列中聚合，并使我们能够微调已经在图像数据上训练的现有ConvNet模型，以用于深度序列的分类，而无需重新训练模型。类似地，Hou
    等人[[54](#bib.bib54)]将S²DDI扩展为空间和时间结构化动态深度图像（STSDDI），采用分层双向rank pooling方法来利用深度序列中包含的时空结构信息，并将其应用于两个主体的交互。与上述纹理图像编码方法不同，Rahmani
    等人[[102](#bib.bib102)]提出了基于深度序列的跨视角动作识别方法。他们的方法包括两个步骤: (i) 从合成深度图像学习通用的视角不变人体姿势模型，以及
    (ii) 建模时间动作变化。为了扩大CNN的训练数据，他们通过将逼真的合成3D人体模型拟合到真实的mocap数据，并从大量视角渲染每个姿势来合成训练数据。对于时空表示，他们使用组稀疏傅立叶时间金字塔，该金字塔编码了建议的人体姿势模型的动作特定辨识输出特征。'
- en: '![Refer to caption](img/26e84afb3b030c2bc2bbccaba8927c1e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/26e84afb3b030c2bc2bbccaba8927c1e.png)'
- en: 'Figure 12: Depth map sequences are encoded into texture color images by using
    the concepts of Depth Motion Maps (DMM) [[165](#bib.bib165)] and pseudo-coloring,
    and at the same time enlarged the training data by scene rotation on the 3D point
    cloud. Three channel of CNN are adopted for feature extraction and classification.
    Figure from [[150](#bib.bib150)].'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：深度图序列通过使用深度运动图（DMM）[[165](#bib.bib165)]和伪彩色技术被编码为纹理颜色图像，同时通过3D点云上的场景旋转扩大了训练数据。采用三通道CNN进行特征提取和分类。图来源于[[150](#bib.bib150)]。
- en: 4.2 Continuous/Online Motion Recognition
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 连续/在线动作识别
- en: 4.2.1 CNN-based Approach
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 基于CNN的方法
- en: For continuous gesture recognition, Wang et al. [[153](#bib.bib153)] first segmented
    the continuous depth sequence into segmented sequences using quantity of movement
    (QOM) [[62](#bib.bib62)], and then adopted improved DMM (IDMM) to encode the dynamics
    of depth sequences into texture images for large-scale continuous gesture recognition.
    To improve the encoding quality of depth sequences, Wang et al. [[148](#bib.bib148)]
    proposed three simple, compact yet effective representations of depth sequences,
    referred to respectively as Dynamic Depth Images (DDI), Dynamic Depth Normal Images
    (DDNI) and Dynamic Depth Motion Normal Images (DDMNI), for continuous action recognition.
    These dynamic images are constructed from a segmented sequence of depth maps using
    hierarchical bidirectional rank pooling to effectively capture the spatial-temporal
    information. Specifically, DDI exploits the dynamics of postures over time while
    DDNI and DDMNI extract the 3D structural information captured by depth maps. The
    image-based representations enable us to fine-tune the existing ConvNet models
    trained on image data without training a large number of parameters from scratch.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续手势识别，Wang等人[[153](#bib.bib153)]首先使用运动量（QOM）[[62](#bib.bib62)]将连续深度序列分段，然后采用改进的DMM（IDMM）将深度序列的动态编码为纹理图像，以进行大规模的连续手势识别。为了提高深度序列的编码质量，Wang等人[[148](#bib.bib148)]提出了三种简单、紧凑但有效的深度序列表示，分别称为动态深度图像（DDI）、动态深度法线图像（DDNI）和动态深度运动法线图像（DDMNI），用于连续动作识别。这些动态图像是通过分段的深度图序列使用层次双向排名池化构建的，以有效捕捉时空信息。具体而言，DDI利用姿势随时间变化的动态，而DDNI和DDMNI提取深度图捕捉的3D结构信息。基于图像的表示使我们能够对现有的在图像数据上训练的ConvNet模型进行微调，而无需从头开始训练大量参数。
- en: 5 Skeleton-based Motion Recognition with Deep Learning
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基于骨架的动作识别与深度学习
- en: 'Differently from RGB and depth, skeleton data contains the positions of human
    joints, which can be considered relatively high-level features for motion recognition.
    There are two common ways to estimate skeletons, one is to use MOCAP systems and
    the other is to estimate skeletons directly from depth maps or RGB images/video.
    Skeletons from MOCAP systems are often robust to scale and illumination changes
    and can be invariant to viewpoints as well as human body rotation and motion speed;
    Skeletons estimated from depth maps or RGB images/video are prone to errors caused
    by a number of factors including viewpoints and occlusion since both factors can
    lead to significant different appearance of same actions. Currently, there are
    mainly three approaches to skeleton-based motion recognition using deep learning:
    (i) RNN-based, (ii) CNN-based and other-architecture-based approaches for segmented
    motion recognition and, (iii) RNN-based approaches for continuous/online motion
    recognition.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与RGB和深度不同，骨架数据包含人体关节的位置，这些位置可以被视为相对高级的动作识别特征。估计骨架有两种常见的方法，一种是使用MOCAP系统，另一种是直接从深度图或RGB图像/视频中估计骨架。MOCAP系统得到的骨架通常对尺度和光照变化具有鲁棒性，并且对视角以及人体旋转和动作速度具有不变性；从深度图或RGB图像/视频估计的骨架容易受到视角和遮挡等多个因素的影响，因为这些因素可能导致相同行为的外观差异很大。目前，使用深度学习的骨架动作识别主要有三种方法：（i）基于RNN的方法，（ii）基于CNN和其他架构的方法用于分段动作识别，以及（iii）基于RNN的方法用于连续/在线动作识别。
- en: 5.1 Segmented Motion Recognition
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 分段动作识别
- en: '![Refer to caption](img/fee2c9a8aa3903b4363dd70664ef1b67.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fee2c9a8aa3903b4363dd70664ef1b67.png)'
- en: 'Figure 13: The JTM framework for skeleton-based motion recognition with CNN.
    Figure from [[155](#bib.bib155)].'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：基于CNN的骨架动作识别的JTM框架。图来源于[[155](#bib.bib155)]。
- en: 5.1.1 CNN-based Approach
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 基于CNN的方法
- en: The main step in this approach is to convert the skeleton sequences into images
    where the spatio-temporal information is reflected in the image properties including
    color and texture. Duet al. [[24](#bib.bib24)] represented a skeleton sequence
    as a matrix by concatenating the joint coordinates at each instant and arranging
    the vector representations in a chronological order. The matrix is then quantified
    into an image and normalized to handle the variable-length problem. The final
    image is fed into a CNN model for feature extraction and recognition. Wang et
    al. [[155](#bib.bib155)] proposed to encode spatio-temporal information contained
    in the skeleton sequence into multiple texture images, namely, Joint Trajectory
    Maps (JTM), by mapping the trajectories into HSV (hue, saturation, value) space.
    Pre-trained models over Imagenet is adopted for fine-tuning over the JTMs to extract
    features and recognize actions. Similarly, Hou et al. [[53](#bib.bib53)] drew
    the skeleton joints with a specific pen to three orthogonal canvases, and encodes
    the dynamic information in the skeleton sequences with color encoding. Li et al. [[75](#bib.bib75)]
    proposed to encode the pair-wise distances of skeleton joints of single or multiple
    subjects into texture images, namely, Joint Distance Maps (JDM), as the input
    of CNN for action recognition. Compared with the works reported by [[155](#bib.bib155)]
    and [[53](#bib.bib53)], JDM is less sensitive to view variations. liu et al. [[83](#bib.bib83)]
    introduced an enhanced skeleton visualization method to represent a skeleton sequence
    as a series of visual and motion enhanced color images. They proposed a sequence-based
    view invariant transform to deal with the view variation problem, and multi-stream
    CNN fusion method is adopted to conduct recognition. Ke et al. [[65](#bib.bib65)]
    designed vector-based features for each body part of human skeleton sequences,
    which are translation, scale and rotation invariant, and transformed the features
    into images to feed into CNN for learning high level and discriminative representation.
    In another effort, Ke et al. [[66](#bib.bib66)] represented the sequence as a
    clip with several gray images for each channel of the 3D coordinates, which reflects
    multiple spatial structural information of the joints. The images are fed to a
    deep CNN to learn high-level features, and the CNN features of all the three clips
    at the same time-step are concatenated in a feature vector. Each feature vector
    represents the temporal information of the entire skeleton sequence and one particular
    spatial relationship of the joints. A Multi-Task Learning Network (MTLN) is adopted
    to jointly process the feature vectors of all time-steps in parallel for action
    recognition. Kim and Reiter [[67](#bib.bib67)] approached the problem differently
    and proposed to use the Temporal Convolutional Neural Networks (TCN) [[71](#bib.bib71)]
    for skeleton based action recognition. They re-designed the original TCN into
    Res-TCN by factoring out the deeper layers into additive residual terms that yields
    both interpretable hidden representations and model parameters.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要步骤是将骨架序列转换为图像，其中时空信息通过图像的颜色和纹理等属性体现出来。Du et al. [[24](#bib.bib24)] 通过在每个时刻连接关节坐标并按时间顺序排列向量表示，将骨架序列表示为矩阵。然后将矩阵量化为图像，并进行归一化处理以解决变量长度问题。最终图像被输入到CNN模型中进行特征提取和识别。Wang
    et al. [[155](#bib.bib155)] 提出了将骨架序列中的时空信息编码为多张纹理图像，即关节轨迹图（JTM），通过将轨迹映射到HSV（色调、饱和度、明度）空间。使用在Imagenet上预训练的模型对JTM进行微调以提取特征并识别动作。类似地，Hou
    et al. [[53](#bib.bib53)] 用特定的笔在三个正交画布上绘制骨架关节，并用颜色编码对骨架序列中的动态信息进行编码。Li et al.
    [[75](#bib.bib75)] 提出了将单个或多个对象的骨架关节的成对距离编码为纹理图像，即关节距离图（JDM），作为CNN进行动作识别的输入。与[[155](#bib.bib155)]和[[53](#bib.bib53)]报道的工作相比，JDM对视角变化的敏感性较低。liu
    et al. [[83](#bib.bib83)] 引入了一种增强的骨架可视化方法，将骨架序列表示为一系列视觉和运动增强的彩色图像。他们提出了一种基于序列的视图不变变换来处理视角变化问题，并采用了多流CNN融合方法进行识别。Ke
    et al. [[65](#bib.bib65)] 为每个身体部位设计了基于向量的特征，这些特征具有平移、缩放和旋转不变性，并将这些特征转换为图像输入到CNN中，以学习高级和区分性的表示。在另一项工作中，Ke
    et al. [[66](#bib.bib66)] 将序列表示为每个3D坐标通道的若干灰度图像的剪辑，反映了关节的多种空间结构信息。这些图像被输入到深度CNN中以学习高级特征，并将所有三个剪辑在相同时间步的CNN特征串联成一个特征向量。每个特征向量表示整个骨架序列的时间信息和关节的特定空间关系。采用了多任务学习网络（MTLN）来并行处理所有时间步的特征向量以进行动作识别。Kim和Reiter
    [[67](#bib.bib67)] 从不同的角度解决了这个问题，并提出使用时间卷积神经网络（TCN）[[71](#bib.bib71)] 进行基于骨架的动作识别。他们通过将原始TCN重新设计为Res-TCN，将更深层次的层次分解为加法残差项，从而获得了既可解释的隐藏表示又可解释的模型参数。
- en: '![Refer to caption](img/b70045e53b17d2ff87b1ca775cd044d2.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b70045e53b17d2ff87b1ca775cd044d2.png)'
- en: 'Figure 14: The LSTM autoencoder model and LSTM future predictor model. Figure
    from [[26](#bib.bib26)].'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：LSTM 自编码器模型和 LSTM 未来预测模型。图源自 [[26](#bib.bib26)]。
- en: 5.1.2 RNN-based Approach
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 基于 RNN 的方法
- en: 'In this class of approaches, skeleton features are input to an RNN in order
    to exploit the temporal evolution. For instance, in a series or works Du et al. [[26](#bib.bib26),
    [25](#bib.bib25)] divided the whole skeleton sequence into five parts according
    to the human physical structure, and separately fed them into five bidirectional
    RNNs/LSTMs. As the number of layers increases, the representations extracted by
    the subnets are hierarchically fused to build a higher-level representation. The
    process is illustrated in Fig. [14](#S5.F14 "Figure 14 ‣ 5.1.1 CNN-based Approach
    ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based Motion Recognition with
    Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey").
    This method explicitly encodes the spatio-temporal-structural information into
    high level representation. Veeriah et al. [[134](#bib.bib134)] proposed a differential
    gating scheme for the LSTM neural network, which emphasizes the change in information
    gain caused by the salient motions between the successive frames. This work is
    one of the first aimed at demonstrating the potential of learning complex time-series
    representations via high-order derivatives of states. Zhu et al. [[181](#bib.bib181)]
    designed two types of regularizations to learn effective features and motion dynamics.
    In the fully connected layers, they introduced regularization to drive the model
    to learn co-occurrence features of the joints at different layers. Furthermore,
    they derived a new dropout and apply it to the LSTM neurons in the last LSTM layer,
    which helps the network to learn complex motion dynamics. Instead of keeping a
    long-term memory of the entire body’s motion in the cell, Shahroudy et al. [[109](#bib.bib109)]
    proposed a part-aware LSTM human action learning model (P-LSTM) wherein memory
    is split across part-based cells. It is argued that keeping the context of each
    body part independent and representing the output of the P-LSTM unit as a combination
    of independent body part context information is more efficient. Previous RNN-based
    3D-action recognition methods have adopted RNN to model the long-term contextual
    information in the temporal domain for motion-based dynamics representation. However,
    there is also strong dependency between joints in the spatial domain. In addition
    the spatial configuration of joints in video frames can be highly discriminative
    for 3D-action recognition task. To exploit this dependency, Liu et al.[[81](#bib.bib81)]
    proposed a spatio-temporal LSTM (ST-LSTM) network which extends the traditional
    LSTM-based learning to both temporal and spatial domains. Rather than concatenate
    the joint-based input features, ST-LSTM explicitly models the dependencies between
    the joints and applies recurrent analysis over spatial and temporal domains concurrently.
    Besides, they introduced a trust gate mechanism to make LSTM robust to noisy input
    data. Song et al. [[120](#bib.bib120)] proposed a spatio-temporal attention model
    with LSTM to automatically mine the discriminative joints and learn the respective
    and different attentions of each frame along the temporal axis. Similarly, Liu
    et al. [[82](#bib.bib82)] proposed a Global Context-Aware Attention LSTM (GCA-LSTM)
    to selectively focus on the informative joints in the action sequence with the
    assistance of global context information. Differently from previous works that
    adopted the coordinates of joints as input, Zhang et al. [[174](#bib.bib174)]
    investigated a set of simple geometric features of skeleton using 3-layer LSTM
    framework, and showed that using joint-line distances as input requires less data
    for training. Based on the notion that LSTM networks with various time-step sizes
    can model various attributes well, Lee et al. [[73](#bib.bib73)] proposed an ensemble
    Temporal Sliding LSTM (TS-LSTM) networks for skeleton-based action recognition.
    The proposed network is composed of multiple parts containing short-term, medium-
    term and long-term TS-LSTM networks, respectively. Li et al. [[76](#bib.bib76)]
    proposed an adaptive and hierarchical framework for fine-grained, large-scale
    skeleton-based action recognition. This work was motivated by the need to distinguish
    fine-grained action classes that are intractable using a single network, and adaptivity
    to new action classes by model augmentation. In the framework, multiple RNNs are
    effectively incorporated in a tree-like hierarchy to mitigate the discriminative
    challenge and thus using a divide-and-conquer strategy. To deal with large view
    variations in captured human actions, Zhang et al. [[173](#bib.bib173)] proposed
    a self-regulated view adaptation scheme which re-positions the observation viewpoints
    dynamically, and integrated the proposed view adaptation scheme into an end-to-end
    LSTM network which automatically determines the “best” observation viewpoints
    during recognition.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一类方法中，骨架特征被输入到 RNN 中，以利用时间演变。例如，在 Du 等人的一系列工作中[[26](#bib.bib26), [25](#bib.bib25)]，他们根据人体结构将整个骨架序列划分为五部分，并分别输入到五个双向
    RNNs/LSTMs 中。随着层数的增加，子网络提取的表示被层次性地融合以构建更高级别的表示。该过程如图[14](#S5.F14 "Figure 14 ‣
    5.1.1 CNN-based Approach ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")所示。这种方法显式地将时空结构信息编码为高级表示。Veeriah 等人[[134](#bib.bib134)]
    提出了一个差分门控方案，用于 LSTM 神经网络，强调了连续帧之间显著运动所导致的信息增益变化。这项工作是首批旨在展示通过状态的高阶导数学习复杂时间序列表示的研究之一。Zhu
    等人[[181](#bib.bib181)] 设计了两种正则化方法来学习有效的特征和运动动态。在全连接层中，他们引入了正则化，以驱动模型学习不同层次上的关节共现特征。此外，他们推导了一种新的
    dropout，并将其应用于最后一层 LSTM 神经元，这有助于网络学习复杂的运动动态。Shahroudy 等人[[109](#bib.bib109)] 提出了一个基于部分的
    LSTM 人体动作学习模型（P-LSTM），其中记忆被拆分到基于部分的单元中。他们认为，将每个身体部位的上下文保持独立，并将 P-LSTM 单元的输出表示为独立身体部位上下文信息的组合更加高效。以前基于
    RNN 的 3D 动作识别方法采用 RNN 来建模时间域中的长期上下文信息，以进行基于运动的动态表示。然而，关节在空间域中也存在强依赖性。此外，视频帧中关节的空间配置对
    3D 动作识别任务可能具有高度的辨别力。为了利用这种依赖性，Liu 等人[[81](#bib.bib81)] 提出了一个时空 LSTM（ST-LSTM）网络，该网络将传统的
    LSTM 基于学习扩展到时间域和空间域。ST-LSTM 不仅连接关节基输入特征，还显式建模关节之间的依赖关系，并同时对空间和时间域进行递归分析。此外，他们引入了一个信任门控机制，使
    LSTM 对噪声输入数据更具鲁棒性。Song 等人[[120](#bib.bib120)] 提出了一个带有 LSTM 的时空注意模型，用于自动挖掘辨别性关节，并学习沿时间轴每帧的相应和不同注意力。类似地，Liu
    等人[[82](#bib.bib82)] 提出了一个全局上下文感知注意 LSTM（GCA-LSTM），以在全局上下文信息的帮助下选择性地关注动作序列中的信息关节。与以前采用关节坐标作为输入的工作不同，Zhang
    等人[[174](#bib.bib174)] 使用 3 层 LSTM 框架研究了一组简单的骨架几何特征，并表明使用关节线距离作为输入需要较少的训练数据。基于
    LSTM 网络可以很好地建模不同时间步长属性的概念，Lee 等人[[73](#bib.bib73)] 提出了一个用于骨架动作识别的集成时间滑动 LSTM（TS-LSTM）网络。该网络由多个部分组成，包括短期、中期和长期
    TS-LSTM 网络。Li 等人[[76](#bib.bib76)] 提出了一个自适应和分层框架，用于细粒度、大规模骨架动作识别。这项工作是由需要区分细粒度动作类别（使用单个网络难以处理）和通过模型扩展适应新动作类别的需求驱动的。在该框架中，多个
    RNN 被有效地纳入树状层次结构中，以减轻辨别挑战，从而采用分而治之的策略。为了处理捕获的人体动作中的大视角变化，Zhang 等人[[173](#bib.bib173)]
    提出了一个自我调节视角适配方案，该方案动态重新定位观察视点，并将所提的视角适配方案集成到端到端的 LSTM 网络中，该网络在识别过程中自动确定“最佳”观察视点。'
- en: '![Refer to caption](img/eea1ad46156bb3793396b48a68ed6b00.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/eea1ad46156bb3793396b48a68ed6b00.png)'
- en: 'Figure 15: The joint classification-regression RNN framework for online action
    detection and forecasting. Figure from [[78](#bib.bib78)].'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：用于在线动作检测和预测的联合分类-回归 RNN 框架。图自[[78](#bib.bib78)]。
- en: '![Refer to caption](img/c92aec62606b560efe88d29635a1c927.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c92aec62606b560efe88d29635a1c927.png)'
- en: 'Figure 16: Conceptual illustration of LieNet architecture. In the network structure,
    the data space of each RotMap/RotPooling layer corresponds to a Lie group, while
    the weight spaces of the RotMap layers are Lie groups as well. Figure from [[56](#bib.bib56)].'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：LieNet 架构的概念图。在网络结构中，每个 RotMap/RotPooling 层的数据空间对应一个李群，而 RotMap 层的权重空间也是李群。图自[[56](#bib.bib56)]。
- en: 5.1.3 Other-architecture-based Approach
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 其他架构基础的方法
- en: 'Besides the RNN- and CNN-based approaches, there are several other deep learning-based
    methods. Salakhutdinov et al. [[107](#bib.bib107)] proposed a new compositional
    learning architecture that integrates deep learning models with structured hierarchical
    Bayesian models. Specifically, this method learns a hierarchical Dirichlet process
    (HDP) [[127](#bib.bib127)] prior over the activities of the top-level features
    in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns novel concepts
    from very few training examples by learning: (i) low-level generic features, (ii)
    high-level features that capture correlations among low-level features and, (iii)
    a category hierarchy for sharing priors over the high-level features that are
    typical of different kinds of concepts. Wu and Shao [[161](#bib.bib161)] adopted
    deep belief networks (DBN) to model the distribution of skeleton joint locations
    and extract high-level features to represent humans at each frame in 3D space.
    Ijjina et al. [[57](#bib.bib57)] adopted stacked auto encoder to learn the underlying
    features of input skeleton data. Huang et al. [[56](#bib.bib56)] incorporated
    the Lie group structure into a deep learning architecture to learn more appropriate
    Lie group features for skeleton based action recognition (see Figure. [16](#S5.F16
    "Figure 16 ‣ 5.1.2 RNN-based Approach ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '除了基于 RNN 和 CNN 的方法，还有几种其他基于深度学习的方法。Salakhutdinov 等人[[107](#bib.bib107)] 提出了一个新的组合学习架构，将深度学习模型与结构化的层次贝叶斯模型相结合。具体而言，该方法在深度玻尔兹曼机（DBM）的顶层特征活动上学习层次狄利克雷过程（HDP）[[127](#bib.bib127)]
    先验。这个复合的 HDP-DBM 模型通过以下方式从非常少的训练样本中学习新概念：（i）低级通用特征，（ii）捕捉低级特征之间相关性的高级特征，以及（iii）用于共享先验的类别层次结构，这些先验通常适用于不同类型的概念。Wu
    和 Shao [[161](#bib.bib161)] 采用了深度信念网络（DBN）来建模骨架关节位置的分布，并提取高级特征以在 3D 空间的每一帧中表示人类。Ijina
    等人[[57](#bib.bib57)] 采用堆叠自编码器来学习输入骨架数据的潜在特征。Huang 等人[[56](#bib.bib56)] 将李群结构纳入深度学习架构中，以学习更适合骨架基础动作识别的李群特征（见图
    [16](#S5.F16 "Figure 16 ‣ 5.1.2 RNN-based Approach ‣ 5.1 Segmented Motion Recognition
    ‣ 5 Skeleton-based Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion
    Recognition with Deep Learning: A Survey")）。'
- en: '![Refer to caption](img/4b0b559cbc7c8e982fcf41116198fb0c.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4b0b559cbc7c8e982fcf41116198fb0c.png)'
- en: 'Figure 17: The deep architecture is composed of five components: (a) Input
    Preprocessing; (b) 3D CNN (C3D); (c) ConvLSTM; (d) Spatial Pyramid Pooling and
    Fully Connected Layers; (e) Multimodal Score Fusion. Figure from [[178](#bib.bib178)].'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：深度架构由五个组件组成：（a）输入预处理；（b）3D CNN（C3D）；（c）ConvLSTM；（d）空间金字塔池化和全连接层；（e）多模态得分融合。图自[[178](#bib.bib178)]。
- en: 5.2 Continuous/Online Motion Recognition
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 连续/在线动作识别
- en: 5.2.1 RNN-based Approach
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 基于 RNN 的方法
- en: 'Differently from previous methods that recognize motion from segmented skeleton
    sequences, Li et al. [[78](#bib.bib78)] proposed a multi-task end-to-end Joint
    Classification-Regression Recurrent Neural Network to explore the action type
    and temporal localization information. They adopted LSTM to capture the complex
    long-range temporal dynamics, which avoids the typical sliding window design and
    thus ensures high computational efficiency. Furthermore, the subtask of regression
    optimization provides the ability to forecast the action prior to its occurrence.
    The framework is shown in Figure. [15](#S5.F15 "Figure 15 ‣ 5.1.2 RNN-based Approach
    ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based Motion Recognition with
    Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey").'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前从分段骨架序列中识别运动的方法不同，Li 等人[[78](#bib.bib78)] 提出了一个多任务端到端的联合分类-回归递归神经网络，以探索动作类型和时间定位信息。他们采用了
    LSTM 来捕捉复杂的长时程动态，这避免了典型的滑动窗口设计，从而确保了高计算效率。此外，回归优化的子任务提供了在动作发生之前进行预测的能力。该框架如图所示。[15](#S5.F15
    "图 15 ‣ 5.1.2 基于 RNN 的方法 ‣ 5.1 分段运动识别 ‣ 5 基于骨架的深度学习运动识别 ‣ 基于 RGB-D 的深度学习人体运动识别：综述")
- en: 6 RGB+D-based Motion Recognition with Deep Learning
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 基于 RGB+D 的深度学习运动识别
- en: As discussed in previous sections, RGB, depth and skeleton modalities have their
    own specific properties, and how to combine the strengths of these modalities
    with deep learning approach is important. To address this problem, several methods
    have been proposed. In general, these methods can be categorized as (i) CNN-based,
    (ii) RNN-based and other-architecture-based approaches for segmented motion recognition
    and, (iii) RNN-based continuous/online motion recognition.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面章节所讨论的，RGB、深度和骨架模态各自具有特定的属性，如何将这些模态的优势与深度学习方法结合起来是很重要的。为了解决这个问题，已经提出了几种方法。通常，这些方法可以分为
    (i) 基于 CNN 的方法，(ii) 基于 RNN 的分段运动识别方法以及 (iii) 基于 RNN 的连续/在线运动识别方法。
- en: 6.1 Segmented Motion Recognition
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 分段运动识别
- en: 6.1.1 CNN-based Approach
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 基于 CNN 的方法
- en: Zhu et al. [[177](#bib.bib177)] fused RGB and depth in a pyramidal 3D convolutional
    network based on C3D [[130](#bib.bib130)] for gesture recognition. They designed
    pyramid input and pyramid fusion for each modality and late score fusion was adopted
    for final recognition. Duan et al. [[27](#bib.bib27)] proposed a convolutional
    two-stream consensus voting network (2SCVN) which explicitly models both the short-term
    and long-term structure of the RGB sequences. To alleviate distractions from background,
    a 3D depth-saliency ConvNet stream (3DDSN) was aggregated in parallel to identify
    subtle motion characteristics. Later score fusion was adopted for final recognition.
    The methods described so far considered RGB and depth as separate channels and
    fused them later. Wang et al. [[151](#bib.bib151)] took a different approach and
    adopted scene flow to extract features that fused the RGB and depth from the onset.
    The new representation based on CNN and named Scene Flow to Action Map (SFAM)
    was used for motion recognition. Different from previous methods, Wang et al. [[154](#bib.bib154)]
    proposed to cooperatively train a single convolutional neural network (named c-ConvNet)
    on both RGB and depth features, and deeply aggregate the two kinds of features
    for action recognition. While the conventional ConvNet learns the deep separable
    features for homogeneous modality-based classification with only one softmax loss
    function, the c-ConvNet enhances the discriminative power of the deeply learned
    features and weakens the undesired modality discrepancy by jointly optimizing
    a ranking loss and a softmax loss for both homogeneous and heterogeneous modalities.
    Rahmani et al. [[101](#bib.bib101)] proposed an end-to-end learning model for
    action recognition from depth and skeleton data. The proposed model learned to
    fuse features from depth and skeletal data, capture the interactions between body-parts
    and/or interactions with environmental objects, and model the temporal structure
    of human actions in an end-to-end learning framework. The proposed method was
    made robust to viewpoint changes, by introducing a deep CNN which transfers visual
    appearance of human body-parts acquired from different unknown views to a view-invariant
    space.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Zhu等人[[177](#bib.bib177)] 在基于C3D[[130](#bib.bib130)]的金字塔3D卷积网络中融合了RGB和深度信息用于手势识别。他们为每种模态设计了金字塔输入和金字塔融合，并采用了后期得分融合进行最终识别。Duan等人[[27](#bib.bib27)]
    提出了一个卷积双流共识投票网络（2SCVN），该网络显式地建模了RGB序列的短期和长期结构。为了减少背景的干扰，3D深度显著性ConvNet流（3DDSN）被并行聚合，以识别微妙的运动特征。后期得分融合被用于最终识别。到目前为止，描述的方法将RGB和深度视为独立通道，然后再进行融合。Wang等人[[151](#bib.bib151)]
    采取了不同的方法，采用场景流从一开始就提取融合RGB和深度的特征。基于CNN的新表示，命名为场景流到动作映射（SFAM），用于动作识别。与之前的方法不同，Wang等人[[154](#bib.bib154)]
    提出了协同训练一个单一卷积神经网络（命名为c-ConvNet），同时处理RGB和深度特征，并深入地聚合这两种特征以进行动作识别。虽然传统的ConvNet仅通过一个softmax损失函数来学习基于同质模态的深度可分特征以进行分类，但c-ConvNet通过联合优化对同质和异质模态都适用的排序损失和softmax损失，增强了深度学习特征的区分能力，并减弱了不希望出现的模态差异。Rahmani等人[[101](#bib.bib101)]
    提出了一个端到端的学习模型，用于从深度和骨骼数据中进行动作识别。该模型学习融合深度和骨骼数据的特征，捕捉身体部位之间和/或与环境对象的交互，并在端到端学习框架中建模人体动作的时间结构。通过引入深度CNN，该方法使得对视点变化具有鲁棒性，该深度CNN将从不同未知视角获得的人体部位视觉外观转移到一个视角不变的空间。
- en: 6.1.2 RNN-based Approach
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 基于RNN的方法
- en: 'For RGB and depth fusion, Pigou et al. [[98](#bib.bib98)] directly considered
    the depth as the fourth channel and CNN was adopted to extract frame-based appearance
    features. Temporal convolutions and RNN were combined to capture the temporal
    information. Li et al. [[79](#bib.bib79)] adopted C3D [[130](#bib.bib130)] to
    extract features separately from RGB and depth modalities, and used the concatenated
    for SVM classifier. Zhu et al. [[178](#bib.bib178)] presented a gesture recognition
    method using C3D [[130](#bib.bib130)] and convolutional LSTM (convLSTM) [[163](#bib.bib163)]
    based on depth and RGB modalities (see Figure [17](#S5.F17 "Figure 17 ‣ 5.1.3
    Other-architecture-based Approach ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")). The major drawback of traditional LSTM in handling
    spatio-temporal data is its usage of full connections in input-to-state and state-to-state
    transitions in which no spatial information is encoded. The ConvLSTM determines
    the future state of a certain cell in the grid by the inputs and past states of
    its local neighbors. Average score fusion was adopted to fuse the two separate
    channel networks for the two modalities. Luo et al. [[86](#bib.bib86)] proposed
    to use a RNN-based encoder-decoder framework to learn a video representation by
    predicting a sequence of basic motions described as atomic 3D flows. The learned
    representation is then extracted from the generated model to recognize activities.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RGB 和深度融合，Pigou 等人[[98](#bib.bib98)] 直接将深度视为第四个通道，并采用 CNN 提取基于帧的外观特征。结合了时间卷积和
    RNN 以捕捉时间信息。Li 等人[[79](#bib.bib79)] 采用 C3D[[130](#bib.bib130)] 分别从 RGB 和深度模态中提取特征，并使用拼接的特征进行
    SVM 分类。Zhu 等人[[178](#bib.bib178)] 提出了一种基于深度和 RGB 模态的手势识别方法，使用 C3D[[130](#bib.bib130)]
    和卷积 LSTM (convLSTM)[[163](#bib.bib163)]（见图 [17](#S5.F17 "图 17 ‣ 5.1.3 其他架构方法 ‣
    5.1 分段运动识别 ‣ 5 基于骨架的深度学习运动识别 ‣ 基于 RGB-D 的人类运动识别综述")）。传统 LSTM 在处理时空数据时的主要缺点是其在输入到状态和状态到状态的转换中使用全连接，而没有编码空间信息。ConvLSTM
    通过网格中某个单元的输入和其局部邻居的过去状态来确定未来状态。采用平均分数融合将两个模态的两个独立通道网络进行融合。Luo 等人[[86](#bib.bib86)]
    提出了使用基于 RNN 的编码器-解码器框架，通过预测一系列描述为原子 3D 流的基本动作来学习视频表示。然后从生成的模型中提取学习到的表示以识别活动。
- en: '![Refer to caption](img/115aa9c045f88bbcb57f1118fe847a99.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/115aa9c045f88bbcb57f1118fe847a99.png)'
- en: 'Figure 18: The framework of using scene flow for motion recognition. Scene
    flow vectors are first transformed into Scene Flow Maps (SFM), and then using
    Channel Transform Kernels to transform SFM into an analogous RGB space to take
    advantage of pre-train models over ImageNet. Figure from [[151](#bib.bib151)].'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：使用场景流进行运动识别的框架。场景流向量首先被转换为场景流图 (SFM)，然后使用通道转换核将 SFM 转换为类似 RGB 的空间，以利用 ImageNet
    上的预训练模型。图示来自[[151](#bib.bib151)]。
- en: '![Refer to caption](img/3c99f1ca0a817d99c03513782430b4ae.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3c99f1ca0a817d99c03513782430b4ae.png)'
- en: 'Figure 19: The framework of PI-based RNNs. It consists of three steps: (1)
    The pre-training step taking both depth maps and skeleton as input. An embedded
    encoder is trained in a standard CNN-RNN pipeline. (2) The trained encoder is
    used to initialize the learning step. A multi-task loss is applied to exploit
    the PI in the regression term as a secondary task. (3) Finally, refinement step
    aims to discover the latent PI by defining a bridging matrix, in order to maximize
    the effectiveness of the PI. The latent PI is utilized to close the gap between
    different information. The latent PI, bridging matrix and the network are optimized
    iteratively in an EM procedure. Figure from [[113](#bib.bib113)].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：基于 PI 的 RNN 框架。包括三个步骤：（1）预训练步骤，将深度图和骨架作为输入。嵌入编码器在标准的 CNN-RNN 流水线中进行训练。（2）使用训练好的编码器初始化学习步骤。应用多任务损失在回归项中利用
    PI 作为次要任务。（3）最后，细化步骤旨在通过定义桥接矩阵发现潜在 PI，以最大化 PI 的有效性。潜在 PI 被用于弥合不同信息之间的差距。潜在 PI、桥接矩阵和网络在
    EM 过程中进行迭代优化。图示来自[[113](#bib.bib113)]。
- en: 'Shi et al. [[113](#bib.bib113)] fused depth and skeleton in a so-called privileged
    information (PI)-based RNN (PRNN) that exploits additional knowledge of skeleton
    sequences to obtain a better estimate of network parameters from depth map sequences.
    A bridging matrix is defined to connect softmax classification loss and regression
    loss by discovering latent PI in the refinement step. The whole process is illustrated
    in Figure [19](#S6.F19 "Figure 19 ‣ 6.1.2 RNN-based Approach ‣ 6.1 Segmented Motion
    Recognition ‣ 6 RGB+D-based Motion Recognition with Deep Learning ‣ RGB-D-based
    Human Motion Recognition with Deep Learning: A Survey").'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 'Shi 等人 [[113](#bib.bib113)] 在一种称为特权信息 (PI) 基于 RNN (PRNN) 的方法中融合了深度图和骨架信息，该方法利用骨架序列的额外知识来更好地估计网络参数。通过在细化步骤中发现潜在的
    PI，定义了一个桥接矩阵来连接 softmax 分类损失和回归损失。整个过程如图 [19](#S6.F19 "Figure 19 ‣ 6.1.2 RNN-based
    Approach ‣ 6.1 Segmented Motion Recognition ‣ 6 RGB+D-based Motion Recognition
    with Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning:
    A Survey") 所示。'
- en: For RGB and skeleton fusion, Mahassein and Todorovic [[89](#bib.bib89)] presented
    a regularization of LSTM learning where the output of another encoder LSTM (eLSTM)
    grounded on 3D human-skeleton training data is used as the regularization. This
    regularization rests on the hypothesis that since videos and skeleton sequences
    are about human motions their respective feature representations should be similar.
    The skeleton sequences, being view-independent and devoid of background clutter,
    are expected to facilitate capturing important motion patterns of human-body joints
    in 3D space.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RGB 和骨架融合，Mahassein 和 Todorovic [[89](#bib.bib89)] 提出了 LSTM 学习的正则化，其中另一编码器
    LSTM (eLSTM) 的输出基于 3D 人体骨架训练数据作为正则化。该正则化基于这样一个假设：由于视频和骨架序列涉及人体运动，它们的特征表示应该相似。骨架序列由于独立于视角且没有背景杂乱，预计可以有助于捕捉人体关节在
    3D 空间中的重要运动模式。
- en: 6.1.3 Other-architecture-based Approach
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 其他架构基础的方法
- en: Shahroudy et al. [[110](#bib.bib110)] extracted hand-crafted features which
    are neither independent nor fully correlated from RGB and depth, and embedded
    the input feature into a space of factorized common and modality-specific components.
    The combination of shared and specific components in input features can be very
    complex and highly non-linear. In order to disentangle them, they stacked layers
    of non-linear auto encoder-based component factorization to form a deep shared-specific
    analysis network.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Shahroudy 等人 [[110](#bib.bib110)] 从 RGB 和深度图中提取了既不独立也不完全相关的手工特征，并将输入特征嵌入到一个由因子化的共同和模态特定组件组成的空间中。输入特征中共享和特定组件的组合可能非常复杂且高度非线性。为了解开这些组件，他们堆叠了非线性自编码器基础的组件因子化层，形成了一个深度共享-特定分析网络。
- en: In a RGB, depth and skeleton fusion method, Wu et al. [[160](#bib.bib160)] adopted
    Gaussian-Bernouilli Deep Belief Network(DBN) to extract high-level skeletal joint
    features and the learned representation is used to estimate the emission probability
    needed to infer gesture sequences. A 3D Convolutional Neural Network (3DCNN) was
    used to extract features from 2D multiple channel inputs such as depth and RGB
    images stacked along the 1D temporal domain. In addition, intermediate and late
    fusion strategies were investigated in combination with the temporal modeling.
    The result of both mechanisms indicates that multiple-channel fusion can outperform
    individual modules.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RGB、深度和骨架融合方法中，Wu 等人 [[160](#bib.bib160)] 采用了高斯-伯努利深度置信网络 (DBN) 来提取高级骨架关节特征，学习到的表示用于估计推断手势序列所需的发射概率。使用了
    3D 卷积神经网络 (3DCNN) 从 2D 多通道输入（如沿 1D 时间域堆叠的深度图和 RGB 图像）中提取特征。此外，还结合时间建模研究了中间和晚期融合策略。两种机制的结果表明，多通道融合可以优于单个模块。
- en: 6.2 Continuous/Online Motion Recognition
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 连续/在线动作识别
- en: 6.2.1 RNN-based Approach
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 基于 RNN 的方法
- en: Chai et al. [[12](#bib.bib12)] proposed to fuse RGB and depth in a two-stream
    RNN (2S-RNN) for gesture recognition. They designed a fusion layer for depth and
    RGB before the LSTM layer. [[93](#bib.bib93)] presented an algorithm for joint
    segmentation and classification of dynamic hand gestures from continuous video
    streams. They proposed a network that employs a recurrent C3D with connectionist
    temporal classification (CTC) [[43](#bib.bib43)]. They trained a separate network
    for each modality and averaged their scores for final recognition. Beside RGB
    and depth modalities, they also adopted stereo-IR modality in their work.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Chai 等人 [[12](#bib.bib12)] 提出了将 RGB 和深度融合到一个双流 RNN（2S-RNN）中用于手势识别。他们在 LSTM 层之前设计了一个用于深度和
    RGB 的融合层。 [[93](#bib.bib93)] 提出了一个用于从连续视频流中联合分割和分类动态手势的算法。他们提出了一个使用连接主义时序分类（CTC）的递归
    C3D 网络 [[43](#bib.bib43)]。他们为每种模态训练了一个单独的网络，并平均它们的分数以进行最终识别。除了 RGB 和深度模态外，他们的工作还采用了立体
    IR 模态。
- en: 7 Discussion
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: 'We presented a comprehensive overview of RGB-D based motion recognition using
    deep learning. We defined a taxonomy covering two groups: segmented and continuous/online
    motion recognition, with four categories in each group based on the adopted modalities.
    From the viewpoint of encoding spatio-temporal-structural information contained
    in the video sequences, CNN, RNN and other networks adopted for motion recognition
    are discussed in each category. In subsequent sections, the relative performance
    of the different methods on several commonly used RGB-D datasets are analysed,
    and from the comparisons we highlight some challenges. The discussion on performance
    and challenges then provides a basis for outlining potential future research directions.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对基于 RGB-D 的运动识别进行了全面概述，使用深度学习定义了一个包含两个组的分类法：分段和连续/在线运动识别，每个组中基于所采用的模式有四个类别。从编码视频序列中包含的时空结构信息的角度来看，讨论了在每个类别中用于运动识别的
    CNN、RNN 和其他网络。在后续章节中，我们分析了不同方法在几个常用 RGB-D 数据集上的相对性能，并从比较中突出了某些挑战。对性能和挑战的讨论为概述潜在的未来研究方向提供了基础。
- en: 7.1 Performance Analysis of the Current Methods
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 当前方法的性能分析
- en: 'In this section, we compare the accuracy of different methods using several
    commonly used datasets, including CMU Mocap, HDM05, MSR-Action3D, MSRC-12, MSRDailyActivity3D,
    UTKinect, G3D, SBU Kinect Interaction, Berkeley MHAD, Northwestern-UCLA Multiview
    Action3D, ChaLearn LAP IsoGD, NTU RGB+D, ChaLearn2014, ChaLearn LAP ConGD, and
    PKU-MMD. These datasets cover motion capture sensor system, structured light cameras
    (Kinect v1) and ToF cameras (Kinect v2). The last three datasets are continuous
    datasets while the others are segmented datasets. The performance is evaluated
    using accuracy for segmented motion recognition, and Jaccard Index is added as
    another criteria for continuous motion recognition. The accuracy is calculated
    as the proportion of accurately labelled samples. The Jaccard index measures the
    average relative overlap between true and predicted sequences of frames for a
    given gesture/action. For a sequence $s$, let $G_{s,i}$ and $P_{s,i}$ be binary
    indicator vectors for which 1-values correspond to frames in which the $i^{th}$
    gesture/action label is being performed. The Jaccard Index for the $i^{th}$ class
    is defined for the sequence $s$ as:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用多个常用数据集比较不同方法的准确性，这些数据集包括 CMU Mocap、HDM05、MSR-Action3D、MSRC-12、MSRDailyActivity3D、UTKinect、G3D、SBU
    Kinect Interaction、Berkeley MHAD、Northwestern-UCLA Multiview Action3D、ChaLearn
    LAP IsoGD、NTU RGB+D、ChaLearn2014、ChaLearn LAP ConGD 和 PKU-MMD。这些数据集涵盖了运动捕捉传感器系统、结构光摄像机（Kinect
    v1）和 ToF 摄像机（Kinect v2）。后三个数据集是连续数据集，而其他数据集是分段数据集。性能评估采用分段运动识别的准确率，并且在连续运动识别中增加了
    Jaccard 指数作为另一标准。准确率计算为标记正确的样本的比例。Jaccard 指数测量的是给定手势/动作的真实帧序列和预测帧序列之间的平均相对重叠。对于序列
    $s$，设 $G_{s,i}$ 和 $P_{s,i}$ 为二进制指示向量，其中 1 值对应于执行第 $i$ 个手势/动作标签的帧。第 $i$ 类的 Jaccard
    指数对于序列 $s$ 定义为：
- en: '|  | $J_{s,i}=\frac{G_{s,i}\bigcap P_{s,i}}{G_{s,i}\bigcup P_{s,i}},$ |  |
    (1) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | $J_{s,i}=\frac{G_{s,i}\bigcap P_{s,i}}{G_{s,i}\bigcup P_{s,i}},$ |  |
    (1) |'
- en: 'where $G_{s,i}$ is the ground truth of the $i^{th}$ gesture/action label in
    sequence $s$, and $P_{s,i}$ is the prediction for the $i^{th}$ label in sequence
    $s$. When $G_{s,i}$ and $P_{s,i}$ are empty, $J_{(s,i)}$ is defined to be 0. Then
    for the sequence $s$ with $l_{s}$ true labels, the Jaccard Index $J_{s}$ is calculated
    as:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G_{s,i}$ 是序列 $s$ 中第 $i$ 个手势/动作标签的真实值，$P_{s,i}$ 是序列 $s$ 中第 $i$ 个标签的预测值。当
    $G_{s,i}$ 和 $P_{s,i}$ 为空时，$J_{(s,i)}$ 定义为 0。然后，对于具有 $l_{s}$ 个真实标签的序列 $s$，Jaccard
    指数 $J_{s}$ 计算为：
- en: '|  | $J_{s}=\frac{1}{l_{s}}\sum_{i=1}^{L}J_{s,i}.$ |  | (2) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $J_{s}=\frac{1}{l_{s}}\sum_{i=1}^{L}J_{s,i}.$ |  | (2) |'
- en: 'For all test sequences $S={s_{1},...,s_{n}}$ with $n$ gestures/actions, the
    mean Jaccard Index $\overline{J_{S}}$ is used as the evaluation criteria and calculated
    as:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有测试序列 $S={s_{1},...,s_{n}}$，其中 $n$ 是手势/动作的数量，平均 Jaccard 指数 $\overline{J_{S}}$
    被用作评估标准，计算公式为：
- en: '|  | $\overline{J_{S}}=\frac{1}{n}\sum_{j=1}^{n}J_{s_{j}}.$ |  | (3) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\overline{J_{S}}=\frac{1}{n}\sum_{j=1}^{n}J_{s_{j}}.$ |  | (3) |'
- en: The detailed comparison of different methods is presented in following Table LABEL:performance.
    From the Table we can see that there is no single approach that is able to produce
    the best performance over all datasets. Generally speaking, methods using multi-modal
    information can have better performance than their single modality counterpart
    due to the complementary properties of the three different modalities. On some
    datasets, such as NTU RGB+D dataset, current results suggest that CNN-based methods
    tend to be better than RNN-based methods. This is probably due to fact that CNN-based
    methods includes human empirical knowledge in the coding process, and could take
    advantage of pre-trained models over large image set, such as ImageNet. The combination
    of CNN and RNN seems to be a good choice for motion recognition, for instance,
    the C3D+ConvLSTM [[178](#bib.bib178)] method achieved promising results on ChaLearn
    LAP IsoGD dataset. For continuous motion recognition, RNN-based methods tend to
    achieve good results.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 不同方法的详细比较见下表 LABEL:performance。从表中可以看出，没有一种方法能够在所有数据集上都产生最佳性能。一般来说，使用多模态信息的方法比其单一模态对应方法有更好的性能，因为三种不同模态的互补特性。在某些数据集上，如
    NTU RGB+D 数据集，目前的结果表明，基于 CNN 的方法往往比基于 RNN 的方法更好。这可能是因为 CNN 基于的方法在编码过程中包含了人类的经验知识，并且可以利用在大规模图像集（如
    ImageNet）上预训练的模型。CNN 和 RNN 的组合似乎是动作识别的一个不错选择，例如，C3D+ConvLSTM [[178](#bib.bib178)]
    方法在 ChaLearn LAP IsoGD 数据集上取得了令人满意的结果。对于连续动作识别，基于 RNN 的方法往往能取得良好的效果。
- en: 7.2 Challenges
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 挑战
- en: 'The advent of low-cost RGB-D sensors that have access to extra depth and skeleton
    data, has motivated the significant development of human motion recognition. Promising
    results have been achieved with deep learning approaches [[150](#bib.bib150),
    [174](#bib.bib174), [81](#bib.bib81)], on several constrained simple datasets,
    such as MSR-Action3D, Berkeley MHAD and SBU Kinect Interaction. Despite this success,
    results are far from satisfactory on some large complex datasets, such as ChaLearn
    LAP IsoGD and NTU RGB+D datasets and especially the continuous/online datasets.
    In fact, it is still very difficult to build a practical intelligent recognition
    system. Such goal poses several challenges:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 低成本 RGB-D 传感器的出现，能够获取额外的深度和骨架数据，促进了人类动作识别的显著发展。在几个受限的简单数据集上，如 MSR-Action3D、Berkeley
    MHAD 和 SBU Kinect Interaction，深度学习方法 [[150](#bib.bib150), [174](#bib.bib174),
    [81](#bib.bib81)] 已取得了有希望的结果。尽管如此，在一些大型复杂数据集上，如 ChaLearn LAP IsoGD 和 NTU RGB+D
    数据集，尤其是连续/在线数据集上的结果仍然远未令人满意。实际上，建立一个实用的智能识别系统仍然非常困难。这一目标面临几个挑战：
- en: Encoding temporal information. As discussed, there are several methods to encode
    temporal information. We can use CNN to extract frame-based features and then
    conduct temporal fusion [[64](#bib.bib64)], or adopt 3D filter and 3D pooling
    layers to learn motion features [[130](#bib.bib130)], or use optical/scene flow
    to extract motion information [[116](#bib.bib116), [151](#bib.bib151)], or encode
    the video into images [[6](#bib.bib6), [150](#bib.bib150), [155](#bib.bib155)],
    or use RNN/LSTM to model the temporal dependences [[22](#bib.bib22), [26](#bib.bib26),
    [82](#bib.bib82)]. However, all these approaches have their drawbacks. Temporal
    fusion method tends to neglect the temporal order; 3D filters and 3D pooling filters
    have a very rigid temporal structure and they only accept a predefined number
    of frames as input which is always short; optical/scene flow methods are computationally
    expensive; sequence to images methods inevitably loses temporal information during
    encoding; the weight sharing mechanism of RNN/LSTM methods make the sequence matching
    imprecise, but rather approximated, so an appropriate distance function must be
    used to predict the match probability. In fact, there is still no perfect method
    for temporal encoding, and how to model temporal information is a big challenge.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 编码时间信息。如前所述，有几种方法来编码时间信息。我们可以使用CNN提取基于帧的特征，然后进行时间融合[[64](#bib.bib64)]，或采用3D滤波器和3D池化层来学习运动特征[[130](#bib.bib130)]，或使用光流/场景流提取运动信息[[116](#bib.bib116),
    [151](#bib.bib151)]，或将视频编码为图像[[6](#bib.bib6), [150](#bib.bib150), [155](#bib.bib155)]，或使用RNN/LSTM建模时间依赖关系[[22](#bib.bib22),
    [26](#bib.bib26), [82](#bib.bib82)]。然而，这些方法各有其缺点。时间融合方法往往忽略时间顺序；3D滤波器和3D池化滤波器有非常僵硬的时间结构，仅接受预定义数量的帧作为输入，且通常较短；光流/场景流方法计算开销大；将序列转换为图像的方法在编码过程中不可避免地丢失时间信息；RNN/LSTM方法的权重共享机制使得序列匹配不够精确，而是近似的，因此必须使用适当的距离函数来预测匹配概率。实际上，尚无完美的时间编码方法，如何建模时间信息仍然是一个重大挑战。
- en: Small training data. Most of available deep learning methods rely on large labeled
    training data [[64](#bib.bib64), [130](#bib.bib130)]. However, in practical scenarios,
    obtaining large labeled training data is costly and laborious, even impossible,
    especially in medical-related applications. It has been shown that fine-tuning
    motion-based networks with spatial data (ImageNet) is more effective than training
    from scratch [[116](#bib.bib116), [155](#bib.bib155), [6](#bib.bib6), [151](#bib.bib151)].
    Strategies for data augmentation are also commonly used [[150](#bib.bib150)].
    Likewise, training mechanisms to avoid overfitting and control learning rate have
    also been studied [[121](#bib.bib121)]. However, it is still a challenge to effectively
    train deep networks from small training data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 小规模训练数据。大多数现有的深度学习方法依赖于大量标记的训练数据[[64](#bib.bib64), [130](#bib.bib130)]。然而，在实际场景中，获取大量标记的训练数据成本高昂且费时，甚至在医学相关应用中几乎不可能。已有研究表明，使用空间数据（ImageNet）对基于运动的网络进行微调比从头开始训练更有效[[116](#bib.bib116),
    [155](#bib.bib155), [6](#bib.bib6), [151](#bib.bib151)]。数据增强策略也被广泛使用[[150](#bib.bib150)]。同样，避免过拟合和控制学习率的训练机制也已被研究[[121](#bib.bib121)]。然而，如何有效地从小规模训练数据中训练深度网络仍然是一个挑战。
- en: Viewpoint variation and occlusion. When skeletons are estimated from RGB images/video
    or depth maps, viewpoint variation may cause significantly different appearance
    of same actions, and occlusion would “crash” the skeleton data. Occlusion includes
    inter-occlusion caused by other subjects or objects, and self-occlusion created
    by the object/subject itself. Most of available datasets require subjects to perform
    actions in a visible and restricted view to avoid occlusion, and this results
    in limited view data collection and less occlusion. However, occlusion is inevitable
    in practical scenarios, especially for interactions. This makes it challenging
    to isolate individuals in overlapping area and extract features of a unique person;
    leading to the ineffectiveness of many of available approaches [[26](#bib.bib26),
    [109](#bib.bib109), [75](#bib.bib75)]. Possible solutions to handle viewpoint
    variation and occlusion include the use of multi-sensor systems [[96](#bib.bib96),
    [143](#bib.bib143), [109](#bib.bib109), [19](#bib.bib19)]. The multi-camera systems
    is able to generate multi-view data, but the drawback is the requirement of synchronization
    and feature/recognition fusion among different views. This usually increases processing
    complexity and computation cost. Several methods have been proposed to handle
    the viewpoint variation and occlusion. Wang et al. [[149](#bib.bib149)] proposed
    to rotate the depth data in 3D point clouds through different angles to deal with
    viewpoint invariance; spherical coordinate system corresponding to body center
    was developed to achieve view-independent motion recognition [[56](#bib.bib56)].
    However, these methods become less effective when occlusion occurs. How to effectively
    handle occlusion using deep learning methods is a new challenge.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 视角变化和遮挡。当从RGB图像/视频或深度图估计骨架时，视角变化可能会导致相同动作的外观显著不同，而遮挡会“破坏”骨架数据。遮挡包括由其他主体或物体引起的相互遮挡，以及由对象/主体自身造成的自我遮挡。大多数现有数据集要求主体在可见和受限的视角下执行动作，以避免遮挡，这导致了有限的视角数据收集和较少的遮挡。然而，在实际场景中，尤其是交互中，遮挡是不可避免的。这使得在重叠区域中隔离个体并提取独特的特征变得具有挑战性；导致许多现有方法的效果不佳 [[26](#bib.bib26),
    [109](#bib.bib109), [75](#bib.bib75)]。处理视角变化和遮挡的可能解决方案包括使用多传感器系统 [[96](#bib.bib96),
    [143](#bib.bib143), [109](#bib.bib109), [19](#bib.bib19)]。多摄像头系统能够生成多视角数据，但缺点是需要在不同视角之间进行同步和特征/识别融合。这通常会增加处理复杂性和计算成本。已有几种方法被提出以处理视角变化和遮挡。Wang等人 [[149](#bib.bib149)]
    提出了通过不同角度旋转深度数据点云来处理视角不变性；开发了与身体中心对应的球面坐标系统以实现视角独立的动作识别 [[56](#bib.bib56)]。然而，当发生遮挡时，这些方法的效果会降低。如何有效使用深度学习方法处理遮挡是一个新的挑战。
- en: Execution rate variation and repetition. The execution rate may vary due to
    the different performing styles and states of individuals. The varying rate results
    in different frames for the same motion. Repetition also bring about this issue.
    The global encoding methods [[53](#bib.bib53), [65](#bib.bib65), [83](#bib.bib83)]
    would become less effective due to the repetition. The commonly used methods to
    handle this problem is up/down sampling [[181](#bib.bib181), [174](#bib.bib174),
    [75](#bib.bib75)]. However, sampling methods would inevitably bring redundant
    or loss of useful information. Effective handling of this problem remains a challenge.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 执行率的变化和重复性。执行率可能因个体的不同表现风格和状态而有所不同。这种变化会导致相同动作的不同帧。重复性也会引发这个问题。由于重复，全球编码方法 [[53](#bib.bib53),
    [65](#bib.bib65), [83](#bib.bib83)] 的效果会降低。常用的处理方法是上下采样 [[181](#bib.bib181), [174](#bib.bib174),
    [75](#bib.bib75)]。然而，采样方法不可避免地带来冗余或有用信息的丢失。有效处理这个问题仍然是一项挑战。
- en: Cross-datasets. Many research works have been carried out to recognize human
    actions from RGB-D video clips. To learn an effective action classifier, most
    of the previous approaches rely on enough training labels. When being required
    to recognize the action in a different dataset, these approaches have to re-train
    the model using new labels. However, labeling video sequences is a very tedious
    and time-consuming task, especially when detailed spatial locations and time durations
    are required. Even though some works have studied this topic [[11](#bib.bib11),
    [123](#bib.bib123), [171](#bib.bib171)], they are all based on hand-crafted features,
    and the results are far from satisfactory due to the large distribution variances
    between different datasets, including different scenarios, different modalities,
    different views, different persons, and even different actions. How to deal with
    cross-datasets RGB-D motion recognition is a big challenge.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 跨数据集。许多研究工作已被开展，以从RGB-D视频片段中识别人体动作。为了学习一个有效的动作分类器，大多数先前的方法依赖于足够的训练标签。当需要在不同的数据集中识别动作时，这些方法必须使用新标签重新训练模型。然而，标记视频序列是一项非常繁琐且耗时的任务，尤其是当需要详细的空间位置和时间持续时。尽管一些研究已经探讨了这个主题[[11](#bib.bib11),
    [123](#bib.bib123), [171](#bib.bib171)]，但它们都是基于手工制作的特征，由于不同数据集之间的分布差异，包括不同的场景、不同的模态、不同的视角、不同的人甚至不同的动作，结果远未令人满意。如何处理跨数据集的RGB-D动作识别是一个巨大的挑战。
- en: Online motion recognition. Most of available methods rely on segmented data,
    and their capability for online recognition is quite limited. Even though continuous
    motion recognition is one improved version where the videos are untrimmed, it
    still assumes that all the videos are available before processing. Thus, proposal-based
    methods [[115](#bib.bib115), [146](#bib.bib146)] can be adopted for offline processing.
    Differently from continuous motion recognition, online motion recognition aims
    to receive continuous streams of unprocessed visual data and recognize actions
    from an unsegmented stream of data in a continuous manner. So far two main approaches
    can be identified for online recognition, sliding window-based and RNN-based.
    Sliding window-based methods [[19](#bib.bib19)] are simple extension of segmented-based
    action recognition methods. They often consider the temporal coherence within
    the window for prediction and the window-based predictions are further fused to
    achieve online recognition. However, the performance of these methods are sensitive
    to the window size which depends on actions and is hard to set. Either too large
    or too small a window size could lead to significant drop in recognition. For
    RNN-based methods [[93](#bib.bib93), [78](#bib.bib78)], even though promising
    results have been achieved, it is still far from satisfactory in terms of performance.
    How to design effective practical online recognition system is a big challenge.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在线动作识别。大多数现有方法依赖于分段数据，它们在在线识别方面的能力相当有限。即使连续动作识别是一种改进版，其中视频未经过剪辑，它仍假设在处理之前所有视频都可用。因此，可以采用基于提议的方法[[115](#bib.bib115),
    [146](#bib.bib146)]进行离线处理。与连续动作识别不同，在线动作识别旨在接收连续的未处理视觉数据流，并从未分段的数据流中以连续的方式识别动作。迄今为止，可以识别出两种主要的在线识别方法，滑动窗口方法和基于RNN的方法。滑动窗口方法[[19](#bib.bib19)]是分段动作识别方法的简单扩展。它们通常考虑窗口内的时间一致性进行预测，并将基于窗口的预测进一步融合以实现在线识别。然而，这些方法的性能对窗口大小很敏感，窗口大小依赖于动作且难以设置。窗口大小过大或过小都会导致识别率显著下降。对于基于RNN的方法[[93](#bib.bib93),
    [78](#bib.bib78)]，尽管取得了有希望的结果，但在性能方面仍远未令人满意。如何设计有效的实际在线识别系统是一个巨大的挑战。
- en: 'Action prediction. We are faced with numerous situations in which we must predict
    what actions other people are about to do in the near future. Predicting future
    actions before they are actually executed is a critical ingredient for enabling
    us to effectively interact with other humans on a daily basis [[104](#bib.bib104),
    [51](#bib.bib51), [69](#bib.bib69), [137](#bib.bib137), [105](#bib.bib105)]. There
    are mainly two challenges for this task: first, we need to capture the subtle
    details inherent in human movements that may imply a future action; second, predictions
    usually should be carried out as quickly as possible in the social world, when
    limited prior observations are available. Predicting the action of a person before
    it is actually executed has a wide range of applications in autonomous robots,
    surveillance and health care. How to develop effective algorithms for action prediction
    is really challenging.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 动作预测。我们面临许多情况，需要预测其他人即将采取的动作。预测未来动作在实际执行之前是使我们能够有效与他人互动的关键因素[[104](#bib.bib104),
    [51](#bib.bib51), [69](#bib.bib69), [137](#bib.bib137), [105](#bib.bib105)]。这个任务主要面临两个挑战：首先，我们需要捕捉人类动作中可能暗示未来动作的细微细节；其次，在社交世界中，预测通常需要在有限的先前观察下尽可能快地进行。预测一个人在实际执行之前的动作在自主机器人、监控和健康护理中有广泛的应用。开发有效的动作预测算法确实具有挑战性。
- en: 7.3 Future Research Directions
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 未来研究方向
- en: The discussion on the challenges faced by available methods allows us to outline
    several future research directions for the development of deep learning methods
    for motion recognition. While the list is not exhaustive, they point at research
    activities that may advance the field.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对现有方法面临挑战的讨论使我们能够概述几种未来研究方向，以推动深度学习方法在运动识别中的发展。虽然列表并不详尽，但它们指出了可能推动该领域进步的研究活动。
- en: Hybrid networks. Most of previous methods adopted one type of neural networks
    for motion recognition. As discussed, there is no perfect solution for temporal
    encoding using single networks. Even though available works such as C3D+ConvLSTM [[178](#bib.bib178)]
    used two types of networks, the cascaded connection makes them dependent on each
    other during training. How to cooperatively train different kinds of networks
    would be a good research direction; for example, using the output of CNN to regularize
    RNN training in parallel.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 混合网络。之前的大多数方法采用了一种类型的神经网络进行运动识别。如前所述，单一网络在时间编码方面没有完美的解决方案。尽管已有工作如C3D+ConvLSTM[[178](#bib.bib178)]使用了两种网络，但级联连接使它们在训练过程中相互依赖。如何协同训练不同类型的网络将是一个良好的研究方向；例如，利用CNN的输出来规范化RNN的并行训练。
- en: 'Simultaneous exploitation of spatial-temporal-structural information. A video
    sequence has three important inherent properties that should be considered for
    motion analysis: spatial information, temporal information and structural information.
    Several previous methods tend to exploit the spatio-temporal information for motion
    recognition, however, structural information contained in the video is rarely
    explicitly mined. Concurrent mining of these three kinds of information with deep
    learning would be an interesting topic in the future [[59](#bib.bib59)].'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 同时利用时空结构信息。视频序列具有三个重要的固有属性，这些属性在运动分析中应被考虑：空间信息、时间信息和结构信息。之前的一些方法倾向于利用时空信息进行运动识别，但视频中的结构信息很少被明确挖掘。未来深度学习中同时挖掘这三种信息将是一个有趣的主题[[59](#bib.bib59)]。
- en: Fusion of multiple modalities. While significant progress has been achieved
    by singly using RGB, skeleton or depth modality, effective deep networks for fusion
    of multi-modal data would be a promising direction. For example, methods such
    as SFAM [[151](#bib.bib151)] and PRNN [[113](#bib.bib113)] have pioneered the
    research in this direction. The work SFAM [[151](#bib.bib151)] proposed to extract
    scene flow for motion analysis. The strategy of fusing the RGB and depth modalities
    at the outset allowed the capture of rich 3D motion information. In PRNN [[113](#bib.bib113)]
    the concept of privileged information (side information) was introduced for deep
    networks training and showed some promise. Zolfaghari et al. [[182](#bib.bib182)]
    proposed the integration of different modalities via a Markov chain, which leads
    to a sequential refinement of action labels. So far, most methods considered the
    three modalities as separate channels and fused them at a later or scoring stage
    using different fusion methods without cooperatively exploiting their complementary
    properties. Cooperative training using different modalities would be a promising
    research area.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态融合。虽然单独使用RGB、骨架或深度模态已经取得了显著进展，但有效的深度网络用于融合多模态数据将是一个有前景的方向。例如，SFAM[[151](#bib.bib151)]和PRNN[[113](#bib.bib113)]等方法在这一方向上开创了研究。SFAM[[151](#bib.bib151)]提出了提取场景流进行动作分析的策略。最初融合RGB和深度模态的策略允许捕捉丰富的3D动作信息。在PRNN[[113](#bib.bib113)]中，引入了特权信息（附加信息）用于深度网络训练，并显示出一些潜力。Zolfaghari等人[[182](#bib.bib182)]提出通过马尔可夫链整合不同模态，这导致了动作标签的顺序精炼。迄今为止，大多数方法将三种模态视为独立通道，并在后续或评分阶段使用不同的融合方法进行融合，而没有合作利用它们的互补特性。使用不同模态进行协作训练将是一个有前途的研究领域。
- en: Large-scale datasets. With the development of data-hungry deep learning approach,
    there is demand for large scale RGB-D datasets. Even though there are several
    large datasets, such as NTU RGB+D Dataset [[109](#bib.bib109)] and ChaLearn LAP
    IsoGD Dataset [[139](#bib.bib139)], they are focused on specific tasks. Various
    large-scale RGB-D datasets are needed to facilitate research in this field. For
    instance, large-scale fine-grained RGB-D motion recognition datasets and large-scale
    occlusion-based RGB-D motion recognition datasets are urgently needed.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模数据集。随着对数据需求巨大的深度学习方法的发展，对大规模RGB-D数据集的需求不断增加。尽管已有几个大型数据集，如NTU RGB+D数据集[[109](#bib.bib109)]和ChaLearn
    LAP IsoGD数据集[[139](#bib.bib139)]，但它们都集中于特定任务。需要各种大规模RGB-D数据集以促进这一领域的研究。例如，大规模的细粒度RGB-D动作识别数据集和大规模基于遮挡的RGB-D动作识别数据集急需出现。
- en: Zero/One-shot learning. As discussed, it is not always easy to collect large
    scale labeled data. Learning from a few examples remains a key challenge in machine
    learning. Despite recent advances in important domains such as vision and language,
    the standard supervised deep learning paradigm does not offer a satisfactory solution
    for learning new concepts rapidly from little data. How to adopt deep learning
    methods for zero/one shot RGB-D-based motion recognition would be an interesting
    research direction. Zero/one-shot learning is about being able to recognize gesture/action
    classes that are never seen or only one training sample per class before. This
    type of recognition should carry embedded information universal to all other gestures/actions.
    In the past few years, there are some works on zero/one-shot learning. For example,
    Wan et al. [[138](#bib.bib138)] proposed the novel spatial-temporal features for
    one-shot learning gesture recognition and have got promising performances on Chalearn
    Gesture Dataset CGD) [[45](#bib.bib45)]. For zero-shot learning, Madapana and
    Wachs [[88](#bib.bib88)] proposed a new paradigm based on adaptive learning which
    it is possible to determine the amount of transfer learning carried out by the
    algorithm and how much knowledge is acquired for a new gesture observation. However,
    the mentioned works used traditional methods (such as bag of visual words model [[140](#bib.bib140)]).
    Mettes et al. [[92](#bib.bib92)] proposed a spatial-aware object embedding for
    zero-shot action localization and classification. The spatial-aware embedding
    generate action tubes by incorporating word embeddings, box locations for actors
    and objects, as well as their spatial relations. However, how to effectively adopt
    deep learning methods for zero/one shot RGB-D based motion recognition would be
    still an interesting research direction especially when using only very few training
    samples.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本/一样本学习。如前所述，收集大规模标注数据并不总是容易。从少量样本中学习仍然是机器学习中的一个关键挑战。尽管在视觉和语言等重要领域取得了最近的进展，标准的监督深度学习范式仍未能提供从少量数据中快速学习新概念的令人满意的解决方案。如何采用深度学习方法进行零样本/一样本
    RGB-D 基于动作的识别将是一个有趣的研究方向。零样本/一样本学习是指能够识别从未见过的或每个类别仅有一个训练样本的手势/动作类别。这种识别应包含对所有其他手势/动作通用的信息。在过去几年中，有一些关于零样本/一样本学习的研究。例如，Wan
    等人[[138](#bib.bib138)]提出了用于一样本学习手势识别的新型时空特征，并在 Chalearn Gesture Dataset (CGD)[[45](#bib.bib45)]上取得了良好的表现。对于零样本学习，Madapana
    和 Wachs[[88](#bib.bib88)]提出了一种基于自适应学习的新范式，可以确定算法执行的迁移学习量以及新手势观察中获得的知识量。然而，提到的工作使用了传统方法（如视觉单词模型[[140](#bib.bib140)]）。Mettes
    等人[[92](#bib.bib92)]提出了一种空间感知对象嵌入用于零样本动作定位和分类。空间感知嵌入通过结合词嵌入、演员和物体的框位置及其空间关系来生成动作管道。然而，如何有效地采用深度学习方法进行零样本/一样本
    RGB-D 基于动作的识别仍将是一个有趣的研究方向，尤其是在仅使用非常少的训练样本时。
- en: Outdoor practical scenarios. Although lots of RGB-D datasets have been collected
    during the last few years, there is a big gap between the collected datasets and
    wild environment due to constrained environment setting and insufficient categories
    and samples. For example, most available datasets do not involve much occlusion
    cases probably due to the collapse of skeleton dataset in case of occlusion. However,
    in practical scenarios, occlusion is inevitable. How to recover or find cues from
    multi-modal data for such recognition tasks would be an interesting research direction.
    Besides, with the development of depth sensors, further distances could be captured,
    and recognition in outdoor practical scenarios will gain the attention of researchers.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 户外实际场景。尽管在过去几年中收集了大量 RGB-D 数据集，但由于环境设置受限以及类别和样本不足，收集的数据集与自然环境之间存在较大差距。例如，大多数现有数据集未涉及多遮挡情况，这可能是由于遮挡情况下骨架数据集的崩溃。然而，在实际场景中，遮挡是不可避免的。如何从多模态数据中恢复或找到线索以进行这种识别任务将是一个有趣的研究方向。此外，随着深度传感器的发展，可以捕捉更远的距离，户外实际场景中的识别将引起研究人员的关注。
- en: Unsupervised learning/Self-learning. Collecting labeled datasets are time-consuming
    and costly, hence learning from unsupervised video data is required. Mobile robots
    mounted with RGB-D cameras need to continuously learn from the environment and
    without human intervention. How to automatically learn from the unlabeled data
    stream to improve the learning capability of deep networks would be a fruitful
    and useful research direction. Generative Adversarial Net (GAN) [[50](#bib.bib50)]
    has got much processes recently in image generation task, such as face generation,
    text-to-image task. Besides, it also can be used for recognition task. For example,
    Luan et al. [[131](#bib.bib131)] proposed a Disentangled Representation learning
    Generative Adversarial Networks (DR-GAN) for pose-invariant face recognition.
    Therefore, we believe the GAN-based techniques also can be used for action/gesture
    recognition, which is a great exciting direction for research. Carl et al. [[136](#bib.bib136)]
    proposed a generative adversarial network for video with spatial-temporal convolutional
    architecture that untangles the scene’s foreground from backgrounds. This is an
    initial work to capitalize on large amounts of unlabeled video in order to learn
    a model of scene dynamic for both video recognition tasks (e.g. action classification)
    and video generation tasks (e.g. future prediction). Increasing research will
    be reported in the coming years on GAN-based methods for video-based recognition.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习/自我学习。收集标注数据集既耗时又昂贵，因此需要从无监督的视频数据中进行学习。配备RGB-D摄像头的移动机器人需要不断从环境中学习且无需人工干预。如何自动从未标记的数据流中学习以提高深度网络的学习能力将是一个富有成效且有用的研究方向。生成对抗网络（GAN）[[50](#bib.bib50)]
    在图像生成任务中最近取得了许多进展，如人脸生成、文本到图像任务。此外，它也可以用于识别任务。例如，Luan等人[[131](#bib.bib131)] 提出了用于姿态不变人脸识别的解耦表示学习生成对抗网络（DR-GAN）。因此，我们相信基于GAN的技术也可以用于动作/手势识别，这是一个非常令人兴奋的研究方向。Carl等人[[136](#bib.bib136)]
    提出了一个具有时空卷积结构的视频生成对抗网络，该网络将场景的前景与背景分离。这是一个初步的工作，旨在利用大量未标记的视频来学习场景动态模型，以用于视频识别任务（如动作分类）和视频生成任务（如未来预测）。未来几年将有更多关于基于GAN的视频识别方法的研究报告。
- en: Online motion recognition and prediction. Online motion recognition and prediction
    is required in practical applications, and arguably this is the final goal of
    motion recognition systems. Differently from segmented recognition, online motion
    recognition requires the analysis of human behavior in a continuous manner, and
    prediction aims to recognize or anticipate actions that would happen. How to design
    effective online recognition and prediction systems with deep learning methods
    has attracted some attention. For example, Vondrick et al. [[135](#bib.bib135)]
    introduced a framework that capitalizes on temporal structure in unlabeled video
    to learn to anticipate human actions and objects based on CNN, and it is likely
    to emerge as an active research area.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在线动作识别与预测。实际应用中需要在线动作识别与预测，这也可以说是动作识别系统的最终目标。与分段识别不同，在线动作识别要求对人类行为进行连续分析，而预测旨在识别或预测将要发生的动作。如何利用深度学习方法设计有效的在线识别与预测系统已引起一定关注。例如，Vondrick等人[[135](#bib.bib135)]
    引入了一个利用未标记视频中的时间结构来学习预测人类动作和物体的框架，基于CNN，这可能会成为一个活跃的研究领域。
- en: 8 Conclusion
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: 'This paper presents a comprehensive survey of RGB-D based motion recognition
    using deep learning. We provide a brief overview of existing commonly used datasets
    and pointed at surveys that focused mainly on datasets. The available methods
    are grouped into four categories according to the modality: RGB-based, depth-based,
    skeleton-based and RGB+D-based. The three modalities have their own specific features
    and lead to different choices of deep learning methods to take advantages of their
    properties. Spatial, temporal and structural information inherent in a video sequence
    is defined, and from the viewpoint of spatio-temporal-structural encoding, we
    analyse the pros and cons of available methods. Based on the insights drawn from
    the survey, several potential research directions are described, indicating the
    numerous opportunities in this field despite the advances achieved to date.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了基于RGB-D的动作识别的全面综述，利用深度学习。我们简要回顾了现有的常用数据集，并指出了主要关注数据集的综述。根据模态，将现有方法分为四类：基于RGB、基于深度、基于骨架和RGB+D。三种模态具有各自的特点，并导致不同的深度学习方法以利用其特性。定义了视频序列中固有的空间、时间和结构信息，并从时空结构编码的角度分析了现有方法的优缺点。根据从综述中得出的见解，描述了若干潜在的研究方向，尽管迄今已取得进展，但在这一领域仍有众多机会。
- en: Acknowledgment
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Jun Wan is partially supported by the National Natural Science Foundation of
    China [61502491]. Sergio Escalera is partially supported by Spanish project [TIN2016-74946-P]
    (MINECO/FEDER, UE) and CERCA Programme / Generalitat de Catalunya.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Jun Wan 部分由中国国家自然科学基金 [61502491] 支持。Sergio Escalera 部分由西班牙项目 [TIN2016-74946-P]（MINECO/FEDER,
    UE）和CERCA计划/加泰罗尼亚自治区政府支持。
- en: References
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: CMU [2001] , 2001. CMU Graphics Lab Motion Capture Database, http://mocap.cs.cmu.edu/.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMU [2001] , 2001. CMU图形实验室动作捕捉数据库，http://mocap.cs.cmu.edu/。
- en: 'Aggarwal and Cai [1999] Aggarwal, J., Cai, Q., 1999. Human motion analysis:
    A review, in: Computer Vision and Image Understanding.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aggarwal 和 Cai [1999] Aggarwal, J., Cai, Q., 1999. 人体运动分析：综述，见：计算机视觉与图像理解。
- en: 'Aggarwal and Xia [2014] Aggarwal, J.K., Xia, L., 2014. Human activity recognition
    from 3D data: A review. Pattern Recognition Letters 48, 70–80.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aggarwal 和 Xia [2014] Aggarwal, J.K., Xia, L., 2014. 基于3D数据的人类活动识别：综述。模式识别通讯
    48, 70–80。
- en: 'Asadi-Aghbolaghi et al. [2017] Asadi-Aghbolaghi, M., Clapes, A., Bellantonio,
    M., Escalante, H.J., Ponce-López, V., Baró, X., Guyon, I., Kasaei, S., Escalera,
    S., 2017. A survey on deep learning based approaches for action and gesture recognition
    in image sequences, in: Automatic Face & Gesture Recognition (FG 2017), 2017 12th
    IEEE International Conference on, IEEE. pp. 476–483.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asadi-Aghbolaghi 等 [2017] Asadi-Aghbolaghi, M., Clapes, A., Bellantonio, M.,
    Escalante, H.J., Ponce-López, V., Baró, X., Guyon, I., Kasaei, S., Escalera, S.,
    2017. 基于深度学习的方法在图像序列中的动作和手势识别的综述，见：自动面部与姿态识别（FG 2017），2017年第12届IEEE国际会议，IEEE.
    页 476–483。
- en: 'Baccouche et al. [2011] Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., Baskurt,
    A., 2011. Sequential deep learning for human action recognition, in: International
    Workshop on Human Behavior Understanding, Springer. pp. 29–39.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baccouche 等 [2011] Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., Baskurt,
    A., 2011. 序列深度学习用于人类动作识别，见：国际人类行为理解研讨会，Springer. 页 29–39。
- en: 'Bilen et al. [2016] Bilen, H., Fernando, B., Gavves, E., Vedaldi, A., Gould,
    S., 2016. Dynamic image networks for action recognition, in: CVPR.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bilen 等 [2016] Bilen, H., Fernando, B., Gavves, E., Vedaldi, A., Gould, S.,
    2016. 动态图像网络用于动作识别，见：CVPR。
- en: 'Bloom et al. [2012] Bloom, V., Makris, D., Argyriou, V., 2012. G3D: A gaming
    action dataset and real time action recognition evaluation framework, in: CVPRW.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bloom 等 [2012] Bloom, V., Makris, D., Argyriou, V., 2012. G3D：一个游戏动作数据集和实时动作识别评估框架，见：CVPRW。
- en: Bobick and Davis [2001] Bobick, A.F., Davis, J.W., 2001. The recognition of
    human movement using temporal templates. IEEE Transactions on pattern analysis
    and machine intelligence 23, 257–267.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bobick 和 Davis [2001] Bobick, A.F., Davis, J.W., 2001. 使用时间模板的人体运动识别。IEEE模式分析与机器智能汇刊
    23, 257–267。
- en: 'Buch et al. [2017] Buch, S., Escorcia, V., Shen, C., Ghanem, B., Niebles, J.C.,
    2017. Sst: Single-stream temporal action proposals, in: CVPR.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buch 等 [2017] Buch, S., Escorcia, V., Shen, C., Ghanem, B., Niebles, J.C., 2017.
    Sst：单流时间动作提案，见：CVPR。
- en: 'Camgoz et al. [2016] Camgoz, N.C., Hadfield, S., Koller, O., Bowden, R., 2016.
    Using convolutional 3D neural networks for user-independent continuous gesture
    recognition, in: 2016 23rd International Conference on Pattern Recognition (ICPR),
    pp. 49–54.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Camgoz 等 [2016] Camgoz, N.C., Hadfield, S., Koller, O., Bowden, R., 2016. 使用卷积
    3D 神经网络进行用户独立的连续手势识别, 见: 2016 第 23 届国际模式识别会议 (ICPR), 第 49–54 页。'
- en: 'Cao et al. [2010] Cao, L., Liu, Z., Huang, T.S., 2010. Cross-dataset action
    detection, in: Computer vision and pattern recognition (CVPR), IEEE. pp. 1998–2005.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cao 等 [2010] Cao, L., Liu, Z., Huang, T.S., 2010. 跨数据集动作检测, 见: 计算机视觉与模式识别 (CVPR),
    IEEE。第 1998–2005 页。'
- en: 'Chai et al. [2016] Chai, X., Liu, Z., Yin, F., Liu, Z., Chen, X., 2016. Two
    streams recurrent neural networks for large-scale continuous gesture recognition,
    in: International Conference on Pattern Recognition Workshops.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chai 等 [2016] Chai, X., Liu, Z., Yin, F., Liu, Z., Chen, X., 2016. 用于大规模连续手势识别的双流递归神经网络,
    见: 国际模式识别会议研讨会。'
- en: Chen [2010] Chen, B., 2010. Deep learning of invariant spatio-temporal features
    from video. Ph.D. thesis. University of British Columbia.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen [2010] Chen, B., 2010. 从视频中深度学习不变的时空特征。博士学位论文。英属哥伦比亚大学。
- en: Chen et al. [2013] Chen, L., Wei, H., Ferryman, J., 2013. A survey of human
    motion analysis using depth imagery. Pattern Recognition Letters 34, 1995–2006.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2013] Chen, L., Wei, H., Ferryman, J., 2013. 使用深度图像进行人体运动分析的综述。模式识别通讯
    34, 1995–2006。
- en: 'Chen et al. [2017] Chen, Y., Shen, C., Wei, X.S., Liu, L., Yang, J., 2017.
    Adversarial posenet: A structure-aware convolutional network for human pose estimation,
    in: The IEEE International Conference on Computer Vision (ICCV).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2017] Chen, Y., Shen, C., Wei, X.S., Liu, L., Yang, J., 2017. 对抗性姿态网络：一种结构感知的卷积网络用于人体姿态估计,
    见: IEEE 国际计算机视觉会议 (ICCV)。'
- en: Cheng et al. [2016] Cheng, H., Yang, L., Liu, Z., 2016. Survey on 3D hand gesture
    recognition. IEEE Transactions on Circuits and Systems for Video Technology 26,
    1659–1673.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等 [2016] Cheng, H., Yang, L., Liu, Z., 2016. 3D 手势识别的综述。IEEE 视频技术电路与系统学报
    26, 1659–1673。
- en: 'Cherian et al. [2017] Cherian, A., Fernando, B., Harandi, M., Gould, S., 2017.
    Generalized rank pooling for activity recognition, in: CVPR.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cherian 等 [2017] Cherian, A., Fernando, B., Harandi, M., Gould, S., 2017. 活动识别的广义排名池化,
    见: CVPR。'
- en: 'Chéron et al. [2015] Chéron, G., Laptev, I., Schmid, C., 2015. P-cnn: Pose-based
    cnn features for action recognition, in: ICCV, pp. 3218–3226.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chéron 等 [2015] Chéron, G., Laptev, I., Schmid, C., 2015. P-cnn：基于姿态的 cnn 特征用于动作识别,
    见: ICCV, 第 3218–3226 页。'
- en: 'Chunhui et al. [2017] Chunhui, L., Yueyu, H., Yanghao, L., Sijie, S., Jiaying,
    L., 2017. Pku-mmd: A large scale benchmark for continuous multi-modal human action
    understanding. arXiv preprint arXiv:1703.07475 .'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chunhui 等 [2017] Chunhui, L., Yueyu, H., Yanghao, L., Sijie, S., Jiaying, L.,
    2017. Pku-mmd：一个大规模的连续多模态人体动作理解基准。arXiv 预印本 arXiv:1703.07475。
- en: 'Dave et al. [2017] Dave, A., Russakovsky, O., Ramanan, D., 2017. Predictive-corrective
    networks for action detection, in: CVPR.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dave 等 [2017] Dave, A., Russakovsky, O., Ramanan, D., 2017. 用于动作检测的预测-校正网络,
    见: CVPR。'
- en: 'De Geest et al. [2016] De Geest, R., Gavves, E., Ghodrati, A., Li, Z., Snoek,
    C., Tuytelaars, T., 2016. Online action detection, in: ECCV, pp. 269–284.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'De Geest 等 [2016] De Geest, R., Gavves, E., Ghodrati, A., Li, Z., Snoek, C.,
    Tuytelaars, T., 2016. 在线动作检测, 见: ECCV, 第 269–284 页。'
- en: 'Donahue et al. [2015] Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach,
    M., Venugopalan, S., Saenko, K., Darrell, T., 2015. Long-term recurrent convolutional
    networks for visual recognition and description, in: CVPR.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Donahue 等 [2015] Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach,
    M., Venugopalan, S., Saenko, K., Darrell, T., 2015. 用于视觉识别和描述的长期递归卷积网络, 见: CVPR。'
- en: 'Du et al. [2017] Du, W., Wang, Y., Qiao, Y., 2017. Rpan: An end-to-end recurrent
    pose-attention network for action recognition in videos, in: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 3725–3734.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等 [2017] Du, W., Wang, Y., Qiao, Y., 2017. Rpan：一个端到端递归姿态注意网络用于视频中的动作识别,
    见: IEEE 计算机视觉与模式识别会议论文集, 第 3725–3734 页。'
- en: 'Du et al. [2015a] Du, Y., Fu, Y., Wang, L., 2015a. Skeleton based action recognition
    with convolutional neural network, in: Pattern Recognition (ACPR), 2015 3rd IAPR
    Asian Conference on, IEEE. pp. 579–583.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等 [2015a] Du, Y., Fu, Y., Wang, L., 2015a. 基于骨架的动作识别与卷积神经网络, 见: 模式识别 (ACPR),
    2015 第 3 届 IAPR 亚洲会议, IEEE。第 579–583 页。'
- en: Du et al. [2016] Du, Y., Fu, Y., Wang, L., 2016. Representation learning of
    temporal dynamics for skeleton-based action recognition. IEEE Transactions on
    Image Processing 25, 3010–3022.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 [2016] Du, Y., Fu, Y., Wang, L., 2016. 基于骨架的动作识别的时间动态表示学习。IEEE 图像处理学报 25,
    3010–3022。
- en: 'Du et al. [2015b] Du, Y., Wang, W., Wang, L., 2015b. Hierarchical recurrent
    neural network for skeleton based action recognition, in: CVPR.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等人 [2015b] Du, Y., Wang, W., Wang, L., 2015b. 用于骨架基础动作识别的层次递归神经网络, 在: CVPR.'
- en: Duan et al. [2016] Duan, J., Zhou, S., Wan, J., Guo, X., Li, S.Z., 2016. Multi-modality
    fusion based on consensus-voting and 3D convolution for isolated gesture recognition.
    arXiv preprint arXiv:1611.06689 .
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等人 [2016] Duan, J., Zhou, S., Wan, J., Guo, X., Li, S.Z., 2016. 基于共识投票和
    3D 卷积的多模态融合用于孤立手势识别. arXiv 预印本 arXiv:1611.06689.
- en: Escalera et al. [2016] Escalera, S., Athitsos, V., Guyon, I., 2016. Challenges
    in multimodal gesture recognition. Journal of Machine Learning Research 17, 1–54.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Escalera 等人 [2016] Escalera, S., Athitsos, V., Guyon, I., 2016. 多模态手势识别中的挑战.
    机器学习研究杂志 17, 1–54.
- en: 'Escalera et al. [2014] Escalera, S., Baró, X., Gonzalez, J., Bautista, M.A.,
    Madadi, M., Reyes, M., Ponce-López, V., Escalante, H.J., Shotton, J., Guyon, I.,
    2014. Chalearn looking at people challenge 2014: Dataset and results, in: Workshop
    at the European Conference on Computer Vision, Springer. pp. 459–473.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Escalera 等人 [2014] Escalera, S., Baró, X., Gonzalez, J., Bautista, M.A., Madadi,
    M., Reyes, M., Ponce-López, V., Escalante, H.J., Shotton, J., Guyon, I., 2014.
    Chalearn 2014 人物挑战：数据集与结果, 在: 欧洲计算机视觉会议工作坊, Springer. 页码 459–473.'
- en: 'Escorcia et al. [2016] Escorcia, V., Heilbron, F.C., Niebles, J.C., Ghanem,
    B., 2016. Daps: Deep action proposals for action understanding, in: European Conference
    on Computer Vision, Springer. pp. 768–784.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Escorcia 等人 [2016] Escorcia, V., Heilbron, F.C., Niebles, J.C., Ghanem, B.,
    2016. Daps: 深度动作提案用于动作理解, 在: 欧洲计算机视觉会议, Springer. 页码 768–784.'
- en: 'Feichtenhofer et al. [2016a] Feichtenhofer, C., Pinz, A., Wildes, R., 2016a.
    Spatiotemporal residual networks for video action recognition, in: Advances in
    Neural Information Processing Systems, pp. 3468–3476.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feichtenhofer 等人 [2016a] Feichtenhofer, C., Pinz, A., Wildes, R., 2016a. 用于视频动作识别的时空残差网络,
    在: 神经信息处理系统进展, 页码 3468–3476.'
- en: 'Feichtenhofer et al. [2016b] Feichtenhofer, C., Pinz, A., Zisserman, A., 2016b.
    Convolutional two-stream network fusion for video action recognition, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1933–1941.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feichtenhofer 等人 [2016b] Feichtenhofer, C., Pinz, A., Zisserman, A., 2016b.
    用于视频动作识别的卷积双流网络融合, 在: IEEE 计算机视觉与模式识别会议论文集, 页码 1933–1941.'
- en: 'Fernando et al. [2016a] Fernando, B., Anderson, P., Hutter, M., Gould, S.,
    2016a. Discriminative hierarchical rank pooling for activity recognition, in:
    CVPR.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fernando 等人 [2016a] Fernando, B., Anderson, P., Hutter, M., Gould, S., 2016a.
    用于活动识别的判别性层次排名池化, 在: CVPR.'
- en: Fernando et al. [2016b] Fernando, B., Gavves, S., Mogrovejo, O., Antonio, J.,
    Ghodrati, A., Tuytelaars, T., 2016b. Rank pooling for action recognition. IEEE
    Transactions on Pattern Analysis and Machine Intelligence .
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernando 等人 [2016b] Fernando, B., Gavves, S., Mogrovejo, O., Antonio, J., Ghodrati,
    A., Tuytelaars, T., 2016b. 用于动作识别的排名池化. IEEE 模式分析与机器智能学报.
- en: 'Fernando and Gould [2016] Fernando, B., Gould, S., 2016. Learning end-to-end
    video classification with rank-pooling, in: ICML.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fernando 和 Gould [2016] Fernando, B., Gould, S., 2016. 使用排名池化进行端到端视频分类, 在:
    ICML.'
- en: 'Fothergill et al. [2012] Fothergill, S., Mentis, H.M., Nowozin, S., Kohli,
    P., 2012. Instructing people for training gestural interactive systems, in: ACM
    Conference on Computer-Human Interaction (ACM HCI).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fothergill 等人 [2012] Fothergill, S., Mentis, H.M., Nowozin, S., Kohli, P.,
    2012. 指导人们进行手势交互系统的训练, 在: ACM 计算机-人类互动会议 (ACM HCI).'
- en: 'Gao et al. [2017] Gao, J., Yang, Z., Sun, C., Chen, K., Nevatia, R., 2017.
    Turn tap: Temporal unit regression network for temporal action proposals, in:
    ICCV.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等人 [2017] Gao, J., Yang, Z., Sun, C., Chen, K., Nevatia, R., 2017. Turn
    tap: 时间单元回归网络用于时间动作提案, 在: ICCV.'
- en: 'Girshick [2015] Girshick, R., 2015. Fast r-cnn, in: Proceedings of the IEEE
    International Conference on Computer Vision, pp. 1440–1448.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Girshick [2015] Girshick, R., 2015. 快速 R-CNN, 在: IEEE 国际计算机视觉会议论文集, 页码 1440–1448.'
- en: 'Girshick et al. [2014] Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014.
    Rich feature hierarchies for accurate object detection and semantic segmentation,
    in: Computer Vision and Pattern Recognition.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Girshick 等人 [2014] Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014.
    精确物体检测和语义分割的丰富特征层次, 在: 计算机视觉与模式识别会议.'
- en: 'Gkioxari and Malik [2015] Gkioxari, G., Malik, J., 2015. Finding action tubes,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 759–768.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gkioxari 和 Malik [2015] Gkioxari, G., Malik, J., 2015. 寻找动作管道, 在: IEEE 计算机视觉与模式识别会议论文集,
    页码 759–768.'
- en: Goodfellow et al. [2016] Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep
    Learning. MIT Press. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2016] 古德费洛，本吉奥，库维尔，2016. 深度学习。麻省理工学院出版社。[http://www.deeplearningbook.org](http://www.deeplearningbook.org)。
- en: 'Goodfellow et al. [2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial
    nets, in: Advances in neural information processing systems, pp. 2672–2680.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2014] 古德费洛，普吉特-阿巴迪，米尔扎，许，华德-法利，奥赛尔，库维尔，本吉奥，2014. 生成对抗神经网络，见：神经信息处理系统进展，第2672–2680页。
- en: 'Graves et al. [2006] Graves, A., Fernández, S., Gomez, F., Schmidhuber, J.,
    2006. Connectionist temporal classification: labelling unsegmented sequence data
    with recurrent neural networks, in: Proceedings of the 23rd international conference
    on Machine learning, ACM. pp. 369–376.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves et al. [2006] Graves，A.，费尔南德兹，S.，戈麦斯，F.，施密德胡伯，J.，2006. 连接主义时序分类：用循环神经网络对非分割序列数据进行标记，见：第23届国际机器学习会议论文集，ACM。第369–376页。
- en: Guo and Lai [2014] Guo, G., Lai, A., 2014. A survey on still image based human
    action recognition. Pattern Recognition 47, 3343–3361.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭，G.，赖，A.，2014. 基于静止图像的人体动作识别综述。图案识别47，3343–3361。
- en: Guyon et al. [2014] Guyon, I., Athitsos, V., Jangyodsuk, P., Escalante, H.J.,
    2014. The chalearn gesture dataset (CGD 2011). Machine Vision and Applications
    25, 1929–1951.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guyon et al. [2014] Guyon，I.，Athitsos，V.，Jangyodsuk，P.，埃斯卡兰特，H.J.，2014. Chalearn手势数据集（CGD
    2011）。机器视觉与应用25，1929–1951。
- en: 'Han et al. [2017] Han, F., Reily, B., Hoff, W., Zhang, H., 2017. Space-time
    representation of people based on 3D skeletal data: A review. Computer Vision
    and Image Understanding .'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. [2017] 韩，F.，莱利，B.，霍夫，W.，张，H.，2017. 基于3D骨骼数据的人的时空表示：综述。计算机视觉与图像理解。
- en: 'He et al. [2016] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 770–778.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2016] 何，K.，张，X.，任，S.，孙，J.，2016. 深度残差学习用于图像识别，见：IEEE计算机视觉与模式识别大会论文集，第770–778页。
- en: 'Heilbron et al. [2015] Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.,
    2015. Activitynet: A large-scale video benchmark for human activity understanding,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 961–970.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heilbron et al. [2015] Heilbron，F.C.，恩斯科西亚，V.，加内姆，B.，尼布尔斯，J.C.，2015. Activitynet：人类活动理解的大规模视频基准，见：IEEE计算机视觉与模式识别大会论文集，第961–970页。
- en: 'Herath et al. [2017] Herath, S., Harandi, M., Porikli, F., 2017. Going deeper
    into action recognition: A survey. Image and Vision Computing 60, 4–21.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Herath et al. [2017] Herath，S.，Harandi，M.，Porikli，F.，2017. 对动作识别的更深入研究：一项调查。图像与视觉计算60，4–21。
- en: 'Ho and Ermon [2016] Ho, J., Ermon, S., 2016. Generative adversarial imitation
    learning, in: Advances in Neural Information Processing Systems, pp. 4565–4573.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho and Ermon [2016] 何宏，埃尔蒙斯，2016. 对抗性生成模仿学习，见：神经信息处理系统进展，第4565–4573页。
- en: Hoai and De la Torre [2014] Hoai, M., De la Torre, F., 2014. Max-margin early
    event detectors. International Journal of Computer Vision 107, 191–202.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoai and De la Torre [2014] Hoai，M.，De la Torre，F.，2014. 最大间隔早期事件探测器。计算机视觉国际期刊107，191–202。
- en: 'Hou et al. [2017a] Hou, R., Chen, C., Shah, M., 2017a. Tube convolutional neural
    network (t-cnn) for action detection in videos, in: ICCV.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. [2017a] 侯荣，陈诚，沙马，2017a. 用于视频动作检测的Tube卷积神经网络（t-cnn）, 见：国际计算机视觉大会。
- en: 'Hou et al. [2016] Hou, Y., Li, Z., Wang, P., Li, W., 2016. Skeleton optical
    spectra based action recognition using convolutional neural networks, in: Circuits
    and Systems for Video Technology, IEEE Transactions on, pp. 1–5.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. [2016] 侯，Y.，李，Z.，王，P.，李，W.，2016. 基于骨架光谱的动作识别使用卷积神经网络，见：视频技术电路与系统，IEEE交易，第1–5页。
- en: Hou et al. [2017b] Hou, Y., Wang, S., Wang, P., Gao, Z., Li, W., 2017b. Spatially
    and temporally structured global to local aggregation of dynamic depth information
    for action recognition. IEEE Access .
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou et al. [2017b] 侯，Y.，王，S.，王，P.，高，Z.，李，W.，2017b. 动作识别的空间和时间结构化全局到本地聚合的动态深度信息。IEEE通讯。
- en: 'Huang et al. [2016] Huang, D.A., Fei-Fei, L., Niebles, J.C., 2016. Connectionist
    temporal modeling for weakly supervised action labeling, in: European Conference
    on Computer Vision, Springer. pp. 137–153.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2016] 黄大A，费 – 菲，尼布尔斯，J.C.，2016. 用于弱监督动作标签的连接主义时间建模, 见：欧洲计算机视觉大会，斯普林格。第137–153页。
- en: 'Huang et al. [2017] Huang, Z., Wan, C., Probst, T., Van Gool, L., 2017. Deep
    learning on lie groups for skeleton-based action recognition, in: CVPR.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等 [2017] Huang, Z., Wan, C., Probst, T., Van Gool, L., 2017. 基于Lie群的骨架动作识别深度学习，见：CVPR。
- en: Ijjina and Krishna Mohan [2016] Ijjina, E.P., Krishna Mohan, C., 2016. Classification
    of human actions using pose-based features and stacked auto encoder. Pattern Recognition
    Letters 83, 268–277.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ijjina和Krishna Mohan [2016] Ijjina, E.P., Krishna Mohan, C., 2016. 基于姿态特征和堆叠自编码器的人类动作分类。《模式识别通讯》83,
    268–277。
- en: 'Insafutdinov et al. [2016] Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
    M., Schiele, B., 2016. Deepercut: A deeper, stronger, and faster multi-person
    pose estimation model, in: European Conference on Computer Vision, Springer. pp.
    34–50.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Insafutdinov等 [2016] Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
    M., Schiele, B., 2016. Deepercut: 更深、更强、更快的多人体姿态估计模型，见：欧洲计算机视觉会议，Springer. 第34–50页。'
- en: 'Jain et al. [2016] Jain, A., Zamir, A.R., Savarese, S., Saxena, A., 2016. Structural-rnn:
    Deep learning on spatio-temporal graphs, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 5308–5317.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jain等 [2016] Jain, A., Zamir, A.R., Savarese, S., Saxena, A., 2016. Structural-rnn:
    在时空图上的深度学习，见：IEEE计算机视觉与模式识别会议论文集，第5308–5317页。'
- en: 'Jhuang et al. [2007] Jhuang, H., Serre, T., Wolf, L., Poggio, T., 2007. A biologically
    inspired system for action recognition, in: Proc. IEEE 11th International Conference
    on Computer Vision, pp. 1–8.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jhuang等 [2007] Jhuang, H., Serre, T., Wolf, L., Poggio, T., 2007. 一种生物启发的动作识别系统，见：IEEE第11届国际计算机视觉大会论文集，第1–8页。
- en: Ji et al. [2013] Ji, S., Xu, W., Yang, M., Yu, K., 2013. 3D convolutional neural
    networks for human action recognition. TPAMI 35, 221–231.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji等 [2013] Ji, S., Xu, W., Yang, M., Yu, K., 2013. 用于人类动作识别的3D卷积神经网络。《TPAMI》35,
    221–231。
- en: Jiang et al. [2015] Jiang, F., Zhang, S., Wu, S., Gao, Y., Zhao, D., 2015. Multi-layered
    gesture recognition with Kinect. Journal of Machine Learning Research 16, 227–254.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等 [2015] Jiang, F., Zhang, S., Wu, S., Gao, Y., Zhao, D., 2015. 基于Kinect的多层手势识别。《机器学习研究杂志》16,
    227–254。
- en: 'Kalogeiton et al. [2017] Kalogeiton, V., Weinzaepfel, P., Ferrari, V., Schmid,
    C., 2017. Action tubelet detector for spatio-temporal action localization, in:
    ICCV.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalogeiton等 [2017] Kalogeiton, V., Weinzaepfel, P., Ferrari, V., Schmid, C.,
    2017. 用于时空动作定位的动作管道检测器，见：ICCV。
- en: 'Karpathy et al. [2014] Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar,
    R., Fei-Fei, L., 2014. Large-scale video classification with convolutional neural
    networks, in: Proc. IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 1725–1732.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy等 [2014] Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar,
    R., Fei-Fei, L., 2014. 使用卷积神经网络的大规模视频分类，见：IEEE计算机视觉与模式识别会议（CVPR）论文集，第1725–1732页。
- en: 'Ke et al. [2017a] Ke, Q., An, S., Bennamoun, M., Sohel, F., Boussaid, F., 2017a.
    Skeletonnet: Mining deep part features for 3D action recognition. IEEE Signal
    Processing Letters .'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ke等 [2017a] Ke, Q., An, S., Bennamoun, M., Sohel, F., Boussaid, F., 2017a.
    Skeletonnet: 挖掘用于3D动作识别的深层部件特征。《IEEE信号处理快报》。'
- en: 'Ke et al. [2017b] Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F., 2017b.
    A new representation of skeleton sequences for 3D action recognition, in: CVPR.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke等 [2017b] Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F., 2017b. 用于3D动作识别的骨架序列新表示，见：CVPR。
- en: 'Kim and Reiter [2017] Kim, T.S., Reiter, A., 2017. Interpretable 3D human action
    analysis with temporal convolutional networks, in: CVPR.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim和Reiter [2017] Kim, T.S., Reiter, A., 2017. 使用时间卷积网络的可解释3D人类动作分析，见：CVPR。
- en: 'Krizhevsky et al. [2012] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    Imagenet classification with deep convolutional neural networks, in: Proc. Annual
    Conference on Neural Information Processing Systems (NIPS), pp. 1106–1114.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等 [2012] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. 使用深度卷积神经网络的Imagenet分类，见：神经信息处理系统年会（NIPS）论文集，第1106–1114页。
- en: 'Lan et al. [2014] Lan, T., Chen, T.C., Savarese, S., 2014. A hierarchical representation
    for future action prediction, in: European Conference on Computer Vision, Springer.
    pp. 689–704.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan等 [2014] Lan, T., Chen, T.C., Savarese, S., 2014. 用于未来动作预测的分层表示，见：欧洲计算机视觉会议，Springer.
    第689–704页。
- en: 'Le et al. [2011] Le, Q.V., Zou, W.Y., Yeung, S.Y., Ng, A.Y., 2011. Learning
    hierarchical invariant spatio-temporal features for action recognition with independent
    subspace analysis, in: Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
    Conference on, IEEE. pp. 3361–3368.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le等 [2011] Le, Q.V., Zou, W.Y., Yeung, S.Y., Ng, A.Y., 2011. 通过独立子空间分析学习用于动作识别的层次不变时空特征，见：计算机视觉与模式识别（CVPR），2011
    IEEE会议，IEEE. 第3361–3368页。
- en: 'Lea et al. [2017] Lea, C., Flynn, M.D., Vidal, R., Reiter, A., Hager, G.D.,
    2017. Temporal convolutional networks for action segmentation and detection, in:
    CVPR.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lea et al. [2017] Lea, C., Flynn, M.D., Vidal, R., Reiter, A., Hager, G.D.,
    2017. 动作分割与检测的时间卷积网络，见：CVPR.
- en: 'Lea et al. [2016] Lea, C., Reiter, A., Vidal, R., Hager, G.D., 2016. Segmental
    spatiotemporal CNNs for fine-grained action segmentation, in: European Conference
    on Computer Vision, Springer. pp. 36–52.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lea et al. [2016] Lea, C., Reiter, A., Vidal, R., Hager, G.D., 2016. 用于精细动作分割的分段时空卷积神经网络，见：欧洲计算机视觉大会，Springer.
    页 36–52.
- en: 'Lee et al. [2017] Lee, I., Kim, D., Kang, S., Lee, S., 2017. Ensemble deep
    learning for skeleton-based action recognition using temporal sliding lstm networks,
    in: ICCV, pp. 1012–1020.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2017] Lee, I., Kim, D., Kang, S., Lee, S., 2017. 基于骨架的动作识别的集成深度学习，使用时间滑动
    LSTM 网络，见：ICCV，页 1012–1020.
- en: 'Lev et al. [2016] Lev, G., Sadeh, G., Klein, B., Wolf, L., 2016. Rnn fisher
    vectors for action recognition and image annotation, in: European Conference on
    Computer Vision, Springer. pp. 833–850.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lev et al. [2016] Lev, G., Sadeh, G., Klein, B., Wolf, L., 2016. 用于动作识别和图像注释的
    RNN Fisher 向量，见：欧洲计算机视觉大会，Springer. 页 833–850.
- en: Li et al. [2017a] Li, C., Hou, Y., Wang, P., Li, W., 2017a. Joint distance maps
    based action recognition with convolutional neural networks. IEEE Signal Processing
    Letters 24, 624–628.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2017a] Li, C., Hou, Y., Wang, P., Li, W., 2017a. 基于联合距离图的动作识别与卷积神经网络.
    IEEE 信号处理快报 24, 624–628.
- en: 'Li et al. [2017b] Li, W., Wen, L., Chang, M.C., Nam Lim, S., Lyu, S., 2017b.
    Adaptive rnn tree for large-scale human action recognition, in: Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1444–1452.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2017b] Li, W., Wen, L., Chang, M.C., Nam Lim, S., Lyu, S., 2017b.
    用于大规模人体动作识别的自适应 RNN 树，见：IEEE 计算机视觉与模式识别会议论文集，页 1444–1452.
- en: 'Li et al. [2010] Li, W., Zhang, Z., Liu, Z., 2010. Action recognition based
    on a bag of 3D points, in: CVPRW.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2010] Li, W., Zhang, Z., Liu, Z., 2010. 基于 3D 点袋的动作识别，见：CVPRW.
- en: 'Li et al. [2016a] Li, Y., Lan, C., Xing, J., Zeng, W., Yuan, C., Liu, J., 2016a.
    Online human action detection using joint classification-regression recurrent
    neural networks, in: European Conference on Computer Vision, Springer. pp. 203–220.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2016a] Li, Y., Lan, C., Xing, J., Zeng, W., Yuan, C., Liu, J., 2016a.
    使用联合分类-回归递归神经网络的在线人体动作检测，见：欧洲计算机视觉大会，Springer. 页 203–220.
- en: 'Li et al. [2016b] Li, Y., Miao, Q., Tian, K., Fan, Y., Xu, X., Li, R., Song,
    J., 2016b. Large-scale gesture recognition with a fusion of RGB-D data based on
    the C3D model, in: Pattern Recognition (ICPR), 2016 23rd International Conference
    on, IEEE. pp. 25–30.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2016b] Li, Y., Miao, Q., Tian, K., Fan, Y., Xu, X., Li, R., Song,
    J., 2016b. 基于 C3D 模型的 RGB-D 数据融合的大规模手势识别，见：图案识别（ICPR），2016 第 23 届国际会议，IEEE. 页
    25–30.
- en: 'Li et al. [2016c] Li, Y., Miao, Q., Tian, K., Fan, Y., Xu, X., Li, R., Song.,
    J., 2016c. Large-scale gesture recognition with a fusion of RGB-D data based on
    the C3D model, in: Proceedings of ICPRW.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2016c] Li, Y., Miao, Q., Tian, K., Fan, Y., Xu, X., Li, R., Song,
    J., 2016c. 基于 C3D 模型的 RGB-D 数据融合的大规模手势识别，见：ICP会议论文集.
- en: 'Liu et al. [2016a] Liu, J., Shahroudy, A., Xu, D., Wang, G., 2016a. Spatio-temporal
    LSTM with trust gates for 3D human action recognition, in: ECCV.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2016a] Liu, J., Shahroudy, A., Xu, D., Wang, G., 2016a. 带有信任门的时空
    LSTM 用于 3D 人体动作识别，见：ECCV.
- en: 'Liu et al. [2017a] Liu, J., Wang, G., Hu, P., Duan, L.Y., Kot, A.C., 2017a.
    Global context-aware attention lstm networks for 3D action recognition, in: CVPR.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2017a] Liu, J., Wang, G., Hu, P., Duan, L.Y., Kot, A.C., 2017a.
    用于 3D 动作识别的全局上下文感知注意力 LSTM 网络，见：CVPR.
- en: Liu et al. [2017b] Liu, M., Liu, H., Chen, C., 2017b. Enhanced skeleton visualization
    for view invariant human action recognition. Pattern Recognition 68, 346–362.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2017b] Liu, M., Liu, H., Chen, C., 2017b. 增强的骨架可视化用于视角不变的人体动作识别.
    图案识别 68, 346–362.
- en: 'Liu et al. [2016b] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.,
    Fu, C.Y., Berg, A.C., 2016b. Ssd: Single shot multibox detector, in: ECCV, Springer.
    pp. 21–37.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2016b] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.,
    Fu, C.Y., Berg, A.C., 2016b. SSD: 单次多框检测器，见：ECCV，Springer. 页 21–37.'
- en: Liu et al. [2016c] Liu, Z., Zhang, C., Tian, Y., 2016c. 3D-based deep convolutional
    neural network for action recognition with depth sequences. Image and Vision Computing
    55, 93–100.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2016c] Liu, Z., Zhang, C., Tian, Y., 2016c. 基于深度卷积神经网络的 3D 动作识别与深度序列.
    图像与视觉计算 55, 93–100.
- en: 'Luo et al. [2017] Luo, Z., Peng, B., Huang, D.A., Alahi, A., Fei-Fei, L., 2017.
    Unsupervised learning of long-term motion dynamics for videos, in: CVPR.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. [2017] Luo, Z., Peng, B., Huang, D.A., Alahi, A., Fei-Fei, L., 2017.
    视频长时态运动动态的无监督学习，见：CVPR.
- en: 'Ma et al. [2016] Ma, S., Sigal, L., Sclaroff, S., 2016. Learning activity progression
    in lstms for activity detection and early detection, in: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 1942–1950.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等人 [2016] Ma, S., Sigal, L., Sclaroff, S., 2016. 在 LSTM 中学习活动进展以进行活动检测和早期检测，见：IEEE
    计算机视觉与模式识别会议论文集，第 1942–1950 页。
- en: 'Madapana and Wachs [2017] Madapana, N., Wachs, J.P., 2017. A semantical & analytical
    approach for zero shot gesture learning, in: FG workshop.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madapana 和 Wachs [2017] Madapana, N., Wachs, J.P., 2017. 零样本手势学习的语义与分析方法，见：FG
    研讨会。
- en: 'Mahasseni and Todorovic [2016] Mahasseni, B., Todorovic, S., 2016. Regularizing
    long short term memory with 3D human-skeleton sequences for action recognition,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 3054–3062.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahasseni 和 Todorovic [2016] Mahasseni, B., Todorovic, S., 2016. 使用 3D 人体骨架序列正则化长短期记忆网络以进行动作识别，见：IEEE
    计算机视觉与模式识别会议论文集，第 3054–3062 页。
- en: 'Mathieu et al. [2016] Mathieu, M., Couprie, C., LeCun, Y., 2016. Deep multi-scale
    video prediction beyond mean square error, in: Proc. International Conference
    on Learning Representations (ICLR).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mathieu 等人 [2016] Mathieu, M., Couprie, C., LeCun, Y., 2016. 超越均方误差的深度多尺度视频预测，见：国际学习表征会议
    (ICLR) 论文集。
- en: 'Memisevic and Hinton [2007] Memisevic, R., Hinton, G., 2007. Unsupervised learning
    of image transformations, in: Computer Vision and Pattern Recognition, IEEE Conference
    on, IEEE. pp. 1–8.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Memisevic 和 Hinton [2007] Memisevic, R., Hinton, G., 2007. 无监督学习图像变换，见：计算机视觉与模式识别，IEEE
    会议，IEEE，第 1–8 页。
- en: 'Mettes and Snoek [2017] Mettes, P., Snoek, C.G., 2017. Spatial-aware object
    embeddings for zero-shot localization and classification of actions, in: ICCV.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mettes 和 Snoek [2017] Mettes, P., Snoek, C.G., 2017. 空间感知对象嵌入用于零样本定位和动作分类，见：ICCV。
- en: 'Molchanov et al. [2016] Molchanov, P., Yang, X., Gupta, S., Kim, K., Tyree,
    S., Kautz, J., 2016. Online detection and classification of dynamic hand gestures
    with recurrent 3D convolutional neural network, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4207–4215.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov 等人 [2016] Molchanov, P., Yang, X., Gupta, S., Kim, K., Tyree, S.,
    Kautz, J., 2016. 使用递归 3D 卷积神经网络在线检测和分类动态手势，见：IEEE 计算机视觉与模式识别会议论文集，第 4207–4215
    页。
- en: Müller et al. [2007] Müller, M., Röder, T., Clausen, M., Eberhardt, B., Krüger,
    B., Weber, A., 2007. Documentation Mocap Database HDM05. Technical Report CG-2007-2.
    Universität Bonn.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Müller 等人 [2007] Müller, M., Röder, T., Clausen, M., Eberhardt, B., Krüger,
    B., Weber, A., 2007. 文档 Mocap 数据库 HDM05。技术报告 CG-2007-2. 波恩大学。
- en: 'Ng et al. [2015] Ng, J., Hausknecht, M., Vijayanarasimhan, S., Vinyals, O.,
    Monga, R., Toderici, G., 2015. Beyond short snippets: Deep networks for video
    classification, in: CVPR.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 等人 [2015] Ng, J., Hausknecht, M., Vijayanarasimhan, S., Vinyals, O., Monga,
    R., Toderici, G., 2015. 超越短片段：用于视频分类的深度网络，见：CVPR。
- en: 'Ofli et al. [2013] Ofli, F., Chaudhry, R., Kurillo, G., Vidal, R., Bajcsy,
    R., 2013. Berkeley MHAD: A comprehensive multimodal human action database, in:
    Proc. IEEE Workshop on Applications of Computer Vision (WACV), pp. 53–60.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ofli 等人 [2013] Ofli, F., Chaudhry, R., Kurillo, G., Vidal, R., Bajcsy, R., 2013.
    Berkeley MHAD：一个综合性的多模态人类动作数据库，见：IEEE 计算机视觉应用研讨会 (WACV) 论文集，第 53–60 页。
- en: 'Peng and Schmid [2016] Peng, X., Schmid, C., 2016. Multi-region two-stream
    r-cnn for action detection, in: European Conference on Computer Vision, Springer.
    pp. 744–759.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 和 Schmid [2016] Peng, X., Schmid, C., 2016. 用于动作检测的多区域双流 R-CNN，见：欧洲计算机视觉会议，Springer，第
    744–759 页。
- en: 'Pigou et al. [2016] Pigou, L., Van Den Oord, A., Dieleman, S., Van Herreweghe,
    M., Dambre, J., 2016. Beyond temporal pooling: Recurrence and temporal convolutions
    for gesture recognition in video. International Journal of Computer Vision , 1–10.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pigou 等人 [2016] Pigou, L., Van Den Oord, A., Dieleman, S., Van Herreweghe, M.,
    Dambre, J., 2016. 超越时间池化：用于视频中手势识别的递归和时间卷积。《计算机视觉国际期刊》，第 1–10 页。
- en: Poppe [2010] Poppe, R., 2010. A survey on vision-based human action recognition.
    Image and vision computing 28, 976–990.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poppe [2010] Poppe, R., 2010. 基于视觉的人类动作识别调查。图像与视觉计算 28, 976–990。
- en: 'Presti and La Cascia [2016] Presti, L.L., La Cascia, M., 2016. 3D skeleton-based
    human action classification: A survey. Pattern Recognition 53, 130–147.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Presti 和 La Cascia [2016] Presti, L.L., La Cascia, M., 2016. 基于 3D 骨架的人类动作分类：一项综述。模式识别
    53, 130–147。
- en: 'Rahmani and Bennamoun [2017] Rahmani, H., Bennamoun, M., 2017. Learning action
    recognition model from depth and skeleton videos, in: ICCV, pp. 5832–5841.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahmani 和 Bennamoun [2017] Rahmani, H., Bennamoun, M., 2017. 从深度和骨架视频中学习动作识别模型，见：ICCV，第
    5832–5841 页。
- en: 'Rahmani and Mian [2016] Rahmani, H., Mian, A., 2016. 3D action recognition
    from novel viewpoints, in: Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 1506–1515.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rahmani 和 Mian [2016] Rahmani, H., Mian, A., 2016. 从新视角进行 3D 动作识别, 见: IEEE
    计算机视觉与模式识别会议论文集, 第1506–1515页。'
- en: 'Ren et al. [2015] Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn:
    Towards real-time object detection with region proposal networks, in: Advances
    in neural information processing systems, pp. 91–99.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等人 [2015] Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: 通过区域提议网络实现实时目标检测,
    见: Advances in neural information processing systems, 第91–99页。'
- en: 'Ryoo [2011] Ryoo, M.S., 2011. Human activity prediction: Early recognition
    of ongoing activities from streaming videos, in: Computer Vision (ICCV), 2011
    IEEE International Conference on, IEEE. pp. 1036–1043.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ryoo [2011] Ryoo, M.S., 2011. 人类活动预测: 从流媒体视频中早期识别正在进行的活动, 见: Computer Vision
    (ICCV), 2011 IEEE 国际会议, IEEE. 第1036–1043页。'
- en: 'Sadegh Aliakbarian et al. [2017] Sadegh Aliakbarian, M., Sadat Saleh, F., Salzmann,
    M., Fernando, B., Petersson, L., Andersson, L., 2017. Encouraging lstms to anticipate
    actions very early, in: ICCV, pp. 280–289.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sadegh Aliakbarian 等人 [2017] Sadegh Aliakbarian, M., Sadat Saleh, F., Salzmann,
    M., Fernando, B., Petersson, L., Andersson, L., 2017. 鼓励 LSTMs 早期预测动作, 见: ICCV,
    第280–289页。'
- en: 'Saha et al. [2017] Saha, S., Singh, G., Cuzzolin, F., 2017. Amtnet: Action-micro-tube
    regression by end-to-end trainable deep architecture, in: ICCV.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saha 等人 [2017] Saha, S., Singh, G., Cuzzolin, F., 2017. AMTNet: 通过端到端可训练深度架构进行动作微管回归,
    见: ICCV。'
- en: Salakhutdinov et al. [2013] Salakhutdinov, R., Tenenbaum, J.B., Torralba, A.,
    2013. Learning with hierarchical-deep models. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 35, 1958–1971.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salakhutdinov 等人 [2013] Salakhutdinov, R., Tenenbaum, J.B., Torralba, A., 2013.
    使用层次化深度模型进行学习。IEEE Transactions on Pattern Analysis and Machine Intelligence 35,
    1958–1971。
- en: 'Sánchez et al. [2013] Sánchez, J., Perronnin, F., Mensink, T., Verbeek, J.,
    2013. Image classification with the fisher vector: Theory and practice. International
    journal of computer vision 105, 222–245.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sánchez 等人 [2013] Sánchez, J., Perronnin, F., Mensink, T., Verbeek, J., 2013.
    使用费舍尔向量的图像分类: 理论与实践。国际计算机视觉期刊 105, 222–245。'
- en: 'Shahroudy et al. [2016] Shahroudy, A., Liu, J., Ng, T.T., Wang, G., 2016. NTU
    RGB+ D: A large scale dataset for 3D human activity analysis, in: CVPR.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shahroudy 等人 [2016] Shahroudy, A., Liu, J., Ng, T.T., Wang, G., 2016. NTU RGB+
    D: 用于 3D 人类活动分析的大规模数据集, 见: CVPR。'
- en: Shahroudy et al. [2017] Shahroudy, A., Ng, T.T., Gong, Y., Wang, G., 2017. Deep
    multimodal feature analysis for action recognition in rgb+ d videos. IEEE Transactions
    on Pattern Analysis and Machine Intelligence .
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shahroudy 等人 [2017] Shahroudy, A., Ng, T.T., Gong, Y., Wang, G., 2017. 针对 RGB+
    D 视频中的动作识别的深度多模态特征分析。IEEE Transactions on Pattern Analysis and Machine Intelligence。
- en: Sharma et al. [2016] Sharma, S., Kiros, R., Salakhutdinov, R., 2016. Action
    recognition using visual attention. ICLRW .
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 [2016] Sharma, S., Kiros, R., Salakhutdinov, R., 2016. 使用视觉注意力进行动作识别。ICLRW。
- en: 'Shi et al. [2017] Shi, Y., Tian, Y., Wang, Y., Zeng, W., Huang, T., 2017. Learning
    long-term dependencies for action recognition with a biologically-inspired deep
    network, in: ICCV, pp. 716–725.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等人 [2017] Shi, Y., Tian, Y., Wang, Y., Zeng, W., Huang, T., 2017. 使用生物启发的深度网络学习长期依赖性进行动作识别,
    见: ICCV, 第716–725页。'
- en: 'Shi and Kim [2017] Shi, Z., Kim, T.K., 2017. Learning and refining of privileged
    information-based rnns for action recognition from depth sequences, in: Proc.
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR).'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 和 Kim [2017] Shi, Z., Kim, T.K., 2017. 基于特权信息的 RNN 学习与优化用于深度序列中的动作识别, 见:
    Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)。'
- en: 'Shotton et al. [2011] Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio,
    M., Moore, R., Kipman, A., Blake, A., 2011. Real-time human pose recognition in
    parts from single depth images, in: Proc. IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR), pp. 1297–1304.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shotton 等人 [2011] Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio,
    M., Moore, R., Kipman, A., Blake, A., 2011. 从单一深度图像中实时识别部位的人体姿势, 见: Proc. IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), 第1297–1304页。'
- en: 'Shou et al. [2016] Shou, Z., Wang, D., Chang, S.F., 2016. Temporal action localization
    in untrimmed videos via multi-stage cnns, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 1049–1058.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shou 等人 [2016] Shou, Z., Wang, D., Chang, S.F., 2016. 通过多阶段 CNN 在未裁剪视频中进行时间动作定位,
    见: IEEE 计算机视觉与模式识别会议论文集, 第1049–1058页。'
- en: 'Simonyan and Zisserman [2014a] Simonyan, K., Zisserman, A., 2014a. Two-stream
    convolutional networks for action recognition in videos, in: NIPS.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Simonyan 和 Zisserman [2014a] Simonyan, K., Zisserman, A., 2014a. 用于视频中动作识别的双流卷积网络,
    见: NIPS。'
- en: Simonyan and Zisserman [2014b] Simonyan, K., Zisserman, A., 2014b. Very deep
    convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556
    .
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman [2014b] Simonyan, K., Zisserman, A., 2014b. 用于大规模图像识别的非常深的卷积网络。arXiv
    预印本 arXiv:1409.1556。
- en: 'Singh et al. [2016] Singh, B., Marks, T.K., Jones, M., Tuzel, O., Shao, M.,
    2016. A multi-stream bi-directional recurrent neural network for fine-grained
    action detection, in: Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 1961–1970.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人 [2016] Singh, B., Marks, T.K., Jones, M., Tuzel, O., Shao, M., 2016.
    用于细粒度动作检测的多流双向递归神经网络，见：IEEE 计算机视觉与模式识别会议论文集，页 1961–1970。
- en: 'Singh et al. [2017] Singh, G., Saha, S., Sapienza, M., Torr, P., Cuzzolin,
    F., 2017. Online real-time multiple spatiotemporal action localisation and prediction,
    in: ICCV.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人 [2017] Singh, G., Saha, S., Sapienza, M., Torr, P., Cuzzolin, F., 2017.
    在线实时多时空动作定位与预测，见：ICCV。
- en: 'Song et al. [2017] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J., 2017. An
    end-to-end spatio-temporal attention model for human action recognition from skeleton
    data, in: Thirty-First AAAI Conference on Artificial Intelligence.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2017] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J., 2017. 从骨架数据中进行人类动作识别的端到端时空注意力模型，见：第
    Thirty-First 届 AAAI 人工智能会议。
- en: 'Srivastava et al. [2014] Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever,
    I., Salakhutdinov, R., 2014. Dropout: a simple way to prevent neural networks
    from overfitting. Journal of Machine Learning Research 15, 1929–1958.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等人 [2014] Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever,
    I., Salakhutdinov, R., 2014. Dropout：防止神经网络过拟合的简单方法。机器学习研究杂志 15，页 1929–1958。
- en: 'Srivastava et al. [2015] Srivastava, N., Mansimov, E., Salakhudinov, R., 2015.
    Unsupervised learning of video representations using lstms, in: ICML, pp. 843–852.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等人 [2015] Srivastava, N., Mansimov, E., Salakhudinov, R., 2015. 使用
    LSTM 的视频表示无监督学习，见：ICML，页 843–852。
- en: 'Sultani and Saleemi [2014] Sultani, W., Saleemi, I., 2014. Human action recognition
    across datasets by foreground-weighted histogram decomposition, in: IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 764–771.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sultani 和 Saleemi [2014] Sultani, W., Saleemi, I., 2014. 通过前景加权直方图分解进行跨数据集的人类动作识别，见：IEEE
    计算机视觉与模式识别会议 (CVPR)，页 764–771。
- en: Sun et al. [2017] Sun, L., Jia, K., Chen, K., Yeung, D.Y., Shi, B.E., Savarese,
    S., 2017. Lattice long short-term memory for human action recognition. arXiv preprint
    arXiv:1708.03958 .
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2017] Sun, L., Jia, K., Chen, K., Yeung, D.Y., Shi, B.E., Savarese,
    S., 2017. 用于人类动作识别的格状长短期记忆网络。arXiv 预印本 arXiv:1708.03958。
- en: 'Sun et al. [2015] Sun, L., Jia, K., Yeung, D.Y., Shi, B.E., 2015. Human action
    recognition using factorized spatio-temporal convolutional networks, in: Proceedings
    of the IEEE International Conference on Computer Vision, pp. 4597–4605.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2015] Sun, L., Jia, K., Yeung, D.Y., Shi, B.E., 2015. 使用分解时空卷积网络的人类动作识别，见：IEEE
    国际计算机视觉会议论文集，页 4597–4605。
- en: 'Taylor et al. [2010] Taylor, G.W., Fergus, R., LeCun, Y., Bregler, C., 2010.
    Convolutional learning of spatio-temporal features, in: Proc. European Conference
    on Computer Vision (ECCV), pp. 140–153.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taylor 等人 [2010] Taylor, G.W., Fergus, R., LeCun, Y., Bregler, C., 2010. 时空特征的卷积学习，见：欧洲计算机视觉会议
    (ECCV) 论文集，页 140–153。
- en: 'Teh et al. [2004] Teh, Y.W., Jordan, M.I., Beal, M.J., Blei, D.M., 2004. Sharing
    clusters among related groups: Hierarchical dirichlet processes., in: NIPS, pp.
    1385–1392.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teh 等人 [2004] Teh, Y.W., Jordan, M.I., Beal, M.J., Blei, D.M., 2004. 相关组之间的簇共享：层次狄利克雷过程，见：NIPS，页
    1385–1392。
- en: 'Theis [2007] Theis, F.J., 2007. Towards a general independent subspace analysis,
    in: Advances in Neural Information Processing Systems, pp. 1361–1368.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theis [2007] Theis, F.J., 2007. 面向通用独立子空间分析，见：神经信息处理系统进展，页 1361–1368。
- en: 'Toshev and Szegedy [2014] Toshev, A., Szegedy, C., 2014. Deeppose: Human pose
    estimation via deep neural networks, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 1653–1660.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toshev 和 Szegedy [2014] Toshev, A., Szegedy, C., 2014. Deeppose：通过深度神经网络进行人体姿态估计，见：IEEE
    计算机视觉与模式识别会议论文集，页 1653–1660。
- en: 'Tran et al. [2015] Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri,
    M., 2015. Learning spatiotemporal features with 3D convolutional networks, in:
    ICCV.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等人 [2015] Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.,
    2015. 使用 3D 卷积网络学习时空特征，见：ICCV。
- en: 'Tran et al. [2017] Tran, L., Yin, X., Liu, X., 2017. Disentangled representation
    learning gan for pose-invariant face recognition, in: CVPR, p. 7.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等人 [2017] Tran, L., Yin, X., Liu, X., 2017. 用于姿态不变面部识别的解耦表示学习 GAN，见：CVPR，第
    7 页。
- en: 'Turaga et al. [2008] Turaga, P., Chellappa, R., Subrahmanian, V.S., Udrea,
    O., 2008. Machine recognition of human activities: A survey. IEEE Transactions
    on Circuits and Systems for Video Technology 18, 1473–1488.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turaga 等人 [2008] Turaga, P., Chellappa, R., Subrahmanian, V.S., Udrea, O., 2008.
    机器识别人体活动：综述。IEEE 电路与系统视频技术学报 18, 1473–1488。
- en: Varol et al. [2016] Varol, G., Laptev, I., Schmid, C., 2016. Long-term temporal
    convolutions for action recognition. arXiv preprint arXiv:1604.04494 .
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Varol 等人 [2016] Varol, G., Laptev, I., Schmid, C., 2016. 用于动作识别的长期时间卷积。arXiv
    预印本 arXiv:1604.04494。
- en: 'Veeriah et al. [2015] Veeriah, V., Zhuang, N., Qi, G.J., 2015. Differential
    recurrent neural networks for action recognition, in: ICCV.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veeriah 等人 [2015] Veeriah, V., Zhuang, N., Qi, G.J., 2015. 用于动作识别的差分递归神经网络，见：ICCV。
- en: 'Vondrick et al. [2016a] Vondrick, C., Pirsiavash, H., Torralba, A., 2016a.
    Anticipating visual representations from unlabeled video, in: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 98–106.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vondrick 等人 [2016a] Vondrick, C., Pirsiavash, H., Torralba, A., 2016a. 从未标记的视频中预测视觉表示，见：IEEE
    计算机视觉与模式识别会议论文集, pp. 98–106。
- en: 'Vondrick et al. [2016b] Vondrick, C., Pirsiavash, H., Torralba, A., 2016b.
    Generating videos with scene dynamics, in: Advances In Neural Information Processing
    Systems, pp. 613–621.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vondrick 等人 [2016b] Vondrick, C., Pirsiavash, H., Torralba, A., 2016b. 生成具有场景动态的视频，见：神经信息处理系统进展,
    pp. 613–621。
- en: 'Vu et al. [2014] Vu, T.H., Olsson, C., Laptev, I., Oliva, A., Sivic, J., 2014.
    Predicting actions from static scenes, in: European Conference on Computer Vision,
    Springer. pp. 421–436.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vu 等人 [2014] Vu, T.H., Olsson, C., Laptev, I., Oliva, A., Sivic, J., 2014. 从静态场景中预测动作，见：欧洲计算机视觉会议，Springer.
    pp. 421–436。
- en: Wan et al. [2016a] Wan, J., Guo, G., Li, S.Z., 2016a. Explore efficient local
    features from RGB-D data for one-shot learning gesture recognition. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 38, 1626–1639.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 [2016a] Wan, J., Guo, G., Li, S.Z., 2016a. 从 RGB-D 数据中探索高效的局部特征用于一次性学习手势识别。IEEE
    计算机学会模式分析与机器智能学报 38, 1626–1639。
- en: 'Wan et al. [2016b] Wan, J., Li, S.Z., Zhao, Y., Zhou, S., Guyon, I., Escalera,
    S., 2016b. Chalearn looking at people RGB-D isolated and continuous datasets for
    gesture recognition, in: CVPRW.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 [2016b] Wan, J., Li, S.Z., Zhao, Y., Zhou, S., Guyon, I., Escalera, S.,
    2016b. Chalearn 看人 RGB-D 隔离和连续数据集用于手势识别，见：CVPRW。
- en: Wan et al. [2013] Wan, J., Ruan, Q., Li, W., Deng, S., 2013. One-shot learning
    gesture recognition from RGB-D data using bag of features. Journal of Machine
    Learning Research 14, 2549–2582.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 [2013] Wan, J., Ruan, Q., Li, W., Deng, S., 2013. 从 RGB-D 数据中使用特征袋进行一次性学习手势识别。机器学习研究学报
    14, 2549–2582。
- en: Wang et al. [2013] Wang, H., Kläser, A., Schmid, C., Liu, C.L., 2013. Dense
    trajectories and motion boundary descriptors for action recognition. International
    Journal of Computer Vision 103, 60–79.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2013] Wang, H., Kläser, A., Schmid, C., Liu, C.L., 2013. 密集轨迹和运动边界描述符用于动作识别。国际计算机视觉学报
    103, 60–79。
- en: 'Wang et al. [2012] Wang, J., Liu, Z., Wu, Y., Yuan, J., 2012. Mining actionlet
    ensemble for action recognition with depth cameras, in: CVPR.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2012] Wang, J., Liu, Z., Wu, Y., Yuan, J., 2012. 使用深度摄像头的动作集成挖掘用于动作识别，见：CVPR。
- en: 'Wang et al. [2014] Wang, J., Nie, X., Xia, Y., Wu, Y., Zhu, S.C., 2014. Cross-view
    action modeling, learning and recognition, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2649–2656.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2014] Wang, J., Nie, X., Xia, Y., Wu, Y., Zhu, S.C., 2014. 跨视角动作建模、学习与识别，见：IEEE
    计算机视觉与模式识别会议论文集, pp. 2649–2656。
- en: Wang et al. [2003] Wang, L., Hu, W., Tan, T., 2003. Recent developments in human
    motion analysis. Pattern recognition 36, 585–601.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2003] Wang, L., Hu, W., Tan, T., 2003. 人体运动分析的最新进展。模式识别 36, 585–601。
- en: 'Wang et al. [2015a] Wang, L., Qiao, Y., Tang, X., 2015a. Action recognition
    with trajectory-pooled deep-convolutional descriptors, in: CVPR, pp. 4305–4314.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2015a] Wang, L., Qiao, Y., Tang, X., 2015a. 使用轨迹池化深度卷积描述符进行动作识别，见：CVPR,
    pp. 4305–4314。
- en: 'Wang et al. [2017a] Wang, L., Xiong, Y., Lin, D., Van Gool, L., 2017a. Untrimmednets
    for weakly supervised action recognition and detection, in: CVPR.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2017a] Wang, L., Xiong, Y., Lin, D., Van Gool, L., 2017a. 用于弱监督动作识别和检测的
    Untrimmednets，见：CVPR。
- en: 'Wang et al. [2016a] Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang,
    X., Van Gool, L., 2016a. Temporal segment networks: towards good practices for
    deep action recognition, in: European Conference on Computer Vision, pp. 20–36.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2016a] Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X.,
    Van Gool, L., 2016a. 时间段网络：朝向良好的深度动作识别实践，见：欧洲计算机视觉会议, pp. 20–36。
- en: Wang et al. [2018a] Wang, P., Li, W., Gao, Z., Tang, C., Ogunbona, P., 2018a.
    Depth pooling based large-scale 3d action recognition with convolutional neural
    networks. IEEE Transactions on Multimedia .
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2018a] Wang, P., Li, W., Gao, Z., Tang, C., Ogunbona, P., 2018a.
    基于深度池化的大规模3D动作识别，使用卷积神经网络。IEEE多媒体交易。
- en: 'Wang et al. [2015b] Wang, P., Li, W., Gao, Z., Tang, C., Zhang, J., Ogunbona,
    P.O., 2015b. Convnets-based action recognition from depth maps through virtual
    cameras and pseudocoloring, in: ACM MM.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2015b] Wang, P., Li, W., Gao, Z., Tang, C., Zhang, J., Ogunbona,
    P.O., 2015b. 基于ConvNets的深度图像动作识别，通过虚拟摄像头和伪着色，见：ACM MM。
- en: Wang et al. [2016b] Wang, P., Li, W., Gao, Z., Zhang, J., Tang, C., Ogunbona,
    P., 2016b. Action recognition from depth maps using deep convolutional neural
    networks. THMS 46, 498–509.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2016b] Wang, P., Li, W., Gao, Z., Zhang, J., Tang, C., Ogunbona,
    P., 2016b. 使用深度卷积神经网络从深度图像中识别动作。THMS 46, 498–509。
- en: 'Wang et al. [2017b] Wang, P., Li, W., Gao, Z., Zhang, Y., Tang, C., Ogunbona,
    P., 2017b. Scene flow to action map: A new representation for RGB-D based action
    recognition with convolutional neural networks, in: CVPR.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2017b] Wang, P., Li, W., Gao, Z., Zhang, Y., Tang, C., Ogunbona,
    P., 2017b. 场景流至动作图：RGB-D动作识别的新表示，使用卷积神经网络，见：CVPR。
- en: 'Wang et al. [2016c] Wang, P., Li, W., Liu, S., Gao, Z., Tang, C., Ogunbona,
    P., 2016c. Large-scale isolated gesture recognition using convolutional neural
    networks, in: Pattern Recognition (ICPR), 2016 23rd International Conference on,
    IEEE. pp. 7–12.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2016c] Wang, P., Li, W., Liu, S., Gao, Z., Tang, C., Ogunbona,
    P., 2016c. 使用卷积神经网络进行大规模孤立手势识别，见：模式识别（ICPR），2016年第23届国际会议，IEEE。第7–12页。
- en: 'Wang et al. [2016d] Wang, P., Li, W., Liu, S., Zhang, Y., Gao, Z., Ogunbona,
    P., 2016d. Large-scale continuous gesture recognition using convolutional neural
    networks, in: 2016 23rd International Conference on Pattern Recognition (ICPR),
    pp. 13–18.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2016d] Wang, P., Li, W., Liu, S., Zhang, Y., Gao, Z., Ogunbona,
    P., 2016d. 使用卷积神经网络进行大规模连续手势识别，见：2016年第23届国际会议，模式识别（ICPR），第13–18页。
- en: 'Wang et al. [2018b] Wang, P., Li, W., Wan, J., Ogunbona, P., Liu, X., 2018b.
    Cooperative training of deep aggregation networks for rgb-d action recognition,
    in: AAAI.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2018b] Wang, P., Li, W., Wan, J., Ogunbona, P., Liu, X., 2018b.
    深度聚合网络的协同训练，用于RGB-D动作识别，见：AAAI。
- en: 'Wang et al. [2016e] Wang, P., Li, Z., Hou, Y., Li, W., 2016e. Action recognition
    based on joint trajectory maps using convolutional neural networks, in: ACM MM.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2016e] Wang, P., Li, Z., Hou, Y., Li, W., 2016e. 基于卷积神经网络的联合轨迹图动作识别，见：ACM
    MM。
- en: 'Wang et al. [2017c] Wang, P., Wang, S., Gao, Z., Hou, Y., Li, W., 2017c. Structured
    images for RGB-D action recognition, in: Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 1005–1014.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2017c] Wang, P., Wang, S., Gao, Z., Hou, Y., Li, W., 2017c. 用于RGB-D动作识别的结构化图像，见：IEEE计算机视觉与模式识别会议论文，第1005–1014页。
- en: 'Wang et al. [2016f] Wang, X., Farhadi, A., Gupta, A., 2016f. Actions~ transformations,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2658–2667.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2016f] Wang, X., Farhadi, A., Gupta, A., 2016f. 动作~变换，见：IEEE计算机视觉与模式识别会议论文，第2658–2667页。
- en: Wang et al. [2016g] Wang, Y., Song, J., Wang, L., Van Gool, L., Hilliges, O.,
    2016g. Two-stream sr-cnns for action recognition in videos, BMVC.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2016g] Wang, Y., Song, J., Wang, L., Van Gool, L., Hilliges, O.,
    2016g. 用于视频动作识别的双流SR-CNNs，BMVC。
- en: 'Weinzaepfel et al. [2015] Weinzaepfel, P., Harchaoui, Z., Schmid, C., 2015.
    Learning to track for spatio-temporal action localization, in: Proceedings of
    the IEEE International Conference on Computer Vision, pp. 3164–3172.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weinzaepfel et al. [2015] Weinzaepfel, P., Harchaoui, Z., Schmid, C., 2015.
    学习跟踪以进行时空动作定位，见：IEEE国际计算机视觉会议论文，第3164–3172页。
- en: Wu et al. [2016] Wu, D., Pigou, L., Kindermans, P.J., Le, N.D.H., Shao, L.,
    Dambre, J., Odobez, J.M., 2016. Deep dynamic neural networks for multimodal gesture
    segmentation and recognition. IEEE transactions on pattern analysis and machine
    intelligence 38, 1583–1597.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2016] Wu, D., Pigou, L., Kindermans, P.J., Le, N.D.H., Shao, L.,
    Dambre, J., Odobez, J.M., 2016. 用于多模态手势分割和识别的深度动态神经网络。IEEE模式分析与机器智能交易 38, 1583–1597。
- en: 'Wu and Shao [2014] Wu, D., Shao, L., 2014. Leveraging hierarchical parametric
    networks for skeletal joints based action segmentation and recognition, in: 2014
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 724–731.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu and Shao [2014] Wu, D., Shao, L., 2014. 利用层次化参数网络进行骨骼关节基础的动作分割和识别，见：2014
    IEEE计算机视觉与模式识别会议，第724–731页。
- en: 'Xia et al. [2012] Xia, L., Chen, C.C., Aggarwal, J., 2012. View invariant human
    action recognition using histograms of 3D joints, in: CVPRW.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. [2012] Xia, L., Chen, C.C., Aggarwal, J., 2012. 使用3D关节直方图进行视角不变的人体动作识别，见：CVPRW。
- en: 'Xingjian et al. [2015] Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong,
    W.K., Woo, W.c., 2015. Convolutional lstm network: A machine learning approach
    for precipitation nowcasting, in: Advances in Neural Information Processing Systems,
    pp. 802–810.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xingjian et al. [2015] Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong,
    W.K., Woo, W.c., 2015. 卷积LSTM网络：用于降水即时预报的机器学习方法，见：神经信息处理系统进展，第802–810页。
- en: 'Yan et al. [2014] Yan, X., Chang, H., Shan, S., Chen, X., 2014. Modeling video
    dynamics with deep dynencoder, in: European Conference on Computer Vision, Springer.
    pp. 215–230.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. [2014] Yan, X., Chang, H., Shan, S., Chen, X., 2014. 使用深度动态编码器建模视频动态，见：欧洲计算机视觉会议，Springer.
    第215–230页。
- en: 'Yang et al. [2012] Yang, X., Zhang, C., Tian, Y., 2012. Recognizing actions
    using depth motion maps-based histograms of oriented gradients, in: ACM MM.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2012] Yang, X., Zhang, C., Tian, Y., 2012. 使用基于深度运动图的方向梯度直方图识别动作，见：ACM
    MM。
- en: 'Ye et al. [2013] Ye, M., Zhang, Q., Wang, L., Zhu, J., Yang, R., Gall, J.,
    2013. A survey on human motion analysis from depth data, in: Time-of-Flight and
    Depth Imaging. Sensors, Algorithms, and Applications, pp. 149–187.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. [2013] Ye, M., Zhang, Q., Wang, L., Zhu, J., Yang, R., Gall, J., 2013.
    基于深度数据的人体运动分析综述，见：飞行时间和深度成像。传感器、算法和应用，第149–187页。
- en: 'Yeung et al. [2017] Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori,
    G., Fei-Fei, L., 2017. Every moment counts: Dense detailed labeling of actions
    in complex videos. International Journal of Computer Vision .'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeung et al. [2017] Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori,
    G., Fei-Fei, L., 2017. 每一刻都重要：复杂视频中动作的密集详细标注。国际计算机视觉期刊。
- en: 'Yeung et al. [2016] Yeung, S., Russakovsky, O., Mori, G., Fei-Fei, L., 2016.
    End-to-end learning of action detection from frame glimpses in videos, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2678–2687.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeung et al. [2016] Yeung, S., Russakovsky, O., Mori, G., Fei-Fei, L., 2016.
    从视频中的帧片段进行端到端动作检测学习，见：IEEE计算机视觉与模式识别会议论文集，第2678–2687页。
- en: 'Yun et al. [2012] Yun, K., Honorio, J., Chattopadhyay, D., Berg, T.L., Samaras,
    D., 2012. Two-person interaction detection using body-pose features and multiple
    instance learning, in: Computer Vision and Pattern Recognition Workshops (CVPRW),
    2012 IEEE Computer Society Conference on, IEEE. pp. 28–35.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yun et al. [2012] Yun, K., Honorio, J., Chattopadhyay, D., Berg, T.L., Samaras,
    D., 2012. 使用体态特征和多实例学习进行双人互动检测，见：计算机视觉与模式识别研讨会（CVPRW），2012 IEEE计算机学会会议，IEEE. 第28–35页。
- en: 'Zhang et al. [2016a] Zhang, B., Wang, L., Wang, Z., Qiao, Y., Wang, H., 2016a.
    Real-time action recognition with enhanced motion vector cnns, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2718–2726.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2016a] Zhang, B., Wang, L., Wang, Z., Qiao, Y., Wang, H., 2016a.
    使用增强运动向量CNN进行实时动作识别，见：IEEE计算机视觉与模式识别会议论文集，第2718–2726页。
- en: 'Zhang et al. [2017a] Zhang, J., Li, W., Ogunbona, P., 2017a. Joint geometrical
    and statistical alignment for visual domain adaptation, in: CVPR.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2017a] Zhang, J., Li, W., Ogunbona, P., 2017a. 视觉领域适应的联合几何和统计对齐，见：CVPR。
- en: 'Zhang et al. [2016b] Zhang, J., Li, W., Ogunbona, P.O., Wang, P., Tang, C.,
    2016b. RGB-D-based action recognition datasets: A survey. Pattern Recognition
    60, 86–105.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2016b] Zhang, J., Li, W., Ogunbona, P.O., Wang, P., Tang, C.,
    2016b. 基于RGB-D的动作识别数据集：综述。模式识别 60, 86–105。
- en: 'Zhang et al. [2017b] Zhang, P., Lan, C., Xing, J., Zeng, W., Xue, J., Zheng,
    N., 2017b. View adaptive recurrent neural networks for high performance human
    action recognition from skeleton data, in: ICCV.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2017b] Zhang, P., Lan, C., Xing, J., Zeng, W., Xue, J., Zheng,
    N., 2017b. 基于骨架数据的高性能人类动作识别的视角自适应递归神经网络，见：ICCV。
- en: 'Zhang et al. [2017c] Zhang, S., Liu, X., Xiao, J., 2017c. On geometric features
    for skeleton-based action recognition using multilayer lstm networks, in: WACV.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2017c] Zhang, S., Liu, X., Xiao, J., 2017c. 基于几何特征的骨架动作识别，使用多层LSTM网络，见：WACV。
- en: 'Zhao et al. [2017] Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Lin, D., Tang, X.,
    2017. Temporal action detection with structured segment networks, in: ICCV.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. [2017] Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Lin, D., Tang, X.,
    2017. 使用结构化分段网络进行时序动作检测，见：ICCV。
- en: 'Zhu et al. [2016a] Zhu, F., Shao, L., Xie, J., Fang, Y., 2016a. From handcrafted
    to learned representations for human action recognition: a survey. Image and Vision
    Computing 55, 42–52.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2016a] Zhu, F., Shao, L., Xie, J., Fang, Y., 2016a. 从手工特征到学习特征的人体动作识别：综述。图像与视觉计算
    55, 42–52。
- en: 'Zhu et al. [2016b] Zhu, G., Zhang, L., Mei, L., Shao, J., Song, J., Shen, P.,
    2016b. Large-scale isolated gesture recognition using pyramidal 3D convolutional
    networks, in: Pattern Recognition (ICPR), 2016 23rd International Conference on,
    IEEE. pp. 19–24.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2016b] Zhu, G., Zhang, L., Mei, L., Shao, J., Song, J., Shen, P.,
    2016b. 使用金字塔3D卷积网络的大规模孤立手势识别，见于：模式识别（ICPR），2016年第23届国际会议，IEEE，第19–24页。
- en: Zhu et al. [2017a] Zhu, G., Zhang, L., Shen, P., Song, J., 2017a. Multimodal
    gesture recognition using 3D convolution and convolutional lstm. IEEE Access .
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2017a] Zhu, G., Zhang, L., Shen, P., Song, J., 2017a. 使用3D卷积和卷积LSTM的多模态手势识别，IEEE
    Access。
- en: 'Zhu et al. [2017b] Zhu, H., Vial, R., Lu, S., 2017b. Tornado: A spatio-temporal
    convolutional regression network for video action proposal, in: Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5813–5821.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2017b] Zhu, H., Vial, R., Lu, S., 2017b. Tornado：一种用于视频动作提议的时空卷积回归网络，见于：IEEE计算机视觉与模式识别会议论文集，第5813–5821页。
- en: 'Zhu et al. [2016c] Zhu, W., Hu, J., Sun, G., Cao, X., Qiao, Y., 2016c. A key
    volume mining deep framework for action recognition, in: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 1991–1999.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2016c] Zhu, W., Hu, J., Sun, G., Cao, X., Qiao, Y., 2016c. 一种用于动作识别的关键体积挖掘深度框架，见于：IEEE计算机视觉与模式识别会议论文集，第1991–1999页。
- en: 'Zhu et al. [2016d] Zhu, W., Lan, C., Xing, J., Zeng, W., Li, Y., Shen, L.,
    Xie, X., 2016d. Co-occurrence feature learning for skeleton based action recognition
    using regularized deep LSTM networks, in: AAAI.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2016d] Zhu, W., Lan, C., Xing, J., Zeng, W., Li, Y., Shen, L., Xie,
    X., 2016d. 基于骨架的动作识别的共现特征学习，使用正则化的深度LSTM网络，见于：AAAI。
- en: 'Zolfaghari et al. [2017] Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox,
    T., 2017. Chained multi-stream networks exploiting pose, motion, and appearance
    for action classification and detection, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zolfaghari et al. [2017] Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox,
    T., 2017. 链式多流网络利用姿态、运动和外观进行动作分类和检测，见于：IEEE计算机视觉与模式识别会议论文集。
