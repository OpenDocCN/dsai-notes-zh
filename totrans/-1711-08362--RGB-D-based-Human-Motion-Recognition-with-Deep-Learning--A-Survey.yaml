- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:08:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[1711.08362] RGB-D-based Human Motion Recognition with Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1711.08362](https://ar5iv.labs.arxiv.org/html/1711.08362)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'RGB-D-based Human Motion Recognition with Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pichao Wang [pw212@uowmail.edu.au](mailto:pw212@uowmail.edu.au) Wanqing Li [wanqing@uow.edu.au](mailto:wanqing@uow.edu.au)
    Philip Ogunbona [philipo@uow.edu.au](mailto:philipo@uow.edu.au) Jun Wan [jun.wan@nlpr.ia.ac.cn](mailto:jun.wan@nlpr.ia.ac.cn)
    Sergio Escalera [sergio@maia.ub.es](mailto:sergio@maia.ub.es) Advanced Multimedia
    Research Lab, University of Wollongong, Australia Motovis Inc., Adelaide, Australia
    Center for Biometrics and Security Research (CBSR)& National Laboratory of Pattern
    Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA),
    Beijing, China University of Barcelona and Computer Vision Center, Campus UAB,
    Barcelona, Spain
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Human motion recognition is one of the most important branches of human-centered
    research activities. In recent years, motion recognition based on RGB-D data has
    attracted much attention. Along with the development in artificial intelligence,
    deep learning techniques have gained remarkable success in computer vision. In
    particular, convolutional neural networks (CNN) have achieved great success for
    image-based tasks, and recurrent neural networks (RNN) are renowned for sequence-based
    problems. Specifically, deep learning methods based on the CNN and RNN architectures
    have been adopted for motion recognition using RGB-D data. In this paper, a detailed
    overview of recent advances in RGB-D-based motion recognition is presented. The
    reviewed methods are broadly categorized into four groups, depending on the modality
    adopted for recognition: RGB-based, depth-based, skeleton-based and RGB+D-based.
    As a survey focused on the application of deep learning to RGB-D-based motion
    recognition, we explicitly discuss the advantages and limitations of existing
    techniques. Particularly, we highlighted the methods of encoding spatial-temporal-structural
    information inherent in video sequence, and discuss potential directions for future
    research.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Human Motion Recognition, RGB-D Data, Deep Learning, Survey^†^†journal: Computer
    Vision and Image Understanding'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the several human-centered research activities (e.g. human detection,
    tracking, pose estimation and motion recognition) in computer vision, human motion
    recognition is particularly important due to its potential application in video
    surveillance, human computer interfaces, ambient assisted living, human-robot
    interaction, intelligent driving, etc. A human motion recognition task can be
    summarised as the automatic identification of human behaviours from images or
    video sequences. The complexity and duration of the motion involved can be used
    as basis for broad categorization into four kinds namely gesture, action, interaction
    and group activity. A gesture can be defined as the basic movement or positioning
    of the hand, arm, body, or head that communicates an idea, emotion, etc. “Hand
    waving” and “nodding” are some typical examples of gestures. Usually, a gesture
    has relatively short duration. An action is considered as a type of motion performed
    by a single person during short time period and involves multiple body parts,
    in contrast with the few body parts that involved in gesture. An activity is composed
    by a sequence of actions. An interaction is a type of motion performed by two
    actors; one actor is human while the other may be human or an object. This implies
    that the interaction category will include human-human or human-object interaction.
    “Hugging each other” and “playing guitar” are examples of these two kinds of interaction,
    respectively. Group activity is the most complex type of activity, and it may
    be a combination of gestures, actions and interactions. Necessarily, it involves
    more than two humans and from zero to multiple objects. Examples of group activities
    would include “two teams playing basketball” and “group meeting”.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Early research on human motion recognition was dominated by the analysis of
    still images or videos [[2](#bib.bib2), [144](#bib.bib144), [132](#bib.bib132),
    [99](#bib.bib99), [44](#bib.bib44), [176](#bib.bib176)]. Most of these efforts
    used color and texture cues in 2D images for recognition. However, the task remains
    challenging due to problems posed by background clutter, partial occlusion, view-point,
    lighting changes, execution rate and biometric variation. This challenge remains
    even with current deep learning approaches [[49](#bib.bib49), [4](#bib.bib4)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: With the recent development of cost-effective RGB-D sensors, such as Microsoft
    Kinect ™and Asus Xtion ™, RGB-D-based motion recognition has attracted much attention.
    This is largely because the extra dimension (depth) is insensitive to illumination
    changes and includes rich 3D structural information of the scene. Additionally,
    3D positions of body joints can be estimated from depth maps [[114](#bib.bib114)].
    As a consequence, several methods based on RGB-D data have been proposed and the
    approach has proven to be a promising direction for human motion analysis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Several survey papers have summarized the research on human motion recognition
    using RGB-D data [[14](#bib.bib14), [166](#bib.bib166), [3](#bib.bib3), [16](#bib.bib16),
    [172](#bib.bib172), [28](#bib.bib28), [100](#bib.bib100), [46](#bib.bib46)]. Specifically,
    Chen et al. [[14](#bib.bib14)] focused on depth sensors, pre-processing of depth
    data, depth-based action recognition methods and datasets. In their work, Ye et
    al. [[166](#bib.bib166)] presented an overview of approaches using depth and skeleton
    modalities for tasks including activity recognition, head/hand pose estimation,
    facial feature detection and gesture recognition. The survey presented by Aggarwal
    and Xia [[3](#bib.bib3)] summarized five categories of representations based on
    3D silhouettes, skeletal joints/body part location, local spatial-temporal features,
    scene flow features and local occupancy features. The work of Cheng et al. [[16](#bib.bib16)]
    focused on RGB-D-based hand gesture recognition datasets and summarized corresponding
    methods from three perspectives: static hand gesture recognition, hand trajectory
    gesture recognition and continuous hand gesture recognition. In another effort
    Escalera et al. [[28](#bib.bib28)] reviewed the challenges and methods for gesture
    recognition using multimodal data. Some of the surveys have focused on available
    datasets for RGB-D research. For example, the work of Zhang et al. [[172](#bib.bib172)]
    described available benchmark RGB-D datasets for action/activity recognition and
    included 27 single-view datasets, 10 multi-view datasets and 7 multi-person datasets.
    Other works as Presti and La Cascia [[100](#bib.bib100)] and Han et al. [[46](#bib.bib46)]
    mainly reviewed skeleton-based representation and approaches for action recognition.
    A short survey on RGB-D action recognition using deep learning was recently presented
    in [[4](#bib.bib4)], analysing RGB and depth cues in terms of 2DCNN, 3DCNN, and
    Deep temporal approaches'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8cac053f00decb67ced6380783a8912c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Categorisation of the methods for RGB-D-based motion recognition
    using deep learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'All above surveys mainly focused on the analysis of handcrafted features. Here,
    we provide a comprehensive review of RGB-D-based human motion recognition using
    deep learning approaches. Even while focusing on deep learning approaches, the
    nature of the input data is still important. RGB-D data for human motion analysis
    comprises three modalities: RGB, depth and skeleton. The main characteristic of
    RGB data is its shape, color and texture which brings the benefits of extracting
    interesting points and optical flow. Compared to RGB videos, the depth modality
    is insensitive to illumination variations, invariant to color and texture changes,
    reliable for estimating body silhouette and skeleton, and provides rich 3D structural
    information of the scene. Differently from RGB and depth, skeleton data containing
    the positions of human joints, is a relatively high-level feature for motion recognition.
    The different properties of the three modalities have inspired the various methods
    found in the literature. For example, optical flow-based methods with Convolutional
    Neural Networks (CNN) is very effective for RGB channel [[27](#bib.bib27)]; depth
    rank pooling based-method with CNN is a good choice for depth modality [[152](#bib.bib152)];
    sequence based method with Recurrent Neural Networks (RNN) [[82](#bib.bib82)]
    and image-based method with CNN [[155](#bib.bib155)] are effective for skeleton;
    and scene flow-based method using CNN are promising for RGB+D channels [[151](#bib.bib151)].
    These methods are very effective for specific modalities, but not always the case
    for all the modalities. Given these observations, this survey identified four
    broad categories of methods based on the modality adopted for human motion recognition.
    The categories include RGB-based, depth-based, skeleton-based and RGB+D-based.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: In each category, two sub-divisions are further identified, namely segmented
    human motion recognition and continuous/online motion recognition. For segmented
    motion recognition, the scenario of the problem can be simply described as classifying
    a well delineated sequence of video frames as one of a set of motion types. This
    is in contrast to continuous/online human motion recognition where there are no
    a priori given boundaries of motion execution. The online situation is compounded
    by the fact that the video sequence is not recorded and the algorithm must deal
    with frames as they are being captured, save for possibly a small data cache.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: During the performance of a specified motion spatial information which refers
    to the spatial configuration of human body at an instant of time (e.g. relative
    positions of the human body parts) can be identified. Similarly, there is the
    temporal information which characterizes the spatial configuration of the body
    over time (i.e. the dynamics of the body). Lastly, the structural information
    encodes the coordination and synchronization of body parts over the period in
    which the action is being performed. It describes the relationship of the spatial
    configurations of human body across different time slots.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In reviewing the various methods, consideration has been given to the manner
    in which the spatial, temporal and structural information have been exploited.
    Hence, the survey discusses the advantages and limitations of the reviewed methods
    from the spatial-temporal-structural encoding viewpoint, and suggests potential
    directions for future research.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'A key novelty of this survey is the focus on three architectures of neural
    networks used in the various deep learning methods reviewed namely CNN-based,
    RNN-based and other structured networks. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey") illustrates
    the taxonomy underpinning this survey.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'This is one of the first surveys dedicated to RGB-D-based human motion recognition
    using deep learning. Apart from this claim, this survey distinguishes itself from
    other surveys through the following contributions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comprehensive coverage of the most recent and advanced deep learning-based methods
    developed in the last five years, thereby providing readers with a complete overview
    of recent research results and state-of-the-art methods.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insightful categorization and analysis of methods based on the different properties
    of the modalities; highlight of the pros and cons of the methods described in
    the reviewed papers from the viewpoint of spatial-temporal-structural encoding.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussion of the challenges of RGB-D-based motion recognition; analysis of
    the limitations of available methods and discussion of potential research directions.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Additionally, several recently released or commonly used RGB-D-based benchmark
    datasets associated with deep learning are surveyed. The main application domain
    of interest in this survey paper is human motion recognition based on RGB-D data,
    including gesture recognition, action/activity recognition and interaction recognition.
    The lack of datasets focused on RGB-D-based group activity recognition has led
    to paucity of research on this topic and thus this survey does not cover this
    topic. Other RGB-D-based human-centered applications, such as human detection,
    tracking and pose estimation, are also not the focus of this paper. For surveys
    on RGB-D data acquisition readers are referred to [[14](#bib.bib14), [16](#bib.bib16),
    [46](#bib.bib46)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequent sections of the this survey are organized as follows. Commonly used
    RGB-D-based benchmark datasets are described in Section [2](#S2 "2 Benchmark Datasets
    ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey"). Sections [3](#S3
    "3 RGB-based Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion
    Recognition with Deep Learning: A Survey") to [6](#S6 "6 RGB+D-based Motion Recognition
    with Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning:
    A Survey") discuss methods of RGB-D-based motion recognition using deep learning
    from four perspectives: RGB-based motion recognition, depth-based motion recognition,
    skeleton-based motion recognition and RGB+D-based motion recognition. Challenges
    of RGB-D-based motion recognition and pointers to future directions are presented
    in Section [7](#S7 "7 Discussion ‣ RGB-D-based Human Motion Recognition with Deep
    Learning: A Survey"). The survey provides concluding remarks in Section [8](#S8
    "8 Conclusion ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 2 Benchmark Datasets
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Over the last decade, a number of RGB-D benchmark datasets have been collected
    and made publicly available for the research community. The sources of the datasets
    are mainly of three categories [[14](#bib.bib14), [16](#bib.bib16), [46](#bib.bib46)]:
    Motion capture (Mocap) system, structured-light cameras (e.g. Kinect v1) and time-of-flight
    (ToF) cameras (e.g. Kinect v2). Hence the modalities of the datasets cover RGB,
    depth, skeleton and their combinations. With the advance of deep learning, deep
    methods have been developed for estimating skeletons directly from single images
    or video sequences, such as DeepPose [[129](#bib.bib129)], Deepercut [[58](#bib.bib58)]
    and Adversarial PoseNet [[15](#bib.bib15)]. A comprehensive survey of these datasets
    have appeared in the literature (see e.g. [[16](#bib.bib16)] for hand gestures
    and [[172](#bib.bib172)] for action recognition). In the present survey only 15
    large-scale datasets that have been commonly adopted for evaluating deep learning-based
    methods are described. The reader is referred to Table LABEL:performance for a
    sample of works (publications) that have used these datasets. For the purpose
    of this survey, the datasets have been divided into two groups: segmented datasets
    and continuous/online datasets.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Segmented Datasets
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By segmented datasets we refer to those datasets where samples correspond to
    a whole begin-end action/gestures, with one segment for one action. They are mainly
    used for classification purposes. The following are several segmented datasets
    commonly used for the evaluation of methods based on deep learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 CMU Mocap
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CMU Graphics Lab Motion Capture Database (CMU Mocap) [[1](#bib.bib1)]([http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/))
    is one of the earliest source of data that covers a wide range of human actions,
    including interactions between two subjects, human locomotion, interactions with
    uneven terrain, sports, and other human actions. This dataset consists of RGB
    and skeleton modalities.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 HDM05
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Motion Capture Database HDM05 [[94](#bib.bib94)] ([http://resources.mpi-inf.mpg.de/HDM05/](http://resources.mpi-inf.mpg.de/HDM05/))
    was captured by an optical marker-based technology with the frequency of 120 Hz,
    which contains 2337 sequences for 130 actions performed by 5 non-professional
    actors, and 31 joints in each frame. Besides skeleton data, this dataset also
    provides RGB data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 MSR-Action3D
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'MSR-Action3D [[77](#bib.bib77)] ([http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets](http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets))
    is the first public benchmark RGB-D action dataset collected using Kinect ™sensor
    by Microsoft Research, Redmond and University of Wollongong in 2010. The dataset
    contains 20 actions: high arm wave, horizontal arm wave, hammer, hand catch, forward
    punch, high throw, draw x, draw tick, draw circle, hand clap, two hand wave, side-boxing,
    bend, forward kick, side kick, jogging, tennis serve, golf swing, pickup and throw.
    Ten subjects performed these actions three times. All the videos were recorded
    from a fixed point of view and the subjects were facing the camera while performing
    the actions. The background of the dataset was removed by some post-processing.
    Specifically, if an action needs to be performed with one arm or one leg, the
    actors were required to perform it using right arm or leg.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4 MSRC-12
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'MSRC-12 dataset [[36](#bib.bib36)]([%****␣CVIU-arxiv.tex␣Line␣400␣****http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/](%****%20CVIU-arxiv.tex%20Line%20400%20****http://research.microsoft.com/en-us/um/cambridge/projects/msrc12/))
    was collected by Microsoft Research Cambridge and University of Cambridge in 2012.
    The authors provided three familiar and easy to prepare instruction modalities
    and their combinations to the participants. The modalities are (1) descriptive
    text breaking down the performance kinematics, (2) an ordered series of static
    images of a person performing the gesture with arrows annotating as appropriate,
    and (3) video (dynamic images) of a person performing the gesture. There are 30
    participants in total and for each gesture, the data were collected as: Text (10
    people), Images (10 people), Video (10 people), Video with text (10 people), Images
    with text (10 people). The dataset was captured using one Kinect ™sensor and only
    the skeleton data are made available.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.5 MSRDailyActivity3D
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MSRDailyActivity3D Dataset [[142](#bib.bib142)]([http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets](http://www.uow.edu.au/~wanqing/#MSRAction3DDatasets))
    was collected by Microsoft and the Northwestern University in 2012 and focused
    on daily activities. The motivation was to cover human daily activities in the
    living room. The actions were performed by 10 actors while sitting on the sofa
    or standing close to the sofa. The camera was fixed in front of the sofa. In addition
    to depth data, skeleton data are also recorded, but the joint positions extracted
    by the tracker are very noisy due to the actors being either sitting on or standing
    close to the sofa.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.6 UTKinect
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: UTKinect dataset [[162](#bib.bib162)]([http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](http://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html))
    was collected by the University of Texas at Austin in 2012\. Ten types of human
    actions were performed twice by 10 subjects. The subjects performed the actions
    from a variety of views. One challenge of the dataset is due to the actions being
    performed with high actor-dependent variability. Furthermore, human-object occlusions
    and body parts being out of the field of view have further increased the difficulty
    of the dataset. Ground truth in terms of action labels and segmentation of sequences
    are provided.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.7 G3D
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Gaming 3D dataset (G3D) [[7](#bib.bib7)]([http://dipersec.king.ac.uk/G3D/](http://dipersec.king.ac.uk/G3D/))
    captured by Kingston University in 2012 focuses on real-time action recognition
    in gaming scenario. It contains 10 subjects performing 20 gaming actions. Each
    subject performed these actions thrice. Two kinds of labels were provided as ground
    truth: the onset and offset of each action and the peak frame of each action.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.8 SBU Kinect Interaction Dataset
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SBU Kinect Interaction Dataset [[169](#bib.bib169)]([http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html](http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html)
    ) was collected by Stony Brook University in 2012\. It contains eight types of
    interactions. All videos were recorded with the same indoor background. Seven
    participants were involved in performing the activities which have interactions
    between two actors. The dataset is segmented into 21 sets and each set contains
    one or two sequences of each action category. Two kinds of ground truth information
    are provided: action labels of each segmented video and identification of “active”
    actor and “inactive” actor.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.9 Berkeley MHAD
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Berkeley Multimodal Human Action Database (Berkeley MHAD) [[96](#bib.bib96)]([http://tele-immersion.citris-uc.org/berkeley_mhad#dl](http://tele-immersion.citris-uc.org/berkeley_mhad#dl)),
    collected by University of California at Berkeley and Johns Hopkins University
    in 2013, was captured in five different modalities to expand the fields of application.
    The modalities are derived from: optical mocap system, four multi-view stereo
    vision cameras, two Microsoft Kinect v1 cameras, six wireless accelerometers and
    four microphones. Twelve subjects performed 11 actions, five times each. Three
    categories of actions are included: (1) actions with movement in full body parts,
    e.g., jumping in place, jumping jacks, throwing, etc., (2) actions with high dynamics
    in upper extremities, e.g., waving hands, clapping hands, etc. and (3) actions
    with high dynamics in lower extremities, e.g., sit down, stand up. The actions
    were executed with style and speed variations.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.10 Northwestern-UCLA Multiview Action 3D
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Northwestern-UCLA Multiview Action 3D [[143](#bib.bib143)]([http://users.eecs.northwestern.edu/~jwa368/my_data.html](http://users.eecs.northwestern.edu/~jwa368/my_data.html))
    was collected by Northwestern University and University of California at Los Angles
    in 2014\. This dataset contains data taken from a variety of viewpoints. The actions
    were performed by 10 actors and captured by three simultaneous Kinect™v1 cameras.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.11 ChaLearn LAP IsoGD
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ChaLearn LAP IsoGD Dataset [[139](#bib.bib139)] ([http://www.cbsr.ia.ac.cn/users/jwan/database/isogd.html](http://www.cbsr.ia.ac.cn/users/jwan/database/isogd.html))
    is a large RGB-D dataset for segmented gesture recognition, and it was collected
    by Kinect v1 camera. It includes 47933 RGB-D depth sequences, each RGB-D video
    representing one gesture instance. There are 249 gestures performed by 21 different
    individuals. The dataset is divided into training, validation and test sets. All
    three sets consist of samples of different subjects to ensure that the gestures
    of one subject in the validation and test sets will not appear in the training
    set.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.12 NTU RGB+D
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: NTU RGB+D Dataset [[109](#bib.bib109)]([https://github.com/shahroudy/NTURGB-D](https://github.com/shahroudy/NTURGB-D))
    is currently the largest action recognition dataset in terms of the number of
    samples per action. The RGB-D data is captured by Kinect v2 cameras. The dataset
    has more than 56 thousand sequences and 4 million frames, containing 60 actions
    performed by 40 subjects aging between 10 and 35\. It consists of front view,
    two side views and left, right 45 degree views.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Continuous/Online Datasets
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Continuous/online datasets refer to those datasets where each video sequence
    may contain one or more actions/gestures, and the segmented position between different
    motion classes are unknown. These datasets are mainly used for action detection,
    localization and online prediction. There are few datasets for this type.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 ChaLearn2014 Multimodal Gesture Recognition
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ChaLearn2014 Multimodal Gesture Recognition [[29](#bib.bib29)] ([http://gesture.chalearn.org/2014-looking-at-people-challenge](http://gesture.chalearn.org/2014-looking-at-people-challenge))
    is multi-modal dataset collected by Kinect v1 sensor, including RGB, depth, skeleton
    and audio modalities. In all sequences, a single user is recorded in front of
    the camera, performing natural communicative Italian gestures. The starting and
    ending frames for each gesture are annotated along with the gesture class label.
    It contains nearly 14K manually labeled (beginning and ending frame) gesture performances
    in continuous video sequences, with a vocabulary of 20 Italian gesture categories.
    There are 1, 720, 800 labeled frames across 13, 858 video fragments of about 1
    to 2 minutes sampled at 20 Hz. The gestures are performed by 27 different individuals
    under diverse conditions; these include varying clothes, positions, backgrounds
    and lighting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Statistics of the public available benchmark datasets that are commonly
    used for evaluation with deep learning. Notation for the header: Seg: Segmented,
    Con:Continuous, D: Depth, S:Skeleton, Au:Audio, Ac:Accelerometer, IR:IR videos,
    #:number of, JI:Jaccard Index.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | year |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '&#124; Acquisition &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; device &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '| Seg/Con | Modality | #Class | #Subjects | #Samples | #Views | Metric |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| CMU Mocap | 2001 | Mocap | Seg | RGB,S | 45 | 144 | 2,235 | 1 | Accuracy
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| HDM05 | 2007 | Mocap | Seg | RGB,S | 130 | 5 | 2337 | 1 | Accuracy |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| MSR-Action3D | 2010 | Kinect v1 | Seg | S,D | 20 | 10 | 567 | 1 | Accuracy
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| MSRC-12 | 2012 | Kinect v1 | Seg | S | 12 | 30 | 594 | 1 | Accuracy |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSR &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DailyActivity3D &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '| 2012 | Kinect v1 | Seg | RGB,D,S | 16 | 10 | 320 | 1 | Accuracy |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| UTKinect | 2012 | Kinect v1 | Seg | RGB,D,S | 10 | 10 | 200 | 1 | Accuracy
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| G3D | 2012 | Kinect v1 | Seg | RGB,D,S | 5 | 5 | 200 | 1 | Accuracy |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SBU Kinect &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Interaction &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '| 2012 | Kinect v1 | Seg | RGB,D,S | 7 | 8 | 300 | 1 | Accuracy |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| Berkeley MHAD | 2013 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '&#124; Mocap &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Kinect v1 &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '| Seg | RGB,D,S,Au,Ac | 12 | 12 | 660 | 4 | Accuracy |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multiview &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Action3D &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 | Kinect v1 | Seg | RGB,D,S | 10 | 10 | 1475 | 3 | Accuracy |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ChaLearn LAP &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; IsoGD &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 | Kinect v1 | Seg | RGB,D | 249 | 21 | 47933 | 1 | Accuracy |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: '| NTU RGB+D | 2016 | Kinect v2 | Seg | RGB,D,S,IR | 60 | 40 | 56880 | 80 |
    Accuracy |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| ChaLearn2014 | 2014 | Kinect v1 | Con | RGB,D,S,Au | 20 | 27 | 13858 | 1
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: '&#124; Accuracy &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; JI etc. &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ChaLearn LAP &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ConGD &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 | Kinect v1 | Con | RGB,D | 249 | 21 | 22535 | 1 | JI |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| PKU-MMD | 2017 | Kinect v2 | Con | RGB,D,S,IR | 51 | 66 | 1076 | 3 | JI etc.
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: 2.2.2 ChaLearn LAP ConGD
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ChaLearn LAP ConGD Dataset [[139](#bib.bib139)] ([http://www.cbsr.ia.ac.cn/users/jwan/database/congd.html](http://www.cbsr.ia.ac.cn/users/jwan/database/congd.html))
    is a large RGB-D dataset for continuous gesture recognition. It was collected
    by Kinect v1 sensor and includes 47933 RGB-D gesture instances in 22535 RGB-D
    gesture videos. Each RGB-D video may contain one or more gestures. There are 249
    gestures performed by 21 different individuals. The dataset is divided into training,
    validation and test sets. All three sets consist of samples of different subjects
    to ensure that the gestures of one subject in the validation and test sets will
    not appear in the training set.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 PKU-MMD
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PKU-MMD [[19](#bib.bib19)] ([http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html))
    is a large scale dataset for continuous multi-modality 3D human action understanding
    and covers a wide range of complex human activities with well annotated information.
    It was captured via the Kinect v2 sensor. PKU-MMD contains 1076 long video sequences
    in 51 action categories, performed by 66 subjects in three camera views. It contains
    almost 20,000 action instances and 5.4 million frames in total. It provides multi-modality
    data sources, including RGB, depth, Infrared Radiation and Skeleton.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S2 "2 Benchmark Datasets ‣ RGB-D-based Human Motion Recognition
    with Deep Learning: A Survey") shows the statistics of publicly available benchmark
    datasets that are commonly used for evaluation of deep learning-based algorithms.
    It can be seen that the surveyed datasets cover a wide range of different types
    of actions including gestures, simple actions, daily activities, human-object
    interactions, human-human interactions. It also covers both segmented and continuous/online
    datasets, with different acquisition devices, modalities, and views. Sample images
    from different datasets are shown in Fig. [2](#S2.F2 "Figure 2 ‣ 2.2.3 PKU-MMD
    ‣ 2.2 Continuous/Online Datasets ‣ 2 Benchmark Datasets ‣ RGB-D-based Human Motion
    Recognition with Deep Learning: A Survey").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/88f9e23c47dc1a73a814d5b7b37667a3.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Sample images from different datasets.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduce the deep learning concepts and architectures that
    are relevant or have been applied to RGB-D-based motion recognition. Readers who
    are interested in more background and techniques are referred to the book by [[41](#bib.bib41)].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 3 RGB-based Motion Recognition with Deep Learning
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RGB is one important channel of RGB-D data with characteristics including shape,
    color and texture that bear rich features. These properties also make it effective
    to directly use networks, such as 2D CNNs [[68](#bib.bib68), [117](#bib.bib117),
    [47](#bib.bib47)], to extract frame-level features. Even though most of the surveyed
    methods for this section are not adapted to RGB-D-based datasets, we argue that
    the following methods could be directly adapted to RGB modality of RGB-D datasets.
    We define three categories namely, CNN-based, RNN-based and other-architecture-based
    approaches for segmented motion recognition; the first two categories are for
    continuous/online motion recognition.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Segmented Motion Recognition
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 CNN-based Approach
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this group of methods, currently there are mainly four approaches to encode
    spatial-temporal-structural information. The first approach applies CNN to extract
    features from individual frames and later, fuse the temporal information. For
    example [[64](#bib.bib64)] investigated four temporal fusion methods, and proposed
    the concept of slow fusion where higher layers get access to progressively more
    global information in both spatial and temporal dimensions (see Fig. [4](#S3.F4
    "Figure 4 ‣ 3.1.1 CNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")). Furthermore, several temporal pooling methods have
    been explored and the suggestion is that max pooling in the temporal domain is
    preferable [[95](#bib.bib95)].'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2dc2c82a891e1c15171c023c9aaa5cf1.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: C3D net has 8 convolutions, 5 max-poolings, and 2 fully connected
    layers, followed by a softmax output layer. All 3D convolution kernels are 3$\times$3$\times$3
    with stride 1 in both spatial and temporal dimensions. Number of filters are indicated
    in each box. The 3D pooling layers are as indicated from pool1 to pool5. All pooling
    kernels are 2$\times$2$\times$2, except for pool1 which is 1$\times$2$\times$2.
    Each fully connected layer has 4096 output units. Figure from [[130](#bib.bib130)].'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/67817e269a71caf68c8c5f976610b3ee.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Different approaches for fusing information over temporal dimension
    through the network. Red, green and blue boxes indicate convolutional, normalization
    and pooling layers respectively. In the Slow Fusion model, the depicted columns
    share parameters. Figure from [[64](#bib.bib64)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The second approach extends convolutional operation into temporal domain. In
    one such implementation, Ji et al. [[61](#bib.bib61)] proposed 3D-convolutional
    networks using 3D kernels (filters extended along the time axis) to extract features
    from both spatial and temporal dimensions. This work empirically showed that the
    3D-convolutional networks outperform their 2D frame-based counterparts. With modern
    deep architectures, such as VGG [[117](#bib.bib117)], and large-scale supervised
    training datasets, such as Sports-1M [[64](#bib.bib64)], Tran et al [[130](#bib.bib130)]
    extended the work presented in [[61](#bib.bib61)] by including 3D pooling layers,
    and proposed a generic descriptor named C3D by averaging the outputs of the first
    fully connected layer of the networks (see Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 CNN-based
    Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion Recognition with
    Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")).
    However, both of these works break the video sequence into short clips and aggregate
    video-level information by late score fusion. This is likely to be suboptimal
    when considering some long action sequence, such as walking or swimming that lasts
    several seconds and spans tens or hundreds of video frames. To handle this problem,
    Varol et al. [[133](#bib.bib133)] investigated the learning of long-term video
    representations and proposed the Long-term Temporal Convolutions (LTC) at the
    expense of decreasing spatial resolution to keep the complexity of networks tractable.
    Despite the fact that this is straightforward and mainstream, extending spatial
    kernels to 3D spatio-temporal derivative inevitably increases the number of parameters
    of the network. To relieve the drawbacks of 3D kernels, Sun et al. [[125](#bib.bib125)]
    factorized a 3D filter into a combination of 2D and 1D filters.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'The third approach is to encode the video into dynamic images that contain
    the spatio-temporal information and then apply CNN for image-based recognition.
    Bilen et al. [[6](#bib.bib6)] proposed to adopt rank pooling [[34](#bib.bib34)]
    to encode the video into one dynamic set of images and used pre-trained models
    over ImageNet [[68](#bib.bib68)] for fine-tuning (see Figure. [6](#S3.F6 "Figure
    6 ‣ 3.1.1 CNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")). The end-to-end learning methods with rank pooling
    has also been proposed in [[6](#bib.bib6), [35](#bib.bib35)]. Hierarchical rank
    pooling [[33](#bib.bib33)] is proposed to learn higher order and non-linear representations
    compared to the original work. Generalized rank pooling [[17](#bib.bib17)] is
    introduced to improve the original method via a quadratic ranking function which
    jointly provides a low-rank approximation to the input data and preserves their
    temporal order in a subspace.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/52d2121334e65c1eeb4cc54f349a75ac.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/52d2121334e65c1eeb4cc54f349a75ac.png)'
- en: 'Figure 5: Two-stream architecture for RGB-based motion recognition. Figure
    from [[116](#bib.bib116)].'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于RGB的运动识别的双流架构。图源自 [[116](#bib.bib116)]。
- en: '![Refer to caption](img/b9374e1639e6afae68b3f24b9156a8cc.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9374e1639e6afae68b3f24b9156a8cc.png)'
- en: 'Figure 6: Rank pooling encodes the RGB video into one dynamic image and CNN
    is adopted for feature extraction and classification. Figure from [[6](#bib.bib6)].'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：排名池化将RGB视频编码成一个动态图像，并采用CNN进行特征提取和分类。图源自 [[6](#bib.bib6)]。
- en: 'Besides the above works that aim to adopt one network to exploit both spatio-temporal
    information contained in the video, the fourth approach separates the two factors
    and adopt multiple stream networks. Simonyan et al. [[116](#bib.bib116)] proposed
    one spatial stream network fed with raw video frames, and one temporal stream
    network accepting optical flow fields as input. The two streams are fused together
    using the softmax scores (see Figure. [5](#S3.F5 "Figure 5 ‣ 3.1.1 CNN-based Approach
    ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion Recognition with Deep
    Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")
    for the two-stream architecture). Wang et al. [[145](#bib.bib145)] extended the
    two-stream networks by integrating improved trajectories [[141](#bib.bib141)],
    where trajectory-constrained sampling and pooling are used to encode deep features
    learned from deep CNN architecture, into effective descriptors. To incorporate
    long-range temporal structure using the two-stream networks, Wang et al. [[147](#bib.bib147)]
    devised a temporal segment network (TSN) that uses a sparse sampling scheme to
    extract short snippets over a long video sequence. With the removal of redundancy
    from consecutive frames and a segmental structure, aggregated information is obtained
    from the sampled snippets. To reduce the expensive calculation of optical flow,
    Zhang et al. [[170](#bib.bib170)] accelerated this two stream structure by replacing
    optical flow with motion vector which can be obtained directly from compressed
    videos without extra calculation. Wang et al. [[158](#bib.bib158)] leveraged semantic
    cues in video by using a two-stream semantic region-based CNNs (SR-CNNs) to incorporate
    human/object detection results into the framework. In their work, Chéron et al. [[18](#bib.bib18)]
    exploit spatial structure of the human pose and extract a pose-based convolutional
    neural network (P-CNN) feature from both RGB frames and optical flow for fine-grained
    action recognition. The work presented in [[157](#bib.bib157)] formulated the
    problem of action recognition from a new perspective and model an action as a
    transformation which changes the state of the environment before the action to
    the state after the action. They designed a Siamese network which models the action
    as a transformation on a high-level feature space based on the two-stream model.
    Based on the two-stream framework, [[180](#bib.bib180)] proposed a key volume
    mining deep framework for action recognition, where they identified key volumes
    and conducted classification simultaneously. Inspired by the success of Residual
    Networks (ResNets) [[47](#bib.bib47)], Feichtenhofer et al. [[31](#bib.bib31)]
    injected residual connections between the two streams to allow spatial-temporal
    interaction between them. Instead of using optical flow for temporal stream, [[72](#bib.bib72)]
    adopted Motion History Image (MHI) [[8](#bib.bib8)] as the motion clue. The MHI
    was combined with RGB frames in a spatio-temporal CNN for fined grained action
    recognition. However, all the methods reviewed above incorporated the two streams
    from separate training regimes; any registration of the two streams was neglected.
    In order to address this gap and propose a new architecture for spatial-temporal
    fusion of the two streams Feichtenhofer et al. [[32](#bib.bib32)] investigated
    three aspects of fusion for the two streams: (i) how to fuse the two networks
    with consideration for spatial registration, (ii) where to fuse the two networks
    and, (iii) how to fuse the networks temporally. One of their conclusions was that
    the results suggest the importance of learning correspondences between highly
    abstract ConvNet features both spatially and temporally.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 RNN-based Approach
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For RNN-based approach, Baccouche et al. [[5](#bib.bib5)] tackled the problem
    of action recognition through a cascade of 3D CNN and LSTM, in which the two networks
    were trained separately. Differently from the separate training, Donahue et al. [[22](#bib.bib22)]
    proposed one Long-term Recurrent Convolutional Network (LRCN) to exploit end-to-end
    training of the two networks( see illustration in Figure. [7](#S3.F7 "Figure 7
    ‣ 3.1.2 RNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion
    Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep
    Learning: A Survey")). To take full advantage of both CNN and RNN, Ng et al. [[95](#bib.bib95)]
    aggregated CNN features with both temporal pooling and LSTM for temporal exploitation,
    and fused the output scores from the feature pooling and LSTM network to conduct
    final action recognition. Pigou et al. [[98](#bib.bib98)] proposed an end-to-end
    trainable neural network architecture incorporating temporal convolutions and
    bidirectional LSTM for gesture recognition. This provided opportunity to mine
    temporal information that is much discriminative for gesture recognition. In their
    work, Sharma et al. [[111](#bib.bib111)] proposed a soft attention model for action
    recognition based on LSTM (see Figure. [8](#S3.F8 "Figure 8 ‣ 3.1.2 RNN-based
    Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based Motion Recognition with
    Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey")).
    The attention model learns the parts in the frames that are relevant for the task
    at hand and attaches higher importance to them. Previous attention-based methods
    have only utilized video-level category as supervision to train LSTM. This strategy
    may lack a detailed and dynamical guidance and consequently restrict their capacity
    for modelling complex motions in videos. Du et al. [[23](#bib.bib23)] address
    this problem by proposing a recurrent pose-attention network (RPAN) for action
    recognition in videos, which can adaptively learn a highly discriminative pose-related
    feature for every-step action prediction of LSTM. To take advantage of both Fisher
    Vector [[108](#bib.bib108)] and RNN, Lev et al. [[74](#bib.bib74)] introduced
    a Recurrent Neural Network Fisher Vector (RNN-FV) where the GMM probabilistic
    model in the fisher vector is replaced by a RNN and thus avoids the need for the
    assumptions of data distribution in the GMM. Even though RNN is remarkably capable
    of modeling temporal dependences, it lacks an intuitive high-level spatial-temporal
    structure. The spatio-temporal-structural information has been mined by Jain et
    al. [[59](#bib.bib59)] through a combination of the powers of spatio-temporal
    graphs and RNN for action recognition. Recently, Sun et al. [[124](#bib.bib124)]
    proposed a Lattice-LSTM (L2STM) network, which extends LSTM by learning independent
    hidden state transitions of memory cells for individual spatial locations. This
    method effectively enhances the ability to model dynamics across time and addresses
    the non-stationary issue of long-term motion dynamics without significantly increasing
    the model complexity. Differently from previous methods that using only feedforward
    connections, Shi et al. [[112](#bib.bib112)] proposed a biologically-inspired
    deep network, called ShuttleNet1\. Unlike traditional RNNs, all processors inside
    ShuttleNet are connected in a loop to mimic the human brain’s feedforward and
    feedback connections. In this manner, the processors are shared across multiple
    pathways in the loop connection. Attention mechanism is then employed to select
    the best information flow pathway.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f4d1a980d6e11dd10559a72a5b0750e6.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: LRCN processes the variable-length visual input with a CNN, whose
    outputs are fed into a stack of recurrent sequence models. The output is a variable-length
    prediction. Figure from [[22](#bib.bib22)].'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6098f5de806d60b51842c79223ca8741.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The CNN takes the video frames as its input and produces a feature
    tube. The model computes the current input $\boldsymbol{x}_{t}$ as an average
    of the feature slices weighted according to the location softmax $\boldsymbol{I}_{t}$.
    At each time-step $t$, the recurrent network takes a feature slice $\boldsymbol{x}_{t}$
    as input. It then propagates $\boldsymbol{x}_{t}$ through three layers of LSTMs
    and predicts the next location probabilities $\boldsymbol{I}_{t+1}$ and the class
    label $\boldsymbol{y}_{t}$. Figure from [[111](#bib.bib111)].'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12443cb4f930c439be39bcb3896dad5f.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The Deep Action Proposals algorithm can localize segments of varied
    duration around actions occurring along a video without exhaustively exploring
    multiple temporal scales. Figure from [[30](#bib.bib30)].'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12619eba7925a48a94ce9508aea4408e.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The LSTM autoencoder model and LSTM future predictor model. Figure
    from [[122](#bib.bib122)].'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Other-architecture-based Approach
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Besides the commonly used CNN- and RNN-based methods for motion recognition
    from RGB modality, there are several other structures that have been adopted for
    this task. Jhuang et al. [[60](#bib.bib60)] used a feedforward hierarchical template
    matching architecture for action recognition with pre-defined spatio-temporal
    filters in the first layer. In his thesis, Chen [[13](#bib.bib13)] adopted the
    convolutional RBM (CRBM) as the basic processing unit and proposed the so-called
    space-time Deep Belief Network (ST-DBN) that alternates the aggregation of spatial
    and temporal information so that higher layers capture longer range statistical
    dependencies in both space and time. Taylor et al. [[126](#bib.bib126)] extended
    the Gated RBM (GRBM) [[91](#bib.bib91)] to convolutional GRBM (convGRBM) that
    shares weights at all locations in an image and inference is performed through
    convolution. Le et al. [[70](#bib.bib70)] presented an extension of the independent
    subspace analysis algorithm [[128](#bib.bib128)] to learn invariant spatio-temporal
    features from unlabeled video data. They scale up the original ISA to larger input
    data by employing two important ideas from convolutional neural networks: convolution
    and stacking. This convolutional stacking idea enables the algorithm to learn
    a hierarchical representation of the data suitable for recognition. Yan et al. [[164](#bib.bib164)]
    proposed Dynencoder, a three layer auto-encoder, to capture video dynamics. Dynencoder
    is shown to be successful in synthesizing dynamic textures, and one can think
    of a Dynencoder as a compact way of representing the spatio-temporal information
    of a video. Similarly, Srivastava et al. [[122](#bib.bib122)] introduced a LSTM
    autoencoder model. The LSTM autoencoder model consists of two RNNs, namely, the
    encoder LSTM and the decoder LSTM. The encoder LSTM accepts a sequence as input
    and learns the corresponding compact representation. The states of the encoder
    LSTM contain the appearance and dynamics of the sequence. The decoder LSTM receives
    the learned representation to reconstruct the input sequence. Inspired by the
    Generative Adversarial Networks (GAN) [[42](#bib.bib42)], Mathieu et al. [[90](#bib.bib90)]
    adopted the adversarial mechanism to train a multi-scale convolutional network
    to generate future frames given an input sequence. To deal with the inherently
    blurry predictions obtained from the standard Mean Squared Error (MSE) loss function,
    they proposed three different and complementary feature learning strategies: a
    multi-scale architecture, an adversarial training method, and an image gradient
    difference loss function.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Continuous/Online Motion Recognition
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the action recognition methods reviewed above rely heavily on segmented
    videos for model training. However, it is very expensive and time-consuming to
    acquire a large-scale trimmed video dataset. The availability of untrimmed video
    datasets (e.g. [[48](#bib.bib48), [167](#bib.bib167), [21](#bib.bib21), [139](#bib.bib139),
    [19](#bib.bib19)]) have encouraged research and challenges/contests in motion
    recognition in this domain.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 CNN-based Approach
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by the success of region-proposal-based object detection using R-CNN [[39](#bib.bib39),
    [38](#bib.bib38), [103](#bib.bib103)], several proposal-based action recognition
    methods from untrimmed video are proposed. These methods first generate a reduced
    number of candidate temporal windows, and then an action classifier discriminates
    each proposal independently into one of the actions of interest. For instance,
    based on the two-stream concept [[116](#bib.bib116)], [[40](#bib.bib40)] classified
    frame-based region proposals of interest using static and motion cues. The regions
    are then linked across frames based on the predictions and their spatial overlap;
    thus producing action tubes respectively for each action and video. Weinzaepfel
    et al. [[159](#bib.bib159)] also started from the frame-level proposals, selected
    the highest scoring ones, tracked them throughout the video, and adopted a multi-scale
    sliding window approach over tracks to detect the temporal content of an action.
    Shou et al. [[115](#bib.bib115)] proposed a multi-stage segment-based 3D CNNs
    to generate candidate segments, that are used to recognize actions and localize
    temporal boundaries. Peng and Schimd [[97](#bib.bib97)] generated rich proposals
    from both RGB and optical flow data by using region proposal networks for frame-level
    action detection, and stacked optical flows to enhance the discriminative power
    of motion R-CNN. Wang et al. [[146](#bib.bib146)] proposed an UntrimmedNet to
    generate clip proposals that may contain action instances for untrimmed action
    recognition. Based on these clip-level representations, the classification module
    aims to predict the scores for each clip proposal and the selection module tries
    to select or rank those clip proposals. Similarly in the same direction, Zhao
    et al. [[175](#bib.bib175)] adopted explicit structural modeling in the temporal
    dimension. In their model, each complete activity instance is considered as a
    composition of three major stages, namely starting, course, and ending, and they
    introduced structured temporal pyramid pooling to produce a global representation
    of the entire proposal. Differently from previous methods, Zhu et al. [[179](#bib.bib179)]
    proposed a framework that integrates the complementary spatial and temporal information
    into an end-to-end trainable system for video action proposal, and a novel and
    efficient path trimming method is proposed to handle untrimmed video by examining
    actionness and background score pattern without using extra detectors. To generalize
    R-CNN from 2D to 3D, Hou et al. [[52](#bib.bib52)] proposed an end-to-end 3D CNN-based
    approach for action detection in videos. A Tube Proposal Network was introduced
    to leverage skip pooling in temporal domain to preserve temporal information for
    action localization in 3D volumes, and Tube-of-Interest pooling layer was proposed
    to effectively alleviate the problem with variable spatial and temporal sizes
    of tube proposals. Saha et al. [[106](#bib.bib106)] proposed a deep net framework
    capable of regressing and classifying 3D region proposals spanning two successive
    video frames. The core of the framework is an evolution of classical region proposal
    networks (RPNs) to 3D RPNs. Similarly, Kalogeiton et al. [[63](#bib.bib63)] extended
    the Single Shot MultiBox Detector (SSD) [[84](#bib.bib84)] framework from 2D to
    3D by proposing an Action Tubelet detector. In order to quickly and accurately
    generate temporal action proposals, Gao et al. [[37](#bib.bib37)] proposed a Temporal
    Unit Regression Network (TURN) model, that jointly predicts action proposals and
    refines the temporal boundaries by temporal coordinate regression using CNN. Similarly,
    Singh et al. [[119](#bib.bib119)] designed an efficient online algorithm to incrementally
    construct and label ’action tubes’ from the SSD frame level detections, making
    it the first real-time (up to 40fps) system able to perform online S/T action
    localisation on the untrimmed videos. Besides the proposal-based methods discussed
    above, Lea et al. [[71](#bib.bib71)] introduced a new class of temporal models,
    called Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal
    convolutions to perform fine-grained action segmentation or detection.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4a0588c64e353b3dff63da6441a92cdd.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The problem of weakly supervised action labelling is tackled where
    only the order of the occurring actions in given during training. The temporal
    model is trained by maximizing the probability of all possible frame-to-label
    alignments. At testing time, no annotation is given. The learned model encodes
    the temporal structure of videos and could predict the actions without further
    information. Figure from [[55](#bib.bib55)].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 RNN-based Approach
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Apart from the proposal-based methods that use CNN, there are several proposal-based
    methods using RNN for temporal modeling. Escorcia et al. [[30](#bib.bib30)] introduced
    the Deep Action Proposals (DAPs) that generate temporal action proposals from
    long untrimmed videos for action detection and classification (see Figure. [9](#S3.F9
    "Figure 9 ‣ 3.1.2 RNN-based Approach ‣ 3.1 Segmented Motion Recognition ‣ 3 RGB-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")). They adopted C3D network [[130](#bib.bib130)] for
    visual encoder and LSTM for sequence encoder. However, all of these methods generated
    proposals by a sliding window approach, dividing the video into short overlapping
    temporal window, which is computationally expensive. To reduce the number of proposals,
    Buch et al. [[9](#bib.bib9)] proposed a single-stream temporal action proposal
    generation method that does not the need to divide input into short overlapping
    clips or temporal windows for batch processing.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the proposal-based methods, there are several methods that are proposal-free.
    Yeung et al. [[168](#bib.bib168)] proposed an end-to-end training model that is
    formulated as a recurrent neural network-based agent. This agent learns a policy
    for sequentially forming and refining hypotheses about action instances based
    on the intuition that the process of detecting actions is naturally one of observation
    and refinement. They adopted two networks namely, observation network and recurrent
    network, for this purpose. Singh et al. [[118](#bib.bib118)] presented a multi-stream
    bi-directional recurrent neural network for fine-grained action detection. They
    adopted a tracking algorithm to locate a bounding box around the person and trained
    two streams on motion and appearance cropped to the tracked bounding box. The
    video sequence was split into fixed long chunks for the input of two-stream networks,
    and bi-directional LSTM was used to model long-term temporal dynamics within and
    between actions. Ma et al. [[87](#bib.bib87)] introduced a novel ranking loss
    within the RNN objective so that the trained model better captures progression
    of activities. The ranking loss constrains the detection score of the correct
    category to be monotonically non-decreasing as the activity progresses. The same
    time, the detection score margin between the correct activity category and all
    other categories is monotonically non-decreasing. Huang et al. [[55](#bib.bib55)]
    proposed a weakly-supervised framework for action labeling in video ( see Figure [11](#S3.F11
    "Figure 11 ‣ 3.2.1 CNN-based Approach ‣ 3.2 Continuous/Online Motion Recognition
    ‣ 3 RGB-based Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion
    Recognition with Deep Learning: A Survey")), where only the order of occurring
    actions is required during training. They proposed an Extended Connectionist Temporal
    Classification (ECTC) framework to efficiently evaluate all possible alignments
    between the input and label sequences via dynamic programming and explicitly enforce
    their consistency with frame-to-frame visual similarities. Taking inspiration
    from classical linear dynamic systems theory for modeling time series, Dave et
    al. [[20](#bib.bib20)] derived a series of recurrent neural networks that sequentially
    make top-down predictions about the future and then correct those predictions
    with bottom-up observations. Their predictive-corrective architecture allows the
    incorporation of insights from time-series analysis: adaptively focus computation
    on “surprising” frames where predictions require large corrections; simplify learning
    in that only “residual-like” corrective terms need to be learned over time and
    naturally decorrelate an input stream in a hierarchical fashion, producing a more
    reliable signal for learning at each layer of a network.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 4 Depth-based Motion Recognition with Deep Learning
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compared with RGB videos, the depth modality is insensitive to illumination
    variations, invariant to color and texture changes, reliable for estimating body
    silhouette and skeleton, and provides rich 3D structural information of the scene.
    However, there are only few published results on depth based action recognition
    using deep learning methods. Two reasons can be adduced for this situation. First,
    the absence of color and texture in depth maps weakens the discriminative representation
    power of CNN models [[85](#bib.bib85)]. Second, existing depth data is relatively
    small-scale. The conventional pipelines are purely data-driven and learn representation
    directly from the pixels. Such model is likely to be at risk of overfitting when
    the network is optimized on limited training data. Currently, there are only CNN-based
    methods for depth-based motion recognition.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Segmented Motion Recognition
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 CNN-based Approach
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Wang et al. [[149](#bib.bib149), [150](#bib.bib150)] proposed a method called
    Weighted Hierarchical Depth Motion Maps (WHDMM) + 3ConvNet, for human action recognition
    from depth maps on small training datasets. Three strategies were developed to
    leverage the capability of ConvNets in mining discriminative features for recognition.
    Firstly, different viewpoints are mimicked by rotating the 3D points of the captured
    depth maps. This not only auguments the data, but also makes the trained ConvNets
    view-tolerant. Secondly, WHDMMs at several temporal scales were constructed to
    encode the spatio-temporal motion patterns of actions into 2D spatial structures.
    The 2D spatial structures are further enhanced for recognition by converting the
    WHDMMs into pseudo-color images. Lastly, the three ConvNets were initialized with
    the models obtained from ImageNet and fine-tuned independently on the color-coded
    WHDMMs constructed in three orthogonal planes. Inspired by the promising results
    achieved by rank pooling method [[6](#bib.bib6)] on RGB data, Wang et al. [[152](#bib.bib152)]
    encoded the depth map sequences into three kinds of dynamic images with rank pooling:
    Dynamic Depth Images (DDI), Dynamic Depth Normal Images (DDNI) and Dynamic Depth
    Motion Normal Images (DDMNI). These three representations capture the posture
    and motion information from three different levels for gesture recognition. Specifically,
    DDI exploits the dynamics of postures over time and DDNI and DDMNI exploit the
    3D structural information captured by depth maps. Wang et al. [[148](#bib.bib148)]
    replaced the bidirectional rank pooling in the method of [[152](#bib.bib152)]
    with hierarchical and bidirectional rank pooling to capture both high order and
    non-linear dynamics effectively for both gesture and action recognition. Recently,
    Wang et al. [[156](#bib.bib156)] proposed to represent a depth map sequence into
    three pairs of structured dynamic images at body, part and joint levels respectively
    through bidirectional rank pooling. Different from previous works that applied
    one ConvNet for each part/joint separately, one pair of structured dynamic images
    is constructed from depth maps at each granularity level and serves as the input
    of a ConvNet. The structured dynamic image not only preserves the spatial-temporal
    information but also enhances the structure information across both body parts/joints
    and at different temporal scales. In addition, it requires low computational cost
    and memory to construct. This new representation, referred to as Spatially Structured
    Dynamic Depth Images (S²DDI), aggregates from global to fine-grained motion and
    structure information in a depth sequence, and enables us to fine-tune the existing
    ConvNet models trained on image data for classification of depth sequences, without
    a need for training the models afresh. Similarly, Hou et al. [[54](#bib.bib54)]
    extended S²DDI to Spatially and Temporally Structured Dynamic Depth Images (STSDDI),
    where a hierarchical bidirectional rank pooling method was adopted to exploit
    the spatio-temporal-structural information contained in the depth sequence and
    it is applied to interactions of two subjects. Differently from the above texture
    image encoding method, Rahmani et al. [[102](#bib.bib102)] proposed a cross-view
    action recognition based on depth sequence. Their method comprises two steps:
    (i) learning a general view-invariant human pose model from synthetic depth images
    and, (ii) modeling the temporal action variations. To enlarge the training data
    for CNN, they generated the training data synthetically by fitting realistic synthetic
    3D human models to real mocap data and then rendering each pose from a large number
    of viewpoints. For spatio-temporal representation, they used group sparse Fourier
    Temporal Pyramid which encodes the action-specific discriminative output features
    of the proposed human pose model.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26e84afb3b030c2bc2bbccaba8927c1e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Depth map sequences are encoded into texture color images by using
    the concepts of Depth Motion Maps (DMM) [[165](#bib.bib165)] and pseudo-coloring,
    and at the same time enlarged the training data by scene rotation on the 3D point
    cloud. Three channel of CNN are adopted for feature extraction and classification.
    Figure from [[150](#bib.bib150)].'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Continuous/Online Motion Recognition
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1 CNN-based Approach
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For continuous gesture recognition, Wang et al. [[153](#bib.bib153)] first segmented
    the continuous depth sequence into segmented sequences using quantity of movement
    (QOM) [[62](#bib.bib62)], and then adopted improved DMM (IDMM) to encode the dynamics
    of depth sequences into texture images for large-scale continuous gesture recognition.
    To improve the encoding quality of depth sequences, Wang et al. [[148](#bib.bib148)]
    proposed three simple, compact yet effective representations of depth sequences,
    referred to respectively as Dynamic Depth Images (DDI), Dynamic Depth Normal Images
    (DDNI) and Dynamic Depth Motion Normal Images (DDMNI), for continuous action recognition.
    These dynamic images are constructed from a segmented sequence of depth maps using
    hierarchical bidirectional rank pooling to effectively capture the spatial-temporal
    information. Specifically, DDI exploits the dynamics of postures over time while
    DDNI and DDMNI extract the 3D structural information captured by depth maps. The
    image-based representations enable us to fine-tune the existing ConvNet models
    trained on image data without training a large number of parameters from scratch.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 5 Skeleton-based Motion Recognition with Deep Learning
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Differently from RGB and depth, skeleton data contains the positions of human
    joints, which can be considered relatively high-level features for motion recognition.
    There are two common ways to estimate skeletons, one is to use MOCAP systems and
    the other is to estimate skeletons directly from depth maps or RGB images/video.
    Skeletons from MOCAP systems are often robust to scale and illumination changes
    and can be invariant to viewpoints as well as human body rotation and motion speed;
    Skeletons estimated from depth maps or RGB images/video are prone to errors caused
    by a number of factors including viewpoints and occlusion since both factors can
    lead to significant different appearance of same actions. Currently, there are
    mainly three approaches to skeleton-based motion recognition using deep learning:
    (i) RNN-based, (ii) CNN-based and other-architecture-based approaches for segmented
    motion recognition and, (iii) RNN-based approaches for continuous/online motion
    recognition.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Segmented Motion Recognition
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fee2c9a8aa3903b4363dd70664ef1b67.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The JTM framework for skeleton-based motion recognition with CNN.
    Figure from [[155](#bib.bib155)].'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 CNN-based Approach
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main step in this approach is to convert the skeleton sequences into images
    where the spatio-temporal information is reflected in the image properties including
    color and texture. Duet al. [[24](#bib.bib24)] represented a skeleton sequence
    as a matrix by concatenating the joint coordinates at each instant and arranging
    the vector representations in a chronological order. The matrix is then quantified
    into an image and normalized to handle the variable-length problem. The final
    image is fed into a CNN model for feature extraction and recognition. Wang et
    al. [[155](#bib.bib155)] proposed to encode spatio-temporal information contained
    in the skeleton sequence into multiple texture images, namely, Joint Trajectory
    Maps (JTM), by mapping the trajectories into HSV (hue, saturation, value) space.
    Pre-trained models over Imagenet is adopted for fine-tuning over the JTMs to extract
    features and recognize actions. Similarly, Hou et al. [[53](#bib.bib53)] drew
    the skeleton joints with a specific pen to three orthogonal canvases, and encodes
    the dynamic information in the skeleton sequences with color encoding. Li et al. [[75](#bib.bib75)]
    proposed to encode the pair-wise distances of skeleton joints of single or multiple
    subjects into texture images, namely, Joint Distance Maps (JDM), as the input
    of CNN for action recognition. Compared with the works reported by [[155](#bib.bib155)]
    and [[53](#bib.bib53)], JDM is less sensitive to view variations. liu et al. [[83](#bib.bib83)]
    introduced an enhanced skeleton visualization method to represent a skeleton sequence
    as a series of visual and motion enhanced color images. They proposed a sequence-based
    view invariant transform to deal with the view variation problem, and multi-stream
    CNN fusion method is adopted to conduct recognition. Ke et al. [[65](#bib.bib65)]
    designed vector-based features for each body part of human skeleton sequences,
    which are translation, scale and rotation invariant, and transformed the features
    into images to feed into CNN for learning high level and discriminative representation.
    In another effort, Ke et al. [[66](#bib.bib66)] represented the sequence as a
    clip with several gray images for each channel of the 3D coordinates, which reflects
    multiple spatial structural information of the joints. The images are fed to a
    deep CNN to learn high-level features, and the CNN features of all the three clips
    at the same time-step are concatenated in a feature vector. Each feature vector
    represents the temporal information of the entire skeleton sequence and one particular
    spatial relationship of the joints. A Multi-Task Learning Network (MTLN) is adopted
    to jointly process the feature vectors of all time-steps in parallel for action
    recognition. Kim and Reiter [[67](#bib.bib67)] approached the problem differently
    and proposed to use the Temporal Convolutional Neural Networks (TCN) [[71](#bib.bib71)]
    for skeleton based action recognition. They re-designed the original TCN into
    Res-TCN by factoring out the deeper layers into additive residual terms that yields
    both interpretable hidden representations and model parameters.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b70045e53b17d2ff87b1ca775cd044d2.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The LSTM autoencoder model and LSTM future predictor model. Figure
    from [[26](#bib.bib26)].'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 RNN-based Approach
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this class of approaches, skeleton features are input to an RNN in order
    to exploit the temporal evolution. For instance, in a series or works Du et al. [[26](#bib.bib26),
    [25](#bib.bib25)] divided the whole skeleton sequence into five parts according
    to the human physical structure, and separately fed them into five bidirectional
    RNNs/LSTMs. As the number of layers increases, the representations extracted by
    the subnets are hierarchically fused to build a higher-level representation. The
    process is illustrated in Fig. [14](#S5.F14 "Figure 14 ‣ 5.1.1 CNN-based Approach
    ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based Motion Recognition with
    Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey").
    This method explicitly encodes the spatio-temporal-structural information into
    high level representation. Veeriah et al. [[134](#bib.bib134)] proposed a differential
    gating scheme for the LSTM neural network, which emphasizes the change in information
    gain caused by the salient motions between the successive frames. This work is
    one of the first aimed at demonstrating the potential of learning complex time-series
    representations via high-order derivatives of states. Zhu et al. [[181](#bib.bib181)]
    designed two types of regularizations to learn effective features and motion dynamics.
    In the fully connected layers, they introduced regularization to drive the model
    to learn co-occurrence features of the joints at different layers. Furthermore,
    they derived a new dropout and apply it to the LSTM neurons in the last LSTM layer,
    which helps the network to learn complex motion dynamics. Instead of keeping a
    long-term memory of the entire body’s motion in the cell, Shahroudy et al. [[109](#bib.bib109)]
    proposed a part-aware LSTM human action learning model (P-LSTM) wherein memory
    is split across part-based cells. It is argued that keeping the context of each
    body part independent and representing the output of the P-LSTM unit as a combination
    of independent body part context information is more efficient. Previous RNN-based
    3D-action recognition methods have adopted RNN to model the long-term contextual
    information in the temporal domain for motion-based dynamics representation. However,
    there is also strong dependency between joints in the spatial domain. In addition
    the spatial configuration of joints in video frames can be highly discriminative
    for 3D-action recognition task. To exploit this dependency, Liu et al.[[81](#bib.bib81)]
    proposed a spatio-temporal LSTM (ST-LSTM) network which extends the traditional
    LSTM-based learning to both temporal and spatial domains. Rather than concatenate
    the joint-based input features, ST-LSTM explicitly models the dependencies between
    the joints and applies recurrent analysis over spatial and temporal domains concurrently.
    Besides, they introduced a trust gate mechanism to make LSTM robust to noisy input
    data. Song et al. [[120](#bib.bib120)] proposed a spatio-temporal attention model
    with LSTM to automatically mine the discriminative joints and learn the respective
    and different attentions of each frame along the temporal axis. Similarly, Liu
    et al. [[82](#bib.bib82)] proposed a Global Context-Aware Attention LSTM (GCA-LSTM)
    to selectively focus on the informative joints in the action sequence with the
    assistance of global context information. Differently from previous works that
    adopted the coordinates of joints as input, Zhang et al. [[174](#bib.bib174)]
    investigated a set of simple geometric features of skeleton using 3-layer LSTM
    framework, and showed that using joint-line distances as input requires less data
    for training. Based on the notion that LSTM networks with various time-step sizes
    can model various attributes well, Lee et al. [[73](#bib.bib73)] proposed an ensemble
    Temporal Sliding LSTM (TS-LSTM) networks for skeleton-based action recognition.
    The proposed network is composed of multiple parts containing short-term, medium-
    term and long-term TS-LSTM networks, respectively. Li et al. [[76](#bib.bib76)]
    proposed an adaptive and hierarchical framework for fine-grained, large-scale
    skeleton-based action recognition. This work was motivated by the need to distinguish
    fine-grained action classes that are intractable using a single network, and adaptivity
    to new action classes by model augmentation. In the framework, multiple RNNs are
    effectively incorporated in a tree-like hierarchy to mitigate the discriminative
    challenge and thus using a divide-and-conquer strategy. To deal with large view
    variations in captured human actions, Zhang et al. [[173](#bib.bib173)] proposed
    a self-regulated view adaptation scheme which re-positions the observation viewpoints
    dynamically, and integrated the proposed view adaptation scheme into an end-to-end
    LSTM network which automatically determines the “best” observation viewpoints
    during recognition.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eea1ad46156bb3793396b48a68ed6b00.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: The joint classification-regression RNN framework for online action
    detection and forecasting. Figure from [[78](#bib.bib78)].'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c92aec62606b560efe88d29635a1c927.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Conceptual illustration of LieNet architecture. In the network structure,
    the data space of each RotMap/RotPooling layer corresponds to a Lie group, while
    the weight spaces of the RotMap layers are Lie groups as well. Figure from [[56](#bib.bib56)].'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Other-architecture-based Approach
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Besides the RNN- and CNN-based approaches, there are several other deep learning-based
    methods. Salakhutdinov et al. [[107](#bib.bib107)] proposed a new compositional
    learning architecture that integrates deep learning models with structured hierarchical
    Bayesian models. Specifically, this method learns a hierarchical Dirichlet process
    (HDP) [[127](#bib.bib127)] prior over the activities of the top-level features
    in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns novel concepts
    from very few training examples by learning: (i) low-level generic features, (ii)
    high-level features that capture correlations among low-level features and, (iii)
    a category hierarchy for sharing priors over the high-level features that are
    typical of different kinds of concepts. Wu and Shao [[161](#bib.bib161)] adopted
    deep belief networks (DBN) to model the distribution of skeleton joint locations
    and extract high-level features to represent humans at each frame in 3D space.
    Ijjina et al. [[57](#bib.bib57)] adopted stacked auto encoder to learn the underlying
    features of input skeleton data. Huang et al. [[56](#bib.bib56)] incorporated
    the Lie group structure into a deep learning architecture to learn more appropriate
    Lie group features for skeleton based action recognition (see Figure. [16](#S5.F16
    "Figure 16 ‣ 5.1.2 RNN-based Approach ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b0b559cbc7c8e982fcf41116198fb0c.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The deep architecture is composed of five components: (a) Input
    Preprocessing; (b) 3D CNN (C3D); (c) ConvLSTM; (d) Spatial Pyramid Pooling and
    Fully Connected Layers; (e) Multimodal Score Fusion. Figure from [[178](#bib.bib178)].'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Continuous/Online Motion Recognition
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.2.1 RNN-based Approach
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Differently from previous methods that recognize motion from segmented skeleton
    sequences, Li et al. [[78](#bib.bib78)] proposed a multi-task end-to-end Joint
    Classification-Regression Recurrent Neural Network to explore the action type
    and temporal localization information. They adopted LSTM to capture the complex
    long-range temporal dynamics, which avoids the typical sliding window design and
    thus ensures high computational efficiency. Furthermore, the subtask of regression
    optimization provides the ability to forecast the action prior to its occurrence.
    The framework is shown in Figure. [15](#S5.F15 "Figure 15 ‣ 5.1.2 RNN-based Approach
    ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based Motion Recognition with
    Deep Learning ‣ RGB-D-based Human Motion Recognition with Deep Learning: A Survey").'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 6 RGB+D-based Motion Recognition with Deep Learning
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in previous sections, RGB, depth and skeleton modalities have their
    own specific properties, and how to combine the strengths of these modalities
    with deep learning approach is important. To address this problem, several methods
    have been proposed. In general, these methods can be categorized as (i) CNN-based,
    (ii) RNN-based and other-architecture-based approaches for segmented motion recognition
    and, (iii) RNN-based continuous/online motion recognition.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Segmented Motion Recognition
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.1.1 CNN-based Approach
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zhu et al. [[177](#bib.bib177)] fused RGB and depth in a pyramidal 3D convolutional
    network based on C3D [[130](#bib.bib130)] for gesture recognition. They designed
    pyramid input and pyramid fusion for each modality and late score fusion was adopted
    for final recognition. Duan et al. [[27](#bib.bib27)] proposed a convolutional
    two-stream consensus voting network (2SCVN) which explicitly models both the short-term
    and long-term structure of the RGB sequences. To alleviate distractions from background,
    a 3D depth-saliency ConvNet stream (3DDSN) was aggregated in parallel to identify
    subtle motion characteristics. Later score fusion was adopted for final recognition.
    The methods described so far considered RGB and depth as separate channels and
    fused them later. Wang et al. [[151](#bib.bib151)] took a different approach and
    adopted scene flow to extract features that fused the RGB and depth from the onset.
    The new representation based on CNN and named Scene Flow to Action Map (SFAM)
    was used for motion recognition. Different from previous methods, Wang et al. [[154](#bib.bib154)]
    proposed to cooperatively train a single convolutional neural network (named c-ConvNet)
    on both RGB and depth features, and deeply aggregate the two kinds of features
    for action recognition. While the conventional ConvNet learns the deep separable
    features for homogeneous modality-based classification with only one softmax loss
    function, the c-ConvNet enhances the discriminative power of the deeply learned
    features and weakens the undesired modality discrepancy by jointly optimizing
    a ranking loss and a softmax loss for both homogeneous and heterogeneous modalities.
    Rahmani et al. [[101](#bib.bib101)] proposed an end-to-end learning model for
    action recognition from depth and skeleton data. The proposed model learned to
    fuse features from depth and skeletal data, capture the interactions between body-parts
    and/or interactions with environmental objects, and model the temporal structure
    of human actions in an end-to-end learning framework. The proposed method was
    made robust to viewpoint changes, by introducing a deep CNN which transfers visual
    appearance of human body-parts acquired from different unknown views to a view-invariant
    space.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 RNN-based Approach
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For RGB and depth fusion, Pigou et al. [[98](#bib.bib98)] directly considered
    the depth as the fourth channel and CNN was adopted to extract frame-based appearance
    features. Temporal convolutions and RNN were combined to capture the temporal
    information. Li et al. [[79](#bib.bib79)] adopted C3D [[130](#bib.bib130)] to
    extract features separately from RGB and depth modalities, and used the concatenated
    for SVM classifier. Zhu et al. [[178](#bib.bib178)] presented a gesture recognition
    method using C3D [[130](#bib.bib130)] and convolutional LSTM (convLSTM) [[163](#bib.bib163)]
    based on depth and RGB modalities (see Figure [17](#S5.F17 "Figure 17 ‣ 5.1.3
    Other-architecture-based Approach ‣ 5.1 Segmented Motion Recognition ‣ 5 Skeleton-based
    Motion Recognition with Deep Learning ‣ RGB-D-based Human Motion Recognition with
    Deep Learning: A Survey")). The major drawback of traditional LSTM in handling
    spatio-temporal data is its usage of full connections in input-to-state and state-to-state
    transitions in which no spatial information is encoded. The ConvLSTM determines
    the future state of a certain cell in the grid by the inputs and past states of
    its local neighbors. Average score fusion was adopted to fuse the two separate
    channel networks for the two modalities. Luo et al. [[86](#bib.bib86)] proposed
    to use a RNN-based encoder-decoder framework to learn a video representation by
    predicting a sequence of basic motions described as atomic 3D flows. The learned
    representation is then extracted from the generated model to recognize activities.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/115aa9c045f88bbcb57f1118fe847a99.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The framework of using scene flow for motion recognition. Scene
    flow vectors are first transformed into Scene Flow Maps (SFM), and then using
    Channel Transform Kernels to transform SFM into an analogous RGB space to take
    advantage of pre-train models over ImageNet. Figure from [[151](#bib.bib151)].'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c99f1ca0a817d99c03513782430b4ae.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The framework of PI-based RNNs. It consists of three steps: (1)
    The pre-training step taking both depth maps and skeleton as input. An embedded
    encoder is trained in a standard CNN-RNN pipeline. (2) The trained encoder is
    used to initialize the learning step. A multi-task loss is applied to exploit
    the PI in the regression term as a secondary task. (3) Finally, refinement step
    aims to discover the latent PI by defining a bridging matrix, in order to maximize
    the effectiveness of the PI. The latent PI is utilized to close the gap between
    different information. The latent PI, bridging matrix and the network are optimized
    iteratively in an EM procedure. Figure from [[113](#bib.bib113)].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Shi et al. [[113](#bib.bib113)] fused depth and skeleton in a so-called privileged
    information (PI)-based RNN (PRNN) that exploits additional knowledge of skeleton
    sequences to obtain a better estimate of network parameters from depth map sequences.
    A bridging matrix is defined to connect softmax classification loss and regression
    loss by discovering latent PI in the refinement step. The whole process is illustrated
    in Figure [19](#S6.F19 "Figure 19 ‣ 6.1.2 RNN-based Approach ‣ 6.1 Segmented Motion
    Recognition ‣ 6 RGB+D-based Motion Recognition with Deep Learning ‣ RGB-D-based
    Human Motion Recognition with Deep Learning: A Survey").'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: For RGB and skeleton fusion, Mahassein and Todorovic [[89](#bib.bib89)] presented
    a regularization of LSTM learning where the output of another encoder LSTM (eLSTM)
    grounded on 3D human-skeleton training data is used as the regularization. This
    regularization rests on the hypothesis that since videos and skeleton sequences
    are about human motions their respective feature representations should be similar.
    The skeleton sequences, being view-independent and devoid of background clutter,
    are expected to facilitate capturing important motion patterns of human-body joints
    in 3D space.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Other-architecture-based Approach
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Shahroudy et al. [[110](#bib.bib110)] extracted hand-crafted features which
    are neither independent nor fully correlated from RGB and depth, and embedded
    the input feature into a space of factorized common and modality-specific components.
    The combination of shared and specific components in input features can be very
    complex and highly non-linear. In order to disentangle them, they stacked layers
    of non-linear auto encoder-based component factorization to form a deep shared-specific
    analysis network.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: In a RGB, depth and skeleton fusion method, Wu et al. [[160](#bib.bib160)] adopted
    Gaussian-Bernouilli Deep Belief Network(DBN) to extract high-level skeletal joint
    features and the learned representation is used to estimate the emission probability
    needed to infer gesture sequences. A 3D Convolutional Neural Network (3DCNN) was
    used to extract features from 2D multiple channel inputs such as depth and RGB
    images stacked along the 1D temporal domain. In addition, intermediate and late
    fusion strategies were investigated in combination with the temporal modeling.
    The result of both mechanisms indicates that multiple-channel fusion can outperform
    individual modules.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Continuous/Online Motion Recognition
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.2.1 RNN-based Approach
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chai et al. [[12](#bib.bib12)] proposed to fuse RGB and depth in a two-stream
    RNN (2S-RNN) for gesture recognition. They designed a fusion layer for depth and
    RGB before the LSTM layer. [[93](#bib.bib93)] presented an algorithm for joint
    segmentation and classification of dynamic hand gestures from continuous video
    streams. They proposed a network that employs a recurrent C3D with connectionist
    temporal classification (CTC) [[43](#bib.bib43)]. They trained a separate network
    for each modality and averaged their scores for final recognition. Beside RGB
    and depth modalities, they also adopted stereo-IR modality in their work.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We presented a comprehensive overview of RGB-D based motion recognition using
    deep learning. We defined a taxonomy covering two groups: segmented and continuous/online
    motion recognition, with four categories in each group based on the adopted modalities.
    From the viewpoint of encoding spatio-temporal-structural information contained
    in the video sequences, CNN, RNN and other networks adopted for motion recognition
    are discussed in each category. In subsequent sections, the relative performance
    of the different methods on several commonly used RGB-D datasets are analysed,
    and from the comparisons we highlight some challenges. The discussion on performance
    and challenges then provides a basis for outlining potential future research directions.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Performance Analysis of the Current Methods
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we compare the accuracy of different methods using several
    commonly used datasets, including CMU Mocap, HDM05, MSR-Action3D, MSRC-12, MSRDailyActivity3D,
    UTKinect, G3D, SBU Kinect Interaction, Berkeley MHAD, Northwestern-UCLA Multiview
    Action3D, ChaLearn LAP IsoGD, NTU RGB+D, ChaLearn2014, ChaLearn LAP ConGD, and
    PKU-MMD. These datasets cover motion capture sensor system, structured light cameras
    (Kinect v1) and ToF cameras (Kinect v2). The last three datasets are continuous
    datasets while the others are segmented datasets. The performance is evaluated
    using accuracy for segmented motion recognition, and Jaccard Index is added as
    another criteria for continuous motion recognition. The accuracy is calculated
    as the proportion of accurately labelled samples. The Jaccard index measures the
    average relative overlap between true and predicted sequences of frames for a
    given gesture/action. For a sequence $s$, let $G_{s,i}$ and $P_{s,i}$ be binary
    indicator vectors for which 1-values correspond to frames in which the $i^{th}$
    gesture/action label is being performed. The Jaccard Index for the $i^{th}$ class
    is defined for the sequence $s$ as:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J_{s,i}=\frac{G_{s,i}\bigcap P_{s,i}}{G_{s,i}\bigcup P_{s,i}},$ |  |
    (1) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: 'where $G_{s,i}$ is the ground truth of the $i^{th}$ gesture/action label in
    sequence $s$, and $P_{s,i}$ is the prediction for the $i^{th}$ label in sequence
    $s$. When $G_{s,i}$ and $P_{s,i}$ are empty, $J_{(s,i)}$ is defined to be 0. Then
    for the sequence $s$ with $l_{s}$ true labels, the Jaccard Index $J_{s}$ is calculated
    as:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J_{s}=\frac{1}{l_{s}}\sum_{i=1}^{L}J_{s,i}.$ |  | (2) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: 'For all test sequences $S={s_{1},...,s_{n}}$ with $n$ gestures/actions, the
    mean Jaccard Index $\overline{J_{S}}$ is used as the evaluation criteria and calculated
    as:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\overline{J_{S}}=\frac{1}{n}\sum_{j=1}^{n}J_{s_{j}}.$ |  | (3) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: The detailed comparison of different methods is presented in following Table LABEL:performance.
    From the Table we can see that there is no single approach that is able to produce
    the best performance over all datasets. Generally speaking, methods using multi-modal
    information can have better performance than their single modality counterpart
    due to the complementary properties of the three different modalities. On some
    datasets, such as NTU RGB+D dataset, current results suggest that CNN-based methods
    tend to be better than RNN-based methods. This is probably due to fact that CNN-based
    methods includes human empirical knowledge in the coding process, and could take
    advantage of pre-trained models over large image set, such as ImageNet. The combination
    of CNN and RNN seems to be a good choice for motion recognition, for instance,
    the C3D+ConvLSTM [[178](#bib.bib178)] method achieved promising results on ChaLearn
    LAP IsoGD dataset. For continuous motion recognition, RNN-based methods tend to
    achieve good results.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Challenges
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The advent of low-cost RGB-D sensors that have access to extra depth and skeleton
    data, has motivated the significant development of human motion recognition. Promising
    results have been achieved with deep learning approaches [[150](#bib.bib150),
    [174](#bib.bib174), [81](#bib.bib81)], on several constrained simple datasets,
    such as MSR-Action3D, Berkeley MHAD and SBU Kinect Interaction. Despite this success,
    results are far from satisfactory on some large complex datasets, such as ChaLearn
    LAP IsoGD and NTU RGB+D datasets and especially the continuous/online datasets.
    In fact, it is still very difficult to build a practical intelligent recognition
    system. Such goal poses several challenges:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Encoding temporal information. As discussed, there are several methods to encode
    temporal information. We can use CNN to extract frame-based features and then
    conduct temporal fusion [[64](#bib.bib64)], or adopt 3D filter and 3D pooling
    layers to learn motion features [[130](#bib.bib130)], or use optical/scene flow
    to extract motion information [[116](#bib.bib116), [151](#bib.bib151)], or encode
    the video into images [[6](#bib.bib6), [150](#bib.bib150), [155](#bib.bib155)],
    or use RNN/LSTM to model the temporal dependences [[22](#bib.bib22), [26](#bib.bib26),
    [82](#bib.bib82)]. However, all these approaches have their drawbacks. Temporal
    fusion method tends to neglect the temporal order; 3D filters and 3D pooling filters
    have a very rigid temporal structure and they only accept a predefined number
    of frames as input which is always short; optical/scene flow methods are computationally
    expensive; sequence to images methods inevitably loses temporal information during
    encoding; the weight sharing mechanism of RNN/LSTM methods make the sequence matching
    imprecise, but rather approximated, so an appropriate distance function must be
    used to predict the match probability. In fact, there is still no perfect method
    for temporal encoding, and how to model temporal information is a big challenge.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Small training data. Most of available deep learning methods rely on large labeled
    training data [[64](#bib.bib64), [130](#bib.bib130)]. However, in practical scenarios,
    obtaining large labeled training data is costly and laborious, even impossible,
    especially in medical-related applications. It has been shown that fine-tuning
    motion-based networks with spatial data (ImageNet) is more effective than training
    from scratch [[116](#bib.bib116), [155](#bib.bib155), [6](#bib.bib6), [151](#bib.bib151)].
    Strategies for data augmentation are also commonly used [[150](#bib.bib150)].
    Likewise, training mechanisms to avoid overfitting and control learning rate have
    also been studied [[121](#bib.bib121)]. However, it is still a challenge to effectively
    train deep networks from small training data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Viewpoint variation and occlusion. When skeletons are estimated from RGB images/video
    or depth maps, viewpoint variation may cause significantly different appearance
    of same actions, and occlusion would “crash” the skeleton data. Occlusion includes
    inter-occlusion caused by other subjects or objects, and self-occlusion created
    by the object/subject itself. Most of available datasets require subjects to perform
    actions in a visible and restricted view to avoid occlusion, and this results
    in limited view data collection and less occlusion. However, occlusion is inevitable
    in practical scenarios, especially for interactions. This makes it challenging
    to isolate individuals in overlapping area and extract features of a unique person;
    leading to the ineffectiveness of many of available approaches [[26](#bib.bib26),
    [109](#bib.bib109), [75](#bib.bib75)]. Possible solutions to handle viewpoint
    variation and occlusion include the use of multi-sensor systems [[96](#bib.bib96),
    [143](#bib.bib143), [109](#bib.bib109), [19](#bib.bib19)]. The multi-camera systems
    is able to generate multi-view data, but the drawback is the requirement of synchronization
    and feature/recognition fusion among different views. This usually increases processing
    complexity and computation cost. Several methods have been proposed to handle
    the viewpoint variation and occlusion. Wang et al. [[149](#bib.bib149)] proposed
    to rotate the depth data in 3D point clouds through different angles to deal with
    viewpoint invariance; spherical coordinate system corresponding to body center
    was developed to achieve view-independent motion recognition [[56](#bib.bib56)].
    However, these methods become less effective when occlusion occurs. How to effectively
    handle occlusion using deep learning methods is a new challenge.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Execution rate variation and repetition. The execution rate may vary due to
    the different performing styles and states of individuals. The varying rate results
    in different frames for the same motion. Repetition also bring about this issue.
    The global encoding methods [[53](#bib.bib53), [65](#bib.bib65), [83](#bib.bib83)]
    would become less effective due to the repetition. The commonly used methods to
    handle this problem is up/down sampling [[181](#bib.bib181), [174](#bib.bib174),
    [75](#bib.bib75)]. However, sampling methods would inevitably bring redundant
    or loss of useful information. Effective handling of this problem remains a challenge.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Cross-datasets. Many research works have been carried out to recognize human
    actions from RGB-D video clips. To learn an effective action classifier, most
    of the previous approaches rely on enough training labels. When being required
    to recognize the action in a different dataset, these approaches have to re-train
    the model using new labels. However, labeling video sequences is a very tedious
    and time-consuming task, especially when detailed spatial locations and time durations
    are required. Even though some works have studied this topic [[11](#bib.bib11),
    [123](#bib.bib123), [171](#bib.bib171)], they are all based on hand-crafted features,
    and the results are far from satisfactory due to the large distribution variances
    between different datasets, including different scenarios, different modalities,
    different views, different persons, and even different actions. How to deal with
    cross-datasets RGB-D motion recognition is a big challenge.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Online motion recognition. Most of available methods rely on segmented data,
    and their capability for online recognition is quite limited. Even though continuous
    motion recognition is one improved version where the videos are untrimmed, it
    still assumes that all the videos are available before processing. Thus, proposal-based
    methods [[115](#bib.bib115), [146](#bib.bib146)] can be adopted for offline processing.
    Differently from continuous motion recognition, online motion recognition aims
    to receive continuous streams of unprocessed visual data and recognize actions
    from an unsegmented stream of data in a continuous manner. So far two main approaches
    can be identified for online recognition, sliding window-based and RNN-based.
    Sliding window-based methods [[19](#bib.bib19)] are simple extension of segmented-based
    action recognition methods. They often consider the temporal coherence within
    the window for prediction and the window-based predictions are further fused to
    achieve online recognition. However, the performance of these methods are sensitive
    to the window size which depends on actions and is hard to set. Either too large
    or too small a window size could lead to significant drop in recognition. For
    RNN-based methods [[93](#bib.bib93), [78](#bib.bib78)], even though promising
    results have been achieved, it is still far from satisfactory in terms of performance.
    How to design effective practical online recognition system is a big challenge.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Action prediction. We are faced with numerous situations in which we must predict
    what actions other people are about to do in the near future. Predicting future
    actions before they are actually executed is a critical ingredient for enabling
    us to effectively interact with other humans on a daily basis [[104](#bib.bib104),
    [51](#bib.bib51), [69](#bib.bib69), [137](#bib.bib137), [105](#bib.bib105)]. There
    are mainly two challenges for this task: first, we need to capture the subtle
    details inherent in human movements that may imply a future action; second, predictions
    usually should be carried out as quickly as possible in the social world, when
    limited prior observations are available. Predicting the action of a person before
    it is actually executed has a wide range of applications in autonomous robots,
    surveillance and health care. How to develop effective algorithms for action prediction
    is really challenging.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Future Research Directions
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The discussion on the challenges faced by available methods allows us to outline
    several future research directions for the development of deep learning methods
    for motion recognition. While the list is not exhaustive, they point at research
    activities that may advance the field.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid networks. Most of previous methods adopted one type of neural networks
    for motion recognition. As discussed, there is no perfect solution for temporal
    encoding using single networks. Even though available works such as C3D+ConvLSTM [[178](#bib.bib178)]
    used two types of networks, the cascaded connection makes them dependent on each
    other during training. How to cooperatively train different kinds of networks
    would be a good research direction; for example, using the output of CNN to regularize
    RNN training in parallel.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Simultaneous exploitation of spatial-temporal-structural information. A video
    sequence has three important inherent properties that should be considered for
    motion analysis: spatial information, temporal information and structural information.
    Several previous methods tend to exploit the spatio-temporal information for motion
    recognition, however, structural information contained in the video is rarely
    explicitly mined. Concurrent mining of these three kinds of information with deep
    learning would be an interesting topic in the future [[59](#bib.bib59)].'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Fusion of multiple modalities. While significant progress has been achieved
    by singly using RGB, skeleton or depth modality, effective deep networks for fusion
    of multi-modal data would be a promising direction. For example, methods such
    as SFAM [[151](#bib.bib151)] and PRNN [[113](#bib.bib113)] have pioneered the
    research in this direction. The work SFAM [[151](#bib.bib151)] proposed to extract
    scene flow for motion analysis. The strategy of fusing the RGB and depth modalities
    at the outset allowed the capture of rich 3D motion information. In PRNN [[113](#bib.bib113)]
    the concept of privileged information (side information) was introduced for deep
    networks training and showed some promise. Zolfaghari et al. [[182](#bib.bib182)]
    proposed the integration of different modalities via a Markov chain, which leads
    to a sequential refinement of action labels. So far, most methods considered the
    three modalities as separate channels and fused them at a later or scoring stage
    using different fusion methods without cooperatively exploiting their complementary
    properties. Cooperative training using different modalities would be a promising
    research area.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale datasets. With the development of data-hungry deep learning approach,
    there is demand for large scale RGB-D datasets. Even though there are several
    large datasets, such as NTU RGB+D Dataset [[109](#bib.bib109)] and ChaLearn LAP
    IsoGD Dataset [[139](#bib.bib139)], they are focused on specific tasks. Various
    large-scale RGB-D datasets are needed to facilitate research in this field. For
    instance, large-scale fine-grained RGB-D motion recognition datasets and large-scale
    occlusion-based RGB-D motion recognition datasets are urgently needed.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Zero/One-shot learning. As discussed, it is not always easy to collect large
    scale labeled data. Learning from a few examples remains a key challenge in machine
    learning. Despite recent advances in important domains such as vision and language,
    the standard supervised deep learning paradigm does not offer a satisfactory solution
    for learning new concepts rapidly from little data. How to adopt deep learning
    methods for zero/one shot RGB-D-based motion recognition would be an interesting
    research direction. Zero/one-shot learning is about being able to recognize gesture/action
    classes that are never seen or only one training sample per class before. This
    type of recognition should carry embedded information universal to all other gestures/actions.
    In the past few years, there are some works on zero/one-shot learning. For example,
    Wan et al. [[138](#bib.bib138)] proposed the novel spatial-temporal features for
    one-shot learning gesture recognition and have got promising performances on Chalearn
    Gesture Dataset CGD) [[45](#bib.bib45)]. For zero-shot learning, Madapana and
    Wachs [[88](#bib.bib88)] proposed a new paradigm based on adaptive learning which
    it is possible to determine the amount of transfer learning carried out by the
    algorithm and how much knowledge is acquired for a new gesture observation. However,
    the mentioned works used traditional methods (such as bag of visual words model [[140](#bib.bib140)]).
    Mettes et al. [[92](#bib.bib92)] proposed a spatial-aware object embedding for
    zero-shot action localization and classification. The spatial-aware embedding
    generate action tubes by incorporating word embeddings, box locations for actors
    and objects, as well as their spatial relations. However, how to effectively adopt
    deep learning methods for zero/one shot RGB-D based motion recognition would be
    still an interesting research direction especially when using only very few training
    samples.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Outdoor practical scenarios. Although lots of RGB-D datasets have been collected
    during the last few years, there is a big gap between the collected datasets and
    wild environment due to constrained environment setting and insufficient categories
    and samples. For example, most available datasets do not involve much occlusion
    cases probably due to the collapse of skeleton dataset in case of occlusion. However,
    in practical scenarios, occlusion is inevitable. How to recover or find cues from
    multi-modal data for such recognition tasks would be an interesting research direction.
    Besides, with the development of depth sensors, further distances could be captured,
    and recognition in outdoor practical scenarios will gain the attention of researchers.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning/Self-learning. Collecting labeled datasets are time-consuming
    and costly, hence learning from unsupervised video data is required. Mobile robots
    mounted with RGB-D cameras need to continuously learn from the environment and
    without human intervention. How to automatically learn from the unlabeled data
    stream to improve the learning capability of deep networks would be a fruitful
    and useful research direction. Generative Adversarial Net (GAN) [[50](#bib.bib50)]
    has got much processes recently in image generation task, such as face generation,
    text-to-image task. Besides, it also can be used for recognition task. For example,
    Luan et al. [[131](#bib.bib131)] proposed a Disentangled Representation learning
    Generative Adversarial Networks (DR-GAN) for pose-invariant face recognition.
    Therefore, we believe the GAN-based techniques also can be used for action/gesture
    recognition, which is a great exciting direction for research. Carl et al. [[136](#bib.bib136)]
    proposed a generative adversarial network for video with spatial-temporal convolutional
    architecture that untangles the scene’s foreground from backgrounds. This is an
    initial work to capitalize on large amounts of unlabeled video in order to learn
    a model of scene dynamic for both video recognition tasks (e.g. action classification)
    and video generation tasks (e.g. future prediction). Increasing research will
    be reported in the coming years on GAN-based methods for video-based recognition.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Online motion recognition and prediction. Online motion recognition and prediction
    is required in practical applications, and arguably this is the final goal of
    motion recognition systems. Differently from segmented recognition, online motion
    recognition requires the analysis of human behavior in a continuous manner, and
    prediction aims to recognize or anticipate actions that would happen. How to design
    effective online recognition and prediction systems with deep learning methods
    has attracted some attention. For example, Vondrick et al. [[135](#bib.bib135)]
    introduced a framework that capitalizes on temporal structure in unlabeled video
    to learn to anticipate human actions and objects based on CNN, and it is likely
    to emerge as an active research area.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper presents a comprehensive survey of RGB-D based motion recognition
    using deep learning. We provide a brief overview of existing commonly used datasets
    and pointed at surveys that focused mainly on datasets. The available methods
    are grouped into four categories according to the modality: RGB-based, depth-based,
    skeleton-based and RGB+D-based. The three modalities have their own specific features
    and lead to different choices of deep learning methods to take advantages of their
    properties. Spatial, temporal and structural information inherent in a video sequence
    is defined, and from the viewpoint of spatio-temporal-structural encoding, we
    analyse the pros and cons of available methods. Based on the insights drawn from
    the survey, several potential research directions are described, indicating the
    numerous opportunities in this field despite the advances achieved to date.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jun Wan is partially supported by the National Natural Science Foundation of
    China [61502491]. Sergio Escalera is partially supported by Spanish project [TIN2016-74946-P]
    (MINECO/FEDER, UE) and CERCA Programme / Generalitat de Catalunya.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CMU [2001] , 2001. CMU Graphics Lab Motion Capture Database, http://mocap.cs.cmu.edu/.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aggarwal and Cai [1999] Aggarwal, J., Cai, Q., 1999. Human motion analysis:
    A review, in: Computer Vision and Image Understanding.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aggarwal and Xia [2014] Aggarwal, J.K., Xia, L., 2014. Human activity recognition
    from 3D data: A review. Pattern Recognition Letters 48, 70–80.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Asadi-Aghbolaghi et al. [2017] Asadi-Aghbolaghi, M., Clapes, A., Bellantonio,
    M., Escalante, H.J., Ponce-López, V., Baró, X., Guyon, I., Kasaei, S., Escalera,
    S., 2017. A survey on deep learning based approaches for action and gesture recognition
    in image sequences, in: Automatic Face & Gesture Recognition (FG 2017), 2017 12th
    IEEE International Conference on, IEEE. pp. 476–483.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baccouche et al. [2011] Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., Baskurt,
    A., 2011. Sequential deep learning for human action recognition, in: International
    Workshop on Human Behavior Understanding, Springer. pp. 29–39.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bilen et al. [2016] Bilen, H., Fernando, B., Gavves, E., Vedaldi, A., Gould,
    S., 2016. Dynamic image networks for action recognition, in: CVPR.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bloom et al. [2012] Bloom, V., Makris, D., Argyriou, V., 2012. G3D: A gaming
    action dataset and real time action recognition evaluation framework, in: CVPRW.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bobick and Davis [2001] Bobick, A.F., Davis, J.W., 2001. The recognition of
    human movement using temporal templates. IEEE Transactions on pattern analysis
    and machine intelligence 23, 257–267.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Buch et al. [2017] Buch, S., Escorcia, V., Shen, C., Ghanem, B., Niebles, J.C.,
    2017. Sst: Single-stream temporal action proposals, in: CVPR.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Camgoz et al. [2016] Camgoz, N.C., Hadfield, S., Koller, O., Bowden, R., 2016.
    Using convolutional 3D neural networks for user-independent continuous gesture
    recognition, in: 2016 23rd International Conference on Pattern Recognition (ICPR),
    pp. 49–54.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2010] Cao, L., Liu, Z., Huang, T.S., 2010. Cross-dataset action
    detection, in: Computer vision and pattern recognition (CVPR), IEEE. pp. 1998–2005.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chai et al. [2016] Chai, X., Liu, Z., Yin, F., Liu, Z., Chen, X., 2016. Two
    streams recurrent neural networks for large-scale continuous gesture recognition,
    in: International Conference on Pattern Recognition Workshops.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen [2010] Chen, B., 2010. Deep learning of invariant spatio-temporal features
    from video. Ph.D. thesis. University of British Columbia.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2013] Chen, L., Wei, H., Ferryman, J., 2013. A survey of human
    motion analysis using depth imagery. Pattern Recognition Letters 34, 1995–2006.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2017] Chen, Y., Shen, C., Wei, X.S., Liu, L., Yang, J., 2017.
    Adversarial posenet: A structure-aware convolutional network for human pose estimation,
    in: The IEEE International Conference on Computer Vision (ICCV).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. [2016] Cheng, H., Yang, L., Liu, Z., 2016. Survey on 3D hand gesture
    recognition. IEEE Transactions on Circuits and Systems for Video Technology 26,
    1659–1673.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cherian et al. [2017] Cherian, A., Fernando, B., Harandi, M., Gould, S., 2017.
    Generalized rank pooling for activity recognition, in: CVPR.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chéron et al. [2015] Chéron, G., Laptev, I., Schmid, C., 2015. P-cnn: Pose-based
    cnn features for action recognition, in: ICCV, pp. 3218–3226.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chunhui et al. [2017] Chunhui, L., Yueyu, H., Yanghao, L., Sijie, S., Jiaying,
    L., 2017. Pku-mmd: A large scale benchmark for continuous multi-modal human action
    understanding. arXiv preprint arXiv:1703.07475 .'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dave et al. [2017] Dave, A., Russakovsky, O., Ramanan, D., 2017. Predictive-corrective
    networks for action detection, in: CVPR.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Geest et al. [2016] De Geest, R., Gavves, E., Ghodrati, A., Li, Z., Snoek,
    C., Tuytelaars, T., 2016. Online action detection, in: ECCV, pp. 269–284.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Donahue et al. [2015] Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach,
    M., Venugopalan, S., Saenko, K., Darrell, T., 2015. Long-term recurrent convolutional
    networks for visual recognition and description, in: CVPR.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2017] Du, W., Wang, Y., Qiao, Y., 2017. Rpan: An end-to-end recurrent
    pose-attention network for action recognition in videos, in: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 3725–3734.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2015a] Du, Y., Fu, Y., Wang, L., 2015a. Skeleton based action recognition
    with convolutional neural network, in: Pattern Recognition (ACPR), 2015 3rd IAPR
    Asian Conference on, IEEE. pp. 579–583.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. [2016] Du, Y., Fu, Y., Wang, L., 2016. Representation learning of
    temporal dynamics for skeleton-based action recognition. IEEE Transactions on
    Image Processing 25, 3010–3022.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2015b] Du, Y., Wang, W., Wang, L., 2015b. Hierarchical recurrent
    neural network for skeleton based action recognition, in: CVPR.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duan et al. [2016] Duan, J., Zhou, S., Wan, J., Guo, X., Li, S.Z., 2016. Multi-modality
    fusion based on consensus-voting and 3D convolution for isolated gesture recognition.
    arXiv preprint arXiv:1611.06689 .
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Escalera et al. [2016] Escalera, S., Athitsos, V., Guyon, I., 2016. Challenges
    in multimodal gesture recognition. Journal of Machine Learning Research 17, 1–54.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Escalera et al. [2014] Escalera, S., Baró, X., Gonzalez, J., Bautista, M.A.,
    Madadi, M., Reyes, M., Ponce-López, V., Escalante, H.J., Shotton, J., Guyon, I.,
    2014. Chalearn looking at people challenge 2014: Dataset and results, in: Workshop
    at the European Conference on Computer Vision, Springer. pp. 459–473.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Escorcia et al. [2016] Escorcia, V., Heilbron, F.C., Niebles, J.C., Ghanem,
    B., 2016. Daps: Deep action proposals for action understanding, in: European Conference
    on Computer Vision, Springer. pp. 768–784.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feichtenhofer et al. [2016a] Feichtenhofer, C., Pinz, A., Wildes, R., 2016a.
    Spatiotemporal residual networks for video action recognition, in: Advances in
    Neural Information Processing Systems, pp. 3468–3476.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feichtenhofer et al. [2016b] Feichtenhofer, C., Pinz, A., Zisserman, A., 2016b.
    Convolutional two-stream network fusion for video action recognition, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1933–1941.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fernando et al. [2016a] Fernando, B., Anderson, P., Hutter, M., Gould, S.,
    2016a. Discriminative hierarchical rank pooling for activity recognition, in:
    CVPR.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernando et al. [2016b] Fernando, B., Gavves, S., Mogrovejo, O., Antonio, J.,
    Ghodrati, A., Tuytelaars, T., 2016b. Rank pooling for action recognition. IEEE
    Transactions on Pattern Analysis and Machine Intelligence .
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fernando and Gould [2016] Fernando, B., Gould, S., 2016. Learning end-to-end
    video classification with rank-pooling, in: ICML.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fothergill et al. [2012] Fothergill, S., Mentis, H.M., Nowozin, S., Kohli,
    P., 2012. Instructing people for training gestural interactive systems, in: ACM
    Conference on Computer-Human Interaction (ACM HCI).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. [2017] Gao, J., Yang, Z., Sun, C., Chen, K., Nevatia, R., 2017.
    Turn tap: Temporal unit regression network for temporal action proposals, in:
    ICCV.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girshick [2015] Girshick, R., 2015. Fast r-cnn, in: Proceedings of the IEEE
    International Conference on Computer Vision, pp. 1440–1448.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girshick et al. [2014] Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014.
    Rich feature hierarchies for accurate object detection and semantic segmentation,
    in: Computer Vision and Pattern Recognition.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gkioxari and Malik [2015] Gkioxari, G., Malik, J., 2015. Finding action tubes,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 759–768.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2016] Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep
    Learning. MIT Press. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow et al. [2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial
    nets, in: Advances in neural information processing systems, pp. 2672–2680.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graves et al. [2006] Graves, A., Fernández, S., Gomez, F., Schmidhuber, J.,
    2006. Connectionist temporal classification: labelling unsegmented sequence data
    with recurrent neural networks, in: Proceedings of the 23rd international conference
    on Machine learning, ACM. pp. 369–376.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo and Lai [2014] Guo, G., Lai, A., 2014. A survey on still image based human
    action recognition. Pattern Recognition 47, 3343–3361.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guyon et al. [2014] Guyon, I., Athitsos, V., Jangyodsuk, P., Escalante, H.J.,
    2014. The chalearn gesture dataset (CGD 2011). Machine Vision and Applications
    25, 1929–1951.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. [2017] Han, F., Reily, B., Hoff, W., Zhang, H., 2017. Space-time
    representation of people based on 3D skeletal data: A review. Computer Vision
    and Image Understanding .'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2016] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 770–778.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heilbron et al. [2015] Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.,
    2015. Activitynet: A large-scale video benchmark for human activity understanding,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 961–970.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Herath et al. [2017] Herath, S., Harandi, M., Porikli, F., 2017. Going deeper
    into action recognition: A survey. Image and Vision Computing 60, 4–21.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ho and Ermon [2016] Ho, J., Ermon, S., 2016. Generative adversarial imitation
    learning, in: Advances in Neural Information Processing Systems, pp. 4565–4573.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoai and De la Torre [2014] Hoai, M., De la Torre, F., 2014. Max-margin early
    event detectors. International Journal of Computer Vision 107, 191–202.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2017a] Hou, R., Chen, C., Shah, M., 2017a. Tube convolutional neural
    network (t-cnn) for action detection in videos, in: ICCV.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2016] Hou, Y., Li, Z., Wang, P., Li, W., 2016. Skeleton optical
    spectra based action recognition using convolutional neural networks, in: Circuits
    and Systems for Video Technology, IEEE Transactions on, pp. 1–5.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. [2017b] Hou, Y., Wang, S., Wang, P., Gao, Z., Li, W., 2017b. Spatially
    and temporally structured global to local aggregation of dynamic depth information
    for action recognition. IEEE Access .
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2016] Huang, D.A., Fei-Fei, L., Niebles, J.C., 2016. Connectionist
    temporal modeling for weakly supervised action labeling, in: European Conference
    on Computer Vision, Springer. pp. 137–153.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2017] Huang, Z., Wan, C., Probst, T., Van Gool, L., 2017. Deep
    learning on lie groups for skeleton-based action recognition, in: CVPR.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ijjina and Krishna Mohan [2016] Ijjina, E.P., Krishna Mohan, C., 2016. Classification
    of human actions using pose-based features and stacked auto encoder. Pattern Recognition
    Letters 83, 268–277.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Insafutdinov et al. [2016] Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
    M., Schiele, B., 2016. Deepercut: A deeper, stronger, and faster multi-person
    pose estimation model, in: European Conference on Computer Vision, Springer. pp.
    34–50.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain et al. [2016] Jain, A., Zamir, A.R., Savarese, S., Saxena, A., 2016. Structural-rnn:
    Deep learning on spatio-temporal graphs, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 5308–5317.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jhuang et al. [2007] Jhuang, H., Serre, T., Wolf, L., Poggio, T., 2007. A biologically
    inspired system for action recognition, in: Proc. IEEE 11th International Conference
    on Computer Vision, pp. 1–8.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. [2013] Ji, S., Xu, W., Yang, M., Yu, K., 2013. 3D convolutional neural
    networks for human action recognition. TPAMI 35, 221–231.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2015] Jiang, F., Zhang, S., Wu, S., Gao, Y., Zhao, D., 2015. Multi-layered
    gesture recognition with Kinect. Journal of Machine Learning Research 16, 227–254.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kalogeiton et al. [2017] Kalogeiton, V., Weinzaepfel, P., Ferrari, V., Schmid,
    C., 2017. Action tubelet detector for spatio-temporal action localization, in:
    ICCV.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karpathy et al. [2014] Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar,
    R., Fei-Fei, L., 2014. Large-scale video classification with convolutional neural
    networks, in: Proc. IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 1725–1732.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ke et al. [2017a] Ke, Q., An, S., Bennamoun, M., Sohel, F., Boussaid, F., 2017a.
    Skeletonnet: Mining deep part features for 3D action recognition. IEEE Signal
    Processing Letters .'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ke et al. [2017b] Ke, Q., Bennamoun, M., An, S., Sohel, F., Boussaid, F., 2017b.
    A new representation of skeleton sequences for 3D action recognition, in: CVPR.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim and Reiter [2017] Kim, T.S., Reiter, A., 2017. Interpretable 3D human action
    analysis with temporal convolutional networks, in: CVPR.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krizhevsky et al. [2012] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    Imagenet classification with deep convolutional neural networks, in: Proc. Annual
    Conference on Neural Information Processing Systems (NIPS), pp. 1106–1114.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. [2014] Lan, T., Chen, T.C., Savarese, S., 2014. A hierarchical representation
    for future action prediction, in: European Conference on Computer Vision, Springer.
    pp. 689–704.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le et al. [2011] Le, Q.V., Zou, W.Y., Yeung, S.Y., Ng, A.Y., 2011. Learning
    hierarchical invariant spatio-temporal features for action recognition with independent
    subspace analysis, in: Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
    Conference on, IEEE. pp. 3361–3368.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lea et al. [2017] Lea, C., Flynn, M.D., Vidal, R., Reiter, A., Hager, G.D.,
    2017. Temporal convolutional networks for action segmentation and detection, in:
    CVPR.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lea et al. [2016] Lea, C., Reiter, A., Vidal, R., Hager, G.D., 2016. Segmental
    spatiotemporal CNNs for fine-grained action segmentation, in: European Conference
    on Computer Vision, Springer. pp. 36–52.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. [2017] Lee, I., Kim, D., Kang, S., Lee, S., 2017. Ensemble deep
    learning for skeleton-based action recognition using temporal sliding lstm networks,
    in: ICCV, pp. 1012–1020.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lev et al. [2016] Lev, G., Sadeh, G., Klein, B., Wolf, L., 2016. Rnn fisher
    vectors for action recognition and image annotation, in: European Conference on
    Computer Vision, Springer. pp. 833–850.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2017a] Li, C., Hou, Y., Wang, P., Li, W., 2017a. Joint distance maps
    based action recognition with convolutional neural networks. IEEE Signal Processing
    Letters 24, 624–628.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2017b] Li, W., Wen, L., Chang, M.C., Nam Lim, S., Lyu, S., 2017b.
    Adaptive rnn tree for large-scale human action recognition, in: Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1444–1452.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2010] Li, W., Zhang, Z., Liu, Z., 2010. Action recognition based
    on a bag of 3D points, in: CVPRW.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2016a] Li, Y., Lan, C., Xing, J., Zeng, W., Yuan, C., Liu, J., 2016a.
    Online human action detection using joint classification-regression recurrent
    neural networks, in: European Conference on Computer Vision, Springer. pp. 203–220.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2016b] Li, Y., Miao, Q., Tian, K., Fan, Y., Xu, X., Li, R., Song,
    J., 2016b. Large-scale gesture recognition with a fusion of RGB-D data based on
    the C3D model, in: Pattern Recognition (ICPR), 2016 23rd International Conference
    on, IEEE. pp. 25–30.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2016c] Li, Y., Miao, Q., Tian, K., Fan, Y., Xu, X., Li, R., Song.,
    J., 2016c. Large-scale gesture recognition with a fusion of RGB-D data based on
    the C3D model, in: Proceedings of ICPRW.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2016a] Liu, J., Shahroudy, A., Xu, D., Wang, G., 2016a. Spatio-temporal
    LSTM with trust gates for 3D human action recognition, in: ECCV.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2017a] Liu, J., Wang, G., Hu, P., Duan, L.Y., Kot, A.C., 2017a.
    Global context-aware attention lstm networks for 3D action recognition, in: CVPR.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2017b] Liu, M., Liu, H., Chen, C., 2017b. Enhanced skeleton visualization
    for view invariant human action recognition. Pattern Recognition 68, 346–362.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2016b] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.,
    Fu, C.Y., Berg, A.C., 2016b. Ssd: Single shot multibox detector, in: ECCV, Springer.
    pp. 21–37.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2016c] Liu, Z., Zhang, C., Tian, Y., 2016c. 3D-based deep convolutional
    neural network for action recognition with depth sequences. Image and Vision Computing
    55, 93–100.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. [2017] Luo, Z., Peng, B., Huang, D.A., Alahi, A., Fei-Fei, L., 2017.
    Unsupervised learning of long-term motion dynamics for videos, in: CVPR.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2016] Ma, S., Sigal, L., Sclaroff, S., 2016. Learning activity progression
    in lstms for activity detection and early detection, in: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 1942–1950.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madapana and Wachs [2017] Madapana, N., Wachs, J.P., 2017. A semantical & analytical
    approach for zero shot gesture learning, in: FG workshop.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahasseni and Todorovic [2016] Mahasseni, B., Todorovic, S., 2016. Regularizing
    long short term memory with 3D human-skeleton sequences for action recognition,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 3054–3062.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathieu et al. [2016] Mathieu, M., Couprie, C., LeCun, Y., 2016. Deep multi-scale
    video prediction beyond mean square error, in: Proc. International Conference
    on Learning Representations (ICLR).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memisevic and Hinton [2007] Memisevic, R., Hinton, G., 2007. Unsupervised learning
    of image transformations, in: Computer Vision and Pattern Recognition, IEEE Conference
    on, IEEE. pp. 1–8.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mettes and Snoek [2017] Mettes, P., Snoek, C.G., 2017. Spatial-aware object
    embeddings for zero-shot localization and classification of actions, in: ICCV.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Molchanov et al. [2016] Molchanov, P., Yang, X., Gupta, S., Kim, K., Tyree,
    S., Kautz, J., 2016. Online detection and classification of dynamic hand gestures
    with recurrent 3D convolutional neural network, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4207–4215.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Müller et al. [2007] Müller, M., Röder, T., Clausen, M., Eberhardt, B., Krüger,
    B., Weber, A., 2007. Documentation Mocap Database HDM05. Technical Report CG-2007-2.
    Universität Bonn.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng et al. [2015] Ng, J., Hausknecht, M., Vijayanarasimhan, S., Vinyals, O.,
    Monga, R., Toderici, G., 2015. Beyond short snippets: Deep networks for video
    classification, in: CVPR.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ofli et al. [2013] Ofli, F., Chaudhry, R., Kurillo, G., Vidal, R., Bajcsy,
    R., 2013. Berkeley MHAD: A comprehensive multimodal human action database, in:
    Proc. IEEE Workshop on Applications of Computer Vision (WACV), pp. 53–60.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng and Schmid [2016] Peng, X., Schmid, C., 2016. Multi-region two-stream
    r-cnn for action detection, in: European Conference on Computer Vision, Springer.
    pp. 744–759.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pigou et al. [2016] Pigou, L., Van Den Oord, A., Dieleman, S., Van Herreweghe,
    M., Dambre, J., 2016. Beyond temporal pooling: Recurrence and temporal convolutions
    for gesture recognition in video. International Journal of Computer Vision , 1–10.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poppe [2010] Poppe, R., 2010. A survey on vision-based human action recognition.
    Image and vision computing 28, 976–990.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Presti and La Cascia [2016] Presti, L.L., La Cascia, M., 2016. 3D skeleton-based
    human action classification: A survey. Pattern Recognition 53, 130–147.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rahmani and Bennamoun [2017] Rahmani, H., Bennamoun, M., 2017. Learning action
    recognition model from depth and skeleton videos, in: ICCV, pp. 5832–5841.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rahmani and Mian [2016] Rahmani, H., Mian, A., 2016. 3D action recognition
    from novel viewpoints, in: Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 1506–1515.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2015] Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn:
    Towards real-time object detection with region proposal networks, in: Advances
    in neural information processing systems, pp. 91–99.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ryoo [2011] Ryoo, M.S., 2011. Human activity prediction: Early recognition
    of ongoing activities from streaming videos, in: Computer Vision (ICCV), 2011
    IEEE International Conference on, IEEE. pp. 1036–1043.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sadegh Aliakbarian et al. [2017] Sadegh Aliakbarian, M., Sadat Saleh, F., Salzmann,
    M., Fernando, B., Petersson, L., Andersson, L., 2017. Encouraging lstms to anticipate
    actions very early, in: ICCV, pp. 280–289.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saha et al. [2017] Saha, S., Singh, G., Cuzzolin, F., 2017. Amtnet: Action-micro-tube
    regression by end-to-end trainable deep architecture, in: ICCV.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salakhutdinov et al. [2013] Salakhutdinov, R., Tenenbaum, J.B., Torralba, A.,
    2013. Learning with hierarchical-deep models. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 35, 1958–1971.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sánchez et al. [2013] Sánchez, J., Perronnin, F., Mensink, T., Verbeek, J.,
    2013. Image classification with the fisher vector: Theory and practice. International
    journal of computer vision 105, 222–245.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shahroudy et al. [2016] Shahroudy, A., Liu, J., Ng, T.T., Wang, G., 2016. NTU
    RGB+ D: A large scale dataset for 3D human activity analysis, in: CVPR.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shahroudy et al. [2017] Shahroudy, A., Ng, T.T., Gong, Y., Wang, G., 2017. Deep
    multimodal feature analysis for action recognition in rgb+ d videos. IEEE Transactions
    on Pattern Analysis and Machine Intelligence .
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. [2016] Sharma, S., Kiros, R., Salakhutdinov, R., 2016. Action
    recognition using visual attention. ICLRW .
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. [2017] Shi, Y., Tian, Y., Wang, Y., Zeng, W., Huang, T., 2017. Learning
    long-term dependencies for action recognition with a biologically-inspired deep
    network, in: ICCV, pp. 716–725.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi and Kim [2017] Shi, Z., Kim, T.K., 2017. Learning and refining of privileged
    information-based rnns for action recognition from depth sequences, in: Proc.
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR).'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shotton et al. [2011] Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio,
    M., Moore, R., Kipman, A., Blake, A., 2011. Real-time human pose recognition in
    parts from single depth images, in: Proc. IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR), pp. 1297–1304.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shou et al. [2016] Shou, Z., Wang, D., Chang, S.F., 2016. Temporal action localization
    in untrimmed videos via multi-stage cnns, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 1049–1058.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonyan and Zisserman [2014a] Simonyan, K., Zisserman, A., 2014a. Two-stream
    convolutional networks for action recognition in videos, in: NIPS.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman [2014b] Simonyan, K., Zisserman, A., 2014b. Very deep
    convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556
    .
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. [2016] Singh, B., Marks, T.K., Jones, M., Tuzel, O., Shao, M.,
    2016. A multi-stream bi-directional recurrent neural network for fine-grained
    action detection, in: Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 1961–1970.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. [2017] Singh, G., Saha, S., Sapienza, M., Torr, P., Cuzzolin,
    F., 2017. Online real-time multiple spatiotemporal action localisation and prediction,
    in: ICCV.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2017] Song, S., Lan, C., Xing, J., Zeng, W., Liu, J., 2017. An
    end-to-end spatio-temporal attention model for human action recognition from skeleton
    data, in: Thirty-First AAAI Conference on Artificial Intelligence.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. [2014] Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever,
    I., Salakhutdinov, R., 2014. Dropout: a simple way to prevent neural networks
    from overfitting. Journal of Machine Learning Research 15, 1929–1958.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. [2015] Srivastava, N., Mansimov, E., Salakhudinov, R., 2015.
    Unsupervised learning of video representations using lstms, in: ICML, pp. 843–852.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sultani and Saleemi [2014] Sultani, W., Saleemi, I., 2014. Human action recognition
    across datasets by foreground-weighted histogram decomposition, in: IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 764–771.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2017] Sun, L., Jia, K., Chen, K., Yeung, D.Y., Shi, B.E., Savarese,
    S., 2017. Lattice long short-term memory for human action recognition. arXiv preprint
    arXiv:1708.03958 .
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2015] Sun, L., Jia, K., Yeung, D.Y., Shi, B.E., 2015. Human action
    recognition using factorized spatio-temporal convolutional networks, in: Proceedings
    of the IEEE International Conference on Computer Vision, pp. 4597–4605.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taylor et al. [2010] Taylor, G.W., Fergus, R., LeCun, Y., Bregler, C., 2010.
    Convolutional learning of spatio-temporal features, in: Proc. European Conference
    on Computer Vision (ECCV), pp. 140–153.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teh et al. [2004] Teh, Y.W., Jordan, M.I., Beal, M.J., Blei, D.M., 2004. Sharing
    clusters among related groups: Hierarchical dirichlet processes., in: NIPS, pp.
    1385–1392.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Theis [2007] Theis, F.J., 2007. Towards a general independent subspace analysis,
    in: Advances in Neural Information Processing Systems, pp. 1361–1368.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Toshev and Szegedy [2014] Toshev, A., Szegedy, C., 2014. Deeppose: Human pose
    estimation via deep neural networks, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 1653–1660.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. [2015] Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri,
    M., 2015. Learning spatiotemporal features with 3D convolutional networks, in:
    ICCV.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. [2017] Tran, L., Yin, X., Liu, X., 2017. Disentangled representation
    learning gan for pose-invariant face recognition, in: CVPR, p. 7.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Turaga et al. [2008] Turaga, P., Chellappa, R., Subrahmanian, V.S., Udrea,
    O., 2008. Machine recognition of human activities: A survey. IEEE Transactions
    on Circuits and Systems for Video Technology 18, 1473–1488.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varol et al. [2016] Varol, G., Laptev, I., Schmid, C., 2016. Long-term temporal
    convolutions for action recognition. arXiv preprint arXiv:1604.04494 .
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Veeriah et al. [2015] Veeriah, V., Zhuang, N., Qi, G.J., 2015. Differential
    recurrent neural networks for action recognition, in: ICCV.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vondrick et al. [2016a] Vondrick, C., Pirsiavash, H., Torralba, A., 2016a.
    Anticipating visual representations from unlabeled video, in: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 98–106.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vondrick et al. [2016b] Vondrick, C., Pirsiavash, H., Torralba, A., 2016b.
    Generating videos with scene dynamics, in: Advances In Neural Information Processing
    Systems, pp. 613–621.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. [2014] Vu, T.H., Olsson, C., Laptev, I., Oliva, A., Sivic, J., 2014.
    Predicting actions from static scenes, in: European Conference on Computer Vision,
    Springer. pp. 421–436.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. [2016a] Wan, J., Guo, G., Li, S.Z., 2016a. Explore efficient local
    features from RGB-D data for one-shot learning gesture recognition. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 38, 1626–1639.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan et al. [2016b] Wan, J., Li, S.Z., Zhao, Y., Zhou, S., Guyon, I., Escalera,
    S., 2016b. Chalearn looking at people RGB-D isolated and continuous datasets for
    gesture recognition, in: CVPRW.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. [2013] Wan, J., Ruan, Q., Li, W., Deng, S., 2013. One-shot learning
    gesture recognition from RGB-D data using bag of features. Journal of Machine
    Learning Research 14, 2549–2582.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2013] Wang, H., Kläser, A., Schmid, C., Liu, C.L., 2013. Dense
    trajectories and motion boundary descriptors for action recognition. International
    Journal of Computer Vision 103, 60–79.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2012] Wang, J., Liu, Z., Wu, Y., Yuan, J., 2012. Mining actionlet
    ensemble for action recognition with depth cameras, in: CVPR.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2014] Wang, J., Nie, X., Xia, Y., Wu, Y., Zhu, S.C., 2014. Cross-view
    action modeling, learning and recognition, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2649–2656.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2003] Wang, L., Hu, W., Tan, T., 2003. Recent developments in human
    motion analysis. Pattern recognition 36, 585–601.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2015a] Wang, L., Qiao, Y., Tang, X., 2015a. Action recognition
    with trajectory-pooled deep-convolutional descriptors, in: CVPR, pp. 4305–4314.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2017a] Wang, L., Xiong, Y., Lin, D., Van Gool, L., 2017a. Untrimmednets
    for weakly supervised action recognition and detection, in: CVPR.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2016a] Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang,
    X., Van Gool, L., 2016a. Temporal segment networks: towards good practices for
    deep action recognition, in: European Conference on Computer Vision, pp. 20–36.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018a] Wang, P., Li, W., Gao, Z., Tang, C., Ogunbona, P., 2018a.
    Depth pooling based large-scale 3d action recognition with convolutional neural
    networks. IEEE Transactions on Multimedia .
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2015b] Wang, P., Li, W., Gao, Z., Tang, C., Zhang, J., Ogunbona,
    P.O., 2015b. Convnets-based action recognition from depth maps through virtual
    cameras and pseudocoloring, in: ACM MM.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2016b] Wang, P., Li, W., Gao, Z., Zhang, J., Tang, C., Ogunbona,
    P., 2016b. Action recognition from depth maps using deep convolutional neural
    networks. THMS 46, 498–509.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2017b] Wang, P., Li, W., Gao, Z., Zhang, Y., Tang, C., Ogunbona,
    P., 2017b. Scene flow to action map: A new representation for RGB-D based action
    recognition with convolutional neural networks, in: CVPR.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2016c] Wang, P., Li, W., Liu, S., Gao, Z., Tang, C., Ogunbona,
    P., 2016c. Large-scale isolated gesture recognition using convolutional neural
    networks, in: Pattern Recognition (ICPR), 2016 23rd International Conference on,
    IEEE. pp. 7–12.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2016d] Wang, P., Li, W., Liu, S., Zhang, Y., Gao, Z., Ogunbona,
    P., 2016d. Large-scale continuous gesture recognition using convolutional neural
    networks, in: 2016 23rd International Conference on Pattern Recognition (ICPR),
    pp. 13–18.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2018b] Wang, P., Li, W., Wan, J., Ogunbona, P., Liu, X., 2018b.
    Cooperative training of deep aggregation networks for rgb-d action recognition,
    in: AAAI.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2016e] Wang, P., Li, Z., Hou, Y., Li, W., 2016e. Action recognition
    based on joint trajectory maps using convolutional neural networks, in: ACM MM.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2017c] Wang, P., Wang, S., Gao, Z., Hou, Y., Li, W., 2017c. Structured
    images for RGB-D action recognition, in: Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 1005–1014.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2016f] Wang, X., Farhadi, A., Gupta, A., 2016f. Actions~ transformations,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2658–2667.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2016g] Wang, Y., Song, J., Wang, L., Van Gool, L., Hilliges, O.,
    2016g. Two-stream sr-cnns for action recognition in videos, BMVC.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weinzaepfel et al. [2015] Weinzaepfel, P., Harchaoui, Z., Schmid, C., 2015.
    Learning to track for spatio-temporal action localization, in: Proceedings of
    the IEEE International Conference on Computer Vision, pp. 3164–3172.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2016] Wu, D., Pigou, L., Kindermans, P.J., Le, N.D.H., Shao, L.,
    Dambre, J., Odobez, J.M., 2016. Deep dynamic neural networks for multimodal gesture
    segmentation and recognition. IEEE transactions on pattern analysis and machine
    intelligence 38, 1583–1597.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu and Shao [2014] Wu, D., Shao, L., 2014. Leveraging hierarchical parametric
    networks for skeletal joints based action segmentation and recognition, in: 2014
    IEEE Conference on Computer Vision and Pattern Recognition, pp. 724–731.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. [2012] Xia, L., Chen, C.C., Aggarwal, J., 2012. View invariant human
    action recognition using histograms of 3D joints, in: CVPRW.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xingjian et al. [2015] Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong,
    W.K., Woo, W.c., 2015. Convolutional lstm network: A machine learning approach
    for precipitation nowcasting, in: Advances in Neural Information Processing Systems,
    pp. 802–810.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. [2014] Yan, X., Chang, H., Shan, S., Chen, X., 2014. Modeling video
    dynamics with deep dynencoder, in: European Conference on Computer Vision, Springer.
    pp. 215–230.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2012] Yang, X., Zhang, C., Tian, Y., 2012. Recognizing actions
    using depth motion maps-based histograms of oriented gradients, in: ACM MM.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. [2013] Ye, M., Zhang, Q., Wang, L., Zhu, J., Yang, R., Gall, J.,
    2013. A survey on human motion analysis from depth data, in: Time-of-Flight and
    Depth Imaging. Sensors, Algorithms, and Applications, pp. 149–187.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yeung et al. [2017] Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori,
    G., Fei-Fei, L., 2017. Every moment counts: Dense detailed labeling of actions
    in complex videos. International Journal of Computer Vision .'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yeung et al. [2016] Yeung, S., Russakovsky, O., Mori, G., Fei-Fei, L., 2016.
    End-to-end learning of action detection from frame glimpses in videos, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2678–2687.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yun et al. [2012] Yun, K., Honorio, J., Chattopadhyay, D., Berg, T.L., Samaras,
    D., 2012. Two-person interaction detection using body-pose features and multiple
    instance learning, in: Computer Vision and Pattern Recognition Workshops (CVPRW),
    2012 IEEE Computer Society Conference on, IEEE. pp. 28–35.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2016a] Zhang, B., Wang, L., Wang, Z., Qiao, Y., Wang, H., 2016a.
    Real-time action recognition with enhanced motion vector cnns, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2718–2726.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2017a] Zhang, J., Li, W., Ogunbona, P., 2017a. Joint geometrical
    and statistical alignment for visual domain adaptation, in: CVPR.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2016b] Zhang, J., Li, W., Ogunbona, P.O., Wang, P., Tang, C.,
    2016b. RGB-D-based action recognition datasets: A survey. Pattern Recognition
    60, 86–105.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2017b] Zhang, P., Lan, C., Xing, J., Zeng, W., Xue, J., Zheng,
    N., 2017b. View adaptive recurrent neural networks for high performance human
    action recognition from skeleton data, in: ICCV.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2017c] Zhang, S., Liu, X., Xiao, J., 2017c. On geometric features
    for skeleton-based action recognition using multilayer lstm networks, in: WACV.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2017] Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Lin, D., Tang, X.,
    2017. Temporal action detection with structured segment networks, in: ICCV.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2016a] Zhu, F., Shao, L., Xie, J., Fang, Y., 2016a. From handcrafted
    to learned representations for human action recognition: a survey. Image and Vision
    Computing 55, 42–52.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2016b] Zhu, G., Zhang, L., Mei, L., Shao, J., Song, J., Shen, P.,
    2016b. Large-scale isolated gesture recognition using pyramidal 3D convolutional
    networks, in: Pattern Recognition (ICPR), 2016 23rd International Conference on,
    IEEE. pp. 19–24.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2017a] Zhu, G., Zhang, L., Shen, P., Song, J., 2017a. Multimodal
    gesture recognition using 3D convolution and convolutional lstm. IEEE Access .
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2017b] Zhu, H., Vial, R., Lu, S., 2017b. Tornado: A spatio-temporal
    convolutional regression network for video action proposal, in: Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5813–5821.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2016c] Zhu, W., Hu, J., Sun, G., Cao, X., Qiao, Y., 2016c. A key
    volume mining deep framework for action recognition, in: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 1991–1999.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2016d] Zhu, W., Lan, C., Xing, J., Zeng, W., Li, Y., Shen, L.,
    Xie, X., 2016d. Co-occurrence feature learning for skeleton based action recognition
    using regularized deep LSTM networks, in: AAAI.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zolfaghari et al. [2017] Zolfaghari, M., Oliveira, G.L., Sedaghat, N., Brox,
    T., 2017. Chained multi-stream networks exploiting pose, motion, and appearance
    for action classification and detection, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
