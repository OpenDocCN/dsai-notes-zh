- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:50:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:50:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.09006] Natural Image Reconstruction from fMRI using Deep Learning: A
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.09006] 使用深度学习进行fMRI自然图像重建：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.09006](https://ar5iv.labs.arxiv.org/html/2110.09006)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.09006](https://ar5iv.labs.arxiv.org/html/2110.09006)
- en: \correspondance\extraAuth
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \correspondance\extraAuth
- en: Zarina Rakhimberdina
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zarina Rakhimberdina
- en: zarina.rakhimberdina@net.c.titech.ac.jp
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: zarina.rakhimberdina@net.c.titech.ac.jp
- en: 'Natural Image Reconstruction from fMRI using Deep Learning: A Survey'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习进行fMRI自然图像重建：综述
- en: Zarina Rakhimberdina ^(1,3), Quentin Jodelet ^(1,3), Xin Liu ^(2,3,∗), Tsuyoshi
    Murata ^(1,3)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Zarina Rakhimberdina ^(1,3)、Quentin Jodelet ^(1,3)、Xin Liu ^(2,3,∗)、Tsuyoshi
    Murata ^(1,3)
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the advent of brain imaging techniques and machine learning tools, much
    effort has been devoted to building computational models to capture the encoding
    of visual information in the human brain. One of the most challenging brain decoding
    tasks is the accurate reconstruction of the perceived natural images from brain
    activities measured by functional magnetic resonance imaging (fMRI). In this work,
    we survey the most recent deep learning methods for natural image reconstruction
    from fMRI. We examine these methods in terms of architectural design, benchmark
    datasets, and evaluation metrics and present a fair performance evaluation across
    standardized evaluation metrics. Finally, we discuss the strengths and limitations
    of existing studies and present potential future directions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着脑成像技术和机器学习工具的出现，许多努力被投入到建立计算模型，以捕捉人脑中视觉信息的编码。最具挑战性的脑解码任务之一是从功能性磁共振成像（fMRI）测量的脑活动中准确重建感知的自然图像。在这项工作中，我们调查了用于从fMRI重建自然图像的最新深度学习方法。我们从架构设计、基准数据集和评估指标等方面审视这些方法，并在标准化评估指标下呈现了公平的性能评估。最后，我们讨论了现有研究的优缺点，并提出了未来可能的发展方向。
- en: \helveticabold
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \helveticabold
- en: '1 Keywords:'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 关键词：
- en: Natural Image Reconstruction, fMRI, Brain Decoding, Neural Decoding, Deep Learning
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自然图像重建、功能性磁共振成像（fMRI）、脑解码、神经解码、深度学习
- en: 2 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 引言
- en: 2.1 Visual decoding using fMRI
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 使用fMRI的视觉解码
- en: Many brain imaging studies focus on decoding how the human brain represents
    information about the outer world. Considering that, the majority of external
    sensory information is processed by the human visual system (Logothetis and Sheinberg,
    [1996](#bib.bib33)), a need for deeper understanding of visual information processing
    in the human brain encourages building complex computational models that can characterize
    the content of visual stimuli. This problem is referred to as human visual decoding
    of perceived images and has gained increasing attention.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 许多脑成像研究集中于解码人脑如何表示外部世界的信息。考虑到大多数外部感官信息是由人类视觉系统处理的（Logothetis和Sheinberg，[1996](#bib.bib33)），对人脑视觉信息处理的深入理解需要构建能够表征视觉刺激内容的复杂计算模型。这个问题被称为人类视觉解码感知图像，并且获得了越来越多的关注。
- en: A great advancement in recent neuroscience research has been achieved through
    functional magnetic resonance imaging (fMRI) (Poldrack and Farah, [2015](#bib.bib45);
    Nestor et al., [2020](#bib.bib42)). The fMRI technique captures neural activity
    in the brain by measuring variations in blood oxygen levels (Ogawa et al., [1990](#bib.bib43);
    Bandettini, [2012](#bib.bib1)). Among the various brain imaging techniques, fMRI
    is noninvasive and has a high spatial resolution. These characteristics allow
    fMRI to be used in a wide range of problems, including neurological disorder diagnosis
    (Zhang et al., [2020](#bib.bib71); Rakhimberdina et al., [2020](#bib.bib50)) and
    human visual decoding (Haxby et al., [2001](#bib.bib17); Kamitani and Tong, [2005](#bib.bib22);
    Horikawa and Kamitani, [2017](#bib.bib19)). The recent progress in human visual
    decoding has shown that beyond merely encoding the information about visual stimuli
    (Poldrack and Farah, [2015](#bib.bib45)), brain activity captured by fMRI can
    be used to reconstruct visual stimuli information (Roelfsema et al., [2018](#bib.bib52);
    Kay et al., [2008](#bib.bib24)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，通过功能性磁共振成像（fMRI）取得了重大进展（Poldrack and Farah, [2015](#bib.bib45); Nestor et
    al., [2020](#bib.bib42)）。fMRI 技术通过测量血氧水平的变化来捕捉大脑中的神经活动（Ogawa et al., [1990](#bib.bib43);
    Bandettini, [2012](#bib.bib1)）。在各种脑成像技术中，fMRI 是非侵入性的，具有较高的空间分辨率。这些特性使得 fMRI 可以应用于广泛的问题，包括神经系统疾病诊断（Zhang
    et al., [2020](#bib.bib71); Rakhimberdina et al., [2020](#bib.bib50)）和人类视觉解码（Haxby
    et al., [2001](#bib.bib17); Kamitani and Tong, [2005](#bib.bib22); Horikawa and
    Kamitani, [2017](#bib.bib19)）。人类视觉解码的最新进展表明，除了仅仅编码关于视觉刺激的信息（Poldrack and Farah,
    [2015](#bib.bib45)），fMRI 捕捉到的脑活动还可以用于重建视觉刺激信息（Roelfsema et al., [2018](#bib.bib52);
    Kay et al., [2008](#bib.bib24)）。
- en: 'Based on the target task, human visual decoding can be categorized into stimuli
    category classification, stimuli identification, and reconstruction (Naselaris
    et al., [2011](#bib.bib40)). In classification, brain activity is used to predict
    discrete object categories of the presented stimuli (Haxby et al., [2001](#bib.bib17);
    Horikawa and Kamitani, [2017](#bib.bib19)). The goal of identification is to identify
    a specific stimulus corresponding to the given pattern of brain activity from
    a known set of stimuli images (Kay et al., [2008](#bib.bib24); Naselaris et al.,
    [2011](#bib.bib40)). In both identification and reconstruction, we aim to recover
    image-specific details, such as object position, size, and angle. However, reconstruction
    is a more challenging task, in which a replica of the stimulus image needs to
    be generated for a given fMRI signal (see Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Visual
    decoding using fMRI ‣ 2 Introduction ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey")). Furthermore, stimulus-related information encoded
    in the fMRI activity, which allows high-accuracy identification, may only partially
    characterize stimuli images and thus be insufficient for proper image reconstruction
    (Kay et al., [2008](#bib.bib24); St-Yves and Naselaris, [2018](#bib.bib62)). With
    the development of sophisticated image reconstruction methods and the increasing
    amount of brain imaging data, more attention has been directed toward visual stimuli
    reconstruction from fMRI activity in the visual cortex (Miyawaki et al., [2008](#bib.bib38);
    Naselaris et al., [2009](#bib.bib41); van Gerven et al., [2010](#bib.bib65)).
    fMRI-based visual reconstruction can improve our understanding of the brain’s
    visual processing mechanisms, and researchers can incorporate this knowledge into
    the development of brain–computer interfaces.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据目标任务，人类视觉解码可以分为刺激类别分类、刺激识别和重建（Naselaris et al., [2011](#bib.bib40)）。在分类中，脑活动被用于预测呈现刺激的离散对象类别（Haxby
    et al., [2001](#bib.bib17); Horikawa and Kamitani, [2017](#bib.bib19)）。识别的目标是从已知的刺激图像集合中识别出与给定脑活动模式对应的特定刺激（Kay
    et al., [2008](#bib.bib24); Naselaris et al., [2011](#bib.bib40)）。在识别和重建中，我们的目标是恢复图像的具体细节，如对象位置、大小和角度。然而，重建是一个更具挑战性的任务，需要为给定的
    fMRI 信号生成刺激图像的副本（参见图 [1](#S2.F1 "图 1 ‣ 2.1 使用 fMRI 的视觉解码 ‣ 2 引言 ‣ 使用深度学习从 fMRI
    重建自然图像：综述")）。此外，fMRI 活动中编码的与刺激相关的信息虽然可以实现高准确度的识别，但可能仅部分描述了刺激图像，因此对于适当的图像重建可能不够充分（Kay
    et al., [2008](#bib.bib24); St-Yves and Naselaris, [2018](#bib.bib62)）。随着图像重建方法的精细化和脑成像数据量的增加，越来越多的关注被转向从视觉皮层的
    fMRI 活动中重建视觉刺激（Miyawaki et al., [2008](#bib.bib38); Naselaris et al., [2009](#bib.bib41);
    van Gerven et al., [2010](#bib.bib65)）。基于 fMRI 的视觉重建可以提高我们对大脑视觉处理机制的理解，研究人员可以将这些知识应用于脑机接口的发展。
- en: '![Refer to caption](img/a58f07c3380f288890e28ca0c5eea6ee.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/a58f07c3380f288890e28ca0c5eea6ee.png)'
- en: 'Figure 1: Framework diagram for natural image reconstruction task.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：自然图像重建任务的框架图。
- en: 2.2 Natural image reconstruction
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 自然图像重建
- en: The variety of visual stimuli used in visual reconstruction tasks can range
    from simple low-level detail images, such as Gabor wavelets and domino patterns
    (Thirion et al., [2006](#bib.bib64)), to more elaborate images depicting alphabetical
    characters, digits (van Gerven et al., [2010](#bib.bib65); Schoenmakers et al.,
    [2013](#bib.bib55)), natural objects, and scenes (Haxby et al., [2001](#bib.bib17);
    Horikawa and Kamitani, [2017](#bib.bib19)). The image reconstruction task for
    low-level detail stimuli does not require expressive models, and linear mapping
    is usually sufficient for learning effective reconstruction (Miyawaki et al.,
    [2008](#bib.bib38)). Among the variety of visual stimuli, natural images are considered
    the most challenging, as they require accurate reconstruction of color, shape,
    and higher-level perceptual features.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉重建任务中使用的视觉刺激种类繁多，从简单的低级细节图像，如 Gabor 小波和多米诺模式（Thirion 等, [2006](#bib.bib64)），到更复杂的图像，如字母字符、数字（van
    Gerven 等, [2010](#bib.bib65); Schoenmakers 等, [2013](#bib.bib55)）、自然对象和场景（Haxby
    等, [2001](#bib.bib17); Horikawa 和 Kamitani, [2017](#bib.bib19)）。低级细节刺激的图像重建任务不需要复杂的模型，线性映射通常足以学习有效的重建（Miyawaki
    等, [2008](#bib.bib38)）。在各种视觉刺激中，自然图像被认为是最具挑战性的，因为它们需要准确重建颜色、形状和更高级的感知特征。
- en: 'Similar to Shen et al. ([2019b](#bib.bib60)), we refer to the task of visual
    stimuli reconstruction from fMRI as natural image reconstruction, where stimuli
    are drawn from a database of natural images. The goal of neural decoding models
    is to learn a mapping function $f:\mathcal{V}\rightarrow\mathcal{X}$, where $\mathcal{X}$
    and $\mathcal{V}$ denote two sets corresponding to stimulus images and fMRI activity
    patterns extracted from the visual cortex. A framework diagram for visual reconstruction
    is shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Visual decoding using fMRI ‣ 2 Introduction
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于 Shen 等（[2019b](#bib.bib60)），我们将来自 fMRI 的视觉刺激重建任务称为自然图像重建，其中刺激来源于自然图像数据库。神经解码模型的目标是学习一个映射函数
    $f:\mathcal{V}\rightarrow\mathcal{X}$，其中 $\mathcal{X}$ 和 $\mathcal{V}$ 表示对应于刺激图像和从视觉皮层提取的
    fMRI 活动模式的两个集合。视觉重建的框架图如图 [1](#S2.F1 "Figure 1 ‣ 2.1 Visual decoding using fMRI
    ‣ 2 Introduction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") 所示。'
- en: The main challenges of natural image reconstruction include the following. First,
    the reconstruction quality must be good enough to capture the similarity between
    reconstructed and original images on multiple levels. In contrast to low-resolution
    image stimuli, such as shape or character patterns, good-quality reconstruction
    of natural images requires that both lower-level details and high-level semantic
    information be preserved. Second, brain’s visual representations are invariant
    to different objects or image details, which is essential for object recognition,
    but imply that brain activation patterns are not necessarily unique for a given
    stimulus object (St-Yves and Naselaris, [2018](#bib.bib62); Quiroga et al., [2005](#bib.bib48)).
    Finally, the lack of a standardized evaluation procedure for assessing the reconstruction
    quality makes it difficult to compare the existing methods. In this work, we will
    primarily focus on the solution to the third challenge.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自然图像重建的主要挑战包括以下几点。首先，重建质量必须足够好，以捕捉重建图像与原始图像在多个层次上的相似性。与低分辨率图像刺激，如形状或字符模式相比，自然图像的高质量重建要求同时保留低级细节和高级语义信息。其次，大脑的视觉表征对不同的对象或图像细节是不变的，这对物体识别至关重要，但这意味着大脑激活模式不一定对特定的刺激对象唯一（St-Yves
    和 Naselaris, [2018](#bib.bib62); Quiroga 等, [2005](#bib.bib48)）。最后，缺乏标准化的评估程序来评估重建质量，使得现有方法难以比较。在这项工作中，我们将主要关注第三个挑战的解决方案。
- en: Contributions. The topic of natural image reconstruction from fMRI is relatively
    new and has attracted much interest over the last few years. The related surveys
    on the field of natural encoding and decoding of visual input give a broad overview
    of the existing techniques to extract information from the brain (Roelfsema et al.,
    [2018](#bib.bib52); Nestor et al., [2020](#bib.bib42)) and focus on the traditional
    machine learning methods (Chen et al., [2014](#bib.bib4)). To our knowledge, there
    is no comprehensive survey on the topic of natural image reconstruction from fMRI
    using deep learning. Given the lack of a standardized evaluation process in terms
    of the benchmark dataset and standard metrics, our main contribution is to provide
    the research community with a fair performance comparison for existing methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献。基于 fMRI 的自然图像重建话题相对较新，并在过去几年引起了广泛关注。与自然视觉输入的编码和解码相关的调查提供了对现有技术的广泛概述（Roelfsema
    等，[2018](#bib.bib52); Nestor 等，[2020](#bib.bib42)），并专注于传统机器学习方法（Chen 等，[2014](#bib.bib4)）。据我们所知，尚无关于使用深度学习进行
    fMRI 自然图像重建的全面调查。鉴于在基准数据集和标准指标方面缺乏标准化的评估过程，我们的主要贡献是为研究社区提供对现有方法的公平性能比较。
- en: 'In this survey, we provide an overview of the deep learning-based natural image
    reconstruction methods. We discuss the differences in architecture, learning paradigms,
    and advantages of deep learning models over traditional methods. In addition,
    we review the evaluation metrics and compare models on the same benchmark: the
    same metrics and the same dataset parameters. The proposed standardised evaluation
    on a common set of metrics offers an opportunity to fairly evaluate and track
    new emerging methods in the field.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们提供了基于深度学习的自然图像重建方法的概述。我们讨论了架构、学习范式的差异以及深度学习模型相对于传统方法的优势。此外，我们回顾了评估指标，并在相同的基准上比较了模型：相同的指标和相同的数据集参数。提出的在一组通用指标上的标准化评估为公平评估和跟踪该领域新兴方法提供了机会。
- en: 'The rest of this paper is organized as follows. In Section [3](#S3 "3 Benchmark
    datasets ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey")
    and Section [4](#S4 "4 Deep learning-based approaches for natural image reconstruction
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey"), we introduce
    popular publicly available datasets for natural image reconstruction and review
    recent state-of-the-art deep learning models for natural image reconstruction,
    respectively. Then, we provide an overview of the evaluation metrics in Section
    [5](#S5 "5 Reconstruction evaluation ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey"), and presents a fair comparative evaluation of
    the methods in Section [6](#S6 "6 Fair comparison across the methods ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey"). Finally, we discuss
    the main challenges and possible future directions of this work. Section [8](#S8
    "8 Conclusion ‣ Natural Image Reconstruction from fMRI using Deep Learning: A
    Survey") concludes the paper.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。在第 [3](#S3 "3 基准数据集 ‣ 使用深度学习进行自然图像重建：综述") 节和第 [4](#S4 "4 基于深度学习的自然图像重建方法
    ‣ 使用深度学习进行自然图像重建：综述") 节中，我们分别介绍了用于自然图像重建的流行公开数据集，并回顾了最新的深度学习模型。然后，在第 [5](#S5 "5
    重建评估 ‣ 使用深度学习进行自然图像重建：综述") 节中，我们提供了评估指标的概述，并在第 [6](#S6 "6 方法的公平比较 ‣ 使用深度学习进行自然图像重建：综述")
    节中呈现了方法的公平比较评估。最后，我们讨论了该工作的主要挑战和可能的未来方向。第 [8](#S8 "8 结论 ‣ 使用深度学习进行自然图像重建：综述")
    节总结了本文。
- en: 'Table 1: Characteristics of benchmark datasets.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 基准数据集的特征。'
- en: '| Reference | Dataset |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 数据集 |'
- en: '&#124; Number of &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数量 &#124;'
- en: '&#124; Subjects &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 受试者 &#124;'
- en: '|'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image stimuli &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像刺激 &#124;'
- en: '&#124; Train/Test &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练/测试 &#124;'
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Repetition time &#124;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重复时间 &#124;'
- en: '&#124; Train/Test &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练/测试 &#124;'
- en: '| ROIs |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| ROIs |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| VanRullen and Reddy ([2019](#bib.bib66)) | Faces | 4 | 88/20 | n/a | n/a
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| VanRullen 和 Reddy ([2019](#bib.bib66)) | 面孔 | 4 | 88/20 | 无 | 无 |'
- en: '| Kay et al. ([2008](#bib.bib24)) | vim-1 | 2 | 1750/120 | 2/13 | V1, V2, V3,
    V4, LO |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Kay 等 ([2008](#bib.bib24)) | vim-1 | 2 | 1750/120 | 2/13 | V1, V2, V3, V4,
    LO |'
- en: '| Horikawa and Kamitani ([2017](#bib.bib19)) | Generic Object Decoding | 5
    | 1200/50 | 1/35 | V1, V2, V3, V4, LOC, FFA, PPA |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Horikawa 和 Kamitani ([2017](#bib.bib19)) | 泛型对象解码 | 5 | 1200/50 | 1/35 |
    V1, V2, V3, V4, LOC, FFA, PPA |'
- en: '| Shen et al. ([2019b](#bib.bib60)) | Deep Image Reconstruction | Natural Images
    | 3 | 1200/50 | 5/24 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Shen 等（[2019b](#bib.bib60)）| 深度图像重建 | 自然图像 | 3 | 1200/50 | 5/24 |'
- en: '| Artificial Shapes | 3 | 0/40 | 0/20 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 人工形状 | 3 | 0/40 | 0/20 |'
- en: '| Alphabetical Letters | 3 | 0/10 | 0/12 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 字母表 | 3 | 0/10 | 0/12 |'
- en: 3 Benchmark datasets
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个基准数据集
- en: 'This section summarizes the publicly available benchmark datasets used in deep
    learning-based natural image reconstruction from fMRI activity. While there exist
    a variety of datasets used for stimuli reconstruction, such as binary contrast
    patterns (BCP) (Miyawaki et al., [2008](#bib.bib38)), 69 dataset of handwritten
    digits (van Gerven et al., [2010](#bib.bib65)), BRAINS dataset of handwritten
    characters (Schoenmakers et al., [2013](#bib.bib55)), we focus on the datasets
    with higher level of perceptual complexity of presented stimuli: dataset of faces,
    grayscale natural images, and natural images from Imagenet. Each sample of these
    datasets represents a labeled pair – fMRI recording paired with the relevant stimuli
    image. Several distinctive characteristics of each dataset are presented in Table
    [1](#S2.T1 "Table 1 ‣ 2.2 Natural image reconstruction ‣ 2 Introduction ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '本节总结了基于深度学习的自然图像重建从 fMRI 活动中公开使用的基准数据集。虽然存在各种用于刺激重建的数据集，比如二进制对比模式（BCP）（Miyawaki
    et al.，[2008](#bib.bib38)），69 个手写数字数据集（van Gerven et al.，[2010](#bib.bib65)），手写字符
    BRAINS 数据集（Schoenmakers et al.，[2013](#bib.bib55)），但我们专注于呈现刺激更高层次感知复杂性的数据集：面部数据集，灰度自然图像和
    Imagenet 的自然图像。这些数据集的每个样本代表一个标记对 - fMRI 记录与相关刺激图像。每个数据集的若干独特特征见表[1](#S2.T1 "Table
    1 ‣ 2.2 Natural image reconstruction ‣ 2 Introduction ‣ Natural Image Reconstruction
    from fMRI using Deep Learning: A Survey") 中呈现。'
- en: 'Faces. VanRullen and Reddy ([2019](#bib.bib66)) used facial stimuli to reconstruct
    human faces from fMRI activity using deep neural networks¹¹1The fMRI dataset is
    available at [https://openneuro.org/datasets/ds001761](https://openneuro.org/datasets/ds001761)..
    The facial stimuli were drawn randomly from the CelebA dataset (Liu et al., [2015](#bib.bib32)),
    and four healthy subjects participated in the experiment. The samples of stimuli
    images are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3 Benchmark datasets ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey") A.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'VanRullen 和 Reddy（[2019](#bib.bib66)）利用面部刺激，利用深度神经网络从 fMRI 活动中重建人脸¹¹1 fMRI
    数据集可在[https://openneuro.org/datasets/ds001761](https://openneuro.org/datasets/ds001761)获得。面部刺激是从
    CelebA 数据集（Liu et al.，[2015](#bib.bib32)）随机抽取的，并有四名健康受试者参与了实验。刺激图像的示例显示在图[2](#S3.F2
    "Figure 2 ‣ 3 Benchmark datasets ‣ Natural Image Reconstruction from fMRI using
    Deep Learning: A Survey") A 中。'
- en: 'vim-1 dataset of grayscale natural images was acquired to study how natural
    images are represented by the human visual system²²2The dataset is available at
    [http://crcns.org/data-sets/vc/vim-1](http://crcns.org/data-sets/vc/vim-1). (Kay
    et al., [2008](#bib.bib24)). The stimuli comprise a set of 1870 grayscale 500
    $\times$ 500 pixels natural images of real-world objects, animals, and indoor
    and outdoor scenes (the samples are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3 Benchmark
    datasets ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey")
    B). Natural images were obtained from the Corel Stock Photo Libraries (Corel Corporation,
    [1994](#bib.bib7)), the Berkeley database of human segmented natural images³³3[https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/)
    (Martin et al., [2001](#bib.bib36)), and an image collection from the authors.
    Two healthy subjects with normal or corrected-to-normal vision were involved in
    the fMRI data acquisition.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'vim-1 数据集的灰度自然图像是为了研究人类视觉系统如何表示自然图像²²2 该数据集可在[http://crcns.org/data-sets/vc/vim-1](http://crcns.org/data-sets/vc/vim-1)获得（Kay
    et al.，[2008](#bib.bib24)）。刺激包括一组 1870 幅 500 $\times$ 500 像素的真实对象、动物和室内外场景的灰度自然图像（样本显示在图[2](#S3.F2
    "Figure 2 ‣ 3 Benchmark datasets ‣ Natural Image Reconstruction from fMRI using
    Deep Learning: A Survey") B 中）。自然图像来自 Corel Stock 照片库（Corel 公司，[1994](#bib.bib7)）、伯克利人类分割自然图像数据库³³3[https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/)（Martin
    et al.，[2001](#bib.bib36)）和作者的图像集。两名视觉正常或矫正到正常的健康受试者参与了 fMRI 数据采集。'
- en: '![Refer to caption](img/374b0b5f99a99e605b2677af267aa4a3.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/374b0b5f99a99e605b2677af267aa4a3.png)'
- en: 'Figure 2: Samples for natural stimuli: (A) images from Faces dataset (VanRullen
    and Reddy, [2019](#bib.bib66)); (B) grayscale natural images from vim-1 dataset
    (Kay et al., [2008](#bib.bib24)); (C) natural images from GOD (Horikawa and Kamitani,
    [2017](#bib.bib19)) and DIR (Shen et al., [2019b](#bib.bib60)) datasets.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：自然刺激样本：（A）来自 Faces 数据集的图像（VanRullen 和 Reddy，[2019](#bib.bib66)）；（B）来自 vim-1
    数据集的灰度自然图像（Kay 等，[2008](#bib.bib24)）；（C）来自 GOD（Horikawa 和 Kamitani，[2017](#bib.bib19)）和
    DIR（Shen 等，[2019b](#bib.bib60)）数据集的自然图像。
- en: 'Natural images from Imagenet. Two natural image datasets released by Kamitani
    Lab are widely used in image reconstruction. The first dataset, also known as
    the Generic Object Decoding⁴⁴4The dataset can be acquired from [http://brainliner.jp/data/brainliner/Generic_Object_Decoding](http://brainliner.jp/data/brainliner/Generic_Object_Decoding).
    dataset or GOD for short, was originally used by Horikawa and Kamitani ([2017](#bib.bib19))
    for the image classification task from the fMRI data and was later adopted for
    image reconstruction (Beliy et al., [2019](#bib.bib2); Ren et al., [2021](#bib.bib51)).
    The dataset consists of pairs of high-resolution 500 $\times$ 500 pixels stimuli
    images (see Figure [2](#S3.F2 "Figure 2 ‣ 3 Benchmark datasets ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey") C) and the corresponding
    fMRI recordings. fMRI scans were obtained from five healthy subjects; the stimuli
    images were selected from the ImageNet dataset (Deng et al., [2009](#bib.bib8))
    and span across 200 object categories.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '来自 Imagenet 的自然图像。由 Kamitani 实验室发布的两个自然图像数据集在图像重建中被广泛使用。第一个数据集，通常称为通用对象解码⁴⁴4
    该数据集可以从 [http://brainliner.jp/data/brainliner/Generic_Object_Decoding](http://brainliner.jp/data/brainliner/Generic_Object_Decoding)
    获取，简称 GOD，最初由 Horikawa 和 Kamitani ([2017](#bib.bib19)) 用于从 fMRI 数据中进行图像分类任务，后来被采用用于图像重建（Beliy
    等，[2019](#bib.bib2)；Ren 等，[2021](#bib.bib51)）。该数据集包含高分辨率 500 $\times$ 500 像素的刺激图像对（见图
    [2](#S3.F2 "Figure 2 ‣ 3 Benchmark datasets ‣ Natural Image Reconstruction from
    fMRI using Deep Learning: A Survey") C）及其对应的 fMRI 记录。fMRI 扫描来自五名健康受试者；刺激图像从 ImageNet
    数据集（Deng 等，[2009](#bib.bib8)）中选择，涵盖 200 个对象类别。'
- en: The second dataset based on the natural image dataset was acquired for the image
    reconstruction task (Shen et al., [2019b](#bib.bib60), [a](#bib.bib59)). It is
    publicly available at OpenNeuro⁵⁵5[https://openneuro.org/datasets/ds001506/versions/1.3.1](https://openneuro.org/datasets/ds001506/versions/1.3.1)
    , where it is cited as Deep Image Reconstruction. We refer to this dataset as
    Deep Image Reconstruction or DIR for short. The DIR dataset contains 1,250 stimuli
    images that are identical to the ones used in GOD. Because of different image
    presentation experiments, in which training and test image stimuli were repeated
    5 and 24 times respectively, the training set of the DIR dataset consists of a
    larger number of stimuli-fMRI pairs (5 $\times$ 1,200 samples) compared to the
    GOD. Three healthy subjects were involved in the image presentation. An appealing
    feature of this dataset is that, in addition to natural images, the dataset contains
    artificial shapes and alphabetical letters. The artificial shapes dataset consists
    of 40 images – a combination of eight colors and five geometric shapes. The alphabetical
    letters dataset consists of 10 letters (A, C, E, I, N, O, R, S, T, U) of consistent
    brightness and color.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个基于自然图像数据集的数据集用于图像重建任务（Shen 等，[2019b](#bib.bib60)，[a](#bib.bib59)）。它可以在 OpenNeuro⁵⁵5
    [https://openneuro.org/datasets/ds001506/versions/1.3.1](https://openneuro.org/datasets/ds001506/versions/1.3.1)
    上公开获取，在那里被引用为深度图像重建。我们称这个数据集为深度图像重建，简称 DIR。DIR 数据集包含 1,250 张与 GOD 中使用的刺激图像相同的图像。由于不同的图像呈现实验，其中训练和测试图像刺激分别重复了
    5 次和 24 次，因此 DIR 数据集的训练集包含更多的刺激-fMRI 对（5 $\times$ 1,200 样本），相比之下，GOD 数据集的数量较少。参与图像呈现的有三名健康受试者。这个数据集的一个吸引人的特点是，除了自然图像外，数据集还包含人工形状和字母。人工形状数据集包含
    40 张图像——由八种颜色和五种几何形状的组合组成。字母数据集包含 10 个字母（A、C、E、I、N、O、R、S、T、U），具有一致的亮度和颜色。
- en: 4 Deep learning-based approaches for natural image reconstruction
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于深度学习的自然图像重建方法
- en: Before deep learning, the traditional methods in natural image reconstruction
    estimated a linear mapping from fMRI signals to hand-crafted image features using
    linear regression models (Kay et al., [2008](#bib.bib24); Fujiwara et al., [2013](#bib.bib13);
    Naselaris et al., [2009](#bib.bib41)). These methods primarily focus on extracting
    predefined low-level features from stimulus images, such as local image structures
    or features of Gabor filters (Fang et al., [2020](#bib.bib12); Beliy et al., [2019](#bib.bib2)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习出现之前，传统的自然图像重建方法使用线性回归模型估计 fMRI 信号到手工制作图像特征的线性映射（Kay 等，[2008](#bib.bib24)；Fujiwara
    等，[2013](#bib.bib13)；Naselaris 等，[2009](#bib.bib41)）。这些方法主要关注从刺激图像中提取预定义的低级特征，如局部图像结构或
    Gabor 滤波器特征（Fang 等，[2020](#bib.bib12)；Beliy 等，[2019](#bib.bib2)）。
- en: In recent years, deep neural networks (DNNs) have significantly advanced computer
    vision research, replacing models based on hand-crafted features. In particular,
    DNN models have achieved better accuracy and improved image quality in various
    computer vision tasks, including image classification (Krizhevsky et al., [2012](#bib.bib28)),
    image segmentation (Chen et al., [2015](#bib.bib3)), and image restoration (Zhang
    et al., [2017](#bib.bib69)). In visual decoding tasks using brain imaging data,
    deep learning approaches have been applied to image classification (Haxby et al.,
    [2001](#bib.bib17); Horikawa and Kamitani, [2017](#bib.bib19)), object segmentation
    (Kamnitsas et al., [2017](#bib.bib23)), and natural image reconstruction (Shen
    et al., [2019b](#bib.bib60), [a](#bib.bib59)). They were shown to be more powerful
    than traditional methods (Zhang et al., [2020](#bib.bib71); Kriegeskorte, [2015](#bib.bib26))
    primarily due to the multilayer architecture allowing to learn nonlinear mappings
    from brain signals to stimulus images (Beliy et al., [2019](#bib.bib2); Shen et al.,
    [2019a](#bib.bib59)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，深度神经网络（DNNs）在计算机视觉研究中取得了显著进展，取代了基于手工特征的模型。特别是，DNN 模型在各种计算机视觉任务中取得了更好的准确性和图像质量改进，包括图像分类（Krizhevsky
    等，[2012](#bib.bib28)）、图像分割（Chen 等，[2015](#bib.bib3)）和图像恢复（Zhang 等，[2017](#bib.bib69)）。在使用脑成像数据的视觉解码任务中，深度学习方法已被应用于图像分类（Haxby
    等，[2001](#bib.bib17)；Horikawa 和 Kamitani，[2017](#bib.bib19)）、目标分割（Kamnitsas 等，[2017](#bib.bib23)）和自然图像重建（Shen
    等，[2019b](#bib.bib60)，[a](#bib.bib59)）。它们被证明比传统方法更强大（Zhang 等，[2020](#bib.bib71)；Kriegeskorte，[2015](#bib.bib26)），主要由于多层架构使其能够从脑信号到刺激图像学习非线性映射（Beliy
    等，[2019](#bib.bib2)；Shen 等，[2019a](#bib.bib59)）。
- en: Motivated by the success of deep learning in image generation, many recent studies
    have widely used DNN models in natural image reconstruction for several reasons.
    First, the deep learning framework conforms to some degree to the visual encoding–decoding
    process occurring in the hierarchical regions of the human visual system (Pinto
    et al., [2009](#bib.bib44); Krizhevsky et al., [2012](#bib.bib28); Schrimpf et al.,
    [2018](#bib.bib57)). Second, the application of deep generative models allows
    the synthesis of high-quality natural-looking images, which is achieved by learning
    the underlying data distribution (Goodfellow et al., [2014](#bib.bib15)). Additionally,
    the training process can be aided by models pretrained on larger image datasets
    (Shen et al., [2019b](#bib.bib60), [a](#bib.bib59)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 受到深度学习在图像生成中的成功启发，最近的许多研究广泛使用 DNN 模型进行自然图像重建，原因有几个。首先，深度学习框架在某种程度上符合人类视觉系统层次区域中发生的视觉编码–解码过程（Pinto
    等，[2009](#bib.bib44)；Krizhevsky 等，[2012](#bib.bib28)；Schrimpf 等，[2018](#bib.bib57)）。其次，深度生成模型的应用使得可以合成高质量的自然图像，这通过学习潜在的数据分布来实现（Goodfellow
    等，[2014](#bib.bib15)）。此外，训练过程可以通过在更大图像数据集上预训练的模型来辅助（Shen 等，[2019b](#bib.bib60)，[a](#bib.bib59)）。
- en: 'In this section, we present the evolution of the state-of-the-art deep learning-based
    methods for natural image reconstruction. We analyze them in terms of DNN architecture,
    use of pretraining, and the choice of the dataset. The most popular deep learning
    models used in natural image reconstruction tasks include non-generative methods
    such as convolutional neural networks, encoder–decoder-based frameworks (Kingma
    and Welling, [2014](#bib.bib25)); and generative methods, such as adversarial
    networks (Goodfellow et al., [2014](#bib.bib15)) and variational autoencoders
    (Larsen et al., [2016](#bib.bib29)). A comparison of the surveyed methods is presented
    in Table [2](#S4.T2 "Table 2 ‣ 4 Deep learning-based approaches for natural image
    reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning: A
    Survey").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了基于深度学习的自然图像重建方法的演变。我们从DNN架构、预训练的使用以及数据集的选择来分析它们。用于自然图像重建任务的最流行的深度学习模型包括非生成性方法，如卷积神经网络、基于编码器–解码器的框架（Kingma和Welling，[2014](#bib.bib25)）；以及生成性方法，如对抗网络（Goodfellow等，[2014](#bib.bib15)）和变分自编码器（Larsen等，[2016](#bib.bib29)）。表[2](#S4.T2
    "Table 2 ‣ 4 Deep learning-based approaches for natural image reconstruction ‣
    Natural Image Reconstruction from fMRI using Deep Learning: A Survey")展示了所调查方法的比较。'
- en: 'Table 2: Comparative table of the surveyed works. E2E represents end-to-end
    training. Loss denotes the loss function (MAE: mean absolute error; MSE: mean
    squared error; KL: KL divergence; Adv: adversarial loss; Cos: cosine similarity.
    The links to the source code are valid as of November, 2021.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：调查工作比较表。E2E代表端到端训练。损失表示损失函数（MAE：平均绝对误差；MSE：均方误差；KL：KL散度；Adv：对抗损失；Cos：余弦相似度。源代码的链接有效截至2021年11月。
- en: '| Method | Authors | Year | Datasets | Loss | E2E | Pre-training | Public code
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 作者 | 年份 | 数据集 | 损失 | E2E | 预训练 | 公开代码 |'
- en: '| SeeligerDCGAN | Seeliger et al. | 2018 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| SeeligerDCGAN | Seeliger等 | 2018 |'
- en: '&#124; BRAINS &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BRAINS &#124;'
- en: '&#124; vim-1 &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; vim-1 &#124;'
- en: '&#124; GOD &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GOD &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MAE &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE &#124;'
- en: '&#124; MSE &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE &#124;'
- en: '| no |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '&#124; Generator pre-trained on ImageNet &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在ImageNet上预训练的生成器 &#124;'
- en: '&#124; (Chrabaszcz et al., [2017](#bib.bib6)), Microsoft COCO &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Chrabaszcz等，[2017](#bib.bib6))，Microsoft COCO &#124;'
- en: '&#124; (Lin et al., [2014](#bib.bib31)), datasets from Maaten ([2009](#bib.bib34))
    &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Lin等，[2014](#bib.bib31))，来自Maaten的数据库 ([2009](#bib.bib34)) &#124;'
- en: '&#124; and Schomaker et al. ([2000](#bib.bib56)). AlexNet-based &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和Schomaker等（[2000](#bib.bib56)）。基于AlexNet &#124;'
- en: '&#124; Comparator trained on ImageNet. &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在ImageNet上训练的比较器。&#124;'
- en: '| no |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '| StYvesEBGAN | St-Yves and Naselaris | 2018 | vim-1 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| StYvesEBGAN | St-Yves和Naselaris | 2018 | vim-1 |'
- en: '&#124; MSE &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE &#124;'
- en: '&#124; Adv &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Adv &#124;'
- en: '| no |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '&#124; The denoiser and generator were pretrained &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 去噪器和生成器进行了预训练 &#124;'
- en: '&#124; on 32 $\times$ 32 color images from the CIFAR-10 &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自CIFAR-10的32 $\times$ 32彩色图像 &#124;'
- en: '&#124; dataset (Krizhevsky, [2009](#bib.bib27)) &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集（Krizhevsky，[2009](#bib.bib27)）&#124;'
- en: '| [yes](https://github.com/styvesg/gan-decoding-supplementary) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [是](https://github.com/styvesg/gan-decoding-supplementary) |'
- en: '| ShenDNN(+DGN) | Shen et al. | 2019 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ShenDNN(+DGN) | Shen等 | 2019 |'
- en: '&#124; DIR: &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DIR: &#124;'
- en: '&#124; Natural images, &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像，&#124;'
- en: '&#124; Artificial Shapes, &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人工形状，&#124;'
- en: '&#124; Alphabetical Letters &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 字母表字母 &#124;'
- en: '| MSE | no |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| MSE | 无 |'
- en: '&#124; VGG-19 pre-trained on ImageNet. &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预训练的VGG-19在ImageNet上。&#124;'
- en: '&#124; Pre-trained DGN (Dosovitskiy and Brox, [2016](#bib.bib10)). &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预训练的DGN（Dosovitskiy和Brox，[2016](#bib.bib10)）。&#124;'
- en: '| [yes](https://github.com/KamitaniLab/DeepImageReconstruction) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [是](https://github.com/KamitaniLab/DeepImageReconstruction) |'
- en: '| ShenGAN | Shen et al. | 2019 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ShenGAN | Shen等 | 2019 |'
- en: '&#124; DIR: &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DIR: &#124;'
- en: '&#124; Natural images, &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像，&#124;'
- en: '&#124; Artificial Shapes, &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人工形状，&#124;'
- en: '&#124; Alphabetical Letters &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 字母表字母 &#124;'
- en: '|'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MSE &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE &#124;'
- en: '&#124; Adv &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Adv &#124;'
- en: '| yes |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '&#124; Caffenet-based Comparator pre-trained &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于Caffenet的比较器预训练 &#124;'
- en: '&#124; on ImageNet &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在ImageNet上 &#124;'
- en: '| [yes](https://github.com/KamitaniLab/End2EndDeepImageReconstruction) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| [是](https://github.com/KamitaniLab/End2EndDeepImageReconstruction) |'
- en: '| BeliyEncDec | Beliy et al. | 2019 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| BeliyEncDec | Beliy等 | 2019 |'
- en: '&#124; GOD &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GOD &#124;'
- en: '&#124; vim-1 &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; vim-1 &#124;'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MSE &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE &#124;'
- en: '&#124; Cos &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Cos &#124;'
- en: '&#124; MAE &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE &#124;'
- en: '| no |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '&#124; Pretrained AlexNet-based encoder &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预训练的AlexNet基础编码器 &#124;'
- en: '| [yes](https://github.com/WeizmannVision/ssfmri2im) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| [是](https://github.com/WeizmannVision/ssfmri2im) |'
- en: '| VanRullenVAE-GAN | VanRullen and Reddy | 2019 | Faces |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| VanRullenVAE-GAN | VanRullen 和 Reddy | 2019 | 面孔 |'
- en: '&#124; MSE &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE &#124;'
- en: '&#124; Adv &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Adv &#124;'
- en: '| no | Pre-trained on CelebA dataset | [yes](https://github.com/rufinv/VAE-GAN-celebA)
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 在 CelebA 数据集上预训练 | [是](https://github.com/rufinv/VAE-GAN-celebA) |'
- en: '| GazivEncDec | Gaziv et al. | 2020 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| GazivEncDec | Gaziv 等 | 2020 |'
- en: '&#124; GOD &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GOD &#124;'
- en: '&#124; vim-1 &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; vim-1 &#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MSE &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE &#124;'
- en: '&#124; Cos &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Cos &#124;'
- en: '&#124; MAE &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE &#124;'
- en: '| no |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '&#124; Pretrained AlexNet-based encoder &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预训练的基于 AlexNet 的编码器 &#124;'
- en: '| no |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '| QiaoGAN-BVRM | Qiao et al. | 2020 | vim-1 | MSE | no |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| QiaoGAN-BVRM | Qiao 等 | 2020 | vim-1 | MSE | 无 |'
- en: '&#124; Generator of BigGAN pre-trained &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BigGAN 预训练生成器 &#124;'
- en: '&#124; on ImageNet &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在 ImageNet 上 &#124;'
- en: '| [partially](https://github.com/KaiQiao1992/ETECRM) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| [部分](https://github.com/KaiQiao1992/ETECRM) |'
- en: '| FangSSGAN | Fang et al. | 2020 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| FangSSGAN | Fang 等 | 2020 |'
- en: '&#124; DIR: &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DIR: &#124;'
- en: '&#124; Natural images &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然图像 &#124;'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MAE &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE &#124;'
- en: '&#124; Adv &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Adv &#124;'
- en: '| no | - | [partially](https://github.com/duolala1/Reconstructing-Perceptive-Images-from-Brain-Activity-by-Shape-Semantic-GAN)
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 无 | - | [部分](https://github.com/duolala1/Reconstructing-Perceptive-Images-from-Brain-Activity-by-Shape-Semantic-GAN)
    |'
- en: '| MozafariBigBiGAN | Mozafari et al. | 2020 | GOD | Adv | no | BigBiGAN pre-trained
    on ImageNet | no |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| MozafariBigBiGAN | Mozafari 等 | 2020 | GOD | Adv | 无 | 在 ImageNet 上预训练的 BigBiGAN
    | 无 |'
- en: '| RenD-VAE/GAN | Ren et al. | 2021 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| RenD-VAE/GAN | Ren 等 | 2021 |'
- en: '&#124; BCP &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BCP &#124;'
- en: '&#124; 6-9 &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 6-9 &#124;'
- en: '&#124; BRAINS &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BRAINS &#124;'
- en: '&#124; GOD &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GOD &#124;'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; KL &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; KL &#124;'
- en: '&#124; Adv &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Adv &#124;'
- en: '| no |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '&#124; Model pre-trained on external &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在外部数据上预训练的模型 &#124;'
- en: '&#124; data from ImageNet &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自 ImageNet 的数据 &#124;'
- en: '| no |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: 4.1 Non-generative methods
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 非生成方法
- en: Convolutional neural network (CNN). Compared to a simpler multilayer feed-forward
    neural network, which disregards the structural information of input images, the
    CNN has a better feature extraction capability because of the information filtering
    performed by convolutional layers within a neighborhood of pixels (LeCun et al.,
    [1989](#bib.bib30)). Stacking convolutional layers on top of each other allows
    learning hierarchical visual features of input images, known as feature abstraction.
    The lower CNN layers learn low-level details, whereas the higher CNN layers extract
    global high-level visual information from images (Mahendran and Vedaldi, [2015](#bib.bib35)).
    The use of CNNs is ubiquitous in image processing tasks, including image reconstruction.
    Specifically, encoder–decoder (Beliy et al., [2019](#bib.bib2); Gaziv et al.,
    [2020](#bib.bib14)), U-Net (Fang et al., [2020](#bib.bib12)), generative adversarial
    network (Goodfellow et al., [2014](#bib.bib15)), and variational autoencoder (Kingma
    and Welling, [2014](#bib.bib25)) are popular architectures that adopt stacked
    convolutional layers to extract features at multiple levels.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）。与忽视输入图像结构信息的简单多层前馈神经网络相比，CNN 由于在像素邻域内进行卷积层的信息过滤，具有更好的特征提取能力（LeCun
    等，[1989](#bib.bib30)）。将卷积层叠加在一起可以学习输入图像的层次化视觉特征，称为特征抽象。较低的 CNN 层学习低级细节，而较高的 CNN
    层从图像中提取全局高级视觉信息（Mahendran 和 Vedaldi，[2015](#bib.bib35)）。CNN 在图像处理任务中广泛使用，包括图像重建。具体而言，编码器–解码器（Beliy
    等，[2019](#bib.bib2)；Gaziv 等，[2020](#bib.bib14)）、U-Net（Fang 等，[2020](#bib.bib12)）、生成对抗网络（Goodfellow
    等，[2014](#bib.bib15)）和变分自编码器（Kingma 和 Welling，[2014](#bib.bib25)）是采用堆叠卷积层在多个层次提取特征的流行架构。
- en: 'Shen et al. (Shen et al., [2019b](#bib.bib60)) utilized a pretrained VGG-19-based
    DNN to extract hierarchical features from stimuli images (see Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Non-generative methods ‣ 4 Deep learning-based approaches for
    natural image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep
    Learning: A Survey") A). The DNN consists of sixteen convolutional layers followed
    by three fully connected layers. This method was motivated by the finding that
    hierarchical image representations obtained from different layers of deep neural
    network correlate with brain activity in the visual cortex (Eickenberg et al.,
    [2017](#bib.bib11); Horikawa and Kamitani, [2017](#bib.bib19)). Using this fact,
    one can establish a hierarchical mapping from fMRI signals in the low/high-level
    areas of visual cortices to the corresponding low/high-level features from the
    DNN. For this task, the authors implemented a feature decoder ${D}$ that maps
    fMRI activity patterns to multilayer DNN features. The decoder ${D}$ is trained
    on the train set before the reconstruction task, using the method from Horikawa
    and Kamitani ([2017](#bib.bib19)). These decoded fMRI features correspond to the
    hierarchical image features obtained from DNN. The optimization is performed on
    the feature space by minimizing the difference between the hierarchical DNN features
    of the image and multilayer features decoded from fMRI activity.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Shen 等（Shen et al., [2019b](#bib.bib60)）利用预训练的基于 VGG-19 的 DNN 从刺激图像中提取层次特征（见图
    [3](#S4.F3 "图 3 ‣ 4.1 非生成方法 ‣ 4 深度学习方法在自然图像重建中的应用 ‣ 使用深度学习的自然图像重建：综述") A）。该 DNN
    由十六层卷积层和三层全连接层组成。这个方法的动机是发现从深度神经网络的不同层次获得的层次图像表示与视觉皮层中的脑活动相关（Eickenberg et al.,
    [2017](#bib.bib11)；Horikawa 和 Kamitani, [2017](#bib.bib19)）。利用这一事实，可以建立从视觉皮层低/高层区域的
    fMRI 信号到 DNN 对应的低/高层特征的层次映射。为此任务，作者实现了一个特征解码器 ${D}$，它将 fMRI 活动模式映射到多层 DNN 特征。解码器
    ${D}$ 在重构任务之前在训练集上进行训练，使用 Horikawa 和 Kamitani 的方法（[2017](#bib.bib19)）。这些解码的 fMRI
    特征对应于从 DNN 获得的层次图像特征。优化在特征空间进行，通过最小化图像的层次 DNN 特征与从 fMRI 活动中解码的多层特征之间的差异。
- en: '![Refer to caption](img/06b89cf203bfdf15d49b74e4d74d6d36.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/06b89cf203bfdf15d49b74e4d74d6d36.png)'
- en: 'Figure 3: Overview of two variations of frameworks proposed by Shen et al.
    ([2019b](#bib.bib60)): (A) ShenDNN and (B) ShenDNN+DGN. The yellow color denotes
    the use of pretrained components.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Shen 等（[2019b](#bib.bib60)）提出的两种框架变体概述：（A）ShenDNN 和（B）ShenDNN+DGN。黄色表示使用了预训练组件。
- en: 'Deterministic encoder–decoder models. In deep learning, encoder–decoder models
    are widely used in image-to-image translation (Isola et al., [2017](#bib.bib20))
    and sequence-to-sequence models (Cho et al., [2014](#bib.bib5)). They learn the
    mapping from an input domain to an output domain via a two-stage architecture:
    an encoder ${E}$ that compresses the input vector $\mathbf{x}$ to the latent representation
    $\mathbf{z}={E}(\mathbf{x})$ and a decoder $\mathbf{y}={D}(\mathbf{z})$ that produces
    the output vector $\mathbf{y}$ from the latent representation $\mathbf{z}$ (Minaee
    et al., [2021](#bib.bib37)). The compressed latent representation vector $\mathbf{z}$
    serves as a bottleneck, which encodes a low-dimensional representation of the
    input. The model is trained to minimize the reconstruction error, which is the
    difference between the reconstructed output and ground-truth input.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性编码器-解码器模型。在深度学习中，编码器-解码器模型广泛应用于图像到图像的翻译（Isola et al., [2017](#bib.bib20)）和序列到序列的模型（Cho
    et al., [2014](#bib.bib5)）。它们通过两阶段的架构学习从输入领域到输出领域的映射：一个编码器 ${E}$ 将输入向量 $\mathbf{x}$
    压缩为潜在表示 $\mathbf{z}={E}(\mathbf{x})$，一个解码器 $\mathbf{y}={D}(\mathbf{z})$ 从潜在表示
    $\mathbf{z}$ 生成输出向量 $\mathbf{y}$（Minaee et al., [2021](#bib.bib37)）。压缩后的潜在表示向量
    $\mathbf{z}$ 作为瓶颈，编码了输入的低维表示。该模型通过最小化重构误差来训练，该误差是重构输出与真实输入之间的差异。
- en: 'Beliy et al. ([2019](#bib.bib2)) presented a CNN-based encoder–decoder model,
    where the encoder ${E}$ learns the mapping from stimulus images to the corresponding
    fMRI activity, and a decoder ${{D}}$ learns the mapping from fMRI activity to
    their corresponding images. The framework of this method, which we refer to as
    BeliyEncDec, is presented in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Non-generative
    methods ‣ 4 Deep learning-based approaches for natural image reconstruction ‣
    Natural Image Reconstruction from fMRI using Deep Learning: A Survey"). By stacking
    the encoder and decoder back-to-back, the authors introduced two combined networks
    ${E}$-${{D}}$ and ${{D}}$-${E}$, whose inputs and outputs are natural images and
    fMRI recordings, respectively. This allowed the training to be self-supervised
    on a larger dataset of unlabeled data. Specifically, 50,000 additional images
    from the ImageNet validation set and test fMRI recordings without stimulus pairs
    were used as unlabeled natural images and unlabeled fMRI samples. The authors
    demonstrated the advantage of their method by achieving competitive results on
    two natural image reconstruction datasets: Generic Object Decoding (Horikawa and
    Kamitani, [2017](#bib.bib19)) and vim-1 (Kay et al., [2008](#bib.bib24)). The
    training was conducted in two steps. In the first step, the encoder ${E}$ builds
    a mapping from stimulus images to fMRI activity. It utilizes the weights of the
    first convolutional layer of the pretrained AlexNet (Krizhevsky et al., [2012](#bib.bib28))
    and is trained in a supervised manner to predict fMRI activity for input images.
    In the second step, the trained encoder ${E}$ is fixed, and the decoder ${{D}}$
    is jointly trained using labeled and unlabeled data. The entire loss of the model
    consists of the fMRI loss of the encoder ${E}$ and the Image loss (RGB and features
    loss) of the decoder ${D}$.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Beliy 等人 ([2019](#bib.bib2)) 提出了一个基于 CNN 的编码器–解码器模型，其中编码器 ${E}$ 学习从刺激图像到相应 fMRI
    活动的映射，解码器 ${{D}}$ 学习从 fMRI 活动到其对应图像的映射。我们称之为 BeliyEncDec 的该方法框架如图 [4](#S4.F4 "图
    4 ‣ 4.1 非生成方法 ‣ 4 基于深度学习的自然图像重建 ‣ 使用深度学习的 fMRI 自然图像重建：综述") 所示。通过将编码器和解码器背靠背堆叠，作者引入了两个组合网络
    ${E}$-${{D}}$ 和 ${{D}}$-${E}$，其输入和输出分别为自然图像和 fMRI 记录。这使得训练可以在更大的未标记数据集上进行自监督。具体而言，使用来自
    ImageNet 验证集的 50,000 张额外图像和没有刺激对的测试 fMRI 记录作为未标记自然图像和未标记 fMRI 样本。作者通过在两个自然图像重建数据集：Generic
    Object Decoding (Horikawa 和 Kamitani, [2017](#bib.bib19)) 和 vim-1 (Kay 等人, [2008](#bib.bib24))
    上取得竞争性的结果，展示了其方法的优势。训练分为两步。在第一步中，编码器 ${E}$ 建立从刺激图像到 fMRI 活动的映射。它利用预训练 AlexNet
    (Krizhevsky 等人, [2012](#bib.bib28)) 的第一卷积层的权重，并以监督方式训练，以预测输入图像的 fMRI 活动。在第二步中，固定训练好的编码器
    ${E}$，并使用标记和未标记数据联合训练解码器 ${{D}}$。模型的整体损失包括编码器 ${E}$ 的 fMRI 损失和解码器 ${D}$ 的图像损失（RGB
    和特征损失）。
- en: '![Refer to caption](img/9df73875917e92e51fb5f5f22daf5fb7.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9df73875917e92e51fb5f5f22daf5fb7.png)'
- en: 'Figure 4: BeliyEncDec framework proposed by Beliy et al. ([2019](#bib.bib2)):
    (A) supervised training of the Encoder; (B) supervised and self-supervised training
    of the Decoder. The weights of the Encoder are fixed. The blue color denotes the
    components of the model trained on external unlabeled data. The image is adapted
    from Beliy et al. ([2019](#bib.bib2)).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Beliy 等人提出的 BeliyEncDec 框架 ([2019](#bib.bib2))：(A) 编码器的监督训练；(B) 解码器的监督和自监督训练。编码器的权重是固定的。蓝色表示在外部未标记数据上训练的模型组件。图像改编自
    Beliy 等人 ([2019](#bib.bib2))。
- en: In a follow-up study, Gaziv et al. ([2020](#bib.bib14)) improved the reconstruction
    accuracy of BeliyEncDec by introducing a loss function based on the perceptual
    similarity measure (Zhang et al., [2018](#bib.bib70)). To calculate perceptual
    similarity loss, the authors first extracted multilayer features from original
    and reconstructed images using VGG and then compared the extracted features layerwise.
    To distinguish it from BeliyEncDec, we refer to the framework proposed by Gaziv
    et al. ([2020](#bib.bib14)) as GazivEncDec.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续研究中，Gaziv 等人 ([2020](#bib.bib14)) 通过引入基于感知相似性度量的损失函数 (Zhang 等人, [2018](#bib.bib70))
    改进了 BeliyEncDec 的重建精度。为了计算感知相似性损失，作者首先使用 VGG 从原始图像和重建图像中提取多层特征，然后逐层比较提取的特征。为了与
    BeliyEncDec 区分开来，我们将 Gaziv 等人 ([2020](#bib.bib14)) 提出的框架称为 GazivEncDec。
- en: 4.2 Generative methods
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 生成方法
- en: Generative models assume that the data is generated from some probability distribution
    $p(\mathbf{x})$ and can be classified as implicit and explicit. Implicit models
    do not define the distribution of the data but instead specify a random sampling
    process with which to draw samples from $p(\mathbf{x})$. Explicit models, on the
    other hand, explicitly define the probability density function, which is used
    to train the model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型假设数据是从某个概率分布 $p(\mathbf{x})$ 生成的，并且可以分为隐式和显式两类。隐式模型不定义数据的分布，而是指定一个随机抽样过程以从
    $p(\mathbf{x})$ 中抽取样本。显式模型则明确定义概率密度函数，用于训练模型。
- en: Generative Adversarial Network (GAN).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）。
- en: A class of implicitly defined generative models called Generative adversarial
    networks (GANs) received much attention due to their ability to produce realistic
    images (Goodfellow et al., [2014](#bib.bib15)). In natural image reconstruction,
    GANs are widely used to learn the distribution of stimulus images. A GAN contains
    generator and discriminator networks. In the image generation task, the generator
    ${G}$ takes a random noise vector $\mathbf{z}$ (generally sampled from a Gaussian
    distribution) and generates a fake sample $G(\mathbf{z})$ with the same statistics
    as the training set images. During training, the generator’s ability to generate
    realistic images continually improves until the discriminator is unable to distinguish
    the difference between a real sample and a generated fake one. GAN-based frameworks
    have several desirable properties compared to other generative methods. First,
    GANs do not require strong assumptions regarding the form of the output probability
    distribution. Second, adversarial training, which uses the discriminator, allows
    unsupervised training of the GAN (St-Yves and Naselaris, [2018](#bib.bib62)).
    An illustration of GAN and details on GAN’s loss function are provided in Supplementary
    Material.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一类隐式定义的生成模型，称为生成对抗网络（GANs），因其能够生成逼真的图像而受到广泛关注（Goodfellow 等， [2014](#bib.bib15)）。在自然图像重建中，GANs
    被广泛应用于学习刺激图像的分布。GAN 包含生成器和判别器网络。在图像生成任务中，生成器 ${G}$ 接受一个随机噪声向量 $\mathbf{z}$（通常从高斯分布中抽样），并生成一个与训练集图像具有相同统计特性的伪样本
    $G(\mathbf{z})$。在训练过程中，生成器生成逼真图像的能力不断提升，直到判别器无法区分真实样本和生成的伪样本。与其他生成方法相比，基于 GAN
    的框架具有几个理想的特性。首先，GAN 不需要对输出概率分布的形式做出强假设。其次，利用判别器的对抗训练允许 GAN 的无监督训练（St-Yves 和 Naselaris，[2018](#bib.bib62)）。GAN
    的示意图和 GAN 损失函数的详细信息请参见补充材料。
- en: 'To ensure that reconstructions resemble natural images Shen et al. ([2019b](#bib.bib60))
    further modified their ShenDNN method by introducing a deep generator network
    (DGN) (Dosovitskiy and Brox, [2016](#bib.bib10)). The framework is shown in Figure
    [3](#S4.F3 "Figure 3 ‣ 4.1 Non-generative methods ‣ 4 Deep learning-based approaches
    for natural image reconstruction ‣ Natural Image Reconstruction from fMRI using
    Deep Learning: A Survey") B. A DGN, pretrained on natural images using the GAN
    training process, is integrated with the DNN to produce realistic images, and
    the optimization is performed on the input space of the DGN. Thus, the reconstructed
    images are constrained to be in the subspace of the images generated by the DGN.
    We refer to these framework variations without and with DGN as ShenDNN and ShenDNN+DGN
    in future references.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '为确保重建结果类似于自然图像，Shen 等人（[2019b](#bib.bib60)）通过引入深度生成网络（DGN）（Dosovitskiy 和 Brox，[2016](#bib.bib10)）进一步修改了他们的
    ShenDNN 方法。该框架如图 [3](#S4.F3 "Figure 3 ‣ 4.1 Non-generative methods ‣ 4 Deep learning-based
    approaches for natural image reconstruction ‣ Natural Image Reconstruction from
    fMRI using Deep Learning: A Survey") B 所示。一个使用 GAN 训练过程预训练的 DGN 被集成到 DNN 中，以生成逼真的图像，优化过程在
    DGN 的输入空间中进行。因此，重建的图像被限制在 DGN 生成的图像的子空间中。我们在后续引用中将这些框架变体称为 ShenDNN 和 ShenDNN+DGN。'
- en: 'Similar to Shen et al. ([2019b](#bib.bib60)), Fang et al. ([2020](#bib.bib12))
    based their work on the finding that visual features are hierarchically represented
    in the visual cortex. In the feature extraction step, the authors proposed two
    decoders, which extract shape and semantic representations from the lower and
    higher areas of visual cortex. The shape decoder ${D}_{sp}$ is a linear model,
    and the semantic decoder ${D}_{sm}$ has a DNN-based architecture (Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") A). In the image reconstruction step, the generator network ${{G}}$
    was trained with GAN using the extracted shape and semantic features as conditions
    for generating the images. We refer to this model as FangSSGAN, where SSGAN stands
    for the shape and semantic GAN. The generator ${{G}}$ is a CNN-based network with
    an encoder–decoder structure (Ronneberger et al., [2015](#bib.bib54)). To enhance
    reconstruction quality, approximately 1,200 additional images, different from
    those in the training/test set, were sampled from the ImageNet dataset to generate
    augmented data. These new images were used to generate shapes and category-average
    semantic features that were further passed into the GAN.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于 Shen 等人 ([2019b](#bib.bib60))，Fang 等人 ([2020](#bib.bib12)) 的工作基于这样的发现：视觉特征在视觉皮层中是分层表示的。在特征提取步骤中，作者提出了两个解码器，它们从视觉皮层的低级和高级区域提取形状和语义表示。形状解码器
    ${D}_{sp}$ 是一个线性模型，而语义解码器 ${D}_{sm}$ 具有基于 DNN 的架构（图 [5](#S4.F5 "Figure 5 ‣ 4.2
    Generative methods ‣ 4 Deep learning-based approaches for natural image reconstruction
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey") A）。在图像重建步骤中，生成器网络
    ${{G}}$ 使用 GAN 进行训练，以提取的形状和语义特征作为生成图像的条件。我们将此模型称为 FangSSGAN，其中 SSGAN 代表形状和语义 GAN。生成器
    ${{G}}$ 是一个基于 CNN 的网络，具有编码器–解码器结构（Ronneberger 等人，[2015](#bib.bib54)）。为了提高重建质量，从
    ImageNet 数据集中抽取了大约 1,200 张与训练/测试集不同的额外图像，以生成增强数据。这些新图像用于生成形状和类别平均的语义特征，然后进一步传递给
    GAN。'
- en: '![Refer to caption](img/a91c0bc9dfac8fe28d3a0106e56739cb.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a91c0bc9dfac8fe28d3a0106e56739cb.png)'
- en: 'Figure 5: GAN-based frameworks. (A) FangSSGAN framework utilised a semantic
    decoder $D_{sm}$ and a shape decoder $D_{sp}$. (B) ShenGAN framework introduced
    a comparator network $C$. (C) StYvesEBGAN framework consists of three components
    trained independently: an encoding model $E_{V}$, denoising autoencoder and $E_{C}$–$D_{C}$
    and a conditional GAN. (D) SeeligerDCGAN framework based on deep convolutional
    GAN. (E) MozafariBigBiGAN framework proposed Mozafari et al. ([2020](#bib.bib39)).
    (F) QiaoGAN-BVRM framework consists of four parts: a classifier, pretrained conditional
    generator, encoder, and evaluator network. For (A)–(F), the pretrained components
    of the framework are highlighted in yellow. The blue color of the components indicates
    that they were trained using additional data.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于 GAN 的框架。（A）FangSSGAN 框架利用了语义解码器 $D_{sm}$ 和形状解码器 $D_{sp}$。 （B）ShenGAN
    框架引入了一个比较网络 $C$。 （C）StYvesEBGAN 框架由三个独立训练的组件组成：编码模型 $E_{V}$，去噪自编码器和 $E_{C}$–$D_{C}$
    以及条件 GAN。 （D）SeeligerDCGAN 框架基于深度卷积 GAN。 （E）MozafariBigBiGAN 框架由 Mozafari 等人 ([2020](#bib.bib39))
    提出。 （F）QiaoGAN-BVRM 框架由四个部分组成：分类器、预训练条件生成器、编码器和评估网络。对于（A）–（F），框架中的预训练组件以黄色突出显示。组件的蓝色表示它们使用额外数据进行训练。
- en: 'Another GAN-based model was proposed by Shen et al. ([2019a](#bib.bib59)).
    The end-to-end model directly learns the mapping from fMRI signals to reconstructed
    images without intermediate transformation or feature extraction (see Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") B). The framework, which we refer to as ShenGAN, was trained using
    three convolutional neural networks: a generator $G$, a comparator $C$, and a
    discriminator $D$. The generator $G$ maps the fMRI data vector $\mathbf{v}$ to
    $G(\mathbf{v})$, and a discriminator $D$ distinguishes between reconstruction
    $G(\mathbf{v})$ and the original image $\mathbf{x}$. A comparator network $C$
    is pretrained on ImageNet (on image classification task) and used to compare the
    reconstruction $G(\mathbf{v})$ with the original image $\mathbf{x}$ by calculating
    the perceptual loss (similarity in feature space). The combined loss function
    is a weighted sum of three terms: loss in image space, perceptual loss and adversarial
    loss.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个基于 GAN 的模型由 Shen 等人 ([2019a](#bib.bib59)) 提出。该端到端模型直接学习从 fMRI 信号到重建图像的映射，无需中间转换或特征提取（见图
    [5](#S4.F5 "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches
    for natural image reconstruction ‣ Natural Image Reconstruction from fMRI using
    Deep Learning: A Survey") B）。该框架，我们称之为 ShenGAN，使用三个卷积神经网络进行训练：生成器 $G$、比较器 $C$
    和判别器 $D$。生成器 $G$ 将 fMRI 数据向量 $\mathbf{v}$ 映射到 $G(\mathbf{v})$，判别器 $D$ 区分重建 $G(\mathbf{v})$
    和原始图像 $\mathbf{x}$。比较器网络 $C$ 在 ImageNet 上进行预训练（用于图像分类任务），并通过计算感知损失（特征空间的相似性）来比较重建
    $G(\mathbf{v})$ 与原始图像 $\mathbf{x}$。组合的损失函数是三个项的加权和：图像空间损失、感知损失和对抗损失。'
- en: 'The GAN-based methods described so far enhanced the quality of reconstruction
    by generating more natural-looking images. However, although GANs can generate
    new plausible samples matching the distribution of samples in the training dataset,
    they do not allow to control any characteristics of the generated data (Jakub
    Langr and Vladimir Bok, [2019](#bib.bib21)). To solve this issue, St-Yves and
    Naselaris ([2018](#bib.bib62)) implemented the conditional generation of images
    using a variation of GAN called the energy-based condition GAN or EBGAN (Zhao
    et al., [2017](#bib.bib72)). In their framework, which we refer to as StYvesEBGAN,
    the authors first implement the encoding model ${E}_{V}$ to learn the mapping
    from stimulus to fMRI, as shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Generative
    methods ‣ 4 Deep learning-based approaches for natural image reconstruction ‣
    Natural Image Reconstruction from fMRI using Deep Learning: A Survey") C. In addition,
    StYvesEBGAN utilizes a denoising autoencoder to compress noisy high-dimensional
    fMRI representations into lower-dimensional representations. These lower-dimensional
    fMRI representations are further used as a condition vector for the GAN to reconstruct
    the stimuli. EBGAN is a more stable framework in terms of training than regular
    GANs. Instead of a binary classifier, it uses a deep autoencoder network as a
    discriminator. The authors observed that the reconstruction quality is highly
    dependent on the voxel denoising autoencoder, which produces a conditioning vector
    that results in the best reconstruction accuracy.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '到目前为止描述的基于 GAN 的方法通过生成更自然的图像来增强重建质量。然而，尽管 GAN 可以生成与训练数据集中的样本分布匹配的新样本，但它们无法控制生成数据的任何特征（Jakub
    Langr 和 Vladimir Bok，[2019](#bib.bib21)）。为了解决这个问题，St-Yves 和 Naselaris ([2018](#bib.bib62))
    实现了一种称为基于能量的条件 GAN 或 EBGAN 的 GAN 变体来进行图像的条件生成（Zhao 等，[2017](#bib.bib72)）。在他们的框架中，我们称之为
    StYvesEBGAN，作者首先实现了编码模型 ${E}_{V}$ 来学习从刺激到 fMRI 的映射，如图 [5](#S4.F5 "Figure 5 ‣ 4.2
    Generative methods ‣ 4 Deep learning-based approaches for natural image reconstruction
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey") C 所示。此外，StYvesEBGAN
    使用去噪自编码器将噪声高维 fMRI 表示压缩成低维表示。这些低维 fMRI 表示进一步用作 GAN 重建刺激的条件向量。与常规 GAN 相比，EBGAN
    在训练过程中是一个更稳定的框架。它使用深度自编码器网络作为判别器，而不是二分类器。作者观察到，重建质量高度依赖于体素去噪自编码器，该自编码器生成的条件向量能实现最佳的重建准确性。'
- en: A group of studies by Seeliger et al. ([2018](#bib.bib58)), Mozafari et al.
    ([2020](#bib.bib39)), and Qiao et al. ([2020](#bib.bib46)) utilized GAN architecture
    with the assumption that there is a linear relationship between brain activity
    and the latent features of GAN. Similar to ShenDNN+DGN, these methods adopted
    the generator of a pretrained GAN as a natural image prior, which ensures that
    the reconstructed images follow similar distributions as natural images.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Seeliger等人（[2018](#bib.bib58)）、Mozafari等人（[2020](#bib.bib39)）和Qiao等人（[2020](#bib.bib46)）的一组研究利用了GAN架构，假设大脑活动与GAN的潜在特征之间存在线性关系。类似于ShenDNN+DGN，这些方法采用了预训练GAN的生成器作为自然图像先验，这确保了重建的图像遵循类似自然图像的分布。
- en: 'Seeliger et al. ([2018](#bib.bib58)) used a deep convolutional GAN (DCGAN)
    architecture (Radford et al., [2016](#bib.bib49)), which introduced improvements
    by stacking convolutional and deconvolutional layers. The authors learn the direct
    linear mapping from the fMRI space to the latent space of GAN (see Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") D). For the natural stimuli image domain, the generator ${{G}}$ was
    pretrained on down-sampled 64 $\times$ 64 converted-to-grayscale images from ImageNet
    (Chrabaszcz et al., [2017](#bib.bib6)) and Microsoft COCO (Lin et al., [2014](#bib.bib31))
    datasets. For the handwritten character stimulus domain, DCGAN was pretrained
    on 15,000 handwritten characters from (Maaten, [2009](#bib.bib34)) and (Schomaker
    et al., [2000](#bib.bib56)). Also, a pretrained comparator network ${{C}}$, based
    on AlexNet, was introduced as a feature-matching network to compute the feature
    loss $L_{feat}$ across different layers. Overall, the loss is computed as a weighted
    sum of the pixelwise image loss $L_{img}$ (MAE) and feature loss $L_{feat}$. We
    refer to this framework as SeeligerDCGAN.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'Seeliger等人（[2018](#bib.bib58)）使用了深度卷积GAN（DCGAN）架构（Radford等人，[2016](#bib.bib49)），通过堆叠卷积层和反卷积层进行了改进。作者从fMRI空间到GAN潜在空间的直接线性映射（见图[5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") D）。对于自然刺激图像领域，生成器${{G}}$在ImageNet（Chrabaszcz等人，[2017](#bib.bib6)）和Microsoft
    COCO（Lin等人，[2014](#bib.bib31)）数据集中预训练了下采样64 $\times$ 64的灰度图像。对于手写字符刺激领域，DCGAN在（Maaten，[2009](#bib.bib34)）和（Schomaker等人，[2000](#bib.bib56)）的15,000个手写字符上进行了预训练。此外，基于AlexNet的预训练比较网络${{C}}$作为特征匹配网络被引入，用于计算不同层之间的特征损失$L_{feat}$。总体上，损失被计算为像素级图像损失$L_{img}$（MAE）和特征损失$L_{feat}$的加权和。我们将此框架称为SeeligerDCGAN。'
- en: 'Mozafari et al. ([2020](#bib.bib39)) used a variation of GAN, called the BigBiGAN
    model (Donahue and Simonyan, [2019](#bib.bib9)), which allowed the reconstruction
    of even more realistic images. The model generates high-level semantic information
    due to the BigBiGAN’s latent space, which extracts high-level image details from
    fMRI data. We refer to this framework as MozafariBigBiGAN. The framework utilizes
    a pretrained encoder ${E}$ that generates a latent space vector $E(\mathbf{x})$
    from the input image $\mathbf{x}$ and generator ${{G}}$ that generates an image
    $G(\mathbf{z})$ from the latent space vector $\mathbf{z}$ (see Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") E). During training, the authors computed the linear mapping $W$ from
    latent vectors $E(\mathbf{x})$ to fMRI activity using a general linear regression
    model. During the test stage, the linear mapping is inverted to compute the latent
    vectors $\mathbf{z}$ from the test fMRI activity.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'Mozafari等人（[2020](#bib.bib39)）使用了一种GAN的变体，称为BigBiGAN模型（Donahue和Simonyan，[2019](#bib.bib9)），该模型可以重建更逼真的图像。由于BigBiGAN的潜在空间，该模型生成高水平的语义信息，从fMRI数据中提取高级图像细节。我们将此框架称为MozafariBigBiGAN。该框架利用了一个预训练编码器${E}$，它从输入图像$\mathbf{x}$生成潜在空间向量$E(\mathbf{x})$，以及一个生成器${{G}}$，它从潜在空间向量$\mathbf{z}$生成图像$G(\mathbf{z})$（见图[5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") E）。在训练过程中，作者使用一般线性回归模型计算潜在向量$E(\mathbf{x})$到fMRI活动的线性映射$W$。在测试阶段，线性映射被反转，以根据测试fMRI活动计算潜在向量$\mathbf{z}$。'
- en: 'The GAN-based Bayesian visual reconstruction model (GAN-BVRM) proposed by Qiao
    et al. ([2020](#bib.bib46)) aims to improve the quality of reconstructions from
    a limited dataset combination and, as the name suggests, uses the combination
    of GAN and Bayesian learning. From Bayesian perspective, a conditional distribution
    $p(\mathbf{v}|\mathbf{x})$ corresponds to an encoder which predicts fMRI activity
    $\mathbf{v}$ from the stimuli image $\mathbf{x}$. On the other hand, an inverse
    conditional distribution $p(\mathbf{x}|\mathbf{v})$ corresponds to a decoder that
    reconstructs the stimuli from the fMRI activity. The goal of image reconstruction
    is to find the image that has the highest posterior probability $p(\mathbf{x}|\mathbf{v})$,
    given the fMRI activity. However, since the posterior distribution is hard to
    compute, Bayesian theorem is used to combine encoding model $p(\mathbf{v}|\mathbf{x})$
    and image prior $p(\mathbf{x})$ through $p(\mathbf{x}|\mathbf{v})\propto p(\mathbf{x})p(\mathbf{v}|\mathbf{x})$.
    The prior distribution $p(\mathbf{x})$ reflects the predefined knowledge about
    natural images and is independent of the fMRI activity. The QiaoGAN-BVRM framework
    is shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based
    approaches for natural image reconstruction ‣ Natural Image Reconstruction from
    fMRI using Deep Learning: A Survey") F, and it consists of four parts: a classifier
    network, pretrained conditional generator ${{G}}$, encoder ${E}$, and evaluator
    network. First, a classifier decodes object categories from fMRI data, and then
    a conditional generator ${{G}}$ of the BigGAN uses the decoded categories to generate
    natural images. The advantage of the pretrained generator is that it has already
    learned the data distribution from more than one million ImageNet natural images.
    Therefore, instead of searching the images one by one in a fixed image dataset
    (Naselaris et al., [2009](#bib.bib41)), the generator can produce the optimal
    image reconstructions that best match with the fMRI activity via backpropagation.
    The generated images are passed to the encoder ${E}$, which predicts the corresponding
    fMRI activity. The proposed visual encoding model and the pre-trained generator
    of BigGAN do not interfere with each other, which helps to improve the fidelity
    and naturalness of reconstruction. The reconstruction accuracy is measured using
    an evaluator network, which computes the negative average mean squared error (MSE)
    between the predicted and actual fMRI activity. The reconstructions are obtained
    by iteratively updating the input noise vector to maximize the evaluator’s score.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '由 Qiao 等人提出的基于 GAN 的贝叶斯视觉重建模型（GAN-BVRM）（[2020](#bib.bib46)）旨在提高从有限数据集组合中进行重建的质量，顾名思义，它结合了
    GAN 和贝叶斯学习。从贝叶斯的角度看，条件分布 $p(\mathbf{v}|\mathbf{x})$ 对应于一个编码器，它从刺激图像 $\mathbf{x}$
    预测 fMRI 活动 $\mathbf{v}$。另一方面，逆条件分布 $p(\mathbf{x}|\mathbf{v})$ 对应于一个解码器，它根据 fMRI
    活动重建刺激图像。图像重建的目标是找到在给定 fMRI 活动的情况下具有最高后验概率 $p(\mathbf{x}|\mathbf{v})$ 的图像。然而，由于后验分布很难计算，因此使用贝叶斯定理通过
    $p(\mathbf{x}|\mathbf{v})\propto p(\mathbf{x})p(\mathbf{v}|\mathbf{x})$ 结合编码模型
    $p(\mathbf{v}|\mathbf{x})$ 和图像先验 $p(\mathbf{x})$。先验分布 $p(\mathbf{x})$ 反映了对自然图像的预定义知识，并且与
    fMRI 活动无关。QiaoGAN-BVRM 框架如图 [5](#S4.F5 "Figure 5 ‣ 4.2 Generative methods ‣ 4
    Deep learning-based approaches for natural image reconstruction ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey") F 所示，包含四个部分：分类网络、预训练的条件生成器
    ${{G}}$、编码器 ${E}$ 和评估网络。首先，分类器从 fMRI 数据中解码物体类别，然后 BigGAN 的条件生成器 ${{G}}$ 使用解码的类别生成自然图像。预训练生成器的优势在于，它已经从超过一百万张
    ImageNet 自然图像中学习了数据分布。因此，生成器可以通过反向传播生成与 fMRI 活动最佳匹配的图像重建，而不是在固定的图像数据集中逐一搜索图像（Naselaris
    等人，[2009](#bib.bib41)）。生成的图像传递给编码器 ${E}$，编码器预测相应的 fMRI 活动。提出的视觉编码模型和 BigGAN 的预训练生成器相互独立，这有助于提高重建的保真度和自然性。重建准确性使用评估网络进行测量，该网络计算预测的
    fMRI 活动和实际 fMRI 活动之间的负平均均方误差（MSE）。通过迭代更新输入噪声向量以最大化评估器的得分，从而获得重建结果。'
- en: VAE-GAN. The variational autoencoder (VAE) proposed by Kingma and Welling ([2014](#bib.bib25))
    is an example of an explicit generative network and is a popular generative algorithm
    used in neural decoding. Similar to autoencoders, the VAE is composed of an encoder
    and a decoder. But rather than encoding a latent vector, VAE encodes a distribution
    over the latent space, making the generative process possible. Thus, the goal
    of VAE is to find a distribution of the latent variable ${\mathbf{z}}$, which
    we can sample from $\mathbf{z}\sim q_{\phi}(\mathbf{z}|\mathbf{x})$ to generate
    new image reconstructions $\mathbf{x}^{\prime}\sim p_{\theta}(\mathbf{x}|\mathbf{z})$.
    $q_{\phi}(\mathbf{z}|\mathbf{x})$ represents a probabilistic encoder, parameterized
    with $\phi$, which embeds the input $\mathbf{x}$ into a latent representation
    $\mathbf{z}$. $p_{\theta}(\mathbf{x}|\mathbf{z})$ represents a probabilistic decoder,
    parameterized with $\theta$, which produces a distribution over the corresponding
    $\mathbf{x}$. The details on VAE and its loss function are provided in Supplementary
    Material.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: VAE-GAN。Kingma 和 Welling ([2014](#bib.bib25)) 提出的变分自编码器（VAE）是一个明确的生成网络的例子，并且是一种在神经解码中广泛使用的生成算法。与自编码器类似，VAE
    由编码器和解码器组成。但 VAE 并不是编码一个潜在向量，而是编码一个潜在空间的分布，使得生成过程成为可能。因此，VAE 的目标是找到潜在变量 ${\mathbf{z}}$
    的分布，我们可以从 $\mathbf{z}\sim q_{\phi}(\mathbf{z}|\mathbf{x})$ 进行采样以生成新的图像重建 $\mathbf{x}^{\prime}\sim
    p_{\theta}(\mathbf{x}|\mathbf{z})$。$q_{\phi}(\mathbf{z}|\mathbf{x})$ 代表一个概率编码器，用
    $\phi$ 参数化，它将输入 $\mathbf{x}$ 嵌入到潜在表示 $\mathbf{z}$ 中。$p_{\theta}(\mathbf{x}|\mathbf{z})$
    代表一个概率解码器，用 $\theta$ 参数化，它在对应的 $\mathbf{x}$ 上生成一个分布。有关 VAE 及其损失函数的详细信息见补充材料。
- en: A hybrid model by Larsen et al. ([2016](#bib.bib29)) integrates both the VAE
    and GAN in a framework called VAE-GAN. VAE-GAN combines VAE to produce latent
    features and GAN discriminator, which learns to discriminate between fake and
    real images. In VAE-GAN, the VAE decoder and GAN generator are combined into one.
    The advantages of VAE-GAN are as follows. First, the GAN’s adversarial loss enables
    generating visually more realistic images. Second, VAE-GAN achieves improved stability
    due to VAE-based optimization. This helps to avoid mode collapse inherent to GANs,
    which refers to a generator producing a limited subset of different outcomes (Ren
    et al., [2021](#bib.bib51); Xu et al., [2021](#bib.bib68)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Larsen 等人提出的混合模型 ([2016](#bib.bib29)) 集成了 VAE 和 GAN，形成了一个名为 VAE-GAN 的框架。VAE-GAN
    结合了 VAE 生成潜在特征和 GAN 判别器，后者学习区分虚假图像和真实图像。在 VAE-GAN 中，VAE 解码器和 GAN 生成器被合并为一个。VAE-GAN
    的优点如下。首先，GAN 的对抗损失使得生成的图像在视觉上更逼真。其次，由于基于 VAE 的优化，VAE-GAN 实现了更好的稳定性。这有助于避免 GAN
    固有的模式崩溃问题，即生成器生成有限子集的不同结果（Ren 等人，[2021](#bib.bib51); Xu 等人，[2021](#bib.bib68)）。
- en: 'A group of studies on reconstructing natural images from brain activity patterns,
    including Ren et al. ([2021](#bib.bib51)) and VanRullen and Reddy ([2019](#bib.bib66)),
    incorporated probabilistic inference using VAE-GAN. In a recent work by Ren et al.
    ([2021](#bib.bib51)), the authors presented a combined network called Dual-Variational
    Autoencoder/ Generative Adversarial Network (D-VAE/GAN). The framework, which
    we named RenD-VAE/GAN, consists of a dual VAE-based encoder and an adversarial
    decoder, as illustrated in Figure [6](#S4.F6 "Figure 6 ‣ 4.2 Generative methods
    ‣ 4 Deep learning-based approaches for natural image reconstruction ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey"). Dual-VAE consists
    of two probabilistic encoders: visual $E_{vis}$ and cognitive $E_{cog}$, which
    encode stimuli images $\mathbf{x}$ and brain activity patterns $\mathbf{v}$ to
    corresponding latent representations $\mathbf{z_{x}}$ and $\mathbf{z_{v}}$. The
    framework is trained in three sequential stages. In the first stage, visual stimuli
    images are used to train the visual encoder $E_{vis}$, generator ${{G}}$, and
    discriminator ${{D}}$. $E_{vis}$ learns the direct mapping from visual images
    into latent representations. Then, using output of $E_{vis}$, the generator ${{G}}$
    is trained to predict the images and ${{D}}$ is trained to discriminate the predicted
    images from real images. In the second stage, $E_{cog}$ is trained to map high-dimensional
    fMRI signals to cognitive latent features. The generator ${{G}}$ is fixed and
    ${{D}}$ is trained to discriminate between the stimuli images produced in the
    first stage and the cognitively-driven reconstructions from cognitive latent features.
    This way, $E_{cog}$ is forced to generate visual and cognitive latent representations
    similar to each other. In the last training stage, $E_{cog}$ is fixed, whereas
    ${{G}}$ and ${{D}}$ are fine-tuned on fMRI signals to improve the accuracy of
    the generated images via cognitive latent representations. In this stage, ${{D}}$
    is trained to discriminate between real stimuli images and reconstructed images.
    During testing, only a trained cognitive encoder and generator were used for the
    inference. Since $E_{vis}$ takes visual stimuli as input, its learned latent representations
    $\mathbf{z_{x}}$ can guide $E_{cog}$ to learn the latent representations $\mathbf{z_{v}}$.
    Thus, in the second training stage, the authors implement the concept of knowledge
    distillation by transferring knowledge from $E_{vis}$ to $E_{cog}$, which together
    represent the teacher and student networks (Hinton et al., [2015](#bib.bib18)).
    The learned latent representation vectors significantly improve the reconstruction
    quality by capturing visual information, such as color, texture, object position,
    and attributes.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '一组关于从脑活动模式重建自然图像的研究，包括 Ren 等人（[2021](#bib.bib51)）和 VanRullen 与 Reddy（[2019](#bib.bib66)），采用了基于
    VAE-GAN 的概率推断。在 Ren 等人（[2021](#bib.bib51)）的最新工作中，作者提出了一种名为双变分自编码器/生成对抗网络（D-VAE/GAN）的联合网络。该框架，我们称之为
    RenD-VAE/GAN，包含一个基于双 VAE 的编码器和一个对抗解码器，如图 [6](#S4.F6 "Figure 6 ‣ 4.2 Generative
    methods ‣ 4 Deep learning-based approaches for natural image reconstruction ‣
    Natural Image Reconstruction from fMRI using Deep Learning: A Survey") 所示。双 VAE
    包含两个概率编码器：视觉编码器 $E_{vis}$ 和认知编码器 $E_{cog}$，它们分别将刺激图像 $\mathbf{x}$ 和脑活动模式 $\mathbf{v}$
    编码为相应的潜在表示 $\mathbf{z_{x}}$ 和 $\mathbf{z_{v}}$。该框架分为三个连续的阶段进行训练。在第一个阶段，使用视觉刺激图像来训练视觉编码器
    $E_{vis}$、生成器 ${{G}}$ 和判别器 ${{D}}$。$E_{vis}$ 学习将视觉图像直接映射到潜在表示。然后，利用 $E_{vis}$
    的输出，训练生成器 ${{G}}$ 来预测图像，而 ${{D}}$ 被训练以区分预测图像和真实图像。在第二个阶段，训练 $E_{cog}$ 以将高维 fMRI
    信号映射到认知潜在特征。生成器 ${{G}}$ 被固定，而 ${{D}}$ 被训练以区分第一阶段生成的刺激图像和从认知潜在特征中驱动的认知重建图像。通过这种方式，$E_{cog}$
    被迫生成视觉和认知潜在表示相似的结果。在最后的训练阶段，$E_{cog}$ 被固定，而 ${{G}}$ 和 ${{D}}$ 则在 fMRI 信号上进行微调，以通过认知潜在表示提高生成图像的准确性。在这一阶段，${{D}}$
    被训练以区分真实的刺激图像和重建图像。在测试过程中，仅使用训练好的认知编码器和生成器进行推断。由于 $E_{vis}$ 以视觉刺激作为输入，它学习到的潜在表示
    $\mathbf{z_{x}}$ 可以指导 $E_{cog}$ 学习潜在表示 $\mathbf{z_{v}}$。因此，在第二个训练阶段，作者通过将知识从 $E_{vis}$
    转移到 $E_{cog}$ 实现了知识蒸馏的概念，这两个网络分别代表教师和学生网络（Hinton 等人，[2015](#bib.bib18)）。学习到的潜在表示向量通过捕捉视觉信息（如颜色、纹理、物体位置和属性）显著提高了重建质量。'
- en: '![Refer to caption](img/ec7f69fc7a23ba3e383104ac100dfe2b.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ec7f69fc7a23ba3e383104ac100dfe2b.png)'
- en: 'Figure 6: RenD-VAE/GAN framework consists of three main components: dual VAE-based
    encoder, adversarial decoder, and discriminator. The visual encoder $E_{vis}$,
    cognitive encoder $E_{cog}$, generator, and discriminator were used during the
    training. During testing, only a trained cognitive encoder and generator were
    used for the inference. The red arrow denotes the transfer of knowledge from the
    teacher network $E_{vis}$ to the student network $E_{cog}$. The components in
    blue denote training on external unlabeled natural images (without fMRI activity)
    from ImageNet, which do not overlap with images in the train/test set.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：RenD-VAE/GAN 框架由三个主要组件组成：基于双重 VAE 的编码器、对抗解码器和判别器。训练过程中使用了视觉编码器 $E_{vis}$、认知编码器
    $E_{cog}$、生成器和判别器。在测试阶段，仅使用训练好的认知编码器和生成器进行推断。红色箭头表示知识从教师网络 $E_{vis}$ 转移到学生网络 $E_{cog}$。蓝色组件表示在来自
    ImageNet 的外部未标记自然图像（没有 fMRI 活动）上进行训练，这些图像与训练/测试集中的图像不重叠。
- en: 'VanRullen and Reddy ([2019](#bib.bib66)) utilized VAE network pretrained on
    CelebA dataset using GAN procedure to learn variational latent space. Similar
    to MozafariBigBiGAN framework, the authors learned a linear mapping between latent
    feature space and fMRI patterns, rather than using probabilistic inference (Güçlütürk
    et al., [2017](#bib.bib16)). In the training stage, the pretrained encoder from
    VAE-GAN is fixed and the linear mapping between latent feature space and fMRI
    patterns is learned. For the test stage, fMRI patterns are first translated into
    VAE latent codes via inverse mapping, and then these codes are used to reconstruct
    the faces. The latent space of a VAE is a variational layer that provides a meaningful
    description of each image and can represent faces and facial features as linear
    combinations of each other. Owing to the training objective of the VAE, the points
    which appear close in this space are mapped onto similar face images, which are
    always visually plausible. Therefore, the VAE’s latent space ensures that the
    brain decoding becomes more robust mapping errors. As a result, the produced reconstructions
    from VAE-GAN appear to be more realistic and closer to the original stimuli images.
    This method not only allows the reconstruction of naturally looking faces but
    also decodes face gender. In terms of architecture, the framework, which we refer
    to as VanRullenVAE-GAN, consists of three networks, as shown in Figure [7](#S4.F7
    "Figure 7 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey").'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 'VanRullen 和 Reddy ([2019](#bib.bib66)) 利用在 CelebA 数据集上经过 GAN 过程预训练的 VAE 网络来学习变分潜在空间。与
    MozafariBigBiGAN 框架类似，作者学习了潜在特征空间与 fMRI 模式之间的线性映射，而不是使用概率推断（Güçlütürk et al.,
    [2017](#bib.bib16)）。在训练阶段，VAE-GAN 的预训练编码器是固定的，潜在特征空间与 fMRI 模式之间的线性映射被学习。在测试阶段，fMRI
    模式首先通过逆映射转换为 VAE 潜在编码，然后这些编码用于重建面孔。VAE 的潜在空间是一个变分层，它提供了对每张图像的有意义的描述，并且可以将面孔和面部特征表示为彼此的线性组合。由于
    VAE 的训练目标，在这个空间中看起来接近的点被映射到相似的面孔图像，这些图像总是视觉上可信的。因此，VAE 的潜在空间确保了大脑解码变得更为稳健，减少了映射错误。因此，从
    VAE-GAN 产生的重建图像看起来更真实，更接近原始刺激图像。这种方法不仅允许重建自然面孔，还可以解码面孔性别。在架构方面，我们称之为 VanRullenVAE-GAN
    的框架由三个网络组成，如图 [7](#S4.F7 "Figure 7 ‣ 4.2 Generative methods ‣ 4 Deep learning-based
    approaches for natural image reconstruction ‣ Natural Image Reconstruction from
    fMRI using Deep Learning: A Survey") 所示。'
- en: '![Refer to caption](img/936bf5407a7790c9ca20321bb95cdd57.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/936bf5407a7790c9ca20321bb95cdd57.png)'
- en: 'Figure 7: VanRullenVAE-GAN framework proposed by VanRullen and Reddy ([2019](#bib.bib66)).
    The encoder $\boldsymbol{E}$ maps a stimulus image onto the latent representation
    $\mathbf{z}$. The generator ${G}$ uses $\mathbf{z}$ to reconstruct the stimuli
    image. The pretrained components are shown in yellow.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：VanRullen 和 Reddy ([2019](#bib.bib66)) 提出的 VanRullenVAE-GAN 框架。编码器 $\boldsymbol{E}$
    将刺激图像映射到潜在表示 $\mathbf{z}$。生成器 ${G}$ 使用 $\mathbf{z}$ 重建刺激图像。预训练的组件以黄色显示。
- en: 5 Reconstruction evaluation
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 重建评估
- en: 'The evaluation of reconstruction methods is based on human-based and image
    metrics, which we schematically present in Figure [8](#S5.F8 "Figure 8 ‣ 5 Reconstruction
    evaluation ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey").
    We first present human-based and image metrics and then describe the differences
    in image comparison settings.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 重建方法的评估基于人工和图像指标，我们在图 8 中以示意图的形式呈现。我们首先介绍人工和图像指标，然后描述图像比较设置的差异。
- en: '![Refer to caption](img/d492a7091d430dc9348f3ea086d8119d.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/d492a7091d430dc9348f3ea086d8119d.png)'
- en: 'Figure 8: Image-metric-based and human-based evaluation.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：基于图像指标和人工评估的评估。
- en: 'Table 3: Comparison of methods in terms of the used evaluation metrics. PCC
    stands for the Pearson correlation coefficient.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：按所用评估指标比较方法。PCC 代表皮尔逊相关系数。
- en: '|  | Human-based metrics | Image metrics |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | 人工评估指标 | 图像指标 |'
- en: '| Reference |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 |'
- en: '&#124; Quantitative &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 定量 &#124;'
- en: '&#124; survey &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 调查 &#124;'
- en: '|'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Visual &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉 &#124;'
- en: '&#124; inspection &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检查 &#124;'
- en: '| Traditional | PSM |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 传统 | PSM |'
- en: '| Seeliger et al. ([2018](#bib.bib58)) | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Seeliger 等 ([2018](#bib.bib58)) | ✓ | ✓ | ✗ | ✗ |'
- en: '| Shen et al. ([2019b](#bib.bib60)) | ✓ | ✓ | Pairwise PCC | ✗ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Shen 等 ([2019b](#bib.bib60)) | ✓ | ✓ | 成对 PCC | ✗ |'
- en: '| Shen et al. ([2019a](#bib.bib59)) | ✓ | ✓ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Shen 等 ([2019a](#bib.bib59)) | ✓ | ✓ |'
- en: '&#124; Pairwise PCC &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 成对 PCC &#124;'
- en: '&#124; Pairwise SSIM &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 成对 SSIM &#124;'
- en: '| ✗ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| ✗ |'
- en: '| Beliy et al. ([2019](#bib.bib2)) | ✗ | ✓ | 2,5,10-way PCC | ✗ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Beliy 等 ([2019](#bib.bib2)) | ✗ | ✓ | 2,5,10-way PCC | ✗ |'
- en: '| Gaziv et al. ([2020](#bib.bib14)) | ✓ | ✓ | ✗ | 2,5,10-way PSM |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Gaziv 等 ([2020](#bib.bib14)) | ✓ | ✓ | ✗ | 2,5,10-way PSM |'
- en: '| Qiao et al. ([2020](#bib.bib46)) | ✗ | ✓ |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Qiao 等 ([2020](#bib.bib46)) | ✗ | ✓ |'
- en: '&#124; PCC &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PCC &#124;'
- en: '&#124; SSIM &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM &#124;'
- en: '| AlexNet |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet |'
- en: '| Fang et al. ([2020](#bib.bib12)) | ✗ | ✓ | Pairwise PCC | ✗ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Fang 等 ([2020](#bib.bib12)) | ✗ | ✓ | 成对 PCC | ✗ |'
- en: '| Mozafari et al. ([2020](#bib.bib39)) | ✗ | ✓ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Mozafari 等 ([2020](#bib.bib39)) | ✗ | ✓ |'
- en: '&#124; Pairwise PCC &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 成对 PCC &#124;'
- en: '&#124; Pix-Comp (2-way PCC) &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Pix-Comp (2-way PCC) &#124;'
- en: '| Inception-V3 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Inception-V3 |'
- en: '| Ren et al. ([2021](#bib.bib51)) | ✓ | ✓ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Ren 等 ([2021](#bib.bib51)) | ✓ | ✓ |'
- en: '&#124; Linear correlation &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 线性相关 &#124;'
- en: '&#124; SSIM &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM &#124;'
- en: '&#124; 2,5,10-way PCC &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2,5,10-way PCC &#124;'
- en: '| ✗ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| ✗ |'
- en: 5.1 Human-based evaluation
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 人工评估
- en: The intuitive method of measuring the quality of reconstruction in natural image
    reconstruction task is by employing human evaluators. Human-based evaluation can
    be conducted quantitatively and qualitatively through visual inspection.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 自然图像重建任务中测量重建质量的直观方法是使用人工评估。人工评估可以通过视觉检查定量和定性地进行。
- en: For quantitative human-based assessment, a behavioral study involving human
    subjects is conducted. In this study the reconstructed image is compared to the
    original or several candidate images, containing the original image. From the
    given candidate images, subjects are instructed to choose the one that appears
    to have a higher resemblance to the original. Such behavioral studies can be conducted
    by employing human evaluators or using Amazon Mechanical Turk⁶⁶6[www.mturk.com](www.mturk.com)
    (Seeliger et al., [2018](#bib.bib58); Gaziv et al., [2020](#bib.bib14)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定量的人工评估，会进行涉及人类受试者的行为研究。在这项研究中，将重建的图像与包含原始图像的原始图像或几个候选图像进行比较。受试者从给定的候选图像中选择出与原始图像相似度较高的图像。这类行为研究可以通过人工评估员或使用
    Amazon Mechanical Turk⁶⁶6[www.mturk.com](www.mturk.com)（Seeliger 等，[2018](#bib.bib58);
    Gaziv 等，[2020](#bib.bib14)）进行。
- en: 'Owing to the additional time and human input required for human-based evaluation,
    several recent studies omit quantitative human evaluation in favor of qualitative
    visual inspections. For visual comparison, the set of original images and their
    reconstructions from different reconstruction methods are juxtaposed for ease
    of comparison (see Figures [9](#S6.F9 "Figure 9 ‣ 6.1 Visual comparison results
    ‣ 6 Fair comparison across the methods ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey") A and [9](#S6.F9 "Figure 9 ‣ 6.1 Visual comparison
    results ‣ 6 Fair comparison across the methods ‣ Natural Image Reconstruction
    from fMRI using Deep Learning: A Survey") B). Reconstructions are usually compared
    in terms of image sharpness/blurriness, matching shapes, colors, and low/high-level
    details. Many recent works focus on emphasizing the “naturalness” of their reconstructions,
    despite the reconstructions deviating significantly from the actual images in
    terms of the object category (see reconstructions in column 4 in Figure [9](#S6.F9
    "Figure 9 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison across the methods
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey") B for
    example).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于人工的评估需要额外的时间和人力投入，最近的几项研究在定量人类评估方面有所省略，而倾向于进行定性的视觉检查。为了视觉对比，原始图像及其通过不同重建方法得到的重建图像被并排展示以便于比较（参见图
    [9](#S6.F9 "图 9 ‣ 6.1 视觉比较结果 ‣ 6 方法间公平比较 ‣ 基于深度学习的 fMRI 自然图像重建综述") A 和 [9](#S6.F9
    "图 9 ‣ 6.1 视觉比较结果 ‣ 6 方法间公平比较 ‣ 基于深度学习的 fMRI 自然图像重建综述") B）。重建图像通常会在图像清晰度/模糊度、形状匹配、颜色以及低级/高级细节方面进行比较。许多最近的研究专注于强调其重建图像的“自然性”，尽管这些重建图像在对象类别方面与实际图像显著偏离（例如，参见图
    [9](#S6.F9 "图 9 ‣ 6.1 视觉比较结果 ‣ 6 方法间公平比较 ‣ 基于深度学习的 fMRI 自然图像重建综述") B 中第4列的重建图像）。
- en: 'Although human-based evaluation is a more reliable measurement of the quality
    of the reconstructed image, it suffers from the following limitations. First,
    human-based evaluation is time consuming and expensive because the process requires
    a well-designed evaluation study and the recruitment of human subjects. Second,
    the results can be heavily affected by subjects’ physical and emotional conditions
    or external conditions, such as lighting or image display (Wang et al., [2004](#bib.bib67);
    Rolls, [2012](#bib.bib53)). Table [3](#S5.T3 "Table 3 ‣ 5 Reconstruction evaluation
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey") shows
    that only several studies conducted the quantitative human-based evaluation.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于人工的评估是衡量重建图像质量的更可靠方法，但它存在以下局限性。首先，基于人工的评估耗时且昂贵，因为该过程需要精心设计的评估研究以及招募受试者。其次，结果可能会受到受试者的身体和情绪状态或外部条件（如光线或图像显示）的严重影响（Wang
    et al., [2004](#bib.bib67); Rolls, [2012](#bib.bib53)）。表 [3](#S5.T3 "表 3 ‣ 5 重建评估
    ‣ 基于深度学习的 fMRI 自然图像重建综述") 显示，只有少数研究进行了定量的基于人工的评估。
- en: 5.2 Image-metric-based evaluation
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基于图像指标的评估
- en: As an alternative to human-based evaluation, image-metric-based evaluation is
    used to accurately and automatically assess image reconstruction quality. The
    use of image metrics for evaluation is more practical, and unlike human-based
    assessment, is unbiased towards external factors. However, the image-metric-based
    evaluation can provide only an approximation of the visual comparison mechanism
    inherent to a human subject, and thus are far from being perfect (Wang et al.,
    [2004](#bib.bib67)).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基于人工评估的替代方案，基于图像指标的评估用于准确且自动地评估图像重建质量。使用图像指标进行评估更为实际，并且与基于人工的评估不同，它对外部因素不偏不倚。然而，基于图像指标的评估只能提供接近于人类主体固有的视觉比较机制的近似值，因此距离完美还有很大差距（Wang
    et al., [2004](#bib.bib67)）。
- en: Nowadays, there exist various image metrics that can compare images at different
    levels of perceptual representation. Image metrics used in the visual decoding
    literature can be categorized into traditional metrics that capture low-level
    perceptual similarities and more recent ones that capture high-level perceptual
    similarity. The conventional metrics, which include the mean squared error (MSE),
    pixelwise Pearson correlation coefficient (PCC), structural similarity index (SSIM),
    and their variants, are computed in pixel space and capture low-level perceptual
    similarity. The metric that captures high-level perceptual similarity relies on
    multilevel feature extraction from DNN and can compare images at a higher level
    of perceptual representation. The high-level metric we considere here is called
    Perceptual Similarity Metric (PSM).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，存在各种图像指标可以在不同层次的感知表示下比较图像。用于视觉解码文献中的图像指标可以分为捕捉低级感知相似性的传统指标和捕捉高级感知相似性的较新指标。传统指标，包括均方误差
    (MSE)、像素级 Pearson 相关系数 (PCC)、结构相似性指数 (SSIM) 及其变体，都是在像素空间中计算的，捕捉低级感知相似性。捕捉高级感知相似性的指标依赖于
    DNN 的多级特征提取，并可以在更高层次的感知表示中比较图像。我们在此考虑的高级指标称为感知相似性指标 (PSM)。
- en: MSE is the simplest traditional metric for assessing image reconstruction quality.
    Given $x_{i}$ and $y_{i}$, which are the flattened one-dimensional representations
    of the original and the reconstructed images, the MSE estimated over $N$ samples
    is computed as
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 是评估图像重建质量的最简单传统指标。给定 $x_{i}$ 和 $y_{i}$，它们是原始图像和重建图像的展平一维表示，基于 $N$ 个样本计算的
    MSE 为
- en: '|  | $MSE=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-y_{i})^{2}.$ |  | (1) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | $MSE=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-y_{i})^{2}.$ |  | (1) |'
- en: 'Several characteristics of MSE, including simplicity of implementation and
    fast computation, make it a widely used performance metric in signal processing.
    However, MSE shows poor correspondence to human visual perception, due to some
    of the underlying assumptions: MSE is independent of the spatial relationship
    between image pixels and considers each of them to be equally important (Wang
    et al., [2004](#bib.bib67)).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 的几个特点，包括实现简单和计算速度快，使其成为信号处理领域广泛使用的性能指标。然而，MSE 与人类视觉感知的对应关系较差，这归因于其一些基本假设：MSE
    不考虑图像像素之间的空间关系，且认为每个像素的重要性是相同的 (Wang et al., [2004](#bib.bib67))。
- en: 'PCC is widely used in statistical analysis to measure the linear relationship
    between two variables. The following equation is used to compute the pixelwise
    Pearson correlation between the flattened 1-D representations of the original
    image $x$ and the reconstructed image $y$:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: PCC 广泛用于统计分析中，以测量两个变量之间的线性关系。以下公式用于计算原始图像 $x$ 和重建图像 $y$ 的展平一维表示之间的像素级 Pearson
    相关性：
- en: '|  | $PCC(x,y)=\frac{\sum(x-\mu_{x})(y-\mu_{y})}{\sqrt{\sum(x-\mu_{x})^{2}\sum(y-\mu_{y})^{2}}},$
    |  | (2) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $PCC(x,y)=\frac{\sum(x-\mu_{x})(y-\mu_{y})}{\sqrt{\sum(x-\mu_{x})^{2}\sum(y-\mu_{y})^{2}}},$
    |  | (2) |'
- en: 'where $\mu_{x}$ and $\mu_{y}$ are the mean intensities of the flattened one-dimensional
    vectors $x$ and $y$, respectively. PCC is the most common metric used across the
    surveyed works (see Table [3](#S5.T3 "Table 3 ‣ 5 Reconstruction evaluation ‣
    Natural Image Reconstruction from fMRI using Deep Learning: A Survey")), with
    slight variations in naming and implementation: Pairwise PCC (Shen et al., [2019b](#bib.bib60),
    [a](#bib.bib59)), pixel correlation (Ren et al., [2021](#bib.bib51)), Pix-Comp
    (Mozafari et al., [2020](#bib.bib39)), and n-way PCC (Beliy et al., [2019](#bib.bib2)).
    The limitation of PCC is its sensitivity to changes in the edge intensity or edge
    misalignment. Thus, the metric tends to assign higher scores to blurry images
    than to images with distinguishable but misaligned shapes (Beliy et al., [2019](#bib.bib2)).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu_{x}$ 和 $\mu_{y}$ 分别是展平的一维向量 $x$ 和 $y$ 的平均强度。PCC 是调查中最常用的指标（见表 [3](#S5.T3
    "表 3 ‣ 5 重建评估 ‣ 基于深度学习的 fMRI 自然图像重建：综述")），在命名和实现上有细微的变化：Pairwise PCC (Shen et
    al., [2019b](#bib.bib60), [a](#bib.bib59))，像素相关性 (Ren et al., [2021](#bib.bib51))，Pix-Comp
    (Mozafari et al., [2020](#bib.bib39))，以及 n-way PCC (Beliy et al., [2019](#bib.bib2))。PCC
    的局限性在于对边缘强度变化或边缘错位的敏感性。因此，该指标往往会对模糊图像赋予更高的分数，而对具有可区分但错位形状的图像则给予较低的评分 (Beliy et
    al., [2019](#bib.bib2))。
- en: 'SSIM is widely used image similarity metric that captures structural information
    from images. Wang et al. proposed SSIM as a quality assessment metric that resembles
    the characteristics of the human visual perception (Wang et al., [2004](#bib.bib67)).
    Unlike PCC, which treats each pixel of the image independently, SSIM measures
    the similarity of spatially close pixels between the reconstructed and original
    images. Given two images, SSIM is computed as a weighted combination of three
    comparative measures: luminance, contrast, and structure. Assuming an equal contribution
    of each measure, the SSIM is first computed locally between the corresponding
    windows $p$ and $q$ of images $x$ and $y$:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: SSIM 是一种广泛使用的图像相似度度量，能够捕捉图像中的结构信息。Wang 等人提出了 SSIM 作为一种质量评估度量，类似于人类视觉感知的特征（Wang
    等人，[2004](#bib.bib67)）。与 PCC 不同，PCC 将图像的每个像素独立对待，而 SSIM 测量重建图像和原始图像之间空间上接近的像素的相似度。给定两幅图像，SSIM
    作为三种比较度量：亮度、对比度和结构的加权组合来计算。假设每种度量的贡献相等，首先在图像 $x$ 和 $y$ 的对应窗口 $p$ 和 $q$ 之间局部计算
    SSIM：
- en: '|  | $SSIM(p,q)=\frac{(2\mu_{p}\mu_{q}+C_{1})(2\sigma_{pq}+C_{2})}{(\mu_{p}^{2}+\mu_{q}^{2}+C_{1})(\sigma_{p}^{2}+\sigma_{q}^{2}+C_{2})},$
    |  | (3) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $SSIM(p,q)=\frac{(2\mu_{p}\mu_{q}+C_{1})(2\sigma_{pq}+C_{2})}{(\mu_{p}^{2}+\mu_{q}^{2}+C_{1})(\sigma_{p}^{2}+\sigma_{q}^{2}+C_{2})},$
    |  | (3) |'
- en: 'where $\mu_{p}$ and $\mu_{q}$ are the mean intensity values of $p$ and $q$,
    respectively; $\sigma_{p}^{2}$ and $\sigma_{q}^{2}$ are the variances of $p$ and
    $q$, respectively; $\sigma_{pq}$ is the covariance of $p$ and $q$, and $C_{1}$
    and $C_{2}$ are constants that ensure stability when the denominator is close
    to zero. The global SSIM score is computed as the average of all $M$ local SSIM
    scores:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mu_{p}$ 和 $\mu_{q}$ 分别是 $p$ 和 $q$ 的均值强度值；$\sigma_{p}^{2}$ 和 $\sigma_{q}^{2}$
    分别是 $p$ 和 $q$ 的方差；$\sigma_{pq}$ 是 $p$ 和 $q$ 的协方差，$C_{1}$ 和 $C_{2}$ 是在分母接近零时确保稳定性的常数。全局
    SSIM 分数计算为所有 $M$ 个局部 SSIM 分数的平均值：
- en: '|  | $SSIM(x,y)=\sum_{i=1}^{M}SSIM(p_{i},q_{i}).$ |  | (4) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $SSIM(x,y)=\sum_{i=1}^{M}SSIM(p_{i},q_{i}).$ |  | (4) |'
- en: PSM. Despite the wide adoption of SSIM as a perceptual metric, it compares poorly
    with many characteristics of human perception (Zhang et al., [2018](#bib.bib70)).
    Several studies, including Güçlütürk et al. ([2017](#bib.bib16)), Qiao et al.
    ([2018](#bib.bib47)), Mozafari et al. ([2020](#bib.bib39)), and Gaziv et al. ([2020](#bib.bib14)),
    emphasize the importance of higher-level perceptual similarity over lower-level
    metrics in evaluation because of the better correspondence of higher-level perceptual
    similarity to human perceptual judgments (Zhang et al., [2018](#bib.bib70)).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: PSM。尽管 SSIM 作为感知度量被广泛采用，但它与许多人类感知特征的比较效果较差（Zhang 等人，[2018](#bib.bib70)）。包括 Güçlütürk
    等人（[2017](#bib.bib16)）、Qiao 等人（[2018](#bib.bib47)）、Mozafari 等人（[2020](#bib.bib39)）和
    Gaziv 等人（[2020](#bib.bib14)）在内的几项研究强调了在评估中高层次感知相似度相较于低层次度量的重要性，因为高层次感知相似度与人类感知判断的对应关系更好（Zhang
    等人，[2018](#bib.bib70)）。
- en: 'As the general principle, a CNN is used for extracting hierarchical multilayer
    features of input images, which are further compared across layers using a distance
    metric of choice. However, the definition and implementation of the perceptual
    similarity metric in terms of the distance metric or feature extraction network
    vary across studies. For example, Qiao et al. ([2020](#bib.bib46)) utilized five
    convolutional layers of the AlexNet (Krizhevsky et al., [2012](#bib.bib28)) to
    extract hierarchical features. The other study by Mozafari et al. ([2020](#bib.bib39)),
    proposed a high-level similarity measure, which measures perceptual similarity
    based on the output of only the last layer of Inception-V3 (Szegedy et al., [2016](#bib.bib63)).
    Finally, Gaziv et al. ([2020](#bib.bib14)) used the PSM definition proposed in
    (Zhang et al., [2018](#bib.bib70)) with the pretrained AlexNet with linear calibration.
    Following Gaziv et al. ([2020](#bib.bib14)), we provide a PSM definition by Zhang
    et al. ([2018](#bib.bib70)) in the following equation:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一般原则是使用 CNN 提取输入图像的层次多层特征，然后使用选择的距离度量在层之间进行比较。然而，感知相似度度量在距离度量或特征提取网络的定义和实现因研究而异。例如，Qiao
    等人（[2020](#bib.bib46)）利用了 AlexNet（Krizhevsky 等人，[2012](#bib.bib28)）的五个卷积层来提取层次特征。Mozafari
    等人（[2020](#bib.bib39)）的另一项研究提出了一种基于 Inception-V3（Szegedy 等人，[2016](#bib.bib63)）最后一层输出的高级相似度度量。最后，Gaziv
    等人（[2020](#bib.bib14)）使用了（Zhang 等人，[2018](#bib.bib70)）提出的 PSM 定义，并对预训练的 AlexNet
    进行了线性校准。按照 Gaziv 等人（[2020](#bib.bib14)）的做法，我们在下式中提供了 Zhang 等人（[2018](#bib.bib70)）的
    PSM 定义：
- en: '|  | $d\left(x,y\right)=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{h,w}\left\&#124;w_{l}\odot\left({f}_{x}^{l}-{f}_{y}^{l}\right)\right\&#124;_{2}^{2},$
    |  | (5) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $d\left(x,y\right)=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{h,w}\left\&#124;w_{l}\odot\left({f}_{x}^{l}-{f}_{y}^{l}\right)\right\&#124;_{2}^{2},$
    |  | (5) |'
- en: where $d\left(x,y\right)$ is the distance between the original image $x$ and
    the reconstructed image $y$. ${f}_{x}^{l},{f}_{y}^{l}$ represent layerwise activations
    normalized across channels for layer $l$. The activations are scaled channelwise
    by vector $w_{l}\in\mathbb{R}^{C_{l}}$, spatially averaged, and summed layerwise.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d\left(x,y\right)$ 是原始图像 $x$ 和重建图像 $y$ 之间的距离。 ${f}_{x}^{l},{f}_{y}^{l}$
    表示第 $l$ 层的逐层激活值，并在通道间进行归一化。激活值按向量 $w_{l}\in\mathbb{R}^{C_{l}}$ 进行通道缩放，空间上进行平均，并按层汇总。
- en: Note that the underlying CNN model used for computing the PSM should be selected
    cautiously. Because many studies use pretrained CNN models, it is important to
    avoid using the same model for both training and evaluation, which may lead to
    a potential bias in evaluation. For example, several methods, including Shen et al.
    ([2019b](#bib.bib60)) and Beliy et al. ([2019](#bib.bib2)), used VGG-19 (Simonyan
    and Zisserman, [2015](#bib.bib61)) for pretraining. Therefore, the VGG-19 model
    should not be used for evaluation, as the objective of evaluation and optimization
    functions would be the same, and the evaluation would produce a higher similarity
    between original and reconstructed images.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，用于计算 PSM 的基础 CNN 模型应谨慎选择。由于许多研究使用预训练的 CNN 模型，因此避免在训练和评估中使用相同的模型非常重要，这可能导致评估中的潜在偏差。例如，Shen
    等（[2019b](#bib.bib60)）和 Beliy 等（[2019](#bib.bib2)）等几种方法使用了 VGG-19（Simonyan 和 Zisserman，[2015](#bib.bib61)）进行预训练。因此，不应使用
    VGG-19 模型进行评估，因为评估和优化函数的目标是相同的，评估将产生原始图像和重建图像之间的更高相似度。
- en: 5.3 Image comparison setting
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 图像比较设置
- en: 'We describe three image comparison settings existing in literature: 1) one-to-one
    comparison, 2) pairwise comparison, and 3) $n$-way comparison. Each of these comparison
    settings can work with any image or human-based metric of choice.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了文献中存在的三种图像比较设置：1）一对一比较，2）成对比较，以及 3）$n$-way 比较。每种比较设置都可以与任何图像或基于人工的度量一起使用。
- en: One-to-one is the simplest comparison setting which computes the similarity
    score of a reconstruction against ground truth using the given metric, for example,
    MSE or PCC. However, the absolute values of qualitative metrics computed only
    on a single pair of original and reconstructed images are challenging to interpret
    (Beliy et al., [2019](#bib.bib2)). Therefore, pairwise similarity and $n$-way
    identification are often used to measure the reconstruction quality across the
    dataset.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一对一是最简单的比较设置，通过使用给定的度量（例如 MSE 或 PCC）计算重建图像与真实图像的相似度分数。然而，仅对一对原始图像和重建图像计算的定性度量的绝对值难以解释（Beliy
    等，[2019](#bib.bib2)）。因此，成对相似度和 $n$-way 识别通常用于测量数据集中重建质量。
- en: 'Pairwise comparison analysis is performed by comparing a reconstructed image
    with two candidate images: the ground-truth image and the image selected from
    the remaining set, resulting in a total of $N(N-1)$ comparisons:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像的成对比较分析是通过将重建图像与两个候选图像进行比较来完成的：真实图像和从其余图像集中选择的图像，总共进行 $N(N-1)$ 次比较：
- en: '|  | $score=\frac{1}{N(N-1)}\sum_{i=1}^{N}\sum_{\begin{subarray}{c}j=1\\ j\neq
    i\end{subarray}}^{N}\sigma\left(m\left(y_{i},x_{i}\right),m\left(y_{i},x_{j}\right)\right),$
    |  | (6) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $score=\frac{1}{N(N-1)}\sum_{i=1}^{N}\sum_{\begin{subarray}{c}j=1\\ j\neq
    i\end{subarray}}^{N}\sigma\left(m\left(y_{i},x_{i}\right),m\left(y_{i},x_{j}\right)\right),$
    |  | (6) |'
- en: where $m$ is the metric of interest and
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 是感兴趣的度量标准。
- en: '|  | $\sigma(a,b)=\left\{\begin{array}[]{ll}1&amp;a>b\\ 0&amp;\text{ otherwise
    }\end{array}\right.$ |  | (7) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma(a,b)=\left\{\begin{array}[]{ll}1&amp;a>b\\ 0&amp;\text{ otherwise
    }\end{array}\right.$ |  | (7) |'
- en: 'The trial is considered correct if the metric score of the reconstructed image
    with the corresponding stimulus image is higher than that with the nonrelevant
    stimulus image. For metrics that imply that the lower, the better (such as MSE),
    the expression in the equation [7](#S5.E7 "In 5.3 Image comparison setting ‣ 5
    Reconstruction evaluation ‣ Natural Image Reconstruction from fMRI using Deep
    Learning: A Survey") is modified to find the smallest value. Finally, the percentage
    of total correct trials is computed as the ratio of correct trials among all trials
    (Shen et al., [2019b](#bib.bib60), [a](#bib.bib59); Beliy et al., [2019](#bib.bib2)).
    The chance-level accuracy is $50\%$.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果重建图像与相应刺激图像的度量分数高于与不相关刺激图像的度量分数，则认为试验是正确的。对于那些表明数值越低越好的度量（如MSE），方程式中的表达式[7](#S5.E7
    "在 5.3 图像比较设置 ‣ 5 重建评估 ‣ 基于深度学习的fMRI自然图像重建：综述")被修改为找到最小值。最后，正确试验的百分比被计算为所有试验中正确试验的比例（Shen等，[2019b](#bib.bib60)，[a](#bib.bib59)；Beliy等，[2019](#bib.bib2)）。偶然水平的准确率为$50\%$。
- en: 'In $\boldsymbol{n}$-way identification each reconstructed image is compared
    to $n$ randomly selected candidate images, including the ground truth. Several
    studies, including Beliy et al. ([2019](#bib.bib2)) and Ren et al. ([2021](#bib.bib51)),
    used $n=2,5,10$ for the $n$-way identification accuracy computed using PCC. In
    a more recent work, Gaziv et al. ([2020](#bib.bib14)) report $n=2,5,10,50$-way
    identification accuracy based on PSM. An addition source of confusion is the absence
    of naming conventions: Ren et al. ([2021](#bib.bib51)) and Mozafari et al. ([2020](#bib.bib39))
    referred to $n$-way identification accuracy computed with PCC as Pixel Correlation
    and pix-Comp, respectively.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在$\boldsymbol{n}$-way识别中，每个重建图像与包括真实图像在内的$n$个随机选择的候选图像进行比较。包括Beliy等（[2019](#bib.bib2)）和Ren等（[2021](#bib.bib51)）在内的几项研究使用了$n=2,5,10$来计算使用PCC的$n$-way识别准确率。在更近期的工作中，Gaziv等（[2020](#bib.bib14)）报告了基于PSM的$n=2,5,10,50$-way识别准确率。另一个混淆源是命名惯例的缺失：Ren等（[2021](#bib.bib51)）和Mozafari等（[2020](#bib.bib39)）分别将使用PCC计算的$n$-way识别准确率称为Pixel
    Correlation和pix-Comp。
- en: 6 Fair comparison across the methods
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 方法的公平比较
- en: 'For fair comparison of the existing methods, we chose those that satisfied
    one of the following criteria: 1) the availability of the complete code for reproducing
    the results and 2) the availability of reconstructed images for running the evaluation.
    This allowed us to compare five state-of-the-art methods on the DIR dataset, both
    visually (Section [6.1](#S6.SS1 "6.1 Visual comparison results ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey")) and quantitatively (Section [6.2](#S6.SS2 "6.2 Quantitative comparison
    results on natural images from DIR ‣ 6 Fair comparison across the methods ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey")). For the GOD,
    because of the lack of a complete set of reconstructions for the chosen methods,
    we only present a visual comparison in Section [6.1](#S6.SS1 "6.1 Visual comparison
    results ‣ 6 Fair comparison across the methods ‣ Natural Image Reconstruction
    from fMRI using Deep Learning: A Survey"). Visual comparison for vim-1 datasets
    is provided in Supplementary Material.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较现有的方法，我们选择了满足以下一个标准的方法：1）提供了完整代码以重现结果，2）提供了重建图像以进行评估。这使我们能够在DIR数据集上对五种最先进的方法进行视觉（第[6.1](#S6.SS1
    "6.1 视觉比较结果 ‣ 6 方法的公平比较 ‣ 基于深度学习的fMRI自然图像重建：综述")节）和定量（第[6.2](#S6.SS2 "6.2 自然图像的定量比较结果
    ‣ 6 方法的公平比较 ‣ 基于深度学习的fMRI自然图像重建：综述")节）比较。对于GOD，由于缺少所选方法的完整重建集，我们仅在第[6.1](#S6.SS1
    "6.1 视觉比较结果 ‣ 6 方法的公平比较 ‣ 基于深度学习的fMRI自然图像重建：综述")节中提供了视觉比较。vim-1数据集的视觉比较见附录材料。
- en: Our analysis of recent works on natural image reconstruction reveals that only
    a few comply with good machine learning practices regarding the fairness of evaluation.
    Unfair evaluation can be reflected in the comparison across different datasets,
    selecting specific subjects in reporting the results, and discrepancies in using
    the evaluation metrics. This motivated us to perform a rigorous empirical evaluation
    of the methods, i.e. cross-subject evaluation across common metrics using a common
    dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对最近自然图像重建工作的分析表明，只有少数工作遵循了良好的机器学习实践，特别是在评估的公平性方面。不公平的评估可以体现在不同数据集之间的比较、选择特定受试者报告结果以及在使用评估指标时的差异。这促使我们对这些方法进行严格的实证评估，即在使用共同数据集的情况下，通过常见指标进行跨受试者评估。
- en: Evaluation on a common dataset. To standardize the objective evaluation process,
    we perform the quantitative assessment on the DIR dataset for methods that we
    found to be reproducible, that is, ShenDNN, ShenDNN+DGN, ShenGAN, and BeliyEncDec.
    For FangSSGAN, we ran an evaluation based on the reconstructions provided by the
    authors.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在共同数据集上的评估。为了标准化客观评估过程，我们对我们认为可重复的方法，即ShenDNN、ShenDNN+DGN、ShenGAN和BeliyEncDec，进行了定量评估。对于FangSSGAN，我们基于作者提供的重建结果进行了评估。
- en: It is important to distinguish the five-subject GOD dataset (Horikawa and Kamitani,
    [2017](#bib.bib19)) from the three-subject DIR dataset (Shen et al., [2019b](#bib.bib60)),
    which uses the same stimuli images but is quite different in terms of the image
    presentation experiment and characteristics of fMRI activity data. Our choice
    of the DIR as a common natural image dataset is due to the following reasons.
    First, unlike the similar GOD dataset, DIR was acquired specifically for the natural
    image reconstruction task and contains a larger number of training samples due
    to increased number of repeats in image presentation experiment. In addition,
    this dataset might be of interest for studying the generalizability of natural
    image reconstruction methods to artificial shapes, which we describe in detail
    in Supplementary Material.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是区分五受试者GOD数据集（Horikawa and Kamitani, [2017](#bib.bib19)）和三受试者DIR数据集（Shen
    et al., [2019b](#bib.bib60)），虽然它们使用相同的刺激图像，但在图像呈现实验和fMRI活动数据的特征方面存在很大差异。我们选择DIR作为共同的自然图像数据集的原因如下。首先，与类似的GOD数据集不同，DIR是专门为自然图像重建任务获取的，并且由于图像呈现实验中的重复次数增加，它包含了更多的训练样本。此外，该数据集可能对研究自然图像重建方法对人工形状的普遍性感兴趣，我们在补充材料中详细描述了这一点。
- en: When training ShenDNN, ShenDNN+DGN, and BeliyEncDec on the DIR, we used the
    original training settings. For ShenGAN, we used the pretrained model provided
    by the authors. To maximize the amount of training data, each presented stimulus
    sample was treated as an individual sample (Shen et al., [2019a](#bib.bib59)).
    For reconstruction, we averaged the test fMRI activations across trials corresponding
    to the same stimulus to increase the signal-to-noise ratio. This resulted in 6,000
    training and 50 test fMRI-image samples. Note that BeliyEncDec was initially implemented
    for GOD dataset. For BeliyEncDec, averaging both training and test fMRI samples
    across the repeats resulted in the best performance. This confirms with the authors’
    observation that an increased number of fMRI repeats results in improved reconstruction
    (Beliy et al., [2019](#bib.bib2)). Additionally, we normalized training fMRI vectors
    to have a zero mean and unit standard deviation. The mean and standard deviation
    of the training fMRI data were used to normalize the test fMRI data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在DIR上训练ShenDNN、ShenDNN+DGN和BeliyEncDec时，我们使用了原始的训练设置。对于ShenGAN，我们使用了作者提供的预训练模型。为了最大化训练数据的量，每个呈现的刺激样本被视为一个单独的样本（Shen
    et al., [2019a](#bib.bib59)）。在重建过程中，我们通过将测试fMRI激活值在对应同一刺激的试次间进行平均，来提高信噪比。这导致了6,000个训练样本和50个测试fMRI图像样本。需要注意的是，BeliyEncDec最初是为GOD数据集实现的。对于BeliyEncDec，平均训练和测试fMRI样本在重复试次中的表现最佳。这与作者的观察一致，即fMRI重复次数的增加会提高重建效果（Beliy
    et al., [2019](#bib.bib2)）。此外，我们将训练fMRI向量标准化为零均值和单位标准差。训练fMRI数据的均值和标准差用于标准化测试fMRI数据。
- en: 'Evaluation across common metrics. We perform the evaluation on natural images
    from the DIR based on MSE, PCC, SSIM, and PSM metrics described in Section [5](#S5
    "5 Reconstruction evaluation ‣ Natural Image Reconstruction from fMRI using Deep
    Learning: A Survey"). We notice that there is no consensus among recent works
    on a standard set of evaluation metrics (see Table [3](#S5.T3 "Table 3 ‣ 5 Reconstruction
    evaluation ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey")).
    Moreover, several studies introduce new evaluation metrics or use variations of
    existing metrics, potentially more favorable for their results. In contrast, we
    present an evaluation of the methods across all the image metrics used in the
    related methods.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '在常见指标上的评估。我们基于MSE、PCC、SSIM和PSM指标对DIR中的自然图像进行了评估，这些指标在第[5](#S5 "5 Reconstruction
    evaluation ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey")节中描述。我们注意到，近期工作中对标准评估指标的共识尚未达成（见表[3](#S5.T3
    "Table 3 ‣ 5 Reconstruction evaluation ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey")）。此外，一些研究引入了新的评估指标或使用了现有指标的变体，这可能对其结果更为有利。相反，我们展示了对所有相关方法中使用的图像指标的评估。'
- en: It is also important to note that different methods generate output images of
    various sizes due to memory restrictions and variations in the pretrained model
    (we refer to Supplementary Material for details on output image resolutions).
    The evaluation metrics can be sensitive to the size of the image and the choice
    of upscaling or downscaling algorithms. For fairness, we rescaled the reconstructions
    for the DIR to the common size and use a bicubic algorithm for image resizing.
    We evaluated the reconstructed images using a resolution of 256 $\times$ 256 pixels,
    which is the highest among the chosen methods. For methods with a lower reconstruction
    image size, we applied image upscaling.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，由于内存限制和预训练模型的差异，不同方法生成的输出图像大小各异（有关输出图像分辨率的详细信息，请参见补充材料）。评估指标对图像的大小以及放大或缩小算法的选择可能非常敏感。为了公平起见，我们将DIR的重建结果调整为统一大小，并使用了双三次插值算法进行图像缩放。我们使用256
    $\times$ 256像素的分辨率评估了重建图像，这是所选方法中最高的。对于重建图像尺寸较小的方法，我们应用了图像放大。
- en: 6.1 Visual comparison results
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 视觉比较结果
- en: '![Refer to caption](img/896871d6b3e66b33c6fac331860dd476.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/896871d6b3e66b33c6fac331860dd476.png)'
- en: 'Figure 9: (A) Visual comparison of methods on Deep Image Reconstruction dataset
    for subject 1\. The reconstructions for all methods except for Fang et al. ([2020](#bib.bib12))
    are obtained by reproducing the experiments. For (A) and (B), the stimulus images
    are shown in the first column. The corresponding reconstructed images from each
    method are shown in the subsequent columns. (B) Visual comparison of the methods
    on the GOD dataset. Due to the unavailability of complete reconstruction data
    for GOD, visual reconstructions correspond to the same image stimuli but different
    subjects. For BeliyEncDec and GazivEncDec, we present the reconstruction for subject
    3\. The reconstructions for all methods are provided by the authors or reported
    in the original papers. SeeligerDCGAN uses the average of the stimuli representations
    for the three subjects.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：（A）在深度图像重建数据集上对主题1的不同方法进行视觉比较。所有方法的重建结果都是通过重复实验获得的，除了Fang等（[2020](#bib.bib12)）。在（A）和（B）中，刺激图像显示在第一列。每种方法对应的重建图像显示在后续列中。（B）在GOD数据集上的方法视觉比较。由于GOD缺乏完整的重建数据，视觉重建对应于相同的图像刺激但不同的受试者。对于BeliyEncDec和GazivEncDec，我们展示了主题3的重建结果。所有方法的重建结果由作者提供或在原始论文中报告。SeeligerDCGAN使用三个主题的刺激表示的平均值。
- en: 'Figure [9](#S6.F9 "Figure 9 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") A shows the reconstructions of sample stimuli images from the test
    set, corresponding to subject 1 from DIR dataset. The reconstructions from all
    methods show a close resemblance to the original images in terms of the object
    shape and position. GAN-based methods, i.e., ShenDNN+DGN and ShenGAN, produce
    sharper and smoother-looking images but in some cases render semantic details
    absent in the original stimuli (which is confirmed by lower pixelwise MSE and
    PCC scores). Reconstructions by FangSSGAN are also natural looking and close to
    real images in terms of shape and color. This is attributed to using a generator
    network conditioned on both shape and semantic information, which preserves low-level
    features, such as texture or shape. Reconstruction by nonGAN⁷⁷7By “nonGAN” methods,
    we mean the models that do not take advantage of the GAN training procedure. BeliyEncDec
    are blurry but accurately capture the shape of the stimuli objects.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [9](#S6.F9 "Figure 9 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") A 显示了来自测试集的样本刺激图像的重建结果，对应于 DIR 数据集中的受试者 1。所有方法的重建结果在物体形状和位置上都与原始图像非常相似。基于
    GAN 的方法，即 ShenDNN+DGN 和 ShenGAN，生成了更锐利且更光滑的图像，但在某些情况下会丧失原始刺激中的语义细节（这通过较低的像素级 MSE
    和 PCC 分数得到证实）。FangSSGAN 的重建结果在形状和颜色上也接近真实图像。原因在于使用了一个同时基于形状和语义信息的生成网络，这保持了低级特征，如纹理或形状。由非GAN⁷⁷7By
    “nonGAN” methods, we mean the models that do not take advantage of the GAN training
    procedure. BeliyEncDec 生成的重建图像较模糊，但准确捕捉了刺激物体的形状。'
- en: 'In addition, we present the reconstructions for GOD dataset in Figure [9](#S6.F9
    "Figure 9 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison across the methods
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey") B. Similar
    to DIR dataset, the GAN-based methods MozafariBigBiGAN and RenD-VAE/GAN produce
    the most natural-looking images. Visually, MozafariBigBiGAN outperforms other
    methods in terms of naturalness. However, this comes at the cost of rendering
    object categories and details different from those presented in the original stimuli.
    We identified GazivEncDec and RenD-VAE/GAN as performing relatively better on
    the reconstruction of shape and color. GazivEncDec is superior in reconstructing
    high-level details of the image, including shape and background. RenD-VAE/GAN
    visually outperforms other methods for the reconstruction of color, background,
    and lower-level details. For GazivEncDec, a significant improvement in the reconstruction
    accuracy was achieved owing to the introduced perceptual similarity loss. According
    to Ren et al. ([2021](#bib.bib51)), the key factors boosting the reconstruction
    quality of the RenD-VAE/GAN include the VAE-GAN architecture instead of the standard
    conditional GAN and visual-feature guidance implemented via GAN-based knowledge
    distillation. In SeeligerDCGAN and BeliyEncDec, the reconstructions are blurry,
    which could be due to the use of pixelwise MSE loss (Seeliger et al., [2018](#bib.bib58)).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们在图 [9](#S6.F9 "Figure 9 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") B 中展示了 GOD 数据集的重建结果。与 DIR 数据集类似，基于 GAN 的方法 MozafariBigBiGAN 和 RenD-VAE/GAN
    生成了最自然的图像。从视觉上看，MozafariBigBiGAN 在自然度方面优于其他方法。然而，这以渲染的物体类别和细节与原始刺激不同为代价。我们发现 GazivEncDec
    和 RenD-VAE/GAN 在形状和颜色的重建上表现相对较好。GazivEncDec 在重建图像的高层次细节（包括形状和背景）方面优于其他方法。RenD-VAE/GAN
    在颜色、背景和低级细节的重建上视觉表现优于其他方法。对于 GazivEncDec，由于引入了感知相似度损失，重建精度显著提高。根据 Ren 等人（[2021](#bib.bib51)）的研究，提升
    RenD-VAE/GAN 重建质量的关键因素包括使用 VAE-GAN 架构而不是标准条件 GAN 以及通过 GAN 基于知识蒸馏实施的视觉特征指导。在 SeeligerDCGAN
    和 BeliyEncDec 中，重建结果模糊，这可能是由于使用了像素级 MSE 损失（Seeliger 等人，[2018](#bib.bib58)）。'
- en: '![Refer to caption](img/eac3e8e05eedb8af687132690c187eee.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eac3e8e05eedb8af687132690c187eee.png)'
- en: 'Figure 10: Reconstructions for two images across three subjects from DIR dataset.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：来自 DIR 数据集的三个受试者的两个图像的重建结果。
- en: 'Since the DIR dataset comprises three-subject data, we additionally show the
    reconstructions across the methods corresponding to three different subjects in
    Figure [10](#S6.F10 "Figure 10 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey"). The reconstructions are shown for the two natural image stimuli. Depending
    on the subject, the reconstructions by different methods show varying degrees
    of resemblance to the original stimuli. For example, the reconstructions from
    ShenDNN+DGN, ShenGAN, and BeliyEncDec are visually better for subject 1, whereas,
    in reconstructions by other methods, neither color nor shape was preserved. This
    shows that the selection of a subject in reporting results can lead to a biased
    evaluation.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DIR数据集包含三个受试者的数据，我们在图[10](#S6.F10 "图10 ‣ 6.1 视觉比较结果 ‣ 6 各方法间公平比较 ‣ 基于深度学习的fMRI自然图像重建：综述")中另外展示了对应三个不同受试者的各方法重建结果。展示了两个自然图像刺激的重建结果。根据受试者的不同，不同方法的重建结果与原始刺激的相似度有所不同。例如，ShenDNN+DGN、ShenGAN和BeliyEncDec的重建对于受试者1来说视觉效果更好，而其他方法的重建既没有保留颜色也没有保留形状。这表明选择受试者进行结果报告可能会导致偏倚的评估。
- en: 6.2 Quantitative comparison results on natural images from DIR
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 自然图像的定量比较结果
- en: '![Refer to caption](img/7292145a2123bf4f89068ec10b784ae9.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7292145a2123bf4f89068ec10b784ae9.png)'
- en: 'Figure 11: Average $n$-way accuracy results computed across subjects using
    (A) MSE, (B) PCC, (C) SSIM, and (D) PSM metrics on natural images from the DIR.
    The horizontal dashed lines indicate the chance level for each metric. The full-way
    comparison corresponds to using all the images in the test set, that is, 50 natural
    images.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：使用(A) MSE、(B) PCC、(C) SSIM和(D) PSM指标计算的跨受试者的平均$n$-way准确度结果。水平虚线表示每个指标的机会水平。全方式比较对应于使用测试集中的所有图像，即50幅自然图像。
- en: 'To eliminate the bias of selecting a specific subject for evaluation, we present
    both subject-specific and cross-subject average results across multiple metrics
    on natural images from DIR. For comprehensive evaluation, we use three comparison
    settings described in Section [5](#S5 "5 Reconstruction evaluation ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey"): 1) one-to-one comparison;
    2) pairwise comparison; and 3) $n$-way comparison. The pairwise evaluation results
    for natural images across the metrics are shown in Table [4](#S6.T4 "Table 4 ‣
    6.2 Quantitative comparison results on natural images from DIR ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey"). The $n$-way scores for natural images are presented in Figure [11](#S6.F11
    "Figure 11 ‣ 6.2 Quantitative comparison results on natural images from DIR ‣
    6 Fair comparison across the methods ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey"). We find that one-to-one results are not well
    suited for cross-method comparison. We therefore present a one-to-one comparison
    in Supplementary Material. The quantitative evaluation of methods is presented
    based on low-level MSE, PCC, and SSIM metrics first, followed by a comparison
    using a high-level PSM metric.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除选择特定主题进行评估的偏差，我们展示了自然图像中针对多个指标的主题特定和跨主题平均结果。为了全面评估，我们使用了在第[5](#S5 "5 重建评估
    ‣ 基于深度学习的fMRI自然图像重建：综述")节中描述的三种比较设置：1) 一对一比较；2) 成对比较；3) $n$-way比较。自然图像的成对评估结果在表[4](#S6.T4
    "表4 ‣ 6.2 自然图像定量比较结果 ‣ 6 各方法间公平比较 ‣ 基于深度学习的fMRI自然图像重建：综述")中展示。自然图像的$n$-way分数展示在图[11](#S6.F11
    "图11 ‣ 6.2 自然图像定量比较结果 ‣ 6 各方法间公平比较 ‣ 基于深度学习的fMRI自然图像重建：综述")中。我们发现一对一的结果不适合跨方法比较。因此，我们在补充材料中展示了一对一比较。方法的定量评估首先基于低层次的MSE、PCC和SSIM指标进行，然后使用高层次的PSM指标进行比较。
- en: 'Table 4: Pairwise evaluation across the methods on natural images from the
    DIR dataset. The best results are presented in bold. ↑ indicates the higher the
    better.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在DIR数据集上的各方法成对评估结果。最佳结果以粗体显示。↑ 表示越高越好。
- en: '| Subject | Method | MSE ↑ | PCC↑ | SSIM ↑ | PSM ↑ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 方法 | MSE ↑ | PCC ↑ | SSIM ↑ | PSM ↑ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| S1 | ShenDNN | 75.80 | 80.69 | 75.59 | 77.67 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| S1 | ShenDNN | 75.80 | 80.69 | 75.59 | 77.67 |'
- en: '| ShenDNN+DGN | 74.53 | 78.98 | 61.27 | 86.61 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| ShenDNN+DGN | 74.53 | 78.98 | 61.27 | 86.61 |'
- en: '| ShenGAN | 71.67 | 79.06 | 62.08 | 92.33 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| ShenGAN | 71.67 | 79.06 | 62.08 | 92.33 |'
- en: '| BeliyEncDec | 76.94 | 86.08 | 59.67 | 73.14 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| BeliyEncDec | 76.94 | 86.08 | 59.67 | 73.14 |'
- en: '| FangSSGAN | 67.71 | 67.18 | 60.37 | 76.12 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| FangSSGAN | 67.71 | 67.18 | 60.37 | 76.12 |'
- en: '| S2 | ShenDNN | 74.98 | 77.27 | 70.82 | 77.14 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| S2 | ShenDNN | 74.98 | 77.27 | 70.82 | 77.14 |'
- en: '| ShenDNN+DGN | 70.78 | 75.43 | 59.55 | 86.41 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| ShenDNN+DGN | 70.78 | 75.43 | 59.55 | 86.41 |'
- en: '| ShenGAN | 68.65 | 74.20 | 59.51 | 90.41 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| ShenGAN | 68.65 | 74.20 | 59.51 | 90.41 |'
- en: '| BeliyEncDec | 71.18 | 76.20 | 58.94 | 75.22 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| BeliyEncDec | 71.18 | 76.20 | 58.94 | 75.22 |'
- en: '| FangSSGAN | 64.00 | 66.24 | 58.69 | 73.71 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| FangSSGAN | 64.00 | 66.24 | 58.69 | 73.71 |'
- en: '| S3 | ShenDNN | 79.71 | 81.47 | 75.06 | 76.20 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| S3 | ShenDNN | 79.71 | 81.47 | 75.06 | 76.20 |'
- en: '| ShenDNN+DGN | 73.59 | 75.02 | 60.24 | 86.20 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| ShenDNN+DGN | 73.59 | 75.02 | 60.24 | 86.20 |'
- en: '| ShenGAN | 74.12 | 78.98 | 62.08 | 91.88 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| ShenGAN | 74.12 | 78.98 | 62.08 | 91.88 |'
- en: '| BeliyEncDec | 78.61 | 81.47 | 60.53 | 76.20 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| BeliyEncDec | 78.61 | 81.47 | 60.53 | 76.20 |'
- en: '| FangSSGAN | 67.88 | 66.45 | 59.96 | 79.02 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| FangSSGAN | 67.88 | 66.45 | 59.96 | 79.02 |'
- en: '| Average result | ShenDNN | 76.83$\pm$2.53 | 79.81$\pm$2.24 | 73.82$\pm$2.62
    | 77.01$\pm$0.74 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 平均结果 | ShenDNN | 76.83$\pm$2.53 | 79.81$\pm$2.24 | 73.82$\pm$2.62 | 77.01$\pm$0.74
    |'
- en: '| ShenDNN+DGN | 72.97$\pm$1.95 | 76.48$\pm$2.18 | 60.35$\pm$0.86 | 86.41$\pm$0.20
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| ShenDNN+DGN | 72.97$\pm$1.95 | 76.48$\pm$2.18 | 60.35$\pm$0.86 | 86.41$\pm$0.20
    |'
- en: '| ShenGAN | 71.48$\pm$2.74 | 77.41$\pm$2.78 | 61.22$\pm$1.48 | 91.54$\pm$1.00
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| ShenGAN | 71.48$\pm$2.74 | 77.41$\pm$2.78 | 61.22$\pm$1.48 | 91.54$\pm$1.00
    |'
- en: '| BeliyEncDec | 75.58$\pm$3.90 | 81.25$\pm$4.94 | 59.71$\pm$0.80 | 74.86$\pm$1.56
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| BeliyEncDec | 75.58$\pm$3.90 | 81.25$\pm$4.94 | 59.71$\pm$0.80 | 74.86$\pm$1.56
    |'
- en: '| FangSSGAN | 66.53$\pm$2.19 | 66.63$\pm$0.49 | 59.67$\pm$0.87 | 76.29$\pm$2.66
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| FangSSGAN | 66.53$\pm$2.19 | 66.63$\pm$0.49 | 59.67$\pm$0.87 | 76.29$\pm$2.66
    |'
- en: 'Performance using low-level metrics. Based on the average results across the
    subjects shown in Table [4](#S6.T4 "Table 4 ‣ 6.2 Quantitative comparison results
    on natural images from DIR ‣ 6 Fair comparison across the methods ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey") and Figure [11](#S6.F11
    "Figure 11 ‣ 6.2 Quantitative comparison results on natural images from DIR ‣
    6 Fair comparison across the methods ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey") A, B, C, two nonGAN methods lead on low-level
    metrics, namely ShenDNN and BeliyEncDec. Together, they outperform other baselines
    across three low-level pairwise metrics (i.e., pairwise MSE, pairwise PCC, and
    pairwise SSIM) as well as across $n$-way MSE and PPC metrics. The high performance
    of BeliyEncDec on low-level metrics can be attributed to efficient low-level feature
    extraction via encoder–decoder architecture and to the self-supervised training
    procedure with the extended set of unlabeled images and fMRI data. The high performance
    of ShenDNN on low-level metrics is potentially due to iterative pixel-level optimization
    of the reconstructed image.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '使用低级指标的性能。基于表格[4](#S6.T4 "Table 4 ‣ 6.2 Quantitative comparison results on
    natural images from DIR ‣ 6 Fair comparison across the methods ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey")和图[11](#S6.F11 "Figure
    11 ‣ 6.2 Quantitative comparison results on natural images from DIR ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey")中的平均结果，A、B、C两个非GAN方法在低级指标上领先，即ShenDNN和BeliyEncDec。它们在三个低级成对指标（即成对MSE、成对PCC和成对SSIM）以及在$n$-way
    MSE和PPC指标上超越了其他基准。BeliyEncDec在低级指标上的高性能归因于通过编码器-解码器架构进行的高效低级特征提取，以及通过扩展的无标签图像和fMRI数据进行的自监督训练程序。ShenDNN在低级指标上的高性能可能归因于对重建图像的迭代像素级优化。'
- en: 'Performance using the high-level PSM metric. Additionally, we compare the selected
    methods on the PSM implemented using AlexNet. From Table [4](#S6.T4 "Table 4 ‣
    6.2 Quantitative comparison results on natural images from DIR ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") and Figure [11](#S6.F11 "Figure 11 ‣ 6.2 Quantitative comparison results
    on natural images from DIR ‣ 6 Fair comparison across the methods ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey") D, we can see that ShenGAN
    performs the best on the high-level PSM metric, computed in a pairwise, and $n$-way
    manner across the subjects and on averages. Overall, GAN-based methods, including
    ShenGAN, ShenDNN+DGN, and FangSSGAN, which were reported to produce more natural-looking
    images, achieved the top three average results in most cases. This supports the
    motivation to utilize PSM for measuring high-level visual similarity in images,
    especially for GAN-based methods whose strength lies in reconstructing high-level
    visual features and more natural-looking images. We attribute the improved performance
    of the three methods to using a pretrained generator network and the superior
    performance of ShenGAN and ShenDNN+DGN to the use of multilayer DNN features for
    computing the multi-level feature loss. Notably, the performance of all metrics
    reduces as the $n$-way comparison becomes increasingly harder with an increasing
    number of samples being used in the comparison.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高级PSM指标的性能。此外，我们比较了使用AlexNet实现的PSM中选定的方法。从表格 [4](#S6.T4 "表格 4 ‣ 6.2 关于DIR中的自然图像的定量比较结果
    ‣ 6 各方法之间的公平比较 ‣ 基于深度学习的fMRI自然图像重建：综述") 和图 [11](#S6.F11 "图 11 ‣ 6.2 关于DIR中的自然图像的定量比较结果
    ‣ 6 各方法之间的公平比较 ‣ 基于深度学习的fMRI自然图像重建：综述") D 中，我们可以看到ShenGAN在高级PSM指标上的表现最佳，通过成对和$n$-方式计算跨受试者及其平均值。总体而言，包括ShenGAN、ShenDNN+DGN和FangSSGAN在内的基于GAN的方法在大多数情况下都获得了前三名的平均结果。这支持了利用PSM来衡量图像中的高级视觉相似性的动机，特别是对于那些强项在于重建高级视觉特征和更自然图像的基于GAN的方法。我们将这三种方法性能的提升归因于使用了预训练的生成网络，而ShenGAN和ShenDNN+DGN的卓越性能则归因于多层DNN特征用于计算多级特征损失。值得注意的是，随着$n$-way比较变得越来越困难，使用的样本数量增加，所有指标的性能均有所下降。
- en: 7 Discussion
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: Even with a relatively small number of the available open-source reconstruction
    frameworks, the visual and quantitative results presented in this work can give
    a general idea of which architectural solution, benchmark dataset, or evaluation
    framework can be chosen for experimental purposes.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有相对较少的开源重建框架，这项工作中呈现的视觉和定量结果也可以大致了解哪种架构解决方案、基准数据集或评估框架可以用于实验目的。
- en: Depending on the target of the reconstruction task, it is vital to consider
    the trade-off between the “naturalness” and the fidelity of the reconstruction.
    Generative methods rely on GAN or VAE-GAN-based architectures to produce the most
    natural-looking images and correspondingly higher PSM scores. However, they often
    require either external data for training or the use of pretrained network components.
    The availability of external image datasets for training becomes a significant
    factor for generating high-quality images for GAN. Most importantly, the methods
    that perform best at “naturalness” do not guarantee that the object categories
    of reconstruction will always match those of the original images, as in the case
    of MozafariBigBiGAN. Other non-generative methods developed for natural image
    reconstruction, such as BeliyEncDec or ShenDNN, do not produce realistic-looking
    images. However, whenever the fidelity of the reconstructions is preferable, these
    non-generative methods should be considered, as they exhibit closer similarity
    to the original images in terms of low-level features, which are supported both
    visually and quantitatively.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 根据重建任务的目标，考虑“自然性”和重建的逼真度之间的权衡是至关重要的。生成方法依赖于基于GAN或VAE-GAN的架构来生成最自然的图像，并相应地获得更高的PSM评分。然而，它们通常需要外部数据进行训练或使用预训练的网络组件。外部图像数据集的可用性成为生成高质量图像的一个重要因素。最重要的是，表现最佳的“自然性”方法并不能保证重建的对象类别总是与原始图像的类别匹配，例如MozafariBigBiGAN的情况。其他为自然图像重建开发的非生成方法，如BeliyEncDec或ShenDNN，不能生成逼真的图像。然而，每当重建的逼真度更为重要时，应考虑这些非生成方法，因为它们在低级特征上与原始图像表现出更接近的相似性，这一点在视觉和定量上都有所支持。
- en: In this work we advocate the fairness in reconstruction evaluation procedure
    and discuss several criteria which should be standardized across the methods.
    At the same time, we believe that the evaluation procedure presented in this work
    can be further improved in the following ways.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们倡导重建评估过程的公平性，并讨论了应在方法之间标准化的若干标准。同时，我们相信这项工作中提出的评估过程可以通过以下方式进一步改进。
- en: Availability of large-scale imaging data. The primary challenge for current
    deep learning-based methods is that they are required to resolve the limitation
    of small-size fMRI data. Nowadays, the lack of training data is compensated by
    pretraining DNN components on external image data (Shen et al., [2019a](#bib.bib59),
    [b](#bib.bib60); Beliy et al., [2019](#bib.bib2)), self-supervision on additional
    image-fMRI pairs (Beliy et al., [2019](#bib.bib2); Gaziv et al., [2020](#bib.bib14))
    and generation of new surrogate fMRI via pretrained encoding models (St-Yves and
    Naselaris, [2018](#bib.bib62)). Several brain imaging datasets are available for
    reconstruction tasks. However, larger scale datasets are still required. The availability
    of large-scale imaging data may improve current state-of-the-art results and foster
    research on reconstructing more complex and dynamic visual perception, including
    imagined images or videos. This, in turn, may lead to broader adoption of the
    proposed frameworks for real-world purposes.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模成像数据的可用性。当前基于深度学习的方法面临的主要挑战是必须解决小规模fMRI数据的限制。目前，缺乏训练数据通过在外部图像数据上预训练DNN组件（Shen等，[2019a](#bib.bib59)，[b](#bib.bib60)；Beliy等，[2019](#bib.bib2)）、在附加图像-fMRI对上进行自监督（Beliy等，[2019](#bib.bib2)；Gaziv等，[2020](#bib.bib14)）以及通过预训练编码模型生成新的替代fMRI（St-Yves和Naselaris，[2018](#bib.bib62)）得以补偿。虽然已经有几个脑成像数据集可用于重建任务，但仍然需要更大规模的数据集。大规模成像数据的可用性可能提高当前的最先进结果，并促进对重建更复杂和动态的视觉感知的研究，包括想象中的图像或视频。这反过来可能会导致提议框架在实际应用中的更广泛采用。
- en: Developing new computational similarity metrics corresponding to human vision.
    While some of the deep learning methods achieve encouraging results on high-level
    perceptual similarity metrics, an open question about the correspondence of these
    computer-based metrics to human vision remains. Because most accuracy evaluation
    metrics are oriented toward computer vision tasks, they often fail to capture
    the characteristics of human vision. Research in this direction might further
    advance natural image reconstruction by developing more advanced learning and
    evaluation metrics.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 开发与人类视觉相对应的新计算相似性度量指标。虽然一些深度学习方法在高层次感知相似性度量上取得了令人鼓舞的结果，但这些计算机基础度量与人类视觉的对应关系仍然是一个未解之谜。由于大多数准确度评估指标针对计算机视觉任务，因此通常无法捕捉人类视觉的特征。朝这个方向的研究可能通过开发更先进的学习和评估指标来进一步推动自然图像重建。
- en: 8 Conclusion
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This paper presented an overview of state-of-the-art methods for natural image
    reconstruction task using deep learning. These methods were compared on multiple
    scales, including architectural design, benchmark datasets, and evaluation metrics.
    We highlighted several ambiguities with the existing evaluation and presented
    a standardized empirical assessment of the methods. This evaluation procedure
    can help researchers in performing a more comprehensive comparative analysis and
    elucidating the reason for the effectiveness of their method. We hope this study
    will serve as a foundation for future research on natural image reconstruction
    targeting fair and rigorous comparisons.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 本文概述了使用深度学习进行自然图像重建任务的最先进方法。这些方法在多个层面上进行了比较，包括架构设计、基准数据集和评估指标。我们突出了现有评估中的若干模糊点，并提出了方法的标准化实证评估。这一评估过程可以帮助研究人员进行更全面的比较分析，并阐明其方法的有效性。我们希望这项研究能为未来自然图像重建研究奠定基础，旨在实现公平和严格的比较。
- en: Author Contributions
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者贡献
- en: 'ZR: Conceptualization, Methodology, Software, Writing, Evaluation. QJ: Software,
    Evaluation, Writing - review and editing. XL: Conceptualization, Data curation,
    Writing - Original draft preparation. TM: Supervision, Writing - review & editing.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 'ZR: 概念化、方法论、软件、撰写、评估。QJ: 软件、评估、撰写 - 评审和编辑。XL: 概念化、数据整理、撰写 - 原始草稿准备。TM: 监督、撰写
    - 评审与编辑。'
- en: Funding
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资金
- en: This work was partly supported by JST CREST (Grant Number JPMJCR1687), JSPS
    Grant-in-Aid for Scientific Research (Grant Number 21K12042, 17H01785), and the
    New Energy and Industrial Technology Development Organization (Grant Number JPNP20006).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了JST CREST（资助号JPMJCR1687）、JSPS科研资助（资助号21K12042, 17H01785）以及新能源与工业技术开发组织（资助号JPNP20006）的支持。
- en: Acknowledgments
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Professor Yukiyasu Kamitani from Neuroinformatics Lab at Kyoto University
    for providing valuable comments that improved the manuscript. We also thank Katja
    Seeliger, Milad Mozafari, Guy Gaziv, Roman Beliy, and Tao Fang for sharing their
    reconstructed images and evaluation codes with us.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢京都大学神经信息学实验室的Yukiyasu Kamitani教授提供了宝贵的意见，改善了手稿。我们还感谢Katja Seeliger、Milad
    Mozafari、Guy Gaziv、Roman Beliy和Tao Fang与我们分享了他们的重建图像和评估代码。
- en: References
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bandettini (2012) Bandettini, P. A. (2012). Twenty years of functional MRI:
    The science and the stories. *NeuroImage* 62, 575–588. [10.1016/j.neuroimage.2012.04.026](https:/doi.org/10.1016/j.neuroimage.2012.04.026).
    Number: 2'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bandettini (2012) Bandettini, P. A. (2012). 二十年的功能性MRI：科学与故事。*NeuroImage* 62,
    575–588。 [10.1016/j.neuroimage.2012.04.026](https:/doi.org/10.1016/j.neuroimage.2012.04.026)。编号:
    2'
- en: 'Beliy et al. (2019) Beliy, R., Gaziv, G., Hoogi, A., Strappini, F., Golan,
    T., and Irani, M. (2019). From voxels to pixels and back: Self-supervision in
    natural-image reconstruction from fMRI. In *Advances in Neural Information Processing
    Systems 32*, eds. H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alché-Buc,
    E. Fox, and R. Garnett (Curran Associates, Inc.). 6517–6527'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beliy et al. (2019) Beliy, R., Gaziv, G., Hoogi, A., Strappini, F., Golan,
    T., and Irani, M. (2019). 从体素到像素再到体素：自然图像重建中的自监督。在*神经信息处理系统进展32*，主编: H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d. Alché-Buc, E. Fox, 和 R. Garnett（Curran Associates,
    Inc.）。6517–6527'
- en: Chen et al. (2015) Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and
    Yuille, A. L. (2015). Semantic Image Segmentation with Deep Convolutional Nets
    and Fully Connected CRFs. In *3rd International Conference on Learning Representations,
    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*, eds.
    Y. Bengio and Y. LeCun
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2015) Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and
    Yuille, A. L. (2015). 使用深度卷积网络和完全连接的CRF进行语义图像分割。在*第三届国际学习表征会议，ICLR 2015，圣地亚哥，加州，美国，2015年5月7-9日，会议论文集*，主编:
    Y. Bengio 和 Y. LeCun'
- en: 'Chen et al. (2014) Chen, M., Han, J., Hu, X., Jiang, X., Guo, L., and Liu,
    T. (2014). Survey of encoding and decoding of visual stimulus via FMRI: an image
    analysis perspective. *Brain Imaging and Behavior* 8, 7–23. [10.1007/s11682-013-9238-z](https:/doi.org/10.1007/s11682-013-9238-z)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2014) Chen, M., Han, J., Hu, X., Jiang, X., Guo, L., and Liu, T.
    (2014). 通过FMRI对视觉刺激的编码和解码的综述：图像分析视角。*脑成像与行为* 8, 7–23。 [10.1007/s11682-013-9238-z](https:/doi.org/10.1007/s11682-013-9238-z)
- en: 'Cho et al. (2014) Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D.,
    Bougares, F., Schwenk, H., et al. (2014). Learning Phrase Representations using
    RNN Encoder–Decoder for Statistical Machine Translation. In *Proceedings of the
    2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (Doha,
    Qatar: Association for Computational Linguistics), 1724–1734. [10.3115/v1/D14-1179](https:/doi.org/10.3115/v1/D14-1179)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cho et al. (2014) Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D.,
    Bougares, F., Schwenk, H., et al. (2014). 使用RNN编码器-解码器进行统计机器翻译的短语表示学习。在*2014年自然语言处理经验方法会议（EMNLP）论文集*（多哈，卡塔尔:
    计算语言学协会），1724–1734。 [10.3115/v1/D14-1179](https:/doi.org/10.3115/v1/D14-1179)'
- en: 'Chrabaszcz et al. (2017) Chrabaszcz, P., Loshchilov, I., and Hutter, F. (2017).
    A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets. *arXiv:1707.08819
    [cs]* ArXiv: 1707.08819'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chrabaszcz et al. (2017) Chrabaszcz, P., Loshchilov, I., and Hutter, F. (2017).
    ImageNet的下采样变体作为CIFAR数据集的替代。*arXiv:1707.08819 [cs]* ArXiv: 1707.08819'
- en: 'Corel Corporation (1994) [Dataset] Corel Corporation (1994). Corel stock photo
    library. ISBN: 9780969660552 Place: Ottawa OCLC: 872594087'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Corel Corporation (1994) [数据集] Corel Corporation (1994). Corel图库照片库。ISBN: 9780969660552
    地点: 渥太华 OCLC: 872594087'
- en: 'Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
    L. (2009). ImageNet: A large-scale hierarchical image database. In *2009 IEEE
    Conference on Computer Vision and Pattern Recognition*. 248–255. [10.1109/CVPR.2009.5206848](https:/doi.org/10.1109/CVPR.2009.5206848).
    ISSN: 1063-6919'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
    L. (2009). ImageNet: 一个大规模层次图像数据库。在*2009年IEEE计算机视觉与模式识别会议*。248–255。 [10.1109/CVPR.2009.5206848](https:/doi.org/10.1109/CVPR.2009.5206848)。ISSN:
    1063-6919'
- en: Donahue and Simonyan (2019) Donahue, J. and Simonyan, K. (2019). Large Scale
    Adversarial Representation Learning. In *Advances in Neural Information Processing
    Systems*, eds. H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alché-Buc, E. Fox,
    and R. Garnett (Curran Associates, Inc.), vol. 32
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donahue 和 Simonyan（2019）Donahue, J. 和 Simonyan, K.（2019）。大规模对抗表示学习。在*神经信息处理系统进展*中，主编：H.
    Wallach, H. Larochelle, A. Beygelzimer, F. d. Alché-Buc, E. Fox, 和 R. Garnett（Curran
    Associates, Inc.），第32卷
- en: Dosovitskiy and Brox (2016) Dosovitskiy, A. and Brox, T. (2016). Inverting Visual
    Representations With Convolutional Networks. 4829–4837
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 和 Brox（2016）Dosovitskiy, A. 和 Brox, T.（2016）。用卷积网络反转视觉表示。4829–4837
- en: 'Eickenberg et al. (2017) Eickenberg, M., Gramfort, A., Varoquaux, G., and Thirion,
    B. (2017). Seeing it all: Convolutional network layers map the function of the
    human visual system. *NeuroImage* 152, 184–194. [10.1016/j.neuroimage.2016.10.001](https:/doi.org/10.1016/j.neuroimage.2016.10.001)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eickenberg 等（2017）Eickenberg, M., Gramfort, A., Varoquaux, G., 和 Thirion, B.（2017）。全面观察：卷积网络层映射人类视觉系统的功能。*神经影像*
    152, 184–194。 [10.1016/j.neuroimage.2016.10.001](https:/doi.org/10.1016/j.neuroimage.2016.10.001)
- en: Fang et al. (2020) Fang, T., Qi, Y., and Pan, G. (2020). Reconstructing Perceptive
    Images from Brain Activity by Shape-Semantic GAN. *Advances in Neural Information
    Processing Systems* 33
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等（2020）Fang, T., Qi, Y., 和 Pan, G.（2020）。通过形状语义生成对抗网络重建感知图像。*神经信息处理系统进展*
    33
- en: 'Fujiwara et al. (2013) Fujiwara, Y., Miyawaki, Y., and Kamitani, Y. (2013).
    Modular encoding and decoding models derived from bayesian canonical correlation
    analysis. *Neural Computation* 25, 979–1005. [10.1162/NECO_a_00423](https:/doi.org/10.1162/NECO_a_00423).
    Number: 4'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fujiwara 等（2013）Fujiwara, Y., Miyawaki, Y., 和 Kamitani, Y.（2013）。从贝叶斯典型相关分析推导出的模块化编码和解码模型。*神经计算*
    25, 979–1005。 [10.1162/NECO_a_00423](https:/doi.org/10.1162/NECO_a_00423)。编号：4
- en: Gaziv et al. (2020) Gaziv, G., Beliy, R., Granot, N., Hoogi, A., Strappini,
    F., Golan, T., et al. (2020). *Self-Supervised Natural Image Reconstruction and
    Rich Semantic Classification from Brain Activity*. preprint, Neuroscience. [10.1101/2020.09.06.284794](https:/doi.org/10.1101/2020.09.06.284794)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaziv 等（2020）Gaziv, G., Beliy, R., Granot, N., Hoogi, A., Strappini, F., Golan,
    T., 等（2020）。*自监督自然图像重建与来自脑活动的丰富语义分类*。预印本，神经科学。 [10.1101/2020.09.06.284794](https:/doi.org/10.1101/2020.09.06.284794)
- en: 'Goodfellow et al. (2014) Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., et al. (2014). Generative adversarial nets. In
    *Proceedings of the 27th International Conference on Neural Information Processing
    Systems - Volume 2* (Cambridge, MA, USA: MIT Press), NIPS’14, 2672–2680'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2014）Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
    D., Ozair, S., 等（2014）。生成对抗网络。在*第27届国际神经信息处理系统会议论文集 - 第二卷*（美国，麻省剑桥：麻省理工学院出版社），NIPS’14,
    2672–2680
- en: Güçlütürk et al. (2017) Güçlütürk, Y., Güçlü, U., Seeliger, K., Bosch, S., van
    Lier, R., and van Gerven, M. A. (2017). Reconstructing perceived faces from brain
    activations with deep adversarial neural decoding. *Advances in Neural Information
    Processing Systems* 30, 4246–4257
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Güçlütürk 等（2017）Güçlütürk, Y., Güçlü, U., Seeliger, K., Bosch, S., van Lier,
    R., 和 van Gerven, M. A.（2017）。通过深度对抗神经解码重建感知面孔。*神经信息处理系统进展* 30, 4246–4257
- en: 'Haxby et al. (2001) Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A.,
    Schouten, J. L., and Pietrini, P. (2001). Distributed and Overlapping Representations
    of Faces and Objects in Ventral Temporal Cortex. *Science* 293, 2425–2430. [10.1126/science.1063736](https:/doi.org/10.1126/science.1063736).
    Number: 5539 Publisher: American Association for the Advancement of Science Section:
    Research Article'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haxby 等（2001）Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten,
    J. L., 和 Pietrini, P.（2001）。在腹侧颞皮层中分布式和重叠的面孔与物体表示。*科学* 293, 2425–2430。 [10.1126/science.1063736](https:/doi.org/10.1126/science.1063736)。编号：5539
    出版社：美国科学促进会 部门：研究文章
- en: 'Hinton et al. (2015) Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling
    the Knowledge in a Neural Network. *arXiv:1503.02531 [cs, stat]* ArXiv: 1503.02531'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hinton 等（2015）Hinton, G., Vinyals, O., 和 Dean, J.（2015）。提炼神经网络中的知识。*arXiv:1503.02531
    [cs, stat]* ArXiv: 1503.02531'
- en: 'Horikawa and Kamitani (2017) Horikawa, T. and Kamitani, Y. (2017). Generic
    decoding of seen and imagined objects using hierarchical visual features. *Nature
    Communications* 8, 15037. [10.1038/ncomms15037](https:/doi.org/10.1038/ncomms15037).
    Number: 1 Publisher: Nature Publishing Group'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horikawa 和 Kamitani（2017）Horikawa, T. 和 Kamitani, Y.（2017）。使用层次视觉特征对观察到和想象中的物体进行通用解码。*自然通讯*
    8, 15037。 [10.1038/ncomms15037](https:/doi.org/10.1038/ncomms15037)。编号：1 出版社：自然出版集团
- en: Isola et al. (2017) Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2017).
    Image-To-Image Translation With Conditional Adversarial Networks. 1125–1134
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola 等（2017）Isola, P., Zhu, J.-Y., Zhou, T., 和 Efros, A. A.（2017）。带条件对抗网络的图像到图像翻译。1125–1134
- en: Jakub Langr and Vladimir Bok (2019) Jakub Langr and Vladimir Bok (2019). *GANs
    in Action* (Manning)
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jakub Langr 和 Vladimir Bok（2019）Jakub Langr 和 Vladimir Bok（2019）。*GANs in Action*（Manning）
- en: 'Kamitani and Tong (2005) Kamitani, Y. and Tong, F. (2005). Decoding the visual
    and subjective contents of the human brain. *Nature Neuroscience* 8, 679–685.
    [10.1038/nn1444](https:/doi.org/10.1038/nn1444). Number: 5'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kamitani 和 Tong（2005）Kamitani, Y. 和 Tong, F.（2005）。解码人脑的视觉和主观内容。*自然神经科学* 8,
    679–685。[10.1038/nn1444](https:/doi.org/10.1038/nn1444)。编号: 5'
- en: Kamnitsas et al. (2017) Kamnitsas, K., Ledig, C., Newcombe, V. F. J., Simpson,
    J. P., Kane, A. D., Menon, D. K., et al. (2017). Efficient multi-scale 3D CNN
    with fully connected CRF for accurate brain lesion segmentation. *Medical Image
    Analysis* 36, 61–78. [10.1016/j.media.2016.10.004](https:/doi.org/10.1016/j.media.2016.10.004)
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamnitsas 等（2017）Kamnitsas, K., Ledig, C., Newcombe, V. F. J., Simpson, J. P.,
    Kane, A. D., Menon, D. K., 等（2017）。用于准确脑损伤分割的高效多尺度3D CNN与全连接CRF。*医学影像分析* 36, 61–78。[10.1016/j.media.2016.10.004](https:/doi.org/10.1016/j.media.2016.10.004)
- en: 'Kay et al. (2008) Kay, K. N., Naselaris, T., Prenger, R. J., and Gallant, J. L.
    (2008). Identifying natural images from human brain activity. *Nature* 452, 352–355.
    [10.1038/nature06713](https:/doi.org/10.1038/nature06713). Number: 7185'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kay 等（2008）Kay, K. N., Naselaris, T., Prenger, R. J., 和 Gallant, J. L.（2008）。从人脑活动中识别自然图像。*自然*
    452, 352–355。[10.1038/nature06713](https:/doi.org/10.1038/nature06713)。编号: 7185'
- en: 'Kingma and Welling (2014) Kingma, D. P. and Welling, M. (2014). Auto-Encoding
    Variational Bayes. *arXiv:1312.6114 [cs, stat]* ArXiv: 1312.6114'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma 和 Welling（2014）Kingma, D. P. 和 Welling, M.（2014）。自编码变分贝叶斯。*arXiv:1312.6114
    [cs, stat]* ArXiv: 1312.6114'
- en: 'Kriegeskorte (2015) Kriegeskorte, N. (2015). Deep Neural Networks: A New Framework
    for Modeling Biological Vision and Brain Information Processing. *Annual Review
    of Vision Science* 1, 417–446. [10.1146/annurev-vision-082114-035447](https:/doi.org/10.1146/annurev-vision-082114-035447)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kriegeskorte（2015）Kriegeskorte, N.（2015）。深度神经网络：建模生物视觉和大脑信息处理的新框架。*视觉科学年鉴* 1,
    417–446。[10.1146/annurev-vision-082114-035447](https:/doi.org/10.1146/annurev-vision-082114-035447)
- en: Krizhevsky (2009) Krizhevsky, A. (2009). *Learning multiple layers of features
    from tiny images*. Tech. rep.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky（2009）Krizhevsky, A.（2009）。*从微小图像中学习多个特征层*。技术报告
- en: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012).
    ImageNet Classification with Deep Convolutional Neural Networks. *Advances in
    Neural Information Processing Systems* 25, 1097–1105
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2012）Krizhevsky, A., Sutskever, I., 和 Hinton, G. E.（2012）。使用深度卷积神经网络进行ImageNet分类。*神经信息处理系统进展*
    25, 1097–1105
- en: 'Larsen et al. (2016) Larsen, A. B. L., Sønderby, S. K., Larochelle, H., and
    Winther, O. (2016). Autoencoding beyond pixels using a learned similarity metric.
    In *International Conference on Machine Learning* (PMLR), 1558–1566. ISSN: 1938-7228'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Larsen 等（2016）Larsen, A. B. L., Sønderby, S. K., Larochelle, H., 和 Winther,
    O.（2016）。利用学习到的相似度度量进行像素之外的自动编码。在 *国际机器学习会议*（PMLR），1558–1566。ISSN: 1938-7228'
- en: 'LeCun et al. (1989) LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard,
    R. E., Hubbard, W., et al. (1989). Backpropagation Applied to Handwritten Zip
    Code Recognition. *Neural Computation* 1, 541–551. [10.1162/neco.1989.1.4.541](https:/doi.org/10.1162/neco.1989.1.4.541).
    Number: 4 Publisher: MIT Press'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeCun 等（1989）LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R.
    E., Hubbard, W., 等（1989）。反向传播应用于手写邮政编码识别。*神经计算* 1, 541–551。[10.1162/neco.1989.1.4.541](https:/doi.org/10.1162/neco.1989.1.4.541)。编号:
    4 出版社: MIT Press'
- en: 'Lin et al. (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., et al. (2014). Microsoft COCO: Common Objects in Context. In *Computer
    Vision – ECCV 2014*, eds. D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars (Cham:
    Springer International Publishing), Lecture Notes in Computer Science, 740–755.
    [10.1007/978-3-319-10602-1_48](https:/doi.org/10.1007/978-3-319-10602-1_48)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等（2014）Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan,
    D., 等（2014）。Microsoft COCO: 语境中的常见对象。在 *计算机视觉 – ECCV 2014*，编者 D. Fleet, T. Pajdla,
    B. Schiele 和 T. Tuytelaars（Cham: Springer International Publishing），计算机科学讲义，740–755。[10.1007/978-3-319-10602-1_48](https:/doi.org/10.1007/978-3-319-10602-1_48)'
- en: 'Liu et al. (2015) Liu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep Learning
    Face Attributes in the Wild. In *2015 IEEE International Conference on Computer
    Vision (ICCV)*. 3730–3738. [10.1109/ICCV.2015.425](https:/doi.org/10.1109/ICCV.2015.425).
    ISSN: 2380-7504'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2015) Liu, Z., Luo, P., Wang, X., 和 Tang, X. (2015). 在野外深度学习面部属性。载于
    *2015 IEEE 国际计算机视觉会议 (ICCV)*。3730–3738。 [10.1109/ICCV.2015.425](https:/doi.org/10.1109/ICCV.2015.425)。ISSN:
    2380-7504'
- en: 'Logothetis and Sheinberg (1996) Logothetis, N. K. and Sheinberg, D. L. (1996).
    Visual Object Recognition. *Annual Review of Neuroscience* 19, 577–621. [10.1146/annurev.ne.19.030196.003045](https:/doi.org/10.1146/annurev.ne.19.030196.003045).
    Number: 1 _eprint: https://doi.org/10.1146/annurev.ne.19.030196.003045'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Logothetis 和 Sheinberg (1996) Logothetis, N. K. 和 Sheinberg, D. L. (1996).
    视觉物体识别。*神经科学年鉴* 19, 577–621。 [10.1146/annurev.ne.19.030196.003045](https:/doi.org/10.1146/annurev.ne.19.030196.003045)。编号:
    1 _eprint: https://doi.org/10.1146/annurev.ne.19.030196.003045'
- en: Maaten (2009) Maaten, L. V. D. (2009). A New Benchmark Dataset for Handwritten
    Character Recognition (Tilburg University, Tilburg, The Netherlands)
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maaten (2009) Maaten, L. V. D. (2009). 手写字符识别的新基准数据集 (蒂尔堡大学, 荷兰蒂尔堡)
- en: 'Mahendran and Vedaldi (2015) Mahendran, A. and Vedaldi, A. (2015). Understanding
    deep image representations by inverting them. In *2015 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*. 5188–5196. [10.1109/CVPR.2015.7299155](https:/doi.org/10.1109/CVPR.2015.7299155).
    ISSN: 1063-6919'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mahendran 和 Vedaldi (2015) Mahendran, A. 和 Vedaldi, A. (2015). 通过反转深度图像表示来理解它们。载于
    *2015 IEEE 计算机视觉与模式识别会议 (CVPR)*。5188–5196。 [10.1109/CVPR.2015.7299155](https:/doi.org/10.1109/CVPR.2015.7299155)。ISSN:
    1063-6919'
- en: Martin et al. (2001) Martin, D., Fowlkes, C., Tal, D., and Malik, J. (2001).
    A database of human segmented natural images and its application to evaluating
    segmentation algorithms and measuring ecological statistics. In *Proceedings Eighth
    IEEE International Conference on Computer Vision. ICCV 2001*. vol. 2, 416–423
    vol.2. [10.1109/ICCV.2001.937655](https:/doi.org/10.1109/ICCV.2001.937655)
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martin 等 (2001) Martin, D., Fowlkes, C., Tal, D., 和 Malik, J. (2001). 人工分割自然图像数据库及其在评估分割算法和测量生态统计中的应用。载于
    *第八届 IEEE 国际计算机视觉会议. ICCV 2001*。第2卷, 416–423。 [10.1109/ICCV.2001.937655](https:/doi.org/10.1109/ICCV.2001.937655)
- en: 'Minaee et al. (2021) Minaee, S., Boykov, Y. Y., Porikli, F., Plaza, A. J.,
    Kehtarnavaz, N., and Terzopoulos, D. (2021). Image Segmentation Using Deep Learning:
    A Survey. *IEEE Transactions on Pattern Analysis and Machine Intelligence* , 1–1[10.1109/TPAMI.2021.3059968](https:/doi.org/10.1109/TPAMI.2021.3059968).
    Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Minaee 等 (2021) Minaee, S., Boykov, Y. Y., Porikli, F., Plaza, A. J., Kehtarnavaz,
    N., 和 Terzopoulos, D. (2021). 使用深度学习的图像分割: 一项综述。*IEEE 模式分析与机器智能学报* , 1–1 [10.1109/TPAMI.2021.3059968](https:/doi.org/10.1109/TPAMI.2021.3059968)。会议名称:
    IEEE 模式分析与机器智能学报'
- en: 'Miyawaki et al. (2008) Miyawaki, Y., Uchida, H., Yamashita, O., Sato, M.-a.,
    Morito, Y., Tanabe, H. C., et al. (2008). Visual Image Reconstruction from Human
    Brain Activity using a Combination of Multiscale Local Image Decoders. *Neuron*
    60, 915–929. [10.1016/j.neuron.2008.11.004](https:/doi.org/10.1016/j.neuron.2008.11.004).
    Number: 5'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miyawaki 等 (2008) Miyawaki, Y., Uchida, H., Yamashita, O., Sato, M.-a., Morito,
    Y., Tanabe, H. C., 等 (2008). 利用多尺度局部图像解码器从人脑活动中重建视觉图像。*神经元* 60, 915–929。 [10.1016/j.neuron.2008.11.004](https:/doi.org/10.1016/j.neuron.2008.11.004)。编号:
    5'
- en: 'Mozafari et al. (2020) Mozafari, M., Reddy, L., and VanRullen, R. (2020). Reconstructing
    Natural Scenes from fMRI Patterns using BigBiGAN. In *2020 International Joint
    Conference on Neural Networks (IJCNN)* (Glasgow, United Kingdom: IEEE), 1–8. [10.1109/IJCNN48605.2020.9206960](https:/doi.org/10.1109/IJCNN48605.2020.9206960)'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mozafari 等 (2020) Mozafari, M., Reddy, L., 和 VanRullen, R. (2020). 使用 BigBiGAN
    从 fMRI 模式重建自然场景。载于 *2020 国际联合神经网络会议 (IJCNN)* (英国格拉斯哥: IEEE), 1–8。 [10.1109/IJCNN48605.2020.9206960](https:/doi.org/10.1109/IJCNN48605.2020.9206960)'
- en: 'Naselaris et al. (2011) Naselaris, T., Kay, K. N., Nishimoto, S., and Gallant,
    J. L. (2011). Encoding and decoding in fMRI. *NeuroImage* 56, 400–410. [10.1016/j.neuroimage.2010.07.073](https:/doi.org/10.1016/j.neuroimage.2010.07.073).
    Number: 2'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Naselaris 等 (2011) Naselaris, T., Kay, K. N., Nishimoto, S., 和 Gallant, J.
    L. (2011). 在 fMRI 中的编码与解码。*神经影像* 56, 400–410。 [10.1016/j.neuroimage.2010.07.073](https:/doi.org/10.1016/j.neuroimage.2010.07.073)。编号:
    2'
- en: 'Naselaris et al. (2009) Naselaris, T., Prenger, R. J., Kay, K. N., Oliver,
    M., and Gallant, J. L. (2009). Bayesian Reconstruction of Natural Images from
    Human Brain Activity. *Neuron* 63, 902–915. [10.1016/j.neuron.2009.09.006](https:/doi.org/10.1016/j.neuron.2009.09.006).
    Number: 6'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Naselaris等（2009）Naselaris, T., Prenger, R. J., Kay, K. N., Oliver, M., 和 Gallant,
    J. L. (2009). 基于贝叶斯重建的自然图像从人脑活动中获取。*神经元* 63, 902–915. [10.1016/j.neuron.2009.09.006](https:/doi.org/10.1016/j.neuron.2009.09.006).
    编号: 6'
- en: 'Nestor et al. (2020) Nestor, A., Lee, A. C. H., Plaut, D. C., and Behrmann,
    M. (2020). The Face of Image Reconstruction: Progress, Pitfalls, Prospects. *Trends
    in Cognitive Sciences* 24, 747–759. [10.1016/j.tics.2020.06.006](https:/doi.org/10.1016/j.tics.2020.06.006).
    Publisher: Elsevier'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nestor等（2020）Nestor, A., Lee, A. C. H., Plaut, D. C., 和 Behrmann, M. (2020).
    图像重建的面貌：进展、陷阱、前景。*认知科学趋势* 24, 747–759. [10.1016/j.tics.2020.06.006](https:/doi.org/10.1016/j.tics.2020.06.006).
    出版社: 爱思唯尔'
- en: 'Ogawa et al. (1990) Ogawa, S., Lee, T. M., Kay, A. R., and Tank, D. W. (1990).
    Brain magnetic resonance imaging with contrast dependent on blood oxygenation.
    *Proceedings of the National Academy of Sciences of the United States of America*
    87, 9868–9872. Number: 24'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ogawa等（1990）Ogawa, S., Lee, T. M., Kay, A. R., 和 Tank, D. W. (1990). 依赖于血氧水平的脑部磁共振成像。*美国国家科学院院刊*
    87, 9868–9872. 编号: 24'
- en: 'Pinto et al. (2009) Pinto, N., Doukhan, D., DiCarlo, J. J., and Cox, D. D.
    (2009). A High-Throughput Screening Approach to Discovering Good Forms of Biologically
    Inspired Visual Representation. *PLOS Computational Biology* 5, e1000579. [10.1371/journal.pcbi.1000579](https:/doi.org/10.1371/journal.pcbi.1000579).
    Publisher: Public Library of Science'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pinto等（2009）Pinto, N., Doukhan, D., DiCarlo, J. J., 和 Cox, D. D. (2009). 高通量筛选方法发现生物启发的视觉表征的良好形式。*PLOS计算生物学*
    5, e1000579. [10.1371/journal.pcbi.1000579](https:/doi.org/10.1371/journal.pcbi.1000579).
    出版社: 公共科学图书馆'
- en: 'Poldrack and Farah (2015) Poldrack, R. A. and Farah, M. J. (2015). Progress
    and challenges in probing the human brain. *Nature* 526, 371–379. [10.1038/nature15692](https:/doi.org/10.1038/nature15692).
    Number: 7573 Publisher: Nature Publishing Group'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Poldrack和Farah（2015）Poldrack, R. A. 和 Farah, M. J. (2015). 探索人脑的进展与挑战。*自然*
    526, 371–379. [10.1038/nature15692](https:/doi.org/10.1038/nature15692). 编号: 7573
    出版社: 自然出版集团'
- en: Qiao et al. (2020) Qiao, K., Chen, J., Wang, L., Zhang, C., Tong, L., and Yan,
    B. (2020). BigGAN-based Bayesian Reconstruction of Natural Images from Human Brain
    Activity. *Neuroscience* 444, 92–105. [10.1016/j.neuroscience.2020.07.040](https:/doi.org/10.1016/j.neuroscience.2020.07.040)
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao等（2020）Qiao, K., Chen, J., Wang, L., Zhang, C., Tong, L., 和 Yan, B. (2020).
    基于BigGAN的贝叶斯重建自然图像从人脑活动中获取。*神经科学* 444, 92–105. [10.1016/j.neuroscience.2020.07.040](https:/doi.org/10.1016/j.neuroscience.2020.07.040)
- en: Qiao et al. (2018) Qiao, K., Zhang, C., Wang, L., Chen, J., Zeng, L., Tong,
    L., et al. (2018). Accurate Reconstruction of Image Stimuli From Human Functional
    Magnetic Resonance Imaging Based on the Decoding Model With Capsule Network Architecture.
    *Frontiers in Neuroinformatics* 12, 62. [10.3389/fninf.2018.00062](https:/doi.org/10.3389/fninf.2018.00062)
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao等（2018）Qiao, K., Zhang, C., Wang, L., Chen, J., Zeng, L., Tong, L., 等 (2018).
    基于胶囊网络结构的图像刺激的准确重建。*神经信息学前沿* 12, 62. [10.3389/fninf.2018.00062](https:/doi.org/10.3389/fninf.2018.00062)
- en: 'Quiroga et al. (2005) Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., and
    Fried, I. (2005). Invariant visual representation by single neurons in the human
    brain. *Nature* 435, 1102–1107. [10.1038/nature03687](https:/doi.org/10.1038/nature03687).
    Number: 7045 Publisher: Nature Publishing Group'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Quiroga等（2005）Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., 和 Fried, I.
    (2005). 人脑中单个神经元的恒定视觉表征。*自然* 435, 1102–1107. [10.1038/nature03687](https:/doi.org/10.1038/nature03687).
    编号: 7045 出版社: 自然出版集团'
- en: 'Radford et al. (2016) Radford, A., Metz, L., and Chintala, S. (2016). Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks.
    In *arXiv:1511.06434 [cs]*. ArXiv: 1511.06434'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Radford等（2016）Radford, A., Metz, L., 和 Chintala, S. (2016). 使用深度卷积生成对抗网络进行无监督表示学习。见
    *arXiv:1511.06434 [cs]*. ArXiv: 1511.06434'
- en: 'Rakhimberdina et al. (2020) Rakhimberdina, Z., Liu, X., and Murata, T. (2020).
    Population Graph-Based Multi-Model Ensemble Method for Diagnosing Autism Spectrum
    Disorder. *Sensors* 20, 6001. [10.3390/s20216001](https:/doi.org/10.3390/s20216001).
    Number: 21 Publisher: Multidisciplinary Digital Publishing Institute'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rakhimberdina等（2020）Rakhimberdina, Z., Liu, X., 和 Murata, T. (2020). 基于群体图的多模型集成方法用于诊断自闭症谱系障碍。*传感器*
    20, 6001. [10.3390/s20216001](https:/doi.org/10.3390/s20216001). 编号: 21 出版社: 多学科数字出版研究所'
- en: Ren et al. (2021) Ren, Z., Li, J., Xue, X., Li, X., Yang, F., Jiao, Z., et al.
    (2021). Reconstructing seen image from brain activity by visually-guided cognitive
    representation and adversarial learning. *NeuroImage* [10.1016/j.neuroimage.2020.117602](https:/doi.org/10.1016/j.neuroimage.2020.117602)
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人（2021）Ren, Z., Li, J., Xue, X., Li, X., Yang, F., Jiao, Z., 等人（2021）。通过视觉引导的认知表示和对抗学习从脑活动中重建视觉图像。*神经影像*
    [10.1016/j.neuroimage.2020.117602](https:/doi.org/10.1016/j.neuroimage.2020.117602)
- en: 'Roelfsema et al. (2018) Roelfsema, P. R., Denys, D., and Klink, P. C. (2018).
    Mind Reading and Writing: The Future of Neurotechnology. *Trends in Cognitive
    Sciences* 22, 598–610. [10.1016/j.tics.2018.04.001](https:/doi.org/10.1016/j.tics.2018.04.001).
    Number: 7'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roelfsema 等人（2018）Roelfsema, P. R., Denys, D., 和 Klink, P. C. (2018). 思维阅读与书写：神经技术的未来。*认知科学趋势*
    22, 598–610. [10.1016/j.tics.2018.04.001](https:/doi.org/10.1016/j.tics.2018.04.001).
    期号: 7'
- en: 'Rolls (2012) Rolls, E. T. (2012). Invariant Visual Object and Face Recognition:
    Neural and Computational Bases, and a Model, VisNet. *Frontiers in Computational
    Neuroscience* 6. [10.3389/fncom.2012.00035](https:/doi.org/10.3389/fncom.2012.00035).
    Publisher: Frontiers'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rolls（2012）Rolls, E. T. (2012). 不变视觉物体和面孔识别：神经和计算基础，以及一个模型，VisNet。*计算神经科学前沿*
    6. [10.3389/fncom.2012.00035](https:/doi.org/10.3389/fncom.2012.00035). 出版社: Frontiers'
- en: 'Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. (2015).
    U-Net: Convolutional Networks for Biomedical Image Segmentation. In *Medical Image
    Computing and Computer-Assisted Intervention – MICCAI 2015*, eds. N. Navab, J. Hornegger,
    W. M. Wells, and A. F. Frangi (Cham: Springer International Publishing), Lecture
    Notes in Computer Science, 234–241. [10.1007/978-3-319-24574-4_28](https:/doi.org/10.1007/978-3-319-24574-4_28)'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger 等人（2015）Ronneberger, O., Fischer, P., 和 Brox, T. (2015). U-Net：用于生物医学图像分割的卷积网络。在
    *医学图像计算与计算机辅助手术 – MICCAI 2015* 中，由 N. Navab, J. Hornegger, W. M. Wells 和 A. F.
    Frangi 编（Cham: Springer International Publishing），计算机科学讲义笔记，234–241. [10.1007/978-3-319-24574-4_28](https:/doi.org/10.1007/978-3-319-24574-4_28)'
- en: Schoenmakers et al. (2013) Schoenmakers, S., Barth, M., Heskes, T., and van
    Gerven, M. (2013). Linear reconstruction of perceived images from human brain
    activity. *NeuroImage* 83, 951–961. [10.1016/j.neuroimage.2013.07.043](https:/doi.org/10.1016/j.neuroimage.2013.07.043)
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schoenmakers 等人（2013）Schoenmakers, S., Barth, M., Heskes, T., 和 van Gerven,
    M. (2013). 从人脑活动中线性重建感知图像。*神经影像* 83, 951–961. [10.1016/j.neuroimage.2013.07.043](https:/doi.org/10.1016/j.neuroimage.2013.07.043)
- en: 'Schomaker et al. (2000) Schomaker, L., Vuurpijl, L., and Schomaker, L. (2000).
    *Forensic writer identification: a benchmark data set and a comparison of two
    systems* (NICI (NIjmegen Institute of Cognitive Information), Katholieke Universiteit
    Nijmegen). Type: Other'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schomaker 等人（2000）Schomaker, L., Vuurpijl, L., 和 Schomaker, L. (2000). *法医书写者识别：基准数据集及两个系统的比较*（NICI（奈梅亨认知信息研究所），奈梅亨大学）。类型:
    其他'
- en: 'Schrimpf et al. (2018) Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J.,
    Rajalingham, R., Issa, E. B., et al. (2018). Brain-Score: Which Artificial Neural
    Network for Object Recognition is most Brain-Like? *bioRxiv* , 407007[10.1101/407007](https:/doi.org/10.1101/407007).
    Publisher: Cold Spring Harbor Laboratory Section: New Results'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schrimpf 等人（2018）Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham,
    R., Issa, E. B., 等人（2018）。Brain-Score：哪个人工神经网络对于物体识别最类似于大脑？ *bioRxiv* , 407007
    [10.1101/407007](https:/doi.org/10.1101/407007). 出版社: Cold Spring Harbor Laboratory
    部分: 新结果'
- en: Seeliger et al. (2018) Seeliger, K., Güçlü, U., Ambrogioni, L., Güçlütürk, Y.,
    and van Gerven, M. A. J. (2018). Generative adversarial networks for reconstructing
    natural images from brain activity. *NeuroImage* 181, 775–785. [10.1016/j.neuroimage.2018.07.043](https:/doi.org/10.1016/j.neuroimage.2018.07.043)
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seeliger 等人（2018）Seeliger, K., Güçlü, U., Ambrogioni, L., Güçlütürk, Y., 和 van
    Gerven, M. A. J. (2018). 用于从脑活动中重建自然图像的生成对抗网络。*神经影像* 181, 775–785. [10.1016/j.neuroimage.2018.07.043](https:/doi.org/10.1016/j.neuroimage.2018.07.043)
- en: 'Shen et al. (2019a) Shen, G., Dwivedi, K., Majima, K., Horikawa, T., and Kamitani,
    Y. (2019a). End-to-End Deep Image Reconstruction From Human Brain Activity. *Frontiers
    in Computational Neuroscience* 13. [10.3389/fncom.2019.00021](https:/doi.org/10.3389/fncom.2019.00021).
    Publisher: Frontiers'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等人（2019a）Shen, G., Dwivedi, K., Majima, K., Horikawa, T., 和 Kamitani,
    Y. (2019a). 从人脑活动中进行端到端的深度图像重建。*计算神经科学前沿* 13. [10.3389/fncom.2019.00021](https:/doi.org/10.3389/fncom.2019.00021).
    出版社: Frontiers'
- en: 'Shen et al. (2019b) Shen, G., Horikawa, T., Majima, K., and Kamitani, Y. (2019b).
    Deep image reconstruction from human brain activity. *PLOS Computational Biology*
    15, e1006633. [10.1371/journal.pcbi.1006633](https:/doi.org/10.1371/journal.pcbi.1006633).
    Number: 1 Publisher: Public Library of Science'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人（2019b）Shen, G., Horikawa, T., Majima, K., 和 Kamitani, Y.（2019b）。从人脑活动中深度重建图像。*PLOS
    计算生物学* 15，e1006633。[10.1371/journal.pcbi.1006633](https:/doi.org/10.1371/journal.pcbi.1006633)。期号：1
    出版社：公共科学图书馆
- en: 'Simonyan and Zisserman (2015) Simonyan, K. and Zisserman, A. (2015). Very Deep
    Convolutional Networks for Large-Scale Image Recognition. *arXiv:1409.1556 [cs]*
    ArXiv: 1409.1556'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Simonyan 和 Zisserman（2015）Simonyan, K. 和 Zisserman, A.（2015）。用于大规模图像识别的非常深层卷积网络。*arXiv:1409.1556
    [cs]* ArXiv: 1409.1556'
- en: 'St-Yves and Naselaris (2018) St-Yves, G. and Naselaris, T. (2018). Generative
    Adversarial Networks Conditioned on Brain Activity Reconstruct Seen Images. In
    *2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)*. 1054–1061.
    [10.1109/SMC.2018.00187](https:/doi.org/10.1109/SMC.2018.00187). ISSN: 2577-1655'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: St-Yves 和 Naselaris（2018）St-Yves, G. 和 Naselaris, T.（2018）。基于脑活动的生成对抗网络重建所见图像。在
    *2018 IEEE 国际系统、人工智能与控制论大会 (SMC)*。1054–1061。[10.1109/SMC.2018.00187](https:/doi.org/10.1109/SMC.2018.00187)。ISSN：2577-1655
- en: 'Szegedy et al. (2016) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and
    Wojna, Z. (2016). Rethinking the Inception Architecture for Computer Vision. In
    *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)* (Las
    Vegas, NV, USA: IEEE), 2818–2826. [10.1109/CVPR.2016.308](https:/doi.org/10.1109/CVPR.2016.308)'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等人（2016）Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., 和 Wojna,
    Z.（2016）。重新思考计算机视觉的 Inception 架构。在 *2016 IEEE 计算机视觉与模式识别大会 (CVPR)*（拉斯维加斯，NV，美国：IEEE），2818–2826。[10.1109/CVPR.2016.308](https:/doi.org/10.1109/CVPR.2016.308)
- en: 'Thirion et al. (2006) Thirion, B., Duchesnay, E., Hubbard, E., Dubois, J.,
    Poline, J.-B., Lebihan, D., et al. (2006). Inverse retinotopy: inferring the visual
    content of images from brain activation patterns. *NeuroImage* 33, 1104–1116.
    [10.1016/j.neuroimage.2006.06.062](https:/doi.org/10.1016/j.neuroimage.2006.06.062)'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thirion 等人（2006）Thirion, B., Duchesnay, E., Hubbard, E., Dubois, J., Poline,
    J.-B., Lebihan, D., 等（2006）。逆视网膜定位：从脑激活模式推断图像的视觉内容。*神经影像* 33，1104–1116。[10.1016/j.neuroimage.2006.06.062](https:/doi.org/10.1016/j.neuroimage.2006.06.062)
- en: 'van Gerven et al. (2010) van Gerven, M. A. J., de Lange, F. P., and Heskes,
    T. (2010). Neural Decoding with Hierarchical Generative Models. *Neural Computation*
    22, 3127–3142. [10.1162/NECO_a_00047](https:/doi.org/10.1162/NECO_a_00047). Number:
    12 Publisher: MIT Press'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Gerven 等人（2010）van Gerven, M. A. J., de Lange, F. P., 和 Heskes, T.（2010）。使用层次生成模型进行神经解码。*神经计算*
    22，3127–3142。[10.1162/NECO_a_00047](https:/doi.org/10.1162/NECO_a_00047)。期号：12
    出版社：MIT Press
- en: 'VanRullen and Reddy (2019) VanRullen, R. and Reddy, L. (2019). Reconstructing
    faces from fMRI patterns using deep generative neural networks. *Communications
    Biology* 2, 1–10. [10.1038/s42003-019-0438-y](https:/doi.org/10.1038/s42003-019-0438-y).
    Number: 1 Publisher: Nature Publishing Group'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VanRullen 和 Reddy（2019）VanRullen, R. 和 Reddy, L.（2019）。利用深度生成神经网络从 fMRI 图案中重建面孔。*通讯生物学*
    2，1–10。[10.1038/s42003-019-0438-y](https:/doi.org/10.1038/s42003-019-0438-y)。期号：1
    出版社：自然出版集团
- en: 'Wang et al. (2004) Wang, Z., Bovik, A., Sheikh, H., and Simoncelli, E. (2004).
    Image quality assessment: from error visibility to structural similarity. *IEEE
    Transactions on Image Processing* 13, 600–612. [10.1109/TIP.2003.819861](https:/doi.org/10.1109/TIP.2003.819861).
    Conference Name: IEEE Transactions on Image Processing'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2004）Wang, Z., Bovik, A., Sheikh, H., 和 Simoncelli, E.（2004）。图像质量评估：从错误可见性到结构相似性。*IEEE
    图像处理汇刊* 13，600–612。[10.1109/TIP.2003.819861](https:/doi.org/10.1109/TIP.2003.819861)。会议名称：IEEE
    图像处理汇刊
- en: 'Xu et al. (2021) Xu, K., Du, C., Li, C., Zhu, J., and Zhang, B. (2021). Learning
    Implicit Generative Models by Teaching Density Estimators. In *Machine Learning
    and Knowledge Discovery in Databases*, eds. F. Hutter, K. Kersting, J. Lijffijt,
    and I. Valera (Cham: Springer International Publishing), Lecture Notes in Computer
    Science, 239–255. [10.1007/978-3-030-67661-2_15](https:/doi.org/10.1007/978-3-030-67661-2_15)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2021）Xu, K., Du, C., Li, C., Zhu, J., 和 Zhang, B.（2021）。通过教学密度估计器学习隐式生成模型。在
    *机器学习与数据库中的知识发现*，编辑 F. Hutter, K. Kersting, J. Lijffijt, 和 I. Valera（查姆：Springer
    国际出版），计算机科学讲义，239–255。[10.1007/978-3-030-67661-2_15](https:/doi.org/10.1007/978-3-030-67661-2_15)
- en: 'Zhang et al. (2017) Zhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L. (2017).
    Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising.
    *IEEE Transactions on Image Processing* 26, 3142–3155. [10.1109/TIP.2017.2662206](https:/doi.org/10.1109/TIP.2017.2662206).
    Conference Name: IEEE Transactions on Image Processing'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2017）Zhang, K., Zuo, W., Chen, Y., Meng, D., 和 Zhang, L.（2017）。超越高斯去噪器：深度卷积神经网络的残差学习用于图像去噪。*IEEE图像处理学报*
    26, 3142–3155。 [10.1109/TIP.2017.2662206](https:/doi.org/10.1109/TIP.2017.2662206)。会议名称：IEEE图像处理学报
- en: 'Zhang et al. (2018) Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and
    Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual
    Metric. In *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*
    (Salt Lake City, UT: IEEE), 586–595. [10.1109/CVPR.2018.00068](https:/doi.org/10.1109/CVPR.2018.00068)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人（2018）Zhang, R., Isola, P., Efros, A. A., Shechtman, E., 和 Wang, O.（2018）。深度特征作为感知度量的非凡有效性。在*2018
    IEEE/CVF计算机视觉与模式识别会议*（盐湖城，UT: IEEE），586–595。 [10.1109/CVPR.2018.00068](https:/doi.org/10.1109/CVPR.2018.00068)'
- en: 'Zhang et al. (2020) Zhang, X., Yao, L., Wang, X., Monaghan, J. J. M., Mcalpine,
    D., and Zhang, Y. (2020). A survey on deep learning-based non-invasive brain signals:
    recent advances and new frontiers. *Journal of Neural Engineering* [10.1088/1741-2552/abc902](https:/doi.org/10.1088/1741-2552/abc902)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2020）Zhang, X., Yao, L., Wang, X., Monaghan, J. J. M., Mcalpine, D.,
    和 Zhang, Y.（2020）。关于基于深度学习的非侵入性脑信号的调查：近期进展与新前沿。*神经工程学杂志* [10.1088/1741-2552/abc902](https:/doi.org/10.1088/1741-2552/abc902)
- en: Zhao et al. (2017) Zhao, J. J., Mathieu, M., and LeCun, Y. (2017). Energy-based
    Generative Adversarial Networks. In *5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings* (OpenReview.net)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人（2017）Zhao, J. J., Mathieu, M., 和 LeCun, Y.（2017）。基于能量的生成对抗网络。在*第5届国际学习表征会议，ICLR
    2017，法国土伦，2017年4月24-26日，会议论文集*（OpenReview.net）
