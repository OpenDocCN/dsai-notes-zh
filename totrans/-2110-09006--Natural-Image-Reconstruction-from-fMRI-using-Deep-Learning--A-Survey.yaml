- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:50:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2110.09006] Natural Image Reconstruction from fMRI using Deep Learning: A
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.09006](https://ar5iv.labs.arxiv.org/html/2110.09006)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \correspondance\extraAuth
  prefs: []
  type: TYPE_NORMAL
- en: Zarina Rakhimberdina
  prefs: []
  type: TYPE_NORMAL
- en: zarina.rakhimberdina@net.c.titech.ac.jp
  prefs: []
  type: TYPE_NORMAL
- en: 'Natural Image Reconstruction from fMRI using Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zarina Rakhimberdina ^(1,3), Quentin Jodelet ^(1,3), Xin Liu ^(2,3,∗), Tsuyoshi
    Murata ^(1,3)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the advent of brain imaging techniques and machine learning tools, much
    effort has been devoted to building computational models to capture the encoding
    of visual information in the human brain. One of the most challenging brain decoding
    tasks is the accurate reconstruction of the perceived natural images from brain
    activities measured by functional magnetic resonance imaging (fMRI). In this work,
    we survey the most recent deep learning methods for natural image reconstruction
    from fMRI. We examine these methods in terms of architectural design, benchmark
    datasets, and evaluation metrics and present a fair performance evaluation across
    standardized evaluation metrics. Finally, we discuss the strengths and limitations
    of existing studies and present potential future directions.
  prefs: []
  type: TYPE_NORMAL
- en: \helveticabold
  prefs: []
  type: TYPE_NORMAL
- en: '1 Keywords:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural Image Reconstruction, fMRI, Brain Decoding, Neural Decoding, Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: 2 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Visual decoding using fMRI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many brain imaging studies focus on decoding how the human brain represents
    information about the outer world. Considering that, the majority of external
    sensory information is processed by the human visual system (Logothetis and Sheinberg,
    [1996](#bib.bib33)), a need for deeper understanding of visual information processing
    in the human brain encourages building complex computational models that can characterize
    the content of visual stimuli. This problem is referred to as human visual decoding
    of perceived images and has gained increasing attention.
  prefs: []
  type: TYPE_NORMAL
- en: A great advancement in recent neuroscience research has been achieved through
    functional magnetic resonance imaging (fMRI) (Poldrack and Farah, [2015](#bib.bib45);
    Nestor et al., [2020](#bib.bib42)). The fMRI technique captures neural activity
    in the brain by measuring variations in blood oxygen levels (Ogawa et al., [1990](#bib.bib43);
    Bandettini, [2012](#bib.bib1)). Among the various brain imaging techniques, fMRI
    is noninvasive and has a high spatial resolution. These characteristics allow
    fMRI to be used in a wide range of problems, including neurological disorder diagnosis
    (Zhang et al., [2020](#bib.bib71); Rakhimberdina et al., [2020](#bib.bib50)) and
    human visual decoding (Haxby et al., [2001](#bib.bib17); Kamitani and Tong, [2005](#bib.bib22);
    Horikawa and Kamitani, [2017](#bib.bib19)). The recent progress in human visual
    decoding has shown that beyond merely encoding the information about visual stimuli
    (Poldrack and Farah, [2015](#bib.bib45)), brain activity captured by fMRI can
    be used to reconstruct visual stimuli information (Roelfsema et al., [2018](#bib.bib52);
    Kay et al., [2008](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the target task, human visual decoding can be categorized into stimuli
    category classification, stimuli identification, and reconstruction (Naselaris
    et al., [2011](#bib.bib40)). In classification, brain activity is used to predict
    discrete object categories of the presented stimuli (Haxby et al., [2001](#bib.bib17);
    Horikawa and Kamitani, [2017](#bib.bib19)). The goal of identification is to identify
    a specific stimulus corresponding to the given pattern of brain activity from
    a known set of stimuli images (Kay et al., [2008](#bib.bib24); Naselaris et al.,
    [2011](#bib.bib40)). In both identification and reconstruction, we aim to recover
    image-specific details, such as object position, size, and angle. However, reconstruction
    is a more challenging task, in which a replica of the stimulus image needs to
    be generated for a given fMRI signal (see Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Visual
    decoding using fMRI ‣ 2 Introduction ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey")). Furthermore, stimulus-related information encoded
    in the fMRI activity, which allows high-accuracy identification, may only partially
    characterize stimuli images and thus be insufficient for proper image reconstruction
    (Kay et al., [2008](#bib.bib24); St-Yves and Naselaris, [2018](#bib.bib62)). With
    the development of sophisticated image reconstruction methods and the increasing
    amount of brain imaging data, more attention has been directed toward visual stimuli
    reconstruction from fMRI activity in the visual cortex (Miyawaki et al., [2008](#bib.bib38);
    Naselaris et al., [2009](#bib.bib41); van Gerven et al., [2010](#bib.bib65)).
    fMRI-based visual reconstruction can improve our understanding of the brain’s
    visual processing mechanisms, and researchers can incorporate this knowledge into
    the development of brain–computer interfaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a58f07c3380f288890e28ca0c5eea6ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Framework diagram for natural image reconstruction task.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Natural image reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The variety of visual stimuli used in visual reconstruction tasks can range
    from simple low-level detail images, such as Gabor wavelets and domino patterns
    (Thirion et al., [2006](#bib.bib64)), to more elaborate images depicting alphabetical
    characters, digits (van Gerven et al., [2010](#bib.bib65); Schoenmakers et al.,
    [2013](#bib.bib55)), natural objects, and scenes (Haxby et al., [2001](#bib.bib17);
    Horikawa and Kamitani, [2017](#bib.bib19)). The image reconstruction task for
    low-level detail stimuli does not require expressive models, and linear mapping
    is usually sufficient for learning effective reconstruction (Miyawaki et al.,
    [2008](#bib.bib38)). Among the variety of visual stimuli, natural images are considered
    the most challenging, as they require accurate reconstruction of color, shape,
    and higher-level perceptual features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Shen et al. ([2019b](#bib.bib60)), we refer to the task of visual
    stimuli reconstruction from fMRI as natural image reconstruction, where stimuli
    are drawn from a database of natural images. The goal of neural decoding models
    is to learn a mapping function $f:\mathcal{V}\rightarrow\mathcal{X}$, where $\mathcal{X}$
    and $\mathcal{V}$ denote two sets corresponding to stimulus images and fMRI activity
    patterns extracted from the visual cortex. A framework diagram for visual reconstruction
    is shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Visual decoding using fMRI ‣ 2 Introduction
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The main challenges of natural image reconstruction include the following. First,
    the reconstruction quality must be good enough to capture the similarity between
    reconstructed and original images on multiple levels. In contrast to low-resolution
    image stimuli, such as shape or character patterns, good-quality reconstruction
    of natural images requires that both lower-level details and high-level semantic
    information be preserved. Second, brain’s visual representations are invariant
    to different objects or image details, which is essential for object recognition,
    but imply that brain activation patterns are not necessarily unique for a given
    stimulus object (St-Yves and Naselaris, [2018](#bib.bib62); Quiroga et al., [2005](#bib.bib48)).
    Finally, the lack of a standardized evaluation procedure for assessing the reconstruction
    quality makes it difficult to compare the existing methods. In this work, we will
    primarily focus on the solution to the third challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Contributions. The topic of natural image reconstruction from fMRI is relatively
    new and has attracted much interest over the last few years. The related surveys
    on the field of natural encoding and decoding of visual input give a broad overview
    of the existing techniques to extract information from the brain (Roelfsema et al.,
    [2018](#bib.bib52); Nestor et al., [2020](#bib.bib42)) and focus on the traditional
    machine learning methods (Chen et al., [2014](#bib.bib4)). To our knowledge, there
    is no comprehensive survey on the topic of natural image reconstruction from fMRI
    using deep learning. Given the lack of a standardized evaluation process in terms
    of the benchmark dataset and standard metrics, our main contribution is to provide
    the research community with a fair performance comparison for existing methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this survey, we provide an overview of the deep learning-based natural image
    reconstruction methods. We discuss the differences in architecture, learning paradigms,
    and advantages of deep learning models over traditional methods. In addition,
    we review the evaluation metrics and compare models on the same benchmark: the
    same metrics and the same dataset parameters. The proposed standardised evaluation
    on a common set of metrics offers an opportunity to fairly evaluate and track
    new emerging methods in the field.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this paper is organized as follows. In Section [3](#S3 "3 Benchmark
    datasets ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey")
    and Section [4](#S4 "4 Deep learning-based approaches for natural image reconstruction
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey"), we introduce
    popular publicly available datasets for natural image reconstruction and review
    recent state-of-the-art deep learning models for natural image reconstruction,
    respectively. Then, we provide an overview of the evaluation metrics in Section
    [5](#S5 "5 Reconstruction evaluation ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey"), and presents a fair comparative evaluation of
    the methods in Section [6](#S6 "6 Fair comparison across the methods ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey"). Finally, we discuss
    the main challenges and possible future directions of this work. Section [8](#S8
    "8 Conclusion ‣ Natural Image Reconstruction from fMRI using Deep Learning: A
    Survey") concludes the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Characteristics of benchmark datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Number of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Subjects &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image stimuli &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Train/Test &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Repetition time &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Train/Test &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ROIs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| VanRullen and Reddy ([2019](#bib.bib66)) | Faces | 4 | 88/20 | n/a | n/a
    |'
  prefs: []
  type: TYPE_TB
- en: '| Kay et al. ([2008](#bib.bib24)) | vim-1 | 2 | 1750/120 | 2/13 | V1, V2, V3,
    V4, LO |'
  prefs: []
  type: TYPE_TB
- en: '| Horikawa and Kamitani ([2017](#bib.bib19)) | Generic Object Decoding | 5
    | 1200/50 | 1/35 | V1, V2, V3, V4, LOC, FFA, PPA |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. ([2019b](#bib.bib60)) | Deep Image Reconstruction | Natural Images
    | 3 | 1200/50 | 5/24 |'
  prefs: []
  type: TYPE_TB
- en: '| Artificial Shapes | 3 | 0/40 | 0/20 |'
  prefs: []
  type: TYPE_TB
- en: '| Alphabetical Letters | 3 | 0/10 | 0/12 |'
  prefs: []
  type: TYPE_TB
- en: 3 Benchmark datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section summarizes the publicly available benchmark datasets used in deep
    learning-based natural image reconstruction from fMRI activity. While there exist
    a variety of datasets used for stimuli reconstruction, such as binary contrast
    patterns (BCP) (Miyawaki et al., [2008](#bib.bib38)), 69 dataset of handwritten
    digits (van Gerven et al., [2010](#bib.bib65)), BRAINS dataset of handwritten
    characters (Schoenmakers et al., [2013](#bib.bib55)), we focus on the datasets
    with higher level of perceptual complexity of presented stimuli: dataset of faces,
    grayscale natural images, and natural images from Imagenet. Each sample of these
    datasets represents a labeled pair – fMRI recording paired with the relevant stimuli
    image. Several distinctive characteristics of each dataset are presented in Table
    [1](#S2.T1 "Table 1 ‣ 2.2 Natural image reconstruction ‣ 2 Introduction ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Faces. VanRullen and Reddy ([2019](#bib.bib66)) used facial stimuli to reconstruct
    human faces from fMRI activity using deep neural networks¹¹1The fMRI dataset is
    available at [https://openneuro.org/datasets/ds001761](https://openneuro.org/datasets/ds001761)..
    The facial stimuli were drawn randomly from the CelebA dataset (Liu et al., [2015](#bib.bib32)),
    and four healthy subjects participated in the experiment. The samples of stimuli
    images are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3 Benchmark datasets ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey") A.'
  prefs: []
  type: TYPE_NORMAL
- en: 'vim-1 dataset of grayscale natural images was acquired to study how natural
    images are represented by the human visual system²²2The dataset is available at
    [http://crcns.org/data-sets/vc/vim-1](http://crcns.org/data-sets/vc/vim-1). (Kay
    et al., [2008](#bib.bib24)). The stimuli comprise a set of 1870 grayscale 500
    $\times$ 500 pixels natural images of real-world objects, animals, and indoor
    and outdoor scenes (the samples are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3 Benchmark
    datasets ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey")
    B). Natural images were obtained from the Corel Stock Photo Libraries (Corel Corporation,
    [1994](#bib.bib7)), the Berkeley database of human segmented natural images³³3[https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/)
    (Martin et al., [2001](#bib.bib36)), and an image collection from the authors.
    Two healthy subjects with normal or corrected-to-normal vision were involved in
    the fMRI data acquisition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/374b0b5f99a99e605b2677af267aa4a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Samples for natural stimuli: (A) images from Faces dataset (VanRullen
    and Reddy, [2019](#bib.bib66)); (B) grayscale natural images from vim-1 dataset
    (Kay et al., [2008](#bib.bib24)); (C) natural images from GOD (Horikawa and Kamitani,
    [2017](#bib.bib19)) and DIR (Shen et al., [2019b](#bib.bib60)) datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Natural images from Imagenet. Two natural image datasets released by Kamitani
    Lab are widely used in image reconstruction. The first dataset, also known as
    the Generic Object Decoding⁴⁴4The dataset can be acquired from [http://brainliner.jp/data/brainliner/Generic_Object_Decoding](http://brainliner.jp/data/brainliner/Generic_Object_Decoding).
    dataset or GOD for short, was originally used by Horikawa and Kamitani ([2017](#bib.bib19))
    for the image classification task from the fMRI data and was later adopted for
    image reconstruction (Beliy et al., [2019](#bib.bib2); Ren et al., [2021](#bib.bib51)).
    The dataset consists of pairs of high-resolution 500 $\times$ 500 pixels stimuli
    images (see Figure [2](#S3.F2 "Figure 2 ‣ 3 Benchmark datasets ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey") C) and the corresponding
    fMRI recordings. fMRI scans were obtained from five healthy subjects; the stimuli
    images were selected from the ImageNet dataset (Deng et al., [2009](#bib.bib8))
    and span across 200 object categories.'
  prefs: []
  type: TYPE_NORMAL
- en: The second dataset based on the natural image dataset was acquired for the image
    reconstruction task (Shen et al., [2019b](#bib.bib60), [a](#bib.bib59)). It is
    publicly available at OpenNeuro⁵⁵5[https://openneuro.org/datasets/ds001506/versions/1.3.1](https://openneuro.org/datasets/ds001506/versions/1.3.1)
    , where it is cited as Deep Image Reconstruction. We refer to this dataset as
    Deep Image Reconstruction or DIR for short. The DIR dataset contains 1,250 stimuli
    images that are identical to the ones used in GOD. Because of different image
    presentation experiments, in which training and test image stimuli were repeated
    5 and 24 times respectively, the training set of the DIR dataset consists of a
    larger number of stimuli-fMRI pairs (5 $\times$ 1,200 samples) compared to the
    GOD. Three healthy subjects were involved in the image presentation. An appealing
    feature of this dataset is that, in addition to natural images, the dataset contains
    artificial shapes and alphabetical letters. The artificial shapes dataset consists
    of 40 images – a combination of eight colors and five geometric shapes. The alphabetical
    letters dataset consists of 10 letters (A, C, E, I, N, O, R, S, T, U) of consistent
    brightness and color.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep learning-based approaches for natural image reconstruction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before deep learning, the traditional methods in natural image reconstruction
    estimated a linear mapping from fMRI signals to hand-crafted image features using
    linear regression models (Kay et al., [2008](#bib.bib24); Fujiwara et al., [2013](#bib.bib13);
    Naselaris et al., [2009](#bib.bib41)). These methods primarily focus on extracting
    predefined low-level features from stimulus images, such as local image structures
    or features of Gabor filters (Fang et al., [2020](#bib.bib12); Beliy et al., [2019](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, deep neural networks (DNNs) have significantly advanced computer
    vision research, replacing models based on hand-crafted features. In particular,
    DNN models have achieved better accuracy and improved image quality in various
    computer vision tasks, including image classification (Krizhevsky et al., [2012](#bib.bib28)),
    image segmentation (Chen et al., [2015](#bib.bib3)), and image restoration (Zhang
    et al., [2017](#bib.bib69)). In visual decoding tasks using brain imaging data,
    deep learning approaches have been applied to image classification (Haxby et al.,
    [2001](#bib.bib17); Horikawa and Kamitani, [2017](#bib.bib19)), object segmentation
    (Kamnitsas et al., [2017](#bib.bib23)), and natural image reconstruction (Shen
    et al., [2019b](#bib.bib60), [a](#bib.bib59)). They were shown to be more powerful
    than traditional methods (Zhang et al., [2020](#bib.bib71); Kriegeskorte, [2015](#bib.bib26))
    primarily due to the multilayer architecture allowing to learn nonlinear mappings
    from brain signals to stimulus images (Beliy et al., [2019](#bib.bib2); Shen et al.,
    [2019a](#bib.bib59)).
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by the success of deep learning in image generation, many recent studies
    have widely used DNN models in natural image reconstruction for several reasons.
    First, the deep learning framework conforms to some degree to the visual encoding–decoding
    process occurring in the hierarchical regions of the human visual system (Pinto
    et al., [2009](#bib.bib44); Krizhevsky et al., [2012](#bib.bib28); Schrimpf et al.,
    [2018](#bib.bib57)). Second, the application of deep generative models allows
    the synthesis of high-quality natural-looking images, which is achieved by learning
    the underlying data distribution (Goodfellow et al., [2014](#bib.bib15)). Additionally,
    the training process can be aided by models pretrained on larger image datasets
    (Shen et al., [2019b](#bib.bib60), [a](#bib.bib59)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we present the evolution of the state-of-the-art deep learning-based
    methods for natural image reconstruction. We analyze them in terms of DNN architecture,
    use of pretraining, and the choice of the dataset. The most popular deep learning
    models used in natural image reconstruction tasks include non-generative methods
    such as convolutional neural networks, encoder–decoder-based frameworks (Kingma
    and Welling, [2014](#bib.bib25)); and generative methods, such as adversarial
    networks (Goodfellow et al., [2014](#bib.bib15)) and variational autoencoders
    (Larsen et al., [2016](#bib.bib29)). A comparison of the surveyed methods is presented
    in Table [2](#S4.T2 "Table 2 ‣ 4 Deep learning-based approaches for natural image
    reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning: A
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Comparative table of the surveyed works. E2E represents end-to-end
    training. Loss denotes the loss function (MAE: mean absolute error; MSE: mean
    squared error; KL: KL divergence; Adv: adversarial loss; Cos: cosine similarity.
    The links to the source code are valid as of November, 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Authors | Year | Datasets | Loss | E2E | Pre-training | Public code
    |'
  prefs: []
  type: TYPE_TB
- en: '| SeeligerDCGAN | Seeliger et al. | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; BRAINS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; vim-1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GOD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MAE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generator pre-trained on ImageNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Chrabaszcz et al., [2017](#bib.bib6)), Microsoft COCO &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Lin et al., [2014](#bib.bib31)), datasets from Maaten ([2009](#bib.bib34))
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and Schomaker et al. ([2000](#bib.bib56)). AlexNet-based &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Comparator trained on ImageNet. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no |'
  prefs: []
  type: TYPE_TB
- en: '| StYvesEBGAN | St-Yves and Naselaris | 2018 | vim-1 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The denoiser and generator were pretrained &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; on 32 $\times$ 32 color images from the CIFAR-10 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; dataset (Krizhevsky, [2009](#bib.bib27)) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [yes](https://github.com/styvesg/gan-decoding-supplementary) |'
  prefs: []
  type: TYPE_TB
- en: '| ShenDNN(+DGN) | Shen et al. | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DIR: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural images, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Artificial Shapes, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Alphabetical Letters &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| MSE | no |'
  prefs: []
  type: TYPE_TB
- en: '&#124; VGG-19 pre-trained on ImageNet. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pre-trained DGN (Dosovitskiy and Brox, [2016](#bib.bib10)). &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [yes](https://github.com/KamitaniLab/DeepImageReconstruction) |'
  prefs: []
  type: TYPE_TB
- en: '| ShenGAN | Shen et al. | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DIR: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural images, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Artificial Shapes, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Alphabetical Letters &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| yes |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Caffenet-based Comparator pre-trained &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; on ImageNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [yes](https://github.com/KamitaniLab/End2EndDeepImageReconstruction) |'
  prefs: []
  type: TYPE_TB
- en: '| BeliyEncDec | Beliy et al. | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; GOD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; vim-1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cos &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MAE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Pretrained AlexNet-based encoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [yes](https://github.com/WeizmannVision/ssfmri2im) |'
  prefs: []
  type: TYPE_TB
- en: '| VanRullenVAE-GAN | VanRullen and Reddy | 2019 | Faces |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no | Pre-trained on CelebA dataset | [yes](https://github.com/rufinv/VAE-GAN-celebA)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GazivEncDec | Gaziv et al. | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; GOD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; vim-1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cos &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MAE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Pretrained AlexNet-based encoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no |'
  prefs: []
  type: TYPE_TB
- en: '| QiaoGAN-BVRM | Qiao et al. | 2020 | vim-1 | MSE | no |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Generator of BigGAN pre-trained &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; on ImageNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [partially](https://github.com/KaiQiao1992/ETECRM) |'
  prefs: []
  type: TYPE_TB
- en: '| FangSSGAN | Fang et al. | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DIR: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Natural images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MAE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no | - | [partially](https://github.com/duolala1/Reconstructing-Perceptive-Images-from-Brain-Activity-by-Shape-Semantic-GAN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MozafariBigBiGAN | Mozafari et al. | 2020 | GOD | Adv | no | BigBiGAN pre-trained
    on ImageNet | no |'
  prefs: []
  type: TYPE_TB
- en: '| RenD-VAE/GAN | Ren et al. | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; BCP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 6-9 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BRAINS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GOD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; KL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Model pre-trained on external &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; data from ImageNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| no |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Non-generative methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional neural network (CNN). Compared to a simpler multilayer feed-forward
    neural network, which disregards the structural information of input images, the
    CNN has a better feature extraction capability because of the information filtering
    performed by convolutional layers within a neighborhood of pixels (LeCun et al.,
    [1989](#bib.bib30)). Stacking convolutional layers on top of each other allows
    learning hierarchical visual features of input images, known as feature abstraction.
    The lower CNN layers learn low-level details, whereas the higher CNN layers extract
    global high-level visual information from images (Mahendran and Vedaldi, [2015](#bib.bib35)).
    The use of CNNs is ubiquitous in image processing tasks, including image reconstruction.
    Specifically, encoder–decoder (Beliy et al., [2019](#bib.bib2); Gaziv et al.,
    [2020](#bib.bib14)), U-Net (Fang et al., [2020](#bib.bib12)), generative adversarial
    network (Goodfellow et al., [2014](#bib.bib15)), and variational autoencoder (Kingma
    and Welling, [2014](#bib.bib25)) are popular architectures that adopt stacked
    convolutional layers to extract features at multiple levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shen et al. (Shen et al., [2019b](#bib.bib60)) utilized a pretrained VGG-19-based
    DNN to extract hierarchical features from stimuli images (see Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Non-generative methods ‣ 4 Deep learning-based approaches for
    natural image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep
    Learning: A Survey") A). The DNN consists of sixteen convolutional layers followed
    by three fully connected layers. This method was motivated by the finding that
    hierarchical image representations obtained from different layers of deep neural
    network correlate with brain activity in the visual cortex (Eickenberg et al.,
    [2017](#bib.bib11); Horikawa and Kamitani, [2017](#bib.bib19)). Using this fact,
    one can establish a hierarchical mapping from fMRI signals in the low/high-level
    areas of visual cortices to the corresponding low/high-level features from the
    DNN. For this task, the authors implemented a feature decoder ${D}$ that maps
    fMRI activity patterns to multilayer DNN features. The decoder ${D}$ is trained
    on the train set before the reconstruction task, using the method from Horikawa
    and Kamitani ([2017](#bib.bib19)). These decoded fMRI features correspond to the
    hierarchical image features obtained from DNN. The optimization is performed on
    the feature space by minimizing the difference between the hierarchical DNN features
    of the image and multilayer features decoded from fMRI activity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06b89cf203bfdf15d49b74e4d74d6d36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of two variations of frameworks proposed by Shen et al.
    ([2019b](#bib.bib60)): (A) ShenDNN and (B) ShenDNN+DGN. The yellow color denotes
    the use of pretrained components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deterministic encoder–decoder models. In deep learning, encoder–decoder models
    are widely used in image-to-image translation (Isola et al., [2017](#bib.bib20))
    and sequence-to-sequence models (Cho et al., [2014](#bib.bib5)). They learn the
    mapping from an input domain to an output domain via a two-stage architecture:
    an encoder ${E}$ that compresses the input vector $\mathbf{x}$ to the latent representation
    $\mathbf{z}={E}(\mathbf{x})$ and a decoder $\mathbf{y}={D}(\mathbf{z})$ that produces
    the output vector $\mathbf{y}$ from the latent representation $\mathbf{z}$ (Minaee
    et al., [2021](#bib.bib37)). The compressed latent representation vector $\mathbf{z}$
    serves as a bottleneck, which encodes a low-dimensional representation of the
    input. The model is trained to minimize the reconstruction error, which is the
    difference between the reconstructed output and ground-truth input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Beliy et al. ([2019](#bib.bib2)) presented a CNN-based encoder–decoder model,
    where the encoder ${E}$ learns the mapping from stimulus images to the corresponding
    fMRI activity, and a decoder ${{D}}$ learns the mapping from fMRI activity to
    their corresponding images. The framework of this method, which we refer to as
    BeliyEncDec, is presented in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Non-generative
    methods ‣ 4 Deep learning-based approaches for natural image reconstruction ‣
    Natural Image Reconstruction from fMRI using Deep Learning: A Survey"). By stacking
    the encoder and decoder back-to-back, the authors introduced two combined networks
    ${E}$-${{D}}$ and ${{D}}$-${E}$, whose inputs and outputs are natural images and
    fMRI recordings, respectively. This allowed the training to be self-supervised
    on a larger dataset of unlabeled data. Specifically, 50,000 additional images
    from the ImageNet validation set and test fMRI recordings without stimulus pairs
    were used as unlabeled natural images and unlabeled fMRI samples. The authors
    demonstrated the advantage of their method by achieving competitive results on
    two natural image reconstruction datasets: Generic Object Decoding (Horikawa and
    Kamitani, [2017](#bib.bib19)) and vim-1 (Kay et al., [2008](#bib.bib24)). The
    training was conducted in two steps. In the first step, the encoder ${E}$ builds
    a mapping from stimulus images to fMRI activity. It utilizes the weights of the
    first convolutional layer of the pretrained AlexNet (Krizhevsky et al., [2012](#bib.bib28))
    and is trained in a supervised manner to predict fMRI activity for input images.
    In the second step, the trained encoder ${E}$ is fixed, and the decoder ${{D}}$
    is jointly trained using labeled and unlabeled data. The entire loss of the model
    consists of the fMRI loss of the encoder ${E}$ and the Image loss (RGB and features
    loss) of the decoder ${D}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9df73875917e92e51fb5f5f22daf5fb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: BeliyEncDec framework proposed by Beliy et al. ([2019](#bib.bib2)):
    (A) supervised training of the Encoder; (B) supervised and self-supervised training
    of the Decoder. The weights of the Encoder are fixed. The blue color denotes the
    components of the model trained on external unlabeled data. The image is adapted
    from Beliy et al. ([2019](#bib.bib2)).'
  prefs: []
  type: TYPE_NORMAL
- en: In a follow-up study, Gaziv et al. ([2020](#bib.bib14)) improved the reconstruction
    accuracy of BeliyEncDec by introducing a loss function based on the perceptual
    similarity measure (Zhang et al., [2018](#bib.bib70)). To calculate perceptual
    similarity loss, the authors first extracted multilayer features from original
    and reconstructed images using VGG and then compared the extracted features layerwise.
    To distinguish it from BeliyEncDec, we refer to the framework proposed by Gaziv
    et al. ([2020](#bib.bib14)) as GazivEncDec.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Generative methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative models assume that the data is generated from some probability distribution
    $p(\mathbf{x})$ and can be classified as implicit and explicit. Implicit models
    do not define the distribution of the data but instead specify a random sampling
    process with which to draw samples from $p(\mathbf{x})$. Explicit models, on the
    other hand, explicitly define the probability density function, which is used
    to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Network (GAN).
  prefs: []
  type: TYPE_NORMAL
- en: A class of implicitly defined generative models called Generative adversarial
    networks (GANs) received much attention due to their ability to produce realistic
    images (Goodfellow et al., [2014](#bib.bib15)). In natural image reconstruction,
    GANs are widely used to learn the distribution of stimulus images. A GAN contains
    generator and discriminator networks. In the image generation task, the generator
    ${G}$ takes a random noise vector $\mathbf{z}$ (generally sampled from a Gaussian
    distribution) and generates a fake sample $G(\mathbf{z})$ with the same statistics
    as the training set images. During training, the generator’s ability to generate
    realistic images continually improves until the discriminator is unable to distinguish
    the difference between a real sample and a generated fake one. GAN-based frameworks
    have several desirable properties compared to other generative methods. First,
    GANs do not require strong assumptions regarding the form of the output probability
    distribution. Second, adversarial training, which uses the discriminator, allows
    unsupervised training of the GAN (St-Yves and Naselaris, [2018](#bib.bib62)).
    An illustration of GAN and details on GAN’s loss function are provided in Supplementary
    Material.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that reconstructions resemble natural images Shen et al. ([2019b](#bib.bib60))
    further modified their ShenDNN method by introducing a deep generator network
    (DGN) (Dosovitskiy and Brox, [2016](#bib.bib10)). The framework is shown in Figure
    [3](#S4.F3 "Figure 3 ‣ 4.1 Non-generative methods ‣ 4 Deep learning-based approaches
    for natural image reconstruction ‣ Natural Image Reconstruction from fMRI using
    Deep Learning: A Survey") B. A DGN, pretrained on natural images using the GAN
    training process, is integrated with the DNN to produce realistic images, and
    the optimization is performed on the input space of the DGN. Thus, the reconstructed
    images are constrained to be in the subspace of the images generated by the DGN.
    We refer to these framework variations without and with DGN as ShenDNN and ShenDNN+DGN
    in future references.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Shen et al. ([2019b](#bib.bib60)), Fang et al. ([2020](#bib.bib12))
    based their work on the finding that visual features are hierarchically represented
    in the visual cortex. In the feature extraction step, the authors proposed two
    decoders, which extract shape and semantic representations from the lower and
    higher areas of visual cortex. The shape decoder ${D}_{sp}$ is a linear model,
    and the semantic decoder ${D}_{sm}$ has a DNN-based architecture (Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") A). In the image reconstruction step, the generator network ${{G}}$
    was trained with GAN using the extracted shape and semantic features as conditions
    for generating the images. We refer to this model as FangSSGAN, where SSGAN stands
    for the shape and semantic GAN. The generator ${{G}}$ is a CNN-based network with
    an encoder–decoder structure (Ronneberger et al., [2015](#bib.bib54)). To enhance
    reconstruction quality, approximately 1,200 additional images, different from
    those in the training/test set, were sampled from the ImageNet dataset to generate
    augmented data. These new images were used to generate shapes and category-average
    semantic features that were further passed into the GAN.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a91c0bc9dfac8fe28d3a0106e56739cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: GAN-based frameworks. (A) FangSSGAN framework utilised a semantic
    decoder $D_{sm}$ and a shape decoder $D_{sp}$. (B) ShenGAN framework introduced
    a comparator network $C$. (C) StYvesEBGAN framework consists of three components
    trained independently: an encoding model $E_{V}$, denoising autoencoder and $E_{C}$–$D_{C}$
    and a conditional GAN. (D) SeeligerDCGAN framework based on deep convolutional
    GAN. (E) MozafariBigBiGAN framework proposed Mozafari et al. ([2020](#bib.bib39)).
    (F) QiaoGAN-BVRM framework consists of four parts: a classifier, pretrained conditional
    generator, encoder, and evaluator network. For (A)–(F), the pretrained components
    of the framework are highlighted in yellow. The blue color of the components indicates
    that they were trained using additional data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another GAN-based model was proposed by Shen et al. ([2019a](#bib.bib59)).
    The end-to-end model directly learns the mapping from fMRI signals to reconstructed
    images without intermediate transformation or feature extraction (see Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") B). The framework, which we refer to as ShenGAN, was trained using
    three convolutional neural networks: a generator $G$, a comparator $C$, and a
    discriminator $D$. The generator $G$ maps the fMRI data vector $\mathbf{v}$ to
    $G(\mathbf{v})$, and a discriminator $D$ distinguishes between reconstruction
    $G(\mathbf{v})$ and the original image $\mathbf{x}$. A comparator network $C$
    is pretrained on ImageNet (on image classification task) and used to compare the
    reconstruction $G(\mathbf{v})$ with the original image $\mathbf{x}$ by calculating
    the perceptual loss (similarity in feature space). The combined loss function
    is a weighted sum of three terms: loss in image space, perceptual loss and adversarial
    loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The GAN-based methods described so far enhanced the quality of reconstruction
    by generating more natural-looking images. However, although GANs can generate
    new plausible samples matching the distribution of samples in the training dataset,
    they do not allow to control any characteristics of the generated data (Jakub
    Langr and Vladimir Bok, [2019](#bib.bib21)). To solve this issue, St-Yves and
    Naselaris ([2018](#bib.bib62)) implemented the conditional generation of images
    using a variation of GAN called the energy-based condition GAN or EBGAN (Zhao
    et al., [2017](#bib.bib72)). In their framework, which we refer to as StYvesEBGAN,
    the authors first implement the encoding model ${E}_{V}$ to learn the mapping
    from stimulus to fMRI, as shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Generative
    methods ‣ 4 Deep learning-based approaches for natural image reconstruction ‣
    Natural Image Reconstruction from fMRI using Deep Learning: A Survey") C. In addition,
    StYvesEBGAN utilizes a denoising autoencoder to compress noisy high-dimensional
    fMRI representations into lower-dimensional representations. These lower-dimensional
    fMRI representations are further used as a condition vector for the GAN to reconstruct
    the stimuli. EBGAN is a more stable framework in terms of training than regular
    GANs. Instead of a binary classifier, it uses a deep autoencoder network as a
    discriminator. The authors observed that the reconstruction quality is highly
    dependent on the voxel denoising autoencoder, which produces a conditioning vector
    that results in the best reconstruction accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: A group of studies by Seeliger et al. ([2018](#bib.bib58)), Mozafari et al.
    ([2020](#bib.bib39)), and Qiao et al. ([2020](#bib.bib46)) utilized GAN architecture
    with the assumption that there is a linear relationship between brain activity
    and the latent features of GAN. Similar to ShenDNN+DGN, these methods adopted
    the generator of a pretrained GAN as a natural image prior, which ensures that
    the reconstructed images follow similar distributions as natural images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seeliger et al. ([2018](#bib.bib58)) used a deep convolutional GAN (DCGAN)
    architecture (Radford et al., [2016](#bib.bib49)), which introduced improvements
    by stacking convolutional and deconvolutional layers. The authors learn the direct
    linear mapping from the fMRI space to the latent space of GAN (see Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") D). For the natural stimuli image domain, the generator ${{G}}$ was
    pretrained on down-sampled 64 $\times$ 64 converted-to-grayscale images from ImageNet
    (Chrabaszcz et al., [2017](#bib.bib6)) and Microsoft COCO (Lin et al., [2014](#bib.bib31))
    datasets. For the handwritten character stimulus domain, DCGAN was pretrained
    on 15,000 handwritten characters from (Maaten, [2009](#bib.bib34)) and (Schomaker
    et al., [2000](#bib.bib56)). Also, a pretrained comparator network ${{C}}$, based
    on AlexNet, was introduced as a feature-matching network to compute the feature
    loss $L_{feat}$ across different layers. Overall, the loss is computed as a weighted
    sum of the pixelwise image loss $L_{img}$ (MAE) and feature loss $L_{feat}$. We
    refer to this framework as SeeligerDCGAN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mozafari et al. ([2020](#bib.bib39)) used a variation of GAN, called the BigBiGAN
    model (Donahue and Simonyan, [2019](#bib.bib9)), which allowed the reconstruction
    of even more realistic images. The model generates high-level semantic information
    due to the BigBiGAN’s latent space, which extracts high-level image details from
    fMRI data. We refer to this framework as MozafariBigBiGAN. The framework utilizes
    a pretrained encoder ${E}$ that generates a latent space vector $E(\mathbf{x})$
    from the input image $\mathbf{x}$ and generator ${{G}}$ that generates an image
    $G(\mathbf{z})$ from the latent space vector $\mathbf{z}$ (see Figure [5](#S4.F5
    "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") E). During training, the authors computed the linear mapping $W$ from
    latent vectors $E(\mathbf{x})$ to fMRI activity using a general linear regression
    model. During the test stage, the linear mapping is inverted to compute the latent
    vectors $\mathbf{z}$ from the test fMRI activity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The GAN-based Bayesian visual reconstruction model (GAN-BVRM) proposed by Qiao
    et al. ([2020](#bib.bib46)) aims to improve the quality of reconstructions from
    a limited dataset combination and, as the name suggests, uses the combination
    of GAN and Bayesian learning. From Bayesian perspective, a conditional distribution
    $p(\mathbf{v}|\mathbf{x})$ corresponds to an encoder which predicts fMRI activity
    $\mathbf{v}$ from the stimuli image $\mathbf{x}$. On the other hand, an inverse
    conditional distribution $p(\mathbf{x}|\mathbf{v})$ corresponds to a decoder that
    reconstructs the stimuli from the fMRI activity. The goal of image reconstruction
    is to find the image that has the highest posterior probability $p(\mathbf{x}|\mathbf{v})$,
    given the fMRI activity. However, since the posterior distribution is hard to
    compute, Bayesian theorem is used to combine encoding model $p(\mathbf{v}|\mathbf{x})$
    and image prior $p(\mathbf{x})$ through $p(\mathbf{x}|\mathbf{v})\propto p(\mathbf{x})p(\mathbf{v}|\mathbf{x})$.
    The prior distribution $p(\mathbf{x})$ reflects the predefined knowledge about
    natural images and is independent of the fMRI activity. The QiaoGAN-BVRM framework
    is shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Generative methods ‣ 4 Deep learning-based
    approaches for natural image reconstruction ‣ Natural Image Reconstruction from
    fMRI using Deep Learning: A Survey") F, and it consists of four parts: a classifier
    network, pretrained conditional generator ${{G}}$, encoder ${E}$, and evaluator
    network. First, a classifier decodes object categories from fMRI data, and then
    a conditional generator ${{G}}$ of the BigGAN uses the decoded categories to generate
    natural images. The advantage of the pretrained generator is that it has already
    learned the data distribution from more than one million ImageNet natural images.
    Therefore, instead of searching the images one by one in a fixed image dataset
    (Naselaris et al., [2009](#bib.bib41)), the generator can produce the optimal
    image reconstructions that best match with the fMRI activity via backpropagation.
    The generated images are passed to the encoder ${E}$, which predicts the corresponding
    fMRI activity. The proposed visual encoding model and the pre-trained generator
    of BigGAN do not interfere with each other, which helps to improve the fidelity
    and naturalness of reconstruction. The reconstruction accuracy is measured using
    an evaluator network, which computes the negative average mean squared error (MSE)
    between the predicted and actual fMRI activity. The reconstructions are obtained
    by iteratively updating the input noise vector to maximize the evaluator’s score.'
  prefs: []
  type: TYPE_NORMAL
- en: VAE-GAN. The variational autoencoder (VAE) proposed by Kingma and Welling ([2014](#bib.bib25))
    is an example of an explicit generative network and is a popular generative algorithm
    used in neural decoding. Similar to autoencoders, the VAE is composed of an encoder
    and a decoder. But rather than encoding a latent vector, VAE encodes a distribution
    over the latent space, making the generative process possible. Thus, the goal
    of VAE is to find a distribution of the latent variable ${\mathbf{z}}$, which
    we can sample from $\mathbf{z}\sim q_{\phi}(\mathbf{z}|\mathbf{x})$ to generate
    new image reconstructions $\mathbf{x}^{\prime}\sim p_{\theta}(\mathbf{x}|\mathbf{z})$.
    $q_{\phi}(\mathbf{z}|\mathbf{x})$ represents a probabilistic encoder, parameterized
    with $\phi$, which embeds the input $\mathbf{x}$ into a latent representation
    $\mathbf{z}$. $p_{\theta}(\mathbf{x}|\mathbf{z})$ represents a probabilistic decoder,
    parameterized with $\theta$, which produces a distribution over the corresponding
    $\mathbf{x}$. The details on VAE and its loss function are provided in Supplementary
    Material.
  prefs: []
  type: TYPE_NORMAL
- en: A hybrid model by Larsen et al. ([2016](#bib.bib29)) integrates both the VAE
    and GAN in a framework called VAE-GAN. VAE-GAN combines VAE to produce latent
    features and GAN discriminator, which learns to discriminate between fake and
    real images. In VAE-GAN, the VAE decoder and GAN generator are combined into one.
    The advantages of VAE-GAN are as follows. First, the GAN’s adversarial loss enables
    generating visually more realistic images. Second, VAE-GAN achieves improved stability
    due to VAE-based optimization. This helps to avoid mode collapse inherent to GANs,
    which refers to a generator producing a limited subset of different outcomes (Ren
    et al., [2021](#bib.bib51); Xu et al., [2021](#bib.bib68)).
  prefs: []
  type: TYPE_NORMAL
- en: 'A group of studies on reconstructing natural images from brain activity patterns,
    including Ren et al. ([2021](#bib.bib51)) and VanRullen and Reddy ([2019](#bib.bib66)),
    incorporated probabilistic inference using VAE-GAN. In a recent work by Ren et al.
    ([2021](#bib.bib51)), the authors presented a combined network called Dual-Variational
    Autoencoder/ Generative Adversarial Network (D-VAE/GAN). The framework, which
    we named RenD-VAE/GAN, consists of a dual VAE-based encoder and an adversarial
    decoder, as illustrated in Figure [6](#S4.F6 "Figure 6 ‣ 4.2 Generative methods
    ‣ 4 Deep learning-based approaches for natural image reconstruction ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey"). Dual-VAE consists
    of two probabilistic encoders: visual $E_{vis}$ and cognitive $E_{cog}$, which
    encode stimuli images $\mathbf{x}$ and brain activity patterns $\mathbf{v}$ to
    corresponding latent representations $\mathbf{z_{x}}$ and $\mathbf{z_{v}}$. The
    framework is trained in three sequential stages. In the first stage, visual stimuli
    images are used to train the visual encoder $E_{vis}$, generator ${{G}}$, and
    discriminator ${{D}}$. $E_{vis}$ learns the direct mapping from visual images
    into latent representations. Then, using output of $E_{vis}$, the generator ${{G}}$
    is trained to predict the images and ${{D}}$ is trained to discriminate the predicted
    images from real images. In the second stage, $E_{cog}$ is trained to map high-dimensional
    fMRI signals to cognitive latent features. The generator ${{G}}$ is fixed and
    ${{D}}$ is trained to discriminate between the stimuli images produced in the
    first stage and the cognitively-driven reconstructions from cognitive latent features.
    This way, $E_{cog}$ is forced to generate visual and cognitive latent representations
    similar to each other. In the last training stage, $E_{cog}$ is fixed, whereas
    ${{G}}$ and ${{D}}$ are fine-tuned on fMRI signals to improve the accuracy of
    the generated images via cognitive latent representations. In this stage, ${{D}}$
    is trained to discriminate between real stimuli images and reconstructed images.
    During testing, only a trained cognitive encoder and generator were used for the
    inference. Since $E_{vis}$ takes visual stimuli as input, its learned latent representations
    $\mathbf{z_{x}}$ can guide $E_{cog}$ to learn the latent representations $\mathbf{z_{v}}$.
    Thus, in the second training stage, the authors implement the concept of knowledge
    distillation by transferring knowledge from $E_{vis}$ to $E_{cog}$, which together
    represent the teacher and student networks (Hinton et al., [2015](#bib.bib18)).
    The learned latent representation vectors significantly improve the reconstruction
    quality by capturing visual information, such as color, texture, object position,
    and attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec7f69fc7a23ba3e383104ac100dfe2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: RenD-VAE/GAN framework consists of three main components: dual VAE-based
    encoder, adversarial decoder, and discriminator. The visual encoder $E_{vis}$,
    cognitive encoder $E_{cog}$, generator, and discriminator were used during the
    training. During testing, only a trained cognitive encoder and generator were
    used for the inference. The red arrow denotes the transfer of knowledge from the
    teacher network $E_{vis}$ to the student network $E_{cog}$. The components in
    blue denote training on external unlabeled natural images (without fMRI activity)
    from ImageNet, which do not overlap with images in the train/test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VanRullen and Reddy ([2019](#bib.bib66)) utilized VAE network pretrained on
    CelebA dataset using GAN procedure to learn variational latent space. Similar
    to MozafariBigBiGAN framework, the authors learned a linear mapping between latent
    feature space and fMRI patterns, rather than using probabilistic inference (Güçlütürk
    et al., [2017](#bib.bib16)). In the training stage, the pretrained encoder from
    VAE-GAN is fixed and the linear mapping between latent feature space and fMRI
    patterns is learned. For the test stage, fMRI patterns are first translated into
    VAE latent codes via inverse mapping, and then these codes are used to reconstruct
    the faces. The latent space of a VAE is a variational layer that provides a meaningful
    description of each image and can represent faces and facial features as linear
    combinations of each other. Owing to the training objective of the VAE, the points
    which appear close in this space are mapped onto similar face images, which are
    always visually plausible. Therefore, the VAE’s latent space ensures that the
    brain decoding becomes more robust mapping errors. As a result, the produced reconstructions
    from VAE-GAN appear to be more realistic and closer to the original stimuli images.
    This method not only allows the reconstruction of naturally looking faces but
    also decodes face gender. In terms of architecture, the framework, which we refer
    to as VanRullenVAE-GAN, consists of three networks, as shown in Figure [7](#S4.F7
    "Figure 7 ‣ 4.2 Generative methods ‣ 4 Deep learning-based approaches for natural
    image reconstruction ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/936bf5407a7790c9ca20321bb95cdd57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: VanRullenVAE-GAN framework proposed by VanRullen and Reddy ([2019](#bib.bib66)).
    The encoder $\boldsymbol{E}$ maps a stimulus image onto the latent representation
    $\mathbf{z}$. The generator ${G}$ uses $\mathbf{z}$ to reconstruct the stimuli
    image. The pretrained components are shown in yellow.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Reconstruction evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The evaluation of reconstruction methods is based on human-based and image
    metrics, which we schematically present in Figure [8](#S5.F8 "Figure 8 ‣ 5 Reconstruction
    evaluation ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey").
    We first present human-based and image metrics and then describe the differences
    in image comparison settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d492a7091d430dc9348f3ea086d8119d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Image-metric-based and human-based evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Comparison of methods in terms of the used evaluation metrics. PCC
    stands for the Pearson correlation coefficient.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Human-based metrics | Image metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Reference |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Quantitative &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; survey &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Visual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; inspection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Traditional | PSM |'
  prefs: []
  type: TYPE_TB
- en: '| Seeliger et al. ([2018](#bib.bib58)) | ✓ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. ([2019b](#bib.bib60)) | ✓ | ✓ | Pairwise PCC | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. ([2019a](#bib.bib59)) | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Pairwise PCC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pairwise SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Beliy et al. ([2019](#bib.bib2)) | ✗ | ✓ | 2,5,10-way PCC | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Gaziv et al. ([2020](#bib.bib14)) | ✓ | ✓ | ✗ | 2,5,10-way PSM |'
  prefs: []
  type: TYPE_TB
- en: '| Qiao et al. ([2020](#bib.bib46)) | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PCC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| AlexNet |'
  prefs: []
  type: TYPE_TB
- en: '| Fang et al. ([2020](#bib.bib12)) | ✗ | ✓ | Pairwise PCC | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Mozafari et al. ([2020](#bib.bib39)) | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Pairwise PCC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pix-Comp (2-way PCC) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Inception-V3 |'
  prefs: []
  type: TYPE_TB
- en: '| Ren et al. ([2021](#bib.bib51)) | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Linear correlation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2,5,10-way PCC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✗ |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Human-based evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The intuitive method of measuring the quality of reconstruction in natural image
    reconstruction task is by employing human evaluators. Human-based evaluation can
    be conducted quantitatively and qualitatively through visual inspection.
  prefs: []
  type: TYPE_NORMAL
- en: For quantitative human-based assessment, a behavioral study involving human
    subjects is conducted. In this study the reconstructed image is compared to the
    original or several candidate images, containing the original image. From the
    given candidate images, subjects are instructed to choose the one that appears
    to have a higher resemblance to the original. Such behavioral studies can be conducted
    by employing human evaluators or using Amazon Mechanical Turk⁶⁶6[www.mturk.com](www.mturk.com)
    (Seeliger et al., [2018](#bib.bib58); Gaziv et al., [2020](#bib.bib14)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Owing to the additional time and human input required for human-based evaluation,
    several recent studies omit quantitative human evaluation in favor of qualitative
    visual inspections. For visual comparison, the set of original images and their
    reconstructions from different reconstruction methods are juxtaposed for ease
    of comparison (see Figures [9](#S6.F9 "Figure 9 ‣ 6.1 Visual comparison results
    ‣ 6 Fair comparison across the methods ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey") A and [9](#S6.F9 "Figure 9 ‣ 6.1 Visual comparison
    results ‣ 6 Fair comparison across the methods ‣ Natural Image Reconstruction
    from fMRI using Deep Learning: A Survey") B). Reconstructions are usually compared
    in terms of image sharpness/blurriness, matching shapes, colors, and low/high-level
    details. Many recent works focus on emphasizing the “naturalness” of their reconstructions,
    despite the reconstructions deviating significantly from the actual images in
    terms of the object category (see reconstructions in column 4 in Figure [9](#S6.F9
    "Figure 9 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison across the methods
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey") B for
    example).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although human-based evaluation is a more reliable measurement of the quality
    of the reconstructed image, it suffers from the following limitations. First,
    human-based evaluation is time consuming and expensive because the process requires
    a well-designed evaluation study and the recruitment of human subjects. Second,
    the results can be heavily affected by subjects’ physical and emotional conditions
    or external conditions, such as lighting or image display (Wang et al., [2004](#bib.bib67);
    Rolls, [2012](#bib.bib53)). Table [3](#S5.T3 "Table 3 ‣ 5 Reconstruction evaluation
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey") shows
    that only several studies conducted the quantitative human-based evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Image-metric-based evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As an alternative to human-based evaluation, image-metric-based evaluation is
    used to accurately and automatically assess image reconstruction quality. The
    use of image metrics for evaluation is more practical, and unlike human-based
    assessment, is unbiased towards external factors. However, the image-metric-based
    evaluation can provide only an approximation of the visual comparison mechanism
    inherent to a human subject, and thus are far from being perfect (Wang et al.,
    [2004](#bib.bib67)).
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, there exist various image metrics that can compare images at different
    levels of perceptual representation. Image metrics used in the visual decoding
    literature can be categorized into traditional metrics that capture low-level
    perceptual similarities and more recent ones that capture high-level perceptual
    similarity. The conventional metrics, which include the mean squared error (MSE),
    pixelwise Pearson correlation coefficient (PCC), structural similarity index (SSIM),
    and their variants, are computed in pixel space and capture low-level perceptual
    similarity. The metric that captures high-level perceptual similarity relies on
    multilevel feature extraction from DNN and can compare images at a higher level
    of perceptual representation. The high-level metric we considere here is called
    Perceptual Similarity Metric (PSM).
  prefs: []
  type: TYPE_NORMAL
- en: MSE is the simplest traditional metric for assessing image reconstruction quality.
    Given $x_{i}$ and $y_{i}$, which are the flattened one-dimensional representations
    of the original and the reconstructed images, the MSE estimated over $N$ samples
    is computed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MSE=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-y_{i})^{2}.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Several characteristics of MSE, including simplicity of implementation and
    fast computation, make it a widely used performance metric in signal processing.
    However, MSE shows poor correspondence to human visual perception, due to some
    of the underlying assumptions: MSE is independent of the spatial relationship
    between image pixels and considers each of them to be equally important (Wang
    et al., [2004](#bib.bib67)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'PCC is widely used in statistical analysis to measure the linear relationship
    between two variables. The following equation is used to compute the pixelwise
    Pearson correlation between the flattened 1-D representations of the original
    image $x$ and the reconstructed image $y$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PCC(x,y)=\frac{\sum(x-\mu_{x})(y-\mu_{y})}{\sqrt{\sum(x-\mu_{x})^{2}\sum(y-\mu_{y})^{2}}},$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mu_{x}$ and $\mu_{y}$ are the mean intensities of the flattened one-dimensional
    vectors $x$ and $y$, respectively. PCC is the most common metric used across the
    surveyed works (see Table [3](#S5.T3 "Table 3 ‣ 5 Reconstruction evaluation ‣
    Natural Image Reconstruction from fMRI using Deep Learning: A Survey")), with
    slight variations in naming and implementation: Pairwise PCC (Shen et al., [2019b](#bib.bib60),
    [a](#bib.bib59)), pixel correlation (Ren et al., [2021](#bib.bib51)), Pix-Comp
    (Mozafari et al., [2020](#bib.bib39)), and n-way PCC (Beliy et al., [2019](#bib.bib2)).
    The limitation of PCC is its sensitivity to changes in the edge intensity or edge
    misalignment. Thus, the metric tends to assign higher scores to blurry images
    than to images with distinguishable but misaligned shapes (Beliy et al., [2019](#bib.bib2)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'SSIM is widely used image similarity metric that captures structural information
    from images. Wang et al. proposed SSIM as a quality assessment metric that resembles
    the characteristics of the human visual perception (Wang et al., [2004](#bib.bib67)).
    Unlike PCC, which treats each pixel of the image independently, SSIM measures
    the similarity of spatially close pixels between the reconstructed and original
    images. Given two images, SSIM is computed as a weighted combination of three
    comparative measures: luminance, contrast, and structure. Assuming an equal contribution
    of each measure, the SSIM is first computed locally between the corresponding
    windows $p$ and $q$ of images $x$ and $y$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SSIM(p,q)=\frac{(2\mu_{p}\mu_{q}+C_{1})(2\sigma_{pq}+C_{2})}{(\mu_{p}^{2}+\mu_{q}^{2}+C_{1})(\sigma_{p}^{2}+\sigma_{q}^{2}+C_{2})},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mu_{p}$ and $\mu_{q}$ are the mean intensity values of $p$ and $q$,
    respectively; $\sigma_{p}^{2}$ and $\sigma_{q}^{2}$ are the variances of $p$ and
    $q$, respectively; $\sigma_{pq}$ is the covariance of $p$ and $q$, and $C_{1}$
    and $C_{2}$ are constants that ensure stability when the denominator is close
    to zero. The global SSIM score is computed as the average of all $M$ local SSIM
    scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SSIM(x,y)=\sum_{i=1}^{M}SSIM(p_{i},q_{i}).$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: PSM. Despite the wide adoption of SSIM as a perceptual metric, it compares poorly
    with many characteristics of human perception (Zhang et al., [2018](#bib.bib70)).
    Several studies, including Güçlütürk et al. ([2017](#bib.bib16)), Qiao et al.
    ([2018](#bib.bib47)), Mozafari et al. ([2020](#bib.bib39)), and Gaziv et al. ([2020](#bib.bib14)),
    emphasize the importance of higher-level perceptual similarity over lower-level
    metrics in evaluation because of the better correspondence of higher-level perceptual
    similarity to human perceptual judgments (Zhang et al., [2018](#bib.bib70)).
  prefs: []
  type: TYPE_NORMAL
- en: 'As the general principle, a CNN is used for extracting hierarchical multilayer
    features of input images, which are further compared across layers using a distance
    metric of choice. However, the definition and implementation of the perceptual
    similarity metric in terms of the distance metric or feature extraction network
    vary across studies. For example, Qiao et al. ([2020](#bib.bib46)) utilized five
    convolutional layers of the AlexNet (Krizhevsky et al., [2012](#bib.bib28)) to
    extract hierarchical features. The other study by Mozafari et al. ([2020](#bib.bib39)),
    proposed a high-level similarity measure, which measures perceptual similarity
    based on the output of only the last layer of Inception-V3 (Szegedy et al., [2016](#bib.bib63)).
    Finally, Gaziv et al. ([2020](#bib.bib14)) used the PSM definition proposed in
    (Zhang et al., [2018](#bib.bib70)) with the pretrained AlexNet with linear calibration.
    Following Gaziv et al. ([2020](#bib.bib14)), we provide a PSM definition by Zhang
    et al. ([2018](#bib.bib70)) in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d\left(x,y\right)=\sum_{l}\frac{1}{H_{l}W_{l}}\sum_{h,w}\left\&#124;w_{l}\odot\left({f}_{x}^{l}-{f}_{y}^{l}\right)\right\&#124;_{2}^{2},$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $d\left(x,y\right)$ is the distance between the original image $x$ and
    the reconstructed image $y$. ${f}_{x}^{l},{f}_{y}^{l}$ represent layerwise activations
    normalized across channels for layer $l$. The activations are scaled channelwise
    by vector $w_{l}\in\mathbb{R}^{C_{l}}$, spatially averaged, and summed layerwise.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the underlying CNN model used for computing the PSM should be selected
    cautiously. Because many studies use pretrained CNN models, it is important to
    avoid using the same model for both training and evaluation, which may lead to
    a potential bias in evaluation. For example, several methods, including Shen et al.
    ([2019b](#bib.bib60)) and Beliy et al. ([2019](#bib.bib2)), used VGG-19 (Simonyan
    and Zisserman, [2015](#bib.bib61)) for pretraining. Therefore, the VGG-19 model
    should not be used for evaluation, as the objective of evaluation and optimization
    functions would be the same, and the evaluation would produce a higher similarity
    between original and reconstructed images.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Image comparison setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We describe three image comparison settings existing in literature: 1) one-to-one
    comparison, 2) pairwise comparison, and 3) $n$-way comparison. Each of these comparison
    settings can work with any image or human-based metric of choice.'
  prefs: []
  type: TYPE_NORMAL
- en: One-to-one is the simplest comparison setting which computes the similarity
    score of a reconstruction against ground truth using the given metric, for example,
    MSE or PCC. However, the absolute values of qualitative metrics computed only
    on a single pair of original and reconstructed images are challenging to interpret
    (Beliy et al., [2019](#bib.bib2)). Therefore, pairwise similarity and $n$-way
    identification are often used to measure the reconstruction quality across the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pairwise comparison analysis is performed by comparing a reconstructed image
    with two candidate images: the ground-truth image and the image selected from
    the remaining set, resulting in a total of $N(N-1)$ comparisons:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $score=\frac{1}{N(N-1)}\sum_{i=1}^{N}\sum_{\begin{subarray}{c}j=1\\ j\neq
    i\end{subarray}}^{N}\sigma\left(m\left(y_{i},x_{i}\right),m\left(y_{i},x_{j}\right)\right),$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $m$ is the metric of interest and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma(a,b)=\left\{\begin{array}[]{ll}1&amp;a>b\\ 0&amp;\text{ otherwise
    }\end{array}\right.$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'The trial is considered correct if the metric score of the reconstructed image
    with the corresponding stimulus image is higher than that with the nonrelevant
    stimulus image. For metrics that imply that the lower, the better (such as MSE),
    the expression in the equation [7](#S5.E7 "In 5.3 Image comparison setting ‣ 5
    Reconstruction evaluation ‣ Natural Image Reconstruction from fMRI using Deep
    Learning: A Survey") is modified to find the smallest value. Finally, the percentage
    of total correct trials is computed as the ratio of correct trials among all trials
    (Shen et al., [2019b](#bib.bib60), [a](#bib.bib59); Beliy et al., [2019](#bib.bib2)).
    The chance-level accuracy is $50\%$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In $\boldsymbol{n}$-way identification each reconstructed image is compared
    to $n$ randomly selected candidate images, including the ground truth. Several
    studies, including Beliy et al. ([2019](#bib.bib2)) and Ren et al. ([2021](#bib.bib51)),
    used $n=2,5,10$ for the $n$-way identification accuracy computed using PCC. In
    a more recent work, Gaziv et al. ([2020](#bib.bib14)) report $n=2,5,10,50$-way
    identification accuracy based on PSM. An addition source of confusion is the absence
    of naming conventions: Ren et al. ([2021](#bib.bib51)) and Mozafari et al. ([2020](#bib.bib39))
    referred to $n$-way identification accuracy computed with PCC as Pixel Correlation
    and pix-Comp, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Fair comparison across the methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For fair comparison of the existing methods, we chose those that satisfied
    one of the following criteria: 1) the availability of the complete code for reproducing
    the results and 2) the availability of reconstructed images for running the evaluation.
    This allowed us to compare five state-of-the-art methods on the DIR dataset, both
    visually (Section [6.1](#S6.SS1 "6.1 Visual comparison results ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey")) and quantitatively (Section [6.2](#S6.SS2 "6.2 Quantitative comparison
    results on natural images from DIR ‣ 6 Fair comparison across the methods ‣ Natural
    Image Reconstruction from fMRI using Deep Learning: A Survey")). For the GOD,
    because of the lack of a complete set of reconstructions for the chosen methods,
    we only present a visual comparison in Section [6.1](#S6.SS1 "6.1 Visual comparison
    results ‣ 6 Fair comparison across the methods ‣ Natural Image Reconstruction
    from fMRI using Deep Learning: A Survey"). Visual comparison for vim-1 datasets
    is provided in Supplementary Material.'
  prefs: []
  type: TYPE_NORMAL
- en: Our analysis of recent works on natural image reconstruction reveals that only
    a few comply with good machine learning practices regarding the fairness of evaluation.
    Unfair evaluation can be reflected in the comparison across different datasets,
    selecting specific subjects in reporting the results, and discrepancies in using
    the evaluation metrics. This motivated us to perform a rigorous empirical evaluation
    of the methods, i.e. cross-subject evaluation across common metrics using a common
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation on a common dataset. To standardize the objective evaluation process,
    we perform the quantitative assessment on the DIR dataset for methods that we
    found to be reproducible, that is, ShenDNN, ShenDNN+DGN, ShenGAN, and BeliyEncDec.
    For FangSSGAN, we ran an evaluation based on the reconstructions provided by the
    authors.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to distinguish the five-subject GOD dataset (Horikawa and Kamitani,
    [2017](#bib.bib19)) from the three-subject DIR dataset (Shen et al., [2019b](#bib.bib60)),
    which uses the same stimuli images but is quite different in terms of the image
    presentation experiment and characteristics of fMRI activity data. Our choice
    of the DIR as a common natural image dataset is due to the following reasons.
    First, unlike the similar GOD dataset, DIR was acquired specifically for the natural
    image reconstruction task and contains a larger number of training samples due
    to increased number of repeats in image presentation experiment. In addition,
    this dataset might be of interest for studying the generalizability of natural
    image reconstruction methods to artificial shapes, which we describe in detail
    in Supplementary Material.
  prefs: []
  type: TYPE_NORMAL
- en: When training ShenDNN, ShenDNN+DGN, and BeliyEncDec on the DIR, we used the
    original training settings. For ShenGAN, we used the pretrained model provided
    by the authors. To maximize the amount of training data, each presented stimulus
    sample was treated as an individual sample (Shen et al., [2019a](#bib.bib59)).
    For reconstruction, we averaged the test fMRI activations across trials corresponding
    to the same stimulus to increase the signal-to-noise ratio. This resulted in 6,000
    training and 50 test fMRI-image samples. Note that BeliyEncDec was initially implemented
    for GOD dataset. For BeliyEncDec, averaging both training and test fMRI samples
    across the repeats resulted in the best performance. This confirms with the authors’
    observation that an increased number of fMRI repeats results in improved reconstruction
    (Beliy et al., [2019](#bib.bib2)). Additionally, we normalized training fMRI vectors
    to have a zero mean and unit standard deviation. The mean and standard deviation
    of the training fMRI data were used to normalize the test fMRI data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation across common metrics. We perform the evaluation on natural images
    from the DIR based on MSE, PCC, SSIM, and PSM metrics described in Section [5](#S5
    "5 Reconstruction evaluation ‣ Natural Image Reconstruction from fMRI using Deep
    Learning: A Survey"). We notice that there is no consensus among recent works
    on a standard set of evaluation metrics (see Table [3](#S5.T3 "Table 3 ‣ 5 Reconstruction
    evaluation ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey")).
    Moreover, several studies introduce new evaluation metrics or use variations of
    existing metrics, potentially more favorable for their results. In contrast, we
    present an evaluation of the methods across all the image metrics used in the
    related methods.'
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that different methods generate output images of
    various sizes due to memory restrictions and variations in the pretrained model
    (we refer to Supplementary Material for details on output image resolutions).
    The evaluation metrics can be sensitive to the size of the image and the choice
    of upscaling or downscaling algorithms. For fairness, we rescaled the reconstructions
    for the DIR to the common size and use a bicubic algorithm for image resizing.
    We evaluated the reconstructed images using a resolution of 256 $\times$ 256 pixels,
    which is the highest among the chosen methods. For methods with a lower reconstruction
    image size, we applied image upscaling.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Visual comparison results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/896871d6b3e66b33c6fac331860dd476.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: (A) Visual comparison of methods on Deep Image Reconstruction dataset
    for subject 1\. The reconstructions for all methods except for Fang et al. ([2020](#bib.bib12))
    are obtained by reproducing the experiments. For (A) and (B), the stimulus images
    are shown in the first column. The corresponding reconstructed images from each
    method are shown in the subsequent columns. (B) Visual comparison of the methods
    on the GOD dataset. Due to the unavailability of complete reconstruction data
    for GOD, visual reconstructions correspond to the same image stimuli but different
    subjects. For BeliyEncDec and GazivEncDec, we present the reconstruction for subject
    3\. The reconstructions for all methods are provided by the authors or reported
    in the original papers. SeeligerDCGAN uses the average of the stimuli representations
    for the three subjects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [9](#S6.F9 "Figure 9 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") A shows the reconstructions of sample stimuli images from the test
    set, corresponding to subject 1 from DIR dataset. The reconstructions from all
    methods show a close resemblance to the original images in terms of the object
    shape and position. GAN-based methods, i.e., ShenDNN+DGN and ShenGAN, produce
    sharper and smoother-looking images but in some cases render semantic details
    absent in the original stimuli (which is confirmed by lower pixelwise MSE and
    PCC scores). Reconstructions by FangSSGAN are also natural looking and close to
    real images in terms of shape and color. This is attributed to using a generator
    network conditioned on both shape and semantic information, which preserves low-level
    features, such as texture or shape. Reconstruction by nonGAN⁷⁷7By “nonGAN” methods,
    we mean the models that do not take advantage of the GAN training procedure. BeliyEncDec
    are blurry but accurately capture the shape of the stimuli objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we present the reconstructions for GOD dataset in Figure [9](#S6.F9
    "Figure 9 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison across the methods
    ‣ Natural Image Reconstruction from fMRI using Deep Learning: A Survey") B. Similar
    to DIR dataset, the GAN-based methods MozafariBigBiGAN and RenD-VAE/GAN produce
    the most natural-looking images. Visually, MozafariBigBiGAN outperforms other
    methods in terms of naturalness. However, this comes at the cost of rendering
    object categories and details different from those presented in the original stimuli.
    We identified GazivEncDec and RenD-VAE/GAN as performing relatively better on
    the reconstruction of shape and color. GazivEncDec is superior in reconstructing
    high-level details of the image, including shape and background. RenD-VAE/GAN
    visually outperforms other methods for the reconstruction of color, background,
    and lower-level details. For GazivEncDec, a significant improvement in the reconstruction
    accuracy was achieved owing to the introduced perceptual similarity loss. According
    to Ren et al. ([2021](#bib.bib51)), the key factors boosting the reconstruction
    quality of the RenD-VAE/GAN include the VAE-GAN architecture instead of the standard
    conditional GAN and visual-feature guidance implemented via GAN-based knowledge
    distillation. In SeeligerDCGAN and BeliyEncDec, the reconstructions are blurry,
    which could be due to the use of pixelwise MSE loss (Seeliger et al., [2018](#bib.bib58)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eac3e8e05eedb8af687132690c187eee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Reconstructions for two images across three subjects from DIR dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the DIR dataset comprises three-subject data, we additionally show the
    reconstructions across the methods corresponding to three different subjects in
    Figure [10](#S6.F10 "Figure 10 ‣ 6.1 Visual comparison results ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey"). The reconstructions are shown for the two natural image stimuli. Depending
    on the subject, the reconstructions by different methods show varying degrees
    of resemblance to the original stimuli. For example, the reconstructions from
    ShenDNN+DGN, ShenGAN, and BeliyEncDec are visually better for subject 1, whereas,
    in reconstructions by other methods, neither color nor shape was preserved. This
    shows that the selection of a subject in reporting results can lead to a biased
    evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Quantitative comparison results on natural images from DIR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7292145a2123bf4f89068ec10b784ae9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Average $n$-way accuracy results computed across subjects using
    (A) MSE, (B) PCC, (C) SSIM, and (D) PSM metrics on natural images from the DIR.
    The horizontal dashed lines indicate the chance level for each metric. The full-way
    comparison corresponds to using all the images in the test set, that is, 50 natural
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To eliminate the bias of selecting a specific subject for evaluation, we present
    both subject-specific and cross-subject average results across multiple metrics
    on natural images from DIR. For comprehensive evaluation, we use three comparison
    settings described in Section [5](#S5 "5 Reconstruction evaluation ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey"): 1) one-to-one comparison;
    2) pairwise comparison; and 3) $n$-way comparison. The pairwise evaluation results
    for natural images across the metrics are shown in Table [4](#S6.T4 "Table 4 ‣
    6.2 Quantitative comparison results on natural images from DIR ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey"). The $n$-way scores for natural images are presented in Figure [11](#S6.F11
    "Figure 11 ‣ 6.2 Quantitative comparison results on natural images from DIR ‣
    6 Fair comparison across the methods ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey"). We find that one-to-one results are not well
    suited for cross-method comparison. We therefore present a one-to-one comparison
    in Supplementary Material. The quantitative evaluation of methods is presented
    based on low-level MSE, PCC, and SSIM metrics first, followed by a comparison
    using a high-level PSM metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Pairwise evaluation across the methods on natural images from the
    DIR dataset. The best results are presented in bold. ↑ indicates the higher the
    better.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Subject | Method | MSE ↑ | PCC↑ | SSIM ↑ | PSM ↑ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | ShenDNN | 75.80 | 80.69 | 75.59 | 77.67 |'
  prefs: []
  type: TYPE_TB
- en: '| ShenDNN+DGN | 74.53 | 78.98 | 61.27 | 86.61 |'
  prefs: []
  type: TYPE_TB
- en: '| ShenGAN | 71.67 | 79.06 | 62.08 | 92.33 |'
  prefs: []
  type: TYPE_TB
- en: '| BeliyEncDec | 76.94 | 86.08 | 59.67 | 73.14 |'
  prefs: []
  type: TYPE_TB
- en: '| FangSSGAN | 67.71 | 67.18 | 60.37 | 76.12 |'
  prefs: []
  type: TYPE_TB
- en: '| S2 | ShenDNN | 74.98 | 77.27 | 70.82 | 77.14 |'
  prefs: []
  type: TYPE_TB
- en: '| ShenDNN+DGN | 70.78 | 75.43 | 59.55 | 86.41 |'
  prefs: []
  type: TYPE_TB
- en: '| ShenGAN | 68.65 | 74.20 | 59.51 | 90.41 |'
  prefs: []
  type: TYPE_TB
- en: '| BeliyEncDec | 71.18 | 76.20 | 58.94 | 75.22 |'
  prefs: []
  type: TYPE_TB
- en: '| FangSSGAN | 64.00 | 66.24 | 58.69 | 73.71 |'
  prefs: []
  type: TYPE_TB
- en: '| S3 | ShenDNN | 79.71 | 81.47 | 75.06 | 76.20 |'
  prefs: []
  type: TYPE_TB
- en: '| ShenDNN+DGN | 73.59 | 75.02 | 60.24 | 86.20 |'
  prefs: []
  type: TYPE_TB
- en: '| ShenGAN | 74.12 | 78.98 | 62.08 | 91.88 |'
  prefs: []
  type: TYPE_TB
- en: '| BeliyEncDec | 78.61 | 81.47 | 60.53 | 76.20 |'
  prefs: []
  type: TYPE_TB
- en: '| FangSSGAN | 67.88 | 66.45 | 59.96 | 79.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Average result | ShenDNN | 76.83$\pm$2.53 | 79.81$\pm$2.24 | 73.82$\pm$2.62
    | 77.01$\pm$0.74 |'
  prefs: []
  type: TYPE_TB
- en: '| ShenDNN+DGN | 72.97$\pm$1.95 | 76.48$\pm$2.18 | 60.35$\pm$0.86 | 86.41$\pm$0.20
    |'
  prefs: []
  type: TYPE_TB
- en: '| ShenGAN | 71.48$\pm$2.74 | 77.41$\pm$2.78 | 61.22$\pm$1.48 | 91.54$\pm$1.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| BeliyEncDec | 75.58$\pm$3.90 | 81.25$\pm$4.94 | 59.71$\pm$0.80 | 74.86$\pm$1.56
    |'
  prefs: []
  type: TYPE_TB
- en: '| FangSSGAN | 66.53$\pm$2.19 | 66.63$\pm$0.49 | 59.67$\pm$0.87 | 76.29$\pm$2.66
    |'
  prefs: []
  type: TYPE_TB
- en: 'Performance using low-level metrics. Based on the average results across the
    subjects shown in Table [4](#S6.T4 "Table 4 ‣ 6.2 Quantitative comparison results
    on natural images from DIR ‣ 6 Fair comparison across the methods ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey") and Figure [11](#S6.F11
    "Figure 11 ‣ 6.2 Quantitative comparison results on natural images from DIR ‣
    6 Fair comparison across the methods ‣ Natural Image Reconstruction from fMRI
    using Deep Learning: A Survey") A, B, C, two nonGAN methods lead on low-level
    metrics, namely ShenDNN and BeliyEncDec. Together, they outperform other baselines
    across three low-level pairwise metrics (i.e., pairwise MSE, pairwise PCC, and
    pairwise SSIM) as well as across $n$-way MSE and PPC metrics. The high performance
    of BeliyEncDec on low-level metrics can be attributed to efficient low-level feature
    extraction via encoder–decoder architecture and to the self-supervised training
    procedure with the extended set of unlabeled images and fMRI data. The high performance
    of ShenDNN on low-level metrics is potentially due to iterative pixel-level optimization
    of the reconstructed image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance using the high-level PSM metric. Additionally, we compare the selected
    methods on the PSM implemented using AlexNet. From Table [4](#S6.T4 "Table 4 ‣
    6.2 Quantitative comparison results on natural images from DIR ‣ 6 Fair comparison
    across the methods ‣ Natural Image Reconstruction from fMRI using Deep Learning:
    A Survey") and Figure [11](#S6.F11 "Figure 11 ‣ 6.2 Quantitative comparison results
    on natural images from DIR ‣ 6 Fair comparison across the methods ‣ Natural Image
    Reconstruction from fMRI using Deep Learning: A Survey") D, we can see that ShenGAN
    performs the best on the high-level PSM metric, computed in a pairwise, and $n$-way
    manner across the subjects and on averages. Overall, GAN-based methods, including
    ShenGAN, ShenDNN+DGN, and FangSSGAN, which were reported to produce more natural-looking
    images, achieved the top three average results in most cases. This supports the
    motivation to utilize PSM for measuring high-level visual similarity in images,
    especially for GAN-based methods whose strength lies in reconstructing high-level
    visual features and more natural-looking images. We attribute the improved performance
    of the three methods to using a pretrained generator network and the superior
    performance of ShenGAN and ShenDNN+DGN to the use of multilayer DNN features for
    computing the multi-level feature loss. Notably, the performance of all metrics
    reduces as the $n$-way comparison becomes increasingly harder with an increasing
    number of samples being used in the comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even with a relatively small number of the available open-source reconstruction
    frameworks, the visual and quantitative results presented in this work can give
    a general idea of which architectural solution, benchmark dataset, or evaluation
    framework can be chosen for experimental purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the target of the reconstruction task, it is vital to consider
    the trade-off between the “naturalness” and the fidelity of the reconstruction.
    Generative methods rely on GAN or VAE-GAN-based architectures to produce the most
    natural-looking images and correspondingly higher PSM scores. However, they often
    require either external data for training or the use of pretrained network components.
    The availability of external image datasets for training becomes a significant
    factor for generating high-quality images for GAN. Most importantly, the methods
    that perform best at “naturalness” do not guarantee that the object categories
    of reconstruction will always match those of the original images, as in the case
    of MozafariBigBiGAN. Other non-generative methods developed for natural image
    reconstruction, such as BeliyEncDec or ShenDNN, do not produce realistic-looking
    images. However, whenever the fidelity of the reconstructions is preferable, these
    non-generative methods should be considered, as they exhibit closer similarity
    to the original images in terms of low-level features, which are supported both
    visually and quantitatively.
  prefs: []
  type: TYPE_NORMAL
- en: In this work we advocate the fairness in reconstruction evaluation procedure
    and discuss several criteria which should be standardized across the methods.
    At the same time, we believe that the evaluation procedure presented in this work
    can be further improved in the following ways.
  prefs: []
  type: TYPE_NORMAL
- en: Availability of large-scale imaging data. The primary challenge for current
    deep learning-based methods is that they are required to resolve the limitation
    of small-size fMRI data. Nowadays, the lack of training data is compensated by
    pretraining DNN components on external image data (Shen et al., [2019a](#bib.bib59),
    [b](#bib.bib60); Beliy et al., [2019](#bib.bib2)), self-supervision on additional
    image-fMRI pairs (Beliy et al., [2019](#bib.bib2); Gaziv et al., [2020](#bib.bib14))
    and generation of new surrogate fMRI via pretrained encoding models (St-Yves and
    Naselaris, [2018](#bib.bib62)). Several brain imaging datasets are available for
    reconstruction tasks. However, larger scale datasets are still required. The availability
    of large-scale imaging data may improve current state-of-the-art results and foster
    research on reconstructing more complex and dynamic visual perception, including
    imagined images or videos. This, in turn, may lead to broader adoption of the
    proposed frameworks for real-world purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Developing new computational similarity metrics corresponding to human vision.
    While some of the deep learning methods achieve encouraging results on high-level
    perceptual similarity metrics, an open question about the correspondence of these
    computer-based metrics to human vision remains. Because most accuracy evaluation
    metrics are oriented toward computer vision tasks, they often fail to capture
    the characteristics of human vision. Research in this direction might further
    advance natural image reconstruction by developing more advanced learning and
    evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presented an overview of state-of-the-art methods for natural image
    reconstruction task using deep learning. These methods were compared on multiple
    scales, including architectural design, benchmark datasets, and evaluation metrics.
    We highlighted several ambiguities with the existing evaluation and presented
    a standardized empirical assessment of the methods. This evaluation procedure
    can help researchers in performing a more comprehensive comparative analysis and
    elucidating the reason for the effectiveness of their method. We hope this study
    will serve as a foundation for future research on natural image reconstruction
    targeting fair and rigorous comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Author Contributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ZR: Conceptualization, Methodology, Software, Writing, Evaluation. QJ: Software,
    Evaluation, Writing - review and editing. XL: Conceptualization, Data curation,
    Writing - Original draft preparation. TM: Supervision, Writing - review & editing.'
  prefs: []
  type: TYPE_NORMAL
- en: Funding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was partly supported by JST CREST (Grant Number JPMJCR1687), JSPS
    Grant-in-Aid for Scientific Research (Grant Number 21K12042, 17H01785), and the
    New Energy and Industrial Technology Development Organization (Grant Number JPNP20006).
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Professor Yukiyasu Kamitani from Neuroinformatics Lab at Kyoto University
    for providing valuable comments that improved the manuscript. We also thank Katja
    Seeliger, Milad Mozafari, Guy Gaziv, Roman Beliy, and Tao Fang for sharing their
    reconstructed images and evaluation codes with us.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bandettini (2012) Bandettini, P. A. (2012). Twenty years of functional MRI:
    The science and the stories. *NeuroImage* 62, 575–588. [10.1016/j.neuroimage.2012.04.026](https:/doi.org/10.1016/j.neuroimage.2012.04.026).
    Number: 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beliy et al. (2019) Beliy, R., Gaziv, G., Hoogi, A., Strappini, F., Golan,
    T., and Irani, M. (2019). From voxels to pixels and back: Self-supervision in
    natural-image reconstruction from fMRI. In *Advances in Neural Information Processing
    Systems 32*, eds. H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alché-Buc,
    E. Fox, and R. Garnett (Curran Associates, Inc.). 6517–6527'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2015) Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and
    Yuille, A. L. (2015). Semantic Image Segmentation with Deep Convolutional Nets
    and Fully Connected CRFs. In *3rd International Conference on Learning Representations,
    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*, eds.
    Y. Bengio and Y. LeCun
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2014) Chen, M., Han, J., Hu, X., Jiang, X., Guo, L., and Liu,
    T. (2014). Survey of encoding and decoding of visual stimulus via FMRI: an image
    analysis perspective. *Brain Imaging and Behavior* 8, 7–23. [10.1007/s11682-013-9238-z](https:/doi.org/10.1007/s11682-013-9238-z)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2014) Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D.,
    Bougares, F., Schwenk, H., et al. (2014). Learning Phrase Representations using
    RNN Encoder–Decoder for Statistical Machine Translation. In *Proceedings of the
    2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (Doha,
    Qatar: Association for Computational Linguistics), 1724–1734. [10.3115/v1/D14-1179](https:/doi.org/10.3115/v1/D14-1179)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chrabaszcz et al. (2017) Chrabaszcz, P., Loshchilov, I., and Hutter, F. (2017).
    A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets. *arXiv:1707.08819
    [cs]* ArXiv: 1707.08819'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Corel Corporation (1994) [Dataset] Corel Corporation (1994). Corel stock photo
    library. ISBN: 9780969660552 Place: Ottawa OCLC: 872594087'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
    L. (2009). ImageNet: A large-scale hierarchical image database. In *2009 IEEE
    Conference on Computer Vision and Pattern Recognition*. 248–255. [10.1109/CVPR.2009.5206848](https:/doi.org/10.1109/CVPR.2009.5206848).
    ISSN: 1063-6919'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Donahue and Simonyan (2019) Donahue, J. and Simonyan, K. (2019). Large Scale
    Adversarial Representation Learning. In *Advances in Neural Information Processing
    Systems*, eds. H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alché-Buc, E. Fox,
    and R. Garnett (Curran Associates, Inc.), vol. 32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dosovitskiy and Brox (2016) Dosovitskiy, A. and Brox, T. (2016). Inverting Visual
    Representations With Convolutional Networks. 4829–4837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eickenberg et al. (2017) Eickenberg, M., Gramfort, A., Varoquaux, G., and Thirion,
    B. (2017). Seeing it all: Convolutional network layers map the function of the
    human visual system. *NeuroImage* 152, 184–194. [10.1016/j.neuroimage.2016.10.001](https:/doi.org/10.1016/j.neuroimage.2016.10.001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2020) Fang, T., Qi, Y., and Pan, G. (2020). Reconstructing Perceptive
    Images from Brain Activity by Shape-Semantic GAN. *Advances in Neural Information
    Processing Systems* 33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fujiwara et al. (2013) Fujiwara, Y., Miyawaki, Y., and Kamitani, Y. (2013).
    Modular encoding and decoding models derived from bayesian canonical correlation
    analysis. *Neural Computation* 25, 979–1005. [10.1162/NECO_a_00423](https:/doi.org/10.1162/NECO_a_00423).
    Number: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaziv et al. (2020) Gaziv, G., Beliy, R., Granot, N., Hoogi, A., Strappini,
    F., Golan, T., et al. (2020). *Self-Supervised Natural Image Reconstruction and
    Rich Semantic Classification from Brain Activity*. preprint, Neuroscience. [10.1101/2020.09.06.284794](https:/doi.org/10.1101/2020.09.06.284794)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow et al. (2014) Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu,
    B., Warde-Farley, D., Ozair, S., et al. (2014). Generative adversarial nets. In
    *Proceedings of the 27th International Conference on Neural Information Processing
    Systems - Volume 2* (Cambridge, MA, USA: MIT Press), NIPS’14, 2672–2680'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Güçlütürk et al. (2017) Güçlütürk, Y., Güçlü, U., Seeliger, K., Bosch, S., van
    Lier, R., and van Gerven, M. A. (2017). Reconstructing perceived faces from brain
    activations with deep adversarial neural decoding. *Advances in Neural Information
    Processing Systems* 30, 4246–4257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haxby et al. (2001) Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A.,
    Schouten, J. L., and Pietrini, P. (2001). Distributed and Overlapping Representations
    of Faces and Objects in Ventral Temporal Cortex. *Science* 293, 2425–2430. [10.1126/science.1063736](https:/doi.org/10.1126/science.1063736).
    Number: 5539 Publisher: American Association for the Advancement of Science Section:
    Research Article'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hinton et al. (2015) Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling
    the Knowledge in a Neural Network. *arXiv:1503.02531 [cs, stat]* ArXiv: 1503.02531'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horikawa and Kamitani (2017) Horikawa, T. and Kamitani, Y. (2017). Generic
    decoding of seen and imagined objects using hierarchical visual features. *Nature
    Communications* 8, 15037. [10.1038/ncomms15037](https:/doi.org/10.1038/ncomms15037).
    Number: 1 Publisher: Nature Publishing Group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isola et al. (2017) Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2017).
    Image-To-Image Translation With Conditional Adversarial Networks. 1125–1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jakub Langr and Vladimir Bok (2019) Jakub Langr and Vladimir Bok (2019). *GANs
    in Action* (Manning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kamitani and Tong (2005) Kamitani, Y. and Tong, F. (2005). Decoding the visual
    and subjective contents of the human brain. *Nature Neuroscience* 8, 679–685.
    [10.1038/nn1444](https:/doi.org/10.1038/nn1444). Number: 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamnitsas et al. (2017) Kamnitsas, K., Ledig, C., Newcombe, V. F. J., Simpson,
    J. P., Kane, A. D., Menon, D. K., et al. (2017). Efficient multi-scale 3D CNN
    with fully connected CRF for accurate brain lesion segmentation. *Medical Image
    Analysis* 36, 61–78. [10.1016/j.media.2016.10.004](https:/doi.org/10.1016/j.media.2016.10.004)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kay et al. (2008) Kay, K. N., Naselaris, T., Prenger, R. J., and Gallant, J. L.
    (2008). Identifying natural images from human brain activity. *Nature* 452, 352–355.
    [10.1038/nature06713](https:/doi.org/10.1038/nature06713). Number: 7185'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Welling (2014) Kingma, D. P. and Welling, M. (2014). Auto-Encoding
    Variational Bayes. *arXiv:1312.6114 [cs, stat]* ArXiv: 1312.6114'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kriegeskorte (2015) Kriegeskorte, N. (2015). Deep Neural Networks: A New Framework
    for Modeling Biological Vision and Brain Information Processing. *Annual Review
    of Vision Science* 1, 417–446. [10.1146/annurev-vision-082114-035447](https:/doi.org/10.1146/annurev-vision-082114-035447)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky (2009) Krizhevsky, A. (2009). *Learning multiple layers of features
    from tiny images*. Tech. rep.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012).
    ImageNet Classification with Deep Convolutional Neural Networks. *Advances in
    Neural Information Processing Systems* 25, 1097–1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Larsen et al. (2016) Larsen, A. B. L., Sønderby, S. K., Larochelle, H., and
    Winther, O. (2016). Autoencoding beyond pixels using a learned similarity metric.
    In *International Conference on Machine Learning* (PMLR), 1558–1566. ISSN: 1938-7228'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun et al. (1989) LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard,
    R. E., Hubbard, W., et al. (1989). Backpropagation Applied to Handwritten Zip
    Code Recognition. *Neural Computation* 1, 541–551. [10.1162/neco.1989.1.4.541](https:/doi.org/10.1162/neco.1989.1.4.541).
    Number: 4 Publisher: MIT Press'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
    Ramanan, D., et al. (2014). Microsoft COCO: Common Objects in Context. In *Computer
    Vision – ECCV 2014*, eds. D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars (Cham:
    Springer International Publishing), Lecture Notes in Computer Science, 740–755.
    [10.1007/978-3-319-10602-1_48](https:/doi.org/10.1007/978-3-319-10602-1_48)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2015) Liu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep Learning
    Face Attributes in the Wild. In *2015 IEEE International Conference on Computer
    Vision (ICCV)*. 3730–3738. [10.1109/ICCV.2015.425](https:/doi.org/10.1109/ICCV.2015.425).
    ISSN: 2380-7504'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logothetis and Sheinberg (1996) Logothetis, N. K. and Sheinberg, D. L. (1996).
    Visual Object Recognition. *Annual Review of Neuroscience* 19, 577–621. [10.1146/annurev.ne.19.030196.003045](https:/doi.org/10.1146/annurev.ne.19.030196.003045).
    Number: 1 _eprint: https://doi.org/10.1146/annurev.ne.19.030196.003045'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maaten (2009) Maaten, L. V. D. (2009). A New Benchmark Dataset for Handwritten
    Character Recognition (Tilburg University, Tilburg, The Netherlands)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahendran and Vedaldi (2015) Mahendran, A. and Vedaldi, A. (2015). Understanding
    deep image representations by inverting them. In *2015 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*. 5188–5196. [10.1109/CVPR.2015.7299155](https:/doi.org/10.1109/CVPR.2015.7299155).
    ISSN: 1063-6919'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martin et al. (2001) Martin, D., Fowlkes, C., Tal, D., and Malik, J. (2001).
    A database of human segmented natural images and its application to evaluating
    segmentation algorithms and measuring ecological statistics. In *Proceedings Eighth
    IEEE International Conference on Computer Vision. ICCV 2001*. vol. 2, 416–423
    vol.2. [10.1109/ICCV.2001.937655](https:/doi.org/10.1109/ICCV.2001.937655)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minaee et al. (2021) Minaee, S., Boykov, Y. Y., Porikli, F., Plaza, A. J.,
    Kehtarnavaz, N., and Terzopoulos, D. (2021). Image Segmentation Using Deep Learning:
    A Survey. *IEEE Transactions on Pattern Analysis and Machine Intelligence* , 1–1[10.1109/TPAMI.2021.3059968](https:/doi.org/10.1109/TPAMI.2021.3059968).
    Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miyawaki et al. (2008) Miyawaki, Y., Uchida, H., Yamashita, O., Sato, M.-a.,
    Morito, Y., Tanabe, H. C., et al. (2008). Visual Image Reconstruction from Human
    Brain Activity using a Combination of Multiscale Local Image Decoders. *Neuron*
    60, 915–929. [10.1016/j.neuron.2008.11.004](https:/doi.org/10.1016/j.neuron.2008.11.004).
    Number: 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mozafari et al. (2020) Mozafari, M., Reddy, L., and VanRullen, R. (2020). Reconstructing
    Natural Scenes from fMRI Patterns using BigBiGAN. In *2020 International Joint
    Conference on Neural Networks (IJCNN)* (Glasgow, United Kingdom: IEEE), 1–8. [10.1109/IJCNN48605.2020.9206960](https:/doi.org/10.1109/IJCNN48605.2020.9206960)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naselaris et al. (2011) Naselaris, T., Kay, K. N., Nishimoto, S., and Gallant,
    J. L. (2011). Encoding and decoding in fMRI. *NeuroImage* 56, 400–410. [10.1016/j.neuroimage.2010.07.073](https:/doi.org/10.1016/j.neuroimage.2010.07.073).
    Number: 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naselaris et al. (2009) Naselaris, T., Prenger, R. J., Kay, K. N., Oliver,
    M., and Gallant, J. L. (2009). Bayesian Reconstruction of Natural Images from
    Human Brain Activity. *Neuron* 63, 902–915. [10.1016/j.neuron.2009.09.006](https:/doi.org/10.1016/j.neuron.2009.09.006).
    Number: 6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nestor et al. (2020) Nestor, A., Lee, A. C. H., Plaut, D. C., and Behrmann,
    M. (2020). The Face of Image Reconstruction: Progress, Pitfalls, Prospects. *Trends
    in Cognitive Sciences* 24, 747–759. [10.1016/j.tics.2020.06.006](https:/doi.org/10.1016/j.tics.2020.06.006).
    Publisher: Elsevier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ogawa et al. (1990) Ogawa, S., Lee, T. M., Kay, A. R., and Tank, D. W. (1990).
    Brain magnetic resonance imaging with contrast dependent on blood oxygenation.
    *Proceedings of the National Academy of Sciences of the United States of America*
    87, 9868–9872. Number: 24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinto et al. (2009) Pinto, N., Doukhan, D., DiCarlo, J. J., and Cox, D. D.
    (2009). A High-Throughput Screening Approach to Discovering Good Forms of Biologically
    Inspired Visual Representation. *PLOS Computational Biology* 5, e1000579. [10.1371/journal.pcbi.1000579](https:/doi.org/10.1371/journal.pcbi.1000579).
    Publisher: Public Library of Science'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poldrack and Farah (2015) Poldrack, R. A. and Farah, M. J. (2015). Progress
    and challenges in probing the human brain. *Nature* 526, 371–379. [10.1038/nature15692](https:/doi.org/10.1038/nature15692).
    Number: 7573 Publisher: Nature Publishing Group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao et al. (2020) Qiao, K., Chen, J., Wang, L., Zhang, C., Tong, L., and Yan,
    B. (2020). BigGAN-based Bayesian Reconstruction of Natural Images from Human Brain
    Activity. *Neuroscience* 444, 92–105. [10.1016/j.neuroscience.2020.07.040](https:/doi.org/10.1016/j.neuroscience.2020.07.040)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao et al. (2018) Qiao, K., Zhang, C., Wang, L., Chen, J., Zeng, L., Tong,
    L., et al. (2018). Accurate Reconstruction of Image Stimuli From Human Functional
    Magnetic Resonance Imaging Based on the Decoding Model With Capsule Network Architecture.
    *Frontiers in Neuroinformatics* 12, 62. [10.3389/fninf.2018.00062](https:/doi.org/10.3389/fninf.2018.00062)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quiroga et al. (2005) Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., and
    Fried, I. (2005). Invariant visual representation by single neurons in the human
    brain. *Nature* 435, 1102–1107. [10.1038/nature03687](https:/doi.org/10.1038/nature03687).
    Number: 7045 Publisher: Nature Publishing Group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radford et al. (2016) Radford, A., Metz, L., and Chintala, S. (2016). Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks.
    In *arXiv:1511.06434 [cs]*. ArXiv: 1511.06434'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rakhimberdina et al. (2020) Rakhimberdina, Z., Liu, X., and Murata, T. (2020).
    Population Graph-Based Multi-Model Ensemble Method for Diagnosing Autism Spectrum
    Disorder. *Sensors* 20, 6001. [10.3390/s20216001](https:/doi.org/10.3390/s20216001).
    Number: 21 Publisher: Multidisciplinary Digital Publishing Institute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2021) Ren, Z., Li, J., Xue, X., Li, X., Yang, F., Jiao, Z., et al.
    (2021). Reconstructing seen image from brain activity by visually-guided cognitive
    representation and adversarial learning. *NeuroImage* [10.1016/j.neuroimage.2020.117602](https:/doi.org/10.1016/j.neuroimage.2020.117602)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roelfsema et al. (2018) Roelfsema, P. R., Denys, D., and Klink, P. C. (2018).
    Mind Reading and Writing: The Future of Neurotechnology. *Trends in Cognitive
    Sciences* 22, 598–610. [10.1016/j.tics.2018.04.001](https:/doi.org/10.1016/j.tics.2018.04.001).
    Number: 7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rolls (2012) Rolls, E. T. (2012). Invariant Visual Object and Face Recognition:
    Neural and Computational Bases, and a Model, VisNet. *Frontiers in Computational
    Neuroscience* 6. [10.3389/fncom.2012.00035](https:/doi.org/10.3389/fncom.2012.00035).
    Publisher: Frontiers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. (2015).
    U-Net: Convolutional Networks for Biomedical Image Segmentation. In *Medical Image
    Computing and Computer-Assisted Intervention – MICCAI 2015*, eds. N. Navab, J. Hornegger,
    W. M. Wells, and A. F. Frangi (Cham: Springer International Publishing), Lecture
    Notes in Computer Science, 234–241. [10.1007/978-3-319-24574-4_28](https:/doi.org/10.1007/978-3-319-24574-4_28)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schoenmakers et al. (2013) Schoenmakers, S., Barth, M., Heskes, T., and van
    Gerven, M. (2013). Linear reconstruction of perceived images from human brain
    activity. *NeuroImage* 83, 951–961. [10.1016/j.neuroimage.2013.07.043](https:/doi.org/10.1016/j.neuroimage.2013.07.043)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schomaker et al. (2000) Schomaker, L., Vuurpijl, L., and Schomaker, L. (2000).
    *Forensic writer identification: a benchmark data set and a comparison of two
    systems* (NICI (NIjmegen Institute of Cognitive Information), Katholieke Universiteit
    Nijmegen). Type: Other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schrimpf et al. (2018) Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J.,
    Rajalingham, R., Issa, E. B., et al. (2018). Brain-Score: Which Artificial Neural
    Network for Object Recognition is most Brain-Like? *bioRxiv* , 407007[10.1101/407007](https:/doi.org/10.1101/407007).
    Publisher: Cold Spring Harbor Laboratory Section: New Results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeliger et al. (2018) Seeliger, K., Güçlü, U., Ambrogioni, L., Güçlütürk, Y.,
    and van Gerven, M. A. J. (2018). Generative adversarial networks for reconstructing
    natural images from brain activity. *NeuroImage* 181, 775–785. [10.1016/j.neuroimage.2018.07.043](https:/doi.org/10.1016/j.neuroimage.2018.07.043)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2019a) Shen, G., Dwivedi, K., Majima, K., Horikawa, T., and Kamitani,
    Y. (2019a). End-to-End Deep Image Reconstruction From Human Brain Activity. *Frontiers
    in Computational Neuroscience* 13. [10.3389/fncom.2019.00021](https:/doi.org/10.3389/fncom.2019.00021).
    Publisher: Frontiers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2019b) Shen, G., Horikawa, T., Majima, K., and Kamitani, Y. (2019b).
    Deep image reconstruction from human brain activity. *PLOS Computational Biology*
    15, e1006633. [10.1371/journal.pcbi.1006633](https:/doi.org/10.1371/journal.pcbi.1006633).
    Number: 1 Publisher: Public Library of Science'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonyan and Zisserman (2015) Simonyan, K. and Zisserman, A. (2015). Very Deep
    Convolutional Networks for Large-Scale Image Recognition. *arXiv:1409.1556 [cs]*
    ArXiv: 1409.1556'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'St-Yves and Naselaris (2018) St-Yves, G. and Naselaris, T. (2018). Generative
    Adversarial Networks Conditioned on Brain Activity Reconstruct Seen Images. In
    *2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)*. 1054–1061.
    [10.1109/SMC.2018.00187](https:/doi.org/10.1109/SMC.2018.00187). ISSN: 2577-1655'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szegedy et al. (2016) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and
    Wojna, Z. (2016). Rethinking the Inception Architecture for Computer Vision. In
    *2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)* (Las
    Vegas, NV, USA: IEEE), 2818–2826. [10.1109/CVPR.2016.308](https:/doi.org/10.1109/CVPR.2016.308)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thirion et al. (2006) Thirion, B., Duchesnay, E., Hubbard, E., Dubois, J.,
    Poline, J.-B., Lebihan, D., et al. (2006). Inverse retinotopy: inferring the visual
    content of images from brain activation patterns. *NeuroImage* 33, 1104–1116.
    [10.1016/j.neuroimage.2006.06.062](https:/doi.org/10.1016/j.neuroimage.2006.06.062)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'van Gerven et al. (2010) van Gerven, M. A. J., de Lange, F. P., and Heskes,
    T. (2010). Neural Decoding with Hierarchical Generative Models. *Neural Computation*
    22, 3127–3142. [10.1162/NECO_a_00047](https:/doi.org/10.1162/NECO_a_00047). Number:
    12 Publisher: MIT Press'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VanRullen and Reddy (2019) VanRullen, R. and Reddy, L. (2019). Reconstructing
    faces from fMRI patterns using deep generative neural networks. *Communications
    Biology* 2, 1–10. [10.1038/s42003-019-0438-y](https:/doi.org/10.1038/s42003-019-0438-y).
    Number: 1 Publisher: Nature Publishing Group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2004) Wang, Z., Bovik, A., Sheikh, H., and Simoncelli, E. (2004).
    Image quality assessment: from error visibility to structural similarity. *IEEE
    Transactions on Image Processing* 13, 600–612. [10.1109/TIP.2003.819861](https:/doi.org/10.1109/TIP.2003.819861).
    Conference Name: IEEE Transactions on Image Processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Xu, K., Du, C., Li, C., Zhu, J., and Zhang, B. (2021). Learning
    Implicit Generative Models by Teaching Density Estimators. In *Machine Learning
    and Knowledge Discovery in Databases*, eds. F. Hutter, K. Kersting, J. Lijffijt,
    and I. Valera (Cham: Springer International Publishing), Lecture Notes in Computer
    Science, 239–255. [10.1007/978-3-030-67661-2_15](https:/doi.org/10.1007/978-3-030-67661-2_15)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Zhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L. (2017).
    Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising.
    *IEEE Transactions on Image Processing* 26, 3142–3155. [10.1109/TIP.2017.2662206](https:/doi.org/10.1109/TIP.2017.2662206).
    Conference Name: IEEE Transactions on Image Processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and
    Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual
    Metric. In *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*
    (Salt Lake City, UT: IEEE), 586–595. [10.1109/CVPR.2018.00068](https:/doi.org/10.1109/CVPR.2018.00068)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Zhang, X., Yao, L., Wang, X., Monaghan, J. J. M., Mcalpine,
    D., and Zhang, Y. (2020). A survey on deep learning-based non-invasive brain signals:
    recent advances and new frontiers. *Journal of Neural Engineering* [10.1088/1741-2552/abc902](https:/doi.org/10.1088/1741-2552/abc902)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2017) Zhao, J. J., Mathieu, M., and LeCun, Y. (2017). Energy-based
    Generative Adversarial Networks. In *5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings* (OpenReview.net)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
