["```py\n**from** **fastai.imports** **import** *\n**from** **fastai.structured** **import** *\n**from** **sklearn.ensemble** **import** RandomForestRegressor, RandomForestClassifier\n**from** **IPython.display** **import** display\n**from** **sklearn** **import** metricsPATH = \"data/bulldozers/\"\n\ndf_raw = pd.read_feather('tmp/bulldozers-raw')\ndf_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')**def** split_vals(a,n): **return** a[:n], a[n:]\nn_valid = 12000\nn_trn = len(df_trn)-n_valid\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)\nraw_train, raw_valid = split_vals(df_raw, n_trn)x_sub = X_train[['YearMade', 'MachineHoursCurrentMeter']]\n```", "```py\n**class** **TreeEnsemble**():\n  **def** __init__(self, x, y, n_trees, sample_sz, min_leaf=5):\n    np.random.seed(42)\n    self.x,self.y,self.sample_sz,self.min_leaf = \n                                           x,y,sample_sz,min_leaf\n    self.trees = [self.create_tree() **for** i **in** range(n_trees)]\n\n  **def** create_tree(self):\n    rnd_idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n    **return** DecisionTree(self.x.iloc[rnd_idxs], self.y[rnd_idxs],\n                            min_leaf=self.min_leaf)\n```", "```py\n**def** predict(self, x):\n    **return** np.mean([t.predict(x) **for** t **in** self.trees], axis=0)**class** **DecisionTree**():\n    **def** __init__(self, x, y, idxs=**None**, min_leaf=5):\n        self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leafm = TreeEnsemble(X_train, y_train, n_trees=10, sample_sz=1000, \n                 min_leaf=3)\n```", "```py\n**class** **DecisionTree**():\n    **def** __init__(self, x, y, idxs=**None**, min_leaf=5):\n        **if** idxs **is** **None**: idxs=np.arange(len(y))\n        self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leaf\n        self.n,self.c = len(idxs), x.shape[1]\n        self.val = np.mean(y[idxs])\n        self.score = float('inf')\n        self.find_varsplit()\n```", "```py\n *# This just does one decision; we'll make it recursive later*\n    **def** find_varsplit(self):\n        **for** i **in** range(self.c): self.find_better_split(i)\n\n    *# We'll write this later!*\n    **def** find_better_split(self, var_idx): **pass**\n\n    @property\n    **def** split_name(self): **return** self.x.columns[self.var_idx]\n\n    @property\n    **def** split_col(self): \n        **return** self.x.values[self.idxs,self.var_idx]\n\n    @property\n    **def** is_leaf(self): **return** self.score == float('inf')\n\n    **def** __repr__(self):\n        s = f'n: **{self.n}**; val:**{self.val}**'\n        **if** **not** self.is_leaf:\n            s += f'; score:**{self.score}**; split:**{self.split}**; var:\n                   **{self.split_name}**'\n        **return** s\n```", "```py\nm = TreeEnsemble(X_train, y_train, n_trees=10, sample_sz=1000,\n                 min_leaf=3)\nm.trees[0]*n: 1000; val:10.079014121552744*\n```", "```py\nens = TreeEnsemble(x_sub, y_train, 1, 1000)\ntree = ens.trees[0]\nx_samp,y_samp = tree.x, tree.y\n```", "```py\nm = RandomForestRegressor(n_estimators=1, max_depth=1,\n                          bootstrap=**False**)\nm.fit(x_samp, y_samp)\ndraw_tree(m.estimators_[0], x_samp, precision=2)\n```", "```py\n**def** find_better_split(self, var_idx):\n   x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs] **for** i **in** range(1,self.n-1):\n      lhs = x<=x[i]\n      rhs = x>x[i]\n      **if** rhs.sum()==0: **continue**\n      lhs_std = y[lhs].std()\n      rhs_std = y[rhs].std()\n      curr_score = lhs_std*lhs.sum() + rhs_std*rhs.sum()\n      **if** curr_score<self.score: \n        self.var_idx,self.score,self.split = var_idx,curr_score,x[i]\n```", "```py\n%timeit find_better_split(tree,1)\ntree76.6 ms \u00b1 11.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)*n: 1000; val:10.079014121552744; score:681.0184057251435; split:3744.0; var:MachineHoursCurrentMeter*\n```", "```py\nfind_better_split(tree,0); tree*n: 1000; val:10.079014121552744; score:658.5510186055949; split:1974.0; var:YearMade*\n```", "```py\ntree = TreeEnsemble(x_sub, y_train, 1, 1000).trees[0]**def** std_agg(cnt, s1, s2): **return** math.sqrt((s2/cnt) - (s1/cnt)**2)\n\n**def** find_better_split_foo(self, var_idx):\n  x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n\n  sort_idx = np.argsort(x)\n  sort_y,sort_x = y[sort_idx], x[sort_idx]\n  rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n  lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n\n  **for** i **in** range(0,self.n-self.min_leaf-1):\n    xi,yi = sort_x[i],sort_y[i]\n    lhs_cnt += 1; rhs_cnt -= 1\n    lhs_sum += yi; rhs_sum -= yi\n    lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n    **if** i<self.min_leaf **or** xi==sort_x[i+1]:\n      **continue**\n\n    lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n    rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n    curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n    **if** curr_score<self.score: \n      self.var_idx,self.score,self.split = var_idx,curr_score,xi\n```", "```py\n %timeit find_better_split_foo(tree,1)\ntree2.2 ms \u00b1 148 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)n: 1000; val:10.079014121552744; score:658.5510186055565; split:1974.0; var:YearMade\n```", "```py\nfind_better_split_foo(tree,0); treen: 1000; val:10.079014121552744; score:658.5510186055565; split:1974.0; var:YearMade\n```", "```py\nDecisionTree.find_better_split = find_better_split_foo\n```", "```py\nDecisionTree.find_better_split = find_better_split_foo\n```", "```py\ntree = TreeEnsemble(x_sub, y_train, 1, 1000).trees[0]; treen: 1000; val:10.079014121552744; score:658.5510186055565; split:1974.0; var:YearMade\n```", "```py\nm = RandomForestRegressor(n_estimators=1, max_depth=2, \n                          bootstrap=**False**) \nm.fit(x_samp, y_samp) \ndraw_tree(m.estimators_[0], x_samp, precision=2)\n```", "```py\n**def** find_varsplit(self):\n    **for** i **in** range(self.c): self.find_better_split(i)\n    **if** self.is_leaf: **return**\n    x = self.split_col\n    lhs = np.nonzero(x<=self.split)[0]\n    rhs = np.nonzero(x>self.split)[0]\n    self.lhs = DecisionTree(self.x, self.y, self.idxs[lhs])\n    self.rhs = DecisionTree(self.x, self.y, self.idxs[rhs])\n```", "```py\nDecisionTree.find_varsplit = find_varsplit\n```", "```py\ntree = TreeEnsemble(x_sub, y_train, 1, 1000).trees[0]; treen: 1000; val:10.079014121552744; score:658.5510186055565; split:1974.0; var:YearMade\n```", "```py\ntree.lhsn: 159; val:9.660892662981706; score:76.82696888346362; split:2800.0; var:MachineHoursCurrentMeter\n```", "```py\ntree.rhsn: 841; val:10.158064432982941; score:571.4803525045031; split:2005.0; var:YearMade\n```", "```py\ntree.lhs.lhsn: 150; val:9.619280538108496; score:71.15906938383463; split:1000.0; var:YearMade\n```", "```py\ntree.lhs.rhsn: 9; val:10.354428077535193\n```", "```py\ncols = ['MachineID', 'YearMade', 'MachineHoursCurrentMeter',\n        'ProductSize', 'Enclosure','Coupler_System', 'saleYear']\n```", "```py\n%time tree = TreeEnsemble(X_train[cols], y_train, 1, 1000).trees[0]\nx_samp,y_samp = tree.x, tree.yCPU times: user 288 ms, sys: 12 ms, total: 300 ms\nWall time: 297 ms\n```", "```py\nm = RandomForestRegressor(n_estimators=1, max_depth=3, \n                          bootstrap=**False**)\nm.fit(x_samp, y_samp)\ndraw_tree(m.estimators_[0], x_samp, precision=2, ratio=0.9, size=7)\n```", "```py\n**def** predict(self, x): \n    **return** np.array([self.predict_row(xi) **for** xi **in** x])\n```", "```py\n**def** predict_row(self, xi):\n    **if** self.is_leaf: **return** self.val\n    t = self.lhs **if** xi[self.var_idx]<=self.split **else** self.rhs\n    **return** t.predict_row(xi)DecisionTree.predict_row = predict_row\n```", "```py\n**if** something:\n    x= do1()\n**else**:\n    x= do2()\n```", "```py\nx = do1() **if** something **else** do2()\n```", "```py\nx = something ? do1() : do2()\n```", "```py\nDecisionTree.predict = predict\n```", "```py\n%time preds = tree.predict(X_valid[cols].values)*CPU times: user 156 ms, sys: 4 ms, total: 160 ms\nWall time: 162 ms*\n```", "```py\nplt.scatter(preds, y_valid, alpha=0.05)\n```", "```py\nmetrics.r2_score(preds, y_valid)0.50371522136882341\n```", "```py\nm = RandomForestRegressor(n_estimators=1, min_samples_leaf=5, bootstrap=**False**)\n%time m.fit(x_samp, y_samp)\npreds = m.predict(X_valid[cols].values)\nplt.scatter(preds, y_valid, alpha=0.05)\n```", "```py\nmetrics.r2_score(preds, y_valid)0.47541053100694797\n```", "```py\n**class** **TreeEnsemble**():\n  **def** __init__(self, x, y, n_trees, sample_sz, min_leaf=5):\n    np.random.seed(42)\n    self.x,self.y,self.sample_sz,self.min_leaf = \n                                        x,y,sample_sz,min_leaf\n    self.trees = [self.create_tree() **for** i **in** range(n_trees)] **def** create_tree(self):\n    idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n    **return** DecisionTree(self.x.iloc[idxs], self.y[idxs], \n                    idxs=np.array(range(self.sample_sz)), \n                    min_leaf=self.min_leaf)\n\n  **def** predict(self, x):\n    **return** np.mean([t.predict(x) **for** t **in** self.trees], axis=0)**def** std_agg(cnt, s1, s2): **return** math.sqrt((s2/cnt) - (s1/cnt)**2)**class** **DecisionTree**():\n  **def** __init__(self, x, y, idxs, min_leaf=5):\n    self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leaf\n    self.n,self.c = len(idxs), x.shape[1]\n    self.val = np.mean(y[idxs])\n    self.score = float('inf')\n    self.find_varsplit()\n\n  **def** find_varsplit(self):\n    **for** i **in** range(self.c): self.find_better_split(i)\n    **if** self.score == float('inf'): **return**\n    x = self.split_col\n    lhs = np.nonzero(x<=self.split)[0]\n    rhs = np.nonzero(x>self.split)[0]\n    self.lhs = DecisionTree(self.x, self.y, self.idxs[lhs])\n    self.rhs = DecisionTree(self.x, self.y, self.idxs[rhs]) **def** find_better_split(self, var_idx):\n    x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n    sort_idx = np.argsort(x)\n    sort_y,sort_x = y[sort_idx], x[sort_idx]\n    rhs_cnt,rhs_sum,rhs_sum2 = self.n,sort_y.sum(),(sort_y**2).sum()\n    lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0. **for** i **in** range(0,self.n-self.min_leaf-1):\n      xi,yi = sort_x[i],sort_y[i]\n      lhs_cnt += 1; rhs_cnt -= 1\n      lhs_sum += yi; rhs_sum -= yi\n      lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n      **if** i<self.min_leaf **or** xi==sort_x[i+1]:\n       **continue** lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n      rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n      curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n      **if** curr_score<self.score: \n       self.var_idx,self.score,self.split = var_idx,curr_score,xi @property\n  **def** split_name(self): **return** self.x.columns[self.var_idx]\n\n  @property\n  **def** split_col(self): **return** self.x.values[self.idxs,self.var_idx] @property\n  **def** is_leaf(self): **return** self.score == float('inf')\n\n  **def** __repr__(self):\n    s = f'n: **{self.n}**; val:**{self.val}**'\n    **if** **not** self.is_leaf:\n      s += f'; score:**{self.score}**; split:**{self.split}**; var:\n           **{self.split_name}**'\n    **return** s **def** predict(self, x):\n    **return** np.array([self.predict_row(xi) **for** xi **in** x]) **def** predict_row(self, xi):\n    **if** self.is_leaf: **return** self.val\n    t = self.lhs **if** xi[self.var_idx]<=self.split **else** self.rhs\n    **return** t.predict_row(xi)\n```", "```py\nens = TreeEnsemble(X_train[cols], y_train, 5, 1000)preds = ens.predict(X_valid[cols].values)plt.scatter(y_valid, preds, alpha=0.1, s=6);\n```", "```py\nmetrics.r2_score(y_valid, preds)*0.71011741571071241*\n```", "```py\n%load_ext Cython\n```", "```py\n**def** fib1(n):\n    a, b = 0, 1\n    **while** b < n:\n        a, b = b, a + b\n```", "```py\n%%cython\n**def** fib2(n):\n    a, b = 0, 1\n    **while** b < n:\n        a, b = b, a + b\n```", "```py\n%%cython\n**def** fib3(int n):\n    **cdef** int b = 1\n    **cdef** int a = 0\n    **cdef** int t = 0\n    **while** b < n:\n        t = a\n        a = b\n        b = a + b\n```", "```py\n%timeit fib1(50)*705 ns \u00b1 62.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)*%timeit fib2(50)*362 ns \u00b1 26.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)*%timeit fib3(50)*70.7 ns \u00b1 4.07 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)*\n```"]