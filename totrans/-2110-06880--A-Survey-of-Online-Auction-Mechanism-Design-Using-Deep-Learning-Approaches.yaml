- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:50:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:50:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.06880] A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.06880] 使用深度学习方法的在线拍卖机制设计综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.06880](https://ar5iv.labs.arxiv.org/html/2110.06880)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.06880](https://ar5iv.labs.arxiv.org/html/2110.06880)
- en: A Survey of Online Auction Mechanism Design Using Deep Learning Approaches
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习方法的在线拍卖机制设计综述
- en: Zhanhao Zhang
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张展豪
- en: Department of Statistics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学系
- en: Columbia University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: zz2760@columbia.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: zz2760@columbia.edu
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Online auction has been very widespread in the recent years. Platform administrators
    are working hard to refine their auction mechanisms that will generate high profits
    while maintaining a fair resource allocation. With the advancement of computing
    technology and the bottleneck in theoretical frameworks, researchers are shifting
    gears towards online auction designs using deep learning approaches. In this article,
    we summarized some common deep learning infrastructures adopted in auction mechanism
    designs and showed how these architectures are evolving. We also discussed how
    researchers are tackling with the constraints and concerns in the large and dynamic
    industrial settings. Finally, we pointed out several currently unresolved issues
    for future directions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在线拍卖在近年来变得非常普及。平台管理员正在努力完善他们的拍卖机制，以实现高利润，同时保持公平的资源分配。随着计算技术的进步和理论框架的瓶颈，研究人员正转向使用深度学习方法的在线拍卖设计。在本文中，我们总结了一些在拍卖机制设计中采用的常见深度学习基础设施，并展示了这些架构的演变过程。我们还讨论了研究人员如何在大型和动态的工业环境中应对约束和关注点。最后，我们指出了几个目前尚未解决的问题，以供未来的研究方向参考。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Auction has been adopted as a way to negotiate the exchanges of goods and commodities
    for centuries. Traditionally, Generalized second price auction (GSP) and Vickrey–Clarke–Groves
    auction (VCG) are widely used. However, GSP is no longer a truthful mechanism
    if a seller has more than one item to bid. VCG is an auction mechanism based on
    sealed-second price auction, where winners are charged on the reductions of social
    welfare of other participants. Nevertheless, the VCG mechanism generates low seller
    revenues and does not enforce monotinicity of seller’s revenues in the set of
    bidders and the amounts bid. It is also a non-truthful mechanism that is susceptible
    to multiple bids under same person or collusion of losing bidders [[5](#bib.bib5)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 拍卖作为一种交换商品和货物的方式已经被采用了几个世纪。传统上，广义第二价格拍卖（GSP）和维克雷-克拉克-格罗夫斯拍卖（VCG）被广泛使用。然而，如果卖方有多个物品进行竞标，GSP
    不再是一个真实的机制。VCG 是一种基于封闭第二价格拍卖的拍卖机制，获胜者根据其他参与者社会福利的减少来收取费用。然而，VCG 机制产生的卖方收入较低，并且未能在竞标者集和竞标金额中强制卖方收入的单调性。它也是一种不真实的机制，易受到同一人多次竞标或失败竞标者的勾结[[5](#bib.bib5)]。
- en: Auction mechanisms with the properties of incentive compatiblility (IC) and
    individual rationality (IR) are highly desirable. If an auction is IC, then all
    bidders will truthfully reveal their private valuations of the items, so that
    platform administrators do not have the burden of considering bidders’ strategic
    behaviors and are therefore able to build a reliable and predictable system. All
    agents are guaranteed to have non-negative utilities if the auction system is
    IR, and it is a very important feature that allows the system to retain its customers
    in the long run.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有激励兼容性（IC）和个体理性（IR）属性的拍卖机制非常理想。如果一个拍卖是IC的，那么所有竞标者将真实地揭示他们对物品的私人估价，这样平台管理员就不必考虑竞标者的策略行为，因此能够建立一个可靠且可预测的系统。如果拍卖系统是IR的，那么所有参与者都保证具有非负效用，这是一项非常重要的特性，使得系统能够在长期内留住客户。
- en: The groundbreaking work by Myerson [[20](#bib.bib20)] has defined the optimal
    strategyproof auction for selling a single item, but limited progress has been
    made in characterizing strategyproof and revenue-maximizing auctions beyond this
    setting [[14](#bib.bib14)]. The dynamic nature of online auction platforms [[6](#bib.bib6),
    [27](#bib.bib27), [31](#bib.bib31)] has made the problems more challenging, as
    the bidders, items, and platform’s objectives are changing over time. In the meantime,
    multiple performance metrics are required to be taken into considerations in order
    to make the auction system attractive to bidders, sellers, and the platform [[16](#bib.bib16),
    [27](#bib.bib27), [31](#bib.bib31)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Myerson 的开创性工作[[20](#bib.bib20)]定义了销售单一物品的最佳策略无关拍卖，但在此设置之外，对于策略无关和收益最大化的拍卖的特征化进展有限[[14](#bib.bib14)]。在线拍卖平台的动态性质[[6](#bib.bib6),
    [27](#bib.bib27), [31](#bib.bib31)]使问题变得更加复杂，因为竞标者、物品和平台的目标随着时间的推移而变化。同时，需要考虑多个性能指标，以使拍卖系统对竞标者、卖家和平台具有吸引力[[16](#bib.bib16),
    [27](#bib.bib27), [31](#bib.bib31)]。
- en: With the advancement of technology, researchers are shifting gears towards deep
    learning approaches for the design of auction mechanisms. To the best of our knowledge,
    the deep learning architecture is built on top of either the hybrid multiple linear
    perceptron infrastructure, the RegretNet infrastructure [[9](#bib.bib9)], the
    reinforcement learning infrastructure, or the DeepSet infrastructure [[30](#bib.bib30)].
    Blessed with the modern computing power, researchers are not only able to maximize
    the revenue, but also dealing with data sparsity and high-dimensional data, optimizing
    multiple performance metrics, preventing fraud, and enhancing fairness.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术的进步，研究人员正在将焦点转向深度学习方法来设计拍卖机制。根据我们所知，深度学习架构建立在混合多层感知机基础设施、RegretNet基础设施[[9](#bib.bib9)]、强化学习基础设施或DeepSet基础设施[[30](#bib.bib30)]之上。得益于现代计算能力，研究人员不仅能够最大化收益，还能处理数据稀疏性和高维数据，优化多个性能指标，防止欺诈，并提升公平性。
- en: 'This article is organized as follows: we will first introduce the four common
    infrastructure of the deep neural networks for auction mechanism design. Then,
    we will discuss how researchers are tackling with the constraints and concerns
    other than maximizing the total revenue. Lastly, we will point out some unresolved
    issues and potential future directions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文组织结构如下：我们将首先介绍拍卖机制设计中深度神经网络的四种常见基础设施。接着，我们将讨论研究人员如何解决除最大化总收益之外的约束和问题。最后，我们将指出一些未解决的问题和潜在的未来方向。
- en: 2 Hybrid Multiple Linear Perceptron (MLP) Infrastructure
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 混合多层感知机 (MLP) 基础设施
- en: Many neural network structures are built by stacking several MLP structure into
    a cohesive one [[24](#bib.bib24), [32](#bib.bib32), [25](#bib.bib25), [17](#bib.bib17)].
    The network is usually comprised of more than one components, where each components
    is a fully-connected feedforward neural network.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经网络结构通过将几个MLP结构堆叠成一个连贯的网络来构建[[24](#bib.bib24), [32](#bib.bib32), [25](#bib.bib25),
    [17](#bib.bib17)]。该网络通常由多个组件组成，每个组件是一个完全连接的前馈神经网络。
- en: The most delicate architecture is the MenuNet [[24](#bib.bib24)], which is comprised
    of a mechanism network and a buyer network. The mechanism takes a one-dimensional
    1 as input and outputs an allocation matrix and a pricing vector. The allocation
    matrix contains the allocation of all items, which is obtained by a fully-connected
    layer followed by a sigmoid activation function. The payment vector is obtained
    by simply multiplying the constant 1 by a vector and it is used to represent the
    prices for different menu items. The buyer network doesn’t require any training.
    It takes the outputs from the mechanism network and computes the final utility
    based on buyer’s value profile. The training of MenuNet is very fast as the network
    structure is very simple. It is built upon the taxation principle [[28](#bib.bib28)],
    which states that simply letting the buyer do the selection can give an IC mechanism.
    It does not require buyer’s utility function since the network only outputs buyer’s
    strategy. It does not make any assumptions about buyer’s valuation and does not
    require any additional constraints (such as IC and IR) to be enforced to the network.
    Theoretical proofs have shown that the MenuNet always return revenue optimal mechanism
    with IC satisfied for menus of size 2 or 3.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最精致的架构是 MenuNet [[24](#bib.bib24)]，它由机制网络和买家网络组成。机制网络以一维 1 为输入，输出分配矩阵和定价向量。分配矩阵包含所有物品的分配，该矩阵通过一个全连接层及其后跟的
    sigmoid 激活函数获得。支付向量是通过将常数 1 乘以一个向量获得的，用于表示不同菜单项的价格。买家网络不需要任何训练。它从机制网络获取输出，并根据买家的价值配置计算最终效用。由于网络结构非常简单，MenuNet
    的训练非常快。它基于税收原则 [[28](#bib.bib28)]，该原则表明，仅让买家进行选择即可提供 IC 机制。它不需要买家的效用函数，因为网络只输出买家的策略。它不对买家的估值做任何假设，也不需要对网络施加任何额外的约束（如
    IC 和 IR）。理论证明表明，MenuNet 总是返回在菜单大小为 2 或 3 时 IC 满足的收入最优机制。
- en: Zhou et al has proposed Deep Interest Network [[32](#bib.bib32)] for the use
    of click-through rate prediction. It is built on the structure of MLP but aims
    to conquer the bottleneck caused by the fixed-length representation vector used
    in traditional MLP, which has limited ability in capturing user’s diverse interests
    from rich historical behaviors. The authors designed a local activation unit that
    can adaptively learn the representation of user interests from historical behaviors
    with respect to a certain advertisements. The data adaptive activation they adopted
    is Dice (Equation [1](#S2.E1 "In 2 Hybrid Multiple Linear Perceptron (MLP) Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches")),
    which is a generalization of PReLu [[13](#bib.bib13)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou 等人提出了用于点击率预测的 Deep Interest Network [[32](#bib.bib32)]。该网络基于 MLP 结构，但旨在解决传统
    MLP 中固定长度表示向量所带来的瓶颈，这种固定长度表示向量在捕捉用户丰富历史行为中的多样兴趣方面能力有限。作者设计了一种局部激活单元，能够根据特定广告从历史行为中自适应地学习用户兴趣的表示。他们采用的数据自适应激活是
    Dice（方程 [1](#S2.E1 "在 2 混合多层感知器 (MLP) 基础设施 ‣ 基于深度学习方法的在线拍卖机制设计调查")），这是 PReLu [[13](#bib.bib13)]
    的一种推广。
- en: '|  | $f(s)=p(s)\cdot s+(1-p(s))\cdot\alpha s,p(s)=\frac{1}{1+e^{-\frac{s-E[s]}{\sqrt{Var[s]+\epsilon}}}}$
    |  | (1) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(s)=p(s)\cdot s+(1-p(s))\cdot\alpha s,p(s)=\frac{1}{1+e^{-\frac{s-E[s]}{\sqrt{Var[s]+\epsilon}}}}$
    |  | (1) |'
- en: In the training phase, $E[s]$ and $Var[s]$ are the mean and variance of input
    in each-minibatch, while in the testing phase, $E[s]$ and $Var[s]$ are moving
    averages of $E[s]$ and $Var[s]$ over data. $\epsilon$ is a small constant and
    is set to be $10^{-8}$ by authors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，$E[s]$ 和 $Var[s]$ 是每个小批量输入的均值和方差，而在测试阶段，$E[s]$ 和 $Var[s]$ 是数据上 $E[s]$
    和 $Var[s]$ 的移动平均。$\epsilon$ 是一个小常数，作者将其设置为 $10^{-8}$。
- en: They adopt different representation vector for different ads. The embedding
    layer uses single embedding vector (one-hot) and multiple embedding vectors (multi-hot)
    in combination. Pooling and concat layers are added to transform the list of embedding
    vectors into the same lengths, so that the network can allow different users to
    have different number of behaviors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 他们对不同的广告采用不同的表示向量。嵌入层结合了单一嵌入向量（one-hot）和多个嵌入向量（multi-hot）。添加了池化和连接层，将嵌入向量列表转换为相同长度，以便网络可以允许不同用户有不同数量的行为。
- en: Shin et al transformed the charging scheduling problem into an auction problem
    using deep learning framework [[25](#bib.bib25)]. It is designed based on the
    concept of the Myerson auction [[20](#bib.bib20)], which is one of the most efficient
    revenue-optimal single-item auctions. One of the most challenging issues about
    charging scheduling is the lack of prior knowledge on the distribution of the
    number of bidders. Employing the auction approach is useful when there is no accurate
    information of buyer’s true valuation, and buyers are not aware of the private
    true values of other buyers. Buyer’s values are represented by the urgency of
    drone machines, while seller’s revenue is generated from the payment from resource
    allocation. As Myerson’s auction system requires full knowledge of the distribution
    of bids in order to compute the expected payment, Shin et al used deep neural
    network to parametrize the virtual valuation function, the allocaton rule, and
    the payment rule.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Shin 等人使用深度学习框架将充电调度问题转化为拍卖问题[[25](#bib.bib25)]。该设计基于Myerson拍卖的概念[[20](#bib.bib20)]，这是最有效的单项拍卖之一，能够实现收入最优化。充电调度的一个主要挑战是对竞标者数量分布缺乏先验知识。当买家的真实估值信息不准确，且买家不知道其他买家的私人真实价值时，使用拍卖方法非常有用。买家的价值由无人机的紧迫性表示，而卖家的收入来自于资源分配的支付。由于Myerson拍卖系统要求完全了解竞标分布以计算期望支付，Shin
    等人使用深度神经网络对虚拟估值函数、分配规则和支付规则进行参数化。
- en: The network begins with a monotonic network that transforms the bids $b_{i}$
    of the drone into $\bar{b}_{i}$ using the virtual valuation function $\phi^{mononet}$
    parametrized by the network. In the $\phi^{mononet}$, all outcomes of $\phi^{shared}$
    (Equation [3](#S2.E3 "In 2 Hybrid Multiple Linear Perceptron (MLP) Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"))
    are computed using the same weights, while the $\phi_{i}$ (Equation [2](#S2.E2
    "In 2 Hybrid Multiple Linear Perceptron (MLP) Infrastructure ‣ A Survey of Online
    Auction Mechanism Design Using Deep Learning Approaches")) calculates the outcome
    using different weights for each bid.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 网络从一个单调网络开始，该网络使用由网络参数化的虚拟估值函数$\phi^{mononet}$将无人机的竞标 $b_{i}$ 转换为 $\bar{b}_{i}$。在
    $\phi^{mononet}$ 中，$\phi^{shared}$ 的所有结果（方程[3](#S2.E3 "在2混合多层感知机（MLP）基础设施 ‣ 基于深度学习方法的在线拍卖机制设计调研")）都使用相同的权重计算，而
    $\phi_{i}$（方程[2](#S2.E2 "在2混合多层感知机（MLP）基础设施 ‣ 基于深度学习方法的在线拍卖机制设计调研")）则使用不同的权重计算每个竞标的结果。
- en: '|  | $b_{i}^{\prime}=\phi_{i}(b_{i})=\min_{1\leq g\leq\mathcal{G}}\{\max_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{i}b_{i}+\beta_{g,n}^{i})\}$ |  | (2) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $b_{i}^{\prime}=\phi_{i}(b_{i})=\min_{1\leq g\leq\mathcal{G}}\{\max_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{i}b_{i}+\beta_{g,n}^{i})\}$ |  | (2) |'
- en: '|  | $\bar{b}_{i}=\phi_{i}^{shared}(b_{i}^{\prime})=\min_{1\leq g\leq\mathcal{G}}\{\max_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{shared}b_{i}^{\prime}+\beta_{g,n}^{shared})\}$ |  |
    (3) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{b}_{i}=\phi_{i}^{shared}(b_{i}^{\prime})=\min_{1\leq g\leq\mathcal{G}}\{\max_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{shared}b_{i}^{\prime}+\beta_{g,n}^{shared})\}$ |  |
    (3) |'
- en: The payment rule network takes in the output $\bar{b}_{i}$ from the monotonic
    network and returns $\bar{p}_{i}$ according to Equation [4](#S2.E4 "In 2 Hybrid
    Multiple Linear Perceptron (MLP) Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches"). Then, the payment value is computed using
    the inverse function of $\phi^{mononet}$ (Equation [5](#S2.E5 "In 2 Hybrid Multiple
    Linear Perceptron (MLP) Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches"), [6](#S2.E6 "In 2 Hybrid Multiple Linear
    Perceptron (MLP) Infrastructure ‣ A Survey of Online Auction Mechanism Design
    Using Deep Learning Approaches")). Finally, the allocation rule network assigns
    the highest winning probability to the highest bidder with positive transformed
    bid.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 支付规则网络接收来自单调网络的输出 $\bar{b}_{i}$，并根据方程[4](#S2.E4 "在2混合多层感知机（MLP）基础设施 ‣ 基于深度学习方法的在线拍卖机制设计调研")
    返回 $\bar{p}_{i}$。然后，支付值使用 $\phi^{mononet}$ 的逆函数（方程[5](#S2.E5 "在2混合多层感知机（MLP）基础设施
    ‣ 基于深度学习方法的在线拍卖机制设计调研")，[6](#S2.E6 "在2混合多层感知机（MLP）基础设施 ‣ 基于深度学习方法的在线拍卖机制设计调研")）计算。最后，分配规则网络将最高的胜出概率分配给具有正变换竞标的最高竞标者。
- en: '|  | $\bar{p}_{i}=ReLU\{\max_{j\neq i}(\bar{b}_{i})\}$ |  | (4) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{p}_{i}=ReLU\{\max_{j\neq i}(\bar{b}_{i})\}$ |  | (4) |'
- en: '|  | $p_{i}^{\prime}=\phi_{shared}^{-1}(\bar{p}_{i})=\max_{1\leq g\leq\mathcal{G}}\{\min_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{shared})^{-1}(\bar{p}_{i}-\beta_{g,n}^{shared})\}$
    |  | (5) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}^{\prime}=\phi_{shared}^{-1}(\bar{p}_{i})=\max_{1\leq g\leq\mathcal{G}}\{\min_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{shared})^{-1}(\bar{p}_{i}-\beta_{g,n}^{shared})\}$
    |  | (5) |'
- en: '|  | $p_{i}=\phi_{i}^{-1}(p_{i}^{\prime})=\max_{1\leq g\leq\mathcal{G}}\{\min_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{i})^{-1}(p_{i}^{\prime}-\beta_{g,n}^{i})\}$ |  | (6)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}=\phi_{i}^{-1}(p_{i}^{\prime})=\max_{1\leq g\leq\mathcal{G}}\{\min_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{i})^{-1}(p_{i}^{\prime}-\beta_{g,n}^{i})\}$ |  | (6)
    |'
- en: 'Luong et al constructed a neural network [[17](#bib.bib17)] for edge computing
    resource management based on analytical solution [[20](#bib.bib20)], which guarantees
    the revenue maximization while ensuring the IC and IR. The network structure also
    has three key components as in [[25](#bib.bib25)]: neural network parametrized
    monotone transformation functions that map bids into transformed versions, an
    allocation rule that maps the transformed bids to a vector of assignment probabilities,
    and a conditional payment rule that is based on the maximum non-negative transformed
    bids. The allocation and payment rules are derived from SPA-0, second price auction
    with 0 reserve price, where the reserve price is the mininum price a seller is
    willing to accept from the buyer.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Luong 等人基于分析解 [[20](#bib.bib20)] 构建了一种神经网络 [[17](#bib.bib17)] 用于边缘计算资源管理，该网络在确保
    IC 和 IR 的同时保证了收入最大化。网络结构还具有三个关键组件，如 [[25](#bib.bib25)] 中所示：将竞标转化为转换版本的单调变换函数的神经网络参数化、将转换后的竞标映射到分配概率向量的分配规则，以及基于最大非负转换竞标的条件支付规则。分配和支付规则是从
    SPA-0 推导出的，即具有 0 保留价的第二价格拍卖，其中保留价是卖家愿意从买家那里接受的最低价格。
- en: 'RochetNet [[9](#bib.bib9)] proposed by Dutting et al. is also an application
    of MLP. The RochetNet is a single-layered neural network that takes in the bids
    and outputs the maximum non-negative transformed values. It is used to model a
    non-negative, monotone, convex, and Lipschitz utility function, using $J$ linear
    functions with non-negative coefficients. The RochetNet easily extends to a single
    bidder with a unit-demand valuation ¹¹1Unit-demand valuation: the value of a subset
    is the maximum individual valuation within that subset.. Each linear function
    in the RochetNet corresponds to an option on the menue, with the allocation probabilities
    and payments encoded through its slope and intercept.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Dutting 等人提出的 RochetNet [[9](#bib.bib9)] 也是 MLP 的一种应用。RochetNet 是一种单层神经网络，它接收竞标并输出最大非负转换值。它用于建模非负、单调、凸和
    Lipschitz 效用函数，使用 $J$ 个具有非负系数的线性函数。RochetNet 很容易扩展到具有单位需求估值的单一竞标者¹¹单位需求估值：子集的价值是该子集内的最大个人估值。RochetNet
    中的每个线性函数对应于菜单上的一个选项，其分配概率和支付通过其斜率和截距进行编码。
- en: The MoulinNet [[11](#bib.bib11)] proposed by Golowich et al. also adopts the
    structure of MLP, which is used to determine the optimal facility locations preferred
    by agents. MoulinNet is a monotone feed-forward neural network that learns the
    generalized median rules [[19](#bib.bib19)]. For single-facility mechanisms, the
    mechanism in Equation [7](#S2.E7 "In 2 Hybrid Multiple Linear Perceptron (MLP)
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches") is strategy-proof, which selects the median of agents’ most preferred
    locations (the agents’ peaks). The inputs of the network are binary-encoded vectors
    $\nu(S)$ that represent whether the bidded items in $S$ are selected. $w$ and
    $b$ are parameters in MoulinNet. The $u_{i}$ is the utility function for agent
    $i$ and $\tau$ represents the peaks of the facility. The output of the network
    is the optimal selection rules based on utilities.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Golowich 等人提出的 MoulinNet [[11](#bib.bib11)] 也采用了 MLP 的结构，用于确定代理人偏好的最优设施位置。MoulinNet
    是一种单调前馈神经网络，它学习广义中位数规则 [[19](#bib.bib19)]。对于单设施机制，方程 [7](#S2.E7 "在 2 混合多层感知器（MLP）基础设施
    ‣ 基于深度学习方法的在线拍卖机制设计综述") 中的机制是策略无关的，它选择代理人最偏好的位置的中位数（代理人的峰值）。网络的输入是二进制编码向量 $\nu(S)$，表示
    $S$ 中竞标的项目是否被选择。$w$ 和 $b$ 是 MoulinNet 中的参数。$u_{i}$ 是代理人 $i$ 的效用函数，$\tau$ 表示设施的峰值。网络的输出是基于效用的最优选择规则。
- en: '|  | $f^{w,b}(u)=\min_{S\subseteq N}\{\max_{i\in S}\{\tau(u_{i}),h^{w,b}(\nu(S))\}\}$
    |  | (7) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $f^{w,b}(u)=\min_{S\subseteq N}\{\max_{i\in S}\{\tau(u_{i}),h^{w,b}(\nu(S))\}\}$
    |  | (7) |'
- en: 3 RegretNet Infrastructure
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 RegretNet 基础设施
- en: The RegretNet [[9](#bib.bib9)] proposed by Dutting et al is comprised of an
    allocation network and a payment network. Both are built upon the MLP infrastructure,
    but the RegretNet has been adopted and extended in the auction designs in various
    settings [[10](#bib.bib10), [11](#bib.bib11), [21](#bib.bib21), [14](#bib.bib14)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: RegretNet [[9](#bib.bib9)]由Dutting等人提出，包含一个分配网络和一个支付网络。这两者都基于MLP基础设施，但RegretNet已在各种设置中的拍卖设计中被采用和扩展[[10](#bib.bib10)，[11](#bib.bib11)，[21](#bib.bib21)，[14](#bib.bib14)]。
- en: 'Two basic assumptions are required by the RegretNet architecture: additive
    valuation ²²2Additive valuation: an agent’s valuation for a subset of items is
    the sum of the individual items’ valuations. and unit-demand valuation. Both of
    the allocation network and the payment network takes in the bids as inputs, feeds
    them into MLP-structured networks with separate parameters, and returns the total
    payments based on the outputs from two networks. Therefore, the two networks are
    trained together. The network uses a sigmoidal unit to normalize the payment vector
    into [0, 1], so that the IR constraint will be enforced, where bidders are never
    charged for more than their expected value for the allocation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: RegretNet架构需要两个基本假设：加性估值²²2加性估值：代理人对一个子集物品的估值是单个物品估值的总和。和单位需求估值。分配网络和支付网络都将投标作为输入，将其输入到具有独立参数的MLP结构网络中，然后根据两个网络的输出返回总支付。因此，这两个网络一起进行训练。网络使用sigmoidal单元将支付向量标准化为[0,
    1]，以确保IR约束得到执行，即投标人不会为分配支付超出其预期价值的费用。
- en: 'The objective function is aiming to minimize the empirical loss (negated revenue)
    subject to the IC and IR constraints. The IC constraint can be enforced by the
    notion of ex post regret for bidders, which is the maximum increase in their utility
    considering all possible non-truthful bids. The ex post regret is estimated by
    the empirical regret, which is denoted as $\hat{rgt}_{i}(w)$. Therefore, the objective
    function becomes (Equation [8](#S3.E8 "In 3 RegretNet Infrastructure ‣ A Survey
    of Online Auction Mechanism Design Using Deep Learning Approaches")):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数旨在最小化经验损失（负收入），同时满足IC和IR约束。IC约束可以通过投标人的经验后悔概念来执行，即考虑所有可能的不诚实投标的情况下，效用的最大增加。经验后悔由经验后悔估计，表示为$\hat{rgt}_{i}(w)$。因此，目标函数变为（方程式[8](#S3.E8
    "在3 RegretNet基础设施 ‣ 使用深度学习方法的在线拍卖机制设计调查")）：
- en: '|  | $\begin{split}\min_{w\in\mathbb{R}^{d}}-\frac{1}{L}\sum_{l=1}^{L}\sum_{i=1}^{n}p_{i}^{w}(v^{(l)})\\
    \text{s.t. }\hat{rgt}_{i}(w)=0,\forall i\in N.\end{split}$ |  | (8) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\min_{w\in\mathbb{R}^{d}}-\frac{1}{L}\sum_{l=1}^{L}\sum_{i=1}^{n}p_{i}^{w}(v^{(l)})\\
    \text{s.t. }\hat{rgt}_{i}(w)=0,\forall i\in N.\end{split}$ |  | (8) |'
- en: 'The optimization is achieved using Lagrange multipliers, augmented with a quadratic
    penalty term for violating the constraints (Equation [9](#S3.E9 "In 3 RegretNet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches")):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 优化通过拉格朗日乘数实现，增加了一个二次惩罚项以违反约束（方程式[9](#S3.E9 "在3 RegretNet基础设施 ‣ 使用深度学习方法的在线拍卖机制设计调查")）：
- en: '|  | $\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i=1}^{n}p_{i}^{w}(v^{(l)})+\sum_{i\in
    N}\lambda_{i}\hat{rgt}_{i}(w)+\frac{\rho}{2}\sum_{i\in N}(\hat{rgt}_{i}(w))^{2}$
    |  | (9) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i=1}^{n}p_{i}^{w}(v^{(l)})+\sum_{i\in
    N}\lambda_{i}\hat{rgt}_{i}(w)+\frac{\rho}{2}\sum_{i\in N}(\hat{rgt}_{i}(w))^{2}$
    |  | (9) |'
- en: 'Feng et al constructed a neural network [[10](#bib.bib10)] built upon the structure
    of RegretNet, which consists of an allocation network and a payment network. It
    extends the RegretNet infrastructure by incorporating the budget constraints as
    well as handling Bayesian Incentive Compatible (BIC) ³³3Bayesian Incentive Compatible:
    truth-telling is the optimal strategy for a bidder in expectation with respect
    to the types of others, given that the other bidders report truthfully. and conditional
    IC constraints. Dutting et al enforces IC by requiring the empirical ex post regret
    to be zero, while Feng et al are able to handle more general forms of IC by constructing
    an appropriate notion of regret.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Feng等人构建了一个基于RegretNet结构的神经网络[[10](#bib.bib10)]，包括一个分配网络和一个支付网络。它通过结合预算约束以及处理Bayesian
    Incentive Compatible (BIC) ³³3Bayesian Incentive Compatible：在其他投标人诚实报告的情况下，真诚告知是对投标人最优策略。和条件IC约束来扩展RegretNet基础设施。Dutting等人通过要求经验后悔为零来执行IC，而Feng等人通过构建适当的后悔概念来处理更一般的IC形式。
- en: Assume we have an auction with rules $(a,p)$. To handle BIC, Feng et al constrain
    the empirical interim regret $rgt_{i}(a,p)$ (Equation [10](#S3.E10 "In 3 RegretNet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches")) to zero. To handle conditional IC/BIC, they constrain the empirical
    conditional regret to zero. They also incorporate the individually rationality
    (IR) $irp_{i}(a,p)$ (Equation [11](#S3.E11 "In 3 RegretNet Infrastructure ‣ A
    Survey of Online Auction Mechanism Design Using Deep Learning Approaches")) and
    budget constraint (BC) $bcp_{i}(a,p)$ (Equation [12](#S3.E12 "In 3 RegretNet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"))
    as penalties.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个规则为 $(a,p)$ 的拍卖。为处理 BIC，Feng 等人将经验中期后悔 $rgt_{i}(a,p)$（方程 [10](#S3.E10
    "In 3 RegretNet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches")）约束为零。为处理条件 IC/BIC，他们将经验条件后悔约束为零。他们还将个体理性（IR） $irp_{i}(a,p)$（方程
    [11](#S3.E11 "In 3 RegretNet Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches")）和预算约束（BC） $bcp_{i}(a,p)$（方程 [12](#S3.E12
    "In 3 RegretNet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches")）作为惩罚。
- en: '|  | $rgt_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max_{t_{i}^{\prime}\in\mathcal{T}_{i}}1_{\mathcal{P}_{i}(t_{i}^{\prime})\leq
    b_{i}}(\mathcal{U}_{i}(t_{i},t_{i}^{\prime})-\mathcal{U}_{i}(t_{i},t_{i}))]$ |  |
    (10) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $rgt_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max_{t_{i}^{\prime}\in\mathcal{T}_{i}}1_{\mathcal{P}_{i}(t_{i}^{\prime})\leq
    b_{i}}(\mathcal{U}_{i}(t_{i},t_{i}^{\prime})-\mathcal{U}_{i}(t_{i},t_{i}))]$ |  |
    (10) |'
- en: '|  | $irp_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max\{0,-\mathcal{U}_{i}(t_{i},t_{i})\}]$
    |  | (11) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $irp_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max\{0,-\mathcal{U}_{i}(t_{i},t_{i})\}]$
    |  | (11) |'
- en: '|  | $bcp_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max\{0,\mathcal{P}_{i}(t_{i})-b_{i}\}]$
    |  | (12) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $bcp_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max\{0,\mathcal{P}_{i}(t_{i})-b_{i}\}]$
    |  | (12) |'
- en: The loss function is the negated expected revenue $\mathcal{L}(a,p)=-E_{t\sim
    F}[\sum_{i=1}^{n}p_{i}(t)]$. Let $w\in\mathbb{R}^{d}$ denote the parameters of
    the allocation network, the induced allocation rule denoted by $a^{w}$, and $w^{\prime}\in\mathbb{R}^{d^{\prime}}$
    denote the parameters of the payment network, the induced payment rule is denoted
    by $p^{w^{\prime}}$. The objective function is finally in Equation [13](#S3.E13
    "In 3 RegretNet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches"). The objective function is trained using Augmented
    Lagrangian Solver as in Dutting et al, where the quadratic penalty terms are added
    for each constraint.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是负期望收益 $\mathcal{L}(a,p)=-E_{t\sim F}[\sum_{i=1}^{n}p_{i}(t)]$。令 $w\in\mathbb{R}^{d}$
    表示分配网络的参数，由 $a^{w}$ 表示的诱导分配规则，以及 $w^{\prime}\in\mathbb{R}^{d^{\prime}}$ 表示支付网络的参数，由
    $p^{w^{\prime}}$ 表示的诱导支付规则。目标函数最终在方程 [13](#S3.E13 "In 3 RegretNet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches")
    中给出。目标函数使用Augmented Lagrangian Solver进行训练，如Dutting等所述，其中对每个约束添加了二次罚项。
- en: '|  | <math   alttext="\begin{split}\min_{w\in\mathbb{R}^{d},w^{\prime}\in\mathbb{R}^{d^{\prime}}}\mathcal{L}(a^{w},p^{w^{\prime}})\\
    \text{s.t. }rgt_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\min_{w\in\mathbb{R}^{d},w^{\prime}\in\mathbb{R}^{d^{\prime}}}\mathcal{L}(a^{w},p^{w^{\prime}})\\
    \text{s.t. }rgt_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\'
- en: irp_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: $irp_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\$
- en: bcp_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: bcp_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\
- en: \end{split}" display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mrow ><mrow ><munder ><mi >min</mi><mrow ><mrow  ><mi
    >w</mi><mo >∈</mo><msup ><mi >ℝ</mi><mi >d</mi></msup></mrow><mo >,</mo><mrow
    ><msup ><mi >w</mi><mo >′</mo></msup><mo >∈</mo><msup ><mi >ℝ</mi><msup ><mi >d</mi><mo
    >′</mo></msup></msup></mrow></mrow></munder><mo lspace="0.167em"  >⁡</mo><mi >ℒ</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi
    >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup ><mi  >w</mi><mo >′</mo></msup></msup><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="right"
    ><mrow ><mrow ><mrow ><mtext  >s.t. </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi  >t</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi
    >p</mi><msup ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mrow ><mi  >i</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >r</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi  >p</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup
    ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mrow ><mi  >b</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >c</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi  >p</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup
    ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><ci >𝑤</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℝ</ci><ci
    >𝑑</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑤</ci><ci >′</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℝ</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑑</ci><ci >′</ci></apply></apply></apply></apply></apply><ci
    >ℒ</ci></apply><interval closure="open"  ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑎</ci><ci >𝑤</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑝</ci><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑤</ci><ci
    >′</ci></apply></apply></interval><ci ><mtext >s.t. </mtext></ci><ci >𝑟</ci><ci  >𝑔</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><ci >𝑖</ci></apply><interval
    closure="open" ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑎</ci><ci
    >𝑤</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑝</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑤</ci><ci >′</ci></apply></apply></interval></apply><cn
    type="integer"  >0</cn></apply><apply ><apply ><apply ><csymbol cd="latexml"  >for-all</csymbol><ci
    >𝑖</ci></apply><apply ><apply ><csymbol cd="latexml"  >delimited-[]</csymbol><ci
    >𝑛</ci></apply><ci >𝑖</ci><ci >𝑟</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑝</ci><ci >𝑖</ci></apply><interval closure="open" ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑎</ci><ci >𝑤</ci></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑝</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑤</ci><ci >′</ci></apply></apply></interval></apply></apply><apply ><cn type="integer"  >0</cn></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="latexml" >for-all</csymbol><ci >𝑖</ci></apply><apply
    ><apply ><csymbol cd="latexml" >delimited-[]</csymbol><ci >𝑛</ci></apply><ci >𝑏</ci><ci  >𝑐</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑝</ci><ci >𝑖</ci></apply><interval
    closure="open" ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑎</ci><ci
    >𝑤</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑝</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑤</ci><ci >′</ci></apply></apply></interval></apply></apply><apply
    ><cn type="integer" >0</cn></apply></apply><apply ><apply ><csymbol cd="latexml"
    >for-all</csymbol><ci >𝑖</ci></apply><apply ><csymbol cd="latexml" >delimited-[]</csymbol><ci
    >𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{split}\min_{w\in\mathbb{R}^{d},w^{\prime}\in\mathbb{R}^{d^{\prime}}}\mathcal{L}(a^{w},p^{w^{\prime}})\\
    \text{s.t. }rgt_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\ irp_{i}(a^{w},p^{w^{\prime}})=0,\forall
    i\in[n]\\ bcp_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\ \end{split}</annotation></semantics></math>
    |  | (13) |
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mrow ><mrow ><munder ><mi >min</mi><mrow ><mrow  ><mi
    >w</mi><mo >∈</mo><msup ><mi >ℝ</mi><mi >d</mi></msup></mrow><mo >,</mo><mrow
    ><msup ><mi >w</mi><mo >′</mo></msup><mo >∈</mo><msup ><mi >ℝ</mi><msup ><mi >d</mi><mo
    >′</mo></msup></msup></mrow></mrow></munder><mo lspace="0.167em"  >⁡</mo><mi >ℒ</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi
    >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup ><mi  >w</mi><mo >′</mo></msup></msup><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="right"
    ><mrow ><mrow ><mrow ><mtext  >s.t. </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi  >t</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi
    >p</mi><msup ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mrow ><mi  >i</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >r</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi  >p</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup
    ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mrow ><mi  >b</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >c</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi  >p</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup
    ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><ci >𝑤</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℝ</ci><ci
    >𝑑</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑤</ci><ci >′</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℝ</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑑</ci><ci >′</ci></apply></apply></apply></apply></apply><ci
    >ℒ</ci></apply><interval closure="open"  ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑎</ci><ci >𝑤</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑝</ci><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑤</ci><ci
    >′</ci></apply></apply></interval><ci ><mtext >s.t. </mtext></ci><ci >𝑟</ci><ci  >𝑔</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><ci >𝑖</ci></apply><interval
    closure="open" ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑎</ci><ci
    >𝑤</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑝</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑤</ci><ci >′</ci></apply></apply></interval></apply><cn
    type="integer"  >0</cn></apply><apply ><apply ><apply ><csymbol cd="latexml"  >for-all</csymbol><ci
    >𝑖</ci></apply><apply ><apply ><csymbol cd="latexml"  >delimited-[]</csymbol><ci
    >𝑛</ci></apply><ci >𝑖</ci><ci >𝑟</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑝</ci><ci >𝑖</ci></apply><interval closure="open" ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑎</ci><ci >𝑤</ci></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑝</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑤</ci><ci >′</ci></apply></apply></interval></apply></apply><apply ><cn type="integer"  >0</cn></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="latexml" >for-all</csymbol><ci >𝑖</ci></apply><apply
    ><apply ><csymbol cd="latexml" >delimited-[]</csymbol><ci >𝑛</ci></apply
- en: Golowich et al proposed RegretNet-nm [[11](#bib.bib11)] that is able to give
    general mechanisms that are not limited by existing characterization results for
    multi-facility location problems. The notion of regret is extended to facility
    location mechanisms as the maximum expected utility gain agents can achieve by
    misreporting their preferences.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Golowich 等人提出了 RegretNet-nm [[11](#bib.bib11)]，它能够提供不受现有多设施位置问题表征结果限制的一般机制。悔恨的概念扩展到设施位置机制中，作为代理通过误报其偏好而能够实现的最大期望效用增益。
- en: The network structure is the same as RegretNet, except for the inputs which
    are agents’ peaks. The misreported peaks are sampled uniformly within $[0,1]$,
    with a granularity of $\frac{1}{M}$. The ex post regret is integrated into the
    objective function using Augmented Lagrangian Solver, which uses a quadratic penalty
    term. The RegretNet-nm opens the door for mechanisms designs for settings without
    money, such as matching and allocation problems, using neural network approaches.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 网络结构与 RegretNet 相同，只是输入是代理的峰值。误报告的峰值在 $[0,1]$ 范围内均匀采样，粒度为 $\frac{1}{M}$。事后悔恨被集成到目标函数中，使用增广拉格朗日求解器，该求解器使用二次惩罚项。RegretNet-nm
    为使用神经网络方法进行无金钱设置的机制设计（如匹配和分配问题）开辟了新天地。
- en: 'PreferenceNet [[21](#bib.bib21)] is another extension of RegretNet. It encodes
    human preferences in auction designs. The network structure is comprised of RegretNet
    and a 3-layer MLP. These two components are trained in an EM-manner: MLP is first
    trained using a uniformly drawn sample of allocations as inputs, and it is optimized
    using binary cross entropy loss based on ground truth labels. Then, the RegretNet
    is trained using Augmented Lagrange Solver.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PreferenceNet [[21](#bib.bib21)] 是 RegretNet 的另一个扩展。它在拍卖设计中编码了人类的偏好。网络结构由 RegretNet
    和一个 3 层的 MLP 组成。这两个组件以 EM 方式进行训练：首先使用均匀抽取的分配样本训练 MLP，并基于真实标签使用二元交叉熵损失进行优化。然后，使用增广拉格朗日求解器训练
    RegretNet。
- en: The loss function for the entire PreferenceNet is defined in Equation [14](#S3.E14
    "In 3 RegretNet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches"), where $\mathcal{L}_{s}$ is the output of the trained
    MLP. Lastly, the allocations and payments are sampled every $c$ epochs from the
    partially trained RegretNet and use them to augment the MLP training set to adapt
    to the distributional shifts in allocations during training.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 PreferenceNet 的损失函数在方程 [14](#S3.E14 "在 3 RegretNet 基础设施 ‣ 基于深度学习方法的在线拍卖机制设计调查")
    中定义，其中 $\mathcal{L}_{s}$ 是训练好的 MLP 的输出。最后，从部分训练好的 RegretNet 中每 $c$ 轮采样分配和支付，并用它们来增强
    MLP 的训练集，以适应训练过程中分配的分布变化。
- en: '|  | $\begin{split}\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i\in
    N}p_{i}^{w}(v^{(l)})+\mathcal{L}_{rgt}-\mathcal{L}_{s}\\ \text{Where }\mathcal{L}_{rgt}=\sum_{i\in
    N}\lambda_{(r,i)}\text{rgt}_{i}(w)+\frac{\rho_{r}}{2}(\sum_{i\in N}\text{rgt}_{i}(w))^{2},\mathcal{L}_{s}=\sum_{j\in
    M}s_{j}\end{split}$ |  | (14) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i\in
    N}p_{i}^{w}(v^{(l)})+\mathcal{L}_{rgt}-\mathcal{L}_{s}\\ \text{其中 }\mathcal{L}_{rgt}=\sum_{i\in
    N}\lambda_{(r,i)}\text{rgt}_{i}(w)+\frac{\rho_{r}}{2}(\sum_{i\in N}\text{rgt}_{i}(w))^{2},\mathcal{L}_{s}=\sum_{j\in
    M}s_{j}\end{split}$ |  | (14) |'
- en: Peri et al. proposed Preference Classification Accuracy (PCA) metric to evaluate
    how well a learned auction model satisfies an arbitrary constraint. PCA is calculated
    by the fraction of test bids that satisfy the ground truth constraint. Then, authors
    use pairwise comparisons between allocations to elicit preferences. Each input
    set of allocations is compared against n other allocations on their preference
    scores and label it as either a positive or negative exemplar, based on if its
    preference score is higher than the majority of others.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Peri 等人提出了偏好分类准确度（PCA）指标来评估学习到的拍卖模型满足任意约束的程度。PCA 是通过满足真实约束的测试投标比例来计算的。然后，作者通过分配之间的成对比较来引出偏好。每组输入分配都与其他
    n 个分配在其偏好分数上进行比较，并根据其偏好分数是否高于大多数其他分配，将其标记为正面或负面样本。
- en: ProportionNet [[14](#bib.bib14)] proposed by Kuo et al. is also based on the
    infrastructure of RegretNet. Like most other neural networks that deal with auction
    mechanism designs, it does not work under the setting of combinatorial valuations.
    Under the assumption of additive valuations and unit-demand valuations, the input
    space of valuations reduces from $2^{M}$ to $M$, where $M$ is the number of items.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Kuo 等人提出的 ProportionNet [[14](#bib.bib14)] 也基于 RegretNet 的基础设施。与大多数处理拍卖机制设计的神经网络一样，它在组合估值设置下不起作用。在加性估值和单位需求估值的假设下，估值的输入空间从
    $2^{M}$ 减少到 $M$，其中 $M$ 是物品数量。
- en: '|  | $\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i\in N}p_{i}^{w}(v^{(l)})+\mathcal{L}_{rgt}+\mathcal{L}_{unf}$
    |  | (15) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i\in N}p_{i}^{w}(v^{(l)})+\mathcal{L}_{rgt}+\mathcal{L}_{unf}$
    |  | (15) |'
- en: 'Kuo et al. follows the core idea of RegretNet: in the Bayesian auction setting,
    one knows the valuation distribution from which samples can presumably be drawn.
    In the meantime, as both of the allocation and payment rules are functions, we
    can parametrize them using neural networks. Strategyproofness can be enforced
    adding constraints that are solvable using Augmented Lagrange Optimizer.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Kuo 等人遵循了 RegretNet 的核心思想：在贝叶斯拍卖设置中，人们知道可以从中抽取样本的估值分布。同时，由于分配和支付规则都是函数，我们可以使用神经网络对其进行参数化。可以通过添加约束来强制策略无关性，这些约束可以通过增强拉格朗日优化器解决。
- en: It adopted same neural network architecture as RegretNet, but adding a constraint
    of unfairness in the loss function [15](#S3.E15 "In 3 RegretNet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"),
    so that discriminatory ad allocations among different demography can be mitigated.
    The regret term $\mathcal{L}_{rgt}$ is consistent with the definition in RegretNet.
    The term $\mathcal{L}_{unf}$ is for quantifying the unfairness and discrimination
    in the auction system, which will be described in more details in Section [6.4](#S6.SS4
    "6.4 Fairness & Fraud Prevention ‣ 6 Constraints and Concerns ‣ A Survey of Online
    Auction Mechanism Design Using Deep Learning Approaches").
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 它采用了与 RegretNet 相同的神经网络架构，但在损失函数中增加了不公平性的约束 [15](#S3.E15 "在 3 RegretNet 基础设施
    ‣ 基于深度学习方法的在线拍卖机制设计综述")，以减少不同人群之间的歧视性广告分配。遗憾项 $\mathcal{L}_{rgt}$ 与 RegretNet
    中的定义一致。项 $\mathcal{L}_{unf}$ 用于量化拍卖系统中的不公平性和歧视，这将在第 [6.4](#S6.SS4 "6.4 公平性与欺诈防范
    ‣ 6 约束与关注点 ‣ 基于深度学习方法的在线拍卖机制设计综述") 节中详细描述。
- en: 4 Reinforcement Learning (RL) Infrastructure
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 强化学习 (RL) 基础设施
- en: As the online auction is more often a dynamic system whose users and platform
    objectives are evolving over time, researchers are more inclined to use dynamically
    trained models to adapt to the current status quo, leveraging reinforcement learning
    infrastructures [[6](#bib.bib6), [27](#bib.bib27), [31](#bib.bib31)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在线拍卖通常是一个动态系统，其用户和平台目标随着时间的推移而不断变化，研究人员更倾向于使用动态训练的模型来适应当前的现状，利用强化学习基础设施 [[6](#bib.bib6),
    [27](#bib.bib27), [31](#bib.bib31)]。
- en: As in the RegretNet infrastructure, Cai et al. also adopted no-regret learning,
    in which case agents only need to reason about their own strategies and their
    interaction with the environment, while they don’t have to know the values of
    competitors or compute payoff-maximizing strategies over a long sequence of rounds.
    Reasoning about the strategies of other parties usually require strong cognitive
    assumption and highly burdensome computing power, which most agents don’t have
    access to.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RegretNet 基础设施一样，Cai 等人也采用了无悔学习，在这种情况下，代理人只需要考虑自己的策略及其与环境的互动，而不必了解竞争者的价值或计算长期轮次中的收益最大化策略。考虑其他方策略通常需要强大的认知假设和高昂的计算能力，而大多数代理人无法获得这些。
- en: Based on well-known bandit algorithms, Cai et al. identified four possible strategies
    for sellers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于著名的多臂老虎机算法，Cai 等人识别出了卖家的四种可能策略。
- en: '1.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '$\epsilon-$Greedy [[29](#bib.bib29)]: With probability $\epsilon$, each seller
    selects a strategy uniformly at random. With probability $1-\epsilon$, the strategy
    with the best observed empirical mean payoff is selected.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\epsilon-$Greedy [[29](#bib.bib29)]: 以概率 $\epsilon$，每个卖家随机均匀选择一个策略。以概率 $1-\epsilon$，选择具有最佳观察到的经验均值收益的策略。'
- en: '2.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '$\epsilon-$First: For a horizon of $\mathcal{T}$ rounds, the seller picks a
    strategy uniformly at random for the first $\epsilon\cdot\mathcal{T}$ rounds,
    and then picks the strategy that maximizes the empirical mean of the observed
    rewards for all the subsequent rounds.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\epsilon-$First: 在 $\mathcal{T}$ 轮的视野中，卖家在前 $\epsilon\cdot\mathcal{T}$ 轮中均匀随机选择策略，然后选择最大化观察到的奖励经验均值的策略用于所有后续轮次。'
- en: '3.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Exponential-weight Algorithm for Exploration and Exploitation (Exp3) [[2](#bib.bib2),
    [4](#bib.bib4)]: In short, Exp3 selects a price according to a weighted distribution
    and then adjust the weights based on payoffs. To be more precise, suppose there
    are $K+1$ possible prices, the probability distribution $\pi_{i}(t)$ of those
    prices at round $t$ is defined in Equation [16](#S4.E16 "In item 3 ‣ 4 Reinforcement
    Learning (RL) Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches"), where $w_{i}(t)$ are the current weight of $i^{th}$
    price at round $t$ and $\gamma$ is a real number between $[0,1]$.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '指数加权算法用于探索和利用（Exp3）[[2](#bib.bib2), [4](#bib.bib4)]: 简而言之，Exp3 根据加权分布选择价格，然后根据收益调整权重。更具体地说，假设有
    $K+1$ 个可能的价格，轮次 $t$ 中这些价格的概率分布 $\pi_{i}(t)$ 在公式 [16](#S4.E16 "在第3项 ‣ 第4项 强化学习（RL）基础设施
    ‣ 基于深度学习方法的在线拍卖机制设计综述") 中定义，其中 $w_{i}(t)$ 是轮次 $t$ 中 $i^{th}$ 价格的当前权重，$\gamma$
    是 $[0,1]$ 之间的实数。'
- en: '|  | $\pi_{i}(t)=(1-\gamma)\frac{w_{i}(t)}{\sum_{j=1}^{K+1}w_{j}(t)}+\gamma\frac{1}{K+1}$
    |  | (16) |'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\pi_{i}(t)=(1-\gamma)\frac{w_{i}(t)}{\sum_{j=1}^{K+1}w_{j}(t)}+\gamma\frac{1}{K+1}$
    |  | (16) |'
- en: Select a price $p_{j}(t)$ according to the distribution above and compute its
    payoff $u_{j}(t)$, then the weight for $j^{th}$ price is updated according to
    Equation [17](#S4.E17 "In item 3 ‣ 4 Reinforcement Learning (RL) Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"),
    while the weights for all the other prices remain unchanged.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据上述分布选择价格 $p_{j}(t)$ 并计算其收益 $u_{j}(t)$，然后根据公式 [17](#S4.E17 "在第3项 ‣ 第4项 强化学习（RL）基础设施
    ‣ 基于深度学习方法的在线拍卖机制设计综述") 更新 $j^{th}$ 价格的权重，而其他所有价格的权重保持不变。
- en: '|  | $w_{j}(t+1)=w_{j}(t)e^{\gamma\frac{u_{j}(t)}{(K+1)\pi_{j}(t)}}$ |  | (17)
    |'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $w_{j}(t+1)=w_{j}(t)e^{\gamma\frac{u_{j}(t)}{(K+1)\pi_{j}(t)}}$ |  | (17)
    |'
- en: '4.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Upper confidence Bound Algorithm (UCB1) [[1](#bib.bib1), [3](#bib.bib3)]: In
    the first $K+1$ rounds, select a price not used before from $[0,\frac{1}{K},\dots,1]$
    and then select the price with the max weighted value in the subsequent rounds.'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '上置信界算法（UCB1）[[1](#bib.bib1), [3](#bib.bib3)]: 在前 $K+1$ 轮中，从 $[0,\frac{1}{K},\dots,1]$
    中选择一个之前未使用的价格，然后在随后的轮次中选择具有最大加权值的价格。'
- en: Initialize the weights for all prices to be 0\. For any round $t\in\{0,\dots.K\}$,
    the seller chooses a price $p_{j}$ such that $p_{j}\in[0,\frac{1}{K},\dots,1]$,
    computes the utility $u_{j}(t)$, and updates the weights for $j^{th}$ price to
    be $x_{j}(t)=\frac{x_{j}(t-1)+u_{j}(t)}{t}$, and keeping the weights for all the
    other prices unchanged.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将所有价格的权重初始化为 0。在任何轮次 $t\in\{0,\dots.K\}$ 中，卖家选择一个价格 $p_{j}$ 使得 $p_{j}\in[0,\frac{1}{K},\dots,1]$，计算效用
    $u_{j}(t)$，并将 $j^{th}$ 价格的权重更新为 $x_{j}(t)=\frac{x_{j}(t-1)+u_{j}(t)}{t}$，其余价格的权重保持不变。
- en: For any round $t\geq K+1$, the seller chooses the price $p_{j}$ according to
    Equation.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于任何轮次 $t\geq K+1$，卖家根据公式选择价格 $p_{j}$。
- en: '|  | $p_{j}(t)=argmax_{j\in\{0,\frac{1}{K},\dots,1\}}x_{j}(t)+\frac{\log_{2}t}{\sum_{\tau=1}^{t}1_{\{p_{j}\text{
    was chosen in round }\tau\}}}$ |  | (18) |'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $p_{j}(t)=argmax_{j\in\{0,\frac{1}{K},\dots,1\}}x_{j}(t)+\frac{\log_{2}t}{\sum_{\tau=1}^{t}1_{\{p_{j}\text{
    在轮次 }\tau\text{ 被选择}\}}}$ |  | (18) |'
- en: $\epsilon-$First and $\epsilon-Greedy$ have a clear distinction between exploration
    and exploitation and belong to the class of semi-uniform strategies. Exp3 makes
    no distributional assumptions about the rewards and is widely used for the full
    information setting and works in the adversarial bandit feedback model [[4](#bib.bib4)].
    UCB1 maintains a certain level of optimism towards less frequently played actions
    and uses the empirical mean of observed actions to choose the action in the next
    round. UCB1 is best suited in scenarios where rewards follow some unknown distributions
    [[6](#bib.bib6)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: $\epsilon-$First 和 $\epsilon-Greedy$ 在探索与利用之间有明显的区别，属于半均匀策略的范畴。Exp3 对奖励没有分布假设，广泛用于全信息设置，并在对抗性赌博反馈模型中有效
    [[4](#bib.bib4)]。UCB1 对较少被选择的行为保持一定的乐观态度，并使用观察到的行为的经验均值来选择下一轮的行为。UCB1 最适用于奖励遵循某些未知分布的场景
    [[6](#bib.bib6)]。
- en: These sellers’ models can model sellers with different degrees of sophistication
    or pricing philosophies, and it is consistent with the recent literature on algorithmic
    mechanism deisgn, in terms of modeling agetn rationality in complex dynamic environments.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些卖家模型可以模拟具有不同复杂程度或定价理念的卖家，并且与近期关于算法机制设计的文献一致，在建模复杂动态环境中的代理理性方面。
- en: Previous researchers have already come up with a few variants of reinforcement
    learning models. The disadvantage of Deep Q-Network (DQN) [[18](#bib.bib18)] is
    that it cannot handle continuous actions or high-dimensional action spaces, as
    stochastic actor-critic algorithms are hard to converge. The Deterministic Policy
    Gradient (DPG) algorithm [[26](#bib.bib26)] is developed to train a deterministic
    policy with parameter vector. The DPG consists of the critic and actor. The critic
    approximates the action-value function, while the actor adjusts the parameters
    of the deterministic policy. Deep Deterministic Policy Gradient (DDPG) [[15](#bib.bib15)]
    is then proposed, as DPG is severely impacted by the high degree of temporal correlation
    that introduces high variance. DDPG stores the experiences of the agetn at each
    time step in a replay buffer and uniformly samples mini-batch from it at random
    for learning, which can eliminate the temporal correlation. DDPG also employs
    target networks for the regularization of the learning algorithm, which updates
    the parameters at a slower rate.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究人员已经提出了一些强化学习模型的变体。深度 Q 网络 (DQN) [[18](#bib.bib18)] 的缺点是它不能处理连续动作或高维动作空间，因为随机演员-评论家算法很难收敛。确定性策略梯度
    (DPG) 算法 [[26](#bib.bib26)] 是为了训练具有参数向量的确定性策略。DPG 由评论家和演员组成。评论家近似动作价值函数，而演员则调整确定性策略的参数。然后提出了深度确定性策略梯度
    (DDPG) [[15](#bib.bib15)]，因为 DPG 受到高时间相关性的严重影响，导致高方差。DDPG 在每个时间步将代理的经验存储在回放缓冲区中，并从中随机均匀抽取小批量进行学习，从而消除时间相关性。DDPG
    还采用目标网络来对学习算法进行正则化，以较慢的速度更新参数。
- en: However, the size of the action space blows up very sharply with the number
    of sellers increases, so an direct application of DDPG will fail to converge.
    In addition, the DDPG is not able to handle variability on the set of sellers,
    since the algorithm uses a two-layer fully connected network and the positions
    of each seller plays an important role [[6](#bib.bib6)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着卖家数量的增加，动作空间的大小急剧膨胀，因此直接应用 DDPG 会导致不收敛。此外，DDPG 无法处理卖家集合上的变异性，因为该算法使用了一个两层的全连接网络，而每个卖家的位置发挥了重要作用
    [[6](#bib.bib6)]。
- en: Cai et al. proposed IA(GRU) [[6](#bib.bib6)] algorithm that aims to mitigate
    the problems from DDPG. It adopted the framework of DDPG by maintaining a sub-actor
    network and a sub-critic network. In each step of training, if utilizes a background
    network to perform a permutation transformations by ordering the sellers according
    to certain metrics, which maintains permutation invariance. In the meantime, it
    applies a recurrent neural network (RNN) on the history of sellers. The outputs
    from the permutation transformation and the outputs from the RNN on histories
    are then integrated together as inputs to the sub-actor and sub-critic networks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Cai 等人提出了 IA(GRU) [[6](#bib.bib6)] 算法，旨在缓解 DDPG 的问题。它采用了 DDPG 的框架，通过维护一个子演员网络和一个子评论家网络来实现。在每一步训练中，如果利用一个背景网络通过根据某些指标对卖家进行排列变换，从而保持排列不变性。同时，它在卖家的历史记录上应用了递归神经网络
    (RNN)。排列变换的输出和 RNN 在历史记录上的输出被整合在一起，作为子演员和子评论家网络的输入。
- en: In reality, participants of online auctions are constrained from both informational
    and computational aspects and therefore they are not fully rational. In addition,
    the historical data can be limited to the ones generated by mechanisms that are
    defined by only few sets of parameters, and therefore we do not have enough exploration
    for the past data. Both participants and auction system designers are impacted
    by multiple and complicated factors, and therefore their decisions are changing
    over time. To overcome those difficulties, Tang et al. models each player as an
    independent local Markov decision process [[27](#bib.bib27)], where a local state
    encodes the part of historical actions and outcomes that the player can observe
    so far.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在线拍卖的参与者在信息和计算方面都受到限制，因此他们并不完全理性。此外，历史数据可能仅限于由仅少数参数集定义的机制生成的数据，因此我们对过去数据的探索不足。参与者和拍卖系统设计师受到多种复杂因素的影响，因此他们的决策随着时间变化。为了克服这些困难，Tang等人将每个玩家建模为一个独立的局部马尔科夫决策过程[[27](#bib.bib27)]，其中局部状态编码了玩家迄今为止可以观察到的历史行动和结果的一部分。
- en: Tang et al. uses DDPG infrastructure to handle continuous action space, but
    it decomposes the original neural netowrk into a set of sub-networks, one for
    each seller, to handle the huge number of states. It depends on the assumption
    that sellers are independent and the Q-values are additive among multiple sellers.
    It uses LSTM to adaptively learn from its past bidding data and feedback to predict
    future bid distribution, while it does not explicitly model each advertiser’s
    bidding strategy. In order to optimize the designer’s markov decision process,
    it discretizes the action space and then use the Monte-Carlo tree search (MCTS)
    [[23](#bib.bib23)] to speed up the forward-looking search. Experiments and case
    studies show that the dynamic pricing scheme proposed by Tang et al. outperforms
    all static schemes with large margins.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Tang等人使用DDPG基础设施处理连续动作空间，但它将原始神经网络分解为一组子网络，每个卖家一个，以处理大量状态。它依赖于卖家相互独立和多个卖家之间Q值可加的假设。它使用LSTM从过去的竞标数据和反馈中自适应地学习，以预测未来的竞标分布，而不明确建模每个广告主的竞标策略。为了优化设计师的马尔科夫决策过程，它将动作空间离散化，然后使用蒙特卡罗树搜索（MCTS）[[23](#bib.bib23)]来加速前瞻性搜索。实验和案例研究表明，Tang等人提出的动态定价方案在大幅度超越所有静态方案。
- en: Another challenge in the dynamic online auction systems is the variety of performance
    metrics users and designers consider when making their decisions, while most state-of-the-art
    auction mechanisms only optimizes a single performance metrics, such as revenue
    or social welfare. Zhang et al. identified a list of performance metrics [[31](#bib.bib31)]
    that can be considered by users, advertisers, and the ad platform.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 动态在线拍卖系统中的另一个挑战是用户和设计师在决策时考虑的绩效指标种类，而大多数先进的拍卖机制只优化单一绩效指标，如收入或社会福利。Zhang等人确定了一份性能指标列表[[31](#bib.bib31)]，用户、广告商和广告平台可以考虑这些指标。
- en: '1.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Revenue Per Mille (RPM): $RPM=\frac{\sum click\times PPC}{\sum impression}\times
    1000$, where PPC is the payment for winning ads.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每千次展示的收入（RPM）：$RPM=\frac{\sum click\times PPC}{\sum impression}\times 1000$，其中PPC是赢得广告的付款。
- en: '2.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Click-Through Rate (CTR): $CTR=\frac{\sum click}{\sum impression}$.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击率（CTR）：$CTR=\frac{\sum click}{\sum impression}$。
- en: '3.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Add-to-Cart Rate (ACR): $ACR=\frac{\sum add-to-cart}{\sum impression}$.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 加入购物车率（ACR）：$ACR=\frac{\sum add-to-cart}{\sum impression}$。
- en: '4.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Conversion Rate (CVR): $CVR=\frac{\sum order}{\sum impression}$.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转化率（CVR）：$CVR=\frac{\sum order}{\sum impression}$。
- en: '5.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'GMV Per Mille (GPM): $GPM=\frac{\sum merchandisevolume}{\sum impression}\times
    1000$.'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每千次展示的商品交易额（GPM）：$GPM=\frac{\sum merchandisevolume}{\sum impression}\times 1000$。
- en: Zhang et al. proposed the Deep GSP [[31](#bib.bib31)] that can optimize multiple
    performance metrics as in Equation [19](#S4.E19 "In 4 Reinforcement Learning (RL)
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches"), where $b$ is the bid vector from users, $\mathcal{M}$ is the auction
    mechanism, $f_{j}$ is the $j^{th}$ performance metrics function, and $w_{j}$ is
    the weights associated with $j^{th}$ performance metrics and can be adjusted by
    the auction platform administrators from time to time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang等人提出了深度GSP [[31](#bib.bib31)]，它可以优化多个性能指标，如公式[19](#S4.E19 "在4强化学习（RL）基础设施
    ‣ 基于深度学习方法的在线拍卖机制设计综述")，其中$b$是用户的竞标向量，$\mathcal{M}$是拍卖机制，$f_{j}$是第$j$个绩效指标函数，$w_{j}$是与第$j$个绩效指标相关的权重，可以由拍卖平台管理员不时调整。
- en: '|  | <math   alttext="\begin{split}\mathcal{M}=argmax_{\mathcal{M}}E_{b\sim\mathcal{D}}[\sum_{j=1}^{L}w_{j}\times
    f_{j}(b;\mathcal{M})]\\ \text{s.t. Game Equilibrium constraints,}\\'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\mathcal{M}=argmax_{\mathcal{M}}E_{b\sim\mathcal{D}}[\sum_{j=1}^{L}w_{j}\times
    f_{j}(b;\mathcal{M})]\\ \text{s.t. 游戏均衡约束，}\\'
- en: \text{Smooth Transition constraints}\end{split}" display="block"><semantics
    ><mtable displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="right"  ><mrow
    ><mi >ℳ</mi><mo >=</mo><mrow ><mi  >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >x</mi><mi >ℳ</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi
    >E</mi><mrow ><mi >b</mi><mo >∼</mo><mi >𝒟</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >[</mo><mrow ><munderover ><mo
    lspace="0em" movablelimits="false" >∑</mo><mrow ><mi >j</mi><mo >=</mo><mn >1</mn></mrow><mi
    >L</mi></munderover><mrow ><mrow ><msub ><mi  >w</mi><mi >j</mi></msub><mo lspace="0.222em"
    rspace="0.222em"  >×</mo><msub ><mi >f</mi><mi >j</mi></msub></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >b</mi><mo >;</mo><mi
    >ℳ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mtext >s.t. Game Equilibrium constraints,</mtext></mtd></mtr><mtr  ><mtd
    columnalign="right"  ><mtext >Smooth Transition constraints</mtext></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >ℳ</ci><apply ><ci  >𝑎</ci><ci >𝑟</ci><ci
    >𝑔</ci><ci  >𝑚</ci><ci >𝑎</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><ci  >ℳ</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐸</ci><apply ><csymbol cd="latexml"  >similar-to</csymbol><ci >𝑏</ci><ci >𝒟</ci></apply></apply><apply
    ><csymbol cd="latexml" >delimited-[]</csymbol><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >𝑗</ci><cn type="integer" >1</cn></apply></apply><ci >𝐿</ci></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑤</ci><ci >𝑗</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑓</ci><ci >𝑗</ci></apply></apply><list
    ><ci >𝑏</ci><ci >ℳ</ci></list></apply></apply></apply><ci ><mtext  >s.t. Game
    Equilibrium constraints,</mtext></ci><ci ><mtext  >Smooth Transition constraints</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{M}=argmax_{\mathcal{M}}E_{b\sim\mathcal{D}}[\sum_{j=1}^{L}w_{j}\times
    f_{j}(b;\mathcal{M})]\\ \text{s.t. Game Equilibrium constraints,}\\ \text{Smooth
    Transition constraints}\end{split}</annotation></semantics></math> |  | (19) |
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: \text{平滑过渡约束}\end{split}" display="block"><semantics ><mtable displaystyle="true"
    rowspacing="0pt" ><mtr  ><mtd columnalign="right"  ><mrow ><mi >ℳ</mi><mo >=</mo><mrow
    ><mi  >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >r</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >m</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >x</mi><mi >ℳ</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi
    >E</mi><mrow ><mi >b</mi><mo >∼</mo><mi >𝒟</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >[</mo><mrow ><munderover ><mo
    lspace="0em" movablelimits="false" >∑</mo><mrow ><mi >j</mi><mo >=</mo><mn >1</mn></mrow><mi
    >L</mi></munderover><mrow ><mrow ><msub ><mi  >w</mi><mi >j</mi></msub><mo lspace="0.222em"
    rspace="0.222em"  >×</mo><msub ><mi >f</mi><mi >j</mi></msub></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >b</mi><mo >;</mo><mi
    >ℳ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mtext >s.t. 游戏均衡约束，</mtext></mtd></mtr><mtr  ><mtd
    columnalign="right"  ><mtext >平滑过渡约束</mtext></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >ℳ</ci><apply ><ci  >𝑎</ci><ci >𝑟</ci><ci
    >𝑔</ci><ci  >𝑚</ci><ci >𝑎</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><ci  >ℳ</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐸</ci><apply ><csymbol cd="latexml"  >similar-to</csymbol><ci >𝑏</ci><ci >𝒟</ci></apply></apply><apply
    ><csymbol cd="latexml" >delimited-[]</csymbol><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >𝑗</ci><cn type="integer" >1</cn></apply></apply><ci >𝐿</ci></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑤</ci><ci >𝑗</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑓</ci><ci >𝑗</ci></apply></apply><list
    ><ci >𝑏</ci><ci >ℳ</ci></list></apply></apply></apply><ci ><mtext  >s.t. 游戏均衡约束，</mtext></ci><ci
    ><mtext  >平滑过渡约束</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{split}\mathcal{M}=argmax_{\mathcal{M}}E_{b\sim\mathcal{D}}[\sum_{j=1}^{L}w_{j}\times
    f_{j}(b;\mathcal{M})]\\ \text{s.t. 游戏均衡约束，}\\ \text{平滑过渡约束}\end{split}</annotation></semantics></math>
    |  | (19) |
- en: The Deep GSP auction is built upon the classical generalized second-price auction.
    It takes in the features of items (e.g. category, historical click-through rate),
    user profile (e.g. gender, age, income), and user preference (e.g. budget, marketing
    demands) as inputs to a deep neural network, which integrates those features with
    the bids into an input vector $x_{i}$ and map them to a rank score $r_{i}=R_{\theta}(b_{i};x_{i})$,
    where $R_{\theta}(b_{i};x_{i})$ is the mapping function. Bidders are then sorted
    based non-increasingly based on their rank scores, and the top-K bidders would
    win the auction. The payments of the winning bidders are based on the bids from
    the next highest bidders.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 深度GSP拍卖建立在经典的广义第二价格拍卖的基础上。它将项目的特征（例如类别、历史点击率）、用户资料（例如性别、年龄、收入）以及用户偏好（例如预算、市场需求）作为输入，送入深度神经网络，该网络将这些特征与竞标结合成一个输入向量
    $x_{i}$，并将其映射到一个排名得分 $r_{i}=R_{\theta}(b_{i};x_{i})$，其中 $R_{\theta}(b_{i};x_{i})$
    是映射函数。然后，投标者根据其排名得分按非递增顺序排序，前K名投标者将赢得拍卖。获胜投标者的支付基于下一个最高投标者的投标。
- en: The mapping function $R_{\theta}(b_{i};x_{i})$ needs to be monotone with respect
    to the bids $b_{i}$, in order to satisfy the game equilibrium constraint. Some
    pieces of previous research enforced monotonicity by designing specific neural
    network architectures, but it increases the computational complexity for the training
    procedure. Therefore, Zhang et al. directly incorporate the monotonicity constraint
    by introducing a point-wise monotonicity penalty term (Equation [20](#S4.E20 "In
    4 Reinforcement Learning (RL) Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches")) into the loss function, where $\pi_{\theta}(b_{i};x_{i})$
    is a non-linear function with bid and is parametrized using a deep neural network.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 映射函数 $R_{\theta}(b_{i};x_{i})$ 需要对投标 $b_{i}$ 保持单调性，以满足博弈均衡约束。一些早期研究通过设计特定的神经网络架构来强制单调性，但这增加了训练过程的计算复杂度。因此，张等人通过在损失函数中引入逐点单调性惩罚项（方程[20](#S4.E20
    "在4 强化学习 (RL) 基础设施 ‣ 使用深度学习方法的在线拍卖机制设计调查")）来直接纳入单调性约束，其中 $\pi_{\theta}(b_{i};x_{i})$
    是一个非线性函数，并通过深度神经网络参数化。
- en: '|  | $\begin{split}\mathcal{L}_{mono}=&amp;\sum_{i=1}^{N}\max(0,-\triangledown_{b}R_{\theta}(b_{i};x_{i}))\\
    =&amp;\sum_{i=1}^{N}\max(0,-(\pi_{\theta}(b_{i};x_{i})+b_{i}\triangledown_{b}\pi_{\theta}(b_{i};x_{i})))\end{split}$
    |  | (20) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}_{mono}=&amp;\sum_{i=1}^{N}\max(0,-\triangledown_{b}R_{\theta}(b_{i};x_{i}))\\
    =&amp;\sum_{i=1}^{N}\max(0,-(\pi_{\theta}(b_{i};x_{i})+b_{i}\triangledown_{b}\pi_{\theta}(b_{i};x_{i})))\end{split}$
    |  | (20) |'
- en: The smooth transition constraint is imposed in Equation [21](#S4.E21 "In 4 Reinforcement
    Learning (RL) Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches"), which ensures that the advertiser’s utility would
    not fluctuate too much when the auction mechanism is switched towards another
    objective.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑过渡约束在方程[21](#S4.E21 "在4 强化学习 (RL) 基础设施 ‣ 使用深度学习方法的在线拍卖机制设计调查")中施加，这确保了当拍卖机制切换到另一个目标时，广告主的效用不会波动太大。
- en: '|  | $u_{i}(\mathcal{M})\geq(1-\epsilon)\times\bar{u}_{i}(\mathcal{M}_{0})$
    |  | (21) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $u_{i}(\mathcal{M})\geq(1-\epsilon)\times\bar{u}_{i}(\mathcal{M}_{0})$
    |  | (21) |'
- en: 5 DeepSet Infrastructure
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 深度集基础设施
- en: Most deep neural network has an implicit constraint on the position of each
    input, while in reality the items in auction do not have an inherent ordering.
    Cai et al. uses permutation transformation to mitigate the position effect by
    ordering the sellers based on some metrics. Liu et al. took a step further by
    removing the ordering effect completely. They introduced Deep Neural Auction (DNA)
    [[16](#bib.bib16)] that is built on the DeepSets [[30](#bib.bib30)] architecture.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度神经网络对每个输入的位置有隐含约束，而实际上拍卖中的物品并没有固有的排序。蔡等人通过根据一些指标对卖家进行排序来使用排列变换来缓解位置效应。刘等人进一步通过完全消除排序效应迈出了步伐。他们引入了基于DeepSets
    [[30](#bib.bib30)] 架构的深度神经拍卖（DNA） [[16](#bib.bib16)]。
- en: The set encoder for DNA is composed of two groups of layers $\phi_{1}$ and $\phi_{2}$.
    Each instance $x_{i}$ is first mapped to a high-dimensional latent space using
    the shared fully connected layers $\phi_{1}$, followed by the Exponential Linear
    Unit (ELU) [[7](#bib.bib7)] activation function $\sigma$. Then, it is processed
    with symmetric aggregation pooling (e.g. avgpool) to build the final set embedding
    $h_{i}^{\prime}$ for each ad $i$ with another fully connected layer $\phi_{2}$.
    The entire procedure is described in Equation [22](#S5.E22 "In 5 DeepSet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"),
    where $h_{-i}$ represent the hidden states from all bidders except $i$.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: DNA 的集合编码器由两组层 $\phi_{1}$ 和 $\phi_{2}$ 组成。每个实例 $x_{i}$ 首先通过共享的全连接层 $\phi_{1}$
    映射到高维潜在空间，然后通过指数线性单元（ELU） [[7](#bib.bib7)] 激活函数 $\sigma$ 处理。接着，通过对称聚合池化（例如 avgpool）处理，以另一全连接层
    $\phi_{2}$ 为每个广告 $i$ 构建最终的集合嵌入 $h_{i}^{\prime}$。整个过程在方程 [22](#S5.E22 "In 5 DeepSet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches") 中描述，其中 $h_{-i}$ 表示除 $i$ 外所有竞标者的隐藏状态。
- en: '|  | $\begin{split}h_{i}=\sigma(\phi_{1}(x_{i}))\\ h_{i}^{\prime}=\sigma(\phi_{2}(\text{avgpool}(h_{-i})))\end{split}$
    |  | (22) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}h_{i}=\sigma(\phi_{1}(x_{i}))\\ h_{i}^{\prime}=\sigma(\phi_{2}(\text{avgpool}(h_{-i})))\end{split}$
    |  | (22) |'
- en: DNA uses context-aware rank score uses a strictly monotone neural network with
    respect to bid, and supports efficient inverse transform given the next highest
    rank score. The rank score can be obtained using Equation [23](#S5.E23 "In 5 DeepSet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches") and the price can be obtained using Equation [24](#S5.E24 "In 5 DeepSet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches"), where $w_{qz}$, $w_{qz}^{\prime}$, and $\alpha_{qz}$ are weights
    of the neural network.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: DNA 使用上下文感知的排名分数，利用相对于投标的严格单调神经网络，并在给定下一个最高排名分数时支持高效的逆变换。排名分数可以使用方程 [23](#S5.E23
    "In 5 DeepSet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches") 获得，价格可以使用方程 [24](#S5.E24 "In 5 DeepSet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches")
    获得，其中 $w_{qz}$、$w_{qz}^{\prime}$ 和 $\alpha_{qz}$ 是神经网络的权重。
- en: '|  | $r_{i}=\min_{q\in[Q]}\max_{z\in[Z]}(e^{w_{qz}}\times b_{i}+w_{qz}^{\prime}\times
    x_{i}^{\prime}+\alpha_{qz})$ |  | (23) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{i}=\min_{q\in[Q]}\max_{z\in[Z]}(e^{w_{qz}}\times b_{i}+w_{qz}^{\prime}\times
    x_{i}^{\prime}+\alpha_{qz})$ |  | (23) |'
- en: '|  | $p_{i}=\max_{z\in[Z]}\min_{q\in[Q]}e^{-W_{qz}}(r_{i+1}-\alpha_{qz}-w_{qz}^{\prime}\times
    x_{i}^{\prime})$ |  | (24) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}=\max_{z\in[Z]}\min_{q\in[Q]}e^{-W_{qz}}(r_{i+1}-\alpha_{qz}-w_{qz}^{\prime}\times
    x_{i}^{\prime})$ |  | (24) |'
- en: This partially monotone MIN-MAX neural network represented by Equation [23](#S5.E23
    "In 5 DeepSet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches") has been proved to be able to approximate any function
    [[8](#bib.bib8)].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由方程 [23](#S5.E23 "In 5 DeepSet Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches") 表示的部分单调 MIN-MAX 神经网络已经被证明能够近似任何函数 [[8](#bib.bib8)]。
- en: '|  | $\hat{M}_{r}[k,:]=\text{softmax}(\frac{(N+1-2k)r-A_{r}\mathbbm{1}}{\tau})$
    |  | (25) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{M}_{r}[k,:]=\text{softmax}(\frac{(N+1-2k)r-A_{r}\mathbbm{1}}{\tau})$
    |  | (25) |'
- en: The DNA also model the whole process of allocation and payment inside the neural
    network framework, as treating allocation and payment as an agnostic environment
    can limit the deep learning results. One of the challenges is that both the allocation
    and payment are built on a basic sorting operation, which is not differentiable.
    Liu et al. overcome this issue by proposing a differentiable sorting engine that
    caters to the top-K selection in the multi-slot auctions, leveraging Neural-Sort
    [[12](#bib.bib12)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: DNA 还在神经网络框架内建模了整个分配和支付过程，因为将分配和支付视为一个不可知环境可能会限制深度学习的结果。其中一个挑战是，分配和支付都建立在一个基本的排序操作上，而这个操作是不可微分的。Liu
    等人通过提出一个适用于多插槽拍卖中前 K 名选择的可微分排序引擎——Neural-Sort [[12](#bib.bib12)] 来克服这个问题。
- en: In Equation [25](#S5.E25 "In 5 DeepSet Infrastructure ‣ A Survey of Online Auction
    Mechanism Design Using Deep Learning Approaches"), the intuitive interpretation
    of $\hat{M}_{r}[k,:]$ is the choice probabilities on all elements for getting
    the $k^{th}$ highest item. Where $A_{r}[i,j]=|r_{i}-r_{j}|$, and $\mathbbm{1}$
    denotes the column vector of all ones. The top-K payments can therefore by recovered
    by a simple matrix multiplication in Equation [26](#S5.E26 "In 5 DeepSet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches").
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 [25](#S5.E25 "In 5 DeepSet Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches") 中，$\hat{M}_{r}[k,:]$ 的直观解释是获取第 $k^{th}$
    高物品的所有元素上的选择概率。其中 $A_{r}[i,j]=|r_{i}-r_{j}|$，$\mathbbm{1}$ 表示全为一的列向量。因此，顶级支付可以通过方程
    [26](#S5.E26 "In 5 DeepSet Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches") 中的简单矩阵乘法来恢复。
- en: '|  | $f_{pay}=\hat{M}_{r}[1:K,:]\cdot[p_{1},p_{2},\dots,p_{N}]^{T}$ |  | (26)
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{pay}=\hat{M}_{r}[1:K,:]\cdot[p_{1},p_{2},\dots,p_{N}]^{T}$ |  | (26)
    |'
- en: 6 Constraints and Concerns
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 约束和关注点
- en: 'Online auction system is complex gaming system among bidders, sellers, and
    auction system administrators. Merely considering the revenue or social welfare
    for one party is more often sub-optimal. To the best of our knowledge, we have
    identified four large categories of constraints and concerns when administrators
    are designing their auction systems: IC & IR, data sparcity & high dimensionality,
    multiple performance metrics and objectives, and fairness & fraud prevention.
    We will illustrate below why these constraints matter and then summarize how current
    researchers are tackling with them.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在线拍卖系统是一个复杂的游戏系统，涉及竞标者、卖家和拍卖系统管理员。仅考虑一个方的收入或社会福利通常是次优的。据我们了解，我们已经识别出在设计拍卖系统时管理员面临的四大类约束和关注点：IC
    & IR、数据稀疏性与高维性、多种性能指标和目标，以及公平性与防欺诈。我们将在下面说明这些约束的重要性，然后总结当前研究人员如何应对这些问题。
- en: 6.1 Incentive Compatibility (IC) & Individual Rationality (IR)
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 激励兼容性 (IC) 和个人理性 (IR)
- en: Auction platform designers are more often interested in maximizing long-term
    objectives [[27](#bib.bib27)]. Therefore, building a reliable system that can
    adapt to the dynamic and complicated environment is crucial for the success.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 拍卖平台设计者通常更关注于最大化长期目标 [[27](#bib.bib27)]。因此，建立一个能够适应动态和复杂环境的可靠系统对成功至关重要。
- en: IC property ensures that all agents will achieve the best outcome by reporting
    their values truthfully. Auction participants may come from many different background
    and therefore informational, cognitive, and computational constraints will limit
    their rationality in different extent. The stability and reliability of the system
    will be much harder to maintain if IC cannot be satisfied, as designers and agents
    have to take all potential strategic behaviors of all other agents into account.
    Some researchers accomplished IC by adopting theoretical frameworks, such as GSP
    by DNA [[16](#bib.bib16)] and taxaction principal [[28](#bib.bib28)] by MenuNet
    [[24](#bib.bib24)]. Other researchers [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [14](#bib.bib14), [21](#bib.bib21)] achieved IC by enforcing ex post regret [[9](#bib.bib9)]
    to zero.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: IC 性质确保所有代理通过真实报告其价值来获得最佳结果。拍卖参与者可能来自不同的背景，因此信息、认知和计算约束会在不同程度上限制他们的理性。如果无法满足
    IC，系统的稳定性和可靠性将更难维持，因为设计者和代理必须考虑所有其他代理的潜在策略行为。一些研究人员通过采用理论框架来实现 IC，例如 DNA 的 GSP
    [[16](#bib.bib16)] 和 MenuNet [[24](#bib.bib24)] 的税收原则 [[28](#bib.bib28)]。其他研究人员
    [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [14](#bib.bib14), [21](#bib.bib21)]
    通过将事后遗憾 [[9](#bib.bib9)] 强制为零来实现 IC。
- en: IR property is also important as it ensures that all agents are receiving non-negative
    payoff. IR can be ensured by building upon the theoretical results from Myerson’s
    system [[20](#bib.bib20)]. It can also be enforced by integrating an additional
    constraint into the objective function and solve it using Augmented Lagrange Solver
    [[9](#bib.bib9), [10](#bib.bib10)]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: IR 性质也很重要，因为它确保所有代理都能获得非负的收益。可以通过基于 Myerson 系统的理论结果来确保 IR [[20](#bib.bib20)]。也可以通过将额外约束集成到目标函数中并使用增强拉格朗日求解器
    [[9](#bib.bib9), [10](#bib.bib10)] 来强制执行。
- en: 6.2 Data Sparsity & High Dimensionality
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 数据稀疏性与高维性
- en: The action space can blow up very quickly as the number of agents increase.
    The high dimensionality in action space can introduce severe computational burdens.
    In order to mitigate the computational burden, Tang et al. and Cai et al. decomposed
    the neural network into sub-networks for each seller and discretized the action
    spaces [[27](#bib.bib27), [6](#bib.bib6)].
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 随着代理数量的增加，动作空间可能迅速膨胀。动作空间中的高维度可能会引入严重的计算负担。为了减轻计算负担，唐等人和蔡等人将神经网络分解为每个卖家的子网络，并对动作空间进行离散化
    [[27](#bib.bib27), [6](#bib.bib6)]。
- en: In addition, many features are high-dimensional one-hot vectors, so data can
    be very sparse. The original regularization approaches take the entire vector
    into computations and the regularized vector has non-zero entries for most of
    the positions, which increases the computational time for sparse data drastically.
    Zhou et al. proposed a mini-batch aware regularization [[32](#bib.bib32)] approach,
    where only parameters of features appearing in the mini-batch participate in the
    computation of regularization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多特征是高维的独热编码向量，因此数据可能非常稀疏。原始正则化方法会将整个向量纳入计算，并且正则化向量在大多数位置上都有非零条目，这会大幅增加稀疏数据的计算时间。周等人提出了一种小批量感知的正则化
    [[32](#bib.bib32)] 方法，其中只有出现在小批量中的特征参数参与正则化的计算。
- en: '|  | $w_{j}\leftarrow w_{j}-\eta[\frac{1}{&#124;\mathcal{B}_{m}&#124;}\sum_{(x,y)\in\mathcal{B}_{m}}\frac{\partial
    L(p(x),y)}{\partial w_{j}}+\lambda\frac{\max_{(x,y)\in\mathcal{B}_{m}}1_{x_{j}\neq
    0}}{n_{j}}w_{j}]$ |  | (27) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{j}\leftarrow w_{j}-\eta[\frac{1}{\lvert\mathcal{B}_{m}\rvert}\sum_{(x,y)\in\mathcal{B}_{m}}\frac{\partial
    L(p(x),y)}{\partial w_{j}}+\lambda\frac{\max_{(x,y)\in\mathcal{B}_{m}}1_{x_{j}\neq
    0}}{n_{j}}w_{j}]$ |  | (27) |'
- en: The mini-batch aware regularization is shown in Equation [27](#S6.E27 "In 6.2
    Data Sparsity & High Dimensionality ‣ 6 Constraints and Concerns ‣ A Survey of
    Online Auction Mechanism Design Using Deep Learning Approaches"), where $\eta$
    is the learning rate, $\mathcal{B}_{m}$ is the $m^{th}$ batch, $w_{j}$ is the
    weight of $j^{th}$ feature, $n_{j}$ is the number of occurrences of $j^{th}$ feature.
    The numerator $\max_{(x,y)\in\mathcal{B}_{m}}1_{x_{j}\neq 0}$ denotes if at least
    one instance in the $m^{th}$ mini-batch has feature $j$.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量感知的正则化如方程 [27](#S6.E27 "在 6.2 数据稀疏性与高维度 ‣ 6 约束与关注 ‣ 深度学习方法在线拍卖机制设计调查") 中所示，其中
    $\eta$ 是学习率，$\mathcal{B}_{m}$ 是第 $m$ 批次，$w_{j}$ 是第 $j$ 特征的权重，$n_{j}$ 是第 $j$ 特征的出现次数。分子
    $\max_{(x,y)\in\mathcal{B}_{m}}1_{x_{j}\neq 0}$ 表示第 $m$ 批次中是否至少有一个实例具有特征 $j$。
- en: 6.3 Multiple Performance Metrics and Objectives
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 多重性能指标与目标
- en: The most intuitive objective for most auction designers is to maximize their
    profit. However, to adapt to the dynamic and complex nature of today’s online
    auction systems, designers may be better-off if they consider multiple performance
    metrics. Zhang et al. listed out several commonly used performance metrics (See
    [4](#S4 "4 Reinforcement Learning (RL) Infrastructure ‣ A Survey of Online Auction
    Mechanism Design Using Deep Learning Approaches")). Zhang et al. and Liu et al.
    optimizes a linear combination of functions of those metrics, where both the linear
    weights and functions of those metrics and be specified by auction designers [[31](#bib.bib31),
    [16](#bib.bib16)]. Over the time, designers are free to adjust the weights and
    functions if their objectives have changed.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数拍卖设计师来说，最直观的目标是最大化他们的利润。然而，为了适应现代在线拍卖系统的动态和复杂性质，设计师们可能需要考虑多个性能指标。张等人列出了几个常用的性能指标（见
    [4](#S4 "4 强化学习 (RL) 基础设施 ‣ 深度学习方法在线拍卖机制设计调查")）。张等人和刘等人优化了这些指标的线性组合，其中线性权重和这些指标的函数都由拍卖设计师指定
    [[31](#bib.bib31), [16](#bib.bib16)]。随着时间的推移，如果目标发生变化，设计师可以自由调整权重和函数。
- en: 6.4 Fairness & Fraud Prevention
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 公平性与防欺诈
- en: Due to the biases in the training data, many online platforms have discriminatory
    ad allocations among different demography [[14](#bib.bib14)]. One of the major
    social problems associated with online advertising is the use in the job market,
    where unfairness can be very detrimental to the equality and the protection of
    underrepresented groups.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练数据中的偏差，许多在线平台在不同人群中的广告分配存在歧视 [[14](#bib.bib14)]。与在线广告相关的一个主要社会问题是在就业市场上的使用，其中不公平性可能对平等和保护弱势群体造成严重损害。
- en: To mitigate the unfairness, PreferenceNet [[21](#bib.bib21)] integrated three
    definitions of fairness into the model, all of which map the allocations $g(b)$
    onto $\mathbb{R}$. In all these equations (Equation [28](#S6.E28 "In item 1 ‣
    6.4 Fairness & Fraud Prevention ‣ 6 Constraints and Concerns ‣ A Survey of Online
    Auction Mechanism Design Using Deep Learning Approaches"), Equation [29](#S6.E29
    "In item 2 ‣ 6.4 Fairness & Fraud Prevention ‣ 6 Constraints and Concerns ‣ A
    Survey of Online Auction Mechanism Design Using Deep Learning Approaches"), and
    Equation [30](#S6.E30 "In item 3 ‣ 6.4 Fairness & Fraud Prevention ‣ 6 Constraints
    and Concerns ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches")), $i$ refers to $i^{th}$ item while $j$ and $j^{\prime}$ refer to
    $j^{th}$ and $j^{\prime th}$ agents.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻不公平性，PreferenceNet [[21](#bib.bib21)] 将三种公平性定义集成到模型中，这些定义将分配 $g(b)$ 映射到
    $\mathbb{R}$。在所有这些方程（方程 [28](#S6.E28 "在项目 1 ‣ 6.4 公平性与欺诈预防 ‣ 6 约束与关注 ‣ 基于深度学习方法的在线拍卖机制设计调研")、方程
    [29](#S6.E29 "在项目 2 ‣ 6.4 公平性与欺诈预防 ‣ 6 约束与关注 ‣ 基于深度学习方法的在线拍卖机制设计调研") 和方程 [30](#S6.E30
    "在项目 3 ‣ 6.4 公平性与欺诈预防 ‣ 6 约束与关注 ‣ 基于深度学习方法的在线拍卖机制设计调研")）中，$i$ 指代第 $i$ 项，而 $j$
    和 $j^{\prime}$ 分别指代第 $j$ 个和第 $j^{\prime}$ 个代理。
- en: '1.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Total Variation Fairness (Equation [28](#S6.E28 "In item 1 ‣ 6.4 Fairness &
    Fraud Prevention ‣ 6 Constraints and Concerns ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches")): the distance between allocations cannot
    be larger than the discrepancies between these two users.'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总变差公平性（方程 [28](#S6.E28 "在项目 1 ‣ 6.4 公平性与欺诈预防 ‣ 6 约束与关注 ‣ 基于深度学习方法的在线拍卖机制设计调研")）：分配之间的距离不能大于这两个用户之间的差异。
- en: '|  | $\sum_{i\in C_{k}}&#124;g(b)_{i,j}-g(b)_{i,j^{\prime}}&#124;\leq d^{k}(j,j^{\prime}),\forall
    k\in\{1,\dots,c\},\forall j,j^{\prime}\in M$ |  | (28) |'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\sum_{i\in C_{k}}&#124;g(b)_{i,j}-g(b)_{i,j^{\prime}}&#124;\leq d^{k}(j,j^{\prime}),\forall
    k\in\{1,\dots,c\},\forall j,j^{\prime}\in M$ |  | (28) |'
- en: '2.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Entropy (Equation [29](#S6.E29 "In item 2 ‣ 6.4 Fairness & Fraud Prevention
    ‣ 6 Constraints and Concerns ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches")): the allocation for an agent tends to be more uniformly
    distributed.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 熵（方程 [29](#S6.E29 "在项目 2 ‣ 6.4 公平性与欺诈预防 ‣ 6 约束与关注 ‣ 基于深度学习方法的在线拍卖机制设计调研")）：一个代理的分配趋向于更均匀分布。
- en: '|  | $\max-\sum_{i=1}^{n}P(\frac{g(b)_{i.}}{\sum_{j}g(b)_{ij}})\log P(\frac{g(b)_{i.}}{\sum_{j}g(b)_{ij}})$
    |  | (29) |'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\max-\sum_{i=1}^{n}P(\frac{g(b)_{i.}}{\sum_{j}g(b)_{ij}})\log P(\frac{g(b)_{i.}}{\sum_{j}g(b)_{ij}})$
    |  | (29) |'
- en: '3.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Quota (Equation [30](#S6.E30 "In item 3 ‣ 6.4 Fairness & Fraud Prevention ‣
    6 Constraints and Concerns ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches")): the smallest allocation to any agent should be greater
    than some threshold.'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配额（方程 [30](#S6.E30 "在项目 3 ‣ 6.4 公平性与欺诈预防 ‣ 6 约束与关注 ‣ 基于深度学习方法的在线拍卖机制设计调研")）：对任何代理的最小分配应大于某个阈值。
- en: '|  | $\min_{j}(\frac{g(b)_{.j}}{\sum_{i}g(b)_{ij}})>t$ |  | (30) |'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{j}(\frac{g(b)_{.j}}{\sum_{i}g(b)_{ij}})>t$ |  | (30) |'
- en: ProportionNet [[14](#bib.bib14)] also adopted the notion of total variation
    fairness, so that the allocations to similar users cannot differ by too much.
    It converted the Equation [28](#S6.E28 "In item 1 ‣ 6.4 Fairness & Fraud Prevention
    ‣ 6 Constraints and Concerns ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches") into an unfairness constraint (Equation [31](#S6.E31
    "In 6.4 Fairness & Fraud Prevention ‣ 6 Constraints and Concerns ‣ A Survey of
    Online Auction Mechanism Design Using Deep Learning Approaches")) that can be
    fed into the Augmented Lagrange Solver, which allows us to quantify the unfairness
    of the auction outcome for all users involved.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ProportionNet [[14](#bib.bib14)] 也采用了总变差公平性的概念，以确保对相似用户的分配差异不会过大。它将方程 [28](#S6.E28
    "在项目 1 ‣ 6.4 公平性与欺诈预防 ‣ 6 约束与关注 ‣ 基于深度学习方法的在线拍卖机制设计调研") 转化为一个不公平性约束（方程 [31](#S6.E31
    "在 6.4 公平性与欺诈预防 ‣ 6 约束与关注 ‣ 基于深度学习方法的在线拍卖机制设计调研")），可以输入到扩展拉格朗日求解器中，从而使我们能够量化所有相关用户的拍卖结果不公平性。
- en: '|  | $unf_{j}=\sum_{j^{\prime}\in M}\sum_{C_{k}\in C}\max(0,\sum_{i\in C_{k}}\max(0,z_{ij}-z_{ij^{\prime}}))-d^{k}(j,j^{\prime}))$
    |  | (31) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $unf_{j}=\sum_{j^{\prime}\in M}\sum_{C_{k}\in C}\max(0,\sum_{i\in C_{k}}\max(0,z_{ij}-z_{ij^{\prime}}))-d^{k}(j,j^{\prime}))$
    |  | (31) |'
- en: 'While unfairness can be introduced by the biases in auction mechanisms, it
    can also be induced by shill bidding behaviors in auctions. Sellers can adopt
    a variety of shill bidding strategies to inflate the final selling price of an
    item. The common four shill bidding strategies have been identified [[22](#bib.bib22)]:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然拍卖机制中的偏见可能引入不公平，但拍卖中的虚假竞标行为也会导致不公平。卖家可以采用各种虚假竞标策略来抬高物品的最终售价。已经确定了常见的四种虚假竞标策略[[22](#bib.bib22)]：
- en: '1.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Evaluator: single bid engagement at an early time with a high amount.'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估者：在早期时间以高额出价进行单次竞标。
- en: '2.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Sniping: single bid engagement in the last moment, not leaving opportunity
    for anybody else to outbid.'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 冲刺：在最后时刻进行单次竞标，不留下任何其他人超出出价的机会。
- en: '3.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Unmasking: multiple bid engagement in a short span of time with a probability
    of intend to exposing the maximum bid or the highest bidders.'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 揭示：在短时间内进行多次竞标，意图暴露最大出价或最高竞标者的概率。
- en: '4.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Skeptic: multiple bid engagement with lowest possible bids each time.'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 怀疑者：每次以最低可能的出价进行多次竞标。
- en: 7 Conclusion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this article, we have gone through the rough evolving process of deep learning
    based online auction systems. Mechanisms designed using MLP infrastructure are
    usually built upon some theoretical results, and the MLP structure is used to
    represent the functions given in the theories. A more sophisticated structure
    came with the appearance of RegretNet, which parametrizes allocation rules and
    payment rules using separate networks. Many researchers have built extensions
    of RegretNet by integrating more constraints into the objective function or slightly
    adjusting the network structure but still keeping allocation and payment networks
    separate. The dynamic nature of online auction has encouraged researchers to adopt
    the deep reinforcement learning framework, which is more often a model-free approach
    that requires less assumptions on the data and is able to keep adapting itself
    as time progresses. As most traditional neural network has an implicit constraints
    on the positions of inputs, it integrates the ordering of auction participants
    into the model training, while in reality there is no inherent ordering among
    them. As a result, a deep learning based on DeepSet infrastructure has emerged,
    which can remove the effects of positions completely. We have also discussed the
    constraints and concerns faced by auction designers and we pointed out how researchers
    have attempted to address them.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了基于深度学习的在线拍卖系统的粗略演变过程。使用MLP基础设施设计的机制通常建立在一些理论结果之上，MLP结构用于表示理论中给出的函数。随着RegretNet的出现，出现了更复杂的结构，该结构使用独立的网络对分配规则和支付规则进行参数化。许多研究人员通过将更多约束集成到目标函数中或稍微调整网络结构来构建RegretNet的扩展，但仍保持分配和支付网络的分离。在线拍卖的动态特性促使研究人员采用深度强化学习框架，这通常是一种模型无关的方法，要求对数据的假设较少，并能够随着时间的推移不断适应。由于大多数传统神经网络对输入的位置有隐含约束，它将拍卖参与者的排序集成到模型训练中，而实际上它们之间没有固有的排序。因此，基于DeepSet基础设施的深度学习应运而生，这可以完全消除位置的影响。我们还讨论了拍卖设计师面临的约束和问题，并指出了研究人员如何尝试解决这些问题。
- en: Although researchers are progressing rapidly to the development of an online
    auction mechanism that can be reliable and profitable in the long term, there
    are still a lot of unresolved issues left for future researchers to investigate.
    As the size of users for online auction system is usually gigantic, the computational
    constraint and convergence problems for high-dimensional data are still non-negligible
    issues. Researchers are either mapping the data to lower dimension or reducing
    the action space by discretizing it, but it remains unclear how much information
    we are losing. In addition, most models assume the unit-demand and additive valuations,
    while this assumption might not be true in the real world. Last but not least,
    as most mechanism frameworks rely on the assumption that auction participants
    are independent from each other, their IC and IR constraints are also computed
    at individual levels. Therefore, their strategies might not be robust to the non-truthful
    behaviors conducted by participants in collusion.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究人员在开发一种在长期内可靠且有利可图的在线拍卖机制方面进展迅速，但仍有许多未解决的问题留待未来的研究者调查。由于在线拍卖系统的用户规模通常非常庞大，高维数据的计算约束和收敛问题仍然是不可忽视的问题。研究人员要么将数据映射到较低维度，要么通过离散化减少动作空间，但我们究竟损失了多少信息仍不清楚。此外，大多数模型假设单位需求和加性估值，而这一假设在现实世界中可能并不成立。最后但同样重要的是，由于大多数机制框架依赖于拍卖参与者彼此独立的假设，它们的IC和IR约束也在个体层面进行计算。因此，它们的策略可能对参与者之间的勾结行为不够稳健。
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Rajeev Agrawal. Sample mean based index policies by o(log n) regret for
    the multi-armed bandit problem. Advances in Applied Probability, 27(4):1054–1078,
    1995.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 拉吉夫·阿格拉瓦尔。《基于样本均值的多臂老虎机问题的o(log n)遗憾指数策略》。应用概率学进展，27(4):1054–1078，1995年。'
- en: '[2] P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. Gambling in a rigged
    casino: The adversarial multi-armed bandit problem. In Proceedings of IEEE 36th
    Annual Foundations of Computer Science, pages 322–331, 1995.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] P·奥尔，N·切萨-比安奇，Y·弗伦德，和R·E·沙皮雷。《在一个被操控的赌场里赌博：对抗性多臂老虎机问题》。在IEEE第36届年度计算机科学基础会议论文集中，322–331页，1995年。'
- en: '[3] Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis
    of the multiarmed bandit problem. Machine Learning, 47(2–3):235–256, 2002.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 彼得·奥尔，尼科洛·切萨-比安奇，和保罗·费舍尔。《多臂老虎机问题的有限时间分析》。机器学习，47(2–3):235–256，2002年。'
- en: '[4] Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The
    nonstochastic multiarmed bandit problem. SIAM J. Comput., 32(1):48–77, January
    2003.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 彼得·奥尔，尼科洛·切萨-比安奇，约阿夫·弗伦德，和罗伯特·E·沙皮雷。《非随机多臂老虎机问题》。SIAM计算杂志，32(1):48–77，2003年1月。'
- en: '[5] Lawrence M. Ausubel and Paul Milgrom. The Lovely but Lonely Vickrey Auction.
    In Combinatorial Auctions, chapter 1, pages 17–40\. MIT Press, 2006.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 劳伦斯·M·奥苏贝尔和保罗·米尔格罗姆。《可爱但孤独的维克雷拍卖》。在《组合拍卖》，第1章，17–40页。麻省理工学院出版社，2006年。'
- en: '[6] Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, and Yiwei Zhang. Reinforcement
    mechanism design for e-commerce, 2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 蔡青鹏，阿里斯·费洛斯-拉西卡斯，唐平中，和张艺伟。《电子商务的强化机制设计》，2018年。'
- en: '[7] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate
    deep network learning by exponential linear units (elus), 2016.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 乔克-阿尔内·克莱维特，托马斯·乌特特纳，和塞普·霍赫赖特。《通过指数线性单元（elus）进行快速准确的深度网络学习》，2016年。'
- en: '[8] Hennie Daniels and Marina Velikova. Monotone and partially monotone neural
    networks. IEEE Transactions on Neural Networks, 21(6):906–917, 2010.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 亨尼·丹尼尔斯和玛丽娜·维利科娃。《单调和部分单调神经网络》。IEEE神经网络学报，21(6):906–917，2010年。'
- en: '[9] Paul Dütting, Zhe Feng, Harikrishna Narasimhan, David C. Parkes, and Sai Srivatsa
    Ravindranath. Optimal auctions through deep learning, 2020.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 保罗·杜廷，冯哲，哈里克里希纳·纳拉西曼，大卫·C·帕克斯，和赛·斯里瓦萨·拉文德拉纳斯。《通过深度学习实现最优拍卖》，2020年。'
- en: '[10] Zhe Feng, Harikrishna Narasimhan, and David C. Parkes. Deep learning for
    revenue-optimal auctions with budgets. In Proceedings of the 17th International
    Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, page 354–362,
    Richland, SC, 2018\. International Foundation for Autonomous Agents and Multiagent
    Systems.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 冯哲，哈里克里希纳·纳拉西曼，和大卫·C·帕克斯。《用于预算的收益最优拍卖的深度学习》。在第17届国际自主代理和多代理系统会议论文集中，AAMAS
    ’18，第354–362页，南卡罗来纳州里士满，2018年。国际自主代理和多代理系统基金会。'
- en: '[11] Noah Golowich, Harikrishna Narasimhan, and David C. Parkes. Deep learning
    for multi-facility location mechanism design. In Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence, IJCAI-18, pages 261–267.
    International Joint Conferences on Artificial Intelligence Organization, 7 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Noah Golowich, Harikrishna Narasimhan 和 David C. Parkes. 多设施位置机制设计的深度学习。发表于第二十七届国际人工智能联合会议论文集，IJCAI-18，页面261–267。国际人工智能联合会议组织，2018年7月。'
- en: '[12] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization
    of sorting networks via continuous relaxations, 2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Aditya Grover, Eric Wang, Aaron Zweig 和 Stefano Ermon. 通过连续松弛的排序网络的随机优化，2019年。'
- en: '[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into
    rectifiers: Surpassing human-level performance on imagenet classification. In
    2015 IEEE International Conference on Computer Vision (ICCV), pages 1026–1034,
    2015.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun. 深入研究整流器：在Imagenet分类上超越人类水平的表现。发表于2015年IEEE国际计算机视觉会议（ICCV），页面1026–1034，2015年。'
- en: '[14] Kevin Kuo, Anthony Ostuni, Elizabeth Horishny, Michael J. Curry, Samuel
    Dooley, Ping yeh Chiang, Tom Goldstein, and John P. Dickerson. Proportionnet:
    Balancing fairness and revenue for auction design with deep learning, 2020.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Kevin Kuo, Anthony Ostuni, Elizabeth Horishny, Michael J. Curry, Samuel
    Dooley, Ping Yeh Chiang, Tom Goldstein 和 John P. Dickerson. Proportionnet: 在拍卖设计中平衡公平性和收益，利用深度学习，2020年。'
- en: '[15] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver 和 Daan Wierstra. 深度强化学习的连续控制，2019年。'
- en: '[16] Xiangyu Liu, Chuan Yu, Zhilin Zhang, Zhenzhe Zheng, Yu Rong, Hongtao Lv,
    Da Huo, Yiqing Wang, Dagui Chen, Jian Xu, Fan Wu, Guihai Chen, and Xiaoqiang Zhu.
    Neural auction: End-to-end learning of auction mechanisms for e-commerce advertising,
    2021.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Xiangyu Liu, Chuan Yu, Zhilin Zhang, Zhenzhe Zheng, Yu Rong, Hongtao Lv,
    Da Huo, Yiqing Wang, Dagui Chen, Jian Xu, Fan Wu, Guihai Chen 和 Xiaoqiang Zhu.
    神经拍卖：电子商务广告的端到端拍卖机制学习，2021年。'
- en: '[17] Nguyen Cong Luong, Zehui Xiong, Ping Wang, and Dusit Niyato. Optimal auction
    for edge computing resource management in mobile blockchain networks: A deep learning
    approach, 2017.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Nguyen Cong Luong, Zehui Xiong, Ping Wang 和 Dusit Niyato. 针对移动区块链网络中的边缘计算资源管理的最优拍卖：一种深度学习方法，2017年。'
- en: '[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel
    Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland,
    Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,
    Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level
    control through deep reinforcement learning. Nature, 518(7540):529–533, February
    2015.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel
    Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland,
    Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,
    Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg 和 Demis Hassabis. 通过深度强化学习实现人类水平的控制。自然，518(7540):529–533，2015年2月。'
- en: '[19] H. Moulin. On strategy-proofness and single peakedness. Public Choice,
    35(4):437–455, 1980.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] H. Moulin. 关于策略无关性和单峰性的研究。公共选择，35(4):437–455，1980年。'
- en: '[20] Roger B. Myerson. Optimal auction design. Mathematics of Operations Research,
    6(1):58–73, 1981.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Roger B. Myerson. 最优拍卖设计。运筹学数学，6(1):58–73，1981年。'
- en: '[21] Neehar Peri, Michael J. Curry, Samuel Dooley, and John P. Dickerson. Preferencenet:
    Encoding human preferences in auction design with deep learning, 2021.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Neehar Peri, Michael J. Curry, Samuel Dooley 和 John P. Dickerson. Preferencenet:
    用深度学习在拍卖设计中编码人类偏好，2021年。'
- en: '[22] Saurabh R. Sangwan and Anshika Arora. Supervised machine learning based
    buyer’s bidding behaviour detection in online auction. Social Science Research
    Network, 2019.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Saurabh R. Sangwan 和 Anshika Arora. 基于监督学习的买家竞标行为检测在线拍卖。社会科学研究网络，2019年。'
- en: '[23] Weiran Shen, Binghui Peng, Hanpeng Liu, Michael Zhang, Ruohan Qian, Yan
    Hong, Zhi Guo, Zongyao Ding, Pengjun Lu, and Pingzhong Tang. Reinforcement mechanism
    design, with applications to dynamic pricing in sponsored search auctions, 2017.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Weiran Shen, Binghui Peng, Hanpeng Liu, Michael Zhang, Ruohan Qian, Yan
    Hong, Zhi Guo, Zongyao Ding, Pengjun Lu 和 Pingzhong Tang. 强化机制设计及其在赞助搜索拍卖中的动态定价应用，2017年。'
- en: '[24] Weiran Shen, Pingzhong Tang, and Song Zuo. Automated mechanism design
    via neural networks, 2021.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Weiran Shen, Pingzhong Tang 和 Song Zuo. 通过神经网络实现自动化机制设计，2021年。'
- en: '[25] MyungJae Shin, Joongheon Kim, and Marco Levorato. Auction-based charging
    scheduling with deep learning framework for multi-drone networks. IEEE Transactions
    on Vehicular Technology, 68(5):4235–4248, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] MyungJae Shin, Joongheon Kim, 和 Marco Levorato. 基于拍卖的充电调度与深度学习框架用于多无人机网络。IEEE
    车辆技术学报, 68(5):4235–4248, 2019。'
- en: '[26] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
    and Martin Riedmiller. Deterministic policy gradient algorithms. In Proceedings
    of the 31st International Conference on International Conference on Machine Learning
    - Volume 32, ICML’14, page I–387–I–395\. JMLR.org, 2014.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
    和 Martin Riedmiller. 确定性策略梯度算法。见于第31届国际机器学习大会论文集 - 第32卷，ICML’14，第I–387–I–395页。JMLR.org,
    2014。'
- en: '[27] Pingzhong Tang. Reinforcement mechanism design. In Proceedings of the
    Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17,
    pages 5146–5150, 2017.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Pingzhong Tang. 强化机制设计。见于第26届国际人工智能联合会议论文集，IJCAI-17，第5146–5150页，2017年。'
- en: '[28] Rakesh Vohra. Mechanism design. a linear programming approach. Mechanism
    Design: A Linear Programming Approach, 01 2010.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Rakesh Vohra. 机制设计：线性规划方法。《机制设计：线性规划方法》，2010年1月。'
- en: '[29] Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards.
    PhD thesis, King’s College, Cambridge, UK, May 1989.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Christopher John Cornish Hellaby Watkins. 从延迟奖励中学习。博士论文，剑桥大学国王学院，英国，1989年5月。'
- en: '[30] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabás Póczos, Ruslan
    Salakhutdinov, and Alexander J. Smola. Deep sets. CoRR, abs/1703.06114, 2017.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabás Póczos, Ruslan
    Salakhutdinov, 和 Alexander J. Smola. 深度集合。CoRR, abs/1703.06114, 2017。'
- en: '[31] Zhilin Zhang, Xiangyu Liu, Zhenzhe Zheng, Chenrui Zhang, Miao Xu, Junwei
    Pan, Chuan Yu, Fan Wu, Jian Xu, and Kun Gai. Optimizing multiple performance metrics
    with deep gsp auctions for e-commerce advertising, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Zhilin Zhang, Xiangyu Liu, Zhenzhe Zheng, Chenrui Zhang, Miao Xu, Junwei
    Pan, Chuan Yu, Fan Wu, Jian Xu, 和 Kun Gai. 使用深度 GSP 拍卖优化多种性能指标用于电子商务广告, 2021。'
- en: '[32] Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma,
    Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through
    rate prediction, 2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma,
    Yanghui Yan, Junqi Jin, Han Li, 和 Kun Gai. 点击率预测的深度兴趣网络, 2018。'
