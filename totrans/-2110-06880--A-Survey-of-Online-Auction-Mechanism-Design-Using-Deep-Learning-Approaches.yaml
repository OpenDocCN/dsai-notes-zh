- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:50:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:50:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.06880] A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.06880] 使用深度学习方法的在线拍卖机制设计综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.06880](https://ar5iv.labs.arxiv.org/html/2110.06880)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.06880](https://ar5iv.labs.arxiv.org/html/2110.06880)
- en: A Survey of Online Auction Mechanism Design Using Deep Learning Approaches
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习方法的在线拍卖机制设计综述
- en: Zhanhao Zhang
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张展豪
- en: Department of Statistics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学系
- en: Columbia University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: zz2760@columbia.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: zz2760@columbia.edu
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Online auction has been very widespread in the recent years. Platform administrators
    are working hard to refine their auction mechanisms that will generate high profits
    while maintaining a fair resource allocation. With the advancement of computing
    technology and the bottleneck in theoretical frameworks, researchers are shifting
    gears towards online auction designs using deep learning approaches. In this article,
    we summarized some common deep learning infrastructures adopted in auction mechanism
    designs and showed how these architectures are evolving. We also discussed how
    researchers are tackling with the constraints and concerns in the large and dynamic
    industrial settings. Finally, we pointed out several currently unresolved issues
    for future directions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在线拍卖在近年来变得非常普及。平台管理员正在努力完善他们的拍卖机制，以实现高利润，同时保持公平的资源分配。随着计算技术的进步和理论框架的瓶颈，研究人员正转向使用深度学习方法的在线拍卖设计。在本文中，我们总结了一些在拍卖机制设计中采用的常见深度学习基础设施，并展示了这些架构的演变过程。我们还讨论了研究人员如何在大型和动态的工业环境中应对约束和关注点。最后，我们指出了几个目前尚未解决的问题，以供未来的研究方向参考。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Auction has been adopted as a way to negotiate the exchanges of goods and commodities
    for centuries. Traditionally, Generalized second price auction (GSP) and Vickrey–Clarke–Groves
    auction (VCG) are widely used. However, GSP is no longer a truthful mechanism
    if a seller has more than one item to bid. VCG is an auction mechanism based on
    sealed-second price auction, where winners are charged on the reductions of social
    welfare of other participants. Nevertheless, the VCG mechanism generates low seller
    revenues and does not enforce monotinicity of seller’s revenues in the set of
    bidders and the amounts bid. It is also a non-truthful mechanism that is susceptible
    to multiple bids under same person or collusion of losing bidders [[5](#bib.bib5)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 拍卖作为一种交换商品和货物的方式已经被采用了几个世纪。传统上，广义第二价格拍卖（GSP）和维克雷-克拉克-格罗夫斯拍卖（VCG）被广泛使用。然而，如果卖方有多个物品进行竞标，GSP
    不再是一个真实的机制。VCG 是一种基于封闭第二价格拍卖的拍卖机制，获胜者根据其他参与者社会福利的减少来收取费用。然而，VCG 机制产生的卖方收入较低，并且未能在竞标者集和竞标金额中强制卖方收入的单调性。它也是一种不真实的机制，易受到同一人多次竞标或失败竞标者的勾结[[5](#bib.bib5)]。
- en: Auction mechanisms with the properties of incentive compatiblility (IC) and
    individual rationality (IR) are highly desirable. If an auction is IC, then all
    bidders will truthfully reveal their private valuations of the items, so that
    platform administrators do not have the burden of considering bidders’ strategic
    behaviors and are therefore able to build a reliable and predictable system. All
    agents are guaranteed to have non-negative utilities if the auction system is
    IR, and it is a very important feature that allows the system to retain its customers
    in the long run.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有激励兼容性（IC）和个体理性（IR）属性的拍卖机制非常理想。如果一个拍卖是IC的，那么所有竞标者将真实地揭示他们对物品的私人估价，这样平台管理员就不必考虑竞标者的策略行为，因此能够建立一个可靠且可预测的系统。如果拍卖系统是IR的，那么所有参与者都保证具有非负效用，这是一项非常重要的特性，使得系统能够在长期内留住客户。
- en: The groundbreaking work by Myerson [[20](#bib.bib20)] has defined the optimal
    strategyproof auction for selling a single item, but limited progress has been
    made in characterizing strategyproof and revenue-maximizing auctions beyond this
    setting [[14](#bib.bib14)]. The dynamic nature of online auction platforms [[6](#bib.bib6),
    [27](#bib.bib27), [31](#bib.bib31)] has made the problems more challenging, as
    the bidders, items, and platform’s objectives are changing over time. In the meantime,
    multiple performance metrics are required to be taken into considerations in order
    to make the auction system attractive to bidders, sellers, and the platform [[16](#bib.bib16),
    [27](#bib.bib27), [31](#bib.bib31)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: With the advancement of technology, researchers are shifting gears towards deep
    learning approaches for the design of auction mechanisms. To the best of our knowledge,
    the deep learning architecture is built on top of either the hybrid multiple linear
    perceptron infrastructure, the RegretNet infrastructure [[9](#bib.bib9)], the
    reinforcement learning infrastructure, or the DeepSet infrastructure [[30](#bib.bib30)].
    Blessed with the modern computing power, researchers are not only able to maximize
    the revenue, but also dealing with data sparsity and high-dimensional data, optimizing
    multiple performance metrics, preventing fraud, and enhancing fairness.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is organized as follows: we will first introduce the four common
    infrastructure of the deep neural networks for auction mechanism design. Then,
    we will discuss how researchers are tackling with the constraints and concerns
    other than maximizing the total revenue. Lastly, we will point out some unresolved
    issues and potential future directions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 2 Hybrid Multiple Linear Perceptron (MLP) Infrastructure
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many neural network structures are built by stacking several MLP structure into
    a cohesive one [[24](#bib.bib24), [32](#bib.bib32), [25](#bib.bib25), [17](#bib.bib17)].
    The network is usually comprised of more than one components, where each components
    is a fully-connected feedforward neural network.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The most delicate architecture is the MenuNet [[24](#bib.bib24)], which is comprised
    of a mechanism network and a buyer network. The mechanism takes a one-dimensional
    1 as input and outputs an allocation matrix and a pricing vector. The allocation
    matrix contains the allocation of all items, which is obtained by a fully-connected
    layer followed by a sigmoid activation function. The payment vector is obtained
    by simply multiplying the constant 1 by a vector and it is used to represent the
    prices for different menu items. The buyer network doesn’t require any training.
    It takes the outputs from the mechanism network and computes the final utility
    based on buyer’s value profile. The training of MenuNet is very fast as the network
    structure is very simple. It is built upon the taxation principle [[28](#bib.bib28)],
    which states that simply letting the buyer do the selection can give an IC mechanism.
    It does not require buyer’s utility function since the network only outputs buyer’s
    strategy. It does not make any assumptions about buyer’s valuation and does not
    require any additional constraints (such as IC and IR) to be enforced to the network.
    Theoretical proofs have shown that the MenuNet always return revenue optimal mechanism
    with IC satisfied for menus of size 2 or 3.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Zhou et al has proposed Deep Interest Network [[32](#bib.bib32)] for the use
    of click-through rate prediction. It is built on the structure of MLP but aims
    to conquer the bottleneck caused by the fixed-length representation vector used
    in traditional MLP, which has limited ability in capturing user’s diverse interests
    from rich historical behaviors. The authors designed a local activation unit that
    can adaptively learn the representation of user interests from historical behaviors
    with respect to a certain advertisements. The data adaptive activation they adopted
    is Dice (Equation [1](#S2.E1 "In 2 Hybrid Multiple Linear Perceptron (MLP) Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches")),
    which is a generalization of PReLu [[13](#bib.bib13)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f(s)=p(s)\cdot s+(1-p(s))\cdot\alpha s,p(s)=\frac{1}{1+e^{-\frac{s-E[s]}{\sqrt{Var[s]+\epsilon}}}}$
    |  | (1) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: In the training phase, $E[s]$ and $Var[s]$ are the mean and variance of input
    in each-minibatch, while in the testing phase, $E[s]$ and $Var[s]$ are moving
    averages of $E[s]$ and $Var[s]$ over data. $\epsilon$ is a small constant and
    is set to be $10^{-8}$ by authors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: They adopt different representation vector for different ads. The embedding
    layer uses single embedding vector (one-hot) and multiple embedding vectors (multi-hot)
    in combination. Pooling and concat layers are added to transform the list of embedding
    vectors into the same lengths, so that the network can allow different users to
    have different number of behaviors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Shin et al transformed the charging scheduling problem into an auction problem
    using deep learning framework [[25](#bib.bib25)]. It is designed based on the
    concept of the Myerson auction [[20](#bib.bib20)], which is one of the most efficient
    revenue-optimal single-item auctions. One of the most challenging issues about
    charging scheduling is the lack of prior knowledge on the distribution of the
    number of bidders. Employing the auction approach is useful when there is no accurate
    information of buyer’s true valuation, and buyers are not aware of the private
    true values of other buyers. Buyer’s values are represented by the urgency of
    drone machines, while seller’s revenue is generated from the payment from resource
    allocation. As Myerson’s auction system requires full knowledge of the distribution
    of bids in order to compute the expected payment, Shin et al used deep neural
    network to parametrize the virtual valuation function, the allocaton rule, and
    the payment rule.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The network begins with a monotonic network that transforms the bids $b_{i}$
    of the drone into $\bar{b}_{i}$ using the virtual valuation function $\phi^{mononet}$
    parametrized by the network. In the $\phi^{mononet}$, all outcomes of $\phi^{shared}$
    (Equation [3](#S2.E3 "In 2 Hybrid Multiple Linear Perceptron (MLP) Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"))
    are computed using the same weights, while the $\phi_{i}$ (Equation [2](#S2.E2
    "In 2 Hybrid Multiple Linear Perceptron (MLP) Infrastructure ‣ A Survey of Online
    Auction Mechanism Design Using Deep Learning Approaches")) calculates the outcome
    using different weights for each bid.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $b_{i}^{\prime}=\phi_{i}(b_{i})=\min_{1\leq g\leq\mathcal{G}}\{\max_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{i}b_{i}+\beta_{g,n}^{i})\}$ |  | (2) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '|  | $\bar{b}_{i}=\phi_{i}^{shared}(b_{i}^{\prime})=\min_{1\leq g\leq\mathcal{G}}\{\max_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{shared}b_{i}^{\prime}+\beta_{g,n}^{shared})\}$ |  |
    (3) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: The payment rule network takes in the output $\bar{b}_{i}$ from the monotonic
    network and returns $\bar{p}_{i}$ according to Equation [4](#S2.E4 "In 2 Hybrid
    Multiple Linear Perceptron (MLP) Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches"). Then, the payment value is computed using
    the inverse function of $\phi^{mononet}$ (Equation [5](#S2.E5 "In 2 Hybrid Multiple
    Linear Perceptron (MLP) Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches"), [6](#S2.E6 "In 2 Hybrid Multiple Linear
    Perceptron (MLP) Infrastructure ‣ A Survey of Online Auction Mechanism Design
    Using Deep Learning Approaches")). Finally, the allocation rule network assigns
    the highest winning probability to the highest bidder with positive transformed
    bid.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bar{p}_{i}=ReLU\{\max_{j\neq i}(\bar{b}_{i})\}$ |  | (4) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '|  | $p_{i}^{\prime}=\phi_{shared}^{-1}(\bar{p}_{i})=\max_{1\leq g\leq\mathcal{G}}\{\min_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{shared})^{-1}(\bar{p}_{i}-\beta_{g,n}^{shared})\}$
    |  | (5) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '|  | $p_{i}=\phi_{i}^{-1}(p_{i}^{\prime})=\max_{1\leq g\leq\mathcal{G}}\{\min_{1\leq
    n\leq\mathcal{N}}(w_{g,n}^{i})^{-1}(p_{i}^{\prime}-\beta_{g,n}^{i})\}$ |  | (6)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: 'Luong et al constructed a neural network [[17](#bib.bib17)] for edge computing
    resource management based on analytical solution [[20](#bib.bib20)], which guarantees
    the revenue maximization while ensuring the IC and IR. The network structure also
    has three key components as in [[25](#bib.bib25)]: neural network parametrized
    monotone transformation functions that map bids into transformed versions, an
    allocation rule that maps the transformed bids to a vector of assignment probabilities,
    and a conditional payment rule that is based on the maximum non-negative transformed
    bids. The allocation and payment rules are derived from SPA-0, second price auction
    with 0 reserve price, where the reserve price is the mininum price a seller is
    willing to accept from the buyer.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'RochetNet [[9](#bib.bib9)] proposed by Dutting et al. is also an application
    of MLP. The RochetNet is a single-layered neural network that takes in the bids
    and outputs the maximum non-negative transformed values. It is used to model a
    non-negative, monotone, convex, and Lipschitz utility function, using $J$ linear
    functions with non-negative coefficients. The RochetNet easily extends to a single
    bidder with a unit-demand valuation ¹¹1Unit-demand valuation: the value of a subset
    is the maximum individual valuation within that subset.. Each linear function
    in the RochetNet corresponds to an option on the menue, with the allocation probabilities
    and payments encoded through its slope and intercept.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The MoulinNet [[11](#bib.bib11)] proposed by Golowich et al. also adopts the
    structure of MLP, which is used to determine the optimal facility locations preferred
    by agents. MoulinNet is a monotone feed-forward neural network that learns the
    generalized median rules [[19](#bib.bib19)]. For single-facility mechanisms, the
    mechanism in Equation [7](#S2.E7 "In 2 Hybrid Multiple Linear Perceptron (MLP)
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches") is strategy-proof, which selects the median of agents’ most preferred
    locations (the agents’ peaks). The inputs of the network are binary-encoded vectors
    $\nu(S)$ that represent whether the bidded items in $S$ are selected. $w$ and
    $b$ are parameters in MoulinNet. The $u_{i}$ is the utility function for agent
    $i$ and $\tau$ represents the peaks of the facility. The output of the network
    is the optimal selection rules based on utilities.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f^{w,b}(u)=\min_{S\subseteq N}\{\max_{i\in S}\{\tau(u_{i}),h^{w,b}(\nu(S))\}\}$
    |  | (7) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: 3 RegretNet Infrastructure
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The RegretNet [[9](#bib.bib9)] proposed by Dutting et al is comprised of an
    allocation network and a payment network. Both are built upon the MLP infrastructure,
    but the RegretNet has been adopted and extended in the auction designs in various
    settings [[10](#bib.bib10), [11](#bib.bib11), [21](#bib.bib21), [14](#bib.bib14)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Two basic assumptions are required by the RegretNet architecture: additive
    valuation ²²2Additive valuation: an agent’s valuation for a subset of items is
    the sum of the individual items’ valuations. and unit-demand valuation. Both of
    the allocation network and the payment network takes in the bids as inputs, feeds
    them into MLP-structured networks with separate parameters, and returns the total
    payments based on the outputs from two networks. Therefore, the two networks are
    trained together. The network uses a sigmoidal unit to normalize the payment vector
    into [0, 1], so that the IR constraint will be enforced, where bidders are never
    charged for more than their expected value for the allocation.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function is aiming to minimize the empirical loss (negated revenue)
    subject to the IC and IR constraints. The IC constraint can be enforced by the
    notion of ex post regret for bidders, which is the maximum increase in their utility
    considering all possible non-truthful bids. The ex post regret is estimated by
    the empirical regret, which is denoted as $\hat{rgt}_{i}(w)$. Therefore, the objective
    function becomes (Equation [8](#S3.E8 "In 3 RegretNet Infrastructure ‣ A Survey
    of Online Auction Mechanism Design Using Deep Learning Approaches")):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\min_{w\in\mathbb{R}^{d}}-\frac{1}{L}\sum_{l=1}^{L}\sum_{i=1}^{n}p_{i}^{w}(v^{(l)})\\
    \text{s.t. }\hat{rgt}_{i}(w)=0,\forall i\in N.\end{split}$ |  | (8) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: 'The optimization is achieved using Lagrange multipliers, augmented with a quadratic
    penalty term for violating the constraints (Equation [9](#S3.E9 "In 3 RegretNet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches")):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i=1}^{n}p_{i}^{w}(v^{(l)})+\sum_{i\in
    N}\lambda_{i}\hat{rgt}_{i}(w)+\frac{\rho}{2}\sum_{i\in N}(\hat{rgt}_{i}(w))^{2}$
    |  | (9) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: 'Feng et al constructed a neural network [[10](#bib.bib10)] built upon the structure
    of RegretNet, which consists of an allocation network and a payment network. It
    extends the RegretNet infrastructure by incorporating the budget constraints as
    well as handling Bayesian Incentive Compatible (BIC) ³³3Bayesian Incentive Compatible:
    truth-telling is the optimal strategy for a bidder in expectation with respect
    to the types of others, given that the other bidders report truthfully. and conditional
    IC constraints. Dutting et al enforces IC by requiring the empirical ex post regret
    to be zero, while Feng et al are able to handle more general forms of IC by constructing
    an appropriate notion of regret.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Assume we have an auction with rules $(a,p)$. To handle BIC, Feng et al constrain
    the empirical interim regret $rgt_{i}(a,p)$ (Equation [10](#S3.E10 "In 3 RegretNet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches")) to zero. To handle conditional IC/BIC, they constrain the empirical
    conditional regret to zero. They also incorporate the individually rationality
    (IR) $irp_{i}(a,p)$ (Equation [11](#S3.E11 "In 3 RegretNet Infrastructure ‣ A
    Survey of Online Auction Mechanism Design Using Deep Learning Approaches")) and
    budget constraint (BC) $bcp_{i}(a,p)$ (Equation [12](#S3.E12 "In 3 RegretNet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"))
    as penalties.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $rgt_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max_{t_{i}^{\prime}\in\mathcal{T}_{i}}1_{\mathcal{P}_{i}(t_{i}^{\prime})\leq
    b_{i}}(\mathcal{U}_{i}(t_{i},t_{i}^{\prime})-\mathcal{U}_{i}(t_{i},t_{i}))]$ |  |
    (10) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '|  | $irp_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max\{0,-\mathcal{U}_{i}(t_{i},t_{i})\}]$
    |  | (11) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '|  | $bcp_{i}(a,p)=E_{t_{i}\sim F_{i}}[\max\{0,\mathcal{P}_{i}(t_{i})-b_{i}\}]$
    |  | (12) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: The loss function is the negated expected revenue $\mathcal{L}(a,p)=-E_{t\sim
    F}[\sum_{i=1}^{n}p_{i}(t)]$. Let $w\in\mathbb{R}^{d}$ denote the parameters of
    the allocation network, the induced allocation rule denoted by $a^{w}$, and $w^{\prime}\in\mathbb{R}^{d^{\prime}}$
    denote the parameters of the payment network, the induced payment rule is denoted
    by $p^{w^{\prime}}$. The objective function is finally in Equation [13](#S3.E13
    "In 3 RegretNet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches"). The objective function is trained using Augmented
    Lagrangian Solver as in Dutting et al, where the quadratic penalty terms are added
    for each constraint.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}\min_{w\in\mathbb{R}^{d},w^{\prime}\in\mathbb{R}^{d^{\prime}}}\mathcal{L}(a^{w},p^{w^{\prime}})\\
    \text{s.t. }rgt_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: irp_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: bcp_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mrow ><mrow ><munder ><mi >min</mi><mrow ><mrow  ><mi
    >w</mi><mo >∈</mo><msup ><mi >ℝ</mi><mi >d</mi></msup></mrow><mo >,</mo><mrow
    ><msup ><mi >w</mi><mo >′</mo></msup><mo >∈</mo><msup ><mi >ℝ</mi><msup ><mi >d</mi><mo
    >′</mo></msup></msup></mrow></mrow></munder><mo lspace="0.167em"  >⁡</mo><mi >ℒ</mi></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi
    >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup ><mi  >w</mi><mo >′</mo></msup></msup><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="right"
    ><mrow ><mrow ><mrow ><mtext  >s.t. </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi  >t</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi
    >p</mi><msup ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mrow ><mi  >i</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >r</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi  >p</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup
    ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><mrow ><mi  >b</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >c</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi  >p</mi><mi
    >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><msup ><mi >a</mi><mi >w</mi></msup><mo >,</mo><msup ><mi >p</mi><msup
    ><mi >w</mi><mo >′</mo></msup></msup><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >=</mo><mn >0</mn></mrow><mo >,</mo><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >i</mi></mrow><mo >∈</mo><mrow ><mo stretchy="false"  >[</mo><mi >n</mi><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><ci >𝑤</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℝ</ci><ci
    >𝑑</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑤</ci><ci >′</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℝ</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑑</ci><ci >′</ci></apply></apply></apply></apply></apply><ci
    >ℒ</ci></apply><interval closure="open"  ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑎</ci><ci >𝑤</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑝</ci><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑤</ci><ci
    >′</ci></apply></apply></interval><ci ><mtext >s.t. </mtext></ci><ci >𝑟</ci><ci  >𝑔</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><ci >𝑖</ci></apply><interval
    closure="open" ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑎</ci><ci
    >𝑤</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑝</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑤</ci><ci >′</ci></apply></apply></interval></apply><cn
    type="integer"  >0</cn></apply><apply ><apply ><apply ><csymbol cd="latexml"  >for-all</csymbol><ci
    >𝑖</ci></apply><apply ><apply ><csymbol cd="latexml"  >delimited-[]</csymbol><ci
    >𝑛</ci></apply><ci >𝑖</ci><ci >𝑟</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑝</ci><ci >𝑖</ci></apply><interval closure="open" ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑎</ci><ci >𝑤</ci></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑝</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑤</ci><ci >′</ci></apply></apply></interval></apply></apply><apply ><cn type="integer"  >0</cn></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="latexml" >for-all</csymbol><ci >𝑖</ci></apply><apply
    ><apply ><csymbol cd="latexml" >delimited-[]</csymbol><ci >𝑛</ci></apply><ci >𝑏</ci><ci  >𝑐</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑝</ci><ci >𝑖</ci></apply><interval
    closure="open" ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑎</ci><ci
    >𝑤</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑝</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑤</ci><ci >′</ci></apply></apply></interval></apply></apply><apply
    ><cn type="integer" >0</cn></apply></apply><apply ><apply ><csymbol cd="latexml"
    >for-all</csymbol><ci >𝑖</ci></apply><apply ><csymbol cd="latexml" >delimited-[]</csymbol><ci
    >𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >\begin{split}\min_{w\in\mathbb{R}^{d},w^{\prime}\in\mathbb{R}^{d^{\prime}}}\mathcal{L}(a^{w},p^{w^{\prime}})\\
    \text{s.t. }rgt_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\ irp_{i}(a^{w},p^{w^{\prime}})=0,\forall
    i\in[n]\\ bcp_{i}(a^{w},p^{w^{\prime}})=0,\forall i\in[n]\\ \end{split}</annotation></semantics></math>
    |  | (13) |
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Golowich et al proposed RegretNet-nm [[11](#bib.bib11)] that is able to give
    general mechanisms that are not limited by existing characterization results for
    multi-facility location problems. The notion of regret is extended to facility
    location mechanisms as the maximum expected utility gain agents can achieve by
    misreporting their preferences.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: The network structure is the same as RegretNet, except for the inputs which
    are agents’ peaks. The misreported peaks are sampled uniformly within $[0,1]$,
    with a granularity of $\frac{1}{M}$. The ex post regret is integrated into the
    objective function using Augmented Lagrangian Solver, which uses a quadratic penalty
    term. The RegretNet-nm opens the door for mechanisms designs for settings without
    money, such as matching and allocation problems, using neural network approaches.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'PreferenceNet [[21](#bib.bib21)] is another extension of RegretNet. It encodes
    human preferences in auction designs. The network structure is comprised of RegretNet
    and a 3-layer MLP. These two components are trained in an EM-manner: MLP is first
    trained using a uniformly drawn sample of allocations as inputs, and it is optimized
    using binary cross entropy loss based on ground truth labels. Then, the RegretNet
    is trained using Augmented Lagrange Solver.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The loss function for the entire PreferenceNet is defined in Equation [14](#S3.E14
    "In 3 RegretNet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches"), where $\mathcal{L}_{s}$ is the output of the trained
    MLP. Lastly, the allocations and payments are sampled every $c$ epochs from the
    partially trained RegretNet and use them to augment the MLP training set to adapt
    to the distributional shifts in allocations during training.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i\in
    N}p_{i}^{w}(v^{(l)})+\mathcal{L}_{rgt}-\mathcal{L}_{s}\\ \text{Where }\mathcal{L}_{rgt}=\sum_{i\in
    N}\lambda_{(r,i)}\text{rgt}_{i}(w)+\frac{\rho_{r}}{2}(\sum_{i\in N}\text{rgt}_{i}(w))^{2},\mathcal{L}_{s}=\sum_{j\in
    M}s_{j}\end{split}$ |  | (14) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: Peri et al. proposed Preference Classification Accuracy (PCA) metric to evaluate
    how well a learned auction model satisfies an arbitrary constraint. PCA is calculated
    by the fraction of test bids that satisfy the ground truth constraint. Then, authors
    use pairwise comparisons between allocations to elicit preferences. Each input
    set of allocations is compared against n other allocations on their preference
    scores and label it as either a positive or negative exemplar, based on if its
    preference score is higher than the majority of others.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: ProportionNet [[14](#bib.bib14)] proposed by Kuo et al. is also based on the
    infrastructure of RegretNet. Like most other neural networks that deal with auction
    mechanism designs, it does not work under the setting of combinatorial valuations.
    Under the assumption of additive valuations and unit-demand valuations, the input
    space of valuations reduces from $2^{M}$ to $M$, where $M$ is the number of items.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{C}_{\rho}(w;\lambda)=-\frac{1}{L}\sum_{l=1}^{L}\sum_{i\in N}p_{i}^{w}(v^{(l)})+\mathcal{L}_{rgt}+\mathcal{L}_{unf}$
    |  | (15) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: 'Kuo et al. follows the core idea of RegretNet: in the Bayesian auction setting,
    one knows the valuation distribution from which samples can presumably be drawn.
    In the meantime, as both of the allocation and payment rules are functions, we
    can parametrize them using neural networks. Strategyproofness can be enforced
    adding constraints that are solvable using Augmented Lagrange Optimizer.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: It adopted same neural network architecture as RegretNet, but adding a constraint
    of unfairness in the loss function [15](#S3.E15 "In 3 RegretNet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"),
    so that discriminatory ad allocations among different demography can be mitigated.
    The regret term $\mathcal{L}_{rgt}$ is consistent with the definition in RegretNet.
    The term $\mathcal{L}_{unf}$ is for quantifying the unfairness and discrimination
    in the auction system, which will be described in more details in Section [6.4](#S6.SS4
    "6.4 Fairness & Fraud Prevention ‣ 6 Constraints and Concerns ‣ A Survey of Online
    Auction Mechanism Design Using Deep Learning Approaches").
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 4 Reinforcement Learning (RL) Infrastructure
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the online auction is more often a dynamic system whose users and platform
    objectives are evolving over time, researchers are more inclined to use dynamically
    trained models to adapt to the current status quo, leveraging reinforcement learning
    infrastructures [[6](#bib.bib6), [27](#bib.bib27), [31](#bib.bib31)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: As in the RegretNet infrastructure, Cai et al. also adopted no-regret learning,
    in which case agents only need to reason about their own strategies and their
    interaction with the environment, while they don’t have to know the values of
    competitors or compute payoff-maximizing strategies over a long sequence of rounds.
    Reasoning about the strategies of other parties usually require strong cognitive
    assumption and highly burdensome computing power, which most agents don’t have
    access to.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Based on well-known bandit algorithms, Cai et al. identified four possible strategies
    for sellers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '$\epsilon-$Greedy [[29](#bib.bib29)]: With probability $\epsilon$, each seller
    selects a strategy uniformly at random. With probability $1-\epsilon$, the strategy
    with the best observed empirical mean payoff is selected.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '$\epsilon-$First: For a horizon of $\mathcal{T}$ rounds, the seller picks a
    strategy uniformly at random for the first $\epsilon\cdot\mathcal{T}$ rounds,
    and then picks the strategy that maximizes the empirical mean of the observed
    rewards for all the subsequent rounds.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Exponential-weight Algorithm for Exploration and Exploitation (Exp3) [[2](#bib.bib2),
    [4](#bib.bib4)]: In short, Exp3 selects a price according to a weighted distribution
    and then adjust the weights based on payoffs. To be more precise, suppose there
    are $K+1$ possible prices, the probability distribution $\pi_{i}(t)$ of those
    prices at round $t$ is defined in Equation [16](#S4.E16 "In item 3 ‣ 4 Reinforcement
    Learning (RL) Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches"), where $w_{i}(t)$ are the current weight of $i^{th}$
    price at round $t$ and $\gamma$ is a real number between $[0,1]$.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\pi_{i}(t)=(1-\gamma)\frac{w_{i}(t)}{\sum_{j=1}^{K+1}w_{j}(t)}+\gamma\frac{1}{K+1}$
    |  | (16) |'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Select a price $p_{j}(t)$ according to the distribution above and compute its
    payoff $u_{j}(t)$, then the weight for $j^{th}$ price is updated according to
    Equation [17](#S4.E17 "In item 3 ‣ 4 Reinforcement Learning (RL) Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"),
    while the weights for all the other prices remain unchanged.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $w_{j}(t+1)=w_{j}(t)e^{\gamma\frac{u_{j}(t)}{(K+1)\pi_{j}(t)}}$ |  | (17)
    |'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '4.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upper confidence Bound Algorithm (UCB1) [[1](#bib.bib1), [3](#bib.bib3)]: In
    the first $K+1$ rounds, select a price not used before from $[0,\frac{1}{K},\dots,1]$
    and then select the price with the max weighted value in the subsequent rounds.'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Initialize the weights for all prices to be 0\. For any round $t\in\{0,\dots.K\}$,
    the seller chooses a price $p_{j}$ such that $p_{j}\in[0,\frac{1}{K},\dots,1]$,
    computes the utility $u_{j}(t)$, and updates the weights for $j^{th}$ price to
    be $x_{j}(t)=\frac{x_{j}(t-1)+u_{j}(t)}{t}$, and keeping the weights for all the
    other prices unchanged.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For any round $t\geq K+1$, the seller chooses the price $p_{j}$ according to
    Equation.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $p_{j}(t)=argmax_{j\in\{0,\frac{1}{K},\dots,1\}}x_{j}(t)+\frac{\log_{2}t}{\sum_{\tau=1}^{t}1_{\{p_{j}\text{
    was chosen in round }\tau\}}}$ |  | (18) |'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: $\epsilon-$First and $\epsilon-Greedy$ have a clear distinction between exploration
    and exploitation and belong to the class of semi-uniform strategies. Exp3 makes
    no distributional assumptions about the rewards and is widely used for the full
    information setting and works in the adversarial bandit feedback model [[4](#bib.bib4)].
    UCB1 maintains a certain level of optimism towards less frequently played actions
    and uses the empirical mean of observed actions to choose the action in the next
    round. UCB1 is best suited in scenarios where rewards follow some unknown distributions
    [[6](#bib.bib6)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: These sellers’ models can model sellers with different degrees of sophistication
    or pricing philosophies, and it is consistent with the recent literature on algorithmic
    mechanism deisgn, in terms of modeling agetn rationality in complex dynamic environments.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些卖家模型可以模拟具有不同复杂程度或定价理念的卖家，并且与近期关于算法机制设计的文献一致，在建模复杂动态环境中的代理理性方面。
- en: Previous researchers have already come up with a few variants of reinforcement
    learning models. The disadvantage of Deep Q-Network (DQN) [[18](#bib.bib18)] is
    that it cannot handle continuous actions or high-dimensional action spaces, as
    stochastic actor-critic algorithms are hard to converge. The Deterministic Policy
    Gradient (DPG) algorithm [[26](#bib.bib26)] is developed to train a deterministic
    policy with parameter vector. The DPG consists of the critic and actor. The critic
    approximates the action-value function, while the actor adjusts the parameters
    of the deterministic policy. Deep Deterministic Policy Gradient (DDPG) [[15](#bib.bib15)]
    is then proposed, as DPG is severely impacted by the high degree of temporal correlation
    that introduces high variance. DDPG stores the experiences of the agetn at each
    time step in a replay buffer and uniformly samples mini-batch from it at random
    for learning, which can eliminate the temporal correlation. DDPG also employs
    target networks for the regularization of the learning algorithm, which updates
    the parameters at a slower rate.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究人员已经提出了一些强化学习模型的变体。深度 Q 网络 (DQN) [[18](#bib.bib18)] 的缺点是它不能处理连续动作或高维动作空间，因为随机演员-评论家算法很难收敛。确定性策略梯度
    (DPG) 算法 [[26](#bib.bib26)] 是为了训练具有参数向量的确定性策略。DPG 由评论家和演员组成。评论家近似动作价值函数，而演员则调整确定性策略的参数。然后提出了深度确定性策略梯度
    (DDPG) [[15](#bib.bib15)]，因为 DPG 受到高时间相关性的严重影响，导致高方差。DDPG 在每个时间步将代理的经验存储在回放缓冲区中，并从中随机均匀抽取小批量进行学习，从而消除时间相关性。DDPG
    还采用目标网络来对学习算法进行正则化，以较慢的速度更新参数。
- en: However, the size of the action space blows up very sharply with the number
    of sellers increases, so an direct application of DDPG will fail to converge.
    In addition, the DDPG is not able to handle variability on the set of sellers,
    since the algorithm uses a two-layer fully connected network and the positions
    of each seller plays an important role [[6](#bib.bib6)].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着卖家数量的增加，动作空间的大小急剧膨胀，因此直接应用 DDPG 会导致不收敛。此外，DDPG 无法处理卖家集合上的变异性，因为该算法使用了一个两层的全连接网络，而每个卖家的位置发挥了重要作用
    [[6](#bib.bib6)]。
- en: Cai et al. proposed IA(GRU) [[6](#bib.bib6)] algorithm that aims to mitigate
    the problems from DDPG. It adopted the framework of DDPG by maintaining a sub-actor
    network and a sub-critic network. In each step of training, if utilizes a background
    network to perform a permutation transformations by ordering the sellers according
    to certain metrics, which maintains permutation invariance. In the meantime, it
    applies a recurrent neural network (RNN) on the history of sellers. The outputs
    from the permutation transformation and the outputs from the RNN on histories
    are then integrated together as inputs to the sub-actor and sub-critic networks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Cai 等人提出了 IA(GRU) [[6](#bib.bib6)] 算法，旨在缓解 DDPG 的问题。它采用了 DDPG 的框架，通过维护一个子演员网络和一个子评论家网络来实现。在每一步训练中，如果利用一个背景网络通过根据某些指标对卖家进行排列变换，从而保持排列不变性。同时，它在卖家的历史记录上应用了递归神经网络
    (RNN)。排列变换的输出和 RNN 在历史记录上的输出被整合在一起，作为子演员和子评论家网络的输入。
- en: In reality, participants of online auctions are constrained from both informational
    and computational aspects and therefore they are not fully rational. In addition,
    the historical data can be limited to the ones generated by mechanisms that are
    defined by only few sets of parameters, and therefore we do not have enough exploration
    for the past data. Both participants and auction system designers are impacted
    by multiple and complicated factors, and therefore their decisions are changing
    over time. To overcome those difficulties, Tang et al. models each player as an
    independent local Markov decision process [[27](#bib.bib27)], where a local state
    encodes the part of historical actions and outcomes that the player can observe
    so far.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Tang et al. uses DDPG infrastructure to handle continuous action space, but
    it decomposes the original neural netowrk into a set of sub-networks, one for
    each seller, to handle the huge number of states. It depends on the assumption
    that sellers are independent and the Q-values are additive among multiple sellers.
    It uses LSTM to adaptively learn from its past bidding data and feedback to predict
    future bid distribution, while it does not explicitly model each advertiser’s
    bidding strategy. In order to optimize the designer’s markov decision process,
    it discretizes the action space and then use the Monte-Carlo tree search (MCTS)
    [[23](#bib.bib23)] to speed up the forward-looking search. Experiments and case
    studies show that the dynamic pricing scheme proposed by Tang et al. outperforms
    all static schemes with large margins.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge in the dynamic online auction systems is the variety of performance
    metrics users and designers consider when making their decisions, while most state-of-the-art
    auction mechanisms only optimizes a single performance metrics, such as revenue
    or social welfare. Zhang et al. identified a list of performance metrics [[31](#bib.bib31)]
    that can be considered by users, advertisers, and the ad platform.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Revenue Per Mille (RPM): $RPM=\frac{\sum click\times PPC}{\sum impression}\times
    1000$, where PPC is the payment for winning ads.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click-Through Rate (CTR): $CTR=\frac{\sum click}{\sum impression}$.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add-to-Cart Rate (ACR): $ACR=\frac{\sum add-to-cart}{\sum impression}$.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Conversion Rate (CVR): $CVR=\frac{\sum order}{\sum impression}$.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GMV Per Mille (GPM): $GPM=\frac{\sum merchandisevolume}{\sum impression}\times
    1000$.'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Zhang et al. proposed the Deep GSP [[31](#bib.bib31)] that can optimize multiple
    performance metrics as in Equation [19](#S4.E19 "In 4 Reinforcement Learning (RL)
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches"), where $b$ is the bid vector from users, $\mathcal{M}$ is the auction
    mechanism, $f_{j}$ is the $j^{th}$ performance metrics function, and $w_{j}$ is
    the weights associated with $j^{th}$ performance metrics and can be adjusted by
    the auction platform administrators from time to time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}\mathcal{M}=argmax_{\mathcal{M}}E_{b\sim\mathcal{D}}[\sum_{j=1}^{L}w_{j}\times
    f_{j}(b;\mathcal{M})]\\ \text{s.t. Game Equilibrium constraints,}\\'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: \text{Smooth Transition constraints}\end{split}" display="block"><semantics
    ><mtable displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="right"  ><mrow
    ><mi >ℳ</mi><mo >=</mo><mrow ><mi  >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >x</mi><mi >ℳ</mi></msub><mo lspace="0em" rspace="0em" >​</mo><msub ><mi
    >E</mi><mrow ><mi >b</mi><mo >∼</mo><mi >𝒟</mi></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >[</mo><mrow ><munderover ><mo
    lspace="0em" movablelimits="false" >∑</mo><mrow ><mi >j</mi><mo >=</mo><mn >1</mn></mrow><mi
    >L</mi></munderover><mrow ><mrow ><msub ><mi  >w</mi><mi >j</mi></msub><mo lspace="0.222em"
    rspace="0.222em"  >×</mo><msub ><mi >f</mi><mi >j</mi></msub></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >b</mi><mo >;</mo><mi
    >ℳ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >]</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mtext >s.t. Game Equilibrium constraints,</mtext></mtd></mtr><mtr  ><mtd
    columnalign="right"  ><mtext >Smooth Transition constraints</mtext></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply  ><ci >ℳ</ci><apply ><ci  >𝑎</ci><ci >𝑟</ci><ci
    >𝑔</ci><ci  >𝑚</ci><ci >𝑎</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><ci  >ℳ</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐸</ci><apply ><csymbol cd="latexml"  >similar-to</csymbol><ci >𝑏</ci><ci >𝒟</ci></apply></apply><apply
    ><csymbol cd="latexml" >delimited-[]</csymbol><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >𝑗</ci><cn type="integer" >1</cn></apply></apply><ci >𝐿</ci></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑤</ci><ci >𝑗</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑓</ci><ci >𝑗</ci></apply></apply><list
    ><ci >𝑏</ci><ci >ℳ</ci></list></apply></apply></apply><ci ><mtext  >s.t. Game
    Equilibrium constraints,</mtext></ci><ci ><mtext  >Smooth Transition constraints</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{M}=argmax_{\mathcal{M}}E_{b\sim\mathcal{D}}[\sum_{j=1}^{L}w_{j}\times
    f_{j}(b;\mathcal{M})]\\ \text{s.t. Game Equilibrium constraints,}\\ \text{Smooth
    Transition constraints}\end{split}</annotation></semantics></math> |  | (19) |
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The Deep GSP auction is built upon the classical generalized second-price auction.
    It takes in the features of items (e.g. category, historical click-through rate),
    user profile (e.g. gender, age, income), and user preference (e.g. budget, marketing
    demands) as inputs to a deep neural network, which integrates those features with
    the bids into an input vector $x_{i}$ and map them to a rank score $r_{i}=R_{\theta}(b_{i};x_{i})$,
    where $R_{\theta}(b_{i};x_{i})$ is the mapping function. Bidders are then sorted
    based non-increasingly based on their rank scores, and the top-K bidders would
    win the auction. The payments of the winning bidders are based on the bids from
    the next highest bidders.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The mapping function $R_{\theta}(b_{i};x_{i})$ needs to be monotone with respect
    to the bids $b_{i}$, in order to satisfy the game equilibrium constraint. Some
    pieces of previous research enforced monotonicity by designing specific neural
    network architectures, but it increases the computational complexity for the training
    procedure. Therefore, Zhang et al. directly incorporate the monotonicity constraint
    by introducing a point-wise monotonicity penalty term (Equation [20](#S4.E20 "In
    4 Reinforcement Learning (RL) Infrastructure ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches")) into the loss function, where $\pi_{\theta}(b_{i};x_{i})$
    is a non-linear function with bid and is parametrized using a deep neural network.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{L}_{mono}=&amp;\sum_{i=1}^{N}\max(0,-\triangledown_{b}R_{\theta}(b_{i};x_{i}))\\
    =&amp;\sum_{i=1}^{N}\max(0,-(\pi_{\theta}(b_{i};x_{i})+b_{i}\triangledown_{b}\pi_{\theta}(b_{i};x_{i})))\end{split}$
    |  | (20) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: The smooth transition constraint is imposed in Equation [21](#S4.E21 "In 4 Reinforcement
    Learning (RL) Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches"), which ensures that the advertiser’s utility would
    not fluctuate too much when the auction mechanism is switched towards another
    objective.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $u_{i}(\mathcal{M})\geq(1-\epsilon)\times\bar{u}_{i}(\mathcal{M}_{0})$
    |  | (21) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: 5 DeepSet Infrastructure
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most deep neural network has an implicit constraint on the position of each
    input, while in reality the items in auction do not have an inherent ordering.
    Cai et al. uses permutation transformation to mitigate the position effect by
    ordering the sellers based on some metrics. Liu et al. took a step further by
    removing the ordering effect completely. They introduced Deep Neural Auction (DNA)
    [[16](#bib.bib16)] that is built on the DeepSets [[30](#bib.bib30)] architecture.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The set encoder for DNA is composed of two groups of layers $\phi_{1}$ and $\phi_{2}$.
    Each instance $x_{i}$ is first mapped to a high-dimensional latent space using
    the shared fully connected layers $\phi_{1}$, followed by the Exponential Linear
    Unit (ELU) [[7](#bib.bib7)] activation function $\sigma$. Then, it is processed
    with symmetric aggregation pooling (e.g. avgpool) to build the final set embedding
    $h_{i}^{\prime}$ for each ad $i$ with another fully connected layer $\phi_{2}$.
    The entire procedure is described in Equation [22](#S5.E22 "In 5 DeepSet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches"),
    where $h_{-i}$ represent the hidden states from all bidders except $i$.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}h_{i}=\sigma(\phi_{1}(x_{i}))\\ h_{i}^{\prime}=\sigma(\phi_{2}(\text{avgpool}(h_{-i})))\end{split}$
    |  | (22) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: DNA uses context-aware rank score uses a strictly monotone neural network with
    respect to bid, and supports efficient inverse transform given the next highest
    rank score. The rank score can be obtained using Equation [23](#S5.E23 "In 5 DeepSet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches") and the price can be obtained using Equation [24](#S5.E24 "In 5 DeepSet
    Infrastructure ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches"), where $w_{qz}$, $w_{qz}^{\prime}$, and $\alpha_{qz}$ are weights
    of the neural network.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $r_{i}=\min_{q\in[Q]}\max_{z\in[Z]}(e^{w_{qz}}\times b_{i}+w_{qz}^{\prime}\times
    x_{i}^{\prime}+\alpha_{qz})$ |  | (23) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '|  | $p_{i}=\max_{z\in[Z]}\min_{q\in[Q]}e^{-W_{qz}}(r_{i+1}-\alpha_{qz}-w_{qz}^{\prime}\times
    x_{i}^{\prime})$ |  | (24) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: This partially monotone MIN-MAX neural network represented by Equation [23](#S5.E23
    "In 5 DeepSet Infrastructure ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches") has been proved to be able to approximate any function
    [[8](#bib.bib8)].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{M}_{r}[k,:]=\text{softmax}(\frac{(N+1-2k)r-A_{r}\mathbbm{1}}{\tau})$
    |  | (25) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: The DNA also model the whole process of allocation and payment inside the neural
    network framework, as treating allocation and payment as an agnostic environment
    can limit the deep learning results. One of the challenges is that both the allocation
    and payment are built on a basic sorting operation, which is not differentiable.
    Liu et al. overcome this issue by proposing a differentiable sorting engine that
    caters to the top-K selection in the multi-slot auctions, leveraging Neural-Sort
    [[12](#bib.bib12)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: In Equation [25](#S5.E25 "In 5 DeepSet Infrastructure ‣ A Survey of Online Auction
    Mechanism Design Using Deep Learning Approaches"), the intuitive interpretation
    of $\hat{M}_{r}[k,:]$ is the choice probabilities on all elements for getting
    the $k^{th}$ highest item. Where $A_{r}[i,j]=|r_{i}-r_{j}|$, and $\mathbbm{1}$
    denotes the column vector of all ones. The top-K payments can therefore by recovered
    by a simple matrix multiplication in Equation [26](#S5.E26 "In 5 DeepSet Infrastructure
    ‣ A Survey of Online Auction Mechanism Design Using Deep Learning Approaches").
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{pay}=\hat{M}_{r}[1:K,:]\cdot[p_{1},p_{2},\dots,p_{N}]^{T}$ |  | (26)
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: 6 Constraints and Concerns
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Online auction system is complex gaming system among bidders, sellers, and
    auction system administrators. Merely considering the revenue or social welfare
    for one party is more often sub-optimal. To the best of our knowledge, we have
    identified four large categories of constraints and concerns when administrators
    are designing their auction systems: IC & IR, data sparcity & high dimensionality,
    multiple performance metrics and objectives, and fairness & fraud prevention.
    We will illustrate below why these constraints matter and then summarize how current
    researchers are tackling with them.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Incentive Compatibility (IC) & Individual Rationality (IR)
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Auction platform designers are more often interested in maximizing long-term
    objectives [[27](#bib.bib27)]. Therefore, building a reliable system that can
    adapt to the dynamic and complicated environment is crucial for the success.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: IC property ensures that all agents will achieve the best outcome by reporting
    their values truthfully. Auction participants may come from many different background
    and therefore informational, cognitive, and computational constraints will limit
    their rationality in different extent. The stability and reliability of the system
    will be much harder to maintain if IC cannot be satisfied, as designers and agents
    have to take all potential strategic behaviors of all other agents into account.
    Some researchers accomplished IC by adopting theoretical frameworks, such as GSP
    by DNA [[16](#bib.bib16)] and taxaction principal [[28](#bib.bib28)] by MenuNet
    [[24](#bib.bib24)]. Other researchers [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [14](#bib.bib14), [21](#bib.bib21)] achieved IC by enforcing ex post regret [[9](#bib.bib9)]
    to zero.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: IR property is also important as it ensures that all agents are receiving non-negative
    payoff. IR can be ensured by building upon the theoretical results from Myerson’s
    system [[20](#bib.bib20)]. It can also be enforced by integrating an additional
    constraint into the objective function and solve it using Augmented Lagrange Solver
    [[9](#bib.bib9), [10](#bib.bib10)]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Data Sparsity & High Dimensionality
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The action space can blow up very quickly as the number of agents increase.
    The high dimensionality in action space can introduce severe computational burdens.
    In order to mitigate the computational burden, Tang et al. and Cai et al. decomposed
    the neural network into sub-networks for each seller and discretized the action
    spaces [[27](#bib.bib27), [6](#bib.bib6)].
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: In addition, many features are high-dimensional one-hot vectors, so data can
    be very sparse. The original regularization approaches take the entire vector
    into computations and the regularized vector has non-zero entries for most of
    the positions, which increases the computational time for sparse data drastically.
    Zhou et al. proposed a mini-batch aware regularization [[32](#bib.bib32)] approach,
    where only parameters of features appearing in the mini-batch participate in the
    computation of regularization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{j}\leftarrow w_{j}-\eta[\frac{1}{&#124;\mathcal{B}_{m}&#124;}\sum_{(x,y)\in\mathcal{B}_{m}}\frac{\partial
    L(p(x),y)}{\partial w_{j}}+\lambda\frac{\max_{(x,y)\in\mathcal{B}_{m}}1_{x_{j}\neq
    0}}{n_{j}}w_{j}]$ |  | (27) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: The mini-batch aware regularization is shown in Equation [27](#S6.E27 "In 6.2
    Data Sparsity & High Dimensionality ‣ 6 Constraints and Concerns ‣ A Survey of
    Online Auction Mechanism Design Using Deep Learning Approaches"), where $\eta$
    is the learning rate, $\mathcal{B}_{m}$ is the $m^{th}$ batch, $w_{j}$ is the
    weight of $j^{th}$ feature, $n_{j}$ is the number of occurrences of $j^{th}$ feature.
    The numerator $\max_{(x,y)\in\mathcal{B}_{m}}1_{x_{j}\neq 0}$ denotes if at least
    one instance in the $m^{th}$ mini-batch has feature $j$.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Multiple Performance Metrics and Objectives
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most intuitive objective for most auction designers is to maximize their
    profit. However, to adapt to the dynamic and complex nature of today’s online
    auction systems, designers may be better-off if they consider multiple performance
    metrics. Zhang et al. listed out several commonly used performance metrics (See
    [4](#S4 "4 Reinforcement Learning (RL) Infrastructure ‣ A Survey of Online Auction
    Mechanism Design Using Deep Learning Approaches")). Zhang et al. and Liu et al.
    optimizes a linear combination of functions of those metrics, where both the linear
    weights and functions of those metrics and be specified by auction designers [[31](#bib.bib31),
    [16](#bib.bib16)]. Over the time, designers are free to adjust the weights and
    functions if their objectives have changed.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Fairness & Fraud Prevention
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the biases in the training data, many online platforms have discriminatory
    ad allocations among different demography [[14](#bib.bib14)]. One of the major
    social problems associated with online advertising is the use in the job market,
    where unfairness can be very detrimental to the equality and the protection of
    underrepresented groups.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate the unfairness, PreferenceNet [[21](#bib.bib21)] integrated three
    definitions of fairness into the model, all of which map the allocations $g(b)$
    onto $\mathbb{R}$. In all these equations (Equation [28](#S6.E28 "In item 1 ‣
    6.4 Fairness & Fraud Prevention ‣ 6 Constraints and Concerns ‣ A Survey of Online
    Auction Mechanism Design Using Deep Learning Approaches"), Equation [29](#S6.E29
    "In item 2 ‣ 6.4 Fairness & Fraud Prevention ‣ 6 Constraints and Concerns ‣ A
    Survey of Online Auction Mechanism Design Using Deep Learning Approaches"), and
    Equation [30](#S6.E30 "In item 3 ‣ 6.4 Fairness & Fraud Prevention ‣ 6 Constraints
    and Concerns ‣ A Survey of Online Auction Mechanism Design Using Deep Learning
    Approaches")), $i$ refers to $i^{th}$ item while $j$ and $j^{\prime}$ refer to
    $j^{th}$ and $j^{\prime th}$ agents.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Total Variation Fairness (Equation [28](#S6.E28 "In item 1 ‣ 6.4 Fairness &
    Fraud Prevention ‣ 6 Constraints and Concerns ‣ A Survey of Online Auction Mechanism
    Design Using Deep Learning Approaches")): the distance between allocations cannot
    be larger than the discrepancies between these two users.'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\sum_{i\in C_{k}}&#124;g(b)_{i,j}-g(b)_{i,j^{\prime}}&#124;\leq d^{k}(j,j^{\prime}),\forall
    k\in\{1,\dots,c\},\forall j,j^{\prime}\in M$ |  | (28) |'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '2.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Entropy (Equation [29](#S6.E29 "In item 2 ‣ 6.4 Fairness & Fraud Prevention
    ‣ 6 Constraints and Concerns ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches")): the allocation for an agent tends to be more uniformly
    distributed.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\max-\sum_{i=1}^{n}P(\frac{g(b)_{i.}}{\sum_{j}g(b)_{ij}})\log P(\frac{g(b)_{i.}}{\sum_{j}g(b)_{ij}})$
    |  | (29) |'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '3.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quota (Equation [30](#S6.E30 "In item 3 ‣ 6.4 Fairness & Fraud Prevention ‣
    6 Constraints and Concerns ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches")): the smallest allocation to any agent should be greater
    than some threshold.'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{j}(\frac{g(b)_{.j}}{\sum_{i}g(b)_{ij}})>t$ |  | (30) |'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: ProportionNet [[14](#bib.bib14)] also adopted the notion of total variation
    fairness, so that the allocations to similar users cannot differ by too much.
    It converted the Equation [28](#S6.E28 "In item 1 ‣ 6.4 Fairness & Fraud Prevention
    ‣ 6 Constraints and Concerns ‣ A Survey of Online Auction Mechanism Design Using
    Deep Learning Approaches") into an unfairness constraint (Equation [31](#S6.E31
    "In 6.4 Fairness & Fraud Prevention ‣ 6 Constraints and Concerns ‣ A Survey of
    Online Auction Mechanism Design Using Deep Learning Approaches")) that can be
    fed into the Augmented Lagrange Solver, which allows us to quantify the unfairness
    of the auction outcome for all users involved.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $unf_{j}=\sum_{j^{\prime}\in M}\sum_{C_{k}\in C}\max(0,\sum_{i\in C_{k}}\max(0,z_{ij}-z_{ij^{\prime}}))-d^{k}(j,j^{\prime}))$
    |  | (31) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: 'While unfairness can be introduced by the biases in auction mechanisms, it
    can also be induced by shill bidding behaviors in auctions. Sellers can adopt
    a variety of shill bidding strategies to inflate the final selling price of an
    item. The common four shill bidding strategies have been identified [[22](#bib.bib22)]:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluator: single bid engagement at an early time with a high amount.'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sniping: single bid engagement in the last moment, not leaving opportunity
    for anybody else to outbid.'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unmasking: multiple bid engagement in a short span of time with a probability
    of intend to exposing the maximum bid or the highest bidders.'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Skeptic: multiple bid engagement with lowest possible bids each time.'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7 Conclusion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we have gone through the rough evolving process of deep learning
    based online auction systems. Mechanisms designed using MLP infrastructure are
    usually built upon some theoretical results, and the MLP structure is used to
    represent the functions given in the theories. A more sophisticated structure
    came with the appearance of RegretNet, which parametrizes allocation rules and
    payment rules using separate networks. Many researchers have built extensions
    of RegretNet by integrating more constraints into the objective function or slightly
    adjusting the network structure but still keeping allocation and payment networks
    separate. The dynamic nature of online auction has encouraged researchers to adopt
    the deep reinforcement learning framework, which is more often a model-free approach
    that requires less assumptions on the data and is able to keep adapting itself
    as time progresses. As most traditional neural network has an implicit constraints
    on the positions of inputs, it integrates the ordering of auction participants
    into the model training, while in reality there is no inherent ordering among
    them. As a result, a deep learning based on DeepSet infrastructure has emerged,
    which can remove the effects of positions completely. We have also discussed the
    constraints and concerns faced by auction designers and we pointed out how researchers
    have attempted to address them.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Although researchers are progressing rapidly to the development of an online
    auction mechanism that can be reliable and profitable in the long term, there
    are still a lot of unresolved issues left for future researchers to investigate.
    As the size of users for online auction system is usually gigantic, the computational
    constraint and convergence problems for high-dimensional data are still non-negligible
    issues. Researchers are either mapping the data to lower dimension or reducing
    the action space by discretizing it, but it remains unclear how much information
    we are losing. In addition, most models assume the unit-demand and additive valuations,
    while this assumption might not be true in the real world. Last but not least,
    as most mechanism frameworks rely on the assumption that auction participants
    are independent from each other, their IC and IR constraints are also computed
    at individual levels. Therefore, their strategies might not be robust to the non-truthful
    behaviors conducted by participants in collusion.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Rajeev Agrawal. Sample mean based index policies by o(log n) regret for
    the multi-armed bandit problem. Advances in Applied Probability, 27(4):1054–1078,
    1995.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. Gambling in a rigged
    casino: The adversarial multi-armed bandit problem. In Proceedings of IEEE 36th
    Annual Foundations of Computer Science, pages 322–331, 1995.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time analysis
    of the multiarmed bandit problem. Machine Learning, 47(2–3):235–256, 2002.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The
    nonstochastic multiarmed bandit problem. SIAM J. Comput., 32(1):48–77, January
    2003.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Lawrence M. Ausubel and Paul Milgrom. The Lovely but Lonely Vickrey Auction.
    In Combinatorial Auctions, chapter 1, pages 17–40\. MIT Press, 2006.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, and Yiwei Zhang. Reinforcement
    mechanism design for e-commerce, 2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate
    deep network learning by exponential linear units (elus), 2016.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Hennie Daniels and Marina Velikova. Monotone and partially monotone neural
    networks. IEEE Transactions on Neural Networks, 21(6):906–917, 2010.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Paul Dütting, Zhe Feng, Harikrishna Narasimhan, David C. Parkes, and Sai Srivatsa
    Ravindranath. Optimal auctions through deep learning, 2020.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Zhe Feng, Harikrishna Narasimhan, and David C. Parkes. Deep learning for
    revenue-optimal auctions with budgets. In Proceedings of the 17th International
    Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, page 354–362,
    Richland, SC, 2018\. International Foundation for Autonomous Agents and Multiagent
    Systems.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Noah Golowich, Harikrishna Narasimhan, and David C. Parkes. Deep learning
    for multi-facility location mechanism design. In Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence, IJCAI-18, pages 261–267.
    International Joint Conferences on Artificial Intelligence Organization, 7 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization
    of sorting networks via continuous relaxations, 2019.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into
    rectifiers: Surpassing human-level performance on imagenet classification. In
    2015 IEEE International Conference on Computer Vision (ICCV), pages 1026–1034,
    2015.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Kevin Kuo, Anthony Ostuni, Elizabeth Horishny, Michael J. Curry, Samuel
    Dooley, Ping yeh Chiang, Tom Goldstein, and John P. Dickerson. Proportionnet:
    Balancing fairness and revenue for auction design with deep learning, 2020.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Xiangyu Liu, Chuan Yu, Zhilin Zhang, Zhenzhe Zheng, Yu Rong, Hongtao Lv,
    Da Huo, Yiqing Wang, Dagui Chen, Jian Xu, Fan Wu, Guihai Chen, and Xiaoqiang Zhu.
    Neural auction: End-to-end learning of auction mechanisms for e-commerce advertising,
    2021.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Nguyen Cong Luong, Zehui Xiong, Ping Wang, and Dusit Niyato. Optimal auction
    for edge computing resource management in mobile blockchain networks: A deep learning
    approach, 2017.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel
    Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland,
    Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,
    Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level
    control through deep reinforcement learning. Nature, 518(7540):529–533, February
    2015.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] H. Moulin. On strategy-proofness and single peakedness. Public Choice,
    35(4):437–455, 1980.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Roger B. Myerson. Optimal auction design. Mathematics of Operations Research,
    6(1):58–73, 1981.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Neehar Peri, Michael J. Curry, Samuel Dooley, and John P. Dickerson. Preferencenet:
    Encoding human preferences in auction design with deep learning, 2021.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Saurabh R. Sangwan and Anshika Arora. Supervised machine learning based
    buyer’s bidding behaviour detection in online auction. Social Science Research
    Network, 2019.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Weiran Shen, Binghui Peng, Hanpeng Liu, Michael Zhang, Ruohan Qian, Yan
    Hong, Zhi Guo, Zongyao Ding, Pengjun Lu, and Pingzhong Tang. Reinforcement mechanism
    design, with applications to dynamic pricing in sponsored search auctions, 2017.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Weiran Shen, Pingzhong Tang, and Song Zuo. Automated mechanism design
    via neural networks, 2021.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] MyungJae Shin, Joongheon Kim, and Marco Levorato. Auction-based charging
    scheduling with deep learning framework for multi-drone networks. IEEE Transactions
    on Vehicular Technology, 68(5):4235–4248, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra,
    and Martin Riedmiller. Deterministic policy gradient algorithms. In Proceedings
    of the 31st International Conference on International Conference on Machine Learning
    - Volume 32, ICML’14, page I–387–I–395\. JMLR.org, 2014.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Pingzhong Tang. Reinforcement mechanism design. In Proceedings of the
    Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17,
    pages 5146–5150, 2017.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Rakesh Vohra. Mechanism design. a linear programming approach. Mechanism
    Design: A Linear Programming Approach, 01 2010.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards.
    PhD thesis, King’s College, Cambridge, UK, May 1989.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabás Póczos, Ruslan
    Salakhutdinov, and Alexander J. Smola. Deep sets. CoRR, abs/1703.06114, 2017.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Zhilin Zhang, Xiangyu Liu, Zhenzhe Zheng, Chenrui Zhang, Miao Xu, Junwei
    Pan, Chuan Yu, Fan Wu, Jian Xu, and Kun Gai. Optimizing multiple performance metrics
    with deep gsp auctions for e-commerce advertising, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma,
    Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through
    rate prediction, 2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
