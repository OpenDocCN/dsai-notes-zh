- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 20:01:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 20:01:20'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2005.00355] Survey on Reliable Deep Learning-Based Person Re-Identification
    Models: Are We There Yet?'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2005.00355] 关于可靠深度学习基础的人体再识别模型的调查：我们已经达到目标了吗？'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2005.00355](https://ar5iv.labs.arxiv.org/html/2005.00355)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2005.00355](https://ar5iv.labs.arxiv.org/html/2005.00355)
- en: 'Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are
    We There Yet?'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于可靠深度学习基础的人体再识别模型的调查：我们已经达到目标了吗？
- en: 'Bahram Lavi Corresponding author: bahram.lavi@ic.unicamp.br Institute of Computing,
    University of Campinas (UNICAMP), Campinas, São Paulo, Brazil. Ihsan Ullah Data
    Mining & Machine Learning Group, Discipline of IT, National University of Ireland
    Galway, Ireland. Mehdi Fatan Department of Computer Engineering and Mathematics,
    University Rovira i Virgili, Tarragona, Spain Anderson Rocha Institute of Computing,
    University of Campinas (UNICAMP), Campinas, São Paulo, Brazil.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Bahram Lavi 通讯作者: bahram.lavi@ic.unicamp.br 巴西圣保罗坎皮纳斯大学计算机学院（UNICAMP）。Ihsan
    Ullah 数据挖掘与机器学习组，爱尔兰国立大学戈尔韦分校。Mehdi Fatan 计算机工程与数学系，西班牙罗维拉·伊·维尔吉里大学，塔拉戈纳。Anderson
    Rocha 巴西圣保罗坎皮纳斯大学计算机学院（UNICAMP）。'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: '*Intelligent video-surveillance* (IVS) is currently an active research field
    in computer vision and machine learning and provides useful tools for surveillance
    operators and forensic video investigators. Person re-identification (PReID) is
    one of the most critical problems in IVS, and it consists of recognizing whether
    or not an individual has already been observed over a camera in a network. Solutions
    to PReID have myriad applications including retrieval of video-sequences showing
    an individual of interest or even pedestrian tracking over multiple camera views.
    Different techniques have been proposed to increase the performance of PReID in
    the literature, and more recently researchers utilized deep neural networks (DNNs)
    given their compelling performance on similar vision problems and fast execution
    at test time. Given the importance and wide range of applications of re-identification
    solutions, our objective herein is to discuss the work carried out in the area
    and come up with a survey of state-of-the-art DNN models being used for this task.
    We present descriptions of each model along with their evaluation on a set of
    benchmark datasets. Finally, we show a detailed comparison among these models,
    which are followed by some discussions on their limitations that can work as guidelines
    for future research.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*智能视频监控*（IVS）目前是计算机视觉和机器学习的一个活跃研究领域，为监控操作员和法医视频调查员提供了有用的工具。人体再识别（PReID）是IVS中的一个重要问题，涉及识别某个个体是否已经在网络中的摄像头上被观察过。PReID的解决方案有众多应用，包括检索显示目标个体的视频序列，甚至多摄像头视角下的行人跟踪。文献中提出了各种技术来提高PReID的性能，最近研究人员利用了深度神经网络（DNNs），因其在类似视觉问题上的出色表现和测试时的快速执行。鉴于再识别解决方案的重要性和广泛应用，我们的目标是讨论该领域的工作，并提出一个关于当前最先进DNN模型的调查。我们介绍了每个模型的描述，并在一组基准数据集上对其进行评估。最后，我们展示了这些模型的详细比较，并讨论了它们的局限性，这些讨论可以作为未来研究的指南。'
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The importance of security and safety of people in society at large is continuously
    growing. Governmental and private organizations are seriously concerned with the
    security of public areas such as airports and shopping malls. It requires significant
    effort and financial expense to provide security to the public. To optimize such
    efforts, video surveillance systems are playing a pivotal role. Nowadays, a panoply
    of video cameras is growing as a useful tool for addressing various kinds of security
    issues such as forensic investigations, crime prevention, and safeguarding restricted
    areas.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 社会上对人们安全和保障的重要性不断增长。政府和私人组织对公共区域如机场和购物中心的安全非常关注。为公众提供安全保障需要大量的努力和财务支出。为了优化这些努力，视频监控系统发挥了关键作用。如今，视频摄像机的使用日益广泛，成为解决各种安全问题的有用工具，例如法医调查、犯罪预防以及保护受限区域。
- en: Daily continuous recording of videos from network cameras results in daunting
    amounts of videos for analysis in a manual video surveillance system. Surveillance
    operators need to analyze them at the same time for specific incidents or anomalies,
    which is a challenging and tiresome task. Intelligent video surveillance systems
    (IVSS) aim to automate the issue of monitoring and analyzing videos from camera
    networks to help surveillance operators in handling and understanding the acquired
    videos. This makes the IVSS area one of the most active and challenging research
    areas in computer engineering and computer science for which computer vision (CV)
    and machine-learning (ML) techniques plays a key role. This field of research
    enables various tools such as  online applications for people/object detection
    and tracking, recognizing suspicious action/behavior from the camera network;
    andoff-line applications to support operators and forensic investigators to retrieve
    images of the individual of interest from video frames acquired on different camera
    views.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络摄像头中每日连续录制视频会导致需要分析大量视频，这在人工视频监控系统中是令人望而生畏的任务。监控操作员需要同时分析这些视频以检测特定事件或异常，这是一项具有挑战性且耗时的任务。智能视频监控系统（IVSS）旨在自动化监控和分析来自摄像头网络的视频，以帮助监控操作员处理和理解获取的视频。这使得IVSS领域成为计算机工程和计算机科学中最活跃和具有挑战性的研究领域之一，其中计算机视觉（CV）和机器学习（ML）技术发挥了关键作用。该研究领域支持各种工具，例如用于人/物检测和跟踪的在线应用程序、从摄像头网络中识别可疑行为的工具，以及支持操作员和法医调查员从不同摄像头视图中获取图像的离线应用程序。
- en: Person identification is one of the problems of interest in IVSS. It consists
    of recognizing an individual over a network of video surveillance cameras with
    possibly non-overlapping fields of view [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)].
    In general, the application of PReID is to support surveillance operators and
    forensic investigators in retrieving videos showing an individual of interest,
    given an image as a query (a.k.a. probe). Therefore, video frames or tracks of
    all the individuals (a.k.a. template gallery) recorded by the camera network are
    sorted in descending order of similarity to the probe. It allows the user to find
    occurrences (if any) of the individual of interest in the top positions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人物识别是IVSS中的一个研究重点。它包括在可能视角不重叠的视频监控摄像头网络中识别个体[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)]。通常，PReID的应用是支持监控操作员和法医调查员从给定的图像（即探针）中检索显示特定个体的视频。因此，所有个体的视频帧或轨迹（即模板库）根据与探针的相似度降序排序。这使用户能够在排名靠前的位置找到特定个体的出现（如果有的话）。
- en: Person re-identification is a challenging task due to low-image resolution,
    unconstrained pose, illumination changes, and occlusions, which adhere to the
    use of robust biometric features like face, among others. Whereas, some cues like
    gait and anthropometric measures have been used in some existing PReID systems.
    Most of the existing techniques rely on defining a specific descriptor of clothing
    (typically including color and texture), and a specific similarity measure between
    a pair of descriptors (evaluated as a matching score) which can be either manually
    defined or learned directly from data [[1](#bib.bib1), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人物再识别是一项具有挑战性的任务，原因包括图像分辨率低、姿态不受约束、光照变化以及遮挡，这些因素都依赖于使用诸如面部等稳健的生物特征。尽管如此，像步态和人体测量这些线索也在一些现有的PReID系统中被使用。大多数现有技术依赖于定义衣物的特定描述符（通常包括颜色和纹理），以及描述符对之间的特定相似度度量（评估为匹配分数），这些度量可以是手动定义的，也可以直接从数据中学习得到[[1](#bib.bib1),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]。
- en: 'Standard PReID Methodology: For a given image of an individual (a.k.a. probe),
    a PReID system aims to seek the corresponding images of that person within the
    gallery of templates. Take into consideration that creating the template gallery
    depends on a re-identification setup, which we can categorize as: (i) single-shot
    which has only one template frame per individual, and (ii) multiple-shots that
    contains more than one template frame per individual. In this case, a continuous
    PReID system is employed in real-time whereby the individual of interest is continuously
    matched against the template image with the gallery set, using the currently seen
    frame as a probe. Figure.[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Survey on Reliable
    Deep Learning-Based Person Re-Identification Models: Are We There Yet?") demonstrates
    a basic PReID framework. After an image description is generated for a probe,
    and the template images of the gallery set, matching scores between each of them
    are computed; and finally, the ranked list is generated by sorting the matching
    scores in decreasing order.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '标准PReID方法论：对于给定的个人图像（即**探测图像**），PReID系统旨在从模板库中寻找该人的对应图像。需要考虑的是，创建模板库依赖于重新识别设置，我们可以将其分类为：(i)
    单帧，即每个人只有一个模板帧，以及(ii) 多帧，即每个人有多个模板帧。在这种情况下，使用连续PReID系统进行实时匹配，其中，目标个体会不断与模板图像进行匹配，使用当前看到的帧作为探测图像。图。[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Survey on Reliable Deep Learning-Based Person Re-Identification
    Models: Are We There Yet?")展示了一个基本的PReID框架。在生成探测图像的描述后，将计算模板库中每个图像之间的匹配分数；最后，通过对匹配分数进行降序排序，生成排名列表。'
- en: '![Refer to caption](img/a606510551c930f937ae49841c1f35f5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a606510551c930f937ae49841c1f35f5.png)'
- en: 'Figure 1: Standard person re-identification system. Given a probe image and
    set of template images, the goal is to generate a robust image signature from
    each and compute the similarity between them, and finally presented by a sorted,
    ranked list.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：标准人员重新识别系统。给定探测图像和一组模板图像，目标是从每个图像生成一个稳健的图像签名，并计算它们之间的相似度，最后通过排序的排名列表呈现。
- en: 'The strategy of many existing descriptors use hand-crafted features. Deep-learning
    (DL) models – e.g.,convolutional neural networks (CNNs) [[8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10)]– have been particularly used to solve the problem of PReID by
    learning from data. A CNN-based model generates a set of feature maps, whereby
    each pixel of a given image corresponds to specific feature representation. The
    desired output is expected at the top of the employed model. There are different
    approaches to train a deep neural network (DNN) model. A DNN model can be trained
    in a Supervised, Semi- and Un-Supervised manner depending on the problem scenarios
    and availability of labelled data. In the task of PReID only a small set of training
    data is available. Thus, developing a learning model in semi- and un-supervised
    manner is usually challenging task and the model might result in failure or poor
    performance in PReID. Most of the papers discussed at the end of this paper engage
    with supervised learning techniques, and only a few of them considered semi- or
    un-supervised approach. Further, we consider the models used for PReID in three
    categories as {single, pairwise, and triplet} feature-learning strategies. Details
    are presented and discussed in section [3](#S3 "3 Deep Neural Networks for PReID
    ‣ Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are
    We There Yet?"). This paper presents the state-of-the-art methods of PReID techniques
    based on DNNs and provides a significant detailed information about them. The
    literature review involves papers which were published between the year 2014 to
    date. We provide a taxonomy of deep feature learning methods for PReID including
    comparisons, limitations, and future research directions and potential as well
    as opportunities for research in the horizon. Unlike [[11](#bib.bib11), [12](#bib.bib12)],
    we provide a comprehensive and detailed review of the existing techniques, particularly,
    the more modern ones that rely upon DNN feature learning strategies. We stress
    that, in this paper, we only consider recent DNN techniques which directly involved
    on the procedure of PReID task. For each technique, we analyze its experimental
    result and further make comparisons of the achieved performances considering different
    perspectives such as comparing DNNs performances when adopting different strategies
    to solve a problem such as the learning strategy (e.g., single, pairwise, and
    triplet learnings).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '许多现有描述符的策略使用手工制作的特征。深度学习（DL）模型——例如卷积神经网络（CNNs）[[8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10)]——特别用于通过从数据中学习来解决PReID问题。基于CNN的模型生成一组特征图，其中每个图像的像素对应于特定的特征表示。期望的输出应位于所用模型的顶部。训练深度神经网络（DNN）模型有不同的方法。DNN模型可以以有监督、半监督和无监督的方式进行训练，这取决于问题场景和标记数据的可用性。在PReID任务中，仅有一小部分训练数据可用。因此，通常在半监督和无监督的方式下开发学习模型是具有挑战性的，模型可能会导致失败或PReID性能较差。本文讨论的大多数论文涉及有监督学习技术，只有少数考虑了半监督或无监督方法。此外，我们将用于PReID的模型分为{单一、对、三重}特征学习策略三类。详细信息在第[3](#S3
    "3 Deep Neural Networks for PReID ‣ Survey on Reliable Deep Learning-Based Person
    Re-Identification Models: Are We There Yet?")节中介绍和讨论。本文介绍了基于DNN的PReID技术的最先进方法，并提供了有关这些方法的详细信息。文献综述包括2014年至今发表的论文。我们提供了PReID深度特征学习方法的分类，包括比较、局限性、未来研究方向和潜力以及研究机会。与[[11](#bib.bib11),
    [12](#bib.bib12)]不同，我们提供了对现有技术的全面和详细的评审，特别是依赖于DNN特征学习策略的更现代的方法。我们强调，在本文中，我们只考虑直接涉及PReID任务程序的最新DNN技术。对于每种技术，我们分析其实验结果，并进一步从不同的角度比较所取得的性能，例如比较采用不同策略解决问题时DNN的表现（例如，单一、对和三重学习）。'
- en: 'The structure of this paper is organized as follows: Section [2](#S2 "2 Person
    Re-identification Benchmark Data sets ‣ Survey on Reliable Deep Learning-Based
    Person Re-Identification Models: Are We There Yet?") briefly explain the benchmark
    datasets employed for PreID. Section[3](#S3 "3 Deep Neural Networks for PReID
    ‣ Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are
    We There Yet?") describes DNN methods by highlighting the impact of their important
    content such as objective function, loss functions, data augmentation, among others.
    Whereas, section [4](#S4 "4 Results and Open Issues ‣ Survey on Reliable Deep
    Learning-Based Person Re-Identification Models: Are We There Yet?") discusses
    performance measures, results and their comparisons, and their limitations and
    future directions. Finally, section[5](#S5 "5 Final Remarks ‣ Survey on Reliable
    Deep Learning-Based Person Re-Identification Models: Are We There Yet?") concludes
    the paper and gives final remarks about the PReID and the paper.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的结构组织如下：第[2](#S2 "2 Person Re-identification Benchmark Data sets ‣ Survey
    on Reliable Deep Learning-Based Person Re-Identification Models: Are We There
    Yet?")节简要说明了用于PreID的基准数据集。第[3](#S3 "3 Deep Neural Networks for PReID ‣ Survey
    on Reliable Deep Learning-Based Person Re-Identification Models: Are We There
    Yet?")节描述了深度神经网络（DNN）方法，突出了其重要内容，如目标函数、损失函数、数据增强等的影响。而第[4](#S4 "4 Results and
    Open Issues ‣ Survey on Reliable Deep Learning-Based Person Re-Identification
    Models: Are We There Yet?")节讨论了性能测量、结果及其比较，以及局限性和未来方向。最后，第[5](#S5 "5 Final Remarks
    ‣ Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are
    We There Yet?")节总结了本文并对PReID和本文作出最终的评论。'
- en: 2 Person Re-identification Benchmark Data sets
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 人员再识别基准数据集
- en: Data is one of the important factors for current DNN models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是当前深度神经网络（DNN）模型的重要因素之一。
- en: 'Some factors must be taken into account to reach a reliable recognition rate
    when evaluating person re-identification solutions. Each dataset is collected
    to specially target one or more of these factors. The factors that create issues
    for PReID task includes occlusion (apparent in i-LIDS dataset) and illumination
    variation (common in most of them). On the other hand, background and foreground
    segmentation to distinguish the person’s body is a challenging task. Some of the
    datasets provide the segmented region of a person’s body (e.g., on VIPeR, ETHZ,
    and CAVIAR datasets). While other datasets have been prepared to evaluate the
    re-identification task. The most widely datasets are VIPeR, CUHK01, and CUHK03.
    VIPeR, CAVIAR, and PRID datasets are used when only two fixed camera views are
    given to evaluate the performance of person re-identification methods. Table. [1](#S2.T1
    "Table 1 ‣ 2 Person Re-identification Benchmark Data sets ‣ Survey on Reliable
    Deep Learning-Based Person Re-Identification Models: Are We There Yet?") gives
    a summary of each dataset. Below we briefly discuss each of them.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '在评估人员再识别解决方案时，必须考虑一些因素以达到可靠的识别率。每个数据集都专门针对一个或多个这些因素进行收集。对于PReID任务造成问题的因素包括遮挡（在i-LIDS数据集中明显存在）和光照变化（在大多数数据集中很常见）。另一方面，背景和前景分割以区分人的身体是一个具有挑战性的任务。一些数据集提供了一个人的身体分割区域（例如，在VIPeR、ETHZ和CAVIAR数据集中）。而其他数据集则为评估再识别任务而准备。最广泛使用的数据集是VIPeR、CUHK01和CUHK03。当仅给出两个固定摄像头视角时，VIPeR、CAVIAR和PRID数据集被用来评估人员再识别方法的性能。[表1](#S2.T1
    "Table 1 ‣ 2 Person Re-identification Benchmark Data sets ‣ Survey on Reliable
    Deep Learning-Based Person Re-Identification Models: Are We There Yet?")提供了每个数据集的摘要。下面我们简要讨论每一个数据集。'
- en: 'VIPeR [[4](#bib.bib4)]: VIPeR is a challenging dataset due to its small number
    of images for each individual. It is made up of two images of 632 individuals
    from two camera views. It consists of pose and illumination variations. The images
    are cropped and scaled to $128\times 48$ pixels. This is one of the most widely
    used datasets for PReID and a good starting point for new researchers in PReID.
    Enhancing Rank-1 performance on this dataset is still an open challenge.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'VIPeR [[4](#bib.bib4)]: VIPeR是一个具有挑战性的数据集，因为每个个体的图像数量很少。它由来自两个摄像头视角的632名个体的两张图像组成。包含姿态和光照变化。图像被裁剪并缩放为$128\times
    48$像素。这是PReID中最广泛使用的数据集之一，也是新研究人员进入PReID领域的良好起点。提高该数据集上的Rank-1性能仍然是一个开放的挑战。'
- en: 'i-LIDS [[13](#bib.bib13)]: It contains 476 images of 119 pedestrians taken
    at an airport hall from non-overlapping cameras with pose and lighting variations
    and strong occlusions. A minimum of two images and an average of four images exist
    for each pedestrian.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 'i-LIDS [[13](#bib.bib13)]: 包含476张来自机场大厅的119名行人的图像，这些图像由不同视角的摄像头拍摄，具有姿态和光照变化以及强遮挡。每个行人至少有两张图像，平均有四张图像。'
- en: 'ETHZ [[14](#bib.bib14)]: It contains three video sequences of a crowded street
    from two moving cameras; images exhibit considerable illumination changes, scale
    variations, and occlusions. The images are of different sizes. This dataset provides
    three sequences of multiple images of an individual from each sequence. Sequences
    1, 2, and 3 have 83, 35, and 28 pedestrians, respectively.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'ETHZ [[14](#bib.bib14)]: 该数据集包含来自两台移动摄像机的三个拥挤街道视频序列；图像展示了显著的光照变化、尺度变化和遮挡。图像大小各异。该数据集提供了来自每个序列的多个个体图像的三个序列。序列1、2和3分别有83、35和28名行人。'
- en: 'CAVIAR [[15](#bib.bib15)]: It contains 72 persons and two views in which 50
    persons appear in both views while 22 persons appear only in one view. Each person
    has five images per view, with different appearance variations due to resolution
    changes, light conditions, occlusions, and different poses.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'CAVIAR [[15](#bib.bib15)]: 该数据集包含72人和两个视角，其中50人同时出现在两个视角中，而22人只出现在一个视角中。每个人在每个视角下有五张图像，由于分辨率变化、光照条件、遮挡和不同姿态而产生了不同的外观变化。'
- en: 'CUHK: This dataset is divided into three distinct partitions with specific
    setups. *CUHK01* [[16](#bib.bib16)] includes $1,942$ images of $971$ pedestrians.
    It consists of two images captured in two disjoint camera views, camera (A) with
    several variations of viewpoints and pose, and camera (B) mainly include images
    of the frontal and back view of the camera.*CUHK02* [[17](#bib.bib17)] contains
    $1,816$ individuals constructed by five pairs of camera views (P1-P5 with ten
    camera views). Each pair includes 971, 306, 107, 193, and 239 individuals, respectively.
    Each individual has two images in each camera view. This dataset is employed to
    evaluate the performance when the camera views in the test are different from
    those in training. Finally, *CUHK03*  [[18](#bib.bib18)] includes $13,164$ images
    of $1,360$ pedestrians. This data set has been captured with six surveillance
    cameras. Each identity is observed by two disjoint camera views and has an average
    of $4.8$ images in each view; all manually cropped pedestrian images exhibit illumination
    changes, misalignment, occlusions, and body parts missing.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'CUHK: 该数据集分为三个不同的部分，每部分都有特定的设置。*CUHK01* [[16](#bib.bib16)] 包括$1,942$张图像，涵盖$971$名行人。它由在两个不重叠的摄像机视角下捕捉的两张图像组成，摄像机（A）具有多个视角和姿态变化，而摄像机（B）主要包括摄像机的正面和背面视图。*CUHK02*
    [[17](#bib.bib17)] 包含$1,816$名个体，由五对摄像机视角（P1-P5，共十个摄像机视角）构成。每对视角包括971、306、107、193和239名个体，每个个体在每个摄像机视角下有两张图像。该数据集用于评估当测试中的摄像机视角与训练中的不同视角时的性能。最后，*CUHK03*
    [[18](#bib.bib18)] 包括$13,164$张图像，涵盖$1,360$名行人。该数据集由六台监控摄像机捕捉。每个身份由两个不重叠的摄像机视角观察，每个视角平均有$4.8$张图像；所有手动裁剪的行人图像展示了光照变化、错位、遮挡和身体部位缺失。'
- en: 'PRID  [[19](#bib.bib19)]: This dataset is specially designed for PReID, focusing
    on a single-shot scenario. It contains two image sets containing 385 and 749 persons
    captured by camera A and camera B, respectively. The two subsets of this dataset
    share 200 persons in common.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'PRID [[19](#bib.bib19)]: 该数据集专门为PReID设计，专注于单次拍摄场景。它包含两个图像集，分别由摄像机A和摄像机B捕捉到385人和749人。这两个子集共享200个相同的人物。'
- en: 'WARD [[20](#bib.bib20)]: This dataset has 4,786 images of 70 persons acquired
    in a real-surveillance scenario with three non-overlapping cameras having huge
    illumination variation, resolution, and pose changes.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 'WARD [[20](#bib.bib20)]: 该数据集包含4,786张图像，涵盖70个人物，这些图像是在一个真实监控场景中获取的，使用了三台非重叠的摄像机，具有巨大的光照变化、分辨率变化和姿态变化。'
- en: 'Re-identification Across indoor-outdoor Dataset (RAiD) [[21](#bib.bib21)]:
    It comprise of 6,920 bounding boxes of 43 identities captured by four cameras.
    The cameras are categorized into four partitions in which the first two cameras
    are indoors while the remaining two are outdoors. Images show considerable illumination
    variations because of indoor and outdoor changes.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 'Re-identification Across indoor-outdoor Dataset (RAiD) [[21](#bib.bib21)]:
    该数据集包含6,920个边界框，涵盖43个身份，由四台摄像机捕捉。摄像机分为四个部分，其中前两台摄像机在室内，剩下的两台在室外。图像显示了由于室内和室外变化而产生的显著光照变化。'
- en: 'Market-1501 [[22](#bib.bib22)]: A total of six cameras are used, including
    5 high-resolution cameras, and one low-resolution camera. Overlap exists among
    different cameras. Overall, this dataset contains 32,668 annotated bounding boxes
    of 1,501 identities. Among them, 12,936 images from 751 identities are used for
    training, and 19,732 images from 750 identities plus distractors are used for
    gallery set.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'Market-1501 [[22](#bib.bib22)]: 总共使用了六台摄像机，包括5台高分辨率摄像机和1台低分辨率摄像机。不同摄像机之间存在重叠。总体而言，该数据集包含32,668个标注的边界框，涉及1,501个身份。其中，来自751个身份的12,936张图像用于训练，来自750个身份加干扰者的19,732张图像用于图库集。'
- en: 'MARS [[23](#bib.bib23)]: This dataset comprises 1,261 identities with each
    identity captured by at least two cameras. It consists of 20,478 tracklets and
    1,191,003 bounding boxes.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'MARS [[23](#bib.bib23)]: 该数据集包含1,261个身份，每个身份至少由两台摄像机捕捉。数据集包含20,478个轨迹片段和1,191,003个边界框。'
- en: 'DukeMTMC [[24](#bib.bib24)]: This dataset contains 36,441 manually-cropped
    images of 1,812 persons captured by eight outdoor cameras. The data set gives
    access to some additional information such as full frames, frame-level ground-truth,
    and calibration details.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'DukeMTMC [[24](#bib.bib24)]: 该数据集包含36,441张手动裁剪的图像，涉及1,812个人物，由八台室外摄像机捕捉。数据集还提供了完整帧、帧级真实数据和校准细节等附加信息。'
- en: 'MSMT [[25](#bib.bib25)]: It consists of 126,441 images of 4,101 individuals
    acquired from 12 indoor and three outdoor cameras, with different illumination
    changes, poses, and scale variations.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'MSMT [[25](#bib.bib25)]: 该数据集包含126,441张图像，涉及4,101个个体，图像来自12台室内摄像机和3台室外摄像机，具有不同的光照变化、姿势和尺度变化。'
- en: 'RPIfield [[26](#bib.bib26)]: This dataset is constructed using 12 synchronized
    cameras provided by 112 explicitly time-stamped actor pedestrians through out
    specific paths among about 4000 distractor pedestrians.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'RPIfield [[26](#bib.bib26)]: 该数据集使用了12台同步摄像机，由112位明确时间戳的行人演员在大约4000名干扰行人中沿特定路径提供。'
- en: 'Indoor Train Station Dataset (ITSD) [[27](#bib.bib27)]: This dataset has the
    images of people from a real-world surveillance camera captured at a railway station.
    It presents the image size of $64\times 128$ pixels and contains 5607 images,
    443 identities, with different viewpoints.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'Indoor Train Station Dataset (ITSD) [[27](#bib.bib27)]: 该数据集包含从铁路车站的真实监控摄像机捕捉到的人员图像。图像大小为$64\times
    128$像素，包含5607张图像，443个身份，具有不同的视角。'
- en: '| Dataset | Year | Multiple images | Multiple camera | Illumination variations
    | Pose variations | Partial occlusions | Scale variations | Crop image size |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | Year | Multiple images | Multiple camera | Illumination variations
    | Pose variations | Partial occlusions | Scale variations | Crop image size |'
- en: '| VIPeR | 2007 | $\times$ | ✓ | ✓ | ✓ | ✓ | $\times$ | $128\times 48$ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| VIPeR | 2007 | $\times$ | ✓ | ✓ | ✓ | ✓ | $\times$ | $128\times 48$ |'
- en: '| ETHZ | 2007 | ✓ | $\times$ | ✓ | $\times$ | ✓ | ✓ | vary |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| ETHZ | 2007 | ✓ | $\times$ | ✓ | $\times$ | ✓ | ✓ | vary |'
- en: '| PRID | 2011 | $\times$ | ✓ | ✓ | ✓ | ✓ | $\times$ | $128\times 64$ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| PRID | 2011 | $\times$ | ✓ | ✓ | ✓ | ✓ | $\times$ | $128\times 64$ |'
- en: '| CAVIAR | 2011 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | vary |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| CAVIAR | 2011 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | vary |'
- en: '| WARD | 2012 | ✓ | ✓ | ✓ | ✓ | $\times$ | $\times$ | $128\times 48$ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| WARD | 2012 | ✓ | ✓ | ✓ | ✓ | $\times$ | $\times$ | $128\times 48$ |'
- en: '| CUHK01 | 2012 | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | $160\times 60$ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| CUHK01 | 2012 | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | $160\times 60$ |'
- en: '| CUHK02 | 2013 | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | $160\times 60$ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| CUHK02 | 2013 | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | $160\times 60$ |'
- en: '| CUHK03 | 2014 | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | vary |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| CUHK03 | 2014 | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | vary |'
- en: '| i-LIDS | 2014 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | vary |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| i-LIDS | 2014 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | vary |'
- en: '| RAiD | 2014 | ✓ | ✓ | ✓ | ✓ | $\times$ | $\times$ | $128\times 64$ |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| RAiD | 2014 | ✓ | ✓ | ✓ | ✓ | $\times$ | $\times$ | $128\times 64$ |'
- en: '| Market-1501 | 2015 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | $128\times 64$ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Market-1501 | 2015 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | $128\times 64$ |'
- en: '| MARS | 2016 | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | $256\times 128$ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| MARS | 2016 | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | $256\times 128$ |'
- en: '| DukeMTMC | 2017 | ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ | vary |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| DukeMTMC | 2017 | ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ | vary |'
- en: '| MSMT | 2018 | ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ | vary |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| MSMT | 2018 | ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ | vary |'
- en: '| RPIfield | 2018 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | vary |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| RPIfield | 2018 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | vary |'
- en: '| ITSD | 2019 | ✓ | ✓ | $\times$ | ✓ | ✓ | $\times$ | $64\times 128$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ITSD | 2019 | ✓ | ✓ | $\times$ | ✓ | ✓ | $\times$ | $64\times 128$ |'
- en: 'Table 1: Summary on benchmark PReID datasets.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基准PReID数据集的总结。
- en: 3 Deep Neural Networks for PReID
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 Deep Neural Networks for PReID
- en: 'Deep learning techniques has been widely applied in several CV problems. This
    is due to the discriminative and generalization power of these learned models
    that results in promising performance and achievements. PReID is one of the challenging
    tasks in the area of CV for which DL models are one of the current best choice
    in research community. In the following section, we provide an overview of recent
    DL works for the task of PReID. Several interesting DL models have been proposed
    to improve PReID performance. These state-of-the-art DL approaches can be categorized
    by taking into account the learning methodology of their models that have been
    utilized in the PReID systems. Some works consider the PReID as a standard classification
    problem. On the other hand, some works have considered the issue of lack of training
    data samples in the PReID task and proposed a learning model to learn more discriminative
    features in a pair or triplet units. Figure [2](#S3.F2 "Figure 2 ‣ 3 Deep Neural
    Networks for PReID ‣ Survey on Reliable Deep Learning-Based Person Re-Identification
    Models: Are We There Yet?") shows the taxonomy of the types of models being used
    for PReID that will be discussed in the coming subsections of this paper.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习技术已经在多个计算机视觉（CV）问题中得到了广泛应用。这是因为这些学习模型的辨别能力和泛化能力带来了令人满意的性能和成果。PReID 是计算机视觉领域中具有挑战性的任务之一，对于这个任务，深度学习模型是当前研究社区中的最佳选择之一。在接下来的部分，我们将概述针对
    PReID 任务的最新深度学习工作。为了提高 PReID 的性能，提出了几种有趣的深度学习模型。这些最先进的深度学习方法可以通过考虑它们在 PReID 系统中所使用的学习方法来分类。一些工作将
    PReID 视为标准分类问题。另一方面，一些工作考虑到 PReID 任务中缺乏训练数据样本的问题，并提出了一种学习模型以在对或三元组单元中学习更具辨别性的特征。图 [2](#S3.F2
    "Figure 2 ‣ 3 Deep Neural Networks for PReID ‣ Survey on Reliable Deep Learning-Based
    Person Re-Identification Models: Are We There Yet?") 显示了用于 PReID 的模型类型分类，将在本文的后续部分讨论。'
- en: '![Refer to caption](img/ebe339df9dab67fc83dbefbf22dbcd48.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/ebe339df9dab67fc83dbefbf22dbcd48.png)'
- en: 'Figure 2: Taxonomy of deep feature-learning methods for PReID'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：PReID 的深度特征学习方法分类
- en: 3.1 Single Feature-Learning Based Methods
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 单特征学习方法
- en: 'A model based on a single feature-learning model or single deep model can be
    developed similarly to other multi-class classification problems. In a PReID system,
    a classification model is designed to determine the probability of identity of
    an individual that it belongs to [[28](#bib.bib28)]. Figure [3](#S3.F3 "Figure
    3 ‣ 3.1 Single Feature-Learning Based Methods ‣ 3 Deep Neural Networks for PReID
    ‣ Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are
    We There Yet?") shows an example of a DL based model for a single feature-learning
    PReID model. This single stream deep model can be further divided in following
    categories as being shown in Figure [2](#S3.F2 "Figure 2 ‣ 3 Deep Neural Networks
    for PReID ‣ Survey on Reliable Deep Learning-Based Person Re-Identification Models:
    Are We There Yet?").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '基于单一特征学习模型或单一深度模型的模型可以类似于其他多类分类问题进行开发。在 PReID 系统中，设计了一个分类模型来确定个体所属身份的概率 [[28](#bib.bib28)]。图 [3](#S3.F3
    "Figure 3 ‣ 3.1 Single Feature-Learning Based Methods ‣ 3 Deep Neural Networks
    for PReID ‣ Survey on Reliable Deep Learning-Based Person Re-Identification Models:
    Are We There Yet?") 显示了一个基于单一特征学习的 PReID 模型的深度学习模型示例。这种单流深度模型可以进一步分为图 [2](#S3.F2
    "Figure 2 ‣ 3 Deep Neural Networks for PReID ‣ Survey on Reliable Deep Learning-Based
    Person Re-Identification Models: Are We There Yet?") 所示的以下几类。'
- en: '![Refer to caption](img/2777c509443dbe6762af9838cc7e6015.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/2777c509443dbe6762af9838cc7e6015.png)'
- en: 'Figure 3: Single feature-learning model in PReID system: The model takes the
    raw image of an individual as input, and computes the probability of the corresponding
    class of the individual.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：PReID 系统中的单特征学习模型：该模型以个体的原始图像作为输入，并计算个体对应类别的概率。
- en: 'Deep model features fusion with hand-crafted features: There are number of
    papers published to boost the performance of PReID by generating deep features.
    Among them some works additionally involved the hand-crafted features as the complementary
    features to be fused alongside DL features. These features are further reduced
    by using traditional dimensionality reduction techniques – e.g., Principal component
    analysis(PCA).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型特征融合与手工特征：已经有大量论文发表，旨在通过生成深度特征来提升 PReID 的性能。其中一些工作还将手工特征作为补充特征，与深度学习特征一起融合。这些特征进一步通过传统的降维技术——例如主成分分析（PCA）进行减少。
- en: Wu et al. [[29](#bib.bib29)] proposed a feature fusion DNN to regularize CNN
    features, with joint of hand-crafted features. The network takes a single image
    of size $224\times 224\times 3$ pixels as input of the network, and hand-crafted
    features extracted using one of the state-of-the-art PReID descriptor (best performance
    obtained from ensemble of local features (ELF) descriptor[[30](#bib.bib30)]).
    Then, both extracted features are followed by a buffer layer and a fully-connected
    layer, which both layers act as a fusion layer. The buffer layer is used for the
    fusion, which is essential since it bridges the gap between two features with
    different domains (i.e., hand-crafted features and deep features). A softmax loss
    layer then takes the output vector of the fully-connected layer to minimizing
    the cross-entropy loss, and outputs the deep feature representation. The whole
    network is trained by applying mini-batch stochastic gradient descent algorithm
    for back propagation. In [[31](#bib.bib31)], two low-level descriptors, SIFT and
    color-histograms, are extracted from the LAB color space over a set of 14 overlapping
    patches in size of $32\times 32$ pixels with 16 pixels of stride. Then, a dimensionality
    reduction method such as PCA, is applied to scale-invariant feature transform
    (SIFT) and color-histogram features to reduce the dimensionality of feature space.
    Those features are further embedded to produce feature representations using Fisher
    vector encoding, which are linear separable. One Fisher vector is computed on
    the SIFT and another one on the color histogram features, and finally, two fisher
    vectors are concatenated as a single feature vector. A hybrid network builds fully-connected
    layers on the input of Fisher vectors and employs the linear discriminative analysis
    (LDA) as an objective function in order to maximize margin between two classes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人 [[29](#bib.bib29)] 提出了一个特征融合 DNN，以规范化 CNN 特征，并结合了手工设计的特征。网络以尺寸为 $224\times
    224\times 3$ 像素的单张图像作为网络输入，并使用先进的 PReID 描述符之一（最佳性能来自局部特征（ELF）描述符的集成 [[30](#bib.bib30)]）提取手工设计的特征。然后，这些提取的特征通过一个缓冲层和一个全连接层，这两个层共同作为融合层。缓冲层用于融合，这是必要的，因为它弥合了两个不同领域特征（即手工设计特征和深度特征）之间的差距。然后，softmax
    损失层接收全连接层的输出向量，以最小化交叉熵损失，并输出深度特征表示。整个网络通过应用小批量随机梯度下降算法进行反向传播训练。在 [[31](#bib.bib31)]
    中，从 LAB 颜色空间的 14 个重叠补丁（每个补丁尺寸为 $32\times 32$ 像素，步幅为 16 像素）中提取了两个低级描述符，SIFT 和颜色直方图。接着，应用诸如
    PCA 的降维方法，来缩放不变特征变换（SIFT）和颜色直方图特征，以减少特征空间的维度。这些特征进一步嵌入以生成特征表示，使用 Fisher 向量编码，这些特征是线性可分的。一个
    Fisher 向量在 SIFT 上计算，另一个在颜色直方图特征上计算，最终将两个 Fisher 向量连接成一个单一的特征向量。一个混合网络在 Fisher
    向量的输入上构建全连接层，并采用线性判别分析（LDA）作为目标函数，以最大化两个类别之间的间隔。
- en: A structured graph Laplacian algorithm was utilized in a CNN-based model [[32](#bib.bib32)].
    Different from traditional contrastive and triplet loss in terms of joint learning,
    the structured graph Laplacian algorithm is additionally embedded at the top of
    the network. They, indeed, formulate the triplet network into a single feature-learning
    method, and further, used the generated deep features for joint learning on the
    training sample. Softmax function is used to maximize the inter-class variations
    of different individual, while the structured graph Laplacian algorithm is employed
    to minimize the intra-class variations. As the authors pointed out, the designed
    network needs no additional network branch, which makes the training process more
    efficient. Later on, the same authors proposed a structured graph Laplacian embedding
    approach [[33](#bib.bib33)]; where joint CNNs are leveraged by reformulating structured
    Euclidean distance relationships into the graph Laplacian form. A triplet embedding
    method was proposed to generate high-level features by taking into account of
    inter-personal dispersion and intra-personal compactness.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个基于 CNN 的模型中使用了结构化图拉普拉斯算法[[32](#bib.bib32)]。与传统的对比损失和三元组损失在联合学习方面不同，结构化图拉普拉斯算法还被额外嵌入到网络的顶部。他们确实将三元组网络构造成一种单一的特征学习方法，并进一步使用生成的深层特征在训练样本上进行联合学习。Softmax
    函数用于最大化不同个体之间的类间变化，而结构化图拉普拉斯算法用于最小化类内变化。正如作者所指出的，设计的网络不需要额外的网络分支，这使得训练过程更高效。后来，同一作者提出了一种结构化图拉普拉斯嵌入方法[[33](#bib.bib33)]，该方法通过将结构化欧几里得距离关系重新表述为图拉普拉斯形式来利用联合
    CNNs。提出了一种三元组嵌入方法，通过考虑个人之间的分散性和个人内部的紧凑性来生成高级特征。
- en: 'Part-based & Body-based features: Some works have attempted to generate more
    discriminant features for their model by extracting features from specific body
    parts as well as extracting features from whole person’s body, that can be used
    as part of feature vector by fusing it with the deep learning model resultant
    features. In [[34](#bib.bib34)], a deep-convolutional model was proposed to handle
    misalignments and pose variations of pedestrian images. The overall multi-class
    person re-identification network is composed by two sub-networks: first a convolutional
    model is adopted to learn global features from the original images; then a part-based
    network is used to learn local features from an image, which includes six different
    parts of pedestrian bodies. Finally, both sub-networks are combined in a fusion
    layer as the output of the network, with shared weight parameters during training.
    The output of the network is further used as an image signature to evaluate the
    performance of their person re-identification approach with Euclidean distance.
    The proposed deep architecture explicitly enables to learn effective feature representations
    on the person’s body part and adaptive similarity measurements. Li et al. [[35](#bib.bib35)]
    designed a multi-scale context aware network to learn powerful features throughout
    the body and different body parts, which can capture knowledge of the local context
    by stacking convolutions of multiple scales in each layer. In addition, instead
    of using predefined rigid parts, the proposed model learns and locates deformable
    pedestrian parts through networks of spatial transformers with new spatial restrictions.
    Because of variations and background clutter that creates some difficulties in
    representations based on body-parts, the learning processes of full-body representation
    is integrated with body-parts for multi-class identification. Chen et al.[[36](#bib.bib36)]
    proposed a Deep Pyramidal Feature Learning (DPFL) CNN architecture for explicitly
    learning multi-scale deep features from a single input image. In addition, a fusion
    branch over $m$ scales was devised for learning complementary combination of multi-scale
    features.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于部位和整体特征：一些研究尝试通过从特定身体部位提取特征以及从整个人体提取特征来生成更多判别性特征，以用于与深度学习模型结果特征融合的特征向量。在 [[34](#bib.bib34)]中，提出了一种深度卷积模型来处理行人图像的错位和姿态变化。整体的多类行人重识别网络由两个子网络组成：首先采用卷积模型从原始图像中学习全局特征；然后使用基于部位的网络从图像中学习局部特征，这些局部特征涵盖了六个不同的行人身体部位。最后，这两个子网络在融合层中结合作为网络的输出，训练期间共享权重参数。网络的输出进一步用作图像签名，以利用欧几里得距离评估其行人重识别方法的性能。提出的深度架构明确支持在行人身体部位上学习有效的特征表示和自适应相似性度量。Li等人[[35](#bib.bib35)]设计了一种多尺度上下文感知网络，以在整个身体及不同身体部位上学习强大的特征，该网络通过在每层中堆叠多尺度的卷积来捕捉局部上下文知识。此外，提出的模型通过具有新空间限制的空间变换网络来学习和定位可变形的行人部位，而不是使用预定义的刚性部件。由于基于身体部位的表示受到变异和背景杂乱的影响，导致一些困难，因此全身表示的学习过程与身体部位表示相结合以进行多类识别。Chen等人[[36](#bib.bib36)]提出了一种深度金字塔特征学习（DPFL）CNN架构，以明确地从单一输入图像中学习多尺度深度特征。此外，设计了一个跨$m$尺度的融合分支，用于学习多尺度特征的互补组合。
- en: 'Embedding Learning: Embedding- and attribute-learning approaches have also
    been considered as a complementary feature by some researchers, where the authors
    proposed to design a model that can jointly learn additional mid-level features
    obtained by joint learning of high- and low-level features. In[[37](#bib.bib37)],
    a matching strategy is proposed to compute the similarity between features maps
    of an individual and corresponding embedding text. Their method is learned by
    optimizing the global and local association between local visual and linguistic
    features, where it computes attention weights for each sample. The attention weights
    are further used by long short-term memory (LSTM) network to enrich the final
    prediction. It shows that learning based on visual information could be more robust.
    Similarly, Chi et al. [[38](#bib.bib38)] proposed a multi-task learning model
    that learns from embedded attributes. The attribute embedding is employed as a
    low-rank attribute embedding integrated with low- and mid-level features to describe
    the person’s appearance. On the other hand, deep features are obtained by utilizing
    a DL framework as a high-level feature extractor. All the features are then learned
    simultaneously by making use of finding a significant correlation among tasks.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入学习：一些研究人员还将嵌入学习和属性学习方法视为互补特征，其中作者提出设计一种可以通过联合学习高级和低级特征来同时学习额外中级特征的模型。在[[37](#bib.bib37)]中，提出了一种匹配策略来计算个体特征图和相应嵌入文本之间的相似性。他们的方法通过优化局部视觉和语言特征之间的全局和局部关联来进行学习，其中计算每个样本的注意力权重。注意力权重进一步由长短期记忆（LSTM）网络用于丰富最终预测。研究表明，基于视觉信息的学习可能更具鲁棒性。同样，Chi等人[[38](#bib.bib38)]提出了一种从嵌入属性中学习的多任务学习模型。属性嵌入被用作与低级和中级特征集成的低秩属性嵌入，用以描述个体的外观。另一方面，通过利用深度学习框架作为高级特征提取器来获得深度特征。然后，通过寻找任务间显著的相关性来同时学习所有特征。
- en: 'Attribute-based Learner: A joint DL network is proposed in [[39](#bib.bib39)],
    which consists of two branches of DL frameworks; in the first branch, the network
    aims to learn the identity information from person’s appearance under a triplet
    Siamese network (see section [3.2.3](#S3.SS2.SSS3 "3.2.3 Triplet-loss methods
    ‣ 3.2 Multi-Stream Network Structure: Pairwise and Triplet Feature-Learning Methods
    ‣ 3 Deep Neural Networks for PReID ‣ Survey on Reliable Deep Learning-Based Person
    Re-Identification Models: Are We There Yet?") for more details), meanwhile, an
    attribute-based classification is utilized in the second branch to learn a hierarchical
    loss-guided structure to extract meaningful features. The obtained feature vectors
    of both branches are then concatenated into a single feature vector. Finally,
    the person’s images in gallery set are ranked according to their feature distances
    to the final representations. A method of attention mask-based feature learning
    is proposed in[[40](#bib.bib40)]; the authors proposed a CNN-based hybrid architecture
    that enables the network to focus on more discriminative parts from person’s image.
    A multi-tasking based solution where the model predicts the attention mask from
    an input image, and further imposes it on the low-level features in order to re-weighting
    local features in the feature space.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '基于属性的学习器：在[[39](#bib.bib39)]中提出了一种联合深度学习网络，该网络由两个深度学习框架分支组成；在第一个分支中，网络旨在通过三重Siamese网络学习个体的身份信息（更多细节见[3.2.3](#S3.SS2.SSS3
    "3.2.3 Triplet-loss methods ‣ 3.2 Multi-Stream Network Structure: Pairwise and
    Triplet Feature-Learning Methods ‣ 3 Deep Neural Networks for PReID ‣ Survey on
    Reliable Deep Learning-Based Person Re-Identification Models: Are We There Yet?")），同时在第二个分支中利用基于属性的分类学习一个层次化的损失引导结构以提取有意义的特征。然后，将两个分支获得的特征向量连接成一个单一的特征向量。最后，依据特征距离对图库中的人员图像进行排序。在[[40](#bib.bib40)]中提出了一种基于注意力掩码的特征学习方法；作者提出了一种基于CNN的混合架构，使网络能够关注个体图像中更具辨别性的部分。这是一种多任务解决方案，其中模型从输入图像中预测注意力掩码，并进一步将其施加于低级特征上，以重新加权特征空间中的局部特征。'
- en: 'Semi- and un-supervised learning: There are also few works related to semi-
    and un-supervised learning methods that attempted to predict person’s identity
    (i.e., probability of corresponding class label for an individual). Li et al.[[41](#bib.bib41)]
    proposed a novel unsupervised learning method attempts to replace the fact of
    manually labelling of data. The method jointly optimizes unlabelled person data
    within-camera view jointly with cross-camera view under the strategy of end-to-end
    classification problem. It utilizes deep features generated by a CNN model for
    the input of their unsupervised learning model. Wang et al.[[42](#bib.bib42)]
    proposed a heterogeneous multi-task model by domain transfer learning and addressed
    the scalable unsupervised learning for the PReID problem. Two branches of CNNs
    were employed to capture and learn identity and attribute from a person’s image
    simultaneously. The output from both branches are fused with another branch which
    composed by a shallow NN for a joint learning manner. The information from both
    branches are inferred into a single attribute space. It showed promising results
    when their model was trained on a source data set and tested on an unlabeled target
    data set.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督和无监督学习：关于半监督和无监督学习方法预测个人身份（即个体对应类别标签的概率）的相关研究也很少。Li等人[[41](#bib.bib41)] 提出了一种新颖的无监督学习方法，旨在替代手动标记数据的方式。该方法在端到端分类问题的策略下，联合优化相机视角内的无标签个人数据以及跨相机视角的数据。它利用CNN模型生成的深度特征作为其无监督学习模型的输入。Wang等人[[42](#bib.bib42)]
    通过领域迁移学习提出了一种异构多任务模型，并解决了PReID问题的可扩展无监督学习。两条CNN分支被用来同时从个人图像中捕捉和学习身份和属性。两条分支的输出与另一条由浅层神经网络组成的分支融合，以实现联合学习。来自两条分支的信息被推断到一个单一的属性空间。当他们的模型在源数据集上训练并在未标记的目标数据集上测试时，显示出了有希望的结果。
- en: The approach in  [[43](#bib.bib43)] addressed issues such as misalignment and
    occlusion in PReID. It aims to extract features from different pre-defined person’s
    body-parts, and considers them as pose features and attention aware feature. Yu
    et al.[[44](#bib.bib44)] proposed a novel unsupervised loss function, in which
    the model can learn the asymmetric metric and further embeds it into an end-to-end
    deep feature learning network. Moreover, Huang et al.[[45](#bib.bib45)] addressed
    the issue of lack of training data by introducing a multi-pseudo regularized label.
    The proposed method attempts to generate images based on an adversarial ML techniques,
    where corresponding class labels are estimated based on semi-supervised learning
    on a small training set. This could be one possible way of creating synthetic
    data to train recent deeper NN models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[[43](#bib.bib43)]中的方法解决了PReID中的对齐和遮挡问题。它旨在从不同预定义的身体部位提取特征，并将其视为姿势特征和注意力感知特征。Yu等人[[44](#bib.bib44)]
    提出了一种新颖的无监督损失函数，其中模型可以学习不对称度量，并进一步将其嵌入到一个端到端的深度特征学习网络中。此外，Huang等人[[45](#bib.bib45)]
    通过引入多伪标签正则化来解决训练数据不足的问题。该方法试图基于对抗性机器学习技术生成图像，其中对应的类别标签是基于在小训练集上进行的半监督学习进行估计的。这可能是生成合成数据以训练最近的深层神经网络模型的一种可能方式。'
- en: 'Data Driven: To address the lack of training data samples, data driven techniques
    have also been considered for the task of PReID. Xiao et al. [[46](#bib.bib46)]
    proposed learning deep feature representations from multiple data sets by using
    CNNs to discover effective neurons for each training set. They first produced
    a strong baseline model that works on multiple data sets simultaneously by combining
    the data and labels from several re-id data sets and training the CNN with a softmax
    loss. Next, for each data set, they performed the forward pass on all its samples
    and compute for each neuron its average impact on the objective function. Then,
    they replaced the standard dropout with the deterministic ’domain guided dropout’
    to learn generalization by dropping certain neurons during training, and continue
    to train the CNN model for several epochs. Some neurons are effective only for
    specific datasets, which might be useless for others due to dataset biases. For
    instance, the i-LIDS is the only dataset that contains pedestrians with luggage,
    thus the neurons that capture luggage features will be useless to recognize people
    from another data set. Another technique to overcome the lack of training data
    samples, data augmentation techniques have proposed. Those techniques are included
    the methods for flipping, rotating, sheering, etc. which can be applied on original
    image. Despite those techniques, in[[47](#bib.bib47)] a novel data augmentation
    technique was proposed for PReID, in which a camera style model was developed
    to generate training data samples via style transfer-learning.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动：为了解决训练数据样本不足的问题，数据驱动技术也被考虑用于 PReID 任务。Xiao 等人 [[46](#bib.bib46)] 提出了通过使用
    CNN 从多个数据集学习深层特征表示，以发现每个训练集的有效神经元。他们首先通过结合多个 re-id 数据集的数据和标签，并使用 softmax 损失训练
    CNN，创建了一个可以同时在多个数据集上工作的强基准模型。接下来，对于每个数据集，他们对其所有样本执行前向传播，并计算每个神经元对目标函数的平均影响。然后，他们用确定性的“领域引导
    dropout”替代了标准的 dropout，通过在训练过程中丢弃某些神经元来学习泛化，并继续训练 CNN 模型若干轮。一些神经元仅对特定数据集有效，而由于数据集偏差，可能对其他数据集无用。例如，i-LIDS
    是唯一包含带行李的行人的数据集，因此捕捉行李特征的神经元对识别来自其他数据集的人可能无用。另一种克服训练数据样本不足的方法是数据增强技术。这些技术包括翻转、旋转、剪切等方法，这些方法可以应用于原始图像。尽管有这些技术，但在
    [[47](#bib.bib47)] 中，提出了一种用于 PReID 的新型数据增强技术，其中开发了一个相机风格模型，通过风格迁移学习生成训练数据样本。
- en: '3.2 Multi-Stream Network Structure: Pairwise and Triplet Feature-Learning Methods'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 多流网络结构：成对和三元组特征学习方法
- en: DL models in the PReID problem are still suffering from the lack of training
    data samples; this is because some of the PReID data sets provide only a few images
    per individual (e.g., VIPeR dataset [[4](#bib.bib4)] which only contains pair
    of images per person) that makes the model to fail on evaluating the performance
    of model caused by overfitting problem. Therefore, Siamese networks have been
    developed to this aim[[18](#bib.bib18)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: PReID 问题中的深度学习模型仍然面临训练数据样本不足的问题；这是因为一些 PReID 数据集仅提供每个人的少量图像（例如，VIPeR 数据集 [[4](#bib.bib4)]
    仅包含每个人的一对图像），这导致模型因过拟合问题而在评估性能时失败。因此，已经开发了 Siamese 网络来解决这一问题 [[18](#bib.bib18)]。
- en: Siamese network models have been widely employed in PReID due to the lack of
    training instances in this research area. Siamese neural network (SNN) is a type
    of NN architectures that contains two or more identical sub-networks (i.e., identical
    refers to sub-networks when they share the same network architecture, parameters,
    and weights –a.k.a. *shared weight parameters*). A Siamese network can be employed
    as a pairwise model (when two sub-networks are included e.g. [[48](#bib.bib48),
    [3](#bib.bib3)]), or triplet model (when three sub-networks are present [[49](#bib.bib49),
    [50](#bib.bib50)]). The output of a Siamese model is a similarity score, which
    takes place at the top of the network. For instance, a model based on pairwise
    feature-learning takes two images as its input and outputs similarity score between
    them. Employing such a siamese model could be an excellent solution to train on
    existing PReID data set[[51](#bib.bib51)], when a few training samples are available.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该研究领域训练样本不足，**Siamese 网络模型**已广泛应用于PReID。Siamese 神经网络（SNN）是一种包含两个或更多相同子网络的神经网络架构（即，相同指的是子网络在共享相同的网络架构、参数和权重时——也称为*共享权重参数*）。Siamese
    网络可以作为成对模型使用（当包括两个子网络时，例如 [[48](#bib.bib48), [3](#bib.bib3)]），或者三元组模型（当存在三个子网络时
    [[49](#bib.bib49), [50](#bib.bib50)]）。Siamese 模型的输出是一个相似度评分，该评分位于网络的顶部。例如，基于成对特征学习的模型接受两张图像作为输入，并输出它们之间的相似度评分。在现有PReID数据集[[51](#bib.bib51)]上训练时，采用这样的Siamese模型可能是一个极佳的解决方案，尤其当训练样本较少时。
- en: '![Refer to caption](img/602203fb6ddc5c91dd223beb165351f8.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/602203fb6ddc5c91dd223beb165351f8.png)'
- en: 'Figure 4: Pairwise-loss feature-learning model.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '图4: 成对损失特征学习模型。'
- en: 'These models can be divided in the way we discussed single stream models in
    previous section [3.1](#S3.SS1 "3.1 Single Feature-Learning Based Methods ‣ 3
    Deep Neural Networks for PReID ‣ Survey on Reliable Deep Learning-Based Person
    Re-Identification Models: Are We There Yet?") and as shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3 Deep Neural Networks for PReID ‣ Survey on Reliable Deep Learning-Based
    Person Re-Identification Models: Are We There Yet?"). However, the rest of this
    section is organized in three subsections. First we gave a brief explanation of
    the similarity functions engaged in DL-based PReID methods. These are essential
    concepts to compute the distance of similarity between the output of the multi
    (two/three) models from the given multi input images during training DL model.
    In the second subsection, we described the published DL-based work for the pairwise
    methods followed by triplet methods in third subsection. Both of these pairwise
    and triplet follows the single stream feature learning approaches.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '这些模型可以按照我们在前一节[3.1](#S3.SS1 "3.1 Single Feature-Learning Based Methods ‣ 3
    Deep Neural Networks for PReID ‣ Survey on Reliable Deep Learning-Based Person
    Re-Identification Models: Are We There Yet?")中讨论的单流模型的方式进行划分，如图[2](#S3.F2 "Figure
    2 ‣ 3 Deep Neural Networks for PReID ‣ Survey on Reliable Deep Learning-Based
    Person Re-Identification Models: Are We There Yet?")所示。然而，本节的其余部分分为三个子部分。首先，我们简要解释了DL-based
    PReID方法中使用的相似度函数。这些是计算在训练DL模型过程中，从给定的多输入图像中获得的多（两个/三个）模型输出之间相似度距离的基本概念。在第二个子部分中，我们描述了针对成对方法的已发布DL-based工作，第三个子部分则介绍了三元组方法。这两种方法都遵循单流特征学习方法。'
- en: 3.2.1 Similarity functions
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 相似度函数
- en: In order to measure the similarity between the pair of input images within a
    siamese network, typically, an objective function is utilized. An objective function
    (a.k.a. loss orcost function) aims to map intuitively some values into a one single
    real number. This represents a cost which is associated with those values. Techniques
    like NNs seek to minimize a loss function optimally. When a loss function is used
    for a siamese model, it depends on the type of the utilized model (i.e., either
    a pairwise or triplet model).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量Siamese 网络中一对输入图像之间的相似度，通常使用目标函数。目标函数（也称为损失函数或成本函数）旨在将某些值直观地映射为一个单一的实数。这表示与这些值相关的成本。像NN这样的技术旨在最小化损失函数。当对Siamese模型使用损失函数时，这取决于所使用模型的类型（即，成对模型或三元组模型）。
- en: In the case of a pairwise model, let $X=\{x_{1},x_{2},\dots,x_{n}\}$ and $Y=\{y_{1},y_{2},\dots,y_{n}\}$
    be a set of images and corresponding labels for each individual, which can be
    formulated as
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于成对模型，设$X=\{x_{1},x_{2},\dots,x_{n}\}$ 和 $Y=\{y_{1},y_{2},\dots,y_{n}\}$为一组图像及其对应的标签，可以表示为
- en: '|  | $I(x_{i},x_{j})=\bigg{\{}\begin{array}[]{lll}positive&amp;if&amp;y_{i}=y_{j},\\
    negative&amp;if&amp;y_{i}!=y_{j}\end{array}$ |  | (1) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(x_{i},x_{j})=\bigg{\{}\begin{array}[]{lll}positive\&if\&y_{i}=y_{j},\\
    negative\&if\&y_{i}!=y_{j}\end{array}$ |  | (1) |'
- en: the goal is to minimize the relative distance between the matched pairs and
    maximize with the mismatched pairs,for given a pair of image representations,
    $x_{1}$ and $x_{2}$, and corresponding labels $y\in\{+1,-1\}$.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是最小化匹配对之间的相对距离，并最大化不匹配对的距离，对于给定的一对图像表示$x_{1}$和$x_{2}$以及相应的标签$y\in\{+1,-1\}$。
- en: Among existing loss functions for pairwise classification models, Hinge, and
    Contrastive loss functions have widely utilized in this vein. Hinge loss function
    refers to maximum-margin classification; the output of this loss is become zero
    when the distance similarity of the positive pairs is greater than the distance
    of the negative ones with respect to the margin $m$. This loss is defined as follow
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的对偶分类模型损失函数中，Hinge和对比损失函数被广泛应用于此领域。Hinge损失函数指的是最大间隔分类；当正对的距离相似度大于负对的距离与边界值$m$相比时，该损失为零。该损失定义如下：
- en: '|  | $I(x_{1},x_{2},y)=\bigg{\{}\begin{array}[]{lll}\left\lVert x_{1}-x_{2}\right\rVert&amp;if&amp;y=1\\
    max(0,m-\left\lVert x_{1}-x_{2}\right\rVert)&amp;if&amp;y=-1\end{array}$ |  |
    (2) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(x_{1},x_{2},y)=\bigg{\{}\begin{array}[]{lll}\left\lVert x_{1}-x_{2}\right\rVert\&if\&y=1\\
    max(0,m-\left\lVert x_{1}-x_{2}\right\rVert)\&if\&y=-1\end{array}$ |  | (2) |'
- en: The Cosine similarity loss function maximizes the cosine value for positive
    pairs and reduce the angle in between them, and at the same time, minimize the
    cosine value for the negative pairs when the value is less than margin.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度损失函数最大化正对的余弦值并减少它们之间的角度，同时，当负对的余弦值小于边界值时，最小化余弦值。
- en: '|  | $I(x_{1},x_{2},y)=\bigg{\{}\begin{array}[]{lll}max(0,cos(x_{1},x_{2})-m)&amp;if&amp;y=1\\
    1-cos(x_{1},x_{2})&amp;if&amp;y=-1\end{array}$ |  | (3) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(x_{1},x_{2},y)=\bigg{\{}\begin{array}[]{lll}max(0,cos(x_{1},x_{2})-m)\&if\&y=1\\
    1-cos(x_{1},x_{2})\&if\&y=-1\end{array}$ |  | (3) |'
- en: 'A Contrastive loss function minimizes meaningful mapping from high to low dimensional
    space maps by keeping the similarity of input vectors of nearby points on its
    output manifold and dissimilar vectors to distant points. Accordingly, the loss
    can be computed as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失函数通过保持输出流形上相邻点的输入向量相似性和远离点的不相似向量来最小化从高维到低维空间映射的有意义映射。因此，损失可以计算为：
- en: '|  | $I(x_{1},x_{2},y)=(1-y)\frac{1}{2}(D)^{2}+(y)\frac{1}{2}\{max(0,m-D)\}^{2}$
    |  | (4) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(x_{1},x_{2},y)=(1-y)\frac{1}{2}(D)^{2}+(y)\frac{1}{2}\{max(0,m-D)\}^{2}$
    |  | (4) |'
- en: where $m>0$ is a margin parameter acting as a boundary, and $D$ is a distance
    between two feature vector that is computed as $D(x_{1},x_{2})=\left\lVert x_{1}-x_{2}\right\rVert_{2}$.
    In order to compute the average of total loss for each above-mentioned pairwise
    loss functions, it can be computed as
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$m>0$是作为边界的边距参数，而$D$是计算为$D(x_{1},x_{2})=\left\lVert x_{1}-x_{2}\right\rVert_{2}$的两个特征向量之间的距离。为了计算上述每个对偶损失函数的平均总损失，可以计算为：
- en: '|  | $\mathcal{L}(X_{1},X_{2},Y)=-\frac{1}{n}\sum_{i=1}^{n}I(x_{i}^{1},x_{i}^{2},y_{i})$
    |  | (5) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(X_{1},X_{2},Y)=-\frac{1}{n}\sum_{i=1}^{n}I(x_{i}^{1},x_{i}^{2},y_{i})$
    |  | (5) |'
- en: 'For a triplet model, an objective function is used to train the network models,
    which creates a margin between the distance metric of positive pair and distance
    metric of negative pair. For this type of Siamese model, a softmax layer is employed
    at the top of the network on both distance outputs. Let $O_{i}=\{(I_{i},I^{+}_{i},I^{-}_{i})\}^{N}_{i=1}$
    be a set of triplet images, in which $I_{i}$ and $I^{+}_{i}$ are images of the
    same person, and $I^{-}_{i}$ is a different person. A triplet loss function is
    used to train the network models, which makes the distance between $I_{i}$ and
    $I^{+}_{i}$less than the mismatched pairs $I_{i}$ and $I^{-}_{i}$ in the learning
    feature space. In the triple-based models, Euclidean loss function is commonly
    used as a distance metric function. The loss function under *L2* distance metric,
    and denoted as $d(W,O_{i})$; where $W={W_{i}}$ is the network parameters, and
    $F_{w}(I)$ represents the network output of image $I$. The difference in the distance
    is computed between the matched pair and the mismatched pair of a single triplet
    unit $O_{i}$:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三元组模型，目标函数用于训练网络模型，创建正样本对和负样本对之间的距离边际。对于这种类型的 Siamese 模型，在两个距离输出的网络顶部使用了 softmax
    层。设 $O_{i}=\{(I_{i},I^{+}_{i},I^{-}_{i})\}^{N}_{i=1}$ 为一组三元组图像，其中 $I_{i}$ 和 $I^{+}_{i}$
    是同一人的图像，而 $I^{-}_{i}$ 是不同的人。三元组损失函数用于训练网络模型，使得 $I_{i}$ 和 $I^{+}_{i}$ 之间的距离小于学习特征空间中的不匹配对
    $I_{i}$ 和 $I^{-}_{i}$ 之间的距离。在基于三元组的模型中，欧几里得损失函数通常用作距离度量函数。损失函数在 *L2* 距离度量下表示为
    $d(W,O_{i})$；其中 $W={W_{i}}$ 是网络参数，$F_{w}(I)$ 表示图像 $I$ 的网络输出。计算匹配对和不匹配对之间的距离差异：
- en: '|  | $d(W,I_{i})=\lVert{F_{W}(I_{i})-F_{W}(I_{i}^{+})}\rVert^{2}-\lVert{F_{W}(I_{i})-F_{W}(I_{i}^{-})}\rVert^{2}$
    |  | (6) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $d(W,I_{i})=\lVert{F_{W}(I_{i})-F_{W}(I_{i}^{+})}\rVert^{2}-\lVert{F_{W}(I_{i})-F_{W}(I_{i}^{-})}\rVert^{2}$
    |  | (6) |'
- en: Moreover, the Hinge loss function is another widely used distance measurements.
    This loss function is a convex approximation in range of 0-1 ranking error loss,
    which approximate the model’s violation of the ranking order specified in the
    triplet.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Hinge 损失函数是另一种广泛使用的距离测量。该损失函数是 0-1 排名误差损失的凸近似，用于近似模型在三元组中指定的排名顺序的违规情况。
- en: '|  | $\mathcal{L}(I_{i},I^{+}_{i},I^{-}_{i})=max(0,g+D(I_{i},I^{+}_{i})-D(I_{i},I^{-}_{i}))$
    |  | (7) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(I_{i},I^{+}_{i},I^{-}_{i})=max(0,g+D(I_{i},I^{+}_{i})-D(I_{i},I^{-}_{i}))$
    |  | (7) |'
- en: 'where $g$ is a margin parameter that regularizes the margin between the distance
    of the two image pairs: $(I_{i},I_{i}^{+})$ and $(I_{i},I_{i}^{-})$, and $D$ is
    the euclidean distance between the two euclidean points.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g$ 是一个边际参数，用于规范化两个图像对 $(I_{i},I_{i}^{+})$ 和 $(I_{i},I_{i}^{-})$ 之间的距离边际，$D$
    是两个欧几里得点之间的欧几里得距离。
- en: 3.2.2 Pairwise-loss methods
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 配对损失方法
- en: Several works rely upon pairwise modeling in order to learn features from small
    set of training data. To this aim, some works proposed novel deep learning architectures
    for learning in pairwise manner. This types of learning treats PReID task as a
    binary-class classification problem [[18](#bib.bib18), [52](#bib.bib52), [53](#bib.bib53)].
    In [[54](#bib.bib54)], a Siamese pair-based model takes two images as the input
    of two sub-networks, where two networks are locally connected to the first convolutional
    layer. They employed a linear SVM on the top of the network instead of using a
    softmax activation function to measure the similarity of input images pair as
    the output of the network.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究依赖于配对建模，以便从少量训练数据中学习特征。为此，一些研究提出了新的深度学习架构，用于以配对方式进行学习。这种学习方法将 PReID 任务视为一个二分类问题
    [[18](#bib.bib18), [52](#bib.bib52), [53](#bib.bib53)]。在 [[54](#bib.bib54)] 中，基于
    Siamese 配对的模型将两张图像作为两个子网络的输入，其中两个网络局部连接到第一个卷积层。他们在网络的顶部使用了线性 SVM，而不是使用 softmax
    激活函数，以测量输入图像对的相似度作为网络的输出。
- en: In [[55](#bib.bib55)], a siamese neural network (SNN) has been designed to learn
    pairwise similarity. Each input image of a pair was first partitioned into three
    overlapping horizontal parts. The part pairs are matched through three independent
    Siamese networks and finally, are combined at score level. Li et al. [[18](#bib.bib18)]
    proposed a deep filter pairing NN to encode photometric transformation across
    camera views. A patch matching layer is further added to the network to multiple
    convolution feature maps of pair of images in different horizontal stripes. Later,
    Ahmed et al.[[52](#bib.bib52)] improved the pair-based Siamese model in which
    the network takes pairs of images as the input, and outputs the probability of
    whether two images in the pair are referred to the same or different persons.
    The generated feature maps are passed through a max-pooling kernel to another
    convolution layer followed by a max-pooling layer in order to decrease the size
    of the feature map. Then a cross-input neighborhood layer computes the differences
    of the features in neighboring locations of the other image.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[55](#bib.bib55)]中，设计了一种孪生神经网络（SNN）来学习成对的相似性。每对输入图像首先被划分为三个重叠的水平部分。这些部分对通过三个独立的孪生网络进行匹配，最后在得分层面上进行合并。李等人[[18](#bib.bib18)]提出了一种深度滤波配对神经网络，用于编码相机视图之间的光度变换。网络中进一步添加了一个补丁匹配层，以处理成对图像在不同水平条带中的多个卷积特征图。随后，Ahmed等人[[52](#bib.bib52)]改进了基于对的孪生模型，其中网络以图像对作为输入，并输出图像对中两张图像是否属于同一人或不同人的概率。生成的特征图通过最大池化核传递到另一个卷积层，然后经过最大池化层，以减少特征图的大小。接着，交叉输入邻域层计算另一张图像相邻位置的特征差异。
- en: 'New Architectures: Wang et al.[[56](#bib.bib56)] developed a CNN model to jointly
    learn single-image representation (SIR) and cross-image representation (CIR) for
    PReID. Their methodology relied on investigating two separate models for comparing
    pairwise and triplet images (explained in next section) with similar deep structure.
    Each of these models configured with different sub-networks for SIR and CIR learning,
    and another sub-network shared by both SIR and CIR learning. For the pairwise
    comparison, they used the Euclidean distance as loss function to learn SIR and
    formulated the CIR learning that provides a binary classification problem and
    employs the standard SVM to learn CIR as its loss function. It uses the combination
    of both loss functions as the overall loss function of pairwise comparison. For
    the triplet comparison, the loss function to learn SIR makes the distance between
    the matched pairs lower than the mismatched pairs. The CIR learning formulates
    a learning-to-rank problem and employs the ’RankSVM’ as its loss function. To
    this end, the combination of both learning methods is used as the overall loss
    function of the triple comparison. The shared sub-networks share parameters during
    the training stage.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 新架构：王等人[[56](#bib.bib56)]开发了一种CNN模型，用于联合学习单图像表示（SIR）和跨图像表示（CIR）以进行PReID。他们的方法依赖于研究两个具有类似深度结构的模型，以比较成对和三重图像（在下一节中进行解释）。每个模型配置了用于SIR和CIR学习的不同子网络，以及一个由SIR和CIR学习共享的子网络。对于成对比较，他们使用欧几里得距离作为损失函数来学习SIR，并制定了CIR学习，将其视为二分类问题，并采用标准SVM来学习CIR作为其损失函数。它使用这两个损失函数的组合作为成对比较的总体损失函数。对于三重比较，学习SIR的损失函数使匹配对之间的距离低于不匹配对之间的距离。CIR学习将其表述为排序学习问题，并采用“RankSVM”作为其损失函数。为此，使用这两种学习方法的组合作为三重比较的总体损失函数。共享子网络在训练阶段共享参数。
- en: Wang et al. [[57](#bib.bib57)] proposed a pairwise Siamese model by embedding
    a metric learning method at the top of the network to learn spatiotemporal features.
    The network takes a pair of images in order to obtain CNN features and outputs
    whether two images belong to the same person by employing the quadratic discriminant
    analysis method.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人[[57](#bib.bib57)]提出了一种成对的孪生模型，通过在网络顶部嵌入度量学习方法来学习时空特征。网络接受一对图像以获得CNN特征，并通过采用二次判别分析方法输出两张图像是否属于同一个人。
- en: To handle the multi-view person images and learning in pairwise manner, a new
    deep multi-view feature learning (DMVFL) model was proposed in [[58](#bib.bib58)]
    to combine handcrafted features (e.g., local maximal occurrence (LOMO)[[59](#bib.bib59)])
    with deep features generated by CNN-based model; and embedding a metric distance
    method at the top of the network in order to learn metric distance. To this aim,
    the cross-view quadratic discriminant analysis (XQDA) metric learning method [[59](#bib.bib59)]
    is utilized to jointly learn handcrafted and DL features. In this manner, it is
    possible to investigate how handcrafted features could be influenced by deep CNN
    features. Further, a two-channel CNN with a new component named Pyramid Person
    Matching Network (PPMN) is proposed[[60](#bib.bib60)] with the same architecture
    of GoogLeNet. The network takes a pair of images and extracts semantic features
    by convolutional layers. Finally, the Pyramid Matching Module learns the corresponding
    similarity between semantic features based on multi-scale convolutional layers.
    A Strict Pyramidal Deep Metric Learning approach was proposed in[[3](#bib.bib3)]
    in which a Siamese network is composed by two strict pyramidal CNN blocks with
    shared parameters between each and produced salient features of an individual
    as the output of the network. The objective was to present a simple network structure
    that can perform well despite fewer parameters and having a trade-off between
    lower computational and memory costs concerning the other NNs. In[[61](#bib.bib61)],
    a Siamese pairwise model is designed as a manner of re-ranking approach where
    a CNN model was adopted to generate high-level features. The obtained feature
    maps from both sub-networks are mapped into a single feature vector. It is then
    divided into $K$ feature groups, in which the output of the network is equivalent
    to the number of feature groups and presented as the similarity scores corresponded
    to each pair. Shen et al.[[62](#bib.bib62)] addressed the issue of spatial information
    from a person’s appearance. They utilized Kronecker-product matching (KPM) for
    aligning feature maps of each individual and further used them in order to generate
    matching confidence maps between pairs of images. At the beginning of their model,
    two CNN models are utilized to generate feature maps for each pair of images,
    separately. Then, the obtained feature maps are used in the KPM method to generate
    wrapped feature maps. The difference between two feature vectors is then mapped
    between generated feature maps from the first image and wrapped feature maps by
    using simple element-wise subtraction. Also, a self-residual attention learning
    is employed on the feature maps of the first image. Finally, the computed features
    maps are further mapped into a single feature map by using element-wise addition.
    The final feature map is then followed by an element-wise square, batch normalization,
    and a softmax layer yields the final probability score between a pair of images.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理多视角人物图像并以成对方式进行学习，提出了一种新的深度多视角特征学习（DMVFL）模型[[58](#bib.bib58)]，该模型将手工特征（例如，局部最大出现（LOMO）[[59](#bib.bib59)])与由基于CNN的模型生成的深度特征相结合；并在网络顶部嵌入度量距离方法以学习度量距离。为此，采用了交叉视角二次判别分析（XQDA）度量学习方法[[59](#bib.bib59)]，以联合学习手工特征和深度学习（DL）特征。通过这种方式，可以研究手工特征如何受到深度CNN特征的影响。此外，提出了一种具有新组件名为金字塔人物匹配网络（PPMN）的双通道CNN[[60](#bib.bib60)]，其架构与GoogLeNet相同。该网络接受一对图像并通过卷积层提取语义特征。最后，金字塔匹配模块基于多尺度卷积层学习语义特征之间的相似性。在[[3](#bib.bib3)]中，提出了一种严格金字塔深度度量学习方法，其中一个Siamese网络由两个严格金字塔CNN块组成，这些块之间具有共享参数，并产生个体的显著特征作为网络的输出。其目标是呈现一个简单的网络结构，即使参数较少也能表现良好，并在较低计算和内存成本与其他神经网络之间进行权衡。在[[61](#bib.bib61)]中，设计了一种Siamese成对模型作为重新排名方法，其中采用CNN模型生成高级特征。从两个子网络中获得的特征图被映射到单个特征向量中。然后将其分为$K$个特征组，其中网络的输出等于特征组的数量，并以与每对图像对应的相似性分数的形式呈现。Shen等人[[62](#bib.bib62)]解决了来自个人外观的空间信息问题。他们利用Kronecker积匹配（KPM）对每个个体的特征图进行对齐，并进一步使用它们生成图像对之间的匹配置信度图。在模型开始时，分别利用两个CNN模型为每对图像生成特征图。然后，利用KPM方法生成包裹特征图。通过简单的逐元素减法，将两个特征向量之间的差异映射到第一幅图像生成的特征图与包裹特征图之间。此外，还在第一幅图像的特征图上采用自残差注意力学习。最后，通过逐元素加法将计算出的特征图进一步映射为单一特征图。最终特征图经过逐元素平方、批量归一化和softmax层处理，得出图像对之间的最终概率分数。
- en: A pairwise multi-task DL based model is proposed  [[63](#bib.bib63)] to use
    a separate softmax for each auxiliary task as identification, pose labeling, and
    each attribute labeling task. A CNN is used to generate image representations
    in which a single image of size $64\times 64$ is used as the input. It takes a
    pair of images by embedding a specific cost function for each. For instance, they
    used a softmax regression cost function for each task in which a multi-class linear
    classifier calculates the probability of a person’s identity. They minimized the
    cost function using SGD concerning the weights of each task; then, the linear
    combination overall cost functions is presented as the final cost function of
    the network. The designed network is composed of three convolutional layers, and
    two max-pooling layers, followed by a final fully-connected layer as the output
    of the network. The hyperbolic tangent activation function was used between each
    convolutional layer, while a linear layer was used between the final convolutional
    layer and the fully-connected layer. The activation of neurons in the fully-connected
    layer gives the feature representation of the input image; dropout regularization
    was used between the final convolutional layer and the fully-connected layer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了一个成对的多任务DL模型[[63](#bib.bib63)]，为每个辅助任务如识别、姿态标记和每个属性标记任务使用一个单独的softmax。使用CNN生成图像表示，其中一张$64\times
    64$大小的单图像作为输入。它处理一对图像，为每对图像嵌入一个特定的成本函数。例如，他们为每个任务使用softmax回归成本函数，其中多类线性分类器计算一个人的身份概率。他们使用SGD最小化成本函数以适应每个任务的权重，然后，将线性组合的整体成本函数作为网络的最终成本函数。设计的网络由三层卷积层和两层最大池化层组成，最后一个全连接层作为网络的输出。每个卷积层之间使用了双曲正切激活函数，而在最后一个卷积层和全连接层之间使用了线性层。全连接层中神经元的激活提供了输入图像的特征表示；在最后一个卷积层和全连接层之间使用了dropout正则化。
- en: A novel Siamese Long-Short Term Memory (LSTM) based architecture was proposed
    in [[64](#bib.bib64)] which aims to leverage contextual dependencies by selecting
    relevant contexts to enhance discriminative capabilities of the local features.
    They proposed a pairwise Siamese model, which contains six LSTM models for each
    sub-network. First, each image is divided into six horizontal non-overlapping
    parts. From each part, an image representation is extracted by using two state-of-the-art
    descriptors (i.e., LOMO and Color Names). Each feature vector is separately fed
    to a single LSTM network with the share parameters. The outputs from each LSTM
    network are combined, and the relative distance of subnets is computed by the
    contrastive loss function. The whole pairwise network is trained with the mini-batch
    SGD algorithm.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了一个基于**Siamese 长短期记忆（LSTM）**的全新架构[[64](#bib.bib64)]，旨在通过选择相关的上下文来利用上下文依赖，从而增强局部特征的判别能力。他们提出了一个成对的Siamese模型，该模型包含六个LSTM模型，每个子网络对应一个。首先，将每张图像分成六个水平的非重叠部分。从每个部分中，通过使用两个先进的描述符（即LOMO和Color
    Names）提取图像表示。每个特征向量分别输入到具有共享参数的单个LSTM网络中。每个LSTM网络的输出被组合，子网络的相对距离通过对比损失函数计算。整个成对网络使用迷你批量SGD算法进行训练。
- en: 'Part- and Body-Based Features Fusion: Furthermore, some researches have also
    considered multi-scale and multi-part feature learning from person’s images. Wang
    et al.[[65](#bib.bib65)] designed an ensemble of multi-scale and multi-part with
    CNNs to jointly learn image representations and similarity measure. The The network
    takes two person images as the input of the network and derived the full scale,
    half scale, top part, and middle part image pairs from the original images. The
    network outputs the similarity score of the pair images. This architecture is
    composed of four separate sub-CNNs with each sub-CNN embedding images of different
    scales or different parts. The first sub-CNN takes full images of size $200\times
    100$ and the second sub-CNN takes down-sampled images of size $100\times 50$.
    The next two sub-CNNs take the top part and middle part as input, respectively.
    Four sub-CNNs are all composed of two convolutional layers, two max-pooling layers,
    a fully-connected layer, and one L2-normalization layer. They obtained the image
    representation from each sub-CNN and then calculated their similarity score. The
    final score is calculated by averaging four separate scores; A ReLU activation
    function is used as the neuron activation function for each layer, dropout layer
    is used in the fully-connected layer to reduce the risk of an over-fitting problem.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 部分与身体基础特征融合：此外，一些研究还考虑了从人物图像中学习多尺度和多部分特征。Wang 等人[[65](#bib.bib65)] 设计了一个结合多尺度和多部分特征的
    CNN 集成方法，以共同学习图像表示和相似性度量。该网络将两张人物图像作为输入，从原始图像中衍生出全尺度、半尺度、上部和中部图像对。网络输出图像对的相似性得分。该架构由四个独立的子-CNN
    组成，每个子-CNN 嵌入不同尺度或不同部分的图像。第一个子-CNN 处理尺寸为 $200\times 100$ 的全图像，第二个子-CNN 处理尺寸为 $100\times
    50$ 的下采样图像。接下来的两个子-CNN 分别处理上部和中部图像。四个子-CNN 都由两个卷积层、两个最大池化层、一个全连接层和一个 L2 正则化层组成。他们从每个子-CNN
    中获得图像表示，然后计算其相似性得分。最终得分通过四个独立得分的平均值计算；每层使用 ReLU 激活函数作为神经元激活函数，全连接层使用 dropout 层以降低过拟合的风险。
- en: Liu et al. [[66](#bib.bib66)] utilized a deep model to integrate a soft attention-based
    model into a Siamese network. The model focuses on local parts of input images
    on the pair-based Siamese model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人[[66](#bib.bib66)] 将一个基于软注意力的深度模型集成到一个孪生网络中。该模型在基于对的孪生模型中关注输入图像的局部部分。
- en: 'Multi-Scale Learning Models: A multi-scale learning model was proposed in  [[67](#bib.bib67)]
    in which the proposed approach can learn discriminant feature from multi-scales
    of an image in different levels of resolution. A saliency-based learning strategy
    is adopted to learn important weight scales. In parallel with the pairwise Siamese
    model, which aims to distinguish whether a pair of images belong to the same person
    or not, a tied layer is also used between each layer of each branch in order to
    verify the identity of an individual. The designed model consists of five components:
    tied convolutional layers, multi-scale stream layers, saliency-based learning
    fusion layer, verification subnet, and classification sub-network. The same authors,
    lately, proposed another approach in[[68](#bib.bib68)] in order to learn pedestrian
    features from different resolution levels of filters over multiple locations and
    spatial scales.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度学习模型：在[[67](#bib.bib67)] 中提出了一种多尺度学习模型，该方法可以从不同分辨率的图像中学习判别特征。采用基于显著性的学习策略来学习重要的权重尺度。与旨在区分图像对是否属于同一个人的对比孪生模型并行，设计了一个
    tied 层用于每个分支的每一层之间，以验证个体身份。设计的模型由五个组件组成：tied 卷积层、多尺度流层、基于显著性的学习融合层、验证子网和分类子网络。最近，同一作者在[[68](#bib.bib68)]
    提出了另一种方法，以从多个位置和空间尺度的不同分辨率滤波器中学习行人特征。
- en: A patch-based feature learning method was proposed in [[69](#bib.bib69)]. A
    pairwise Siamese network takes a CNN features pair as input and outputs the similarity
    value between them by applying the cosine and Euclidean distance function. Each
    sub-network contains a CNN-based model to obtain deep features of each input image
    pair, and then, each image is split into three overlapping color patches. The
    deep network built in three different branches, and each branch takes a single
    patch as its input; finally, the three branches are concluded by a fully-connected
    layer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[69](#bib.bib69)] 中提出了一种基于补丁的特征学习方法。一个成对的Siamese网络将 CNN 特征对作为输入，并通过应用余弦和欧几里得距离函数输出它们之间的相似度值。每个子网络包含一个基于
    CNN 的模型，以获取每对输入图像的深度特征，然后每幅图像被分割成三个重叠的颜色补丁。深度网络建立在三个不同的分支中，每个分支以单个补丁作为输入；最后，三个分支通过一个全连接层汇总。
- en: Some works attempted to adopt metric- and transfer-learning methods in pairwise
    feature-learning models. Chen et al. [[70](#bib.bib70)] proposed a deep ranking
    model to jointly learn image representation and similarities for comparing pairwise
    images. To this aim, a deep CNN is trained to assign a higher similarity score
    to the positive pair than any negative pairs in each ranking unit by utilizing
    the logistic activation function, which is employed as $\sigma(x)=log_{2}(1+2^{-x})$.
    They first stitched a pair of images horizontally to form an image which is used
    as an input, and then, the network returns a similarity score as its output. A
    Deep Hybrid Similarity Learning model (DHSL) [[71](#bib.bib71)] based on CNN is
    proposed to learn the similarity between pair of images. This two-channel CNN
    with ten layers aims to learn pair feature vectors, discriminate input pairs to
    minimize the network’s output value for similar pair images, and to maximize for
    different ones. A new hybrid distance method using element-wise absolute difference
    and multiplication is proposed to improve the CNN in similarity metrics learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究尝试在成对特征学习模型中采用度量学习和迁移学习方法。陈等人[[70](#bib.bib70)] 提出了一个深度排序模型，用于共同学习图像表示和相似度，以便比较成对的图像。为此，训练一个深度
    CNN，利用逻辑激活函数 $\sigma(x)=log_{2}(1+2^{-x})$，将正样本对赋予比任何负样本对更高的相似度评分。他们首先将一对图像水平拼接形成一个输入图像，然后网络返回一个相似度评分作为输出。基于
    CNN 的深度混合相似性学习模型（DHSL）[[71](#bib.bib71)] 被提出用于学习图像对之间的相似性。该双通道 CNN 具有十层，旨在学习成对特征向量，区分输入对以最小化对相似图像对的网络输出值，同时最大化对不同图像对的输出值。提出了一种新的混合距离方法，利用元素级绝对差和乘法来改进
    CNN 在相似性度量学习中的表现。
- en: 'Transfer learning: It is a technique which consists of fine-tuning the parameters
    of a network that has been already trained on a different dataset, in order to
    adapt it into a new system. Franco et al.[[72](#bib.bib72)] proposed a coarse-to-fine
    approach to achieve generic-to-specific knowledge through transfer learning. The
    approach follows three steps: first a hybrid network is trained to recognize a
    person, then another hybrid network employed to discriminate the gender of person;
    the output of two networks are passed through the coarse-to-fine transfer learning
    method to a pairwise Siamese network to accomplish the final PReID goal in terms
    of measuring the similarity between those two features. Later, the same authors
    proposed a different type of features based on convolutional covariance descriptor
    (CCF)[[73](#bib.bib73)]. They intend to obtain a set of local covariance matrices
    over the feature maps extracted by the hybrid network under the same strategy
    of the above-proposed method.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习：这是一种技术，包括对已经在不同数据集上训练过的网络进行微调，以便将其适应到新的系统中。弗朗哥等人[[72](#bib.bib72)] 提出了一个粗到细的方法，通过迁移学习实现从通用到特定的知识。该方法包括三个步骤：首先训练一个混合网络以识别一个人，然后训练另一个混合网络以区分该人的性别；两个网络的输出通过粗到细的迁移学习方法传递到成对的Siamese网络中，以实现最终的
    PReID 目标，即测量这两个特征之间的相似性。随后，同样的作者提出了一种基于卷积协方差描述符（CCF）[[73](#bib.bib73)] 的不同特征类型。他们打算在上述方法的相同策略下，获取混合网络提取的特征图上的一组局部协方差矩阵。
- en: 3.2.3 Triplet-loss methods
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 三元组损失方法
- en: 'Several works proposed novel PReID systems based on deep learning architecture
    for learning in a triplet manner. Triplet models mainly introduced for image-retrieval [[74](#bib.bib74)]
    and face recognition[[75](#bib.bib75)] problems. Such that model takes three images
    of individual, in a formation as a triplet unit, aiming to minimize the relevant
    similarity distance between the same person, and maximize from different one.
    Figure[5](#S3.F5 "Figure 5 ‣ 3.2.3 Triplet-loss methods ‣ 3.2 Multi-Stream Network
    Structure: Pairwise and Triplet Feature-Learning Methods ‣ 3 Deep Neural Networks
    for PReID ‣ Survey on Reliable Deep Learning-Based Person Re-Identification Models:
    Are We There Yet?") shows a basic triplet model. This type of models either can
    share weights or keep them independent.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究提出了基于深度学习架构的创新 PReID 系统，用于以三重方式学习。三重模型主要用于图像检索[[74](#bib.bib74)]和人脸识别[[75](#bib.bib75)]问题。该模型采用三张个体图像作为三重单元，旨在最小化同一人的相关相似度距离，并最大化不同人的相似度距离。图[5](#S3.F5
    "图 5 ‣ 3.2.3 三重损失方法 ‣ 3.2 多流网络结构：对比和三重特征学习方法 ‣ 3 深度神经网络用于 PReID ‣ 可靠的深度学习基础人员再识别模型的调查：我们到达了吗？")
    显示了一个基本的三重模型。这类模型可以共享权重，也可以保持独立。
- en: '![Refer to caption](img/6a770f26e1af296e3f9e5aa87ed60fa8.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6a770f26e1af296e3f9e5aa87ed60fa8.png)'
- en: 'Figure 5: Triplet-loss feature-learning model.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：三重损失特征学习模型。
- en: Ding et al. [[76](#bib.bib76)] is the first work in PReID task which adopted
    a triplet deep CNN-based models to produce robust feature representations from
    raw images. It takes input image size of $250\times 100$ pixels as a triplet unit,
    where the weights are shared between each sub-networks. It aims to maximize the
    relative distance between pairs of images of the same person and a different person
    under $L_{2}$ loss function. The model is trained with the SGD algorithm with
    respect to the output feature of the network.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 丁等人[[76](#bib.bib76)] 是第一个在 PReID 任务中采用三重深度 CNN 模型来从原始图像中生成鲁棒特征表示的工作。该模型以$250\times
    100$像素的图像大小作为三重单元，并且每个子网络之间的权重是共享的。其目标是在$L_{2}$损失函数下最大化同一人和不同人的图像对之间的相对距离。该模型使用
    SGD 算法对网络的输出特征进行训练。
- en: 'A learning approach was proposed in [[77](#bib.bib77)] to reformulate a multi-tasking
    problem in PReID task; whereby the method considered as a joint system overall
    image retrieval technique across disjoint camera views jointly with deep features
    and hash-learning functions. A deep architecture of NN was utilized to produce
    the hashing codes with the weight matrix by taking raw images of size $250\times
    100$ pixels as input of the network. The network was trained in a triplet manner
    for similarity feature-learning to enforce that the images of same person should
    have similar hash codes. For each triplet unit, it maximize the margin between
    the matched pairs and the mismatched pairs. It uses Alexnet pre-trained network
    that consists of ten layers: the first six layers form the convolution-pooling
    network with rectified linear activation and average pooling operation. They used
    32, 64, and 128 kernels with size $5\times 5$ in the first, second, and third
    convolutional layers and the stride of 2 pixels in every convolution layer. The
    stride for pooling is 1 and they set the pooling operator size as $2\times 2$.
    The last four layers consists of two fully-connected layers, and a tangent like
    layer to generate the output as the hash codes, and an element-wise connected
    layer to manipulate the hash code length by weighting each bin of the hashing
    codes. The number of units set 512 in the first fully-connected layer and the
    output of the second fully-connected layer equals to the length of hash code.
    The activation function of the second fully-connected layer is the tanh-like function,
    while ReLu activation function is adopted for the others.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了一个学习方法[[77](#bib.bib77)]，旨在重新定义PReID任务中的多任务问题；该方法被认为是一种联合系统总体图像检索技术，结合了不同摄像头视角的深度特征和哈希学习函数。利用深度神经网络架构生成哈希码，权重矩阵通过将尺寸为$250\times
    100$像素的原始图像作为网络输入。该网络以三元组方式训练，以相似特征学习来确保同一人的图像具有相似的哈希码。对于每个三元组单元，它最大化匹配对和不匹配对之间的间隔。使用了预训练的Alexnet网络，该网络由十层组成：前六层形成具有修正线性激活和平均池化操作的卷积-池化网络。他们在第一、第二和第三卷积层中分别使用了32、64和128个$5\times
    5$的内核，每个卷积层的步幅为2像素。池化的步幅为1，池化操作符的大小设置为$2\times 2$。最后四层包括两个全连接层和一个类似正切的层，用于生成哈希码输出，以及一个逐元素连接层，通过加权每个哈希码的箱子来调整哈希码长度。第一全连接层设置为512个单元，第二全连接层的输出等于哈希码的长度。第二全连接层的激活函数为tanh-like函数，而其他层则采用ReLu激活函数。
- en: 'Part- and Body-based features: Cheng et al. [[49](#bib.bib49)] proposed a triplet
    loss function in which the network takes a triplet unit of images as input, and
    jointly learn from the global full-body and local body-parts features as robust
    representation. The fusion of these two types of features at the top of the network
    is presented as the output of the network. The utilized CNN model begins with
    a convolution layer, divided into four equal parts, and each part forms the first
    layer of an independent body-part channel aiming to learn features from that body
    part. The four body-part channels with the full-body channel constitute five independent
    channels that are trained separately from each other (with no parameter sharing
    between each). At the top of the network, the outputs obtained from the five separate
    channels are concatenated into a single vector, and is passed through a final
    fully-connected layer. Bai et al.[[78](#bib.bib78)] proposed a deep-person model
    to generate global- and part-based feature representations of a person’s body.
    Each image of triplet unit is fed into a backbone CNN to generate low-level features
    with the shared parameters. Output features of the backbone network are further
    fed into a two-layer Bidirectional LSTM aiming to generate a part-based feature
    representation; an LSTM is adopted because of its discriminative ability of part
    representation with contextual information, handling the misalignment with the
    sequence-level person representation. At the same time, layer output features
    are also fed into another network branch, with a global average pooling, a fully-connected,
    and a Softmax layer for global feature learning. Finally, output features learn
    similarity distances under a triplet loss function by adopting another branch
    of network during the training of the whole network. A coherent and conscious
    DL approach was introduced that is able to cover whole network cameras[[79](#bib.bib79)].
    The proposed approach aims to seek the globally optimal matching over different
    cameras. The deep features are generated over full body and part body under a
    triplet framework, in which each image within a triple unit is presented with
    a sample image of one camera view, while the other images are presented from other
    camera views. Once deep features are generated, the cosine similarity is used
    to obtain similarity scores between them, and afterward, the gradient descent
    is adopted to obtain the final optimal association. All calculations are involved
    in both forward and backward propagation to update CNN features.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于部件和全身的特征：Cheng等人[[49](#bib.bib49)]提出了一种三元组损失函数，其中网络将一组三元组图像作为输入，并从全身和局部部件特征中共同学习，以获得稳健的表示。网络顶部这两种特征的融合被呈现为网络的输出。使用的CNN模型从一个卷积层开始，分为四个相等的部分，每部分形成一个独立的部件通道，旨在从该部件学习特征。这四个部件通道与全身通道构成五个独立的通道，这些通道彼此分开训练（没有参数共享）。在网络顶部，从这五个独立通道获得的输出被串联成一个单一向量，并通过最终的全连接层。Bai等人[[78](#bib.bib78)]提出了一种深度人物模型，用于生成人物身体的全局和部件特征表示。每组三元组图像被输入到一个骨干CNN中，以生成具有共享参数的低级特征。骨干网络的输出特征进一步输入到一个两层双向LSTM中，旨在生成基于部件的特征表示；采用LSTM是因为它具有上下文信息的部件表示的判别能力，处理与序列级人物表示的不对齐。同时，层输出特征也输入到另一个网络分支，进行全局特征学习，包括全局平均池化、全连接和Softmax层。最终，输出特征通过采用另一个网络分支在整个网络训练过程中学习相似性距离。引入了一种连贯且有意识的深度学习方法，能够覆盖整个网络相机[[79](#bib.bib79)]。该方法旨在寻求不同相机间的全局最佳匹配。深度特征在三元组框架下生成全身和部件特征，其中每组三元组图像中的一个样本图像来自一个相机视角，而其他图像来自其他相机视角。一旦生成深度特征，就使用余弦相似度来获得它们之间的相似性得分，然后采用梯度下降法获得最终的最佳关联。所有计算都涉及前向和反向传播，以更新CNN特征。
- en: 'Attribute based models: An attribute-based method is proposed by Chen et al.[[80](#bib.bib80)]
    that uses embedding learning to drive attributes and identity annotations from
    a person’s appearance, whereby two embedding-based CNNs are learned, simultaneously.
    The pre-defined attributes of this work, mainly, rely on pedestrian’s appearance
    in order to extract similar cues between the same person –i.e., if a pedestrian
    wears a red T-shirt and/or a black backpack at the same time. An improved triplet
    loss is used to learn the fusion of them. Due to spatial variations caused by
    pose/ view point changes, the proposed model is robust in terms of the diversity
    on the appearance of pedestrian attributes. A multi-image re-ranking approach
    was proposed[[27](#bib.bib27)] in which an image pool was formed to collect the
    images of each identity. It uses a CNN-based model in a triplet manner, where
    the feature vectors obtained from the network during the re-ranking step, is used
    to compute the similarities between image pools and templates in gallery set.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 属性基模型：Chen 等人提出了一种基于属性的方法[[80](#bib.bib80)]，该方法使用嵌入学习从一个人的外观中驱动属性和身份注释，同时学习两个基于嵌入的
    CNN。该工作的预定义属性主要依赖于行人的外观，以便提取同一人之间的相似线索——例如，如果一个行人同时穿着红色 T 恤和/或黑色背包。采用改进的三元组损失来学习它们的融合。由于姿势/视角变化引起的空间变化，该模型在行人属性外观的多样性方面表现出强大的鲁棒性。提出了一种多图像重新排序方法[[27](#bib.bib27)]，其中形成了一个图像池以收集每个身份的图像。该方法使用基于
    CNN 的三元组模型，其中在重新排序步骤中从网络获得的特征向量用于计算图像池和图库中模板的相似性。
- en: 'Multi-scale Learning: Multi-part and multi-scale approaches are also considered
    in a triplet manner. Liu et al. [[50](#bib.bib50)] proposed a multi-scale triplet
    network by employing a single CNN-based network and two shallow NNs (i.e., to
    produce less invariance and low-level appearance features from images), with shared
    parameters between them. The deep network designed with five convolutional layers,
    five max-pooling layers, two local normalization layers, and three fully-connection
    layers, while each shallow network composed by two convolutional layers followed
    by two pooling layers. The output of each network is further combined at an embedding
    layer in order to generate final feature representation. Wu et al.[[81](#bib.bib81)]
    proposed an attention-multi-scale deep learning technique for joint-learning of
    low- and high-level features. The proposed deep architecture consists of five
    branches in which the first branch of the network used to learn deep features
    via attention block. A triplet and four classification losses adopted to learn
    the global descriptor through the second and third branches, respectively. Furthermore,
    a multi-scale feature learning is applied in the fourth and fifth branches of
    their network.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度学习：多部分和多尺度的方法也以三元组的方式进行考虑。Liu 等人[[50](#bib.bib50)] 提出了一个多尺度三元组网络，通过使用单个基于
    CNN 的网络和两个浅层神经网络（即，从图像中生成较少的不变性和低级外观特征），它们之间共享参数。深层网络设计有五个卷积层、五个最大池化层、两个局部归一化层和三个全连接层，而每个浅层网络由两个卷积层和两个池化层组成。每个网络的输出在嵌入层进一步组合，以生成最终的特征表示。Wu
    等人[[81](#bib.bib81)] 提出了一种注意力多尺度深度学习技术，用于低级和高级特征的联合学习。该深度架构包含五个分支，其中网络的第一个分支用于通过注意力块学习深度特征。采用三元组和四个分类损失，通过第二和第三分支学习全局描述符。此外，多尺度特征学习应用于他们网络的第四和第五分支。
- en: 'Semi-supervised Approach: A novel semi-supervised Deep Attribute Learning approach
    is proposed in[[82](#bib.bib82)], which contains three deep CNN-based networks
    and the whole network is trained with attributes triplet loss. The first network
    is trained on an independent data set to predict the predefined attributes (e.g.,).
    Second network is trained on another data set plus the predicted attributes labels
    from the first sub-network. Finally, the last network is used to distinguished
    attributes and trained on another data sets with individual class labels. The
    proposed method is more reliable on real-world scenario for PReID, in which the
    proposed solution can be performed onto another unknown target dataset.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督方法：提出了一种新颖的半监督深度属性学习方法，详见[[82](#bib.bib82)]，该方法包含三个基于深度卷积神经网络的网络，整个网络通过属性三元组损失进行训练。第一个网络在独立数据集上训练以预测预定义的属性（例如）。第二个网络在另一个数据集上训练，结合第一个子网络的预测属性标签。最后，最后一个网络用于区分属性，并在另一个数据集上通过各个类标签进行训练。该方法在实际场景中对于PReID更为可靠，并且所提出的解决方案可以应用于其他未知目标数据集。
- en: 4 Results and Open Issues
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与开放问题
- en: 'Table 2: Comparison of existing DL models based on Rank-1 recognition rates
    PReID. Type of models (single, pairwise, and triplet) are denoted by $S$, $P$,
    and $T$ and colored by blue, red, and green, respectively. This table is best
    viewed in color.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基于Rank-1识别率PReID的现有深度学习模型比较。模型类型（单一、成对和三元组）由$S$、$P$和$T$表示，并分别用蓝色、红色和绿色标记。此表格最佳以彩色查看。
- en: '|  | Rank-1 Recognition Rate on specific Datasets |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 特定数据集上的Rank-1识别率 |'
- en: '| Ref.# | Year | Model | VIPeR | CUHK01 | CUHK03 | i-LIDS | PRID-2011 | CAVIAR
    | MARS | Market-1501 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Ref.# | 年份 | 模型 | VIPeR | CUHK01 | CUHK03 | i-LIDS | PRID-2011 | CAVIAR |
    MARS | Market-1501 |'
- en: '| Li [[18](#bib.bib18)] | 2014 | ${\color[rgb]{1,0,0}P}$ | – | 20.65 | – |
    – | – | – | – | – |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Li [[18](#bib.bib18)] | 2014 | ${\color[rgb]{1,0,0}P}$ | – | 20.65 | – |
    – | – | – | – | – |'
- en: '| Zhang [[54](#bib.bib54)] | 2014 | ${\color[rgb]{1,0,0}P}$ | 12.50 | – | –
    | – | – | 7.20 | – | – |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[54](#bib.bib54)] | 2014 | ${\color[rgb]{1,0,0}P}$ | 12.50 | – | –
    | – | – | 7.20 | – | – |'
- en: '| Yi [[55](#bib.bib55)] | 2014 | ${\color[rgb]{1,0,0}P}$ | – | 28.23 | – |
    – | – | – | – | – |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Yi [[55](#bib.bib55)] | 2014 | ${\color[rgb]{1,0,0}P}$ | – | 28.23 | – |
    – | – | – | – | – |'
- en: '| Ahmed [[52](#bib.bib52)] | 2015 | ${\color[rgb]{1,0,0}P}$ | 34.81 | 65.00
    | 54.74 | – | – | – | – | – |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Ahmed [[52](#bib.bib52)] | 2015 | ${\color[rgb]{1,0,0}P}$ | 34.81 | 65.00
    | 54.74 | – | – | – | – | – |'
- en: '| Ding [[76](#bib.bib76)] | 2015 | ${\color[rgb]{0,1,0}T}$ | 40.50 | – | –
    | 52.10 | – | – | – | – |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Ding [[76](#bib.bib76)] | 2015 | ${\color[rgb]{0,1,0}T}$ | 40.50 | – | –
    | 52.10 | – | – | – | – |'
- en: '| Zhang [[77](#bib.bib77)] | 2015 | ${\color[rgb]{0,1,0}T}$ | – | – | 18.74
    | – | – | – | – | – |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Zhang [[77](#bib.bib77)] | 2015 | ${\color[rgb]{0,1,0}T}$ | – | – | 18.74
    | – | – | – | – | – |'
- en: '| Shi [[83](#bib.bib83)] | 2015 | ${\color[rgb]{1,0,0}P}$ | 40.91 | 86.59 |
    59.05 | – | – | – | – | – |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Shi [[83](#bib.bib83)] | 2015 | ${\color[rgb]{1,0,0}P}$ | 40.91 | 86.59 |
    59.05 | – | – | – | – | – |'
- en: '| Liu [[66](#bib.bib66)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | 81.40 | 65.65
    | – | – | – | – | 48.24 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Liu [[66](#bib.bib66)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | 81.40 | 65.65
    | – | – | – | – | 48.24 |'
- en: '| Cheng [[49](#bib.bib49)] | 2016 | ${\color[rgb]{0,1,0}T}$ | 47.80 | 53.70
    | – | – | 22.00 | – | – | – |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Cheng [[49](#bib.bib49)] | 2016 | ${\color[rgb]{0,1,0}T}$ | 47.80 | 53.70
    | – | – | 22.00 | – | – | – |'
- en: '| Chen [[70](#bib.bib70)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 52.85 | 57.28
    | – | – | – | 53.60 | – | – |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[70](#bib.bib70)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 52.85 | 57.28
    | – | – | – | 53.60 | – | – |'
- en: '| Wu [[29](#bib.bib29)] | 2016 | ${\color[rgb]{0,0,1}S}$ | 51.06 | 55.51 |
    – | – | 66.62 | – | – | – |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Wu [[29](#bib.bib29)] | 2016 | ${\color[rgb]{0,0,1}S}$ | 51.06 | 55.51 |
    – | – | 66.62 | – | – | – |'
- en: '| Xiao [[46](#bib.bib46)] | 2016 | ${\color[rgb]{0,0,1}S}$ | 38.60 | 66.60
    | 75.30 | 64.60 | 64.00 | – | – | – |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Xiao [[46](#bib.bib46)] | 2016 | ${\color[rgb]{0,0,1}S}$ | 38.60 | 66.60
    | 75.30 | 64.60 | 64.00 | – | – | – |'
- en: '| Wu [[53](#bib.bib53)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | 71.14 | 64.90
    | – | – | – | – | 37.21 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Wu [[53](#bib.bib53)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | 71.14 | 64.90
    | – | – | – | – | 37.21 |'
- en: '| Li [[84](#bib.bib84)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | – | – | – |
    – | – | – | 59.56 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Li [[84](#bib.bib84)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | – | – | – |
    – | – | – | 59.56 |'
- en: '| Shi [[69](#bib.bib69)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 40.91 | 69.00 |
    – | – | – | – | – | – |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Shi [[69](#bib.bib69)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 40.91 | 69.00 |
    – | – | – | – | – | – |'
- en: '| Varior [[64](#bib.bib64)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 42.40 | – |
    57.30 | – | – | – | – | 61.60 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Varior [[64](#bib.bib64)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 42.40 | – |
    57.30 | – | – | – | – | 61.60 |'
- en: '| Wang [[65](#bib.bib65)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 40.51 | 57.02
    | 55.89 | – | – | – | – | – |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Wang [[65](#bib.bib65)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 40.51 | 57.02
    | 55.89 | – | – | – | – | – |'
- en: '| Wang [[56](#bib.bib56)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 29.75 | 58.93
    | 43.36 | – | – | – | – | – |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Wang [[56](#bib.bib56)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 29.75 | 58.93
    | 43.36 | – | – | – | – | – |'
- en: '| Wang [[56](#bib.bib56)] | 2016 | ${\color[rgb]{0,1,0}T}$ | 35.13 | 65.21
    | 51.33 | – | – | – | – | – |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Wang [[56](#bib.bib56)] | 2016 | ${\color[rgb]{0,1,0}T}$ | 35.13 | 65.21
    | 51.33 | – | – | – | – | – |'
- en: '| Franco [[72](#bib.bib72)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | 44.94 |
    63.51 | 62.30 | 53.33 | – | – | – |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Franco [[72](#bib.bib72)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | 44.94 |
    63.51 | 62.30 | 53.33 | – | – | – |'
- en: '| Wu [[31](#bib.bib31)] | 2016 | ${\color[rgb]{0,0,1}S}$ | 44.11 | 67.12 |
    – | – | – | – | – | 48.15 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Wu [[31](#bib.bib31)] | 2016 | ${\color[rgb]{0,0,1}S}$ | 44.11 | 67.12 |
    – | – | – | – | – | 48.15 |'
- en: '| Wang [[57](#bib.bib57)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | 38.28 | 27.92
    | – | – | – | – | – |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Wang [[57](#bib.bib57)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | 38.28 | 27.92
    | – | – | – | – | – |'
- en: '| Su [[82](#bib.bib82)] | 2016 | ${\color[rgb]{0,1,0}T}$ | 43.50 | – | – |
    – | 22.60 | – | – | – |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Su [[82](#bib.bib82)] | 2016 | ${\color[rgb]{0,1,0}T}$ | 43.50 | – | – |
    – | 22.60 | – | – | – |'
- en: '| Mclaughlin [[63](#bib.bib63)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 33.60 |
    – | – | – | – | – | – | – |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Mclaughlin [[63](#bib.bib63)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 33.60 |
    – | – | – | – | – | – | – |'
- en: '| McLaughlin [[85](#bib.bib85)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | – |
    – | 85.00 | 70.00 | – | – | – |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| McLaughlin [[85](#bib.bib85)] | 2016 | ${\color[rgb]{1,0,0}P}$ | – | – |
    – | 85.00 | 70.00 | – | – | – |'
- en: '| Liu [[50](#bib.bib50)] | 2016 | ${\color[rgb]{0,1,0}T}$ | – | – | – | – |
    – | – | – | 55.40 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Liu [[50](#bib.bib50)] | 2016 | ${\color[rgb]{0,1,0}T}$ | – | – | – | – |
    – | – | – | 55.40 |'
- en: '| Iodice [[3](#bib.bib3)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 18.04 | – | –
    | – | – | – | – | – |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Iodice [[3](#bib.bib3)] | 2016 | ${\color[rgb]{1,0,0}P}$ | 18.04 | – | –
    | – | – | – | – | – |'
- en: '| Su [[34](#bib.bib34)] | 2017 | ${\color[rgb]{0,0,1}S}$ | 51.27 | – | 78.29
    | – | – | – | – | 63.14 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Su [[34](#bib.bib34)] | 2017 | ${\color[rgb]{0,0,1}S}$ | 51.27 | – | 78.29
    | – | – | – | – | 63.14 |'
- en: '| Li [[35](#bib.bib35)] | 2017 | ${\color[rgb]{0,0,1}S}$ | 38.08 | – | 74.21
    | – | – | – | 71.77 | 80.31 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Li [[35](#bib.bib35)] | 2017 | ${\color[rgb]{0,0,1}S}$ | 38.08 | – | 74.21
    | – | – | – | 71.77 | 80.31 |'
- en: '| Franco [[73](#bib.bib73)] | 2017 | ${\color[rgb]{1,0,0}P}$ | – | 63.85 |
    63.90 | 55.85 | – | – | – | – |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Franco [[73](#bib.bib73)] | 2017 | ${\color[rgb]{1,0,0}P}$ | – | 63.85 |
    63.90 | 55.85 | – | – | – | – |'
- en: '| Qian [[67](#bib.bib67)] | 2017 | ${\color[rgb]{1,0,0}P}\&amp;{\color[rgb]{0,0,1}S}$
    | 43.30 | 79.01 | 76.87 | 41.00 | 65.00 | – | – | – |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Qian [[67](#bib.bib67)] | 2017 | ${\color[rgb]{1,0,0}P}\&amp;{\color[rgb]{0,0,1}S}$
    | 43.30 | 79.01 | 76.87 | 41.00 | 65.00 | – | – | – |'
- en: '| Zhu [[71](#bib.bib71)] | 2017 | ${\color[rgb]{1,0,0}P}$ | 44.87 | – | – |
    – | – | – | – | – |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Zhu [[71](#bib.bib71)] | 2017 | ${\color[rgb]{1,0,0}P}$ | 44.87 | – | – |
    – | – | – | – | – |'
- en: '| Cheng [[32](#bib.bib32)] | 2017 | ${\color[rgb]{0,1,0}T}$ | – | 70.09 | 84.70
    | – | – | – | – | 83.6 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Cheng [[32](#bib.bib32)] | 2017 | ${\color[rgb]{0,1,0}T}$ | – | 70.09 | 84.70
    | – | – | – | – | 83.6 |'
- en: '| Tao [[58](#bib.bib58)] | 2017 | ${\color[rgb]{1,0,0}P}$ | 46.00 | – | – |
    – | – | – |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Tao [[58](#bib.bib58)] | 2017 | ${\color[rgb]{1,0,0}P}$ | 46.00 | – | – |
    – | – | – |  |  |'
- en: '| Mao [[60](#bib.bib60)] | 2017 | ${\color[rgb]{1,0,0}P}$ | 45.82 | 93.10 |
    85.50 | – | – | – | – | – |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Mao [[60](#bib.bib60)] | 2017 | ${\color[rgb]{1,0,0}P}$ | 45.82 | 93.10 |
    85.50 | – | – | – | – | – |'
- en: '| Lin [[79](#bib.bib79)] | 2017 | ${\color[rgb]{0,1,0}T}$ | – | – | – | – |
    – | – | – | 81.15 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Lin [[79](#bib.bib79)] | 2017 | ${\color[rgb]{0,1,0}T}$ | – | – | – | – |
    – | – | – | 81.15 |'
- en: '| Bai [[78](#bib.bib78)] | 2017 | ${\color[rgb]{0,1,0}T}$ | – | – | 91.50 |
    – | – | – | – | 92.31 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Bai [[78](#bib.bib78)] | 2017 | ${\color[rgb]{0,1,0}T}$ | – | – | 91.50 |
    – | – | – | – | 92.31 |'
- en: '| Chung [[48](#bib.bib48)] | 2017 | ${\color[rgb]{0,1,0}T}$ | – | – | – | 60.00
    | 78.00 | – | – | – |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Chung [[48](#bib.bib48)] | 2017 | ${\color[rgb]{0,1,0}T}$ | – | – | – | 60.00
    | 78.00 | – | – | – |'
- en: '| Chen [[86](#bib.bib86)] | 2017 | ${\color[rgb]{0,0,1}S}$ | 50.30 | 74.50
    | 84.30 | – | – | – | – | 68.70 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[86](#bib.bib86)] | 2017 | ${\color[rgb]{0,0,1}S}$ | 50.30 | 74.50
    | 84.30 | – | – | – | – | 68.70 |'
- en: '| Li [[41](#bib.bib41)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | – | 44.70 |
    26.70 | 49.40 | – | 43.80 | 63.70 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Li [[41](#bib.bib41)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | – | 44.70 |
    26.70 | 49.40 | – | 43.80 | 63.70 |'
- en: '| Chen [[37](#bib.bib37)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | 84.08 | 92.50
    | – | – | – | – | 93.30 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[37](#bib.bib37)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | 84.08 | 92.50
    | – | – | – | – | 93.30 |'
- en: '| Chi [[38](#bib.bib38)] | 2018 | ${\color[rgb]{0,0,1}S}$ | 45.40 | – | – |
    56.40 | 21.00 | – | – | – |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Chi [[38](#bib.bib38)] | 2018 | ${\color[rgb]{0,0,1}S}$ | 45.40 | – | – |
    56.40 | 21.00 | – | – | – |'
- en: '| Sun [[39](#bib.bib39)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | – | – | – |
    – | – | – | 87.05 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Sun [[39](#bib.bib39)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | – | – | – |
    – | – | – | 87.05 |'
- en: '| Wang [[42](#bib.bib42)] | 2018 | ${\color[rgb]{0,0,1}S}$ | 38.50 | – | –
    | – | – | 34.80 | – | 58.20 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Wang [[42](#bib.bib42)] | 2018 | ${\color[rgb]{0,0,1}S}$ | 38.50 | – | –
    | – | – | 34.80 | – | 58.20 |'
- en: '| Xu [[43](#bib.bib43)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | 88.07 | 91.39
    | – | – | – | – | 88.69 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Xu [[43](#bib.bib43)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | 88.07 | 91.39
    | – | – | – | – | 88.69 |'
- en: '| Chen [[36](#bib.bib36)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | – | 86.70
    | – | – | – | – | 88.90 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[36](#bib.bib36)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | – | 86.70
    | – | – | – | – | 88.90 |'
- en: '| Shen [[61](#bib.bib61)] | 2018 | ${\color[rgb]{1,0,0}P}$ | – | – | 94.90
    | – | – | – | – | 82.50 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Shen [[61](#bib.bib61)] | 2018 | ${\color[rgb]{1,0,0}P}$ | – | – | 94.90
    | – | – | – | – | 82.50 |'
- en: '| Huang [[45](#bib.bib45)] | 2018 | ${\color[rgb]{0,0,1}S}$ | 54.65 | 78.83
    | 81.28 | – | – | – | – | 87.96 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Huang [[45](#bib.bib45)] | 2018 | ${\color[rgb]{0,0,1}S}$ | 54.65 | 78.83
    | 81.28 | – | – | – | – | 87.96 |'
- en: '| Shen [[62](#bib.bib62)] | 2018 | ${\color[rgb]{1,0,0}P}$ | – | – | 93.40
    | – | – | – | – | 90.10 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Shen [[62](#bib.bib62)] | 2018 | ${\color[rgb]{1,0,0}P}$ | – | – | 93.40
    | – | – | – | – | 90.10 |'
- en: '| Chen [[80](#bib.bib80)] | 2018 | ${\color[rgb]{0,1,0}T}$ | – | – | 65.00
    | – | – | – | – | – |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[80](#bib.bib80)] | 2018 | ${\color[rgb]{0,1,0}T}$ | – | – | 65.00
    | – | – | – | – | – |'
- en: '| Cheng [[33](#bib.bib33)] | 2018 | ${\color[rgb]{0,1,0}T}$ | – | 70.90 | 84.70
    | – | – | – | – | 83.60 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Cheng [[33](#bib.bib33)] | 2018 | ${\color[rgb]{0,1,0}T}$ | – | 70.90 | 84.70
    | – | – | – | – | 83.60 |'
- en: '| Chen [[87](#bib.bib87)] | 2018 | ${\color[rgb]{1,0,0}P}$ | – | – | 90.20
    | – | – | – | – | 93.50 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[87](#bib.bib87)] | 2018 | ${\color[rgb]{1,0,0}P}$ | – | – | 90.20
    | – | – | – | – | 93.50 |'
- en: '| Li [[88](#bib.bib88)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | – | 44.40 |
    – | – | – | – | 91.20 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Li [[88](#bib.bib88)] | 2018 | ${\color[rgb]{0,0,1}S}$ | – | – | 44.40 |
    – | – | – | – | 91.20 |'
- en: '| Yu [[44](#bib.bib44)] | 2018 | ${\color[rgb]{0,0,1}S}$ | 34.15 | 69.00 |
    45.82 | – | – | – | – | 60.24 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Yu [[44](#bib.bib44)] | 2018 | ${\color[rgb]{0,0,1}S}$ | 34.15 | 69.00 |
    45.82 | – | – | – | – | 60.24 |'
- en: '| Ding [[40](#bib.bib40)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | 42.60 | –
    | – | – | – | – | 86.00 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Ding [[40](#bib.bib40)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | 42.60 | –
    | – | – | – | – | 86.00 |'
- en: '| Yuan [[27](#bib.bib27)] | 2019 | ${\color[rgb]{0,1,0}T}$ | – | – | – | 66.00
    | 81.00 | – | – | – |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Yuan [[27](#bib.bib27)] | 2019 | ${\color[rgb]{0,1,0}T}$ | – | – | – | 66.00
    | 81.00 | – | – | – |'
- en: '| Xiong [[89](#bib.bib89)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | 63.50
    | – | – | – | – | 92.50 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Xiong [[89](#bib.bib89)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | 63.50
    | – | – | – | – | 92.50 |'
- en: '| Zhong [[47](#bib.bib47)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | – | –
    | – | – | – | 89.49 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Zhong [[47](#bib.bib47)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | – | –
    | – | – | – | 89.49 |'
- en: '| Yao [[90](#bib.bib90)] | 2019 | ${\color[rgb]{0,0,1}S}$ | 56.65 | – | 82.75
    | – | – | – | – | 88.02 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Yao [[90](#bib.bib90)] | 2019 | ${\color[rgb]{0,0,1}S}$ | 56.65 | – | 82.75
    | – | – | – | – | 88.02 |'
- en: '| Chen [[91](#bib.bib91)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | – | –
    | – | – | – | 94.50 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Chen [[91](#bib.bib91)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | – | –
    | – | – | – | 94.50 |'
- en: '| Zheng [[92](#bib.bib92)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | 45.88
    | – | – | – | – | 87.33 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Zheng [[92](#bib.bib92)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | 45.88
    | – | – | – | – | 87.33 |'
- en: '| Zheng [[93](#bib.bib93)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | – | 88.00
    | 95.30 | – | 87.20 | – |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Zheng [[93](#bib.bib93)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | – | – | 88.00
    | 95.30 | – | 87.20 | – |'
- en: '| Qian [[68](#bib.bib68)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | 87.55 | 95.84
    | – | – | – | – | 95.34 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Qian [[68](#bib.bib68)] | 2019 | ${\color[rgb]{0,0,1}S}$ | – | 87.55 | 95.84
    | – | – | – | – | 95.34 |'
- en: '| Wu [[81](#bib.bib81)] | 2019 | ${\color[rgb]{0,1,0}T}$ | – | – | 81.00 |
    – | – | – | – | 95.50 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Wu [[81](#bib.bib81)] | 2019 | ${\color[rgb]{0,1,0}T}$ | – | – | 81.00 |
    – | – | – | – | 95.50 |'
- en: Performance Measure To evaluate the performance of a PReID system, the cumulative
    matching characteristic (CMC) curve is typically calculated and demonstrated as
    a standard recognition rate of which the individuals are correctly identified
    within a sorted, ranked list. In other words, a CMC curve is defined as the probability
    that the correct identity is within the first $\mathrm{{}^{\prime}r^{\prime}}$
    ranks, where $\mathrm{r}=1,2,\ldots,n$, and $n$ is the total number of template
    images involved during the testing of a PReID system. By definition, the CMC curve
    increases with $\mathrm{{}^{\prime}r^{\prime}}$, and eventually equals 1 for $\mathrm{r}=n$.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 性能测量 为了评估PReID系统的性能，通常会计算累积匹配特征（CMC）曲线，并将其作为标准识别率来展示，其中个体在排序的排名列表中被正确识别。换句话说，CMC曲线定义为正确身份在前$\mathrm{{}^{\prime}r^{\prime}}$排名中的概率，其中$\mathrm{r}=1,2,\ldots,n$，$n$是测试PReID系统时涉及的模板图像总数。根据定义，CMC曲线随着$\mathrm{{}^{\prime}r^{\prime}}$的增加而增加，并且在$\mathrm{r}=n$时最终等于1。
- en: 'We attempted to collect the original CMC curves presented at each of the previously
    discussed works for the sake of a comprehensive comparison. However, the CMC curves
    of most of those works are not available publicly. We therefore listed in table [2](#S4.T2
    "Table 2 ‣ 4 Results and Open Issues ‣ Survey on Reliable Deep Learning-Based
    Person Re-Identification Models: Are We There Yet?") and compared only the first-rank
    (Rank-1) recognition rate of existing deep PReID techniques since 2014 till date.
    Rank-1 has a higher importance in PReID due to the reason that the system needs
    to recognize the person from the limited hard to recognize available data in the
    first glance. Further, we showed the type of classification models i.e. single,
    pairwise, and triplet denoted in the table by $S$, $P$, and $T$, and colored by
    blue, red, and green, respectively. The global best results among the methods
    are shown in bold. Moreover, Fig. [6](#S4.F6 "Figure 6 ‣ 4 Results and Open Issues
    ‣ Survey on Reliable Deep Learning-Based Person Re-Identification Models: Are
    We There Yet?") demonstrates the Rank-1 recognition accuracy (%) over years per
    data set. In the following, we will discuss and highlight the best methodology
    and combination of training algorithm with loss function and optimizer to attain
    significant performance in PReID.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试收集之前讨论的每项工作中展示的原始 CMC 曲线，以进行全面的比较。然而，大多数这些工作的 CMC 曲线并未公开。因此，我们在表 [2](#S4.T2
    "表 2 ‣ 4 结果和开放问题 ‣ 关于可靠深度学习人脸重识别模型的调查：我们已经到达了吗？") 中列出了现有深度 PReID 技术自 2014 年以来的第一名（Rank-1）识别率，并进行了比较。Rank-1
    在 PReID 中具有更高的重要性，因为系统需要在第一眼就从有限且难以识别的数据中识别出人。进一步地，我们展示了分类模型的类型，即表中用 $S$、$P$ 和
    $T$ 表示的单模型、配对模型和三元组模型，分别用蓝色、红色和绿色标示。表中粗体显示了方法中的全球最佳结果。此外，图 [6](#S4.F6 "图 6 ‣ 4
    结果和开放问题 ‣ 关于可靠深度学习人脸重识别模型的调查：我们已经到达了吗？") 展示了每个数据集多年间的 Rank-1 识别准确率（%）。接下来，我们将讨论并强调最佳的方法以及训练算法与损失函数和优化器的组合，以获得
    PReID 的显著性能。
- en: Best Result per Dataset VIPeR is a small but challenging dataset. Therefore,
    mostly single models are utilized. The best result for VIPeR to date is shown
    by a single model i.e., 56.65\. Whereas, the best result for VIPeR by Pairwise
    and Triplet model is 52.85 and 47.80. CUHK01 and CUHK03 are still challenging
    datasets. The best performance for these datasets is given by Pairwise models
    i.e., 93.10 and 94.90, respectively. Triplet models show second best result for
    these datasets, whereas Single models have reduced performance on these datasets.
    On i-LIDS, the single model shows the best result followed by Pairwise and then
    Triplet i.e., 88.0, 85.0 and 66.0, respectively. A single model showed the best
    result for PRID-2011 followed by Triplet model and then pairwise model i.e., 95.30,
    81.0, and 70.0. CAVIAR is used only three times in which two times the model was
    Pairwise whereas, once it is evaluated over a single model. However, the best
    result is shown by Pairwise. On the other hand, MARS is also used three times,
    and each time it is evaluated with a single model. Market-1501 is the most evaluated
    dataset where the best result is shown by Single, Pairwise, and then Triplet model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集的最佳结果 VIPeR 是一个小而具有挑战性的数据集。因此，主要使用单模型。目前 VIPeR 的最佳结果由单模型给出，即 56.65。相比之下，VIPeR
    的 Pairwise 和 Triplet 模型的最佳结果分别为 52.85 和 47.80。CUHK01 和 CUHK03 仍然是具有挑战性的数据集。这些数据集的最佳表现由
    Pairwise 模型给出，即分别为 93.10 和 94.90。Triplet 模型在这些数据集上显示了第二佳结果，而 Single 模型在这些数据集上的表现有所下降。在
    i-LIDS 上，单模型显示了最佳结果，其次是 Pairwise，然后是 Triplet，分别为 88.0、85.0 和 66.0。单模型在 PRID-2011
    上显示了最佳结果，其次是 Triplet 模型，然后是 Pairwise 模型，即 95.30、81.0 和 70.0。CAVIAR 仅使用了三次，其中两次模型为
    Pairwise，而一次使用了单模型。然而，最佳结果由 Pairwise 模型给出。另一方面，MARS 也使用了三次，每次都是用单模型进行评估。Market-1501
    是被评估次数最多的数据集，其中最佳结果由 Single、Pairwise，然后是 Triplet 模型给出。
- en: Finally, only [[79](#bib.bib79)] has evaluated their methodology on WARD dataset.
    The achieved Rank-1 rate for WARD dataset is $99.71\%$, which shows an ideal and
    almost optimal performance leaving little margin for future research. However,
    still in surveillance, one needs $100\%$ recognition rate to avoid the anomalies.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，只有[[79](#bib.bib79)]评估了他们的方法在 WARD 数据集上的表现。WARD 数据集的 Rank-1 率为 $99.71\%$，显示出理想且几乎最优的性能，为未来的研究留下的余地很小。然而，在监控领域，仍然需要
    $100\%$ 的识别率以避免异常情况。
- en: '![Refer to caption](img/965a267c43c64685f726aa9931e03d1a.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/965a267c43c64685f726aa9931e03d1a.png)'
- en: 'Figure 6: Recognition accuracy on the benchmark data sets over the years.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：多年来在基准数据集上的识别准确率。
- en: 'Comparison All solutions discussed here have trained their model with*SGD*
    with back-propagation algorithm. Majority of these works evaluated their models
    for PReID on CUHK03 (33), Market-1501 (29), VIPeR (27), and CUHK01 (26) datasets.
    Table. [2](#S4.T2 "Table 2 ‣ 4 Results and Open Issues ‣ Survey on Reliable Deep
    Learning-Based Person Re-Identification Models: Are We There Yet?") shows that
    VIPeR dataset is one of the most used in PReID problem since 2014, but it remains
    one of the most challenging datasets. One of the reason is its small size. However,
    future models need to be able to show good result either directly or through transfer
    learning.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '比较：这里讨论的所有解决方案都使用*SGD*和反向传播算法对其模型进行了训练。这些研究中的大多数评估了它们在PReID任务中的模型，数据集包括CUHK03
    (33)、Market-1501 (29)、VIPeR (27) 和CUHK01 (26)。表格[2](#S4.T2 "Table 2 ‣ 4 Results
    and Open Issues ‣ Survey on Reliable Deep Learning-Based Person Re-Identification
    Models: Are We There Yet?")显示，VIPeR数据集自2014年以来是PReID问题中最常用的数据集之一，但它仍然是最具挑战性的数据集之一。原因之一是它的规模较小。然而，未来的模型需要能够通过直接学习或迁移学习展示良好的结果。'
- en: Good performances have been shown in various large models; however, in real
    scenarios, the models need to be fast and effective. Almost in many video surveillance
    system, the concept of processing time is neglected for the sake to achieve higher
    accuracy. However, it should always be taken into consideration since it is very
    costly due to the requirement of powerful computers to run these deep models.
    Efforts have to be made in this regard to make methods more efficient and compatible
    for achieving high performance despite the smaller size of the network [[3](#bib.bib3)].
    The network can be reduced by either reducing the number of layers, number of
    parameters, or introducing a new scheme that has lower connectivity. In [[94](#bib.bib94),
    [95](#bib.bib95)], authors aimed at a trade-off between ranking accuracy and processing
    time by proposing a multi-stage ranking system and showed promising results.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管各种大型模型已经显示出了良好的性能，但在实际应用场景中，模型需要快速且有效。几乎在许多视频监控系统中，为了获得更高的准确性，处理时间的概念被忽略。然而，由于需要强大的计算机来运行这些深度模型，这通常是非常昂贵的，因此处理时间应始终被考虑在内。必须在这方面做出努力，使方法更高效，并能够在网络规模较小的情况下实现高性能[[3](#bib.bib3)]。可以通过减少层数、参数数量或引入具有较低连接性的全新方案来减少网络规模。在[[94](#bib.bib94)，
    [95](#bib.bib95)]中，作者通过提出多阶段排名系统，旨在在排名准确性和处理时间之间找到平衡，并展示了有希望的结果。
- en: 'Limitations and Future Directions: The task of PReID still suffers from the
    lack of training data samples. Although, this problem is addressed with the joint
    help of pairwise Siamese networks and data augmentation, and showed promising
    performances. However, utilizing such technique still has a major limitation to
    be applicable where it brings noises into the original data set that can effect
    the performance of the model in real-world scenarios. Large scale datasets are
    needed to make models more reliable to tackle challenges such as pose and viewpoint
    variations in the images. In ML, a classification problem can be more often adopted
    to the problems with a limited number of classes in which a massive number of
    instances per class are highly demanded. To this end, the existing methods of
    ML, such as artificial neural networks allow solving classification problems with
    the limitations mentioned above. In PReID, some persons and the corresponding
    classes are increasing day by day. However, the number of instances acquired from
    camera networks is minimal. In this manner, PReID cannot be entirely taken in
    a position as a standard classification problem, particularly with DNNs. In contrary
    to a traditional classification problem, metric learning methods, as discussed
    in this paper, can help and overcome the limitation of deep models as an appropriate
    tool for solving PReID problem.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性与未来方向：PReID任务仍然受到训练数据样本不足的困扰。尽管通过成对的Siamese网络和数据增强的联合帮助来解决了这个问题，并展示了有希望的性能，但这种技术仍然存在一个主要的局限性，即其引入噪声到原始数据集中，这可能影响模型在现实世界场景中的表现。需要大规模的数据集来使模型更可靠，以应对图像中的姿态和视角变化等挑战。在机器学习（ML）中，分类问题通常适用于类别数量有限但每个类别实例数量庞大的问题。为此，现有的机器学习方法，如人工神经网络，允许解决具有上述限制的分类问题。在PReID中，某些人员和相应的类别每天都在增加。然而，从摄像头网络获取的实例数量仍然很少。因此，PReID不能完全被视为一个标准分类问题，尤其是在深度神经网络（DNN）中。与传统分类问题相对，本文讨论的度量学习方法可以作为解决PReID问题的合适工具，帮助克服深度模型的局限性。
- en: Many recent application areas such as autonomous-vehicles and aerial vehicles
    [[96](#bib.bib96)], [[97](#bib.bib97)], [[98](#bib.bib98)] use synthetic data
    for training. Till date, no such dataset has been released for PReID problem.
    Using a game engine to generate and release a synthetic dataset for PreID can
    be a possible and viable solution. This could help PReID researchers in training
    the models and than using it in transfer learning over a smaller dataset.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 许多近期应用领域，如自动驾驶车辆和空中车辆[[96](#bib.bib96)], [[97](#bib.bib97)], [[98](#bib.bib98)]，使用合成数据进行训练。迄今为止，尚未发布用于PReID问题的数据集。使用游戏引擎生成并发布合成数据集用于PreID可以是一个可能且可行的解决方案。这将有助于PReID研究人员在训练模型时使用，并在较小的数据集上进行迁移学习。
- en: The technique proposed for image-based PReID data set are not yet applied on
    video-based dataset in order to generate the sequence of target samples. This
    can be considered also as future direction on this research community.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 针对基于图像的PReID数据集提出的技术尚未应用于基于视频的数据集，以生成目标样本序列。这也可以被视为该研究社区的未来方向。
- en: Cross-modality approach is also another hot research topic on PReID. This type
    of methods enables a PReID system to interact with other modalities to obtain
    alternative information about pedestrians. Those information can be further help
    the system to have a better analysis with hight performance accuracy under different
    scenarios. For instance,  [[99](#bib.bib99)] proposes a cross-modality PReID for
    joint learning of thermal and visible domains, and addresses the issue of PReID
    in night-time. In this vein, domain knowledge transfer[[100](#bib.bib100)] is
    another interesting research line for PReID. Developing a system to learn specific
    knowledge (e.g., learning attributes) on a labeled dataset, and further evaluating
    it on an*unseen* data, which can make PReID system to be more deploy-able also
    in PReID open-set scenarios.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模态方法也是PReID的另一个热门研究主题。这种方法使PReID系统能够与其他模态互动，以获取关于行人的替代信息。这些信息可以进一步帮助系统在不同场景下进行更好的分析，从而提高性能准确度。例如，[[99](#bib.bib99)]提出了一种用于热成像和可见光领域联合学习的跨模态PReID，并解决了夜间PReID的问题。在这方面，领域知识迁移[[100](#bib.bib100)]是PReID的另一条有趣的研究路线。开发一个系统在标记数据集上学习特定知识（例如，学习属性），并在*未见*数据上进一步评估，这可以使PReID系统在PReID开放集场景中更具可部署性。
- en: The discussed models in this paper mainly tried to address a short-term scenario
    by considering to camera views. Currently, to deploy a PReID system in a real-world
    over a long-term scenario is a daunting task. Also, a few works have considered
    to train the model in semi- and un-supervised manner. Such way of learning is
    more realistic to deploy a PReID system in a real-world scenario. However, the
    existing semi- and un-supervised methods have shown much weaker performance than
    supervised learning models which highlights that much work can be done in this
    area which can avoid the need of label data or can aid in generating label data.
    To this end, open-set PReID scenario [[101](#bib.bib101)] addressed by a few works,
    particularly none deep learning-based methods. This is a challenging scenario
    which needs also to be considered more in order to accomplish the main goal of
    PReID.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论的模型主要尝试通过考虑相机视角来解决短期场景。当前，在实际长期场景中部署PReID系统是一项艰巨的任务。此外，少数工作考虑了以半监督和无监督的方式训练模型。这种学习方式更符合在实际场景中部署PReID系统的现实。然而，现有的半监督和无监督方法表现出比监督学习模型弱得多的性能，这突显了在这一领域还需做大量工作，以避免对标签数据的需求或有助于生成标签数据。为此，开放集PReID场景[[101](#bib.bib101)]已被少数工作探讨，特别是非深度学习方法。这是一个具有挑战性的场景，需要更多考虑以实现PReID的主要目标。
- en: 'Each model in Table [2](#S4.T2 "Table 2 ‣ 4 Results and Open Issues ‣ Survey
    on Reliable Deep Learning-Based Person Re-Identification Models: Are We There
    Yet?") shows good performance for one or two benchmark datasets, but narrow to
    apply to a realistic scenario of PReID due limitations as pointed above. However,
    out of the 60 models, only one model (i.e., [[93](#bib.bib93)]) shows optimal
    results for more than one database. This highlights the weakness in the current
    models and emphasizes researchers to come up with such models that can show good
    performance on at-least 50% of the available datasets. On the other hand, a possible
    solution in the future research could be to propose specific rules/scenarios for
    combining all the datasets. Besides, rather individually releasing a new dataset,
    it will be good to add the new set of images to the old dataset and than evaluating
    their models. This will help the researchers to evaluate their model over a single
    dataset.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S4.T2 "Table 2 ‣ 4 Results and Open Issues ‣ Survey on Reliable Deep
    Learning-Based Person Re-Identification Models: Are We There Yet?")中的每个模型在一个或两个基准数据集上表现良好，但由于上述限制，应用于实际PReID场景的范围较窄。然而，在60个模型中，只有一个模型（即[[93](#bib.bib93)]）在多个数据库上显示了最佳结果。这突显了当前模型的弱点，并促使研究人员提出能够在至少50%的可用数据集上表现良好的模型。另一方面，未来研究中的一个可能解决方案是提出特定规则/场景来结合所有数据集。此外，除了单独发布新数据集外，将新图像集添加到旧数据集中，并评估其模型将是一个更好的选择。这将帮助研究人员在单一数据集上评估他们的模型。'
- en: An important factor to highlight is that while using DNN-based models, one has
    to take care of the size of the networks. DNNs have large number of parameters
    and the trained model require more disk space. Hence, when using pairwise or triplet
    models, these can result in a much heavier trained model. Eventually, it will
    be hard to store them on small embedded devices with limited memory. Therefore,
    models with fewer parameters and equal or better performance should be considered.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的一个重要因素是，使用基于DNN的模型时，必须注意网络的大小。DNN具有大量的参数，训练后的模型需要更多的磁盘空间。因此，当使用成对或三元组模型时，这些模型可能会导致更重的训练模型。最终，这将使得在内存有限的小型嵌入式设备上存储这些模型变得困难。因此，应该考虑具有较少参数且性能相等或更好的模型。
- en: 5 Final Remarks
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: Person re-identification is a challenging task for an intelligent video surveillance
    system with open application areas in numerous fields. Despite high importance
    it is still facing problems due to the poor performance of models in real-world
    scenarios. In this survey, we summarized recent advances with DNNs for the PReID
    task from 2014 to date. We have shown the type of models by taking into account
    of their implementation details for PReID. Besides, we highlighted all the available
    datasets in this domain. VIPeR dataset is the most challenging and widely used
    dataset available thus far. To handle the issue of lack of data, utilizing synthetic
    data is being proposed as a viable solution. Finally, it is essential to consider
    that besides enhancing the performance of the models, the size of the models (by
    reducing layers or number of parameters in the model) needs to be decreased without
    degrading the overall Rank-1 recognition rate.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 人员再识别是智能视频监控系统中的一个具有挑战性的任务，应用领域广泛。尽管其重要性很高，但由于模型在实际场景中的表现不佳，仍面临许多问题。在本次调查中，我们总结了从2014年至今，使用深度神经网络（DNNs）在人员再识别（PReID）任务中的最新进展。我们展示了考虑到其实现细节的不同模型类型。此外，我们还重点介绍了该领域所有可用的数据集。VIPeR数据集是迄今为止最具挑战性和广泛使用的数据集。为了解决数据不足的问题，利用合成数据被提议作为一个可行的解决方案。最后，除了提升模型的性能外，还必须考虑减少模型的大小（通过减少层数或模型中的参数数量），而不降低整体的Rank-1识别率。
- en: Acknowledgements
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This research was supported by São Paulo Research Foundation (FAPESP), under
    the thematic project ”*DéjàVu: Feature-Space-Time Coherence from Heterogeneous
    Data for Media Integrity Analytics and Interpretation of Events*” with grant number
    18/05668-3.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究得到了圣保罗研究基金会（FAPESP）的资助，资助主题项目为“*DéjàVu: Feature-Space-Time Coherence from
    Heterogeneous Data for Media Integrity Analytics and Interpretation of Events*”，资助编号为18/05668-3。'
- en: References
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Bedagkar.Gala, A., Shah, S.K.: ‘A survey of approaches and trends in person
    re-identification’, *Image and Vision Computing*, 2014, 32, (4), pp. 270–286'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Bedagkar.Gala, A., Shah, S.K.: ‘A survey of approaches and trends in person
    re-identification’, *Image and Vision Computing*, 2014, 32, (4), pp. 270–286'
- en: '[2] Saghafi, M.A., Hussain, A., Zaman, H.B., Saad, M.H.M.: ‘Review of person
    re-identification techniques’, *IET Computer Vision*, 2014, 8, (6), pp. 455–474'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Saghafi, M.A., Hussain, A., Zaman, H.B., Saad, M.H.M. 《人员重识别技术综述》，*IET计算机视觉*，2014年，8卷（6期），页码455–474'
- en: '[3] Iodice, S., Petrosino, A., Ullah, I. ‘Strict pyramidal deep architectures
    for person re-identification’. In: Advances in Neural Networks. (Springer, 2016\.
    pp.  179–186'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Iodice, S., Petrosino, A., Ullah, I. 《严格的金字塔深度架构用于人员重识别》。载于：**《神经网络进展》**。（Springer,
    2016年，页码179–186）'
- en: '[4] Gray, D., Tao, H. ‘Viewpoint invariant pedestrian recognition with an ensemble
    of localized features’. In: European Conference on Computer Vision (ECCV). (Springer,
    2008. pp.  262–275'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Gray, D., Tao, H. 《通过本地化特征集成的视角不变行人识别》。载于：**《欧洲计算机视觉会议（ECCV）》**。（Springer,
    2008年，页码262–275）'
- en: '[5] Farenzena, M., Bazzani, L., Perina, A., Murino, V., Cristani, M. ‘Person
    re-identification by symmetry-driven accumulation of local features’. In: IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR). (IEEE, 2010\. pp. 
    2360–2367'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Farenzena, M., Bazzani, L., Perina, A., Murino, V., Cristani, M. 《通过对称驱动的局部特征累积进行人员重识别》。载于：**IEEE计算机视觉与模式识别会议（CVPR）**。（IEEE,
    2010年，页码2360–2367）'
- en: '[6] Hirzer, M., Roth, P.M., Bischof, H. ‘Person re-identification by efficient
    impostor-based metric learning’. In: The IEEE Ninth International Conference on
    Advanced Video and Signal-Based Surveillance (AVSS). (IEEE, 2012\. pp.  203–208'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Hirzer, M., Roth, P.M., Bischof, H. 《通过高效的冒名顶替者基础度量学习进行人员重识别》。载于：**IEEE第九届国际高级视频与信号基础监控会议（AVSS）**。（IEEE,
    2012年，页码203–208）'
- en: '[7] Ma, B., Su, Y., Jurie, F.: ‘Covariance descriptor based on bio-inspired
    features for person re-identification and face verification’, *Image and Vision
    Computing*, 2014, 32, (6), pp. 379–390'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ma, B., Su, Y., Jurie, F. 《基于生物启发特征的协方差描述符用于人员重识别与面部验证》，*图像与视觉计算*，2014年，32卷（6期），页码379–390'
- en: '[8] LeCun, Y., Kavukcuoglu, K., Farabet, C. ‘Convolutional networks and applications
    in vision’. In: Proceedings of 2010 IEEE International Symposium on Circuits and
    Systems (ISCAS). (IEEE, 2010\. pp.  253–256'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] LeCun, Y., Kavukcuoglu, K., Farabet, C. 《卷积网络及其在视觉中的应用》。载于：**2010 IEEE国际电路与系统研讨会（ISCAS）**。
    （IEEE, 2010年，页码253–256）'
- en: '[9] Krizhevsky, A., Sutskever, I., Hinton, G.E. ‘Imagenet classification with
    deep convolutional neural networks’. In: Advances in neural information processing
    systems. (, 2012\. pp.  1097–1105'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Krizhevsky, A., Sutskever, I., Hinton, G.E. 《使用深度卷积神经网络的Imagenet分类》。载于：**神经信息处理系统进展**。（2012年，页码1097–1105）'
- en: '[10] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al.:
    ‘Imagenet large scale visual recognition challenge’, *International Journal of
    Computer Vision*, 2015, 115, (3), pp. 211–252'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., 等：《Imagenet
    大规模视觉识别挑战》，*国际计算机视觉杂志*，2015年，115卷（3期），页码211–252'
- en: '[11] Wu, D., Zheng, S.J., Zhang, X.P., Yuan, C.A., Cheng, F., Zhao, Y., et al.:
    ‘Deep learning-based methods for person re-identification: A comprehensive review’,
    *Neurocomputing*, 2019,'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Wu, D., Zheng, S.J., Zhang, X.P., Yuan, C.A., Cheng, F., Zhao, Y., 等：《基于深度学习的方法进行人员重识别：综合评述》，*神经计算*，2019年'
- en: '[12] Leng, Q., Ye, M., Tian, Q.: ‘A survey of open-world person re-identification’,
    *IEEE Transactions on Circuits and Systems for Video Technology*, 2019,'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Leng, Q., Ye, M., Tian, Q. 《开放世界人员重识别综述》，*IEEE视频技术电路与系统汇刊*，2019年'
- en: '[13] Branch, H.O.S.D. ‘Imagery library for intelligent detection systems (i-lids)’.
    In: The Institution of Engineering and Technology Conference on Crime and Security,
    2006\. (IET, 2006\. pp.  445–448'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Branch, H.O.S.D. 《用于智能检测系统的图像库（i-lids）》。载于：**工程与技术学会犯罪与安全会议**，2006年。（IET,
    2006年，页码445–448）'
- en: '[14] Ess, A., Leibe, B., Van.Gool, L. ‘Depth and appearance for mobile scene
    analysis’. In: IEEE 11th International Conference on Computer Vision (ICCV). (IEEE,
    2007\. pp.  1–8'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Ess, A., Leibe, B., Van.Gool, L. 《用于移动场景分析的深度与外观》。载于：**IEEE第11届国际计算机视觉会议（ICCV）**。（IEEE,
    2007年，页码1–8）'
- en: '[15] Cheng, D.S., Cristani, M., Stoppa, M., Bazzani, L., Murino, V. ‘Custom
    pictorial structures for re-identification.’. In: Bmvc. vol. 1\. (Citeseer, 2011\.
    p. 6'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Cheng, D.S., Cristani, M., Stoppa, M., Bazzani, L., Murino, V. 《用于重识别的定制图形结构》。载于：**BMVC**。第1卷。（Citeseer,
    2011年，页码6）'
- en: '[16] Li, W., Zhao, R., Wang, X. ‘Human reidentification with transferred metric
    learning’. In: Asian Conference on Computer Vision. (Springer, 2012\. pp.  31–44'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Li, W., Zhao, R., Wang, X. 《通过迁移度量学习进行人体重识别》。载于：**《亚洲计算机视觉会议》**。（Springer,
    2012年，页码31–44）'
- en: '[17] Li, W., Wang, X. ‘Locally aligned feature transforms across views’. In:
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR). (, 2013\. pp.  3594–3601'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Li, W., Wang, X. 《跨视角的局部对齐特征变换》。收录于：IEEE计算机视觉与模式识别会议（CVPR）论文集。（, 2013\.
    页码3594–3601）'
- en: '[18] Li, W., Zhao, R., Xiao, T., Wang, X. ‘Deepreid: Deep filter pairing neural
    network for person re-identification’. In: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR). (, 2014\. pp.  152–159'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Li, W., Zhao, R., Xiao, T., Wang, X. 《Deepreid：用于人员重识别的深度滤波配对神经网络》。收录于：IEEE计算机视觉与模式识别会议（CVPR）论文集。（,
    2014\. 页码152–159）'
- en: '[19] Hirzer, M., Beleznai, C., Roth, P.M., Bischof, H. ‘Person re-identification
    by descriptive and discriminative classification’. In: Image Analysis. (Springer,
    2011\. pp.  91–102'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Hirzer, M., Beleznai, C., Roth, P.M., Bischof, H. 《通过描述性和判别性分类进行人员重识别》。收录于：图像分析。（Springer,
    2011\. 页码91–102）'
- en: '[20] Martinel, N., Micheloni, C. ‘Re-identify people in wide area camera network’.
    In: Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer
    Society Conference on. (IEEE, 2012\. pp.  31–36'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Martinel, N., Micheloni, C. 《在广域摄像头网络中重新识别人员》。收录于：计算机视觉与模式识别研讨会（CVPRW），2012
    IEEE计算机学会会议。（IEEE, 2012\. 页码31–36）'
- en: '[21] Das, A., Chakraborty, A., Roy.Chowdhury, A.K. ‘Consistent re-identification
    in a camera network’. In: European Conference on Computer Vision (ECCV). (Springer,
    2014. pp.  330–345'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Das, A., Chakraborty, A., Roy.Chowdhury, A.K. 《摄像头网络中的一致性重识别》。收录于：欧洲计算机视觉会议（ECCV）。（Springer,
    2014\. 页码330–345）'
- en: '[22] Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q. ‘Scalable
    person re-identification: A benchmark’. In: Proceedings of the IEEE International
    Conference on Computer Vision. (, 2015\. pp.  1116–1124'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q. 《可扩展的人员重识别：一个基准》。收录于：IEEE国际计算机视觉会议论文集。（,
    2015\. 页码1116–1124）'
- en: '[23] Springer. ‘MARS: A Video Benchmark for Large-Scale Person Re-identification’,
    2016'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Springer. 《MARS：用于大规模人员重识别的视频基准》，2016'
- en: '[24] Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C. ‘Performance
    measures and a data set for multi-target, multi-camera tracking’. In: European
    Conference on Computer Vision (ECCV). (Springer, 2016. pp.  17–35'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C. 《多目标、多摄像头跟踪的性能测量和数据集》。收录于：欧洲计算机视觉会议（ECCV）。（Springer,
    2016\. 页码17–35）'
- en: '[25] Wei, L., Zhang, S., Gao, W., Tian, Q. ‘Person trasfer gan to bridge domain
    gap for person re-identification’. In: Computer Vision and Pattern Recognition
    (CVPR), IEEE International Conference on. (, 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Wei, L., Zhang, S., Gao, W., Tian, Q. 《人员转移GAN：为人员重识别弥合领域间隙》。收录于：计算机视觉与模式识别会议（CVPR），IEEE国际会议。（,
    2018）'
- en: '[26] Zheng, M., Karanam, S., Radke, R.J. ‘Rpifield: A new dataset for temporally
    evaluating person re-identification’. In: Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition Workshops. (, 2018\. pp.  1893–1895'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Zheng, M., Karanam, S., Radke, R.J. 《Rpifield：一个用于时间性评估人员重识别的新数据集》。收录于：IEEE计算机视觉与模式识别会议研讨会论文集。（,
    2018\. 页码1893–1895）'
- en: '[27] Yuan, M., Yin, D., Ding, J., Zhou, Z., Zhu, C., Zhang, R., et al.: ‘A
    multi-image joint re-ranking framework with updateable image pool for person re-identification’,
    *Journal of Visual Communication and Image Representation*, 2019, 59, pp. 527–536'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Yuan, M., Yin, D., Ding, J., Zhou, Z., Zhu, C., Zhang, R., 等. 《具有可更新图像池的多图像联合重排序框架用于人员重识别》，*视觉通信与图像表示杂志*，2019,
    59, 页码527–536'
- en: '[28] Cheng, L., Jing, X.Y., Zhu, X., Qi, F., Ma, F., Jia, X., et al. ‘A hybrid
    2d and 3d convolution based recurrent network for video-based person re-identification’.
    In: International Conference on Neural Information Processing. (Springer, 2018\.
    pp.  439–451'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Cheng, L., Jing, X.Y., Zhu, X., Qi, F., Ma, F., Jia, X., 等. 《基于混合2D和3D卷积的递归网络用于视频基础的人员重识别》。收录于：神经信息处理国际会议。（Springer,
    2018\. 页码439–451）'
- en: '[29] Wu, S., Chen, Y.C., Li, X., Wu, A.C., You, J.J., Zheng, W.S. ‘An enhanced
    deep feature representation for person re-identification’. In: IEEE Winter Conference
    on Applications of Computer Vision (WACV). (IEEE, 2016\. pp.  1–8'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Wu, S., Chen, Y.C., Li, X., Wu, A.C., You, J.J., Zheng, W.S. 《增强的深度特征表示用于人员重识别》。收录于：IEEE冬季计算机视觉应用会议（WACV）。（IEEE,
    2016\. 页码1–8）'
- en: '[30] Ma, B., Su, Y., Jurie, F. ‘Local descriptors encoded by fisher vectors
    for person re-identification’. In: European Conference on Computer Vision (ECCV).
    (Springer, 2012. pp.  413–422'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Ma, B., Su, Y., Jurie, F. 《通过Fisher向量编码的局部描述符用于人员重识别》。收录于：欧洲计算机视觉会议（ECCV）。（Springer,
    2012\. 页码413–422）'
- en: '[31] Wu, L., Shen, C., van den Hengel, A.: ‘Deep linear discriminant analysis
    on fisher networks: A hybrid architecture for person re-identification’, *Pattern
    Recognition*, 2017, 65, pp. 238–250'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Wu, L., Shen, C., van den Hengel, A.: ‘在费舍尔网络上的深度线性判别分析：用于人物重识别的混合架构’，
    *模式识别*，2017年，65，第238–250页'
- en: '[32] Cheng, D., Gong, Y., Chang, X., Shi, W., Hauptmann, A., Zheng, N.: ‘Deep
    feature learning via structured graph laplacian embedding for person re-identification’,
    *Pattern Recognition*, 2018, 82, pp. 94–104'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Cheng, D., Gong, Y., Chang, X., Shi, W., Hauptmann, A., Zheng, N.: ‘通过结构化图拉普拉斯嵌入进行深度特征学习用于人物重识别’，
    *模式识别*，2018年，82，第94–104页'
- en: '[33] De, C., Yihong, G., Xiaojun, C., Weiwei, S., Alexander, H., Nanning, Z.:
    ‘Deep feature learning via structured graph laplacian embedding for person re-identification’,
    *Pattern Recognition*, 2018, 82, pp. 94–104'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] De, C., Yihong, G., Xiaojun, C., Weiwei, S., Alexander, H., Nanning, Z.:
    ‘通过结构化图拉普拉斯嵌入进行深度特征学习用于人物重识别’， *模式识别*，2018年，82，第94–104页'
- en: '[34] Su, C., Li, J., Zhang, S., Xing, J., Gao, W., Tian, Q. ‘Pose-driven deep
    convolutional model for person re-identification’. In: IEEE International Conference
    on Computer Vision (ICCV). (IEEE, 2017\. pp.  3980–3989'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Su, C., Li, J., Zhang, S., Xing, J., Gao, W., Tian, Q. ‘基于姿态的深度卷积模型用于人物重识别’。
    见：IEEE国际计算机视觉会议（ICCV）。 (IEEE，2017年，第3980–3989页'
- en: '[35] Li, D., Chen, X., Zhang, Z., Huang, K. ‘Learning deep context-aware features
    over body and latent parts for person re-identification’. In: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (, 2017\. pp. 
    384–393'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Li, D., Chen, X., Zhang, Z., Huang, K. ‘通过深度上下文感知特征学习进行身体和潜在部分的人物重识别’。
    见：IEEE计算机视觉与模式识别会议（CVPR）论文集。 (，2017年，第384–393页'
- en: '[36] Chen, Y., Zhu, X., Gong, S., et al.: ‘Person re-identification by deep
    learning multi-scale representations’, , 2018, pp.  2590–2600'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Chen, Y., Zhu, X., Gong, S., 等.: ‘通过深度学习多尺度表示进行人物重识别’， 2018年，第2590–2600页'
- en: '[37] Chen, D., Li, H., Liu, X., Shen, Y., Shao, J., Yuan, Z., et al. ‘Improving
    deep visual representation for person re-identification by global and local image-language
    association’. In: European Conference on Computer Vision (ECCV). (Springer, 2018.
    pp.  56–73'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Chen, D., Li, H., Liu, X., Shen, Y., Shao, J., Yuan, Z., 等. ‘通过全局和局部图像-语言关联改进深度视觉表示用于人物重识别’。
    见：欧洲计算机视觉会议（ECCV）。 (Springer，2018年，第56–73页'
- en: '[38] Su, C., Yang, F., Zhang, S., Tian, Q., Davis, L.S., Gao, W.: ‘Multi-task
    learning with low rank attribute embedding for multi-camera person re-identification’,
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2018, 40, (5),
    pp. 1167–1181'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Su, C., Yang, F., Zhang, S., Tian, Q., Davis, L.S., Gao, W.: ‘低秩属性嵌入的多任务学习用于多摄像头人物重识别’，
    *IEEE模式分析与机器智能汇刊*，2018年，40，（5），第1167–1181页'
- en: '[39] Sun, C., Jiang, N., Zhang, L., Wang, Y., Wu, W., Zhou, Z. ‘Unified framework
    for joint attribute classification and person re-identification’. In: International
    Conference on Artificial Neural Networks. (Springer, 2018\. pp.  637–647'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Sun, C., Jiang, N., Zhang, L., Wang, Y., Wu, W., Zhou, Z. ‘联合属性分类与人物重识别的统一框架’。
    见：国际人工神经网络会议。 (Springer，2018年，第637–647页'
- en: '[40] Ding, G., Khan, S., Tang, Z., Porikli, F.: ‘Feature mask network for person
    re-identification’, *Pattern Recognition Letters*, 2019,'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Ding, G., Khan, S., Tang, Z., Porikli, F.: ‘用于人物重识别的特征掩码网络’， *模式识别快报*，2019年'
- en: '[41] Li, M., Zhu, X., Gong, S.: ‘Unsupervised person re-identification by deep
    learning tracklet association’, , 2018, pp.  737–753'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Li, M., Zhu, X., Gong, S.: ‘通过深度学习轨迹关联进行无监督的人物重识别’， 2018年，第737–753页'
- en: '[42] Wang, J., Zhu, X., Gong, S., Li, W.: ‘Transferable joint attribute-identity
    deep learning for unsupervised person re-identification’, , 2018, pp.  2275–2284'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Wang, J., Zhu, X., Gong, S., Li, W.: ‘可转移的联合属性-身份深度学习用于无监督的人物重识别’， 2018年，第2275–2284页'
- en: '[43] Xu, J., Zhao, R., Zhu, F., Wang, H., Ouyang, W.: ‘Attention-aware compositional
    network for person re-identification’, , 2018, pp.  2119–2128'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Xu, J., Zhao, R., Zhu, F., Wang, H., Ouyang, W.: ‘关注意识的组合网络用于人物重识别’， 2018年，第2119–2128页'
- en: '[44] Yu, H.X., Wu, A., Zheng, W.S.: ‘Unsupervised person re-identification
    by deep asymmetric metric embedding’, *IEEE transactions on pattern analysis and
    machine intelligence*, 2018,'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Yu, H.X., Wu, A., Zheng, W.S.: ‘通过深度非对称度量嵌入进行无监督的人物重识别’， *IEEE模式分析与机器智能汇刊*，2018年'
- en: '[45] Huang, Y., Xu, J., Wu, Q., Zheng, Z., Zhang, Z., Zhang, J.: ‘Multi-pseudo
    regularized label for generated data in person re-identification’, *IEEE Transactions
    on Image Processing*, 2018,'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Huang, Y., Xu, J., Wu, Q., Zheng, Z., Zhang, Z., Zhang, J.: ‘用于生成数据的多伪正则化标签在人物重识别中的应用’，
    *IEEE图像处理汇刊*，2018年'
- en: '[46] Xiao, T., Li, H., Ouyang, W., Wang, X. ‘Learning deep feature representations
    with domain guided dropout for person re-identification’. In: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (, 2016\. pp. 
    1249–1258'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Xiao, T., Li, H., Ouyang, W., Wang, X. ‘利用领域引导的丢弃法学习深度特征表示用于人员重识别’. 见：IEEE计算机视觉与模式识别会议（CVPR）论文集.
    (, 2016\. 第1249–1258页'
- en: '[47] Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: ‘Camstyle: a novel
    data augmentation method for person re-identification’, *IEEE Transactions on
    Image Processing*, 2019, 28, (3), pp. 1176–1190'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Zhong, Z., Zheng, L., Zheng, Z., Li, S., Yang, Y.: ‘Camstyle：一种用于人员重识别的新型数据增强方法’，*IEEE图像处理汇刊*，2019，28，（3），第1176–1190页'
- en: '[48] Chung, D., Tahboub, K., Delp, E.J. ‘A two stream siamese convolutional
    neural network for person re-identification’. In: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR). (, 2017\. pp.  1983–1991'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Chung, D., Tahboub, K., Delp, E.J. ‘一种用于人员重识别的双流孪生卷积神经网络’. 见：IEEE计算机视觉与模式识别会议（CVPR）论文集.
    (, 2017\. 第1983–1991页'
- en: '[49] Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N. ‘Person re-identification
    by multi-channel parts-based cnn with improved triplet loss function’. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (, 2016\.
    pp.  1335–1344'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N. ‘通过多通道基于部分的cnn与改进的三元组损失函数进行人员重识别’.
    见：IEEE计算机视觉与模式识别会议（CVPR）论文集. (, 2016\. 第1335–1344页'
- en: '[50] Liu, J., Zha, Z.J., Tian, Q., Liu, D., Yao, T., Ling, Q., et al. ‘Multi-scale
    triplet cnn for person re-identification’. In: Proceedings of the 2016 ACM on
    Multimedia Conference. (ACM, 2016\. pp.  192–196'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Liu, J., Zha, Z.J., Tian, Q., Liu, D., Yao, T., Ling, Q., et al. ‘多尺度三元组cnn用于人员重识别’.
    见：2016年ACM多媒体会议论文集.（ACM，2016\. 第192–196页'
- en: '[51] Zheng, L., Yang, Y., Hauptmann, A.G.: ‘Person re-identification: Past,
    present and future’, *arXiv preprint arXiv:161002984*, 2016,'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Zheng, L., Yang, Y., Hauptmann, A.G.: ‘人员重识别：过去、现在与未来’，*arXiv预印本 arXiv:161002984*，2016，'
- en: '[52] Ahmed, E., Jones, M., Marks, T.K. ‘An improved deep learning architecture
    for person re-identification’. In: Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR). (, 2015\. pp.  3908–3916'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Ahmed, E., Jones, M., Marks, T.K. ‘改进的深度学习架构用于人员重识别’. 见：IEEE计算机视觉与模式识别会议（CVPR）论文集.
    (, 2015\. 第3908–3916页'
- en: '[53] Wu, L., Shen, C., Hengel, A.v.d.: ‘Personnet: Person re-identification
    with deep convolutional neural networks’, *arXiv preprint arXiv:160107255*, 2016,'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Wu, L., Shen, C., Hengel, A.v.d.: ‘Personnet：基于深度卷积神经网络的人员重识别’，*arXiv预印本
    arXiv:160107255*，2016，'
- en: '[54] Zhang, G., Kato, J., Wang, Y., Mase, K. ‘People re-identification using
    deep convolutional neural network’. In: International Conference on Computer Vision
    Theory and Applications (VISAPP). vol. 3\. (IEEE, 2014\. pp.  216–223'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Zhang, G., Kato, J., Wang, Y., Mase, K. ‘基于深度卷积神经网络的人员重识别’. 见：计算机视觉理论与应用国际会议（VISAPP）。第3卷.（IEEE，2014\.
    第216–223页'
- en: '[55] Yi, D., Lei, Z., Li, S.Z.: ‘Deep metric learning for practical person
    re-identification’, *arXiv preprint arXiv:14074979*, 2014,'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Yi, D., Lei, Z., Li, S.Z.: ‘用于实际人员重识别的深度度量学习’，*arXiv预印本 arXiv:14074979*，2014，'
- en: '[56] Wang, F., Zuo, W., Lin, L., Zhang, D., Zhang, L. ‘Joint learning of single-image
    and cross-image representations for person re-identification’. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (, 2016\.
    pp.  1288–1296'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Wang, F., Zuo, W., Lin, L., Zhang, D., Zhang, L. ‘单图像与跨图像表示的联合学习用于人员重识别’.
    见：IEEE计算机视觉与模式识别会议（CVPR）论文集. (, 2016\. 第1288–1296页'
- en: '[57] Wang, S., Zhang, C., Duan, L., Wang, L., Wu, S., Chen, L. ‘Person re-identification
    based on deep spatio-temporal features and transfer learning’. In: International
    Joint Conference on Neural Networks (IJCNN). (IEEE, 2016\. pp.  1660–1665'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Wang, S., Zhang, C., Duan, L., Wang, L., Wu, S., Chen, L. ‘基于深度时空特征和迁移学习的人员重识别’.
    见：国际神经网络联合会议（IJCNN）。（IEEE，2016\. 第1660–1665页'
- en: '[58] Tao, D., Guo, Y., Yu, B., Pang, J., Yu, Z.: ‘Deep multi-view feature learning
    for person re-identification’, *IEEE Transactions on Circuits and Systems for
    Video Technology*, 2018, 28, (10), pp. 2657–2666'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Tao, D., Guo, Y., Yu, B., Pang, J., Yu, Z.: ‘深度多视角特征学习用于人员重识别’，*IEEE视频技术电路与系统汇刊*，2018，28，（10），第2657–2666页'
- en: '[59] Liao, S., Hu, Y., Zhu, X., Li, S.Z. ‘Person re-identification by local
    maximal occurrence representation and metric learning’. In: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (, 2015\. pp. 
    2197–2206'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Liao, S., Hu, Y., Zhu, X., Li, S.Z. ‘基于局部最大出现表示和度量学习的行人重识别’。发表于：IEEE计算机视觉与模式识别会议（CVPR）论文集。（2015年，pp.
    2197–2206）'
- en: '[60] Mao, C., Li, Y., Zhang, Z., Zhang, Y., Li, X. ‘Pyramid person matching
    network for person re-identification’. In: Asian Conference on Machine Learning.
    (, 2017\. pp.  487–497'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Mao, C., Li, Y., Zhang, Z., Zhang, Y., Li, X. ‘用于行人重识别的金字塔匹配网络’。发表于：亚洲机器学习会议。（2017年，pp.
    487–497）'
- en: '[61] Shen, Y., Li, H., Xiao, T., Yi, S., Chen, D., Wang, X. ‘Deep group-shuffling
    random walk for person re-identification’. In: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR). (, 2018\. pp.  2265–2274'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Shen, Y., Li, H., Xiao, T., Yi, S., Chen, D., Wang, X. ‘深度组洗牌随机游走用于行人重识别’。发表于：IEEE计算机视觉与模式识别会议（CVPR）论文集。（2018年，pp.
    2265–2274）'
- en: '[62] Shen, Y., Xiao, T., Li, H., Yi, S., Wang, X. ‘End-to-end deep kronecker-product
    matching for person re-identification’. In: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR). (, 2018\. pp.  6886–6895'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Shen, Y., Xiao, T., Li, H., Yi, S., Wang, X. ‘端到端深度克罗内克积匹配用于行人重识别’。发表于：IEEE计算机视觉与模式识别会议（CVPR）论文集。（2018年，pp.
    6886–6895）'
- en: '[63] McLaughlin, N., del Rincon, J.M., Miller, P.: ‘Person re-identification
    using deep convnets with multi-task learning’, *IEEE Transactions on Circuits
    and Systems for Video Technology*, 2017, 27, (3), pp. 525–539'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] McLaughlin, N., del Rincon, J.M., Miller, P.: ‘使用深度卷积网络和多任务学习进行行人重识别’，*IEEE视频技术电路与系统汇刊*，2017年，第27卷，第3期，pp.
    525–539'
- en: '[64] Varior, R.R., Shuai, B., Lu, J., Xu, D., Wang, G. ‘A siamese long short-term
    memory architecture for human re-identification’. In: European Conference on Computer
    Vision (ECCV). (Springer, 2016. pp.  135–153'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Varior, R.R., Shuai, B., Lu, J., Xu, D., Wang, G. ‘用于人类重识别的孪生长短期记忆架构’。发表于：欧洲计算机视觉会议（ECCV）。(Springer,
    2016年，pp. 135–153)'
- en: '[65] Wang, J., Wang, Z., Gao, C., Sang, N., Huang, R.: ‘Deeplist: Learning
    deep features with adaptive listwise constraint for person re-identification’,
    *IEEE Transactions on Circuits and Systems for Video Technology*, 2017, 27, (3),
    pp. 513–524'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Wang, J., Wang, Z., Gao, C., Sang, N., Huang, R.: ‘Deeplist：通过自适应列表约束学习深度特征用于行人重识别’，*IEEE视频技术电路与系统汇刊*，2017年，第27卷，第3期，pp.
    513–524'
- en: '[66] Liu, H., Feng, J., Qi, M., Jiang, J., Yan, S.: ‘End-to-end comparative
    attention networks for person re-identification’, *IEEE Transactions on Image
    Processing*, 2017, 26, (7), pp. 3492–3506'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Liu, H., Feng, J., Qi, M., Jiang, J., Yan, S.: ‘端到端比较注意力网络用于行人重识别’，*IEEE图像处理汇刊*，2017年，第26卷，第7期，pp.
    3492–3506'
- en: '[67] Qian, X., Fu, Y., Jiang, Y.G., Xiang, T., Xue, X.: ‘Multi-scale deep learning
    architectures for person re-identification’, , 2017, pp.  5399–5408'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Qian, X., Fu, Y., Jiang, Y.G., Xiang, T., Xue, X.: ‘用于行人重识别的多尺度深度学习架构’，2017年，pp.
    5399–5408'
- en: '[68] Qian, X., Fu, Y., Xiang, T., Jiang, Y.G., Xue, X.: ‘Leader-based multi-scale
    attention deep architecture for person re-identification’, *IEEE transactions
    on pattern analysis and machine intelligence*, 2019,'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Qian, X., Fu, Y., Xiang, T., Jiang, Y.G., Xue, X.: ‘基于领导者的多尺度注意力深度架构用于行人重识别’，*IEEE模式分析与机器智能汇刊*，2019年'
- en: '[69] Shi, H., Yang, Y., Zhu, X., Liao, S., Lei, Z., Zheng, W., et al. ‘Embedding
    deep metric for person re-identification: A study against large variations’. In:
    European Conference on Computer Vision (ECCV). (Springer, 2016. pp.  732–748'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Shi, H., Yang, Y., Zhu, X., Liao, S., Lei, Z., Zheng, W., 等人 ‘嵌入深度度量用于行人重识别：针对大变化的研究’。发表于：欧洲计算机视觉会议（ECCV）。(Springer,
    2016年，pp. 732–748)'
- en: '[70] Chen, S.Z., Guo, C.C., Lai, J.H.: ‘Deep ranking for person re-identification
    via joint representation learning’, *IEEE Transactions on Image Processing*, 2016,
    25, (5), pp. 2353–2367'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Chen, S.Z., Guo, C.C., Lai, J.H.: ‘通过联合表示学习进行行人重识别的深度排名’，*IEEE图像处理汇刊*，2016年，第25卷，第5期，pp.
    2353–2367'
- en: '[71] Zhu, J., Zeng, H., Liao, S., Lei, Z., Cai, C., Zheng, L.: ‘Deep hybrid
    similarity learning for person re-identification’, *IEEE Transactions on Circuits
    and Systems for Video Technology*, 2018, 28, (11), pp. 3183–3193'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Zhu, J., Zeng, H., Liao, S., Lei, Z., Cai, C., Zheng, L.: ‘深度混合相似性学习用于行人重识别’，*IEEE视频技术电路与系统汇刊*，2018年，第28卷，第11期，pp.
    3183–3193'
- en: '[72] Franco, A., Oliveira, L. ‘A coarse-to-fine deep learning for person re-identification’.
    In: IEEE Winter Conference on Applications of Computer Vision (WACV). (IEEE, 2016\.
    pp.  1–7'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Franco, A., Oliveira, L. ‘粗到细的深度学习用于行人重识别’。发表于：IEEE计算机视觉应用冬季会议（WACV）。(IEEE,
    2016年，pp. 1–7)'
- en: '[73] Alexandre, F., Luciano, O.: ‘Convolutional covariance features: Conception,
    integration and performance in person re-identification’, *Pattern Recognition*,
    2017, 61, pp. 593–609'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Alexandre, F., Luciano, O.: ‘卷积协方差特征：在人员重新识别中的构思、整合和性能’, *模式识别*, 2017,
    61, 第 593–609 页'
- en: '[74] Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., et al.
    ‘Learning fine-grained image similarity with deep ranking’. In: Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition. (, 2014\. pp. 
    1386–1393'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Wang, J., Song, Y., Leung, T., Rosenberg, C., Wang, J., Philbin, J., 等.
    ‘通过深度排名学习细粒度图像相似性’. 见：IEEE 计算机视觉与模式识别会议论文集. (, 2014. 第 1386–1393 页)'
- en: '[75] Schroff, F., Kalenichenko, D., Philbin, J. ‘Facenet: A unified embedding
    for face recognition and clustering’. In: Proceedings of the IEEE conference on
    computer vision and pattern recognition. (, 2015\. pp.  815–823'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Schroff, F., Kalenichenko, D., Philbin, J. ‘Facenet: 用于人脸识别和聚类的统一嵌入’.
    见：IEEE 计算机视觉与模式识别会议论文集. (, 2015. 第 815–823 页)'
- en: '[76] Ding, S., Lin, L., Wang, G., Chao, H.: ‘Deep feature learning with relative
    distance comparison for person re-identification’, *Pattern Recognition*, 2015,
    48, (10), pp. 2993–3003'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Ding, S., Lin, L., Wang, G., Chao, H.: ‘基于相对距离比较的深度特征学习用于人员重新识别’, *模式识别*,
    2015, 48, (10), 第 2993–3003 页'
- en: '[77] Zhang, R., Lin, L., Zhang, R., Zuo, W., Zhang, L.: ‘Bit-scalable deep
    hashing with regularized similarity learning for image retrieval and person re-identification’,
    *IEEE Transactions on Image Processing*, 2015, 24, (12), pp. 4766–4779'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Zhang, R., Lin, L., Zhang, R., Zuo, W., Zhang, L.: ‘带有正则化相似度学习的位可扩展深度哈希用于图像检索和人员重新识别’,
    *IEEE 图像处理期刊*, 2015, 24, (12), 第 4766–4779 页'
- en: '[78] Bai, X., Yang, M., Huang, T., Dou, Z., Yu, R., Xu, Y.: ‘Deep-person: Learning
    discriminative deep features for person re-identification’, *arXiv preprint arXiv:171110658*,
    2017,'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Bai, X., Yang, M., Huang, T., Dou, Z., Yu, R., Xu, Y.: ‘Deep-person: 学习用于人员重新识别的判别性深度特征’,
    *arXiv 预印本 arXiv:171110658*, 2017,'
- en: '[79] Lin, J., Ren, L., Lu, J., Feng, J., Zhou, J. ‘Consistent-aware deep learning
    for person re-identification in a camera network’. In: The IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR). vol. 6\. (, 2017\. pp.  5771–5780'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Lin, J., Ren, L., Lu, J., Feng, J., Zhou, J. ‘在摄像头网络中进行人员重新识别的一致性感知深度学习’.
    见：IEEE 计算机视觉与模式识别会议（CVPR）。第 6 卷. (, 2017. 第 5771–5780 页)'
- en: '[80] Chen, Y., Duffner, S., Stoian, A., Dufour, J.Y., Baskurt, A.: ‘Deep and
    low-level feature based attribute learning for person re-identification’, *Image
    and Vision Computing*, 2018, 79, pp. 25–34'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Chen, Y., Duffner, S., Stoian, A., Dufour, J.Y., Baskurt, A.: ‘基于深度和低级特征的人员重新识别属性学习’,
    *图像与视觉计算*, 2018, 79, 第 25–34 页'
- en: '[81] Wu, D., Wang, C., Wu, Y., Huang, D.S.: ‘Attention deep model with multi-scale
    deep supervision for person re-identification’, *arXiv preprint arXiv:191110335*,
    2019,'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Wu, D., Wang, C., Wu, Y., Huang, D.S.: ‘具有多尺度深度监督的注意力深度模型用于人员重新识别’, *arXiv
    预印本 arXiv:191110335*, 2019,'
- en: '[82] Su, C., Zhang, S., Xing, J., Gao, W., Tian, Q. ‘Deep attributes driven
    multi-camera person re-identification’. In: European Conference on Computer Vision
    (ECCV). (Springer, 2016. pp.  475–491'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Su, C., Zhang, S., Xing, J., Gao, W., Tian, Q. ‘深度属性驱动的多摄像头人员重新识别’. 见：欧洲计算机视觉会议（ECCV）。(Springer,
    2016. 第 475–491 页)'
- en: '[83] Shi, H., Zhu, X., Liao, S., Lei, Z., Yang, Y., Li, S.Z.: ‘Constrained
    deep metric learning for person re-identification’, *arXiv preprint arXiv:151107545*,
    2015,'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Shi, H., Zhu, X., Liao, S., Lei, Z., Yang, Y., Li, S.Z.: ‘约束的深度度量学习用于人员重新识别’,
    *arXiv 预印本 arXiv:151107545*, 2015,'
- en: '[84] Li, S., Liu, X., Liu, W., Ma, H., Zhang, H. ‘A discriminative null space
    based deep learning approach for person re-identification’. In: International
    Conference on Cloud Computing and Intelligence Systems (CCIS). (IEEE, 2016\. pp. 
    480–484'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Li, S., Liu, X., Liu, W., Ma, H., Zhang, H. ‘基于判别空白空间的深度学习方法用于人员重新识别’.
    见：国际云计算与智能系统会议（CCIS）。(IEEE, 2016. 第 480–484 页)'
- en: '[85] McLaughlin, N., del Rincon, J.M., Miller, P. ‘Recurrent convolutional
    network for video-based person re-identification’. In: The IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR). (IEEE, 2016\. pp.  1325–1334'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] McLaughlin, N., del Rincon, J.M., Miller, P. ‘用于视频人员重新识别的递归卷积网络’. 见：IEEE
    计算机视觉与模式识别会议（CVPR）。(IEEE, 2016. 第 1325–1334 页)'
- en: '[86] Chen, Y.C., Zhu, X., Zheng, W.S., Lai, J.H.: ‘Person re-identification
    by camera correlation aware feature augmentation’, *IEEE transactions on pattern
    analysis and machine intelligence*, 2017, 40, (2), pp. 392–408'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Chen, Y.C., Zhu, X., Zheng, W.S., Lai, J.H.: ‘通过摄像头相关感知特征增强进行人员重新识别’,
    *IEEE 模式分析与机器智能期刊*, 2017, 40, (2), 第 392–408 页'
- en: '[87] Chen, D., Xu, D., Li, H., Sebe, N., Wang, X. ‘Group consistent similarity
    learning via deep crf for person re-identification’. In: Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR). (, 2018\. pp.  8649–8658'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Chen, D., Xu, D., Li, H., Sebe, N., Wang, X. ‘通过深度条件随机场的群体一致相似性学习用于人员再识别’.
    In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR). (, 2018\. pp.  8649–8658'
- en: '[88] Li, W., Zhu, X., Gong, S. ‘Harmonious attention network for person re-identification’.
    In: Computer Vision and Pattern Recognition (CVPR). vol. 1\. (, 2018. p. 2'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Li, W., Zhu, X., Gong, S. ‘和谐注意力网络用于人员再识别’. In: Computer Vision and Pattern
    Recognition (CVPR). vol. 1\. (, 2018. p. 2'
- en: '[89] Xiong, F., Xiao, Y., Cao, Z., Gong, K., Fang, Z., Zhou, J.T. ‘Good practices
    on building effective cnn baseline model for person re-identification’. In: Tenth
    International Conference on Graphics and Image Processing (ICGIP 2018). vol. 11069\.
    (International Society for Optics and Photonics, 2019\. p. 110690I'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Xiong, F., Xiao, Y., Cao, Z., Gong, K., Fang, Z., Zhou, J.T. ‘为人员再识别构建有效cnn基线模型的良好实践’.
    In: Tenth International Conference on Graphics and Image Processing (ICGIP 2018).
    vol. 11069\. (国际光学与光子学学会, 2019\. p. 110690I'
- en: '[90] Yao, H., Zhang, S., Hong, R., Zhang, Y., Xu, C., Tian, Q.: ‘Deep representation
    learning with part loss for person re-identification’, *IEEE Transactions on Image
    Processing*, 2019, 28, (6), pp. 2860–2871'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Yao, H., Zhang, S., Hong, R., Zhang, Y., Xu, C., Tian, Q.: ‘部分损失下的深度表征学习用于人员再识别’,
    *IEEE Transactions on Image Processing*, 2019, 28, (6), pp. 2860–2871'
- en: '[91] Chen, B., Zha, Y., Min, W., Yuan, Z. ‘Person re-identification based on
    linear classification margin’. In: IOP Conference Series: Materials Science and
    Engineering. vol. 490\. (IOP Publishing, 2019\. p. 042006'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Chen, B., Zha, Y., Min, W., Yuan, Z. ‘基于线性分类裕度的人员再识别’. In: IOP Conference
    Series: Materials Science and Engineering. vol. 490\. (IOP Publishing, 2019\.
    p. 042006'
- en: '[92] Zheng, L., Huang, Y., Lu, H., Yang, Y.: ‘Pose invariant embedding for
    deep person re-identification’, *IEEE Transactions on Image Processing*, 2019,'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Zheng, L., Huang, Y., Lu, H., Yang, Y.: ‘深度人员再识别的姿势不变嵌入’, *IEEE Transactions
    on Image Processing*, 2019,'
- en: '[93] Zhang, R., Li, J., Sun, H., Ge, Y., Luo, P., Wang, X., et al.: ‘Scan:
    Self-and-collaborative attention network for video person re-identification’,
    *IEEE Transactions on Image Processing*, 2019,'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Zhang, R., Li, J., Sun, H., Ge, Y., Luo, P., Wang, X., et al.: ‘扫描：视频人员再识别的自注意力网络和协作注意力网络’,
    *IEEE Transactions on Image Processing*, 2019,'
- en: '[94] Lavi, B., Fumera, G., Roli, F.: ‘Multi-stage ranking approach for fast
    person re-identification’, *IET Computer Vision*, 2018,'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Lavi, B., Fumera, G., Roli, F.: ‘快速人员再识别的多阶段排名方法’, *IET Computer Vision*,
    2018,'
- en: '[95] Lavi, B., Fumera, G., Roli, F. ‘A multi-stage approach for fast person
    re-identification’. In: Joint IAPR International Workshops on Statistical Techniques
    in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition
    (SSPR). (Springer, 2016\. pp.  63–73'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Lavi, B., Fumera, G., Roli, F. ‘用于快速人员再识别的多阶段方法’. In: Joint IAPR International
    Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural
    and Syntactic Pattern Recognition (SSPR). (Springer, 2016\. pp.  63–73'
- en: '[96] Huang, S., Ramanan, D. ‘Expecting the unexpected: Training detectors for
    unusual pedestrians with adversarial imposters’. In: 2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR). (, 2017\. pp.  4664–4673'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Huang, S., Ramanan, D. ‘期望之外：用于异常行人的检测器训练与对抗冒名顶替者’. In: 2017 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR). (, 2017\. pp.  4664–4673'
- en: '[97] Gaidon, A., Wang, Q., Cabon, Y., Vig, E. ‘Virtual worlds as proxy for
    multi-object tracking analysis’. In: CVPR. (, 2016.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Gaidon, A., Wang, Q., Cabon, Y., Vig, E. ‘虚拟世界作为多目标跟踪分析的代理’. In: CVPR.
    (, 2016.'
- en: '[98] Smyth, D.L., Fennell, J., Abinesh, S., Karimi, N.B., Glavin, F.G., Ullah,
    I., et al. ‘A virtual environment with multi-robot navigation, analytics, and
    decision support for critical incident investigation’. In: IJCAI. (, 2018.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Smyth, D.L., Fennell, J., Abinesh, S., Karimi, N.B., Glavin, F.G., Ullah,
    I., et al. ‘用于关键事件调查的多机器人导航、分析和决策支持的虚拟环境’. In: IJCAI. (, 2018.'
- en: '[99] Ye, M., Wang, Z., Lan, X., Yuen, P.C. ‘Visible thermal person re-identification
    via dual-constrained top-ranking.’. In: IJCAI. (, 2018\. pp.  1092–1099'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Ye, M., Wang, Z., Lan, X., Yuen, P.C. ‘可见热人员再识别通过双约束排名。’. In: IJCAI. (,
    2018\. pp.  1092–1099'
- en: '[100] Narayan, N., Sankaran, N., Setlur, S., Govindaraju, V.: ‘Learning deep
    features for online person tracking using non-overlapping cameras: A survey’,
    *Image and Vision Computing*, 2019, 89, pp. 222–235'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Narayan, N., Sankaran, N., Setlur, S., Govindaraju, V.: ‘通过非重叠摄像机进行在线人员追踪的深度特征学习：一项调查’,
    *Image and Vision Computing*, 2019, 89, pp. 222–235'
- en: '[101] Wang, H., Zhu, X., Xiang, T., Gong, S. ‘Towards unsupervised open-set
    person re-identification’. In: 2016 IEEE International Conference on Image Processing
    (ICIP). (IEEE, 2016\. pp.  769–773'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] 王华，朱晓，向涛，龚斯特。‘迈向无监督的开放集行人重识别’。发表于：2016年IEEE国际图像处理会议（ICIP）。（IEEE，2016年。页769–773）'
