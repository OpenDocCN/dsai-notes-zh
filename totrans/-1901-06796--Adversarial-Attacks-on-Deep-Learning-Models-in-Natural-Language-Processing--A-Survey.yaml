- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1901.06796] Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1901.06796] 对深度学习模型在自然语言处理中的对抗攻击：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1901.06796](https://ar5iv.labs.arxiv.org/html/1901.06796)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1901.06796](https://ar5iv.labs.arxiv.org/html/1901.06796)
- en: 'Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对深度学习模型在自然语言处理中的对抗攻击：综述
- en: Wei Emma Zhang [w.zhang@mq.edu.au](mailto:w.zhang@mq.edu.au) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID identifier") ,  Quan Z. Sheng [michael.sheng@mq.edu.au](mailto:michael.sheng@mq.edu.au)
    [1234-5678-9012](https://orcid.org/1234-5678-9012 "ORCID identifier") ,  Ahoud
    Alhazmi [ahoud.alhazmi@hdr.mq.edu.au](mailto:ahoud.alhazmi@hdr.mq.edu.au) Macquarie
    UniversitySydneyAustraliaNSW 2109  and  Chenliang Li Wuhan UniversityWuhanChina
    [cllee@whu.edu.cn](mailto:cllee@whu.edu.cn)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Wei Emma Zhang [w.zhang@mq.edu.au](mailto:w.zhang@mq.edu.au) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID identifier")， Quan Z. Sheng [michael.sheng@mq.edu.au](mailto:michael.sheng@mq.edu.au)
    [1234-5678-9012](https://orcid.org/1234-5678-9012 "ORCID identifier")， Ahoud Alhazmi
    [ahoud.alhazmi@hdr.mq.edu.au](mailto:ahoud.alhazmi@hdr.mq.edu.au) 麦考瑞大学悉尼澳大利亚NSW
    2109 和 Chenliang Li 武汉大学武汉中国 [cllee@whu.edu.cn](mailto:cllee@whu.edu.cn)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: With the development of high computational devices, deep neural networks (DNNs),
    in recent years, have gained significant popularity in many Artificial Intelligence
    (AI) applications. However, previous efforts have shown that DNNs were vulnerable
    to strategically modified samples, named adversarial examples. These samples are
    generated with some imperceptible perturbations, but can fool the DNNs to give
    false predictions. Inspired by the popularity of generating adversarial examples
    for image DNNs, research efforts on attacking DNNs for textual applications emerges
    in recent years. However, existing perturbation methods for images cannot be directly
    applied to texts as text data is discrete in nature. In this article, we review
    research works that address this difference and generate textual adversarial examples
    on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive
    way and cover all the related information to make the article self-contained.
    Finally, drawing on the reviewed literature, we provide further discussions and
    suggestions on this topic.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着高性能计算设备的发展，近年来**深度神经网络**（DNNs）在许多人工智能（AI）应用中获得了显著的普及。然而，以前的研究表明，DNNs 对经过战略性修改的样本，即对抗样本，存在脆弱性。这些样本通过一些不易察觉的扰动生成，但可以欺骗
    DNNs 给出错误的预测。受到为图像 DNNs 生成对抗样本的热门启发，近年来对攻击 DNNs 在文本应用中的研究逐渐出现。然而，现有的图像扰动方法不能直接应用于文本，因为文本数据本质上是离散的。本文回顾了处理这种差异并在
    DNNs 上生成文本对抗样本的研究工作。我们以全面的方式收集、选择、总结、讨论和分析这些工作，并涵盖所有相关信息以使文章自成体系。最后，基于回顾的文献，我们提供了进一步的讨论和建议。
- en: 'Deep neural networks, adversarial examples, textual data, natural language
    processing^†^†ccs: Computing methodologies Natural language processing^†^†ccs:
    Computing methodologies Neural networks'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '深度神经网络、对抗样本、文本数据、自然语言处理^†^†ccs: 计算方法学 自然语言处理^†^†ccs: 计算方法学 神经网络'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: Deep neural networks (DNNs) are large neural networks whose architecture is
    organized as a series of layers of neurons, each of which serves as the individual
    computing units. Neurons are connected by links with different weights and biases
    and transmit the results of its activation function on its inputs to the neurons
    of the next layer. Deep neural networks try to mimic the biological neural networks
    of human brains to learn and build knowledge from examples. Thus they are shown
    the strengths in dealing with complicated tasks that are not easily to be modelled
    as linear or non-linear problems. Further more, empowered by continuous real-valued
    vector representations (i.e., embeddings) they are good at handling data with
    various modalities, e.g., image, text, video and audio.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）是大型神经网络，其架构组织为一系列神经元层，每层作为独立的计算单元。神经元通过具有不同权重和偏置的连接链接，并将其激活函数在输入上的结果传递到下一层的神经元。深度神经网络试图模拟人脑的生物神经网络，以从示例中学习和建立知识。因此，它们在处理复杂的任务时表现出优势，这些任务不易建模为线性或非线性问题。此外，得益于连续的实值向量表示（即嵌入），它们擅长处理各种模态的数据，例如图像、文本、视频和音频。
- en: 'With the development of high computational devices, deep neural networks, in
    recent years have gained significant popularity in many Artificial Intelligence
    (AI) communities such as Computer Vision (Krizhevsky et al., [2012](#bib.bib67);
    Simonyan and Zisserman, [2015](#bib.bib127)), Natural Language Processing (Kumar
    et al., [2016](#bib.bib68); Bowman et al., [2016](#bib.bib19)), Web Mining (Palangi
    et al., [2016](#bib.bib103); Yang et al., [2015](#bib.bib150)) and Game theory
    (Schuurmans and Zinkevich, [2016](#bib.bib120)). However, the interpretability
    of deep neural networks is still unsatisfactory as they work as black boxes, which
    means it is difficult to get intuitions from what each neuron exactly has learned.
    One of the problems of the poor interpretability is evaluating the robustness
    of deep neural networks. In recent years, research works (Szegedy et al., [2014](#bib.bib133);
    Goodfellow et al., [2015](#bib.bib43)) used small unperceivable perturbations
    to evaluate the robustness of deep neural networks and found that they are not
    robust to these perturbations. Szegedy et al. (Szegedy et al., [2014](#bib.bib133))
    first evaluated the state-of-the-art deep neural networks used for image classification
    with small generated perturbations on the input images. They found that the image
    classifier were fooled with high probability, but human judgment is not affected.
    The perturbed image pixels were named adversarial examples and this notation is
    later used to denote all kinds of perturbed samples in a general manner. As the
    generation of adversarial examples is costly and impractical in (Szegedy et al.,
    [2014](#bib.bib133)), Goodfellow et al. (Goodfellow et al., [2015](#bib.bib43))
    proposed a fast generation method which popularized this research topic (Section
    [3.1](#S3.SS1 "3.1\. Crafting Adversarial Examples: Inspiring Works in Computer
    Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey") provides further discussion on these
    works). Followed their works, many research efforts have been made and the purposes
    of these works can be summarized as: i) evaluating the deep neural networks by
    fooling them with unperceivable perturbations; ii) intentionally changing the
    output of the deep neural networks; and iii) detecting the oversensitivity and
    over-stability points of the deep neural networks and finding solutions to defense
    the attack.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '随着高性能计算设备的发展，近年来深度神经网络在许多人工智能（AI）领域如计算机视觉（Krizhevsky et al., [2012](#bib.bib67);
    Simonyan and Zisserman, [2015](#bib.bib127)）、自然语言处理（Kumar et al., [2016](#bib.bib68);
    Bowman et al., [2016](#bib.bib19)）、网页挖掘（Palangi et al., [2016](#bib.bib103); Yang
    et al., [2015](#bib.bib150)）和博弈论（Schuurmans and Zinkevich, [2016](#bib.bib120)）中获得了显著的关注。然而，深度神经网络的可解释性仍然不尽如人意，因为它们作为黑箱工作，这意味着很难从每个神经元具体学到了什么。可解释性差的问题之一是评估深度神经网络的鲁棒性。近年来，研究工作（Szegedy
    et al., [2014](#bib.bib133); Goodfellow et al., [2015](#bib.bib43)）使用微小的不可感知的扰动来评估深度神经网络的鲁棒性，并发现它们对这些扰动并不鲁棒。Szegedy
    et al.（Szegedy et al., [2014](#bib.bib133)）首次使用输入图像上的小生成扰动来评估用于图像分类的最先进深度神经网络。他们发现图像分类器很容易被欺骗，但人类判断并未受到影响。这些被扰动的图像像素被称为对抗样本，这一术语后来被用来泛指各种扰动样本。由于生成对抗样本的成本高且不切实际（Szegedy
    et al., [2014](#bib.bib133)），Goodfellow et al.（Goodfellow et al., [2015](#bib.bib43)）提出了一种快速生成方法，这一研究主题因此得到了广泛关注（第
    [3.1](#S3.SS1 "3.1\. Crafting Adversarial Examples: Inspiring Works in Computer
    Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey") 节提供了进一步讨论）。在他们的工作之后，许多研究工作得到了展开，这些工作的目的可以总结为：i)
    通过欺骗深度神经网络来评估它们；ii) 有意改变深度神经网络的输出；iii) 检测深度神经网络的过度敏感和过度稳定点，并找到防御攻击的解决方案。'
- en: 'Jia and Liang (Jia and Liang, [2017](#bib.bib56)) are the first to consider
    adversarial example generation (or adversarial attack, we will use these two expressions
    interchangeably hereafter) on deep neural networks for text-based tasks (namely
    textual deep neural networks). Their work quickly gained research attention in
    Natural Language Processing (NLP) community. However, due to intrinsic differences
    between images and textual data, the adversarial attack methods on images cannot
    be directly applied to the latter one. First of all, image data (e.g., pixel values)
    is continuous, but textual data is discrete. Conventionally, we vectorize the
    texts before inputting them into the deep neural networks. Traditional vectoring
    methods include leveraging term frequency and inverse document frequency, and
    one-hot representation (details in Section [3.3](#S3.SS3 "3.3\. Vectorizing Textual
    Inputs and Perturbation Measurements ‣ 3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey")). When applying
    gradient-based adversarial attacks adopted from images on these representations,
    the generated adversarial examples are invalid characters or word sequences (Zhao
    et al., [2017](#bib.bib157)). One solution is to use word embeddings as the input
    of deep neural networks. However, this will also generate words that can not be
    matched with any words in the word embedding space (Gong et al., [2018](#bib.bib40)).
    Secondly, the perturbation of images are small change of pixel values that are
    hard to be perceived by human eyes, thus humans can correctly classify the images,
    showing the poor robustness of deep neural models. But for adversarial attack
    on texts, small perturbations are easily perceptible. For example, replacement
    of characters or words would generate invalid words or syntactically-incorrect
    sentences. Further, it would alter the semantics of the sentence drastically.
    Therefore, the perturbations are easily to be perceived–in this case, even human
    being cannot provide correct predictions.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '贾和梁（贾和梁，[2017](#bib.bib56)）首次考虑了在基于文本的任务（即文本深度神经网络）上生成对抗样本（或对抗攻击，后文中这两个表达将交替使用）。他们的工作迅速引起了自然语言处理（NLP）社区的研究关注。然而，由于图像数据和文本数据之间的内在差异，对图像的对抗攻击方法不能直接应用于文本数据。首先，图像数据（例如，像素值）是连续的，而文本数据是离散的。传统上，我们会在将文本输入深度神经网络之前对其进行向量化。传统的向量化方法包括利用词频和逆文档频率，以及独热表示（详细信息见[3.3](#S3.SS3
    "3.3\. Vectorizing Textual Inputs and Perturbation Measurements ‣ 3\. From Image
    to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")）。当将从图像中采用的基于梯度的对抗攻击应用于这些表示时，生成的对抗样本是无效的字符或词序列（赵等，[2017](#bib.bib157)）。一种解决方案是使用词嵌入作为深度神经网络的输入。然而，这也会生成无法与词嵌入空间中的任何词匹配的词（龚等，[2018](#bib.bib40)）。其次，图像的扰动是像素值的小变化，这些变化难以被人眼察觉，因此人们可以正确地分类图像，这显示了深度神经模型的鲁棒性差。但对于文本的对抗攻击，小扰动很容易被察觉。例如，字符或单词的替换会生成无效的词或语法错误的句子。此外，它会极大地改变句子的语义。因此，扰动很容易被察觉——在这种情况下，即使是人类也无法提供正确的预测。'
- en: To address the aforementioned differences and challenges, many attacking methods
    are proposed since the pioneer work of Jia and Liang (Jia and Liang, [2017](#bib.bib56)).
    Despite the popularity of the topic in NLP community, there is no comprehensive
    review paper that collect and summarize the efforts in this research direction.
    There is a need for this kind of work that helps successive researchers and practitioners
    to have an overview of these methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述差异和挑战，自贾和梁（贾和梁，[2017](#bib.bib56)）的开创性工作以来，提出了许多攻击方法。尽管该主题在NLP社区中很受欢迎，但尚无综合评述论文来收集和总结这一研究方向的努力。这类工作有助于后续研究人员和从业者对这些方法有一个概述，因此是必要的。
- en: Related surveys and the differences to this survey. In (Barreno et al., [2010](#bib.bib11)),
    the authors presented comprehensive review on different classes of attacks and
    defenses against machine learning systems. Specifically, they proposed a taxonomy
    for identifying and analyzing these attacks and applied the attacks on a machine
    learning based application, i.e., a statistical spam filter, to illustrate the
    effectiveness of the attack and defense. This work targeted machine learning algorithms
    rather than neural models. Inspired by (Barreno et al., [2010](#bib.bib11)), the
    authors in (Gilmer et al., [2018](#bib.bib37)) reviewed the defences of adversarial
    attack in the security point of view. The work is not limited to machine learning
    algorithms or neural models, but a generic report about adversarial defenses on
    security related applications. The authors found that existing security related
    defense works lack of clear motivations and explanations on how the attacks are
    related to the real security problems and how the attack and defense are meaningfully
    evaluated. Thus they established a taxonomy of motivations, constraints, and abilities
    for more plausible adversaries. (Biggio and Roli, [2018](#bib.bib16)) provides
    a thorough overview of the evolution of the adversarial attack research over the
    last ten years, and focuses on the research works from computer vision and cyber
    security. The paper covers the works from pioneering non-deep leaning algorithms
    to recent deep learning algorithms. It is also from the security point of view
    to provide detailed analysis on the effect of the attacks and defenses. The authors
    of (Liu et al., [2018](#bib.bib82)) reviewed the same problem in a data-driven
    perspective. They analyzed the attacks and defenses according to the learning
    phases, i.e., the training phase and test phase.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 相关调查与本调查的不同之处。在（Barreno et al., [2010](#bib.bib11)）中，作者对不同类别的攻击和防御进行了全面综述。他们提出了一种分类法来识别和分析这些攻击，并将攻击应用于基于机器学习的应用程序，即统计垃圾邮件过滤器，以说明攻击和防御的有效性。这项工作针对机器学习算法，而不是神经模型。受到（Barreno
    et al., [2010](#bib.bib11)）的启发，（Gilmer et al., [2018](#bib.bib37)）的作者从安全角度回顾了对抗攻击的防御。这项工作不限于机器学习算法或神经模型，而是关于安全相关应用中的对抗防御的通用报告。作者发现现有的安全相关防御工作缺乏明确的动机和解释，即攻击如何与实际安全问题相关联，攻击和防御如何有意义地评估。因此，他们建立了一个动机、约束和能力的分类法，以应对更具可信度的对手。（Biggio
    and Roli, [2018](#bib.bib16)）提供了对过去十年对抗攻击研究的全面概述，重点关注计算机视觉和网络安全的研究工作。论文涵盖了从开创性的非深度学习算法到最近的深度学习算法的工作。它也从安全角度详细分析了攻击和防御的效果。（Liu
    et al., [2018](#bib.bib82)）的作者从数据驱动的角度审视了同样的问题。他们根据学习阶段，即训练阶段和测试阶段，分析了攻击和防御。
- en: Unlike previous works that discuss generally on the attack methods on machine
    learning algorithms, (Yuan et al., [2017](#bib.bib155)) focuses on the adversarial
    examples on deep learning models. It reviews current research efforts on attacking
    various deep neural networks in different applications. The defense methods are
    also extensively surveyed. However, they mainly discussed adversarial examples
    for image classification and object recognition tasks. The work in (Akhtar and
    Mian, [2018](#bib.bib3)) provides a comprehensive review on the adversarial attacks
    on deep learning models used in computer vision tasks. It is an application-driven
    survey that groups the attack methods according to the sub-tasks under computer
    vision area. The article also comprehensively reports the works on the defense
    side, the methods of which are mainly grouped into three categories.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前讨论机器学习算法攻击方法的工作不同，（Yuan et al., [2017](#bib.bib155)）专注于深度学习模型中的对抗样本。它回顾了当前对各种深度神经网络进行攻击的研究工作，并广泛调查了防御方法。然而，他们主要讨论了用于图像分类和物体识别任务的对抗样本。（Akhtar
    and Mian, [2018](#bib.bib3)）的工作提供了对用于计算机视觉任务的深度学习模型上的对抗攻击的全面综述。这是一项以应用为驱动的调查，将攻击方法按计算机视觉领域下的子任务进行分组。文章还全面报告了防御方面的工作，这些方法主要分为三类。
- en: All the mentioned works either target general overview of the attacks and defenses
    on machine learning models or focus on specific domains such as computer vision
    and cyber security. Our work differs with them that we specifically focus on the
    attacks and defenses on textual deep learning models. Furthermore, we provide
    a comprehensive review that covers information from different aspects to make
    this survey self-contained.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有提到的工作要么是针对机器学习模型攻击和防御的一般概述，要么是关注于计算机视觉和网络安全等特定领域。我们的工作不同之处在于，我们专注于文本深度学习模型的攻击和防御。此外，我们提供了一个全面的综述，涵盖了不同方面的信息，使得本次调查具有自洽性。
- en: 'Papers selection. The papers we reviewd in this article are high quality papers
    selected from top NLP and AI conferences, including ACL¹¹1Annual Meeting of the
    Association for Computational Linguistics, COLING²²2International Conference on
    Computational Linguistics, NAACL³³3Annual Conference of the North American Chapter
    of the Association for Computational Linguistics, EMNLP⁴⁴4Empirical Methods in
    Natural Language Processing, ICLR⁵⁵5International Conference on Learning Representations,
    AAAI⁶⁶6AAAI Conference on Artificial Intelligence and IJCAI⁷⁷7International Joint
    Conference on Artificial Intelligence. Other than accepted papers in aforementioned
    conferences, we also consider good papers in e-Print archive⁸⁸8arXiv.org, as it
    reflects the latest research outputs. We select papers from archive with three
    metrics: paper quality, method novelty and the number of citations (optional⁹⁹9As
    the research topic emerges from 2017, we relax the citation number to over five
    if it is published more than one year. If the paper has less than five citations,
    but is very recent and satisfies the other two metrics, we also include it in
    this paper.).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 论文选择。我们在本文中审阅的论文是从顶级NLP和AI会议中精选的高质量论文，包括ACL¹¹1计算语言学协会年会、COLING²²2计算语言学国际会议、NAACL³³3北美计算语言学协会年会、EMNLP⁴⁴4自然语言处理实证方法会议、ICLR⁵⁵5学习表征国际会议、AAAI⁶⁶6AAAI人工智能会议以及IJCAI⁷⁷7国际人工智能联合会议。除了上述会议接受的论文，我们还考虑了e-Print
    archive⁸⁸8arXiv.org中的优秀论文，因为它反映了最新的研究成果。我们从存档中选择论文的三个标准是：论文质量、方法新颖性和引用次数（可选⁹⁹9由于研究主题自2017年开始兴起，我们对引用次数的要求放宽至超过五次，如果发表时间超过一年。如果论文引用少于五次，但非常近期且满足其他两个标准，我们也会将其纳入本文）。
- en: 'Contributions of this survey. The aim of this survey is to provide a comprehensive
    review on the research efforts on generating adversarial examples on textual deep
    neural networks. It is motivated by the drastically increasing attentions on this
    topic. This survey will serve researchers and practitioners who are interested
    in attacking textual deep neural models. More broadly, it can serve as a reference
    on how deep learning is applied in NLP community. We expect that the readers have
    some basic knowledge of the deep neural networks architectures, which are not
    the focus in this article. To summarize, the key contributions of this survey
    are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述的贡献。综述的目的是提供关于生成文本深度神经网络对抗样本研究工作的全面回顾。其动机是对这一主题的关注急剧增加。本综述将服务于对攻击文本深度神经模型感兴趣的研究人员和实践者。更广泛地说，它可以作为深度学习在NLP社区应用的参考。我们期望读者具备一些基本的深度神经网络架构知识，这些内容不在本文的重点范围内。总结来说，本综述的主要贡献包括：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct a comprehensive review for adversarial attacks on textual deep neural
    models and propose different classification schemes to organize the reviewed literature;
    this is the first work of this kind;
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对文本深度神经模型的对抗攻击进行了全面的综述，并提出了不同的分类方案来组织所综述的文献；这是首个此类工作；
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide all related information to make the survey self-contained and thus
    it is easy for readers who have limited NLP knowledge to understand;
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了所有相关信息，以确保综述内容自洽，因此即使对NLP知识有限的读者也容易理解；
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss some open issues, and identify some possible research directions
    in this research field aims to build more robust textual deep learning models
    with the help of adversarial examples.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了一些开放问题，并识别了该研究领域的可能研究方向，旨在通过对抗样本构建更具鲁棒性的文本深度学习模型。
- en: 'The remainder of this paper is organized as follows: We introduce the preliminaries
    for adversarial attacks on deep learning models in Section [2](#S2 "2\. Overview
    of Adversarial Attacks and Deep Learning Techniques in Natural Language Processing
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey") including the taxonomy of adversarial attacks and deep learning models
    used in NLP. In Section [3](#S3 "3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey"), we address
    the difference of attacking image data and textual data and briefly reviewed exemplary
    works for attacking image DNN that inspired their follow-ups in NLP. Section [4](#S4
    "4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey") first presents
    our classification on the literature and then gives a detailed introduction to
    the state of the art. We discuss the defense strategies in Section [5](#S5 "5\.
    Defense ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey") and point out the open issues in Section [6](#S6 "6\. Discussions and
    Open Issues ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey"). Finally, the article is concluded in Section [7](#S7 "7\.
    Conclusion ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：在第[2](#S2 "2\. 综述对抗攻击和深度学习技术在自然语言处理中的应用 ‣ 对抗攻击对深度学习模型的调查")节中，我们介绍了对抗攻击和深度学习模型的预备知识，包括对抗攻击的分类和自然语言处理中的深度学习模型。在第[3](#S3
    "3\. 从图像到文本 ‣ 对抗攻击对深度学习模型的调查")节中，我们讨论了攻击图像数据和文本数据的区别，并简要回顾了攻击图像DNN的典型研究，这些研究启发了其在NLP中的后续工作。在第[4](#S4
    "4\. 攻击NLP中的神经模型：最前沿 ‣ 对抗攻击对深度学习模型的调查")节中，首先介绍了我们对文献的分类，然后详细介绍了最前沿的技术。在第[5](#S5
    "5\. 防御 ‣ 对抗攻击对深度学习模型的调查")节中，我们讨论了防御策略，并在第[6](#S6 "6\. 讨论和开放问题 ‣ 对抗攻击对深度学习模型的调查")节中指出了存在的开放问题。最后，第[7](#S7
    "7\. 结论 ‣ 对抗攻击对深度学习模型的调查")节总结了文章内容。
- en: 2\. Overview of Adversarial Attacks and Deep Learning Techniques in Natural
    Language Processing
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 综述对抗攻击和深度学习技术在自然语言处理中的应用
- en: Before we dive into the details of this survey, we start with an introduction
    to the general taxonomy of adversarial attack on deep learning models. We also
    introduce the deep learning techniques and their applications in natural language
    processing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨本调查的细节之前，我们首先介绍了对深度学习模型的对抗攻击的通用分类。我们还介绍了深度学习技术及其在自然语言处理中的应用。
- en: '2.1\. Adversarial Attacks on Deep Learning Models: The General Taxonomy'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 对抗攻击对深度学习模型的通用分类
- en: In this section, we provide the definitions of adversarial attacks and introduce
    different aspects of the attacks, followed by the measurement of perturbations
    and the evaluation metrics of the effectiveness of the attacks in a general manner
    that applies to any data modality.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了对抗攻击的定义，并介绍了攻击的不同方面，随后测量了扰动并以一般性方式评估了攻击效果的指标，这适用于任何数据模态。
- en: 2.1.1\. Definitions
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 定义
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Deep Neural Network (DNN). A deep neural network (we use DNN and deep learning
    model interchangeably hereafter) can be simply presented as a nonlinear function
    $f_{\theta}:\mathbf{X}\rightarrow\mathbf{Y}$, where $\mathbf{X}$ is the input
    features/attributes, $\mathbf{Y}$ is the output predictions that can be a discrete
    set of classes or a sequence of objects. $\mathbf{\theta}$ represents the DNN
    parameters and are learned via gradient-based back-propagation during the model
    training. Best parameters would be obtained by minimizing the the gap between
    the model’s prediction $f_{\mathbf{\theta}}(\mathbf{X})$ and the correct label
    $\mathbf{Y}$, where the gap is measured by loss function $J(f_{\mathbf{\theta}}(\mathbf{X}),\mathbf{Y})$.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）。深度神经网络（以下简称DNN和深度学习模型可互换使用）可以简单表示为一个非线性函数 $f_{\theta}:\mathbf{X}\rightarrow\mathbf{Y}$，其中
    $\mathbf{X}$ 是输入特征/属性，$\mathbf{Y}$ 是输出预测，可能是离散的类别集合或对象序列。 $\mathbf{\theta}$ 代表DNN的参数，这些参数通过模型训练中的基于梯度的反向传播进行学习。最佳参数是通过最小化模型预测
    $f_{\mathbf{\theta}}(\mathbf{X})$ 与正确标签 $\mathbf{Y}$ 之间的差距来获得的，其中差距由损失函数 $J(f_{\mathbf{\theta}}(\mathbf{X}),\mathbf{Y})$
    测量。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Perturbations. Perturbations are intently created small noises that to be added
    to the original input data examples in test stage, aiming to fool the deep learning
    models.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扰动。扰动是故意创建的小噪声，这些噪声被添加到测试阶段的原始输入数据示例中，旨在欺骗深度学习模型。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adversarial Examples. An adversarial example $\mathbf{x}^{\prime}$ is an example
    created via worst-case perturbation of the input to a deep learning model. An
    ideal DNN would still assign correct class $\mathbf{y}$ (in the case of classification
    task) to $\mathbf{x}^{\prime}$, while a victim DNN would have high confidence
    on wrong prediction of $\mathbf{x}^{\prime}$. $\mathbf{x}^{\prime}$ can be formalized
    as:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗样本。对抗样本 $\mathbf{x}^{\prime}$ 是通过对深度学习模型的输入进行最坏情况扰动而创建的样本。理想的深度神经网络（DNN）仍应将正确的类别
    $\mathbf{y}$（在分类任务中）分配给 $\mathbf{x}^{\prime}$，而受害者 DNN 对 $\mathbf{x}^{\prime}$
    的错误预测将有较高的置信度。$\mathbf{x}^{\prime}$ 可以形式化为：
- en: '| (1) |  | $\displaystyle\mathbf{x}^{\prime}=\mathbf{x}+\mathbf{\eta},f(\mathbf{x})=\mathbf{y},\mathbf{x}\in\mathbf{X}$
    |  |'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle\mathbf{x}^{\prime}=\mathbf{x}+\mathbf{\eta},f(\mathbf{x})=\mathbf{y},\mathbf{x}\in\mathbf{X}$
    |  |'
- en: '|  | $\displaystyle f(\mathbf{x}^{\prime})\neq\mathbf{y}$ |  |'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle f(\mathbf{x}^{\prime})\neq\mathbf{y}$ |  |'
- en: '|  | $\displaystyle\textrm{or }f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime},\mathbf{y}^{\prime}\neq\mathbf{y}$
    |  |'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\textrm{或 }f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime},\mathbf{y}^{\prime}\neq\mathbf{y}$
    |  |'
- en: where $\eta$ is the worst-case perturbation. The goal of the adversarial attack
    can be deviating the label to incorrect one ($f(\mathbf{x}^{\prime})\neq\mathbf{y}$)
    or specified one ($f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime}$).
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\eta$ 是最坏情况下的扰动。对抗攻击的目标可以是将标签偏离为错误的标签 ($f(\mathbf{x}^{\prime})\neq\mathbf{y}$)
    或指定的标签 ($f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime}$)。
- en: 2.1.2\. Threat Model
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 威胁模型
- en: We adopt the definition of Threat Model for attacking DNN from (Yuan et al.,
    [2017](#bib.bib155)). In this section, we discuss several aspects of the threat
    model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了 (Yuan et al., [2017](#bib.bib155)) 中对攻击 DNN 的威胁模型定义。在本节中，我们讨论威胁模型的几个方面。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model Knowledge. The adversarial examples can be generated using black-box or
    white-box strategies in terms of the knowledge of the attacked DNN. Black-box
    attack is performed when the architectures, parameters, loss function, activation
    functions and training data of the DNN are not accessible. Adversarial examples
    are generated by directly accessing the test dataset, or by querying the DNN and
    checking the output change. On the contrary, white-box attack is based on the
    knowledge of certain aforementioned information of DNN.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型知识。对抗样本可以使用黑箱或白箱策略生成，这取决于攻击的 DNN 的知识。黑箱攻击是在 DNN 的架构、参数、损失函数、激活函数和训练数据不可访问的情况下进行的。对抗样本是通过直接访问测试数据集，或通过查询
    DNN 并检查输出变化来生成的。相反，白箱攻击则基于对 DNN 的上述某些信息的知识。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Target. The generated adversarial examples can change the output prediction
    to be incorrect or to specific result as shown in Eq. ([1](#S2.E1 "In 3rd item
    ‣ 2.1.1\. Definitions ‣ 2.1\. Adversarial Attacks on Deep Learning Models: The
    General Taxonomy ‣ 2\. Overview of Adversarial Attacks and Deep Learning Techniques
    in Natural Language Processing ‣ Adversarial Attacks on Deep Learning Models in
    Natural Language Processing: A Survey")). Compared to the un-targeted attack ($f(\mathbf{x}^{\prime})\neq\mathbf{y}$),
    targeted attack ($f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime}$) is more strict
    as it not only changes the prediction, but also enforces constraint on the output
    to generate specified prediction. For binary tasks, e.g., binary classification,
    un-targeted attack equals to the targeted attack.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '目标。生成的对抗样本可以将输出预测更改为错误的结果或特定的结果，如方程式 ([1](#S2.E1 "In 3rd item ‣ 2.1.1\. Definitions
    ‣ 2.1\. Adversarial Attacks on Deep Learning Models: The General Taxonomy ‣ 2\.
    Overview of Adversarial Attacks and Deep Learning Techniques in Natural Language
    Processing ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")) 所示。与无目标攻击 ($f(\mathbf{x}^{\prime})\neq\mathbf{y}$) 相比，目标攻击 ($f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime}$)
    更为严格，因为它不仅更改了预测结果，还对输出施加了约束以生成指定的预测。对于二分类任务，例如，二分类，无目标攻击等同于目标攻击。'
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Granularity. The attack granularity refers to the level of data on which the
    adversarial examples are generated from. For example, it is usually the image
    pixels for image data. Regarding the textual data, it could be character, word,
    and sentence-level embedding. Section [3.3](#S3.SS3 "3.3\. Vectorizing Textual
    Inputs and Perturbation Measurements ‣ 3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey") will give further
    introduction on attack granularity for textual DNN.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '颗粒度。攻击颗粒度指的是生成对抗样本的数据层级。例如，对于图像数据，通常是图像像素。对于文本数据，可以是字符、词汇和句子级别的嵌入。第[3.3](#S3.SS3
    "3.3\. Vectorizing Textual Inputs and Perturbation Measurements ‣ 3\. From Image
    to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")节将进一步介绍文本DNN的攻击颗粒度。'
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Motivation. Generating adversarial examples is motivated by two goals: attack
    and defense. The attack aims to examine the robustness of the target DNN, while
    the defense takes a step further utilizing generated adversarial examples to robustify
    the target DNN. Section [5](#S5 "5\. Defense ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey") will give more details.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '动机。生成对抗样本的动机有两个目标：攻击和防御。攻击旨在检查目标DNN的鲁棒性，而防御则进一步利用生成的对抗样本来增强目标DNN的鲁棒性。第[5](#S5
    "5\. Defense ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey")节将提供更多细节。'
- en: 2.1.3\. Measurement
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 测量
- en: Two groups of measurements are required in the adversarial attack for i) controlling
    the perturbations and ii) evaluating the effectiveness of the attack, respectively.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗攻击中，需要两组测量：i) 控制扰动和 ii) 评估攻击效果。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Perturbation Constraint. As aforementioned, the perturbation $\eta$ should
    not change the true class label of the input - that is, an ideal DNN classifier,
    if we take classification as example, will provide the same prediction on the
    adversarial example to the original example. $\eta$ cannot be too small as well,
    to avoid ending up with no affect on target DNNs. Ideally, effective perturbation
    is the maximum value in a constrained range. (Szegedy et al., [2014](#bib.bib133))
    firstly put a constraint that $(\mathbf{x}+\mathbf{\eta})\in[0,1]^{n}$ for image
    adversarial examples, ensuring the adversarial example has the same range of pixel
    values as the original data (Warde-Farley and Goodfellow, [2016](#bib.bib144)).
    (Goodfellow et al., [2015](#bib.bib43)) simplifies the solution and use max norm
    to constrain $\mathbf{\eta}$: $||\mathbf{\eta}||_{\infty}\leq\epsilon$. This was
    inspired by the intuitive observation that a perturbation which does not change
    any specific pixel by more than some amount $\epsilon$ cannot change the output
    class (Warde-Farley and Goodfellow, [2016](#bib.bib144)). Using max-norm is sufficient
    enough for image classification/object recognition tasks. Later on, other norms,
    e.g., $L_{2}$ and $L_{0}$, were used to control the perturbation in attacking
    DNN in computer vision. Constraining $\mathbf{\eta}$ for textual adversarial attack
    is somehow different. Section [3.3](#S3.SS3 "3.3\. Vectorizing Textual Inputs
    and Perturbation Measurements ‣ 3\. From Image to Text ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey") will give more
    details.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '扰动约束。如前所述，扰动$\eta$不应改变输入的真实类别标签——也就是说，如果我们以分类为例，理想的DNN分类器会对对抗样本和原始样本给出相同的预测。$\eta$也不能太小，以避免对目标DNN没有影响。理想情况下，有效的扰动是受限范围内的最大值。（Szegedy等人，[2014](#bib.bib133)）首次对图像对抗样本提出了约束$(\mathbf{x}+\mathbf{\eta})\in[0,1]^{n}$，确保对抗样本具有与原始数据相同的像素值范围（Warde-Farley和Goodfellow，[2016](#bib.bib144)）。(Goodfellow等人，[2015](#bib.bib43))简化了这一解决方案，使用最大范数来约束$\mathbf{\eta}$：$||\mathbf{\eta}||_{\infty}\leq\epsilon$。这源于一个直观观察：如果扰动没有改变任何特定像素超过某个量$\epsilon$，则无法改变输出类别（Warde-Farley和Goodfellow，[2016](#bib.bib144)）。使用最大范数足以应对图像分类/对象识别任务。随后，其他范数，例如$L_{2}$和$L_{0}$，被用于控制计算机视觉中DNN的扰动。对文本对抗攻击的$\mathbf{\eta}$约束有些不同。第[3.3](#S3.SS3
    "3.3\. Vectorizing Textual Inputs and Perturbation Measurements ‣ 3\. From Image
    to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")节将提供更多细节。'
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Attack Evaluation. Adversarial attacks are designed to degrade the performance
    of DNNs. Therefore, evaluating the effectiveness of the attack is based on the
    performance metrics of different tasks. For example, classification tasks has
    metrics such as accuracy, F1 score and AUC score. We leave the metrics for different
    NLP as out-of-scope content in this article and suggest readers refer to specific
    tasks for information.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 攻击评估。对抗性攻击旨在降低深度神经网络（DNN）的性能。因此，评估攻击效果是基于不同任务的性能指标。例如，分类任务有如准确率、F1 分数和 AUC 分数等指标。我们将不同
    NLP 任务的指标作为本文的范围之外内容，并建议读者参考具体任务的信息。
- en: 2.2\. Deep Learning in NLP
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 自然语言处理中的深度学习
- en: Neural networks have been gaining increasing popularity in NLP community in
    recent years and various DNN models have been adopted in different NLP tasks.
    Apart from the feed forward neural networks and Convolutional Neural Networks
    (CNN), Recurrent/Recursive Neural Networks (RNN) and their variants are the most
    common neural networks used in NLP, because of their natural ability of handling
    sequences. In recent years, two important breakthroughs in deep learning are brought
    into NLP. They are sequence-to-sequence learning (Sutskever et al., [2014](#bib.bib132))
    and attention modeling (Bahdanau et al., [2014](#bib.bib9)). Reinforcement learning
    and generative models are also gained much popularity (Young et al., [2018](#bib.bib153)).
    In this section, we will briefly overview the DNN architectures and techniques
    applied in NLP that are closely related to this survey. We suggest readers refer
    to detailed reviews of neural networks in NLP in (Otter et al., [2018](#bib.bib102);
    Young et al., [2018](#bib.bib153)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，神经网络在自然语言处理（NLP）社区中越来越受欢迎，各种深度神经网络（DNN）模型已被应用于不同的 NLP 任务。除了前馈神经网络和卷积神经网络（CNN）之外，递归/递归神经网络（RNN）及其变体是
    NLP 中最常用的神经网络，因为它们在处理序列方面具有天然的能力。近年来，深度学习的两个重要突破被引入到 NLP 中。它们是序列到序列学习（Sutskever
    等，[2014](#bib.bib132)）和注意力建模（Bahdanau 等，[2014](#bib.bib9)）。强化学习和生成模型也获得了广泛关注（Young
    等，[2018](#bib.bib153)）。在本节中，我们将简要概述与本调查密切相关的 NLP 中应用的 DNN 架构和技术。我们建议读者参考有关 NLP
    神经网络的详细综述（Otter 等，[2018](#bib.bib102)；Young 等，[2018](#bib.bib153)）。
- en: 2.2.1\. Feed Forward Networks
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 前馈网络
- en: Feed-forward network, in particular multi-layer perceptrons (MLP), is the simplest
    neural network. It has several forward layers and each node in a layer connects
    to each node in the following layer, making the network fully connected. MLP utilizes
    nonlinear activation function to distinguish data that is not linearly separable.
    MLP works with fixed-sized inputs and do not record the order of the elements.
    Thus it is mostly used in the tasks that can be formed as supervised learning
    problems. In NLP, it can be used in any application. The major drawback of feed
    forward networks in NLP is that it cannot handle well the text sequences in which
    the word order matters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络，特别是多层感知器（MLP），是最简单的神经网络。它具有若干前馈层，每一层的每个节点都连接到下一层的每个节点，使得网络完全连接。MLP 利用非线性激活函数来区分不可线性分离的数据。MLP
    处理固定大小的输入，不记录元素的顺序。因此，它主要用于可以形成监督学习问题的任务。在 NLP 中，它可以用于任何应用。前馈网络在 NLP 中的主要缺点是无法很好地处理词序重要的文本序列。
- en: As the feed forward network is easy to implement, there are various implementations
    and no standard benchmark architecture worth examining. To evaluate the robustness
    of feed forward network in NLP, adversarial examples are often generated for specific
    architectures in real applications. For example, authors of (Grosse et al., [2016](#bib.bib46),
    [2017](#bib.bib47); Al-Dujaili et al., [2018a](#bib.bib4)) worked on the specified
    malware detection models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前馈网络易于实现，因此存在各种实现方式，并且没有值得研究的标准基准架构。为了评估前馈网络在 NLP 中的鲁棒性，通常会为特定架构生成对抗样本以用于实际应用。例如，(Grosse
    等，[2016](#bib.bib46)，[2017](#bib.bib47)；Al-Dujaili 等，[2018a](#bib.bib4)) 的作者研究了特定的恶意软件检测模型。
- en: 2.2.2\. Convolutional Neural Network (CNN)
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 卷积神经网络（CNN）
- en: Convolutional Neural Network contains convolutional layers and pooling (down-sampling)
    layers and final fully-connected layer. Activation functions are used to connect
    the down-sampled layer to the next convolutional layer or fully-connected layer.
    CNN allows arbitrarily-sized inputs. Convolutional layer uses convolution operation
    to extract meaningful local patterns of input. Pooling layer reduces the parameters
    and computation in the network and it allows the network to be deeper and less-overfitting.
    Overall, CNN identifies local predictors and combines them together to generate
    a fixed-sized vector for the inputs, which contains the most or important informative
    aspects for the application task. In addition, it is order-sensitive. Therefore,
    it excels in computer vision tasks and later is widely adopted in NLP applications.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络包含卷积层和池化（下采样）层以及最终的全连接层。激活函数用于将下采样层连接到下一个卷积层或全连接层。CNN 允许任意大小的输入。卷积层使用卷积操作来提取输入的有意义的局部模式。池化层减少了网络中的参数和计算量，使得网络可以更深且过拟合更少。总体而言，CNN
    识别局部预测因子并将其结合在一起，生成一个固定大小的向量，该向量包含应用任务中最重要的信息方面。此外，它对顺序敏感。因此，它在计算机视觉任务中表现出色，后来被广泛应用于
    NLP 应用。
- en: Yoon Kim (Kim, [2014](#bib.bib61)) adopted CNN for sentence classification.
    He used Word2Vec to represent words as input. Then the convolutional operation
    is limited to the direction of word sequence, rather than the word embeddings.
    Multiple filters in pooling layers deal with the variable length of sentences.
    The model demonstrated excellent performances on several benchmark datasets against
    multiple state-of-the-art works. This work became a benchmark work of adopting
    CNN in NLP applications. Zhang et al. (Zhang et al., [2015](#bib.bib156)) presented
    CNN for text classification at character level. They used one-hot representation
    in alphabet for each of the character. To control the generalization error of
    the proposed CNN, they additionally performed data augmentation by replacing words
    and phrases with their synonyms. These two representative textual CNNs are evaluated
    via adversarial examples in many applications (Liang et al., [2017](#bib.bib79);
    Gao et al., [2018](#bib.bib36); Ebrahimi et al., [2018](#bib.bib32); Belinkov
    and Bisk, [2018](#bib.bib14); Ebrahimi et al., [[n. d.]](#bib.bib31)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Yoon Kim (Kim, [2014](#bib.bib61)) 采用了 CNN 来进行句子分类。他使用了 Word2Vec 将单词表示为输入。然后，卷积操作仅限于单词序列的方向，而不是单词嵌入。池化层中的多个滤波器处理句子的可变长度。该模型在多个基准数据集上展示了出色的性能，超越了多个最先进的工作。这项工作成为了在
    NLP 应用中采用 CNN 的基准工作。Zhang 等人（Zhang et al., [2015](#bib.bib156)）提出了用于字符级文本分类的 CNN。他们为每个字符使用了字母表中的
    one-hot 表示。为了控制所提出的 CNN 的泛化误差，他们还通过用同义词替换单词和短语来进行数据增强。这两个代表性的文本 CNN 在许多应用中通过对抗性示例进行了评估（Liang
    et al., [2017](#bib.bib79)；Gao et al., [2018](#bib.bib36)；Ebrahimi et al., [2018](#bib.bib32)；Belinkov
    and Bisk, [2018](#bib.bib14)；Ebrahimi et al., [[n. d.]](#bib.bib31)）。
- en: 2.2.3\. Recurrent Neural Networks/ Recursive Neural Networks
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. 循环神经网络/递归神经网络
- en: Recurrent Neural Networks are neural models adapted from feed-forward neural
    networks for learning mappings between sequential inputs and outputs (Rumelhart
    et al., [1986](#bib.bib117)). RNNs allows data with arbitrary length and it introduces
    cycles in their computational graph to model efficiently the influence of time
    (Goodfellow et al., [2016](#bib.bib41)). The model does not suffer from statistical
    estimation problems stemming from data sparsity and thus leads to impressive performance
    in dealing with sequential data (Goldberg, [2017](#bib.bib38)). Recursive neural
    networks (Goller and Kuchler, [1996](#bib.bib39)) extends recurrent neural networks
    from sequences to tree, which respects the hierarchy of the language. In some
    situations, backwards dependencies exist, which is in need for the backward analysis.
    Bi-directional RNN thus was proposed for looking at sentences in both directions,
    forwards and backwards, using two parallel RNN networks, and combining their outputs.
    Bengio et al. (Bengio et al., [2003](#bib.bib15)) is one of the first to apply
    RNN in NLP. Specifically, they utilized RNN in language model, where the probability
    of a sequence of words is computed in an recurrent manner. The input to RNN is
    the feature vectors for all the preceding words, and the output is the conditional
    probability distribution over the output vocabulary. Since RNN is a natural choice
    to model various kinds of sequential data, it has been applied to many NLP tasks.
    Hence RNN has drawn great interest for adversarial attack (Papernot et al., [2016b](#bib.bib105)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是从前馈神经网络中改编而来的神经模型，用于学习序列输入和输出之间的映射（Rumelhart 等，[1986](#bib.bib117)）。RNN
    允许处理任意长度的数据，并在其计算图中引入循环，以有效地建模时间的影响（Goodfellow 等，[2016](#bib.bib41)）。该模型不会受到数据稀疏带来的统计估计问题，从而在处理序列数据时表现出色（Goldberg，[2017](#bib.bib38)）。递归神经网络（Goller
    和 Kuchler，[1996](#bib.bib39)）将循环神经网络从序列扩展到树形结构，尊重语言的层次结构。在某些情况下，存在向后的依赖关系，这需要进行反向分析。因此，双向
    RNN 被提出用于同时从两个方向（前向和后向）查看句子，使用两个并行的 RNN 网络，并结合它们的输出。Bengio 等（Bengio 等，[2003](#bib.bib15)）是最早将
    RNN 应用于自然语言处理（NLP）领域的研究者之一。他们具体利用 RNN 进行语言建模，其中单词序列的概率以递归方式计算。RNN 的输入是所有前面单词的特征向量，输出是对输出词汇的条件概率分布。由于
    RNN 是建模各种序列数据的自然选择，它已被应用于许多 NLP 任务。因此，RNN 引起了对对抗攻击的极大兴趣（Papernot 等，[2016b](#bib.bib105)）。
- en: RNN has many variants, among which Long Short-Term Memory (LSTM) network (Hochreiter
    and Schmidhuber, [1997](#bib.bib52)) gains the most popularity. LSTM is a specific
    RNN that was designed to capture the long-term dependencies. In LSTM, the hidden
    state are computed through combination of three gates, i.e., input gate, forget
    gate and output gate, that control information flow drawing on the logistic function.
    LSTM networks have subsequently proved to be more effective than conventional
    RNNs (Graves et al., [2013](#bib.bib45)). GRUs is a simplified version of LSTM
    that it only consists two gates, thus it is more efficient in terms of training
    and prediction. Some popular LSTM variants are proposed to solve various NLP tasks
    (Hochreiter and Schmidhuber, [1997](#bib.bib52); Tai et al., [2015](#bib.bib134);
    Wang and Jiang, [2016b](#bib.bib142); Chen et al., [2017](#bib.bib24); Wu et al.,
    [2016](#bib.bib147); Rocktäschel et al., [2016](#bib.bib113); Chen et al., [2017](#bib.bib24)).
    These representative works received the interests of evaluation with adversarial
    examples recently (Gao et al., [2018](#bib.bib36); Sun et al., [2018](#bib.bib131);
    Sato et al., [2018](#bib.bib119); Papernot et al., [2016b](#bib.bib105); Iyyer
    et al., [2018](#bib.bib55); Jia and Liang, [2017](#bib.bib56); Zhao et al., [2017](#bib.bib157);
    Minervini and Riedel, [2018](#bib.bib93); Rocktäschel et al., [2016](#bib.bib113)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 有许多变体，其中长短期记忆（LSTM）网络（Hochreiter 和 Schmidhuber，[1997](#bib.bib52)）最受欢迎。LSTM
    是一种专门设计用于捕捉长期依赖关系的 RNN。在 LSTM 中，隐状态通过三种门的组合计算，即输入门、遗忘门和输出门，这些门通过逻辑函数控制信息流。LSTM
    网络随后被证明比传统 RNN 更有效（Graves 等人，[2013](#bib.bib45)）。GRU 是 LSTM 的简化版本，仅由两个门组成，因此在训练和预测方面更高效。一些流行的
    LSTM 变体被提出用于解决各种 NLP 任务（Hochreiter 和 Schmidhuber，[1997](#bib.bib52)；Tai 等人，[2015](#bib.bib134)；Wang
    和 Jiang，[2016b](#bib.bib142)；Chen 等人，[2017](#bib.bib24)；Wu 等人，[2016](#bib.bib147)；Rocktäschel
    等人，[2016](#bib.bib113)；Chen 等人，[2017](#bib.bib24)）。这些具有代表性的工作最近受到了对抗性样本评估的关注（Gao
    等人，[2018](#bib.bib36)；Sun 等人，[2018](#bib.bib131)；Sato 等人，[2018](#bib.bib119)；Papernot
    等人，[2016b](#bib.bib105)；Iyyer 等人，[2018](#bib.bib55)；Jia 和 Liang，[2017](#bib.bib56)；Zhao
    等人，[2017](#bib.bib157)；Minervini 和 Riedel，[2018](#bib.bib93)；Rocktäschel 等人，[2016](#bib.bib113)）。
- en: 2.2.4\. Sequence-to-Sequence Learning (Seq2Seq) Models
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. 序列到序列学习（Seq2Seq）模型
- en: 'Sequence-to-sequence learning (Seq2Seq) (Sutskever et al., [2014](#bib.bib132))
    is one of the important breakthroughs in deep learning and is now widely used
    for NLP applications. Seq2Seq model has the superior capacity to generate another
    sequence information for a given sequence information with an encoder-decoder
    architecture (Li Deng, [2018](#bib.bib78)). Usually, a Seq2Seq model consists
    of two recurrent neural networks: an encoder that processes the input and compresses
    it into a vector representation, a decoder that predicts the output. Latent Variable
    Hierarchical Recurrent Encoder-Decoder (VHRED) model (Serban et al., [2016](#bib.bib123))
    is a recently popular Seq2Seq model that generate sequences leveraging the complex
    dependencies between subsequences. (Cho et al., [2014](#bib.bib26)) is one of
    the first neural machine translation (NMT) model that adopt the Seq2Seq model.
    OpenNMT (Klein et al., [2017](#bib.bib65)), a Seq2Seq NMT model proposed recently,
    becomes one of the benchmark works in NMT. As they are adopted and applied widely,
    attack works also emerge (Niu and Bansal, [2018](#bib.bib100); Ebrahimi et al.,
    [2018](#bib.bib32); Cheng et al., [2018](#bib.bib25); Singh et al., [2018](#bib.bib128)).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列学习（Seq2Seq）（Sutskever 等人，[2014](#bib.bib132)）是深度学习中的一个重要突破，现在广泛用于 NLP 应用。Seq2Seq
    模型具有将给定序列信息生成另一个序列信息的优越能力，采用编码器-解码器架构（Li Deng，[2018](#bib.bib78)）。通常，Seq2Seq 模型由两个递归神经网络组成：一个编码器处理输入并将其压缩成向量表示，另一个解码器预测输出。潜在变量层次递归编码器-解码器（VHRED）模型（Serban
    等人，[2016](#bib.bib123)）是最近流行的 Seq2Seq 模型，利用子序列之间的复杂依赖关系生成序列。（Cho 等人，[2014](#bib.bib26)）是首批采用
    Seq2Seq 模型的神经机器翻译（NMT）模型之一。OpenNMT（Klein 等人，[2017](#bib.bib65)），一种最近提出的 Seq2Seq
    NMT 模型，已成为 NMT 的基准工作之一。由于它们被广泛采用和应用，攻击性工作也随之出现（Niu 和 Bansal，[2018](#bib.bib100)；Ebrahimi
    等人，[2018](#bib.bib32)；Cheng 等人，[2018](#bib.bib25)；Singh 等人，[2018](#bib.bib128)）。
- en: 2.2.5\. Attention Models
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5\. 注意力模型
- en: Attention mechanism (Bahdanau et al., [2015](#bib.bib10)) is another breakthrough
    in deep leaning. It was initially developed to overcome the difficulty of encoding
    a long sequence required in Seq2Seq models (Li Deng, [2018](#bib.bib78)). Attention
    allows the decoder to look back on the hidden states of the source sequence. The
    hidden states then provide a weighted average as additional input to the decoder.
    This mechanism pays attention on informative parts of the sequence. Rather than
    looking at the input sequence in vanilla attention models, self-attention (Vaswani
    et al., [2017](#bib.bib137)) in NLP is used to look at the surrounding words in
    a sequence to obtain more contextually sensitive word representations (Young et al.,
    [2018](#bib.bib153)). BiDAF (Seo et al., [2016](#bib.bib122)) is a bidirectional
    attention flow mechanism for machine comprehension and achieved outstanding performance
    when proposed. (Jia and Liang, [2017](#bib.bib56); Singh et al., [2018](#bib.bib128))
    evaluated the robustness of this model via adversarial examples and became the
    first few works using adversarial examples for attacking textual DNNs. Other attention-based
    DNNs (Costa-Jussà and Fonollosa, [2016](#bib.bib27); Parikh et al., [2016](#bib.bib109))
    also received adversarial attacks recently (Ebrahimi et al., [[n. d.]](#bib.bib31);
    Minervini and Riedel, [2018](#bib.bib93)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制（Bahdanau et al., [2015](#bib.bib10)）是深度学习中的另一个突破。它最初是为了克服Seq2Seq模型中对长序列编码的困难而开发的（Li
    Deng, [2018](#bib.bib78)）。注意力机制允许解码器回顾源序列的隐藏状态。隐藏状态提供加权平均值，作为额外输入传递给解码器。这个机制关注序列中的信息部分。与传统的注意力模型不同，自注意力（Vaswani
    et al., [2017](#bib.bib137)）在NLP中用于查看序列中周围的词汇，以获得更具上下文敏感性的词表示（Young et al., [2018](#bib.bib153)）。BiDAF（Seo
    et al., [2016](#bib.bib122)）是一种用于机器理解的双向注意力流机制，并在提出时取得了出色的表现。（Jia and Liang, [2017](#bib.bib56);
    Singh et al., [2018](#bib.bib128)）通过对抗样本评估了该模型的鲁棒性，并成为首批使用对抗样本攻击文本DNN的研究之一。其他基于注意力的DNN（Costa-Jussà
    and Fonollosa, [2016](#bib.bib27); Parikh et al., [2016](#bib.bib109)）最近也遭受了对抗攻击（Ebrahimi
    et al., [[n. d.]](#bib.bib31); Minervini and Riedel, [2018](#bib.bib93)）。
- en: 2.2.6\. Reinforcement Learning Models
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.6. 强化学习模型
- en: Reinforcement learning trains an agent by giving a reward after agents performing
    discrete actions. In NLP, reinforcement learning framework usually consist of
    an agent (a DNN), a policy (guiding action) and a reward. The agent picks an action
    (e.g., predicting next word in a sequence) based on a policy, then updates its
    internal state accordingly, until arriving the end of the sequence where a reward
    is calculated. Reinforcement learning requires proper handling of the action and
    the states, which may limit the expressive power and learning capacity of the
    models (Young et al., [2018](#bib.bib153)). But it gains much interests in task-oriented
    dialogue systems (Li et al., [2016](#bib.bib76)) as they share the fundamental
    principle as decision making processes. Limited works so far can be found to attack
    the reinforcement learning model in NLP (Niu and Bansal, [2018](#bib.bib100)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习通过在智能体执行离散动作后给予奖励来训练智能体。在NLP中，强化学习框架通常包括一个智能体（一个DNN）、一个策略（指导动作）和一个奖励。智能体根据策略选择一个动作（例如，预测序列中的下一个词），然后相应地更新其内部状态，直到到达序列末尾，此时计算奖励。强化学习需要对动作和状态进行适当处理，这可能限制模型的表达能力和学习能力（Young
    et al., [2018](#bib.bib153)）。但它在任务导向对话系统（Li et al., [2016](#bib.bib76)）中引起了较大的兴趣，因为它们与决策过程的基本原理相同。目前为止，仅能找到有限的研究来攻击NLP中的强化学习模型（Niu
    and Bansal, [2018](#bib.bib100)）。
- en: 2.2.7\. Deep Generative Models
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.7. 深度生成模型
- en: 'In recent years, two powerful deep generative models, Generative Adversarial
    Networks (GANs) (Goodfellow et al., [2014](#bib.bib42)) and Variational Auto-Encoders
    (VAEs) (Kingma and Welling, [2014](#bib.bib64)) are proposed and gain much research
    attention. Generative models are able to generate realistic data that are very
    similar to ground truth data in a latent space. In NLP field, they are used to
    generate textual data. GANs (Goodfellow et al., [2014](#bib.bib42)) consist of
    two adversarial networks: a generator and a discriminator. Discriminator is to
    discriminate the real and generated samples, while the generator is to generate
    realistic samples that aims to fool the discriminator. GAN uses a min-max loss
    function to train two neural networks simultaneously. VAEs consist of encoder
    and generator networks. Encoder encodes an input into a latent space and the generator
    generates samples from the latent space. Deep generative models is not easy to
    train and evaluate. Hence, these deficiencies hinder their wide usage in many
    real-world applications (Young et al., [2018](#bib.bib153)). Although they have
    been adopted in generating texts, so far no work examines their robustness using
    adversarial examples.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，两个强大的深度生成模型——生成对抗网络（GANs）（Goodfellow等，[2014](#bib.bib42)）和变分自编码器（VAEs）（Kingma和Welling，[2014](#bib.bib64)）被提出并引起了大量研究关注。生成模型能够在潜在空间中生成与真实数据非常相似的逼真数据。在NLP领域，它们被用于生成文本数据。GANs（Goodfellow等，[2014](#bib.bib42)）由两个对抗网络组成：生成器和鉴别器。鉴别器用于区分真实样本和生成样本，而生成器则生成旨在欺骗鉴别器的逼真样本。GAN使用最小-最大损失函数同时训练两个神经网络。VAEs由编码器和生成器网络组成。编码器将输入编码到潜在空间中，生成器则从潜在空间生成样本。深度生成模型训练和评估并不容易。因此，这些缺陷阻碍了它们在许多实际应用中的广泛使用（Young等，[2018](#bib.bib153)）。尽管它们已被用于生成文本，但迄今为止尚未有工作检查其使用对抗样本的鲁棒性。
- en: 3\. From Image to Text
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 从图像到文本
- en: Adversarial attacks are originated from computer vision community. In this section,
    we introduce representative works, discuss differences between attacking image
    data and textual data, and present preliminary knowledge when performing adversarial
    attacks on textual DNNs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击源自计算机视觉领域。在本节中，我们介绍了具有代表性的工作，讨论了攻击图像数据和文本数据之间的差异，并在对文本DNN进行对抗攻击时提供了初步知识。
- en: '3.1\. Crafting Adversarial Examples: Inspiring Works in Computer Vision'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 制作对抗样本：计算机视觉领域的启发性工作
- en: Since adversarial examples are first proposed for attacking object recognition
    DNNs in computer vision community  (Szegedy et al., [2014](#bib.bib133); Goodfellow
    et al., [2015](#bib.bib43); Papernot et al., [2016a](#bib.bib107); Carlini and
    Wagner, [2017](#bib.bib21); Moosavi-Dezfooli et al., [2016](#bib.bib97); Papernot
    et al., [2017](#bib.bib106); Zhao et al., [2017](#bib.bib157)), this research
    direction has been receiving sustained attentions. We briefly introduce some works
    that inspired their followers in NLP community in this section, allowing the reader
    to better understand the adversarial attacks on textual DNNs. For comprehensive
    review of attack works in computer vision, please refer to (Akhtar and Mian, [2018](#bib.bib3)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自从对抗样本首次在计算机视觉领域用于攻击目标识别DNN（Szegedy等，[2014](#bib.bib133)；Goodfellow等，[2015](#bib.bib43)；Papernot等，[2016a](#bib.bib107)；Carlini和Wagner，[2017](#bib.bib21)；Moosavi-Dezfooli等，[2016](#bib.bib97)；Papernot等，[2017](#bib.bib106)；Zhao等，[2017](#bib.bib157)）提出以来，该研究方向一直受到持续关注。我们在本节中简要介绍了一些在NLP领域启发后续研究者的工作，以帮助读者更好地了解对文本DNN的对抗攻击。有关计算机视觉领域攻击工作的全面综述，请参考（Akhtar和Mian，[2018](#bib.bib3)）。
- en: L-BFGS
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: L-BFGS
- en: 'Szegedy et al. invented the adversarial examples notation (Szegedy et al.,
    [2014](#bib.bib133)). They proposed a explicitly designed method to cause the
    model to give wrong prediction of adversarial input ($\mathbf{x}+\mathbf{\eta}$)
    for image classification task. It came to solve the optimization problem:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy等发明了对抗样本符号（Szegedy等，[2014](#bib.bib133)）。他们提出了一种明确设计的方法，以使模型对图像分类任务中的对抗输入（$\mathbf{x}+\mathbf{\eta}$）做出错误预测。它来解决优化问题：
- en: '| (2) |  | $\displaystyle\mathbf{\eta}=\arg\min_{\mathbf{\eta}}\lambda&#124;&#124;\mathbf{\eta}&#124;&#124;^{2}_{2}+J(\mathbf{x}+\mathbf{\eta},y^{\prime})~{}~{}~{}s.t.~{}~{}(\mathbf{x}+\mathbf{\eta})\in[0,1],$
    |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle\mathbf{\eta}=\arg\min_{\mathbf{\eta}}\lambda&#124;&#124;\mathbf{\eta}&#124;&#124;^{2}_{2}+J(\mathbf{x}+\mathbf{\eta},y^{\prime})~{}~{}~{}s.t.~{}~{}(\mathbf{x}+\mathbf{\eta})\in[0,1],$
    |  |'
- en: 'where $y^{\prime}$ is the target output of ($\mathbf{x}^{\prime}+\mathbf{\eta}$),
    but incorrect given an ideal classifier. $J$ denotes the cost function of the
    DNN and $\lambda$ is a hyperparameter to balance the two parts of the equation.
    This minimization was initially performed with a box-constrained Limited memory
    Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm and thus was named after it.
    The optimization was repeated multiple times until reaching a minimum $\lambda$
    that satisfy Eq. ([2](#S3.E2 "In L-BFGS ‣ 3.1\. Crafting Adversarial Examples:
    Inspiring Works in Computer Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey")).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $y^{\prime}$ 是 ($\mathbf{x}^{\prime}+\mathbf{\eta}$) 的目标输出，但在理想分类器下是不正确的。$J$
    表示 DNN 的成本函数，$\lambda$ 是一个超参数，用于平衡方程的两部分。最初，这种最小化是通过带有盒约束的有限内存 Broyden-Fletcher-Goldfarb-Shanno
    (L-BFGS) 算法进行的，因此得名。优化被重复多次，直到达到满足 Eq. ([2](#S3.E2 "In L-BFGS ‣ 3.1\. Crafting
    Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\. From Image to Text
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")) 的最小值 $\lambda$。'
- en: Fast Gradient Sign Method (FGSM)
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 快速梯度符号法 (FGSM)
- en: 'L-BFGS is very effective, but highly expensive - this inspired Goodfellow et
    al. (Goodfellow et al., [2015](#bib.bib43)) to find a simplified solution. Instead
    of fixing $y^{\prime}$ and minimizing $\mathbf{\eta}$ in L-BFGS, FGSM fixed size
    of $\mathbf{\eta}$ and maximized the cost (Eq. ([3](#S3.E3 "In Fast Gradient Sign
    Method (FGSM) ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer
    Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey"))). Then they linearized the problem
    with a first-order Taylor series approximation (Eq. ([4](#S3.E4 "In Fast Gradient
    Sign Method (FGSM) ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer
    Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey"))), and got the closed-form solution
    of $\mathbf{\eta}$ (Eq. ([5](#S3.E5 "In Fast Gradient Sign Method (FGSM) ‣ 3.1\.
    Crafting Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\. From Image
    to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey"))) (Warde-Farley and Goodfellow, [2016](#bib.bib144)):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'L-BFGS 非常有效，但代价高昂——这激发了 Goodfellow 等人 (Goodfellow et al., [2015](#bib.bib43))
    寻找简化解决方案。FGSM 并没有在 L-BFGS 中固定 $y^{\prime}$ 并最小化 $\mathbf{\eta}$，而是固定了 $\mathbf{\eta}$
    的大小，并最大化了成本 (Eq. ([3](#S3.E3 "In Fast Gradient Sign Method (FGSM) ‣ 3.1\. Crafting
    Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\. From Image to Text
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")))。然后，他们通过一阶泰勒级数近似 (Eq. ([4](#S3.E4 "In Fast Gradient Sign Method (FGSM)
    ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\.
    From Image to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey")))，将问题线性化，得到了 $\mathbf{\eta}$ 的封闭形式解 (Eq. ([5](#S3.E5 "In
    Fast Gradient Sign Method (FGSM) ‣ 3.1\. Crafting Adversarial Examples: Inspiring
    Works in Computer Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep
    Learning Models in Natural Language Processing: A Survey"))) (Warde-Farley 和 Goodfellow,
    [2016](#bib.bib144))：'
- en: '| (3) |  | $\displaystyle\mathbf{\eta}=\arg\max_{\mathbf{\eta}}J(\mathbf{x}+\mathbf{\eta},y)~{}~{}~{}s.t.~{}~{}&#124;&#124;\mathbf{\eta}&#124;&#124;_{\infty}\leq\epsilon,$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle\mathbf{\eta}=\arg\max_{\mathbf{\eta}}J(\mathbf{x}+\mathbf{\eta},y)~{}~{}~{}s.t.~{}~{}&#124;&#124;\mathbf{\eta}&#124;&#124;_{\infty}\leq\epsilon,$
    |  |'
- en: '| (4) |  | $\displaystyle\mathbf{\eta}=\arg\max_{\mathbf{\eta}}J(\mathbf{x},y)+\mathbf{\eta}^{\mathbf{T}}\nabla_{\mathbf{x}}J(\mathbf{x},y)~{}~{}~{}s.t.~{}~{}&#124;&#124;\mathbf{\eta}&#124;&#124;_{\infty}\leq\epsilon,$
    |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\mathbf{\eta}=\arg\max_{\mathbf{\eta}}J(\mathbf{x},y)+\mathbf{\eta}^{\mathbf{T}}\nabla_{\mathbf{x}}J(\mathbf{x},y)~{}~{}~{}s.t.~{}~{}&#124;&#124;\mathbf{\eta}&#124;&#124;_{\infty}\leq\epsilon,$
    |  |'
- en: '| (5) |  | $\displaystyle\mathbf{\eta}=\epsilon\cdot\text{sign}(\nabla_{\mathbf{x}}J(\mathbf{x},\mathbf{y}))$
    |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle\mathbf{\eta}=\epsilon\cdot\text{sign}(\nabla_{\mathbf{x}}J(\mathbf{x},\mathbf{y}))$
    |  |'
- en: where $\epsilon$ is a parameter set by attacker, controlling the perturbation’s
    magnitude. sign(x) is the sign function which returns 1 when $x>0$, and $-1$ when
    $x<0$, otherwise returns $0$. $\nabla_{\mathbf{x}}J(\mathbf{x},\mathbf{y})$ denotes
    the gradient of loss function respect to the input, and can be calculated via
    back-propagation. FGSM attracts the most follow-up works in NLP.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$ 是由攻击者设定的参数，用于控制扰动的幅度。sign(x) 是符号函数，当 $x>0$ 时返回 1，当 $x<0$ 时返回 $-1$，否则返回
    $0$。$\nabla_{\mathbf{x}}J(\mathbf{x},\mathbf{y})$ 表示损失函数对输入的梯度，可以通过反向传播计算。FGSM
    吸引了最多的后续研究工作在 NLP 领域。
- en: Jacobian Saliency Map Adversary (JSMA)
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 雅可比显著性映射对抗者 (JSMA)
- en: 'Unlike FGSM using gradients to attack, Papernot et al. (Papernot et al., [2016a](#bib.bib107))
    generated adversarial examples using forward derivatives (i.e., model Jacobian).
    This method evaluates the neural model’s output sensitivity to each input component
    using its Jacobian Matrix and gives greater control to adversaries given the perturbations.
    Jacobian matrices form the adversarial saliency maps that rank each input component’s
    contribution to the adversarial target. A perturbation is then selected from the
    maps. Thus the method was named Jacobian-based Saliency Map Attack. The Jacobian
    matrix of a given $\mathbf{x}$ is given by:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用梯度攻击的FGSM不同，Papernot等人（Papernot et al., [2016a](#bib.bib107)）通过使用前向导数（即模型雅可比矩阵）生成对抗样本。这种方法使用雅可比矩阵评估神经模型对每个输入组件的输出敏感性，并在扰动条件下给予对手更大的控制权。雅可比矩阵形成了对抗显著性图，排名每个输入组件对对抗目标的贡献。然后从这些图中选择扰动。因此，该方法被称为基于雅可比矩阵的显著性图攻击。给定
    $\mathbf{x}$ 的雅可比矩阵表示为：
- en: '| (6) |  | $\displaystyle Jacb_{F}[i,j]=\frac{\partial F_{i}}{\partial\mathbf{x}_{j}}$
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle Jacb_{F}[i,j]=\frac{\partial F_{i}}{\partial\mathbf{x}_{j}}$
    |  |'
- en: where $\mathbf{x}_{i}$ is the $i$-th component of the input and $F_{j}$ is the
    $j$-th component of the output. Here $F$ denotes the logits (i.e., the inputs
    to the softmax function) layer. $J_{F}[i,j]$ measures the sensitivity of $F_{j}$
    with respect to $\mathbf{x}_{i}$.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x}_{i}$ 是输入的第 $i$ 个组件，$F_{j}$ 是输出的第 $j$ 个组件。这里 $F$ 表示 logits（即 softmax
    函数的输入）层。 $J_{F}[i,j]$ 测量 $F_{j}$ 对 $\mathbf{x}_{i}$ 的敏感性。
- en: C&W Attack
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: C&W 攻击
- en: 'Carlini and Wagner (Carlini and Wagner, [2017](#bib.bib21)) aimed to evaluate
    the defensive distillation strategy (Hinton et al., [2015](#bib.bib51)) for mitigating
    the adversarial attacks. They restricted the perturbations with $l_{p}$ norms
    where $p$ equals to $0,2$ and $\infty$ and proposed seven versions of $J$ for
    the following optimization problem:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Carlini和Wagner（Carlini and Wagner, [2017](#bib.bib21)）旨在评估防御性蒸馏策略（Hinton et
    al., [2015](#bib.bib51)）以减轻对抗攻击。他们限制了扰动的 $l_{p}$ 范数，其中 $p$ 等于 $0,2$ 和 $\infty$，并提出了七种不同的
    $J$ 版本来解决以下优化问题：
- en: '| (7) |  | $\displaystyle\mathbf{\eta}=\arg\min_{\mathbf{\eta}}&#124;&#124;\mathbf{\eta}&#124;&#124;_{p}+\lambda
    J(\mathbf{x}+\mathbf{\eta},y^{\prime})~{}~{}~{}s.t.~{}~{}(\mathbf{x}+\mathbf{\eta})\in[0,1],$
    |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\displaystyle\mathbf{\eta}=\arg\min_{\mathbf{\eta}}&#124;&#124;\mathbf{\eta}&#124;&#124;_{p}+\lambda
    J(\mathbf{x}+\mathbf{\eta},y^{\prime})~{}~{}~{}s.t.~{}~{}(\mathbf{x}+\mathbf{\eta})\in[0,1],$
    |  |'
- en: and the formulation shares the same notation with aforementioned works.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其公式与上述工作中的符号表示相同。
- en: DeepFool
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DeepFool
- en: DeepFool (Moosavi-Dezfooli et al., [2016](#bib.bib97)) is an iterative $L_{2}$-regularized
    algorithm. The authors first assumed the neural network is linear, thus they can
    separate the classes with a hyperplane. They simplified the problem and found
    optimal solution based on this assumption and constructed adversarial examples.
    To address the non-linearity fact of the neural network, they repeated the process
    until a true adversarial example is found.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: DeepFool（Moosavi-Dezfooli et al., [2016](#bib.bib97)）是一种迭代的 $L_{2}$ 正则化算法。作者首先假设神经网络是线性的，因此他们可以通过超平面分离类别。他们在这个假设下简化了问题，并找到最佳解决方案来构建对抗样本。为了应对神经网络的非线性，他们重复这一过程直到找到真正的对抗样本。
- en: Substitute Attack
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 替代攻击
- en: 'The above mentioned representative works are all white-box methods, which require
    the full knowledge of the neural model’s parameters and structures. However, in
    practice, it is not always possible for attackers to craft adversaries in white-box
    manner due to the limited access to the model. The limitation was addressed by
    Papernot et al. (Papernot et al., [2017](#bib.bib106)) and they introduced a black-box
    attack strategy: They trained a substitute model to approximate the decision boundaries
    of the target model with the labels obtained by querying the target model. Then
    they conducted white-box attack on this substitute and generate adversarial examples
    on the substitute. Specifically, they adopted FSGM and JSMA in generating adversarial
    examples for the substitute DNN.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代表性工作都是白盒方法，需要对神经模型的参数和结构有完全了解。然而，实际上，由于对模型的访问受限，攻击者并不总是能够以白盒方式生成对抗样本。Papernot等人（Papernot
    et al., [2017](#bib.bib106)）解决了这一限制，并引入了一种黑盒攻击策略：他们训练了一个替代模型来近似目标模型的决策边界，使用通过查询目标模型获得的标签。然后，他们对这个替代模型进行了白盒攻击，并在替代模型上生成对抗样本。具体来说，他们采用了FSGM和JSMA来生成替代DNN的对抗样本。
- en: GAN-like Attack
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类GAN攻击
- en: 'There are another branch of black-box attack leverages the Generative Adversarial
    Neural (GAN) models. Zhao et al. (Zhao et al., [2017](#bib.bib157)) firstly trained
    a generative model, WGAN, on the training dataset $\mathbf{X}$. WGAN could generate
    data points that follows the same distribution with $\mathbf{X}$. Then they separately
    trained an inverter to map data sample $\mathbf{x}$ to $\mathbf{z}$ in the latent
    dense space by minimizing the reconstruction error. Instead of perturbing $\mathbf{x}$,
    they searched for adversaries $z*$ in the neighbour of $\mathbf{z}$ in the latent
    space. Then they mapped $\mathbf{z*}$ back to $\mathbf{x*}$ and check if $\mathbf{x*}$
    would change the prediction. They introduced two search algorithms: iterative
    stochastic search and hybrid shrinking search. The former one used expanding strategy
    that gradually expand the search space, while the later one used shrinking strategy
    that starts from a wide range and recursively tightens the upper bound of the
    search range.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类黑箱攻击利用了生成对抗网络（GAN）模型。赵等人（Zhao et al., [2017](#bib.bib157)）首先在训练数据集$\mathbf{X}$上训练了一个生成模型WGAN。WGAN可以生成与$\mathbf{X}$相同分布的数据点。接着，他们分别训练了一个反向器，通过最小化重建误差，将数据样本$\mathbf{x}$映射到潜在的密集空间中的$\mathbf{z}$。他们没有扰动$\mathbf{x}$，而是搜索潜在空间中$\mathbf{z}$的邻域中的对抗样本$z*$。然后，他们将$\mathbf{z*}$映射回$\mathbf{x*}$并检查$\mathbf{x*}$是否会改变预测。他们引入了两种搜索算法：迭代随机搜索和混合缩小搜索。前者使用逐渐扩展的策略扩展搜索空间，而后者使用从宽范围开始并递归收紧搜索范围上界的缩小策略。
- en: 3.2\. Attacking Image DNNs vs Attacking Textual DNNs
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 攻击图像DNN与攻击文本DNN
- en: 'To attack a textual DNN model, we cannot directly apply the approaches from
    the image DNN attackers as there are three main differences between them:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击文本DNN模型时，我们不能直接应用来自图像DNN攻击者的方法，因为它们之间有三个主要区别：
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Discrete vs Continuous Inputs. Image inputs are continuous, typically the methods
    use $L_{p}$ norm measures the distance between clean data point with the perturbed
    data point. However, textual data is symbolic, thus discrete. It is hard to define
    the perturbations on texts. Carefully designed variants or distance measurements
    for textual perturbations are required. Another choice is to firstly map the textual
    data to continuous data, then adopt the attack method from computer vision. We
    will give further discussion in Section [3.3](#S3.SS3 "3.3\. Vectorizing Textual
    Inputs and Perturbation Measurements ‣ 3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey").'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 离散与连续输入。图像输入是连续的，通常使用$L_{p}$范数来衡量干净数据点与扰动数据点之间的距离。然而，文本数据是符号性的，因此是离散的。定义文本上的扰动很困难。需要精心设计的文本扰动变体或距离度量。另一种选择是首先将文本数据映射到连续数据，然后采用计算机视觉中的攻击方法。我们将在第[3.3](#S3.SS3
    "3.3\. 向量化文本输入与扰动度量 ‣ 3\. 从图像到文本 ‣ 自然语言处理中的深度学习模型的对抗攻击：综述")节中进一步讨论。
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Perceivable vs Unperceivable. Small change of the image pixels usually can not
    be easily perceived by human beings, hence the adversarial examples will not change
    the human judgment, but only fool the DNN models. But small changes on texts,
    e.g., character or word change, will easily be perceived, rendering the possibility
    of attack failure. For example, the changes could be identified or corrected by
    spelling-check and grammar check before inputting into textual DNN models. Therefore,
    it is nontrivial to find unperceivalble textual adversaries.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可感知与不可感知。图像像素的微小变化通常不易被人类感知，因此对抗样本不会改变人类的判断，只会欺骗深度神经网络（DNN）模型。但文本中的微小变化，例如字符或词语的变化，容易被感知，导致攻击失败的可能性。例如，变化可以通过拼写检查和语法检查在输入到文本DNN模型之前被识别或纠正。因此，找到不可感知的文本对抗样本并非易事。
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Semantic vs Semantic-less. In the case of images, small changes usually do not
    change the semantics of the image as they are trivial and unperceivable. However,
    perturbation on texts would easily change the semantics of a word and a sentence,
    thus can be easily detected and heavily affect the model output. For example,
    deleting a negation word would change the sentiment of a sentence. But this is
    not the case in computer vision where perturbing individual pixels does not turn
    the image from a cat to another animal. Changing semantics of the input is against
    the goal of adversarial attack that keep the correct prediction unchanged while
    fooling an victim DNN.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语义与非语义。对于图像而言，小的变化通常不会改变图像的语义，因为它们是微不足道且不可感知的。然而，文本上的扰动容易改变单词和句子的语义，因此可以被轻易检测并严重影响模型输出。例如，删除否定词会改变句子的情感。但在计算机视觉中，扰动单个像素不会将图像从猫变成另一种动物。改变输入的语义违背了对抗攻击的目标，即保持正确预测不变，同时欺骗受害
    DNN。
- en: Due to these differences, current state-of-the-art textual DNN attackers either
    carefully adjust the methods from image DNN attackers by enforcing additional
    constraints, or propose novel methods using different techniques.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些差异，目前最先进的文本 DNN 攻击者要么通过施加额外约束来仔细调整图像 DNN 攻击者的方法，要么提出使用不同技术的新方法。
- en: 3.3\. Vectorizing Textual Inputs and Perturbation Measurements
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 文本输入的向量化和扰动测量
- en: 'Vectorizing Textual Input. DNN models require vectors as input, for image tasks,
    the normal way is to use the pixel value to form the vectors/matrices as DNN input.
    But for textual models, special operations are needed to transform the text into
    vectors. There are three main branches of methods: word-count based encoding,
    one-hot encoding and dense encoding (or feature embedding) and the later two are
    mostly used in DNN models of textual applications.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 文本向量化。DNN 模型需要向量作为输入，对于图像任务，通常使用像素值来形成向量/矩阵作为 DNN 输入。但对于文本模型，需要特殊操作将文本转换为向量。主要有三种方法：基于词频的编码、独热编码和密集编码（或特征嵌入），后两者在文本应用的
    DNN 模型中使用较多。
- en: •
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Word-Count Based Encoding. Bag-of-words (BOW) method has the longest history
    in vectorizing text. In BOW model, an zero-encoded vector with length of the vocabulary
    size is initialized. Then the dimension in vector is replaced by the count of
    corresponding word’s appearance in the given sentence. Another word-count based
    encoding is to utilize the term frequency-inverse document frequency (TF-IDF)
    of a word (term), and the dimension in the vector is the TF-IDF value of the word.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于词频的编码。词袋（BOW）方法在文本向量化中历史最悠久。在 BOW 模型中，初始化一个长度为词汇表大小的零编码向量。然后，向量中的维度由给定句子中对应单词的出现次数替换。另一种基于词频的编码是利用词项频率-逆文档频率（TF-IDF），向量中的维度为词的
    TF-IDF 值。
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'One-hot Encoding. In one-hot encoding, a vector feature represents a token–a
    token could be a character (character-level model) or a word (word-level model).
    For character-level one-hot encoding, the representation can be formulated as
    (Ebrahimi et al., [2018](#bib.bib32)):'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 独热编码。在独热编码中，向量特征表示一个标记——标记可以是字符（字符级模型）或单词（词级模型）。对于字符级独热编码，表示可以形式化为（Ebrahimi
    等，[2018](#bib.bib32)）：
- en: '| (8) |  | $\displaystyle\mathbf{x}=[(x_{11},...x_{1n});...(x_{m1},...x_{mn})]$
    |  |'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (8) |  | $\displaystyle\mathbf{x}=[(x_{11},...x_{1n});...(x_{m1},...x_{mn})]$
    |  |'
- en: 'where $\mathbf{x}$ be a text of $L$ characters, $x_{ij}\in\{0,1\}^{|A|}$ and
    $|A|$ is the alphabet (in some works, $|A|$ also include symbols). In Equation [8](#S3.E8
    "In 2nd item ‣ 3.3\. Vectorizing Textual Inputs and Perturbation Measurements
    ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models in Natural
    Language Processing: A Survey"), $m$ is the number of words, $n$ is the maximum
    number of characters for a word in sequence x. Thus each word has the same-fixed
    length of vector representation and the length is decided by the maximum number
    of characters of the words. For word-level one-hot encoding, following the above
    notations, the text $x$ can be represented as:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x}$ 为 $L$ 个字符的文本，$x_{ij}\in\{0,1\}^{|A|}$，$|A|$ 为字母表（在一些工作中，$|A|$
    还包括符号）。在方程 [8](#S3.E8 "在第 2 项 ‣ 3.3\. 文本输入的向量化和扰动测量 ‣ 3\. 从图像到文本 ‣ 自然语言处理中的深度学习模型对抗攻击：综述")
    中，$m$ 为单词数量，$n$ 为序列 x 中单词的最大字符数。因此，每个单词具有相同固定长度的向量表示，长度由单词的最大字符数决定。对于词级独热编码，按照上述符号，文本
    $x$ 可以表示为：
- en: '| (9) |  | $\displaystyle\mathbf{x}=[(x_{1},...,x_{m},x_{m+1}...x_{k})]$ |  |'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (9) |  | $\displaystyle\mathbf{x}=[(x_{1},...,x_{m},x_{m+1}...x_{k})]$ |  |'
- en: where $x_{ij}\in\{0,1\}^{|V|}$ and $|V|$ is the vocabulary, which contains all
    words in a corpus. $k$ is the maximum number of words allowed for a text, so that
    $[(x_{m+1}...x_{k})]$ is zero-paddings if $m+1<k$. One-hot encoding produces vectors
    with only 0 and 1 values, where 1 indicates the corresponding character/word appears
    in the sentence/paragraph, while 0 indicate it does not appear. Thus one-hot encoding
    usually generates sparse vectors/matrices. DNNs have proven to be very successful
    in learning values from the sparse representations as they can learn more dense
    distributed representations from the one-hot vectors during the training procedure.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$x_{ij}\in\{0,1\}^{|V|}$，$|V|$是词汇表，包含语料库中的所有单词。$k$是允许文本的最大单词数，因此如果$m+1<k$，则$[(x_{m+1}...x_{k})]$为零填充。独热编码生成的向量仅包含0和1值，其中1表示相应的字符/单词出现在句子/段落中，而0表示它没有出现。因此，独热编码通常生成稀疏的向量/矩阵。DNN已经证明在从稀疏表示中学习值方面非常成功，因为它们可以在训练过程中从独热向量中学习到更密集的分布式表示。
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dense Encoding. Comparing to one-hot encoding, dense encoding generates low
    dimensional and distributed representations for textual data. Word2Vec citenips/MikolovSCCD13
    uses continuous bag-of-words (CBOW) and skip-gram models to generate dense representation
    for words, i.e., word embeddings. It is based on the distributional assumption
    that words appearing within similar context possess similar meaning. Word embeddings,
    to some extend, alleviates the discreteness and data-sparsity problems for vectorizing
    textual data (Goldberg, [2017](#bib.bib38)). Extensions of word embeddings such
    as doc2vec and paragraph2vec (Le and Mikolov, [2014](#bib.bib71)) encode sentences/paragraphs
    to dense vectors.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 密集编码。与独热编码相比，密集编码为文本数据生成低维且分布式的表示。Word2Vec citenips/MikolovSCCD13使用连续词袋（CBOW）和跳字模型来生成单词的密集表示，即单词嵌入。它基于分布假设，即在相似上下文中出现的单词具有相似的意义。单词嵌入在一定程度上缓解了向量化文本数据的离散性和数据稀疏性问题（Goldberg，[2017](#bib.bib38)）。单词嵌入的扩展，如doc2vec和paragraph2vec（Le和Mikolov，[2014](#bib.bib71)），将句子/段落编码为密集向量。
- en: 'Perturbation Measurement. As described in Section [2.1.3](#S2.SS1.SSS3 "2.1.3\.
    Measurement ‣ 2.1\. Adversarial Attacks on Deep Learning Models: The General Taxonomy
    ‣ 2\. Overview of Adversarial Attacks and Deep Learning Techniques in Natural
    Language Processing ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey"), there needs a way to measure the size of the perturbation,
    so that it can be controlled to ensure the ability of fooling the victim DNN while
    remain unperceivable. However, the measurement in textual perturbations is drastically
    different with the perturbations in image. Usually, the size of the perturbation
    is measured by the distance between clean data $\mathbf{x}$ and its adversarial
    example $\mathbf{x^{\prime}}$. But in texts, the distance measurement also need
    to consider the grammar correctness, syntax correctness and semantic-preservance.
    We here list the measurements used in the reviewed in this survey.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '扰动测量。如在第[2.1.3](#S2.SS1.SSS3 "2.1.3\. Measurement ‣ 2.1\. Adversarial Attacks
    on Deep Learning Models: The General Taxonomy ‣ 2\. Overview of Adversarial Attacks
    and Deep Learning Techniques in Natural Language Processing ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey")节中描述的，需要一种方法来测量扰动的大小，以便可以控制它，确保能够欺骗受害者的DNN同时保持不可感知。然而，文本扰动的测量与图像扰动的测量有很大不同。通常，扰动的大小是通过干净数据$\mathbf{x}$与其对抗样本$\mathbf{x^{\prime}}$之间的距离来测量的。但在文本中，距离测量还需要考虑语法正确性、句法正确性和语义保持。我们在此列出了本调查中回顾使用的测量方法。'
- en: •
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Norm-based measurement. Directly adopting norms such as $L_{p},p\in{0,1,2,\infty}$
    requires the input data are continuous. One solution is to use continuous and
    dense presentation (e.g., embedding) to represent the texts. But this usually
    results in invalid and incomprehensible texts, that need to involve other constrains.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于范数的测量。直接采用范数如$L_{p},p\in{0,1,2,\infty}$要求输入数据是连续的。一种解决方案是使用连续和密集的表示（例如，嵌入）来表示文本。但这通常会导致无效和难以理解的文本，需要引入其他约束。
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Grammar and syntax related measurement. Ensuring the grammar or syntactic correctness
    makes the adversarial examples not easily perceived.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语法和句法相关测量。确保语法或句法正确性使得对抗样本不易被感知。
- en: –
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Grammar and syntax checker are used in some works to ensure the textual adversarial
    examples generated are valid.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语法和句法检查器在一些工作中被用来确保生成的文本对抗示例是有效的。
- en: –
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Perplexity is usually used to measure the quality of a language model. In one
    reviewed literature (Minervini and Riedel, [2018](#bib.bib93)), the authors used
    perplexity to ensure the generated adversarial examples (sentences) are valid.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 困惑度通常用于衡量语言模型的质量。在一篇已审阅的文献 (Minervini 和 Riedel, [2018](#bib.bib93)) 中，作者使用困惑度来确保生成的对抗示例（句子）是有效的。
- en: –
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Paraphrase is controlled and can be regarded as a type of adversarial example
    ([4.3.3](#S4.SS3.SSS3 "4.3.3\. Paraphrase-based Adversaries ‣ 4.3\. Black-box
    Attack ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey")). When perturbing,
    the validity of paraphrases is ensured in the generation process.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 释义控制可以视为一种对抗示例 ([4.3.3](#S4.SS3.SSS3 "4.3.3\. 基于释义的对抗 ‣ 4.3\. 黑盒攻击 ‣ 4\. 攻击NLP中的神经模型：最前沿
    ‣ 自然语言处理中的深度学习模型对抗攻击：综述"))。在扰动过程中，确保释义的有效性。
- en: •
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Semantic-preserving measurement. Measuring semantic similarity/distance is
    often performed on word vectors by adopting vectors’ similarity/distance measurements.
    Given two $n$-dimensional word vectors $\mathbf{p}=(p_{1},p_{2},...,p_{n})$ and
    $\mathbf{q}=(q_{1},q_{2},...,q_{n})$:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语义保留度量。测量语义相似性/距离通常通过采用向量的相似性/距离度量来对词向量进行。给定两个 $n$ 维词向量 $\mathbf{p}=(p_{1},p_{2},...,p_{n})$
    和 $\mathbf{q}=(q_{1},q_{2},...,q_{n})$：
- en: –
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Euclidean Distance is a distance of two vectors in the Euclidean space:'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欧几里得距离是欧几里得空间中两个向量的距离：
- en: '| (10) |  | $\displaystyle d(\mathbf{p},\mathbf{q})=\sqrt{(p_{1}-q_{1})^{2}+p_{2}-q_{2})^{2}+..(p_{n}-q_{n})^{2}}$
    |  |'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle d(\mathbf{p},\mathbf{q})=\sqrt{(p_{1}-q_{1})^{2}+p_{2}-q_{2})^{2}+..(p_{n}-q_{n})^{2}}$
    |  |'
- en: –
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Cosine Similarity computes cosine value of the angle between the two vectors:'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 余弦相似度计算两个向量之间夹角的余弦值：
- en: '| (11) |  | $\displaystyle cos(\mathbf{p},\mathbf{q})=\frac{\sum_{i=1}^{n}p_{i}\times
    q_{i}}{\sqrt{\sum_{i=1}^{n}(p_{i})^{2}}\times\sqrt{\sum_{i=1}^{n}(q_{i})^{2}}}$
    |  |'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle cos(\mathbf{p},\mathbf{q})=\frac{\sum_{i=1}^{n}p_{i}\times
    q_{i}}{\sqrt{\sum_{i=1}^{n}(p_{i})^{2}}\times\sqrt{\sum_{i=1}^{n}(q_{i})^{2}}}$
    |  |'
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Edit-based measurement. Edit distance is a way of quantifying the minimum changes
    from one string to the other. Different definitions of edit distance use different
    sets of string operations (Li et al., [2019](#bib.bib75)).
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于编辑的度量。编辑距离是量化从一个字符串到另一个字符串的最小变化的一种方法。不同的编辑距离定义使用不同的字符串操作集合 (Li et al., [2019](#bib.bib75))。
- en: –
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Levenshtein Distance uses insertion, removal and substitution operations.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 莱文斯坦距离使用插入、删除和替换操作。
- en: –
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Word Mover’s Distance (WMD) (Kusner et al., [2015](#bib.bib70)) is an edit
    distance operated on word embedding. It measures the minimum amount of distance
    that the embedded words of one document need to travel to reach the embedded words
    of the other document (Gong et al., [2018](#bib.bib40)). The minimization is formulated
    as:'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Word Mover’s Distance (WMD) (Kusner et al., [2015](#bib.bib70)) 是在词嵌入上操作的编辑距离。它衡量一个文档的嵌入单词需要移动的最小距离，以到达另一个文档的嵌入单词
    (Gong et al., [2018](#bib.bib40))。最小化被表述为：
- en: '| (12) |  | $\displaystyle\min\sum^{n}_{i,j=1}\textbf{T}_{ij}&#124;&#124;\mathbf{e_{i}}-\mathbf{e_{j}}&#124;&#124;_{2}$
    |  |'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| (12) |  | $\displaystyle\min\sum^{n}_{i,j=1}\textbf{T}_{ij}&#124;&#124;\mathbf{e_{i}}-\mathbf{e_{j}}&#124;&#124;_{2}$
    |  |'
- en: '|  | $\displaystyle s.t.,\sum^{n}_{j=1}\textbf{T}_{ij}=d_{i},\forall i\in\{i,...,n\},\sum^{n}_{i=1}\textbf{T}_{ij}=d_{i}^{\prime},\forall
    j\in\{i,...,n\}$ |  |'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle s.t.,\sum^{n}_{j=1}\textbf{T}_{ij}=d_{i},\forall i\in\{i,...,n\},\sum^{n}_{i=1}\textbf{T}_{ij}=d_{i}^{\prime},\forall
    j\in\{i,...,n\}$ |  |'
- en: where $\mathbf{e_{i}}$ and $\mathbf{e_{j}}$ ared word embedding of word $i$
    and word $j$ respectively. $n$ is the number of words. $\textbf{T}\in\mathcal{R}^{n\times
    n}$ be a flow matrix, where $\textbf{T}_{ij}\leq 0$ denotes how much of word $i$
    in $\mathbf{d}$ travels to word $j$ in $\mathbf{d}^{\prime}$. $\mathbf{d}$ and
    $\mathbf{d}^{\prime}$ are normalized bag-of-words vectors of the source document
    and target document respectively.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{e_{i}}$ 和 $\mathbf{e_{j}}$ 分别是单词 $i$ 和单词 $j$ 的词嵌入。$n$ 是单词的数量。$\textbf{T}\in\mathcal{R}^{n\times
    n}$ 是一个流矩阵，其中 $\textbf{T}_{ij}\leq 0$ 表示单词 $i$ 在 $\mathbf{d}$ 中转移到单词 $j$ 在 $\mathbf{d}^{\prime}$
    中的量。$\mathbf{d}$ 和 $\mathbf{d}^{\prime}$ 分别是源文档和目标文档的归一化词袋向量。
- en: –
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Number of changes is a simple way to measure the edits and it is adopted in
    some reviewed literature.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变更数量是测量编辑的一种简单方法，并且在一些已审阅的文献中采用了这种方法。
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Jaccard similarity coefficient is used for measuring similarity of finite sample
    sets utilising intersection and union of the sets.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Jaccard 相似度系数用于测量有限样本集的相似性，利用集合的交集和并集。
- en: '| (13) |  | $\displaystyle J(A,B)=\frac{&#124;A\cap B&#124;}{&#124;A\cup B&#124;}$
    |  |'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (13) |  | $\displaystyle J(A,B)=\frac{&#124;A\cap B&#124;}{&#124;A\cup B&#124;}$
    |  |'
- en: In texts, $A$, $B$ are two documents (or sentences). $|A\cap B|$ denotes the
    number of words appear in both documents, $|A\cup B|$ refers to the number of
    unique words in total.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在文本中，$A$ 和 $B$ 是两个文档（或句子）。$|A\cap B|$ 表示出现在两个文档中的单词数，$|A\cup B|$ 表示总的唯一单词数。
- en: '4\. Attacking Neural Models in NLP: State-of-The-Art'
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 攻击神经模型：最新进展
- en: In this section, we first introduce the categories of attack methods on textual
    deep learning models and then highlight the state-of-the-art research works, aiming
    to identify the most promising advances in recent years.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍对文本深度学习模型的攻击方法类别，然后重点介绍最新的研究成果，旨在确定近年来最有前景的进展。
- en: 4.1\. Categories of Attack Methods on Textual Deep Learning Models
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 文本深度学习模型攻击方法的分类
- en: 'We categorize existing adversarial attack methods based on different criteria.
    Figure [1](#S4.F1 "Figure 1 ‣ 4.1\. Categories of Attack Methods on Textual Deep
    Learning Models ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial
    Attacks on Deep Learning Models in Natural Language Processing: A Survey") generalizes
    the categories.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据不同的标准对现有的对抗攻击方法进行了分类。图 [1](#S4.F1 "图 1 ‣ 4.1\. 文本深度学习模型攻击方法的分类 ‣ 4\. 攻击神经模型：最新进展
    ‣ 深度学习模型在自然语言处理中的对抗攻击：综述") 概述了这些类别。
- en: '![Refer to caption](img/0c646270239be613efa6be0d96f3b0f8.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0c646270239be613efa6be0d96f3b0f8.png)'
- en: Figure 1\. Categories of Adversarial Attack Methods on Textual Deep Learning
    Models
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 文本深度学习模型的对抗攻击方法分类
- en: 'In this article, five strategies are used to categorize the attack methods:
    i) By model access group refers to the knowledge of attacked model when the attack
    is performed. In the following section, we focus on the discussion using this
    categorization strategy. ii) By application group refers the methods via different
    NLP applications. More detailed discussion will be provided in Section [4.5](#S4.SS5
    "4.5\. Benchmark Datasets by Applications ‣ 4\. Attacking Neural Models in NLP:
    State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey"). iii) By target group refers to the goal of the attack
    is enforcing incorrect prediction or targeting specific results. iv) By granularity
    group considers on what granularity the model is attacked. v) We have discussed
    the attacked DNNs in Section [2.2](#S2.SS2 "2.2\. Deep Learning in NLP ‣ 2\. Overview
    of Adversarial Attacks and Deep Learning Techniques in Natural Language Processing
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey"). In following sections, we will continuously provide information about
    different categories that the methods belong to.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，采用了五种策略来对攻击方法进行分类：i) 通过模型访问组，指的是在进行攻击时对被攻击模型的知识。在下面的部分，我们将重点讨论使用这种分类策略。ii)
    通过应用组，指的是通过不同的 NLP 应用程序的方法。更详细的讨论将在第 [4.5](#S4.SS5 "4.5\. 按应用的基准数据集 ‣ 4\. 攻击神经模型：最新进展
    ‣ 深度学习模型在自然语言处理中的对抗攻击：综述") 节提供。iii) 通过目标组，指的是攻击的目标是强制错误预测或针对特定结果。iv) 通过粒度组，考虑模型攻击的粒度。v)
    我们在第 [2.2](#S2.SS2 "2.2\. 自然语言处理中的深度学习 ‣ 2\. 对抗攻击和深度学习技术概述 ‣ 深度学习模型在自然语言处理中的对抗攻击：综述")
    节讨论了被攻击的 DNN。在接下来的部分中，我们将持续提供关于方法所属不同类别的信息。
- en: 'One important group of methods need to be noted is the cross-modal attacks,
    in which the attacked model consider the tasks dealing with multi-modal data,
    e.g., image and text data. They are not attacks for pure textual DNNs, hence we
    discuss this category of methods separately in Section [4.4](#S4.SS4 "4.4\. Multi-modal
    Attacks ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey") in addition
    to white-box attacks in Section [4.2](#S4.SS2 "4.2\. White-Box Attack ‣ 4\. Attacking
    Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey") and black-box attacks in Section
    [4.3](#S4.SS3 "4.3\. Black-box Attack ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey").'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要注意的重要方法组是跨模态攻击，其中攻击模型处理多模态数据任务，例如图像和文本数据。它们并不是针对纯文本DNN的攻击，因此我们在[4.4](#S4.SS4
    "4.4\. 多模态攻击 ‣ 4\. 攻击NLP中的神经模型：最前沿 ‣ 自然语言处理中的深度学习模型的对抗攻击：调查")节中单独讨论这类方法，除此之外还有[4.2](#S4.SS2
    "4.2\. 白盒攻击 ‣ 4\. 攻击NLP中的神经模型：最前沿 ‣ 自然语言处理中的深度学习模型的对抗攻击：调查")节中的白盒攻击和[4.3](#S4.SS3
    "4.3\. 黑盒攻击 ‣ 4\. 攻击NLP中的神经模型：最前沿 ‣ 自然语言处理中的深度学习模型的对抗攻击：调查")节中的黑盒攻击。
- en: 4.2\. White-Box Attack
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 白盒攻击
- en: In white-box attack, the attack requires the access to the model’s full information,
    including architecture, parameters, loss functions, activation functions, input
    and output data. White-box attacks typically approximate the worst-case attack
    for a particular model and input, incorporating a set of perturbations. This adversary
    strategy is often very effective. In this section, we group white-box attacks
    on textual DNNs into seven categories.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在白盒攻击中，攻击者需要访问模型的全部信息，包括架构、参数、损失函数、激活函数、输入和输出数据。白盒攻击通常会对特定模型和输入进行最坏情况的近似，结合一系列扰动。这种对抗策略通常非常有效。在本节中，我们将白盒攻击在文本DNN上的分类分为七类。
- en: 4.2.1\. FGSM-based
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 基于FGSM的
- en: 'FGSM is one of the first attack methods on images (Section [3.1](#S3.SS1.SSS0.Px2
    "Fast Gradient Sign Method (FGSM) ‣ 3.1\. Crafting Adversarial Examples: Inspiring
    Works in Computer Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep
    Learning Models in Natural Language Processing: A Survey")). It gains many follow-up
    works in attacking textual DNNs. TextFool (Liang et al., [2017](#bib.bib79)) uses
    the concept of FGSM to approximate the contribution of text items that possess
    significant contribution to the text classification task. Instead of using sign
    of the cost gradient in FGSM, this work considers the magnitude. The authors proposed
    three attacks: insertion, modification and removal. Specifically, they computed
    cost gradient $\Delta_{x}J(f,x,c^{\prime})$ of each training sample $x$, employing
    back propagation, where $f$ is the model function, $x$ is the original data sample,
    and $c^{\prime}$ is the target text class. Then they identified the characters
    that contain the dimensions with the highest gradient magnitude and named them
    hot characters. Phrases that contain enough hot characters and occur the most
    frequently are chosen as Hot Training Phrases (HTPs). In the insertion strategy,
    adversarial examples are crafted by inserting a few HTPs of the target class $c^{\prime}$
    nearby the phrases with significant contribution to the original class $c$. The
    authors further leveraged external sources like Wikipedia and forged fact to select
    the valid and believable sentences. In the modification Strategy, the authors
    identified Hot Sample Phrase (HSP) to the current classification using similar
    way of identifying HTPs. Then they replaced the characters in HTPs by common misspellings
    or characters visually similar. In the removal strategy, the inessential adjective
    or adverb in HSPs are removed. The three strategies and their combinations are
    evaluated on a CNN text classifier (Zhang et al., [2015](#bib.bib156)). However,
    these methods are performed manually, as mentioned by the authors.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'FGSM 是图像攻击方法中的最早方法之一（见 [3.1](#S3.SS1.SSS0.Px2 "Fast Gradient Sign Method (FGSM)
    ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\.
    From Image to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey")）。它在攻击文本 DNNs 上引发了许多后续工作。TextFool（Liang et al., [2017](#bib.bib79)）使用
    FGSM 的概念来近似对文本分类任务贡献显著的文本项的贡献。该工作没有使用 FGSM 中成本梯度的符号，而是考虑了其幅度。作者提出了三种攻击：插入、修改和删除。具体来说，他们计算了每个训练样本
    $x$ 的成本梯度 $\Delta_{x}J(f,x,c^{\prime})$，使用反向传播，其中 $f$ 是模型函数，$x$ 是原始数据样本，$c^{\prime}$
    是目标文本类别。然后，他们识别了包含最高梯度幅度的维度的字符，并将其命名为热字符。包含足够热字符且出现频率最高的短语被选为热训练短语（HTPs）。在插入策略中，通过在对原始类别
    $c$ 有显著贡献的短语附近插入几个目标类别 $c^{\prime}$ 的 HTPs 来制作对抗样本。作者进一步利用外部来源，如 Wikipedia 和伪造事实，选择有效且可信的句子。在修改策略中，作者使用类似于识别
    HTPs 的方法确定当前分类的热样本短语（HSP）。然后，他们通过常见的拼写错误或视觉上相似的字符替换 HTPs 中的字符。在删除策略中，删除 HSPs 中的不必要的形容词或副词。这三种策略及其组合在
    CNN 文本分类器（Zhang et al., [2015](#bib.bib156)）上进行了评估。然而，如作者所述，这些方法是手动执行的。'
- en: The work in (Samanta and Mehta, [2018](#bib.bib118)) adopted the same idea as
    TextFool, but it provides a removal-addition-replacement strategy that firstly
    tries to remove the adverb ($w_{i}$) which contributed the most to the text classification
    task (measured using loss gradient). If the output sentences in this step have
    incorrect grammar, the method will insert a word $p_{j}$ before $w_{i}$. $p_{j}$
    is selected from a candidate pool, in which the synonyms and typos and genre specific
    keywords (identified via term frequency) are candidate words. If the output cannot
    satisfy the highest cost gradient for all the $p_{j}$, then the method replaces
    $w_{i}$ with $p_{j}$. The authors showed that their method is more effective than
    TextFool. As the method ordered the words with their contribution ranking and
    crafted adversarial samples according to the order, it is a greedy method that
    always get the minimum manipulation until the output changes. To avoid being detected
    by the human eyes, the authors constrained the replaced/added words to not affect
    the grammar and POS of the original words.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: (Samanta 和 Mehta, [2018](#bib.bib118)) 的工作采用了与 TextFool 相同的思路，但提供了一种移除-添加-替换策略，该策略首先尝试移除对文本分类任务贡献最大的副词
    ($w_{i}$)（使用损失梯度进行度量）。如果该步骤中的输出句子语法不正确，则该方法会在 $w_{i}$ 之前插入一个词 $p_{j}$。$p_{j}$
    从候选池中选择，其中同义词、错别字和特定领域的关键词（通过词频识别）都是候选词。如果输出不能满足所有 $p_{j}$ 的最高成本梯度，那么该方法将 $w_{i}$
    替换为 $p_{j}$。作者表明他们的方法比 TextFool 更有效。由于该方法按贡献排名对词进行排序，并根据顺序制造对抗样本，因此是一种贪婪方法，总是进行最小的操作直到输出发生变化。为了避免被人眼检测到，作者将替换/添加的词限制为不影响原词的语法和词性。
- en: In malware detection, an portable executable (PE) is represented by binary vector
    $\{x_{1},...,x_{m}\}$, $x_{i}\in\{0,1\}$ that using 1 and 0 to indicate the PE
    is present or not where $m$ is the number of PEs. Using PEs’ vectors as features,
    malware detection DNNs can identify the malicious software. It is not a typical
    textual application, but also targets discrete data, which share similar methods
    with textual applications. The authors of (Al-Dujaili et al., [2018a](#bib.bib4))
    investigated the methods to generate binary-encoded adversarial examples. To preserve
    the functionality of the adversarial examples, they incorporated four bounding
    methods to craft perturbations. The first two methods adopt FSGM^k (Kurakin et al.,
    [2017](#bib.bib69)), the multi-step variant of FGSM, restricting the perturbations
    in a binary domain by introducing deterministic rounding (dFGSM^k) and randomized
    rounding (rFGSM^k). These two bounding methods are similar to $L_{\infty}$-ball
    constraints on images (Goodfellow et al., [2015](#bib.bib43)). The third method
    multi-step Bit Gradient Ascent (BGA^k) sets the bit of the $j$-th feature if the
    corresponding partial derivative of the loss is greater than or equal to the loss
    gradient’s $L_{2}$-norm divided by $\sqrt{m}$. The fourth method multi-step Bit
    Coordinate Ascent (BCA^k) updates one bit in each step by considering the feature
    with the maximum corresponding partial derivative of the loss. These two last
    methods actually visit multiple feasible vertices. The work also proposed a adversarial
    learning framework aims to robustify the malware detection model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在恶意软件检测中，便携式可执行文件（PE）由二进制向量 $\{x_{1},...,x_{m}\}$ 表示，其中 $x_{i}\in\{0,1\}$ 用于指示PE是否存在，$m$
    是PE的数量。使用PE的向量作为特征，恶意软件检测DNNs能够识别恶意软件。这不是一种典型的文本应用，但也针对离散数据，采用与文本应用类似的方法。 (Al-Dujaili
    et al., [2018a](#bib.bib4)) 的作者研究了生成二进制编码对抗样本的方法。为了保留对抗样本的功能性，他们结合了四种边界方法来制造扰动。前两种方法采用
    FSGM^k (Kurakin et al., [2017](#bib.bib69))，FGSM 的多步骤变体，通过引入确定性舍入 (dFGSM^k) 和随机舍入
    (rFGSM^k) 限制扰动在二进制领域内。这两种边界方法类似于图像上的 $L_{\infty}$-ball 约束 (Goodfellow et al.,
    [2015](#bib.bib43))。第三种方法多步骤位梯度上升 (BGA^k) 在对应损失的部分导数大于或等于损失梯度的 $L_{2}$-范数除以 $\sqrt{m}$
    时，设置第 $j$ 个特征的位。第四种方法多步骤位坐标上升 (BCA^k) 通过考虑具有最大对应损失部分导数的特征来更新每一步中的一个位。这两种最后的方法实际上访问了多个可行的顶点。该工作还提出了一个对抗学习框架，旨在增强恶意软件检测模型的鲁棒性。
- en: (Rosenberg et al., [2017](#bib.bib115)) also attacks malware detection DNNs.
    The authors made perturbations on the embedding presentation of the binary sequences
    and reconstructed the perturbed examples to its binary representation. Particularly,
    they appended a uniformly random sequence of bytes (payload) to the original binary
    sequence. Then they embed the new binary to its embedding and performed FGSM only
    on the embedding of the payload. The perturbation is performed iteratively until
    the detector output incorrect prediction. Since the perturbation is only performed
    on payload, instead of the input, this method will preserve the functionality
    of the malware. Finally, they reconstructed adverse embedding to valid binary
    file by mapping the adversary embedding to its closest neighbour in the valid
    embedding space.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: (Rosenberg 等，[2017](#bib.bib115)) 也攻击了恶意软件检测 DNNs。作者对二进制序列的嵌入表示进行了扰动，并将扰动后的示例重建为其二进制表示。特别是，他们在原始二进制序列中附加了一段均匀随机的字节序列（有效负载）。然后，他们将新的二进制序列嵌入其嵌入空间，并仅对有效负载的嵌入执行
    FGSM。扰动是迭代进行的，直到检测器输出错误的预测。由于扰动仅在有效负载上进行，而不是在输入上，因此这种方法将保留恶意软件的功能。最后，他们通过将对抗嵌入映射到有效嵌入空间中最接近的邻居，重建了有效的二进制文件。
- en: 'Many works directly adopt FGSM for adversarial training, i.e., put it as regularizer
    when training the model. We will discuss some representatives in Section [5](#S5
    "5\. Defense ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey").'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究直接采用 FGSM 进行对抗训练，即在训练模型时将其作为正则化项。我们将在第 [5](#S5 "5\. 防御 ‣ 自然语言处理中的深度学习模型的对抗攻击：调查")
    节中讨论一些代表性工作。
- en: 4.2.2\. JSMA-based
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 基于 JSMA
- en: 'JSMA is another pioneer work on attacking neural models for image applications
    (refers to Section [3.1](#S3.SS1.SSS0.Px3 "Jacobian Saliency Map Adversary (JSMA)
    ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\.
    From Image to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey")). The work (Papernot et al., [2016b](#bib.bib105)) used
    forward derivative as JSMA to find the most contributable sequence towards the
    adversary direction. The network’s Jacobian had been calculated by leveraging
    computational graph unfolding (Mozer, [1995](#bib.bib98)). They crafted adversarial
    sequences for two types of RNN models whose output is categorical and sequential
    data respectively. For categorical RNN, the adversarial examples are generated
    by considering the Jacobian $Jacb_{F}[:,j]$ column corresponding to one of the
    output components $j$. Specifically, for each word $i$, they identified the direction
    of perturbation by:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: JSMA 是攻击图像应用神经模型的另一项开创性工作（参考第 [3.1](#S3.SS1.SSS0.Px3 "雅可比显著性图对抗者 (JSMA) ‣ 3.1\.
    制作对抗示例：计算机视觉中的启发性工作 ‣ 3\. 从图像到文本 ‣ 自然语言处理中的深度学习模型的对抗攻击") 节）。这项工作（Papernot 等，[2016b](#bib.bib105)）使用前向导数作为
    JSMA，以寻找对抗方向上最具贡献的序列。通过利用计算图展开（Mozer，[1995](#bib.bib98)），计算了网络的雅可比矩阵。他们为两种类型的
    RNN 模型制作了对抗序列，这些模型分别处理分类和序列数据。对于分类 RNN，通过考虑与输出组件 $j$ 对应的雅可比 $Jacb_{F}[:,j]$ 列，生成对抗示例。具体而言，对于每个词
    $i$，他们通过以下方式确定扰动方向：
- en: '| (14) |  | $\displaystyle\text{sign}(Jacb_{F}(x^{\prime})[i,g(x^{\prime})])$
    |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\displaystyle\text{sign}(Jacb_{F}(x^{\prime})[i,g(x^{\prime})])$
    |  |'
- en: '| (15) |  | $\displaystyle g(x^{\prime})=\arg\max_{0,1}(p_{j})$ |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\displaystyle g(x^{\prime})=\arg\max_{0,1}(p_{j})$ |  |'
- en: where $p_{j}$ is the output probability of the target class. As in JSMA, they
    instead to choose logit to replace probability in this equation. They further
    projected the perturbed examples onto the closest vector in the embedding space
    to get valid embedding. For sequential RNN, after computing the Jacobian matrix,
    they altered the subset of input setps $\{i\}$ with high Jacobian values $Jacb_{F}[i,j]$
    and low Jacobian values $Jacb_{F}[i,k]$ for $k\neq j$ to achieve modification
    on a subset of output steps $\{j\}$.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{j}$ 是目标类别的输出概率。与 JSMA 相似，他们在这个方程中选择了 logit 来替代概率。他们进一步将扰动的示例投影到嵌入空间中最接近的向量，以获得有效的嵌入。对于序列
    RNN，计算雅可比矩阵后，他们修改了输入步骤集合 $\{i\}$ 中具有高雅可比值 $Jacb_{F}[i,j]$ 和低雅可比值 $Jacb_{F}[i,k]$
    的子集（其中 $k\neq j$），以对输出步骤子集 $\{j\}$ 进行修改。
- en: '(Grosse et al., [2016](#bib.bib46)) (and (Grosse et al., [2017](#bib.bib47)))
    is the first work to attack neural malware detector. They firstly performed feature
    engineering and obtained more than 545K static features for software applications.
    They used binary indicator feature vector to represent an application. Then they
    crafted adversarial examples on the input feature vectors by adopting JSMA: they
    computed gradient of model Jacobian to estimate the perturbation direction. Later,
    the method chooses a perturbation $\eta$ given input sample that with maximal
    positive gradient into the target class. In particular, the perturbations are
    chosen via index $i$, satisfying:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: (Grosse et al., [2016](#bib.bib46))（以及 (Grosse et al., [2017](#bib.bib47)))
    是首个攻击神经恶意软件检测器的研究。他们首先进行了特征工程，并为软件应用程序获得了超过 545K 的静态特征。他们使用二进制指示特征向量来表示一个应用程序。然后，他们通过采用
    JSMA 方法在输入特征向量上制作对抗样本：计算模型雅可比矩阵的梯度以估计扰动方向。随后，该方法选择一个扰动 $\eta$，使得输入样本在目标类别中具有最大的正梯度。特别地，扰动通过索引
    $i$ 选择，满足以下条件：
- en: '| (16) |  | $\displaystyle i=\arg\max_{j\in[1,m],\mathbf{X}_{j}=y^{\prime}}f_{y}^{\prime}(\mathbf{X}_{j})$
    |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\displaystyle i=\arg\max_{j\in[1,m],\mathbf{X}_{j}=y^{\prime}}f_{y}^{\prime}(\mathbf{X}_{j})$
    |  |'
- en: where $y^{\prime}$ is the target class, $m$ is the number of features. On the
    binary feature vectors, the perturbations are ($0\rightarrow 1$) or ($1\rightarrow
    0$). This method preserves the functionality of the applications. In order to
    ensure that modifications caused by the perturbations do not change the application
    much, which will keep the malware application’s functionality complete, the authors
    used the $L_{1}$ norm to bound the overall number of features modified, and further
    bound the number of features to 20. In addition, the authors provided three methods
    to defense against the attacks, namely feature reduction, distillation and adversarial
    training. They found adversarial training is the most effective defense method.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y^{\prime}$ 是目标类别，$m$ 是特征的数量。在二元特征向量中，扰动为（$0\rightarrow 1$）或（$1\rightarrow
    0$）。该方法保留了应用程序的功能。为了确保扰动所导致的修改不会对应用程序产生重大变化，从而保持恶意软件应用程序的功能完整性，作者使用了 $L_{1}$ 范数来限制修改的特征总数，并进一步将特征数限制为
    20。此外，作者提供了三种防御攻击的方法，即特征减少、蒸馏和对抗训练。他们发现对抗训练是最有效的防御方法。
- en: 4.2.3\. C&W-based
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 基于 C&W
- en: 'The work in (Sun et al., [2018](#bib.bib131)) adopted C&W method (refers to
    Section [3.1](#S3.SS1 "3.1\. Crafting Adversarial Examples: Inspiring Works in
    Computer Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey")) for attacking predictive models
    of medical records. The aim is to detect susceptible events and measurements in
    each patient’s medical records, which provide guidance for the clinical usage.
    The authors used standard LSTM as predictive model. Given the patient EHR data
    being presented by a matrix $X^{i}\in\mathbf{R}^{d\times t_{i}}$ ($d$ is the number
    of medical features and $t_{i}$ is the time index of medical check), the generation
    of the adversarial example is formulated as:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '(Sun et al., [2018](#bib.bib131)) 的研究采用了 C&W 方法（参见第 [3.1](#S3.SS1 "3.1\. Crafting
    Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\. From Image to Text
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey") 节）来攻击医疗记录的预测模型。其目的是检测每个患者医疗记录中易受攻击的事件和测量，以提供临床使用的指导。作者使用了标准 LSTM 作为预测模型。给定患者的
    EHR 数据呈现为矩阵 $X^{i}\in\mathbf{R}^{d\times t_{i}}$（$d$ 是医疗特征的数量，$t_{i}$ 是医疗检查的时间索引），对抗样本的生成公式为：'
- en: '| (17) |  | $\displaystyle\min_{\hat{X}}\max\{-\epsilon,[logit(\mathbf{x^{\prime}})]_{y}-[logit(\mathbf{x})]_{y^{\prime}}\}+\lambda&#124;&#124;\mathbf{x^{\prime}}-\mathbf{x}&#124;&#124;_{1}$
    |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\displaystyle\min_{\hat{X}}\max\{-\epsilon,[logit(\mathbf{x^{\prime}})]_{y}-[logit(\mathbf{x})]_{y^{\prime}}\}+\lambda&#124;&#124;\mathbf{x^{\prime}}-\mathbf{x}&#124;&#124;_{1}$
    |  |'
- en: where $logit(\cdot)$ denotes the logit layer output, $\lambda$ is the regularization
    parameter which controls the $L_{1}$ norm regularization, $y^{\prime}$ is the
    targeted label while $y$ is the original label. After generating adversarial examples,
    the authors picked the optimal example according to their proposed evaluation
    scheme that considers both the perturbation magnitude and the structure of the
    attacks. Finally they used the adversarial example to compute the susceptibility
    score for the EHR as well as the cumulative susceptibility score for different
    measurements.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $logit(\cdot)$ 表示 logit 层输出，$\lambda$ 是控制 $L_{1}$ 范数正则化的正则化参数，$y^{\prime}$
    是目标标签，而 $y$ 是原始标签。在生成对抗样本后，作者根据其提出的评价方案选择了最佳样本，该方案考虑了扰动幅度和攻击结构。最后，他们使用对抗样本计算了
    EHR 的易感性评分以及不同测量的累积易感性评分。
- en: 'Seq2Sick (Cheng et al., [2018](#bib.bib25)) attacked the seq2seq models using
    two targeted attacks: non-overlapping attack and keywords attack. For non-overlapping
    attack, the authors aimed to generate adversarial sequences that are entirely
    different from the original outputs. They proposed a hinge-like loss function
    that optimizes on the logit layer of the neural network:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Seq2Sick (Cheng et al., [2018](#bib.bib25)) 使用两种有针对性的攻击攻击 seq2seq 模型：非重叠攻击和关键词攻击。对于非重叠攻击，作者旨在生成与原始输出完全不同的对抗序列。他们提出了一种类似
    hinge 的损失函数，用于优化神经网络的 logit 层：
- en: '| (18) |  | $\displaystyle\sum^{&#124;K&#124;}_{i=1}\min_{t\in[M]}\{m_{t}(\max\{-\epsilon,\max_{y\neq
    k_{i}}\{z_{t}^{(y)}\}-z_{t}^{(k_{i})}\})\}$ |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\displaystyle\sum^{\lvert K \rvert}_{i=1}\min_{t\in[M]}\{m_{t}(\max\{-\epsilon,\max_{y\neq
    k_{i}}\{z_{t}^{(y)}\}-z_{t}^{(k_{i})}\})\}$ |  |'
- en: 'where $\{s_{t}\}$ are the original output sequence, $\{z_{t}\}$ indicates the
    logit layer outputs of the adversarial example. For the keyword attack, targeted
    keywords are expected to appear in the output sequence. The authors also put the
    optimization on the logit layer and tried to ensure that the targeted keyword’s
    logit be the largest among all words. Further more, they defined mask function
    $m$ to solve the keyword collision problem. The loss function then becomes:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\{s_{t}\}$ 是原始输出序列，$\{z_{t}\}$ 表示对抗样本的 logit 层输出。对于关键词攻击，期望目标关键词出现在输出序列中。作者还对
    logit 层进行了优化，并尝试确保目标关键词的 logit 在所有单词中最大。此外，他们定义了掩码函数 $m$ 来解决关键词碰撞问题。损失函数变为：
- en: '| (19) |  | $\displaystyle L_{keywords}=\sum^{&#124;K&#124;}_{i=1}\min_{t\in[M]}\{m_{t}(\max\{-\epsilon,\max_{y\neq
    k_{i}}\{z_{t}^{(y)}\}-z_{t}^{(k_{i})}\})\}$ |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\displaystyle L_{keywords}=\sum^{\lvert K \rvert}_{i=1}\min_{t\in[M]}\{m_{t}(\max\{-\epsilon,\max_{y\neq
    k_{i}}\{z_{t}^{(y)}\}-z_{t}^{(k_{i})}\})\}$ |  |'
- en: 'where $k_{i}$ denotes the $i$-th word in output vocabulary. To ensure the generated
    word embedding is valid, this work also considers two regularization methods:
    group lasso regularization to enforce the group sparsity, and group gradient regularization
    to make adversaries are in the permissible region of the embedding space.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k_{i}$ 表示输出词汇中的第 $i$ 个词。为了确保生成的词嵌入是有效的，该工作还考虑了两种正则化方法：群体套索正则化以强制群体稀疏，以及群体梯度正则化以使对抗样本位于嵌入空间的可接受区域。
- en: 4.2.4\. Direction-based
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 基于方向
- en: 'HotFlip (Ebrahimi et al., [2018](#bib.bib32)) performs atomic flip operations
    to generate adversarial examples. Instead of leveraging gradient of loss, HotFlip
    use the directional derivatives. Specifically, HotFlip represents character-level
    operations, i.e., swap, insert and delete, as vectors in the input space and estimated
    the change in loss by directional derivatives with respect to these vectors. Specifically,
    given one-hot representation of inputs, a character flip in the $j$-th character
    of the $i$-th word (a$\rightarrow$b) can be represented by the vector:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: HotFlip (Ebrahimi et al., [2018](#bib.bib32)) 执行原子翻转操作以生成对抗样本。与利用损失梯度不同，HotFlip
    使用方向导数。具体而言，HotFlip 将字符级操作（即交换、插入和删除）表示为输入空间中的向量，并通过这些向量的方向导数估计损失的变化。具体而言，给定输入的
    one-hot 表示，第 $i$ 个词的第 $j$ 个字符的字符翻转（a$\rightarrow$b）可以通过向量表示为：
- en: '| (20) |  | $\displaystyle\overrightarrow{v}_{ijb}=(\mathbf{0},..;(\mathbf{0},..(0,..-1,0,..,1,0)_{j},..\mathbf{0})_{i};\mathbf{0},..)$
    |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\displaystyle\overrightarrow{v}_{ijb}=(\mathbf{0},..;(\mathbf{0},..(0,..-1,0,..,1,0)_{j},..\mathbf{0})_{i};\mathbf{0},..)$
    |  |'
- en: 'where -1 and 1 are in the corresponding positions for the a-th and b-th characters
    of the alphabet, respectively. Then the best character swap can be found by maximizing
    a first-order approximation of loss change via directional derivative along the
    operation vector:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 -1 和 1 分别在字母表中的第 a 个和第 b 个字符对应的位置。然后，通过最大化沿操作向量的损失变化的一阶近似，可以找到最佳字符交换。
- en: '| (21) |  | $\displaystyle\max\nabla_{x}J(x,y)^{T}\cdot\overrightarrow{v}_{ijb}=\max_{ijv}\frac{\partial
    J^{(b)}}{\partial x_{ij}}-\frac{\partial J^{(a)}}{\partial x_{ij}}$ |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $\displaystyle\max\nabla_{x}J(x,y)^{T}\cdot\overrightarrow{v}_{ijb}=\max_{ijv}\frac{\partial
    J^{(b)}}{\partial x_{ij}}-\frac{\partial J^{(a)}}{\partial x_{ij}}$ |  |'
- en: where $J(x,y)$ is the model’s loss function with input $x$ and true output $y$.
    Similarly, insertion at the $j$-th position of the $i$-th word can also be treated
    as a character flip, followed by more flips as characters are shifted to the right
    until the end of the word. The character deletion is a number of character flips
    as characters are shifted to the left. Using the beam search, HotFlip efficiently
    finds the best directions for multiple flips.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $J(x,y)$ 是模型的损失函数，输入为 $x$ 和真实输出 $y$。类似地，将第 $i$ 个词的第 $j$ 个位置的插入也可以视为字符翻转，随后随着字符向右移动直到词的末尾进行更多翻转。字符删除是将字符向左移动的多次字符翻转。使用束搜索，HotFlip
    能有效地找到多次翻转的最佳方向。
- en: The work (Ebrahimi et al., [[n. d.]](#bib.bib31)) extended HotFlip by adding
    targeted attacks. Besides the swap, insertion and deletion as provided in HotFlip,
    the authors proposed a controlled attack, which is to remove a specific word from
    the output, and a targeted attack, which is to replace a specific word by a chosen
    one. To achieve these attacks, they maximized the loss function $J(x,y_{t})$ and
    minimize $J(x,y_{t}^{\prime})$, where $t$ is the target word for the controlled
    attack, and $t^{\prime}$ is the word to replace $t$. Further, they proposed three
    types of attacks that provide multiple modifications. In one-hot attack, they
    manipulated all the words in the text with the best operation. In Greedy attack,
    they make another forward and backward pass, in addition to picking the best operation
    from the whole text. In Beam search attack, they replaced the search method in
    greedy with the beam search. In all the attacks proposed in this work, the authors
    set threshold for the maximum number of changes, e.g., 20% of characters are allowed
    to be changed.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作 (Ebrahimi et al., [[n. d.]](#bib.bib31)) 通过添加目标攻击扩展了 HotFlip。除了 HotFlip
    中提供的交换、插入和删除外，作者提出了一种控制攻击，即从输出中移除特定的词，以及一种目标攻击，即用选择的词替换特定的词。为了实现这些攻击，他们最大化了损失函数
    $J(x,y_{t})$ 并最小化 $J(x,y_{t}^{\prime})$，其中 $t$ 是控制攻击的目标词，$t^{\prime}$ 是替换 $t$
    的词。此外，他们提出了三种提供多种修改的攻击类型。在一热攻击中，他们对文本中的所有词进行最佳操作的处理。在贪婪攻击中，他们除了从整个文本中挑选最佳操作外，还进行了一次前向和后向传递。在束搜索攻击中，他们将贪婪的搜索方法替换为束搜索。在这项工作提出的所有攻击中，作者设置了最大变化数量的阈值，例如，允许更改的字符占20%。
- en: '| Strategy | Work | Granularity | Target | Attacked Models | Perturb Ctrl.
    | App. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 工作 | 粒度 | 目标 | 攻击模型 | 扰动控制 | 应用 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| FSGM-based | (Liang et al., [2017](#bib.bib79)) | character,word | Y | CNN
    (Zhang et al., [2015](#bib.bib156)) | $L_{\infty}$ | TC |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 基于FSGM | (Liang et al., [2017](#bib.bib79)) | 字符，词 | 是 | CNN (Zhang et al.,
    [2015](#bib.bib156)) | $L_{\infty}$ | TC |'
- en: '| (Samanta and Mehta, [2018](#bib.bib118)) | word | N | CNN (Zhang et al.,
    [2015](#bib.bib156)) | $L_{\infty}$, Grammar and POS correctness | TC |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| (Samanta and Mehta, [2018](#bib.bib118)) | 词 | 否 | CNN (Zhang et al., [2015](#bib.bib156))
    | $L_{\infty}$，语法和词性正确性 | TC |'
- en: '| (Rosenberg et al., [2017](#bib.bib115)) | PE | binary | CNN in (Dahl et al.,
    [2013](#bib.bib28)) | Boundaries employ $L_{\infty}$ and $L_{2}$ | MAD |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| (Rosenberg et al., [2017](#bib.bib115)) | PE | 二进制 | CNN 在 (Dahl et al.,
    [2013](#bib.bib28)) | 边界使用 $L_{\infty}$ 和 $L_{2}$ | MAD |'
- en: '| (Al-Dujaili et al., [2018a](#bib.bib4)) | PE embedding | binary | MalConv
    (Raff et al., [[n. d.]](#bib.bib111)) | $L_{\infty}$ | MAD |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| (Al-Dujaili et al., [2018a](#bib.bib4)) | PE 嵌入 | 二进制 | MalConv (Raff et
    al., [[n. d.]](#bib.bib111)) | $L_{\infty}$ | MAD |'
- en: '| JSMA-based | (Papernot et al., [2016b](#bib.bib105)) | word embedding | binary
    | LSTM | – | TC |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 基于JSMA | (Papernot et al., [2016b](#bib.bib105)) | 词嵌入 | 二进制 | LSTM | – |
    TC |'
- en: '| (Grosse et al., [2016](#bib.bib46), [2017](#bib.bib47)) | application features
    | binary | Feed forward | $L_{1}$ | MAD |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| (Grosse et al., [2016](#bib.bib46), [2017](#bib.bib47)) | 应用特征 | 二进制 | 前馈
    | $L_{1}$ | MAD |'
- en: '| C&W-based | (Sun et al., [2018](#bib.bib131)) | medical features | Y | LSTM
    | $L_{1}$ | MSP |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 基于C&W | (Sun et al., [2018](#bib.bib131)) | 医疗特征 | 是 | LSTM | $L_{1}$ | MSP
    |'
- en: '| (Cheng et al., [2018](#bib.bib25)) | word embedding | Y | OpenNMT-py (Klein
    et al., [2017](#bib.bib65)) | $L_{2}$+gradient regularization | TS, MT |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| (Cheng 等人，[2018](#bib.bib25)) | 词嵌入 | 是 | OpenNMT-py (Klein 等人，[2017](#bib.bib65))
    | $L_{2}$+梯度正则化 | TS, MT |'
- en: '| Direction-based | (Ebrahimi et al., [2018](#bib.bib32)) | character | N |
    CharCNN-LSTM (Kim et al., [2016](#bib.bib62)) | – | TC |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 基于方向的 | (Ebrahimi 等人，[2018](#bib.bib32)) | 字符 | 否 | CharCNN-LSTM (Kim 等人，[2016](#bib.bib62))
    | – | TC |'
- en: '| (Ebrahimi et al., [[n. d.]](#bib.bib31)) | character | Y | CharCNN-LSTM (Costa-Jussà
    and Fonollosa, [2016](#bib.bib27)) | Number of changes | MT |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| (Ebrahimi 等人，[[n. d.]](#bib.bib31)) | 字符 | 是 | CharCNN-LSTM (Costa-Jussà
    和 Fonollosa，[2016](#bib.bib27)) | 更改次数 | MT |'
- en: '| Attention-based | (Blohm et al., [2018](#bib.bib17)) | word, sentence | N
    | (Wang and Jiang, [2016a](#bib.bib141); Liu et al., [2017b](#bib.bib83); Dzendzik
    et al., [2017](#bib.bib30)), CNN, LSTM and ensembles | Number of changes | MRC,
    QA |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 基于注意力的 | (Blohm 等人，[2018](#bib.bib17)) | 词、句子 | 否 | (Wang 和 Jiang，[2016a](#bib.bib141);
    Liu 等人，[2017b](#bib.bib83); Dzendzik 等人，[2017](#bib.bib30))，CNN，LSTM 和集成模型 | 更改次数
    | MRC, QA |'
- en: '| Reprogramming | (Neekhara et al., [2018](#bib.bib99)) | word | N | CNN, LSTM,
    Bi-LSTM | – | TC |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 重新编程 | (Neekhara 等人，[2018](#bib.bib99)) | 词 | 否 | CNN, LSTM, Bi-LSTM | –
    | TC |'
- en: '| Hybrid | (Gong et al., [2018](#bib.bib40)) | word embedding | N | CNN | WMD
    | TC, SA |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 混合攻击 | (Gong 等人，[2018](#bib.bib40)) | 词嵌入 | 否 | CNN | WMD | TC, SA |'
- en: 'Table 1\. Summary of reviewed white-box attack methods. PE: portable executable;
    TC: text classification; SA: sentiment analysis; TS: text summarisation; MT: machine
    translation MAD: malware detection; MSP: Medical Status Prediction; MRC: machine
    reading comprehension; QA: question answering; WMD: Word Mover’s Distance; –:
    not available.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1\. 已审阅的白盒攻击方法汇总。PE: 可移植执行文件；TC: 文本分类；SA: 情感分析；TS: 文本摘要；MT: 机器翻译 MAD: 恶意软件检测；MSP:
    医疗状态预测；MRC: 机器阅读理解；QA: 问答；WMD: 词移动距离；–: 不可用。'
- en: 4.2.5\. Attention-based
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. 基于注意力的
- en: (Blohm et al., [2018](#bib.bib17)) proposed two white-box attacks for the purpose
    of comparing the robustness of CNN verses RNN. They leveraged the model’s internal
    attention distribution to find the pivotal sentence which is assigned a larger
    weight by the model to derive the correct answer. Then they exchanged the words
    which received the most attention with the randomly chosen words in a known vocabulary.
    They also performed another white-box attack by removing the whole sentence that
    gets the highest attention. Although they focused on attention-based models, their
    attacks do not examine the attention mechanism itself, but solely leverages the
    outputs of the attention component (i.e., attention score).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: (Blohm 等人，[2018](#bib.bib17)) 提出了两种白盒攻击，目的是比较 CNN 和 RNN 的鲁棒性。他们利用模型内部的注意力分布来找到模型赋予更大权重的关键句子，以得出正确答案。然后，他们用从已知词汇中随机选择的词替换了得到最多注意力的词。他们还通过移除获得最高注意力的整个句子进行了另一种白盒攻击。尽管他们专注于基于注意力的模型，但他们的攻击并未检查注意力机制本身，而是仅利用注意力组件的输出（即注意力分数）。
- en: 4.2.6\. Reprogramming
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.6\. 重新编程
- en: (Neekhara et al., [2018](#bib.bib99)) adopts adversarial reprogramming (AP)
    to attack sequence neural classifiers. AP (Elsayed et al., [2018](#bib.bib33))
    is a recently proposed adversarial attack where a adversarial reprogramming function
    $g_{\theta}$ is trained to re-purpose the attacked DNN to perform a alternate
    task (e.g., question classification to name classification) without modifying
    the DNN’s parameters. AP adopts idea from transfer learning, but keeps the parameters
    unchanged. The authors in (Neekhara et al., [2018](#bib.bib99)) proposed both
    white-box and black-box attacks. In white-box, Gumbel-Softmax is applied to train
    $g_{\theta}$ who can work on discrete data. We discuss the black-box method later.
    They evaluated their methods on various text classification tasks and confirmed
    the effectiveness of their methods.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: (Neekhara 等人，[2018](#bib.bib99)) 采用对抗性重新编程（AP）攻击序列神经分类器。AP (Elsayed 等人，[2018](#bib.bib33))
    是一种新提出的对抗攻击，其中一个对抗性重新编程函数 $g_{\theta}$ 被训练来将攻击的 DNN 重新用于执行替代任务（例如，从问题分类到名称分类），而无需修改
    DNN 的参数。AP 借鉴了迁移学习的理念，但保持参数不变。文献 (Neekhara 等人，[2018](#bib.bib99)) 提出了白盒和黑盒攻击。在白盒中，应用
    Gumbel-Softmax 训练 $g_{\theta}$，以便处理离散数据。我们稍后将讨论黑盒方法。他们在各种文本分类任务上评估了他们的方法，并确认了其有效性。
- en: 4.2.7\. Hybrid
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.7\. 混合攻击
- en: Authors of the work (Gong et al., [2018](#bib.bib40)) perturbed the input text
    on word embedding against the CNN model. This is a general method that is applicable
    to most of the attack methods developed for computer vision DNNs. The authors
    specifically applied FGSM and DeepFool. Directly applying methods from computer
    vision would generate meaningless adversarial examples. To address this issue,
    the authors rounded the adversarial examples to the nearest meaningful word vectors
    by using Word Mover’s Distance (WMD) as the distance measurements. The evaluations
    on sentiment analysis and text classification datasets show that WMD is a qualified
    metric for controlling the perturbations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 工作的作者（Gong 等，[2018](#bib.bib40)）对CNN模型进行了词嵌入的输入文本扰动。这是一种适用于大多数计算机视觉深度神经网络攻击方法的通用方法。作者特别应用了FGSM和DeepFool。直接应用计算机视觉的方法会生成无意义的对抗样本。为了解决这个问题，作者通过使用词移动距离（WMD）作为距离度量，将对抗样本四舍五入到最近的有意义的词向量。对情感分析和文本分类数据集的评估表明，WMD是控制扰动的合格指标。
- en: 'Summary of White-box Attack. We summarize the reviewed white-box attack works
    in Table [1](#S4.T1 "Table 1 ‣ 4.2.4\. Direction-based ‣ 4.2\. White-Box Attack
    ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey"). We highlight
    four aspects include granularity-on which level the attack is performed; target-whether
    the method is target or un-target; the attacked model, perturbation control-methods
    to control the size of the perturbation, and applications. It is worth noting
    that in binary classifications, target and untarget methods show same effect,
    so we point out their target as ”binary” in the table.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '白箱攻击总结。我们在表[1](#S4.T1 "Table 1 ‣ 4.2.4\. Direction-based ‣ 4.2\. White-Box
    Attack ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey")中总结了审查过的白箱攻击工作。我们重点关注四个方面，包括粒度——攻击执行的级别；目标——方法是否有目标或无目标；攻击模型，扰动控制——控制扰动大小的方法，以及应用。值得注意的是，在二分类中，目标和无目标方法显示出相同的效果，因此我们在表中将它们的目标标记为“binary”。'
- en: 4.3\. Black-box Attack
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 黑箱攻击
- en: Black-box attack does not require the details of the neural networks, but can
    access the input and output. This type of attacks often rely on heuristics to
    generate adversarial examples, and it is more practical as in many real-world
    applications the details of the DNN is a black box to the attacker. In this article,
    we group black-box attacks on textual DNNs into five categories.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 黑箱攻击不需要神经网络的详细信息，但可以访问输入和输出。这类攻击通常依赖于启发式方法生成对抗样本，并且在许多实际应用中，深度神经网络的细节对于攻击者来说是黑箱，这使得它更具实用性。在本文中，我们将黑箱攻击在文本深度神经网络上分为五类。
- en: 4.3.1\. Concatenation Adversaries
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 拼接对手
- en: '![Refer to caption](img/a0ff089ff55c79a77594c5075e698134.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a0ff089ff55c79a77594c5075e698134.png)'
- en: Figure 2\. Concatenation adversarial attack on reading comprehension DNN. After
    adding distracting sentences (in blue) the answer changes from correct one (green)
    to incorrect one (red) (Jia and Liang, [2017](#bib.bib56)).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 拼接对抗攻击在阅读理解深度神经网络中的应用。在添加了分散注意力的句子（蓝色）后，答案从正确的（绿色）变为错误的（红色）（Jia 和 Liang，[2017](#bib.bib56)）。
- en: '![Refer to caption](img/655a4da17536c23a498c3c1c44f68eb9.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/655a4da17536c23a498c3c1c44f68eb9.png)'
- en: Figure 3\. General principle of concatenation adversaries. Correct output are
    often utilized to generate distorted output, which later will be used to build
    distracting contents. Appending distracting contents to the original paragraph
    as adversarial input to the attacked DNN and cause the attacked DNN produce incorrect
    output.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 拼接对手的通用原理。正确的输出通常用于生成扭曲的输出，这些输出随后将用于构建分散注意力的内容。将分散注意力的内容附加到原始段落中作为对抗输入，导致被攻击的深度神经网络产生错误的输出。
- en: '(Jia and Liang, [2017](#bib.bib56)) is the first work to attack reading comprehension
    systems. The authors proposed concatenation adversaries, which is to append distracting
    but meaningless sentences at the end of the paragraph. These distracting sentences
    do not change the semantics of the paragraph and the question answers, but will
    fool the neural model. The distracting sentences are either carefully-generated
    informative sentences or arbitrary sequence of words using a pool of 20 random
    common words. Both perturbations were obtained by iteratively querying the neural
    network until the output changes. Figure [2](#S4.F2 "Figure 2 ‣ 4.3.1\. Concatenation
    Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey") illustrates an example from (Jia and Liang, [2017](#bib.bib56)) that
    after adding distracting sentences (in blue) the answer changes from correct one
    (green) to incorrect one (red). The authors of (Wang and Bansal, [2018](#bib.bib143))
    improved the work by varying the locations where the distracting sentences are
    placed and expanding the set of fake answers for generating the distracting sentences,
    rendering new adversarial examples that can help training more robust neural models.
    Also, the work (Blohm et al., [2018](#bib.bib17)) utilized the distracting sentences
    to evaluate the robustness of their reading comprehension model. Specifically,
    they use a pool of ten random common words in conjunction with all question words
    and the words from all incorrect answer candidates to generate the distracting
    sentences. In this work, a simple word-level black-box attack is also performed
    by replacing the most frequent words via their synonyms. As aforementioned, the
    authors also provided two white-box strategies. Figure [3](#S4.F3 "Figure 3 ‣
    4.3.1\. Concatenation Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking Neural
    Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey") illustrates the general workflow for
    concatenation attack. Correct output (i.e., answer in MRC tasks) are often leveraged
    to generate distorted output, which later will be used to build distracting contents.
    Appending distracting contents to the original paragraph as adversarial input
    to the attacked DNN. The distracting contents will not distract human being and
    ideal DNNs, but can make vulunerable DNNs to produce incorrect output.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: (Jia 和 Liang, [2017](#bib.bib56)) 是首个针对阅读理解系统的研究。作者提出了拼接对抗样本的方法，即在段落末尾添加分散注意力但无意义的句子。这些分散注意力的句子不会改变段落的语义和问题答案，但会欺骗神经模型。这些分散注意力的句子既可以是精心生成的信息句子，也可以是使用20个随机常见单词池生成的任意单词序列。这两种扰动方法都是通过迭代查询神经网络直到输出变化来获得的。图
    [2](#S4.F2 "图 2 ‣ 4.3.1\. 拼接对抗样本 ‣ 4.3\. 黑箱攻击 ‣ 4\. 攻击 NLP 中的神经模型：前沿研究 ‣ 对深度学习模型的对抗攻击：综述")
    说明了 (Jia 和 Liang, [2017](#bib.bib56)) 的一个示例：在添加分散注意力的句子（蓝色）后，答案从正确（绿色）变为不正确（红色）。(Wang
    和 Bansal, [2018](#bib.bib143)) 的作者通过改变分散注意力句子的位置和扩展生成分散句子的假答案集合，改进了这项工作，提出了新的对抗样本，从而有助于训练更强健的神经模型。此外，(Blohm
    等人, [2018](#bib.bib17)) 利用分散注意力的句子来评估他们的阅读理解模型的鲁棒性。具体而言，他们使用了十个随机常见单词池以及所有问题词和所有不正确答案候选词生成分散注意力的句子。在这项工作中，还通过用同义词替换最频繁的单词进行了一种简单的词级黑箱攻击。如前所述，作者还提供了两种白箱策略。图
    [3](#S4.F3 "图 3 ‣ 4.3.1\. 拼接对抗样本 ‣ 4.3\. 黑箱攻击 ‣ 4\. 攻击 NLP 中的神经模型：前沿研究 ‣ 对深度学习模型的对抗攻击：综述")
    说明了拼接攻击的一般工作流程。正确的输出（即 MRC 任务中的答案）通常用于生成扭曲的输出，这些扭曲的输出随后将用于构建分散注意力的内容。将分散注意力的内容附加到原始段落作为对抗输入，供被攻击的
    DNN 使用。分散注意力的内容不会分散人类和理想的 DNN，但可以使易受攻击的 DNN 产生不正确的输出。
- en: 4.3.2\. Edit Adversaries
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 编辑对抗样本
- en: '![Refer to caption](img/754b8594c63d487460b30bbb719c81fa.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/754b8594c63d487460b30bbb719c81fa.png)'
- en: Figure 4\. Edit adversarial attack on sentiment analysis DNN. After editing
    words (red), the prediction changes from 100% of Negative to 89% of Positive (Li
    et al., [2019](#bib.bib75)).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 对情感分析 DNN 的编辑对抗攻击。在编辑了词语（红色）后，预测从100%的负面变为89%的正面 (Li 等人, [2019](#bib.bib75))。
- en: '![Refer to caption](img/7918a3f33d7c7548d60f84bd2de0200f.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7918a3f33d7c7548d60f84bd2de0200f.png)'
- en: Figure 5\. General principle of edit adversaries. Perturbations are performed
    on sentences, words or characters by edit strategies such as replace, delete,
    add and swap.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 编辑对抗者的一般原则。扰动通过编辑策略（如替换、删除、添加和交换）对句子、单词或字符进行操作。
- en: 'The work in (Belinkov and Bisk, [2018](#bib.bib14)) perturbed the input data
    of neural machine translation applications in two ways: Synthetic, which performed
    the character order changes, such as swap, middle random (i.e., randomly change
    orders of characters except the first and the last), fully random (i.e., randomly
    change orders of all characters) and keyboard type. They also collected typos
    and misspellings as adversaries. natural, leveraged the typos from the datasets.
    Furthermore, (Niu and Bansal, [2018](#bib.bib100)) attacked the neural models
    for dialogue generation. They applied various perturbations in dialogue context,
    namely Random Swap (randomly transposing neighboring tokens) and Stopword Dropout
    (randomly removing stopwords), Paraphrasing (replacing words with their paraphrases),
    Grammar Errors (e.g., changing a verb to the wrong tense) for the Should-Not-Change
    attacks, and the Add Negation strategy (negates the root verb of the source input)
    and Antonym strategy (changes verbs, adjectives, or adverbs to their antonyms)
    for Should-Change attacks. DeepWordBug (Gao et al., [2018](#bib.bib36)) is a simple
    method that uses character transformations to generate adversarial examples. The
    authors first identified the important ‘tokens’, i.e., words or characters that
    affect the model prediction the most by scoring functions developed by measuring
    the DNN classifier’s output. Then they modified the identified tokens using four
    strategies: replace, delete, add and swap. The authors evaluated their method
    on a variety of NLP tasks, e.g., text classification, sentiment analysis and spam
    detection. (Li et al., [2019](#bib.bib75)) followed (Gao et al., [2018](#bib.bib36)),
    refining the scoring function. Also this work provided white-box attack adopting
    JSMA. One contribution of this work lies on the perturbations are restricted using
    four textual similarity measurement: edit distance of text; Jaccard similarity
    coefficient; Euclidean distance on word vector; and cosine similarity on word
    embedding. Their method had been evaluated only on sentiment analysis task.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: (Belinkov 和 Bisk, [2018](#bib.bib14)) 的工作以两种方式扰动神经机器翻译应用的输入数据：合成的，进行字符顺序变化，如交换、中间随机（即随机更改字符顺序，除了第一个和最后一个），完全随机（即随机更改所有字符的顺序）和键盘输入。他们还收集了错别字和拼写错误作为对抗样本。自然的，利用数据集中的错别字。此外，(Niu
    和 Bansal, [2018](#bib.bib100)) 攻击了对话生成的神经模型。他们在对话上下文中应用了各种扰动，即随机交换（随机转置相邻的标记）和停用词丢弃（随机删除停用词）、同义词替换（用同义词替换单词）、语法错误（例如，将动词改为错误的时态）作为不应更改的攻击，以及添加否定策略（对源输入的根动词进行否定）和反义词策略（将动词、形容词或副词更改为其反义词）作为应更改的攻击。DeepWordBug
    (Gao et al., [2018](#bib.bib36)) 是一种简单的方法，利用字符转换生成对抗样本。作者首先通过测量 DNN 分类器输出的评分函数来识别重要的“标记”，即对模型预测影响最大的单词或字符。然后，他们使用四种策略修改识别出的标记：替换、删除、添加和交换。作者在各种
    NLP 任务上评估了他们的方法，例如文本分类、情感分析和垃圾邮件检测。(Li et al., [2019](#bib.bib75)) 继承了 (Gao et
    al., [2018](#bib.bib36)) 的方法，改进了评分函数。此外，这项工作提供了采用 JSMA 的白盒攻击。这项工作的一个贡献在于扰动通过四种文本相似性测量限制：文本的编辑距离；Jaccard
    相似系数；词向量上的欧几里得距离；以及词嵌入上的余弦相似度。他们的方法仅在情感分析任务上进行了评估。
- en: 'The authors in (Minervini and Riedel, [2018](#bib.bib93)) proposed a method
    for automatically generating adversarial examples that violate a set of given
    First-Order Logic constraints in natural language inference (NLI). They proposed
    an inconsistency loss to measure the degree to which a set of sentences causes
    a model to violate a rule. The adversarial example generation is the process for
    finding the mapping between variables in rules to sentences that maximize the
    inconsistency loss and are composed by sentences with a low perplexity (defined
    by a language model). To generate low-perplexity adversarial sentence examples,
    they used three edit perturbations: i) change one word in one of the input sentences;
    i) remove one parse subtree from one of the input sentences; iii) insert one parse
    sub-tree from one sentence in the corpus in the parse tree of the another sentence.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在(Minervini和Riedel, [2018](#bib.bib93))中，作者提出了一种自动生成违反自然语言推理（NLI）中给定的一组一阶逻辑约束的对抗性示例的方法。他们提出了一种不一致损失来衡量一组句子导致模型违反规则的程度。对抗性示例生成是寻找规则中变量与句子之间的映射的过程，以最大化不一致损失，并由低困惑度的句子（由语言模型定义）组成。为了生成低困惑度的对抗性句子示例，他们使用了三种编辑扰动：i)
    更改输入句子中的一个词；ii) 从输入句子中删除一个解析子树；iii) 将语料库中的一个解析子树插入到另一个句子的解析树中。
- en: The work in (Alzantot et al., [2018](#bib.bib6)) uses genetic algorithm (GA)
    for minimising the number of word replacement from the original text, but at the
    same time can change the result of the attacked model. They adopted crossover
    and mutation operations in GA to generate perturbations. The authors measured
    the effectiveness of the word replacement accoding to the impact on attacked DNNs.
    Their attack focused on sentiment analysis and textual entailment DNNs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在(Alzantot et al., [2018](#bib.bib6))中，研究者使用遗传算法（GA）来最小化原文中的词汇替换数量，同时能够改变被攻击模型的结果。他们在GA中采用了交叉和突变操作来生成扰动。作者根据对被攻击DNN的影响来衡量词汇替换的有效性。他们的攻击集中在情感分析和文本蕴涵DNN上。
- en: In (Chan et al., [2018](#bib.bib22)), the authors proposed a framerwork for
    adversarial attack on Differentiable Neural Computer (DNC). DNC is a computing
    machine with DNN as its central controller operating on an external memory module
    for data processing. Their method uses two new automated and scalable strategies
    to generate grammatically corret adversairal attacks in question answering domian,
    utilising metamorphic transformation. The first strategy, Pick-n-Plug, consists
    of a pick operator pick to draw adversarial sentences from a particular task (source
    task) and plug operator plug to inject these sentences into a story from another
    task (target task), without changing its correct answers. Another strategy, Pick-Permute-Plug,
    extends the adversarial capability of PPick-n-Plug by an additional permute operator
    after picking sentences (gpick) from a source task. Words in a particular adversarial
    sentence can be permuted with its synonyms to generate a wider range of possible
    attacks.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在(Chan et al., [2018](#bib.bib22))中，作者提出了一种针对可微分神经计算机（DNC）的对抗攻击框架。DNC是一个以DNN作为中央控制器的计算机器，操作于外部记忆模块进行数据处理。他们的方法利用两种新的自动化和可扩展策略生成语法正确的对抗性攻击，应用了变形转换。第一种策略，Pick-n-Plug，由选择操作符pick和插入操作符plug组成，前者用于从特定任务（源任务）中提取对抗性句子，后者用于将这些句子注入到另一个任务（目标任务）的故事中，而不改变其正确答案。另一种策略，Pick-Permute-Plug，通过在从源任务中选择句子后添加一个排列操作符（gpick）扩展了Pick-n-Plug的对抗能力。对抗句子中的词汇可以用其同义词进行排列，从而生成更广泛的攻击范围。
- en: '![Refer to caption](img/d0b4ad8e50675ff6dfd86e82f2841f34.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d0b4ad8e50675ff6dfd86e82f2841f34.png)'
- en: Figure 6\. General principle of paraphrase-based adversaries. Carefully designed
    (controlled) paraphrases are regarded as adversarial examples, which fool DNN
    to produce incorrect output.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. 基于释义的对抗者的通用原理。精心设计（受控）的释义被视为对抗性示例，这会欺骗DNN生成错误的输出。
- en: 4.3.3\. Paraphrase-based Adversaries
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3. 基于释义的对抗者
- en: 'SCPNs (Iyyer et al., [2018](#bib.bib55)) produces a paraphrase of the given
    sentence with desired syntax by inputting the sentence and a targeted syntactic
    form into an encoder-decoder architecture. Specifically, the method first encodes
    the original sentence, then inputs the paraphrases generated by back-translation
    and the targeted syntactic tree into the decoder, whose output is the targeted
    paraphrase of the original sentence. One major contribution lies on the selection
    and processing of the parse templates. The authors trained a parse generator separately
    from SCPNs and selected 20 most frequent templates in PARANMT-50M. After generating
    paraphrases using the selected parse templates, they further pruned non-sensible
    sentences by checking n-gram overlap and paraphrastic similarity. The attacked
    classifier can correctly predict the label of the original sentence but fails
    on its paraphrase, which is regarded as the adversarial example. SCPNs had been
    evaluated on sentiment analysis and textual entailment DNNs and showed significant
    impact on the attacked models. Although this method use target strategy to generate
    adversarial examples, it does not specify targeted output. Therefore, we group
    it to untarget attack. Furthermore, the work in (Singh et al., [2018](#bib.bib128))
    used the idea of paraphrase generation techniques that create semantically equivalent
    adversaries (SEA). They generated paraphrases of a input sentence $x$, and got
    predictions from $f$ until the original prediction is changed with considering
    the semantically equivalent to $x^{\prime}$ that is $1$ if $x$ is semantically
    equivalent to $x^{\prime}$ and $0$ otherwise as shown in Eq.([22](#S4.E22 "In
    4.3.3\. Paraphrase-based Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking
    Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey")). After that, this work proposes
    semantic-equivalent rule based method for generalizing these generated adversaries
    into semantically equivalent rules in order to understand and fix the most impactful
    bug.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: SCPNs（Iyyer等，[2018](#bib.bib55)）通过将句子和目标句法形式输入到编码-解码架构中，生成具有所需句法的句子的同义句。具体而言，该方法首先对原始句子进行编码，然后将通过反向翻译生成的同义句和目标句法树输入到解码器中，解码器的输出是原始句子的目标同义句。一个主要贡献在于解析模板的选择和处理。作者将解析生成器与SCPNs分开训练，并在PARANMT-50M中选择了20个最常见的模板。在使用所选解析模板生成同义句后，他们通过检查n-gram重叠和同义句相似性进一步修剪了不合理的句子。被攻击的分类器能够正确预测原始句子的标签，但在其同义句上失败，这被视为对抗样本。SCPNs已在情感分析和文本蕴含的深度神经网络（DNNs）上进行评估，并对被攻击的模型产生了显著影响。虽然该方法使用目标策略生成对抗样本，但未指定目标输出。因此，我们将其归为非目标攻击。此外，(Singh等，[2018](#bib.bib128))的工作使用了生成同义句技术的思路，创建了语义等效的对抗样本（SEA）。他们生成了输入句子$x$的同义句，并从$f$中获得预测，直到原始预测发生变化，同时考虑到与$x^{\prime}$的语义等效性，即如果$x$与$x^{\prime}$语义等效，则为$1$，否则为$0$，如公式([22](#S4.E22
    "在4.3.3\. 基于同义句的对抗样本 ‣ 4.3\. 黑盒攻击 ‣ 4\. 攻击NLP中的神经模型：最新进展 ‣ 自然语言处理中的深度学习模型的对抗攻击：综述"))所示。之后，这项工作提出了基于语义等效规则的方法，将生成的对抗样本推广为语义等效规则，以便理解和修复最具影响力的漏洞。
- en: '| (22) |  | $\displaystyle\mathbf{SEA(x,x^{\prime})}=\mathbf{1}[SemEq(x,x^{\prime})\wedge
    f(\mathbf{x})\neq f(\mathbf{x}^{\prime})]$ |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\displaystyle\mathbf{SEA(x,x^{\prime})}=\mathbf{1}[SemEq(x,x^{\prime})\wedge
    f(\mathbf{x})\neq f(\mathbf{x}^{\prime})]$ |  |'
- en: 4.3.4\. GAN-based Adversaries
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4\. 基于GAN的对抗样本
- en: 'Some works proposed to leverage Generative Adversarial Network (GAN) (Goodfellow
    et al., [2014](#bib.bib42)) to generate adversaries (Zhao et al., [2017](#bib.bib157)).
    The purpose of adopting GAN is to make the adversarial examples mroe natural.
    In (Zhao et al., [2017](#bib.bib157)), the model proposed to generate adversarial
    exsamples consists of two key components: a GAN, which generate fake data samples,
    and an inverter that maps input $x$ to its latent representation $z^{\prime}$).
    The two components are trained on the original input by minimizing reconstruction
    error between original input and the adversarial examples. Perturbation is performed
    in the latent dense space by identifying the perturbed sample $\hat{z}$ in the
    neighborhood of $z^{\prime}$. Two search approaches, namely iterative stochastic
    search and hybrid shrinking search, are proposed to identify the proper $\hat{z}$.
    However, it requires querying the attacked model each time to find the $\hat{z}$
    that can make the model give incorrect prediction. Therefore, this method is quite
    time-consuming. The work is applicable to both image and textual data as it intrinsically
    eliminates the problem raised by the discrete attribute of textual data. The authors
    evaluated their method on three applications namely: textual entailment, machine
    translation and image classification.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究提出利用生成对抗网络（GAN）（Goodfellow 等人，[2014](#bib.bib42)）来生成对抗样本（Zhao 等人，[2017](#bib.bib157)）。采用
    GAN 的目的是使对抗样本更加自然。在（Zhao 等人，[2017](#bib.bib157)）中，提出的生成对抗样本的模型包含两个关键组件：一个是生成伪数据样本的
    GAN，另一个是将输入 $x$ 映射到其潜在表示 $z^{\prime}$ 的反转器。这两个组件通过最小化原始输入与对抗样本之间的重建误差来进行训练。扰动在潜在密集空间中进行，通过在
    $z^{\prime}$ 的邻域中识别扰动样本 $\hat{z}$。提出了两种搜索方法，即迭代随机搜索和混合缩小搜索，以识别合适的 $\hat{z}$。然而，这种方法每次都需要查询被攻击模型以找到能够使模型给出错误预测的
    $\hat{z}$。因此，这种方法相当耗时。该方法适用于图像和文本数据，因为它本质上消除了文本数据离散属性带来的问题。作者在三个应用场景上评估了他们的方法，即：文本蕴涵、机器翻译和图像分类。
- en: 4.3.5\. Substitution
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5\. 替代
- en: 'The work in (Hu and Tan, [2017](#bib.bib54)) proposes a black-box framework
    that attacks RNN model for malware detection. The framework consists of two models:
    one is a generative RNN, the other is a substitute RNN. The generative RNN aims
    to generate adversarial API sequence from the malware’s API sequence. It is based
    on the seq2seq model proposed in (Sutskever et al., [2014](#bib.bib132)). It particularly
    generates a small piece of API sequence and inserts the sequence after the input
    sequence. The substitute RNN, which is a bi-directional RNN with attention mechanism,
    is to mimic the behavior of the attacked RNN. Therefore, generating adversarial
    examples will not query the original attacked RNN, but its substitution. The substitute
    RNN is trained on both malware and benign sequences, as well as the Gumbel-Softmax
    outputs of the generative RNN. Here, Gumbel-softmax is used to enable the joint
    training of the two RNN models, because the original output of the generative
    RNN is discrete. Specifically, it enables the gradient to be back-propagated from
    generative RNN to substitute RNN. This method performs attack on API, which is
    represented as a one-hot vector, i.e., given $M$ APIs, the vector for the $i$-th
    API is an M-dimensional binary vector that the $i$-th dimension is 1 while other
    dimensions are 0s.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: （Hu 和 Tan，[2017](#bib.bib54)）中的工作提出了一个黑箱框架，用于攻击 RNN 模型以进行恶意软件检测。该框架包含两个模型：一个是生成
    RNN，另一个是替代 RNN。生成 RNN 旨在从恶意软件的 API 序列生成对抗 API 序列。它基于（Sutskever 等人，[2014](#bib.bib132)）提出的
    seq2seq 模型。它特别生成一小段 API 序列，并将该序列插入到输入序列之后。替代 RNN 是一个具有注意力机制的双向 RNN，旨在模仿被攻击的 RNN
    的行为。因此，生成对抗样本不会查询原始被攻击 RNN，而是其替代模型。替代 RNN 在恶意软件和良性序列以及生成 RNN 的 Gumbel-Softmax
    输出上进行训练。这里，Gumbel-softmax 用于实现两个 RNN 模型的联合训练，因为生成 RNN 的原始输出是离散的。具体而言，它使梯度可以从生成
    RNN 反向传播到替代 RNN。这种方法对 API 进行攻击，API 表示为一个 one-hot 向量，即，给定 $M$ 个 API，$i$-th API
    的向量是一个 $M$ 维的二进制向量，其中第 $i$ 维为 1，而其他维为 0。
- en: '| Strategy | Work | Granularity | Target | Attacked Models | Perturb Ctrl.
    | App. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 工作 | 粒度 | 目标 | 攻击模型 | 扰动控制 | 应用 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Concatenation | (Jia and Liang, [2017](#bib.bib56)) | word | N | BiDAF, Match-LSTM
    | – | MRC |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 连接 | (Jia 和 Liang，[2017](#bib.bib56)) | 词 | N | BiDAF, Match-LSTM | – | MRC
    |'
- en: '| (Wang and Bansal, [2018](#bib.bib143)) | word, character | N | BiDAF+Self-Attn+ELMo
    (Peters et al., [2018](#bib.bib110)) | – | MRC |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| (Wang and Bansal, [2018](#bib.bib143)) | 单词，字符 | N | BiDAF+Self-Attn+ELMo
    (Peters et al., [2018](#bib.bib110)) | – | MRC |'
- en: '|  | (Blohm et al., [2018](#bib.bib17)) | word, sentence | N | (Wang and Jiang,
    [2016a](#bib.bib141); Liu et al., [2017b](#bib.bib83); Dzendzik et al., [2017](#bib.bib30)),
    CNN, LSTM and ensembles | Number of changes | MRC, QA |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | (Blohm et al., [2018](#bib.bib17)) | 单词，句子 | N | (Wang and Jiang, [2016a](#bib.bib141);
    Liu et al., [2017b](#bib.bib83); Dzendzik et al., [2017](#bib.bib30))，CNN，LSTM
    和集成方法 | 变更次数 | MRC, QA |'
- en: '| Edit | (Belinkov and Bisk, [2018](#bib.bib14)) | character, word | N | Nematus
    (Sennrich et al., [2017](#bib.bib121)), char2char (Lee et al., [2017](#bib.bib72)),
    charCNN (Kim et al., [2016](#bib.bib62)) | – | MT |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 编辑 | (Belinkov and Bisk, [2018](#bib.bib14)) | 字符，单词 | N | Nematus (Sennrich
    et al., [2017](#bib.bib121))，char2char (Lee et al., [2017](#bib.bib72))，charCNN
    (Kim et al., [2016](#bib.bib62)) | – | MT |'
- en: '| (Niu and Bansal, [2018](#bib.bib100)) | word, phrase | N | VHRED (Serban
    et al., [2017](#bib.bib124))+attn, RL in (Li et al., [2016](#bib.bib76)), DynoNet
    (He et al., [2017](#bib.bib50)) | – | DA |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| (Niu and Bansal, [2018](#bib.bib100)) | 单词，短语 | N | VHRED (Serban et al.,
    [2017](#bib.bib124))+attn，RL (Li et al., [2016](#bib.bib76))，DynoNet (He et al.,
    [2017](#bib.bib50)) | – | DA |'
- en: '| (Gao et al., [2018](#bib.bib36)) | character, word | N | Word-level LSTM,
    Character-level CNN | – | SA, TC |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| (Gao et al., [2018](#bib.bib36)) | 字符，单词 | N | 单词级 LSTM，字符级 CNN | – | SA,
    TC |'
- en: '| (Li et al., [2019](#bib.bib75)) | character, word | N | Word-level LSTM,
    Character-level CNN | EdDist, JSC, EuDistV, CSE | SA |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| (Li et al., [2019](#bib.bib75)) | 字符，单词 | N | 单词级 LSTM，字符级 CNN | EdDist,
    JSC, EuDistV, CSE | SA |'
- en: '| (Minervini and Riedel, [2018](#bib.bib93)) | word, phrase | N | cBiLSTM,
    DAM, ESIM | Perplexity | NLI |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| (Minervini and Riedel, [2018](#bib.bib93)) | 单词，短语 | N | cBiLSTM，DAM，ESIM
    | 困惑度 | NLI |'
- en: '| (Alzantot et al., [2018](#bib.bib6)) | word | N | LSTM | EuDistV | SA, TE
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| (Alzantot et al., [2018](#bib.bib6)) | 单词 | N | LSTM | EuDistV | SA, TE |'
- en: '| (Chan et al., [2018](#bib.bib22)) | word, sentence | N | DNC | – | QA |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| (Chan et al., [2018](#bib.bib22)) | 单词，句子 | N | DNC | – | QA |'
- en: '| Paraphrase-based | (Iyyer et al., [2018](#bib.bib55)) | word | N | LSTM |
    Syntax-ctrl paraphrase | SA and TE |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 基于释义 | (Iyyer et al., [2018](#bib.bib55)) | 单词 | N | LSTM | 语法控制释义 | SA 和
    TE |'
- en: '| (Singh et al., [2018](#bib.bib128)) | word | N | BiDAF, Visual7W (Zhu et al.,
    [2016](#bib.bib158)), fastText (Grave et al., [2017](#bib.bib44)) | Self-defined
    semantic-equivalency | MRC, SA, VQA |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| (Singh et al., [2018](#bib.bib128)) | 单词 | N | BiDAF，Visual7W (Zhu et al.,
    [2016](#bib.bib158))，fastText (Grave et al., [2017](#bib.bib44)) | 自定义语义等价性 |
    MRC, SA, VQA |'
- en: '| GAN-based | (Zhao et al., [2017](#bib.bib157)) | word | N | LSTM, TreeLSTM,
    Google Translate (En-to-Ge) | GAN-constraints | TE, MT |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 基于 GAN | (Zhao et al., [2017](#bib.bib157)) | 单词 | N | LSTM，TreeLSTM，Google
    翻译 (En-to-Ge) | GAN 约束 | TE, MT |'
- en: '| Substitution | (Hu and Tan, [2017](#bib.bib54)) | API | N | LSTM, BiLSTM
    and variants | – | MD |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 替换 | (Hu and Tan, [2017](#bib.bib54)) | API | N | LSTM，BiLSTM 和变体 | – | MD
    |'
- en: '| Reprogramming | (Neekhara et al., [2018](#bib.bib99)) | word | N | CNN, LSTM,
    Bi-LSTM | – | TC |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 重新编程 | (Neekhara et al., [2018](#bib.bib99)) | 单词 | N | CNN, LSTM, Bi-LSTM
    | – | TC |'
- en: 'Table 2\. Summary of reviewed black-box attack methods. MRC: machine reading
    comprehension; QA: question answering; VQA: visual question answering; DA: dialogue
    generation; TC: text classification; MT: machine translation; SA: sentiment analysis;
    NLI: natural language inference; TE: textual entailment; MD: malware detection.
    EdDist: edit distance of text, JSC: Jaccard similarity coeffcient, EuDistV: Euclidean
    distance on word vector, CSE: cosine similarity on word embedding. ’-’: not available.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2\. 已审查的黑箱攻击方法总结。MRC: 机器阅读理解；QA: 问答；VQA: 可视化问答；DA: 对话生成；TC: 文本分类；MT: 机器翻译；SA:
    情感分析；NLI: 自然语言推理；TE: 文本蕴含；MD: 恶意软件检测。EdDist: 文本编辑距离，JSC: Jaccard 相似系数，EuDistV:
    单词向量的欧几里得距离，CSE: 单词嵌入的余弦相似度。’-’：不可用。'
- en: 4.3.6\. Reprogramming
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.6\. 重新编程
- en: As aforementioned, (Neekhara et al., [2018](#bib.bib99)) provides both white-box
    and black-box attacks. We describe black-box attack here. In black-box attack,
    the authors fomulated the sequence generation as a reinforcement learning problem,
    and the adversarial reprogramming function $g_{\theta}$ is the policy network.
    Then they applied REINFORCE-based optimisation to train $g_{\theta}$.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，(Neekhara et al., [2018](#bib.bib99)) 提供了白箱和黑箱攻击。我们在此描述黑箱攻击。在黑箱攻击中，作者将序列生成问题表述为一个强化学习问题，而对抗重新编程函数
    $g_{\theta}$ 则是策略网络。然后他们应用基于 REINFORCE 的优化方法来训练 $g_{\theta}$。
- en: 'Summary of Black-box Attack. We summarise the reviewed black-box attack works
    in Table [2](#S4.T2 "Table 2 ‣ 4.3.5\. Substitution ‣ 4.3\. Black-box Attack ‣
    4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey"). We highlight
    four aspects include granularity-on which level the attack is performed; target-whether
    the method is target or un-target; the attacked model, perturbation control, and
    applications.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '黑盒攻击总结。我们在表 [2](#S4.T2 "Table 2 ‣ 4.3.5\. Substitution ‣ 4.3\. Black-box Attack
    ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey") 中总结了审查过的黑盒攻击工作。我们重点突出四个方面，包括攻击的粒度（即攻击在哪一层级进行）；目标——方法是否针对特定目标；被攻击模型、扰动控制和应用。'
- en: 4.4\. Multi-modal Attacks
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. 多模态攻击
- en: Some works attack DNNs that are dealing with cross-modal data. For example,
    the neural models contain an internal component that performs image-to-text or
    speech-to-text conversion. Although these attacks are not for pure textual data,
    we briefly introduce the representative ones for the purpose of a comprehensive
    review.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作攻击处理跨模态数据的 DNNs。例如，神经模型包含一个内部组件，该组件执行图像到文本或语音到文本的转换。尽管这些攻击并非针对纯文本数据，但我们简要介绍一些具有代表性的工作，以便进行全面回顾。
- en: 4.4.1\. Image-to-Text
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 图像到文本
- en: Image-to-text models is a class of techniques that generate textual description
    for an image based on the semantic content of the latter.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到文本模型是一类基于图像语义内容生成文本描述的技术。
- en: 'Optical Character Recognition (OCR). Recognizing characters from images is
    a problem named Optical Character Recognition (OCR). OCR is a multimodal learning
    task that takes an image as input and output the recognized text. Authors in (Song
    and Shmatikov, [2018](#bib.bib130)) proposed a white-box attack on OCR and follow-up
    NLP applications. They firstly used the original text to render a clean image
    (conversion DNNs). Then they found words in the text that have antonyms in WordNet
    and satisfy edit distance threshold. Only the antonyms that are valid and keep
    semantic inconsistencies will be kept. Later, the method locates the lines in
    the clean image containing the aforementioned words, which can be replaced by
    their selected antonyms. The method then transforms the target word to target
    sequence. Given the input/target images and sequences, the authors formed the
    generating of adversarial example is an optimisation problem:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 光学字符识别（OCR）。识别图像中的字符是一个名为光学字符识别（OCR）的问题。OCR 是一个多模态学习任务，它将图像作为输入，并输出识别出的文本。作者在（Song
    和 Shmatikov，[2018](#bib.bib130)）中提出了对 OCR 及后续 NLP 应用的白盒攻击。他们首先使用原始文本渲染出一张干净的图像（转换
    DNNs）。然后，他们在文本中找到具有 WordNet 反义词的单词，并满足编辑距离阈值。只有那些有效并保持语义不一致的反义词会被保留。接着，该方法定位到干净图像中包含上述单词的行，这些单词可以被其选择的反义词替换。该方法随后将目标单词转换为目标序列。给定输入/目标图像和序列，作者将生成对抗样本的过程视为一个优化问题：
- en: '| (23) |  | $\displaystyle\min_{\omega}c\cdot J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})+&#124;&#124;\mathbf{x}-\mathbf{x}^{\prime}&#124;&#124;_{2}^{2}$
    |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $\displaystyle\min_{\omega}c\cdot J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})+&#124;&#124;\mathbf{x}-\mathbf{x}^{\prime}&#124;&#124;_{2}^{2}$
    |  |'
- en: '| (24) |  | $\displaystyle\mathbf{x^{\prime}}=(\alpha\cdot\tanh({\omega})+\beta)/2$
    |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $\displaystyle\mathbf{x^{\prime}}=(\alpha\cdot\tanh({\omega})+\beta)/2$
    |  |'
- en: '|  | $\displaystyle\alpha=(\mathbf{x}_{max}-\mathbf{x}_{min})/2,\beta=(\mathbf{x}_{max}+\mathbf{x}_{min})/2$
    |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\alpha=(\mathbf{x}_{max}-\mathbf{x}_{min})/2,\beta=(\mathbf{x}_{max}+\mathbf{x}_{min})/2$
    |  |'
- en: '| (25) |  | $\displaystyle J_{CTC}(f(\mathbf{x},t))=-\log p(t&#124;\mathbf{x})$
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $\displaystyle J_{CTC}(f(\mathbf{x},t))=-\log p(t&#124;\mathbf{x})$
    |  |'
- en: 'where $f(\mathbf{x})$ is the neural system model, $J_{CTC}(\cdot)$ is the Connectionist
    Temporal Classification (CTC) loss function, $\mathbf{x}$ is the input image,
    $t$ is the ground truth sequence, $\mathbf{x}^{\prime}$ is the adversarial example,
    $t^{\prime}$ is the target sequence, $\omega,\alpha,\beta$ are parameters controlling
    adversarial examples to satisfy the box-constraint of $\mathbf{x}^{\prime}\in[\mathbf{x}_{min},\mathbf{x}_{max}]^{p}$,
    where $p$ is the number of pixels ensuring valid $x^{\prime}$. After generating
    adversarial examples, the method replaces the images of the corresponding lines
    in the text image. The authors evaluated this method in three aspects: single
    word recognition, whole document recognition, and NLP applications which based
    on the recognised text (sentiment analysis and document categorisation specifically).
    They also addressed that the proposed method suffers from limitatios such as low
    transferability across data and models, and physical unrelalizability.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(\mathbf{x})$ 是神经系统模型，$J_{CTC}(\cdot)$ 是连接时序分类（CTC）损失函数，$\mathbf{x}$ 是输入图像，$t$
    是真实序列，$\mathbf{x}^{\prime}$ 是对抗样本，$t^{\prime}$ 是目标序列，$\omega,\alpha,\beta$ 是控制对抗样本满足
    $\mathbf{x}^{\prime}\in[\mathbf{x}_{min},\mathbf{x}_{max}]^{p}$ 约束的参数，其中 $p$ 是确保有效
    $x^{\prime}$ 的像素数量。生成对抗样本后，该方法替换文本图像中相应行的图像。作者从三个方面评估了这种方法：单词识别、整个文档识别和基于识别文本的
    NLP 应用（特别是情感分析和文档分类）。他们还指出，提出的方法存在一些局限性，如跨数据和模型的低可转移性，以及物理不可实现性。
- en: 'Scene Text Recognition (STR). STR is also an image-to-text application. In
    STR, the entire image is mapped to word strings directly. In contrast, the recognition
    in OCR is a pipeline process: first segments the words to characters, then performs
    the recognition on single characters. AdaptiveAttack (Yuan et al., [2018](#bib.bib154))
    evaluated the possibility of performing adversarial attack for scene text recognition.
    The authors proposed two attacks, namely basic attack and adaptive attack. Basic
    attack is similar to the work in (Song and Shmatikov, [2018](#bib.bib130)) and
    it also formulates the adversarial example generation as an optimisation problem:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 场景文本识别（STR）。STR 也是一种图像到文本的应用。在 STR 中，整个图像直接映射到词串。相比之下，OCR 的识别是一个管道过程：首先将单词分割成字符，然后对单个字符进行识别。AdaptiveAttack（Yuan
    等人，[2018](#bib.bib154)）评估了对场景文本识别进行对抗攻击的可能性。作者提出了两种攻击方法，即基本攻击和自适应攻击。基本攻击类似于（Song
    和 Shmatikov，[2018](#bib.bib130)）中的工作，它还将对抗样本生成公式化为优化问题：
- en: '| (26) |  | $\displaystyle\min_{\omega}J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})+\lambda\mathcal{D}(\mathbf{x},\mathbf{x}^{\prime})$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $\displaystyle\min_{\omega}J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})+\lambda\mathcal{D}(\mathbf{x},\mathbf{x}^{\prime})$
    |  |'
- en: '| (27) |  | $\displaystyle\mathbf{x^{\prime}}=\tanh({\omega})$ |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $\displaystyle\mathbf{x^{\prime}}=\tanh({\omega})$ |  |'
- en: 'where $\mathcal{D}(\cdot)$ is Euclidean distance. The differences to (Song
    and Shmatikov, [2018](#bib.bib130)) lie on the definition of $\mathbf{x^{\prime}}$
    (Eq. ([24](#S4.E24 "In 4.4.1\. Image-to-Text ‣ 4.4\. Multi-modal Attacks ‣ 4\.
    Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep
    Learning Models in Natural Language Processing: A Survey")) vs Eq. ([27](#S4.E27
    "In 4.4.1\. Image-to-Text ‣ 4.4\. Multi-modal Attacks ‣ 4\. Attacking Neural Models
    in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models in Natural
    Language Processing: A Survey"))), and the distance measurement between $\mathbf{x}$,
    $\mathbf{x^{\prime}}$ ($L_{2}$ norm vs Euclidean distance), and the parameter
    $\lambda$, which balances the importance of being adversarial example and close
    to the original image. As searching for proper $\lambda$ is quite time-consuming,
    the authors proposed another method to adaptively find $\lambda$. They named this
    method Adaptive Attack, in which they defined the likelihood of a sequential classification
    task following a Gaussian distribution and derived the adaptive optimization for
    sequential adversarial examples as:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathcal{D}(\cdot)$ 是欧几里得距离。与 (Song and Shmatikov, [2018](#bib.bib130))
    的不同之处在于 $\mathbf{x^{\prime}}$ 的定义（Eq. ([24](#S4.E24 "In 4.4.1\. Image-to-Text
    ‣ 4.4\. Multi-modal Attacks ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")) 对比 Eq. ([27](#S4.E27 "In 4.4.1\. Image-to-Text ‣ 4.4\. Multi-modal
    Attacks ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey"))），以及 $\mathbf{x}$
    和 $\mathbf{x^{\prime}}$ 之间的距离测量（$L_{2}$ 范数对比欧几里得距离），和参数 $\lambda$，该参数平衡对抗样本的必要性和与原始图像的接近度。由于寻找合适的
    $\lambda$ 非常耗时，作者提出了另一种方法来自适应地寻找 $\lambda$。他们将这种方法命名为自适应攻击，在其中他们定义了序列分类任务的概率分布为高斯分布，并推导了序列对抗样本的自适应优化公式：'
- en: '| (28) |  | $\displaystyle\min\frac{&#124;&#124;\mathbf{x}-\mathbf{x}^{\prime}&#124;&#124;_{2}^{2}}{\lambda_{1}^{2}}+\frac{J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})}{\lambda_{2}^{2}}+\log\lambda_{1}^{2}+T\log\lambda_{2}^{2}+\frac{1}{\lambda_{2}^{2}}$
    |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $\displaystyle\min\frac{&#124;&#124;\mathbf{x}-\mathbf{x}^{\prime}&#124;&#124;_{2}^{2}}{\lambda_{1}^{2}}+\frac{J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})}{\lambda_{2}^{2}}+\log\lambda_{1}^{2}+T\log\lambda_{2}^{2}+\frac{1}{\lambda_{2}^{2}}$
    |  |'
- en: where $\lambda_{1}$ and $\lambda_{2}$ are two parameters to balance perturbation
    and CTC loss, $T$ is the number of valid paths given targeted sequential output.
    Adaptive Attack can be applied to generate adversarial examples on both non-sequential
    and sequential classification problems. Here we only highlight the equation for
    sequential data. The authors evaluated their proposed methods on tasks that targeting
    the text insertion, deletion and substitution in output. The results demonstrated
    that Adaptive Attack is much faster than basic attack.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\lambda_{1}$ 和 $\lambda_{2}$ 是用于平衡扰动和 CTC 损失的两个参数，$T$ 是给定目标序列输出的有效路径数量。自适应攻击可以应用于生成非序列和序列分类问题中的对抗样本。这里我们仅突出序列数据的公式。作者在针对文本插入、删除和替换的任务上评估了他们提出的方法。结果表明，自适应攻击比基本攻击要快得多。
- en: 'Image Captioning. Image captioning is another multimodal learning task that
    takes an image as input and generates a textual caption describing its visual
    contents. Show-and-Fool (Chen et al., [2018](#bib.bib23)) generates adversarial
    examples to attack the CNN-RNN based image captioning model. The CNN-RNN model
    attacked uses a CNN as encoder for image feature extraction and a RNN as decoder
    for caption generation. Show-and-Fool has two attack strategies: targeted caption
    (i.e., the generated caption matches the target caption) and targeted keywords
    (i.e., the generated caption contains the targeted keywords). In general, they
    formulated the two tasks using the following formulation:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述。图像描述是另一种多模态学习任务，它以图像为输入，生成描述其视觉内容的文本描述。Show-and-Fool（Chen et al., [2018](#bib.bib23)）生成对抗样本来攻击基于
    CNN-RNN 的图像描述模型。被攻击的 CNN-RNN 模型使用 CNN 作为图像特征提取的编码器，并使用 RNN 作为描述生成的解码器。Show-and-Fool
    有两种攻击策略：目标描述（即生成的描述与目标描述匹配）和目标关键词（即生成的描述包含目标关键词）。一般而言，他们使用以下公式来表述这两个任务：
- en: '| (29) |  | $\displaystyle\min_{\omega}c\cdot J(\mathbf{x^{\prime}})+&#124;&#124;\mathbf{x^{\prime}}-\mathbf{x}&#124;&#124;_{2}^{2}$
    |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $\displaystyle\min_{\omega}c\cdot J(\mathbf{x^{\prime}})+&#124;&#124;\mathbf{x^{\prime}}-\mathbf{x}&#124;&#124;_{2}^{2}$
    |  |'
- en: '|  | $\displaystyle\mathbf{x^{\prime}}=\mathbf{x}+\eta$ |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{x^{\prime}}=\mathbf{x}+\eta$ |  |'
- en: '|  | $\displaystyle x=\tanh{(y),~{}~{}\mathbf{x^{\prime}}=\tanh{(\omega+y)}}$
    |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x=\tanh{(y),~{}~{}\mathbf{x^{\prime}}=\tanh{(\omega+y)}}$
    |  |'
- en: 'where $c>0$ is a pre-specified regularization constant, $\eta$ is the perturbation,
    $\omega,y$ are parameters controlling $\mathbf{x^{\prime}}\in[-1,1]$. The difference
    between these two strategies is the definition of the loss function $J(\cdot)$.
    For targeted caption strategy, provided the targeted caption as $S=(S_{1},S_{2},...S_{t},...S_{N})$,
    where $S_{t}$ refers to the index of the $t$-th word in the vocabulary and $N$
    is the length of the caption, the loss is formulated as:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c>0$ 是一个预先指定的正则化常数，$\eta$ 是扰动，$\omega,y$ 是控制 $\mathbf{x^{\prime}}\in[-1,1]$
    的参数。这两种策略的区别在于损失函数 $J(\cdot)$ 的定义。对于目标标题策略，给定目标标题为 $S=(S_{1},S_{2},...S_{t},...S_{N})$，其中
    $S_{t}$ 表示词汇表中第 $t$ 个单词的索引，$N$ 是标题的长度，损失公式为：
- en: '| (30) |  | $\displaystyle J_{S,logit}(\mathbf{x^{\prime}})=\sum_{t=2}^{N-1}\max\{-\epsilon,\max_{k\neq
    S_{t}}\{z_{t}^{(k)}\}-z_{t}^{(S_{t})}\}$ |  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $\displaystyle J_{S,logit}(\mathbf{x^{\prime}})=\sum_{t=2}^{N-1}\max\{-\epsilon,\max_{k\neq
    S_{t}}\{z_{t}^{(k)}\}-z_{t}^{(S_{t})}\}$ |  |'
- en: 'where $S_{t}$ is the target word, $z_{t}^{(S_{t})}$ is the logit of the target
    word. In fact, this method mininises the difference between the maximumn logit
    except $S_{t}$, and the logit of $S_{t}$. For the targeted keywords strategy,
    given the targeted keywords $\mathcal{K}:={K_{1},...,K_{M}}$, the loss function
    is:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S_{t}$ 是目标词，$z_{t}^{(S_{t})}$ 是目标词的对数几率。实际上，这种方法最小化除了 $S_{t}$ 之外的最大对数几率与
    $S_{t}$ 的对数几率之间的差异。对于目标关键词策略，给定目标关键词 $\mathcal{K}:={K_{1},...,K_{M}}$，损失函数为：
- en: '| (31) |  | $\displaystyle J_{K,logit}(\mathbf{x^{\prime}})=\sum_{j=1}^{M}\min_{t\in[N]}\{\max\{-\epsilon,\max_{k\neq
    K_{j}}\{z_{t}^{(k)}\}-z_{t}^{(K_{j})}\}\}$ |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $\displaystyle J_{K,logit}(\mathbf{x^{\prime}})=\sum_{j=1}^{M}\min_{t\in[N]}\{\max\{-\epsilon,\max_{k\neq
    K_{j}}\{z_{t}^{(k)}\}-z_{t}^{(K_{j})}\}\}$ |  |'
- en: The authors performed extensive experiments on Show-and-Tell (Vinyals et al.,
    [2015](#bib.bib138)) and varied the parameters in the attacking loss. They found
    that Show-and-Fool is not only effective on attacking Show-and-Tell, the CNN-RNN
    based image captioning model, but is also highly transferable to another model
    Show-Attend-and-Tell (Xu et al., [2015](#bib.bib148)).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 Show-and-Tell (Vinyals et al., [2015](#bib.bib138)) 上进行了广泛实验，并在攻击损失中改变了参数。他们发现
    Show-and-Fool 不仅对攻击 Show-and-Tell（基于 CNN-RNN 的图像标题生成模型）有效，而且对另一模型 Show-Attend-and-Tell
    (Xu et al., [2015](#bib.bib148)) 也具有很好的迁移性。
- en: 'Visual Question Answering (VQA). Given an image and a natural language question
    about the image, VQA is to provide an accurate answer in natural language. The
    work in (Xu et al., [2018](#bib.bib149)) proposed a iterative optimisation method
    to attack two VQA models. The objective function proposed maximises the probability
    of the target answer and unweights the preference of adversarial examples with
    smaller distance to the original image when this distance is below a threshold.
    Specifically, the objective contains three components. The first one is similar
    to Eq. ([26](#S4.E26 "In 4.4.1\. Image-to-Text ‣ 4.4\. Multi-modal Attacks ‣ 4\.
    Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep
    Learning Models in Natural Language Processing: A Survey")), that replaces the
    loss function to the loss of the VQA model and using $||\mathbf{x}-\mathbf{x^{\prime}}||_{2}/\sqrt{N}$
    as distance between $\mathbf{x^{\prime}}$ and $\mathbf{x}$. The second component
    maximises the difference between the softmax output and the prediction when it
    is different with the target answer. The third component ensures the distance
    between $\mathbf{x^{\prime}}$ and $\mathbf{x}$ is under a lower bound. The attacks
    are evaluated by checking whether better success rate is obtained over the previous
    attacks, and the confidence score of the model to predict the target answer. Based
    on the evaluations, the authors concluded that that attention, bounding box localization
    and compositional internal structures are vulnerable to adversarial attacks. This
    work also attacked a image captioning neural model. We refer the original paper
    for further information.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答 (VQA)。给定一张图像和关于图像的自然语言问题，VQA 的任务是提供一个准确的自然语言答案。在 (Xu 等，[2018](#bib.bib149))
    的工作中，提出了一种迭代优化方法来攻击两个 VQA 模型。提出的目标函数最大化目标答案的概率，并在原始图像到攻击图像的距离低于阈值时，降低对距离较小的对抗样本的偏好。具体而言，目标包含三个组成部分。第一个部分类似于方程
    ([26](#S4.E26 "在 4.4.1\. 图像到文本 ‣ 4.4\. 多模态攻击 ‣ 4\. 攻击 NLP 中的神经模型：最新进展 ‣ 深度学习模型在自然语言处理中的对抗攻击：综述"))，将损失函数替换为
    VQA 模型的损失，并使用 $||\mathbf{x}-\mathbf{x^{\prime}}||_{2}/\sqrt{N}$ 作为 $\mathbf{x^{\prime}}$
    和 $\mathbf{x}$ 之间的距离。第二个部分最大化软最大输出和预测与目标答案不同时的差异。第三个部分确保 $\mathbf{x^{\prime}}$
    和 $\mathbf{x}$ 之间的距离低于下界。通过检查是否在先前攻击上获得了更好的成功率以及模型对目标答案的预测信心分数来评估攻击。根据评估，作者得出结论认为，注意力、边界框定位和组合内部结构容易受到对抗攻击。这项工作还攻击了一个图像描述神经模型。有关更多信息，请参考原始论文。
- en: '| Multi-modal | Application | Work | Target | Access | Attacked Models | Perturb
    Ctrl. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 多模态 | 应用 | 工作 | 目标 | 访问 | 攻击模型 | 扰动控制 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Image-to-Text | Optical Character Recognition | (Song and Shmatikov, [2018](#bib.bib130))
    | Y | white-box | Tesseract (Tesseract, [2016](#bib.bib136)) | $L_{2}$, EdDist
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 图像到文本 | 光学字符识别 | (Song 和 Shmatikov，[2018](#bib.bib130)) | Y | 透明盒子 | Tesseract
    (Tesseract，[2016](#bib.bib136)) | $L_{2}$, EdDist |'
- en: '| Scene Text Recognition | (Yuan et al., [2018](#bib.bib154)) | Y | white-box
    | CRNN (Shi et al., [2017](#bib.bib125)) | $L_{2}$ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 场景文本识别 | (Yuan 等，[2018](#bib.bib154)) | Y | 透明盒子 | CRNN (Shi 等，[2017](#bib.bib125))
    | $L_{2}$ |'
- en: '| Image Captioning | (Chen et al., [2018](#bib.bib23)) | Y | white-box | Show-and-Tell
    (Vinyals et al., [2015](#bib.bib138)) | $L_{2}$ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 图像描述 | (Chen 等，[2018](#bib.bib23)) | Y | 透明盒子 | Show-and-Tell (Vinyals 等，[2015](#bib.bib138))
    | $L_{2}$ |'
- en: '| Visual Question Answering | (Xu et al., [2018](#bib.bib149)) | Y | white-box
    | MCB (Fukui et al., [2016](#bib.bib35)), N2NMN (Hu et al., [2017](#bib.bib53))
    | $L_{2}$ |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 视觉问答 | (Xu 等，[2018](#bib.bib149)) | Y | 透明盒子 | MCB (Fukui 等，[2016](#bib.bib35))，N2NMN
    (Hu 等，[2017](#bib.bib53)) | $L_{2}$ |'
- en: '|  | Visual-Semantic Embeddings | (Shi et al., [2018](#bib.bib126)) | N | black-box
    | VSE++ (Faghri et al., [2017](#bib.bib34)) | – |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | 视觉-语义嵌入 | (Shi 等，[2018](#bib.bib126)) | N | 黑盒子 | VSE++ (Faghri 等，[2017](#bib.bib34))
    | – |'
- en: '| Speech-to-Text | Speech Recognition | (Carlini and Wagner, [[n. d.]](#bib.bib20))
    | Y | white-box | DeepSpeech (Hannun et al., [2014](#bib.bib49)) | $L_{2}$ |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 语音到文本 | 语音识别 | (Carlini 和 Wagner，[[n. d.]](#bib.bib20)) | Y | 透明盒子 | DeepSpeech
    (Hannun 等，[2014](#bib.bib49)) | $L_{2}$ |'
- en: 'Table 3\. Summary of reviewed cross-modal attacks. EdDist: edit distance of
    text, -: not available.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 综述的跨模态攻击摘要。EdDist：文本的编辑距离，-：不可用。
- en: 'Visual-Semantic Embeddings (VSE). The aim of VSE is to bridge natural language
    and the underlying visual world. In VSE, the embedding spaces of both images and
    descriptive texts (captions) are jointly optimized and aligned. (Shi et al., [2018](#bib.bib126))
    attacked the latest VSE model by generating adversarial examples in the test set
    and evaluated the robustness of the VSE modesls. They performed the attack on
    textual part by introducing three method: i) replace nouns in the image captions
    utilizing the hypernymy/hyponymy relations in WordNet; ii) change the numerals
    to different ones and singularize or pluralize the corresponding nouns when necessary;
    iii) detect the relations and shuffle the non-interchangeable noun phrases or
    replace the prepositions. This method can be considered as a black-box edit adversary
    .'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉-语义嵌入（VSE）。VSE 的目的是桥接自然语言和底层视觉世界。在 VSE 中，图像和描述性文本（说明）的嵌入空间是联合优化和对齐的。（Shi 等，
    [2018](#bib.bib126)）通过在测试集上生成对抗样本来攻击最新的 VSE 模型，并评估了 VSE 模型的鲁棒性。他们通过引入三种方法对文本部分进行攻击：i)
    利用 WordNet 中的上义词/下义词关系替换图像说明中的名词；ii) 将数字替换为不同的数字，并在必要时将相应的名词单数化或复数化；iii) 检测关系并打乱不可互换的名词短语或替换介词。这种方法可以被视为黑箱编辑对抗攻击。
- en: 4.4.2\. Speech-to-Text
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2. 语音转文本
- en: 'Speech-to-text is also known as speech recognition. The task is to recognize
    and translate the spoken language into text automatically. (Carlini and Wagner,
    [[n. d.]](#bib.bib20)) attacked a state-of-the-art speech-to-text transcription
    neural network (based on LSTM), named DeepSpeech. Given a natural waveform, the
    authors constructed a audio perturbation that is almost inaudible but can be recognized
    by adding into the original waveform. The perturbation is constructed by adopting
    the idea from C&W method (refers to section [3.1](#S3.SS1.SSS0.Px4 "C&W Attack
    ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\.
    From Image to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey") ), which measures the image distortion by the maximum amount
    of changed pixels. Adapting this idea, they measured the audio distortion by calculating
    relative loudness of an audio and proposed to use Connectionist Temporal Classification
    loss for the optimization task. Then they solved this task with Adam optimizer
    (Kingma and Ba, [2015](#bib.bib63)).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '语音转文本也称为语音识别。任务是自动识别和翻译口语语言为文本。（Carlini 和 Wagner，[[n. d.]](#bib.bib20)）攻击了一种基于
    LSTM 的最先进的语音转文本转录神经网络，名为 DeepSpeech。给定自然波形，作者构造了一种几乎听不见的音频扰动，但通过将其添加到原始波形中可以被识别。该扰动是采用
    C&W 方法的理念（参见第 [3.1](#S3.SS1.SSS0.Px4 "C&W Attack ‣ 3.1\. Crafting Adversarial
    Examples: Inspiring Works in Computer Vision ‣ 3\. From Image to Text ‣ Adversarial
    Attacks on Deep Learning Models in Natural Language Processing: A Survey") 节），通过最大改变像素的数量来测量图像失真。采用这一理念，他们通过计算音频的相对响度来测量音频失真，并提出使用连接时序分类损失进行优化任务。然后，他们使用
    Adam 优化器（Kingma 和 Ba，[2015](#bib.bib63)）解决了这个任务。'
- en: 4.5\. Benchmark Datasets by Applications
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 按应用程序的基准数据集
- en: 'In recent years, neural networks gain success in different NLP domains and
    the popular applications include text classification, reading comprehension, machine
    translation, text summarization, question answering, dialogue generation, to name
    a few. In this section, we review the current works on generating adversarial
    examples on the neural networks in the perspective of NLP applications. Table
    [4](#S4.T4 "Table 4 ‣ 4.5\. Benchmark Datasets by Applications ‣ 4\. Attacking
    Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey") summarizes the works we reviewed
    in this article according to their application domain. We further list the benchmark
    datasets used in these works in the table as auxiliary information– thus we refer
    readers to the links/references we collect for the detailed descriptions of the
    datasets. Note that the auxiliary datasets which help to generate adversarial
    examples are not included. Instead, we only present the dataset used to evaluate
    the attacked neural networks.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '最近几年，神经网络在不同的自然语言处理（NLP）领域取得了成功，流行的应用包括文本分类、阅读理解、机器翻译、文本摘要、问答、对话生成等。在本节中，我们从
    NLP 应用的角度回顾了在神经网络上生成对抗样本的当前研究。表格 [4](#S4.T4 "Table 4 ‣ 4.5\. Benchmark Datasets
    by Applications ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial
    Attacks on Deep Learning Models in Natural Language Processing: A Survey") 总结了我们在本文中回顾的工作，并按照应用领域进行分类。我们进一步在表格中列出了这些工作中使用的基准数据集作为辅助信息——因此，我们建议读者参考我们收集的链接/参考文献，以获取数据集的详细描述。请注意，帮助生成对抗样本的辅助数据集未包含在内。我们只呈现了用于评估被攻击的神经网络的数据集。'
- en: '| Applications | Representative Works | Benchmark Datasets |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 应用领域 | 代表性工作 | 基准数据集 |'
- en: '| Classification | Text Classification | (Liang et al., [2017](#bib.bib79);
    Gong et al., [2018](#bib.bib40); Gao et al., [2018](#bib.bib36); Ebrahimi et al.,
    [2018](#bib.bib32); Sato et al., [2018](#bib.bib119); Neekhara et al., [2018](#bib.bib99))
    | DBpedia, Reuters Newswires, AG’s news, Sogou News, Yahoo! Answers, RCV1, Surname
    Classification Dataset |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 文本分类 | (Liang 等, [2017](#bib.bib79); Gong 等, [2018](#bib.bib40); Gao
    等, [2018](#bib.bib36); Ebrahimi 等, [2018](#bib.bib32); Sato 等, [2018](#bib.bib119);
    Neekhara 等, [2018](#bib.bib99)) | DBpedia, Reuters Newswires, AG’s news, Sogou
    News, Yahoo! Answers, RCV1, 姓名分类数据集 |'
- en: '| Sentiment Analysis | (Papernot et al., [2016b](#bib.bib105); Samanta and
    Mehta, [2018](#bib.bib118); Gao et al., [2018](#bib.bib36); Ebrahimi et al., [2018](#bib.bib32);
    Sato et al., [2018](#bib.bib119); Neekhara et al., [2018](#bib.bib99); Iyyer et al.,
    [2018](#bib.bib55); Singh et al., [2018](#bib.bib128)) | SST, IMDB Review, Yelp
    Review, Elec, Rotten Tomatoes Review, Amazon Review, Arabic Tweets Sentiment |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | (Papernot 等, [2016b](#bib.bib105); Samanta 和 Mehta, [2018](#bib.bib118);
    Gao 等, [2018](#bib.bib36); Ebrahimi 等, [2018](#bib.bib32); Sato 等, [2018](#bib.bib119);
    Neekhara 等, [2018](#bib.bib99); Iyyer 等, [2018](#bib.bib55); Singh 等, [2018](#bib.bib128))
    | SST, IMDB 评论, Yelp 评论, Elec, Rotten Tomatoes 评论, Amazon 评论, 阿拉伯语推文情感 |'
- en: '| Spam Detection | (Gao et al., [2018](#bib.bib36)) | Enron Spam, Datasets
    from (Zhang et al., [2015](#bib.bib156)) |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾邮件检测 | (Gao 等, [2018](#bib.bib36)) | Enron 垃圾邮件，来自 (Zhang 等, [2015](#bib.bib156))
    的数据集 |'
- en: '| Gender Identification | (Samanta and Mehta, [2018](#bib.bib118)) | Twitter
    Gender |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 性别识别 | (Samanta 和 Mehta, [2018](#bib.bib118)) | Twitter 性别 |'
- en: '| Grammar Error Detection | (Sato et al., [2018](#bib.bib119)) | FCE-public
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 语法错误检测 | (Sato 等, [2018](#bib.bib119)) | FCE-public |'
- en: '| Medical Status Prediction | (Sun et al., [2018](#bib.bib131)) | Electronic
    Health Records (EHR) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 医疗状态预测 | (Sun 等, [2018](#bib.bib131)) | 电子健康记录 (EHR) |'
- en: '| Malware Detection | (Grosse et al., [2016](#bib.bib46), [2017](#bib.bib47);
    Rosenberg et al., [2017](#bib.bib115); Hu and Tan, [2017](#bib.bib54); Al-Dujaili
    et al., [2018b](#bib.bib5)) | DREBIN, Microsoft Kaggle |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 恶意软件检测 | (Grosse 等, [2016](#bib.bib46), [2017](#bib.bib47); Rosenberg 等,
    [2017](#bib.bib115); Hu 和 Tan, [2017](#bib.bib54); Al-Dujaili 等, [2018b](#bib.bib5))
    | DREBIN, Microsoft Kaggle |'
- en: '| Relation Extraction | (Wu et al., [2017](#bib.bib146); Bekoulis et al., [2018a](#bib.bib12))
    | NYT Relation, UW Relation, ACE04, CoNLL04 EC, Dutch Real Estate Classifieds,
    Adverse Drug Events |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 关系提取 | (Wu 等, [2017](#bib.bib146); Bekoulis 等, [2018a](#bib.bib12)) | NYT
    关系, UW 关系, ACE04, CoNLL04 EC, 荷兰房地产分类广告, 不良药物事件 |'
- en: '| Machine Translation | (Belinkov and Bisk, [2018](#bib.bib14); Ebrahimi et al.,
    [[n. d.]](#bib.bib31); Cheng et al., [2018](#bib.bib25); Zhao et al., [2017](#bib.bib157))
    | TED Talks, WMT’16 Multimodal Translation Task |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 机器翻译 | (Belinkov 和 Bisk, [2018](#bib.bib14); Ebrahimi 等, [[n. d.]](#bib.bib31);
    Cheng 等, [2018](#bib.bib25); Zhao 等, [2017](#bib.bib157)) | TED 演讲, WMT’16 多模态翻译任务
    |'
- en: '| Machine Comprehension | (Jia and Liang, [2017](#bib.bib56); Wang and Bansal,
    [2018](#bib.bib143); Blohm et al., [2018](#bib.bib17); Chan et al., [2018](#bib.bib22))
    | SQuAD, MovieQA Multiple Choice, Logical QA |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 机器理解 | (Jia 和 Liang, [2017](#bib.bib56); Wang 和 Bansal, [2018](#bib.bib143);
    Blohm 等, [2018](#bib.bib17); Chan 等, [2018](#bib.bib22)) | SQuAD, MovieQA 多项选择,
    Logical QA |'
- en: '| Text Summarization | (Cheng et al., [2018](#bib.bib25)) | DUC2003, DUC2004,
    Gigaword |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 文本摘要 | (Cheng 等, [2018](#bib.bib25)) | DUC2003, DUC2004, Gigaword |'
- en: '| Text Entailment | (Kang et al., [2018](#bib.bib58); Minervini and Riedel,
    [2018](#bib.bib93); Iyyer et al., [2018](#bib.bib55); Zhao et al., [2017](#bib.bib157))
    | SNLI, SciTail, MultiNLI, SICK |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 文本蕴含 | (Kang 等, [2018](#bib.bib58); Minervini 和 Riedel, [2018](#bib.bib93);
    Iyyer 等, [2018](#bib.bib55); Zhao 等, [2017](#bib.bib157)) | SNLI, SciTail, MultiNLI,
    SICK |'
- en: '| POS Tagging | (Yasunaga et al., [2018](#bib.bib152)) | WSJ portion of PTB,
    Treebanks in UD |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | (Yasunaga 等, [2018](#bib.bib152)) | PTB 的 WSJ 部分, UD 的树库 |'
- en: '| Dialogue System | (Niu and Bansal, [2018](#bib.bib100)) | Ubuntu Dialogue,
    CoCoA, |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 对话系统 | (Niu 和 Bansal, [2018](#bib.bib100)) | Ubuntu 对话, CoCoA |'
- en: '| Cross-model | Optical Character Recognition | (Song and Shmatikov, [2018](#bib.bib130))
    | Hillary Clinton’s emails |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 跨模型 | 光学字符识别 | (Song 和 Shmatikov, [2018](#bib.bib130)) | 希拉里·克林顿的邮件 |'
- en: '| Scene Text Recognition | (Yuan et al., [2018](#bib.bib154)) | Street View
    Text, ICDAR 2013, IIIT5K |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 场景文本识别 | (Yuan 等, [2018](#bib.bib154)) | 街景文本, ICDAR 2013, IIIT5K |'
- en: '| Image Captioning | (Chen et al., [2018](#bib.bib23); Xu et al., [2018](#bib.bib149))
    | MSCOCO, Visual Genome |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 图像描述 | (Chen 等, [2018](#bib.bib23); Xu 等, [2018](#bib.bib149)) | MSCOCO,
    Visual Genome |'
- en: '| Visual Question Answering | (Xu et al., [2018](#bib.bib149)) | Datasets from
    (Antol et al., [2015](#bib.bib7)), Datasets from (Zhu et al., [2016](#bib.bib158))
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 视觉问答 | (Xu 等, [2018](#bib.bib149)) | 来自 (Antol 等, [2015](#bib.bib7)) 的数据集，来自
    (Zhu 等, [2016](#bib.bib158)) 的数据集 |'
- en: '|  | Visual-Semantic Embedding | (Shi et al., [2018](#bib.bib126)) | MSCOCO
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | 视觉-语义嵌入 | (Shi 等, [2018](#bib.bib126)) | MSCOCO |'
- en: '|  | Speech Tecognition | (Carlini and Wagner, [[n. d.]](#bib.bib20)) | Mozilla
    Common Voice |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | 语音识别 | (Carlini 和 Wagner, [[n. d.]](#bib.bib20)) | Mozilla Common Voice
    |'
- en: Table 4\. Attacked Applications and Benchmark Datasets
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 攻击应用和基准数据集
- en: 'Text Classification. Majority of the surveyed works attack the deep neural
    networks for text classification, since these tasks can be framed as a classification
    problem. Sentiment analysis aims to classify the sentiment to several groups (e.g.,
    in 3-group scheme: neural, positive and negative). Gender identification, Grammatical
    error detection and malware detection can be framed as binary classification problems.
    Relation extraction can be formulated as single or multi-classification problem.
    Predict medical status is a multi-class problem that the classes are defined by
    medical experts. These works usually use multiple datasets to evaluate their attack
    strategies to show the generality and robustness of their method. (Liang et al.,
    [2017](#bib.bib79)) used DBpedia ontology dataset (Lehmann et al., [2015](#bib.bib73))
    to classify the document samples into 14 high-level classes. (Gong et al., [2018](#bib.bib40))
    used IMDB movie reviews (Maas et al., [2011](#bib.bib86)) for sentiment analysis,
    and Reuters-2 and Reuters-5 newswires dataset provided by NLTK package^(10)^(10)10https://www.nltk.org/
    for categorization. (Papernot et al., [2016b](#bib.bib105)) used a un-specified
    movie review dataset for sentiment analysis. (Samanta and Mehta, [2018](#bib.bib118))
    also used IMDB movie review dataset for sentiment analysis. The work also performed
    gender classification on and Twitter dataset^(11)^(11)11https://www.kaggle.com/crowdflower/twitter-user-gender-cla2013.
    for gender detection. (Gao et al., [2018](#bib.bib36)) performed spam detection
    on Enron Spam Dataset (Metsis et al., [2006](#bib.bib92)) and adopted six large
    datasets from (Zhang et al., [2015](#bib.bib156)), i.e., AG’s news^(12)^(12)12https://www.di.unipi.it/˜gulli/,
    Sogou news (Wang et al., [2008](#bib.bib139)), DBPedia ontology dataset, Yahoo!
    Answers^(13)^(13)13Yahoo! Answers Comprehensive Questions and Answers version
    1.0 dataset through the Yahoo! Webscope program. for text categorization and Yelp
    reviews^(14)^(14)14Yelp Dataset Challenge in 2015, Amazon reviews (McAuley and
    Leskovec, [2013](#bib.bib91)) for sentiment analysis. (Ebrahimi et al., [2018](#bib.bib32))
    also used AG’s news for text classification. Further, they used Stanford Sentiment
    Treebank (SST) dataset (Socher et al., [2013](#bib.bib129)) for sentiment analysis.
    (Sato et al., [2018](#bib.bib119)) conducted evaluation on three tasks: sentiment
    analysis (IMDB movie review, Elec (Johnson and Zhang, [2015](#bib.bib57)), Rotten
    Tomatoes (Pang and Lee, [2005](#bib.bib104))), text categorization (DBpedia Ontology
    dataset and RCV1 (Lewis et al., [2004](#bib.bib74))) and grammatical error detection
    (FCE-public (Yannakoudakis et al., [2011](#bib.bib151))). (Sun et al., [2018](#bib.bib131))
    generated adversarial examples on the neural medical status prediction system
    with real-world electronic health records data. Many works target the malware
    detection models. (Grosse et al., [2016](#bib.bib46), [2017](#bib.bib47)) performed
    attack on neural malware detection systems. They used DREBIN dataset which contains
    both benigh and malicious android applications (Arp et al., [2014](#bib.bib8)).
    (Rosenberg et al., [2017](#bib.bib115)) collected benigh windows application files
    and used Microsoft Malware Classification Challenge dataset (Ronen et al., [2018](#bib.bib114))
    as the malicious part. (Hu and Tan, [2017](#bib.bib54)) crawled 180 programs with
    corresponding behavior reports from a website for malware analysis^(15)^(15)15https://malwr.com/.
    70% of the crawled programs are malware. (Neekhara et al., [2018](#bib.bib99))
    proposed another kind of attack, called reprogramming. They specifically targeted
    the text classification neural models and used four datasets to evaluate their
    attack methods: Surname Classification Dataset^(16)^(16)16Classifying names with
    a character-level rnn - pytroch tutorial. , Experimental Data for Question Classification
    (Li and Roth, [2002](#bib.bib77)), Arabic Tweets Sentiment Classification Dataset
    (Abdulla et al., [2013](#bib.bib2)) and IMDB movie review dataset. In (Wu et al.,
    [2017](#bib.bib146)), the authors modelled the relation extraction as a classification
    problem, where the goal is to predict the relations exist between entity pairs
    given text mentions. They used two relation datasets: NYT dataset (Riedel et al.,
    [2010](#bib.bib112)) and UW dataset (Liu et al., [2016](#bib.bib81)). The work
    (Bekoulis et al., [2018a](#bib.bib12)) targeted at improving the efficacy of the
    neural networks for joint entity and relation extraction. Different to the method
    in (Wu et al., [2017](#bib.bib146)), the authors modelled the relation extraction
    task as a multi-label head selection problem. The four datasets are used in their
    work: ACE04 dataset (Doddington et al., [2004](#bib.bib29)), CoNLL04 EC tasks
    (Roth and Yih, [2004](#bib.bib116)), Dutch Real Estate Classifieds (DREC) dataset
    (Bekoulis et al., [2018b](#bib.bib13)), and Adverse Drug Events (ADE) (Gurulingappa
    et al., [2012](#bib.bib48)).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类。大多数调查的研究攻击了用于文本分类的深度神经网络，因为这些任务可以被框架为分类问题。情感分析旨在将情感分类到几个组别（例如，在三组方案中：神经、积极和消极）。性别识别、语法错误检测和恶意软件检测可以被框架为二分类问题。关系抽取可以被表述为单一或多类别分类问题。预测医疗状态是一个多类别问题，其类别由医疗专家定义。这些研究通常使用多个数据集来评估其攻击策略，以展示其方法的普遍性和稳健性。
    (Liang et al., [2017](#bib.bib79)) 使用了 DBpedia 本体数据集 (Lehmann et al., [2015](#bib.bib73))
    将文档样本分类为 14 个高层次类别。(Gong et al., [2018](#bib.bib40)) 使用了 IMDB 电影评论 (Maas et al.,
    [2011](#bib.bib86)) 进行情感分析，并使用 NLTK 包提供的 Reuters-2 和 Reuters-5 新闻数据集^(10)^(10)10https://www.nltk.org/
    进行分类。(Papernot et al., [2016b](#bib.bib105)) 使用了未指定的电影评论数据集进行情感分析。(Samanta 和 Mehta,
    [2018](#bib.bib118)) 也使用了 IMDB 电影评论数据集进行情感分析。该工作还对 Twitter 数据集^(11)^(11)11https://www.kaggle.com/crowdflower/twitter-user-gender-cla2013
    进行了性别分类。(Gao et al., [2018](#bib.bib36)) 对 Enron 垃圾邮件数据集 (Metsis et al., [2006](#bib.bib92))
    进行了垃圾邮件检测，并采用了 (Zhang et al., [2015](#bib.bib156)) 的六个大型数据集，即 AG 的新闻^(12)^(12)12https://www.di.unipi.it/˜gulli/，搜狗新闻
    (Wang et al., [2008](#bib.bib139))，DBPedia 本体数据集，Yahoo! Answers^(13)^(13)13Yahoo!
    Answers Comprehensive Questions and Answers version 1.0 dataset 通过 Yahoo! Webscope
    程序进行文本分类，以及 Yelp 评论^(14)^(14)14Yelp Dataset Challenge in 2015，亚马逊评论 (McAuley 和
    Leskovec, [2013](#bib.bib91)) 进行情感分析。(Ebrahimi et al., [2018](#bib.bib32)) 也使用了
    AG 的新闻进行文本分类。此外，他们还使用了 Stanford Sentiment Treebank (SST) 数据集 (Socher et al., [2013](#bib.bib129))
    进行情感分析。(Sato et al., [2018](#bib.bib119)) 对三个任务进行了评估：情感分析（IMDB 电影评论，Elec (Johnson
    和 Zhang, [2015](#bib.bib57))，烂番茄 (Pang 和 Lee, [2005](#bib.bib104))），文本分类（DBpedia
    本体数据集和 RCV1 (Lewis et al., [2004](#bib.bib74))) 以及语法错误检测（FCE-public (Yannakoudakis
    et al., [2011](#bib.bib151)))。(Sun et al., [2018](#bib.bib131)) 在神经医疗状态预测系统上生成了具有真实世界电子健康记录数据的对抗示例。许多研究针对恶意软件检测模型。(Grosse
    et al., [2016](#bib.bib46)，[2017](#bib.bib47)) 对神经恶意软件检测系统进行了攻击。他们使用了 DREBIN 数据集，该数据集包含了良性和恶意的安卓应用程序
    (Arp et al., [2014](#bib.bib8))。(Rosenberg et al., [2017](#bib.bib115)) 收集了良性的
    Windows 应用程序文件，并使用 Microsoft Malware Classification Challenge 数据集 (Ronen et al.,
    [2018](#bib.bib114)) 作为恶意部分。(Hu 和 Tan, [2017](#bib.bib54)) 从一个网站抓取了 180 个程序及其相应的行为报告进行恶意软件分析^(15)^(15)15https://malwr.com/。70%
    的抓取程序是恶意软件。(Neekhara et al., [2018](#bib.bib99)) 提出了另一种攻击方式，称为重编程。他们特别针对文本分类神经模型，并使用四个数据集评估他们的攻击方法：姓氏分类数据集^(16)^(16)16Classifying
    names with a character-level rnn - pytroch tutorial.，问题分类实验数据 (Li 和 Roth, [2002](#bib.bib77))，阿拉伯语推文情感分类数据集
    (Abdulla et al., [2013](#bib.bib2)) 和 IMDB 电影评论数据集。在 (Wu et al., [2017](#bib.bib146))
    中，作者将关系抽取建模为分类问题，目标是预测给定文本提及的实体对之间存在的关系。他们使用了两个关系数据集：NYT 数据集 (Riedel et al., [2010](#bib.bib112))
    和 UW 数据集 (Liu et al., [2016](#bib.bib81))。工作 (Bekoulis et al., [2018a](#bib.bib12))
    旨在提高神经网络在联合实体和关系抽取中的有效性。与 (Wu et al., [2017](#bib.bib146)) 中的方法不同，作者将关系抽取任务建模为多标签头选择问题。他们在工作中使用了四个数据集：ACE04
    数据集 (Doddington et al., [2004](#bib.bib29))，CoNLL04 EC 任务 (Roth 和 Yih, [2004](#bib.bib116))，荷兰房地产分类数据集
    (DREC) (Bekoulis et al., [2018b](#bib.bib13)) 和不良药物事件 (ADE) (Gurulingappa et al.,
    [2012](#bib.bib48))。
- en: Machine Translation. Machine Translation works on parallel datasets, one of
    which uses source language and the other one is in the target language. (Belinkov
    and Bisk, [2018](#bib.bib14)) used the TED talks parallel corpus prepared for
    IWSLT 2016 (Mauro et al., [2012](#bib.bib90)) for testing the NMT systems. They
    also collected French, German and Czech corpus for generating natural noises to
    build a look-up table which contains possible lexical replacements that later
    be used for generating adversarial examples. (Ebrahimi et al., [[n. d.]](#bib.bib31))
    also used the same TED talks corpus and used German to English, Czech to English,
    and French to English pairs.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译。机器翻译基于平行数据集，一个使用源语言，另一个使用目标语言。 (Belinkov 和 Bisk, [2018](#bib.bib14)) 使用了为
    IWSLT 2016 准备的 TED 演讲平行语料库 (Mauro 等, [2012](#bib.bib90)) 来测试 NMT 系统。他们还收集了法语、德语和捷克语语料库，用于生成自然噪声，建立一个包含可能的词汇替代项的查找表，后续将用于生成对抗性样本。
    (Ebrahimi 等, [[n. d.]](#bib.bib31)) 也使用了相同的 TED 演讲语料库，并使用了德语到英语、捷克语到英语和法语到英语的配对。
- en: Machine Comprehension. Machine comprehension datasets usually provide context
    documents or paragraphs to the machines. Based on the comprehension of the contexts,
    machine comprehension models can answer a question. Jia and Liang are one of the
    first to consider the textual adversary and they targeted the neural machine comprehension
    models (Jia and Liang, [2017](#bib.bib56)). They used the Stanford Question Answering
    Dataset (SQuAD) to evaluate the impact of their attack on the neural machine comprehension
    models. SQuAD is a widely recognised benchmark dataset for machine comprehension.
    (Wang and Bansal, [2018](#bib.bib143)) followed the previous works and also worked
    on SQuAD dataset. Althouth the focus of the work (Blohm et al., [2018](#bib.bib17))
    is to develop a robust machine comprehension model rather than attacking MC models,
    they used the adversarial examples to evaluate their proposed system. They used
    MovieQA multiple choice question answering dataset (Tapaswi et al., [2016](#bib.bib135))
    for the evaluation. (Chan et al., [2018](#bib.bib22)) targeted attacks on differentiable
    neural computer (DNC), which is a novel computing machine with DNN. They evaluated
    the attacks on logical question answering using bAbI tasks^(17)^(17)17https://research.fb.com/downloads/babi/.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 机器理解。机器理解数据集通常向机器提供上下文文档或段落。基于对上下文的理解，机器理解模型可以回答问题。 Jia 和 Liang 是最早考虑文本对抗者之一，他们以神经机器理解模型为目标
    (Jia 和 Liang, [2017](#bib.bib56))。他们使用了斯坦福问答数据集 (SQuAD) 来评估他们的攻击对神经机器理解模型的影响。
    SQuAD 是一个广泛认可的机器理解基准数据集。 (Wang 和 Bansal, [2018](#bib.bib143)) 继承了之前的工作，也在 SQuAD
    数据集上进行了研究。虽然 (Blohm 等, [2018](#bib.bib17)) 的工作重点是开发一个稳健的机器理解模型，而非攻击 MC 模型，他们使用对抗性样本来评估他们提出的系统。他们使用了
    MovieQA 多项选择题问答数据集 (Tapaswi 等, [2016](#bib.bib135)) 进行评估。 (Chan 等, [2018](#bib.bib22))
    对可微分神经计算机 (DNC) 进行了攻击，这是一种具有 DNN 的新型计算机。他们评估了对逻辑问答的攻击，使用了 bAbI 任务^(17)^(17)17https://research.fb.com/downloads/babi/。
- en: Text Summarization. The goal for text summarization is to summarize the core
    meaning of a given document or paragraph with succinct expressions. There is no
    surveyed papers that only target the application of text summarization. (Cheng
    et al., [2018](#bib.bib25)) evaluated their attack on multiple applications including
    text summarization and they used DUC2003^(18)^(18)18http://duc.nist.gov/duc2003/tasks.html,
    DUC2004^(19)^(19)19http://duc.nist.gov/duc2004/, and Gigaword^(20)^(20)20https://catalog.ldc.upenn.edu/LDC2003T05
    for evaluating the effectiveness of adversarial examples.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要。文本摘要的目标是用简洁的表达总结给定文档或段落的核心含义。目前尚未有针对文本摘要应用的专门调查论文。 (Cheng 等, [2018](#bib.bib25))
    评估了他们对包括文本摘要在内的多种应用的攻击，并使用了 DUC2003^(18)^(18)18http://duc.nist.gov/duc2003/tasks.html、DUC2004^(19)^(19)19http://duc.nist.gov/duc2004/
    和 Gigaword^(20)^(20)20https://catalog.ldc.upenn.edu/LDC2003T05 来评估对抗性样本的有效性。
- en: 'Text Entailment. The fundamental task of text entailment is to decide whether
    a premise text entails a hypothesis, i.e., the truth of one text fragment follows
    from another text. (Kang et al., [2018](#bib.bib58)) assessed various models on
    two entailment datasets: Standord Natural Lauguage Inference (SNLI) (Bowman et al.,
    [2015](#bib.bib18)) and SciTail (Khot et al., [2018](#bib.bib60)). (Minervini
    and Riedel, [2018](#bib.bib93)) also used SNLI dataset. Furthermore, they used
    MultiNLI (Williams et al., [2018](#bib.bib145)) dataset.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 文本蕴含。文本蕴含的基本任务是判断一个前提文本是否蕴含一个假设，即一个文本片段的真实性是否从另一个文本中得出。（Kang et al., [2018](#bib.bib58)）在两个蕴含数据集上评估了各种模型：Standord
    Natural Language Inference (SNLI)（Bowman et al., [2015](#bib.bib18)）和SciTail（Khot
    et al., [2018](#bib.bib60)）。（Minervini and Riedel, [2018](#bib.bib93)）也使用了SNLI数据集。此外，他们还使用了MultiNLI（Williams
    et al., [2018](#bib.bib145)）数据集。
- en: 'Part-of-Speech (POS) Tagging. The purpose for POS tagging is to resolve the
    part-of-speech for each word in a sentence, such as noun, verb etc. It is one
    of the fundamental NLP tasks to facilitate other NLP tasks, e.g., syntactic parsing.
    Neural networks are also adopted for this NLP task. (Yasunaga et al., [2018](#bib.bib152))
    adopted the method in (Miyato et al., [2016a](#bib.bib95)) to build a more robust
    neural network by introducing adversarial training, but they applied the strategy
    (with minor modifications) in POS tagging. By training on the mixture of clean
    and adversarial example, the authors found that adversarial examples not only
    help improving the tagging accuracy, but also contribute to downstream task of
    dependency parsing and is generally effective in different sequence labelling
    tasks. The datasets used in their evaluation include: the Wall Street Journal
    (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., [1993](#bib.bib88)) and
    treebanks from Universal Dependencies (UD) v1.2 (Nivre et al., [2015](#bib.bib101)).'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注（POS Tagging）。词性标注的目的是确定句子中每个单词的词性，例如名词、动词等。它是基础的自然语言处理（NLP）任务之一，有助于其他NLP任务，比如句法分析。神经网络也被应用于这个NLP任务中。（Yasunaga
    et al., [2018](#bib.bib152)）采用了（Miyato et al., [2016a](#bib.bib95)）的方法，通过引入对抗训练来构建更强大的神经网络，但他们将这一策略（经过小的修改）应用于词性标注。通过在干净样本和对抗样本的混合数据上进行训练，作者发现对抗样本不仅有助于提高标注准确性，还对下游任务如依存句法分析有贡献，并且在不同的序列标注任务中普遍有效。他们评估中使用的数据集包括：Penn
    Treebank (PTB)（Marcus et al., [1993](#bib.bib88)）的《华尔街日报》（WSJ）部分以及Universal Dependencies
    (UD) v1.2（Nivre et al., [2015](#bib.bib101)）的树库。
- en: Dialogue Generation. Dialogue generation is a fundamental component for real-world
    virtual assistants such as Siri^(21)^(21)21https://www.apple.com/au/siri/ and
    Alexa^(22)^(22)22https://en.wikipedia.org/wiki/Amazon_Echo. It is the text generation
    task that automatically generate a response given a post by the user. (Niu and
    Bansal, [2018](#bib.bib100)) is one of the first to attack the generative dialogue
    models. They used the Ubuntu Dialogue Corpus (Lowe et al., [2015](#bib.bib85))
    and Dynamic Knowledge Graph Network with the Collaborative Communicating Agents
    (CoCoA) dataset (He et al., [2017](#bib.bib50)) for the evaluation of their two
    attack strategies.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 对话生成。对话生成是现实世界虚拟助手（如Siri^(21)^(21)21https://www.apple.com/au/siri/ 和 Alexa^(22)^(22)22https://en.wikipedia.org/wiki/Amazon_Echo）的一个基本组件。它是一个文本生成任务，自动生成用户发出的帖子对应的回复。（Niu
    and Bansal, [2018](#bib.bib100)）是首批研究生成对话模型的学者之一。他们使用了Ubuntu对话语料库（Lowe et al.,
    [2015](#bib.bib85)）和带有协作通信代理（CoCoA）数据集（He et al., [2017](#bib.bib50)）的动态知识图谱网络来评估他们的两种攻击策略。
- en: Cross-model Applications. (Song and Shmatikov, [2018](#bib.bib130)) evaluated
    the OCR systems with adversarial examples using Hillary Clinton’s emails^(23)^(23)23https://www.
    kaggle.com/kaggle/hillary-clinton-emails/data, which is in the form of images.
    They also conducted the attack on NLP applications using Rotten Tomatoes and IMDB
    review datasets. The work in (Yuan et al., [2018](#bib.bib154)) attacked the neural
    networks designed for scene text rcognition. They conducted experiments on three
    standard benchmarks for cropped word image recognition, namely the Street View
    Text dataset (SVT) (Wang et al., [2011](#bib.bib140)) the ICDAR 2013 dataset (IC13)
    (Karatzas et al., [2013](#bib.bib59)) and the IIIT 5K-word dataset (IIIT5K) (Mishra
    et al., [2012](#bib.bib94)). (Chen et al., [2018](#bib.bib23)) attacked the image
    captioning neural models. The dataset they used is the Microsoft COCO (MSCOCO)
    dataset (Lin et al., [2014](#bib.bib80)). (Xu et al., [2018](#bib.bib149)) worked
    on the problems of attacking neural models for image captioning and visual question
    answering. For the first task, they used Visual Genome dataset (Krishna et al.,
    [2017](#bib.bib66)). For the second task, they used the VQA datasets collected
    and processed in (Antol et al., [2015](#bib.bib7)). (Shi et al., [2018](#bib.bib126))
    worked on Visual-Semantic Embedding applications, where the MSCOCO dataset is
    used. (Carlini and Wagner, [[n. d.]](#bib.bib20)) targeted the speech recognition
    problem. The datasets they used is the Mozilla Common Voice dataset^(24)^(24)24https://voice.mozilla.org/en.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模型应用。（Song 和 Shmatikov, [2018](#bib.bib130)）使用希拉里·克林顿的电子邮件^(23)^(23)23https://www.kaggle.com/kaggle/hillary-clinton-emails/data，这些邮件以图像形式存在，评估了带有对抗样本的OCR系统。他们还对NLP应用进行了攻击，使用了Rotten
    Tomatoes和IMDB的评论数据集。（Yuan 等人, [2018](#bib.bib154)）攻击了设计用于场景文本识别的神经网络。他们在三个标准基准数据集上进行了实验，这些数据集用于裁剪的词图像识别，包括街景文本数据集（SVT）（Wang
    等人, [2011](#bib.bib140)）、ICDAR 2013数据集（IC13）（Karatzas 等人, [2013](#bib.bib59)）和IIIT
    5K词数据集（IIIT5K）（Mishra 等人, [2012](#bib.bib94)）。(Chen 等人, [2018](#bib.bib23)) 攻击了图像描述神经模型。他们使用的数据集是微软COCO（MSCOCO）数据集（Lin
    等人, [2014](#bib.bib80)）。(Xu 等人, [2018](#bib.bib149)) 处理了攻击神经模型用于图像描述和视觉问答的问题。对于第一个任务，他们使用了Visual
    Genome数据集（Krishna 等人, [2017](#bib.bib66)）。对于第二个任务，他们使用了（Antol 等人, [2015](#bib.bib7)）收集和处理的VQA数据集。（Shi
    等人, [2018](#bib.bib126)）研究了视觉-语义嵌入应用，使用了MSCOCO数据集。（Carlini 和 Wagner, [[n. d.]](#bib.bib20)）针对了语音识别问题。他们使用的数据集是Mozilla
    Common Voice数据集^(24)^(24)24https://voice.mozilla.org/en。
- en: 'Multi-Applications Some works adapt their attack methods into different applications,
    namely, they evaluate their method’s trasferability across applications. (Cheng
    et al., [2018](#bib.bib25)) attacked the sequence-to-sequence models. Specifically,
    they evaluated their attack on two applications: text summarization and machine
    translation. For text summarization, as mentioned before, they used three datasets
    DUC2003, DUC2004, and Gigaword. For the machine translation, they sampled a subset
    form WMT’16 Multimodal Translation dataset^(25)^(25)25http://www.statmt.org/wmt16/translation-task.html.
    (Iyyer et al., [2018](#bib.bib55)) proposed syntactically adversarial paraphrase
    and evaluated the attack on sentiment analysis and text entailment applications.
    They used SST for sentimental analysis and SICK (Marelli et al., [2014](#bib.bib89))
    for text entailment. (Zhao et al., [2017](#bib.bib157)) is a generic approach
    for generating adversarial examples on neural models. The applications investigated
    include image classification (MINIST digital image dataset), textual entailment
    (SNLI), and machine translation. (Miyato et al., [2016a](#bib.bib95)) evaluated
    their attacks on five datasets, covering both sentiment analysis (IMDB movie review,
    Elec product review, Rotten Tomatoes movie review) and text categorization (DBpedia
    Ontology, RCV1 news articles). (Singh et al., [2018](#bib.bib128)) targeted two
    applications. For sentiment analysis, they used Rotten Tomato movie reviews and
    IMDB movie reviews datasets. For visual question answering, they tested on dataset
    provided by Zhu et al. (Zhu et al., [2016](#bib.bib158)).'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 多应用 一些研究将其攻击方法适配到不同的应用中，即评估其方法在不同应用中的迁移能力。（Cheng et al., [2018](#bib.bib25)）对序列到序列模型进行了攻击。具体来说，他们在两个应用中评估了他们的攻击：文本摘要和机器翻译。对于文本摘要，如前所述，他们使用了三个数据集
    DUC2003、DUC2004 和 Gigaword。对于机器翻译，他们从 WMT’16 多模态翻译数据集中抽取了一个子集^(25)^(25)25http://www.statmt.org/wmt16/translation-task.html。（Iyyer
    et al., [2018](#bib.bib55)）提出了语法对抗性释义，并在情感分析和文本蕴含应用中评估了攻击。他们使用了 SST 进行情感分析，使用了
    SICK (Marelli et al., [2014](#bib.bib89)) 进行文本蕴含。（Zhao et al., [2017](#bib.bib157)）提出了一种生成神经模型对抗样本的通用方法。调查的应用包括图像分类（MINIST
    数字图像数据集）、文本蕴含（SNLI）和机器翻译。（Miyato et al., [2016a](#bib.bib95)）在五个数据集上评估了他们的攻击，涵盖了情感分析（IMDB
    电影评论、Elec 产品评论、Rotten Tomatoes 电影评论）和文本分类（DBpedia Ontology、RCV1 新闻文章）。（Singh et
    al., [2018](#bib.bib128)）针对两个应用进行了测试。对于情感分析，他们使用了 Rotten Tomato 电影评论和 IMDB 电影评论数据集。对于视觉问答，他们在
    Zhu et al. 提供的数据集上进行了测试（Zhu et al., [2016](#bib.bib158)）。
- en: 5\. Defense
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 防御
- en: 'An essential purpose for generating adversarial examples for neural networks
    is to utilize these adversarial examples to enhance the model’s robustness (Goodfellow
    et al., [2015](#bib.bib43)). There are two common ways in textual DNN to achieve
    this goal: adversarial training and knowledge distillation. Adversarial training
    incorporates adversarial examples in the model training process. Knowledge distillation
    manipulates the neural network model and trains a new model. In this section,
    we introduce some representative studies belonging to these two directions. For
    more comprehensive defense strategies on machine learning and deep leaning models
    and applications, please refer to (Biggio and Roli, [2018](#bib.bib16); Akhtar
    and Mian, [2018](#bib.bib3)).'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 生成神经网络对抗样本的一个重要目的是利用这些对抗样本来增强模型的鲁棒性（Goodfellow et al., [2015](#bib.bib43)）。在文本
    DNN 中实现这一目标有两种常见方式：对抗训练和知识蒸馏。对抗训练将对抗样本纳入模型训练过程。知识蒸馏则操作神经网络模型并训练一个新的模型。在本节中，我们介绍了一些属于这两个方向的代表性研究。有关机器学习和深度学习模型及应用的更全面的防御策略，请参考（Biggio
    和 Roli, [2018](#bib.bib16); Akhtar 和 Mian, [2018](#bib.bib3)）。
- en: 5.1\. Adversarial Training
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 对抗训练
- en: Szegedy et al. (Szegedy et al., [2014](#bib.bib133)) invented adversarial training,
    a strategy that consists of training a neural network to correctly classify both
    normal examples and adversarial examples. Goodfellow et al. (Goodfellow et al.,
    [2015](#bib.bib43)) employed explicit training with adversarial examples. In this
    section, we describe works utilizing data augmentation, model regularization and
    robust optimization for the defense purpose on textual adversarial attacks.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy et al.（Szegedy et al., [2014](#bib.bib133)）发明了对抗训练，这是一种策略，包含训练神经网络以正确分类正常样本和对抗样本。Goodfellow
    et al.（Goodfellow et al., [2015](#bib.bib43)）采用了显式的对抗样本训练。在本节中，我们描述了利用数据增强、模型正则化和鲁棒优化来防御文本对抗攻击的研究工作。
- en: 5.1.1\. Data Augmentation
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 数据增强
- en: Data augmentation extends the original training set with the generated adversarial
    examples and try to let the model see more data during the training process. Data
    augmentation is commonly used against black-box attacks with additional training
    epochs on the attacked DNN with adversarial examples.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强通过生成对抗样本扩展了原始训练集，并尝试在训练过程中让模型看到更多的数据。数据增强通常用于针对黑箱攻击，通过在攻击的深度神经网络上进行额外的训练轮次来实现。
- en: 'The authors in work (Jia and Liang, [2017](#bib.bib56)) try to enhance the
    reading comprehension model with training on the augmented dataset that includes
    the adversarial examples. They showed that this data augmentation is effective
    and robust against the attack that uses the same adversarial examples. However,
    their work also demonstrated that this augmentation strategy would be still vulnerable
    against the attacks with other kinds of adversarial examples. (Wang and Bansal,
    [2018](#bib.bib143)) shared similar idea to augment the training dataset, but
    selected further informative adversarial examples as discussed in Section [4.3.1](#S4.SS3.SSS1
    "4.3.1\. Concatenation Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking Neural
    Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey").'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '在（Jia 和 Liang，[2017](#bib.bib56)）的研究中，作者尝试通过在包含对抗样本的增强数据集上进行训练来增强阅读理解模型。他们展示了这种数据增强在使用相同对抗样本的攻击下是有效且稳健的。然而，他们的研究还表明，这种增强策略对其他类型的对抗样本的攻击仍然脆弱。（Wang
    和 Bansal，[2018](#bib.bib143)）分享了类似的思想来增强训练数据集，但选择了更具信息量的对抗样本，如第[4.3.1](#S4.SS3.SSS1
    "4.3.1\. Concatenation Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking Neural
    Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey")节所讨论。'
- en: 'The work in (Kang et al., [2018](#bib.bib58)) trains the text entailment system
    augmented with adversarial examples. The purpose is to make the system more robust.
    They proposed three methods to generate more data with diverse characteristics:
    (1) knowledge-based, which replaces words with their hypernym/hyponym provided
    in several given knowledge bases; (2) hand-crafted, which adds negations to the
    the existing entailment; (3) neural-based, which leverages a seq2seq model to
    generate an entailment examples by enforcing the loss function to measure the
    cross-entropy between the original hypothesis and the predicted hypothesis. During
    the training process, they adopt the idea from generative adversarial network
    to train a discriminator and a generator, and incorporating the adversarial examples
    in the discriminator’s optimization step.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: （Kang 等人，[2018](#bib.bib58)）的研究通过对抗样本增强训练文本蕴涵系统。目的是使系统更为稳健。他们提出了三种生成更多具有多样性特征的数据的方法：（1）基于知识的方法，用于将单词替换为几个给定知识库中的超义词/下义词；（2）手工制作的方法，用于向现有蕴涵添加否定；（3）基于神经网络的方法，利用
    seq2seq 模型生成蕴涵样本，通过强制损失函数来测量原始假设与预测假设之间的交叉熵。在训练过程中，他们借鉴了生成对抗网络的思想，训练了一个鉴别器和一个生成器，并将对抗样本纳入鉴别器的优化步骤。
- en: (Belinkov and Bisk, [2018](#bib.bib14)) explores another way for data augmentation.
    It takes the average character embedding as a word representation and incorporate
    it into the input. This approach is intrinsically insensitive to character scrambling
    such as swap, mid and Rand, thus can resists to noises caused by these scrambling
    attacks proposed in the work. However, this defense is ineffective to other attacks
    that do not perturb on characters’ orders.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: （Belinkov 和 Bisk，[2018](#bib.bib14)）探讨了另一种数据增强方法。它将平均字符嵌入作为词表示并将其纳入输入。这种方法对字符的扰动（如交换、中间和随机）本质上不敏感，因此可以抵抗这些扰动攻击带来的噪声。然而，这种防御对其他不干扰字符顺序的攻击效果不佳。
- en: 5.1.2\. Model Regularization
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 模型正则化
- en: 'Model regularization enforces the generated adversarial examples as the regularizer
    and follows the form of:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 模型正则化将生成的对抗样本作为正则化项，并遵循以下形式：
- en: '| (32) |  | $\displaystyle\min(J(f(x),y)+\lambda J(f(x^{\prime}),y))$ |  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $\displaystyle\min(J(f(x),y)+\lambda J(f(x^{\prime}),y))$ |  |'
- en: where $\lambda$ is a hyperparameter.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是一个超参数。
- en: 'Following (Goodfellow et al., [2015](#bib.bib43)), the work (Miyato et al.,
    [2016a](#bib.bib95)) constructed the adversarial training with a linear approximation
    as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 根据（Goodfellow 等人，[2015](#bib.bib43)），该研究（Miyato 等人，[2016a](#bib.bib95)）使用线性近似构建了对抗训练，如下所示：
- en: '| (33) |  | $\displaystyle-\log p(y&#124;x+-\epsilon g/&#124;&#124;g&#124;&#124;_{2},;\theta)$
    |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $\displaystyle-\log p(y&#124;x+-\epsilon g/&#124;&#124;g&#124;&#124;_{2},;\theta)$
    |  |'
- en: '|  | $\displaystyle g=\partial_{x}\log p(y&#124;x;\hat{\theta})$ |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g=\partial_{x}\log p(y&#124;x;\hat{\theta})$ |  |'
- en: where $||g||_{2}$ is the $L_{2}$ norm regularization, $\theta$ is the parameter
    of the neural model, and $\hat{\theta}$ is a constant copy of $\theta$. The difference
    to (Goodfellow et al., [2015](#bib.bib43)) is that, the authors performed the
    adversarial generation and training in terms of the word embedding. Further, they
    extended their previous work on attacking image deep neural model (Miyato et al.,
    [2016b](#bib.bib96)), where the local distribution smoothness (LDS) is defined
    as the negative of the KL divergence of two distributions (original data and the
    adversaries). LDS measures the robustness of the model against the perturbation
    in local and ‘virtual’ adversarial direction. In this sense, the adversary is
    calculated as the direction to which the model distribution is most sensitive
    in terms of KL divergence. They also applied this attack strategy on word embedding
    and performed adversarial training by adding adversarial examples as regularizer.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $||g||_{2}$ 是 $L_{2}$ 范数正则化，$\theta$ 是神经模型的参数，而 $\hat{\theta}$ 是 $\theta$
    的一个常量副本。与 (Goodfellow et al., [2015](#bib.bib43)) 的不同之处在于，作者在词嵌入的基础上进行了对抗生成和训练。此外，他们扩展了之前在攻击图像深度神经模型上的工作
    (Miyato et al., [2016b](#bib.bib96))，其中局部分布光滑度（LDS）定义为两个分布（原始数据和对抗样本）的KL散度的负值。LDS
    衡量模型在局部和“虚拟”对抗方向上的鲁棒性。从这个意义上讲，对抗样本是根据模型分布在KL散度方面最敏感的方向来计算的。他们还将这种攻击策略应用于词嵌入，并通过添加对抗样本作为正则化器进行对抗训练。
- en: 'The work (Sato et al., [2018](#bib.bib119)) follows the idea from (Miyato et al.,
    [2016a](#bib.bib95)) and extends the adversarial training on LSTM. The authors
    followed FGSM to incorporate the adversarial training as a regularizer. But in
    order to enable the interpretability of adversarial examples, i.e., the word embedding
    of the adversaries should be valid word embeddings in the vocabulary, they introduced
    a direction vector which associates the perturbed embedding to the valid word
    embedding. (Wu et al., [2017](#bib.bib146)) simply adopts the regularizer utilized
    in (Miyato et al., [2016a](#bib.bib95)), but applies the perturbations on pre-trained
    word embedding and in a different task: relation extraction. Other similar works
    that adopt (Miyato et al., [2016a](#bib.bib95)) are (Wu et al., [2017](#bib.bib146);
    Yasunaga et al., [2018](#bib.bib152); Sato et al., [2018](#bib.bib119); Bekoulis
    et al., [2018a](#bib.bib12)). We will not cover all these works in this article,
    since they simply adopting this method.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究 (Sato et al., [2018](#bib.bib119)) 延续了 (Miyato et al., [2016a](#bib.bib95))
    的思想，并扩展了对LSTM的对抗训练。作者遵循了FGSM，将对抗训练作为正则化器。但是，为了使对抗样本具有可解释性，即对抗样本的词嵌入应该在词汇表中是有效的词嵌入，他们引入了一个方向向量，将扰动的嵌入与有效的词嵌入相关联。(Wu
    et al., [2017](#bib.bib146)) 简单地采用了 (Miyato et al., [2016a](#bib.bib95)) 中使用的正则化器，但将扰动应用于预训练的词嵌入，并在不同的任务中：关系提取。其他采用
    (Miyato et al., [2016a](#bib.bib95)) 的类似工作包括 (Wu et al., [2017](#bib.bib146);
    Yasunaga et al., [2018](#bib.bib152); Sato et al., [2018](#bib.bib119); Bekoulis
    et al., [2018a](#bib.bib12))。我们在本文中不会覆盖所有这些工作，因为它们只是简单地采用了这种方法。
- en: 5.1.3\. Robust Optimisation
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 鲁棒优化
- en: 'Madry et al. (Madry et al., [2018](#bib.bib87)) cast DNN model learning as
    a robust optimization with min-max (saddle point) formulation, which is the composition
    of an inner non-concave maximization problem (attack) and an outer non-convex
    minimization problem (defense). According to Danskin’s theorem, gradients at inner
    maximizers correspond to descent directions for the min-max problem, thus the
    optimization can still apply back-propagation to proceed. The approach successfully
    demonstrated robustness of DNNs against adversarial images by training and learning
    universally. (Al-Dujaili et al., [2018a](#bib.bib4)) adopts the idea and applies
    on malware detection DNN that handles discrete data. Their leaning objective is
    formulated as:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: Madry et al. (Madry et al., [2018](#bib.bib87)) 将DNN模型学习视为具有最小-最大（鞍点）形式的鲁棒优化，这是一种内在非凹最大化问题（攻击）和外在非凸最小化问题（防御）的组合。根据Danskin定理，内在最大化器的梯度对应于最小-最大问题的下降方向，因此优化仍然可以应用反向传播来进行。该方法通过训练和学习通用性成功展示了DNN对抗对抗图像的鲁棒性。(Al-Dujaili
    et al., [2018a](#bib.bib4)) 采用了这一思路，并应用于处理离散数据的恶意软件检测DNN。他们的学习目标被公式化为：
- en: '| (34) |  | $\displaystyle\theta^{*}=arg\min_{\theta}\mathbb{E}_{(x,y)\sim
    D}[\max_{x^{\prime}\in S(x)}L(\theta,x^{\prime},y)]$ |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $\displaystyle\theta^{*}=arg\min_{\theta}\mathbb{E}_{(x,y)\sim
    D}[\max_{x^{\prime}\in S(x)}L(\theta,x^{\prime},y)]$ |  |'
- en: where $S(x)$ is the set of binary indicator vectors that preserve the functionality
    of malware $x$, $L$ is the loss function for the original classification model,
    $y$ is the groundtruth label, $\theta$ is the learnable parameters, $D$ denotes
    the distribution of data sample $x$.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S(x)$ 是保留恶意软件 $x$ 功能的二进制指示向量集合，$L$ 是原始分类模型的损失函数，$y$ 是真实标签，$\theta$ 是可学习的参数，$D$
    表示数据样本 $x$ 的分布。
- en: It is worth noting that the proposed robust optimisation method is an universal
    framework under which other adversarial training strategies have natural interpretation.
    We describe it separately keeping in view its popularity in the literature.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，提出的鲁棒优化方法是一个通用框架，其他对抗训练策略在此框架下有自然的解释。我们单独描述它，以便考虑到其在文献中的受欢迎程度。
- en: 5.2\. Distillation
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 蒸馏
- en: 'Papernot et al. (Papernot et al., [2016c](#bib.bib108)) proposed distillation
    as another possible defense against adversarial examples. The principle is to
    use the softmax output (e.g., the class probabilities in classfication DNNs) of
    the original DNN to train the second DNN, which has the same structure with the
    original one. The softmax of the original DNN is also modified by introducing
    a temperature parameter $T$:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: Papernot 等人 (Papernot et al., [2016c](#bib.bib108)) 提出了蒸馏作为对抗样本的另一种可能防御方法。其原理是使用原始
    DNN 的 softmax 输出（例如，分类 DNN 中的类别概率）来训练具有与原始 DNN 相同结构的第二个 DNN。原始 DNN 的 softmax 也通过引入温度参数
    $T$ 进行修改：
- en: '| (35) |  | $\displaystyle q_{i}=\frac{\exp{(z_{i}/T)}}{\sum_{k}\exp{(z_{k}/T)}}$
    |  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $\displaystyle q_{i}=\frac{\exp{(z_{i}/T)}}{\sum_{k}\exp{(z_{k}/T)}}$
    |  |'
- en: 'where $z_{i}$ is input of softmax layer. $T$ controls the level of knowledge
    distillation. When $T=1$, Eq. ([35](#S5.E35 "In 5.2\. Distillation ‣ 5\. Defense
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")) turns back to the normal softmax function. If $T$ is large, $q_{i}$
    is close to a uniform distribution, when it is small, the function will output
    more extreme values. (Grosse et al., [2017](#bib.bib47)) adopts distillation defense
    for DNNs on discrete data and applied a high temperature $T$, as high-temperature
    softmax is proved to reduce the model sensitivity to small perturbations (Papernot
    et al., [2016c](#bib.bib108)). They trained the second DNN with the augmentation
    of original dataset and the softmax outputs from the original DNN. From the evaluations,
    they found adversarial training is the more effective than distillation. (I like
    if there is answers that explains why adversarial training is the more effective
    than distillation )'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $z_{i}$ 是 softmax 层的输入。$T$ 控制知识蒸馏的程度。当 $T=1$ 时，公式 ([35](#S5.E35 "In 5.2\.
    Distillation ‣ 5\. Defense ‣ Adversarial Attacks on Deep Learning Models in Natural
    Language Processing: A Survey")) 变回正常的 softmax 函数。如果 $T$ 很大，$q_{i}$ 接近均匀分布；如果
    $T$ 很小，函数将输出更极端的值。 (Grosse et al., [2017](#bib.bib47)) 采用蒸馏防御对抗 DNNs 处理离散数据，并应用了较高温度
    $T$，因为高温 softmax 被证明可以降低模型对小扰动的敏感性 (Papernot et al., [2016c](#bib.bib108))。他们使用原始数据集和原始
    DNN 的 softmax 输出训练了第二个 DNN。从评估中，他们发现对抗训练比蒸馏更有效。（我希望能有解释为什么对抗训练比蒸馏更有效的答案）'
- en: 6\. Discussions and Open Issues
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 讨论与开放问题
- en: Generating textual adversarial examples has relatively shorter history than
    generating image adversarial examples on DNNs because it is more challenging to
    make perturbation on discrete data, and meanwhile preserving the valid syntactic,
    grammar and semantics. We discuss some of the issues in this section and provide
    suggestions on future directions.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本对抗样本的历史比生成图像对抗样本的历史较短，因为对离散数据进行扰动更具挑战性，同时还要保持有效的语法、语法和语义。我们在本节中讨论一些问题，并对未来方向提供建议。
- en: 6.1\. Perceivability
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 可感知性
- en: Perturbations in image pixels are usually hard to be perceived, thus do not
    affect human judgment, but can only fool the deep neural networks. However, the
    perturbation on text is obvious, no matter the perturbation is flipping characters
    or changing words. Invalid words and syntactic errors can be easily identified
    by human and detected by the grammar check software, hence the perturbation is
    hard to attack a real NLP system. However, many research works generate such types
    of adversarial examples. It is acceptable only if the purpose is utilizing adversarial
    examples to robustify the attacked DNN models. In semantic-preserving perspective,
    changing a word in a sentence sometimes changes its semantics drastically and
    is easily detected by human beings. For NLP applications such as reading comprehension,
    and sentiment analysis, the adversarial examples need to be carefully designed
    in order not to change the should-be output. Otherwise, both correct output and
    perturbed output change, violating the purpose of generating adversarial examples.
    This is challenging and limited works reviewed considered this constraint. Therefore,
    for practical attack, we need to propose methods that make the perturbations not
    only unperceivable, but preserve correct grammar and semantics.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图像像素中的扰动通常难以被感知，因此不会影响人类判断，但会欺骗深度神经网络。然而，文本中的扰动是明显的，无论是翻转字符还是更改单词。无效的单词和语法错误很容易被人类识别，也能被语法检查软件检测出来，因此扰动难以攻击真实的NLP系统。然而，许多研究生成了这种类型的对抗样本。只有在目的是利用对抗样本来增强被攻击的DNN模型时，这才是可以接受的。从语义保持的角度来看，句子中改变一个单词有时会极大地改变其语义，且容易被人类检测到。对于如阅读理解和情感分析等NLP应用，对抗样本需要精心设计，以避免改变应有的输出。否则，正确的输出和扰动的输出都会变化，违反了生成对抗样本的目的。这是具有挑战性的，现有的研究工作对此限制考虑得较少。因此，对于实际攻击，我们需要提出的方法使扰动不仅不可感知，还要保持正确的语法和语义。
- en: 6.2\. Transferability
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 可转移性
- en: 'Transferability is a common property for adversarial examples. It reflects
    the generalization of the attack methods. Transferability means adversarial examples
    generated for one deep neural network on a dataset can also effectively attack
    another deep neural network (i.e., cross-model generalization) or dataset (i.e.,
    cross-data generalization). This property is more often exploited in black-box
    attacks as the details of the deep neural networks does not affect the attack
    method much. It is also shown that untargeted adversarial examples are much more
    transferable than targeted ones (Liu et al., [2017a](#bib.bib84)). Transferability
    can be organized into three levels in deep neural networks: (1) same architecture
    with different data; (2) different architectures with same application; (3) different
    architectures with different data (Yuan et al., [2017](#bib.bib155)). Although
    current works on textual attacks cover both three levels, the performance of the
    transferred attacks still decrease drastically compared to it on the original
    architecture and data, i.e., poor generalization ability. More efforts are expected
    to deliver better generalization ability.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 可转移性是对抗样本的一个常见特性。它反映了攻击方法的泛化能力。可转移性意味着在一个数据集上为一个深度神经网络生成的对抗样本也可以有效攻击另一个深度神经网络（即跨模型泛化）或数据集（即跨数据泛化）。这一特性在黑箱攻击中更常被利用，因为深度神经网络的细节不会对攻击方法产生太大影响。研究表明，非定向对抗样本的可转移性远高于定向对抗样本（Liu
    et al., [2017a](#bib.bib84)）。可转移性可以在深度神经网络中分为三个层次：（1）相同架构但数据不同；（2）不同架构但应用相同；（3）不同架构和不同数据（Yuan
    et al., [2017](#bib.bib155)）。尽管目前的文本攻击工作涵盖了这三个层次，但与原始架构和数据上的攻击相比，转移攻击的性能仍然显著下降，即泛化能力较差。期望更多的努力能够带来更好的泛化能力。
- en: 6.3\. Automation
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 自动化
- en: Some reviewed works are able to generate adversarial examples automatically,
    while others cannot. In white-box attacks, leveraging the loss function of the
    DNN can identify the most affected points (e.g., character, word) in a text automatically.
    Then the attacks are performed on these points by automatically modifying the
    corresponding texts. In black-box attacks, some attacks, e.g. substitution train
    substitute DNNs and apply white-box attack strategies on the substitution. This
    can be achieved automatically. However, most of the other works craft the adversarial
    examples in a manual manner. For example, (Jia and Liang, [2017](#bib.bib56))
    concatenated manually-chosen meaningless paragraphs to fool the reading comprehension
    systems, in order to discover the vulnerability of the victim DNNs. Many research
    works followed their way, not aiming on practical attacks, but more on examining
    robustness of the target network. These manaul works are time-consuming and impractical.
    We believe that more efforts in this line could pass through this barrier in future.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 一些已审查的研究能够自动生成对抗样本，而其他则不能。在白盒攻击中，利用DNN的损失函数可以自动识别文本中最受影响的点（例如，字符、单词）。然后，通过自动修改相应文本来对这些点进行攻击。在黑盒攻击中，一些攻击，如替代攻击，通过训练替代DNN并对替代进行白盒攻击策略，这可以自动实现。然而，大多数其他工作手工制作对抗样本。例如，（Jia和Liang，[2017](#bib.bib56)）手动连接选择的无意义段落以欺骗阅读理解系统，以发现受害DNN的脆弱性。许多研究工作沿袭了他们的方法，目标并非实际攻击，而是更多地考察目标网络的鲁棒性。这些手工操作既费时又不切实际。我们相信，未来更多的努力可以突破这一障碍。
- en: 6.4\. New Architectures
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 新架构
- en: 'Although most of the common textual DNNs have gained attention from the perspective
    of adversarial attack (Section [2.2](#S2.SS2 "2.2\. Deep Learning in NLP ‣ 2\.
    Overview of Adversarial Attacks and Deep Learning Techniques in Natural Language
    Processing ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")), many DNNs haven’t been attacked so far. For example, the generative
    neural models: Generative Adversarial Networks (GANs) and Variational Auto-Encoders
    (VAEs). In NLP, they are used to generate texts. Deep generative models requires
    more sophisticated skill for model training. This would explain that these techniques
    have been mainly overlooked by adversarial attack so far. Future works may consider
    about generating adversarial examples for these generative DNNs. Another example
    is differentiable neural computer (DNC). Only one work attacked DNC so far (Chan
    et al., [2018](#bib.bib22)). Attention mechanism is somehow become a standard
    component in most of the sequential models. But there is no work examined the
    mechanism itself. Instead, works are either attack the overall system that contain
    attentions, or leverage attention scores to identify the word for perturbation
    (Blohm et al., [2018](#bib.bib17)).'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管大多数常见的文本深度神经网络（DNNs）已从对抗攻击的角度获得关注（见第[2.2节](#S2.SS2 "2.2\. Deep Learning in
    NLP ‣ 2\. Overview of Adversarial Attacks and Deep Learning Techniques in Natural
    Language Processing ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey")），但许多DNNs迄今为止尚未受到攻击。例如，生成式神经模型：生成对抗网络（GANs）和变分自编码器（VAEs）。在自然语言处理（NLP）中，它们用于生成文本。深度生成模型需要更复杂的模型训练技能。这就解释了这些技术至今主要被对抗攻击忽视的原因。未来的研究可能会考虑为这些生成式DNNs生成对抗样本。另一个例子是可微分神经计算机（DNC）。迄今为止，只有一项研究攻击了DNC（Chan等，[2018](#bib.bib22)）。注意力机制在大多数序列模型中已成为某种标准组件。但目前没有研究检查机制本身。相反，研究要么攻击包含注意力的整体系统，要么利用注意力分数识别需要扰动的单词（Blohm等，[2018](#bib.bib17)）。'
- en: 6.5\. Iterative vs One-off
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 迭代式与一次性
- en: Iterative attacks iteratively search and update the perturbations based on the
    gradient of the output of the attacked DNN model. Thus it shows high quality and
    effectiveness, that is the perturbations can be small enough and hard to defense.
    However, these methods usually require long time to find the proper perturbations,
    rendering an obstacle for attacking in real-time. Therefore, one-off attacks are
    proposed to tackle this problem. FGSM (Goodfellow et al., [2015](#bib.bib43))
    is one example of one-off attack. Natually, one-off attack is much faster than
    iterative attack, but is less effective and easier to be defensed (Yuan et al.,
    [2018](#bib.bib154)). When designing attack methods on a real application, attackers
    need to carefully consider the trade off between efficiency and effectiveness
    of the attack.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代攻击通过基于攻击的DNN模型输出的梯度迭代地搜索和更新扰动。因此，它显示出高质量和有效性，即扰动可以足够小且难以防御。然而，这些方法通常需要较长时间来找到合适的扰动，从而对实时攻击构成障碍。因此，提出了一次性攻击来解决这个问题。FGSM（Goodfellow等，[2015](#bib.bib43)）是一次性攻击的一个例子。自然地，一次性攻击比迭代攻击快得多，但效果较差且更容易防御（Yuan等，[2018](#bib.bib154)）。在设计实际应用中的攻击方法时，攻击者需要仔细考虑攻击效率和效果之间的权衡。
- en: 7\. Conclusion
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 结论
- en: This article presents the first comprehensive survey in the direction of generating
    textual adversarial examples on deep neural networks. We review recent research
    efforts and develop classification schemes to organize existing literature. Additionally
    we summarize and analyze them from different aspects. We attempt to provide a
    good reference for researchers to gain insight of the challenges, methods and
    issues in this research topic and shed lights on future directions. We hope more
    robust deep neural models are proposed based on the knowledge of the adversarial
    attacks.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了生成文本对抗样本的首个全面调查。我们回顾了近期的研究工作，并制定了分类方案来组织现有文献。此外，我们从不同方面对这些研究进行了总结和分析。我们试图为研究人员提供一个良好的参考，以便深入了解本研究主题中的挑战、方法和问题，并为未来的研究方向提供启示。我们希望基于对对抗攻击的认识，提出更为鲁棒的深度神经模型。
- en: References
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Abdulla et al. (2013) Nawaf A Abdulla, Nizar A Ahmed, Mohammed A Shehab, and
    Mahmoud Al-Ayyoub. 2013. Arabic sentiment analysis: Lexicon-based and corpus-based.
    In *Proc. of the 2013 IEEE Jordan Conference on Applied Electrical Engineering
    and Computing Technologies (AEECT 2013)*. IEEE, 1–6.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdulla等人（2013）Nawaf A Abdulla、Nizar A Ahmed、Mohammed A Shehab和Mahmoud Al-Ayyoub。2013年。《阿拉伯语情感分析：基于词典和语料库》。发表于*2013年IEEE约旦应用电气工程与计算技术会议（AEECT
    2013）论文集*。IEEE，第1–6页。
- en: 'Akhtar and Mian (2018) Naveed Akhtar and Ajmal S. Mian. 2018. Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey. *IEEE Access* 6 (2018),
    14410–14430.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akhtar和Mian（2018）Naveed Akhtar和Ajmal S. Mian。2018年。《计算机视觉中深度学习的对抗攻击威胁：综述》。*IEEE
    Access* 6（2018），第14410–14430页。
- en: Al-Dujaili et al. (2018a) Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and
    Una-May O’Reilly. 2018a. Adversarial Deep Learning for Robust Detection of Binary
    Encoded Malware. In *Proc. of the 2018 IEEE Security and Privacy Workshops (SPW
    2018)*. Francisco, CA, USA, 76–82.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Dujaili等人（2018a）Abdullah Al-Dujaili、Alex Huang、Erik Hemberg和Una-May O’Reilly。2018a年。《对抗深度学习用于二进制编码恶意软件的鲁棒检测》。发表于*2018年IEEE安全与隐私研讨会（SPW
    2018）论文集*。美国加利福尼亚州旧金山，第76–82页。
- en: Al-Dujaili et al. (2018b) Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and
    Una-May O’Reilly. 2018b. Adversarial Deep Learning for Robust Detection of Binary
    Encoded Malware. In *Proc. of the 2018 IEEE Security and Privacy Workshops (SP
    Workshops 2018)*. San Francisco, CA, USA, 76–82.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Dujaili等人（2018b）Abdullah Al-Dujaili、Alex Huang、Erik Hemberg和Una-May O’Reilly。2018b年。《对抗深度学习用于二进制编码恶意软件的鲁棒检测》。发表于*2018年IEEE安全与隐私研讨会（SP
    Workshops 2018）论文集*。美国加利福尼亚州旧金山，第76–82页。
- en: Alzantot et al. (2018) Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang
    Ho, Mani B. Srivastava, and Kai-Wei Chang. 2018. Generating Natural Language Adversarial
    Examples. In *Proc. of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2018)*. Brussels, Belgium, 2890–2896.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alzantot等人（2018）Moustafa Alzantot、Yash Sharma、Ahmed Elgohary、Bo-Jhang Ho、Mani
    B. Srivastava和Kai-Wei Chang。2018年。《生成自然语言对抗样本》。发表于*2018年自然语言处理经验方法会议（EMNLP 2018）论文集*。比利时布鲁塞尔，第2890–2896页。
- en: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual
    Question Answering. In *Proc. of the 2015 IEEE International Conference on Computer
    Vision (ICCV 2015)*. Santiago, Chile, 2425–2433.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Antol 等人 (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell,
    Dhruv Batra, C. Lawrence Zitnick, 和 Devi Parikh. 2015. VQA: 视觉问答。发表于 *2015年IEEE计算机视觉国际会议
    (ICCV 2015) 论文集*。智利圣地亚哥，2425–2433。'
- en: 'Arp et al. (2014) Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon,
    and Konrad Rieck. 2014. DREBIN: Effective and Explainable Detection of Android
    Malware in Your Pocket. In *Proc. of the 21st Annual Network and Distributed System
    Security Symposium (NDSS 2014)*. San Diego, California, USA.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arp 等人 (2014) Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon,
    和 Konrad Rieck. 2014. DREBIN: 在口袋中有效且可解释的Android恶意软件检测。发表于 *第21届年度网络和分布式系统安全研讨会
    (NDSS 2014) 论文集*。美国加州圣地亚哥。'
- en: Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
    Neural Machine Translation by Jointly Learning to Align and Translate. *CoRR*
    abs/1409.0473 (2014). arXiv:1409.0473 [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等人 (2014) Dzmitry Bahdanau, Kyunghyun Cho, 和 Yoshua Bengio. 2014. 通过联合学习对齐和翻译的神经机器翻译。*CoRR*
    abs/1409.0473 (2014)。arXiv:1409.0473 [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
    Neural Machine Translation by Jointly Learning to Align and Translate. (2015).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等人 (2015) Dzmitry Bahdanau, Kyunghyun Cho, 和 Yoshua Bengio. 2015. 通过联合学习对齐和翻译的神经机器翻译。（2015）。
- en: Barreno et al. (2010) Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D.
    Tygar. 2010. The security of machine learning. *Machine Learning* 81, 2 (2010),
    121–148.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barreno 等人 (2010) Marco Barreno, Blaine Nelson, Anthony D. Joseph, 和 J. D. Tygar.
    2010. 机器学习的安全性。*机器学习* 81, 2 (2010), 121–148。
- en: Bekoulis et al. (2018a) Giannis Bekoulis, Johannes Deleu, Thomas Demeester,
    and Chris Develder. 2018a. Adversarial training for multi-context joint entity
    and relation extraction. In *Proc. of the 2018 Conference on Empirical Methods
    in Natural Language Processing (EMNLP 2018)*. Brussels, Belgium, 2830–2836.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bekoulis 等人 (2018a) Giannis Bekoulis, Johannes Deleu, Thomas Demeester, 和 Chris
    Develder. 2018a. 多上下文联合实体和关系抽取的对抗训练。发表于 *2018年自然语言处理实证方法会议 (EMNLP 2018) 论文集*。比利时布鲁塞尔，2830–2836。
- en: Bekoulis et al. (2018b) Giannis Bekoulis, Johannes Deleu, Thomas Demeester,
    and Chris Develder. 2018b. An attentive neural architecture for joint segmentation
    and parsing and its application to real estate ads. *Expert Systems with Applications*
    102 (2018), 100–112.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bekoulis 等人 (2018b) Giannis Bekoulis, Johannes Deleu, Thomas Demeester, 和 Chris
    Develder. 2018b. 一种用于联合分割和解析的注意力神经架构及其在房地产广告中的应用。*应用专家系统* 102 (2018), 100–112。
- en: Belinkov and Bisk (2018) Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
    and Natural Noise Both Break Neural Machine Translation. *arXiv preprint arXiv:1711.02173\.
    ICLR* (2018).
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belinkov 和 Bisk (2018) Yonatan Belinkov 和 Yonatan Bisk. 2018. 合成噪声和自然噪声均会破坏神经机器翻译。*arXiv
    预印本 arXiv:1711.02173\. ICLR* (2018)。
- en: Bengio et al. (2003) Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian
    Janvin. 2003. A Neural Probabilistic Language Model. *Journal of Machine Learning
    Research* 3 (2003), 1137–1155.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等人 (2003) Yoshua Bengio, Réjean Ducharme, Pascal Vincent, 和 Christian
    Janvin. 2003. 神经概率语言模型。*机器学习研究期刊* 3 (2003), 1137–1155。
- en: 'Biggio and Roli (2018) Battista Biggio and Fabio Roli. 2018. Wild patterns:
    Ten years after the rise of adversarial machine learning. *Pattern Recognition*
    84 (2018), 317–331.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biggio 和 Roli (2018) Battista Biggio 和 Fabio Roli. 2018. 野生模式：对抗性机器学习兴起十年后的回顾。*模式识别*
    84 (2018), 317–331。
- en: 'Blohm et al. (2018) Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang Yu,
    and Ngoc Thang Vu. 2018. Comparing Attention-Based Convolutional and Recurrent
    Neural Networks: Success and Limitations in Machine Reading Comprehension. In
    *Proc. of the 22nd Conference on Computational Natural Language Learning (CoNLL
    2018)*. Brussels, Belgium, 108–118.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blohm 等人 (2018) Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang Yu, 和 Ngoc
    Thang Vu. 2018. 比较基于注意力的卷积和递归神经网络：机器阅读理解中的成功与局限性。发表于 *第22届计算自然语言学习会议 (CoNLL 2018)
    论文集*。比利时布鲁塞尔，108–118。
- en: Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and
    Christopher D. Manning. 2015. A large annotated corpus for learning natural language
    inference. In *Proc. of the 2015 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2015)*. Lisbon, Portugal, 632–642.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowman 等人 (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, 和 Christopher
    D. Manning. 2015. 用于学习自然语言推理的大型注释语料库。发表于 *2015年自然语言处理实证方法会议 (EMNLP 2015) 论文集*。葡萄牙里斯本，632–642。
- en: Bowman et al. (2016) Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M.
    Dai, Rafal Józefowicz, and Samy Bengio. 2016. Generating Sentences from a Continuous
    Space. In *Proc. of the 20th SIGNLL Conference on Computational Natural Language
    Learning (CoNLL 2016)*. Berlin, Germany, 10–21.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowman et al. (2016) Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M.
    Dai, Rafal Józefowicz, 和 Samy Bengio. 2016. 从连续空间生成句子。见于 *第20届SIGNLL计算自然语言学习会议（CoNLL
    2016）会议录*。德国柏林，第10–21页。
- en: 'Carlini and Wagner ([n. d.]) Nicholas Carlini and David A. Wagner. [n. d.].
    Audio Adversarial Examples: Targeted Attacks on Speech-to-Text.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 和 Wagner ([n. d.]) Nicholas Carlini 和 David A. Wagner. [n. d.]. 音频对抗样本：对语音识别的定向攻击。
- en: Carlini and Wagner (2017) Nicholas Carlini and David A. Wagner. 2017. Towards
    Evaluating the Robustness of Neural Networks. In *Proc. of the 2017 IEEE Symposium
    on Security and Privacy (SP 2017)*. San Jose, CA, USA, 39–57.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 和 Wagner (2017) Nicholas Carlini 和 David A. Wagner. 2017. 评估神经网络鲁棒性的研究。见于
    *2017年IEEE安全与隐私研讨会（SP 2017）会议录*。美国加州圣荷西，第39–57页。
- en: Chan et al. (2018) Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu,
    and Yew Soon Ong. 2018. Metamorphic Relation Based Adversarial Attacks on Differentiable
    Neural Computer. *CoRR* abs/1809.02444 (2018). arXiv:1809.02444 [http://arxiv.org/abs/1809.02444](http://arxiv.org/abs/1809.02444)
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan et al. (2018) Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu,
    和 Yew Soon Ong. 2018. 基于变形关系的对抗攻击在可微神经计算机上的应用。*CoRR* abs/1809.02444 (2018). arXiv:1809.02444
    [http://arxiv.org/abs/1809.02444](http://arxiv.org/abs/1809.02444)
- en: 'Chen et al. (2018) Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui
    Hsieh. 2018. Attacking Visual Language Grounding with Adversarial Examples: A
    Case Study on Neural Image Captioning. In *Proceedings of ACL 2018*.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2018) Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, 和 Cho-Jui
    Hsieh. 2018. 使用对抗样本攻击视觉语言基础：神经图像描述的案例研究。见于 *ACL 2018会议录*。
- en: Chen et al. (2017) Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang,
    and Diana Inkpen. 2017. Enhanced LSTM for Natural Language Inference. In *Proc.
    of the 55th Annual Meeting of the Association for Computational Linguistics (ACL
    2017)*. Vancouver, BC, Canada, 1657–1668.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2017) Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang,
    和 Diana Inkpen. 2017. 为自然语言推理增强的 LSTM。见于 *第55届计算语言学协会年会（ACL 2017）会议录*。加拿大温哥华，第1657–1668页。
- en: 'Cheng et al. (2018) Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and
    Cho-Jui Hsieh. 2018. Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence
    Models with Adversarial Examples. *arXiv preprint arXiv:1803.01128* (2018).'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2018) Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, 和 Cho-Jui
    Hsieh. 2018. Seq2Sick：使用对抗样本评估序列到序列模型的鲁棒性。*arXiv 预印本 arXiv:1803.01128* (2018).
- en: Cho et al. (2014) Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
    Representations using RNN Encoder-Decoder for Statistical Machine Translation.
    In *Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2014)*. Doha, Qatar, 1724–1734.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho et al. (2014) Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, 和 Yoshua Bengio. 2014. 使用 RNN 编码器-解码器学习短语表示，用于统计机器翻译。见于
    *2014年自然语言处理经验方法会议（EMNLP 2014）会议录*。卡塔尔多哈，第1724–1734页。
- en: Costa-Jussà and Fonollosa (2016) Marta R. Costa-Jussà and José A. R. Fonollosa.
    2016. Character-based Neural Machine Translation. In *Proc. of the 54th Annual
    Meeting of the Association for Computational Linguistics (ACL 2016)*. Berlin,
    Germany.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa-Jussà 和 Fonollosa (2016) Marta R. Costa-Jussà 和 José A. R. Fonollosa.
    2016. 基于字符的神经机器翻译。见于 *第54届计算语言学协会年会（ACL 2016）会议录*。德国柏林。
- en: Dahl et al. (2013) George E. Dahl, Jack W. Stokes, Li Deng, and Dong Yu. 2013.
    Large-scale malware classification using random projections and neural networks.
    In *Proc. of the 38th International Conference on Acoustics, Speech and Signal
    Processing (ICASSP 2013)*. Vancouver, BC, Canada, 3422–3426.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dahl et al. (2013) George E. Dahl, Jack W. Stokes, Li Deng, 和 Dong Yu. 2013.
    使用随机投影和神经网络的大规模恶意软件分类。见于 *第38届国际声学、语音和信号处理会议（ICASSP 2013）会议录*。加拿大温哥华，第3422–3426页。
- en: Doddington et al. (2004) George Doddington, Alexis Mitchell, Mark Przybocki,
    Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content
    Extraction (ACE) Program -Tasks, Data, and Evaluation. In *Proc. of the Fourth
    International Conference on Language Resources and Evaluation (LREC’04)*. Lisbon,
    Portugal.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doddington et al. (2004) George Doddington, Alexis Mitchell, Mark Przybocki,
    Lance Ramshaw, Stephanie Strassel, 和 Ralph Weischedel. 2004. 自动内容提取（ACE）计划 - 任务、数据和评估。见于
    *第四届国际语言资源与评估会议（LREC’04）会议录*。葡萄牙里斯本。
- en: Dzendzik et al. (2017) Daria Dzendzik, Carl Vogel, and Qun Liu. 2017. Who framed
    roger rabbit? multiple choice questions answering about movie plot. (2017).
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dzendzik 等（2017）Daria Dzendzik, Carl Vogel 和 Qun Liu。2017年。谁陷害了罗杰·拉比特？关于电影情节的多项选择问题。（2017年）。
- en: Ebrahimi et al. ([n. d.]) Javid Ebrahimi, Daniel Lowd, and Dejing Dou. [n. d.].
    On Adversarial Examples for Character-Level Neural Machine Translation. In *Proc.
    of the 27th International Conference on Computational Linguistics (COLING 2018)*.
    Santa Fe, New Mexico, USA, 653–663.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ebrahimi 等（[n. d.]）Javid Ebrahimi, Daniel Lowd 和 Dejing Dou。[n. d.]。关于字符级神经机器翻译的对抗样本。在
    *第 27 届计算语言学国际会议（COLING 2018）论文集* 中。美国新墨西哥州圣菲，653–663页。
- en: 'Ebrahimi et al. (2018) Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou.
    2018. HotFlip: White-Box Adversarial Examples for Text Classification. In *Proc.
    of the 56th Annual Meeting of the Association for Computational Linguistics (ACL
    2018)*. Melbourne, Australia, 31–36.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ebrahimi 等（2018）Javid Ebrahimi, Anyi Rao, Daniel Lowd 和 Dejing Dou。2018年。HotFlip：文本分类的白盒对抗样本。在
    *第 56 屆计算语言学协会年会（ACL 2018）论文集* 中。澳大利亚墨尔本，31–36页。
- en: Elsayed et al. (2018) Gamaleldin F. Elsayed, Ian J. Goodfellow, and Jascha Sohl-Dickstein.
    2018. Adversarial Reprogramming of Neural Networks. *CoRR* abs/1806.11146 (2018).
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsayed 等（2018）Gamaleldin F. Elsayed, Ian J. Goodfellow 和 Jascha Sohl-Dickstein。2018年。神经网络的对抗重编程。*CoRR*
    abs/1806.11146（2018年）。
- en: 'Faghri et al. (2017) Fartash Faghri, David J. Fleet, Ryan Kiros, and Sanja
    Fidler. 2017. VSE++: Improved Visual-Semantic Embeddings. *CoRR* abs/1707.05612
    (2017).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faghri 等（2017）Fartash Faghri, David J. Fleet, Ryan Kiros 和 Sanja Fidler。2017年。VSE++：改进的视觉-语义嵌入。*CoRR*
    abs/1707.05612（2017年）。
- en: Fukui et al. (2016) Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,
    Trevor Darrell, and Marcus Rohrbach. 2016. Multimodal Compact Bilinear Pooling
    for Visual Question Answering and Visual Grounding. In *Proc. of the 2016 Conference
    on Empirical Methods in Natural Language Processing (EMNLP 2016)*. Austin, Texas,
    USA, 457–468.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fukui 等（2016）Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor
    Darrell 和 Marcus Rohrbach。2016年。用于视觉问答和视觉定位的多模态紧凑双线性池化。在 *2016 年自然语言处理经验方法会议（EMNLP
    2016）论文集* 中。美国德克萨斯州奥斯汀，457–468页。
- en: Gao et al. (2018) Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018.
    Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers.
    *arXiv preprint arXiv:1801.04354* (2018).
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2018）Ji Gao, Jack Lanchantin, Mary Lou Soffa 和 Yanjun Qi。2018年。黑箱生成对抗文本序列以规避深度学习分类器。*arXiv
    预印本 arXiv:1801.04354*（2018年）。
- en: Gilmer et al. (2018) Justin Gilmer, Ryan P. Adams, Ian J. Goodfellow, David
    Andersen, and George E. Dahl. 2018. Motivating the Rules of the Game for Adversarial
    Example Research. *CoRR* abs/1807.06732 (2018). arXiv:1807.06732 [http://arxiv.org/abs/1807.06732](http://arxiv.org/abs/1807.06732)
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilmer 等（2018）Justin Gilmer, Ryan P. Adams, Ian J. Goodfellow, David Andersen
    和 George E. Dahl。2018年。激励对抗样本研究的规则。*CoRR* abs/1807.06732（2018年）。arXiv:1807.06732
    [http://arxiv.org/abs/1807.06732](http://arxiv.org/abs/1807.06732)
- en: Goldberg (2017) Yoav Goldberg. 2017. *Neural Network Methods for Natural Language
    Processing*. Morgan & Claypool Publishers. [https://doi.org/10.2200/S00762ED1V01Y201703HLT037](https://doi.org/10.2200/S00762ED1V01Y201703HLT037)
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldberg（2017）Yoav Goldberg。2017年。*神经网络方法在自然语言处理中的应用*。Morgan & Claypool Publishers。[https://doi.org/10.2200/S00762ED1V01Y201703HLT037](https://doi.org/10.2200/S00762ED1V01Y201703HLT037)
- en: Goller and Kuchler (1996) Christoph Goller and Andreas Kuchler. 1996. Learning
    task-dependent distributed representations by backpropagation through structure.
    *Neural Networks* 1 (1996), 347–352.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goller 和 Kuchler（1996）Christoph Goller 和 Andreas Kuchler。1996年。通过结构反向传播学习任务相关的分布式表示。*神经网络*
    1（1996年），347–352页。
- en: Gong et al. (2018) Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, and Wei-Shinn
    Ku. 2018. Adversarial Texts with Gradient Methods. *arXiv preprint arXiv:1801.07175*
    (2018).
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong 等（2018）Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song 和 Wei-Shinn Ku。2018年。对抗文本与梯度方法。*arXiv
    预印本 arXiv:1801.07175*（2018年）。
- en: Goodfellow et al. (2016) Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
    2016. *Deep learning*. Vol. 1.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2016）Ian Goodfellow, Yoshua Bengio 和 Aaron Courville。2016年。*深度学习*。第
    1 卷。
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    2014. Generative Adversarial Nets. In *Proc. of the Annual Conference on Neural
    Information Processing Systems 2014 (NIPS 2014)*. Montreal, Quebec, Canada, 2672–2680.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2014）Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
    David Warde-Farley, Sherjil Ozair, Aaron C. Courville 和 Yoshua Bengio。2014年。生成对抗网络。在
    *2014 年神经信息处理系统年会（NIPS 2014）论文集* 中。加拿大蒙特利尔，2672–2680页。
- en: Goodfellow et al. (2015) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    2015. Explaining and Harnessing Adversarial Examples. In *Proc. of the 3rd International
    Conference on Learning Representations (ICLR 2015)*.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2015) Ian J Goodfellow, Jonathon Shlens, 和 Christian Szegedy.
    2015. 解释与利用对抗样本。发表于 *第三届国际学习表征会议 (ICLR 2015)*。
- en: Grave et al. (2017) Edouard Grave, Tomas Mikolov, Armand Joulin, and Piotr Bojanowski.
    2017. Bag of Tricks for Efficient Text Classification. In *Proc. of the 15th Conference
    of the European Chapter of the Association for Computational Linguistics (EACL
    2017)*. Valencia, Spain, 427–431.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grave et al. (2017) Edouard Grave, Tomas Mikolov, Armand Joulin, 和 Piotr Bojanowski.
    2017. 高效文本分类的技巧集。发表于 *第15届欧洲计算语言学协会年会 (EACL 2017)*. 西班牙瓦伦西亚，427–431。
- en: Graves et al. (2013) Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton.
    2013. Speech recognition with deep recurrent neural networks. In *Proc. of IEEE
    2013 International Conference on Acoustics, Speech and Signal Processing (ICASSP
    2013)*. Vancouver, BC, Canada, 6645–6649.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves et al. (2013) Alex Graves, Abdel-rahman Mohamed, 和 Geoffrey E. Hinton.
    2013. 使用深度递归神经网络的语音识别。发表于 *IEEE 2013 年声学、语音与信号处理国际会议 (ICASSP 2013)*. 加拿大温哥华，6645–6649。
- en: Grosse et al. (2016) Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael
    Backes, and Patrick McDaniel. 2016. Adversarial perturbations against deep neural
    networks for malware classification. *arXiv preprint arXiv:1606.04435* (2016).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grosse et al. (2016) Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael
    Backes, 和 Patrick McDaniel. 2016. 针对深度神经网络的恶意软件分类的对抗性扰动。*arXiv 预印本 arXiv:1606.04435*
    (2016)。
- en: Grosse et al. (2017) Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael
    Backes, and Patrick D. McDaniel. 2017. Adversarial Examples for Malware Detection.
    In *Proc. of the 22nd European Symposium on Research in Computer Security (ESORICS
    2017)*. Oslo, Norway, 62–79.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grosse et al. (2017) Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael
    Backes, 和 Patrick D. McDaniel. 2017. 用于恶意软件检测的对抗样本。发表于 *第22届欧洲计算机安全研究研讨会 (ESORICS
    2017)*. 挪威奥斯陆，62–79。
- en: Gurulingappa et al. (2012) Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts,
    Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. 2012. Development of a
    benchmark corpus to support the automatic extraction of drug-related adverse effects
    from medical case reports. *Journal of Biomedical Informatics* 45, 5 (2012), 885–892.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gurulingappa et al. (2012) Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts,
    Juliane Fluck, Martin Hofmann-Apitius, 和 Luca Toldo. 2012. 开发一个基准语料库以支持从医疗病例报告中自动提取药物相关的不良反应。*生物医学信息学杂志*
    45, 5 (2012), 885–892。
- en: 'Hannun et al. (2014) Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro,
    Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam
    Coates, and Andrew Y. Ng. 2014. Deep Speech: Scaling up end-to-end speech recognition.
    *CoRR* abs/1412.5567 (2014).'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hannun et al. (2014) Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro,
    Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam
    Coates, 和 Andrew Y. Ng. 2014. 深度语音：扩大端到端语音识别。*CoRR* abs/1412.5567 (2014)。
- en: He et al. (2017) He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. 2017.
    Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph
    Embeddings. In *Proc. of the 55th Annual Meeting of the Association for Computational
    Linguistics (ACL 2017)*. Vancouver, Canada, 1766–1776.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2017) He He, Anusha Balakrishnan, Mihail Eric, 和 Percy Liang. 2017.
    使用动态知识图谱嵌入学习对称协作对话代理。发表于 *第55届计算语言学协会年会 (ACL 2017)*. 加拿大温哥华，1766–1776。
- en: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.
    Distilling the Knowledge in a Neural Network. *CoRR* abs/1503.02531 (2015). arXiv:1503.02531
    [http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531)
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, 和 Jeffrey Dean. 2015.
    提炼神经网络中的知识。*CoRR* abs/1503.02531 (2015). arXiv:1503.02531 [http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531)
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long Short-Term Memory. *Neural Computation* 9, 8 (1997), 1735–1780.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 1997.
    长短期记忆。*Neural Computation* 9, 8 (1997), 1735–1780。
- en: 'Hu et al. (2017) Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell,
    and Kate Saenko. 2017. Learning to Reason: End-to-End Module Networks for Visual
    Question Answering. In *Proc. of IEEE International Conference on Computer Vision
    (ICCV 2017)*. Venice, Italy, 804–813.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2017) Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell,
    和 Kate Saenko. 2017. 学习推理：用于视觉问答的端到端模块网络。发表于 *IEEE 国际计算机视觉大会 (ICCV 2017)*. 意大利威尼斯，804–813。
- en: Hu and Tan (2017) Weiwei Hu and Ying Tan. 2017. Black-Box Attacks against RNN
    based Malware Detection Algorithms. *arXiv preprint arXiv:1705.08131* (2017).
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡伟伟和谭颖（2017）胡伟伟和谭颖。2017年。《基于RNN的恶意软件检测算法的黑箱攻击》。*arXiv预印本 arXiv:1705.08131*（2017年）。
- en: 'Iyyer et al. (2018) Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer.
    2018. Adversarial Example Generation with Syntactically Controlled Paraphrase
    Networks. In *Proc. of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)*.
    New Orleans, Louisiana, USA, 1875–1885.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyyer等（2018）Mohit Iyyer、John Wieting、Kevin Gimpel 和 Luke Zettlemoyer。2018年。《具有句法控制的同义句生成对抗示例》。在*2018年北美计算语言学协会：人类语言技术会议（NAACL-HLT）论文集*中。美国路易斯安那州新奥尔良市，1875–1885页。
- en: Jia and Liang (2017) Robin Jia and Percy Liang. 2017. Adversarial Examples for
    Evaluating Reading Comprehension Systems. In *Proc. of the 2017 Conference on
    Empirical Methods in Natural Language Processing (EMNLP 2017)*. Copenhagen, Denmark,
    2021–2031.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia和Liang（2017）Robin Jia 和 Percy Liang。2017年。《用于评估阅读理解系统的对抗样本》。在*2017年自然语言处理实证方法会议（EMNLP
    2017）论文集*中。丹麦哥本哈根，2021–2031页。
- en: Johnson and Zhang (2015) Rie Johnson and Tong Zhang. 2015. Semi-supervised Convolutional
    Neural Networks for Text Categorization via Region Embedding. In *Proc. of the
    Annual Conference on Neural Information Processing Systems 2015 (NIPS 2015)*.
    Montreal, Quebec, Canada, 919–927.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson和Zhang（2015）Rie Johnson 和 Tong Zhang。2015年。《通过区域嵌入的半监督卷积神经网络进行文本分类》。在*2015年神经信息处理系统年会（NIPS
    2015）论文集*中。加拿大魁北克省蒙特利尔，919–927页。
- en: 'Kang et al. (2018) Dongyeop Kang, Tushar Khot, Ashish Sabharwal, , and Eduard
    Hovy. 2018. AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided
    Examples. In *Proceedings of ACL 2018*.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang等（2018）Dongyeop Kang、Tushar Khot、Ashish Sabharwal 和 Eduard Hovy。2018年。《AdvEntuRe：具有知识引导示例的文本蕴含对抗训练》。在*ACL
    2018会议论文集*中。
- en: Karatzas et al. (2013) Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,
    Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernández
    Mota, Jon Almazán, and Lluís-Pere de las Heras. 2013. ICDAR 2013 Robust Reading
    Competition. In *Proc. of the 12th International Conference on Document Analysis
    and Recognition (ICDAR 2013)*. Washington, DC, USA, 1484–1493.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karatzas等（2013）Dimosthenis Karatzas、Faisal Shafait、Seiichi Uchida、Masakazu Iwamura、Lluis
    Gomez i Bigorda、Sergi Robles Mestre、Joan Mas、David Fernández Mota、Jon Almazán
    和 Lluís-Pere de las Heras。2013年。《ICDAR 2013鲁棒阅读竞赛》。在*第12届国际文档分析与识别会议（ICDAR 2013）论文集*中。美国华盛顿特区，1484–1493页。
- en: 'Khot et al. (2018) Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTaiL:
    A Textual Entailment Dataset from Science Question Answering. In *Proc. of the
    32nd AAAI Conference on Artificial Intelligence (AAAI 2018)*. New Orleans, Louisiana,
    USA, 5189–5197.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khot等（2018）Tushar Khot、Ashish Sabharwal 和 Peter Clark。2018年。《SciTaiL：来自科学问答的文本蕴含数据集》。在*第32届AAAI人工智能会议（AAAI
    2018）论文集*中。美国路易斯安那州新奥尔良市，5189–5197页。
- en: Kim (2014) Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
    In *Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2014)*. Doha, Qatar, 1746–1751.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim（2014）Yoon Kim。2014年。《用于句子分类的卷积神经网络》。在*2014年自然语言处理实证方法会议（EMNLP 2014）论文集*中。卡塔尔多哈，1746–1751页。
- en: Kim et al. (2016) Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush.
    2016. Character-Aware Neural Language Models. In *Proc. of the 13th AAAI Conference
    on Artificial Intelligence (AAAI 2016)*. Phoenix, Arizona, USA, 2741–2749.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等（2016）Yoon Kim、Yacine Jernite、David Sontag 和 Alexander M. Rush。2016年。《字符感知神经语言模型》。在*第13届AAAI人工智能会议（AAAI
    2016）论文集*中。美国亚利桑那州凤凰城，2741–2749页。
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. dam: A Method for
    Stochastic Optimization. In *Proc. of the 3rd International Conference on Learning
    Representations (ICLR 2015)*. San Diego, CA, USA.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma和Ba（2015）Diederik P. Kingma 和 Jimmy Ba。2015年。《dam：一种随机优化方法》。在*第三届国际学习表征会议（ICLR
    2015）论文集*中。美国加州圣地亚哥。
- en: Kingma and Welling (2014) Diederik P. Kingma and Max Welling. 2014. Auto-Encoding
    Variational Bayes. In *Proc. of the 2014 International Conference on Learning
    Representations (ICLR 2014)*.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma和Welling（2014）Diederik P. Kingma 和 Max Welling。2014年。《自编码变分贝叶斯》。在*2014年国际学习表征会议（ICLR
    2014）论文集*中。
- en: 'Klein et al. (2017) Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart,
    and Alexander M. Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation.
    In *Proc. of the 55th Annual Meeting of the Association for Computational Linguistics
    (ACL 2017)*. Vancouver, BC, Canada, 67–72.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Klein 等（2017）Guillaume Klein、Yoon Kim、Yuntian Deng、Jean Senellart 和 Alexander
    M. Rush. 2017. OpenNMT: 开源神经机器翻译工具包。见于 *第55届计算语言学协会年会（ACL 2017）论文集*。加拿大温哥华，67–72。'
- en: 'Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
    Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A.
    Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting
    Language and Vision Using Crowdsourced Dense Image Annotations. *International
    Journal of Computer Vision* 123, 1 (2017), 32–73.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Krishna 等（2017）Ranjay Krishna、Yuke Zhu、Oliver Groth、Justin Johnson、Kenji Hata、Joshua
    Kravitz、Stephanie Chen、Yannis Kalantidis、Li-Jia Li、David A. Shamma、Michael S.
    Bernstein 和 Li Fei-Fei. 2017. Visual Genome: 使用众包的密集图像注释连接语言和视觉。*计算机视觉国际杂志* 123,
    1（2017），32–73。'
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. In *Proc.
    of the 26th Annual Conference on Neural Information Processing Systems (NIPS 2012)*.
    Lake Tahoe, Nevada, USA, 1106–1114.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2012）Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E. Hinton. 2012.
    使用深度卷积神经网络的 ImageNet 分类。见于 *第26届神经信息处理系统年会（NIPS 2012）论文集*。美国内华达州湖塔霍，1106–1114。
- en: 'Kumar et al. (2016) Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James
    Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016.
    Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. In *Proc.
    of the 33nd International Conference on Machine Learning (ICML 2016)*. New York
    City, NY, USA, 1378–1387.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等（2016）Ankit Kumar、Ozan Irsoy、Peter Ondruska、Mohit Iyyer、James Bradbury、Ishaan
    Gulrajani、Victor Zhong、Romain Paulus 和 Richard Socher. 2016. 随便问我什么：用于自然语言处理的动态记忆网络。见于
    *第33届国际机器学习大会（ICML 2016）论文集*。美国纽约市，1378–1387。
- en: Kurakin et al. (2017) Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017.
    Adversarial Machine Learning at Scale. In *Proc. of the 5th International Conference
    on Learning Representations (ICLR 2017)*. oulon, France.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurakin 等（2017）Alexey Kurakin、Ian J. Goodfellow 和 Samy Bengio. 2017. 大规模对抗性机器学习。见于
    *第5届国际学习表示大会（ICLR 2017）论文集*。法国里昂。
- en: Kusner et al. (2015) Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q.
    Weinberger. 2015. From Word Embeddings To Document Distances. In *Proc. of the
    32nd International Conference on Machine Learning (ICML 2015))*. Lille, France,
    957–966.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kusner 等（2015）Matt J. Kusner、Yu Sun、Nicholas I. Kolkin 和 Kilian Q. Weinberger.
    2015. 从词嵌入到文档距离。见于 *第32届国际机器学习大会（ICML 2015）论文集*。法国里尔，957–966。
- en: Le and Mikolov (2014) Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations
    of Sentences and Documents. In *Proc. of the 31th International Conference on
    Machine Learning (ICML 2014)*. Beijing, China, 1188–1196.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 和 Mikolov（2014）Quoc V. Le 和 Tomas Mikolov. 2014. 句子和文档的分布式表示。见于 *第31届国际机器学习大会（ICML
    2014）论文集*。中国北京，1188–1196。
- en: Lee et al. (2017) Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2017. Fully
    Character-Level Neural Machine Translation without Explicit Segmentation. *TACL*
    5 (2017), 365–378.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2017）Jason Lee、Kyunghyun Cho 和 Thomas Hofmann. 2017. 完全字符级别的神经机器翻译，无需显式分词。*TACL*
    5（2017），365–378。
- en: Lehmann et al. (2015) Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
    Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick
    van Kleef, Sören Auer, and Christian Bizer. 2015. DBpedia - A large-scale, multilingual
    knowledge base extracted from Wikipedia. *Semantic Web* 6, 2 (2015), 167–195.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lehmann 等（2015）Jens Lehmann、Robert Isele、Max Jakob、Anja Jentzsch、Dimitris Kontokostas、Pablo
    N. Mendes、Sebastian Hellmann、Mohamed Morsey、Patrick van Kleef、Sören Auer 和 Christian
    Bizer. 2015. DBpedia - 从维基百科提取的大规模多语言知识库。*语义网* 6, 2（2015），167–195。
- en: 'Lewis et al. (2004) David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li.
    2004. RCV1: A New Benchmark Collection for Text Categorization Research. *Journal
    of Machine Learning Research* 5 (2004), 361–397.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis 等（2004）David D. Lewis、Yiming Yang、Tony G. Rose 和 Fan Li. 2004. RCV1:
    一个用于文本分类研究的新基准集合。*机器学习研究杂志* 5（2004），361–397。'
- en: 'Li et al. (2019) Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang.
    2019. TextBugger: Generating Adversarial Text Against Real-world Applications.
    In *Proc. of 26th Annual Network and Distributed System Security Symposium (NDSS
    2019)*. San Diego, California, USA.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2019）Jinfeng Li、Shouling Ji、Tianyu Du、Bo Li 和 Ting Wang. 2019. TextBugger:
    生成针对真实世界应用的对抗性文本。见于 *第26届年度网络和分布式系统安全研讨会（NDSS 2019）论文集*。美国加州圣地亚哥。'
- en: Li et al. (2016) Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley,
    and Jianfeng Gao. 2016. Deep Reinforcement Learning for Dialogue Generation. In
    *Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2016)*. Austin, Texas, USA, 1192–1202.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2016）Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, 和
    Jianfeng Gao. 2016. 对话生成的深度强化学习。发表于*2016年自然语言处理实证方法会议（EMNLP 2016）论文集*。美国德克萨斯州奥斯汀，1192–1202。
- en: Li and Roth (2002) Xin Li and Dan Roth. 2002. Learning Question Classifiers.
    In *Proc. of the 19th International Conference on Computational Linguistics (COLING
    2002)*. aipei, Taiwan.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li和Roth（2002）Xin Li 和 Dan Roth. 2002. 学习问题分类器。发表于*第19届国际计算语言学会议（COLING 2002）论文集*。台湾台北。
- en: Li Deng (2018) Yang Liu Li Deng. 2018. *Deep Learning in Natural Language Processing*.
    Springer Singapore. [https://doi.org/10.1007/978-981-10-5209-5](https://doi.org/10.1007/978-981-10-5209-5)
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li Deng（2018）Yang Liu Li Deng. 2018. *自然语言处理中的深度学习*。新加坡：Springer。 [https://doi.org/10.1007/978-981-10-5209-5](https://doi.org/10.1007/978-981-10-5209-5)
- en: Liang et al. (2017) Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong
    Li, and Wenchang Shi. 2017. Deep Text Classification Can be Fooled. *arXiv preprint
    arXiv:1704.08006* (2017).
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等（2017）Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, 和 Wenchang
    Shi. 2017. 深度文本分类可能被欺骗。*arXiv预印本arXiv:1704.08006*（2017）。
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft
    COCO: Common Objects in Context. In *Proc. of the 13th European Conference on
    Computer Vision (ECCV 2014)*. Zurich, Switzerland, 740–755.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2014）Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro
    Perona, Deva Ramanan, Piotr Dollár, 和 C. Lawrence Zitnick. 2014. 微软COCO：上下文中的常见物体。发表于*第13届欧洲计算机视觉会议（ECCV
    2014）论文集*。瑞士苏黎世，740–755。
- en: Liu et al. (2016) Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H.
    Lin, Xiao Ling, and Daniel S. Weld. 2016. Effective Crowd Annotation for Relation
    Extraction. In *Proc. of the 2016 Conference of the North American Chapter of
    the Association for Computational Linguistics (NAACL 2016)*. San Diego California,
    USA, 897–906.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2016）Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H. Lin,
    Xiao Ling, 和 Daniel S. Weld. 2016. 有效的人群注释用于关系提取。发表于*2016年北美计算语言学协会年会（NAACL 2016）论文集*。美国加利福尼亚州圣地亚哥，897–906。
- en: 'Liu et al. (2018) Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor
    C. M. Leung. 2018. A Survey on Security Threats and Defensive Techniques of Machine
    Learning: A Data Driven View. *IEEE Access* 6 (2018), 12103–12117.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2018）Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, 和 Victor C. M. Leung.
    2018. 关于机器学习的安全威胁和防御技术的调查：一种数据驱动的视角。*IEEE Access* 6（2018），12103–12117。
- en: Liu et al. (2017b) Tzu-Chien Liu, Yu-Hsueh Wu, and Hung-yi Lee. 2017b. Attention-based
    CNN Matching Net. *CoRR* abs/1709.05036 (2017).
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2017b）Tzu-Chien Liu, Yu-Hsueh Wu, 和 Hung-yi Lee. 2017b. 基于注意力的CNN匹配网络。*CoRR*
    abs/1709.05036（2017）。
- en: Liu et al. (2017a) Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017a.
    Delving into Transferable Adversarial Examples and Black-box Attacks. In *Proc.
    of the 2017 International Conference on Learning Representations (ICLR 2017)*.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2017a）Yanpei Liu, Xinyun Chen, Chang Liu, 和 Dawn Song. 2017a. 深入探讨可转移的对抗性示例和黑箱攻击。发表于*2017年国际学习表征会议（ICLR
    2017）论文集*。
- en: 'Lowe et al. (2015) Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau.
    2015. The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
    Multi-Turn Dialogue Systems. In *Proc. of the 16th Annual Meeting of the Special
    Interest Group on Discourse and Dialogue (SIGDIAL 2015)*. Prague, Czech Republic,
    285–294.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe等（2015）Ryan Lowe, Nissan Pow, Iulian Serban, 和 Joelle Pineau. 2015. Ubuntu对话语料库：用于非结构化多轮对话系统研究的大型数据集。发表于*第16届特邀小组会议（SIGDIAL
    2015）论文集*。捷克共和国布拉格，285–294。
- en: Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment
    Analysis. In *Proc. of the 49th Annual Meeting of the Association for Computational
    Linguistics (ACL 2011)*. Portland, Oregon, USA, 142–150.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maas等（2011）Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew
    Y. Ng, 和 Christopher Potts. 2011. 用于情感分析的词向量学习。发表于*第49届计算语言学协会年会（ACL 2011）论文集*。美国俄勒冈州波特兰，142–150。
- en: Madry et al. (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
    Attacks. In *Proc. of the 6th International Conference on Learning Representations
    (ICLR 2018)*. Vancouver, BC, Canada.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madry 等 (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, 和 Adrian Vladu. 2018. 朝着抗对抗攻击的深度学习模型迈进。见 *第6届国际学习表示会议 (ICLR 2018)* 论文集。加拿大温哥华。
- en: 'Marcus et al. (1993) Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
    1993. Building a Large Annotated Corpus of English: The Penn Treebank. *Computational
    Linguistics* 19, 2 (1993), 313–330.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcus 等 (1993) Mitchell P. Marcus, Beatrice Santorini, 和 Mary Ann Marcinkiewicz.
    1993. 构建大规模英文注释语料库：Penn Treebank。*计算语言学* 19, 2 (1993), 第313–330页。
- en: 'Marelli et al. (2014) Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella
    Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. SemEval-2014 Task 1: Evaluation
    of Compositional Distributional Semantic Models on Full Sentences through Semantic
    Relatedness and Textual Entailment. In *Proc. of the 8th International Workshop
    on Semantic Evaluation (SemEval@COLING 2014)*. Dublin, Ireland, 1–8.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marelli 等 (2014) Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi,
    Stefano Menini, 和 Roberto Zamparelli. 2014. SemEval-2014 任务 1: 通过语义相关性和文本蕴涵评估完整句子上的组合分布式语义模型。见
    *第8届国际语义评估研讨会 (SemEval@COLING 2014)* 论文集。都柏林，爱尔兰，第1–8页。'
- en: 'Mauro et al. (2012) Cettolo Mauro, Girardi Christian, and Federico Marcello.
    2012. Wit3: Web Inventory of Transcribed and Translated Talks. In *Conference
    of European Association for Machine Translation*. 261–268.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mauro 等 (2012) Cettolo Mauro, Girardi Christian, 和 Federico Marcello. 2012.
    Wit3: 转录和翻译讲座的网络清单。见 *欧洲机器翻译协会会议*。第261–268页。'
- en: 'McAuley and Leskovec (2013) Julian J. McAuley and Jure Leskovec. 2013. Hidden
    factors and hidden topics: understanding rating dimensions with review text. In
    *Proc. of the 7th ACM Conference on Recommender Systems (RecSys 2013)*. Hong Kong,
    China, 165–172.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McAuley 和 Leskovec (2013) Julian J. McAuley 和 Jure Leskovec. 2013. 隐含因素和隐含主题：通过评论文本理解评分维度。见
    *第7届ACM推荐系统会议 (RecSys 2013)* 论文集。中国香港，第165–172页。
- en: Metsis et al. (2006) Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras.
    2006. Spam Filtering with Naive Bayes - Which Naive Bayes?. In *Proc. of the Third
    Conference on Email and Anti-Spam (CEAS 2006)*. Mountain View, California, USA.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metsis 等 (2006) Vangelis Metsis, Ion Androutsopoulos, 和 Georgios Paliouras.
    2006. 使用朴素贝叶斯进行垃圾邮件过滤 - 哪种朴素贝叶斯？在 *第三届电子邮件和反垃圾邮件会议 (CEAS 2006)* 论文集中。加利福尼亚州山景城，美国。
- en: Minervini and Riedel (2018) Pasquale Minervini and Sebastian Riedel. 2018. Adversarially
    Regularising Neural NLI Models to Integrate Logical Background Knowledge. In *Proc.
    of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018)*.
    Brussels, Belgium, 65–74.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minervini 和 Riedel (2018) Pasquale Minervini 和 Sebastian Riedel. 2018. 对抗性正则化神经NLI模型以整合逻辑背景知识。见
    *第22届计算自然语言学习会议 (CoNLL 2018)* 论文集。比利时布鲁塞尔，第65–74页。
- en: Mishra et al. (2012) Anand Mishra, Karteek Alahari, and C. V. Jawahar. 2012.
    Scene Text Recognition using Higher Order Language Priors. In *Proc. of the 23rd
    British Machine Vision Conference (BMVC 2012)*. Surrey, UK, 1–11.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等 (2012) Anand Mishra, Karteek Alahari, 和 C. V. Jawahar. 2012. 使用高阶语言先验进行场景文本识别。见
    *第23届英国机器视觉会议 (BMVC 2012)* 论文集。萨里，英国，第1–11页。
- en: Miyato et al. (2016a) Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2016a.
    Adversarial training methods for semi-supervised text classification. *arXiv preprint
    arXiv:1605.07725* (2016).
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyato 等 (2016a) Takeru Miyato, Andrew M Dai, 和 Ian Goodfellow. 2016a. 半监督文本分类的对抗训练方法。*arXiv
    预印本 arXiv:1605.07725* (2016)。
- en: Miyato et al. (2016b) Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae,
    and Shin Ishii. 2016b. Distributional smoothing with virtual adversarial training.
    In *Proc. of the 4th International Conference on Learning Representations (ICLR
    2016)*.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyato 等 (2016b) Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae,
    和 Shin Ishii. 2016b. 通过虚拟对抗训练进行分布式平滑。见 *第4届国际学习表示会议 (ICLR 2016)* 论文集。
- en: 'Moosavi-Dezfooli et al. (2016) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
    and Pascal Frossard. 2016. DeepFool: A Simple and Accurate Method to Fool Deep
    Neural Networks. In *Proc. of the 2016 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR 2016)*. Las Vegas, NV, USA, 2574–2582.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moosavi-Dezfooli 等 (2016) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, 和
    Pascal Frossard. 2016. DeepFool: 一种简单而准确的欺骗深度神经网络的方法。见 *2016 IEEE计算机视觉与模式识别会议
    (CVPR 2016)* 论文集。拉斯维加斯，内华达州，美国，第2574–2582页。'
- en: 'Mozer (1995) Michael C Mozer. 1995. A focused backpropagation algorithm for
    temporal. *Backpropagation: Theory, architectures, and applications* 137 (1995).'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mozer (1995) Michael C Mozer. 1995. 一种用于时间序列的聚焦反向传播算法。*反向传播：理论、架构和应用* 137 (1995).
- en: Neekhara et al. (2018) Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, and
    Farinaz Koushanfar. 2018. Adversarial Reprogramming of Sequence Classification
    Neural Networks. *CoRR* abs/1809.01829 (2018). arXiv:1809.01829 [http://arxiv.org/abs/1809.01829](http://arxiv.org/abs/1809.01829)
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neekhara 等 (2018) Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, 和 Farinaz
    Koushanfar. 2018. 序列分类神经网络的对抗重编程。*CoRR* abs/1809.01829 (2018). arXiv:1809.01829
    [http://arxiv.org/abs/1809.01829](http://arxiv.org/abs/1809.01829)
- en: Niu and Bansal (2018) Tong Niu and Mohit Bansal. 2018. Adversarial Over-Sensitivity
    and Over-Stability Strategies for Dialogue Models. In *Proc. of the 22nd Conference
    on Computational Natural Language Learning (CoNLL 2018)*. Brussels, Belgium, 486–496.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 和 Bansal (2018) Tong Niu 和 Mohit Bansal. 2018. 对话模型的对抗过度敏感性和过度稳定性策略。在 *第22届计算自然语言学习会议论文集
    (CoNLL 2018)*. 比利时布鲁塞尔，486–496.
- en: Nivre et al. (2015) Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki
    Asahara, Aitziber Atutxa, Miguel Ballesteros, John Bauer, Kepa Bengoetxea, Riyaz Ahmad
    Bhat, Cristina Bosco, et al. 2015. Universal Dependencies 1.2. (2015).
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nivre 等 (2015) Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki Asahara,
    Aitziber Atutxa, Miguel Ballesteros, John Bauer, Kepa Bengoetxea, Riyaz Ahmad
    Bhat, Cristina Bosco 等. 2015. Universal Dependencies 1.2. (2015).
- en: Otter et al. (2018) Daniel W. Otter, Julian R. Medina, and Jugal K. Kalita.
    2018. A Survey of the Usages of Deep Learning in Natural Language Processing.
    *CoRR* abs/1807.10854 (2018).
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Otter 等 (2018) Daniel W. Otter, Julian R. Medina, 和 Jugal K. Kalita. 2018. 深度学习在自然语言处理中的应用调查。*CoRR*
    abs/1807.10854 (2018).
- en: 'Palangi et al. (2016) Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong
    He, Jianshu Chen, Xinying Song, and Rabab K. Ward. 2016. Deep Sentence Embedding
    Using Long Short-Term Memory Networks: Analysis and Application to Information
    Retrieval. *IEEE/ACM Trans. Audio, Speech & Language Processing* 24, 4 (2016),
    694–707.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palangi 等 (2016) Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong
    He, Jianshu Chen, Xinying Song, 和 Rabab K. Ward. 2016. 使用长短期记忆网络的深度句子嵌入：分析及其在信息检索中的应用。*IEEE/ACM
    音频、语音与语言处理学报* 24, 4 (2016), 694–707.
- en: 'Pang and Lee (2005) Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploiting
    Class Relationships for Sentiment Categorization with Respect to Rating Scales.
    In *Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics
    (ACL 2005)*. Michigan, USA, 115–124.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 和 Lee (2005) Bo Pang 和 Lillian Lee. 2005. 观察星星：利用类别关系进行情感分类相对于评分尺度的应用。在
    *第43届计算语言学协会年会论文集 (ACL 2005)*. 美国密歇根州，115–124.
- en: Papernot et al. (2016b) Nicolas Papernot, Patrick McDaniel, Ananthram Swami,
    and Richard Harang. 2016b. Crafting Adversarial Input Sequences for Recurrent
    Neural Networks. In *Military Communications Conference, MILCOM 2016-2016 IEEE*.
    IEEE, 49–54.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等 (2016b) Nicolas Papernot, Patrick McDaniel, Ananthram Swami, 和 Richard
    Harang. 2016b. 为递归神经网络设计对抗输入序列。在 *军事通信会议，MILCOM 2016-2016 IEEE*。IEEE，49–54.
- en: Papernot et al. (2017) Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow,
    Somesh Jha, Z. Berkay Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks
    against Machine Learning. In *Proc. of the 2017 ACM on Asia Conference on Computer
    and Communications Security (AsiaCCS 2017)*. Abu Dhabi, United Arab Emirates,
    506–519.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等 (2017) Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow,
    Somesh Jha, Z. Berkay Celik, 和 Ananthram Swami. 2017. 针对机器学习的实用黑盒攻击。在 *2017年ACM亚洲计算机与通信安全会议论文集
    (AsiaCCS 2017)*. 阿布扎比，阿联酋，506–519.
- en: Papernot et al. (2016a) Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt
    Fredrikson, Z. Berkay Celik, and Ananthram Swami. 2016a. The Limitations of Deep
    Learning in Adversarial Settings. In *IEEE European Symposium on Security and
    Privacy (EuroS&P 2016)*. Saarbrücken, Germany, 372–387.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等 (2016a) Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson,
    Z. Berkay Celik, 和 Ananthram Swami. 2016a. 深度学习在对抗环境中的局限性。在 *IEEE欧洲安全与隐私研讨会 (EuroS&P
    2016)*. 德国萨尔布吕肯，372–387.
- en: Papernot et al. (2016c) Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh
    Jha, and Ananthram Swami. 2016c. Distillation as a Defense to Adversarial Perturbations
    Against Deep Neural Networks. In *Proc. of the 2016 IEEE Symposium on Security
    and Privacy (SP 2016)*. San Jose, CA, USA, 582–597.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等 (2016c) Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha,
    和 Ananthram Swami. 2016c. 作为防御对抗扰动的蒸馏方法。 在 *2016年IEEE安全与隐私研讨会论文集 (SP 2016)*. 美国加州圣荷西，582–597.
- en: Parikh et al. (2016) Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob
    Uszkoreit. 2016. A Decomposable Attention Model for Natural Language Inference.
    In *Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2016)*. Austin, Texas, USA, 2249–2255.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parikh 等人 (2016) Ankur P. Parikh, Oscar Täckström, Dipanjan Das, 和 Jakob Uszkoreit.
    2016. 一种用于自然语言推理的可分解注意力模型。载于 *2016 年自然语言处理实证方法会议论文集 (EMNLP 2016)*。美国德克萨斯州奥斯汀,
    2249–2255。
- en: 'Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized
    Word Representations. In *Proc. of the 2018 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies
    (NAACL-HLT 2018)*. New Orleans, Louisiana, USAr, 2227–2237.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 等人 (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, 和 Luke Zettlemoyer. 2018. 深度上下文化词表示。载于 *2018 年北美计算语言学协会：人类语言技术会议论文集
    (NAACL-HLT 2018)*。美国路易斯安那州新奥尔良, 2227–2237。
- en: Raff et al. ([n. d.]) Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon,
    Bryan Catanzaro, and Charles K. Nicholas. [n. d.]. Malware Detection by Eating
    a Whole EXE. In *The Workshops of the The Thirty-Second AAAI Conference on Artificial
    Intelligence*. New Orleans, Louisiana, USA, 268–276.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raff 等人 ([n. d.]) Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon,
    Bryan Catanzaro, 和 Charles K. Nicholas. [n. d.]. 通过处理完整的 EXE 文件进行恶意软件检测。载于 *第32届
    AAAI 人工智能会议工作坊*。美国路易斯安那州新奥尔良, 268–276。
- en: Riedel et al. (2010) Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010.
    Modeling Relations and Their Mentions without Labeled Text. In *Proc. of 2010
    European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD
    2010)*. Barcelona, Spain, 148–163.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riedel 等人 (2010) Sebastian Riedel, Limin Yao, 和 Andrew McCallum. 2010. 在没有标注文本的情况下建模关系及其提及。载于
    *2010 年欧洲机器学习和数据库知识发现会议论文集 (ECML/PKDD 2010)*。西班牙巴塞罗那, 148–163。
- en: Rocktäschel et al. (2016) Tim Rocktäschel, Edward Grefenstette, Karl Moritz
    Hermann, Tomás Kociský, and Phil Blunsom. 2016. Reasoning about Entailment with
    Neural Attention. In *Proc. of the 2016 International Conference on Learning Representations
    (ICLR 2016)*.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rocktäschel 等人 (2016) Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann,
    Tomás Kociský, 和 Phil Blunsom. 2016. 使用神经注意力推理蕴含关系。载于 *2016 年国际学习表征会议论文集 (ICLR
    2016)*。
- en: Ronen et al. (2018) Royi Ronen, Marian Radu, Corina Feuerstein, Elad Yom-Tov,
    and Mansour Ahmadi. 2018. Microsoft Malware Classification Challenge. *CoRR* abs/1802.10135
    (2018). arXiv:1802.10135 [http://arxiv.org/abs/1802.10135](http://arxiv.org/abs/1802.10135)
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronen 等人 (2018) Royi Ronen, Marian Radu, Corina Feuerstein, Elad Yom-Tov, 和
    Mansour Ahmadi. 2018. 微软恶意软件分类挑战。*CoRR* abs/1802.10135 (2018)。arXiv:1802.10135
    [http://arxiv.org/abs/1802.10135](http://arxiv.org/abs/1802.10135)
- en: Rosenberg et al. (2017) Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval
    Elovici. 2017. Generic Black-Box End-to-End Attack against RNNs and Other API
    Calls Based Malware Classifiers. *arXiv preprint arXiv:1707.05970* (2017).
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenberg 等人 (2017) Ishai Rosenberg, Asaf Shabtai, Lior Rokach, 和 Yuval Elovici.
    2017. 针对 RNN 及其他 API 调用的通用黑箱端到端攻击。*arXiv 预印本 arXiv:1707.05970* (2017)。
- en: Roth and Yih (2004) Dan Roth and Wen-tau Yih. 2004. A Linear Programming Formulation
    for Global Inference in Natural Language Tasks. In *Proc. of the 8th Conference
    on Computational Natural Language Learning (CoNLL 2004)*. Boston, Massachusetts,
    1–8.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roth 和 Yih (2004) Dan Roth 和 Wen-tau Yih. 2004. 自然语言任务中的全局推理线性规划形式化。载于 *第8届计算自然语言学习会议论文集
    (CoNLL 2004)*。美国马萨诸塞州波士顿, 1–8。
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 等人 (1986) David E Rumelhart, Geoffrey E Hinton, 和 Ronald J Williams.
    1986. 通过反向传播误差学习表示。*nature* 323, 6088 (1986), 533。
- en: Samanta and Mehta (2018) Suranjana Samanta and Sameep Mehta. 2018. Generating
    Adversarial Text Samples. In *Proc. of the 40th European Conference on IR Research
    (ECIR 2018)*. Grenoble, France, 744–749.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samanta 和 Mehta (2018) Suranjana Samanta 和 Sameep Mehta. 2018. 生成对抗文本样本。载于 *第40届欧洲信息检索研究会议论文集
    (ECIR 2018)*。法国格勒诺布尔, 744–749。
- en: Sato et al. (2018) Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Matsumoto.
    2018. Interpretable Adversarial Perturbation in Input Embedding Space for Text.
    *arXiv preprint arXiv:1805.02917* (2018).
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sato 等人 (2018) Motoki Sato, Jun Suzuki, Hiroyuki Shindo, 和 Yuji Matsumoto. 2018.
    输入嵌入空间中的可解释对抗扰动。*arXiv 预印本 arXiv:1805.02917* (2018)。
- en: Schuurmans and Zinkevich (2016) Dale Schuurmans and Martin Zinkevich. 2016.
    Deep Learning Games. In *Proc. of the Annual Conference on Neural Information
    Processing Systems 2016 (NIPS 2016)*. Barcelona, Spain, 1678–1686.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuurmans 和 Zinkevich（2016）Dale Schuurmans 和 Martin Zinkevich。2016。深度学习游戏。在
    *2016年神经信息处理系统年会（NIPS 2016）* 上。西班牙巴塞罗那，1678–1686。
- en: 'Sennrich et al. (2017) Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra
    Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel Läubli,
    Antonio Valerio Miceli Barone, Jozef Mokry, and Maria Nadejde. 2017. Nematus:
    a Toolkit for Neural Machine Translation. In *Proc. of the 15th Conference of
    the European Chapter of the Association for Computational Linguistics (EACL 2017),
    Demo*. Valencia, Spain, 65–68.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich 等人（2017）Rico Sennrich、Orhan Firat、Kyunghyun Cho、Alexandra Birch、Barry
    Haddow、Julian Hitschler、Marcin Junczys-Dowmunt、Samuel Läubli、Antonio Valerio Miceli
    Barone、Jozef Mokry 和 Maria Nadejde。2017。Nematus：一个神经机器翻译工具包。在 *第15届欧洲计算语言学协会年会（EACL
    2017），演示* 上。西班牙瓦伦西亚，65–68。
- en: Seo et al. (2016) Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh
    Hajishirzi. 2016. Bidirectional Attention Flow for Machine Comprehension. *CoRR*
    abs/1611.01603 (2016). arXiv:1611.01603 [http://arxiv.org/abs/1611.01603](http://arxiv.org/abs/1611.01603)
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seo 等人（2016）Min Joon Seo、Aniruddha Kembhavi、Ali Farhadi 和 Hannaneh Hajishirzi。2016。用于机器理解的双向注意力流。*CoRR*
    abs/1611.01603（2016）。arXiv:1611.01603 [http://arxiv.org/abs/1611.01603](http://arxiv.org/abs/1611.01603)
- en: Serban et al. (2016) Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio,
    Aaron C. Courville, and Joelle Pineau. 2016. Building End-To-End Dialogue Systems
    Using Generative Hierarchical Neural Network Models. In *Proc. of the Thirtieth
    AAAI Conference on Artificial Intelligence (AAAI 2016)*. Phoenix, Arizona, USA,
    3776–3784.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serban 等人（2016）Iulian Vlad Serban、Alessandro Sordoni、Yoshua Bengio、Aaron C.
    Courville 和 Joelle Pineau。2016。使用生成层次神经网络模型构建端到端对话系统。在 *第三十届人工智能AAAI会议（AAAI 2016）*
    上。美国亚利桑那州凤凰城，3776–3784。
- en: Serban et al. (2017) Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent
    Charlin, Joelle Pineau, Aaron C. Courville, and Yoshua Bengio. 2017. A Hierarchical
    Latent Variable Encoder-Decoder Model for Generating Dialogues. In *Proc. of the
    31st AAAI Conference on Artificial Intelligence (AAAI 2017)*. San Francisco, California,
    USA, 3295–3301.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serban 等人（2017）Iulian Vlad Serban、Alessandro Sordoni、Ryan Lowe、Laurent Charlin、Joelle
    Pineau、Aaron C. Courville 和 Yoshua Bengio。2017。用于生成对话的层次潜变量编码器-解码器模型。在 *第31届人工智能AAAI会议（AAAI
    2017）* 上。美国加州旧金山，3295–3301。
- en: Shi et al. (2017) Baoguang Shi, Xiang Bai, and Cong Yao. 2017. An end-to-end
    trainable neural network for image-based sequence recognition and its application
    to scene text recognition. *IEEE transactions on pattern analysis and machine
    intelligence* 39, 11 (2017), 2298–2304.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2017）Baoguang Shi、Xiang Bai 和 Cong Yao。2017。用于基于图像的序列识别的端到端可训练神经网络及其在场景文本识别中的应用。*IEEE模式分析与机器智能汇刊*
    39, 11（2017），2298–2304。
- en: Shi et al. (2018) Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang, and Jian
    Sun. 2018. Learning Visually-Grounded Semantics from Contrastive Adversarial Samples.
    In *Proc. of the 27th International Conference on Computational Linguistics (COLING
    2018)*. Santa Fe, New Mexico, USA, 3715–3727.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2018）Haoyue Shi、Jiayuan Mao、Tete Xiao、Yuning Jiang 和 Jian Sun。2018。通过对比对抗样本学习视觉基础语义。在
    *第27届国际计算语言学会议（COLING 2018）* 上。美国新墨西哥州圣菲，3715–3727。
- en: Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. Very
    Deep Convolutional Networks for Large-Scale Image Recognition. In *Proc. of the
    3rd International Conference on Learning Representations (ICLR 2015*. San Diego,
    CA, USA.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman（2015）Karen Simonyan 和 Andrew Zisserman。2015。用于大规模图像识别的非常深度卷积网络。在
    *第三届学习表征国际会议（ICLR 2015）* 上。美国加州圣地亚哥。
- en: Singh et al. (2018) Sameer Singh, Carlos Guestrin, and Marco Túlio Ribeiro.
    2018. Semantically Equivalent Adversarial Rules for Debugging NLP models. In *Proc.
    of the 56th Annual Meeting of the Association for Computational Linguistics (ACL
    2018)*. Melbourne, Australia, 856–865.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人（2018）Sameer Singh、Carlos Guestrin 和 Marco Túlio Ribeiro。2018。用于调试NLP模型的语义等效对抗规则。在
    *第56届计算语言学协会年会（ACL 2018）* 上。澳大利亚墨尔本，856–865。
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive Deep
    Models for Semantic Compositionality Over a Sentiment Treebank. In *Proc. of the
    2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)*.
    Seattle, Washington, USA, 1631–1642.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher 等（2013）理查德·索切尔、亚历克斯·佩雷尔金、让·吴、杰森·庄、克里斯托弗·D·曼宁、安德鲁·Y·吴和克里斯托弗·波茨。2013。针对情感树库的语义组合递归深度模型。在
    *2013年自然语言处理实证方法会议（EMNLP 2013）* 的会议论文集。华盛顿州西雅图，1631–1642。
- en: Song and Shmatikov (2018) Congzheng Song and Vitaly Shmatikov. 2018. Fooling
    OCR Systems with Adversarial Text Images. *CoRR* abs/1802.05385 (2018). arXiv:1802.05385
    [http://arxiv.org/abs/1802.05385](http://arxiv.org/abs/1802.05385)
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 和 Shmatikov（2018）宋从政和维塔利·什马提科夫。2018。用对抗文本图像欺骗OCR系统。*CoRR* abs/1802.05385（2018）。arXiv:1802.05385
    [http://arxiv.org/abs/1802.05385](http://arxiv.org/abs/1802.05385)
- en: Sun et al. (2018) Mengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang, and Jiayu
    Zhou. 2018. Identify Susceptible Locations in Medical Records via Adversarial
    Attacks on Deep Predictive Models. In *Proc. of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining (KDD 2018)*. London, UK, 793–801.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2018）孙梦颖、唐丰义、易金峰、王飞和周佳宇。2018。通过对深度预测模型的对抗性攻击识别医疗记录中的易感位置。在 *第24届ACM SIGKDD国际知识发现与数据挖掘大会（KDD
    2018）* 的会议论文集。英国伦敦，793–801。
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
    Sequence to Sequence Learning with Neural Networks. In *Proc. of the Annual Conference
    on Neural Information Processing Systems 2014 (NIPS 2014)*. Montreal, Quebec,
    Canada, 2672–2680.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等（2014）伊利亚·苏茨克维尔、奥里奥尔·维尼亚尔斯和阮伟龙。2014。基于神经网络的序列到序列学习。在 *2014年神经信息处理系统大会（NIPS
    2014）* 的会议论文集。加拿大魁北克省蒙特利尔，2672–2680。
- en: Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, and
    Joan Bruna. 2014. Intriguing properties of neural networks. In *Proc. of the 2nd
    International Conference on Learning Representations (ICLR 2014)*.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2014）克里斯蒂安·谢杰迪、沃伊切赫·扎伦巴、伊利亚·苏茨克维尔和琼·布鲁纳。2014。神经网络的引人入胜的特性。在 *第二届国际学习表示大会（ICLR
    2014）* 的会议论文集。
- en: Tai et al. (2015) Kai Sheng Tai, Richard Socher, and Christopher D. Manning.
    2015. Improved Semantic Representations From Tree-Structured Long Short-Term Memory
    Networks. In *Proc. of the 53rd Annual Meeting of the Association for Computational
    Linguistics (ACL 2015)*. Beijing, China, 1556–1566.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tai 等（2015）台生凯、理查德·索切尔和克里斯托弗·D·曼宁。2015。基于树结构长短期记忆网络的改进语义表示。在 *第53届计算语言学协会年会（ACL
    2015）* 的会议论文集。中国北京，1556–1566。
- en: 'Tapaswi et al. (2016) Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio
    Torralba, Raquel Urtasun, and Sanja Fidler. 2016. MovieQA: Understanding Stories
    in Movies through Question-Answering. In *Proc. of the 2016 IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR 2016)*. Las Vegas, NV, USA, 4631–4640.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tapaswi 等（2016）马卡朗德·塔帕斯维、朱玉坤、赖纳·斯蒂费尔哈根、安东尼奥·托拉尔巴、拉奎尔·乌尔塔苏恩和桑贾·费德勒。2016。MovieQA：通过问答理解电影故事。在
    *2016年IEEE计算机视觉与模式识别会议（CVPR 2016）* 的会议论文集。美国内华达州拉斯维加斯，4631–4640。
- en: Tesseract (2016) Tesseract. 2016. https://github.com/tesseract-ocr/.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tesseract（2016）Tesseract。2016。 https://github.com/tesseract-ocr/。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is All you Need. In *Proc. of the Annual Conference on Neural Information Processing
    Systems 2017 (NIPS 2017)*. Long Beach, CA, USA, 6000–6010.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔玛尔、雅各布·乌斯科雷特、利昂·琼斯、艾丹·N·戈麦斯、卢卡斯·凯泽和伊利亚·波洛苏欣。2017。注意力机制就是一切。
    在 *2017年神经信息处理系统大会（NIPS 2017）* 的会议论文集。美国加利福尼亚州长滩，6000–6010。
- en: 'Vinyals et al. (2015) Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru
    Erhan. 2015. Show and tell: A neural image caption generator. In *Proc. of IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR 2015)*. Boston, MA,
    USA, 3156–3164.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等（2015）奥里奥尔·维尼亚尔斯、亚历山大·托谢夫、萨米·本吉奥和杜米特鲁·厄尔汉。2015。展示与讲述：一种神经图像描述生成器。在
    *IEEE计算机视觉与模式识别会议（CVPR 2015）* 的会议论文集。美国马萨诸塞州波士顿，3156–3164。
- en: Wang et al. (2008) Canhui Wang, Min Zhang, Shaoping Ma, and Liyun Ru. 2008.
    Automatic online news issue construction in web environment. In *Proc. of the
    17th International Conference on World Wide Web (WWW 2008)*. Beijing, China, 457–466.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2008）王灿辉、张敏、马少平和如利云。2008。在网页环境中自动生成在线新闻问题。在 *第17届国际万维网大会（WWW 2008）* 的会议论文集。中国北京，457–466。
- en: Wang et al. (2011) Kai Wang, Boris Babenko, and Serge J. Belongie. 2011. End-to-end
    scene text recognition. In *Proc. of the 2011 IEEE International Conference on
    Computer Vision (ICCV 2011)*. Barcelona, Spain, 1457–1464.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2011) Kai Wang, Boris Babenko, 和 Serge J. Belongie. 2011. End-to-end
    scene text recognition. 在 *2011年IEEE国际计算机视觉会议 (ICCV 2011)*. 西班牙巴塞罗那, 1457–1464.
- en: Wang and Jiang (2016a) Shuohang Wang and Jing Jiang. 2016a. A compare-aggregate
    model for matching text sequences. *arXiv preprint arXiv:1611.01747* (2016).
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Jiang (2016a) Shuohang Wang 和 Jing Jiang. 2016a. A compare-aggregate
    model for matching text sequences. *arXiv预印本 arXiv:1611.01747* (2016).
- en: Wang and Jiang (2016b) Shuohang Wang and Jing Jiang. 2016b. Machine Comprehension
    Using Match-LSTM and Answer Pointer. *CoRR* abs/1608.07905 (2016). arXiv:1608.07905
    [http://arxiv.org/abs/1608.07905](http://arxiv.org/abs/1608.07905)
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Jiang (2016b) Shuohang Wang 和 Jing Jiang. 2016b. Machine Comprehension
    Using Match-LSTM and Answer Pointer. *CoRR* abs/1608.07905 (2016). arXiv:1608.07905
    [http://arxiv.org/abs/1608.07905](http://arxiv.org/abs/1608.07905)
- en: 'Wang and Bansal (2018) Yicheng Wang and Mohit Bansal. 2018. Robust Machine
    Comprehension Models via Adversarial Training. In *Proc. of the 2018 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (NAACL-HLT)*. New Orleans, Louisiana, 575–581.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Bansal (2018) Yicheng Wang 和 Mohit Bansal. 2018. Robust Machine Comprehension
    Models via Adversarial Training. 在 *2018年北美计算语言学协会人类语言技术会议 (NAACL-HLT)*. 美国路易斯安那州新奥尔良,
    575–581.
- en: Warde-Farley and Goodfellow (2016) David Warde-Farley and Ian Goodfellow. 2016.
    Adversarial Perturbations of Deep Neural Networks. *Perturbations, Optimization,
    and Statistics* 311 (2016).
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Warde-Farley and Goodfellow (2016) David Warde-Farley 和 Ian Goodfellow. 2016.
    Adversarial Perturbations of Deep Neural Networks. *Perturbations, Optimization,
    and Statistics* 311 (2016).
- en: Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R. Bowman.
    2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.
    In *Proc. of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics (NAACL 2018)*. New Orleans, Louisiana, USA, 1112–1122.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams et al. (2018) Adina Williams, Nikita Nangia, 和 Samuel R. Bowman. 2018.
    A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.
    在 *2018年北美计算语言学协会会议 (NAACL 2018)*. 美国路易斯安那州新奥尔良, 1112–1122.
- en: Wu et al. (2017) Yi Wu, David Bamman, and Stuart Russell. 2017. Adversarial
    training for relation extraction. In *Proceedings of the 2017 Conference on Empirical
    Methods in Natural Language Processing*. 1778–1783.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2017) Yi Wu, David Bamman, 和 Stuart Russell. 2017. Adversarial training
    for relation extraction. 在 *2017年自然语言处理实证方法会议论文集*. 1778–1783.
- en: 'Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad
    Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
    Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,
    Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant
    Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
    Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s Neural Machine
    Translation System: Bridging the Gap between Human and Machine Translation. *CoRR*
    abs/1609.08144 (2016). arXiv:1609.08144 [http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144)'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad
    Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
    Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,
    Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant
    Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
    Greg Corrado, Macduff Hughes, 和 Jeffrey Dean. 2016. Google的神经机器翻译系统：弥合人类与机器翻译之间的差距。
    *CoRR* abs/1609.08144 (2016). arXiv:1609.08144 [http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144)
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville,
    Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend
    and Tell: Neural Image Caption Generation with Visual Attention. In *Proc. of
    the 32nd International Conference on Machine Learning (ICML 2015)*. Lille, France,
    2048–2057.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville,
    Ruslan Salakhutdinov, Richard S. Zemel, 和 Yoshua Bengio. 2015. Show, Attend and
    Tell: Neural Image Caption Generation with Visual Attention. 在 *第32届国际机器学习会议 (ICML
    2015)*. 法国里尔, 2048–2057.'
- en: Xu et al. (2018) Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell,
    and Dawn Song. 2018. Fooling Vision and Language Models Despite Localization and
    Attention Mechanism. In *Proc. of 2018 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR 2018)*. Salt Lake City, UT, USA, 4951–4961.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2018) Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell,
    和 Dawn Song. 2018. Fooling Vision and Language Models Despite Localization and
    Attention Mechanism. 在 *2018年IEEE计算机视觉与模式识别会议 (CVPR 2018)*. 美国犹他州盐湖城, 4951–4961.
- en: Yang et al. (2015) Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and
    Li Deng. 2015. Embedding Entities and Relations for Learning and Inference in
    Knowledge Bases. In *Proc. of the 3rd International Conference on Learning Representations
    (ICLR 2015)*. San Diego, CA, USA.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2015) Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, 和 Li
    Deng. 2015. 为知识库中的学习和推理嵌入实体和关系。见 *第3届国际学习表征会议论文集（ICLR 2015）*。美国加利福尼亚州圣地亚哥。
- en: Yannakoudakis et al. (2011) Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
    2011. A New Dataset and Method for Automatically Grading ESOL Texts. In *Proc.
    of the 49th Annual Meeting of the Association for Computational Linguistics (ACL
    2011)*. Portland, Oregon, USA, 180–189.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yannakoudakis et al. (2011) Helen Yannakoudakis, Ted Briscoe, 和 Ben Medlock.
    2011. 自动评分ESOL文本的新数据集和方法。见 *第49届计算语言学协会年会论文集（ACL 2011）*。美国俄勒冈州波特兰，180–189。
- en: Yasunaga et al. (2018) Michihiro Yasunaga, Jungo Kasai, and Dragomir R. Radev.
    2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In
    *Proc. of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics (NAACL 2018)*. New Orleans, Louisiana, USA, 976–986.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yasunaga et al. (2018) Michihiro Yasunaga, Jungo Kasai, 和 Dragomir R. Radev.
    2018. 通过对抗训练实现鲁棒的多语言词性标注。见 *2018年北美计算语言学协会年会论文集（NAACL 2018）*。美国路易斯安那州新奥尔良，976–986。
- en: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik
    Cambria. 2018. Recent Trends in Deep Learning Based Natural Language Processing.
    *IEEE Computational Intelligence Magazine* 13, 3 (2018), 55–75.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, 和 Erik Cambria.
    2018. 基于深度学习的自然语言处理最新趋势。*IEEE计算智能杂志* 13, 3 (2018), 55–75。
- en: Yuan et al. (2018) Xiaoyong Yuan, Pan He, and Xiaolin Andy Li. 2018. Adaptive
    Adversarial Attack on Scene Text Recognition. *CoRR* abs/1807.03326 (2018). arXiv:1807.03326
    [http://arxiv.org/abs/1807.03326](http://arxiv.org/abs/1807.03326)
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2018) Xiaoyong Yuan, Pan He, 和 Xiaolin Andy Li. 2018. 对场景文本识别的自适应对抗攻击。*CoRR*
    abs/1807.03326 (2018). arXiv:1807.03326 [http://arxiv.org/abs/1807.03326](http://arxiv.org/abs/1807.03326)
- en: 'Yuan et al. (2017) Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and
    Xiaolin Li. 2017. Adversarial Examples: Attacks and Defenses for Deep Learning.
    *CoRR* abs/1712.07107 (2017). arXiv:1712.07107 [http://arxiv.org/abs/1712.07107](http://arxiv.org/abs/1712.07107)'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2017) Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, 和 Xiaolin
    Li. 2017. 对抗样本：深度学习的攻击与防御。*CoRR* abs/1712.07107 (2017). arXiv:1712.07107 [http://arxiv.org/abs/1712.07107](http://arxiv.org/abs/1712.07107)
- en: Zhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level
    Convolutional Networks for Text Classification. In *Proc. in Annual Conference
    on Neural Information Processing Systems 2015 (NIPS 2015)*. Montreal, Quebec,
    Canada, 649–657.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, 和 Yann LeCun. 2015. 基于字符级卷积网络的文本分类。见
    *2015年神经信息处理系统年会论文集（NIPS 2015）*。加拿大魁北克蒙特利尔，649–657。
- en: Zhao et al. (2017) Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2017. Generating
    natural adversarial examples. *arXiv preprint arXiv:1710.11342* (2017).
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2017) Zhengli Zhao, Dheeru Dua, 和 Sameer Singh. 2017. 生成自然对抗样本。*arXiv预印本
    arXiv:1710.11342* (2017).
- en: 'Zhu et al. (2016) Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei.
    2016. Visual7W: Grounded Question Answering in Images. In *Proc. of the 2016 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR 2016)*. Las Vegas,
    NV, USA, 4995–5004.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2016) Yuke Zhu, Oliver Groth, Michael S. Bernstein, 和 Li Fei-Fei.
    2016. Visual7W：图像中的基础问答。见 *2016年IEEE计算机视觉与模式识别会议（CVPR 2016）论文集*。美国内华达州拉斯维加斯，4995–5004。
