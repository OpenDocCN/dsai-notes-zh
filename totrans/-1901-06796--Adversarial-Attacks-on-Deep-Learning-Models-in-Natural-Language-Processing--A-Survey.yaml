- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:06:42'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1901.06796] Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1901.06796](https://ar5iv.labs.arxiv.org/html/1901.06796)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wei Emma Zhang [w.zhang@mq.edu.au](mailto:w.zhang@mq.edu.au) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID identifier") ,  Quan Z. Sheng [michael.sheng@mq.edu.au](mailto:michael.sheng@mq.edu.au)
    [1234-5678-9012](https://orcid.org/1234-5678-9012 "ORCID identifier") ,  Ahoud
    Alhazmi [ahoud.alhazmi@hdr.mq.edu.au](mailto:ahoud.alhazmi@hdr.mq.edu.au) Macquarie
    UniversitySydneyAustraliaNSW 2109  and  Chenliang Li Wuhan UniversityWuhanChina
    [cllee@whu.edu.cn](mailto:cllee@whu.edu.cn)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the development of high computational devices, deep neural networks (DNNs),
    in recent years, have gained significant popularity in many Artificial Intelligence
    (AI) applications. However, previous efforts have shown that DNNs were vulnerable
    to strategically modified samples, named adversarial examples. These samples are
    generated with some imperceptible perturbations, but can fool the DNNs to give
    false predictions. Inspired by the popularity of generating adversarial examples
    for image DNNs, research efforts on attacking DNNs for textual applications emerges
    in recent years. However, existing perturbation methods for images cannot be directly
    applied to texts as text data is discrete in nature. In this article, we review
    research works that address this difference and generate textual adversarial examples
    on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive
    way and cover all the related information to make the article self-contained.
    Finally, drawing on the reviewed literature, we provide further discussions and
    suggestions on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep neural networks, adversarial examples, textual data, natural language
    processing^†^†ccs: Computing methodologies Natural language processing^†^†ccs:
    Computing methodologies Neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep neural networks (DNNs) are large neural networks whose architecture is
    organized as a series of layers of neurons, each of which serves as the individual
    computing units. Neurons are connected by links with different weights and biases
    and transmit the results of its activation function on its inputs to the neurons
    of the next layer. Deep neural networks try to mimic the biological neural networks
    of human brains to learn and build knowledge from examples. Thus they are shown
    the strengths in dealing with complicated tasks that are not easily to be modelled
    as linear or non-linear problems. Further more, empowered by continuous real-valued
    vector representations (i.e., embeddings) they are good at handling data with
    various modalities, e.g., image, text, video and audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the development of high computational devices, deep neural networks, in
    recent years have gained significant popularity in many Artificial Intelligence
    (AI) communities such as Computer Vision (Krizhevsky et al., [2012](#bib.bib67);
    Simonyan and Zisserman, [2015](#bib.bib127)), Natural Language Processing (Kumar
    et al., [2016](#bib.bib68); Bowman et al., [2016](#bib.bib19)), Web Mining (Palangi
    et al., [2016](#bib.bib103); Yang et al., [2015](#bib.bib150)) and Game theory
    (Schuurmans and Zinkevich, [2016](#bib.bib120)). However, the interpretability
    of deep neural networks is still unsatisfactory as they work as black boxes, which
    means it is difficult to get intuitions from what each neuron exactly has learned.
    One of the problems of the poor interpretability is evaluating the robustness
    of deep neural networks. In recent years, research works (Szegedy et al., [2014](#bib.bib133);
    Goodfellow et al., [2015](#bib.bib43)) used small unperceivable perturbations
    to evaluate the robustness of deep neural networks and found that they are not
    robust to these perturbations. Szegedy et al. (Szegedy et al., [2014](#bib.bib133))
    first evaluated the state-of-the-art deep neural networks used for image classification
    with small generated perturbations on the input images. They found that the image
    classifier were fooled with high probability, but human judgment is not affected.
    The perturbed image pixels were named adversarial examples and this notation is
    later used to denote all kinds of perturbed samples in a general manner. As the
    generation of adversarial examples is costly and impractical in (Szegedy et al.,
    [2014](#bib.bib133)), Goodfellow et al. (Goodfellow et al., [2015](#bib.bib43))
    proposed a fast generation method which popularized this research topic (Section
    [3.1](#S3.SS1 "3.1\. Crafting Adversarial Examples: Inspiring Works in Computer
    Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey") provides further discussion on these
    works). Followed their works, many research efforts have been made and the purposes
    of these works can be summarized as: i) evaluating the deep neural networks by
    fooling them with unperceivable perturbations; ii) intentionally changing the
    output of the deep neural networks; and iii) detecting the oversensitivity and
    over-stability points of the deep neural networks and finding solutions to defense
    the attack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Jia and Liang (Jia and Liang, [2017](#bib.bib56)) are the first to consider
    adversarial example generation (or adversarial attack, we will use these two expressions
    interchangeably hereafter) on deep neural networks for text-based tasks (namely
    textual deep neural networks). Their work quickly gained research attention in
    Natural Language Processing (NLP) community. However, due to intrinsic differences
    between images and textual data, the adversarial attack methods on images cannot
    be directly applied to the latter one. First of all, image data (e.g., pixel values)
    is continuous, but textual data is discrete. Conventionally, we vectorize the
    texts before inputting them into the deep neural networks. Traditional vectoring
    methods include leveraging term frequency and inverse document frequency, and
    one-hot representation (details in Section [3.3](#S3.SS3 "3.3\. Vectorizing Textual
    Inputs and Perturbation Measurements ‣ 3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey")). When applying
    gradient-based adversarial attacks adopted from images on these representations,
    the generated adversarial examples are invalid characters or word sequences (Zhao
    et al., [2017](#bib.bib157)). One solution is to use word embeddings as the input
    of deep neural networks. However, this will also generate words that can not be
    matched with any words in the word embedding space (Gong et al., [2018](#bib.bib40)).
    Secondly, the perturbation of images are small change of pixel values that are
    hard to be perceived by human eyes, thus humans can correctly classify the images,
    showing the poor robustness of deep neural models. But for adversarial attack
    on texts, small perturbations are easily perceptible. For example, replacement
    of characters or words would generate invalid words or syntactically-incorrect
    sentences. Further, it would alter the semantics of the sentence drastically.
    Therefore, the perturbations are easily to be perceived–in this case, even human
    being cannot provide correct predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the aforementioned differences and challenges, many attacking methods
    are proposed since the pioneer work of Jia and Liang (Jia and Liang, [2017](#bib.bib56)).
    Despite the popularity of the topic in NLP community, there is no comprehensive
    review paper that collect and summarize the efforts in this research direction.
    There is a need for this kind of work that helps successive researchers and practitioners
    to have an overview of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Related surveys and the differences to this survey. In (Barreno et al., [2010](#bib.bib11)),
    the authors presented comprehensive review on different classes of attacks and
    defenses against machine learning systems. Specifically, they proposed a taxonomy
    for identifying and analyzing these attacks and applied the attacks on a machine
    learning based application, i.e., a statistical spam filter, to illustrate the
    effectiveness of the attack and defense. This work targeted machine learning algorithms
    rather than neural models. Inspired by (Barreno et al., [2010](#bib.bib11)), the
    authors in (Gilmer et al., [2018](#bib.bib37)) reviewed the defences of adversarial
    attack in the security point of view. The work is not limited to machine learning
    algorithms or neural models, but a generic report about adversarial defenses on
    security related applications. The authors found that existing security related
    defense works lack of clear motivations and explanations on how the attacks are
    related to the real security problems and how the attack and defense are meaningfully
    evaluated. Thus they established a taxonomy of motivations, constraints, and abilities
    for more plausible adversaries. (Biggio and Roli, [2018](#bib.bib16)) provides
    a thorough overview of the evolution of the adversarial attack research over the
    last ten years, and focuses on the research works from computer vision and cyber
    security. The paper covers the works from pioneering non-deep leaning algorithms
    to recent deep learning algorithms. It is also from the security point of view
    to provide detailed analysis on the effect of the attacks and defenses. The authors
    of (Liu et al., [2018](#bib.bib82)) reviewed the same problem in a data-driven
    perspective. They analyzed the attacks and defenses according to the learning
    phases, i.e., the training phase and test phase.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike previous works that discuss generally on the attack methods on machine
    learning algorithms, (Yuan et al., [2017](#bib.bib155)) focuses on the adversarial
    examples on deep learning models. It reviews current research efforts on attacking
    various deep neural networks in different applications. The defense methods are
    also extensively surveyed. However, they mainly discussed adversarial examples
    for image classification and object recognition tasks. The work in (Akhtar and
    Mian, [2018](#bib.bib3)) provides a comprehensive review on the adversarial attacks
    on deep learning models used in computer vision tasks. It is an application-driven
    survey that groups the attack methods according to the sub-tasks under computer
    vision area. The article also comprehensively reports the works on the defense
    side, the methods of which are mainly grouped into three categories.
  prefs: []
  type: TYPE_NORMAL
- en: All the mentioned works either target general overview of the attacks and defenses
    on machine learning models or focus on specific domains such as computer vision
    and cyber security. Our work differs with them that we specifically focus on the
    attacks and defenses on textual deep learning models. Furthermore, we provide
    a comprehensive review that covers information from different aspects to make
    this survey self-contained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Papers selection. The papers we reviewd in this article are high quality papers
    selected from top NLP and AI conferences, including ACL¹¹1Annual Meeting of the
    Association for Computational Linguistics, COLING²²2International Conference on
    Computational Linguistics, NAACL³³3Annual Conference of the North American Chapter
    of the Association for Computational Linguistics, EMNLP⁴⁴4Empirical Methods in
    Natural Language Processing, ICLR⁵⁵5International Conference on Learning Representations,
    AAAI⁶⁶6AAAI Conference on Artificial Intelligence and IJCAI⁷⁷7International Joint
    Conference on Artificial Intelligence. Other than accepted papers in aforementioned
    conferences, we also consider good papers in e-Print archive⁸⁸8arXiv.org, as it
    reflects the latest research outputs. We select papers from archive with three
    metrics: paper quality, method novelty and the number of citations (optional⁹⁹9As
    the research topic emerges from 2017, we relax the citation number to over five
    if it is published more than one year. If the paper has less than five citations,
    but is very recent and satisfies the other two metrics, we also include it in
    this paper.).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Contributions of this survey. The aim of this survey is to provide a comprehensive
    review on the research efforts on generating adversarial examples on textual deep
    neural networks. It is motivated by the drastically increasing attentions on this
    topic. This survey will serve researchers and practitioners who are interested
    in attacking textual deep neural models. More broadly, it can serve as a reference
    on how deep learning is applied in NLP community. We expect that the readers have
    some basic knowledge of the deep neural networks architectures, which are not
    the focus in this article. To summarize, the key contributions of this survey
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conduct a comprehensive review for adversarial attacks on textual deep neural
    models and propose different classification schemes to organize the reviewed literature;
    this is the first work of this kind;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide all related information to make the survey self-contained and thus
    it is easy for readers who have limited NLP knowledge to understand;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss some open issues, and identify some possible research directions
    in this research field aims to build more robust textual deep learning models
    with the help of adversarial examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of this paper is organized as follows: We introduce the preliminaries
    for adversarial attacks on deep learning models in Section [2](#S2 "2\. Overview
    of Adversarial Attacks and Deep Learning Techniques in Natural Language Processing
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey") including the taxonomy of adversarial attacks and deep learning models
    used in NLP. In Section [3](#S3 "3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey"), we address
    the difference of attacking image data and textual data and briefly reviewed exemplary
    works for attacking image DNN that inspired their follow-ups in NLP. Section [4](#S4
    "4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey") first presents
    our classification on the literature and then gives a detailed introduction to
    the state of the art. We discuss the defense strategies in Section [5](#S5 "5\.
    Defense ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey") and point out the open issues in Section [6](#S6 "6\. Discussions and
    Open Issues ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey"). Finally, the article is concluded in Section [7](#S7 "7\.
    Conclusion ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Overview of Adversarial Attacks and Deep Learning Techniques in Natural
    Language Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we dive into the details of this survey, we start with an introduction
    to the general taxonomy of adversarial attack on deep learning models. We also
    introduce the deep learning techniques and their applications in natural language
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: '2.1\. Adversarial Attacks on Deep Learning Models: The General Taxonomy'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we provide the definitions of adversarial attacks and introduce
    different aspects of the attacks, followed by the measurement of perturbations
    and the evaluation metrics of the effectiveness of the attacks in a general manner
    that applies to any data modality.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. Definitions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Neural Network (DNN). A deep neural network (we use DNN and deep learning
    model interchangeably hereafter) can be simply presented as a nonlinear function
    $f_{\theta}:\mathbf{X}\rightarrow\mathbf{Y}$, where $\mathbf{X}$ is the input
    features/attributes, $\mathbf{Y}$ is the output predictions that can be a discrete
    set of classes or a sequence of objects. $\mathbf{\theta}$ represents the DNN
    parameters and are learned via gradient-based back-propagation during the model
    training. Best parameters would be obtained by minimizing the the gap between
    the model’s prediction $f_{\mathbf{\theta}}(\mathbf{X})$ and the correct label
    $\mathbf{Y}$, where the gap is measured by loss function $J(f_{\mathbf{\theta}}(\mathbf{X}),\mathbf{Y})$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perturbations. Perturbations are intently created small noises that to be added
    to the original input data examples in test stage, aiming to fool the deep learning
    models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adversarial Examples. An adversarial example $\mathbf{x}^{\prime}$ is an example
    created via worst-case perturbation of the input to a deep learning model. An
    ideal DNN would still assign correct class $\mathbf{y}$ (in the case of classification
    task) to $\mathbf{x}^{\prime}$, while a victim DNN would have high confidence
    on wrong prediction of $\mathbf{x}^{\prime}$. $\mathbf{x}^{\prime}$ can be formalized
    as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (1) |  | $\displaystyle\mathbf{x}^{\prime}=\mathbf{x}+\mathbf{\eta},f(\mathbf{x})=\mathbf{y},\mathbf{x}\in\mathbf{X}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle f(\mathbf{x}^{\prime})\neq\mathbf{y}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle\textrm{or }f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime},\mathbf{y}^{\prime}\neq\mathbf{y}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $\eta$ is the worst-case perturbation. The goal of the adversarial attack
    can be deviating the label to incorrect one ($f(\mathbf{x}^{\prime})\neq\mathbf{y}$)
    or specified one ($f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime}$).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.1.2\. Threat Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We adopt the definition of Threat Model for attacking DNN from (Yuan et al.,
    [2017](#bib.bib155)). In this section, we discuss several aspects of the threat
    model.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Knowledge. The adversarial examples can be generated using black-box or
    white-box strategies in terms of the knowledge of the attacked DNN. Black-box
    attack is performed when the architectures, parameters, loss function, activation
    functions and training data of the DNN are not accessible. Adversarial examples
    are generated by directly accessing the test dataset, or by querying the DNN and
    checking the output change. On the contrary, white-box attack is based on the
    knowledge of certain aforementioned information of DNN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Target. The generated adversarial examples can change the output prediction
    to be incorrect or to specific result as shown in Eq. ([1](#S2.E1 "In 3rd item
    ‣ 2.1.1\. Definitions ‣ 2.1\. Adversarial Attacks on Deep Learning Models: The
    General Taxonomy ‣ 2\. Overview of Adversarial Attacks and Deep Learning Techniques
    in Natural Language Processing ‣ Adversarial Attacks on Deep Learning Models in
    Natural Language Processing: A Survey")). Compared to the un-targeted attack ($f(\mathbf{x}^{\prime})\neq\mathbf{y}$),
    targeted attack ($f(\mathbf{x}^{\prime})=\mathbf{y}^{\prime}$) is more strict
    as it not only changes the prediction, but also enforces constraint on the output
    to generate specified prediction. For binary tasks, e.g., binary classification,
    un-targeted attack equals to the targeted attack.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Granularity. The attack granularity refers to the level of data on which the
    adversarial examples are generated from. For example, it is usually the image
    pixels for image data. Regarding the textual data, it could be character, word,
    and sentence-level embedding. Section [3.3](#S3.SS3 "3.3\. Vectorizing Textual
    Inputs and Perturbation Measurements ‣ 3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey") will give further
    introduction on attack granularity for textual DNN.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Motivation. Generating adversarial examples is motivated by two goals: attack
    and defense. The attack aims to examine the robustness of the target DNN, while
    the defense takes a step further utilizing generated adversarial examples to robustify
    the target DNN. Section [5](#S5 "5\. Defense ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey") will give more details.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.1.3\. Measurement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Two groups of measurements are required in the adversarial attack for i) controlling
    the perturbations and ii) evaluating the effectiveness of the attack, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perturbation Constraint. As aforementioned, the perturbation $\eta$ should
    not change the true class label of the input - that is, an ideal DNN classifier,
    if we take classification as example, will provide the same prediction on the
    adversarial example to the original example. $\eta$ cannot be too small as well,
    to avoid ending up with no affect on target DNNs. Ideally, effective perturbation
    is the maximum value in a constrained range. (Szegedy et al., [2014](#bib.bib133))
    firstly put a constraint that $(\mathbf{x}+\mathbf{\eta})\in[0,1]^{n}$ for image
    adversarial examples, ensuring the adversarial example has the same range of pixel
    values as the original data (Warde-Farley and Goodfellow, [2016](#bib.bib144)).
    (Goodfellow et al., [2015](#bib.bib43)) simplifies the solution and use max norm
    to constrain $\mathbf{\eta}$: $||\mathbf{\eta}||_{\infty}\leq\epsilon$. This was
    inspired by the intuitive observation that a perturbation which does not change
    any specific pixel by more than some amount $\epsilon$ cannot change the output
    class (Warde-Farley and Goodfellow, [2016](#bib.bib144)). Using max-norm is sufficient
    enough for image classification/object recognition tasks. Later on, other norms,
    e.g., $L_{2}$ and $L_{0}$, were used to control the perturbation in attacking
    DNN in computer vision. Constraining $\mathbf{\eta}$ for textual adversarial attack
    is somehow different. Section [3.3](#S3.SS3 "3.3\. Vectorizing Textual Inputs
    and Perturbation Measurements ‣ 3\. From Image to Text ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey") will give more
    details.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attack Evaluation. Adversarial attacks are designed to degrade the performance
    of DNNs. Therefore, evaluating the effectiveness of the attack is based on the
    performance metrics of different tasks. For example, classification tasks has
    metrics such as accuracy, F1 score and AUC score. We leave the metrics for different
    NLP as out-of-scope content in this article and suggest readers refer to specific
    tasks for information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2\. Deep Learning in NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural networks have been gaining increasing popularity in NLP community in
    recent years and various DNN models have been adopted in different NLP tasks.
    Apart from the feed forward neural networks and Convolutional Neural Networks
    (CNN), Recurrent/Recursive Neural Networks (RNN) and their variants are the most
    common neural networks used in NLP, because of their natural ability of handling
    sequences. In recent years, two important breakthroughs in deep learning are brought
    into NLP. They are sequence-to-sequence learning (Sutskever et al., [2014](#bib.bib132))
    and attention modeling (Bahdanau et al., [2014](#bib.bib9)). Reinforcement learning
    and generative models are also gained much popularity (Young et al., [2018](#bib.bib153)).
    In this section, we will briefly overview the DNN architectures and techniques
    applied in NLP that are closely related to this survey. We suggest readers refer
    to detailed reviews of neural networks in NLP in (Otter et al., [2018](#bib.bib102);
    Young et al., [2018](#bib.bib153)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. Feed Forward Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Feed-forward network, in particular multi-layer perceptrons (MLP), is the simplest
    neural network. It has several forward layers and each node in a layer connects
    to each node in the following layer, making the network fully connected. MLP utilizes
    nonlinear activation function to distinguish data that is not linearly separable.
    MLP works with fixed-sized inputs and do not record the order of the elements.
    Thus it is mostly used in the tasks that can be formed as supervised learning
    problems. In NLP, it can be used in any application. The major drawback of feed
    forward networks in NLP is that it cannot handle well the text sequences in which
    the word order matters.
  prefs: []
  type: TYPE_NORMAL
- en: As the feed forward network is easy to implement, there are various implementations
    and no standard benchmark architecture worth examining. To evaluate the robustness
    of feed forward network in NLP, adversarial examples are often generated for specific
    architectures in real applications. For example, authors of (Grosse et al., [2016](#bib.bib46),
    [2017](#bib.bib47); Al-Dujaili et al., [2018a](#bib.bib4)) worked on the specified
    malware detection models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Convolutional Neural Network (CNN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolutional Neural Network contains convolutional layers and pooling (down-sampling)
    layers and final fully-connected layer. Activation functions are used to connect
    the down-sampled layer to the next convolutional layer or fully-connected layer.
    CNN allows arbitrarily-sized inputs. Convolutional layer uses convolution operation
    to extract meaningful local patterns of input. Pooling layer reduces the parameters
    and computation in the network and it allows the network to be deeper and less-overfitting.
    Overall, CNN identifies local predictors and combines them together to generate
    a fixed-sized vector for the inputs, which contains the most or important informative
    aspects for the application task. In addition, it is order-sensitive. Therefore,
    it excels in computer vision tasks and later is widely adopted in NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Yoon Kim (Kim, [2014](#bib.bib61)) adopted CNN for sentence classification.
    He used Word2Vec to represent words as input. Then the convolutional operation
    is limited to the direction of word sequence, rather than the word embeddings.
    Multiple filters in pooling layers deal with the variable length of sentences.
    The model demonstrated excellent performances on several benchmark datasets against
    multiple state-of-the-art works. This work became a benchmark work of adopting
    CNN in NLP applications. Zhang et al. (Zhang et al., [2015](#bib.bib156)) presented
    CNN for text classification at character level. They used one-hot representation
    in alphabet for each of the character. To control the generalization error of
    the proposed CNN, they additionally performed data augmentation by replacing words
    and phrases with their synonyms. These two representative textual CNNs are evaluated
    via adversarial examples in many applications (Liang et al., [2017](#bib.bib79);
    Gao et al., [2018](#bib.bib36); Ebrahimi et al., [2018](#bib.bib32); Belinkov
    and Bisk, [2018](#bib.bib14); Ebrahimi et al., [[n. d.]](#bib.bib31)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. Recurrent Neural Networks/ Recursive Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recurrent Neural Networks are neural models adapted from feed-forward neural
    networks for learning mappings between sequential inputs and outputs (Rumelhart
    et al., [1986](#bib.bib117)). RNNs allows data with arbitrary length and it introduces
    cycles in their computational graph to model efficiently the influence of time
    (Goodfellow et al., [2016](#bib.bib41)). The model does not suffer from statistical
    estimation problems stemming from data sparsity and thus leads to impressive performance
    in dealing with sequential data (Goldberg, [2017](#bib.bib38)). Recursive neural
    networks (Goller and Kuchler, [1996](#bib.bib39)) extends recurrent neural networks
    from sequences to tree, which respects the hierarchy of the language. In some
    situations, backwards dependencies exist, which is in need for the backward analysis.
    Bi-directional RNN thus was proposed for looking at sentences in both directions,
    forwards and backwards, using two parallel RNN networks, and combining their outputs.
    Bengio et al. (Bengio et al., [2003](#bib.bib15)) is one of the first to apply
    RNN in NLP. Specifically, they utilized RNN in language model, where the probability
    of a sequence of words is computed in an recurrent manner. The input to RNN is
    the feature vectors for all the preceding words, and the output is the conditional
    probability distribution over the output vocabulary. Since RNN is a natural choice
    to model various kinds of sequential data, it has been applied to many NLP tasks.
    Hence RNN has drawn great interest for adversarial attack (Papernot et al., [2016b](#bib.bib105)).
  prefs: []
  type: TYPE_NORMAL
- en: RNN has many variants, among which Long Short-Term Memory (LSTM) network (Hochreiter
    and Schmidhuber, [1997](#bib.bib52)) gains the most popularity. LSTM is a specific
    RNN that was designed to capture the long-term dependencies. In LSTM, the hidden
    state are computed through combination of three gates, i.e., input gate, forget
    gate and output gate, that control information flow drawing on the logistic function.
    LSTM networks have subsequently proved to be more effective than conventional
    RNNs (Graves et al., [2013](#bib.bib45)). GRUs is a simplified version of LSTM
    that it only consists two gates, thus it is more efficient in terms of training
    and prediction. Some popular LSTM variants are proposed to solve various NLP tasks
    (Hochreiter and Schmidhuber, [1997](#bib.bib52); Tai et al., [2015](#bib.bib134);
    Wang and Jiang, [2016b](#bib.bib142); Chen et al., [2017](#bib.bib24); Wu et al.,
    [2016](#bib.bib147); Rocktäschel et al., [2016](#bib.bib113); Chen et al., [2017](#bib.bib24)).
    These representative works received the interests of evaluation with adversarial
    examples recently (Gao et al., [2018](#bib.bib36); Sun et al., [2018](#bib.bib131);
    Sato et al., [2018](#bib.bib119); Papernot et al., [2016b](#bib.bib105); Iyyer
    et al., [2018](#bib.bib55); Jia and Liang, [2017](#bib.bib56); Zhao et al., [2017](#bib.bib157);
    Minervini and Riedel, [2018](#bib.bib93); Rocktäschel et al., [2016](#bib.bib113)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4\. Sequence-to-Sequence Learning (Seq2Seq) Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sequence-to-sequence learning (Seq2Seq) (Sutskever et al., [2014](#bib.bib132))
    is one of the important breakthroughs in deep learning and is now widely used
    for NLP applications. Seq2Seq model has the superior capacity to generate another
    sequence information for a given sequence information with an encoder-decoder
    architecture (Li Deng, [2018](#bib.bib78)). Usually, a Seq2Seq model consists
    of two recurrent neural networks: an encoder that processes the input and compresses
    it into a vector representation, a decoder that predicts the output. Latent Variable
    Hierarchical Recurrent Encoder-Decoder (VHRED) model (Serban et al., [2016](#bib.bib123))
    is a recently popular Seq2Seq model that generate sequences leveraging the complex
    dependencies between subsequences. (Cho et al., [2014](#bib.bib26)) is one of
    the first neural machine translation (NMT) model that adopt the Seq2Seq model.
    OpenNMT (Klein et al., [2017](#bib.bib65)), a Seq2Seq NMT model proposed recently,
    becomes one of the benchmark works in NMT. As they are adopted and applied widely,
    attack works also emerge (Niu and Bansal, [2018](#bib.bib100); Ebrahimi et al.,
    [2018](#bib.bib32); Cheng et al., [2018](#bib.bib25); Singh et al., [2018](#bib.bib128)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5\. Attention Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention mechanism (Bahdanau et al., [2015](#bib.bib10)) is another breakthrough
    in deep leaning. It was initially developed to overcome the difficulty of encoding
    a long sequence required in Seq2Seq models (Li Deng, [2018](#bib.bib78)). Attention
    allows the decoder to look back on the hidden states of the source sequence. The
    hidden states then provide a weighted average as additional input to the decoder.
    This mechanism pays attention on informative parts of the sequence. Rather than
    looking at the input sequence in vanilla attention models, self-attention (Vaswani
    et al., [2017](#bib.bib137)) in NLP is used to look at the surrounding words in
    a sequence to obtain more contextually sensitive word representations (Young et al.,
    [2018](#bib.bib153)). BiDAF (Seo et al., [2016](#bib.bib122)) is a bidirectional
    attention flow mechanism for machine comprehension and achieved outstanding performance
    when proposed. (Jia and Liang, [2017](#bib.bib56); Singh et al., [2018](#bib.bib128))
    evaluated the robustness of this model via adversarial examples and became the
    first few works using adversarial examples for attacking textual DNNs. Other attention-based
    DNNs (Costa-Jussà and Fonollosa, [2016](#bib.bib27); Parikh et al., [2016](#bib.bib109))
    also received adversarial attacks recently (Ebrahimi et al., [[n. d.]](#bib.bib31);
    Minervini and Riedel, [2018](#bib.bib93)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.6\. Reinforcement Learning Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reinforcement learning trains an agent by giving a reward after agents performing
    discrete actions. In NLP, reinforcement learning framework usually consist of
    an agent (a DNN), a policy (guiding action) and a reward. The agent picks an action
    (e.g., predicting next word in a sequence) based on a policy, then updates its
    internal state accordingly, until arriving the end of the sequence where a reward
    is calculated. Reinforcement learning requires proper handling of the action and
    the states, which may limit the expressive power and learning capacity of the
    models (Young et al., [2018](#bib.bib153)). But it gains much interests in task-oriented
    dialogue systems (Li et al., [2016](#bib.bib76)) as they share the fundamental
    principle as decision making processes. Limited works so far can be found to attack
    the reinforcement learning model in NLP (Niu and Bansal, [2018](#bib.bib100)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.7\. Deep Generative Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In recent years, two powerful deep generative models, Generative Adversarial
    Networks (GANs) (Goodfellow et al., [2014](#bib.bib42)) and Variational Auto-Encoders
    (VAEs) (Kingma and Welling, [2014](#bib.bib64)) are proposed and gain much research
    attention. Generative models are able to generate realistic data that are very
    similar to ground truth data in a latent space. In NLP field, they are used to
    generate textual data. GANs (Goodfellow et al., [2014](#bib.bib42)) consist of
    two adversarial networks: a generator and a discriminator. Discriminator is to
    discriminate the real and generated samples, while the generator is to generate
    realistic samples that aims to fool the discriminator. GAN uses a min-max loss
    function to train two neural networks simultaneously. VAEs consist of encoder
    and generator networks. Encoder encodes an input into a latent space and the generator
    generates samples from the latent space. Deep generative models is not easy to
    train and evaluate. Hence, these deficiencies hinder their wide usage in many
    real-world applications (Young et al., [2018](#bib.bib153)). Although they have
    been adopted in generating texts, so far no work examines their robustness using
    adversarial examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. From Image to Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adversarial attacks are originated from computer vision community. In this section,
    we introduce representative works, discuss differences between attacking image
    data and textual data, and present preliminary knowledge when performing adversarial
    attacks on textual DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '3.1\. Crafting Adversarial Examples: Inspiring Works in Computer Vision'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since adversarial examples are first proposed for attacking object recognition
    DNNs in computer vision community  (Szegedy et al., [2014](#bib.bib133); Goodfellow
    et al., [2015](#bib.bib43); Papernot et al., [2016a](#bib.bib107); Carlini and
    Wagner, [2017](#bib.bib21); Moosavi-Dezfooli et al., [2016](#bib.bib97); Papernot
    et al., [2017](#bib.bib106); Zhao et al., [2017](#bib.bib157)), this research
    direction has been receiving sustained attentions. We briefly introduce some works
    that inspired their followers in NLP community in this section, allowing the reader
    to better understand the adversarial attacks on textual DNNs. For comprehensive
    review of attack works in computer vision, please refer to (Akhtar and Mian, [2018](#bib.bib3)).
  prefs: []
  type: TYPE_NORMAL
- en: L-BFGS
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Szegedy et al. invented the adversarial examples notation (Szegedy et al.,
    [2014](#bib.bib133)). They proposed a explicitly designed method to cause the
    model to give wrong prediction of adversarial input ($\mathbf{x}+\mathbf{\eta}$)
    for image classification task. It came to solve the optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\displaystyle\mathbf{\eta}=\arg\min_{\mathbf{\eta}}\lambda&#124;&#124;\mathbf{\eta}&#124;&#124;^{2}_{2}+J(\mathbf{x}+\mathbf{\eta},y^{\prime})~{}~{}~{}s.t.~{}~{}(\mathbf{x}+\mathbf{\eta})\in[0,1],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $y^{\prime}$ is the target output of ($\mathbf{x}^{\prime}+\mathbf{\eta}$),
    but incorrect given an ideal classifier. $J$ denotes the cost function of the
    DNN and $\lambda$ is a hyperparameter to balance the two parts of the equation.
    This minimization was initially performed with a box-constrained Limited memory
    Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm and thus was named after it.
    The optimization was repeated multiple times until reaching a minimum $\lambda$
    that satisfy Eq. ([2](#S3.E2 "In L-BFGS ‣ 3.1\. Crafting Adversarial Examples:
    Inspiring Works in Computer Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: Fast Gradient Sign Method (FGSM)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'L-BFGS is very effective, but highly expensive - this inspired Goodfellow et
    al. (Goodfellow et al., [2015](#bib.bib43)) to find a simplified solution. Instead
    of fixing $y^{\prime}$ and minimizing $\mathbf{\eta}$ in L-BFGS, FGSM fixed size
    of $\mathbf{\eta}$ and maximized the cost (Eq. ([3](#S3.E3 "In Fast Gradient Sign
    Method (FGSM) ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer
    Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey"))). Then they linearized the problem
    with a first-order Taylor series approximation (Eq. ([4](#S3.E4 "In Fast Gradient
    Sign Method (FGSM) ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer
    Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey"))), and got the closed-form solution
    of $\mathbf{\eta}$ (Eq. ([5](#S3.E5 "In Fast Gradient Sign Method (FGSM) ‣ 3.1\.
    Crafting Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\. From Image
    to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey"))) (Warde-Farley and Goodfellow, [2016](#bib.bib144)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\displaystyle\mathbf{\eta}=\arg\max_{\mathbf{\eta}}J(\mathbf{x}+\mathbf{\eta},y)~{}~{}~{}s.t.~{}~{}&#124;&#124;\mathbf{\eta}&#124;&#124;_{\infty}\leq\epsilon,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (4) |  | $\displaystyle\mathbf{\eta}=\arg\max_{\mathbf{\eta}}J(\mathbf{x},y)+\mathbf{\eta}^{\mathbf{T}}\nabla_{\mathbf{x}}J(\mathbf{x},y)~{}~{}~{}s.t.~{}~{}&#124;&#124;\mathbf{\eta}&#124;&#124;_{\infty}\leq\epsilon,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (5) |  | $\displaystyle\mathbf{\eta}=\epsilon\cdot\text{sign}(\nabla_{\mathbf{x}}J(\mathbf{x},\mathbf{y}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon$ is a parameter set by attacker, controlling the perturbation’s
    magnitude. sign(x) is the sign function which returns 1 when $x>0$, and $-1$ when
    $x<0$, otherwise returns $0$. $\nabla_{\mathbf{x}}J(\mathbf{x},\mathbf{y})$ denotes
    the gradient of loss function respect to the input, and can be calculated via
    back-propagation. FGSM attracts the most follow-up works in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Jacobian Saliency Map Adversary (JSMA)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Unlike FGSM using gradients to attack, Papernot et al. (Papernot et al., [2016a](#bib.bib107))
    generated adversarial examples using forward derivatives (i.e., model Jacobian).
    This method evaluates the neural model’s output sensitivity to each input component
    using its Jacobian Matrix and gives greater control to adversaries given the perturbations.
    Jacobian matrices form the adversarial saliency maps that rank each input component’s
    contribution to the adversarial target. A perturbation is then selected from the
    maps. Thus the method was named Jacobian-based Saliency Map Attack. The Jacobian
    matrix of a given $\mathbf{x}$ is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\displaystyle Jacb_{F}[i,j]=\frac{\partial F_{i}}{\partial\mathbf{x}_{j}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{x}_{i}$ is the $i$-th component of the input and $F_{j}$ is the
    $j$-th component of the output. Here $F$ denotes the logits (i.e., the inputs
    to the softmax function) layer. $J_{F}[i,j]$ measures the sensitivity of $F_{j}$
    with respect to $\mathbf{x}_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: C&W Attack
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Carlini and Wagner (Carlini and Wagner, [2017](#bib.bib21)) aimed to evaluate
    the defensive distillation strategy (Hinton et al., [2015](#bib.bib51)) for mitigating
    the adversarial attacks. They restricted the perturbations with $l_{p}$ norms
    where $p$ equals to $0,2$ and $\infty$ and proposed seven versions of $J$ for
    the following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\displaystyle\mathbf{\eta}=\arg\min_{\mathbf{\eta}}&#124;&#124;\mathbf{\eta}&#124;&#124;_{p}+\lambda
    J(\mathbf{x}+\mathbf{\eta},y^{\prime})~{}~{}~{}s.t.~{}~{}(\mathbf{x}+\mathbf{\eta})\in[0,1],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and the formulation shares the same notation with aforementioned works.
  prefs: []
  type: TYPE_NORMAL
- en: DeepFool
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: DeepFool (Moosavi-Dezfooli et al., [2016](#bib.bib97)) is an iterative $L_{2}$-regularized
    algorithm. The authors first assumed the neural network is linear, thus they can
    separate the classes with a hyperplane. They simplified the problem and found
    optimal solution based on this assumption and constructed adversarial examples.
    To address the non-linearity fact of the neural network, they repeated the process
    until a true adversarial example is found.
  prefs: []
  type: TYPE_NORMAL
- en: Substitute Attack
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The above mentioned representative works are all white-box methods, which require
    the full knowledge of the neural model’s parameters and structures. However, in
    practice, it is not always possible for attackers to craft adversaries in white-box
    manner due to the limited access to the model. The limitation was addressed by
    Papernot et al. (Papernot et al., [2017](#bib.bib106)) and they introduced a black-box
    attack strategy: They trained a substitute model to approximate the decision boundaries
    of the target model with the labels obtained by querying the target model. Then
    they conducted white-box attack on this substitute and generate adversarial examples
    on the substitute. Specifically, they adopted FSGM and JSMA in generating adversarial
    examples for the substitute DNN.'
  prefs: []
  type: TYPE_NORMAL
- en: GAN-like Attack
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are another branch of black-box attack leverages the Generative Adversarial
    Neural (GAN) models. Zhao et al. (Zhao et al., [2017](#bib.bib157)) firstly trained
    a generative model, WGAN, on the training dataset $\mathbf{X}$. WGAN could generate
    data points that follows the same distribution with $\mathbf{X}$. Then they separately
    trained an inverter to map data sample $\mathbf{x}$ to $\mathbf{z}$ in the latent
    dense space by minimizing the reconstruction error. Instead of perturbing $\mathbf{x}$,
    they searched for adversaries $z*$ in the neighbour of $\mathbf{z}$ in the latent
    space. Then they mapped $\mathbf{z*}$ back to $\mathbf{x*}$ and check if $\mathbf{x*}$
    would change the prediction. They introduced two search algorithms: iterative
    stochastic search and hybrid shrinking search. The former one used expanding strategy
    that gradually expand the search space, while the later one used shrinking strategy
    that starts from a wide range and recursively tightens the upper bound of the
    search range.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Attacking Image DNNs vs Attacking Textual DNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To attack a textual DNN model, we cannot directly apply the approaches from
    the image DNN attackers as there are three main differences between them:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discrete vs Continuous Inputs. Image inputs are continuous, typically the methods
    use $L_{p}$ norm measures the distance between clean data point with the perturbed
    data point. However, textual data is symbolic, thus discrete. It is hard to define
    the perturbations on texts. Carefully designed variants or distance measurements
    for textual perturbations are required. Another choice is to firstly map the textual
    data to continuous data, then adopt the attack method from computer vision. We
    will give further discussion in Section [3.3](#S3.SS3 "3.3\. Vectorizing Textual
    Inputs and Perturbation Measurements ‣ 3\. From Image to Text ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perceivable vs Unperceivable. Small change of the image pixels usually can not
    be easily perceived by human beings, hence the adversarial examples will not change
    the human judgment, but only fool the DNN models. But small changes on texts,
    e.g., character or word change, will easily be perceived, rendering the possibility
    of attack failure. For example, the changes could be identified or corrected by
    spelling-check and grammar check before inputting into textual DNN models. Therefore,
    it is nontrivial to find unperceivalble textual adversaries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic vs Semantic-less. In the case of images, small changes usually do not
    change the semantics of the image as they are trivial and unperceivable. However,
    perturbation on texts would easily change the semantics of a word and a sentence,
    thus can be easily detected and heavily affect the model output. For example,
    deleting a negation word would change the sentiment of a sentence. But this is
    not the case in computer vision where perturbing individual pixels does not turn
    the image from a cat to another animal. Changing semantics of the input is against
    the goal of adversarial attack that keep the correct prediction unchanged while
    fooling an victim DNN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Due to these differences, current state-of-the-art textual DNN attackers either
    carefully adjust the methods from image DNN attackers by enforcing additional
    constraints, or propose novel methods using different techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Vectorizing Textual Inputs and Perturbation Measurements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Vectorizing Textual Input. DNN models require vectors as input, for image tasks,
    the normal way is to use the pixel value to form the vectors/matrices as DNN input.
    But for textual models, special operations are needed to transform the text into
    vectors. There are three main branches of methods: word-count based encoding,
    one-hot encoding and dense encoding (or feature embedding) and the later two are
    mostly used in DNN models of textual applications.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word-Count Based Encoding. Bag-of-words (BOW) method has the longest history
    in vectorizing text. In BOW model, an zero-encoded vector with length of the vocabulary
    size is initialized. Then the dimension in vector is replaced by the count of
    corresponding word’s appearance in the given sentence. Another word-count based
    encoding is to utilize the term frequency-inverse document frequency (TF-IDF)
    of a word (term), and the dimension in the vector is the TF-IDF value of the word.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One-hot Encoding. In one-hot encoding, a vector feature represents a token–a
    token could be a character (character-level model) or a word (word-level model).
    For character-level one-hot encoding, the representation can be formulated as
    (Ebrahimi et al., [2018](#bib.bib32)):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (8) |  | $\displaystyle\mathbf{x}=[(x_{11},...x_{1n});...(x_{m1},...x_{mn})]$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'where $\mathbf{x}$ be a text of $L$ characters, $x_{ij}\in\{0,1\}^{|A|}$ and
    $|A|$ is the alphabet (in some works, $|A|$ also include symbols). In Equation [8](#S3.E8
    "In 2nd item ‣ 3.3\. Vectorizing Textual Inputs and Perturbation Measurements
    ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning Models in Natural
    Language Processing: A Survey"), $m$ is the number of words, $n$ is the maximum
    number of characters for a word in sequence x. Thus each word has the same-fixed
    length of vector representation and the length is decided by the maximum number
    of characters of the words. For word-level one-hot encoding, following the above
    notations, the text $x$ can be represented as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (9) |  | $\displaystyle\mathbf{x}=[(x_{1},...,x_{m},x_{m+1}...x_{k})]$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $x_{ij}\in\{0,1\}^{|V|}$ and $|V|$ is the vocabulary, which contains all
    words in a corpus. $k$ is the maximum number of words allowed for a text, so that
    $[(x_{m+1}...x_{k})]$ is zero-paddings if $m+1<k$. One-hot encoding produces vectors
    with only 0 and 1 values, where 1 indicates the corresponding character/word appears
    in the sentence/paragraph, while 0 indicate it does not appear. Thus one-hot encoding
    usually generates sparse vectors/matrices. DNNs have proven to be very successful
    in learning values from the sparse representations as they can learn more dense
    distributed representations from the one-hot vectors during the training procedure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense Encoding. Comparing to one-hot encoding, dense encoding generates low
    dimensional and distributed representations for textual data. Word2Vec citenips/MikolovSCCD13
    uses continuous bag-of-words (CBOW) and skip-gram models to generate dense representation
    for words, i.e., word embeddings. It is based on the distributional assumption
    that words appearing within similar context possess similar meaning. Word embeddings,
    to some extend, alleviates the discreteness and data-sparsity problems for vectorizing
    textual data (Goldberg, [2017](#bib.bib38)). Extensions of word embeddings such
    as doc2vec and paragraph2vec (Le and Mikolov, [2014](#bib.bib71)) encode sentences/paragraphs
    to dense vectors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perturbation Measurement. As described in Section [2.1.3](#S2.SS1.SSS3 "2.1.3\.
    Measurement ‣ 2.1\. Adversarial Attacks on Deep Learning Models: The General Taxonomy
    ‣ 2\. Overview of Adversarial Attacks and Deep Learning Techniques in Natural
    Language Processing ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey"), there needs a way to measure the size of the perturbation,
    so that it can be controlled to ensure the ability of fooling the victim DNN while
    remain unperceivable. However, the measurement in textual perturbations is drastically
    different with the perturbations in image. Usually, the size of the perturbation
    is measured by the distance between clean data $\mathbf{x}$ and its adversarial
    example $\mathbf{x^{\prime}}$. But in texts, the distance measurement also need
    to consider the grammar correctness, syntax correctness and semantic-preservance.
    We here list the measurements used in the reviewed in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Norm-based measurement. Directly adopting norms such as $L_{p},p\in{0,1,2,\infty}$
    requires the input data are continuous. One solution is to use continuous and
    dense presentation (e.g., embedding) to represent the texts. But this usually
    results in invalid and incomprehensible texts, that need to involve other constrains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grammar and syntax related measurement. Ensuring the grammar or syntactic correctness
    makes the adversarial examples not easily perceived.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Grammar and syntax checker are used in some works to ensure the textual adversarial
    examples generated are valid.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Perplexity is usually used to measure the quality of a language model. In one
    reviewed literature (Minervini and Riedel, [2018](#bib.bib93)), the authors used
    perplexity to ensure the generated adversarial examples (sentences) are valid.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paraphrase is controlled and can be regarded as a type of adversarial example
    ([4.3.3](#S4.SS3.SSS3 "4.3.3\. Paraphrase-based Adversaries ‣ 4.3\. Black-box
    Attack ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey")). When perturbing,
    the validity of paraphrases is ensured in the generation process.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Semantic-preserving measurement. Measuring semantic similarity/distance is
    often performed on word vectors by adopting vectors’ similarity/distance measurements.
    Given two $n$-dimensional word vectors $\mathbf{p}=(p_{1},p_{2},...,p_{n})$ and
    $\mathbf{q}=(q_{1},q_{2},...,q_{n})$:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Euclidean Distance is a distance of two vectors in the Euclidean space:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (10) |  | $\displaystyle d(\mathbf{p},\mathbf{q})=\sqrt{(p_{1}-q_{1})^{2}+p_{2}-q_{2})^{2}+..(p_{n}-q_{n})^{2}}$
    |  |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosine Similarity computes cosine value of the angle between the two vectors:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (11) |  | $\displaystyle cos(\mathbf{p},\mathbf{q})=\frac{\sum_{i=1}^{n}p_{i}\times
    q_{i}}{\sqrt{\sum_{i=1}^{n}(p_{i})^{2}}\times\sqrt{\sum_{i=1}^{n}(q_{i})^{2}}}$
    |  |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edit-based measurement. Edit distance is a way of quantifying the minimum changes
    from one string to the other. Different definitions of edit distance use different
    sets of string operations (Li et al., [2019](#bib.bib75)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Levenshtein Distance uses insertion, removal and substitution operations.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Word Mover’s Distance (WMD) (Kusner et al., [2015](#bib.bib70)) is an edit
    distance operated on word embedding. It measures the minimum amount of distance
    that the embedded words of one document need to travel to reach the embedded words
    of the other document (Gong et al., [2018](#bib.bib40)). The minimization is formulated
    as:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (12) |  | $\displaystyle\min\sum^{n}_{i,j=1}\textbf{T}_{ij}&#124;&#124;\mathbf{e_{i}}-\mathbf{e_{j}}&#124;&#124;_{2}$
    |  |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle s.t.,\sum^{n}_{j=1}\textbf{T}_{ij}=d_{i},\forall i\in\{i,...,n\},\sum^{n}_{i=1}\textbf{T}_{ij}=d_{i}^{\prime},\forall
    j\in\{i,...,n\}$ |  |'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
- en: where $\mathbf{e_{i}}$ and $\mathbf{e_{j}}$ ared word embedding of word $i$
    and word $j$ respectively. $n$ is the number of words. $\textbf{T}\in\mathcal{R}^{n\times
    n}$ be a flow matrix, where $\textbf{T}_{ij}\leq 0$ denotes how much of word $i$
    in $\mathbf{d}$ travels to word $j$ in $\mathbf{d}^{\prime}$. $\mathbf{d}$ and
    $\mathbf{d}^{\prime}$ are normalized bag-of-words vectors of the source document
    and target document respectively.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of changes is a simple way to measure the edits and it is adopted in
    some reviewed literature.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaccard similarity coefficient is used for measuring similarity of finite sample
    sets utilising intersection and union of the sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (13) |  | $\displaystyle J(A,B)=\frac{&#124;A\cap B&#124;}{&#124;A\cup B&#124;}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: In texts, $A$, $B$ are two documents (or sentences). $|A\cap B|$ denotes the
    number of words appear in both documents, $|A\cup B|$ refers to the number of
    unique words in total.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4\. Attacking Neural Models in NLP: State-of-The-Art'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first introduce the categories of attack methods on textual
    deep learning models and then highlight the state-of-the-art research works, aiming
    to identify the most promising advances in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Categories of Attack Methods on Textual Deep Learning Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We categorize existing adversarial attack methods based on different criteria.
    Figure [1](#S4.F1 "Figure 1 ‣ 4.1\. Categories of Attack Methods on Textual Deep
    Learning Models ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial
    Attacks on Deep Learning Models in Natural Language Processing: A Survey") generalizes
    the categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0c646270239be613efa6be0d96f3b0f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Categories of Adversarial Attack Methods on Textual Deep Learning
    Models
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, five strategies are used to categorize the attack methods:
    i) By model access group refers to the knowledge of attacked model when the attack
    is performed. In the following section, we focus on the discussion using this
    categorization strategy. ii) By application group refers the methods via different
    NLP applications. More detailed discussion will be provided in Section [4.5](#S4.SS5
    "4.5\. Benchmark Datasets by Applications ‣ 4\. Attacking Neural Models in NLP:
    State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey"). iii) By target group refers to the goal of the attack
    is enforcing incorrect prediction or targeting specific results. iv) By granularity
    group considers on what granularity the model is attacked. v) We have discussed
    the attacked DNNs in Section [2.2](#S2.SS2 "2.2\. Deep Learning in NLP ‣ 2\. Overview
    of Adversarial Attacks and Deep Learning Techniques in Natural Language Processing
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey"). In following sections, we will continuously provide information about
    different categories that the methods belong to.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One important group of methods need to be noted is the cross-modal attacks,
    in which the attacked model consider the tasks dealing with multi-modal data,
    e.g., image and text data. They are not attacks for pure textual DNNs, hence we
    discuss this category of methods separately in Section [4.4](#S4.SS4 "4.4\. Multi-modal
    Attacks ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks
    on Deep Learning Models in Natural Language Processing: A Survey") in addition
    to white-box attacks in Section [4.2](#S4.SS2 "4.2\. White-Box Attack ‣ 4\. Attacking
    Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey") and black-box attacks in Section
    [4.3](#S4.SS3 "4.3\. Black-box Attack ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. White-Box Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In white-box attack, the attack requires the access to the model’s full information,
    including architecture, parameters, loss functions, activation functions, input
    and output data. White-box attacks typically approximate the worst-case attack
    for a particular model and input, incorporating a set of perturbations. This adversary
    strategy is often very effective. In this section, we group white-box attacks
    on textual DNNs into seven categories.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. FGSM-based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'FGSM is one of the first attack methods on images (Section [3.1](#S3.SS1.SSS0.Px2
    "Fast Gradient Sign Method (FGSM) ‣ 3.1\. Crafting Adversarial Examples: Inspiring
    Works in Computer Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep
    Learning Models in Natural Language Processing: A Survey")). It gains many follow-up
    works in attacking textual DNNs. TextFool (Liang et al., [2017](#bib.bib79)) uses
    the concept of FGSM to approximate the contribution of text items that possess
    significant contribution to the text classification task. Instead of using sign
    of the cost gradient in FGSM, this work considers the magnitude. The authors proposed
    three attacks: insertion, modification and removal. Specifically, they computed
    cost gradient $\Delta_{x}J(f,x,c^{\prime})$ of each training sample $x$, employing
    back propagation, where $f$ is the model function, $x$ is the original data sample,
    and $c^{\prime}$ is the target text class. Then they identified the characters
    that contain the dimensions with the highest gradient magnitude and named them
    hot characters. Phrases that contain enough hot characters and occur the most
    frequently are chosen as Hot Training Phrases (HTPs). In the insertion strategy,
    adversarial examples are crafted by inserting a few HTPs of the target class $c^{\prime}$
    nearby the phrases with significant contribution to the original class $c$. The
    authors further leveraged external sources like Wikipedia and forged fact to select
    the valid and believable sentences. In the modification Strategy, the authors
    identified Hot Sample Phrase (HSP) to the current classification using similar
    way of identifying HTPs. Then they replaced the characters in HTPs by common misspellings
    or characters visually similar. In the removal strategy, the inessential adjective
    or adverb in HSPs are removed. The three strategies and their combinations are
    evaluated on a CNN text classifier (Zhang et al., [2015](#bib.bib156)). However,
    these methods are performed manually, as mentioned by the authors.'
  prefs: []
  type: TYPE_NORMAL
- en: The work in (Samanta and Mehta, [2018](#bib.bib118)) adopted the same idea as
    TextFool, but it provides a removal-addition-replacement strategy that firstly
    tries to remove the adverb ($w_{i}$) which contributed the most to the text classification
    task (measured using loss gradient). If the output sentences in this step have
    incorrect grammar, the method will insert a word $p_{j}$ before $w_{i}$. $p_{j}$
    is selected from a candidate pool, in which the synonyms and typos and genre specific
    keywords (identified via term frequency) are candidate words. If the output cannot
    satisfy the highest cost gradient for all the $p_{j}$, then the method replaces
    $w_{i}$ with $p_{j}$. The authors showed that their method is more effective than
    TextFool. As the method ordered the words with their contribution ranking and
    crafted adversarial samples according to the order, it is a greedy method that
    always get the minimum manipulation until the output changes. To avoid being detected
    by the human eyes, the authors constrained the replaced/added words to not affect
    the grammar and POS of the original words.
  prefs: []
  type: TYPE_NORMAL
- en: In malware detection, an portable executable (PE) is represented by binary vector
    $\{x_{1},...,x_{m}\}$, $x_{i}\in\{0,1\}$ that using 1 and 0 to indicate the PE
    is present or not where $m$ is the number of PEs. Using PEs’ vectors as features,
    malware detection DNNs can identify the malicious software. It is not a typical
    textual application, but also targets discrete data, which share similar methods
    with textual applications. The authors of (Al-Dujaili et al., [2018a](#bib.bib4))
    investigated the methods to generate binary-encoded adversarial examples. To preserve
    the functionality of the adversarial examples, they incorporated four bounding
    methods to craft perturbations. The first two methods adopt FSGM^k (Kurakin et al.,
    [2017](#bib.bib69)), the multi-step variant of FGSM, restricting the perturbations
    in a binary domain by introducing deterministic rounding (dFGSM^k) and randomized
    rounding (rFGSM^k). These two bounding methods are similar to $L_{\infty}$-ball
    constraints on images (Goodfellow et al., [2015](#bib.bib43)). The third method
    multi-step Bit Gradient Ascent (BGA^k) sets the bit of the $j$-th feature if the
    corresponding partial derivative of the loss is greater than or equal to the loss
    gradient’s $L_{2}$-norm divided by $\sqrt{m}$. The fourth method multi-step Bit
    Coordinate Ascent (BCA^k) updates one bit in each step by considering the feature
    with the maximum corresponding partial derivative of the loss. These two last
    methods actually visit multiple feasible vertices. The work also proposed a adversarial
    learning framework aims to robustify the malware detection model.
  prefs: []
  type: TYPE_NORMAL
- en: (Rosenberg et al., [2017](#bib.bib115)) also attacks malware detection DNNs.
    The authors made perturbations on the embedding presentation of the binary sequences
    and reconstructed the perturbed examples to its binary representation. Particularly,
    they appended a uniformly random sequence of bytes (payload) to the original binary
    sequence. Then they embed the new binary to its embedding and performed FGSM only
    on the embedding of the payload. The perturbation is performed iteratively until
    the detector output incorrect prediction. Since the perturbation is only performed
    on payload, instead of the input, this method will preserve the functionality
    of the malware. Finally, they reconstructed adverse embedding to valid binary
    file by mapping the adversary embedding to its closest neighbour in the valid
    embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many works directly adopt FGSM for adversarial training, i.e., put it as regularizer
    when training the model. We will discuss some representatives in Section [5](#S5
    "5\. Defense ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. JSMA-based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'JSMA is another pioneer work on attacking neural models for image applications
    (refers to Section [3.1](#S3.SS1.SSS0.Px3 "Jacobian Saliency Map Adversary (JSMA)
    ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\.
    From Image to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey")). The work (Papernot et al., [2016b](#bib.bib105)) used
    forward derivative as JSMA to find the most contributable sequence towards the
    adversary direction. The network’s Jacobian had been calculated by leveraging
    computational graph unfolding (Mozer, [1995](#bib.bib98)). They crafted adversarial
    sequences for two types of RNN models whose output is categorical and sequential
    data respectively. For categorical RNN, the adversarial examples are generated
    by considering the Jacobian $Jacb_{F}[:,j]$ column corresponding to one of the
    output components $j$. Specifically, for each word $i$, they identified the direction
    of perturbation by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $\displaystyle\text{sign}(Jacb_{F}(x^{\prime})[i,g(x^{\prime})])$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (15) |  | $\displaystyle g(x^{\prime})=\arg\max_{0,1}(p_{j})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $p_{j}$ is the output probability of the target class. As in JSMA, they
    instead to choose logit to replace probability in this equation. They further
    projected the perturbed examples onto the closest vector in the embedding space
    to get valid embedding. For sequential RNN, after computing the Jacobian matrix,
    they altered the subset of input setps $\{i\}$ with high Jacobian values $Jacb_{F}[i,j]$
    and low Jacobian values $Jacb_{F}[i,k]$ for $k\neq j$ to achieve modification
    on a subset of output steps $\{j\}$.
  prefs: []
  type: TYPE_NORMAL
- en: '(Grosse et al., [2016](#bib.bib46)) (and (Grosse et al., [2017](#bib.bib47)))
    is the first work to attack neural malware detector. They firstly performed feature
    engineering and obtained more than 545K static features for software applications.
    They used binary indicator feature vector to represent an application. Then they
    crafted adversarial examples on the input feature vectors by adopting JSMA: they
    computed gradient of model Jacobian to estimate the perturbation direction. Later,
    the method chooses a perturbation $\eta$ given input sample that with maximal
    positive gradient into the target class. In particular, the perturbations are
    chosen via index $i$, satisfying:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | $\displaystyle i=\arg\max_{j\in[1,m],\mathbf{X}_{j}=y^{\prime}}f_{y}^{\prime}(\mathbf{X}_{j})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $y^{\prime}$ is the target class, $m$ is the number of features. On the
    binary feature vectors, the perturbations are ($0\rightarrow 1$) or ($1\rightarrow
    0$). This method preserves the functionality of the applications. In order to
    ensure that modifications caused by the perturbations do not change the application
    much, which will keep the malware application’s functionality complete, the authors
    used the $L_{1}$ norm to bound the overall number of features modified, and further
    bound the number of features to 20. In addition, the authors provided three methods
    to defense against the attacks, namely feature reduction, distillation and adversarial
    training. They found adversarial training is the most effective defense method.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3\. C&W-based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The work in (Sun et al., [2018](#bib.bib131)) adopted C&W method (refers to
    Section [3.1](#S3.SS1 "3.1\. Crafting Adversarial Examples: Inspiring Works in
    Computer Vision ‣ 3\. From Image to Text ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey")) for attacking predictive models
    of medical records. The aim is to detect susceptible events and measurements in
    each patient’s medical records, which provide guidance for the clinical usage.
    The authors used standard LSTM as predictive model. Given the patient EHR data
    being presented by a matrix $X^{i}\in\mathbf{R}^{d\times t_{i}}$ ($d$ is the number
    of medical features and $t_{i}$ is the time index of medical check), the generation
    of the adversarial example is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $\displaystyle\min_{\hat{X}}\max\{-\epsilon,[logit(\mathbf{x^{\prime}})]_{y}-[logit(\mathbf{x})]_{y^{\prime}}\}+\lambda&#124;&#124;\mathbf{x^{\prime}}-\mathbf{x}&#124;&#124;_{1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $logit(\cdot)$ denotes the logit layer output, $\lambda$ is the regularization
    parameter which controls the $L_{1}$ norm regularization, $y^{\prime}$ is the
    targeted label while $y$ is the original label. After generating adversarial examples,
    the authors picked the optimal example according to their proposed evaluation
    scheme that considers both the perturbation magnitude and the structure of the
    attacks. Finally they used the adversarial example to compute the susceptibility
    score for the EHR as well as the cumulative susceptibility score for different
    measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seq2Sick (Cheng et al., [2018](#bib.bib25)) attacked the seq2seq models using
    two targeted attacks: non-overlapping attack and keywords attack. For non-overlapping
    attack, the authors aimed to generate adversarial sequences that are entirely
    different from the original outputs. They proposed a hinge-like loss function
    that optimizes on the logit layer of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $\displaystyle\sum^{&#124;K&#124;}_{i=1}\min_{t\in[M]}\{m_{t}(\max\{-\epsilon,\max_{y\neq
    k_{i}}\{z_{t}^{(y)}\}-z_{t}^{(k_{i})}\})\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\{s_{t}\}$ are the original output sequence, $\{z_{t}\}$ indicates the
    logit layer outputs of the adversarial example. For the keyword attack, targeted
    keywords are expected to appear in the output sequence. The authors also put the
    optimization on the logit layer and tried to ensure that the targeted keyword’s
    logit be the largest among all words. Further more, they defined mask function
    $m$ to solve the keyword collision problem. The loss function then becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | $\displaystyle L_{keywords}=\sum^{&#124;K&#124;}_{i=1}\min_{t\in[M]}\{m_{t}(\max\{-\epsilon,\max_{y\neq
    k_{i}}\{z_{t}^{(y)}\}-z_{t}^{(k_{i})}\})\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $k_{i}$ denotes the $i$-th word in output vocabulary. To ensure the generated
    word embedding is valid, this work also considers two regularization methods:
    group lasso regularization to enforce the group sparsity, and group gradient regularization
    to make adversaries are in the permissible region of the embedding space.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4\. Direction-based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'HotFlip (Ebrahimi et al., [2018](#bib.bib32)) performs atomic flip operations
    to generate adversarial examples. Instead of leveraging gradient of loss, HotFlip
    use the directional derivatives. Specifically, HotFlip represents character-level
    operations, i.e., swap, insert and delete, as vectors in the input space and estimated
    the change in loss by directional derivatives with respect to these vectors. Specifically,
    given one-hot representation of inputs, a character flip in the $j$-th character
    of the $i$-th word (a$\rightarrow$b) can be represented by the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (20) |  | $\displaystyle\overrightarrow{v}_{ijb}=(\mathbf{0},..;(\mathbf{0},..(0,..-1,0,..,1,0)_{j},..\mathbf{0})_{i};\mathbf{0},..)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where -1 and 1 are in the corresponding positions for the a-th and b-th characters
    of the alphabet, respectively. Then the best character swap can be found by maximizing
    a first-order approximation of loss change via directional derivative along the
    operation vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (21) |  | $\displaystyle\max\nabla_{x}J(x,y)^{T}\cdot\overrightarrow{v}_{ijb}=\max_{ijv}\frac{\partial
    J^{(b)}}{\partial x_{ij}}-\frac{\partial J^{(a)}}{\partial x_{ij}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $J(x,y)$ is the model’s loss function with input $x$ and true output $y$.
    Similarly, insertion at the $j$-th position of the $i$-th word can also be treated
    as a character flip, followed by more flips as characters are shifted to the right
    until the end of the word. The character deletion is a number of character flips
    as characters are shifted to the left. Using the beam search, HotFlip efficiently
    finds the best directions for multiple flips.
  prefs: []
  type: TYPE_NORMAL
- en: The work (Ebrahimi et al., [[n. d.]](#bib.bib31)) extended HotFlip by adding
    targeted attacks. Besides the swap, insertion and deletion as provided in HotFlip,
    the authors proposed a controlled attack, which is to remove a specific word from
    the output, and a targeted attack, which is to replace a specific word by a chosen
    one. To achieve these attacks, they maximized the loss function $J(x,y_{t})$ and
    minimize $J(x,y_{t}^{\prime})$, where $t$ is the target word for the controlled
    attack, and $t^{\prime}$ is the word to replace $t$. Further, they proposed three
    types of attacks that provide multiple modifications. In one-hot attack, they
    manipulated all the words in the text with the best operation. In Greedy attack,
    they make another forward and backward pass, in addition to picking the best operation
    from the whole text. In Beam search attack, they replaced the search method in
    greedy with the beam search. In all the attacks proposed in this work, the authors
    set threshold for the maximum number of changes, e.g., 20% of characters are allowed
    to be changed.
  prefs: []
  type: TYPE_NORMAL
- en: '| Strategy | Work | Granularity | Target | Attacked Models | Perturb Ctrl.
    | App. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FSGM-based | (Liang et al., [2017](#bib.bib79)) | character,word | Y | CNN
    (Zhang et al., [2015](#bib.bib156)) | $L_{\infty}$ | TC |'
  prefs: []
  type: TYPE_TB
- en: '| (Samanta and Mehta, [2018](#bib.bib118)) | word | N | CNN (Zhang et al.,
    [2015](#bib.bib156)) | $L_{\infty}$, Grammar and POS correctness | TC |'
  prefs: []
  type: TYPE_TB
- en: '| (Rosenberg et al., [2017](#bib.bib115)) | PE | binary | CNN in (Dahl et al.,
    [2013](#bib.bib28)) | Boundaries employ $L_{\infty}$ and $L_{2}$ | MAD |'
  prefs: []
  type: TYPE_TB
- en: '| (Al-Dujaili et al., [2018a](#bib.bib4)) | PE embedding | binary | MalConv
    (Raff et al., [[n. d.]](#bib.bib111)) | $L_{\infty}$ | MAD |'
  prefs: []
  type: TYPE_TB
- en: '| JSMA-based | (Papernot et al., [2016b](#bib.bib105)) | word embedding | binary
    | LSTM | – | TC |'
  prefs: []
  type: TYPE_TB
- en: '| (Grosse et al., [2016](#bib.bib46), [2017](#bib.bib47)) | application features
    | binary | Feed forward | $L_{1}$ | MAD |'
  prefs: []
  type: TYPE_TB
- en: '| C&W-based | (Sun et al., [2018](#bib.bib131)) | medical features | Y | LSTM
    | $L_{1}$ | MSP |'
  prefs: []
  type: TYPE_TB
- en: '| (Cheng et al., [2018](#bib.bib25)) | word embedding | Y | OpenNMT-py (Klein
    et al., [2017](#bib.bib65)) | $L_{2}$+gradient regularization | TS, MT |'
  prefs: []
  type: TYPE_TB
- en: '| Direction-based | (Ebrahimi et al., [2018](#bib.bib32)) | character | N |
    CharCNN-LSTM (Kim et al., [2016](#bib.bib62)) | – | TC |'
  prefs: []
  type: TYPE_TB
- en: '| (Ebrahimi et al., [[n. d.]](#bib.bib31)) | character | Y | CharCNN-LSTM (Costa-Jussà
    and Fonollosa, [2016](#bib.bib27)) | Number of changes | MT |'
  prefs: []
  type: TYPE_TB
- en: '| Attention-based | (Blohm et al., [2018](#bib.bib17)) | word, sentence | N
    | (Wang and Jiang, [2016a](#bib.bib141); Liu et al., [2017b](#bib.bib83); Dzendzik
    et al., [2017](#bib.bib30)), CNN, LSTM and ensembles | Number of changes | MRC,
    QA |'
  prefs: []
  type: TYPE_TB
- en: '| Reprogramming | (Neekhara et al., [2018](#bib.bib99)) | word | N | CNN, LSTM,
    Bi-LSTM | – | TC |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid | (Gong et al., [2018](#bib.bib40)) | word embedding | N | CNN | WMD
    | TC, SA |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1\. Summary of reviewed white-box attack methods. PE: portable executable;
    TC: text classification; SA: sentiment analysis; TS: text summarisation; MT: machine
    translation MAD: malware detection; MSP: Medical Status Prediction; MRC: machine
    reading comprehension; QA: question answering; WMD: Word Mover’s Distance; –:
    not available.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5\. Attention-based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (Blohm et al., [2018](#bib.bib17)) proposed two white-box attacks for the purpose
    of comparing the robustness of CNN verses RNN. They leveraged the model’s internal
    attention distribution to find the pivotal sentence which is assigned a larger
    weight by the model to derive the correct answer. Then they exchanged the words
    which received the most attention with the randomly chosen words in a known vocabulary.
    They also performed another white-box attack by removing the whole sentence that
    gets the highest attention. Although they focused on attention-based models, their
    attacks do not examine the attention mechanism itself, but solely leverages the
    outputs of the attention component (i.e., attention score).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.6\. Reprogramming
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (Neekhara et al., [2018](#bib.bib99)) adopts adversarial reprogramming (AP)
    to attack sequence neural classifiers. AP (Elsayed et al., [2018](#bib.bib33))
    is a recently proposed adversarial attack where a adversarial reprogramming function
    $g_{\theta}$ is trained to re-purpose the attacked DNN to perform a alternate
    task (e.g., question classification to name classification) without modifying
    the DNN’s parameters. AP adopts idea from transfer learning, but keeps the parameters
    unchanged. The authors in (Neekhara et al., [2018](#bib.bib99)) proposed both
    white-box and black-box attacks. In white-box, Gumbel-Softmax is applied to train
    $g_{\theta}$ who can work on discrete data. We discuss the black-box method later.
    They evaluated their methods on various text classification tasks and confirmed
    the effectiveness of their methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.7\. Hybrid
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Authors of the work (Gong et al., [2018](#bib.bib40)) perturbed the input text
    on word embedding against the CNN model. This is a general method that is applicable
    to most of the attack methods developed for computer vision DNNs. The authors
    specifically applied FGSM and DeepFool. Directly applying methods from computer
    vision would generate meaningless adversarial examples. To address this issue,
    the authors rounded the adversarial examples to the nearest meaningful word vectors
    by using Word Mover’s Distance (WMD) as the distance measurements. The evaluations
    on sentiment analysis and text classification datasets show that WMD is a qualified
    metric for controlling the perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary of White-box Attack. We summarize the reviewed white-box attack works
    in Table [1](#S4.T1 "Table 1 ‣ 4.2.4\. Direction-based ‣ 4.2\. White-Box Attack
    ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey"). We highlight
    four aspects include granularity-on which level the attack is performed; target-whether
    the method is target or un-target; the attacked model, perturbation control-methods
    to control the size of the perturbation, and applications. It is worth noting
    that in binary classifications, target and untarget methods show same effect,
    so we point out their target as ”binary” in the table.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Black-box Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Black-box attack does not require the details of the neural networks, but can
    access the input and output. This type of attacks often rely on heuristics to
    generate adversarial examples, and it is more practical as in many real-world
    applications the details of the DNN is a black box to the attacker. In this article,
    we group black-box attacks on textual DNNs into five categories.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1\. Concatenation Adversaries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a0ff089ff55c79a77594c5075e698134.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Concatenation adversarial attack on reading comprehension DNN. After
    adding distracting sentences (in blue) the answer changes from correct one (green)
    to incorrect one (red) (Jia and Liang, [2017](#bib.bib56)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/655a4da17536c23a498c3c1c44f68eb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. General principle of concatenation adversaries. Correct output are
    often utilized to generate distorted output, which later will be used to build
    distracting contents. Appending distracting contents to the original paragraph
    as adversarial input to the attacked DNN and cause the attacked DNN produce incorrect
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '(Jia and Liang, [2017](#bib.bib56)) is the first work to attack reading comprehension
    systems. The authors proposed concatenation adversaries, which is to append distracting
    but meaningless sentences at the end of the paragraph. These distracting sentences
    do not change the semantics of the paragraph and the question answers, but will
    fool the neural model. The distracting sentences are either carefully-generated
    informative sentences or arbitrary sequence of words using a pool of 20 random
    common words. Both perturbations were obtained by iteratively querying the neural
    network until the output changes. Figure [2](#S4.F2 "Figure 2 ‣ 4.3.1\. Concatenation
    Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking Neural Models in NLP: State-of-The-Art
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey") illustrates an example from (Jia and Liang, [2017](#bib.bib56)) that
    after adding distracting sentences (in blue) the answer changes from correct one
    (green) to incorrect one (red). The authors of (Wang and Bansal, [2018](#bib.bib143))
    improved the work by varying the locations where the distracting sentences are
    placed and expanding the set of fake answers for generating the distracting sentences,
    rendering new adversarial examples that can help training more robust neural models.
    Also, the work (Blohm et al., [2018](#bib.bib17)) utilized the distracting sentences
    to evaluate the robustness of their reading comprehension model. Specifically,
    they use a pool of ten random common words in conjunction with all question words
    and the words from all incorrect answer candidates to generate the distracting
    sentences. In this work, a simple word-level black-box attack is also performed
    by replacing the most frequent words via their synonyms. As aforementioned, the
    authors also provided two white-box strategies. Figure [3](#S4.F3 "Figure 3 ‣
    4.3.1\. Concatenation Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking Neural
    Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey") illustrates the general workflow for
    concatenation attack. Correct output (i.e., answer in MRC tasks) are often leveraged
    to generate distorted output, which later will be used to build distracting contents.
    Appending distracting contents to the original paragraph as adversarial input
    to the attacked DNN. The distracting contents will not distract human being and
    ideal DNNs, but can make vulunerable DNNs to produce incorrect output.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2\. Edit Adversaries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/754b8594c63d487460b30bbb719c81fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Edit adversarial attack on sentiment analysis DNN. After editing
    words (red), the prediction changes from 100% of Negative to 89% of Positive (Li
    et al., [2019](#bib.bib75)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7918a3f33d7c7548d60f84bd2de0200f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. General principle of edit adversaries. Perturbations are performed
    on sentences, words or characters by edit strategies such as replace, delete,
    add and swap.
  prefs: []
  type: TYPE_NORMAL
- en: 'The work in (Belinkov and Bisk, [2018](#bib.bib14)) perturbed the input data
    of neural machine translation applications in two ways: Synthetic, which performed
    the character order changes, such as swap, middle random (i.e., randomly change
    orders of characters except the first and the last), fully random (i.e., randomly
    change orders of all characters) and keyboard type. They also collected typos
    and misspellings as adversaries. natural, leveraged the typos from the datasets.
    Furthermore, (Niu and Bansal, [2018](#bib.bib100)) attacked the neural models
    for dialogue generation. They applied various perturbations in dialogue context,
    namely Random Swap (randomly transposing neighboring tokens) and Stopword Dropout
    (randomly removing stopwords), Paraphrasing (replacing words with their paraphrases),
    Grammar Errors (e.g., changing a verb to the wrong tense) for the Should-Not-Change
    attacks, and the Add Negation strategy (negates the root verb of the source input)
    and Antonym strategy (changes verbs, adjectives, or adverbs to their antonyms)
    for Should-Change attacks. DeepWordBug (Gao et al., [2018](#bib.bib36)) is a simple
    method that uses character transformations to generate adversarial examples. The
    authors first identified the important ‘tokens’, i.e., words or characters that
    affect the model prediction the most by scoring functions developed by measuring
    the DNN classifier’s output. Then they modified the identified tokens using four
    strategies: replace, delete, add and swap. The authors evaluated their method
    on a variety of NLP tasks, e.g., text classification, sentiment analysis and spam
    detection. (Li et al., [2019](#bib.bib75)) followed (Gao et al., [2018](#bib.bib36)),
    refining the scoring function. Also this work provided white-box attack adopting
    JSMA. One contribution of this work lies on the perturbations are restricted using
    four textual similarity measurement: edit distance of text; Jaccard similarity
    coefficient; Euclidean distance on word vector; and cosine similarity on word
    embedding. Their method had been evaluated only on sentiment analysis task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors in (Minervini and Riedel, [2018](#bib.bib93)) proposed a method
    for automatically generating adversarial examples that violate a set of given
    First-Order Logic constraints in natural language inference (NLI). They proposed
    an inconsistency loss to measure the degree to which a set of sentences causes
    a model to violate a rule. The adversarial example generation is the process for
    finding the mapping between variables in rules to sentences that maximize the
    inconsistency loss and are composed by sentences with a low perplexity (defined
    by a language model). To generate low-perplexity adversarial sentence examples,
    they used three edit perturbations: i) change one word in one of the input sentences;
    i) remove one parse subtree from one of the input sentences; iii) insert one parse
    sub-tree from one sentence in the corpus in the parse tree of the another sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: The work in (Alzantot et al., [2018](#bib.bib6)) uses genetic algorithm (GA)
    for minimising the number of word replacement from the original text, but at the
    same time can change the result of the attacked model. They adopted crossover
    and mutation operations in GA to generate perturbations. The authors measured
    the effectiveness of the word replacement accoding to the impact on attacked DNNs.
    Their attack focused on sentiment analysis and textual entailment DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In (Chan et al., [2018](#bib.bib22)), the authors proposed a framerwork for
    adversarial attack on Differentiable Neural Computer (DNC). DNC is a computing
    machine with DNN as its central controller operating on an external memory module
    for data processing. Their method uses two new automated and scalable strategies
    to generate grammatically corret adversairal attacks in question answering domian,
    utilising metamorphic transformation. The first strategy, Pick-n-Plug, consists
    of a pick operator pick to draw adversarial sentences from a particular task (source
    task) and plug operator plug to inject these sentences into a story from another
    task (target task), without changing its correct answers. Another strategy, Pick-Permute-Plug,
    extends the adversarial capability of PPick-n-Plug by an additional permute operator
    after picking sentences (gpick) from a source task. Words in a particular adversarial
    sentence can be permuted with its synonyms to generate a wider range of possible
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0b4ad8e50675ff6dfd86e82f2841f34.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. General principle of paraphrase-based adversaries. Carefully designed
    (controlled) paraphrases are regarded as adversarial examples, which fool DNN
    to produce incorrect output.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. Paraphrase-based Adversaries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SCPNs (Iyyer et al., [2018](#bib.bib55)) produces a paraphrase of the given
    sentence with desired syntax by inputting the sentence and a targeted syntactic
    form into an encoder-decoder architecture. Specifically, the method first encodes
    the original sentence, then inputs the paraphrases generated by back-translation
    and the targeted syntactic tree into the decoder, whose output is the targeted
    paraphrase of the original sentence. One major contribution lies on the selection
    and processing of the parse templates. The authors trained a parse generator separately
    from SCPNs and selected 20 most frequent templates in PARANMT-50M. After generating
    paraphrases using the selected parse templates, they further pruned non-sensible
    sentences by checking n-gram overlap and paraphrastic similarity. The attacked
    classifier can correctly predict the label of the original sentence but fails
    on its paraphrase, which is regarded as the adversarial example. SCPNs had been
    evaluated on sentiment analysis and textual entailment DNNs and showed significant
    impact on the attacked models. Although this method use target strategy to generate
    adversarial examples, it does not specify targeted output. Therefore, we group
    it to untarget attack. Furthermore, the work in (Singh et al., [2018](#bib.bib128))
    used the idea of paraphrase generation techniques that create semantically equivalent
    adversaries (SEA). They generated paraphrases of a input sentence $x$, and got
    predictions from $f$ until the original prediction is changed with considering
    the semantically equivalent to $x^{\prime}$ that is $1$ if $x$ is semantically
    equivalent to $x^{\prime}$ and $0$ otherwise as shown in Eq.([22](#S4.E22 "In
    4.3.3\. Paraphrase-based Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking
    Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey")). After that, this work proposes
    semantic-equivalent rule based method for generalizing these generated adversaries
    into semantically equivalent rules in order to understand and fix the most impactful
    bug.'
  prefs: []
  type: TYPE_NORMAL
- en: '| (22) |  | $\displaystyle\mathbf{SEA(x,x^{\prime})}=\mathbf{1}[SemEq(x,x^{\prime})\wedge
    f(\mathbf{x})\neq f(\mathbf{x}^{\prime})]$ |  |'
  prefs: []
  type: TYPE_TB
- en: 4.3.4\. GAN-based Adversaries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some works proposed to leverage Generative Adversarial Network (GAN) (Goodfellow
    et al., [2014](#bib.bib42)) to generate adversaries (Zhao et al., [2017](#bib.bib157)).
    The purpose of adopting GAN is to make the adversarial examples mroe natural.
    In (Zhao et al., [2017](#bib.bib157)), the model proposed to generate adversarial
    exsamples consists of two key components: a GAN, which generate fake data samples,
    and an inverter that maps input $x$ to its latent representation $z^{\prime}$).
    The two components are trained on the original input by minimizing reconstruction
    error between original input and the adversarial examples. Perturbation is performed
    in the latent dense space by identifying the perturbed sample $\hat{z}$ in the
    neighborhood of $z^{\prime}$. Two search approaches, namely iterative stochastic
    search and hybrid shrinking search, are proposed to identify the proper $\hat{z}$.
    However, it requires querying the attacked model each time to find the $\hat{z}$
    that can make the model give incorrect prediction. Therefore, this method is quite
    time-consuming. The work is applicable to both image and textual data as it intrinsically
    eliminates the problem raised by the discrete attribute of textual data. The authors
    evaluated their method on three applications namely: textual entailment, machine
    translation and image classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5\. Substitution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The work in (Hu and Tan, [2017](#bib.bib54)) proposes a black-box framework
    that attacks RNN model for malware detection. The framework consists of two models:
    one is a generative RNN, the other is a substitute RNN. The generative RNN aims
    to generate adversarial API sequence from the malware’s API sequence. It is based
    on the seq2seq model proposed in (Sutskever et al., [2014](#bib.bib132)). It particularly
    generates a small piece of API sequence and inserts the sequence after the input
    sequence. The substitute RNN, which is a bi-directional RNN with attention mechanism,
    is to mimic the behavior of the attacked RNN. Therefore, generating adversarial
    examples will not query the original attacked RNN, but its substitution. The substitute
    RNN is trained on both malware and benign sequences, as well as the Gumbel-Softmax
    outputs of the generative RNN. Here, Gumbel-softmax is used to enable the joint
    training of the two RNN models, because the original output of the generative
    RNN is discrete. Specifically, it enables the gradient to be back-propagated from
    generative RNN to substitute RNN. This method performs attack on API, which is
    represented as a one-hot vector, i.e., given $M$ APIs, the vector for the $i$-th
    API is an M-dimensional binary vector that the $i$-th dimension is 1 while other
    dimensions are 0s.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Strategy | Work | Granularity | Target | Attacked Models | Perturb Ctrl.
    | App. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Concatenation | (Jia and Liang, [2017](#bib.bib56)) | word | N | BiDAF, Match-LSTM
    | – | MRC |'
  prefs: []
  type: TYPE_TB
- en: '| (Wang and Bansal, [2018](#bib.bib143)) | word, character | N | BiDAF+Self-Attn+ELMo
    (Peters et al., [2018](#bib.bib110)) | – | MRC |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Blohm et al., [2018](#bib.bib17)) | word, sentence | N | (Wang and Jiang,
    [2016a](#bib.bib141); Liu et al., [2017b](#bib.bib83); Dzendzik et al., [2017](#bib.bib30)),
    CNN, LSTM and ensembles | Number of changes | MRC, QA |'
  prefs: []
  type: TYPE_TB
- en: '| Edit | (Belinkov and Bisk, [2018](#bib.bib14)) | character, word | N | Nematus
    (Sennrich et al., [2017](#bib.bib121)), char2char (Lee et al., [2017](#bib.bib72)),
    charCNN (Kim et al., [2016](#bib.bib62)) | – | MT |'
  prefs: []
  type: TYPE_TB
- en: '| (Niu and Bansal, [2018](#bib.bib100)) | word, phrase | N | VHRED (Serban
    et al., [2017](#bib.bib124))+attn, RL in (Li et al., [2016](#bib.bib76)), DynoNet
    (He et al., [2017](#bib.bib50)) | – | DA |'
  prefs: []
  type: TYPE_TB
- en: '| (Gao et al., [2018](#bib.bib36)) | character, word | N | Word-level LSTM,
    Character-level CNN | – | SA, TC |'
  prefs: []
  type: TYPE_TB
- en: '| (Li et al., [2019](#bib.bib75)) | character, word | N | Word-level LSTM,
    Character-level CNN | EdDist, JSC, EuDistV, CSE | SA |'
  prefs: []
  type: TYPE_TB
- en: '| (Minervini and Riedel, [2018](#bib.bib93)) | word, phrase | N | cBiLSTM,
    DAM, ESIM | Perplexity | NLI |'
  prefs: []
  type: TYPE_TB
- en: '| (Alzantot et al., [2018](#bib.bib6)) | word | N | LSTM | EuDistV | SA, TE
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Chan et al., [2018](#bib.bib22)) | word, sentence | N | DNC | – | QA |'
  prefs: []
  type: TYPE_TB
- en: '| Paraphrase-based | (Iyyer et al., [2018](#bib.bib55)) | word | N | LSTM |
    Syntax-ctrl paraphrase | SA and TE |'
  prefs: []
  type: TYPE_TB
- en: '| (Singh et al., [2018](#bib.bib128)) | word | N | BiDAF, Visual7W (Zhu et al.,
    [2016](#bib.bib158)), fastText (Grave et al., [2017](#bib.bib44)) | Self-defined
    semantic-equivalency | MRC, SA, VQA |'
  prefs: []
  type: TYPE_TB
- en: '| GAN-based | (Zhao et al., [2017](#bib.bib157)) | word | N | LSTM, TreeLSTM,
    Google Translate (En-to-Ge) | GAN-constraints | TE, MT |'
  prefs: []
  type: TYPE_TB
- en: '| Substitution | (Hu and Tan, [2017](#bib.bib54)) | API | N | LSTM, BiLSTM
    and variants | – | MD |'
  prefs: []
  type: TYPE_TB
- en: '| Reprogramming | (Neekhara et al., [2018](#bib.bib99)) | word | N | CNN, LSTM,
    Bi-LSTM | – | TC |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2\. Summary of reviewed black-box attack methods. MRC: machine reading
    comprehension; QA: question answering; VQA: visual question answering; DA: dialogue
    generation; TC: text classification; MT: machine translation; SA: sentiment analysis;
    NLI: natural language inference; TE: textual entailment; MD: malware detection.
    EdDist: edit distance of text, JSC: Jaccard similarity coeffcient, EuDistV: Euclidean
    distance on word vector, CSE: cosine similarity on word embedding. ’-’: not available.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.6\. Reprogramming
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As aforementioned, (Neekhara et al., [2018](#bib.bib99)) provides both white-box
    and black-box attacks. We describe black-box attack here. In black-box attack,
    the authors fomulated the sequence generation as a reinforcement learning problem,
    and the adversarial reprogramming function $g_{\theta}$ is the policy network.
    Then they applied REINFORCE-based optimisation to train $g_{\theta}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary of Black-box Attack. We summarise the reviewed black-box attack works
    in Table [2](#S4.T2 "Table 2 ‣ 4.3.5\. Substitution ‣ 4.3\. Black-box Attack ‣
    4\. Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on
    Deep Learning Models in Natural Language Processing: A Survey"). We highlight
    four aspects include granularity-on which level the attack is performed; target-whether
    the method is target or un-target; the attacked model, perturbation control, and
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Multi-modal Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some works attack DNNs that are dealing with cross-modal data. For example,
    the neural models contain an internal component that performs image-to-text or
    speech-to-text conversion. Although these attacks are not for pure textual data,
    we briefly introduce the representative ones for the purpose of a comprehensive
    review.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1\. Image-to-Text
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image-to-text models is a class of techniques that generate textual description
    for an image based on the semantic content of the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optical Character Recognition (OCR). Recognizing characters from images is
    a problem named Optical Character Recognition (OCR). OCR is a multimodal learning
    task that takes an image as input and output the recognized text. Authors in (Song
    and Shmatikov, [2018](#bib.bib130)) proposed a white-box attack on OCR and follow-up
    NLP applications. They firstly used the original text to render a clean image
    (conversion DNNs). Then they found words in the text that have antonyms in WordNet
    and satisfy edit distance threshold. Only the antonyms that are valid and keep
    semantic inconsistencies will be kept. Later, the method locates the lines in
    the clean image containing the aforementioned words, which can be replaced by
    their selected antonyms. The method then transforms the target word to target
    sequence. Given the input/target images and sequences, the authors formed the
    generating of adversarial example is an optimisation problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $\displaystyle\min_{\omega}c\cdot J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})+&#124;&#124;\mathbf{x}-\mathbf{x}^{\prime}&#124;&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (24) |  | $\displaystyle\mathbf{x^{\prime}}=(\alpha\cdot\tanh({\omega})+\beta)/2$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\alpha=(\mathbf{x}_{max}-\mathbf{x}_{min})/2,\beta=(\mathbf{x}_{max}+\mathbf{x}_{min})/2$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (25) |  | $\displaystyle J_{CTC}(f(\mathbf{x},t))=-\log p(t&#124;\mathbf{x})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $f(\mathbf{x})$ is the neural system model, $J_{CTC}(\cdot)$ is the Connectionist
    Temporal Classification (CTC) loss function, $\mathbf{x}$ is the input image,
    $t$ is the ground truth sequence, $\mathbf{x}^{\prime}$ is the adversarial example,
    $t^{\prime}$ is the target sequence, $\omega,\alpha,\beta$ are parameters controlling
    adversarial examples to satisfy the box-constraint of $\mathbf{x}^{\prime}\in[\mathbf{x}_{min},\mathbf{x}_{max}]^{p}$,
    where $p$ is the number of pixels ensuring valid $x^{\prime}$. After generating
    adversarial examples, the method replaces the images of the corresponding lines
    in the text image. The authors evaluated this method in three aspects: single
    word recognition, whole document recognition, and NLP applications which based
    on the recognised text (sentiment analysis and document categorisation specifically).
    They also addressed that the proposed method suffers from limitatios such as low
    transferability across data and models, and physical unrelalizability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scene Text Recognition (STR). STR is also an image-to-text application. In
    STR, the entire image is mapped to word strings directly. In contrast, the recognition
    in OCR is a pipeline process: first segments the words to characters, then performs
    the recognition on single characters. AdaptiveAttack (Yuan et al., [2018](#bib.bib154))
    evaluated the possibility of performing adversarial attack for scene text recognition.
    The authors proposed two attacks, namely basic attack and adaptive attack. Basic
    attack is similar to the work in (Song and Shmatikov, [2018](#bib.bib130)) and
    it also formulates the adversarial example generation as an optimisation problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (26) |  | $\displaystyle\min_{\omega}J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})+\lambda\mathcal{D}(\mathbf{x},\mathbf{x}^{\prime})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (27) |  | $\displaystyle\mathbf{x^{\prime}}=\tanh({\omega})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{D}(\cdot)$ is Euclidean distance. The differences to (Song
    and Shmatikov, [2018](#bib.bib130)) lie on the definition of $\mathbf{x^{\prime}}$
    (Eq. ([24](#S4.E24 "In 4.4.1\. Image-to-Text ‣ 4.4\. Multi-modal Attacks ‣ 4\.
    Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep
    Learning Models in Natural Language Processing: A Survey")) vs Eq. ([27](#S4.E27
    "In 4.4.1\. Image-to-Text ‣ 4.4\. Multi-modal Attacks ‣ 4\. Attacking Neural Models
    in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models in Natural
    Language Processing: A Survey"))), and the distance measurement between $\mathbf{x}$,
    $\mathbf{x^{\prime}}$ ($L_{2}$ norm vs Euclidean distance), and the parameter
    $\lambda$, which balances the importance of being adversarial example and close
    to the original image. As searching for proper $\lambda$ is quite time-consuming,
    the authors proposed another method to adaptively find $\lambda$. They named this
    method Adaptive Attack, in which they defined the likelihood of a sequential classification
    task following a Gaussian distribution and derived the adaptive optimization for
    sequential adversarial examples as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (28) |  | $\displaystyle\min\frac{&#124;&#124;\mathbf{x}-\mathbf{x}^{\prime}&#124;&#124;_{2}^{2}}{\lambda_{1}^{2}}+\frac{J_{CTC}f(\mathbf{x^{\prime}},t^{\prime})}{\lambda_{2}^{2}}+\log\lambda_{1}^{2}+T\log\lambda_{2}^{2}+\frac{1}{\lambda_{2}^{2}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{1}$ and $\lambda_{2}$ are two parameters to balance perturbation
    and CTC loss, $T$ is the number of valid paths given targeted sequential output.
    Adaptive Attack can be applied to generate adversarial examples on both non-sequential
    and sequential classification problems. Here we only highlight the equation for
    sequential data. The authors evaluated their proposed methods on tasks that targeting
    the text insertion, deletion and substitution in output. The results demonstrated
    that Adaptive Attack is much faster than basic attack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image Captioning. Image captioning is another multimodal learning task that
    takes an image as input and generates a textual caption describing its visual
    contents. Show-and-Fool (Chen et al., [2018](#bib.bib23)) generates adversarial
    examples to attack the CNN-RNN based image captioning model. The CNN-RNN model
    attacked uses a CNN as encoder for image feature extraction and a RNN as decoder
    for caption generation. Show-and-Fool has two attack strategies: targeted caption
    (i.e., the generated caption matches the target caption) and targeted keywords
    (i.e., the generated caption contains the targeted keywords). In general, they
    formulated the two tasks using the following formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (29) |  | $\displaystyle\min_{\omega}c\cdot J(\mathbf{x^{\prime}})+&#124;&#124;\mathbf{x^{\prime}}-\mathbf{x}&#124;&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathbf{x^{\prime}}=\mathbf{x}+\eta$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle x=\tanh{(y),~{}~{}\mathbf{x^{\prime}}=\tanh{(\omega+y)}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $c>0$ is a pre-specified regularization constant, $\eta$ is the perturbation,
    $\omega,y$ are parameters controlling $\mathbf{x^{\prime}}\in[-1,1]$. The difference
    between these two strategies is the definition of the loss function $J(\cdot)$.
    For targeted caption strategy, provided the targeted caption as $S=(S_{1},S_{2},...S_{t},...S_{N})$,
    where $S_{t}$ refers to the index of the $t$-th word in the vocabulary and $N$
    is the length of the caption, the loss is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (30) |  | $\displaystyle J_{S,logit}(\mathbf{x^{\prime}})=\sum_{t=2}^{N-1}\max\{-\epsilon,\max_{k\neq
    S_{t}}\{z_{t}^{(k)}\}-z_{t}^{(S_{t})}\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $S_{t}$ is the target word, $z_{t}^{(S_{t})}$ is the logit of the target
    word. In fact, this method mininises the difference between the maximumn logit
    except $S_{t}$, and the logit of $S_{t}$. For the targeted keywords strategy,
    given the targeted keywords $\mathcal{K}:={K_{1},...,K_{M}}$, the loss function
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (31) |  | $\displaystyle J_{K,logit}(\mathbf{x^{\prime}})=\sum_{j=1}^{M}\min_{t\in[N]}\{\max\{-\epsilon,\max_{k\neq
    K_{j}}\{z_{t}^{(k)}\}-z_{t}^{(K_{j})}\}\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: The authors performed extensive experiments on Show-and-Tell (Vinyals et al.,
    [2015](#bib.bib138)) and varied the parameters in the attacking loss. They found
    that Show-and-Fool is not only effective on attacking Show-and-Tell, the CNN-RNN
    based image captioning model, but is also highly transferable to another model
    Show-Attend-and-Tell (Xu et al., [2015](#bib.bib148)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual Question Answering (VQA). Given an image and a natural language question
    about the image, VQA is to provide an accurate answer in natural language. The
    work in (Xu et al., [2018](#bib.bib149)) proposed a iterative optimisation method
    to attack two VQA models. The objective function proposed maximises the probability
    of the target answer and unweights the preference of adversarial examples with
    smaller distance to the original image when this distance is below a threshold.
    Specifically, the objective contains three components. The first one is similar
    to Eq. ([26](#S4.E26 "In 4.4.1\. Image-to-Text ‣ 4.4\. Multi-modal Attacks ‣ 4\.
    Attacking Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep
    Learning Models in Natural Language Processing: A Survey")), that replaces the
    loss function to the loss of the VQA model and using $||\mathbf{x}-\mathbf{x^{\prime}}||_{2}/\sqrt{N}$
    as distance between $\mathbf{x^{\prime}}$ and $\mathbf{x}$. The second component
    maximises the difference between the softmax output and the prediction when it
    is different with the target answer. The third component ensures the distance
    between $\mathbf{x^{\prime}}$ and $\mathbf{x}$ is under a lower bound. The attacks
    are evaluated by checking whether better success rate is obtained over the previous
    attacks, and the confidence score of the model to predict the target answer. Based
    on the evaluations, the authors concluded that that attention, bounding box localization
    and compositional internal structures are vulnerable to adversarial attacks. This
    work also attacked a image captioning neural model. We refer the original paper
    for further information.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multi-modal | Application | Work | Target | Access | Attacked Models | Perturb
    Ctrl. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Image-to-Text | Optical Character Recognition | (Song and Shmatikov, [2018](#bib.bib130))
    | Y | white-box | Tesseract (Tesseract, [2016](#bib.bib136)) | $L_{2}$, EdDist
    |'
  prefs: []
  type: TYPE_TB
- en: '| Scene Text Recognition | (Yuan et al., [2018](#bib.bib154)) | Y | white-box
    | CRNN (Shi et al., [2017](#bib.bib125)) | $L_{2}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Image Captioning | (Chen et al., [2018](#bib.bib23)) | Y | white-box | Show-and-Tell
    (Vinyals et al., [2015](#bib.bib138)) | $L_{2}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Visual Question Answering | (Xu et al., [2018](#bib.bib149)) | Y | white-box
    | MCB (Fukui et al., [2016](#bib.bib35)), N2NMN (Hu et al., [2017](#bib.bib53))
    | $L_{2}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Visual-Semantic Embeddings | (Shi et al., [2018](#bib.bib126)) | N | black-box
    | VSE++ (Faghri et al., [2017](#bib.bib34)) | – |'
  prefs: []
  type: TYPE_TB
- en: '| Speech-to-Text | Speech Recognition | (Carlini and Wagner, [[n. d.]](#bib.bib20))
    | Y | white-box | DeepSpeech (Hannun et al., [2014](#bib.bib49)) | $L_{2}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3\. Summary of reviewed cross-modal attacks. EdDist: edit distance of
    text, -: not available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual-Semantic Embeddings (VSE). The aim of VSE is to bridge natural language
    and the underlying visual world. In VSE, the embedding spaces of both images and
    descriptive texts (captions) are jointly optimized and aligned. (Shi et al., [2018](#bib.bib126))
    attacked the latest VSE model by generating adversarial examples in the test set
    and evaluated the robustness of the VSE modesls. They performed the attack on
    textual part by introducing three method: i) replace nouns in the image captions
    utilizing the hypernymy/hyponymy relations in WordNet; ii) change the numerals
    to different ones and singularize or pluralize the corresponding nouns when necessary;
    iii) detect the relations and shuffle the non-interchangeable noun phrases or
    replace the prepositions. This method can be considered as a black-box edit adversary
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2\. Speech-to-Text
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Speech-to-text is also known as speech recognition. The task is to recognize
    and translate the spoken language into text automatically. (Carlini and Wagner,
    [[n. d.]](#bib.bib20)) attacked a state-of-the-art speech-to-text transcription
    neural network (based on LSTM), named DeepSpeech. Given a natural waveform, the
    authors constructed a audio perturbation that is almost inaudible but can be recognized
    by adding into the original waveform. The perturbation is constructed by adopting
    the idea from C&W method (refers to section [3.1](#S3.SS1.SSS0.Px4 "C&W Attack
    ‣ 3.1\. Crafting Adversarial Examples: Inspiring Works in Computer Vision ‣ 3\.
    From Image to Text ‣ Adversarial Attacks on Deep Learning Models in Natural Language
    Processing: A Survey") ), which measures the image distortion by the maximum amount
    of changed pixels. Adapting this idea, they measured the audio distortion by calculating
    relative loudness of an audio and proposed to use Connectionist Temporal Classification
    loss for the optimization task. Then they solved this task with Adam optimizer
    (Kingma and Ba, [2015](#bib.bib63)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Benchmark Datasets by Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In recent years, neural networks gain success in different NLP domains and
    the popular applications include text classification, reading comprehension, machine
    translation, text summarization, question answering, dialogue generation, to name
    a few. In this section, we review the current works on generating adversarial
    examples on the neural networks in the perspective of NLP applications. Table
    [4](#S4.T4 "Table 4 ‣ 4.5\. Benchmark Datasets by Applications ‣ 4\. Attacking
    Neural Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning
    Models in Natural Language Processing: A Survey") summarizes the works we reviewed
    in this article according to their application domain. We further list the benchmark
    datasets used in these works in the table as auxiliary information– thus we refer
    readers to the links/references we collect for the detailed descriptions of the
    datasets. Note that the auxiliary datasets which help to generate adversarial
    examples are not included. Instead, we only present the dataset used to evaluate
    the attacked neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Applications | Representative Works | Benchmark Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Classification | Text Classification | (Liang et al., [2017](#bib.bib79);
    Gong et al., [2018](#bib.bib40); Gao et al., [2018](#bib.bib36); Ebrahimi et al.,
    [2018](#bib.bib32); Sato et al., [2018](#bib.bib119); Neekhara et al., [2018](#bib.bib99))
    | DBpedia, Reuters Newswires, AG’s news, Sogou News, Yahoo! Answers, RCV1, Surname
    Classification Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment Analysis | (Papernot et al., [2016b](#bib.bib105); Samanta and
    Mehta, [2018](#bib.bib118); Gao et al., [2018](#bib.bib36); Ebrahimi et al., [2018](#bib.bib32);
    Sato et al., [2018](#bib.bib119); Neekhara et al., [2018](#bib.bib99); Iyyer et al.,
    [2018](#bib.bib55); Singh et al., [2018](#bib.bib128)) | SST, IMDB Review, Yelp
    Review, Elec, Rotten Tomatoes Review, Amazon Review, Arabic Tweets Sentiment |'
  prefs: []
  type: TYPE_TB
- en: '| Spam Detection | (Gao et al., [2018](#bib.bib36)) | Enron Spam, Datasets
    from (Zhang et al., [2015](#bib.bib156)) |'
  prefs: []
  type: TYPE_TB
- en: '| Gender Identification | (Samanta and Mehta, [2018](#bib.bib118)) | Twitter
    Gender |'
  prefs: []
  type: TYPE_TB
- en: '| Grammar Error Detection | (Sato et al., [2018](#bib.bib119)) | FCE-public
    |'
  prefs: []
  type: TYPE_TB
- en: '| Medical Status Prediction | (Sun et al., [2018](#bib.bib131)) | Electronic
    Health Records (EHR) |'
  prefs: []
  type: TYPE_TB
- en: '| Malware Detection | (Grosse et al., [2016](#bib.bib46), [2017](#bib.bib47);
    Rosenberg et al., [2017](#bib.bib115); Hu and Tan, [2017](#bib.bib54); Al-Dujaili
    et al., [2018b](#bib.bib5)) | DREBIN, Microsoft Kaggle |'
  prefs: []
  type: TYPE_TB
- en: '| Relation Extraction | (Wu et al., [2017](#bib.bib146); Bekoulis et al., [2018a](#bib.bib12))
    | NYT Relation, UW Relation, ACE04, CoNLL04 EC, Dutch Real Estate Classifieds,
    Adverse Drug Events |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Translation | (Belinkov and Bisk, [2018](#bib.bib14); Ebrahimi et al.,
    [[n. d.]](#bib.bib31); Cheng et al., [2018](#bib.bib25); Zhao et al., [2017](#bib.bib157))
    | TED Talks, WMT’16 Multimodal Translation Task |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Comprehension | (Jia and Liang, [2017](#bib.bib56); Wang and Bansal,
    [2018](#bib.bib143); Blohm et al., [2018](#bib.bib17); Chan et al., [2018](#bib.bib22))
    | SQuAD, MovieQA Multiple Choice, Logical QA |'
  prefs: []
  type: TYPE_TB
- en: '| Text Summarization | (Cheng et al., [2018](#bib.bib25)) | DUC2003, DUC2004,
    Gigaword |'
  prefs: []
  type: TYPE_TB
- en: '| Text Entailment | (Kang et al., [2018](#bib.bib58); Minervini and Riedel,
    [2018](#bib.bib93); Iyyer et al., [2018](#bib.bib55); Zhao et al., [2017](#bib.bib157))
    | SNLI, SciTail, MultiNLI, SICK |'
  prefs: []
  type: TYPE_TB
- en: '| POS Tagging | (Yasunaga et al., [2018](#bib.bib152)) | WSJ portion of PTB,
    Treebanks in UD |'
  prefs: []
  type: TYPE_TB
- en: '| Dialogue System | (Niu and Bansal, [2018](#bib.bib100)) | Ubuntu Dialogue,
    CoCoA, |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-model | Optical Character Recognition | (Song and Shmatikov, [2018](#bib.bib130))
    | Hillary Clinton’s emails |'
  prefs: []
  type: TYPE_TB
- en: '| Scene Text Recognition | (Yuan et al., [2018](#bib.bib154)) | Street View
    Text, ICDAR 2013, IIIT5K |'
  prefs: []
  type: TYPE_TB
- en: '| Image Captioning | (Chen et al., [2018](#bib.bib23); Xu et al., [2018](#bib.bib149))
    | MSCOCO, Visual Genome |'
  prefs: []
  type: TYPE_TB
- en: '| Visual Question Answering | (Xu et al., [2018](#bib.bib149)) | Datasets from
    (Antol et al., [2015](#bib.bib7)), Datasets from (Zhu et al., [2016](#bib.bib158))
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Visual-Semantic Embedding | (Shi et al., [2018](#bib.bib126)) | MSCOCO
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Speech Tecognition | (Carlini and Wagner, [[n. d.]](#bib.bib20)) | Mozilla
    Common Voice |'
  prefs: []
  type: TYPE_TB
- en: Table 4\. Attacked Applications and Benchmark Datasets
  prefs: []
  type: TYPE_NORMAL
- en: 'Text Classification. Majority of the surveyed works attack the deep neural
    networks for text classification, since these tasks can be framed as a classification
    problem. Sentiment analysis aims to classify the sentiment to several groups (e.g.,
    in 3-group scheme: neural, positive and negative). Gender identification, Grammatical
    error detection and malware detection can be framed as binary classification problems.
    Relation extraction can be formulated as single or multi-classification problem.
    Predict medical status is a multi-class problem that the classes are defined by
    medical experts. These works usually use multiple datasets to evaluate their attack
    strategies to show the generality and robustness of their method. (Liang et al.,
    [2017](#bib.bib79)) used DBpedia ontology dataset (Lehmann et al., [2015](#bib.bib73))
    to classify the document samples into 14 high-level classes. (Gong et al., [2018](#bib.bib40))
    used IMDB movie reviews (Maas et al., [2011](#bib.bib86)) for sentiment analysis,
    and Reuters-2 and Reuters-5 newswires dataset provided by NLTK package^(10)^(10)10https://www.nltk.org/
    for categorization. (Papernot et al., [2016b](#bib.bib105)) used a un-specified
    movie review dataset for sentiment analysis. (Samanta and Mehta, [2018](#bib.bib118))
    also used IMDB movie review dataset for sentiment analysis. The work also performed
    gender classification on and Twitter dataset^(11)^(11)11https://www.kaggle.com/crowdflower/twitter-user-gender-cla2013.
    for gender detection. (Gao et al., [2018](#bib.bib36)) performed spam detection
    on Enron Spam Dataset (Metsis et al., [2006](#bib.bib92)) and adopted six large
    datasets from (Zhang et al., [2015](#bib.bib156)), i.e., AG’s news^(12)^(12)12https://www.di.unipi.it/˜gulli/,
    Sogou news (Wang et al., [2008](#bib.bib139)), DBPedia ontology dataset, Yahoo!
    Answers^(13)^(13)13Yahoo! Answers Comprehensive Questions and Answers version
    1.0 dataset through the Yahoo! Webscope program. for text categorization and Yelp
    reviews^(14)^(14)14Yelp Dataset Challenge in 2015, Amazon reviews (McAuley and
    Leskovec, [2013](#bib.bib91)) for sentiment analysis. (Ebrahimi et al., [2018](#bib.bib32))
    also used AG’s news for text classification. Further, they used Stanford Sentiment
    Treebank (SST) dataset (Socher et al., [2013](#bib.bib129)) for sentiment analysis.
    (Sato et al., [2018](#bib.bib119)) conducted evaluation on three tasks: sentiment
    analysis (IMDB movie review, Elec (Johnson and Zhang, [2015](#bib.bib57)), Rotten
    Tomatoes (Pang and Lee, [2005](#bib.bib104))), text categorization (DBpedia Ontology
    dataset and RCV1 (Lewis et al., [2004](#bib.bib74))) and grammatical error detection
    (FCE-public (Yannakoudakis et al., [2011](#bib.bib151))). (Sun et al., [2018](#bib.bib131))
    generated adversarial examples on the neural medical status prediction system
    with real-world electronic health records data. Many works target the malware
    detection models. (Grosse et al., [2016](#bib.bib46), [2017](#bib.bib47)) performed
    attack on neural malware detection systems. They used DREBIN dataset which contains
    both benigh and malicious android applications (Arp et al., [2014](#bib.bib8)).
    (Rosenberg et al., [2017](#bib.bib115)) collected benigh windows application files
    and used Microsoft Malware Classification Challenge dataset (Ronen et al., [2018](#bib.bib114))
    as the malicious part. (Hu and Tan, [2017](#bib.bib54)) crawled 180 programs with
    corresponding behavior reports from a website for malware analysis^(15)^(15)15https://malwr.com/.
    70% of the crawled programs are malware. (Neekhara et al., [2018](#bib.bib99))
    proposed another kind of attack, called reprogramming. They specifically targeted
    the text classification neural models and used four datasets to evaluate their
    attack methods: Surname Classification Dataset^(16)^(16)16Classifying names with
    a character-level rnn - pytroch tutorial. , Experimental Data for Question Classification
    (Li and Roth, [2002](#bib.bib77)), Arabic Tweets Sentiment Classification Dataset
    (Abdulla et al., [2013](#bib.bib2)) and IMDB movie review dataset. In (Wu et al.,
    [2017](#bib.bib146)), the authors modelled the relation extraction as a classification
    problem, where the goal is to predict the relations exist between entity pairs
    given text mentions. They used two relation datasets: NYT dataset (Riedel et al.,
    [2010](#bib.bib112)) and UW dataset (Liu et al., [2016](#bib.bib81)). The work
    (Bekoulis et al., [2018a](#bib.bib12)) targeted at improving the efficacy of the
    neural networks for joint entity and relation extraction. Different to the method
    in (Wu et al., [2017](#bib.bib146)), the authors modelled the relation extraction
    task as a multi-label head selection problem. The four datasets are used in their
    work: ACE04 dataset (Doddington et al., [2004](#bib.bib29)), CoNLL04 EC tasks
    (Roth and Yih, [2004](#bib.bib116)), Dutch Real Estate Classifieds (DREC) dataset
    (Bekoulis et al., [2018b](#bib.bib13)), and Adverse Drug Events (ADE) (Gurulingappa
    et al., [2012](#bib.bib48)).'
  prefs: []
  type: TYPE_NORMAL
- en: Machine Translation. Machine Translation works on parallel datasets, one of
    which uses source language and the other one is in the target language. (Belinkov
    and Bisk, [2018](#bib.bib14)) used the TED talks parallel corpus prepared for
    IWSLT 2016 (Mauro et al., [2012](#bib.bib90)) for testing the NMT systems. They
    also collected French, German and Czech corpus for generating natural noises to
    build a look-up table which contains possible lexical replacements that later
    be used for generating adversarial examples. (Ebrahimi et al., [[n. d.]](#bib.bib31))
    also used the same TED talks corpus and used German to English, Czech to English,
    and French to English pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Comprehension. Machine comprehension datasets usually provide context
    documents or paragraphs to the machines. Based on the comprehension of the contexts,
    machine comprehension models can answer a question. Jia and Liang are one of the
    first to consider the textual adversary and they targeted the neural machine comprehension
    models (Jia and Liang, [2017](#bib.bib56)). They used the Stanford Question Answering
    Dataset (SQuAD) to evaluate the impact of their attack on the neural machine comprehension
    models. SQuAD is a widely recognised benchmark dataset for machine comprehension.
    (Wang and Bansal, [2018](#bib.bib143)) followed the previous works and also worked
    on SQuAD dataset. Althouth the focus of the work (Blohm et al., [2018](#bib.bib17))
    is to develop a robust machine comprehension model rather than attacking MC models,
    they used the adversarial examples to evaluate their proposed system. They used
    MovieQA multiple choice question answering dataset (Tapaswi et al., [2016](#bib.bib135))
    for the evaluation. (Chan et al., [2018](#bib.bib22)) targeted attacks on differentiable
    neural computer (DNC), which is a novel computing machine with DNN. They evaluated
    the attacks on logical question answering using bAbI tasks^(17)^(17)17https://research.fb.com/downloads/babi/.
  prefs: []
  type: TYPE_NORMAL
- en: Text Summarization. The goal for text summarization is to summarize the core
    meaning of a given document or paragraph with succinct expressions. There is no
    surveyed papers that only target the application of text summarization. (Cheng
    et al., [2018](#bib.bib25)) evaluated their attack on multiple applications including
    text summarization and they used DUC2003^(18)^(18)18http://duc.nist.gov/duc2003/tasks.html,
    DUC2004^(19)^(19)19http://duc.nist.gov/duc2004/, and Gigaword^(20)^(20)20https://catalog.ldc.upenn.edu/LDC2003T05
    for evaluating the effectiveness of adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text Entailment. The fundamental task of text entailment is to decide whether
    a premise text entails a hypothesis, i.e., the truth of one text fragment follows
    from another text. (Kang et al., [2018](#bib.bib58)) assessed various models on
    two entailment datasets: Standord Natural Lauguage Inference (SNLI) (Bowman et al.,
    [2015](#bib.bib18)) and SciTail (Khot et al., [2018](#bib.bib60)). (Minervini
    and Riedel, [2018](#bib.bib93)) also used SNLI dataset. Furthermore, they used
    MultiNLI (Williams et al., [2018](#bib.bib145)) dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part-of-Speech (POS) Tagging. The purpose for POS tagging is to resolve the
    part-of-speech for each word in a sentence, such as noun, verb etc. It is one
    of the fundamental NLP tasks to facilitate other NLP tasks, e.g., syntactic parsing.
    Neural networks are also adopted for this NLP task. (Yasunaga et al., [2018](#bib.bib152))
    adopted the method in (Miyato et al., [2016a](#bib.bib95)) to build a more robust
    neural network by introducing adversarial training, but they applied the strategy
    (with minor modifications) in POS tagging. By training on the mixture of clean
    and adversarial example, the authors found that adversarial examples not only
    help improving the tagging accuracy, but also contribute to downstream task of
    dependency parsing and is generally effective in different sequence labelling
    tasks. The datasets used in their evaluation include: the Wall Street Journal
    (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., [1993](#bib.bib88)) and
    treebanks from Universal Dependencies (UD) v1.2 (Nivre et al., [2015](#bib.bib101)).'
  prefs: []
  type: TYPE_NORMAL
- en: Dialogue Generation. Dialogue generation is a fundamental component for real-world
    virtual assistants such as Siri^(21)^(21)21https://www.apple.com/au/siri/ and
    Alexa^(22)^(22)22https://en.wikipedia.org/wiki/Amazon_Echo. It is the text generation
    task that automatically generate a response given a post by the user. (Niu and
    Bansal, [2018](#bib.bib100)) is one of the first to attack the generative dialogue
    models. They used the Ubuntu Dialogue Corpus (Lowe et al., [2015](#bib.bib85))
    and Dynamic Knowledge Graph Network with the Collaborative Communicating Agents
    (CoCoA) dataset (He et al., [2017](#bib.bib50)) for the evaluation of their two
    attack strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-model Applications. (Song and Shmatikov, [2018](#bib.bib130)) evaluated
    the OCR systems with adversarial examples using Hillary Clinton’s emails^(23)^(23)23https://www.
    kaggle.com/kaggle/hillary-clinton-emails/data, which is in the form of images.
    They also conducted the attack on NLP applications using Rotten Tomatoes and IMDB
    review datasets. The work in (Yuan et al., [2018](#bib.bib154)) attacked the neural
    networks designed for scene text rcognition. They conducted experiments on three
    standard benchmarks for cropped word image recognition, namely the Street View
    Text dataset (SVT) (Wang et al., [2011](#bib.bib140)) the ICDAR 2013 dataset (IC13)
    (Karatzas et al., [2013](#bib.bib59)) and the IIIT 5K-word dataset (IIIT5K) (Mishra
    et al., [2012](#bib.bib94)). (Chen et al., [2018](#bib.bib23)) attacked the image
    captioning neural models. The dataset they used is the Microsoft COCO (MSCOCO)
    dataset (Lin et al., [2014](#bib.bib80)). (Xu et al., [2018](#bib.bib149)) worked
    on the problems of attacking neural models for image captioning and visual question
    answering. For the first task, they used Visual Genome dataset (Krishna et al.,
    [2017](#bib.bib66)). For the second task, they used the VQA datasets collected
    and processed in (Antol et al., [2015](#bib.bib7)). (Shi et al., [2018](#bib.bib126))
    worked on Visual-Semantic Embedding applications, where the MSCOCO dataset is
    used. (Carlini and Wagner, [[n. d.]](#bib.bib20)) targeted the speech recognition
    problem. The datasets they used is the Mozilla Common Voice dataset^(24)^(24)24https://voice.mozilla.org/en.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Applications Some works adapt their attack methods into different applications,
    namely, they evaluate their method’s trasferability across applications. (Cheng
    et al., [2018](#bib.bib25)) attacked the sequence-to-sequence models. Specifically,
    they evaluated their attack on two applications: text summarization and machine
    translation. For text summarization, as mentioned before, they used three datasets
    DUC2003, DUC2004, and Gigaword. For the machine translation, they sampled a subset
    form WMT’16 Multimodal Translation dataset^(25)^(25)25http://www.statmt.org/wmt16/translation-task.html.
    (Iyyer et al., [2018](#bib.bib55)) proposed syntactically adversarial paraphrase
    and evaluated the attack on sentiment analysis and text entailment applications.
    They used SST for sentimental analysis and SICK (Marelli et al., [2014](#bib.bib89))
    for text entailment. (Zhao et al., [2017](#bib.bib157)) is a generic approach
    for generating adversarial examples on neural models. The applications investigated
    include image classification (MINIST digital image dataset), textual entailment
    (SNLI), and machine translation. (Miyato et al., [2016a](#bib.bib95)) evaluated
    their attacks on five datasets, covering both sentiment analysis (IMDB movie review,
    Elec product review, Rotten Tomatoes movie review) and text categorization (DBpedia
    Ontology, RCV1 news articles). (Singh et al., [2018](#bib.bib128)) targeted two
    applications. For sentiment analysis, they used Rotten Tomato movie reviews and
    IMDB movie reviews datasets. For visual question answering, they tested on dataset
    provided by Zhu et al. (Zhu et al., [2016](#bib.bib158)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Defense
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An essential purpose for generating adversarial examples for neural networks
    is to utilize these adversarial examples to enhance the model’s robustness (Goodfellow
    et al., [2015](#bib.bib43)). There are two common ways in textual DNN to achieve
    this goal: adversarial training and knowledge distillation. Adversarial training
    incorporates adversarial examples in the model training process. Knowledge distillation
    manipulates the neural network model and trains a new model. In this section,
    we introduce some representative studies belonging to these two directions. For
    more comprehensive defense strategies on machine learning and deep leaning models
    and applications, please refer to (Biggio and Roli, [2018](#bib.bib16); Akhtar
    and Mian, [2018](#bib.bib3)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Adversarial Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Szegedy et al. (Szegedy et al., [2014](#bib.bib133)) invented adversarial training,
    a strategy that consists of training a neural network to correctly classify both
    normal examples and adversarial examples. Goodfellow et al. (Goodfellow et al.,
    [2015](#bib.bib43)) employed explicit training with adversarial examples. In this
    section, we describe works utilizing data augmentation, model regularization and
    robust optimization for the defense purpose on textual adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data augmentation extends the original training set with the generated adversarial
    examples and try to let the model see more data during the training process. Data
    augmentation is commonly used against black-box attacks with additional training
    epochs on the attacked DNN with adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors in work (Jia and Liang, [2017](#bib.bib56)) try to enhance the
    reading comprehension model with training on the augmented dataset that includes
    the adversarial examples. They showed that this data augmentation is effective
    and robust against the attack that uses the same adversarial examples. However,
    their work also demonstrated that this augmentation strategy would be still vulnerable
    against the attacks with other kinds of adversarial examples. (Wang and Bansal,
    [2018](#bib.bib143)) shared similar idea to augment the training dataset, but
    selected further informative adversarial examples as discussed in Section [4.3.1](#S4.SS3.SSS1
    "4.3.1\. Concatenation Adversaries ‣ 4.3\. Black-box Attack ‣ 4\. Attacking Neural
    Models in NLP: State-of-The-Art ‣ Adversarial Attacks on Deep Learning Models
    in Natural Language Processing: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The work in (Kang et al., [2018](#bib.bib58)) trains the text entailment system
    augmented with adversarial examples. The purpose is to make the system more robust.
    They proposed three methods to generate more data with diverse characteristics:
    (1) knowledge-based, which replaces words with their hypernym/hyponym provided
    in several given knowledge bases; (2) hand-crafted, which adds negations to the
    the existing entailment; (3) neural-based, which leverages a seq2seq model to
    generate an entailment examples by enforcing the loss function to measure the
    cross-entropy between the original hypothesis and the predicted hypothesis. During
    the training process, they adopt the idea from generative adversarial network
    to train a discriminator and a generator, and incorporating the adversarial examples
    in the discriminator’s optimization step.'
  prefs: []
  type: TYPE_NORMAL
- en: (Belinkov and Bisk, [2018](#bib.bib14)) explores another way for data augmentation.
    It takes the average character embedding as a word representation and incorporate
    it into the input. This approach is intrinsically insensitive to character scrambling
    such as swap, mid and Rand, thus can resists to noises caused by these scrambling
    attacks proposed in the work. However, this defense is ineffective to other attacks
    that do not perturb on characters’ orders.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Model Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Model regularization enforces the generated adversarial examples as the regularizer
    and follows the form of:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (32) |  | $\displaystyle\min(J(f(x),y)+\lambda J(f(x^{\prime}),y))$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda$ is a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following (Goodfellow et al., [2015](#bib.bib43)), the work (Miyato et al.,
    [2016a](#bib.bib95)) constructed the adversarial training with a linear approximation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (33) |  | $\displaystyle-\log p(y&#124;x+-\epsilon g/&#124;&#124;g&#124;&#124;_{2},;\theta)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle g=\partial_{x}\log p(y&#124;x;\hat{\theta})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $||g||_{2}$ is the $L_{2}$ norm regularization, $\theta$ is the parameter
    of the neural model, and $\hat{\theta}$ is a constant copy of $\theta$. The difference
    to (Goodfellow et al., [2015](#bib.bib43)) is that, the authors performed the
    adversarial generation and training in terms of the word embedding. Further, they
    extended their previous work on attacking image deep neural model (Miyato et al.,
    [2016b](#bib.bib96)), where the local distribution smoothness (LDS) is defined
    as the negative of the KL divergence of two distributions (original data and the
    adversaries). LDS measures the robustness of the model against the perturbation
    in local and ‘virtual’ adversarial direction. In this sense, the adversary is
    calculated as the direction to which the model distribution is most sensitive
    in terms of KL divergence. They also applied this attack strategy on word embedding
    and performed adversarial training by adding adversarial examples as regularizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The work (Sato et al., [2018](#bib.bib119)) follows the idea from (Miyato et al.,
    [2016a](#bib.bib95)) and extends the adversarial training on LSTM. The authors
    followed FGSM to incorporate the adversarial training as a regularizer. But in
    order to enable the interpretability of adversarial examples, i.e., the word embedding
    of the adversaries should be valid word embeddings in the vocabulary, they introduced
    a direction vector which associates the perturbed embedding to the valid word
    embedding. (Wu et al., [2017](#bib.bib146)) simply adopts the regularizer utilized
    in (Miyato et al., [2016a](#bib.bib95)), but applies the perturbations on pre-trained
    word embedding and in a different task: relation extraction. Other similar works
    that adopt (Miyato et al., [2016a](#bib.bib95)) are (Wu et al., [2017](#bib.bib146);
    Yasunaga et al., [2018](#bib.bib152); Sato et al., [2018](#bib.bib119); Bekoulis
    et al., [2018a](#bib.bib12)). We will not cover all these works in this article,
    since they simply adopting this method.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3\. Robust Optimisation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Madry et al. (Madry et al., [2018](#bib.bib87)) cast DNN model learning as
    a robust optimization with min-max (saddle point) formulation, which is the composition
    of an inner non-concave maximization problem (attack) and an outer non-convex
    minimization problem (defense). According to Danskin’s theorem, gradients at inner
    maximizers correspond to descent directions for the min-max problem, thus the
    optimization can still apply back-propagation to proceed. The approach successfully
    demonstrated robustness of DNNs against adversarial images by training and learning
    universally. (Al-Dujaili et al., [2018a](#bib.bib4)) adopts the idea and applies
    on malware detection DNN that handles discrete data. Their leaning objective is
    formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (34) |  | $\displaystyle\theta^{*}=arg\min_{\theta}\mathbb{E}_{(x,y)\sim
    D}[\max_{x^{\prime}\in S(x)}L(\theta,x^{\prime},y)]$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $S(x)$ is the set of binary indicator vectors that preserve the functionality
    of malware $x$, $L$ is the loss function for the original classification model,
    $y$ is the groundtruth label, $\theta$ is the learnable parameters, $D$ denotes
    the distribution of data sample $x$.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the proposed robust optimisation method is an universal
    framework under which other adversarial training strategies have natural interpretation.
    We describe it separately keeping in view its popularity in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Papernot et al. (Papernot et al., [2016c](#bib.bib108)) proposed distillation
    as another possible defense against adversarial examples. The principle is to
    use the softmax output (e.g., the class probabilities in classfication DNNs) of
    the original DNN to train the second DNN, which has the same structure with the
    original one. The softmax of the original DNN is also modified by introducing
    a temperature parameter $T$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (35) |  | $\displaystyle q_{i}=\frac{\exp{(z_{i}/T)}}{\sum_{k}\exp{(z_{k}/T)}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $z_{i}$ is input of softmax layer. $T$ controls the level of knowledge
    distillation. When $T=1$, Eq. ([35](#S5.E35 "In 5.2\. Distillation ‣ 5\. Defense
    ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")) turns back to the normal softmax function. If $T$ is large, $q_{i}$
    is close to a uniform distribution, when it is small, the function will output
    more extreme values. (Grosse et al., [2017](#bib.bib47)) adopts distillation defense
    for DNNs on discrete data and applied a high temperature $T$, as high-temperature
    softmax is proved to reduce the model sensitivity to small perturbations (Papernot
    et al., [2016c](#bib.bib108)). They trained the second DNN with the augmentation
    of original dataset and the softmax outputs from the original DNN. From the evaluations,
    they found adversarial training is the more effective than distillation. (I like
    if there is answers that explains why adversarial training is the more effective
    than distillation )'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Discussions and Open Issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generating textual adversarial examples has relatively shorter history than
    generating image adversarial examples on DNNs because it is more challenging to
    make perturbation on discrete data, and meanwhile preserving the valid syntactic,
    grammar and semantics. We discuss some of the issues in this section and provide
    suggestions on future directions.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Perceivability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perturbations in image pixels are usually hard to be perceived, thus do not
    affect human judgment, but can only fool the deep neural networks. However, the
    perturbation on text is obvious, no matter the perturbation is flipping characters
    or changing words. Invalid words and syntactic errors can be easily identified
    by human and detected by the grammar check software, hence the perturbation is
    hard to attack a real NLP system. However, many research works generate such types
    of adversarial examples. It is acceptable only if the purpose is utilizing adversarial
    examples to robustify the attacked DNN models. In semantic-preserving perspective,
    changing a word in a sentence sometimes changes its semantics drastically and
    is easily detected by human beings. For NLP applications such as reading comprehension,
    and sentiment analysis, the adversarial examples need to be carefully designed
    in order not to change the should-be output. Otherwise, both correct output and
    perturbed output change, violating the purpose of generating adversarial examples.
    This is challenging and limited works reviewed considered this constraint. Therefore,
    for practical attack, we need to propose methods that make the perturbations not
    only unperceivable, but preserve correct grammar and semantics.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Transferability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transferability is a common property for adversarial examples. It reflects
    the generalization of the attack methods. Transferability means adversarial examples
    generated for one deep neural network on a dataset can also effectively attack
    another deep neural network (i.e., cross-model generalization) or dataset (i.e.,
    cross-data generalization). This property is more often exploited in black-box
    attacks as the details of the deep neural networks does not affect the attack
    method much. It is also shown that untargeted adversarial examples are much more
    transferable than targeted ones (Liu et al., [2017a](#bib.bib84)). Transferability
    can be organized into three levels in deep neural networks: (1) same architecture
    with different data; (2) different architectures with same application; (3) different
    architectures with different data (Yuan et al., [2017](#bib.bib155)). Although
    current works on textual attacks cover both three levels, the performance of the
    transferred attacks still decrease drastically compared to it on the original
    architecture and data, i.e., poor generalization ability. More efforts are expected
    to deliver better generalization ability.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Automation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some reviewed works are able to generate adversarial examples automatically,
    while others cannot. In white-box attacks, leveraging the loss function of the
    DNN can identify the most affected points (e.g., character, word) in a text automatically.
    Then the attacks are performed on these points by automatically modifying the
    corresponding texts. In black-box attacks, some attacks, e.g. substitution train
    substitute DNNs and apply white-box attack strategies on the substitution. This
    can be achieved automatically. However, most of the other works craft the adversarial
    examples in a manual manner. For example, (Jia and Liang, [2017](#bib.bib56))
    concatenated manually-chosen meaningless paragraphs to fool the reading comprehension
    systems, in order to discover the vulnerability of the victim DNNs. Many research
    works followed their way, not aiming on practical attacks, but more on examining
    robustness of the target network. These manaul works are time-consuming and impractical.
    We believe that more efforts in this line could pass through this barrier in future.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4\. New Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although most of the common textual DNNs have gained attention from the perspective
    of adversarial attack (Section [2.2](#S2.SS2 "2.2\. Deep Learning in NLP ‣ 2\.
    Overview of Adversarial Attacks and Deep Learning Techniques in Natural Language
    Processing ‣ Adversarial Attacks on Deep Learning Models in Natural Language Processing:
    A Survey")), many DNNs haven’t been attacked so far. For example, the generative
    neural models: Generative Adversarial Networks (GANs) and Variational Auto-Encoders
    (VAEs). In NLP, they are used to generate texts. Deep generative models requires
    more sophisticated skill for model training. This would explain that these techniques
    have been mainly overlooked by adversarial attack so far. Future works may consider
    about generating adversarial examples for these generative DNNs. Another example
    is differentiable neural computer (DNC). Only one work attacked DNC so far (Chan
    et al., [2018](#bib.bib22)). Attention mechanism is somehow become a standard
    component in most of the sequential models. But there is no work examined the
    mechanism itself. Instead, works are either attack the overall system that contain
    attentions, or leverage attention scores to identify the word for perturbation
    (Blohm et al., [2018](#bib.bib17)).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5\. Iterative vs One-off
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Iterative attacks iteratively search and update the perturbations based on the
    gradient of the output of the attacked DNN model. Thus it shows high quality and
    effectiveness, that is the perturbations can be small enough and hard to defense.
    However, these methods usually require long time to find the proper perturbations,
    rendering an obstacle for attacking in real-time. Therefore, one-off attacks are
    proposed to tackle this problem. FGSM (Goodfellow et al., [2015](#bib.bib43))
    is one example of one-off attack. Natually, one-off attack is much faster than
    iterative attack, but is less effective and easier to be defensed (Yuan et al.,
    [2018](#bib.bib154)). When designing attack methods on a real application, attackers
    need to carefully consider the trade off between efficiency and effectiveness
    of the attack.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article presents the first comprehensive survey in the direction of generating
    textual adversarial examples on deep neural networks. We review recent research
    efforts and develop classification schemes to organize existing literature. Additionally
    we summarize and analyze them from different aspects. We attempt to provide a
    good reference for researchers to gain insight of the challenges, methods and
    issues in this research topic and shed lights on future directions. We hope more
    robust deep neural models are proposed based on the knowledge of the adversarial
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdulla et al. (2013) Nawaf A Abdulla, Nizar A Ahmed, Mohammed A Shehab, and
    Mahmoud Al-Ayyoub. 2013. Arabic sentiment analysis: Lexicon-based and corpus-based.
    In *Proc. of the 2013 IEEE Jordan Conference on Applied Electrical Engineering
    and Computing Technologies (AEECT 2013)*. IEEE, 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akhtar and Mian (2018) Naveed Akhtar and Ajmal S. Mian. 2018. Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey. *IEEE Access* 6 (2018),
    14410–14430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Dujaili et al. (2018a) Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and
    Una-May O’Reilly. 2018a. Adversarial Deep Learning for Robust Detection of Binary
    Encoded Malware. In *Proc. of the 2018 IEEE Security and Privacy Workshops (SPW
    2018)*. Francisco, CA, USA, 76–82.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Dujaili et al. (2018b) Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and
    Una-May O’Reilly. 2018b. Adversarial Deep Learning for Robust Detection of Binary
    Encoded Malware. In *Proc. of the 2018 IEEE Security and Privacy Workshops (SP
    Workshops 2018)*. San Francisco, CA, USA, 76–82.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alzantot et al. (2018) Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang
    Ho, Mani B. Srivastava, and Kai-Wei Chang. 2018. Generating Natural Language Adversarial
    Examples. In *Proc. of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2018)*. Brussels, Belgium, 2890–2896.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual
    Question Answering. In *Proc. of the 2015 IEEE International Conference on Computer
    Vision (ICCV 2015)*. Santiago, Chile, 2425–2433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arp et al. (2014) Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon,
    and Konrad Rieck. 2014. DREBIN: Effective and Explainable Detection of Android
    Malware in Your Pocket. In *Proc. of the 21st Annual Network and Distributed System
    Security Symposium (NDSS 2014)*. San Diego, California, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
    Neural Machine Translation by Jointly Learning to Align and Translate. *CoRR*
    abs/1409.0473 (2014). arXiv:1409.0473 [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
    Neural Machine Translation by Jointly Learning to Align and Translate. (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barreno et al. (2010) Marco Barreno, Blaine Nelson, Anthony D. Joseph, and J. D.
    Tygar. 2010. The security of machine learning. *Machine Learning* 81, 2 (2010),
    121–148.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bekoulis et al. (2018a) Giannis Bekoulis, Johannes Deleu, Thomas Demeester,
    and Chris Develder. 2018a. Adversarial training for multi-context joint entity
    and relation extraction. In *Proc. of the 2018 Conference on Empirical Methods
    in Natural Language Processing (EMNLP 2018)*. Brussels, Belgium, 2830–2836.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bekoulis et al. (2018b) Giannis Bekoulis, Johannes Deleu, Thomas Demeester,
    and Chris Develder. 2018b. An attentive neural architecture for joint segmentation
    and parsing and its application to real estate ads. *Expert Systems with Applications*
    102 (2018), 100–112.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Belinkov and Bisk (2018) Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
    and Natural Noise Both Break Neural Machine Translation. *arXiv preprint arXiv:1711.02173\.
    ICLR* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2003) Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian
    Janvin. 2003. A Neural Probabilistic Language Model. *Journal of Machine Learning
    Research* 3 (2003), 1137–1155.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biggio and Roli (2018) Battista Biggio and Fabio Roli. 2018. Wild patterns:
    Ten years after the rise of adversarial machine learning. *Pattern Recognition*
    84 (2018), 317–331.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blohm et al. (2018) Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang Yu,
    and Ngoc Thang Vu. 2018. Comparing Attention-Based Convolutional and Recurrent
    Neural Networks: Success and Limitations in Machine Reading Comprehension. In
    *Proc. of the 22nd Conference on Computational Natural Language Learning (CoNLL
    2018)*. Brussels, Belgium, 108–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and
    Christopher D. Manning. 2015. A large annotated corpus for learning natural language
    inference. In *Proc. of the 2015 Conference on Empirical Methods in Natural Language
    Processing (EMNLP 2015)*. Lisbon, Portugal, 632–642.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bowman et al. (2016) Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M.
    Dai, Rafal Józefowicz, and Samy Bengio. 2016. Generating Sentences from a Continuous
    Space. In *Proc. of the 20th SIGNLL Conference on Computational Natural Language
    Learning (CoNLL 2016)*. Berlin, Germany, 10–21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlini and Wagner ([n. d.]) Nicholas Carlini and David A. Wagner. [n. d.].
    Audio Adversarial Examples: Targeted Attacks on Speech-to-Text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini and Wagner (2017) Nicholas Carlini and David A. Wagner. 2017. Towards
    Evaluating the Robustness of Neural Networks. In *Proc. of the 2017 IEEE Symposium
    on Security and Privacy (SP 2017)*. San Jose, CA, USA, 39–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chan et al. (2018) Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu,
    and Yew Soon Ong. 2018. Metamorphic Relation Based Adversarial Attacks on Differentiable
    Neural Computer. *CoRR* abs/1809.02444 (2018). arXiv:1809.02444 [http://arxiv.org/abs/1809.02444](http://arxiv.org/abs/1809.02444)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, and Cho-Jui
    Hsieh. 2018. Attacking Visual Language Grounding with Adversarial Examples: A
    Case Study on Neural Image Captioning. In *Proceedings of ACL 2018*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2017) Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang,
    and Diana Inkpen. 2017. Enhanced LSTM for Natural Language Inference. In *Proc.
    of the 55th Annual Meeting of the Association for Computational Linguistics (ACL
    2017)*. Vancouver, BC, Canada, 1657–1668.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2018) Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and
    Cho-Jui Hsieh. 2018. Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence
    Models with Adversarial Examples. *arXiv preprint arXiv:1803.01128* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2014) Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
    Representations using RNN Encoder-Decoder for Statistical Machine Translation.
    In *Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2014)*. Doha, Qatar, 1724–1734.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Costa-Jussà and Fonollosa (2016) Marta R. Costa-Jussà and José A. R. Fonollosa.
    2016. Character-based Neural Machine Translation. In *Proc. of the 54th Annual
    Meeting of the Association for Computational Linguistics (ACL 2016)*. Berlin,
    Germany.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dahl et al. (2013) George E. Dahl, Jack W. Stokes, Li Deng, and Dong Yu. 2013.
    Large-scale malware classification using random projections and neural networks.
    In *Proc. of the 38th International Conference on Acoustics, Speech and Signal
    Processing (ICASSP 2013)*. Vancouver, BC, Canada, 3422–3426.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doddington et al. (2004) George Doddington, Alexis Mitchell, Mark Przybocki,
    Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content
    Extraction (ACE) Program -Tasks, Data, and Evaluation. In *Proc. of the Fourth
    International Conference on Language Resources and Evaluation (LREC’04)*. Lisbon,
    Portugal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dzendzik et al. (2017) Daria Dzendzik, Carl Vogel, and Qun Liu. 2017. Who framed
    roger rabbit? multiple choice questions answering about movie plot. (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ebrahimi et al. ([n. d.]) Javid Ebrahimi, Daniel Lowd, and Dejing Dou. [n. d.].
    On Adversarial Examples for Character-Level Neural Machine Translation. In *Proc.
    of the 27th International Conference on Computational Linguistics (COLING 2018)*.
    Santa Fe, New Mexico, USA, 653–663.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ebrahimi et al. (2018) Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou.
    2018. HotFlip: White-Box Adversarial Examples for Text Classification. In *Proc.
    of the 56th Annual Meeting of the Association for Computational Linguistics (ACL
    2018)*. Melbourne, Australia, 31–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elsayed et al. (2018) Gamaleldin F. Elsayed, Ian J. Goodfellow, and Jascha Sohl-Dickstein.
    2018. Adversarial Reprogramming of Neural Networks. *CoRR* abs/1806.11146 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Faghri et al. (2017) Fartash Faghri, David J. Fleet, Ryan Kiros, and Sanja
    Fidler. 2017. VSE++: Improved Visual-Semantic Embeddings. *CoRR* abs/1707.05612
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fukui et al. (2016) Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,
    Trevor Darrell, and Marcus Rohrbach. 2016. Multimodal Compact Bilinear Pooling
    for Visual Question Answering and Visual Grounding. In *Proc. of the 2016 Conference
    on Empirical Methods in Natural Language Processing (EMNLP 2016)*. Austin, Texas,
    USA, 457–468.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2018) Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018.
    Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers.
    *arXiv preprint arXiv:1801.04354* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilmer et al. (2018) Justin Gilmer, Ryan P. Adams, Ian J. Goodfellow, David
    Andersen, and George E. Dahl. 2018. Motivating the Rules of the Game for Adversarial
    Example Research. *CoRR* abs/1807.06732 (2018). arXiv:1807.06732 [http://arxiv.org/abs/1807.06732](http://arxiv.org/abs/1807.06732)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goldberg (2017) Yoav Goldberg. 2017. *Neural Network Methods for Natural Language
    Processing*. Morgan & Claypool Publishers. [https://doi.org/10.2200/S00762ED1V01Y201703HLT037](https://doi.org/10.2200/S00762ED1V01Y201703HLT037)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goller and Kuchler (1996) Christoph Goller and Andreas Kuchler. 1996. Learning
    task-dependent distributed representations by backpropagation through structure.
    *Neural Networks* 1 (1996), 347–352.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2018) Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, and Wei-Shinn
    Ku. 2018. Adversarial Texts with Gradient Methods. *arXiv preprint arXiv:1801.07175*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2016) Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
    2016. *Deep learning*. Vol. 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    2014. Generative Adversarial Nets. In *Proc. of the Annual Conference on Neural
    Information Processing Systems 2014 (NIPS 2014)*. Montreal, Quebec, Canada, 2672–2680.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2015) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    2015. Explaining and Harnessing Adversarial Examples. In *Proc. of the 3rd International
    Conference on Learning Representations (ICLR 2015)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grave et al. (2017) Edouard Grave, Tomas Mikolov, Armand Joulin, and Piotr Bojanowski.
    2017. Bag of Tricks for Efficient Text Classification. In *Proc. of the 15th Conference
    of the European Chapter of the Association for Computational Linguistics (EACL
    2017)*. Valencia, Spain, 427–431.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves et al. (2013) Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton.
    2013. Speech recognition with deep recurrent neural networks. In *Proc. of IEEE
    2013 International Conference on Acoustics, Speech and Signal Processing (ICASSP
    2013)*. Vancouver, BC, Canada, 6645–6649.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grosse et al. (2016) Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael
    Backes, and Patrick McDaniel. 2016. Adversarial perturbations against deep neural
    networks for malware classification. *arXiv preprint arXiv:1606.04435* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grosse et al. (2017) Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael
    Backes, and Patrick D. McDaniel. 2017. Adversarial Examples for Malware Detection.
    In *Proc. of the 22nd European Symposium on Research in Computer Security (ESORICS
    2017)*. Oslo, Norway, 62–79.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gurulingappa et al. (2012) Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts,
    Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. 2012. Development of a
    benchmark corpus to support the automatic extraction of drug-related adverse effects
    from medical case reports. *Journal of Biomedical Informatics* 45, 5 (2012), 885–892.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hannun et al. (2014) Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro,
    Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam
    Coates, and Andrew Y. Ng. 2014. Deep Speech: Scaling up end-to-end speech recognition.
    *CoRR* abs/1412.5567 (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. 2017.
    Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph
    Embeddings. In *Proc. of the 55th Annual Meeting of the Association for Computational
    Linguistics (ACL 2017)*. Vancouver, Canada, 1766–1776.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.
    Distilling the Knowledge in a Neural Network. *CoRR* abs/1503.02531 (2015). arXiv:1503.02531
    [http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long Short-Term Memory. *Neural Computation* 9, 8 (1997), 1735–1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2017) Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell,
    and Kate Saenko. 2017. Learning to Reason: End-to-End Module Networks for Visual
    Question Answering. In *Proc. of IEEE International Conference on Computer Vision
    (ICCV 2017)*. Venice, Italy, 804–813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu and Tan (2017) Weiwei Hu and Ying Tan. 2017. Black-Box Attacks against RNN
    based Malware Detection Algorithms. *arXiv preprint arXiv:1705.08131* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iyyer et al. (2018) Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer.
    2018. Adversarial Example Generation with Syntactically Controlled Paraphrase
    Networks. In *Proc. of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)*.
    New Orleans, Louisiana, USA, 1875–1885.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia and Liang (2017) Robin Jia and Percy Liang. 2017. Adversarial Examples for
    Evaluating Reading Comprehension Systems. In *Proc. of the 2017 Conference on
    Empirical Methods in Natural Language Processing (EMNLP 2017)*. Copenhagen, Denmark,
    2021–2031.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson and Zhang (2015) Rie Johnson and Tong Zhang. 2015. Semi-supervised Convolutional
    Neural Networks for Text Categorization via Region Embedding. In *Proc. of the
    Annual Conference on Neural Information Processing Systems 2015 (NIPS 2015)*.
    Montreal, Quebec, Canada, 919–927.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2018) Dongyeop Kang, Tushar Khot, Ashish Sabharwal, , and Eduard
    Hovy. 2018. AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided
    Examples. In *Proceedings of ACL 2018*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karatzas et al. (2013) Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,
    Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernández
    Mota, Jon Almazán, and Lluís-Pere de las Heras. 2013. ICDAR 2013 Robust Reading
    Competition. In *Proc. of the 12th International Conference on Document Analysis
    and Recognition (ICDAR 2013)*. Washington, DC, USA, 1484–1493.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khot et al. (2018) Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTaiL:
    A Textual Entailment Dataset from Science Question Answering. In *Proc. of the
    32nd AAAI Conference on Artificial Intelligence (AAAI 2018)*. New Orleans, Louisiana,
    USA, 5189–5197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim (2014) Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
    In *Proc. of the 2014 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2014)*. Doha, Qatar, 1746–1751.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2016) Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush.
    2016. Character-Aware Neural Language Models. In *Proc. of the 13th AAAI Conference
    on Artificial Intelligence (AAAI 2016)*. Phoenix, Arizona, USA, 2741–2749.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. dam: A Method for
    Stochastic Optimization. In *Proc. of the 3rd International Conference on Learning
    Representations (ICLR 2015)*. San Diego, CA, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2014) Diederik P. Kingma and Max Welling. 2014. Auto-Encoding
    Variational Bayes. In *Proc. of the 2014 International Conference on Learning
    Representations (ICLR 2014)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klein et al. (2017) Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart,
    and Alexander M. Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation.
    In *Proc. of the 55th Annual Meeting of the Association for Computational Linguistics
    (ACL 2017)*. Vancouver, BC, Canada, 67–72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
    Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A.
    Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting
    Language and Vision Using Crowdsourced Dense Image Annotations. *International
    Journal of Computer Vision* 123, 1 (2017), 32–73.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. In *Proc.
    of the 26th Annual Conference on Neural Information Processing Systems (NIPS 2012)*.
    Lake Tahoe, Nevada, USA, 1106–1114.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar et al. (2016) Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James
    Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016.
    Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. In *Proc.
    of the 33nd International Conference on Machine Learning (ICML 2016)*. New York
    City, NY, USA, 1378–1387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurakin et al. (2017) Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017.
    Adversarial Machine Learning at Scale. In *Proc. of the 5th International Conference
    on Learning Representations (ICLR 2017)*. oulon, France.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kusner et al. (2015) Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q.
    Weinberger. 2015. From Word Embeddings To Document Distances. In *Proc. of the
    32nd International Conference on Machine Learning (ICML 2015))*. Lille, France,
    957–966.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le and Mikolov (2014) Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations
    of Sentences and Documents. In *Proc. of the 31th International Conference on
    Machine Learning (ICML 2014)*. Beijing, China, 1188–1196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2017) Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2017. Fully
    Character-Level Neural Machine Translation without Explicit Segmentation. *TACL*
    5 (2017), 365–378.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lehmann et al. (2015) Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
    Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick
    van Kleef, Sören Auer, and Christian Bizer. 2015. DBpedia - A large-scale, multilingual
    knowledge base extracted from Wikipedia. *Semantic Web* 6, 2 (2015), 167–195.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2004) David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li.
    2004. RCV1: A New Benchmark Collection for Text Categorization Research. *Journal
    of Machine Learning Research* 5 (2004), 361–397.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019) Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang.
    2019. TextBugger: Generating Adversarial Text Against Real-world Applications.
    In *Proc. of 26th Annual Network and Distributed System Security Symposium (NDSS
    2019)*. San Diego, California, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley,
    and Jianfeng Gao. 2016. Deep Reinforcement Learning for Dialogue Generation. In
    *Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2016)*. Austin, Texas, USA, 1192–1202.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Roth (2002) Xin Li and Dan Roth. 2002. Learning Question Classifiers.
    In *Proc. of the 19th International Conference on Computational Linguistics (COLING
    2002)*. aipei, Taiwan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li Deng (2018) Yang Liu Li Deng. 2018. *Deep Learning in Natural Language Processing*.
    Springer Singapore. [https://doi.org/10.1007/978-981-10-5209-5](https://doi.org/10.1007/978-981-10-5209-5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2017) Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong
    Li, and Wenchang Shi. 2017. Deep Text Classification Can be Fooled. *arXiv preprint
    arXiv:1704.08006* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft
    COCO: Common Objects in Context. In *Proc. of the 13th European Conference on
    Computer Vision (ECCV 2014)*. Zurich, Switzerland, 740–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2016) Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H.
    Lin, Xiao Ling, and Daniel S. Weld. 2016. Effective Crowd Annotation for Relation
    Extraction. In *Proc. of the 2016 Conference of the North American Chapter of
    the Association for Computational Linguistics (NAACL 2016)*. San Diego California,
    USA, 897–906.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018) Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor
    C. M. Leung. 2018. A Survey on Security Threats and Defensive Techniques of Machine
    Learning: A Data Driven View. *IEEE Access* 6 (2018), 12103–12117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2017b) Tzu-Chien Liu, Yu-Hsueh Wu, and Hung-yi Lee. 2017b. Attention-based
    CNN Matching Net. *CoRR* abs/1709.05036 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2017a) Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017a.
    Delving into Transferable Adversarial Examples and Black-box Attacks. In *Proc.
    of the 2017 International Conference on Learning Representations (ICLR 2017)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lowe et al. (2015) Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau.
    2015. The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
    Multi-Turn Dialogue Systems. In *Proc. of the 16th Annual Meeting of the Special
    Interest Group on Discourse and Dialogue (SIGDIAL 2015)*. Prague, Czech Republic,
    285–294.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment
    Analysis. In *Proc. of the 49th Annual Meeting of the Association for Computational
    Linguistics (ACL 2011)*. Portland, Oregon, USA, 142–150.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madry et al. (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
    Attacks. In *Proc. of the 6th International Conference on Learning Representations
    (ICLR 2018)*. Vancouver, BC, Canada.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marcus et al. (1993) Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
    1993. Building a Large Annotated Corpus of English: The Penn Treebank. *Computational
    Linguistics* 19, 2 (1993), 313–330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marelli et al. (2014) Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella
    Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. SemEval-2014 Task 1: Evaluation
    of Compositional Distributional Semantic Models on Full Sentences through Semantic
    Relatedness and Textual Entailment. In *Proc. of the 8th International Workshop
    on Semantic Evaluation (SemEval@COLING 2014)*. Dublin, Ireland, 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mauro et al. (2012) Cettolo Mauro, Girardi Christian, and Federico Marcello.
    2012. Wit3: Web Inventory of Transcribed and Translated Talks. In *Conference
    of European Association for Machine Translation*. 261–268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McAuley and Leskovec (2013) Julian J. McAuley and Jure Leskovec. 2013. Hidden
    factors and hidden topics: understanding rating dimensions with review text. In
    *Proc. of the 7th ACM Conference on Recommender Systems (RecSys 2013)*. Hong Kong,
    China, 165–172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metsis et al. (2006) Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras.
    2006. Spam Filtering with Naive Bayes - Which Naive Bayes?. In *Proc. of the Third
    Conference on Email and Anti-Spam (CEAS 2006)*. Mountain View, California, USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minervini and Riedel (2018) Pasquale Minervini and Sebastian Riedel. 2018. Adversarially
    Regularising Neural NLI Models to Integrate Logical Background Knowledge. In *Proc.
    of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018)*.
    Brussels, Belgium, 65–74.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishra et al. (2012) Anand Mishra, Karteek Alahari, and C. V. Jawahar. 2012.
    Scene Text Recognition using Higher Order Language Priors. In *Proc. of the 23rd
    British Machine Vision Conference (BMVC 2012)*. Surrey, UK, 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miyato et al. (2016a) Takeru Miyato, Andrew M Dai, and Ian Goodfellow. 2016a.
    Adversarial training methods for semi-supervised text classification. *arXiv preprint
    arXiv:1605.07725* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miyato et al. (2016b) Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae,
    and Shin Ishii. 2016b. Distributional smoothing with virtual adversarial training.
    In *Proc. of the 4th International Conference on Learning Representations (ICLR
    2016)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moosavi-Dezfooli et al. (2016) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
    and Pascal Frossard. 2016. DeepFool: A Simple and Accurate Method to Fool Deep
    Neural Networks. In *Proc. of the 2016 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR 2016)*. Las Vegas, NV, USA, 2574–2582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mozer (1995) Michael C Mozer. 1995. A focused backpropagation algorithm for
    temporal. *Backpropagation: Theory, architectures, and applications* 137 (1995).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neekhara et al. (2018) Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, and
    Farinaz Koushanfar. 2018. Adversarial Reprogramming of Sequence Classification
    Neural Networks. *CoRR* abs/1809.01829 (2018). arXiv:1809.01829 [http://arxiv.org/abs/1809.01829](http://arxiv.org/abs/1809.01829)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niu and Bansal (2018) Tong Niu and Mohit Bansal. 2018. Adversarial Over-Sensitivity
    and Over-Stability Strategies for Dialogue Models. In *Proc. of the 22nd Conference
    on Computational Natural Language Learning (CoNLL 2018)*. Brussels, Belgium, 486–496.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nivre et al. (2015) Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki
    Asahara, Aitziber Atutxa, Miguel Ballesteros, John Bauer, Kepa Bengoetxea, Riyaz Ahmad
    Bhat, Cristina Bosco, et al. 2015. Universal Dependencies 1.2. (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otter et al. (2018) Daniel W. Otter, Julian R. Medina, and Jugal K. Kalita.
    2018. A Survey of the Usages of Deep Learning in Natural Language Processing.
    *CoRR* abs/1807.10854 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Palangi et al. (2016) Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong
    He, Jianshu Chen, Xinying Song, and Rabab K. Ward. 2016. Deep Sentence Embedding
    Using Long Short-Term Memory Networks: Analysis and Application to Information
    Retrieval. *IEEE/ACM Trans. Audio, Speech & Language Processing* 24, 4 (2016),
    694–707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang and Lee (2005) Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploiting
    Class Relationships for Sentiment Categorization with Respect to Rating Scales.
    In *Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics
    (ACL 2005)*. Michigan, USA, 115–124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papernot et al. (2016b) Nicolas Papernot, Patrick McDaniel, Ananthram Swami,
    and Richard Harang. 2016b. Crafting Adversarial Input Sequences for Recurrent
    Neural Networks. In *Military Communications Conference, MILCOM 2016-2016 IEEE*.
    IEEE, 49–54.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papernot et al. (2017) Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow,
    Somesh Jha, Z. Berkay Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks
    against Machine Learning. In *Proc. of the 2017 ACM on Asia Conference on Computer
    and Communications Security (AsiaCCS 2017)*. Abu Dhabi, United Arab Emirates,
    506–519.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papernot et al. (2016a) Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt
    Fredrikson, Z. Berkay Celik, and Ananthram Swami. 2016a. The Limitations of Deep
    Learning in Adversarial Settings. In *IEEE European Symposium on Security and
    Privacy (EuroS&P 2016)*. Saarbrücken, Germany, 372–387.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papernot et al. (2016c) Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh
    Jha, and Ananthram Swami. 2016c. Distillation as a Defense to Adversarial Perturbations
    Against Deep Neural Networks. In *Proc. of the 2016 IEEE Symposium on Security
    and Privacy (SP 2016)*. San Jose, CA, USA, 582–597.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parikh et al. (2016) Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob
    Uszkoreit. 2016. A Decomposable Attention Model for Natural Language Inference.
    In *Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing
    (EMNLP 2016)*. Austin, Texas, USA, 2249–2255.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peters et al. (2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized
    Word Representations. In *Proc. of the 2018 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies
    (NAACL-HLT 2018)*. New Orleans, Louisiana, USAr, 2227–2237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raff et al. ([n. d.]) Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon,
    Bryan Catanzaro, and Charles K. Nicholas. [n. d.]. Malware Detection by Eating
    a Whole EXE. In *The Workshops of the The Thirty-Second AAAI Conference on Artificial
    Intelligence*. New Orleans, Louisiana, USA, 268–276.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riedel et al. (2010) Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010.
    Modeling Relations and Their Mentions without Labeled Text. In *Proc. of 2010
    European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD
    2010)*. Barcelona, Spain, 148–163.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rocktäschel et al. (2016) Tim Rocktäschel, Edward Grefenstette, Karl Moritz
    Hermann, Tomás Kociský, and Phil Blunsom. 2016. Reasoning about Entailment with
    Neural Attention. In *Proc. of the 2016 International Conference on Learning Representations
    (ICLR 2016)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ronen et al. (2018) Royi Ronen, Marian Radu, Corina Feuerstein, Elad Yom-Tov,
    and Mansour Ahmadi. 2018. Microsoft Malware Classification Challenge. *CoRR* abs/1802.10135
    (2018). arXiv:1802.10135 [http://arxiv.org/abs/1802.10135](http://arxiv.org/abs/1802.10135)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenberg et al. (2017) Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval
    Elovici. 2017. Generic Black-Box End-to-End Attack against RNNs and Other API
    Calls Based Malware Classifiers. *arXiv preprint arXiv:1707.05970* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roth and Yih (2004) Dan Roth and Wen-tau Yih. 2004. A Linear Programming Formulation
    for Global Inference in Natural Language Tasks. In *Proc. of the 8th Conference
    on Computational Natural Language Learning (CoNLL 2004)*. Boston, Massachusetts,
    1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samanta and Mehta (2018) Suranjana Samanta and Sameep Mehta. 2018. Generating
    Adversarial Text Samples. In *Proc. of the 40th European Conference on IR Research
    (ECIR 2018)*. Grenoble, France, 744–749.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sato et al. (2018) Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Matsumoto.
    2018. Interpretable Adversarial Perturbation in Input Embedding Space for Text.
    *arXiv preprint arXiv:1805.02917* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schuurmans and Zinkevich (2016) Dale Schuurmans and Martin Zinkevich. 2016.
    Deep Learning Games. In *Proc. of the Annual Conference on Neural Information
    Processing Systems 2016 (NIPS 2016)*. Barcelona, Spain, 1678–1686.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sennrich et al. (2017) Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra
    Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel Läubli,
    Antonio Valerio Miceli Barone, Jozef Mokry, and Maria Nadejde. 2017. Nematus:
    a Toolkit for Neural Machine Translation. In *Proc. of the 15th Conference of
    the European Chapter of the Association for Computational Linguistics (EACL 2017),
    Demo*. Valencia, Spain, 65–68.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seo et al. (2016) Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh
    Hajishirzi. 2016. Bidirectional Attention Flow for Machine Comprehension. *CoRR*
    abs/1611.01603 (2016). arXiv:1611.01603 [http://arxiv.org/abs/1611.01603](http://arxiv.org/abs/1611.01603)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serban et al. (2016) Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio,
    Aaron C. Courville, and Joelle Pineau. 2016. Building End-To-End Dialogue Systems
    Using Generative Hierarchical Neural Network Models. In *Proc. of the Thirtieth
    AAAI Conference on Artificial Intelligence (AAAI 2016)*. Phoenix, Arizona, USA,
    3776–3784.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serban et al. (2017) Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent
    Charlin, Joelle Pineau, Aaron C. Courville, and Yoshua Bengio. 2017. A Hierarchical
    Latent Variable Encoder-Decoder Model for Generating Dialogues. In *Proc. of the
    31st AAAI Conference on Artificial Intelligence (AAAI 2017)*. San Francisco, California,
    USA, 3295–3301.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2017) Baoguang Shi, Xiang Bai, and Cong Yao. 2017. An end-to-end
    trainable neural network for image-based sequence recognition and its application
    to scene text recognition. *IEEE transactions on pattern analysis and machine
    intelligence* 39, 11 (2017), 2298–2304.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2018) Haoyue Shi, Jiayuan Mao, Tete Xiao, Yuning Jiang, and Jian
    Sun. 2018. Learning Visually-Grounded Semantics from Contrastive Adversarial Samples.
    In *Proc. of the 27th International Conference on Computational Linguistics (COLING
    2018)*. Santa Fe, New Mexico, USA, 3715–3727.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. Very
    Deep Convolutional Networks for Large-Scale Image Recognition. In *Proc. of the
    3rd International Conference on Learning Representations (ICLR 2015*. San Diego,
    CA, USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2018) Sameer Singh, Carlos Guestrin, and Marco Túlio Ribeiro.
    2018. Semantically Equivalent Adversarial Rules for Debugging NLP models. In *Proc.
    of the 56th Annual Meeting of the Association for Computational Linguistics (ACL
    2018)*. Melbourne, Australia, 856–865.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive Deep
    Models for Semantic Compositionality Over a Sentiment Treebank. In *Proc. of the
    2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)*.
    Seattle, Washington, USA, 1631–1642.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song and Shmatikov (2018) Congzheng Song and Vitaly Shmatikov. 2018. Fooling
    OCR Systems with Adversarial Text Images. *CoRR* abs/1802.05385 (2018). arXiv:1802.05385
    [http://arxiv.org/abs/1802.05385](http://arxiv.org/abs/1802.05385)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2018) Mengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang, and Jiayu
    Zhou. 2018. Identify Susceptible Locations in Medical Records via Adversarial
    Attacks on Deep Predictive Models. In *Proc. of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining (KDD 2018)*. London, UK, 793–801.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
    Sequence to Sequence Learning with Neural Networks. In *Proc. of the Annual Conference
    on Neural Information Processing Systems 2014 (NIPS 2014)*. Montreal, Quebec,
    Canada, 2672–2680.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, and
    Joan Bruna. 2014. Intriguing properties of neural networks. In *Proc. of the 2nd
    International Conference on Learning Representations (ICLR 2014)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tai et al. (2015) Kai Sheng Tai, Richard Socher, and Christopher D. Manning.
    2015. Improved Semantic Representations From Tree-Structured Long Short-Term Memory
    Networks. In *Proc. of the 53rd Annual Meeting of the Association for Computational
    Linguistics (ACL 2015)*. Beijing, China, 1556–1566.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tapaswi et al. (2016) Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio
    Torralba, Raquel Urtasun, and Sanja Fidler. 2016. MovieQA: Understanding Stories
    in Movies through Question-Answering. In *Proc. of the 2016 IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR 2016)*. Las Vegas, NV, USA, 4631–4640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tesseract (2016) Tesseract. 2016. https://github.com/tesseract-ocr/.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is All you Need. In *Proc. of the Annual Conference on Neural Information Processing
    Systems 2017 (NIPS 2017)*. Long Beach, CA, USA, 6000–6010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. (2015) Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru
    Erhan. 2015. Show and tell: A neural image caption generator. In *Proc. of IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR 2015)*. Boston, MA,
    USA, 3156–3164.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2008) Canhui Wang, Min Zhang, Shaoping Ma, and Liyun Ru. 2008.
    Automatic online news issue construction in web environment. In *Proc. of the
    17th International Conference on World Wide Web (WWW 2008)*. Beijing, China, 457–466.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2011) Kai Wang, Boris Babenko, and Serge J. Belongie. 2011. End-to-end
    scene text recognition. In *Proc. of the 2011 IEEE International Conference on
    Computer Vision (ICCV 2011)*. Barcelona, Spain, 1457–1464.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Jiang (2016a) Shuohang Wang and Jing Jiang. 2016a. A compare-aggregate
    model for matching text sequences. *arXiv preprint arXiv:1611.01747* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Jiang (2016b) Shuohang Wang and Jing Jiang. 2016b. Machine Comprehension
    Using Match-LSTM and Answer Pointer. *CoRR* abs/1608.07905 (2016). arXiv:1608.07905
    [http://arxiv.org/abs/1608.07905](http://arxiv.org/abs/1608.07905)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Bansal (2018) Yicheng Wang and Mohit Bansal. 2018. Robust Machine
    Comprehension Models via Adversarial Training. In *Proc. of the 2018 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (NAACL-HLT)*. New Orleans, Louisiana, 575–581.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warde-Farley and Goodfellow (2016) David Warde-Farley and Ian Goodfellow. 2016.
    Adversarial Perturbations of Deep Neural Networks. *Perturbations, Optimization,
    and Statistics* 311 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R. Bowman.
    2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.
    In *Proc. of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics (NAACL 2018)*. New Orleans, Louisiana, USA, 1112–1122.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2017) Yi Wu, David Bamman, and Stuart Russell. 2017. Adversarial
    training for relation extraction. In *Proceedings of the 2017 Conference on Empirical
    Methods in Natural Language Processing*. 1778–1783.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad
    Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
    Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,
    Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant
    Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
    Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s Neural Machine
    Translation System: Bridging the Gap between Human and Machine Translation. *CoRR*
    abs/1609.08144 (2016). arXiv:1609.08144 [http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville,
    Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend
    and Tell: Neural Image Caption Generation with Visual Attention. In *Proc. of
    the 32nd International Conference on Machine Learning (ICML 2015)*. Lille, France,
    2048–2057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018) Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell,
    and Dawn Song. 2018. Fooling Vision and Language Models Despite Localization and
    Attention Mechanism. In *Proc. of 2018 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR 2018)*. Salt Lake City, UT, USA, 4951–4961.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2015) Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and
    Li Deng. 2015. Embedding Entities and Relations for Learning and Inference in
    Knowledge Bases. In *Proc. of the 3rd International Conference on Learning Representations
    (ICLR 2015)*. San Diego, CA, USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yannakoudakis et al. (2011) Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
    2011. A New Dataset and Method for Automatically Grading ESOL Texts. In *Proc.
    of the 49th Annual Meeting of the Association for Computational Linguistics (ACL
    2011)*. Portland, Oregon, USA, 180–189.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yasunaga et al. (2018) Michihiro Yasunaga, Jungo Kasai, and Dragomir R. Radev.
    2018. Robust Multilingual Part-of-Speech Tagging via Adversarial Training. In
    *Proc. of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics (NAACL 2018)*. New Orleans, Louisiana, USA, 976–986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young et al. (2018) Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik
    Cambria. 2018. Recent Trends in Deep Learning Based Natural Language Processing.
    *IEEE Computational Intelligence Magazine* 13, 3 (2018), 55–75.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2018) Xiaoyong Yuan, Pan He, and Xiaolin Andy Li. 2018. Adaptive
    Adversarial Attack on Scene Text Recognition. *CoRR* abs/1807.03326 (2018). arXiv:1807.03326
    [http://arxiv.org/abs/1807.03326](http://arxiv.org/abs/1807.03326)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2017) Xiaoyong Yuan, Pan He, Qile Zhu, Rajendra Rana Bhat, and
    Xiaolin Li. 2017. Adversarial Examples: Attacks and Defenses for Deep Learning.
    *CoRR* abs/1712.07107 (2017). arXiv:1712.07107 [http://arxiv.org/abs/1712.07107](http://arxiv.org/abs/1712.07107)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level
    Convolutional Networks for Text Classification. In *Proc. in Annual Conference
    on Neural Information Processing Systems 2015 (NIPS 2015)*. Montreal, Quebec,
    Canada, 649–657.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2017) Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2017. Generating
    natural adversarial examples. *arXiv preprint arXiv:1710.11342* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2016) Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei.
    2016. Visual7W: Grounded Question Answering in Images. In *Proc. of the 2016 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR 2016)*. Las Vegas,
    NV, USA, 4995–5004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
