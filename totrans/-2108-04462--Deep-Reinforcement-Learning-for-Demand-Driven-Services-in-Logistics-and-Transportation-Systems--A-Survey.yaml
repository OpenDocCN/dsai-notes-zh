- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:52:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:52:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.04462] Deep Reinforcement Learning for Demand Driven Services in Logistics
    and Transportation Systems: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.04462] 深度强化学习在需求驱动的物流与运输系统中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.04462](https://ar5iv.labs.arxiv.org/html/2108.04462)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2108.04462](https://ar5iv.labs.arxiv.org/html/2108.04462)
- en: 'Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation
    Systems: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在需求驱动的物流与运输系统中的应用：综述
- en: 'Zefang Zong, Tao Feng, Tong Xia, Depeng Jin,  and Yong Li Z. Zong, T. Feng,
    T. Xia, D. Jin, and Y. Li are with Beijing National Research Center for Information
    Science and Technology (BNRist) and with Department of Electronic Engineering,
    Tsinghua University, Beijing 100084, China. E-mail: zongzf19@mails.tsinghua.edu.cn,
    {liyong07, jindp}@tsinghua.edu.cn.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zefang Zong, Tao Feng, Tong Xia, Depeng Jin 和 Yong Li Z. Zong, T. Feng, T. Xia,
    D. Jin 和 Y. Li 现为北京国家信息科学与技术研究中心（BNRist）以及清华大学电子工程系，北京 100084，中国。电子邮件：zongzf19@mails.tsinghua.edu.cn，{liyong07,
    jindp}@tsinghua.edu.cn。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Recent technology development brings the booming of numerous new Demand-Driven
    Services (DDS) into urban lives, including ridesharing, on-demand delivery, express
    systems and warehousing. In DDS, a service loop is an elemental structure, including
    its service worker, the service providers and corresponding service targets. The
    service workers should transport either humans or parcels from the providers to
    the target locations. Various planning tasks within DDS can thus be classified
    into two individual stages: 1) Dispatching, which is to form service loops from
    demand/supply distributions, and 2)Routing, which is to decide specific serving
    orders within the constructed loops. Generating high-quality strategies in both
    stages is important to develop DDS but faces several challenging. Meanwhile, deep
    reinforcement learning (DRL) has been developed rapidly in recent years. It is
    a powerful tool to solve these problems since DRL can learn a parametric model
    without relying on too many problem-based assumptions and optimize long-term effect
    by learning sequential decisions. In this survey, we first define DDS, then highlight
    common applications and important decision/control problems within. For each problem,
    we comprehensively introduce the existing DRL solutions, and further summarize
    them in https://github.com/tsinghua-fib-lab/DDS_Survey. We also introduce open
    simulation environments for development and evaluation of DDS applications. Finally,
    we analyze remaining challenges and discuss further research opportunities in
    DRL solutions for DDS.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近期技术发展带来了大量新兴的需求驱动服务（DDS），包括共享出行、按需配送、快递系统和仓储。在 DDS 中，服务循环是基本结构，包括服务工作者、服务提供者和相应的服务目标。服务工作者应将人或包裹从提供者运输到目标地点。因此，DDS
    中的各种规划任务可以分为两个阶段：1）调度，即从需求/供应分布中形成服务循环，2）路线规划，即决定在构建的循环内的具体服务顺序。在这两个阶段生成高质量的策略对于
    DDS 的发展至关重要，但面临许多挑战。同时，深度强化学习（DRL）近年来得到了快速发展。DRL 是解决这些问题的有力工具，因为它可以在不依赖太多问题基础假设的情况下学习参数模型，并通过学习序列决策来优化长期效果。在这项综述中，我们首先定义
    DDS，然后重点介绍其中的常见应用和重要决策/控制问题。对于每个问题，我们全面介绍现有的 DRL 解决方案，并进一步总结在 https://github.com/tsinghua-fib-lab/DDS_Survey。我也介绍了开发和评估
    DDS 应用程序的开放模拟环境。最后，我们分析了剩余的挑战，并讨论了在 DDS 的 DRL 解决方案中进一步的研究机会。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep reinforcement learning, Transportation, Logistics, Dispatching, Routing,
    Order matching, Fleet management, Vehicle routing problems
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习，运输，物流，调度，路线规划，订单匹配，车队管理，车辆路径问题
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The continuous urbanization and development of mobile communication have brought
    many new application demands into urban daily lives. Among all, the services that
    transport either humans or parcels to provided destinations following individual
    demands or system requirements are critical in both urban logistics and transportation
    nowadays. We define such kinds of services as Demand-Driven Services (DDS). For
    example, the on-demand food delivery service as a typical DDS is widely used since
    it improves diet convenience significantly. More than 30 million orders are generated
    every day on the Meituan-Dianping platform, one of the world’s largest on-demand
    delivery service providers [[1](#bib.bib1)]. As another example, large-scale online
    ridesharing services such as Uber and DiDi have substantially transformed the
    transportation landscape, offering huge opportunities for boosting the current
    transportation efficiency. These DDS applications provide striking efficiency
    to city operations in both logistics and transportation as well as many opportunities
    to related research fields. Intelligent control with minimal manual intervention
    upon DDS systems is critical to guarantee their effectiveness and has drawn many
    research interests.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 持续的城市化和移动通信的发展带来了许多新的应用需求，进入了城市日常生活。其中，按照个人需求或系统要求将人或包裹运输到指定目的地的服务在当今城市物流和运输中至关重要。我们将这种服务定义为需求驱动服务（DDS）。例如，按需配送服务作为典型的DDS被广泛使用，因为它显著提高了饮食便利性。每一天在美团点评平台上生成的订单超过3000万，这是全球最大的按需配送服务提供商之一[[1](#bib.bib1)]。另一个例子是，大规模的在线共享出行服务，如Uber和滴滴，已经显著改变了交通格局，为提升当前的交通效率提供了巨大的机会。这些DDS应用在城市运营中提供了显著的效率提升，同时也为相关研究领域提供了许多机会。对DDS系统进行智能控制，尽量减少人工干预，对于保证其有效性至关重要，并引起了许多研究兴趣。
- en: 'In a typical DDS task, there are several roles involved that an implemented
    system should consider, including the service workers, service providers, and
    corresponding targets. For example, in an on-demand delivery system, a man who
    orders food can be seen as a service target, while the restaurant from which the
    food is ordered is the service provider. A group of such delivery tasks is then
    assigned to and accomplished by a courier, i.e., the worker. These core DDS elements
    form a DDS loop, and such an example is illustrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Deep Reinforcement Learning for Demand Driven Services in
    Logistics and Transportation Systems: A Survey"). The same formulation can also
    be constructed within the ride-sharing scenario. Each customer who calls for a
    ride has his/her destination as the target, and the driver serves as the service
    worker. The DDS platforms that support either delivery or ride-sharing services
    are supposed to provide corresponding algorithms to 1) construct reasonable service
    loops and 2) conduct workers to complete assignments within loops. We show DDS
    loop formulations in several typical scenarios in Table [I](#S1.T1 "TABLE I ‣
    1 Introduction ‣ Deep Reinforcement Learning for Demand Driven Services in Logistics
    and Transportation Systems: A Survey"), including on-demand delivery, ridesharing,
    express systems, and warehousing.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '在一个典型的DDS任务中，涉及多个角色，实施的系统应考虑这些角色，包括服务工作者、服务提供者和相应的目标。例如，在一个按需配送系统中，订餐的人可以被视为服务目标，而餐厅则是服务提供者。这些配送任务会被分配给快递员，即工作者来完成。这些核心DDS元素形成了一个DDS循环，这种示例在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep Reinforcement Learning for Demand Driven Services
    in Logistics and Transportation Systems: A Survey")中进行了说明。在共享出行场景中也可以构建相同的模型。每个叫车的顾客有其目的地作为目标，而司机则充当服务工作者。支持配送或共享出行服务的DDS平台应提供相应的算法，以1)
    构建合理的服务循环和2) 指导工作者在循环中完成任务。我们在表[I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Reinforcement
    Learning for Demand Driven Services in Logistics and Transportation Systems: A
    Survey")中展示了多个典型场景下的DDS循环模型，包括按需配送、共享出行、快递系统和仓储。'
- en: '|   DDS Scenario | DDS Loop |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|   DDS场景 | DDS循环 |'
- en: '| Service Provider | Service Target | Service Worker |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 服务提供者 | 服务目标 | 服务工作者 |'
- en: '| On-demand Delivery | Restaurant | Customer | Courier |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 按需配送 | 餐厅 | 顾客 | 快递员 |'
- en: '| Ridesharing | Passenger Origin | Destination | Driver |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 共享出行 | 乘客起点 | 目的地 | 司机 |'
- en: '| Express (Sending) | Consignor | Depot | Courier |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 快递（寄件） | 发件人 | 仓库 | 快递员 |'
- en: '| Express (Delivery) | Depot | Consignee | Courier |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 快递（配送） | 仓库 | 收货人 | 快递员 |'
- en: '| Warehousing | Shelf, Entry, Station | Shelf, Entry, Station | AGV |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 仓储 | 货架、入口、车站 | 货架、入口、车站 | AGV |'
- en: '|   |  |  |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: 'TABLE I: Elements in a DDS loop of several typical DDS scenarios, including
    on-demand delivery, ridesharing, express systems and warehousing. AGV is the abbreviation
    for Autonomous Guiding Vehicle.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：多个典型DDS场景（包括按需配送、拼车、快递系统和仓储）的DDS循环中的元素。AGV是自主引导车辆的缩写。
- en: '![Refer to caption](img/7a28d3b30637e7cb669cfb0e3a872ee2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7a28d3b30637e7cb669cfb0e3a872ee2.png)'
- en: 'Figure 1: The visualization of two independent service loops using instant
    delivery as an example. The restaurant, customer, and courier serve as the service
    provider, service target, and service worker respectively.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用即时配送作为示例的两个独立服务循环的可视化。餐馆、客户和快递员分别充当服务提供者、服务目标和服务工作者。
- en: With fundamental DDS elements defined, how to manage different service demand
    pairs (providers and targets), schedule available service workers and control
    the entire service system become the major objectives of developing a centralized
    intelligent DDS platform. Major research problems can be classified into two aspects.
    First, forming DDS loops upon demand pairs and workers, which is also named Dispatching
    , is the first-hand challenge to deal with. The loop forming process, i.e., the
    Matching between demands and workers can be originated from the traditional bipartite
    graph matching problem, while the dynamic features in the entire environment bring
    much more complexity. A good dispatching mechanism should not only consider the
    current states of workers with scattered demands but also take future distributions
    into account for long-term optimization. Furthermore, even a worker is not matched
    with service demands at present, there still remains a large action space to arrange
    the idling workers into other areas, which forms the Fleet Management problem.
    The loop-forming stage can be seen as the first stage for the complete DDS.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了基本的DDS元素后，如何管理不同的服务需求对（提供者和目标），调度可用的服务工作者并控制整个服务系统成为开发集中智能DDS平台的主要目标。主要研究问题可以分为两个方面。首先，围绕需求对和工作者形成DDS循环，也称为调度，是需要解决的首要挑战。循环形成过程，即需求与工作者之间的匹配，可以源于传统的二分图匹配问题，而整个环境中的动态特征带来了更多复杂性。一个好的调度机制不仅要考虑工作者的当前状态和分散的需求，还要考虑未来的分布以实现长期优化。此外，即使一个工作者目前未与服务需求匹配，仍然存在大量的操作空间将空闲工作者安排到其他区域，这形成了车队管理问题。循环形成阶段可以被视为完整DDS的第一个阶段。
- en: 'Second, after being assigned with numerous demands to satisfy, how to execute
    formed loops, i.e., to schedule the detailed Routing strategies including planning
    the visiting orders of the demand set and the selection on real-world road maps
    is also critical to determine the entire system efficiency. The routing problem
    can be originated from the conventional Traveling Salesman Problem  [[2](#bib.bib2)],
    where a salesman is supposed to visit all cities without revisiting anyone of
    them. The further Vehicle Routing Problems (VRP) [[3](#bib.bib3)], and its variants
    are valuable in the mathematical formulation of most real-world routing scenarios [[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]. A high-quality routing strategy
    should minimize the total traveling distance to decrease the expenses of the workers.
    The routing stage can be seen as the second stage after dispatching. A robust
    and stable routing strategy generation is also important to provide decision information
    back to the dispatching stage. We illustrate the relationship between the two
    stages in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Reinforcement Learning
    for Demand Driven Services in Logistics and Transportation Systems: A Survey").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在被分配大量需求后，如何执行已形成的循环，即调度详细的路径规划策略，包括计划需求集的访问顺序和对实际道路地图的选择，也是决定整个系统效率的关键。路径规划问题可以源于传统的旅行商问题[[2](#bib.bib2)]，其中一个销售员需要访问所有城市而不重复访问其中任何一个。进一步的车辆路径规划问题（VRP）[[3](#bib.bib3)]及其变体在大多数实际路径规划场景的数学表述中具有重要价值[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]。高质量的路径规划策略应最小化总旅行距离，以减少工作者的开支。路径规划阶段可以被视为调度之后的第二阶段。生成稳健且稳定的路径规划策略也很重要，以将决策信息反馈给调度阶段。我们在图[2](#S1.F2
    "图 2 ‣ 1 介绍 ‣ 物流与运输系统中的需求驱动服务：综述")中说明了两个阶段之间的关系。
- en: '![Refer to caption](img/47d7f982a49b2c773e039a796f1026f1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/47d7f982a49b2c773e039a796f1026f1.png)'
- en: 'Figure 2: The overview of DDS problems, including the dispatching stage and
    the routing stage. We demonstrate the transformation from originated mathematical
    formulation to industrial applications in the vertical axis, and distinguish the
    two different planning stages in the horizontal axis. Note that the two stages
    are not rigidly separated, but such a classification is necessary to concentrate
    on primary challenges in different practical scenarios. A low demand/worker ratio
    implies that the primary challenge is to determine how workers and demands should
    be matched, while a large one indicates that the major optimization space lies
    in the routing stage. We will discuss such a relationship in details in Sec [2](#S2
    "2 Background ‣ Deep Reinforcement Learning for Demand Driven Services in Logistics
    and Transportation Systems: A Survey").3.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：DDS 问题的概述，包括调度阶段和路线规划阶段。我们在纵轴上展示了从原始数学公式到工业应用的转化，并在横轴上区分了两个不同的规划阶段。请注意，这两个阶段并不是严格分开的，但这样的分类有助于集中关注不同实际场景中的主要挑战。低需求/工人比例意味着主要挑战在于确定如何匹配工人与需求，而较大的比例则表明主要的优化空间在于路线规划阶段。我们将在第[2](#S2
    "2 背景 ‣ 基于深度强化学习的需求驱动服务在物流和运输系统中的应用：综述")节中详细讨论这种关系。
- en: The solutions to the mathematical formulations of both two stages were widely
    studied previously. For instance, the Kuhn-Munkres (KM) algorithm for Bipartite-Graph
    Matching and Branch-and-Bound for TSP and VRP could provide exact solutions for
    simple static problems with limited scales [[8](#bib.bib8), [9](#bib.bib9)]. Considering
    multiple real-world constraints and additional factors, more complicated dispatching
    and routing problems are also further investigated extensively in the field of
    operations research, applied maths, etc [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)]. In complicated scenarios with larger problem scales, exact
    optimizations are almost impossible to obtain. Meanwhile, heuristics and meta-heuristics
    were widely accepted as an alternative to generate approximate solutions within
    a much more reasonable time in both stages of DDS [[14](#bib.bib14), [15](#bib.bib15)].
    These heuristics-based methods could generate satisfactory solutions in online
    scenarios and are thus practical in many real-world DDS systems. However, there
    is still much potential in exploring better solutions with higher quality, higher
    efficiency on larger scales.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 两个阶段的数学公式的解决方案之前已广泛研究。例如，Kuhn-Munkres (KM) 算法用于二分图匹配，分支限界法用于旅行商问题（TSP）和车辆路径问题（VRP），可以为简单的静态问题提供精确解[[8](#bib.bib8),
    [9](#bib.bib9)]。考虑到多种现实世界的约束和额外因素，更复杂的调度和路线规划问题在运筹学、应用数学等领域也得到了广泛的研究[[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]。在规模较大的复杂场景中，几乎不可能获得精确的优化解。与此同时，启发式和元启发式方法被广泛接受为一种替代方案，以在这两个
    DDS 阶段内在更合理的时间内生成近似解[[14](#bib.bib14), [15](#bib.bib15)]。这些基于启发式的方法可以在在线场景中生成令人满意的解决方案，因此在许多现实世界的
    DDS 系统中是实用的。然而，在更大规模下探索更高质量、更高效率的解决方案仍有很大潜力。
- en: As machine learning showing astonishing performance in recent years, it is of
    great potential to utilize the learning-based techniques to further develop DDS
    systems. Reinforcement Learning (RL) methods have developed and been applied in
    many planning tasks [[16](#bib.bib16)]. RL could generate strategies by modeling
    a decision process as a Markov Decision Process (MDP). A predefined reward from
    a long-term perspective works as the feedback signal to any action attempts so
    that RL can optimize sequential decisions. The trial-and-error process could train
    the agent to learn to select the best action corresponding to different inner
    states and outside environments. As deep neural networks providing much stronger
    ability on feature representative and pattern recognition, combining neural networks
    and RL shows great performances [[17](#bib.bib17)]. Many deep RL (DRL) algorithms
    are further proposed and become state-of-the-art frameworks in control and scheduling
    tasks. DRL does not have to rely on manually designed assumptions and features
    by training a parameterized model to learn the optimal control. It is trivial
    to consider using it as the structure for solving the series of planning tasks
    in DDS.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于近年来机器学习展现出了惊人的性能，利用基于学习的技术进一步发展DDS系统具有巨大的潜力。强化学习（RL）方法已经在许多规划任务中得到了发展和应用[[16](#bib.bib16)]。RL通过将决策过程建模为马尔可夫决策过程（MDP）来生成策略。从长期角度来看，预定义的奖励作为对任何行动尝试的反馈信号，以便RL可以优化连续决策。试错过程可以训练代理选择与不同内部状态和外部环境相对应的最佳行动。由于深度神经网络在特征表示和模式识别上提供了更强大的能力，将神经网络与RL结合显示出了卓越的性能[[17](#bib.bib17)]。许多深度RL（DRL）算法被进一步提出，并成为控制和调度任务中的最新框架。DRL不必依赖于手动设计的假设和特征，通过训练一个参数化模型来学习最优控制。考虑将其作为解决DDS中一系列规划任务的结构是显而易见的。
- en: 'In this survey, we focus on how DRL can benefit to the development of DDS systems
    in both the dispatching stage and the routing stage respectively. We first introduce
    major DRL algorithms and four typical DDS in urban operations. Then we summarize
    existing DRL based solutions according to the following dimensions:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们重点关注DRL如何在调度阶段和路线阶段分别促进DDS系统的发展。我们首先介绍主要的DRL算法和城市操作中的四种典型DDS。然后，我们根据以下维度总结现有的基于DRL的解决方案：
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Problem. We classify the research problems in both dispatching and routing stages
    into preciser sub-problems. Order dispatching along with fleet management included
    in the dispatching stage are investigated. As for numerous DRL solutions for the
    routing stage, we first introduce the ones solving typical Capacitied VRP (CVRP)
    as mathematical solutions, while more practical routing solutions for VRP variants
    are also discussed. We consider four variant problems with additional constraints
    in this survey, including dynamic VRP (DVRP), Electric VRP (EVRP), VRP with Time
    Windows (VRPTW) and VRP with pickup and delivery (VRPPD).
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题。我们将调度和路线阶段的研究问题细分为更精确的子问题。调度阶段包括订单调度和车队管理的研究。至于路线阶段的众多DRL解决方案，我们首先介绍解决典型的容量VRP（CVRP）的数学解决方案，同时还讨论了针对VRP变体的更实际的路线解决方案。在本调查中，我们考虑了四种具有额外约束的变体问题，包括动态VRP（DVRP）、电动VRP（EVRP）、带时间窗口的VRP（VRPTW）和带取件和交付的VRP（VRPPD）。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scenario. The aforementioned research problems exist in several applicable scenarios,
    and four common DDS scenarios are included in this survey. In transportation systems,
    we introduce ridesharing services, where vehicles are assigned to transport passengers
    to their destinations. Specifically, ridesharing can be further classified into
    ride-hailing, where each driver serves only one passenger in a loop, and ride-pooling,
    where multiple passengers can share a ride at the same time. As for the logistic
    systems where parcels are transported from providers to targets, we summarize
    solutions for both on-demand delivery systems that fulfill people’s instant demands
    and traditional express systems with longer service duration. We also introduce
    modern warehousing systems where Autonomous Guiding Vehicles (AGVs) transport
    parcels from locations to another. Note that some important literature providing
    solutions within a mathematical formulation is also included [[18](#bib.bib18),
    [19](#bib.bib19)].
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景。上述研究问题存在于几个适用场景中，本调查包括了四种常见的DDS场景。在交通系统中，我们介绍了共享出行服务，其中车辆被分配去将乘客送到目的地。具体来说，共享出行可以进一步分为打车服务，每位司机仅为一个乘客提供循环服务，以及拼车服务，多个乘客可以同时共享一辆车。至于物流系统中包裹从供应商运输到目标的情况，我们总结了两种解决方案，一种是满足人们即时需求的按需配送系统，另一种是服务时间较长的传统快递系统。我们还介绍了现代仓储系统，其中自主导引车辆（AGVs）将包裹从一个地点运输到另一个地点。需要注意的是，一些提供数学公式解决方案的重要文献也被包括在内[[18](#bib.bib18),
    [19](#bib.bib19)]。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Algorithm. We distinguish the detailed RL algorithm used during model training.
    Most commonly used ones in existing works belong to model-free RL methods, including
    DQN [[17](#bib.bib17)], PPO [[20](#bib.bib20)], REINFORCE [[21](#bib.bib21)],
    etc. We also discuss whether the DDS task is constructed as a single agent MDP
    or a multi-agent one.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 算法。我们区分了模型训练过程中使用的详细RL算法。现有工作中最常用的算法属于无模型RL方法，包括DQN[[17](#bib.bib17)]、PPO[[20](#bib.bib20)]、REINFORCE[[21](#bib.bib21)]等。我们还讨论了DDS任务是否被构建为单代理MDP或多代理MDP。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Network Structure. We also distinguish the neural network design in each literature.
    Commonly used networks include Convolutional Neural Networks (CNN), Graph Neural
    Networks (GNN) and its variants (including GCN and others), attention (ATT) based
    networks and its variants (including single$\verb|/|$multi head attentions).
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络结构。我们还区分了文献中的神经网络设计。常用的网络包括卷积神经网络（CNN）、图神经网络（GNN）及其变体（包括GCN等）、基于注意力（ATT）的网络及其变体（包括单头$\verb|/|$多头注意力）。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Data Type and Data Scheme. We indicate the data type used in each literature,
    either real-world or generated based on pre-defined random seeds with a given
    distribution. Meanwhile, spatial locations of data are utilized in several ways
    for simplification to a different extent. Generally, there are four data schemes
    originated from the real road networks as follows: 4-way connectivity with cardinal
    directions, 8-way connectivity with ordinal directions, 6-way connectivity based
    on hexagon-grids, and original discrete graph-based structure. Note that the first
    two can also be summarized as square-grids. Different data schemes are shown in
    Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Reinforcement Learning for
    Demand Driven Services in Logistics and Transportation Systems: A Survey").'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据类型和数据方案。我们指出了每篇文献中使用的数据类型，既包括基于预定义随机种子和给定分布生成的真实数据，也包括基于预定义随机种子生成的虚拟数据。同时，数据的空间位置以不同程度的简化方式被利用。一般来说，来自真实道路网络的数据方案有四种，如下所示：具有基本方向的4向连接、具有序数方向的8向连接、基于六边形网格的6向连接，以及原始的离散图结构。需要注意的是，前两种也可以总结为方格网。不同的数据方案见图[3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Deep Reinforcement Learning for Demand Driven Services
    in Logistics and Transportation Systems: A Survey")。'
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data and Code Availability. To present the extent of reproducibility of the
    investigated literature, we report the data availability of the proposed methods.
    A checkmark means that the data is released by the researchers or could be easily
    found via a direct web search. We also report the availability of the code. Both
    original open-sourced codes and re-implementation from the third party are considered.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据和代码可用性。为了展示所调查文献的可重复性，我们报告了所提出方法的数据可用性。一个勾号表示数据由研究人员发布，或可以通过直接的网络搜索轻松找到。我们还报告了代码的可用性。既考虑了原始的开源代码，也考虑了第三方的重新实现。
- en: Besides, we also introduce the available simulation environments for DDS, which
    is critical to simulate real-world scenarios with much fewer expenses. Finally,
    several challenges of using DRL to solve DDS and remaining open research problems
    are summarized.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还介绍了 DDS 可用的模拟环境，这对以更少的成本模拟现实世界场景至关重要。最后，总结了使用 DRL 解决 DDS 的几个挑战以及仍然存在的研究问题。
- en: 'Previous literature investigating relative problems includes surveys by Haydari
    et al. [[22](#bib.bib22)], Qin et al. [[23](#bib.bib23)] and several reviews on
    VRP [[24](#bib.bib24)]. However, Haydari et al. [[22](#bib.bib22)] focused on
    the general planning problems in Intelligent Transportation Systems from where
    Transportation Signal Control (TSC) and Autonomous Driving are emphasized. Qin
    et al. [[23](#bib.bib23)] only investigated the dispatching problems in ridesharing
    scenarios, and Mazyavkina et al. [[24](#bib.bib24)] introduced DRL solutions on
    mathematical VRPs included in more general combinatorial optimizations. In contrast,
    we are the first to define DDS from a practical system level and classify specific
    research problems in several scenarios with DRL-based solutions. We investigate
    how DRL can benefit to its development. The two stages of DDS are discussed, including
    dispatching that forms service loops and routing that executes services loops.
    The related literature is summarized in Table [II](#S3.T2 "TABLE II ‣ 3.1 Order
    Matching ‣ 3 Stage 1: Dispatching ‣ Deep Reinforcement Learning for Demand Driven
    Services in Logistics and Transportation Systems: A Survey") and Table [III](#S4.T3
    "TABLE III ‣ 4.1 Formulation of Typical CVRP ‣ 4 Stage2: Routing ‣ Deep Reinforcement
    Learning for Demand Driven Services in Logistics and Transportation Systems: A
    Survey").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '以往文献中探讨相关问题的综述包括 Haydari 等人[[22](#bib.bib22)]、Qin 等人[[23](#bib.bib23)]和关于 VRP
    的若干评论[[24](#bib.bib24)]。然而，Haydari 等人[[22](#bib.bib22)] 关注的是智能交通系统中的一般规划问题，其中强调了交通信号控制
    (TSC) 和自动驾驶。Qin 等人[[23](#bib.bib23)] 仅研究了共享出行场景中的调度问题，而 Mazyavkina 等人[[24](#bib.bib24)]
    引入了数学 VRP 的 DRL 解决方案，这些问题包含在更一般的组合优化中。相比之下，我们首次从实际系统层面定义 DDS，并在多个场景中分类特定的研究问题，并提出基于
    DRL 的解决方案。我们调查了 DRL 如何促进其发展。讨论了 DDS 的两个阶段，包括形成服务循环的调度和执行服务循环的路由。相关文献总结见表[II](#S3.T2
    "TABLE II ‣ 3.1 Order Matching ‣ 3 Stage 1: Dispatching ‣ Deep Reinforcement Learning
    for Demand Driven Services in Logistics and Transportation Systems: A Survey")和表[III](#S4.T3
    "TABLE III ‣ 4.1 Formulation of Typical CVRP ‣ 4 Stage2: Routing ‣ Deep Reinforcement
    Learning for Demand Driven Services in Logistics and Transportation Systems: A
    Survey")。'
- en: 'Overall, this paper presents a comprehensive survey on DRL techniques for solving
    planning problems in DDS systems. Our contributions can be summarized as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，本文对解决 DDS 系统中的规划问题的 DRL 技术进行了全面的综述。我们的贡献可以总结如下：
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first comprehensive survey that thoroughly
    defines and investigates DDS systems and up-to-date DRL techniques as solutions.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一次全面定义和研究 DDS 系统及最新 DRL 技术作为解决方案的综述。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We classify different stages within a complete DDS system, including the dispatching
    stage and the routing stage. We also investigate the common applications corresponding
    to the two stages, introduce the theoretical background of DRL from a broad perspective
    and explain several important algorithms.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对完整的 DDS 系统中的不同阶段进行分类，包括调度阶段和路由阶段。我们还探讨了这两个阶段对应的常见应用，从广泛的角度介绍了 DRL 的理论背景，并解释了几个重要的算法。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We investigate existing works that utilize DRL for DDS systems. We summarize
    these works in several dimensions and discuss the individual approaches.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们调查了利用 DRL 处理 DDS 系统的现有工作。我们从几个维度总结了这些工作，并讨论了各自的方法。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We illustrate the challenges and several open problems in DDS problems using
    DRL. We believe the summarized research directions will benefit relevant research
    and help to direct future work.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过 DRL 说明了 DDS 问题中的挑战和若干开放问题。我们相信，总结出的研究方向将有助于相关研究，并为未来的工作提供指导。
- en: 'The remaining survey is organized as follows. We first introduce the background
    of this survey, including DRL and four common DDS scenarios in Sec  [2](#S2 "2
    Background ‣ Deep Reinforcement Learning for Demand Driven Services in Logistics
    and Transportation Systems: A Survey"). The stage definition and more specific
    problems with corresponding solutions of both dispatching and routing are summarized
    in Sec [3](#S3 "3 Stage 1: Dispatching ‣ Deep Reinforcement Learning for Demand
    Driven Services in Logistics and Transportation Systems: A Survey") and Sec  [4](#S4
    "4 Stage2: Routing ‣ Deep Reinforcement Learning for Demand Driven Services in
    Logistics and Transportation Systems: A Survey") respectively. The commonly used
    simulation environments for both stages are introduced in Sec [5](#S5 "5 Open
    Simulators and Datasets for DDS ‣ Deep Reinforcement Learning for Demand Driven
    Services in Logistics and Transportation Systems: A Survey"). Then we summarize
    several challenges of DRL for DDS design and open research problems in Sec [6](#S6
    "6 Challenges ‣ Deep Reinforcement Learning for Demand Driven Services in Logistics
    and Transportation Systems: A Survey") and Sec [7](#S7 "7 Open Problems ‣ Deep
    Reinforcement Learning for Demand Driven Services in Logistics and Transportation
    Systems: A Survey"). Finally, we summarize this survey in Sec [8](#S8 "8 Conclusion
    ‣ Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation
    Systems: A Survey").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余部分的调查组织如下。我们首先介绍本调查的背景，包括 DRL 和四种常见的 DDS 场景，见第 [2](#S2 "2 背景 ‣ 需求驱动服务中的深度强化学习：物流与运输系统的调查")
    节。调度和路线的阶段定义及更具体的问题和相应的解决方案分别总结在第 [3](#S3 "3 阶段 1：调度 ‣ 需求驱动服务中的深度强化学习：物流与运输系统的调查")
    节和第 [4](#S4 "4 阶段 2：路线 ‣ 需求驱动服务中的深度强化学习：物流与运输系统的调查") 节。两阶段中常用的模拟环境在第 [5](#S5 "5
    开放模拟器和数据集 ‣ 需求驱动服务中的深度强化学习：物流与运输系统的调查") 节介绍。然后，我们在第 [6](#S6 "6 挑战 ‣ 需求驱动服务中的深度强化学习：物流与运输系统的调查")
    节和第 [7](#S7 "7 开放问题 ‣ 需求驱动服务中的深度强化学习：物流与运输系统的调查") 节总结了 DRL 在 DDS 设计中的若干挑战和开放研究问题。最后，在第
    [8](#S8 "8 结论 ‣ 需求驱动服务中的深度强化学习：物流与运输系统的调查") 节总结了本调查。
- en: '![Refer to caption](img/f4efd205a943ea3c6def73eb210c4b55.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f4efd205a943ea3c6def73eb210c4b55.png)'
- en: 'Figure 3: A sample of different grid-based navigation and partitioning schemes:
    (a) 4-way connectivity through cardinal directions, (b) 8-way connectivity with
    ordinal directions, (c) 6-way connectivity using hexagon-based representations.
    (d) Full connectivity, can also be modeled as a graph structure.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同网格基础导航和分区方案的示例：（a）通过基本方向的 4 路连通，（b）带有序方向的 8 路连通，（c）使用六边形表示的 6 路连通。（d）完全连通，也可以建模为图结构。
- en: 2 Background
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: '![Refer to caption](img/8230742845b9a9bd22242b4b17bbbd1b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8230742845b9a9bd22242b4b17bbbd1b.png)'
- en: 'Figure 4: Reinforcement learning control loop.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：强化学习控制循环。
- en: '![Refer to caption](img/3e6194e64dab60e860c112c2c4c8f3ea.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3e6194e64dab60e860c112c2c4c8f3ea.png)'
- en: 'Figure 5: Classification and development of DRL algorithms. The solid arrow
    indicates the category attribution, and the dashed arrow indicates the development
    of the method.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：DRL 算法的分类与发展。实箭头表示类别归属，虚箭头表示方法的发展。
- en: 2.1 Reinforcement Learning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 强化学习
- en: RL is a kind of learning that maps from environmental state to action. The goal
    is to enable the agent to obtain the largest cumulative reward in the process
    of interacting with the environment [[25](#bib.bib25)]. Usually, the Markov Decision
    Process (MDP) can be used to model RL problems. There are several core elements
    within RL under an MDP setting, including the agent, the environment, the state,
    the action, reward, and transition. We drew Figure 4 to represent the reinforcement
    learning control loop and the detailed descriptions are as follows,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: RL 是一种从环境状态映射到动作的学习方式。其目标是使代理在与环境互动的过程中获得最大的累积奖励 [[25](#bib.bib25)]。通常，马尔可夫决策过程
    (MDP) 可以用来建模 RL 问题。在 MDP 设置下，RL 有几个核心元素，包括代理、环境、状态、动作、奖励和转移。我们绘制了图 4 来表示强化学习控制循环，详细描述如下，
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Environment. The environment of DRL is the fundamental setting that provides
    basic information from exogenous dynamics.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 环境。DRL 的环境是提供来自外部动态的基本信息的基本设置。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Agent. The agent in the RL is supposed to provide actions and interact with
    the entire environment. There could be even more than one agent, which further
    forms the multi-agent RL setting.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理。RL 中的代理应提供动作并与整个环境进行互动。甚至可能有多个代理，这进一步形成了多代理 RL 设置。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: State. $S$ is the set of all environmental states. By modeling the planning
    task as an MDP as the prior, the state of the agent, $s_{t}\in S$, at decision
    step $t$ describes the latest situation. The state of the agent serves as the
    endogenous feature that influences the decision making.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态。$S$ 是所有环境状态的集合。通过将规划任务建模为 MDP 作为先验，代理在决策步骤 $t$ 的状态 $s_{t}\in S$ 描述了最新情况。代理的状态作为影响决策的内生特征。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Action. $A$ is the set of executable actions of the agent. The action, $a_{t}$
    is the way that agents interact with the environment at decision step $t$. Any
    action could influence the current state of the agent.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动作。$A$ 是代理可执行动作的集合。动作 $a_{t}$ 是代理在决策步骤 $t$ 时与环境互动的方式。任何动作都可能影响代理的当前状态。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reward. $f:S\times A\rightarrow R$ is the reward function. By continuously carrying
    out actions to change states, the agent will finally obtain the corresponding
    reward, $r_{t}\sim f(s_{t},a_{t})$ that is related to the task which is obtained
    by the agent performing the action $a_{t}$ in the state $s_{t}$ at decision step
    $t$. With $R$ as the task signal, the entire training process of RL is to obtain
    a high reward, which represents how successful the agent is in completing the
    given task.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励。$f:S\times A\rightarrow R$ 是奖励函数。通过不断执行动作以改变状态，代理最终将获得与任务相关的相应奖励 $r_{t}\sim
    f(s_{t},a_{t})$，该奖励是代理在决策步骤 $t$ 时在状态 $s_{t}$ 执行动作 $a_{t}$ 后获得的。以 $R$ 作为任务信号，RL
    的整个训练过程是为了获得高奖励，这代表代理在完成给定任务上的成功程度。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transition. $p:S\times A\times S\rightarrow[0,1]$ is the state transition probability
    distribution function. $s_{t+1}\times p(s_{t},a_{t})$ represents the probability
    that the agent performs the action at in the state $s_{t}$ and transits to the
    next state $s_{t+1}$.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转移。$p:S\times A\times S\rightarrow[0,1]$ 是状态转移概率分布函数。$s_{t+1}\times p(s_{t},a_{t})$
    表示代理在状态 $s_{t}$ 执行动作 $a_{t}$ 并转移到下一个状态 $s_{t+1}$ 的概率。
- en: In RL, policy $\pi:S\rightarrow A$ is a mapping from state space to action space.
    It means that the agent selects an action with state $s_{t}$, executes the action
    $a_{t}$ and transits to the next state $s_{t+1}$ with probability $p(s_{t},a_{t})$,
    and receives the reward $r_{t}$ from environmental feedback at the same time.
    Assuming that the immediate reward obtained at each time step in the future must
    be multiplied by a discount factor $\gamma$. From the time $t$ to the end of the
    episode at time $T$, the cumulative reward is defined as $R_{t}=\sum_{t^{{}^{\prime}}=t}^{T}\gamma^{t^{{}^{\prime}}-t}r_{t^{{}^{\prime}}}$,
    where $\gamma\in[0,1]$, which is used to weigh the impact of future rewards.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RL 中，策略 $\pi:S\rightarrow A$ 是从状态空间到动作空间的映射。这意味着代理在状态 $s_{t}$ 下选择一个动作，执行动作
    $a_{t}$ 并以概率 $p(s_{t},a_{t})$ 转移到下一个状态 $s_{t+1}$，同时从环境反馈中获得奖励 $r_{t}$。假设未来每个时间步骤获得的即时奖励必须乘以折扣因子
    $\gamma$。从时间 $t$ 到在时间 $T$ 的 episode 结束，累积奖励定义为 $R_{t}=\sum_{t^{{}^{\prime}}=t}^{T}\gamma^{t^{{}^{\prime}}-t}r_{t^{{}^{\prime}}}$，其中
    $\gamma\in[0,1]$，用于权衡未来奖励的影响。
- en: The state action value function $Q^{\pi}(s,a)$ refers to the cumulative reward
    obtained by the agent during the process of executing action $a$ in the current
    state $s$ and following the strategy $\pi$ until the end of the episode, which
    can be expressed as $Q^{\pi}(s,a)=E[R_{t}|s_{t}=s,a_{t}=a,\pi]$. For all state-action
    pairs, if the expected return of one policy $\pi^{\ast}$ is greater than or equal
    to the expected return of all other policies, then policy $\pi^{\ast}$ is called
    the optimal strategy. There may be more than one optimal policy, but they share
    a state-action value function $Q^{\ast}(s,a)=max_{\pi}E[R_{t}|s_{t}=s,a_{t}=a,\pi]$,
    which is called the optimal state-action value function. Such a function follows
    the Bellman optimality equation, $Q^{\ast}(s,a)=E_{s^{{}^{\prime}}\sim S}[r+\gamma
    max_{a^{{}^{\prime}}}Q(s^{{}^{\prime}},a^{{}^{\prime}})|s,a].$
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 状态动作值函数$Q^{\pi}(s,a)$指的是代理在当前状态$s$下执行动作$a$并按照策略$\pi$执行直到回合结束过程中获得的累计奖励，可以表示为$Q^{\pi}(s,a)=E[R_{t}|s_{t}=s,a_{t}=a,\pi]$。对于所有状态-动作对，如果某策略$\pi^{\ast}$的期望回报大于或等于所有其他策略的期望回报，则策略$\pi^{\ast}$称为最优策略。可能存在多个最优策略，但它们共享一个状态-动作值函数$Q^{\ast}(s,a)=\max_{\pi}E[R_{t}|s_{t}=s,a_{t}=a,\pi]$，这个函数称为最优状态-动作值函数。这样的函数遵循贝尔曼最优性方程$Q^{\ast}(s,a)=E_{s^{{}^{\prime}}\sim
    S}[r+\gamma \max_{a^{{}^{\prime}}}Q(s^{{}^{\prime}},a^{{}^{\prime}})|s,a]$。
- en: 'In traditional RL, solving the $Q$ value function is generally through iterating
    the Bellman equation $Q_{i+1}(s,a)=E_{s^{{}^{\prime}}\sim S}[r+\gamma max_{a^{{}^{\prime}}}Q_{i}(s^{{}^{\prime}},a^{{}^{\prime}})|s,a],$
    Through continuous iteration, the state-action value function will eventually
    converge, thereby obtaining the optimal strategy: $\pi^{\ast}=argmax_{a\in A}Q^{\ast}(s,a)$.
    However, for practical problems, such a process to search for an optimal strategy
    is not feasible, since the computation cost of iterating the Bellman equation
    grows rapidly due to the large state space. To tackle such a problem, deep learning
    (DL) is introduced in RL to form deep reinforcement learning (DRL), which utilizes
    deep neural networks for function approximation in the traditional RL model and
    significantly improves the performances of many challenging applications [[26](#bib.bib26),
    [17](#bib.bib17), [27](#bib.bib27)]. In general, an RL agent can act in two ways:
    (1) by knowing/modeling state transition, which is called model-based RL, and
    (2) by interacting with the environment without modeling a transition model, which
    is called model-free RL. Model-free RL algorithms include two categories of algorithms:
    value-based methods and policy-based methods. In value-based RL, an agent learns
    the value function of a state-action pair and then selects actions based on such
    a value function [[25](#bib.bib25)]. While in policy-based RL, the action is determined
    by a policy network directly, which is trained by policy gradient [[25](#bib.bib25)].
    We will first introduce value-based methods and policy-based methods, and then
    discuss the combinations of them. We drew Figure 5 to show the classification
    and development of these methods. Besides, we also introduce multi-agent RL as
    a special category.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的强化学习（RL）中，解决$Q$值函数通常通过迭代贝尔曼方程$Q_{i+1}(s,a)=E_{s^{{}^{\prime}}\sim S}[r+\gamma
    \max_{a^{{}^{\prime}}}Q_{i}(s^{{}^{\prime}},a^{{}^{\prime}})|s,a]$进行。通过不断迭代，状态-动作值函数最终会收敛，从而获得最优策略：$\pi^{\ast}=arg\max_{a\in
    A}Q^{\ast}(s,a)$。然而，对于实际问题，这种搜索最优策略的过程是不可行的，因为迭代贝尔曼方程的计算成本因状态空间庞大而迅速增长。为了解决这个问题，深度学习（DL）被引入到RL中，形成了深度强化学习（DRL），它利用深度神经网络进行传统RL模型中的函数近似，并显著提高了许多挑战性应用的性能 [[26](#bib.bib26),
    [17](#bib.bib17), [27](#bib.bib27)]。一般来说，一个RL代理可以通过两种方式进行操作：（1）通过了解/建模状态转移，这称为基于模型的RL，和（2）通过与环境互动而不建模转移模型，这称为无模型RL。无模型RL算法包括两类算法：基于价值的方法和基于策略的方法。在基于价值的RL中，代理学习状态-动作对的价值函数，然后基于该价值函数选择动作 [[25](#bib.bib25)]。而在基于策略的RL中，动作直接由策略网络决定，该网络通过策略梯度训练 [[25](#bib.bib25)]。我们将首先介绍基于价值的方法和基于策略的方法，然后讨论它们的组合。我们绘制了图5以展示这些方法的分类和发展。此外，我们还介绍了作为一个特殊类别的多智能体RL。
- en: Value-based RL. Mnih et al. [[26](#bib.bib26), [17](#bib.bib17), [27](#bib.bib27)]
    first combined the convolutional neural network with the $Q$ learning [[28](#bib.bib28)]
    algorithm from traditional RL, and proposed the Deep Q-Network (DQN) framework.
    This model is first used to process visual perception, which is a pioneering and
    representative work in the field of value-based RL. DQN uses an experience replay
    mechanism [[29](#bib.bib29)] in the training process, and processes the transferred
    samples $e_{t}=(s_{t},a_{t},r_{t},s_{t+1})$ for training. At each time step $t$,
    the transferred samples obtained from the interaction between the agent and the
    environment are stored in a replay buffer $D={e_{1},...e_{t}}$. During training,
    a small batch of transferred samples is randomly selected from $D$ each time,
    and the stochastic gradient descent (SGD) algorithm and TD error [[30](#bib.bib30)]
    is used to update the network parameters $\theta$. During training, samples are
    usually required to be independent of each other. Such a random sampling method
    greatly reduces the relevance between samples, thereby improves the stability
    of the algorithm. In addition to using a deep convolutional network with parameter
    $\theta$ to approximate the current value function, DQN also uses another network
    to generate the target $Q$ value. Specifically, $Q(s,a,\theta)$ represents the
    output of the current value network, which is used to evaluate the value function
    of the action pair in the current state. Meanwhile, $Q(s,a,\theta_{-})$ represents
    the output of the target value network, which is used to approximate the value
    function, namely the target $Q$ value. The parameters $\theta$ of the current
    value network are updated in real-time. After every $N$ iterations, the parameters
    of target value network $\theta_{-}$ are updated by $\theta$ and kept frozen for
    another $N$ iterations. The entire network is trained by minimizing the mean square
    error between the current $Q$ value and the target $Q$. Such a frozen target mechanism
    reduces the correlation between the current $Q$ value and the target $Q$ value
    and thus improves the stability of the training process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的强化学习（Value-based RL）。Mnih 等人 [[26](#bib.bib26), [17](#bib.bib17), [27](#bib.bib27)]
    首次将卷积神经网络与传统强化学习中的 $Q$ 学习 [[28](#bib.bib28)] 算法相结合，并提出了深度 Q 网络（DQN）框架。该模型首次用于处理视觉感知，是基于价值的强化学习领域中的开创性和代表性工作。DQN
    在训练过程中使用经验回放机制 [[29](#bib.bib29)]，并处理转移样本 $e_{t}=(s_{t},a_{t},r_{t},s_{t+1})$
    进行训练。在每个时间步 $t$，从代理与环境的交互中获得的转移样本被存储在回放缓冲区 $D={e_{1},...e_{t}}$ 中。在训练过程中，每次从 $D$
    中随机选择一个小批量的转移样本，利用随机梯度下降（SGD）算法和 TD 误差 [[30](#bib.bib30)] 更新网络参数 $\theta$。在训练过程中，样本通常要求彼此独立。这种随机采样方法大大减少了样本之间的相关性，从而提高了算法的稳定性。除了使用具有参数
    $\theta$ 的深度卷积网络来逼近当前的价值函数外，DQN 还使用另一个网络来生成目标 $Q$ 值。具体来说，$Q(s,a,\theta)$ 表示当前价值网络的输出，用于评估当前状态下动作对的价值函数。同时，$Q(s,a,\theta_{-})$
    表示目标价值网络的输出，用于逼近价值函数，即目标 $Q$ 值。当前价值网络的参数 $\theta$ 实时更新。在每 $N$ 次迭代后，目标价值网络的参数 $\theta_{-}$
    由 $\theta$ 更新，并在接下来的 $N$ 次迭代中保持不变。整个网络通过最小化当前 $Q$ 值与目标 $Q$ 之间的均方误差来训练。这样的冻结目标机制减少了当前
    $Q$ 值与目标 $Q$ 值之间的相关性，从而提高了训练过程的稳定性。
- en: 'The selection and evaluation of actions are based on the target value network
    $\theta_{i}^{-}$, which will easily overestimate the $Q$ value in the learning
    process. Tackling this problem, researchers have proposed a series of methods
    based on DQN. Hasselt et al.  [[31](#bib.bib31)] proposed the Deep Double Q-Network
    (DDQN) algorithm based on the double Q-learning algorithm [[32](#bib.bib32)] (double
    Q-learning). There are two different sets of parameters in double Q-learning:
    $\theta$ and $\theta^{-}$. Where $\theta$ is used to select the action corresponding
    to the maximum $Q$ value, and $\theta^{-}$ is used to evaluate the $Q$ value of
    the optimal action. Such a parameter separation separates action selection and
    strategy evaluation so as to reduce the risk of overestimating the Q value. Experiments
    show that DDQN can estimate $Q$ value more accurately than DQN. The success of
    DDQN shows that reducing evaluation error on $Q$ value improves performance. Inspired
    by this, Bellemare et al. [[33](#bib.bib33)] defined a new operator based on advantage
    learning (AL) [[34](#bib.bib34)] in the Bellman equation to increase the difference
    between the optimal action value and the sub-optimal action value, in order to
    alleviate the evaluation error caused by the action corresponding to the largest
    $Q$ value. Experiments show that AL error terms can effectively reduce the deviation
    in the evaluation of the $Q$ value and thus promote learning quality. In addition,
    Wang et al.  [[35](#bib.bib35)] improved the network architecture based on DQN,
    and proposed Dueling DQN, which greatly accelerates task learning speed.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 动作的选择和评估是基于目标值网络$\theta_{i}^{-}$，这在学习过程中容易高估$Q$值。为了解决这个问题，研究人员提出了一系列基于DQN的方法。Hasselt等人[[31](#bib.bib31)]
    提出了基于双重Q学习算法[[32](#bib.bib32)]（double Q-learning）的深度双重Q网络（DDQN）算法。双重Q学习中有两组不同的参数：$\theta$
    和 $\theta^{-}$。其中，$\theta$ 用于选择对应最大$Q$值的动作，而 $\theta^{-}$ 用于评估最优动作的$Q$值。这种参数分离将动作选择和策略评估分开，从而减少了高估$Q$值的风险。实验表明，DDQN能够比DQN更准确地估计$Q$值。DDQN的成功表明，减少$Q$值评估误差能够提高性能。受此启发，Bellemare等人[[33](#bib.bib33)]
    在贝尔曼方程中基于优势学习（AL）[[34](#bib.bib34)] 定义了一种新操作符，以增加最优动作值与次优动作值之间的差异，以缓解由最大$Q$值对应的动作引起的评估误差。实验表明，AL误差项能够有效减少$Q$值评估中的偏差，从而提升学习质量。此外，Wang等人[[35](#bib.bib35)]
    基于DQN改进了网络架构，提出了对决DQN，这大大加快了任务学习速度。
- en: Value-based RL methods are suitable for low-dimensional discrete action spaces.
    However, they cannot solve the decision-making problems in the continuous action
    space, such as autonomous driving, robot movement, etc. Therefore, we further
    introduce the policy-based RL methods that are capable of solving continuous decision-making
    problems.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的强化学习方法适用于低维离散动作空间。然而，它们无法解决连续动作空间中的决策问题，如自动驾驶、机器人运动等。因此，我们进一步引入了基于策略的强化学习方法，这些方法能够解决连续决策问题。
- en: 'Policy-based RL. Policy-based RL [[36](#bib.bib36)] updates the policy parameters
    directly by computing the gradient of the cumulative reward of the policy with
    respect to the policy parameters, and finally converges to the optimal policy
    $max_{\theta}E[R|\pi_{\theta}],$ where $R=\sum_{t=0}^{T-1}r_{t}$ represents the
    sum of rewards received in an episode. The most common idea of policy gradient
    is to increase the probability of of the trajectories with higher reward. Assume
    the state, action and reward trajectory of a complete episode is $\tau=\{s_{0},a_{0},r_{0},s_{1},a_{1},r_{1},...,s_{T-1},a_{T-1},r_{T-1},s_{T}\}$.
    Then the policy gradient is expressed as : $g=\sum_{t=0}^{T-1}R\triangledown_{\theta}log\pi(a_{t}|s_{t};\theta).$
    Such a gradient can be used to adjust the policy parameters $\theta\leftarrow\theta+\alpha
    g,$ where $\alpha$ is the learning rate, which controls the rate of policy parameter
    update. The gradient term $\sum_{t=0}^{T-1}\triangledown_{\theta}log\pi(a_{t}|s_{t};\theta)$
    represents the direction that can increase the probability of occurrence of trajectory
    $\tau$. After multiplying by the score function $R$, it can make the probability
    density of the trajectories with higher reward greater. While trajectories with
    different total rewards are collected, the above training process will guide the
    probability density to these trajectories with higher total rewards and maximize
    the corresponding appearance probability.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的 RL。基于策略的 RL [[36](#bib.bib36)] 通过计算策略的累积奖励关于策略参数的梯度直接更新策略参数，最终收敛到最优策略
    $max_{\theta}E[R|\pi_{\theta}],$ 其中 $R=\sum_{t=0}^{T-1}r_{t}$ 表示在一个回合中获得的奖励总和。策略梯度的最常见思想是提高高奖励轨迹的概率。假设完整回合的状态、动作和奖励轨迹是
    $\tau=\{s_{0},a_{0},r_{0},s_{1},a_{1},r_{1},...,s_{T-1},a_{T-1},r_{T-1},s_{T}\}$。然后策略梯度表示为：$g=\sum_{t=0}^{T-1}R\triangledown_{\theta}log\pi(a_{t}|s_{t};\theta).$
    这样的梯度可以用来调整策略参数 $\theta\leftarrow\theta+\alpha g,$ 其中 $\alpha$ 是学习率，控制策略参数更新的速度。梯度项
    $\sum_{t=0}^{T-1}\triangledown_{\theta}log\pi(a_{t}|s_{t};\theta)$ 表示可以增加轨迹 $\tau$
    出现概率的方向。通过与得分函数 $R$ 相乘，可以使高奖励轨迹的概率密度更大。在收集到不同总奖励的轨迹时，上述训练过程将引导概率密度向这些高奖励轨迹靠拢，并最大化相应的出现概率。
- en: 'However, the above method lacks the ability of distinguishing trajectories
    with different quality, which will lead to a slow and unstable training process.
    In order to solve these problems, Williams et al. [[21](#bib.bib21)] proposed
    the REINFORCE algorithm with a baseline as a relative standard for the reward
    : $g=\sum_{t=0}^{T-1}\triangledown_{\theta}log\pi(a_{t}|s_{t};\theta)(R-b),$ where
    $b$ is a baseline related to the current trajectory $\tau$, which is usually set
    as an expected estimate of $R$ in order to reduce the variance of $R$. It can
    be seen that the more $R$ exceeds the reference $b$, the greater the probability
    that the corresponding trajectory $\tau$ will be selected. Therefore, in the DRL
    task of large-scale state, the policy can be parameterized by the deep neural
    network, and the traditional policy gradient method can be used to solve the optimal
    policy.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述方法缺乏区分不同质量轨迹的能力，这将导致训练过程缓慢且不稳定。为了解决这些问题，Williams 等人 [[21](#bib.bib21)]
    提出了 REINFORCE 算法，该算法以基准作为奖励的相对标准：$g=\sum_{t=0}^{T-1}\triangledown_{\theta}log\pi(a_{t}|s_{t};\theta)(R-b),$
    其中 $b$ 是与当前轨迹 $\tau$ 相关的基准，通常设置为 $R$ 的期望估计，以减少 $R$ 的方差。可以看出，$R$ 越超过参考 $b$，对应轨迹
    $\tau$ 被选择的概率越大。因此，在大规模状态的 DRL 任务中，策略可以通过深度神经网络参数化，传统的策略梯度方法可以用来求解最优策略。
- en: However, the policy-based RL methods are very unstable during training due to
    the inaccurate estimation of baseline $b$ and are inefficient due to that complete
    episodes are required for parameter updates. In order to solve these problems,
    researchers proposed actor critic methods, which combine the value-based RL methods
    and policy-based RL methods.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于基准 $b$ 的估计不准确，基于策略的强化学习（RL）方法在训练过程中非常不稳定，并且由于需要完整的回合来更新参数，这些方法效率低下。为了解决这些问题，研究人员提出了演员-评论家方法，该方法结合了基于价值的
    RL 方法和基于策略的 RL 方法。
- en: 'Actor Critic RL. V. R. Konda et al.  [[37](#bib.bib37)] first proposed the
    actor-critic (AC) methods leveraging advantages from both value-based and policy-based
    methods. The AC methods include two estimators: an actor that plays the role of
    the policy-based method via interacting with the environment and generating actions
    according to the current policy, while a critic who plays the role of the value-based
    method by estimating the value of the current state during training. In AC methods,
    the critic’s estimation of the value of the current state makes the RL training
    process more stable. In addition, there are some actor-critic RL methods introducing
    gradient restrictions or replay buffers so that the collected data can be reused,
    and therefore improve the training efficiency.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Actor Critic 强化学习。V. R. Konda 等人 [[37](#bib.bib37)] 首次提出了演员-评论家（AC）方法，这些方法结合了基于值的方法和基于策略的方法的优点。AC
    方法包括两个估计器：一个演员，通过与环境互动并根据当前策略生成动作，扮演基于策略的方法的角色；而评论家则通过在训练过程中估计当前状态的价值，扮演基于值的方法的角色。在
    AC 方法中，评论家对当前状态价值的估计使得强化学习训练过程更为稳定。此外，还有一些演员-评论家强化学习方法引入了梯度限制或重放缓存，以便重用收集的数据，从而提高训练效率。
- en: R. S. Sutton et al. [[25](#bib.bib25)] proposed the Advantage Actor-Critic (A2C)
    method, which adds a baseline to the $Q$ value so that the feedback can be either
    positive or negative. V. Mnih et al.  [[38](#bib.bib38)] introduced distributed
    machine learning methods into A2C and got a new algorithm named Asynchronous Advantage
    Actor-Critic (A3C), which greatly improved the efficiency of the A2C algorithm.
    Wang et al. combined the AC method with experience replay and proposed actor-critic
    with experience replay (ACER) [[39](#bib.bib39)] method. This method enables the
    AC framework to train in an off-policy way to improve data utilization efficiency.
    Lillicrap et al.  [[40](#bib.bib40)] leveraged the idea of DQN to extend the Q
    learning algorithm to transform the Deterministic Policy Gradient [[41](#bib.bib41)]
    (DPG) method, and proposed a Deep Deterministic Policy Gradient method (DDPG)
    based on the actor-critic framework, which can be used to solve decision-making
    problems in continuous action space. Moreover, it also introduced the replay buffer
    so that the collected data can be reused to improve training efficiency. Although
    DDPG can sometimes achieve good performance, it is still fragile in terms of hyperparameters.
    A common failure reason for DDPG is overestimating the real $Q$ value, thus making
    the learned policy worse. To solve this problem, Fujimoto et al.  [[42](#bib.bib42)]
    proposed Twin Delayed DDPG (TD3), which introduced three techniques based on DDPG.
    TD3 employs clipped double-Q learning to reduce the deviation of the $Q$ value
    estimation. It also utilizes delayed policy updating and target policy smoothing
    to reduce the impact of the $Q$ value estimation deviation on policy training.
    Furthermore, Schulman et al. [[20](#bib.bib20)] employed the importance sampling [[43](#bib.bib43)]
    method and tailored the network gradient update of reinforcement learning to make
    the training process more robust. Schulman et al. [[44](#bib.bib44)] also proposed
    a method called Trust Region Policy Optimization (TRPO). The core idea of TRPO
    is to force the $KL$ differences of the prediction distribution of the old and
    new policies on the same batch of data so as to avoid excessive gradient updates
    and ensure the stability of the training process. However, TRPO employs the conjugate
    gradient algorithm to solve the constrained optimization problem, which greatly
    reduces the computational efficiency and increases the implementation cost. Therefore,
    Schulman et al. [[20](#bib.bib20)] proposed a Proximal Policy Optimization algorithm
    (PPO) to get rid of the calculations generated by constrained optimization by
    introducing a reduced proxy objective function.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: R. S. Sutton等人[[25](#bib.bib25)] 提出了Advantage Actor-Critic (A2C) 方法，该方法在$Q$值中添加了基准线，从而使反馈可以是正面的也可以是负面的。V.
    Mnih等人[[38](#bib.bib38)] 将分布式机器学习方法引入A2C，并提出了一个新算法，称为Asynchronous Advantage Actor-Critic
    (A3C)，大大提高了A2C算法的效率。Wang等人将AC方法与经验重放结合，提出了actor-critic with experience replay
    (ACER) [[39](#bib.bib39)] 方法。该方法使AC框架能够以离策略方式进行训练，以提高数据利用效率。Lillicrap等人[[40](#bib.bib40)]
    利用DQN的思想将Q学习算法扩展到确定性策略梯度[[41](#bib.bib41)] (DPG) 方法，并提出了一种基于actor-critic框架的Deep
    Deterministic Policy Gradient方法 (DDPG)，可用于解决连续动作空间中的决策问题。此外，它还引入了重放缓冲区，以便重用收集的数据来提高训练效率。虽然DDPG有时能取得良好表现，但在超参数方面仍然较为脆弱。DDPG常见的失败原因是对真实$Q$值的过高估计，从而使学习到的策略变差。为了解决这个问题，Fujimoto等人[[42](#bib.bib42)]
    提出了Twin Delayed DDPG (TD3)，该方法在DDPG的基础上引入了三种技术。TD3采用了剪辑双Q学习来减少$Q$值估计的偏差。它还利用延迟策略更新和目标策略平滑来降低$Q$值估计偏差对策略训练的影响。此外，Schulman等人[[20](#bib.bib20)]
    采用了重要性采样[[43](#bib.bib43)] 方法，并对强化学习的网络梯度更新进行了调整，使训练过程更加稳健。Schulman等人[[44](#bib.bib44)]
    还提出了一种名为Trust Region Policy Optimization (TRPO) 的方法。TRPO的核心思想是强制将旧策略和新策略在相同数据批次上的$KL$差异保持在一定范围内，以避免过度梯度更新并确保训练过程的稳定性。然而，TRPO采用了共轭梯度算法来解决约束优化问题，这大大降低了计算效率并增加了实现成本。因此，Schulman等人[[20](#bib.bib20)]
    提出了Proximal Policy Optimization算法 (PPO)，通过引入简化的代理目标函数来摆脱约束优化产生的计算。
- en: Multi-Agent RL. Many real-world problems require interaction modeling among
    different agents, and multi-agent RL algorithms are thus needed. A common approach
    is to assign each agent with a separate training mechanism. Such a distributed
    learning architecture reduces the implementation of learning difficulty and computational
    complexity. For DRL problems with large-scale state space, using the DQN algorithm
    instead of the Q learning algorithm to train each agent individually can construct
    a simple multi-agent DRL system. Tampuu et al. [[45](#bib.bib45)] dynamically
    adjust the reward model according to different goals and proposed a DRL model
    in which multiple agents can cooperate and compete with each other. When faced
    with reasoning tasks that require multiple agents to communicate with each other,
    the DQN models usually fail to learn an effective strategy. To solve this problem,
    Foerster et al. [[46](#bib.bib46)] proposed a method called Deep Distributed Recurrent
    Q-Networks (DDRQN) model for multi-agent communication and cooperation with partially
    observable state. Except for distributed learning, other mechanisms including
    cooperative learning, competitive learning, and direct parameter sharing are also
    used in different multi-agent scenarios [[47](#bib.bib47)].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体强化学习。许多现实世界的问题需要不同智能体之间的互动建模，因此需要多智能体强化学习算法。一个常见的方法是为每个智能体分配一个独立的训练机制。这种分布式学习架构减少了学习实施的难度和计算复杂度。对于具有大规模状态空间的深度强化学习（DRL）问题，使用DQN算法而不是Q学习算法来单独训练每个智能体，可以构建一个简单的多智能体DRL系统。Tampuu
    等人[[45](#bib.bib45)] 根据不同的目标动态调整奖励模型，并提出了一种多智能体可以相互合作和竞争的DRL模型。当面临需要多个智能体相互沟通的推理任务时，DQN模型通常无法学习到有效的策略。为了解决这个问题，Foerster
    等人[[46](#bib.bib46)] 提出了一种称为深度分布式递归Q网络（DDRQN）的方法，用于多智能体通信和合作，具有部分可观察状态。除了分布式学习，合作学习、竞争学习和直接参数共享等其他机制也被应用于不同的多智能体场景[[47](#bib.bib47)]。
- en: 2.2 Application Overview
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 应用概述
- en: 'As defined above, a DDS system transports either humans or parcels to provided
    destinations following individual demands or systematic requirements. We briefly
    introduce several urban DDS applications that have significant importance in our
    daily lives, as illustrated in Figure [6](#S2.F6 "Figure 6 ‣ 2.2 Application Overview
    ‣ 2 Background ‣ Deep Reinforcement Learning for Demand Driven Services in Logistics
    and Transportation Systems: A Survey").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，DDS系统根据个人需求或系统要求将人或包裹运送到指定目的地。我们简要介绍几个在我们日常生活中具有重要意义的城市DDS应用，如图[6](#S2.F6
    "图6 ‣ 2.2 应用概述 ‣ 2 背景 ‣ 面向需求的物流和交通系统的深度强化学习：综述")所示。
- en: '![Refer to caption](img/ec0ea98e49cb1d77e399366a84da169d.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec0ea98e49cb1d77e399366a84da169d.png)'
- en: 'Figure 6: An overview of several urban DDS applications that have significant
    importance in our daily lives.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：几个在我们日常生活中具有重要意义的城市DDS应用概述。
- en: 2.2.1 Ridesharing
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 拼车
- en: Compared to traditional taxi-hailing services in which passengers are offered
    rides by chance, a ridesharing service matches passengers with drivers according
    to their demands from mobile apps, such as DiDi [[48](#bib.bib48)], Uber [[49](#bib.bib49)],
    etc. When a potential passenger submits a request from the apps to the centralized
    platform, the platform will first estimate the trip price and send it back. If
    the passenger accepts it, a matching module will attempt to match the passenger
    to a nearby available driver. The matching process may take time due to real-time
    vehicle availability and thus pre-matching cancellation may exist. After a successful
    match, the driver will drive to the passenger and transport him/her to the destination.
    A trip fare will be obtained by the driver after arrival. To reduce the average
    waiting time for a successful match, the platforms usually utilize a fleet management
    module in the backend to rebalance idling vehicles continuously by guiding vehicles
    to places with a higher possibility of new requests. The decisions from matching
    and fleet management are finally executed within the routing stage. Vehicles are
    navigated to serve passengers or repositioned to new areas following these strategies.
    In the ridesharing scenario, the service worker of a loop refers to the vehicle,
    while the provider and the target refer to the passengers’ pickup locations and
    their destinations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于传统的出租车叫车服务，其中乘客的接送是随机的，共享乘车服务根据乘客通过移动应用程序（如滴滴[[48](#bib.bib48)]、优步[[49](#bib.bib49)]等）提交的需求来匹配乘客和司机。当潜在的乘客通过应用程序向中央平台提交请求时，平台会首先估算行程价格并返回。如果乘客接受，匹配模块会尝试将乘客与附近的可用司机匹配。由于实时车辆的可用性，匹配过程可能需要时间，因此可能会出现预匹配取消的情况。在成功匹配后，司机将前往乘客处，并将其送到目的地。司机在到达后将获得行程费用。为了减少成功匹配的平均等待时间，平台通常在后台利用车队管理模块，通过引导车辆到新的请求可能性更高的地方来不断重新平衡闲置车辆。匹配和车队管理的决策最终在路线阶段执行。车辆会根据这些策略被导航服务乘客或重新定位到新区域。在共享乘车场景中，循环服务的工作者指的是车辆，而提供者和目标分别指乘客的接送地点和目的地。
- en: A ridesharing service can be further classified into ride-hailing where a driver
    is assigned only one passenger at a time, and ride-pooling (also known as carpool)
    where several passengers share a vehicle at a time. Note that in some literature
    the scenario of multiple passengers is also named as ridesharing. In this survey,
    we use ride-pooling specifically for disambiguation following  [[23](#bib.bib23)].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 共享乘车服务可以进一步分为叫车服务（即每次仅分配一个乘客的司机）和拼车服务（也称为拼车），其中多个乘客共享一辆车。注意，在一些文献中，多个乘客的场景也被称为共享乘车。在本调查中，我们特指拼车服务以避免歧义，参考[[23](#bib.bib23)]。
- en: 2.2.2 On-demand Delivery
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 按需配送
- en: 'Many platforms around the world provide food delivery services such as PrimeNow [[50](#bib.bib50)],
    UberEats [[51](#bib.bib51)], MeiTuan [[1](#bib.bib1)], and Eleme [[52](#bib.bib52)].
    Except for delivering food, the newly rising instant delivery services can also
    deliver small parcels from one customer to another or helps to purchase other
    daily merchandise directly from local shops or pharmacies, such as medicines.
    Both food and instant delivery can be seen as a type of on-demand delivery. Compared
    with traditional delivery platforms, e.g., FedEx and UPS, the orders in on-demand
    delivery platforms are expected to be fulfilled in a relatively short time, e.g.,
    30 minutes to 1 hour. A typical on-demand delivery process involves four parties:
    a customer as the service target, a merchant as the service provider, a courier
    as the worker, and the centralized platform. A customer-first places an order
    in a smartphone app of a platform, while a merchant starts to prepare the order
    and the platform assigns a courier to pick up the order. Finally, the courier
    delivers the order to the customer.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 全球许多平台提供食品配送服务，如PrimeNow[[50](#bib.bib50)]、UberEats[[51](#bib.bib51)]、美团[[1](#bib.bib1)]和饿了么[[52](#bib.bib52)]。除了送餐外，新兴的即时配送服务还可以从一个客户处配送小包裹到另一个客户处，或者直接从当地商店或药房（如药品）购买其他日常商品。食品配送和即时配送都可以被视为一种按需配送。与传统的配送平台（如FedEx和UPS）相比，按需配送平台的订单期望在相对较短的时间内完成，例如30分钟到1小时。典型的按需配送过程涉及四方：作为服务目标的客户、作为服务提供者的商家、作为工作者的快递员，以及中央平台。客户首先在平台的手机应用程序中下单，商家开始准备订单，平台分配快递员去取货。最后，快递员将订单送达客户处。
- en: 2.2.3 Express Systems
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 快递系统
- en: As a long-existing DDS system, an express system is required to both pick up
    parcels from the consignors to the fixed depots, and deliver parcels that were
    loaded from the depots to the consignees. In a practical express system such as
    FedEx [[53](#bib.bib53)], Cainiao [[54](#bib.bib54)], the pickup and delivery
    are usually considered simultaneously and could be obtained within the same service
    loop. A courier loads parcels at the depot and then delivers them to their destinations
    one by one via a delivery van. Meanwhile, new pick-up requests may come from local
    customers during the delivery process, each of which is associated with a service
    location. The courier should also go to these places to fulfill pickup requests.
    Couriers are required to depart from and return to the depots by a specific time,
    to fit the schedule of trucks that send and pick packages to or from stations
    regularly.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个长期存在的DDS系统，快递系统需要同时完成从发货人处取件到固定仓库，并将从仓库装载的包裹送达收货人的任务。在实际的快递系统中，如FedEx [[53](#bib.bib53)]和Cainiao [[54](#bib.bib54)]，取件和送货通常被同时考虑，并且可以在同一服务循环内完成。快递员在仓库装载包裹，然后通过送货车将包裹逐一送达目的地。同时，在送货过程中，可能会有来自本地客户的新取件请求，每个请求都与一个服务地点相关。快递员还需要前往这些地点完成取件请求。快递员需要在特定时间内从仓库出发并返回，以配合定期往返于车站的卡车的时间表。
- en: 2.2.4 Warehousing
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 仓储
- en: Except for the DDS applications that have direct interactions with humans, the
    rising autonomous technologies enable unmanned management in local warehousing.
    The shipment requests of cargoes, usually with large size and weight, are common
    within a repository or among several repositories. Cargoes are moved into a targeted
    shelf and moved out continuously to accommodate the global shipping requirements.
    To reduce expenses and improve efficiency, autonomous guiding vehicles (AGVs)
    are commonly used in the modern warehousing scenario. In a warehousing service
    loop, the service provider refers to the original shelf, and the service target
    refers to the corresponding destination. AGVs serve as the workers in the entire
    process. An intelligent centralized platform is responsible to control all AGVs
    for efficient operations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与人直接互动的DDS应用外，新兴的自主技术使得本地仓储能够无人管理。货物的发运请求，通常是大型和重型货物，在一个仓库内或多个仓库之间很常见。货物被移入目标货架，并持续移动以满足全球发货要求。为了减少开支并提高效率，现代仓储场景中通常使用自主导向车辆（AGVs）。在仓储服务循环中，服务提供者指的是原始货架，而服务目标指的是相应的目的地。AGVs在整个过程中充当工人。一个智能集中平台负责控制所有AGVs，以实现高效操作。
- en: '![Refer to caption](img/1b5f35eef6cd9d65aa27358141a63c05.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1b5f35eef6cd9d65aa27358141a63c05.png)'
- en: (a) Illustration of On-demand Delivery.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 按需配送的示意图。
- en: '![Refer to caption](img/eb757dc4832ebf68ee08c93ed09a7fa1.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb757dc4832ebf68ee08c93ed09a7fa1.png)'
- en: (b) Illustration of Ridesharing.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 拼车的示意图。
- en: '![Refer to caption](img/c601c1273e5da9acf28de4c36f484338.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c601c1273e5da9acf28de4c36f484338.png)'
- en: (c) Illustration of Express Systems.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 快递系统的示意图。
- en: '![Refer to caption](img/b38fc04348bfe43002aaf031f2c2cd1a.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b38fc04348bfe43002aaf031f2c2cd1a.png)'
- en: (d) Illustration of Warehousing.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 仓储的示意图。
- en: 'Figure 7: Illustration of the four typical DDS applications.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：四种典型DDS应用的示意图。
- en: 2.3 Relationship between Two Stages
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 两个阶段之间的关系
- en: Generally, the research problems within practical DDS systems can be classified
    into two stages, i.e., dispatching and routing. The dispatching stage mainly handles
    the relationship between service workers and demand pairs and thus constructs
    service loops, while the routing stage focuses on how to execute the services
    within each established loop. We hereby note that the two stages are not rigidly
    separated. A reasonable dispatching algorithm should consider future in-loop routing
    strategies as a measurement proxy. Whether a better routing solution can be generated
    is a direct criterion to judge different dispatching strategies. For example,
    a courier should not be assigned with a demand request which is far away from
    him since the routing distance within such a loop will be too long. On the other
    hand, in practical routing scenarios where a fleet of workers are on duty, it
    is implicated that the cooperation among different workers needs consideration
    and thus the dispatching is included.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，实际的 DDS 系统中的研究问题可以分为两个阶段，即派遣和路由。派遣阶段主要处理服务工人与需求对之间的关系，从而构建服务循环，而路由阶段则专注于如何在每个建立的循环中执行服务。我们在此指出，这两个阶段并不是严格分开的。一个合理的派遣算法应该将未来的循环内路由策略作为测量代理。是否能够生成更好的路由解决方案是判断不同派遣策略的直接标准。例如，快递员不应被分配一个距离他较远的需求请求，因为这样的循环中的路由距离将过长。另一方面，在实际的路由场景中，如果有一支工人队伍在值班，那么不同工人之间的合作需要考虑，因此派遣也包括在内。
- en: However, such a classification is necessary to concentrate on primary challenges
    in different practical scenarios. An important reference metric for such a classification
    we demonstrate in this survey is the Demand/Worker Ratio. A low ratio means that
    the number of workers and demand pairs are balanced in each constructed loop and
    thus the major space of optimization is to determine how different requests should
    be assigned. For instance, a driver can only take one passenger in ride-hailing
    and no more than two passengers in ride-pooling. How to match drivers with customer
    requests is critical to global efficiency, while computing in-loop routing strategies
    is not computationally expensive. Meanwhile, in scenarios with a large ratio,
    it implies that a worker has to serve lots of demand requests within its loop.
    The routing stage, i.e., how to execute the loops thus has a high problem complexity
    and requires an intensive optimization process. For instance, a courier in express
    systems may be assigned with hundreds of parcels, and generating its optimal routing
    strategy becomes the primary challenge due to its NP-hard nature.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样的分类是必要的，以便集中关注不同实际场景中的主要挑战。本次调查中展示的一个重要参考指标是需求/工人比例。低比例意味着在每个构建的循环中，工人和需求对的数量是平衡的，因此主要的优化空间是确定不同请求的分配方式。例如，司机在打车服务中只能接一名乘客，在拼车服务中则不超过两名乘客。如何将司机与客户请求匹配对整体效率至关重要，而计算循环内的路由策略并不计算开销很大。同时，在比例较大的场景中，这意味着一个工人必须在其循环内处理大量的需求请求。路由阶段，即如何执行循环，因此具有较高的问题复杂性，需要进行密集的优化过程。例如，快递系统中的快递员可能会被分配到数百个包裹，由于其
    NP-难度性质，生成其最佳路由策略成为主要挑战。
- en: In the following sections, we will focus on the dispatching and routing stages.
    We will discuss the within subproblems and introduce existing solutions respectively.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将重点讨论派遣和路由阶段。我们将分别讨论各自的子问题，并介绍现有的解决方案。
- en: '3 Stage 1: Dispatching'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三阶段 1：派遣
- en: 'Given the information of available workers and continuously updated service
    demand pairs, the first stage of DDS is to coordinate the relationship between
    demands along with the available workers, and thus establish service loops both
    effectively and efficiently. We name such a loop forming process as ’Dispatching’.
    Generally, the dispatching stage consists of two aspects: 1) Order matching, which
    aims to find the best matching strategy between workers and demands, and 2) Fleet
    management, which repositions idling workers to balance the local demand-supply
    ratio so that better order matching could be obtained in the future. Figure [8](#S3.F8
    "Figure 8 ‣ 3 Stage 1: Dispatching ‣ Deep Reinforcement Learning for Demand Driven
    Services in Logistics and Transportation Systems: A Survey") shows an overview
    of the dispatching phase in DDS.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '在提供的工人信息和持续更新的服务需求对的基础上，DDS 的第一阶段是协调需求与可用工人之间的关系，从而有效且高效地建立服务循环。我们将这种循环形成过程称为“调度”。通常，调度阶段包括两个方面：1)
    订单匹配，旨在找到工人和需求之间的最佳匹配策略，2) 车队管理，重新定位闲置工人以平衡本地的供需比，从而在未来获得更好的订单匹配。图[8](#S3.F8 "图
    8 ‣ 第1阶段: 调度 ‣ 面向需求的物流和交通系统中的深度强化学习：综述")展示了 DDS 中调度阶段的概述。'
- en: Formulated as an optimization problem, both tasks in the dispatching scenario
    are complicated due to three challenges. First, the continuously changing demand
    distributions and worker states bring high dynamics to the entire Markov Decision
    Process. It is non-trivial to accurately evaluate returns of different decision
    attempts. Second, a successful matching strategy should consider long-term returns [[55](#bib.bib55)].
    A simple maximum result only considering current service distributions may result
    in a long-term loss. For example, assigning all vehicles to serve every current
    demand may be a local maximum in ridesharing, but may decrease the profit in the
    next time window since some vehicles are assigned to areas where barely any new
    demands appear. Third, a centralized platform should consider multiple, even large
    amount of workers simultaneously. Effectively modeling the cooperation and sometimes
    competition among them is critical to improving the system efficiency.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将其制定为优化问题，调度场景中的两个任务因三个挑战而变得复杂。首先，不断变化的需求分布和工人状态给整个马尔可夫决策过程带来了高度动态性。准确评估不同决策尝试的回报并非易事。其次，一个成功的匹配策略应考虑长期回报[[55](#bib.bib55)]。一个仅考虑当前服务分布的简单最大结果可能会导致长期损失。例如，将所有车辆分配到当前的所有需求中可能是拼车中的局部最大值，但可能会在下一个时间窗口中减少利润，因为一些车辆被分配到几乎没有新需求的区域。第三，集中式平台应考虑多个甚至大量工人。有效建模他们之间的合作和有时的竞争对于提高系统效率至关重要。
- en: Concerning the given challenges, DRL has its natural advantage to solve the
    order matching problem compared to conventional methods and other learning-based
    methods. Many online reinforcement learning methods are developed to handle the
    non-stationariness in MDP modeling. Taking expected returns as learning signals,
    DRL is a proper framework to optimize sequential decision tasks, including dispatching
    tasks. Besides, modeling workers as agents is a natural way to handle the decision
    problem, either by homogeneously modeling all workers using the same policy, or
    consider the in-between interactions among multiple agents.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些挑战，相较于传统方法和其他基于学习的方法，DRL 在解决订单匹配问题上具有天然优势。许多在线强化学习方法已被开发出来，以处理 MDP 建模中的非平稳性。将预期回报作为学习信号，DRL
    是优化顺序决策任务（包括调度任务）的合适框架。此外，将工人建模为代理是一种自然的决策问题处理方式，无论是通过使用相同策略对所有工人进行均质建模，还是考虑多个代理之间的互动。
- en: '![Refer to caption](img/127f2ed4138b2624a0a9fc87cb4e4d60.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/127f2ed4138b2624a0a9fc87cb4e4d60.png)'
- en: 'Figure 8: Overall dispatching architecture proposed by [[56](#bib.bib56)].'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：[[56](#bib.bib56)] 提出的总体调度架构。
- en: 'In this section, we introduce both order matching and fleet management problems.
    Specifically for each problem, we first introduce the problem definition and common
    metrics, along with several conventional methods respectively. Then we thoroughly
    discuss detailed applications for transportation and logistics. The DRL-based
    literature for the dispatching stage is summarized in Tabel [II](#S3.T2 "TABLE
    II ‣ 3.1 Order Matching ‣ 3 Stage 1: Dispatching ‣ Deep Reinforcement Learning
    for Demand Driven Services in Logistics and Transportation Systems: A Survey").'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍了订单匹配和车队管理问题。具体而言，对于每个问题，我们首先介绍问题定义和常见指标，并分别介绍几种传统方法。然后，我们详细讨论运输和物流的具体应用。基于DRL的调度阶段文献总结在表[II](#S3.T2
    "TABLE II ‣ 3.1 Order Matching ‣ 3 Stage 1: Dispatching ‣ Deep Reinforcement Learning
    for Demand Driven Services in Logistics and Transportation Systems: A Survey")。'
- en: 3.1 Order Matching
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 订单匹配
- en: An order matching process is to assign current unserved service demands to available
    workers. It is also defined with other names, such as order-driver assignment
    in ridesharing services. The mathematical formulation originates from the online
    bipartite graph matching problem, where both supplies (the workers) and the demands
    are dynamic. It is an important module in the real-time online DDS applications
    with high dynamics, such as ridesharing and on-demand delivery [[14](#bib.bib14),
    [11](#bib.bib11), [57](#bib.bib57)]. Information including unserved demands, travel
    costs, and worker availability is updated continuously, which brings complexity
    to the problem.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 订单匹配过程是将当前未服务的需求分配给可用的工人。它也被称为其他名称，例如共享出行服务中的订单-司机分配。数学公式源于在线二分图匹配问题，其中供应（工人）和需求都是动态的。这是实时在线DDS应用中一个重要的模块，如共享出行和按需配送[[14](#bib.bib14),
    [11](#bib.bib11), [57](#bib.bib57)]。包括未服务的需求、旅行成本和工人可用性等信息持续更新，这给问题带来了复杂性。
- en: Without purely assigning demands to workers, practical DDS systems also consider
    other additional action choices. For instance, vehicles in a ridesharing system
    can be designated to idle when no proper demand can be assigned to them. As electric
    vehicles are widely used and deployed, whether to recharge or continue to accept
    new demands forms new decision problems [[58](#bib.bib58)]. Furthermore, controlling
    the number of demands assigned to the same worker also expands the action space,
    such as considering ride-hailing and ride-pooling scenarios simultaneously [[58](#bib.bib58)].
    When each driver can have more than one customer in a loop, the action is to determine
    how many customers and which ones to pick up.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了纯粹将需求分配给工人，实际的DDS系统还考虑其他附加的行动选择。例如，当没有合适的需求分配给车辆时，共享出行系统中的车辆可以被指定为空闲状态。随着电动车的广泛使用和部署，是否充电或继续接受新需求形成了新的决策问题[[58](#bib.bib58)]。此外，控制分配给同一工人的需求数量也扩展了行动空间，例如同时考虑打车和拼车场景[[58](#bib.bib58)]。当每个司机可以在一个循环中接送多个客户时，行动是决定接送多少客户以及接送哪些客户。
- en: 'As for the goal of order matching, there are generally two aspects to consider,
    including optimizing profits for the platform and experience from the demands’
    side:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 关于订单匹配的目标，通常有两个方面需要考虑，包括优化平台的利润和需求方的体验：
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Maximize the Gross Merchandise Volume (GMV).[[56](#bib.bib56)] With each service
    loop priced, a core evaluation metric of an effective order matching system is
    to maximize the total revenue of all services over time. In the ride-hailing services
    specifically, it is also called Accumulated Driver Income (ADI) in some literature[[59](#bib.bib59)].
    Generally, the profit perspective stands for the interest of both workers and
    the entire platform.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大化总商品交易量（GMV）。[[56](#bib.bib56)] 对于每个服务周期的定价，一个有效的订单匹配系统的核心评估指标是最大化所有服务的总收入。在专门的打车服务中，某些文献中也称为累积司机收入（ADI）[[59](#bib.bib59)]。通常，从利润角度来看，代表了工人和整个平台的利益。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Maximize the Order Response Rate (ORR).[[59](#bib.bib59)] Since not fulfilling
    all demands is usual in real-world scenarios, another goal is to maximize the
    ORR, which evaluates the satisfaction from the demands’ side. Based on the intuition
    that total response time increases along with ORR, it is also an alternative to
    represent the interest of customers. Note that ORR is highly correlated to GMV
    since that the more demands are fulfilled, the more revenues the platform can
    obtain within a certain period.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大化订单响应率（ORR）。[[59](#bib.bib59)] 由于在现实世界中未能满足所有需求是常见的，另一个目标是最大化ORR，这评估了需求方的满意度。基于总响应时间随着ORR增加的直觉，它也可以作为表示客户兴趣的另一种方式。注意，ORR与GMV高度相关，因为满足的需求越多，平台在一定时间内获得的收入也越多。
- en: 'TABLE II: Applications using DRL to solve DDS problems. The information of
    each literature reference consists of the publishing year, the problem solved,
    the DRL training paradigm used, the data type (Dtype) used, whether the data is
    available (Davail) and whether the code is released.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：使用DRL解决DDS问题的应用。每个文献参考的信息包括出版年份、解决的问题、使用的DRL训练范式、数据类型（Dtype）、数据是否可用（Davail）以及代码是否发布。
- en: '|  Reference | Year | Problem | Scenario | Algorithm | Network Structure |
    Dscheme | Dtype | Davail | Code |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 年份 | 问题 | 场景 | 算法 | 网络结构 | 数据方案 | 数据类型 | 数据可用性 | 代码 |'
- en: '|  Li et al.[[59](#bib.bib59)] | 2019 | Order Matching | Ridesharing | MFRL[[60](#bib.bib60)]
    | MLP | Hexagon-Grid | real | x | x |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Li et al.[[59](#bib.bib59)] | 2019 | 订单匹配 | 共享出行 | MFRL[[60](#bib.bib60)]
    | MLP | 六边形网格 | 真实 | x | x |'
- en: '| Zhou et al.[[61](#bib.bib61)] | 2019 | Order Matching | Ridesharing | Double-DQN[[31](#bib.bib31)]
    | MLP | Hexagon-Grid | real, sim | ✓ | x |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Zhou et al.[[61](#bib.bib61)] | 2019 | 订单匹配 | 共享出行 | 双重DQN[[31](#bib.bib31)]
    | MLP | 六边形网格 | 真实, 仿真 | ✓ | x |'
- en: '| Xu et al.[[56](#bib.bib56)] | 2018 | Order Matching | Ridesharing | TD[[30](#bib.bib30)]
    | - | Square-Grid | real, sim | x | x |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al.[[56](#bib.bib56)] | 2018 | 订单匹配 | 共享出行 | TD[[30](#bib.bib30)] |
    - | 方形网格 | 真实, 仿真 | x | x |'
- en: '| Wang et al.[[62](#bib.bib62)] | 2018 | Order Matching | Ridesharing | DQN[[17](#bib.bib17)]
    | MLP, CNN | Hexagon-Grid | real | x | x |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al.[[62](#bib.bib62)] | 2018 | 订单匹配 | 共享出行 | DQN[[17](#bib.bib17)]
    | MLP, CNN | 六边形网格 | 真实 | x | x |'
- en: '| Tang et al.[[63](#bib.bib63)] | 2019 | Order Matching | Ridesharing | double-DQN[[31](#bib.bib31)]
    | MLP | Hexagon-Grid | real | x | x |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Tang et al.[[63](#bib.bib63)] | 2019 | 订单匹配 | 共享出行 | 双重DQN[[31](#bib.bib31)]
    | MLP | 六边形网格 | 真实 | x | x |'
- en: '| Jindal et al.[[58](#bib.bib58)] | 2018 | Order Matching | Ride-pooling |
    DQN[[17](#bib.bib17)] | MLP | Square-Grid | real | ✓ | x |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Jindal et al.[[58](#bib.bib58)] | 2018 | 订单匹配 | 共享出行 | DQN[[17](#bib.bib17)]
    | MLP | 方形网格 | 真实 | ✓ | x |'
- en: '| He et al.[[64](#bib.bib64)] | 2019 | Order Matching | Ridesharing | Double
    DQN[[31](#bib.bib31)] | MLP, CNN | Square-Grid | real | x | x |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| He et al.[[64](#bib.bib64)] | 2019 | 订单匹配 | 共享出行 | 双重DQN[[31](#bib.bib31)]
    | MLP, CNN | 方形网格 | 真实 | x | x |'
- en: '| Al-Abbasi et al.[[65](#bib.bib65)] | 2019 | Order Matching | Ridesharing
    | DQN[[17](#bib.bib17)] | CNN | Square-Grid | real | x | x |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Al-Abbasi et al.[[65](#bib.bib65)] | 2019 | 订单匹配 | 共享出行 | DQN[[17](#bib.bib17)]
    | CNN | 方形网格 | 真实 | x | x |'
- en: '| Qin et al.[[66](#bib.bib66)] | 2021 | Order Matching | Ridesharing | AC [[37](#bib.bib37)],
    ACER [[39](#bib.bib39)] | MLP | Square-Grid | real | x | x |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Qin et al.[[66](#bib.bib66)] | 2021 | 订单匹配 | 共享出行 | AC [[37](#bib.bib37)],
    ACER [[39](#bib.bib39)] | MLP | 方形网格 | 真实 | x | x |'
- en: '| Wang et al.[[67](#bib.bib67)] | 2019 | Order Matching | Ridesharing | Q-Learning [[28](#bib.bib28)]
    | - | Graph Based | real, sim | ✓ | x |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al.[[67](#bib.bib67)] | 2019 | 订单匹配 | 共享出行 | Q-Learning [[28](#bib.bib28)]
    | - | 基于图 | 真实, 仿真 | ✓ | x |'
- en: '| Ke et al.[[68](#bib.bib68)] | 2020 | Order Matching | Ridesharing | DQN [[17](#bib.bib17)],
    A2C [[25](#bib.bib25)], ACER [[39](#bib.bib39)], PPO [[20](#bib.bib20)] | MLP
    | Square/Hexagon-Grid | real, sim | ✓ | x |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Ke et al.[[68](#bib.bib68)] | 2020 | 订单匹配 | 共享出行 | DQN [[17](#bib.bib17)],
    A2C [[25](#bib.bib25)], ACER [[39](#bib.bib39)], PPO [[20](#bib.bib20)] | MLP
    | 方形/六边形网格 | 真实, 仿真 | ✓ | x |'
- en: '| Yang et al.[[69](#bib.bib69)] | 2021 | Order Matching | Ridesharing | TD [[30](#bib.bib30)]
    | MLP | Square-Grid | real | x | x |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al.[[69](#bib.bib69)] | 2021 | 订单匹配 | 共享出行 | TD [[30](#bib.bib30)]
    | MLP | 方形网格 | 真实 | x | x |'
- en: '| Chen et al.[[70](#bib.bib70)] | 2019 | Order Matching | On-demand Delivery
    | PPO[[20](#bib.bib20)] | MLP | Square-Grid | real, sim | x | x |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al.[[70](#bib.bib70)] | 2019 | 订单匹配 | 按需配送 | PPO[[20](#bib.bib20)]
    | MLP | 方形网格 | 真实, 仿真 | x | x |'
- en: '| Li et al.[[59](#bib.bib59)] | 2019 | Order Matching | Express | DQN [[17](#bib.bib17)]
    | MLP, CNN | Square-Grid | real | x | x |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Li et al.[[59](#bib.bib59)] | 2019 | 订单匹配 | 快递 | DQN [[17](#bib.bib17)] |
    MLP, CNN | 方形网格 | 真实 | x | x |'
- en: '| Li et al.[[71](#bib.bib71)] | 2020 | Order Matching | Express | DQN [[17](#bib.bib17)]
    | MLP, CNN | Square-Grid | real | x | x |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Li et al.[[71](#bib.bib71)] | 2020 | 订单匹配 | 快递 | DQN [[17](#bib.bib17)] |
    MLP, CNN | 方形网格 | 实际 | x | x |'
- en: '| Hu et al.[[72](#bib.bib72)] | 2020 | Order Matching | Warehousing | DQN [[17](#bib.bib17)]
    | MLP | Graph-based | real | x | x |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Hu et al.[[72](#bib.bib72)] | 2020 | 订单匹配 | 仓储 | DQN [[17](#bib.bib17)] |
    MLP | 基于图的 | 实际 | x | x |'
- en: '| Lin et al.[[73](#bib.bib73)] | 2018 | Fleet Management | Ridesharing | A2C [[25](#bib.bib25)],
    DQN[[17](#bib.bib17)] | MLP | Hexagon-Grid | real | ✓ | ✓ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Lin et al.[[73](#bib.bib73)] | 2018 | 车队管理 | 共享出行 | A2C [[25](#bib.bib25)],
    DQN [[17](#bib.bib17)] | MLP | 六边形网格 | 实际 | ✓ | ✓ |'
- en: '| Zhang et al.[[74](#bib.bib74)] | 2020 | Fleet Management | Ridesharing |
    Dueling DQN [[35](#bib.bib35)] | MLP | Hexagon-Grid | real | ✓ | ✓ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al.[[74](#bib.bib74)] | 2020 | 车队管理 | 共享出行 | 对抗 DQN [[35](#bib.bib35)]
    | MLP | 六边形网格 | 实际 | ✓ | ✓ |'
- en: '| Wen et al.[[75](#bib.bib75)] | 2017 | Fleet Management | Ridesharing | DQN[[17](#bib.bib17)]
    | MLP | Square-Grid | real, sim | x | ✓ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Wen et al.[[75](#bib.bib75)] | 2017 | 车队管理 | 共享出行 | DQN [[17](#bib.bib17)]
    | MLP | 方形网格 | 实际, 模拟 | x | ✓ |'
- en: '| Oda et al.[[76](#bib.bib76)] | 2018 | Fleet Management | Ridesharing | DQN[[17](#bib.bib17)]
    | CNN | Square-Grid | real | x | x |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Oda et al.[[76](#bib.bib76)] | 2018 | 车队管理 | 共享出行 | DQN [[17](#bib.bib17)]
    | CNN | 方形网格 | 实际 | x | x |'
- en: '| Liu et al.[[77](#bib.bib77)] | 2020 | Fleet Management | Ridesharing | DQN [[17](#bib.bib17)]
    | GCN [[78](#bib.bib78)] | Square-Grid | real | ✓ | ✓ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al.[[77](#bib.bib77)] | 2020 | 车队管理 | 共享出行 | DQN [[17](#bib.bib17)]
    | GCN [[78](#bib.bib78)] | 方形网格 | 实际 | ✓ | ✓ |'
- en: '| Shou et al.[[79](#bib.bib79)] | 2020 | Fleet Management | Ridesharing | DQN [[17](#bib.bib17)]
    | AC [[37](#bib.bib37)] | Square-Grid | real | ✓ | ✓ |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Shou et al.[[79](#bib.bib79)] | 2020 | 车队管理 | 共享出行 | DQN [[17](#bib.bib17)]
    | AC [[37](#bib.bib37)] | 方形网格 | 实际 | ✓ | ✓ |'
- en: '| Jin et al.[[80](#bib.bib80)] | 2019 | Matching+Fleet Management | Ridesharing
    | DDPG[[40](#bib.bib40)] | MLP, RNN | Hexagon-Grid | real | ✓ | ✓ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Jin et al.[[80](#bib.bib80)] | 2019 | 匹配+车队管理 | 共享出行 | DDPG [[40](#bib.bib40)]
    | MLP, RNN | 六边形网格 | 实际 | ✓ | ✓ |'
- en: '| Holler et al.[[81](#bib.bib81)] | 2019 | Matching+Fleet Management | Ridesharing
    | DQN[[17](#bib.bib17)], PPO[[20](#bib.bib20)] | MLP | Square-Grid | real, sim
    | x | x |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Holler et al.[[81](#bib.bib81)] | 2019 | 匹配+车队管理 | 共享出行 | DQN [[17](#bib.bib17)],
    PPO [[20](#bib.bib20)] | MLP | 方形网格 | 实际, 模拟 | x | x |'
- en: '| Guo et al.[[82](#bib.bib82)] | 2020 | Matching+Fleet Management | Ridesharing
    | Double DQN[[31](#bib.bib31)] | CNN | Square-Grid | sim | x | x |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Guo et al.[[82](#bib.bib82)] | 2020 | 匹配+车队管理 | 共享出行 | 双重 DQN [[31](#bib.bib31)]
    | CNN | 方形网格 | 模拟 | x | x |'
- en: '| Liang et al.[[83](#bib.bib83)] | 2021 | Matching+Fleet Management | Ridesharing
    | DQN[[17](#bib.bib17)], A2C [[25](#bib.bib25)] | MLP | Graph-based | real | x
    | x |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Liang et al.[[83](#bib.bib83)] | 2021 | 匹配+车队管理 | 共享出行 | DQN [[17](#bib.bib17)],
    A2C [[25](#bib.bib25)] | MLP | 基于图的 | 实际 | x | x |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: 3.1.1 Conventional Methods for Order Matching
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 订单匹配的传统方法
- en: The order matching problem and many variants were widely studied in the field
    of Operations Research (OR). Given the deterministic information of both workers
    and demands, the problem can be summarized as bipartite matching and can be solved
    via the traditional Khun-Munkres (KM) algorithm[[8](#bib.bib8)]. Early methods
    were proposed using greedy algorithms to assign the nearest available vehicle
    to a ride request[[10](#bib.bib10)]. These methods omit the global demands and
    supplies, and thus cannot achieve optimal performances in the long run. With new
    demands and worker states updating continuously, stochastic modeling becomes a
    major challenge. Researchers developed heuristics to deal with it efficiently [[11](#bib.bib11),
    [84](#bib.bib84), [85](#bib.bib85), [14](#bib.bib14)]. Based on historical data
    and the predictable pattern of demands, Sungur et al.[[84](#bib.bib84)] use stochastic
    programming to model the uncertain demands in the courier delivery scenario. Lowalekar
    et al.[[85](#bib.bib85)] tackle the problem with stochastic optimization with
    Bender’s decomposition and propose a matching framework for on-demand ride-hailing.
    Hu and Zhou et al.[[14](#bib.bib14)] also formulate it as a dynamic problem and
    use heuristic policies to explore the structural space of the optimal.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 订单匹配问题及其许多变体在运筹学（OR）领域得到了广泛研究。在给定工作者和需求的确定性信息的情况下，该问题可以总结为二分匹配，并可以通过传统的Khun-Munkres（KM）算法[[8](#bib.bib8)]解决。早期的方法使用贪心算法将最近的可用车辆分配给乘车请求[[10](#bib.bib10)]。这些方法忽略了全局的需求和供应，因此无法在长期内实现最佳性能。随着新需求和工作者状态的不断更新，随机建模成为一个主要挑战。研究人员开发了启发式方法来有效应对[[11](#bib.bib11),
    [84](#bib.bib84), [85](#bib.bib85), [14](#bib.bib14)]。根据历史数据和需求的可预测模式，Sungur等[[84](#bib.bib84)]使用随机编程来建模快递送货场景中的不确定需求。Lowalekar等[[85](#bib.bib85)]通过Bender分解的随机优化方法解决了该问题，并提出了一个按需打车的匹配框架。Hu和Zhou等[[14](#bib.bib14)]也将其制定为动态问题，并使用启发式策略来探索最佳的结构空间。
- en: 3.1.2 DRL for order matching in Transportation Systems
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 运输系统中的订单匹配的深度强化学习
- en: Order matching is an essential decision and optimization problem in applications
    in transportation systems, such as ride-sharing services. Modern taxi and in-service
    vehicles can share their real-time coordinates and states to the centralized platform
    via mobile networks. On the other hand, each customer can generate new requests
    including the provided pick-up locations and the destinations as a demand pair.
    The platform receives emerging demands and thus executes online matching policies.
    In transportation DDS where the demand/worker ratio is relatively low, the coordination
    between demands and supplies is the principal issue. Thus the order matching among
    them is critical to improving the system operation efficiency. From the agents’
    perspective, an intuitive idea to formulate the MDP of order matching is to model
    all drivers in the system as different agents, leveraging multi-agent RL (MARL)
    techniques [[59](#bib.bib59), [61](#bib.bib61)]. However, direct multi-agent formulation
    in real-world scenarios without any simplification may suffer from the enormous
    joint action space of thousands of agents. As a solution, Li et al. [[59](#bib.bib59)]
    used the Mean Field Reinforcement Learning (MFRL) [[60](#bib.bib60)] that models
    the interactions in-between as each agent with the average of others. Zhou et
    al. [[61](#bib.bib61)] argue that no explicit cooperation or communication is
    needed in a large-scale scenario. They propose a decentralized execution method
    to dispatch orders following a joint evaluation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 订单匹配是运输系统应用中的一个重要决策和优化问题，例如拼车服务。现代出租车和在服务中的车辆可以通过移动网络将其实时坐标和状态分享给集中平台。另一方面，每个客户可以生成新的请求，包括提供的接送地点和目的地作为需求对。平台接收新出现的需求，因此执行在线匹配策略。在需求/工作者比例相对较低的运输DDS中，需求与供应之间的协调是主要问题。因此，它们之间的订单匹配对于提高系统操作效率至关重要。从代理的角度来看，制定订单匹配MDP的直观想法是将系统中的所有司机建模为不同的代理，利用多智能体强化学习（MARL）技术[[59](#bib.bib59),
    [61](#bib.bib61)]。然而，现实世界场景中直接的多智能体建模可能会遭遇数千个代理的庞大联合动作空间。作为解决方案，Li等[[59](#bib.bib59)]使用了均值场强化学习（MFRL）[[60](#bib.bib60)]，将代理之间的相互作用建模为每个代理与其他代理的平均值。Zhou等[[61](#bib.bib61)]认为在大规模场景中不需要明确的合作或通信。他们提出了一种去中心化执行方法，通过联合评估来调度订单。
- en: As a comparison, another simplified and commonly accepted method to model the
    cooperation is to train a single policy and implement it to all workers online [[56](#bib.bib56),
    [62](#bib.bib62), [63](#bib.bib63), [86](#bib.bib86)]. In this formulation, all
    workers are defined with homogeneous state, action space, and reward definitions.
    Even though the system is still multi-agent from the global perspective, the training
    stage only considers a single one. Specifically, Xu et al. [[56](#bib.bib56)]
    model order matching as a sequential decision-making problem and develop a joint
    learning-and-planning approach. They use Temporal Difference (TD) [[30](#bib.bib30)]
    to learn the approximate driver value function in the learning stage, and then
    use the KM algorithm to solve the bipartite matching problem based on learned
    values during planning. Wang et al. [[62](#bib.bib62)] propose a transfer learning
    method to increase the learning adaptability and efficiency, where the learned
    order matching model can be transferred to other cities. They use the DQN algorithm
    to estimate the value network. Tang et al. [[63](#bib.bib63)] further utilize
    the double-DQN framework to obtain a more stable learning process. Since online
    dynamic order matching scenario requires comprehensive consideration upon spatial-temporal
    features, they develop a special network structure using hierarchical coarse coding
    and cerebellar embedding memories for better representations. Leveraging the ST-features,
    He et al. [[64](#bib.bib64)] also develops a capsule-based network for better
    representations. Jindal et al. [[58](#bib.bib58)] only concentrated on the ride-pooling
    task, and design their agent to decide whether a vehicle is to take a single or
    multiple passengers. Detailed matching is left to low-level algorithms. The homogeneous
    agent formulation avoids common challenges of multi-agent RL, including the exponential
    decision space of different agents. Besides, complicated communication is also
    avoided since all agents share the same state.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对比，另一种简化且广泛接受的合作建模方法是训练一个单一的策略并将其应用于所有工作者在线[[56](#bib.bib56), [62](#bib.bib62),
    [63](#bib.bib63), [86](#bib.bib86)]。在这种表述中，所有工作者都被定义为具有相同的状态、动作空间和奖励定义。尽管从全局角度来看系统仍是多智能体，但训练阶段只考虑一个智能体。具体而言，Xu等人[[56](#bib.bib56)]将订单匹配建模为一个顺序决策问题，并开发了一个联合学习和规划的方法。他们在学习阶段使用时间差分（TD）[[30](#bib.bib30)]来学习近似的驱动值函数，然后在规划阶段使用KM算法解决基于学习值的二分匹配问题。Wang等人[[62](#bib.bib62)]提出了一种迁移学习方法，以提高学习的适应性和效率，其中学习到的订单匹配模型可以迁移到其他城市。他们使用DQN算法来估计值网络。Tang等人[[63](#bib.bib63)]进一步利用双DQN框架来获得更稳定的学习过程。由于在线动态订单匹配场景需要综合考虑时空特征，他们开发了一种特殊的网络结构，使用层次化粗编码和小脑嵌入记忆以获得更好的表示。利用ST特征，He等人[[64](#bib.bib64)]还开发了一种基于胶囊的网络以获得更好的表示。Jindal等人[[58](#bib.bib58)]只集中于拼车任务，并设计了他们的智能体来决定一辆车是接单个乘客还是多个乘客。详细匹配则留给低级算法。均质智能体的表述避免了多智能体强化学习的常见挑战，包括不同智能体的指数决策空间。此外，由于所有智能体共享相同的状态，也避免了复杂的通信。
- en: Instead of referring different workers as agents, a request of the complete
    request list is treated as the agent. Yang et al. [[69](#bib.bib69)] models each
    demand as an agent and train a value network to estimate the values of demands
    instead of workers. A separate many-to-many matching process is further executed
    based on the learned values. Since online order matching includes non-stationariness
    from high dynamics, some literature also attempts to find solutions by concentrating
    on each time window to transform it into a static problem [[68](#bib.bib68), [67](#bib.bib67)]
    following such an agent modeling. Ke et al. [[68](#bib.bib68)] models each request
    as an agent, and all agents share the same policy. The action space of each agent
    is considered as whether to delay the current request to the next time window
    for further matching decisions. Wang et al. [[67](#bib.bib67)] train a single
    agent which represents the entire request list and decides how long the current
    window lasts. In both formulations, eventual matching results are generated by
    static bipartite graph matching.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在不将不同的工作者视为代理的情况下，整个请求列表被视为代理。杨等人[[69](#bib.bib69)]将每个需求建模为一个代理，并训练一个价值网络来估计需求的价值，而不是工作者。基于学习到的价值，进一步执行一个独立的多对多匹配过程。由于在线订单匹配包含来自高动态的非平稳性，一些文献还尝试通过关注每个时间窗口，将其转化为静态问题[[68](#bib.bib68)，[67](#bib.bib67)]，以这种代理建模方式。柯等人[[68](#bib.bib68)]将每个请求建模为一个代理，所有代理共享相同的策略。每个代理的动作空间被视为是否将当前请求延迟到下一个时间窗口以做进一步匹配决策。王等人[[67](#bib.bib67)]训练一个代表整个请求列表的单一代理，并决定当前窗口持续多久。在这两种形式中，最终的匹配结果是通过静态二分图匹配生成的。
- en: 3.1.3 DRL for Order Matching in Logistic Systems
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 物流系统中的订单匹配的DRL
- en: Not only important in practical applications in transportation, but order matching
    is also essential in modern logistic systems. As pick-up requests come in real-time
    with many couriers picking up packages, how to manage couriers to ensure cooperation
    among them and to complete more pick-up tasks in a long time is important but
    challenging.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 订单匹配不仅在运输实践中重要，在现代物流系统中也至关重要。由于取件请求实时出现，许多快递员进行包裹取件，如何管理快递员以确保他们之间的合作，并在较长时间内完成更多的取件任务是重要且具有挑战性的。
- en: With the requirement of fast responding to on-demand delivery customers, modern
    on-demand delivery systems need effective matching strategies to assign new demands
    to couriers. Chen et al. proposed a framework that utilizes multi-layer images
    of the spatial-temporal maps to capture real-time representations in the service
    areas. They model different couriers as multiple agents and use Proximal Policy
    Optimization (PPO) [[20](#bib.bib20)] to train the corresponding policy. As for
    the more common express systems, researchers also focus on developing an effective
    and efficient intelligent express system by optimizing the order matching problem.
    Zhang et al. first systematically study the large-scale dynamic city express problem,
    and adopt a batch assignment strategy that computes the pickup-delivery routes
    for a group of requests received in a short period rather than dealing with each
    request individually [[87](#bib.bib87)]. Rather than using the heuristic-based
    methods, Li et al. further proposed a soft-label clustering algorithm named BDSB
    to dispatch parcels to couriers in each region [[59](#bib.bib59)]. A novel Contextual
    Cooperative Reinforcement Learning (CCRL) model is further proposed to guide where
    should each courier deliver and serve in each short period. Rather than considering
    both pickup and delivery tasks, Li et al. further proposed a Cooperative Multi-Agent
    Reinforcement Learning model to learn courier dispatching policies [[71](#bib.bib71)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要快速响应按需配送客户，现代按需配送系统需要有效的匹配策略来将新需求分配给快递员。陈等人提出了一个框架，利用空间-时间地图的多层图像捕捉服务区域的实时表示。他们将不同的快递员建模为多个代理，并使用Proximal
    Policy Optimization (PPO) [[20](#bib.bib20)] 来训练相应的策略。对于更常见的快递系统，研究人员也专注于通过优化订单匹配问题来开发有效且高效的智能快递系统。张等人首先系统地研究了大规模动态城市快递问题，并采用批量分配策略，为在短时间内接收到的一组请求计算取货-配送路线，而不是逐个处理每个请求[[87](#bib.bib87)]。李等人进一步提出了一种名为BDSB的软标签聚类算法，将包裹分派给每个区域的快递员[[59](#bib.bib59)]。进一步提出了一种新的上下文协作强化学习（CCRL）模型，以指导每个快递员在每个短时间内的配送和服务。李等人还提出了一种合作多智能体强化学习模型，以学习快递员调度策略[[71](#bib.bib71)]。
- en: 3.2 Fleet Management
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 车队管理
- en: When a service worker is not assigned with demands and idling temporarily, a
    well-considered reposition strategy upon him can increase the possibility of future
    service chances and thus increase the entire platform’s revenue. Such a repositioning
    process forms the important fleet management problem which is also presented as
    vehicle positioning or taxi dispatching [[77](#bib.bib77)]. A straightforward
    intuition is that reasonable management can help balance the demands and supplies
    in different regions, thus help to improve the demand matching rate. We present
    the commonly accepted MDP modeling for fleet management problem and investigate
    the related DRL applications.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务人员没有被分配任务并暂时闲置时，针对他们的良好重新定位策略可以增加未来服务机会的可能性，从而增加整个平台的收入。这样的重新定位过程形成了重要的车队管理问题，也被称为车辆定位或出租车调度[[77](#bib.bib77)]。一个简单的直觉是，合理的管理可以帮助平衡不同区域的需求和供应，从而提高需求匹配率。我们介绍了车队管理问题的常见
    MDP 建模，并调查了相关的 DRL 应用。
- en: 3.2.1 Conventional Methods for Fleet Management
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 传统的车队管理方法
- en: Balancing the distributions of both DDS workers and demands was extensively
    studied, especially for the transportation systems. For instance, the balance
    of taxis and customers is essential in an efficient transportation system [[88](#bib.bib88)].
    Traditional methods were mostly based on data-driven approaches, which highly
    investigate the historical records of the supply and demand distributions. Miao
    et al. capture the uncertain sets of random demand probability distributions via
    spatial-temporal features [[89](#bib.bib89)]. Yuan et al. and Qu et al. also construct
    a recommend system for vehicles to provide recommended options for repositioning [[90](#bib.bib90),
    [91](#bib.bib91)]. Various techniques, including mixed-integer programming and
    combinatorial optimizations, are utilized to model and solve the fleet management
    problem [[92](#bib.bib92), [93](#bib.bib93)].
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于运输系统中 DDS 工作人员和需求的分配平衡进行了广泛的研究。例如，出租车和顾客的平衡在高效的运输系统中至关重要[[88](#bib.bib88)]。传统的方法主要基于数据驱动的方法，这些方法高度依赖于对供应和需求分布的历史记录进行调查。苗等人通过时空特征捕捉随机需求概率分布的不确定集合[[89](#bib.bib89)]。袁等人和屈等人还构建了一个推荐系统，为车辆提供重新定位的推荐选项[[90](#bib.bib90),
    [91](#bib.bib91)]。各种技术，包括混合整数规划和组合优化，均被用于建模和解决车队管理问题[[92](#bib.bib92), [93](#bib.bib93)]。
- en: 3.2.2 DRL for Fleet Management
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 用于车队管理的 DRL
- en: Following the idea of partitioning the city area into local grids to reduce
    computation cost, the MDP modeling of fleet management is also constructed based
    on the discrete dimension space. Given the spatial-temporal states of the workers
    in the fleet as individual agents and the information of dynamically updated customers,
    an intuition is to reposition available workers to locations with a larger demand/supply
    ratio than their current ones. For computational efficiency, the agents within
    the same grid at the same period are often considered as the same agents[[73](#bib.bib73)].
    The goal of the platform is to maximize the long-term revenue of the entire platform
    of all agents or the total response rate, so as in order matching. Since measurement
    includes detailed matching between demands and workers, an intuitive assumption
    is that a worker can only be matched with the demand providers from its current
    neighbor grids. The action of each agent is defined based on the grid maps, which
    contains $x+1$ discrete action choices including moving to one of its neighbors
    in the $x$-way connected grids or staying as where it is.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 根据将城市区域划分为局部网格以降低计算成本的想法，车队管理的 MDP 建模也基于离散维度空间构建。考虑到车队中工作人员的时空状态作为个体代理和动态更新的顾客信息，直观的做法是将可用的工作人员重新定位到需求/供应比率高于当前地点的位置。为了计算效率，同一网格在同一时间段内的代理通常被视为相同的代理[[73](#bib.bib73)]。平台的目标是最大化所有代理的长期收入或总响应率，因此在订单匹配中也是如此。由于测量包括需求和工作人员之间的详细匹配，一个直观的假设是一个工作人员只能与其当前邻近网格的需求提供者匹配。每个代理的动作是基于网格地图定义的，网格地图包含
    $x+1$ 个离散动作选择，包括移动到 $x$-方式连接网格中的一个邻居或保持当前位置。
- en: Following such a formulation, many DRL-based methods have been proposed recently
    to address the fleet management problem [[73](#bib.bib73), [76](#bib.bib76), [75](#bib.bib75),
    [94](#bib.bib94), [95](#bib.bib95), [74](#bib.bib74), [77](#bib.bib77), [79](#bib.bib79)]
    in recent years. Lin et al. [[73](#bib.bib73)] model the cooperation within the
    fleet as a multi-agent environment and propose a MARL-based solution for fleet
    management. Zhang et al. [[74](#bib.bib74)] develop a DDQN [[35](#bib.bib35)]
    based framework to learn to rewrite the current repositioning policy. Wen [[75](#bib.bib75)]
    explore a new taxi driver perspective upon the fleet management problem. They
    focus on increasing the individual incomes of drivers and demonstrate that higher
    revenues for drivers can help bring more drivers into the platform, and thus improve
    service availability for service customers. Shou et al. [[79](#bib.bib79)] further
    address the suboptimal equilibrium due to the competition among different drivers.
    They propose a reward design scheme and establish multi-agent modeling of different
    drivers. In these works, as the action space could be extremely large for fleet
    management in a city, deep Q-network learning [[17](#bib.bib17)] has been commonly
    adopted by state-of-the-art approaches to accelerate the policy learning process.
    The agents could fast interact with the environment based on the learned Q-value
    and decide their next movements accordingly.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这样的表述，近年来已经提出了许多基于 DRL 的方法来解决车队管理问题 [[73](#bib.bib73), [76](#bib.bib76), [75](#bib.bib75),
    [94](#bib.bib94), [95](#bib.bib95), [74](#bib.bib74), [77](#bib.bib77), [79](#bib.bib79)]。Lin
    等人 [[73](#bib.bib73)] 将车队内的合作建模为多代理环境，并提出了一种基于 MARL 的车队管理解决方案。Zhang 等人 [[74](#bib.bib74)]
    开发了一种基于 DDQN [[35](#bib.bib35)] 的框架，以学习重写当前的重新定位策略。Wen [[75](#bib.bib75)] 探索了对车队管理问题的新出租车司机视角。他们专注于提高司机的个人收入，并证明司机的更高收入可以帮助吸引更多的司机加入平台，从而提高服务客户的服务可用性。Shou
    等人 [[79](#bib.bib79)] 进一步解决了由于不同司机之间的竞争而导致的次优均衡。他们提出了一种奖励设计方案，并建立了不同司机的多代理建模。在这些工作中，由于城市中车队管理的动作空间可能非常大，因此深度
    Q 网络学习 [[17](#bib.bib17)] 已被最先进的方法广泛采用，以加速策略学习过程。代理可以基于学习到的 Q 值快速与环境交互，并相应地决定他们的下一步动作。
- en: 3.3 Joint Scheduling of Order Matching and Fleet Management
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 订单匹配与车队管理的联合调度
- en: Besides individual studies upon order matching and fleet management, researchers
    also attempt to develop algorithms for both problems and consider it as an integrated
    dispatching stage[[80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82)].
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对订单匹配和车队管理的个别研究外，研究人员还尝试开发针对这两个问题的算法，并将其视为一个综合调度阶段[[80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82)]。
- en: Since the action spaces of the two problems are heterogeneous, Jin et al. [[80](#bib.bib80)]
    proposed a hierarchical DRL-based structure to measure the two stages. Specifically,
    they design a unified action as the ranking weight vector to rank and select the
    specific order for matching or the destination for fleet managing. Holler et al. [[81](#bib.bib81)]
    separate the two phases of the joint platform. They first treat the drivers as
    individual agents for order matching and then establish a central fleet management
    agent that is responsible for all individual drivers. Guo et al. [[82](#bib.bib82),
    [8](#bib.bib8)] use a double DQN based framework to solve the fleet management
    problem ahead and leave the detailed Order Matching to the traditional Khun-Munkres
    (KM) algorithm. Liang et al. [[83](#bib.bib83)] preserve the topology of the initial
    graph-based supply-demand distribution structure instead of discretizing them
    using a grid view. A special centralized programming planning module is developed
    to dispatch thousands of taxis on a real-time basis.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个问题的动作空间是异质的，Jin 等人 [[80](#bib.bib80)] 提出了一个基于层次化 DRL 的结构来度量这两个阶段。具体来说，他们设计了一个统一的动作作为排序权重向量，以对匹配的具体订单或车队管理的目的地进行排序和选择。Holler
    等人 [[81](#bib.bib81)] 将联合平台的两个阶段分开。他们首先将司机视为订单匹配的独立代理，然后建立一个中央车队管理代理，负责所有独立的司机。Guo
    等人 [[82](#bib.bib82), [8](#bib.bib8)] 使用基于双重 DQN 的框架来提前解决车队管理问题，并将详细的订单匹配留给传统的
    Khun-Munkres (KM) 算法。Liang 等人 [[83](#bib.bib83)] 保留了初始图形化的供需分布结构的拓扑，而不是使用网格视图对其进行离散化。开发了一个特殊的集中编程规划模块，用于实时调度成千上万的出租车。
- en: A major challenge of integrating the entire dispatching stage is that it is
    difficult to model the heterogeneous actions in two individual phases. A well-designed
    unified latent representation of agent states is essential to augment the policy
    exploration ability and the robustness of training in a joint DRL framework. Further
    joint research of the two phases remains as the opportunity for effective DDS.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 整合整个调度阶段的主要挑战在于很难建模两个个别阶段中的异质性动作。设计良好的统一潜在状态表示对于增强策略探索能力和在联合 DRL 框架中的训练鲁棒性至关重要。进一步联合研究这两个阶段仍然是有效
    DDS 的机会。
- en: '4 Stage2: Routing'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '4 Stage2: 路线规划'
- en: By assigning tasks to different workers and balancing the relationship between
    workers and demands, the service loops are constructed. The second stage of DDS
    scheduling is to determine how to serve each demand pair within the constructed
    service loops. For example, in the ride-pooling situation, a driver at a time
    may have several customers on the car and should decide the individual service
    priority. Routing is a more common problem in logistic systems where the ratio
    of worker/demand is much larger. For example, an express van may be assigned with
    more than a hundred delivery demands within its current service loop. A well-considered
    visiting strategy to execute the loop is critical to reducing the expenses.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将任务分配给不同的工作者并平衡工作者与需求之间的关系，服务循环被构建。DDS 调度的第二阶段是确定如何在构建的服务循环内服务每对需求。例如，在拼车情况下，司机可能在车上同时有几位乘客，并需要决定每位乘客的个体服务优先级。路线规划是物流系统中一个更常见的问题，其中工作者/需求的比例要大得多。例如，一辆快递车可能在其当前服务循环内被分配超过一百个配送需求。精心设计的访问策略对于减少开支至关重要。
- en: Generally, the routing problems can be derived from the conventional VRP problem.
    For convenience, we first provide a mathematical formulation of typical Capacitied
    VRP (CVRP), then discuss the recent DRL-based solutions on solving the routing
    problems.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，路线规划问题可以从传统的 VRP 问题中推导出来。为了方便起见，我们首先提供典型容量限制 VRP (CVRP) 的数学表述，然后讨论最近基于 DRL
    的解决方案在解决路线规划问题上的应用。
- en: 4.1 Formulation of Typical CVRP
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 典型 CVRP 的表述
- en: The basic requirement of VRP is to design a routing strategy with a minimum
    cost for a fleet of vehicles, given the demands of a set of known customers. All
    customers must be assigned to one vehicle to have their parcels either be picked
    up or delivered. All vehicles have limited capacities, $c_{i}$, and should originate
    and terminate at a given depot, $v_{0}$, which also offers reloading service.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: VRP 的基本要求是为一组已知客户的需求设计一项最小成本的路线策略。所有客户必须被分配到一辆车上，以便其包裹能够被取件或配送。所有车辆都有有限的容量 $c_{i}$，并应从给定的仓库
    $v_{0}$ 出发和结束，仓库还提供重新装载服务。
- en: We represent a fleet of vehicles denoted by $V$, and a set of $n$ known customers
    by $C$, which formulate a directed graph $G$. The total graph includes $|C|+2$
    vertices, where the depot is double represented by vertex $0$ and $n+1$. The set
    of arcs denoted by $A$ represents the traveling cost between customers and the
    depot and among customers. We associate a spatial distance $c_{ij}$ and a temporal
    distance cost $t_{ij}$ with each $arc(i,j)$ when $i\neq j$. $G$ includes $|V|$
    subgraphs. Each connected subgraph represents a single route by vehicle $m$ and
    has to start from vertex $0$ and ends at vertex $n+1$ with several customers in-between,
    denoted by $G_{m}$. Each vehicle $m$ has a capacity $c_{m}$. Each customer $i$
    has a demand $d_{i}$. The real-time shipment should not exceed $c_{m}$.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表示一个由 $V$ 表示的车队，以及一个由 $C$ 表示的 $n$ 个已知客户的集合，这些形成了一个有向图 $G$。整个图包括 $|C|+2$ 个顶点，其中仓库由顶点
    $0$ 和 $n+1$ 双重表示。表示客户和仓库之间以及客户之间旅行成本的弧集用 $A$ 表示。当 $i \neq j$ 时，我们将空间距离 $c_{ij}$
    和时间距离成本 $t_{ij}$ 与每个 $arc(i,j)$ 相关联。$G$ 包括 $|V|$ 个子图。每个连接的子图表示由车辆 $m$ 行驶的单一路径，并必须从顶点
    $0$ 开始，结束于顶点 $n+1$，中间有几个客户，表示为 $G_{m}$。每辆车 $m$ 有一个容量 $c_{m}$。每个客户 $i$ 有一个需求 $d_{i}$。实时运输不应超过
    $c_{m}$。
- en: 'We further denote two decision variables $x_{ijm}$ and $s_{im}$, and define
    $x_{ijm}=1$, if and only if the $arc(i,j)$ is included in $G_{m}$, where $i\neq
    j,i\neq n+1,j\neq 0$, while $s_{im}$ represents the time stamp when vehicle $m$
    serves customer $i$. By such denotations, we formulate the VRP mathematically
    as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步表示两个决策变量 $x_{ijm}$ 和 $s_{im}$，并定义 $x_{ijm}=1$，当且仅当 $arc(i,j)$ 包含在 $G_{m}$
    中，其中 $i \neq j, i \neq n+1, j \neq 0$，而 $s_{im}$ 表示车辆 $m$ 服务客户 $i$ 的时间戳。通过这些表示，我们将
    VRP 数学化表述如下：
- en: '|  |  | $\displaystyle\min\ \sum\nolimits_{m=1}^{&#124;V&#124;}\sum\nolimits_{i=0}^{&#124;C&#124;}\sum\nolimits_{j=1}^{&#124;C&#124;+1}c_{ij}x_{ijm},$
    |  | (1) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\min\ \sum\nolimits_{m=1}^{|V|}\sum\nolimits_{i=0}^{|C|}\sum\nolimits_{j=1}^{|C|+1}c_{ij}x_{ijm},$
    |  | (1) |'
- en: '|  |  | $\displaystyle\ s.t.\ \ \sum\nolimits_{m=1}^{&#124;V&#124;}\sum\nolimits_{j=1}^{&#124;C&#124;+1}x_{ijm},\forall
    i\in C,$ |  | (2) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\ s.t.\ \ \sum\nolimits_{m=1}^{|V|}\sum\nolimits_{j=1}^{|C|+1}x_{ijm},\forall
    i\in C,$ |  | (2) |'
- en: '|  |  | $\displaystyle\quad\ \ \ \ \sum\nolimits_{i=0}^{&#124;C&#124;}x_{i,n+1,m}=\sum\nolimits_{j=1}^{&#124;C&#124;+1}x_{0,j,m}=1,\forall
    m\in V,$ |  | (3) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\ \ \ \ \sum\nolimits_{i=0}^{|C|}x_{i,n+1,m}=\sum\nolimits_{j=1}^{|C|+1}x_{0,j,m}=1,\forall
    m\in V,$ |  | (3) |'
- en: '|  |  | $\displaystyle\quad\quad\ x_{ijm}\in\{0,1\},\forall i,j\in C,\forall
    minV,$ |  | (4) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\quad\ x_{ijm}\in\{0,1\},\forall i,j\in C,\forall
    minV,$ |  | (4) |'
- en: '|  |  | $\displaystyle\quad\ \ \ \ \sum\nolimits_{i=1}^{&#124;C&#124;}d_{i}\sum\nolimits_{j=1}^{&#124;C&#124;+1}x_{ijm}\leq
    c_{m},\forall minV,$ |  | (5) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad\ \ \ \ \sum\nolimits_{i=1}^{|C|}d_{i}\sum\nolimits_{j=1}^{|C|+1}x_{ijm}\leq
    c_{m},\forall minV,$ |  | (5) |'
- en: where (1) represents the routing objective. Constraints (2), (3), and (4) make
    all customers visited and only visited once. (5) indicates that a vehicle should
    always yield to its capacity limit, and (6) requires all services made within
    the individual time windows.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 (1) 表示路由目标。约束条件 (2)、(3) 和 (4) 确保所有客户都被访问且仅访问一次。(5) 表明车辆应始终遵守其容量限制，(6) 要求所有服务都在各自的时间窗口内完成。
- en: '![Refer to caption](img/34bd0a8a0b10cbe7253f0cca639ad09f.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34bd0a8a0b10cbe7253f0cca639ad09f.png)'
- en: 'Figure 9: A summary illustration of the typical VRP and its common variants.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：典型VRP及其常见变体的概述插图。
- en: 'TABLE III: Applications of using deep reinforcement learning to solve DDS problems.
    The information of each literature reference consists of the published year, the
    problem solved, the DRL training paradigm used, the data type (Dtype) used, whether
    the data is available (Davail) and whether the code is released.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：使用深度强化学习解决DDS问题的应用。每篇文献参考的信息包括出版年份、解决的问题、所使用的DRL训练范式、使用的数据类型（Dtype）、数据是否可用（Davail）以及代码是否发布。
- en: '|  Reference | Year | Problem | Scenario | Algorithm | Network | Dscheme |
    Dtype | Davail | Code |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  参考 | 年份 | 问题 | 场景 | 算法 | 网络 | Dscheme | Dtype | 数据是否可用 | 代码 |'
- en: '|  Nazari et al. [[18](#bib.bib18)] | 2018 | Typical VRP | Mathematical | REINFORCE [[21](#bib.bib21)],
    A3C[[38](#bib.bib38)] | RNN | Graph-based | sim | ✓ | ✓ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  Nazari 等人 [[18](#bib.bib18)] | 2018 | 典型VRP | 数学 | REINFORCE [[21](#bib.bib21)],
    A3C[[38](#bib.bib38)] | RNN | 基于图的 | 仿真 | ✓ | ✓ |'
- en: '| Kool et al. [[19](#bib.bib19)] | 2019 | Typical VRP | Mathematical | REINFORCE[[21](#bib.bib21)]
    | ATT | Graph-based | sim | ✓ | ✓ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Kool 等人 [[19](#bib.bib19)] | 2019 | 典型VRP | 数学 | REINFORCE[[21](#bib.bib21)]
    | ATT | 基于图的 | 仿真 | ✓ | ✓ |'
- en: '| Chen et al. [[96](#bib.bib96)] | 2019 | Typical VRP | Mathematical | A2C[[25](#bib.bib25)]
    | MLP | Graph-based | sim | ✓ | ✓ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 [[96](#bib.bib96)] | 2019 | 典型VRP | 数学 | A2C[[25](#bib.bib25)] |
    MLP | 基于图的 | 仿真 | ✓ | ✓ |'
- en: '| Lu et al. [[97](#bib.bib97)] | 2019 | Typical VRP | Mathematical | REINFORCE [[21](#bib.bib21)]
    | MLP, ATT | Graph-based | sim | ✓ | ✓ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Lu 等人 [[97](#bib.bib97)] | 2019 | 典型VRP | 数学 | REINFORCE [[21](#bib.bib21)]
    | MLP, ATT | 基于图的 | 仿真 | ✓ | ✓ |'
- en: '| Duan et al. [[98](#bib.bib98)] | 2020 | Typical VRP | Logistics | REINFORCE[[21](#bib.bib21)]
    | GCN, ATT | Graph-based | real | x | x |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Duan 等人 [[98](#bib.bib98)] | 2020 | 典型VRP | 物流 | REINFORCE[[21](#bib.bib21)]
    | GCN, ATT | 基于图的 | 真实 | x | x |'
- en: '| Delarue et al. [[99](#bib.bib99)] | 2020 | Typical VRP | Mathematical | Model-based
    | MLP | Graph-based | sim | x | x |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Delarue 等人 [[99](#bib.bib99)] | 2020 | 典型VRP | 数学 | 基于模型 | MLP | 基于图的 | 仿真
    | x | x |'
- en: '| Xin et al. [[100](#bib.bib100)] | 2020 | Typical VRP | Mathematical | REINFORCE[[21](#bib.bib21)]
    | ATT | Graph-based | sim | x | x |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Xin 等人 [[100](#bib.bib100)] | 2020 | 典型VRP | 数学 | REINFORCE[[21](#bib.bib21)]
    | ATT | 基于图的 | 仿真 | x | x |'
- en: '| Joe et al. [[101](#bib.bib101)] | 2020 | dynamic VRP | Logistics | DQN[[17](#bib.bib17)]
    | MLP | Graph-based | real | x | x |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Joe 等人 [[101](#bib.bib101)] | 2020 | 动态VRP | 物流 | DQN[[17](#bib.bib17)] |
    MLP | 基于图的 | 真实 | x | x |'
- en: '| Ottoni et al. [[102](#bib.bib102)] | 2021 | TSP with Refueling (as EVRP)
    | Mathematical | Q-Learning [[28](#bib.bib28)], SARSA [[16](#bib.bib16)] | - |
    Graph-based | sim | x | x |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Ottoni 等人 [[102](#bib.bib102)] | 2021 | 带加油的TSP（作为EVRP） | 数学 | Q-Learning [[28](#bib.bib28)],
    SARSA [[16](#bib.bib16)] | - | 基于图的 | 仿真 | x | x |'
- en: '| Qin et al.[[103](#bib.bib103)] | 2021 | Heterogeneous VRP | Mathematical
    | Double-DQN[[31](#bib.bib31)] | MLP, CNN | Graph-based | sim | x | x |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Qin 等人 [[103](#bib.bib103)] | 2021 | 异质VRP | 数学 | Double-DQN[[31](#bib.bib31)]
    | MLP, CNN | 基于图的 | 仿真 | x | x |'
- en: '| Bogyrbayeva et al.[[104](#bib.bib104)] | 2021 | Electric VRP | Ridesharing
    | REINFORCE[[21](#bib.bib21)] | RNN | Graph-based | sim | x | x |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Bogyrbayeva et al.[[104](#bib.bib104)] | 2021 | 电动VRP | 共享出行 | REINFORCE[[21](#bib.bib21)]
    | RNN | 基于图的 | 模拟 | x | x |'
- en: '| Shi et al.[[105](#bib.bib105)] | 2020 | Dynamic Electric VRP | Ridesharing
    | TD[[30](#bib.bib30)] | MLP | Graph-based | sim | x | x |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Shi et al.[[105](#bib.bib105)] | 2020 | 动态电动VRP | 共享出行 | TD[[30](#bib.bib30)]
    | MLP | 基于图的 | 模拟 | x | x |'
- en: '| James et al.[[106](#bib.bib106)] | 2019 | Electric with Time Windows | Logistics
    | REINFORCE[[21](#bib.bib21)] | RNN | Graph-based | real | x | ✓ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| James et al.[[106](#bib.bib106)] | 2019 | 带时间窗的电动VRP | 物流 | REINFORCE[[21](#bib.bib21)]
    | RNN | 基于图的 | 真实 | x | ✓ |'
- en: '| Lin et al. [[107](#bib.bib107)] | 2020 | Electric VRP with Time Windows |
    Logistics | REINFORCE[[21](#bib.bib21)] | ATT, RNN | Graph-based | sim | x | x
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Lin et al. [[107](#bib.bib107)] | 2020 | 带时间窗的电动VRP | 物流 | REINFORCE[[21](#bib.bib21)]
    | ATT, RNN | 基于图的 | 模拟 | x | x |'
- en: '| Zhang et al. [[108](#bib.bib108)] | 2020 | VRP with Time Windows | Logistics
    | REINFORCE[[21](#bib.bib21)] | ATT | Graph-based | sim | x | x |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. [[108](#bib.bib108)] | 2020 | 带时间窗的VRP | 物流 | REINFORCE[[21](#bib.bib21)]
    | ATT | 基于图的 | 模拟 | x | x |'
- en: '| Falkner et al.[[109](#bib.bib109)] | 2020 | VRP with Time Windows | Mathematical
    | REINFORCE[[21](#bib.bib21)] | MLP, ATT | Graph-based | sim | ✓ | x |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Falkner et al.[[109](#bib.bib109)] | 2020 | 带时间窗的车辆路径问题 (VRP) | 数学模型 | REINFORCE[[21](#bib.bib21)]
    | MLP, ATT | 基于图的 | 模拟 | ✓ | x |'
- en: '| Zhao et al. [[110](#bib.bib110)] | 2020 | VRP with Time Windows | Mathematical
    | AC [[37](#bib.bib37)] | ATT | Graph-based | sim | ✓ | x |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. [[110](#bib.bib110)] | 2020 | 带时间窗的VRP | 数学模型 | AC [[37](#bib.bib37)]
    | ATT | 基于图的 | 模拟 | ✓ | x |'
- en: '| Li et al. [[111](#bib.bib111)] | 2021 | VRP with Pickup and Delivery | Mathematical
    | REINFORCE[[21](#bib.bib21)] | ATT | Graph-based | sim | x | x |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[111](#bib.bib111)] | 2021 | 带取送货的VRP | 数学模型 | REINFORCE[[21](#bib.bib21)]
    | ATT | 基于图的 | 模拟 | x | x |'
- en: '| Li et al.[[112](#bib.bib112)] | 2021 | VRP with Pickup and Delivery | Logistics
    | Double-DQN[[31](#bib.bib31)] | MLP, ATT | Graph-based | real | x | x |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Li et al.[[112](#bib.bib112)] | 2021 | 带取送货的VRP | 物流 | Double-DQN[[31](#bib.bib31)]
    | MLP, ATT | 基于图的 | 真实 | x | x |'
- en: '| Lee et al.[[113](#bib.bib113)] | 2021 | VRP with Pickup and Delivery | Warehousing
    | Q-Learning[[28](#bib.bib28)] | - | Square-Grid | Sim | x | x |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Lee et al.[[113](#bib.bib113)] | 2021 | 带取送货的车辆路径问题 (VRP) | 仓储 | Q-Learning[[28](#bib.bib28)]
    | - | 方格网 | 模拟 | x | x |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: 4.2 Realistic Routing Problems
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 现实世界的路线规划问题
- en: 'Besides the typical VRP setting, real-world routing problems often require
    additional considerations with more realistic constraints and objectives. Many
    variants of VRP that tackle these practical constraints are thus closer to industrial
    applications and are also widely studied by researchers. We briefly introduce
    several important VRP variants, including dynamic VRP (DVRP), electric VRP (EVRP),
    VRP with time windows (VRPTW), and VRP with pickup and deliveries (VRPPD). An
    overview of the typical VRP and the above variants is illustrated in Figure [9](#S4.F9
    "Figure 9 ‣ 4.1 Formulation of Typical CVRP ‣ 4 Stage2: Routing ‣ Deep Reinforcement
    Learning for Demand Driven Services in Logistics and Transportation Systems: A
    Survey").'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '除了典型的VRP设置外，现实世界的路线规划问题通常需要额外的考虑，具备更现实的约束和目标。因此，许多解决这些实际约束的VRP变种更贴近工业应用，并且被研究者广泛研究。我们简要介绍几种重要的VRP变种，包括动态VRP
    (DVRP)、电动VRP (EVRP)、带时间窗的VRP (VRPTW) 和带取送货的VRP (VRPPD)。典型VRP及上述变种的概述见图 [9](#S4.F9
    "Figure 9 ‣ 4.1 Formulation of Typical CVRP ‣ 4 Stage2: Routing ‣ Deep Reinforcement
    Learning for Demand Driven Services in Logistics and Transportation Systems: A
    Survey")。'
- en: 4.2.1 Dynamic VRP (DVRP)
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 动态VRP (DVRP)
- en: Service demands may not be pre-obtained by the platform in real-world scenarios,
    thus the newly updated demands require assignment with workers dynamically [[6](#bib.bib6)].
    This is the same common challenge as discussed in the dispatching stage. However,
    rather than simply coordinating demands with customers, the routing stage also
    requires a specific routing strategy with the visiting orders of the matched demands
    for each worker. Joe et al. [[101](#bib.bib101)] utilize DQN to estimate the Q-value
    of the individual states for vehicles and insert new demands into the existing
    solution sequence. DRL is with potential to estimate the future reward with possible
    action attempts and is suitable to solve dynamic VRPs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，服务需求可能不会被平台预先获取，因此，新的需求需要动态地分配给工作人员[[6](#bib.bib6)]。这与调度阶段讨论的常见挑战相同。然而，与单纯协调需求不同，路线阶段还需要针对每个工作人员的匹配需求的访问订单制定特定的路线策略。Joe
    等人[[101](#bib.bib101)]利用 DQN 估计车辆个体状态的 Q 值，并将新需求插入现有的解决方案序列中。深度强化学习有潜力估计未来的奖励，并适合解决动态车辆路径规划问题。
- en: 4.2.2 Electric VRP (EVRP)
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 电动汽车路径规划问题（EVRP）
- en: As electric vehicles (EVs) become commonly accepted in recent years, researchers
    gradually establish their interest in how to route the EV fleets and thus form
    a special Electric VRP (EVRP) problem [[4](#bib.bib4)]. They focus on the application
    potential of EV in both ride-hailing and express systems [[105](#bib.bib105),
    [106](#bib.bib106)]. Since current EVs have shorter battery lives than traditional
    vehicles, EVRP considers the charging phase as an additional and essential action
    of the EV agents. Furthermore, the environment often contains information on the
    locations of charging stations.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 随着电动汽车（EV）近年来的广泛接受，研究人员逐渐对如何规划电动汽车车队的路线产生了兴趣，从而形成了一个特殊的电动汽车路径规划问题（EVRP）[[4](#bib.bib4)]。他们关注电动汽车在叫车和快递系统中的应用潜力[[105](#bib.bib105),
    [106](#bib.bib106)]。由于当前电动汽车的电池寿命比传统车辆更短，EVRP 将充电阶段视为电动汽车代理的一个额外且必要的行动。此外，环境通常包含充电站位置的信息。
- en: For EV usage in ride-hailing services, Shi et al. model the EV fleet operating
    as a dynamic EVRP[[105](#bib.bib105)]. At each decision step, an EV agent could
    either pass to keep idling, to charge at the local station, or to serve the customer
    demands. The detailed order dispatching of customer assignment is executed by
    KM algorithm[[8](#bib.bib8)]. For EV usage in delivery and express systems, James
    et al. consider both charging requirements of EVs and the possibility that not
    all customers are visited within the given time [[106](#bib.bib106)]. The optimization
    goal of such a framework is to both maximize the number of delivered logistic
    requests and minimize the total driving distance of all EVs. The two objectives
    are considered simultaneously using a weighted sum. Lin et al. consider the EVRP
    modeling along with individual time window limits of different customers, which
    will be discussed in the following  [[107](#bib.bib107)].
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电动汽车（EV）在叫车服务中的使用，Shi 等人将电动汽车车队建模为动态电动汽车路径规划问题（EVRP）[[105](#bib.bib105)]。在每个决策步骤中，电动汽车代理可以选择继续空闲、在本地站点充电或满足客户需求。客户分配的详细订单调度由
    KM 算法执行[[8](#bib.bib8)]。在送货和快递系统中，James 等人考虑了电动汽车的充电需求以及在给定时间内并非所有客户都能访问的可能性[[106](#bib.bib106)]。这种框架的优化目标是最大化已交付的物流请求数量并最小化所有电动汽车的总行驶距离。两个目标通过加权和同时考虑。Lin
    等人考虑了电动汽车路径规划问题（EVRP）建模以及不同客户的个体时间窗限制，这将在以下内容中讨论[[107](#bib.bib107)]。
- en: 4.2.3 VRP with Time Windows (VRPTW)
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 带时间窗的车辆路径规划问题（VRPTW）
- en: When a service demand is provided, it may also be attached with a corresponding
    time window that the worker should satisfy, which means the service must arrive
    at the service target location within the given time window [[5](#bib.bib5), [114](#bib.bib114)].
    In practice, a customer who orders food from a restaurant may expect its food
    to be delivered before it cools down. Detailed consideration of time window limits
    is essential in practical routing scenarios.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务需求提供时，可能还会附带相应的时间窗，工作人员需要在给定的时间窗内满足服务，即服务必须在规定的时间窗内到达服务目标位置[[5](#bib.bib5),
    [114](#bib.bib114)]。在实际操作中，从餐馆订餐的客户可能希望食物在变凉之前送达。时间窗限制的详细考虑在实际路线规划场景中至关重要。
- en: Zhang et al.[[108](#bib.bib108)] proposed a multi-agent framework by constructing
    the time window constraint as an additional penalty and generate the routing solutions
    of different vehicles one after another. James et al. [[106](#bib.bib106)] also
    consider the same constraint in the online electric vehicle routing problem, while
    does not force the vehicles to visit all given demands. Falkner et al. [[109](#bib.bib109)]
    proposed a joint attention mechanism to balance the coordination between vehicles
    and demands. Zhao et al. [[110](#bib.bib110)] designed a hybrid structure of both
    DRL and local search to solve both typical VRP and VRPTW.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang等人[[108](#bib.bib108)]通过将时间窗口约束构建为额外的惩罚，提出了一个多代理框架，并一个接一个地生成不同车辆的路由解决方案。James等人[[106](#bib.bib106)]也在在线电动车路由问题中考虑了相同的约束，但不强制车辆访问所有给定的需求。Falkner等人[[109](#bib.bib109)]提出了一种联合注意力机制来平衡车辆和需求之间的协调。Zhao等人[[110](#bib.bib110)]设计了DRL和局部搜索的混合结构，以解决典型的VRP和VRPTW问题。
- en: 4.2.4 VRP with Pickup and Deliveries (VRPPD)
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 带取货和送货的VRP (VRPPD)
- en: Other than the simplified situation where the service provider and the service
    destination share the same location, VRPPD is a more common problem setting in
    practical usage [[7](#bib.bib7)]. For example, the driver for ride-sharing is
    supposed to first pick up the customer from the origination, and then send him
    to his destination. How to handle the relationship between different service provider-target
    pairs, i.e., pickup-delivery pairs, is of great challenge. Li et al. [[111](#bib.bib111)]
    [[111](#bib.bib111)] proposes an attention-based structure by designing a special
    heterogeneous attention. They design several heterogeneous attention to leverage
    the different relations between customers within the static graph, including the
    pickup with paired-delivery, the pickup with other-deliveries, the pickup with
    other-pickups, and counter-wise if we switch the role of pickups and deliveries.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 除了服务提供者和服务目标共享相同位置的简化情况外，VRPPD在实际应用中是更常见的问题设置[[7](#bib.bib7)]。例如，共享乘车的司机应首先从起点接送客户，然后送到目的地。如何处理不同服务提供者-目标对之间的关系，即取送对，是一个重大挑战。Li等人[[111](#bib.bib111)]
    [[111](#bib.bib111)]提出了一种基于注意力的结构，通过设计特殊的异质注意力来处理。他们设计了几种异质注意力来利用静态图中客户之间的不同关系，包括配对送货的取货、其他送货的取货、其他取货的取货，以及如果我们交换取货和送货的角色则反之亦然。
- en: 4.3 Conventional Methods for Routing
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 路由的传统方法
- en: When VRP was defined in the early stage [[3](#bib.bib3)], researchers attempted
    to find exact methods to explore the exact optimal strategies.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期阶段定义了VRP时[[3](#bib.bib3)]，研究人员尝试找到精确的方法来探索最佳策略。
- en: Researchers attempted to find exact methods for solving VRP at the very beginning
    when it was early defined and constructed. The branch-and-bound method as a common
    approach for combinatorial optimizations was used as a solution [[9](#bib.bib9)].
    Lagrange relaxation based methods were proposed [[115](#bib.bib115), [116](#bib.bib116)],
    by which the problem could be solved with a minimum degree-constrained K-tree
    problem. Besides, Desrochers et al. firstly used the column generation to solve
    VRP[[117](#bib.bib117)]. The following column generation based methods initialize
    the problem with a small subset of variables and compute a corresponding solution,
    and keep improving the results based on linear programming gradually. However,
    due to the NP-hard nature of VRP, the performances of exact approaches are often
    poor and computationally expensive. The exact methods could only generate results
    slowly on small-sized datasets.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员在VRP最初定义和构建时尝试找到解决VRP的精确方法。分支定界法作为组合优化的常见方法被用作解决方案[[9](#bib.bib9)]。提出了基于拉格朗日松弛的方法[[115](#bib.bib115),
    [116](#bib.bib116)]，通过这些方法，问题可以通过最小度约束的K-树问题来解决。此外，Desrochers等人首次使用列生成法解决VRP[[117](#bib.bib117)]。随后的基于列生成的方法初始化问题时使用小子集的变量，计算相应的解决方案，并基于线性规划逐步改进结果。然而，由于VRP的NP难度，精确方法的表现通常较差且计算开销大。精确方法只能在小型数据集上缓慢生成结果。
- en: 'As a complementary to the poor performances, many heuristic-based methods were
    further developed to find near-optimal results instead. Compromise to the complexity
    of VRP and its variants, an acceptable loss on the solution quality can earn great
    efficiency improvement. For instance, the tabu search and local search as conventional
    metaheuristics were proposed to solve VRP[[118](#bib.bib118), [119](#bib.bib119)].
    New solutions in the neighborhood of the current one are continuously established
    and evaluated. On the contrary, genetic algorithms operate in a series of solutions
    instead of only one solution[[120](#bib.bib120), [116](#bib.bib116)]. Following
    the idea of genetics, children’s solutions are generated from the best solution
    parents from the previous generation. Such an iteration can help to find the approximate
    optimal. Instead of treating objectives to be optimized altogether, ant colony
    optimizations utilize several ant colonies to optimize different functions: the
    number of vehicles, the total distance, and others[[12](#bib.bib12)].'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对性能较差的补充，许多基于启发式的方法进一步发展，以找到近似最优的结果。考虑到 VRP 及其变体的复杂性，接受解决方案质量上的损失可以获得显著的效率提升。例如，禁忌搜索和局部搜索作为传统的元启发式方法被提出用于解决
    VRP[[118](#bib.bib118), [119](#bib.bib119)]。当前解邻域中的新解不断建立和评估。相反，遗传算法在一系列解中进行操作，而不是仅仅一个解[[120](#bib.bib120),
    [116](#bib.bib116)]。遵循遗传学的思想，从上一代的最佳解父母生成子代解。这种迭代可以帮助找到近似最优解。蚁群优化方法则利用多个蚁群来优化不同的函数，如车辆数量、总距离等[[12](#bib.bib12)]。
- en: Even though these heuristics outperform the exact methods in finding better
    solutions, they are limited in real-time decision-making. A recreate search method,
    for example, takes hours to generate solutions for ten thousand instances with
    100 customers each, which is not suitable in real-time applications. As another
    drawback, the optimal approximation of the heuristic methods highly relies on
    manually defined rules and expert knowledge, which is far from enough compared
    to the enormous searching space. New technology mechanisms are needed to further
    improve the solution quality.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些启发式方法在找到更好解决方案方面优于精确方法，但它们在实时决策中存在限制。例如，重新搜索方法需要几个小时为一万个实例（每个实例有 100 个客户）生成解决方案，这在实时应用中不适用。作为另一个缺点，启发式方法的最优逼近高度依赖于手动定义的规则和专家知识，这远远不够，无法覆盖巨大的搜索空间。需要新的技术机制来进一步提升解决方案质量。
- en: 4.4 DRL for Routing
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 深度强化学习用于路径规划
- en: '![Refer to caption](img/6512752f060e2925a699d31acc7510c2.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6512752f060e2925a699d31acc7510c2.png)'
- en: 'Figure 10: The illustration of sequence generation methods for generating VRP
    solutions via RL. In each decision step, the agent selects the next provider/target
    location to visit.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：通过 RL 生成 VRP 解决方案的序列生成方法的示意图。在每个决策步骤中，代理选择下一个要访问的供应商/目标位置。
- en: In recent years, many researchers attempt to utilize the deep reinforcement
    learning (RL) method to solve VRP and other combinatorial optimizations due to
    the ability to improve solution quality via a self-driven mechanism and the potential
    of an efficient solution generation process. With solution quality guaranteed,
    DRL-based methods benefit from the separation of offline training and online inferring.
    Even though it may take hours or even days to train a fully converged policy in
    the offline stage, the inference to new problem instances in industrial online
    applications may only take a second compared to the meta-heuristics that will
    take minutes or hours [[19](#bib.bib19)]. Generally, current works using DRL for
    solving VRP and its variants can be classified into sequence generation based
    methods and rewriting based methods.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，许多研究者尝试利用深度强化学习（RL）方法解决 VRP 和其他组合优化问题，因为它能通过自驱动机制提高解决方案质量，并且具有高效的解决方案生成过程。保证了解决方案质量后，基于
    DRL 的方法受益于离线训练和在线推断的分离。尽管在离线阶段训练一个完全收敛的策略可能需要几个小时甚至几天，但在工业在线应用中对新问题实例的推断可能只需一秒钟，而相比之下，元启发式方法可能需要几分钟或几小时[[19](#bib.bib19)]。一般来说，当前使用
    DRL 解决 VRP 及其变体的工作可以分为基于序列生成的方法和基于重写的方法。
- en: 4.4.1 Sequence Generation Methods
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 序列生成方法
- en: 'A common approach to generate VRP solutions is to generate a partial sequence
    gradually and finally obtains a complete solution. In such an MDP modeling, the
    updating between different states is to include a new unvisited node into the
    current solution, which naturally forms the action of the sequence generation
    agent. In each decision step, the action space is the selection upon all unvisited
    nodes, and the agent selects one of them as the next to visit. It is notable that
    in more realistic VRP variants, practical constraints may limit the selection
    space since unfeasible solutions are possible to be generated. The agent thus
    should consider these constraints, and usually design a mask scheme to filter
    out the unfeasible choices. The sequence generation method is illustrated in Figure [10](#S4.F10
    "Figure 10 ‣ 4.4 DRL for Routing ‣ 4 Stage2: Routing ‣ Deep Reinforcement Learning
    for Demand Driven Services in Logistics and Transportation Systems: A Survey").'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '生成 VRP 解的常见方法是逐步生成部分序列，最终获得完整的解。在这种 MDP 建模中，不同状态之间的更新是将一个新的未访问节点纳入当前解中，这自然形成了序列生成代理的动作。在每一个决策步骤中，动作空间是对所有未访问节点的选择，代理选择其中一个作为下一个访问的节点。值得注意的是，在更实际的
    VRP 变体中，实际约束可能会限制选择空间，因为可能会生成不可行的解。因此，代理需要考虑这些约束，通常设计一个掩码方案来过滤掉不可行的选择。序列生成方法如图
    [10](#S4.F10 "Figure 10 ‣ 4.4 DRL for Routing ‣ 4 Stage2: Routing ‣ Deep Reinforcement
    Learning for Demand Driven Services in Logistics and Transportation Systems: A
    Survey") 所示。'
- en: A special $PointerNetwork$ (PN) was first proposed under such a mechanism. Following
    a classic encoder-decoder structure, the PN structure is independent of the encoder
    length, and the output sequence is a subset of the input with a generated order [[121](#bib.bib121)].
    Even though the original PN was trained using supervised learning, it started
    following research exploration via DRL for more advanced VRP solutions. The basic
    structure of it is commonly utilized in the following research for routing problems.
    Bello et al. [[122](#bib.bib122)] first developed a special neural combinatorial
    optimization framework (NCO) via DRL, which showed its effectiveness on both performance
    and generating efficiency on TSP and the knapsack problem. Even though the typical
    VRP was not studied, NCO serves as an important benchmark that utilizes DRL to
    explore more effective combinatorial optimization solution structures. Nazari
    et al. [[18](#bib.bib18)] further followed the NCO structure and first applied
    it to VRP. Kool et al. [[19](#bib.bib19)] combined the structure with attention
    mechanism as augmentation and obtained performance improvement. They investigated
    several routing formulations, including the TSP, typical CVRP, Split Delivery
    VRP (SDVRP), Orienteering Problem (OP), Prize Collecting TSP (PCTSP), and Stochastic
    PCTSP (SPCTSP). The attention-based structure was further developed by the following
    researchers. Rather than relying on a single decoder for sequence generation,
    Xin et al.[[100](#bib.bib100)] proposed a multi-decoder mechanism to generate
    several partial solutions simultaneously and combine them using tree search to
    expand the searching space. Duan et al.[[98](#bib.bib98)] on the other hand, focused
    on more effective feature representation ability of the network itself. They augmented
    the structure with GCN, and develop a joint learning approach using both DRL and
    supervised learning. Delarue et al.[[99](#bib.bib99)] models the action selection
    from each state as a mixed-integer program(MIP) and combines the combinatorial
    structure of the action space with the pre-trained neural value functions by adapting
    the branch-and-cut algorithm[[123](#bib.bib123)].
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特殊的$PointerNetwork$（PN）在这种机制下首次提出。PN结构遵循经典的编码器-解码器结构，独立于编码器长度，输出序列是输入的一个子集，并且有生成的顺序[[121](#bib.bib121)]。尽管原始PN是使用监督学习训练的，但它开始通过DRL进行研究探索，以获得更先进的VRP解决方案。其基本结构通常用于以下研究中的路由问题。Bello等人[[122](#bib.bib122)]首先通过DRL开发了一个特殊的神经组合优化框架（NCO），在TSP和背包问题上表现出了在性能和生成效率上的有效性。尽管典型的VRP没有被研究，NCO作为一个重要的基准，利用DRL探索更有效的组合优化解决方案结构。Nazari等人[[18](#bib.bib18)]进一步跟随NCO结构，首次将其应用于VRP。Kool等人[[19](#bib.bib19)]将该结构与注意力机制结合作为增强，并获得了性能提升。他们调查了几种路由问题，包括TSP、典型的CVRP、分裂交付VRP（SDVRP）、定向问题（OP）、奖品收集TSP（PCTSP）和随机PCTSP（SPCTSP）。基于注意力的结构被后续研究者进一步发展。Xin等人[[100](#bib.bib100)]提出了一种多解码器机制，以同时生成多个部分解，并使用树搜索将它们结合起来以扩展搜索空间。另一方面，Duan等人[[98](#bib.bib98)]关注于网络自身更有效的特征表示能力。他们通过GCN增强了该结构，并使用DRL和监督学习开发了联合学习方法。Delarue等人[[99](#bib.bib99)]将每个状态的动作选择建模为混合整数规划（MIP），并将动作空间的组合结构与通过适应分支-切割算法的预训练神经价值函数相结合[[123](#bib.bib123)]。
- en: Owing to the independent training and inference stages, the sequence generation
    methods benefit from high inference result generation speed. For instance, generating
    routing solutions for 100 customers takes only 8 seconds, while the state-of-the-art
    heuristic-based solver, LKH3[[124](#bib.bib124)], takes more than 13 hours as
    a comparison. Sequence generation-based methods have great potential in handling
    new demand changes in online platform implementation. When new DDS demands arrive
    or past ones are either modified or canceled, such a fast framework could make
    real-time responses to these changes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于独立的训练和推理阶段，序列生成方法具有高推理结果生成速度。例如，为100个客户生成路由解决方案只需8秒，而最先进的基于启发式的方法LKH3[[124](#bib.bib124]]需要超过13小时作为比较。基于序列生成的方法在处理在线平台实施中的新需求变化方面具有很大的潜力。当新的DDS需求到达或过去的需求被修改或取消时，这样的快速框架可以实时响应这些变化。
- en: 4.4.2 Rewriting based Methods
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 基于重写的方法
- en: 'Besides generating partial solutions until completeness, researchers also searched
    for other MDP formulations for solving VRPs. An intuition originates from the
    continuous modification of current solutions in operations research, which is
    the core idea of many practical heuristics for VRP. Following this idea, researchers
    attempted to parameterize such a modification process to continuously improve
    solution quality. Such a modification process is also called Rewriting-based methods [[96](#bib.bib96)].
    The rewriting-based method is illustrated in Figure [11](#S4.F11 "Figure 11 ‣
    4.4.2 Rewriting based Methods ‣ 4.4 DRL for Routing ‣ 4 Stage2: Routing ‣ Deep
    Reinforcement Learning for Demand Driven Services in Logistics and Transportation
    Systems: A Survey").'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '除了生成部分解直到完整性外，研究人员还探索了其他 MDP 形式来解决 VRP 问题。直觉来源于运筹学中对当前解的持续修改，这也是许多实际启发式算法的核心思想。基于这一思想，研究人员尝试将这种修改过程参数化，以不断提高解的质量。这种修改过程也被称为基于重写的方法[[96](#bib.bib96)]。基于重写的方法在图[11](#S4.F11
    "Figure 11 ‣ 4.4.2 Rewriting based Methods ‣ 4.4 DRL for Routing ‣ 4 Stage2: Routing
    ‣ Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation
    Systems: A Survey")中进行了说明。'
- en: '![Refer to caption](img/acecb33bf501babe0654fdff60787455.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/acecb33bf501babe0654fdff60787455.png)'
- en: 'Figure 11: The illustration of rewriting based methods for generating VRP solutions
    via RL. An initial solution is pre-established. In each decision step, the agent
    utilize the one of the pre-defined rules to modify the current solution, and thus
    improve the overall solution quality.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：基于重写的方法通过 RL 生成 VRP 解的示意图。初始解已预先建立。在每个决策步骤中，代理利用预定义的规则之一来修改当前解，从而提高整体解的质量。
- en: A framework is proposed by Chen et al.[[96](#bib.bib96)] where a complete solution
    is constructed at the first stage and promoted gradually guided by the RL agent.
    A local rewriting rule is designed that keeps updating the current solutions.
    Lu et al. [[97](#bib.bib97)] further propose a Learn-to-Iterate (L2I) framework
    which not only improves the current solution but also creates perturbation for
    more exploitation choices. These methods borrow ideas from the operations research
    to keep rewriting the current solution. These rewriting-based methods are relatively
    slower compared with the sequence generation ones but have more potential in obtaining
    better performance due to the extended exploration ability.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人[[96](#bib.bib96)] 提出了一个框架，在第一阶段构建一个完整的解决方案，并在 RL 代理的引导下逐步推广。设计了一个局部重写规则，该规则不断更新当前的解决方案。Lu
    等人[[97](#bib.bib97)] 进一步提出了一个 Learn-to-Iterate (L2I) 框架，该框架不仅改进了当前的解决方案，还为更多的探索选择创造了扰动。这些方法借鉴了运筹学的思想，持续重写当前解。这些基于重写的方法相对于序列生成方法较慢，但由于扩展的探索能力，在获得更好性能方面具有更大的潜力。
- en: 5 Open Simulators and Datasets for DDS
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个用于 DDS 的开放模拟器和数据集
- en: Since the existing RL methods for DDS problems are model-free, a large amount
    of training data generated by interaction with the environment is required. However,
    direct interactions with the real environment indicate high costs and high risks.
    Therefore, simulating upon DDS scenarios is very necessary. A reliable simulator
    is of great practical significance. There are already some existing DDS simulators.
    We will introduce existing open simulators and datasets.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现有的 DDS 问题的 RL 方法是无模型的，因此需要通过与环境的交互生成大量的训练数据。然而，直接与真实环境交互会导致高成本和高风险。因此，在 DDS
    场景中进行模拟是非常必要的。一个可靠的模拟器具有重要的实际意义。目前已有一些现有的 DDS 模拟器。我们将介绍现有的开放模拟器和数据集。
- en: Simulator and dataset for Dispatching.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 用于调度的模拟器和数据集。
- en: 'Simulators for dispatching learn orders’ generation and state transition from
    the real data [[63](#bib.bib63)]. There are many public dispatching-related data
    sets. The most commonly used data set is provided by the New York City TLC (Taxi
    and Limousine Commission)  [[125](#bib.bib125)], which contains travel records
    for various services, Yellow taxis, Green taxis, and FHV (For-Hire Vehicle) from
    2009 to 2020\.  [[126](#bib.bib126)] provides a subset of NYC FHV data, which
    contains GPS coordinates for pickup locations. Travel time data between OD pairs
    can be obtained through Uber Movement  [[127](#bib.bib127)]. In addition, Didi
    Chuxing has released the travel records (regular and aggregated) and vehicle trajectory
    datasets of Chengdu, China through [[128](#bib.bib128)], from which they also
    developed a simulator to model the dispatching state. The simulator usually consists
    of two parts: order generation model, driver movement, and transition model. The
    order generation model learns the order generation and distribution, while the
    driver movement and transition model learn the state transition from the dataset.
    The simulator for dispatching often verifies the real effectiveness of the simulator
    by comparing the gross merchandise volume (GMV) generated by its simulation with
    the GMV of real data [[73](#bib.bib73)]. To make the simulator more realistic,
    sometimes the climate and traffic conditions are modeled in more details [[129](#bib.bib129)].
    Recently, DiDi  [[130](#bib.bib130)] has developed its dispatching simulation
    platform based on the research of existing dispatching simulators, which serves
    as an open-source ride-hailing environment for training dispatching agents.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 调度模拟器从实际数据中学习订单生成和状态转换[[63](#bib.bib63)]。有许多公开的调度相关数据集。最常用的数据集由纽约市 TLC（出租车和豪华轿车委员会）提供[[125](#bib.bib125)]，其中包含了
    2009 年到 2020 年间各种服务的旅行记录，包括黄出租车、绿出租车和 FHV（租赁车辆）。[[126](#bib.bib126)] 提供了 NYC FHV
    数据的子集，包含了取件地点的 GPS 坐标。OD 对之间的旅行时间数据可以通过 Uber Movement[[127](#bib.bib127)] 获得。此外，滴滴出行通过[[128](#bib.bib128)]
    发布了中国成都的旅行记录（常规和汇总）及车辆轨迹数据集，从中还开发了一个模拟器来建模调度状态。模拟器通常包括两个部分：订单生成模型、司机移动和转换模型。订单生成模型学习订单的生成和分布，而司机移动和转换模型则从数据集中学习状态转换。调度模拟器通常通过将其模拟生成的总商品交易额（GMV）与真实数据的
    GMV 进行比较来验证模拟器的实际效果[[73](#bib.bib73)]。为了使模拟器更真实，有时会更详细地建模气候和交通条件[[129](#bib.bib129)]。最近，滴滴[[130](#bib.bib130)]
    基于现有调度模拟器的研究开发了其调度模拟平台，作为一个开源的网约车环境，用于训练调度代理。
- en: Simulator and dataset for Routing.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 路由的模拟器和数据集。
- en: Simulator for routing learns the orders’ and vehicles’ generation and state
    transition from historical data [[112](#bib.bib112)]. When the simulation starts,
    it first initializes orders’ and vehicles’ states. The RL agent observes the state
    and then dispatches a determined vehicle to serve an order. After the RL agent
    implements the learned policy, the generation model and vehicle state transition
    model are utilized to update the selected vehicle’s information. This process
    repeats until the end of the simulation. Recently, Huawei [[131](#bib.bib131)]
    has developed its simulation platform based on the research of existing routing
    simulators, which serves as an open-source vehicle routing environment for training
    dispatching agents to tackle the scenario in logistics based on dynamic VRP with
    pickup and delivery. As for open datasets for routing,  [[132](#bib.bib132)] summarized
    several open CVRP instance datasets with different scales with maximal to $30000$.
    VRPTW as a special variant routing problem has its public dataset included in
    Solomon dataset [[133](#bib.bib133)] and Homberg dataset [[134](#bib.bib134)].
    Meanwhile, the Li&Lim benchmark [[135](#bib.bib135)] tackles VRPPD with time windows
    specifically.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 路由模拟器从历史数据中学习订单和车辆的生成及状态转换[[112](#bib.bib112)]。当模拟开始时，它首先初始化订单和车辆的状态。强化学习（RL）代理观察状态，然后调度一辆确定的车辆来服务订单。在
    RL 代理实施学习到的策略后，生成模型和车辆状态转换模型被用来更新选定车辆的信息。这个过程会重复直到模拟结束。最近，华为[[131](#bib.bib131)]基于现有路由模拟器的研究开发了其模拟平台，作为一个开源的车辆路由环境，用于训练调度代理以应对基于动态
    VRP 的物流场景，包括取件和送件。关于路由的开放数据集，[[132](#bib.bib132)]总结了几个规模不同的开放 CVRP 实例数据集，最大到 $30000$。作为一种特殊变种的路由问题，VRPTW
    的公共数据集包括在 Solomon 数据集[[133](#bib.bib133)]和 Homberg 数据集[[134](#bib.bib134)]中。同时，Li&Lim
    基准[[135](#bib.bib135)]专门处理带时间窗的 VRPPD。
- en: 6 Challenges
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 个挑战
- en: Owing to the advantages of DRL, many practical frameworks to tackle the two
    stages for DDS are developed and could generate high-quality solutions efficiently
    on large scales. However, challenges still exist in building more practical DDS
    applications. We briefly summarize the major challenges in developing DDS solutions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DRL的优势，许多实际框架被开发以解决DDS的两个阶段，并能够在大规模上高效生成高质量的解决方案。然而，在构建更实际的DDS应用程序时，挑战仍然存在。我们简要总结了开发DDS解决方案的主要挑战。
- en: 6.1 Coupled Spatial-Temporal Representations
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 关联空间-时间表示
- en: Capturing the dynamic changes of distributions within different service loops
    is essential to an effective DDS application. A good ST-representation can well
    reflect the in-between spatial relationships and the potential temporal consumption
    to accomplish the services. For instance, He et al. [[64](#bib.bib64)] develop
    a capsule-based network to capture the representations of both new demands from
    passengers and available drivers in the order matching task. A well-learned representation
    can enhance the performance of the overall framework.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉不同服务循环中分布的动态变化对有效的DDS应用至关重要。良好的ST表示可以很好地反映空间关系和完成服务所需的潜在时间消耗。例如，He等人 [[64](#bib.bib64)]
    开发了一种基于胶囊的网络，以捕捉来自乘客的新需求和订单匹配任务中可用司机的表示。良好学习的表示可以提升整体框架的性能。
- en: Even though ST-representation is not a new research topic and has been addressed
    as an important task in urban computation literature [[136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138)], the coupled ST forms a new challenge. In most DDS applications,
    a service target is a bond with its provider, thus the ST representation should
    reflect such a paired relationship. Some literature proposes special designs for
    the coupled challenge. For instance, Li et al. [[111](#bib.bib111)] propose a
    special attention-based structure to leverage different relationships among all
    customer nodes in VRP with pick and delivery. Six different attention mechanisms
    in total are computed as a thorough measurement upon all nodes. However, the above
    solution follows an ergodic way to consider all possible solutions in all extent
    of measurement based on the given network structure. Developing more eligible
    and flexible representation methods, learning mechanisms and overall algorithms
    are still challenges for DDS development.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ST表示不是一个新的研究主题，并且在城市计算文献中已被作为重要任务讨论过 [[136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138)]，但关联ST形成了新的挑战。在大多数DDS应用中，服务目标是与其提供者之间的绑定关系，因此ST表示应该反映这种配对关系。一些文献提出了针对关联挑战的特殊设计。例如，Li等人 [[111](#bib.bib111)]
    提出了一种基于注意力的特殊结构，以利用VRP中的所有客户节点之间的不同关系进行挑选和配送。总共有六种不同的注意力机制被计算作为对所有节点的全面测量。然而，上述解决方案按照遍历的方式考虑所有可能的解决方案，基于给定的网络结构进行全面测量。开发更多合适和灵活的表示方法、学习机制和整体算法仍然是DDS发展的挑战。
- en: 6.2 Fleet Heterogeneous
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 船队异质性
- en: In both real-world transportation and logistic systems, it is common that workers
    in a given worker fleet may be heterogeneous. For example, vehicles with different
    capacities can be deployed for parcel delivery, and electric taxis with different
    battery volumes can be arranged together for ride-hailing services. Under such
    circumstances, how to consider the heterogeneity of the entire fleet forms another
    challenge for real-world DDS. Most current solutions in both dispatching and routing
    stages make the hypothesis that all workers in the system are homogeneous to greatly
    reduce computation costs in large-scale settings. However, heterogeneity is still
    an inevitable challenge.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的交通和物流系统中，给定的工作队伍中的工人通常是异质的。例如，不同容量的车辆可以用于包裹配送，而不同电池容量的电动出租车可以安排在一起用于打车服务。在这种情况下，如何考虑整个车队的异质性形成了现实世界DDS的另一个挑战。目前大多数调度和路线规划阶段的解决方案假设系统中的所有工人是同质的，以大大降低大规模设置中的计算成本。然而，异质性仍然是一个不可避免的挑战。
- en: For a DRL-based approach, an intuitive idea is to model such a problem as a
    MARL model, where several types of agents cooperate to accomplish the entire service
    tasks given by the platform. However, the state and action space will grow rapidly
    with the agent scale. How to solve such a problem using MARL, or finding an alternative
    method using centralized modeling remains a valuable question to solve the fleet
    heterogeneous challenge.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于DRL的方法，一个直观的想法是将此类问题建模为MARL模型，其中几种类型的代理合作完成平台给出的整个服务任务。然而，随着代理数量的增加，状态和动作空间将迅速增长。如何使用MARL解决此类问题，或者找到一种使用集中建模的替代方法仍然是解决车队异质性挑战的宝贵问题。
- en: 6.3 Variant Constraints in MDP Modeling
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 MDP建模中的变体约束
- en: Considering multiple constraints in DRL designs is an important research problem
    that has been widely studied [[139](#bib.bib139), [140](#bib.bib140)]. Meanwhile,
    the practical constraints in DDS design are especially significant. For instance,
    a practical routing problem is much more complicated than the mathematical VRP
    due to numerous constraints, including time windows, charging requirements, structural
    limitations between pickups and deliveries. Effective DRL training requires corresponding
    considerations on these additional constraints.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在DRL设计中考虑多个约束是一个重要的研究问题，已经被广泛研究[[139](#bib.bib139), [140](#bib.bib140)]。与此同时，DDS设计中的实际约束尤为重要。例如，实际的路径规划问题比数学上的VRP复杂得多，因为它涉及到众多的约束条件，包括时间窗口、充电要求、取件和送件之间的结构限制。有效的DRL训练需要对这些额外的约束进行相应的考虑。
- en: A commonly used solution is to develop ad-hoc designs to these constraints.
    However, adopting such a solution for all constraints may result in complicated
    structure design when the constraint amount increases. It may be much more difficult
    to train the entire model. Another solution is to measure the limitations into
    soft constraints. For instance, to deal with VPR with time windows, Zhang et al. [[108](#bib.bib108)]
    measure possible violation on time windows as a penalty to the total reward. The
    agent can learn to minimize the exceeds upon the constraint. However, such a solution
    is not suitable in scenarios where limits are strict and can not be violated even
    sightly. How to well model these practical challenges remains a critical challenge.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用的解决方案是针对这些约束进行专门设计。然而，为所有约束采用这种解决方案可能会导致结构设计复杂化，当约束数量增加时，训练整个模型可能会变得更加困难。另一种解决方案是将这些限制转化为软约束。例如，为了处理带有时间窗口的VPR，Zhang等人[[108](#bib.bib108)]将时间窗口的可能违约作为对总奖励的惩罚。代理可以学习最小化对约束的超出。然而，这种解决方案不适用于严格的、甚至略微不能违反的限制场景。如何有效地建模这些实际挑战仍然是一个关键挑战。
- en: 6.4 Large-Scale Deployment
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 大规模部署
- en: Implementing algorithms on large scales is a necessary step from pure research
    to industrial usage. However, training models directly in large-scale scenarios
    requires enormous computation resources and time. To tackle such a challenge,
    a commonly used method is to abandon the natural formulation of multi-agent on
    different service workers. Either directly provide centralized control or model
    them as homogeneous agents with shared parameters can help to simplify the training
    process [[56](#bib.bib56), [62](#bib.bib62), [63](#bib.bib63)]. Another approach
    of reducing computation burden is to utilize the idea of divide and conquer by
    reducing a city-wide planning task into multiple regional ones [[59](#bib.bib59)].
    Such an idea is widely used in real-world on-demand delivery systems, where the
    entire delivery scope is divided into regions and couriers are assigned to accomplish
    ”last kilometer deliveries” [[141](#bib.bib141)].
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模上实施算法是从纯研究到工业应用的必要步骤。然而，直接在大规模场景中训练模型需要巨大的计算资源和时间。为了应对这一挑战，常用的方法是放弃多代理在不同服务工作者上的自然表达。直接提供集中控制或将它们建模为具有共享参数的同质代理可以帮助简化训练过程[[56](#bib.bib56),
    [62](#bib.bib62), [63](#bib.bib63)]。减少计算负担的另一种方法是利用“分而治之”的思想，将城市范围的规划任务划分为多个区域任务[[59](#bib.bib59)]。这种思想在现实世界的按需配送系统中被广泛使用，其中整个配送范围被划分为若干个区域，快递员被指派完成“最后一公里配送”[[141](#bib.bib141)]。
- en: However, current solutions are still far from enough to solve the large-scale
    problem, especially in the routing stage. With an NP-hard nature, the complexity
    of generating an optimal solution grows exponentially along with the problem scales.
    As a result, most existing literature on solving routing problems limit their
    experiment scales to no more than one hundred demand nodes within a graph-based
    data scheme [[19](#bib.bib19), [97](#bib.bib97)]. Developing new training frameworks
    via either more agile formulation or more advanced lightweight training algorithms
    can help to fit in large-scale environment and promote deployment ability.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前的解决方案仍然远远不足以解决大规模问题，特别是在路由阶段。由于其 NP 难度的特性，生成最佳解决方案的复杂性随着问题规模的增长而呈指数级增长。因此，大多数现有文献在解决路由问题时将实验规模限制在图形数据方案中的一百个需求节点以内[[19](#bib.bib19),
    [97](#bib.bib97)]。通过更敏捷的公式或更先进的轻量级训练算法来开发新的训练框架可以帮助适应大规模环境并促进部署能力。
- en: 6.5 Dynamics and Real-time Scheduling
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 动态性与实时调度
- en: Real-world DDS scenarios include high dynamics from the environment. New demands
    arrive continuously and the existing ones may also change. For instance, a passenger
    who calls for a ride-hailing service may change his destination or even cancel
    the current request directly. Such dynamics are critical in real-time scheduling
    in both stages.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 真实世界的 DDS 场景包括来自环境的高动态性。新的需求不断出现，现有需求也可能发生变化。例如，呼叫打车服务的乘客可能会更改目的地，甚至直接取消当前请求。这些动态在两个阶段的实时调度中至关重要。
- en: Much existing literature using DRL for dispatching measures the dynamic features
    explicitly using specially designed network structures. For instance, Tang et
    al. [[63](#bib.bib63)] represent the spatial-temporal features using hierarchical
    coarse coding, and He et al.[[64](#bib.bib64)] develop a special capsule-based
    network accordingly. As for the routing stage, DVRP is specially constructed to
    leverage the dynamics in real-time scheduling. Changing demands update the current
    service loops and thus bring more complexity. However, only a limited number of
    DRL solutions for such a dynamic routing scenario are proposed curretnly [[101](#bib.bib101),
    [105](#bib.bib105)]. Real-time scheduling based on the dynamics is still challenging
    for DDS development.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现有文献使用 DRL 进行调度时，通过特别设计的网络结构明确测量动态特征。例如，Tang 等人[[63](#bib.bib63)] 使用分层粗编码来表示时空特征，而
    He 等人[[64](#bib.bib64)] 则相应地开发了一种特殊的胶囊网络。至于路由阶段，DVRP 被特别构建以利用实时调度中的动态性。需求的变化更新当前的服务循环，从而带来更多的复杂性。然而，目前只有有限数量的
    DRL 解决方案针对这种动态路由场景提出[[101](#bib.bib101), [105](#bib.bib105)]。基于动态性的实时调度对于 DDS
    的发展仍然具有挑战性。
- en: 7 Open Problems
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 个开放问题
- en: With multiple challenges, there are still many open problems with future research
    opportunities in developing more effective DDS systems. In this section, we briefly
    discuss some research directions that we feel may be potentials in this area.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 面对多重挑战，开发更有效的 DDS 系统仍然存在许多未解的难题和未来的研究机会。在这一部分，我们简要讨论了一些我们认为在这一领域具有潜力的研究方向。
- en: 7.1 Advanced DRL Methods for DDS
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 高级 DRL 方法用于 DDS
- en: As DRL theory and methods develop rapidly in recent years, new advanced DRL
    algorithms are of great potential in developing more robust, effective, and efficient
    DDS applications. For example, leveraging dispatching problems, most literature
    we investigate in this survey utilizes DQN as the training algorithm. However,
    such an off-policy framework suffers from limitations to interact with the environment
    even multi-sourced data is used. Consequently, reproducibility within other environments
    will face great difficulty and thus cause fairness issues on performance comparison.
    While recent development on off-line RL [[142](#bib.bib142)] provides new opportunities
    to tackle the challenges respectively. A complete off-line learning paradigm based
    on large-scale agent experience data may help to improve training robustness and
    solve the reproducibility problem. Besides off-line RL, other advanced RL techniques
    such as causal RL [[143](#bib.bib143)] leveraging multi-objective optimization
    may also bring new research opportunities into DDS development.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 随着DRL理论和方法近年来的快速发展，新型先进的DRL算法在开发更强大、有效和高效的DDS应用方面具有巨大潜力。例如，利用调度问题，我们在这项调查中研究的大多数文献使用了DQN作为训练算法。然而，这种离线策略框架在与环境互动方面存在局限性，即使使用了多源数据。因此，其他环境中的可重复性将面临巨大困难，从而导致性能比较上的公平性问题。虽然最近离线RL[[142](#bib.bib142)]的发展提供了分别解决这些挑战的新机会。基于大规模代理经验数据的完整离线学习范式可能有助于提高训练的稳健性并解决可重复性问题。除了离线RL，其他先进的RL技术，如利用多目标优化的因果RL[[143](#bib.bib143)]，也可能为DDS的发展带来新的研究机会。
- en: 7.2 Joint Optimization of Two Stages
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 两个阶段的联合优化
- en: Currently, even though both the dispatching phase and the routing phase are
    well studied, works that consider both stages into a DRL learning paradigm are
    still missing. This forms a major problem, especially in the reaction speed to
    new changes in applicable systems. For example, current order learning-based dispatching
    systems are still computationally intensive since conventional VRP solvers are
    adopted instead of the DRL based [[144](#bib.bib144), [145](#bib.bib145)], which
    serves as the role to help predict future income and vehicle states. A joint consideration
    in two stages can help to improve the overall performance, including both planning
    quality and inference speed. A major challenge of such modeling is the even more
    complicated state space and the heterogeneity of different action spaces. Research
    potential lies in the cross-stage representation for both states and actions.
    The planning quality will be highly related to the hierarchical framework design.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，尽管调度阶段和路线规划阶段都得到了充分研究，但将这两个阶段纳入DRL学习范式的研究仍然缺乏。这是一个主要问题，特别是在对适用系统中新变化的反应速度上。例如，当前基于订单学习的调度系统仍然计算密集，因为采用的是传统的VRP求解器，而不是基于DRL的[[144](#bib.bib144),
    [145](#bib.bib145)]，后者有助于预测未来收入和车辆状态。两个阶段的联合考虑可以帮助提高整体性能，包括规划质量和推理速度。此类建模的主要挑战在于更加复杂的状态空间和不同动作空间的异质性。研究潜力在于跨阶段的状态和动作表示。规划质量将与层次化框架设计高度相关。
- en: 7.3 Fairness from Workers’ Perspective
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 从工人的角度看公平性
- en: In the current solutions for DDS problems, it almost defaults to set the scheduling
    objective to maximize the profit of the entire platform. Even though new objectives
    are proposed, such as Order Response Rate (ORR) in Sec 3.1.1, they still conform
    to the overall centralized profit. Rather than it, few works stand from the perspective
    of the service workers. As both DDS and AI ethics develop, researchers of social
    science gradually focus on how service workers think about their roles in DDS.
    While keeping optimizing the centralized profit as the prime goal, how to consider
    the individual differences among the group of service workers and their initiatives
    is a research question with potentials. On the one hand, fairness between the
    workers is an essential problem. Maximizing the overall profit alone might result
    in extreme differences in individual incomes. A well-designed DDS system should
    guarantee the fairness of different service workers. On the other hand, individual
    workers might have his or her preference for the dispatching or routing strategies.
    Personal historical patterns with no intelligent algorithm intervention may help
    in finding these preferences, and may further be considered as a factor in using
    intelligent ones to guide them.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的DDS问题解决方案中，几乎默认将调度目标设置为最大化整个平台的利润。尽管提出了新的目标，如第3.1.1节中的订单响应率（ORR），但这些目标仍然符合整体集中的利润。与此不同的是，少数研究从服务工作者的角度出发。随着DDS和AI伦理的发展，社会科学研究者逐渐关注服务工作者如何看待他们在DDS中的角色。在继续将优化集中利润作为首要目标的同时，如何考虑服务工作者之间的个体差异及其主动性是一个具有潜力的研究问题。一方面，工人之间的公平性是一个基本问题。仅仅最大化整体利润可能导致个人收入的极端差异。一个设计良好的DDS系统应该保障不同服务工作者的公平性。另一方面，个别工人可能对调度或路线策略有自己的偏好。个人历史模式在没有智能算法干预的情况下可能有助于发现这些偏好，并可能进一步被考虑作为使用智能算法指导的因素。
- en: 7.4 Partial Compliance Consideration
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 部分合规性考虑
- en: In the dispatching stage, current algorithms usually consider full compliance
    from the service workers as a simplified assumption. However in real-world applications,
    workers may reject recommendations from the centralized platform and operate based
    on individual preference. Such disobeying may result in inaccurate overall performance
    prediction and thus requires additional investigation.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在调度阶段，当前的算法通常将服务工作者完全服从视为简化假设。然而，在实际应用中，工人可能会拒绝来自集中平台的建议，而根据个人偏好进行操作。这种不服从可能导致整体性能预测的不准确，因此需要额外的研究。
- en: Besides considering partial compliance as a factor in the system, the reason
    that results in such compliance and corresponding solutions form another important
    research task. For example, couriers on rainy days usually have reluctance on
    accepting distant delivery tasks. A solution is to provide extra allowance to
    the couriers. Joint decision on generating order matching decisions and determining
    the specific allowance quota for different couriers based on specific tasks is
    a sequential decision task, which is suitable for DRL modeling.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将部分合规性作为系统中的一个因素外，导致这种合规性的原因和相应的解决方案形成了另一个重要的研究任务。例如，雨天的快递员通常对接受远程配送任务有抵触情绪。一个解决方案是向快递员提供额外津贴。基于特定任务生成订单匹配决策和确定不同快递员的具体津贴配额的联合决策是一个适合DRL建模的序列决策任务。
- en: 7.5 Pricing Problems
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 定价问题
- en: Other than directly scheduling how different roles in the human-engaged DDS
    loops should operate to improve efficiency as a whole, another important research
    question lies in how to determine the price of service provided by the workers.
    Dynamic pricing problems exist in the DDS applications with short serve durations
    and human engagement, such as ridesharing and instant/food delivery. The pricing
    module influence both supply distribution from the service workers and new service
    demands from the customers’ side.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接调度不同角色在人工参与的DDS循环中如何操作以提高整体效率外，另一个重要的研究问题在于如何确定工人提供的服务价格。动态定价问题存在于短服务时长和人类参与的DDS应用中，如拼车和即时/食品配送。定价模块影响服务工作者的供应分配和客户方面的新服务需求。
- en: RL-based approaches for dynamic pricing were studied and used in one-sided retail
    markets [[146](#bib.bib146), [147](#bib.bib147)]. In these scenarios, pricing
    changes only the demand pattern from the customers’ perspective. However, in much
    more complicated DDS systems, a good pricing strategy should consider both customers
    and workers and other dynamic spatial-temporal information.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 动态定价的基于强化学习的方法在单边零售市场中进行了研究和应用[[146](#bib.bib146)，[147](#bib.bib147)]。在这些场景中，定价只会从顾客的角度改变需求模式。然而，在更为复杂的DDS系统中，一个好的定价策略应该考虑到顾客、工作者和其他动态时空信息。
- en: It is not trivial to develop a high-quality pricing module within human-engaged
    DDS applications due to two major challenges. First, optimizing the pricing is
    closely coupled with other scheduling tasks in DDS as discussed in this survey.
    The routing estimation in advance is a decisive factor to pricing, and further
    influences the quality of the dispatching stage. How to optimize several modules
    jointly is a critical research problem. Second, designing the metrics to evaluate
    pricing quality is another challenge. A good pricing strategy should both be reasonable
    and explainable to customers and help to improve the overall income of the platform
    and all service workers as well as maintaining fairness. Multi-side consideration
    of the problem formulation and the explainable RL design of the algorithm itself
    could thus be a core research focus in the future.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在人力参与的DDS应用中，开发高质量的定价模块并不容易，主要有两个挑战。首先，由于与DDS中的其他调度任务密切相关，优化定价任务非常复杂，本文中已经讨论过这个问题。提前估计路径对价格的影响很大，并进一步影响调度阶段的质量。如何联合优化几个模块是一个非常关键的研究问题。其次，设计用于评估定价质量的指标是另一个挑战。一个好的定价策略既应该合理又要让顾客能够理解，并有助于提高平台和所有服务工作者的总收入，同时保持公平性。因此，问题表述的多方面考虑和算法本身的可解释性强化学习设计可能成为未来的核心研究方向。
- en: 7.6 Simulation Environments
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6模拟环境
- en: For scenarios that need high interaction with the environment, such as ridesharing
    or on-demand delivery with frequent updating to demand and worker distributions,
    it is too expensive to evaluate the algorithm by directly interacting with real-world
    scenarios. Thus it is essential to deploy and evaluate the algorithms in simulation
    environments. However, current simulators are rather simple in environment settings
    and are not enough for complete modeling for both dispatching or realistic routing
    in DDS. As for the dispatching stage, many real-world issues that might happen
    should be considered in the simulation test to evaluate the robustness of proposed
    algorithms, including stochastic cancellation of requests, intelligent pricing
    strategy upon different matching results, and the individual preference of different
    drivers, etc. As for the routing stage, real-world execution in the routing can
    seldom be as exact as the algorithm predicts. Simulation upon environment changes,
    real-time traffic congestion, and demand updates are essential to dynamic decision-making.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要与环境进行高度互动的场景，例如经常更新需求和工作者分布的顺风车或即时配送，通过直接与真实场景进行交互来评估算法的成本太高了。因此，在模拟环境中部署和评估算法非常重要。然而，当前的模拟器在环境设置方面相当简单，对于DDS中的调度或真实路径规划的完整建模来说还不够。至于调度阶段，许多可能发生的真实世界问题应该在模拟测试中考虑，以评估所提出算法的鲁棒性，包括请求的随机取消、不同匹配结果的智能定价策略以及不同驾驶员的个人偏好等。至于路径规划阶段，实际的路径执行很少能像算法预测的那样准确。模拟环境的改变、实时交通拥堵和需求更新对于动态决策至关重要。
- en: As far as we know, few simulators considering the factors above are released.
    Great research potential lies in developing simulators that could allow agents
    to entirely interact with the most realistic environments and thus has enough
    robustness for deployment. A well-designed simulator can be critical as the role
    of the offline training environment for advanced DRL algorithm for DDS.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，目前很少有考虑上述因素的模拟器发布。在开发能够使代理与最逼真的环境完全互动的模拟器方面，具有很大的研究潜力，并因此具备足够的鲁棒性进行部署。设计良好的模拟器对于DDS的先离线训练环境而言非常关键。
- en: 7.7 Large-Scale Online Scheduling System
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7大规模在线调度系统
- en: 'Tackling the challenges as discussed in Sec [6](#S6 "6 Challenges ‣ Deep Reinforcement
    Learning for Demand Driven Services in Logistics and Transportation Systems: A
    Survey"), the ultimate benchmark of utilizing DRL in DDS is to build a large-scale
    online scheduling system to handle real-world DDS tasks. Complete system development
    requires thorough consideration on solving coupled and dynamic features, modeling
    heterogeneity within fleets, remaining high efficiency in large scales, and adapting
    to practical constraints. Both general high-quality and robust algorithms and
    ad-hoc considerations in specific scenarios are needed in constructing a centralized
    platform for DDS design. Developing a large-scale online scheduling DDS system
    via DRL will have a strong impact on both relative research and industrial areas.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '针对第[6](#S6 "6 Challenges ‣ Deep Reinforcement Learning for Demand Driven Services
    in Logistics and Transportation Systems: A Survey")节中讨论的挑战，利用深度强化学习（DRL）在需求驱动服务（DDS）中的**终极**基准是建立一个大规模的在线调度系统，以处理现实世界中的DDS任务。完整的系统开发需要对解决耦合和动态特性、建模车队中的异质性、在大规模中保持高效性以及适应实际约束进行深入的考虑。构建DDS设计的集中平台既需要通用的高质量和稳健的算法，也需要在特定场景中的特定考虑。通过DRL开发的大规模在线调度DDS系统将对相关研究和工业领域产生强大的影响。'
- en: 8 Conclusion
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: Demand driven services (DDS), such as ride-hailing and express systems, are
    of great importance in urban life nowadays. The planning and scheduling process
    within these applications require effectiveness and efficiency. In this survey,
    we focus on the DDS problems and derive the entire DDS into two stages, the dispatching
    stage and the routing stage respectively. The dispatching stage is responsible
    to coordinate the unassigned service demands and the available service workers,
    while the routing stage generates strategies within each service loops. We investigate
    the recent works using deep reinforcement learning (DRL) techniques to solve DDS
    problems in the two stages. We also discuss the further challenges and open problems
    in using DRL to help build high-quality DDS systems.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 需求驱动服务（DDS），如打车和快递系统，在现代城市生活中具有重要意义。这些应用中的规划和调度过程需要有效性和高效性。在本调查中，我们将DDS问题分为两个阶段，即调度阶段和路径阶段。调度阶段负责协调未分配的服务需求和可用服务工作者，而路径阶段则在每个服务循环中生成策略。我们研究了利用深度强化学习（DRL）技术解决这两个阶段的DDS问题的最新工作。我们还讨论了使用DRL构建高质量DDS系统的进一步挑战和开放问题。
- en: Acknowledgments
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported in part by The National Key Research and Development
    Program of China under grant 2018YFB1800804, the National Nature Science Foundation
    of China under U1936217, 61971267, 61972223, 61941117, 61861136003, Beijing Natural
    Science Foundation under L182038, Beijing National Research Center for Information
    Science and Technology under 20031887521, and research fund of Tsinghua University
    - Tencent Joint Laboratory for Internet Innovation Technology.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到了中国国家重点研发计划（资助号2018YFB1800804）、中国国家自然科学基金（资助号U1936217、61971267、61972223、61941117、61861136003）、北京市自然科学基金（资助号L182038）、北京市信息科学与技术国家研究中心（资助号20031887521）以及清华大学-腾讯互联网创新技术联合实验室的研究基金的支持。
- en: References
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Meituan, “Meituan homepage,” https://about.meituan.com/en.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 美团，“美团首页，” https://about.meituan.com/en。'
- en: '[2] M. M. Flood, “The traveling-salesman problem,” *Operations research*, vol. 4,
    no. 1, pp. 61–75, 1956.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. M. Flood，“旅行推销员问题，” *运筹学*，第4卷，第1期，页码61–75，1956年。'
- en: '[3] G. B. Dantzig and J. H. Ramser, “The truck dispatching problem,” *Management
    science*, vol. 6, no. 1, pp. 80–91, 1959.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] G. B. Dantzig 和 J. H. Ramser，“卡车调度问题，” *管理科学*，第6卷，第1期，页码80–91，1959年。'
- en: '[4] M. Schneider, A. Stenger, and D. Goeke, “The electric vehicle-routing problem
    with time windows and recharging stations,” *Transportation science*, vol. 48,
    no. 4, pp. 500–520, 2014.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Schneider, A. Stenger, 和 D. Goeke，“具有时间窗和充电站的电动汽车路径问题，” *交通科学*，第48卷，第4期，页码500–520，2014年。'
- en: '[5] J. Desrosiers, F. Soumis, and M. Desrochers, “Routing with time windows
    by column generation,” *Networks*, vol. 14, no. 4, pp. 545–565, 1984.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Desrosiers, F. Soumis, 和 M. Desrochers，“通过列生成进行时间窗路由，” *网络*，第14卷，第4期，页码545–565，1984年。'
- en: '[6] H. N. Psaraftis, “Dynamic vehicle routing problems,” *Vehicle routing:
    Methods and studies*, vol. 16, pp. 223–248, 1988.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] H. N. Psaraftis，“动态车辆路径问题，” *车辆路径：方法与研究*，第16卷，页码223–248，1988年。'
- en: '[7] H. Min, “The multiple vehicle routing problem with simultaneous delivery
    and pick-up points,” *Transportation Research Part A: General*, vol. 23, no. 5,
    pp. 377–386, 1989.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] H. Min， “具有同时交付和取件点的多车路径问题，” *运输研究 A 部分：一般*，第 23 卷，第 5 期，第 377–386 页，1989
    年。'
- en: '[8] J. Munkres, “Algorithms for the assignment and transportation problems,”
    *Journal of the society for industrial and applied mathematics*, vol. 5, no. 1,
    pp. 32–38, 1957.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Munkres， “任务分配和运输问题的算法，” *工业与应用数学学会期刊*，第 5 卷，第 1 期，第 32–38 页，1957 年。'
- en: '[9] P. Toth and D. Vigo, “Branch-and-bound algorithms for the capacitated vrp,”
    in *The vehicle routing problem*.   SIAM, 2002, pp. 29–51.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. Toth 和 D. Vigo， “用于容量限制 VRP 的分支界限算法，” 收录于 *车辆路径问题*。SIAM，2002 年，第 29–51
    页。'
- en: '[10] Z. Liao, “Real-time taxi dispatching using global positioning systems,”
    *Communications of the ACM*, vol. 46, no. 5, pp. 81–83, 2003.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Z. Liao， “基于全球定位系统的实时出租车调度，” *ACM 通信*，第 46 卷，第 5 期，第 81–83 页，2003 年。'
- en: '[11] E. Özkan and A. R. Ward, “Dynamic matching for real-time ride sharing,”
    *Stochastic Systems*, vol. 10, no. 1, pp. 29–70, 2020.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] E. Özkan 和 A. R. Ward， “实时拼车的动态匹配，” *随机系统*，第 10 卷，第 1 期，第 29–70 页，2020
    年。'
- en: '[12] L. M. Gambardella, Éric Taillard, and G. Agazzi, “Macs-vrptw: A multiple
    colony system for vehicle routing problems with time windows,” in *New Ideas in
    Optimization*.   McGraw-Hill, 1999, pp. 63–76.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L. M. Gambardella, Éric Taillard, 和 G. Agazzi， “MACS-VRPTW：一种用于带时间窗的车辆路径问题的多殖民系统，”
    收录于 *优化新思想*。麦格劳-希尔，1999 年，第 63–76 页。'
- en: '[13] G. Schrimpf, J. Schneider, H. Stamm-Wilbrandt, and G. Dueck, “Record breaking
    optimization results using the ruin and recreate principle,” *Journal of Computational
    Physics*, vol. 159, no. 2, pp. 139 – 171, 2000\. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0021999199964136'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] G. Schrimpf, J. Schneider, H. Stamm-Wilbrandt, 和 G. Dueck， “使用破坏与重建原则的记录破纪录优化结果，”
    *计算物理学期刊*，第 159 卷，第 2 期，第 139–171 页，2000 年。[在线]. 可用: http://www.sciencedirect.com/science/article/pii/S0021999199964136'
- en: '[14] M. Hu and Y. Zhou, “Dynamic type matching,” *Manufacturing & Service Operations
    Management*, 2021.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Hu 和 Y. Zhou， “动态类型匹配，” *制造与服务运营管理*，2021 年。'
- en: '[15] D. Favaretto, E. Moretti, and P. Pellegrini, “Ant colony system for a
    vrp with multiple time windows and multiple visits,” *Journal of Interdisciplinary
    Mathematics*, vol. 10, no. 2, pp. 263–284, 2007.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. Favaretto, E. Moretti, 和 P. Pellegrini， “用于具有多个时间窗和多个访问点的 VRP 的蚁群系统，”
    *跨学科数学期刊*，第 10 卷，第 2 期，第 263–284 页，2007 年。'
- en: '[16] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.   MIT
    press, 2018.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] R. S. Sutton 和 A. G. Barto， *强化学习：导论*。MIT 出版社，2018 年。'
- en: '[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *nature*, vol. 518, no. 7540, pp.
    529–533, 2015.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*， “通过深度强化学习实现人类水平的控制，”
    *自然*，第 518 卷，第 7540 期，第 529–533 页，2015 年。'
- en: '[18] M. Nazari, A. Oroojlooy, L. V. Snyder, and M. Takáč, “Reinforcement learning
    for solving the vehicle routing problem,” 2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Nazari, A. Oroojlooy, L. V. Snyder, 和 M. Takáč， “解决车辆路径问题的强化学习，” 2018
    年。'
- en: '[19] W. Kool, H. van Hoof, and M. Welling, “Attention, learn to solve routing
    problems!” 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] W. Kool, H. van Hoof, 和 M. Welling， “注意力，学习解决路径问题！” 2018 年。'
- en: '[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, 和 O. Klimov， “近端策略优化算法，”
    *arXiv 预印本 arXiv:1707.06347*，2017 年。'
- en: '[21] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine learning*, vol. 8, no. 3-4, pp.
    229–256, 1992.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] R. J. Williams， “用于连接主义强化学习的简单统计梯度跟随算法，” *机器学习*，第 8 卷，第 3-4 期，第 229–256
    页，1992 年。'
- en: '[22] A. Haydari and Y. Yilmaz, “Deep reinforcement learning for intelligent
    transportation systems: A survey,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2020.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Haydari 和 Y. Yilmaz， “用于智能交通系统的深度强化学习：综述，” *IEEE 智能交通系统汇刊*，2020 年。'
- en: '[23] Z. Qin, H. Zhu, and J. Ye, “Reinforcement learning for ridesharing: A
    survey,” *arXiv preprint arXiv:2105.01099*, 2021.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Z. Qin, H. Zhu, 和 J. Ye， “共享出行的强化学习：综述，” *arXiv 预印本 arXiv:2105.01099*，2021。'
- en: '[24] N. Mazyavkina, S. Sviridov, S. Ivanov, and E. Burnaev, “Reinforcement
    learning for combinatorial optimization: A survey,” *Computers & Operations Research*,
    p. 105400, 2021.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] N. Mazyavkina, S. Sviridov, S. Ivanov, 和 E. Burnaev， “用于组合优化的强化学习：综述，”
    *计算机与运筹研究*，第 105400 页，2021 年。'
- en: '[25] R. S. Sutton and A. G. Barto, “Reinforcement learning: An introduction,”
    2011.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] R. S. Sutton 和 A. G. Barto，“强化学习：导论，” 2011年。'
- en: '[26] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    and M. Riedmiller, “Playing atari with deep reinforcement learning,” *arXiv preprint
    arXiv:1312.5602*, 2013.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra
    和 M. Riedmiller，“使用深度强化学习玩 Atari 游戏，” *arXiv 预印本 arXiv:1312.5602*，2013年。'
- en: '[27] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *nature*, vol. 529,
    no. 7587, pp. 484–489, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *等*，“通过深度神经网络和树搜索掌握围棋，”
    *自然*，第529卷，第7587期，第484–489页，2016年。'
- en: '[28] C. J. C. H. Watkins, “Learning from delayed rewards,” 1989.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. J. C. H. Watkins，“从延迟奖励中学习，” 1989年。'
- en: '[29] L.-J. Lin, *Reinforcement learning for robots using neural networks*.   Carnegie
    Mellon University, 1992.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] L.-J. Lin, *使用神经网络进行机器人的强化学习*。 卡内基梅隆大学，1992年。'
- en: '[30] R. S. Sutton, “Learning to predict by the methods of temporal differences,”
    *Machine learning*, vol. 3, no. 1, pp. 9–44, 1988.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] R. S. Sutton，“通过时间差分方法学习预测，” *机器学习*，第3卷，第1期，第9–44页，1988年。'
- en: '[31] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 30, no. 1, 2016.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] H. Van Hasselt, A. Guez 和 D. Silver，“使用双重 Q 学习的深度强化学习，” 载于 *AAAI 人工智能会议论文集*，第30卷，第1期，2016年。'
- en: '[32] H. Hasselt, “Double q-learning,” *Advances in neural information processing
    systems*, vol. 23, pp. 2613–2621, 2010.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] H. Hasselt，“双重 Q 学习，” *神经信息处理系统进展*，第23卷，第2613–2621页，2010年。'
- en: '[33] M. G. Bellemare, G. Ostrovski, A. Guez, P. Thomas, and R. Munos, “Increasing
    the action gap: New operators for reinforcement learning,” in *Proceedings of
    the AAAI Conference on Artificial Intelligence*, vol. 30, no. 1, 2016.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] M. G. Bellemare, G. Ostrovski, A. Guez, P. Thomas 和 R. Munos，“增加动作间隙：强化学习的新操作符，”
    载于 *AAAI 人工智能会议论文集*，第30卷，第1期，2016年。'
- en: '[34] L. C. Baird III, “Reinforcement learning through gradient descent,” CARNEGIE-MELLON
    UNIV PITTSBURGH PA DEPT OF COMPUTER SCIENCE, Tech. Rep., 1999.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] L. C. Baird III, “通过梯度下降进行强化学习，” 卡内基梅隆大学计算机科学系，技术报告，1999年。'
- en: '[35] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,
    “Dueling network architectures for deep reinforcement learning,” in *International
    conference on machine learning*.   PMLR, 2016, pp. 1995–2003.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot 和 N. Freitas，“用于深度强化学习的对抗网络架构，”
    载于 *国际机器学习会议*。 PMLR，2016年，第1995–2003页。'
- en: '[36] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour *et al.*, “Policy
    gradient methods for reinforcement learning with function approximation.” in *NIPs*,
    vol. 99.   Citeseer, 1999, pp. 1057–1063.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour *等*，“用于函数逼近的策略梯度方法。”
    载于 *NIPs*，第99卷。 Citeseer，1999年，第1057–1063页。'
- en: '[37] V. R. Konda and J. N. Tsitsiklis, “Actor-critic algorithms,” in *Advances
    in neural information processing systems*, 2000, pp. 1008–1014.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] V. R. Konda 和 J. N. Tsitsiklis，“演员-评论员算法，” 载于 *神经信息处理系统进展*，2000年，第1008–1014页。'
- en: '[38] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International conference on machine learning*.   PMLR, 2016, pp. 1928–1937.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver 和 K. Kavukcuoglu，“深度强化学习的异步方法，” 载于 *国际机器学习会议*。 PMLR，2016年，第1928–1937页。'
- en: '[39] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas,
    “Sample efficient actor-critic with experience replay,” *arXiv preprint arXiv:1611.01224*,
    2016.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu 和 N. de
    Freitas，“样本高效的演员-评论员与经验重放，” *arXiv 预印本 arXiv:1611.01224*，2016年。'
- en: '[40] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver 和 D. Wierstra，“使用深度强化学习进行连续控制，” *arXiv 预印本 arXiv:1509.02971*，2015年。'
- en: '[41] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” in *International conference on machine
    learning*.   PMLR, 2014, pp. 387–395.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra 和 M. Riedmiller，“确定性策略梯度算法，”
    载于 *国际机器学习会议*。 PMLR，2014年，第387–395页。'
- en: '[42] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation
    error in actor-critic methods,” in *International Conference on Machine Learning*.   PMLR,
    2018, pp. 1587–1596.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. Fujimoto, H. Hoof, 和 D. Meger，“在演员-评论家方法中解决函数逼近误差，” 在 *国际机器学习会议*。PMLR，2018年，第1587–1596页。'
- en: '[43] C. R. Shelton, “Importance sampling for reinforcement learning with multiple
    objectives,” 2001.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C. R. Shelton，“多目标强化学习中的重要性采样，” 2001年。'
- en: '[44] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International conference on machine learning*.   PMLR,
    2015, pp. 1889–1897.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] J. Schulman, S. Levine, P. Abbeel, M. Jordan, 和 P. Moritz，“信任区域策略优化，”
    在 *国际机器学习会议*。PMLR，2015年，第1889–1897页。'
- en: '[45] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru,
    and R. Vicente, “Multiagent cooperation and competition with deep reinforcement
    learning,” *PloS one*, vol. 12, no. 4, p. e0172395, 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J.
    Aru, 和 R. Vicente，“使用深度强化学习进行多智能体合作与竞争，” *PloS one*，第12卷，第4期，第e0172395页，2017年。'
- en: '[46] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, “Learning
    to communicate to solve riddles with deep distributed recurrent q-networks,” *arXiv
    preprint arXiv:1602.02672*, 2016.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. N. Foerster, Y. M. Assael, N. de Freitas, 和 S. Whiteson，“学习通过深度分布式递归Q网络来解决谜题，”
    *arXiv预印本arXiv:1602.02672*，2016年。'
- en: '[47] L. Buşoniu, R. Babuška, and B. De Schutter, “Multi-agent reinforcement
    learning: An overview,” *Innovations in multi-agent systems and applications-1*,
    pp. 183–221, 2010.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] L. Buşoniu, R. Babuška, 和 B. De Schutter，“多智能体强化学习：概述，” *多智能体系统与应用-1*，第183–221页，2010年。'
- en: '[48] DiDi, “Didi homepage,” https://www.didiglobal.com.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] DiDi，“滴滴首页，” https://www.didiglobal.com。'
- en: '[49] Uber, “Uber homepage,” https://www.uber.com/.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Uber，“优步首页，” https://www.uber.com/。'
- en: '[50] PrimeNow, “Primenow homepage,” https://primenow.amazon.com/onboard?forceOnboard=1&sourceUrl=%2Fhome.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] PrimeNow，“PrimeNow首页，” https://primenow.amazon.com/onboard?forceOnboard=1&sourceUrl=%2Fhome。'
- en: '[51] UberEats, “Ubereats homepage,” https://www.ubereats.com.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] UberEats，“优食首页，” https://www.ubereats.com。'
- en: '[52] Eleme, “Eleme homepage,” https://www.ele.me.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Eleme，“饿了么首页，” https://www.ele.me。'
- en: '[53] FedEx, “Fedex homepage,” https://www.fedex.com/en-us/home.html, 2021.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] FedEx，“联邦快递首页，” https://www.fedex.com/en-us/home.html，2021年。'
- en: '[54] Cainiao, “Cainiao homepage,” https://global.cainiao.com, 2021.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Cainiao，“菜鸟首页，” https://global.cainiao.com，2021年。'
- en: '[55] L. Zhang, T. Hu, Y. Min, G. Wu, J. Zhang, P. Feng, P. Gong, and J. Ye,
    “A taxi order dispatch model based on combinatorial optimization,” in *Proceedings
    of the 23rd ACM SIGKDD international conference on knowledge discovery and data
    mining*, 2017, pp. 2151–2159.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L. Zhang, T. Hu, Y. Min, G. Wu, J. Zhang, P. Feng, P. Gong, 和 J. Ye，“基于组合优化的出租车订单调度模型，”
    在 *第23届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2017年，第2151–2159页。'
- en: '[56] Z. Xu, Z. Li, Q. Guan, D. Zhang, Q. Li, J. Nan, C. Liu, W. Bian, and J. Ye,
    “Large-scale order dispatch in on-demand ride-hailing platforms: A learning and
    planning approach,” in *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, 2018, pp. 905–913.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Z. Xu, Z. Li, Q. Guan, D. Zhang, Q. Li, J. Nan, C. Liu, W. Bian, 和 J.
    Ye，“大规模订单调度在按需打车平台中的应用：学习与规划方法，” 在 *第24届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2018年，第905–913页。'
- en: '[57] C. Yan, H. Zhu, N. Korolko, and D. Woodard, “Dynamic pricing and matching
    in ride-hailing platforms,” *Naval Research Logistics (NRL)*, vol. 67, no. 8,
    pp. 705–724, 2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] C. Yan, H. Zhu, N. Korolko, 和 D. Woodard，“打车平台中的动态定价与匹配，” *海军研究物流（NRL）*，第67卷，第8期，第705–724页，2020年。'
- en: '[58] I. Jindal, Z. T. Qin, X. Chen, M. Nokleby, and J. Ye, “Optimizing taxi
    carpool policies via reinforcement learning and spatio-temporal mining,” in *2018
    IEEE International Conference on Big Data (Big Data)*.   IEEE, 2018, pp. 1417–1426.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] I. Jindal, Z. T. Qin, X. Chen, M. Nokleby, 和 J. Ye，“通过强化学习和时空挖掘优化出租车拼车政策，”
    在 *2018 IEEE国际大数据会议（Big Data）*。IEEE，2018年，第1417–1426页。'
- en: '[59] Y. Li, Y. Zheng, and Q. Yang, “Efficient and effective express via contextual
    cooperative reinforcement learning,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019, pp. 510–519.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Li, Y. Zheng, 和 Q. Yang，“通过上下文协作强化学习实现高效和有效的快递服务，” 在 *第25届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2019年，第510–519页。'
- en: '[60] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean field multi-agent
    reinforcement learning,” in *International Conference on Machine Learning*.   PMLR,
    2018, pp. 5571–5580.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, 和 J. Wang，“均值场多智能体强化学习，” 在
    *国际机器学习会议*。PMLR，2018年，第5571–5580页。'
- en: '[61] M. Zhou, J. Jin, W. Zhang, Z. Qin, Y. Jiao, C. Wang, G. Wu, Y. Yu, and
    J. Ye, “Multi-agent reinforcement learning for order-dispatching via order-vehicle
    distribution matching,” in *Proceedings of the 28th ACM International Conference
    on Information and Knowledge Management*, 2019, pp. 2645–2653.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] M. Zhou, J. Jin, W. Zhang, Z. Qin, Y. Jiao, C. Wang, G. Wu, Y. Yu, 和 J.
    Ye，“通过订单-车辆分配匹配的多智能体强化学习用于订单调度，”收录于 *第28届ACM国际信息与知识管理会议论文集*，2019，第2645–2653页。'
- en: '[62] Z. Wang, Z. Qin, X. Tang, J. Ye, and H. Zhu, “Deep reinforcement learning
    with knowledge transfer for online rides order dispatching,” in *2018 IEEE International
    Conference on Data Mining (ICDM)*.   IEEE, 2018, pp. 617–626.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Z. Wang, Z. Qin, X. Tang, J. Ye, 和 H. Zhu，“基于知识迁移的深度强化学习用于在线接单调度，”收录于
    *2018 IEEE国际数据挖掘会议 (ICDM)*。IEEE, 2018，第617–626页。'
- en: '[63] X. Tang, Z. Qin, F. Zhang, Z. Wang, Z. Xu, Y. Ma, H. Zhu, and J. Ye, “A
    deep value-network based approach for multi-driver order dispatching,” in *Proceedings
    of the 25th ACM SIGKDD international conference on knowledge discovery & data
    mining*, 2019, pp. 1780–1790.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] X. Tang, Z. Qin, F. Zhang, Z. Wang, Z. Xu, Y. Ma, H. Zhu, 和 J. Ye，“一种基于深度价值网络的多司机订单调度方法，”收录于
    *第25届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2019，第1780–1790页。'
- en: '[64] S. He and K. G. Shin, “Spatio-temporal capsule-based reinforcement learning
    for mobility-on-demand network coordination,” in *The World Wide Web Conference*,
    2019, pp. 2806–2813.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S. He 和 K. G. Shin，“基于时空胶囊的强化学习用于按需网络协调，”收录于 *世界万维网会议*，2019，第2806–2813页。'
- en: '[65] A. O. Al-Abbasi, A. Ghosh, and V. Aggarwal, “Deeppool: Distributed model-free
    algorithm for ride-sharing using deep reinforcement learning,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 20, no. 12, pp. 4714–4727, 2019.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. O. Al-Abbasi, A. Ghosh, 和 V. Aggarwal，“Deeppool：一种分布式无模型算法用于共享出行的深度强化学习，”
    *IEEE智能交通系统事务*，第20卷，第12期，第4714–4727页，2019年。'
- en: '[66] G. Qin, Q. Luo, Y. Yin, J. Sun, and J. Ye, “Optimizing matching time intervals
    for ride-hailing services using reinforcement learning,” *Transportation Research
    Part C: Emerging Technologies*, vol. 129, p. 103239, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] G. Qin, Q. Luo, Y. Yin, J. Sun, 和 J. Ye，“使用强化学习优化网约车服务的匹配时间间隔，” *运输研究C部分：新兴技术*，第129卷，第103239页，2021年。'
- en: '[67] Y. Wang, Y. Tong, C. Long, P. Xu, K. Xu, and W. Lv, “Adaptive dynamic
    bipartite graph matching: A reinforcement learning approach,” in *2019 IEEE 35th
    International Conference on Data Engineering (ICDE)*.   IEEE, 2019, pp. 1478–1489.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. Wang, Y. Tong, C. Long, P. Xu, K. Xu, 和 W. Lv，“自适应动态二分图匹配：一种强化学习方法，”收录于
    *2019 IEEE第35届国际数据工程会议 (ICDE)*。IEEE, 2019，第1478–1489页。'
- en: '[68] K. Jintao, H. Yang, J. Ye *et al.*, “Learning to delay in ride-sourcing
    systems: a multi-agent deep reinforcement learning framework,” *IEEE Transactions
    on Knowledge and Data Engineering*, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] K. Jintao, H. Yang, J. Ye *等*，“在网约车系统中学习延迟：一种多智能体深度强化学习框架，” *IEEE知识与数据工程事务*，2020年。'
- en: '[69] L. Yang, X. Yu, J. Cao, X. Liu, and P. Zhou, “Exploring deep reinforcement
    learning for task dispatching in autonomous on-demand services,” *ACM Transactions
    on Knowledge Discovery from Data (TKDD)*, vol. 15, no. 3, pp. 1–23, 2021.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] L. Yang, X. Yu, J. Cao, X. Liu, 和 P. Zhou，“探索深度强化学习在自主按需服务中的任务调度，” *ACM数据知识发现事务
    (TKDD)*，第15卷，第3期，第1–23页，2021年。'
- en: '[70] Y. Chen, Y. Qian, Y. Yao, Z. Wu, R. Li, Y. Zhou, H. Hu, and Y. Xu, “Can
    sophisticated dispatching strategy acquired by reinforcement learning?” in *Proceedings
    of the 18th International Conference on Autonomous Agents and MultiAgent Systems*,
    2019, pp. 1395–1403.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Y. Chen, Y. Qian, Y. Yao, Z. Wu, R. Li, Y. Zhou, H. Hu, 和 Y. Xu，“复杂调度策略是否可以通过强化学习获得？”收录于
    *第18届国际自主代理与多智能体系统会议论文集*，2019，第1395–1403页。'
- en: '[71] Y. Li, Y. Zheng, and Q. Yang, “Cooperative multi-agent reinforcement learning
    in express system,” in *Proceedings of the 29th ACM International Conference on
    Information & Knowledge Management*, 2020, pp. 805–814.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Y. Li, Y. Zheng, 和 Q. Yang，“快递系统中的合作多智能体强化学习，”收录于 *第29届ACM国际信息与知识管理会议论文集*，2020，第805–814页。'
- en: '[72] H. Hu, X. Jia, Q. He, S. Fu, and K. Liu, “Deep reinforcement learning
    based agvs real-time scheduling with mixed rule for flexible shop floor in industry
    4.0,” *Computers & Industrial Engineering*, vol. 149, p. 106749, 2020.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] H. Hu, X. Jia, Q. He, S. Fu, 和 K. Liu，“基于深度强化学习的AGV实时调度与混合规则用于工业4.0的灵活车间，”
    *计算机与工业工程*，第149卷，第106749页，2020年。'
- en: '[73] K. Lin, R. Zhao, Z. Xu, and J. Zhou, “Efficient large-scale fleet management
    via multi-agent deep reinforcement learning,” in *Proceedings of the 24th ACM
    SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2018, pp.
    1774–1783.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] K. Lin, R. Zhao, Z. Xu, 和 J. Zhou，“通过多智能体深度强化学习进行大规模车队管理”，发表于 *第24届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*，2018年，第1774–1783页。'
- en: '[74] W. Zhang, Q. Wang, J. Li, and C. Xu, “Dynamic fleet management with rewriting
    deep reinforcement learning,” *IEEE Access*, vol. 8, pp. 143 333–143 341, 2020.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] W. Zhang, Q. Wang, J. Li, 和 C. Xu，“使用重写深度强化学习的动态车队管理”，*IEEE Access*，第8卷，第143 333–143 341页，2020年。'
- en: '[75] J. Wen, J. Zhao, and P. Jaillet, “Rebalancing shared mobility-on-demand
    systems: A reinforcement learning approach,” in *2017 IEEE 20th International
    Conference on Intelligent Transportation Systems (ITSC)*.   Ieee, 2017, pp. 220–225.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] J. Wen, J. Zhao, 和 P. Jaillet，“共享按需移动系统的再平衡：一种强化学习方法”，发表于 *2017年IEEE第20届国际智能交通系统会议（ITSC）*。IEEE，2017年，第220–225页。'
- en: '[76] T. Oda and C. Joe-Wong, “Movi: A model-free approach to dynamic fleet
    management,” in *IEEE INFOCOM 2018-IEEE Conference on Computer Communications*.   IEEE,
    2018, pp. 2708–2716.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] T. Oda 和 C. Joe-Wong，“Movi: 一种无模型的动态车队管理方法”，发表于 *IEEE INFOCOM 2018-IEEE计算机通信会议*。IEEE，2018年，第2708–2716页。'
- en: '[77] Z. Liu, J. Li, and K. Wu, “Context-aware taxi dispatching at city-scale
    using deep reinforcement learning,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2020.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Z. Liu, J. Li, 和 K. Wu，“基于深度强化学习的城市规模上下文感知出租车调度”，*IEEE智能交通系统汇刊*，2020年。'
- en: '[78] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *arXiv preprint arXiv:1609.02907*, 2016.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. N. Kipf 和 M. Welling，“基于图卷积网络的半监督分类”，*arXiv预印本arXiv:1609.02907*，2016年。'
- en: '[79] Z. Shou and X. Di, “Reward design for driver repositioning using multi-agent
    reinforcement learning,” *Transportation research part C: emerging technologies*,
    vol. 119, p. 102738, 2020.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Z. Shou 和 X. Di，“使用多智能体强化学习的司机重定位奖励设计”，*运输研究C部分：新兴技术*，第119卷，第102738页，2020年。'
- en: '[80] J. Jin, M. Zhou, W. Zhang, M. Li, Z. Guo, Z. Qin, Y. Jiao, X. Tang, C. Wang,
    J. Wang *et al.*, “Coride: joint order dispatching and fleet management for multi-scale
    ride-hailing platforms,” in *Proceedings of the 28th ACM International Conference
    on Information and Knowledge Management*, 2019, pp. 1983–1992.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] J. Jin, M. Zhou, W. Zhang, M. Li, Z. Guo, Z. Qin, Y. Jiao, X. Tang, C.
    Wang, J. Wang *等人*，“Coride: 多尺度叫车平台的联合订单调度和车队管理”，发表于 *第28届ACM国际信息与知识管理会议论文集*，2019年，第1983–1992页。'
- en: '[81] J. Holler, R. Vuorio, Z. Qin, X. Tang, Y. Jiao, T. Jin, S. Singh, C. Wang,
    and J. Ye, “Deep reinforcement learning for multi-driver vehicle dispatching and
    repositioning problem,” in *2019 IEEE International Conference on Data Mining
    (ICDM)*.   IEEE, 2019, pp. 1090–1095.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. Holler, R. Vuorio, Z. Qin, X. Tang, Y. Jiao, T. Jin, S. Singh, C. Wang,
    和 J. Ye，“用于多驾驶员车辆调度和重定位问题的深度强化学习”，发表于 *2019年IEEE数据挖掘国际会议（ICDM）*。IEEE，2019年，第1090–1095页。'
- en: '[82] G. Guo and Y. Xu, “A deep reinforcement learning approach to ride-sharing
    vehicles dispatching in autonomous mobility-on-demand systems,” *IEEE Intelligent
    Transportation Systems Magazine*, 2020.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] G. Guo 和 Y. Xu，“一种用于自主按需移动系统中拼车车辆调度的深度强化学习方法”，*IEEE智能交通系统杂志*，2020年。'
- en: '[83] E. Liang, K. Wen, W. H. Lam, A. Sumalee, and R. Zhong, “An integrated
    reinforcement learning and centralized programming approach for online taxi dispatching,”
    *IEEE Transactions on Neural Networks and Learning Systems*, 2021.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] E. Liang, K. Wen, W. H. Lam, A. Sumalee, 和 R. Zhong，“用于在线出租车调度的强化学习和集中编程集成方法”，*IEEE神经网络与学习系统汇刊*，2021年。'
- en: '[84] I. Sungur, Y. Ren, F. Ordóñez, M. Dessouky, and H. Zhong, “A model and
    algorithm for the courier delivery problem with uncertainty,” *Transportation
    science*, vol. 44, no. 2, pp. 193–205, 2010.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] I. Sungur, Y. Ren, F. Ordóñez, M. Dessouky, 和 H. Zhong，“带有不确定性的快递配送问题模型及算法”，*运输科学*，第44卷，第2期，第193–205页，2010年。'
- en: '[85] M. Lowalekar, P. Varakantham, and P. Jaillet, “Online spatio-temporal
    matching in stochastic and dynamic domains,” *Artificial Intelligence*, vol. 261,
    pp. 71–112, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] M. Lowalekar, P. Varakantham, 和 P. Jaillet，“在随机和动态领域中的在线时空匹配”，*人工智能*，第261卷，第71–112页，2018年。'
- en: '[86] Z. Qin, X. Tang, Y. Jiao, F. Zhang, Z. Xu, H. Zhu, and J. Ye, “Ride-hailing
    order dispatching at didi via reinforcement learning,” *INFORMS Journal on Applied
    Analytics*, vol. 50, no. 5, pp. 272–286, 2020.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Z. Qin, X. Tang, Y. Jiao, F. Zhang, Z. Xu, H. Zhu, 和 J. Ye，“通过强化学习在滴滴出行平台上的订单调度”，*INFORMS应用分析期刊*，第50卷，第5期，第272–286页，2020年。'
- en: '[87] S. Zhang, L. Qin, Y. Zheng, and H. Cheng, “Effective and efficient: Large-scale
    dynamic city express,” *IEEE Transactions on Knowledge and Data Engineering*,
    vol. 28, no. 12, pp. 3203–3217, 2016.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. Zhang, L. Qin, Y. Zheng, 和 H. Cheng, “有效且高效：大规模动态城市快递，” *IEEE知识与数据工程汇刊*，第28卷，第12期，页码
    3203–3217，2016年。'
- en: '[88] M. Nourinejad and M. Ramezani, “Developing a large-scale taxi dispatching
    system for urban networks,” in *2016 IEEE 19th International Conference on Intelligent
    Transportation Systems (ITSC)*.   IEEE, 2016, pp. 441–446.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. Nourinejad 和 M. Ramezani, “为城市网络开发大规模出租车调度系统，” 见 *2016 IEEE第19届智能交通系统国际会议*。IEEE,
    2016年，页码 441–446。'
- en: '[89] F. Miao, S. Han, A. M. Hendawi, M. E. Khalefa, J. A. Stankovic, and G. J.
    Pappas, “Data-driven distributionally robust vehicle balancing using dynamic region
    partitions,” in *Proceedings of the 8th International Conference on Cyber-Physical
    Systems*, 2017, pp. 261–271.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] F. Miao, S. Han, A. M. Hendawi, M. E. Khalefa, J. A. Stankovic, 和 G. J.
    Pappas, “基于数据的分布鲁棒车辆平衡，使用动态区域分区，” 见 *第8届国际网络物理系统会议论文集*，2017年，页码 261–271。'
- en: '[90] M. Qu, H. Zhu, J. Liu, G. Liu, and H. Xiong, “A cost-effective recommender
    system for taxi drivers,” in *Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining*, 2014, pp. 45–54.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Qu, H. Zhu, J. Liu, G. Liu, 和 H. Xiong, “一个成本效益高的出租车司机推荐系统，” 见 *第20届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*，2014年，页码 45–54。'
- en: '[91] N. J. Yuan, Y. Zheng, L. Zhang, and X. Xie, “T-finder: A recommender system
    for finding passengers and vacant taxis,” *IEEE Transactions on knowledge and
    data engineering*, vol. 25, no. 10, pp. 2390–2403, 2012.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] N. J. Yuan, Y. Zheng, L. Zhang, 和 X. Xie, “T-finder: 一个寻找乘客和空车的推荐系统，”
    *IEEE知识与数据工程汇刊*，第25卷，第10期，页码 2390–2403，2012年。'
- en: '[92] J. Xu, R. Rahmatizadeh, L. Bölöni, and D. Turgut, “Taxi dispatch planning
    via demand and destination modeling,” in *2018 IEEE 43rd Conference on Local Computer
    Networks (LCN)*.   IEEE, 2018, pp. 377–384.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Xu, R. Rahmatizadeh, L. Bölöni, 和 D. Turgut, “基于需求和目的地建模的出租车调度规划，”
    见 *2018 IEEE第43届本地计算网络会议*。IEEE, 2018年，页码 377–384。'
- en: '[93] X. Xie, F. Zhang, and D. Zhang, “Privatehunt: Multi-source data-driven
    dispatching in for-hire vehicle systems,” *Proceedings of the ACM on Interactive,
    Mobile, Wearable and Ubiquitous Technologies*, vol. 2, no. 1, pp. 1–26, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] X. Xie, F. Zhang, 和 D. Zhang, “Privatehunt: 多源数据驱动的租车系统调度，” *ACM交互式、移动、可穿戴及无处不在技术论文集*，第2卷，第1期，页码
    1–26，2018年。'
- en: '[94] T. Verma, P. Varakantham, S. Kraus, and H. C. Lau, “Augmenting decisions
    of taxi drivers through reinforcement learning for improving revenues,” in *Proceedings
    of the International Conference on Automated Planning and Scheduling*, vol. 27,
    no. 1, 2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] T. Verma, P. Varakantham, S. Kraus, 和 H. C. Lau, “通过强化学习增强出租车司机的决策以提高收入，”
    见 *自动规划与调度国际会议论文集*，第27卷，第1期，2017年。'
- en: '[95] Y. Gao, D. Jiang, and Y. Xu, “Optimize taxi driving strategies based on
    reinforcement learning,” *International Journal of Geographical Information Science*,
    vol. 32, no. 8, pp. 1677–1696, 2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Gao, D. Jiang, 和 Y. Xu, “基于强化学习优化出租车驾驶策略，” *国际地理信息科学杂志*，第32卷，第8期，页码
    1677–1696，2018年。'
- en: '[96] X. Chen and Y. Tian, “Learning to perform local rewriting for combinatorial
    optimization,” in *Advances in Neural Information Processing Systems*, 2019, pp.
    6278–6289.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] X. Chen 和 Y. Tian, “学习执行局部重写以进行组合优化，” 见 *神经信息处理系统进展*，2019年，页码 6278–6289。'
- en: '[97] H. Lu, X. Zhang, and S. Yang, “A learning-based iterative method for solving
    vehicle routing problems,” in *International Conference on Learning Representations*,
    2020.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. Lu, X. Zhang, 和 S. Yang, “一种基于学习的迭代方法来解决车辆路径问题，” 见 *学习表示国际会议*，2020年。'
- en: '[98] L. Duan, Y. Zhan, H. Hu, Y. Gong, J. Wei, X. Zhang, and Y. Xu, “Efficiently
    solving the practical vehicle routing problem: A novel joint learning approach,”
    in *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*, 2020, pp. 3054–3063.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] L. Duan, Y. Zhan, H. Hu, Y. Gong, J. Wei, X. Zhang, 和 Y. Xu, “高效解决实际车辆路径问题：一种新颖的联合学习方法，”
    见 *第26届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，2020年，页码 3054–3063。'
- en: '[99] A. Delarue, R. Anderson, and C. Tjandraatmadja, “Reinforcement learning
    with combinatorial actions: An application to vehicle routing,” *arXiv preprint
    arXiv:2010.12001*, 2020.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Delarue, R. Anderson, 和 C. Tjandraatmadja, “具有组合动作的强化学习：在车辆调度中的应用，”
    *arXiv预印本arXiv:2010.12001*，2020年。'
- en: '[100] L. Xin, W. Song, Z. Cao, and J. Zhang, “Multi-decoder attention model
    with embedding glimpse for solving vehicle routing problems,” *arXiv preprint
    arXiv:2012.10638*, 2020.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] L. Xin, W. Song, Z. Cao, 和 J. Zhang, “用于解决车辆路径规划问题的多解码器注意力模型与嵌入视角，” *arXiv
    预印本 arXiv:2012.10638*，2020年。'
- en: '[101] W. Joe and H. C. Lau, “Deep reinforcement learning approach to solve
    dynamic vehicle routing problem with stochastic customers,” in *Proceedings of
    the International Conference on Automated Planning and Scheduling*, vol. 30, 2020,
    pp. 394–402.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] W. Joe 和 H. C. Lau, “解决具有随机客户的动态车辆路径规划问题的深度强化学习方法，” 收录于 *国际自动化规划与调度会议论文集*，第
    30 卷，2020年，第 394–402 页。'
- en: '[102] A. L. Ottoni, E. G. Nepomuceno, M. S. de Oliveira, and D. C. de Oliveira,
    “Reinforcement learning for the traveling salesman problem with refueling,” *Complex
    & Intelligent Systems*, pp. 1–15, 2021.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. L. Ottoni, E. G. Nepomuceno, M. S. de Oliveira, 和 D. C. de Oliveira,
    “用于旅行商问题的强化学习与加油，” *复杂与智能系统*，第 1–15 页，2021年。'
- en: '[103] W. Qin, Z. Zhuang, Z. Huang, and H. Huang, “A novel reinforcement learning-based
    hyper-heuristic for heterogeneous vehicle routing problem,” *Computers & Industrial
    Engineering*, vol. 156, p. 107252, 2021.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] W. Qin, Z. Zhuang, Z. Huang, 和 H. Huang, “一种新型的基于强化学习的超启发式方法用于异质车辆路径规划问题，”
    *计算机与工业工程*，第 156 卷，第 107252 页，2021年。'
- en: '[104] A. Bogyrbayeva, S. Jang, A. Shah, Y. J. Jang, and C. Kwon, “A reinforcement
    learning approach for rebalancing electric vehicle sharing systems,” *IEEE Transactions
    on Intelligent Transportation Systems*, 2021.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] A. Bogyrbayeva, S. Jang, A. Shah, Y. J. Jang, 和 C. Kwon, “用于电动车共享系统重新平衡的强化学习方法，”
    *IEEE 智能交通系统汇刊*，2021年。'
- en: '[105] J. Shi, Y. Gao, W. Wang, N. Yu, and P. A. Ioannou, “Operating electric
    vehicle fleet for ride-hailing services with reinforcement learning,” *IEEE Transactions
    on Intelligent Transportation Systems*, vol. 21, no. 11, pp. 4822–4834, 2019.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] J. Shi, Y. Gao, W. Wang, N. Yu, 和 P. A. Ioannou, “使用强化学习操作电动车队进行叫车服务，”
    *IEEE 智能交通系统汇刊*，第 21 卷，第 11 期，第 4822–4834 页，2019年。'
- en: '[106] J. James, W. Yu, and J. Gu, “Online vehicle routing with neural combinatorial
    optimization and deep reinforcement learning,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 20, no. 10, pp. 3806–3817, 2019.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. James, W. Yu, 和 J. Gu, “结合神经组合优化与深度强化学习的在线车辆路径规划，” *IEEE 智能交通系统汇刊*，第
    20 卷，第 10 期，第 3806–3817 页，2019年。'
- en: '[107] B. Lin, B. Ghaddar, and J. Nathwani, “Deep reinforcement learning for
    electric vehicle routing problem with time windows,” *arXiv preprint arXiv:2010.02068*,
    2020.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] B. Lin, B. Ghaddar, 和 J. Nathwani, “用于具有时间窗口的电动车路径问题的深度强化学习，” *arXiv
    预印本 arXiv:2010.02068*，2020年。'
- en: '[108] K. Zhang, F. He, Z. Zhang, X. Lin, and M. Li, “Multi-vehicle routing
    problems with soft time windows: A multi-agent reinforcement learning approach,”
    *Transportation Research Part C: Emerging Technologies*, vol. 121, p. 102861,
    2020.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] K. Zhang, F. He, Z. Zhang, X. Lin, 和 M. Li, “具有软时间窗口的多车辆路径规划问题：一种多智能体强化学习方法，”
    *运输研究 C 部分：新兴技术*，第 121 卷，第 102861 页，2020年。'
- en: '[109] J. K. Falkner and L. Schmidt-Thieme, “Learning to solve vehicle routing
    problems with time windows through joint attention,” *arXiv preprint arXiv:2006.09100*,
    2020.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] J. K. Falkner 和 L. Schmidt-Thieme, “通过联合注意力学习解决具有时间窗口的车辆路径规划问题，” *arXiv
    预印本 arXiv:2006.09100*，2020年。'
- en: '[110] J. Zhao, M. Mao, X. Zhao, and J. Zou, “A hybrid of deep reinforcement
    learning and local search for the vehicle routing problems,” *IEEE Transactions
    on Intelligent Transportation Systems*, 2020.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Zhao, M. Mao, X. Zhao, 和 J. Zou, “深度强化学习与局部搜索的混合方法用于车辆路径规划问题，” *IEEE
    智能交通系统汇刊*，2020年。'
- en: '[111] J. Li, L. Xin, Z. Cao, A. Lim, W. Song, and J. Zhang, “Heterogeneous
    attentions for solving pickup and delivery problem via deep reinforcement learning,”
    *IEEE Transactions on Intelligent Transportation Systems*, 2021.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] J. Li, L. Xin, Z. Cao, A. Lim, W. Song, 和 J. Zhang, “用于解决取送货问题的异质注意力深度强化学习方法，”
    *IEEE 智能交通系统汇刊*，2021年。'
- en: '[112] X. Li, W. Luo, M. Yuan, J. Wang, J. Lu, J. Wang, J. Lü, and J. Zeng,
    “Learning to optimize industry-scale dynamic pickup and delivery problems,” in
    *2021 IEEE 37th International Conference on Data Engineering (ICDE)*.   IEEE,
    2021, pp. 2511–2522.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] X. Li, W. Luo, M. Yuan, J. Wang, J. Lu, J. Wang, J. Lü, 和 J. Zeng, “学习优化行业规模的动态取送货问题，”
    收录于 *2021 IEEE 第37届国际数据工程会议 (ICDE)*。IEEE，2021年，第 2511–2522 页。'
- en: '[113] H. Lee and J. Jeong, “Mobile robot path optimization technique based
    on reinforcement learning algorithm in warehouse environment,” *Applied Sciences*,
    vol. 11, no. 3, p. 1209, 2021.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] H. Lee 和 J. Jeong, “基于强化学习算法的移动机器人路径优化技术在仓储环境中的应用，” *应用科学*，第 11 卷，第 3
    期，第 1209 页，2021年。'
- en: '[114] M. M. Solomon, “Algorithms for the vehicle routing and scheduling problems
    with time window constraints,” *Operations research*, vol. 35, no. 2, pp. 254–265,
    1987.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] M. M. Solomon，“带有时间窗约束的车辆路径规划和调度问题的算法”，《运筹学》杂志，卷35，第2期，页码254-265，1987年。'
- en: '[115] O. B. G. Madsen, M. L. Fisher, and K. O. Jornsten, *Vehicle routing with
    time windows: Two optimization algorithms*, 1997.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] O. B. G. Madsen, M. L. Fisher, and K. O. Jornsten，《带时间窗的车辆路径规划：两种优化算法》，1997年。'
- en: '[116] J. H. Holland, *Adaptation in Natural and Artificial System*, 1992.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. H. Holland，“自然和人工系统中的适应性”，1992年。'
- en: '[117] M. Desrochers, J. Desrosiers, and M. Solomon, “A new optimization algorithm
    for the vehicle routing problem with time windows,” *Operations Research*, vol. 40,
    pp. 342–354, 04 1992.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] M. Desrochers, J. Desrosiers, and M. Solomon，“一种针对带时间窗车辆路径问题的新型优化算法”，《运筹学》，卷40，页码342-354，1992年4月。'
- en: '[118] F. Glover, “Tabu search - part i,” *INFORMS Journal on Computing*, vol. 2,
    pp. 4–32, 01 1990.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] F. Glover，“禁忌搜索 - 第一部分”，《计算机管理研究》杂志，卷2，页码4-32，1990年1月。'
- en: '[119] C. Groër, B. Golden, and E. Wasil, “A library of local search heuristics
    for the vehicle routing problem,” *Mathematical Programming Computation*, vol. 2,
    no. 2, pp. 79–101, 2010.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] C. Groër, B. Golden, and E. Wasil，“车辆路径问题的局部搜索启发式方法库”，《数学规划计算》，卷2，第2期，页码79-101，2010年。'
- en: '[120] D. E. Goldberg, “Genetic algorithms in search,” *Optimization, and MachineLearning*,
    1989.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] D. E. Goldberg，“遗传算法在搜索、优化和机器学习中的应用”，1989年。'
- en: '[121] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” in *Advances
    in neural information processing systems*, 2015, pp. 2692–2700.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] O. Vinyals, M. Fortunato, and N. Jaitly，“指针网络”，《神经信息处理系统进展》，2015年，页码2692-2700。'
- en: '[122] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio, “Neural combinatorial
    optimization with reinforcement learning,” 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio，“利用强化学习的神经组合优化”，2016年。'
- en: '[123] R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and J. P. Vielma,
    “Strong mixed-integer programming formulations for trained neural networks,” *Mathematical
    Programming*, pp. 1–37, 2020.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and J. P. Vielma，“针对训练神经网络的强混合整数规划公式”，《数学规划》，页码1-37，2020年。'
- en: '[124] K. Helsgaun, “An extension of the lin-kernighan-helsgaun tsp solver for
    constrained traveling salesman and vehicle routing problems,” *Roskilde: Roskilde
    University*, 2017.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] K. Helsgaun，“一个用于约束旅行推销员问题和车辆路径问题的lin-kernighan-helsgaun旅行推销员问题求解器的扩展”，《Roskilde：Roskilde大学》，2017年。'
- en: '[125] TLC, “Nyc taxi & limousine commission trip record data. 2020,” https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page,
    2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] TLC，“纽约出租车和私人豪华车委员会旅行记录数据。2020年。” https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page
    ，2020年。'
- en: '[126] Kaggle, “Uber pickups in new york city - trip data for over 20 million
    uber (and other for-hire vehicle) trips in nyc. 2017.” https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city",
    2017.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Kaggle，“纽约城市的优步接送 - 超过2000万次优步（以及其他出租车）在纽约的旅行数据。2017年。” https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city"
    ，2017年。'
- en: '[127] Uber, “Uber movement. 2021.” https://movement.uber.com/?lang=en-US, 2021.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Uber，“Uber移动平台。2021年。” https://movement.uber.com/?lang=en-US ，2021年。'
- en: '[128] GAIA, “Didi gaia open data set: Kdd cup 2020\. 2020.” https://outreach.didichuxing.com/appEn-vue/KDD_CUP_2020?id=1005,
    2020.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] GAIA，“Didi gaia开放数据集：Kdd cup 2020。2020年。” https://outreach.didichuxing.com/appEn-vue/KDD_CUP_2020?id=1005
    ，2020年。'
- en: '[129] Y. Li, Y. Zheng, and Q. Yang, “Dynamic bike reposition: A spatio-temporal
    reinforcement learning approach,” in *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2018, pp. 1724–1733.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. Li, Y. Zheng, and Q. Yang，“动态自行车重新定位：一种时空强化学习方法”，《第24届ACM SIGKDD国际数据挖掘与知识发现会议论文集》，2018年，页码1724-1733。'
- en: '[130] GAIA, “Kdd cup 2020: Learning to dispatch and reposition on a mobility-on-demand
    platform. 2020.” https://www.biendata.xyz/competition/kdd_didi/, 2020.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] GAIA，“Kdd cup 2020: 学习调度和重新定位的移动出行平台。2020年。” https://www.biendata.xyz/competition/kdd_didi/
    ，2020年。'
- en: '[131] huawei, “Icaps 2021:the dynamic pickup and delivery problem,” https://competition.huaweicloud.com/information/1000041411/introduction,
    2021.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] 华为，“ICAPS 2021：动态接送问题”，https://competition.huaweicloud.com/information/1000041411/introduction
    ，2021年。'
- en: '[132] CVRPLib, “Cvrplib homepage,” http://vrp.atd-lab.inf.puc-rio.br/index.php/en/.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] CVRPLib，“Cvrplib主页”，http://vrp.atd-lab.inf.puc-rio.br/index.php/en/ 。'
- en: '[133] Solomon-benchmark, “Solomon-benchmark. 2008.” https://www.sintef.no/Projectweb/TOP/VRPTW/Solomon-benchmark/,
    2008.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Solomon-基准，“Solomon-基准。2008年。” https://www.sintef.no/Projectweb/TOP/VRPTW/Solomon-benchmark/
    ，2008年。'
- en: '[134] homberger benchmark, “homberger-benchmark. 2008.” https://www.sintef.no/projectweb/top/vrptw/homberger-benchmark/,
    2008.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] homberger benchmark, “homberger-benchmark. 2008.” https://www.sintef.no/projectweb/top/vrptw/homberger-benchmark/，2008年。'
- en: '[135] Li&Lim-benchmark, “Li&lim-benchmark. 2008.” https://www.sintef.no/projectweb/top/pdptw/,
    2008.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Li&Lim-benchmark, “Li&lim-benchmark. 2008.” https://www.sintef.no/projectweb/top/pdptw/，2008年。'
- en: '[136] Z. Zong, J. Feng, K. Liu, H. Shi, and Y. Li, “Deepdpm: Dynamic population
    mapping via deep neural network,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 33, no. 01, 2019, pp. 1294–1301.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Z. Zong, J. Feng, K. Liu, H. Shi, 和 Y. Li, “Deepdpm：通过深度神经网络进行动态人群映射，”
    见 *AAAI 人工智能会议论文集*，第33卷，第01期，2019年，第1294–1301页。'
- en: '[137] J. Feng, Y. Li, C. Zhang, F. Sun, F. Meng, A. Guo, and D. Jin, “Deepmove:
    Predicting human mobility with attentional recurrent networks,” in *Proceedings
    of the 2018 world wide web conference*, 2018, pp. 1459–1468.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. Feng, Y. Li, C. Zhang, F. Sun, F. Meng, A. Guo, 和 D. Jin, “Deepmove：使用注意力递归网络预测人类移动，”
    见 *2018年全球网络会议论文集*，2018年，第1459–1468页。'
- en: '[138] Z. Lin, J. Feng, Z. Lu, Y. Li, and D. Jin, “Deepstn+: Context-aware spatial-temporal
    neural network for crowd flow prediction in metropolis,” in *Proceedings of the
    AAAI conference on artificial intelligence*, vol. 33, no. 01, 2019, pp. 1020–1027.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Z. Lin, J. Feng, Z. Lu, Y. Li, 和 D. Jin, “Deepstn+：用于大都市人流预测的上下文感知空间-时间神经网络，”
    见 *AAAI 人工智能会议论文集*，第33卷，第01期，2019年，第1020–1027页。'
- en: '[139] P. Geibel, “Reinforcement learning for mdps with constraints,” in *European
    Conference on Machine Learning*.   Springer, 2006, pp. 646–653.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] P. Geibel, “带约束的MDP的强化学习，” 见 *欧洲机器学习会议*。Springer，2006年，第646–653页。'
- en: '[140] S. Miryoosefi, K. Brantley, H. Daumé III, M. Dudík, and R. Schapire,
    “Reinforcement learning with convex constraints,” *arXiv preprint arXiv:1906.09323*,
    2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] S. Miryoosefi, K. Brantley, H. Daumé III, M. Dudík, 和 R. Schapire, “带有凸约束的强化学习，”
    *arXiv 预印本 arXiv:1906.09323*，2019年。'
- en: '[141] E. Taniguchi and R. G. Thompson, *City logistics: Mapping the future*.   CRC
    Press, 2014.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] E. Taniguchi 和 R. G. Thompson, *城市物流：映射未来*。CRC Press，2014年。'
- en: '[142] R. Agarwal, D. Schuurmans, and M. Norouzi, “An optimistic perspective
    on offline reinforcement learning,” in *International Conference on Machine Learning*.   PMLR,
    2020, pp. 104–114.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] R. Agarwal, D. Schuurmans, 和 M. Norouzi, “离线强化学习的乐观视角，” 见 *国际机器学习大会*。PMLR，2020年，第104–114页。'
- en: '[143] A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine, “Meta-reinforcement
    learning of structured exploration strategies,” *arXiv preprint arXiv:1802.07245*,
    2018.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, 和 S. Levine, “结构化探索策略的元强化学习，”
    *arXiv 预印本 arXiv:1802.07245*，2018年。'
- en: '[144] X. Yu and S. Shen, “An integrated decomposition and approximate dynamic
    programming approach for on-demand ride pooling,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 21, no. 9, pp. 3811–3820, 2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] X. Yu 和 S. Shen, “一种集成分解和近似动态规划的方法用于按需拼车，” *IEEE 智能交通系统汇刊*，第21卷，第9期，第3811–3820页，2019年。'
- en: '[145] S. Shah, M. Lowalekar, and P. Varakantham, “Neural approximate dynamic
    programming for on-demand ride-pooling,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 34, no. 01, 2020, pp. 507–515.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] S. Shah, M. Lowalekar, 和 P. Varakantham, “用于按需拼车的神经近似动态规划，” 见 *AAAI 人工智能会议论文集*，第34卷，第01期，2020年，第507–515页。'
- en: '[146] C. Raju, Y. Narahari, and K. Ravikumar, “Reinforcement learning applications
    in dynamic pricing of retail markets,” in *EEE International Conference on E-Commerce,
    2003\. CEC 2003.*   IEEE, 2003, pp. 339–346.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] C. Raju, Y. Narahari, 和 K. Ravikumar, “零售市场动态定价中的强化学习应用，” 见 *EEE 国际电子商务会议，2003年。CEC
    2003*。IEEE，2003年，第339–346页。'
- en: '[147] D. Bertsimas and G. Perakis, “Dynamic pricing: A learning approach,”
    in *Mathematical and computational models for congestion charging*.   Springer,
    2006, pp. 45–79.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] D. Bertsimas 和 G. Perakis, “动态定价：一种学习方法，” 见 *拥堵收费的数学和计算模型*。Springer，2006年，第45–79页。'
- en: '| ![[Uncaptioned image]](img/8c23f2b4111719e5f9acaca80794609c.png) | Zefang
    Zong received the B.S. degree in Department of Electronic Engineering, Tsinghua
    University, Beijing, China, in 2019\. At present, he is studying for the Ph.D.
    degree in big data from Department of Electronic Engineering, Tsinghua University,
    Beijing, China. His research interests include reinforcement learning, combinatorial
    optimization and urban computing. |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/8c23f2b4111719e5f9acaca80794609c.png) | Zefang Zong 于2019年获得了中国北京清华大学电子工程系的学士学位。目前，他正在清华大学电子工程系攻读大数据方向的博士学位。他的研究兴趣包括强化学习、组合优化和城市计算。
    |'
- en: '| ![[Uncaptioned image]](img/d756bd92a09e42884f83d016e82e8701.png) | Tao Feng
    received the B.S. degree in School of Electrical and Information Engineering,
    Tianjin University, Tianjin, China, in 2019\. He is currently a M.S. student in
    the Department of Electronic Engineering, Tsinghua University, Beijing, China.
    His researches focus on reinforcement learning, graph neural networks and deep
    learning. |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| ![[未加标题的图片]](img/d756bd92a09e42884f83d016e82e8701.png) | 冯涛于2019年获得天津大学电气与信息工程学院的学士学位。他目前是清华大学电子工程系的硕士研究生。他的研究方向集中在强化学习、图神经网络和深度学习。
    |'
- en: '| ![[Uncaptioned image]](img/6f0b7ffdae94a2d523325bb354b22719.png) | Tong Xia
    received her B.S. degree in electrical engineering from School of Electrical Information,
    Wuhan University, Wuhan, China, in 2017, and the M.S. degree in electronic engineering
    from Tsinghua University, Beijing, China, in 2020\. At present, she is studying
    for the Ph.D degree in machine learning from Department of Computer Science, the
    University of Cambridge, Cambridge, UK. Her research interests include user behavior
    modelling, ubiquitous computing and artificial intelligence. |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| ![[未加标题的图片]](img/6f0b7ffdae94a2d523325bb354b22719.png) | 夏彤于2017年获得武汉大学电气信息学院的电气工程学士学位，并于2020年获得清华大学电子工程硕士学位。目前，她在剑桥大学计算机科学系攻读机器学习博士学位。她的研究兴趣包括用户行为建模、普适计算和人工智能。
    |'
- en: '| ![[Uncaptioned image]](img/241e7ee02bd35bac80764d656cc8fa15.png) | Depeng
    Jin (M’2009) received his B.S. and Ph.D. degrees from Tsinghua University, Beijing,
    China, in 1995 and 1999 respectively both in electronics engineering. Now he is
    an associate professor at Tsinghua University and vice chair of Department of
    Electronic Engineering. Dr. Jin was awarded National Scientific and Technological
    Innovation Prize (Second Class) in 2002\. His research fields include telecommunications,
    high-speed networks, ASIC design and future internet architecture. |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| ![[未加标题的图片]](img/241e7ee02bd35bac80764d656cc8fa15.png) | 金德鹏（M’2009）于1995年和1999年分别获得清华大学电子工程学士和博士学位。他现在是清华大学电子工程系的副教授兼副主任。金博士于2002年获得国家科技创新二等奖。他的研究领域包括电信、高速网络、ASIC设计和未来互联网架构。
    |'
- en: '| ![[Uncaptioned image]](img/2462e7dec33f93bd70268613c9fb2591.png) | Yong Li
    (M’09-SM’16) is currently a Tenured Associate Professor of the Department of Electronic
    Engineering, Tsinghua University. He received the Ph.D. degree in electronic engineering
    from Tsinghua University in 2012\. His research interests include machine learning
    and big data mining, particularly, automatic machine learning and spatial-temporal
    data mining for urban computing, recommender systems, and knowledge graphs. Dr.
    Li has served as General Chair, TPC Chair, SPC/TPC Member for several international
    workshops and conferences, and he is on the editorial board of two IEEE journals.
    He has published over 100 papers on first-tier international conferences and journals,
    including KDD, WWW, UbiComp, SIGIR, AAAI, TKDE, TMC etc, and his papers have total
    citations more than 8300\. Among them, ten are ESI Highly Cited Papers in Computer
    Science, and five receive conference Best Paper (run-up) Awards. He received IEEE
    2016 ComSoc Asia-Pacific Outstanding Young Researchers, Young Talent Program of
    China Association for Science and Technology, and the National Youth Talent Support
    Program. |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| ![[未加标题的图片]](img/2462e7dec33f93bd70268613c9fb2591.png) | 李勇（M’09-SM’16）目前是清华大学电子工程系的终身副教授。他于2012年获得清华大学电子工程博士学位。他的研究兴趣包括机器学习和大数据挖掘，特别是自动机器学习和城市计算的时空数据挖掘、推荐系统以及知识图谱。李博士曾担任多个国际研讨会和会议的总主席、TPC主席、SPC/TPC成员，并且是两个IEEE期刊的编辑委员会成员。他在KDD、WWW、UbiComp、SIGIR、AAAI、TKDE、TMC等顶级国际会议和期刊上发表了100多篇论文，总引用次数超过8300次。其中，有十篇是计算机科学领域的ESI高被引论文，五篇获得了会议最佳论文（提名）奖。他获得了IEEE
    2016 ComSoc亚太区杰出青年研究员、中华全国科学技术协会青年人才计划和国家青年人才支持计划。 |'
