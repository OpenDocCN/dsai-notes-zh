- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:45:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:45:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2207.03820] Deep Learning for Anomaly Detection in Log Data: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2207.03820] 深度学习在日志数据异常检测中的应用综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2207.03820](https://ar5iv.labs.arxiv.org/html/2207.03820)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2207.03820](https://ar5iv.labs.arxiv.org/html/2207.03820)
- en: 'Deep Learning for Anomaly Detection in Log Data: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在日志数据异常检测中的应用综述
- en: Max Landauer, Sebastian Onder, Florian Skopik, and Markus Wurzenberger Manuscript
    published in the Machine Learning with Applications, vol. 12 (2023) under the
    CC BY license.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Max Landauer, Sebastian Onder, Florian Skopik, 和 Markus Wurzenberger 发表在《机器学习应用》第12卷（2023年），采用CC
    BY许可。
- en: https://doi.org/10.1016/j.mlwa.2023.100470
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: https://doi.org/10.1016/j.mlwa.2023.100470
- en: M. Landauer, S. Onder, F. Skopik, and M. Wurzenberger are with the Center for
    Digital Safety and Security, AIT Austrian Institute of Technology, Vienna, Austria.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: M. Landauer, S. Onder, F. Skopik 和 M. Wurzenberger 隶属于奥地利维也纳AIT奥地利科技研究院数字安全与保障中心。
- en: 'E-mail: firstname.lastname@ait.ac.at'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：firstname.lastname@ait.ac.at
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Automatic log file analysis enables early detection of relevant incidents such
    as system failures. In particular, self-learning anomaly detection techniques
    capture patterns in log data and subsequently report unexpected log event occurrences
    to system operators without the need to provide or manually model anomalous scenarios
    in advance. Recently, an increasing number of approaches leveraging deep learning
    neural networks for this purpose have been presented. These approaches have demonstrated
    superior detection performance in comparison to conventional machine learning
    techniques and simultaneously resolve issues with unstable data formats. However,
    there exist many different architectures for deep learning and it is non-trivial
    to encode raw and unstructured log data to be analyzed by neural networks. We
    therefore carry out a systematic literature review that provides an overview of
    deployed models, data pre-processing mechanisms, anomaly detection techniques,
    and evaluations. The survey does not quantitatively compare existing approaches
    but instead aims to help readers understand relevant aspects of different model
    architectures and emphasizes open issues for future work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自动日志文件分析能够及早发现相关事件，如系统故障。特别是，自学习异常检测技术可以捕捉日志数据中的模式，并随后向系统操作员报告意外的日志事件，无需事先提供或手动建模异常场景。近年来，越来越多利用深度学习神经网络的相关方法被提出。这些方法在检测性能上优于传统机器学习技术，同时解决了不稳定的数据格式问题。然而，深度学习存在许多不同的架构，将原始且非结构化的日志数据编码以供神经网络分析并非易事。因此，我们进行了系统的文献综述，概述了已部署的模型、数据预处理机制、异常检测技术及其评估。该综述未对现有方法进行定量比较，而是旨在帮助读者理解不同模型架构的相关方面，并强调未来工作的开放问题。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: log data, anomaly detection, neural networks, deep learning
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 日志数据，异常检测，神经网络，深度学习
- en: I Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Log files provide a rich source of information when it comes to monitoring computer
    systems. Thereby, the majority of log events are usually generated as consequences
    of normal system operations, such as starting and stopping of processes, restarting
    of virtual machines, users accessing resources, etc. However, applications also
    produce logs when faulty or otherwise undesired system states occur, for example,
    failed processes, availability issues, or security incidents. These traces of
    unexpected and possibly unsafe system activities are important for system operators
    that timely need to act upon them to prevent or diminish system damage and avoid
    adverse cascading effects.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 日志文件在计算机系统监控中提供了丰富的信息来源。因此，大多数日志事件通常是正常系统操作的结果，例如进程的启动和停止、虚拟机的重启、用户访问资源等。然而，当系统出现故障或其他不希望的状态时，应用程序也会生成日志，例如失败的进程、可用性问题或安全事件。这些意外且可能不安全的系统活动的痕迹对系统操作员非常重要，他们需要及时采取行动以防止或减少系统损害，并避免不利的级联效应。
- en: The main problem for this kind of log file analysis is that it is non-trivial
    to identify these relevant log events within the much larger number of less interesting
    traces of standard system usage. In particular, the sheer amount of logs produced
    by modern applications renders manual analysis infeasible and necessitates automatic
    mechanisms [[1](#bib.bib1)]. Unfortunately, manually coded signatures and rules
    that search for specific keywords in logs only have limited applicability and
    are not suitable for scenarios that are not known beforehand [[2](#bib.bib2)].
    It is therefore necessary to deploy anomaly detection techniques that automatically
    learn models representing the normal baseline of system behavior and subsequently
    disclose any deviations from these models as possibly adverse activities that
    require attention by human operators.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种日志文件分析的主要问题在于，在大量不那么有趣的标准系统使用痕迹中识别相关日志事件并非易事。特别是，现代应用程序生成的日志数量庞大，使得人工分析不可行，必须采用自动机制[[1](#bib.bib1)]。不幸的是，手动编码的签名和规则仅能在有限的适用范围内工作，且不适用于事先未知的场景[[2](#bib.bib2)]。因此，有必要部署异常检测技术，自动学习表示系统正常基线的模型，并随后揭示与这些模型的任何偏差，以作为可能需要人工操作员关注的有害活动。
- en: Machine learning provides many viable techniques for the purpose of anomaly
    detection in log files and many different approaches have been proposed in the
    past, including clustering [[3](#bib.bib3)] and workflow mining [[4](#bib.bib4)],
    statistical analysis of event parameters [[5](#bib.bib5)], time-series analysis
    to recognize changes of event frequencies [[6](#bib.bib6)], and many more [[7](#bib.bib7),
    [2](#bib.bib2)]. Recently, researchers started using deep neural networks for
    log-based anomaly detection in an attempt to repeat the successes of deep learning
    from image and speech recognition that outperform conventional machine learning
    methods [[8](#bib.bib8)]. However, as system log events are generally unstructured
    and involve intricate dependencies, it is non-trivial to prepare the data in a
    way to enable ingestion by neural networks and extract features that are relevant
    for detection. Moreover, the wide variety of existing deep learning architectures
    such as recurrent or convolutional neural networks makes it difficult to select
    an appropriate model for a specific use-case at hand and understand their respective
    requirements on the format and properties of the input data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习为日志文件中的异常检测提供了许多可行的技术，过去提出了许多不同的方法，包括聚类[[3](#bib.bib3)]和工作流挖掘[[4](#bib.bib4)]、事件参数的统计分析[[5](#bib.bib5)]、时间序列分析以识别事件频率的变化[[6](#bib.bib6)]，以及更多方法[[7](#bib.bib7)、[2](#bib.bib2)]。最近，研究人员开始使用深度神经网络进行基于日志的异常检测，试图重现深度学习在图像和语音识别中的成功，这些成功超越了传统机器学习方法[[8](#bib.bib8)]。然而，由于系统日志事件通常是非结构化的，并涉及复杂的依赖关系，因此很难准备数据以使神经网络能够处理并提取对检测相关的特征。此外，现有的深度学习架构如递归神经网络或卷积神经网络的广泛多样性，使得为特定用例选择合适的模型以及理解对输入数据格式和属性的要求变得困难。
- en: To the best of our knowledge there is currently only a limited overview of the
    state-of-the-art of log-based anomaly detection with deep learning [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)]. As a consequence it is
    difficult to understand what features are suitable to be extracted from raw log
    data, how these features could be transformed into a format that is adequate to
    be ingested by neural networks, and which model architectures are appropriate
    for detecting specific patterns in logs. Existing surveys only compare few anomaly
    detection approaches and focus mainly on sequential patterns in log data [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)], present broad studies on system log data
    analysis that do not sufficiently cover deep learning models and challenges [[13](#bib.bib13),
    [14](#bib.bib14)], or focus on network traffic rather than system log data [[12](#bib.bib12)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，目前对基于深度学习的日志异常检测的最先进技术了解有限[[9](#bib.bib9)、[10](#bib.bib10)、[11](#bib.bib11)、[12](#bib.bib12)]。因此，很难理解哪些特征适合从原始日志数据中提取，这些特征如何转化为适合神经网络处理的格式，以及哪些模型架构适合检测日志中的特定模式。现有的调查仅比较了少数异常检测方法，主要关注日志数据中的序列模式[[9](#bib.bib9)、[10](#bib.bib10)、[11](#bib.bib11)]，提供了对系统日志数据分析的广泛研究，但未能充分涵盖深度学习模型及其挑战[[13](#bib.bib13)、[14](#bib.bib14)]，或关注网络流量而非系统日志数据[[12](#bib.bib12)]。
- en: 'We therefore carry out a systematic literature review on deep learning for
    anomaly detection in log data. Our main focus is thereby to survey scientific
    publications on the deployed model architectures, their respective requirements
    and transformations for handling unstructured input log data, the methods used
    to differentiate between normal and anomalous data samples, and the presented
    evaluations. The results of this study are beneficial for researchers and industries
    alike, because a better understanding of challenges and features of different
    deep learning algorithms avoids pitfalls when developing anomaly detection techniques
    and eases selection of existing detection systems for both academic and real-world
    use-cases. Moreover, a detailed investigation of pre-processing strategies is
    essential to utilize all information available in the logs when carrying out anomaly
    detection and to understand the influence of data representations on the detection
    capabilities, in particular, what types of anomalies can be detected under which
    circumstances. Regarding scientific evaluations, we particularly pay attention
    to relevant aspects of experiment design, including data sets, metrics, and reproducibility,
    to point out deficiencies in prevalent evaluation strategies and suggest remedies.
    Finally, our study also aims to create a work of reference and establish a starting
    point for future research. We point out that this survey does not quantitatively
    compare detection performances of the reviewed approaches as only few open-source
    implementations are available and comparisons of these approaches are already
    presented in other surveys [[9](#bib.bib9), [10](#bib.bib10)]. In alignment with
    the aforementioned goals we formulate the research questions of this survey as
    follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们对深度学习在日志数据异常检测中的应用进行了系统的文献综述。我们的主要关注点是调查科学出版物中关于已部署模型架构的内容，以及这些架构处理非结构化输入日志数据的要求和转换、区分正常数据样本与异常数据样本的方法以及所展示的评估。这项研究的结果对研究人员和行业都非常有益，因为更好地理解不同深度学习算法的挑战和特性可以避免在开发异常检测技术时出现陷阱，并且简化了学术和实际应用场景下现有检测系统的选择。此外，详细调查预处理策略对于利用日志中所有可用信息进行异常检测是必不可少的，并且有助于理解数据表示对检测能力的影响，特别是哪些类型的异常可以在什么情况下被检测到。关于科学评估，我们特别关注实验设计的相关方面，包括数据集、指标和可重复性，以指出当前评估策略中的不足并提出改进建议。最后，我们的研究还旨在创建一部参考性工作，并为未来的研究奠定基础。我们指出，本综述并未定量比较所审查方法的检测性能，因为只有少量开源实现可用，并且这些方法的比较已经在其他综述中呈现
    [[9](#bib.bib9)， [10](#bib.bib10)]。根据上述目标，我们将本综述的研究问题表述如下：
- en: 'RQ1:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ1：
- en: What are the main challenges of log-based anomaly detection with deep learning?
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于日志的深度学习异常检测面临的主要挑战是什么？
- en: 'RQ2:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ2：
- en: What state-of-the-art deep learning algorithms are typically applied?
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常应用哪些最先进的深度学习算法？
- en: 'RQ3:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ3：
- en: How is log data pre-processed to be ingested by deep learning models?
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 日志数据如何预处理以供深度学习模型使用？
- en: 'RQ4:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ4：
- en: What types of anomalies are detected and how are they identified as such?
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检测到哪些类型的异常，以及它们是如何被识别为异常的？
- en: 'RQ5:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ5：
- en: How are the proposed models evaluated?
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出的模型如何进行评估？
- en: 'RQ6:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ6：
- en: To what extent do the approaches rely on labeled data and support incremental
    learning?
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些方法在多大程度上依赖标记数据并支持增量学习？
- en: 'RQ7:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ7：
- en: To what extent are the presented results reproducible in terms of availability
    of source code and used data?
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所呈现结果在源代码和使用数据的可用性方面在多大程度上可重复？
- en: 'The remainder of this paper is structured as follows. Section [II](#S2 "II
    Background ‣ Deep Learning for Anomaly Detection in Log Data: A Survey") first
    explains the terms deep learning, log data, and anomaly detection, and then provides
    an overview of common challenges. We explain our methodology for selecting relevant
    publications and carrying out the survey in Sect. [III](#S3 "III Survey Method
    ‣ Deep Learning for Anomaly Detection in Log Data: A Survey"). Section [IV](#S4
    "IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")
    presents all results of our survey in detail. We discuss these results and answer
    our research questions in Sect. [V](#S5 "V Discussion ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey"). Finally, Sect. [VI](#S6 "VI Conclusion ‣ Deep
    Learning for Anomaly Detection in Log Data: A Survey") concludes this paper.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分结构如下。第 [II](#S2 "II Background ‣ Deep Learning for Anomaly Detection
    in Log Data: A Survey") 节首先解释了深度学习、日志数据和异常检测的术语，然后提供了常见挑战的概述。我们在第 [III](#S3 "III
    Survey Method ‣ Deep Learning for Anomaly Detection in Log Data: A Survey") 节中解释了选择相关文献和进行调查的方法。第
    [IV](#S4 "IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data:
    A Survey") 节详细呈现了我们的调查结果。我们在第 [V](#S5 "V Discussion ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey") 节中讨论这些结果并回答我们的研究问题。最后，第 [VI](#S6 "VI Conclusion
    ‣ Deep Learning for Anomaly Detection in Log Data: A Survey") 节总结了本文。'
- en: II Background
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: In this section we first clarify some general concepts and terms relevant for
    anomaly detection in log data based on deep learning. We then outline scientific
    challenges that are specific to that research field.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先澄清一些与基于深度学习的日志数据异常检测相关的通用概念和术语。然后，我们概述该研究领域特有的科学挑战。
- en: II-A Preliminary Definitions
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 初步定义
- en: 'The study carried out in this paper hinges on an understanding of three main
    concepts: deep learning, log data, and anomaly detection. However, the exact characteristics
    and consequential requirements of the respective fields may be used differently
    across research areas and existing literature. In the following, we therefore
    describe the basic properties of these three concepts.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究依赖于对三个主要概念的理解：深度学习、日志数据和异常检测。然而，不同研究领域和现有文献中对这些领域的具体特征和随之产生的要求可能有所不同。因此，下面我们描述这三个概念的基本属性。
- en: II-A1 Deep Learning
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 深度学习
- en: Artificial neural networks (ANN) have been developed in an attempt to recreate
    biological information processing systems in the form of connected communication
    nodes. For this purpose, varying numbers of nodes are arranged in sequences of
    layers, in particular, an input layer that reads in the data, several hidden layers
    connecting neighboring layers with weighted edges, and an output layer. Nodes
    activate when receiving specific signals on their connected edges, which in turn
    generates the input for subsequent layers. The main idea is that such networks
    are capable of recognizing non-linear structures in the input data and subsequently
    classifying the processed instances through training, which involves minimizing
    the error of classifications by adjusting the weights of edges accordingly. Thereby,
    ANN enable supervised training where labels for all classes are available (i.e.,
    data samples are marked with labels such as normal or anomalous), semi-supervised
    training where labels of some classes are available, as well as unsupervised learning
    where no labels are available.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）是为了模拟生物信息处理系统的连接通信节点而开发的。为此，节点以不同数量按层次排列，特别是一个读取数据的输入层、若干个通过加权边连接相邻层的隐藏层，以及一个输出层。节点在接收到其连接边上的特定信号时会被激活，从而生成后续层的输入。主要思想是，这些网络能够识别输入数据中的非线性结构，并通过训练对处理后的实例进行分类，训练过程中通过相应调整边的权重来最小化分类错误。因此，ANN
    支持有监督训练（所有类别都有标签，即数据样本标记为正常或异常）、半监督训练（某些类别有标签）以及无监督学习（没有标签可用）。
- en: In general, deep learning algorithms are understood as neural networks with
    multiple hidden layers. Several different architectures of deep neural networks
    have been proposed in the past, such as recurrent neural networks (RNN) for sequential
    input data. Deep learning has been shown to outperform conventional machine learning
    methods (e.g., support vector machines or decision trees) and even human experts
    in many application areas such as image classification, speech recognition, and
    many more [[8](#bib.bib8), [15](#bib.bib15)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，深度学习算法被理解为具有多个隐藏层的神经网络。过去提出了几种不同的深度神经网络架构，例如用于顺序输入数据的递归神经网络（RNN）。深度学习已经证明在许多应用领域（例如，图像分类、语音识别等）优于传统的机器学习方法（例如，支持向量机或决策树）甚至人类专家[[8](#bib.bib8),
    [15](#bib.bib15)]。
- en: II-A2 Log Data
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 日志数据
- en: Log data are a chronological sequence of single- or multi-line events generated
    by applications to permanently capture specific system states, in particular,
    for manual forensic analysis in case that failures or other unexpected incidents
    occur. Log events are usually available in textual form and range from structured
    vectors (e.g., comma-separated values) over semi-structured objects (e.g., key-value
    pairs) to unstructured human-readable messages with heterogeneous event types.
    Despite the fact that no unified log format exists, log events usually contain
    their generation time stamp as one of their event parameters. Other parameters
    that are sometimes present in different types of log data are logging levels (e.g.,
    INFO or ERROR) or process identifiers that link sequences of related events [[3](#bib.bib3)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 日志数据是应用程序生成的单行或多行事件的时间顺序，以永久捕捉特定系统状态，特别是在发生故障或其他意外事件时进行手动取证分析。日志事件通常以文本形式存在，范围从结构化向量（例如，逗号分隔值）到半结构化对象（例如，键值对）再到具有异构事件类型的非结构化人类可读消息。尽管没有统一的日志格式，但日志事件通常包含其生成时间戳作为其事件参数之一。其他有时存在于不同类型日志数据中的参数是日志级别（例如，INFO或ERROR）或将相关事件序列链接起来的进程标识符[[3](#bib.bib3)]。
- en: While single log events describe (part of) the system state in one particular
    point in time, groups of log events represent the dynamic workflows of the underlying
    program logic. The reason for this is that log events are generated by print statements
    purposefully placed by software developers throughout their code to support understanding
    of program activities and debugging. These statements comprise of static parts,
    i.e., hard-coded strings, and variable parts, i.e., parameters that are dynamically
    determined during program runtime. In the past, a large amount of research was
    directed towards automatic extraction of so-called log keys (also known as log
    signatures, log templates, or simply log events) that represent templates for
    the original print statements and enable parsing of logs [[16](#bib.bib16)]. These
    parsers allow to derive values from logs that are more suitable to be used for
    subsequent analysis than unstructured log messages, in particular, through (i)
    assignment of event type identifiers to log events and (ii) extraction of parameters
    from log messages [[17](#bib.bib17)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然单个日志事件描述了系统在特定时间点的（部分）状态，但日志事件的组表示了底层程序逻辑的动态工作流。这是因为日志事件是由软件开发人员在代码中故意放置的打印语句生成的，以支持对程序活动的理解和调试。这些语句包括静态部分，即硬编码字符串，以及动态部分，即在程序运行时动态确定的参数。在过去，大量研究致力于自动提取所谓的日志键（也称为日志签名、日志模板或简单的日志事件），这些日志键代表原始打印语句的模板，并使日志解析成为可能[[16](#bib.bib16)]。这些解析器允许从日志中提取比非结构化日志消息更适合用于后续分析的值，特别是通过（i）将事件类型标识符分配给日志事件以及（ii）从日志消息中提取参数[[17](#bib.bib17)]。
- en: II-A3 Anomaly Detection
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 异常检测
- en: Anomalies are those instances in a data set that exhibit rare or otherwise unexpected
    characteristics and thus stand out from the rest of the data [[7](#bib.bib7)].
    For the purpose of detection, the conformity of these data instances is usually
    measured through one or more continuous or categorical attributes that are associated
    with each instance and enable the computation of similarity metrics. For independent
    data, it is sufficient to declare single or small groups of instances with high
    dissimilarities to all other data points as outliers, which are also referred
    to as point anomalies. For all data where instances are not independent from each
    other, e.g., all kinds of ordered data including log data, two additional types
    of anomalies occur. First, contextual anomalies are instances that are only anomalous
    with respect to the context they occur (but not otherwise), such as the time of
    occurrence. For example, consider the start of a daily executed data backup procedure
    that suddenly takes place outside of the scheduled times. Second, collective anomalies
    are groups of instances that are only anomalous due to their combined occurrence
    (but not individually), such as a specific sequence of log events.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 异常是数据集中那些表现出稀有或其他意外特征的实例，从而在数据中脱颖而出[[7](#bib.bib7)]。为了检测这些异常，通常通过与每个实例相关的一个或多个连续或分类属性来衡量这些数据实例的一致性，并计算相似性度量。对于独立数据，声明与所有其他数据点高度不相似的单个或小组实例为异常值是足够的，这也被称为点异常。对于实例之间不独立的所有数据，例如各种有序数据，包括日志数据，会出现另外两种类型的异常。首先，背景异常是指仅在其发生的背景下（而不是其他情况下）异常的实例，例如发生的时间。例如，考虑一个每天执行的数据备份程序的开始，突然发生在计划时间之外。其次，集体异常是指由于其组合出现而仅表现为异常的实例组（而不是单独的），例如特定的日志事件序列。
- en: An implicit assumption of most anomaly detection techniques is that the analyzed
    data holds far fewer anomalies than normal instances. This enables that detection
    takes place in a fully unsupervised manner, i.e., no labeled data is necessary
    to train the models. However, many scientific approaches instead pursue semi-supervised
    detection, where training data containing only normal instances are available
    and evaluation then takes place on a test data set comprising both normal and
    anomalous instances. The main advantages of semi-supervised operation is that
    anomalous instances are not learned by the models and that it is often relatively
    simple to gather normal data, while modeling and labeling anomalies is less straightforward.
    Accordingly, approaches for supervised anomaly detection have lower applicability
    and are comparatively rare [[4](#bib.bib4)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数异常检测技术隐含的假设是，分析的数据中异常实例远少于正常实例。这使得检测可以在完全无监督的方式下进行，即不需要标记数据来训练模型。然而，许多科学方法转而采用半监督检测，其中仅有正常实例的训练数据可用，然后在包含正常和异常实例的测试数据集上进行评估。半监督操作的主要优点是异常实例不会被模型学习，并且通常收集正常数据相对简单，而建模和标记异常则不那么直接。因此，监督异常检测的方法适用性较低且相对较少[[4](#bib.bib4)]。
- en: II-B Challenges
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 挑战
- en: Log-based anomaly detection has been an active field of research for decades.
    Thereby, most of the presented approaches rely on conventional machine learning
    techniques. However, the last few years have seen a strong increase of approaches
    that leverage deep learning to disclose anomalous log events that relate to unexpected
    system behavior. In the following, we summarize the main challenges that need
    to be overcome for effective and applicable detection.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于日志的异常检测已经成为一个活跃的研究领域几十年。因此，大多数已提出的方法依赖于传统的机器学习技术。然而，近年来，利用深度学习揭示与意外系统行为相关的异常日志事件的方法显著增加。接下来，我们总结了在有效和适用检测中需要克服的主要挑战。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data representation. Deep learning systems generally consume structured and
    numeric input data. It is non-trivial to feed log data into neural networks as
    they frequently involve a mix of heterogeneous event types, unstructured messages,
    and categorical parameters [[11](#bib.bib11), [18](#bib.bib18)].
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据表示。深度学习系统通常处理结构化和数值输入数据。将日志数据输入神经网络并非易事，因为这些数据经常涉及异质事件类型、非结构化消息和分类参数的混合[[11](#bib.bib11),
    [18](#bib.bib18)]。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data instability. As applications evolve, new log event types may occur that
    differ from the ones in the training data. In addition, the observed system behavior
    patterns are subject to change as technological environments and their utilization
    vary over time. Deep learning systems therefore need to incrementally update their
    models and adapt their baseline for normal system behavior to enable real-time
    detection [[19](#bib.bib19), [11](#bib.bib11), [18](#bib.bib18), [9](#bib.bib9),
    [10](#bib.bib10)].
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据不稳定。随着应用的演变，可能会出现与训练数据中的日志事件类型不同的新事件。此外，观察到的系统行为模式会随着技术环境及其利用情况的变化而变化。因此，深度学习系统需要逐步更新其模型并调整正常系统行为的基线，以实现实时检测
    [[19](#bib.bib19), [11](#bib.bib11), [18](#bib.bib18), [9](#bib.bib9), [10](#bib.bib10)]。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Class imbalance. Anomaly detection inherently assumes that normal events outnumber
    anomalous ones. Many approaches based on neural networks are known to perform
    sub-optimally for imbalanced data sets [[18](#bib.bib18), [10](#bib.bib10)].
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类别不平衡。异常检测本质上假设正常事件的数量多于异常事件。许多基于神经网络的方法在处理不平衡数据集时表现不佳 [[18](#bib.bib18), [10](#bib.bib10)]。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Anomalous artifact diversity. Manifestations of anomalies affect log events
    as well as parameters thereof in various ways, including changes of sequential
    patterns, frequencies, correlations, inter-arrival times, etc. Detection techniques
    are often designed only for properties of specific anomaly types and are therefore
    not generally applicable.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 异常伪影多样性。异常的表现方式影响日志事件及其参数，包括顺序模式、频率、相关性、到达间隔等的变化。检测技术通常仅为特定异常类型的属性而设计，因此并不具有通用性。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Label availability. As anomalies represent unexpected system behavior, there
    are generally no labeled anomaly instances available for training. This restricts
    applications to semi- and unsupervised deep learning systems, which are known
    to achieve lower detection performance than supervised approaches [[9](#bib.bib9),
    [10](#bib.bib10)].
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标签可用性。由于异常代表了系统的意外行为，通常没有标记的异常实例可用于训练。这限制了应用于半监督和无监督深度学习系统，这些系统的检测性能通常低于监督方法
    [[9](#bib.bib9), [10](#bib.bib10)]。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Stream processing. Logs are generated as a continuous stream of data. To enable
    on-the-fly monitoring rather than forensic analysis, deep learning systems need
    to be designed for single-pass data processing when it comes to detection and
    model updating [[3](#bib.bib3), [10](#bib.bib10)].
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 流处理。日志以连续的数据流形式生成。为了实现实时监控而非事后分析，深度学习系统需要设计为单次通过数据处理，以便进行检测和模型更新 [[3](#bib.bib3),
    [10](#bib.bib10)]。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data volume. Log data is generated in high volumes, with some systems producing
    millions [[20](#bib.bib20)] or even billions [[21](#bib.bib21)] of events daily.
    Efficient algorithms are required to ensure real-time processing in practical
    applications, in particular, when running on machines with few computational resources
    such as edge devices.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据量。日志数据生成量很大，一些系统每天产生数百万 [[20](#bib.bib20)] 甚至数十亿 [[21](#bib.bib21)] 条事件。需要高效的算法以确保在实际应用中的实时处理，特别是在运行在计算资源较少的机器（如边缘设备）时。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Interleaving logs. Sequences of related log events may be interleaving each
    other when many processes operate simultaneously or distributed logs are collected
    centrally. It is non-trivial to retrieve the original event sequences when events
    lack session identifiers [[10](#bib.bib10)].
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 日志交错。当多个进程同时运行或分布式日志被集中收集时，相关日志事件的序列可能会互相交错。当事件缺乏会话标识符时，检索原始事件序列并非易事 [[10](#bib.bib10)]。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data quality. Low data quality may be the result of improper log collection
    or technical issues during log generation and cause negative effects on machine
    learning effectiveness. Common problems involve incorrect time stamp information,
    event ordering, missing events, duplicated records, mislabeled events, etc. [[22](#bib.bib22),
    [23](#bib.bib23)].
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据质量。低数据质量可能由于日志收集不当或日志生成过程中的技术问题而导致，对机器学习的有效性产生负面影响。常见问题包括时间戳信息不正确、事件排序错误、缺失事件、重复记录、事件标签错误等
    [[22](#bib.bib22), [23](#bib.bib23)]。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model explainability. Approaches based on neural networks generally suffer from
    a lower explainability than conventional machine learning methods. Difficulties
    to understand the reasons behind both correct and incorrect classifications are
    especially problematic when it comes to making justified decisions in response
    to critical system behavior or security incidents [[18](#bib.bib18), [9](#bib.bib9)].
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型可解释性。基于神经网络的方法通常比传统机器学习方法的可解释性较低。在做出针对系统关键行为或安全事件的有依据的决策时，理解正确和错误分类的原因特别困难，这是一大问题[[18](#bib.bib18),
    [9](#bib.bib9)]。
- en: III Survey Method
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 调查方法
- en: This section outlines the method that was used to carry out the systematic literature
    review. We first describe our strategy for collecting relevant literature and
    then present the evaluation criteria that we used to analyze the retrieved papers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了进行系统文献综述所使用的方法。我们首先描述了收集相关文献的策略，然后介绍了用于分析检索到的论文的评估标准。
- en: III-A Search Strategy
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 搜索策略
- en: In this section we describe the process of gathering relevant publications to
    be included in the survey. First, an initial collection of literature is collected
    using a web search. Subsequently, relevant papers are selected using inclusion,
    exclusion, and quality criteria.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了收集要纳入调查的相关出版物的过程。首先，通过网络搜索收集初步文献集合。随后，使用包含、排除和质量标准选择相关论文。
- en: III-A1 Initial Literature Collection with Search String
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 使用搜索字符串进行初步文献收集
- en: 'In order to obtain an initial set of approaches from the state-of-the-art,
    we assemble a search string to query common databases for scientific publications.
    In particular, we design the search string so that only publications containing
    the three main concepts relevant for this survey are retrieved: log data, anomaly
    detection, and deep learning. Since some publications use different terminology
    and to decrease the likelihood that relevant publications are missed, we also
    use the terms “system log(s)”, “event log(s)”, “log file(s)”, “log event(s)” as
    alternatives for “log data”, and “neural network(s)” as an alternative for “deep
    learning”. We omit the term “log” without any other word as it yields many results
    that contain logarithms but are not relevant for our study. Figure [1](#S3.F1
    "Figure 1 ‣ III-A1 Initial Literature Collection with Search String ‣ III-A Search
    Strategy ‣ III Survey Method ‣ Deep Learning for Anomaly Detection in Log Data:
    A Survey") displays the final search string.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '为了从前沿技术中获得初步的研究方法，我们构建了一个搜索字符串来查询科学出版物的常见数据库。特别地，我们设计了搜索字符串，以确保仅检索包含本次调查三个主要概念的出版物：日志数据、异常检测和深度学习。由于一些出版物使用不同的术语，并且为了减少遗漏相关出版物的可能性，我们还使用了“系统日志”、“事件日志”、“日志文件”、“日志事件”作为“日志数据”的替代词，以及“神经网络”作为“深度学习”的替代词。我们省略了“日志”这一单独词汇，因为它会产生许多包含对数的结果，但与我们的研究无关。图[1](#S3.F1
    "Figure 1 ‣ III-A1 Initial Literature Collection with Search String ‣ III-A Search
    Strategy ‣ III Survey Method ‣ Deep Learning for Anomaly Detection in Log Data:
    A Survey")显示了最终的搜索字符串。'
- en: '![Refer to caption](img/c3c1ab3ac18e1e086c3bdbc568f601a4.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c3c1ab3ac18e1e086c3bdbc568f601a4.png)'
- en: 'Figure 1: Composition of the search string used to retrieve relevant literature.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：用于检索相关文献的搜索字符串组成。
- en: 'We then use this search string to gather publications from the following databases:
    Science Direct¹¹1https://www.sciencedirect.com/, Scopus²²2https://www.scopus.com/,
    SpringerLink³³3https://link.springer.com/, ACM Digital Library⁴⁴4https://dl.acm.org/,
    IEEE Xplore⁵⁵5https://ieeexplore.ieee.org/, Google Scholar⁶⁶6https://scholar.google.com/,
    and Web of Science⁷⁷7https://www.webofscience.com/. The search was conducted in
    January of 2022 and returned a total of 2925 publications. In the following section,
    we describe our method for selecting relevant publications from this set.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用这个搜索字符串从以下数据库中收集出版物：Science Direct¹¹1https://www.sciencedirect.com/，Scopus²²2https://www.scopus.com/，SpringerLink³³3https://link.springer.com/，ACM
    Digital Library⁴⁴4https://dl.acm.org/，IEEE Xplore⁵⁵5https://ieeexplore.ieee.org/，Google
    Scholar⁶⁶6https://scholar.google.com/，以及Web of Science⁷⁷7https://www.webofscience.com/。搜索于2022年1月进行，返回了总计2925篇出版物。在接下来的部分，我们将描述从这一集合中选择相关出版物的方法。
- en: III-A2 Selection of Relevant Publications
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 相关出版物的选择
- en: 'To sort out publications that are not relevant for this survey and reduce the
    set of publications to a manageable size, we define multiple selection criteria
    and apply them on our initial collection. Our main criterion for including the
    publication in the survey is as follows: The model proposed in the publication
    applies deep learning techniques (i.e., a multi-layered neural network) for anomaly
    detection in heterogeneous and unstructured log data. Moreover, we define several
    exclusion criteria that we use to omit publications with low relevance or otherwise
    inappropriate format. The list of exclusion criteria is as follows.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了筛选出与本调查不相关的出版物，并将出版物集减少到可管理的规模，我们定义了多个选择标准，并将其应用于我们的初步集合。我们纳入调查的主要标准如下：出版物中提出的模型应用深度学习技术（即多层神经网络）来检测异质和非结构化的日志数据。此外，我们还定义了几个排除标准，用于排除相关性低或格式不合适的出版物。排除标准列表如下。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: There is no indication stated in the paper that the presented approach is applicable
    or designed for application with log data. Our survey does not attempt to adopt
    methods from other domains for log data analysis.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文中没有明确说明所提出的方法是否适用于日志数据或专为日志数据分析设计。我们的调查不尝试采用其他领域的方法来进行日志数据分析。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: There exists a more recent publication that presents the same or a similar study.
    The purpose of this criteria is to ensure that the most up-to-date versions of
    specific approaches are analyzed.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 存在更近期的出版物展示了相同或类似的研究。该标准的目的是确保分析最更新的特定方法版本。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The publication only applies an existing approach without novel modifications
    from the original concept.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该出版物仅应用了现有的方法，没有对原始概念进行新的修改。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The publication is in any language other than English.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该出版物使用的语言不是英语。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The publication is not available in electronic form.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该出版物没有电子版。
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The publication is a book, technical report, lecture note, presentation, review,
    or thesis. In these cases, the corresponding conference or journal publications
    were reviewed if possible.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该出版物是书籍、技术报告、讲义、演示文稿、综述或论文。在这些情况下，我们会在可能的情况下审查相关的会议或期刊出版物。
- en: Note that we do not constraint the publications to a specific time range in
    order to avoid missing any older publications that are nonetheless relevant for
    this survey. However, we aim to omit publications of generally lower quality that
    do not meet the minimum scientific standards. We therefore only select publications
    that meet the following quality criteria in addition to our main inclusion criterion.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有将出版物限制在特定的时间范围内，以避免遗漏任何尽管较旧但仍然与本调查相关的出版物。然而，我们旨在排除那些不符合最低科学标准的一般质量较低的出版物。因此，我们仅选择符合以下质量标准的出版物，除了我们的主要纳入标准外。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The purpose of the study and its findings are explicitly stated, e.g., the design
    of a deep learning system for the purpose of anomaly detection is stated as the
    main contribution of the paper.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究的目的及其发现被明确陈述，例如，论文将设计深度学习系统以进行异常检测作为主要贡献。
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The applied deep learning models and their parameters are rigorously described,
    i.e., it is clear to the reader what type of deep learning model was selected
    and how its layout was designed, e.g., how many layers it comprises.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所应用的深度学习模型及其参数被严格描述，即读者可以清楚地知道选择了哪种深度学习模型及其布局设计，例如包含了多少层。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The publication includes a sound evaluation of the presented approach, comprising
    a convincing motivation for the design of the conducted experiments, a comprehensive
    description of the evaluation process and overall setup, an explanation for choosing
    the captured metrics, and a detailed discussion of the gathered results and their
    implications.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该出版物包含对所提出方法的合理评估，包括对实验设计的令人信服的动机、评估过程和整体设置的全面描述、选择捕获指标的解释，以及对收集结果及其意义的详细讨论。
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The data sets used for evaluating the approach are referenced or described.
    This criteria ensures that our survey does not include publications presenting
    potentially misleading findings originating from data sets that are inadequate
    for anomaly detection.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于评估该方法的数据集被引用或描述。这一标准确保我们的调查不包括那些由于数据集不适合异常检测而可能导致误导性发现的出版物。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Visualizations are clear and readable. Incomprehensible presentation of results
    are misleading and lack scientific value.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可视化清晰可读。不易理解的结果展示具有误导性，缺乏科学价值。
- en: Our selection procedure is a two-stage process. First, we reduce the initial
    collections of publications using the inclusion and exclusion criteria based on
    the title and abstract of each paper. After this stage 331 publications remained.
    In the second stage, we carry out the selection using all aforementioned criteria
    based on the contents of each paper. We eventually obtained 62 papers that were
    included in this survey. The following section outlines our method for analyzing
    these publications.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的选择程序是一个两阶段过程。首先，我们根据每篇论文的标题和摘要使用包含和排除标准减少初步文献集合。在这一阶段后，剩下了331篇文献。在第二阶段，我们根据每篇论文的内容使用上述所有标准进行选择。最终我们获得了62篇论文，这些论文被纳入了本次调查。下一节概述了我们分析这些文献的方法。
- en: III-B Reviewed features
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 审核的特征
- en: To ensure that we analyze the selected publications on a common scheme and address
    our research questions, we formulate a list of features that we assess for each
    paper. The following set of questions concerns the applied deep learning (DL)
    model and mode of operation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们在一个共同的方案下分析选定的文献并回答我们的研究问题，我们制定了一份对每篇论文进行评估的特征列表。以下一组问题涉及应用的深度学习（DL）模型和操作模式。
- en: 'DL-1:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DL-1:'
- en: Which deep learning models are used?
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用了哪些深度学习模型？
- en: 'DL-2:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DL-2:'
- en: Which training loss functions are applied?
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用了哪些训练损失函数？
- en: 'DL-3:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DL-3:'
- en: Does the approach support online or incremental⁸⁸8Online or incremental processing
    refers to single-pass procedures where the runtime grows approximately linear
    with the number of processed lines. learning?
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法是否支持在线或增量学习？⁸⁸8在线或增量处理指的是单次处理程序，其中运行时间随着处理的行数的增加大致呈线性增长。
- en: 'DL-4:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DL-4:'
- en: Does training take place in un-, semi-, or supervised manner?
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练是在无监督、半监督还是有监督的方式下进行的？
- en: We then analyze the different ways how the reviewed approaches feed raw log
    data into deep learning models. The following questions therefore address the
    pre-processing (PP) and transformation of logs into numeric vector or matrix representations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们分析了审查方法如何将原始日志数据输入深度学习模型。以下问题因此涉及日志的预处理（PP）和转换为数值向量或矩阵表示。
- en: 'PP-1:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PP-1:'
- en: How are raw logs pre-processed?
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始日志如何进行预处理？
- en: 'PP-2:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PP-2:'
- en: What features are extracted from pre-processed logs?
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从预处理的日志中提取了哪些特征？
- en: 'PP-3:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PP-3:'
- en: How are extracted features represented as vectors?
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提取的特征如何表示为向量？
- en: The next set of questions deals with the anomaly detection (AD). In particular,
    we are interested in the different types of anomalies to understand whether they
    are linked to the features extracted from the raw logs and their representations
    as vectors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下一组问题涉及异常检测（AD）。特别是，我们对不同类型的异常感兴趣，以了解它们是否与从原始日志中提取的特征及其作为向量的表示有关。
- en: 'AD-1:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AD-1:'
- en: What types of anomalies are detected by the approach?
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法检测了哪些类型的异常？
- en: 'AD-2:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AD-2:'
- en: How is the output of the deep neural network⁹⁹9The output layer of the neural
    network comprises one or more nodes with certain numeric values. used for anomaly
    detection?
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度神经网络的输出如何用于异常检测？⁹⁹9神经网络的输出层由一个或多个具有特定数值的节点组成。
- en: 'AD-3:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AD-3:'
- en: How are anomalies differentiated from normal data samples?
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何将异常与正常数据样本区分开？
- en: Utilizing openly accessible data sets for evaluations as well as publishing
    source code alongside papers is not only good scientific standard but also essential
    for others to validate presented results and carry out comparisons. The last set
    of questions therefore concerns evaluation and reproducibility (ER), in particular,
    employed evaluation metrics as well as availability of data sets and source code.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 利用公开的数据集进行评估以及在论文中发布源代码不仅是良好的科学标准，也是他人验证结果和进行比较的关键。因此，最后一组问题涉及评估和可重复性（ER），特别是使用的评估指标、数据集和源代码的可用性。
- en: 'ER-1:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ER-1:'
- en: What log data sets are used for evaluating the approach?
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于评估该方法的日志数据集有哪些？
- en: 'ER-2:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ER-2:'
- en: What evaluation metrics are employed?
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用了哪些评估指标？
- en: 'ER-3:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ER-3:'
- en: Does the evaluation consider runtime performance measurements?
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估是否考虑了运行时性能测量？
- en: 'ER-4:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ER-4:'
- en: What approaches are used as benchmarks?
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用了哪些方法作为基准？
- en: 'ER-5:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ER-5:'
- en: Are the used data sets publicly available?
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用的数据集是否公开？
- en: 'ER-6:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ER-6:'
- en: Is the source code of the approach publicly available?
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法的源代码是否公开？
- en: 'All aforementioned questions were assessed for each publication individually.
    The resulting feature matrix is presented in Table [II](#S4.T2 "TABLE II ‣ IV-A2
    Citations ‣ IV-A Bibliometrics ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey") in the following section and serves as the basis
    for our analyses and discussions.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '所有上述问题都是对每篇文献进行单独评估的。结果特征矩阵在接下来的表格[II](#S4.T2 "TABLE II ‣ IV-A2 Citations ‣
    IV-A Bibliometrics ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in
    Log Data: A Survey")中呈现，并作为我们分析和讨论的基础。'
- en: IV Survey Results
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 调查结果
- en: This section provides the assessments of all reviewed publications with respect
    to the features outlined in the previous section. We first provide some general
    information on the meta-data of publications before going over each reviewed feature
    in detail.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对所有评审文献的评估，涉及到前一节中概述的特征。我们首先提供一些关于文献元数据的总体信息，然后详细讨论每个评审特征。
- en: IV-A Bibliometrics
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 引文计量
- en: This section provides an overview of the distribution of publications per year
    as well as their citation counts.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了每年出版文献数量及其引文计数的概述。
- en: IV-A1 Publications per Year
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 每年出版的文献数量
- en: 'Deep learning for anomaly detection in log data is a relatively new research
    field that has increasingly gained traction in the last years. Accordingly, a
    majority of the publications in this research area have only been published in
    the last two to three years. Figure [2](#S4.F2 "Figure 2 ‣ IV-A1 Publications
    per Year ‣ IV-A Bibliometrics ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey") shows an overview of the publication years of
    all publications reviewed for this survey. As expected, 58 out of the 62 considered
    publications were published in 2019 or later. As the search for relevant literature
    was carried out in the beginning of 2022, only two publications from that year
    are included. However, we expect to see an even higher number of publications
    in 2022 and beyond following the overall trend visible in the plot.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '基于深度学习的日志数据异常检测是一个相对较新的研究领域，在近年来逐渐获得了关注。因此，该研究领域的大多数文献仅在过去两到三年内发表。图[2](#S4.F2
    "Figure 2 ‣ IV-A1 Publications per Year ‣ IV-A Bibliometrics ‣ IV Survey Results
    ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")显示了所有评审文献的出版年份概览。如预期的那样，62篇文献中有58篇在2019年或以后发表。由于相关文献的搜索是在2022年初进行的，因此仅包括了该年两篇文献。然而，我们预计在2022年及以后会有更多的文献出现，符合图中显示的总体趋势。'
- en: '![Refer to caption](img/a4cc4e821c314f2502f9cd3499408478.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a4cc4e821c314f2502f9cd3499408478.png)'
- en: 'Figure 2: Distribution of the number of publications per year.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：每年出版文献数量的分布。
- en: IV-A2 Citations
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 引用
- en: 'Citation counts are a common indicator to assess the relevance and influence
    of publications. We therefore state the top six publication with the highest citation
    counts (according to Google Scholar) in Table [I](#S4.T1 "TABLE I ‣ IV-A2 Citations
    ‣ IV-A Bibliometrics ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection
    in Log Data: A Survey"). As of January 2023, the paper presenting DeepLog by Du
    et al. [[24](#bib.bib24)] that was published in 2017 has the highest citation
    count and is arguably the most influential of all reviewed publications as they
    were the first to propose an approach based on deep learning that enables detection
    of anomalous event sequences in log data. Several of the subsequently published
    papers rely on the groundwork of DeepLog and it is therefore fair to assume that
    this paper is at least to some degree responsible for the increase of relevant
    publications from 2019 and onward that is visible in Fig. [2](#S4.F2 "Figure 2
    ‣ IV-A1 Publications per Year ‣ IV-A Bibliometrics ‣ IV Survey Results ‣ Deep
    Learning for Anomaly Detection in Log Data: A Survey").'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '引文计数是评估文献相关性和影响力的常见指标。因此，我们在表格[I](#S4.T1 "TABLE I ‣ IV-A2 Citations ‣ IV-A
    Bibliometrics ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log
    Data: A Survey")中列出了引用次数最高的六篇文献（根据Google Scholar）。截至2023年1月，由Du等人于2017年发表的论文展示了DeepLog，该论文拥有最高的引用次数，并且可以说是所有评审文献中最具影响力的，因为他们首次提出了一种基于深度学习的方法，能够检测日志数据中的异常事件序列。随后发表的几篇论文都依赖于DeepLog的基础工作，因此可以公平地认为这篇论文在2019年及以后相关文献数量的增加中起到了一定的作用，这在图[2](#S4.F2
    "Figure 2 ‣ IV-A1 Publications per Year ‣ IV-A Bibliometrics ‣ IV Survey Results
    ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")中是可以看出的。'
- en: 'Note that the publication by Yang et al. [[25](#bib.bib25)] predates DeepLog
    [[24](#bib.bib24)] but has a significantly lower citation count. The main reason
    for this is that the paper focuses on the analysis of tokens in single log events,
    a topic that received far less attention in subsequent research than the analysis
    of event sequences. This differentiation as well as assessments for all other
    features stated in Sect. [III-B](#S3.SS2 "III-B Reviewed features ‣ III Survey
    Method ‣ Deep Learning for Anomaly Detection in Log Data: A Survey") are presented
    in Table [II](#S4.T2 "TABLE II ‣ IV-A2 Citations ‣ IV-A Bibliometrics ‣ IV Survey
    Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，Yang 等人的出版物 [[25](#bib.bib25)] 早于 DeepLog [[24](#bib.bib24)]，但引用次数显著较少。主要原因是该论文关注于单个日志事件中的令牌分析，而这一主题在后续研究中受到了远低于事件序列分析的关注。此差异以及第
    [III-B](#S3.SS2 "III-B Reviewed features ‣ III Survey Method ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey") 节中列出的所有其他特征的评估已在表 [II](#S4.T2 "TABLE
    II ‣ IV-A2 Citations ‣ IV-A Bibliometrics ‣ IV Survey Results ‣ Deep Learning
    for Anomaly Detection in Log Data: A Survey") 中展示。'
- en: 'TABLE I: Top six most cited publications'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 六大最被引用的出版物'
- en: '| Citations | Approach | Year | Authors | Paper Title |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 引用次数 | 方法 | 年份 | 作者 | 论文标题 |'
- en: '| 963 | DeepLog | 2017 | [[24](#bib.bib24)] | DeepLog: Anomaly Detection and
    Diagnosis from System Logs through Deep Learning |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 963 | DeepLog | 2017 | [[24](#bib.bib24)] | DeepLog: 基于深度学习的系统日志异常检测与诊断 |'
- en: '| 227 | LogRobust | 2019 | [[19](#bib.bib19)] | Robust Log-Based Anomaly Detection
    on Unstable Log Data |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 227 | LogRobust | 2019 | [[19](#bib.bib19)] | 基于日志的鲁棒异常检测 |'
- en: '| 209 | LogAnomaly | 2019 | [[26](#bib.bib26)] | LogAnomaly: Unsupervised Detection
    of Sequential and Quantitative Anomalies in Unstructured Logs |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 209 | LogAnomaly | 2019 | [[26](#bib.bib26)] | LogAnomaly: 无监督检测无结构日志中的序列和定量异常
    |'
- en: '| 92 | - | 2018 | [[27](#bib.bib27)] | Detecting Anomaly in Big Data System
    Logs Using Convolutional Neural Network |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 92 | - | 2018 | [[27](#bib.bib27)] | 使用卷积神经网络检测大数据系统日志中的异常 |'
- en: '| 53 | Logsy | 2020 | [[28](#bib.bib28)] | Self-Attentive Classification-Based
    Anomaly Detection in Unstructured Logs |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 53 | Logsy | 2020 | [[28](#bib.bib28)] | 基于自注意力的无结构日志异常检测 |'
- en: '| 45 | LogBERT | 2021 | [[29](#bib.bib29)] | LogBERT: Log Anomaly Detection
    via BERT |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 45 | LogBERT | 2021 | [[29](#bib.bib29)] | LogBERT: 基于 BERT 的日志异常检测 |'
- en: 'TABLE II: Survey results. DL-1: Multi-Layer Perceptron (MLP), Convolutional
    Neural Network (CNN), Recursive Neural Network (RNN), Autoencoder (AE), Generative
    Adversarial Network (GAN), Transformer (TF), Attention mechanism (AT), Graph Neural
    Network (GNN), Evolving Granular Neural Network (EGNN); DL-2: Cross-Entropy (CE),
    Hyper-Sphere (HS), Mean Squared Error (MSE), Kullback-Leibler Divergence (KL),
    Marginal Likelihood (ML), Custom Loss Function (CF), Adversarial Training (AT),
    Not Available (NA); DL-3: Online (ON), Offline (OFF); DL-4: Supervised (SUP),
    Semi-supervised (SEMI), Unsupervised (UN); PP-1: Log key (KEY), Token (TOK), Combination
    (COM); PP-2: Token Sequence (TS), Token Count (TC), Event Sequence (ES), Event
    Count (EC), Parameter (PA), Event Interval Time (EI); PP-3: Event ID sequence
    (ID), Count Vector (CV), Statistical Feature Vector (FV), Semantic Vector (SV),
    Positional Embedding (PE), One-Hot Encoding (OH), Embedding Layer/Matrix (EL),
    Deep Encoded Embedding (DE), Parameter Vector (PV), Time Embedding (TE), Graph
    (G), Transfer Matrix (TM); AD-1: Outlier (OUT), Sequential (SEQ), Frequency (FREQ),
    Statistical (STAT); AD-2: Binary Classification (BIN), Input Vector Transformations
    (TRA), Reconstruction Error (RE), Multi-class Classification (MC), Probability
    Distribution (PRD), Numeric Vector (VEC); AD-3: Label (LAB), Threshold (THR),
    Highest Probabilities (TOP).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 调查结果。DL-1: 多层感知器 (MLP)，卷积神经网络 (CNN)，递归神经网络 (RNN)，自编码器 (AE)，生成对抗网络 (GAN)，变换器
    (TF)，注意机制 (AT)，图神经网络 (GNN)，进化粒度神经网络 (EGNN)；DL-2: 交叉熵 (CE)，超球面 (HS)，均方误差 (MSE)，Kullback-Leibler
    散度 (KL)，边际似然 (ML)，自定义损失函数 (CF)，对抗训练 (AT)，不可用 (NA)；DL-3: 在线 (ON)，离线 (OFF)；DL-4:
    有监督 (SUP)，半监督 (SEMI)，无监督 (UN)；PP-1: 日志关键 (KEY)，令牌 (TOK)，组合 (COM)；PP-2: 令牌序列 (TS)，令牌计数
    (TC)，事件序列 (ES)，事件计数 (EC)，参数 (PA)，事件间隔时间 (EI)；PP-3: 事件 ID 序列 (ID)，计数向量 (CV)，统计特征向量
    (FV)，语义向量 (SV)，位置嵌入 (PE)，独热编码 (OH)，嵌入层/矩阵 (EL)，深度编码嵌入 (DE)，参数向量 (PV)，时间嵌入 (TE)，图
    (G)，转移矩阵 (TM)；AD-1: 异常值 (OUT)，序列 (SEQ)，频率 (FREQ)，统计 (STAT)；AD-2: 二分类 (BIN)，输入向量变换
    (TRA)，重构误差 (RE)，多类分类 (MC)，概率分布 (PRD)，数值向量 (VEC)；AD-3: 标签 (LAB)，阈值 (THR)，最高概率 (TOP)。'
- en: '| Approach | DL-1 | DL-2 | DL-3 | DL-4 | PP-1 | PP-2 | PP-3 | AD-1 | AD-2 |
    AD-3 | ER-6 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | DL-1 | DL-2 | DL-3 | DL-4 | PP-1 | PP-2 | PP-3 | AD-1 | AD-2 | AD-3
    | ER-6 |'
- en: '| Baril et al. [[30](#bib.bib30)] (NoTIL) | RNN | CE | OFF | SEMI | KEY | ES,
    EC | CV | FREQ | VEC | THR | NO |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Baril 等人 [[30](#bib.bib30)] (NoTIL) | RNN | CE | OFF | SEMI | KEY | ES, EC
    | CV | FREQ | VEC | THR | NO |'
- en: '| Bursic et al. [[31](#bib.bib31)] | RNN, AE | MSE | OFF | UN | TOK | PA, TS
    | DE | OUT | RE | THR | NO |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Bursic 等人 [[31](#bib.bib31)] | RNN, AE | MSE | OFF | UN | TOK | PA, TS |
    DE | OUT | RE | THR | NO |'
- en: '| Catillo et al. [[32](#bib.bib32)] (AutoLog) | AE | MSE | ON | SEMI | COM
    | STAT, TC | FV | FREQ | RE | THR | YES |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Catillo 等人 [[32](#bib.bib32)] (AutoLog) | AE | MSE | ON | SEMI | COM | STAT,
    TC | FV | FREQ | RE | THR | YES |'
- en: '| Cheansunan et al. [[33](#bib.bib33)] | CNN | NA | OFF | SEMI | KEY | ES |
    EL | SEQ | PRD | TOP | NO |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Cheansunan 等人 [[33](#bib.bib33)] | CNN | NA | OFF | SEMI | KEY | ES | EL
    | SEQ | PRD | TOP | NO |'
- en: '| Chen et al. [[34](#bib.bib34)] | RNN, CNN | NA | OFF | SEMI | KEY | ES, EC
    | SV, CV | SEQ, FREQ | VEC, PRD | TOP | NO |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 [[34](#bib.bib34)] | RNN, CNN | NA | OFF | SEMI | KEY | ES, EC |
    SV, CV | SEQ, FREQ | VEC, PRD | TOP | NO |'
- en: '| Chen et al. [[35](#bib.bib35)] (LogTransfer) | RNN, CNN | NA | OFF | SUP
    | KEY | ES | SV | SEQ | BIN | LAB | YES |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 [[35](#bib.bib35)] (LogTransfer) | RNN, CNN | NA | OFF | SUP | KEY
    | ES | SV | SEQ | BIN | LAB | YES |'
- en: '| Decker et al. [[36](#bib.bib36)] | EGNN | CF | OFF | SUP | KEY | EC, STAT
    | FV | FREQ | MC | THR, LAB | NO |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Decker 等人 [[36](#bib.bib36)] | EGNN | CF | OFF | SUP | KEY | EC, STAT | FV
    | FREQ | MC | THR, LAB | NO |'
- en: '| Du et al. [[24](#bib.bib24)] (DeepLog) | RNN | CE, MSE | ON | SEMI | KEY
    | ES, PA, EI | PV, OH | SEQ | VEC, PRD | THR, TOP | RE |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Du 等人 [[24](#bib.bib24)] (DeepLog) | RNN | CE, MSE | ON | SEMI | KEY | ES,
    PA, EI | PV, OH | SEQ | VEC, PRD | THR, TOP | RE |'
- en: '| Du et al. [[37](#bib.bib37)] (LogAttention) | TF, AM | NA | OFF | SUP | KEY
    | ES | SV | SEQ | BIN | LAB | NO |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Du 等人 [[37](#bib.bib37)] (LogAttention) | TF, AM | NA | OFF | SUP | KEY |
    ES | SV | SEQ | BIN | LAB | NO |'
- en: '| Farzad et al. [[38](#bib.bib38)] | RNN, AE | CE | OFF | SUP | TOK | TS |
    EL | SEQ | BIN | LAB | NO |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Farzad 等人 [[38](#bib.bib38)] | RNN, AE | CE | OFF | SUP | TOK | TS | EL |
    SEQ | BIN | LAB | NO |'
- en: '| Farzad et al. [[39](#bib.bib39)] | RNN, AE, CNN, GAN | CE | OFF | SUP | TOK
    | TS | EL | OUT | BIN | LAB | NO |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Farzad 等人 [[39](#bib.bib39)] | RNN, AE, CNN, GAN | CE | OFF | SUP | TOK |
    TS | EL | OUT | BIN | LAB | NO |'
- en: '| Farzad et al. [[40](#bib.bib40)] | RNN | CE | OFF | SUP | TOK | TS | EL |
    OUT | BIN | LAB | NO |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Farzad 等人 [[40](#bib.bib40)] | RNN | CE | OFF | SUP | TOK | TS | EL | OUT
    | BIN | LAB | NO |'
- en: '| Gu et al. [[41](#bib.bib41)] | RNN, AM | CE | OFF | SEMI | KEY | ES | SV
    | SEQ | PRD | TOP | NO |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Gu 等人 [[41](#bib.bib41)] | RNN, AM | CE | OFF | SEMI | KEY | ES | SV | SEQ
    | PRD | TOP | NO |'
- en: '| Guo et al. [[42](#bib.bib42)] (FLOGCNN) | CNN | CE | OFF | SUP | COM | TS
    | SV | SEQ | BIN | LAB | NO |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Guo 等人 [[42](#bib.bib42)] (FLOGCNN) | CNN | CE | OFF | SUP | COM | TS | SV
    | SEQ | BIN | LAB | NO |'
- en: '| Guo et al. [[29](#bib.bib29)] (LogBERT) | TF, AM | CE, HS | OFF | SEMI |
    KEY | ES | PE, EL | SEQ | PRD | TOP | YES |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Guo 等人 [[29](#bib.bib29)] (LogBERT) | TF, AM | CE, HS | OFF | SEMI | KEY
    | ES | PE, EL | SEQ | PRD | TOP | YES |'
- en: '| Guo et al. [[43](#bib.bib43)] (TransLog) | TF, AM | CE | OFF | SUP | KEY
    | ES | SV | SEQ | BIN | LAB | NO |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Guo 等人 [[43](#bib.bib43)] (TransLog) | TF, AM | CE | OFF | SUP | KEY | ES
    | SV | SEQ | BIN | LAB | NO |'
- en: '| Han et al. [[44](#bib.bib44)] (LogTAD) | RNN, GAN | HS, CF, AT | OFF | UN
    | KEY | ES | SV | SEQ | TRA | THR | YES |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Han 等人 [[44](#bib.bib44)] (LogTAD) | RNN, GAN | HS, CF, AT | OFF | UN | KEY
    | ES | SV | SEQ | TRA | THR | YES |'
- en: '| Hashemi et al. [[45](#bib.bib45)] (OneLog) | CNN | CE | OFF | SUP | TOK |
    ES | DE | SEQ | BIN | LAB | YES |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Hashemi 等人 [[45](#bib.bib45)] (OneLog) | CNN | CE | OFF | SUP | TOK | ES
    | DE | SEQ | BIN | LAB | YES |'
- en: '| Hirakawa et al. [[46](#bib.bib46)] | CNN, TF | CF | OFF | UN | TOK | ES |
    SV | OUT | BIN | THR | NO |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Hirakawa 等人 [[46](#bib.bib46)] | CNN, TF | CF | OFF | UN | TOK | ES | SV
    | OUT | BIN | THR | NO |'
- en: '| Huang et al. [[47](#bib.bib47)] (HitAnomaly) | TF, AM | CE | OFF | SUP |
    KEY | ES, PA | SV, PV | SEQ | BIN | LAB | NO |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Huang 等人 [[47](#bib.bib47)] (HitAnomaly) | TF, AM | CE | OFF | SUP | KEY
    | ES, PA | SV, PV | SEQ | BIN | LAB | NO |'
- en: '| Le et al. [[48](#bib.bib48)] (NeuralLog) | TF | CE | OFF | SUP | TOK | ES
    | SV, PE | SEQ | BIN | LAB | YES |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Le 等人 [[48](#bib.bib48)] (NeuralLog) | TF | CE | OFF | SUP | TOK | ES | SV,
    PE | SEQ | BIN | LAB | YES |'
- en: '| Li et al. [[49](#bib.bib49)] (LogSpy) | CNN, MLP, AM | CE | OFF | SUP | KEY
    | STAT | DE, EL | STAT | BIN | LAB | NO |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 [[49](#bib.bib49)] (LogSpy) | CNN, MLP, AM | CE | OFF | SUP | KEY |
    STAT | DE, EL | STAT | BIN | LAB | NO |'
- en: '| Li et al. [[50](#bib.bib50)] (SwissLog) | RNN, AM | CE | OFF | SUP | KEY
    | ES, EI | SV, TE | STAT, SEQ | BIN | LAB | NO |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 [[50](#bib.bib50)] (SwissLog) | RNN, AM | CE | OFF | SUP | KEY | ES,
    EI | SV, TE | STAT, SEQ | BIN | LAB | NO |'
- en: '| Liu et al. [[51](#bib.bib51)] (LogNADS) | RNN | MSE | OFF | SUP | KEY | ES,
    PA | SV, PV | SEQ | BIN | LAB | NO |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 [[51](#bib.bib51)] (LogNADS) | RNN | MSE | OFF | SUP | KEY | ES, PA
    | SV, PV | SEQ | BIN | LAB | NO |'
- en: '| Lu et al. [[27](#bib.bib27)] | CNN | NA | OFF | SUP | KEY | ES | EL | SEQ
    | BIN | LAB | NO |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Lu 等人 [[27](#bib.bib27)] | CNN | NA | OFF | SUP | KEY | ES | EL | SEQ | BIN
    | LAB | NO |'
- en: '| Lv et al. [[52](#bib.bib52)] (ConAnomaly) | RNN | NA | OFF | SUP | KEY |
    ES | SV | SEQ | BIN | THR | NO |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Lv 等人 [[52](#bib.bib52)] (ConAnomaly) | RNN | NA | OFF | SUP | KEY | ES |
    SV | SEQ | BIN | THR | NO |'
- en: '| Meng et al. [[26](#bib.bib26)] (LogAnomaly) | RNN | NA | OFF | SEMI | KEY
    | ES, EC | SV, CV | SEQ, FREQ | VEC, PRD | TOP | RE |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Meng et al. [[26](#bib.bib26)] (LogAnomaly) | RNN | NA | OFF | SEMI | KEY
    | ES, EC | SV, CV | SEQ, FREQ | VEC, PRD | TOP | RE |'
- en: '| Nedelkoski et al. [[28](#bib.bib28)] (Logsy) | TF | HS | OFF | UN | TOK |
    TS | PE, EL | OUT | TRA | THR | RE |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Nedelkoski et al. [[28](#bib.bib28)] (Logsy) | TF | HS | OFF | UN | TOK |
    TS | PE, EL | OUT | TRA | THR | RE |'
- en: '| Otomo et al. [[53](#bib.bib53)] | AE | KL, ML, CF | OFF | SEMI | KEY | ES,
    EC | CV, OH | FREQ | TRA | THR | NO |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Otomo et al. [[53](#bib.bib53)] | AE | KL, ML, CF | OFF | SEMI | KEY | ES,
    EC | CV, OH | FREQ | TRA | THR | NO |'
- en: '| Ott et al. [[54](#bib.bib54)] | RNN | CE, MSE | OFF | SEMI | KEY | ES | SV
    | SEQ | VEC, PRD | THR, TOP | NO |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Ott et al. [[54](#bib.bib54)] | RNN | CE, MSE | OFF | SEMI | KEY | ES | SV
    | SEQ | VEC, PRD | THR, TOP | NO |'
- en: '| Patil et al. [[55](#bib.bib55)] | RNN | CE | OFF | SUP | KEY | ES | OH |
    SEQ | BIN | LAB | NO |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Patil et al. [[55](#bib.bib55)] | RNN | CE | OFF | SUP | KEY | ES | OH |
    SEQ | BIN | LAB | NO |'
- en: '| Qian et al. [[56](#bib.bib56)] (VeLog) | AE | CF | ON | SEMI | KEY | ES,
    EC | ID, CV | SEQ, FREQ | RE | THR | NO |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Qian et al. [[56](#bib.bib56)] (VeLog) | AE | CF | ON | SEMI | KEY | ES,
    EC | ID, CV | SEQ, FREQ | RE | THR | NO |'
- en: '| Studiawan et al. [[57](#bib.bib57)] | AE | MSE | OFF | SEMI | KEY | EC, STAT,
    EI, TC | FV | STAT, FREQ | RE | THR | NO |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Studiawan et al. [[57](#bib.bib57)] | AE | MSE | OFF | SEMI | KEY | EC, STAT,
    EI, TC | FV | STAT, FREQ | RE | THR | NO |'
- en: '| Studiawan et al. [[58](#bib.bib58)] (pylogsentiment) | RNN | CE | OFF | SUP
    | TOK | TS | SV | OUT | BIN | LAB | YES |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Studiawan et al. [[58](#bib.bib58)] (pylogsentiment) | RNN | CE | OFF | SUP
    | TOK | TS | SV | OUT | BIN | LAB | YES |'
- en: '| Sun et al. [[59](#bib.bib59)] (AllContext) | RNN, AM | CE | OFF | SUP | KEY
    | ES | SV | OUT, SEQ | BIN, MC | LAB | NO |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Sun et al. [[59](#bib.bib59)] (AllContext) | RNN, AM | CE | OFF | SUP | KEY
    | ES | SV | OUT, SEQ | BIN, MC | LAB | NO |'
- en: '| Sundqvist et al. [[60](#bib.bib60)] (BoostLog) | RNN | NA | ON | SEMI | KEY
    | ES | ID | SEQ | PRD | THR | NO |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Sundqvist et al. [[60](#bib.bib60)] (BoostLog) | RNN | NA | ON | SEMI | KEY
    | ES | ID | SEQ | PRD | THR | NO |'
- en: '| Syngal et al. [[61](#bib.bib61)] | RNN, AE | CE | OFF | SUP | TOK | ES, EC
    | CV, OH | SEQ, FREQ | RE | THR | NO |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Syngal et al. [[61](#bib.bib61)] | RNN, AE | CE | OFF | SUP | TOK | ES, EC
    | CV, OH | SEQ, FREQ | RE | THR | NO |'
- en: '| Wadekar et al. [[62](#bib.bib62)] | AE | CE, KL, ML | OFF | UN | KEY | ES
    | OH | SEQ | BIN | THR | NO |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Wadekar et al. [[62](#bib.bib62)] | AE | CE, KL, ML | OFF | UN | KEY | ES
    | OH | SEQ | BIN | THR | NO |'
- en: '| Wan et al. [[63](#bib.bib63)] (GLAD-PAW) | GNN | CE | OFF | UN | KEY | ES
    | G | SEQ | PRD | TOP | NO |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Wan et al. [[63](#bib.bib63)] (GLAD-PAW) | GNN | CE | OFF | UN | KEY | ES
    | G | SEQ | PRD | TOP | NO |'
- en: '| Wang et al. [[64](#bib.bib64)] | RNN | NA | OFF | SUP | TOK | TS | SV | OUT
    | BIN | LAB | NO |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[64](#bib.bib64)] | RNN | NA | OFF | SUP | TOK | TS | SV | OUT
    | BIN | LAB | NO |'
- en: '| Wang et al. [[1](#bib.bib1)] (CATLog) | AE, MLP, TF | CE, CF | OFF | SUP
    | KEY | ES, TC | SV, DE, CV | SEQ, FREQ | BIN | LAB | NO |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[1](#bib.bib1)] (CATLog) | AE, MLP, TF | CE, CF | OFF | SUP
    | KEY | ES, TC | SV, DE, CV | SEQ, FREQ | BIN | LAB | NO |'
- en: '| Wang et al. [[65](#bib.bib65)] (LightLog) | CNN | CE | OFF | SUP | KEY |
    ES | SV | SEQ | BIN | LAB | YES |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[65](#bib.bib65)] (LightLog) | CNN | CE | OFF | SUP | KEY |
    ES | SV | SEQ | BIN | LAB | YES |'
- en: '| Wang et al. [[66](#bib.bib66)] (OC4Seq) | RNN | HS | OFF | UN | KEY | ES
    | EL | SEQ | TRA | THR | YES |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[66](#bib.bib66)] (OC4Seq) | RNN | HS | OFF | UN | KEY | ES
    | EL | SEQ | TRA | THR | YES |'
- en: '| Wibisono et al. [[67](#bib.bib67)] | TF, AM | NA | OFF | SEMI | KEY | ES
    | OH | SEQ | PRD | TOP | NO |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Wibisono et al. [[67](#bib.bib67)] | TF, AM | NA | OFF | SEMI | KEY | ES
    | OH | SEQ | PRD | TOP | NO |'
- en: '| Wittkopp et al. [[68](#bib.bib68)] (A2Log) | TF, AM | CF | OFF | UN | TOK
    | TS | SV | OUT | BIN, TRA | THR | NO |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Wittkopp et al. [[68](#bib.bib68)] (A2Log) | TF, AM | CF | OFF | UN | TOK
    | TS | SV | OUT | BIN, TRA | THR | NO |'
- en: '| Xi et al. [[69](#bib.bib69)] | RNN, AM | CE | OFF | SEMI | KEY | ES | SV
    | SEQ | PRD | TOP | NO |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Xi et al. [[69](#bib.bib69)] | RNN, AM | CE | OFF | SEMI | KEY | ES | SV
    | SEQ | PRD | TOP | NO |'
- en: '| Xia et al. [[70](#bib.bib70)] (LogGAN) | RNN, GAN | AT | ON | SEMI | KEY
    | ES | OH | SEQ | PRD | THR | NO |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Xia et al. [[70](#bib.bib70)] (LogGAN) | RNN, GAN | AT | ON | SEMI | KEY
    | ES | OH | SEQ | PRD | THR | NO |'
- en: '| Xiao et al. [[71](#bib.bib71)] | RNN, CNN | CE | OFF | SEMI | KEY | ES |
    EL | SEQ | PRD | TOP | NO |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Xiao et al. [[71](#bib.bib71)] | RNN, CNN | CE | OFF | SEMI | KEY | ES |
    EL | SEQ | PRD | TOP | NO |'
- en: '| Xie et al. [[72](#bib.bib72)] (ATT-GRU) | RNN, AM | MSE | OFF | SEMI | KEY
    | ES, PA, EI | PV, EL | SEQ | VEC, PRD | THR, TOP | NO |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Xie et al. [[72](#bib.bib72)] (ATT-GRU) | RNN, AM | MSE | OFF | SEMI | KEY
    | ES, PA, EI | PV, EL | SEQ | VEC, PRD | THR, TOP | NO |'
- en: '| Yang et al. [[25](#bib.bib25)] | RNN | CE | OFF | SEMI | TOK | TS | SV |
    OUT | VEC | THR | NO |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. [[25](#bib.bib25)] | RNN | CE | OFF | SEMI | TOK | TS | SV |
    OUT | VEC | THR | NO |'
- en: '| Yang et al. [[73](#bib.bib73)] (nLSALog) | RNN, AM | CE | OFF | SEMI | KEY
    | ES | EL | SEQ | PRD | TOP | NO |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. [[73](#bib.bib73)] (nLSALog) | RNN, AM | CE | OFF | SEMI | KEY
    | ES | EL | SEQ | PRD | TOP | NO |'
- en: '| Yang et al. [[74](#bib.bib74)] (PLELog) | RNN | NA | OFF | SEMI | KEY | ES
    | SV | SEQ | BIN | LAB | NO |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. [[74](#bib.bib74)] (PLELog) | RNN | NA | OFF | SEMI | KEY | ES
    | SV | SEQ | BIN | LAB | NO |'
- en: '| Yen et al. [[75](#bib.bib75)] (CausalConvLSTM) | RNN, CNN | CE | ON | SEMI
    | KEY | ES | OH | SEQ | PRD | TOP | NO |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Yen et al. [[75](#bib.bib75)] (CausalConvLSTM) | RNN, CNN | CE | ON | SEMI
    | KEY | ES | OH | SEQ | PRD | TOP | NO |'
- en: '| Yin et al. [[76](#bib.bib76)] (LogC) | RNN | CE | OFF | SEMI | KEY | ES,
    PA | OH | SEQ | PRD | TOP | NO |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Yin et al. [[76](#bib.bib76)] (LogC) | RNN | CE | OFF | SEMI | KEY | ES,
    PA | OH | SEQ | PRD | TOP | NO |'
- en: '| Yu et al. [[77](#bib.bib77)] | RNN | CE | OFF | SEMI | KEY | ES, EC | SV,
    ID, CV | SEQ, FREQ | PRD | TOP | NO |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Yu et al. [[77](#bib.bib77)] | RNN | CE | OFF | SEMI | KEY | ES, EC | SV,
    ID, CV | SEQ, FREQ | PRD | TOP | NO |'
- en: '| Zhang et al. [[19](#bib.bib19)] (LogRobust) | RNN, AM | CE | OFF | SUP |
    KEY | ES | SV | SEQ | BIN | LAB | RE |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. [[19](#bib.bib19)] (LogRobust) | RNN, AM | CE | OFF | SUP |
    KEY | ES | SV | SEQ | BIN | LAB | RE |'
- en: '| Zhang et al. [[78](#bib.bib78)] (LogAttn) | AE, CNN, AM | CE | OFF | SEMI
    | TOK | ES, EC | SV, CV | SEQ, FREQ | RE | THR | NO |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. [[78](#bib.bib78)] (LogAttn) | AE, CNN, AM | CE | OFF | SEMI
    | TOK | ES, EC | SV, CV | SEQ, FREQ | RE | THR | NO |'
- en: '| Zhang et al. [[79](#bib.bib79)] (LSADNET) | CNN, TF | CE | ON | SEMI | KEY
    | ES, STAT | SV, TM | STAT, SEQ | PRD | TOP | NO |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. [[79](#bib.bib79)] (LSADNET) | CNN, TF | CE | ON | SEMI | KEY
    | ES, STAT | SV, TM | STAT, SEQ | PRD | TOP | NO |'
- en: '| Zhang et al. [[80](#bib.bib80)] (SentiLog) | RNN | NA | OFF | SUP | TOK |
    TS | SV | OUT | BIN | LAB | NO |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. [[80](#bib.bib80)] (SentiLog) | RNN | NA | OFF | SUP | TOK |
    TS | SV | OUT | BIN | LAB | NO |'
- en: '| Zhao et al. [[81](#bib.bib81)] (Trine) | TF, GAN | NA | OFF | SEMI | TOK
    | ES | SV, DE, PE | SEQ | BIN | LAB | NO |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. [[81](#bib.bib81)] (Trine) | TF, GAN | NA | OFF | SEMI | TOK
    | ES | SV, DE, PE | SEQ | BIN | LAB | NO |'
- en: '| Zhou et al. [[82](#bib.bib82)] (LogSayer) | RNN, CNN | NA | ON | SUP | KEY
    | EC, STAT | FV | STAT | BIN | LAB | NO |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Zhou et al. [[82](#bib.bib82)] (LogSayer) | RNN, CNN | NA | ON | SUP | KEY
    | EC, STAT | FV | STAT | BIN | LAB | NO |'
- en: '| Zhu et al. [[83](#bib.bib83)] (LogNL) | RNN | NA | OFF | SEMI | KEY | ES,
    PA, EI | SV, PV | SEQ | VEC, PRD | THR | NO |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Zhu et al. [[83](#bib.bib83)] (LogNL) | RNN | NA | OFF | SEMI | KEY | ES,
    PA, EI | SV, PV | SEQ | VEC, PRD | THR | NO |'
- en: IV-B Deep Learning Techniques
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 深度学习技术
- en: This section provides an overview of the properties of deep learning models
    applied in reviewed publications.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了在回顾的文献中应用的深度学习模型的特性。
- en: IV-B1 Deep Learning Models
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 深度学习模型
- en: There are many different types of deep learning models (DL-1) that are suitable
    to be used for anomaly detection in log data [[15](#bib.bib15), [84](#bib.bib84)].
    The most basic form of a deep learning neural network is that of a Multi-Layer
    Perceptron (MLP), where all layers in the network are fully connected. Due to
    their simplicity, their classification accuracies are usually outperformed by
    other deep learning models that are specifically designed to capture common characteristics
    present in sequential data. Accordingly, they are rarely considered in the reviewed
    literature and only occur in combination with other deep learning models or as
    supplementary attention mechanisms [[1](#bib.bib1), [49](#bib.bib49)].
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同类型的深度学习模型（DL-1）适用于日志数据中的异常检测[[15](#bib.bib15), [84](#bib.bib84)]。最基本的深度学习神经网络形式是多层感知器（MLP），其中网络中的所有层都是完全连接的。由于其简单性，其分类准确性通常被专门设计用于捕捉序列数据中常见特征的其他深度学习模型所超越。因此，它们在回顾的文献中很少被考虑，仅在与其他深度学习模型结合使用或作为补充注意机制时出现[[1](#bib.bib1),
    [49](#bib.bib49)]。
- en: Convolutional Neural Networks (CNN) extend upon the architecture of MLPs by
    inserting convolutional and max pooling layers within the hidden layers. These
    layers enable that the neural networks capture more abstract features of the input
    data and at the same time reduce the input dimensions. This has proven especially
    effective when classifying 2-dimensional input data from images, where features
    such as lines are learned independent from their exact location in the image.
    This functionality is transferred to log data by arranging the log keys within
    a matrix so that the relationships between events, i.e., their temporal dependencies,
    are captured by the network [[27](#bib.bib27)]. There also exist several approaches
    that rely on specific types of CNNs, such as temporal convolutional networks (TCN)
    that are specifically designed to process time-series and capture their short-
    and long-term dependencies through dilated causal convolutions [[65](#bib.bib65),
    [78](#bib.bib78)].
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）通过在隐藏层中插入卷积层和最大池化层，扩展了多层感知器（MLP）的架构。这些层使得神经网络能够捕捉输入数据的更多抽象特征，同时减少输入维度。在对来自图像的二维输入数据进行分类时，这种方法尤其有效，因为特征如线条可以独立于其在图像中的确切位置进行学习。这一功能通过将日志键排列在矩阵中，从而使事件之间的关系，即它们的时间依赖性，被网络捕捉到，也被转移到日志数据中[[27](#bib.bib27)]。此外，还有几种依赖于特定类型卷积神经网络的方法，例如时间卷积网络（TCN），专门设计用于处理时间序列数据，并通过扩张的因果卷积捕捉其短期和长期依赖性[[65](#bib.bib65),
    [78](#bib.bib78)]。
- en: 'As visible in Table [II](#S4.T2 "TABLE II ‣ IV-A2 Citations ‣ IV-A Bibliometrics
    ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey"),
    Recurrent Neural Networks (RNN) are the most commonly used neural network architectures
    in the surveyed literature, with 36 out of 62 reviewed approaches leveraging RNNs
    for anomaly detection. The main reason for this is that the architecture of RNNs
    leverages feedback mechanisms that retain their states over time and thus directly
    enable learning of sequential event execution patterns in input data, which are
    the key identifiers for anomalies in log data sets that are commonly used in evaluations
    (cf. Sect. [IV-E](#S4.SS5 "IV-E Evaluation & Reproducibility ‣ IV Survey Results
    ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")). Several different
    types of RNNs have successfully been applied for this purpose. One of the most
    widespread architectures are Long Short-Term Memory (LSTM) RNNs that are developed
    to enable long-time storage of states and comprise cells with input gates, output
    gates, and forget gates. A majority of the approaches leveraging LSTM RNNs train
    the network with sequences of event occurrences (or modified and enriched versions
    thereof) and subsequently disclose unusual sequential patterns in test data as
    anomalies [[24](#bib.bib24)]. Some approaches make use of Bi-LSTM RNNs, which
    are basically two independent LSTM RNNs that work in parallel and process sequences
    in opposite directions, i.e., while one LSTM RNN processes the input sequences
    as usual from the first to the last element, the other LSTM RNN processes sequence
    elements starting from the last entry and predicts elements that chronologically
    precede them. Experiments suggest that Bi-LSTM RNNs outperform LSTM RNNs [[19](#bib.bib19),
    [54](#bib.bib54), [50](#bib.bib50), [38](#bib.bib38), [80](#bib.bib80), [61](#bib.bib61),
    [77](#bib.bib77), [59](#bib.bib59)]. Another popular choice for RNNs are Gated
    Recurrent Units (GRU) that simplify the cell architecture as they only rely on
    update and reset gates. One of the main benefits of GRUs is that they are computationally
    more efficient than LSTM RNNs, which is a relevant aspect for use cases focusing
    on edge devices [[72](#bib.bib72), [66](#bib.bib66), [39](#bib.bib39), [38](#bib.bib38),
    [60](#bib.bib60), [74](#bib.bib74), [57](#bib.bib57), [41](#bib.bib41), [25](#bib.bib25)].'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格[II](#S4.T2 "TABLE II ‣ IV-A2 Citations ‣ IV-A Bibliometrics ‣ IV Survey
    Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")所示，循环神经网络（RNN）是调查文献中最常用的神经网络架构，62种被评审的方法中有36种利用RNN进行异常检测。主要原因在于RNN的架构利用了反馈机制，这些机制能够随着时间的推移保留其状态，从而直接实现对输入数据中序列事件执行模式的学习，这些模式是日志数据集中异常的关键标识符，这些数据集通常用于评估（参见第[IV-E](#S4.SS5
    "IV-E Evaluation & Reproducibility ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey")节）。几种不同类型的RNN已成功应用于这一目的。最广泛使用的架构之一是长短期记忆（LSTM）RNN，这种架构被开发出来以实现长时间的状态存储，并包含具有输入门、输出门和遗忘门的单元。大多数利用LSTM
    RNN的方法使用事件发生序列（或其修改和丰富版本）训练网络，然后在测试数据中揭示出异常的非凡序列模式[[24](#bib.bib24)]。一些方法使用双向LSTM
    RNN，即基本上是两个独立的LSTM RNN，它们并行工作并在相反的方向处理序列，即一个LSTM RNN从第一个元素到最后一个元素处理输入序列，而另一个LSTM
    RNN则从最后一个条目开始处理序列元素，并预测其时间上前面的元素。实验表明，双向LSTM RNN优于LSTM RNN[[19](#bib.bib19), [54](#bib.bib54),
    [50](#bib.bib50), [38](#bib.bib38), [80](#bib.bib80), [61](#bib.bib61), [77](#bib.bib77),
    [59](#bib.bib59)]。另一种流行的RNN选择是门控循环单元（GRU），它通过只依赖更新门和重置门来简化单元架构。GRU的主要优点之一是其计算效率比LSTM
    RNN更高，这对于关注边缘设备的应用场景而言是一个重要方面[[72](#bib.bib72), [66](#bib.bib66), [39](#bib.bib39),
    [38](#bib.bib38), [60](#bib.bib60), [74](#bib.bib74), [57](#bib.bib57), [41](#bib.bib41),
    [25](#bib.bib25)]。'
- en: While aforementioned deep learning models are primarily used for classification
    problems across different research fields, there are also models that are specifically
    designed to operate in unsupervised manner and are thus a natural choice for anomaly
    detection. One of them are Autoencoders (AE), which first create a code from the
    input data using an encoder and then try to approximate the input from the code
    using a decoder, thereby avoiding the need for labeled input data. The main idea
    is that through this process the neural network learns the main features from
    the input but neglects the noise in the data, similar to dimension reduction techniques
    such as principal component analysis (PCA). Any input data that is fed into an
    already trained network and yields a high reconstruction error is then considered
    as anomalous. Besides the standard model for Autoencoders, there are also several
    related types, such as Variational Autoencoders (VAE) that operate on statistical
    distributions [[62](#bib.bib62), [56](#bib.bib56), [1](#bib.bib1)], Conditional
    Variational Autoencoders (CVAE) that add conditional information such as event
    types to the training [[53](#bib.bib53)], and Convolutional Autoencoder (CAE)
    that leverage the advantages of CNNs regarding learning of location-independent
    features [[62](#bib.bib62)].
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习模型主要用于各个研究领域的分类问题，但也有一些模型专门设计用于无监督操作，因此非常适合异常检测。其中之一是自动编码器（AE），它首先通过编码器从输入数据创建一个代码，然后使用解码器尝试从代码中近似输入数据，从而避免了对标记输入数据的需求。其主要思想是通过这个过程，神经网络学习输入的主要特征，但忽略数据中的噪声，类似于主成分分析（PCA）等降维技术。任何输入数据如果在已训练的网络中产生高重建误差，则被视为异常。除了标准的自动编码器模型，还有几种相关类型，如操作于统计分布上的变分自动编码器（VAE）[[62](#bib.bib62),
    [56](#bib.bib56), [1](#bib.bib1)]，添加条件信息（如事件类型）的条件变分自动编码器（CVAE）[[53](#bib.bib53)]，以及利用卷积神经网络（CNN）学习位置无关特征的卷积自动编码器（CAE）[[62](#bib.bib62)]。
- en: 'Generative Adversarial Networks (GAN) are another approach for unsupervised
    deep learning. They actually consist of two separate components that compete with
    each other: a generator that produces new data that resembles the input data,
    and a discriminator that estimates the probability that some data stems from the
    input data, which is used to improve the generator. Existing approaches use different
    models to construct GANs, including LSTM RNNs [[70](#bib.bib70), [44](#bib.bib44)],
    CNNs in combination with GRUs [[39](#bib.bib39)], and Transformers [[81](#bib.bib81),
    [1](#bib.bib1)].'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）是另一种无监督深度学习的方法。它实际上由两个相互竞争的独立组件组成：一个生成器，产生类似于输入数据的新数据；一个判别器，估计某些数据源自输入数据的概率，这被用来改进生成器。现有的方法使用不同的模型构建GAN，包括LSTM
    RNN[[70](#bib.bib70), [44](#bib.bib44)]，与GRU结合的CNN[[39](#bib.bib39)]，以及Transformers[[81](#bib.bib81),
    [1](#bib.bib1)]。
- en: 'Transformers (TF) make use of so-called self-attention mechanisms to embed
    data instances into a vector space, where similar instances should be closer to
    each other than dissimilar ones [[28](#bib.bib28), [29](#bib.bib29), [37](#bib.bib37)].
    The goal of Transformers is to assign weights to specific inputs according to
    the context of their occurrence, such as words in sentences. Accordingly, Transformers
    have been particularly successful in the area of natural language processing (NLP).
    Attention mechanisms (AT) do not just appear in Transformers, but are also frequently
    used to improve classification and detection in other deep neural networks such
    as RNNs by weighting relevant inputs higher. This effect is particularly strong
    when long sequences are ingested by RNNs [[78](#bib.bib78)]. Attention mechanisms
    are usually realized as trainable networks such as MLPs [[49](#bib.bib49)]. In
    order to avoid confusions with the Transformer model, we state these additional
    attention mechanisms explicitly in Table [II](#S4.T2 "TABLE II ‣ IV-A2 Citations
    ‣ IV-A Bibliometrics ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection
    in Log Data: A Survey").'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformers (TF) 利用所谓的自注意力机制将数据实例嵌入到一个向量空间，其中相似的实例应比不相似的实例更靠近 [[28](#bib.bib28),
    [29](#bib.bib29), [37](#bib.bib37)]。Transformers 的目标是根据它们出现的上下文为特定输入分配权重，例如句子中的单词。因此，Transformers
    在自然语言处理 (NLP) 领域特别成功。注意力机制（AT）不仅出现在 Transformers 中，还经常用于通过提高相关输入的权重来改善其他深度神经网络（如
    RNN）的分类和检测。这种效果在 RNN 处理长序列时尤其明显 [[78](#bib.bib78)]。注意力机制通常以可训练的网络形式实现，例如 MLP [[49](#bib.bib49)]。为了避免与
    Transformer 模型混淆，我们在表 [II](#S4.T2 "TABLE II ‣ IV-A2 Citations ‣ IV-A Bibliometrics
    ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")
    中明确列出这些额外的注意力机制。'
- en: Wan et al. [[63](#bib.bib63)] are the only authors to utilize Graph Neural Networks
    (GNN). While neural networks typically ingest ordered data, e.g., CNNs rely on
    2-dimensional input data and RNNs require sequences of observations, GNNs are
    designed to ingest graph inputs, i.e., sets of vertices and edges. One possibility
    to transform log data into graphs is to generate session graphs, with vertices
    representing events and edges their sequential executions. Another less commonly
    applied type of deep learning model is the Evolving Granular Neural Network (EGNN),
    a fuzzy inference system that is gradually constructed from online data streams
    [[36](#bib.bib36)].
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Wan 等人 [[63](#bib.bib63)] 是唯一使用图神经网络（GNN）的作者。虽然神经网络通常处理有序数据，例如 CNN 依赖于二维输入数据，RNN
    需要观测序列，但 GNN 设计用来处理图形输入，即顶点和边的集合。将日志数据转换为图的一种可能性是生成会话图，其中顶点表示事件，边表示其顺序执行。另一种较少应用的深度学习模型是演变粒度神经网络（EGNN），这是一种模糊推理系统，逐渐从在线数据流中构建
    [[36](#bib.bib36)]。
- en: Our survey shows that many of the reviewed approaches rely on only one type
    of deep learning model. However, some authors also use combinations of different
    models. For example, Wang et al. [[1](#bib.bib1)] propose to use a MLP to combine
    the output of a VAE with that of a Transformer trained with adversarial learning.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查显示，许多被审查的方法仅依赖于一种深度学习模型。然而，一些作者也使用不同模型的组合。例如，Wang 等人 [[1](#bib.bib1)] 提出使用
    MLP 将 VAE 的输出与经过对抗学习训练的 Transformer 的输出结合起来。
- en: IV-B2 Training loss function
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 训练损失函数
- en: 'TABLE III: Definitions of common training loss functions'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：常见训练损失函数的定义
- en: '| Name | Description | Equation |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 | 方程 |'
- en: '| Cross-Entropy | Loss between a ground truth label $y$ and the predicted label
    $\hat{y}$. | $H(y,\hat{y})=-\sum_{j=1}^{N}y_{j}log\left(\hat{y}_{j}\right)$ |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 交叉熵 | 实际标签 $y$ 和预测标签 $\hat{y}$ 之间的损失。 | $H(y,\hat{y})=-\sum_{j=1}^{N}y_{j}log\left(\hat{y}_{j}\right)$
    |'
- en: '| Hyper-Sphere Objective Function | Distance between embedding vector $y$ and
    hyper-sphere center $c$. | $L_{HS}=\frac{1}{N}\sum_{j=1}^{N}\left\&#124;y_{j}-c\right\&#124;^{2}$
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 超球面目标函数 | 嵌入向量 $y$ 和超球面中心 $c$ 之间的距离。 | $L_{HS}=\frac{1}{N}\sum_{j=1}^{N}\left\&#124;y_{j}-c\right\&#124;^{2}$
    |'
- en: '| Mean Squared Error | Loss between a ground truth label $y$ and the predicted
    label $\hat{y}$. | $L_{MSE}(y,\hat{y})=\frac{1}{N}\sum_{j=1}^{N}\left(y_{j}-\hat{y}_{j}\right)^{2}$
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 均方误差 | 实际标签 $y$ 和预测标签 $\hat{y}$ 之间的损失。 | $L_{MSE}(y,\hat{y})=\frac{1}{N}\sum_{j=1}^{N}\left(y_{j}-\hat{y}_{j}\right)^{2}$
    |'
- en: '| Kullback-Leibler Divergence | Statistical divergence between probability
    distributions $P$ and $Q$. | $KL(P\&#124;Q)=\sum_{x\in\mathcal{X}}P(x)log\left(\frac{P(x)}{Q(x)}\right)$
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Kullback-Leibler 散度 | 概率分布 $P$ 和 $Q$ 之间的统计散度。 | $KL(P\&#124;Q)=\sum_{x\in\mathcal{X}}P(x)log\left(\frac{P(x)}{Q(x)}\right)$
    |'
- en: '| Adversarial Training Function | Loss for vector $y$ and prediction $\hat{y}$
    with generator $G$ and discriminator $D$. | $\!\begin{aligned} L_{AT}=&amp;\min_{G}\max_{D}(\mathbb{E}\left(log\left(D(G(\hat{y}))\right)\right)+\\
    &amp;\mathbb{E}\left(log\left(1-D(G(y))\right)\right))\end{aligned}$ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 对抗训练函数 | 对于生成器$G$和鉴别器$D$，向量$y$和预测$\hat{y}$的损失。 | $\!\begin{aligned} L_{AT}=&\min_{G}\max_{D}(\mathbb{E}\left(log\left(D(G(\hat{y}))\right)\right)+\\
    &\mathbb{E}\left(log\left(1-D(G(y))\right)\right))\end{aligned}$ |'
- en: Loss functions (DL-2) are essential for training of deep neural networks as
    they quantify the difference between the output of the neural network and the
    expected result [[85](#bib.bib85)]. The most common loss function in the reviewed
    publications is the Cross-Entropy (CE), in particular, the categorical cross-entropy
    for multi-class prediction [[24](#bib.bib24), [61](#bib.bib61)] or binary cross-entropy
    that only differentiates between the normal and anomalous class [[65](#bib.bib65)].
    Other common loss functions include the Hyper-Sphere Objective Function (HS) where
    the distance to the center of a hyper-sphere represents the anomaly score [[28](#bib.bib28),
    [66](#bib.bib66), [29](#bib.bib29), [44](#bib.bib44)], the Mean Squared Error
    (MSE) that is used for regression [[31](#bib.bib31), [54](#bib.bib54), [32](#bib.bib32),
    [51](#bib.bib51), [57](#bib.bib57), [72](#bib.bib72), [24](#bib.bib24)], and the
    Kullback-Leibler Divergence (KL) and its extension Marginal Likelihood (ML) that
    are useful to measure loss in probability distributions [[53](#bib.bib53), [62](#bib.bib62)].
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数（DL-2）对于深度神经网络的训练至关重要，因为它们量化了神经网络输出与期望结果之间的差异[[85](#bib.bib85)]。在审阅的出版物中，最常见的损失函数是交叉熵（CE），特别是用于多类预测的类别交叉熵[[24](#bib.bib24),
    [61](#bib.bib61)]，或仅区分正常类和异常类的二元交叉熵[[65](#bib.bib65)]。其他常见的损失函数包括超球目标函数（HS），其中到超球中心的距离表示异常评分[[28](#bib.bib28),
    [66](#bib.bib66), [29](#bib.bib29), [44](#bib.bib44)]，用于回归的均方误差（MSE）[[31](#bib.bib31),
    [54](#bib.bib54), [32](#bib.bib32), [51](#bib.bib51), [57](#bib.bib57), [72](#bib.bib72),
    [24](#bib.bib24)]，以及用于测量概率分布损失的Kullback-Leibler散度（KL）及其扩展边际似然（ML）[[53](#bib.bib53),
    [62](#bib.bib62)]。
- en: 'Some of the presented approaches are trained with Custom Loss Functions (CF),
    including combinations of CE and HS [[68](#bib.bib68)]. Some authors also define
    objective functions specifically for Adversarial Training (AT) of GANs [[70](#bib.bib70),
    [44](#bib.bib44)]. Table [III](#S4.T3 "TABLE III ‣ IV-B2 Training loss function
    ‣ IV-B Deep Learning Techniques ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey") summarizes the main loss functions. Out of all
    publications, 14 do not state the loss function and are therefore marked as Not
    Available (NA).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '一些提出的方法使用了自定义损失函数（CF），包括CE和HS的组合[[68](#bib.bib68)]。一些作者还专门为生成对抗网络（GANs）的对抗训练（AT）定义了目标函数[[70](#bib.bib70),
    [44](#bib.bib44)]。表[III](#S4.T3 "TABLE III ‣ IV-B2 Training loss function ‣ IV-B
    Deep Learning Techniques ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection
    in Log Data: A Survey")总结了主要的损失函数。在所有出版物中，有14篇没有说明损失函数，因此标记为不可用（NA）。'
- en: IV-B3 Operation mode
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 操作模式
- en: When applying anomaly detection in real world scenarios, not all log data is
    available at any time; instead, events are generated as a continuous stream and
    should be analyzed only at their time of occurrence in order to enable (close
    to) real-time detection. Thereby, the structural integrity and statistical properties
    of the generated logs vary over time, e.g. the overall system utilization is not
    stationary as user interactions and the technological environment are subject
    to change. In addition, log templates change or new log events occur when applications
    generating these logs are modified [[19](#bib.bib19)].
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中应用异常检测时，并非所有日志数据都随时可用；相反，事件以连续流的形式生成，应仅在事件发生时进行分析，以实现（接近）实时检测。因此，生成日志的结构完整性和统计属性随着时间变化，例如整体系统利用率并非静态，因为用户交互和技术环境会发生变化。此外，当生成这些日志的应用程序发生修改时，日志模板会发生变化或出现新的日志事件[[19](#bib.bib19)]。
- en: 'To keep up with these changes in an automated way, algorithms need to adopt
    online or incremental learning (DL-3), i.e., ingest input data with linear time
    complexity so that data sets are processed in a single pass where each data instance
    is only handled once. While it is usually always possible to carry out detection
    in an online fashion when trained models are considered static, it is significantly
    more challenging to develop algorithms that support online learning and dynamically
    update their models to adapt to new events or patterns by incorporating them into
    the baseline for detection [[86](#bib.bib86), [87](#bib.bib87)]. As continuous
    learning is still an open problem, we mark all approaches that enable dynamic
    model updates at least to some degree as online (ON) and all other approaches
    with offline training phases as offline (OFF) in Table [II](#S4.T2 "TABLE II ‣
    IV-A2 Citations ‣ IV-A Bibliometrics ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey").'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '为了以自动化方式跟上这些变化，算法需要采用在线或增量学习（DL-3），即以线性时间复杂度处理输入数据，以便在单次遍历中处理数据集，每个数据实例仅处理一次。虽然在考虑训练模型为静态时，通常总是可以进行在线检测，但开发支持在线学习的算法并动态更新模型以适应新事件或模式，按照基线进行检测，是具有挑战性的[[86](#bib.bib86),
    [87](#bib.bib87)]。由于持续学习仍然是一个未解决的问题，我们将所有至少在某种程度上支持动态模型更新的方法标记为在线（ON），而所有具有离线训练阶段的方法标记为离线（OFF），见表[II](#S4.T2
    "TABLE II ‣ IV-A2 Citations ‣ IV-A Bibliometrics ‣ IV Survey Results ‣ Deep Learning
    for Anomaly Detection in Log Data: A Survey")。'
- en: The reviewed approaches addressed dynamic model adaptation in multiple ways.
    Du et al. [[24](#bib.bib24)] suggest to update the weights of their neural network
    when false positives are identified during the detection to reflect the correct
    event probability distributions without the need to re-train from scratch. Meng
    et al. [[26](#bib.bib26)] argue that such a manual feedback loop is infeasible
    in practice and therefore resort to re-training where new event types are mapped
    to existing ones. Yen et al. [[75](#bib.bib75)] automatically re-train their neural
    network on batches of new log data when false positive rates exceed a certain
    threshold. However, calculating the false positive rates relies on labeled data
    and thus does not remove the human from the loop. A promising solution to aforementioned
    problems is presented by Decker et al. [[36](#bib.bib36)], who employ an evolving
    classifier that is specifically designed to handle unstable data streams and could
    thus enable continuous learning.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 综述的方法在动态模型适应方面采取了多种方式。Du等人[[24](#bib.bib24)]建议在检测过程中识别到假阳性时，更新其神经网络的权重，以反映正确的事件概率分布，而无需从头开始重新训练。Meng等人[[26](#bib.bib26)]认为这种手动反馈回路在实践中是不可行的，因此
    resort to re-training，将新的事件类型映射到现有的事件类型。Yen等人[[75](#bib.bib75)]在假阳性率超过某个阈值时，自动在新日志数据的批次上重新训练他们的神经网络。然而，计算假阳性率依赖于标记数据，因此无法完全消除人工参与。Decker等人[[36](#bib.bib36)]提出了一种有前景的解决方案，他们采用了一种专门设计来处理不稳定数据流的演变分类器，从而实现了持续学习。
- en: In general, both online and offline models can work in supervised (SUP) as well
    as unsupervised (UN) fashion (DL-4). However, we noticed that almost all supervised
    learning approaches in the reviewed publications opt for an offline training phase.
    This is reasonable as the label information required for supervised learning relies
    on manual analysis or validation and therefore can only be generated forensically
    for delimited data sets, but not for data streams.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在线和离线模型可以在监督（SUP）和非监督（UN）模式下工作（DL-4）。然而，我们注意到，几乎所有综述中提到的监督学习方法都选择了离线训练阶段。这是合理的，因为监督学习所需的标签信息依赖于手动分析或验证，因此只能在有限的数据集上进行法医学生成，而不能在数据流中生成。
- en: We also made the observation that many reviewed approaches claim to enable unsupervised
    learning, but are in fact operated in semi-supervised (SEMI) fashion as they usually
    assume that training takes place only on normal data that is free of anomalies
    [[34](#bib.bib34)]. The main problem is that anomalies that are present in training
    data would incorrectly change the weights of the neural networks and thus deteriorate
    their detection in the subsequent detection phase. Accordingly, only deep learning
    models that are designed for unsupervised learning are capable of handling anomalies
    in training data, for example, the approach based on a CVAE model presented by
    Otomo et al. [[53](#bib.bib53)]. Note that neural network architectures that would
    support unsupervised learning may also be applied for semi-supervised detection
    and are therefore not necessarily marked as unsupervised.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还观察到许多已审查的方法声称能够实现无监督学习，但实际上是以半监督（SEMI）方式进行的，因为它们通常假设训练只在没有异常的正常数据上进行[[34](#bib.bib34)]。主要问题是，训练数据中存在的异常会错误地改变神经网络的权重，从而在后续的检测阶段恶化其检测效果。因此，只有专为无监督学习设计的深度学习模型能够处理训练数据中的异常，例如，Otomo等人提出的基于CVAE模型的方法[[53](#bib.bib53)]。需要注意的是，支持无监督学习的神经网络架构也可以用于半监督检测，因此不一定标记为无监督。
- en: IV-C Log Data Preparation
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 日志数据准备
- en: 'This section outlines all steps necessary to prepare raw and textual log data
    for deep learning. This includes grouping of events into windows or sessions,
    tokenization and log parsing, extraction of features from the logs, as well as
    transformation of these features into vector representations that are suitable
    as input to neural networks. Figure [3](#S4.F3 "Figure 3 ‣ IV-C Log Data Preparation
    ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")
    provides an overview of these data preparation methods used in the reviewed literature
    and illustrates them on the sample log lines L1-L8\. In the following sections,
    we discuss all stated methods in detail.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '本节概述了将原始和文本日志数据准备为深度学习所需的所有步骤。这包括将事件分组为窗口或会话、分词和日志解析、从日志中提取特征，以及将这些特征转换为适合作为神经网络输入的向量表示。图[3](#S4.F3
    "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey")提供了在审查文献中使用的数据准备方法的概述，并在示例日志行L1-L8上进行了说明。在接下来的部分中，我们将详细讨论所有列出的方法。'
- en: '![Refer to caption](img/f62af9535be42b81d5f66279bdc85908.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f62af9535be42b81d5f66279bdc85908.png)'
- en: 'Figure 3: Different stages of data preparation to transform raw log data into
    numeric vectors.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：将原始日志数据转换为数字向量的不同数据准备阶段。
- en: IV-C1 Pre-processing
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 预处理
- en: 'As log data is generally unstructured it is necessary to pre-process them in
    some way before feeding them into deep learning systems (PP-1). Our survey shows
    that there are two main approaches to handle unstructured data. The most common
    approach is to leverage parsers, which are usually referred to as log keys (KEY),
    to extract unique event identifiers for each line as well as event parameter values
    as described in Sect. [II-A2](#S2.SS1.SSS2 "II-A2 Log Data ‣ II-A Preliminary
    Definitions ‣ II Background ‣ Deep Learning for Anomaly Detection in Log Data:
    A Survey"). Thereby, authors usually re-use existing log keys for well-known data
    sets or create the keys using state-of-the-art approaches for parser generation,
    such as Drain [[88](#bib.bib88)] or Spell [[89](#bib.bib89)]. Typically, each
    line matches exactly one key; it is thus easy to assign a unique event type identifier
    to each log line during event mapping. For example, in Fig. [3](#S4.F3 "Figure
    3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey") line L1 is mapped to event type identifier E1
    as it matches the corresponding log key. Moreover, the figure also shows that
    parsing allows to extract all parameters from log events, in particular, the time
    stamp and identifiers for packet responder and file block are extracted from L1
    and stored in a list.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '由于日志数据通常是非结构化的，因此在将其输入深度学习系统之前需要以某种方式进行预处理（PP-1）。我们的调查显示，处理非结构化数据有两种主要方法。最常见的方法是利用解析器，这些解析器通常被称为日志键（KEY），以提取每行的唯一事件标识符以及事件参数值，如第[II-A2](#S2.SS1.SSS2
    "II-A2 Log Data ‣ II-A Preliminary Definitions ‣ II Background ‣ Deep Learning
    for Anomaly Detection in Log Data: A Survey")节所述。因此，作者通常会重新使用现有的日志键来处理已知的数据集，或者使用最先进的解析器生成方法（例如Drain[[88](#bib.bib88)]或Spell[[89](#bib.bib89)]）来创建这些键。通常，每行数据正好匹配一个键；因此，在事件映射期间，很容易为每条日志行分配唯一的事件类型标识符。例如，在图[3](#S4.F3
    "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey")中，L1行被映射到事件类型标识符E1，因为它与相应的日志键匹配。此外，图中还显示了解析可以提取日志事件中的所有参数，特别是时间戳和数据包响应者及文件块的标识符从L1中提取并存储在列表中。'
- en: An alternative to parsing are token-based (TOK) strategies that split log messages
    into lists of words, for example, by splitting them at white spaces. It is then
    common to clean the data by transforming all letters to lowercase and removing
    special characters and stop words before obtaining the final word vectors. While
    such approaches draw less semantic information from the single tokens, they have
    the advantage of being more flexible as they rely on generally applicable heuristics
    rather than pre-defined parsers and are therefore widely applicable. Some approaches
    make use of a combination (COM) of parsing and token-based pre-processing strategies,
    in particular, by generating token vectors from parsed events rather than raw
    log lines [[48](#bib.bib48), [32](#bib.bib32), [42](#bib.bib42)].
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 解析的另一种方法是基于令牌（TOK）的策略，它将日志消息拆分成单词列表，例如，通过在空白处拆分。然后，通常会通过将所有字母转换为小写字母并去除特殊字符和停用词来清理数据，然后再获得最终的单词向量。虽然这种方法从单个令牌中提取的语义信息较少，但它们具有更大的灵活性，因为它们依赖于通用的启发式方法，而不是预定义的解析器，因此具有广泛的适用性。一些方法使用解析和基于令牌的预处理策略的组合（COM），特别是通过从解析的事件而不是原始日志行中生成令牌向量[[48](#bib.bib48)、[32](#bib.bib32)、[42](#bib.bib42)]。
- en: IV-C2 Event grouping
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 事件分组
- en: 'As pointed out in Sect. [II-A3](#S2.SS1.SSS3 "II-A3 Anomaly Detection ‣ II-A
    Preliminary Definitions ‣ II Background ‣ Deep Learning for Anomaly Detection
    in Log Data: A Survey"), simple outlier detection does not require any grouping
    of logs since single unusual events are regarded as anomalies independent from
    the context in which they occur in. However, deep learning is most often applied
    to disclose unusual patterns of multiple log events, such as changes of event
    sequences or temporal log correlations. For these cases it is necessary to logically
    organize events into groups that are then analyzed individually or in relation
    to each other.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[II-A3](#S2.SS1.SSS3 "II-A3 Anomaly Detection ‣ II-A Preliminary Definitions
    ‣ II Background ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")节中指出，简单的异常值检测不需要对日志进行分组，因为单个异常事件被视为与其发生的上下文无关的异常。然而，深度学习通常应用于揭示多个日志事件中的异常模式，例如事件序列的变化或时间日志关联。对于这些情况，需要将事件逻辑上组织成组，然后分别或相互关系分析。'
- en: 'We illustrate common event grouping strategies in the top right of Fig. [3](#S4.F3
    "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey"). Grouping into time windows is almost
    always feasible as log events are generally produced with a time stamp that documents
    their time of generation [[3](#bib.bib3)]. Since time stamps commonly occur in
    the beginning of log lines and are thus relatively easy to extract, it is not
    necessary to parse the remainder of the usually more complex log messages. There
    are two main strategies for time-based grouping [[4](#bib.bib4)]. First, sliding
    time windows are windows of a specific duration that are shifted across the log
    data with a fixed step size that is generally smaller and a whole number divisor
    of the window size. For every step where the time window is moved all logs with
    a time stamp within the current start and end time of the window are allocated
    to the same group, where each log line may appear in multiple groups as time windows
    overlap. For the sample logs in Fig. [3](#S4.F3 "Figure 3 ‣ IV-C Log Data Preparation
    ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")
    we assume time window of 2 hours starting at 20:00:00 and a step size of 30 minutes.
    As visible in the figure, lines L5, L6, and L7 are contained in both $Tb$ and
    $Tc$. Second, fixed time windows are special cases of sliding time windows where
    the step size is set to the same value as the window size. While this strategy
    results in a less fine-grained view on the data, the advantage is that each log
    event will only be allocated to exactly one time window, which makes subsequent
    computations such as time-series analysis easier. The log events in Fig. [3](#S4.F3
    "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey") are exemplarily grouped into fixed time
    windows of 2 hours, yielding $T1$ containing the first set of four lines and $T2$
    containing the subsequent set of four lines. It must be noted that grouping based
    on sliding or fixed windows may also be carried out by numbers of lines rather
    than time so that each group of lines has the same size. While this avoids the
    need to process time stamps and ensures that the group sizes are fixed (e.g.,
    there cannot be any empty groups), it is more difficult to consider event frequencies
    as time-series as the resulting windows represent varying time spans.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[3](#S4.F3 "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣
    Deep Learning for Anomaly Detection in Log Data: A Survey")的右上角展示了常见的事件分组策略。按时间窗口进行分组几乎总是可行的，因为日志事件通常会生成带有时间戳的记录[[3](#bib.bib3)]。由于时间戳通常出现在日志行的开头，因此相对容易提取，无需解析通常更复杂的日志消息。时间基础分组有两种主要策略[[4](#bib.bib4)]。首先，滑动时间窗口是具有特定持续时间的窗口，这些窗口在日志数据中以固定的步长滑动，步长通常较小且为窗口大小的整数倍。每次滑动时间窗口时，所有时间戳在当前窗口的开始和结束时间内的日志都被分配到同一组，其中每个日志行可能出现在多个组中，因为时间窗口会重叠。对于图[3](#S4.F3
    "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey")中的样本日志，我们假设时间窗口为2小时，从20:00:00开始，步长为30分钟。如图所示，L5、L6和L7行都包含在$Tb$和$Tc$中。第二，固定时间窗口是滑动时间窗口的特例，其中步长设置为与窗口大小相同的值。虽然这种策略导致对数据的视图较不细粒度，但优点在于每个日志事件将仅分配到一个时间窗口，这使得后续计算如时间序列分析变得更容易。图[3](#S4.F3
    "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey")中的日志事件被示例性地分组为2小时的固定时间窗口，得到$T1$包含前四行和$T2$包含随后的四行。必须注意，基于滑动或固定窗口的分组也可以按行数进行，而不是时间，这样每组行具有相同的大小。虽然这避免了处理时间戳的需要，并确保了组大小是固定的（例如，不可能有空组），但考虑事件频率作为时间序列更为困难，因为结果窗口表示不同的时间跨度。'
- en: 'An entirely different grouping strategy are session windows that rely on an
    event parameter that acts as an identifier for a specific task or process where
    the event originated from. Grouping log events by these session identifiers allows
    to extract event sequences that correctly depict underlying program workflows
    even when multiple sessions are carried out in parallel on the monitored system.
    Unfortunately, not all types of log data come with such an identifier for sessions.
    In the sample logs depicted in Fig. [3](#S4.F3 "Figure 3 ‣ IV-C Log Data Preparation
    ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")
    the file block identifier (e.g., blk_388) acts as a session identifier and thus
    allows us to exemplarily extract three event sequences.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '完全不同的分组策略是会话窗口，它依赖于一个事件参数，该参数作为特定任务或过程的标识符，标识事件的来源。通过这些会话标识符对日志事件进行分组，可以提取出正确描绘基础程序工作流的事件序列，即使在被监控的系统上同时进行多个会话时也不例外。不幸的是，并非所有类型的日志数据都带有这样的会话标识符。在图[3](#S4.F3
    "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey")中显示的示例日志中，文件块标识符（例如，blk_388）作为会话标识符，从而允许我们举例提取出三个事件序列。'
- en: IV-C3 Feature extraction
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C3 特征提取
- en: 'The parsing- and token-based pre-processing strategies described in Sect. [IV-C1](#S4.SS3.SSS1
    "IV-C1 Pre-processing ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning
    for Anomaly Detection in Log Data: A Survey") enable the extraction of structured
    features from the otherwise unstructured logs (PP-2). Some of these features are
    directly derived from pre-processing logs, e.g., the Token Sequence (TS) may be
    used without any further modifications for analyzing each log line as a sentence
    of words. On the other hand, Token Counts (TC) require an additional step of computation
    where the tokens in each line are compared and counted, including advanced weighting
    techniques for each token such as term frequency–inverse document frequency (TF-IDF)
    that estimates token relevance based on token occurrences across all observed
    log lines.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[IV-C1](#S4.SS3.SSS1 "IV-C1 Pre-processing ‣ IV-C Log Data Preparation ‣
    IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")节中描述的基于解析和令牌的预处理策略使得能够从原本无结构的日志中提取结构化特征（PP-2）。其中一些特征是直接从预处理日志中得出的，例如，令牌序列（TS）可以在不进行任何进一步修改的情况下，用于将每行日志分析为词汇的句子。另一方面，令牌计数（TC）需要额外的计算步骤，其中每行的令牌被比较和计数，包括对每个令牌的高级加权技术，如词频-逆文档频率（TF-IDF），它根据在所有观察到的日志行中的令牌出现情况来估计令牌的相关性。'
- en: 'Considering the outcome of the event mapping step it is simple to extract Event
    Sequences (ES), i.e., sequences of event type identifiers that are usually separated
    by fixed, sliding, or session windows (cf. Sect. [IV-C2](#S4.SS3.SSS2 "IV-C2 Event
    grouping ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey")). For example, Fig. [3](#S4.F3 "Figure 3 ‣ IV-C
    Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection
    in Log Data: A Survey") shows that the event sequence for file block blk_388 is
    $\left[E1,E1,E2,E1\right]$ corresponding to the event identifiers for the respective
    log keys matching the lines L1-L4\. Event Counts (EC) are vectors of length $d$,
    where the $i$-th element of the vector depicts the number of occurrences of the
    $i$-th log key and $d$ is the total number of available log keys. The example
    in Fig. [3](#S4.F3 "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣
    Deep Learning for Anomaly Detection in Log Data: A Survey") shows the event count
    vector for time window $T1$ as $\left[3,1,0,0\right]$, indicating that three log
    lines corresponding to the first log key (E1) appeared in $T1$, in particular,
    the lines L1, L2, and L4\. Besides frequencies, many other statistical properties
    (STAT) may be computed from event occurrences, such as the percentage of seasonal
    logs [[82](#bib.bib82)], the lengths of log messages [[57](#bib.bib57)], log activity
    rates [[36](#bib.bib36)], entropy-based scores for chunks of log lines [[32](#bib.bib32)],
    or the presence of sudden bursts in event occurrences [[82](#bib.bib82)].'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到事件映射步骤的结果，提取事件序列（ES）是简单的，即事件类型标识符的序列，通常由固定、滑动或会话窗口分隔（参见第[IV-C2节](#S4.SS3.SSS2
    "IV-C2 Event grouping ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning
    for Anomaly Detection in Log Data: A Survey")）。例如，图[3](#S4.F3 "Figure 3 ‣ IV-C
    Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection
    in Log Data: A Survey")显示了文件块blk_388的事件序列为$\left[E1,E1,E2,E1\right]$，对应于与L1-L4行匹配的日志键的事件标识符。事件计数（EC）是长度为$d$的向量，其中向量的第$i$个元素表示第$i$个日志键的出现次数，而$d$是可用日志键的总数。图[3](#S4.F3
    "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey")中的示例显示了时间窗口$T1$的事件计数向量为$\left[3,1,0,0\right]$，表示在$T1$中出现了三个对应于第一个日志键（E1）的日志行，特别是L1、L2和L4行。除了频率之外，还可以从事件发生中计算许多其他统计属性（STAT），例如季节性日志的百分比[[82](#bib.bib82)]、日志消息的长度[[57](#bib.bib57)]、日志活动率[[36](#bib.bib36)]、基于熵的日志行块得分[[32](#bib.bib32)]，或事件发生中的突然爆发[[82](#bib.bib82)]。'
- en: Parameters (PA) of log events are extracted as lists of values. Since the semantic
    meaning of each parameter is known after parsing, the values in each vector can
    be analyzed with methods that are appropriate for the respective value types,
    e.g., numeric or categorical. One special parameter of log events is the time
    stamp as it allows to put event occurrences into chronological order and infer
    dynamic dependencies. Accordingly, Event Interval Times (EI), i.e., inter-arrival
    times of log lines that belong to the same event type, are a frequently extracted
    feature for anomaly detection.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 日志事件的参数（PA）作为值的列表提取。由于在解析后每个参数的语义含义是已知的，因此每个向量中的值可以使用适合于相应值类型的方法进行分析，例如，数值或分类。日志事件的一个特殊参数是时间戳，因为它可以将事件发生按时间顺序排列并推断动态依赖关系。因此，事件间隔时间（EI），即属于同一事件类型的日志行的到达间隔时间，是进行异常检测时经常提取的特征。
- en: IV-C4 Feature representation
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C4 特征表示
- en: The extracted features described in the previous section comprise numeric or
    categorical vectors and are suitable to be consumed by neural networks. For example,
    event sequences are represented as Event ID sequence vectors (ID), i.e., chronologically
    ordered sequences of log key identifiers, and fed into RNNs to learn dependencies
    of event occurrences and disclose unusual sequence patterns as anomalies [[60](#bib.bib60)].
    Event counts are represented as ordered Count Vectors (CV) and are also similarly
    used as input for RNNs [[30](#bib.bib30)]. Event statistics are another type of
    input that do not require any specific processing other than representing them
    in a Statistical Feature Vector (FV) where the position of each element in the
    vector corresponds to one particular feature.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节描述的提取特征包括数值或分类向量，适合被神经网络处理。例如，事件序列被表示为事件ID序列向量（ID），即按时间顺序排列的日志键标识符序列，并输入到RNN中以学习事件发生的依赖关系，并揭示异常的序列模式
    [[60](#bib.bib60)]。事件计数被表示为有序计数向量（CV），也类似地用作RNN的输入 [[30](#bib.bib30)]。事件统计是另一种输入类型，不需要任何特定处理，除了将其表示为统计特征向量（FV），其中向量中每个元素的位置对应于一个特定特征。
- en: While it is possible to directly use the extracted features, most of the approaches
    presented in the reviewed publications in fact rely on combinations or otherwise
    transformed vector representations of the original feature vectors (PP-3). Thereby,
    the most common representation is that of a Semantic Vector (SV). Within the field
    of NLP it is common practice to transform words of a sentence into so called semantic
    vectors that encode context-based semantics (e.g. Word2Vec [[90](#bib.bib90)],
    BERT [[91](#bib.bib91)], or GloVe [[92](#bib.bib92)]) or language statistics (e.g.
    TF-IDF [[93](#bib.bib93)]). Since each log line comprises sequences of tokens
    analogous to words in a sentence, it stands to reason to apply methods from natural
    language processing on the token sequences. Similarly, a sequence of multiple
    subsequent events can be regarded as a sentence, where each unique log key represents
    a word. Semantic encoding is typically achieved by training deep neural methods
    on a specific log file or by relying on pre-trained models. Semantic vectors are
    sometimes used in combination with Positional Embedding (PE), where elements (typically
    tokens) are encoded based on their relative positions in a sequence. To add the
    positional information to the encoded log messages authors usually use sine and
    cosine functions for even and odd token indices respectively [[48](#bib.bib48),
    [28](#bib.bib28), [81](#bib.bib81)].
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以直接使用提取的特征，但在回顾的文献中，大多数方法实际上依赖于原始特征向量的组合或其他变换的向量表示（PP-3）。因此，最常见的表示方式是语义向量（SV）。在自然语言处理领域，通常将句子的单词转换为所谓的语义向量，这些向量编码了基于上下文的语义（例如，Word2Vec
    [[90](#bib.bib90)]、BERT [[91](#bib.bib91)] 或 GloVe [[92](#bib.bib92)]）或语言统计（例如，TF-IDF
    [[93](#bib.bib93)]）。由于每条日志行包含的令牌序列类似于句子中的单词，因此自然语言处理方法也适用于令牌序列。同样，一系列连续的事件可以被视为一个句子，其中每个独特的日志键代表一个单词。语义编码通常通过在特定日志文件上训练深度神经网络方法或依赖于预训练模型来实现。语义向量有时与位置嵌入（PE）结合使用，其中元素（通常是令牌）根据其在序列中的相对位置进行编码。为了将位置信息添加到编码的日志消息中，作者通常使用正弦和余弦函数分别处理偶数和奇数令牌索引
    [[48](#bib.bib48)、[28](#bib.bib28)、[81](#bib.bib81)]。
- en: One-Hot Encoding (OH) is one of the most common techniques to handle categorical
    data and is therefore frequently applied on event types (as a finite number of
    log keys is defined in the parser) or token values. Formally, the one-hot encoding
    of a value $i$ from an ordered list of $d$ values is a vector of length $d$ where
    the $i$-th element is $1$ and all others are $0$ [[24](#bib.bib24)]. While most
    authors use one-hot encoded data directly as an input to neural networks, it is
    also possible to combine it with other features such as count vectors so that
    the applied neural network is capable of discerning the input and learn separate
    models for different log keys [[53](#bib.bib53)].
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码（OH）是处理分类数据的最常见技术之一，因此通常应用于事件类型（因为解析器中定义了有限数量的日志键）或令牌值。形式上，来自有序列表中 $d$ 个值的值
    $i$ 的一热编码是一个长度为 $d$ 的向量，其中第 $i$ 个元素为 $1$，其余元素为 $0$ [[24](#bib.bib24)]。虽然大多数作者直接将一热编码数据用作神经网络的输入，但也可以将其与其他特征（例如计数向量）结合使用，以便应用的神经网络能够区分输入并为不同的日志键学习独立的模型
    [[53](#bib.bib53)]。
- en: Embedding Layers/Matrices (EL) are typically used to resolve the problems with
    respect to sparsity of high-dimensional input data such as one-hot encoded event
    types when a large number of log keys are required to parse the logs [[66](#bib.bib66),
    [73](#bib.bib73)]. They are usually randomly initialized parameters which are
    trained alongside the classification models to create optimal vector encodings
    for log messages [[27](#bib.bib27)]. The vector encodings are usually arranged
    in a matrix so that the respective vector for a particular log key is obtained
    by multiplying the matrix with a one-hot encoded log key vector. The main difference
    to semantic vectors is that embedding layers/matrices are generally not trained
    towards NLP objectives, i.e., they do not aim to learn the semantics of words
    like Word2Vec; instead, embedding layers/matrices are only trained to minimize
    the loss function of the classification network. Some authors also use custom
    embedding models based on deep learning; we refer to their output as Deep Encoded
    Embeddings (DE). This includes a combination of character-, event- and sequence-based
    embeddings [[45](#bib.bib45)], attention mechanisms using MLPs and CNNs [[49](#bib.bib49)],
    and token counts with label information fed into VAEs [[1](#bib.bib1)].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层/矩阵（EL）通常用于解决高维输入数据（如需要解析大量日志键的一次性编码事件类型）的稀疏性问题[[66](#bib.bib66), [73](#bib.bib73)]。这些通常是随机初始化的参数，与分类模型一起训练，以创建日志消息的最佳向量编码[[27](#bib.bib27)]。这些向量编码通常被排列在矩阵中，以便通过将矩阵与一次性编码的日志键向量相乘来获得特定日志键的相应向量。与语义向量的主要区别在于，嵌入层/矩阵通常不针对NLP目标进行训练，即，它们不旨在学习像Word2Vec那样的词语语义；相反，嵌入层/矩阵仅训练以最小化分类网络的损失函数。一些作者还使用基于深度学习的自定义嵌入模型；我们称其输出为深度编码嵌入（DE）。这包括基于字符、事件和序列的嵌入[[45](#bib.bib45)]、使用MLP和CNN的注意机制[[49](#bib.bib49)]，以及将标签信息输入VAE的令牌计数[[1](#bib.bib1)]。
- en: Other than aforementioned methods that operate with event types and mostly use
    tokens only for encoding these events in vector format, approaches that rely on
    Parameter Vectors (PV) directly use the actual values extracted from the parsed
    log messages. Thereby, extracted parameters that are numeric may be used for multi-variate
    time-series analysis with RNNs [[72](#bib.bib72), [83](#bib.bib83), [24](#bib.bib24)],
    while categorical values could be one-hot encoded and vectorized with word embedding
    methods [[47](#bib.bib47)]. Either way, as different log events have varying numbers
    of parameters with different semantic meaning, it is usually necessary to analyze
    the parameters of each event type on their own [[72](#bib.bib72), [83](#bib.bib83),
    [24](#bib.bib24)]. The time stamp of log events is a special parameter as it allows
    to put other parameters in temporal context, which is required for time-series
    analysis. However, the time stamps themselves may be used for Time Embedding (TE)
    and serve as input to neural networks [[31](#bib.bib31)]. For this purpose, Li
    et al. [[50](#bib.bib50)] generate vectors for sequences of time differences between
    event occurrences by applying soft one-hot encoding.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述操作事件类型并主要仅使用令牌以向量格式编码这些事件的方法外，依赖参数向量（PV）的方法直接使用从解析的日志消息中提取的实际值。因此，提取的数值参数可以用于使用RNN进行多变量时间序列分析[[72](#bib.bib72),
    [83](#bib.bib83), [24](#bib.bib24)]，而分类值可以进行一次性编码并使用词嵌入方法向量化[[47](#bib.bib47)]。无论哪种方式，由于不同的日志事件具有不同的参数和不同的语义含义，通常需要单独分析每种事件类型的参数[[72](#bib.bib72),
    [83](#bib.bib83), [24](#bib.bib24)]。日志事件的时间戳是一个特殊参数，因为它允许将其他参数放在时间上下文中，这是时间序列分析所必需的。然而，时间戳本身可以用于时间嵌入（TE）并作为神经网络的输入[[31](#bib.bib31)]。为此，Li等人[[50](#bib.bib50)]通过应用软一次性编码生成事件发生之间时间差的序列向量。
- en: While aforementioned representations are used in different variations in several
    publications, Graphs (G) are an entirely different approach that is only employed
    by Wan et al. [[63](#bib.bib63)]. The key idea is to transform event sequences
    into session graphs and then apply neural networks that are specifically designed
    for these data structures. Another less common strategy for encoding dependencies
    between log messages is the so-called Transfer Matrix (TM). In particular, the
    $d\times d$-dimensional matrix encodes the probabilities that any of the $d$ log
    keys follows another [[79](#bib.bib79)].
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述表示在多个出版物中以不同的变体使用，但图形（G）是完全不同的方法，仅由Wan等人[[63](#bib.bib63)]使用。关键思想是将事件序列转换为会话图，然后应用专门为这些数据结构设计的神经网络。另一种编码日志消息之间依赖关系的较少见的策略是所谓的转移矩阵（TM）。特别地，$d\times
    d$维矩阵编码了任何$d$个日志键跟随另一个键的概率[[79](#bib.bib79)]。
- en: IV-D Anomaly Detection Techniques
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 异常检测技术
- en: The previous sections described methods to prepare the input log data for ingestion
    by neural networks. However, in many cases it is non-trivial to retrieve the information
    whether a data sample presented to the neural network is anomalous or not as there
    are no dedicated output nodes for anomalies that are not known beforehand. This
    section therefore outlines the anomaly detection techniques applied by the reviewed
    approaches. First, we state different types of anomalies that are commonly targeted
    by approaches. Second, we investigate how the detection or classification result
    is retrieved from the network output. Finally, we state different decision criteria
    that are used to differentiate normal from anomalous samples based on these resulting
    measures.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节描述了准备输入日志数据以供神经网络处理的方法。然而，在许多情况下，检索数据样本是否异常并非易事，因为没有专门的输出节点来处理事先未知的异常。因此，本节概述了被评审方法应用的异常检测技术。首先，我们列出了不同类型的异常，这些异常通常是方法所关注的。其次，我们调查如何从网络输出中获取检测或分类结果。最后，我们列出了用于根据这些结果措施区分正常样本和异常样本的不同决策标准。
- en: IV-D1 Anomaly types
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 异常类型
- en: 'The reviewed approaches address different types of anomalies as outlined in
    Sect. [II-A3](#S2.SS1.SSS3 "II-A3 Anomaly Detection ‣ II-A Preliminary Definitions
    ‣ II Background ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")
    (AD-1). Outliers (OUT) are single log events that do not fit to the overall structure
    of the data set. Most commonly, outlier events are detected based on their unusual
    parameter values [[46](#bib.bib46)], token sequences [[64](#bib.bib64), [68](#bib.bib68)],
    or occurrence times [[31](#bib.bib31)]. Comparatively few approaches consider
    outliers since the majority of reviewed approaches focus on collective anomalies,
    in particular, involving sequences of events.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '被评审的方法处理了不同类型的异常，如第[II-A3节](#S2.SS1.SSS3 "II-A3 Anomaly Detection ‣ II-A Preliminary
    Definitions ‣ II Background ‣ Deep Learning for Anomaly Detection in Log Data:
    A Survey")（AD-1）所述。离群点（OUT）是指那些不符合数据集整体结构的单个日志事件。最常见的是，离群点事件是基于其异常参数值[[46](#bib.bib46)]、令牌序列[[64](#bib.bib64)、[68](#bib.bib68)]或发生时间[[31](#bib.bib31)]被检测到的。相对较少的方法考虑离群点，因为大多数被评审的方法侧重于集体异常，特别是涉及事件序列的异常。'
- en: Sequential (SEQ) anomalies are detected when execution paths change, i.e., the
    applications that generate logs execute events differently than before. This could
    result in additional, missing, or differently ordered events within otherwise
    normal event sequences as well as completely new sequences that could even involve
    previously unseen event types. A common method to detect these anomalies is to
    check whether a specific event type in a sequence of events is expected to occur
    given all the events that occur before or thereafter.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行路径发生变化时，即生成日志的应用程序以不同于以前的方式执行事件时，会检测到顺序（SEQ）异常。这可能导致在其他正常事件序列中出现额外、缺失或顺序不同的事件，以及可能涉及以前未见过的事件类型的全新序列。检测这些异常的常见方法是检查在事件序列中是否期望特定事件类型发生，基于所有发生在之前或之后的事件。
- en: While the detection of sequential anomalies inherently assumes that events occur
    as ordered sequences, frequency (FREQ) anomalies only consider the number of event
    occurrences. Nonetheless, event frequencies may be used to infer dependencies
    between events, for example, the numbers of events related to opening and closing
    files should be the same as every file will eventually be closed and needs to
    be opened before doing so [[26](#bib.bib26)]. The main idea for detecting frequency
    anomalies is that changes of system behavior affect the number of usual event
    occurrences that are most often counted within time windows.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然序列异常的检测固有地假设事件按顺序发生，但频率异常（FREQ）仅考虑事件发生的次数。尽管如此，事件频率可以用于推断事件之间的依赖关系，例如，相关于打开和关闭文件的事件数量应该相同，因为每个文件最终都会被关闭，并且需要在关闭之前打开[[26](#bib.bib26)]。检测频率异常的主要思想是系统行为的变化会影响通常在时间窗口内计数的事件发生次数。
- en: Some approaches also consider anomalies that base on certain quantitatively
    expressed properties of multiple log events that go beyond event counts, such
    as their inter-arrival times [[50](#bib.bib50)] or seasonal occurrence patterns
    [[82](#bib.bib82)]. We refer to them as statistical (STAT) anomalies, because
    their detection generally assumes that the event occurrences follow specific stable
    distributions over time. The following section describes how the output of the
    neural networks is used to determine the aforementioned types of anomalies.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法还考虑了基于多个日志事件的某些定量表达属性的异常，这些属性超出了事件计数，例如它们的到达间隔时间[[50](#bib.bib50)]或季节性发生模式[[82](#bib.bib82)]。我们将它们称为统计异常（STAT），因为它们的检测通常假设事件发生遵循特定的稳定分布。以下部分描述了神经网络输出如何用于确定上述类型的异常。
- en: IV-D2 Network output
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 网络输出
- en: In general, the output of a neural network consists either of a single node
    or multiple nodes in its final layer (AD-2). Accordingly, the resulting value
    extracted from the network is a scalar or vector of numeric values. One possibility
    is to consider these results as an anomaly score that expresses to what degree
    the log events presented to the network represent an anomaly or not. As these
    scores are generally difficult to interpret on their own, it is usually necessary
    to compare them with some threshold. In binary classification (BIN) this idea
    is used to estimate whether the input presented to the neural network is either
    normal or anomalous. For supervised approaches the numeric output can be interpreted
    as probabilities that the input corresponds to either class. On the other hand,
    anomaly scores that are generated by semi- or unsupervised approaches are generally
    not normalized, e.g., the distance between the input and the center of a hyper-spherical
    cluster can become arbitrarily large, and therefore need to be compared to empirically
    determined thresholds. Similarly, Input vector transformations (TRA) that transform
    the input into a new vector space and generate clusters for normal data are capable
    of detecting outliers by their large distances to cluster centers. Another related
    method is to leverage the reconstruction error (RE) of Autoencoders that first
    encode the input data in a lower dimensional space and then attempt to reconstruct
    them to their original form. In this case, input samples are considered anomalous
    if they are difficult to reconstruct, i.e., yield a large reconstruction error,
    because they do not correspond to the normal data that the network was trained
    on.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络的输出由最终层（AD-2）中的单个节点或多个节点组成。因此，从网络中提取的结果是一个标量或数值向量。一个可能性是将这些结果视为异常分数，该分数表示呈现给网络的日志事件在多大程度上代表异常。由于这些分数通常难以单独解释，通常需要将其与某个阈值进行比较。在二分类（BIN）中，这种想法用于估计呈现给神经网络的输入是正常的还是异常的。对于监督学习方法，数值输出可以解释为输入对应于某个类别的概率。另一方面，由半监督或无监督方法生成的异常分数通常是未归一化的，例如，输入与超球形簇中心之间的距离可以变得任意大，因此需要与经验确定的阈值进行比较。类似地，将输入向量转换（TRA）为新向量空间并为正常数据生成簇的方法可以通过距离簇中心的距离来检测异常值。另一种相关的方法是利用自动编码器（Autoencoders）的重构误差（RE），自动编码器首先将输入数据编码到一个较低维空间，然后尝试将其重构为原始形式。在这种情况下，如果输入样本难以重构，即产生大的重构误差，则被认为是异常的，因为它们不对应于网络所训练的正常数据。
- en: While aforementioned approaches pursue binary classification that separates
    normal from anomalous input, there are also concepts that are capable of differentiating
    between more than two classes. Multi-class classification (MC) assigns distinct
    labels to specific types of anomalies but requires supervised learning in order
    to capture the patterns specific to these classes in the training phase. To resolve
    this issue, it is also possible to consider event types as the target of classification.
    The most common approach for this is to train the models to predict the next log
    key following a sequence of observed log events. When a softmax function is used
    as an activation for the output, this prediction yields a probability distribution
    (PRD) with the individual probabilities for each log key. The problem of predicting
    the next log event in a sequence can also be formulated as a regression task when
    events are considered as numeric vectors (VEC), in particular, semantic or parameter
    vectors. Thereby, the neural network outputs a vector representing the expected
    event vector instead of the respective event type.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: IV-D3 Detection method
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The different strategies for obtaining the network output described in the previous
    section already give a rough idea on the methods used to differentiate normal
    from anomalous behavior and eventually report anomalies (AD-3). When the network
    output directly corresponds to a particular label (LAB), for example, as accomplished
    by binary classification, it is simple to generate anomalies for all samples that
    are labeled as anomalous. For all approaches that output some kind of numeric
    value or anomaly score it is straightforward to use a threshold (THR) for differentiation.
    This threshold is also useful to tune the detection performance of the approach
    and find an acceptable tradeoff between TPR and FPR by empirical experimentation.
    Another approach is to model the anomaly scores obtained from the network as statistical
    distributions. In particular, Du et al. [[24](#bib.bib24)] and Xie et al. [[72](#bib.bib72)]
    use a Gaussian distribution to detect parameter vectors with errors outside of
    specific confidence intervals as anomalous. A different approach is proposed by
    Otomo et al. [[53](#bib.bib53)], who apply clustering on the reconstruction errors
    and detect all samples that are sufficiently far away from the normal clusters
    as anomalies.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d447c9404dce11b08c85ee3d58eb5c94.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Most common combinations of network output types and detection techniques.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'When the output of the neural network is a multi-class probability distribution
    for known log keys it is common to consider the top $n$ log keys with the highest
    probabilities (TOP) as candidates for classification. Thereby, an anomaly is only
    detected if the actual type of the log event is not within the set of candidates.
    The number of considered candidates $n$ regulates the tradeoff between TPR and
    FPR analogous to the aforementioned threshold. Figure [4](#S4.F4 "Figure 4 ‣ IV-D3
    Detection method ‣ IV-D Anomaly Detection Techniques ‣ IV Survey Results ‣ Deep
    Learning for Anomaly Detection in Log Data: A Survey") shows how the network output
    relates to the applied detection techniques. Both BIN and MC rely on supervised
    learning and are therefore able to directly assign labels to new input samples.
    All other techniques enable semi- or unsupervised training. In particular, RE,
    TRA, and VEC produce anomaly scores that are compared against thresholds, while
    PRD is typically compared against the top log keys with highest probabilities.
    Note that there are some exceptions to this overall pattern. For example, the
    approach by Yang et al. [[74](#bib.bib74)] supports semi-supervised training through
    the use of probabilistic labels, and the approaches by Zhang et al. [[19](#bib.bib19)]
    and Syngal et al. [[61](#bib.bib61)] are supervised despite relying on reconstruction
    errors.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '当神经网络的输出是已知日志键的多类别概率分布时，通常会将概率最高的前 $n$ 个日志键（TOP）视为分类的候选者。因此，只有当日志事件的实际类型不在候选者集合中时，才会检测到异常。所考虑的候选者数量
    $n$ 调节了TPR和FPR之间的权衡，类似于前述的阈值。图 [4](#S4.F4 "Figure 4 ‣ IV-D3 Detection method ‣
    IV-D Anomaly Detection Techniques ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey") 显示了网络输出如何与应用的检测技术相关。BIN 和 MC 依赖于监督学习，因此能够直接为新的输入样本分配标签。所有其他技术则支持半监督或无监督训练。特别是，RE、TRA
    和 VEC 产生异常分数，并与阈值进行比较，而 PRD 通常与概率最高的日志键进行比较。注意，这种总体模式有一些例外。例如，杨等人[[74](#bib.bib74)]的做法通过使用概率标签支持半监督训练，而张等人[[19](#bib.bib19)]和
    Syngal 等人[[61](#bib.bib61)]的做法虽然依赖于重构误差，但仍是监督学习。'
- en: IV-E Evaluation & Reproducibility
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 评估与可重复性
- en: This section provides an overview of evaluations carried out in the reviewed
    publications. We present a list of openly available data sets that are useful
    for evaluations and state commonly used evaluation metrics and benchmarks. We
    also discuss reproducibility of evaluations with respect to the availability of
    open-source implementations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对所审阅出版物中进行的评估的概述。我们列出了对评估有用的公开数据集，并说明了常用的评估指标和基准。我们还讨论了关于开源实现的可用性对评估的可重复性的影响。
- en: IV-E1 Data sets
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E1 数据集
- en: 'Data sets are essential in scientific publications to validate the approach
    and show improvements over state-of-the-art detection rates (ER-1). Our literature
    review reveals that there are only few data sets that are commonly used when evaluating
    log data anomaly detection approaches using deep learning. Table [IV](#S4.T4 "TABLE
    IV ‣ IV-E1 Data sets ‣ IV-E Evaluation & Reproducibility ‣ IV Survey Results ‣
    Deep Learning for Anomaly Detection in Log Data: A Survey") shows an overview
    of all data sets used in the reviewed publications. As visible in the table, the
    vast majority of evaluations rely on only four data sets (HDFS, BGL, Thunderbird,
    and OpenStack). In the following, we briefly describe each data set.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集在科学出版物中至关重要，以验证方法并展示相较于最新检测率的改进（ER-1）。我们的文献综述表明，当使用深度学习评估日志数据异常检测方法时，常用的数据集非常有限。表
    [IV](#S4.T4 "TABLE IV ‣ IV-E1 Data sets ‣ IV-E Evaluation & Reproducibility ‣
    IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")
    显示了所有在所审阅出版物中使用的数据集的概况。从表中可以看出，绝大多数评估仅依赖于四个数据集（HDFS、BGL、Thunderbird 和 OpenStack）。接下来，我们简要描述每个数据集。'
- en: 'TABLE IV: Common public log data sets'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：常见公共日志数据集
- en: '| Data set | Year | Use-case | Labels | Sessions | Anomaly sources | Used in
    evaluation |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 用例 | 标签 | 会话 | 异常源 | 用于评估 |'
- en: '| HDFS [[20](#bib.bib20)] | 2009 | High-perf. comp. | ✓ | ✓ | Failures | [[19](#bib.bib19),
    [78](#bib.bib78), [65](#bib.bib65), [31](#bib.bib31), [27](#bib.bib27), [82](#bib.bib82),
    [24](#bib.bib24), [34](#bib.bib34), [26](#bib.bib26), [52](#bib.bib52), [29](#bib.bib29),
    [73](#bib.bib73), [1](#bib.bib1), [50](#bib.bib50), [72](#bib.bib72), [66](#bib.bib66),
    [60](#bib.bib60), [83](#bib.bib83), [74](#bib.bib74), [69](#bib.bib69), [55](#bib.bib55),
    [33](#bib.bib33), [56](#bib.bib56), [41](#bib.bib41), [42](#bib.bib42), [77](#bib.bib77),
    [76](#bib.bib76), [51](#bib.bib51), [43](#bib.bib43), [45](#bib.bib45), [75](#bib.bib75),
    [62](#bib.bib62), [37](#bib.bib37), [63](#bib.bib63), [35](#bib.bib35), [79](#bib.bib79),
    [70](#bib.bib70), [59](#bib.bib59), [47](#bib.bib47), [81](#bib.bib81), [71](#bib.bib71),
    [48](#bib.bib48)] |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| HDFS [[20](#bib.bib20)] | 2009 | 高性能计算 | ✓ | ✓ | 故障 | [[19](#bib.bib19),
    [78](#bib.bib78), [65](#bib.bib65), [31](#bib.bib31), [27](#bib.bib27), [82](#bib.bib82),
    [24](#bib.bib24), [34](#bib.bib34), [26](#bib.bib26), [52](#bib.bib52), [29](#bib.bib29),
    [73](#bib.bib73), [1](#bib.bib1), [50](#bib.bib50), [72](#bib.bib72), [66](#bib.bib66),
    [60](#bib.bib60), [83](#bib.bib83), [74](#bib.bib74), [69](#bib.bib69), [55](#bib.bib55),
    [33](#bib.bib33), [56](#bib.bib56), [41](#bib.bib41), [42](#bib.bib42), [77](#bib.bib77),
    [76](#bib.bib76), [51](#bib.bib51), [43](#bib.bib43), [45](#bib.bib45), [75](#bib.bib75),
    [62](#bib.bib62), [37](#bib.bib37), [63](#bib.bib63), [35](#bib.bib35), [79](#bib.bib79),
    [70](#bib.bib70), [59](#bib.bib59), [47](#bib.bib47), [81](#bib.bib81), [71](#bib.bib71),
    [48](#bib.bib48)] |'
- en: '| BlueGene/L (BGL) [[94](#bib.bib94)] | 2007 | High-perf. comp. | ✓ | - | Failures
    | [[78](#bib.bib78), [65](#bib.bib65), [24](#bib.bib24), [34](#bib.bib34), [26](#bib.bib26),
    [68](#bib.bib68), [52](#bib.bib52), [29](#bib.bib29), [40](#bib.bib40), [73](#bib.bib73),
    [1](#bib.bib1), [44](#bib.bib44), [50](#bib.bib50), [66](#bib.bib66), [28](#bib.bib28),
    [38](#bib.bib38), [39](#bib.bib39), [74](#bib.bib74), [69](#bib.bib69), [58](#bib.bib58),
    [41](#bib.bib41), [32](#bib.bib32), [51](#bib.bib51), [43](#bib.bib43), [45](#bib.bib45),
    [37](#bib.bib37), [63](#bib.bib63), [79](#bib.bib79), [70](#bib.bib70), [59](#bib.bib59),
    [47](#bib.bib47), [46](#bib.bib46), [48](#bib.bib48)] |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| BlueGene/L (BGL) [[94](#bib.bib94)] | 2007 | 高性能计算 | ✓ | - | 故障 | [[78](#bib.bib78),
    [65](#bib.bib65), [24](#bib.bib24), [34](#bib.bib34), [26](#bib.bib26), [68](#bib.bib68),
    [52](#bib.bib52), [29](#bib.bib29), [40](#bib.bib40), [73](#bib.bib73), [1](#bib.bib1),
    [44](#bib.bib44), [50](#bib.bib50), [66](#bib.bib66), [28](#bib.bib28), [38](#bib.bib38),
    [39](#bib.bib39), [74](#bib.bib74), [69](#bib.bib69), [58](#bib.bib58), [41](#bib.bib41),
    [32](#bib.bib32), [51](#bib.bib51), [43](#bib.bib43), [45](#bib.bib45), [37](#bib.bib37),
    [63](#bib.bib63), [79](#bib.bib79), [70](#bib.bib70), [59](#bib.bib59), [47](#bib.bib47),
    [46](#bib.bib46), [48](#bib.bib48)] |'
- en: '| Thunderbird [[94](#bib.bib94)] | 2007 | High-perf. comp. | ✓ | - | Failures
    | [[78](#bib.bib78), [68](#bib.bib68), [29](#bib.bib29), [40](#bib.bib40), [44](#bib.bib44),
    [28](#bib.bib28), [38](#bib.bib38), [39](#bib.bib39), [64](#bib.bib64), [42](#bib.bib42),
    [76](#bib.bib76), [43](#bib.bib43), [59](#bib.bib59), [48](#bib.bib48)] |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Thunderbird [[94](#bib.bib94)] | 2007 | 高性能计算 | ✓ | - | 故障 | [[78](#bib.bib78),
    [68](#bib.bib68), [29](#bib.bib29), [40](#bib.bib40), [44](#bib.bib44), [28](#bib.bib28),
    [38](#bib.bib38), [39](#bib.bib39), [64](#bib.bib64), [42](#bib.bib42), [76](#bib.bib76),
    [43](#bib.bib43), [59](#bib.bib59), [48](#bib.bib48)] |'
- en: '| OpenStack [[24](#bib.bib24)] | 2017 | Virtual machines | ✓ | ✓ | Failures
    | [[82](#bib.bib82), [24](#bib.bib24), [30](#bib.bib30), [40](#bib.bib40), [54](#bib.bib54),
    [38](#bib.bib38), [39](#bib.bib39), [83](#bib.bib83), [56](#bib.bib56), [45](#bib.bib45),
    [67](#bib.bib67), [47](#bib.bib47), [81](#bib.bib81)] |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| OpenStack [[24](#bib.bib24)] | 2017 | 虚拟机 | ✓ | ✓ | 故障 | [[82](#bib.bib82),
    [24](#bib.bib24), [30](#bib.bib30), [40](#bib.bib40), [54](#bib.bib54), [38](#bib.bib38),
    [39](#bib.bib39), [83](#bib.bib83), [56](#bib.bib56), [45](#bib.bib45), [67](#bib.bib67),
    [47](#bib.bib47), [81](#bib.bib81)] |'
- en: '| Hadoop [[95](#bib.bib95)] | 2016 | High-perf. comp. | ✓ | ✓ | Failures |
    [[32](#bib.bib32), [45](#bib.bib45), [35](#bib.bib35), [58](#bib.bib58)] |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Hadoop [[95](#bib.bib95)] | 2016 | 高性能计算 | ✓ | ✓ | 故障 | [[32](#bib.bib32),
    [45](#bib.bib45), [35](#bib.bib35), [58](#bib.bib58)] |'
- en: '| Spirit [[94](#bib.bib94)] | 2007 | High-perf. comp. | ✓ | - | Failures |
    [[68](#bib.bib68), [28](#bib.bib28), [48](#bib.bib48)] |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Spirit [[94](#bib.bib94)] | 2007 | 高性能计算 | ✓ | - | 故障 | [[68](#bib.bib68),
    [28](#bib.bib28), [48](#bib.bib48)] |'
- en: '| Digital Corpora [[96](#bib.bib96)] | 2009 | OS logs | - | - | Failures |
    [[58](#bib.bib58), [57](#bib.bib57)] |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Digital Corpora [[96](#bib.bib96)] | 2009 | 操作系统日志 | - | - | 故障 | [[58](#bib.bib58),
    [57](#bib.bib57)] |'
- en: '| DFRWS 2009 [[97](#bib.bib97)] | 2009 | OS logs | - | - | Exfiltration | [[58](#bib.bib58),
    [57](#bib.bib57)] |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| DFRWS 2009 [[97](#bib.bib97)] | 2009 | 操作系统日志 | - | - | 数据泄露 | [[58](#bib.bib58),
    [57](#bib.bib57)] |'
- en: '| Honeynet 2011 [[98](#bib.bib98)] | 2011 | OS logs | - | - | Intrusions |
    [[58](#bib.bib58), [57](#bib.bib57)] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Honeynet 2011 [[98](#bib.bib98)] | 2011 | 操作系统日志 | - | - | 入侵 | [[58](#bib.bib58),
    [57](#bib.bib57)] |'
- en: '| Windows [[99](#bib.bib99)] | 2020 | OS logs | - | - | - | [[58](#bib.bib58),
    [57](#bib.bib57)] |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Windows [[99](#bib.bib99)] | 2020 | 操作系统日志 | - | - | - | [[58](#bib.bib58),
    [57](#bib.bib57)] |'
- en: '| Linux Security Logs [[100](#bib.bib100)] | 2020 | OS logs | - | - | Intrusions
    | [[57](#bib.bib57)] |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Linux 安全日志 [[100](#bib.bib100)] | 2020 | 操作系统日志 | - | - | 入侵 | [[57](#bib.bib57)]
    |'
- en: '| Honeynet 2010 [[101](#bib.bib101)] | 2010 | OS logs | - | - | Intrusions
    | [[58](#bib.bib58)] |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Honeynet 2010 [[101](#bib.bib101)] | 2010 | 操作系统日志 | - | - | 入侵 | [[58](#bib.bib58)]
    |'
- en: '| Android [[99](#bib.bib99)] | 2020 | Mobile OS logs | - | - | - | [[50](#bib.bib50)]
    |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Android [[99](#bib.bib99)] | 2020 | 移动操作系统日志 | - | - | - | [[50](#bib.bib50)]
    |'
- en: '| HPC RAS [[102](#bib.bib102)] | 2011 | High-perf. comp. | - | ✓ | Failures
    | [[28](#bib.bib28)] |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| HPC RAS [[102](#bib.bib102)] | 2011 | 高性能计算 | - | ✓ | 失败 | [[28](#bib.bib28)]
    |'
- en: '| Spark [[99](#bib.bib99)] | 2020 | High-perf. comp. | - | - | Failures | [[58](#bib.bib58)]
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Spark [[99](#bib.bib99)] | 2020 | 高性能计算 | - | - | 失败 | [[58](#bib.bib58)]
    |'
- en: '| Zookeeper [[99](#bib.bib99)] | 2007 | High-perf. comp. | - | - | Failures
    | [[58](#bib.bib58)] |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Zookeeper [[99](#bib.bib99)] | 2007 | 高性能计算 | - | - | 失败 | [[58](#bib.bib58)]
    |'
- en: 'The HDFS log data set stems from the Hadoop Distributed File System (HDFS)
    running on a high-performance computing cluster with 203 nodes that computes many
    standard MapReduce jobs. More than 24 million logs are collected over a period
    of two days. The data set comprises sequences of heterogeneous log events for
    specific file blocks that act as identifiers for sessions. Some of the event sequences
    correspond to anomalous execution paths that are mostly related to performance
    issues such as write exceptions, which were manually labeled [[103](#bib.bib103)].
    The sample logs shown in Fig. [3](#S4.F3 "Figure 3 ‣ IV-C Log Data Preparation
    ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")
    are simplified versions of the logs from the HDFS data set.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 'HDFS 日志数据集来源于 Hadoop 分布式文件系统（HDFS），运行在一个包含 203 个节点的高性能计算集群上，计算许多标准 MapReduce
    作业。收集了超过 2400 万条日志，时间跨度为两天。数据集包含针对特定文件块的异质日志事件序列，这些文件块作为会话标识符。一些事件序列对应于异常的执行路径，这些路径主要与性能问题有关，如写入异常，这些异常是手动标记的
    [[103](#bib.bib103)]。图 [3](#S4.F3 "Figure 3 ‣ IV-C Log Data Preparation ‣ IV Survey
    Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey") 中显示的示例日志是来自
    HDFS 数据集的简化版本。'
- en: The BGL data set comprises more than four million log events that were collected
    over more than 200 days from a BlueGene/L (BGL) supercomputer running at the Lawrence
    Livermore National Labs. The log events were generated with a severity field that
    allows to separate them into classes; however, the logs were additionally labeled
    manually by system administrators. The anomalies occurring in the logs correspond
    to both hardware and software problems. Other log data sets from the same family
    that are also presented in the study by Oliner et al. [[94](#bib.bib94)] are Thunderbird
    and Spirit. These data sets comprise system logs (syslog) and similarly comprise
    alerts related to system problems such as disk failures. The events in both data
    sets include automatically generated alert tags that can be used as labels. The
    HPC RAS data set [[102](#bib.bib102)] is another log data set from the same family
    of supercomputers. Reliability, Availability, and Serviceability (RAS) logs are
    usually used to understand the reasons for system failure. However, this log data
    set does not include labels for anomalies.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: BGL 数据集包含超过四百万条日志事件，这些事件在劳伦斯利弗莫尔国家实验室运行的 BlueGene/L (BGL) 超级计算机上收集，时间超过 200
    天。这些日志事件生成了一个严重性字段，可以将其分为不同的类别；然而，日志还由系统管理员手动标记。日志中的异常对应于硬件和软件问题。Oliner 等人 [[94](#bib.bib94)]
    研究中还介绍了其他同类日志数据集，包括 Thunderbird 和 Spirit。这些数据集包含系统日志（syslog），并包含与系统问题相关的警报，例如磁盘故障。这两个数据集中的事件都包括可以用作标签的自动生成的警报标签。HPC
    RAS 数据集 [[102](#bib.bib102)] 是来自同一系列超级计算机的另一个日志数据集。可靠性、可用性和可维护性（RAS）日志通常用于了解系统故障的原因。然而，这个日志数据集不包括异常标签。
- en: The OpenStack data set was collected from an OpenStack platform where automatic
    scripts continuously and randomly carry out tasks related to handling of virtual
    machines, including creation, pausing, deletion, etc. Other than aforementioned
    data sets that mostly comprise randomly occurring failures, the authors of the
    OpenStack data set purposefully injected anomalies at specific points in time,
    including timeouts and errors. The events include instance identifiers that can
    be used to identify sessions [[24](#bib.bib24)].
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack 数据集收集自一个 OpenStack 平台，自动脚本持续且随机地执行与虚拟机处理相关的任务，包括创建、暂停、删除等。与前述数据集大多包含随机发生的故障不同，OpenStack
    数据集的作者有意在特定时间点注入异常，包括超时和错误。事件包括实例标识符，可用于识别会话 [[24](#bib.bib24)]。
- en: Similar to the HDFS data, the Hadoop data set comprises logs from a computing
    cluster that runs the MapReduce jobs WordCount and PageRank. After an initial
    training phase, the authors purposefully trigger failures on the nodes, in particular,
    by shutting down a machine, disconnecting the network, and filling up the hard
    disk of one server. The logs are separated into different files according to application
    identifiers that act similar to session identifiers and are also used by the authors
    to assign labels to anomalous program executions [[95](#bib.bib95)].
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 HDFS 数据，Hadoop 数据集包含来自运行 MapReduce 作业 WordCount 和 PageRank 的计算集群的日志。在初始训练阶段之后，作者会故意在节点上触发故障，特别是通过关闭一台机器、断开网络连接和填满一个服务器的硬盘来触发故障。根据类似于会话标识符的应用标识符，将日志分为不同文件，并由作者用于为异常的程序执行分配标签
    [[95](#bib.bib95)]。
- en: Loghub [[99](#bib.bib99)] comprises several data sets that are used in evaluations,
    including logs from high-performance computing systems. The Spark data set contains
    logs from the distributed data processing engine Apache Spark running on 32 machines.
    The logs include normal and anomalous executions, but are not labeled. ZooKeeper
    is another Apache service used in distributed computing and configuration management.
    Loghub also comprises logs from conventional operating systems. The Windows log
    data set was collected on a laboratory Windows 7 machine by monitoring CBS (Component
    Based Servicing), which operates on a package and update level. Similarly, the
    Android data set was collected from a mobile phone in a laboratory setting. Both
    data sets are not labeled and do not involve any purposefully injected anomalies.
    Logs from the Linux operating system are provided in the disk images of the Digital
    Corpora, where failures such as invalid authentications occur in the data [[96](#bib.bib96)].
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Loghub [[99](#bib.bib99)] 包括用于评估的多个数据集，包括来自高性能计算系统的日志。Spark 数据集包含来自分布式数据处理引擎
    Apache Spark 在32台机器上运行的日志。这些日志包含正常和异常执行情况，但没有标记。ZooKeeper 是另一个用于分布式计算和配置管理的 Apache
    服务。Loghub 还包括来自传统操作系统的日志。Windows 日志数据集是通过监视基于组件的维护 (CBS) 在实验室 Windows 7 机器上收集的，CBS
    用于对软件包和更新级别进行操作。类似地，Android 数据集是从实验室环境中的一部手机收集的。这两个数据集没有标记，并且不涉及任何有意注入的异常。来自 Linux
    操作系统的日志可以在 Digital Corpora 的磁盘镜像中找到，其中包含诸如无效身份验证之类的故障 [[96](#bib.bib96)]。
- en: When it comes to the detection of anomalous behavior, all aforementioned data
    sets mainly provide failure events that are generated as part of legitimate system
    usage. However, there are also data sets that instead involve events generated
    from malicious activities and thus enable evaluation of anomaly-based intrusion
    detection systems. For example, the DFRWS 2009 data set contains system logs from
    Linux devices that involve data exfiltration, unauthorized accesses, as well as
    the use of backdoor software [[97](#bib.bib97)]. The Honeynet 2010 [[101](#bib.bib101)]
    and Honeynet 2011 [[98](#bib.bib98)] data sets comprise common log files from
    compromised Linux machines that were illegitimately accessed. The Public Security
    Log Sharing Site [[100](#bib.bib100)] provides Linux Security Logs from diverse
    sources and affected by different real-world intrusions, such as brute-force attacks.
    Unfortunately, none of these security log files come with labels for malicious
    events and thus authors need to generate their own ground truths to evaluate their
    approaches on these data sets [[57](#bib.bib57)].
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到异常行为的检测时，前述所有数据集主要提供作为合法系统使用的一部分生成的故障事件。然而，也有一些数据集包括了由恶意活动生成的事件，从而可以评估基于异常的入侵检测系统。例如，DFRWS
    2009 数据集包含涉及数据渗透、未经授权访问以及后门软件使用的 Linux 设备系统日志 [[97](#bib.bib97)]。Honeynet 2010
    [[101](#bib.bib101)] 和 Honeynet 2011 [[98](#bib.bib98)] 数据集包括来自被非法访问的受攻击 Linux
    机器的常见日志文件。公共安全日志共享站点 [[100](#bib.bib100)] 提供了来自不同来源且受不同现实世界入侵影响的 Linux 安全日志，例如暴力破解攻击。不幸的是，这些安全日志文件中没有恶意事件的标签，因此作者需要生成自己的基准来评估他们在这些数据集上的方法
    [[57](#bib.bib57)]。
- en: IV-E2 Evaluation metrics
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E2 评估指标
- en: Quantitative evaluation (ER-2) of anomaly detection approaches typically revolves
    around counting the numbers of correctly detected anomalous samples as true positives
    ($TP$), incorrectly detected non-anomalous samples as false positives ($FP$),
    incorrectly undetected anomalous samples as false negatives ($FN$), and correctly
    undetected non-anomalous samples as true negatives ($TN$). In the most basic setting
    where events are labeled individually and samples represent single events (e.g.,
    as in the BGL data set), it is relatively straightforward to evaluate detected
    events with binary classification [[38](#bib.bib38), [40](#bib.bib40)]. Some of
    the reviewed papers additionally consider a multi-class classification problem
    for data sets where different types of failures have distinct labels by computing
    the averages of evaluation metrics over all classes [[59](#bib.bib59)] or plotting
    confusion matrices [[36](#bib.bib36)].
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测方法的定量评估（ER-2）通常围绕将正确检测到的异常样本计为真正的正例（$TP$）、错误检测的非异常样本计为假正例（$FP$）、错误未检测到的异常样本计为假负例（$FN$）以及正确未检测到的非异常样本计为真正的负例（$TN$）。在事件被逐个标记且样本表示单个事件的最基本设置中（例如，在BGL数据集中），使用二分类方法评估检测事件相对直接[[38](#bib.bib38),
    [40](#bib.bib40)]。一些审阅的论文还考虑了多类别分类问题，对于标签有不同类型故障的数据集，通过计算所有类别的评估指标的平均值[[59](#bib.bib59)]或绘制混淆矩阵[[36](#bib.bib36)]。
- en: Given that log events are sometimes aggregated with diverse methods prior to
    detection, it stands to reason that there are different ways to determine whether
    a sample is anomalous or not, and whether it counts as a correct detection or
    not. For example, aggregation of logs in windows could require to count detected
    events as true positives as long as they are close enough to the actual anomaly
    in the event sequence [[30](#bib.bib30), [61](#bib.bib61)]. Since a majority of
    the reviewed papers rely on the HDFS data set where labels are only available
    for whole event sessions rather than single events, the most common method to
    compute aforementioned metrics relies on counting of in-/correctly identified
    non-/anomalous sessions [[34](#bib.bib34), [37](#bib.bib37), [47](#bib.bib47),
    [49](#bib.bib49), [66](#bib.bib66)].
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到日志事件有时会在检测之前通过多种方法进行汇总，因此确定样本是否异常以及是否被正确检测的方式也可能不同。例如，在窗口中汇总日志可能需要将检测到的事件计为真正的正例，只要它们在事件序列中足够接近实际异常[[30](#bib.bib30),
    [61](#bib.bib61)]。由于大多数审阅的论文依赖于HDFS数据集，其中标签仅适用于整个事件会话而不是单个事件，因此计算上述指标的最常见方法依赖于计算错误/正确识别的非/异常会话[[34](#bib.bib34),
    [37](#bib.bib37), [47](#bib.bib47), [49](#bib.bib49), [66](#bib.bib66)]。
- en: Regardless of how the positive and negative samples are counted, almost all
    authors eventually evaluate their approaches using the well-known metrics precision
    ($P=\frac{TP}{TP+FP}$), recall or true positive rate ($R=TPR=\frac{TP}{TP+FN}$),
    false positive rate ($FPR=\frac{FP}{FP+TN}$) and F1-score ($F1=\frac{2\cdot P\cdot
    R}{P+R}$). Less common evaluation metrics are the accuracy ($ACC=\frac{TP+TN}{TP+TN+FP+FN}$)
    used in 15 publications as well as the area under curve which is computed for
    precision-recall-curves [[35](#bib.bib35)] and receiver operator characteristic
    (ROC) curves [[64](#bib.bib64), [46](#bib.bib46)]. Other metrics that are more
    specific to deep learning applications are the number of model parameters [[42](#bib.bib42),
    [65](#bib.bib65)] and time to train models or run the detection (ER-3) [[51](#bib.bib51),
    [41](#bib.bib41), [56](#bib.bib56), [33](#bib.bib33), [36](#bib.bib36), [72](#bib.bib72)].
    Some authors also assess characteristics of their approaches that go beyond standard
    anomaly detection evaluations, for example, whether training on combinations of
    multiple data sets improves the overall performance of classification [[45](#bib.bib45)]
    or whether their approaches are robust against changes of log patterns over time
    [[19](#bib.bib19), [47](#bib.bib47), [45](#bib.bib45)].
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 无论正样本和负样本如何计算，几乎所有作者最终都使用知名的指标来评估他们的方法，包括精度（$P=\frac{TP}{TP+FP}$）、召回率或真正例率（$R=TPR=\frac{TP}{TP+FN}$）、假阳性率（$FPR=\frac{FP}{FP+TN}$）和
    F1 分数（$F1=\frac{2\cdot P\cdot R}{P+R}$）。不太常见的评估指标有 15 篇文献中使用的准确率（$ACC=\frac{TP+TN}{TP+TN+FP+FN}$），以及为精度-召回曲线
    [[35](#bib.bib35)] 和接收操作特征（ROC）曲线 [[64](#bib.bib64), [46](#bib.bib46)] 计算的曲线下面积。其他更具体于深度学习应用的指标包括模型参数数量
    [[42](#bib.bib42), [65](#bib.bib65)] 和训练模型或运行检测的时间（ER-3） [[51](#bib.bib51), [41](#bib.bib41),
    [56](#bib.bib56), [33](#bib.bib33), [36](#bib.bib36), [72](#bib.bib72)]。一些作者还评估了超越标准异常检测评估的特征，例如，训练是否通过组合多个数据集来提高分类的总体性能
    [[45](#bib.bib45)] 或他们的方法是否对日志模式随时间变化的鲁棒性 [[19](#bib.bib19), [47](#bib.bib47),
    [45](#bib.bib45)]。
- en: IV-E3 Benchmark approaches
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E3 基准方法
- en: 'Most publications compare the evaluation metrics stated in the previous section
    with benchmark approaches to show their improvements over the state-of-the-art
    (ER-4). Figure [5](#S4.F5 "Figure 5 ‣ IV-E3 Benchmark approaches ‣ IV-E Evaluation
    & Reproducibility ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in
    Log Data: A Survey") shows the most commonly used benchmarks in the reviewed publications.
    Note that we only considered approaches that appear in at least two different
    publications for this visualization.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数文献将前述的评估指标与基准方法进行比较，以展示它们相对于最新技术的改进（ER-4）。图 [5](#S4.F5 "图 5 ‣ IV-E3 基准方法
    ‣ IV-E 评估与可重复性 ‣ IV 调查结果 ‣ 深度学习在日志数据异常检测中的应用：一项调查") 显示了回顾文献中最常用的基准。请注意，我们仅考虑了在至少两篇不同文献中出现的方法进行可视化。
- en: '![Refer to caption](img/0acc8545080f3fe946e49fc980625460.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0acc8545080f3fe946e49fc980625460.png)'
- en: 'Figure 5: Overview of common benchmark approaches used in evaluations.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：评估中常用基准方法的概述。
- en: 'As visible in the figure, DeepLog [[24](#bib.bib24)] is the most commonly used
    benchmark with appearances in 38 out of the 62 reviewed publications. DeepLog
    relies on LSTM RNNs to predict upcoming log events in sequences and thereby disclose
    observed events as anomalies if they are expected to occur with low probabilities.
    The popularity of DeepLog for comparisons can be explained by the facts that DeepLog
    was the first to detect sequential anomalies in log data using deep learning (cf.
    Sect. [IV-A2](#S4.SS1.SSS2 "IV-A2 Citations ‣ IV-A Bibliometrics ‣ IV Survey Results
    ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")) and that open-source
    re-implementations are available online.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，DeepLog [[24](#bib.bib24)] 是最常用的基准，在 62 篇回顾文献中的 38 篇中出现。DeepLog 依赖于 LSTM
    RNNs 来预测序列中的即将发生的日志事件，从而将观察到的事件披露为异常，如果它们被期望以低概率发生。DeepLog 在比较中的流行可以解释为 DeepLog
    是首个使用深度学习检测日志数据中的序列异常的方法（参见 Sect. [IV-A2](#S4.SS1.SSS2 "IV-A2 引用 ‣ IV-A 文献计量 ‣
    IV 调查结果 ‣ 深度学习在日志数据异常检测中的应用：一项调查")），并且有开源的重新实现可在线获取。
- en: The second most commonly used benchmark leverages principal component analysis
    (PCA) and relies on event counts rather than sequences. In particular, Xu et al.
    [[103](#bib.bib103)] create message count vectors for event type identifiers and
    use PCA to transform them into subspaces where anomalies appear as outliers that
    have a high distance to all other samples. Lou et al. [[104](#bib.bib104)] also
    generate message count vectors but use Invariant Mining to discover linear relationships
    between log events that represent execution workflows. Event sequences that violate
    previously identified invariants are declared as anomalies.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种最常用的基准利用主成分分析（PCA）并依赖于事件计数而非序列。特别地，Xu等人 [[103](#bib.bib103)] 为事件类型标识符创建消息计数向量，并使用PCA将其转换到异常作为高距离的离群点的子空间中。Lou等人
    [[104](#bib.bib104)] 也生成消息计数向量，但使用Invariant Mining来发现表示执行工作流的日志事件之间的线性关系。违反先前识别的不变量的事件序列被声明为异常。
- en: LogCluster [[95](#bib.bib95)] generates vectors for log sequences and then clusters
    them using a similarity metric. The approach is mainly designed for log filtering
    but can also be applied for detection of unusual log patterns. Support vector
    machines (SVM) [[105](#bib.bib105)] are yet another method relying on event count
    vectors. However, other than PCA and invariant mining, SVM typically operate in
    supervised manner. To alleviate this issue and enable application in semi- or
    unsupervised cases, authors therefore resort to one-class SVM [[106](#bib.bib106)]
    or Support Vector Data Description (SVDD) [[107](#bib.bib107)].
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: LogCluster [[95](#bib.bib95)] 生成日志序列的向量，然后使用相似性度量对它们进行聚类。这种方法主要用于日志过滤，但也可以应用于检测不寻常的日志模式。支持向量机（SVM）
    [[105](#bib.bib105)] 是另一种依赖于事件计数向量的方法。然而，除了PCA和不变量挖掘之外，SVM通常以监督方式进行操作。为了缓解这一问题并使其能够在半监督或无监督情况下应用，作者因此求助于一类SVM
    [[106](#bib.bib106)] 或支持向量数据描述（SVDD） [[107](#bib.bib107)]。
- en: Similar to DeepLog and contrary to aforementioned conventional machine learning
    approaches, LogAnomaly [[26](#bib.bib26)] leverages LSTM RNNs to analyze log event
    sequences. To this end, they propose the so-called template2Vec embedding method
    to extract semantic vectors from the tokens that make up the events. LogRobust
    [[19](#bib.bib19)] also makes use of semantic vectors but is specifically designed
    to handle unknown log events that appear as part of software evolution.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 与DeepLog类似，但与上述传统机器学习方法相反，LogAnomaly [[26](#bib.bib26)] 利用LSTM RNNs来分析日志事件序列。为此，他们提出了所谓的template2Vec嵌入方法，以从组成事件的标记中提取语义向量。LogRobust
    [[19](#bib.bib19)] 也使用语义向量，但特别设计用于处理作为软件演变一部分出现的未知日志事件。
- en: Isolation Forest [[108](#bib.bib108)] is an anomaly detection technique where
    the analyzed data is recursively split until a single data instance is isolated
    from all other points; thereby, anomalous points are identified as they are expected
    to require fewer splits until their isolation. Logistic Regression [[109](#bib.bib109)]
    is a classifier for numeric input data that works best in linear classification
    cases [[57](#bib.bib57)]. Decision Tree [[110](#bib.bib110)] on the other hand
    is a supervised classification method where instances traverse a binary search
    tree. Thereby, each internal node splits the data by a specific predicate and
    each leaf of the tree determines the class of the instances.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Isolation Forest [[108](#bib.bib108)] 是一种异常检测技术，其中分析的数据被递归地拆分，直到一个数据实例从所有其他点中隔离出来；因此，异常点被识别为它们通常需要更少的拆分才能隔离。Logistic
    Regression [[109](#bib.bib109)] 是一种针对数值输入数据的分类器，在线性分类案例中效果最佳 [[57](#bib.bib57)]。另一方面，Decision
    Tree [[110](#bib.bib110)] 是一种监督分类方法，其中实例通过二叉搜索树进行遍历。因此，每个内部节点通过特定的谓词来拆分数据，每个叶节点确定实例的类别。
- en: Another benchmark that relies on deep learning are CNN. In particular, the approach
    by Lu et al. [[27](#bib.bib27)] semantically encodes sequences of event identifiers
    and embeds them into a matrix for convolution. Finally, nLSAlog [[73](#bib.bib73)]
    leverages LSTM RNNs and is thus similar to DeepLog. Out of all reviewed publications,
    only eight do not involve any benchmark approach for comparison.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种依赖于深度学习的基准是CNN。特别是，Lu等人 [[27](#bib.bib27)] 的方法对事件标识符序列进行语义编码，并将其嵌入到用于卷积的矩阵中。最后，nLSAlog
    [[73](#bib.bib73)] 利用LSTM RNNs，因此类似于DeepLog。在所有审阅的出版物中，只有八篇没有涉及任何基准方法进行比较。
- en: IV-E4 Reproducibility
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E4 可重复性
- en: Authors of scientific publications should pursue to enable reproducibility of
    their presented results for many reasons, including the possibility for others
    to validate the correctness of the approach, to extend the algorithms with additional
    features, to carry out their own experiments on other data sets, and to use the
    approach as benchmarks in new publications. We consider an approach reproducible
    when both the data used in the evaluation (ER-5) as well as the original source
    code (ER-6) are publicly available.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 科学出版物的作者应致力于使其呈现的结果具有可重复性，原因包括其他人可以验证方法的正确性、扩展算法的附加特性、在其他数据集上进行自己的实验，以及在新的出版物中将该方法用作基准。我们认为，当评估中使用的数据（ER-5）以及原始源代码（ER-6）都可以公开获得时，该方法是可重复的。
- en: 'As outlined in Sect. [IV-E1](#S4.SS5.SSS1 "IV-E1 Data sets ‣ IV-E Evaluation
    & Reproducibility ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in
    Log Data: A Survey"), a majority of the reviewed publications carry out their
    evaluations on the same few data sets that are publicly available. Some authors
    also evaluate their approaches on private data sets that are synthetically generated
    in testbeds [[30](#bib.bib30), [60](#bib.bib60), [80](#bib.bib80), [25](#bib.bib25),
    [67](#bib.bib67), [49](#bib.bib49), [66](#bib.bib66)], collected from academic
    institutions [[53](#bib.bib53), [74](#bib.bib74)], or obtained from industrial
    real-world applications [[19](#bib.bib19), [68](#bib.bib68), [61](#bib.bib61),
    [74](#bib.bib74), [36](#bib.bib36), [32](#bib.bib32), [35](#bib.bib35)]. Overall,
    55 out of the 62 reviewed publications involve evaluations on at least one of
    the publicly available data sets from Table [IV](#S4.T4 "TABLE IV ‣ IV-E1 Data
    sets ‣ IV-E Evaluation & Reproducibility ‣ IV Survey Results ‣ Deep Learning for
    Anomaly Detection in Log Data: A Survey").'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[IV-E1](#S4.SS5.SSS1 "IV-E1 Data sets ‣ IV-E Evaluation & Reproducibility
    ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")节所述，大多数被审阅的文献在少数几个公开的数据集上进行了评估。一些作者还在合成生成的私有数据集[[30](#bib.bib30),
    [60](#bib.bib60), [80](#bib.bib80), [25](#bib.bib25), [67](#bib.bib67), [49](#bib.bib49),
    [66](#bib.bib66)]、从学术机构收集的数据[[53](#bib.bib53), [74](#bib.bib74)]或从工业实际应用中获得的数据[[19](#bib.bib19),
    [68](#bib.bib68), [61](#bib.bib61), [74](#bib.bib74), [36](#bib.bib36), [32](#bib.bib32),
    [35](#bib.bib35)]上评估了他们的方法。总体而言，62篇审阅文献中有55篇涉及至少一个来自表格[IV](#S4.T4 "TABLE IV ‣
    IV-E1 Data sets ‣ IV-E Evaluation & Reproducibility ‣ IV Survey Results ‣ Deep
    Learning for Anomaly Detection in Log Data: A Survey")的公开数据集的评估。'
- en: 'While it is relatively common to evaluate approaches on public data sets, there
    are unfortunately only few authors that publish implementations of their approaches
    alongside the papers. During our review we were only able to find the original
    source code of presented approaches from 8 publications. However, we also point
    out that there exist some re-implementations of scientific approaches in the deep-loglizer
    toolbox provided by Chen et al. [[9](#bib.bib9)]. We mark approaches where implementations
    by the original authors exist with (YES), re-implementations by other authors
    as (RE), and all others as (NO) in Table [IV](#S4 "IV Survey Results ‣ Deep Learning
    for Anomaly Detection in Log Data: A Survey"). We encourage authors to publish
    their code to improve the reproducibility of their results and hope to see more
    open-source implementations in the future.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管在公共数据集上评估方法相对较为普遍，但不幸的是，只有少数作者会在论文中发布其方法的实现。在我们的审查中，我们仅能从8篇文献中找到呈现方法的原始源代码。然而，我们还指出，陈等人提供的deep-loglizer工具箱中存在一些科学方法的重新实现[[9](#bib.bib9)]。我们在表格[IV](#S4
    "IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")中用（YES）标记了存在原作者实现的方法，用（RE）标记了其他作者的重新实现，用（NO）标记了所有其他方法。我们鼓励作者发布他们的代码，以提高结果的可重复性，并希望未来能看到更多的开源实现。'
- en: V Discussion
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 讨论
- en: 'The previous sections presented the results of our survey in detail. In the
    following we summarize these results, discuss open issues in the research area
    on log-based anomaly detection using deep learning, and propose ideas for future
    research in course of answering our research questions from Sect. [I](#S1 "I Introduction
    ‣ Deep Learning for Anomaly Detection in Log Data: A Survey").'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '前面的章节详细介绍了我们调查的结果。在接下来的部分，我们总结这些结果，讨论基于日志的异常检测研究领域中的开放问题，并提出未来研究的想法，以回答第[I](#S1
    "I Introduction ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")节中的研究问题。'
- en: 'RQ1: What are the main challenges of log-based anomaly detection with deep
    learning? When carrying out our systematic literature review we assessed whether
    and to what degree the current state-of-the-art addresses the research challenges
    enumerated in Sect. [II-B](#S2.SS2 "II-B Challenges ‣ II Background ‣ Deep Learning
    for Anomaly Detection in Log Data: A Survey"). It turns out that data instability,
    i.e., the appearance of previously unknown events, is one of the main issues addressed
    by the reviewed approaches. The key idea to resolving this problem is currently
    to represent logs as semantic vectors so that new or changed events can still
    be compared to known events by measuring their similarities [[48](#bib.bib48),
    [19](#bib.bib19), [52](#bib.bib52), [54](#bib.bib54), [50](#bib.bib50), [61](#bib.bib61),
    [74](#bib.bib74), [69](#bib.bib69), [44](#bib.bib44), [78](#bib.bib78), [32](#bib.bib32)].
    There are many techniques for generating numeric vectors to represent log events
    (cf. Sect. [IV-C4](#S4.SS3.SSS4 "IV-C4 Feature representation ‣ IV-C Log Data
    Preparation ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log Data:
    A Survey")) and thus resolve the issue of feeding unstructured and textual input
    data to neural networks. Imbalanced data sets are another challenge that is specifically
    addressed by some approaches. In particular, authors suggest to use sampling techniques
    as well as context-aware embedding methods as possible solutions [[66](#bib.bib66),
    [39](#bib.bib39), [70](#bib.bib70), [59](#bib.bib59), [49](#bib.bib49)].'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ1：基于日志的深度学习异常检测面临的主要挑战是什么？在进行系统文献回顾时，我们评估了当前最先进的技术在多大程度上解决了第[II-B](#S2.SS2
    "II-B Challenges ‣ II Background ‣ Deep Learning for Anomaly Detection in Log
    Data: A Survey")节中列举的研究挑战。结果表明，数据不稳定性，即之前未知事件的出现，是所评审方法所解决的主要问题之一。解决此问题的关键思路目前是将日志表示为语义向量，以便通过测量相似性，将新的或变化的事件与已知事件进行比较[[48](#bib.bib48),
    [19](#bib.bib19), [52](#bib.bib52), [54](#bib.bib54), [50](#bib.bib50), [61](#bib.bib61),
    [74](#bib.bib74), [69](#bib.bib69), [44](#bib.bib44), [78](#bib.bib78), [32](#bib.bib32)]。有许多技术用于生成数值向量来表示日志事件（参见第[IV-C4](#S4.SS3.SSS4
    "IV-C4 Feature representation ‣ IV-C Log Data Preparation ‣ IV Survey Results
    ‣ Deep Learning for Anomaly Detection in Log Data: A Survey")节），从而解决将非结构化和文本输入数据喂给神经网络的问题。不平衡数据集是另一个由一些方法专门解决的挑战。特别地，作者建议使用采样技术以及上下文感知嵌入方法作为可能的解决方案[[66](#bib.bib66),
    [39](#bib.bib39), [70](#bib.bib70), [59](#bib.bib59), [49](#bib.bib49)]。'
- en: Some approaches are specifically designed to enable applicability in scenarios
    that demand efficient and lightweight algorithms, e.g., deployment on edge devices.
    This is achieved by leveraging low-dimensional vector representations as well
    as convolutional neural networks that are more efficient than recurrent neural
    networks [[65](#bib.bib65), [42](#bib.bib42), [33](#bib.bib33)]. Similarly, some
    approaches support log stream processing and enable adaptive learning (i.e., dynamically
    changing the baselines for anomaly detection) by incrementally re-training the
    models with manually identified and labeled false positive samples [[75](#bib.bib75)].
    It must be noted that there is currently no solution how to automatically determine
    that re-training is required without label information or manual intervention.
    The challenge of interleaving logs is generally solved by leveraging session identifiers
    directly from parsed log data; however, there are also approaches that evade the
    need for sequences altogether, e.g., by relying on sentiment analysis [[80](#bib.bib80)].
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法专门设计用于在需要高效且轻量级算法的场景中应用，例如，在边缘设备上部署。这是通过利用低维向量表示以及比递归神经网络更高效的卷积神经网络来实现的[[65](#bib.bib65),
    [42](#bib.bib42), [33](#bib.bib33)]。类似地，一些方法支持日志流处理，并通过逐步重新训练模型来实现自适应学习（即，动态改变异常检测的基线），以手动标识和标记的假阳性样本为基础[[75](#bib.bib75)]。需要注意的是，目前还没有解决如何在没有标签信息或人工干预的情况下自动确定是否需要重新训练的问题。交错日志的挑战通常通过直接利用从解析的日志数据中获得的会话标识符来解决；然而，也有一些方法完全绕过了序列的需求，例如，通过依赖情感分析[[80](#bib.bib80)]。
- en: A way to address the challenge that only few labeled data is available is provided
    by transfer learning, where models are trained on one system and tested on another
    [[43](#bib.bib43), [35](#bib.bib35)]. The main idea is that the log event patterns
    learned by the neural networks are similar across different domains and that already
    seen anomalies can be recognized and classified. Guo et al. [[42](#bib.bib42)]
    are the only authors to consider federated learning, where learning takes place
    in a distributed manner across multiple systems. Hashemi et al. [[45](#bib.bib45)]
    also go into this direction as they combine multiple data sets to evaluate whether
    this affects the performance of their model. We believe that federated learning
    could be an interesting topic for future publications as there exist many real-world
    scenarios where log data is monitored in distributed machines but orchestration
    of deployed detectors takes place centrally [[111](#bib.bib111)].
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 处理仅有少量标记数据的挑战的一种方法是迁移学习，其中模型在一个系统上训练并在另一个系统上测试 [[43](#bib.bib43), [35](#bib.bib35)]。其主要思想是神经网络学习到的日志事件模式在不同领域之间是相似的，因此可以识别和分类已经见过的异常。Guo
    等人 [[42](#bib.bib42)] 是唯一考虑联邦学习的作者，在这种学习方法中，学习以分布式的方式在多个系统上进行。Hashemi 等人 [[45](#bib.bib45)]
    也朝这个方向发展，他们结合了多个数据集来评估这是否影响了模型的性能。我们认为联邦学习可能是未来出版物中的一个有趣话题，因为存在许多实际场景，其中日志数据在分布式机器中被监控，但部署的检测器的协调是在中央进行
    [[111](#bib.bib111)]。
- en: The challenge of facing diverse artifacts of anomalies is only partially addressed
    since the vast majority of approaches focus on sequences and frequencies of log
    events, but only few consider event parameters or inter-arrival times for detection.
    We recommend to also consider techniques that address other patterns that appear
    in normal system behavior and may be useful to detect specific anomalies, such
    as changes of parameter value correlations, periodic behavior, statistical distributions,
    etc. Moreover, we observed that the explainability of the proposed deep learning
    models is relatively low, i.e., it is non-trivial to understand the criteria for
    classifications and thus detection of model bias as well as interpretation of
    false positives and false negatives is generally difficult. This hinders root
    cause analysis of detected anomalies and produces an overhead for system operators.
    Specifically in security-critical systems (e.g., intrusion detection) it is vital
    to understand the functioning - and thus the limits - of deployed anomaly detectors.
    We would therefore recommend to direct future research in log-based anomaly detection
    towards explainable artificial intelligence [[112](#bib.bib112)].
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 面对多样异常特征的挑战仅得到部分解决，因为绝大多数方法集中于日志事件的序列和频率，但很少考虑事件参数或到达时间的检测。我们建议还应考虑解决正常系统行为中出现的其他模式的技术，这些模式可能有助于检测特定异常，如参数值相关性的变化、周期性行为、统计分布等。此外，我们观察到所提出的深度学习模型的可解释性相对较低，即理解分类标准并因此检测模型偏差以及解释假阳性和假阴性通常是困难的。这妨碍了对检测到的异常的根本原因分析，并给系统操作员带来了额外的负担。特别是在安全关键系统（例如入侵检测）中，了解部署的异常检测器的功能及其限制至关重要。因此，我们建议将未来基于日志的异常检测研究方向转向可解释的人工智能
    [[112](#bib.bib112)]。
- en: 'RQ2: What state-of-the-art deep learning algorithms are typically applied?
    Our review shows that diverse types of deep learning algorithms are used in scientific
    publications and that it is common to combine several approaches. Thereby, RNNs
    are clearly the most applied models, because they are a natural choice for capturing
    sequential patterns in log data. CNNs are used as an efficient alternative to
    RNNs as they are also able to pick up event dependencies. On the other hand, Autoencoders
    and Transformers are frequently applied as they support unsupervised learning.
    While GANs, MLPs, GNNs, and EGNNs are only used by few approaches, they have beneficial
    properties (cf. Sect. [IV-B1](#S4.SS2.SSS1 "IV-B1 Deep Learning Models ‣ IV-B
    Deep Learning Techniques ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection
    in Log Data: A Survey")) that make these models worth considering. Similarly,
    we are convinced that other deep learning architectures that are not explored
    in the reviewed literature could yield interesting insights, e.g., deep belief
    networks or deep reinforcement learning [[84](#bib.bib84), [15](#bib.bib15)].'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ2：通常应用哪些最先进的深度学习算法？我们的回顾显示，科学文献中使用了多种类型的深度学习算法，并且通常会结合几种方法。因此，RNN 显然是应用最广泛的模型，因为它们是捕捉日志数据中顺序模式的自然选择。CNN
    被用作 RNN 的有效替代，因为它们也能捕捉事件依赖关系。另一方面，Autoencoders 和 Transformers 经常被应用，因为它们支持无监督学习。虽然
    GANs、MLPs、GNNs 和 EGNNs 仅被少数方法使用，但它们具有有益的特性（参见第 [IV-B1](#S4.SS2.SSS1 "IV-B1 Deep
    Learning Models ‣ IV-B Deep Learning Techniques ‣ IV Survey Results ‣ Deep Learning
    for Anomaly Detection in Log Data: A Survey") 节），使得这些模型值得考虑。类似地，我们相信，其他未在回顾文献中探讨的深度学习架构可能会提供有趣的见解，例如深度信念网络或深度强化学习
    [[84](#bib.bib84), [15](#bib.bib15)]。'
- en: 'We believe that the application of specific techniques is mostly motivated
    by the log data to be analyzed and anomalies to be detected. The fact that all
    of the commonly used log data sets involve anomalies that manifest as sequentially
    occurring events (cf. Sect. [IV-E1](#S4.SS5.SSS1 "IV-E1 Data sets ‣ IV-E Evaluation
    & Reproducibility ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in
    Log Data: A Survey")) thus explains the tendency towards RNNs. However, as anomalies
    could also manifest as point or contextual anomalies (cf. Sect. [II-A3](#S2.SS1.SSS3
    "II-A3 Anomaly Detection ‣ II-A Preliminary Definitions ‣ II Background ‣ Deep
    Learning for Anomaly Detection in Log Data: A Survey")) we recommend to consider
    alternative log data sets with different types of anomalies and to develop approaches
    for these cases. For example, in our earlier works [[113](#bib.bib113), [114](#bib.bib114)]
    we published log data sets where anomalies affect combinations, compositions,
    and distributions of event parameter values in addition to frequencies and sequences
    of log events.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '我们认为，特定技术的应用主要受到待分析日志数据和待检测异常的驱动。所有常用的日志数据集都涉及异常，这些异常表现为顺序发生的事件（参见第 [IV-E1](#S4.SS5.SSS1
    "IV-E1 Data sets ‣ IV-E Evaluation & Reproducibility ‣ IV Survey Results ‣ Deep
    Learning for Anomaly Detection in Log Data: A Survey") 节），这解释了为何倾向于使用 RNN。然而，由于异常也可能表现为点异常或上下文异常（参见第
    [II-A3](#S2.SS1.SSS3 "II-A3 Anomaly Detection ‣ II-A Preliminary Definitions ‣
    II Background ‣ Deep Learning for Anomaly Detection in Log Data: A Survey") 节），我们建议考虑具有不同类型异常的替代日志数据集，并为这些情况开发相应的方法。例如，在我们早期的研究中
    [[113](#bib.bib113), [114](#bib.bib114)]，我们发布了日志数据集，其中异常影响事件参数值的组合、组成和分布，除了日志事件的频率和顺序之外。'
- en: 'RQ3: How is log data pre-processed to be ingested by deep learning models?
    Our survey shows that there are three main ways to approach feature extraction
    from raw log data. First, by tokenizing log messages, which is a simple method
    that does not require any parsers but lacks semantic interpretation of the tokens.
    Second, by parsing the messages and extracting information from collections of
    log events, such as sequences, counts, or statistics. Third, by extracting parameters
    including the time stamps from parsed log events. There are a multitude of methods
    to represent these features as numeric vectors to be ingested by deep learning
    models.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3：日志数据如何预处理以便被深度学习模型接收？我们的调查显示，从原始日志数据中提取特征的主要方法有三种。首先，通过对日志消息进行标记化，这是一种简单的方法，不需要任何解析器，但缺乏对标记的语义解释。其次，通过解析消息并从日志事件集合中提取信息，如序列、计数或统计数据。第三，通过提取包含时间戳的解析日志事件的参数。还有多种方法可以将这些特征表示为数值向量，以便被深度学习模型接收。
- en: While traditional machine learning methods such as SVM or PCA work best with
    event count vectors, most approaches leveraging deep learning neural networks
    use semantic vectors to yield the best results [[1](#bib.bib1)]. Thereby, the
    tokens that make up the log messages are represented as numeric vectors and considered
    in the context of their sequence of event occurrences. Most approaches employ
    these sequential features, while frequencies, one-hot encoded data, and embedding
    layers are used less often or only as a contributing feature.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习方法，如SVM或PCA，最适合处理事件计数向量，而大多数利用深度学习神经网络的方法使用语义向量以获得最佳结果 [[1](#bib.bib1)]。因此，构成日志消息的令牌被表示为数值向量，并在事件发生的顺序上下文中考虑。大多数方法使用这些顺序特征，而频率、独热编码数据和嵌入层则较少使用或仅作为辅助特征。
- en: 'RQ4: What types of anomalies are detected and how are they identified as such?
    Almost all reviewed approaches focus on sequential anomalies that either manifest
    in the sequences of events, the sequences of tokens within events, or a combination
    of both. Only few approaches make use of event counts or detect single log lines
    as outliers without their context of occurrence. The detection technique is generally
    driven by the output of the neural networks. While binary or multi-class classifications
    are directly used to report anomalies, all numeric outputs such as anomaly scores
    or reconstruction errors are compared against pre-defined thresholds and probability
    distributions of log events are used to check whether the actual events is within
    the top candidates. Determining these thresholds is usually carried out empirically
    for a particular log file.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ4: 检测到的异常类型是什么？它们如何被识别？几乎所有审查的方法都集中在顺序异常上，这些异常表现为事件序列、事件内令牌序列或两者的组合。只有少数方法利用事件计数或将单个日志行作为离群点进行检测，而不考虑其发生的上下文。检测技术通常由神经网络的输出驱动。虽然二元或多类别分类直接用于报告异常，但所有数值输出如异常分数或重建误差都与预定义的阈值进行比较，日志事件的概率分布用于检查实际事件是否在前几个候选项中。确定这些阈值通常是对特定日志文件进行经验性操作。'
- en: 'RQ5: How are the proposed models evaluated? Our survey on commonly used log
    data sets from Sect. [IV-E1](#S4.SS5.SSS1 "IV-E1 Data sets ‣ IV-E Evaluation &
    Reproducibility ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log
    Data: A Survey") shows that there are only four data sets that are used by a majority
    of the approaches: HDFS, BGL, Thunderbird, and OpenStack. All these data sets
    are collected from scenarios involving high-performance computing and virtual
    machines that are affected by randomly occurring failures. It is obvious that
    the availability of anomaly labels make these data sets particularly attractive
    for scientific evaluations. As mentioned before, we argue that a larger and more
    diverse set of input data sets would be beneficial to evaluate whether the proposed
    approaches are capable of detecting anomalous artifacts other than unusual sequences.
    In particular, the consequences of cyber attacks rather than failures could result
    in log artifacts that are suitable for detection and an appropriate use-case for
    anomaly detection.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ5: 提出的模型如何评估？我们对第[IV-E1](#S4.SS5.SSS1 "IV-E1 数据集 ‣ IV-E 评估与可重复性 ‣ IV 调查结果
    ‣ 深度学习在日志数据异常检测中的应用：一项调查")节常用日志数据集的调查显示，大多数方法仅使用四个数据集：HDFS、BGL、Thunderbird和OpenStack。所有这些数据集都来自高性能计算和受随机发生故障影响的虚拟机场景。显然，异常标签的可用性使这些数据集在科学评估中特别有吸引力。正如前面提到的，我们认为更大和更多样化的输入数据集将有助于评估所提出的方法是否能够检测到除异常序列之外的异常现象。特别是，网络攻击的后果而非故障可能会导致适合检测的日志伪影，并成为异常检测的一个合适用例。'
- en: 'We manually analyzed the HDFS data set as it is the most popular of all and
    found that it is far from challenging to achieve competitive detection rates.
    The reason for this is that many of the anomalous event sequences are trivial
    to identify as they involve event types that never occur in the training data
    or involve fewer elements than the shortest normal sequences. Using these two
    heuristics we are able to achieve $F1=90.41$%, $ACC=99.48$%, $P=98.37$%, $R=83.65$%,
    $FPR=0.04$% on the test data. Moreover, using these heuristics in combination
    with the simple Stide algorithm [[115](#bib.bib115)] that moves a sliding window
    of a given length over the data and looks for sub-sequences that have not been
    seen before in the training data further improves the evaluation metrics to $F1=95.14$%,
    $ACC=99.71$%, $P=95.27$%, $R=95.01$%, $FPR=0.14$% when using a window size of
    $2$. Omitting sequential information altogether and only leveraging similarities
    of event count vectors further pushes these evaluation metrics to $F1=98.86$%,
    $ACC=99.93$%, $P=97.81$%, $R=99.92$%, $FPR=00.07$%. For comparison, DeepLog only
    yields detection scores of $F1=95.72$%, $ACC=99.75$%, $P=95.12$%, $R=96.32$%,
    $FPR=0.15$% [[24](#bib.bib24)]. We provide the code for our experiments in a reproducible
    form as open-source implementations (in separate repositories for Stide^(10)^(10)10https://github.com/ait-aecid/stide
    and similarity-based event count vector clustering^(11)^(11)11https://github.com/ait-aecid/count-vector-clustering).
    It is not clear to us why such approaches were not used as benchmarks in any of
    the reviewed publications: They only take a fraction of the time for training
    and processing the test data in comparison to deep (and also most conventional)
    learning models, and additionally have a much better explainability than neural
    networks. Similar conclusions have been drawn for the case of log event prediction
    using the HDFS data set [[116](#bib.bib116)]. We argue that the fact that such
    simple algorithms achieve competitive detection rates to deep learning models
    further urges authors to consider additional data sets where more diverse anomalous
    artifacts are present and the benefits of their approaches such as robustness
    against data instability become apparent.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手动分析了HDFS数据集，因为它是最受欢迎的数据集，并发现要实现具有竞争力的检测率远非难事。原因在于许多异常事件序列很容易识别，因为它们涉及在训练数据中从未出现的事件类型或涉及的元素少于最短的正常序列。利用这两种启发式方法，我们能够在测试数据上获得$F1=90.41$%、$ACC=99.48$%、$P=98.37$%、$R=83.65$%、$FPR=0.04$%。此外，将这些启发式方法与简单的Stide算法[[115](#bib.bib115)]结合使用，后者在数据上移动给定长度的滑动窗口并查找训练数据中未见过的子序列，可以进一步改善评估指标，使其在窗口大小为$2$时达到$F1=95.14$%、$ACC=99.71$%、$P=95.27$%、$R=95.01$%、$FPR=0.14$%。完全忽略序列信息，只利用事件计数向量的相似性，将这些评估指标进一步推高至$F1=98.86$%、$ACC=99.93$%、$P=97.81$%、$R=99.92$%、$FPR=00.07$%。相比之下，DeepLog的检测得分仅为$F1=95.72$%、$ACC=99.75$%、$P=95.12$%、$R=96.32$%、$FPR=0.15$%[[24](#bib.bib24)]。我们以可重复的形式提供了实验代码作为开源实现（Stide的单独代码库：https://github.com/ait-aecid/stide
    和基于相似性的事件计数向量聚类的代码库：https://github.com/ait-aecid/count-vector-clustering）。我们不清楚为何在任何已审查的出版物中都未将此类方法作为基准使用：与深度（以及大多数传统）学习模型相比，它们训练和处理测试数据所需的时间仅为其一小部分，而且比神经网络具有更好的可解释性。对于使用HDFS数据集的日志事件预测，已得出类似结论[[116](#bib.bib116)]。我们认为，简单算法能够实现与深度学习模型相竞争的检测率，这进一步促使作者考虑额外的数据集，其中存在更多样的异常伪影，并且它们的方法如对数据不稳定性的鲁棒性等优点将变得更加明显。
- en: Another issue that we noticed is that evaluations are usually carried out on
    the basis of anomalous sequences, i.e., the whole sequence is considered normal
    or anomalous rather than its elements [[24](#bib.bib24)]. However, parts of long
    sequences may actually represent normal system behavior while only a few elements
    should be considered anomalies. It would be interesting to evaluate whether detection
    approaches are able to pinpoint exactly which parts of sequences are anomalous,
    which would also be practical for manual investigations of reported anomalies
    by system operators. Obviously this requires that data sets are labeled on the
    granularity of single events rather than sessions [[116](#bib.bib116)].
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到的另一个问题是，评估通常是基于异常序列进行的，即整个序列被视为正常或异常，而不是其元素[[24](#bib.bib24)]。然而，长序列的部分可能实际上代表正常的系统行为，而只有少数几个元素应被视为异常。评估检测方法是否能够准确确定序列中哪些部分是异常的会很有趣，这对于系统操作员手动调查报告的异常也会很实用。显然，这需要数据集在单个事件的粒度上进行标记，而不是按会话标记[[116](#bib.bib116)]。
- en: 'Finally, we note that almost all evaluations rely on metrics such as the F-score
    (cf. Sect. [IV-E2](#S4.SS5.SSS2 "IV-E2 Evaluation metrics ‣ IV-E Evaluation &
    Reproducibility ‣ IV Survey Results ‣ Deep Learning for Anomaly Detection in Log
    Data: A Survey")) that are known to not accurately depict the classification or
    detection performance when data sets are highly imbalanced. To avoid misinterpretations
    of evaluation results, it is recommended to also compute metrics that are more
    robust against class imbalance, such as the specificity or true negative rate
    [[10](#bib.bib10)].'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们注意到几乎所有评估都依赖于如F-score等度量（参见[IV-E2](#S4.SS5.SSS2 "IV-E2 Evaluation metrics
    ‣ IV-E Evaluation & Reproducibility ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey")），而这些度量在数据集高度不平衡时往往无法准确反映分类或检测性能。为了避免对评估结果的误解，建议还计算对类别不平衡更具鲁棒性的度量，例如特异性或真正负率[[10](#bib.bib10)]。'
- en: 'RQ6: To what extent do the approaches rely on labeled data and support incremental
    learning? Log-based anomaly detection is most often applied in use-cases that
    aim to disclose unexpected system behavior such as failures or cyber attacks.
    Since these artifacts are not known beforehand and thus no labels can exist, un-
    or semi-supervised approaches are generally more widely applicable and therefore
    preferable [[7](#bib.bib7)]. Since semi-supervised learning can be achieved with
    most neural network architectures including RNNs, but only specific deep learning
    models support fully unsupervised operation, we find 28 semi-supervised approaches
    in our reviewed literature as opposed to only 8 out of 62 approaches that are
    unsupervised. We did not expect to see the relatively large amount of 26 supervised
    approaches that require at least partially labeled anomalies for training. Moreover,
    with 54 out of 62 a vast majority of approaches only support offline training.
    This includes most supervised models and also all other approaches that do not
    intend to dynamically and automatically update the trained models over time. Only
    8 of the reviewed approaches enable continuous model adjustments through re-training
    or EGNN model architectures [[36](#bib.bib36)].'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: RQ6：这些方法在多大程度上依赖标记数据，并支持增量学习？基于日志的异常检测通常应用于旨在揭示意外系统行为（如故障或网络攻击）的用例中。由于这些异常情况事先未知，因此无法存在标签，因此无监督或半监督的方法通常更广泛适用，因此更受欢迎[[7](#bib.bib7)]。由于半监督学习可以在包括RNN在内的大多数神经网络架构中实现，但仅有特定的深度学习模型支持完全无监督的操作，我们在审查文献中发现了28种半监督方法，而在62种方法中只有8种是无监督的。我们没有预料到有26种监督方法需要至少部分标记的异常数据进行训练。此外，在62种方法中，有54种方法仅支持离线训练。这包括大多数监督模型，以及所有不打算动态和自动更新训练模型的其他方法。只有8种被审查的方法通过重新训练或EGNN模型架构[[36](#bib.bib36)]支持连续的模型调整。
- en: 'RQ7: How reproducible are the presented results in terms of availability of
    source code and used data? As pointed out in Sect. [IV-E4](#S4.SS5.SSS4 "IV-E4
    Reproducibility ‣ IV-E Evaluation & Reproducibility ‣ IV Survey Results ‣ Deep
    Learning for Anomaly Detection in Log Data: A Survey"), a majority of the reviewed
    approaches make use of publicly available data sets to evaluate their approaches
    and only 7 publications rely on private data sets. The reproducibility of the
    presented results is thus relatively high, assuming that readers are willing to
    re-implement the approaches based on the descriptions from the papers from scratch.
    We could only find 9 publicly available source codes of approaches published by
    the original authors as well as 4 re-implementations, indicating a low reproducibility
    overall. We encourage authors to publish reproducible experiments in the future
    to also enable large-scale quantitative comparisons in surveys.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 'RQ7：在源代码和使用数据的可获得性方面，所呈现的结果的可重复性如何？正如在[IV-E4](#S4.SS5.SSS4 "IV-E4 Reproducibility
    ‣ IV-E Evaluation & Reproducibility ‣ IV Survey Results ‣ Deep Learning for Anomaly
    Detection in Log Data: A Survey")中指出的，大多数被审查的方法使用公开可用的数据集来评估其方法，仅有7篇文献依赖于私有数据集。因此，所呈现结果的可重复性相对较高，前提是读者愿意根据论文中的描述从头重新实现这些方法。我们只能找到9个由原作者发布的公开源代码和4个重新实现的版本，整体上显示出较低的可重复性。我们鼓励作者在未来发布可重复的实验，以便在调查中进行大规模的定量比较。'
- en: 'Recommendations. Based on the aforementioned answers to our research questions
    and the identified issues, we propose the following research agenda: First of
    all, adequate and diverse log data sets with state-of-the-art benchmarks are needed
    to ensure applicability of deep learning in generic anomaly detection use-cases
    and demonstrate their superiority over simple or conventional detection methods.
    Second, low explainability of detection results is a primary concern that permeates
    the entire research field and needs to be addressed appropriately. Resolving these
    two issues largely improves the comprehensibility and reliability of proposed
    methods and facilitates the development of novel deep learning detection algorithms.
    With this research agenda in mind, we summarize our recommendations for future
    research as follows.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐。基于我们对研究问题的上述回答和识别出的问题，我们提出以下研究议程：首先，需要充分且多样的日志数据集以及先进的基准，以确保深度学习在通用异常检测应用中的适用性，并展示其优于简单或传统检测方法的优势。其次，检测结果的低解释性是一个主要关注点，贯穿整个研究领域，需要得到适当解决。解决这两个问题大大提高了所提方法的可理解性和可靠性，并促进了新型深度学习检测算法的发展。考虑到这一研究议程，我们总结了未来研究的建议如下。
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Create or identify new log data sets that specifically involve sequential anomalies
    and are less affected by other types of anomalies when evaluating approaches that
    ingest log data as sequences.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建或识别专门涉及序列异常的新日志数据集，并且在评估将日志数据作为序列输入的方法时，受其他类型异常的影响较小。
- en: •
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Work on methods that improve the explainability of proposed approaches for anomaly
    detection using deep learning, for example, by the extraction of specific detection
    rules from the models or by determining the main features responsible for the
    detection of specific instances and augmenting detection results with that information.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究提高深度学习方法在异常检测中的解释性的技术，例如，通过从模型中提取具体的检测规则，或确定主要特征以解释特定实例的检测，并用这些信息增强检测结果。
- en: •
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Consider log artifacts other than event sequences for anomaly detection or use
    them as additional input to deep learning models.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑日志工件（而非事件序列）用于异常检测，或将其作为深度学习模型的额外输入。
- en: •
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Propose novel anomaly detection methods or deep learning architectures that
    resolve common challenges for practical applications, specifically regarding incremental
    and stream processing or log data, adaptive learning, as well as efficient and
    low-resource training.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出新颖的异常检测方法或深度学习架构，以解决实际应用中的常见挑战，特别是关于增量和流处理或日志数据、自适应学习，以及高效和低资源训练的问题。
- en: •
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Consider pinpointing anomalies within sequences rather than detecting whole
    sequences as anomalous.
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑在序列中定位异常，而不是将整个序列检测为异常。
- en: •
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Allow researchers to reproduce and extend presented results by publishing developed
    code as open-source and used log data sets on data sharing repositories.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 允许研究人员通过将开发的代码作为开源发布并将使用过的日志数据集上传到数据共享仓库，从而复制和扩展所展示的结果。
- en: VI Conclusion
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: This paper presents a survey of 62 scientific approaches that pursue the detection
    of anomalous events or processes in system log data using deep learning. The survey
    shows that diverse model architectures are suitable for this purpose, including
    models for sequential input data such as recurrent or convolutional neural networks,
    language-based models such as transformers, as well as unsupervised models such
    as Autoencoders or generative adversarial networks. Similarly, there are different
    features used for training and subsequently for detection, such as sequences and
    counts of events or tokens as well as parameter values or statistics derived from
    the events. To enable processing of these features as input to neural networks
    it is necessary to encode them as numeric vectors, for example, through semantic
    vectorization or one-hot encoding. Anomalies are then detected either directly
    through classification or by deriving some kind of anomaly score from the network
    that allows to discern normal from anomalous system behavior. The survey shows
    that there are open challenges that are not sufficiently resolved by existing
    approaches, including detection techniques that go beyond sequential anomalies,
    low explainability of trained models and classification results, lack of representative
    evaluation data sets containing diverse attack artifacts, and a low reproducibility.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 62 种利用深度学习检测系统日志数据中异常事件或过程的科学方法的综述。综述表明，包括递归神经网络或卷积神经网络等适用于序列输入数据的模型、基于语言的模型如变压器，以及无监督模型如自编码器或生成对抗网络在内的多种模型架构均适用于此目的。类似地，还有不同的特征用于训练和随后的检测，例如事件或标记的序列和计数以及从事件中导出的参数值或统计数据。为了将这些特征处理为神经网络的输入，有必要将其编码为数值向量，例如通过语义向量化或独热编码。然后，通过分类直接检测异常或通过从网络中推导某种异常评分来区分正常与异常的系统行为。综述显示，现有方法尚未充分解决一些开放性挑战，包括超越序列异常的检测技术、训练模型和分类结果的低可解释性、缺乏包含多样攻击特征的代表性评估数据集以及低复现性。
- en: Acknowledgments
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was partly funded by the European Defence Fund (EDF) projects AInception
    (101103385) and PANDORA (SI2.835928), and the FFG project DECEPT (873980).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分由欧洲防务基金 (EDF) 项目 AInception (101103385) 和 PANDORA (SI2.835928) 资助，以及 FFG
    项目 DECEPT (873980) 资助。
- en: References
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Q. Wang, X. Zhang, X. Wang, and Z. Cao, “Log sequence anomaly detection
    method based on contrastive adversarial training and dual feature extraction,”
    *Entropy*, vol. 24, no. 1, p. 69, 2021.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Q. Wang, X. Zhang, X. Wang, 和 Z. Cao, “基于对比对抗训练和双重特征提取的日志序列异常检测方法，” *熵*，第
    24 卷，第 1 期，第 69 页，2021 年。'
- en: '[2] H.-J. Liao, C.-H. R. Lin, Y.-C. Lin, and K.-Y. Tung, “Intrusion detection
    system: A comprehensive review,” *Journal of Network and Computer Applications*,
    vol. 36, no. 1, pp. 16–24, 2013.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H.-J. Liao, C.-H. R. Lin, Y.-C. Lin, 和 K.-Y. Tung, “入侵检测系统：全面综述，” *网络与计算机应用杂志*，第
    36 卷，第 1 期，第 16–24 页，2013 年。'
- en: '[3] M. Landauer, F. Skopik, M. Wurzenberger, and A. Rauber, “System log clustering
    approaches for cyber security applications: A survey,” *Computers & Security*,
    vol. 92, p. 101739, 2020.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Landauer, F. Skopik, M. Wurzenberger, 和 A. Rauber, “网络安全应用的系统日志聚类方法：综述，”
    *计算机与安全*，第 92 卷，第 101739 页，2020 年。'
- en: '[4] S. He, J. Zhu, P. He, and M. R. Lyu, “Experience report: System log analysis
    for anomaly detection,” in *2016 IEEE 27th international symposium on software
    reliability engineering (ISSRE)*.   IEEE, 2016, pp. 207–218.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. He, J. Zhu, P. He, 和 M. R. Lyu, “经验报告：用于异常检测的系统日志分析，” 见 *2016 IEEE 第
    27 届国际软件可靠性工程研讨会 (ISSRE)*，IEEE，2016 年，第 207–218 页。'
- en: '[5] C. Kruegel and G. Vigna, “Anomaly detection of web-based attacks,” in *Proceedings
    of the 10th ACM conference on Computer and communications security*, 2003, pp.
    251–261.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] C. Kruegel 和 G. Vigna, “基于网络的攻击异常检测，” 见 *第十届 ACM 计算机与通信安全大会论文集*，2003 年，第
    251–261 页。'
- en: '[6] M. Landauer, M. Wurzenberger, F. Skopik, G. Settanni, and P. Filzmoser,
    “Dynamic log file analysis: An unsupervised cluster evolution approach for anomaly
    detection,” *computers & security*, vol. 79, pp. 94–116, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Landauer, M. Wurzenberger, F. Skopik, G. Settanni, 和 P. Filzmoser, “动态日志文件分析：一种无监督的集群演化方法用于异常检测，”
    *计算机与安全*，第 79 卷，第 94–116 页，2018 年。'
- en: '[7] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A survey,”
    *ACM computing surveys (CSUR)*, vol. 41, no. 3, pp. 1–58, 2009.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] V. Chandola, A. Banerjee, 和 V. Kumar, “异常检测：综述，” *ACM 计算调查 (CSUR)*，第 41
    卷，第 3 期，第 1–58 页，2009 年。'
- en: '[8] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习，” *自然*, 第521卷，第7553期，页码436–444，2015年。'
- en: '[9] Z. Chen, J. Liu, W. Gu, Y. Su, and M. R. Lyu, “Experience report: Deep
    learning-based system log analysis for anomaly detection,” *arXiv preprint arXiv:2107.05908*,
    2021.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Z. Chen, J. Liu, W. Gu, Y. Su, 和 M. R. Lyu, “经验报告：基于深度学习的系统日志分析用于异常检测，”
    *arXiv预印本 arXiv:2107.05908*，2021年。'
- en: '[10] V.-H. Le and H. Zhang, “Log-based anomaly detection with deep learning:
    How far are we?” in *Proceedings of the 44th International Conference on Software
    Engineering*, 2022, pp. 1356–1367.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] V.-H. Le 和 H. Zhang, “基于日志的深度学习异常检测：我们走多远了？” 见 *第44届国际软件工程会议论文集*，2022年，页码1356–1367。'
- en: '[11] R. B. Yadav, P. S. Kumar, and S. V. Dhavale, “A survey on log anomaly
    detection using deep learning,” in *2020 8th International Conference on Reliability,
    Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)*.   IEEE,
    2020, pp. 1215–1220.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] R. B. Yadav, P. S. Kumar, 和 S. V. Dhavale, “基于深度学习的日志异常检测调查，” 见 *2020年第8届国际可靠性、信息通信技术与优化会议（趋势与未来方向）（ICRITO）*。
    IEEE, 2020年，页码1215–1220。'
- en: '[12] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, and K. J. Kim, “A survey of
    deep learning-based network anomaly detection,” *Cluster Computing*, vol. 22,
    no. 1, pp. 949–961, 2019.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, 和 K. J. Kim, “基于深度学习的网络异常检测调查，”
    *Cluster Computing*, 第22卷，第1期，页码949–961，2019年。'
- en: '[13] D. A. Bhanage, A. V. Pawar, and K. Kotecha, “It infrastructure anomaly
    detection and failure handling: A systematic literature review focusing on datasets,
    log preprocessing, machine & deep learning approaches and automated tool,” *IEEE
    Access*, 2021.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] D. A. Bhanage, A. V. Pawar, 和 K. Kotecha, “IT基础设施异常检测与故障处理：针对数据集、日志预处理、机器学习和深度学习方法以及自动化工具的系统文献综述，”
    *IEEE Access*，2021年。'
- en: '[14] N. Zhao, H. Wang, Z. Li, X. Peng, G. Wang, Z. Pan, Y. Wu, Z. Feng, X. Wen,
    W. Zhang *et al.*, “An empirical investigation of practical log anomaly detection
    for online service systems,” in *Proceedings of the 29th ACM Joint Meeting on
    European Software Engineering Conference and Symposium on the Foundations of Software
    Engineering*, 2021, pp. 1404–1415.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] N. Zhao, H. Wang, Z. Li, X. Peng, G. Wang, Z. Pan, Y. Wu, Z. Feng, X.
    Wen, W. Zhang *等*, “对在线服务系统实际日志异常检测的实证研究，” 见 *第29届ACM欧洲软件工程会议和软件工程基础研讨会联合会议论文集*，2021年，页码1404–1415。'
- en: '[15] I. H. Sarker, “Deep learning: a comprehensive overview on techniques,
    taxonomy, applications and research directions,” *SN Computer Science*, vol. 2,
    no. 6, pp. 1–20, 2021.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] I. H. Sarker, “深度学习：技术、分类、应用及研究方向的全面概述，” *SN计算机科学*, 第2卷，第6期，页码1–20，2021年。'
- en: '[16] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, and M. R. Lyu, “Tools
    and benchmarks for automated log parsing,” in *2019 IEEE/ACM 41st International
    Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)*.   IEEE,
    2019, pp. 121–130.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, 和 M. R. Lyu, “自动化日志解析的工具和基准测试，”
    见 *2019 IEEE/ACM第41届国际软件工程会议：实践中的软件工程（ICSE-SEIP）*。 IEEE, 2019年，页码121–130。'
- en: '[17] L. Bao, Q. Li, P. Lu, J. Lu, T. Ruan, and K. Zhang, “Execution anomaly
    detection in large-scale systems through console log analysis,” *Journal of Systems
    and Software*, vol. 143, pp. 172–186, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. Bao, Q. Li, P. Lu, J. Lu, T. Ruan, 和 K. Zhang, “通过控制台日志分析检测大规模系统中的执行异常，”
    *系统与软件期刊*, 第143卷，页码172–186，2018年。'
- en: '[18] R. Chalapathy and S. Chawla, “Deep learning for anomaly detection: A survey,”
    *arXiv preprint arXiv:1901.03407*, 2019.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] R. Chalapathy 和 S. Chawla, “异常检测的深度学习：综述，” *arXiv预印本 arXiv:1901.03407*，2019年。'
- en: '[19] X. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang,
    Q. Cheng, Z. Li *et al.*, “Robust log-based anomaly detection on unstable log
    data,” in *Proceedings of the 2019 27th ACM Joint Meeting on European Software
    Engineering Conference and Symposium on the Foundations of Software Engineering*,
    2019, pp. 807–817.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] X. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang,
    Q. Cheng, Z. Li *等*, “在不稳定日志数据上的鲁棒日志异常检测，” 见 *2019年第27届ACM欧洲软件工程会议和软件工程基础研讨会联合会议论文集*，2019年，页码807–817。'
- en: '[20] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, “Detecting large-scale
    system problems by mining console logs,” in *Proceedings of the ACM SIGOPS 22nd
    symposium on Operating systems principles*, 2009, pp. 117–132.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] W. Xu, L. Huang, A. Fox, D. Patterson, 和 M. I. Jordan, “通过挖掘控制台日志检测大规模系统问题，”
    见 *ACM SIGOPS第22届操作系统原理研讨会论文集*，2009年，页码117–132。'
- en: '[21] H. Mi, H. Wang, Y. Zhou, M. R.-T. Lyu, and H. Cai, “Toward fine-grained,
    unsupervised, scalable performance diagnosis for production cloud computing systems,”
    *IEEE Transactions on Parallel and Distributed Systems*, vol. 24, no. 6, pp. 1245–1255,
    2013.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] S. Suriadi, R. Andrews, A. H. ter Hofstede, and M. T. Wynn, “Event log
    imperfection patterns for process mining: Towards a systematic approach to cleaning
    event logs,” *Information systems*, vol. 64, pp. 132–150, 2017.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] D. A. Fischer, K. Goel, R. Andrews, C. G. J. van Dun, M. T. Wynn, and
    M. Röglinger, “Enhancing event log quality: Detecting and quantifying timestamp
    imperfections,” in *Business Process Management: 18th International Conference,
    BPM 2020, Seville, Spain, September 13–18, 2020, Proceedings 18*.   Springer,
    2020, pp. 309–326.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] M. Du, F. Li, G. Zheng, and V. Srikumar, “Deeplog: Anomaly detection and
    diagnosis from system logs through deep learning,” in *Proceedings of the 2017
    ACM SIGSAC conference on computer and communications security*, 2017, pp. 1285–1298.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] T. Yang and V. Agrawal, “Log file anomaly detection,” *CS224d Fall*, vol.
    2016, 2016, online: https://cs224d.stanford.edu/reports/YangAgrawal.pdf.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] W. Meng, Y. Liu, Y. Zhu, S. Zhang, D. Pei, Y. Liu, Y. Chen, R. Zhang,
    S. Tao, P. Sun *et al.*, “Loganomaly: Unsupervised detection of sequential and
    quantitative anomalies in unstructured logs,” in *IJCAI*, vol. 19, no. 7, 2019,
    pp. 4739–4745.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] S. Lu, X. Wei, Y. Li, and L. Wang, “Detecting anomaly in big data system
    logs using convolutional neural network,” in *2018 IEEE 16th Intl Conf on Dependable,
    Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing,
    4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology
    Congress (DASC/PiCom/DataCom/CyberSciTech)*.   IEEE, 2018, pp. 151–158.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, “Self-attentive
    classification-based anomaly detection in unstructured logs,” in *2020 IEEE International
    Conference on Data Mining (ICDM)*.   IEEE, 2020, pp. 1196–1201.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] H. Guo, S. Yuan, and X. Wu, “Logbert: Log anomaly detection via bert,”
    in *2021 International Joint Conference on Neural Networks (IJCNN)*.   IEEE, 2021,
    pp. 1–8.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] X. Baril, O. Coustié, J. Mothe, and O. Teste, “Application performance
    anomaly detection with lstm on temporal irregularities in logs,” in *Proceedings
    of the 29th ACM International Conference on Information & Knowledge Management*,
    2020, pp. 1961–1964.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Bursic, V. Cuculo, and A. D’Amelio, “Anomaly detection from log files
    using unsupervised deep learning,” in *International Symposium on Formal Methods*.   Springer,
    2019, pp. 200–207.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M. Catillo, A. Pecchia, and U. Villano, “Autolog: Anomaly detection by
    deep autoencoding of system logs,” *Expert Systems with Applications*, vol. 191,
    p. 116263, 2022.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. Cheansunan and P. Phunchongharn, “Detecting anomalous events on distributed
    systems using convolutional neural networks,” in *2019 IEEE 10th International
    Conference on Awareness Science and Technology (iCAST)*.   IEEE, 2019, pp. 1–5.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Cheansunan 和 P. Phunchongharn，“使用卷积神经网络检测分布式系统中的异常事件，”发表于*2019 IEEE第10届国际意识科学与技术会议（iCAST）*。IEEE，2019年，第1–5页。'
- en: '[34] H. Chen, R. Xiao, and S. Jin, “Unsupervised anomaly detection based on
    system logs,” in *Proceedings of the 33rd International Conference on Software
    Engineering & Knowledge Engineering (SEKE 2021)*, 2021, pp. 92–97.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] H. Chen, R. Xiao, 和 S. Jin，“基于系统日志的无监督异常检测，”发表于*第33届国际软件工程与知识工程会议（SEKE
    2021）*，2021年，第92–97页。'
- en: '[35] R. Chen, S. Zhang, D. Li, Y. Zhang, F. Guo, W. Meng, D. Pei, Y. Zhang,
    X. Chen, and Y. Liu, “Logtransfer: Cross-system log anomaly detection for software
    systems with transfer learning,” in *2020 IEEE 31st International Symposium on
    Software Reliability Engineering (ISSRE)*.   IEEE, 2020, pp. 37–47.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] R. Chen, S. Zhang, D. Li, Y. Zhang, F. Guo, W. Meng, D. Pei, Y. Zhang,
    X. Chen, 和 Y. Liu，“Logtransfer：用于软件系统的跨系统日志异常检测的迁移学习，”发表于*2020 IEEE第31届国际软件可靠性工程研讨会（ISSRE）*。IEEE，2020年，第37–47页。'
- en: '[36] L. Decker, D. Leite, F. Viola, and D. Bonacorsi, “Comparison of evolving
    granular classifiers applied to anomaly detection for predictive maintenance in
    computing centers,” in *2020 IEEE Conference on Evolving and Adaptive Intelligent
    Systems (EAIS)*.   IEEE, 2020, pp. 1–8.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] L. Decker, D. Leite, F. Viola, 和 D. Bonacorsi，“应用于计算中心预测性维护的演变粒度分类器的比较，”发表于*2020
    IEEE演变与自适应智能系统会议（EAIS）*。IEEE，2020年，第1–8页。'
- en: '[37] Q. Du, L. Zhao, J. Xu, Y. Han, and S. Zhang, “Log-based anomaly detection
    with multi-head scaled dot-product attention mechanism,” in *International Conference
    on Database and Expert Systems Applications*.   Springer, 2021, pp. 335–347.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Q. Du, L. Zhao, J. Xu, Y. Han, 和 S. Zhang，“基于日志的异常检测与多头缩放点积注意力机制，”发表于*国际数据库与专家系统应用会议*。Springer，2021年，第335–347页。'
- en: '[38] A. Farzad and T. A. Gulliver, “Log message anomaly detection and classification
    using auto-b/lstm and auto-gru,” *arXiv preprint arXiv:1911.08744*, 2019.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Farzad 和 T. A. Gulliver，“使用auto-b/lstm和auto-gru的日志消息异常检测与分类，”*arXiv预印本
    arXiv:1911.08744*，2019年。'
- en: '[39] A. Farzad, “Log message anomaly detection with oversampling,” *International
    Journal of Artificial Intelligence and Applications (IJAIA)*, vol. 11, no. 4,
    2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. Farzad，“基于过采样的日志消息异常检测，”*国际人工智能与应用期刊（IJAIA）*，第11卷，第4期，2020年。'
- en: '[40] A. Farzad and T. A. Gulliver, “Two class pruned log message anomaly detection,”
    *SN Computer Science*, vol. 2, no. 5, pp. 1–18, 2021.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. Farzad 和 T. A. Gulliver，“两类剪枝日志消息异常检测，”*SN计算机科学*，第2卷，第5期，第1–18页，2021年。'
- en: '[41] S. Gu, Y. Chu, W. Zhang, P. Liu, Q. Yin, and Q. Li, “Research on system
    log anomaly detection combining two-way slice gru and ga-attention mechanism,”
    in *2021 4th International Conference on Artificial Intelligence and Big Data
    (ICAIBD)*.   IEEE, 2021, pp. 577–583.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Gu, Y. Chu, W. Zhang, P. Liu, Q. Yin, 和 Q. Li，“结合双向切片gru和ga-attention机制的系统日志异常检测研究，”发表于*2021年第4届人工智能与大数据国际会议（ICAIBD）*。IEEE，2021年，第577–583页。'
- en: '[42] Y. Guo, Y. Wu, Y. Zhu, B. Yang, and C. Han, “Anomaly detection using distributed
    log data: A lightweight federated learning approach,” in *2021 International Joint
    Conference on Neural Networks (IJCNN)*.   IEEE, 2021, pp. 1–8.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Guo, Y. Wu, Y. Zhu, B. Yang, 和 C. Han，“使用分布式日志数据的异常检测：一种轻量级的联邦学习方法，”发表于*2021年国际联合神经网络会议（IJCNN）*。IEEE，2021年，第1–8页。'
- en: '[43] H. Guo, X. Lin, J. Yang, Y. Zhuang, J. Bai, B. Zhang, T. Zheng, and Z. Li,
    “Translog: A unified transformer-based framework for log anomaly detection,” *arXiv
    preprint arXiv:2201.00016*, 2021.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H. Guo, X. Lin, J. Yang, Y. Zhuang, J. Bai, B. Zhang, T. Zheng, 和 Z. Li，“Translog：一个基于变换器的统一日志异常检测框架，”*arXiv预印本
    arXiv:2201.00016*，2021年。'
- en: '[44] X. Han and S. Yuan, “Unsupervised cross-system log anomaly detection via
    domain adaptation,” in *Proceedings of the 30th ACM International Conference on
    Information & Knowledge Management*, 2021, pp. 3068–3072.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] X. Han 和 S. Yuan，“通过领域适应的无监督跨系统日志异常检测，”发表于*第30届ACM国际信息与知识管理会议论文集*，2021年，第3068–3072页。'
- en: '[45] S. Hashemi and M. Mäntylä, “Onelog: Towards end-to-end training in software
    log anomaly detection,” *arXiv preprint arXiv:2104.07324*, 2021.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Hashemi 和 M. Mäntylä，“Onelog：致力于软件日志异常检测的端到端训练，”*arXiv预印本 arXiv:2104.07324*，2021年。'
- en: '[46] R. Hirakawa, K. Tominaga, and Y. Nakatoh, “Software log anomaly detection
    through one class clustering of transformer encoder representation,” in *International
    Conference on Human-Computer Interaction*.   Springer, 2020, pp. 655–661.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Hirakawa, K. Tominaga, 和 Y. Nakatoh, “通过变换器编码器表示的一类聚类进行软件日志异常检测，” 见
    *国际人机交互会议*。Springer，2020 年，页码 655–661。'
- en: '[47] S. Huang, Y. Liu, C. Fung, R. He, Y. Zhao, H. Yang, and Z. Luan, “Hitanomaly:
    Hierarchical transformers for anomaly detection in system log,” *IEEE Transactions
    on Network and Service Management*, vol. 17, no. 4, pp. 2064–2076, 2020.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] S. Huang, Y. Liu, C. Fung, R. He, Y. Zhao, H. Yang, 和 Z. Luan, “Hitanomaly:
    用于系统日志异常检测的层次变换器，” *IEEE 网络与服务管理汇刊*，第 17 卷，第 4 期，页码 2064–2076，2020 年。'
- en: '[48] V.-H. Le and H. Zhang, “Log-based anomaly detection without log parsing,”
    in *2021 36th IEEE/ACM International Conference on Automated Software Engineering
    (ASE)*.   IEEE, 2021, pp. 492–504.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] V.-H. Le 和 H. Zhang, “无需日志解析的基于日志的异常检测，” 见 *2021 第 36 届 IEEE/ACM 自动化软件工程国际会议（ASE）*。IEEE，2021
    年，页码 492–504。'
- en: '[49] H. Li and Y. Li, “Logspy: System log anomaly detection for distributed
    systems,” in *2020 International Conference on Artificial Intelligence and Computer
    Engineering (ICAICE)*.   IEEE, 2020, pp. 347–352.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] H. Li 和 Y. Li, “Logspy: 用于分布式系统的系统日志异常检测，” 见 *2020 国际人工智能与计算机工程会议（ICAICE）*。IEEE，2020
    年，页码 347–352。'
- en: '[50] X. Li, P. Chen, L. Jing, Z. He, and G. Yu, “Swisslog: Robust and unified
    deep learning based log anomaly detection for diverse faults,” in *2020 IEEE 31st
    International Symposium on Software Reliability Engineering (ISSRE)*.   IEEE,
    2020, pp. 92–103.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. Li, P. Chen, L. Jing, Z. He, 和 G. Yu, “Swisslog: 针对多样故障的稳健统一深度学习日志异常检测，”
    见 *2020 IEEE 第 31 届国际软件可靠性工程研讨会（ISSRE）*。IEEE，2020 年，页码 92–103。'
- en: '[51] X. Liu, W. Liu, X. Di, J. Li, B. Cai, W. Ren, and H. Yang, “Lognads: Network
    anomaly detection scheme based on log semantics representation,” *Future Generation
    Computer Systems*, vol. 124, pp. 390–405, 2021.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. Liu, W. Liu, X. Di, J. Li, B. Cai, W. Ren, 和 H. Yang, “Lognads: 基于日志语义表示的网络异常检测方案，”
    *未来一代计算机系统*，第 124 卷，页码 390–405，2021 年。'
- en: '[52] D. Lv, N. Luktarhan, and Y. Chen, “Conanomaly: Content-based anomaly detection
    for system logs,” *Sensors*, vol. 21, no. 18, p. 6125, 2021.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] D. Lv, N. Luktarhan, 和 Y. Chen, “Conanomaly: 基于内容的系统日志异常检测，” *传感器*，第 21
    卷，第 18 期，页码 6125，2021 年。'
- en: '[53] K. Otomo, S. Kobayashi, K. Fukuda, and H. Esaki, “Latent variable based
    anomaly detection in network system logs,” *IEICE TRANSACTIONS on Information
    and Systems*, vol. 102, no. 9, pp. 1644–1652, 2019.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] K. Otomo, S. Kobayashi, K. Fukuda, 和 H. Esaki, “基于潜变量的网络系统日志异常检测，” *IEICE
    信息与系统汇刊*，第 102 卷，第 9 期，页码 1644–1652，2019 年。'
- en: '[54] H. Ott, J. Bogatinovski, A. Acker, S. Nedelkoski, and O. Kao, “Robust
    and transferable anomaly detection in log data using pre-trained language models,”
    in *2021 IEEE/ACM International Workshop on Cloud Intelligence (CloudIntelligence)*.   IEEE,
    2021, pp. 19–24.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Ott, J. Bogatinovski, A. Acker, S. Nedelkoski, 和 O. Kao, “使用预训练语言模型进行稳健且可转移的日志数据异常检测，”
    见 *2021 IEEE/ACM 国际云智能研讨会（CloudIntelligence）*。IEEE，2021 年，页码 19–24。'
- en: '[55] A. Patil, A. Wadekar, T. Gupta, R. Vijan, and F. Kazi, “Explainable lstm
    model for anomaly detection in hdfs log file using layerwise relevance propagation,”
    in *2019 IEEE Bombay Section Signature Conference (IBSSC)*.   IEEE, 2019, pp.
    1–6.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] A. Patil, A. Wadekar, T. Gupta, R. Vijan, 和 F. Kazi, “用于 HDFS 日志文件异常检测的可解释
    LSTM 模型，采用层级相关传播，” 见 *2019 IEEE 孟买分区签名会议（IBSSC）*。IEEE，2019 年，页码 1–6。'
- en: '[56] Y. Qian, S. Ying, and B. Wang, “Anomaly detection in distributed systems
    via variational autoencoders,” in *2020 IEEE International Conference on Systems,
    Man, and Cybernetics (SMC)*.   IEEE, 2020, pp. 2822–2829.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Qian, S. Ying, 和 B. Wang, “通过变分自编码器进行分布式系统中的异常检测，” 见 *2020 IEEE 国际系统、人机与控制大会（SMC）*。IEEE，2020
    年，页码 2822–2829。'
- en: '[57] H. Studiawan and F. Sohel, “Anomaly detection in a forensic timeline with
    deep autoencoders,” *Journal of Information Security and Applications*, vol. 63,
    p. 103002, 2021.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] H. Studiawan 和 F. Sohel, “在法医时间线中的异常检测与深度自编码器，” *信息安全与应用杂志*，第 63 卷，页码
    103002，2021 年。'
- en: '[58] H. Studiawan, F. Sohel, and C. Payne, “Anomaly detection in operating
    system logs with deep learning-based sentiment analysis,” *IEEE Transactions on
    Dependable and Secure Computing*, vol. 18, no. 5, pp. 2136–2148, 2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Studiawan, F. Sohel, 和 C. Payne, “基于深度学习的情感分析在操作系统日志中的异常检测，” *IEEE
    可靠与安全计算汇刊*，第 18 卷，第 5 期，页码 2136–2148，2020 年。'
- en: '[59] P. Sun, E. Yuepeng, T. Li, Y. Wu, J. Ge, J. You, and B. Wu, “Context-aware
    learning for anomaly detection with imbalanced log data,” in *2020 IEEE 22nd International
    Conference on High Performance Computing and Communications; IEEE 18th International
    Conference on Smart City; IEEE 6th International Conference on Data Science and
    Systems (HPCC/SmartCity/DSS)*.   IEEE, 2020, pp. 449–456.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. Sundqvist, M. H. Bhuyan, J. Forsman, and E. Elmroth, “Boosted ensemble
    learning for anomaly detection in 5g ran,” in *IFIP International Conference on
    Artificial Intelligence Applications and Innovations*.   Springer, 2020, pp. 15–30.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] S. Syngal, S. Verma, K. Karthik, Y. Katyal, and S. Ghosh, “Server-language
    processing: A semi-supervised approach to server failure detection,” in *2021
    2nd International Conference on Computing, Networks and Internet of Things*, 2021,
    pp. 1–7.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] A. Wadekar, T. Gupta, R. Vijan, and F. Kazi, “Hybrid cae-vae for unsupervised
    anomaly detection in log file systems,” in *2019 10th International Conference
    on Computing, Communication and Networking Technologies (ICCCNT)*.   IEEE, 2019,
    pp. 1–7.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Wan, Y. Liu, D. Wang, and Y. Wen, “Glad-paw: Graph-based log anomaly
    detection by position aware weighted graph attention network,” in *Pacific-Asia
    Conference on Knowledge Discovery and Data Mining*.   Springer, 2021, pp. 66–77.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. Wang, L. Xu, and L. Guo, “Anomaly detection of system logs based on
    natural language processing and deep learning,” in *2018 4th International Conference
    on Frontiers of Signal Processing (ICFSP)*.   IEEE, 2018, pp. 140–144.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Z. Wang, J. Tian, H. Fang, L. Chen, and J. Qin, “Lightlog: A lightweight
    temporal convolutional network for log anomaly detection on the edge,” *Computer
    Networks*, vol. 203, p. 108616, 2022.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Z. Wang, Z. Chen, J. Ni, H. Liu, H. Chen, and J. Tang, “Multi-scale one-class
    recurrent neural networks for discrete event sequence anomaly detection,” in *Proceedings
    of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining*, 2021,
    pp. 3726–3734.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] S. R. Wibisono and A. I. Kistijantoro, “Log anomaly detection using adaptive
    universal transformer,” in *2019 International Conference of Advanced Informatics:
    Concepts, Theory and Applications (ICAICTA)*.   IEEE, 2019, pp. 1–6.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] T. Wittkopp, A. Acker, S. Nedelkoski, J. Bogatinovski, D. Scheinert, W. Fan,
    and O. Kao, “A2log: attentive augmented log anomaly detection,” *arXiv preprint
    arXiv:2109.09537*, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] L. Xi, Y. Xin, S. Luo, Y. Shang, and Q. Tang, “Anomaly detection mechanism
    based on hierarchical weights through large-scale log data,” in *2021 International
    Conference on Computer Communication and Artificial Intelligence (CCAI)*.   IEEE,
    2021, pp. 106–115.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] B. Xia, Y. Bai, J. Yin, Y. Li, and J. Xu, “Loggan: A log-level generative
    adversarial network for anomaly detection using permutation event modeling,” *Information
    Systems Frontiers*, vol. 23, no. 2, pp. 285–298, 2021.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C. Xiao, J. Huang, and W. Wu, “Detecting anomalies in cluster system using
    hybrid deep learning model,” in *International Symposium on Parallel Architectures,
    Algorithms and Programming*.   Springer, 2019, pp. 393–404.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] C. Xiao, J. Huang, 和 W. Wu，“使用混合深度学习模型检测集群系统中的异常”，发表于*国际并行体系结构、算法与编程研讨会*。Springer，2019，第393–404页。'
- en: '[72] Y. Xie, L. Ji, and X. Cheng, “An attention-based gru network for anomaly
    detection from system logs,” *IEICE TRANSACTIONS on Information and Systems*,
    vol. 103, no. 8, pp. 1916–1919, 2020.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Xie, L. Ji, 和 X. Cheng，“基于注意力的GRU网络用于系统日志的异常检测”，*IEICE信息与系统交易*，第103卷，第8期，第1916–1919页，2020。'
- en: '[73] R. Yang, D. Qu, Y. Gao, Y. Qian, and Y. Tang, “Nlsalog: An anomaly detection
    framework for log sequence in security management,” *IEEE Access*, vol. 7, pp.
    181 152–181 164, 2019.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] R. Yang, D. Qu, Y. Gao, Y. Qian, 和 Y. Tang，“Nlsalog：用于安全管理中日志序列的异常检测框架”，*IEEE
    Access*，第7卷，第181 152–181 164页，2019。'
- en: '[74] L. Yang, J. Chen, Z. Wang, W. Wang, J. Jiang, X. Dong, and W. Zhang, “Semi-supervised
    log-based anomaly detection via probabilistic label estimation,” in *2021 IEEE/ACM
    43rd International Conference on Software Engineering (ICSE)*.   IEEE, 2021, pp.
    1448–1460.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] L. Yang, J. Chen, Z. Wang, W. Wang, J. Jiang, X. Dong, 和 W. Zhang，“通过概率标签估计的半监督基于日志的异常检测”，发表于*2021
    IEEE/ACM第43届国际软件工程会议（ICSE）*。IEEE，2021，第1448–1460页。'
- en: '[75] S. Yen, M. Moh, and T.-S. Moh, “Causalconvlstm: Semi-supervised log anomaly
    detection through sequence modeling,” in *2019 18th IEEE International Conference
    On Machine Learning And Applications (ICMLA)*.   IEEE, 2019, pp. 1334–1341.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. Yen, M. Moh, 和 T.-S. Moh，“Causalconvlstm：通过序列建模的半监督日志异常检测”，发表于*2019年第18届IEEE国际机器学习与应用会议（ICMLA）*。IEEE，2019，第1334–1341页。'
- en: '[76] K. Yin, M. Yan, L. Xu, Z. Xu, Z. Li, D. Yang, and X. Zhang, “Improving
    log-based anomaly detection with component-aware analysis,” in *2020 IEEE International
    Conference on Software Maintenance and Evolution (ICSME)*.   IEEE, 2020, pp. 667–671.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] K. Yin, M. Yan, L. Xu, Z. Xu, Z. Li, D. Yang, 和 X. Zhang，“通过组件感知分析改进基于日志的异常检测”，发表于*2020
    IEEE国际软件维护与演化会议（ICSME）*。IEEE，2020，第667–671页。'
- en: '[77] D. Yu, X. Hou, C. Li, Q. Lv, Y. Wang, and N. Li, “Anomaly detection in
    unstructured logs using attention-based bi-lstm network,” in *2021 7th IEEE International
    Conference on Network Intelligence and Digital Content (IC-NIDC)*.   IEEE, 2021,
    pp. 403–407.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] D. Yu, X. Hou, C. Li, Q. Lv, Y. Wang, 和 N. Li，“使用基于注意力的双向LSTM网络进行非结构化日志异常检测”，发表于*2021年第7届IEEE国际网络智能与数字内容会议（IC-NIDC）*。IEEE，2021，第403–407页。'
- en: '[78] L. Zhang, W. Li, Z. Zhang, Q. Lu, C. Hou, P. Hu, T. Gui, and S. Lu, “Logattn:
    Unsupervised log anomaly detection with an autoencoder based attention mechanism,”
    in *International Conference on Knowledge Science, Engineering and Management*.   Springer,
    2021, pp. 222–235.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] L. Zhang, W. Li, Z. Zhang, Q. Lu, C. Hou, P. Hu, T. Gui, 和 S. Lu，“Logattn：基于自编码器的注意机制的无监督日志异常检测”，发表于*国际知识科学、工程与管理会议*。Springer，2021，第222–235页。'
- en: '[79] C. Zhang, X. Wang, H. Zhang, H. Zhang, and P. Han, “Log sequence anomaly
    detection based on local information extraction and globally sparse transformer
    model,” *IEEE Transactions on Network and Service Management*, vol. 18, no. 4,
    pp. 4119–4133, 2021.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C. Zhang, X. Wang, H. Zhang, H. Zhang, 和 P. Han，“基于局部信息提取和全局稀疏变换器模型的日志序列异常检测”，*IEEE网络与服务管理交易*，第18卷，第4期，第4119–4133页，2021。'
- en: '[80] D. Zhang, D. Dai, R. Han, and M. Zheng, “Sentilog: Anomaly detecting on
    parallel file systems via log-based sentiment analysis,” in *Proceedings of the
    13th ACM Workshop on Hot Topics in Storage and File Systems*, 2021, pp. 86–93.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. Zhang, D. Dai, R. Han, 和 M. Zheng，“Sentilog：通过基于日志的情感分析在并行文件系统上进行异常检测”，发表于*第13届ACM存储与文件系统热点话题研讨会论文集*，2021，第86–93页。'
- en: '[81] Z. Zhao, W. Niu, X. Zhang, R. Zhang, Z. Yu, and C. Huang, “Trine: Syslog
    anomaly detection with three transformer encoders in one generative adversarial
    network,” *Applied Intelligence*, pp. 1–10, 2021.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Z. Zhao, W. Niu, X. Zhang, R. Zhang, Z. Yu, 和 C. Huang，“Trine：在一个生成对抗网络中使用三个变换器编码器进行系统日志异常检测”，*应用智能*，第1–10页，2021。'
- en: '[82] P. Zhou, Y. Wang, Z. Li, X. Wang, G. Tyson, and G. Xie, “Logsayer: Log
    pattern-driven cloud component anomaly diagnosis with machine learning,” in *2020
    IEEE/ACM 28th International Symposium on Quality of Service (IWQoS)*.   IEEE,
    2020, pp. 1–10.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] P. Zhou, Y. Wang, Z. Li, X. Wang, G. Tyson, 和 G. Xie，“Logsayer：基于日志模式驱动的云组件异常诊断与机器学习”，发表于*2020
    IEEE/ACM第28届国际服务质量研讨会（IWQoS）*。IEEE，2020，第1–10页。'
- en: '[83] B. Zhu, J. Li, R. Gu, and L. Wang, “An approach to cloud platform log
    anomaly detection based on natural language processing and lstm,” in *2020 3rd
    International Conference on Algorithms, Computing and Artificial Intelligence*,
    2020, pp. 1–7.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] I. H. Sarker, “Deep cybersecurity: a comprehensive overview from neural
    network and deep learning perspective,” *SN Computer Science*, vol. 2, no. 3,
    pp. 1–16, 2021.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Z. Zhang and M. Sabuncu, “Generalized cross entropy loss for training
    deep neural networks with noisy labels,” *Advances in neural information processing
    systems*, vol. 31, 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y. Cui, S. Ahmad, and J. Hawkins, “Continuous online sequence learning
    with an unsupervised neural network model,” *Neural computation*, vol. 28, no. 11,
    pp. 2474–2504, 2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] R. Hadsell, D. Rao, A. A. Rusu, and R. Pascanu, “Embracing change: Continual
    learning in deep neural networks,” *Trends in cognitive sciences*, vol. 24, no. 12,
    pp. 1028–1040, 2020.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, “Drain: An online log parsing
    approach with fixed depth tree,” in *2017 IEEE international conference on web
    services (ICWS)*.   IEEE, 2017, pp. 33–40.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] M. Du and F. Li, “Spell: Streaming parsing of system event logs,” in *2016
    IEEE 16th International Conference on Data Mining (ICDM)*.   IEEE, 2016, pp. 859–864.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
    word representations in vector space,” *arXiv preprint arXiv:1301.3781*, 2013.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for
    word representation,” in *Proceedings of the 2014 conference on empirical methods
    in natural language processing (EMNLP)*, 2014, pp. 1532–1543.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] C. Manning, P. Raghavan, and H. Schütze, “Introduction to information
    retrieval,” *Natural Language Engineering*, vol. 16, no. 1, pp. 100–103, 2010.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Oliner and J. Stearley, “What supercomputers say: A study of five system
    logs,” in *37th annual IEEE/IFIP international conference on dependable systems
    and networks (DSN’07)*.   IEEE, 2007, pp. 575–584.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Q. Lin, H. Zhang, J.-G. Lou, Y. Zhang, and X. Chen, “Log clustering based
    problem identification for online service systems,” in *Proceedings of the 38th
    International Conference on Software Engineering Companion*, 2016, pp. 102–111.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] S. Garfinkel, P. Farrell, V. Roussev, and G. Dinolt, “Bringing science
    to digital forensics with standardized forensic corpora,” *digital investigation*,
    vol. 6, pp. S2–S11, 2009.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] C. Eoghan and G. R. I. Golden, “Dfrws 2009 forensics challenge,” http://old.dfrws.org/2009/challenge/index.shtml,
    accessed: 2022-05-13.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] G. Arcas, H. Gonzales, and J. Cheng, “The honeynet project 2011 challenge
    7 – forensic analysis of a compromised server,” https://www.honeynet.org/challenges/forensic-challenge-7-analysis-of-a-compromised-server/,
    accessed: 2022-05-13.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. He, J. Zhu, P. He, and M. R. Lyu, “Loghub: a large collection of system
    log datasets towards automated log analytics,” *arXiv preprint arXiv:2008.06448*,
    2020.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] A. Chuvakin, “Public security log sharing site,” https://log-sharing.dreamhosters.com/,
    Nov. 2010, accessed: 2021-10-18.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] R. Marty, A. Chuvakin, and S. Tricaud, “The honeynet project 2010 challenge
    5 – log mysteries,” https://www.honeynet.org/challenges/forensic-challenge-7-analysis-of-a-compromised-server/,
    accessed: 2022-05-13.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Z. Zheng, L. Yu, W. Tang, Z. Lan, R. Gupta, N. Desai, S. Coghlan, and
    D. Buettner, “Co-analysis of ras log and job log on blue gene/p,” in *2011 IEEE
    International Parallel & Distributed Processing Symposium*.   IEEE, 2011, pp.
    840–851.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] W. Xu, L. Huang, A. Fox, D. Patterson, and M. Jordan, “Largescale system
    problem detection by mining console logs,” *Proceedings of SOSP’09*, 2009.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J.-G. Lou, Q. Fu, S. Yang, Y. Xu, and J. Li, “Mining invariants from
    console logs for system problem detection,” in *2010 USENIX Annual Technical Conference
    (USENIX ATC 10)*, 2010.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Liang, Y. Zhang, H. Xiong, and R. Sahoo, “Failure prediction in ibm
    bluegene/l event logs,” in *Seventh IEEE International Conference on Data Mining
    (ICDM 2007)*.   IEEE, 2007, pp. 583–588.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] B. Schölkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson,
    “Estimating the support of a high-dimensional distribution,” *Neural computation*,
    vol. 13, no. 7, pp. 1443–1471, 2001.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] D. M. Tax and R. P. Duin, “Support vector data description,” *Machine
    learning*, vol. 54, no. 1, pp. 45–66, 2004.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in *2008 eighth
    ieee international conference on data mining*.   IEEE, 2008, pp. 413–422.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] P. Bodik, M. Goldszmidt, A. Fox, D. B. Woodard, and H. Andersen, “Fingerprinting
    the datacenter: automated classification of performance crises,” in *Proceedings
    of the 5th European conference on Computer systems*, 2010, pp. 111–124.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M. Chen, A. X. Zheng, J. Lloyd, M. I. Jordan, and E. Brewer, “Failure
    diagnosis using decision trees,” in *International Conference on Autonomic Computing,
    2004\. Proceedings.*   IEEE, 2004, pp. 36–43.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] D. Preuveneers, V. Rimmer, I. Tsingenopoulos, J. Spooren, W. Joosen,
    and E. Ilie-Zudor, “Chained anomaly detection models for federated learning: An
    intrusion detection case study,” *Applied Sciences*, vol. 8, no. 12, p. 2663,
    2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,
    A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins *et al.*, “Explainable
    artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges
    toward responsible ai,” *Information fusion*, vol. 58, pp. 82–115, 2020.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,
    A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins *等*，“可解释人工智能（xai）：概念、分类、机遇与挑战，迈向负责任的人工智能”，*信息融合*，第58卷，第82–115页，2020年。'
- en: '[113] M. Landauer, F. Skopik, M. Wurzenberger, W. Hotwagner, and A. Rauber,
    “Have it your way: generating customized log datasets with a model-driven simulation
    testbed,” *IEEE Transactions on Reliability*, vol. 70, no. 1, pp. 402–415, 2020.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. Landauer, F. Skopik, M. Wurzenberger, W. Hotwagner, 和 A. Rauber，“按需定制：利用模型驱动的模拟测试平台生成自定义日志数据集”，*IEEE可靠性交易*，第70卷，第1期，第402–415页，2020年。'
- en: '[114] M. Landauer, F. Skopik, M. Frank, W. Hotwagner, M. Wurzenberger, and
    A. Rauber, “Maintainable log datasets for evaluation of intrusion detection systems,”
    *arXiv preprint arXiv:2203.08580*, 2022.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] M. Landauer, F. Skopik, M. Frank, W. Hotwagner, M. Wurzenberger, 和 A.
    Rauber，“用于入侵检测系统评估的可维护日志数据集”，*arXiv预印本 arXiv:2203.08580*，2022年。'
- en: '[115] S. Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaff, “A sense
    of self for unix processes,” in *Proceedings 1996 IEEE Symposium on Security and
    Privacy*.   IEEE, 1996, pp. 120–128.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] S. Forrest, S. A. Hofmeyr, A. Somayaji, 和 T. A. Longstaff，“UNIX进程的自我感知”，见于*1996年IEEE安全与隐私研讨会论文集*。
    IEEE，1996年，第120–128页。'
- en: '[116] M. Mäntylä, M. Varela, and S. Hashemi, “Pinpointing anomaly events in
    logs from stability testing–n-grams vs. deep-learning,” in *2022 IEEE International
    Conference on Software Testing, Verification and Validation Workshops (ICSTW)*.   IEEE,
    2022, pp. 285–292.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] M. Mäntylä, M. Varela, 和 S. Hashemi，“在稳定性测试日志中定位异常事件——n-grams与深度学习的比较”，见于*2022年IEEE国际软件测试、验证与验证研讨会（ICSTW）*。
    IEEE，2022年，第285–292页。'
