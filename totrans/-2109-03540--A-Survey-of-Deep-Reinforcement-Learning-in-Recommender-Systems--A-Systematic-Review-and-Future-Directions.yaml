- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:51:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2109.03540] A Survey of Deep Reinforcement Learning in Recommender Systems:
    A Systematic Review and Future Directions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2109.03540] 深度强化学习在推荐系统中的调查：系统回顾与未来方向'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2109.03540](https://ar5iv.labs.arxiv.org/html/2109.03540)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2109.03540](https://ar5iv.labs.arxiv.org/html/2109.03540)
- en: 'A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic
    Review and Future Directions'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在推荐系统中的调查：系统回顾与未来方向
- en: Xiaocong Chen [xiaocong.chen@unsw.edu.au](mailto:xiaocong.chen@unsw.edu.au)
    University of New South WalesSydneyNSWAustralia ,  Lina Yao [lina.yao@unsw.edu.au](mailto:lina.yao@unsw.edu.au)
    University of New South WalesSydneyNSWAustralia ,  Julian Mcauley [jmcauley@eng.ucsd.edu](mailto:jmcauley@eng.ucsd.edu)
    University of California, San DiegoCAUSA ,  Guanglin Zhou [guanglin.zhou@unsw.edu.au](mailto:guanglin.zhou@unsw.edu.au)
    University of New South WalesSydneyNSWAustralia  and  Xianzhi Wang [xianzhi.wang@uts.edu.au](mailto:xianzhi.wang@uts.edu.au)
    University of Technology SydneySydneyNSWAustralia(2021)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xiaocong Chen [xiaocong.chen@unsw.edu.au](mailto:xiaocong.chen@unsw.edu.au)
    新南威尔士大学 悉尼 NSW 澳大利亚，Lina Yao [lina.yao@unsw.edu.au](mailto:lina.yao@unsw.edu.au)
    新南威尔士大学 悉尼 NSW 澳大利亚，Julian Mcauley [jmcauley@eng.ucsd.edu](mailto:jmcauley@eng.ucsd.edu)
    加州大学圣地亚哥分校 CA USA，Guanglin Zhou [guanglin.zhou@unsw.edu.au](mailto:guanglin.zhou@unsw.edu.au)
    新南威尔士大学 悉尼 NSW 澳大利亚，以及 Xianzhi Wang [xianzhi.wang@uts.edu.au](mailto:xianzhi.wang@uts.edu.au)
    先进技术大学 悉尼 NSW 澳大利亚（2021）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In light of the emergence of deep reinforcement learning (DRL) in recommender
    systems research and several fruitful results in recent years, this survey aims
    to provide a timely and comprehensive overview of the recent trends of deep reinforcement
    learning in recommender systems. We start with the motivation of applying DRL
    in recommender systems. Then, we provide a taxonomy of current DRL-based recommender
    systems and a summary of existing methods. We discuss emerging topics and open
    issues, and provide our perspective on advancing the domain. This survey serves
    as introductory material for readers from academia and industry into the topic
    and identifies notable opportunities for further research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于深度强化学习（DRL）在推荐系统研究中的兴起以及近年来取得的一些丰硕成果，本调查旨在提供对深度强化学习在推荐系统中最新趋势的及时而全面的概述。我们从应用DRL于推荐系统的动机开始，然后提供当前基于DRL的推荐系统的分类法和现有方法的总结。我们讨论了新兴话题和开放性问题，并提供了我们对推动该领域发展的观点。本调查作为学术界和工业界读者的入门材料，并识别了进一步研究的显著机会。
- en: 'Deep Reinforcement Learning, Deep Learning, recommender systems^†^†copyright:
    acmcopyright^†^†journalyear: 2021^†^†doi: 10.1145/1122445.1122456^†^†journal:
    JACM^†^†journalvolume: 37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth:
    9^†^†ccs: Information systems Recommender systems^†^†ccs: Computing methodologies Reinforcement
    learning^†^†ccs: Computing methodologies Neural networks'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习、深度学习、推荐系统^†^†版权：acmcopyright^†^†期刊年份：2021^†^†doi：10.1145/1122445.1122456^†^†期刊：JACM^†^†期刊卷号：37^†^†期刊期号：4^†^†文章：111^†^†出版月份：9^†^†ccs：信息系统
    推荐系统^†^†ccs：计算方法 强化学习^†^†ccs：计算方法 神经网络
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Recent years have seen significant progress in recommendation techniques, from
    traditional recommendation techniques, e.g., collaborative filtering, content-based
    recommendation and matrix factorization (Lu et al., [2015](#bib.bib63)), to deep
    learning based techniques. In particular, deep learning show strong advantages
    in solving complex tasks and dealing with complex data, due to its capability
    to capture non-linear user-item relationships and deal with various types of data
    sources such as images and text. It has thus been increasingly used in recommender
    systems. Deep learning-based recommender systems have limitations in capturing
    interest dynamics (Chen et al., [2020b](#bib.bib18); Zhang et al., [2019b](#bib.bib116))
    due to distribution shift, i.e., the training phase is based on an existing dataset
    which may not reflect real user preferences that undergo rapid change. In contrast,
    deep reinforcement learning (DRL) aims to train an agent that can learn from interaction
    trajectories provided by the environment by combining the power of deep learning
    and reinforcement learning. Since an agent in DRL can actively learn from users’
    real-time feedback to infer dynamic user preferences, DRL is especially suitable
    for learning from interactions, such as human-robot collaboration; it has also
    driven significant advances in a range of interactive applications ranging from
    video games, Alpha Go to autonomous driving (Arulkumaran et al., [2017](#bib.bib4)).
    In light of the significance and recent progresses in DRL for recommender sytsems,
    we aim to timely summaize and comment on DRL-based recommendation systems in this
    survey.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: A recent survey on reinforcement learning based recommender systems (Afsar et al.,
    [2021](#bib.bib3)) provides a general review about reinforcement learning in recommender
    systems without a sophsiticated investigation of the growing area of deep reinforcement
    learning. Our survey distinguishes itself in providing a systematic and comprehensive
    overview of existing methods in DRL-based recommender systems, along with a discussion
    of emerging topics, open issues, and future directions. This survey introduces
    researchers, practitioners and educators into this topic and fostering an understanding
    of the key techniques in the area.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this survey include the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide an up-to-date comprehensive review of deep reinforcement learning
    in recommender systems, with state of the art techniques and pointers to core
    references. To the best of our knowledge, this the first comprehensive survey
    in deep reinforcement learning based recommender systems.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present a taxonomy of the literature of deep reinforcement learning in recommender
    systems. Along with the outlined taxonomy and literature overview, we discuss
    the benefits, drawbacks and give suggestions for future research directions.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We shed light on emerging topics and open issues for DRL-based recommender systems.
    We also point out future directions that could be crucial for advancing DRL-based
    recommender systems.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们阐明了基于 DRL 的推荐系统的新兴话题和开放性问题。我们还指出了可能对推进基于 DRL 的推荐系统至关重要的未来方向。
- en: 'The remainder of this survey is organized as follows: Section 2 provides an
    overview of recommender systems, DRL and their integration. Section 3 provides
    a literature review with a taxonomy and classification mechanism. Section 4 reviews
    emerging topics, and Section 5 points out open questions. Finally, Section 6 provides
    a few promising future directions for further advances in this domain.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分组织如下：第2节提供了推荐系统、DRL 及其集成的概述。第3节提供了文献综述，包含分类和分类机制。第4节回顾了新兴话题，第5节指出了开放性问题。最后，第6节提供了一些有前景的未来方向，以促进该领域的进一步发展。
- en: '![Refer to caption](img/38e5bc0de6f7934538a6bd40e290e058.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/38e5bc0de6f7934538a6bd40e290e058.png)'
- en: Figure 1. Taxonomy of Deep Reinforcement Learning based Recommender Systems
    in this survey
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 本调查中的基于深度强化学习的推荐系统的分类
- en: 2\. Background
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: In this section,we introduce key concepts related to dynamic recommender systems
    (RS) and deep reinforcement learning (DRL), and motivate the introduction of DRL
    to dynamic recommender systems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍与动态推荐系统（RS）和深度强化学习（DRL）相关的关键概念，并激励将 DRL 引入动态推荐系统。
- en: 2.1\. Why Deep Reinforcement Learning for Recommendation?
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 为什么选择深度强化学习进行推荐？
- en: Recommender systems require coping with *dynamic* environments by estimating
    rapidly changing users’ preferences and proactively recommending items to users.
    Let $\mathcal{U}$ be a set of users of cardinality $|\mathcal{U}|$ and $\mathcal{I}$
    be a set of items of cardinality $|\mathcal{I}|$. For each user $u\in\mathcal{U}$,
    we observe a sequence of user actions $\mathbb{X}^{u}=[x_{1}^{u},x_{2}^{u},\cdots,x_{T_{u}}^{u}]$
    with item $x_{t}^{u}\in\mathcal{I}$, i.e., each event in a user sequence comes
    from the item set. We refer to a user making a decision as an interaction with
    an item. Suppose the feedback (e.g., ratings or clicking behavior) provided by
    users is $\mathcal{F}$, then a dynamic recommender system maintains the corresponding
    recommendation policy $\pi^{u}_{t}$, which will be updated systematically based
    on the feedback $f^{u}_{i}\in\mathcal{F}$ received during the interaction for
    item $i\in\mathcal{I}$ at the timestamp $t$.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统需要通过估计快速变化的用户偏好并主动向用户推荐项目来应对*动态*环境。设 $\mathcal{U}$ 为一个用户集合，其基数为 $|\mathcal{U}|$，$\mathcal{I}$
    为一个项目集合，其基数为 $|\mathcal{I}|$。对于每个用户 $u\in\mathcal{U}$，我们观察到一个用户行为序列 $\mathbb{X}^{u}=[x_{1}^{u},x_{2}^{u},\cdots,x_{T_{u}}^{u}]$，其中项目
    $x_{t}^{u}\in\mathcal{I}$，即用户序列中的每个事件都来自于项目集合。我们将用户做出决策称为与项目的交互。假设用户提供的反馈（例如，评分或点击行为）为
    $\mathcal{F}$，则动态推荐系统维护相应的推荐策略 $\pi^{u}_{t}$，该策略将根据在时间戳 $t$ 与项目 $i\in\mathcal{I}$
    交互过程中收到的反馈 $f^{u}_{i}\in\mathcal{F}$ 系统地更新。
- en: 'The marriage of deep learning and reinforcement learning has fueled breakthroughs
    in recommender systems. DRL-based RS consists of a pipeline with three building
    blocks: environment construction, state representation and recommendation policy
    learning. Environment construction aims to build an environment based on a set
    of users’ historical behaviors. State representation is provided by the environment
    containing certain user information including historical behavior, demographic
    data (etc.). Recommendation policy learning is the key component to understand
    and predict users’ future behavior. DL-based RS receives user feedback (e.g.,
    ratings or clicks) to reflect users’ interests and update the recommender, while
    DRL-based RS receives the reward provided by the environment to update the policy.
    The reward provided by the environment is a pre-defined function containing several
    factors. The detailed process of DL based RS and DRL-based RS mapping can be found
    in [Figure 3](#S2.F3 "In 2.3\. DRL meets RS: Problem Formulation ‣ 2\. Background
    ‣ A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic
    Review and Future Directions").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与强化学习的结合推动了推荐系统的突破。基于DRL的推荐系统由三个构建块组成：环境构建、状态表示和推荐策略学习。环境构建的目标是基于一组用户的历史行为建立一个环境。状态表示由包含特定用户信息的环境提供，包括历史行为、人口统计数据（等）。推荐策略学习是理解和预测用户未来行为的关键组件。基于DL的推荐系统接收用户反馈（例如评分或点击）以反映用户兴趣并更新推荐系统，而基于DRL的推荐系统则接收环境提供的奖励以更新策略。环境提供的奖励是一个预定义的函数，包含几个因素。DL-based
    RS和DRL-based RS映射的详细过程可以在 [图 3](#S2.F3 "在 2.3\. DRL与RS相遇：问题表述 ‣ 2\. 背景 ‣ 深度强化学习在推荐系统中的系统评估及未来方向")中找到。
- en: 2.2\. Preliminaries of Deep Reinforcement Learning
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 深度强化学习的基础
- en: The typical defining feature of DRL is to use the deep learning to approximate
    reinforcement learning’s value function and solve high-dimensional Markov Decision
    Processes (MDPs) (Arulkumaran et al., [2017](#bib.bib4)). Formally, a MDP can
    be represented as a tuple ($\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma$).
    The agent chooses an action $a_{t}\in\mathcal{A}$ according to the policy $\pi_{t}(s_{t})$
    at state $s_{t}\in\mathcal{S}$. The environment receives the action and produces
    a reward $r_{t+1}\in\mathcal{R}$ and transfers the reward into the next state
    $s_{t+1}$ according to the transition probability $P(s_{t+1}|s_{t},a_{t})\in\mathcal{P}$.
    The transition probability $\mathcal{P}$ is unknown beforehand in DRL. Such a
    process continues until the agent reaches the terminal state or exceeds a pre-defined
    maximum time step. The overall objective is to maximize the expected discounted
    cumulative reward,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DRL的典型定义特征是利用深度学习来近似强化学习的价值函数，并解决高维的马尔可夫决策过程（MDPs） (Arulkumaran et al., [2017](#bib.bib4))。形式上，MDP可以表示为一个元组（$\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma$）。代理在状态$s_{t}\in\mathcal{S}$下，根据策略$\pi_{t}(s_{t})$选择一个动作$a_{t}\in\mathcal{A}$。环境接收该动作并产生一个奖励$r_{t+1}\in\mathcal{R}$，然后根据转移概率$P(s_{t+1}|s_{t},a_{t})\in\mathcal{P}$将奖励转移到下一个状态$s_{t+1}$。在DRL中，转移概率$\mathcal{P}$事先是未知的。这样的过程继续进行，直到代理达到终止状态或超过预定义的最大时间步。总体目标是最大化期望的折扣累积奖励，
- en: '| (1) |  | $\displaystyle\mathbb{E}_{\pi}[r_{t}]=\mathbb{E}_{\pi}\big{[}\sum_{0}^{\infty}\gamma^{k}r_{t+k}\big{]}$
    |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle\mathbb{E}_{\pi}[r_{t}]=\mathbb{E}_{\pi}\big{[}\sum_{0}^{\infty}\gamma^{k}r_{t+k}\big{]}$
    |  |'
- en: where $\gamma\in[0,1]$ is the discount factor that balances the future reward
    and the immediate reward.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\gamma\in[0,1]$是折扣因子，用于平衡未来奖励和即时奖励。
- en: '![Refer to caption](img/7a6b948b547739e72dda6a0233d5c3ec.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7a6b948b547739e72dda6a0233d5c3ec.png)'
- en: Figure 2. Taxonomy of Deep Reinforcement Learning in Recommender Systems
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 推荐系统中深度强化学习的分类
- en: 'Deep reinforcement learning can be divided into two categories: *model-based*
    and *model-free* methods (a detailed taxonomy can be found in [Figure 2](#S2.F2
    "In 2.2\. Preliminaries of Deep Reinforcement Learning ‣ 2\. Background ‣ A Survey
    of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and
    Future Directions")). The major *d*ifference between the two is whether the agent
    can learn a model of the environment. Model-based methods aim to estimate the
    transition function and reward function, while model-free methods aim to estimate
    the value function or policy from experience. In model-based methods, the agent
    accesses the environment and plans ahead while model-free methods gain sample
    efficiency from using models which are more extensively developed and tested than
    model-based methods in recent literature (Arulkumaran et al., [2017](#bib.bib4)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习可以分为两类：*基于模型* 和 *无模型* 方法（详细分类可以参见[图 2](#S2.F2 "在 2.2. 深度强化学习的预备知识 ‣ 2.
    背景 ‣ 推荐系统中深度强化学习的系统性综述及未来方向")）。这两者之间的主要*d*ifference在于代理是否能够学习环境模型。基于模型的方法旨在估计转换函数和奖励函数，而无模型的方法旨在从经验中估计价值函数或策略。在基于模型的方法中，代理访问环境并进行前瞻性规划，而无模型的方法通过使用比基于模型的方法更广泛开发和测试的模型来提高样本效率（Arulkumaran等，[2017](#bib.bib4)）。
- en: 'Deep reinforcement learning approaches are divided into three streams: *value-based*,
    *policy-based* and *hybrid* methods. In value-based methods, the agent updates
    the value function to learn a policy; policy-based methods learn the policy directly;
    and hybrid methods combine value-based and policy-based methods called *actor-critic*
    methods. Actor-critic contains two different networks where an actor network uses
    a policy-based method and the critic uses a value-based method to evaluate the
    policy learned by the agent.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习方法被分为三种流派：*基于价值*，*基于策略* 和 *混合* 方法。在基于价值的方法中，代理更新价值函数以学习策略；基于策略的方法直接学习策略；而混合方法结合了基于价值和基于策略的方法，称为*actor-critic*方法。Actor-critic包含两个不同的网络，其中一个actor网络使用基于策略的方法，critic网络使用基于价值的方法来评估代理学习的策略。
- en: Table 1. Notations
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 符号
- en: '| Notations | Name | Notations | Name | Notes |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 名称 | 符号 | 名称 | 注释 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $Q(\cdot)$ | Q-Value Function | $s$ | State | users’ preference |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| $Q(\cdot)$ | Q-价值函数 | $s$ | 状态 | 用户的偏好 |'
- en: '| $V(\cdot)$ | Value Function | $a$ | Action | Recommended item(s) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| $V(\cdot)$ | 价值函数 | $a$ | 行动 | 推荐项目 |'
- en: '| $\gamma$ | Discount Factor | $\pi$, $\mu(\cdot)$ | Policy | Recommendation
    policy |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma$ | 折扣因子 | $\pi$, $\mu(\cdot)$ | 策略 | 推荐策略 |'
- en: '| $\mathbb{E}$ | Expected Value | $r(\cdot,\cdot)$ | Reward | users’ click
    behavior |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbb{E}$ | 期望值 | $r(\cdot,\cdot)$ | 奖励 | 用户的点击行为 |'
- en: '| $\theta$ | Model Parameter | $\alpha$ | constant $\in[0,1]$ | - |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $\theta$ | 模型参数 | $\alpha$ | 常数 $\in[0,1]$ | - |'
- en: '| $p(\cdot)$ | Transition Probability | $\tau$ | Sampled Trajectory | A tuple
    $(s_{t},a_{t},s^{\prime}_{t},r_{t})$ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| $p(\cdot)$ | 转移概率 | $\tau$ | 采样轨迹 | 一个元组 $(s_{t},a_{t},s^{\prime}_{t},r_{t})$
    |'
- en: Deep reinforcement learning can be divided into *on-policy* and *off-policy*
    methods. In off-policy methods, the behavior policy $\pi_{b}$ is used for exploration
    while the target policy $\pi$ is used for decision-making. For on-policy methods,
    the behavior policy is the same as the target policy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习可以分为*在策略*和*离策略* 方法。在离策略方法中，行为策略 $\pi_{b}$ 用于探索，而目标策略 $\pi$ 用于决策。对于在策略方法，行为策略与目标策略相同。
- en: 'Q-learning (Watkins and Dayan, [1992](#bib.bib102)) is an off-policy value-based
    learning scheme for finding a greedy target policy:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning（Watkins和Dayan，[1992](#bib.bib102)）是一种离策略的基于价值的学习方案，用于寻找贪婪目标策略：
- en: '| (2) |  | $\displaystyle\pi(s)=\operatorname*{arg\,max}_{a}Q_{\pi}(s,a)$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle\pi(s)=\operatorname*{arg\,max}_{a}Q_{\pi}(s,a)$ |  |'
- en: where $Q_{u}(s,a)$ denotes the $Q$-value and is used in a small discrete action
    space. For a deterministic policy, the $Q$ value can be calculated as follows
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q_{u}(s,a)$ 表示 $Q$-值，并用于小型离散动作空间。对于确定性策略，$Q$ 值可以计算如下
- en: '| (3) |  | $\displaystyle Q(s_{t},a_{t})=\mathbb{E}_{\tau\sim\pi}[r(s_{t},a_{t})+\gamma
    Q(s^{\prime}_{t},a^{\prime}_{t})].$ |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle Q(s_{t},a_{t})=\mathbb{E}_{\tau\sim\pi}[r(s_{t},a_{t})+\gamma
    Q(s^{\prime}_{t},a^{\prime}_{t})].$ |  |'
- en: 'Deep Q learning (DQN) (Mnih et al., [2015](#bib.bib68)) uses deep learning
    to approximate a non-liner Q function parameterized by $\theta_{q}$: $Q_{\theta_{q}}(s,a)$.
    DQN designs a network $Q_{\theta_{q}}$ that is asynchronously updated by minimizing
    the MSE:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习（DQN）（Mnih 等人，[2015](#bib.bib68)）使用深度学习来近似由 $\theta_{q}$ 参数化的非线性 Q 函数：$Q_{\theta_{q}}(s,a)$。DQN
    设计了一个网络 $Q_{\theta_{q}}$，通过最小化均方误差（MSE）来异步更新：
- en: '| (4) |  | $\displaystyle\mathcal{L}(\theta_{q})=\mathbb{E}_{\tau\sim\pi}\Big{[}Q_{\theta_{q}}(s_{t},a_{t})-(r(s_{t},a_{t})+\gamma
    Q_{\theta_{q}}(s^{\prime}_{t},a^{\prime}_{t}))\Big{]}^{2}$ |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\mathcal{L}(\theta_{q})=\mathbb{E}_{\tau\sim\pi}\Big{[}Q_{\theta_{q}}(s_{t},a_{t})-(r(s_{t},a_{t})+\gamma
    Q_{\theta_{q}}(s^{\prime}_{t},a^{\prime}_{t}))\Big{]}^{2}$ |  |'
- en: where $\tau$ is the sampled trajectory containing $(s,a,s^{\prime},r(s,a))$.
    In particular, $s^{\prime}_{t}$ and $a^{\prime}_{t}$ come from the behavior policy
    $\pi_{b}$ while $s,a$ comes from the target policy $\pi$. It is worth mentioning
    that the value function $V_{\pi}(s)$ represents the expected return. $V_{\pi}(s)$
    is used to evaluate the goodness of the state while $Q_{\pi}(s_{t},a_{t})$ is
    used to evaluate the action. $V_{\pi}(s)$ can be defined as
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau$ 是包含 $(s,a,s^{\prime},r(s,a))$ 的采样轨迹。特别地，$s^{\prime}_{t}$ 和 $a^{\prime}_{t}$
    来自行为策略 $\pi_{b}$，而 $s,a$ 来自目标策略 $\pi$。值得一提的是，值函数 $V_{\pi}(s)$ 代表期望回报。$V_{\pi}(s)$
    用于评估状态的好坏，而 $Q_{\pi}(s_{t},a_{t})$ 用于评估动作。$V_{\pi}(s)$ 可以定义为
- en: '| (5) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}_{\tau\sim\pi}\bigg{[}\sum_{t=0}^{T}\gamma^{t}r(s,a)&#124;s_{0}=s\bigg{]}.$
    |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}_{\tau\sim\pi}\bigg{[}\sum_{t=0}^{T}\gamma^{t}r(s,a)&#124;s_{0}=s\bigg{]}.$
    |  |'
- en: '$V_{\pi}(\cdot)$ and $Q_{\pi}(\cdot)$ have the following relationship:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: $V_{\pi}(\cdot)$ 和 $Q_{\pi}(\cdot)$ 具有以下关系：
- en: '| (6) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}_{a\sim\pi}[Q_{\pi}(s,a)].$
    |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}_{a\sim\pi}[Q_{\pi}(s,a)].$
    |  |'
- en: The value function is updated using the following rule with the Temporal Difference
    (TD) method,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数使用以下规则与时序差分（TD）方法进行更新，
- en: '| (7) |  | $\displaystyle V_{\pi}(s_{t})\leftarrow V_{\pi}(s_{t})+\alpha[\underbrace{r(s^{\prime}_{t},a^{\prime}_{t})+\gamma
    V_{\pi}(s^{\prime}_{t})-V_{\pi}(s_{t})}_{\text{TD-error}}]$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\displaystyle V_{\pi}(s_{t})\leftarrow V_{\pi}(s_{t})+\alpha[\underbrace{r(s^{\prime}_{t},a^{\prime}_{t})+\gamma
    V_{\pi}(s^{\prime}_{t})-V_{\pi}(s_{t})}_{\text{TD-error}}]$ |  |'
- en: where $\alpha$ is a constant.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是一个常数。
- en: 'Policy gradient (Williams, [1992](#bib.bib103)) is an on-policy policy-based
    method which can handle high-dimensional or continuous actions which cannot be
    easily handled by Q-learning. Policy gradient aims to find the parameter $\theta$
    of $\pi_{\theta}$ to maximize the accumulated reward. To this end, it maximizes
    the expected return from the start state:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度（Williams, [1992](#bib.bib103)）是一种基于策略的方法，它可以处理高维或连续的动作，这些动作不能被 Q 学习轻松处理。策略梯度的最终目标是找到
    $\pi_{\theta}$ 的参数 $\theta$ 以最大化累计奖励。为此，它最大化从起始状态的期望回报：
- en: '| (8) |  | $\displaystyle J(\pi_{\theta})=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)]=\int\pi_{\theta}(\tau)r(\tau)d\tau$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\displaystyle J(\pi_{\theta})=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)]=\int\pi_{\theta}(\tau)r(\tau)d\tau$
    |  |'
- en: 'where $\pi_{\theta}(\tau)$ is the probability of the occurrence of $\tau$.
    Policy gradient learns the parameter $\theta$ by the gradient $\nabla_{\theta}J(\pi_{\theta})$
    as defined below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi_{\theta}(\tau)$ 是 $\tau$ 发生的概率。策略梯度通过下述梯度 $\nabla_{\theta}J(\pi_{\theta})$
    来学习参数 $\theta$：
- en: '|  | $\displaystyle\nabla_{\theta}J(\pi_{\theta})=\int\pi_{\theta}(\tau)r(\tau)d\tau$
    | $\displaystyle=\int\pi_{\theta}(\tau)\nabla_{\theta}\log\pi_{\theta}(\tau)r(\tau)d\tau$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{\theta}J(\pi_{\theta})=\int\pi_{\theta}(\tau)r(\tau)d\tau$
    | $\displaystyle=\int\pi_{\theta}(\tau)\nabla_{\theta}\log\pi_{\theta}(\tau)r(\tau)d\tau$
    |  |'
- en: '| (9) |  |  | $\displaystyle=\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}[\sum_{t=1}^{T}r(s_{t},a_{t})\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})].$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  |  | $\displaystyle=\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}[\sum_{t=1}^{T}r(s_{t},a_{t})\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})].$
    |  |'
- en: The above derivations contain the following substitution,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述推导包含以下替代，
- en: '| (10) |  | $\displaystyle\pi_{\theta}(\tau)=p(s_{1})\prod_{t=1}^{T}\pi_{\theta}(s_{t},a_{t})p(s_{t+1}&#124;s_{t},a_{t})$
    |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle\pi_{\theta}(\tau)=p(s_{1})\prod_{t=1}^{T}\pi_{\theta}(s_{t},a_{t})p(s_{t+1}&#124;s_{t},a_{t})$
    |  |'
- en: where $p(\cdot)$ are independent from the policy parameter $\theta$, which is
    omitted during the derivation. Monte-Carlo sampling has been used by previous
    policy gradient algorithm (e.g,. REINFORCE) for $\tau\sim d_{\pi_{\theta}}$.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p(\cdot)$ 与策略参数 $\theta$ 无关，在推导过程中被省略。蒙特卡洛采样已被以前的策略梯度算法（例如 REINFORCE）用于
    $\tau\sim d_{\pi_{\theta}}$。
- en: 'Actor-critic networks combine the advantages from Q-learning and policy gradient.
    They can be either on-policy (Konda and Tsitsiklis, [2000](#bib.bib50)) or off-policy (Degris
    et al., [2012](#bib.bib22)). An actor-critic network consists of two components:
    i) an actor, which optimizes the policy $\pi_{\theta}$ under the guidance of $\nabla_{\theta}J(\pi_{\theta})$;
    and ii) a critic, which evaluates the learned policy $\pi_{\theta}$ by using $Q_{\theta_{q}}(s,a)$.
    The overall gradient is represented as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic 网络结合了 Q 学习和策略梯度的优点。它们可以是在线的（Konda 和 Tsitsiklis， [2000](#bib.bib50)）或离线的（Degris
    等， [2012](#bib.bib22)）。一个 actor-critic 网络由两个组件组成：i) actor，它在 $\nabla_{\theta}J(\pi_{\theta})$
    的指导下优化策略 $\pi_{\theta}$；ii) critic，它通过使用 $Q_{\theta_{q}}(s,a)$ 来评估学习到的策略 $\pi_{\theta}$。整体梯度表示如下：
- en: '| (11) |  | $\displaystyle\mathbb{E}_{s\sim d_{\pi_{\theta}}}[Q_{\theta_{q}}(s,a)\nabla_{\theta}\log\pi_{\theta}(s,a)].$
    |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle\mathbb{E}_{s\sim d_{\pi_{\theta}}}[Q_{\theta_{q}}(s,a)\nabla_{\theta}\log\pi_{\theta}(s,a)].$
    |  |'
- en: 'When dealing with off-policy learning, the value function for $\pi_{\theta}(a|s)$
    can be further determined by deterministic policy gradient (DPG) as shown below:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理离线策略学习时，值函数 $\pi_{\theta}(a|s)$ 可以通过确定性策略梯度（DPG）进一步确定，如下所示：
- en: '| (12) |  | $\displaystyle\mathbb{E}_{s\sim d_{\pi_{\theta}}}[\nabla_{a}Q_{\theta_{q}}(s,a)&#124;_{a=\pi_{\theta}(s)}\nabla_{\theta}\pi_{\theta}(s,a)].$
    |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\displaystyle\mathbb{E}_{s\sim d_{\pi_{\theta}}}[\nabla_{a}Q_{\theta_{q}}(s,a)|_{a=\pi_{\theta}(s)}\nabla_{\theta}\pi_{\theta}(s,a)].$
    |  |'
- en: 'While traditional policy gradient calculates the integral for both the state
    space $\mathcal{S}$ and the action space $\mathcal{A}$, DPG only requires computing
    the integral to the state space $\mathcal{S}$. Given a state $s\in\mathcal{S}$,
    there will be only one corresponding action $a\in\mathcal{A}:\mu_{\theta}(s)=a$
    using DPG. Specifically, deep Deterministic Policy Gradients (DDPG) is an algorithm
    that combines techniques from DQN and DPG. DDPG contains four different neural
    networks: Q Network $Q$, policy network, target Q network $Q^{\mathit{tar}}$,
    and target policy network. It uses the target network for both the Q Network $Q$
    and policy network $\mu$ to ensure stability during training. Assume $\theta_{q},\theta_{\pi},\theta_{q^{\prime}}$
    and $\theta_{\pi^{\prime}}$ are parameters of the above networks; then DDPG soft-updates
    the parameters for the target network (Lillicrap et al., [2015](#bib.bib57)):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的策略梯度计算状态空间 $\mathcal{S}$ 和动作空间 $\mathcal{A}$ 的积分，而 DPG 只需计算状态空间 $\mathcal{S}$
    的积分。给定状态 $s\in\mathcal{S}$，使用 DPG 将只有一个相应的动作 $a\in\mathcal{A}:\mu_{\theta}(s)=a$。具体而言，深度确定性策略梯度（DDPG）是一个结合了
    DQN 和 DPG 技术的算法。DDPG 包含四个不同的神经网络：Q 网络 $Q$、策略网络、目标 Q 网络 $Q^{\mathit{tar}}$ 和目标策略网络。它使用
    Q 网络 $Q$ 和策略网络 $\mu$ 的目标网络以确保训练过程中的稳定性。假设 $\theta_{q},\theta_{\pi},\theta_{q^{\prime}}$
    和 $\theta_{\pi^{\prime}}$ 是上述网络的参数；然后 DDPG 对目标网络进行软更新（Lillicrap 等， [2015](#bib.bib57)）：
- en: '| (13) |  | $\displaystyle\text{Actor: }\theta_{\pi^{\prime}}\leftarrow\alpha\theta_{\pi}+(1-\alpha)\theta_{\pi^{\prime}}\text{
    Critic: }\theta_{q^{\prime}}\leftarrow\alpha\theta_{q}+(1-\alpha)\theta_{q^{\prime}}$
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\displaystyle\text{Actor: }\theta_{\pi^{\prime}}\leftarrow\alpha\theta_{\pi}+(1-\alpha)\theta_{\pi^{\prime}}\text{
    Critic: }\theta_{q^{\prime}}\leftarrow\alpha\theta_{q}+(1-\alpha)\theta_{q^{\prime}}$
    |  |'
- en: '2.3\. DRL meets RS: Problem Formulation'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. DRL 遇到 RS：问题表述
- en: 'DRL is normally formulated as a Markov Decision Process (MDP). Given a set
    of users $\mathcal{U}=\{u,u_{1},u_{2},u_{3},...\}$, a set of items $\mathcal{I}=\{i,i_{1},i_{2},i_{3},...\}$,
    the system first recommends item $i$ to user $u$ and then gets feedback $f_{i}^{u}$.
    The system aims to incorporate the feedback to improve future recommendations
    and needs to determine an optimal policy $\pi^{*}$ regarding which item to recommend
    to the user to achieve positive feedback. The MDP modelling of the problem treats
    the user as the environment and the system as the agent. The key components of
    the MDP in DRL-based RS include the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 通常被表述为马尔可夫决策过程（MDP）。给定一组用户 $\mathcal{U}=\{u,u_{1},u_{2},u_{3},...\}$ 和一组项目
    $\mathcal{I}=\{i,i_{1},i_{2},i_{3},...\}$，系统首先向用户 $u$ 推荐项目 $i$，然后获取反馈 $f_{i}^{u}$。系统旨在将反馈纳入考虑，以改善未来的推荐，并需要确定一个关于向用户推荐哪个项目的**最优策略**
    $\pi^{*}$ 以获得积极反馈。MDP 的建模将用户视为环境，将系统视为代理。DRL 基于 RS 的 MDP 的关键组成部分包括：
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'State $\mathcal{S}$: A state $S_{t}\in\mathcal{S}$ is determined by both users’
    information and the recent $l$ items in which the user was interested before time
    $t$.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态 $\mathcal{S}$：状态 $S_{t}\in\mathcal{S}$ 由用户信息和用户在时间 $t$ 之前感兴趣的最近 $l$ 项目决定。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Action $\mathcal{A}$: An action $a_{t}\in\mathcal{A}$ represents users’ dynamic
    preference at time $t$ as predicted by the agent. $\mathcal{A}$ represents the
    whole set of (potentially millions of) candidate items.'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transition Probability $\mathcal{P}$: The transition probability $p(s_{t+1}|s_{t},a_{t})$
    is defined as the probability of state transition from $s_{t}$ to $s_{t+1}$ when
    action $a_{t}$ is executed by the recommendation agent. In a recommender system,
    the transition probability refers to users’ behavior probability. $\mathcal{P}$
    is only used in model-based methods.'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward $\mathcal{R}$: Once the agent chooses a suitable action $a_{t}$ based
    on the current state $S_{t}$ at time $t$, the user will receive the item recommended
    by the agent. Users’ feedback on the recommended item accounts for the reward
    $r(S_{t},a_{t})$. The feedback is used to improve the policy $\pi$ learned by
    the recommendation agent.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discount Factor $\gamma$: The discount factor $\gamma\in[0,1]$ is used to balance
    between future and immediate rewards—the agent focuses only on the immediate reward
    when $\gamma=0$ and takes into account all the (immediate and future) rewards
    otherwise.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The DRL-based recommendation problem can be defined by using MDP as follows.
    Given the historical MDP, i.e., $(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$,
    the goal is to find a set of recommendation polices ($\{\pi\}:\mathcal{S}\to\mathcal{A}$)
    that maximizes the cumulative reward during interaction with users.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Problem Formulation 0.
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given an environment that contains all items $\mathcal{I}$, when user $u\in\mathcal{U}$
    interacts with the system, an initial state $s$ is sampled from the environment
    which contains a list of candidate items and users’ historical data. The DRL agent
    needs to work out a recommendation policy $\pi$ based on the state $s$ and produces
    the corresponding recommended item list $a$. The user will provide feedback on
    the list which is normally represented as click or not click. The DRL agent will
    then utilize the feedback to improve the recommendation policy and move to the
    next interaction episode.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8f91855d595097748cc98151f983900.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: (a) Deep learning based recommender systems
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30037123b8d741494b73136d02c92a70.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: (b) Deep reinforcement learning based recommender systems
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3. Difference between deep learning based RS and DRL-based RS. Deep learning
    based RSs may only update the recommendation policy during the training stage.
    They often require re-training, which is computationally inefficient, when users’
    interests change significantly. DRL-based RS will update the recommendation policy
    time over time as new rewards are received.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Deep Reinforcement Learning in Recommender Systems
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DRL-based RS has some unique challenges such as state construction, reward
    estimation and environment simulation. We categorize the existing work of DRL-based
    recommendation into model-based and model-free methods (the taxonomy is shown
    in [Figure 2](#S2.F2 "In 2.2\. Preliminaries of Deep Reinforcement Learning ‣
    2\. Background ‣ A Survey of Deep Reinforcement Learning in Recommender Systems:
    A Systematic Review and Future Directions")).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DRL的推荐系统面临一些独特的挑战，如状态构建、奖励估计和环境模拟。我们将现有的基于DRL的推荐工作分为基于模型的方法和无模型的方法（分类见[图2](#S2.F2
    "在 2.2\. 深度强化学习预备知识 ‣ 2\. 背景 ‣ 深度强化学习在推荐系统中的系统综述与未来方向")）。
- en: 3.1\. Model-based Deep Reinforcement Learning based Methods
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 基于模型的深度强化学习方法
- en: Model-based methods assume an expected reward or action available for the next
    step to help the agent update the policy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法假设下一步的期望奖励或动作可用，以帮助代理更新策略。
- en: Table 2. List of publications in model-based DRL-based RS
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 基于模型的DRL推荐系统的出版物列表
- en: '| Method | Work |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 工作 |'
- en: '| --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Value-based | (Zhang et al., [2017](#bib.bib120); Chen et al., [2019c](#bib.bib19);
    Zou et al., [2020](#bib.bib136); Wang et al., [2021](#bib.bib97)) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 基于价值 | (张等人, [2017](#bib.bib120); 陈等人, [2019c](#bib.bib19); 邹等人, [2020](#bib.bib136);
    王等人, [2021](#bib.bib97)) |'
- en: '| Policy-based | (Bai et al., [2019](#bib.bib5); Hong et al., [2020](#bib.bib39))
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 基于策略 | (白等人, [2019](#bib.bib5); 洪等人, [2020](#bib.bib39)) |'
- en: '| Hybrid | (Zhao et al., [2020c](#bib.bib126)) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 混合 | (赵等人, [2020c](#bib.bib126)) |'
- en: Policy-based methods
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于策略的方法
- en: IRecGAN (Bai et al., [2019](#bib.bib5)) is a model-based method that adopts
    generative adversarial training to improve the robustness of policy learning.
    It can reduce the cost of interaction for RS by using offline data instead of
    the simulated environment. IRecGAN employs a generative adversarial network (Goodfellow
    et al., [2014](#bib.bib33)) to generate user data based on the offline dataset.
    It trains a recommendation agent using a policy gradient-based DRL method called
    REINFORCE. The agent aims to learn a policy based on the following gradient,
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: IRecGAN (白等人, [2019](#bib.bib5)) 是一种基于模型的方法，采用生成对抗训练来提高策略学习的鲁棒性。通过使用离线数据代替模拟环境，它可以减少推荐系统的交互成本。IRecGAN利用生成对抗网络
    (Goodfellow 等人, [2014](#bib.bib33)) 来基于离线数据集生成用户数据。它使用一种基于策略梯度的DRL方法称为REINFORCE来训练推荐代理。该代理旨在根据以下梯度学习策略，
- en: '| (14) |  | $\displaystyle\mathbb{E}_{\tau\sim\{g,data\}}\big{[}\sum_{t=0}^{T}\sum_{t^{\prime}=t}^{T}\gamma^{t^{\prime}-t}q_{D}(\tau_{0:t}^{n})r_{t}\nabla_{\theta_{a}}(c_{t}\in\pi_{\theta_{a}}(s_{t})\big{]},q_{D}(\tau_{0:t}^{n})=\frac{1}{N}\sum_{n=1}^{N}D(\tau_{0:T}^{n}),\tau_{0:T}^{n}\in
    MC^{\mathcal{U}}(N)$ |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\displaystyle\mathbb{E}_{\tau\sim\{g,data\}}\big{[}\sum_{t=0}^{T}\sum_{t^{\prime}=t}^{T}\gamma^{t^{\prime}-t}q_{D}(\tau_{0:t}^{n})r_{t}\nabla_{\theta_{a}}(c_{t}\in\pi_{\theta_{a}}(s_{t})\big{]},q_{D}(\tau_{0:t}^{n})=\frac{1}{N}\sum_{n=1}^{N}D(\tau_{0:T}^{n}),\tau_{0:T}^{n}\in
    MC^{\mathcal{U}}(N)$ |  |'
- en: where the $\mathit{MC}^{\mathcal{U}}(N)$ represents the sampled $N$ sequences
    from the interaction between $\mathcal{U}$ and the agent using the Monte-Carlo
    tree search algorithm, $D$ is the discriminator, $T$ is the length of $\tau$,
    $g$ represents the offline data, and data represents the generated data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathit{MC}^{\mathcal{U}}(N)$表示从$\mathcal{U}$与代理之间的交互中采样的$N$个序列，使用蒙特卡罗树搜索算法，$D$是鉴别器，$T$是$\tau$的长度，$g$表示离线数据，数据表示生成的数据。
- en: 'Hong et al. ([2020](#bib.bib39)) propose NRSS for personalized music recommendation.
    NRSS uses wireless sensing data to learn users’ current preferences. NRSS considers
    three different types of feedback: score, option, and wireless sensing data. Because
    multiple factors are considered as the reward, NRSS designs a reward model which
    consists of users’ preference reward $r_{p}$ and a novel transition reward $r_{\textit{trans}}$
    which are parameterized by $\theta_{r_{p}}$ and $\theta_{r_{\textit{trans}}}$.
    The goal for NRSS is to find the optimal parameters $\theta_{r_{p}}$ and $\theta_{r_{\textit{trans}}}$
    by using the Monte-Carlo tree search thus improving recommendation performance.
    However, wireless sensing feedback lacks generalization ability as it is only
    available for certain tasks or scenarios, making it hard to determine dynamic
    user interest.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Hong 等人 ([2020](#bib.bib39)) 提出了 NRSS 用于个性化音乐推荐。NRSS 使用无线传感数据来学习用户的当前偏好。NRSS
    考虑三种不同类型的反馈：分数、选项和无线传感数据。由于多个因素被视为奖励，NRSS 设计了一个奖励模型，包括用户偏好奖励 $r_{p}$ 和一个新的过渡奖励
    $r_{\textit{trans}}$，它们由 $\theta_{r_{p}}$ 和 $\theta_{r_{\textit{trans}}}$ 参数化。NRSS
    的目标是通过使用蒙特卡洛树搜索来找到最佳参数 $\theta_{r_{p}}$ 和 $\theta_{r_{\textit{trans}}}$，从而提高推荐性能。然而，无线传感反馈缺乏泛化能力，因为它仅适用于某些任务或场景，这使得动态用户兴趣的确定变得困难。
- en: Value-based methods
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于价值的方法
- en: Prior to Q-learning, value iteration is a more traditional value-based reinforcement
    learning algorithm that focuses on the iteration of the value function. Gradient
    Value Iteration (GVI) (Zhang et al., [2017](#bib.bib120)) is proposed to improve
    the traditional value iteration algorithm by utilizing the transition probability
    and a multi-agent setting to predict chronological author collaborations. It introduces
    a new parameter named ‘status’ to reflect the amount of knowledge that the agent
    needs to learn from this state. The policy is updated only when the distance between
    the new status and the old status is lower than a pre-defined threshold. However,
    value iteration requires the transition probability, which is hard to obtain in
    most cases. Hence, Q-learning and its variants are widely used in DRL-based RS.
    Cascading DQN (CDQN) with a generative user model (Chen et al., [2019c](#bib.bib19))
    is proposed to deal with the environment with unknown reward and environment dynamics.
    The generative user model adopts GANs to generate a user model based on an offline
    dataset. Different from previous work, it will generate the reward function for
    each user to explain the users’ behavior. The user model can be written as,
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Q-learning 之前，价值迭代是一种更传统的基于价值的强化学习算法，重点在于价值函数的迭代。Gradient Value Iteration
    (GVI) (Zhang 等人，[2017](#bib.bib120)) 被提出以利用转移概率和多智能体设置来改进传统的价值迭代算法，用于预测时间顺序的作者合作。它引入了一个名为‘状态’的新参数，以反映代理需要从这个状态中学习的知识量。只有当新状态与旧状态之间的距离小于预定义阈值时，策略才会更新。然而，价值迭代需要转移概率，这在大多数情况下很难获得。因此，Q-learning
    及其变体在基于 DRL 的 RS 中被广泛使用。Cascading DQN (CDQN) 与生成用户模型 (Chen 等人，[2019c](#bib.bib19))
    被提出以处理具有未知奖励和环境动态的环境。生成用户模型采用 GANs 基于离线数据集生成用户模型。与以往的工作不同，它将为每个用户生成奖励函数以解释用户的行为。用户模型可以写作，
- en: '| (15) |  | $\displaystyle\operatorname*{arg\,max}_{\phi\in\triangle^{k-1}}\mathbb{E}_{\phi}[r(s_{t},a_{t})]-R(\phi)/\eta$
    |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\displaystyle\operatorname*{arg\,max}_{\phi\in\triangle^{k-1}}\mathbb{E}_{\phi}[r(s_{t},a_{t})]-R(\phi)/\eta$
    |  |'
- en: where $\triangle^{k-1}$ is the probability simplex, $R(\phi)$ is the regularization
    term for exploration and $\eta$ is a constant.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\triangle^{k-1}$ 是概率单纯形，$R(\phi)$ 是探索的正则化项，$\eta$ 是一个常数。
- en: Pseudo Dyna-Q (PDQ) (Zou et al., [2020](#bib.bib136)) points out that Monte-Carlo
    tree search may lead to an extremely large action space and an unbounded importance
    weight of training samples. Hence, a world model is proposed to reduce the instability
    of convergence and high computation cost for interacting with users by imitating
    the offline dataset. With the world model, the agent will interact with the learned
    world model instead of the environment to improve the sample efficiency and convergence
    stability. The world model learning process introduced in PDQ can be described
    as finding the parameter $\theta_{M}$,
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 伪Dyna-Q (PDQ) (Zou et al., [2020](#bib.bib136))指出，蒙特卡罗树搜索可能会导致极大的行动空间和训练样本的重要性权重无界。因此，提出了一个世界模型，通过模仿离线数据集来减少与用户交互时的收敛不稳定性和高计算成本。通过世界模型，代理将与学习的世界模型交互，而不是环境，以提高样本效率和收敛稳定性。PDQ中引入的世界模型学习过程可以描述为寻找参数$\theta_{M}$，
- en: '| (16) |  | $\displaystyle\operatorname*{arg\,min}_{\theta_{M}}\mathbb{E}_{\xi\in
    P_{\xi}^{\pi}}[\sum_{t}^{T-1}\gamma^{t}\prod_{j=0}^{t}\frac{\pi(s_{j},a_{j})}{\pi_{b}(s_{j},a_{j})}\Delta_{t}(\theta_{M})]$
    |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\displaystyle\operatorname*{arg\,min}_{\theta_{M}}\mathbb{E}_{\xi\in
    P_{\xi}^{\pi}}[\sum_{t}^{T-1}\gamma^{t}\prod_{j=0}^{t}\frac{\pi(s_{j},a_{j})}{\pi_{b}(s_{j},a_{j})}\Delta_{t}(\theta_{M})]$
    |  |'
- en: where $\xi$ is generated by the logged policy $\pi_{b}$, $\prod_{j=0}^{t}\frac{\pi(s_{j},a_{j})}{\pi_{b}(s_{j},a_{j})}$
    is the ratio used for importance sampling and $\Delta$ is the difference between
    the reward in the world model and real reward. Furthermore, GoalRec (Wang et al.,
    [2021](#bib.bib97)) designs a disentangled universal value function to be integrated
    with the world model to help the agent deal with different recommendation tasks.
    The universal value function is defined as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\xi$由记录的策略$\pi_{b}$生成，$\prod_{j=0}^{t}\frac{\pi(s_{j},a_{j})}{\pi_{b}(s_{j},a_{j})}$是用于重要性采样的比例，$\Delta$是世界模型中的奖励和真实奖励之间的差异。此外，GoalRec
    (Wang et al., [2021](#bib.bib97))设计了一个解耦的通用价值函数，集成到世界模型中，以帮助代理处理不同的推荐任务。通用价值函数定义为
- en: '| (17) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}[\sum_{t=0}^{\infty}r(s_{t},a_{t})\prod_{k=0}^{t}\gamma
    s_{k}&#124;s_{0}=s].$ |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}[\sum_{t=0}^{\infty}r(s_{t},a_{t})\prod_{k=0}^{t}\gamma
    s_{k}&#124;s_{0}=s].$ |  |'
- en: Moreover, GoalRec introduces a new variable goal $g\in G$ used to represent
    users’ future trajectory and measurement $m\in M$. $m$ is an indicator that reflects
    users’ response to the given future trajectory based on historical behaviors.
    Based on that, the optimal action will be selected based on
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GoalRec引入了一个新的变量目标$g\in G$，用于表示用户的未来轨迹和测量$ m\in M $。$m$是一个指标，反映了用户对基于历史行为的给定未来轨迹的响应。基于此，最佳行动将根据
- en: '| (18) |  | $\displaystyle a^{*}=\max_{a}U(M(s,a),g)$ |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\displaystyle a^{*}=\max_{a}U(M(s,a),g)$ |  |'
- en: with a customized liner function $U(\cdot)$.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义的线性函数$U(\cdot)$。
- en: '![Refer to caption](img/e4e670a61100905528daa94226c41b94.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e4e670a61100905528daa94226c41b94.png)'
- en: Figure 4. Left is the general structure of model-free methods. Right is the
    structure for GoalRec which is a model-based method. A sample trajectory is used
    to demonstrate the difference between them (Wang et al., [2021](#bib.bib97)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4。左侧是无模型方法的一般结构。右侧是GoalRec的结构，它是基于模型的方法。一个示例轨迹用来展示它们之间的区别 (Wang et al., [2021](#bib.bib97))。
- en: Hybrid methods
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混合方法
- en: 'Hybrid method can be recognized as a midpoint between value-based and policy
    gradient-based methods. DeepChain (Zhao et al., [2020c](#bib.bib126)) uses the
    multi-agent setting to relieve the sub-optimality problem. The sub-optimality
    problem is caused by the one for all setting that optimizes one policy for all
    users. Hence, DeepChain designs a multi-agent setting that adopts several agents
    to learn consecutive scenarios and jointly optimizes multiple recommendation policies.
    The main training algorithm used is DDPG. To this end, users’ actions can be formulated
    in a model-based form as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法可以被认为是基于价值的方法和基于策略梯度的方法之间的一个中间点。DeepChain (Zhao et al., [2020c](#bib.bib126))利用多代理设置来缓解次优性问题。次优性问题是由于“一个适用于所有”的设置优化了一个策略来服务所有用户。因此，DeepChain设计了一个多代理设置，采用多个代理来学习连续场景，并联合优化多个推荐策略。主要的训练算法是DDPG。为此，用户的行动可以用基于模型的形式表示如下：
- en: '| (19) |  | $\displaystyle\sum_{m,d}[p_{m}^{s}(s_{t},a_{t})\gamma Q_{\theta}(s^{\prime}_{t},\pi_{m}(s^{\prime}_{t}))+p_{m}^{c}(s_{t},a_{t})(r_{t}+\gamma
    Q_{\theta}(s^{\prime}_{t},\pi_{d}(s^{\prime}_{t})))+p_{m}^{l}(s_{t},a_{t})r_{t}]1_{m}$
    |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\displaystyle\sum_{m,d}[p_{m}^{s}(s_{t},a_{t})\gamma Q_{\theta}(s^{\prime}_{t},\pi_{m}(s^{\prime}_{t}))+p_{m}^{c}(s_{t},a_{t})(r_{t}+\gamma
    Q_{\theta}(s^{\prime}_{t},\pi_{d}(s^{\prime}_{t})))+p_{m}^{l}(s_{t},a_{t})r_{t}]1_{m}$
    |  |'
- en: where $m$ represents the number of actor networks, $c,l,s$ represent the three
    different scenarios, $1_{m}$ is used to control the activation of two actors and
    $(m,d)\in\{(1,2),(2,1)\}$.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$m$表示演员网络的数量，$c,l,s$表示三种不同的场景，$1_{m}$用于控制两个演员的激活，$(m,d)\in\{(1,2),(2,1)\}$。
- en: Discussion
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: Model-based methods aim to learn a model or representation to represent the
    whole environment so that the agent can plan ahead and receive better sample efficiency.
    The drawback of such a method is that the ground-truth representation of the environment
    is unavailable in recommendation scenarios as it dynamically changes, leading
    to a biased representation. Moreover, model-based methods use the transition probability
    function $\mathcal{P}$ to estimate the optimal policy. As mentioned, the transition
    probability function is normally equivalent to users’ behavior probability which
    is hard to determine in a recommender system. Hence, existing works (Zhao et al.,
    [2020c](#bib.bib126); Bai et al., [2019](#bib.bib5); Wang et al., [2021](#bib.bib97);
    Zou et al., [2020](#bib.bib136); Chen et al., [2019c](#bib.bib19); Zhang et al.,
    [2017](#bib.bib120)) approximate $\mathcal{P}$ using a neural network or embedding
    it into the world model. Zhao et al. ([2020c](#bib.bib126)) design a probability
    network to estimate $\mathcal{P}$ while (Bai et al., [2019](#bib.bib5); Chen et al.,
    [2019c](#bib.bib19)) uses a GAN to generate user behavior where $\mathcal{P}$
    is embedded in the latent space. Different from them, (Wang et al., [2021](#bib.bib97);
    Zou et al., [2020](#bib.bib136)) relies on the world model to predict users’ next
    behavior and feed it into the policy learning process.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法旨在学习一个模型或表示来表征整个环境，以便代理可以提前规划并获得更好的样本效率。这种方法的缺点是，在推荐场景中，由于环境动态变化，无法获得环境的真实表示，从而导致偏差的表示。此外，基于模型的方法使用转移概率函数$\mathcal{P}$来估计最优策略。如前所述，转移概率函数通常等同于用户的行为概率，而在推荐系统中很难确定。因此，现有的工作（Zhao
    et al., [2020c](#bib.bib126); Bai et al., [2019](#bib.bib5); Wang et al., [2021](#bib.bib97);
    Zou et al., [2020](#bib.bib136); Chen et al., [2019c](#bib.bib19); Zhang et al.,
    [2017](#bib.bib120)）通过神经网络近似$\mathcal{P}$或将其嵌入到世界模型中。Zhao et al. ([2020c](#bib.bib126))设计了一个概率网络来估计$\mathcal{P}$，而（Bai
    et al., [2019](#bib.bib5); Chen et al., [2019c](#bib.bib19)）则使用GAN生成用户行为，其中$\mathcal{P}$嵌入在潜在空间中。与他们不同的是，（Wang
    et al., [2021](#bib.bib97); Zou et al., [2020](#bib.bib136)）依赖于世界模型来预测用户的下一步行为，并将其输入到策略学习过程中。
- en: 'The challenges of model-based DRL are not widely used in RS and can be summarized
    into the following facets:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的DRL面临的挑战在推荐系统中使用不广泛，可以总结为以下几个方面：
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathcal{P}$ is hard to determine in real-world recommender systems.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在现实世界的推荐系统中，$\mathcal{P}$很难确定。
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If approximation is used to estimate $\mathcal{P}$, the overall model complexity
    will substantially increase as it requires approximating two different functions
    $\mathcal{P}$ and the recommendation policy $\pi$ by using a large amount of user
    behavior data.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用近似来估计$\mathcal{P}$，总体模型复杂性将显著增加，因为这需要通过大量的用户行为数据来近似两个不同的函数$\mathcal{P}$和推荐策略$\pi$。
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: World model-based methods require periodic re-training to ensure the model can
    reflect user interests in time which increases the computation cost.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于世界模型的方法需要定期重新训练，以确保模型能够及时反映用户的兴趣，这增加了计算成本。
- en: '![Refer to caption](img/9c8dc8d38fb4c611a56e2f54e6a10b25.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9c8dc8d38fb4c611a56e2f54e6a10b25.png)'
- en: (a) DQN
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (a) DQN
- en: '![Refer to caption](img/137e75095b5bb7dec7ae8e5dde6f602a.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/137e75095b5bb7dec7ae8e5dde6f602a.png)'
- en: (b) DDPG
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (b) DDPG
- en: Figure 5. The typical structure of DQN and DDPG
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. DQN和DDPG的典型结构
- en: 3.2\. Model-free deep reinforcement learning based methods
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于模型无关的深度强化学习方法
- en: 'Compared with model-based methods, model-free methods are relatively well-studied.
    Different from model-based methods, $\mathcal{P}$ is unknown and not required
    in model-free methods. Model-based methods enable the agent to learn from previous
    experiences. In this subsection, we categorize model-free based DRL in RS into
    three parts: value-based, policy-based and hybrid methods.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于模型的方法相比，基于模型的方法较少研究。与基于模型的方法不同，在基于模型的方法中，$\mathcal{P}$ 是未知的且不需要。在这一小节中，我们将基于模型的方法的DRL在推荐系统中分为三个部分：基于值的方法、基于策略的方法和混合方法。
- en: Table 3. List of reviewed publications in Model-free DRL-based RS
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表3. 综述的无模型DRL基础推荐系统文献列表
- en: '| Tasks | Note | Work |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 注释 | 工作 |'
- en: '| --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Value-based | Vanilla DQN and its extensions | (Zheng et al., [2018](#bib.bib131);
    Zhao et al., [2018a](#bib.bib125); Lei et al., [2019](#bib.bib55)) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 基于值的方法 | 原始DQN及其扩展 | (郑等，[2018](#bib.bib131)；赵等，[2018a](#bib.bib125)；雷等，[2019](#bib.bib55))
    |'
- en: '| DQN with state/action space optimization | (Xiao et al., [2020](#bib.bib105);
    Lei and Li, [2019](#bib.bib53); Zou et al., [2019](#bib.bib135); Ie et al., [2019b](#bib.bib43))
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 优化状态/动作空间的DQN | (肖等，[2020](#bib.bib105)；雷和李，[2019](#bib.bib53)；邹等，[2019](#bib.bib135)；艾等，[2019b](#bib.bib43))
    |'
- en: '| DQN with graph/image input | (Lei et al., [2020](#bib.bib54); Gui et al.,
    [2019](#bib.bib34); Oyeleke et al., [2018](#bib.bib72); Zhao et al., [2018b](#bib.bib128);
    Takanobu et al., [2019](#bib.bib89); Gao et al., [2019](#bib.bib29); Zhou et al.,
    [2020](#bib.bib132)) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 图像/图形输入的DQN | (雷等，[2020](#bib.bib54)；桂等，[2019](#bib.bib34)；奥耶莱克等，[2018](#bib.bib72)；赵等，[2018b](#bib.bib128)；高等，[2019](#bib.bib29)；周等，[2020](#bib.bib132))
    |'
- en: '| DQN for joint learning | (Pei et al., [2019](#bib.bib74); Zhao et al., [2020e](#bib.bib130);
    Zhao et al., [2021](#bib.bib124)) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 联合学习的DQN | (裴等，[2019](#bib.bib74)；赵等，[2020e](#bib.bib130)；赵等，[2021](#bib.bib124))
    |'
- en: '| Policy-based | Vanilla REINFORCE | (Pan et al., [2019](#bib.bib73); Wang
    et al., [2018a](#bib.bib100); Chen et al., [2019a](#bib.bib14); Xu et al., [2020](#bib.bib108);
    Ma et al., [2020](#bib.bib65); Chen et al., [2021](#bib.bib15); Montazeralghaem
    et al., [2020](#bib.bib69); Ji et al., [2020](#bib.bib46); Yu et al., [2019](#bib.bib111))
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 基于策略的方法 | 原始REINFORCE | (潘等，[2019](#bib.bib73)；王等，[2018a](#bib.bib100)；陈等，[2019a](#bib.bib14)；徐等，[2020](#bib.bib108)；马等，[2020](#bib.bib65)；陈等，[2021](#bib.bib15)；蒙塔泽拉赫姆等，[2020](#bib.bib69)；季等，[2020](#bib.bib46)；余等，[2019](#bib.bib111))
    |'
- en: '| REINFORCE uses graph structure/input | (Wang et al., [2020a](#bib.bib99);
    Xian et al., [2019](#bib.bib104); Wang et al., [2020b](#bib.bib101); Chen et al.,
    [2019b](#bib.bib12)) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| REINFORCE使用图结构/输入 | (王等，[2020a](#bib.bib99)；萧等，[2019](#bib.bib104)；王等，[2020b](#bib.bib101)；陈等，[2019b](#bib.bib12))
    |'
- en: '| Non-REINFORCE based | (Hu et al., [2018](#bib.bib40); Zhang et al., [2019d](#bib.bib115))
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 非REINFORCE基础 | (胡等，[2018](#bib.bib40)；张等，[2019d](#bib.bib115)) |'
- en: '| Hybrid | Vanilla DDPG | (Zhao et al., [2019b](#bib.bib129), [2018a](#bib.bib125);
    Liu et al., [2020b](#bib.bib60); Wang et al., [2018b](#bib.bib98); Cai et al.,
    [2018](#bib.bib10)) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 混合方法 | 原始DDPG | (赵等，[2019b](#bib.bib129)，[2018a](#bib.bib125)；刘等，[2020b](#bib.bib60)；王等，[2018b](#bib.bib98)；蔡等，[2018](#bib.bib10))
    |'
- en: '| with Knowledge Graph | (Chen et al., [2020b](#bib.bib18); Zhao et al., [2020b](#bib.bib122);
    Feng et al., [2018](#bib.bib25); Zhang et al., [2021](#bib.bib118); He et al.,
    [2020](#bib.bib37), [2020](#bib.bib37); Haarnoja et al., [2018](#bib.bib36); Zhao
    et al., [2020d](#bib.bib121); Xie et al., [2021](#bib.bib106)) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 知识图谱 | (陈等，[2020b](#bib.bib18)；赵等，[2020b](#bib.bib122)；冯等，[2018](#bib.bib25)；张等，[2021](#bib.bib118)；何等，[2020](#bib.bib37)，[2020](#bib.bib37)；哈诺贾等，[2018](#bib.bib36)；赵等，[2020d](#bib.bib121)；谢等，[2021](#bib.bib106))
    |'
- en: Value based methods
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于值的方法
- en: As mentioned, Deep Q-learning and its variants are typical value-based DRL methods
    widely used in DRL-based RS. DRN (Zheng et al., [2018](#bib.bib131)) is the first
    work utilizing Deep Q-Networks (DQN) in RS. It adopts Double DQN (DDQN) (Van Hasselt
    et al., [2016](#bib.bib92)) to build a user profile and designs an activeness
    score to reflect how frequently a user returns after one recommendation plus users’
    action (click or not) as the reward. DRN provides a new approach to integrating
    DRL into RS when dealing with a dynamic environment. The key objective function
    can be found as follows,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度Q学习及其变体是典型的基于值的深度强化学习（DRL）方法，广泛应用于基于DRL的推荐系统（RS）。DRN（郑等，[2018](#bib.bib131)）是首个在推荐系统中利用深度Q网络（DQN）的工作。它采用双重DQN（DDQN）（范·哈塞尔特等，[2016](#bib.bib92)）来构建用户画像，并设计了一个活跃度评分，以反映用户在一次推荐后返回的频率，以及用户的行为（点击或未点击）作为奖励。DRN提供了一种在处理动态环境时将DRL集成到推荐系统中的新方法。关键目标函数如下，
- en: '| (20) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma Q_{\theta_{t}^{\prime}}(s_{t+1},\operatorname*{arg\,max}_{a^{\prime}}Q_{\theta_{t}}(s_{t},a^{\prime}))]$
    |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma Q_{\theta_{t}^{\prime}}(s_{t+1},\operatorname*{arg\,max}_{a^{\prime}}Q_{\theta_{t}}(s_{t},a^{\prime}))]$
    |  |'
- en: 'where $a^{\prime}$ is the action that gives the maximum future reward according
    to $\theta_{t}$, $\theta_{t}$ and $\theta_{t}^{\prime}$ are different parameters
    for two different DQNs. Zhao et al. ([2018a](#bib.bib125)) points out that negative
    feedback will also affect recommendation performance which DRN does not consider.
    Moreover, positive feedback is sparse due to the large number of candidate items
    in RS. Only using positive feedback would lead to convergence problems. Hence,
    DEERS is proposed to consider both positive and negative feedback simultaneously
    by using DQN. Gated Recurrent Units (GRU) are employed to capture users’ preferences
    for both a positive state $s^{+}$ and negative state $s^{-}$ and the final objective
    function can be computed as:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a^{\prime}$ 是根据 $\theta_{t}$ 给出的最大未来奖励的动作，$\theta_{t}$ 和 $\theta_{t}^{\prime}$
    是两个不同 DQN 的不同参数。赵等人（[2018a](#bib.bib125)）指出，负反馈也会影响推荐性能，而 DRN 并未考虑这一点。此外，由于 RS
    中候选项的数量庞大，正反馈是稀疏的。仅使用正反馈会导致收敛问题。因此，DEERS 被提出以同时考虑正反馈和负反馈，并使用 DQN。使用门控递归单元（GRU）来捕捉用户对正状态
    $s^{+}$ 和负状态 $s^{-}$ 的偏好，最终的目标函数可以计算为：
- en: '| (21) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma\max_{a_{t+1}}Q_{\theta_{q}}(s^{+}_{t+1},s^{-}_{t+1},a_{t+1})&#124;s^{+}_{t},s^{-}_{t},a_{t}].$
    |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma\max_{a_{t+1}}Q_{\theta_{q}}(s^{+}_{t+1},s^{-}_{t+1},a_{t+1})\mid
    s^{+}_{t},s^{-}_{t},a_{t}]$ |  |'
- en: Lei et al. ([2019](#bib.bib55)) introduces attention mechanisms into the DQN
    to leverage social influence among users. To be specific, a social impact representation
    $U_{v}$ is introduced into the state representation. Matrix factorization is adopted
    to determine similarity among users and hence present the social influence. Social
    attention is introduced to distill the final state representation. In addition,
    a few studies focus on user profiling to improve recommendation performance (Xiao
    et al., [2020](#bib.bib105); Lei and Li, [2019](#bib.bib53); Zou et al., [2019](#bib.bib135)).
    Lei and Li ([2019](#bib.bib53)) claims that user feedback contains useful information
    in the previous feedback even when the user does not like the recommended items.
    Some existing studies focus on final feedback which ignore the importance from
    earlier steps to later ones. Hence, user-specific DQN (UQDN) is proposed to consider
    multi-step feedback from users. It employs Matrix Factorization to generate user-specific
    latent state spaces. The newly defined objective function with the user-specific
    latent state space can be represented as
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 雷等人（[2019](#bib.bib55)）将注意力机制引入 DQN，以利用用户之间的社会影响。具体而言，社会影响表示 $U_{v}$ 被引入状态表示中。采用矩阵分解来确定用户之间的相似性，从而呈现社会影响。引入社会注意力以提炼最终的状态表示。此外，一些研究关注用户画像以提高推荐性能（肖等人，[2020](#bib.bib105)；雷和李，[2019](#bib.bib53)；邹等人，[2019](#bib.bib135)）。雷和李（[2019](#bib.bib53)）声称，即使用户不喜欢推荐的项目，用户反馈中也包含有用的信息。一些现有研究关注最终反馈，却忽略了早期步骤到后续步骤的重要性。因此，提出了用户特定
    DQN（UQDN）以考虑来自用户的多步骤反馈。它采用矩阵分解生成用户特定的潜在状态空间。使用用户特定潜在状态空间的新定义目标函数可以表示为：
- en: '| (22) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma\max_{a_{t+1}}\overline{Q}_{\theta_{q}}(s_{t+1},a_{t+1})+\overline{\textbf{b}}_{u}-Q_{\theta_{q}}(s_{t+1},a_{t+1})]$
    |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma\max_{a_{t+1}}\overline{Q}_{\theta_{q}}(s_{t+1},a_{t+1})+\overline{\textbf{b}}_{u}-Q_{\theta_{q}}(s_{t+1},a_{t+1})]$
    |  |'
- en: 'where $\overline{\textbf{b}}_{u}$ is the learned user latent representation.
    Zou et al. ([2019](#bib.bib135)) also points out that most studies do not consider
    users’ long-term engagement in the state representation as they focus on the immediate
    reward. FeedRec is proposed that combines both instant feedback and delayed feedback
    into the model to represent the long-term reward and optimize the long-term engagement
    by using DQN. To be specific, time-LSTM is employed to track users’ hierarchical
    behavior over time to represent the delayed feedback which contains three different
    operations: $h_{\mathit{skip}},h_{\mathit{choose}},h_{\mathit{order}}$. The state
    space is the concatenation of those operations and users’ latent representation.
    Differently, Xiao et al. ([2020](#bib.bib105)) focuses on the user privacy issue
    in recommender systems. Deep user profile perturbation (DUPP) is proposed to add
    perturbation into the user profile by using DQN during the recommendation process.
    Specifically, DUPP adds a perturbation vector into users’ clicked items as well
    as the state space, which contains users’ previous behavior.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\overline{\textbf{b}}_{u}$ 是学习得到的用户潜在表示。Zou 等人（[2019](#bib.bib135)）还指出，大多数研究未考虑用户在状态表示中的长期参与，因为它们专注于即时奖励。FeedRec
    被提出，它将即时反馈和延迟反馈结合到模型中，以表示长期奖励，并通过使用 DQN 优化长期参与。具体而言，time-LSTM 被用于跟踪用户的层次行为，以表示包含三种不同操作的延迟反馈：$h_{\mathit{skip}},h_{\mathit{choose}},h_{\mathit{order}}$。状态空间是这些操作和用户潜在表示的串联。不同的是，Xiao
    等人（[2020](#bib.bib105)）关注推荐系统中的用户隐私问题。Deep user profile perturbation (DUPP) 被提出在推荐过程中通过使用
    DQN 向用户配置文件添加扰动。具体来说，DUPP 将扰动向量添加到用户点击的项目以及状态空间中，这些空间包含用户的先前行为。
- en: Distinct from previous studies which focus on optimizing user profiles or state
    spaces, some studies aim to optimize the action space formed by interactions with
    items. In the situation of basket recommendation, the user is suggested multiple
    items as a bundle, which is called a recommendation slate. It leads to combinatorially
    large action spaces making it intractable for DQN based recommendation models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与专注于优化用户配置文件或状态空间的先前研究不同，一些研究旨在优化由与项目互动形成的动作空间。在篮子推荐的情况下，用户被建议多个作为一组的项目，这称为推荐
    slate。它导致组合上很大的动作空间，使得基于 DQN 的推荐模型不可处理。
- en: SlateQ (Ie et al., [2019b](#bib.bib43)) is proposed to decompose slate Q-value
    to estimate a long-term value for individual items, and it is represented as,
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: SlateQ（Ie 等人，[2019b](#bib.bib43)）被提出用于将 slate Q 值分解，以估计单个项目的长期价值，它的表示为，
- en: '| (23) |  | $\displaystyle Q_{\theta_{q}}(s_{t},a_{t})=\sum_{i\in a_{t}}p(i&#124;s_{t},a_{t})\overline{Q}_{\theta_{q}}(s_{t},i)$
    |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $\displaystyle Q_{\theta_{q}}(s_{t},a_{t})=\sum_{i\in a_{t}}p(i|s_{t},a_{t})\overline{Q}_{\theta_{q}}(s_{t},i)$
    |  |'
- en: where $\overline{Q}_{\theta}(s,i)$ is the decomposed Q-value for item $i$. The
    decomposed Q-value will be updated by the following rule which is similar to traditional
    DQN,
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\overline{Q}_{\theta}(s,i)$ 是项 $i$ 的分解 Q 值。分解 Q 值将通过以下规则进行更新，该规则类似于传统的 DQN，
- en: '| (24) |  | $\displaystyle\overline{Q}_{\theta_{q}}(s_{t},i)\leftarrow\alpha\bigg{(}r_{t}+\gamma\sum_{j\in
    a_{t+1}}p(j&#124;s_{t+1},a_{t+1})\overline{Q}_{\theta_{q}}(s_{t+1},j)\bigg{)}+(1-\alpha)\overline{Q}_{\theta_{q}}(s_{t},i).$
    |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $\displaystyle\overline{Q}_{\theta_{q}}(s_{t},i)\leftarrow\alpha\bigg{(}r_{t}+\gamma\sum_{j\in
    a_{t+1}}p(j|s_{t+1},a_{t+1})\overline{Q}_{\theta_{q}}(s_{t+1},j)\bigg{)}+(1-\alpha)\overline{Q}_{\theta_{q}}(s_{t},i).$
    |  |'
- en: Different from other mode-free methods, Slate-Q assumes that the transition
    probability $p(i|s_{t},a_{t})$ is known.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他无模型方法不同，Slate-Q 假设转移概率 $p(i|s_{t},a_{t})$ 是已知的。
- en: 'Vanilla DQN methods may not have sufficient knowledge to handle complex data
    such as images and graphs. Tang and Wang ([2018](#bib.bib90)) firstly models users’
    click behavior as an embedding matrix in the latent space to include the skip
    behaviors of sequence patterns for sequential recommendation. Based on that, Gao
    et al. ([2019](#bib.bib29)) propose DRCGR, which adopts CNN and GAN into DQN to
    help the agent to better understand high-dimensional data, e.g., a matrix. Two
    different convolution kernels are used to capture users’ positive feedback. In
    the meantime, DRCGR uses GANs to learn a negative feedback representation to improve
    robustness. Another typical data format is the graph, which is widely used in
    RS, including knowledge graphs. Lei et al. ([2020](#bib.bib54)) propose GCQN which
    adopts Graph Convolutional Networks (GCN) (Kipf and Welling, [2017](#bib.bib49))
    into the DQN which constructs the state and action space as a graph-aware representation.
    Differently, GCQN introduces the attention aggregator: $\sum_{w\in\mathcal{N}(i)}\alpha_{iu}e_{u}$
    which demonstrates better performance than the mean-aggregator and pooling-aggregator.
    For item $i$, the graph-aware representation can be represented as,'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Vanilla DQN 方法可能没有足够的知识来处理复杂的数据，如图像和图表。Tang 和 Wang ([2018](#bib.bib90)) 首次将用户的点击行为建模为潜在空间中的嵌入矩阵，以包括序列模式的跳过行为，用于序列推荐。基于此，Gao
    等人 ([2019](#bib.bib29)) 提出了 DRCGR，该方法将 CNN 和 GAN 引入 DQN，以帮助代理更好地理解高维数据，例如矩阵。使用了两种不同的卷积核来捕捉用户的积极反馈。同时，DRCGR
    使用 GAN 学习负反馈表示，以提高鲁棒性。另一种典型的数据格式是图，这在推荐系统中广泛使用，包括知识图谱。Lei 等人 ([2020](#bib.bib54))
    提出了 GCQN，该方法将图卷积网络（GCN） (Kipf 和 Welling, [2017](#bib.bib49)) 引入 DQN，构建状态和动作空间作为图感知表示。与不同的是，GCQN
    引入了注意力聚合器：$\sum_{w\in\mathcal{N}(i)}\alpha_{iu}e_{u}$，其性能优于均值聚合器和池化聚合器。对于项目 $i$，图感知表示可以表示为，
- en: '| (25) |  | $\displaystyle\sigma\bigg{(}W_{fc}[e_{i}\oplus\sum_{w\in\mathcal{N}(i)}\alpha_{iu}e_{u}+b_{fc}]\bigg{)}$
    |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $\displaystyle\sigma\bigg{(}W_{fc}[e_{i}\oplus\sum_{w\in\mathcal{N}(i)}\alpha_{iu}e_{u}+b_{fc}]\bigg{)}$
    |  |'
- en: where $W_{fc},b_{fc}$ are the parameters for the fully-connected layer, $e_{u}$
    is the embedding for user $u$ and $\mathcal{N}(i)$ is the set of one-hot neighbours
    of item $i$ in graph $G(i)$. Zhou et al. ([2020](#bib.bib132)) propose KGQR uses
    a similar strategy to transform the information into a knowledge graph which is
    fed into the GCN to generate the state representation. Notably, KGQR presents
    a different state representation generation method. For given node $i$, the neighbourhood
    representation with a $k$-hop neighborhood aggregator can be represented as,
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{fc},b_{fc}$ 是全连接层的参数，$e_{u}$ 是用户 $u$ 的嵌入，$\mathcal{N}(i)$ 是图 $G(i)$ 中项目
    $i$ 的一热邻居集合。Zhou 等人 ([2020](#bib.bib132)) 提出了 KGQR，采用类似的策略将信息转化为知识图谱，并输入 GCN 以生成状态表示。值得注意的是，KGQR
    提出了不同的状态表示生成方法。对于给定节点 $i$，具有 $k$-跳邻域聚合器的邻域表示可以表示为，
- en: '| (26) |  | $\displaystyle e_{i}^{k}=\sigma\bigg{(}W_{k}\frac{1}{&#124;\mathcal{N}(i)&#124;}\sum_{t\in\mathcal{N}(i)}e_{t}^{k-1}+B_{k}e_{i}^{k-1}\bigg{)}$
    |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $\displaystyle e_{i}^{k}=\sigma\bigg{(}W_{k}\frac{1}{\left|\mathcal{N}(i)\right|}\sum_{t\in\mathcal{N}(i)}e_{t}^{k-1}+B_{k}e_{i}^{k-1}\bigg{)}$
    |  |'
- en: where $\mathcal{N}(i)$ is the set of neighboring nodes, $W_{k},B_{k}$ are the
    parameter of the aggregator. Those neighbourhood representations will be fed into
    a GRU and the state representation will be generated. Another application domain
    for using graph data is job recommendation which requires considering multiple
    factors jointly such as salary, job description, job location etc. SRDQN (Sun
    et al., [2021](#bib.bib88)) constructs a probability graph to represent a candidate’s
    skill set and employs a multiple-task DQN structure to process these different
    factors concurrently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{N}(i)$ 是邻接节点的集合，$W_{k},B_{k}$ 是聚合器的参数。这些邻域表示将被输入到 GRU 中，生成状态表示。另一个使用图数据的应用领域是职位推荐，这需要考虑多个因素，如薪水、职位描述、职位地点等。SRDQN
    (Sun 等人, [2021](#bib.bib88)) 构建了一个概率图来表示候选人的技能集，并采用多任务 DQN 结构来同时处理这些不同的因素。
- en: 'There are some studies targeting recommendation and advertising simultaneously
    in e-commerce environments (Pei et al., [2019](#bib.bib74); Zhao et al., [2020e](#bib.bib130);
    Zhao et al., [2021](#bib.bib124)). Pei et al. ([2019](#bib.bib74)) mentions when
    deploying RS into real-world platforms such as e-commerce scenarios, the expectation
    is to improve the profit of the system. A new metric, Gross Merchandise Volume
    (GMV), is proposed to measure the profitability of the RS to provide a new view
    about evaluating RS in advertising. Different from GMV, Zhao et al. ([2020e](#bib.bib130))
    separates recommendation and advertising as two different tasks and proposes the
    Rec/Ads Mixed display (RAM) framework. RAM designs two agents: a recommendation
    agent and an advertising agent, where each agent employs a CDQN to conduct the
    corresponding task. Zhao et al. ([2021](#bib.bib124)) find that advertising and
    recommendation may harm each other and formulate a rec/ads trade-off. Their proposed
    solution, DEARS, contains two RNNs. Two RNNs are employed to capture user preferences
    toward recommendations and ads separately. Based on that, DQN is employed to take
    those two outputs as the input to construct the state and output the advertising.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有些研究在电子商务环境中同时针对推荐和广告（Pei et al., [2019](#bib.bib74); Zhao et al., [2020e](#bib.bib130);
    Zhao et al., [2021](#bib.bib124)）。Pei 等人 ([2019](#bib.bib74)) 提到，当将推荐系统部署到如电子商务场景等现实世界平台时，期望是提高系统的利润。提出了一种新的度量标准，即总商品交易额（GMV），以衡量推荐系统的盈利能力，为广告中评估推荐系统提供了新的视角。与
    GMV 不同，赵等人 ([2020e](#bib.bib130)) 将推荐和广告分为两个不同的任务，并提出了 Rec/Ads Mixed display (RAM)
    框架。RAM 设计了两个代理：一个推荐代理和一个广告代理，其中每个代理都使用 CDQN 来执行相应的任务。赵等人 ([2021](#bib.bib124))
    发现广告和推荐可能会相互影响，并提出了推荐/广告权衡。他们提出的解决方案 DEARS 包含两个 RNN。两个 RNN 被用来分别捕捉用户对推荐和广告的偏好。在此基础上，DQN
    被用来将这两个输出作为输入来构建状态并输出广告。
- en: Policy-based methods
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于策略的方法
- en: Policy-based DRL can be divided into two parts which are Constrained Policy
    Optimization (CPO)  (Achiam et al., [2017](#bib.bib2)) and policy gradient. Zhang
    et al. ([2019d](#bib.bib115)) uses CPO to identify the contradiction between text
    feedback and historical preferences. It provides a solution for using DRL in the
    situation where users’ feedback is entirely different from previous feedback in
    RS. Policy gradient-based methods are the other stream in policy-based DRL methods
    for RS. These methods aims to optimize the policy $\pi$ directly instead of estimating
    the Q-value like DQN. A well-known and widely used policy gradient method in RS
    is REINFORCE which uses the following rule for policy $\pi_{\theta_{\pi}}$,
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的 DRL 可以分为两个部分，即约束策略优化（CPO） (Achiam et al., [2017](#bib.bib2)) 和策略梯度。张等人
    ([2019d](#bib.bib115)) 使用 CPO 识别文本反馈与历史偏好之间的矛盾。这为在用户反馈与先前反馈完全不同的推荐系统（RS）中使用 DRL
    提供了解决方案。基于策略梯度的方法是基于策略的 DRL 方法中的另一类方法。这些方法旨在直接优化策略 $\pi$，而不是像 DQN 那样估计 Q 值。在推荐系统中，一个著名且广泛使用的策略梯度方法是
    REINFORCE，它对策略 $\pi_{\theta_{\pi}}$ 使用以下规则，
- en: '| (27) |  | $\displaystyle\theta\leftarrow\theta+\alpha\mathbb{E}_{\tau\sim
    d_{\pi_{\theta_{\pi}}}}\bigg{[}\sum_{t=1}^{T}r(s_{t}^{i},a_{t}^{i})\sum_{t=1}^{T}\nabla_{\theta_{\pi}}\log\pi_{\theta_{\pi}}(s_{t}^{i},a_{t}^{i})\bigg{]}$
    |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $\displaystyle\theta\leftarrow\theta+\alpha\mathbb{E}_{\tau\sim
    d_{\pi_{\theta_{\pi}}}}\bigg{[}\sum_{t=1}^{T}r(s_{t}^{i},a_{t}^{i})\sum_{t=1}^{T}\nabla_{\theta_{\pi}}\log\pi_{\theta_{\pi}}(s_{t}^{i},a_{t}^{i})\bigg{]}$
    |  |'
- en: where $i$ is sampled trajectories from $\pi_{\theta}(a_{t}|s_{t})$. Pan et al.
    ([2019](#bib.bib73)) propose Policy Gradient for Contextual Recommendation (PGCR),
    which adopts REINFORCE and considers contextual information. PGCR assumes that
    the policy follows the multinoulli distribution, in which case the transition
    probability can be estimated easily through sampling from previously seen context.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $i$ 是从 $\pi_{\theta}(a_{t}|s_{t})$ 中采样的轨迹。潘等人 ([2019](#bib.bib73)) 提出了用于情境推荐的策略梯度（PGCR），它采用
    REINFORCE 并考虑了情境信息。PGCR 假设策略遵循多项分布，在这种情况下，可以通过从之前见过的情境中采样来轻松估计转移概率。
- en: Wang et al. ([2018a](#bib.bib100)) incorporate CNNs and attention mechanisms
    in REINFORCE for explainable recommendation. Specifically, this work designs a
    coupled agent structure where one agent generates the explanation and the other
    makes recommendations based on the generated explanation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人 ([2018a](#bib.bib100)) 将 CNN 和注意力机制结合到 REINFORCE 中，以实现可解释推荐。具体来说，这项工作设计了一个耦合代理结构，其中一个代理生成解释，而另一个代理基于生成的解释进行推荐。
- en: Chen et al. ([2019a](#bib.bib14)) increases the scalability of REINFORCE to
    ensure it can deal with the extremely large action space under recommendation
    scenarios. To be specific, it introduces a policy correction gradient estimator
    into REINFORCE to reduce the variance of each gradient by doing importance sampling.
    The new update rule becomes
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人 ([2019a](#bib.bib14)) 提高了 REINFORCE 的可扩展性，以确保它能够处理推荐场景下的极大动作空间。具体而言，它在
    REINFORCE 中引入了策略修正梯度估计器，通过进行重要性采样来减少每个梯度的方差。新的更新规则变为
- en: '| (28) |  | $\displaystyle\theta_{\pi}\leftarrow\theta_{\pi}+\alpha\sum_{\tau\sim\beta}\bigg{[}\sum_{t=1}^{T}\frac{\pi_{\theta_{\pi}}(s_{t},a_{t})}{\pi_{\beta}(s_{t},a_{t})}r(s_{t}^{i},a_{t}^{i})\sum_{t=1}^{T}\nabla_{\theta_{\pi}}\log\pi_{\theta_{\pi}}(s_{t}^{i},a_{t}^{i})\bigg{]}$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $\displaystyle\theta_{\pi}\leftarrow\theta_{\pi}+\alpha\sum_{\tau\sim\beta}\bigg{[}\sum_{t=1}^{T}\frac{\pi_{\theta_{\pi}}(s_{t},a_{t})}{\pi_{\beta}(s_{t},a_{t})}r(s_{t}^{i},a_{t}^{i})\sum_{t=1}^{T}\nabla_{\theta_{\pi}}\log\pi_{\theta_{\pi}}(s_{t}^{i},a_{t}^{i})\bigg{]}$
    |  |'
- en: where $\pi_{\beta}$ is the behavior policy trained by state-action pairs without
    the long-term reward and $\pi_{\theta}$ is trained based on the long-term reward
    only. It is worth mentioning that the vanilla REINFORCE algorithm is on-policy,
    and importance sampling will make REINFORCE behave like an off-policy method with
    the following gradient format,
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi_{\beta}$ 是基于状态-动作对的行为策略，不考虑长期奖励，而 $\pi_{\theta}$ 是仅基于长期奖励训练的。值得一提的是，普通
    REINFORCE 算法是在线策略，而重要性采样会使 REINFORCE 行为类似于离线策略，具有以下梯度格式，
- en: '| (29) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}\bigg{[}\prod_{t^{\prime}=1}^{t}\frac{\pi_{\theta}(s_{t},a_{t})}{\pi_{\theta_{s}}(s_{t^{\prime}},a_{t^{\prime}})}\sum_{t^{\prime}=t}^{T}r(s_{t},a_{t})\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})\bigg{]}$
    |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}\bigg{[}\prod_{t^{\prime}=1}^{t}\frac{\pi_{\theta}(s_{t},a_{t})}{\pi_{\theta_{s}}(s_{t^{\prime}},a_{t^{\prime}})}\sum_{t^{\prime}=t}^{T}r(s_{t},a_{t})\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})\bigg{]}$
    |  |'
- en: where $\pi_{\theta_{s}}$ is the sample policy parameter. Xu et al. ([2020](#bib.bib108))
    also finds that the REINFORCE method suffers from a high variance gradient problem
    and Pairwise Policy Gradient (PPG) is proposed. Different from policy correction,
    PPG uses Monte Carlo sampling to sample two different actions $a,b$ and compare
    the gradient to update $\theta$,
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi_{\theta_{s}}$ 是样本策略参数。Xu 等人 ([2020](#bib.bib108)) 还发现 REINFORCE 方法存在高方差梯度问题，并提出了成对策略梯度（PPG）。不同于策略修正，PPG
    使用蒙特卡洛采样来采样两个不同的动作 $a,b$ 并比较梯度以更新 $\theta$，
- en: '| (30) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta_{\pi}}}}\bigg{(}\sum_{a}\sum_{b}(r(s,a)-r(s,b))\sum_{t=1}^{T}(\nabla_{\theta_{\pi}}\log_{\pi_{\theta_{\pi}}}(s_{t},a_{t})-\nabla_{\theta_{\pi}}\log_{\pi_{\theta_{\pi}}}(s_{t},b_{t}))\bigg{)}.$
    |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta_{\pi}}}}\bigg{(}\sum_{a}\sum_{b}(r(s,a)-r(s,b))\sum_{t=1}^{T}(\nabla_{\theta_{\pi}}\log_{\pi_{\theta_{\pi}}}(s_{t},a_{t})-\nabla_{\theta_{\pi}}\log_{\pi_{\theta_{\pi}}}(s_{t},b_{t}))\bigg{)}.$
    |  |'
- en: Ma et al. ([2020](#bib.bib65)) extends the policy correction gradient estimator
    into a two-stage setting which are $p(s_{t},a^{p})$ and $q(s_{t},a|a^{p})$ and
    the policy can be written as
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Ma 等人 ([2020](#bib.bib65)) 将策略修正梯度估计器扩展到两阶段设置，其中 $p(s_{t},a^{p})$ 和 $q(s_{t},a|a^{p})$，策略可以表示为
- en: '| (31) |  | $\displaystyle\sum_{a^{p}}p(s_{t},a^{p})q(s_{t},a&#124;a^{p}).$
    |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $\displaystyle\sum_{a^{p}}p(s_{t},a^{p})q(s_{t},a&#124;a^{p}).$
    |  |'
- en: In addition, weight capping and self-normalized importance sampling are used
    to further reduce the variance. Moreover, a large state space and action space
    will cause sample inefficiency problems as REINFORCE relies on the current sampled
    trajectories $\tau$. Chen et al. ([2021](#bib.bib15)) finds that the auxiliary
    loss can help improve the sample efficiency (Jaderberg et al., [2016](#bib.bib45);
    Sermanet et al., [2018](#bib.bib82)). Specifically, a linear projection is applied
    to the state $s_{t}$, the output is combined with action $a_{t}$ to calculate
    the auxiliary loss and appended into the final overall objective function for
    optimization.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用权重限制和自归一化重要性采样进一步减少方差。此外，大状态空间和动作空间会导致样本效率问题，因为 REINFORCE 依赖于当前采样的轨迹 $\tau$。Chen
    等人 ([2021](#bib.bib15)) 发现辅助损失可以帮助提高样本效率（Jaderberg 等人，[2016](#bib.bib45)；Sermanet
    等人，[2018](#bib.bib82)）。具体而言，将线性投影应用于状态 $s_{t}$，输出与动作 $a_{t}$ 结合以计算辅助损失，并将其附加到最终的整体目标函数中进行优化。
- en: Another prototype of vanilla policy gradient in DRL-based RS is the policy network.
    Montazeralghaem et al. ([2020](#bib.bib69)) designs a policy network to extract
    features and represent the relevant feedback that can help the agent make a decision.
    Similar to DQN, this work uses a neural network to approximate the Q-value and
    the policy directly without theoretical analysis. Ji et al. ([2020](#bib.bib46))
    extend the policy network by introducing spatio-temporal feature fusion to help
    the agent understand complex features. Specifically, it considers both the current
    number and the future number of vacant taxis on the route to recommend routes
    for taxis. Yu et al. ([2019](#bib.bib111)) introduces multi-modal data as new
    features to conduct vision-language recommendation by using historical data to
    train REINFORCE. ResNet and attention are used to encode vision and text information,
    respectively. Moreover, two rewards are introduced with a customized ratio $\lambda$
    to balance vision and text information.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种基于DRL的推荐系统的普通策略梯度原型是策略网络。Montazeralghaem 等人 ([2020](#bib.bib69)) 设计了一个策略网络，以提取特征并表示相关反馈，帮助代理做出决策。与DQN类似，该工作使用神经网络直接近似Q值和策略，而没有进行理论分析。Ji
    等人 ([2020](#bib.bib46)) 通过引入时空特征融合来扩展策略网络，以帮助代理理解复杂特征。具体而言，它同时考虑了当前和未来路线上的空闲出租车数量，以推荐出租车路线。Yu
    等人 ([2019](#bib.bib111)) 引入了多模态数据作为新特征，通过使用历史数据训练REINFORCE来进行视觉-语言推荐。ResNet 和注意力机制分别用于编码视觉和文本信息。此外，引入了两个奖励，并使用自定义比率
    $\lambda$ 来平衡视觉和文本信息。
- en: Knowledge Graphs (KG) are widely used in RS to enrich side information, provide
    explainability and improve recommendation performance. Similar to DQN, vanilla
    REINFORCE cannot properly handle graph-like data. Wang et al. ([2020a](#bib.bib99))
    propose a method named Knowledge-guided Reinforcement Learning (KERL), which integrates
    knowledge graphs into the REINFORCE algorithm. To be specific, KERL adopts TransE (Bordes
    et al., [2013](#bib.bib7)) to transfer the knowledge graph into a graph embedding
    and utilizes a multilayer perceptron (MLP) to predict future knowledge of user
    preferences. The state representation can be written as
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱（KG）在推荐系统（RS）中被广泛应用，以丰富辅助信息、提供可解释性并提高推荐性能。与DQN类似，普通的REINFORCE无法很好地处理图状数据。王等人
    ([2020a](#bib.bib99)) 提出了一种名为知识引导强化学习（KERL）的方法，将知识图谱集成到REINFORCE算法中。具体而言，KERL采用TransE（Bordes
    et al., [2013](#bib.bib7)）将知识图谱转化为图嵌入，并利用多层感知器（MLP）来预测用户偏好的未来知识。状态表示可以写作：
- en: '| (32) |  | $\displaystyle h_{t}\oplus\mathit{TransE}(\mathcal{G})\oplus\mathit{MLP}(\mathit{TransE}(\mathcal{G}))$
    |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $\displaystyle h_{t}\oplus\mathit{TransE}(\mathcal{G})\oplus\mathit{MLP}(\mathit{TransE}(\mathcal{G}))$
    |  |'
- en: where $h_{t}$ is the hidden representation from the GRU for sequential behavior
    and $\mathcal{G}$ is the knowledge graph.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{t}$ 是来自GRU的隐藏表示，用于序列行为，而 $\mathcal{G}$ 是知识图谱。
- en: Different from KERL, Xian et al. ([2019](#bib.bib104)) propose Policy-Guided
    Path Reasoning (PGPR), which formulates the whole environment as a knowledge graph.
    The agent is trained to find the policy to find good items conditioned on the
    starting user in the KG by using REINFORCE. PGPR uses the tuple $(u,e_{t},h_{t})$
    to represent the state instead of the graph embedding where $e_{t}$ is the entity
    the agent has reached at $t$ for user $u$ and $h_{t}$ is the previous action before
    $t$. The action in PGPR is defined as the prediction of all outgoing edges for
    $e_{t}$ based on $h_{t}$.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与KERL不同，Xian 等人 ([2019](#bib.bib104)) 提出了政策引导路径推理（PGPR），将整个环境建模为知识图谱。代理通过使用REINFORCE被训练以找到在KG中基于起始用户的良好项目。PGPR使用元组
    $(u,e_{t},h_{t})$ 来表示状态，而不是图嵌入，其中 $e_{t}$ 是代理在时间 $t$ 对用户 $u$ 到达的实体，$h_{t}$ 是在时间
    $t$ 之前的先前动作。PGPR中的动作被定义为基于 $h_{t}$ 对 $e_{t}$ 所有出边的预测。
- en: Wang et al. ([2020b](#bib.bib101)) propose a knowledge graph policy network
    (KGPolicy) which puts the KG into the policy network and adopts REINFORCE to optimize
    it. In addition, KGPolicy uses negative sampling instead of stochastic sampling
    to overcome the false negative issue—sampled items behave differently during training
    and inference. Similar to GCQN, attention is also employed to establish the representation
    for its neighbors.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人 ([2020b](#bib.bib101)) 提出了一个知识图谱策略网络（KGPolicy），将知识图谱（KG）嵌入到策略网络中，并采用REINFORCE进行优化。此外，KGPolicy使用负采样而非随机采样，以克服假负样本问题——采样项在训练和推断过程中的行为有所不同。与GCQN类似，KGPolicy也使用注意力机制来建立其邻居的表示。
- en: Due to the on-policy nature of REINFORCE, it is difficult to apply it to large-scale
    RS as the convergence speed will be a key issue. To relieve this, Chen et al.
    ([2019b](#bib.bib12)) propose TPGR, which designs a tree-structured policy gradient
    method to handle the large discrete action space hierarchically. TPGR uses balanced
    hierarchical clustering to construct a clustering tree. Specifically, it splits
    a large-scale data into several levels and maintains multiple policy networks
    for each level to conduct the recommendation. The results are integrated at the
    final stage.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于REINFORCE的策略性质，它在大规模推荐系统中应用困难，因为收敛速度将是一个关键问题。为了解决这个问题，Chen等人 ([2019b](#bib.bib12))
    提出了TPGR，该方法设计了一种树状结构的策略梯度方法，以分层处理大规模离散动作空间。TPGR使用平衡的层次聚类来构建聚类树。具体来说，它将大规模数据分割成多个层级，并为每个层级维护多个策略网络来进行推荐。结果在最终阶段进行整合。
- en: As mentioned, policy gradient can be further extended to deterministic policy
    gradient (DPG) (Silver et al., [2014](#bib.bib87)). Hu et al. ([2018](#bib.bib40))
    propose Deterministic Policy Gradient with Full Backup Estimation (DPG-FBE) to
    complete a sub-task of recommendation. DPG-FBE considers a search session MDP
    (SSMDP) that contains a limited number of samples, where the stochastic policy
    gradient method like REINFORCE cannot work well.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，策略梯度可以进一步扩展到确定性策略梯度（DPG）（Silver等人，[2014](#bib.bib87)）。Hu等人（[2018](#bib.bib40)）提出了具有完全备份估计的确定性策略梯度（DPG-FBE），用于完成推荐的一个子任务。DPG-FBE考虑了一个包含有限样本的搜索会话MDP（SSMDP），在这种情况下，像REINFORCE这样的随机策略梯度方法效果不佳。
- en: Hybrid methods
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混合方法
- en: The most common model-free hybrid method used would be the actor-critic algorithm
    where the critic network uses the DQN and the actor uses the policy gradient.
    The common algorithm used to train actor-critic is DDPG with the following objective
    function,
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的无模型混合方法是演员-评论员算法，其中评论员网络使用DQN，而演员使用策略梯度。用于训练演员-评论员的常见算法是DDPG，其目标函数如下，
- en: '| (33) |  | $\displaystyle\mathbb{E}[r_{t}+\gamma Q_{\theta_{q}^{\prime}}(s_{t+1},\mu_{\theta_{\pi}^{\prime}}(s_{t+1}))-Q_{\theta_{q}}(s_{t},a_{t})]$
    |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $\displaystyle\mathbb{E}[r_{t}+\gamma Q_{\theta_{q}^{\prime}}(s_{t+1},\mu_{\theta_{\pi}^{\prime}}(s_{t+1}))-Q_{\theta_{q}}(s_{t},a_{t})]$
    |  |'
- en: where $\theta_{q},\theta_{q}^{\prime}$ is the parameter for Q-learning at time
    $t,t+1$ while $\theta_{\pi}^{\prime}$ is the parameter for deterministic policy
    gradient at time $t+1$. Zhao et al. ([2019b](#bib.bib129)) propose LIRD, which
    uses the vanilla actor-critic framework to conduct list-wise recommendations.
    In order to demonstrate the effectiveness of LIRD, a pre-trained user simulator
    is used to evaluate the effectiveness of LIRD where the transition probability
    is approximated using the cosine similarity for a given state-action pair $s_{t},a_{t}$.
    Zhao et al. ([2018a](#bib.bib125)) further extend LIRD into page-wise recommendation
    and proposed DeepPage. Similar to other previous work, GRU is employed to process
    the sequential pattern. Moreover, similar to DRCGR, DeepPage formulates the state
    as a page, then CNNs are employed to capture features and fed to the critic network.
    The final state representation is the concatenation of the sequential pattern
    and the page features. Additionally, there are a few studies focusing on different
    scenarios such as top-aware recommendation (Liu et al., [2020b](#bib.bib60)),
    treatment recommendation (Wang et al., [2018b](#bib.bib98)), allocating impressions (Cai
    et al., [2018](#bib.bib10)) etc. Liu et al. ([2020b](#bib.bib60)) introduces a
    supervised learning module (SLC) as the indicator to identify the difference between
    the current policy and historical preferences. SLC will conduct the ranking process
    to ensure the recommendation policy will not be affected by the positional bias
    – the item appearing on top receives more clicks. Similarly, Wang et al. ([2018b](#bib.bib98))
    also integrates the supervised learning paradigm into DRL but in a different way.
    An expert action $\hat{a}_{t}$ is provided when the critic evaluates the policy
    and the update rule is slightly different than normal DQN,
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta_{q},\theta_{q}^{\prime}$ 是时间 $t,t+1$ 的 Q-learning 参数，而 $\theta_{\pi}^{\prime}$
    是时间 $t+1$ 的确定性策略梯度参数。赵等人 ([2019b](#bib.bib129)) 提出了 LIRD，它使用基础的 actor-critic 框架进行列表推荐。为了展示
    LIRD 的有效性，使用预训练的用户模拟器来评估 LIRD 的有效性，其中过渡概率通过给定状态-动作对 $s_{t},a_{t}$ 的余弦相似度进行近似。赵等人
    ([2018a](#bib.bib125)) 进一步将 LIRD 扩展到页面级推荐，并提出了 DeepPage。类似于其他先前的工作，GRU 被用来处理序列模式。此外，类似于
    DRCGR，DeepPage 将状态公式化为页面，然后使用 CNN 来捕捉特征，并将其传递给评论网络。最终的状态表示是序列模式和页面特征的连接。此外，还有一些研究关注于不同的场景，例如顶端感知推荐
    (Liu et al., [2020b](#bib.bib60))、治疗推荐 (Wang et al., [2018b](#bib.bib98))、印象分配
    (Cai et al., [2018](#bib.bib10)) 等。Liu et al. ([2020b](#bib.bib60)) 介绍了一个监督学习模块
    (SLC) 作为指标来识别当前策略与历史偏好之间的差异。SLC 将进行排名过程，以确保推荐策略不会受到位置偏差的影响——在顶部出现的项目会获得更多点击。同样，Wang
    et al. ([2018b](#bib.bib98)) 也将监督学习范式整合到 DRL 中，但方式不同。当评论员评估策略时，会提供一个专家行动 $\hat{a}_{t}$，更新规则与正常的
    DQN 略有不同。
- en: '| (34) |  | $\displaystyle\theta_{q}\leftarrow\theta_{q}+\alpha\sum_{t}[Q_{\theta_{q}}(s_{t},\hat{a}_{t})-r_{t}-\gamma
    Q_{\theta_{q^{\prime}}}(s_{t},\mu_{\theta_{\pi^{\prime}}}(s_{t}))]\nabla_{\theta_{q}}Q_{\theta_{q}}(s_{t},a_{t}).$
    |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $\displaystyle\theta_{q}\leftarrow\theta_{q}+\alpha\sum_{t}[Q_{\theta_{q}}(s_{t},\hat{a}_{t})-r_{t}-\gamma
    Q_{\theta_{q^{\prime}}}(s_{t},\mu_{\theta_{\pi^{\prime}}}(s_{t}))]\nabla_{\theta_{q}}Q_{\theta_{q}}(s_{t},a_{t}).$
    |  |'
- en: However, such a method is not universal as the acquisition of expert action
    is difficult and depends on the application domain.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法并不是普遍适用的，因为获取专家行动是困难的，并且依赖于应用领域。
- en: Similar to policy gradient and DQN, Knowledge Graphs (KG) are also used in actor-critic-based
    methods. Chen et al. ([2020b](#bib.bib18)) propose KGRL to incorporate the substantial
    information of knowledge graphs to help the critic to better evaluate the generated
    policy. A knowledge graph is embedded into the critic network. Different from
    previous studies which use the KG as the environment or state representation,
    KGRL uses KG as a component in the critic, which can guide the actor to find a
    better recommendation policy by measuring the proximity from the optimal path.
    Specifically, a graph convolutional network is used to weight the graph and Dijkstra’s
    algorithm is employed to find the optimal path for finally identifying the corresponding
    Q-value. Zhao et al. ([2020b](#bib.bib122)) claim that human’s demonstration could
    improve path searching and propose ADAC. ADAC also searches for the optimal path
    in the KG but further adopts adversarial imitation learning and uses expert paths
    to facilitate the search process. Feng et al. ([2018](#bib.bib25)) propose MA-RDPG,
    which extends the standard actor-critic algorithm to deal with multiple scenarios
    by utilizing a multi-actor reinforcement learning setting. Specifically, two different
    actor-networks are initialized while only one critic network will make the final
    decision. Those two actor networks can communicate with each other to share information
    and approximate the global state. Zhang et al. ([2021](#bib.bib118)) find that
    there are multiple factors can affect the selection of electric charging station.
    Hence, it uses a similar idea to recommend the electric vehicle charging station
    by considering current supply, future supply, and future demand. He et al. ([2020](#bib.bib37))
    figure out that the communication mechanism in MA-RDPG will harm actors as they
    are dealing with independent modules, and there is no intersection. Hence, He
    et al. ([2020](#bib.bib37)) extend MA-RDPG into multi-agent settings which contain
    multiple pairs of actors and critics and remove the communication mechanism to
    ensure independence.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于策略梯度和DQN，知识图谱（KG）也被用于基于演员-评论家的方法中。陈等人（[2020b](#bib.bib18)）提出了KGRL，将知识图谱的丰富信息融入评论家网络，以帮助评论家更好地评估生成的策略。一个知识图谱被嵌入到评论家网络中。与以前将KG用作环境或状态表示的研究不同，KGRL将KG作为评论家中的一个组件，这可以通过测量与最优路径的接近程度来指导演员找到更好的推荐策略。具体来说，图卷积网络用于加权图，Dijkstra算法用于找到最优路径，以最终确定相应的Q值。赵等人（[2020b](#bib.bib122)）认为人的示范可以改善路径搜索，并提出了ADAC。ADAC也在KG中搜索最优路径，但进一步采用了对抗性模仿学习，并使用专家路径来促进搜索过程。冯等人（[2018](#bib.bib25)）提出了MA-RDPG，该方法通过利用多演员强化学习设置扩展了标准的演员-评论家算法，以处理多个场景。具体而言，初始化两个不同的演员网络，而只有一个评论家网络会做出最终决策。这两个演员网络可以相互通信以共享信息并近似全局状态。张等人（[2021](#bib.bib118)）发现有多个因素可以影响电动充电站的选择。因此，它使用类似的思想，通过考虑当前供应、未来供应和未来需求来推荐电动车充电站。何等人（[2020](#bib.bib37)）发现MA-RDPG中的通信机制会对演员造成伤害，因为他们处理的是独立模块，且没有交集。因此，何等人（[2020](#bib.bib37)）将MA-RDPG扩展到包含多个演员-评论家对的多智能体设置，并移除了通信机制以确保独立性。
- en: 'Different from (Feng et al., [2018](#bib.bib25)), He et al. ([2020](#bib.bib37))
    use ‘soft’ actor-critic (SAC) (Haarnoja et al., [2018](#bib.bib36)), which introduces
    a maximum entropy term $\mathcal{H}(\pi(s_{t},\phi_{t}))$ to actor-critic to improve
    exploration and stability with the stochastic policy $\pi(s_{t},\phi_{t})$. Similar
    to the multi-agent idea, Zhao et al. ([2020d](#bib.bib121)) use a hierarchical
    setting to help the agent learn multiple goals by setting multiple actors and
    critics. In comparison, hierarchical RL uses multiple actor-critic networks for
    the same task. It splits a recommendation task into two sub-tasks: discovering
    long-term behavior and capturing short-term behavior. The final recommendation
    policy is the combination of the optimal policies for the two sub-tasks. Similarly,
    Xie et al. ([2021](#bib.bib106)) use the hierarchical setting for integrated recommendation
    by using different sourced data. The objective is to work out the sub-polices
    for each source hierarchically and form the final recommendation policy afterward.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 与（Feng等，[2018](#bib.bib25)）不同，He等人（[2020](#bib.bib37)）使用“软”演员-评论家（SAC）（Haarnoja等，[2018](#bib.bib36)），该方法将最大熵项$\mathcal{H}(\pi(s_{t},\phi_{t}))$引入演员-评论家，以改进具有随机策略$\pi(s_{t},\phi_{t})$的探索和稳定性。类似于多智能体的想法，Zhao等人（[2020d](#bib.bib121)）使用层次设置，通过设置多个演员和评论家来帮助智能体学习多个目标。相比之下，层次RL使用多个演员-评论家网络来处理同一任务。它将推荐任务分成两个子任务：发现长期行为和捕捉短期行为。最终的推荐策略是这两个子任务最佳策略的组合。同样，Xie等人（[2021](#bib.bib106)）通过使用不同来源的数据来进行综合推荐。其目标是逐层制定每个来源的子策略，然后形成最终的推荐策略。
- en: Discussion
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In RS, model-free methods are generally more flexible than model-based methods
    as they do not require knowing the transition probability. We summarize the advantages
    and disadvantages of the three kinds of methods described under the model-free
    category. DQN is the first DRL method used in RS, which is suitable for small
    discrete action spaces. The problems with DQN in RS are:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在RS中，模型自由的方法通常比基于模型的方法更灵活，因为它们不需要知道过渡概率。我们总结了模型自由类别下三种方法的优缺点。DQN是第一个用于RS的DRL方法，适用于小的离散动作空间。DQN在RS中的问题是：
- en: •
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RS normally contains large and high-dimensional action spaces.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RS通常包含大而高维的动作空间。
- en: •
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The reward function is hard to determine which will lead to inaccurate value
    function approximation.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励函数难以确定，这会导致价值函数近似不准确。
- en: Specifically, the high dimensional action space in context of recommender systems
    is recognized as a major drawback of DQN (Mnih et al., [2015](#bib.bib68); Tavakoli
    et al., [2018](#bib.bib91)). The reason lies in the large number of the candidate
    items. Hence, DQN, as one of the most popular schemes, is not the best choice
    for RS in many situations. Moreover, some unique factors need to be considered
    when designing the reward function for RS such as social inference. It introduces
    extra parameters to the Q-network and hinders the convergence.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在推荐系统的背景下，高维度的动作空间被认为是DQN的主要缺点（Mnih等，[2015](#bib.bib68)；Tavakoli等，[2018](#bib.bib91)）。原因在于候选项的数量庞大。因此，DQN作为最流行的方案之一，在许多情况下并不是RS的最佳选择。此外，设计RS奖励函数时需要考虑一些独特的因素，例如社会推断。它会引入额外的参数到Q网络中，并阻碍收敛。
- en: Policy gradient does not require the reward function to estimate the value function.
    Instead, it estimates the policy directly. However, policy gradient is designed
    for continuous action spaces. More importantly, it will introduce high variance
    in the gradient. Actor-critic algorithms combine the advantages of DQN and policy
    gradient. Nonetheless, actor-critic will map the large discrete action space into
    a small continuous action space to ensure it is differentiable, which may cause
    potential information loss. Actor-critic uses DDPG and thus inherits disadvantages
    from DQN and DPG, including difficulty in determining the reward function and
    poor exploration ability.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度不需要奖励函数来估计价值函数。相反，它直接估计策略。然而，策略梯度是为连续动作空间设计的。更重要的是，它会在梯度中引入高方差。演员-评论家算法结合了DQN和策略梯度的优点。然而，演员-评论家会将大的离散动作空间映射到一个小的连续动作空间，以确保其可微分，这可能导致潜在的信息损失。演员-评论家使用DDPG，因此继承了DQN和DPG的缺点，包括确定奖励函数的困难和探索能力差。
- en: 3.3\. Component Optimization in Deep Reinforcement Learning based RS
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 深度强化学习中基于RS的组件优化
- en: 'There are a few studies that use DRL in RS for goals other than improving recommendation
    performance or proposing new application domains. We split the literature based
    on the following components: environment, state representation, and reward function.
    Exisitng studies usually focus on optimizing one single component in the DRL setting
    (as illustrated in [Figure 1](#S1.F1 "In 1\. Introduction ‣ A Survey of Deep Reinforcement
    Learning in Recommender Systems: A Systematic Review and Future Directions")).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些研究使用深度强化学习（DRL）在推荐系统（RS）中的目标不是提升推荐性能或提出新的应用领域。我们根据以下组件划分文献：环境、状态表示和奖励函数。现有研究通常专注于优化DRL设置中的一个单一组件（如[图
    1](#S1.F1 "在 1. 引言 ‣ 深度强化学习在推荐系统中的调查：系统评审与未来方向")所示）。
- en: Table 4. List of publications reviewed in this section
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4. 本节回顾的文献列表
- en: '| Component | Work |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 工作 |'
- en: '| --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Environment | (Shi et al., [2019a](#bib.bib84); Rohde et al., [2018](#bib.bib76);
    Shi et al., [2019b](#bib.bib85); Ie et al., [2019a](#bib.bib42); Shang et al.,
    [2019](#bib.bib83); Huang et al., [2020](#bib.bib41); Santana et al., [2020](#bib.bib77);
    Zhao et al., [2019a](#bib.bib127)) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | (Shi et al., [2019a](#bib.bib84); Rohde et al., [2018](#bib.bib76);
    Shi et al., [2019b](#bib.bib85); Ie et al., [2019a](#bib.bib42); Shang et al.,
    [2019](#bib.bib83); Huang et al., [2020](#bib.bib41); Santana et al., [2020](#bib.bib77);
    Zhao et al., [2019a](#bib.bib127)) |'
- en: '| State | (Liu et al., [2018](#bib.bib61); Liu et al., [2020a](#bib.bib59);
    Liu et al., [2020c](#bib.bib62)) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | (Liu et al., [2018](#bib.bib61); Liu et al., [2020a](#bib.bib59); Liu
    et al., [2020c](#bib.bib62)) |'
- en: '| Reward | (Chen et al., [2018](#bib.bib16)) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 奖励 | (Chen et al., [2018](#bib.bib16)) |'
- en: 3.3.1\. Environment Simulation and Reconstruction
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 环境仿真与重建
- en: 'Many environments are available for evaluating deep reinforcement learning.
    Two popular ones are OpenAI gym-based environment (Brockman et al., [2016](#bib.bib9))
    and MuJoCo¹¹1http://mujoco.org/. Unfortunately, there is no standardized simulation
    platform or benchmark specific to reinforcement learning based recommender systems.
    Existing work on DRL in RS is usually evaluated through offline datasets or via
    deployment in real applications. The drawback for evaluating offline datasets
    include:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 许多环境可用于评估深度强化学习。其中两个较为流行的环境是基于OpenAI gym的环境（Brockman et al., [2016](#bib.bib9)）和MuJoCo¹¹1http://mujoco.org/。不幸的是，目前没有针对强化学习推荐系统的标准化仿真平台或基准。现有的深度强化学习（DRL）在推荐系统（RS）中的工作通常通过离线数据集或实际应用中的部署进行评估。评估离线数据集的缺点包括：
- en: •
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different studies use different environment construction methods which leads
    to unfair comparison. For instance, some studies use the KG as the environment
    while some studies assume the environment is gym-like or design a simulator for
    specific tasks.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同的研究使用不同的环境构建方法，这导致了不公平的比较。例如，一些研究使用知识图谱（KG）作为环境，而一些研究假设环境类似于gym，或者为特定任务设计了模拟器。
- en: •
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: With offline datasets, users’ dynamic interests, and environment dynamics are
    hard to maintain. Deploying the method into a real application is difficult for
    academic research as it takes time and costs money. Hence, a standardized simulation
    environment is a desirable solution.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用离线数据集时，用户的动态兴趣和环境动态难以维护。将这种方法应用于实际应用中对于学术研究来说是困难的，因为这需要时间和费用。因此，标准化的仿真环境是一个理想的解决方案。
- en: '![Refer to caption](img/6b1804850302f7b678c24a9dbf0ebf66.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6b1804850302f7b678c24a9dbf0ebf66.png)'
- en: Figure 6. Left is the traditional MDP transition; Right is the POMDP which considers
    the environmental confounders such as social influence (Shang et al., [2019](#bib.bib83)).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. 左侧是传统的MDP转移；右侧是考虑环境混淆因素如社会影响的POMDP（Shang et al., [2019](#bib.bib83)）。
- en: 'There are several different studies that provide standardized gym²²2https://gym.openai.com/-based
    simulation platforms for DRL-based RS research in different scenarios. RecSim (Ie
    et al., [2019a](#bib.bib42)) is a configurable platform that supports sequential
    interaction between the system and users. RecSim contains three different tasks:
    interest evolution, interest exploration and long-term satisfaction. RecoGym (Rohde
    et al., [2018](#bib.bib76)) provides an environment for recommendation and advertising.
    In addition, RecoGym also provides simulation of online experiments such as A/B-tests.
    However, RecSim and RecoGym are designed for bandit behavior which means users’
    interests will not change over time. VirtualTB (Shi et al., [2019b](#bib.bib85))
    is proposed to relieve such problems. VirtualTB employs imitation learning to
    learn a user model to interact with the agent. GANs are employed to generate users’
    interests. Similar to VirtualTB, Recsimu (Zhao et al., [2019a](#bib.bib127)) uses
    a GAN to tackle the complex item distribution. In addition, PyRecGym (Shi et al.,
    [2019a](#bib.bib84)) accommodates standard benchmark datasets into a gym-based
    environment. MARS-Gym (Santana et al., [2020](#bib.bib77)) provides a benchmark
    framework for marketplace recommendation. (Huang et al., [2020](#bib.bib41)) suggests
    that existing simulation environments are biased because of biased logged data.
    Two common biases are discussed: popularity bias and positivity bias. To reduce
    the effect from those biases, SOFA introduces an Intermediate Bias Mitigation
    Step for debiasing purposes.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 有多项研究提供了标准化的基于**Gym**²²2https://gym.openai.com/-的模拟平台，用于在不同场景下的基于DRL的推荐系统研究。RecSim (Ie
    et al., [2019a](#bib.bib42))是一个可配置的平台，支持系统与用户之间的顺序交互。RecSim包含三种不同的任务：兴趣演变、兴趣探索和长期满意度。RecoGym (Rohde
    et al., [2018](#bib.bib76))提供了推荐和广告的环境。此外，RecoGym还提供了在线实验的模拟，如A/B测试。然而，RecSim和RecoGym设计用于赌博行为，这意味着用户的兴趣不会随时间变化。VirtualTB (Shi
    et al., [2019b](#bib.bib85))旨在缓解这些问题。VirtualTB采用模仿学习来学习用户模型与代理进行交互。GAN被用于生成用户兴趣。类似于VirtualTB，Recsimu (Zhao
    et al., [2019a](#bib.bib127))使用GAN来解决复杂的项目分布。此外，PyRecGym (Shi et al., [2019a](#bib.bib84))将标准基准数据集整合到基于Gym的环境中。MARS-Gym (Santana
    et al., [2020](#bib.bib77))提供了市场推荐的基准框架。 (Huang et al., [2020](#bib.bib41))指出，现有的模拟环境由于有偏的日志数据而存在偏差。讨论了两种常见的偏差：受欢迎偏差和积极性偏差。为了减少这些偏差的影响，SOFA引入了中间偏差缓解步骤以进行去偏处理。
- en: 'One work discusses environment reconstruction by considering confounders. (Shang
    et al., [2019](#bib.bib83)) claims that users’ interests may be affected by social
    networks which may introduce extra bias to the state and affect the decision-making
    process. A multi-agent setting is introduced to treat the environment as an agent
    which can partially relieve the hidden confounder effect. Specifically, a deconfounded
    environment reconstruction method DEMER is proposed. Different from previously
    mentioned methods, DEMER assumes the environment is partially observed and models
    the whole recommendation task as a Partially Observed MDP (POMDP). Different from
    an MDP, a POMDP contains one more component observation $o\in\mathcal{O}$ and
    the action $a_{t}$ is derived based on the observation $o_{t}$ instead of the
    state $s_{t}$ by $a_{t}=\pi_{a}(o_{t})$. DEMER assumes there is a confounder policy
    $\pi_{h}$ for observation $o_{h}$ which is composed by $a_{t}$ and $o_{t}$: $a_{h}=\pi_{h}(a_{t},o_{t})$.
    Moreover, another observation $o_{b}$ is introduced to observe the transition
    as well. $\pi_{b}$ is the corresponding policy and $a_{b}=\pi_{b}(o_{b})=\pi_{b}(o_{t},a_{t},a_{h})$.
    DEMER uses generative adversarial imitation learning (GAIL) to imitate the policy
    $A,B$. Given trajectory $\{o_{t},o_{h},o_{b}\}$ for different policies $A$ and
    $B$, the objective function is defined as'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一项工作讨论了通过考虑混杂因素进行环境重建。 (Shang et al., [2019](#bib.bib83))声称，用户的兴趣可能会受到社交网络的影响，这可能引入额外的偏差到状态中并影响决策过程。引入了多智能体设置，将环境视为一个智能体，这可以部分缓解隐藏的混杂因素效应。具体来说，提出了一种去混杂环境重建方法DEMER。与之前提到的方法不同，DEMER假设环境是部分观测的，并将整个推荐任务建模为部分观测的马尔可夫决策过程（POMDP）。与MDP不同，POMDP包含一个额外的观测组件$o\in\mathcal{O}$，且行动$a_{t}$是基于观测$o_{t}$而非状态$s_{t}$推导的，即$a_{t}=\pi_{a}(o_{t})$。DEMER假设对观测$o_{h}$存在一个混杂策略$\pi_{h}$，它由$a_{t}$和$o_{t}$组成：$a_{h}=\pi_{h}(a_{t},o_{t})$。此外，还引入了另一个观测$o_{b}$来观察过渡。$\pi_{b}$是对应的策略，$a_{b}=\pi_{b}(o_{b})=\pi_{b}(o_{t},a_{t},a_{h})$。DEMER使用生成对抗模仿学习（GAIL）来模仿策略$A,B$。给定不同策略$A$和$B$的轨迹$\{o_{t},o_{h},o_{b}\}$，目标函数被定义为
- en: '|  | $\displaystyle(\pi_{a},\pi_{b},\pi_{h})=\operatorname*{arg\,min}_{(\pi_{a},\pi_{b},\pi_{h})}\mathbb{E}_{s\sim\tau}(L(s,a_{t},a_{b}))$
    |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| (35) |  | $\displaystyle\text{where }L(s,a_{t},a_{b})=\mathbb{E}_{(\pi_{a},\pi_{b},\pi_{h})}[\log
    D(s,a_{t},a_{b})]-\lambda\sum_{\pi\in\{\pi_{a},\pi_{b},\pi_{h}\}}H(\pi)$ |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: where $L(\cdot)$ is the loss function, $D(\cdot)$ is a discriminator and $H(\pi)$
    is introduced in GAIL.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. State Representation
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'State representation is another component in DRL-based RS which exists in both
    model-based and model-free methods. Liu et al. ([2018](#bib.bib61)) find that
    the state representation in model-free methods would affect recommendation performance.
    Existing studies usually directly use the embedding as the state representation.
    Liu et al. ([2020a](#bib.bib59)); Liu et al. ([2020c](#bib.bib62)) propose a supervised
    learning method to generate a better state representation by utilizing an attention
    mechanism and a pooling operation as shown in [Figure 7](#S3.F7 "In 3.3.2\. State
    Representation ‣ 3.3\. Component Optimization in Deep Reinforcement Learning based
    RS ‣ 3\. Deep Reinforcement Learning in Recommender Systems ‣ A Survey of Deep
    Reinforcement Learning in Recommender Systems: A Systematic Review and Future
    Directions"). Such a representation method requires training a representation
    network when training the main policy network, which increases the model complexity.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95436f55716b2b057a9c5cd37fd9e4f8.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: Figure 7. State representation used in works (Liu et al., [2020a](#bib.bib59);
    Liu et al., [2020c](#bib.bib62)). $h_{t}$ is the output of an attention layer
    that takes the representation of users’ history at time $t$ as input and $g(\cdot)$
    is the pooling operation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Robustness of Reward Functions
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The reward function is crucial for methods involving DQN. A robust reward function
    can significantly improve training efficiency and performance. Kostrikov et al.
    ([2019](#bib.bib51)) find that the DQN may not receive the correct reward value
    when entering the absorbing state. That is, when the absorbing state is reached,
    the agent will receive zero reward and harm policy learning. The reason behind
    this is that when designing the environment zero reward is implicitly assigned
    to the absorbing state as it is hard to determine the reward value in such a state.
    Chen et al. ([2018](#bib.bib16)) propose a robust DQN method, which can stabilize
    the reward value when facing the absorbing state. The new reward formula can improve
    the robustness, which is defined as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '| (36) |  | $\displaystyle r=\begin{cases}r_{t}&amp;\text{if }s_{t+1}\text{
    is an absorbing state}\\ r_{t}+\gamma Q_{\theta^{\prime}}(s_{t+1},a_{t+1})&amp;\text{otherwise}.\end{cases}$
    |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: The major difference is that $r_{t}$ is assigned to the absorbing state to ensure
    the agent can continue learning. One remaining problem in current DRL-based RS
    is the reward sparsity, i.e,. the large state and action spaces make the reward
    sparsity problem more serious. One of the possible solution would be a better
    designed reward by using the reward shaping (Ng et al., [1999](#bib.bib70)).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于 $r_{t}$ 被分配给吸收状态，以确保智能体可以继续学习。目前基于深度强化学习的推荐系统中仍然存在一个问题，即奖励稀疏性，即大状态和动作空间使得奖励稀疏性问题更加严重。一个可能的解决方案是通过使用奖励塑形（Ng
    等，[1999](#bib.bib70)）来更好地设计奖励。
- en: 4\. Emerging Topics
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 新兴话题
- en: While existing studies have established a solid foundation for DRL-based RS
    research, this section outlines several promising emerging research directions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有研究为基于深度强化学习的推荐系统研究奠定了坚实的基础，本节概述了几个有前景的新兴研究方向。
- en: 4.1\. Multi-Agent and Hierarchical Deep Reinforcement Learning-based RS
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 基于多智能体和层次化深度强化学习的推荐系统
- en: Recommender systems are monolithic systems containing tasks such as searching,
    ranking, recommendation, advertising, personalization, and diverse stakeholders
    such as users and items. Most existing methods are based on single agent.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是包含搜索、排序、推荐、广告、个性化等任务的整体系统，以及用户和项目等多样化利益相关者。大多数现有方法基于单一智能体。
- en: Multi-Agent Reinforcement Learning (MARL) is a subfield of reinforcement learning
    and it is capable of learning multiple policies and strategies.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体强化学习（MARL）是强化学习的一个子领域，能够学习多个策略和策略。
- en: 'While a single-agent reinforcement learning framework can only handle a single
    task, many studies consider the multi-task situation in RS and employ multi-agent
    DRL (MADRL) or hierarchical DRL (HDRL). HDRL (Kulkarni et al., [2016](#bib.bib52))
    is proposed to handle complex tasks by splitting such tasks into several small
    components and asks the agent to determine sub-policies. HDRL belongs to a single-agent
    reinforcement learning framework such that the agent contains a meta-controller
    and several controllers. The meta-controller splits the task, and the controllers
    learn the value and reward functions for designated tasks to get a series of sub-policies.
    There are a few studies already utilizing HDRL in RS. Xie et al. ([2021](#bib.bib106))
    target integrated recommendation to capture user preferences on both heterogeneous
    items and recommendation channels. Specifically, the meta-controller is used for
    item recommendation, and controllers aim to find the personalized channel according
    to user channel-level preferences. Zhang et al. ([2019a](#bib.bib114)) uses HDRL
    for course recommendation in MOOCs, which contains two different tasks: profile
    reviser and recommendation. The meta-controller aims to make course recommendations
    by using the revised profile pruned by the controllers.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管单一智能体强化学习框架只能处理单一任务，但许多研究考虑了推荐系统中的多任务情况，并采用了多智能体深度强化学习（MADRL）或层次化深度强化学习（HDRL）。HDRL（Kulkarni
    等，[2016](#bib.bib52)）旨在通过将复杂任务拆分为几个小组件来处理这些任务，并要求智能体确定子策略。HDRL 属于单一智能体强化学习框架，其中智能体包含一个元控制器和多个控制器。元控制器拆分任务，而控制器则为指定任务学习价值和奖励函数，以获得一系列子策略。目前已有一些研究在推荐系统中使用
    HDRL。Xie 等（[2021](#bib.bib106)）针对集成推荐，以捕捉用户对异质项目和推荐渠道的偏好。具体而言，元控制器用于项目推荐，控制器则根据用户渠道层级偏好来寻找个性化渠道。Zhang
    等（[2019a](#bib.bib114)）将 HDRL 应用于大规模在线课程推荐，包含两个不同任务：个人信息修订和推荐。元控制器旨在通过使用控制器修订后的个人信息来进行课程推荐。
- en: 'Different from HDRL, MADRL (Egorov, [2016](#bib.bib24)) introduces multiple
    agents to handle the sub-tasks. Gui et al. ([2019](#bib.bib34)) uses the MADRL
    for twitter mention recommendation where three agents are initialized. The three
    agents need to generate different representations for the following tasks: query
    text, historical text from authors and historical text from candidate users. Once
    the representations are finalized, the model will conduct the recommendation based
    on the concatenation of representations. Feng et al. ([2018](#bib.bib25)) and
    He et al. ([2020](#bib.bib37)) provide two different views of the communication
    mechanism in MADRL and demonstrate that agents could work collaboratively or individually.
    Zhao et al. ([2020e](#bib.bib130)) designs a MADRL framework for two tasks where
    two agents are designed to conduct advertising and recommendation respectively.
    Zhang et al. ([2017](#bib.bib120)) uses MADRL for collaborative recommendation
    where each agent is responsible for a single user. MADRL is adopted to help the
    recommender consider both collaboration and potential competition between users.
    Zhang et al. ([2021](#bib.bib118)) designs a charging recommender system for intelligent
    electric vehicles by using decentralized agents to handle sub-tasks and a centralized
    critic to make the final decision.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 与HDRL不同，MADRL（Egorov, [2016](#bib.bib24)）引入了多个代理来处理子任务。Gui等人（[2019](#bib.bib34)）使用MADRL进行Twitter提及推荐，其中初始化了三个代理。这三个代理需要为以下任务生成不同的表示：查询文本、来自作者的历史文本以及来自候选用户的历史文本。一旦表示被最终确定，模型将根据表示的拼接进行推荐。Feng等人（[2018](#bib.bib25)）和He等人（[2020](#bib.bib37)）提供了MADRL中通信机制的两种不同视角，并演示了代理可以协作或单独工作。Zhao等人（[2020e](#bib.bib130)）设计了一个MADRL框架用于两个任务，其中两个代理分别用于广告和推荐。Zhang等人（[2017](#bib.bib120)）使用MADRL进行协同推荐，每个代理负责一个用户。MADRL被采用以帮助推荐系统同时考虑用户间的协作和潜在竞争。Zhang等人（[2021](#bib.bib118)）设计了一个用于智能电动车的充电推荐系统，利用去中心化的代理处理子任务，并使用中心化的评判者做出最终决定。
- en: Hierarchical multi-agent RL (HMARL) (Makar et al., [2001](#bib.bib67)) proves
    that MARL and HRL can be combined. Recently, Yang et al. ([2018](#bib.bib110))
    introduces HMADRL into the continuous action space, which provides a direction
    for RS. Zhao et al. ([2020d](#bib.bib121)) uses HMARL for multi-goal recommendation
    where the meta-controller considers users’ long-term preferences and controllers
    focus on short-term click behavior. While the meta-controller and controllers
    in HDRL deal with sub-tasks that belong to a single task, HMARL focuses on multi-task
    or multi-goal learning where the meta-controller and controllers belong to different
    agents and deal with different tasks or goals.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 分层多代理强化学习（HMARL）（Makar等人，[2001](#bib.bib67)）证明了MARL和HRL可以结合。最近，Yang等人（[2018](#bib.bib110)）将HMADRL引入连续动作空间，为推荐系统提供了一个方向。Zhao等人（[2020d](#bib.bib121)）使用HMARL进行多目标推荐，其中元控制器考虑用户的长期偏好，控制器则关注短期点击行为。尽管HDRL中的元控制器和控制器处理属于单一任务的子任务，HMARL则关注多任务或多目标学习，其中元控制器和控制器属于不同的代理，并处理不同的任务或目标。
- en: HMADRL would be a suitable solution for future research work in DRL-based RS
    where HDRL can be used to split a complex task into several sub-tasks such as
    users’ long-term interests and short-term click behavior, and MADRL can jointly
    consider multiple factors such as advertising Zhao et al. ([2020d](#bib.bib121)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: HMADRL将是未来基于DRL的推荐系统研究中的一种合适解决方案，其中HDRL可以用于将复杂任务分解为多个子任务，如用户的长期兴趣和短期点击行为，而MADRL可以联合考虑多个因素，如广告（Zhao等人，[2020d](#bib.bib121)）。
- en: 4.2\. Inverse Deep Reinforcement Learning for RS
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 用于推荐系统的逆深度强化学习
- en: As mentioned, the reward function plays a critical role in DRL-based recommender
    systems. In many existing works, reward functions are manually designed. The common
    method uses users’ click behavior to represent the reward and to reflect users’
    interests. However, such a setting can not represent users’ long-term goals (Zou
    et al., [2019](#bib.bib135)) as clicking or not only depicts part of the feedback
    information from users. It requires significant effort to design a reward function,
    due to the large number of factors that can affect users’ decision, such as social
    engagement or bad product reviews, which may adversely affect recommendation performance.
    It is difficult to include all potential factors into the reward function because
    not every factor can be represented properly. A few works (Gong et al., [2019](#bib.bib32);
    Chen et al., [2020c](#bib.bib20)) show that manually designed reward functions
    can be omitted by employing inverse reinforcement learning (IRL) (Ng et al., [2000](#bib.bib71))
    or generative adversarial imitation learning (GAIL) (Ho and Ermon, [2016](#bib.bib38)).
    Such inverse DRL-based methods require using expert demonstration as the ground
    truth. However, expert demonstration is often hard to obtain for recommendation
    scenarios. Those two studies conduct experiments in an offline dataset-based simulation
    environment that can access expert demonstration. In contrast, Chen et al. ([2020c](#bib.bib20))
    use IRL as the main algorithm to train the agent while Gong et al. ([2019](#bib.bib32))
    use both demonstration and reward to train the agent. Zhao et al. ([2020b](#bib.bib122))
    also employ GAIL to improve recommendation performance. In this work, GAIL is
    used to learn the reasoning path inside the KG to provide side information to
    help the agent learn the policy. Although IRL achieves some progress in RS, the
    lack of demonstration is a key shortcoming that impedes adoption in RS. One possibility
    is to use the IRL method in casual reasoning to help improve interpretability (Bica
    et al., [2020](#bib.bib6)) thus boosting recommendation performance. Alternately,
    IRL may be suitable for learning users’ long-term and static behavior to support
    the reward function.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Graph Neural Networks for Boosting DRL-based RS
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph data and KG are widely used in RS. Graph modeling enables an RS to leverage
    interactions between users and the recommender for reasoning or improving interpretability.
    According to existing studies about deep learning-based RS (Zhang et al., [2019b](#bib.bib116)),
    embedding is a technique used to get the representation for the input data. Graph
    embedding is a common solution to handle graph-like data. GCN is a type of graph
    embedding method which are broadly used in RS to process graph data. Wang et al.
    ([2019](#bib.bib95)) propose a variant of GCN to learn the embedding for KG. Specifically,
    they propose knowledge graph convolutional networks (KGCN) to capture the high-order
    structural proximity among entities in a knowledge graph.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据和知识图谱在推荐系统中被广泛使用。图建模使推荐系统能够利用用户和推荐者之间的互动进行推理或提高可解释性。根据现有关于深度学习推荐系统的研究 (Zhang
    等人，[2019b](#bib.bib116))，嵌入是一种用于获取输入数据表示的技术。图嵌入是处理图状数据的常见解决方案。GCN 是一种广泛用于推荐系统处理图数据的图嵌入方法。Wang
    等人 ([2019](#bib.bib95)) 提出了一个 GCN 的变体，用于学习知识图谱的嵌入。具体而言，他们提出了知识图谱卷积网络 (KGCN)，以捕捉知识图谱中实体之间的高阶结构接近性。
- en: In DRL-based RS, graph data are handled similarly—s underthe transformed into
    an embedding and fed to the agent. Wang et al. ([2020a](#bib.bib99)) uses a traditional
    graph embedding method TransE (Bordes et al., [2013](#bib.bib7)) to generate the
    state representation for DRL-based RS. There are several studies that use GCN
    in DRL for recommendations under different settings. Jiang et al. ([2020](#bib.bib47))
    propose a graph convolutional RL (DGN) method which integrates the GCN into the
    Q-learning framework for general RL problems by replacing the state encoding layer
    with the GCN layer. Lei et al. ([2020](#bib.bib54)) extend this method into the
    deep learning field and apply it to recommender systems. To be specific, multiple
    GCN layers are employed to process the sub-graphs for a given item $i$. Chen et al.
    ([2020b](#bib.bib18)) employs KG inside the actor-critic algorithm to help the
    agent learn the policy. Specifically, the critic network contains a GCN layer
    to give weight to the graph and conduct searches in the graph to find an optimal
    path and hence guide the optimization of policy learning. However, such a method
    is relatively computationally expensive as it requires jointly training the GCN
    and the actor-critic network. Gong et al. ([2019](#bib.bib32)) adopts a Graph
    Attention Network (GAT) (Veličković et al., [2017](#bib.bib93)) into the actor-critic
    network to conduct recommendation. In addition, the GAT is used as an encoder
    to obtain a state representation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DRL 基于的推荐系统中，图数据的处理方式类似——将其转化为嵌入并输入到代理中。Wang 等人 ([2020a](#bib.bib99)) 使用传统的图嵌入方法
    TransE (Bordes 等人，[2013](#bib.bib7)) 为 DRL 基于的推荐系统生成状态表示。有几个研究在不同设置下使用 GCN 在 DRL
    中进行推荐。Jiang 等人 ([2020](#bib.bib47)) 提出了一个图卷积 RL (DGN) 方法，该方法将 GCN 集成到 Q 学习框架中，通过用
    GCN 层替换状态编码层来解决一般 RL 问题。Lei 等人 ([2020](#bib.bib54)) 将此方法扩展到深度学习领域，并将其应用于推荐系统。具体来说，多个
    GCN 层被用来处理给定项目 $i$ 的子图。Chen 等人 ([2020b](#bib.bib18)) 在演员-评论家算法中应用知识图谱来帮助代理学习策略。具体而言，评论家网络包含一个
    GCN 层，用来给图赋权并在图中进行搜索以找到最优路径，从而指导策略学习的优化。然而，这种方法计算开销相对较大，因为它需要联合训练 GCN 和演员-评论家网络。Gong
    等人 ([2019](#bib.bib32)) 将图注意力网络 (GAT) (Veličković 等人，[2017](#bib.bib93)) 应用于演员-评论家网络中以进行推荐。此外，GAT
    还用作编码器以获得状态表示。
- en: A common way of using GCN or its variants in DRL-based RS is the state encoder.
    The related challenge is the difficulty for the environment to provide a graph-like
    input to the GCN.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DRL 基于的推荐系统中使用 GCN 或其变体的一种常见方式是状态编码器。相关挑战是环境难以向 GCN 提供图状输入。
- en: 4.4\. Self-Supervised DRL-based RS
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 自监督 DRL 基于推荐系统
- en: Self-supervised learning (SSL) is a technique in which the model is trained
    by itself without external label information. SSL-DRL is receiving growing interest
    in robotics  (Kahn et al., [2018](#bib.bib48); Zeng et al., [2018](#bib.bib113)).
    Kahn et al. ([2018](#bib.bib48)) shows that SSL can be used to learn the policy
    when doing navigation by providing real-world experience. Zeng et al. ([2018](#bib.bib113))
    demonstrates that SSL-DRL can be used to help the agent learn synergies between
    two similar policies, thus empowering the agent to conduct two different tasks.
    Recent advances in SSL RL show that SSL can also provide interpretability for
    RL, which is promising for interpretable RS research (Shi et al., [2020](#bib.bib86)).
    Shi et al. ([2020](#bib.bib86)) shows that SSL based RL can highlight the task-relevant
    information to guide the agent’s behavior. Moreover, Xin et al. ([2020](#bib.bib107))
    shows that SSL can be used to provide negative feedback for DRL-based RS to improve
    recommendation performance. To be specific, a self-supervised loss function is
    appended into the normal DRL loss function,
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '| (37) |  | $\displaystyle-\sum_{i=1}^{n}Y_{i}\log\bigg{(}\frac{e^{y_{i}}}{\sum_{i^{\prime}=1}^{n}e^{y_{i^{\prime}}}}\bigg{)}+L_{\mathit{DRL}}$
    |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: 'where $Y_{i}$ is an indicator function to show users interact with the item
    $i$ or not. $L_{\mathit{DRL}}$ could vary, if the DQN is adopted, [Equation 4](#S2.E4
    "In 2.2\. Preliminaries of Deep Reinforcement Learning ‣ 2\. Background ‣ A Survey
    of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and
    Future Directions") should be used. SSL demonstrates promising performance in
    visual representation in recent years, which would be a possible solution to generate
    the state representation as there are a few DRL-based RS studies that adopt CNNs
    to process image-like data and transform it into a state (Liu et al., [2020b](#bib.bib60);
    Gao et al., [2019](#bib.bib29)). Furthermore, as an unsupervised learning approach,
    SSL would provide a new direction about defining the reward function by learning
    common patterns between different states as well as multi-task learning.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Open Questions
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we outline several open questions and challenges that exist
    in DRL-based RS research. We believe these issues could be critical for the future
    development of DRL-based RS.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Sample Efficiency
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sample inefficiency is a well-known challenge in model-free DRL methods. Model-free
    DRL requires a significant number of samples as there is no guarantee that the
    received state is useful. Normally, after a substantial number of episodes, the
    agent may start learning as the agent finally receives a useful state and reward
    signal. A common solution is the experience replay technique, which only works
    in off-policy methods. Experience replay still suffers the sample inefficiency
    problem (Schaul et al., [2015](#bib.bib78)) as not every past experience is worth
    replaying. Isele and Cosgun ([2018](#bib.bib44)) propose selected experience replay
    (SER) that only stores valuable experience into the replay buffer and thus improves
    sample efficiency. while traditional DRL environments only contain several³³3For
    example, the number of actions in MuJoCo is less than one hounded. candidate items,
    in DRL-based RS, the agent must deal with a significantly larger action space
    as RS may contain lots of candidate items. Existing DRL-based RS studies on traditional
    experience replay methods often demonstrate slow converge speed. Chen et al. ([2021](#bib.bib15))
    design a user model to improve the sample efficiency through auxiliary learning.
    Specifically, they apply the auxiliary loss with the state representation, and
    the model distinguishes low-activity users and asks the agent to update the recommendation
    policy based on high-activity users more frequently.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 样本效率低下是无模型深度强化学习（DRL）方法中的一个著名挑战。无模型DRL需要大量样本，因为没有保证接收到的状态是有用的。通常，在经过大量回合后，代理可能会开始学习，因为代理最终收到一个有用的状态和奖励信号。一个常见的解决方案是经验回放技术，这仅在脱离策略的方法中有效。经验回放仍然存在样本效率低下的问题（Schaul
    et al., [2015](#bib.bib78)），因为不是每一个过去的经验都值得回放。Isele 和 Cosgun ([2018](#bib.bib44))
    提出了选择性经验回放（SER），只将有价值的经验存储到回放缓冲区，从而提高样本效率。虽然传统DRL环境中通常只有几个候选项，例如MuJoCo中的动作数量少于一百，但在基于DRL的推荐系统中，代理必须处理一个显著更大的动作空间，因为推荐系统可能包含大量候选项。现有的基于DRL的推荐系统研究中的传统经验回放方法通常显示出较慢的收敛速度。Chen
    等人 ([2021](#bib.bib15)) 设计了一种用户模型，通过辅助学习来提高样本效率。具体而言，他们将辅助损失应用于状态表示，模型区分低活跃用户，并要求代理根据高活跃用户更频繁地更新推荐策略。
- en: On the other hand, model-based methods are more sample efficient. However, they
    introduce extra complexity as the agent is required to learn the environment model
    as well as the policy. Due to the extremely large action space and possibly large
    state space (depending on users’ contextual information) in RS, approximating
    the environment model and policy simultaneously becomes challenging.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于模型的方法在样本效率方面更高。然而，这些方法引入了额外的复杂性，因为代理需要同时学习环境模型和策略。由于推荐系统（RS）中的动作空间极其庞大以及可能存在的大状态空间（取决于用户的上下文信息），同时近似环境模型和策略变得具有挑战性。
- en: 5.2\. Exploration and Exploitation
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2. 探索与利用
- en: The exploration and exploitation dilemma is a fundamental and challenging problem
    in reinforcement learning research and receives lots of attention in DRL. This
    dilemma describes a trade-off between obtaining new knowledge and the need to
    use that knowledge to improve performance. Many DQN-based methods focus on exploration
    before the replay buffer is full and exploitation afterward. Consequently, it
    requires an extremely large replay buffer to allow all possibilities in recommendation
    can be stored. DRN employs Dueling Bandit Gradient Descent (DBGD) (Yue and Joachims,
    [2009](#bib.bib112)) to encourage exploration while  (Chen et al., [2019c](#bib.bib19);
    He et al., [2020](#bib.bib37)) introduces a regularization or entropy term into
    the objective function to do so. (Ie et al., [2019b](#bib.bib43)) uses the sheer
    size of the action space to ensure sufficient exploration. (Wang et al., [2020a](#bib.bib99);
    Xian et al., [2019](#bib.bib104); Wang et al., [2020b](#bib.bib101)) uses a separate
    KG or elaborated graph exploration operation to conduct exploration. (Chen et al.,
    [2019a](#bib.bib14)) employs Boltzmann exploration to get the benefit of exploratory
    data without negatively impacting user experience. In addition, $\epsilon$-greedy
    is the most common technique used to encourage exploration  (Zou et al., [2020](#bib.bib136);
    Lei et al., [2020](#bib.bib54); Lei and Li, [2019](#bib.bib53); Zou et al., [2019](#bib.bib135);
    Lei et al., [2019](#bib.bib55); Wang et al., [2021](#bib.bib97); Cai et al., [2018](#bib.bib10);
    Liu et al., [2020b](#bib.bib60); Xie et al., [2021](#bib.bib106)). Remaining studies
    rely on a simulator to conduct exploration. However, it may suffer from noise
    and over-fitting (Xie et al., [2021](#bib.bib106)) because of the gap between
    simulation and real online application. For most DRL-based methods such as vanilla
    DQN, policy gradient, or actor-critic-based methods, $\epsilon$-greedy would be
    a good choice for exploration. In addition, injecting noise into the action space
    would also be helpful for those actor-critic-based methods (Lillicrap et al.,
    [2015](#bib.bib57)). For methods involving KGs, $\epsilon$-greedy may help, but
    the elaborated graph exploration methods may receive better performance.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用的困境是强化学习研究中的一个基本且具有挑战性的问题，并在深度强化学习（DRL）中受到广泛关注。这个困境描述了获取新知识和利用这些知识来提高性能之间的权衡。许多基于DQN的方法在重放缓冲区未满时专注于探索，而在缓冲区满后则专注于利用。因此，它需要一个极大的重放缓冲区来存储所有推荐的可能性。DRN采用对抗性赌博梯度下降（DBGD）（Yue
    和 Joachims, [2009](#bib.bib112)）来鼓励探索，而（Chen 等, [2019c](#bib.bib19); He 等, [2020](#bib.bib37)）则在目标函数中引入正则化或熵项来实现这一点。（Ie
    等, [2019b](#bib.bib43)）利用动作空间的庞大来确保足够的探索。（Wang 等, [2020a](#bib.bib99); Xian 等,
    [2019](#bib.bib104); Wang 等, [2020b](#bib.bib101)）则使用单独的KG或精细的图探索操作来进行探索。（Chen
    等, [2019a](#bib.bib14)）采用Boltzmann探索来获得探索数据的好处而不会对用户体验产生负面影响。此外，$\epsilon$-贪婪是最常用的鼓励探索的技术（Zou
    等, [2020](#bib.bib136); Lei 等, [2020](#bib.bib54); Lei 和 Li, [2019](#bib.bib53);
    Zou 等, [2019](#bib.bib135); Lei 等, [2019](#bib.bib55); Wang 等, [2021](#bib.bib97);
    Cai 等, [2018](#bib.bib10); Liu 等, [2020b](#bib.bib60); Xie 等, [2021](#bib.bib106)）。其余研究依赖于模拟器进行探索。然而，由于模拟与真实在线应用之间的差距，它可能会受到噪声和过拟合的影响（Xie
    等, [2021](#bib.bib106)）。对于大多数基于DRL的方法，如原始DQN、策略梯度或演员-评论家方法，$\epsilon$-贪婪是探索的一个不错选择。此外，将噪声注入动作空间也对那些基于演员-评论家的方法有帮助（Lillicrap
    等, [2015](#bib.bib57)）。对于涉及KG的方法，$\epsilon$-贪婪可能有帮助，但精细的图探索方法可能会有更好的表现。
- en: 5.3\. Generalizing from Simulation to Real-World Recommendation
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 从模拟到现实世界推荐的泛化
- en: Existing work generally trains DRL algorithms in simulation environments or
    offline datasets. Deploying DRL algorithms into real applications is challenging
    due to the gap between simulation and real-world applications. Simulation environments
    do not contain domain knowledge or social impact. They can not cover the domain
    knowledge and task-specific engineering in the real-world recommendation. How
    to bridge the gap between simulation and real applications is a challenging topic.
    Sim2real (Zhao et al., [2020a](#bib.bib123)) is a transfer learning approach that
    transfers DRL policies from simulation environments to reality. Sim2real uses
    domain adaption techniques to help agents transfer learned policy. Specifically,
    it adopts GANs to conduct adaption by generating different samples. RL-CycleGAN (Rao
    et al., [2020](#bib.bib75)) is a sim2real method for vision-based tasks. It uses
    CycleGAN (Zhu et al., [2017](#bib.bib133)) to conduct pixel-level domain adaption.
    Specifically, it maintains cycle consistency during GAN training and encourages
    the adapted image to retain certain attributes of the input image. In DRL-based
    RS, sim2real would be a possible solution for generalizing the learned policy
    from simulation environments to reality. However, sim2real is a new technique
    still under exploration. It shows an adequate capability in simple tasks and requires
    more effort to handle the complex task such as recommendation. We believe it is
    a workable solution for generalizing from simulation to reality.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Bias (Unfairness)
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chen et al. ([2020a](#bib.bib13)) observe that user behavior data are not experimental
    but observational, which leads to problems of bias and unfairness.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: There are two reasons why bias is so common. First, the inherent characteristic
    of user behavior data is not experimental but observational. In other words, data
    that are fed into recommender systems are subject to selection bias (Schnabel
    et al., [2016](#bib.bib79)). For instance, users in a video recommendation system
    tend to watch, rate, and comment on those movies that they are interested in.
    Second, a distribution discrepancy exists, which means the distributions of users
    and items in the recommender system are not even. Recommender systems may suffer
    from ’popularity bias’, where popular items are recommended far more frequently
    than the others. However, the ignored products in the “long tail” can be equally
    critical for businesses as they are the ones less likely to be discovered.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Friedman and Nissenbaum ([1996](#bib.bib27)) denote the unfairness as that the
    system systematically and unfairly discriminates against certain individuals or
    groups of individuals in favor of others.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: A large number of studies explore dynamic recommendation systems by utilizing
    the agent mechanism in reinforcement learning (RL), considering the information
    seeking and decision-making as sequential interactions. How to evaluate a policy
    efficiently is a big challenge for RL-based recommenders. Online A/B tests are
    not only expensive and time-consuming but also sometimes hurt the user experience.
    Off-policy evaluation is an alternative strategy that historical user behavior
    data are used to evaluate the policy. However, user behavior data are biased,
    as mentioned before, which causes a gap between the policy of RL-based RS and
    the optimal policy.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: To eliminate the effects of bias and unfairness, Chen et al. ([2019a](#bib.bib14))
    use the inverse of the probability of historical policy to weight the policy gradients.
    Huang et al. ([2020](#bib.bib41)) introduce a debiasing step that corrects the
    biases presented in the logged data before it is used to simulate user behavior.
    Zou et al. ([2020](#bib.bib136)) propose to build a customer simulator that is
    designed to simulate the environment and handle the selection bias of logged data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 5.5\. Explainability
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although deep learning-based models can generally improve the performance of
    recommender systems, they are not easily interpretable. As a result, it becomes
    an important task to make recommender results explainable, along with providing
    high-quality recommendations. High explainability in recommender systems not only
    helps end-users understand the items recommended but also enables system designers
    to check the internal mechanisms of recommender systems. Zhang and Chen ([2020](#bib.bib119))
    review different information sources and various types of models that can facilitate
    explainable recommendation. Attention mechanisms and knowledge graph techniques
    currently play an important role in realizing explainability in RS.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Attention models have great advantages in both enhancing predictive performance
    and having greater explainability (Zhang et al., [2019c](#bib.bib117)). Wang et al.
    ([2018a](#bib.bib100)) introduce a reinforcement learning framework incorporated
    with an attention model for explainable recommendation. Firstly, it achieves model-agnosticism
    by separating the recommendation model from the explanation generator. Secondly,
    the agents that are instantiated by attention-based neural networks can generate
    sentence-level explanations.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs contain rich information about users and items, which can help
    to generate intuitive and more tailored explanations for the recommendation system
    (Zhang and Chen, [2020](#bib.bib119)). Recent work has achieved greater explainability
    by using reinforcement and knowledge graph reasoning. The algorithm from (Xian
    et al., [2019](#bib.bib104)) learns to find a path that navigates from users to
    items of interest by interacting with the knowledge graph environment. Zhao et al.
    ([2020b](#bib.bib122)) extract imperfect path demonstrations with minimum labeling
    effort and propose an adversarial actor-critic model for demonstration-guided
    path-finding. Moreover, it achieves better recommendation accuracy and explainability
    by reinforcement learning and knowledge graph reasoning.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 5.6\. Robustness on Adversarial Samples and Attacks
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial samples demonstrate that deep learning-based methods are vulnerable.
    Hence, robustness becomes an open question for both RS and DRL. Specifically,
    adversarial attack and defense in RS have received a lot of attention in recent
    years (Deldjoo et al., [2021](#bib.bib23)) as security is crucial in RS. Moreover,
    DRL policies are vulnerable to adversarial perturbations to agent’s observations (Lin
    et al., [2017](#bib.bib58)). Gleave et al. ([2020](#bib.bib31)) provide an adversarial
    attack method for perturbing the observations, thus affecting the learned policy.
    Hence, improving the robustness is the common interest for DRL and RS, which would
    be a critical problem for DRL-based RS. Cao et al. ([2020](#bib.bib11)) provide
    an adversarial attack detection method for DRL-based RS which uses the GRU to
    encode the action space into a low-dimensional space and design decoders to detect
    the potential attack. However, it only considers Fast Gradient Sign Method (FGSM)-based
    attacks and strategically-timed attacks (Lin et al., [2017](#bib.bib58)). Thus,
    it lacks the capability to detect other types of attack. Moreover, it only provides
    the detection method while the defence is still an opening question.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: We believe zero-shot learning techniques would be a good direction for training
    a universal adversarial attack detector. For defence, it is still an open question
    for DRL-based RS, though recent advances in adversarial defence in DRL may provide
    some insights (Lütjens et al., [2020](#bib.bib64); Wang et al., [2020c](#bib.bib94);
    Chen et al., [2019d](#bib.bib17)).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Future Directions
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide a few potential future directions of DRL-based RS.
    Benefiting from recent advances in DRL research, we believe those topics can boost
    the progress of DRL-based RS research.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Causal and Counterfactual Inference
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Causality is a generic relationship between a cause and effect. Moreover, inferring
    causal effects is a fundamental problem in many applications like computational
    advertising, search engines, and recommender systems (Bottou et al., [2013](#bib.bib8)).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Recently, some researchers have connected reinforcement learning with learning
    causality to improve the effects for solving sequential decision-making problems.
    Besides, Learning agents in reinforcement learning frameworks face a more complicated
    environment where a large number of heterogeneous data are integrated. From our
    point of view, causal relationships would be capable of improving the recommendation
    results by introducing the directionality of cause and the effect. The users’
    previous choices have impact on the subsequent actions. This can be cast as an
    interventional data generating the dynamics of recommender systems. By viewing
    a policy in RL as an intervention, we can detect unobserved confounders in RL
    and choose a policy on the expected reward to better estimate the causal effect
    (Shang et al., [2019](#bib.bib83)). Some studies improve RL models with causal
    knowledge as side information. Another line of work uses causal inference methods
    to achieve unbiased reward prediction (Guo et al., [2020](#bib.bib35)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Yang et al. ([2021](#bib.bib109)) propose a Causal Inference Q-network which
    introduces observational inference into DRL by applying extra noise and uncertain
    inventions to improve resilience. Specifically, in this work, noise and uncertainty
    are added into the state space during the training state, and the agent is required
    to learn a causal inference model by considering the perturbation. Dasgupta et al.
    ([2019](#bib.bib21)) give the first demonstration that model-free reinforcement
    learning can be used for causal reasoning. They explore meta-reinforcement learning
    to solve the problem of causal reasoning. The agents trained by a recurrent network
    able to make causal inferences from observational data and output counterfactual
    predictions. Forney et al. ([2017](#bib.bib26)) bridge RL and causality by data-fusion
    for reinforcement learners. Specifically, online agents combine observations,
    experiments and counterfactual data to learn about the environment, even if unobserved
    confounders exist. Similarly, Gasse et al. ([2021](#bib.bib30)) make the model-based
    RL agents work in a causal way to explore the environment under the Partially-Observable
    Markov Decision Process (POMDP) setting. They consider interventional data and
    observational data jointly and interprete model-based reinforcement learning as
    a causal inference problem. In this way, they bridge the gap between RL and causality
    by relating common concepts in RL and causality.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Regarding explainability in RL, Madumal et al. ([2020](#bib.bib66)) propose
    to explain the behavior of agents in reinforcement learning with the help of causal
    science. The authors encode causal relationships and learn a structural causal
    model in RL, which is used to generate explanations based on counterfactual analysis.
    With counterfactual exploration, this work is able to generate two contrastive
    explanations for ‘why’ and ‘why not’ questions.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: It is so important to search for a Directed Acyclic Graph (DAG) in causal discovery.
    Considering traditional methods rely on local heuristics and predefined score
    functions, Zhu et al. ([2019](#bib.bib134)) propose to use reinforcement learning
    to search DAG for causal discovery. They use observational data as an input, RL
    agents as a search strategy and output the causal graph generated from an encoder-decoder
    NN model.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Offline DRL and Meta DRL
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recommender systems often need to deal with multiple scenarios such as joint
    recommendation and adverting, offline DRL and meta DRL provide a promising direction
    for achieving multiple scenarios at the same time.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Offline DRL is a new paradigm of DRL that can be combined with existing methods
    such as self-supervised learning and transfer learning to move toward real-world
    settings. Offline DRL (Levine et al., [2020](#bib.bib56)) (also known as batch
    DRL) is designed for tasks which contain huge amounts of data. Given a large dataset
    that contains past interactions, offline DRL uses the dataset for training across
    many epochs but does not interact with the environment. Offline DRL provides a
    solution that can be generalized to new scenarios as it was trained by a large
    sized dataset. Such generalization ability is critical to RSs, which may need
    to deal with multiple scenarios or multiple customers. While offline DRL could
    provide a new direction for DRL-based RS, it still faces a few problems regarding
    handling the distributional shifts between existing datasets and real-world interactions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Meta DRL (Wang et al., [2016](#bib.bib96)) is defined as meta learning in the
    filed of DRL. Meta DRL is another approach to help agents to generalize to new
    tasks or environments. Different from offline DRL, meta DRL contains a memory
    unit which is formed by the recurrent neural network to memorize the common knowledge
    for different tasks. Different from offline DRL, meta DRL does not require a large
    amount of data to train.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Further Developments in Actor-Critic Methods
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An actor-critic method uses the traditional policy gradient method, which suffers
    from the high variance problem due to the gap between behavior policy (i.e., the
    policy that is being used by an agent for action select) and target policy (i.e.,
    the policy that an agent is trying to learn). A method commonly used to relieve
    the high variance problem is Advantage Actor-critic (A2C). Different from traditional
    actor-critic methods, A2C uses an advantage function to replace the Q-function
    inside the critic network. The advantage function $A(s_{t})$ is defined as the
    expected value of the TD-error. The new objective function for policy gradient
    can be written as,
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '| (38) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}[\sum_{t=1}^{T}\underbrace{(Q(s_{t},a_{t})-V(s_{t}))}_{A(s_{t})}\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})].$
    |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: However, A2C still uses DDPG as the main training algorithm, which may suffer
    function approximation errors when estimating the Q value. Twin-Delayed DDPG (TD3) (Fujimoto
    et al., [2018](#bib.bib28)) is designed to improve the function approximation
    problem in DDPG which uses clipped double Q-learning to update the critic. The
    gradient update can be expressed as,
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '| (39) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}[\sum_{t=1}^{T}r(s_{t},a_{t})+\gamma\min(Q_{1}(s_{t},a_{t}+\epsilon),Q_{2}(s_{t},a_{t}+\epsilon))\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})].$
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: where $\epsilon\sim\textit{clip}(\mathcal{N}(0,\sigma,-c,c))$, $\sigma$ is the
    standard deviation and $c$ is a constant for clipping.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Another two ways to improve actor-critic methods are Trust Region Policy Optimization
    (TRPO) (Schulman et al., [2015](#bib.bib80)) and Proximal Policy Optimization
    (PPO) (Schulman et al., [2017](#bib.bib81)), which focus on modification of the
    advantage function. TRPO aims to limit the step size for each gradient to ensure
    it will not change too much. The core idea is to add a constraint to the advantage
    function,
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '| (40) |  | $\displaystyle\frac{\pi(a&#124;s)}{\pi_{old}(a&#124;s)}A(s),$ |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: where the KL divergence will be used to measure the distance between the current
    policy and the old policy is small enough. PPO has the same goal as TRPO which
    is to try to find the biggest possible improvement step on a policy using the
    current data. PPO is a simplified version of TRPO which introduces the clip operation,
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '| (41) |  | $\displaystyle\min\bigg{(}\frac{\pi(a&#124;s)}{\pi_{old}(a&#124;s)}A(s),\text{clip}\bigg{(}\frac{\pi(a&#124;s)}{\pi_{old}(a&#124;s)}A(s),1-\epsilon,1+\epsilon\bigg{)}A(s)\bigg{)}.$
    |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: Soft Actor-Critic (SAC) (Haarnoja et al., [2018](#bib.bib36)) is another promising
    variant of the actor-critic algorithm and is widely used in DRL research. SAC
    uses the entropy term to encourage the agent to explore, which could be a possible
    direction to solve the exploration and exploitation dilemma. Moreover, SAC assigns
    an equal probability to actions that are equally attractive to the agent to capture
    those near-optimal policies. An example of related work (He et al., [2020](#bib.bib37))
    uses SAC to improve the stability of the training process in RS.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we provide a comprehensive overview the use of deep reinforcement
    learning in recommender systems. We introduce a classification scheme for existing
    studies and discuss them by category. We also provide an overview of such existing
    emerging topics and point out a few promising directions. We hope this survey
    can provide a systematic understanding of the key concepts in DRL-based RS and
    valuable insights for future research.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. (2017) Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
    2017. Constrained policy optimization. In *International Conference on Machine
    Learning*. PMLR, 22–31.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Afsar et al. (2021) M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2021. Reinforcement
    learning based recommender systems: A survey. *arXiv preprint arXiv:2101.06286*
    (2021).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arulkumaran et al. (2017) Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage,
    and Anil Anthony Bharath. 2017. Deep reinforcement learning: A brief survey. *IEEE
    Signal Processing Magazine* 34, 6 (2017), 26–38.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2019) Xueying Bai, Jian Guan, and Hongning Wang. 2019. A Model-Based
    Reinforcement Learning with Adversarial Training for Online Recommendation. *Advances
    in Neural Information Processing Systems* 32 (2019), 10735–10746.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bica et al. (2020) Ioana Bica, Daniel Jarrett, Alihan Hüyük, and Mihaela van der
    Schaar. 2020. Learning” What-if” Explanations for Sequential Decision-Making.
    In *International Conference on Learning Representations*.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bordes et al. (2013) Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
    Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling
    multi-relational data. In *Neural Information Processing Systems (NIPS)*. 1–9.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bottou et al. (2013) Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela,
    Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard,
    and Ed Snelson. 2013. Counterfactual Reasoning and Learning Systems: The Example
    of Computational Advertising. *Journal of Machine Learning Research* 14, 11 (2013).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brockman et al. (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
    Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. Openai gym. *arXiv
    preprint arXiv:1606.01540* (2016).
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2018) Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, and Yiwei
    Zhang. 2018. Reinforcement Mechanism Design for e-commerce. In *Proceedings of
    the 2018 World Wide Web Conference*. 1339–1348.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2020) Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang, and
    Wei Emma Zhang. 2020. Adversarial Attacks and Detection on Reinforcement Learning-Based
    Interactive Recommender Systems. In *Proceedings of the 43rd International ACM
    SIGIR Conference on Research and Development in Information Retrieval*. 1669–1672.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019b) Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang,
    Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019b. Large-scale interactive recommendation
    with tree-structured policy gradient. In *Proceedings of the AAAI Conference on
    Artificial Intelligence*, Vol. 33. 3312–3320.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020a) Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang,
    and Xiangnan He. 2020a. Bias and Debias in Recommender System: A Survey and Future
    Directions. arXiv:arXiv:2010.03240'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019a) Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois
    Belletti, and Ed H Chi. 2019a. Top-k off-policy correction for a REINFORCE recommender
    system. In *Proceedings of the Twelfth ACM International Conference on Web Search
    and Data Mining*. 456–464.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Minmin Chen, Bo Chang, Can Xu, and Ed H Chi. 2021. User Response
    Models to Improve a REINFORCE Recommender System. In *Proceedings of the 14th
    ACM International Conference on Web Search and Data Mining*. 121–129.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang,
    and Hai-Hong Tang. 2018. Stabilizing reinforcement learning in dynamic environment
    with application to online recommendation. In *Proceedings of the 24th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 1187–1196.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019d) Tong Chen, Jiqiang Liu, Yingxiao Xiang, Wenjia Niu, Endong
    Tong, and Zhen Han. 2019d. Adversarial attack and defense in reinforcement learning-from
    AI security view. *Cybersecurity* 2, 1 (2019), 1–22.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020b) Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wenjie
    Zhang, et al. 2020b. Knowledge-guided deep reinforcement learning for interactive
    recommendation. In *2020 International Joint Conference on Neural Networks (IJCNN)*.
    IEEE, 1–8.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019c) Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi,
    and Le Song. 2019c. Generative adversarial user model for reinforcement learning
    based recommendation system. In *International Conference on Machine Learning*.
    PMLR, 1052–1061.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020c) Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei
    Xu, and Liming Zhu. 2020c. Generative Inverse Deep Reinforcement Learning for
    Online Recommendation. *arXiv preprint arXiv:2011.02248* (2020).
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasgupta et al. (2019) Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic,
    Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick,
    and Zeb Kurth-Nelson. 2019. Causal reasoning from meta-reinforcement learning.
    *arXiv preprint arXiv:1901.08162* (2019).
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Degris et al. (2012) Thomas Degris, Martha White, and Richard S Sutton. 2012.
    Off-policy actor-critic. *arXiv preprint arXiv:1205.4839* (2012).
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deldjoo et al. (2021) Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra.
    2021. A survey on adversarial recommender systems: from attack/defense strategies
    to generative adversarial networks. *ACM Computing Surveys (CSUR)* 54, 2 (2021),
    1–38.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Egorov (2016) Maxim Egorov. 2016. Multi-agent deep reinforcement learning.
    *CS231n: convolutional neural networks for visual recognition* (2016), 1–8.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2018) Jun Feng, Heng Li, Minlie Huang, Shichen Liu, Wenwu Ou,
    Zhirong Wang, and Xiaoyan Zhu. 2018. Learning to collaborate: Multi-scenario ranking
    via multi-agent reinforcement learning. In *Proceedings of the 2018 World Wide
    Web Conference*. 1939–1948.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forney et al. (2017) Andrew Forney, Judea Pearl, and Elias Bareinboim. 2017.
    Counterfactual data-fusion for online reinforcement learners. In *International
    Conference on Machine Learning*. PMLR, 1156–1164.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Friedman and Nissenbaum (1996) Batya Friedman and Helen Nissenbaum. 1996. Bias
    in computer systems. *ACM Transactions on Information Systems (TOIS)* 14, 3 (1996),
    330–347.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujimoto et al. (2018) Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing
    function approximation error in actor-critic methods. In *International Conference
    on Machine Learning*. PMLR, 1587–1596.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2019) Rong Gao, Haifeng Xia, Jing Li, Donghua Liu, Shuai Chen,
    and Gang Chun. 2019. DRCGR: Deep reinforcement learning framework incorporating
    CNN and GAN-based for interactive recommendation. In *2019 IEEE International
    Conference on Data Mining (ICDM)*. IEEE, 1048–1053.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gasse et al. (2021) Maxime Gasse, Damien Grasset, Guillaume Gaudron, and Pierre-Yves
    Oudeyer. 2021. Causal Reinforcement Learning using Observational and Interventional
    Data. arXiv:2106.14421 [cs.LG]
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gleave et al. (2020) Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey
    Levine, and Stuart Russell. 2020. Adversarial Policies: Attacking Deep Reinforcement
    Learning. In *International Conference on Learning Representations*. [https://openreview.net/forum?id=HJgEMpVFwB](https://openreview.net/forum?id=HJgEMpVFwB)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2019) Yu Gong, Yu Zhu, Lu Duan, Qingwen Liu, Ziyu Guan, Fei Sun,
    Wenwu Ou, and Kenny Q Zhu. 2019. Exact-k recommendation via maximal clique optimization.
    In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*. 617–626.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
    2014. Generative adversarial networks. *arXiv preprint arXiv:1406.2661* (2014).
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gui et al. (2019) Tao Gui, Peng Liu, Qi Zhang, Liang Zhu, Minlong Peng, Yunhua
    Zhou, and Xuanjing Huang. 2019. Mention recommendation in Twitter with cooperative
    multi-agent reinforcement learning. In *Proceedings of the 42nd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 535–544.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Ruocheng Guo, Lu Cheng, Jundong Li, P. Richard Hahn, and
    Huan Liu. 2020. A Survey of Learning Causality with Data: Problems and Methods.
    *ACM Comput. Surv.* 53, 4, Article 75 (July 2020), 37 pages. [https://doi.org/10.1145/3397269](https://doi.org/10.1145/3397269)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
    Levine. 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement
    learning with a stochastic actor. In *International conference on machine learning*.
    PMLR, 1861–1870.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) Xu He, Bo An, Yanghua Li, Haikai Chen, Rundong Wang, Xinrun
    Wang, Runsheng Yu, Xin Li, and Zhirong Wang. 2020. Learning to Collaborate in
    Multi-Module Recommendation via Multi-Agent Reinforcement Learning without Communication.
    In *Fourteenth ACM Conference on Recommender Systems*. 210–219.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho and Ermon (2016) Jonathan Ho and Stefano Ermon. 2016. Generative adversarial
    imitation learning. *arXiv preprint arXiv:1606.03476* (2016).
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. (2020) Daocheng Hong, Yang Li, and Qiwen Dong. 2020. Nonintrusive-Sensing
    and Reinforcement-Learning Based Adaptive Personalized Music Recommendation. In
    *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development
    in Information Retrieval*. 1721–1724.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2018) Yujing Hu, Qing Da, Anxiang Zeng, Yang Yu, and Yinghui Xu.
    2018. Reinforcement learning to rank in e-commerce search engine: Formalization,
    analysis, and application. In *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 368–377.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2020) Jin Huang, Harrie Oosterhuis, Maarten de Rijke, and Herke
    van Hoof. 2020. Keeping Dataset Biases out of the Simulation: A Debiased Simulator
    for Reinforcement Learning based Recommender Systems. In *Fourteenth ACM Conference
    on Recommender Systems*. 190–199.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ie et al. (2019a) Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit
    Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019a. Recsim: A configurable
    simulation platform for recommender systems. *arXiv preprint arXiv:1909.04847*
    (2019).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ie et al. (2019b) Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh
    Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019b. SlateQ:
    A Tractable Decomposition for Reinforcement Learning with Recommendation Sets.
    In *Proceedings of the Twenty-eighth International Joint Conference on Artificial
    Intelligence (IJCAI-19)*. Macau, China, 2592–2599.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isele and Cosgun (2018) David Isele and Akansel Cosgun. 2018. Selective experience
    replay for lifelong learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 32.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2016) Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki,
    Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. 2016. Reinforcement
    learning with unsupervised auxiliary tasks. *arXiv preprint arXiv:1611.05397*
    (2016).
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2020) Shenggong Ji, Zhaoyuan Wang, Tianrui Li, and Yu Zheng. 2020.
    Spatio-temporal feature fusion for dynamic taxi route recommendation via deep
    reinforcement learning. *Knowledge-Based Systems* 205 (2020), 106302.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2020) Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu.
    2020. Graph Convolutional Reinforcement Learning. In *International Conference
    on Learning Representations*. [https://openreview.net/forum?id=HkxdQkSYDB](https://openreview.net/forum?id=HkxdQkSYDB)
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kahn et al. (2018) Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel,
    and Sergey Levine. 2018. Self-supervised deep reinforcement learning with generalized
    computation graphs for robot navigation. In *2018 IEEE International Conference
    on Robotics and Automation (ICRA)*. IEEE, 5129–5136.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. In *International Conference
    on Learning Representations (ICLR)*.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Konda and Tsitsiklis (2000) Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic
    algorithms. In *Advances in neural information processing systems*. Citeseer,
    1008–1014.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kostrikov et al. (2019) Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi,
    Sergey Levine, and Jonathan Tompson. 2019. Discriminator-Actor-Critic: Addressing
    Sample Inefficiency and Reward Bias in Adversarial Imitation Learning. In *International
    Conference on Learning Representations*. [https://openreview.net/forum?id=Hk4fpoA5Km](https://openreview.net/forum?id=Hk4fpoA5Km)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kulkarni et al. (2016) Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi,
    and Josh Tenenbaum. 2016. Hierarchical deep reinforcement learning: Integrating
    temporal abstraction and intrinsic motivation. *Advances in neural information
    processing systems* 29 (2016), 3675–3683.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei and Li (2019) Yu Lei and Wenjie Li. 2019. Interactive recommendation with
    user-specific deep reinforcement learning. *ACM Transactions on Knowledge Discovery
    from Data (TKDD)* 13, 6 (2019), 1–15.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2020) Yu Lei, Hongbin Pei, Hanqi Yan, and Wenjie Li. 2020. Reinforcement
    learning based recommendation with graph convolutional q-network. In *Proceedings
    of the 43rd International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 1757–1760.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2019) Yu Lei, Zhitao Wang, Wenjie Li, and Hongbin Pei. 2019. Social
    Attentive Deep Q-network for Recommendation. In *Proceedings of the 42nd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 1189–1192.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin
    Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on
    open problems. *arXiv preprint arXiv:2005.01643* (2020).'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillicrap et al. (2015) Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous
    control with deep reinforcement learning. *arXiv preprint arXiv:1509.02971* (2015).
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2017) Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih,
    Ming-Yu Liu, and Min Sun. 2017. Tactics of adversarial attack on deep reinforcement
    learning agents. *arXiv preprint arXiv:1703.06748* (2017).
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Feng Liu, Huifeng Guo, Xutao Li, Ruiming Tang, Yunming Ye,
    and Xiuqiang He. 2020a. End-to-end deep reinforcement learning based recommendation
    with supervised embedding. In *Proceedings of the 13th International Conference
    on Web Search and Data Mining*. 384–392.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Feng Liu, Ruiming Tang, Huifeng Guo, Xutao Li, Yunming Ye,
    and Xiuqiang He. 2020b. Top-aware reinforcement learning based recommendation.
    *Neurocomputing* 417 (2020), 255–269.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye,
    Haokun Chen, Huifeng Guo, and Yuzhou Zhang. 2018. Deep reinforcement learning
    based recommendation with explicit user-item interactions modeling. *arXiv preprint
    arXiv:1810.12027* (2018).
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020c) Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye,
    Haokun Chen, Huifeng Guo, Yuzhou Zhang, and Xiuqiang He. 2020c. State representation
    modeling for deep reinforcement learning based recommendation. *Knowledge-Based
    Systems* 205 (2020), 106170.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2015) Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan
    Zhang. 2015. Recommender system application developments: a survey. *Decision
    Support Systems* 74 (2015), 12–32.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lütjens et al. (2020) Björn Lütjens, Michael Everett, and Jonathan P How. 2020.
    Certified adversarial robustness for deep reinforcement learning. In *Conference
    on Robot Learning*. PMLR, 1328–1337.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020) Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi
    Tang, Lichan Hong, and Ed H Chi. 2020. Off-policy learning in two-stage recommender
    systems. In *Proceedings of The Web Conference 2020*. 463–473.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madumal et al. (2020) Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank
    Vetere. 2020. Explainable reinforcement learning through a causal lens. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 34. 2493–2500.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makar et al. (2001) Rajbala Makar, Sridhar Mahadevan, and Mohammad Ghavamzadeh.
    2001. Hierarchical multi-agent reinforcement learning. In *Proceedings of the
    fifth international conference on Autonomous agents*. 246–253.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
    Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement
    learning. *nature* 518, 7540 (2015), 529–533.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montazeralghaem et al. (2020) Ali Montazeralghaem, Hamed Zamani, and James Allan.
    2020. A Reinforcement Learning Framework for Relevance Feedback. In *Proceedings
    of the 43rd International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 59–68.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng et al. (1999) Andrew Y Ng, Daishi Harada, and Stuart Russell. 1999. Policy
    invariance under reward transformations: Theory and application to reward shaping.
    In *Icml*, Vol. 99. 278–287.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ng et al. (2000) Andrew Y Ng, Stuart J Russell, et al. 2000. Algorithms for
    inverse reinforcement learning.. In *Icml*, Vol. 1. 2.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oyeleke et al. (2018) Richard O Oyeleke, Chen-Yeou Yu, and Carl K Chang. 2018.
    Situ-centric reinforcement learning for recommendation of tasks in activities
    of daily living in smart homes. In *2018 IEEE 42nd Annual Computer Software and
    Applications Conference (COMPSAC)*, Vol. 2\. IEEE, 317–322.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2019) Feiyang Pan, Qingpeng Cai, Pingzhong Tang, Fuzhen Zhuang,
    and Qing He. 2019. Policy gradients for contextual recommendations. In *The World
    Wide Web Conference*. 1421–1431.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pei et al. (2019) Changhua Pei, Xinru Yang, Qing Cui, Xiao Lin, Fei Sun, Peng
    Jiang, Wenwu Ou, and Yongfeng Zhang. 2019. Value-aware recommendation based on
    reinforcement profit maximization. In *The World Wide Web Conference*. 3123–3129.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rao et al. (2020) Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian
    Ibarz, and Mohi Khansari. 2020. Rl-cyclegan: Reinforcement learning aware simulation-to-real.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    11157–11166.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rohde et al. (2018) David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile,
    and Alexandros Karatzoglou. 2018. Recogym: A reinforcement learning environment
    for the problem of product recommendation in online advertising. *arXiv preprint
    arXiv:1808.00720* (2018).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Santana et al. (2020) Marlesson RO Santana, Luckeciano C Melo, Fernando HF
    Camargo, Bruno Brandão, Anderson Soares, Renan M Oliveira, and Sandor Caetano.
    2020. MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems
    for Marketplaces. *arXiv preprint arXiv:2010.07035* (2020).'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaul et al. (2015) Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
    2015. Prioritized experience replay. *arXiv preprint arXiv:1511.05952* (2015).
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schnabel et al. (2016) Tobias Schnabel, Adith Swaminathan, Ashudeep Singh,
    Navin Chandak, and Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing
    Learning and Evaluation. arXiv:arXiv:1602.05352'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael
    Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In *International
    conference on machine learning*. PMLR, 1889–1897.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347* (2017).
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sermanet et al. (2018) Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine
    Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. 2018. Time-contrastive
    networks: Self-supervised learning from video. In *2018 IEEE International Conference
    on Robotics and Automation (ICRA)*. IEEE, 1134–1141.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shang et al. (2019) Wenjie Shang, Yang Yu, Qingyang Li, Zhiwei Qin, Yiping Meng,
    and Jieping Ye. 2019. Environment reconstruction with hidden confounders for reinforcement
    learning based recommendation. In *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 566–576.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2019a) Bichen Shi, Makbule Gulcin Ozsoy, Neil Hurley, Barry Smyth,
    Elias Z Tragos, James Geraci, and Aonghus Lawlor. 2019a. PyRecGym: a reinforcement
    learning gym for recommender systems. In *Proceedings of the 13th ACM Conference
    on Recommender Systems*. 491–495.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2019b) Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang
    Zeng. 2019b. Virtual-taobao: Virtualizing real-world online retail environment
    for reinforcement learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 33\. 4902–4909.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2020) Wenjie Shi, Gao Huang, Shiji Song, Zhuoyuan Wang, Tingyu Lin,
    and Cheng Wu. 2020. Self-Supervised Discovering of Interpretable Features for
    Reinforcement Learning. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
    (2020).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. (2014) David Silver, Guy Lever, Nicolas Heess, Thomas Degris,
    Daan Wierstra, and Martin Riedmiller. 2014. Deterministic policy gradient algorithms.
    In *International conference on machine learning*. PMLR, 387–395.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Ying Sun, Fuzhen Zhuang, Hengshu Zhu, Qing He, and Hui Xiong.
    2021. Cost-Effective and Interpretable Job Skill Recommendation with Deep Reinforcement
    Learning. In *Proceedings of the Web Conference 2021*. 3827–3838.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takanobu et al. (2019) Ryuichi Takanobu, Tao Zhuang, Minlie Huang, Jun Feng,
    Haihong Tang, and Bo Zheng. 2019. Aggregating e-commerce search results from heterogeneous
    sources via hierarchical reinforcement learning. In *The World Wide Web Conference*.
    1771–1781.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang and Wang (2018) Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential
    recommendation via convolutional sequence embedding. In *Proceedings of the Eleventh
    ACM International Conference on Web Search and Data Mining*. 565–573.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tavakoli et al. (2018) Arash Tavakoli, Fabio Pardo, and Petar Kormushev. 2018.
    Action branching architectures for deep reinforcement learning. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 32.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Hasselt et al. (2016) Hado Van Hasselt, Arthur Guez, and David Silver. 2016.
    Deep reinforcement learning with double q-learning. In *Proceedings of the AAAI
    Conference on Artificial Intelligence*, Vol. 30.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks.
    *arXiv preprint arXiv:1710.10903* (2017).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Feng Wang, Chen Zhong, M Cenk Gursoy, and Senem Velipasalar.
    2020c. Defense strategies against adversarial jamming attacks via deep reinforcement
    learning. In *2020 54th annual conference on information sciences and systems
    (CISS)*. IEEE, 1–6.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo.
    2019. Knowledge graph convolutional networks for recommender systems. In *The
    world wide web conference*. 3307–3313.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2016) Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer,
    Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.
    2016. Learning to reinforcement learn. *arXiv preprint arXiv:1611.05763* (2016).
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Kai Wang, Zhene Zou, Qilin Deng, Runze Wu, Jianrong Tao,
    Changjie Fan, Liang Chen, and Peng Cui. 2021. Reinforcement Learning with a Disentangled
    Universal Value Function for Item Recommendation. In *Proceedings of the AAAI
    conference on artificial intelligence*, Vol. 35\. 4427–4435.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018b) Lu Wang, Wei Zhang, Xiaofeng He, and Hongyuan Zha. 2018b.
    Supervised reinforcement learning with recurrent neural network for dynamic treatment
    recommendation. In *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. 2447–2456.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Pengfei Wang, Yu Fan, Long Xia, Wayne Xin Zhao, Shaozhang
    Niu, and Jimmy Huang. 2020a. KERL: A knowledge-guided reinforcement learning model
    for sequential recommendation. In *Proceedings of the 43rd International ACM SIGIR
    Conference on Research and Development in Information Retrieval*. 209–218.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018a) Xiting Wang, Yiru Chen, Jie Yang, Le Wu, Zhengtao Wu, and
    Xing Xie. 2018a. A reinforcement learning framework for explainable recommendation.
    In *2018 IEEE international conference on data mining (ICDM)*. IEEE, 587–596.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Xiang Wang, Yaokun Xu, Xiangnan He, Yixin Cao, Meng Wang,
    and Tat-Seng Chua. 2020b. Reinforced negative sampling over knowledge graph for
    recommendation. In *Proceedings of The Web Conference 2020*. 99–109.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watkins and Dayan (1992) Christopher JCH Watkins and Peter Dayan. 1992. Q-learning.
    *Machine learning* 8, 3-4 (1992), 279–292.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (1992) Ronald J Williams. 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. *Machine learning* 8, 3-4
    (1992), 229–256.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xian et al. (2019) Yikun Xian, Zuohui Fu, S Muthukrishnan, Gerard De Melo, and
    Yongfeng Zhang. 2019. Reinforcement knowledge graph reasoning for explainable
    recommendation. In *Proceedings of the 42nd international ACM SIGIR conference
    on research and development in information retrieval*. 285–294.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2020) Yilin Xiao, Liang Xiao, Xiaozhen Lu, Hailu Zhang, Shui Yu,
    and H Vincent Poor. 2020. Deep reinforcement learning based user profile perturbation
    for privacy aware recommendation. *IEEE Internet of Things Journal* (2020).
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2021) Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu
    Lin. 2021. Hierarchical Reinforcement Learning for Integrated Recommendation.
    In *Proceedings of AAAI*.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xin et al. (2020) Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M
    Jose. 2020. Self-Supervised Reinforcement Learning for Recommender Systems. In
    *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development
    in Information Retrieval*. 931–940.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Jun Xu, Zeng Wei, Long Xia, Yanyan Lan, Dawei Yin, Xueqi Cheng,
    and Ji-Rong Wen. 2020. Reinforcement Learning to Rank with Pairwise Policy Gradient.
    In *Proceedings of the 43rd International ACM SIGIR Conference on Research and
    Development in Information Retrieval*. 509–518.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021) Chao-Han Huck Yang, I Hung, Te Danny, Yi Ouyang, and Pin-Yu
    Chen. 2021. Causal Inference Q-Network: Toward Resilient Reinforcement Learning.
    *arXiv preprint arXiv:2102.09677* (2021).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2018) Zhaoyang Yang, Kathryn Merrick, Lianwen Jin, and Hussein A
    Abbass. 2018. Hierarchical deep reinforcement learning for continuous action control.
    *IEEE transactions on neural networks and learning systems* 29, 11 (2018), 5174–5184.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019) Tong Yu, Yilin Shen, Ruiyi Zhang, Xiangyu Zeng, and Hongxia
    Jin. 2019. Vision-language recommendation via attribute augmented multimodal reinforcement
    learning. In *Proceedings of the 27th ACM International Conference on Multimedia*.
    39–47.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yue and Joachims (2009) Yisong Yue and Thorsten Joachims. 2009. Interactively
    optimizing information retrieval systems as a dueling bandits problem. In *Proceedings
    of the 26th Annual International Conference on Machine Learning*. 1201–1208.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2018) Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto
    Rodriguez, and Thomas Funkhouser. 2018. Learning synergies between pushing and
    grasping with self-supervised deep reinforcement learning. In *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*. IEEE, 4238–4245.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) Jing Zhang, Bowen Hao, Bo Chen, Cuiping Li, Hong Chen,
    and Jimeng Sun. 2019a. Hierarchical reinforcement learning for course recommendation
    in MOOCs. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 33. 435–442.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019d) Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, and Changyou
    Chen. 2019d. Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement
    Learning. In *Advances in Neural Information Processing Systems*, H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.),
    Vol. 32\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2019/file/52130c418d4f02c74f74a5bc1f8020b2-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/52130c418d4f02c74f74a5bc1f8020b2-Paper.pdf)
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019b) Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019b. Deep
    learning based recommender system: A survey and new perspectives. *ACM Computing
    Surveys (CSUR)* 52, 1 (2019), 1–38.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019c) Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019c. Deep
    Learning Based Recommender System: A Survey and New Perspectives. *ACM Comput.
    Surv.* 52, 1, Article 5 (Feb. 2019), 38 pages. [https://doi.org/10.1145/3285029](https://doi.org/10.1145/3285029)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Weijia Zhang, Hao Liu, Fan Wang, Tong Xu, Haoran Xin, Dejing
    Dou, and Hui Xiong. 2021. Intelligent Electric Vehicle Charging Recommendation
    Based on Multi-Agent Reinforcement Learning. In *Proceedings of the Web Conference
    2021*. 1856–1867.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Chen (2020) Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation:
    A Survey and New Perspectives. *Foundations and Trends® in Information Retrieval*
    14, 1 (2020), 1–101. [https://doi.org/10.1561/1500000066](https://doi.org/10.1561/1500000066)'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Yang Zhang, Chenwei Zhang, and Xiaozhong Liu. 2017. Dynamic
    scholarly collaborator recommendation via competitive multi-agent reinforcement
    learning. In *Proceedings of the Eleventh ACM Conference on Recommender Systems*.
    331–335.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020d) Dongyang Zhao, Liang Zhang, Bo Zhang, Lizhou Zheng, Yongjun
    Bao, and Weipeng Yan. 2020d. MaHRL: Multi-goals Abstraction Based Deep Hierarchical
    Reinforcement Learning for Recommendations. In *Proceedings of the 43rd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 871–880.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020b) Kangzhi Zhao, Xiting Wang, Yuren Zhang, Li Zhao, Zheng Liu,
    Chunxiao Xing, and Xing Xie. 2020b. Leveraging Demonstrations for Reinforcement
    Recommendation Reasoning over Knowledge Graphs. In *Proceedings of the 43rd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 239–248.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020a) Wenshuai Zhao, Jorge Peña Queralta, and Tomi Westerlund.
    2020a. Sim-to-real transfer in deep reinforcement learning for robotics: a survey.
    In *2020 IEEE Symposium Series on Computational Intelligence (SSCI)*. IEEE, 737–744.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang,
    Xiaobing Liu, Hui Liu, and Jiliang Tang. 2021. DEAR: Deep Reinforcement Learning
    for Online Advertising Impression in Recommender Systems. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, Vol. 35\. 750–758.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018a) Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei
    Yin, and Jiliang Tang. 2018a. Deep reinforcement learning for page-wise recommendations.
    In *Proceedings of the 12th ACM Conference on Recommender Systems*. 95–103.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020c) Xiangyu Zhao, Long Xia, Lixin Zou, Hui Liu, Dawei Yin, and
    Jiliang Tang. 2020c. Whole-Chain Recommendations. In *Proceedings of the 29th
    ACM International Conference on Information & Knowledge Management*. 1883–1891.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019a) Xiangyu Zhao, Long Xia, Lixin Zou, Dawei Yin, and Jiliang
    Tang. 2019a. Toward simulating environments in reinforcement learning based recommendations.
    *arXiv preprint arXiv:1906.11462* (2019).
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018b) Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang
    Tang, and Dawei Yin. 2018b. Recommendations with negative feedback via pairwise
    deep reinforcement learning. In *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 1040–1048.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019b) Xiangyu Zhao, Liang Zhang, Long Xia, Zhuoye Ding, Dawei
    Yin, and Jiliang Tang. 2019b. Deep reinforcement learning for list-wise recommendations.
    *DRL4KDD ’19* (2019).
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020e) Xiangyu Zhao, Xudong Zheng, Xiwang Yang, Xiaobing Liu, and
    Jiliang Tang. 2020e. Jointly learning to recommend and advertise. In *Proceedings
    of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. 3319–3327.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2018) Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang,
    Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement
    learning framework for news recommendation. In *Proceedings of the 2018 World
    Wide Web Conference*. 167–176.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020) Sijin Zhou, Xinyi Dai, Haokun Chen, Weinan Zhang, Kan Ren,
    Ruiming Tang, Xiuqiang He, and Yong Yu. 2020. Interactive recommender system via
    knowledge graph-enhanced reinforcement learning. In *Proceedings of the 43rd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 179–188.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *Proceedings of the IEEE international conference on computer vision*. 2223–2232.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2019) Shengyu Zhu, Ignavier Ng, and Zhitang Chen. 2019. Causal discovery
    with reinforcement learning. *arXiv preprint arXiv:1906.04477* (2019).
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2019) Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu,
    and Dawei Yin. 2019. Reinforcement learning to optimize long-term user engagement
    in recommender systems. In *Proceedings of the 25th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. 2810–2818.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. (2020) Lixin Zou, Long Xia, Pan Du, Zhuo Zhang, Ting Bai, Weidong
    Liu, Jian-Yun Nie, and Dawei Yin. 2020. Pseudo Dyna-Q: A reinforcement learning
    framework for interactive recommendation. In *Proceedings of the 13th International
    Conference on Web Search and Data Mining*. 816–824.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
