- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:51:37'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2109.03540] A Survey of Deep Reinforcement Learning in Recommender Systems:
    A Systematic Review and Future Directions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2109.03540](https://ar5iv.labs.arxiv.org/html/2109.03540)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic
    Review and Future Directions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xiaocong Chen [xiaocong.chen@unsw.edu.au](mailto:xiaocong.chen@unsw.edu.au)
    University of New South WalesSydneyNSWAustralia ,  Lina Yao [lina.yao@unsw.edu.au](mailto:lina.yao@unsw.edu.au)
    University of New South WalesSydneyNSWAustralia ,  Julian Mcauley [jmcauley@eng.ucsd.edu](mailto:jmcauley@eng.ucsd.edu)
    University of California, San DiegoCAUSA ,  Guanglin Zhou [guanglin.zhou@unsw.edu.au](mailto:guanglin.zhou@unsw.edu.au)
    University of New South WalesSydneyNSWAustralia  and  Xianzhi Wang [xianzhi.wang@uts.edu.au](mailto:xianzhi.wang@uts.edu.au)
    University of Technology SydneySydneyNSWAustralia(2021)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In light of the emergence of deep reinforcement learning (DRL) in recommender
    systems research and several fruitful results in recent years, this survey aims
    to provide a timely and comprehensive overview of the recent trends of deep reinforcement
    learning in recommender systems. We start with the motivation of applying DRL
    in recommender systems. Then, we provide a taxonomy of current DRL-based recommender
    systems and a summary of existing methods. We discuss emerging topics and open
    issues, and provide our perspective on advancing the domain. This survey serves
    as introductory material for readers from academia and industry into the topic
    and identifies notable opportunities for further research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Reinforcement Learning, Deep Learning, recommender systems^†^†copyright:
    acmcopyright^†^†journalyear: 2021^†^†doi: 10.1145/1122445.1122456^†^†journal:
    JACM^†^†journalvolume: 37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth:
    9^†^†ccs: Information systems Recommender systems^†^†ccs: Computing methodologies Reinforcement
    learning^†^†ccs: Computing methodologies Neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent years have seen significant progress in recommendation techniques, from
    traditional recommendation techniques, e.g., collaborative filtering, content-based
    recommendation and matrix factorization (Lu et al., [2015](#bib.bib63)), to deep
    learning based techniques. In particular, deep learning show strong advantages
    in solving complex tasks and dealing with complex data, due to its capability
    to capture non-linear user-item relationships and deal with various types of data
    sources such as images and text. It has thus been increasingly used in recommender
    systems. Deep learning-based recommender systems have limitations in capturing
    interest dynamics (Chen et al., [2020b](#bib.bib18); Zhang et al., [2019b](#bib.bib116))
    due to distribution shift, i.e., the training phase is based on an existing dataset
    which may not reflect real user preferences that undergo rapid change. In contrast,
    deep reinforcement learning (DRL) aims to train an agent that can learn from interaction
    trajectories provided by the environment by combining the power of deep learning
    and reinforcement learning. Since an agent in DRL can actively learn from users’
    real-time feedback to infer dynamic user preferences, DRL is especially suitable
    for learning from interactions, such as human-robot collaboration; it has also
    driven significant advances in a range of interactive applications ranging from
    video games, Alpha Go to autonomous driving (Arulkumaran et al., [2017](#bib.bib4)).
    In light of the significance and recent progresses in DRL for recommender sytsems,
    we aim to timely summaize and comment on DRL-based recommendation systems in this
    survey.
  prefs: []
  type: TYPE_NORMAL
- en: A recent survey on reinforcement learning based recommender systems (Afsar et al.,
    [2021](#bib.bib3)) provides a general review about reinforcement learning in recommender
    systems without a sophsiticated investigation of the growing area of deep reinforcement
    learning. Our survey distinguishes itself in providing a systematic and comprehensive
    overview of existing methods in DRL-based recommender systems, along with a discussion
    of emerging topics, open issues, and future directions. This survey introduces
    researchers, practitioners and educators into this topic and fostering an understanding
    of the key techniques in the area.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main contributions of this survey include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide an up-to-date comprehensive review of deep reinforcement learning
    in recommender systems, with state of the art techniques and pointers to core
    references. To the best of our knowledge, this the first comprehensive survey
    in deep reinforcement learning based recommender systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present a taxonomy of the literature of deep reinforcement learning in recommender
    systems. Along with the outlined taxonomy and literature overview, we discuss
    the benefits, drawbacks and give suggestions for future research directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We shed light on emerging topics and open issues for DRL-based recommender systems.
    We also point out future directions that could be crucial for advancing DRL-based
    recommender systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of this survey is organized as follows: Section 2 provides an
    overview of recommender systems, DRL and their integration. Section 3 provides
    a literature review with a taxonomy and classification mechanism. Section 4 reviews
    emerging topics, and Section 5 points out open questions. Finally, Section 6 provides
    a few promising future directions for further advances in this domain.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/38e5bc0de6f7934538a6bd40e290e058.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. Taxonomy of Deep Reinforcement Learning based Recommender Systems
    in this survey
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section,we introduce key concepts related to dynamic recommender systems
    (RS) and deep reinforcement learning (DRL), and motivate the introduction of DRL
    to dynamic recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Why Deep Reinforcement Learning for Recommendation?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recommender systems require coping with *dynamic* environments by estimating
    rapidly changing users’ preferences and proactively recommending items to users.
    Let $\mathcal{U}$ be a set of users of cardinality $|\mathcal{U}|$ and $\mathcal{I}$
    be a set of items of cardinality $|\mathcal{I}|$. For each user $u\in\mathcal{U}$,
    we observe a sequence of user actions $\mathbb{X}^{u}=[x_{1}^{u},x_{2}^{u},\cdots,x_{T_{u}}^{u}]$
    with item $x_{t}^{u}\in\mathcal{I}$, i.e., each event in a user sequence comes
    from the item set. We refer to a user making a decision as an interaction with
    an item. Suppose the feedback (e.g., ratings or clicking behavior) provided by
    users is $\mathcal{F}$, then a dynamic recommender system maintains the corresponding
    recommendation policy $\pi^{u}_{t}$, which will be updated systematically based
    on the feedback $f^{u}_{i}\in\mathcal{F}$ received during the interaction for
    item $i\in\mathcal{I}$ at the timestamp $t$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The marriage of deep learning and reinforcement learning has fueled breakthroughs
    in recommender systems. DRL-based RS consists of a pipeline with three building
    blocks: environment construction, state representation and recommendation policy
    learning. Environment construction aims to build an environment based on a set
    of users’ historical behaviors. State representation is provided by the environment
    containing certain user information including historical behavior, demographic
    data (etc.). Recommendation policy learning is the key component to understand
    and predict users’ future behavior. DL-based RS receives user feedback (e.g.,
    ratings or clicks) to reflect users’ interests and update the recommender, while
    DRL-based RS receives the reward provided by the environment to update the policy.
    The reward provided by the environment is a pre-defined function containing several
    factors. The detailed process of DL based RS and DRL-based RS mapping can be found
    in [Figure 3](#S2.F3 "In 2.3\. DRL meets RS: Problem Formulation ‣ 2\. Background
    ‣ A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic
    Review and Future Directions").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Preliminaries of Deep Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The typical defining feature of DRL is to use the deep learning to approximate
    reinforcement learning’s value function and solve high-dimensional Markov Decision
    Processes (MDPs) (Arulkumaran et al., [2017](#bib.bib4)). Formally, a MDP can
    be represented as a tuple ($\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma$).
    The agent chooses an action $a_{t}\in\mathcal{A}$ according to the policy $\pi_{t}(s_{t})$
    at state $s_{t}\in\mathcal{S}$. The environment receives the action and produces
    a reward $r_{t+1}\in\mathcal{R}$ and transfers the reward into the next state
    $s_{t+1}$ according to the transition probability $P(s_{t+1}|s_{t},a_{t})\in\mathcal{P}$.
    The transition probability $\mathcal{P}$ is unknown beforehand in DRL. Such a
    process continues until the agent reaches the terminal state or exceeds a pre-defined
    maximum time step. The overall objective is to maximize the expected discounted
    cumulative reward,
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\displaystyle\mathbb{E}_{\pi}[r_{t}]=\mathbb{E}_{\pi}\big{[}\sum_{0}^{\infty}\gamma^{k}r_{t+k}\big{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\gamma\in[0,1]$ is the discount factor that balances the future reward
    and the immediate reward.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7a6b948b547739e72dda6a0233d5c3ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2. Taxonomy of Deep Reinforcement Learning in Recommender Systems
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep reinforcement learning can be divided into two categories: *model-based*
    and *model-free* methods (a detailed taxonomy can be found in [Figure 2](#S2.F2
    "In 2.2\. Preliminaries of Deep Reinforcement Learning ‣ 2\. Background ‣ A Survey
    of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and
    Future Directions")). The major *d*ifference between the two is whether the agent
    can learn a model of the environment. Model-based methods aim to estimate the
    transition function and reward function, while model-free methods aim to estimate
    the value function or policy from experience. In model-based methods, the agent
    accesses the environment and plans ahead while model-free methods gain sample
    efficiency from using models which are more extensively developed and tested than
    model-based methods in recent literature (Arulkumaran et al., [2017](#bib.bib4)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep reinforcement learning approaches are divided into three streams: *value-based*,
    *policy-based* and *hybrid* methods. In value-based methods, the agent updates
    the value function to learn a policy; policy-based methods learn the policy directly;
    and hybrid methods combine value-based and policy-based methods called *actor-critic*
    methods. Actor-critic contains two different networks where an actor network uses
    a policy-based method and the critic uses a value-based method to evaluate the
    policy learned by the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1. Notations
  prefs: []
  type: TYPE_NORMAL
- en: '| Notations | Name | Notations | Name | Notes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $Q(\cdot)$ | Q-Value Function | $s$ | State | users’ preference |'
  prefs: []
  type: TYPE_TB
- en: '| $V(\cdot)$ | Value Function | $a$ | Action | Recommended item(s) |'
  prefs: []
  type: TYPE_TB
- en: '| $\gamma$ | Discount Factor | $\pi$, $\mu(\cdot)$ | Policy | Recommendation
    policy |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathbb{E}$ | Expected Value | $r(\cdot,\cdot)$ | Reward | users’ click
    behavior |'
  prefs: []
  type: TYPE_TB
- en: '| $\theta$ | Model Parameter | $\alpha$ | constant $\in[0,1]$ | - |'
  prefs: []
  type: TYPE_TB
- en: '| $p(\cdot)$ | Transition Probability | $\tau$ | Sampled Trajectory | A tuple
    $(s_{t},a_{t},s^{\prime}_{t},r_{t})$ |'
  prefs: []
  type: TYPE_TB
- en: Deep reinforcement learning can be divided into *on-policy* and *off-policy*
    methods. In off-policy methods, the behavior policy $\pi_{b}$ is used for exploration
    while the target policy $\pi$ is used for decision-making. For on-policy methods,
    the behavior policy is the same as the target policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q-learning (Watkins and Dayan, [1992](#bib.bib102)) is an off-policy value-based
    learning scheme for finding a greedy target policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\displaystyle\pi(s)=\operatorname*{arg\,max}_{a}Q_{\pi}(s,a)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $Q_{u}(s,a)$ denotes the $Q$-value and is used in a small discrete action
    space. For a deterministic policy, the $Q$ value can be calculated as follows
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\displaystyle Q(s_{t},a_{t})=\mathbb{E}_{\tau\sim\pi}[r(s_{t},a_{t})+\gamma
    Q(s^{\prime}_{t},a^{\prime}_{t})].$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Deep Q learning (DQN) (Mnih et al., [2015](#bib.bib68)) uses deep learning
    to approximate a non-liner Q function parameterized by $\theta_{q}$: $Q_{\theta_{q}}(s,a)$.
    DQN designs a network $Q_{\theta_{q}}$ that is asynchronously updated by minimizing
    the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle\mathcal{L}(\theta_{q})=\mathbb{E}_{\tau\sim\pi}\Big{[}Q_{\theta_{q}}(s_{t},a_{t})-(r(s_{t},a_{t})+\gamma
    Q_{\theta_{q}}(s^{\prime}_{t},a^{\prime}_{t}))\Big{]}^{2}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$ is the sampled trajectory containing $(s,a,s^{\prime},r(s,a))$.
    In particular, $s^{\prime}_{t}$ and $a^{\prime}_{t}$ come from the behavior policy
    $\pi_{b}$ while $s,a$ comes from the target policy $\pi$. It is worth mentioning
    that the value function $V_{\pi}(s)$ represents the expected return. $V_{\pi}(s)$
    is used to evaluate the goodness of the state while $Q_{\pi}(s_{t},a_{t})$ is
    used to evaluate the action. $V_{\pi}(s)$ can be defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}_{\tau\sim\pi}\bigg{[}\sum_{t=0}^{T}\gamma^{t}r(s,a)&#124;s_{0}=s\bigg{]}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '$V_{\pi}(\cdot)$ and $Q_{\pi}(\cdot)$ have the following relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}_{a\sim\pi}[Q_{\pi}(s,a)].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The value function is updated using the following rule with the Temporal Difference
    (TD) method,
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\displaystyle V_{\pi}(s_{t})\leftarrow V_{\pi}(s_{t})+\alpha[\underbrace{r(s^{\prime}_{t},a^{\prime}_{t})+\gamma
    V_{\pi}(s^{\prime}_{t})-V_{\pi}(s_{t})}_{\text{TD-error}}]$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ is a constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy gradient (Williams, [1992](#bib.bib103)) is an on-policy policy-based
    method which can handle high-dimensional or continuous actions which cannot be
    easily handled by Q-learning. Policy gradient aims to find the parameter $\theta$
    of $\pi_{\theta}$ to maximize the accumulated reward. To this end, it maximizes
    the expected return from the start state:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\displaystyle J(\pi_{\theta})=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)]=\int\pi_{\theta}(\tau)r(\tau)d\tau$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\pi_{\theta}(\tau)$ is the probability of the occurrence of $\tau$.
    Policy gradient learns the parameter $\theta$ by the gradient $\nabla_{\theta}J(\pi_{\theta})$
    as defined below:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\theta}J(\pi_{\theta})=\int\pi_{\theta}(\tau)r(\tau)d\tau$
    | $\displaystyle=\int\pi_{\theta}(\tau)\nabla_{\theta}\log\pi_{\theta}(\tau)r(\tau)d\tau$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (9) |  |  | $\displaystyle=\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}[\sum_{t=1}^{T}r(s_{t},a_{t})\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The above derivations contain the following substitution,
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\displaystyle\pi_{\theta}(\tau)=p(s_{1})\prod_{t=1}^{T}\pi_{\theta}(s_{t},a_{t})p(s_{t+1}&#124;s_{t},a_{t})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $p(\cdot)$ are independent from the policy parameter $\theta$, which is
    omitted during the derivation. Monte-Carlo sampling has been used by previous
    policy gradient algorithm (e.g,. REINFORCE) for $\tau\sim d_{\pi_{\theta}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actor-critic networks combine the advantages from Q-learning and policy gradient.
    They can be either on-policy (Konda and Tsitsiklis, [2000](#bib.bib50)) or off-policy (Degris
    et al., [2012](#bib.bib22)). An actor-critic network consists of two components:
    i) an actor, which optimizes the policy $\pi_{\theta}$ under the guidance of $\nabla_{\theta}J(\pi_{\theta})$;
    and ii) a critic, which evaluates the learned policy $\pi_{\theta}$ by using $Q_{\theta_{q}}(s,a)$.
    The overall gradient is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $\displaystyle\mathbb{E}_{s\sim d_{\pi_{\theta}}}[Q_{\theta_{q}}(s,a)\nabla_{\theta}\log\pi_{\theta}(s,a)].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'When dealing with off-policy learning, the value function for $\pi_{\theta}(a|s)$
    can be further determined by deterministic policy gradient (DPG) as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $\displaystyle\mathbb{E}_{s\sim d_{\pi_{\theta}}}[\nabla_{a}Q_{\theta_{q}}(s,a)&#124;_{a=\pi_{\theta}(s)}\nabla_{\theta}\pi_{\theta}(s,a)].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'While traditional policy gradient calculates the integral for both the state
    space $\mathcal{S}$ and the action space $\mathcal{A}$, DPG only requires computing
    the integral to the state space $\mathcal{S}$. Given a state $s\in\mathcal{S}$,
    there will be only one corresponding action $a\in\mathcal{A}:\mu_{\theta}(s)=a$
    using DPG. Specifically, deep Deterministic Policy Gradients (DDPG) is an algorithm
    that combines techniques from DQN and DPG. DDPG contains four different neural
    networks: Q Network $Q$, policy network, target Q network $Q^{\mathit{tar}}$,
    and target policy network. It uses the target network for both the Q Network $Q$
    and policy network $\mu$ to ensure stability during training. Assume $\theta_{q},\theta_{\pi},\theta_{q^{\prime}}$
    and $\theta_{\pi^{\prime}}$ are parameters of the above networks; then DDPG soft-updates
    the parameters for the target network (Lillicrap et al., [2015](#bib.bib57)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $\displaystyle\text{Actor: }\theta_{\pi^{\prime}}\leftarrow\alpha\theta_{\pi}+(1-\alpha)\theta_{\pi^{\prime}}\text{
    Critic: }\theta_{q^{\prime}}\leftarrow\alpha\theta_{q}+(1-\alpha)\theta_{q^{\prime}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '2.3\. DRL meets RS: Problem Formulation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DRL is normally formulated as a Markov Decision Process (MDP). Given a set
    of users $\mathcal{U}=\{u,u_{1},u_{2},u_{3},...\}$, a set of items $\mathcal{I}=\{i,i_{1},i_{2},i_{3},...\}$,
    the system first recommends item $i$ to user $u$ and then gets feedback $f_{i}^{u}$.
    The system aims to incorporate the feedback to improve future recommendations
    and needs to determine an optimal policy $\pi^{*}$ regarding which item to recommend
    to the user to achieve positive feedback. The MDP modelling of the problem treats
    the user as the environment and the system as the agent. The key components of
    the MDP in DRL-based RS include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'State $\mathcal{S}$: A state $S_{t}\in\mathcal{S}$ is determined by both users’
    information and the recent $l$ items in which the user was interested before time
    $t$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action $\mathcal{A}$: An action $a_{t}\in\mathcal{A}$ represents users’ dynamic
    preference at time $t$ as predicted by the agent. $\mathcal{A}$ represents the
    whole set of (potentially millions of) candidate items.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transition Probability $\mathcal{P}$: The transition probability $p(s_{t+1}|s_{t},a_{t})$
    is defined as the probability of state transition from $s_{t}$ to $s_{t+1}$ when
    action $a_{t}$ is executed by the recommendation agent. In a recommender system,
    the transition probability refers to users’ behavior probability. $\mathcal{P}$
    is only used in model-based methods.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward $\mathcal{R}$: Once the agent chooses a suitable action $a_{t}$ based
    on the current state $S_{t}$ at time $t$, the user will receive the item recommended
    by the agent. Users’ feedback on the recommended item accounts for the reward
    $r(S_{t},a_{t})$. The feedback is used to improve the policy $\pi$ learned by
    the recommendation agent.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discount Factor $\gamma$: The discount factor $\gamma\in[0,1]$ is used to balance
    between future and immediate rewards—the agent focuses only on the immediate reward
    when $\gamma=0$ and takes into account all the (immediate and future) rewards
    otherwise.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The DRL-based recommendation problem can be defined by using MDP as follows.
    Given the historical MDP, i.e., $(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)$,
    the goal is to find a set of recommendation polices ($\{\pi\}:\mathcal{S}\to\mathcal{A}$)
    that maximizes the cumulative reward during interaction with users.
  prefs: []
  type: TYPE_NORMAL
- en: Problem Formulation 0.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given an environment that contains all items $\mathcal{I}$, when user $u\in\mathcal{U}$
    interacts with the system, an initial state $s$ is sampled from the environment
    which contains a list of candidate items and users’ historical data. The DRL agent
    needs to work out a recommendation policy $\pi$ based on the state $s$ and produces
    the corresponding recommended item list $a$. The user will provide feedback on
    the list which is normally represented as click or not click. The DRL agent will
    then utilize the feedback to improve the recommendation policy and move to the
    next interaction episode.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8f91855d595097748cc98151f983900.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Deep learning based recommender systems
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30037123b8d741494b73136d02c92a70.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Deep reinforcement learning based recommender systems
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3. Difference between deep learning based RS and DRL-based RS. Deep learning
    based RSs may only update the recommendation policy during the training stage.
    They often require re-training, which is computationally inefficient, when users’
    interests change significantly. DRL-based RS will update the recommendation policy
    time over time as new rewards are received.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Deep Reinforcement Learning in Recommender Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DRL-based RS has some unique challenges such as state construction, reward
    estimation and environment simulation. We categorize the existing work of DRL-based
    recommendation into model-based and model-free methods (the taxonomy is shown
    in [Figure 2](#S2.F2 "In 2.2\. Preliminaries of Deep Reinforcement Learning ‣
    2\. Background ‣ A Survey of Deep Reinforcement Learning in Recommender Systems:
    A Systematic Review and Future Directions")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Model-based Deep Reinforcement Learning based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model-based methods assume an expected reward or action available for the next
    step to help the agent update the policy.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2. List of publications in model-based DRL-based RS
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Work |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Value-based | (Zhang et al., [2017](#bib.bib120); Chen et al., [2019c](#bib.bib19);
    Zou et al., [2020](#bib.bib136); Wang et al., [2021](#bib.bib97)) |'
  prefs: []
  type: TYPE_TB
- en: '| Policy-based | (Bai et al., [2019](#bib.bib5); Hong et al., [2020](#bib.bib39))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid | (Zhao et al., [2020c](#bib.bib126)) |'
  prefs: []
  type: TYPE_TB
- en: Policy-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: IRecGAN (Bai et al., [2019](#bib.bib5)) is a model-based method that adopts
    generative adversarial training to improve the robustness of policy learning.
    It can reduce the cost of interaction for RS by using offline data instead of
    the simulated environment. IRecGAN employs a generative adversarial network (Goodfellow
    et al., [2014](#bib.bib33)) to generate user data based on the offline dataset.
    It trains a recommendation agent using a policy gradient-based DRL method called
    REINFORCE. The agent aims to learn a policy based on the following gradient,
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $\displaystyle\mathbb{E}_{\tau\sim\{g,data\}}\big{[}\sum_{t=0}^{T}\sum_{t^{\prime}=t}^{T}\gamma^{t^{\prime}-t}q_{D}(\tau_{0:t}^{n})r_{t}\nabla_{\theta_{a}}(c_{t}\in\pi_{\theta_{a}}(s_{t})\big{]},q_{D}(\tau_{0:t}^{n})=\frac{1}{N}\sum_{n=1}^{N}D(\tau_{0:T}^{n}),\tau_{0:T}^{n}\in
    MC^{\mathcal{U}}(N)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the $\mathit{MC}^{\mathcal{U}}(N)$ represents the sampled $N$ sequences
    from the interaction between $\mathcal{U}$ and the agent using the Monte-Carlo
    tree search algorithm, $D$ is the discriminator, $T$ is the length of $\tau$,
    $g$ represents the offline data, and data represents the generated data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hong et al. ([2020](#bib.bib39)) propose NRSS for personalized music recommendation.
    NRSS uses wireless sensing data to learn users’ current preferences. NRSS considers
    three different types of feedback: score, option, and wireless sensing data. Because
    multiple factors are considered as the reward, NRSS designs a reward model which
    consists of users’ preference reward $r_{p}$ and a novel transition reward $r_{\textit{trans}}$
    which are parameterized by $\theta_{r_{p}}$ and $\theta_{r_{\textit{trans}}}$.
    The goal for NRSS is to find the optimal parameters $\theta_{r_{p}}$ and $\theta_{r_{\textit{trans}}}$
    by using the Monte-Carlo tree search thus improving recommendation performance.
    However, wireless sensing feedback lacks generalization ability as it is only
    available for certain tasks or scenarios, making it hard to determine dynamic
    user interest.'
  prefs: []
  type: TYPE_NORMAL
- en: Value-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prior to Q-learning, value iteration is a more traditional value-based reinforcement
    learning algorithm that focuses on the iteration of the value function. Gradient
    Value Iteration (GVI) (Zhang et al., [2017](#bib.bib120)) is proposed to improve
    the traditional value iteration algorithm by utilizing the transition probability
    and a multi-agent setting to predict chronological author collaborations. It introduces
    a new parameter named ‘status’ to reflect the amount of knowledge that the agent
    needs to learn from this state. The policy is updated only when the distance between
    the new status and the old status is lower than a pre-defined threshold. However,
    value iteration requires the transition probability, which is hard to obtain in
    most cases. Hence, Q-learning and its variants are widely used in DRL-based RS.
    Cascading DQN (CDQN) with a generative user model (Chen et al., [2019c](#bib.bib19))
    is proposed to deal with the environment with unknown reward and environment dynamics.
    The generative user model adopts GANs to generate a user model based on an offline
    dataset. Different from previous work, it will generate the reward function for
    each user to explain the users’ behavior. The user model can be written as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (15) |  | $\displaystyle\operatorname*{arg\,max}_{\phi\in\triangle^{k-1}}\mathbb{E}_{\phi}[r(s_{t},a_{t})]-R(\phi)/\eta$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\triangle^{k-1}$ is the probability simplex, $R(\phi)$ is the regularization
    term for exploration and $\eta$ is a constant.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudo Dyna-Q (PDQ) (Zou et al., [2020](#bib.bib136)) points out that Monte-Carlo
    tree search may lead to an extremely large action space and an unbounded importance
    weight of training samples. Hence, a world model is proposed to reduce the instability
    of convergence and high computation cost for interacting with users by imitating
    the offline dataset. With the world model, the agent will interact with the learned
    world model instead of the environment to improve the sample efficiency and convergence
    stability. The world model learning process introduced in PDQ can be described
    as finding the parameter $\theta_{M}$,
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | $\displaystyle\operatorname*{arg\,min}_{\theta_{M}}\mathbb{E}_{\xi\in
    P_{\xi}^{\pi}}[\sum_{t}^{T-1}\gamma^{t}\prod_{j=0}^{t}\frac{\pi(s_{j},a_{j})}{\pi_{b}(s_{j},a_{j})}\Delta_{t}(\theta_{M})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\xi$ is generated by the logged policy $\pi_{b}$, $\prod_{j=0}^{t}\frac{\pi(s_{j},a_{j})}{\pi_{b}(s_{j},a_{j})}$
    is the ratio used for importance sampling and $\Delta$ is the difference between
    the reward in the world model and real reward. Furthermore, GoalRec (Wang et al.,
    [2021](#bib.bib97)) designs a disentangled universal value function to be integrated
    with the world model to help the agent deal with different recommendation tasks.
    The universal value function is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $\displaystyle V_{\pi}(s)=\mathbb{E}[\sum_{t=0}^{\infty}r(s_{t},a_{t})\prod_{k=0}^{t}\gamma
    s_{k}&#124;s_{0}=s].$ |  |'
  prefs: []
  type: TYPE_TB
- en: Moreover, GoalRec introduces a new variable goal $g\in G$ used to represent
    users’ future trajectory and measurement $m\in M$. $m$ is an indicator that reflects
    users’ response to the given future trajectory based on historical behaviors.
    Based on that, the optimal action will be selected based on
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $\displaystyle a^{*}=\max_{a}U(M(s,a),g)$ |  |'
  prefs: []
  type: TYPE_TB
- en: with a customized liner function $U(\cdot)$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4e670a61100905528daa94226c41b94.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4. Left is the general structure of model-free methods. Right is the
    structure for GoalRec which is a model-based method. A sample trajectory is used
    to demonstrate the difference between them (Wang et al., [2021](#bib.bib97)).
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Hybrid method can be recognized as a midpoint between value-based and policy
    gradient-based methods. DeepChain (Zhao et al., [2020c](#bib.bib126)) uses the
    multi-agent setting to relieve the sub-optimality problem. The sub-optimality
    problem is caused by the one for all setting that optimizes one policy for all
    users. Hence, DeepChain designs a multi-agent setting that adopts several agents
    to learn consecutive scenarios and jointly optimizes multiple recommendation policies.
    The main training algorithm used is DDPG. To this end, users’ actions can be formulated
    in a model-based form as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | $\displaystyle\sum_{m,d}[p_{m}^{s}(s_{t},a_{t})\gamma Q_{\theta}(s^{\prime}_{t},\pi_{m}(s^{\prime}_{t}))+p_{m}^{c}(s_{t},a_{t})(r_{t}+\gamma
    Q_{\theta}(s^{\prime}_{t},\pi_{d}(s^{\prime}_{t})))+p_{m}^{l}(s_{t},a_{t})r_{t}]1_{m}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $m$ represents the number of actor networks, $c,l,s$ represent the three
    different scenarios, $1_{m}$ is used to control the activation of two actors and
    $(m,d)\in\{(1,2),(2,1)\}$.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Model-based methods aim to learn a model or representation to represent the
    whole environment so that the agent can plan ahead and receive better sample efficiency.
    The drawback of such a method is that the ground-truth representation of the environment
    is unavailable in recommendation scenarios as it dynamically changes, leading
    to a biased representation. Moreover, model-based methods use the transition probability
    function $\mathcal{P}$ to estimate the optimal policy. As mentioned, the transition
    probability function is normally equivalent to users’ behavior probability which
    is hard to determine in a recommender system. Hence, existing works (Zhao et al.,
    [2020c](#bib.bib126); Bai et al., [2019](#bib.bib5); Wang et al., [2021](#bib.bib97);
    Zou et al., [2020](#bib.bib136); Chen et al., [2019c](#bib.bib19); Zhang et al.,
    [2017](#bib.bib120)) approximate $\mathcal{P}$ using a neural network or embedding
    it into the world model. Zhao et al. ([2020c](#bib.bib126)) design a probability
    network to estimate $\mathcal{P}$ while (Bai et al., [2019](#bib.bib5); Chen et al.,
    [2019c](#bib.bib19)) uses a GAN to generate user behavior where $\mathcal{P}$
    is embedded in the latent space. Different from them, (Wang et al., [2021](#bib.bib97);
    Zou et al., [2020](#bib.bib136)) relies on the world model to predict users’ next
    behavior and feed it into the policy learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The challenges of model-based DRL are not widely used in RS and can be summarized
    into the following facets:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{P}$ is hard to determine in real-world recommender systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If approximation is used to estimate $\mathcal{P}$, the overall model complexity
    will substantially increase as it requires approximating two different functions
    $\mathcal{P}$ and the recommendation policy $\pi$ by using a large amount of user
    behavior data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: World model-based methods require periodic re-training to ensure the model can
    reflect user interests in time which increases the computation cost.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c8dc8d38fb4c611a56e2f54e6a10b25.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) DQN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/137e75095b5bb7dec7ae8e5dde6f602a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) DDPG
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5. The typical structure of DQN and DDPG
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Model-free deep reinforcement learning based methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Compared with model-based methods, model-free methods are relatively well-studied.
    Different from model-based methods, $\mathcal{P}$ is unknown and not required
    in model-free methods. Model-based methods enable the agent to learn from previous
    experiences. In this subsection, we categorize model-free based DRL in RS into
    three parts: value-based, policy-based and hybrid methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3. List of reviewed publications in Model-free DRL-based RS
  prefs: []
  type: TYPE_NORMAL
- en: '| Tasks | Note | Work |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Value-based | Vanilla DQN and its extensions | (Zheng et al., [2018](#bib.bib131);
    Zhao et al., [2018a](#bib.bib125); Lei et al., [2019](#bib.bib55)) |'
  prefs: []
  type: TYPE_TB
- en: '| DQN with state/action space optimization | (Xiao et al., [2020](#bib.bib105);
    Lei and Li, [2019](#bib.bib53); Zou et al., [2019](#bib.bib135); Ie et al., [2019b](#bib.bib43))
    |'
  prefs: []
  type: TYPE_TB
- en: '| DQN with graph/image input | (Lei et al., [2020](#bib.bib54); Gui et al.,
    [2019](#bib.bib34); Oyeleke et al., [2018](#bib.bib72); Zhao et al., [2018b](#bib.bib128);
    Takanobu et al., [2019](#bib.bib89); Gao et al., [2019](#bib.bib29); Zhou et al.,
    [2020](#bib.bib132)) |'
  prefs: []
  type: TYPE_TB
- en: '| DQN for joint learning | (Pei et al., [2019](#bib.bib74); Zhao et al., [2020e](#bib.bib130);
    Zhao et al., [2021](#bib.bib124)) |'
  prefs: []
  type: TYPE_TB
- en: '| Policy-based | Vanilla REINFORCE | (Pan et al., [2019](#bib.bib73); Wang
    et al., [2018a](#bib.bib100); Chen et al., [2019a](#bib.bib14); Xu et al., [2020](#bib.bib108);
    Ma et al., [2020](#bib.bib65); Chen et al., [2021](#bib.bib15); Montazeralghaem
    et al., [2020](#bib.bib69); Ji et al., [2020](#bib.bib46); Yu et al., [2019](#bib.bib111))
    |'
  prefs: []
  type: TYPE_TB
- en: '| REINFORCE uses graph structure/input | (Wang et al., [2020a](#bib.bib99);
    Xian et al., [2019](#bib.bib104); Wang et al., [2020b](#bib.bib101); Chen et al.,
    [2019b](#bib.bib12)) |'
  prefs: []
  type: TYPE_TB
- en: '| Non-REINFORCE based | (Hu et al., [2018](#bib.bib40); Zhang et al., [2019d](#bib.bib115))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid | Vanilla DDPG | (Zhao et al., [2019b](#bib.bib129), [2018a](#bib.bib125);
    Liu et al., [2020b](#bib.bib60); Wang et al., [2018b](#bib.bib98); Cai et al.,
    [2018](#bib.bib10)) |'
  prefs: []
  type: TYPE_TB
- en: '| with Knowledge Graph | (Chen et al., [2020b](#bib.bib18); Zhao et al., [2020b](#bib.bib122);
    Feng et al., [2018](#bib.bib25); Zhang et al., [2021](#bib.bib118); He et al.,
    [2020](#bib.bib37), [2020](#bib.bib37); Haarnoja et al., [2018](#bib.bib36); Zhao
    et al., [2020d](#bib.bib121); Xie et al., [2021](#bib.bib106)) |'
  prefs: []
  type: TYPE_TB
- en: Value based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As mentioned, Deep Q-learning and its variants are typical value-based DRL methods
    widely used in DRL-based RS. DRN (Zheng et al., [2018](#bib.bib131)) is the first
    work utilizing Deep Q-Networks (DQN) in RS. It adopts Double DQN (DDQN) (Van Hasselt
    et al., [2016](#bib.bib92)) to build a user profile and designs an activeness
    score to reflect how frequently a user returns after one recommendation plus users’
    action (click or not) as the reward. DRN provides a new approach to integrating
    DRL into RS when dealing with a dynamic environment. The key objective function
    can be found as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '| (20) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma Q_{\theta_{t}^{\prime}}(s_{t+1},\operatorname*{arg\,max}_{a^{\prime}}Q_{\theta_{t}}(s_{t},a^{\prime}))]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $a^{\prime}$ is the action that gives the maximum future reward according
    to $\theta_{t}$, $\theta_{t}$ and $\theta_{t}^{\prime}$ are different parameters
    for two different DQNs. Zhao et al. ([2018a](#bib.bib125)) points out that negative
    feedback will also affect recommendation performance which DRN does not consider.
    Moreover, positive feedback is sparse due to the large number of candidate items
    in RS. Only using positive feedback would lead to convergence problems. Hence,
    DEERS is proposed to consider both positive and negative feedback simultaneously
    by using DQN. Gated Recurrent Units (GRU) are employed to capture users’ preferences
    for both a positive state $s^{+}$ and negative state $s^{-}$ and the final objective
    function can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (21) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma\max_{a_{t+1}}Q_{\theta_{q}}(s^{+}_{t+1},s^{-}_{t+1},a_{t+1})&#124;s^{+}_{t},s^{-}_{t},a_{t}].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Lei et al. ([2019](#bib.bib55)) introduces attention mechanisms into the DQN
    to leverage social influence among users. To be specific, a social impact representation
    $U_{v}$ is introduced into the state representation. Matrix factorization is adopted
    to determine similarity among users and hence present the social influence. Social
    attention is introduced to distill the final state representation. In addition,
    a few studies focus on user profiling to improve recommendation performance (Xiao
    et al., [2020](#bib.bib105); Lei and Li, [2019](#bib.bib53); Zou et al., [2019](#bib.bib135)).
    Lei and Li ([2019](#bib.bib53)) claims that user feedback contains useful information
    in the previous feedback even when the user does not like the recommended items.
    Some existing studies focus on final feedback which ignore the importance from
    earlier steps to later ones. Hence, user-specific DQN (UQDN) is proposed to consider
    multi-step feedback from users. It employs Matrix Factorization to generate user-specific
    latent state spaces. The newly defined objective function with the user-specific
    latent state space can be represented as
  prefs: []
  type: TYPE_NORMAL
- en: '| (22) |  | $\displaystyle\mathbb{E}[r_{t+1}+\gamma\max_{a_{t+1}}\overline{Q}_{\theta_{q}}(s_{t+1},a_{t+1})+\overline{\textbf{b}}_{u}-Q_{\theta_{q}}(s_{t+1},a_{t+1})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\overline{\textbf{b}}_{u}$ is the learned user latent representation.
    Zou et al. ([2019](#bib.bib135)) also points out that most studies do not consider
    users’ long-term engagement in the state representation as they focus on the immediate
    reward. FeedRec is proposed that combines both instant feedback and delayed feedback
    into the model to represent the long-term reward and optimize the long-term engagement
    by using DQN. To be specific, time-LSTM is employed to track users’ hierarchical
    behavior over time to represent the delayed feedback which contains three different
    operations: $h_{\mathit{skip}},h_{\mathit{choose}},h_{\mathit{order}}$. The state
    space is the concatenation of those operations and users’ latent representation.
    Differently, Xiao et al. ([2020](#bib.bib105)) focuses on the user privacy issue
    in recommender systems. Deep user profile perturbation (DUPP) is proposed to add
    perturbation into the user profile by using DQN during the recommendation process.
    Specifically, DUPP adds a perturbation vector into users’ clicked items as well
    as the state space, which contains users’ previous behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Distinct from previous studies which focus on optimizing user profiles or state
    spaces, some studies aim to optimize the action space formed by interactions with
    items. In the situation of basket recommendation, the user is suggested multiple
    items as a bundle, which is called a recommendation slate. It leads to combinatorially
    large action spaces making it intractable for DQN based recommendation models.
  prefs: []
  type: TYPE_NORMAL
- en: SlateQ (Ie et al., [2019b](#bib.bib43)) is proposed to decompose slate Q-value
    to estimate a long-term value for individual items, and it is represented as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $\displaystyle Q_{\theta_{q}}(s_{t},a_{t})=\sum_{i\in a_{t}}p(i&#124;s_{t},a_{t})\overline{Q}_{\theta_{q}}(s_{t},i)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\overline{Q}_{\theta}(s,i)$ is the decomposed Q-value for item $i$. The
    decomposed Q-value will be updated by the following rule which is similar to traditional
    DQN,
  prefs: []
  type: TYPE_NORMAL
- en: '| (24) |  | $\displaystyle\overline{Q}_{\theta_{q}}(s_{t},i)\leftarrow\alpha\bigg{(}r_{t}+\gamma\sum_{j\in
    a_{t+1}}p(j&#124;s_{t+1},a_{t+1})\overline{Q}_{\theta_{q}}(s_{t+1},j)\bigg{)}+(1-\alpha)\overline{Q}_{\theta_{q}}(s_{t},i).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Different from other mode-free methods, Slate-Q assumes that the transition
    probability $p(i|s_{t},a_{t})$ is known.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vanilla DQN methods may not have sufficient knowledge to handle complex data
    such as images and graphs. Tang and Wang ([2018](#bib.bib90)) firstly models users’
    click behavior as an embedding matrix in the latent space to include the skip
    behaviors of sequence patterns for sequential recommendation. Based on that, Gao
    et al. ([2019](#bib.bib29)) propose DRCGR, which adopts CNN and GAN into DQN to
    help the agent to better understand high-dimensional data, e.g., a matrix. Two
    different convolution kernels are used to capture users’ positive feedback. In
    the meantime, DRCGR uses GANs to learn a negative feedback representation to improve
    robustness. Another typical data format is the graph, which is widely used in
    RS, including knowledge graphs. Lei et al. ([2020](#bib.bib54)) propose GCQN which
    adopts Graph Convolutional Networks (GCN) (Kipf and Welling, [2017](#bib.bib49))
    into the DQN which constructs the state and action space as a graph-aware representation.
    Differently, GCQN introduces the attention aggregator: $\sum_{w\in\mathcal{N}(i)}\alpha_{iu}e_{u}$
    which demonstrates better performance than the mean-aggregator and pooling-aggregator.
    For item $i$, the graph-aware representation can be represented as,'
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $\displaystyle\sigma\bigg{(}W_{fc}[e_{i}\oplus\sum_{w\in\mathcal{N}(i)}\alpha_{iu}e_{u}+b_{fc}]\bigg{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $W_{fc},b_{fc}$ are the parameters for the fully-connected layer, $e_{u}$
    is the embedding for user $u$ and $\mathcal{N}(i)$ is the set of one-hot neighbours
    of item $i$ in graph $G(i)$. Zhou et al. ([2020](#bib.bib132)) propose KGQR uses
    a similar strategy to transform the information into a knowledge graph which is
    fed into the GCN to generate the state representation. Notably, KGQR presents
    a different state representation generation method. For given node $i$, the neighbourhood
    representation with a $k$-hop neighborhood aggregator can be represented as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (26) |  | $\displaystyle e_{i}^{k}=\sigma\bigg{(}W_{k}\frac{1}{&#124;\mathcal{N}(i)&#124;}\sum_{t\in\mathcal{N}(i)}e_{t}^{k-1}+B_{k}e_{i}^{k-1}\bigg{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{N}(i)$ is the set of neighboring nodes, $W_{k},B_{k}$ are the
    parameter of the aggregator. Those neighbourhood representations will be fed into
    a GRU and the state representation will be generated. Another application domain
    for using graph data is job recommendation which requires considering multiple
    factors jointly such as salary, job description, job location etc. SRDQN (Sun
    et al., [2021](#bib.bib88)) constructs a probability graph to represent a candidate’s
    skill set and employs a multiple-task DQN structure to process these different
    factors concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some studies targeting recommendation and advertising simultaneously
    in e-commerce environments (Pei et al., [2019](#bib.bib74); Zhao et al., [2020e](#bib.bib130);
    Zhao et al., [2021](#bib.bib124)). Pei et al. ([2019](#bib.bib74)) mentions when
    deploying RS into real-world platforms such as e-commerce scenarios, the expectation
    is to improve the profit of the system. A new metric, Gross Merchandise Volume
    (GMV), is proposed to measure the profitability of the RS to provide a new view
    about evaluating RS in advertising. Different from GMV, Zhao et al. ([2020e](#bib.bib130))
    separates recommendation and advertising as two different tasks and proposes the
    Rec/Ads Mixed display (RAM) framework. RAM designs two agents: a recommendation
    agent and an advertising agent, where each agent employs a CDQN to conduct the
    corresponding task. Zhao et al. ([2021](#bib.bib124)) find that advertising and
    recommendation may harm each other and formulate a rec/ads trade-off. Their proposed
    solution, DEARS, contains two RNNs. Two RNNs are employed to capture user preferences
    toward recommendations and ads separately. Based on that, DQN is employed to take
    those two outputs as the input to construct the state and output the advertising.'
  prefs: []
  type: TYPE_NORMAL
- en: Policy-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Policy-based DRL can be divided into two parts which are Constrained Policy
    Optimization (CPO)  (Achiam et al., [2017](#bib.bib2)) and policy gradient. Zhang
    et al. ([2019d](#bib.bib115)) uses CPO to identify the contradiction between text
    feedback and historical preferences. It provides a solution for using DRL in the
    situation where users’ feedback is entirely different from previous feedback in
    RS. Policy gradient-based methods are the other stream in policy-based DRL methods
    for RS. These methods aims to optimize the policy $\pi$ directly instead of estimating
    the Q-value like DQN. A well-known and widely used policy gradient method in RS
    is REINFORCE which uses the following rule for policy $\pi_{\theta_{\pi}}$,
  prefs: []
  type: TYPE_NORMAL
- en: '| (27) |  | $\displaystyle\theta\leftarrow\theta+\alpha\mathbb{E}_{\tau\sim
    d_{\pi_{\theta_{\pi}}}}\bigg{[}\sum_{t=1}^{T}r(s_{t}^{i},a_{t}^{i})\sum_{t=1}^{T}\nabla_{\theta_{\pi}}\log\pi_{\theta_{\pi}}(s_{t}^{i},a_{t}^{i})\bigg{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $i$ is sampled trajectories from $\pi_{\theta}(a_{t}|s_{t})$. Pan et al.
    ([2019](#bib.bib73)) propose Policy Gradient for Contextual Recommendation (PGCR),
    which adopts REINFORCE and considers contextual information. PGCR assumes that
    the policy follows the multinoulli distribution, in which case the transition
    probability can be estimated easily through sampling from previously seen context.
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. ([2018a](#bib.bib100)) incorporate CNNs and attention mechanisms
    in REINFORCE for explainable recommendation. Specifically, this work designs a
    coupled agent structure where one agent generates the explanation and the other
    makes recommendations based on the generated explanation.
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al. ([2019a](#bib.bib14)) increases the scalability of REINFORCE to
    ensure it can deal with the extremely large action space under recommendation
    scenarios. To be specific, it introduces a policy correction gradient estimator
    into REINFORCE to reduce the variance of each gradient by doing importance sampling.
    The new update rule becomes
  prefs: []
  type: TYPE_NORMAL
- en: '| (28) |  | $\displaystyle\theta_{\pi}\leftarrow\theta_{\pi}+\alpha\sum_{\tau\sim\beta}\bigg{[}\sum_{t=1}^{T}\frac{\pi_{\theta_{\pi}}(s_{t},a_{t})}{\pi_{\beta}(s_{t},a_{t})}r(s_{t}^{i},a_{t}^{i})\sum_{t=1}^{T}\nabla_{\theta_{\pi}}\log\pi_{\theta_{\pi}}(s_{t}^{i},a_{t}^{i})\bigg{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\pi_{\beta}$ is the behavior policy trained by state-action pairs without
    the long-term reward and $\pi_{\theta}$ is trained based on the long-term reward
    only. It is worth mentioning that the vanilla REINFORCE algorithm is on-policy,
    and importance sampling will make REINFORCE behave like an off-policy method with
    the following gradient format,
  prefs: []
  type: TYPE_NORMAL
- en: '| (29) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}\bigg{[}\prod_{t^{\prime}=1}^{t}\frac{\pi_{\theta}(s_{t},a_{t})}{\pi_{\theta_{s}}(s_{t^{\prime}},a_{t^{\prime}})}\sum_{t^{\prime}=t}^{T}r(s_{t},a_{t})\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})\bigg{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\pi_{\theta_{s}}$ is the sample policy parameter. Xu et al. ([2020](#bib.bib108))
    also finds that the REINFORCE method suffers from a high variance gradient problem
    and Pairwise Policy Gradient (PPG) is proposed. Different from policy correction,
    PPG uses Monte Carlo sampling to sample two different actions $a,b$ and compare
    the gradient to update $\theta$,
  prefs: []
  type: TYPE_NORMAL
- en: '| (30) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta_{\pi}}}}\bigg{(}\sum_{a}\sum_{b}(r(s,a)-r(s,b))\sum_{t=1}^{T}(\nabla_{\theta_{\pi}}\log_{\pi_{\theta_{\pi}}}(s_{t},a_{t})-\nabla_{\theta_{\pi}}\log_{\pi_{\theta_{\pi}}}(s_{t},b_{t}))\bigg{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Ma et al. ([2020](#bib.bib65)) extends the policy correction gradient estimator
    into a two-stage setting which are $p(s_{t},a^{p})$ and $q(s_{t},a|a^{p})$ and
    the policy can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '| (31) |  | $\displaystyle\sum_{a^{p}}p(s_{t},a^{p})q(s_{t},a&#124;a^{p}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In addition, weight capping and self-normalized importance sampling are used
    to further reduce the variance. Moreover, a large state space and action space
    will cause sample inefficiency problems as REINFORCE relies on the current sampled
    trajectories $\tau$. Chen et al. ([2021](#bib.bib15)) finds that the auxiliary
    loss can help improve the sample efficiency (Jaderberg et al., [2016](#bib.bib45);
    Sermanet et al., [2018](#bib.bib82)). Specifically, a linear projection is applied
    to the state $s_{t}$, the output is combined with action $a_{t}$ to calculate
    the auxiliary loss and appended into the final overall objective function for
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Another prototype of vanilla policy gradient in DRL-based RS is the policy network.
    Montazeralghaem et al. ([2020](#bib.bib69)) designs a policy network to extract
    features and represent the relevant feedback that can help the agent make a decision.
    Similar to DQN, this work uses a neural network to approximate the Q-value and
    the policy directly without theoretical analysis. Ji et al. ([2020](#bib.bib46))
    extend the policy network by introducing spatio-temporal feature fusion to help
    the agent understand complex features. Specifically, it considers both the current
    number and the future number of vacant taxis on the route to recommend routes
    for taxis. Yu et al. ([2019](#bib.bib111)) introduces multi-modal data as new
    features to conduct vision-language recommendation by using historical data to
    train REINFORCE. ResNet and attention are used to encode vision and text information,
    respectively. Moreover, two rewards are introduced with a customized ratio $\lambda$
    to balance vision and text information.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Graphs (KG) are widely used in RS to enrich side information, provide
    explainability and improve recommendation performance. Similar to DQN, vanilla
    REINFORCE cannot properly handle graph-like data. Wang et al. ([2020a](#bib.bib99))
    propose a method named Knowledge-guided Reinforcement Learning (KERL), which integrates
    knowledge graphs into the REINFORCE algorithm. To be specific, KERL adopts TransE (Bordes
    et al., [2013](#bib.bib7)) to transfer the knowledge graph into a graph embedding
    and utilizes a multilayer perceptron (MLP) to predict future knowledge of user
    preferences. The state representation can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '| (32) |  | $\displaystyle h_{t}\oplus\mathit{TransE}(\mathcal{G})\oplus\mathit{MLP}(\mathit{TransE}(\mathcal{G}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $h_{t}$ is the hidden representation from the GRU for sequential behavior
    and $\mathcal{G}$ is the knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: Different from KERL, Xian et al. ([2019](#bib.bib104)) propose Policy-Guided
    Path Reasoning (PGPR), which formulates the whole environment as a knowledge graph.
    The agent is trained to find the policy to find good items conditioned on the
    starting user in the KG by using REINFORCE. PGPR uses the tuple $(u,e_{t},h_{t})$
    to represent the state instead of the graph embedding where $e_{t}$ is the entity
    the agent has reached at $t$ for user $u$ and $h_{t}$ is the previous action before
    $t$. The action in PGPR is defined as the prediction of all outgoing edges for
    $e_{t}$ based on $h_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. ([2020b](#bib.bib101)) propose a knowledge graph policy network
    (KGPolicy) which puts the KG into the policy network and adopts REINFORCE to optimize
    it. In addition, KGPolicy uses negative sampling instead of stochastic sampling
    to overcome the false negative issue—sampled items behave differently during training
    and inference. Similar to GCQN, attention is also employed to establish the representation
    for its neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the on-policy nature of REINFORCE, it is difficult to apply it to large-scale
    RS as the convergence speed will be a key issue. To relieve this, Chen et al.
    ([2019b](#bib.bib12)) propose TPGR, which designs a tree-structured policy gradient
    method to handle the large discrete action space hierarchically. TPGR uses balanced
    hierarchical clustering to construct a clustering tree. Specifically, it splits
    a large-scale data into several levels and maintains multiple policy networks
    for each level to conduct the recommendation. The results are integrated at the
    final stage.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, policy gradient can be further extended to deterministic policy
    gradient (DPG) (Silver et al., [2014](#bib.bib87)). Hu et al. ([2018](#bib.bib40))
    propose Deterministic Policy Gradient with Full Backup Estimation (DPG-FBE) to
    complete a sub-task of recommendation. DPG-FBE considers a search session MDP
    (SSMDP) that contains a limited number of samples, where the stochastic policy
    gradient method like REINFORCE cannot work well.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The most common model-free hybrid method used would be the actor-critic algorithm
    where the critic network uses the DQN and the actor uses the policy gradient.
    The common algorithm used to train actor-critic is DDPG with the following objective
    function,
  prefs: []
  type: TYPE_NORMAL
- en: '| (33) |  | $\displaystyle\mathbb{E}[r_{t}+\gamma Q_{\theta_{q}^{\prime}}(s_{t+1},\mu_{\theta_{\pi}^{\prime}}(s_{t+1}))-Q_{\theta_{q}}(s_{t},a_{t})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\theta_{q},\theta_{q}^{\prime}$ is the parameter for Q-learning at time
    $t,t+1$ while $\theta_{\pi}^{\prime}$ is the parameter for deterministic policy
    gradient at time $t+1$. Zhao et al. ([2019b](#bib.bib129)) propose LIRD, which
    uses the vanilla actor-critic framework to conduct list-wise recommendations.
    In order to demonstrate the effectiveness of LIRD, a pre-trained user simulator
    is used to evaluate the effectiveness of LIRD where the transition probability
    is approximated using the cosine similarity for a given state-action pair $s_{t},a_{t}$.
    Zhao et al. ([2018a](#bib.bib125)) further extend LIRD into page-wise recommendation
    and proposed DeepPage. Similar to other previous work, GRU is employed to process
    the sequential pattern. Moreover, similar to DRCGR, DeepPage formulates the state
    as a page, then CNNs are employed to capture features and fed to the critic network.
    The final state representation is the concatenation of the sequential pattern
    and the page features. Additionally, there are a few studies focusing on different
    scenarios such as top-aware recommendation (Liu et al., [2020b](#bib.bib60)),
    treatment recommendation (Wang et al., [2018b](#bib.bib98)), allocating impressions (Cai
    et al., [2018](#bib.bib10)) etc. Liu et al. ([2020b](#bib.bib60)) introduces a
    supervised learning module (SLC) as the indicator to identify the difference between
    the current policy and historical preferences. SLC will conduct the ranking process
    to ensure the recommendation policy will not be affected by the positional bias
    – the item appearing on top receives more clicks. Similarly, Wang et al. ([2018b](#bib.bib98))
    also integrates the supervised learning paradigm into DRL but in a different way.
    An expert action $\hat{a}_{t}$ is provided when the critic evaluates the policy
    and the update rule is slightly different than normal DQN,
  prefs: []
  type: TYPE_NORMAL
- en: '| (34) |  | $\displaystyle\theta_{q}\leftarrow\theta_{q}+\alpha\sum_{t}[Q_{\theta_{q}}(s_{t},\hat{a}_{t})-r_{t}-\gamma
    Q_{\theta_{q^{\prime}}}(s_{t},\mu_{\theta_{\pi^{\prime}}}(s_{t}))]\nabla_{\theta_{q}}Q_{\theta_{q}}(s_{t},a_{t}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: However, such a method is not universal as the acquisition of expert action
    is difficult and depends on the application domain.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to policy gradient and DQN, Knowledge Graphs (KG) are also used in actor-critic-based
    methods. Chen et al. ([2020b](#bib.bib18)) propose KGRL to incorporate the substantial
    information of knowledge graphs to help the critic to better evaluate the generated
    policy. A knowledge graph is embedded into the critic network. Different from
    previous studies which use the KG as the environment or state representation,
    KGRL uses KG as a component in the critic, which can guide the actor to find a
    better recommendation policy by measuring the proximity from the optimal path.
    Specifically, a graph convolutional network is used to weight the graph and Dijkstra’s
    algorithm is employed to find the optimal path for finally identifying the corresponding
    Q-value. Zhao et al. ([2020b](#bib.bib122)) claim that human’s demonstration could
    improve path searching and propose ADAC. ADAC also searches for the optimal path
    in the KG but further adopts adversarial imitation learning and uses expert paths
    to facilitate the search process. Feng et al. ([2018](#bib.bib25)) propose MA-RDPG,
    which extends the standard actor-critic algorithm to deal with multiple scenarios
    by utilizing a multi-actor reinforcement learning setting. Specifically, two different
    actor-networks are initialized while only one critic network will make the final
    decision. Those two actor networks can communicate with each other to share information
    and approximate the global state. Zhang et al. ([2021](#bib.bib118)) find that
    there are multiple factors can affect the selection of electric charging station.
    Hence, it uses a similar idea to recommend the electric vehicle charging station
    by considering current supply, future supply, and future demand. He et al. ([2020](#bib.bib37))
    figure out that the communication mechanism in MA-RDPG will harm actors as they
    are dealing with independent modules, and there is no intersection. Hence, He
    et al. ([2020](#bib.bib37)) extend MA-RDPG into multi-agent settings which contain
    multiple pairs of actors and critics and remove the communication mechanism to
    ensure independence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from (Feng et al., [2018](#bib.bib25)), He et al. ([2020](#bib.bib37))
    use ‘soft’ actor-critic (SAC) (Haarnoja et al., [2018](#bib.bib36)), which introduces
    a maximum entropy term $\mathcal{H}(\pi(s_{t},\phi_{t}))$ to actor-critic to improve
    exploration and stability with the stochastic policy $\pi(s_{t},\phi_{t})$. Similar
    to the multi-agent idea, Zhao et al. ([2020d](#bib.bib121)) use a hierarchical
    setting to help the agent learn multiple goals by setting multiple actors and
    critics. In comparison, hierarchical RL uses multiple actor-critic networks for
    the same task. It splits a recommendation task into two sub-tasks: discovering
    long-term behavior and capturing short-term behavior. The final recommendation
    policy is the combination of the optimal policies for the two sub-tasks. Similarly,
    Xie et al. ([2021](#bib.bib106)) use the hierarchical setting for integrated recommendation
    by using different sourced data. The objective is to work out the sub-polices
    for each source hierarchically and form the final recommendation policy afterward.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In RS, model-free methods are generally more flexible than model-based methods
    as they do not require knowing the transition probability. We summarize the advantages
    and disadvantages of the three kinds of methods described under the model-free
    category. DQN is the first DRL method used in RS, which is suitable for small
    discrete action spaces. The problems with DQN in RS are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RS normally contains large and high-dimensional action spaces.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward function is hard to determine which will lead to inaccurate value
    function approximation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Specifically, the high dimensional action space in context of recommender systems
    is recognized as a major drawback of DQN (Mnih et al., [2015](#bib.bib68); Tavakoli
    et al., [2018](#bib.bib91)). The reason lies in the large number of the candidate
    items. Hence, DQN, as one of the most popular schemes, is not the best choice
    for RS in many situations. Moreover, some unique factors need to be considered
    when designing the reward function for RS such as social inference. It introduces
    extra parameters to the Q-network and hinders the convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient does not require the reward function to estimate the value function.
    Instead, it estimates the policy directly. However, policy gradient is designed
    for continuous action spaces. More importantly, it will introduce high variance
    in the gradient. Actor-critic algorithms combine the advantages of DQN and policy
    gradient. Nonetheless, actor-critic will map the large discrete action space into
    a small continuous action space to ensure it is differentiable, which may cause
    potential information loss. Actor-critic uses DDPG and thus inherits disadvantages
    from DQN and DPG, including difficulty in determining the reward function and
    poor exploration ability.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Component Optimization in Deep Reinforcement Learning based RS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a few studies that use DRL in RS for goals other than improving recommendation
    performance or proposing new application domains. We split the literature based
    on the following components: environment, state representation, and reward function.
    Exisitng studies usually focus on optimizing one single component in the DRL setting
    (as illustrated in [Figure 1](#S1.F1 "In 1\. Introduction ‣ A Survey of Deep Reinforcement
    Learning in Recommender Systems: A Systematic Review and Future Directions")).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4. List of publications reviewed in this section
  prefs: []
  type: TYPE_NORMAL
- en: '| Component | Work |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Environment | (Shi et al., [2019a](#bib.bib84); Rohde et al., [2018](#bib.bib76);
    Shi et al., [2019b](#bib.bib85); Ie et al., [2019a](#bib.bib42); Shang et al.,
    [2019](#bib.bib83); Huang et al., [2020](#bib.bib41); Santana et al., [2020](#bib.bib77);
    Zhao et al., [2019a](#bib.bib127)) |'
  prefs: []
  type: TYPE_TB
- en: '| State | (Liu et al., [2018](#bib.bib61); Liu et al., [2020a](#bib.bib59);
    Liu et al., [2020c](#bib.bib62)) |'
  prefs: []
  type: TYPE_TB
- en: '| Reward | (Chen et al., [2018](#bib.bib16)) |'
  prefs: []
  type: TYPE_TB
- en: 3.3.1\. Environment Simulation and Reconstruction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many environments are available for evaluating deep reinforcement learning.
    Two popular ones are OpenAI gym-based environment (Brockman et al., [2016](#bib.bib9))
    and MuJoCo¹¹1http://mujoco.org/. Unfortunately, there is no standardized simulation
    platform or benchmark specific to reinforcement learning based recommender systems.
    Existing work on DRL in RS is usually evaluated through offline datasets or via
    deployment in real applications. The drawback for evaluating offline datasets
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different studies use different environment construction methods which leads
    to unfair comparison. For instance, some studies use the KG as the environment
    while some studies assume the environment is gym-like or design a simulator for
    specific tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With offline datasets, users’ dynamic interests, and environment dynamics are
    hard to maintain. Deploying the method into a real application is difficult for
    academic research as it takes time and costs money. Hence, a standardized simulation
    environment is a desirable solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b1804850302f7b678c24a9dbf0ebf66.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6. Left is the traditional MDP transition; Right is the POMDP which considers
    the environmental confounders such as social influence (Shang et al., [2019](#bib.bib83)).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several different studies that provide standardized gym²²2https://gym.openai.com/-based
    simulation platforms for DRL-based RS research in different scenarios. RecSim (Ie
    et al., [2019a](#bib.bib42)) is a configurable platform that supports sequential
    interaction between the system and users. RecSim contains three different tasks:
    interest evolution, interest exploration and long-term satisfaction. RecoGym (Rohde
    et al., [2018](#bib.bib76)) provides an environment for recommendation and advertising.
    In addition, RecoGym also provides simulation of online experiments such as A/B-tests.
    However, RecSim and RecoGym are designed for bandit behavior which means users’
    interests will not change over time. VirtualTB (Shi et al., [2019b](#bib.bib85))
    is proposed to relieve such problems. VirtualTB employs imitation learning to
    learn a user model to interact with the agent. GANs are employed to generate users’
    interests. Similar to VirtualTB, Recsimu (Zhao et al., [2019a](#bib.bib127)) uses
    a GAN to tackle the complex item distribution. In addition, PyRecGym (Shi et al.,
    [2019a](#bib.bib84)) accommodates standard benchmark datasets into a gym-based
    environment. MARS-Gym (Santana et al., [2020](#bib.bib77)) provides a benchmark
    framework for marketplace recommendation. (Huang et al., [2020](#bib.bib41)) suggests
    that existing simulation environments are biased because of biased logged data.
    Two common biases are discussed: popularity bias and positivity bias. To reduce
    the effect from those biases, SOFA introduces an Intermediate Bias Mitigation
    Step for debiasing purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One work discusses environment reconstruction by considering confounders. (Shang
    et al., [2019](#bib.bib83)) claims that users’ interests may be affected by social
    networks which may introduce extra bias to the state and affect the decision-making
    process. A multi-agent setting is introduced to treat the environment as an agent
    which can partially relieve the hidden confounder effect. Specifically, a deconfounded
    environment reconstruction method DEMER is proposed. Different from previously
    mentioned methods, DEMER assumes the environment is partially observed and models
    the whole recommendation task as a Partially Observed MDP (POMDP). Different from
    an MDP, a POMDP contains one more component observation $o\in\mathcal{O}$ and
    the action $a_{t}$ is derived based on the observation $o_{t}$ instead of the
    state $s_{t}$ by $a_{t}=\pi_{a}(o_{t})$. DEMER assumes there is a confounder policy
    $\pi_{h}$ for observation $o_{h}$ which is composed by $a_{t}$ and $o_{t}$: $a_{h}=\pi_{h}(a_{t},o_{t})$.
    Moreover, another observation $o_{b}$ is introduced to observe the transition
    as well. $\pi_{b}$ is the corresponding policy and $a_{b}=\pi_{b}(o_{b})=\pi_{b}(o_{t},a_{t},a_{h})$.
    DEMER uses generative adversarial imitation learning (GAIL) to imitate the policy
    $A,B$. Given trajectory $\{o_{t},o_{h},o_{b}\}$ for different policies $A$ and
    $B$, the objective function is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle(\pi_{a},\pi_{b},\pi_{h})=\operatorname*{arg\,min}_{(\pi_{a},\pi_{b},\pi_{h})}\mathbb{E}_{s\sim\tau}(L(s,a_{t},a_{b}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (35) |  | $\displaystyle\text{where }L(s,a_{t},a_{b})=\mathbb{E}_{(\pi_{a},\pi_{b},\pi_{h})}[\log
    D(s,a_{t},a_{b})]-\lambda\sum_{\pi\in\{\pi_{a},\pi_{b},\pi_{h}\}}H(\pi)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $L(\cdot)$ is the loss function, $D(\cdot)$ is a discriminator and $H(\pi)$
    is introduced in GAIL.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. State Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'State representation is another component in DRL-based RS which exists in both
    model-based and model-free methods. Liu et al. ([2018](#bib.bib61)) find that
    the state representation in model-free methods would affect recommendation performance.
    Existing studies usually directly use the embedding as the state representation.
    Liu et al. ([2020a](#bib.bib59)); Liu et al. ([2020c](#bib.bib62)) propose a supervised
    learning method to generate a better state representation by utilizing an attention
    mechanism and a pooling operation as shown in [Figure 7](#S3.F7 "In 3.3.2\. State
    Representation ‣ 3.3\. Component Optimization in Deep Reinforcement Learning based
    RS ‣ 3\. Deep Reinforcement Learning in Recommender Systems ‣ A Survey of Deep
    Reinforcement Learning in Recommender Systems: A Systematic Review and Future
    Directions"). Such a representation method requires training a representation
    network when training the main policy network, which increases the model complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95436f55716b2b057a9c5cd37fd9e4f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7. State representation used in works (Liu et al., [2020a](#bib.bib59);
    Liu et al., [2020c](#bib.bib62)). $h_{t}$ is the output of an attention layer
    that takes the representation of users’ history at time $t$ as input and $g(\cdot)$
    is the pooling operation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Robustness of Reward Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The reward function is crucial for methods involving DQN. A robust reward function
    can significantly improve training efficiency and performance. Kostrikov et al.
    ([2019](#bib.bib51)) find that the DQN may not receive the correct reward value
    when entering the absorbing state. That is, when the absorbing state is reached,
    the agent will receive zero reward and harm policy learning. The reason behind
    this is that when designing the environment zero reward is implicitly assigned
    to the absorbing state as it is hard to determine the reward value in such a state.
    Chen et al. ([2018](#bib.bib16)) propose a robust DQN method, which can stabilize
    the reward value when facing the absorbing state. The new reward formula can improve
    the robustness, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (36) |  | $\displaystyle r=\begin{cases}r_{t}&amp;\text{if }s_{t+1}\text{
    is an absorbing state}\\ r_{t}+\gamma Q_{\theta^{\prime}}(s_{t+1},a_{t+1})&amp;\text{otherwise}.\end{cases}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The major difference is that $r_{t}$ is assigned to the absorbing state to ensure
    the agent can continue learning. One remaining problem in current DRL-based RS
    is the reward sparsity, i.e,. the large state and action spaces make the reward
    sparsity problem more serious. One of the possible solution would be a better
    designed reward by using the reward shaping (Ng et al., [1999](#bib.bib70)).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Emerging Topics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While existing studies have established a solid foundation for DRL-based RS
    research, this section outlines several promising emerging research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Multi-Agent and Hierarchical Deep Reinforcement Learning-based RS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recommender systems are monolithic systems containing tasks such as searching,
    ranking, recommendation, advertising, personalization, and diverse stakeholders
    such as users and items. Most existing methods are based on single agent.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Agent Reinforcement Learning (MARL) is a subfield of reinforcement learning
    and it is capable of learning multiple policies and strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'While a single-agent reinforcement learning framework can only handle a single
    task, many studies consider the multi-task situation in RS and employ multi-agent
    DRL (MADRL) or hierarchical DRL (HDRL). HDRL (Kulkarni et al., [2016](#bib.bib52))
    is proposed to handle complex tasks by splitting such tasks into several small
    components and asks the agent to determine sub-policies. HDRL belongs to a single-agent
    reinforcement learning framework such that the agent contains a meta-controller
    and several controllers. The meta-controller splits the task, and the controllers
    learn the value and reward functions for designated tasks to get a series of sub-policies.
    There are a few studies already utilizing HDRL in RS. Xie et al. ([2021](#bib.bib106))
    target integrated recommendation to capture user preferences on both heterogeneous
    items and recommendation channels. Specifically, the meta-controller is used for
    item recommendation, and controllers aim to find the personalized channel according
    to user channel-level preferences. Zhang et al. ([2019a](#bib.bib114)) uses HDRL
    for course recommendation in MOOCs, which contains two different tasks: profile
    reviser and recommendation. The meta-controller aims to make course recommendations
    by using the revised profile pruned by the controllers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from HDRL, MADRL (Egorov, [2016](#bib.bib24)) introduces multiple
    agents to handle the sub-tasks. Gui et al. ([2019](#bib.bib34)) uses the MADRL
    for twitter mention recommendation where three agents are initialized. The three
    agents need to generate different representations for the following tasks: query
    text, historical text from authors and historical text from candidate users. Once
    the representations are finalized, the model will conduct the recommendation based
    on the concatenation of representations. Feng et al. ([2018](#bib.bib25)) and
    He et al. ([2020](#bib.bib37)) provide two different views of the communication
    mechanism in MADRL and demonstrate that agents could work collaboratively or individually.
    Zhao et al. ([2020e](#bib.bib130)) designs a MADRL framework for two tasks where
    two agents are designed to conduct advertising and recommendation respectively.
    Zhang et al. ([2017](#bib.bib120)) uses MADRL for collaborative recommendation
    where each agent is responsible for a single user. MADRL is adopted to help the
    recommender consider both collaboration and potential competition between users.
    Zhang et al. ([2021](#bib.bib118)) designs a charging recommender system for intelligent
    electric vehicles by using decentralized agents to handle sub-tasks and a centralized
    critic to make the final decision.'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical multi-agent RL (HMARL) (Makar et al., [2001](#bib.bib67)) proves
    that MARL and HRL can be combined. Recently, Yang et al. ([2018](#bib.bib110))
    introduces HMADRL into the continuous action space, which provides a direction
    for RS. Zhao et al. ([2020d](#bib.bib121)) uses HMARL for multi-goal recommendation
    where the meta-controller considers users’ long-term preferences and controllers
    focus on short-term click behavior. While the meta-controller and controllers
    in HDRL deal with sub-tasks that belong to a single task, HMARL focuses on multi-task
    or multi-goal learning where the meta-controller and controllers belong to different
    agents and deal with different tasks or goals.
  prefs: []
  type: TYPE_NORMAL
- en: HMADRL would be a suitable solution for future research work in DRL-based RS
    where HDRL can be used to split a complex task into several sub-tasks such as
    users’ long-term interests and short-term click behavior, and MADRL can jointly
    consider multiple factors such as advertising Zhao et al. ([2020d](#bib.bib121)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Inverse Deep Reinforcement Learning for RS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned, the reward function plays a critical role in DRL-based recommender
    systems. In many existing works, reward functions are manually designed. The common
    method uses users’ click behavior to represent the reward and to reflect users’
    interests. However, such a setting can not represent users’ long-term goals (Zou
    et al., [2019](#bib.bib135)) as clicking or not only depicts part of the feedback
    information from users. It requires significant effort to design a reward function,
    due to the large number of factors that can affect users’ decision, such as social
    engagement or bad product reviews, which may adversely affect recommendation performance.
    It is difficult to include all potential factors into the reward function because
    not every factor can be represented properly. A few works (Gong et al., [2019](#bib.bib32);
    Chen et al., [2020c](#bib.bib20)) show that manually designed reward functions
    can be omitted by employing inverse reinforcement learning (IRL) (Ng et al., [2000](#bib.bib71))
    or generative adversarial imitation learning (GAIL) (Ho and Ermon, [2016](#bib.bib38)).
    Such inverse DRL-based methods require using expert demonstration as the ground
    truth. However, expert demonstration is often hard to obtain for recommendation
    scenarios. Those two studies conduct experiments in an offline dataset-based simulation
    environment that can access expert demonstration. In contrast, Chen et al. ([2020c](#bib.bib20))
    use IRL as the main algorithm to train the agent while Gong et al. ([2019](#bib.bib32))
    use both demonstration and reward to train the agent. Zhao et al. ([2020b](#bib.bib122))
    also employ GAIL to improve recommendation performance. In this work, GAIL is
    used to learn the reasoning path inside the KG to provide side information to
    help the agent learn the policy. Although IRL achieves some progress in RS, the
    lack of demonstration is a key shortcoming that impedes adoption in RS. One possibility
    is to use the IRL method in casual reasoning to help improve interpretability (Bica
    et al., [2020](#bib.bib6)) thus boosting recommendation performance. Alternately,
    IRL may be suitable for learning users’ long-term and static behavior to support
    the reward function.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Graph Neural Networks for Boosting DRL-based RS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph data and KG are widely used in RS. Graph modeling enables an RS to leverage
    interactions between users and the recommender for reasoning or improving interpretability.
    According to existing studies about deep learning-based RS (Zhang et al., [2019b](#bib.bib116)),
    embedding is a technique used to get the representation for the input data. Graph
    embedding is a common solution to handle graph-like data. GCN is a type of graph
    embedding method which are broadly used in RS to process graph data. Wang et al.
    ([2019](#bib.bib95)) propose a variant of GCN to learn the embedding for KG. Specifically,
    they propose knowledge graph convolutional networks (KGCN) to capture the high-order
    structural proximity among entities in a knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: In DRL-based RS, graph data are handled similarly—s underthe transformed into
    an embedding and fed to the agent. Wang et al. ([2020a](#bib.bib99)) uses a traditional
    graph embedding method TransE (Bordes et al., [2013](#bib.bib7)) to generate the
    state representation for DRL-based RS. There are several studies that use GCN
    in DRL for recommendations under different settings. Jiang et al. ([2020](#bib.bib47))
    propose a graph convolutional RL (DGN) method which integrates the GCN into the
    Q-learning framework for general RL problems by replacing the state encoding layer
    with the GCN layer. Lei et al. ([2020](#bib.bib54)) extend this method into the
    deep learning field and apply it to recommender systems. To be specific, multiple
    GCN layers are employed to process the sub-graphs for a given item $i$. Chen et al.
    ([2020b](#bib.bib18)) employs KG inside the actor-critic algorithm to help the
    agent learn the policy. Specifically, the critic network contains a GCN layer
    to give weight to the graph and conduct searches in the graph to find an optimal
    path and hence guide the optimization of policy learning. However, such a method
    is relatively computationally expensive as it requires jointly training the GCN
    and the actor-critic network. Gong et al. ([2019](#bib.bib32)) adopts a Graph
    Attention Network (GAT) (Veličković et al., [2017](#bib.bib93)) into the actor-critic
    network to conduct recommendation. In addition, the GAT is used as an encoder
    to obtain a state representation.
  prefs: []
  type: TYPE_NORMAL
- en: A common way of using GCN or its variants in DRL-based RS is the state encoder.
    The related challenge is the difficulty for the environment to provide a graph-like
    input to the GCN.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Self-Supervised DRL-based RS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Self-supervised learning (SSL) is a technique in which the model is trained
    by itself without external label information. SSL-DRL is receiving growing interest
    in robotics  (Kahn et al., [2018](#bib.bib48); Zeng et al., [2018](#bib.bib113)).
    Kahn et al. ([2018](#bib.bib48)) shows that SSL can be used to learn the policy
    when doing navigation by providing real-world experience. Zeng et al. ([2018](#bib.bib113))
    demonstrates that SSL-DRL can be used to help the agent learn synergies between
    two similar policies, thus empowering the agent to conduct two different tasks.
    Recent advances in SSL RL show that SSL can also provide interpretability for
    RL, which is promising for interpretable RS research (Shi et al., [2020](#bib.bib86)).
    Shi et al. ([2020](#bib.bib86)) shows that SSL based RL can highlight the task-relevant
    information to guide the agent’s behavior. Moreover, Xin et al. ([2020](#bib.bib107))
    shows that SSL can be used to provide negative feedback for DRL-based RS to improve
    recommendation performance. To be specific, a self-supervised loss function is
    appended into the normal DRL loss function,
  prefs: []
  type: TYPE_NORMAL
- en: '| (37) |  | $\displaystyle-\sum_{i=1}^{n}Y_{i}\log\bigg{(}\frac{e^{y_{i}}}{\sum_{i^{\prime}=1}^{n}e^{y_{i^{\prime}}}}\bigg{)}+L_{\mathit{DRL}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $Y_{i}$ is an indicator function to show users interact with the item
    $i$ or not. $L_{\mathit{DRL}}$ could vary, if the DQN is adopted, [Equation 4](#S2.E4
    "In 2.2\. Preliminaries of Deep Reinforcement Learning ‣ 2\. Background ‣ A Survey
    of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and
    Future Directions") should be used. SSL demonstrates promising performance in
    visual representation in recent years, which would be a possible solution to generate
    the state representation as there are a few DRL-based RS studies that adopt CNNs
    to process image-like data and transform it into a state (Liu et al., [2020b](#bib.bib60);
    Gao et al., [2019](#bib.bib29)). Furthermore, as an unsupervised learning approach,
    SSL would provide a new direction about defining the reward function by learning
    common patterns between different states as well as multi-task learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Open Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we outline several open questions and challenges that exist
    in DRL-based RS research. We believe these issues could be critical for the future
    development of DRL-based RS.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Sample Efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sample inefficiency is a well-known challenge in model-free DRL methods. Model-free
    DRL requires a significant number of samples as there is no guarantee that the
    received state is useful. Normally, after a substantial number of episodes, the
    agent may start learning as the agent finally receives a useful state and reward
    signal. A common solution is the experience replay technique, which only works
    in off-policy methods. Experience replay still suffers the sample inefficiency
    problem (Schaul et al., [2015](#bib.bib78)) as not every past experience is worth
    replaying. Isele and Cosgun ([2018](#bib.bib44)) propose selected experience replay
    (SER) that only stores valuable experience into the replay buffer and thus improves
    sample efficiency. while traditional DRL environments only contain several³³3For
    example, the number of actions in MuJoCo is less than one hounded. candidate items,
    in DRL-based RS, the agent must deal with a significantly larger action space
    as RS may contain lots of candidate items. Existing DRL-based RS studies on traditional
    experience replay methods often demonstrate slow converge speed. Chen et al. ([2021](#bib.bib15))
    design a user model to improve the sample efficiency through auxiliary learning.
    Specifically, they apply the auxiliary loss with the state representation, and
    the model distinguishes low-activity users and asks the agent to update the recommendation
    policy based on high-activity users more frequently.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, model-based methods are more sample efficient. However, they
    introduce extra complexity as the agent is required to learn the environment model
    as well as the policy. Due to the extremely large action space and possibly large
    state space (depending on users’ contextual information) in RS, approximating
    the environment model and policy simultaneously becomes challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Exploration and Exploitation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The exploration and exploitation dilemma is a fundamental and challenging problem
    in reinforcement learning research and receives lots of attention in DRL. This
    dilemma describes a trade-off between obtaining new knowledge and the need to
    use that knowledge to improve performance. Many DQN-based methods focus on exploration
    before the replay buffer is full and exploitation afterward. Consequently, it
    requires an extremely large replay buffer to allow all possibilities in recommendation
    can be stored. DRN employs Dueling Bandit Gradient Descent (DBGD) (Yue and Joachims,
    [2009](#bib.bib112)) to encourage exploration while  (Chen et al., [2019c](#bib.bib19);
    He et al., [2020](#bib.bib37)) introduces a regularization or entropy term into
    the objective function to do so. (Ie et al., [2019b](#bib.bib43)) uses the sheer
    size of the action space to ensure sufficient exploration. (Wang et al., [2020a](#bib.bib99);
    Xian et al., [2019](#bib.bib104); Wang et al., [2020b](#bib.bib101)) uses a separate
    KG or elaborated graph exploration operation to conduct exploration. (Chen et al.,
    [2019a](#bib.bib14)) employs Boltzmann exploration to get the benefit of exploratory
    data without negatively impacting user experience. In addition, $\epsilon$-greedy
    is the most common technique used to encourage exploration  (Zou et al., [2020](#bib.bib136);
    Lei et al., [2020](#bib.bib54); Lei and Li, [2019](#bib.bib53); Zou et al., [2019](#bib.bib135);
    Lei et al., [2019](#bib.bib55); Wang et al., [2021](#bib.bib97); Cai et al., [2018](#bib.bib10);
    Liu et al., [2020b](#bib.bib60); Xie et al., [2021](#bib.bib106)). Remaining studies
    rely on a simulator to conduct exploration. However, it may suffer from noise
    and over-fitting (Xie et al., [2021](#bib.bib106)) because of the gap between
    simulation and real online application. For most DRL-based methods such as vanilla
    DQN, policy gradient, or actor-critic-based methods, $\epsilon$-greedy would be
    a good choice for exploration. In addition, injecting noise into the action space
    would also be helpful for those actor-critic-based methods (Lillicrap et al.,
    [2015](#bib.bib57)). For methods involving KGs, $\epsilon$-greedy may help, but
    the elaborated graph exploration methods may receive better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Generalizing from Simulation to Real-World Recommendation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing work generally trains DRL algorithms in simulation environments or
    offline datasets. Deploying DRL algorithms into real applications is challenging
    due to the gap between simulation and real-world applications. Simulation environments
    do not contain domain knowledge or social impact. They can not cover the domain
    knowledge and task-specific engineering in the real-world recommendation. How
    to bridge the gap between simulation and real applications is a challenging topic.
    Sim2real (Zhao et al., [2020a](#bib.bib123)) is a transfer learning approach that
    transfers DRL policies from simulation environments to reality. Sim2real uses
    domain adaption techniques to help agents transfer learned policy. Specifically,
    it adopts GANs to conduct adaption by generating different samples. RL-CycleGAN (Rao
    et al., [2020](#bib.bib75)) is a sim2real method for vision-based tasks. It uses
    CycleGAN (Zhu et al., [2017](#bib.bib133)) to conduct pixel-level domain adaption.
    Specifically, it maintains cycle consistency during GAN training and encourages
    the adapted image to retain certain attributes of the input image. In DRL-based
    RS, sim2real would be a possible solution for generalizing the learned policy
    from simulation environments to reality. However, sim2real is a new technique
    still under exploration. It shows an adequate capability in simple tasks and requires
    more effort to handle the complex task such as recommendation. We believe it is
    a workable solution for generalizing from simulation to reality.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Bias (Unfairness)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Chen et al. ([2020a](#bib.bib13)) observe that user behavior data are not experimental
    but observational, which leads to problems of bias and unfairness.
  prefs: []
  type: TYPE_NORMAL
- en: There are two reasons why bias is so common. First, the inherent characteristic
    of user behavior data is not experimental but observational. In other words, data
    that are fed into recommender systems are subject to selection bias (Schnabel
    et al., [2016](#bib.bib79)). For instance, users in a video recommendation system
    tend to watch, rate, and comment on those movies that they are interested in.
    Second, a distribution discrepancy exists, which means the distributions of users
    and items in the recommender system are not even. Recommender systems may suffer
    from ’popularity bias’, where popular items are recommended far more frequently
    than the others. However, the ignored products in the “long tail” can be equally
    critical for businesses as they are the ones less likely to be discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Friedman and Nissenbaum ([1996](#bib.bib27)) denote the unfairness as that the
    system systematically and unfairly discriminates against certain individuals or
    groups of individuals in favor of others.
  prefs: []
  type: TYPE_NORMAL
- en: A large number of studies explore dynamic recommendation systems by utilizing
    the agent mechanism in reinforcement learning (RL), considering the information
    seeking and decision-making as sequential interactions. How to evaluate a policy
    efficiently is a big challenge for RL-based recommenders. Online A/B tests are
    not only expensive and time-consuming but also sometimes hurt the user experience.
    Off-policy evaluation is an alternative strategy that historical user behavior
    data are used to evaluate the policy. However, user behavior data are biased,
    as mentioned before, which causes a gap between the policy of RL-based RS and
    the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: To eliminate the effects of bias and unfairness, Chen et al. ([2019a](#bib.bib14))
    use the inverse of the probability of historical policy to weight the policy gradients.
    Huang et al. ([2020](#bib.bib41)) introduce a debiasing step that corrects the
    biases presented in the logged data before it is used to simulate user behavior.
    Zou et al. ([2020](#bib.bib136)) propose to build a customer simulator that is
    designed to simulate the environment and handle the selection bias of logged data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5\. Explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although deep learning-based models can generally improve the performance of
    recommender systems, they are not easily interpretable. As a result, it becomes
    an important task to make recommender results explainable, along with providing
    high-quality recommendations. High explainability in recommender systems not only
    helps end-users understand the items recommended but also enables system designers
    to check the internal mechanisms of recommender systems. Zhang and Chen ([2020](#bib.bib119))
    review different information sources and various types of models that can facilitate
    explainable recommendation. Attention mechanisms and knowledge graph techniques
    currently play an important role in realizing explainability in RS.
  prefs: []
  type: TYPE_NORMAL
- en: Attention models have great advantages in both enhancing predictive performance
    and having greater explainability (Zhang et al., [2019c](#bib.bib117)). Wang et al.
    ([2018a](#bib.bib100)) introduce a reinforcement learning framework incorporated
    with an attention model for explainable recommendation. Firstly, it achieves model-agnosticism
    by separating the recommendation model from the explanation generator. Secondly,
    the agents that are instantiated by attention-based neural networks can generate
    sentence-level explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graphs contain rich information about users and items, which can help
    to generate intuitive and more tailored explanations for the recommendation system
    (Zhang and Chen, [2020](#bib.bib119)). Recent work has achieved greater explainability
    by using reinforcement and knowledge graph reasoning. The algorithm from (Xian
    et al., [2019](#bib.bib104)) learns to find a path that navigates from users to
    items of interest by interacting with the knowledge graph environment. Zhao et al.
    ([2020b](#bib.bib122)) extract imperfect path demonstrations with minimum labeling
    effort and propose an adversarial actor-critic model for demonstration-guided
    path-finding. Moreover, it achieves better recommendation accuracy and explainability
    by reinforcement learning and knowledge graph reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6\. Robustness on Adversarial Samples and Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial samples demonstrate that deep learning-based methods are vulnerable.
    Hence, robustness becomes an open question for both RS and DRL. Specifically,
    adversarial attack and defense in RS have received a lot of attention in recent
    years (Deldjoo et al., [2021](#bib.bib23)) as security is crucial in RS. Moreover,
    DRL policies are vulnerable to adversarial perturbations to agent’s observations (Lin
    et al., [2017](#bib.bib58)). Gleave et al. ([2020](#bib.bib31)) provide an adversarial
    attack method for perturbing the observations, thus affecting the learned policy.
    Hence, improving the robustness is the common interest for DRL and RS, which would
    be a critical problem for DRL-based RS. Cao et al. ([2020](#bib.bib11)) provide
    an adversarial attack detection method for DRL-based RS which uses the GRU to
    encode the action space into a low-dimensional space and design decoders to detect
    the potential attack. However, it only considers Fast Gradient Sign Method (FGSM)-based
    attacks and strategically-timed attacks (Lin et al., [2017](#bib.bib58)). Thus,
    it lacks the capability to detect other types of attack. Moreover, it only provides
    the detection method while the defence is still an opening question.
  prefs: []
  type: TYPE_NORMAL
- en: We believe zero-shot learning techniques would be a good direction for training
    a universal adversarial attack detector. For defence, it is still an open question
    for DRL-based RS, though recent advances in adversarial defence in DRL may provide
    some insights (Lütjens et al., [2020](#bib.bib64); Wang et al., [2020c](#bib.bib94);
    Chen et al., [2019d](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide a few potential future directions of DRL-based RS.
    Benefiting from recent advances in DRL research, we believe those topics can boost
    the progress of DRL-based RS research.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Causal and Counterfactual Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Causality is a generic relationship between a cause and effect. Moreover, inferring
    causal effects is a fundamental problem in many applications like computational
    advertising, search engines, and recommender systems (Bottou et al., [2013](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: Recently, some researchers have connected reinforcement learning with learning
    causality to improve the effects for solving sequential decision-making problems.
    Besides, Learning agents in reinforcement learning frameworks face a more complicated
    environment where a large number of heterogeneous data are integrated. From our
    point of view, causal relationships would be capable of improving the recommendation
    results by introducing the directionality of cause and the effect. The users’
    previous choices have impact on the subsequent actions. This can be cast as an
    interventional data generating the dynamics of recommender systems. By viewing
    a policy in RL as an intervention, we can detect unobserved confounders in RL
    and choose a policy on the expected reward to better estimate the causal effect
    (Shang et al., [2019](#bib.bib83)). Some studies improve RL models with causal
    knowledge as side information. Another line of work uses causal inference methods
    to achieve unbiased reward prediction (Guo et al., [2020](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: Yang et al. ([2021](#bib.bib109)) propose a Causal Inference Q-network which
    introduces observational inference into DRL by applying extra noise and uncertain
    inventions to improve resilience. Specifically, in this work, noise and uncertainty
    are added into the state space during the training state, and the agent is required
    to learn a causal inference model by considering the perturbation. Dasgupta et al.
    ([2019](#bib.bib21)) give the first demonstration that model-free reinforcement
    learning can be used for causal reasoning. They explore meta-reinforcement learning
    to solve the problem of causal reasoning. The agents trained by a recurrent network
    able to make causal inferences from observational data and output counterfactual
    predictions. Forney et al. ([2017](#bib.bib26)) bridge RL and causality by data-fusion
    for reinforcement learners. Specifically, online agents combine observations,
    experiments and counterfactual data to learn about the environment, even if unobserved
    confounders exist. Similarly, Gasse et al. ([2021](#bib.bib30)) make the model-based
    RL agents work in a causal way to explore the environment under the Partially-Observable
    Markov Decision Process (POMDP) setting. They consider interventional data and
    observational data jointly and interprete model-based reinforcement learning as
    a causal inference problem. In this way, they bridge the gap between RL and causality
    by relating common concepts in RL and causality.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding explainability in RL, Madumal et al. ([2020](#bib.bib66)) propose
    to explain the behavior of agents in reinforcement learning with the help of causal
    science. The authors encode causal relationships and learn a structural causal
    model in RL, which is used to generate explanations based on counterfactual analysis.
    With counterfactual exploration, this work is able to generate two contrastive
    explanations for ‘why’ and ‘why not’ questions.
  prefs: []
  type: TYPE_NORMAL
- en: It is so important to search for a Directed Acyclic Graph (DAG) in causal discovery.
    Considering traditional methods rely on local heuristics and predefined score
    functions, Zhu et al. ([2019](#bib.bib134)) propose to use reinforcement learning
    to search DAG for causal discovery. They use observational data as an input, RL
    agents as a search strategy and output the causal graph generated from an encoder-decoder
    NN model.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Offline DRL and Meta DRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recommender systems often need to deal with multiple scenarios such as joint
    recommendation and adverting, offline DRL and meta DRL provide a promising direction
    for achieving multiple scenarios at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Offline DRL is a new paradigm of DRL that can be combined with existing methods
    such as self-supervised learning and transfer learning to move toward real-world
    settings. Offline DRL (Levine et al., [2020](#bib.bib56)) (also known as batch
    DRL) is designed for tasks which contain huge amounts of data. Given a large dataset
    that contains past interactions, offline DRL uses the dataset for training across
    many epochs but does not interact with the environment. Offline DRL provides a
    solution that can be generalized to new scenarios as it was trained by a large
    sized dataset. Such generalization ability is critical to RSs, which may need
    to deal with multiple scenarios or multiple customers. While offline DRL could
    provide a new direction for DRL-based RS, it still faces a few problems regarding
    handling the distributional shifts between existing datasets and real-world interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Meta DRL (Wang et al., [2016](#bib.bib96)) is defined as meta learning in the
    filed of DRL. Meta DRL is another approach to help agents to generalize to new
    tasks or environments. Different from offline DRL, meta DRL contains a memory
    unit which is formed by the recurrent neural network to memorize the common knowledge
    for different tasks. Different from offline DRL, meta DRL does not require a large
    amount of data to train.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Further Developments in Actor-Critic Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An actor-critic method uses the traditional policy gradient method, which suffers
    from the high variance problem due to the gap between behavior policy (i.e., the
    policy that is being used by an agent for action select) and target policy (i.e.,
    the policy that an agent is trying to learn). A method commonly used to relieve
    the high variance problem is Advantage Actor-critic (A2C). Different from traditional
    actor-critic methods, A2C uses an advantage function to replace the Q-function
    inside the critic network. The advantage function $A(s_{t})$ is defined as the
    expected value of the TD-error. The new objective function for policy gradient
    can be written as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (38) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}[\sum_{t=1}^{T}\underbrace{(Q(s_{t},a_{t})-V(s_{t}))}_{A(s_{t})}\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: However, A2C still uses DDPG as the main training algorithm, which may suffer
    function approximation errors when estimating the Q value. Twin-Delayed DDPG (TD3) (Fujimoto
    et al., [2018](#bib.bib28)) is designed to improve the function approximation
    problem in DDPG which uses clipped double Q-learning to update the critic. The
    gradient update can be expressed as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (39) |  | $\displaystyle\mathbb{E}_{\tau\sim d_{\pi_{\theta}}}[\sum_{t=1}^{T}r(s_{t},a_{t})+\gamma\min(Q_{1}(s_{t},a_{t}+\epsilon),Q_{2}(s_{t},a_{t}+\epsilon))\sum_{t=1}^{T}\nabla_{\theta}\log\pi_{\theta}(s_{t},a_{t})].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon\sim\textit{clip}(\mathcal{N}(0,\sigma,-c,c))$, $\sigma$ is the
    standard deviation and $c$ is a constant for clipping.
  prefs: []
  type: TYPE_NORMAL
- en: Another two ways to improve actor-critic methods are Trust Region Policy Optimization
    (TRPO) (Schulman et al., [2015](#bib.bib80)) and Proximal Policy Optimization
    (PPO) (Schulman et al., [2017](#bib.bib81)), which focus on modification of the
    advantage function. TRPO aims to limit the step size for each gradient to ensure
    it will not change too much. The core idea is to add a constraint to the advantage
    function,
  prefs: []
  type: TYPE_NORMAL
- en: '| (40) |  | $\displaystyle\frac{\pi(a&#124;s)}{\pi_{old}(a&#124;s)}A(s),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the KL divergence will be used to measure the distance between the current
    policy and the old policy is small enough. PPO has the same goal as TRPO which
    is to try to find the biggest possible improvement step on a policy using the
    current data. PPO is a simplified version of TRPO which introduces the clip operation,
  prefs: []
  type: TYPE_NORMAL
- en: '| (41) |  | $\displaystyle\min\bigg{(}\frac{\pi(a&#124;s)}{\pi_{old}(a&#124;s)}A(s),\text{clip}\bigg{(}\frac{\pi(a&#124;s)}{\pi_{old}(a&#124;s)}A(s),1-\epsilon,1+\epsilon\bigg{)}A(s)\bigg{)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Soft Actor-Critic (SAC) (Haarnoja et al., [2018](#bib.bib36)) is another promising
    variant of the actor-critic algorithm and is widely used in DRL research. SAC
    uses the entropy term to encourage the agent to explore, which could be a possible
    direction to solve the exploration and exploitation dilemma. Moreover, SAC assigns
    an equal probability to actions that are equally attractive to the agent to capture
    those near-optimal policies. An example of related work (He et al., [2020](#bib.bib37))
    uses SAC to improve the stability of the training process in RS.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we provide a comprehensive overview the use of deep reinforcement
    learning in recommender systems. We introduce a classification scheme for existing
    studies and discuss them by category. We also provide an overview of such existing
    emerging topics and point out a few promising directions. We hope this survey
    can provide a systematic understanding of the key concepts in DRL-based RS and
    valuable insights for future research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. (2017) Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
    2017. Constrained policy optimization. In *International Conference on Machine
    Learning*. PMLR, 22–31.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Afsar et al. (2021) M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2021. Reinforcement
    learning based recommender systems: A survey. *arXiv preprint arXiv:2101.06286*
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arulkumaran et al. (2017) Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage,
    and Anil Anthony Bharath. 2017. Deep reinforcement learning: A brief survey. *IEEE
    Signal Processing Magazine* 34, 6 (2017), 26–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2019) Xueying Bai, Jian Guan, and Hongning Wang. 2019. A Model-Based
    Reinforcement Learning with Adversarial Training for Online Recommendation. *Advances
    in Neural Information Processing Systems* 32 (2019), 10735–10746.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bica et al. (2020) Ioana Bica, Daniel Jarrett, Alihan Hüyük, and Mihaela van der
    Schaar. 2020. Learning” What-if” Explanations for Sequential Decision-Making.
    In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bordes et al. (2013) Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
    Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling
    multi-relational data. In *Neural Information Processing Systems (NIPS)*. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bottou et al. (2013) Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela,
    Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard,
    and Ed Snelson. 2013. Counterfactual Reasoning and Learning Systems: The Example
    of Computational Advertising. *Journal of Machine Learning Research* 14, 11 (2013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brockman et al. (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
    Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. Openai gym. *arXiv
    preprint arXiv:1606.01540* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2018) Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, and Yiwei
    Zhang. 2018. Reinforcement Mechanism Design for e-commerce. In *Proceedings of
    the 2018 World Wide Web Conference*. 1339–1348.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2020) Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang, and
    Wei Emma Zhang. 2020. Adversarial Attacks and Detection on Reinforcement Learning-Based
    Interactive Recommender Systems. In *Proceedings of the 43rd International ACM
    SIGIR Conference on Research and Development in Information Retrieval*. 1669–1672.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019b) Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang,
    Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019b. Large-scale interactive recommendation
    with tree-structured policy gradient. In *Proceedings of the AAAI Conference on
    Artificial Intelligence*, Vol. 33. 3312–3320.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020a) Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang,
    and Xiangnan He. 2020a. Bias and Debias in Recommender System: A Survey and Future
    Directions. arXiv:arXiv:2010.03240'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019a) Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois
    Belletti, and Ed H Chi. 2019a. Top-k off-policy correction for a REINFORCE recommender
    system. In *Proceedings of the Twelfth ACM International Conference on Web Search
    and Data Mining*. 456–464.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Minmin Chen, Bo Chang, Can Xu, and Ed H Chi. 2021. User Response
    Models to Improve a REINFORCE Recommender System. In *Proceedings of the 14th
    ACM International Conference on Web Search and Data Mining*. 121–129.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang,
    and Hai-Hong Tang. 2018. Stabilizing reinforcement learning in dynamic environment
    with application to online recommendation. In *Proceedings of the 24th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 1187–1196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019d) Tong Chen, Jiqiang Liu, Yingxiao Xiang, Wenjia Niu, Endong
    Tong, and Zhen Han. 2019d. Adversarial attack and defense in reinforcement learning-from
    AI security view. *Cybersecurity* 2, 1 (2019), 1–22.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020b) Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wenjie
    Zhang, et al. 2020b. Knowledge-guided deep reinforcement learning for interactive
    recommendation. In *2020 International Joint Conference on Neural Networks (IJCNN)*.
    IEEE, 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019c) Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi,
    and Le Song. 2019c. Generative adversarial user model for reinforcement learning
    based recommendation system. In *International Conference on Machine Learning*.
    PMLR, 1052–1061.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020c) Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei
    Xu, and Liming Zhu. 2020c. Generative Inverse Deep Reinforcement Learning for
    Online Recommendation. *arXiv preprint arXiv:2011.02248* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasgupta et al. (2019) Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic,
    Pedro Ortega, David Raposo, Edward Hughes, Peter Battaglia, Matthew Botvinick,
    and Zeb Kurth-Nelson. 2019. Causal reasoning from meta-reinforcement learning.
    *arXiv preprint arXiv:1901.08162* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Degris et al. (2012) Thomas Degris, Martha White, and Richard S Sutton. 2012.
    Off-policy actor-critic. *arXiv preprint arXiv:1205.4839* (2012).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deldjoo et al. (2021) Yashar Deldjoo, Tommaso Di Noia, and Felice Antonio Merra.
    2021. A survey on adversarial recommender systems: from attack/defense strategies
    to generative adversarial networks. *ACM Computing Surveys (CSUR)* 54, 2 (2021),
    1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Egorov (2016) Maxim Egorov. 2016. Multi-agent deep reinforcement learning.
    *CS231n: convolutional neural networks for visual recognition* (2016), 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2018) Jun Feng, Heng Li, Minlie Huang, Shichen Liu, Wenwu Ou,
    Zhirong Wang, and Xiaoyan Zhu. 2018. Learning to collaborate: Multi-scenario ranking
    via multi-agent reinforcement learning. In *Proceedings of the 2018 World Wide
    Web Conference*. 1939–1948.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forney et al. (2017) Andrew Forney, Judea Pearl, and Elias Bareinboim. 2017.
    Counterfactual data-fusion for online reinforcement learners. In *International
    Conference on Machine Learning*. PMLR, 1156–1164.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Friedman and Nissenbaum (1996) Batya Friedman and Helen Nissenbaum. 1996. Bias
    in computer systems. *ACM Transactions on Information Systems (TOIS)* 14, 3 (1996),
    330–347.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujimoto et al. (2018) Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing
    function approximation error in actor-critic methods. In *International Conference
    on Machine Learning*. PMLR, 1587–1596.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2019) Rong Gao, Haifeng Xia, Jing Li, Donghua Liu, Shuai Chen,
    and Gang Chun. 2019. DRCGR: Deep reinforcement learning framework incorporating
    CNN and GAN-based for interactive recommendation. In *2019 IEEE International
    Conference on Data Mining (ICDM)*. IEEE, 1048–1053.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gasse et al. (2021) Maxime Gasse, Damien Grasset, Guillaume Gaudron, and Pierre-Yves
    Oudeyer. 2021. Causal Reinforcement Learning using Observational and Interventional
    Data. arXiv:2106.14421 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gleave et al. (2020) Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey
    Levine, and Stuart Russell. 2020. Adversarial Policies: Attacking Deep Reinforcement
    Learning. In *International Conference on Learning Representations*. [https://openreview.net/forum?id=HJgEMpVFwB](https://openreview.net/forum?id=HJgEMpVFwB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gong et al. (2019) Yu Gong, Yu Zhu, Lu Duan, Qingwen Liu, Ziyu Guan, Fei Sun,
    Wenwu Ou, and Kenny Q Zhu. 2019. Exact-k recommendation via maximal clique optimization.
    In *Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*. 617–626.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
    2014. Generative adversarial networks. *arXiv preprint arXiv:1406.2661* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gui et al. (2019) Tao Gui, Peng Liu, Qi Zhang, Liang Zhu, Minlong Peng, Yunhua
    Zhou, and Xuanjing Huang. 2019. Mention recommendation in Twitter with cooperative
    multi-agent reinforcement learning. In *Proceedings of the 42nd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 535–544.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Ruocheng Guo, Lu Cheng, Jundong Li, P. Richard Hahn, and
    Huan Liu. 2020. A Survey of Learning Causality with Data: Problems and Methods.
    *ACM Comput. Surv.* 53, 4, Article 75 (July 2020), 37 pages. [https://doi.org/10.1145/3397269](https://doi.org/10.1145/3397269)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
    Levine. 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement
    learning with a stochastic actor. In *International conference on machine learning*.
    PMLR, 1861–1870.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) Xu He, Bo An, Yanghua Li, Haikai Chen, Rundong Wang, Xinrun
    Wang, Runsheng Yu, Xin Li, and Zhirong Wang. 2020. Learning to Collaborate in
    Multi-Module Recommendation via Multi-Agent Reinforcement Learning without Communication.
    In *Fourteenth ACM Conference on Recommender Systems*. 210–219.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho and Ermon (2016) Jonathan Ho and Stefano Ermon. 2016. Generative adversarial
    imitation learning. *arXiv preprint arXiv:1606.03476* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong et al. (2020) Daocheng Hong, Yang Li, and Qiwen Dong. 2020. Nonintrusive-Sensing
    and Reinforcement-Learning Based Adaptive Personalized Music Recommendation. In
    *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development
    in Information Retrieval*. 1721–1724.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2018) Yujing Hu, Qing Da, Anxiang Zeng, Yang Yu, and Yinghui Xu.
    2018. Reinforcement learning to rank in e-commerce search engine: Formalization,
    analysis, and application. In *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 368–377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2020) Jin Huang, Harrie Oosterhuis, Maarten de Rijke, and Herke
    van Hoof. 2020. Keeping Dataset Biases out of the Simulation: A Debiased Simulator
    for Reinforcement Learning based Recommender Systems. In *Fourteenth ACM Conference
    on Recommender Systems*. 190–199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ie et al. (2019a) Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit
    Narvekar, Jing Wang, Rui Wu, and Craig Boutilier. 2019a. Recsim: A configurable
    simulation platform for recommender systems. *arXiv preprint arXiv:1909.04847*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ie et al. (2019b) Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh
    Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019b. SlateQ:
    A Tractable Decomposition for Reinforcement Learning with Recommendation Sets.
    In *Proceedings of the Twenty-eighth International Joint Conference on Artificial
    Intelligence (IJCAI-19)*. Macau, China, 2592–2599.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isele and Cosgun (2018) David Isele and Akansel Cosgun. 2018. Selective experience
    replay for lifelong learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2016) Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki,
    Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. 2016. Reinforcement
    learning with unsupervised auxiliary tasks. *arXiv preprint arXiv:1611.05397*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2020) Shenggong Ji, Zhaoyuan Wang, Tianrui Li, and Yu Zheng. 2020.
    Spatio-temporal feature fusion for dynamic taxi route recommendation via deep
    reinforcement learning. *Knowledge-Based Systems* 205 (2020), 106302.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2020) Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu.
    2020. Graph Convolutional Reinforcement Learning. In *International Conference
    on Learning Representations*. [https://openreview.net/forum?id=HkxdQkSYDB](https://openreview.net/forum?id=HkxdQkSYDB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kahn et al. (2018) Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel,
    and Sergey Levine. 2018. Self-supervised deep reinforcement learning with generalized
    computation graphs for robot navigation. In *2018 IEEE International Conference
    on Robotics and Automation (ICRA)*. IEEE, 5129–5136.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. In *International Conference
    on Learning Representations (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Konda and Tsitsiklis (2000) Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic
    algorithms. In *Advances in neural information processing systems*. Citeseer,
    1008–1014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kostrikov et al. (2019) Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi,
    Sergey Levine, and Jonathan Tompson. 2019. Discriminator-Actor-Critic: Addressing
    Sample Inefficiency and Reward Bias in Adversarial Imitation Learning. In *International
    Conference on Learning Representations*. [https://openreview.net/forum?id=Hk4fpoA5Km](https://openreview.net/forum?id=Hk4fpoA5Km)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kulkarni et al. (2016) Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi,
    and Josh Tenenbaum. 2016. Hierarchical deep reinforcement learning: Integrating
    temporal abstraction and intrinsic motivation. *Advances in neural information
    processing systems* 29 (2016), 3675–3683.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei and Li (2019) Yu Lei and Wenjie Li. 2019. Interactive recommendation with
    user-specific deep reinforcement learning. *ACM Transactions on Knowledge Discovery
    from Data (TKDD)* 13, 6 (2019), 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2020) Yu Lei, Hongbin Pei, Hanqi Yan, and Wenjie Li. 2020. Reinforcement
    learning based recommendation with graph convolutional q-network. In *Proceedings
    of the 43rd International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 1757–1760.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2019) Yu Lei, Zhitao Wang, Wenjie Li, and Hongbin Pei. 2019. Social
    Attentive Deep Q-network for Recommendation. In *Proceedings of the 42nd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 1189–1192.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin
    Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on
    open problems. *arXiv preprint arXiv:2005.01643* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillicrap et al. (2015) Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous
    control with deep reinforcement learning. *arXiv preprint arXiv:1509.02971* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2017) Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih,
    Ming-Yu Liu, and Min Sun. 2017. Tactics of adversarial attack on deep reinforcement
    learning agents. *arXiv preprint arXiv:1703.06748* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Feng Liu, Huifeng Guo, Xutao Li, Ruiming Tang, Yunming Ye,
    and Xiuqiang He. 2020a. End-to-end deep reinforcement learning based recommendation
    with supervised embedding. In *Proceedings of the 13th International Conference
    on Web Search and Data Mining*. 384–392.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Feng Liu, Ruiming Tang, Huifeng Guo, Xutao Li, Yunming Ye,
    and Xiuqiang He. 2020b. Top-aware reinforcement learning based recommendation.
    *Neurocomputing* 417 (2020), 255–269.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye,
    Haokun Chen, Huifeng Guo, and Yuzhou Zhang. 2018. Deep reinforcement learning
    based recommendation with explicit user-item interactions modeling. *arXiv preprint
    arXiv:1810.12027* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020c) Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye,
    Haokun Chen, Huifeng Guo, Yuzhou Zhang, and Xiuqiang He. 2020c. State representation
    modeling for deep reinforcement learning based recommendation. *Knowledge-Based
    Systems* 205 (2020), 106170.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2015) Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan
    Zhang. 2015. Recommender system application developments: a survey. *Decision
    Support Systems* 74 (2015), 12–32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lütjens et al. (2020) Björn Lütjens, Michael Everett, and Jonathan P How. 2020.
    Certified adversarial robustness for deep reinforcement learning. In *Conference
    on Robot Learning*. PMLR, 1328–1337.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020) Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi
    Tang, Lichan Hong, and Ed H Chi. 2020. Off-policy learning in two-stage recommender
    systems. In *Proceedings of The Web Conference 2020*. 463–473.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madumal et al. (2020) Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank
    Vetere. 2020. Explainable reinforcement learning through a causal lens. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 34. 2493–2500.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makar et al. (2001) Rajbala Makar, Sridhar Mahadevan, and Mohammad Ghavamzadeh.
    2001. Hierarchical multi-agent reinforcement learning. In *Proceedings of the
    fifth international conference on Autonomous agents*. 246–253.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
    Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement
    learning. *nature* 518, 7540 (2015), 529–533.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montazeralghaem et al. (2020) Ali Montazeralghaem, Hamed Zamani, and James Allan.
    2020. A Reinforcement Learning Framework for Relevance Feedback. In *Proceedings
    of the 43rd International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 59–68.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng et al. (1999) Andrew Y Ng, Daishi Harada, and Stuart Russell. 1999. Policy
    invariance under reward transformations: Theory and application to reward shaping.
    In *Icml*, Vol. 99. 278–287.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ng et al. (2000) Andrew Y Ng, Stuart J Russell, et al. 2000. Algorithms for
    inverse reinforcement learning.. In *Icml*, Vol. 1. 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oyeleke et al. (2018) Richard O Oyeleke, Chen-Yeou Yu, and Carl K Chang. 2018.
    Situ-centric reinforcement learning for recommendation of tasks in activities
    of daily living in smart homes. In *2018 IEEE 42nd Annual Computer Software and
    Applications Conference (COMPSAC)*, Vol. 2\. IEEE, 317–322.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2019) Feiyang Pan, Qingpeng Cai, Pingzhong Tang, Fuzhen Zhuang,
    and Qing He. 2019. Policy gradients for contextual recommendations. In *The World
    Wide Web Conference*. 1421–1431.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pei et al. (2019) Changhua Pei, Xinru Yang, Qing Cui, Xiao Lin, Fei Sun, Peng
    Jiang, Wenwu Ou, and Yongfeng Zhang. 2019. Value-aware recommendation based on
    reinforcement profit maximization. In *The World Wide Web Conference*. 3123–3129.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rao et al. (2020) Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian
    Ibarz, and Mohi Khansari. 2020. Rl-cyclegan: Reinforcement learning aware simulation-to-real.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    11157–11166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rohde et al. (2018) David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile,
    and Alexandros Karatzoglou. 2018. Recogym: A reinforcement learning environment
    for the problem of product recommendation in online advertising. *arXiv preprint
    arXiv:1808.00720* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Santana et al. (2020) Marlesson RO Santana, Luckeciano C Melo, Fernando HF
    Camargo, Bruno Brandão, Anderson Soares, Renan M Oliveira, and Sandor Caetano.
    2020. MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems
    for Marketplaces. *arXiv preprint arXiv:2010.07035* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaul et al. (2015) Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
    2015. Prioritized experience replay. *arXiv preprint arXiv:1511.05952* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schnabel et al. (2016) Tobias Schnabel, Adith Swaminathan, Ashudeep Singh,
    Navin Chandak, and Thorsten Joachims. 2016. Recommendations as Treatments: Debiasing
    Learning and Evaluation. arXiv:arXiv:1602.05352'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael
    Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In *International
    conference on machine learning*. PMLR, 1889–1897.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sermanet et al. (2018) Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine
    Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. 2018. Time-contrastive
    networks: Self-supervised learning from video. In *2018 IEEE International Conference
    on Robotics and Automation (ICRA)*. IEEE, 1134–1141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shang et al. (2019) Wenjie Shang, Yang Yu, Qingyang Li, Zhiwei Qin, Yiping Meng,
    and Jieping Ye. 2019. Environment reconstruction with hidden confounders for reinforcement
    learning based recommendation. In *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 566–576.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2019a) Bichen Shi, Makbule Gulcin Ozsoy, Neil Hurley, Barry Smyth,
    Elias Z Tragos, James Geraci, and Aonghus Lawlor. 2019a. PyRecGym: a reinforcement
    learning gym for recommender systems. In *Proceedings of the 13th ACM Conference
    on Recommender Systems*. 491–495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2019b) Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang
    Zeng. 2019b. Virtual-taobao: Virtualizing real-world online retail environment
    for reinforcement learning. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 33\. 4902–4909.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2020) Wenjie Shi, Gao Huang, Shiji Song, Zhuoyuan Wang, Tingyu Lin,
    and Cheng Wu. 2020. Self-Supervised Discovering of Interpretable Features for
    Reinforcement Learning. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. (2014) David Silver, Guy Lever, Nicolas Heess, Thomas Degris,
    Daan Wierstra, and Martin Riedmiller. 2014. Deterministic policy gradient algorithms.
    In *International conference on machine learning*. PMLR, 387–395.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2021) Ying Sun, Fuzhen Zhuang, Hengshu Zhu, Qing He, and Hui Xiong.
    2021. Cost-Effective and Interpretable Job Skill Recommendation with Deep Reinforcement
    Learning. In *Proceedings of the Web Conference 2021*. 3827–3838.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takanobu et al. (2019) Ryuichi Takanobu, Tao Zhuang, Minlie Huang, Jun Feng,
    Haihong Tang, and Bo Zheng. 2019. Aggregating e-commerce search results from heterogeneous
    sources via hierarchical reinforcement learning. In *The World Wide Web Conference*.
    1771–1781.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang and Wang (2018) Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential
    recommendation via convolutional sequence embedding. In *Proceedings of the Eleventh
    ACM International Conference on Web Search and Data Mining*. 565–573.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tavakoli et al. (2018) Arash Tavakoli, Fabio Pardo, and Petar Kormushev. 2018.
    Action branching architectures for deep reinforcement learning. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Hasselt et al. (2016) Hado Van Hasselt, Arthur Guez, and David Silver. 2016.
    Deep reinforcement learning with double q-learning. In *Proceedings of the AAAI
    Conference on Artificial Intelligence*, Vol. 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks.
    *arXiv preprint arXiv:1710.10903* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Feng Wang, Chen Zhong, M Cenk Gursoy, and Senem Velipasalar.
    2020c. Defense strategies against adversarial jamming attacks via deep reinforcement
    learning. In *2020 54th annual conference on information sciences and systems
    (CISS)*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo.
    2019. Knowledge graph convolutional networks for recommender systems. In *The
    world wide web conference*. 3307–3313.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2016) Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer,
    Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick.
    2016. Learning to reinforcement learn. *arXiv preprint arXiv:1611.05763* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Kai Wang, Zhene Zou, Qilin Deng, Runze Wu, Jianrong Tao,
    Changjie Fan, Liang Chen, and Peng Cui. 2021. Reinforcement Learning with a Disentangled
    Universal Value Function for Item Recommendation. In *Proceedings of the AAAI
    conference on artificial intelligence*, Vol. 35\. 4427–4435.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018b) Lu Wang, Wei Zhang, Xiaofeng He, and Hongyuan Zha. 2018b.
    Supervised reinforcement learning with recurrent neural network for dynamic treatment
    recommendation. In *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. 2447–2456.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Pengfei Wang, Yu Fan, Long Xia, Wayne Xin Zhao, Shaozhang
    Niu, and Jimmy Huang. 2020a. KERL: A knowledge-guided reinforcement learning model
    for sequential recommendation. In *Proceedings of the 43rd International ACM SIGIR
    Conference on Research and Development in Information Retrieval*. 209–218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018a) Xiting Wang, Yiru Chen, Jie Yang, Le Wu, Zhengtao Wu, and
    Xing Xie. 2018a. A reinforcement learning framework for explainable recommendation.
    In *2018 IEEE international conference on data mining (ICDM)*. IEEE, 587–596.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Xiang Wang, Yaokun Xu, Xiangnan He, Yixin Cao, Meng Wang,
    and Tat-Seng Chua. 2020b. Reinforced negative sampling over knowledge graph for
    recommendation. In *Proceedings of The Web Conference 2020*. 99–109.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watkins and Dayan (1992) Christopher JCH Watkins and Peter Dayan. 1992. Q-learning.
    *Machine learning* 8, 3-4 (1992), 279–292.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (1992) Ronald J Williams. 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. *Machine learning* 8, 3-4
    (1992), 229–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xian et al. (2019) Yikun Xian, Zuohui Fu, S Muthukrishnan, Gerard De Melo, and
    Yongfeng Zhang. 2019. Reinforcement knowledge graph reasoning for explainable
    recommendation. In *Proceedings of the 42nd international ACM SIGIR conference
    on research and development in information retrieval*. 285–294.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2020) Yilin Xiao, Liang Xiao, Xiaozhen Lu, Hailu Zhang, Shui Yu,
    and H Vincent Poor. 2020. Deep reinforcement learning based user profile perturbation
    for privacy aware recommendation. *IEEE Internet of Things Journal* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2021) Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu
    Lin. 2021. Hierarchical Reinforcement Learning for Integrated Recommendation.
    In *Proceedings of AAAI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xin et al. (2020) Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M
    Jose. 2020. Self-Supervised Reinforcement Learning for Recommender Systems. In
    *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development
    in Information Retrieval*. 931–940.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Jun Xu, Zeng Wei, Long Xia, Yanyan Lan, Dawei Yin, Xueqi Cheng,
    and Ji-Rong Wen. 2020. Reinforcement Learning to Rank with Pairwise Policy Gradient.
    In *Proceedings of the 43rd International ACM SIGIR Conference on Research and
    Development in Information Retrieval*. 509–518.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021) Chao-Han Huck Yang, I Hung, Te Danny, Yi Ouyang, and Pin-Yu
    Chen. 2021. Causal Inference Q-Network: Toward Resilient Reinforcement Learning.
    *arXiv preprint arXiv:2102.09677* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2018) Zhaoyang Yang, Kathryn Merrick, Lianwen Jin, and Hussein A
    Abbass. 2018. Hierarchical deep reinforcement learning for continuous action control.
    *IEEE transactions on neural networks and learning systems* 29, 11 (2018), 5174–5184.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019) Tong Yu, Yilin Shen, Ruiyi Zhang, Xiangyu Zeng, and Hongxia
    Jin. 2019. Vision-language recommendation via attribute augmented multimodal reinforcement
    learning. In *Proceedings of the 27th ACM International Conference on Multimedia*.
    39–47.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yue and Joachims (2009) Yisong Yue and Thorsten Joachims. 2009. Interactively
    optimizing information retrieval systems as a dueling bandits problem. In *Proceedings
    of the 26th Annual International Conference on Machine Learning*. 1201–1208.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2018) Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto
    Rodriguez, and Thomas Funkhouser. 2018. Learning synergies between pushing and
    grasping with self-supervised deep reinforcement learning. In *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*. IEEE, 4238–4245.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) Jing Zhang, Bowen Hao, Bo Chen, Cuiping Li, Hong Chen,
    and Jimeng Sun. 2019a. Hierarchical reinforcement learning for course recommendation
    in MOOCs. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 33. 435–442.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019d) Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, and Changyou
    Chen. 2019d. Text-Based Interactive Recommendation via Constraint-Augmented Reinforcement
    Learning. In *Advances in Neural Information Processing Systems*, H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.),
    Vol. 32\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2019/file/52130c418d4f02c74f74a5bc1f8020b2-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/52130c418d4f02c74f74a5bc1f8020b2-Paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019b) Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019b. Deep
    learning based recommender system: A survey and new perspectives. *ACM Computing
    Surveys (CSUR)* 52, 1 (2019), 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019c) Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019c. Deep
    Learning Based Recommender System: A Survey and New Perspectives. *ACM Comput.
    Surv.* 52, 1, Article 5 (Feb. 2019), 38 pages. [https://doi.org/10.1145/3285029](https://doi.org/10.1145/3285029)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Weijia Zhang, Hao Liu, Fan Wang, Tong Xu, Haoran Xin, Dejing
    Dou, and Hui Xiong. 2021. Intelligent Electric Vehicle Charging Recommendation
    Based on Multi-Agent Reinforcement Learning. In *Proceedings of the Web Conference
    2021*. 1856–1867.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Chen (2020) Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation:
    A Survey and New Perspectives. *Foundations and Trends® in Information Retrieval*
    14, 1 (2020), 1–101. [https://doi.org/10.1561/1500000066](https://doi.org/10.1561/1500000066)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Yang Zhang, Chenwei Zhang, and Xiaozhong Liu. 2017. Dynamic
    scholarly collaborator recommendation via competitive multi-agent reinforcement
    learning. In *Proceedings of the Eleventh ACM Conference on Recommender Systems*.
    331–335.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020d) Dongyang Zhao, Liang Zhang, Bo Zhang, Lizhou Zheng, Yongjun
    Bao, and Weipeng Yan. 2020d. MaHRL: Multi-goals Abstraction Based Deep Hierarchical
    Reinforcement Learning for Recommendations. In *Proceedings of the 43rd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 871–880.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020b) Kangzhi Zhao, Xiting Wang, Yuren Zhang, Li Zhao, Zheng Liu,
    Chunxiao Xing, and Xing Xie. 2020b. Leveraging Demonstrations for Reinforcement
    Recommendation Reasoning over Knowledge Graphs. In *Proceedings of the 43rd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 239–248.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020a) Wenshuai Zhao, Jorge Peña Queralta, and Tomi Westerlund.
    2020a. Sim-to-real transfer in deep reinforcement learning for robotics: a survey.
    In *2020 IEEE Symposium Series on Computational Intelligence (SSCI)*. IEEE, 737–744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang,
    Xiaobing Liu, Hui Liu, and Jiliang Tang. 2021. DEAR: Deep Reinforcement Learning
    for Online Advertising Impression in Recommender Systems. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, Vol. 35\. 750–758.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018a) Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei
    Yin, and Jiliang Tang. 2018a. Deep reinforcement learning for page-wise recommendations.
    In *Proceedings of the 12th ACM Conference on Recommender Systems*. 95–103.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020c) Xiangyu Zhao, Long Xia, Lixin Zou, Hui Liu, Dawei Yin, and
    Jiliang Tang. 2020c. Whole-Chain Recommendations. In *Proceedings of the 29th
    ACM International Conference on Information & Knowledge Management*. 1883–1891.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019a) Xiangyu Zhao, Long Xia, Lixin Zou, Dawei Yin, and Jiliang
    Tang. 2019a. Toward simulating environments in reinforcement learning based recommendations.
    *arXiv preprint arXiv:1906.11462* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018b) Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang
    Tang, and Dawei Yin. 2018b. Recommendations with negative feedback via pairwise
    deep reinforcement learning. In *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 1040–1048.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019b) Xiangyu Zhao, Liang Zhang, Long Xia, Zhuoye Ding, Dawei
    Yin, and Jiliang Tang. 2019b. Deep reinforcement learning for list-wise recommendations.
    *DRL4KDD ’19* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020e) Xiangyu Zhao, Xudong Zheng, Xiwang Yang, Xiaobing Liu, and
    Jiliang Tang. 2020e. Jointly learning to recommend and advertise. In *Proceedings
    of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. 3319–3327.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2018) Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang,
    Nicholas Jing Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement
    learning framework for news recommendation. In *Proceedings of the 2018 World
    Wide Web Conference*. 167–176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020) Sijin Zhou, Xinyi Dai, Haokun Chen, Weinan Zhang, Kan Ren,
    Ruiming Tang, Xiuqiang He, and Yong Yu. 2020. Interactive recommender system via
    knowledge graph-enhanced reinforcement learning. In *Proceedings of the 43rd International
    ACM SIGIR Conference on Research and Development in Information Retrieval*. 179–188.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    2017. Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *Proceedings of the IEEE international conference on computer vision*. 2223–2232.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2019) Shengyu Zhu, Ignavier Ng, and Zhitang Chen. 2019. Causal discovery
    with reinforcement learning. *arXiv preprint arXiv:1906.04477* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2019) Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu,
    and Dawei Yin. 2019. Reinforcement learning to optimize long-term user engagement
    in recommender systems. In *Proceedings of the 25th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*. 2810–2818.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou et al. (2020) Lixin Zou, Long Xia, Pan Du, Zhuo Zhang, Ting Bai, Weidong
    Liu, Jian-Yun Nie, and Dawei Yin. 2020. Pseudo Dyna-Q: A reinforcement learning
    framework for interactive recommendation. In *Proceedings of the 13th International
    Conference on Web Search and Data Mining*. 816–824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
