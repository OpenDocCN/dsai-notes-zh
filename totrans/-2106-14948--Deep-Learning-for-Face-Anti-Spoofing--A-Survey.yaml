- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:53:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:53:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2106.14948] Deep Learning for Face Anti-Spoofing: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2106.14948] 深度学习在人脸反欺骗中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.14948](https://ar5iv.labs.arxiv.org/html/2106.14948)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2106.14948](https://ar5iv.labs.arxiv.org/html/2106.14948)
- en: 'Deep Learning for Face Anti-Spoofing: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在人脸反欺骗中的应用：综述
- en: Zitong Yu, , Yunxiao Qin, Xiaobai Li, , Chenxu Zhao,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 朱彤·余，云霄·秦，肖白·李，陈旭·赵，
- en: 'Zhen Lei,  and Guoying Zhao, Z. Yu, X. Li and G. Zhao are with Center for Machine
    Vision and Signal Analysis, University of Oulu, Oulu 90014, Finland. E-mail: {zitong.yu,
    xiaobai.li, guoying.zhao}@oulu.fi Y. Qin is with Communication University of China,
    Beijing 100024, China. E-mail: qinyunxiao@cuc.edu.cn C. Zhao is with SailYond
    Technology, Beijing 100000, China. E-mail: zhaochenxu@sailyond.com Z. Lei is with
    the National Laboratory of Pattern Recognition (NLPR), Center for Biometrics and
    Security Research (CBSR), Institute of Automation, Chinese Academy of Sciences
    (CASIA), Beijing 100190, China, also with the School of Artificial Intelligence,
    University of Chinese Academy of Sciences (UCAS), Beijing 100049, China, and also
    with the Centre for Artificial Intelligence and Robotics, Hong Kong Institute
    of Science & Innovation, Chinese Academy of Sciences, Hong Kong, SAR. E-mail:
    zlei@nlpr.ia.ac.cn Manuscript received June 26, 2021; revised May 15 and September
    2, 2022\. (Corresponding author: Guoying Zhao)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 雷震，赵国英，朱彤·余，肖白·李和赵国英在芬兰奥卢大学机器视觉与信号分析中心工作。电子邮件：{zitong.yu, xiaobai.li, guoying.zhao}@oulu.fi
    云霄·秦在中国传媒大学工作。电子邮件：qinyunxiao@cuc.edu.cn 陈旭·赵在SailYond科技公司工作。电子邮件：zhaochenxu@sailyond.com
    雷震在中国科学院自动化研究所模式识别国家实验室（NLPR）、生物特征识别与安全研究中心（CBSR）工作，同时在中国科学院大学人工智能学院（UCAS）和香港科学技术创新研究院人工智能与机器人中心（CAS-HK）工作。电子邮件：zlei@nlpr.ia.ac.cn
    手稿收到时间：2021年6月26日；修订时间：2022年5月15日和9月2日。（通讯作者：赵国英）
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Face anti-spoofing (FAS) has lately attracted increasing attention due to its
    vital role in securing face recognition systems from presentation attacks (PAs).
    As more and more realistic PAs with novel types spring up, early-stage FAS methods
    based on handcrafted features become unreliable due to their limited representation
    capacity. With the emergence of large-scale academic datasets in the recent decade,
    deep learning based FAS achieves remarkable performance and dominates this area.
    However, existing reviews in this field mainly focus on the handcrafted features,
    which are outdated and uninspiring for the progress of FAS community. In this
    paper, to stimulate future research, we present the first comprehensive review
    of recent advances in deep learning based FAS. It covers several novel and insightful
    components: 1) besides supervision with binary label (e.g., ‘0’ for bonafide vs.
    ‘1’ for PAs), we also investigate recent methods with pixel-wise supervision (e.g.,
    pseudo depth map); 2) in addition to traditional intra-dataset evaluation, we
    collect and analyze the latest methods specially designed for domain generalization
    and open-set FAS; and 3) besides commercial RGB camera, we summarize the deep
    learning applications under multi-modal (e.g., depth and infrared) or specialized
    (e.g., light field and flash) sensors. We conclude this survey by emphasizing
    current open issues and highlighting potential prospects.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸反欺骗（FAS）由于在保护人脸识别系统免受展示攻击（PAs）方面发挥的关键作用，最近引起了越来越多的关注。随着越来越多新型的现实展示攻击的出现，基于手工特征的早期FAS方法由于其有限的表示能力变得不可靠。近年来，大规模学术数据集的出现使得基于深度学习的FAS取得了显著的成果，并主导了这一领域。然而，现有的综述主要关注手工特征，这些特征已经过时，对FAS社区的进展并没有激励作用。为了激发未来的研究，我们提出了首个全面回顾深度学习基础的FAS的最新进展的综述。它涵盖了几个新颖而有见地的组件：1）除了使用二元标签（例如，‘0’代表真实
    vs. ‘1’代表PAs）的监督，我们还研究了使用像素级监督（例如，伪深度图）的最新方法；2）除了传统的内部数据集评估，我们收集并分析了专门为领域泛化和开放集FAS设计的最新方法；3）除了商业RGB摄像头，我们总结了在多模态（例如，深度和红外）或专业（例如，光场和闪光）传感器下的深度学习应用。我们通过强调当前的开放问题和突出潜在的前景来总结这项调查。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: face anti-spoofing, presentation attack, deep learning, pixel-wise supervision,
    multi-modal, domain generalization.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸反欺骗，展示攻击，深度学习，像素级监督，多模态，领域泛化。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Due to its convenience and remarkable accuracy, face recognition technology [[1](#bib.bib1)]
    has been applied in a few interactive intelligent applications such as checking-in
    and mobile payment. However, existing face recognition systems are vulnerable
    to presentation attacks (PAs) ranging from print, replay, makeup, 3D-mask, etc.
    Therefore, both academia and industry have paid extensive attention to developing
    face anti-spoofing (FAS) technology for securing the face recognition system.
    As illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for
    Face Anti-Spoofing: A Survey"), FAS (namely ‘face presentation attack detection’
    or ‘face liveness detection’) is an active research topic in computer vision and
    has received an increasing number of publications in recent years.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '由于其便利性和显著的准确性，人脸识别技术[[1](#bib.bib1)]已被应用于一些互动智能应用中，如签到和移动支付。然而，现有的人脸识别系统易受各种展示攻击（PAs）的威胁，如打印、回放、化妆、3D面具等。因此，学术界和工业界都对开发人脸防伪（FAS）技术以保障人脸识别系统的安全给予了广泛关注。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing: A Survey")所示，FAS（即“人脸展示攻击检测”或“人脸活体检测”）是计算机视觉中的一个活跃研究主题，近年来获得了越来越多的出版物。'
- en: '![Refer to caption](img/ded8e39502ecc62064533ee29bd14efd.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ded8e39502ecc62064533ee29bd14efd.png)'
- en: 'Figure 1: The increasing research interest in the FAS field, obtained through
    Google scholar search with key-words: allintitle: “face anti-spoofing”, “face
    presentation attack detection”, and “face liveness detection”.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：FAS领域日益增长的研究兴趣，通过Google Scholar搜索关键词：allintitle: “face anti-spoofing”、“face
    presentation attack detection”和“face liveness detection”获得。'
- en: In the early stage, plenty of traditional handcrafted feature [[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] based methods
    have been proposed for presentation attack detection (PAD). Most traditional algorithms
    are designed based on human liveness cues and handcrafted features, which need
    rich task-aware prior knowledge for design. In term of the methods based on the
    liveness cues, eye-blinking [[2](#bib.bib2), [7](#bib.bib7), [8](#bib.bib8)],
    face and head movement [[9](#bib.bib9), [10](#bib.bib10)] (e.g., nodding and smiling),
    gaze tracking [[11](#bib.bib11), [12](#bib.bib12)] and remote physiological signals
    (e.g., rPPG [[3](#bib.bib3), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)])
    are explored for dynamic discrimination. However, these physiological liveness
    cues are usually captured from long-term interactive face videos, which is inconvenient
    for practical deployment. Furthermore, the liveness cues are easily mimicked by
    video attacks, making them less reliable. On the other hand, classical handcrafted
    descriptors (e.g., LBP [[16](#bib.bib16), [4](#bib.bib4)], SIFT [[6](#bib.bib6)],
    SURF [[17](#bib.bib17)], HOG [[5](#bib.bib5)] and DoG [[18](#bib.bib18)]) are
    designed for extracting effective spoofing patterns from various color spaces
    (RGB, HSV, and YCbCr). It can be seen from Table-A 1 (in Appendix) that the FAS
    surveys before 2018 mainly focus on this category.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，已经提出了大量基于传统手工特征[[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6)]的方法用于展示攻击检测（PAD）。大多数传统算法是基于人类活体线索和手工特征设计的，这需要丰富的任务感知先验知识进行设计。关于基于活体线索的方法，研究了眼睛眨动[[2](#bib.bib2),
    [7](#bib.bib7), [8](#bib.bib8)]、面部和头部运动[[9](#bib.bib9), [10](#bib.bib10)]（如点头和微笑）、注视追踪[[11](#bib.bib11),
    [12](#bib.bib12)]以及远程生理信号（如rPPG[[3](#bib.bib3), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15)]）用于动态区分。然而，这些生理活体线索通常需要从长期互动面部视频中捕捉，这对于实际部署不够方便。此外，活体线索易被视频攻击模拟，使其不够可靠。另一方面，经典的手工描述符（如LBP[[16](#bib.bib16),
    [4](#bib.bib4)]、SIFT[[6](#bib.bib6)]、SURF[[17](#bib.bib17)]、HOG[[5](#bib.bib5)]和DoG[[18](#bib.bib18)]）则设计用于从各种色彩空间（RGB、HSV和YCbCr）中提取有效的伪造模式。从附录中的表A
    1可以看出，2018年前的FAS调查主要集中在这一类别。
- en: '![Refer to caption](img/e022b72762d63ff32936afd61f4eacc6.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e022b72762d63ff32936afd61f4eacc6.png)'
- en: 'Figure 2: Topology of the deep learning based FAS methods.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的FAS方法的拓扑结构。
- en: Subsequently, a few hybrid (handcrafted+deep learning) [[19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)] and end-to-end deep learning based methods [[23](#bib.bib23),
    [24](#bib.bib24), [13](#bib.bib13), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)] are proposed for both static and dynamic face PAD. Most works[[29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35)] treat FAS as a binary classification problem (e.g., ‘0’ for
    live while ‘1’ for spoofing faces, or vice versa) thus supervised by a simple
    binary cross-entropy loss. Different from other binary vision tasks, the FAS is
    a self-evolving problem (i.e., attack vs. defense develop iteratively), which
    makes it more challenging. Furthermore, other binary vision tasks (e.g., human
    gender classification) highly rely on the obvious appearance-based semantic clues
    (e.g., hair style, wearing, facial shape) while the intrinsic features (e.g.,
    material and geometry) in FAS are usually content-irrelevant (e.g., not related
    to facial attribute and ID), subtle and with fine-grained details, which are very
    challenging to distinguish by even human eyes. Thus, convolutional neural networks
    (CNNs) with single binary loss might reasonably mine different kinds of semantic
    features for binary vision tasks like gender classification but discover arbitrary
    and unfaithful clues (e.g., screen bezel) for spoofing patterns. Fortunately,
    such intrinsic live/spoof clues are usually closely related with some position-aware
    auxiliary tasks. For instance, the face surface of print/replay and transparent
    mask attacks are usually with irregular/limited geometric depth distribution and
    abnormal reflection, respectively. Based on these physical evidences, recently,
    pixel-wise supervision [[26](#bib.bib26), [13](#bib.bib13), [24](#bib.bib24),
    [36](#bib.bib36), [32](#bib.bib32), [37](#bib.bib37)] attracts more attention
    as it provides more fine-grained context-aware supervision signals, which is beneficial
    for deep models learning intrinsic spoofing cues. On one hand, pseudo depth labels [[26](#bib.bib26),
    [13](#bib.bib13)], reflection maps [[24](#bib.bib24), [36](#bib.bib36)], binary
    mask label [[32](#bib.bib32), [38](#bib.bib38), [39](#bib.bib39)] and 3D point
    cloud maps [[40](#bib.bib40)] are typical pixel-wise auxiliary supervisions, which
    describe the local live/spoof cues in pixel/patch level. On the other hand, besides
    physical-guided auxiliary signals, a few generative deep FAS methods model the
    intrinsic spoofing patterns via relaxed pixel-wise reconstruction constraints [[33](#bib.bib33),
    [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]. As shown in Table-A 1 (in
    Appendix), the latest FAS surveys from 2018 to 2020 investigate limited numbers
    ($\textless$50) of deep learning based methods, which hardly provide comprehensive
    elaborations for the community researchers. Note that most data-driven methods
    introduced in previous surveys are supervised by traditional binary loss, and
    there is still a blank for summarizing the arisen pixel-wise supervision methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，提出了一些混合（手工+深度学习）[[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]和端到端深度学习方法[[23](#bib.bib23),
    [24](#bib.bib24), [13](#bib.bib13), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)]，用于静态和动态面部伪造检测（PAD）。大多数工作[[29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)]将面部伪造攻击（FAS）视为二分类问题（例如，“0”表示真实，“1”表示伪造，反之亦然），因此由简单的二分类交叉熵损失进行监督。与其他二分类视觉任务不同，FAS是一个自我演变的问题（即攻击与防御迭代发展），这使得它更加具有挑战性。此外，其他二分类视觉任务（例如，人类性别分类）高度依赖明显的外观语义线索（例如，发型、穿着、面部形状），而FAS中的内在特征（例如，材料和几何形状）通常与内容无关（例如，不与面部属性和身份相关），细微且具有细粒度的细节，即使是人眼也很难区分。因此，具有单一二分类损失的卷积神经网络（CNNs）可能会合理挖掘各种语义特征用于像性别分类这样的二分类视觉任务，但发现任意且不可靠的线索（例如，屏幕边框）用于伪造模式。幸运的是，这些内在的真实/伪造线索通常与一些位置感知辅助任务密切相关。例如，打印/重放和透明面具攻击的面部表面通常具有不规则/有限的几何深度分布和异常反射。基于这些物理证据，最近，像素级监督[[26](#bib.bib26),
    [13](#bib.bib13), [24](#bib.bib24), [36](#bib.bib36), [32](#bib.bib32), [37](#bib.bib37)]受到了更多关注，因为它提供了更细粒度的上下文感知监督信号，这对深度模型学习内在的伪造线索是有益的。一方面，伪深度标签[[26](#bib.bib26),
    [13](#bib.bib13)]、反射图[[24](#bib.bib24), [36](#bib.bib36)]、二进制掩膜标签[[32](#bib.bib32),
    [38](#bib.bib38), [39](#bib.bib39)]和3D点云图[[40](#bib.bib40)]是典型的像素级辅助监督，它们描述了像素/补丁级的局部真实/伪造线索。另一方面，除了物理引导的辅助信号外，一些生成式深度FAS方法通过放松的像素级重建约束建模内在伪造模式[[33](#bib.bib33),
    [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]。如附录中的表A 1所示，2018至2020年的最新FAS调查研究了有限数量（$\textless$50）的深度学习方法，这些方法很难为社区研究人员提供全面的阐述。请注意，大多数在之前的调查中介绍的数据驱动方法由传统的二分类损失进行监督，且仍缺乏对出现的像素级监督方法的总结。
- en: Meanwhile, the emergence of large-scale public FAS datasets with rich attack
    types and recorded sensors also greatly boosts the research community. First,
    the datasets with vast samples and subjects have been released. For instance,
    CelebA-Spoof [[44](#bib.bib44)], recorded from 10177 subjects, contains 156384
    and 469153 face images for bonafide and PAs, respectively. Second, besides the
    common PA types (e.g., print and replay attacks), some up-to-date datasets contain
    richer challenging PA types (e.g., SiW-M [[38](#bib.bib38)] and WMCA [[45](#bib.bib45)]
    with more than 10 PA types). However, we can find from Table-A 1 (in Appendix)
    that existing surveys only investigate a handful of ($\textless$15) old and small-scale
    FAS datasets, which cannot provide fair benchmarks for deep learning based methods.
    Third, in terms of modality and hardware for recording, besides commercial visible
    RGB camera, numerous multimodal and specialized sensors benefit the FAS task.
    For example, CASIA-SURF [[28](#bib.bib28)] and WMCA [[45](#bib.bib45)] show the
    effectiveness of PAD via fusing RGB/depth/NIR information while dedicated systems
    with multispectral SWIR [[46](#bib.bib46)] and four-directional polarized [[47](#bib.bib47)]
    cameras significantly benefit for spoofing material perception. However, previous
    surveys mostly focus on single RGB modality using a commercial visible camera,
    and neglect the deep learning applications on the multimodal and specialized systems
    for high-security scenarios.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，大规模公共FAS数据集的出现，涵盖了丰富的攻击类型和记录传感器，也极大地推动了研究界的发展。首先，已经发布了样本和受试者数量庞大的数据集。例如，CelebA-Spoof
    [[44](#bib.bib44)]，记录了10177名受试者，包含了156384张和469153张分别用于真实和PA的面部图像。其次，除了常见的PA类型（如打印和重播攻击），一些最新的数据集包含了更多具有挑战性的PA类型（如SiW-M
    [[38](#bib.bib38)] 和 WMCA [[45](#bib.bib45)]，包含超过10种PA类型）。然而，从表A 1（附录中）可以发现，现有的调查只研究了一些（<15）旧的和小规模的FAS数据集，这些数据集无法为基于深度学习的方法提供公平的基准。第三，就记录的模式和硬件而言，除了商业可见RGB相机外，许多多模态和专业传感器对FAS任务有很大帮助。例如，CASIA-SURF
    [[28](#bib.bib28)] 和 WMCA [[45](#bib.bib45)] 展示了通过融合RGB/深度/NIR信息进行PAD的有效性，而配备有多光谱SWIR
    [[46](#bib.bib46)] 和四向偏振 [[47](#bib.bib47)] 相机的专用系统在欺诈材料感知方面显著受益。然而，之前的调查大多集中于使用商业可见相机的单一RGB模式，并忽略了在高安全场景下多模态和专业系统的深度学习应用。
- en: 'From the perspective of evaluation protocols, traditional ‘intra-dataset intra-type’
    and ‘cross-dataset intra-type’ protocols are widely investigated in previous FAS
    surveys (see Table-A 1 in Appendix). As FAS is actually an open-set problem in
    practice, the uncertain gaps (e.g., environments and attack types) between training
    and testing conditions should be considered. However, no existing reviews consider
    the issues about unseen domain generalization [[48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51)] and unknown PAD [[52](#bib.bib52), [38](#bib.bib38),
    [53](#bib.bib53), [54](#bib.bib54)]. Most reviewed FAS methods design or train
    the FAS model on predefined scenarios and PAs. Thus, the trained models easily
    overfit on several specific domains and attack types, and are vulnerable to unseen
    domains and unknown attacks. To bridge the gaps between academic research and
    real-world applications, in this paper, we fully investigate deep learning based
    methods under four FAS protocols, including challenging domain generalization
    and open-set PAD situations. Compared with existing literatures, the major contributions
    of this work can be summarized as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从评估协议的角度来看，传统的“数据集内部类型”和“数据集间类型”协议在之前的FAS调查中得到了广泛研究（见附录中的表A 1）。由于FAS实际上在实践中是一个开放集问题，因此训练和测试条件之间的不确定差距（例如环境和攻击类型）应该被考虑。然而，现有的综述没有考虑到关于未见领域泛化
    [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)] 和未知PAD
    [[52](#bib.bib52), [38](#bib.bib38), [53](#bib.bib53), [54](#bib.bib54)] 的问题。大多数审查的FAS方法设计或训练FAS模型在预定义的场景和PA上。因此，训练出的模型容易在几个特定领域和攻击类型上过拟合，并且对未见领域和未知攻击较为脆弱。为了弥合学术研究与实际应用之间的差距，本文在四种FAS协议下全面研究了基于深度学习的方法，包括具有挑战性的领域泛化和开放集PAD情况。与现有文献相比，本工作的主要贡献可以总结如下：
- en: '![Refer to caption](img/109861a38c16bf94daaed06a499e1bb9.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/109861a38c16bf94daaed06a499e1bb9.png)'
- en: 'Figure 3: Typical face spoofing attacks and face anti-spoofing pipeline. (a)
    FAS could be integrated with face recognition systems with paralled or serial
    scheme for reliable face ID matching. (b) Visualization of several classical face
    spoofing attack types [[55](#bib.bib55)] in terms of impersonation/obfuscation,
    2D/3D, and whole/partial evidences.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：典型的面部欺骗攻击和面部反欺骗流程。（a）FAS可以与面部识别系统集成，采用并行或串行方案以确保可靠的面部ID匹配。（b）几种经典面部欺骗攻击类型[[55](#bib.bib55)]的可视化，包括伪装/混淆、2D/3D和整体/部分证据。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first survey paper to comprehensively
    cover ($\textgreater$100) deep learning methods for both single- and multi-modal
    FAS with generalized protocols. Compared with previous surveys only considering
    the methods with binary loss supervision, we also elaborate on those with auxiliary/generative
    pixel-wise supervision.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一篇全面涵盖（$\textgreater$100）深度学习方法的调查论文，涉及单模态和多模态FAS的广泛协议。与之前只考虑二进制损失监督方法的调查不同，我们还详细阐述了那些具有辅助/生成像素级监督的方法。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As opposed to existing reviews [[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58)]
    with only limited numbers ($\textless$15) of small-scale datasts, we show detailed
    comparisons among past-to-present 35 public datasets including various kinds of
    PAs as well as advanced recording sensors.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与现有仅包含有限数量（$\textless$15）小规模数据集的综述[[56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58)]相比，我们展示了过去到现在35个公共数据集的详细比较，包括各种PAs以及先进的录制传感器。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This paper covers the most recent and advanced progress of deep learning on
    four practical FAS protocols (i.e., intra-dataset intra-type, cross-dataset intra-type,
    intra-dataset cross-type, and cross-dataset cross-type testings). Therefore, it
    provides the readers with state-of-the-art methods with different application
    scenarios (e.g., unseen domain generalization and unknown attack detection).
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文涵盖了深度学习在四种实际FAS协议上的最新进展（即，数据集内类型、数据集间类型、数据集内跨类型和数据集间跨类型测试）。因此，它为读者提供了具有不同应用场景（例如，未见领域泛化和未知攻击检测）的最新方法。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Comprehensive comparisons of existing deep FAS methods with insightful taxonomy
    are provided in Tables-A 5, 6, 7, 8, 9, 10, and 11 (in Appendix), with brief summaries
    and discussions being presented.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在附录中的表格A 5, 6, 7, 8, 9, 10和11中，提供了对现有深度FAS方法的全面比较和有洞察力的分类，同时呈现了简要总结和讨论。
- en: 'We summarize the topology of deep learning based FAS methods with the commercial
    monocular RGB camera and advanced sensors in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Deep Learning for Face Anti-Spoofing: A Survey"). On one hand, as commercial
    RGB camera is widely used in many real-world applicational scenarios (e.g., access
    control system and mobile device unlocking), there are richer research works based
    on this branch. It includes three main categories: 1) hybrid learning methods
    combining both handcrafted and deep learning features; 2) traditional end-to-end
    supervised deep learning based methods; and 3) generalized deep learning methods
    to both unseen domain and unknown attack types. Besides the commercial RGB camera,
    researchers have also developed sensor-aware deep learning methods for efficient
    FAS using specialized sensors/hardwares. Meanwhile, as multi-spectrum imaging
    systems with acceptable costs are increasingly used in real-world applications,
    multi-modal deep learning based methods become hot and active in the FAS research
    community.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")中总结了基于深度学习的FAS方法在商业单目RGB相机和先进传感器中的拓扑结构。一方面，商业RGB相机在许多实际应用场景（如门禁系统和移动设备解锁）中广泛使用，因此基于这一领域的研究更加丰富。它包括三个主要类别：1）结合手工特征和深度学习特征的混合学习方法；2）传统的端到端监督深度学习方法；以及3）针对未知领域和未知攻击类型的广义深度学习方法。除了商业RGB相机，研究人员还开发了使用专用传感器/硬件的传感器感知深度学习方法，以提高FAS效率。同时，由于多光谱成像系统在实际应用中越来越普及，多模态深度学习方法在FAS研究社区中变得非常热门和活跃。'
- en: 'The structure of this paper is as follows. Section [2](#S2 "2 background ‣
    Deep Learning for Face Anti-Spoofing: A Survey") introduces the research background,
    including presentation attacks, datasets, evaluation metrics, and protocols for
    the FAS task. Section [3](#S3 "3 Deep FAS with Commercial RGB Camera ‣ Deep Learning
    for Face Anti-Spoofing: A Survey") reviews the methods for visible RGB based FAS
    according to two kinds of supervision signals (i.e., binary loss and pixel-wise
    loss) as well as generalized learning for unseen domains and unknown attacks.
    Section [4](#S4 "4 Deep FAS with Advanced Sensors ‣ Deep Learning for Face Anti-Spoofing:
    A Survey") gives a comparison about the recording sensors as well as modalities,
    and then presents the methods for specific recorded inputs. Section [5](#S5 "5
    Discussion and Future Directions ‣ Deep Learning for Face Anti-Spoofing: A Survey")
    discusses the current issues of deep FAS, and indicates the future directions.
    Finally, conclusions are given in Section [6](#S6 "6 Conclusion ‣ Deep Learning
    for Face Anti-Spoofing: A Survey"). Researchers can track an up-to-date list at
    [https://github.com/ZitongYu/DeepFAS](https://github.com/ZitongYu/DeepFAS).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '本文结构如下。第[2](#S2 "2 background ‣ Deep Learning for Face Anti-Spoofing: A Survey")节介绍了研究背景，包括呈现攻击、数据集、评估指标以及FAS任务的协议。第[3](#S3
    "3 Deep FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")节回顾了基于可见RGB的FAS方法，按照两种监督信号（即二元损失和像素级损失）以及对未知领域和攻击的泛化学习进行讨论。第[4](#S4
    "4 Deep FAS with Advanced Sensors ‣ Deep Learning for Face Anti-Spoofing: A Survey")节对记录传感器及其模式进行比较，然后介绍特定记录输入的方法。第[5](#S5
    "5 Discussion and Future Directions ‣ Deep Learning for Face Anti-Spoofing: A
    Survey")节讨论了深度FAS的当前问题，并指出未来的研究方向。最后，第[6](#S6 "6 Conclusion ‣ Deep Learning for
    Face Anti-Spoofing: A Survey")节给出了结论。研究人员可以在[https://github.com/ZitongYu/DeepFAS](https://github.com/ZitongYu/DeepFAS)上跟踪最新列表。'
- en: '![Refer to caption](img/4f8b3640ca7819c6e57c9f13f6ecc3fe.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4f8b3640ca7819c6e57c9f13f6ecc3fe.png)'
- en: 'Figure 4: Visualization of the bonafide and spoofing samples from the HiFiMask
    dataset [[59](#bib.bib59)] (a) with various cameras under different lighting conditions [[59](#bib.bib59)];
    and the WMCA dataset [[45](#bib.bib45)] (b) with multiple modalities such as visible
    RGB, depth, infrared, and thermal.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：HiFiMask数据集[[59](#bib.bib59)]中真实和伪造样本的可视化 (a)，在不同光照条件下使用不同相机[[59](#bib.bib59)]；以及WMCA数据集[[45](#bib.bib45)]
    (b)，包括可见RGB、深度、红外和热成像等多种模式。
- en: 2 background
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: In this section, we will introduce the common face spoofing attacks first, and
    then investigate the existing FAS datasets as well as their evaluation metrics
    and protocols.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将首先介绍常见的人脸伪造攻击，然后调查现有的FAS数据集及其评估指标和协议。
- en: 2.1 Face Spoofing Attacks
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 人脸伪造攻击
- en: 'Attacks on automatic face recognition (AFR) system usually divide into two
    categories: digital manipulation [[60](#bib.bib60), [61](#bib.bib61)] and physical
    presentation attacks [[62](#bib.bib62)]. The former one fools the face system
    via imperceptibly visual manipulation in the digital virtual domain, while the
    latter usually misleads the real-world AFR systems via presenting face upon physical
    mediums in front of the imaging sensors. In this paper, we focus on the detection
    of physical face presentation attacks, whose pipeline is illustrated in Fig. [3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing: A Survey")(a).
    It can be seen that there are two kinds of schemes [[63](#bib.bib63)] for integrating
    FAS with AFR systems: 1) parallel fusion [[64](#bib.bib64)] with the predicted
    scores from FAS and AFR systems. The combined new final score is used to determine
    if the sample comes from a genuine user or not; and 2) serial scheme [[65](#bib.bib65)]
    for early face PAs detection and spoof rejection, thus avoiding the spoof face
    accessing the subsequent face recognition phase.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '对自动人脸识别（AFR）系统的攻击通常分为两类：数字化操作[[60](#bib.bib60), [61](#bib.bib61)]和物理呈现攻击[[62](#bib.bib62)]。前者通过在数字虚拟领域内微妙的视觉操作来欺骗面部系统，而后者通常通过在成像传感器前展示物理介质上的人脸来误导现实世界的AFR系统。本文重点关注物理人脸呈现攻击的检测，其流程如图[3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing: A Survey")(a)所示。可以看到，将FAS与AFR系统集成有两种方案[[63](#bib.bib63)]：1)
    并行融合[[64](#bib.bib64)]，通过FAS和AFR系统的预测分数来确定样本是否来自真实用户；2) 串行方案[[65](#bib.bib65)]，用于早期人脸PAs检测和欺骗拒绝，从而避免伪造人脸进入后续的人脸识别阶段。'
- en: 'In Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")(b), some representative spoofing attack types are illustrated. According
    to the attackers’ intention, face PAs [[66](#bib.bib66)] can be divided into two
    typical cases: 1) impersonation, which entails the use of spoof to be recognized
    as someone else via copying a genuine user’s facial attributes to special mediums
    such as photo, electronic screen, and 3D mask; and 2) obfuscation, which entails
    the use to hide or remove the attacker’s own identity using various methods such
    as glasses, makeup, wig, and disguised face.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")(b)中，展示了一些代表性的伪造攻击类型。根据攻击者的意图，面部伪造攻击[[66](#bib.bib66)]可以分为两种典型情况：1)
    冒充，即通过复制真实用户的面部特征到特殊媒介如照片、电子屏幕和3D面具上，以使伪造物被识别为其他人；2) 隐匿，即通过各种方法如眼镜、化妆、假发和伪装面部来隐藏或移除攻击者自身的身份。'
- en: Based on the geometry property, PAs are broadly classified into 2D and 3D attacks.
    2D PAs [[67](#bib.bib67)] are carried out by presenting facial attributes using
    photo or video to the sensor. Flat/wrapped printed photos, eye/mouth-cut photos,
    and digital replay of videos are common 2D attack variants. With the maturity
    of 3D printing technology, face 3D mask [[57](#bib.bib57)] has become a new type
    of PA to threaten AFR systems. Compared with traditional 2D PAs, face masks are
    more realistic in terms of color, texture, and geometry structure. 3D masks are
    made of different materials, e.g., hard/rigid masks can be made from paper, resin,
    plaster, or plastic while flexible soft masks are usually composed of silicon
    or latex.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据几何属性，伪造攻击（PAs）大致分为2D和3D攻击。2D伪造攻击[[67](#bib.bib67)]是通过使用照片或视频呈现面部特征来进行的。常见的2D攻击变体包括平面/包裹打印照片、眼睛/嘴巴切割照片和视频的数字重播。随着3D打印技术的成熟，面部3D面具[[57](#bib.bib57)]成为了一种新型的伪造攻击，威胁到面部识别系统。与传统的2D伪造攻击相比，面具在颜色、纹理和几何结构方面更为逼真。3D面具由不同材料制成，例如，硬质/刚性面具可以由纸、树脂、石膏或塑料制成，而柔性软面具通常由硅胶或乳胶组成。
- en: 'In consideration of the facial region covering, PAs can be also separated to
    whole or partial attacks. As shown in Fig. [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Deep Learning for Face Anti-Spoofing: A Survey")(b), compared with common PAs
    (e.g., print photo, video replay, and 3D mask) covering the whole face region,
    a few partial attacks only placed upon specific facial regions (e.g., part-cut
    print photo, funny eyeglass worn in the eyes region and partial tattoo on the
    cheek region), which are more obscure and challenging to detect.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到面部区域的覆盖，伪造攻击也可以分为整体攻击或部分攻击。如图[3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep
    Learning for Face Anti-Spoofing: A Survey")(b)所示，与覆盖整个面部区域的常见伪造攻击（例如，打印照片、视频重播和3D面具）相比，一些部分攻击仅针对特定面部区域（例如，部分切割的打印照片、眼部佩戴的搞笑眼镜以及脸颊上的部分纹身），这些攻击更加隐蔽且更具挑战性。'
- en: 'TABLE I: A summary of public available datasets for face anti-spoofing. The
    upper part of the table lists the datasets recorded via commercial RGB camera
    while the half bottom investigates the datasets with multiple modalities or specialized
    sensors. In the column ‘#Live/Spoof’, ‘I’ and ‘V’ denotes ‘images’ and ‘videos’,
    respectively. ‘#Sub.’ is short for Subjects.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：公开可用的面部防伪数据集汇总。表格的上半部分列出了通过商业RGB相机记录的数据集，而下半部分则探讨了包含多种模态或专用传感器的数据集。在“#真实/伪造”列中，“I”和“V”分别表示“图像”和“视频”。“#样本”是“Subjects”的缩写。
- en: '| Dataset & Reference | Year | #Live/Spoof | #Sub. | M&H | Setup | Attack Types
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 数据集与参考 | 年份 | #真实/伪造 | #样本 | M&H | 设置 | 攻击类型 |'
- en: '| NUAA [[18](#bib.bib18)] | 2010 | 5105/7509(I) | 15 | VIS | N/R | Print(flat,
    wrapped) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| NUAA [[18](#bib.bib18)] | 2010 | 5105/7509(I) | 15 | VIS | N/R | 打印（平面、包裹）
    |'
- en: '| YALE_Recaptured [[68](#bib.bib68)] | 2011 | 640/1920(I) | 10 | VIS | 50cm-distance
    from 3 LCD minitors | Print(flat) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| YALE_Recaptured [[68](#bib.bib68)] | 2011 | 640/1920(I) | 10 | VIS | 距离3个LCD显示器50厘米
    | 打印（平面） |'
- en: '| CASIA-MFSD  [[69](#bib.bib69)] | 2012 | 150/450(V) | 50 | VIS | 7 scenarios
    and 3 image quality | Print(flat, wrapped, cut), Replay(tablet) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-MFSD  [[69](#bib.bib69)] | 2012 | 150/450(V) | 50 | VIS | 7种场景和3种图像质量
    | 打印（平面、包裹、切割）、重播（平板） |'
- en: '| REPLAY-ATTACK  [[70](#bib.bib70)] | 2012 | 200/1000(V) | 50 | VIS | Lighting
    and holding | Print(flat), Replay(tablet, phone) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| REPLAY-ATTACK  [[70](#bib.bib70)] | 2012 | 200/1000(V) | 50 | VIS | 照明和持有
    | 打印（平面）、重播（平板、手机） |'
- en: '| Kose and Dugelay  [[71](#bib.bib71)] | 2013 | 200/198(I) | 20 | VIS | N/R
    | Mask(hard resin) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Kose and Dugelay  [[71](#bib.bib71)] | 2013 | 200/198(I) | 20 | VIS | N/R
    | 面具（硬树脂） |'
- en: '| MSU-MFSD  [[72](#bib.bib72)] | 2014 | 70/210(V) | 35 | VIS | Indoor scenario;
    2 types of cameras | Print(flat), Replay(tablet, phone) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| MSU-MFSD [[72](#bib.bib72)] | 2014 | 70/210(V) | 35 | VIS | 室内场景；2种相机类型 |
    打印（平面），重播（平板，手机） |'
- en: '| UVAD  [[73](#bib.bib73)] | 2015 | 808/16268(V) | 404 | VIS |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| UVAD [[73](#bib.bib73)] | 2015 | 808/16268(V) | 404 | VIS |'
- en: '&#124; Different lighting, background &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不同的光照，背景 &#124;'
- en: '&#124; and places in two sections &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和两个部分的地点 &#124;'
- en: '| Replay(monitor) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 重播（显示器） |'
- en: '| REPLAY-Mobile  [[74](#bib.bib74)] | 2016 | 390/640(V) | 40 | VIS | 5 lighting
    conditions | Print(flat), Replay(monitor) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| REPLAY-Mobile [[74](#bib.bib74)] | 2016 | 390/640(V) | 40 | VIS | 5种光照条件
    | 打印（平面），重播（显示器） |'
- en: '| HKBU-MARs V2  [[75](#bib.bib75)] | 2016 | 504/504(V) | 12 | VIS |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| HKBU-MARs V2 [[75](#bib.bib75)] | 2016 | 504/504(V) | 12 | VIS |'
- en: '&#124; 7 cameras from stationary and mobile &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 7台相机来自静态和移动 &#124;'
- en: '&#124; devices and 6 lighting settings &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设备和6种光照设置 &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mask(hard resin) from &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面具（硬树脂）来自 &#124;'
- en: '&#124; Thatsmyface and REAL-f &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Thatsmyface 和 REAL-f &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MSU USSA  [[6](#bib.bib6)] | 2016 | 1140/9120(I) | 1140 | VIS | Uncontrolled;
    2 types of cameras | Print(flat), Replay(laptop, tablet, phone) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| MSU USSA [[6](#bib.bib6)] | 2016 | 1140/9120(I) | 1140 | VIS | 不受控制；2种相机类型
    | 打印（平面），重播（笔记本电脑，平板，手机） |'
- en: '| SMAD  [[76](#bib.bib76)] | 2017 | 65/65(V) | - | VIS | Color images from
    online resources | Mask(silicone) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| SMAD [[76](#bib.bib76)] | 2017 | 65/65(V) | - | VIS | 来自在线资源的彩色图像 | 面具（硅胶）
    |'
- en: '| OULU-NPU  [[77](#bib.bib77)] | 2017 | 720/2880(V) | 55 | VIS | Lighting &
    background in 3 sections | Print(flat), Replay(phone) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| OULU-NPU [[77](#bib.bib77)] | 2017 | 720/2880(V) | 55 | VIS | 3个部分的光照和背景
    | 打印（平面），重播（手机） |'
- en: '| Rose-Youtu  [[78](#bib.bib78)] | 2018 | 500/2850(V) | 20 | VIS |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Rose-Youtu [[78](#bib.bib78)] | 2018 | 500/2850(V) | 20 | VIS |'
- en: '&#124; 5 front-facing phone camera; &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5台前置手机摄像头； &#124;'
- en: '&#124; 5 different illumination conditions &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 5种不同的光照条件 &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(flat), Replay(monitor, laptop), &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（平面），重播（显示器，笔记本电脑）， &#124;'
- en: '&#124; Mask(paper, crop-paper) &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面具（纸，裁剪纸） &#124;'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SiW  [[13](#bib.bib13)] | 2018 | 1320/3300(V) | 165 | VIS |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| SiW [[13](#bib.bib13)] | 2018 | 1320/3300(V) | 165 | VIS |'
- en: '&#124; 4 sessions with variations of distance, &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4个会话有距离变化， &#124;'
- en: '&#124; pose, illumination and expression &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 姿势、光照和表情 &#124;'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(flat, wrapped), &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（平面，包裹）， &#124;'
- en: '&#124; Replay(phone, tablet, monitor) &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重播（手机，平板，显示器） &#124;'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| WFFD  [[34](#bib.bib34)] | 2019 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| WFFD [[34](#bib.bib34)] | 2019 |'
- en: '&#124; 2300/2300(I) &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2300/2300(I) &#124;'
- en: '&#124; 140/145(V) &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 140/145(V) &#124;'
- en: '| 745 | VIS |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 745 | VIS |'
- en: '&#124; Collected online; super-realistic; &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在线收集；超真实； &#124;'
- en: '&#124; removed low-quality faces &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 去除低质量面孔 &#124;'
- en: '| Waxworks(wax) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 蜡像（蜡） |'
- en: '| SiW-M  [[38](#bib.bib38)] | 2019 | 660/968(V) | 493 | VIS |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| SiW-M [[38](#bib.bib38)] | 2019 | 660/968(V) | 493 | VIS |'
- en: '&#124; Indoor environment with pose, &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内环境与姿势， &#124;'
- en: '&#124; lighting and expression variations &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光照和表情变化 &#124;'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(flat), Replay, Mask(hard resin, &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（平面），重播，面具（硬树脂， &#124;'
- en: '&#124; plastic, silicone, paper, Mannequin), &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 塑料，硅胶，纸，假人）， &#124;'
- en: '&#124; Makeup(cosmetics, impersonation, &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 化妆（化妆品，伪装， &#124;'
- en: '&#124; Obfuscation), Partial(glasses, cut paper) &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模糊），部分（眼镜，剪纸） &#124;'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Swax  [[79](#bib.bib79)] | 2020 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Swax [[79](#bib.bib79)] | 2020 |'
- en: '&#124; Total 1812(I) &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计1812(I) &#124;'
- en: '&#124; 110(V) &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 110(V) &#124;'
- en: '| 55 | VIS |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 55 | VIS |'
- en: '&#124; Collected online; captured &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在线收集；捕获 &#124;'
- en: '&#124; under uncontrolled scenarios &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在不受控制的场景下 &#124;'
- en: '| Waxworks(wax) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 蜡像（蜡） |'
- en: '| CelebA-Spoof  [[44](#bib.bib44)] | 2020 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| CelebA-Spoof [[44](#bib.bib44)] | 2020 |'
- en: '&#124; 156384/ &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 156384/ &#124;'
- en: '&#124; 469153(I) &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 469153(I) &#124;'
- en: '| 10177 | VIS |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 10177 | VIS |'
- en: '&#124; 4 illumination conditions; &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4种照明条件； &#124;'
- en: '&#124; indoor & outdoor; rich annotations &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内和室外；丰富的注释 &#124;'
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(flat, wrapped), Replay(monitor, &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（平面，包裹），重播（显示器， &#124;'
- en: '&#124; tablet, phone), Mask(paper) &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平板，手机），面具（纸） &#124;'
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RECOD- &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RECOD- &#124;'
- en: '&#124; Mtablet &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Mtablet &#124;'
- en: '[[80](#bib.bib80)] | 2020 | 450/1800(V) | 45 | VIS |'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[[80](#bib.bib80)] | 2020 | 450/1800(V) | 45 | VIS |'
- en: '&#124; Outdoor environment and &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 户外环境和 &#124;'
- en: '&#124; low-light & dynamic sessions &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低光照与动态会话 &#124;'
- en: '| Print(flat), Replay(monitor) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 打印（平面），重播（显示器） |'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CASIA-SURF &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CASIA-SURF &#124;'
- en: '&#124; 3DMask &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3DMask &#124;'
- en: '[[37](#bib.bib37)] | 2020 | 288/864(V) | 48 | VIS |'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[[37](#bib.bib37)] | 2020 | 288/864(V) | 48 | VIS |'
- en: '&#124; High-quality identity-preserved; &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高质量身份保留； &#124;'
- en: '&#124; 3 decorations and 6 environments &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3 种装饰和 6 种环境 &#124;'
- en: '| Mask(mannequin with 3D print) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 面具（3D 打印假人） |'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HiFiMask &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HiFiMask &#124;'
- en: '[[59](#bib.bib59)] | 2021 | 13650/40950(V) | 75 | VIS |'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[[59](#bib.bib59)] | 2021 | 13650/40950(V) | 75 | 可见光 |'
- en: '&#124; three mask decorations; 7 recording &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三种面具装饰；7 种录制 &#124;'
- en: '&#124; devices; 6 lighting conditions &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设备；6 种光照条件 &#124;'
- en: '&#124; (periodic/random); 6 scenes &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （周期性/随机）；6 个场景 &#124;'
- en: '| Mask(transparent, plaster, resin) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 面具（透明，石膏，树脂） |'
- en: '| 3DMAD  [[81](#bib.bib81)] | 2013 | 170/85(V) | 17 | VIS, Depth | 3 sessions
    (2 weeks interval) | Mask(paper, hard resin) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 3DMAD  [[81](#bib.bib81)] | 2013 | 170/85(V) | 17 | 可见光，深度 | 3 次（间隔 2 周）
    | 面具（纸张，硬树脂） |'
- en: '| GUC-LiFFAD  [[82](#bib.bib82)] | 2015 | 1798/3028(V) | 80 | Light field |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GUC-LiFFAD  [[82](#bib.bib82)] | 2015 | 1798/3028(V) | 80 | 光场 |'
- en: '&#124; Distance of 1.5$\sim$2 m in &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 距离 1.5$\sim$2 m &#124;'
- en: '&#124; constrained conditions &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 受限条件 &#124;'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(Inkjet paper, Laserjet paper), &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（喷墨纸，激光纸），&#124;'
- en: '&#124; Replay(tablet) &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重播（平板） &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 3DFS-DB  [[83](#bib.bib83)] | 2016 | 260/260(V) | 26 | VIS, Depth | Head
    movement with rich angles | Mask(plastic) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 3DFS-DB  [[83](#bib.bib83)] | 2016 | 260/260(V) | 26 | 可见光，深度 | 头部运动角度丰富
    | 面具（塑料） |'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BRSU &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BRSU &#124;'
- en: '&#124; Skin/Face/Spoof &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 皮肤/面部/伪造 &#124;'
- en: '[[46](#bib.bib46)] | 2016 | 102/404(I) | 137 | VIS, SWIR |'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[[46](#bib.bib46)] | 2016 | 102/404(I) | 137 | 可见光，SWIR |'
- en: '&#124; multispectral SWIR with 4 wavebands &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多光谱 SWIR 具有 4 个波段 &#124;'
- en: '&#124; 935nm, 1060nm, 1300nm and 1550nm &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 935nm，1060nm，1300nm 和 1550nm &#124;'
- en: '| Mask(silicon, plastic, resin, latex) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 面具（硅胶，塑料，树脂，乳胶） |'
- en: '| Msspoof  [[84](#bib.bib84)] | 2016 | 1470/3024(I) | 21 | VIS, NIR | 7 environment
    conditions | Black&white Print(flat) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Msspoof  [[84](#bib.bib84)] | 2016 | 1470/3024(I) | 21 | 可见光，NIR | 7 个环境条件
    | 黑白打印（平面） |'
- en: '| MLFP  [[85](#bib.bib85)] | 2017 | 150/1200(V) | 10 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| MLFP  [[85](#bib.bib85)] | 2017 | 150/1200(V) | 10 |'
- en: '&#124; VIS, NIR, &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可见光，NIR，&#124;'
- en: '&#124; Thermal &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 热成像 &#124;'
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Indoor and outdoor with fixed &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内和室外固定 &#124;'
- en: '&#124; and random backgrounds &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和随机背景 &#124;'
- en: '| Mask(latex, paper) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 面具（乳胶，纸张） |'
- en: '| ERPA  [[86](#bib.bib86)] | 2017 | Total 86(V) | 5 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ERPA  [[86](#bib.bib86)] | 2017 | 总计 86(V) | 5 |'
- en: '&#124; VIS, Depth, &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可见光，深度，&#124;'
- en: '&#124; NIR, Thermal &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIR，热成像 &#124;'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Subject positioned close (0.3$\sim$0.5m) &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 受试者接近位置（0.3$\sim$0.5m） &#124;'
- en: '&#124; to the 2 types of cameras &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2 种相机类型 &#124;'
- en: '|'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(flat), Replay(monitor), &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（平面），重播（显示器），&#124;'
- en: '&#124; Mask(resin, silicone) &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面具（树脂，硅胶） &#124;'
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LF-SAD  [[87](#bib.bib87)] | 2018 | 328/596(I) | 50 | Light field |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| LF-SAD  [[87](#bib.bib87)] | 2018 | 328/596(I) | 50 | 光场 |'
- en: '&#124; Indoor fix background, captured &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内固定背景，捕获 &#124;'
- en: '&#124; by Lytro ILLUM camera &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由 Lytro ILLUM 相机拍摄 &#124;'
- en: '| Print(flat, wrapped), Replay(monitor) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 打印（平面，包裹），重播（显示器） |'
- en: '| CSMAD  [[88](#bib.bib88)] | 2018 | 104/159(V+I) | 14 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| CSMAD  [[88](#bib.bib88)] | 2018 | 104/159(V+I) | 14 |'
- en: '&#124; VIS, Depth, &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可见光，深度，&#124;'
- en: '&#124; NIR, Thermal &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIR，热成像 &#124;'
- en: '| 4 lighting conditions | Mask(custom silicone) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 4 种光照条件 | 面具（定制硅胶） |'
- en: '| 3DMA  [[89](#bib.bib89)] | 2019 | 536/384(V) | 67 | VIS, NIR |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 3DMA  [[89](#bib.bib89)] | 2019 | 536/384(V) | 67 | 可见光，NIR |'
- en: '&#124; 48 masks with different ID; 2 illumi- &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 48 个不同 ID 的面具；2 种照明 &#124;'
- en: '&#124; nation & 4 capturing distances &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 国家 & 4 个捕获距离 &#124;'
- en: '| Mask(plastics) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 面具（塑料） |'
- en: '| CASIA-SURF  [[90](#bib.bib90)] | 2019 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| CASIA-SURF  [[90](#bib.bib90)] | 2019 |'
- en: '&#124; 3000/ &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3000/ &#124;'
- en: '&#124; 18000(V) &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 18000(V) &#124;'
- en: '| 1000 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 1000 |'
- en: '&#124; VIS, Depth, &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可见光，深度，&#124;'
- en: '&#124; NIR &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIR &#124;'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Background removed; Randomly &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 背景去除；随机 &#124;'
- en: '&#124; cut eyes, nose or mouth areas &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛、鼻子或嘴巴区域被遮挡 &#124;'
- en: '| Print(flat, wrapped, cut) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 打印（平面，包裹，裁剪） |'
- en: '| WMCA  [[45](#bib.bib45)] | 2019 | 347/1332(V) | 72 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| WMCA  [[45](#bib.bib45)] | 2019 | 347/1332(V) | 72 |'
- en: '&#124; VIS, Depth, &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可见光，深度，&#124;'
- en: '&#124; NIR, Thermal &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIR，热成像 &#124;'
- en: '|'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 6 sessions with different &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 6 次会话与不同 &#124;'
- en: '&#124; backgrounds and illumination; &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 背景和光照； &#124;'
- en: '&#124; pulse data for bonafide recordings &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 真实记录的脉冲数据 &#124;'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(flat), Replay(tablet), &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（平面），重播（平板），&#124;'
- en: '&#124; Partial(glasses), Mask(plastic, &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 部分（眼镜），面具（塑料， &#124;'
- en: '&#124; silicone, and paper, Mannequin) &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硅胶和纸张，假人） &#124;'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CeFA  [[91](#bib.bib91)] | 2020 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| CeFA  [[91](#bib.bib91)] | 2020 |'
- en: '&#124; 6300/ &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 6300/ &#124;'
- en: '&#124; 27900(V) &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 27900(V) &#124;'
- en: '| 1607 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 1607 |'
- en: '&#124; VIS, Depth, &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可见光，深度，&#124;'
- en: '&#124; NIR &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIR &#124;'
- en: '|'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3 ethnicities; outdoor & indoor; &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3种民族；室内外 &#124;'
- en: '&#124; decoration with wig and glasses &#124;'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带假发和眼镜的装饰 &#124;'
- en: '|'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(flat, wrapped), Replay, &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（平面，包裹），重播 &#124;'
- en: '&#124; Mask(3D print, silica gel) &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面具（3D打印，硅胶） &#124;'
- en: '|'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| HQ-WMCA  [[55](#bib.bib55)] | 2020 | 555/2349(V) | 51 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| HQ-WMCA [[55](#bib.bib55)] | 2020 | 555/2349(V) | 51 |'
- en: '&#124; VIS, Depth, &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可见光，深度 &#124;'
- en: '&#124; NIR, SWIR, &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIR，SWIR &#124;'
- en: '&#124; Thermal &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 热成像 &#124;'
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Indoor; 14 ‘modalities’, including &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内；14种‘模态’，包括 &#124;'
- en: '&#124; 4 NIR and 7 SWIR wavelengths; &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4种NIR和7种SWIR波长； &#124;'
- en: '&#124; masks and mannequins were &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面具和假人 &#124;'
- en: '&#124; heated up to reach body temperature &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加热至体温 &#124;'
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Laser or inkjet Print(flat), &#124;'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 激光或喷墨打印（平面），&#124;'
- en: '&#124; Replay(tablet, phone), Mask(plastic, &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重播（平板，手机），面具（塑料，&#124;'
- en: '&#124; silicon, paper, mannequin), Makeup, &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硅胶，纸张，假人），化妆 &#124;'
- en: '&#124; Partial(glasses, wigs, tatoo) &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 部分（眼镜，假发，纹身） &#124;'
- en: '|'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PADISI-Face  [[92](#bib.bib92)] | 2021 | 1105/924(V) | 360 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| PADISI-Face [[92](#bib.bib92)] | 2021 | 1105/924(V) | 360 |'
- en: '&#124; VIS, Depth, &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可见光，深度 &#124;'
- en: '&#124; NIR, SWIR, &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIR，SWIR &#124;'
- en: '&#124; Thermal &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 热成像 &#124;'
- en: '|'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Indoor, fixed green background, &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 室内，固定的绿色背景 &#124;'
- en: '&#124; 60-frame sequence of &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 60帧序列 &#124;'
- en: '&#124; 1984 × 1264 pixel images &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1984 × 1264像素图像 &#124;'
- en: '|'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print(flat), Replay(tablet, phone), &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印（平面），重播（平板，手机），&#124;'
- en: '&#124; Partial(glasses,funny eye), Mask(plastic, &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 部分（眼镜，有趣的眼睛），面具（塑料，&#124;'
- en: '&#124; silicone, transparent, Mannequin) &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硅胶，透明，假人）&#124;'
- en: '| ![Refer to caption](img/e35a2537b1f25b2ea4dccb2c0fdef353.png)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![参见说明](img/e35a2537b1f25b2ea4dccb2c0fdef353.png)'
- en: 'Figure 5: The performance of deep FAS approaches on four mainstream evaluation
    protocols. The lower ACER, HTER and EER, the better performance. (a) Intra-dataset
    intra-type testing on the Protocol-4 of OULU-NPU. (b) Cross-dataset intra-type
    testing on CASIA-MFSD when training on single Replay-Attack dataset (see green
    columns) or multiple datasets including OULU-NPU, MSU-MFSD, and Replay-Attack
    (see purple columns). (c) Intra-dataset cross-type testing on SiW-M with leave-one-type-out
    setting. (d) Cross-dataset cross-type testing on 3D mask FAS datasets including
    HKBU-MARs [[75](#bib.bib75)] and CASIA-SURF 3DMask when training on OULU-NPU and
    SiW datasets with only 2D attacks.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：四种主流评估协议下深度FAS方法的性能。ACER、HTER和EER越低，性能越好。（a）在OULU-NPU的协议-4上的数据集内类型测试。（b）在CASIA-MFSD上的数据集间类型测试，当在单一Replay-Attack数据集（见绿色柱）或包括OULU-NPU、MSU-MFSD和Replay-Attack（见紫色柱）的多个数据集上训练时。（c）在SiW-M上的数据集内交叉类型测试，采用留一类型的设置。（d）在3D面具FAS数据集（包括HKBU-MARs
    [[75](#bib.bib75)] 和CASIA-SURF 3DMask）上的数据集间交叉类型测试，当在OULU-NPU和SiW数据集上训练时，仅包含2D攻击。
- en: 2.2 Datasets for Face Anti-Spoofing
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 面部反欺诈数据集
- en: 'Large-scale and diverse datasets are pivotal for deep learning based methods
    during both training and evaluating phases. We summarize prevailing public FAS
    datasets in Table [I](#S2.T1 "TABLE I ‣ 2.1 Face Spoofing Attacks ‣ 2 background
    ‣ Deep Learning for Face Anti-Spoofing: A Survey") in terms of data amount, subject
    numbers, modality/sensor, environmental setup, and attack types. We also visualize
    some samples under different environmental conditions and modalities in Fig. [4](#S1.F4
    "Figure 4 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing: A Survey")(a)
    and (b), respectively.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '大规模和多样化的数据集在深度学习方法的训练和评估阶段至关重要。我们在表[I](#S2.T1 "TABLE I ‣ 2.1 Face Spoofing
    Attacks ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing: A Survey")中总结了当前流行的公共FAS数据集，涉及数据量、受试者数量、模态/传感器、环境设置和攻击类型。我们还在图[4](#S1.F4
    "Figure 4 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing: A Survey")（a）和（b）中可视化了在不同环境条件和模态下的一些样本。'
- en: 'It can be seen from Table [I](#S2.T1 "TABLE I ‣ 2.1 Face Spoofing Attacks ‣
    2 background ‣ Deep Learning for Face Anti-Spoofing: A Survey") that most datasets [[18](#bib.bib18),
    [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72),
    [73](#bib.bib73)] contain only a few attack types (e.g., print and replay attacks)
    under simple recording conditions (e.g., indoor scene) from the early stage (i.e.,
    year 2010-2015), which have limited variations in samples for generalized FAS
    training and evaluation. Subsequently, there are three main trends for dataset
    progress: 1) large scale data amount. For example, the recently released datasets
    CelebA-Spoof [[44](#bib.bib44)] and HiFiMask [[59](#bib.bib59)] contain more than
    600000 images and 50000 videos, respectively, where most of them are with PAs;
    2) diverse data distribution. Besides common print and replay attacks recorded
    in controllable indoor scenario, more and more novel attack types as well as complex
    recording conditions are considered in recent FAS datasets. For example, there
    are 13 fine-grained attack types in SiW-M [[38](#bib.bib38)] while HiFiMask [[59](#bib.bib59)]
    consists of 3D masks attacks with three kinds of materials (transparent, plaster,
    resin) recorded under six lighting conditions and six indoor/outdoor scenes; and
    3) multiple modalities and specialized sensors. Apart from traditional visible
    RGB camera, some recent datasets also consider various modalities (e.g., NIR [[55](#bib.bib55),
    [91](#bib.bib91), [90](#bib.bib90), [45](#bib.bib45)], Depth [[55](#bib.bib55),
    [91](#bib.bib91), [90](#bib.bib90), [45](#bib.bib45)], Thermal [[55](#bib.bib55),
    [45](#bib.bib45)], and SWIR [[55](#bib.bib55)]) and other specialized sensors
    (e.g., Light field camera [[82](#bib.bib82), [87](#bib.bib87)]). All these advanced
    factors facilitate the area of FAS in both academic research and industrial deployment.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '从表格 [I](#S2.T1 "TABLE I ‣ 2.1 Face Spoofing Attacks ‣ 2 background ‣ Deep Learning
    for Face Anti-Spoofing: A Survey") 可以看出，大多数数据集 [[18](#bib.bib18), [68](#bib.bib68),
    [69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73)]
    仅包含在简单录制条件下（例如室内场景）下的少数攻击类型（例如打印和重播攻击），这些数据主要来自早期阶段（即2010-2015年），样本变化有限，对泛化FAS训练和评估具有限制。随后，数据集进展有三个主要趋势：1)
    大规模数据量。例如，最近发布的数据集CelebA-Spoof [[44](#bib.bib44)] 和 HiFiMask [[59](#bib.bib59)]
    分别包含超过600000张图像和50000个视频，其中大部分带有PA；2) 多样的数据分布。除了在可控的室内场景中记录的常见打印和重播攻击外，最近的FAS数据集还考虑了越来越多的新型攻击类型以及复杂的录制条件。例如，SiW-M [[38](#bib.bib38)]
    中有13种细粒度攻击类型，而HiFiMask [[59](#bib.bib59)] 包含三种材料（透明、石膏、树脂）下记录的3D面具攻击，录制了六种光照条件和六种室内/室外场景；3)
    多模态和专业传感器。除了传统的可见RGB相机，最近的一些数据集还考虑了各种模态（例如NIR [[55](#bib.bib55), [91](#bib.bib91),
    [90](#bib.bib90), [45](#bib.bib45)], Depth [[55](#bib.bib55), [91](#bib.bib91),
    [90](#bib.bib90), [45](#bib.bib45)], Thermal [[55](#bib.bib55), [45](#bib.bib45)],
    和 SWIR [[55](#bib.bib55)]）和其他专业传感器（例如光场相机 [[82](#bib.bib82), [87](#bib.bib87)]）。所有这些先进因素促进了FAS在学术研究和工业应用中的发展。'
- en: '![Refer to caption](img/a09f2b77941c8b71ac1d0bbe2fa7ab75.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a09f2b77941c8b71ac1d0bbe2fa7ab75.png)'
- en: 'Figure 6: Chronological overview of the milestone deep learning based FAS methods
    using commercial RGB camera.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：基于商业RGB相机的深度学习里程碑方法的时间顺序概述。
- en: 2.3 Evaluation Metrics
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 评估指标
- en: As FAS systems usually focus on the concept of bonafide and PA acceptance and
    rejection, two basic metrics False Rejection Rate (FRR) and False Acceptance Rate
    (FAR) [[93](#bib.bib93)] are widely used. The ratio of incorrectly accepted spoofing
    attacks defines FAR, whereas FRR stands for the ratio of incorrectly rejected
    live accesses [[94](#bib.bib94)]. FAS follows ISO/IEC DIS 30107- 3:2017 [[95](#bib.bib95)]
    standards to evaluate the performance of the FAS systems under different scenarios.
    The most commonly used metrics in both intra- and cross-testing scenarios is Half
    Total Error Rate (HTER) [[94](#bib.bib94)], Equal Error Rate (EER) [[67](#bib.bib67)],
    and Area Under the Curve (AUC). HTER is found out by calculating the average of
    FRR (ratio of incorrectly rejected bonafide score) and FAR (ratio of incorrectly
    accepted PA). EER is a specific value of HTER at which FAR and FRR have equal
    values. AUC represents the degree of separability between bonafide and spoofings.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 由于面部反欺诈系统通常关注真人和PA的接受和拒绝概念，所以广泛采用两个基本指标：虚警率（FAR）和漏警率（FRR）[[93](#bib.bib93)]。虚警率定义为错误接受欺骗攻击的比率，而漏警率表示错误拒绝活体访问的比率[[94](#bib.bib94)]。面部反欺诈遵循ISO/IEC
    DIS 30107- 3:2017[[95](#bib.bib95)]标准，评估不同场景下的系统性能。在数据集内部和跨测试场景中最常用的指标是一半总误差率（HTER）[[94](#bib.bib94)]、等误差率（EER）[[67](#bib.bib67)]和曲线下面积（AUC）。HTER通过计算漏警率（错误拒绝真人的比率）和虚警率（错误接受PA的比率）的平均值得出。EER是在FAR和FRR相等时HTER的特定值。AUC代表真人和欺骗之间的可分离程度。
- en: Recently, Attack Presentation Classification Error Rate (APCER), Bonafide Presentation
    Classification Error Rate (BPCER) and Average Classification Error Rate (ACER)
    suggested in ISO standard [[95](#bib.bib95)] are also used for intra-dataset testings [[77](#bib.bib77),
    [13](#bib.bib13)]. BPCER and APCER measure bonafide and attack classification
    error rates, respectively. ACER is calculated as the mean of BPCER and APCER,
    evaluating the reliability of intra-dataset performance.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，国际标准组织（ISO）标准[[95](#bib.bib95)]中提到的攻击演示分类错误率（APCER）、真人演示分类错误率（BPCER）和平均分类错误率（ACER）也被用于数据集内部测试[[77](#bib.bib77),
    [13](#bib.bib13)]。BPCER和APCER分别衡量真人和攻击分类错误率。ACER是BPCER和APCER的平均值，用于评估数据集内部性能的可靠性。
- en: 2.4 Evaluation Protocols
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 评估协议
- en: 'To evaluate the discrimination and generalization capacities of the deep FAS
    models, various protocols have been established. We summarize the development
    of deep FAS approaches on four representative protocols in Fig. [5](#S2.F5 "Figure
    5 ‣ 2.1 Face Spoofing Attacks ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing:
    A Survey") and Tables-A 2, 3 and 4 (in Appendix).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估深度面部反欺诈模型的辨别能力和泛化能力，已经建立了多种协议。我们在图[5](#S2.F5 "Figure 5 ‣ 2.1 Face Spoofing
    Attacks ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing: A Survey")和附录的表-A
    2、3和4中总结了深度面部反欺诈方法在四个代表性协议上的发展。'
- en: 'Intra-Dataset Intra-Type Protocol. Intra-dataset intra-type protocol has been
    widely used in most FAS datasets to evaluate the model’s discrimination ability
    for spoofing detection under scenarios with slight domain shift. As the training
    and testing data are sampled from the same datasets, they share similar domain
    distribution in terms of the recording environment, subject behavior, etc. (see
    Fig. [4](#S1.F4 "Figure 4 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")(a) for examples). The most classical intra-dataset intra-type testing
    is the Protocol-4 of OULU-NPU dataset [[77](#bib.bib77)], and the performance
    comparison of recent deep FAS methods on this protocol is shown in Fig. [5](#S2.F5
    "Figure 5 ‣ 2.1 Face Spoofing Attacks ‣ 2 background ‣ Deep Learning for Face
    Anti-Spoofing: A Survey")(a). Due to the strong discriminative feature representation
    ability via deep learning, many methods (e.g., CDCN++ [[23](#bib.bib23)], FAS-SGTD [[96](#bib.bib96)],
    Disentangled [[97](#bib.bib97)], MT-FAS [[43](#bib.bib43)], DC-CDN [[98](#bib.bib98)],
    STDN [[99](#bib.bib99)], NAS-FAS [[37](#bib.bib37)], FasTCo [[100](#bib.bib100)]
    and PatchNet [[101](#bib.bib101)]) have reached satisfied performance ($\textless$5%
    ACER) under small domain shifts in terms of external environment, attack mediums
    and recording camera variation. More intra-dataset intra-type results on OULU-NPU
    (4 sub-protocols) and SiW (3 sub-protocols) datasets are listed in Table-A 2 (in
    Appendix).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '内数据集内类型协议。 内数据集内类型协议已被广泛应用于大多数FAS数据集中，以评估模型在轻微领域变化场景下的伪造检测能力。由于训练和测试数据都取自相同的数据集，它们在录音环境、受试者行为等方面具有相似的领域分布（参见图[4](#S1.F4
    "Figure 4 ‣ 1 Introduction ‣ Deep Learning for Face Anti-Spoofing: A Survey")(a)的示例）。最经典的内数据集内类型测试是OULU-NPU数据集的Protocol-4[[77](#bib.bib77)]，最近深度FAS方法在此协议下的性能比较见图[5](#S2.F5
    "Figure 5 ‣ 2.1 Face Spoofing Attacks ‣ 2 background ‣ Deep Learning for Face
    Anti-Spoofing: A Survey")(a)。由于深度学习具有强大的判别特征表示能力，许多方法（例如，CDCN++ [[23](#bib.bib23)]、FAS-SGTD [[96](#bib.bib96)]、Disentangled [[97](#bib.bib97)]、MT-FAS [[43](#bib.bib43)]、DC-CDN [[98](#bib.bib98)]、STDN [[99](#bib.bib99)]、NAS-FAS [[37](#bib.bib37)]、FasTCo [[100](#bib.bib100)]和PatchNet [[101](#bib.bib101)]）在外部环境、攻击介质和录制摄像机变化方面的小领域偏移下均达到了令人满意的性能（$\textless$5%
    ACER）。OULU-NPU（4个子协议）和SiW（3个子协议）数据集的更多内数据集内类型结果列于附录中的表-A 2。'
- en: 'Cross-Dataset Intra-Type Protocol. This protocol focuses on cross-dataset level
    domain generalization ability measurement, which usually trains models on one
    or several datasets (source domains) and then tests on unseen datasets (shifted
    target domain). We summarize recent deep FAS approaches on two favorite cross-dataset
    testings [[23](#bib.bib23), [48](#bib.bib48)] in Fig. [5](#S2.F5 "Figure 5 ‣ 2.1
    Face Spoofing Attacks ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing: A
    Survey")(b). It can be seen from green columns that, when trained on Replay-Attack
    and tested on CASIA-MFSD, most deep models perform poorly ($\textgreater$20% HTER)
    due to the serious lighting and camera resolution variations. In contrast, when
    trained on multiple source datasets (i.e., OULU-NPU, MSU-MFSD, and Replay-Attack),
    domain generalization based methods achieve acceptable performance (especially
    SSDG [[51](#bib.bib51)] and SSAN [[102](#bib.bib102)] with 10.44% and 10.00% HTER,
    respectively). In real-world cross-testing cases, small amount of target domain
    data are easily obtained, which can also be utilized for domain adaptation [[103](#bib.bib103)]
    to mitigate domain shifts further. More cross-dataset intra-type testings among
    OULU-NPU, CASIA-MFSD, Replay-Attack, and MSU-MFSD datasets with different numbers
    of source domains for training can be found in Table-A 3 (in Appendix).'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '跨数据集内类型协议。 该协议侧重于跨数据集层面的领域泛化能力测量，通常在一个或多个数据集（源领域）上训练模型，然后在未见过的数据集（偏移目标领域）上进行测试。我们在图[5](#S2.F5
    "Figure 5 ‣ 2.1 Face Spoofing Attacks ‣ 2 background ‣ Deep Learning for Face
    Anti-Spoofing: A Survey")(b)中总结了最近的深度FAS方法在两种常见的跨数据集测试中的表现[[23](#bib.bib23)、[48](#bib.bib48)]。从绿色柱子可以看出，当在Replay-Attack上训练并在CASIA-MFSD上测试时，大多数深度模型由于严重的光照和摄像机分辨率变化表现不佳（$\textgreater$20%
    HTER）。相比之下，当在多个源数据集（即OULU-NPU、MSU-MFSD和Replay-Attack）上训练时，基于领域泛化的方法实现了可接受的性能（尤其是SSDG [[51](#bib.bib51)]和SSAN [[102](#bib.bib102)]，分别为10.44%和10.00%
    HTER）。在实际的跨测试案例中，可以轻松获得少量目标领域数据，这些数据也可用于领域适应[[103](#bib.bib103)]，以进一步缓解领域偏移。在附录中的表-A
    3可以找到在不同数量源领域进行训练的OULU-NPU、CASIA-MFSD、Replay-Attack和MSU-MFSD数据集之间的更多跨数据集内类型测试。'
- en: 'Intra-Dataset Cross-Type Protocol. The protocol adopts ‘leave one attack type
    out’ to validate the model’s generalization for unknown attack types, i.e., one
    kind of attack type only appears in the testing stage. Considering the rich (13
    kinds) attack types, SiW-M [[38](#bib.bib38)] is investigated in this protocol,
    and the corresponding results are illustrated in Fig. [5](#S2.F5 "Figure 5 ‣ 2.1
    Face Spoofing Attacks ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing: A
    Survey")(c). Most of the deep models achieve around 10% EER and with large standard
    deviations among all attack types, which indicates the huge challenges in this
    protocol. Benefited from the large-scale pretraining, ViTranZFAS [[38](#bib.bib38)]
    achieves surprising 6.7% EER, implying the promising usage of transfer learning
    for unknown attack type detection. Detailed intra-dataset cross-type testing results
    on SiW-M with the leave-one-type-out setting are shown in Table-A 4 (in Appendix).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集内跨类型协议。该协议采用“留出一个攻击类型”来验证模型对未知攻击类型的泛化能力，即一种攻击类型仅在测试阶段出现。考虑到丰富的（13种）攻击类型，在该协议中调查了
    SiW-M [[38](#bib.bib38)]，对应的结果如图[5](#S2.F5 "图 5 ‣ 2.1 面部欺诈攻击 ‣ 2 背景 ‣ 深度学习在面部反欺诈中的应用：综述")
    (c) 所示。大多数深度模型在所有攻击类型中的 EER 约为 10%，且标准差较大，这表明该协议面临巨大挑战。得益于大规模预训练，ViTranZFAS [[38](#bib.bib38)]
    实现了令人惊讶的 6.7% EER，表明迁移学习在未知攻击类型检测中的应用前景广阔。SiW-M 数据集内跨类型测试结果（留一个类型外设置）详见附录中的表 A
    4。
- en: 'Cross-Dataset Cross-Type Protocol. Although the three protocols mentioned above
    mimic most factors in real-world applications, they do not consider the most challenging
    case, i.e., cross-dataset cross-type testing. [[37](#bib.bib37)] proposes a ’Cross-Dataset
    Cross-Type Protocol’ to measure the FAS model’s generalization on both unseen
    domain and unknown attack types. In this protocol, OULU-NPU and SiW (with 2D attacks)
    are mixed for training, while HKBU-MARs and 3DMask (with 3D attacks) are used
    for testing. It can be seen from Fig. [5](#S2.F5 "Figure 5 ‣ 2.1 Face Spoofing
    Attacks ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing: A Survey")(d) that
    recent deep models (DTN [[38](#bib.bib38)] and NAS-FAS [[37](#bib.bib37)]) hold
    good generalization for lab-controlled low-fidelity 3D mask detection on HKBU-MARs
    but still cannot satisfactorily detect unrestricted high fidelity masks on 3DMask.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 跨数据集跨类型协议。尽管上述三种协议模拟了大多数实际应用中的因素，但它们并未考虑最具挑战性的情况，即跨数据集跨类型测试。[[37](#bib.bib37)]
    提出了“跨数据集跨类型协议”，以测量FAS模型在未知领域和未知攻击类型上的泛化能力。在该协议中，将 OULU-NPU 和 SiW（带有 2D 攻击）混合用于训练，而
    HKBU-MARs 和 3DMask（带有 3D 攻击）则用于测试。从图[5](#S2.F5 "图 5 ‣ 2.1 面部欺诈攻击 ‣ 2 背景 ‣ 深度学习在面部反欺诈中的应用：综述")
    (d) 可以看出，近年来的深度模型（DTN [[38](#bib.bib38)] 和 NAS-FAS [[37](#bib.bib37)]）在 HKBU-MARs
    上对实验室控制的低保真 3D 面具检测表现出良好的泛化能力，但仍无法令人满意地检测 3DMask 上的无限制高保真面具。
- en: 'Besides these four mainstream evaluation protocols, more new trends about practical
    protocol settings (e.g., semi-/un-supervised, real-world open-set, and dynamic
    multimodality) will be discussed in Section [5](#S5 "5 Discussion and Future Directions
    ‣ Deep Learning for Face Anti-Spoofing: A Survey").'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这四种主流评估协议，关于实际协议设置的新趋势（例如，半监督/无监督、现实世界开放集和动态多模态）将在第[5](#S5 "5 讨论与未来方向 ‣ 深度学习在面部反欺诈中的应用：综述")节中讨论。
- en: 3 Deep FAS with Commercial RGB Camera
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于商业 RGB 摄像头的深度 FAS
- en: 'As commercial RGB camera is widely used in many real-world applicational scenarios
    (e.g., access control system and mobile device unlocking), in this section, we
    will review existing commercial RGB camera based FAS methods. Several milestone
    deep FAS methods are illustrated in Fig. [6](#S2.F6 "Figure 6 ‣ 2.2 Datasets for
    Face Anti-Spoofing ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing: A Survey").'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于商业 RGB 摄像头在许多现实世界应用场景中被广泛使用（例如，门禁系统和移动设备解锁），在本节中，我们将回顾现有的基于商业 RGB 摄像头的 FAS
    方法。几个里程碑式的深度 FAS 方法在图[6](#S2.F6 "图 6 ‣ 2.2 面部反欺诈数据集 ‣ 2 背景 ‣ 深度学习在面部反欺诈中的应用：综述")中进行了说明。
- en: 3.1 Hybrid (Handcraft + Deep Learning) Method
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 混合（手工制作 + 深度学习）方法
- en: Although deep learning and CNNs have achieved great success in many computer
    vision tasks (e.g., image classification [[104](#bib.bib104), [105](#bib.bib105)],
    semantic segmentation [[106](#bib.bib106)], and object detection [[107](#bib.bib107)]),
    they suffer from the overfitting problem for the FAS task due to the limited amount
    and diversity of the training data. As handcrafted features (e.g., LBP [[108](#bib.bib108)],
    HOG [[109](#bib.bib109)] descriptors, image quality [[110](#bib.bib110)], optical
    flow motion [[111](#bib.bib111)], and rPPG clues [[112](#bib.bib112)]) have been
    proven to be discriminative to distinguish bonafide from PAs, some recent hybrid
    works combine handcrafted features with deep features for FAS. Typical properties
    of these hybrid methods are summarized in Table-A 5 (in Appendix).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习和卷积神经网络（CNN）在许多计算机视觉任务中（如图像分类 [[104](#bib.bib104), [105](#bib.bib105)]，语义分割 [[106](#bib.bib106)]，和目标检测 [[107](#bib.bib107)]）取得了巨大成功，但由于训练数据的数量和多样性有限，它们在FAS任务中仍然面临过拟合问题。由于手工特征（如LBP [[108](#bib.bib108)]，HOG [[109](#bib.bib109)]描述符，图像质量 [[110](#bib.bib110)]，光流运动 [[111](#bib.bib111)]，以及rPPG线索 [[112](#bib.bib112)]）已经被证明对区分真实与伪造图像具有很强的判别能力，近期一些混合工作将手工特征与深度特征结合用于FAS。这些混合方法的典型特性总结在附录的表-A
    5中。
- en: 'Some FAS approaches firstly extract handcrafted features from face inputs,
    and then employ CNNs for semantic feature representation (see Fig. [7](#S3.F7
    "Figure 7 ‣ 3.1 Hybrid (Handcraft + Deep Learning) Method ‣ 3 Deep FAS with Commercial
    RGB Camera ‣ Deep Learning for Face Anti-Spoofing: A Survey")(a) for paradigm).
    On one hand, color texture based static features are extracted from each frame,
    and then are feed into the deep model. Based on the rich low-level texture features,
    deep model is able to mine texture-aware semantic clues. To this end, Cai and
    Chen [[113](#bib.bib113)] adopt multi-scale color LBP features as local texture
    descriptors, then a random forest is cascaded for semantic representation. Similarly,
    Khammari [[22](#bib.bib22)] extracts LBP and Weber local descriptor encoded CNN
    features, which are combined to preservate the local intensity and edge orientation
    information. However, compared with the original face input, local descriptor
    based features lose pixel-level details thus limiting the model performance. On
    the other hand, dynamic features (e.g., motion, illumination changes, physiological
    signals) across temporal frames are also effective CNN inputs. Feng et al. [[114](#bib.bib114)]
    propose to train a multi-layer perceptron from the extracted dense optical flow-based
    motions, which reveal anomalies in print attacks. Moreover, Yu et al. [[115](#bib.bib115)]
    construct spatio-temporal rPPG maps from face videos, and use a vision transformer
    to capture the periodic heartbeat liveness features for the bonafide. However,
    head motions and rPPG signals are easily imitated in the replay attack, making
    such dynamic clues less reliable. Basing on the fact that replay attacks usually
    have abnormal reflection changes, Li et al. [[116](#bib.bib116)] propose to capture
    such illumination changes using a 1D CNN with inputs of the intensity difference
    histograms from reflectance images.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一些FAS方法首先从面部输入中提取手工特征，然后采用CNN进行语义特征表示（参见图 [7](#S3.F7 "图7 ‣ 3.1 混合（手工 + 深度学习）方法
    ‣ 3 使用商业RGB相机的深度FAS ‣ 面部反欺诈的深度学习：综述")(a) 的范式）。一方面，从每一帧中提取基于颜色纹理的静态特征，然后输入深度模型。基于丰富的低级纹理特征，深度模型能够挖掘出纹理感知的语义线索。为此，Cai和Chen [[113](#bib.bib113)]
    采用多尺度颜色LBP特征作为局部纹理描述符，然后级联一个随机森林进行语义表示。同样，Khammari [[22](#bib.bib22)] 提取了编码了CNN特征的LBP和Weber局部描述符，这些特征被结合以保留局部强度和边缘方向信息。然而，与原始面部输入相比，基于局部描述符的特征丢失了像素级细节，从而限制了模型的性能。另一方面，跨时间帧的动态特征（如运动、光照变化、生理信号）也是有效的CNN输入。Feng等人 [[114](#bib.bib114)]
    提出从提取的密集光流运动中训练一个多层感知机，这些运动揭示了打印攻击中的异常。此外，Yu等人 [[115](#bib.bib115)] 从面部视频中构建了时空rPPG图，并使用视觉变换器捕捉周期性心跳活性特征以辨别真实。然而，头部运动和rPPG信号在重放攻击中容易被模仿，使得这些动态线索的可靠性降低。基于重放攻击通常具有异常反射变化这一事实，Li等人 [[116](#bib.bib116)]
    提出使用1D CNN捕捉这些光照变化，输入为反射图像的强度差异直方图。
- en: '![Refer to caption](img/b834af76b0cfd8360a79cd86df9e7c0a.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b834af76b0cfd8360a79cd86df9e7c0a.png)'
- en: 'Figure 7: Hybrid frameworks for FAS. (a) Deep features from handcrafted features.
    (b) Handcrafted features from deep features. (c) Fused handcrafted and deep features.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：FAS的混合框架。（a）来自手工特征的深度特征。（b）来自深度特征的手工特征。（c）融合的手工特征和深度特征。
- en: 'Several other hybrid FAS methods extract handcrafted features from deep convolutional
    features, which follow the hybrid framework in Fig. [7](#S3.F7 "Figure 7 ‣ 3.1
    Hybrid (Handcraft + Deep Learning) Method ‣ 3 Deep FAS with Commercial RGB Camera
    ‣ Deep Learning for Face Anti-Spoofing: A Survey")(b). To reduce the FAS-unrelated
    redundancy, Li et al. [[30](#bib.bib30)] utilize the block principal component
    analysis (PCA) to filter out the irrelevant deep features from pretrained VGG-face
    model. Besides PCA-based dimension reduction, Agarwal et al. [[117](#bib.bib117)]
    explicitly extract the color LBP descriptor from the shallow convolutional features,
    which contains richer low-level statistics. In addition to static spoof patterns,
    some works also explore handcrafted dynamic temporal clues from well-trained deep
    models. Asim et al. [[20](#bib.bib20)] and Shao et al. [[118](#bib.bib118)] extract
    deep dynamic textures and motion features using LBP-TOP [[119](#bib.bib119)] and
    optical flow from the sequential convolutional features, respectively. One limitation
    of this hybrid framework is that the handcrafted features are highly dependent
    on the well-trained convolutional features, and it is still unknown whether shallow
    or deep convolutional features are more suitable for different kinds of handcrafted
    features.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '其他几种混合FAS方法从深度卷积特征中提取手工制作的特征，这些方法遵循了图[7](#S3.F7 "Figure 7 ‣ 3.1 Hybrid (Handcraft
    + Deep Learning) Method ‣ 3 Deep FAS with Commercial RGB Camera ‣ Deep Learning
    for Face Anti-Spoofing: A Survey")(b)中的混合框架。为了减少与FAS无关的冗余，Li等人[[30](#bib.bib30)]利用块主成分分析（PCA）从预训练的VGG-face模型中过滤掉无关的深度特征。除了基于PCA的维度减少，Agarwal等人[[117](#bib.bib117)]还明确从浅层卷积特征中提取颜色LBP描述符，该描述符包含更丰富的低级统计信息。除了静态欺骗模式，一些工作还探索了从训练良好的深度模型中提取的手工制作的动态时间线索。Asim等人[[20](#bib.bib20)]和Shao等人[[118](#bib.bib118)]分别使用LBP-TOP[[119](#bib.bib119)]和光流从顺序卷积特征中提取深度动态纹理和运动特征。该混合框架的一个限制是手工制作的特征高度依赖于训练良好的卷积特征，仍不清楚浅层还是深层卷积特征更适合不同种类的手工制作特征。'
- en: 'As handcrafted and deep convolutional features hold different properties, another
    favorite hybrid framework (see Fig. [7](#S3.F7 "Figure 7 ‣ 3.1 Hybrid (Handcraft
    + Deep Learning) Method ‣ 3 Deep FAS with Commercial RGB Camera ‣ Deep Learning
    for Face Anti-Spoofing: A Survey")(c)) fuses them for more generic representation.
    To make more reliable predictions, Sharifi [[120](#bib.bib120)] proposes to fuse
    the predicted scores from both handcrafted LBP features and deep VGG16 model.
    However, it is challenging to determine the optimal score weights for these two
    kinds of features. Besides score-level fusion, Rehmana et al. [[121](#bib.bib121),
    [21](#bib.bib21)] propose to utilize HOG and LBP maps to perturb and modulate
    the low-level convolutional features. Despite the fact that local prior knowledge
    from handcrafted features enhances discriminative capacity, the overall model
    suffers from semantic representation degradation. In terms of the temporal methods,
    to leverage the dynamic discrepancy between the bonafide and PAs, Li et al. [[122](#bib.bib122)]
    extract intensity variation features via 1D CNN, which are fused with the motion
    blur features from motion magnified face videos for replay attack detection.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '由于手工制作特征和深度卷积特征具有不同的属性，另一个受欢迎的混合框架（见图[7](#S3.F7 "Figure 7 ‣ 3.1 Hybrid (Handcraft
    + Deep Learning) Method ‣ 3 Deep FAS with Commercial RGB Camera ‣ Deep Learning
    for Face Anti-Spoofing: A Survey")(c)）将它们融合以获得更通用的表示。为了做出更可靠的预测，Sharifi[[120](#bib.bib120)]提出将手工制作的LBP特征和深度VGG16模型的预测分数进行融合。然而，确定这两种特征的最佳分数权重具有挑战性。除了分数级融合，Rehmana等人[[121](#bib.bib121),
    [21](#bib.bib21)]建议利用HOG和LBP图来扰动和调节低级卷积特征。尽管来自手工制作特征的局部先验知识增强了判别能力，但整体模型仍然受到语义表示退化的影响。在时间方法方面，为了利用真实和伪造样本之间的动态差异，Li等人[[122](#bib.bib122)]通过1D
    CNN提取强度变化特征，并将其与来自运动放大面部视频的运动模糊特征融合，用于回放攻击检测。'
- en: 'Overall, benefitted from the explicit expert-designed feature extraction, hybrid
    methods are able to represent particular non-texture clues (e.g., temporal rPPG
    and motion blur), which are hard to capture via end-to-end texture-based FAS models.
    However, the shortcomings are also obvious: 1) handcrafted features highly rely
    on the expert knowledge and not learnable, which are inefficient once enough training
    data are available; and 2) there might be feature gaps/incompatibility between
    handcrafted and deep features, resulting in performance saturation.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，由于显式的专家设计特征提取的好处，混合方法能够表示特定的非纹理线索（例如，时间相对光谱图（rPPG）和运动模糊），这些线索很难通过端到端的基于纹理的FAS模型捕捉。然而，缺点也很明显：1）手工制作的特征高度依赖于专家知识，并且不可学习，一旦有足够的训练数据可用时效率较低；2）手工特征和深度特征之间可能存在特征差距/不兼容，导致性能饱和。
- en: 3.2 Traditional Deep Learning Method
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 传统深度学习方法
- en: 'Benefited from the development of the advanced CNN architectures [[105](#bib.bib105),
    [123](#bib.bib123)] and regularization [[124](#bib.bib124), [125](#bib.bib125)]
    techniques as well as the recent released large-scale FAS datasets [[77](#bib.bib77),
    [44](#bib.bib44), [59](#bib.bib59)], end-to-end deep learning based methods attract
    more and more attention, and dominate the field of FAS. Different from the hybrid
    methods which integrate parts of handcrafted features without learnable parameters,
    Traditional deep learning based methods directly learn the mapping functions from
    face inputs to spoof detection. Traditional deep learning frameworks usually include:
    1) direct supervision with binary cross-entropy loss (see Fig. [8](#S3.F8 "Figure
    8 ‣ 3.2.2 Pixel-wise Supervision ‣ 3.2 Traditional Deep Learning Method ‣ 3 Deep
    FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing: A Survey")(a));
    and 2) pixel-wise supervision with auxiliary tasks (see Fig. [8](#S3.F8 "Figure
    8 ‣ 3.2.2 Pixel-wise Supervision ‣ 3.2 Traditional Deep Learning Method ‣ 3 Deep
    FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing: A Survey")(b))
    or generative models (see Fig. [8](#S3.F8 "Figure 8 ‣ 3.2.2 Pixel-wise Supervision
    ‣ 3.2 Traditional Deep Learning Method ‣ 3 Deep FAS with Commercial RGB Camera
    ‣ Deep Learning for Face Anti-Spoofing: A Survey")(c)).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '由于先进CNN架构 [[105](#bib.bib105), [123](#bib.bib123)] 和正则化 [[124](#bib.bib124),
    [125](#bib.bib125)] 技术的发展以及最近发布的大规模FAS数据集 [[77](#bib.bib77), [44](#bib.bib44),
    [59](#bib.bib59)]，基于端到端深度学习的方法越来越受到关注，并主导了FAS领域。不同于融合部分手工特征的混合方法，传统的深度学习方法直接学习从面部输入到伪造检测的映射函数。传统的深度学习框架通常包括：1）用二元交叉熵损失直接监督（见图 [8](#S3.F8
    "Figure 8 ‣ 3.2.2 Pixel-wise Supervision ‣ 3.2 Traditional Deep Learning Method
    ‣ 3 Deep FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")(a)）；2）用辅助任务（见图 [8](#S3.F8 "Figure 8 ‣ 3.2.2 Pixel-wise Supervision
    ‣ 3.2 Traditional Deep Learning Method ‣ 3 Deep FAS with Commercial RGB Camera
    ‣ Deep Learning for Face Anti-Spoofing: A Survey")(b)）或生成模型（见图 [8](#S3.F8 "Figure
    8 ‣ 3.2.2 Pixel-wise Supervision ‣ 3.2 Traditional Deep Learning Method ‣ 3 Deep
    FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing: A Survey")(c)）进行像素级监督。'
- en: 3.2.1 Direct Supervision with Binary Cross Entropy Loss
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 使用二元交叉熵损失的直接监督
- en: As FAS can be intuitively treated as a binary (bonafide vs. PA) classification
    task, numerous end-to-end deep learning methods are directly supervised with binary
    cross-entropy (CE) loss as well as extended losses (e.g., triplet loss [[126](#bib.bib126)]),
    which are summarized in Table-A 6 (in Appendix).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 由于FAS可以直观地视为一个二分类（真实 vs. 假冒）任务，许多端到端的深度学习方法直接用二元交叉熵（CE）损失以及扩展损失（例如，三元组损失 [[126](#bib.bib126)]）进行监督，这些方法在表A
    6（附录中）中总结。
- en: On one side, researchers have proposed various network architectures supervised
    by binary CE loss for FAS. Yang et al. [[29](#bib.bib29)] propose the first end-to-end
    deep FAS method using 8-layer shallow CNN for feature representation. However,
    due to the limited scale and diversity of datasets, CNN-based models easily overfit
    in the FAS task. To alleviate this issue, some works [[127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129)] finetune the ImageNet-pretrained models (e.g., VGG16, ResNet18
    and vision transformer) for FAS. Towards moblie-level FAS applications, Heusch
    et al. [[55](#bib.bib55)] consider using the lightweight MobileNetV2 [[130](#bib.bib130)]
    for efficient FAS. The above-mentioned generic backbones usually focus on high-level
    semantic representation while neglect low-level features, which are also important
    for spoof pattern mining. To better leverage the multi-scale features for FAS,
    Deb and Jain [[131](#bib.bib131)] propose to use a shallow fully convolutional
    network (FCN) to learn local discriminative cues from face images in a self-supervised
    manner. Besides the single-frame-based appearance features, several works [[132](#bib.bib132),
    [133](#bib.bib133), [25](#bib.bib25), [134](#bib.bib134)] consider the temporal
    discrepancy between bonafide and PAs, and cascade multi-frame-based CNN features
    with LSTM [[135](#bib.bib135)] for robust dynamic clues propagation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，研究人员提出了多种由二元CE损失监督的FAS网络架构。Yang等人[[29](#bib.bib29)]提出了第一种使用8层浅层CNN进行特征表示的端到端深度FAS方法。然而，由于数据集的规模和多样性有限，基于CNN的模型在FAS任务中容易过拟合。为了解决这个问题，一些工作[[127](#bib.bib127),
    [128](#bib.bib128), [129](#bib.bib129)]对ImageNet预训练的模型（例如VGG16、ResNet18和视觉变换器）进行微调以用于FAS。为了适应移动级别的FAS应用，Heusch等人[[55](#bib.bib55)]考虑使用轻量级的MobileNetV2[[130](#bib.bib130)]进行高效的FAS。上述通用骨干网络通常专注于高级语义表示，而忽视了同样重要的低级特征，这些特征对伪造模式挖掘也很重要。为了更好地利用多尺度特征进行FAS，Deb和Jain[[131](#bib.bib131)]提出使用浅层全卷积网络（FCN）以自监督的方式从面部图像中学习局部判别线索。除了基于单帧的外观特征外，几项工作[[132](#bib.bib132),
    [133](#bib.bib133), [25](#bib.bib25), [134](#bib.bib134)]考虑了真实样本和PAs之间的时间差异，并将多帧CNN特征与LSTM[[135](#bib.bib135)]级联，以实现动态线索的强鲁棒性传播。
- en: On the other side, considering the weak intra- and inter-class constraints from
    binary CE, a few works modify binary CE loss to provide CNNs more discriminative
    supervision signals. Instead of binary constraints, Xu et al. [[100](#bib.bib100)]
    rephrase FAS as a fine-grained classification problem, and the type labels (e.g.,
    bonafide, print, and replay) are used for multi-class supervision. In this way,
    the particular properties (e.g., materials) of PAs could be represented. However,
    FAS models supervised with multi-class CE loss still have confused live/spoof
    distributions especially on hard live/spoof samples. For instance, high-fidelity
    PAs have similar appearance clues as the corresponding bonafide. On one hand,
    to learn a compact space with small intra-class distances but large inter-class
    distances, Hao [[136](#bib.bib136)] and Almeida et al. [[80](#bib.bib80)] introduce
    contrastive loss and triplet loss, respectively. However, different from vision
    retrieval tasks, the bonafide and PAs in FAS task usually hold asymmetric intra-distributions
    (more compact and diverse, respectively). Based on this evidence, Wang et al. [[101](#bib.bib101)]
    propose to supervise the FAS patch models via an asymmetric angular-margin softmax
    loss to relax the intra-class constraints among PAs. On the other hand, to provide
    more confident predictions on hard samples, Chen et al. [[137](#bib.bib137)] adopt
    the binary focal loss to guide the model to enlarge the margin between live/spoof
    samples and achieve strong discrimination for hard samples.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，考虑到二元CE中的弱类内和类间约束，一些研究工作修改了二元CE损失，为CNN提供了更具判别性的监督信号。Xu等人[[100](#bib.bib100)]将FAS重新表述为细粒度分类问题，使用类型标签（例如真实、打印和重播）进行多类别监督。通过这种方式，可以表示PAs的特定属性（例如材料）。然而，使用多类别CE损失监督的FAS模型在难度较大的真实/伪造样本上仍然存在混淆的真实/伪造分布。例如，高保真PAs与相应的真实样本具有相似的外观线索。一方面，为了学习一个具有小类内距离但大类间距离的紧凑空间，Hao[[136](#bib.bib136)]和Almeida等人[[80](#bib.bib80)]分别引入了对比损失和三元组损失。然而，与视觉检索任务不同，FAS任务中的真实样本和PAs通常具有不对称的类内分布（分别更紧凑和多样）。基于这一证据，Wang等人[[101](#bib.bib101)]提出通过不对称角度边界softmax损失来监督FAS补丁模型，以放宽PAs之间的类内约束。另一方面，为了对难样本提供更有信心的预测，Chen等人[[137](#bib.bib137)]采用二元焦点损失来指导模型扩大真实/伪造样本之间的间隔，并实现对难样本的强判别。
- en: Overall, both binary CE loss and its extended losses are easy and efficient
    to use, which supervise deep FAS models to fastly converge. However, these supervision
    signals only provide global (spatial/temporal) constraints for live/spoof embedding
    learning, which may causes FAS models to easily overfit to unfaithful patterns.
    Furthermore, FAS models with binary supervision are usually black-box and the
    characteristic of their learned features are hard to understand.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，二进制交叉熵损失及其扩展损失使用简单高效，能够快速推动深度FAS模型收敛。然而，这些监督信号仅提供了对真实/伪造嵌入学习的全局（空间/时间）约束，这可能导致FAS模型容易过拟合到不真实的模式。此外，具有二进制监督的FAS模型通常是黑箱的，其学习特征的特性难以理解。
- en: 3.2.2 Pixel-wise Supervision
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 逐像素监督
- en: Deep models directly supervised by binary loss might easily learn unfaithful
    patterns (e.g., screen bezel). In contrast, pixel-wise supervision can provide
    more fine-grained and contextual task-related clues for better intrinsic feature
    learning. On one hand, based on the physical clues and discriminative design philosophy,
    auxiliary supervision signals such as pseudo depth labels [[26](#bib.bib26), [13](#bib.bib13)],
    binary mask label [[32](#bib.bib32), [38](#bib.bib38), [39](#bib.bib39)] and reflection
    maps [[24](#bib.bib24), [36](#bib.bib36)] are developed for local live/spoof clues
    description. On the other hand, generative models with explicit pixel-wise supervision
    (e.g., original face input reconstruction [[138](#bib.bib138), [42](#bib.bib42)])
    are recently utilized for generic spoof pattern estimation. We summarize the representative
    pixel-wise supervision methods in Table-A 7 (in Appendix).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型直接通过二进制损失进行监督可能会容易学习到不真实的模式（例如，屏幕边框）。相比之下，逐像素监督可以提供更多细粒度和上下文相关的任务线索，从而更好地进行内在特征学习。一方面，基于物理线索和区分设计理念，开发了如伪深度标签 [[26](#bib.bib26),
    [13](#bib.bib13)]、二进制掩码标签 [[32](#bib.bib32), [38](#bib.bib38), [39](#bib.bib39)]
    和反射图 [[24](#bib.bib24), [36](#bib.bib36)]等辅助监督信号，用于描述局部真实/伪造线索。另一方面，最近采用了具有显式逐像素监督的生成模型（例如，原始面部输入重建 [[138](#bib.bib138),
    [42](#bib.bib42)]），用于通用伪造模式估计。我们在表-A 7（附录中）总结了代表性的逐像素监督方法。
- en: '![Refer to caption](img/7107273accc9382dd243b0f7f1198718.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7107273accc9382dd243b0f7f1198718.png)'
- en: 'Figure 8: Traditional end-to-end deep learning frameworks for FAS. (a) Direct
    supervision with binary cross entropy loss. (b) Pixel-wise supervision with auxiliary
    tasks. (c) Pixel-wise supervision with generative model for implicit spoof pattern
    representation.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：传统的端到端深度学习FAS框架。（a）使用二进制交叉熵损失进行直接监督。（b）通过辅助任务进行逐像素监督。（c）通过生成模型进行逐像素监督，以隐式伪造模式表示。
- en: '![Refer to caption](img/2f9d4f0f7ac34bbdc1cb7b7c8f12bc2e.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f9d4f0f7ac34bbdc1cb7b7c8f12bc2e.png)'
- en: 'Figure 9: The shared architecture of CDCN [[23](#bib.bib23)]/DepthNet [[13](#bib.bib13)].
    Inside the blue block are the convolutional filters with 3x3 kernel size and their
    feature dimensionalities. ‘CDC’ and ‘Conv’ suggest central difference convolution
    adopted in CDCN and vanilla convolution adopted in DepthNet, respectively.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：CDCN [[23](#bib.bib23)]/DepthNet [[13](#bib.bib13)]的共享架构。蓝色块内是具有3x3卷积核的卷积滤波器及其特征维度。“CDC”和“Conv”分别表示CDCN中采用的中心差分卷积和DepthNet中采用的普通卷积。
- en: '![Refer to caption](img/5224492f7c060c553b003718cbb61615.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5224492f7c060c553b003718cbb61615.png)'
- en: 'Figure 10: The network structure of FAS-SGTD [[96](#bib.bib96)]. The inputs
    are consecutive frames with a fixed interval. Each frame is processed by cascaded
    Residual Spatial Gradient Block (RSGB) with a shared backbone which generates
    a corresponding coarse depth map. The number in RSGB cubes denotes the output
    channels. Spatio-Temporal Propagation Module (STPM) is plugged between frames
    for estimating the temporal depth and refining the corresponding coarse depth
    map.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：FAS-SGTD [[96](#bib.bib96)]的网络结构。输入是固定间隔的连续帧。每帧由级联残差空间梯度块（RSGB）处理，具有一个共享的主干网络，生成相应的粗略深度图。RSGB立方体中的数字表示输出通道。空间-时间传播模块（STPM）被插入帧之间，用于估计时间深度并细化相应的粗略深度图。
- en: 'Pixel-wise supervision with Auxiliary Task. According to the human prior knowledge
    of FAS, most PAs (e.g., plain printed paper and electronic screen) merely have
    no genuine facial depth information, which could be utilized as discriminative
    supervision signals. As a result, some recent works [[26](#bib.bib26), [139](#bib.bib139),
    [23](#bib.bib23), [96](#bib.bib96)] adopt pixel-wise pseudo depth labels to guide
    the deep models, enforcing them predict the genuine depth for live samples while
    zero maps for the spoof ones. Atoum et al. [[26](#bib.bib26)] first leverage pseudo
    depth labels to guide the multi-scale FCN (namely ‘DepthNet’ for simplicity).
    Thus, the well-trained DepthNet is able to predict holistic depth maps as decision
    evidence. To further improve the fine-grained intrinsic feature representation
    capacity, Yu et al. [[23](#bib.bib23)] replace vanilla convolution in DepthNet
    with central difference convolution (CDC) to form the CDCN architecture (see Fig. [9](#S3.F9
    "Figure 9 ‣ 3.2.2 Pixel-wise Supervision ‣ 3.2 Traditional Deep Learning Method
    ‣ 3 Deep FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing:
    A Survey") for detailed structures). In terms of static architectures, DepthNet
    and CDCN are favorite and widely used in the deep FAS community due to their compactness
    and excellent performance. Many recent variants [[37](#bib.bib37), [98](#bib.bib98),
    [140](#bib.bib140)] are established based on the DepthNet/CDCN. As for the temporal
    architectures, FAS-SGTD [[96](#bib.bib96)] is classical and well-known for its
    excellent short- and long-term micro-motion estimation, which can be utilized
    for accurate facial depth prediction. The detailed architecture of FAS-SGTD is
    illustrated in Fig. [10](#S3.F10 "Figure 10 ‣ 3.2.2 Pixel-wise Supervision ‣ 3.2
    Traditional Deep Learning Method ‣ 3 Deep FAS with Commercial RGB Camera ‣ Deep
    Learning for Face Anti-Spoofing: A Survey"), which is later modified and extended
    in a transformer counterpart [[141](#bib.bib141)].'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '辅助任务的像素级监督。根据对FAS的先验知识，大多数PA（例如，普通打印纸和电子屏幕）几乎没有真实的面部深度信息，这些信息可以作为判别性监督信号。因此，一些最近的工作[[26](#bib.bib26),
    [139](#bib.bib139), [23](#bib.bib23), [96](#bib.bib96)]采用像素级伪深度标签来指导深度模型，强制它们对真实样本预测真实的深度，而对伪造样本预测零深度图。Atoum等人[[26](#bib.bib26)]首次利用伪深度标签来指导多尺度FCN（为简化起见称为“DepthNet”）。因此，训练良好的DepthNet能够预测整体深度图作为决策依据。为了进一步提升细粒度内在特征表示能力，Yu等人[[23](#bib.bib23)]用中央差分卷积（CDC）替代DepthNet中的普通卷积，形成了CDCN架构（见图[9](#S3.F9
    "Figure 9 ‣ 3.2.2 Pixel-wise Supervision ‣ 3.2 Traditional Deep Learning Method
    ‣ 3 Deep FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")获取详细结构）。在静态架构方面，由于其紧凑性和优异性能，DepthNet和CDCN在深度FAS社区中受到青睐并被广泛使用。许多最近的变体[[37](#bib.bib37),
    [98](#bib.bib98), [140](#bib.bib140)]基于DepthNet/CDCN建立。至于时间架构，FAS-SGTD[[96](#bib.bib96)]因其出色的短期和长期微动作估计而著名，可以用于准确的面部深度预测。FAS-SGTD的详细架构如图[10](#S3.F10
    "Figure 10 ‣ 3.2.2 Pixel-wise Supervision ‣ 3.2 Traditional Deep Learning Method
    ‣ 3 Deep FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")所示，随后在变换器对等体[[141](#bib.bib141)]中进行了修改和扩展。'
- en: Synthesizing 3D shape labels for every training sample is costly and not accurate
    enough, and also lacks the reasonability for the PAs with real depth (e.g., 3D
    mask and Mannequin). In contrast, binary mask label [[38](#bib.bib38), [32](#bib.bib32),
    [142](#bib.bib142), [143](#bib.bib143), [99](#bib.bib99)] is easier to be generated
    and more generalizable to all PAs. Specifically, binary supervision would be provided
    for the deep embedding features in each spatial position. In other words, through
    the binary mask label, we can find whether PAs occur in the corresponding patches,
    which is attack-type-agnostic and spatially interpretable. George and Marcel [[32](#bib.bib32)]
    are the first to introduce deep pixel-wise binary supervision to predict the intermediate
    confidence map for the cascaded final binary classification. With sufficient pixel-wise
    supervision, the backbone DenseNet121 converges well and is able to provide patch-wise
    live/spoof predictions. As subtle spoof clues (e.g., moiré pattern) usually exist
    in different spatial regions with different intensity, vanilla pixel-wise binary
    supervision treats all patches with equal contributions, which might lead to biased
    feature representation. To tackle this issue, Hossaind et al. [[142](#bib.bib142)]
    propose to add a learnable attention module for feature refinement before calculating
    the deep pixel-wise binary loss, which benefits the salient information propagation.
    Though flexible and easy to use, current binary mask labels usually assume all
    pixels in the face region have the same live/spoof distributions thus generate
    all ‘one’ and ‘zero’ maps for bonafide and PAs, respectively. However, such labels
    are inaccurate and noisy to learn when encountering partial attacks (e.g., FunnyEye).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个训练样本合成 3D 形状标签是昂贵且不够准确的，同时对于具有真实深度的 PAs（例如 3D 面具和假人）也缺乏合理性。相对而言，二值面具标签 [[38](#bib.bib38),
    [32](#bib.bib32), [142](#bib.bib142), [143](#bib.bib143), [99](#bib.bib99)] 更易生成，并且对所有
    PAs 更具通用性。具体来说，二值监督将提供给每个空间位置的深度嵌入特征。换句话说，通过二值面具标签，我们可以找出 PAs 是否出现在对应的补丁中，这种方法不依赖攻击类型，并且空间上具有解释性。George
    和 Marcel [[32](#bib.bib32)] 是首批引入深度像素级二值监督来预测级联最终二值分类的中间置信度图的人。在充分的像素级监督下，骨干网络
    DenseNet121 表现良好，能够提供补丁级的真实/伪造预测。由于细微的伪造线索（例如摩尔纹）通常存在于不同的空间区域且强度不同，普通的像素级二值监督对所有补丁赋予相同的贡献，这可能导致特征表示偏差。为了解决这个问题，Hossaind
    等人 [[142](#bib.bib142)] 提出了在计算深度像素级二值损失之前添加一个可学习的注意力模块来进行特征细化，这有助于显著信息的传播。尽管灵活且易于使用，目前的二值面具标签通常假设面部区域的所有像素具有相同的真实/伪造分布，因此为真实和
    PAs 生成所有的 ‘1’ 和 ‘0’ 图。然而，当遇到部分攻击（例如 FunnyEye）时，这些标签在学习过程中是不准确和噪声较大的。
- en: Besides the mainstream depth map and binary mask labels, there are several informative
    auxiliary supervisions (e.g., pseudo reflection map [[36](#bib.bib36), [24](#bib.bib24),
    [44](#bib.bib44)], 3D point cloud map [[40](#bib.bib40)], ternary map [[39](#bib.bib39)],
    and Fourier spectra [[144](#bib.bib144)]). According to the discrepancy of facial
    material-related albedo between the live skin and spoof mediums, Kim et al. [[36](#bib.bib36)]
    propose to supervise deep models with both depth and reflection labels. Moreover,
    to further enhance the type-agnostic generalization, binary mask maps are introduced
    in [[24](#bib.bib24)] to train the bilateral convolutional networks with all these
    three pixel-wise supervisions simultaneously. Unlike binary mask labels considering
    all spatial positions including live/spoof-unrelated background, Sun et al. [[39](#bib.bib39)]
    remove the face-unrelated parts and leave the entire face regions as a refined
    binary mask called ‘ternary map’, which eliminates the noise outside the face
    and benefits the facial spoof clue mining. Based on the rich texture and geometry
    discrepancy between the bonafide and PAs, deep models with other auxiliary supervisions
    from the Fourier map [[33](#bib.bib33), [144](#bib.bib144)], LBP texture map [[97](#bib.bib97)],
    and sparse 3D point cloud map [[40](#bib.bib40)], also show their excellent representation
    capability.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 除了主流的深度图和二进制掩膜标签外，还有几种信息丰富的辅助监督（例如，伪反射图[[36](#bib.bib36), [24](#bib.bib24),
    [44](#bib.bib44)]，3D点云图[[40](#bib.bib40)]，三元图[[39](#bib.bib39)]，和傅里叶谱[[144](#bib.bib144)]）。根据真实皮肤和伪造介质之间的面部材料相关反射率的差异，Kim等人[[36](#bib.bib36)]提出用深度和反射标签对深度模型进行监督。此外，为了进一步增强对类型无关的泛化能力，[[24](#bib.bib24)]中引入了二进制掩膜图，以同时训练具有所有这三种像素级监督的双边卷积网络。与考虑所有空间位置（包括与真实/伪造无关背景）的二进制掩膜标签不同，Sun等人[[39](#bib.bib39)]去除了与面部无关的部分，保留整个面部区域作为一种精细化的二进制掩膜，称为“三元图”，它消除了面部外的噪声，有助于面部伪造线索的挖掘。基于真实和伪造图像之间丰富的纹理和几何差异，结合傅里叶图[[33](#bib.bib33),
    [144](#bib.bib144)]，LBP纹理图[[97](#bib.bib97)]和稀疏3D点云图[[40](#bib.bib40)]的其他辅助监督的深度模型也展现了其优秀的表征能力。
- en: 'Overall, pixel-wise auxiliary supervision benefits the physically meaningful
    and explainable live/spoof feature learning (e.g., reflection and depth supervisions
    for material and geometry representation, respectively). Moreover, a reliable
    and generalized FAS model can be supervised with multiple complementary auxiliary
    supervisions (e.g., depth, reflection, and albedo) in a multi-task learning fashion [[24](#bib.bib24)].
    However, two limitations of auxiliary supervision should be mentioned: 1) pixel-wise
    supervision usually relies on the high-quality (e.g., high-resolution) training
    data for fine-grained spoof clue mining, and is harder to provide effective supervision
    signals when training data are too noisy and with low quality; and 2) the pseudo
    auxiliary labels are either human-designed or generated by other off-the-shelf
    algorithms, which are not always trustworthy.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，像素级辅助监督有助于物理上有意义且可解释的真实/伪造特征学习（例如，用于材料和几何体表示的反射和深度监督）。此外，一个可靠且泛化的FAS模型可以通过多任务学习方式使用多种互补的辅助监督（例如，深度、反射和反射率）进行监督[[24](#bib.bib24)]。然而，辅助监督有两个限制需要提及：1）像素级监督通常依赖于高质量（例如，高分辨率）训练数据进行精细的伪造线索挖掘，当训练数据噪声过多且质量较低时，提供有效的监督信号更为困难；2）伪辅助标签要么是人工设计的，要么是由其他现成算法生成的，这些标签并不总是值得信赖的。
- en: Pixel-wise Supervision with Generative Model. Despite the fine-grained supervision
    signal in the auxiliary task, it is still hard to understand whether the deep
    black-box models represent intrinsic FAS features. Recently, one hot trend is
    to mine the visual spoof patterns existing in the spoof samples, aiming to provide
    a more intuitive interpretation of the sample’s spoofness. We summarize such kind
    of generative models with pixel-wise supervision in the lower part of Table-A
    7 (in Appendix). In consideration of the strong physical-inspired constraints
    of auxiliary pixel-wise supervision, several works relax such explicit supervision
    signals and provide a broader space for implicit spoof clues mining. Jourabloo
    et al. [[33](#bib.bib33)] rephrase FAS as a spoof noise modeling problem, and
    design an encoder-decoder architecture to estimate the underlying spoof patterns
    with relaxed pixel-wise supervisions (e.g., zero-noise map for live faces). With
    such unilateral constraint on the bonafide, the models are able to mine the spoof
    clues flexibly for PAs. Similarly, Feng et al. [[41](#bib.bib41)] design a spoof
    cue generator to minimize the spoof cues of live samples while imposes no explicit
    constraints on those of spoof samples. Unlike above-mentioned works forcing strict
    constraints on live samples, Mohammadi et al. [[138](#bib.bib138)] use the reconstruction-error
    maps computed from a live-face-pretrained autoencoder for spoofing detection.
    As such error maps are generated from the residual noises of reconstructed live
    faces without human-defined elements, they are robust under domain shift with
    knowledge clue change. However, the low-quality reconstructed faces from autoencoder
    may lead to noisy residual error maps.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成模型的像素级监督。尽管辅助任务中存在细粒度的监督信号，但仍然很难理解深层黑箱模型是否代表了内在的FAS特征。最近，一个热门趋势是挖掘伪造样本中存在的视觉伪造模式，旨在提供对样本伪造性的更直观解释。我们在表-A
    7（附录）下部分总结了这种类型的生成模型。考虑到辅助像素级监督的强物理启发式约束，一些工作放宽了这种显式监督信号，并提供了更广泛的空间来挖掘隐式伪造线索。Jourabloo
    等人[[33](#bib.bib33)] 将FAS重新定义为伪造噪声建模问题，并设计了一个编码器-解码器架构，以利用放宽的像素级监督（例如，针对真实人脸的零噪声图）估计潜在的伪造模式。通过对真实样本施加这样的单方面约束，这些模型能够灵活地挖掘伪造线索。类似地，Feng
    等人[[41](#bib.bib41)] 设计了一个伪造线索生成器，以最小化真实样本的伪造线索，同时对伪造样本不施加显式约束。与上述强制对真实样本施加严格约束的工作不同，Mohammadi
    等人[[138](#bib.bib138)] 使用从真实人脸预训练自编码器计算的重建误差图进行伪造检测。由于这些误差图是从重建的真实人脸的残差噪声中生成的，没有人工定义的元素，因此在知识线索变化的领域迁移下具有鲁棒性。然而，从自编码器生成的低质量重建人脸可能导致噪声残差误差图。
- en: Besides direct spoof pattern generation, Qin et al. [[43](#bib.bib43)] propose
    to automatically generate pixel-wise labels via a meta-teacher framework, which
    is able to provide better-suited supervision for the student FAS models to learn
    sufficient and intrinsic spoofing cues. However, only the learnable spoof supervision
    is generated in [[43](#bib.bib43)]. Therefore, how to generate the optimal pixel-wise
    signals automatically for both live and spoof samples is still worth exploring.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接生成伪造模式，Qin 等人[[43](#bib.bib43)] 提出了通过元教师框架自动生成像素级标签，该框架能够为学生FAS模型提供更合适的监督，以学习足够且内在的伪造线索。然而，在[[43](#bib.bib43)]中，仅生成了可学习的伪造监督。因此，如何自动生成针对真实样本和伪造样本的最佳像素级信号仍值得探索。
- en: Overall, pixel-wise supervision with generative model usually relaxes the expert-designed
    hard constraints (e.g., auxiliary tasks), and leaves the decoder to reconstruct
    more natural spoof-related trace. Thus, the predicted spoof patterns are strongly
    data-driven and have explainable views. The generated spoof patterns are visually
    insightful, and are challenging to manually describe with human prior knowledge.
    However, such soft pixel-wise supervision might easily fall into the local optimum
    and overfit on unexpected interference (e.g., sensor noise), which would generalize
    poorly under real-world scenarios. Combining explicit auxiliary supervision with
    generative model based supervision for jointly training might alleviate this issue.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，使用生成模型的像素级监督通常会放宽专家设计的严格约束（例如，辅助任务），并让解码器重建更自然的伪造相关痕迹。因此，预测的伪造模式强烈依赖于数据，并且具有可解释的视角。生成的伪造模式在视觉上具有洞察力，并且用人工先验知识很难描述。然而，这种软性像素级监督可能容易陷入局部最优，并对意外干扰（例如，传感器噪声）过拟合，这会在现实世界场景下表现不佳。将显式辅助监督与基于生成模型的监督结合进行联合训练，可能有助于缓解这一问题。
- en: 3.3 Generalized Deep Learning Method
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 泛化深度学习方法
- en: Traditional end-to-end deep learning based FAS methods might generalize poorly
    on unseen dominant conditions (e.g., illumination, facial appearance, and camera
    quality) and unknown attack types (e.g., emerging high fidelity mask made of new
    materials). Thus, these methods are unreliable to be applied in practical applications
    with strong security needs. In light of this, more and more researchers focus
    on enhancing the generalization capacity of the deep FAS models. On one hand,
    domain adaptation and generalization techniques are leveraged for robust live/spoof
    classification under unlimited domain variations. On the other hand, zero/few-shot
    learning as well as anomaly detection frameworks are applied for unknown face
    PA types detection. In this paper, the unseen domains indicate the spoof-irrelated
    external changes (e.g., lighting and sensor noise) but actually influence the
    appearance quality. In contrast, the unknown spoofing attacks usually mean the
    novel attack types with intrinsic physical properties (e.g., material and geometry)
    which have not occurred in the training phase. Representative generalized deep
    FAS methods on unseen domains and unknown attack types are summarized in Tables-A
    8 and 9 (in Appendix), respectively.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的端到端深度学习基础的 FAS 方法在未见的主导条件（例如，照明、面部外观和相机质量）和未知攻击类型（例如，新材料制成的高保真面具）上可能泛化效果不佳。因此，这些方法在具有强安全需求的实际应用中不可靠。鉴于此，越来越多的研究者专注于提升深度
    FAS 模型的泛化能力。一方面，领域适应和泛化技术被用来在无限领域变化下进行鲁棒的实时/欺骗分类。另一方面，零样本/少样本学习以及异常检测框架被应用于未知面部伪造类型的检测。本文中的未见领域指的是与伪造无关的外部变化（例如，照明和传感器噪声），但实际上会影响外观质量。相比之下，未知的伪造攻击通常指的是在训练阶段未出现的具有内在物理特性（例如，材料和几何）的新攻击类型。在附录中的表
    A 8 和表 9 中总结了代表性的对未见领域和未知攻击类型的泛化深度 FAS 方法。
- en: 3.3.1 Generalization to Unseen Domain
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 对未见领域的泛化
- en: 'As shown in Fig. [11](#S3.F11 "Figure 11 ‣ 3.3.1 Generalization to Unseen Domain
    ‣ 3.3 Generalized Deep Learning Method ‣ 3 Deep FAS with Commercial RGB Camera
    ‣ Deep Learning for Face Anti-Spoofing: A Survey"), serious domain shifts exist
    among source domains and target domain, which easily leads to poor performance
    on biased target dataset (e.g., MSU-MFSD) when training deep models directly on
    sources datasets (e.g., OULU-NPU, CASIA-MFSD, and Replay-Attack). Domain adaptation
    technique leverages the knowledge from target domain to bridge the gap between
    source and target domains. In contrast, domain generalization helps the FAS model
    learn generalized feature representation from multiple source domains directly
    without any access to target data, which is more practical for real-world deployment.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [11](#S3.F11 "图 11 ‣ 3.3.1 对未见领域的泛化 ‣ 3.3 泛化深度学习方法 ‣ 3 商业 RGB 摄像头下的深度 FAS
    ‣ 人脸反欺诈的深度学习：综述") 所示，源领域和目标领域之间存在严重的领域迁移，这容易导致直接在源数据集（例如，OULU-NPU、CASIA-MFSD 和
    Replay-Attack）上训练深度模型时在偏倚目标数据集（例如，MSU-MFSD）上的性能较差。领域适应技术利用目标领域的知识来弥合源领域和目标领域之间的差距。相比之下，领域泛化帮助
    FAS 模型从多个源领域直接学习泛化特征表示，而无需访问目标数据，这在实际应用中更为实用。
- en: '![Refer to caption](img/68929ead6686604fb8ab098e8bf583fb.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/68929ead6686604fb8ab098e8bf583fb.png)'
- en: 'Figure 11: Framework comparison among domain adaptation (DA) and domain generalization
    (DG). (a) The DA methods need the (unlabeled) target domain data to learn the
    model while (b) DG methods learn generalized model without knowledge from the
    target domain.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：领域适应（DA）与领域泛化（DG）的框架比较。(a) DA 方法需要（未标记的）目标领域数据来学习模型，而 (b) DG 方法在没有目标领域知识的情况下学习泛化模型。
- en: Domain Adaptation. Domain adaptation technique alleviates the discrepancy between
    source and target domains. The distribution of source and target features are
    usually matched in a learned feature space. If the features share similar distributions,
    a classifier trained on features of the source samples can be used to classify
    the target samples.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应。 领域适应技术减轻了源领域和目标领域之间的差异。源领域和目标领域的特征分布通常在一个学习到的特征空间中匹配。如果特征具有相似的分布，则可以使用在源样本特征上训练的分类器来分类目标样本。
- en: To align the features space between source and target domain data, Li et al. [[78](#bib.bib78)]
    propose the unsupervised domain adaptation to learn a mapping function to align
    the source-target embedded subspaces via minimizing their Maximum Mean Discrepancy
    (MMD) [[145](#bib.bib145)]. To further enhance the generalization between both
    domains, UDA-Net [[146](#bib.bib146), [147](#bib.bib147)] is proposed with unsupervised
    adversarial domain adaptation in order to jointly optimize the source and target
    domain encoders. The domain-aware common feature space is learned when the features
    cannot be distinguished from both domains. The domain-invariant features constrained
    via MMD and adversarial domain adaptation are still with weak discrimination capacity
    because the label information in the target domain is inaccessible. To alleviate
    this problem, semi-supervised learning was introduced to domain adaptation by
    two works [[103](#bib.bib103), [148](#bib.bib148)], where a few labeled data and
    a large amount unlabeled data in the target domain can be utilized. Jia et al. [[103](#bib.bib103)]
    propose a unified unsupervised and semi-supervised domain adaptation network to
    represent the domain-invariant feature space, and find that leveraging few labeled
    target data (three to five) can significantly improve the performance on the target
    domain. Similarly, Quan et al. [[148](#bib.bib148)] propose a semi-supervised
    learning FAS method using only a few labeled training data for pretraining, and
    progressively adopt reliable unlabeled data during training to reduce the domain
    gap. Despite excellent adaptation, such semi-supervised methods heavily rely on
    class-balanced few-shot labeled data (i.e., with both live and spoof samples simultaneously),
    and performance degrade obviously when labeled spoof samples are unavailable.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对齐源领域和目标领域数据的特征空间，Li 等人[[78](#bib.bib78)] 提出了无监督领域自适应方法，通过最小化最大均值差异（MMD）[[145](#bib.bib145)]
    来学习一个映射函数，以对齐源目标嵌入子空间。为了进一步增强两个领域之间的泛化能力，UDA-Net[[146](#bib.bib146), [147](#bib.bib147)]
    提出了无监督对抗领域自适应方法，以联合优化源领域和目标领域编码器。当特征在两个领域之间不可区分时，学习领域感知的公共特征空间。由于目标领域中的标签信息不可获得，通过
    MMD 和对抗领域自适应约束的领域不变特征仍具有较弱的区分能力。为了缓解这一问题，半监督学习被引入领域自适应，两个工作[[103](#bib.bib103),
    [148](#bib.bib148)] 允许在目标领域中利用少量标记数据和大量未标记数据。Jia 等人[[103](#bib.bib103)] 提出了一个统一的无监督和半监督领域自适应网络来表示领域不变特征空间，并发现利用少量标记的目标数据（三到五个）可以显著提高目标领域的性能。类似地，Quan
    等人[[148](#bib.bib148)] 提出了一个半监督学习 FAS 方法，仅使用少量标记的训练数据进行预训练，并在训练过程中逐步采用可靠的未标记数据以减少领域间的差距。尽管适应效果优异，但这种半监督方法严重依赖于类平衡的少样本标记数据（即同时包含真实和伪造样本），当标记的伪造样本不可用时，性能会显著下降。
- en: Different from the above-mentioned methods only adapting the final classifier
    layer, there are a few works designing and adapting the whole FAS networks. As
    different deep layers share different granularities of domain information, Authors
    of [[149](#bib.bib149)] consider multi-layer distribution adaptation on both the
    representation layers and the classifier layer with MMD loss. Despite efficient
    adaptation via multi-level clues, the architecture might be redundant and have
    limited generalization capacity per se. To obtain more generalized architectures,
    Mohammadi et al. [[150](#bib.bib150)] propose to prune the filters with high feature
    divergence that do not generalize well from one dataset to another, thus the performance
    of the network on target domain can be improved. Different from the network pruning
    in specific filters/layers, Li et al. [[151](#bib.bib151)] propose to distill
    the whole FAS model for the application-specific domain from a well-trained teacher
    network, which is regularized with feature MMD and pair-wise similarity embedding
    from both domains. In this way, lightweight yet generalized FAS models could be
    discovered but with weaker discrimination capacities compared with teacher FAS
    networks.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述仅调整最终分类器层的方法不同，有一些工作设计并调整整个伪造攻击检测网络。由于不同的深层次共享不同粒度的领域信息，[[149](#bib.bib149)]
    的作者考虑在表示层和分类器层上进行多层次的分布适应，使用 MMD 损失。尽管通过多层次线索实现了有效的适应，但这种架构可能是冗余的，并且自身的泛化能力有限。为了获得更通用的架构，Mohammadi
    等人 [[150](#bib.bib150)] 提出了修剪在不同数据集间泛化较差的高特征差异的滤波器，从而提高网络在目标域上的性能。与特定滤波器/层的网络修剪不同，Li
    等人 [[151](#bib.bib151)] 提出了从经过良好训练的教师网络中蒸馏整个伪造攻击检测模型，采用了来自两个领域的特征 MMD 和成对相似性嵌入进行正则化。通过这种方式，能够发现轻量但更通用的伪造攻击检测模型，但与教师伪造攻击检测网络相比，区分能力较弱。
- en: Although domain adaptation benefits to minimize the distribution discrepancy
    between the source and the target domain by utilizing unlabeled target data, in
    many real-world FAS scenarios, it is difficult and expensive to collect a lot
    of unlabeled target data (especially the spoofing attacks) for training. In addition,
    in consideration of the privacy issue, the source face data are usually inaccessible
    when deploying FAS models on the target domain.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管领域适应通过利用未标记的目标数据来最小化源域和目标域之间的分布差异，但在许多现实世界的伪造攻击检测场景中，收集大量未标记的目标数据（特别是伪造攻击数据）进行训练是困难且昂贵的。此外，考虑到隐私问题，在目标域上部署伪造攻击检测模型时，源脸数据通常无法访问。
- en: Domain Generalization. Domain generalization assumes that there exists a generalized
    feature space underlying the seen multiple source domains and the unseen but related
    target domain, on which the learned model from the seen source domains can generalize
    well to the unseen domain.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 域泛化。 域泛化假设存在一个通用特征空间，这个空间位于已见的多个源域和未见但相关的目标域之下，在这个空间上，从已见源域学习的模型可以很好地泛化到未见域。
- en: 'On one hand, a few works adopt domain-aware adversarial constraints to learn
    discriminative but domain-unrelated features. They assume that such features contain
    intrinsic clues across all seen domains thus would generalize well on unseen domain.
    Shao et al. [[48](#bib.bib48)] are the first to propose to learn a generalized
    feature space shared by multiple source domains via a multi-adversarial discriminative
    domain generalization framework. Meanwhile, a domain generalization benchmark
    across four FAS datasets is also established in [[48](#bib.bib48)]. However, there
    are two limitations: 1) such domain-independent features might still contain spoof-unrelated
    clues (e.g., subject ID and sensor noise); and 2) the discrimination of the domain
    generalized features is still unsatisfactory. To improve the first limitation,
    Wang et al. [[50](#bib.bib50)] propose to disentangle generalized FAS features
    from subject discriminative and domain-dependent features. As for the second limitation,
    in consideration of the large distribution discrepancies among spoof faces of
    different domains, Jia et al. [[51](#bib.bib51)] propose to learn a discriminative
    and generalized feature space, where the feature distribution of the bonafide
    is compact while that of the PAs is dispersed among domains but is compact within
    each domain.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，一些工作采用领域感知对抗约束来学习具有判别性的但与领域无关的特征。他们假设这些特征包含跨所有见过的领域的内在线索，因此在未见过的领域中具有较好的泛化能力。Shao
    等人[[48](#bib.bib48)] 首次提出通过多对抗判别领域泛化框架来学习由多个源领域共享的广义特征空间。同时，还在[[48](#bib.bib48)]中建立了跨四个
    FAS 数据集的领域泛化基准。然而，有两个局限性：1）这种领域独立特征可能仍然包含与欺诈无关的线索（例如，主题 ID 和传感器噪声）；2）领域广义特征的判别效果仍然不令人满意。为了解决第一个局限性，Wang
    等人[[50](#bib.bib50)] 提出将广义 FAS 特征从主题判别特征和领域依赖特征中解开。至于第二个局限性，考虑到不同领域的欺诈面孔之间的大分布差异，Jia
    等人[[51](#bib.bib51)] 提出学习一个判别和广义的特征空间，其中真实特征的分布是紧凑的，而欺诈特征在领域间是分散的，但在每个领域内是紧凑的。
- en: On the other hand, several representative works utilize domain-aware meta-learning
    to learn the domain generalized feature space. Specifically, faces from partial
    source domains are used as query set while those from remained non-overlap domains
    as support set. Based on this setting, Shao et al. [[49](#bib.bib49)] propose
    to regularize the FAS model by finding generalized learning directions in the
    fine-grained domain-aware meta-learning process. To alternatively force the meta-learner
    to perform well on support sets (domains), the learned models have robust generalization
    capacity. However, such domain-aware meta learning strictly needs the domain labels
    of the source data to construct the query and support sets but domain labels are
    not always available in real-world cases. Without using domain labels, Chen et
    al. [[152](#bib.bib152)] propose to train a generalized FAS model using the domain
    dynamic adjustment meta-learning, which iteratively divides mixture domains into
    clusters with pseudo domain labels. However, the spoof-discriminative and domain-aware
    features are disentangled via a simple channel attention module, making the domain
    features unreliable for pseudo domain label assignment. From the perspective of
    feature normalization, based on the evidence that instance normalization is effective
    to remove domain discrepancy, Liu et al. [[153](#bib.bib153)] propose to adaptively
    aggregate batch and instance normalizations for generalized representation via
    meta-learning. Note that the adaptive tradeoffs between batch and instance normalizations
    might weaken the live/spoof discrimination capacity.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一些代表性工作利用领域感知元学习来学习领域广泛的特征空间。具体而言，来自部分源领域的面孔被用作查询集，而来自剩余非重叠领域的面孔则用作支持集。基于这一设置，Shao
    等人[[49](#bib.bib49)] 提出通过在细粒度领域感知元学习过程中找到广义学习方向来规范 FAS 模型。为了迫使元学习者在支持集（领域）上表现良好，学习的模型具有强大的泛化能力。然而，这种领域感知元学习严格依赖源数据的领域标签来构建查询集和支持集，但在实际情况中领域标签并不总是可用的。Chen
    等人[[152](#bib.bib152)] 提出使用领域动态调整元学习来训练广义 FAS 模型，该方法通过伪领域标签迭代地将混合领域划分为簇。然而，伪领域标签分配中领域特征的不可靠性是由于通过简单的通道注意模块解开了欺诈鉴别和领域感知特征。从特征归一化的角度来看，基于实例归一化有效去除领域差异的证据，Liu
    等人[[153](#bib.bib153)] 提出通过元学习自适应地聚合批量和实例归一化以实现广义表示。请注意，批量和实例归一化之间的自适应权衡可能会削弱实时/欺诈识别能力。
- en: Overall, domain generalization for FAS is a new hot spot in recent three years,
    and some potential and exciting trends such as combining domain generalization
    with adaptation [[154](#bib.bib154)], and learning without domain labels [[152](#bib.bib152)]
    are investigated. However, there still lacks of the works lifting the veil about
    discrimination and generalization capacities. In other words, domain generalization
    benefits FAS models to perform well in unseen domain, but it is still unknown
    whether it deteriorates the discrimination capability for spoofing detection under
    the seen scenarios.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，FAS的领域泛化是近年来的一个新热点，一些潜在和令人兴奋的趋势，如将领域泛化与适应性结合[[154](#bib.bib154)]，以及在没有领域标签的情况下进行学习[[152](#bib.bib152)]，也在进行研究。然而，仍然缺乏揭示辨别和泛化能力的研究。换句话说，领域泛化使FAS模型在未见领域中表现良好，但它是否会降低在已见场景中的欺骗检测能力仍然未知。
- en: 3.3.2 Generalization to Unknown Attack Types
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 对未知攻击类型的泛化
- en: Besides domain shift issues, FAS models are vulnerable to emerging novel PAs
    in real-world practical applications. Most previous deep learning methods formulate
    FAS as a close-set problem to detect various pre-defined PAs, which need large-scale
    training data to cover as many attacks as possible. However, the trained model
    can easily overfit several common attacks (e.g., print and replay) and is still
    vulnerable to unknown attack types (e.g., mask and makeup). Recently, many researches
    focus on developing generalized FAS models for unknown spoofing attack detection.
    On one side, zero/few-shot learning is employed for improving novel spoofing detection
    with very few or even none samples of target attack types. On the other side,
    FAS can also be treated as a one-class classification task where the bonafide
    samples are clustered compactly, and anomaly detection is used for detecting the
    out-of-distribution PA samples.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 除了领域转移问题外，FAS模型在现实应用中也容易受到新型PAs的攻击。大多数先前的深度学习方法将FAS视为一个闭集问题，以检测各种预定义的PAs，这需要大规模的训练数据来覆盖尽可能多的攻击。然而，训练好的模型可能会过拟合一些常见攻击（例如打印和重放），并且仍然容易受到未知攻击类型（例如面具和化妆）的影响。最近，许多研究集中在开发用于未知欺骗攻击检测的泛化FAS模型。一方面，零样本/少样本学习被用于通过极少量甚至没有目标攻击类型的样本来提高新型欺骗检测。另一方面，FAS也可以被视为一种单类分类任务，其中真实样本被紧密聚类，异常检测用于检测分布外的PA样本。
- en: Zero/Few-Shot Learning. One straightforward way for novel attack detection is
    to finetune the FAS model with sufficient samples of the new attacks. However,
    collecting labeled data for every new attack is expensive and time-consuming since
    the spoofing keeps evolving. To overcome this challenge, several works [[38](#bib.bib38),
    [53](#bib.bib53), [155](#bib.bib155)] propose to treat FAS as an open-set zero-
    and few-shot learning problem. Zero-shot learning aims to learn generalized and
    discriminative features from the predefined PAs for unknown novel PA detection.
    Few-shot learning aims to quickly adapt the FAS model to new attacks by learning
    from both the predefined PAs and the collected very few samples of the new attack.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本/少样本学习。 检测新型攻击的一个直接方法是使用足够的新攻击样本来微调FAS模型。然而，由于欺骗手段不断演变，为每种新攻击收集标记数据是昂贵且耗时的。为了克服这一挑战，一些工作[[38](#bib.bib38),
    [53](#bib.bib53), [155](#bib.bib155)]提议将FAS视为一个开放集零样本和少样本学习问题。零样本学习旨在从预定义的PAs中学习出用于未知新型PA检测的泛化和辨别特征。少样本学习旨在通过从预定义的PAs和收集的少量新攻击样本中学习，快速适应FAS模型到新攻击。
- en: Without any prior knowledge of the unknown spoof attacks, Liu et al. [[38](#bib.bib38)]
    design a Deep Tree Network (DTN) to learn the semantic attributes of pre-defined
    attacks and partition the spoof samples into semantic sub-groups in an unsupervised
    fashion. Based on the similarity of the embedding features, DTN adaptively routes
    the known or unknown PAs to the corresponding spoof clusters. The live/spoof tree
    topology is constructed via DTN automatically, which is more semantic and generalized
    compared with the human-defined category relationship. However, without any prior
    knowledge of unknown attacks, the zero-shot DTN may fail to detect novel high-fidelity
    attacks. To alleviate this issue, two works adopt an open-set few-shot framework
    to introduce partial yet effective unknown attack knowledge for representation
    learning. Qin et al. [[53](#bib.bib53)] unify the zero- and few-shot FAS tasks
    together by fusion training a meta-learner with an adaptive inner-update learning
    rate strategy. Training meta-learner on both zero- and few-shot tasks simultaneously
    enhances the discrimination and generalization capacities of FAS models from pre-defined
    PAs and few instances of the new PAs. However, directly using few-shot meta learning
    on novel attacks easily suffers from catastrophic forgetting about the pre-defined
    PAs. To tackle this issue, Perez-Cabo et al. [[155](#bib.bib155)] propose a continual
    few-shot learning paradigm, which incrementally extends the acquired knowledge
    from the continuous stream of data, and detects new PAs using a small number of
    training samples via a meta-learning solution.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有任何未知欺骗攻击的先验知识的情况下，Liu等人[[38](#bib.bib38)]设计了一个深度树网络（DTN），以无监督的方式学习预定义攻击的语义属性，并将欺骗样本划分为语义子组。基于嵌入特征的相似性，DTN自适应地将已知或未知的PA路由到相应的欺骗簇。通过DTN自动构建的实时/欺骗树拓扑比人工定义的类别关系更具语义和泛化能力。然而，没有任何未知攻击的先验知识，零-shot
    DTN可能无法检测到新的高保真攻击。为了解决这个问题，两个工作采用了开放集少量学习框架，引入了部分但有效的未知攻击知识用于表示学习。Qin等人[[53](#bib.bib53)]通过融合训练一个具有自适应内部更新学习率策略的元学习器，将零-shot和少量-shot
    FAS任务统一在一起。与此同时训练元学习器可以增强FAS模型对预定义PA和新PA少量实例的区分能力和泛化能力。然而，直接将少量-shot元学习应用于新攻击容易遭遇对预定义PA的灾难性遗忘。为了解决这个问题，Perez-Cabo等人[[155](#bib.bib155)]提出了一种持续少量学习范式，该范式通过不断扩展从连续数据流中获得的知识，并通过元学习解决方案使用少量训练样本检测新的PA。
- en: Although few-shot learning benefits the FAS models for unknown attack detection,
    the performance drops obviously when the data of the target attack types are unavailable
    for adaptation (i.e., zero-shot case). We observe that the failed detection usually
    occurs in the challenging attack types (e.g., transparent mask, funny eye, and
    makeup), which share similar appearance distribution with the bonafide.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管少量学习对FAS模型在未知攻击检测方面有好处，但当目标攻击类型的数据不可用于适应（即零-shot情况）时，性能明显下降。我们观察到，失败的检测通常发生在具有挑战性的攻击类型（例如，透明面具、搞笑眼睛和化妆），这些类型与真实攻击的外观分布相似。
- en: Anomaly Detection. Anomaly detection based FAS assumes that the live samples
    are in a normal class as they share more similar and compact feature representation
    while features from the spoof samples have large distribution discrepancies in
    the anomalous sample space due to the high variance of attack types and materials.
    Based on the assumption, anomaly detection usually firstly trains a reliable one-class
    classifier to accurately cluster the live samples. Then any samples (e.g., unknown
    attacks) outside the margin of the live sample cluster would be detected as attacks.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测。异常检测基础的FAS假设真实样本属于正常类别，因为它们具有更相似和紧凑的特征表示，而欺骗样本的特征由于攻击类型和材料的高变异性，在异常样本空间中具有较大的分布差异。基于这一假设，异常检测通常首先训练一个可靠的单类分类器来准确地聚类真实样本。然后，任何在真实样本簇的边缘之外的样本（例如，未知攻击）将被检测为攻击。
- en: Arashloo et al. [[52](#bib.bib52)] is the first to evaluate one-class anomaly
    detection and traditional binary classification FAS systems on cross-type testing
    protocols. They find that anomaly-based methods using one-class SVM are not inferior
    compared to binary classification approaches using two-class SVM. To better represent
    the probability distribution of bonafide samples, Nikisins et al. [[156](#bib.bib156)]
    propose to replace traditional one-class SVM with Gaussian Mixture Model (GMM)
    as the anomaly detector. Besides one-class SVM and GMM, Xiong and AbdAlmageed [[157](#bib.bib157)]
    also consider the autoencoder based outliers detector with LBP feature extractor
    for open-set unknown PAD. The above-mentioned works separate the feature extraction
    with the one-class classifier, which makes the bonafide representation learning
    challenging and sub-optimal. In contrast, Baweja et al. [[158](#bib.bib158)] present
    an end-to-end anomaly detection approach to train the one-class classifier and
    feature representations together. Moreover, to learn robust bonafide representation
    against out-of-distribution perturbations, they generate pseudo negative features
    to mimic the PA class and force the one-class classifier to be discriminative
    for PAD. However, the generated pseudo PA features cannot represent diverse real-world
    features, making the one-class anomaly detection system less reliable for real-world
    deployment.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Arashloo 等人 [[52](#bib.bib52)] 是首个在跨类型测试协议上评估一类异常检测与传统二分类 FAS 系统的研究。他们发现，使用一类
    SVM 的异常基方法并不逊色于使用二类 SVM 的二分类方法。为了更好地表示真实样本的概率分布，Nikisins 等人 [[156](#bib.bib156)]
    提议用高斯混合模型（GMM）替代传统的一类 SVM 作为异常检测器。除了一个类 SVM 和 GMM，Xiong 和 AbdAlmageed [[157](#bib.bib157)]
    还考虑了基于自编码器的离群点检测器，并结合 LBP 特征提取器用于开放集未知 PAD。上述工作将特征提取与一类分类器分开，这使得真实样本的表示学习具有挑战性且次优。相比之下，Baweja
    等人 [[158](#bib.bib158)] 提出了一个端到端的异常检测方法，旨在将一类分类器和特征表示一起训练。此外，为了学习对抗分布外扰动的鲁棒真实样本表示，他们生成伪负特征以模拟
    PA 类，并强制一类分类器对 PAD 进行区分。然而，生成的伪 PA 特征无法代表多样的真实世界特征，使得一类异常检测系统在实际部署中不够可靠。
- en: Though reasonable, utilizing only live faces to train the classifier usually
    limits the anomaly detection model’s generalization on new PA types. Instead of
    using only live samples, some works train the generalized anomaly detection systems
    with both live and spoof samples via metric learning. Pérez-Cabo et al. [[159](#bib.bib159)]
    propose to regularize the FAS model by a triplet focal loss to learn discriminative
    feature representation, and then introduce a few-shot posteriori probability estimation
    as anomaly detector for unknown PA detection. Similarly, George and Marcel [[160](#bib.bib160)]
    design a pair-wise one-class contrastive loss (OCCL) to force the network to learn
    a compact embedding for bonafide class while being far from the representation
    of attacks. Then an one-class GMM is cascaded for unknown PA detection. Although
    discriminative embedding could be learned via triplet or contrastive loss, the
    works [[159](#bib.bib159), [160](#bib.bib160)] still need extra anomaly detectors
    (e.g., one-class GMM) cascaded after embedding features, which influences the
    end-to-end representation learning. In contrast, Li et al. [[161](#bib.bib161)]
    propose to supervise deep FAS models with a novel hypersphere loss to keep the
    intra-class live compactness as well as inter-class live-spoof separation. The
    unknown attacks could be directly detected on learned feature space with no need
    of additional anomaly detection classifiers. One limitation is that the predicted
    live/spoof score is calculated from the square of L2-norm of the embedding features,
    which is hard to select a suitable predefined threshold for detecting different
    kinds of attack types.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管合理，仅利用真实面孔训练分类器通常会限制异常检测模型对新PA类型的泛化能力。某些研究通过度量学习训练了具有真实和伪造样本的通用异常检测系统，而不是仅使用真实样本。Pérez-Cabo等人 [[159](#bib.bib159)]
    提出通过三元组焦点损失来正则化FAS模型，以学习区分特征表示，然后引入少量后验概率估计作为未知PA检测的异常检测器。类似地，George和Marcel [[160](#bib.bib160)]
    设计了一种成对一类对比损失（OCCL），迫使网络为真实类学习一个紧凑的嵌入，同时远离攻击的表示。然后，级联一个一类高斯混合模型（GMM）进行未知PA检测。尽管可以通过三元组或对比损失学习到区分性嵌入，但这些研究 [[159](#bib.bib159),
    [160](#bib.bib160)] 仍需要在嵌入特征后级联额外的异常检测器（例如一类GMM），这影响了端到端表示学习。相比之下，Li等人 [[161](#bib.bib161)]
    提出通过一种新颖的超球面损失来监督深度FAS模型，以保持类内真实的紧凑性以及类间真实与伪造的分离。未知攻击可以直接在学习的特征空间中检测，无需额外的异常检测分类器。一个限制是，预测的真实/伪造分数是从嵌入特征的L2范数的平方计算得到的，这很难选择适合的预定义阈值来检测不同类型的攻击。
- en: Despite satisfactory generalization capacity for unknown attack detection, anomaly
    detection based FAS methods would suffer from discrimination degradation compared
    with conventional live/spoof classification in the real-world open-set scenarios
    (i.e., both known and unknown attacks).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在未知攻击检测方面具有令人满意的泛化能力，基于异常检测的FAS方法在现实世界的开放集场景中（即已知和未知攻击）相较于传统的真实/伪造分类方法会遭遇辨别能力的下降。
- en: 4 Deep FAS with Advanced Sensors
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度FAS与先进传感器
- en: 'Commercial RGB camera-based FAS would be an excellent tradeoff solution in
    terms of security and hardware cost in daily face recognition applications. However,
    some high-security scenarios (face payment and vault entrance guard) require very
    low false acceptance errors. Recently, advanced sensors with various modalities
    are developed to facilitate the ultra-secure FAS. Merits and demerits of various
    sensors and hardware modules for FAS in terms of environmental conditions (lighting
    and distance) and attack types (print, replay, and 3D mask) are listed in Table [II](#S4.T2
    "TABLE II ‣ 4 Deep FAS with Advanced Sensors ‣ Deep Learning for Face Anti-Spoofing:
    A Survey").'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '基于商用RGB相机的面部识别系统（FAS）在日常面部识别应用中在安全性和硬件成本方面是一个优秀的折中方案。然而，一些高安全性场景（如面部支付和保险库入口守卫）要求非常低的误接受错误。最近，开发了具有各种模式的先进传感器，以促进超安全的FAS。各种传感器和硬件模块在环境条件（如照明和距离）以及攻击类型（如打印、重播和3D面具）方面的优缺点列在表格 [II](#S4.T2
    "TABLE II ‣ 4 Deep FAS with Advanced Sensors ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")中。'
- en: Compared with monocular visible RGB camera (VIS), stereo cameras (VIS-Stereo) [[162](#bib.bib162)]
    benefits the 3D geometry information reconstruction for 2D spoofing detection.
    When assembling with dynamic flash light on the presentation face, VIS-Flash [[163](#bib.bib163)]
    is able to capture intrinsic reflection-based material clues to detect all three
    attack types.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 与单目可见光RGB相机（VIS）相比，立体相机（VIS-Stereo）[[162](#bib.bib162)] 在2D欺骗检测中有助于3D几何信息重建。当与动态闪光灯结合使用于演示面时，VIS-闪光[[163](#bib.bib163)]
    能够捕捉基于内在反射的材料线索，从而检测所有三种攻击类型。
- en: Besides visible RGB modality, depth and NIR modalities are also widely used
    in practical FAS deployment with acceptable costs. Two kinds of depth sensors
    including Time of Flight (TOF) [[164](#bib.bib164)] and 3D Structured Light (SL) [[165](#bib.bib165)]
    have been embedded in mainstream mobile phone platforms (e.g., Iphone, Sumsung,
    OPPO, and Huawei). They provide the accurate 3D depth distribution of the captured
    face for 2D spoofing detection. Compared with SL, TOF is more robust to environmental
    conditions such as lighting and distance. In contrast, NIR [[166](#bib.bib166)]
    modality is a complementary spectrum (900 to 1800nm) besides VIS, which effectively
    exploits reflection differences between live and spoof faces but is with poor
    imaging quality in long distance. In addition, the VIS-NIR integration hardware
    module is with a high performance-price ratio for many access control systems.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可见光RGB模式，深度和NIR模式在实际FAS部署中也被广泛使用，且成本可接受。两种深度传感器，包括飞行时间（TOF）[[164](#bib.bib164)]
    和3D结构光（SL）[[165](#bib.bib165)] 已嵌入主流手机平台（如iPhone、三星、OPPO和华为）。它们提供了捕获面部的准确3D深度分布，用于2D欺骗检测。与SL相比，TOF对环境条件如光照和距离更具鲁棒性。相对而言，NIR[[166](#bib.bib166)]
    模式是可见光（VIS）之外的补充光谱（900至1800nm），有效利用活体和欺骗面之间的反射差异，但在长距离下成像质量较差。此外，VIS-NIR集成硬件模块在许多门禁控制系统中具有高性价比。
- en: Meanwhile, several niche but effective sensors are introduced in FAS. Shortwave
    infrared (SWIR) [[55](#bib.bib55)] with the wavelengths of 940nm and 1450nm bands
    discriminates live skin material from non-skin pixels in face images via measuring
    water absorption, which is reliable for generic spoofing attacks detection. A
    thermal camera [[167](#bib.bib167)] is an alternative sensor for efficient FAS
    via face temperature estimation. However, it performs poorly when subjects wear
    transparent masks. Expensive Light Field camera [[87](#bib.bib87)] and four-directional
    Polarization sensor [[47](#bib.bib47)] are also explored for FAS according to
    their excellent representation for facial depth and reflection/refraction light,
    respectively.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，FAS中引入了几种小众但有效的传感器。短波红外（SWIR）[[55](#bib.bib55)]，其波长为940nm和1450nm波段，通过测量水分吸收来区分面部图像中的活体皮肤材料与非皮肤像素，这对于检测通用欺骗攻击是可靠的。热成像相机[[167](#bib.bib167)]是另一种有效的FAS传感器，通过面部温度估计实现。然而，当被试佩戴透明面具时，其表现较差。昂贵的光场相机[[87](#bib.bib87)]和四向偏振传感器[[47](#bib.bib47)]也被探索用于FAS，因为它们分别在面部深度和反射/折射光的表现上非常出色。
- en: 'TABLE II: Comparison with sensor/hardware for FAS under 2 environments (lighting
    condition and distance) and 3 attack types (print, replay and 3D mask). ‘TOF’,
    ‘SL’, ‘C’, ’M’, ’E’, ‘P’, ’G’, ’VG’ are short for ‘Time of Flight’, ‘ Structured
    Light’, ‘Cheap’, ‘Medium’, ‘Expensive’, ‘Poor’, ‘Good’, ‘Very Good’, respectively.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：在两种环境（光照条件和距离）和三种攻击类型（打印、重播和3D面具）下对FAS传感器/硬件的比较。‘TOF’，‘SL’，‘C’，‘M’，‘E’，‘P’，‘G’，‘VG’
    分别是‘飞行时间’，‘结构光’，‘便宜’，‘中等’，‘昂贵’，‘差’，‘好’，‘非常好’的缩写。
- en: '| Sensor | Cost | Environment | Attack Type |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 传感器 | 成本 | 环境 | 攻击类型 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Lighting &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 照明 &#124;'
- en: '|'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Distance &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 距离 &#124;'
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Print &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 打印 &#124;'
- en: '|'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Replay &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重播 &#124;'
- en: '|'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mask &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面具 &#124;'
- en: '|'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| VIS | C | M | M | M | M | M |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| VIS | C | M | M | M | M | M |'
- en: '| VIS-Stereo | M | M | M | VG | VG | M |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| VIS-Stereo | M | M | M | VG | VG | M |'
- en: '| VIS-Flash | C | M | M | G | G | M |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| VIS-闪光 | C | M | M | G | G | M |'
- en: '| Depth(TOF) | M | M | G | VG | VG | P |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 深度（TOF） | M | M | G | VG | VG | P |'
- en: '| Depth(SL) | C | P | P | VG | VG | P |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 深度（SL） | C | P | P | VG | VG | P |'
- en: '| NIR | C | G | P | G | VG | M |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| NIR | C | G | P | G | VG | M |'
- en: '| VIS-NIR | M | G | M | G | VG | G |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| VIS-NIR | M | G | M | G | VG | G |'
- en: '| SWIR | E | G | M | VG | VG | G |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| SWIR | E | G | M | VG | VG | G |'
- en: '| Thermal | E | G | M | G | VG | M |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 热成像 | E | G | M | G | VG | M |'
- en: '| Light Field | E | P | M | VG | VG | M |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 光场 | E | P | M | VG | VG | M |'
- en: '| Polarization | E | G | M | VG | VG | G |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 偏振 | E | G | M | VG | VG | G |'
- en: 4.1 Uni-Modal Deep Learning upon Specialized Sensor
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于专用传感器的单模态深度学习
- en: Based on the specialized sensor/hardware for distinct imaging, researchers have
    developed sensor-aware deep learning methods for efficient FAS, which are summarized
    in Table-A 10 (in Appendix). Seo and Chung [[167](#bib.bib167)] propose a lightweight
    Thermal Face-CNN to estimate the facial temperature from the thermal image, and
    detect the spoofing with abnormal temperature (e.g., out of scope from 36 to 37
    degrees). They find that the thermal image is more suitable than the RGB image
    for replay attack detection. However, such thermal-based method is vulnerable
    to the transparent mask attack. In terms of stereo-based FAS, several works [[162](#bib.bib162),
    [168](#bib.bib168), [169](#bib.bib169)] prove that leveraging the estimated disparity
    or depth/normal maps from the stereo inputs (from stereo and dual pixel (DP) sensors)
    via CNN could achieve remarkable performance on 2D print and replay attack detection.
    However, it usually performs poorly on the 3D mask attack with similar geometric
    distribution of live faces. To further capture detailed 3D local patterns, Liu
    et al. [[87](#bib.bib87)] propose to extract the ray difference and microlens
    images from a single-shot light field camera, and then a shallow CNN is used for
    face PAD. Due to the rich 3D information in light field imaging, the method is
    potential to classify fine-grained spoofing types. Towards real-time and mobile-level
    deployment, Tian et al. [[47](#bib.bib47)] propose to use lightweight MobileNetV2
    to extract efficient DOLP features from an on-chip integrated polarization imaging
    sensor. The above-mentioned methods aim at tackling specific PA types (e.g., replay
    and print), which cannot generalize well across all PA types. In contrast, Heusch
    et al. [[55](#bib.bib55)] propose to use a multi-channel CNN for deep material-related
    feature extraction from the selected SWIR-difference inputs, which is able to
    almost perfectly detect all impersonation attacks while ensuring low bonafide
    classification errors.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 基于用于不同成像的专用传感器/硬件，研究人员开发了传感器感知的深度学习方法来实现高效的FAS，这些方法总结在附录中的表-A 10中。Seo和Chung
    [[167](#bib.bib167)] 提出了一个轻量级的热脸部卷积神经网络（Thermal Face-CNN），用于从热图像中估计面部温度，并检测具有异常温度的伪造（例如，超出36到37度的范围）。他们发现，热图像比RGB图像更适合于重放攻击检测。然而，这种基于热成像的方法对透明面具攻击比较脆弱。在立体基FAS方面，一些研究
    [[162](#bib.bib162), [168](#bib.bib168), [169](#bib.bib169)] 证明，通过卷积神经网络（CNN）利用来自立体和双像素（DP）传感器的估计视差或深度/法线图可以在2D打印和重放攻击检测中取得显著性能。然而，它通常在3D面具攻击中表现较差，因为3D面具的几何分布与真实面孔相似。为了进一步捕捉详细的3D局部模式，Liu等人
    [[87](#bib.bib87)] 提出了从单次拍摄的光场相机中提取光线差异和微透镜图像，然后使用浅层CNN进行面部伪造检测。由于光场成像中的丰富3D信息，该方法有可能对细粒度伪造类型进行分类。为了实现实时和移动级别的部署，Tian等人
    [[47](#bib.bib47)] 提出了使用轻量级MobileNetV2从片上集成的偏振成像传感器中提取高效的DOLP特征。上述方法旨在解决特定的PA类型（例如重放和打印），而无法很好地推广到所有PA类型。相比之下，Heusch等人
    [[55](#bib.bib55)] 提出了使用多通道CNN从选择的SWIR差异输入中提取深度材料相关特征，该方法能够几乎完美地检测所有伪造攻击，同时确保低的真实分类错误率。
- en: Apart from using specialized hardware such as infrared dot projectors and dedicated
    cameras, some deep FAS methods are developed based on visible cameras with extra
    environmental flash. In [[163](#bib.bib163)] and  [[170](#bib.bib170)], dynamic
    flash from the smartphone screen is utilized to illuminate a user’s face from
    multiple directions, which enables the recovery of the face surface normals via
    photometric stereo. Such dynamic normal cues are then fed into CNN to predict
    facial depth and light CAPTCHA for PA detection. Similarly, Ebihara et al. [[171](#bib.bib171)]
    design a novel descriptor to represent the specular and diffuse reflections leveraging
    the difference cues with and without flash, which outperforms the end-to-end ResNet
    with concatenated flash inputs. These methods are easy to deploy without extra
    hardware integration, and have been used in mobile verification and payment systems
    such as Alipay and WeChat Pay. However, dynamic flash is sensitive under outdoor
    environments and is not user-friendly due to the long temporal activation time.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用专门的硬件如红外点投影仪和专用摄像头外，一些深度FAS方法基于带有额外环境闪光的可见光摄像头开发。在[[163](#bib.bib163)]和[[170](#bib.bib170)]中，利用智能手机屏幕的动态闪光从多个方向照亮用户的面部，这使得通过光度立体法恢复面部表面法线成为可能。这些动态法线线索随后被输入到CNN中，以预测面部深度和用于PA检测的光CAPTCHA。类似地，Ebihara等人[[171](#bib.bib171)]设计了一种新颖的描述符来表示镜面反射和漫反射，利用闪光有无的差异线索，这优于端到端的ResNet与连接闪光输入。这些方法易于部署，无需额外的硬件集成，并已在移动验证和支付系统如支付宝和微信支付中使用。然而，动态闪光在户外环境中敏感，并且由于长时间的激活，用户体验不佳。
- en: 4.2 Multi-Modal Deep Learning
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 多模态深度学习
- en: Meanwhile, multi-modal learning based methods become hot and active in the FAS
    research community. Representative multi-modal fusion and cross-modal translation
    approaches for FAS are collected in Table-A 11 (in Appendix).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，多模态学习方法在FAS研究社区中变得热门和活跃。表-A 11（见附录）中收集了代表性的多模态融合和跨模态转换方法。
- en: Multi-Modal Fusion. With multi-modal inputs, mainstream FAS methods extract
    complementary multi-modal features using feature-level fusion strategies. As there
    are redundancy across multi-modal features, direct feature concatenation easily
    results in high-dimensional features and overiftting. To alleviate this issue,
    Zhang et al. [[28](#bib.bib28)] propose the SD-Net using a feature re-weighting
    mechanism to select the informative and discard the redundant channel features
    among RGB, depth, and NIR modalities. However, the re-weighting fusion in SD-Net
    is only conducted on the high-level features but neglecting the multi-modal low-level
    clues. To further boost the multi-modal feature interaction at different levels,
    authors from [[172](#bib.bib172)] and [[173](#bib.bib173)] introduce a multi-modal
    multi-layer fusion branch to enhance the contextual clues among modalities. Despite
    advanced fusion strategies, multi-modal fusion is easily dominated by partial
    modalities (e.g., depth) thus performs poorly when these modalities are noisy
    or missing. To tackle this issue, Shen et al. [[174](#bib.bib174)] design a Modal
    Feature Erasing operation to randomly dropout partial-modal features to prevent
    modality-aware overfitting. In addition, George and Marcel [[175](#bib.bib175)]
    present a cross-modal focal loss to modulate the loss contribution of each modality,
    which benefits the model to learn complementary information among modalities.
    Overall, feature-level fusion is flexible and effective for multi-modal clue aggregation.
    However, modality features are usually extracted from separate branches with high
    computational cost.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态融合。 通过多模态输入，主流的FAS方法使用特征级融合策略提取互补的多模态特征。由于多模态特征之间存在冗余，直接特征拼接很容易导致高维特征和过拟合。为缓解这一问题，Zhang等人[[28](#bib.bib28)]提出了SD-Net，使用特征重新加权机制来选择信息量大的特征，并丢弃RGB、深度和NIR模态中的冗余通道特征。然而，SD-Net中的重新加权融合仅在高层特征上进行，而忽视了多模态低层线索。为了进一步提升不同层级的多模态特征交互，[[172](#bib.bib172)]和[[173](#bib.bib173)]的作者引入了多模态多层融合分支，以增强模态间的上下文线索。尽管有先进的融合策略，但多模态融合容易受到部分模态（例如深度）的主导，因此当这些模态噪声或缺失时，表现较差。为解决此问题，Shen等人[[174](#bib.bib174)]设计了一种模态特征擦除操作，以随机丢弃部分模态特征，防止模态感知的过拟合。此外，George和Marcel[[175](#bib.bib175)]提出了一种跨模态焦点损失来调节每个模态的损失贡献，这有利于模型学习模态间的互补信息。总体而言，特征级融合对多模态线索聚合灵活且有效。然而，模态特征通常从独立分支提取，计算成本高。
- en: Besides feature-level fusion, there are a few works that consider input-level
    and decision-level fusions. Input-level fusion assumes that multi-modal inputs
    are already aligned spatially, and can be fused in the channel dimension directly.
    In [[176](#bib.bib176)], the composite image is fused from gray-scale, depth,
    and NIR modalities by stacking the normalized images, and then fed to deep PA
    detectors. Similarly, Liu et al. [[177](#bib.bib177)] composite VIS-NIR inputs
    via different fusion operators (i.e., stack, summation, and difference), and all
    fused face images are forwarded by a multi-modal FAS network for live/spoof prediction.
    These input-level fusion methods are efficient and with a little extra computational
    cost (mostly on fusion operator and the first network layer). However, fusion
    in too early stage easily vanishes multi-modal clues in the subsequent mid- and
    high-level spaces. In contrast, to tradeoff the individual modality bias and make
    reliable binary decision, some works adopt decision-level fusion based on the
    predicted score from each modality branch. On one hand, Yu et al. [[27](#bib.bib27)]
    directly average the predicted binary scores of individual models from RGB, depth,
    and NIR modalities, which outperforms the input- and feature-level fusions on
    CeFA [[91](#bib.bib91)] dataset. On the other hand, Zhang et al. [[178](#bib.bib178)]
    design a decision-level fusion strategy to firstly aggregate scores from several
    models using depth modality, and then cascaded with the score from the IR model
    for final live/spoof classification. Despite reliable prediction, decision-level
    fusion is inefficient as it needs separate well-trained models for particular
    modalities.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征级融合，还有一些研究考虑了输入级和决策级融合。输入级融合假设多模态输入已经在空间上对齐，并可以直接在通道维度进行融合。在 [[176](#bib.bib176)]中，通过堆叠归一化图像来融合灰度、深度和NIR模态的复合图像，然后将其输入到深度PA检测器中。类似地，Liu等人 [[177](#bib.bib177)]通过不同的融合操作符（即堆叠、求和和差异）组合VIS-NIR输入，所有融合的面部图像都通过一个多模态FAS网络进行实时/伪造预测。这些输入级融合方法高效且额外计算成本较少（主要集中在融合操作符和第一个网络层）。然而，过早的融合阶段容易在随后的中高层空间中消失多模态线索。相比之下，为了权衡个别模态偏差并做出可靠的二元决策，一些研究采用了基于每个模态分支预测得分的决策级融合。一方面，Yu等人 [[27](#bib.bib27)]直接对来自RGB、深度和NIR模态的个别模型的预测二元得分进行平均，这在CeFA [[91](#bib.bib91)]数据集上优于输入级和特征级融合。另一方面，Zhang等人 [[178](#bib.bib178)]设计了一种决策级融合策略，首先使用深度模态聚合多个模型的得分，然后与IR模型的得分级联，以进行最终的实时/伪造分类。尽管预测可靠，但决策级融合效率较低，因为它需要针对特定模态的单独训练良好的模型。
- en: Cross-Modal Translation. Multi-modal FAS system needs additional sensors for
    imaging face inputs with different modalities. However, in some conventional scenarios,
    only partial modalities (e.g., RGB) can be available. To tackle this modality
    missing issues at the inference stage, a few works adopt the cross-modal translation
    technique to generate the missing modal data for multi-modal FAS. To generate
    the corresponding NIR images from RGB face images, Jiang et al. [[179](#bib.bib179)]
    first propose a novel multiple categories (live/spoof, genuine/synthetic) image
    translation cycle-GAN. Based on the generated NIR and original RGB inputs, the
    method is able to extract more robust fused features compared with using only
    the RGB images. However, the generated NIR images from raw cycle-GAN are with
    low quality, which limits the performance of the fused features. To generate high-fidelity
    target NIR modality, Liu et al. [[180](#bib.bib180)] design a novel subspace-based
    modality regularization in the cross-modal translation framework. Besides generating
    the NIR images, Mallat and Dugelay [[181](#bib.bib181)] propose a visible-to-thermal
    conversion scheme to synthesize thermal attacks from RGB face images using a cascaded
    refinement network. Though effectiveness on intra-dataset testings, one main concern
    of these methods is that the domain shifts and unknown attacks might significantly
    influence the generated modality’s quality, and the fused features would be unreliable
    using paired noisy modality data.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 跨模态翻译。 多模态FAS系统需要额外的传感器来对不同模态的面部输入进行成像。然而，在一些传统场景中，可能只有部分模态（例如RGB）可用。为了解决推理阶段的模态缺失问题，一些研究采用了跨模态翻译技术来生成缺失的模态数据以供多模态FAS使用。为了从RGB面部图像生成相应的NIR图像，Jiang等人[[179](#bib.bib179)]首次提出了一种新颖的多类别（实时/伪造、真实/合成）图像翻译循环GAN。基于生成的NIR和原始RGB输入，该方法能够提取比仅使用RGB图像更为稳健的融合特征。然而，原始循环GAN生成的NIR图像质量较低，这限制了融合特征的性能。为生成高保真度的目标NIR模态，Liu等人[[180](#bib.bib180)]在跨模态翻译框架中设计了一种新颖的基于子空间的模态正则化。除了生成NIR图像外，Mallat和Dugelay[[181](#bib.bib181)]提出了一种可见光到热成像的转换方案，通过级联精炼网络从RGB面部图像合成热攻击。尽管在数据集内部测试中效果显著，这些方法的一个主要问题是领域转移和未知攻击可能会显著影响生成模态的质量，而使用配对的噪声模态数据时，融合特征可能会变得不可靠。
- en: Despite a rising trend since 2019, the progress of sensor-based multi-modal
    FAS is still slow compared with RGB based unimodal methods. It is worth noting
    that multi-modal approaches also exist in deep FAS with commercial RGB camera.
    For instance, decision-level fusion of two RGB video based modalities (i.e., remote
    physiological signals and face visual image) has been explored in [[14](#bib.bib14)].
    Therefore, to effectively fuse such natural modalities from commercial RGB camera
    with those from advanced sensors will be an interesting and valuable direction.
    Meanwhile, some advanced sensors (e.g., SWIR, light field, and polarization) are
    expensive and non-portable for real-world deployment. More efficient FAS-dedicated
    sensors as well as multi-modal approaches should be explored.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自2019年以来有上升趋势，基于传感器的多模态FAS进展仍然较慢，相比于基于RGB的单模态方法。值得注意的是，深度FAS中也存在多模态方法，尽管使用的是商业RGB相机。例如，已在[[14](#bib.bib14)]中探讨了两种基于RGB视频的模态（即远程生理信号和面部视觉图像）的决策级融合。因此，将来自商业RGB相机的自然模态与来自先进传感器的模态有效融合将是一个有趣且有价值的方向。同时，一些先进传感器（如SWIR、光场和偏振）价格昂贵且不便于实际部署。应探索更高效的专用FAS传感器以及多模态方法。
- en: 5 Discussion and Future Directions
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与未来方向
- en: 'Thanks to the recent advances in deep learning, FAS has achieved rapid improvement
    over the past few years. As can be seen from Fig. [5](#S2.F5 "Figure 5 ‣ 2.1 Face
    Spoofing Attacks ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing: A Survey"),
    recent deep FAS methods refresh the state of the arts and obtain satisfied performance
    (e.g., $\textless$5% ACER, $\textless$15% HTER, $\textless$10% EER, and $\textless$20%
    HTER) on four evaluation protocols, respectively. On one hand, advanced architectures
    (e.g., NAS-FAS [[37](#bib.bib37)] and FAS-SGTD [[96](#bib.bib96)]) and pixel-wise
    supervision (e.g., pseudo depth and reflection maps) benefit the 2D attack detection
    as well as the fine-grained spoof material perception (e.g., silicone and transparent
    3D masks). On the other hand, domain and attack generalization based methods (e.g.,
    SSDG [[51](#bib.bib51)], FGHV [[182](#bib.bib182)], and SSAN [[102](#bib.bib102)])
    mine the intrinsic live/spoof clues across multiple source domains and attack
    types, which can generalize well even on unseen domains and unknown attacks. These
    generalized deep learning based methods usually detect different kinds of attacks
    (2D & 3D) under diverse scenarios more stably (with lower standard deviation errors)
    under leave-one-out cross-testing protocols. Furthermore, some insightful conclusions
    could be drawn from Tables-A 2, 3, and 4 (in Appendix): 1) Advanced architectures
    (e.g., DC-CDN [[98](#bib.bib98)]) with elaborate supervisions (e.g., pseudo depth
    supervision) dominate the testing performance when training on single source domain.
    In contrast, when training on multiple (three) domains, generalized learning strategies
    play more important roles. 2) Transfer learning from large-scale pre-trained models
    (e.g., SSAN [[102](#bib.bib102)] and ViTranZFAS [[38](#bib.bib38)] using ResNet18
    and vision transformer pretrained from ImageNet1K and ImageNet21K, respectively)
    alleviates the overfitting issue caused by limited-scale live/spoof data, thus
    improves the generalization capacity and benefits cross-dataset and cross-type
    testings.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '得益于最近在深度学习方面的进展，FAS在过去几年中取得了快速的改进。如图[5](#S2.F5 "Figure 5 ‣ 2.1 Face Spoofing
    Attacks ‣ 2 background ‣ Deep Learning for Face Anti-Spoofing: A Survey")所示，最近的深度FAS方法刷新了现有的技术水平，并在四个评估协议上分别获得了令人满意的性能（例如，$\textless$5%
    ACER，$\textless$15% HTER，$\textless$10% EER，和$\textless$20% HTER）。一方面，先进的架构（例如，NAS-FAS [[37](#bib.bib37)]
    和 FAS-SGTD [[96](#bib.bib96)]）以及逐像素监督（例如，伪深度图和反射图）有利于2D攻击检测以及细粒度的欺骗材料感知（例如，硅胶和透明3D面具）。另一方面，基于领域和攻击泛化的方法（例如，SSDG [[51](#bib.bib51)]，FGHV [[182](#bib.bib182)]，和
    SSAN [[102](#bib.bib102)]）挖掘了跨多个源领域和攻击类型的内在活体/欺骗线索，这些方法即使在未见过的领域和未知攻击上也能很好地泛化。这些基于深度学习的泛化方法通常能在不同场景下更稳定地检测不同类型的攻击（2D和3D）（具有较低的标准差误差），在留一交叉测试协议下表现更佳。此外，从附录的表A
    2、3和4中可以得出一些有见地的结论：1）在单一源领域训练时，具有精细监督（例如，伪深度监督）的先进架构（例如，DC-CDN [[98](#bib.bib98)]）主导了测试性能。相反，当在多个（三个）领域进行训练时，泛化学习策略扮演了更重要的角色。2）从大规模预训练模型（例如，使用ResNet18和从ImageNet1K和ImageNet21K预训练的视觉变换器的SSAN [[102](#bib.bib102)]
    和 ViTranZFAS [[38](#bib.bib38)]）进行迁移学习，缓解了由于有限规模的活体/欺骗数据导致的过拟合问题，从而提高了泛化能力，并有利于跨数据集和跨类型测试。'
- en: 'However, FAS is still an unsolved problem due to the challenges such as subtle
    spoof pattern representation, complex real-world domain gaps, and rapidly iterative
    novel attacks. We conclude the limitations of the current development as follows:
    1) Limited live/spoof representation capacity with sub-optimal deep architectures,
    supervisions, and learning strategies. Learning discriminative and generalized
    live/spoof features is vital for deep FAS. Until now, it is still hard to find
    the best-suited architectures as well as the supervisions across all different
    evaluation benchmarks. For example, CDCN with pixel-wise supervision achieves
    excellent and poor performance on intra-dataset and multi-source-domain cross-dataset
    testings, respectively, while ResNet with binary cross-entropy loss performs inversely.
    2) Evaluation under saturating and unpractical testing benchmarks and protocols.
    For example, for intra-testing on the OULU-NPU dataset, ACER of 0.4% and 0.8%
    might make slight difference and indicate the performance saturation on such small-scale
    and monotonous test set. And the cross-domain testings are still far from real-world
    scenarios as only limited sorts of attack types are considered. 3) Isolating the
    anti-spoofing task on only the face area and physical attacks. Besides physical
    presentation attacks in the face area, spoofing in more general applications (e.g.,
    commodity and document) and even digital attacks via stronger and stronger face
    swapping and generative models should be considered. These tasks might share partial
    intrinsic knowledge and benefit the representation learning. 4) Insufficient consideration
    about the interpretability and privacy issues. Most existing FAS researches devote
    to developing novel algorithms against state-of-the-art performance but rarely
    think about the interpretability behind. Such black-box methods are hard to make
    reliable decisions in real-world cases. In addition, most existing works train
    and adapt deep FAS models with huge stored source face data, and neglect the privacy
    and biometric sensitivity issue. According to the discussion above, we summarize
    some solutions and potential research directions in the following subsections.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于诸如细微的伪造模式表示、复杂的真实世界领域差距以及快速迭代的新攻击等挑战，FAS 仍然是一个未解决的问题。我们总结了当前发展的局限性如下：1)
    受限于子最优的深度架构、监督和学习策略，实时/伪造表示能力有限。学习具有区分性和通用性的实时/伪造特征对深度 FAS 至关重要。直到现在，仍然很难找到最适合的架构和监督方法来适应所有不同的评估基准。例如，具有像素级监督的
    CDCN 在内部数据集和多源领域交叉数据集测试中表现优异和差异明显，而使用二元交叉熵损失的 ResNet 则表现相反。2) 在饱和和不切实际的测试基准和协议下进行评估。例如，对于
    OULU-NPU 数据集的内部测试，0.4% 和 0.8% 的 ACER 可能只是微小的差异，并表明在这样的小规模单调测试集上的性能饱和。交叉领域测试仍然远未达到真实世界场景，因为仅考虑了有限的攻击类型。3)
    将抗伪造任务仅限于面部区域和物理攻击。除了面部区域的物理呈现攻击外，还应考虑更一般应用（例如，商品和文档）中的伪造，以及通过更强大的面部交换和生成模型进行的数字攻击。这些任务可能共享部分内在知识，并有助于表示学习。4)
    对解释性和隐私问题考虑不足。大多数现有的 FAS 研究致力于开发针对最新技术性能的新算法，但很少考虑其背后的解释性。这种黑箱方法在实际案例中很难做出可靠的决策。此外，大多数现有工作使用大量存储的源面部数据训练和适应深度
    FAS 模型，忽视了隐私和生物识别敏感性问题。根据上述讨论，我们总结了一些解决方案和潜在的研究方向，在以下小节中进行了描述。
- en: 5.1 Architecture, Supervision and Interpretability
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 架构、监督和解释性
- en: 'As can be seen from Sections [3](#S3 "3 Deep FAS with Commercial RGB Camera
    ‣ Deep Learning for Face Anti-Spoofing: A Survey") and [4](#S4 "4 Deep FAS with
    Advanced Sensors ‣ Deep Learning for Face Anti-Spoofing: A Survey"), most of the
    researchers choose the off-the-shelf network architectures as well as handcrafted
    supervision signals for deep FAS, which might be sub-optimal and hard to leverage
    the large-scale training data adequately. Although several recent works have applied
    AutoML in FAS for searching well-suited architecture [[23](#bib.bib23), [37](#bib.bib37)],
    loss function [[54](#bib.bib54)], and auxiliary supervision [[43](#bib.bib43)],
    they focus on uni-modality and single-frame configuration while neglecting the
    temporal or multi-modal situation. Hence, one promising direction is to automatically
    search and find the best-suited temporal architectures especially for multi-modal
    usage. In this way, more reasonable fusion strategies would be discovered among
    modalities instead of coarse handcrafted design. In addition, rich temporal context
    should be considered in dynamic supervision design instead of static binary or
    pixel-wise supervision.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '从第[3](#S3 "3 Deep FAS with Commercial RGB Camera ‣ Deep Learning for Face Anti-Spoofing:
    A Survey")和[4](#S4 "4 Deep FAS with Advanced Sensors ‣ Deep Learning for Face
    Anti-Spoofing: A Survey")节可以看出，大多数研究人员选择现成的网络架构以及手工监督信号来进行深度FAS，这可能是次优的，且难以充分利用大规模训练数据。虽然几项近期工作已将AutoML应用于FAS以搜索适合的架构[[23](#bib.bib23),
    [37](#bib.bib37)]、损失函数[[54](#bib.bib54)]和辅助监督[[43](#bib.bib43)]，但它们专注于单一模态和单帧配置，而忽视了时间序列或多模态情况。因此，一个有前途的方向是自动搜索并找到最适合的时间序列架构，特别是对于多模态应用。这样，会发现模态之间的更合理的融合策略，而不是粗略的手工设计。此外，动态监督设计中应考虑丰富的时间上下文，而不是静态的二值或像素级监督。'
- en: On the other hand, to design efficient network architecture is vital for real-time
    FAS in mobile devices. Over the past years, most research focuses on tackling
    the accuracy and generalization issues in FAS while only a few works consider
    lightweight [[143](#bib.bib143)] or distilled [[151](#bib.bib151)] CNNs for efficient
    deployment. Besides CNN with strong inductive bias, researchers should also rethink
    the usage of some flexible architectures (e.g., vision transformer [[115](#bib.bib115),
    [129](#bib.bib129)]) in terms of efficiency and computational cost.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，设计高效的网络架构对移动设备上的实时FAS至关重要。在过去几年中，大多数研究集中在解决FAS中的准确性和泛化问题，而只有少数研究考虑了轻量级[[143](#bib.bib143)]或蒸馏[[151](#bib.bib151)]CNN以实现高效部署。除了具有强
    inductive bias 的CNN外，研究人员还应重新考虑一些灵活架构（例如，视觉变换器[[115](#bib.bib115), [129](#bib.bib129)]）在效率和计算成本方面的使用。
- en: Recently, great efforts have been achieved on interpretable FAS [[183](#bib.bib183)].
    Some methods try to localize the spoof regions according to the feature activation
    using visual interpretability tools (e.g., Grad-CAM [[184](#bib.bib184)]) or soft-gating
    strategy [[131](#bib.bib131)]. In addition, auxiliary supervised [[13](#bib.bib13),
    [24](#bib.bib24)] and generative [[33](#bib.bib33), [42](#bib.bib42)] FAS models
    devote to estimating the underlying spoof maps. Besides the visual activation
    maps, natural language [[185](#bib.bib185)] has been introduced for explaining
    the FAS predictions with meaningful sentence-level descriptions. All these trials
    help researchers understand and localize the spoof patterns, and convince the
    FAS decision. However, due to the lack of precious pixel-level spoof annotation,
    the estimated spoof maps are still coarse and easily influenced by unfaithful
    clues (e.g., hands). More advanced feature visualization manners and fine-grained
    pixel-wise spoof segmentation should be developed for interpretable FAS.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在可解释的FAS方面取得了重大进展[[183](#bib.bib183)]。一些方法尝试利用视觉解释工具（例如，Grad-CAM [[184](#bib.bib184)]）或软门控策略[[131](#bib.bib131)]根据特征激活定位伪造区域。此外，辅助监督[[13](#bib.bib13),
    [24](#bib.bib24)]和生成[[33](#bib.bib33), [42](#bib.bib42)]的FAS模型致力于估计潜在的伪造图。除了视觉激活图，*自然语言*[[185](#bib.bib185)]也被引入用来用有意义的句子级描述来解释FAS预测。所有这些尝试帮助研究人员理解和定位伪造模式，并增强FAS决策的说服力。然而，由于缺乏珍贵的像素级伪造注释，估计的伪造图仍然粗略，并且容易受到不真实线索（例如，手部）的影响。应该开发更先进的特征可视化方法和精细的像素级伪造分割技术以实现可解释的FAS。
- en: 5.2 Representation Learning
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 表征学习
- en: Learning discriminative and intrinsic feature representation is the key to reliable
    FAS. A handful of previous researches have proven the effectiveness of transfer
    learning [[127](#bib.bib127), [172](#bib.bib172)] and disentangled learning [[97](#bib.bib97),
    [42](#bib.bib42)] for FAS. The former leverages the pre-trained semantic features
    from other large-scale datasets to alleviate the overfitting issue, while the
    latter aims to disentangle the intrinsic spoofing clues from the noisy representation.
    To learn discriminative embedding spaces with compact distributions among live
    faces and distinguishable distances between live/spoof faces, deep metric learning
    is used for training FAS models. However, the uncertainty of the model prediction
    is still high in the extreme/noisy scenario (e.g., presenting with very high-quality
    spoof and low-quality live samples). More advanced metric learning techniques
    (e.g., on hyperbolic manifold space) could be explored in the future for mining
    subtle spoof patterns. Moreover, rephrasing FAS as a fine-grained recognition [[24](#bib.bib24),
    [101](#bib.bib101)] problem to learn type-discriminative representation is worth
    exploring, which is inspired by the fact that humans could detect spoofing via
    recognizing the specific attack types.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 学习判别性和内在特征表示是可靠的面部欺诈检测（FAS）的关键。一些先前的研究已经证明了迁移学习[[127](#bib.bib127)、[172](#bib.bib172)]和解耦学习[[97](#bib.bib97)、[42](#bib.bib42)]在FAS中的有效性。前者利用来自其他大规模数据集的预训练语义特征，以缓解过拟合问题，而后者旨在从噪声表示中解耦内在的欺诈线索。为了学习在活体面孔之间具有紧凑分布和在活体/欺诈面孔之间具有可区分距离的判别性嵌入空间，深度度量学习被用于训练FAS模型。然而，在极端/噪声场景（例如，呈现非常高质量的欺诈样本和低质量的活体样本）中，模型预测的不确定性仍然很高。未来可以探索更先进的度量学习技术（例如，在双曲流形空间上）以挖掘微妙的欺诈模式。此外，将FAS重新表述为细粒度识别[[24](#bib.bib24)、[101](#bib.bib101)]问题以学习类型判别性表示是值得探索的，这受到人类通过识别特定攻击类型来检测欺诈的事实的启发。
- en: Researchers should also get hung up on fully exploiting the live/spoof training
    data with or without labels for representation enhancement. On one side, self-supervised
    on large-scale combined datasets might reduce the risk of overfitting, and actively
    mine the intrinsic knowledge (e.g., high similarity among intra face patches).
    On the other side, in real-world scenarios, daily unlabeled face data are collected
    from various face recognition terminals continuously, which could be utilized
    for semi-supervised learning [[148](#bib.bib148)]. One challenge is how to make
    full use of the unlabeled imbalanced (i.e., live $\gg$ spoof) data, avoiding unexpected
    performance drop. In addition, suitable data augmentation strategies [[98](#bib.bib98)]
    for FAS are rarely investigated. Adversarial learning might be a good choice for
    adaptive data augmentation in more diverse domains.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还应注重充分利用有或没有标签的活体/欺诈训练数据来增强表示。一方面，在大规模组合数据集上进行自监督可能会减少过拟合的风险，并主动挖掘内在知识（例如，面孔补丁之间的高相似性）。另一方面，在现实世界场景中，来自各种面部识别终端的日常无标签面部数据被不断收集，这些数据可以用于半监督学习[[148](#bib.bib148)]。一个挑战是如何充分利用无标签的不平衡（即，活体
    $\gg$ 欺诈）数据，避免意外的性能下降。此外，适合FAS的数据增强策略[[98](#bib.bib98)]仍然很少被研究。对抗性学习可能是更广泛领域中的自适应数据增强的一个好选择。
- en: 5.3 Real-World Open-Set FAS
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 现实世界中的开放集FAS
- en: 'As discussed in Section [2.4](#S2.SS4 "2.4 Evaluation Protocols ‣ 2 background
    ‣ Deep Learning for Face Anti-Spoofing: A Survey"), traditional FAS evaluation
    protocols usually consider intra-domain [[77](#bib.bib77)], cross-domain [[48](#bib.bib48)],
    and cross-type [[38](#bib.bib38)] testings within one or several small-scale datasets.
    The state-of-the-art methods in such protocols cannot guarantee consistently good
    performance in practical scenarios because 1) the data amount (especially testing
    set) is relatively small thus the high performance is not very convincing; and
    2) the protocols focus on a single factor (e.g., seen/unseen domains or known/unknown
    attack types), which cannot satisfy the need of complex real-world scenarios.
    Recently, more practical protocols such as GrandTest [[155](#bib.bib155)] and
    open-set [[59](#bib.bib59), [42](#bib.bib42)] are proposed. GrandTest contains
    large-scale mixed-domain data, while open-set testing considers models’ discrimination
    and generalization capacities on both known and unknown attack types. However,
    real-world open-set situations with simultaneous domains and attack types are
    still neglected. More comprehensive protocols (e.g., domain- and type-aware open-set)
    should be explored for fair and practical evaluation to bridge the gap between
    academia and industry.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2.4节](#S2.SS4 "2.4 评估协议 ‣ 2 背景 ‣ 人脸反欺诈深度学习综述")中讨论的那样，传统的FAS评估协议通常考虑在一个或几个小规模数据集内进行的领域内[[77](#bib.bib77)]、跨领域[[48](#bib.bib48)]和跨类型[[38](#bib.bib38)]测试。这些协议中的最先进方法在实际场景中无法保证始终如一的良好表现，因为1)
    数据量（尤其是测试集）相对较小，因此高性能并不十分令人信服；2) 协议只关注单一因素（例如，已见/未见领域或已知/未知攻击类型），这无法满足复杂现实场景的需求。最近，提出了更实际的协议，例如GrandTest[[155](#bib.bib155)]和开放集[[59](#bib.bib59),
    [42](#bib.bib42)]。GrandTest包含大规模混合领域数据，而开放集测试则考虑模型对已知和未知攻击类型的辨别和泛化能力。然而，现实世界中同时涉及领域和攻击类型的开放集情况仍然被忽视。应该探索更全面的协议（例如，领域和类型感知的开放集），以进行公平和实际的评估，弥合学术界和工业界之间的差距。
- en: 'As for the multi-modal protocols, training data with multiple modalities are
    assumed available, and two testing settings are widely used: 1) with corresponding
    multiple modalities [[186](#bib.bib186)]; and 2) only single modality [[175](#bib.bib175),
    [180](#bib.bib180)] (usually RGB). However, there are various kinds of modality
    combinations [[187](#bib.bib187)] (e.g., RGB-NIR, RGB-D, NIR-D, and RGB-D-NIR)
    in real-world deployment according to different user terminal devices. Therefore,
    it is pretty costly and inefficient to train individual models for each multi-modal
    combination. Although pseudo modalities could be generated via cross-modality
    translation [[179](#bib.bib179), [180](#bib.bib180)], their fidelity and stability
    are still weaker compared with modalities from real-world sensors. To design a
    dynamic multi-modal framework to propagate the learned multi-modal knowledge to
    various modality combinations might be a possible direction for unlimited multi-modal
    deployment.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多模态协议，假设有多种模态的训练数据可用，并且广泛使用两种测试设置：1) 对应的多模态[[186](#bib.bib186)]；2) 仅单一模态[[175](#bib.bib175),
    [180](#bib.bib180)]（通常是RGB）。然而，根据不同用户终端设备，现实世界中存在各种模态组合[[187](#bib.bib187)]（例如，RGB-NIR、RGB-D、NIR-D和RGB-D-NIR）。因此，为每种多模态组合训练单独的模型是相当昂贵和低效的。虽然可以通过跨模态翻译[[179](#bib.bib179),
    [180](#bib.bib180)]生成伪模态，但其真实性和稳定性仍然不如来自真实传感器的模态。设计一个动态的多模态框架，以将学到的多模态知识传播到各种模态组合中，可能是实现无限多模态部署的一个可能方向。
- en: 5.4 Generic and Unified PA Detection
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 通用和统一的PA检测
- en: Understanding the intrinsic property of face PAD with other related tasks (e.g.,
    generic PAD, and digital face attack detection) is important for explainable FAS.
    On one hand, ‘generic’ assumes that both face and other object presentation attacks
    might have independent content but share intrinsic spoofing patterns [[188](#bib.bib188)].
    For instance, replay attacks about different objects (e.g., a face and a football)
    are made of the same glass material [[24](#bib.bib24)], and with abnormal reflection
    clues. Thus, generic PAD and material recognition datasets could be introduced
    in face PAD for common live/spoof feature representation in a multi-task learning
    fashion.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 理解面部PAD的内在特性以及其他相关任务（例如通用PAD和数字面部攻击检测）对于可解释的FAS非常重要。一方面，“通用”假设面部和其他物体的展示攻击可能具有独立的内容，但共享内在的伪造模式[[188](#bib.bib188)]。例如，不同物体（例如，面部和足球）的重播攻击是由相同的玻璃材料[[24](#bib.bib24)]制成的，并带有异常反射线索。因此，可以在面部PAD中引入通用PAD和材料识别数据集，以多任务学习的方式进行通用的实时/伪造特征表示。
- en: '![Refer to caption](img/ed06c90c34a7c7237d03cdfced833ef6.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ed06c90c34a7c7237d03cdfced833ef6.png)'
- en: 'Figure 12: Illustration of the physical adversarial faces generated by Adv-glasses [[189](#bib.bib189)],
    Adv-hat [[190](#bib.bib190)], Adv-makeup [[191](#bib.bib191)], and Adv-sticker [[192](#bib.bib192)].'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：由Adv-glasses[[189](#bib.bib189)]、Adv-hat[[190](#bib.bib190)]、Adv-makeup[[191](#bib.bib191)]和Adv-sticker[[192](#bib.bib192)]生成的物理对抗面孔示意图。
- en: 'Apart from common PAs, two kinds of physical adversarial attacks (AFR-aware
    and FAS-aware) should be considered for generic PAD. As illustrated in Fig. [12](#S5.F12
    "Figure 12 ‣ 5.4 Generic and Unified PA Detection ‣ 5 Discussion and Future Directions
    ‣ Deep Learning for Face Anti-Spoofing: A Survey"), physical eyeglass [[189](#bib.bib189)]
    and hat [[190](#bib.bib190)] achieved from adversarial generators, or special
    stickers [[192](#bib.bib192)] containing feature patterns proved to be effective
    against deep learning based AFR systems can be printed out and wore by attackers
    to spoof such systems. Moreover, imperceptible makeup [[191](#bib.bib191)] nearby
    the eye regions have been verified for attacking commercial AFR systems. Besides
    AFR-aware adversarial attacks, adversarial print/replay attacks [[193](#bib.bib193)]
    with perturbation before physical broadcast are developed to fool the FAS system.
    Therefore, it is expected and necessary to establish large-scale FAS datasets
    with diverse physical adversarial attacks as well as annotated attack localization
    labels.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 除了常见的物理伪造攻击外，对于通用的PAD，还应考虑两种物理对抗攻击（AFR-aware和FAS-aware）。如图[12](#S5.F12 "图 12
    ‣ 5.4 通用和统一的PA检测 ‣ 5 讨论与未来方向 ‣ 基于深度学习的人脸防伪：综述")所示，从对抗生成器获得的物理眼镜[[189](#bib.bib189)]和帽子[[190](#bib.bib190)]，或含有特征模式的特殊贴纸[[192](#bib.bib192)]，被证明对基于深度学习的AFR系统有效，这些可以被攻击者打印出来并佩戴以伪造这些系统。此外，附近眼部的不可察觉的化妆[[191](#bib.bib191)]已被验证对商业AFR系统进行攻击。除了AFR-aware对抗攻击外，带有扰动的对抗打印/重播攻击[[193](#bib.bib193)]也被开发出来，以愚弄FAS系统。因此，有必要和预期建立具有多样化物理对抗攻击的大规模FAS数据集，以及带有标注攻击定位标签的数据集。
- en: On the other hand, besides physical face presentation attacks, there are many
    vicious digital manipulation attacks (e.g., Deepfake [[194](#bib.bib194)]) and
    morphing attacks (e.g., via generative model StyleGAN[[195](#bib.bib195)]) on
    face videos. As generative models become stronger and stronger, these direct digital
    attacks from generative models become bigger threats. Despite different generation
    manners with diverse attack traces and visual qualities, parts of these attacks
    might still have coherent properties. In [[196](#bib.bib196), yu2022benchmarking],
    a unified digital and physical face attack detection framework is proposed to
    learn joint representations for coherent attacks. However, there are serious imbalanced
    numbers among digital and physical attack types due to data collection costs.
    In other words, large-scale digital attacks are easier to generate compared with
    high-cost presentation attacks. Such imbalanced distribution might harm the intrinsic
    representation during the multi-task learning, which needs to think about in the
    future.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，除了物理面部展示攻击，还有许多恶意数字操控攻击（例如，Deepfake[[194](#bib.bib194)]）和变形攻击（例如，通过生成模型StyleGAN[[195](#bib.bib195)]）对面部视频进行攻击。随着生成模型越来越强大，这些来自生成模型的直接数字攻击变得越来越具有威胁性。尽管生成方式和攻击痕迹及视觉质量各异，但这些攻击的一部分仍可能具有一致的特性。在[[196](#bib.bib196),
    yu2022benchmarking]中，提出了一个统一的数字和物理面部攻击检测框架，用于学习一致攻击的联合表示。然而，由于数据收集成本，数字攻击和物理攻击类型之间存在严重的不平衡。换句话说，大规模数字攻击比高成本的展示攻击更容易生成。这种不平衡的分布可能会在多任务学习过程中损害内在表示，需要在未来考虑。
- en: 5.5 Privacy-Preserved Training
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 隐私保护训练
- en: Leveraging large-scale live/spoof face data, deep learning based FAS has achieved
    huge breakthroughs. However, the legal and privacy issues of the face data attract
    more and more attention. For example, the GDPR (General Data Protection Regulation) [[197](#bib.bib197)],
    came into effect in May 2018, brings the importance of preserving the privacy
    of personal information (e.g., face images) to the forefront. Therefore, a noteworthy
    direction is to alleviate the privacy issue (i.e., storing/sharing large-scale
    users’ face data) but maintaining satisfied performance for deep FAS models.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 利用大规模真实/伪造面部数据，基于深度学习的FAS取得了巨大的突破。然而，面部数据的法律和隐私问题越来越受到关注。例如，GDPR（通用数据保护条例）[[197](#bib.bib197)]于2018年5月生效，将保护个人信息（例如，面部图像）隐私的重要性推到前台。因此，一个值得注意的方向是减轻隐私问题（即，存储/共享大规模用户面部数据），同时保持深度FAS模型的良好性能。
- en: On one hand, the live/spoof face training data are usually not directly shared
    between data owners (domains). To tackle this challenge, federate learning [[198](#bib.bib198)],
    a distributed and privacy-preserving machine learning technique, is introduced
    in FAS to simultaneously take advantage of rich live/spoof information available
    at different data owners while maintaining data privacy. To be specific, each
    data center/owner locally trains its own FAS model. Then a server learns a global
    FAS model by iteratively aggregating model updates from all data centers without
    accessing original private data in each of them. Finally, the converged global
    FAS model would be utilized for inference. To enhance the generalization ability
    of the server model, in [[199](#bib.bib199)], a federated domain disentanglement
    strategy is introduced, which treats each data center as one domain and decomposes
    the FAS model into domain-invariant and domain-specific parts in each data center.
    Overall, the existing federated learning based FAS usually focuses on the privacy
    problem of data sets but neglects the privacy issues in the model level. Thus,
    the training of the global model needs multiple teams to share their own local
    models, which might harm the commercial competition.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，真实/伪造面部训练数据通常不会在数据所有者（领域）之间直接共享。为了解决这个问题，联邦学习[[198](#bib.bib198)]，一种分布式和隐私保护的机器学习技术，被引入到FAS中，以同时利用不同数据所有者处丰富的真实/伪造信息，同时维护数据隐私。具体来说，每个数据中心/所有者本地训练自己的FAS模型。然后，服务器通过迭代聚合所有数据中心的模型更新来学习一个全局FAS模型，而无需访问每个数据中心中的原始私人数据。最后，收敛的全局FAS模型将用于推断。为了增强服务器模型的泛化能力，在[[199](#bib.bib199)]中，提出了一种联邦领域解耦策略，该策略将每个数据中心视为一个领域，并将FAS模型分解为每个数据中心中的领域不变部分和领域特定部分。总体而言，现有的基于联邦学习的FAS通常关注数据集的隐私问题，但忽视了模型层面的隐私问题。因此，全球模型的训练需要多个团队共享各自的本地模型，这可能会损害商业竞争。
- en: On the other hand, due to privacy and security concerns of human faces, source
    data are usually inaccessible during adaptation for practical deployment. Specifically,
    in a source-free [[200](#bib.bib200)] setting, a FAS model is first pre-trained
    on the (large-scale) source data and is released for deployment. In the deployment
    phase, the source data cannot be shared for adapting the pre-trained model to
    the target data, as they contain sensitive biometric information. Lv et al. [[201](#bib.bib201)]
    benchmark the source-free setting for FAS via directly applying a self-training
    approach, which easily obtains noisy target pseudo labels due to the challenges
    in the FAS task (e.g., the intra-class distance between live faces of different
    identities probably exceeds the inter-class distance between live and spoof faces
    of the same identity). Thus, the performance gain (1.9% HTER reduction on average)
    by adaptation is quite limited. To efficiently and accurately adapt the source
    knowledge without accessing source data is worth exploring in the future.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于人脸隐私和安全问题，在实际部署时源数据通常不可访问。具体而言，在一个无源[[200](#bib.bib200)]设置中，FAS模型首先在（大规模）源数据上进行预训练，并释放用于部署。在部署阶段，源数据不能用于将预训练模型适配到目标数据，因为它们包含敏感的生物特征信息。Lv
    等人[[201](#bib.bib201)]通过直接应用自我训练方法对FAS的无源设置进行了基准测试，由于FAS任务中的挑战（例如，不同身份的真实面部之间的类内距离可能超过相同身份的真实面部与伪造面部之间的类间距离），容易获得噪声目标伪标签。因此，通过适配获得的性能提升（平均减少1.9%的HTER）非常有限。未来值得探索如何在不访问源数据的情况下高效准确地适配源知识。
- en: 6 Conclusion
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper has presented a contemporary survey of the deep learning based methods,
    datasets as well as protocols for face anti-spoofing (FAS). A comprehensive taxonomy
    of these methods have been presented. Merits and demerits of various methods and
    sensors for FAS are also covered, with potential research directions being listed.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对基于深度学习的面部反欺诈（FAS）方法、数据集以及协议进行了现代化的综述。提供了这些方法的全面分类。还涵盖了各种FAS方法和传感器的优缺点，并列出了潜在的研究方向。
- en: Acknowledgments  This work was supported by the Academy of Finland (Academy
    Professor project EmotionAI with grant numbers 336116 and 345122, and ICT2023
    project with grant number 345948), the National Natural Science Foundation of
    China (No. 61876178, 61976229, and 62106264), and Beijing Academy of Artificial
    Intelligence (BAAI).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢 本工作得到了芬兰学术院（Academy Professor项目EmotionAI资助编号336116和345122，以及ICT2023项目资助编号345948）、中国国家自然科学基金（编号61876178、61976229和62106264）和北京人工智能学会（BAAI）的支持。
- en: References
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Guo, X. Zhu, C. Zhao, D. Cao, Z. Lei, and S. Z. Li, “Learning meta face
    recognition in unseen domains,” in *CVPR*, 2020.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Guo, X. Zhu, C. Zhao, D. Cao, Z. Lei 和 S. Z. Li，“在未见领域中学习元面部识别，”在*CVPR*，2020年。'
- en: '[2] G. Pan, L. Sun, Z. Wu, and S. Lao, “Eyeblink-based anti-spoofing in face
    recognition from a generic webcamera,” in *ICCV*, 2007.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] G. Pan, L. Sun, Z. Wu 和 S. Lao，“基于眼睑的面部识别反欺诈方法，”在*ICCV*，2007年。'
- en: '[3] X. Li, J. Komulainen, G. Zhao, P.-C. Yuen, and M. Pietikäinen, “Generalized
    face anti-spoofing by detecting pulse from face videos,” in *ICPR*, 2016.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. Li, J. Komulainen, G. Zhao, P.-C. Yuen 和 M. Pietikäinen，“通过检测面部视频中的脉搏来实现通用面部反欺诈，”在*ICPR*，2016年。'
- en: '[4] T. de Freitas Pereira, A. Anjos, J. M. De Martino, and S. Marcel, “Lbp-
    top based countermeasure against face spoofing attacks,” in *ACCV*, 2012.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. de Freitas Pereira, A. Anjos, J. M. De Martino 和 S. Marcel，“基于Lbp-top的面部伪造攻击对策，”在*ACCV*，2012年。'
- en: '[5] J. Komulainen, A. Hadid, and M. Pietikainen, “Context based face anti-spoofing,”
    in *BTAS*, 2013.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Komulainen, A. Hadid 和 M. Pietikainen，“基于上下文的面部反欺诈，”在*BTAS*，2013年。'
- en: '[6] K. Patel, H. Han, and A. K. Jain, “Secure face unlock: Spoof detection
    on smartphones,” *TIFS*, 2016.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] K. Patel, H. Han 和 A. K. Jain，“安全面部解锁：智能手机上的欺诈检测，”*TIFS*，2016年。'
- en: '[7] H.-K. Jee, S.-U. Jung, and J.-H. Yoo, “Liveness detection for embedded
    face recognition system,” *International Journal of Biological and Medical Sciences*,
    2006.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] H.-K. Jee, S.-U. Jung 和 J.-H. Yoo，“嵌入式面部识别系统的活体检测，”*国际生物与医学科学期刊*，2006年。'
- en: '[8] J.-W. Li, “Eye blink detection based on multiple gabor response waves,”
    in *ICMLC*, vol. 5.   IEEE, 2008, pp. 2852–2856.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J.-W. Li，“基于多重Gabor响应波的眼睑检测，”在*ICMLC*，第5卷。IEEE，2008年，第2852–2856页。'
- en: '[9] L. Wang, X. Ding, and C. Fang, “Face live detection method based on physiological
    motion analysis,” *Tsinghua Science & Technology*, vol. 14, no. 6, pp. 685–690,
    2009.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. Wang, X. Ding 和 C. Fang，“基于生理运动分析的面部活体检测方法，”*清华大学科技*，第14卷，第6期，第685–690页，2009年。'
- en: '[10] W. Bao, H. Li, N. Li, and W. Jiang, “A liveness detection method for face
    recognition based on optical flow field,” in *ICASSP*, 2009.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] W. Bao, H. Li, N. Li, 和 W. Jiang, “一种基于光流场的人脸识别活体检测方法，” 发表在 *ICASSP*，2009年。'
- en: '[11] J. Bigun, H. Fronthaler, and K. Kollreider, “Assuring liveness in biometric
    identity authentication by real-time face tracking,” in *CIHSPS*.   IEEE, 2004.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Bigun, H. Fronthaler, 和 K. Kollreider, “通过实时人脸跟踪确保生物识别身份验证中的活体检测，”
    发表在 *CIHSPS*。IEEE，2004年。'
- en: '[12] A. Ali, F. Deravi, and S. Hoque, “Liveness detection using gaze collinearity,”
    in *ICEST*.   IEEE, 2012.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Ali, F. Deravi, 和 S. Hoque, “利用视线共线性进行活体检测，” 发表在 *ICEST*。IEEE，2012年。'
- en: '[13] Y. Liu, A. Jourabloo, and X. Liu, “Learning deep models for face anti-spoofing:
    Binary or auxiliary supervision,” in *CVPR*, 2018.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Liu, A. Jourabloo, 和 X. Liu, “学习深度模型进行人脸反欺诈：二元还是辅助监督，” 发表在 *CVPR*，2018年。'
- en: '[14] B. Lin, X. Li, Z. Yu, and G. Zhao, “Face liveness detection by rppg features
    and contextual patch-based cnn,” in *ICBEA*.   ACM, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] B. Lin, X. Li, Z. Yu, 和 G. Zhao, “通过RPPG特征和基于上下文的补丁卷积神经网络进行人脸活体检测，” 发表在
    *ICBEA*。ACM，2019年。'
- en: '[15] Z. Yu, W. Peng, X. Li, X. Hong, and G. Zhao, “Remote heart rate measurement
    from highly compressed facial videos: an end-to-end deep learning solution with
    video enhancement,” in *ICCV*, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Z. Yu, W. Peng, X. Li, X. Hong, 和 G. Zhao, “从高度压缩的面部视频中远程测量心率：一种端到端深度学习解决方案结合视频增强，”
    发表在 *ICCV*，2019年。'
- en: '[16] Z. Boulkenafet, J. Komulainen, and A. Hadid, “Face anti-spoofing based
    on color texture analysis,” in *ICIP*, 2015.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Z. Boulkenafet, J. Komulainen, 和 A. Hadid, “基于颜色纹理分析的人脸反欺诈，” 发表在 *ICIP*，2015年。'
- en: '[17] Boulkenafet, Zinelabidine and Komulainen, Jukka and Hadid, Abdenour, “Face
    antispoofing using speeded-up robust features and fisher vector encoding,” *SPL*,
    vol. 24, no. 2, pp. 141–145, 2016.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Boulkenafet, Zinelabidine 和 Komulainen, Jukka 和 Hadid, Abdenour, “使用加速鲁棒特征和Fisher向量编码的人脸反欺诈，”
    *SPL*，第24卷，第2期，页码141–145，2016年。'
- en: '[18] X. Tan, Y. Li, J. Liu, and L. Jiang, “Face liveness detection from a single
    image with sparse low rank bilinear discriminative model,” in *ECCV*.   Springer,
    2010.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] X. Tan, Y. Li, J. Liu, 和 L. Jiang, “基于稀疏低秩双线性判别模型的单图像人脸活体检测，” 发表在 *ECCV*。Springer，2010年。'
- en: '[19] X. Song, X. Zhao, L. Fang, and T. Lin, “Discriminative representation
    combinations for accurate face spoofing detection,” *Pattern Recognition*, 2019.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] X. Song, X. Zhao, L. Fang, 和 T. Lin, “用于准确人脸欺诈检测的判别表示组合，” *Pattern Recognition*，2019年。'
- en: '[20] M. Asim, Z. Ming, and M. Y. Javed, “Cnn based spatio-temporal feature
    extraction for face anti-spoofing,” in *ICIVC*.   IEEE, 2017.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] M. Asim, Z. Ming, 和 M. Y. Javed, “基于卷积神经网络的时空特征提取用于人脸反欺诈，” 发表在 *ICIVC*。IEEE，2017年。'
- en: '[21] Y. A. U. Rehman, L.-M. Po, and J. Komulainen, “Enhancing deep discriminative
    feature maps via perturbation for face presentation attack detection,” *Image
    and Vision Computing*, vol. 94, p. 103858, 2020.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. A. U. Rehman, L.-M. Po, 和 J. Komulainen, “通过扰动增强深度判别特征图用于人脸呈现攻击检测，”
    *Image and Vision Computing*，第94卷，页码103858，2020年。'
- en: '[22] M. Khammari, “Robust face anti-spoofing using cnn with lbp and wld,” *IET
    Image Processing*, 2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Khammari, “使用带有LBP和WLD的卷积神经网络进行鲁棒的人脸反欺诈，” *IET Image Processing*，2019年。'
- en: '[23] Z. Yu, C. Zhao, Z. Wang, Y. Qin, Z. Su, X. Li, F. Zhou, and G. Zhao, “Searching
    central difference convolutional networks for face anti-spoofing,” in *CVPR*,
    2020.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Z. Yu, C. Zhao, Z. Wang, Y. Qin, Z. Su, X. Li, F. Zhou, 和 G. Zhao, “搜索中央差分卷积网络用于人脸反欺诈，”
    发表在 *CVPR*，2020年。'
- en: '[24] Z. Yu, X. Li, X. Niu, J. Shi, and G. Zhao, “Face anti-spoofing with human
    material perception,” in *ECCV*, 2020.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Z. Yu, X. Li, X. Niu, J. Shi, 和 G. Zhao, “基于人类材料感知的人脸反欺诈，” 发表在 *ECCV*，2020年。'
- en: '[25] X. Yang, W. Luo, L. Bao, Y. Gao, D. Gong, S. Zheng, Z. Li, and W. Liu,
    “Face anti-spoofing: Model matters, so does data,” in *CVPR*, 2019.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] X. Yang, W. Luo, L. Bao, Y. Gao, D. Gong, S. Zheng, Z. Li, 和 W. Liu, “人脸反欺诈：模型重要，数据同样重要，”
    发表在 *CVPR*，2019年。'
- en: '[26] Y. Atoum, Y. Liu, A. Jourabloo, and X. Liu, “Face anti-spoofing using
    patch and depth-based cnns,” in *IJCB*, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Atoum, Y. Liu, A. Jourabloo, 和 X. Liu, “基于补丁和深度的卷积神经网络进行人脸反欺诈，” 发表在
    *IJCB*，2017年。'
- en: '[27] Z. Yu, Y. Qin, X. Li, Z. Wang, C. Zhao, Z. Lei, and G. Zhao, “Multi-modal
    face anti-spoofing based on central difference networks,” in *CVPRW*, 2020.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Yu, Y. Qin, X. Li, Z. Wang, C. Zhao, Z. Lei, 和 G. Zhao, “基于中央差分网络的多模态人脸反欺诈，”
    发表在 *CVPRW*，2020年。'
- en: '[28] S. Zhang, A. Liu, J. Wan, Y. Liang, G. Guo, S. Escalera, H. J. Escalante,
    and S. Z. Li, “Casia-surf: A large-scale multi-modal benchmark for face anti-spoofing,”
    *TBIOM*, vol. 2, no. 2, pp. 182–193, 2020.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Zhang, A. Liu, J. Wan, Y. Liang, G. Guo, S. Escalera, H. J. Escalante,
    和 S. Z. Li, “Casia-surf: 大规模多模态基准用于人脸反欺诈，” *TBIOM*，第2卷，第2期，页码182–193，2020年。'
- en: '[29] J. Yang, Z. Lei, and S. Z. Li, “Learn convolutional neural network for
    face anti-spoofing,” *arXiv preprint arXiv:1408.5601*, 2014.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Yang, Z. Lei, 和 S. Z. Li, “学习用于人脸反欺骗的卷积神经网络，” *arXiv 预印本 arXiv:1408.5601*，2014。'
- en: '[30] L. Li, X. Feng, Z. Boulkenafet, Z. Xia, M. Li, and A. Hadid, “An original
    face anti-spoofing approach using partial convolutional neural network,” in *IPTA*,
    2016.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] L. Li, X. Feng, Z. Boulkenafet, Z. Xia, M. Li, 和 A. Hadid, “一种使用部分卷积神经网络的人脸反欺骗原始方法，”
    在 *IPTA*，2016。'
- en: '[31] K. Patel, H. Han, and A. K. Jain, “Cross-database face antispoofing with
    robust feature representation,” in *CCBR*, 2016.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] K. Patel, H. Han, 和 A. K. Jain, “具有鲁棒特征表示的跨数据库人脸反欺骗，” 在 *CCBR*，2016。'
- en: '[32] A. George and S. Marcel, “Deep pixel-wise binary supervision for face
    presentation attack detection,” in *ICB*, no. CONF, 2019.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. George 和 S. Marcel, “用于人脸呈现攻击检测的深度像素级二元监督，” 在 *ICB*，无. CONF，2019。'
- en: '[33] A. Jourabloo, Y. Liu, and X. Liu, “Face de-spoofing: Anti-spoofing via
    noise modeling,” in *ECCV*, 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Jourabloo, Y. Liu, 和 X. Liu, “人脸去欺骗：通过噪声建模的反欺骗，” 在 *ECCV*，2018。'
- en: '[34] S. Jia, X. Li, C. Hu, G. Guo, and Z. Xu, “3d face anti-spoofing with factorized
    bilinear coding,” *arXiv preprint arXiv:2005.06514*, 2020.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Jia, X. Li, C. Hu, G. Guo, 和 Z. Xu, “3D人脸反欺骗与分解双线性编码，” *arXiv 预印本 arXiv:2005.06514*，2020。'
- en: '[35] L. Li, Z. Xia, X. Jiang, F. Roli, and X. Feng, “Compactnet: learning a
    compact space for face presentation attack detection,” *Neurocomputing*, 2020.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] L. Li, Z. Xia, X. Jiang, F. Roli, 和 X. Feng, “Compactnet: 为人脸呈现攻击检测学习紧凑空间，”
    *Neurocomputing*，2020。'
- en: '[36] T. Kim, Y. Kim, I. Kim, and D. Kim, “Basn: Enriching feature representation
    using bipartite auxiliary supervisions for face anti-spoofing,” in *ICCVW*, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Kim, Y. Kim, I. Kim, 和 D. Kim, “Basn: 使用双分辅助监督丰富特征表示以进行人脸反欺骗，” 在 *ICCVW*，2019。'
- en: '[37] Z. Yu, J. Wan, Y. Qin, X. Li, S. Z. Li, and G. Zhao, “Nas-fas: Static-dynamic
    central difference network search for face anti-spoofing,” *IEEE TPAMI*, pp. 1–1,
    2020.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Z. Yu, J. Wan, Y. Qin, X. Li, S. Z. Li, 和 G. Zhao, “Nas-fas: 静态-动态中心差分网络搜索用于人脸反欺骗，”
    *IEEE TPAMI*，第 1-1 页，2020。'
- en: '[38] Y. Liu, J. Stehouwer, A. Jourabloo, and X. Liu, “Deep tree learning for
    zero-shot face anti-spoofing,” in *CVPR*, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Liu, J. Stehouwer, A. Jourabloo, 和 X. Liu, “用于零样本人脸反欺骗的深度树学习，” 在 *CVPR*，2019。'
- en: '[39] W. Sun, Y. Song, C. Chen, J. Huang, and A. C. Kot, “Face spoofing detection
    based on local ternary label supervision in fully convolutional networks,” *TIFS*,
    2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] W. Sun, Y. Song, C. Chen, J. Huang, 和 A. C. Kot, “基于局部三元标签监督的全卷积网络人脸欺骗检测，”
    *TIFS*，2020。'
- en: '[40] X. Li, J. Wan, Y. Jin, A. Liu, G. Guo, and S. Z. Li, “3dpc-net: 3d point
    cloud network for face anti-spoofing,” 2020.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] X. Li, J. Wan, Y. Jin, A. Liu, G. Guo, 和 S. Z. Li, “3dpc-net: 用于人脸反欺骗的3D点云网络，”
    2020。'
- en: '[41] H. Feng, Z. Hong, H. Yue, Y. Chen, K. Wang, J. Han, J. Liu, and E. Ding,
    “Learning generalized spoof cues for face anti-spoofing,” *arXiv preprint arXiv:2005.03922*,
    2020.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Feng, Z. Hong, H. Yue, Y. Chen, K. Wang, J. Han, J. Liu, 和 E. Ding,
    “学习通用的欺骗线索以进行人脸反欺骗，” *arXiv 预印本 arXiv:2005.03922*，2020。'
- en: '[42] Y. Liu and X. Liu, “Physics-guided spoof trace disentanglement for generic
    face anti-spoofing,” *arXiv preprint arXiv:2012.05185*, 2020.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Liu 和 X. Liu, “基于物理引导的欺骗痕迹解缠绕用于通用人脸反欺骗，” *arXiv 预印本 arXiv:2012.05185*，2020。'
- en: '[43] Y. Qin, Z. Yu, L. Yan, Z. Wang, C. Zhao, and Z. Lei, “Meta-teacher for
    face anti-spoofing,” *TPAMI*, 2021.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Qin, Z. Yu, L. Yan, Z. Wang, C. Zhao, 和 Z. Lei, “用于人脸反欺骗的元教师，” *TPAMI*，2021。'
- en: '[44] Y. Zhang, Z. Yin, Y. Li, G. Yin, J. Yan, J. Shao, and Z. Liu, “Celeba-spoof:
    Large-scale face anti-spoofing dataset with rich annotations,” in *ECCV*.   Springer,
    2020.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Zhang, Z. Yin, Y. Li, G. Yin, J. Yan, J. Shao, 和 Z. Liu, “Celeba-spoof:
    大规模人脸反欺骗数据集及其丰富注释，” 在 *ECCV*。Springer，2020。'
- en: '[45] A. George, Z. Mostaani, D. Geissenbuhler, O. Nikisins, A. Anjos, and S. Marcel,
    “Biometric face presentation attack detection with multi-channel convolutional
    neural network,” *TIFS*, 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. George, Z. Mostaani, D. Geissenbuhler, O. Nikisins, A. Anjos, 和 S.
    Marcel, “基于多通道卷积神经网络的人脸呈现攻击检测，” *TIFS*，2019。'
- en: '[46] H. Steiner, A. Kolb, and N. Jung, “Reliable face anti-spoofing using multispectral
    swir imaging,” in *ICB*.   IEEE, 2016.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Steiner, A. Kolb, 和 N. Jung, “使用多光谱SWIR成像的可靠人脸反欺骗，” 在 *ICB*。IEEE，2016。'
- en: '[47] Y. Tian, K. Zhang, L. Wang, and Z. Sun, “Face anti-spoofing by learning
    polarization cues in a real-world scenario,” *arXiv preprint arXiv:2003.08024*,
    2020.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. Tian, K. Zhang, L. Wang, 和 Z. Sun, “通过学习实际场景中的极化线索进行人脸反欺骗，” *arXiv
    预印本 arXiv:2003.08024*，2020。'
- en: '[48] R. Shao, X. Lan, J. Li, and P. C. Yuen, “Multi-adversarial discriminative
    deep domain generalization for face presentation attack detection,” in *CVPR*,
    2019.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] R. Shao, X. Lan, J. Li, 和 P. C. Yuen, “用于人脸呈现攻击检测的多对抗辨别深度领域泛化，” 在 *CVPR*，2019。'
- en: '[49] R. Shao, X. Lan, and P. C. Yuen, “Regularized fine-grained meta face anti-spoofing,”
    in *AAAI*, 2020.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] R. Shao, X. Lan, 和 P. C. Yuen，“正则化细粒度元面部反欺骗”，收录于*AAAI*，2020。'
- en: '[50] G. Wang, H. Han, S. Shan, and X. Chen, “Cross-domain face presentation
    attack detection via multi-domain disentangled representation learning,” in *CVPR*,
    2020.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] G. Wang, H. Han, S. Shan, 和 X. Chen，“通过多域解耦表示学习进行跨域面部呈现攻击检测”，收录于*CVPR*，2020。'
- en: '[51] Y. Jia, J. Zhang, S. Shan, and X. Chen, “Single-side domain generalization
    for face anti-spoofing,” in *CVPR*, 2020.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Jia, J. Zhang, S. Shan, 和 X. Chen，“单侧领域泛化的面部反欺骗”，收录于*CVPR*，2020。'
- en: '[52] S. R. Arashloo, J. Kittler, and W. Christmas, “An anomaly detection approach
    to face spoofing detection: A new formulation and evaluation protocol,” *IEEE
    access*, 2017.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. R. Arashloo, J. Kittler, 和 W. Christmas，“面部欺骗检测的异常检测方法：一种新的表述和评估协议”，*IEEE访问*，2017。'
- en: '[53] Y. Qin, C. Zhao, X. Zhu, Z. Wang, Z. Yu, T. Fu, F. Zhou, J. Shi, and Z. Lei,
    “Learning meta model for zero-and few-shot face anti-spoofing,” in *AAAI*, 2020.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Qin, C. Zhao, X. Zhu, Z. Wang, Z. Yu, T. Fu, F. Zhou, J. Shi, 和 Z. Lei，“学习元模型以应对零样本和少样本面部反欺骗”，收录于*AAAI*，2020。'
- en: '[54] Y. Qin, W. Zhang, J. Shi, Z. Wang, and L. Yan, “One-class adaptation face
    anti-spoofing with loss function search,” *Neurocomputing*, vol. 417, pp. 384–395,
    2020.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Y. Qin, W. Zhang, J. Shi, Z. Wang, 和 L. Yan，“单类自适应面部反欺骗与损失函数搜索”，*神经计算*，第417卷，第384–395页，2020。'
- en: '[55] G. Heusch, A. George, D. Geissbühler, Z. Mostaani, and S. Marcel, “Deep
    models and shortwave infrared information to detect face presentation attacks,”
    *TBIOM*, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] G. Heusch, A. George, D. Geissbühler, Z. Mostaani, 和 S. Marcel，“深度模型和短波红外信息用于检测面部呈现攻击”，*TBIOM*，2020。'
- en: '[56] L. A. Pereira, A. Pinto, F. A. Andaló, A. M. Ferreira, B. Lavi, A. Soriano-Vargas,
    M. V. Cirne, and A. Rocha, “The rise of data-driven models in presentation attack
    detection,” in *Deep Biometrics*.   Springer, 2020, pp. 289–311.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] L. A. Pereira, A. Pinto, F. A. Andaló, A. M. Ferreira, B. Lavi, A. Soriano-Vargas,
    M. V. Cirne, 和 A. Rocha，“数据驱动模型在呈现攻击检测中的兴起”，收录于*深度生物识别*。Springer，2020，第289–311页。'
- en: '[57] S. Jia, G. Guo, and Z. Xu, “A survey on 3d mask presentation attack detection
    and countermeasures,” *Pattern Recognition*, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Jia, G. Guo, 和 Z. Xu，“关于3D面具呈现攻击检测和对策的调查”，*模式识别*，2020。'
- en: '[58] Y. S. El-Din, M. N. Moustafa, and H. Mahdi, “Deep convolutional neural
    networks for face and iris presentation attack detection: survey and case study,”
    *IET Biometrics*, 2020.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. S. El-Din, M. N. Moustafa, 和 H. Mahdi，“用于面部和虹膜呈现攻击检测的深度卷积神经网络：调查和案例研究”，*IET生物识别*，2020。'
- en: '[59] A. Liu, C. Zhao, Z. Yu, J. Wan, A. Su, X. Liu, Z. Tan, S. Escalera, J. Xing,
    Y. Liang *et al.*, “Contrastive context-aware learning for 3d high-fidelity mask
    face presentation attack detection,” *arXiv preprint arXiv:2104.06148*, 2021.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. Liu, C. Zhao, Z. Yu, J. Wan, A. Su, X. Liu, Z. Tan, S. Escalera, J. Xing,
    Y. Liang *等*，“用于3D高保真面具面部呈现攻击检测的对比上下文感知学习”，*arXiv预印本 arXiv:2104.06148*，2021。'
- en: '[60] R. Tolosana, R. Vera-Rodriguez, J. Fierrez, A. Morales, and J. Ortega-Garcia,
    “Deepfakes and beyond: A survey of face manipulation and fake detection,” *Information
    Fusion*, 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] R. Tolosana, R. Vera-Rodriguez, J. Fierrez, A. Morales, 和 J. Ortega-Garcia，“深度伪造及其未来：面部操控和虚假检测的调查”，*信息融合*，2020。'
- en: '[61] G. Goswami, A. Agarwal, N. Ratha, R. Singh, and M. Vatsa, “Detecting and
    mitigating adversarial perturbations for robust face recognition,” *International
    Journal of Computer Vision*, 2019.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] G. Goswami, A. Agarwal, N. Ratha, R. Singh, 和 M. Vatsa，“检测和减轻对抗扰动以增强面部识别的鲁棒性”，*国际计算机视觉杂志*，2019。'
- en: '[62] A. Liu, X. Li, J. Wan, Y. Liang, S. Escalera, H. J. Escalante, M. Madadi,
    Y. Jin, Z. Wu, X. Yu *et al.*, “Cross-ethnicity face anti-spoofing recognition
    challenge: A review,” *IET Biometrics*, 2021.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. Liu, X. Li, J. Wan, Y. Liang, S. Escalera, H. J. Escalante, M. Madadi,
    Y. Jin, Z. Wu, X. Yu *等*，“跨种族面部反欺骗识别挑战：综述”，*IET生物识别*，2021。'
- en: '[63] J. Hernandez-Ortega, J. Fierrez, A. Morales, and J. Galbally, “Introduction
    to face presentation attack detection,” in *Handbook of Biometric Anti-Spoofing*.   Springer,
    2019, pp. 187–206.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J. Hernandez-Ortega, J. Fierrez, A. Morales, 和 J. Galbally，“面部呈现攻击检测简介”，收录于*生物特征反欺骗手册*。Springer，2019，第187–206页。'
- en: '[64] T. de Freitas Pereira, A. Anjos, J. M. De Martino, and S. Marcel, “Can
    face anti-spoofing countermeasures work in a real world scenario?” in *ICB*.   IEEE,
    2013, pp. 1–8.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] T. de Freitas Pereira, A. Anjos, J. M. De Martino, 和 S. Marcel，“面部反欺骗对策能在现实世界场景中有效吗？”收录于*ICB*。IEEE，2013，第1–8页。'
- en: '[65] L. Li, P. L. Correia, and A. Hadid, “Face recognition under spoofing attacks:
    countermeasures and research directions,” *IET Biometrics*, vol. 7, no. 1, pp.
    3–14, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] L. Li, P. L. Correia, 和 A. Hadid，“面部识别中的欺骗攻击：对策和研究方向”，*IET生物识别*，第7卷，第1期，第3–14页，2018。'
- en: '[66] S. Marcel, M. S. Nixon, J. Fierrez, and N. Evans, *Handbook of biometric
    anti-spoofing: Presentation attack detection*.   Springer, 2019.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Marcel, M. S. Nixon, J. Fierrez 和 N. Evans，*生物识别防伪手册：呈现攻击检测*。Springer，2019年。'
- en: '[67] R. Ramachandra and C. Busch, “Presentation attack detection methods for
    face recognition systems: A comprehensive survey,” *ACM Computing Surveys (CSUR)*,
    vol. 50, no. 1, pp. 1–37, 2017.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] R. Ramachandra 和 C. Busch，“面部识别系统的呈现攻击检测方法：全面综述”，发表于 *ACM Computing Surveys
    (CSUR)*，第50卷，第1期，第1–37页，2017年。'
- en: '[68] B. Peixoto, C. Michelassi, and A. Rocha, “Face liveness detection under
    bad illumination conditions,” in *ICIP*.   IEEE, 2011.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] B. Peixoto, C. Michelassi 和 A. Rocha，“在不良照明条件下的面部活体检测”，发表于 *ICIP*。IEEE，2011年。'
- en: '[69] Z. Zhang, J. Yan, S. Liu, Z. Lei, D. Yi, and S. Z. Li, “A face antispoofing
    database with diverse attacks,” in *ICB*, 2012.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Z. Zhang, J. Yan, S. Liu, Z. Lei, D. Yi 和 S. Z. Li，“一个具有多样攻击的面部防伪数据库”，发表于
    *ICB*，2012年。'
- en: '[70] I. Chingovska, A. Anjos, and S. Marcel, “On the effectiveness of local
    binary patterns in face anti-spoofing,” in *Biometrics Special Interest Group*,
    2012.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] I. Chingovska, A. Anjos 和 S. Marcel，“局部二值模式在面部防伪中的有效性”，发表于 *Biometrics
    Special Interest Group*，2012年。'
- en: '[71] N. Kose and J.-L. Dugelay, “Shape and texture based countermeasure to
    protect face recognition systems against mask attacks,” in *CVPRW*, 2013.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] N. Kose 和 J.-L. Dugelay，“基于形状和纹理的对策，保护面部识别系统免受面具攻击”，发表于 *CVPRW*，2013年。'
- en: '[72] D. Wen, H. Han, and A. K. Jain, “Face spoof detection with image distortion
    analysis,” *TIFS*, 2015.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] D. Wen, H. Han 和 A. K. Jain，“通过图像失真分析进行面部伪造检测”，发表于 *TIFS*，2015年。'
- en: '[73] A. Pinto, W. R. Schwartz, H. Pedrini, and A. de Rezende Rocha, “Using
    visual rhythms for detecting video-based facial spoof attacks,” *TIFS*, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] A. Pinto, W. R. Schwartz, H. Pedrini 和 A. de Rezende Rocha，“利用视觉节奏检测基于视频的面部伪造攻击”，发表于
    *TIFS*，2015年。'
- en: '[74] A. Costa-Pazo, S. Bhattacharjee, E. Vazquez-Fernandez, and S. Marcel,
    “The replay-mobile face presentation-attack database,” in *BIOSIG*.   IEEE, 2016.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] A. Costa-Pazo, S. Bhattacharjee, E. Vazquez-Fernandez 和 S. Marcel，“回放-移动面部呈现攻击数据库”，发表于
    *BIOSIG*。IEEE，2016年。'
- en: '[75] S. Liu, P. C. Yuen, S. Zhang, and G. Zhao, “3d mask face anti-spoofing
    with remote photoplethysmography,” in *ECCV*.   Springer, 2016.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. Liu, P. C. Yuen, S. Zhang 和 G. Zhao，“基于远程光脉搏波的3D面具防伪”，发表于 *ECCV*。Springer，2016年。'
- en: '[76] I. Manjani, S. Tariyal, M. Vatsa, R. Singh, and A. Majumdar, “Detecting
    silicone mask-based presentation attack via deep dictionary learning,” *TIFS*,
    2017.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] I. Manjani, S. Tariyal, M. Vatsa, R. Singh 和 A. Majumdar，“通过深度字典学习检测硅胶面具攻击”，发表于
    *TIFS*，2017年。'
- en: '[77] Z. Boulkenafet, J. Komulainen, L. Li, X. Feng, and A. Hadid, “Oulu-npu:
    A mobile face presentation attack database with real-world variations,” in *FGR*,
    2017.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Z. Boulkenafet, J. Komulainen, L. Li, X. Feng 和 A. Hadid，“Oulu-npu：一个具有现实世界变化的移动面部呈现攻击数据库”，发表于
    *FGR*，2017年。'
- en: '[78] H. Li, W. Li, H. Cao, S. Wang, F. Huang, and A. C. Kot, “Unsupervised
    domain adaptation for face anti-spoofing,” *TIFS*, 2018.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] H. Li, W. Li, H. Cao, S. Wang, F. Huang 和 A. C. Kot，“面部防伪的无监督领域适应”，发表于
    *TIFS*，2018年。'
- en: '[79] R. H. Vareto, A. M. Saldanha, and W. R. Schwartz, “The swax benchmark:
    Attacking biometric systems with wax figures,” in *ICASSP*, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] R. H. Vareto, A. M. Saldanha 和 W. R. Schwartz，“SWAX基准：用蜡像攻击生物识别系统”，发表于
    *ICASSP*，2020年。'
- en: '[80] W. R. Almeida, F. A. Andaló, R. Padilha, G. Bertocco, W. Dias, R. d. S.
    Torres, J. Wainer, and A. Rocha, “Detecting face presentation attacks in mobile
    devices with a patch-based cnn and a sensor-aware loss function,” *PloS one*,
    2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] W. R. Almeida, F. A. Andaló, R. Padilha, G. Bertocco, W. Dias, R. d. S.
    Torres, J. Wainer 和 A. Rocha，“通过基于补丁的CNN和传感器感知损失函数检测移动设备中的面部呈现攻击”，发表于 *PloS one*，2020年。'
- en: '[81] N. Erdogmus and S. Marcel, “Spoofing face recognition with 3d masks,”
    *TIFS*, 2014.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] N. Erdogmus 和 S. Marcel，“用3D面具伪造面部识别”，发表于 *TIFS*，2014年。'
- en: '[82] R. Raghavendra, K. B. Raja, and C. Busch, “Presentation attack detection
    for face recognition using light field camera,” *TIP*, vol. 24, no. 3, pp. 1060–1075,
    2015.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] R. Raghavendra, K. B. Raja 和 C. Busch，“使用光场相机进行面部识别的呈现攻击检测”，发表于 *TIP*，第24卷，第3期，第1060–1075页，2015年。'
- en: '[83] J. Galbally and R. Satta, “Three-dimensional and two-and-a-half-dimensional
    face recognition spoofing using three-dimensional printed models,” *IET Biometrics*,
    2016.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. Galbally 和 R. Satta，“使用三维打印模型进行三维和两维半面部识别伪造”，发表于 *IET Biometrics*，2016年。'
- en: '[84] I. Chingovska, N. Erdogmus, A. Anjos, and S. Marcel, “Face recognition
    systems under spoofing attacks,” in *Face Recognition Across the Imaging Spectrum*.   Springer,
    2016, pp. 165–194.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] I. Chingovska, N. Erdogmus, A. Anjos 和 S. Marcel，“伪造攻击下的面部识别系统”，发表于 *面部识别跨成像谱*。Springer，2016年，第165–194页。'
- en: '[85] A. Agarwal, D. Yadav, N. Kohli, R. Singh, M. Vatsa, and A. Noore, “Face
    presentation attack with latex masks in multispectral videos,” in *CVPRW*, 2017.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] A. Agarwal, D. Yadav, N. Kohli, R. Singh, M. Vatsa 和 A. Noore，“多光谱视频中的乳胶面具人脸呈现攻击，”发表于
    *CVPRW*，2017。'
- en: '[86] S. Bhattacharjee and S. Marcel, “What you can’t see can help you-extended-range
    imaging for 3d-mask presentation attack detection,” in *BIOSIG*.   IEEE, 2017.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] S. Bhattacharjee 和 S. Marcel，“看不见的东西可以帮助你——扩展范围成像用于 3D 面具呈现攻击检测，”发表于 *BIOSIG*。IEEE,
    2017。'
- en: '[87] M. Liu, H. Fu, Y. Wei, Y. A. U. Rehman, L.-m. Po, and W. L. Lo, “Light
    field-based face liveness detection with convolutional neural networks,” *Journal
    of Electronic Imaging*, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Liu, H. Fu, Y. Wei, Y. A. U. Rehman, L.-m. Po 和 W. L. Lo，“基于光场的面部活性检测与卷积神经网络，”发表于
    *Journal of Electronic Imaging*，2019。'
- en: '[88] S. Bhattacharjee, A. Mohammadi, and S. Marcel, “Spoofing deep face recognition
    with custom silicone masks,” in *BTAS*, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] S. Bhattacharjee, A. Mohammadi 和 S. Marcel，“用定制硅胶面具欺骗深度人脸识别，”发表于 *BTAS*，2018。'
- en: '[89] J. Xiao, Y. Tang, J. Guo, Y. Yang, X. Zhu, Z. Lei, and S. Z. Li, “3dma:
    A multi-modality 3d mask face anti-spoofing database,” in *AVSS*.   IEEE, 2019.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Xiao, Y. Tang, J. Guo, Y. Yang, X. Zhu, Z. Lei 和 S. Z. Li，“3DMA：一个多模态
    3D 面具人脸反欺诈数据库，”发表于 *AVSS*。IEEE, 2019。'
- en: '[90] S. Zhang, X. Wang, A. Liu, C. Zhao, J. Wan, S. Escalera, H. Shi, Z. Wang,
    and S. Z. Li, “A dataset and benchmark for large-scale multi-modal face anti-spoofing,”
    in *CVPR*, 2019.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] S. Zhang, X. Wang, A. Liu, C. Zhao, J. Wan, S. Escalera, H. Shi, Z. Wang
    和 S. Z. Li，“大规模多模态人脸反欺诈数据集和基准，”发表于 *CVPR*，2019。'
- en: '[91] A. Li, Z. Tan, X. Li, J. Wan, S. Escalera, G. Guo, and S. Z. Li, “Casia-surf
    cefa: A benchmark for multi-modal cross-ethnicity face anti-spoofing,” *WACV*,
    2021.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] A. Li, Z. Tan, X. Li, J. Wan, S. Escalera, G. Guo 和 S. Z. Li，“Casia-surf
    cefa：一个多模态跨种族人脸反欺诈基准，”发表于 *WACV*，2021。'
- en: '[92] M. Rostami, L. Spinoulas, M. Hussein, J. Mathai, and W. Abd-Almageed,
    “Detection and continual learning of novel face presentation attacks,” in *ICCV*,
    2021.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] M. Rostami, L. Spinoulas, M. Hussein, J. Mathai 和 W. Abd-Almageed，“新型人脸呈现攻击的检测与持续学习，”发表于
    *ICCV*，2021。'
- en: '[93] J. Galbally, F. Alonso-Fernandez, J. Fierrez, and J. Ortega-Garcia, “A
    high performance fingerprint liveness detection method based on quality related
    features,” *Future Generation Computer Systems*, vol. 28, no. 1, pp. 311–321,
    2012.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Galbally, F. Alonso-Fernandez, J. Fierrez 和 J. Ortega-Garcia，“基于质量相关特征的高性能指纹活性检测方法，”发表于
    *Future Generation Computer Systems*，第 28 卷，第 1 期，页码 311–321，2012。'
- en: '[94] I. Chingovska, A. R. Dos Anjos, and S. Marcel, “Biometrics evaluation
    under spoofing attacks,” *IEEE TIFS*, 2014.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] I. Chingovska, A. R. Dos Anjos 和 S. Marcel，“在欺骗攻击下的生物特征评估，”发表于 *IEEE TIFS*，2014。'
- en: '[95] I. J. S. Biometrics., “Information technology–biometric presentation attack
    detection–part 3: testing and reporting,” 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] I. J. S. Biometrics，“信息技术——生物特征呈现攻击检测——第 3 部分：测试与报告，”2017。'
- en: '[96] Z. Wang, Z. Yu, C. Zhao, X. Zhu, Y. Qin, Q. Zhou, F. Zhou, and Z. Lei,
    “Deep spatial gradient and temporal depth learning for face anti-spoofing,” in
    *CVPR*, 2020.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Z. Wang, Z. Yu, C. Zhao, X. Zhu, Y. Qin, Q. Zhou, F. Zhou 和 Z. Lei，“深度空间梯度和时间深度学习用于人脸反欺诈，”发表于
    *CVPR*，2020。'
- en: '[97] K.-Y. Zhang, T. Yao, J. Zhang, Y. Tai, S. Ding, J. Li, F. Huang, H. Song,
    and L. Ma, “Face anti-spoofing via disentangled representation learning,” in *ECCV*.   Springer,
    2020.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K.-Y. Zhang, T. Yao, J. Zhang, Y. Tai, S. Ding, J. Li, F. Huang, H. Song
    和 L. Ma，“通过解耦表示学习的人脸反欺诈，”发表于 *ECCV*。Springer, 2020。'
- en: '[98] Z. Yu, Y. Qin, H. Zhao, X. Li, and G. Zhao, “Dual-cross central difference
    network for face anti-spoofing,” in *IJCAI*, 2021.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Z. Yu, Y. Qin, H. Zhao, X. Li 和 G. Zhao，“用于人脸反欺诈的双交叉中心差分网络，”发表于 *IJCAI*，2021。'
- en: '[99] Y. Liu, J. Stehouwer, and X. Liu, “On disentangling spoof trace for generic
    face anti-spoofing,” in *ECCV*.   Springer, 2020.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Liu, J. Stehouwer 和 X. Liu，“解开伪造痕迹以实现通用人脸反欺诈，”发表于 *ECCV*。Springer,
    2020。'
- en: '[100] X. Xu, Y. Xiong, and W. Xia, “On improving temporal consistency for online
    face liveness detection,” in *ICCVW*, 2021.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Xu, Y. Xiong 和 W. Xia，“提升在线人脸活性检测的时间一致性，”发表于 *ICCVW*，2021。'
- en: '[101] C.-Y. Wang, Y.-D. Lu, S.-T. Yang, and S.-H. Lai, “Patchnet: A simple
    face anti-spoofing framework via fine-grained patch recognition,” in *CVPR*, 2022.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C.-Y. Wang, Y.-D. Lu, S.-T. Yang 和 S.-H. Lai，“Patchnet：通过细粒度补丁识别的简单人脸反欺诈框架，”发表于
    *CVPR*，2022。'
- en: '[102] Z. Wang, Z. Wang, Z. Yu, W. Deng, J. Li, S. Li, and Z. Wang, “Domain
    generalization via shuffled style assembly for face anti-spoofing,” in *CVPR*,
    2022.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Z. Wang, Z. Wang, Z. Yu, W. Deng, J. Li, S. Li 和 Z. Wang，“通过混合风格组装的领域泛化用于人脸反欺诈，”发表于
    *CVPR*，2022。'
- en: '[103] Y. Jia, J. Zhang, S. Shan, and X. Chen, “Unified unsupervised and semi-supervised
    domain adaptation network for cross-scenario face anti-spoofing,” *Pattern Recognition*,
    2021.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Jia, J. Zhang, S. Shan, 和 X. Chen，"用于跨场景面部防伪的统一无监督和半监督领域适应网络"，*模式识别*，2021年。'
- en: '[104] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” 2016.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] K. He, X. Zhang, S. Ren, 和 J. Sun，"用于图像识别的深度残差学习"，2016年。'
- en: '[105] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *CVPR*, 2017.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] G. Huang, Z. Liu, L. Van Der Maaten, 和 K. Q. Weinberger，"密集连接卷积网络"，发表于*CVPR*，2017年。'
- en: '[106] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” in *CVPR*, 2015.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. Long, E. Shelhamer, 和 T. Darrell，"用于语义分割的全卷积网络"，发表于*CVPR*，2015年。'
- en: '[107] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards real-time
    object detection with region proposal networks,” *IEEE TPAMI*, vol. 39, no. 6,
    pp. 1137–1149, 2016.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] S. Ren, K. He, R. Girshick, 和 J. Sun，"Faster R-CNN: 面向实时目标检测的区域提议网络"，*IEEE
    TPAMI*，第39卷，第6期，页码1137–1149，2016年。'
- en: '[108] T. Ahonen, A. Hadid, and M. Pietikainen, “Face description with local
    binary patterns: Application to face recognition,” *TPAMI*, no. 12, pp. 2037–2041,
    2006.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] T. Ahonen, A. Hadid, 和 M. Pietikainen，"使用局部二值模式的面部描述: 面部识别应用"，*TPAMI*，第12期，页码2037–2041，2006年。'
- en: '[109] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *CVPR*.   IEEE, 2005.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] N. Dalal 和 B. Triggs，"用于人体检测的方向梯度直方图"，发表于*CVPR*，IEEE，2005年。'
- en: '[110] J. Galbally and S. Marcel, “Face anti-spoofing based on general image
    quality assessment,” in *ICPR*, 2014.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. Galbally 和 S. Marcel，"基于通用图像质量评估的面部防伪"，发表于*ICPR*，2014年。'
- en: '[111] T. Brox and J. Malik, “Large displacement optical flow: descriptor matching
    in variational motion estimation,” *TPAMI*, vol. 33, no. 3, pp. 500–513, 2010.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] T. Brox 和 J. Malik，"大位移光流: 变分运动估计中的描述符匹配"，*TPAMI*，第33卷，第3期，页码500–513，2010年。'
- en: '[112] X. Niu, Z. Yu, H. Han, X. Li, S. Shan, and G. Zhao, “Video-based remote
    physiological measurement via cross-verified feature disentangling,” in *ECCV*.   Springer,
    2020.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] X. Niu, Z. Yu, H. Han, X. Li, S. Shan, 和 G. Zhao，"通过交叉验证特征解耦的视频基础远程生理测量"，发表于*ECCV*，Springer，2020年。'
- en: '[113] R. Cai and C. Chen, “Learning deep forest with multi-scale local binary
    pattern features for face anti-spoofing,” *arXiv preprint arXiv:1910.03850*, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] R. Cai 和 C. Chen，"使用多尺度局部二值模式特征的深度森林学习用于面部防伪"，*arXiv 预印本 arXiv:1910.03850*，2019年。'
- en: '[114] L. Feng, L.-M. Po, Y. Li, X. Xu, F. Yuan, T. C.-H. Cheung, and K.-W.
    Cheung, “Integration of image quality and motion cues for face anti-spoofing:
    A neural network approach,” *Journal of Visual Communication and Image Representation*,
    2016.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] L. Feng, L.-M. Po, Y. Li, X. Xu, F. Yuan, T. C.-H. Cheung, 和 K.-W. Cheung，"图像质量和运动线索的集成用于面部防伪:
    神经网络方法"，*视觉通信与图像表示期刊*，2016年。'
- en: '[115] Z. Yu, X. Li, P. Wang, and G. Zhao, “Transrppg: Remote photoplethysmography
    transformer for 3d mask face presentation attack detection,” *IEEE SPL*, 2021.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Z. Yu, X. Li, P. Wang, 和 G. Zhao，"Transrppg: 用于三维面具面部呈现攻击检测的远程光电容积描记变换器"，*IEEE
    SPL*，2021年。'
- en: '[116] L. Li, Z. Xia, X. Jiang, Y. Ma, F. Roli, and X. Feng, “3d face mask presentation
    attack detection based on intrinsic image analysis,” *IET Biometrics*, 2020.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] L. Li, Z. Xia, X. Jiang, Y. Ma, F. Roli, 和 X. Feng，"基于内在图像分析的三维面具呈现攻击检测"，*IET
    Biometrics*，2020年。'
- en: '[117] A. Agarwal, M. Vatsa, and R. Singh, “Chif: Convoluted histogram image
    features for detecting silicone mask based face presentation attack,” in *BTAS*,
    2019.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. Agarwal, M. Vatsa, 和 R. Singh，"CHIF: 用于检测硅胶面具的卷积直方图图像特征"，发表于*BTAS*，2019年。'
- en: '[118] R. Shao, X. Lan, and P. C. Yuen, “Joint discriminative learning of deep
    dynamic textures for 3d mask face anti-spoofing,” *TIFS*, 2018.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] R. Shao, X. Lan, 和 P. C. Yuen，"用于三维面具面部防伪的深度动态纹理联合判别学习"，*TIFS*，2018年。'
- en: '[119] G. Zhao and M. Pietikainen, “Dynamic texture recognition using local
    binary patterns with an application to facial expressions,” *IEEE TPAMI*, vol. 29,
    no. 6, pp. 915–928, 2007.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] G. Zhao 和 M. Pietikainen，"使用局部二值模式的动态纹理识别及其在面部表情中的应用"，*IEEE TPAMI*，第29卷，第6期，页码915–928，2007年。'
- en: '[120] O. Sharifi, “Score-level-based face anti-spoofing system using handcrafted
    and deep learned characteristics,” *International Journal of Image, Graphics and
    Signal Processing*, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] O. Sharifi，"基于得分级的面部防伪系统，使用手工设计和深度学习特征"，*国际图像、图形和信号处理期刊*，2019年。'
- en: '[121] Y. A. U. Rehman, L.-M. Po, M. Liu, Z. Zou, and W. Ou, “Perturbing convolutional
    feature maps with histogram of oriented gradients for face liveness detection,”
    in *CISIS and ICEUTE 2019*.   Springer, 2019.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y. A. U. Rehman, L.-M. Po, M. Liu, Z. Zou 和 W. Ou，“利用方向梯度直方图扰动卷积特征图进行人脸活体检测，”
    见 *CISIS 和 ICEUTE 2019*。Springer，2019年。'
- en: '[122] L. Li, Z. Xia, A. Hadid, X. Jiang, H. Zhang, and X. Feng, “Replayed video
    attack detection based on motion blur analysis,” *TIFS*, 2019.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] L. Li, Z. Xia, A. Hadid, X. Jiang, H. Zhang 和 X. Feng，“基于运动模糊分析的重播视频攻击检测，”
    *TIFS*，2019年。'
- en: '[123] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] O. Ronneberger, P. Fischer 和 T. Brox，“U-net：用于生物医学图像分割的卷积网络，” 见 *国际医学图像计算与计算机辅助干预大会*。Springer，2015年。'
- en: '[124] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” in *International conference on
    machine learning*.   PMLR, 2015.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] S. Ioffe 和 C. Szegedy，“批量归一化：通过减少内部协变量偏移来加速深度网络训练，” 见 *国际机器学习大会*。PMLR，2015年。'
- en: '[125] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The journal
    of machine learning research*, 2014.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever 和 R. Salakhutdinov，“Dropout：一种防止神经网络过拟合的简单方法，”
    *机器学习研究期刊*，2014年。'
- en: '[126] A. Hermans, L. Beyer, and B. Leibe, “In defense of the triplet loss for
    person re-identification,” *arXiv preprint arXiv:1703.07737*, 2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] A. Hermans, L. Beyer 和 B. Leibe，“捍卫三元组损失用于人员重新识别，” *arXiv 预印本 arXiv:1703.07737*，2017年。'
- en: '[127] O. Lucena, A. Junior, V. Moia, R. Souza, E. Valle, and R. Lotufo, “Transfer
    learning using convolutional neural networks for face anti-spoofing,” in *ICIAR*,
    2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] O. Lucena, A. Junior, V. Moia, R. Souza, E. Valle 和 R. Lotufo，“使用卷积神经网络进行迁移学习以实现人脸防伪，”
    见 *ICIAR*，2017年。'
- en: '[128] H. Chen, G. Hu, Z. Lei, Y. Chen, N. M. Robertson, and S. Z. Li, “Attention-based
    two-stream convolutional networks for face spoofing detection,” *TIFS*, 2019.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] H. Chen, G. Hu, Z. Lei, Y. Chen, N. M. Robertson 和 S. Z. Li，“基于注意力的双流卷积网络进行人脸伪造检测，”
    *TIFS*，2019年。'
- en: '[129] A. George and S. Marcel, “On the effectiveness of vision transformers
    for zero-shot face anti-spoofing,” *arXiv preprint arXiv:2011.08019*, 2020.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] A. George 和 S. Marcel，“关于视觉变换器在零样本人脸防伪中的有效性，” *arXiv 预印本 arXiv:2011.08019*，2020年。'
- en: '[130] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *CVPR*, 2018, pp. 4510–4520.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov 和 L.-C. Chen，“Mobilenetv2：倒置残差和线性瓶颈，”
    见 *CVPR*，2018年，第4510–4520页。'
- en: '[131] D. Deb and A. K. Jain, “Look locally infer globally: A generalizable
    face anti-spoofing approach,” *TIFS*, 2020.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] D. Deb 和 A. K. Jain，“局部观察全局推断：一种可推广的人脸防伪方法，” *TIFS*，2020年。'
- en: '[132] Z. Xu, S. Li, and W. Deng, “Learning temporal features using lstm-cnn
    architecture for face anti-spoofing,” in *ACPR*, 2015.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Z. Xu, S. Li 和 W. Deng，“使用 LSTM-CNN 架构学习时间特征以进行人脸防伪，” 见 *ACPR*，2015年。'
- en: '[133] U. Muhammad, T. Holmberg, W. C. de Melo, and A. Hadid, “Face anti-spoofing
    via sample learning based recurrent neural network (rnn).” in *BMVC*, 2019.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] U. Muhammad, T. Holmberg, W. C. de Melo 和 A. Hadid，“通过样本学习的递归神经网络（RNN）进行人脸防伪。”
    见 *BMVC*，2019年。'
- en: '[134] H. Ge, X. Tu, W. Ai, Y. Luo, Z. Ma, and M. Xie, “Face anti-spoofing by
    the enhancement of temporal motion,” in *CTISC*.   IEEE, 2020.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] H. Ge, X. Tu, W. Ai, Y. Luo, Z. Ma 和 M. Xie，“通过增强时间运动进行人脸防伪，” 见 *CTISC*。IEEE，2020年。'
- en: '[135] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，” *神经计算*，第9卷，第8期，1735–1780页，1997年。'
- en: '[136] H. Hao, M. Pei, and M. Zhao, “Face liveness detection based on client
    identity using siamese network,” in *PRCV*.   Springer, 2019.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] H. Hao, M. Pei 和 M. Zhao，“基于客户身份的面部活体检测，使用孪生网络，” 见 *PRCV*。Springer，2019年。'
- en: '[137] B. Chen, W. Yang, H. Li, S. Wang, and S. Kwong, “Camera invariant feature
    learning for generalized face anti-spoofing,” *TIFS*, 2021.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] B. Chen, W. Yang, H. Li, S. Wang 和 S. Kwong，“用于广义人脸防伪的相机不变特征学习，” *TIFS*，2021年。'
- en: '[138] A. Mohammadi, S. Bhattacharjee, and S. Marcel, “Improving cross-dataset
    performance of face presentation attack detection systems using face recognition
    datasets,” in *ICASSP*, 2020.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Mohammadi, S. Bhattacharjee 和 S. Marcel，“利用人脸识别数据集提高人脸展示攻击检测系统的跨数据集性能，”
    见 *ICASSP*，2020年。'
- en: '[139] D. Peng, J. Xiao, R. Zhu, and G. Gao, “Ts-fen: Probing feature selection
    strategy for face anti-spoofing,” in *ICASS*.   IEEE, 2020.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] D. Peng, J. Xiao, R. Zhu, 和 G. Gao， “TS-FEN：面部防伪的特征选择策略探究，” 发表在 *ICASS*，IEEE，2020年。'
- en: '[140] H. Wu, D. Zeng, Y. Hu, H. Shi, and T. Mei, “Dual spoof disentanglement
    generation for face anti-spoofing with depth uncertainty learning,” *TCSVT*, 2021.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] H. Wu, D. Zeng, Y. Hu, H. Shi, 和 T. Mei， “针对面部防伪的双重伪造解缠生成功能与深度不确定性学习，”
    *TCSVT*，2021年。'
- en: '[141] Z. Wang, Q. Wang, W. Deng, and G. Guo, “Learning multi-granularity temporal
    characteristics for face anti-spoofing,” *TIFS*, 2022.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Z. Wang, Q. Wang, W. Deng, 和 G. Guo， “学习多粒度时间特征以防止面部伪造，” *TIFS*，2022年。'
- en: '[142] M. S. Hossain, L. Rupty, K. Roy, M. Hasan, S. Sengupta, and N. Mohammed,
    “A-deeppixbis: Attentional angular margin for face anti-spoofing,” 2020.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] M. S. Hossain, L. Rupty, K. Roy, M. Hasan, S. Sengupta, 和 N. Mohammed，
    “A-deeppixbis：面部防伪的注意力角度边距，” 2020年。'
- en: '[143] Z. Yu, Y. Qin, X. Xu, C. Zhao, Z. Wang, Z. Lei, and G. Zhao, “Auto-fas:
    Searching lightweight networks for face anti-spoofing,” in *ICASSP*, 2020.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Z. Yu, Y. Qin, X. Xu, C. Zhao, Z. Wang, Z. Lei, 和 G. Zhao， “Auto-fas：为面部防伪搜索轻量级网络，”
    发表在 *ICASSP*，2020年。'
- en: '[144] K. Roy, M. Hasan, L. Rupty, M. Hossain, S. Sengupta, S. N. Taus, N. Mohammed
    *et al.*, “Bi-fpnfas: Bi-directional feature pyramid network for pixel-wise face
    anti-spoofing by leveraging fourier spectra,” *Sensors*, vol. 21, no. 8, p. 2799,
    2021.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] K. Roy, M. Hasan, L. Rupty, M. Hossain, S. Sengupta, S. N. Taus, N. Mohammed
    *等*， “Bi-fpnfas：利用傅里叶谱进行像素级面部防伪的双向特征金字塔网络，” *传感器*，第21卷，第8期，第2799页，2021年。'
- en: '[145] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola,
    “A kernel two-sample test,” *The Journal of Machine Learning Research*, 2012.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, 和 A. Smola， “一种核两样本检验，”
    *机器学习研究杂志*，2012年。'
- en: '[146] G. Wang, H. Han, S. Shan, and X. Chen, “Improving cross-database face
    presentation attack detection via adversarial domain adaptation,” in *ICB*.   IEEE,
    2019.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] G. Wang, H. Han, S. Shan, 和 X. Chen， “通过对抗领域适应提高跨数据库面部呈现攻击检测，” 发表在 *ICB*，IEEE，2019年。'
- en: '[147] Wang, Guoqing and Han, Hu and Shan, Shiguang and Chen, Xilin, “Unsupervised
    adversarial domain adaptation for cross-domain face presentation attack detection,”
    *TIFS*, 2020.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Wang, Guoqing 和 Han, Hu 和 Shan, Shiguang 和 Chen, Xilin， “用于跨领域面部呈现攻击检测的无监督对抗领域适应，”
    *TIFS*，2020年。'
- en: '[148] R. Quan, Y. Wu, X. Yu, and Y. Yang, “Progressive transfer learning for
    face anti-spoofing,” *TIP*, vol. 30, pp. 3946–3955, 2021.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] R. Quan, Y. Wu, X. Yu, 和 Y. Yang， “面部防伪的渐进转移学习，” *TIP*，第30卷，第3946–3955页，2021年。'
- en: '[149] F. Zhou, C. Gao, F. Chen, C. Li, X. Li, F. Yang, and Y. Zhao, “Face anti-spoofing
    based on multi-layer domain adaptation,” in *ICMEW*.   IEEE, 2019.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] F. Zhou, C. Gao, F. Chen, C. Li, X. Li, F. Yang, 和 Y. Zhao， “基于多层领域适应的面部防伪，”
    发表在 *ICMEW*，IEEE，2019年。'
- en: '[150] A. Mohammadi, S. Bhattacharjee, and S. Marcel, “Domain adaptation for
    generalization of face presentation attack detection in mobile settengs with minimal
    information,” in *ICASSP*, 2020.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] A. Mohammadi, S. Bhattacharjee, 和 S. Marcel， “在移动环境中最小信息下的面部呈现攻击检测的领域适应，”
    发表在 *ICASSP*，2020年。'
- en: '[151] H. Li, S. Wang, P. He, and A. Rocha, “Face anti-spoofing with deep neural
    network distillation,” *IEEE Journal of Selected Topics in Signal Processing*,
    2020.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] H. Li, S. Wang, P. He, 和 A. Rocha， “基于深度神经网络蒸馏的面部防伪，” *IEEE 选择信号处理杂志*，2020年。'
- en: '[152] Z. Chen, T. Yao, K. Sheng, S. Ding, Y. Tai, J. Li, F. Huang, and X. Jin,
    “Generalizable representation learning for mixture domain face anti-spoofing,”
    in *AAAI*, 2021.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Z. Chen, T. Yao, K. Sheng, S. Ding, Y. Tai, J. Li, F. Huang, 和 X. Jin，
    “用于混合领域面部防伪的通用表示学习，” 发表在 *AAAI*，2021年。'
- en: '[153] S. Liu, K.-Y. Zhang, T. Yao, M. Bi, S. Ding, J. Li, F. Huang, and L. Ma,
    “Adaptive normalized representation learning for generalizable face anti-spoofing,”
    in *ACM MM*, 2021.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] S. Liu, K.-Y. Zhang, T. Yao, M. Bi, S. Ding, J. Li, F. Huang, 和 L. Ma，
    “用于通用面部防伪的自适应标准化表示学习，” 发表在 *ACM MM*，2021年。'
- en: '[154] J. Wang, J. Zhang, Y. Bian, Y. Cai, C. Wang, and S. Pu, “Self-domain
    adaptation for face anti-spoofing,” in *AAAI*, 2021.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] J. Wang, J. Zhang, Y. Bian, Y. Cai, C. Wang, 和 S. Pu， “面部防伪的自域适应，” 发表在
    *AAAI*，2021年。'
- en: '[155] D. Pérez-Cabo, D. Jiménez-Cabello, A. Costa-Pazo, and R. J. López-Sastre,
    “Learning to learn face-pad: a lifelong learning approach,” in *IJCB*.   IEEE,
    2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] D. Pérez-Cabo, D. Jiménez-Cabello, A. Costa-Pazo, 和 R. J. López-Sastre，
    “学习学习面部贴片：一种终身学习方法，” 发表在 *IJCB*，IEEE，2020年。'
- en: '[156] O. Nikisins, A. Mohammadi, A. Anjos, and S. Marcel, “On effectiveness
    of anomaly detection approaches against unseen presentation attacks in face anti-spoofing,”
    in *ICB*.   IEEE, 2018.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] O. Nikisins, A. Mohammadi, A. Anjos, 和 S. Marcel， “针对未见伪造攻击的异常检测方法的有效性，”
    发表在 *ICB*，IEEE，2018年。'
- en: '[157] F. Xiong and W. AbdAlmageed, “Unknown presentation attack detection with
    face rgb images,” in *BTAS*, 2018.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] F. Xiong 和 W. AbdAlmageed, “基于面部RGB图像的未知呈现攻击检测，” 见 *BTAS*，2018年。'
- en: '[158] Y. Baweja, P. Oza, P. Perera, and V. M. Patel, “Anomaly detection-based
    unknown face presentation attack detection,” *arXiv preprint arXiv:2007.05856*,
    2020.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Y. Baweja, P. Oza, P. Perera, 和 V. M. Patel, “基于异常检测的未知面孔呈现攻击检测，” *arXiv
    预印本 arXiv:2007.05856*，2020年。'
- en: '[159] D. Pérez-Cabo, D. Jiménez-Cabello, A. Costa-Pazo, and R. J. López-Sastre,
    “Deep anomaly detection for generalized face anti-spoofing,” in *CVPRW*, 2019.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] D. Pérez-Cabo, D. Jiménez-Cabello, A. Costa-Pazo, 和 R. J. López-Sastre,
    “用于广义面部反欺骗的深度异常检测，” 见 *CVPRW*，2019年。'
- en: '[160] A. George and S. Marcel, “Learning one class representations for face
    presentation attack detection using multi-channel convolutional neural networks,”
    *TIFS*, 2020.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. George 和 S. Marcel, “使用多通道卷积神经网络学习单类表示以进行面部呈现攻击检测，” *TIFS*，2020年。'
- en: '[161] Z. Li, H. Li, K.-Y. Lam, and A. C. Kot, “Unseen face presentation attack
    detection with hypersphere loss,” in *ICASSP*, 2020.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Z. Li, H. Li, K.-Y. Lam, 和 A. C. Kot, “使用超球面损失进行未见面部呈现攻击检测，” 见 *ICASSP*，2020年。'
- en: '[162] Y. A. U. Rehman, L.-M. Po, and M. Liu, “Slnet: Stereo face liveness detection
    via dynamic disparity-maps and convolutional neural network,” *Expert Systems
    with Applications*, 2020.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. A. U. Rehman, L.-M. Po, 和 M. Liu, “Slnet：通过动态视差图和卷积神经网络进行立体面部活性检测，”
    *专家系统与应用*，2020年。'
- en: '[163] W. Hu, G. Te, J. He, D. Chen, and Z. Guo, “Aurora guard: Real-time face
    anti-spoofing via light reflection,” *arXiv preprint arXiv: 1902.10311*, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] W. Hu, G. Te, J. He, D. Chen, 和 Z. Guo, “Aurora Guard：通过光反射进行实时面部反欺骗，”
    *arXiv 预印本 arXiv: 1902.10311*，2019年。'
- en: '[164] B. Wu, M. Pan, and Y. Zhang, “A review of face anti-spoofing and its
    applications in china,” in *International Conference on Harmony Search Algorithm*.   Springer,
    2019, pp. 35–43.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] B. Wu, M. Pan, 和 Y. Zhang, “面部反欺骗及其在中国的应用综述，” 见 *国际和谐搜索算法会议*，Springer，2019年，第35–43页。'
- en: '[165] J. Connell, N. Ratha, J. Gentile, and R. Bolle, “Fake iris detection
    using structured light,” in *ICASSP*, 2013.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] J. Connell, N. Ratha, J. Gentile, 和 R. Bolle, “使用结构光的假虹膜检测，” 见 *ICASSP*，2013年。'
- en: '[166] X. Sun, L. Huang, and C. Liu, “Context based face spoofing detection
    using active near-infrared images,” in *ICPR*, 2016.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] X. Sun, L. Huang, 和 C. Liu, “基于上下文的面部欺骗检测，使用主动近红外图像，” 见 *ICPR*，2016年。'
- en: '[167] J. Seo and I.-J. Chung, “Face liveness detection using thermal face-cnn
    with external knowledge,” *Symmetry*, 2019.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Seo 和 I.-J. Chung, “使用外部知识的热成像面部CNN进行面部活性检测，” *对称性*，2019年。'
- en: '[168] M. Kang, J. Choe, H. Ha, H.-G. Jeon, S. Im, and I. S. Kweon, “Facial
    depth and normal estimation using single dual-pixel camera,” *arXiv preprint arXiv:2111.12928*,
    2021.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] M. Kang, J. Choe, H. Ha, H.-G. Jeon, S. Im, 和 I. S. Kweon, “使用单个双像素摄像头进行面部深度和法线估计，”
    *arXiv 预印本 arXiv:2111.12928*，2021年。'
- en: '[169] X. Wu, J. Zhou, J. Liu, F. Ni, and H. Fan, “Single-shot face anti-spoofing
    for dual pixel camera,” *TIFS*, 2020.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] X. Wu, J. Zhou, J. Liu, F. Ni, 和 H. Fan, “用于双像素摄像头的单次面部反欺骗，” *TIFS*，2020年。'
- en: '[170] H. Farrukh, R. M. Aburas, S. Cao, and H. Wang, “Facerevelio: a face liveness
    detection system for smartphones with a single front camera,” in *Proceedings
    of the 26th Annual International Conference on Mobile Computing and Networking*,
    2020.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] H. Farrukh, R. M. Aburas, S. Cao, 和 H. Wang, “Facerevelio：用于智能手机单前置摄像头的面部活性检测系统，”
    见 *第26届年度国际移动计算与网络会议论文集*，2020年。'
- en: '[171] A. F. Ebihara, K. Sakurai, and H. Imaoka, “Specular-and diffuse-reflection-based
    face spoofing detection for mobile devices,” in *IJCB*.   IEEE, 2020.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] A. F. Ebihara, K. Sakurai, 和 H. Imaoka, “基于镜面反射和漫反射的移动设备面部欺骗检测，” 见 *IJCB*，IEEE，2020年。'
- en: '[172] A. Parkin and O. Grinchuk, “Recognizing multi-modal face spoofing with
    face recognition networks,” in *CVPRW*, 2019.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] A. Parkin 和 O. Grinchuk, “使用面部识别网络识别多模态面部欺骗，” 见 *CVPRW*，2019年。'
- en: '[173] H. Kuang, R. Ji, H. Liu, S. Zhang, X. Sun, F. Huang, and B. Zhang, “Multi-modal
    multi-layer fusion network with average binary center loss for face anti-spoofing,”
    in *ACM MM*, 2019.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] H. Kuang, R. Ji, H. Liu, S. Zhang, X. Sun, F. Huang, 和 B. Zhang, “具有平均二进制中心损失的多模态多层融合网络用于面部反欺骗，”
    见 *ACM MM*，2019年。'
- en: '[174] T. Shen, Y. Huang, and Z. Tong, “Facebagnet: Bag-of-local-features model
    for multi-modal face anti-spoofing,” in *CVPRW*, 2019.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] T. Shen, Y. Huang, 和 Z. Tong, “Facebagnet：用于多模态面部反欺骗的局部特征包模型，” 见 *CVPRW*，2019年。'
- en: '[175] A. George and S. Marcel, “Cross modal focal loss for rgbd face anti-spoofing,”
    in *CVPR*, 2021.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] A. George 和 S. Marcel, “用于RGBD面部反欺骗的交叉模态焦点损失，” 见 *CVPR*，2021年。'
- en: '[176] O. Nikisins, A. George, and S. Marcel, “Domain adaptation in multi-channel
    autoencoder based features for robust face anti-spoofing,” in *ICB*.   IEEE, 2019.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] O. Nikisins, A. George 和 S. Marcel，“多通道自编码器基础特征的领域适应，用于稳健的面部反欺骗”，发表于
    *ICB*，IEEE，2019年。'
- en: '[177] W. Liu, X. Wei, T. Lei, X. Wang, H. Meng, and A. K. Nandi, “Data fusion
    based two-stage cascade framework for multi-modality face anti-spoofing,” *TCDS*,
    2021.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] W. Liu, X. Wei, T. Lei, X. Wang, H. Meng 和 A. K. Nandi，“基于数据融合的两阶段级联框架用于多模态面部反欺骗”，*TCDS*，2021年。'
- en: '[178] P. Zhang, F. Zou, Z. Wu, N. Dai, S. Mark, M. Fu, J. Zhao, and K. Li,
    “Feathernets: Convolutional neural networks as light as feather for face anti-spoofing,”
    in *CVPRW*, 2019.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] P. Zhang, F. Zou, Z. Wu, N. Dai, S. Mark, M. Fu, J. Zhao 和 K. Li，“Feathernets：如羽毛般轻便的卷积神经网络用于面部反欺骗”，发表于
    *CVPRW*，2019年。'
- en: '[179] F. Jiang, P. Liu, X. Shao, and X. Zhou, “Face anti-spoofing with generated
    near-infrared images,” *Multimedia Tools and Applications*, vol. 79, no. 29, pp.
    21 299–21 323, 2020.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] F. Jiang, P. Liu, X. Shao 和 X. Zhou，“通过生成的近红外图像进行面部反欺骗”，*Multimedia Tools
    and Applications*，第79卷，第29期，第21,299–21,323页，2020年。'
- en: '[180] A. Liu, Z. Tan, J. Wan, Y. Liang, Z. Lei, G. Guo, and S. Z. Li, “Face
    anti-spoofing via adversarial cross-modality translation,” *TIFS*, 2021.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] A. Liu, Z. Tan, J. Wan, Y. Liang, Z. Lei, G. Guo 和 S. Z. Li，“通过对抗性跨模态翻译进行面部反欺骗”，*TIFS*，2021年。'
- en: '[181] K. Mallat and J.-L. Dugelay, “Indirect synthetic attack on thermal face
    biometric systems via visible-to-thermal spectrum conversion,” in *CVPRW*, 2021.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] K. Mallat 和 J.-L. Dugelay，"通过可见光到热成像光谱转换对热面部生物识别系统进行间接合成攻击"，发表于 *CVPRW*，2021年。'
- en: '[182] S. Liu, S. Lu, H. Xu, J. Yang, S. Ding, and L. Ma, “Feature generation
    and hypothesis verification for reliable face anti-spoofing,” in *AAAI*, 2022.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] S. Liu, S. Lu, H. Xu, J. Yang, S. Ding 和 L. Ma，“可靠面部反欺骗的特征生成与假设验证”，发表于
    *AAAI*，2022年。'
- en: '[183] A. F. Sequeira, T. Gonçalves, W. Silva, J. R. Pinto, and J. S. Cardoso,
    “An exploratory study of interpretability for face presentation attack detection,”
    *IET Biometrics*, 2021.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] A. F. Sequeira, T. Gonçalves, W. Silva, J. R. Pinto 和 J. S. Cardoso，“面部呈现攻击检测的可解释性探索研究”，*IET
    Biometrics*，2021年。'
- en: '[184] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    in *ICCV*, 2017.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh 和 D. Batra，“Grad-CAM：通过基于梯度的定位从深度网络中获得的可视化解释”，发表于
    *ICCV*，2017年。'
- en: '[185] H. Mirzaalian, M. E. Hussein, L. Spinoulas, J. May, and W. Abd-Almageed,
    “Explaining face presentation attack detection using natural language,” in *FG*.   IEEE,
    2021.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] H. Mirzaalian, M. E. Hussein, L. Spinoulas, J. May 和 W. Abd-Almageed，“使用自然语言解释面部呈现攻击检测”，发表于
    *FG*，IEEE，2021年。'
- en: '[186] A. Liu, X. Li, J. Wan, Y. Liang, S. Escalera, H. J. Escalante, M. Madadi,
    Y. Jin, Z. Wu, X. Yu *et al.*, “Cross-ethnicity face anti-spoofing recognition
    challenge: A review,” *IET Biometrics*, vol. 10, no. 1, pp. 24–43, 2021.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] A. Liu, X. Li, J. Wan, Y. Liang, S. Escalera, H. J. Escalante, M. Madadi,
    Y. Jin, Z. Wu, X. Yu *等*，“跨种族面部反欺骗识别挑战：综述”，*IET Biometrics*，第10卷，第1期，第24–43页，2021年。'
- en: '[187] Z. Yu, C. Zhao, K. H. Cheng, X. Cheng, and G. Zhao, “Flexible-modal face
    anti-spoofing: A benchmark,” *arXiv preprint arXiv:2202.08192*, 2022.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Z. Yu, C. Zhao, K. H. Cheng, X. Cheng 和 G. Zhao，“灵活模态面部反欺骗：基准”，*arXiv
    预印本 arXiv:2202.08192*，2022年。'
- en: '[188] J. Stehouwer, A. Jourabloo, Y. Liu, and X. Liu, “Noise modeling, synthesis
    and classification for generic object anti-spoofing,” in *CVPR*, 2020.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] J. Stehouwer, A. Jourabloo, Y. Liu 和 X. Liu，“用于通用对象反欺骗的噪声建模、合成和分类”，发表于
    *CVPR*，2020年。'
- en: '[189] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “A general framework
    for adversarial examples with objectives,” *ACM Transactions on Privacy and Security
    (TOPS)*, 2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] M. Sharif, S. Bhagavatula, L. Bauer 和 M. K. Reiter，“具有目标的一般对抗样本框架”，*ACM
    Transactions on Privacy and Security (TOPS)*，2019年。'
- en: '[190] S. Komkov and A. Petiushko, “Advhat: Real-world adversarial attack on
    arcface face id system,” in *ICPR*, 2021.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] S. Komkov 和 A. Petiushko，“Advhat：对 ArcFace 面部识别系统的真实世界对抗攻击”，发表于 *ICPR*，2021年。'
- en: '[191] B. Yin, W. Wang, T. Yao, J. Guo, Z. Kong, S. Ding, J. Li, and C. Liu,
    “Adv-makeup: A new imperceptible and transferable attack on face recognition,”
    in *IJCAI*, 2021.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] B. Yin, W. Wang, T. Yao, J. Guo, Z. Kong, S. Ding, J. Li 和 C. Liu，“Adv-makeup：一种新的隐形且可转移的面部识别攻击”，发表于
    *IJCAI*，2021年。'
- en: '[192] Y. Guo, X. Wei, G. Wang, and B. Zhang, “Meaningful adversarial stickers
    for face recognition in physical world,” *arXiv preprint arXiv:2104.06728*, 2021.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Y. Guo, X. Wei, G. Wang 和 B. Zhang，“物理世界中面部识别的有意义对抗性贴纸”，*arXiv 预印本 arXiv:2104.06728*，2021年。'
- en: '[193] B. Zhang, B. Tondi, and M. Barni, “Attacking cnn-based anti-spoofing
    face authentication in the physical domain,” *arXiv preprint arXiv:1910.00327*,
    2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] B. Zhang, B. Tondi 和 M. Barni, “在物理领域攻击基于 CNN 的反欺诈人脸认证”，*arXiv 预印本 arXiv:1910.00327*,
    2019。'
- en: '[194] U. A. Ciftci, I. Demir, and L. Yin, “Fakecatcher: Detection of synthetic
    portrait videos using biological signals,” *TPAMI*, 2020.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] U. A. Ciftci, I. Demir 和 L. Yin, “FakeCatcher：利用生物信号检测合成肖像视频”，*TPAMI*,
    2020。'
- en: '[195] E. Sarkar, P. Korshunov, L. Colbois, and S. Marcel, “Are gan-based morphs
    threatening face recognition?” in *ICASSP*, 2022.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] E. Sarkar, P. Korshunov, L. Colbois 和 S. Marcel, “基于 GAN 的变形是否威胁人脸识别？”在
    *ICASSP*, 2022。'
- en: '[196] D. Deb, X. Liu, and A. K. Jain, “Unified detection of digital and physical
    face attacks,” *arXiv preprint arXiv:2104.02156*, 2021.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] D. Deb, X. Liu 和 A. K. Jain, “数字和物理人脸攻击的统一检测”，*arXiv 预印本 arXiv:2104.02156*,
    2021。'
- en: '[197] P. Voigt and A. Von dem Bussche, “The eu general data protection regulation
    (gdpr),” *A Practical Guide, 1st Ed., Cham: Springer International Publishing*,
    2017.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] P. Voigt 和 A. Von dem Bussche, “欧盟通用数据保护条例 (GDPR)”，*实用指南，第1版，Cham: Springer
    International Publishing*, 2017。'
- en: '[198] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient
    learning of deep networks from decentralized data,” in *Artificial Intelligence
    and Statistics*.   PMLR, 2017.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] B. McMahan, E. Moore, D. Ramage, S. Hampson 和 B. A. y Arcas, “从去中心化数据中进行通信高效的深度网络学习”，在
    *人工智能与统计*，PMLR，2017。'
- en: '[199] R. Shao, B. Zhang, P. C. Yuen, and V. M. Patel, “Federated test-time
    adaptive face presentation attack detection with dual-phase privacy preservation,”
    in *FG*.   IEEE, 2021.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] R. Shao, B. Zhang, P. C. Yuen 和 V. M. Patel, “具有双阶段隐私保护的联邦测试时自适应人脸展示攻击检测”，在
    *FG*，IEEE，2021。'
- en: '[200] J. N. Kundu, N. Venkat, R. V. Babu *et al.*, “Universal source-free domain
    adaptation,” in *CVPR*, 2020.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] J. N. Kundu, N. Venkat, R. V. Babu *等*，“通用无源领域适应”，在 *CVPR*，2020。'
- en: '[201] L. Lv, Y. Xiang, X. Li, H. Huang, R. Ruan, X. Xu, and Y. Fu, “Combining
    dynamic image and prediction ensemble for cross-domain face anti-spoofing,” in
    *ICASSP*, 2021.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] L. Lv, Y. Xiang, X. Li, H. Huang, R. Ruan, X. Xu 和 Y. Fu, “结合动态图像和预测集成进行跨域人脸反欺诈”，在
    *ICASSP*, 2021。'
- en: '[202] O. Kähm and N. Damer, “2d face liveness detection: An overview,” in *BIOSIG*.   IEEE,
    2012.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] O. Kähm 和 N. Damer, “2D 人脸活体检测：概述”，在 *BIOSIG*，IEEE，2012。'
- en: '[203] A. Hadid, “Face biometrics under spoofing attacks: Vulnerabilities, countermeasures,
    open issues, and research directions,” in *CVPRW*, 2014.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] A. Hadid, “人脸生物特征在欺诈攻击下：漏洞、对策、开放问题和研究方向”，在 *CVPRW*, 2014。'
- en: '[204] J. Galbally, S. Marcel, and J. Fierrez, “Biometric antispoofing methods:
    A survey in face recognition,” *IEEE Access*, 2014.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] J. Galbally, S. Marcel 和 J. Fierrez, “生物特征反欺诈方法：人脸识别中的调查”，*IEEE Access*,
    2014。'
- en: '[205] S. Kumar, S. Singh, and J. Kumar, “A comparative study on face spoofing
    attacks,” in *ICCCA*.   IEEE, 2017.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] S. Kumar, S. Singh 和 J. Kumar, “人脸欺诈攻击的比较研究”，在 *ICCCA*，IEEE，2017。'
- en: '[206] D. R. Kisku and R. D. Rakshit, “Face spoofing and counter-spoofing: A
    survey of state-of-the-art algorithms,” *Transactions on Machine Learning and
    Artificial Intelligence*, vol. 5, no. 2, pp. 31–31, 2017.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] D. R. Kisku 和 R. D. Rakshit, “人脸欺诈与反欺诈：最先进算法的综述”，*机器学习与人工智能学报*, 第5卷，第2期，第31–31页,
    2017。'
- en: '[207] L. Souza, L. Oliveira, M. Pamplona, and J. Papa, “How far did we get
    in face spoofing detection?” *Engineering Applications of Artificial Intelligence*,
    vol. 72, pp. 368–381, 2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] L. Souza, L. Oliveira, M. Pamplona 和 J. Papa, “我们在人脸欺诈检测方面取得了多远的进展？”*工程应用人工智能*,
    第72卷，第368–381页, 2018。'
- en: '[208] E. A. Raheem, S. M. S. Ahmad, and W. A. W. Adnan, “Insight on face liveness
    detection: A systematic literature review,” *International Journal of Electrical
    and Computer Engineering*, 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] E. A. Raheem, S. M. S. Ahmad 和 W. A. W. Adnan, “人脸活体检测的洞察：系统文献综述”，*International
    Journal of Electrical and Computer Engineering*, 2019。'
- en: '[209] Z. Ming, M. Visani, M. M. Luqman, and J.-C. Burie, “A survey on anti-spoofing
    methods for facial recognition with rgb cameras of generic consumer devices,”
    *Journal of Imaging*, 2020.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Z. Ming, M. Visani, M. M. Luqman 和 J.-C. Burie, “针对通用消费设备 RGB 摄像头的面部识别反欺诈方法综述”，*Journal
    of Imaging*, 2020。'
- en: '[210] R. Cai, H. Li, S. Wang, C. Chen, and A. C. Kot, “Drl-fas: A novel framework
    based on deep reinforcement learning for face anti-spoofing,” *TIFS*, vol. 16,
    pp. 937–951, 2020.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] R. Cai, H. Li, S. Wang, C. Chen 和 A. C. Kot, “DRL-FAS：一种基于深度强化学习的人脸反欺诈新框架”，*TIFS*,
    第16卷，第937–951页, 2020。'
- en: '[211] Z. Yu, X. Li, J. Shi, Z. Xia, and G. Zhao, “Revisiting pixel-wise supervision
    for face anti-spoofing,” *IEEE TBIOM*, 2021.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Z. Yu, X. Li, J. Shi, Z. Xia 和 G. Zhao, “重新审视像素级监督在防人脸欺诈中的作用”，*IEEE TBIOM*,
    2021。'
- en: '[212] T. Kim and Y. Kim, “Suppressing spoof-irrelevant factors for domain-agnostic
    face anti-spoofing,” *arXiv preprint arXiv:2012.01271*, 2020.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] T. Kim 和 Y. Kim，“抑制与伪造无关的因素以实现领域无关的面部防伪”，*arXiv preprint arXiv:2012.01271*，2020年。'
- en: '[213] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale
    and rotation invariant texture classification with local binary patterns,” *TPAMI*,
    vol. 24, no. 7, pp. 971–987, 2002.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] T. Ojala, M. Pietikainen, 和 T. Maenpaa，“使用局部二值模式的多分辨率灰度和旋转不变纹理分类”，*TPAMI*，第24卷，第7期，页码971–987，2002年。'
- en: '[214] L. Li and X. Feng, “Face anti-spoofing via deep local binary pattern,”
    in *Deep Learning in Object Detection and Recognition*.   Springer, 2019, pp.
    91–111.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] L. Li 和 X. Feng，“通过深度局部二值模式进行面部防伪”，发表于*Deep Learning in Object Detection
    and Recognition*。 Springer，2019年，页码91–111。'
- en: '[215] H. Chen, Y. Chen, X. Tian, and R. Jiang, “A cascade face spoofing detector
    based on face anti-spoofing r-cnn and improved retinex lbp,” *IEEE Access*, 2019.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] H. Chen, Y. Chen, X. Tian, 和 R. Jiang，“一种基于面部防伪R-CNN和改进的Retinex LBP的级联面部伪造检测器”，*IEEE
    Access*，2019年。'
- en: '[216] P. K. Das, B. Hu, C. Liu, K. Cui, P. Ranjan, and G. Xiong, “A new approach
    for face anti-spoofing using handcrafted and deep network features,” in *SOLI*.   IEEE,
    2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] P. K. Das, B. Hu, C. Liu, K. Cui, P. Ranjan, 和 G. Xiong，“一种基于手工制作和深度网络特征的新型面部防伪方法”，发表于*SOLI*。
    IEEE，2019年。'
- en: '[217] D. Menotti, G. Chiachia, A. Pinto, W. R. Schwartz, H. Pedrini, A. X.
    Falcao, and A. Rocha, “Deep representations for iris, face, and fingerprint spoofing
    detection,” *TIFS*, 2015.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] D. Menotti, G. Chiachia, A. Pinto, W. R. Schwartz, H. Pedrini, A. X.
    Falcao, 和 A. Rocha，“用于虹膜、面部和指纹伪造检测的深度表示”，*TIFS*，2015年。'
- en: '[218] L. Li, Z. Xia, L. Li, X. Jiang, X. Feng, and F. Roli, “Face anti-spoofing
    via hybrid convolutional neural network,” in *FADS*.   IEEE, 2017.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] L. Li, Z. Xia, L. Li, X. Jiang, X. Feng, 和 F. Roli，“通过混合卷积神经网络进行面部防伪”，发表于*FADS*。
    IEEE，2017年。'
- en: '[219] C. Nagpal and S. R. Dubey, “A performance evaluation of convolutional
    neural networks for face anti spoofing,” in *IJCNN*.   IEEE, 2019.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] C. Nagpal 和 S. R. Dubey，“卷积神经网络在面部防伪中的性能评估”，发表于*IJCNN*。 IEEE，2019年。'
- en: '[220] X. Tu and Y. Fang, “Ultra-deep neural network for face anti-spoofing,”
    in *International Conference on Neural Information Processing*.   Springer, 2017.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] X. Tu 和 Y. Fang，“超深神经网络用于面部防伪”，发表于*International Conference on Neural
    Information Processing*。 Springer，2017年。'
- en: '[221] Y. A. U. Rehman, L. M. Po, and M. Liu, “Deep learning for face anti-spoofing:
    An end-to-end approach,” in *SPA*.   IEEE, 2017, pp. 195–200.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Y. A. U. Rehman, L. M. Po, 和 M. Liu，“深度学习在面部防伪中的应用：一种端到端的方法”，发表于*SPA*。
    IEEE，2017年，页码195–200。'
- en: '[222] C. Lin, Z. Liao, P. Zhou, J. Hu, and B. Ni, “Live face verification with
    multiple instantialized local homographic parameterization.” in *IJCAI*, 2018.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] C. Lin, Z. Liao, P. Zhou, J. Hu, 和 B. Ni，“使用多个瞬时局部单应性参数化进行实时面部验证”，发表于*IJCAI*，2018年。'
- en: '[223] G. B. de Souza, J. P. Papa, and A. N. Marana, “On the learning of deep
    local features for robust face spoofing detection,” in *SIBGRAPI*.   IEEE, 2018.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] G. B. de Souza, J. P. Papa, 和 A. N. Marana，“关于深度局部特征学习以提高面部伪造检测的鲁棒性”，发表于*SIBGRAPI*。
    IEEE，2018年。'
- en: '[224] Y. A. U. Rehman, L. M. Po, and M. Liu, “Livenet: Improving features generalization
    for face liveness detection using convolution neural networks,” *Expert Systems
    with Applications*, vol. 108, pp. 159–169, 2018.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Y. A. U. Rehman, L. M. Po, 和 M. Liu，“Livenet：通过卷积神经网络提高面部活体检测特征的泛化能力”，*Expert
    Systems with Applications*，第108卷，页码159–169，2018年。'
- en: '[225] S. Luo, M. Kan, S. Wu, X. Chen, and S. Shan, “Face anti-spoofing with
    multi-scale information,” in *ICPR*, 2018.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] S. Luo, M. Kan, S. Wu, X. Chen, 和 S. Shan，“基于多尺度信息的面部防伪”，发表于*ICPR*，2018年。'
- en: '[226] K. Larbi, W. Ouarda, H. Drira, B. B. Amor, and C. B. Amar, “Deepcolorfasd:
    Face anti spoofing solution using a multi channeled color spaces cnn,” in *SMC*.   IEEE,
    2018.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] K. Larbi, W. Ouarda, H. Drira, B. B. Amor, 和 C. B. Amar，“Deepcolorfasd：一种基于多通道颜色空间CNN的面部防伪解决方案”，发表于*SMC*。
    IEEE，2018年。'
- en: '[227] R. Bresan, A. Pinto, A. Rocha, C. Beluzo, and T. Carvalho, “Facespoof
    buster: a presentation attack detector based on intrinsic image properties and
    deep learning,” *arXiv preprint arXiv:1902.02845*, 2019.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] R. Bresan, A. Pinto, A. Rocha, C. Beluzo, 和 T. Carvalho，“Facespoof buster：一种基于内在图像属性和深度学习的展示攻击检测器”，*arXiv
    preprint arXiv:1902.02845*，2019年。'
- en: '[228] Y. A. U. Rehman, L.-M. Po, M. Liu, Z. Zou, W. Ou, and Y. Zhao, “Face
    liveness detection using convolutional-features fusion of real and deep network
    generated face images,” *Journal of Visual Communication and Image Representation*,
    2019.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Y. A. U. Rehman, L.-M. Po, M. Liu, Z. Zou, W. Ou, 和 Y. Zhao，“使用卷积特征融合真实和深度网络生成的面部图像进行面部活体检测”，*Journal
    of Visual Communication and Image Representation*，2019年。'
- en: '[229] R. Laurensi, A. Israel, L. T. Menon, N. Penna, O. Manoel Camillo, A. L.
    Koerich, and A. S. Britto Jr, “Style transfer applied to face liveness detection
    with user-centered models,” *arXiv*, pp. arXiv–1907, 2019.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] R. Laurensi, A. Israel, L. T. Menon, N. Penna, O. Manoel Camillo, A.
    L. Koerich 和 A. S. Britto Jr，“风格迁移应用于面部活性检测中的用户中心模型，” *arXiv*，第arXiv–1907页，2019年。'
- en: '[230] X. Tu, Z. Ma, J. Zhao, G. Du, M. Xie, and J. Feng, “Learning generalizable
    and identity-discriminative representations for face anti-spoofing,” *ACM TIST*,
    vol. 11, no. 5, pp. 1–19, 2020.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] X. Tu, Z. Ma, J. Zhao, G. Du, M. Xie 和 J. Feng，“学习可泛化且具有身份区分的面部防伪表示，”
    *ACM TIST*，第11卷，第5期，第1–19页，2020年。'
- en: '[231] J. Guo, X. Zhu, J. Xiao, Z. Lei, G. Wan, and S. Z. Li, “Improving face
    anti-spoofing by 3d virtual synthesis,” in *ICB*, 2019, pp. 1–8.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] J. Guo, X. Zhu, J. Xiao, Z. Lei, G. Wan 和 S. Z. Li，“通过3D虚拟合成提高面部防伪，”
    在 *ICB*，2019年，第1–8页。'
- en: '[232] A. Pinto, S. Goldenstein, A. Ferreira, T. Carvalho, H. Pedrini, and A. Rocha,
    “Leveraging shape, reflectance and albedo from shading for face presentation attack
    detection,” *TIFS*, 2020.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] A. Pinto, S. Goldenstein, A. Ferreira, T. Carvalho, H. Pedrini 和 A. Rocha，“利用形状、反射率和从阴影中获得的反射率进行面部呈现攻击检测，”
    *TIFS*，2020年。'
- en: '[233] Y. Zuo, W. Gao, and J. Wang, “Face liveness detection algorithm based
    on livenesslight network,” in *HPBD&IS*.   IEEE, 2020.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] Y. Zuo, W. Gao 和 J. Wang，“基于活性光网络的面部活性检测算法，” 在 *HPBD&IS*。 IEEE，2020年。'
- en: '[234] B. Chen, W. Yang, and S. Wang, “Face anti-spoofing by fusing high and
    low frequency features for advanced generalization capability,” in *MIPR*.   IEEE,
    2020, pp. 199–204.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] B. Chen, W. Yang 和 S. Wang，“通过融合高低频特征实现面部防伪的高级泛化能力，” 在 *MIPR*。 IEEE，2020年，第199–204页。'
- en: '[235] A. Parkin and O. Grinchuk, “Creating artificial modalities to solve rgb
    liveness,” *arXiv preprint arXiv:2006.16028*, 2020.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] A. Parkin 和 O. Grinchuk，“创建人工模态以解决RGB活性问题，” *arXiv 预印本 arXiv:2006.16028*，2020年。'
- en: '[236] Y. Huang, W. Zhang, and J. Wang, “Deep frequent spatial temporal learning
    for face anti-spoofing,” *arXiv preprint arXiv:2002.03723*, 2020.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] Y. Huang, W. Zhang 和 J. Wang，“深度频繁时空学习用于面部防伪，” *arXiv 预印本 arXiv:2002.03723*，2020年。'
- en: '[237] Y. Ma, L. Wu, Z. Li *et al.*, “A novel face presentation attack detection
    scheme based on multi-regional convolutional neural networks,” *PR Letters*, vol.
    131, pp. 261–267, 2020.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Y. Ma, L. Wu, Z. Li *等*，“基于多区域卷积神经网络的新型面部呈现攻击检测方案，” *PR Letters*，第131卷，第261–267页，2020年。'
- en: '[238] W. Sun, Y. Song, H. Zhao, and Z. Jin, “A face spoofing detection method
    based on domain adaptation and lossless size adaptation,” *IEEE Access*, 2020.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] W. Sun, Y. Song, H. Zhao 和 Z. Jin，“基于领域适应和无损尺寸适应的面部伪造检测方法，” *IEEE Access*，2020年。'
- en: '[239] W. Zheng, M. Yue, S. Zhao, and S. Liu, “Attention-based spatial-temporal
    multi-scale network for face anti-spoofing,” *TBIOM*, 2021.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] W. Zheng, M. Yue, S. Zhao 和 S. Liu，“基于注意力的时空多尺度网络用于面部防伪，” *TBIOM*，2021年。'
- en: '[240] Y. Chen, T. Wang, J. Wang, P. Shi, and H. Snoussi, “Towards good practices
    in face anti-spoofing: An image reconstruction based method,” in *CAC*.   IEEE,
    2019.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Y. Chen, T. Wang, J. Wang, P. Shi 和 H. Snoussi，“面部防伪的最佳实践：一种基于图像重建的方法，”
    在 *CAC*。 IEEE，2019年。'
- en: '[241] X. Tu, H. Zhang, M. Xie, Y. Luo, Y. Zhang, and Z. Ma, “Deep transfer
    across domains for face antispoofing,” *Journal of Electronic Imaging*, 2019.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] X. Tu, H. Zhang, M. Xie, Y. Luo, Y. Zhang 和 Z. Ma，“跨领域深度迁移用于面部防伪，” *电子成像杂志*，2019年。'
- en: '[242] S. Saha, W. Xu, M. Kanakis, S. Georgoulis, Y. Chen, D. Pani Paudel, and
    L. Van Gool, “Domain agnostic feature learning for image and video based face
    anti-spoofing,” in *CVPRW*, 2020.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] S. Saha, W. Xu, M. Kanakis, S. Georgoulis, Y. Chen, D. Pani Paudel 和
    L. Van Gool，“图像和视频基础的面部防伪的领域无关特征学习，” 在 *CVPRW*，2020年。'
- en: '[243] S. Fatemifar, S. R. Arashloo, M. Awais, and J. Kittler, “Spoofing attack
    detection by anomaly detection,” in *ICASSP*, 2019.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] S. Fatemifar, S. R. Arashloo, M. Awais 和 J. Kittler，“通过异常检测进行伪造攻击检测，”
    在 *ICASSP*，2019年。'
- en: '[244] S. Fatemifar, M. Awais, A. Akbari, and J. Kittler, “A stacking ensemble
    for anomaly based client-specific face spoofing detection,” in *ICIP*.   IEEE,
    2020.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] S. Fatemifar, M. Awais, A. Akbari 和 J. Kittler，“用于异常基础客户端特定面部伪造检测的堆叠集成，”
    在 *ICIP*。 IEEE，2020年。'
- en: '[245] S. Fatemifar, S. R. Arashloo, M. Awais, and J. Kittler, “Client-specific
    anomaly detection for face presentation attack detection,” *Pattern Recognition*,
    2020.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] S. Fatemifar, S. R. Arashloo, M. Awais 和 J. Kittler，“面部呈现攻击检测的客户端特定异常检测，”
    *模式识别*，2020年。'
- en: '[246] M. Kowalski, “A study on presentation attack detection in thermal infrared,”
    *Sensors*, vol. 20, no. 14, p. 3988, 2020.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] M. Kowalski，“热红外中的呈现攻击检测研究，” *传感器*，第20卷，第14期，第3988页，2020年。'
- en: '[247] G. Wang, C. Lan, H. Han, S. Shan, and X. Chen, “Multi-modal face presentation
    attack detection via spatial and channel attentions,” in *CVPRW*, 2019.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] G. Wang, C. Lan, H. Han, S. Shan 和 X. Chen, “通过空间和通道注意力的多模态人脸呈现攻击检测，”
    *CVPRW* 会议，2019.'
- en: '[248] L. Li, Z. Gao, L. Huang, H. Zhang, and M. Lin, “A dual-modal face anti-spoofing
    method via light-weight networks,” in *ASID*.   IEEE, 2019.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] L. Li, Z. Gao, L. Huang, H. Zhang 和 M. Lin, “一种通过轻量级网络的双模态人脸反欺骗方法，” *ASID*
    会议。IEEE, 2019.'
- en: '[249] X. Li, W. Wu, T. Li, Y. Su, and L. Yang, “Face liveness detection based
    on parallel cnn,” in *Journal of Physics: Conference Series*.   IOP Publishing,
    2020.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] X. Li, W. Wu, T. Li, Y. Su 和 L. Yang, “基于并行CNN的人脸活体检测，” *Journal of Physics:
    Conference Series*。IOP Publishing, 2020.'
- en: '[250] A. George and S. Marcel, “Can your face detector do anti-spoofing? face
    presentation attack detection with a multi-channel face detector,” *arXiv preprint
    arXiv:2006.16836*, 2020.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] A. George 和 S. Marcel, “你的面部检测器能做反欺骗吗？使用多通道面部检测器的人脸呈现攻击检测，” *arXiv预印本
    arXiv:2006.16836*，2020.'
- en: '[251] Q. Yang, X. Zhu, J.-K. Fwu, Y. Ye, G. You, and Y. Zhu, “Pipenet: Selective
    modal pipeline of fusion network for multi-modal face anti-spoofing,” in *CVPRW*,
    2020.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] Q. Yang, X. Zhu, J.-K. Fwu, Y. Ye, G. You 和 Y. Zhu, “Pipenet: 多模态人脸反欺骗的选择性模态融合网络管道，”
    *CVPRW* 会议，2020.'
- en: '[252] G. Te, W. Hu, and Z. Guo, “Exploring hypergraph representation on face
    anti-spoofing beyond 2d attacks,” in *ICME*.   IEEE, 2020.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] G. Te, W. Hu 和 Z. Guo, “探索超图表示在超越2D攻击的人脸反欺骗中的应用，” *ICME* 会议。IEEE, 2020.'
- en: '| ![[Uncaptioned image]](img/063c5e600f24eb807a920c1e445b9db5.png) | Zitong
    Yu received the M.S. degree in multimedia from University of Nantes, France, in
    2016, and he received the Ph.D. degree in computer science from University of
    Oulu, Finland, in 2022\. His research interests include face anti-spoofing, remote
    physiological measurement and video understanding. He led the team and won the
    1st Place in the ChaLearn multi-modal face anti-spoofing attack detection challenge
    with CVPR 2020. |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/063c5e600f24eb807a920c1e445b9db5.png) | 子桐于2016年在法国南特大学获得多媒体硕士学位，并于2022年在芬兰奥卢大学获得计算机科学博士学位。他的研究兴趣包括人脸反欺骗、远程生理测量和视频理解。他领导的团队在CVPR
    2020的ChaLearn多模态人脸反欺骗攻击检测挑战中获得第一名。 |'
- en: '| ![[Uncaptioned image]](img/b539c27ef5e639bddfd59c354d7a2573.png) | Yunxiao
    Qin received the M.S. degree in control theory and control engineering from the
    School of Marine Science and Technology, Northwestern Polytechnical University,
    Xi’an, China, in 2015, and he received the Ph.D. degree in control science and
    engineering from the School of Automation, Northwestern Polytechnical University,
    Xi’an, China, in 2021\. His current research interests include meta-learning,
    face anti-spoofing, and deep reinforcement learning. |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/b539c27ef5e639bddfd59c354d7a2573.png) | 云霄秦于2015年在中国西安的西北工业大学海洋科学与工程学院获得控制理论与控制工程硕士学位，并于2021年在西北工业大学自动化学院获得控制科学与工程博士学位。他当前的研究兴趣包括元学习、人脸反欺骗和深度强化学习。
    |'
- en: '| ![[Uncaptioned image]](img/8cb096f1958a01d1b94437d3bcec4c4a.png) | Xiaobai
    Li received her B.Sc. degree in Psychology from Peking University, M.Sc. degree
    in Biophysics from the Chinese Academy of Science, and Ph.D. degree in Computer
    Science from University of Oulu. She is currently an assistant professor in the
    Center for Machine Vision and Signal Analysis of University of Oulu. Her research
    of interests includes facial expression recognition, micro-expression analysis,
    remote physiological signal measurement from facial videos, and related applications
    in affective computing, healthcare and biometrics. She is an associate editor
    for IEEE TCSVT, Frontiers in Psychology, and Image and Vision Computing. Dr. Li
    was a co-chair of several international workshops in CVPR, ICCV, FG and ACM Multimedia.
    |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/8cb096f1958a01d1b94437d3bcec4c4a.png) | 小白李在北京大学获得心理学学士学位，在中国科学院获得生物物理硕士学位，并在奥卢大学获得计算机科学博士学位。她目前是奥卢大学机器视觉与信号分析中心的助理教授。她的研究兴趣包括面部表情识别、微表情分析、从面部视频中测量远程生理信号以及在情感计算、医疗保健和生物识别中的相关应用。她是IEEE
    TCSVT、Frontiers in Psychology和Image and Vision Computing的副编辑。李博士曾担任CVPR、ICCV、FG和ACM
    Multimedia若干国际研讨会的共同主席。 |'
- en: '| ![[Uncaptioned image]](img/c508c52923ee7cedd492b23a07ea05f0.png) | Chenxu
    Zhao received M.S. Degree from Beihang University, Beijing, China, in 2016, and
    was in the joint programme with National Laboratory of Pattern Recognition (NLPR)
    Laboratory of Institute of Automation, Chinese Academy of Sciences, from 2014
    to 2016. He is currently served as a Co-Founder in SailYond Technology, Beijing,
    China. He served as a Research Director in MiningLamp Technology, Beijing, China.
    His major research areas include face analysis, anomaly detection and meta-learning.
    |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/c508c52923ee7cedd492b23a07ea05f0.png) | 赵晨旭于2016年获得北京航空航天大学的硕士学位，并于2014至2016年在中国科学院自动化研究所模式识别国家实验室联合项目中工作。目前他是北京赛阳科技有限公司的联合创始人，并曾在北京矿灯科技有限公司担任研究总监。他的主要研究领域包括人脸分析、异常检测和元学习。
    |'
- en: '| ![[Uncaptioned image]](img/075ae8b881d388e183cd25b7d5f02715.png) | Zhen Lei
    received the B.S. degree in automation from the University of Science and Technology
    of China, in 2005, and the Ph.D. degree from the Institute of Automation, Chinese
    Academy of Sciences, in 2010, where he is currently a Professor. He is IAPR Fellow
    and AAIA Fellow. He has published over 200 papers in international journals and
    conferences with 21000+ citations in Google Scholar and h-index 71\. He was competition
    co-chair of IJCB2022 and has served as area chairs for server conferences and
    is associate editor for IEEE Trans. on Information Forensics and Security, Pattern
    Recognition, Neurocomputing and IET Computer Vision journals. His research interests
    are in computer vision, pattern recognition, image processing, and face recognition
    in particular. He is the winner of 2019 IAPR Young Biometrics Investigator Award.
    |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/075ae8b881d388e183cd25b7d5f02715.png) | 雷振获得了中国科技大学自动化专业的学士学位（2005年）和中国科学院自动化研究所的博士学位（2010年），现为该研究所教授。他是IAPR
    Fellow 和 AAIA Fellow。 他在国际期刊和会议上发表了超过200篇论文，Google Scholar引用次数超过21000次，h-index为71。他曾担任IJCB2022的竞赛共同主席，并担任多个会议的领域主席，目前是IEEE信息取证与安全期刊、模式识别期刊、神经计算期刊以及IET计算机视觉期刊的副编辑。他的研究兴趣包括计算机视觉、模式识别、图像处理，特别是人脸识别。他是2019年IAPR青年生物特征研究员奖的获得者。
    |'
- en: '| ![[Uncaptioned image]](img/56a714e51fa89f097025631dc1b984cf.png) | Guoying
    Zhao (IEEE Fellow 2022) received the Ph.D. degree in computer science from the
    Chinese Academy of Sciences, Beijing, China, in 2005\. She is currently an Academy
    Professor and full Professor (tenured in 2017) with University of Oulu. She is
    also a visiting professor with Aalto University. She is a member of Finnish Academy
    of Sciences and Letters, IAPR Fellow and AAIA Fellow. She has authored or co-authored
    more than 280 papers in journals and conferences with 20100+ citations in Google
    Scholar and h-index 66\. She is panel chair for FG 2023, was co-program chair
    for ACM International Conference on Multimodal Interaction (ICMI 2021), co-publicity
    chair for FG2018, and has served as area chairs for several conferences and was/is
    associate editor for IEEE Trans. on Multimedia, Pattern Recognition, IEEE Trans.
    on Circuits and Systems for Video Technology, and Image and Vision Computing Journals.
    Her current research interests include image and video descriptors, facial-expression
    and micro-expression recognition, emotional gesture analysis, affective computing,
    and biometrics. Her research has been reported by Finnish TV programs, newspapers
    and MIT Technology Review. |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/56a714e51fa89f097025631dc1b984cf.png) | 郭莹赵（IEEE Fellow 2022）于2005年在中国科学院计算技术研究所获得计算机科学博士学位。她目前是奥卢大学的院士教授和终身教授（2017年终身），同时也是阿尔托大学的访问教授。她是芬兰科学院和文学学院的会员，IAPR
    Fellow 和 AAIA Fellow。她在期刊和会议上发表了超过280篇论文，Google Scholar引用次数超过20100次，h-index为66。她是FG
    2023的评审委员会主席，曾担任ACM国际多模态交互会议（ICMI 2021）的共同程序主席，FG2018的共同宣传主席，并曾担任多个会议的领域主席，还曾是/现在是IEEE多媒体期刊、模式识别期刊、IEEE电路与系统视频技术期刊以及图像与视觉计算期刊的副编辑。她当前的研究兴趣包括图像和视频描述符、面部表情和微表情识别、情感手势分析、情感计算和生物特征识别。她的研究已被芬兰电视节目、报纸和《麻省理工学院科技评论》报道。
    |'
- en: 'TABLE III: A summary of existing surveys in FAS. Most of them focus on handcrafted
    feature based methods under single RGB modality, and investigate limited number
    of public datasets as well as evaluation protocols. ‘DL’ and ‘M&H’ is short for
    ‘deep learning’ and ‘modality & hardware’, respectively. ‘VIS’, ‘NIR’, ‘SWIR’,‘LF’,
    and ‘Polarized’ denotes using commercial visible RGB, near infrared, short-wave
    infrared, light field, and four-directional polarized camera, respectively.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：FAS中现有调查的总结。大多数关注基于手工特征的方法，仅在单一RGB模态下进行，且研究了有限数量的公开数据集以及评估协议。‘DL’和‘M&H’分别是‘深度学习’和‘模态与硬件’的缩写。‘VIS’，‘NIR’，‘SWIR’，‘LF’，和‘Polarized’分别表示使用商业可见RGB，相近红外，短波红外，光场和四向偏振相机。
- en: '| Title & Reference | Year | DL | M&H | Datasets | Protocol |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| 标题与参考 | 年份 | 深度学习 | 模态与硬件 | 数据集 | 协议 |'
- en: '| 2D Face Liveness Detection: an Overview [[202](#bib.bib202)] | 2014 | No
    | VIS | 2 | Intra-dataset intra-type |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| 2D面部活体检测：概述[[202](#bib.bib202)] | 2014 | 否 | VIS | 2 | 数据集内部类型 |'
- en: '|'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Face Biometrics under Spoofing Attacks: Vulnerabilities, Countermeasures,
    &#124;'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部生物识别在欺骗攻击下：脆弱性，对策， &#124;'
- en: '&#124; Open Issues and Research Directions &#124;'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开放问题和研究方向 &#124;'
- en: '[[203](#bib.bib203)] | 2014 | No | VIS | 4 | Intra-dataset intra-type |'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '[[203](#bib.bib203)] | 2014 | 否 | VIS | 4 | 数据集内部类型 |'
- en: '| Biometric Anti-spoofing Methods: A Survey in Face Recognition [[204](#bib.bib204)]
    | 2015 | No | VIS | 6 | Intra-dataset intra-type |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| 生物特征反欺骗方法：面部识别中的调查[[204](#bib.bib204)] | 2015 | 否 | VIS | 6 | 数据集内部类型 |'
- en: '| A Comparative Study on Face Spoofing Attacks [[205](#bib.bib205)] | 2017
    | No | VIS | 9 | Intra-dataset intra-type |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| 关于面部欺骗攻击的比较研究[[205](#bib.bib205)] | 2017 | 否 | VIS | 9 | 数据集内部类型 |'
- en: '| Face Spoofing and Counter-Spoofing: A Survey of State-of-the-art Algorithms [[206](#bib.bib206)]
    | 2017 | No | VIS | 6 |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| 面部欺骗与反欺骗：最新算法的调查[[206](#bib.bib206)] | 2017 | 否 | VIS | 6 |'
- en: '&#124; Intra-dataset intra-type, &#124;'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集内部类型， &#124;'
- en: '&#124; Cross-dataset intra-type &#124;'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨数据集内部类型 &#124;'
- en: '|'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Presentation Attack Detection Methods for Face Recognition Systems:
    &#124;'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部识别系统的呈现攻击检测方法： &#124;'
- en: '&#124; A Comprehensive Survey &#124;'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 综合调查 &#124;'
- en: '[[67](#bib.bib67)] | 2017 | No | VIS | 11 | Intra-dataset intra-type |'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '[[67](#bib.bib67)] | 2017 | 否 | VIS | 11 | 数据集内部类型 |'
- en: '| How Far Did We Get in Face Spoofing Detection? [[207](#bib.bib207)] | 2018
    |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| 我们在面部欺骗检测方面取得了多远的进展？[[207](#bib.bib207)] | 2018 |'
- en: '&#124; Yes &#124;'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Few, $\textless$10) &#124;'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (少量, $\textless$10) &#124;'
- en: '| VIS | 9 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| VIS | 9 |'
- en: '&#124; Intra-dataset intra-type, &#124;'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集内部类型， &#124;'
- en: '&#124; Cross-dataset intra-type &#124;'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨数据集内部类型 &#124;'
- en: '|'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Insight on Face Liveness Detection: A Systematic Literature Review [[208](#bib.bib208)]
    | 2019 | No | VIS | 14 | Intra-dataset intra-type |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| 面部活体检测的见解：系统的文献综述[[208](#bib.bib208)] | 2019 | 否 | VIS | 14 | 数据集内部类型 |'
- en: '| The Rise of Data-driven Models in Presentation Attack Detection [[56](#bib.bib56)]
    | 2019 |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| 数据驱动模型在呈现攻击检测中的崛起[[56](#bib.bib56)] | 2019 |'
- en: '&#124; Yes &#124;'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Few, $\textless$10) &#124;'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (少量, $\textless$10) &#124;'
- en: '| VIS | 7 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| VIS | 7 |'
- en: '&#124; Intra-dataset intra-type, &#124;'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集内部类型， &#124;'
- en: '&#124; Cross-dataset intra-type &#124;'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨数据集内部类型 &#124;'
- en: '|'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| A Survey on 3D Mask Presentation Attack Detection and Countermeasures [[57](#bib.bib57)]
    | 2020 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| 关于3D面具呈现攻击检测和对策的调查[[57](#bib.bib57)] | 2020 |'
- en: '&#124; Yes &#124;'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Few, $\textless$10) &#124;'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (少量, $\textless$10) &#124;'
- en: '| VIS | 10 | Intra-dataset intra-type |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| VIS | 10 | 数据集内部类型 |'
- en: '|'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Deep Convolutional Neural Networks for Face and Iris Presentation Attack
    &#124;'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度卷积神经网络用于面部和虹膜呈现攻击 &#124;'
- en: '&#124; Detection: Survey and Case Study &#124;'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测：调查和案例研究 &#124;'
- en: '[[58](#bib.bib58)] | 2020 |'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '[[58](#bib.bib58)] | 2020 |'
- en: '&#124; Yes &#124;'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Few, $\textless$30) &#124;'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (少量, $\textless$30) &#124;'
- en: '| VIS | 8 |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| VIS | 8 |'
- en: '&#124; Intra-dataset intra-type, &#124;'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集内部类型， &#124;'
- en: '&#124; Cross-dataset intra-type &#124;'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨数据集内部类型 &#124;'
- en: '|'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A Survey On Anti-Spoofing Methods For Face Recognition with &#124;'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部识别的反欺骗方法调查 &#124;'
- en: '&#124; RGB Cameras of Generic Consumer Devices &#124;'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通用消费设备的RGB相机 &#124;'
- en: '[[209](#bib.bib209)] | 2020 |'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '[[209](#bib.bib209)] | 2020 |'
- en: '&#124; Yes &#124;'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 是 &#124;'
- en: '&#124; (Few, $\textless$50) &#124;'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (少量, $\textless$50) &#124;'
- en: '| VIS | 12 |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| VIS | 12 |'
- en: '&#124; Intra-dataset intra-type, &#124;'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集内部类型， &#124;'
- en: '&#124; Cross-dataset intra-type &#124;'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨数据集内部类型 &#124;'
- en: '|'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Deep Learning for Face Anti-spoofing: A Survey (Ours) | 2022 |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
- en: '&#124; Yes &#124;'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Full, $\textgreater$100) &#124;'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VIS, Flash, &#124;'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIR, Thermal, &#124;'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth, SWIR, &#124;'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LF, Polarized &#124;'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: '| 36 |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
- en: '&#124; Intra-dataset intra-type, &#124;'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cross-dataset intra-type, &#124;'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Intra-dataset cross-type, &#124;'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cross-dataset cross-type &#124;'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: ACER (%) results of the intra-dataset intra-type testings on OULU-NPU
    (4 sub-protocols) and SiW (3 sub-protocols) datasets for common deep learning
    methods with binary cross-entropy supervision and pixel-wise supervision.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Venue | OULU-NPU | SiW |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
- en: '|  | Prot. 1 | Prot. 2 | Prot. 3 | Prot. 4 | Prot. 1 | Prot. 2 | Prot. 3 |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
- en: '|  | STASN [[25](#bib.bib25)] | CVPR’19 | 1.9 | 2.2 | 2.8$\pm$1.6 | 7.5$\pm$4.7
    | 1.00 | 0.28$\pm$0.05 | 12.10$\pm$ 1.50 |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
- en: '|  | TSCNN [[128](#bib.bib128)] | TIFS’19 | 5.9 | 4.9 | 5.6$\pm$1.6 | 9.8$\pm$4.2
    | - | - | - |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
- en: '|    Binary | CIFL [[137](#bib.bib137)] | TIFS’21 | 3.4 | 2.4 | 2.5$\pm$0.8
    | 6.1$\pm$4.1 | - | - | - |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
- en: '| Cross-entropy | DRL-FAS [[210](#bib.bib210)] | TIFS’20 | 4.7 | 1.9 | 3.0$\pm$1.5
    | 7.2$\pm$3.9 | 0.00 | 0.00$\pm$0.00 | 4.51$\pm$ 0.00 |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
- en: '| Supervision | SSR-FCN [[131](#bib.bib131)] | TIFS’20 | 4.6 | 3.4 | 2.8 $\pm$2.2
    | 10:8$\pm$ 5:1 | - | - | - |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
- en: '|  | FasTCo [[100](#bib.bib100)] | ICCVW’21 | 0.8 | 1.1 | 1.1$\pm$0.8 | 1.5$\pm$1.2
    | 0.0003 | 0.01$\pm$0.01 | 2.00$\pm$0.56 |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
- en: '|  | PatchNet [[101](#bib.bib101)] | CVPR’22 | 0.0 | 1.2 | 1.18$\pm$1.26 |
    2.9$\pm$3.0 | 0.00 | 0.00$\pm$0.00 | 2.45$\pm$ 0.45 |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
- en: '|  | Auxiliary [[13](#bib.bib13)] | CVPR’18 | 1.6 | 2.7 | 2.9$\pm$1.5 | 9.5$\pm$6.0
    | 3.58 | 0.57$\pm$0.69 | 8.31$\pm$3.81 |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
- en: '|  | PixBiS [[32](#bib.bib32)] | IJCB’19 | 0.4 | 6.0 | 11.1$\pm$9.4 | 25.0$\pm$12.7
    | - | - | - |'
  id: totrans-721
  prefs: []
  type: TYPE_TB
- en: '|  | FAS-SGTD [[96](#bib.bib96)] | CVPR’20 | 1.0 | 1.9 | 2.7$\pm$0.6 | 5.0$\pm$2.2
    | 0.40 | 0.02$\pm$ 0.04 | 2.78$\pm$ 3.57 |'
  id: totrans-722
  prefs: []
  type: TYPE_TB
- en: '| Pixel-wise | De-Spoof [[33](#bib.bib33)] | ECCV’20 | 1.5 | 4.3 | 3.6$\pm$1.6
    | 5.6$\pm$5.7 | - | - | - |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
- en: '| Supervision | Disentangled [[97](#bib.bib97)] | ECCV’20 | 1.3 | 2.4 | 2.2$\pm$
    2.2 | 4.4$\pm$ 3.0 | 0.28 | 0.10$\pm$ 0.04 | 5.59$\pm$ 4.37 |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
- en: '|  | STDN [[99](#bib.bib99)] | ECCV’20 | 1.1 | 1.9 | 2.8$\pm$ 3.3 | 3.8$\pm$
    4.2 | 0.00 | 0.00$\pm$ 0.00 | 7.9$\pm$ 3.3 |'
  id: totrans-725
  prefs: []
  type: TYPE_TB
- en: '|  | BCN [[24](#bib.bib24)] | ECCV’20 | 0.8 | 1.7 | 2.5$\pm$ 1.1 | 5.2$\pm$
    3.7 | 0.36 | 0.11$\pm$0.08 | 2.45$\pm$ 0.68 |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
- en: '|  | CDCN [[23](#bib.bib23)] | CVPR’20 | 1.0 | 1.5 | 2.3$\pm$ 1.4 | 6.9$\pm$
    2.9 | 0.12 | 0.06$\pm$ 0.04 | 1.71$\pm$ 0.11 |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
- en: '|  | DC-CDN [[98](#bib.bib98)] | IJCAI’21 | 0.4 | 1.3 | 1.9$\pm$ 1.1 | 4.0$\pm$
    3.1 | - | - | - |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
- en: '|  | NAS-FAS [[37](#bib.bib37)] | PAMI’21 | 0.2 | 1.2 | 1.7$\pm$0.6 | 2.9$\pm$2.8
    | 0.12 | 0.04$\pm$0.05 | 1.52$\pm$0.13 |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
- en: '|  | MT-FAS [[43](#bib.bib43)] | PAMI’21 | 0.4 | 1.4 | 2.1$\pm$ 1.7 | 3.7$\pm$
    2.9 | - | - | - |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: HTER (%) results of the cross-dataset intra-type testings among OULU-NPU
    (O), CASIA-MFSD (C), Replay-Attack (I), and MSU-MFSD (M) datasets with different
    numbers of source domains for training. For example, ‘C to I’ means training on
    CASIA-MFSD and then testing on Replay-Attack.'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Venue | 1 source domain | 2 source domains | 3 source domains
    |'
  id: totrans-732
  prefs: []
  type: TYPE_TB
- en: '|  | C to I | I to C | M&I to C | M&I to O | O&C&I to M | O&M&I to C | O&C&M
    to I | I&C&M to O |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
- en: '|  | Auxiliary [[13](#bib.bib13)] | CVPR’18 | 27.6 | 28.4 | - | - | - | 28.4
    | 27.6 | - |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
- en: '|  | CDCN [[23](#bib.bib23)] | CVPR’20 | 15.5 | 32.6 | - | - | - | - | - |
    - |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
- en: '| Traditional | FAS-SGTD [[96](#bib.bib96)] | CVPR’20 | 17.0 | 22.8 | - | -
    | - | - | - | - |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
- en: '|    Deep | BCN [[24](#bib.bib24)] | ECCV’20 | 16.6 | 36.4 | - | - | 19.81
    | 25.12 | 22.75 | 21.24 |'
  id: totrans-737
  prefs: []
  type: TYPE_TB
- en: '| Learning | NAS-FAS [[37](#bib.bib37)] | PAMI’21 | - | - | - | - | 16.85 |
    15.21 | 11.63 | 13.16 |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
- en: '|  | MT-FAS [[43](#bib.bib43)] | PAMI’21 | - | - | - | - | 11.67 | 18.44 |
    11.93 | 16.23 |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
- en: '|  | PS [[211](#bib.bib211)] | TBIOM’21 | 13.8 | 31.3 | - | - | 20.42 | 18.25
    | 19.55 | 15.76 |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
- en: '|  | DC-CDN [[98](#bib.bib98)] | IJCAI’21 | 6.0 | 30.1 | - | - | 25.31 | 15.00
    | 15.88 | 18.82 |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
- en: '|  | MADDG [[48](#bib.bib48)] | CVPR’19 | - | - | 41.02 | 39.35 | 17.69 | 24.50
    | 22.19 | 27.98 |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
- en: '|  | PAD-GAN [[50](#bib.bib50)] | CVPR’20 | - | - | 31.67 | 34.02 | 17.02 |
    19.68 | 20.87 | 25.02 |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
- en: '|  | RF-Meta [[49](#bib.bib49)] | AAAI’20 | - | - | - | - | 13.89 | 20.27 |
    17.30 | 16.45 |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
- en: '|  | SSDG [[51](#bib.bib51)] | CVPR’20 | - | - | 31.89 | 36.01 | 7.38 | 10.44
    | 11.71 | 15.61 |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
- en: '| Generalized | SDA [[154](#bib.bib154)] | AAAI’21 | - | - | - | - | 15.40
    | 24.50 | 15.60 | 23.10 |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
- en: '|    Deep | D²AM [[152](#bib.bib152)] | AAAI’21 | - | - | - | - | 12.70 | 20.98
    | 15.43 | 15.27 |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
- en: '| Learning | DASN [[212](#bib.bib212)] | Access’21 | - | - | - | - | 8.33 |
    12.04 | 13.38 | 11.77 |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
- en: '|  | DRDG [liu2021dual] | IJCAI’21 | - | - | 31.28 | 33.35 | 12.43 | 19.05
    | 15.56 | 15.63 |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
- en: '|  | ANRL [[153](#bib.bib153)] | MM’21 | - | - | 31.06 | 30.73 | 10.83 | 17.83
    | 16.03 | 15.67 |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
- en: '|  | FGHV [[182](#bib.bib182)] | AAAI’22 | - | - | - | - | 9.17 | 12.47 | 16.29
    | 13.58 |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
- en: '|  | SSAN [[102](#bib.bib102)] | CVPR’22 | - | - | 30.00 | 29.44 | 6.67 | 10.00
    | 8.88 | 13.72 |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: EER (%) results of the Intra-dataset cross-type testings on SiW-M
    with the leave-one-type-out setting.'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Venue | Replay | Print | Mask Attacks | Makeup Attacks | Partial
    Attacks | Average |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
- en: '|  |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
- en: '&#124; Half &#124;'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Silicone &#124;'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Trans. &#124;'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paper &#124;'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Manne. &#124;'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Obfusc. &#124;'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Im. &#124;'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cos. &#124;'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fun. &#124;'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Glasses &#124;'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Partial &#124;'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
- en: '|  | Auxiliary [[13](#bib.bib13)] | CVPR’18 | 14.0 | 4.3 | 11.6 | 12.4 | 24.6
    | 7.8 | 10.0 | 72.3 | 10.1 | 9.4 | 21.4 | 18.6 | 4.0 | 17.0$\pm$17.7 |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
- en: '|  | CDCN [[23](#bib.bib23)] | CVPR’20 | 8.2 | 7.8 | 8.3 | 7.4 | 20.5 | 5.9
    | 5.0 | 47.8 | 1.6 | 14.0 | 24.5 | 18.3 | 1.1 | 13.1$\pm$ 12.6 |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
- en: '|  | STDN  [[99](#bib.bib99)] | ECCV’20 | 7.6 | 3.8 | 8.4 | 13.8 | 14.5 | 5.3
    | 4.4 | 35.4 | 0.0 | 19.3 | 21.0 | 20.8 | 1.6 | 12.0$\pm$ 10.0 |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
- en: '| Traditional | BCN  [[24](#bib.bib24)] | ECCV’20 | 13.4 | 5.2 | 8.3 | 9.7
    | 13.6 | 5.8 | 2.5 | 33.8 | 0.0 | 14.0 | 23.3 | 16.6 | 1.2 | 11.3$\pm$ 9.5 |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
- en: '|    Deep | SSR-FCN [[131](#bib.bib131)] | TIFS’20 | 6.8 | 11.2 | 2.8 | 6.3
    | 28.5 | 0.4 | 3.3 | 17.8 | 3.9 | 11.7 | 21.6 | 13.5 | 3.6 | 10.1$\pm$ 8.4 |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
- en: '| Learning | PS [[211](#bib.bib211)] | TBIOM’21 | 10.3 | 7.8 | 8.3 | 7.4 |
    10.2 | 5.9 | 5.0 | 43.4 | 0.0 | 12.0 | 23.9 | 15.9 | 0.0 | 11.5$\pm$11.4 |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
- en: '|  | NAS-FAS [[211](#bib.bib211)] | PAMI’21 | 10.3 | 7.8 | 8.3 | 7.4 | 10.2
    | 5.9 | 5.0 | 43.4 | 0.0 | 12.0 | 23.9 | 15.9 | 0.0 | 11.5$\pm$11.4 |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
- en: '|  | DC-CDN [[98](#bib.bib98)] | IJCAI’21 | 10.3 | 8.7 | 11.1 | 7.4 | 12.5
    | 5.9 | 0.0 | 39.1 | 0.0 | 12.0 | 18.9 | 13.5 | 1.2 | 10.8$\pm$10.1 |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
- en: '|  | MT-FAS [[43](#bib.bib43)] | PAMI’21 | 7.8 | 4.4 | 11.2 | 5.8 | 11.2 |
    2.8 | 2.7 | 38.9 | 0.2 | 10.1 | 20.5 | 18.9 | 1.3 | 10.4$\pm$10.2 |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
- en: '|  | ViTranZFAS [[129](#bib.bib129)] | IJCB’21 | 15.2 | 5.8 | 5.8 | 4.9 | 5.9
    | 0.1 | 3.2 | 9.8 | 0.4 | 10.7 | 20.1 | 2.9 | 1.9 | 6.7$\pm$5.6 |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
- en: '|  | DTN [[38](#bib.bib38)] | CVPR’19 | 10.0 | 2.1 | 14.4 | 18.6 | 26.5 | 5.7
    | 9.6 | 50.2 | 10.1 | 13.2 | 19.8 | 20.5 | 8.8 | 16.1$\pm$ 12.2 |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
- en: '| Generalized | Hypersphere [[161](#bib.bib161)] | ICASSP’20 | 13.2 | 14.0
    | 18.1 | 24.0 | 12.4 | 3.1 | 6.2 | 34.8 | 3.1 | 16.3 | 21.4 | 21.7 | 9.3 | 15.2$\pm$
    9.0 |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
- en: '| Learning | FGHV [[102](#bib.bib102)] | AAAI’22 | 9.0 | 8.0 | 5.9 | 9.9 |
    14.3 | 3.7 | 4.8 | 19.3 | 2.0 | 9.2 | 18.9 | 8.5 | 4.7 | 9.1$\pm$ 5.4 |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: Summary of the hybrid (handcraft+deep learning) FAS methods with
    binary cross-entropy supervision. ‘S/D’, ‘CE’, ‘OF’, ‘OFM’, ‘NN’, ‘HOG’, ‘LBP’
    are short for ‘Static/Dynamic’, ‘cross-entropy’, ‘optical flow’, ‘optical flow
    magnitude’, ‘nearest neighbor’, ‘histogram of oriented gradients [[109](#bib.bib109)]’
    and ‘local binary pattern [[213](#bib.bib213)]’, respectively.'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Backbone | Loss | Input | S/D | Description |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
- en: '| DPCNN [[30](#bib.bib30)] | 2016 | VGG-Face | Trained with SVM | RGB | S |
    deep partial features with Blocks PCA |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
- en: '| Multi-cues+NN [[114](#bib.bib114)] | 2016 | MLP | Binary CE loss |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB+OFM &#124;'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
- en: '| D | fused features from image quality cues and motion cues |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
- en: '| CNN LBP-TOP [[20](#bib.bib20)] | 2017 | 5-layer CNN |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SVM &#124;'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | D |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
- en: '&#124; cascading LBP-TOP with CNN to extract &#124;'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; discriminative spatio-temporal features &#124;'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
- en: '| DF-MSLBP [[113](#bib.bib113)] | 2018 | Deep forest | Binary CE loss | HSV+YCbCr
    | S | multi-scale LBP based Tree-Ensembled features |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
- en: '| SPMT+SSD [[19](#bib.bib19)] | 2018 | VGG16 |'
  id: totrans-807
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SVM &#124;'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; bbox regression &#124;'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Landmarks &#124;'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-814
  prefs: []
  type: TYPE_TB
- en: '&#124; hand-crafted texture&depth features cascaded &#124;'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with fast deep face spoofing detector &#124;'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: '| CHIF [[117](#bib.bib117)] | 2019 | VGG-Face | Trained with SVM | RGB | S
    |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
- en: '&#124; convoluted histogram image features for fine- &#124;'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; grained mask texture representation &#124;'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: '| DeepLBP [[214](#bib.bib214)] | 2019 | VGG-Face |'
  id: totrans-822
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SVM &#124;'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB,HSV, &#124;'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; YCbCr &#124;'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-828
  prefs: []
  type: TYPE_TB
- en: '&#124; extracted the handcrafted features from the &#124;'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; convolutional responses of the fine-tuned CNN model &#124;'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN+LBP &#124;'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +WLD &#124;'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
- en: '[[22](#bib.bib22)] | 2019 | CaffeNet | Binary CE loss | RGB | S |'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; combined CNN features with LBP/WLD for preserving &#124;'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; both semantic feature and local information &#124;'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: '| Intrinsic [[116](#bib.bib116)] | 2019 | 1D-CNN | Trained with SVM | Reflection
    | D | deep temporal cues from reflection intensity histogram |'
  id: totrans-839
  prefs: []
  type: TYPE_TB
- en: '| FARCNN [[215](#bib.bib215)] | 2019 |'
  id: totrans-840
  prefs: []
  type: TYPE_TB
- en: '&#124; Multi-scale &#124;'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; attentional CNN &#124;'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Regression loss &#124;'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Crystal loss &#124;'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Center loss &#124;'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | S |'
  id: totrans-847
  prefs: []
  type: TYPE_TB
- en: '&#124; cascade detector features with &#124;'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; improved Retinex based LBP &#124;'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
- en: '| CNN-LSP [[122](#bib.bib122)] | 2019 | 1D-CNN | Trained with SVM | RGB | D
    |'
  id: totrans-851
  prefs: []
  type: TYPE_TB
- en: '&#124; joint learned temporal features with attentional &#124;'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spatial regions and channels from magnified videos &#124;'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: '| DT-Mask [[118](#bib.bib118)] | 2019 | VGG16 |'
  id: totrans-855
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Channel&Spatial- &#124;'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; discriminability &#124;'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB+OF | D |'
  id: totrans-859
  prefs: []
  type: TYPE_TB
- en: '&#124; joint learned discriminative features with &#124;'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; attentional spatial regions and channels &#124;'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: '| VGG+LBP [[216](#bib.bib216)] | 2019 | VGG16 | Binary CE loss | RGB | S |'
  id: totrans-863
  prefs: []
  type: TYPE_TB
- en: '&#124; combining deep CNN features, and LBP features &#124;'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; from brightness and chrominance channels &#124;'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
- en: '| CNN+OVLBP [[120](#bib.bib120)] | 2019 | VGG16 |'
  id: totrans-867
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NN classifier &#124;'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | S |'
  id: totrans-870
  prefs: []
  type: TYPE_TB
- en: '&#124; hybrid decisions using majority vote of CNN, over- &#124;'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; lapped histograms of LBP and their fused vector &#124;'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
- en: '| HOG-Pert. [[121](#bib.bib121)] | 2019 | Multi-scale CNN | Binary CE loss
    |'
  id: totrans-874
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB+HOG &#124;'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: '| S | hybrid convolutional features and HOG features |'
  id: totrans-876
  prefs: []
  type: TYPE_TB
- en: '| LBP-Pert. [[21](#bib.bib21)] | 2020 | Multi-scale CNN | Binary CE loss |'
  id: totrans-877
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB+LBP &#124;'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
- en: '| S | discriminative features enhanced by LBP perturbation |'
  id: totrans-879
  prefs: []
  type: TYPE_TB
- en: '| TransRPPG [[115](#bib.bib115)] | 2021 | Vision Transformer | Binary CE loss
    |'
  id: totrans-880
  prefs: []
  type: TYPE_TB
- en: '&#124; rPPG map &#124;'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: '| D | intrinsic liveness features via fully attentional transformer |'
  id: totrans-882
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: Summary of the representative traditional deep learning'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
- en: based FAS methods with binary cross-entropy supervision. ‘S/D’ and ‘CE’ are
    short for ‘Static/Dynamic’ and ‘cross-entropy’, respectively. ‘Reflect.’, ‘OF’,
    and ‘RP’ denote the generated reflection map, optical flow, and rank pooling,
    respectively. Method Year Backbone Loss Input S/D Description CNN1 [[29](#bib.bib29)]
    2014 8-layer CNN Trained with SVM RGB S deep features from different spatial scales
    LSTM-CNN [[132](#bib.bib132)] 2015 CNN+LSTM Binary CE loss RGB D long-range local
    and dense features from sequence SpoofNet [[217](#bib.bib217)] 2015 2-layer CNN
    Binary CE loss RGB S deep representation with architecture optimization HybridCNN [[218](#bib.bib218)]
    2017 VGG-Face Trained with SVM RGB S hybrid CNN for both global face and facial
    patches CNN2 [[219](#bib.bib219)] 2017 VGG11 Binary CE loss RGB S model trained
    with continuous data-randomization Ultra-Deep [[220](#bib.bib220)] 2017 ResNet50+LSTM
    Binary CE loss RGB D ultra-deep features with rich long-range temporal context
    FASNet [[127](#bib.bib127)] 2017 VGG16 Binary CE loss RGB S transfer learned features
    based on a pre-trained CNN CNN3 [[221](#bib.bib221)] 2018 Inception, ResNet Binary
    CE loss RGB S transferred deep feature MILHP [[222](#bib.bib222)] 2018 ResNet+STN
    Multiple Instances CE loss RGB D underlying subtle motion features LSCNN [[223](#bib.bib223)]
    2018 9 PatchNets Binary CE loss RGB S global feature via aggregating 9 deep local
    features LiveNet [[224](#bib.bib224)] 2018 VGG11 Binary CE loss RGB S model trained
    with continuous data-randomization MS-FANS [[225](#bib.bib225)] 2018 AlexNet+LSTM
    Binary CE loss RGB S multi-scale deep feature with rich spatial context DeepColorFAS [[226](#bib.bib226)]
    2018 5-layer CNN Binary CE loss RGB, HSV, YCbCr S investigates the effect of multi-channel
    space colors on CNN architectures and proposes a fusion based voting method for
    FAS Siamese [[136](#bib.bib136)] 2019 AlexNet Contrastive loss RGB S deep features
    guided by client identity information FSBuster [[227](#bib.bib227)] 2019 ResNet50
    Trained with SVM RGB S fused deep features from Intrinsic Image Properties FuseDNG [[228](#bib.bib228)]
    2019 7-layer CNN Binary CE loss Reconstruction loss RGB S adaptive fusion of deep
    features learned from real-world face and deep autoencoder generated face STASN [[25](#bib.bib25)]
    2019 ResNet50+LSTM Binary CE loss RGB D deep spatio-temporal feature from local
    salient regions TSCNN [[128](#bib.bib128)] 2019 ResNet18 Binary CE loss RGB MSR
    S attentional illumination-invariant features with discriminative high-frequency
    information FAS-UCM [[229](#bib.bib229)] 2019 MobileNetV2 VGG19 Binary CE loss
    Style loss RGB S deep features trained from generated style transferred images
    SLRNN [[133](#bib.bib133)] 2019 ResNet50+LSTM Binary CE loss RGB D augmented temporal
    features via sparse filtering GFA-CNN [[230](#bib.bib230)] 2019 VGG16 Binary CE
    loss RGB S generalizable features via multitask and metric learning 3DSynthesis [[231](#bib.bib231)]
    2019 ResNet15 Binary CE loss RGB S trained on synthesized virtual data of print
    attacks CompactNet [[35](#bib.bib35)] 2020 VGG19 Points-to-Center triplet loss
    RGB S deep features on the learned color-liked compact space SSR-FCN [[131](#bib.bib131)]
    2020 FCN with 6 layers Binary CE loss RGB S local discriminative features from
    Self-Regional Supervision DRL-FAS [[210](#bib.bib210)] 2020 ResNet18+GRU Binary
    CE loss RGB S fused local(sub-patches) & global(entire face) features SfSNet [[232](#bib.bib232)]
    2020 6-layer CNN Binary CE loss Albedo, Depth, Reflect. S intrinsic features from
    shape-from-shading generated pseduo albedo, depth, and reflectance maps LivenesSlight [[233](#bib.bib233)]
    2020 6-layer CNN Binary CE loss RGB S lightweight model and takes less training
    time Motion- Enhancement  [[134](#bib.bib134)] 2020 VGGface+LSTM Binary CE loss
    RGB D deep temporal dynamics features with eulerian motion magnification and temporal
    attention mechanism CFSA-FAS [[234](#bib.bib234)] 2020 ResNet18 Binary CE loss
    RGB S fuse high and low frequency information with cross- frequency spatial and
    self-channel attention modules MC-FBC [[34](#bib.bib34)] 2020 VGG16 ResNet50 Binary
    CE loss RGB S fine-grained features via factorizing bilinear coding of multiple
    color channels SimpleNet [[235](#bib.bib235)] 2020 Multi-stream 5-layer CNN Binary
    CE loss RGB, OF, RP D using intermediate representations from RankPooling and
    optical flow to increase model’s robustness PatchCNN [[80](#bib.bib80)] 2020 SqueezeNet
    v1.1 Binary CE loss Triplet loss RGB S trained with multi-resolution patches and
    a multi-objective loss function FreqSpatial- TempNet  [[236](#bib.bib236)] 2020
    ResNet18 Binary CE loss RGB, HSV, Spectral D discriminative fused features of
    frequent, spatial and temporal information ViTranZFAS [[129](#bib.bib129)] 2021
    Vision Transformer Binary CE loss RGB S transfer learning from the pre-trained
    vision transformer model CIFL [[137](#bib.bib137)] 2021 ResNet18 Binary focal
    loss camear type loss RGB S camera-invariant spoofing features in the high- frequency
    domain and enhanced image FasTCo [[100](#bib.bib100)] 2021 ResNet50 MobileNetV2
    Multi-class CE loss Temporal Consistency loss Class Consistency loss RGB D temporal
    consistent features as well as temporal smooothed predictions PatchNet [[101](#bib.bib101)]
    2022 ResNet18 Asymmetric AM-Softmax loss self-supervised similarity loss RGB patch
    S fine-grained patch-type live/spoof recognition with strong patch embedding space
    regularization
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: Summary of the representative traditional deep learning based FAS
    methods with pixel-wise supervision. Most methods (in the upper part) are supervised
    with auxiliary tasks while the methods in the last eight rows are based on the
    generative models. ‘S/D’ is short for Static/Dynamic. ‘NAS’ denotes neural searched
    architecture. ‘TSM’ and ‘FPN’ denote temporal shift module and feature pyramid
    network, respectively. ‘Info-VAE’ means information maximizing variational autoencoder.
    Note that some methods also consider classification loss (e.g., binary cross entropy
    loss, triplet loss, and adversarial loss), which are not listed in the ‘Supervision’
    column.'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：基于逐像素监督的代表性传统深度学习FAS方法总结。大多数方法（在上半部分）通过辅助任务进行监督，而最后八行的方法基于生成模型。‘S/D’是静态/动态的缩写。‘NAS’表示神经网络搜索架构。‘TSM’和‘FPN’分别表示时间移位模块和特征金字塔网络。‘Info-VAE’表示信息最大化变分自编码器。请注意，有些方法还考虑了分类损失（例如，二元交叉熵损失、三元组损失和对抗损失），这些在‘监督’一栏中没有列出。
- en: '| Method | Year | Supervision | Backbone | Input | S/D | Description |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 监督 | 主干 | 输入 | S/D | 描述 |'
- en: '| Depth&Patch [[26](#bib.bib26)] | 2017 | Depth |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '| Depth&Patch [[26](#bib.bib26)] | 2017 | 深度 |'
- en: '&#124; PatchNet &#124;'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PatchNet &#124;'
- en: '&#124; DepthNet &#124;'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度网 &#124;'
- en: '|'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; YCbCr &#124;'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; YCbCr &#124;'
- en: '&#124; HSV &#124;'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HSV &#124;'
- en: '| S |'
  id: totrans-893
  prefs: []
  type: TYPE_TB
  zh: '| S |'
- en: '&#124; local patch features and holistic &#124;'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 局部补丁特征和整体 &#124;'
- en: '&#124; depth maps extracted by two-stream CNNs &#124;'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由双流CNN提取的深度图 &#124;'
- en: '|'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Auxiliary [[13](#bib.bib13)] | 2018 |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
  zh: '| Auxiliary [[13](#bib.bib13)] | 2018 |'
- en: '&#124; Depth &#124;'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度 &#124;'
- en: '&#124; rPPG spectrum &#124;'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; rPPG谱 &#124;'
- en: '| DepthNet |'
  id: totrans-900
  prefs: []
  type: TYPE_TB
  zh: '| 深度网 |'
- en: '&#124; RGB &#124;'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB &#124;'
- en: '&#124; HSV &#124;'
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HSV &#124;'
- en: '| D |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
  zh: '| D |'
- en: '&#124; local temporal features learned from CNN-RNN model &#124;'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从CNN-RNN模型中学习的局部时间特征 &#124;'
- en: '&#124; with pixel-wise depth and sequence-wise rPPG supervision &#124;'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 具有逐像素深度和序列级rPPG监督 &#124;'
- en: '|'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| BASN [[36](#bib.bib36)] | 2019 |'
  id: totrans-907
  prefs: []
  type: TYPE_TB
  zh: '| BASN [[36](#bib.bib36)] | 2019 |'
- en: '&#124; Depth &#124;'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度 &#124;'
- en: '&#124; Reflection &#124;'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反射 &#124;'
- en: '|'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DepthNet &#124;'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度网 &#124;'
- en: '&#124; Enrichment &#124;'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 丰富化 &#124;'
- en: '|'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RGB &#124;'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB &#124;'
- en: '&#124; HSV &#124;'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HSV &#124;'
- en: '| S |'
  id: totrans-916
  prefs: []
  type: TYPE_TB
  zh: '| S |'
- en: '&#124; generalizable features via bipartite auxiliary supervision &#124;'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过二分辅助监督获取通用特征 &#124;'
- en: '|'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DTN [[38](#bib.bib38)] | 2019 | BinaryMask | Tree Network |'
  id: totrans-919
  prefs: []
  type: TYPE_TB
  zh: '| DTN [[38](#bib.bib38)] | 2019 | BinaryMask | 树网络 |'
- en: '&#124; RGB &#124;'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB &#124;'
- en: '&#124; HSV &#124;'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HSV &#124;'
- en: '| S |'
  id: totrans-922
  prefs: []
  type: TYPE_TB
  zh: '| S |'
- en: '&#124; partition the spoof samples into semantic &#124;'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将欺骗样本划分为语义区域 &#124;'
- en: '&#124; sub-groups in an unsupervised fashion &#124;'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无监督方式下的子组 &#124;'
- en: '|'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PixBiS [[32](#bib.bib32)] | 2019 | BinaryMask | DenseNet161 | RGB | S |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
  zh: '| PixBiS [[32](#bib.bib32)] | 2019 | BinaryMask | DenseNet161 | RGB | S |'
- en: '&#124; deep pixel-wise binary supervision without trivial depth synthesis &#124;'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度逐像素二元监督，无需简单的深度合成 &#124;'
- en: '|'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| A-PixBiS [[142](#bib.bib142)] | 2020 | BinaryMask | DenseNet161 | RGB | S
    |'
  id: totrans-929
  prefs: []
  type: TYPE_TB
  zh: '| A-PixBiS [[142](#bib.bib142)] | 2020 | BinaryMask | DenseNet161 | RGB | S
    |'
- en: '&#124; incorporate a variant of binary cross entropy that enforces &#124;'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引入变体的二元交叉熵以强制 &#124;'
- en: '&#124; a margin in angular space for attentive pixel wise supervision &#124;'
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在角空间中设置边距以进行逐像素的注意力监督 &#124;'
- en: '|'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Auto-FAS [[143](#bib.bib143)] | 2020 | BinaryMask | NAS | RGB | S |'
  id: totrans-933
  prefs: []
  type: TYPE_TB
  zh: '| Auto-FAS [[143](#bib.bib143)] | 2020 | BinaryMask | NAS | RGB | S |'
- en: '&#124; well-suitable lightweight networks searched for mobile-level FAS &#124;'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适合于移动级FAS的轻量级网络搜索 &#124;'
- en: '|'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MRCNN [[237](#bib.bib237)] | 2020 | BinaryMask | Shallow CNN | RGB | S |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
  zh: '| MRCNN [[237](#bib.bib237)] | 2020 | BinaryMask | 浅层CNN | RGB | S |'
- en: '&#124; introducing local losses to patches, and constraints the &#124;'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 为补丁引入局部损失，并对DepthNet施加约束 &#124;'
- en: '&#124; entire face region to avoid over-emphasizing certain local areas &#124;'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 整个面部区域以避免过度强调某些局部区域 &#124;'
- en: '|'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| FCN-LSA [[238](#bib.bib238)] | 2020 | BinaryMask | DepthNet | RGB | S | high
    frequent spoof cues from lossless size adaptation module |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
  zh: '| FCN-LSA [[238](#bib.bib238)] | 2020 | BinaryMask | 深度网 | RGB | S | 从无损大小调整模块中提取高频伪造线索
    |'
- en: '| CDCN [[23](#bib.bib23)] | 2020 | Depth | DepthNet | RGB | S |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
  zh: '| CDCN [[23](#bib.bib23)] | 2020 | 深度 | 深度网 | RGB | S |'
- en: '&#124; intrinsic detailed patterns via aggregating both intensity and &#124;'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过聚合强度和深度信息获取内在细节模式 &#124;'
- en: '&#124; gradient information from stacked central difference convolutions. &#124;'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过堆叠中心差分卷积获取梯度信息 &#124;'
- en: '|'
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| FAS-SGTD [[96](#bib.bib96)] | 2020 | Depth |'
  id: totrans-945
  prefs: []
  type: TYPE_TB
  zh: '| FAS-SGTD [[96](#bib.bib96)] | 2020 | 深度 |'
- en: '&#124; DepthNet &#124;'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度网 &#124;'
- en: '&#124; STPM &#124;'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; STPM &#124;'
- en: '| RGB | D |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
  zh: '| RGB | D |'
- en: '&#124; detailed discriminative dynamics cues from stacked Residual &#124;'
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Spatial Gradient Block and Spatio-Temporal Propagation Module &#124;'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
- en: '| TS-FEN [[139](#bib.bib139)] | 2020 | Depth |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet34 &#124;'
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FCN &#124;'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; YCbCr &#124;'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HSV &#124;'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-959
  prefs: []
  type: TYPE_TB
- en: '&#124; discriminative fused features from &#124;'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; depth-stream and chroma-stream networks &#124;'
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
- en: '| SAPLC [[39](#bib.bib39)] | 2020 | TernaryMap | DepthNet |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB &#124;'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HSV &#124;'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-966
  prefs: []
  type: TYPE_TB
- en: '&#124; accurate image-level decision via spatial aggregation of &#124;'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pixel-level local classifiers even with insufficient training samples
    &#124;'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
- en: '| BCN [[24](#bib.bib24)] | 2020 |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
- en: '&#124; BinaryMask &#124;'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth &#124;'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reflection &#124;'
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
- en: '| DepthNet | RGB | S |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
- en: '&#124; intrinsic material-based patterns captured via &#124;'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; aggregating multi-level bilateral macro- and micro- information &#124;'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
- en: '| Disentangled [[97](#bib.bib97)] | 2020 |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth &#124;'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TextureMap &#124;'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
- en: '| DepthNet | RGB | S |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
- en: '&#124; liveness and content features via disentangled representation learning
    &#124;'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: '| AENet [[44](#bib.bib44)] | 2020 |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth &#124;'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reflection &#124;'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
- en: '| ResNet18 | RGB | S |'
  id: totrans-987
  prefs: []
  type: TYPE_TB
- en: '&#124; rich semantic features using Auxiliary Information, &#124;'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Embedding Network with multi-task learning framework &#124;'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DPC-Net [[40](#bib.bib40)] | 2020 | 3D Point Cloud | ResNet18 | RGB | S
    | discriminative features via fine-grained 3D Point Cloud supervision |'
  id: totrans-991
  prefs: []
  type: TYPE_TB
- en: '| PS [[211](#bib.bib211)] | 2020 |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
- en: '&#124; BinaryMask &#124;'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth &#124;'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet50 &#124;'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CDCN &#124;'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | S |'
  id: totrans-998
  prefs: []
  type: TYPE_TB
- en: '&#124; pyramid supervision guides models to learn both local &#124;'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; details and global semantics from multi-scale spatial context &#124;'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
- en: '| NAS-FAS [[37](#bib.bib37)] | 2020 |'
  id: totrans-1002
  prefs: []
  type: TYPE_TB
- en: '&#124; BinaryMask &#124;'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth &#124;'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
- en: '| NAS | RGB | D |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
- en: '&#124; leveraging cross-domain/type knowledge and static-dynamic &#124;'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; representation for central difference network search &#124;'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
- en: '| DAM [[239](#bib.bib239)] | 2021 |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth &#124;'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VGG16 &#124;'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TSM &#124;'
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | D |'
  id: totrans-1014
  prefs: []
  type: TYPE_TB
- en: '&#124; attentional fused depth and multi-scale temporal clues using a &#124;'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; two-stream network as well as a self-supervised symmetry loss &#124;'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
- en: '| Bi-FPNFAS [[144](#bib.bib144)] | 2021 |'
  id: totrans-1018
  prefs: []
  type: TYPE_TB
- en: '&#124; Fourier spectra &#124;'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EfficientNetB0 &#124;'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FPN &#124;'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | S |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
- en: '&#124; multiscale bidirectional propagated features with &#124;'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-generated frequency spectra supervision &#124;'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
- en: '| DC-CDN [[98](#bib.bib98)] | 2021 |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth &#124;'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
- en: '| CDCN | RGB | S |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
- en: '&#124; efficient feature learning on dual-cross central difference &#124;'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; network with Cross Feature Interaction Modules &#124;'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
- en: '| De-Spoof [[33](#bib.bib33)] | 2018 |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth &#124;'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BinaryMask &#124;'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FourierMap &#124;'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DSNet &#124;'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DepthNet &#124;'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HSV &#124;'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
- en: '&#124; inversely decomposing a spoof face into a spoof &#124;'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; noise and a live face, and estimating subtle &#124;'
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spoof noise with proper supervisions &#124;'
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
- en: '| Reconstruction [[240](#bib.bib240)] | 2019 |'
  id: totrans-1048
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB Input (live) &#124;'
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ZeroMap (spoof) &#124;'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
- en: '| U-Net | RGB | S | multi-level semantic features from autoencoder |'
  id: totrans-1051
  prefs: []
  type: TYPE_TB
- en: '| LGSC [[41](#bib.bib41)] | 2020 | ZeroMap (live) |'
  id: totrans-1052
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net &#124;'
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | S |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
- en: '&#124; discriminative live-spoof differences learned within a residual- &#124;'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; learning framework with the perspective of anomaly detection &#124;'
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TAE &#124;'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
- en: '[[138](#bib.bib138)] | 2020 |'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction loss &#124;'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Info-VAE+ &#124;'
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DenseNet161 &#124;'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | S |'
  id: totrans-1067
  prefs: []
  type: TYPE_TB
- en: '&#124; self-pretrained autoencoder in large-scale face recognition &#124;'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; datasets to obtain the reconstruction-error images for FAS &#124;'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: '| STDN [[99](#bib.bib99)] | 2020 |'
  id: totrans-1071
  prefs: []
  type: TYPE_TB
- en: '&#124; BinaryMask &#124;'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB Input (live) &#124;'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net &#124;'
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PatchGAN &#124;'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | S |'
  id: totrans-1077
  prefs: []
  type: TYPE_TB
- en: '&#124; disentangled spoof trace via adversarial learning and &#124;'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; hierarchical combination of patterns at multiple scales &#124;'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
- en: '| GOGen [[188](#bib.bib188)] | 2020 | RGB input |'
  id: totrans-1081
  prefs: []
  type: TYPE_TB
- en: '&#124; DepthNet &#124;'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB+one- &#124;'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; hot vector &#124;'
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1086
  prefs: []
  type: TYPE_TB
- en: '&#124; GAN-based architecture to synthesize and identify the &#124;'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spoof noise patterns from medium/sensor combinations &#124;'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: '| PhySTD [[42](#bib.bib42)] | 2021 |'
  id: totrans-1090
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth &#124;'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB Input (live) &#124;'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net &#124;'
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PatchGAN &#124;'
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Frequency &#124;'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Trace &#124;'
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1099
  prefs: []
  type: TYPE_TB
- en: '&#124; disentangling spoof faces into the spoof traces and &#124;'
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; live counterparts guided by physical properties &#124;'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
- en: '| MT-FAS [[43](#bib.bib43)] | 2021 |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
- en: '&#124; ZeroMap (live) &#124;'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LearnableMap (Spoof) &#124;'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: '| DepthNet | RGB | S |'
  id: totrans-1106
  prefs: []
  type: TYPE_TB
- en: '&#124; train a meta-teacher to generate optimal pixel-wise &#124;'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; signals for supervising the spoofing detector &#124;'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE X: Summary of the representative generalized deep learning FAS methods
    to unseen domain (domain adaptation and domain generalization). ‘MMD’ is short
    for ‘Maximum Mean Discrepancy’.'
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Year | Backbone | Loss | S/D | Description |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
- en: '|  |'
  id: totrans-1113
  prefs: []
  type: TYPE_TB
- en: '&#124; OR-DA &#124;'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
- en: '[[78](#bib.bib78)] | 2018 |'
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AlexNet &#124;'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MMD loss &#124;'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
- en: '&#124; learned classifier for target domain, and embedding space &#124;'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with similar distribution for source and target domains &#124;'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1124
  prefs: []
  type: TYPE_TB
- en: '&#124; DTCNN &#124;'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: '[[241](#bib.bib241)] | 2019 |'
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AlexNet &#124;'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MMD loss &#124;'
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1131
  prefs: []
  type: TYPE_TB
- en: '&#124; domain invariant features using a few &#124;'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; labeled samples from the target domain &#124;'
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
- en: '&#124; Adversarial &#124;'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
- en: '[[146](#bib.bib146)] | 2019 |'
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Triplet loss &#124;'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adversarial loss &#124;'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1142
  prefs: []
  type: TYPE_TB
- en: '&#124; learn a shared embedding space by both source and &#124;'
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; target domain models via adversarial domain adaptation &#124;'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1146
  prefs: []
  type: TYPE_TB
- en: '&#124; ML-MMD &#124;'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
- en: '[[149](#bib.bib149)] | 2019 |'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-scale &#124;'
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FCN &#124;'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CE loss &#124;'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MMD loss &#124;'
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1154
  prefs: []
  type: TYPE_TB
- en: '&#124; adapt in both representation and classifier &#124;'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; layers to bridge for the domain discrepancy &#124;'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1158
  prefs: []
  type: TYPE_TB
- en: '&#124; OCA-FAS &#124;'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
- en: '[[54](#bib.bib54)] | 2020 |'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DepthNet &#124;'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pixel-wise binary loss &#124;'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S &#124;'
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; train a meta-learner with loss function search on &#124;'
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; one-class adaptation FAS tasks with only live samples &#124;'
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Domain &#124;'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adaptation &#124;'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DR-UDA &#124;'
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
- en: '[[147](#bib.bib147)] | 2020 |'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Center&Triplet loss &#124;'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adversarial loss &#124;'
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Disentangled loss &#124;'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1182
  prefs: []
  type: TYPE_TB
- en: '&#124; disentangles the features irrelevant to specific &#124;'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; domains, and learn a shared embedding &#124;'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; space by both source and target domains &#124;'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1187
  prefs: []
  type: TYPE_TB
- en: '&#124; DGP &#124;'
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
- en: '[[150](#bib.bib150)] | 2020 |'
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DenseNet161 &#124;'
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature divergence measure &#124;'
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BinaryMask &#124;'
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1194
  prefs: []
  type: TYPE_TB
- en: '&#124; prune the filters specific to the source dataset &#124;'
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for performance improvement on target dataset &#124;'
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1198
  prefs: []
  type: TYPE_TB
- en: '&#124; Distillation &#124;'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
- en: '[[151](#bib.bib151)] | 2020 |'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AlexNet &#124;'
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MMD loss &#124;'
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paired Similarity &#124;'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1206
  prefs: []
  type: TYPE_TB
- en: '&#124; spoofing-specific information captured by distilled &#124;'
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; deep network on the application-specific domain &#124;'
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1210
  prefs: []
  type: TYPE_TB
- en: '&#124; S-CNN &#124;'
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +PL+TC &#124;'
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
- en: '[[148](#bib.bib148)] | 2021 |'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CE Loss in labeled &#124;'
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and unlabeled sets &#124;'
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
- en: '| D |'
  id: totrans-1218
  prefs: []
  type: TYPE_TB
- en: '&#124; semi-supervised learning framework with only a few &#124;'
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; labeled training data, and progressively adopt the &#124;'
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; unlabeled data with reliable pseudo labels. &#124;'
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1223
  prefs: []
  type: TYPE_TB
- en: '&#124; USDAN &#124;'
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
- en: '[[103](#bib.bib103)] | 2021 |'
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adaptive binary CE loss &#124;'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Entropy loss &#124;'
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adversarial loss &#124;'
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1231
  prefs: []
  type: TYPE_TB
- en: '&#124; design different distribution alignment operations to &#124;'
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; enhance generalization for un- & semi-supervised &#124;'
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; domain adaptation to address cross-scenario problem &#124;'
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1236
  prefs: []
  type: TYPE_TB
- en: '&#124; MADDG &#124;'
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
- en: '[[48](#bib.bib48)] | 2019 |'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DepthNet &#124;'
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE & Depth loss &#124;'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-adversarial loss &#124;'
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dual-force Triplet loss &#124;'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1244
  prefs: []
  type: TYPE_TB
- en: '&#124; leverage the large variability present in FR datasets &#124;'
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to induce invariance to factors that cause domain-shift &#124;'
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1248
  prefs: []
  type: TYPE_TB
- en: '&#124; PAD-GAN &#124;'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
- en: '[[50](#bib.bib50)] | 2020 |'
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE & GAN loss &#124;'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction loss &#124;'
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1255
  prefs: []
  type: TYPE_TB
- en: '&#124; disentangled and domain-independent features rather &#124;'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; than subject discriminative and domain related features &#124;'
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1259
  prefs: []
  type: TYPE_TB
- en: '&#124; SSDG &#124;'
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
- en: '[[51](#bib.bib51)] | 2020 |'
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Single-Side adversarial loss &#124;'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Asymmetric Triplet loss &#124;'
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1267
  prefs: []
  type: TYPE_TB
- en: '&#124; learn a generalized space where the feature distribution &#124;'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of real faces is compact while that of fake ones is disper- &#124;'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; sed among domains but compact within each domain &#124;'
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1272
  prefs: []
  type: TYPE_TB
- en: '&#124; RF-Meta &#124;'
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
- en: '[[49](#bib.bib49)] | 2020 |'
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DepthNet &#124;'
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth loss &#124;'
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1279
  prefs: []
  type: TYPE_TB
- en: '&#124; meta-learned generalized features across multiple &#124;'
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; source domains with auxiliary regularization &#124;'
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1283
  prefs: []
  type: TYPE_TB
- en: '&#124; CCDD &#124;'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
- en: '[[242](#bib.bib242)] | 2020 |'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet50 &#124;'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +LSTM &#124;'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Class-conditional loss &#124;'
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
- en: '| D |'
  id: totrans-1291
  prefs: []
  type: TYPE_TB
- en: '&#124; learn discriminative but domain-robust features with &#124;'
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; class-conditional domain discriminator module and GRL &#124;'
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1295
  prefs: []
  type: TYPE_TB
- en: '&#124; DASN &#124;'
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
- en: '[[212](#bib.bib212)] | 2021 |'
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE & Spoof- &#124;'
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; irrelevant factor loss &#124;'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1302
  prefs: []
  type: TYPE_TB
- en: '&#124; adopt doubly adversarial learning to suppress the &#124;'
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spoof-irrelevant factors, and intensify spoof factors. &#124;'
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Domain &#124;'
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Generalization &#124;'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SDA &#124;'
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
- en: '[[154](#bib.bib154)] | 2021 |'
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DepthNet &#124;'
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE & Depth loss &#124;'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction loss &#124;'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Orthogonality regularization &#124;'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1317
  prefs: []
  type: TYPE_TB
- en: '&#124; use meta-learning based adaptor learning for better &#124;'
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adaptor initialization, and an unsupervised adaptor &#124;'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; loss for appropriate adaptor optimization &#124;'
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1322
  prefs: []
  type: TYPE_TB
- en: '&#124; D²AM &#124;'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
- en: '[[152](#bib.bib152)] | 2021 |'
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DepthNet &#124;'
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth loss &#124;'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MMD loss &#124;'
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1330
  prefs: []
  type: TYPE_TB
- en: '&#124; iteratively divide mixture domains via discriminative &#124;'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; domain representation and train generalizable models &#124;'
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with meta-learning without using domain labels &#124;'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1335
  prefs: []
  type: TYPE_TB
- en: '&#124; DRDG [liu2021dual] &#124;'
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  id: totrans-1337
  prefs: []
  type: TYPE_TB
- en: '&#124; DepthNet &#124;'
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE & Depth loss &#124;'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; domain loss &#124;'
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1342
  prefs: []
  type: TYPE_TB
- en: '&#124; iteratively reweight the relative importance between samples &#124;'
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and features to extract domain-irrelevant features &#124;'
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1346
  prefs: []
  type: TYPE_TB
- en: '&#124; ANRL [[153](#bib.bib153)] &#124;'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  id: totrans-1348
  prefs: []
  type: TYPE_TB
- en: '&#124; DepthNet &#124;'
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE & Depth loss &#124;'
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; inter-domain compatible loss &#124;'
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; inter-class separable loss &#124;'
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1354
  prefs: []
  type: TYPE_TB
- en: '&#124; adaptively select feature normalization methods to learn &#124;'
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; domain-agnostic and discriminative representation &#124;'
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1358
  prefs: []
  type: TYPE_TB
- en: '&#124; FGHV [[102](#bib.bib102)] &#124;'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
- en: '| 2022 |'
  id: totrans-1360
  prefs: []
  type: TYPE_TB
- en: '&#124; DepthNet &#124;'
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Variance, relative correlation &#124;'
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; distribution discrimination constraints &#124;'
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1365
  prefs: []
  type: TYPE_TB
- en: '&#124; feature generation networks generate hypotheses of real &#124;'
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; faces and known attacks, and two hypothesis verification &#124;'
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modules are applied to judge real/generative distributions &#124;'
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1370
  prefs: []
  type: TYPE_TB
- en: '&#124; SSAN [[102](#bib.bib102)] &#124;'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
- en: '| 2022 |'
  id: totrans-1372
  prefs: []
  type: TYPE_TB
- en: '&#124; DepthNet &#124;'
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet &#124;'
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; domain adversarial loss &#124;'
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; contrastive loss &#124;'
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1379
  prefs: []
  type: TYPE_TB
- en: '&#124; extract and reassemble different content and style features &#124;'
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for a stylized feature space, and emphasize liveness-related &#124;'
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; style information while suppress the domain-specific one &#124;'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XI: Summary of the generalized deep learning FAS methods to unknown attack
    types (zero/few-shot learning and anomaly detection). ‘OCSVM’, ‘MD’, ‘GMM’, and
    ‘OCCL’ are short for ‘One-Class Support Vector Machine’, ‘Mahalanobis-distance’,
    ‘Gaussian Mixture Model’, and ‘One-Class Contrastive Loss’, respectively.'
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Year | Backbone | Loss | Input | Description |'
  id: totrans-1385
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-1386
  prefs: []
  type: TYPE_TB
- en: '|  |'
  id: totrans-1387
  prefs: []
  type: TYPE_TB
- en: '&#124; DTN &#124;'
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
- en: '[[38](#bib.bib38)] | 2019 |'
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Deep Tree &#124;'
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Network &#124;'
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pixel-wise binary loss &#124;'
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Unsupervised Tree loss &#124;'
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HSV &#124;'
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adaptively routing the attacks to the most similar &#124;'
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spoof cluster, and makes the binary decision &#124;'
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Zero/Few- &#124;'
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Shot &#124;'
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AIM-FAS &#124;'
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
- en: '[[53](#bib.bib53)] | 2020 |'
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DepthNet &#124;'
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth loss &#124;'
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Contrastive Depth loss &#124;'
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB |'
  id: totrans-1413
  prefs: []
  type: TYPE_TB
- en: '&#124; adaptive inner-updated meta features generalized &#124;'
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to unseen spoof types from predefined PAs &#124;'
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1417
  prefs: []
  type: TYPE_TB
- en: '&#124; CM-PAD &#124;'
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
- en: '[[155](#bib.bib155)] | 2021 |'
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DepthNet &#124;'
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet &#124;'
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth loss &#124;'
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradient alignment &#124;'
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB |'
  id: totrans-1426
  prefs: []
  type: TYPE_TB
- en: '&#124; continual meta-learning PAD solution that &#124;'
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; can be trained on unseen attack scenarios &#124;'
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; catastrophic seen attack forgetting &#124;'
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1431
  prefs: []
  type: TYPE_TB
- en: '&#124; AE+LBP &#124;'
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
- en: '[[157](#bib.bib157)] | 2018 |'
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AutoEncoder &#124;'
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction loss &#124;'
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB |'
  id: totrans-1437
  prefs: []
  type: TYPE_TB
- en: '&#124; embedding features (cascaded with LBP) from outlier &#124;'
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; detection based neural network autoencoder &#124;'
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1441
  prefs: []
  type: TYPE_TB
- en: '&#124; Anomaly &#124;'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
- en: '[[159](#bib.bib159)] | 2019 |'
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet50 &#124;'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Triplet focal loss &#124;'
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Metric-Softmax loss &#124;'
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; deep anomaly detection via introducing a &#124;'
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; few-shot posteriori probability estimation &#124;'
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1454
  prefs: []
  type: TYPE_TB
- en: '&#124; Anomaly2 &#124;'
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
- en: '[[243](#bib.bib243)] | 2019 |'
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GoogLeNet &#124;'
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet50 &#124;'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MD &#124;'
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; subject specific anomaly detector is trained on &#124;'
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; genuine accesses only using one-class classifiers &#124;'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1467
  prefs: []
  type: TYPE_TB
- en: '&#124; Hypersphere &#124;'
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
- en: '[[161](#bib.bib161)] | 2020 |'
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hypersphere loss &#124;'
  id: totrans-1472
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HSV &#124;'
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; deep anomaly detection supervised by hypersphere &#124;'
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; loss, and detects PAs directly on learned feature space &#124;'
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Anomaly- &#124;'
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detection &#124;'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ensemble- &#124;'
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Anomaly &#124;'
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
- en: '[[244](#bib.bib244)] | 2020 |'
  id: totrans-1486
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GoogLeNet &#124;'
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet50 &#124;'
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GMM &#124;'
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (not end-to-end) &#124;'
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; patches &#124;'
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ensemble of one-class classifiers from different &#124;'
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; facial regions, CNNs, and anomaly detectors &#124;'
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1499
  prefs: []
  type: TYPE_TB
- en: '&#124; MCCNN &#124;'
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
- en: '[[160](#bib.bib160)] | 2020 |'
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LightCNN &#124;'
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Contrastive loss &#124;'
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Grayscale, &#124;'
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; IR, Depth, &#124;'
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Thermal &#124;'
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; learn a compact embedding for bonafide while &#124;'
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; being far from the representation of PAs via &#124;'
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; OCCL, and cascaded with a one-class GMM &#124;'
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1515
  prefs: []
  type: TYPE_TB
- en: '&#124; End2End- &#124;'
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Anomaly &#124;'
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
- en: '[[158](#bib.bib158)] | 2020 |'
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VGG-Face &#124;'
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pairwise confusion &#124;'
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; both classifier and representations are learned &#124;'
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; end-to-end with pseudo negative class &#124;'
  id: totrans-1527
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-1529
  prefs: []
  type: TYPE_TB
- en: '&#124; ClientAnomaly &#124;'
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
- en: '[[245](#bib.bib245)] | 2020 |'
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet50 &#124;'
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GoogLeNet &#124;'
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VGG16 &#124;'
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; OCSVM &#124;'
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GMM &#124;'
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MD &#124;'
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB &#124;'
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; client-specific knowledge are leveraged for &#124;'
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; anomaly-based spoofing detectors as well as &#124;'
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; determination thresholds &#124;'
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1545
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XII: Summary of the representative deep learning FAS methods with specialized
    sensor/hardware inputs. ‘S/D’, ‘SD’, ‘AD’, ‘FM’, ‘APD’, ‘LFC’, ’DP’ and ‘DOLP’
    are short for ‘Static/Dynamic’, ‘Square Disparity’, ‘Absolute Disparity’, ‘Feature
    Multiplication’, ‘Approximate Disparity’, ‘Light Field Camera’, ‘Dual Pixel’ and
    ‘Degree of Linear Polarization’, respectively.'
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Backbone | Loss | Input | S/D | Description |'
  id: totrans-1547
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Thermal- &#124;'
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FaceCNN &#124;'
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
- en: '[[167](#bib.bib167)] | 2019 |'
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AlexNet &#124;'
  id: totrans-1552
  prefs: []
  type: TYPE_NORMAL
- en: '| Regression loss |'
  id: totrans-1553
  prefs: []
  type: TYPE_TB
- en: '&#124; Thermal infrared &#124;'
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; face image &#124;'
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1556
  prefs: []
  type: TYPE_TB
- en: '&#124; temperature related features based on the fact that &#124;'
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; real face temperature is 36$\sim$37 degrees on average &#124;'
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SLNet &#124;'
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
- en: '[[162](#bib.bib162)] | 2019 |'
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 17-layer CNN &#124;'
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
- en: '| Binary CE loss |'
  id: totrans-1564
  prefs: []
  type: TYPE_TB
- en: '&#124; Stereo (left&right) &#124;'
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; face images &#124;'
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1567
  prefs: []
  type: TYPE_TB
- en: '&#124; disparities between deep features are &#124;'
  id: totrans-1568
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; learned using SD, AD, FM, and APD operations &#124;'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Aurora &#124;'
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Guard &#124;'
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
- en: '[[163](#bib.bib163)] | 2019 | U-Net |'
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth regression &#124;'
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Light Regression &#124;'
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Casted face with dynamic &#124;'
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; changing light specified by &#124;'
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; random light CAPTCHA &#124;'
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
- en: '| D |'
  id: totrans-1582
  prefs: []
  type: TYPE_TB
- en: '&#124; based on the normal cues extracted from &#124;'
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the light reflection, multi-tasck CNN recovers &#124;'
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; both subjects’ depth maps and light CAPTCHA &#124;'
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
- en: '| LFC [[87](#bib.bib87)] | 2019 | AlexNet | Binary CE loss |'
  id: totrans-1587
  prefs: []
  type: TYPE_TB
- en: '&#124; Ray difference/microlens &#124;'
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images from LFC &#124;'
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1590
  prefs: []
  type: TYPE_TB
- en: '&#124; meaningful features extracted from single-shot &#124;'
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LFC images with rich depth information of objects &#124;'
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
- en: '| PAAS [[47](#bib.bib47)] | 2020 | MobileNetV2 |'
  id: totrans-1594
  prefs: []
  type: TYPE_TB
- en: '&#124; Contrastive loss &#124;'
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SVM &#124;'
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Four-directional polarized &#124;'
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; face image &#124;'
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1600
  prefs: []
  type: TYPE_TB
- en: '&#124; learned discriminative and robust features from DOLP &#124;'
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; as polarization reveals the intrinsic attributes &#124;'
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Face- &#124;'
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Revelio &#124;'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
- en: '[[170](#bib.bib170)] | 2020 |'
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Siamese- &#124;'
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AlexNet &#124;'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
- en: '| L1 distance |'
  id: totrans-1610
  prefs: []
  type: TYPE_TB
- en: '&#124; Four flash lights displayed &#124;'
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; on four quarters of a screen &#124;'
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
- en: '| D |'
  id: totrans-1613
  prefs: []
  type: TYPE_TB
- en: '&#124; varying illumination enables the recovery &#124;'
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of the face surface normals via photometric stereo &#124;'
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1617
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SpecDiff &#124;'
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
- en: '[[171](#bib.bib171)] | 2020 |'
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet4 &#124;'
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
- en: '| Binary CE loss |'
  id: totrans-1621
  prefs: []
  type: TYPE_TB
- en: '&#124; Concatenated face images &#124;'
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; w/ and w/o flash &#124;'
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1624
  prefs: []
  type: TYPE_TB
- en: '&#124; a novel descriptor based on specular and diffuse &#124;'
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reflections, with a flash-based deep FAS baseline &#124;'
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MC- &#124;'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PixBiS &#124;'
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
- en: '[[55](#bib.bib55)] | 2020 |'
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DenseNet161 &#124;'
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
- en: '| Binary mask loss | SWIR images differences | S |'
  id: totrans-1633
  prefs: []
  type: TYPE_TB
- en: '&#124; discriminative features for Impersonation attacks as &#124;'
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; water is very absorbing in some SWIR wavelengths &#124;'
  id: totrans-1635
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Thermal- &#124;'
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ization &#124;'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
- en: '[[246](#bib.bib246)] | 2020 |'
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; YOLO V3+ &#124;'
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GoogLeNet &#124;'
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
- en: '| Binary CE loss |'
  id: totrans-1643
  prefs: []
  type: TYPE_TB
- en: '&#124; Thermal infrared &#124;'
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; face image &#124;'
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1646
  prefs: []
  type: TYPE_TB
- en: '&#124; learned specific physical features &#124;'
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; from Thermal infrared imaging of PAs &#124;'
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DP Bin- &#124;'
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -Cls-Net &#124;'
  id: totrans-1652
  prefs: []
  type: TYPE_NORMAL
- en: '[[169](#bib.bib169)] | 2021 |'
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Shallow U-Net &#124;'
  id: totrans-1654
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + Xception &#124;'
  id: totrans-1655
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transformation consistency &#124;'
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Relative disparity loss &#124;'
  id: totrans-1658
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DP image pair &#124;'
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
- en: '| S |'
  id: totrans-1662
  prefs: []
  type: TYPE_TB
- en: '&#124; reconstructed depth based on the DP pair with &#124;'
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-supervised loss for planar attack detection &#124;'
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XIII: Summary of the multi-modal deep learning FAS methods. ‘MFEM’, ‘SPM’,
    ‘LFV’ and ‘MLP’ are short for ‘Modal Feature Erasing Module’, ‘Selective Modal
    Pipeline’, ‘Limited Frame Vote’ and ‘Multilayer Perceptron’, respectively.'
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Backbone | Loss | Input | Fusion | Description |'
  id: totrans-1667
  prefs: []
  type: TYPE_TB
- en: '| FaceBagNet [[174](#bib.bib174)] | 2019 |'
  id: totrans-1668
  prefs: []
  type: TYPE_TB
- en: '&#124; Multi-stream &#124;'
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN &#124;'
  id: totrans-1670
  prefs: []
  type: TYPE_NORMAL
- en: '| Binary CE loss |'
  id: totrans-1671
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, Depth, NIR &#124;'
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; face patches &#124;'
  id: totrans-1673
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1675
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spoof-specific features from patch CNN, and &#124;'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MFEM to prevent overfitting and better fusion &#124;'
  id: totrans-1679
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
- en: '| FeatherNets [[178](#bib.bib178)] | 2019 |'
  id: totrans-1681
  prefs: []
  type: TYPE_TB
- en: '&#124; Ensemble- &#124;'
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FeatherNet &#124;'
  id: totrans-1683
  prefs: []
  type: TYPE_NORMAL
- en: '| Binary CE loss |'
  id: totrans-1684
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth, NIR &#124;'
  id: totrans-1685
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Decision- &#124;'
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; single compact FeatherNet trained by depth image, &#124;'
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; then fused with “ensemble + cascade” structure &#124;'
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
- en: '| Attention [[247](#bib.bib247)] | 2019 | ResNet18 |'
  id: totrans-1693
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Center loss &#124;'
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, Depth, NIR &#124;'
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; using channel and spatial attention module &#124;'
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; to refine the multimodal features &#124;'
  id: totrans-1703
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1704
  prefs: []
  type: TYPE_NORMAL
- en: '| mmfCNN [[173](#bib.bib173)] | 2019 | ResNet34 |'
  id: totrans-1705
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1706
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary Center Loss &#124;'
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, NIR, Depth, &#124;'
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HSV, YCbCr &#124;'
  id: totrans-1710
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1711
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1713
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; fuses multi-level features among modalities in &#124;'
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; a unified framework with weight-adaptation &#124;'
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1717
  prefs: []
  type: TYPE_NORMAL
- en: '| MM-FAS [[172](#bib.bib172)] | 2019 | ResNet18/50 |'
  id: totrans-1718
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, NIR, Depth &#124;'
  id: totrans-1721
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; leverages multimodal data and aggregates intra- &#124;'
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; channel features at multiple network layers &#124;'
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1728
  prefs: []
  type: TYPE_NORMAL
- en: '| AEs+MLP [[176](#bib.bib176)] | 2019 |'
  id: totrans-1729
  prefs: []
  type: TYPE_TB
- en: '&#124; Autoencoder &#124;'
  id: totrans-1730
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MLP &#124;'
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1732
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstruction loss &#124;'
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Grayscale-Depth- &#124;'
  id: totrans-1736
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Infrared composition &#124;'
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input- &#124;'
  id: totrans-1739
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1741
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; trasfer learning within facial patches from the &#124;'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; facial RGB appearance to multi-channel modalities &#124;'
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
- en: '| SD-Net [[28](#bib.bib28)] | 2019 | ResNet18 |'
  id: totrans-1745
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1746
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1747
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, NIR, &#124;'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth &#124;'
  id: totrans-1749
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1750
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1751
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multimodal fusion via feature re-weighting to &#124;'
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; select more informative channels for modalities &#124;'
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1756
  prefs: []
  type: TYPE_NORMAL
- en: '| Dual-modal [[248](#bib.bib248)] | 2019 | MoblienetV3 |'
  id: totrans-1757
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, IR &#124;'
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1761
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; light-weight networks to extract and merge &#124;'
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; embedding features from NIR-VIS image pairs &#124;'
  id: totrans-1766
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1767
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Parallel-CNN &#124;'
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
- en: '[[249](#bib.bib249)] | 2020 |'
  id: totrans-1770
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Attentional- &#124;'
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN &#124;'
  id: totrans-1772
  prefs: []
  type: TYPE_NORMAL
- en: '| Binary CE loss |'
  id: totrans-1773
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth, NIR &#124;'
  id: totrans-1774
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; fused deep depth and IR features from paralleled &#124;'
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; attentional CNN with spatial pyramid pooling &#124;'
  id: totrans-1780
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-Channel &#124;'
  id: totrans-1783
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detector &#124;'
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
- en: '[[250](#bib.bib250)] | 2020 |'
  id: totrans-1785
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RetinaNet &#124;'
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (FPN+ResNet18) &#124;'
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Landmark regression &#124;'
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Focal loss &#124;'
  id: totrans-1790
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Grayscale-Depth- &#124;'
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Infrared composition &#124;'
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1794
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input- &#124;'
  id: totrans-1795
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1796
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; learned joint face detection-based and PAD-based &#124;'
  id: totrans-1798
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; representation from fused 3 channel images &#124;'
  id: totrans-1799
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1800
  prefs: []
  type: TYPE_NORMAL
- en: '| PSMM-Net [[91](#bib.bib91)] | 2020 | ResNet18 |'
  id: totrans-1801
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for each stream &#124;'
  id: totrans-1803
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, Depth, NIR &#124;'
  id: totrans-1805
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1809
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; static-dynamic fusion mechanism with part- &#124;'
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ially shared fusion strategy is proposed &#124;'
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
- en: '| PipeNet [[251](#bib.bib251)] | 2020 | SENet154 |'
  id: totrans-1813
  prefs: []
  type: TYPE_TB
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1814
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, Depth, NIR &#124;'
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; face patches &#124;'
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1819
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1821
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SMP takes full advantage of multi-modal data. &#124;'
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LFV ensures stable video-level prediction &#124;'
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
- en: '| MM-CDCN [[27](#bib.bib27)] | 2020 | CDCN |'
  id: totrans-1825
  prefs: []
  type: TYPE_TB
- en: '&#124; Pixel-wise binary &#124;'
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; loss, Contrastive &#124;'
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; depth loss &#124;'
  id: totrans-1828
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1829
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, Depth, NIR &#124;'
  id: totrans-1830
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature& &#124;'
  id: totrans-1832
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Decision &#124;'
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1835
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; capture central-difference-based intrinsic &#124;'
  id: totrans-1836
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spoofing patterns among three modalities &#124;'
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1838
  prefs: []
  type: TYPE_NORMAL
- en: '| HGCNN [[252](#bib.bib252)] | 2020 |'
  id: totrans-1839
  prefs: []
  type: TYPE_TB
- en: '&#124; Hypergraph- &#124;'
  id: totrans-1840
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN + MLP &#124;'
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, Depth &#124;'
  id: totrans-1845
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1846
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1848
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; auxiliary depth fused with texture in the feature &#124;'
  id: totrans-1850
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; domain from hypergraph convolution &#124;'
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
- en: '| MCT-GAN [[179](#bib.bib179)] | 2020 |'
  id: totrans-1853
  prefs: []
  type: TYPE_TB
- en: '&#124; CycleGAN &#124;'
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet50 &#124;'
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN loss &#124;'
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1858
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, NIR &#124;'
  id: totrans-1860
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input- &#124;'
  id: totrans-1862
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1864
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; generate NIR counterpart for VIS inputs &#124;'
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; via GAN, and learn fusing features &#124;'
  id: totrans-1866
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1867
  prefs: []
  type: TYPE_NORMAL
- en: '| D-M-Net [[177](#bib.bib177)] | 2021 |'
  id: totrans-1868
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNeXt &#124;'
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1870
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1871
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multi-preprocessed &#124;'
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth, RGB-NIR &#124;'
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; composition &#124;'
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input& &#124;'
  id: totrans-1877
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; two-stage cascade architecture to fuse depth features &#124;'
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with multi-scale RGB-NIR composite features &#124;'
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1883
  prefs: []
  type: TYPE_NORMAL
- en: '| CMFL [[175](#bib.bib175)] | 2021 |'
  id: totrans-1884
  prefs: []
  type: TYPE_TB
- en: '&#124; DenseNet161 &#124;'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1886
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cross modal focal loss &#124;'
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1889
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, Depth &#124;'
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1893
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1894
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modulate the loss contribution and comple- &#124;'
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mentary information from the two modalities &#124;'
  id: totrans-1896
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
- en: '| MA-Net [[180](#bib.bib180)] | 2021 |'
  id: totrans-1898
  prefs: []
  type: TYPE_TB
- en: '&#124; CycleGAN &#124;'
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet18 &#124;'
  id: totrans-1900
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN loss &#124;'
  id: totrans-1902
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1903
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, NIR &#124;'
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1907
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1908
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; translate the visible inputs into NIR &#124;'
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images, and then extract VIS-NIR features &#124;'
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FlexModal &#124;'
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -FAS [[187](#bib.bib187)] &#124;'
  id: totrans-1915
  prefs: []
  type: TYPE_NORMAL
- en: '| 2022 |'
  id: totrans-1916
  prefs: []
  type: TYPE_TB
- en: '&#124; CDCN &#124;'
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet50, ViT &#124;'
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Binary CE loss &#124;'
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pixel-wise binary loss &#124;'
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, Depth, NIR &#124;'
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1924
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature- &#124;'
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level &#124;'
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cross-attention fusion to efficiently mine &#124;'
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cross-modal clues for flexible-modal deployment &#124;'
  id: totrans-1929
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
