- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:36:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:36:49'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2310.00633] A Survey of Robustness and Safety of 2D and 3D Deep Learning Models
    Against Adversarial Attacks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2310.00633] 2D和3D深度学习模型对抗攻击的鲁棒性与安全性调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.00633](https://ar5iv.labs.arxiv.org/html/2310.00633)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.00633](https://ar5iv.labs.arxiv.org/html/2310.00633)
- en: A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2D和3D深度学习模型对抗攻击的鲁棒性与安全性调查
- en: Yanjie Li [yanjie.li@connect.polyu.hk](mailto:yanjie.li@connect.polyu.hk) [0000-0001-8859-8331](https://orcid.org/0000-0001-8859-8331
    "ORCID identifier") The Hong Kong Polytechnic University11, Yuk Choi Road, Hung
    Hom, KLNHong Kong ,  Bin Xie [xiebin.sc@gmail.com](mailto:xiebin.sc@gmail.com)
    [0000-0001-5118-3570](https://orcid.org/0000-0001-5118-3570 "ORCID identifier")
    The Hong Kong Polytechnic University11, Yuk Choi Road, Hung Hom, KLNHong Kong
    ,  Songtao Guo [guosongtao@cqu.edu.cn](mailto:guosongtao@cqu.edu.cn) Chongqing
    UniversityChongqingChina ,  Yuanyuan Yang [yuanyuan.yang@stonybrook.edu](mailto:yuanyuan.yang@stonybrook.edu)
    [0000-0001-7296-9222](https://orcid.org/0000-0001-7296-9222 "ORCID identifier")
    Stony Brook UniversityStony Brook, NYUSA  and  Bin Xiao [csbxiao@comp.polyu.edu.hk](mailto:csbxiao@comp.polyu.edu.hk)
    [0000-0003-4223-8220](https://orcid.org/0000-0003-4223-8220 "ORCID identifier")
    The Hong Kong Polytechnic University11, Yuk Choi Road, Hung Hom, KLNHong Kong
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 李艳杰 [yanjie.li@connect.polyu.hk](mailto:yanjie.li@connect.polyu.hk) [0000-0001-8859-8331](https://orcid.org/0000-0001-8859-8331
    "ORCID identifier") 香港理工大学11, Yuk Choi Road, Hung Hom, KLNHong Kong ，谢彬 [xiebin.sc@gmail.com](mailto:xiebin.sc@gmail.com)
    [0000-0001-5118-3570](https://orcid.org/0000-0001-5118-3570 "ORCID identifier")
    香港理工大学11, Yuk Choi Road, Hung Hom, KLNHong Kong ，郭松涛 [guosongtao@cqu.edu.cn](mailto:guosongtao@cqu.edu.cn)
    重庆大学重庆中国 ，杨媛媛 [yuanyuan.yang@stonybrook.edu](mailto:yuanyuan.yang@stonybrook.edu)
    [0000-0001-7296-9222](https://orcid.org/0000-0001-7296-9222 "ORCID identifier")
    纽约州立大学石溪分校Stony Brook, NYUSA 以及 谢斌 [csbxiao@comp.polyu.edu.hk](mailto:csbxiao@comp.polyu.edu.hk)
    [0000-0003-4223-8220](https://orcid.org/0000-0003-4223-8220 "ORCID identifier")
    香港理工大学11, Yuk Choi Road, Hung Hom, KLNHong Kong
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Benefiting from the rapid development of deep learning, 2D and 3D computer vision
    applications are deployed in many safe-critical systems, such as autopilot and
    identity authentication. However, deep learning models are not trustworthy enough
    because of their limited robustness against adversarial attacks. The physically
    realizable adversarial attacks further pose fatal threats to the application and
    human safety. Lots of papers have emerged to investigate the robustness and safety
    of deep learning models against adversarial attacks. To lead to trustworthy AI,
    we first construct a general threat model from different perspectives and then
    comprehensively review the latest progress of both 2D and 3D adversarial attacks.
    We extend the concept of adversarial examples beyond imperceptive perturbations
    and collate over 170 papers to give an overview of deep learning model robustness
    against various adversarial attacks. To the best of our knowledge, we are the
    first to systematically investigate adversarial attacks for 3D models, a flourishing
    field applied to many real-world applications. In addition, we examine physical
    adversarial attacks that lead to safety violations. Last but not least, we summarize
    present popular topics, give insights on challenges, and shed light on future
    research on trustworthy AI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于深度学习的快速发展，2D和3D计算机视觉应用被部署在许多安全关键系统中，如自动驾驶和身份认证。然而，由于对抗攻击的鲁棒性有限，深度学习模型尚不足以信赖。实际可实现的对抗攻击进一步对应用和人身安全构成致命威胁。许多论文已经出现，以研究深度学习模型在对抗攻击下的鲁棒性和安全性。为了引领可信的人工智能，我们首先从不同角度构建了一个通用威胁模型，然后全面回顾了2D和3D对抗攻击的最新进展。我们将对抗样本的概念扩展到超越不可察觉的扰动，并整理了超过170篇论文，概述了深度学习模型对各种对抗攻击的鲁棒性。据我们所知，我们是首个系统地研究3D模型对抗攻击的团队，这一领域正蓬勃发展并应用于许多现实世界应用中。此外，我们还考察了导致安全违规的物理对抗攻击。最后但同样重要的是，我们总结了当前的热门话题，提供了挑战的见解，并为可信人工智能的未来研究提供了启示。
- en: 'Deep learning; 3D computer vision; Adversarial attack; Robustness;^†^†submissionid:
    CSUR-2022-0640^†^†ccs: Security and privacy^†^†ccs: Computing methodologies Machine
    learning^†^†ccs: Computing methodologies Computer vision'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习; 3D计算机视觉; 对抗攻击; 鲁棒性;^†^†提交ID: CSUR-2022-0640^†^†分类: 安全与隐私^†^†分类: 计算方法 机器学习^†^†分类:
    计算方法 计算机视觉'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: The significant strides of deep learning (DL) algorithms have driven considerable
    technological progress in computer vision (CV) tasks, which are widely deployed
    in various safety-critical and mission-critical systems like identity authentication
    and self-driving vehicles. These applications depend on the assumption that these
    deep learning models are trustworthy and robust against small perturbations, which
    means these models can produce consistent predictions when noise exists. However,
    studies have shown that deep-learning models are vulnerable to adversarial examples
    (AEs), which makes the deep-learning models produce false predictions by crafting
    elaborate imperceptive or semantic-preserving perturbations. To realize trustworthy
    AI, continuous efforts have been spent on improving the model’s robustness and
    safety against adversarial attacks and finding these models’ robustness upper
    bound by constructing stronger adversarial attacks. This is a relentless race
    about adversarial attacks and defenses. This paper thoroughly surveys the latest
    progress of this adversarial attack and defense competition from the adversary’s
    perspective.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）算法的显著进展推动了计算机视觉（CV）任务的技术进步，这些任务广泛应用于身份认证和自动驾驶等各种安全关键和任务关键系统。这些应用依赖于假设这些深度学习模型是可信赖的，并且对小扰动具有鲁棒性，这意味着这些模型在存在噪声时能够产生一致的预测。然而，研究表明，深度学习模型容易受到对抗样本（AEs）的攻击，这会使深度学习模型通过精心设计的隐蔽或语义保持扰动产生错误预测。为了实现可信赖的人工智能，持续的努力已被投入到提高模型对抗攻击的鲁棒性和安全性，并通过构建更强的对抗攻击来寻找这些模型的鲁棒性上限。这是一场关于对抗攻击和防御的无休止的竞赛。本文从对手的角度全面调查了这一对抗攻击和防御竞争的最新进展。
- en: This paper primarily discusses the phenomenon of adversarial attacks in the
    realm of computer vision tasks. While most computer vision tasks tend to focus
    on image processing, recent attention has been directed toward 3D tasks. 3D data
    can supplement 2D data by providing depth information about an image, thereby
    allowing for a more reliable and detailed analysis of targets, and can be utilized
    in various applications. For instance, autonomous vehicles often rely on a combination
    of cameras and lidar to perceive their surroundings. However, due to the unordered
    nature of 3D data, direct application of 2D adversarial attacks is not feasible.
    3D adversarial attacks have been proposed, building upon the principles of 2D
    adversarial attacks but with specific designs tailored to the characteristics
    of 3D data. For example, 3DAdv (Xiao et al., [2018a](#bib.bib150)) is based on
    the C&W attack but with novel distance metrics. 3Dhacker (Tao et al., [2023](#bib.bib130))
    is based on the boundary attack but fuses the point cloud in the spectral domain
    rather than via coordinate-wise average operation. Due to the close relationship
    between 2D and 3D computer vision tasks and the shared theoretical foundation
    of 2D and 3D attacks, this survey summarizes the latest progress of adversarial
    attacks in both 2D and 3D computer vision.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主要讨论了计算机视觉任务领域的对抗攻击现象。虽然大多数计算机视觉任务倾向于关注图像处理，但最近的关注已转向3D任务。3D数据可以通过提供图像的深度信息来补充2D数据，从而允许对目标进行更可靠和详细的分析，并可用于各种应用。例如，自动驾驶车辆通常依赖于摄像头和激光雷达的组合来感知其周围环境。然而，由于3D数据的无序性质，直接应用2D对抗攻击是不切实际的。3D对抗攻击已经被提出，它们基于2D对抗攻击的原理，但具有针对3D数据特性的具体设计。例如，3DAdv（Xiao等，[2018a](#bib.bib150)）基于C&W攻击，但具有新颖的距离度量。3Dhacker（Tao等，[2023](#bib.bib130)）基于边界攻击，但在频谱域中融合点云，而不是通过坐标方向的平均操作。由于2D和3D计算机视觉任务之间的紧密关系以及2D和3D攻击的共享理论基础，本文总结了2D和3D计算机视觉中对抗攻击的最新进展。
- en: 'The aim of this survey is to systematize the latest progress of 2D and 3D adversarial
    attacks to help researchers construct stronger AEs to evaluate model robustness,
    design robust models, and ensure safety in real-world applications. To select
    literature for systematic review, we first clarify the review scope and thoroughly
    search publications in top computer vision or security conferences and journals
    within the past several years. Then, we select high-cited or representative work
    related to 2D and 3D adversarial attacks. These articles are categorized and compared
    based on the targets, scenarios, and methods for comprehensiveness. The main contributions
    of this work are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的目的是系统化最新的 2D 和 3D 对抗性攻击进展，以帮助研究人员构建更强的 AE 来评估模型鲁棒性，设计鲁棒模型，并确保现实世界应用中的安全性。为了选择系统评审的文献，我们首先明确审查范围，并在过去几年内彻底搜索顶级计算机视觉或安全会议和期刊的出版物。然后，我们选择与
    2D 和 3D 对抗性攻击相关的高引用或代表性工作。这些文章根据目标、场景和方法进行分类和比较，以确保全面性。这项工作的主要贡献如下：
- en: •
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We summarize the latest studies on the adversarial robustness and safety of
    2D and 3D deep learning models against adversarial attacks. Over 170 papers in
    recent years have been collated and compared. Moreover, We divide them into 2D
    and 3D attacks according to the data characteristics and application scenarios.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结了有关 2D 和 3D 深度学习模型对抗攻击的最新研究。整理并比较了近年来的 170 多篇论文。此外，我们根据数据特性和应用场景将其分为 2D
    和 3D 攻击。
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For 2D adversarial attacks, we extend the meaning of adversarial attack from
    imperceptible perturbation to semantic-preserving perturbation, such as color
    space distortion and spatial transformation distortion. We classify these attacks
    according to methodologies and compare their pros and cons to give a full-scope
    view.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 2D 对抗性攻击，我们将对抗攻击的意义从不可察觉的扰动扩展到语义保持扰动，例如颜色空间扭曲和空间变换扭曲。我们根据方法论对这些攻击进行分类，并比较它们的优缺点，以提供全面的视角。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 3D data is increasingly leveraged in safety-critical fields like self-driving.
    However, the 3D model robustness is hardly reviewed. To help design robust 3D
    deep learning models, we are the first to organize 3D adversarial attacks systematically
    and classify them according to their algorithms.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3D 数据在自动驾驶等安全关键领域中越来越被利用。然而，3D 模型的鲁棒性很少被审查。为了帮助设计鲁棒的 3D 深度学习模型，我们首次系统地组织了 3D
    对抗性攻击，并根据其算法对其进行了分类。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When deep learning models are deployed in the real world, safety is the primary
    premise. We comprehensively examine related works of adversarial attacks for safe-critical
    missions, especially for camera-based and Lidar-based self-driving cars and face
    recognition.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当深度学习模型在现实世界中部署时，安全是首要前提。我们全面审查了针对安全关键任务的对抗性攻击相关工作，特别是基于摄像头和激光雷达的自动驾驶汽车和人脸识别。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At the end of this review, we summarize the present hot research topics into
    several points and identify their challenges, such as improving the attack transferability,
    generating semantic perturbation, and evaluating the robustness of 3D deep learning
    models. We also provide some useful advice for future research directions.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这次评审的最后，我们将当前热门的研究主题总结为几个要点，并确定它们面临的挑战，如提高攻击可迁移性、生成语义扰动以及评估 3D 深度学习模型的鲁棒性。我们还为未来的研究方向提供了一些有用的建议。
- en: The structure of this survey is shown in Figure.[1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks"). In Sec.[2](#S2 "2\. Background ‣ A Survey of Robustness
    and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks"), we
    clarify the basic concepts of deep learning and computer vision. In Sec.[4](#S4
    "4\. Threat model ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning
    Models Against Adversarial Attacks"), we build a general threat model for deep-learning-based
    computer vision systems by examining the attack surface from different perspectives.
    In Sec.[3](#S3 "3\. Related work ‣ A Survey of Robustness and Safety of 2D and
    3D Deep Learning Models Against Adversarial Attacks"), we summarize the latest
    related reviews. In Sec.[5](#S5 "5\. Adversarial attacks for 2D deep learning
    models ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks") and [6](#S6 "6\. Adversarial attacks for 3D deep learning
    models ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks"), we summarize the representative works that evaluate the
    robustness and safety of deep learning models by 2D and 3D adversarial examples,
    respectively. In each section, we first introduce our taxonomy based on attack
    methodology or target applications, then summarize the representative digital-world
    and physical-world adversarial attacks. Finally, in Sec.[7](#S7 "7\. Future directions
    and challenges ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning
    Models Against Adversarial Attacks"), we identify the present obstacles in the
    adversarial attacks, coupled with some viewpoints for future studies.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的结构如图.[1](#S1.F1 "图 1 ‣ 1\. 引言 ‣ 针对对抗攻击的2D和3D深度学习模型的鲁棒性和安全性调查")所示。在第.[2](#S2
    "2\. 背景 ‣ 针对对抗攻击的2D和3D深度学习模型的鲁棒性和安全性调查")节中，我们阐明了深度学习和计算机视觉的基本概念。在第.[4](#S4 "4\.
    威胁模型 ‣ 针对对抗攻击的2D和3D深度学习模型的鲁棒性和安全性调查")节中，我们通过从不同角度审视攻击面，构建了深度学习计算机视觉系统的一般威胁模型。在第.[3](#S3
    "3\. 相关工作 ‣ 针对对抗攻击的2D和3D深度学习模型的鲁棒性和安全性调查")节中，我们总结了最新的相关综述。在第.[5](#S5 "5\. 针对2D深度学习模型的对抗攻击
    ‣ 针对对抗攻击的2D和3D深度学习模型的鲁棒性和安全性调查")节和[6](#S6 "6\. 针对3D深度学习模型的对抗攻击 ‣ 针对对抗攻击的2D和3D深度学习模型的鲁棒性和安全性调查")节中，我们总结了评估深度学习模型鲁棒性和安全性的代表性工作，分别使用了2D和3D对抗样本。在每一节中，我们首先介绍了基于攻击方法或目标应用的分类方法，然后总结了代表性的数字世界和物理世界对抗攻击。最后，在第.[7](#S7
    "7\. 未来方向和挑战 ‣ 针对对抗攻击的2D和3D深度学习模型的鲁棒性和安全性调查")节中，我们识别了当前对抗攻击中的障碍，并提供了一些未来研究的观点。
- en: To unify the symbols of different articles, Table.[1](#S1.T1 "Table 1 ‣ 1\.
    Introduction ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models
    Against Adversarial Attacks") shows some common notations in this survey.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了统一不同文章的符号，表.[1](#S1.T1 "表 1 ‣ 1\. 引言 ‣ 针对对抗攻击的2D和3D深度学习模型的鲁棒性和安全性调查")展示了本调查中的一些常见符号。
- en: '![Refer to caption](img/417c9946512bba03981678ebe68fc5b2.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/417c9946512bba03981678ebe68fc5b2.png)'
- en: Figure 1\. The structure of this survey
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 本调查的结构
- en: \Description
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: The structure of this paper.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构。
- en: Table 1\. Common notations used in this survey
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 本调查中使用的常见符号
- en: '| Notations | Description | Notations | Description |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 | 符号 | 描述 |'
- en: '| $D_{tr},D_{te}$ | Training`\`testing dataset | $x$ | Normal inputs |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| $D_{tr},D_{te}$ | 训练`\`测试数据集 | $x$ | 正常输入 |'
- en: '| $x^{\prime},x^{adv}$ | Modified`\`adversarial inputs | $\delta$ | Adversarial
    perturbations |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| $x^{\prime},x^{adv}$ | 修改`\`对抗输入 | $\delta$ | 对抗扰动 |'
- en: '| $y$ | Ground truth labels | $y^{\prime}$ | Adversarial target labels |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| $y$ | 真实标签 | $y^{\prime}$ | 对抗目标标签 |'
- en: '| $\mathcal{F}$ | Classification model | $\hat{\mathcal{F}}$ | Substitute model
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{F}$ | 分类模型 | $\hat{\mathcal{F}}$ | 替代模型 |'
- en: '| $g$ | The model’s gradient upon $x$ | $\hat{g}$ | The estimated model gradient
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| $g$ | 模型在$x$上的梯度 | $\hat{g}$ | 估计的模型梯度 |'
- en: '| $l_{p}$ | The $p$-norm distance | $Z_{i}(x)$ | Output logits of $i^{th}$-to-last
    layer |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| $l_{p}$ | $p$-范数距离 | $Z_{i}(x)$ | 第$i^{th}$层的输出对数 |'
- en: '| $\theta$ | Model parameters | $\mathcal{L}(x^{\prime},y)$ | Loss function
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| $\theta$ | 模型参数 | $\mathcal{L}(x^{\prime},y)$ | 损失函数 |'
- en: '| $\mathcal{G}$ | The GAN’s generator | $\mathcal{D}$ | The GAN’s discriminator
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{G}$ | GAN的生成器 | $\mathcal{D}$ | GAN的判别器 |'
- en: '| $\mathcal{P}$ | Original point cloud | $\mathcal{P^{\prime}}$ | Adversarial
    point cloud |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{P}$ | 原始点云 | $\mathcal{P^{\prime}}$ | 对抗点云 |'
- en: 2\. Background
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: Deep learning is a popular representation learning algorithm for its outstanding
    performance on image classification and other tasks. It can learn complex functions
    through a composition of superficial but non-linear layers. Suppose $x$ is an
    image or a point cloud, $\mathcal{F}_{\theta}$ is a deep learning model with model
    parameters $\theta$. The goal of the object classification task is to find $\theta$
    that can minimize the difference between the ground truth label $y$ and the prediction
    $\mathcal{F}_{\theta}(x)$, that is
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种流行的表示学习算法，因其在图像分类及其他任务中的出色表现而受到青睐。它可以通过多个表面但非线性层的组合来学习复杂的函数。假设 $x$ 是一张图像或一个点云，$\mathcal{F}_{\theta}$
    是一个具有模型参数 $\theta$ 的深度学习模型。目标分类任务的目标是找到能最小化真实标签 $y$ 和预测值 $\mathcal{F}_{\theta}(x)$
    之间差异的 $\theta$，即
- en: '| (1) |  | $arg\min_{\theta}\mathcal{L}(\mathcal{F}_{\theta}(x),y),$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $arg\min_{\theta}\mathcal{L}(\mathcal{F}_{\theta}(x),y),$ |  |'
- en: where $\mathcal{L}$ is a loss function to measure the entropy between the $\mathcal{F}_{\theta}(x)$
    and y. The $\theta$ is usually optimized through gradient descent algorithms,
    such as Adam. After training, the deep learning model is deployed into real-world
    applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}$ 是一个损失函数，用来衡量 $\mathcal{F}_{\theta}(x)$ 和 y 之间的熵。$\theta$ 通常通过梯度下降算法（如
    Adam）进行优化。训练完成后，深度学习模型被部署到实际应用中。
- en: 'Many deep learning model variants have been proposed to boost performance and
    adapt to different task characteristics, such as multi-layer perceptions, stacked
    autoencoders, convolutional neural networks, deep brief networks, and vision transformers.
    These models are widely used in various 2D computer vision tasks, such as object
    detection, image segmentation, image classification, action recognition, and motion
    tracking. Recently, with the boom of 3D data in self-driving systems and other
    applications, deep learning for 3D computer vision has attracted much attention.
    Various models came out, such as MLP-based models (e.g. PointNet (Qi et al., [2017](#bib.bib106))),
    convolutional models (e.g. Pointwise-CNN (Hua et al., [2018](#bib.bib53))), graph-based
    convolutional models (e.g. DGCNN (Wang et al., [2019](#bib.bib141))), and transformer-based
    models (e.g. PCT (Guo et al., [2021](#bib.bib44))). The major applications of
    the deep learning model in 3D computer vision can be categorized into three tasks:
    3D object detection and tracking, 3D object classification, and 3D object segment.
    Because most 3D deep learning models are extensions of 2D models, many security
    threats in the 2D domain also exist in the 3D domain.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出许多深度学习模型变体以提升性能并适应不同的任务特性，例如多层感知机、堆叠自编码器、卷积神经网络、深度简要网络和视觉变换器。这些模型广泛应用于各种
    2D 计算机视觉任务，如目标检测、图像分割、图像分类、动作识别和运动追踪。最近，随着自驾系统及其他应用中 3D 数据的兴起，3D 计算机视觉的深度学习引起了广泛关注。出现了各种模型，如基于
    MLP 的模型（例如 PointNet (Qi et al., [2017](#bib.bib106)））、卷积模型（例如 Pointwise-CNN (Hua
    et al., [2018](#bib.bib53)））、基于图的卷积模型（例如 DGCNN (Wang et al., [2019](#bib.bib141)））和基于变换器的模型（例如
    PCT (Guo et al., [2021](#bib.bib44)））。深度学习模型在 3D 计算机视觉中的主要应用可以分为三类任务：3D 目标检测与跟踪、3D
    目标分类和 3D 目标分割。由于大多数 3D 深度学习模型是 2D 模型的扩展，因此许多 2D 领域的安全威胁在 3D 领域也存在。
- en: 3\. Related work
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 相关工作
- en: To boost this adversarial machine learning arms race and mitigate the risk of
    adversarial attacks, some previous surveys have tried to summarize the latest
    relevant studies. For example, Serban (Serban et al., [2020](#bib.bib116)) collected
    adversarial attacks in the object recognition task. However, they did not include
    3D adversarial attacks and semantic attacks like color space perturbations. Miller
    et al. (Miller et al., [2020](#bib.bib89)) and Machado et al. (Machado et al.,
    [2021](#bib.bib86)) reviewed the recent progress of adversarial machine learning,
    but from the defense rather than the adversary perspective. Other reviews regard
    the adversarial attack as part of the AI attacks without a separate detailed investigation.
    For example, Liu et al. (Liu et al., [2018](#bib.bib79)) classified AI attacks
    into integrity, availability, and confidentiality attacks according to the traditional
    taxonomy of security violation. Papernot et al. (Papernot et al., [2018](#bib.bib103))
    and Liu et al. (Liu et al., [2020b](#bib.bib80)) categorized attacks based on
    the deep learning pipeline, dividing them into training and test phase attacks.
    He et al. (He et al., [2022a](#bib.bib48)) categorized the AI attacks into adversarial
    attacks, poisoning attacks, model extraction, and inversion attacks. Moreover,
    these surveys lack summaries of the latest research trends, such as the growing
    interest in transferable adversarial examples, semantic perturbations, 3D adversarial
    attacks, and physical-realizable adversarial attacks. In addition, although attack-agnostic
    robustness evaluation has emerged in recent years, the adversarial attack is still
    one of the most efficient and reliable ways to evaluate model robustness. There
    is still a lack of a systematic review to collate these developments and discuss
    future directions in light of these latest efforts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了推动对抗机器学习的军备竞赛并减少对抗攻击的风险，一些早期的调研试图总结最新的相关研究。例如，Serban (Serban et al., [2020](#bib.bib116))
    收集了在对象识别任务中的对抗攻击。然而，他们没有包括 3D 对抗攻击和像颜色空间扰动这样的语义攻击。Miller et al. (Miller et al.,
    [2020](#bib.bib89)) 和 Machado et al. (Machado et al., [2021](#bib.bib86)) 回顾了对抗机器学习的最新进展，但从防御的角度而非对抗者的角度进行的。其他综述将对抗攻击视为
    AI 攻击的一部分，没有进行单独详细的调查。例如，Liu et al. (Liu et al., [2018](#bib.bib79)) 根据传统的安全漏洞分类将
    AI 攻击分为完整性、可用性和保密性攻击。Papernot et al. (Papernot et al., [2018](#bib.bib103)) 和
    Liu et al. (Liu et al., [2020b](#bib.bib80)) 基于深度学习流程对攻击进行了分类，将其分为训练阶段和测试阶段攻击。He
    et al. (He et al., [2022a](#bib.bib48)) 将 AI 攻击分类为对抗攻击、投毒攻击、模型提取和反演攻击。此外，这些调查缺乏对最新研究趋势的总结，例如对可转移对抗样本、语义扰动、3D
    对抗攻击和物理可实现对抗攻击的日益关注。此外，尽管近年来出现了攻击无关的鲁棒性评估，对抗攻击仍然是评估模型鲁棒性的最有效和可靠的方法之一。目前仍缺乏系统的综述来整理这些进展，并根据这些最新的努力讨论未来的发展方向。
- en: 4\. Threat model
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 威胁模型
- en: The threat model evaluates the possible risk and security level of the system
    by describing the target, capability, and knowledge of the attacker. By establishing
    a full-scale threat model, we can understand the security problems of the deep
    learning system more comprehensively. We first identify the complete and thorough
    attack surface when the deep neural networks are deployed into computer vision
    tasks, as shown in Figure.[2](#S4.F2 "Figure 2 ‣ 4.1.3\. Deep learning model.
    ‣ 4.1\. The attack surface of deep learning models ‣ 4\. Threat model ‣ A Survey
    of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial
    Attacks"). Then we analyze the adversary’s goals, capabilities, and knowledge
    to construct a threat panorama.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 威胁模型通过描述攻击者的目标、能力和知识来评估系统的可能风险和安全级别。通过建立一个全面的威胁模型，我们可以更全面地理解深度学习系统的安全问题。我们首先识别深度神经网络部署到计算机视觉任务中时的完整和彻底的攻击面，如图[2](#S4.F2
    "Figure 2 ‣ 4.1.3\. Deep learning model. ‣ 4.1\. The attack surface of deep learning
    models ‣ 4\. Threat model ‣ A Survey of Robustness and Safety of 2D and 3D Deep
    Learning Models Against Adversarial Attacks")所示。然后，我们分析对手的目标、能力和知识，以构建一个威胁全景。
- en: 4.1\. The attack surface of deep learning models
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 深度学习模型的攻击面
- en: The attack surface is composed of all possible vulnerabilities in a system that
    an unauthorized user can access. As mentioned above, deep learning applications
    train the model according to the model performance on the training dataset and
    then deploy the model to real-world applications for various 2D or 3D tasks. In
    this process, the attack surface includes the training and test dataset and the
    deep learning model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击面由系统中所有可能的漏洞组成，这些漏洞可能被未经授权的用户访问。如上所述，深度学习应用根据模型在训练数据集上的表现训练模型，然后将模型部署到实际应用中，进行各种
    2D 或 3D 任务。在这个过程中，攻击面包括训练和测试数据集以及深度学习模型。
- en: 4.1.1\. Training dataset.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 训练数据集。
- en: Collecting a training dataset with satisfactory quality requires a great amount
    of effort and money. Meanwhile, some datasets, like medical diagnostic records
    and personal genomic data, may contain sensitive information. Therefore, the privacy
    and confidentiality of these training datasets are essential. Membership inference
    attacks (Shokri et al., [2017](#bib.bib123)) and data reconstruction attacks (Salem
    et al., [2020](#bib.bib112)) can leak the attribute or information of training
    data. In addition, more capable attackers may be able to modify the training datasets.
    Under this assumption, backdoor attacks and poisoning attacks are proposed. The
    backdoor attacks make the model behave normally for clean samples but behave incorrectly
    for the same samples with specific stickers by adding triggers in the training
    process. The poisoning attacks modify the sample’s label in the training phase
    so that the model classifies the clean sample incorrectly.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 收集具有令人满意质量的训练数据集需要大量的努力和资金。同时，一些数据集，例如医疗诊断记录和个人基因组数据，可能包含敏感信息。因此，这些训练数据集的隐私和机密性至关重要。会员推断攻击（Shokri
    et al., [2017](#bib.bib123)）和数据重建攻击（Salem et al., [2020](#bib.bib112)）可能会泄露训练数据的属性或信息。此外，更具能力的攻击者可能会修改训练数据集。在这种假设下，提出了后门攻击和污染攻击。后门攻击使得模型在处理干净样本时正常工作，但在处理带有特定触发器的相同样本时表现不正确。污染攻击则在训练阶段修改样本标签，使得模型错误地分类干净样本。
- en: 4.1.2\. Test dataset.
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 测试数据集。
- en: The main threat to the test dataset is adversarial attack. The adversarial attack
    can make the model produce wrong results by adding invisible or semantic perturbations
    to the input data. In recent years, physical adversarial attacks have gained more
    and more attention. Through 2D patches (Duan et al., [2020](#bib.bib34)), 3D printing
    (Athalye et al., [2018b](#bib.bib7)), optical illumination (Gnanasambandam et al.,
    [2021](#bib.bib41)) or sensor injection (Cao et al., [2019b](#bib.bib17)), the
    attacker makes the camera or radar collect modified 2D or 3D data and output incorrect
    classification results. It is worth mentioning that this paper divides the adversarial
    attack into 2D and 3D attacks according to the data and model type rather than
    the attack media. For example, Athalye et al. (Athalye et al., [2018b](#bib.bib7))
    rendered 2D adversarial images onto the 3D-printed turtles. Although the turtles
    are 3D objects, the final input of the 2D model is the turtle image collected
    by the camera, so it is still a 2D adversarial attack.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集的主要威胁是对抗攻击。对抗攻击可以通过向输入数据添加不可见或语义扰动使模型产生错误结果。近年来，物理对抗攻击引起了越来越多的关注。通过2D补丁（Duan
    et al., [2020](#bib.bib34)）、3D打印（Athalye et al., [2018b](#bib.bib7)）、光学照明（Gnanasambandam
    et al., [2021](#bib.bib41)）或传感器注入（Cao et al., [2019b](#bib.bib17)），攻击者使得相机或雷达收集修改过的2D或3D数据，并输出错误的分类结果。值得一提的是，本文根据数据和模型类型将对抗攻击划分为2D和3D攻击，而不是攻击介质。例如，Athalye
    et al.（Athalye et al., [2018b](#bib.bib7)）将2D对抗图像渲染到3D打印的海龟上。虽然海龟是3D对象，但2D模型的最终输入仍是相机收集的海龟图像，因此仍然是2D对抗攻击。
- en: 4.1.3\. Deep learning model.
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 深度学习模型。
- en: Training deep learning models requires a lot of resources and time, so a well-trained
    deep learning model successfully applied to practice has great value. However,
    there are already some model extraction attacks that can steal sensitive information
    such as model parameters and decision boundaries. In addition, the model itself
    may also be subject to model revising attack. For example, an adversary can make
    the model misclassify by only flipping several bits of the model (Rakin et al.,
    [2021](#bib.bib109)) or poisoning the open-source pre-trained weights (Kurita
    et al., [2020](#bib.bib67)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型需要大量资源和时间，因此成功应用于实践的深度学习模型具有很大价值。然而，已经存在一些模型提取攻击，可以窃取敏感信息，如模型参数和决策边界。此外，模型本身也可能受到模型修订攻击。例如，攻击者可以通过仅翻转模型的几个比特（Rakin
    et al., [2021](#bib.bib109)）或污染开源预训练权重（Kurita et al., [2020](#bib.bib67)）使模型错误分类。
- en: This paper mainly discusses the adversarial attack because it poses a more practical
    threat than other attacks. It does not need to have knowledge about the training
    dataset and even the model architecture (for black-box attacks) and, therefore,
    can be executed in the inference stage. In addition, generating strong AEs efficiently
    can improve the effectiveness and efficiency of adversarial training, which is
    essential for training robust models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主要讨论对抗攻击，因为它相比其他攻击具有更实际的威胁。它不需要了解训练数据集甚至模型架构（对于黑箱攻击），因此可以在推理阶段执行。此外，高效生成强对抗样本可以提高对抗训练的有效性和效率，这对于训练鲁棒模型至关重要。
- en: '![Refer to caption](img/9e69f12c67b660a931a76c86fe7b21f9.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9e69f12c67b660a931a76c86fe7b21f9.png)'
- en: Figure 2\. An overview of the attack surface when deep learning models are deployed
    into computer vision applications.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 深度学习模型部署到计算机视觉应用中的攻击面概述。
- en: \Description
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: The structure of this paper.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构。
- en: 4.2\. Adversarial goal
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 对抗目标
- en: According to the attacker’s goal and the level of security violation, deep learning
    attacks can be divided into confidentiality attack, integrity attack, and availability
    attack. The main goal of the confidentiality attack is to leak the privacy of
    the model or data, such as the model inverse attack and membership inference attack.
    The primary aim of the integrity attack is to maliciously change the model’s output
    by modifying the training data, the test data, or even the model itself, such
    as backdoor attacks, adversarial attacks, and weight modification attacks. The
    availability attack aims to make the ML services unavailable to legal users. A
    typical example of availability attacks is to make the dataset not exploitable
    by maliciously poisoning the whole training dataset. For example, the Shortcut
    attack (Yu et al., [2022](#bib.bib158)) uses linearly separable noises assigned
    with target labels to mislead the neural network, resulting in the trained model
    with low accuracy on the test dataset. The current security research in deep learning
    focuses on confidentiality and integrity.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据攻击者的目标和安全性违背的程度，深度学习攻击可以分为保密性攻击、完整性攻击和可用性攻击。保密性攻击的主要目标是泄露模型或数据的隐私，如模型逆向攻击和成员推断攻击。完整性攻击的主要目的是通过修改训练数据、测试数据或甚至模型本身来恶意更改模型的输出，如后门攻击、对抗攻击和权重修改攻击。可用性攻击旨在使机器学习服务对合法用户不可用。可用性攻击的一个典型例子是通过恶意污染整个训练数据集来使数据集不可利用。例如，Shortcut攻击（Yu
    et al., [2022](#bib.bib158)）使用与目标标签分配的线性可分噪声来误导神经网络，导致训练后的模型在测试数据集上的准确性较低。目前深度学习中的安全研究主要集中在保密性和完整性上。
- en: 4.3\. Adversarial capabilities and knowledge
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 对抗能力与知识
- en: 'The attacker’s capabilities refer to the amount of information the attackers
    have and the actions they can perform. In the context of deep learning systems,
    the attacker’s abilities, from weak to strong, can be classified as follows: only
    being able to access hard labels and revising test data; being able to access
    model output confidence; having the ability to access model parameters and training
    data; modifying training data; modifying model parameters.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者的能力指的是攻击者拥有的信息量和他们可以执行的操作。在深度学习系统的背景下，攻击者的能力从弱到强可以分为：仅能访问硬标签和修订测试数据；能访问模型输出置信度；能访问模型参数和训练数据；修改训练数据；修改模型参数。
- en: According to the attackers’ understanding of models and data, the adversarial
    knowledge can be classified into black-box, gray-box, and white-box models. There
    is no clear dividing line between the three. Generally, the attacks that know
    the model’s internal structure and parameters are called white-box attacks. The
    attacks that can only access the classification results are called black-box attacks,
    and the attacks in between are called gray-box attacks. In reality, adversarial
    knowledge is usually determined by adversarial ability, so the two are closely
    related.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据攻击者对模型和数据的理解，对抗知识可以分为黑盒、灰盒和白盒模型。三者之间没有明确的分界线。一般来说，只了解模型内部结构和参数的攻击被称为白盒攻击。只能访问分类结果的攻击被称为黑盒攻击，处于两者之间的攻击被称为灰盒攻击。在现实中，对抗知识通常由对抗能力决定，因此两者密切相关。
- en: 5\. Adversarial attacks for 2D deep learning models
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 2D深度学习模型的对抗攻击
- en: The adversarial attack, one of the most threatening attacks, has attracted much
    research interest. In this and the next section, we will discuss the adversarial
    attacks in the deep learning-based computer vision system, in which the adversary’s
    goal is to cause the deep learning models to produce wrong predictions. We divide
    them into 2D and 3D attacks according to the different target model types. The
    2D attacks mainly target the 2D models, while the 3D attacks mainly target the
    3D model by contaminating 3D data like point clouds.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击是最具威胁性的攻击之一，引起了广泛的研究兴趣。在本节和下一节中，我们将讨论基于深度学习的计算机视觉系统中的对抗攻击，其中对手的目标是使深度学习模型产生错误预测。根据不同的目标模型类型，我们将其分为2D和3D攻击。2D攻击主要针对2D模型，而3D攻击主要通过污染3D数据（如点云）来影响3D模型。
- en: 5.1\. Catalog of 2D adversarial attacks
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 2D对抗攻击目录
- en: Although the 2D adversarial attack has been summarized in some previous literature
    (Serban et al., [2020](#bib.bib116); Machado et al., [2021](#bib.bib86); He et al.,
    [2022a](#bib.bib48)), they usually organize from a certain aspect, lacking systematization
    and the latest research progress. Therefore, there is a need to reorganize the
    latest studies, especially the emerging ones that cannot be classified into any
    previous categories, such as color space perturbations and other semantic perturbations.
    This paper summarizes the most recent progress to help readers keep abreast of
    the newest tendency. Our survey classified 2D adversarial attacks from various
    dimensions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以前的一些文献中已经总结了2D对抗攻击（Serban等人，[2020](#bib.bib116); Machado等人，[2021](#bib.bib86);
    He等人，[2022a](#bib.bib48)），但它们通常是从某个特定方面进行组织，缺乏系统性和最新的研究进展。因此，有必要重新组织最新的研究，特别是那些无法归类到以前任何类别的新兴研究，比如颜色空间扰动和其他语义扰动。本文总结了最新的研究进展，以帮助读者及时了解最新趋势。我们的调查从各个方面对2D对抗攻击进行了分类。
- en: 5.1.1\. Classifying according to different distance metrics
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 根据不同距离度量分类
- en: When the term adversarial example was first proposed in 2013 (Szegedy et al.,
    [2013](#bib.bib129)), its definition is leveraging crafted imperceptible perturbation
    to fool the deep neural network, that is
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当“对抗样本”一词在2013年首次提出时（Szegedy等人，[2013](#bib.bib129)），其定义是利用精心制作的不可察觉的扰动来欺骗深度神经网络，也就是
- en: '| (2) |  | $\min_{x^{\prime}}\left\&#124;{x^{\prime}-x}\right\&#124;_{p}\text{,
    subject to }\mathcal{F}(x^{\prime})\neq\mathcal{F}(x)\text{ and }x^{\prime}\in[0,1]^{m},$
    |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\min_{x^{\prime}}\left\&#124;{x^{\prime}-x}\right\&#124;_{p}\text{,
    subject to }\mathcal{F}(x^{\prime})\neq\mathcal{F}(x)\text{ and }x^{\prime}\in[0,1]^{m},$
    |  |'
- en: where $p\in\{0,1,2,\infty\}$. Besides $l_{p}$-norm distance, recently there
    are also other distance metrics are proposed to constrain the perturbation size,
    such as color space distance (Shamsabadi et al., [2020b](#bib.bib119); Laidlaw
    and Feizi, [2019](#bib.bib68)), geodesic distance (Fawzi and Frossard, [2015](#bib.bib37)),
    Wasserstein distance (Zheng et al., [2019a](#bib.bib174)), or even without any
    distance limitation (Song et al., [2018](#bib.bib125)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p\in\{0,1,2,\infty\}$。除了$l_{p}$-范数距离，最近还提出了其他限制扰动大小的距离度量，例如颜色空间距离（Shamsabadi等人，[2020b](#bib.bib119);
    Laidlaw和Feizi，[2019](#bib.bib68)），测地线距离（Fawzi和Frossard，[2015](#bib.bib37)），Wasserstein距离（Zheng等人，[2019a](#bib.bib174)），甚至没有任何距离限制（Song等人，[2018](#bib.bib125)）。
- en: 5.1.2\. Classifying according to physical achievability
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 根据物理可实现性分类
- en: Based on whether they can be realized in the physical world, 2D adversarial
    attacks can be divided into digital attack and physical attacks. The digital adversarial
    attacks assume that the adversary can directly modify the digital images, while
    the physical adversarial attacks suppose that the attacker cannot immediately
    revise the neural network’s input and, therefore, revise the real-world objects
    instead. Physical attacks are more difficult than digital attacks because of limited
    perturbation space and various environmental variables, such as different viewpoints,
    distance, and background illumination.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 根据是否可以在物理世界中实现，2D 对抗攻击可以分为数字攻击和物理攻击。数字对抗攻击假设对手可以直接修改数字图像，而物理对抗攻击则假设攻击者不能直接修改神经网络的输入，因此改动现实世界的对象。物理攻击比数字攻击更为困难，因为受限于扰动空间和各种环境变量，如不同的视角、距离和背景照明。
- en: 5.1.3\. Classifying according to adversary’s knowledge level
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 根据对手的知识水平进行分类
- en: As mentioned in the previous section, according to the adversary’s knowledge
    extent, 2D digital adversarial attacks can be divided into white-box, gray-box,
    and black-box attacks. Different knowledge extent can result in disparate choices
    of attack methods because of different difficulties. Therefore, we introduce the
    white-box attacks and black-box attacks separately.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，根据对手的知识范围，2D 数字对抗攻击可以分为白盒、灰盒和黑盒攻击。不同的知识范围会导致攻击方法的不同选择，因为难度不同。因此，我们将白盒攻击和黑盒攻击分别介绍。
- en: 5.1.4\. Classifying according to the definition of perturbation
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4\. 根据扰动定义进行分类
- en: According to the perturbation’s difference, Gilmer et al. (Gilmer et al., [2018](#bib.bib40))
    classified adversarial perturbations into imperceptible, content-preserving, and
    unconstrained perturbation. The first one sets the original image as the starting
    point and adds invisible noises to this image. This constraint makes the AE almost
    the same as the clean one in human eyes and usually uses $l_{p}$-norm as their
    distance metrics. For content-preserving perturbation, the adversary retains the
    semantics of the original image and misleads the classifier at the same time by
    methods like changing colors (Hosseini and Poovendran, [2018](#bib.bib51); Laidlaw
    and Feizi, [2019](#bib.bib68)) and spatially transforms the images (Fawzi and
    Frossard, [2015](#bib.bib37)). These attacks usually use semantic distance to
    restrain the perturbation. For unconstrained input, the adversary induces erroneous
    results from the neural networks through any examples. For instance, Song et al.
    (Song et al., [2018](#bib.bib125)) generates AEs from scratch.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据扰动的差异，Gilmer 等人 (Gilmer et al., [2018](#bib.bib40)) 将对抗扰动分为不可察觉的、保持内容的和无约束的扰动。第一种扰动以原始图像为起点，并在图像中添加不可见的噪声。这种约束使得
    AE 在人眼中几乎与干净的图像相同，通常使用 $l_{p}$-范数作为距离度量。对于保持内容的扰动，对手保留原始图像的语义，同时通过改变颜色 (Hosseini
    and Poovendran, [2018](#bib.bib51); Laidlaw and Feizi, [2019](#bib.bib68)) 和空间变换图像
    (Fawzi and Frossard, [2015](#bib.bib37)) 等方法误导分类器。这些攻击通常使用语义距离来限制扰动。对于无约束的输入，对手通过任何示例引起神经网络的错误结果。例如，Song
    等人 (Song et al., [2018](#bib.bib125)) 从头生成 AE。
- en: Based on the above observation, we classify 2D adversarial attacks into digital
    white-box and black-box adversarial attacks, and physical adversarial attacks,
    as shown in Figure.[1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey of Robustness
    and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks").
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述观察，我们将 2D 对抗攻击分类为数字白盒和黑盒对抗攻击，以及物理对抗攻击，如图。[1](#S1.F1 "图 1 ‣ 1\. 介绍 ‣ 关于 2D
    和 3D 深度学习模型对抗攻击的鲁棒性和安全性调查")。
- en: Table 2\. Summary of main white-box adversarial attacks in 2D CV tasks sorted
    by the algorithm and published year
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 2D CV 任务中主要白盒对抗攻击的总结，按算法和发表年份排序
- en: '| Attacks | Year | Threat model | Algorithm | Distance | Performance | Key
    idea |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 年份 | 威胁模型 | 算法 | 距离 | 性能 | 关键思想 |'
- en: '| Goal | Knowl.^* | Efficiency | Camouflage | ASR ^(**) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 知识 ^* | 效率 | 伪装 | ASR ^(**) |'
- en: '| L-BGFS (Szegedy et al., [2013](#bib.bib129)) | 2014 | T | $\square$ | Optimization
    | $L_{2}$ | Costly | Invisible | 100.0% | Box constrained L-BGFS |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| L-BGFS (Szegedy et al., [2013](#bib.bib129)) | 2014 | T | $\square$ | 优化
    | $L_{2}$ | 昂贵 | 不可见 | 100.0% | 盒约束的 L-BGFS |'
- en: '| JSMA (Papernot et al., [2016b](#bib.bib102)) | 2016 | T | $\square$ | Optimization
    | $L_{0}$ | Efficient | Slight | 97.2% | $L_{0}$ + greedy |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| JSMA (Papernot et al., [2016b](#bib.bib102)) | 2016 | T | $\square$ | 优化
    | $L_{0}$ | 高效 | 稍微 | 97.2% | $L_{0}$ + 贪心 |'
- en: '| UAP (Moosavi-Dezfooli et al., [2017](#bib.bib92)) | 2017 | U | $\square$
    | Optimization | $L_{1},L_{\infty}$ | Medium | Invisible | 93.7% | Universal perturbation
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| UAP (Moosavi-Dezfooli et al., [2017](#bib.bib92)) | 2017 | U | $\square$
    | 优化 | $L_{1},L_{\infty}$ | 中等 | 隐形 | 93.7% | 通用扰动 |'
- en: '| C&W (Carlini and Wagner, [2017](#bib.bib18)) | 2017 | T&U | $\square$ | Optimization
    | $L_{0},L_{2},L_{\infty}$ | Medium | Invisible | 100.0% | Substitute loss function
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| C&W (Carlini and Wagner, [2017](#bib.bib18)) | 2017 | T&U | $\square$ | 优化
    | $L_{0},L_{2},L_{\infty}$ | 中等 | 隐形 | 100.0% | 替代损失函数 |'
- en: '| EAD (Chen et al., [2018b](#bib.bib22)) | 2017 | T&U | $\square$ | Optimization
    | $L_{1},L_{2}$ | Costly | Invisible | 100.0% | $L_{1}$ + $L_{2}$ distance |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| EAD (Chen et al., [2018b](#bib.bib22)) | 2017 | T&U | $\square$ | 优化 | $L_{1},L_{2}$
    | 昂贵 | 隐形 | 100.0% | $L_{1}$ + $L_{2}$ 距离 |'
- en: '| DAG (Xie et al., [2017](#bib.bib154)) | 2017 | U | $\square$, | Optimization
    | $L_{\infty}$ | Efficient | Invisible | 69.0% | Segmentation |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| DAG (Xie et al., [2017](#bib.bib154)) | 2017 | U | $\square$, | 优化 | $L_{\infty}$
    | 高效 | 隐形 | 69.0% | 分割 |'
- en: '| SV-UAP (Khrulkov and Oseledets, [2018](#bib.bib64)) | 2018 | U | $\square$
    | Optimization | $L_{2},L_{\infty}$ | Medium | Marked | 60.0% | Singular vector
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SV-UAP (Khrulkov and Oseledets, [2018](#bib.bib64)) | 2018 | U | $\square$
    | 优化 | $L_{2},L_{\infty}$ | 中等 | 标记 | 60.0% | 奇异向量 |'
- en: '| GD-UAP(Mopuri et al., [2018a](#bib.bib94)) | 2018 | U | $\square$,$\color[rgb]{.5,.5,.5}\blacksquare$
    | Optimization | $L_{\infty}$ | Medium | Invisible | 83.5% | Data-free UAP |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GD-UAP(Mopuri et al., [2018a](#bib.bib94)) | 2018 | U | $\square$,$\color[rgb]{.5,.5,.5}\blacksquare$
    | 优化 | $L_{\infty}$ | 中等 | 隐形 | 83.5% | 无数据 UAP |'
- en: '| RobustAdv (Luo et al., [2018](#bib.bib82)) | 2018 | U | $\square$,$\blacksquare$
    | Optimization | Percep. | Costly | Slight | 98.5% | Perceptual sensitivity |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| RobustAdv (Luo et al., [2018](#bib.bib82)) | 2018 | U | $\square$,$\blacksquare$
    | 优化 | 感知 | 昂贵 | 稍微 | 98.5% | 感知敏感度 |'
- en: '| TAP (Zhou et al., [2018](#bib.bib177)) | 2018 | U | $\square$ | Optimization
    | $L_{2}$ | Efficient | Marked | 46.7% | Low-pass filter |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| TAP (Zhou et al., [2018](#bib.bib177)) | 2018 | U | $\square$ | 优化 | $L_{2}$
    | 高效 | 标记 | 46.7% | 低通滤波器 |'
- en: '| DAA(Zheng et al., [2019a](#bib.bib174)) | 2019 | U | $\square$ | Optimization
    | $L_{\infty}$ | Medium | Marked | 45.0% | Advl-data distribution |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| DAA(Zheng et al., [2019a](#bib.bib174)) | 2019 | U | $\square$ | 优化 | $L_{\infty}$
    | 中等 | 标记 | 45.0% | 对抗数据分布 |'
- en: '| SparseFool (Modas et al., [2019](#bib.bib90)) | 2019 | U | $\square$ | Optimization
    | $L_{1}$ | Medium | Slight | 100.0% | Projected $L_{1}$-DeepFool |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| SparseFool (Modas et al., [2019](#bib.bib90)) | 2019 | U | $\square$ | 优化
    | $L_{1}$ | 中等 | 稍微 | 100.0% | 投影 $L_{1}$-DeepFool |'
- en: '| DF-UAP(Zhang et al., [2020b](#bib.bib160)) | 2020 | T&U | $\square$ | Optimization
    | $L_{\infty}$ | Efficient | Slight | 96.2% | Targeted data-free UAP |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| DF-UAP(Zhang et al., [2020b](#bib.bib160)) | 2020 | T&U | $\square$ | 优化
    | $L_{\infty}$ | 高效 | 稍微 | 96.2% | 定向无数据 UAP |'
- en: '| sC&W (Zhang et al., [2020a](#bib.bib162)) | 2020 | T&U | $\square$ | Optimization
    | Smooth | Medium | Marked | 98.0% | Smooth perturbation |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| sC&W (Zhang et al., [2020a](#bib.bib162)) | 2020 | T&U | $\square$ | 优化 |
    平滑 | 中等 | 标记 | 98.0% | 平滑扰动 |'
- en: '| GreedyFool (Dong et al., [2020](#bib.bib31)) | 2020 | U | $\square$ | Optimization
    | $L_{0}$ | Costly | Invisible | 94.6% | Greedy algorithm |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| GreedyFool (Dong et al., [2020](#bib.bib31)) | 2020 | U | $\square$ | 优化
    | $L_{0}$ | 昂贵 | 隐形 | 94.6% | 贪心算法 |'
- en: '| SSAH (Luo et al., [2022](#bib.bib83)) | 2022 | T&U | $\square$ | Optimization
    | Low Freq. | Medium | Invisible | 99.8% | High-frequency limitation |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SSAH (Luo et al., [2022](#bib.bib83)) | 2022 | T&U | $\square$ | 优化 | 低频
    | 中等 | 隐形 | 99.8% | 高频限制 |'
- en: '| FGSM (Goodfellow et al., [2014](#bib.bib42)) | 2015 | U | $\square$ | Fast
    Gradient | $L_{\infty}$ | Efficient | Marked | 72.3% | One-step gradient attack
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| FGSM (Goodfellow et al., [2014](#bib.bib42)) | 2015 | U | $\square$ | 快速梯度
    | $L_{\infty}$ | 高效 | 标记 | 72.3% | 一步梯度攻击 |'
- en: '| Deepfool (Moosavi-Dezfooli et al., [2016](#bib.bib93)) | 2016 | U | $\square$
    | Fast Gradient | $L_{1},L_{2},L_{\infty}$ | Medium | Invisible | 90.0% | Boundary
    dist. estimation |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Deepfool (Moosavi-Dezfooli et al., [2016](#bib.bib93)) | 2016 | U | $\square$
    | 快速梯度 | $L_{1},L_{2},L_{\infty}$ | 中等 | 隐形 | 90.0% | 边界距离估计 |'
- en: '| BIM&ILCM (Kurakin et al., [2018](#bib.bib66)) | 2016 | T&U | $\square$ |
    Fast Gradient | $L_{\infty}$ | Medium | Slight | 90.0% | Iteration FGSM |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| BIM&ILCM (Kurakin et al., [2018](#bib.bib66)) | 2016 | T&U | $\square$ |
    快速梯度 | $L_{\infty}$ | 中等 | 稍微 | 90.0% | 迭代 FGSM |'
- en: '| MI-FGSM (Dong et al., [2018](#bib.bib32)) | 2017 | T&U | $\square$, | Fast
    Gradient | $L2,L_{\infty}$ | Efficient | Slight | 100.0% | Momentum |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| MI-FGSM (Dong et al., [2018](#bib.bib32)) | 2017 | T&U | $\square$, | 快速梯度
    | $L2,L_{\infty}$ | 高效 | 稍微 | 100.0% | 动量 |'
- en: '| PGD (Madry et al., [2017](#bib.bib87)) | 2017 | U | $\square$ | Fast Gradient
    | $L_{2},L_{\infty}$ | Medium | Slight | 99.2% | Projection grad. descent |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| PGD (Madry 等，[2017](#bib.bib87)) | 2017 | U | $\square$ | 快速梯度 | $L_{2},L_{\infty}$
    | 中等 | 轻微 | 99.2% | 投影梯度下降 |'
- en: '| R-FGSM (Tramèr et al., [2017a](#bib.bib131)) | 2018 | T&U | $\square$ | Fast
    Gradient | $L_{\infty}$ | Efficient | Slight | 64.8% | Random FGSM |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| R-FGSM (Tramèr 等，[2017a](#bib.bib131)) | 2018 | T&U | $\square$ | 快速梯度 |
    $L_{\infty}$ | 高效 | 轻微 | 64.8% | 随机FGSM |'
- en: '| BPDA&EOT (Athalye et al., [2018a](#bib.bib6)) | 2018 | U | $\square$ | Fast
    Gradient | $L_{2},L_{\infty}$ | Efficient | Invisible | 100.0% | Attack obfuscated
    grad. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| BPDA&EOT (Athalye 等，[2018a](#bib.bib6)) | 2018 | U | $\square$ | 快速梯度 | $L_{2},L_{\infty}$
    | 高效 | 隐形 | 100.0% | 攻击混淆梯度 |'
- en: '| MDI2FGSM(Xie et al., [2019](#bib.bib155)) | 2019 | T&U | $\square$  $\blacksquare$
    | Fast Gradient | $L_{\infty}$ | Costly | Slight | 62.2% | Random transformation
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MDI2FGSM (Xie 等，[2019](#bib.bib155)) | 2019 | T&U | $\square$  $\blacksquare$
    | 快速梯度 | $L_{\infty}$ | 成本高 | 轻微 | 62.2% | 随机变换 |'
- en: '| TI-BIM (Dong et al., [2019](#bib.bib33)) | 2019 | U | $\square$ | Fast Gradient
    | $L_{\infty}$ | Costly | Slight | 82.0% | Translation kernels |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| TI-BIM (Dong 等，[2019](#bib.bib33)) | 2019 | U | $\square$ | 快速梯度 | $L_{\infty}$
    | 成本高 | 轻微 | 82.0% | 翻译核 |'
- en: '| DDN (Rony et al., [2019](#bib.bib111)) | 2019 | U | $\square$ | Fast Gradient
    | $L_{2}$ | Efficient | Slight | 100.0% | Adjustable step size |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| DDN (Rony 等，[2019](#bib.bib111)) | 2019 | U | $\square$ | 快速梯度 | $L_{2}$
    | 高效 | 轻微 | 100.0% | 可调步长 |'
- en: '| HP-UAP(Zhang et al., [2021](#bib.bib161)) | 2021 | T&U | $\square$,$\blacksquare$
    | Fast Gradient | $L_{\infty}$ | Efficient | Invisible | 91.1% | Frequency filter
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| HP-UAP (Zhang 等，[2021](#bib.bib161)) | 2021 | T&U | $\square$,$\blacksquare$
    | 快速梯度 | $L_{\infty}$ | 高效 | 隐形 | 91.1% | 频率滤波 |'
- en: '| NAG-UAP (Mopuri et al., [2018b](#bib.bib95)) | 2018 | U | $\square$ | GAN
    | $L_{\infty}$ | Costly | Slight | 94.1% | Generative UAP |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| NAG-UAP (Mopuri 等，[2018b](#bib.bib95)) | 2018 | U | $\square$ | GAN | $L_{\infty}$
    | 成本高 | 轻微 | 94.1% | 生成UAP |'
- en: '| ATN (Baluja and Fischer, [2018](#bib.bib9)) | 2018 | T | $\square$ | GAN
    | $L_{2}$ | Efficient | Slight | 95.9% | GAN+reranking |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ATN (Baluja 和 Fischer，[2018](#bib.bib9)) | 2018 | T | $\square$ | GAN | $L_{2}$
    | 高效 | 轻微 | 95.9% | GAN+重排序 |'
- en: '| UnresGM(Song et al., [2018](#bib.bib125)) | 2018 | T | $\square$ | GAN |
    $-$ | Costly | Marked | 84.0% | GAN from scratch |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| UnresGM (Song 等，[2018](#bib.bib125)) | 2018 | T | $\square$ | GAN | $-$ |
    成本高 | 标记 | 84.0% | 从头开始的GAN |'
- en: '| GAP (Poursaeed et al., [2018](#bib.bib105)) | 2018 | T&U | $\square$ | GAN
    | $L_{2},L_{\infty}$ | Efficient | Slight | 74.1% | UAP and img-dependent |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GAP (Poursaeed 等，[2018](#bib.bib105)) | 2018 | T&U | $\square$ | GAN | $L_{2},L_{\infty}$
    | 高效 | 轻微 | 74.1% | UAP 和图像依赖 |'
- en: '| AdvAttGAN(Joshi et al., [2019](#bib.bib61)) | 2019 | U | $\square$ | GAN
    | $-$ | Costly | Marked | 98.0% | Modify facial attribute |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| AdvAttGAN (Joshi 等，[2019](#bib.bib61)) | 2019 | U | $\square$ | GAN | $-$
    | 成本高 | 标记 | 98.0% | 修改面部属性 |'
- en: '| SemAdv (Qiu et al., [2020](#bib.bib107)) | 2020 | T | $\square$,$\blacksquare$
    | GAN | $-$ | Costly | Marked | 67.7% | Modify visual attribute |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| SemAdv (Qiu 等，[2020](#bib.bib107)) | 2020 | T | $\square$,$\blacksquare$
    | GAN | $-$ | 成本高 | 标记 | 67.7% | 修改视觉属性 |'
- en: '| SparseGAN (He et al., [2022b](#bib.bib49)) | 2022 | U | $\square$ | GAN |
    $L_{0}$ | Efficient | Invisible | 58.4% | Perturbation decoupling |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SparseGAN (He 等，[2022b](#bib.bib49)) | 2022 | U | $\square$ | GAN | $L_{0}$
    | 高效 | 隐形 | 58.4% | 扰动解耦 |'
- en: '| Manitest (Fawzi and Frossard, [2015](#bib.bib37)) | 2015 | U | $\square$
    | Spatial Trans. | Geodesic | Costly | Invisible | 35.6% | Geodesics on manifold
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Manitest (Fawzi 和 Frossard，[2015](#bib.bib37)) | 2015 | U | $\square$ | 空间变换
    | 测地线 | 成本高 | 隐形 | 35.6% | 测地线上的流形 |'
- en: '| SimpleTrans (Engstrom et al., [2018](#bib.bib35)) | 2017 | U | $\square$  $\blacksquare$
    | Spatial Trans. | $-$ | Medium | Invisible | 90.0% | Simple transformation |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| SimpleTrans (Engstrom 等，[2018](#bib.bib35)) | 2017 | U | $\square$  $\blacksquare$
    | 空间变换 | $-$ | 中等 | 隐形 | 90.0% | 简单变换 |'
- en: '| Manifool (Kanbak et al., [2018](#bib.bib62)) | 2018 | U | $\square$ | Spatial
    Trans. | Geodesic | Efficient | Invisible | 75.0% | Iterative method |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Manifool (Kanbak 等，[2018](#bib.bib62)) | 2018 | U | $\square$ | 空间变换 | 测地线
    | 高效 | 隐形 | 75.0% | 迭代方法 |'
- en: '| stAdv (Xiao et al., [2018b](#bib.bib152)) | 2018 | T | $\square$ | Spatial
    Trans. | T.V. | Efficient | Slight | 99.6% | Flow field |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| stAdv (Xiao 等，[2018b](#bib.bib152)) | 2018 | T | $\square$ | 空间变换 | T.V.
    | 高效 | 轻微 | 99.6% | 流场 |'
- en: '| Adef (Alaifari et al., [2018](#bib.bib4)) | 2019 | T | $\square$ | Spatial
    Trans. | $L_{2}$ | Medium | Invisible | 99.0% | Iterative deforming |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Adef (Alaifari 等，[2018](#bib.bib4)) | 2019 | T | $\square$ | 空间变换 | $L_{2}$
    | 中等 | 隐形 | 99.0% | 迭代变形 |'
- en: '| 3DRender (Zeng et al., [2019](#bib.bib159)) | 2019 | U | $\square$ | Spatial
    Trans. | $L_{2}$ | Costly | Marked | 90.7% | 3D renderer |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 3DRender (Zeng 等，[2019](#bib.bib159)) | 2019 | U | $\square$ | 空间变换 | $L_{2}$
    | 成本高 | 标记 | 90.7% | 3D渲染器 |'
- en: '| PSI (Zheng et al., [2019a](#bib.bib174)) | 2019 | U | $\square$ | Spatial
    Trans. | Wasserstein | Efficient | Invisible | 91.7% | Wasserstein distance |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| PSI (Zheng et al., [2019a](#bib.bib174)) | 2019 | U | $\square$ | 空间变换 |
    Wasserstein | 高效 | 隐形 | 91.7% | Wasserstein 距离 |'
- en: '| EdgeFool (Shamsabadi et al., [2020a](#bib.bib117)) | 2020 | U | $\square$
    | Spatial Trans. | Smooth | Costly | Marked | 99.0% | Image enhancement |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| EdgeFool (Shamsabadi et al., [2020a](#bib.bib117)) | 2020 | U | $\square$
    | 空间变换 | 光滑 | 成本高 | 标记 | 99.0% | 图像增强 |'
- en: '| Chroma-shift (Aydin et al., [2021](#bib.bib8)) | 2021 | T&U | $\square$ |
    Spatial Trans. | $-$ | Efficient | Invisible | 96.1% | YUV colorspace |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Chroma-shift (Aydin et al., [2021](#bib.bib8)) | 2021 | T&U | $\square$ |
    空间变换 | $-$ | 高效 | 隐形 | 96.1% | YUV 颜色空间 |'
- en: '| FilterFool (Shamsabadi et al., [2021](#bib.bib118)) | 2021 | U | $\square$
    | Spatial Trans. | SSIM | Costly | Slight | 48.3% | Mimic filter |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| FilterFool (Shamsabadi et al., [2021](#bib.bib118)) | 2021 | U | $\square$
    | 空间变换 | SSIM | 成本高 | 轻微 | 48.3% | 模仿滤波器 |'
- en: '| Semantic (Hosseini and Poovendran, [2018](#bib.bib51)) | 2018 | U | $\square$
    | Color Trans. | $-$ | Costly | Marked | 94.3% | HSV colorspace |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Semantic (Hosseini and Poovendran, [2018](#bib.bib51)) | 2018 | U | $\square$
    | 颜色变换 | $-$ | 成本高 | 标记 | 94.3% | HSV 颜色空间 |'
- en: '| Blind-Spot (Zhang et al., [2018a](#bib.bib163)) | 2019 | U | $\square$ |
    Color Trans. | kNN | Efficient | Marked | 100.0% | Blind Spot |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Blind-Spot (Zhang et al., [2018a](#bib.bib163)) | 2019 | U | $\square$ |
    颜色变换 | kNN | 高效 | 标记 | 100.0% | 盲点 |'
- en: '| ReColorAdv (Laidlaw and Feizi, [2019](#bib.bib68)) | 2019 | U | $\square$
    | Color Trans. | Smooth | Medium | Slight | 97.0% | RGB/CIELUV colorspace |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| ReColorAdv (Laidlaw and Feizi, [2019](#bib.bib68)) | 2019 | U | $\square$
    | 颜色变换 | 光滑 | 中等 | 轻微 | 97.0% | RGB/CIELUV 颜色空间 |'
- en: '| cAdv&tAdv (Bhattad et al., [2019](#bib.bib11)) | 2020 | T | $\square$ | Color
    Trans. | $-$ | Medium | Marked | 99.7% | Colorization and texture |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| cAdv&tAdv (Bhattad et al., [2019](#bib.bib11)) | 2020 | T | $\square$ | 颜色变换
    | $-$ | 中等 | 标记 | 99.7% | 颜色化和纹理 |'
- en: '| PerC attack(Zhao et al., [2020a](#bib.bib172)) | 2020 | U | $\square$ | Color
    Trans. | CIEDE2000 | Efficient | Invisible | 100.0% | Perceptual color distance
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| PerC attack(Zhao et al., [2020a](#bib.bib172)) | 2020 | U | $\square$ | 颜色变换
    | CIEDE2000 | 高效 | 隐形 | 100.0% | 感知颜色距离 |'
- en: '| ColorFool (Shamsabadi et al., [2020b](#bib.bib119)) | 2020 | U | $\square$,$\blacksquare$
    | Color Trans. | $-$ | Costly | Marked | 95.90% | Adversarial colorization |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| ColorFool (Shamsabadi et al., [2020b](#bib.bib119)) | 2020 | U | $\square$,$\blacksquare$
    | 颜色变换 | $-$ | 成本高 | 标记 | 95.90% | 对抗颜色化 |'
- en: '*'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: 'This column is the adversarial knowledge of different attacks. $\square$: white-box.
    $\blacksquare$: black-box. ${\color[rgb]{.5,.5,.5}\blacksquare}$: gray-box.'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '本栏目介绍了不同攻击的对抗知识。$\square$: 白盒。$\blacksquare$: 黑盒。${\color[rgb]{.5,.5,.5}\blacksquare}$:
    灰盒。'
- en: '**'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**'
- en: We only count the best result of the most difficult attack reported in the papers.
    E.g. for works that have both white-box and black-box attacks, we only report
    the latter, for it is more difficult. For works with both untargeted and targeted
    attacks, we report the latter for the same reason.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们仅统计论文中报告的最困难攻击的最佳结果。例如，对于同时具有白盒和黑盒攻击的工作，我们只报告后者，因为它更具挑战性。对于同时具有非定向和定向攻击的工作，我们也报告后者，原因相同。
- en: 5.2\. White-box adversarial attack for 2D deep learning models
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 2D 深度学习模型的白盒对抗攻击
- en: Table [2](#S5.T2 "Table 2 ‣ 5.1.4\. Classifying according to the definition
    of perturbation ‣ 5.1\. Catalog of 2D adversarial attacks ‣ 5\. Adversarial attacks
    for 2D deep learning models ‣ A Survey of Robustness and Safety of 2D and 3D Deep
    Learning Models Against Adversarial Attacks") summarizes recent year’s 2D adversarial
    attacks for the white-box model. We divide them into the following categories
    according to their specific attack methodologies.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S5.T2 "Table 2 ‣ 5.1.4\. 根据扰动定义进行分类 ‣ 5.1\. 2D 对抗攻击目录 ‣ 5\. 2D 深度学习模型的对抗攻击
    ‣ 2D 和 3D 深度学习模型对抗攻击的鲁棒性和安全性调查") 总结了近年来针对白盒模型的 2D 对抗攻击。我们根据具体的攻击方法将其划分为以下几类。
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Optimization-based attacks. This kind of attack usually describe finding AEs
    as objective optimization problems and solve these problems by existing or self-defined
    objective optimization method, include L-BFGS (Szegedy et al., [2013](#bib.bib129)),
    UAP (Moosavi-Dezfooli et al., [2017](#bib.bib92)), C&W (Carlini and Wagner, [2017](#bib.bib18)),
    EAD (Chen et al., [2018b](#bib.bib22)) and OptMargin (He et al., [2018](#bib.bib47)),
    etc.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于优化的攻击。这类攻击通常将寻找对抗样本描述为目标优化问题，并通过现有或自定义的目标优化方法解决这些问题，包括 L-BFGS (Szegedy et al.,
    [2013](#bib.bib129))、UAP (Moosavi-Dezfooli et al., [2017](#bib.bib92))、C&W (Carlini
    and Wagner, [2017](#bib.bib18))、EAD (Chen et al., [2018b](#bib.bib22)) 和 OptMargin
    (He et al., [2018](#bib.bib47)) 等。
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fast-gradient-based attacks. Instead of reaching a local minimum through optimizing
    an objective, this kind of attack finds adversarial examples through direct and
    explicit gradient computation. Therefore, they can usually find an adversarial
    example very quickly, although the perturbation may not be the optimum. Moreover,
    because these attacks lack clear objective functions, they are usually untargeted.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于快速梯度的攻击。与通过优化目标函数来达到局部最小值不同，这种攻击通过直接和显式的梯度计算找到对抗样本。因此，尽管扰动可能不是最优的，但它们通常可以非常快速地找到对抗样本。此外，由于这些攻击缺乏明确的目标函数，它们通常是无目标的。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GAN-based attacks. Instead of optimizing the noises through gradient descent,
    this kind of attack generates the perturbation through the generative adversarial
    network (GAN) and optimizes the variable in the latent space. For example, unrestricted
    AE (Song et al., [2018](#bib.bib125)) can generate semantic adversarial examples
    from drafts.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于GAN的攻击。与通过梯度下降优化噪声不同，这种攻击通过生成对抗网络（GAN）生成扰动，并优化潜在空间中的变量。例如，无限制自编码器（Song等，[2018](#bib.bib125)）可以从草稿中生成语义对抗样本。
- en: •
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Spatial-transformation-based attacks. Unlike additive noises, this attack utilizes
    global or local spatial transformations to generate adversarial examples. The
    former includes Manifool (Kanbak et al., [2018](#bib.bib62)) etc. and the latter
    includes stAdv (Xiao et al., [2018b](#bib.bib152)) etc. This kind of attack usually
    uses semantic loss rather than p-norm-based distance metrics.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于空间变换的攻击。与加性噪声不同，这种攻击利用全局或局部空间变换来生成对抗样本。前者包括Manifool（Kanbak et al.，[2018](#bib.bib62)）等，后者包括stAdv（Xiao
    et al.，[2018b](#bib.bib152)）等。这种攻击通常使用语义损失而不是p-范数距离度量。
- en: •
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Colorization-transformation-based attacks. Some works also transform images
    into different color spaces and manipulate the color space instead because human
    beings prefer to classify objects according to their shapes rather than colors.
    These attacks include ColorFool (Shamsabadi et al., [2020b](#bib.bib119)), Chroma-shift
    (Aydin et al., [2021](#bib.bib8)) and SemanticAdv (Hosseini and Poovendran, [2018](#bib.bib51)),
    etc.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于颜色化变换的攻击。一些研究还将图像转换为不同的颜色空间，并操作颜色空间，因为人类更倾向于根据形状而不是颜色来分类对象。这些攻击包括ColorFool（Shamsabadi
    et al.，[2020b](#bib.bib119)）、Chroma-shift（Aydin et al.，[2021](#bib.bib8)）和SemanticAdv（Hosseini
    and Poovendran，[2018](#bib.bib51)）等。
- en: Some attacks also combine several different methods. For example, Zhao et al.
    (Zhao et al., [2019](#bib.bib169)) combined transformation and pixel-wise perturbations
    to enhance the attack strength. However, most attacks adopt one of the above methods
    as their main methodology. Therefore, we will introduce these attacks according
    to their primary methodology.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击还结合了几种不同的方法。例如，赵等人（Zhao et al.，[2019](#bib.bib169)）结合了变换和逐像素扰动来增强攻击强度。然而，大多数攻击采用上述方法中的一种作为其主要方法。因此，我们将根据这些攻击的主要方法进行介绍。
- en: 5.2.1\. Fast-gradient-based attacks
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 基于快速梯度的攻击
- en: The greatest difference between optimization-based and fast-gradient-based attacks
    is that the latter can quickly find adversarial examples in one or several iterations.
    However, they usually cannot reach a local optimum and need larger perturbation
    to mislead the models. Because of their high efficiency in finding adversarial
    examples, they can better combine with the adversarial training process to train
    robust neural networks.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 优化基础攻击和基于快速梯度攻击的最大区别在于后者可以在一轮或几轮迭代中迅速找到对抗样本。然而，它们通常无法达到局部最优，需要较大的扰动来误导模型。由于在寻找对抗样本方面的高效率，它们可以更好地与对抗训练过程结合，以训练稳健的神经网络。
- en: Single-step fast gradient methods
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单步快速梯度方法
- en: In 2015, Goodfellow et al. (Goodfellow et al., [2014](#bib.bib42)) conjectured
    that the AE exists because of the linear representation of features in high-level
    space and proposed gradient-based one-step attack method named fast gradient sign
    method (FGSM), which estimates adversarial examples by $x^{\prime}=x+\epsilon
    sign(\bigtriangledown_{x}\mathcal{L}_{\mathcal{F}}(x,y)).$ Moreover, they embedded
    FGSM into adversarial training to mine hard examples and train robust models.
    The loss function of FGSM-based adversarial training is
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年，Goodfellow等人（Goodfellow et al.，[2014](#bib.bib42)）猜测对抗样本的存在是由于特征在高层空间中的线性表示，并提出了一种名为快速梯度符号方法（FGSM）的基于梯度的单步攻击方法，该方法通过$x^{\prime}=x+\epsilon
    sign(\bigtriangledown_{x}\mathcal{L}_{\mathcal{F}}(x,y))$来估计对抗样本。此外，他们将FGSM嵌入对抗训练中以挖掘困难样本并训练稳健的模型。FGSM基础对抗训练的损失函数是
- en: '| (3) |  | $\widetilde{\mathcal{L}}_{\mathcal{F}}=\lambda\mathcal{L}_{\mathcal{F}}(x,y)+(1-\lambda)\mathcal{L}_{\mathcal{F}}(x+\epsilon
    sign(\bigtriangledown_{x}\mathcal{L}_{\mathcal{F}}(x,y)),y).$ |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\widetilde{\mathcal{L}}_{\mathcal{F}}=\lambda\mathcal{L}_{\mathcal{F}}(x,y)+(1-\lambda)\mathcal{L}_{\mathcal{F}}(x+\epsilon
    sign(\bigtriangledown_{x}\mathcal{L}_{\mathcal{F}}(x,y)),y).$ |  |'
- en: The loss function has two parts. The first part is the same as the normal training
    procedure, while the second part considers the negative influence of adversarial
    examples.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数包含两个部分。第一部分与正常训练过程相同，而第二部分考虑了对抗样本的负面影响。
- en: Multi-step fast gradient methods
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多步快速梯度方法
- en: Kurakin et al. (Kurakin et al., [2018](#bib.bib66)) extended the FGSM method
    to an iterative method (BIM). At the end of each step, they clipped the image
    into [0,1]. They also proposed LLCM, which uses the least likely label as the
    target label. In addition, they tried to attack the camera through printed images.
    but their attacks suffered low ASRs in the physical setting, for they lacked consideration
    of fabrication distortion.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Kurakin 等人 (Kurakin et al., [2018](#bib.bib66)) 将 FGSM 方法扩展为一种迭代方法 (BIM)。在每一步结束时，他们将图像裁剪到
    [0,1] 范围内。他们还提出了 LLCM 方法，该方法使用最不可能的标签作为目标标签。此外，他们尝试通过打印图像攻击相机，但由于缺乏对制作失真情况的考虑，他们的攻击在实际环境中
    ASR 较低。
- en: Moosavi et al. (Moosavi-Dezfooli et al., [2016](#bib.bib93)) proposed Deepfool
    attack, which iteratively estimates the distance from the normal samples to the
    classifying hyperplane. Because of the nonlinearity of the classification boundary,
    they linearize the hyperplane at each iteration. If the model $\mathcal{F}(x)$
    is a binary classifier, at each iteration, $x$ is updated by $x^{t+1}\leftarrow
    x^{t}-\frac{\mathcal{F}(x^{t})}{\left\|{\bigtriangledown\mathcal{F}(x^{t})}\right\|_{2}^{2}}\bigtriangledown\mathcal{F}(x^{t})$.
    If the model $\mathcal{F}$ is a multiclass differentiable classifier, they use
    a polyhedron $\widetilde{P}_{i}$ to approximate the label space. Deepfool is also
    untargeted but can achieve smaller perturbations than the FGSM attack.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Moosavi 等人 (Moosavi-Dezfooli et al., [2016](#bib.bib93)) 提出了 Deepfool 攻击，该攻击通过迭代估计正常样本到分类超平面的距离。由于分类边界的非线性，他们在每次迭代中线性化超平面。如果模型
    $\mathcal{F}(x)$ 是一个二分类器，在每次迭代中，$x$ 的更新方式为 $x^{t+1}\leftarrow x^{t}-\frac{\mathcal{F}(x^{t})}{\left\|{\bigtriangledown\mathcal{F}(x^{t})}\right\|_{2}^{2}}\bigtriangledown\mathcal{F}(x^{t})$。如果模型
    $\mathcal{F}$ 是一个多类可微分类器，他们使用多面体 $\widetilde{P}_{i}$ 来逼近标签空间。Deepfool 也是非目标型攻击，但能比
    FGSM 攻击实现更小的扰动。
- en: Madry et. al. (Madry et al., [2017](#bib.bib87)) proposed projection gradient
    descent (PGD), which projects $x_{t}$ onto the neighborhood of the input at the
    end of each iteration through $x^{t+1}=\textit{proj}_{x+\mathcal{S}}(x^{t}+\epsilon\,sign(\bigtriangledown_{x}L(\theta,x,y)))$.
    Osadchy et al. (Osadchy et al., [2017](#bib.bib99)) also proposed a similar method
    called I-FGSM and apply it to the image CAPTCHAs to distinguish between computers
    and humans.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Madry 等人 (Madry et al., [2017](#bib.bib87)) 提出了投影梯度下降 (PGD) 方法，该方法通过 $x^{t+1}=\textit{proj}_{x+\mathcal{S}}(x^{t}+\epsilon\,sign(\bigtriangledown_{x}L(\theta,x,y)))$
    在每次迭代结束时将 $x_{t}$ 投影到输入的邻域中。Osadchy 等人 (Osadchy et al., [2017](#bib.bib99)) 也提出了类似的方法，称为
    I-FGSM，并将其应用于图像 CAPTCHA，以区分计算机和人类。
- en: Tramer et al. (Tramèr et al., [2017a](#bib.bib131)) found that the steep curvature
    artifacts near the inputs may reduce the attack strength of single-step attack.
    Therefore, they proposed R-FGSM, which introduces random noises into the input
    images. The update formulation is $x^{adv}=x^{\prime}+(\epsilon-\alpha)\cdot sign(\bigtriangledown_{x^{\prime}}\mathcal{L}(x^{\prime},y_{true}))$,
    where $x^{\prime}=x+\alpha\cdot sign(\mathcal{N}(\textbf{0}^{m},\textbf{I}^{m}))$,
    $\mathcal{N}(\textbf{0}^{m},\textbf{I}^{m})$ is the standard normal distribution.
    R-FGSM attack can be regarded as a single-step variant of the PGD attack.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Tramer 等人 (Tramèr et al., [2017a](#bib.bib131)) 发现，输入处的陡峭曲率伪影可能会降低单步攻击的攻击强度。因此，他们提出了
    R-FGSM 方法，该方法在输入图像中引入随机噪声。更新公式为 $x^{adv}=x^{\prime}+(\epsilon-\alpha)\cdot sign(\bigtriangledown_{x^{\prime}}\mathcal{L}(x^{\prime},y_{true}))$，其中
    $x^{\prime}=x+\alpha\cdot sign(\mathcal{N}(\textbf{0}^{m},\textbf{I}^{m}))$，$\mathcal{N}(\textbf{0}^{m},\textbf{I}^{m})$
    是标准正态分布。R-FGSM 攻击可以视为 PGD 攻击的单步变体。
- en: MI-FSGM (Dong et al., [2018](#bib.bib32)) introduced a momentum term into the
    iterative FGSM and achieved better performance than FGSM and BIM in the untargeted
    attack. It firstly updated the momentum by $g_{t+1}=\mu\cdot g_{t}+\frac{{\bigtriangledown}_{x}\mathcal{L}(x_{t}^{\prime},y)}{\left\|{\bigtriangledown}_{x}\mathcal{L}(x_{t}^{\prime},y)\right\|_{1}}$,
    then updated the adversarial example $x_{t}^{\prime}$ by $x_{t+1}^{\prime}=x_{t}^{\prime}+\alpha\cdot
    sign(g_{t+1})$. By generating a velocity vector in the direction of gradient descent,
    MI-FGSM alleviates the problem of unstableness after multi-step iterations and
    enhances the transferability of AEs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: MI-FSGM（Dong 等，[2018](#bib.bib32)）在迭代 FGSM 中引入了动量项，并在无目标攻击中比 FGSM 和 BIM 取得了更好的性能。它首先通过
    $g_{t+1}=\mu\cdot g_{t}+\frac{{\bigtriangledown}_{x}\mathcal{L}(x_{t}^{\prime},y)}{\left\|{\bigtriangledown}_{x}\mathcal{L}(x_{t}^{\prime},y)\right\|_{1}}$
    更新动量，然后通过 $x_{t+1}^{\prime}=x_{t}^{\prime}+\alpha\cdot sign(g_{t+1})$ 更新对抗样本 $x_{t}^{\prime}$。通过生成沿梯度下降方向的速度向量，MI-FGSM
    缓解了多步迭代后的不稳定性问题，并增强了对抗样本的迁移性。
- en: M-DI2-FGSM (Xie et al., [2019](#bib.bib155)) integrated random transformations
    into the MI-FGSM to improve the adversarial example’s transferability. Random
    resize and padding are used as the main transforms, and the transferability is
    increased by about 20% on the ImageNet dataset. However, this method may cause
    the loss to decline unstably. Dong et al. (Dong et al., [2019](#bib.bib33)) also
    proposed a similar attack named TI-BIM based on BIM and transformation. Because
    the gradient of transformed images equals transforming the gradient of original
    examples, they multiply the gradient of original images with a uniform, linear
    or Gaussian kernel at each iteration to represent different translations.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: M-DI2-FGSM（Xie 等，[2019](#bib.bib155)）将随机变换集成到 MI-FGSM 中，以提高对抗样本的迁移性。随机缩放和填充作为主要变换，迁移性在
    ImageNet 数据集上提高了约 20%。然而，该方法可能导致损失不稳定下降。Dong 等（Dong 等，[2019](#bib.bib33)）还提出了一种类似的攻击，名为
    TI-BIM，基于 BIM 和变换。由于变换图像的梯度等于变换原始样本的梯度，他们在每次迭代中将原始图像的梯度与均匀、线性或高斯核相乘，以表示不同的变换。
- en: Rony et al. (Rony et al., [2019](#bib.bib111)) proposed Decoupling Direction
    and Norm attack (DDN) based on PGD. At each iteration, the perturbation $\delta_{k}$
    is projected to an $\epsilon_{k}$-ball around $x$ through $x_{k}^{\prime}\leftarrow
    x+\epsilon_{k}\frac{\delta_{k}}{\left\|\delta_{k}\right\|_{2}}$. Different from
    PGD that sets $\epsilon_{k}$ as a constant, they increase the $\epsilon_{k}$ at
    each iteration if $x_{k}^{\prime}$ is still not adversarial and reduce the $\epsilon_{k}$
    on the other hand. Their method achieved competitive results compared with state-of-art
    $L_{2}$-norm attacks and needed fewer iterations.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Rony 等（Rony 等，[2019](#bib.bib111)）提出了基于 PGD 的解耦方向和范数攻击（DDN）。在每次迭代中，扰动 $\delta_{k}$
    通过 $x_{k}^{\prime}\leftarrow x+\epsilon_{k}\frac{\delta_{k}}{\left\|\delta_{k}\right\|_{2}}$
    被投影到 $x$ 周围的 $\epsilon_{k}$-球内。不同于将 $\epsilon_{k}$ 设置为常量的 PGD，如果 $x_{k}^{\prime}$
    仍未对抗，他们在每次迭代中增加 $\epsilon_{k}$，另一方面则减少 $\epsilon_{k}$。他们的方法在与最先进的 $L_{2}$-范数攻击相比时取得了有竞争力的结果，并且需要的迭代次数较少。
- en: The relationships between different fast-gradient-based adversarial attacks
    are shown in Figure.[3](#S5.F3 "Figure 3 ‣ Multi-step fast gradient methods ‣
    5.2.1\. Fast-gradient-based attacks ‣ 5.2\. White-box adversarial attack for 2D
    deep learning models ‣ 5\. Adversarial attacks for 2D deep learning models ‣ A
    Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial
    Attacks").
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 不同基于快速梯度的对抗攻击之间的关系如图 [3](#S5.F3 "图 3 ‣ 多步快速梯度方法 ‣ 5.2.1\. 基于快速梯度的攻击 ‣ 5.2\.
    2D 深度学习模型的白盒对抗攻击 ‣ 5\. 2D 深度学习模型的对抗攻击 ‣ 2D 和 3D 深度学习模型对抗攻击的鲁棒性与安全性调查") 所示。
- en: '![Refer to caption](img/cb439d2c5ca793e3b95e3d01a6870fad.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cb439d2c5ca793e3b95e3d01a6870fad.png)'
- en: Figure 3\. The relationships between different fast-gradient-based 2D adversarial
    attacks
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 不同基于快速梯度的 2D 对抗攻击之间的关系
- en: \Description
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: The relationships between different fast-gradient-based attacks
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 不同基于快速梯度的攻击之间的关系
- en: 5.2.2\. Optimization-based attacks
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 基于优化的攻击
- en: Optimization-based attacks usually model the adversarial as optimization objectives
    and then leverage existing optimization methods or design custom optimization
    methods to solve these optimization problems. Moreover, most optimization-based
    attacks are based on $L_{p}$-norm distance. Different distance metrics can generate
    different kinds of perturbations. $L_{2}$ and $L_{\infty}$ distances usually generate
    uniform and dense perturbation, while $L_{0}$ and $L_{1}$ distances tend to generate
    sparse perturbation. Moreover, Besides image-independent attacks, universal perturbation
    has also attracted much research interest. Therefore, according to the different
    kinds of perturbations, we divide optimization-based attacks into dense, sparse,
    and universal perturbations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优化的攻击通常将对抗样本建模为优化目标，然后利用现有的优化方法或设计自定义优化方法来解决这些优化问题。此外，大多数基于优化的攻击都是基于 $L_{p}$-范数距离。不同的距离度量可以生成不同类型的扰动。$L_{2}$
    和 $L_{\infty}$ 距离通常生成均匀和密集的扰动，而 $L_{0}$ 和 $L_{1}$ 距离倾向于生成稀疏的扰动。此外，除了图像无关攻击外，通用扰动也引起了大量研究兴趣。因此，根据不同类型的扰动，我们将基于优化的攻击分为密集、稀疏和通用扰动。
- en: Dense perturbation.
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 密集扰动。
- en: When the concept of adversarial example is first proposed, it is defined as
    minimizing the total perturbation in $L_{2}$ or $L_{\infty}$ distance because
    they are easier to compute gradient than other distances. These metrics lead to
    uniform perturbation on the whole image. The representative attacks include L-BFGS
    (Szegedy et al., [2013](#bib.bib129)), C&W (Carlini and Wagner, [2017](#bib.bib18)),
    etc.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当对抗样本的概念首次提出时，它被定义为最小化 $L_{2}$ 或 $L_{\infty}$ 距离中的总扰动，因为它们比其他距离更容易计算梯度。这些度量会导致整个图像的均匀扰动。代表性的攻击包括
    L-BFGS（Szegedy et al., [2013](#bib.bib129)）、C&W（Carlini 和 Wagner, [2017](#bib.bib18)）等。
- en: The concept of adversarial example was proposed by Szegedy et al. (Szegedy et al.,
    [2013](#bib.bib129)) in 2013\. They formulated the problem of finding adversarial
    examples as a box-constrained optimization problem under $l_{2}$ distance and
    utilized L-BFGS algorithm to find an approximate solution. In detail, for an input
    image $x\in\mathbb{R}^{m}$, they minimize $c\left\|{\delta}\right\|_{2}+\mathcal{L}_{\mathcal{F}}(x+\delta,y^{\prime}),\text{s.t.
    }x+\delta\in[0,1]^{m}$, in which $\mathcal{L}_{\mathcal{F}}(\cdot,\cdot)$ is a
    cross-entropy function. In addition, they also analyzed the upper Lipschitz constant
    of the model and proposed that the regularization of the model parameters may
    avoid the existence of adversarial examples. However, experiments show that L-BFGS
    has the weakness of inefficiency.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本的概念是由 Szegedy 等人（Szegedy et al., [2013](#bib.bib129)）在 2013 年提出的。他们将寻找对抗样本的问题表述为一个基于
    $l_{2}$ 距离的箱约束优化问题，并利用 L-BFGS 算法来寻找近似解。具体来说，对于一个输入图像 $x\in\mathbb{R}^{m}$，他们最小化
    $c\left\|{\delta}\right\|_{2}+\mathcal{L}_{\mathcal{F}}(x+\delta,y^{\prime}),\text{s.t.
    }x+\delta\in[0,1]^{m}$，其中 $\mathcal{L}_{\mathcal{F}}(\cdot,\cdot)$ 是交叉熵函数。此外，他们还分析了模型的上界
    Lipschitz 常数，并提出模型参数的正则化可能会避免对抗样本的存在。然而，实验表明 L-BFGS 存在低效的弱点。
- en: Defensive distillation (Papernot et al., [2016c](#bib.bib104)) improved the
    model robustness by training models on soft training labels to reduce the network’s
    Jacobian matrix, making the model less sensitive to the input and avoiding overfitting
    against any samples. C&W (Carlini and Wagner, [2017](#bib.bib18)) designed an
    alternative loss to break defensive distillation, that is $max(max({Z_{2}(x^{\prime})_{i:i\neq
    y^{\prime}}})-Z_{2}(x^{\prime})_{y^{\prime}},-\kappa),$ where $\kappa$ is a parameter
    to control the strength of AE. Moreover, some former attacks, like BIM (Kurakin
    et al., [2018](#bib.bib66)), directly clip $x^{\prime}$ into $[0,1]^{m}$, which
    may result in zero gradients. C&W changed the optimized variable from $\delta$
    to $w\coloneqq arctanh(\delta)$. Because $tanh$ is a bounded and differential
    function, C&W can remove the box constraint and use Adam optimizer.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 防御蒸馏（Papernot et al., [2016c](#bib.bib104)）通过在软标签上训练模型来提高模型的鲁棒性，从而减少网络的雅可比矩阵，使模型对输入的敏感度降低，避免对任何样本的过拟合。C&W（Carlini
    和 Wagner, [2017](#bib.bib18)）设计了一种替代损失函数来打破防御蒸馏，即 $max(max({Z_{2}(x^{\prime})_{i:i\neq
    y^{\prime}}})-Z_{2}(x^{\prime})_{y^{\prime}},-\kappa),$ 其中 $\kappa$ 是控制对抗样本强度的参数。此外，一些早期攻击方法，如
    BIM（Kurakin et al., [2018](#bib.bib66)），直接将 $x^{\prime}$ 裁剪到 $[0,1]^{m}$，这可能导致梯度为零。C&W
    将优化变量从 $\delta$ 改为 $w\coloneqq arctanh(\delta)$。由于 $tanh$ 是一个有界且可微分的函数，C&W 可以去除箱约束并使用
    Adam 优化器。
- en: Gradient mask (Papernot et al., [2017](#bib.bib101)) prevented adversarial attack
    through shattered, randomized, or vanishing/exploding gradients. Athalye et al.
    (Athalye et al., [2018a](#bib.bib6)) claimed that gradient masks also cannot provide
    enough robustness. They proposed BPDA, EOT, and Reparameterization attacks to
    break these defenses, respectively. Take BPDA as an example. Gradient shattering
    introduces a non-differentiable layer $f^{i}(x)$ to prevent the adversary from
    getting the gradient. BPDA attack adopts a differentiable layer $g(x)$ to substitute
    $f^{i}(x)$ in the back-propagation process.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度掩蔽（Papernot et al., [2017](#bib.bib101)）通过破碎、随机化或消失/爆炸梯度来防止对抗攻击。Athalye et
    al.（Athalye et al., [2018a](#bib.bib6)）声称梯度掩蔽也无法提供足够的鲁棒性。他们分别提出了 BPDA、EOT 和重参数化攻击来破解这些防御。例如，BPDA
    使用了一个非可微分层 $f^{i}(x)$ 来防止对手获得梯度。BPDA 攻击在反向传播过程中用可微分层 $g(x)$ 替代了 $f^{i}(x)$。
- en: Adversarial training (Szegedy et al., [2013](#bib.bib129)) is also a popular
    defensive method. It uses adversarial examples as training data to improve model
    robustness (See Eq.[3](#S5.E3 "In Single-step fast gradient methods ‣ 5.2.1\.
    Fast-gradient-based attacks ‣ 5.2\. White-box adversarial attack for 2D deep learning
    models ‣ 5\. Adversarial attacks for 2D deep learning models ‣ A Survey of Robustness
    and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks")). Adversarial
    training through PGD (Madry et al., [2017](#bib.bib87)) has been proven effective
    in defending most first-order attacks, like FGSM and DeepFool. However, Zhang
    et al. (Zhang et al., [2018a](#bib.bib163)) found that adversarial trained networks
    may have blind spots or unusual examples. They first use kNN to measure the difference
    from the test example to the training dataset and adopt simple methods like scaling
    and shifting all pixels to find the blind-spot example of the model. Then, they
    attack these infrequent samples by C&W attack.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练（Szegedy et al., [2013](#bib.bib129)）也是一种流行的防御方法。它使用对抗样本作为训练数据以提高模型的鲁棒性（见
    Eq.[3](#S5.E3 "在单步快速梯度方法 ‣ 5.2.1. 快速梯度攻击 ‣ 5.2. 白盒对抗攻击 2D 深度学习模型 ‣ 5. 对抗攻击 2D
    深度学习模型 ‣ 2D 和 3D 深度学习模型的鲁棒性和安全性调查")）。通过 PGD（Madry et al., [2017](#bib.bib87)）的对抗训练已被证明在防御大多数一阶攻击（如
    FGSM 和 DeepFool）中有效。然而，Zhang et al.（Zhang et al., [2018a](#bib.bib163)）发现对抗训练网络可能存在盲点或异常样本。他们首先使用
    kNN 测量测试样本与训练数据集之间的差异，并采用简单的方法如缩放和移动所有像素来找到模型的盲点样本。然后，他们通过 C&W 攻击攻击这些不常见的样本。
- en: To improve the transferability of adversarial perturbation, TAP attack (Zhou
    et al., [2018](#bib.bib177)) increases the feature distance of all latent layers
    between the normal and adversarial image. In addition, they also found that applying
    a low-pass filter on the perturbation can also improve transferability. Observing
    that optimizing on an independent example cannot be globally optimal, DAA (Zheng
    et al., [2019a](#bib.bib174)) searches an adversarial data distribution that satisfies
    $L_{\infty}$ constraint while maximizing the generalization error by Wasserstein
    gradient flows. DAA can reduce the accuracy of CIFAR to 44.71% for adversarial-trained
    networks. To make the perturbation more indistinguishable, sC&W (Zhang et al.,
    [2020a](#bib.bib162)) adds Laplacian smoothing constraint on the original C&W
    attack. This constraint makes the perturbation locally smooth on the even areas
    and dissonant on the high variance area. Like the sC&W attack, SSAH (Luo et al.,
    [2022](#bib.bib83)) composes a semantic similarity loss and a low-frequency restraint.
    The former minimizes the cosine similarity of the features between the original
    and target image. The latter restrains the perturbation into the high-frequency
    regions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高对抗扰动的可迁移性，TAP 攻击（Zhou et al., [2018](#bib.bib177)）增加了正常图像和对抗图像之间所有潜在层的特征距离。此外，他们还发现对扰动应用低通滤波器也能改善可迁移性。由于优化独立样本不能全局最优，DAA（Zheng
    et al., [2019a](#bib.bib174)）通过 Wasserstein 梯度流搜索一个满足 $L_{\infty}$ 约束的对抗数据分布，同时最大化泛化误差。DAA
    可以将 CIFAR 的准确率降低到对抗训练网络的 44.71%。为了使扰动更难以区分，sC&W（Zhang et al., [2020a](#bib.bib162)）在原始
    C&W 攻击中添加了拉普拉斯平滑约束。这个约束使扰动在均匀区域局部平滑，而在高方差区域则不协调。类似于 sC&W 攻击，SSAH（Luo et al., [2022](#bib.bib83)）结合了语义相似性损失和低频约束。前者最小化原始图像和目标图像之间特征的余弦相似性。后者将扰动限制在高频区域内。
- en: Semantic segmentation and object-detecting tasks are also significant in computer
    vision. Xie et al. (Xie et al., [2017](#bib.bib154)) proposed dense adversary
    generation (DAG) attack against multi-object detection task. Observing that this
    task usually needs to classify plural targets in an image, DAG attacks all targets
    simultaneously using the iterative gradient descent. DAG can also transfer among
    different models, which is probably because it accumulates perturbations from
    multiple targets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割和目标检测任务在计算机视觉中也非常重要。Xie 等人（Xie et al., [2017](#bib.bib154)）提出了针对多目标检测任务的密集对抗生成（DAG）攻击。观察到这一任务通常需要在图像中分类多个目标，DAG
    使用迭代梯度下降同时攻击所有目标。DAG 还可以在不同模型之间转移，这可能是因为它积累了来自多个目标的扰动。
- en: Sparse perturbation.
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏扰动。
- en: In some scenarios, the number of pixels being modified is more crucial than
    the overall magnitude of the perturbation. Therefore, some researchers proposed
    generating sparse perturbations through $l_{0}$ or $l_{1}$ distance, equivalent
    to modifying as few pixels as possible. However, specific techniques are needed
    because these distances are non-differentiable. For example, JSMA (Papernot et al.,
    [2016b](#bib.bib102)) selects the most meaningful pixels by comparing the gradient
    of $Z(x)$ and adds them to the set of modified pixels one by one until the attack
    succeeds or a certain threshold is reached. EDA attack (Chen et al., [2018b](#bib.bib22))
    formulates the problem as an Elastic-net regularized optimization problem that
    combines the $l_{1}$ and $l_{2}$ distance. Because $l_{1}$ distance is non-differentiable,
    EDA applies the FISTA algorithm to solve this optimization function. The $l_{1}$-DeepFool
    (Moosavi-Dezfooli et al., [2016](#bib.bib93)) can generate sparse perturbations
    efficiently through linearizing the boundary, but it is greatly affected by the
    clipping function. To solve this problem, SparseFool (Modas et al., [2019](#bib.bib90))
    projects $x^{\prime}$ on one component of the normal vector at each iteration.
    If the projection cannot generate an adversarial example, this direction will
    be ignored in the next iteration. SparseFool iteratively estimates the minimum
    perturbation to the boundary until the predicted label is changed. RobustAdv (Luo
    et al., [2018](#bib.bib82)) generates imperceptible and robust adversarial examples
    through perception distance, which is defined as $D(X^{\prime},X)=\sum_{i=1}^{N}\delta_{i}\cdot
    Sen(x_{i})$, where $N$ is the total pixels being modified, $\delta_{i}$ is the
    perturbation of the pixel $x_{i}$, and $Sen(x_{i})$ is the perturbation sensitivity
    of $x_{i}$. Because humans are more sensitive to the perturbation in smoothing
    regions, RobustAdv defines $Sen(x_{i})$ as the reciprocal of the standard deviation
    around $x_{i}$. GreedyFool (Dong et al., [2020](#bib.bib31)) uses a two-step greedy-based
    optimization to generate sparse perturbation. Firstly, the candidate manipulation
    locations are selected according to their gradient and their distortion map generated
    by a GAN. Secondly, unnecessary points are discarded to improve the sparsity.
    There are also some black-box attacks that can generate sparse perturbation, such
    as OnePixel (Su et al., [2019](#bib.bib126)) and CornerSearch (Croce and Hein,
    [2019](#bib.bib30)) and GeoDA(Rahmati et al., [2020](#bib.bib108)). We will discuss
    them in the next subsection.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，修改的像素数量比扰动的整体大小更重要。因此，一些研究人员通过$l_{0}$或$l_{1}$距离生成稀疏扰动，目标是尽可能少地修改像素。然而，由于这些距离是不可微分的，所以需要特定的技术。例如，JSMA（Papernot等，[2016b](#bib.bib102)）通过比较$Z(x)$的梯度选择最有意义的像素，并将它们逐个添加到修改的像素集合中，直到攻击成功或达到一定的阈值。EDA攻击（Chen等，[2018b](#bib.bib22)）将问题制定为一个弹性网络正则化优化问题，结合了$l_{1}$和$l_{2}$距离。由于$l_{1}$距离不可微分，EDA应用FISTA算法来求解该优化函数。$l_{1}$-DeepFool（Moosavi-Dezfooli等，[2016](#bib.bib93)）能够通过线性化边界来高效地生成稀疏扰动，但它受到了截断函数的影响。为了解决这个问题，SparseFool（Modas等，[2019](#bib.bib90)）在每次迭代中将$x^{\prime}$投影到法向量的一个分量上。如果投影不能生成对抗性示例，则在下一次迭代中忽略该方向。SparseFool迭代地估计边界的最小扰动，直到预测标签发生改变。RobustAdv（Luo等，[2018](#bib.bib82)）通过感知距离生成不可察觉且鲁棒的对抗性示例，其中感知距离定义为$D(X^{\prime},X)=\sum_{i=1}^{N}\delta_{i}\cdot
    Sen(x_{i})$，其中$N$是修改的总像素数量，$\delta_{i}$是像素$x_{i}$的扰动，$Sen(x_{i})$是$x_{i}$的扰动敏感性。由于人类对平滑区域的扰动更为敏感，RobustAdv将$Sen(x_{i})$定义为$x_{i}$周围标准差的倒数。GreedyFool（Dong等，[2020](#bib.bib31)）使用两步贪心优化来生成稀疏扰动。首先，根据其梯度和由GAN生成的失真图选择候选操纵位置。然后，丢弃不必要的点以提高稀疏性。还有一些能够生成稀疏扰动的黑盒攻击，例如OnePixel（Su等，[2019](#bib.bib126)）和CornerSearch（Croce和Hein，[2019](#bib.bib30)）以及GeoDA（Rahmati等，[2020](#bib.bib108)）。我们将在下一小节中讨论它们。
- en: Universal perturbation.
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通用扰动
- en: Rather than optimizing the perturbation on a specific image, universal perturbation
    produces image-agnostic perturbations through optimizing perturbation over the
    whole or part of the training dataset.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与在特定图像上优化扰动不同，通用扰动通过在整个或部分训练数据集上优化扰动来生成与图像无关的扰动。
- en: In 2017, Moosavi-Dezfooli et al. (Moosavi-Dezfooli et al., [2017](#bib.bib92))
    firstly proposed the concept of universal adversarial perturbations (UAP). They
    successfully attacked most images in the validation set by aggregating the minimum
    perturbations on the training images. Experiments show that UAP attack can not
    only attack unseen images but also transfer between different models with a success
    rate of 40~60%. Later, in 2018, Khrulkov et al. (Khrulkov and Oseledets, [2018](#bib.bib64))
    proposed SV-UAP attack, which approximates universal perturbation problem as a
    (p, q)-singular problem. Suppose $J_{i}(x)$ is the Jacobian matrix of $i^{th}$-to-last
    layer. For a small vector $\delta$, $Z_{i}(x+\delta)-Z_{i}(x)\approx J_{i}(x)\delta$.
    Therefore, the problem of finding a universal perturbation can be formulated as
    $\max_{\delta}\;\sum_{x_{j}\in X_{b}}\left\|J_{i}(x_{j})\delta\right\|_{q}^{q},\;s.t.\;\left\|\delta\right\|_{p}=C$
    where $X_{b}$ is a subset of the training dataset. Then, they use the stochastic
    power method to solve this (p,q)-singular vector problem and achieved a fooling
    rate of 60% on 50000 images by using only 64 images for optimization. Mopuri et
    al. (Mopuri et al., [2018a](#bib.bib94)) proposed a generalizable data-free UAP
    attack (GD-UAP) in which the adversary does not need to access the exact training
    data. The idea is to find an image-agnostic perturbation that triggers more additional
    model activation. The propagation effect of the neural network will eventually
    lead to misclassification. They use the knowledge about the distribution of the
    training dataset to improve the fooling rate. In 2020, Zhang et al. (Zhang et al.,
    [2020b](#bib.bib160)) firstly proposed a targeted UAP attack named DF-UAP. They
    analyzed the similarity between the predicted logits of clean images and UAP using
    the Pearson correlation coefficient and found that UAP has a dominant role in
    prediction compared with clean images. Based on this discovery, they randomly
    sample the proxy dataset at each iteration to update the perturbation. Inspired
    by the success of steganography of neural networks, In 2021, Zhang et al. (Zhang
    et al., [2021](#bib.bib161)) proposed HP-UAP, which adds a high-pass filter in
    the iteration to make the UAP more invisible to human eyes with only a small decrease
    of attack success rate.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，Moosavi-Dezfooli等人（Moosavi-Dezfooli et al., [2017](#bib.bib92)）首次提出了通用对抗扰动（UAP）的概念。他们通过聚合训练图像上的最小扰动成功攻击了验证集中的大多数图像。实验表明，UAP攻击不仅可以攻击未见过的图像，还可以在不同模型之间转移，成功率达到40~60%。后来，在2018年，Khrulkov等人（Khrulkov
    and Oseledets, [2018](#bib.bib64)）提出了SV-UAP攻击，将通用扰动问题近似为一个(p, q)-奇异问题。假设$J_{i}(x)$是第$i$层的雅可比矩阵。对于一个小向量$\delta$，$Z_{i}(x+\delta)-Z_{i}(x)\approx
    J_{i}(x)\delta$。因此，寻找通用扰动的问题可以表述为$\max_{\delta}\;\sum_{x_{j}\in X_{b}}\left\|J_{i}(x_{j})\delta\right\|_{q}^{q},\;s.t.\;\left\|\delta\right\|_{p}=C$，其中$X_{b}$是训练数据集的一个子集。然后，他们使用随机幂方法解决这个(p,q)-奇异向量问题，并通过仅使用64张图像进行优化，在50000张图像上达到了60%的欺骗率。Mopuri等人（Mopuri
    et al., [2018a](#bib.bib94)）提出了一种可泛化的无数据UAP攻击（GD-UAP），在这种攻击中，对手无需访问精确的训练数据。其思想是寻找一种与图像无关的扰动，从而触发更多的模型激活。神经网络的传播效应最终将导致误分类。他们利用关于训练数据集分布的知识来提高欺骗率。在2020年，Zhang等人（Zhang
    et al., [2020b](#bib.bib160)）首次提出了一种有针对性的UAP攻击，称为DF-UAP。他们使用皮尔逊相关系数分析了干净图像与UAP之间预测logits的相似性，发现UAP在预测中相比干净图像具有主导作用。基于这一发现，他们在每次迭代时随机抽取代理数据集以更新扰动。受到神经网络隐写成功的启发，在2021年，Zhang等人（Zhang
    et al., [2021](#bib.bib161)）提出了HP-UAP，该方法在迭代过程中添加了高通滤波器，使得UAP对人眼更不易察觉，同时仅有轻微的攻击成功率下降。
- en: 5.2.3\. GAN-based attacks
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 基于GAN的攻击
- en: 'The generative adversarial network (GAN) was proposed in 2014 by Goodfellow
    and quickly became a powerful tool for learning the distribution of training datasets.
    The first related work, adversarial transformation network (ATN) (Baluja and Fischer,
    [2018](#bib.bib9)), is reported in 2017\. Its loss function is $\min_{\theta}\sum_{x\in\mathcal{X}}\beta
    L_{2}(\mathcal{G}(x),x)+L_{2}(f(\mathcal{G}(x)),r(f(x),t))$, where $r$ is a reranking
    function to maintain the logits’ ranking order except the target class. Song et
    al. (Song et al., [2018](#bib.bib125)) proposed to use GAN to generate unrestricted
    AEs from noises $z$ rather than adding perturbation. The loss function includes
    three parts. The first is to make the victim model predict $\mathcal{G}(z,y_{s})$
    as the target class. The second is to limit the search region of $z$, and the
    third is to make the auxiliary classifier predict $\mathcal{G}(z,y_{s})$ as the
    source class $y_{s}$. They also engage humans to evaluate the fidelity of AEs.
    Moreover, they proposed a noise-augmented version to improve the ASR. Konda et
    al. (Mopuri et al., [2018b](#bib.bib95)) proposed NAG-UAP, which models the distribution
    of UAP by a UAP generator. The loss function of the GAN network includes two parts:
    a fooling loss and a diversity loss. The former’s goal is to minimize the classifier’s
    output on the ground truth label, while the latter’s goal is to produce diversified
    UAPs by maximizing the gap of the hidden layer’s outputs on different UAPs. Poursaeed
    et al. (Poursaeed et al., [2018](#bib.bib105)) proposed GAP attack that generates
    both universal and image-independent noises for targeted and untargeted attacks.
    For universal attacks, they use image-to-image translation networks to generate
    universal perturbations from random noises and crop the perturbations to have
    a fixed norm. Next, the perturbation is added to the normal images and sent to
    the victim model to calculate the fooling loss. For image-independent attacks,
    the perturbations are generated from the original image instead. He et al. (He
    et al., [2022b](#bib.bib49)) utilized a generative model to improve the transferability
    of sparse perturbation. They first decoupled perturbation into a magnitude component
    and a location component. Then, they add a sparse loss onto the location component.
    Because the location component is a binary operator, they proposed a binary quantization
    operator to train the generator.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）由Goodfellow于2014年提出，并迅速成为学习训练数据集分布的强大工具。第一个相关工作，即对抗变换网络（ATN）（Baluja
    和 Fischer，[2018](#bib.bib9)），于2017年报告。其损失函数为$\min_{\theta}\sum_{x\in\mathcal{X}}\beta
    L_{2}(\mathcal{G}(x),x)+L_{2}(f(\mathcal{G}(x)),r(f(x),t))$，其中$r$是一个重新排序函数，用于保持logits的排序顺序，除了目标类别。Song
    等人（Song 等人，[2018](#bib.bib125)）提出使用GAN从噪声$z$生成无限制的对抗样本（AEs），而不是添加扰动。损失函数包括三个部分。第一个是使受害模型将$\mathcal{G}(z,y_{s})$预测为目标类别。第二个是限制$z$的搜索区域，第三个是使辅助分类器将$\mathcal{G}(z,y_{s})$预测为源类别$y_{s}$。他们还让人类评估AEs的真实性。此外，他们提出了一个噪声增强版本来提高ASR。Konda
    等人（Mopuri 等人，[2018b](#bib.bib95)）提出了NAG-UAP，通过UAP生成器建模UAP的分布。GAN网络的损失函数包括两部分：欺骗损失和多样性损失。前者的目标是最小化分类器在真实标签上的输出，而后者的目标是通过最大化不同UAP上隐藏层输出的差距来生成多样化的UAP。Poursaeed
    等人（Poursaeed 等人，[2018](#bib.bib105)）提出了GAP攻击，生成通用和图像无关的噪声用于有针对性和无针对性的攻击。对于通用攻击，他们使用图像到图像的翻译网络从随机噪声生成通用扰动，并将扰动裁剪为固定的范数。接着，将扰动添加到正常图像中，并送入受害模型计算欺骗损失。对于图像无关攻击，扰动则从原始图像中生成。He
    等人（He 等人，[2022b](#bib.bib49)）利用生成模型来提高稀疏扰动的可转移性。他们首先将扰动分解为幅度组件和位置组件。然后，他们在位置组件上添加稀疏损失。由于位置组件是二元操作符，他们提出了一种二元量化操作符来训练生成器。
- en: The emergence of conditional neural networks makes directly editing artificially
    designed properties possible (e.g. with and without eyeglasses). Joshi et al.
    (Joshi et al., [2019](#bib.bib61)) used a conditional generative model to generate
    semantic adversarial examples. Their attack optimized over the attributes space
    of the conditional generative model to manipulate semantic features like wearing
    glasses or not and different skin colors to fool the deep learning models. Qiu
    et al. (Qiu et al., [2020](#bib.bib107)) also proposed a similar method. But their
    method can choose arbitrary targets to attack. In addition, they manipulated the
    interpolated feature space instead of the attributes manifold.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 条件神经网络的出现使得直接编辑人工设计的属性成为可能（例如，戴眼镜与不戴眼镜）。Joshi 等（Joshi 等，[2019](#bib.bib61)）使用条件生成模型生成语义对抗样本。他们的攻击在条件生成模型的属性空间上优化，以操控诸如戴眼镜与否以及不同肤色等语义特征，从而欺骗深度学习模型。Qiu
    等（Qiu 等，[2020](#bib.bib107)）也提出了类似的方法。但他们的方法可以选择任意目标进行攻击。此外，他们操控了插值特征空间，而不是属性流形。
- en: 5.2.4\. Spatial-transformation-based attacks
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 基于空间变换的攻击
- en: Although deep neural networks like CNN are designed to be invariant and robust
    to transformations like translation and rotation, researchers have shown this
    is not always the case.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 CNN 这样的深度神经网络被设计为对诸如平移和旋转等变换具有不变性和鲁棒性，但研究人员已经证明，这并非总是如此。
- en: '![Refer to caption](img/0d3c69c452915e4e41b3d2ac1e0a76a4.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0d3c69c452915e4e41b3d2ac1e0a76a4.png)'
- en: (a) ManiFool
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ManiFool
- en: '![Refer to caption](img/97c6ab7f1c5897d0955d025a930fb0b3.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/97c6ab7f1c5897d0955d025a930fb0b3.png)'
- en: (b) stAdv
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: (b) stAdv
- en: Figure 4\. Comparing globally and locally transformation attacks. (a) ManiFool
    attack (Kanbak et al., [2018](#bib.bib62)) optimizes rotation and transition parameters
    on the manifolds. (b) stAdv attack (Xiao et al., [2018b](#bib.bib152)) optimizes
    the flow field on pixel coordinate space. The pixel values are viewed as a function
    of the pixel coordinates. The flow field is obtained by backpropagation from the
    adversarial loss function to pixel coordinates.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 比较全局和局部变换攻击。 (a) ManiFool 攻击（Kanbak 等，[2018](#bib.bib62)）在流形上优化旋转和过渡参数。
    (b) stAdv 攻击（Xiao 等，[2018b](#bib.bib152)）在像素坐标空间上优化流场。像素值被视为像素坐标的函数。流场通过从对抗损失函数反向传播到像素坐标来获得。
- en: \Description
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: The structure of this paper.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构。
- en: Globally spatial-transformation-based attacks
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于全局空间变换的攻击
- en: 'The first work of transformation-based attacks, Manitest (Fawzi and Frossard,
    [2015](#bib.bib37)), was proposed in 2015\. It casts the problem of computing
    the gap between converted images as the geodesics along the transformation manifold,
    which is defined as the minimum length of the curve on the manifold. Manitest
    utilizes fast marching to find adversarial transformations, which can assure minimal
    transformation on the searched grids. However, it suffers a low efficiency when
    the transformation variety increases. SimpleTrans (Engstrom et al., [2018](#bib.bib35))
    also uses simple spatial transformations to fool networks. It contains a white-box
    algorithm based on gradient descent and a black-box algorithm based on grid search.
    In 2018, Kanbak et al. (Kanbak et al., [2018](#bib.bib62)) proposed Manifool to
    analyze the transformation robustness of deep learning models, as shown in Figure.[4](#S5.F4
    "Figure 4 ‣ 5.2.4\. Spatial-transformation-based attacks ‣ 5.2\. White-box adversarial
    attack for 2D deep learning models ‣ 5\. Adversarial attacks for 2D deep learning
    models ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks") (a). They also used the geodesic distance to measure the
    two transformations’ distance. The Manifool consists of two steps: determining
    the changing direction of the image and mapping this change onto the manifold.
    Although their method is not as accurate as Manitest’s, they can achieve high
    efficiency even with operating similarity and affine transformations simultaneously.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 变换基攻击的第一个工作是 Manitest（Fawzi 和 Frossard，[2015](#bib.bib37)），它于 2015 年提出。它将计算转换图像之间的差距的问题转化为沿变换流形的测地线，定义为流形上曲线的最小长度。Manitest
    利用快速行进算法来寻找对抗性变换，这可以保证在搜索网格上的最小变换。然而，当变换种类增加时，它的效率较低。SimpleTrans（Engstrom 等，[2018](#bib.bib35)）也使用简单的空间变换来欺骗网络。它包含一个基于梯度下降的白盒算法和一个基于网格搜索的黑盒算法。2018
    年，Kanbak 等（Kanbak 等，[2018](#bib.bib62)）提出了 Manifool 来分析深度学习模型的变换鲁棒性，如图 [4](#S5.F4
    "图 4 ‣ 5.2.4\. 基于空间变换的攻击 ‣ 5.2\. 2D 深度学习模型的白盒对抗攻击 ‣ 5\. 2D 深度学习模型的对抗攻击 ‣ 2D 和
    3D 深度学习模型对抗攻击的鲁棒性和安全性调查")（a）所示。他们还使用了测地距离来度量两个变换之间的距离。Manifool 包含两个步骤：确定图像的变化方向，并将这种变化映射到流形上。尽管他们的方法不如
    Manitest 准确，但即使同时进行操作相似性和仿射变换，他们也能实现高效性。
- en: Locally spatial-transformation-based attacks
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于局部空间变换的攻击
- en: As shown in Figure.[4](#S5.F4 "Figure 4 ‣ 5.2.4\. Spatial-transformation-based
    attacks ‣ 5.2\. White-box adversarial attack for 2D deep learning models ‣ 5\.
    Adversarial attacks for 2D deep learning models ‣ A Survey of Robustness and Safety
    of 2D and 3D Deep Learning Models Against Adversarial Attacks") (b), the stAdv
    attack (Xiao et al., [2018b](#bib.bib152)) defined the transformation as a spatial
    flow field, where each flow vector corresponds to a displacement of the corresponding
    pixel. To enhance the local smoothness of the flow field to make the AE more natural,
    they define a flow cost as the total variation of the flow field. They also visualized
    the CAM attention and showed that their method can better distract attention than
    C&W methods. In 2019, Alaifari et al. (Alaifari et al., [2018](#bib.bib4)) proposed
    ADef attack inspired by Deepfool. They iteratively deform the original image through
    gradient descent to move the image over the boundary of the classification. The
    pixel’s value is modeled as a function of its coordinate. Moreover, a two-dimensional
    Gaussian filter is used to inflict smoothness on the flow field. In 2020, EdgeFool
    (Shamsabadi et al., [2020a](#bib.bib117)) generates adversarial examples through
    enhancing image details. Contrary to ColorFool (Shamsabadi et al., [2020b](#bib.bib119)),
    EdgeFool only modifies the $L$ channel and leave $a$ and $b$ channel unmodified.
    It firstly trains an FCNN network to learn a smoothing image $I_{s}$ from the
    input image $I$, and modifies the image details $I_{d}:=I-I_{s}$ to generate adversarial
    examples. Inspired by EdgeFool, FilterFool (Shamsabadi et al., [2021](#bib.bib118))
    viciously modifies a picture by imitating normal filter functions like detail
    enhancement, gamma correction, and log transformation. Their attack comprises
    an FCNN network, a traditional filter, and a classifier. The FCNN network is trained
    to generate perturbation that simultaneously fools the classifier and makes the
    perturbed image’s structure similar to the filtered image. Experiments show that
    FilterFool can improve the transferability of adversarial examples. Wong et al.
    (Wong et al., [2019](#bib.bib147)) generated AEs by Wasserstein distance instead
    of $l_{p}$ distance, which equals moving pixel mass in the images. Their attack
    follows the PGD attack but projects $x$ onto a Wasserstein ball rather than $l_{\infty}$
    ball and uses projected Sinkhorn iteration (PSI) to accelerate the computation.
    Compared to the $l_{p}$-based perturbation that indiscriminately affects both
    foreground and background pixels, Wasserstein perturbation only changes the foreground
    for images with monochrome backgrounds.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如图.[4](#S5.F4 "Figure 4 ‣ 5.2.4\. Spatial-transformation-based attacks ‣ 5.2\.
    White-box adversarial attack for 2D deep learning models ‣ 5\. Adversarial attacks
    for 2D deep learning models ‣ A Survey of Robustness and Safety of 2D and 3D Deep
    Learning Models Against Adversarial Attacks") (b) 所示，stAdv 攻击（Xiao 等，[2018b](#bib.bib152)）将变换定义为空间流场，其中每个流向量对应于相应像素的位移。为了增强流场的局部平滑性，使得对抗样本（AE）更自然，他们将流成本定义为流场的总变化量。他们还可视化了
    CAM 注意力，并展示了他们的方法能够比 C&W 方法更好地分散注意力。2019 年，Alaifari 等（Alaifari 等，[2018](#bib.bib4)）提出了受
    Deepfool 启发的 ADef 攻击。他们通过梯度下降迭代变形原始图像，以使图像超出分类边界。像素的值被建模为其坐标的函数。此外，使用二维高斯滤波器对流场施加平滑。在
    2020 年，EdgeFool（Shamsabadi 等，[2020a](#bib.bib117)）通过增强图像细节生成对抗样本。与 ColorFool（Shamsabadi
    等，[2020b](#bib.bib119)）不同，EdgeFool 仅修改 $L$ 通道，保留 $a$ 和 $b$ 通道不变。它首先训练一个 FCNN 网络，从输入图像
    $I$ 中学习一个平滑图像 $I_{s}$，并修改图像细节 $I_{d}:=I-I_{s}$ 以生成对抗样本。受 EdgeFool 启发，FilterFool（Shamsabadi
    等，[2021](#bib.bib118)）通过模仿正常的滤波器函数（如细节增强、伽马校正和对数变换）恶意修改图片。他们的攻击包括一个 FCNN 网络、一个传统滤波器和一个分类器。FCNN
    网络被训练来生成扰动，既能欺骗分类器，又能使扰动图像的结构类似于滤波后的图像。实验表明，FilterFool 可以提高对抗样本的可转移性。Wong 等（Wong
    等，[2019](#bib.bib147)）通过 Wasserstein 距离生成对抗样本，而不是 $l_{p}$ 距离，这等同于在图像中移动像素质量。他们的攻击遵循
    PGD 攻击，但将 $x$ 投影到 Wasserstein 球上，而不是 $l_{\infty}$ 球上，并使用投影 Sinkhorn 迭代（PSI）加速计算。与不加区别地影响前景和背景像素的
    $l_{p}$-based 扰动相比，Wasserstein 扰动仅改变背景为单色的图像的前景。
- en: 5.2.5\. Colorization-transformation-based attack
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5\. 基于色彩化变换的攻击
- en: Besides manipulating images in the spatial domain, some works transform images
    into different color spaces and then modify the color space instead, for humans
    prefer to classify images based on geometry rather than colors. These attacks
    define the perturbation as ”content-preserving perturbation”, which is usually
    measured by perceptual similarity (Wang et al., [2021c](#bib.bib142); Aydin et al.,
    [2021](#bib.bib8)) rather than $l_{p}$ distance.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在空间域中处理图像外，一些研究将图像转换到不同的颜色空间，然后修改颜色空间，因为人们更倾向于基于几何形状而非颜色来分类图像。这些攻击将扰动定义为“内容保持扰动”，其通常通过感知相似度（Wang
    et al., [2021c](#bib.bib142)；Aydin et al., [2021](#bib.bib8)）来衡量，而不是$l_{p}$距离。
- en: In 2018, Hosseini et al. (Hosseini and Poovendran, [2018](#bib.bib51)) first
    proposed SemanticAdv. It transforms the original image from RGB to HSV domain
    and leverages random search to modify the Hue and Saturation components. However,
    their adversarial examples may look factitious because hue and saturation components
    are modified at the same time. ColorFool (Shamsabadi et al., [2020b](#bib.bib119))
    expanded SemanticAdv to generate more realistic pictures by conducting content-based
    segmentation on target images and revising the interesting and unimportant regions,
    respectively. They modify the $a$ and $b$ channels in the $Lab$ color space and
    leave the lightness channel $L$ unchanged. In 2019, Laidlaw et al. (Laidlaw and
    Feizi, [2019](#bib.bib68)) proposed ReColorAdv attack, which computes the flow
    field in RGB or CIELUV color space. They also define a smooth loss to improve
    imperceptibility. In addition, they prove that combining the color space distortion
    with additive noises can expand the perturbation space. Zeng et al. (Zeng et al.,
    [2019](#bib.bib159)) firstly edited the 3D object’s physical properties, such
    as affine transformation, color, and illumination, and then rendered it into an
    image to fool the classifier. They optimize the physical variables by FGSM for
    derivable factors or the zeroth-order optimization approach for non-derivable
    factors. Bhattad et al. (Bhattad et al., [2019](#bib.bib11)) generated unrestricted
    AEs through semantic perturbation like colorization (cAdv) and texture transfer
    (tAdv). For cAdv attack, a pre-trained colorization network (Zhang et al., [2016](#bib.bib166))
    is used to modify the color. They attack the input hints and masks at the same
    time to make the color more realistic. For the tAdv attack, they extract the texture
    from the target image using a VGG19 network and leverage an extra restraint on
    the cross-layer gram matrices to control the texture transfer’s strength. Experiments
    show that they can generate photorealistic AEs. In 2020, Zhao et al. (Zhao et al.,
    [2020a](#bib.bib172)) proposed PerC-C&W and PerC-AL attack. Their attacks directly
    operate in the RGB color space and use CIEDE2000 as the perceptual color distance.
    PerC-AL improves the optimizing efficiency by alternatively minimizing the adversarial
    loss and perceptual loss. Experiments show that although the perturbation size
    is slightly higher than C&W method in $L_{2}$ metric, it is still imperceptible
    for human eyes and achieves better transferability. In 2012, inspired by the stAdv
    attack, Aydin et al. (Aydin et al., [2021](#bib.bib8)) proposed a more imperceptible
    attack called Spatial Chroma-shift. The basic idea is that humans are less sensitive
    to color changes than brightness changes. Therefore, they only apply the spatial
    transformation to the colorspace. They transform the color space from RGB to YUV
    and then only compute the flow field on UV channels. Moreover, they use LPIPS
    and DISTS to evaluate the perceptual similarity and show that their method outperforms
    C&W and stAdv attacks.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年，Hosseini等人（Hosseini and Poovendran, [2018](#bib.bib51)）首次提出了**SemanticAdv**。它将原始图像从RGB域转换到HSV域，并利用随机搜索来修改色调和饱和度分量。然而，由于色调和饱和度分量同时被修改，他们的对抗样本可能显得虚假。**ColorFool**（Shamsabadi等人，[2020b](#bib.bib119)）扩展了SemanticAdv，通过对目标图像进行基于内容的分割并分别修正有趣和不重要的区域，生成更真实的图像。他们在$Lab$颜色空间中修改$a$和$b$通道，保持亮度通道$L$不变。2019年，Laidlaw等人（Laidlaw
    and Feizi, [2019](#bib.bib68)）提出了**ReColorAdv**攻击，该攻击计算RGB或CIELUV颜色空间中的流场。他们还定义了一种平滑损失来提高不可察觉性。此外，他们证明了将颜色空间失真与加性噪声结合可以扩展扰动空间。Zeng等人（Zeng
    et al., [2019](#bib.bib159)）首先编辑了3D对象的物理属性，如仿射变换、颜色和光照，然后将其渲染成图像以欺骗分类器。他们通过**FGSM**优化可导因子，或通过零阶优化方法优化不可导因子。Bhattad等人（Bhattad
    et al., [2019](#bib.bib11)）通过语义扰动生成了不受限制的对抗样本，如颜色化（cAdv）和纹理迁移（tAdv）。对于cAdv攻击，使用预训练的颜色化网络（Zhang
    et al., [2016](#bib.bib166)）来修改颜色。他们同时攻击输入提示和掩膜，使颜色更加真实。对于tAdv攻击，他们使用**VGG19**网络从目标图像中提取纹理，并在跨层gram矩阵上施加额外约束，以控制纹理迁移的强度。实验表明，他们可以生成**摄影级**对抗样本。2020年，Zhao等人（Zhao
    et al., [2020a](#bib.bib172)）提出了**PerC-C&W**和**PerC-AL**攻击。他们的攻击直接在RGB颜色空间中操作，并使用CIEDE2000作为感知颜色距离。**PerC-AL**通过交替最小化对抗损失和感知损失来提高优化效率。实验表明，尽管在$L_{2}$度量中，扰动大小略高于C&W方法，但对人眼仍不可察觉，并且具有更好的迁移性。2012年，受到**stAdv**攻击的启发，Aydin等人（Aydin
    et al., [2021](#bib.bib8)）提出了一种更不可察觉的攻击称为**Spatial Chroma-shift**。基本思路是人类对颜色变化的敏感度低于对亮度变化的敏感度。因此，他们仅将空间变换应用于颜色空间。他们将颜色空间从RGB转换到YUV，然后仅在UV通道上计算流场。此外，他们使用**LPIPS**和**DISTS**评估感知相似性，并表明他们的方法优于C&W和stAdv攻击。
- en: 5.3\. Black-box adversarial attack for 2D deep learning models
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 黑盒对抗攻击针对2D深度学习模型
- en: All the above attacks generated AEs in the white-box assumption. However, in
    reality, the adversary often has little prior knowledge of the model internal
    architecture and parameters. Black-box attacks are proposed to solve this problem.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '-   上述攻击在白盒假设下生成了对抗样本。然而，实际上，攻击者往往对模型的内部架构和参数了解甚少。为了解决这个问题，提出了黑盒攻击。'
- en: 5.3.1\. Different black-box scenarios
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 不同黑盒场景
- en: According to the information the adversary can access, the black-box settings
    can be classified into the query-constrained scenario, score-based scenario, and
    hard label scenario (Ilyas et al., [2018b](#bib.bib56)). For the first circumstance,
    the attacker can only query the victim model a limited number of times. For example,
    some APIs may charge fees if the adversary queries many times. For the second
    circumstance, confidence scores are available, such as the Google Cloud Vision
    API. In the last circumstance, only top-1 or top-k predicted labels are output.
    For example, the Apple Photo app can automatically classify the user’s pictures
    without displaying score information.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '-   根据攻击者可以访问的信息，黑盒设置可以分为查询受限场景、基于分数的场景和硬标签场景（Ilyas et al., [2018b](#bib.bib56)）。对于第一个情况，攻击者只能有限次查询受害模型。例如，一些API可能会收取费用，如果攻击者查询次数过多。对于第二种情况，提供置信度分数，例如Google
    Cloud Vision API。在最后一种情况下，仅输出前1或前k的预测标签。例如，Apple Photo应用程序可以自动分类用户的照片而不显示分数信息。'
- en: Table 3\. Summary of main black-box adversarial attacks in 2D CV tasks sorted
    by the algorithm and published year
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 按算法和出版年份排序的主要黑盒对抗攻击总结（针对2D计算机视觉任务）
- en: '| Attacks | Year | Threat Model | Algorithm | Distance | Performance | Key
    idea |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 年份 | 威胁模型 | 算法 | 距离 | 性能 | 关键思想 |'
- en: '| Goal | Knowl.^* | Efficiency^(**) | ASR^(***) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 知识^* | 效率^(**) | 成功率^(***) |'
- en: '| TML (Papernot et al., [2016a](#bib.bib100)) | 2016 | U | $\blacksquare$ |
    Subs. model | $L_{\infty}$ | Costly | 96.19% | Data augment. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| TML (Papernot et al., [2016a](#bib.bib100)) | 2016 | U | $\blacksquare$ |
    替代模型 | $L_{\infty}$ | 昂贵 | 96.19% | 数据增强 |'
- en: '| TIMI (Dong et al., [2019](#bib.bib33)) | 2019 | U | $\blacksquare$ | Subs.
    model | $L_{2}$ | Efficient | 49.00% | Tranform-invariant |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| TIMI (Dong et al., [2019](#bib.bib33)) | 2019 | U | $\blacksquare$ | 替代模型
    | $L_{2}$ | 高效 | 49.00% | 转换不变 |'
- en: '| FDA (Ganeshan et al., [2019](#bib.bib38)) | 2019 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 80.20% | Feature distortion |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| FDA (Ganeshan et al., [2019](#bib.bib38)) | 2019 | U | $\blacksquare$ | 替代模型
    | $L_{\infty}$ | 昂贵 | 80.20% | 特征扭曲 |'
- en: '| ILA (Huang et al., [2019](#bib.bib55)) | 2019 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 85.80% | Feature distortion |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ILA (Huang et al., [2019](#bib.bib55)) | 2019 | U | $\blacksquare$ | 替代模型
    | $L_{\infty}$ | 昂贵 | 85.80% | 特征扭曲 |'
- en: '| SIM (Lin et al., [2019](#bib.bib73)) | 2019 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Medium | 77.20% | Scale-invariant |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| SIM (Lin et al., [2019](#bib.bib73)) | 2019 | U | $\blacksquare$ | 替代模型 |
    $L_{\infty}$ | 中等 | 77.20% | 尺度不变 |'
- en: '| TTTA (Li et al., [2020](#bib.bib69)) | 2020 | T | $\blacksquare$ | Subs.
    model | Poincare | Efficient | 42.90% | Pointcare distance |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| TTTA (Li et al., [2020](#bib.bib69)) | 2020 | T | $\blacksquare$ | 替代模型 |
    Poincare | 高效 | 42.90% | Pointcare距离 |'
- en: '| RDI (Zou et al., [2020](#bib.bib179)) | 2020 | T&U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 67.80% | Multi-scale gradient |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| RDI (Zou et al., [2020](#bib.bib179)) | 2020 | T&U | $\blacksquare$ | 替代模型
    | $L_{\infty}$ | 昂贵 | 67.80% | 多尺度梯度 |'
- en: '| DI-MI-TI (Zhao et al., [2021b](#bib.bib173)) | 2021 | T | $\blacksquare$
    | Subs. model | $L_{\infty}$ | Costly | 62.20% | Enough iteration |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| DI-MI-TI (Zhao et al., [2021b](#bib.bib173)) | 2021 | T | $\blacksquare$
    | 替代模型 | $L_{\infty}$ | 昂贵 | 62.20% | 足够迭代 |'
- en: '| FIA (Wang et al., [2021a](#bib.bib143)) | 2021 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 83.50% | Feature importance |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| FIA (Wang et al., [2021a](#bib.bib143)) | 2021 | U | $\blacksquare$ | 替代模型
    | $L_{\infty}$ | 昂贵 | 83.50% | 特征重要性 |'
- en: '| VT(Wang and He, [2021](#bib.bib139)) | 2021 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Medium | 76.50% | Gradient variance |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| VT(Wang and He, [2021](#bib.bib139)) | 2021 | U | $\blacksquare$ | 替代模型 |
    $L_{\infty}$ | 中等 | 76.50% | 梯度方差 |'
- en: '| NAA(Zhang et al., [2022b](#bib.bib164)) | 2022 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 85.0% | Feature importance |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| NAA(Zhang et al., [2022b](#bib.bib164)) | 2022 | U | $\blacksquare$ | 替代模型
    | $L_{\infty}$ | 昂贵 | 85.0% | 特征重要性 |'
- en: '| ODI(Byun et al., [2022](#bib.bib15)) | 2022 | T | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Efficient | 81.6% | 3D render |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| ODI(Byun et al., [2022](#bib.bib15)) | 2022 | T | $\blacksquare$ | 替代模型 |
    $L_{\infty}$ | 高效 | 81.6% | 3D渲染 |'
- en: '| Img2video (Wei et al., [2022](#bib.bib144)) | 2022 | U | $\blacksquare$ |
    Subs. model | $L_{\infty}$ | Costly | 77.88% | Cross-modality |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Img2video (Wei 等，[2022](#bib.bib144)) | 2022 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | 昂贵 | 77.88% | 跨模态 |'
- en: '| GNAE(Zhao et al., [2018](#bib.bib171)) | 2018 | U | $\blacksquare$ | GAN
    | $L_{2}$ | Costly | 78.00% | Natural AE |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| GNAE (Zhao 等，[2018](#bib.bib171)) | 2018 | U | $\blacksquare$ | GAN | $L_{2}$
    | 昂贵 | 78.00% | 自然 AE |'
- en: '| AdvGAN(Xiao et al., [2018a](#bib.bib150)) | 2018 | T | $\blacksquare$,$\color[rgb]{.5,.5,.5}\blacksquare$
    | GAN | $L_{2}$ | Efficient | 92.76% | Distilled model |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| AdvGAN (Xiao 等，[2018a](#bib.bib150)) | 2018 | T | $\blacksquare$,$\color[rgb]{.5,.5,.5}\blacksquare$
    | GAN | $L_{2}$ | 高效 | 92.76% | 蒸馏模型 |'
- en: '| ATTA(Wu et al., [2021](#bib.bib149)) | 2021 | U | $\blacksquare$ | GAN |
    $L_{\infty}$ | Costly | 61.80% | Adv transform |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ATTA (Wu 等，[2021](#bib.bib149)) | 2021 | U | $\blacksquare$ | GAN | $L_{\infty}$
    | 昂贵 | 61.80% | 对抗变换 |'
- en: '| Boundary Attack (Brendel et al., [2017](#bib.bib12)) | 2017 | T&U | $\blacksquare$
    | Decision | $L_{0}$ | Costly | $-$ | Random walk |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Boundary Attack (Brendel 等，[2017](#bib.bib12)) | 2017 | T&U | $\blacksquare$
    | 决策 | $L_{0}$ | 昂贵 | $-$ | 随机游走 |'
- en: '| BiasedBA (Brunner et al., [2019](#bib.bib14)) | 2019 | T&U | $\blacksquare$
    | Decision | $L_{2}$ | Efficient | 85.00% | Biased sampling |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| BiasedBA (Brunner 等，[2019](#bib.bib14)) | 2019 | T&U | $\blacksquare$ | 决策
    | $L_{2}$ | 高效 | 85.00% | 有偏采样 |'
- en: '| OPT(Cheng et al., [2019b](#bib.bib27)) | 2019 | T&U | $\blacksquare$ | Decision
    | $L_{2}$ | Medium | 100.00% | Binary search |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| OPT (Cheng 等，[2019b](#bib.bib27)) | 2019 | T&U | $\blacksquare$ | 决策 | $L_{2}$
    | 中等 | 100.00% | 二分搜索 |'
- en: '| HopSkipJump (Chen et al., [2020](#bib.bib21)) | 2020 | T&U | $\blacksquare$
    | Decision | $L_{2}$,$L_{\infty}$ | Efficient | 60.00% | Grandient+BA |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| HopSkipJump (Chen 等，[2020](#bib.bib21)) | 2020 | T&U | $\blacksquare$ | 决策
    | $L_{2}$,$L_{\infty}$ | 高效 | 60.00% | 梯度+BA |'
- en: '| SignOPT (Cheng et al., [2020](#bib.bib26)) | 2020 | T&U | $\blacksquare$
    | Decision | $L_{2}$ | Efficient | 94.00% | Estimate gradient sign |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| SignOPT (Cheng 等，[2020](#bib.bib26)) | 2020 | T&U | $\blacksquare$ | 决策 |
    $L_{2}$ | 高效 | 94.00% | 估计梯度符号 |'
- en: '| RayS (Chen and Gu, [2020](#bib.bib20)) | 2020 | U | $\blacksquare$ | Decision
    | $L_{\infty}$ | Efficient | 99.80% | Early stop |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| RayS (Chen 和 Gu，[2020](#bib.bib20)) | 2020 | U | $\blacksquare$ | 决策 | $L_{\infty}$
    | 高效 | 99.80% | 提前停止 |'
- en: '| QAIR(Li et al., [2021](#bib.bib70)) | 2021 | U | $\blacksquare$ | Decision
    | $L_{\infty}$ | Efficient | 98.00% | Image retrieval |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| QAIR (Li 等，[2021](#bib.bib70)) | 2021 | U | $\blacksquare$ | 决策 | $L_{\infty}$
    | 高效 | 98.00% | 图像检索 |'
- en: '| BayesAttack (Shukla et al., [2021](#bib.bib124)) | 2021 | T&U | $\blacksquare$
    | Decision | $L_{2}$,$L_{\infty}$ | Efficient | 67.48% | Bayesian optimization
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| BayesAttack (Shukla 等，[2021](#bib.bib124)) | 2021 | T&U | $\blacksquare$
    | 决策 | $L_{2}$,$L_{\infty}$ | 高效 | 67.48% | 贝叶斯优化 |'
- en: '| Surfree (Maho et al., [2021](#bib.bib88)) | 2021 | T&U | $\blacksquare$ |
    Geometric | $L_{2}$ | Efficient | 90.00% | Orthogonal projection |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Surfree (Maho 等，[2021](#bib.bib88)) | 2021 | T&U | $\blacksquare$ | 几何 |
    $L_{2}$ | 高效 | 90.00% | 正交投影 |'
- en: '| TangentAttack (Ma et al., [2021](#bib.bib84)) | 2021 | T&U | $\blacksquare$
    | Geometric | $L_{2}$ | Efficient | - | Semi-ellipsoid |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| TangentAttack (Ma 等，[2021](#bib.bib84)) | 2021 | T&U | $\blacksquare$ | 几何
    | $L_{2}$ | 高效 | - | 半椭圆体 |'
- en: '| TriangletAttack (Ma et al., [2021](#bib.bib84)) | 2022 | T&U | $\blacksquare$
    | Geometric | $L_{2}$ | Efficient | 44.5% | Triangle inequality |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| TriangletAttack (Ma 等，[2021](#bib.bib84)) | 2022 | T&U | $\blacksquare$ |
    几何 | $L_{2}$ | 高效 | 44.5% | 三角不等式 |'
- en: '| ZOO(Chen et al., [2017](#bib.bib23)) | 2017 | T | $\blacksquare$ | Score
    | $L_{2}$ | Costly | 97.00% | Finite difference |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| ZOO (Chen 等，[2017](#bib.bib23)) | 2017 | T | $\blacksquare$ | 评分 | $L_{2}$
    | 昂贵 | 97.00% | 有限差分 |'
- en: '| LocSearchAdv(Narodytska and Kasiviswanathan, [2017](#bib.bib96)) | 2017 |
    T&U | $\blacksquare$, $\color[rgb]{.5,.5,.5}\blacksquare$ | Score | $L_{0}$ |
    Costly | 70.78% | Local Search |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| LocSearchAdv (Narodytska 和 Kasiviswanathan，[2017](#bib.bib96)) | 2017 | T&U
    | $\blacksquare$, $\color[rgb]{.5,.5,.5}\blacksquare$ | 评分 | $L_{0}$ | 昂贵 | 70.78%
    | 局部搜索 |'
- en: '| Autozoom(Tu et al., [2019](#bib.bib134)) | 2018 | T | $\blacksquare$ | Score
    | $-$ | Efficient | 93.00% | Autoencoder |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Autozoom (Tu 等，[2019](#bib.bib134)) | 2018 | T | $\blacksquare$ | 评分 | $-$
    | 高效 | 93.00% | 自编码器 |'
- en: '| PCA(Bhagoji et al., [2018](#bib.bib10)) | 2018 | U | $\blacksquare$ | Score
    | $L_{\infty}$ | Costly | 89.50% | Principal component |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| PCA (Bhagoji 等，[2018](#bib.bib10)) | 2018 | U | $\blacksquare$ | 评分 | $L_{\infty}$
    | 昂贵 | 89.50% | 主成分 |'
- en: '| SimBA(Guo et al., [2019a](#bib.bib43)) | 2019 | T&U | $\blacksquare$ | Score
    | $L_{2}$ | Medium | 96.50% | Orthonormal basis |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| SimBA (Guo 等，[2019a](#bib.bib43)) | 2019 | T&U | $\blacksquare$ | 评分 | $L_{2}$
    | 中等 | 96.50% | 正交基 |'
- en: '| CornerSearch(Croce and Hein, [2019](#bib.bib30)) | 2019 | U | $\blacksquare$
    | Score | $L_{0}$ | Costly | 99.56% | Sparse noises |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| CornerSearch (Croce 和 Hein，[2019](#bib.bib30)) | 2019 | U | $\blacksquare$
    | 评分 | $L_{0}$ | 昂贵 | 99.56% | 稀疏噪声 |'
- en: '| OnePixel(Su et al., [2019](#bib.bib126)) | 2019 | T&U | $\blacksquare$, $\color[rgb]{.5,.5,.5}\blacksquare$
    | Score | $L_{0}$ | Costly | 67.97% | Differential evolution |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| OnePixel（Su et al., [2019](#bib.bib126)） | 2019 | T&U | $\blacksquare$, $\color[rgb]{.5,.5,.5}\blacksquare$
    | 评分 | $L_{0}$ | 成本高 | 67.97% | 差分进化 |'
- en: '| PBBA(Moon et al., [2019](#bib.bib91)) | 2019 | T&U | $\blacksquare$ | Score
    | $L_{\infty}$ | Medium | 99.90% | Discrete surrogate |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| PBBA（Moon et al., [2019](#bib.bib91)） | 2019 | T&U | $\blacksquare$ | 评分
    | $L_{\infty}$ | 中等 | 99.90% | 离散替代 |'
- en: '| Nattack (Li et al., [2019](#bib.bib71)) | 2019 | U | $\blacksquare$ | Score
    | $L_{2}$,$L_{\infty}$ | Costly | 100.00% | Adversarial distribution |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Nattack（Li et al., [2019](#bib.bib71)） | 2019 | U | $\blacksquare$ | 评分 |
    $L_{2}$,$L_{\infty}$ | 成本高 | 100.00% | 对抗分布 |'
- en: '| Bandis (Ilyas et al., [2018a](#bib.bib57)) | 2019 | U | $\blacksquare$ |
    Score | $L_{2}$,$L_{\infty}$ | Medium | 95.40% | Gradient priors |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Bandis（Ilyas et al., [2018a](#bib.bib57)） | 2019 | U | $\blacksquare$ | 评分
    | $L_{2}$,$L_{\infty}$ | 中等 | 95.40% | 梯度先验 |'
- en: '| SignHunter(Al-Dujaili and O’Reilly, [2019](#bib.bib3)) | 2020 | T&U | $\blacksquare$
    | Score | $L_{\infty}$,$L_{2}$ | Efficient | 91.47% | Grad. sign estimator |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| SignHunter（Al-Dujaili 和 O’Reilly, [2019](#bib.bib3)） | 2020 | T&U | $\blacksquare$
    | 评分 | $L_{\infty}$,$L_{2}$ | 高效 | 91.47% | 梯度符号估计器 |'
- en: '| Square attack (Andriushchenko et al., [2020](#bib.bib5)) | 2020 | T&U | $\blacksquare$
    | Score | $L_{2}$,$L_{\infty}$ | Efficient | 99.40% | Shrinking squares |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Square attack（Andriushchenko et al., [2020](#bib.bib5)） | 2020 | T&U | $\blacksquare$
    | 评分 | $L_{2}$,$L_{\infty}$ | 高效 | 99.40% | 收缩方块 |'
- en: '| Sparse-RS (Croce et al., [2022](#bib.bib29)) | 2022 | T&U | $\blacksquare$
    | Score | $L_{\infty}$ | Costly | 95.80% | Random search |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Sparse-RS（Croce et al., [2022](#bib.bib29)） | 2022 | T&U | $\blacksquare$
    | 评分 | $L_{\infty}$ | 成本高 | 95.80% | 随机搜索 |'
- en: '| NES(Ilyas et al., [2018b](#bib.bib56)) | 2018 | T&U | $\blacksquare$ | Score/Decision
    | $L_{\infty}$ | Costly | 88.20% | Natural evolution |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| NES（Ilyas et al., [2018b](#bib.bib56)） | 2018 | T&U | $\blacksquare$ | 评分/决策
    | $L_{\infty}$ | 成本高 | 88.20% | 自然进化 |'
- en: '| Subspace(Guo et al., [2019b](#bib.bib45)) | 2019 | U | $\blacksquare$ | Subs.+Score
    | $L_{\infty}$ | Costly | 96.60% | Multi-subs. model |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Subspace（Guo et al., [2019b](#bib.bib45)） | 2019 | U | $\blacksquare$ | 替代+评分
    | $L_{\infty}$ | 成本高 | 96.60% | 多替代模型 |'
- en: '| P-RGF(Cheng et al., [2019a](#bib.bib28)) | 2019 | U | $\blacksquare$ | Subs.+Score
    | $L_{2}$ | Medium | 99.10% | Transfer-based priors |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| P-RGF（Cheng et al., [2019a](#bib.bib28)） | 2019 | U | $\blacksquare$ | 替代+评分
    | $L_{2}$ | 中等 | 99.10% | 基于转移的先验 |'
- en: '| SimBA++(Yang et al., [2020](#bib.bib156)) | 2020 | U | $\blacksquare$ | Subs.+Score
    | $L_{2}$ | Efficient | 99.40% | Mixed method |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| SimBA++（Yang et al., [2020](#bib.bib156)） | 2020 | U | $\blacksquare$ | 替代+评分
    | $L_{2}$ | 高效 | 99.40% | 混合方法 |'
- en: •
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '* This column contains different attacks’ adversarial knowledge. $\square$:
    white-box. $\blacksquare$: black-box. ${\color[rgb]{.5,.5,.5}\blacksquare}$: gray-box.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* 该列包含不同攻击的对抗知识。$\square$: 白箱。$\blacksquare$: 黑箱。${\color[rgb]{.5,.5,.5}\blacksquare}$:
    灰箱。'
- en: •
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '** For query-based attacks, we use efficiency to evaluate the queries needed
    to find the adversarial example.'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '** 对于基于查询的攻击，我们使用效率来评估找到对抗样本所需的查询次数。'
- en: •
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*** ASR is the abbr. of attack success rate. As mentioned, we only count the
    best result of the hardest attack reported in the paper.'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*** ASR 是攻击成功率的缩写。如前所述，我们只计算论文中报告的最困难攻击的最佳结果。'
- en: 5.3.2\. Categories of black-box 2D digital adversarial attacks
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 黑箱2D数字对抗攻击的分类
- en: Different settings result in various methods. This review classifies black-box
    attacks into three different categories according to their settings and corresponding
    algorithms.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的设置导致不同的方法。本综述根据设置和相应算法将黑箱攻击分类为三种不同的类别。
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Substitute-model-based attacks. This method utilizes the transferability of
    adversarial examples, which can convert black-box problems into white-box problems,
    therefore significantly improving efficiency. However, limited by the transferability
    of the surrogate model, the attack success rate is usually not very high.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 替代模型基础的攻击。这种方法利用了对抗样本的可转移性，可以将黑箱问题转化为白箱问题，从而显著提高效率。然而，由于替代模型的可转移性限制，攻击成功率通常不是很高。
- en: •
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Score-based query attacks. We classify query-based attacks into score-based
    and decision-based. The continuous confidence score can be accessed for score-based
    attacks to estimate the decision boundary or gradient.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于评分的查询攻击。我们将基于查询的攻击分为基于评分的和基于决策的。对于基于评分的攻击，可以访问连续的置信度评分来估计决策边界或梯度。
- en: •
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Decision-based query attacks. These attacks suppose the adversary can only access
    the predicted labels of the model. They are far more difficult than score-based
    attacks because less information can be leveraged.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于决策的查询攻击。这些攻击假设对手只能访问模型的预测标签。与基于分数的攻击相比，这些攻击更为困难，因为可以利用的信息更少。
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Geometry-based query attacks. These attacks also aim at hard-label settings.
    However, they use geometric information of the decision boundary rather than estimate
    the gradients to improve the query efficiency.
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于几何的查询攻击。这些攻击也针对硬标签设置。然而，它们使用决策边界的几何信息，而不是估计梯度来提高查询效率。
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generative-model-based attacks. GAN can also be used for black-box attacks.
    For example, in AdvGAN (Xiao et al., [2018a](#bib.bib150)), the classifier is
    displaced by a distilled model, which can be viewed as a transfer-based attack.
    In GNAE (Zhao et al., [2018](#bib.bib171)), random search algorithms are operated
    in the $z$ latent space, which can be regarded as a query-based attack.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于生成模型的攻击。GAN 也可以用于黑箱攻击。例如，在 AdvGAN (Xiao et al., [2018a](#bib.bib150)) 中，分类器被一个精炼的模型取代，这可以视为一种基于转移的攻击。在
    GNAE (Zhao et al., [2018](#bib.bib171)) 中，随机搜索算法在 $z$ 潜在空间中运行，这可以视为一种基于查询的攻击。
- en: Some attacks also combine different methods together to improve the efficiency
    and attack success rate, such as SimBA++ (Yang et al., [2020](#bib.bib156)), P-RGF
    (Cheng et al., [2019a](#bib.bib28)), and Subspace attacks (Guo et al., [2019b](#bib.bib45)).
    Table.[3](#S5.T3 "Table 3 ‣ 5.3.1\. Different black-box scenarios ‣ 5.3\. Black-box
    adversarial attack for 2D deep learning models ‣ 5\. Adversarial attacks for 2D
    deep learning models ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning
    Models Against Adversarial Attacks") shows some representative 2D black-box adversarial
    attacks.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击还结合了不同的方法来提高效率和攻击成功率，例如 SimBA++ (Yang et al., [2020](#bib.bib156))、P-RGF
    (Cheng et al., [2019a](#bib.bib28)) 和 Subspace 攻击 (Guo et al., [2019b](#bib.bib45))。表格。[3](#S5.T3
    "表格 3 ‣ 5.3.1\. 不同的黑箱场景 ‣ 5.3\. 黑箱对抗攻击用于 2D 深度学习模型 ‣ 5\. 2D 深度学习模型的对抗攻击 ‣ 2D 和
    3D 深度学习模型对抗攻击的鲁棒性和安全性综述") 显示了一些代表性的 2D 黑箱对抗攻击。
- en: 5.3.3\. Substitute-model-based black-box attacks.
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 替代模型基础的黑箱攻击。
- en: The substitute-model-based attack is based on the transferability of adversarial
    examples. Therefore, improving the transferability is crucial for this kind of
    method.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 替代模型基础的攻击是基于对抗样本的可转移性。因此，提高可转移性对这种方法至关重要。
- en: Data augmentation and transformation are widely used for improving transferability.
    TML (Papernot et al., [2016a](#bib.bib100)) used Jacobian-based dataset augmentation
    to expand the dataset and train the substitute model alternatively, which can
    be formulated as $\mathcal{D}_{\rho+1}=(x+\lambda_{\rho}\text{sign}(J_{\hat{\mathcal{F}}}(x:x\in\mathcal{D}_{\rho})))\cup\mathcal{D}_{\rho}$,
    where $\hat{\mathcal{F}}$ is the surrogate model. $\mathcal{D}_{\rho}$ and $\mathcal{D}_{\rho+1}$
    is the training dataset before and after $\rho^{th}$ iteration, $\lambda_{\rho}$
    is a periodical step size. In addition, they introduced reservoir sampling into
    the data augmentation process to reduce the iteration number. DI (Xie et al.,
    [2019](#bib.bib155)) promoted transferability through randomly resizing and padding
    the dataset. TIMI (Dong et al., [2019](#bib.bib33)) improved the transferability
    of MI-FGSM by enriching the input varieties, which is equivalent to multiplying
    the gradient with a smoothing kernel, such as a rotation, transition, or rescale
    matrix. SIM (Lin et al., [2019](#bib.bib73)) used Nesterov accelerated gradient
    and resized the image into different scales to prevent the local model from overfitting.
    RDI (Zou et al., [2020](#bib.bib179)) improved DI and TIMI through multi-scale
    gradient and region fitting.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强和转换被广泛用于提高可转移性。TML (Papernot et al., [2016a](#bib.bib100)) 使用基于雅可比矩阵的数据集增强来扩展数据集，并交替训练替代模型，这可以表示为
    $\mathcal{D}_{\rho+1}=(x+\lambda_{\rho}\text{sign}(J_{\hat{\mathcal{F}}}(x:x\in\mathcal{D}_{\rho})))\cup\mathcal{D}_{\rho}$，其中
    $\hat{\mathcal{F}}$ 是替代模型。$\mathcal{D}_{\rho}$ 和 $\mathcal{D}_{\rho+1}$ 分别是第 $\rho^{th}$
    次迭代前后的训练数据集，$\lambda_{\rho}$ 是周期性步长。此外，他们在数据增强过程中引入了水库抽样以减少迭代次数。DI (Xie et al.,
    [2019](#bib.bib155)) 通过随机调整数据集的大小和填充来促进可转移性。TIMI (Dong et al., [2019](#bib.bib33))
    通过丰富输入的多样性来提高 MI-FGSM 的可转移性，这等同于用平滑核（如旋转、过渡或缩放矩阵）乘以梯度。SIM (Lin et al., [2019](#bib.bib73))
    使用 Nesterov 加速梯度，并将图像调整为不同的尺度，以防止局部模型过拟合。RDI (Zou et al., [2020](#bib.bib179))
    通过多尺度梯度和区域拟合改进了 DI 和 TIMI。
- en: Targeted attacks are more difficult than untargeted attacks for surrogate-model-based
    methods. Zhao et al. (Zhao et al., [2021b](#bib.bib173)) found that a simple targeted
    function $Z_{y^{\prime}}(F(x^{adv}))$ with sufficient iterations can produce SOTA
    results, where $Z_{y^{\prime}}$ is the output logit of target label. Byun et al.
    (Byun et al., [2022](#bib.bib15)) improved the transferability of the targeted
    attack through object-based diverse input (ODI), which utilized a differentiable
    3D renderer to render the 2D adversarial examples on 3D objects. Noticing that
    the gradient of MI-FGSM tends to be dominated by the past gradients after a few
    updates, Li et al. (Li et al., [2020](#bib.bib69)) proposed TTTA that adaptively
    adjusts the gradient through Pointcare distance. Through Pointcare distance, the
    gradient increases when and only when the $f(x)$ comes closer to the target class.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 针对性攻击比非针对性攻击在基于替代模型的方法中更困难。Zhao et al.（Zhao et al., [2021b](#bib.bib173)）发现，使用简单的针对性函数
    $Z_{y^{\prime}}(F(x^{adv}))$ 进行充分迭代可以产生SOTA结果，其中 $Z_{y^{\prime}}$ 是目标标签的输出对数值。Byun
    et al.（Byun et al., [2022](#bib.bib15)）通过基于对象的多样输入（ODI）提高了针对性攻击的迁移性，该方法利用可微分的3D渲染器在3D对象上渲染2D对抗样本。Li
    et al.（Li et al., [2020](#bib.bib69)）注意到MI-FGSM的梯度在经过几次更新后往往被过去的梯度主导，因此提出了TTTA，通过Pointcare距离自适应调整梯度。通过Pointcare距离，梯度仅在
    $f(x)$ 更接近目标类别时才会增加。
- en: Some works found that manipulating the surrogate model’s latent layer features
    can alleviate overfitting and utilize the shared features, resulting in transferability
    improvement. Intermediate level attack (ILA) (Huang et al., [2019](#bib.bib55))
    improved C&W and FGSM attacks by maximizing the distortion of a pre-specified
    layer, while Feature disruptive attack (FDA) (Ganeshan et al., [2019](#bib.bib38))
    disrupted all latent layer’s output. However, these two attacks regard all neurons
    as equally important. The neurons that negatively influence the ground truth prediction
    should be amplified rather than suppressed. FIA (Wang et al., [2021a](#bib.bib143))
    judged the neuron importance by the mean gradient of a set of transformed images.
    But this method suffers from gradient saturation. To solve this, NAA (Zhang et al.,
    [2022b](#bib.bib164)) quantified neuron importance by computing neuron attribution.
    Besides image classification tasks, Img2video attack (Wei et al., [2022](#bib.bib144))
    attacked the black-box video model by generating the AE of each frame on a surrogate
    model. They maximize the distance of low-level features between normal and adversarial
    video frames.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究发现，通过操控替代模型的隐层特征可以减轻过拟合并利用共享特征，从而改善迁移性。中间层攻击（ILA）（Huang et al., [2019](#bib.bib55)）通过最大化预设层的扭曲来改善C&W和FGSM攻击，而特征干扰攻击（FDA）（Ganeshan
    et al., [2019](#bib.bib38)）则干扰所有隐层的输出。然而，这两种攻击将所有神经元视为同等重要。那些对真实标签预测产生负面影响的神经元应被放大而非抑制。FIA（Wang
    et al., [2021a](#bib.bib143)）通过一组变换图像的平均梯度来评估神经元的重要性。但这种方法存在梯度饱和的问题。为了解决这个问题，NAA（Zhang
    et al., [2022b](#bib.bib164)）通过计算神经元归因来量化神经元的重要性。除了图像分类任务，Img2video攻击（Wei et al.,
    [2022](#bib.bib144)）通过在替代模型上生成每一帧的AE来攻击黑箱视频模型。他们最大化正常和对抗视频帧之间低级特征的距离。
- en: Some attacks combine the surrogate model and other methods to get better performance.
    P-RGF attack (Cheng et al., [2019a](#bib.bib28)) utilized transfer-based priors
    to get more accurate gradient estimation and save queries. They formulated the
    gradient estimation problem as $\min_{\hat{g}}L(\hat{g})=\mathbb{E}\left\|\bigtriangledown_{x}f(x)-b\hat{g}\right\|_{2}^{2}$,
    where $b$ is a scaling factor. They utilized the estimated cosine similarity between
    the actual and transferred gradient to control the transfer strength. Their method
    can also be incorporated with other priors to save queries. Subspace attack (Guo
    et al., [2019b](#bib.bib45)) shrank the search space of stochastic vectors by
    transferring the gradients from a group of substitute networks. SimBA++ (Yang
    et al., [2020](#bib.bib156)) also integrated transferability-based and queries-based
    model. Moreover, SimBA++ updated the surrogate model based on the query results.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击结合了替代模型和其他方法，以获得更好的性能。P-RGF攻击（Cheng et al., [2019a](#bib.bib28)）利用了基于迁移的先验来获得更准确的梯度估计并节省查询。他们将梯度估计问题公式化为$\min_{\hat{g}}L(\hat{g})=\mathbb{E}\left\|\bigtriangledown_{x}f(x)-b\hat{g}\right\|_{2}^{2}$，其中$b$是缩放因子。他们利用实际和迁移梯度之间的估计余弦相似度来控制迁移强度。他们的方法还可以与其他先验结合以节省查询。子空间攻击（Guo
    et al., [2019b](#bib.bib45)）通过从一组替代网络中迁移梯度来缩小随机向量的搜索空间。SimBA++（Yang et al., [2020](#bib.bib156)）也整合了基于迁移性和基于查询的模型。此外，SimBA++基于查询结果更新了替代模型。
- en: 5.3.4\. Score-based query attacks
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4\. 基于分数的查询攻击
- en: The goal of query-based attacks is to craft adversarial examples through fewer
    queries and achieve a higher attack success rate. Based on whether the continuous
    confidence score is accessible,query-based methods can be classified into score-based
    and decision-based query methods.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 查询基础攻击的目标是通过更少的查询来构造对抗样本，并实现更高的攻击成功率。根据是否可以访问连续的置信度分数，查询基础方法可以分为基于分数和基于决策的查询方法。
- en: Gradient-estimation-based methods
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于梯度估计的方法
- en: Most of the score-based query attacks are based on gradient estimation. The
    simplest gradient estimation method is the finite difference, such as zero-order
    optimization attack (ZOO) (Chen et al., [2017](#bib.bib23)). For ZOO, the gradient
    is estimated through $\frac{\partial f(x)}{\partial x_{i}}\approx\frac{f(x+\delta
    e_{i})-f(x-\delta e_{i})}{2\delta}$, where $e_{i}$ is a standard basis, $\delta$
    is a random vector sampled from Gaussian distribution near the original data point.
    Moreover, they used a resize function and increased dimension gradually to improve
    efficiency. However, because they need to use all the standard basis vectors in
    each iteration, the query cost is proportional to the image size, which is computationally
    expensive. Later, some improvements were proposed to reduce the random vectors
    to be sampled. PCA attack (Bhagoji et al., [2018](#bib.bib10)) reduced search
    dimensions by analyzing the principal components of input data. Autozoom (Tu et al.,
    [2019](#bib.bib134)) firstly trained an Autoencoder to encode the image of the
    target class into latent space. Then, it sampled the random vectors from the latent
    space and mapped them to random perturbations with reduced dimensions to compute
    the finite difference. Al-Dujaili et al. (Al-Dujaili and O’Reilly, [2019](#bib.bib3))
    proposed a sign-based gradient estimation algorithm. The basic idea is to estimate
    the sign of the directional derivative. They proposed a divide and conquer method
    named SignHunter that leverages the separability of the derivative of the adversarial
    objective function, which decreases the query number from $2^{n}$ to $O(n)$.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于分数的查询攻击是基于梯度估计的。最简单的梯度估计方法是有限差分，例如零阶优化攻击（ZOO）（Chen et al., [2017](#bib.bib23)）。对于ZOO，梯度通过$\frac{\partial
    f(x)}{\partial x_{i}}\approx\frac{f(x+\delta e_{i})-f(x-\delta e_{i})}{2\delta}$来估计，其中$e_{i}$是标准基向量，$\delta$是从高斯分布中采样的随机向量，接近原始数据点。此外，他们使用了调整大小函数，并逐渐增加维度以提高效率。然而，由于他们需要在每次迭代中使用所有标准基向量，查询成本与图像大小成正比，这在计算上是昂贵的。后来，提出了一些改进方法来减少需要采样的随机向量。PCA攻击（Bhagoji
    et al., [2018](#bib.bib10)）通过分析输入数据的主成分来减少搜索维度。Autozoom（Tu et al., [2019](#bib.bib134)）首先训练了一个自编码器，将目标类别的图像编码到潜在空间中。然后，它从潜在空间中采样随机向量，并将其映射到减少维度的随机扰动以计算有限差分。Al-Dujaili
    et al.（Al-Dujaili和O’Reilly，[2019](#bib.bib3)）提出了一种基于符号的梯度估计算法。基本思想是估计方向导数的符号。他们提出了一种名为SignHunter的分治方法，利用对抗目标函数导数的可分性，将查询次数从$2^{n}$减少到$O(n)$。
- en: Heuristic algorithms can also be used for gradient estimation. Ilylas et al.
    (Ilyas et al., [2018b](#bib.bib56)) estimated the gradient through a variant of
    natural evolution strategy (NES). For scoreless settings, they leveraged noise
    robustness to substitute the confidence score. Specifically, they sampled a set
    of $\delta_{i}$ from a normal distribution by antithetic sampling. Then, the expectation
    of estimated gradient of $F(x^{\prime})$ approximately equals to $\frac{1}{\sigma
    n}\sum_{i=1}^{n}\delta_{i}F(x^{\prime}+\sigma\delta_{i})$. Later, Ilylas et al.
    (Ilyas et al., [2018a](#bib.bib57)) formulated the gradient estimation problem
    as finding a vector to maximize $\mathbb{E}[\hat{g}^{T}g^{*}]$ and used the least
    squares method to solve this problem and proved it is an equivalence of NES. They
    utilized two kinds of priors to improve efficiency. Firstly, the present gradient
    highly correlates with the gradient of the last step. Secondly, adjacent pixels
    often have similar gradients. Therefore, they designed a gradient estimation framework
    based on bandit optimization, where the action is a priors-based gradient estimation
    in each round.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式算法也可以用于梯度估计。Ilyas等人（Ilyas et al., [2018b](#bib.bib56)）通过自然进化策略（NES）的变体估计了梯度。在无评分设置中，他们利用噪声鲁棒性来替代置信评分。具体来说，他们通过反相关抽样从正态分布中采样了一组$\delta_{i}$。然后，估计的$F(x^{\prime})$梯度的期望大致等于$\frac{1}{\sigma
    n}\sum_{i=1}^{n}\delta_{i}F(x^{\prime}+\sigma\delta_{i})$。随后，Ilyas等人（Ilyas et
    al., [2018a](#bib.bib57)）将梯度估计问题表述为寻找一个向量以最大化$\mathbb{E}[\hat{g}^{T}g^{*}]$，并使用最小二乘法解决了这个问题，证明了它是NES的等价形式。他们利用了两种先验知识来提高效率。首先，当前梯度与上一步的梯度高度相关。其次，相邻像素的梯度通常是相似的。因此，他们设计了一个基于赌博机优化的梯度估计框架，其中每一轮的动作是基于先验知识的梯度估计。
- en: Greedy-algorithm-based methods
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于贪心算法的方法
- en: SimBA (Guo et al., [2019a](#bib.bib43)) is a simple but powerful attack based
    on the greedy algorithm. Instead of computing all basis vectors at each iteration
    (like ZOO attack), it randomly chooses one vector from a predefined orthogonal
    basis at each iteration. If this vector can reduce the confidence score, it is
    applied to the image, and the next random vector is sampled. Otherwise, this direction
    is deserted. They found that the DCT basis is especially efficient. Although this
    method is simple, it outperforms NES and ZOO in the aspect of query cost. SimBA++
    (Yang et al., [2020](#bib.bib156)) improved the query efficiency by combining
    SimBA with the transferability-based attack TIMI (Dong et al., [2019](#bib.bib33)).
    Instead of randomly sampling changing direction, they correlate the sampling probability
    to the surrogate model’s gradient. Moreover, they proposed high-order gradient
    approximation (HOGA) to distill the target model in both forward and backward
    steps. SimBA++ greatly decreases the query times and achieves a higher attack
    success rate than SimBA.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: SimBA（Guo et al., [2019a](#bib.bib43)）是一种简单而强大的基于贪心算法的攻击方法。与每次迭代都计算所有基向量（如ZOO攻击）不同，它在每次迭代时随机选择一个来自预定义正交基的向量。如果该向量能够降低置信评分，则应用于图像，并抽取下一个随机向量。否则，该方向被舍弃。他们发现DCT基特别高效。尽管这种方法简单，但在查询成本方面优于NES和ZOO。SimBA++（Yang
    et al., [2020](#bib.bib156)）通过将SimBA与基于可转移攻击的TIMI（Dong et al., [2019](#bib.bib33)）相结合，提高了查询效率。它们不再随机抽样改变方向，而是将抽样概率与代理模型的梯度相关联。此外，他们提出了高阶梯度近似（HOGA），以在前向和后向步骤中提炼目标模型。SimBA++显著减少了查询次数，并比SimBA实现了更高的攻击成功率。
- en: Grid-search-based methods
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于网格搜索的方法
- en: $\mathcal{N}$attack (Li et al., [2019](#bib.bib71)) tries to find a distribution
    of AEs in the neighborhood of $x$. But it does not need to access the model parameters.
    It formulates the optimization problem of finding adversarial distribution around
    $x$ as $\min_{\theta}J(\theta):=\int{f(x^{\prime})\pi_{S}(x^{\prime}|\theta)dx^{\prime}}$,
    where $f(x^{\prime})$ is an untargeted attack loss function, $\theta=(\mu,\sigma^{2})$
    are the parameters of Gaussian distribution and $x^{\prime}=x+\text{proj}_{S}(1/2(tanh(z)+1))$,
    where $z\sim\mathcal{N}(z|\theta)$. To solve this problem, it uses grid search
    to find the best $\sigma$ and updates $\mu$ through NES. PBBA (Moon et al., [2019](#bib.bib91))
    searched AEs along the border of $l_{\infty}$ ball. This simplification makes
    it possible to refine the query performance by optimizing discrete surrogate problems.
    It firstly zones the images as squares and operates regional optimization in the
    grid by choosing $x^{\prime}$ from $\{x-\epsilon,x+\epsilon\}$. Then, it adjusts
    the squares and duplicates this procedure until it finds an AE. Square attack
    reduced the search space through diminishing squares. Unlike PBBA (Moon et al.,
    [2019](#bib.bib91)) that used a fixed grid, the locations of squares are optimized.
    Moreover, they sampled $\delta$ along the boundary of $L_{p}$-norm ball to improve
    the query efficiency. Specifically speaking, for $L_{\infty}$-square attack, $x_{i}^{\prime}\in\{x_{i}-\epsilon,x_{i}+\epsilon\}$.
    For $L_{2}$-square attack, because noise values are correlated, they increase
    the budget of one window and simultaneously decrease the budget of another.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{N}$attack (Li et al., [2019](#bib.bib71)) 尝试寻找 $x$ 邻域内的对抗样本分布。但它不需要访问模型参数。它将围绕
    $x$ 找到对抗分布的优化问题表述为 $\min_{\theta}J(\theta):=\int{f(x^{\prime})\pi_{S}(x^{\prime}|\theta)dx^{\prime}}$，其中
    $f(x^{\prime})$ 是无目标攻击损失函数，$\theta=(\mu,\sigma^{2})$ 是高斯分布的参数，$x^{\prime}=x+\text{proj}_{S}(1/2(tanh(z)+1))$，其中
    $z\sim\mathcal{N}(z|\theta)$。为了解决这个问题，它使用网格搜索来寻找最佳的 $\sigma$，并通过 NES 更新 $\mu$。PBBA
    (Moon et al., [2019](#bib.bib91)) 在 $l_{\infty}$ 球的边界上搜索对抗样本。这种简化使得通过优化离散代理问题来改进查询性能成为可能。它首先将图像划分为方块，并通过从
    $\{x-\epsilon,x+\epsilon\}$ 中选择 $x^{\prime}$ 在网格中进行区域优化。然后，它调整方块并重复此过程，直到找到一个对抗样本。Square
    attack 通过减少方块的数量来缩小搜索空间。与 PBBA (Moon et al., [2019](#bib.bib91)) 使用固定网格不同，方块的位置经过优化。此外，它们在
    $L_{p}$-范数球的边界上采样 $\delta$ 以提高查询效率。具体来说，对于 $L_{\infty}$-方块攻击，$x_{i}^{\prime}\in\{x_{i}-\epsilon,x_{i}+\epsilon\}$。对于
    $L_{2}$-方块攻击，由于噪声值是相关的，它们增加了一个窗口的预算，同时减少了另一个窗口的预算。
- en: Dimension-reduction-based methods
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于降维的方法
- en: Some works reduced the search dimensions by reducing the number of crafted pixels.
    The first work is LocSearchAdv (Narodytska and Kasiviswanathan, [2017](#bib.bib96)),
    which used local search to estimate implicit gradient. It first randomly picked
    up some pixels as initial points. At each round, the adjacent pixels of these
    points are checked and updated. It only needed to manipulate 0.5% pixels for the
    untargeted attack on ImageNet. CornerSearch (Croce and Hein, [2019](#bib.bib30))
    also used local search to find sparse perturbations, but it only added perturbations
    to pixels with high variation to make them imperceptible. OnePixel attack (Su
    et al., [2019](#bib.bib126)) considered the extreme situation and showed the possibility
    of fooling the classifier by only revising one pixel. It used differential evolution
    to optimize this problem. Each candidate solution contains its pixel coordinates
    and color values. The candidate solutions are updated by random crossover. Sparse-RS
    (Croce et al., [2022](#bib.bib29)) utilized random search, which is appropriate
    for zero-order optimization problems with sparse restraint. It achieved the SOTA
    attack success rate on multiple attacks, including the $l_{0}$-norm noises, adversarial
    patches, and adversarial borders.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作通过减少合成像素的数量来降低搜索维度。第一个工作是 LocSearchAdv (Narodytska 和 Kasiviswanathan, [2017](#bib.bib96))，它使用局部搜索来估计隐式梯度。它首先随机选择一些像素作为初始点。在每一轮中，检查并更新这些点的相邻像素。对于
    ImageNet 上的无目标攻击，它只需要操作 0.5% 的像素。CornerSearch (Croce 和 Hein, [2019](#bib.bib30))
    也使用局部搜索来找到稀疏的扰动，但它只对高变异性的像素添加扰动，以使其不可察觉。OnePixel attack (Su et al., [2019](#bib.bib126))
    考虑了极端情况，并展示了仅通过修改一个像素来欺骗分类器的可能性。它使用差分进化来优化这个问题。每个候选解决方案包含其像素坐标和颜色值。候选解决方案通过随机交叉进行更新。Sparse-RS
    (Croce et al., [2022](#bib.bib29)) 利用随机搜索，这适用于具有稀疏约束的零阶优化问题。它在多个攻击中实现了 SOTA 攻击成功率，包括
    $l_{0}$-范数噪声、对抗补丁和对抗边界。
- en: 5.3.5\. Decision-based query attacks
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.5\. 基于决策的查询攻击
- en: In some scenarios, the adversary can only get the final decision ( hard label)
    rather than the confidence score (soft label). This is a more challenging case
    because the gradient value is hard to estimate.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景中，对手只能获得最终决策（硬标签），而无法获得置信度评分（软标签）。这是一个更具挑战性的情况，因为梯度值难以估计。
- en: Gradient-estimation-based methods
  id: totrans-296
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于梯度估计的方法
- en: 'Some works proposed to estimate the gradient directions instead. The first
    work is Boundary attack (Brendel et al., [2017](#bib.bib12)), which first initializes
    a point outside the ground truth region and then uses a random walk algorithm
    to make it closer to the boundary at each iteration. Although it can realize almost
    the same perturbation level compared with white-box attacks, it needs exponential
    time to find adversarial examples. Biased boundary attack (Brunner et al., [2019](#bib.bib14))
    used bias sampling to re-understand and improve boundary attacks. Three different
    biases are utilized. The first bias is sampling more vectors in the direction
    of low-frequency distortion. The second bias is sampling denser data from regions
    where the adversarial and benign images have larger differences. The last bias
    is the prior-based gradient from the substitute model. HSJA (Chen et al., [2020](#bib.bib21))
    includes three steps: binary search to find the boundary point, estimation of
    gradient direction, and geometric progression to update the boundary point. Because
    the gradient guides the search direction, HSJA significantly decreases query number
    compared to the Boundary attack.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究提出了估计梯度方向的方法。第一个研究是边界攻击（Brendel 等，[2017](#bib.bib12)），该方法首先初始化一个位于真实区域外的点，然后使用随机游走算法使其在每次迭代时更接近边界。尽管与白盒攻击相比，它可以实现几乎相同的扰动水平，但需要指数时间来找到对抗样本。偏置边界攻击（Brunner
    等，[2019](#bib.bib14)）通过偏置采样重新理解和改进了边界攻击。使用了三种不同的偏置。第一个偏置是采样更多在低频失真方向上的向量。第二个偏置是从对抗样本和正常样本差异较大的区域中采样更密集的数据。最后一个偏置是基于先验的替代模型梯度。HSJA（Chen
    等，[2020](#bib.bib21)）包括三个步骤：二分搜索以找到边界点、梯度方向估计和几何级数更新边界点。由于梯度指导搜索方向，HSJA 相比于边界攻击显著减少了查询次数。
- en: OPT (Cheng et al., [2019b](#bib.bib27)) is another representative hard-label-based
    attack, which estimated the shortest distance of the benign sample from the boundary
    by fine-grained search and binary search. They estimated the gradient of the search
    direction through the randomized gradient-free method and updated the search direction
    through gradient descent. SignOPT (Cheng et al., [2020](#bib.bib26)) promoted
    OPT’s efficiency by only estimating the sign of gradient over search direction
    by just one query and averaging the gradient sign over a group of random directions.
    RayS (Chen and Gu, [2020](#bib.bib20)) reframed the successive problem of finding
    the nearest classification hyperplane as a discontinuous problem without gradient
    estimation, which searches over a group of ray orientations. Moreover, all unnecessary
    directions are terminated early through a quick check. This greatly promoted search
    efficiency. Bayes attacks (Shukla et al., [2021](#bib.bib124)) reduced query budgets
    to 1000 through Bayesian optimization. However, because Bayesian optimization
    is not suitable for search space with large dimensions, they reduce the search
    dimension through FFT and use nearest-neighbor upsampling to find adversarial
    examples. Their method significantly reduced the query times compared with OPT
    and Sign-OPT attacks. QAIR (Li et al., [2021](#bib.bib70)) considers a more difficult
    setting where the attacker can only get top-k unlabeled images from the target
    model in the image retrieval task. They proposed a correlation-based loss by the
    difference of top-k unlabeled feedback retrieved by benign and adversarial images.
    Moreover, they improved the efficiency through recursive network theft to get
    gradient priors.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: OPT（Cheng 等，[2019b](#bib.bib27)）是另一种代表性的基于硬标签的攻击方法，该方法通过精细搜索和二分搜索估计了良性样本与边界之间的最短距离。他们通过随机梯度无关的方法估计搜索方向的梯度，并通过梯度下降更新搜索方向。SignOPT（Cheng
    等，[2020](#bib.bib26)）通过仅估计搜索方向上梯度的符号（仅需一次查询）并对一组随机方向上的梯度符号取平均，提升了 OPT 的效率。RayS（Chen
    和 Gu，[2020](#bib.bib20)）将寻找最近分类超平面的连续问题重新构造为一个不需要梯度估计的不连续问题，并在一组光线方向上进行搜索。此外，所有不必要的方向通过快速检查被提前终止，这大大提高了搜索效率。Bayes
    攻击（Shukla 等，[2021](#bib.bib124)）通过贝叶斯优化将查询预算减少到 1000。然而，由于贝叶斯优化不适用于高维搜索空间，他们通过
    FFT 降低搜索维度，并使用最近邻上采样来寻找对抗样本。他们的方法显著减少了与 OPT 和 Sign-OPT 攻击相比的查询次数。QAIR（Li 等，[2021](#bib.bib70)）考虑了一种更困难的设置，即攻击者只能从目标模型中获取
    top-k 未标记图像（在图像检索任务中）。他们通过良性图像和对抗图像检索的 top-k 未标记反馈的差异提出了一种基于相关性的损失。此外，他们通过递归网络窃取提高了效率，以获得梯度先验。
- en: Geometry-based methods
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于几何的方法
- en: Geometry attacks also aim at the hard-label scenario. However, they directly
    use geometric information of the decision boundary rather than estimate the gradients
    to improve efficiency. Surfree (Maho et al., [2021](#bib.bib88)) assumes the boundary
    is a hyperplane and iteratively runs binary searches over orthonormal directions
    to find the clean image’s projection on the boundary. Compared to HSJA (Chen et al.,
    [2020](#bib.bib21)), the new direction is randomly sampled from the low-frequency
    subband generated by DCT rather than using gradient directions, which reduces
    the queries to a few hundred. TangentAttack (Ma et al., [2021](#bib.bib84)) assumed
    the boundary is a semi-ellipsoid and adjusted the search direction to follow the
    optimal tangent line rather than the gradient directions, which need less distortion
    than HSJA. TriangleAttack (Wang et al., [2022b](#bib.bib140)) utilized triangle
    inequality to search the boundary point and also used DCT for dimension reduction.
    Less than a thousand quires are required in this attack. CGBA (Reza et al., [2023](#bib.bib110))
    restrict the search on a semicircular path to make sure to find a boundary point
    regardless of the boundary curvature, which is quite efficient for untargeted
    attack..
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 几何攻击也针对硬标签场景。然而，它们直接使用决策边界的几何信息，而不是估算梯度以提高效率。Surfree (Maho et al., [2021](#bib.bib88))
    假设边界是一个超平面，并在正交方向上迭代进行二分搜索，以找到干净图像在边界上的投影。与 HSJA (Chen et al., [2020](#bib.bib21))
    相比，新方向是从由 DCT 生成的低频子带中随机采样，而不是使用梯度方向，这将查询次数减少到几百次。TangentAttack (Ma et al., [2021](#bib.bib84))
    假设边界是半椭球体，并调整搜索方向以跟随最佳切线，而不是梯度方向，这比 HSJA 需要更少的失真。TriangleAttack (Wang et al.,
    [2022b](#bib.bib140)) 利用三角不等式搜索边界点，并使用 DCT 进行降维。在这次攻击中需要不到一千次查询。CGBA (Reza et
    al., [2023](#bib.bib110)) 限制在半圆路径上进行搜索，以确保找到边界点而不受边界曲率的影响，这对于无目标攻击相当高效。
- en: 5.3.6\. Generative-model-based black-box attacks
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.6\. 基于生成模型的黑箱攻击
- en: GAN can also be used for black-box attacks. GNAE (Zhao et al., [2018](#bib.bib171))
    generates natural AEs by searching for perturbations in the latent space rather
    than the input space. It first learns a generator $\mathcal{G}_{\theta}$ that
    maps normally distributed variables $z$ to the input $x$. It then learns an inverter
    $\mathcal{I}_{\gamma}$ that maps $x$ back to latent space by minimizing the divergence
    between $z$ and reconstructed $\mathcal{I}_{\gamma}(x)$. Their objective is $\min_{\tilde{z}}\left\|\tilde{z}-\mathcal{I}_{\gamma}(x)\right\|_{2}\;s.t.\;\mathcal{F}(\mathcal{G}_{\theta}(\tilde{z}))\neq\mathcal{F}(x)$.
    The feasible $\tilde{z}$ is found through iterative stochastic search and hybrid
    shrinking search.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 也可以用于黑箱攻击。GNAE (Zhao et al., [2018](#bib.bib171)) 通过在潜在空间中搜索扰动而不是输入空间来生成自然
    AE。它首先学习一个生成器 $\mathcal{G}_{\theta}$，将正态分布变量 $z$ 映射到输入 $x$。然后它学习一个反向器 $\mathcal{I}_{\gamma}$，通过最小化
    $z$ 和重建的 $\mathcal{I}_{\gamma}(x)$ 之间的散度，将 $x$ 映射回潜在空间。他们的目标是 $\min_{\tilde{z}}\left\|\tilde{z}-\mathcal{I}_{\gamma}(x)\right\|_{2}\;s.t.\;\mathcal{F}(\mathcal{G}_{\theta}(\tilde{z}))\neq\mathcal{F}(x)$。可行的
    $\tilde{z}$ 通过迭代随机搜索和混合缩小搜索找到。
- en: Xiao et al. (Xiao et al., [2018a](#bib.bib150)) proposed AdvGAN to improve the
    attack transferability. AdvGAN consists of a perturbation generator, a discriminator,
    and a classifier. The generator is trained to generate perturbations from the
    original instance. The discriminator is trained to distinguish AEs from clean
    instances. The classifier is a distilled model of the target black-box network.
    They alternately train these modules by first training the GAN and then training
    the classifier through $\min_{f}\mathbb{E}_{x\sim D_{data}}\mathcal{L}(f(x),b(x))+\mathcal{L}(f(x+\mathcal{G}(x)),b(x+\mathcal{G}(x)))$
    , where $b(x)$ is the target model, and $f(x)$ is the distilled model. $\mathcal{L}$
    is a cross-entropy loss function. Their black-box attack achieved 92.7% on the
    MNIST dataset.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao et al. (Xiao et al., [2018a](#bib.bib150)) 提出了 AdvGAN 来提高攻击的可转移性。AdvGAN
    包括一个扰动生成器、一个鉴别器和一个分类器。生成器被训练生成来自原始实例的扰动。鉴别器被训练区分 AE 和干净实例。分类器是目标黑箱网络的蒸馏模型。他们交替训练这些模块，首先训练
    GAN，然后通过 $\min_{f}\mathbb{E}_{x\sim D_{data}}\mathcal{L}(f(x),b(x))+\mathcal{L}(f(x+\mathcal{G}(x)),b(x+\mathcal{G}(x)))$
    训练分类器，其中 $b(x)$ 是目标模型，$f(x)$ 是蒸馏模型。$\mathcal{L}$ 是交叉熵损失函数。他们的黑箱攻击在 MNIST 数据集上的准确率达到了
    92.7%。
- en: Some previous transfer-based attacks improved transferability through simple
    transformations, like rotation and translation (Dong et al., [2019](#bib.bib33)),
    random resizing and padding (Xie et al., [2019](#bib.bib155)) and multi-scaling
    (Lin et al., [2019](#bib.bib73)). Wu et al. (Wu et al., [2021](#bib.bib149)) proposed
    ATTA to improve transferability through learning adversarial transformations that
    can better alleviate the adversarial property. They formulate this problem as
    a min-max problem. The inner problem is to find $x_{adv}$ that maximizes the classification
    loss. The outer problem is to learn a transformation through CNN to minimize the
    classification loss.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 一些早期的基于迁移的攻击通过简单的变换提高了迁移性，例如旋转和平移（Dong et al., [2019](#bib.bib33)）、随机缩放和填充（Xie
    et al., [2019](#bib.bib155)）以及多尺度（Lin et al., [2019](#bib.bib73)）。Wu et al. (Wu
    et al., [2021](#bib.bib149)) 提出了 ATTA，通过学习对抗性变换来提高迁移性，以更好地缓解对抗特性。他们将这个问题形式化为一个最小最大问题。内部问题是找到
    $x_{adv}$ 以最大化分类损失。外部问题是通过 CNN 学习一个变换以最小化分类损失。
- en: 5.4\. The safety impact of physical-realizable 2D adversarial attacks
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 物理可实现 2D 对抗攻击的安全影响
- en: Table 4\. Summary of main works that examine the safety of real-world 2D CV
    applications through physical adversarial attacks
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 总结了通过物理对抗攻击检查现实世界 2D CV 应用程序安全性的主要工作
- en: '| Attacks | Year | Threat Model | Algorithm | Distance | Performance | Scenario
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 年份 | 威胁模型 | 算法 | 距离 | 性能 | 场景 |'
- en: '| Goal | Knowl.^* | Interface | Camouflage | Succ. ^(**) |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 知识^* | 接口 | 伪装 | 成功 ^(**) |'
- en: '| UAP-patch (Liu et al., [2020a](#bib.bib74)) | 2020 | U | $\square$ | Patch
    | Gradient | Style loss | Marked | 74.1% | Auto check-Out |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| UAP-patch (Liu et al., [2020a](#bib.bib74)) | 2020 | U | $\square$ | 补丁 |
    梯度 | 风格损失 | 已标记 | 74.1% | 自动检查 |'
- en: '| Facial Acc. (Sharif et al., [2016](#bib.bib120)) | 2016 | T&U | $\square$
    | Patch | Gradient | $L_{\infty}$ | Marked | 80.0% | Face rcg. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 面部识别 (Sharif et al., [2016](#bib.bib120)) | 2016 | T&U | $\square$ | 补丁 |
    梯度 | $L_{\infty}$ | 已标记 | 80.0% | 人脸识别 |'
- en: '| AGN(Sharif et al., [2019](#bib.bib121)) | 2019 | T&U | $\square$ | Patch
    | GAN | $-$ | Slight | 70.0% | Face rcg. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| AGN(Sharif et al., [2019](#bib.bib121)) | 2019 | T&U | $\square$ | 补丁 | GAN
    | $-$ | 稍微 | 70.0% | 人脸识别 |'
- en: '| FRSadv(Nguyen et al., [2020](#bib.bib97)) | 2020 | T | $\square$ | Light
    | Gradient | $L_{\infty}$ | Marked | 92.0% | Face rcg. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| FRSadv(Nguyen et al., [2020](#bib.bib97)) | 2020 | T | $\square$ | 光照 | 梯度
    | $L_{\infty}$ | 已标记 | 92.0% | 人脸识别 |'
- en: '| GenAP (Xiao et al., [2021](#bib.bib153)) | 2021 | T&U | $\blacksquare$ |
    Patch | GAN | $L_{\infty}$ | Marked | 65.0% | Face rcg. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| GenAP (Xiao et al., [2021](#bib.bib153)) | 2021 | T&U | $\blacksquare$ |
    补丁 | GAN | $L_{\infty}$ | 已标记 | 65.0% | 人脸识别 |'
- en: '| AdvMakeUp (Yin et al., [2021](#bib.bib157)) | 2021 | T | $\square$ | Patch
    | Gradient | Style, Content | Slight | 22.0% | Face rcg. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| AdvMakeUp (Yin et al., [2021](#bib.bib157)) | 2021 | T | $\square$ | 补丁 |
    梯度 | 风格、内容 | 稍微 | 22.0% | 人脸识别 |'
- en: '| AdvPatch (Brown et al., [2017](#bib.bib13)) | 2017 | T | $\square$ | Patch
    | Gradient | $L_{\infty}$ | Marked | 93.0% | Img clas. |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| AdvPatch (Brown et al., [2017](#bib.bib13)) | 2017 | T | $\square$ | 补丁 |
    梯度 | $L_{\infty}$ | 已标记 | 93.0% | 图像分类 |'
- en: '| EoT(Athalye et al., [2018b](#bib.bib7)) | 2018 | T | $\square$ | Patch |
    Gradient | $L_{2}$ | Slight | 82.0% | Img clas. |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| EoT(Athalye et al., [2018b](#bib.bib7)) | 2018 | T | $\square$ | 补丁 | 梯度
    | $L_{2}$ | 稍微 | 82.0% | 图像分类 |'
- en: '| CiPer(Agarwal et al., [2020](#bib.bib2)) | 2020 | U | $\blacksquare$ | Sensor
    | Greedy | $L_{\infty}$ | Marked | 33.0% | Img clas. |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| CiPer(Agarwal et al., [2020](#bib.bib2)) | 2020 | U | $\blacksquare$ | 传感器
    | 贪婪 | $L_{\infty}$ | 已标记 | 33.0% | 图像分类 |'
- en: '| Nat-Patch (Hu et al., [2021](#bib.bib52)) | 2021 | U | $\square$ | Patch
    | GAN | Smoth loss | Invisible | 48.0% | Object detect |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Nat-Patch (Hu et al., [2021](#bib.bib52)) | 2021 | U | $\square$ | 补丁 | GAN
    | 平滑损失 | 隐形 | 48.0% | 物体检测 |'
- en: '| CAMOU (Zhang et al., [2018b](#bib.bib167)) | 2018 | U | $\blacksquare$ |
    Patch | Subs. Model | $-$ | Marked | 32.7% | Self-driving |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| CAMOU (Zhang et al., [2018b](#bib.bib167)) | 2018 | U | $\blacksquare$ |
    补丁 | 替代模型 | $-$ | 已标记 | 32.7% | 自驾车 |'
- en: '| RP2 (Eykholt et al., [2018](#bib.bib36)) | 2018 | T | $\square$ | Patch |
    Gradient | $L_{1},L_{2}$ | Marked | 84.8% | Self-driving |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| RP2 (Eykholt et al., [2018](#bib.bib36)) | 2018 | T | $\square$ | 补丁 | 梯度
    | $L_{1},L_{2}$ | 已标记 | 84.8% | 自驾车 |'
- en: '| ShapeShifter (Chen et al., [2018a](#bib.bib24)) | 2018 | T&U | $\square$
    | Patch | Gradient | $L_{2}$ | Marked | 87.0% | Self-driving |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| ShapeShifter (Chen et al., [2018a](#bib.bib24)) | 2018 | T&U | $\square$
    | 补丁 | 梯度 | $L_{2}$ | 已标记 | 87.0% | 自驾车 |'
- en: '| MeshAdv (Xiao et al., [2019](#bib.bib151)) | 2019 | T&U | $\square$ | Renderer
    | Gradient | $Lp.$ | Slight | 100.0% | Self-driving |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| MeshAdv (Xiao et al., [2019](#bib.bib151)) | 2019 | T&U | $\square$ | 渲染器
    | 梯度 | $Lp.$ | 稍微 | 100.0% | 自驾车 |'
- en: '| AdvCam(Duan et al., [2020](#bib.bib34)) | 2020 | T&U | $\square$ | Patch
    | Gradient | Style loss | Slight | 80.0% | Self-driving |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| AdvCam（Duan 等，[2020](#bib.bib34)） | 2020 | T&U | $\square$ | 补丁 | 梯度 | 风格损失
    | 轻微 | 80.0% | 自驾 |'
- en: '| DAS (Wang et al., [2021b](#bib.bib138)) | 2021 | U | $\square$ | Patch |
    Gradient | $L_{2},T.V.$ | Marked | 20.0% | Self-driving |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| DAS（Wang 等，[2021b](#bib.bib138)） | 2021 | U | $\square$ | 补丁 | 梯度 | $L_{2},T.V.$
    | 标记 | 20.0% | 自驾 |'
- en: '| AMPLE (Ji et al., [2021](#bib.bib59)) | 2021 | T&U | $\blacksquare$ | Sensor
    | Bayesian | Energy loss | Marked | 87.9% | Self-driving |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| AMPLE（Ji 等，[2021](#bib.bib59)） | 2021 | T&U | $\blacksquare$ | 传感器 | 贝叶斯
    | 能量损失 | 标记 | 87.9% | 自驾 |'
- en: '| DirtyRoad (Sato et al., [2021](#bib.bib113)) | 2021 | U | $\square$ | Patch
    | Gradient | $L_{1},L_{2},L_{\infty}$ | Invisible | 97.5% | Self-driving |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| DirtyRoad（Sato 等，[2021](#bib.bib113)） | 2021 | U | $\square$ | 补丁 | 梯度 |
    $L_{1},L_{2},L_{\infty}$ | 隐形 | 97.5% | 自驾 |'
- en: '| OPAD(Gnanasambandam et al., [2021](#bib.bib41)) | 2021 | T | $\square$ |
    Light | Gradient | $L_{2},L_{\infty}$ | Slight | 48.0% | Self-driving |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| OPAD（Gnanasambandam 等， [2021](#bib.bib41)） | 2021 | T | $\square$ | 光线 |
    梯度 | $L_{2},L_{\infty}$ | 轻微 | 48.0% | 自驾 |'
- en: '| DTA (Suryanto et al., [2022](#bib.bib128)) | 2022 | T | $\square$ | Patch
    | Gradient | $-$ | Marked | 63.0% | Self-driving |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| DTA（Suryanto 等，[2022](#bib.bib128)） | 2022 | T | $\square$ | 补丁 | 梯度 | $-$
    | 标记 | 63.0% | 自驾 |'
- en: '| TrajAdv (Zhang et al., [2022a](#bib.bib165)) | 2022 | U | $\square$$\blacksquare$
    | Patch | Gradient,PSO | $L_{\infty}$ | Slight | 62.2% | Self-driving |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| TrajAdv（Zhang 等，[2022a](#bib.bib165)） | 2022 | U | $\square$$\blacksquare$
    | 补丁 | 梯度，PSO | $L_{\infty}$ | 轻微 | 62.2% | 自驾 |'
- en: '| FCA(Wang et al., [2022a](#bib.bib137)) | 2022 | U | $\square$ | Patch | Gradient
    | Smooth loss | Marked | 60.0% | Self-driving |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| FCA（Wang 等，[2022a](#bib.bib137)） | 2022 | U | $\square$ | 补丁 | 梯度 | 平滑损失
    | 标记 | 60.0% | 自驾 |'
- en: •
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '* This column is the adversarial knowledge of different attacks. $\square$:
    white-box. $\blacksquare$: black-box. ${\color[rgb]{.5,.5,.5}\blacksquare}$: gray-box.'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* 本栏是不同攻击的对抗知识。$\square$：白盒。$\blacksquare$：黑盒。${\color[rgb]{.5,.5,.5}\blacksquare}$：灰盒。'
- en: •
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '** For objector detection, the attack success rate means the average rate of
    escaping from being detected. As mentioned, we only count the best result of the
    hardest attack reported in the paper.'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '** 对于目标检测，攻击成功率指的是逃避检测的平均比率。如前所述，我们仅计算论文中报告的最难攻击的最佳结果。'
- en: When deep learning models are employed in the real world, ensuring the safety
    of human life and property is the primary concern. A number of studies have investigated
    adversarial attacks in safety-critical applications, such as face recognition
    and self-driving systems. referred to as physical adversarial attack. This subsection
    reviews 2D physical adversarial attacks in safety-critical applications.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 当深度学习模型被应用于现实世界时，确保人身和财产安全是首要任务。一些研究探讨了安全关键应用中的对抗攻击，例如面部识别和自驾系统，称为物理对抗攻击。本小节回顾了安全关键应用中的2D物理对抗攻击。
- en: The main challenge of physical adversarial attacks is the ever-changing physical
    environment, such as the background light noise, varying viewpoints, and different
    distances. Moreover, because the adversary cannot directly modify the input images
    at the pixel level, certain mediums are needed to pollute the data obliquely,
    such as patches (Sharif et al., [2016](#bib.bib120); Xiao et al., [2021](#bib.bib153)),
    illumination (Gnanasambandam et al., [2021](#bib.bib41); Li et al., [2023](#bib.bib72)),
    and sensors (Sayles et al., [2021](#bib.bib114)). The patch-based attack is the
    most common physical attack. It uses printed patches to cover the whole or part
    of the target object to spoof the classifier. Some works also render 2D adversarial
    images on 3D objects, like turtles (Athalye et al., [2018b](#bib.bib7)) and cars
    (Wang et al., [2021b](#bib.bib138)). This can be seen as a more general form of
    adversarial patches.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 物理对抗攻击的主要挑战是不断变化的物理环境，如背景光噪声、不同的视角和不同的距离。此外，由于攻击者不能直接在像素级别修改输入图像，需要某些介质来间接污染数据，例如补丁（Sharif
    等，[2016](#bib.bib120)；Xiao 等，[2021](#bib.bib153)）、照明（Gnanasambandam 等，[2021](#bib.bib41)；Li
    等，[2023](#bib.bib72)）和传感器（Sayles 等，[2021](#bib.bib114)）。基于补丁的攻击是最常见的物理攻击。它使用打印的补丁覆盖目标对象的全部或部分，以欺骗分类器。一些工作还将2D对抗图像渲染到3D对象上，如海龟（Athalye
    等，[2018b](#bib.bib7)）和汽车（Wang 等，[2021b](#bib.bib138)）。这可以视为对抗补丁的更一般形式。
- en: 5.4.1\. The safety of face recognition system
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 面部识别系统的安全性
- en: As mentioned before, a major challenge for the 2D physical-world adversarial
    attack is the changing environment, like varying distances and ambient light.
    To make the AE more robust to these changes and can fool the classifier consistently,
    Athalye et al. (Athalye et al., [2018b](#bib.bib7)) proposed expectation on transformation
    (EoT) that calculates the expectation of the log-likelihood of the target class
    on transformed images, which is formulated as
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，2D 物理世界对抗攻击的一大挑战是环境的变化，例如距离和环境光的变化。为了使 AE 对这些变化更加鲁棒，并能始终如一地欺骗分类器，Athalye
    等人（Athalye et al., [2018b](#bib.bib7)）提出了变换期望（EoT），它计算目标类别在变换图像上的对数似然的期望，其公式为
- en: '| (4) |  | $x^{adv}=arg\min_{\Delta x}\Sigma^{k}_{i=1}(w_{i}\mathcal{L}(\mathcal{T}_{i}(x)+\Delta
    x,y)),\;s.t.\&#124;\Delta x\&#124;_{p}\leq\epsilon,$ |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $x^{adv}=arg\min_{\Delta x}\Sigma^{k}_{i=1}(w_{i}\mathcal{L}(\mathcal{T}_{i}(x)+\Delta
    x,y)),\;s.t.\&#124;\Delta x\&#124;_{p}\leq\epsilon,$ |  |'
- en: where $\mathcal{T}_{i}$ is the $i_{th}$ transformation, such as random rotations
    and transitions, $w_{i}$ is the weight and subject to $\Sigma_{i=1}^{k}w_{i}=1$.
    Besides rotation and transition, they also regarded the 3D rendering as a transformation.
    They use the $l_{2}$ norm in the LAB color space as the distance metric because
    it is closer to human perception. Brown et al. (Brown et al., [2017](#bib.bib13))
    inducted EOT, patch position, and multiple training images into the optimization
    round to generate targeted, universal, and robust adversarial patterns. However,
    their method has poor transferability when the pattern size is small.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{T}_{i}$ 是第 $i_{th}$ 种变换，例如随机旋转和过渡，$w_{i}$ 是权重，且满足 $\Sigma_{i=1}^{k}w_{i}=1$。除了旋转和过渡外，他们还将
    3D 渲染视为一种变换。他们使用 LAB 色彩空间中的 $l_{2}$ 范数作为距离度量，因为它更接近人类的感知。Brown 等人（Brown et al.,
    [2017](#bib.bib13)）将 EOT、补丁位置和多张训练图像纳入优化过程，以生成目标性、通用性和鲁棒性强的对抗模式。然而，他们的方法在补丁尺寸较小时传递性较差。
- en: In 2016, Sharif et al. (Sharif et al., [2016](#bib.bib120)) proposed facial
    accessory attack. The perturbation is limited to the area of facial accessories
    like eyeglass frames to make the attack more reasonable. They also use EOT to
    improve the physical attack’s robustness. Moreover, they minimize the total variation
    of the patches to make them more natural and use the NPS score to guarantee the
    patch is printable. The total loss function is $arg\min_{\delta}\sum_{x\in X}L_{s}(\mathcal{F}(x+\delta),l)+k_{1}TV(\delta)+k_{2}NPS(\delta)$,
    where $L_{s}$ is a Softmax loss on the target label. $TV$ is the total variance
    loss. $NPS$ is the non-printable score that defined as $NPS(\hat{p})=\prod_{p\in
    P}|\hat{p}-p|$, where $P$ is the set of printable colors. In 2019, Sharif et al.
    proposed AGN attack (Sharif et al., [2019](#bib.bib121)), which improved the inconspicuousness
    of adversarial patches through a generative framework to generate AEs with multiple
    objectives, such as robustness, imperceptibility, and scalability. Unlike previous
    work that uses total variation as smoothness loss, they use GAN to ensure the
    generated patches look like real-world designs.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 2016 年，Sharif 等人（Sharif et al., [2016](#bib.bib120)）提出了面部配件攻击。扰动仅限于眼镜框等面部配件区域，以使攻击更加合理。他们还使用
    EOT 来提高物理攻击的鲁棒性。此外，他们通过最小化补丁的总变化量来使其更自然，并使用 NPS 评分来确保补丁可以打印。总损失函数为 $arg\min_{\delta}\sum_{x\in
    X}L_{s}(\mathcal{F}(x+\delta),l)+k_{1}TV(\delta)+k_{2}NPS(\delta)$，其中 $L_{s}$
    是目标标签上的 Softmax 损失。$TV$ 是总方差损失。$NPS$ 是定义为 $NPS(\hat{p})=\prod_{p\in P}|\hat{p}-p|$
    的不可打印评分，其中 $P$ 是可打印颜色的集合。2019 年，Sharif 等人提出了 AGN 攻击（Sharif et al., [2019](#bib.bib121)），通过生成框架提高了对抗补丁的隐蔽性，生成具有多个目标（如鲁棒性、不可察觉性和可扩展性）的
    AE。与之前使用总变化作为平滑损失的工作不同，他们使用 GAN 来确保生成的补丁看起来像真实世界的设计。
- en: In 2020, FRSadv (Nguyen et al., [2020](#bib.bib97)) attacked the face recognition
    system through adversarial illumination. They project adversarial patterns onto
    the human face, and the camera captures the images and predicts wrong labels.
    They also used EOT to improve the robustness. Adv-Makeup (Yin et al., [2021](#bib.bib157))
    fooled the FRS through adversarial makeup. Their attack first generates realistic
    eye shadow through GAN, then blends the generated eye shadow onto the source image
    through the gradient, content, and style losses. Adv-Makeup attacks an ensemble
    of models to improve transferability. FaceAdv (Shen et al., [2021](#bib.bib122))
    has a sticker generator and a converter. The generator chooses the most vulnerable
    area to attack, and the converter renders the patches to the face with different
    angles and sizes to improve physical robustness.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年，FRSadv（Nguyen et al., [2020](#bib.bib97)）通过对抗性照明攻击了面部识别系统。他们将对抗模式投射到人脸上，然后相机捕捉图像并预测错误的标签。他们还使用了EOT来提高鲁棒性。Adv-Makeup（Yin
    et al., [2021](#bib.bib157)）通过对抗化妆欺骗了面部识别系统。他们的攻击首先通过GAN生成逼真的眼影，然后通过梯度、内容和风格损失将生成的眼影与源图像混合。Adv-Makeup攻击一个模型集成以提高可迁移性。FaceAdv（Shen
    et al., [2021](#bib.bib122)）拥有一个贴纸生成器和一个转换器。生成器选择最脆弱的攻击区域，转换器将补丁以不同的角度和大小渲染到面部，以提高物理鲁棒性。
- en: In 2021, Xiao et al. (Xiao et al., [2021](#bib.bib153)) proposed the GenAP attack,
    which regularizes the patches on the latent space of GAN to make the adversarial
    patches more natural and transferable. They first trained a StyleGAN on normal
    face datasets, then used it to generate AEs and crop them into the patch area.
    Instead of optimizing the image directly, they optimized the latent variable $w$
    on the $\mathcal{W}+$ space of StyleGAN to minimize the feature distance between
    $x^{adv}$ and the target $x_{t}$. Experiments show that their methods can improve
    the adversarial pattern’s transferability. However, their method needs a pre-trained
    generator trained on the victim examples. Moreover, their methods are difficult
    to converge, which may be because the cropping function unexpectedly changes the
    AEs.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年，Xiao等人（Xiao et al., [2021](#bib.bib153)）提出了GenAP攻击，该攻击通过对GAN的潜在空间中的补丁进行正则化，使对抗补丁更自然、更具可迁移性。他们首先在正常面部数据集上训练了一个StyleGAN，然后用它生成对抗样本（AEs）并将其裁剪成补丁区域。他们并不是直接优化图像，而是在StyleGAN的$\mathcal{W}+$空间上优化潜在变量$w$，以最小化$x^{adv}$与目标$x_{t}$之间的特征距离。实验表明，他们的方法可以提高对抗模式的可迁移性。然而，他们的方法需要一个在受害者示例上训练好的预训练生成器。此外，他们的方法难以收敛，这可能是因为裁剪函数意外地改变了AEs。
- en: 5.4.2\. The safety of self-driving system
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2. 自驾系统的安全性
- en: '![Refer to caption](img/32541c9566ab22dd5265af87030cbaa9.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/32541c9566ab22dd5265af87030cbaa9.png)'
- en: Figure 5\. The general procedure of generating robust physical perturbation
    through expectation over transformations (Eykholt et al., [2018](#bib.bib36);
    Athalye et al., [2018b](#bib.bib7); Brown et al., [2017](#bib.bib13); Sharif et al.,
    [2016](#bib.bib120)). Images are sampled from different distances and view angles
    to improve the real-world attack’s robustness against position changes.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 通过对变换的期望生成鲁棒的物理扰动的一般过程（Eykholt et al., [2018](#bib.bib36); Athalye et al.,
    [2018b](#bib.bib7); Brown et al., [2017](#bib.bib13); Sharif et al., [2016](#bib.bib120)）。图像从不同的距离和视角采样，以提高实际攻击对位置变化的鲁棒性。
- en: A relatively simple setting for adversarial attacks in self-driving is to hide
    planar objects from the detector or classifier, like traffic signs and roads.
    Chen et al. (Chen et al., [2018a](#bib.bib24)) proposed ShapeShifter to attack
    the Fast R-CNN detector. They applied EoT to the objection detector field to improve
    the attack robustness. Rather than using the adversarial patch, they modify the
    background of the stop sign directly. Eykholt et al. (Eykholt et al., [2018](#bib.bib36))
    proposed robust physical perturbation (RP2) to make deep learning models misclassify
    the road sign. They improved EoT by sampling from a set of both simulated and
    physical distortions. The general procedure is shown in Figure.[5](#S5.F5 "Figure
    5 ‣ 5.4.2\. The safety of self-driving system ‣ 5.4\. The safety impact of physical-realizable
    2D adversarial attacks ‣ 5\. Adversarial attacks for 2D deep learning models ‣
    A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial
    Attacks"). They first operate $L_{1}$ optimization to find approximate positions
    of the adversarial patches and then operate $L_{2}$ optimization to modify their
    RGB values. AdvCam (Duan et al., [2020](#bib.bib34)) generated adversarial traffic
    signs with natural styles through neural style transfer. The loss function consists
    of an adversarial loss, a style loss, a content loss, and a smoothness loss. The
    style loss and content loss are calculated through the features extracted from
    the shallow and deep layers of the feature extraction neural network. Sato et
    al. (Sato et al., [2021](#bib.bib113)) attacked the automated lane centering system
    through dirty road patches, which may result in the car running out of the street.
    Their objective function is to make the car drive out of the lane in the shortest
    time. They achieved 97.5% ASR and only needed 0.903 sec for each attack. TrajAdv
    (Zhang et al., [2022a](#bib.bib165)) evaluated the robustness of the trajectory
    prediction of the self-driving car through single-frame or multi-frame adversarial
    perturbation. They considered the acceleration and speed limitation to make the
    adversarial perturbation more natural. OPAD (Gnanasambandam et al., [2021](#bib.bib41))
    cheated the traffic sign classifier by optical adversarial perturbation. To overcome
    the nonlinear effect of the projector, they estimated the radiometric and spectral
    response to rectify the distortion.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶中，相对简单的对抗攻击设置是从检测器或分类器中隐藏平面物体，如交通标志和道路。Chen 等人（Chen et al., [2018a](#bib.bib24)）提出了
    ShapeShifter 来攻击 Fast R-CNN 检测器。他们在目标检测器领域应用了 EoT 以提高攻击的鲁棒性。他们不是使用对抗性补丁，而是直接修改了停车标志的背景。Eykholt
    等人（Eykholt et al., [2018](#bib.bib36)）提出了鲁棒物理扰动（RP2），使深度学习模型错误分类道路标志。他们通过从模拟和物理失真集中采样来改进
    EoT。一般程序如图所示。[5](#S5.F5 "Figure 5 ‣ 5.4.2\. The safety of self-driving system
    ‣ 5.4\. The safety impact of physical-realizable 2D adversarial attacks ‣ 5\.
    Adversarial attacks for 2D deep learning models ‣ A Survey of Robustness and Safety
    of 2D and 3D Deep Learning Models Against Adversarial Attacks") 他们首先进行 $L_{1}$
    优化以找到对抗性补丁的大致位置，然后进行 $L_{2}$ 优化以修改其 RGB 值。AdvCam（Duan et al., [2020](#bib.bib34)）通过神经风格迁移生成具有自然风格的对抗性交通标志。损失函数包括对抗损失、风格损失、内容损失和光滑性损失。风格损失和内容损失通过从浅层和深层特征提取神经网络中提取的特征计算。Sato
    等人（Sato et al., [2021](#bib.bib113)）通过肮脏的道路补丁攻击了自动车道居中系统，这可能导致汽车驶出街道。他们的目标函数是使汽车在最短时间内驶出车道。他们实现了
    97.5% 的 ASR，并且每次攻击只需 0.903 秒。TrajAdv（Zhang et al., [2022a](#bib.bib165)）通过单帧或多帧对抗性扰动评估了自动驾驶汽车的轨迹预测的鲁棒性。他们考虑了加速度和速度限制，使对抗扰动更加自然。OPAD（Gnanasambandam
    et al., [2021](#bib.bib41)）通过光学对抗扰动欺骗了交通标志分类器。为了克服投影仪的非线性效应，他们估计了辐射和光谱响应以纠正失真。
- en: A more complex scenario is to hide nonplanar objects like cars from the detector.
    Because the 3D renderer is not differential, and the vehicle detector is usually
    a black box. CAMOU (Zhang et al., [2018b](#bib.bib167)) used a distilled model
    to learn the behavior of the 3D renderer and detector jointly. When camouflage
    is changed, the original distilled model may fail to fit the 3D transformations
    and vehicle detector. Therefore, they alternatively train the distilled model
    and optimize the adversarial patches. Some studies utilized a 3D differential
    renderer to map 3D adversarial mesh or camouflage to 2D images. Xiao et al. (Xiao
    et al., [2019](#bib.bib151)) proposed MeshAdv to hide cars from 2D detectors by
    printable 3D mesh. They leverage a differential renderer (Kato et al., [2018](#bib.bib63))
    to map 3D adversarial mesh to 2D images. Laplacian loss is applied on the mesh
    vertices to improve the smoothness. DAS (Wang et al., [2021b](#bib.bib138)) hides
    the car from the detector by simultaneously distracting the model and human attention.
    They optimize the model attention distraction loss function by minimizing the
    salient region and decreasing the salient region’s pixel values. To make the image
    unnoticeable to humans, they initial the adversarial camouflage as a natural image
    and then constrain the perturbation in its edge area. AMPLE (Ji et al., [2021](#bib.bib59))
    can hide or create a car through a sensor-injection attack by rotating and vibrating
    the camera to blur the image. It uses Bayesian methods to optimize the rotation
    and vibration parameters. To cheat the detector in multiple viewpoints, FCA (Wang
    et al., [2022a](#bib.bib137)) colored the adversarial camouflage onto the car’s
    whole surface through a differential renderer. The loss function includes three
    parts. The first part is to cut down the IoU between the original and predicted
    boxes. The second part is to decrease the objectness confidence. The third part
    is to lower the predicted logits of the target class. Suryato et al. (Suryanto
    et al., [2022](#bib.bib128)) found that the differential neural renderer that
    previous works used fails to perform diverse physical world conversions because
    of an absence of domination of environment variances. Therefore, they proposed
    DTA attack to hide the car from the detector, which utilized a differential transformation
    network to get photorealistic images.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更复杂的场景是将非平面物体如汽车隐藏在探测器的视野之外。因为3D渲染器不是微分的，且车辆探测器通常是一个黑箱。CAMOU（Zhang et al.,
    [2018b](#bib.bib167)）使用了一个提炼模型来联合学习3D渲染器和探测器的行为。当伪装发生变化时，原始提炼模型可能无法适应3D变换和车辆探测器。因此，他们交替训练提炼模型并优化对抗补丁。一些研究利用了3D微分渲染器将3D对抗网格或伪装映射到2D图像。Xiao
    et al.（Xiao et al., [2019](#bib.bib151)）提出了MeshAdv，通过可打印的3D网格将汽车隐藏在2D探测器中。他们利用了一个微分渲染器（Kato
    et al., [2018](#bib.bib63)）将3D对抗网格映射到2D图像上。在网格顶点上应用Laplacian损失，以提高平滑度。DAS（Wang
    et al., [2021b](#bib.bib138)）通过同时分散模型和人类注意力来隐藏汽车。他们通过最小化显著区域并减少显著区域的像素值来优化模型注意力分散损失函数。为了使图像对人类不易察觉，他们将对抗伪装初始为自然图像，然后在其边缘区域约束扰动。AMPLE（Ji
    et al., [2021](#bib.bib59)）通过旋转和振动相机来模糊图像，从而通过传感器注入攻击隐藏或创建一辆车。它使用贝叶斯方法来优化旋转和振动参数。为了在多个视角中欺骗探测器，FCA（Wang
    et al., [2022a](#bib.bib137)）将对抗伪装涂在汽车的整个表面上，通过一个微分渲染器。损失函数包括三个部分。第一部分是减少原始框和预测框之间的IoU。第二部分是降低物体置信度。第三部分是降低目标类别的预测logits。Suryato
    et al.（Suryanto et al., [2022](#bib.bib128)）发现之前工作的微分神经渲染器无法执行多样的物理世界转换，因为缺乏对环境变化的主导。因此，他们提出了DTA攻击来隐藏汽车，该攻击利用微分变换网络获取逼真的图像。
- en: Besides face recognition and self-driving scenario, the safety of other scenarios
    against adversarial attacks also are investigated. Liu et al. (Liu et al., [2020a](#bib.bib74))
    evaluated the safety of the automatic check-out system through universal adversarial
    patches. They leveraged perceptual and semantic biases of models to improve the
    generalization ability. Hu et al. (Hu et al., [2021](#bib.bib52)) leveraged StyleGAN
    and BigGAN to generate adversarial patches with natural-looking content to evade
    from person detector. They minimized the objectness score and classification confidence
    of the target class simultaneously. Agarwal et al. (Agarwal et al., [2020](#bib.bib2))
    leveraged the noise produced by the environment and imaging process to reduce
    the classification accuracy of black-box models. The noises are extracted through
    Gaussian and Laplacian filters.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 除了人脸识别和自动驾驶场景外，还研究了其他场景对抗攻击的安全性。刘等人（Liu et al.，[2020a](#bib.bib74)）通过通用对抗贴片评估了自动结账系统的安全性。他们利用模型的感知和语义偏见来提高其泛化能力。胡等人（Hu
    et al.，[2021](#bib.bib52)）利用StyleGAN和BigGAN来生成具有自然外观内容的对抗贴片，以规避人员检测器。他们同时最小化目标类别的客体分数和分类置信度。阿加瓦尔等人（Agarwal
    et al.，[2020](#bib.bib2)）利用环境和成像过程产生的噪音来降低黑盒模型的分类准确性。这些噪音是通过高斯滤波器和拉普拉斯滤波器提取的。
- en: 6\. Adversarial attacks for 3D deep learning models
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 3D深度学习模型的对抗攻击
- en: 6.1\. The difference between 3D and 2D adversarial attacks
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 3D和2D对抗攻击的不同之处
- en: 'As a result of affordable 3D data acquisition devices and the rich information
    provided by the geometry feature, 3D data are widely used in many safe-critical
    tasks. As a result, their security also attracts more and more attention. Compared
    with 2D adversarial attacks, 3D adversarial attacks have some significant differences:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可以负担得起的3D数据采集设备和几何特征提供的丰富信息，在许多关键安全任务中广泛使用3D数据。因此，它们的安全性也越来越受到关注。与 2D 对抗攻击相比，3D
    对抗攻击有一些显著的差异：
- en: •
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compared with images, point clouds contain point-wise coordinates with unordered
    and irregular sampling values. Some 2D attacks that utilize the image structure
    cannot be directly applied to these kinds of data, such as the boundary attack,
    which estimates the boundary point by the weighted average of pixel value between
    source and target images. Interpolation between point clouds directly will deform
    the 3D shapes.
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与图像不同，点云包含无序和不规则采样值的点坐标。一些利用图像结构的2D攻击无法直接应用于这类数据，例如边界攻击，该攻击通过源图像和目标图像之间的像素值加权平均估计边界点。直接在点云之间进行插值会使3D形状变形。
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 3D adversarial attacks have a larger disturbance space and degree of freedom
    than 2D adversarial attacks. In addition to modifying the value, the adversary
    can add and delete points.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3D对抗攻击比2D对抗攻击具有更大的扰动空间和自由度。除了修改数值外，对手还可以添加和删除点。
- en: •
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unlike 2D adversarial attacks, which use all pixels for classification, point
    clouds do not use all points. Due to the large number of points, the point cloud
    is usually sampled before the classification. For example, PointNet sampled 1024
    points for 3D object classification on the ModelNet40 dataset.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与2D对抗攻击不同的是，点云在分类时并不使用所有点。由于点的数量众多，点云在分类之前通常需要取样。例如，PointNet在ModelNet40数据集上对3D对象进行分类时取样了1024个点。
- en: •
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most 2D adversarial attacks are based on pixel-wise $L_{p}$ distance. However,
    point-wise attacks can be easily defended by outlier point removal, and $L_{p}$
    distance is unsuitable for disordered data, like the point cloud.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数2D对抗攻击都基于像素级的$L_{p}$距离。然而，点对攻击可以通过异常点移除来轻松防御，而$L_{p}$距离并不适合无序数据，比如点云。
- en: Therefore, previous cannot be directly used for 3D adversarial attacks. To solve
    these problems, 3D adversarial attacks are proposed for 3D data specifically.
    We will introduce these attacks according to their algorithms and scenarios.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以前的方法不能直接用于3D对抗攻击。为了解决这些问题，特别针对3D数据提出了3D对抗攻击。我们将根据它们的算法和场景介绍这些攻击。
- en: 6.2\. Catalog of 3D adversarial attacks
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 3D对抗攻击目录
- en: We summarize the recent works of 3D digital and physical adversarial attacks
    in Table.[5](#S6.T5 "Table 5 ‣ 6.3.1\. Gradient-based method ‣ 6.3\. 3D adversarial
    attacks in the digital world ‣ 6\. Adversarial attacks for 3D deep learning models
    ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks"). According to the attack algorithms used for generating
    adversarial examples, we classify the 3D adversarial attacks into the following
    categories,
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格中总结了最近的3D数字和物理对抗攻击工作。[5](#S6.T5 "表5 ‣ 6.3.1\. 基于梯度的方法 ‣ 6.3\. 数字世界中的3D对抗攻击
    ‣ 6\. 针对3D深度学习模型的对抗攻击 ‣ 关于2D和3D深度学习模型对抗攻击的鲁棒性和安全性的调查")。根据用于生成对抗样本的攻击算法，我们将3D对抗攻击分为以下几类，
- en: •
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Gradient-based optimization methods. For most 3D adversarial attacks, gradient-based
    methods are used. Some of them are based on $C\&amp;W$ loss, such as (Xiao et al.,
    [2018a](#bib.bib150)) and (Tsai et al., [2020](#bib.bib133)), while others are
    based on fast-gradient methods, such as (Liu et al., [2019](#bib.bib77)). Some
    works relax $L_{0}$-norm attack to differential versions, such as (Zheng et al.,
    [2019b](#bib.bib175)).
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于梯度的优化方法。对于大多数3D对抗攻击，使用的是基于梯度的方法。其中一些基于 $C\&W$ 损失，例如（Xiao et al., [2018a](#bib.bib150)）和（Tsai
    et al., [2020](#bib.bib133)），而其他的则基于快速梯度方法，例如（Liu et al., [2019](#bib.bib77)）。一些工作将
    $L_{0}$-范数攻击放宽到微分版本，例如（Zheng et al., [2019b](#bib.bib175)）。
- en: •
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generative-model-based methods. Some works, like AdvPC (Hamdi et al., [2020](#bib.bib46))
    and LG-GAN (Zhou et al., [2020](#bib.bib176)), utilize a generative model to generate
    adversarial point clouds. These attacks can improve the transferability of 3D
    adversarial examples.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于生成模型的方法。一些工作，如AdvPC（Hamdi et al., [2020](#bib.bib46)）和LG-GAN（Zhou et al.,
    [2020](#bib.bib176)），利用生成模型生成对抗点云。这些攻击可以提高3D对抗样本的可转移性。
- en: •
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Heuristic-algorithm-based methods. In black-box settings, the gradient of the
    3D model is unavailable. Hence, some black-box attacks generate adversarial examples
    using heuristic algorithms like evolution algorithms.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 启发式算法方法。在黑盒设置下，3D模型的梯度不可用。因此，一些黑盒攻击使用像进化算法这样的启发式算法生成对抗样本。
- en: •
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 3D-transformation-based methods. Rather than modifying the point cloud, some
    attacks found that 3D models are also vulnerable to rigid-body transformations
    in the 3D Euclidean space, like rotations and translations.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于3D变换的方法。与其修改点云，一些攻击发现3D模型也容易受到3D欧几里得空间中的刚体变换（如旋转和平移）的影响。
- en: Recently, some works also proposed 3D physical-realizable adversarial examples
    by 3D printing (Tsai et al., [2020](#bib.bib133)) or laser emitter (Cao et al.,
    [2019a](#bib.bib16)). In addition, some studies (Wen et al., [2020](#bib.bib145);
    Huang et al., [2022](#bib.bib54)) classified the 3D digital adversarial attacks
    into point dropping, point adding, and point shifting attacks according to different
    perturbation types. However, some 3D attacks are not based on the point cloud,
    such as 3D mesh attack (Cao et al., [2019b](#bib.bib17)) and volumetric network
    attack (Wicker and Kwiatkowska, [2019](#bib.bib146)). Therefore, we classify 3D
    attacks by the algorithms rather than the perturbation types.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些研究还提出了通过3D打印（Tsai et al., [2020](#bib.bib133)）或激光发射器（Cao et al., [2019a](#bib.bib16)）生成3D物理可实现的对抗样本。此外，一些研究（Wen
    et al., [2020](#bib.bib145)；Huang et al., [2022](#bib.bib54)）将3D数字对抗攻击分类为点丢弃、点添加和点移动攻击，依据不同的扰动类型。然而，一些3D攻击并不是基于点云的，比如3D网格攻击（Cao
    et al., [2019b](#bib.bib17)）和体积网络攻击（Wicker and Kwiatkowska, [2019](#bib.bib146)）。因此，我们通过算法而非扰动类型来分类3D攻击。
- en: 6.3\. 3D adversarial attacks in the digital world
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 数字世界中的3D对抗攻击
- en: 6.3.1\. Gradient-based method
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1\. 基于梯度的方法
- en: Some attacks are based on 2D adversarial attacks but with new loss functions.
    The first work of 3D adversarial attack, 3DAdv (Xiao et al., [2018a](#bib.bib150)),
    is based on C&W. It includes the independent point and adversarial cluster attacks.
    The first attack uses Hausdor and Chamfer distance to measure the maximum and
    average distance between the original and the adversarial point cloud. Chamfer
    distance is defined as $\mathcal{D}_{C}(x,x^{\prime})=\frac{1}{\left\|x^{\prime}\right\|}\sum_{p^{\prime}\in
    x^{\prime}}\left(\min_{p\in x}\left\|p^{\prime}-p\right\|_{2}^{2}\right)$. The
    second attack used the Chamfer and the farthest pair-wise distance to generate
    an adversarial cluster. They achieved a 100% ASR with an acceptable noise budget.
    However, they only evaluated the 3D-Adv attack on the PointNet network, which
    was proven less adversarial robust than other 3D models, like PointNet++. Liu
    et.al. (Liu et al., [2019](#bib.bib77)) extended PGD to 3D-PGD. They projected
    the perturbation to the tangent plane of the adversarial point cloud to reduce
    the outlier points. This is equivalent to changing the distribution of the surface
    points. However, Tsai et.al. stated that this attack could be easily defended
    by a resampling method (Tsai et al., [2020](#bib.bib133)).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击基于2D对抗攻击，但具有新的损失函数。第一个3D对抗攻击的工作，3DAdv（肖等，[2018a](#bib.bib150)），基于C&W。它包括独立点攻击和对抗性簇攻击。第一个攻击使用Hausdor和Chamfer距离来测量原始点云和对抗点云之间的最大和平均距离。Chamfer距离定义为$\mathcal{D}_{C}(x,x^{\prime})=\frac{1}{\left\|x^{\prime}\right\|}\sum_{p^{\prime}\in
    x^{\prime}}\left(\min_{p\in x}\left\|p^{\prime}-p\right\|_{2}^{2}\right)$。第二个攻击使用Chamfer距离和最远的成对距离来生成对抗性簇。他们在可接受的噪声预算下达到了100%的ASR。然而，他们仅在PointNet网络上评估了3D-Adv攻击，而PointNet已被证明比其他3D模型（如PointNet++）的对抗鲁棒性差。刘等（刘等，[2019](#bib.bib77)）将PGD扩展到3D-PGD。他们将扰动投影到对抗点云的切平面上，以减少异常点。这相当于改变表面点的分布。然而，蔡等人指出，这种攻击可以通过重新采样方法轻松防御（蔡等，[2020](#bib.bib133)）。
- en: 'Some attacks discard important points or generate points with specific shapes.
    Zheng et al. (Zheng et al., [2019b](#bib.bib175)) evaluated each point’s importance
    through point discarding. Because point discarding is non-differential, they relax
    it by moving points towards the point cloud’s interior. Then, the critical score
    is evaluated by the directional derivative in the spherical coordinates. ShapeAdv
    (Liu et al., [2020c](#bib.bib78)) includes three kinds of perturbations: uniform
    distribution perturbation, adversarial sticks, and adversarial sinks. The first
    one generates evenly distributed perturbation through resampling during the optimization.
    The second one adds auxiliary features like adversarial sticks. However, because
    the stick’s angle is difficult to optimize, they use a projection and clip function
    to approximate it. The last one pulls points into the inside of the point cloud.
    MinimalAdv (Kim et al., [2021](#bib.bib65)) fools the classifier by manipulating
    fewest points. They formulated the problem as a $L_{0}$ and $L_{2}$ optimization
    problem and relaxed it to $L_{1}$ problem. However, their method drops significantly
    after the resampling process because only a few points are adversarial, and most
    points (¿95%) are benign.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击会丢弃重要的点或生成具有特定形状的点。郑等（郑等，[2019b](#bib.bib175)）通过点丢弃评估每个点的重要性。由于点丢弃是非差分的，他们通过将点移向点云内部来放宽这一过程。然后，通过在球面坐标系中的方向导数来评估关键得分。ShapeAdv（刘等，[2020c](#bib.bib78)）包括三种扰动：均匀分布扰动、对抗性棒和对抗性汇。第一个通过在优化过程中重新采样生成均匀分布的扰动。第二个则添加了诸如对抗性棒的辅助特征。然而，由于棒的角度难以优化，他们使用投影和剪切函数来近似它。最后一个则将点拉入点云内部。MinimalAdv（金等，[2021](#bib.bib65)）通过操作最少的点来欺骗分类器。他们将问题形式化为$L_{0}$和$L_{2}$优化问题，并将其放宽为$L_{1}$问题。然而，由于只有少量点是对抗性的，而大多数点（¿95%）是良性的，他们的方法在重新采样过程中显著下降。
- en: Some attacks are proposed to improve the smoothness of the AEs. Tsai et.al.
    (Tsai et al., [2020](#bib.bib133)) used Chamfer and kNN distance to generate a
    geometry-aware AE. The kNN loss can restrict the distance between neighboring
    points in the point cloud and, hence, can significantly reduce outlier points.
    They successfully achieved targeted attacks on PointNet++ in the physical world
    by 3D-printed objects. However, their method results in rough surfaces. $GeoA^{3}$
    (Wen et al., [2020](#bib.bib145)) improves the fidelity of AE by combining Chamfer
    distance and the consistency of local curvatures between $\mathcal{P}$ and $\mathcal{P}^{\prime}$,
    which is measured through the direction of normal vectors. Moreover, to make $\mathcal{P}^{\prime}$
    more robust to resampling, they proposed $GeoA_{+}^{3}$ that contains a uniformity
    loss to promote the regularity of surface points. Then, they proposed iterative
    normal projection to optimize this objective function. Huang et al. (Huang et al.,
    [2022](#bib.bib54)) proposed shape-invariant attack to improve the AE’s surface
    smoothness. They shift the point along the tangent plane of the surface so that
    the perturbations are more imperceptible than the previous methods. Moreover,
    they combined the surrogate model and query-based methods to improve the black-box
    attack’s efficiency. They first calculated a sensitivity map on the surrogate
    model according to the maximum gradient magnitude. Then, they shifted points according
    to the sensitivity ranking. This query method can reduce the query times by about
    20% compared with SimBA (Guo et al., [2019a](#bib.bib43)), and SimBA++ (Yang et al.,
    [2020](#bib.bib156)) attacks.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击被提出以提高对抗样本的平滑性。Tsai 等（Tsai et al., [2020](#bib.bib133)）使用Chamfer和kNN距离生成几何感知对抗样本。kNN损失可以限制点云中邻近点之间的距离，从而显著减少异常点。他们成功地通过3D打印物体在物理世界中对PointNet++进行目标攻击。然而，他们的方法导致了粗糙的表面。$GeoA^{3}$（Wen
    et al., [2020](#bib.bib145)）通过结合Chamfer距离和$\mathcal{P}$与$\mathcal{P}^{\prime}$之间的局部曲率一致性来提高对抗样本的真实性，这一一致性通过法向量的方向来测量。此外，为了使$\mathcal{P}^{\prime}$对重采样更具鲁棒性，他们提出了包含均匀性损失的$GeoA_{+}^{3}$，以促进表面点的规则性。随后，他们提出了迭代法向量投影来优化这一目标函数。Huang
    等（Huang et al., [2022](#bib.bib54)）提出了形状不变攻击以提高对抗样本的表面平滑性。他们沿着表面的切平面移动点，使得扰动比以前的方法更不易察觉。此外，他们结合了替代模型和基于查询的方法以提高黑箱攻击的效率。他们首先根据最大梯度幅度在替代模型上计算灵敏度图。然后，他们根据灵敏度排名移动点。该查询方法可以将查询次数减少约20%，相比于SimBA（Guo
    et al., [2019a](#bib.bib43)）和SimBA++（Yang et al., [2020](#bib.bib156)）攻击。
- en: Recently, statistical outlier removal (SOR) has been proposed to defend against
    adversarial point clouds. To break this countermeasure, JGBA (Ma et al., [2020](#bib.bib85))
    embeds SOR into the attack algorithm. But because SOR is not differentiable, they
    replace it with a relaxation function using first-order approximation. Then, they
    optimize the original and SOR-filtered point cloud simultaneously. This attack
    successfully defeated SOR and SOR-based DUP-Net defenses.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，统计异常值移除（SOR）被提出用于防御对抗点云。为了破解这一对策，JGBA（Ma et al., [2020](#bib.bib85)）将SOR嵌入到攻击算法中。但由于SOR不可导，他们用一阶近似的放松函数代替它。然后，他们同时优化原始点云和SOR滤波后的点云。这种攻击成功击败了SOR及基于SOR的DUP-Net防御。
- en: Some attacks aim to improve the transferability of 3D adversarial examples.
    ITA (Liu and Hu, [2022](#bib.bib76)) limits the perturbation along the direction
    of the normal vector and improves the transferability through adversarial transformations.
    The adversarial transformation composes a simple two-layer neural network and
    is learned through an adversarial training procedure. AOF (Liu et al., [2022](#bib.bib75))
    improves the transferability by boosting the classification loss of low-frequency
    component, which is separated from the original PC by orthogonal decomposition
    of the graph Laplacian matrix.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击旨在提高3D对抗样本的可迁移性。ITA（Liu and Hu, [2022](#bib.bib76)）限制扰动沿法向量方向，并通过对抗变换提高可迁移性。对抗变换由一个简单的两层神经网络组成，并通过对抗训练过程进行学习。AOF（Liu
    et al., [2022](#bib.bib75)）通过提升低频组件的分类损失来提高可迁移性，该组件通过图拉普拉斯矩阵的正交分解从原始点云中分离出来。
- en: Table 5\. Summary of main digital and physical adversarial attacks in 3D CV
    tasks sorted by the algorithm and published year
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 主要数字和物理对抗攻击在3D CV任务中的总结，按算法和发表年份排序
- en: '| Attacks | Year | Threat Model | Method | Distance^(**) | Performance | Key
    idea |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方式 | 年份 | 威胁模型 | 方法 | 距离^(**) | 性能 | 关键思想 |'
- en: '| Goal | Knowl^* | Interface | Efficiency | Succ. |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Goal | 知识^* | 接口 | 效率 | 成功 |'
- en: '| 3DAdv (Xiao et al., [2018a](#bib.bib150)) | 2019 | T | $\square$ | Digital
    | Gradient | $L_{2}$, $Cf.$ | Efficient | 99% | Chamfer loss |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 3DAdv (Xiao et al., [2018a](#bib.bib150)) | 2019 | T | $\square$ | 数字 | 梯度
    | $L_{2}$, $Cf.$ | 高效 | 99% | Chamfer 损失 |'
- en: '| 3D-PGD(Liu et al., [2019](#bib.bib77)) | 2019 | U | $\square$ | Digital |
    Gradient | $L_{2}$ | Efficient | 88.10% | Gradient projection |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 3D-PGD(Liu et al., [2019](#bib.bib77)) | 2019 | U | $\square$ | 数字 | 梯度 |
    $L_{2}$ | 高效 | 88.10% | 梯度投影 |'
- en: '| Saliency(Zheng et al., [2019b](#bib.bib175)) | 2019 | U | $\square$ | Digital
    | Gradient | $L_{0}$ | Efficient | 40% | Saliency map |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| Saliency(Zheng et al., [2019b](#bib.bib175)) | 2019 | U | $\square$ | 数字
    | 梯度 | $L_{0}$ | 高效 | 40% | 显著性图 |'
- en: '| LidarAdv (Cao et al., [2019b](#bib.bib17)) | 2019 | T&U | $\square$ | Physical
    | Gradient | $L_{2},Lp.$ | Costly | 71% | Proxy function |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| LidarAdv (Cao et al., [2019b](#bib.bib17)) | 2019 | T&U | $\square$ | 物理
    | 梯度 | $L_{2},Lp.$ | 成本高 | 71% | 代理函数 |'
- en: '| Adv-Lidar (Cao et al., [2019a](#bib.bib16)) | 2019 | T&U | $\square$ | Physical
    | Gradient | $L_{0}$ | Costly | 75% | Global sampling |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Adv-Lidar (Cao et al., [2019a](#bib.bib16)) | 2019 | T&U | $\square$ | 物理
    | 梯度 | $L_{0}$ | 成本高 | 75% | 全球采样 |'
- en: '| KNN(Tsai et al., [2020](#bib.bib133)) | 2020 | U&T | $\square$ | Physical
    | Gradient | $Cf.,kNN$ | Efficient | 94.69% | 3D printable |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| KNN(Tsai et al., [2020](#bib.bib133)) | 2020 | U&T | $\square$ | 物理 | 梯度
    | $Cf.,kNN$ | 高效 | 94.69% | 3D 可打印 |'
- en: '| Rooftop(Tu et al., [2020](#bib.bib135)) | 2020 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Physical | Gradient | $Lp.$ | Costly | 80% | Adversarial mesh |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| Rooftop(Tu et al., [2020](#bib.bib135)) | 2020 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | 物理 | 梯度 | $Lp.$ | 成本高 | 80% | 对抗网格 |'
- en: '| $GeoA^{3}$, $GeoA_{+}^{3}$(Wen et al., [2020](#bib.bib145)) | 2020 | T |
    $\square$ | Digital | Gradient | $Cf.,Hd.,LC$ | Efficient | 100% | Local curvatures
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| $GeoA^{3}$, $GeoA_{+}^{3}$(Wen et al., [2020](#bib.bib145)) | 2020 | T |
    $\square$ | 数字 | 梯度 | $Cf.,Hd.,LC$ | 高效 | 100% | 局部曲率 |'
- en: '| ShapeAdv(Liu et al., [2020c](#bib.bib78)) | 2020 | U | $\square$ | Digital
    | Gradient | $L_{2}$ | Costly | 95% | Shape attack |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| ShapeAdv(Liu et al., [2020c](#bib.bib78)) | 2020 | U | $\square$ | 数字 | 梯度
    | $L_{2}$ | 成本高 | 95% | 形状攻击 |'
- en: '| JGBA (Ma et al., [2020](#bib.bib85)) | 2020 | T&U | $\square$  $\color[rgb]{.5,.5,.5}\blacksquare$
    | Digital | Gradient | $L_{2}$ | Efficient | 98.90% | Break SOR |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| JGBA (Ma et al., [2020](#bib.bib85)) | 2020 | T&U | $\square$  $\color[rgb]{.5,.5,.5}\blacksquare$
    | 数字 | 梯度 | $L_{2}$ | 高效 | 98.90% | 打破 SOR |'
- en: '| CTRI(Zhao et al., [2020b](#bib.bib170)) | 2020 | T | $\square$ | Digital
    | Gradient | Spectral | Efficient | 98% | Restricted isometry |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| CTRI(Zhao et al., [2020b](#bib.bib170)) | 2020 | T | $\square$ | 数字 | 梯度
    | 光谱 | 高效 | 98% | 受限等距 |'
- en: '| MinimalAdv(Kim et al., [2021](#bib.bib65)) | 2021 | U | $\square$ | Digital
    | Gradient | $L_{2},L_{0},Cf.,Hd.$ | Costly | 89% | Minimal perturbation |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| MinimalAdv(Kim et al., [2021](#bib.bib65)) | 2021 | U | $\square$ | 数字 |
    梯度 | $L_{2},L_{0},Cf.,Hd.$ | 成本高 | 89% | 最小扰动 |'
- en: '| ShapeInv(Huang et al., [2022](#bib.bib54)) | 2022 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Digital | Gradient | $L_{\infty}$ | Efficient | 100.00% | Sensitivity map |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| ShapeInv(Huang et al., [2022](#bib.bib54)) | 2022 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | 数字 | 梯度 | $L_{\infty}$ | 高效 | 100.00% | 敏感度图 |'
- en: '| AOF (Liu et al., [2022](#bib.bib75)) | 2022 | U | $\square$ | Physical |
    Gradient | $L_{\infty}$ | Costly | 99.76% | Low frequency |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| AOF (Liu et al., [2022](#bib.bib75)) | 2022 | U | $\square$ | 物理 | 梯度 | $L_{\infty}$
    | 成本高 | 99.76% | 低频 |'
- en: '| AdvPC (Hamdi et al., [2020](#bib.bib46)) | 2020 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Digital | GAN | $L_{\infty}$ | Costly | 64.40% | GAN |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| AdvPC (Hamdi et al., [2020](#bib.bib46)) | 2020 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | 数字 | GAN | $L_{\infty}$ | 成本高 | 64.40% | GAN |'
- en: '| LG-GAN (Zhou et al., [2020](#bib.bib176)) | 2020 | T | $\square$ | Digital
    | GAN | $L_{2}$ | Efficient | 97% | Generative model |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| LG-GAN (Zhou et al., [2020](#bib.bib176)) | 2020 | T | $\square$ | 数字 | GAN
    | $L_{2}$ | 高效 | 97% | 生成模型 |'
- en: '| ITA (Liu and Hu, [2022](#bib.bib76)) | 2022 | T&U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Digital | GAN | $L_{2},Cf.$ | Efficient | 29.89% | Adv. transform |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| ITA (Liu and Hu, [2022](#bib.bib76)) | 2022 | T&U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | 数字 | GAN | $L_{2},Cf.$ | 高效 | 29.89% | 对抗变换 |'
- en: '| EvolutionAdv(Cao et al., [2019b](#bib.bib17)) | 2019 | U | $\color[rgb]{0,0,0}\blacksquare$
    | Physical | Evolution | $-$ | Costly | 62% | Evolution |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| EvolutionAdv(Cao et al., [2019b](#bib.bib17)) | 2019 | U | $\color[rgb]{0,0,0}\blacksquare$
    | 物理 | 进化 | $-$ | 成本高 | 62% | 进化 |'
- en: '| Camdar-adv (Chen and Huang, [2021](#bib.bib19)) | 2021 | T | $\color[rgb]{0,0,0}\blacksquare$
    | Physical | Evolution | $Lp.$ | Costly | 99% | Multi-modality |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Camdar-adv (Chen and Huang, [2021](#bib.bib19)) | 2021 | T | $\color[rgb]{0,0,0}\blacksquare$
    | 物理 | 进化 | $Lp.$ | 成本高 | 99% | 多模态 |'
- en: '| ISO (Wicker and Kwiatkowska, [2019](#bib.bib146)) | 2019 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Digital | Greedy | $L_{0}$ | Costly | 100% | Critical point set |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| ISO（Wicker 和 Kwiatkowska，[2019](#bib.bib146)） | 2019 | U | $\square$ $\color[rgb]{0,0,0}\blacksquare$
    | 数字 | 贪婪 | $L_{0}$ | 昂贵 | 100% | 临界点集 |'
- en: '| OcclusionPoint(Sun et al., [2020](#bib.bib127)) | 2020 | U | $\color[rgb]{0,0,0}\blacksquare$
    | Physical | Greedy | $L_{0}$ | Efficient | 80% | Lidar occlusion |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| OcclusionPoint（Sun 等，[2020](#bib.bib127)） | 2020 | U | $\color[rgb]{0,0,0}\blacksquare$
    | 物理 | 贪婪 | $L_{0}$ | 高效 | 80% | 激光雷达遮挡 |'
- en: '| TSI(Zhao et al., [2020b](#bib.bib170)) | 2020 | U | $\color[rgb]{0,0,0}\blacksquare$
    | Digital | Random | $-$ | Efficient | 95% | Thompson sampling |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| TSI（赵等，[2020b](#bib.bib170)） | 2020 | U | $\color[rgb]{0,0,0}\blacksquare$
    | 数字 | 随机 | $-$ | 高效 | 95% | 汤普森采样 |'
- en: •
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '* This column is the adversarial knowledge of different attacks. $\square$:
    white-box. $\blacksquare$: black-box. ${\color[rgb]{.5,.5,.5}\blacksquare}$: gray-box.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '* 这一列是不同攻击的对抗知识。 $\square$: 白盒。 $\blacksquare$: 黑盒。 ${\color[rgb]{.5,.5,.5}\blacksquare}$:
    灰盒。'
- en: •
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '** Cf.: Chamfer distance. Hd: Hausdorff distance. Lp: Laplacian distance. LC:
    Local curvature distance.'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '** 比较：Chamfer 距离。Hd：Hausdorff 距离。Lp：Laplacian 距离。LC：局部曲率距离。'
- en: 6.3.2\. Generative-model-based methods
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. 基于生成模型的方法
- en: Hamdi et al. (Hamdi et al., [2020](#bib.bib46)) proposed AdvPC attack, which
    used GAN model to produce an adversarial point cloud. The loss of AdvPC has two
    parts. The first one is the pre-trained classifier’s loss. The second is the auto-encoder
    loss to ensure that the reconstructed point cloud can still fool the classifier.
    Experiments show that AdvPC attacks can improve the transferability of adversarial
    point clouds. Moreover, AdvPC outperforms 3D-adv and KNN attacks on several different
    defense methods. However, they only evaluated untargeted attacks.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: Hamdi 等（Hamdi 等，[2020](#bib.bib46)）提出了 AdvPC 攻击，该攻击使用 GAN 模型生成对抗点云。AdvPC 的损失有两个部分。第一个是预训练分类器的损失。第二个是自编码器损失，以确保重建的点云仍能欺骗分类器。实验表明，AdvPC
    攻击可以提高对抗点云的可迁移性。此外，AdvPC 在几种不同的防御方法上优于 3D-adv 和 KNN 攻击。然而，他们只评估了非目标攻击。
- en: LG-GAN (Zhou et al., [2020](#bib.bib176)) generated targeted adversarial examples
    through the multi-branch generative network. It first learns the multi-layer features
    of the input 3D data through the GAN and then unitizes a class encoder to mix
    the label information into the multi-layer features. At last, the 3D data are
    rebuilt from the features. The GAN includes a classification loss, a $L_{2}$-norm
    reconstruction loss, and a graph adversarial loss.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: LG-GAN（Zhou 等，[2020](#bib.bib176)）通过多分支生成网络生成针对性的对抗样本。它首先通过 GAN 学习输入 3D 数据的多层特征，然后利用一个类编码器将标签信息混合到多层特征中。最后，从特征中重建
    3D 数据。GAN 包括一个分类损失，一个 $L_{2}$-范数重建损失，以及一个图对抗损失。
- en: 6.3.3\. Heuristic-algorithm-based methods
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3\. 基于启发式算法的方法
- en: In the black-box scenario, the gradient is hard to obtain. Therefore, black-box
    attacks are often based on heuristic algorithms. Cao et al. (Cao et al., [2019b](#bib.bib17))
    proposed an evolution-based black-box algorithm EvolutionAdv. They set the population
    as mesh vertices of the object and the adaption function as $-L(f(x^{adv}))$ and
    randomly add some novel individuals at each period with perturbations sampling
    from a normal distribution. Wicker et al. (Wicker and Kwiatkowska, [2019](#bib.bib146))
    proposed an Iterative Salience Occlusion attack (ISO) to break PointNet and volumetric
    networks through a greedy algorithm. They first identify the vital point set through
    queries. Then, they drop the most critical points iteratively until the classification
    result is false.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在黑盒场景中，梯度很难获取。因此，黑盒攻击通常基于启发式算法。Cao 等（Cao 等，[2019b](#bib.bib17)）提出了一种基于进化的黑盒算法
    EvolutionAdv。他们将种群设置为对象的网格顶点，并将适应函数设置为 $-L(f(x^{adv}))$，并在每个周期随机添加一些新的个体，扰动样本来自正态分布。Wicker
    等（Wicker 和 Kwiatkowska，[2019](#bib.bib146)）提出了一种迭代显著性遮挡攻击（ISO），通过贪婪算法打破 PointNet
    和体积网络。他们首先通过查询识别重要点集。然后，他们迭代地删除最关键的点，直到分类结果错误。
- en: 6.3.4\. 3D-transformation-based methods
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4\. 基于 3D 变换的方法
- en: TSI (Zhao et al., [2020b](#bib.bib170)) found that 3D models are vulnerable
    to affine transformations like rotations and transitions. Therefore, they proposed
    to attack the 3D model by isometric transformation. A random algorithm based on
    Thompson sampling is proposed to optimize the rotation angles. Compared with random
    sampling, Thompson sampling is more productive and can unitize the prior knowledge.
    CTRI (Zhao et al., [2020b](#bib.bib170)) searched an adversarial isometric transformation
    (such as rotation) by minimizing the spectral-norm loss, which is equivalent to
    finding a smallest $\delta$ such that $(1-\delta)\left\|x\right\|^{2}\leq\left\|Ax\right\|^{2}\leq(1+\delta)\left\|x\right\|^{2}$,
    where $A$ is the rotation matrix and $x$ is the point cloud.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: TSI（Zhao 等人，[2020b](#bib.bib170)）发现 3D 模型对仿射变换如旋转和过渡比较脆弱。因此，他们提出通过等距变换攻击 3D
    模型。提出了一种基于 Thompson 采样的随机算法来优化旋转角度。与随机采样相比，Thompson 采样更具生产力，并且可以利用先验知识。CTRI（Zhao
    等人，[2020b](#bib.bib170)）通过最小化谱范数损失来寻找对抗性等距变换（如旋转），这等同于寻找一个最小的 $\delta$ 使得 $(1-\delta)\left\|x\right\|^{2}\leq\left\|Ax\right\|^{2}\leq(1+\delta)\left\|x\right\|^{2}$，其中
    $A$ 是旋转矩阵，$x$ 是点云。
- en: 6.4\. The safety impact of physical-realizable 3D adversarial attacks
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 物理可实现的 3D 对抗攻击的安全影响
- en: In recent years, 3D physical adversarial attacks have been proposed to cheat
    Lidar-based detectors through printable mesh (Tu et al., [2020](#bib.bib135))
    or laser emitter (Cao et al., [2019a](#bib.bib16)). These attacks pose severe
    threats to real-world applications. We classify these attacks into simulation-based
    and physical-system-based 3D attacks based on their implementation methods.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，3D 物理对抗攻击已经被提出，用于通过可打印的网格（Tu 等人，[2020](#bib.bib135)）或激光发射器（Cao 等人，[2019a](#bib.bib16)）欺骗基于
    Lidar 的检测器。这些攻击对实际应用构成了严重威胁。我们根据其实现方法将这些攻击分为基于仿真和基于物理系统的 3D 攻击。
- en: 6.4.1\. Simulation-based 3D attacks
  id: totrans-418
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1\. 基于仿真的 3D 攻击
- en: Some of them are implemented in the simulation setting through a Lidar renderer.
    For example, Cao et al. (Cao et al., [2019b](#bib.bib17)) proposed an optimization-based
    algorithm LidarAdv for hiding 3D mesh from Lidar detection. They first soften
    the preprocess function through the differential proxy function. Then, they generated
    adversarial mesh through $L_{2}$ loss. However, they only evaluated their attack
    on the Apollo platform in the simulation setting rather than the newest detection
    models. Tu et al. (Tu et al., [2020](#bib.bib135)) proposed Rooftop attack to
    hide the self-driving car from Lidar by placing an adversarial mesh on the vehicle.
    They optimize the initial mesh through a fooling loss and a Laplacian loss to
    improve its smoothness. Their vehicle-agnostic perturbation can achieve an 80%
    occlusion rate. But they also only simulated their attack with CAD models and
    a Lidar renderer.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些通过 Lidar 渲染器在仿真设置中实现。例如，Cao 等人（Cao 等人，[2019b](#bib.bib17)）提出了一种基于优化的算法 LidarAdv，用于隐藏
    Lidar 检测中的 3D 网格。他们首先通过差分代理函数来软化预处理函数。然后，他们通过 $L_{2}$ 损失生成对抗性网格。然而，他们仅在仿真设置中的
    Apollo 平台上评估了他们的攻击，而非最新的检测模型。Tu 等人（Tu 等人，[2020](#bib.bib135)）提出了 Rooftop 攻击，通过在车辆上放置对抗性网格来隐藏自动驾驶汽车免受
    Lidar 检测。他们通过欺骗损失和拉普拉斯损失来优化初始网格，以提高其平滑度。他们的车辆无关扰动可以达到 80% 的遮挡率。但他们也仅使用 CAD 模型和
    Lidar 渲染器模拟了他们的攻击。
- en: 6.4.2\. Physical-system-based 3D attacks
  id: totrans-420
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2\. 基于物理系统的 3D 攻击
- en: Later, some attacks are proposed to attack real-world Lidar sensors. Cao et
    al. (Cao et al., [2019a](#bib.bib16)) proposed Adv-Lidar that can mislead a real-world
    self-driving system to detect nonexistent obstacles. Rather than modifying 3D
    mesh, they manipulate the 3D point cloud by laser emitter instead. They first
    analyzed a real-world self-driving system and found that previous attacks cannot
    create unreal barriers because of the preprocessing steps. So, they considered
    the preprocessing steps in the optimization. Then, they added adversarial points
    into the pristine point cloud in the limited angle and distance ranges. In addition,
    they used global sampling to avoid being caught in a local minimum. Their method
    successfully cheated a real-world self-driving system. Chen et al. (Chen and Huang,
    [2021](#bib.bib19)) proposed a multi-modality attack that attacks Lidar and the
    camera simultaneously through the 3D adversarial mesh and 2D adversarial patch.
    They used the evolution algorithm to find 2D perturbations and used Tu et al.’s
    method (Tu et al., [2020](#bib.bib135)) to find 3D adversarial mesh. Sun et al.
    (Sun et al., [2020](#bib.bib127)) proposed a black-box attack against the Lidar
    detector in the self-driving setting. They noticed that the detect models are
    vulnerable to distant and occluded vehicle attacks, in which they can fool the
    detector by just a few points. Their attacks can cheat BEV-based, voxel-based,
    and point-based 3D object detectors.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，一些攻击方法被提出以攻击现实世界中的激光雷达传感器。曹等（Cao et al., [2019a](#bib.bib16)）提出了Adv-Lidar，可以误导现实世界的自动驾驶系统检测不存在的障碍物。他们不是修改3D网格，而是通过激光发射器操控3D点云。他们首先分析了一个现实世界的自动驾驶系统，发现以前的攻击无法创建虚假的障碍物，因为预处理步骤的存在。因此，他们在优化中考虑了预处理步骤。然后，他们在有限的角度和距离范围内将对抗点添加到原始点云中。此外，他们使用全局采样以避免陷入局部最小值。他们的方法成功欺骗了一个现实世界的自动驾驶系统。陈等（Chen
    and Huang, [2021](#bib.bib19)）提出了一种多模态攻击，通过3D对抗网格和2D对抗补丁同时攻击激光雷达和相机。他们使用进化算法找到2D扰动，并使用Tu等的方法（Tu
    et al., [2020](#bib.bib135)）找到3D对抗网格。孙等（Sun et al., [2020](#bib.bib127)）提出了一种黑箱攻击，针对自动驾驶设置中的激光雷达探测器。他们注意到探测模型对远距离和遮挡的车辆攻击很脆弱，其中只需几个点就能欺骗探测器。他们的攻击可以欺骗基于BEV、体素和点的3D物体检测器。
- en: 7\. Future directions and challenges
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 未来方向与挑战
- en: 7.1\. Improving the transferability of adversarial examples
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 提高对抗样本的可迁移性
- en: A real-world feasible adversarial attack should be able to attack unseen models.
    Therefore, the transferability of adversarial samples has attracted more and more
    attention. There are several promising directions to improve the transferability
    of adversarial examples, including
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实可行的对抗攻击应该能够攻击未见过的模型。因此，对抗样本的可迁移性受到越来越多的关注。有几个有前景的方向可以提高对抗样本的可迁移性，包括
- en: •
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Random/adversarial transformations. These methods assume that adversarial examples
    that can survive random or adversarial transformations can better transfer between
    different models. The existing methods include isometry transformation (Dong et al.,
    [2019](#bib.bib33)), random resizing (Xie et al., [2019](#bib.bib155)), multi-scale
    images (Lin et al., [2019](#bib.bib73)), or learnable adversarial transformation
    (Wu et al., [2021](#bib.bib149)). However, most of these transformations are very
    simple. It is still unknown why such vanilla transformations can improve the transferability
    and whether better transformations exist.
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机/对抗变换。这些方法假设能够经受随机或对抗变换的对抗样本可以更好地在不同模型之间迁移。现有方法包括等距变换（Dong et al., [2019](#bib.bib33)）、随机缩放（Xie
    et al., [2019](#bib.bib155)）、多尺度图像（Lin et al., [2019](#bib.bib73)），或可学习的对抗变换（Wu
    et al., [2021](#bib.bib149)）。然而，这些变换大多数非常简单。尚不清楚为什么这些基础变换能提高可迁移性，以及是否存在更好的变换。
- en: •
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generating perturbation by generative model. Some works (Xiao et al., [2018a](#bib.bib150),
    [2021](#bib.bib153)) proposed optimizing the latent feature can improve transferability.
    However, in our experiment, we find that searching adversarial perturbation on
    the latent space may cause the optimization to be unstable or non-converging.
    So, there is still room for improving the generative model’s architecture and
    loss function.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过生成模型生成扰动。一些研究（Xiao et al., [2018a](#bib.bib150), [2021](#bib.bib153)）提出优化潜在特征可以提高可迁移性。然而，在我们的实验中，我们发现对潜在空间中的对抗扰动进行搜索可能导致优化不稳定或不收敛。因此，生成模型的架构和损失函数还有改进的空间。
- en: •
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Manipulating the latent layer features. Some works (Ganeshan et al., [2019](#bib.bib38);
    Huang et al., [2019](#bib.bib55); Wang et al., [2021a](#bib.bib143); Zhang et al.,
    [2022b](#bib.bib164)) maximize the middle layer features’ difference between the
    pristine and adversarial images to improve the transferability. But Zhang et al.
    (Zhang et al., [2022b](#bib.bib164)) have shown that these methods also have great
    promotion space.
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 操作潜在层特征。一些研究（Ganeshan et al., [2019](#bib.bib38); Huang et al., [2019](#bib.bib55);
    Wang et al., [2021a](#bib.bib143); Zhang et al., [2022b](#bib.bib164)）通过最大化原始图像与对抗图像之间中间层特征的差异来提高转移性。然而，Zhang
    et al.（Zhang et al., [2022b](#bib.bib164)）表明这些方法也有很大的提升空间。
- en: •
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ensemble-based approaches. Some works also improve transferability by attacking
    an ensemble of models (Dong et al., [2018](#bib.bib32); Yin et al., [2021](#bib.bib157)).
    However, training multiple substitute models is computationally expensive. How
    to train ensemble models efficiently and how to design or select substitute models
    are still open problems.
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于集成的方法。一些研究通过攻击多个模型的集成来提高转移性（Dong et al., [2018](#bib.bib32); Yin et al., [2021](#bib.bib157)）。然而，训练多个替代模型的计算成本很高。如何高效地训练集成模型以及如何设计或选择替代模型仍然是未解决的问题。
- en: Moreover, at present, most of these methods improve transferability empirically.
    Few of them proved a lower bound of the transferability or explained the mechanism
    behind the transferability. One possible way of understanding transferability
    is through the similarity of decision boundaries. For example, Liu et al. (Liu
    et al., [2016](#bib.bib81)), and Tramèr et al. (Tramèr et al., [2017b](#bib.bib132))
    analyzed the decision boundary of different models and contributed the transferability
    to their similar boundaries. However, this theory cannot explain the asymmetry
    of transferability (Wu et al., [2018](#bib.bib148)). Ilyas et al. (Ilyas et al.,
    [2019](#bib.bib58)) suggests adversarial perturbation is non-robust features rather
    than noise, and different models may learn the same non-robust features. Therefore,
    the AEs can transfer between them. However, they only prove their hypothesis on
    a simple dataset with two classes and cannot explain why sometimes the two models
    predict the same AE as different false labels. Finding more reasonable theoretical
    interpretations of transferability is still an urgent task for robust deep learning.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，目前大多数这些方法是通过经验来提高转移性的。很少有方法证明了转移性的下界或解释了转移性背后的机制。一种理解转移性的方法是通过决策边界的相似性。例如，Liu
    et al.（Liu et al., [2016](#bib.bib81)）和Tramèr et al.（Tramèr et al., [2017b](#bib.bib132)）分析了不同模型的决策边界，并将转移性归因于它们相似的边界。然而，这一理论无法解释转移性的非对称性（Wu
    et al., [2018](#bib.bib148)）。Ilyas et al.（Ilyas et al., [2019](#bib.bib58)）认为对抗性扰动是非鲁棒特征而不是噪声，不同的模型可能学习相同的非鲁棒特征。因此，对抗样本可以在它们之间转移。然而，他们仅在一个具有两个类别的简单数据集上验证了他们的假设，并无法解释为什么有时两个模型对同一个对抗样本给出不同的错误标签。寻找更合理的转移性理论解释仍然是鲁棒深度学习的一个紧迫任务。
- en: 7.2\. Towards semantic perturbation
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2. 向语义扰动迈进
- en: 'Imperceptible perturbation by $L_{p}$-norm distance is vulnerable to physical
    variables, like distance and background light. Therefore, some works proposed
    to fool the classifier while maintaining semantic consistency for human beings.
    At present, the main directions and challenges of semantic perturbation include:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 $L_{p}$-范数距离产生的不可感知扰动容易受到物理变量的影响，如距离和背景光。因此，一些研究提出在保持人类语义一致性的同时欺骗分类器。目前，语义扰动的主要方向和挑战包括：
- en: •
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Color-space transformations. Some studies claimed that the DNN is biased toward
    texture, while human beings like to classify objects according to their shapes
    (Geirhos et al., [2018](#bib.bib39)). Therefore, some works operated color distortion
    in different color spaces to generate semantic-preserving perturbation. However,
    how to better measure the perceptual distance to be consistent with human perception
    is still an open problem.
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 颜色空间变换。一些研究声称深度神经网络偏向于纹理，而人类则喜欢根据物体的形状来分类（Geirhos et al., [2018](#bib.bib39)）。因此，一些研究在不同的颜色空间中操作颜色失真以生成语义保持的扰动。然而，如何更好地测量感知距离以与人类感知一致仍然是一个未解决的问题。
- en: •
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Global/Local spatial transformations. Global and local spatial transformations
    can also generate adversarial examples while reserving semantics. However, because
    it’s hard to compute gradients regarding the transformation parameters, there
    is still a demand to find an efficient way to optimize the transformation parameters.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全局/局部空间变换。全局和局部空间变换也可以在保留语义的同时生成对抗样本。然而，由于难以计算有关变换参数的梯度，仍然需要找到一种有效的方式来优化变换参数。
- en: •
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Manipulating image content attributes. Some attacks utilized the conditional
    GAN to modify the image content attributes. However, better disentangling the
    latent features and controlling the attributes still need research.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 操作图像内容属性。一些攻击利用条件生成对抗网络（GAN）来修改图像内容属性。然而，更好地解开潜在特征和控制属性仍需要研究。
- en: •
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Attacks based on diffusion model. Recently, the diffusion model (Ho et al.,
    [2020](#bib.bib50)) has been proposed to produce high-quality images, which has
    outperformed GAN on some tasks. A few works have used them for generating unconstrained
    semantic AEs (Chen et al., [2023](#bib.bib25)). But these works usually estimate
    the gradients roughly. A more precise method to calculate the gradients, like
    the stochastic differential equation, may improve the performance.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于扩散模型的攻击。最近，扩散模型（Ho 等，[2020](#bib.bib50)）被提出用于生成高质量图像，在某些任务上超越了GAN。一些研究已使用这些模型生成不受约束的语义对抗样本（Chen
    等，[2023](#bib.bib25)）。但这些研究通常粗略估计梯度。像随机微分方程这样的更精确的梯度计算方法可能会提高性能。
- en: In addition, although many works explore the semantic-reserving perturbations,
    few of them tried to apply themselves to the physical world to bypass the camera
    system. There still is a question of whether these semantic perturbations can
    behave consistently in the physical setting.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管许多研究探索了语义保留的扰动，但很少有研究尝试将其应用于物理世界以绕过相机系统。仍然存在这些语义扰动在物理环境中是否能一致表现的问题。
- en: 7.3\. Making adversarial examples physically achievable
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. 使对抗样本在物理上可实现
- en: Generating physical-realizable AEs that are robust enough to physical condition
    changes and hard to notice simultaneously is still an open question. Physical
    attacks are more difficult than digital attacks because of the changing physical
    variables like view angles and distance. Some work (Athalye et al., [2018b](#bib.bib7);
    Brown et al., [2017](#bib.bib13)) proposed to improve the attack robustness through
    expectation over transformation (EoT). However, EoT significantly improves the
    perturbation size. One possible solution to this trade-off is using semantic perturbation,
    like colorization-based attacks. The multi-objective optimization strategy (Jia
    et al., [2022](#bib.bib60)) is also a possible way to balance stealthiness and
    attack strength.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 生成在物理条件变化下足够强健且难以察觉的物理可实现对抗样本仍然是一个未解问题。物理攻击比数字攻击更为困难，因为存在视角和距离等变化的物理变量。一些研究（Athalye
    等，[2018b](#bib.bib7); Brown 等，[2017](#bib.bib13)）提出通过变换期望（EoT）来提高攻击的鲁棒性。然而，EoT显著增加了扰动的大小。一种可能的解决方案是使用语义扰动，如基于颜色化的攻击。多目标优化策略（Jia
    等，[2022](#bib.bib60)）也是平衡隐蔽性和攻击强度的一个可能方法。
- en: Moreover, because implementing physical experiments needs lots of expenditure,
    some researchers simulated the physical adversarial attack through a differential
    3D renderer. However, current rendering techniques are susceptible to geometric
    and lighting transformations that distort the synthesized image. The gap between
    realistic photos and rendered images can affect the attack success rate or even
    make the adversarial attack ineffective. There is still a need to find better
    renderer or simulation methods or construct real-world datasets for evaluation.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于实施物理实验需要大量费用，一些研究者通过差分3D渲染器模拟物理对抗攻击。然而，当前的渲染技术对几何和光照变化敏感，容易扭曲合成图像。真实照片与渲染图像之间的差距可能会影响攻击成功率，甚至使对抗攻击失效。仍需寻找更好的渲染器或模拟方法，或构建真实世界的数据集以进行评估。
- en: In addition, present physical AEs often attack the target deep learning model
    directly. However, off-the-shelf applications often contain complete pipelines,
    including data acquisition and preprocessing, which may unexpectedly influence
    the effect of AEs. For example, the image filter and the point cloud sampling
    process may influence the ASRs. There is still promotion space for present physical
    attacks to compromise these off-the-shelf applications.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，目前的物理对抗样本通常直接攻击目标深度学习模型。然而，现成的应用程序通常包含完整的管道，包括数据采集和预处理，这可能会意外地影响对抗样本的效果。例如，图像滤镜和点云采样过程可能会影响攻击成功率。目前的物理攻击在突破这些现成应用程序方面仍有提升空间。
- en: 7.4\. Designing efficient 3D black-box adversarial attacks
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 设计高效的3D黑箱对抗攻击
- en: Only a few works (Cao et al., [2019b](#bib.bib17); Sun et al., [2020](#bib.bib127);
    Wicker and Kwiatkowska, [2019](#bib.bib146)) explored the 3D black-box adversarial
    attacks, while most used very primitive or heuristic algorithms like random sampling,
    greedy search, and evolution algorithm. Many techniques like priors-transferring
    and boundary estimation have been exploited for 2D black-box attacks. Migrating
    these skills to 3D data may foster more efficient algorithms with fewer queries.
    For example, recently, 3DHacker (Tao et al., [2023](#bib.bib130)) used boundary
    estimation to improve the query efficiency of hard-label attacks.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 仅有少数研究 (Cao et al., [2019b](#bib.bib17); Sun et al., [2020](#bib.bib127); Wicker
    and Kwiatkowska, [2019](#bib.bib146)) 探讨了3D黑箱对抗攻击，而大多数使用了非常原始或启发式的算法，如随机采样、贪婪搜索和进化算法。许多技术如先验转移和边界估计已被用于2D黑箱攻击。将这些技能迁移到3D数据可能会促进更高效的算法，减少查询次数。例如，最近，3DHacker
    (Tao et al., [2023](#bib.bib130)) 使用边界估计来提高硬标签攻击的查询效率。
- en: 7.5\. Evaluating the robustness of newly proposed models
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5\. 评估新提议模型的鲁棒性
- en: Novel 2D and 3D computer vision models’ robustness, e.g., Transformer (Vaswani
    et al., [2017](#bib.bib136)) and Point Transformer (Zhao et al., [2021a](#bib.bib168))
    , still need to be studies. Additionally, likelihood-based generative models have
    emerged, like the diffusion model (Ho et al., [2020](#bib.bib50)), which can produce
    high-quality images. Some pilot studies have already evaluated their robustness
    (Zhuang et al., [2023](#bib.bib178)). But these works are very rudimentary, and
    there is a lot of room for improvement. Moreover, the diffusion model is also
    helpful to improve the performance of generative-model-based attacks, such as
    unrestricted semantic attacks. However, how to backward its gradients to improve
    attack efficiency and how to disentangle its latent noises to control image attributes
    better still needs further research.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 新型2D和3D计算机视觉模型的鲁棒性，例如，Transformer (Vaswani et al., [2017](#bib.bib136)) 和 Point
    Transformer (Zhao et al., [2021a](#bib.bib168))，仍需进一步研究。此外，基于似然的生成模型，如扩散模型 (Ho
    et al., [2020](#bib.bib50))，能够生成高质量图像。一些初步研究已经评估了它们的鲁棒性 (Zhuang et al., [2023](#bib.bib178))。但这些工作仍然很初步，还有很大的改进空间。此外，扩散模型也有助于提升生成模型攻击的性能，例如无限制语义攻击。然而，如何反向传播梯度以提高攻击效率，以及如何解开其潜在噪声以更好地控制图像属性仍需进一步研究。
- en: 7.6\. Evaluating the safety of novel CV applications
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6\. 评估新型计算机视觉应用的安全性
- en: An increasing number of novel computer vision applications have incorporated
    deep learning, such as medical image processing, rain and fog removal, and pedestrian
    detection. The security requirements and potential attack surfaces of these emerging
    tasks vary a lot. Consequently, designing adversarial attacks for these novel
    tasks necessitates special considerations. For example, Schmalfuss et al. proposed
    an adversarial weather attack for motion estimation (Schmalfuss et al., [2023](#bib.bib115))
    that simulates weather effects by utilizing adversarially optimized particles.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的新型计算机视觉应用已纳入深度学习，例如医学图像处理、雨雾去除和行人检测。这些新兴任务的安全需求和潜在攻击面差异很大。因此，为这些新任务设计对抗攻击需要特别考虑。例如，Schmalfuss
    等人提出了一种用于运动估计的对抗天气攻击 (Schmalfuss et al., [2023](#bib.bib115))，通过利用对抗优化的粒子模拟天气效应。
- en: 7.7\. Breaking newly proposed defenses
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7\. 破解新提议的防御措施
- en: Many adversarial attacks are proposed specifically targeting certain defense
    mechanisms, such as the C&W attack to break the defensive distillation, the BPDA
    attack to break gradient shattering, and the BlindSpot attack to break the adversarial
    training. With the development of this race of adversarial machine learning, an
    increasing number of novel defenses are proposed, such as provable defenses and
    adversarial purification (Nie et al., [2022](#bib.bib98)). It is still unknown
    whether these defense mechanisms can be defeated by stronger adversarial attacks.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 许多对抗攻击特别针对某些防御机制，例如 C&W 攻击用于突破防御蒸馏，BPDA 攻击用于突破梯度破碎，BlindSpot 攻击用于突破对抗训练。随着对抗机器学习竞赛的发展，提出了越来越多的新型防御措施，如可证明的防御和对抗净化（Nie
    等，[2022](#bib.bib98)）。这些防御机制是否能被更强的对抗攻击击败仍然未知。
- en: 8\. Conclusion
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: In this survey, we comprehensively review the recent progress of adversarial
    attacks that damage the robustness and safety of deep learning models in computer
    vision tasks. In contrast to previous works, we summarize the adversarial attacks
    for 3D computer vision for the first time. Moreover, we extend the connotation
    of adversarial examples to imperceptible and semantic perturbations. For semantic
    perturbations, we systematically summarize the latest methods, including local
    and global spatial transformation, color space distortion, and attribution modification.
    What’s more, we investigate the physically realizable adversarial attacks towards
    safety-critical missions like self-driving vehicles and face recognition. In the
    end, we give some understanding of improving the transferability of AEs, making
    the AEs physically achievable, boosting the 3D black-box attack’s efficiency,
    evaluating the robustness of emerging models, and designing adversarial attacks
    for novel applications and defenses.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们全面回顾了损害深度学习模型在计算机视觉任务中鲁棒性和安全性的对抗攻击的最新进展。与之前的工作相比，我们首次总结了3D计算机视觉中的对抗攻击。此外，我们将对抗样本的涵义扩展到不可察觉的和语义扰动方面。对于语义扰动，我们系统地总结了最新的方法，包括局部和全局空间变换、颜色空间失真和属性修改。更重要的是，我们调查了针对安全关键任务（如自动驾驶车辆和人脸识别）的物理可实现对抗攻击。最后，我们对提高对抗样本的可转移性、使对抗样本在物理上可实现、提高3D黑箱攻击的效率、评估新兴模型的鲁棒性以及为新应用和防御设计对抗攻击等方面进行了理解。
- en: References
  id: totrans-459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Agarwal et al. (2020) Akshay Agarwal, Mayank Vatsa, Richa Singh, and Nalini K
    Ratha. 2020. Noise is inside me! generating adversarial perturbations with noise
    derived from natural filters. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops*. 774–775.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等（2020） Akshay Agarwal、Mayank Vatsa、Richa Singh 和 Nalini K Ratha。2020。噪声在我内部！生成源自自然滤波器的对抗扰动。在
    *IEEE/CVF计算机视觉与模式识别会议工作坊论文集*。第774–775页。
- en: Al-Dujaili and O’Reilly (2019) Abdullah Al-Dujaili and Una-May O’Reilly. 2019.
    There are no bit parts for sign bits in black-box attacks. *arXiv preprint arXiv:1902.06894*
    (2019).
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Dujaili 和 O’Reilly（2019） Abdullah Al-Dujaili 和 Una-May O’Reilly。2019。黑箱攻击中没有位段用于符号位。*arXiv预印本
    arXiv:1902.06894*（2019）。
- en: 'Alaifari et al. (2018) Rima Alaifari, Giovanni S Alberti, and Tandri Gauksson.
    2018. ADef: an Iterative Algorithm to Construct Adversarial Deformations. In *International
    Conference on Learning Representations*.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alaifari 等（2018） Rima Alaifari、Giovanni S Alberti 和 Tandri Gauksson。2018。ADef:
    一种构造对抗变形的迭代算法。在 *国际学习表示会议*。'
- en: 'Andriushchenko et al. (2020) Maksym Andriushchenko, Francesco Croce, Nicolas
    Flammarion, and Matthias Hein. 2020. Square attack: a query-efficient black-box
    adversarial attack via random search. In *European Conference on Computer Vision*.
    Springer, 484–501.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Andriushchenko 等（2020） Maksym Andriushchenko、Francesco Croce、Nicolas Flammarion
    和 Matthias Hein。2020。Square attack: 一种通过随机搜索的查询高效黑箱对抗攻击。在 *欧洲计算机视觉大会*。Springer，第484–501页。'
- en: 'Athalye et al. (2018a) Anish Athalye, Nicholas Carlini, and David Wagner. 2018a.
    Obfuscated gradients give a false sense of security: Circumventing defenses to
    adversarial examples. In *International conference on machine learning*. PMLR,
    274–283.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athalye 等（2018a） Anish Athalye、Nicholas Carlini 和 David Wagner。2018a。模糊梯度给人一种虚假的安全感：绕过对抗样本的防御。在
    *国际机器学习会议*。PMLR，第274–283页。
- en: Athalye et al. (2018b) Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin
    Kwok. 2018b. Synthesizing robust adversarial examples. In *International conference
    on machine learning*. PMLR, 284–293.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athalye 等（2018b） Anish Athalye、Logan Engstrom、Andrew Ilyas 和 Kevin Kwok。2018b。合成鲁棒的对抗样本。在
    *国际机器学习会议*。PMLR，第284–293页。
- en: Aydin et al. (2021) Ayberk Aydin, Deniz Sen, Berat Tuna Karli, Oguz Hanoglu,
    and Alptekin Temizel. 2021. Imperceptible Adversarial Examples by Spatial Chroma-Shift.
    In *Proceedings of the 1st International Workshop on Adversarial Learning for
    Multimedia*. 8–14.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aydin等人（2021）Ayberk Aydin、Deniz Sen、Berat Tuna Karli、Oguz Hanoglu和Alptekin Temizel。2021年。通过空间色差实现不可察觉的对抗样本。见于*第1届国际多媒体对抗学习研讨会论文集*。8–14。
- en: 'Baluja and Fischer (2018) Shumeet Baluja and Ian Fischer. 2018. Learning to
    attack: Adversarial transformation networks. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 32.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baluja和Fischer（2018）Shumeet Baluja和Ian Fischer。2018年。学习攻击：对抗变换网络。见于*AAAI人工智能会议论文集*，第32卷。
- en: Bhagoji et al. (2018) Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song.
    2018. Practical black-box attacks on deep neural networks using efficient query
    mechanisms. In *Proceedings of the European Conference on Computer Vision (ECCV)*.
    154–169.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhagoji等人（2018）Arjun Nitin Bhagoji、Warren He、Bo Li和Dawn Song。2018年。利用高效查询机制对深度神经网络进行实际黑盒攻击。见于*欧洲计算机视觉会议（ECCV）论文集*。154–169。
- en: Bhattad et al. (2019) Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and
    DA Forsyth. 2019. Unrestricted Adversarial Examples via Semantic Manipulation.
    In *International Conference on Learning Representations*.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhattad等人（2019）Anand Bhattad、Min Jin Chong、Kaizhao Liang、Bo Li和DA Forsyth。2019年。通过语义操控实现无限制对抗样本。见于*国际学习表征会议*。
- en: 'Brendel et al. (2017) Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2017.
    Decision-based adversarial attacks: Reliable attacks against black-box machine
    learning models. *arXiv preprint arXiv:1712.04248* (2017).'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brendel等人（2017）Wieland Brendel、Jonas Rauber和Matthias Bethge。2017年。基于决策的对抗攻击：针对黑盒机器学习模型的可靠攻击。*arXiv预印本
    arXiv:1712.04248*（2017）。
- en: Brown et al. (2017) Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and
    Justin Gilmer. 2017. Adversarial patch. *arXiv preprint arXiv:1712.09665* (2017).
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2017）Tom B Brown、Dandelion Mané、Aurko Roy、Martín Abadi和Justin Gilmer。2017年。对抗性贴片。*arXiv预印本
    arXiv:1712.09665*（2017）。
- en: 'Brunner et al. (2019) Thomas Brunner, Frederik Diehl, Michael Truong Le, and
    Alois Knoll. 2019. Guessing smart: Biased sampling for efficient black-box adversarial
    attacks. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*. 4958–4966.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brunner等人（2019）Thomas Brunner、Frederik Diehl、Michael Truong Le和Alois Knoll。2019年。智能猜测：用于高效黑盒对抗攻击的偏差采样。见于*IEEE/CVF国际计算机视觉会议论文集*。4958–4966。
- en: Byun et al. (2022) Junyoung Byun, Seungju Cho, Myung-Joon Kwon, Hee-Seon Kim,
    and Changick Kim. 2022. Improving the Transferability of Targeted Adversarial
    Examples through Object-Based Diverse Input. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 15244–15253.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Byun等人（2022）Junyoung Byun、Seungju Cho、Myung-Joon Kwon、Hee-Seon Kim和Changick
    Kim。2022年。通过基于对象的多样输入提高目标对抗样本的可转移性。见于*IEEE/CVF计算机视觉与模式识别会议论文集*。15244–15253。
- en: Cao et al. (2019a) Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won
    Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, and Z Morley Mao. 2019a. Adversarial
    sensor attack on lidar-based perception in autonomous driving. In *Proceedings
    of the 2019 ACM SIGSAC conference on computer and communications security*. 2267–2281.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人（2019a）Yulong Cao、Chaowei Xiao、Benjamin Cyr、Yimeng Zhou、Won Park、Sara Rampazzi、Qi
    Alfred Chen、Kevin Fu和Z Morley Mao。2019a年。对基于激光雷达的自动驾驶感知的对抗传感器攻击。见于*2019年ACM SIGSAC计算机与通信安全会议论文集*。2267–2281。
- en: Cao et al. (2019b) Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang
    Yang, Mingyan Liu, and Bo Li. 2019b. Adversarial objects against lidar-based autonomous
    driving systems. *arXiv preprint arXiv:1907.05418* (2019).
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人（2019b）Yulong Cao、Chaowei Xiao、Dawei Yang、Jing Fang、Ruigang Yang、Mingyan
    Liu和Bo Li。2019b年。针对基于激光雷达的自动驾驶系统的对抗物体。*arXiv预印本 arXiv:1907.05418*（2019）。
- en: Carlini and Wagner (2017) Nicholas Carlini and David Wagner. 2017. Towards evaluating
    the robustness of neural networks. In *2017 ieee symposium on security and privacy
    (sp)*. Ieee, 39–57.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini和Wagner（2017）Nicholas Carlini和David Wagner。2017年。评估神经网络鲁棒性的方法。见于*2017年IEEE安全与隐私研讨会（SP）*。IEEE，39–57。
- en: 'Chen and Huang (2021) Chang Chen and Teng Huang. 2021. Camdar-adv: generating
    adversarial patches on 3D object. *International Journal of Intelligent Systems*
    36, 3 (2021), 1441–1453.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen和Huang（2021）Chang Chen和Teng Huang。2021年。Camdar-adv：在3D对象上生成对抗性贴片。*智能系统国际期刊*
    36, 3 (2021), 1441–1453。
- en: 'Chen and Gu (2020) Jinghui Chen and Quanquan Gu. 2020. Rays: A ray searching
    method for hard-label adversarial attack. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 1739–1747.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 和 Gu（2020）Jinghui Chen 和 Quanquan Gu. 2020. Rays: 一种用于硬标签对抗攻击的射线搜索方法。发表于
    *第26届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。1739–1747。'
- en: 'Chen et al. (2020) Jianbo Chen, Michael I Jordan, and Martin J Wainwright.
    2020. Hopskipjumpattack: A query-efficient decision-based attack. In *2020 ieee
    symposium on security and privacy (sp)*. IEEE, 1277–1294.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2020）Jianbo Chen, Michael I Jordan, 和 Martin J Wainwright. 2020. Hopskipjumpattack:
    一种查询高效的基于决策的攻击。发表于 *2020 IEEE 安全与隐私研讨会*。IEEE, 1277–1294。'
- en: 'Chen et al. (2018b) Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui
    Hsieh. 2018b. Ead: elastic-net attacks to deep neural networks via adversarial
    examples. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 32.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2018b）Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, 和 Cho-Jui Hsieh.
    2018b. EAD: 通过对抗样本对深度神经网络进行弹性网络攻击。发表于 *AAAI 人工智能会议论文集*，第32卷。'
- en: 'Chen et al. (2017) Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui
    Hsieh. 2017. Zoo: Zeroth order optimization based black-box attacks to deep neural
    networks without training substitute models. In *Proceedings of the 10th ACM workshop
    on artificial intelligence and security*. 15–26.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2017）Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, 和 Cho-Jui Hsieh.
    2017. Zoo: 基于零阶优化的黑箱攻击，针对深度神经网络，无需训练替代模型。发表于 *第10届ACM人工智能与安全研讨会论文集*。15–26。'
- en: 'Chen et al. (2018a) Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen
    Horng Polo Chau. 2018a. Shapeshifter: Robust physical adversarial attack on faster
    r-cnn object detector. In *Joint European Conference on Machine Learning and Knowledge
    Discovery in Databases*. Springer, 52–68.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2018a）Shang-Tse Chen, Cory Cornelius, Jason Martin, 和 Duen Horng Polo
    Chau. 2018a. Shapeshifter: 针对 Faster R-CNN 目标检测器的稳健物理对抗攻击。发表于 *第六届欧洲机器学习与知识发现数据库联合会议*。Springer,
    52–68。'
- en: Chen et al. (2023) Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong Ding,
    and Wenqiang Zhang. 2023. Content-based Unrestricted Adversarial Attack. *arXiv
    preprint arXiv:2305.10665* (2023).
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2023）Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong Ding, 和 Wenqiang
    Zhang. 2023. 基于内容的无限制对抗攻击。*arXiv 预印本 arXiv:2305.10665* (2023)。
- en: 'Cheng et al. (2020) Minhao Cheng, Simranjit Singh, Patrick H Chen, Pin-Yu Chen,
    Sijia Liu, and Cho-Jui Hsieh. 2020. Sign-OPT: A Query-Efficient Hard-label Adversarial
    Attack. In *International Conference on Learning Representations*.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等（2020）Minhao Cheng, Simranjit Singh, Patrick H Chen, Pin-Yu Chen, Sijia
    Liu, 和 Cho-Jui Hsieh. 2020. Sign-OPT: 一种查询高效的硬标签对抗攻击。发表于 *国际学习表征会议*。'
- en: 'Cheng et al. (2019b) Minhao Cheng, Huan Zhang, Cho-Jui Hsieh, Thong Le, Pin-Yu
    Chen, and Jinfeng Yi. 2019b. Query-efficient hard-label black-box attack: An optimization-based
    approach. In *International Conference on Learning Representations*. International
    Conference on Learning Representations, ICLR.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2019b）Minhao Cheng, Huan Zhang, Cho-Jui Hsieh, Thong Le, Pin-Yu Chen,
    和 Jinfeng Yi. 2019b. 查询高效的硬标签黑箱攻击：一种基于优化的方法。发表于 *国际学习表征会议*。国际学习表征会议，ICLR。
- en: Cheng et al. (2019a) Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun
    Zhu. 2019a. Improving black-box adversarial attacks with a transfer-based prior.
    *Advances in neural information processing systems* 32 (2019).
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2019a）Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, 和 Jun Zhu. 2019a.
    通过基于迁移的先验改进黑箱对抗攻击。*神经信息处理系统进展* 32 (2019)。
- en: 'Croce et al. (2022) Francesco Croce, Maksym Andriushchenko, Naman D Singh,
    Nicolas Flammarion, and Matthias Hein. 2022. Sparse-rs: a versatile framework
    for query-efficient sparse black-box adversarial attacks. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, Vol. 36\. 6437–6445.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Croce 等（2022）Francesco Croce, Maksym Andriushchenko, Naman D Singh, Nicolas
    Flammarion, 和 Matthias Hein. 2022. Sparse-rs: 一个通用的查询高效稀疏黑箱对抗攻击框架。发表于 *AAAI 人工智能会议论文集*，第36卷。6437–6445。'
- en: Croce and Hein (2019) Francesco Croce and Matthias Hein. 2019. Sparse and Imperceivable
    Adversarial Attacks. In *2019 IEEE/CVF International Conference on Computer Vision
    (ICCV)*. 4723–4731. [https://doi.org/10.1109/ICCV.2019.00482](https://doi.org/10.1109/ICCV.2019.00482)
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croce 和 Hein（2019）Francesco Croce 和 Matthias Hein. 2019. 稀疏且难以察觉的对抗攻击。发表于 *2019
    IEEE/CVF 国际计算机视觉会议 (ICCV)*。4723–4731。 [https://doi.org/10.1109/ICCV.2019.00482](https://doi.org/10.1109/ICCV.2019.00482)
- en: 'Dong et al. (2020) Xiaoyi Dong, Dongdong Chen, Jianmin Bao, Chuan Qin, Lu Yuan,
    Weiming Zhang, Nenghai Yu, and Dong Chen. 2020. GreedyFool: Distortion-aware sparse
    adversarial attack. *Advances in Neural Information Processing Systems* 33 (2020),
    11226–11236.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等（2020） Xiaoyi Dong, Dongdong Chen, Jianmin Bao, Chuan Qin, Lu Yuan, Weiming
    Zhang, Nenghai Yu, 和 Dong Chen. 2020. GreedyFool: 失真感知稀疏对抗攻击。*神经信息处理系统进展* 33 (2020),
    11226–11236。'
- en: Dong et al. (2018) Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu,
    Xiaolin Hu, and Jianguo Li. 2018. Boosting adversarial attacks with momentum.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    9185–9193.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2018） Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin
    Hu, 和 Jianguo Li. 2018. 用动量提升对抗攻击。在 *IEEE 计算机视觉与模式识别会议论文集*。 9185–9193。
- en: Dong et al. (2019) Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. 2019. Evading
    defenses to transferable adversarial examples by translation-invariant attacks.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    4312–4321.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2019） Yinpeng Dong, Tianyu Pang, Hang Su, 和 Jun Zhu. 2019. 通过平移不变攻击规避可转移对抗示例的防御。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*。 4312–4321。
- en: 'Duan et al. (2020) Ranjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A. K.
    Qin, and Yun Yang. 2020. Adversarial Camouflage: Hiding Physical-World Attacks
    With Natural Styles. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等（2020） Ranjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A. K. Qin, 和
    Yun Yang. 2020. 对抗伪装：用自然风格隐藏物理世界攻击。 在 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）论文集*。
- en: 'Engstrom et al. (2018) Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig
    Schmidt, and Aleksander Madry. 2018. A rotation and a translation suffice: Fooling
    cnns with simple transformations. (2018).'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engstrom 等（2018） Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt,
    和 Aleksander Madry. 2018. 旋转和位移足够：用简单变换欺骗 CNN。 (2018)。
- en: Eykholt et al. (2018) Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li,
    Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018.
    Robust physical-world attacks on deep learning visual classification. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 1625–1634.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eykholt 等（2018） Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir
    Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, 和 Dawn Song. 2018. 对深度学习视觉分类的强健物理世界攻击。
    在 *IEEE 计算机视觉与模式识别会议论文集*。 1625–1634。
- en: 'Fawzi and Frossard (2015) Alhussein Fawzi and Pascal Frossard. 2015. Manitest:
    Are classifiers really invariant?. In *British Machine Vision Conference (BMVC)*.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fawzi 和 Frossard（2015） Alhussein Fawzi 和 Pascal Frossard. 2015. Manitest: 分类器真的不变吗？
    在 *英国机器视觉会议（BMVC）*。'
- en: 'Ganeshan et al. (2019) Aditya Ganeshan, Vivek BS, and R Venkatesh Babu. 2019.
    Fda: Feature disruptive attack. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 8069–8079.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ganeshan 等（2019） Aditya Ganeshan, Vivek BS, 和 R Venkatesh Babu. 2019. Fda:
    特征破坏攻击。 在 *IEEE/CVF 国际计算机视觉会议论文集*。 8069–8079。'
- en: Geirhos et al. (2018) Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias
    Bethge, Felix A Wichmann, and Wieland Brendel. 2018. ImageNet-trained CNNs are
    biased towards texture; increasing shape bias improves accuracy and robustness.
    *arXiv preprint arXiv:1811.12231* (2018).
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geirhos 等（2018） Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias
    Bethge, Felix A Wichmann, 和 Wieland Brendel. 2018. ImageNet 训练的 CNN 对纹理有偏见；增加形状偏见提高准确性和鲁棒性。
    *arXiv 预印本 arXiv:1811.12231*（2018）。
- en: Gilmer et al. (2018) Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen,
    and George E Dahl. 2018. Motivating the rules of the game for adversarial example
    research. *arXiv preprint arXiv:1807.06732* (2018).
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilmer 等（2018） Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen,
    和 George E Dahl. 2018. 为对抗示例研究动机的游戏规则。 *arXiv 预印本 arXiv:1807.06732*（2018）。
- en: Gnanasambandam et al. (2021) Abhiram Gnanasambandam, Alex M Sherman, and Stanley H
    Chan. 2021. Optical adversarial attack. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 92–101.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gnanasambandam 等（2021） Abhiram Gnanasambandam, Alex M Sherman, 和 Stanley H Chan.
    2021. 光学对抗攻击。 在 *IEEE/CVF 国际计算机视觉会议论文集*。 92–101。
- en: Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    2014. Explaining and harnessing adversarial examples. *arXiv preprint arXiv:1412.6572*
    (2014).
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2014） Ian J Goodfellow, Jonathon Shlens, 和 Christian Szegedy. 2014.
    解释和利用对抗示例。 *arXiv 预印本 arXiv:1412.6572*（2014）。
- en: Guo et al. (2019a) Chuan Guo, Jacob Gardner, Yurong You, Andrew Gordon Wilson,
    and Kilian Weinberger. 2019a. Simple black-box adversarial attacks. In *International
    Conference on Machine Learning*. PMLR, 2484–2493.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等人 (2019a) 郭川、雅各布·加德纳、愉荣·尤、安德鲁·戈登·威尔逊、基利安·温伯格。2019a。简单的黑箱对抗攻击。载于*国际机器学习会议*。PMLR，2484–2493。
- en: 'Guo et al. (2021) Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu,
    Ralph R Martin, and Shi-Min Hu. 2021. Pct: Point cloud transformer. *Computational
    Visual Media* 7, 2 (2021), 187–199.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等人 (2021) 孟浩·郭、俊雄·蔡、郑宁·刘、台江·穆、拉尔夫·R·马丁、施敏·胡。2021。PCT：点云变换器。*计算视觉媒体* 7, 2 (2021)，187–199。
- en: 'Guo et al. (2019b) Yiwen Guo, Ziang Yan, and Changshui Zhang. 2019b. Subspace
    attack: Exploiting promising subspaces for query-efficient black-box attacks.
    *Advances in Neural Information Processing Systems* 32 (2019).'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等人 (2019b) 义文·郭、子昂·阎、常水·张。2019b。子空间攻击：利用有前景的子空间进行查询高效的黑箱攻击。*神经信息处理系统进展* 32
    (2019)。
- en: 'Hamdi et al. (2020) Abdullah Hamdi, Sara Rojas, Ali Thabet, and Bernard Ghanem.
    2020. Advpc: Transferable adversarial perturbations on 3d point clouds. In *European
    Conference on Computer Vision*. Springer, 241–257.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈姆迪等人 (2020) 阿卜杜拉·哈姆迪、萨拉·罗哈斯、阿里·塔贝特、伯纳德·加内姆。2020。ADVPC：3D点云上的可迁移对抗扰动。载于*欧洲计算机视觉会议*。施普林格，241–257。
- en: He et al. (2018) Warren He, Bo Li, and Dawn Song. 2018. Decision boundary analysis
    of adversarial examples. In *International Conference on Learning Representations*.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何等人 (2018) 沃伦·何、博·李、道恩·宋。2018。对抗样本的决策边界分析。载于*国际学习表征会议*。
- en: 'He et al. (2022a) Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen
    He. 2022a. Towards Security Threats of Deep Learning Systems: A Survey. *IEEE
    Transactions on Software Engineering* 48, 5 (2022), 1743–1770.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何等人 (2022a) 英哲·何、国柱·孟、凯·陈、兴博·胡、晋文·何。2022a。深度学习系统的安全威胁：一项调查。*IEEE软件工程学报* 48,
    5 (2022)，1743–1770。
- en: He et al. (2022b) Ziwen He, Wei Wang, Jing Dong, and Tieniu Tan. 2022b. Transferable
    Sparse Adversarial Attack. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 14963–14972.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何等人 (2022b) 子文·何、魏·王、晶·董、铁牛·谭。2022b。可迁移稀疏对抗攻击。载于*IEEE/CVF计算机视觉与模式识别大会论文集*，14963–14972。
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    diffusion probabilistic models. *Advances in neural information processing systems*
    33 (2020), 6840–6851.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍等人 (2020) 乔纳森·霍、阿杰·贾因、彼得·阿比尔。2020。去噪扩散概率模型。*神经信息处理系统进展* 33 (2020)，6840–6851。
- en: Hosseini and Poovendran (2018) Hossein Hosseini and Radha Poovendran. 2018.
    Semantic adversarial examples. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition Workshops*. 1614–1619.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍塞尼和普文德兰 (2018) 霍辛·霍塞尼和拉达·普文德兰。2018。语义对抗样本。载于*IEEE计算机视觉与模式识别会议工作坊论文集*，1614–1619。
- en: Hu et al. (2021) Yu-Chih-Tuan Hu, Bo-Han Kung, Daniel Stanley Tan, Jun-Cheng
    Chen, Kai-Lung Hua, and Wen-Huang Cheng. 2021. Naturalistic physical adversarial
    patch for object detectors. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 7848–7857.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等人 (2021) 余志团·胡、博汉·孔、丹尼尔·斯坦利·谭、俊成·陈、凯龙·华、文煌·程。2021。自然物理对抗贴片用于目标检测器。载于*IEEE/CVF国际计算机视觉会议论文集*，7848–7857。
- en: Hua et al. (2018) Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. 2018. Pointwise
    convolutional neural networks. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*. 984–993.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 华等人 (2018) 笨孫·華、敏开·陈、赛基特·杨。2018。逐点卷积神经网络。载于*IEEE计算机视觉与模式识别会议论文集*，984–993。
- en: Huang et al. (2022) Qidong Huang, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming
    Zhang, and Nenghai Yu. 2022. Shape-invariant 3D Adversarial Point Clouds. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 15335–15344.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 (2022) 戚东·黄、晓怡·董、东东·陈、杭周、伟明·张、能海·余。2022。形状不变的3D对抗点云。载于*IEEE/CVF计算机视觉与模式识别大会论文集*，15335–15344。
- en: Huang et al. (2019) Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie,
    and Ser-Nam Lim. 2019. Enhancing adversarial example transferability with an intermediate
    level attack. In *Proceedings of the IEEE/CVF international conference on computer
    vision*. 4733–4742.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 (2019) 黄倩、伊萨伊·卡茨曼、霍雷斯·赫、泽祁·顾、谢尔盖·贝隆吉、申南·林。2019。通过中间层攻击增强对抗样本的可迁移性。载于*IEEE/CVF国际计算机视觉会议论文集*，4733–4742。
- en: Ilyas et al. (2018b) Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy
    Lin. 2018b. Black-box adversarial attacks with limited queries and information.
    In *International Conference on Machine Learning*. PMLR, 2137–2146.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilyas et al. (2018b) Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy
    Lin. 2018b. 带有限查询和信息的黑盒对抗攻击。见于 *国际机器学习会议*。PMLR，2137–2146。
- en: 'Ilyas et al. (2018a) Andrew Ilyas, Logan Engstrom, and Aleksander Madry. 2018a.
    Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors. In *International
    Conference on Learning Representations*.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilyas et al. (2018a) Andrew Ilyas, Logan Engstrom, and Aleksander Madry. 2018a.
    先验信念：带有强盗和先验的黑盒对抗攻击。见于 *国际学习表征会议*。
- en: Ilyas et al. (2019) Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan
    Engstrom, Brandon Tran, and Aleksander Madry. 2019. Adversarial examples are not
    bugs, they are features. *Advances in neural information processing systems* 32
    (2019).
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilyas et al. (2019) Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan
    Engstrom, Brandon Tran, and Aleksander Madry. 2019. 对抗样本不是错误，它们是特征。*神经信息处理系统进展*
    32 (2019)。
- en: 'Ji et al. (2021) Xiaoyu Ji, Yushi Cheng, Yuepeng Zhang, Kai Wang, Chen Yan,
    Wenyuan Xu, and Kevin Fu. 2021. Poltergeist: Acoustic adversarial machine learning
    against cameras and computer vision. In *2021 IEEE Symposium on Security and Privacy
    (SP)*. IEEE, 160–175.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2021) Xiaoyu Ji, Yushi Cheng, Yuepeng Zhang, Kai Wang, Chen Yan,
    Wenyuan Xu, and Kevin Fu. 2021. Poltergeist：针对摄像头和计算机视觉的声学对抗机器学习。见于 *2021 IEEE安全与隐私研讨会
    (SP)*。IEEE，160–175。
- en: 'Jia et al. (2022) Shuai Jia, Bangjie Yin, Taiping Yao, Shouhong Ding, Chunhua
    Shen, Xiaokang Yang, and Chao Ma. 2022. Adv-attribute: Inconspicuous and transferable
    adversarial attack on face recognition. *Advances in Neural Information Processing
    Systems* 35 (2022), 34136–34147.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia et al. (2022) Shuai Jia, Bangjie Yin, Taiping Yao, Shouhong Ding, Chunhua
    Shen, Xiaokang Yang, and Chao Ma. 2022. Adv-attribute：对人脸识别的不显眼且可转移的对抗攻击。*神经信息处理系统进展*
    35 (2022)，34136–34147。
- en: 'Joshi et al. (2019) Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, and Chinmay
    Hegde. 2019. Semantic adversarial attacks: Parametric transformations that fool
    deep classifiers. In *Proceedings of the IEEE/CVF international conference on
    computer vision*. 4773–4783.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi et al. (2019) Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, and Chinmay
    Hegde. 2019. 语义对抗攻击：欺骗深度分类器的参数变换。见于 *IEEE/CVF国际计算机视觉会议论文集*。4773–4783。
- en: 'Kanbak et al. (2018) Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
    Frossard. 2018. Geometric robustness of deep networks: analysis and improvement.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    4441–4449.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanbak et al. (2018) Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard.
    2018. 深度网络的几何鲁棒性：分析与改进。见于 *IEEE计算机视觉与模式识别会议论文集*。4441–4449。
- en: Kato et al. (2018) Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. 2018.
    Neural 3d mesh renderer. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 3907–3916.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kato et al. (2018) Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. 2018.
    神经3D网格渲染器。见于 *IEEE计算机视觉与模式识别会议论文集*。3907–3916。
- en: Khrulkov and Oseledets (2018) Valentin Khrulkov and Ivan Oseledets. 2018. Art
    of singular vectors and universal adversarial perturbations. In *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition*. 8562–8570.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khrulkov and Oseledets (2018) Valentin Khrulkov and Ivan Oseledets. 2018. 奇异向量艺术与通用对抗扰动。见于
    *IEEE计算机视觉与模式识别会议论文集*。8562–8570。
- en: Kim et al. (2021) Jaeyeon Kim, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung.
    2021. Minimal adversarial examples for deep learning on 3d point clouds. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*. 7797–7806.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2021) Jaeyeon Kim, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung.
    2021. 针对3D点云的最小对抗样本。见于 *IEEE/CVF国际计算机视觉会议论文集*。7797–7806。
- en: Kurakin et al. (2018) Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. 2018.
    Adversarial examples in the physical world. In *Artificial intelligence safety
    and security*. Chapman and Hall/CRC, 99–112.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurakin et al. (2018) Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. 2018.
    物理世界中的对抗样本。见于 *人工智能安全与保障*。Chapman and Hall/CRC，99–112。
- en: Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. Weight
    Poisoning Attacks on Pretrained Models. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*. Association for Computational
    Linguistics, 2793–2806.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. 对预训练模型的权重中毒攻击。见于
    *第58届计算语言学协会年会论文集*。计算语言学协会，2793–2806。
- en: Laidlaw and Feizi (2019) Cassidy Laidlaw and Soheil Feizi. 2019. Functional
    adversarial attacks. *Advances in neural information processing systems* 32 (2019).
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laidlaw 和 Feizi (2019) Cassidy Laidlaw 和 Soheil Feizi. 2019. 功能对抗攻击. *神经信息处理系统进展*
    32 (2019)。
- en: Li et al. (2020) Maosen Li, Cheng Deng, Tengjiao Li, Junchi Yan, Xinbo Gao,
    and Heng Huang. 2020. Towards transferable targeted attack. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*. 641–649.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2020) Maosen Li, Cheng Deng, Tengjiao Li, Junchi Yan, Xinbo Gao, 和 Heng
    Huang. 2020. 朝向可转移的定向攻击. 见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*。641–649。
- en: 'Li et al. (2021) Xiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He,
    Shuhui Wang, Hang Su, and Hui Xue. 2021. Qair: Practical query-efficient black-box
    attacks for image retrieval. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 3330–3339.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2021) Xiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He, Shuhui
    Wang, Hang Su, 和 Hui Xue. 2021. Qair：实用的查询高效黑盒攻击用于图像检索. 见于 *IEEE/CVF 计算机视觉与模式识别会议论文集*。3330–3339。
- en: 'Li et al. (2019) Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing
    Gong. 2019. Nattack: Learning the distributions of adversarial examples for an
    improved black-box attack on deep neural networks. In *International Conference
    on Machine Learning*. PMLR, 3866–3876.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2019) Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, 和 Boqing Gong.
    2019. Nattack：学习对抗样本的分布以改进黑盒攻击深度神经网络. 见于 *国际机器学习会议*。PMLR，3866–3876。
- en: Li et al. (2023) Yanjie Li, Yiquan Li, Xuelong Dai, Songtao Guo, and Bin Xiao.
    2023. Physical-World Optical Adversarial Attacks on 3D Face Recognition. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.
    24699–24708.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) Yanjie Li, Yiquan Li, Xuelong Dai, Songtao Guo, 和 Bin Xiao. 2023.
    物理世界光学对抗攻击对 3D 面部识别的影响. 见于 *IEEE/CVF 计算机视觉与模式识别会议（CVPR）论文集*。24699–24708。
- en: Lin et al. (2019) Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E
    Hopcroft. 2019. Nesterov Accelerated Gradient and Scale Invariance for Adversarial
    Attacks. In *International Conference on Learning Representations*.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2019) Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, 和 John E Hopcroft.
    2019. Nesterov 加速梯度与对抗攻击的尺度不变性. 见于 *国际学习表征会议*。
- en: Liu et al. (2020a) Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi
    Zhang, and Hang Yu. 2020a. Bias-based universal adversarial patch attack for automatic
    check-out. In *European conference on computer vision*. Springer, 395–410.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2020a) Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang,
    和 Hang Yu. 2020a. 基于偏差的通用对抗补丁攻击用于自动结账. 见于 *欧洲计算机视觉会议*。Springer，395–410。
- en: Liu et al. (2022) Binbin Liu, Jinlai Zhang, and Jihong Zhu. 2022. Boosting 3D
    Adversarial Attacks with Attacking On Frequency. *IEEE Access* 10 (2022), 50974–50984.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2022) Binbin Liu, Jinlai Zhang, 和 Jihong Zhu. 2022. 通过在频域上攻击来提升 3D 对抗攻击.
    *IEEE Access* 10 (2022)，50974–50984。
- en: Liu and Hu (2022) Daizong Liu and Wei Hu. 2022. Imperceptible Transfer Attack
    and Defense on 3D Point Cloud Classification. *IEEE Transactions on Pattern Analysis
    and Machine Intelligence* (2022), 1–18.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Hu (2022) Daizong Liu 和 Wei Hu. 2022. 对 3D 点云分类的不可感知转移攻击与防御. *IEEE 模式分析与机器智能学报*
    (2022)，1–18。
- en: Liu et al. (2019) Daniel Liu, Ronald Yu, and Hao Su. 2019. Extending adversarial
    attacks and defenses to deep 3d point cloud classifiers. In *2019 IEEE International
    Conference on Image Processing (ICIP)*. IEEE, 2279–2283.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2019) Daniel Liu, Ronald Yu, 和 Hao Su. 2019. 将对抗攻击和防御扩展到深度 3D 点云分类器.
    见于 *2019 IEEE 国际图像处理会议 (ICIP)*。IEEE，2279–2283。
- en: Liu et al. (2020c) Daniel Liu, Ronald Yu, and Hao Su. 2020c. Adversarial shape
    perturbations on 3d point clouds. In *European Conference on Computer Vision*.
    Springer, 88–104.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2020c) Daniel Liu, Ronald Yu, 和 Hao Su. 2020c. 3D 点云上的对抗形状扰动. 见于 *欧洲计算机视觉会议*。Springer，88–104。
- en: 'Liu et al. (2018) Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor CM
    Leung. 2018. A survey on security threats and defensive techniques of machine
    learning: A data driven view. *IEEE access* 6 (2018), 12103–12117.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2018) Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, 和 Victor CM
    Leung. 2018. 关于机器学习的安全威胁和防御技术的调查：基于数据的视角. *IEEE Access* 6 (2018)，12103–12117。
- en: 'Liu et al. (2020b) Ximeng Liu, Lehui Xie, Yaopeng Wang, Jian Zou, Jinbo Xiong,
    Zuobin Ying, and Athanasios V Vasilakos. 2020b. Privacy and security issues in
    deep learning: A survey. *IEEE Access* 9 (2020), 4566–4593.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2020b) Ximeng Liu, Lehui Xie, Yaopeng Wang, Jian Zou, Jinbo Xiong, Zuobin
    Ying, 和 Athanasios V Vasilakos. 2020b. 深度学习中的隐私和安全问题：综述. *IEEE Access* 9 (2020)，4566–4593。
- en: Liu et al. (2016) Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2016. Delving
    into Transferable Adversarial Examples and Black-box Attacks. In *International
    Conference on Learning Representations*.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2016) Yanpei Liu, Xinyun Chen, Chang Liu, 和 Dawn Song. 2016. 探索可迁移的对抗样本和黑箱攻击。载于*International
    Conference on Learning Representations*。
- en: Luo et al. (2018) Bo Luo, Yannan Liu, Lingxiao Wei, and Qiang Xu. 2018. Towards
    imperceptible and robust adversarial example attacks against neural networks.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 32.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2018) Bo Luo, Yannan Liu, Lingxiao Wei, 和 Qiang Xu. 2018. 朝着不可察觉且鲁棒的对抗样本攻击神经网络。载于*Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 32。
- en: Luo et al. (2022) Cheng Luo, Qinliang Lin, Weicheng Xie, Bizhu Wu, Jinheng Xie,
    and Linlin Shen. 2022. Frequency-driven Imperceptible Adversarial Attack on Semantic
    Similarity. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*. 15315–15324.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2022) Cheng Luo, Qinliang Lin, Weicheng Xie, Bizhu Wu, Jinheng Xie,
    和 Linlin Shen. 2022. 基于频率的不可察觉对抗攻击针对语义相似性。载于*Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*。15315–15324。
- en: Ma et al. (2021) Chen Ma, Xiangyu Guo, Li Chen, Jun-Hai Yong, and Yisen Wang.
    2021. Finding optimal tangent points for reducing distortions of hard-label attacks.
    *Advances in Neural Information Processing Systems* 34 (2021), 19288–19300.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2021) Chen Ma, Xiangyu Guo, Li Chen, Jun-Hai Yong, 和 Yisen Wang.
    2021. 寻找最优切点以减少硬标签攻击的失真。*Advances in Neural Information Processing Systems* 34
    (2021), 19288–19300。
- en: Ma et al. (2020) Chengcheng Ma, Weiliang Meng, Baoyuan Wu, Shibiao Xu, and Xiaopeng
    Zhang. 2020. Efficient joint gradient based attack against sor defense for 3d
    point cloud classification. In *Proceedings of the 28th ACM International Conference
    on Multimedia*. 1819–1827.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2020) Chengcheng Ma, Weiliang Meng, Baoyuan Wu, Shibiao Xu, 和 Xiaopeng
    Zhang. 2020. 针对3D点云分类的高效联合梯度攻击。载于*Proceedings of the 28th ACM International Conference
    on Multimedia*。1819–1827。
- en: 'Machado et al. (2021) Gabriel Resende Machado, Eugênio Silva, and Ronaldo Ribeiro
    Goldschmidt. 2021. Adversarial machine learning in image classification: a survey
    toward the defender’s perspective. *ACM Computing Surveys (CSUR)* 55, 1 (2021),
    1–38.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Machado et al. (2021) Gabriel Resende Machado, Eugênio Silva, 和 Ronaldo Ribeiro
    Goldschmidt. 2021. 图像分类中的对抗机器学习：面向防御者的综述。*ACM Computing Surveys (CSUR)* 55, 1
    (2021), 1–38。
- en: Madry et al. (2017) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, and Adrian Vladu. 2017. Towards deep learning models resistant to adversarial
    attacks. *arXiv preprint arXiv:1706.06083* (2017).
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madry et al. (2017) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, 和 Adrian Vladu. 2017. 朝着抗对抗攻击的深度学习模型。*arXiv preprint arXiv:1706.06083*
    (2017)。
- en: 'Maho et al. (2021) Thibault Maho, Teddy Furon, and Erwan Le Merrer. 2021. Surfree:
    a fast surrogate-free black-box attack. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 10430–10439.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maho et al. (2021) Thibault Maho, Teddy Furon, 和 Erwan Le Merrer. 2021. Surfree：一种快速的无替代黑箱攻击。载于*Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*。10430–10439。
- en: 'Miller et al. (2020) David J Miller, Zhen Xiang, and George Kesidis. 2020.
    Adversarial learning targeting deep neural network classification: A comprehensive
    review of defenses against attacks. *Proc. IEEE* 108, 3 (2020), 402–433.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller et al. (2020) David J Miller, Zhen Xiang, 和 George Kesidis. 2020. 目标深度神经网络分类的对抗学习：针对攻击的防御全面综述。*Proc.
    IEEE* 108, 3 (2020), 402–433。
- en: 'Modas et al. (2019) Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
    Frossard. 2019. Sparsefool: a few pixels make a big difference. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*. 9087–9096.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Modas et al. (2019) Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, 和 Pascal
    Frossard. 2019. Sparsefool：少量像素带来的巨大差异。载于*Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*。9087–9096。
- en: Moon et al. (2019) Seungyong Moon, Gaon An, and Hyun Oh Song. 2019. Parsimonious
    black-box adversarial attacks via efficient combinatorial optimization. In *International
    Conference on Machine Learning*. PMLR, 4636–4645.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon et al. (2019) Seungyong Moon, Gaon An, 和 Hyun Oh Song. 2019. 通过高效组合优化进行简约的黑箱对抗攻击。载于*International
    Conference on Machine Learning*。PMLR, 4636–4645。
- en: Moosavi-Dezfooli et al. (2017) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
    Omar Fawzi, and Pascal Frossard. 2017. Universal adversarial perturbations. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    1765–1773.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moosavi-Dezfooli et al. (2017) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
    Omar Fawzi, 和 Pascal Frossard. 2017. 通用对抗扰动。载于*Proceedings of the IEEE conference
    on computer vision and pattern recognition*。1765–1773。
- en: 'Moosavi-Dezfooli et al. (2016) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
    and Pascal Frossard. 2016. Deepfool: a simple and accurate method to fool deep
    neural networks. In *Proceedings of the IEEE conference on computer vision and
    pattern recognition*. 2574–2582.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moosavi-Dezfooli 等人 (2016) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi 和
    Pascal Frossard. 2016. Deepfool：一种简单而准确的方法来欺骗深度神经网络。收录于 *IEEE 计算机视觉与模式识别会议论文集*，2574–2582。
- en: Mopuri et al. (2018a) Konda Reddy Mopuri, Aditya Ganeshan, and R Venkatesh Babu.
    2018a. Generalizable data-free objective for crafting universal adversarial perturbations.
    *IEEE transactions on pattern analysis and machine intelligence* 41, 10 (2018),
    2452–2465.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mopuri 等人 (2018a) Konda Reddy Mopuri, Aditya Ganeshan 和 R Venkatesh Babu. 2018a.
    通用对抗扰动的可泛化无数据目标。*IEEE 模式分析与机器智能事务* 41, 10 (2018), 2452–2465。
- en: 'Mopuri et al. (2018b) Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, and R Venkatesh
    Babu. 2018b. Nag: Network for adversary generation. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 742–751.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mopuri 等人 (2018b) Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg 和 R Venkatesh
    Babu. 2018b. Nag：对抗样本生成网络。收录于 *IEEE 计算机视觉与模式识别会议论文集*，742–751。
- en: Narodytska and Kasiviswanathan (2017) Nina Narodytska and Shiva Prasad Kasiviswanathan.
    2017. Simple Black-Box Adversarial Attacks on Deep Neural Networks.. In *CVPR
    Workshops*, Vol. 2\. 2.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narodytska 和 Kasiviswanathan (2017) Nina Narodytska 和 Shiva Prasad Kasiviswanathan.
    2017. 简单的黑箱对抗攻击针对深度神经网络。收录于 *CVPR 工作坊*，第 2 卷，2。
- en: 'Nguyen et al. (2020) Dinh-Luan Nguyen, Sunpreet S Arora, Yuhang Wu, and Hao
    Yang. 2020. Adversarial light projection attacks on face recognition systems:
    A feasibility study. In *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition workshops*. 814–815.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人 (2020) Dinh-Luan Nguyen, Sunpreet S Arora, Yuhang Wu 和 Hao Yang. 2020.
    面部识别系统中的对抗光投影攻击：可行性研究。收录于 *IEEE/CVF 计算机视觉与模式识别会议工作坊论文集*，814–815。
- en: Nie et al. (2022) Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat,
    and Anima Anandkumar. 2022. Diffusion models for adversarial purification. *arXiv
    preprint arXiv:2205.07460* (2022).
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等人 (2022) Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat
    和 Anima Anandkumar. 2022. 对抗性净化的扩散模型。*arXiv 预印本 arXiv:2205.07460* (2022)。
- en: Osadchy et al. (2017) Margarita Osadchy, Julio Hernandez-Castro, Stuart Gibson,
    Orr Dunkelman, and Daniel Pérez-Cabo. 2017. No Bot Expects the DeepCAPTCHA! Introducing
    Immutable Adversarial Examples, With Applications to CAPTCHA Generation. *IEEE
    Transactions on Information Forensics and Security* 12, 11 (2017), 2640–2653.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osadchy 等人 (2017) Margarita Osadchy, Julio Hernandez-Castro, Stuart Gibson,
    Orr Dunkelman 和 Daniel Pérez-Cabo. 2017. 没有机器人期待 DeepCAPTCHA！引入不可变的对抗样本，及其在 CAPTCHA
    生成中的应用。*IEEE 信息取证与安全事务* 12, 11 (2017), 2640–2653。
- en: 'Papernot et al. (2016a) Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.
    2016a. Transferability in machine learning: from phenomena to black-box attacks
    using adversarial samples. *arXiv preprint arXiv:1605.07277* (2016).'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等人 (2016a) Nicolas Papernot, Patrick McDaniel 和 Ian Goodfellow. 2016a.
    机器学习中的可迁移性：从现象到使用对抗样本的黑箱攻击。*arXiv 预印本 arXiv:1605.07277* (2016)。
- en: Papernot et al. (2017) Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh
    Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against
    machine learning. In *Proceedings of the 2017 ACM on Asia conference on computer
    and communications security*. 506–519.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等人 (2017) Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh
    Jha, Z Berkay Celik 和 Ananthram Swami. 2017. 实用的黑箱攻击针对机器学习。收录于 *2017 年 ACM 亚洲计算机与通信安全会议论文集*，506–519。
- en: Papernot et al. (2016b) Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
    Fredrikson, Z Berkay Celik, and Ananthram Swami. 2016b. The limitations of deep
    learning in adversarial settings. In *2016 IEEE European symposium on security
    and privacy (EuroS&P)*. IEEE, 372–387.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等人 (2016b) Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson,
    Z Berkay Celik 和 Ananthram Swami. 2016b. 深度学习在对抗性环境中的局限性。收录于 *2016 年 IEEE 欧洲安全与隐私研讨会
    (EuroS&P)*。IEEE，372–387。
- en: 'Papernot et al. (2018) Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and
    Michael P Wellman. 2018. Sok: Security and privacy in machine learning. In *2018
    IEEE European Symposium on Security and Privacy (EuroS&P)*. IEEE, 399–414.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papernot 等人 (2018) Nicolas Papernot, Patrick McDaniel, Arunesh Sinha 和 Michael
    P Wellman. 2018. Sok: 机器学习中的安全性和隐私。收录于 *2018 年 IEEE 欧洲安全与隐私研讨会 (EuroS&P)*。IEEE，399–414。'
- en: Papernot et al. (2016c) Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha,
    and Ananthram Swami. 2016c. Distillation as a defense to adversarial perturbations
    against deep neural networks. In *2016 IEEE symposium on security and privacy
    (SP)*. IEEE, 582–597.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等人（2016c）Nicolas Papernot、Patrick McDaniel、Xi Wu、Somesh Jha 和 Ananthram
    Swami。2016c。蒸馏作为对抗深度神经网络对抗扰动的防御。在*2016 IEEE安全与隐私研讨会（SP）*中。IEEE，582–597。
- en: Poursaeed et al. (2018) Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge
    Belongie. 2018. Generative adversarial perturbations. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 4422–4431.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poursaeed 等人（2018）Omid Poursaeed、Isay Katsman、Bicheng Gao 和 Serge Belongie。2018年。生成对抗扰动。在*IEEE计算机视觉与模式识别会议论文集*中。4422–4431。
- en: 'Qi et al. (2017) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. 2017.
    Pointnet: Deep learning on point sets for 3d classification and segmentation.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    652–660.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi 等人（2017）Charles R Qi、Hao Su、Kaichun Mo 和 Leonidas J Guibas。2017年。Pointnet:
    基于点集的3D分类和分割的深度学习。在*IEEE计算机视觉与模式识别会议论文集*中。652–660。'
- en: 'Qiu et al. (2020) Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan, Honglak
    Lee, and Bo Li. 2020. Semanticadv: Generating adversarial examples via attribute-conditioned
    image editing. In *European Conference on Computer Vision*. Springer, 19–37.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiu 等人（2020）Haonan Qiu、Chaowei Xiao、Lei Yang、Xinchen Yan、Honglak Lee 和 Bo Li。2020年。Semanticadv:
    通过属性条件的图像编辑生成对抗示例。在*欧洲计算机视觉会议*中。Springer，19–37。'
- en: 'Rahmati et al. (2020) Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard,
    and Huaiyu Dai. 2020. Geoda: a geometric framework for black-box adversarial attacks.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*.
    8446–8455.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rahmati 等人（2020）Ali Rahmati、Seyed-Mohsen Moosavi-Dezfooli、Pascal Frossard 和
    Huaiyu Dai。2020年。Geoda: 用于黑箱对抗攻击的几何框架。在*IEEE/CVF计算机视觉与模式识别会议论文集*中。8446–8455。'
- en: 'Rakin et al. (2021) Adnan Siraj Rakin, Zhezhi He, Jingtao Li, Fan Yao, Chaitali
    Chakrabarti, and Deliang Fan. 2021. T-bfa: Targeted bit-flip adversarial weight
    attack. *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2021).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rakin 等人（2021）Adnan Siraj Rakin、Zhezhi He、Jingtao Li、Fan Yao、Chaitali Chakrabarti
    和 Deliang Fan。2021年。T-bfa: 目标比特翻转对抗性权重攻击。*IEEE Transactions on Pattern Analysis
    and Machine Intelligence*（2021年）。'
- en: 'Reza et al. (2023) Md Farhamdur Reza, Ali Rahmati, Tianfu Wu, and Huaiyu Dai.
    2023. CGBA: Curvature-aware Geometric Black-box Attack. *arXiv preprint arXiv:2308.03163*
    (2023).'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reza 等人（2023）Md Farhamdur Reza、Ali Rahmati、Tianfu Wu 和 Huaiyu Dai。2023年。CGBA:
    曲率感知几何黑箱攻击。*arXiv预印本 arXiv:2308.03163*（2023年）。'
- en: Rony et al. (2019) Jérôme Rony, Luiz G Hafemann, Luiz S Oliveira, Ismail Ben
    Ayed, Robert Sabourin, and Eric Granger. 2019. Decoupling direction and norm for
    efficient gradient-based l2 adversarial attacks and defenses. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 4322–4330.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rony 等人（2019）Jérôme Rony、Luiz G Hafemann、Luiz S Oliveira、Ismail Ben Ayed、Robert
    Sabourin 和 Eric Granger。2019年。解耦方向和范数以提高基于梯度的l2对抗攻击和防御的效率。在*IEEE/CVF计算机视觉与模式识别会议论文集*中。4322–4330。
- en: 'Salem et al. (2020) Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario
    Fritz, and Yang Zhang. 2020. $\{$Updates-Leak$\}$: Data Set Inference and Reconstruction
    Attacks in Online Learning. In *29th USENIX security symposium (USENIX Security
    20)*. 1291–1308.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Salem 等人（2020）Ahmed Salem、Apratim Bhattacharya、Michael Backes、Mario Fritz 和
    Yang Zhang。2020年。$\{$Updates-Leak$\}$: 在线学习中的数据集推断和重建攻击。在*第29届USENIX安全研讨会（USENIX
    Security 20）*中。1291–1308。'
- en: 'Sato et al. (2021) Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jia, Xue
    Lin, and Qi Alfred Chen. 2021. Dirty road can attack: Security of deep learning
    based automated lane centering under $\{$Physical-World$\}$ attack. In *30th USENIX
    Security Symposium (USENIX Security 21)*. 3309–3326.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sato 等人（2021）Takami Sato、Junjie Shen、Ningfei Wang、Yunhan Jia、Xue Lin 和 Qi Alfred
    Chen。2021年。脏道路攻击：基于深度学习的自动车道居中在$\{$物理世界$\}$攻击下的安全性。在*第30届USENIX安全研讨会（USENIX Security
    21）*中。3309–3326。
- en: 'Sayles et al. (2021) Athena Sayles, Ashish Hooda, Mohit Gupta, Rahul Chatterjee,
    and Earlence Fernandes. 2021. Invisible perturbations: Physical adversarial examples
    exploiting the rolling shutter effect. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 14666–14675.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sayles 等人（2021）Athena Sayles、Ashish Hooda、Mohit Gupta、Rahul Chatterjee 和 Earlence
    Fernandes。2021年。隐形扰动：利用滚动快门效应的物理对抗示例。在*IEEE/CVF计算机视觉与模式识别会议论文集*中。14666–14675。
- en: 'Schmalfuss et al. (2023) Jenny Schmalfuss, Lukas Mehl, and Andrés Bruhn. 2023.
    Distracting Downpour: Adversarial Weather Attacks for Motion Estimation. *arXiv
    preprint arXiv:2305.06716* (2023).'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schmalfuss 等（2023）Jenny Schmalfuss、Lukas Mehl 和 Andrés Bruhn. 2023. Distracting
    Downpour: 运动估计的对抗性天气攻击. *arXiv预印本 arXiv:2305.06716* (2023)。'
- en: 'Serban et al. (2020) Alex Serban, Erik Poll, and Joost Visser. 2020. Adversarial
    examples on object recognition: A comprehensive survey. *ACM Computing Surveys
    (CSUR)* 53, 3 (2020), 1–38.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serban 等（2020）Alex Serban、Erik Poll 和 Joost Visser. 2020. 对象识别中的对抗样本：全面调查. *ACM计算调查（CSUR）*
    53, 3 (2020)，1–38。
- en: 'Shamsabadi et al. (2020a) Ali Shahin Shamsabadi, Changjae Oh, and Andrea Cavallaro.
    2020a. EdgeFool: an adversarial image enhancement filter. In *ICASSP 2020-2020
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.
    IEEE, 1898–1902.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shamsabadi 等（2020a）Ali Shahin Shamsabadi、Changjae Oh 和 Andrea Cavallaro. 2020a.
    EdgeFool: 一种对抗性图像增强滤镜. 见 *ICASSP 2020-2020 IEEE国际声学、语音与信号处理会议论文集*。IEEE，1898–1902。'
- en: Shamsabadi et al. (2021) Ali Shahin Shamsabadi, Changjae Oh, and Andrea Cavallaro.
    2021. Semantically adversarial learnable filters. *IEEE Transactions on Image
    Processing* 30 (2021), 8075–8087.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shamsabadi 等（2021）Ali Shahin Shamsabadi、Changjae Oh 和 Andrea Cavallaro. 2021.
    语义对抗可学习滤镜. *IEEE图像处理汇刊* 30 (2021)，8075–8087。
- en: 'Shamsabadi et al. (2020b) Ali Shahin Shamsabadi, Ricardo Sanchez-Matilla, and
    Andrea Cavallaro. 2020b. Colorfool: Semantic adversarial colorization. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 1151–1160.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shamsabadi 等（2020b）Ali Shahin Shamsabadi、Ricardo Sanchez-Matilla 和 Andrea Cavallaro.
    2020b. Colorfool: 语义对抗色彩化. 见 *IEEE/CVF计算机视觉与模式识别会议论文集*，1151–1160。'
- en: 'Sharif et al. (2016) Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K
    Reiter. 2016. Accessorize to a crime: Real and stealthy attacks on state-of-the-art
    face recognition. In *Proceedings of the 2016 acm sigsac conference on computer
    and communications security*. 1528–1540.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharif 等（2016）Mahmood Sharif、Sruti Bhagavatula、Lujo Bauer 和 Michael K Reiter.
    2016. 犯罪配件：对最先进人脸识别技术的真实与隐秘攻击. 见 *2016 ACM SIGSAC计算机与通信安全会议论文集*，1528–1540。
- en: Sharif et al. (2019) Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K
    Reiter. 2019. A general framework for adversarial examples with objectives. *ACM
    Transactions on Privacy and Security (TOPS)* 22, 3 (2019), 1–30.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharif 等（2019）Mahmood Sharif、Sruti Bhagavatula、Lujo Bauer 和 Michael K Reiter.
    2019. 具有目标的对抗样本通用框架. *ACM隐私与安全事务汇刊（TOPS）* 22, 3 (2019)，1–30。
- en: Shen et al. (2021) Meng Shen, Hao Yu, Liehuang Zhu, Ke Xu, Qi Li, and Jiankun
    Hu. 2021. Effective and robust physical-world attacks on deep learning face recognition
    systems. *IEEE Transactions on Information Forensics and Security* 16 (2021),
    4063–4077.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等（2021）Meng Shen、Hao Yu、Liehuang Zhu、Ke Xu、Qi Li 和 Jiankun Hu. 2021. 对深度学习人脸识别系统的有效且鲁棒的物理世界攻击.
    *IEEE信息取证与安全汇刊* 16 (2021)，4063–4077。
- en: Shokri et al. (2017) Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
    Shmatikov. 2017. Membership inference attacks against machine learning models.
    In *2017 IEEE symposium on security and privacy (SP)*. IEEE, 3–18.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shokri 等（2017）Reza Shokri、Marco Stronati、Congzheng Song 和 Vitaly Shmatikov.
    2017. 针对机器学习模型的成员推断攻击. 见 *2017 IEEE安全与隐私研讨会（SP）*。IEEE，3–18。
- en: Shukla et al. (2021) Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott,
    and Zico Kolter. 2021. Simple and efficient hard label black-box adversarial attacks
    in low query budget regimes. In *Proceedings of the 27th ACM SIGKDD Conference
    on Knowledge Discovery & Data Mining*. 1461–1469.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shukla 等（2021）Satya Narayan Shukla、Anit Kumar Sahu、Devin Willmott 和 Zico Kolter.
    2021. 简单高效的硬标签黑箱对抗攻击在低查询预算下. 见 *第27届ACM SIGKDD知识发现与数据挖掘会议论文集*，1461–1469。
- en: Song et al. (2018) Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. 2018.
    Constructing unrestricted adversarial examples with generative models. *Advances
    in Neural Information Processing Systems* 31 (2018).
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2018）Yang Song、Rui Shu、Nate Kushman 和 Stefano Ermon. 2018. 使用生成模型构建无限制对抗样本.
    *神经信息处理系统进展* 31 (2018)。
- en: Su et al. (2019) Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai.
    2019. One pixel attack for fooling deep neural networks. *IEEE Transactions on
    Evolutionary Computation* 23, 5 (2019), 828–841.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等（2019）Jiawei Su、Danilo Vasconcellos Vargas 和 Kouichi Sakurai. 2019. 一像素攻击以欺骗深度神经网络.
    *IEEE进化计算汇刊* 23, 5 (2019)，828–841。
- en: 'Sun et al. (2020) Jiachen Sun, Yulong Cao, Qi Alfred Chen, and Z Morley Mao.
    2020. Towards robust $\{$LiDAR-based$\}$ perception in autonomous driving: General
    black-box adversarial sensor attack and countermeasures. In *29th USENIX Security
    Symposium (USENIX Security 20)*. 877–894.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等（2020）Jiachen Sun, Yulong Cao, Qi Alfred Chen, 和Z Morley Mao. 2020. 面向自动驾驶的鲁棒$\{$LiDAR-based$\}$感知：通用黑箱对抗传感器攻击及对策。见于*第29届USENIX安全研讨会（USENIX
    Security 20）*。877–894。
- en: 'Suryanto et al. (2022) Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma
    Larasati, Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, and Howon Kim.
    2022. DTA: Physical Camouflage Attacks using Differentiable Transformation Network.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    15305–15314.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Suryanto等（2022）Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma Larasati,
    Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, 和Howon Kim. 2022. DTA:
    使用可微分变换网络的物理伪装攻击。见于*IEEE/CVF计算机视觉与模式识别会议论文集*。15305–15314。'
- en: Szegedy et al. (2013) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties
    of neural networks. *arXiv preprint arXiv:1312.6199* (2013).
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy等（2013）Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
    Dumitru Erhan, Ian Goodfellow, 和Rob Fergus. 2013. 神经网络的有趣特性。*arXiv预印本 arXiv:1312.6199*（2013）。
- en: 'Tao et al. (2023) Yunbo Tao, Daizong Liu, Pan Zhou, Yulai Xie, Wei Du, and
    Wei Hu. 2023. 3DHacker: Spectrum-based Decision Boundary Generation for Hard-label
    3D Point Cloud Attack. *arXiv preprint arXiv:2308.07546* (2023).'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tao等（2023）Yunbo Tao, Daizong Liu, Pan Zhou, Yulai Xie, Wei Du, 和Wei Hu. 2023.
    3DHacker: 基于频谱的决策边界生成用于硬标签3D点云攻击。*arXiv预印本 arXiv:2308.07546*（2023）。'
- en: 'Tramèr et al. (2017a) Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian
    Goodfellow, Dan Boneh, and Patrick McDaniel. 2017a. Ensemble adversarial training:
    Attacks and defenses. *arXiv preprint arXiv:1705.07204* (2017).'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tramèr等（2017a）Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow,
    Dan Boneh, 和Patrick McDaniel. 2017a. 集成对抗训练：攻击与防御。*arXiv预印本 arXiv:1705.07204*（2017）。
- en: Tramèr et al. (2017b) Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan
    Boneh, and Patrick McDaniel. 2017b. The space of transferable adversarial examples.
    *arXiv preprint arXiv:1704.03453* (2017).
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tramèr等（2017b）Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, 和Patrick
    McDaniel. 2017b. 可转移对抗样本的空间。*arXiv预印本 arXiv:1704.03453*（2017）。
- en: Tsai et al. (2020) Tzungyu Tsai, Kaichen Yang, Tsung-Yi Ho, and Yier Jin. 2020.
    Robust adversarial objects against deep learning models. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, Vol. 34. 954–962.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai等（2020）Tzungyu Tsai, Kaichen Yang, Tsung-Yi Ho, 和Yier Jin. 2020. 针对深度学习模型的鲁棒对抗物体。见于*AAAI人工智能会议论文集*，第34卷。954–962。
- en: 'Tu et al. (2019) Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang,
    Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. 2019. Autozoom: Autoencoder-based
    zeroth order optimization method for attacking black-box neural networks. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 33\. 742–749.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tu等（2019）Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng
    Yi, Cho-Jui Hsieh, 和Shin-Ming Cheng. 2019. Autozoom: 基于自编码器的零阶优化方法用于攻击黑箱神经网络。见于*AAAI人工智能会议论文集*，第33卷。742–749。'
- en: Tu et al. (2020) James Tu, Mengye Ren, Sivabalan Manivasagam, Ming Liang, Bin
    Yang, Richard Du, Frank Cheng, and Raquel Urtasun. 2020. Physically realizable
    adversarial examples for lidar object detection. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 13716–13725.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu等（2020）James Tu, Mengye Ren, Sivabalan Manivasagam, Ming Liang, Bin Yang,
    Richard Du, Frank Cheng, 和Raquel Urtasun. 2020. 物理上可实现的激光雷达物体检测对抗样本。见于*IEEE/CVF计算机视觉与模式识别会议论文集*。13716–13725。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems* 30 (2017).
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等（2017）Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Łukasz Kaiser, 和Illia Polosukhin. 2017. 注意力机制就是你所需要的一切。*神经信息处理系统进展*
    第30卷（2017）。
- en: 'Wang et al. (2022a) Donghua Wang, Tingsong Jiang, Jialiang Sun, Weien Zhou,
    Zhiqiang Gong, Xiaoya Zhang, Wen Yao, and Xiaoqian Chen. 2022a. Fca: Learning
    a 3d full-coverage vehicle camouflage for multi-view physical adversarial attack.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36\.
    2414–2422.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等（2022a）Donghua Wang, Tingsong Jiang, Jialiang Sun, Weien Zhou, Zhiqiang
    Gong, Xiaoya Zhang, Wen Yao, 和Xiaoqian Chen. 2022a. FCA: 学习3D全覆盖车辆伪装用于多视角物理对抗攻击。见于*AAAI人工智能会议论文集*，第36卷。2414–2422。'
- en: 'Wang et al. (2021b) Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu
    Tang, and Xianglong Liu. 2021b. Dual attention suppression attack: Generate adversarial
    camouflage in physical world. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 8565–8574.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021b) Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu
    Tang, and Xianglong Liu. 2021b. 双重注意力抑制攻击：在物理世界中生成对抗伪装。发表于*IEEE/CVF 计算机视觉与模式识别大会*。8565–8574。
- en: Wang and He (2021) Xiaosen Wang and Kun He. 2021. Enhancing the transferability
    of adversarial attacks through variance tuning. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 1924–1933.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and He (2021) Xiaosen Wang 和 Kun He. 2021. 通过方差调整提升对抗攻击的转移性。发表于*IEEE/CVF
    计算机视觉与模式识别大会*。1924–1933。
- en: 'Wang et al. (2022b) Xiaosen Wang, Zeliang Zhang, Kangheng Tong, Dihong Gong,
    Kun He, Zhifeng Li, and Wei Liu. 2022b. Triangle attack: A query-efficient decision-based
    adversarial attack. In *European Conference on Computer Vision*. Springer, 156–174.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022b) Xiaosen Wang, Zeliang Zhang, Kangheng Tong, Dihong Gong,
    Kun He, Zhifeng Li, and Wei Liu. 2022b. 三角攻击：一种查询高效的基于决策的对抗攻击。发表于*欧洲计算机视觉大会*。Springer，156–174。
- en: Wang et al. (2019) Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M
    Bronstein, and Justin M Solomon. 2019. Dynamic graph cnn for learning on point
    clouds. *Acm Transactions On Graphics (tog)* 38, 5 (2019), 1–12.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael
    M Bronstein, and Justin M Solomon. 2019. 用于点云学习的动态图 CNN。*ACM 图形学汇刊 (tog)* 38,
    5 (2019)，1–12。
- en: 'Wang et al. (2021c) Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yu-an
    Tan, and Quanxin Zhang. 2021c. Demiguise attack: Crafting invisible semantic adversarial
    perturbations with perceptual similarity. *arXiv preprint arXiv:2107.01396* (2021).'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021c) Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yu-an
    Tan, and Quanxin Zhang. 2021c. Demiguise 攻击：创造具有感知相似性的隐形语义对抗扰动。*arXiv 预印本 arXiv:2107.01396*
    (2021)。
- en: Wang et al. (2021a) Zhibo Wang, Hengchang Guo, Zhifei Zhang, Wenxin Liu, Zhan
    Qin, and Kui Ren. 2021a. Feature importance-aware transferable adversarial attacks.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*.
    7639–7648.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) Zhibo Wang, Hengchang Guo, Zhifei Zhang, Wenxin Liu, Zhan
    Qin, and Kui Ren. 2021a. 特征重要性感知的可转移对抗攻击。发表于*IEEE/CVF 国际计算机视觉大会*。7639–7648。
- en: Wei et al. (2022) Zhipeng Wei, Jingjing Chen, Zuxuan Wu, and Yu-Gang Jiang.
    2022. Cross-Modal Transferable Adversarial Attacks from Images to Videos. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 15064–15073.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Zhipeng Wei, Jingjing Chen, Zuxuan Wu, and Yu-Gang Jiang.
    2022. 从图像到视频的跨模态可转移对抗攻击。发表于*IEEE/CVF 计算机视觉与模式识别大会*。15064–15073。
- en: Wen et al. (2020) Yuxin Wen, Jiehong Lin, Ke Chen, CL Philip Chen, and Kui Jia.
    2020. Geometry-aware generation of adversarial point clouds. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence* (2020).
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. (2020) Yuxin Wen, Jiehong Lin, Ke Chen, CL Philip Chen, 和 Kui Jia.
    2020. 几何感知的对抗点云生成。*IEEE 模式分析与机器智能汇刊* (2020)。
- en: Wicker and Kwiatkowska (2019) Matthew Wicker and Marta Kwiatkowska. 2019. Robustness
    of 3d deep learning in an adversarial setting. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 11767–11775.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wicker and Kwiatkowska (2019) Matthew Wicker 和 Marta Kwiatkowska. 2019. 对抗设置中
    3D 深度学习的鲁棒性。发表于*IEEE/CVF 计算机视觉与模式识别大会*。11767–11775。
- en: Wong et al. (2019) Eric Wong, Frank Schmidt, and Zico Kolter. 2019. Wasserstein
    adversarial examples via projected sinkhorn iterations. In *International Conference
    on Machine Learning*. PMLR, 6808–6817.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong et al. (2019) Eric Wong, Frank Schmidt, and Zico Kolter. 2019. 通过投影 Sinkhorn
    迭代生成 Wasserstein 对抗样本。发表于*国际机器学习大会*。PMLR，6808–6817。
- en: Wu et al. (2018) Lei Wu, Zhanxing Zhu, Cheng Tai, et al. 2018. Understanding
    and enhancing the transferability of adversarial examples. *arXiv preprint arXiv:1802.09707*
    (2018).
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2018) Lei Wu, Zhanxing Zhu, Cheng Tai 等。2018. 理解和提升对抗样本的转移性。*arXiv
    预印本 arXiv:1802.09707* (2018)。
- en: Wu et al. (2021) Weibin Wu, Yuxin Su, Michael R Lyu, and Irwin King. 2021. Improving
    the transferability of adversarial samples with adversarial transformations. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    9024–9033.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021) Weibin Wu, Yuxin Su, Michael R Lyu, and Irwin King. 2021. 通过对抗变换提高对抗样本的转移性。发表于*IEEE/CVF
    计算机视觉与模式识别大会*。9024–9033。
- en: Xiao et al. (2018a) Chaowei Xiao, Bo Li, Jun Yan Zhu, Warren He, Mingyan Liu,
    and Dawn Song. 2018a. Generating adversarial examples with adversarial networks.
    In *27th International Joint Conference on Artificial Intelligence, IJCAI 2018*.
    International Joint Conferences on Artificial Intelligence, 3905–3911.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等（2018a）Chaowei Xiao、Bo Li、Jun Yan Zhu、Warren He、Mingyan Liu 和 Dawn Song。2018a。使用对抗网络生成对抗样本。发表于*第27届国际人工智能联合会议，IJCAI
    2018*。国际人工智能联合会议，3905–3911。
- en: 'Xiao et al. (2019) Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu.
    2019. Meshadv: Adversarial meshes for visual recognition. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 6898–6907.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao等（2019）Chaowei Xiao、Dawei Yang、Bo Li、Jia Deng 和 Mingyan Liu。2019。Meshadv:
    用于视觉识别的对抗网格。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。6898–6907。'
- en: Xiao et al. (2018b) Chaowei Xiao, Jun Yan Zhu, Bo Li, Warren He, Mingyan Liu,
    and Dawn Song. 2018b. Spatially transformed adversarial examples. In *6th International
    Conference on Learning Representations, ICLR 2018*.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等（2018b）Chaowei Xiao、Jun Yan Zhu、Bo Li、Warren He、Mingyan Liu 和 Dawn Song。2018b。空间变换的对抗样本。发表于*第六届国际学习表征会议，ICLR
    2018*。
- en: Xiao et al. (2021) Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei Gao,
    Xiaolu Zhang, Jun Zhou, and Jun Zhu. 2021. Improving transferability of adversarial
    patches on face recognition with generative models. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 11845–11854.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等（2021）Zihao Xiao、Xianfeng Gao、Chilin Fu、Yinpeng Dong、Wei Gao、Xiaolu Zhang、Jun
    Zhou 和 Jun Zhu。2021。通过生成模型提高面部识别中对抗补丁的可迁移性。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。11845–11854。
- en: Xie et al. (2017) Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi
    Xie, and Alan Yuille. 2017. Adversarial examples for semantic segmentation and
    object detection. In *Proceedings of the IEEE international conference on computer
    vision*. 1369–1378.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等（2017）Cihang Xie、Jianyu Wang、Zhishuai Zhang、Yuyin Zhou、Lingxi Xie 和 Alan
    Yuille。2017。用于语义分割和目标检测的对抗样本。发表于*IEEE国际计算机视觉会议论文集*。1369–1378。
- en: Xie et al. (2019) Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang,
    Zhou Ren, and Alan L Yuille. 2019. Improving transferability of adversarial examples
    with input diversity. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 2730–2739.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等（2019）Cihang Xie、Zhishuai Zhang、Yuyin Zhou、Song Bai、Jianyu Wang、Zhou Ren
    和 Alan L Yuille。2019。通过输入多样性提高对抗样本的可迁移性。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。2730–2739。
- en: Yang et al. (2020) Jiancheng Yang, Yangzhou Jiang, Xiaoyang Huang, Bingbing
    Ni, and Chenglong Zhao. 2020. Learning black-box attackers with transferable priors
    and query feedback. *Advances in Neural Information Processing Systems* 33 (2020),
    12288–12299.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等（2020）Jiancheng Yang、Yangzhou Jiang、Xiaoyang Huang、Bingbing Ni 和 Chenglong
    Zhao。2020。使用可迁移先验和查询反馈学习黑箱攻击者。*神经信息处理系统进展* 33（2020），12288–12299。
- en: 'Yin et al. (2021) Bangjie Yin, Wenxuan Wang, Taiping Yao, Junfeng Guo, Zelun
    Kong, Shouhong Ding, Jilin Li, and Cong Liu. 2021. Adv-Makeup: A New Imperceptible
    and Transferable Attack on Face Recognition. *International Joint Conferences
    on Artificial Intelligence (IJCAI)* (2021).'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yin等（2021）Bangjie Yin、Wenxuan Wang、Taiping Yao、Junfeng Guo、Zelun Kong、Shouhong
    Ding、Jilin Li 和 Cong Liu。2021。Adv-Makeup: 一种新的不可感知且可迁移的面部识别攻击。*国际人工智能联合会议（IJCAI）*（2021）。'
- en: Yu et al. (2022) Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu.
    2022. Availability attacks create shortcuts. In *Proceedings of the 28th ACM SIGKDD
    Conference on Knowledge Discovery and Data Mining*. 2367–2376.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等（2022）Da Yu、Huishuai Zhang、Wei Chen、Jian Yin 和 Tie-Yan Liu。2022。可用性攻击创造捷径。发表于*第28届ACM
    SIGKDD知识发现与数据挖掘会议论文集*。2367–2376。
- en: Zeng et al. (2019) Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi
    Xie, Yu-Wing Tai, Chi-Keung Tang, and Alan L Yuille. 2019. Adversarial attacks
    beyond the image space. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 4302–4311.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng等（2019）Xiaohui Zeng、Chenxi Liu、Yu-Siang Wang、Weichao Qiu、Lingxi Xie、Yu-Wing
    Tai、Chi-Keung Tang 和 Alan L Yuille。2019。超越图像空间的对抗攻击。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。4302–4311。
- en: Zhang et al. (2020b) Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In So Kweon.
    2020b. Understanding adversarial examples from the mutual influence of images
    and perturbations. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 14521–14530.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2020b）Chaoning Zhang、Philipp Benz、Tooba Imtiaz 和 In So Kweon。2020b。通过图像和扰动的相互影响理解对抗样本。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*。14521–14530。
- en: 'Zhang et al. (2021) Chaoning Zhang, Philipp Benz, Adil Karjauv, and In So Kweon.
    2021. Universal adversarial perturbations through the lens of deep steganography:
    Towards a fourier perspective. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 35\. 3296–3304.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021) Chaoning Zhang、Philipp Benz、Adil Karjauv 和 In So Kweon。2021。通过深度隐写学的视角看通用对抗扰动：朝着傅里叶视角迈进。在
    *AAAI 人工智能会议论文集*，第 35 卷，3296–3304 页。
- en: Zhang et al. (2020a) Hanwei Zhang, Yannis Avrithis, Teddy Furon, and Laurent
    Amsaleg. 2020a. Smooth adversarial examples. *EURASIP Journal on Information Security*
    2020, 1 (2020), 1–12.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020a) Hanwei Zhang、Yannis Avrithis、Teddy Furon 和 Laurent Amsaleg。2020a。平滑对抗样本。*EURASIP
    信息安全期刊* 2020，第 1 卷（2020），1–12 页。
- en: Zhang et al. (2018a) Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S
    Dhillon, and Cho-Jui Hsieh. 2018a. The Limitations of Adversarial Training and
    the Blind-Spot Attack. In *International Conference on Learning Representations*.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018a) Huan Zhang、Hongge Chen、赵宋、Duane Boning、Inderjit S Dhillon
    和 Cho-Jui Hsieh。2018a。对抗训练的局限性与盲点攻击。在 *国际学习表征会议*。
- en: Zhang et al. (2022b) Jianping Zhang, Weibin Wu, Jen-tse Huang, Yizhan Huang,
    Wenxuan Wang, Yuxin Su, and Michael R Lyu. 2022b. Improving Adversarial Transferability
    via Neuron Attribution-Based Attacks. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 14993–15002.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022b) Jianping Zhang、Weibin Wu、Jen-tse Huang、Yizhan Huang、Wenxuan
    Wang、Yuxin Su 和 Michael R Lyu。2022b。通过神经元归属攻击提升对抗转移性。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*。14993–15002
    页。
- en: Zhang et al. (2022a) Qingzhao Zhang, Shengtuo Hu, Jiachen Sun, Qi Alfred Chen,
    and Z Morley Mao. 2022a. On adversarial robustness of trajectory prediction for
    autonomous vehicles. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 15159–15168.
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022a) Qingzhao Zhang、Shengtuo Hu、Jiachen Sun、Qi Alfred Chen 和
    Z Morley Mao。2022a。关于自动驾驶车辆轨迹预测的对抗鲁棒性。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*。15159–15168
    页。
- en: Zhang et al. (2016) Richard Zhang, Phillip Isola, and Alexei A Efros. 2016.
    Colorful image colorization. In *European conference on computer vision*. Springer,
    649–666.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2016) Richard Zhang、Phillip Isola 和 Alexei A Efros。2016。色彩丰富的图像着色。在
    *欧洲计算机视觉会议*。Springer，649–666 页。
- en: 'Zhang et al. (2018b) Yang Zhang, Hassan Foroosh, Philip David, and Boqing Gong.
    2018b. CAMOU: Learning physical vehicle camouflages to adversarially attack detectors
    in the wild. In *International Conference on Learning Representations*.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018b) Yang Zhang、Hassan Foroosh、Philip David 和 Boqing Gong。2018b。CAMOU：学习物理车辆伪装以对抗野外探测器。在
    *国际学习表征会议*。
- en: Zhao et al. (2021a) Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and
    Vladlen Koltun. 2021a. Point transformer. In *Proceedings of the IEEE/CVF international
    conference on computer vision*. 16259–16268.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2021a) Hengshuang Zhao、Li Jiang、Jiaya Jia、Philip HS Torr 和 Vladlen
    Koltun。2021a。点变换器。在 *IEEE/CVF 国际计算机视觉会议论文集*。16259–16268 页。
- en: 'Zhao et al. (2019) He Zhao, Trung Le, Paul Montague, Olivier De Vel, Tamas
    Abraham, and Dinh Phung. 2019. Perturbations are not enough: Generating adversarial
    examples with spatial distortions. *arXiv preprint arXiv:1910.01329* (2019).'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2019) 何赵、Trung Le、Paul Montague、Olivier De Vel、Tamas Abraham 和
    Dinh Phung。2019。**扰动还不够**：通过空间失真生成对抗样本。*arXiv 预印本 arXiv:1910.01329*（2019）。
- en: Zhao et al. (2020b) Yue Zhao, Yuwei Wu, Caihua Chen, and Andrew Lim. 2020b.
    On isometry robustness of deep 3d point cloud models under adversarial attacks.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    1201–1210.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2020b) 岳赵、余伟 吴、蔡华 陈 和 Andrew Lim。2020b。深度 3D 点云模型在对抗攻击下的等距鲁棒性。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*。1201–1210 页。
- en: Zhao et al. (2018) Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating
    Natural Adversarial Examples. In *International Conference on Learning Representations*.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2018) 郑丽 赵、Dheeru Dua 和 Sameer Singh。2018。生成自然对抗样本。在 *国际学习表征会议*。
- en: Zhao et al. (2020a) Zhengyu Zhao, Zhuoran Liu, and Martha Larson. 2020a. Towards
    large yet imperceptible adversarial image perturbations with perceptual color
    distance. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*. 1039–1048.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2020a) 郑宇 赵、朱然 刘 和 Martha Larson。2020a。朝着大型但不可察觉的对抗图像扰动与感知颜色距离。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*。1039–1048 页。
- en: 'Zhao et al. (2021b) Zhengyu Zhao, Zhuoran Liu, and Martha Larson. 2021b. On
    success and simplicity: A second look at transferable targeted attacks. *Advances
    in Neural Information Processing Systems* 34 (2021), 6115–6128.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2021b) 郑宇 Zhao、刘卓然 Zhuoran 和 玛莎·拉尔森 Martha Larson。2021b。《关于成功与简易性：对可转移目标攻击的再审视》。*神经信息处理系统进展*
    34 (2021)，6115–6128。
- en: Zheng et al. (2019a) Tianhang Zheng, Changyou Chen, and Kui Ren. 2019a. Distributionally
    adversarial attack. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 33\. 2253–2260.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2019a) 郑天航 Tianhang、陈常友 Changyou 和 任奎 Kui。2019a。《分布对抗攻击》。在 *AAAI人工智能会议论文集*，第33卷。2253–2260。
- en: Zheng et al. (2019b) Tianhang Zheng, Changyou Chen, Junsong Yuan, Bo Li, and
    Kui Ren. 2019b. Pointcloud saliency maps. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 1598–1606.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2019b) 郑天航 Tianhang、陈常友 Changyou、袁俊松 Junsong、李博 Bo 和 任奎 Kui。2019b。《点云显著性图》。在
    *IEEE/CVF国际计算机视觉会议论文集*。1598–1606。
- en: 'Zhou et al. (2020) Hang Zhou, Dongdong Chen, Jing Liao, Kejiang Chen, Xiaoyi
    Dong, Kunlin Liu, Weiming Zhang, Gang Hua, and Nenghai Yu. 2020. Lg-gan: Label
    guided adversarial network for flexible targeted attack of point cloud based deep
    networks. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*. 10356–10365.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2020) 侯航 Hang、陈东东 Dongdong、廖晶 Jing、陈克江 Kejiang、董小义 Xiaoyi、刘昆林 Kunlin、张伟明
    Weiming、华岗 Gang 和 余能海 Nenghai。2020。《Lg-gan：用于灵活目标攻击的标签引导对抗网络》。在 *IEEE/CVF计算机视觉与模式识别会议论文集*。10356–10365。
- en: Zhou et al. (2018) Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang,
    Xiang Gan, and Yong Yang. 2018. Transferable adversarial perturbations. In *Proceedings
    of the European Conference on Computer Vision (ECCV)*. 452–467.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2018) 周闻 Wen、侯欣 Xin、陈永俊 Yongjun、唐梦云 Mengyun、黄祥琦 Xiangqi、甘翔 Xiang
    和 杨勇 Yong。2018。《可转移对抗扰动》。在 *欧洲计算机视觉会议（ECCV）论文集*。452–467。
- en: Zhuang et al. (2023) Haomin Zhuang, Yihua Zhang, and Sijia Liu. 2023. A pilot
    study of query-free adversarial attack against stable diffusion. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2384–2391.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2023) 庄浩敏 Haomin、张艺华 Yihua 和 刘思佳 Sijia。2023。《无查询对抗攻击稳定扩散的初步研究》。在
    *IEEE/CVF计算机视觉与模式识别会议论文集*。2384–2391。
- en: Zou et al. (2020) Junhua Zou, Zhisong Pan, Junyang Qiu, Xin Liu, Ting Rui, and
    Wei Li. 2020. Improving the transferability of adversarial examples with resized-diverse-inputs,
    diversity-ensemble and region fitting. In *European Conference on Computer Vision*.
    Springer, 563–579.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. (2020) 邹军华 Junhua、潘志松 Zhisong、邱俊阳 Junyang、刘欣 Xin、睿婷 Ting 和 李伟 Wei。2020。《通过调整尺寸多样化输入、多样性集成和区域拟合提高对抗样本的可转移性》。在
    *欧洲计算机视觉会议*。Springer，563–579。
