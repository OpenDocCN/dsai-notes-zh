- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:49'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2310.00633] A Survey of Robustness and Safety of 2D and 3D Deep Learning Models
    Against Adversarial Attacks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.00633](https://ar5iv.labs.arxiv.org/html/2310.00633)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yanjie Li [yanjie.li@connect.polyu.hk](mailto:yanjie.li@connect.polyu.hk) [0000-0001-8859-8331](https://orcid.org/0000-0001-8859-8331
    "ORCID identifier") The Hong Kong Polytechnic University11, Yuk Choi Road, Hung
    Hom, KLNHong Kong ,  Bin Xie [xiebin.sc@gmail.com](mailto:xiebin.sc@gmail.com)
    [0000-0001-5118-3570](https://orcid.org/0000-0001-5118-3570 "ORCID identifier")
    The Hong Kong Polytechnic University11, Yuk Choi Road, Hung Hom, KLNHong Kong
    ,  Songtao Guo [guosongtao@cqu.edu.cn](mailto:guosongtao@cqu.edu.cn) Chongqing
    UniversityChongqingChina ,  Yuanyuan Yang [yuanyuan.yang@stonybrook.edu](mailto:yuanyuan.yang@stonybrook.edu)
    [0000-0001-7296-9222](https://orcid.org/0000-0001-7296-9222 "ORCID identifier")
    Stony Brook UniversityStony Brook, NYUSA  and  Bin Xiao [csbxiao@comp.polyu.edu.hk](mailto:csbxiao@comp.polyu.edu.hk)
    [0000-0003-4223-8220](https://orcid.org/0000-0003-4223-8220 "ORCID identifier")
    The Hong Kong Polytechnic University11, Yuk Choi Road, Hung Hom, KLNHong Kong
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Benefiting from the rapid development of deep learning, 2D and 3D computer vision
    applications are deployed in many safe-critical systems, such as autopilot and
    identity authentication. However, deep learning models are not trustworthy enough
    because of their limited robustness against adversarial attacks. The physically
    realizable adversarial attacks further pose fatal threats to the application and
    human safety. Lots of papers have emerged to investigate the robustness and safety
    of deep learning models against adversarial attacks. To lead to trustworthy AI,
    we first construct a general threat model from different perspectives and then
    comprehensively review the latest progress of both 2D and 3D adversarial attacks.
    We extend the concept of adversarial examples beyond imperceptive perturbations
    and collate over 170 papers to give an overview of deep learning model robustness
    against various adversarial attacks. To the best of our knowledge, we are the
    first to systematically investigate adversarial attacks for 3D models, a flourishing
    field applied to many real-world applications. In addition, we examine physical
    adversarial attacks that lead to safety violations. Last but not least, we summarize
    present popular topics, give insights on challenges, and shed light on future
    research on trustworthy AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning; 3D computer vision; Adversarial attack; Robustness;^†^†submissionid:
    CSUR-2022-0640^†^†ccs: Security and privacy^†^†ccs: Computing methodologies Machine
    learning^†^†ccs: Computing methodologies Computer vision'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The significant strides of deep learning (DL) algorithms have driven considerable
    technological progress in computer vision (CV) tasks, which are widely deployed
    in various safety-critical and mission-critical systems like identity authentication
    and self-driving vehicles. These applications depend on the assumption that these
    deep learning models are trustworthy and robust against small perturbations, which
    means these models can produce consistent predictions when noise exists. However,
    studies have shown that deep-learning models are vulnerable to adversarial examples
    (AEs), which makes the deep-learning models produce false predictions by crafting
    elaborate imperceptive or semantic-preserving perturbations. To realize trustworthy
    AI, continuous efforts have been spent on improving the model’s robustness and
    safety against adversarial attacks and finding these models’ robustness upper
    bound by constructing stronger adversarial attacks. This is a relentless race
    about adversarial attacks and defenses. This paper thoroughly surveys the latest
    progress of this adversarial attack and defense competition from the adversary’s
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: This paper primarily discusses the phenomenon of adversarial attacks in the
    realm of computer vision tasks. While most computer vision tasks tend to focus
    on image processing, recent attention has been directed toward 3D tasks. 3D data
    can supplement 2D data by providing depth information about an image, thereby
    allowing for a more reliable and detailed analysis of targets, and can be utilized
    in various applications. For instance, autonomous vehicles often rely on a combination
    of cameras and lidar to perceive their surroundings. However, due to the unordered
    nature of 3D data, direct application of 2D adversarial attacks is not feasible.
    3D adversarial attacks have been proposed, building upon the principles of 2D
    adversarial attacks but with specific designs tailored to the characteristics
    of 3D data. For example, 3DAdv (Xiao et al., [2018a](#bib.bib150)) is based on
    the C&W attack but with novel distance metrics. 3Dhacker (Tao et al., [2023](#bib.bib130))
    is based on the boundary attack but fuses the point cloud in the spectral domain
    rather than via coordinate-wise average operation. Due to the close relationship
    between 2D and 3D computer vision tasks and the shared theoretical foundation
    of 2D and 3D attacks, this survey summarizes the latest progress of adversarial
    attacks in both 2D and 3D computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The aim of this survey is to systematize the latest progress of 2D and 3D adversarial
    attacks to help researchers construct stronger AEs to evaluate model robustness,
    design robust models, and ensure safety in real-world applications. To select
    literature for systematic review, we first clarify the review scope and thoroughly
    search publications in top computer vision or security conferences and journals
    within the past several years. Then, we select high-cited or representative work
    related to 2D and 3D adversarial attacks. These articles are categorized and compared
    based on the targets, scenarios, and methods for comprehensiveness. The main contributions
    of this work are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize the latest studies on the adversarial robustness and safety of
    2D and 3D deep learning models against adversarial attacks. Over 170 papers in
    recent years have been collated and compared. Moreover, We divide them into 2D
    and 3D attacks according to the data characteristics and application scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For 2D adversarial attacks, we extend the meaning of adversarial attack from
    imperceptible perturbation to semantic-preserving perturbation, such as color
    space distortion and spatial transformation distortion. We classify these attacks
    according to methodologies and compare their pros and cons to give a full-scope
    view.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D data is increasingly leveraged in safety-critical fields like self-driving.
    However, the 3D model robustness is hardly reviewed. To help design robust 3D
    deep learning models, we are the first to organize 3D adversarial attacks systematically
    and classify them according to their algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When deep learning models are deployed in the real world, safety is the primary
    premise. We comprehensively examine related works of adversarial attacks for safe-critical
    missions, especially for camera-based and Lidar-based self-driving cars and face
    recognition.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of this review, we summarize the present hot research topics into
    several points and identify their challenges, such as improving the attack transferability,
    generating semantic perturbation, and evaluating the robustness of 3D deep learning
    models. We also provide some useful advice for future research directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The structure of this survey is shown in Figure.[1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks"). In Sec.[2](#S2 "2\. Background ‣ A Survey of Robustness
    and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks"), we
    clarify the basic concepts of deep learning and computer vision. In Sec.[4](#S4
    "4\. Threat model ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning
    Models Against Adversarial Attacks"), we build a general threat model for deep-learning-based
    computer vision systems by examining the attack surface from different perspectives.
    In Sec.[3](#S3 "3\. Related work ‣ A Survey of Robustness and Safety of 2D and
    3D Deep Learning Models Against Adversarial Attacks"), we summarize the latest
    related reviews. In Sec.[5](#S5 "5\. Adversarial attacks for 2D deep learning
    models ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks") and [6](#S6 "6\. Adversarial attacks for 3D deep learning
    models ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks"), we summarize the representative works that evaluate the
    robustness and safety of deep learning models by 2D and 3D adversarial examples,
    respectively. In each section, we first introduce our taxonomy based on attack
    methodology or target applications, then summarize the representative digital-world
    and physical-world adversarial attacks. Finally, in Sec.[7](#S7 "7\. Future directions
    and challenges ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning
    Models Against Adversarial Attacks"), we identify the present obstacles in the
    adversarial attacks, coupled with some viewpoints for future studies.
  prefs: []
  type: TYPE_NORMAL
- en: To unify the symbols of different articles, Table.[1](#S1.T1 "Table 1 ‣ 1\.
    Introduction ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models
    Against Adversarial Attacks") shows some common notations in this survey.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/417c9946512bba03981678ebe68fc5b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The structure of this survey
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: The structure of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Common notations used in this survey
  prefs: []
  type: TYPE_NORMAL
- en: '| Notations | Description | Notations | Description |'
  prefs: []
  type: TYPE_TB
- en: '| $D_{tr},D_{te}$ | Training`\`testing dataset | $x$ | Normal inputs |'
  prefs: []
  type: TYPE_TB
- en: '| $x^{\prime},x^{adv}$ | Modified`\`adversarial inputs | $\delta$ | Adversarial
    perturbations |'
  prefs: []
  type: TYPE_TB
- en: '| $y$ | Ground truth labels | $y^{\prime}$ | Adversarial target labels |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{F}$ | Classification model | $\hat{\mathcal{F}}$ | Substitute model
    |'
  prefs: []
  type: TYPE_TB
- en: '| $g$ | The model’s gradient upon $x$ | $\hat{g}$ | The estimated model gradient
    |'
  prefs: []
  type: TYPE_TB
- en: '| $l_{p}$ | The $p$-norm distance | $Z_{i}(x)$ | Output logits of $i^{th}$-to-last
    layer |'
  prefs: []
  type: TYPE_TB
- en: '| $\theta$ | Model parameters | $\mathcal{L}(x^{\prime},y)$ | Loss function
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{G}$ | The GAN’s generator | $\mathcal{D}$ | The GAN’s discriminator
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{P}$ | Original point cloud | $\mathcal{P^{\prime}}$ | Adversarial
    point cloud |'
  prefs: []
  type: TYPE_TB
- en: 2\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is a popular representation learning algorithm for its outstanding
    performance on image classification and other tasks. It can learn complex functions
    through a composition of superficial but non-linear layers. Suppose $x$ is an
    image or a point cloud, $\mathcal{F}_{\theta}$ is a deep learning model with model
    parameters $\theta$. The goal of the object classification task is to find $\theta$
    that can minimize the difference between the ground truth label $y$ and the prediction
    $\mathcal{F}_{\theta}(x)$, that is
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $arg\min_{\theta}\mathcal{L}(\mathcal{F}_{\theta}(x),y),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}$ is a loss function to measure the entropy between the $\mathcal{F}_{\theta}(x)$
    and y. The $\theta$ is usually optimized through gradient descent algorithms,
    such as Adam. After training, the deep learning model is deployed into real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many deep learning model variants have been proposed to boost performance and
    adapt to different task characteristics, such as multi-layer perceptions, stacked
    autoencoders, convolutional neural networks, deep brief networks, and vision transformers.
    These models are widely used in various 2D computer vision tasks, such as object
    detection, image segmentation, image classification, action recognition, and motion
    tracking. Recently, with the boom of 3D data in self-driving systems and other
    applications, deep learning for 3D computer vision has attracted much attention.
    Various models came out, such as MLP-based models (e.g. PointNet (Qi et al., [2017](#bib.bib106))),
    convolutional models (e.g. Pointwise-CNN (Hua et al., [2018](#bib.bib53))), graph-based
    convolutional models (e.g. DGCNN (Wang et al., [2019](#bib.bib141))), and transformer-based
    models (e.g. PCT (Guo et al., [2021](#bib.bib44))). The major applications of
    the deep learning model in 3D computer vision can be categorized into three tasks:
    3D object detection and tracking, 3D object classification, and 3D object segment.
    Because most 3D deep learning models are extensions of 2D models, many security
    threats in the 2D domain also exist in the 3D domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To boost this adversarial machine learning arms race and mitigate the risk of
    adversarial attacks, some previous surveys have tried to summarize the latest
    relevant studies. For example, Serban (Serban et al., [2020](#bib.bib116)) collected
    adversarial attacks in the object recognition task. However, they did not include
    3D adversarial attacks and semantic attacks like color space perturbations. Miller
    et al. (Miller et al., [2020](#bib.bib89)) and Machado et al. (Machado et al.,
    [2021](#bib.bib86)) reviewed the recent progress of adversarial machine learning,
    but from the defense rather than the adversary perspective. Other reviews regard
    the adversarial attack as part of the AI attacks without a separate detailed investigation.
    For example, Liu et al. (Liu et al., [2018](#bib.bib79)) classified AI attacks
    into integrity, availability, and confidentiality attacks according to the traditional
    taxonomy of security violation. Papernot et al. (Papernot et al., [2018](#bib.bib103))
    and Liu et al. (Liu et al., [2020b](#bib.bib80)) categorized attacks based on
    the deep learning pipeline, dividing them into training and test phase attacks.
    He et al. (He et al., [2022a](#bib.bib48)) categorized the AI attacks into adversarial
    attacks, poisoning attacks, model extraction, and inversion attacks. Moreover,
    these surveys lack summaries of the latest research trends, such as the growing
    interest in transferable adversarial examples, semantic perturbations, 3D adversarial
    attacks, and physical-realizable adversarial attacks. In addition, although attack-agnostic
    robustness evaluation has emerged in recent years, the adversarial attack is still
    one of the most efficient and reliable ways to evaluate model robustness. There
    is still a lack of a systematic review to collate these developments and discuss
    future directions in light of these latest efforts.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Threat model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The threat model evaluates the possible risk and security level of the system
    by describing the target, capability, and knowledge of the attacker. By establishing
    a full-scale threat model, we can understand the security problems of the deep
    learning system more comprehensively. We first identify the complete and thorough
    attack surface when the deep neural networks are deployed into computer vision
    tasks, as shown in Figure.[2](#S4.F2 "Figure 2 ‣ 4.1.3\. Deep learning model.
    ‣ 4.1\. The attack surface of deep learning models ‣ 4\. Threat model ‣ A Survey
    of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial
    Attacks"). Then we analyze the adversary’s goals, capabilities, and knowledge
    to construct a threat panorama.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. The attack surface of deep learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attack surface is composed of all possible vulnerabilities in a system that
    an unauthorized user can access. As mentioned above, deep learning applications
    train the model according to the model performance on the training dataset and
    then deploy the model to real-world applications for various 2D or 3D tasks. In
    this process, the attack surface includes the training and test dataset and the
    deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1\. Training dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Collecting a training dataset with satisfactory quality requires a great amount
    of effort and money. Meanwhile, some datasets, like medical diagnostic records
    and personal genomic data, may contain sensitive information. Therefore, the privacy
    and confidentiality of these training datasets are essential. Membership inference
    attacks (Shokri et al., [2017](#bib.bib123)) and data reconstruction attacks (Salem
    et al., [2020](#bib.bib112)) can leak the attribute or information of training
    data. In addition, more capable attackers may be able to modify the training datasets.
    Under this assumption, backdoor attacks and poisoning attacks are proposed. The
    backdoor attacks make the model behave normally for clean samples but behave incorrectly
    for the same samples with specific stickers by adding triggers in the training
    process. The poisoning attacks modify the sample’s label in the training phase
    so that the model classifies the clean sample incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. Test dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The main threat to the test dataset is adversarial attack. The adversarial attack
    can make the model produce wrong results by adding invisible or semantic perturbations
    to the input data. In recent years, physical adversarial attacks have gained more
    and more attention. Through 2D patches (Duan et al., [2020](#bib.bib34)), 3D printing
    (Athalye et al., [2018b](#bib.bib7)), optical illumination (Gnanasambandam et al.,
    [2021](#bib.bib41)) or sensor injection (Cao et al., [2019b](#bib.bib17)), the
    attacker makes the camera or radar collect modified 2D or 3D data and output incorrect
    classification results. It is worth mentioning that this paper divides the adversarial
    attack into 2D and 3D attacks according to the data and model type rather than
    the attack media. For example, Athalye et al. (Athalye et al., [2018b](#bib.bib7))
    rendered 2D adversarial images onto the 3D-printed turtles. Although the turtles
    are 3D objects, the final input of the 2D model is the turtle image collected
    by the camera, so it is still a 2D adversarial attack.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3\. Deep learning model.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training deep learning models requires a lot of resources and time, so a well-trained
    deep learning model successfully applied to practice has great value. However,
    there are already some model extraction attacks that can steal sensitive information
    such as model parameters and decision boundaries. In addition, the model itself
    may also be subject to model revising attack. For example, an adversary can make
    the model misclassify by only flipping several bits of the model (Rakin et al.,
    [2021](#bib.bib109)) or poisoning the open-source pre-trained weights (Kurita
    et al., [2020](#bib.bib67)).
  prefs: []
  type: TYPE_NORMAL
- en: This paper mainly discusses the adversarial attack because it poses a more practical
    threat than other attacks. It does not need to have knowledge about the training
    dataset and even the model architecture (for black-box attacks) and, therefore,
    can be executed in the inference stage. In addition, generating strong AEs efficiently
    can improve the effectiveness and efficiency of adversarial training, which is
    essential for training robust models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9e69f12c67b660a931a76c86fe7b21f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. An overview of the attack surface when deep learning models are deployed
    into computer vision applications.
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: The structure of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Adversarial goal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to the attacker’s goal and the level of security violation, deep learning
    attacks can be divided into confidentiality attack, integrity attack, and availability
    attack. The main goal of the confidentiality attack is to leak the privacy of
    the model or data, such as the model inverse attack and membership inference attack.
    The primary aim of the integrity attack is to maliciously change the model’s output
    by modifying the training data, the test data, or even the model itself, such
    as backdoor attacks, adversarial attacks, and weight modification attacks. The
    availability attack aims to make the ML services unavailable to legal users. A
    typical example of availability attacks is to make the dataset not exploitable
    by maliciously poisoning the whole training dataset. For example, the Shortcut
    attack (Yu et al., [2022](#bib.bib158)) uses linearly separable noises assigned
    with target labels to mislead the neural network, resulting in the trained model
    with low accuracy on the test dataset. The current security research in deep learning
    focuses on confidentiality and integrity.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Adversarial capabilities and knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The attacker’s capabilities refer to the amount of information the attackers
    have and the actions they can perform. In the context of deep learning systems,
    the attacker’s abilities, from weak to strong, can be classified as follows: only
    being able to access hard labels and revising test data; being able to access
    model output confidence; having the ability to access model parameters and training
    data; modifying training data; modifying model parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: According to the attackers’ understanding of models and data, the adversarial
    knowledge can be classified into black-box, gray-box, and white-box models. There
    is no clear dividing line between the three. Generally, the attacks that know
    the model’s internal structure and parameters are called white-box attacks. The
    attacks that can only access the classification results are called black-box attacks,
    and the attacks in between are called gray-box attacks. In reality, adversarial
    knowledge is usually determined by adversarial ability, so the two are closely
    related.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Adversarial attacks for 2D deep learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The adversarial attack, one of the most threatening attacks, has attracted much
    research interest. In this and the next section, we will discuss the adversarial
    attacks in the deep learning-based computer vision system, in which the adversary’s
    goal is to cause the deep learning models to produce wrong predictions. We divide
    them into 2D and 3D attacks according to the different target model types. The
    2D attacks mainly target the 2D models, while the 3D attacks mainly target the
    3D model by contaminating 3D data like point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Catalog of 2D adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the 2D adversarial attack has been summarized in some previous literature
    (Serban et al., [2020](#bib.bib116); Machado et al., [2021](#bib.bib86); He et al.,
    [2022a](#bib.bib48)), they usually organize from a certain aspect, lacking systematization
    and the latest research progress. Therefore, there is a need to reorganize the
    latest studies, especially the emerging ones that cannot be classified into any
    previous categories, such as color space perturbations and other semantic perturbations.
    This paper summarizes the most recent progress to help readers keep abreast of
    the newest tendency. Our survey classified 2D adversarial attacks from various
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Classifying according to different distance metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When the term adversarial example was first proposed in 2013 (Szegedy et al.,
    [2013](#bib.bib129)), its definition is leveraging crafted imperceptible perturbation
    to fool the deep neural network, that is
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\min_{x^{\prime}}\left\&#124;{x^{\prime}-x}\right\&#124;_{p}\text{,
    subject to }\mathcal{F}(x^{\prime})\neq\mathcal{F}(x)\text{ and }x^{\prime}\in[0,1]^{m},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $p\in\{0,1,2,\infty\}$. Besides $l_{p}$-norm distance, recently there
    are also other distance metrics are proposed to constrain the perturbation size,
    such as color space distance (Shamsabadi et al., [2020b](#bib.bib119); Laidlaw
    and Feizi, [2019](#bib.bib68)), geodesic distance (Fawzi and Frossard, [2015](#bib.bib37)),
    Wasserstein distance (Zheng et al., [2019a](#bib.bib174)), or even without any
    distance limitation (Song et al., [2018](#bib.bib125)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Classifying according to physical achievability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on whether they can be realized in the physical world, 2D adversarial
    attacks can be divided into digital attack and physical attacks. The digital adversarial
    attacks assume that the adversary can directly modify the digital images, while
    the physical adversarial attacks suppose that the attacker cannot immediately
    revise the neural network’s input and, therefore, revise the real-world objects
    instead. Physical attacks are more difficult than digital attacks because of limited
    perturbation space and various environmental variables, such as different viewpoints,
    distance, and background illumination.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3\. Classifying according to adversary’s knowledge level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned in the previous section, according to the adversary’s knowledge
    extent, 2D digital adversarial attacks can be divided into white-box, gray-box,
    and black-box attacks. Different knowledge extent can result in disparate choices
    of attack methods because of different difficulties. Therefore, we introduce the
    white-box attacks and black-box attacks separately.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4\. Classifying according to the definition of perturbation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: According to the perturbation’s difference, Gilmer et al. (Gilmer et al., [2018](#bib.bib40))
    classified adversarial perturbations into imperceptible, content-preserving, and
    unconstrained perturbation. The first one sets the original image as the starting
    point and adds invisible noises to this image. This constraint makes the AE almost
    the same as the clean one in human eyes and usually uses $l_{p}$-norm as their
    distance metrics. For content-preserving perturbation, the adversary retains the
    semantics of the original image and misleads the classifier at the same time by
    methods like changing colors (Hosseini and Poovendran, [2018](#bib.bib51); Laidlaw
    and Feizi, [2019](#bib.bib68)) and spatially transforms the images (Fawzi and
    Frossard, [2015](#bib.bib37)). These attacks usually use semantic distance to
    restrain the perturbation. For unconstrained input, the adversary induces erroneous
    results from the neural networks through any examples. For instance, Song et al.
    (Song et al., [2018](#bib.bib125)) generates AEs from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the above observation, we classify 2D adversarial attacks into digital
    white-box and black-box adversarial attacks, and physical adversarial attacks,
    as shown in Figure.[1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey of Robustness
    and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks").
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Summary of main white-box adversarial attacks in 2D CV tasks sorted
    by the algorithm and published year
  prefs: []
  type: TYPE_NORMAL
- en: '| Attacks | Year | Threat model | Algorithm | Distance | Performance | Key
    idea |'
  prefs: []
  type: TYPE_TB
- en: '| Goal | Knowl.^* | Efficiency | Camouflage | ASR ^(**) |'
  prefs: []
  type: TYPE_TB
- en: '| L-BGFS (Szegedy et al., [2013](#bib.bib129)) | 2014 | T | $\square$ | Optimization
    | $L_{2}$ | Costly | Invisible | 100.0% | Box constrained L-BGFS |'
  prefs: []
  type: TYPE_TB
- en: '| JSMA (Papernot et al., [2016b](#bib.bib102)) | 2016 | T | $\square$ | Optimization
    | $L_{0}$ | Efficient | Slight | 97.2% | $L_{0}$ + greedy |'
  prefs: []
  type: TYPE_TB
- en: '| UAP (Moosavi-Dezfooli et al., [2017](#bib.bib92)) | 2017 | U | $\square$
    | Optimization | $L_{1},L_{\infty}$ | Medium | Invisible | 93.7% | Universal perturbation
    |'
  prefs: []
  type: TYPE_TB
- en: '| C&W (Carlini and Wagner, [2017](#bib.bib18)) | 2017 | T&U | $\square$ | Optimization
    | $L_{0},L_{2},L_{\infty}$ | Medium | Invisible | 100.0% | Substitute loss function
    |'
  prefs: []
  type: TYPE_TB
- en: '| EAD (Chen et al., [2018b](#bib.bib22)) | 2017 | T&U | $\square$ | Optimization
    | $L_{1},L_{2}$ | Costly | Invisible | 100.0% | $L_{1}$ + $L_{2}$ distance |'
  prefs: []
  type: TYPE_TB
- en: '| DAG (Xie et al., [2017](#bib.bib154)) | 2017 | U | $\square$, | Optimization
    | $L_{\infty}$ | Efficient | Invisible | 69.0% | Segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| SV-UAP (Khrulkov and Oseledets, [2018](#bib.bib64)) | 2018 | U | $\square$
    | Optimization | $L_{2},L_{\infty}$ | Medium | Marked | 60.0% | Singular vector
    |'
  prefs: []
  type: TYPE_TB
- en: '| GD-UAP(Mopuri et al., [2018a](#bib.bib94)) | 2018 | U | $\square$,$\color[rgb]{.5,.5,.5}\blacksquare$
    | Optimization | $L_{\infty}$ | Medium | Invisible | 83.5% | Data-free UAP |'
  prefs: []
  type: TYPE_TB
- en: '| RobustAdv (Luo et al., [2018](#bib.bib82)) | 2018 | U | $\square$,$\blacksquare$
    | Optimization | Percep. | Costly | Slight | 98.5% | Perceptual sensitivity |'
  prefs: []
  type: TYPE_TB
- en: '| TAP (Zhou et al., [2018](#bib.bib177)) | 2018 | U | $\square$ | Optimization
    | $L_{2}$ | Efficient | Marked | 46.7% | Low-pass filter |'
  prefs: []
  type: TYPE_TB
- en: '| DAA(Zheng et al., [2019a](#bib.bib174)) | 2019 | U | $\square$ | Optimization
    | $L_{\infty}$ | Medium | Marked | 45.0% | Advl-data distribution |'
  prefs: []
  type: TYPE_TB
- en: '| SparseFool (Modas et al., [2019](#bib.bib90)) | 2019 | U | $\square$ | Optimization
    | $L_{1}$ | Medium | Slight | 100.0% | Projected $L_{1}$-DeepFool |'
  prefs: []
  type: TYPE_TB
- en: '| DF-UAP(Zhang et al., [2020b](#bib.bib160)) | 2020 | T&U | $\square$ | Optimization
    | $L_{\infty}$ | Efficient | Slight | 96.2% | Targeted data-free UAP |'
  prefs: []
  type: TYPE_TB
- en: '| sC&W (Zhang et al., [2020a](#bib.bib162)) | 2020 | T&U | $\square$ | Optimization
    | Smooth | Medium | Marked | 98.0% | Smooth perturbation |'
  prefs: []
  type: TYPE_TB
- en: '| GreedyFool (Dong et al., [2020](#bib.bib31)) | 2020 | U | $\square$ | Optimization
    | $L_{0}$ | Costly | Invisible | 94.6% | Greedy algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| SSAH (Luo et al., [2022](#bib.bib83)) | 2022 | T&U | $\square$ | Optimization
    | Low Freq. | Medium | Invisible | 99.8% | High-frequency limitation |'
  prefs: []
  type: TYPE_TB
- en: '| FGSM (Goodfellow et al., [2014](#bib.bib42)) | 2015 | U | $\square$ | Fast
    Gradient | $L_{\infty}$ | Efficient | Marked | 72.3% | One-step gradient attack
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deepfool (Moosavi-Dezfooli et al., [2016](#bib.bib93)) | 2016 | U | $\square$
    | Fast Gradient | $L_{1},L_{2},L_{\infty}$ | Medium | Invisible | 90.0% | Boundary
    dist. estimation |'
  prefs: []
  type: TYPE_TB
- en: '| BIM&ILCM (Kurakin et al., [2018](#bib.bib66)) | 2016 | T&U | $\square$ |
    Fast Gradient | $L_{\infty}$ | Medium | Slight | 90.0% | Iteration FGSM |'
  prefs: []
  type: TYPE_TB
- en: '| MI-FGSM (Dong et al., [2018](#bib.bib32)) | 2017 | T&U | $\square$, | Fast
    Gradient | $L2,L_{\infty}$ | Efficient | Slight | 100.0% | Momentum |'
  prefs: []
  type: TYPE_TB
- en: '| PGD (Madry et al., [2017](#bib.bib87)) | 2017 | U | $\square$ | Fast Gradient
    | $L_{2},L_{\infty}$ | Medium | Slight | 99.2% | Projection grad. descent |'
  prefs: []
  type: TYPE_TB
- en: '| R-FGSM (Tramèr et al., [2017a](#bib.bib131)) | 2018 | T&U | $\square$ | Fast
    Gradient | $L_{\infty}$ | Efficient | Slight | 64.8% | Random FGSM |'
  prefs: []
  type: TYPE_TB
- en: '| BPDA&EOT (Athalye et al., [2018a](#bib.bib6)) | 2018 | U | $\square$ | Fast
    Gradient | $L_{2},L_{\infty}$ | Efficient | Invisible | 100.0% | Attack obfuscated
    grad. |'
  prefs: []
  type: TYPE_TB
- en: '| MDI2FGSM(Xie et al., [2019](#bib.bib155)) | 2019 | T&U | $\square$  $\blacksquare$
    | Fast Gradient | $L_{\infty}$ | Costly | Slight | 62.2% | Random transformation
    |'
  prefs: []
  type: TYPE_TB
- en: '| TI-BIM (Dong et al., [2019](#bib.bib33)) | 2019 | U | $\square$ | Fast Gradient
    | $L_{\infty}$ | Costly | Slight | 82.0% | Translation kernels |'
  prefs: []
  type: TYPE_TB
- en: '| DDN (Rony et al., [2019](#bib.bib111)) | 2019 | U | $\square$ | Fast Gradient
    | $L_{2}$ | Efficient | Slight | 100.0% | Adjustable step size |'
  prefs: []
  type: TYPE_TB
- en: '| HP-UAP(Zhang et al., [2021](#bib.bib161)) | 2021 | T&U | $\square$,$\blacksquare$
    | Fast Gradient | $L_{\infty}$ | Efficient | Invisible | 91.1% | Frequency filter
    |'
  prefs: []
  type: TYPE_TB
- en: '| NAG-UAP (Mopuri et al., [2018b](#bib.bib95)) | 2018 | U | $\square$ | GAN
    | $L_{\infty}$ | Costly | Slight | 94.1% | Generative UAP |'
  prefs: []
  type: TYPE_TB
- en: '| ATN (Baluja and Fischer, [2018](#bib.bib9)) | 2018 | T | $\square$ | GAN
    | $L_{2}$ | Efficient | Slight | 95.9% | GAN+reranking |'
  prefs: []
  type: TYPE_TB
- en: '| UnresGM(Song et al., [2018](#bib.bib125)) | 2018 | T | $\square$ | GAN |
    $-$ | Costly | Marked | 84.0% | GAN from scratch |'
  prefs: []
  type: TYPE_TB
- en: '| GAP (Poursaeed et al., [2018](#bib.bib105)) | 2018 | T&U | $\square$ | GAN
    | $L_{2},L_{\infty}$ | Efficient | Slight | 74.1% | UAP and img-dependent |'
  prefs: []
  type: TYPE_TB
- en: '| AdvAttGAN(Joshi et al., [2019](#bib.bib61)) | 2019 | U | $\square$ | GAN
    | $-$ | Costly | Marked | 98.0% | Modify facial attribute |'
  prefs: []
  type: TYPE_TB
- en: '| SemAdv (Qiu et al., [2020](#bib.bib107)) | 2020 | T | $\square$,$\blacksquare$
    | GAN | $-$ | Costly | Marked | 67.7% | Modify visual attribute |'
  prefs: []
  type: TYPE_TB
- en: '| SparseGAN (He et al., [2022b](#bib.bib49)) | 2022 | U | $\square$ | GAN |
    $L_{0}$ | Efficient | Invisible | 58.4% | Perturbation decoupling |'
  prefs: []
  type: TYPE_TB
- en: '| Manitest (Fawzi and Frossard, [2015](#bib.bib37)) | 2015 | U | $\square$
    | Spatial Trans. | Geodesic | Costly | Invisible | 35.6% | Geodesics on manifold
    |'
  prefs: []
  type: TYPE_TB
- en: '| SimpleTrans (Engstrom et al., [2018](#bib.bib35)) | 2017 | U | $\square$  $\blacksquare$
    | Spatial Trans. | $-$ | Medium | Invisible | 90.0% | Simple transformation |'
  prefs: []
  type: TYPE_TB
- en: '| Manifool (Kanbak et al., [2018](#bib.bib62)) | 2018 | U | $\square$ | Spatial
    Trans. | Geodesic | Efficient | Invisible | 75.0% | Iterative method |'
  prefs: []
  type: TYPE_TB
- en: '| stAdv (Xiao et al., [2018b](#bib.bib152)) | 2018 | T | $\square$ | Spatial
    Trans. | T.V. | Efficient | Slight | 99.6% | Flow field |'
  prefs: []
  type: TYPE_TB
- en: '| Adef (Alaifari et al., [2018](#bib.bib4)) | 2019 | T | $\square$ | Spatial
    Trans. | $L_{2}$ | Medium | Invisible | 99.0% | Iterative deforming |'
  prefs: []
  type: TYPE_TB
- en: '| 3DRender (Zeng et al., [2019](#bib.bib159)) | 2019 | U | $\square$ | Spatial
    Trans. | $L_{2}$ | Costly | Marked | 90.7% | 3D renderer |'
  prefs: []
  type: TYPE_TB
- en: '| PSI (Zheng et al., [2019a](#bib.bib174)) | 2019 | U | $\square$ | Spatial
    Trans. | Wasserstein | Efficient | Invisible | 91.7% | Wasserstein distance |'
  prefs: []
  type: TYPE_TB
- en: '| EdgeFool (Shamsabadi et al., [2020a](#bib.bib117)) | 2020 | U | $\square$
    | Spatial Trans. | Smooth | Costly | Marked | 99.0% | Image enhancement |'
  prefs: []
  type: TYPE_TB
- en: '| Chroma-shift (Aydin et al., [2021](#bib.bib8)) | 2021 | T&U | $\square$ |
    Spatial Trans. | $-$ | Efficient | Invisible | 96.1% | YUV colorspace |'
  prefs: []
  type: TYPE_TB
- en: '| FilterFool (Shamsabadi et al., [2021](#bib.bib118)) | 2021 | U | $\square$
    | Spatial Trans. | SSIM | Costly | Slight | 48.3% | Mimic filter |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic (Hosseini and Poovendran, [2018](#bib.bib51)) | 2018 | U | $\square$
    | Color Trans. | $-$ | Costly | Marked | 94.3% | HSV colorspace |'
  prefs: []
  type: TYPE_TB
- en: '| Blind-Spot (Zhang et al., [2018a](#bib.bib163)) | 2019 | U | $\square$ |
    Color Trans. | kNN | Efficient | Marked | 100.0% | Blind Spot |'
  prefs: []
  type: TYPE_TB
- en: '| ReColorAdv (Laidlaw and Feizi, [2019](#bib.bib68)) | 2019 | U | $\square$
    | Color Trans. | Smooth | Medium | Slight | 97.0% | RGB/CIELUV colorspace |'
  prefs: []
  type: TYPE_TB
- en: '| cAdv&tAdv (Bhattad et al., [2019](#bib.bib11)) | 2020 | T | $\square$ | Color
    Trans. | $-$ | Medium | Marked | 99.7% | Colorization and texture |'
  prefs: []
  type: TYPE_TB
- en: '| PerC attack(Zhao et al., [2020a](#bib.bib172)) | 2020 | U | $\square$ | Color
    Trans. | CIEDE2000 | Efficient | Invisible | 100.0% | Perceptual color distance
    |'
  prefs: []
  type: TYPE_TB
- en: '| ColorFool (Shamsabadi et al., [2020b](#bib.bib119)) | 2020 | U | $\square$,$\blacksquare$
    | Color Trans. | $-$ | Costly | Marked | 95.90% | Adversarial colorization |'
  prefs: []
  type: TYPE_TB
- en: '*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This column is the adversarial knowledge of different attacks. $\square$: white-box.
    $\blacksquare$: black-box. ${\color[rgb]{.5,.5,.5}\blacksquare}$: gray-box.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We only count the best result of the most difficult attack reported in the papers.
    E.g. for works that have both white-box and black-box attacks, we only report
    the latter, for it is more difficult. For works with both untargeted and targeted
    attacks, we report the latter for the same reason.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2\. White-box adversarial attack for 2D deep learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [2](#S5.T2 "Table 2 ‣ 5.1.4\. Classifying according to the definition
    of perturbation ‣ 5.1\. Catalog of 2D adversarial attacks ‣ 5\. Adversarial attacks
    for 2D deep learning models ‣ A Survey of Robustness and Safety of 2D and 3D Deep
    Learning Models Against Adversarial Attacks") summarizes recent year’s 2D adversarial
    attacks for the white-box model. We divide them into the following categories
    according to their specific attack methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization-based attacks. This kind of attack usually describe finding AEs
    as objective optimization problems and solve these problems by existing or self-defined
    objective optimization method, include L-BFGS (Szegedy et al., [2013](#bib.bib129)),
    UAP (Moosavi-Dezfooli et al., [2017](#bib.bib92)), C&W (Carlini and Wagner, [2017](#bib.bib18)),
    EAD (Chen et al., [2018b](#bib.bib22)) and OptMargin (He et al., [2018](#bib.bib47)),
    etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast-gradient-based attacks. Instead of reaching a local minimum through optimizing
    an objective, this kind of attack finds adversarial examples through direct and
    explicit gradient computation. Therefore, they can usually find an adversarial
    example very quickly, although the perturbation may not be the optimum. Moreover,
    because these attacks lack clear objective functions, they are usually untargeted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GAN-based attacks. Instead of optimizing the noises through gradient descent,
    this kind of attack generates the perturbation through the generative adversarial
    network (GAN) and optimizes the variable in the latent space. For example, unrestricted
    AE (Song et al., [2018](#bib.bib125)) can generate semantic adversarial examples
    from drafts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatial-transformation-based attacks. Unlike additive noises, this attack utilizes
    global or local spatial transformations to generate adversarial examples. The
    former includes Manifool (Kanbak et al., [2018](#bib.bib62)) etc. and the latter
    includes stAdv (Xiao et al., [2018b](#bib.bib152)) etc. This kind of attack usually
    uses semantic loss rather than p-norm-based distance metrics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Colorization-transformation-based attacks. Some works also transform images
    into different color spaces and manipulate the color space instead because human
    beings prefer to classify objects according to their shapes rather than colors.
    These attacks include ColorFool (Shamsabadi et al., [2020b](#bib.bib119)), Chroma-shift
    (Aydin et al., [2021](#bib.bib8)) and SemanticAdv (Hosseini and Poovendran, [2018](#bib.bib51)),
    etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Some attacks also combine several different methods. For example, Zhao et al.
    (Zhao et al., [2019](#bib.bib169)) combined transformation and pixel-wise perturbations
    to enhance the attack strength. However, most attacks adopt one of the above methods
    as their main methodology. Therefore, we will introduce these attacks according
    to their primary methodology.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Fast-gradient-based attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The greatest difference between optimization-based and fast-gradient-based attacks
    is that the latter can quickly find adversarial examples in one or several iterations.
    However, they usually cannot reach a local optimum and need larger perturbation
    to mislead the models. Because of their high efficiency in finding adversarial
    examples, they can better combine with the adversarial training process to train
    robust neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Single-step fast gradient methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In 2015, Goodfellow et al. (Goodfellow et al., [2014](#bib.bib42)) conjectured
    that the AE exists because of the linear representation of features in high-level
    space and proposed gradient-based one-step attack method named fast gradient sign
    method (FGSM), which estimates adversarial examples by $x^{\prime}=x+\epsilon
    sign(\bigtriangledown_{x}\mathcal{L}_{\mathcal{F}}(x,y)).$ Moreover, they embedded
    FGSM into adversarial training to mine hard examples and train robust models.
    The loss function of FGSM-based adversarial training is
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\widetilde{\mathcal{L}}_{\mathcal{F}}=\lambda\mathcal{L}_{\mathcal{F}}(x,y)+(1-\lambda)\mathcal{L}_{\mathcal{F}}(x+\epsilon
    sign(\bigtriangledown_{x}\mathcal{L}_{\mathcal{F}}(x,y)),y).$ |  |'
  prefs: []
  type: TYPE_TB
- en: The loss function has two parts. The first part is the same as the normal training
    procedure, while the second part considers the negative influence of adversarial
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-step fast gradient methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Kurakin et al. (Kurakin et al., [2018](#bib.bib66)) extended the FGSM method
    to an iterative method (BIM). At the end of each step, they clipped the image
    into [0,1]. They also proposed LLCM, which uses the least likely label as the
    target label. In addition, they tried to attack the camera through printed images.
    but their attacks suffered low ASRs in the physical setting, for they lacked consideration
    of fabrication distortion.
  prefs: []
  type: TYPE_NORMAL
- en: Moosavi et al. (Moosavi-Dezfooli et al., [2016](#bib.bib93)) proposed Deepfool
    attack, which iteratively estimates the distance from the normal samples to the
    classifying hyperplane. Because of the nonlinearity of the classification boundary,
    they linearize the hyperplane at each iteration. If the model $\mathcal{F}(x)$
    is a binary classifier, at each iteration, $x$ is updated by $x^{t+1}\leftarrow
    x^{t}-\frac{\mathcal{F}(x^{t})}{\left\|{\bigtriangledown\mathcal{F}(x^{t})}\right\|_{2}^{2}}\bigtriangledown\mathcal{F}(x^{t})$.
    If the model $\mathcal{F}$ is a multiclass differentiable classifier, they use
    a polyhedron $\widetilde{P}_{i}$ to approximate the label space. Deepfool is also
    untargeted but can achieve smaller perturbations than the FGSM attack.
  prefs: []
  type: TYPE_NORMAL
- en: Madry et. al. (Madry et al., [2017](#bib.bib87)) proposed projection gradient
    descent (PGD), which projects $x_{t}$ onto the neighborhood of the input at the
    end of each iteration through $x^{t+1}=\textit{proj}_{x+\mathcal{S}}(x^{t}+\epsilon\,sign(\bigtriangledown_{x}L(\theta,x,y)))$.
    Osadchy et al. (Osadchy et al., [2017](#bib.bib99)) also proposed a similar method
    called I-FGSM and apply it to the image CAPTCHAs to distinguish between computers
    and humans.
  prefs: []
  type: TYPE_NORMAL
- en: Tramer et al. (Tramèr et al., [2017a](#bib.bib131)) found that the steep curvature
    artifacts near the inputs may reduce the attack strength of single-step attack.
    Therefore, they proposed R-FGSM, which introduces random noises into the input
    images. The update formulation is $x^{adv}=x^{\prime}+(\epsilon-\alpha)\cdot sign(\bigtriangledown_{x^{\prime}}\mathcal{L}(x^{\prime},y_{true}))$,
    where $x^{\prime}=x+\alpha\cdot sign(\mathcal{N}(\textbf{0}^{m},\textbf{I}^{m}))$,
    $\mathcal{N}(\textbf{0}^{m},\textbf{I}^{m})$ is the standard normal distribution.
    R-FGSM attack can be regarded as a single-step variant of the PGD attack.
  prefs: []
  type: TYPE_NORMAL
- en: MI-FSGM (Dong et al., [2018](#bib.bib32)) introduced a momentum term into the
    iterative FGSM and achieved better performance than FGSM and BIM in the untargeted
    attack. It firstly updated the momentum by $g_{t+1}=\mu\cdot g_{t}+\frac{{\bigtriangledown}_{x}\mathcal{L}(x_{t}^{\prime},y)}{\left\|{\bigtriangledown}_{x}\mathcal{L}(x_{t}^{\prime},y)\right\|_{1}}$,
    then updated the adversarial example $x_{t}^{\prime}$ by $x_{t+1}^{\prime}=x_{t}^{\prime}+\alpha\cdot
    sign(g_{t+1})$. By generating a velocity vector in the direction of gradient descent,
    MI-FGSM alleviates the problem of unstableness after multi-step iterations and
    enhances the transferability of AEs.
  prefs: []
  type: TYPE_NORMAL
- en: M-DI2-FGSM (Xie et al., [2019](#bib.bib155)) integrated random transformations
    into the MI-FGSM to improve the adversarial example’s transferability. Random
    resize and padding are used as the main transforms, and the transferability is
    increased by about 20% on the ImageNet dataset. However, this method may cause
    the loss to decline unstably. Dong et al. (Dong et al., [2019](#bib.bib33)) also
    proposed a similar attack named TI-BIM based on BIM and transformation. Because
    the gradient of transformed images equals transforming the gradient of original
    examples, they multiply the gradient of original images with a uniform, linear
    or Gaussian kernel at each iteration to represent different translations.
  prefs: []
  type: TYPE_NORMAL
- en: Rony et al. (Rony et al., [2019](#bib.bib111)) proposed Decoupling Direction
    and Norm attack (DDN) based on PGD. At each iteration, the perturbation $\delta_{k}$
    is projected to an $\epsilon_{k}$-ball around $x$ through $x_{k}^{\prime}\leftarrow
    x+\epsilon_{k}\frac{\delta_{k}}{\left\|\delta_{k}\right\|_{2}}$. Different from
    PGD that sets $\epsilon_{k}$ as a constant, they increase the $\epsilon_{k}$ at
    each iteration if $x_{k}^{\prime}$ is still not adversarial and reduce the $\epsilon_{k}$
    on the other hand. Their method achieved competitive results compared with state-of-art
    $L_{2}$-norm attacks and needed fewer iterations.
  prefs: []
  type: TYPE_NORMAL
- en: The relationships between different fast-gradient-based adversarial attacks
    are shown in Figure.[3](#S5.F3 "Figure 3 ‣ Multi-step fast gradient methods ‣
    5.2.1\. Fast-gradient-based attacks ‣ 5.2\. White-box adversarial attack for 2D
    deep learning models ‣ 5\. Adversarial attacks for 2D deep learning models ‣ A
    Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial
    Attacks").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb439d2c5ca793e3b95e3d01a6870fad.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. The relationships between different fast-gradient-based 2D adversarial
    attacks
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: The relationships between different fast-gradient-based attacks
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Optimization-based attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optimization-based attacks usually model the adversarial as optimization objectives
    and then leverage existing optimization methods or design custom optimization
    methods to solve these optimization problems. Moreover, most optimization-based
    attacks are based on $L_{p}$-norm distance. Different distance metrics can generate
    different kinds of perturbations. $L_{2}$ and $L_{\infty}$ distances usually generate
    uniform and dense perturbation, while $L_{0}$ and $L_{1}$ distances tend to generate
    sparse perturbation. Moreover, Besides image-independent attacks, universal perturbation
    has also attracted much research interest. Therefore, according to the different
    kinds of perturbations, we divide optimization-based attacks into dense, sparse,
    and universal perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: Dense perturbation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When the concept of adversarial example is first proposed, it is defined as
    minimizing the total perturbation in $L_{2}$ or $L_{\infty}$ distance because
    they are easier to compute gradient than other distances. These metrics lead to
    uniform perturbation on the whole image. The representative attacks include L-BFGS
    (Szegedy et al., [2013](#bib.bib129)), C&W (Carlini and Wagner, [2017](#bib.bib18)),
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of adversarial example was proposed by Szegedy et al. (Szegedy et al.,
    [2013](#bib.bib129)) in 2013\. They formulated the problem of finding adversarial
    examples as a box-constrained optimization problem under $l_{2}$ distance and
    utilized L-BFGS algorithm to find an approximate solution. In detail, for an input
    image $x\in\mathbb{R}^{m}$, they minimize $c\left\|{\delta}\right\|_{2}+\mathcal{L}_{\mathcal{F}}(x+\delta,y^{\prime}),\text{s.t.
    }x+\delta\in[0,1]^{m}$, in which $\mathcal{L}_{\mathcal{F}}(\cdot,\cdot)$ is a
    cross-entropy function. In addition, they also analyzed the upper Lipschitz constant
    of the model and proposed that the regularization of the model parameters may
    avoid the existence of adversarial examples. However, experiments show that L-BFGS
    has the weakness of inefficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Defensive distillation (Papernot et al., [2016c](#bib.bib104)) improved the
    model robustness by training models on soft training labels to reduce the network’s
    Jacobian matrix, making the model less sensitive to the input and avoiding overfitting
    against any samples. C&W (Carlini and Wagner, [2017](#bib.bib18)) designed an
    alternative loss to break defensive distillation, that is $max(max({Z_{2}(x^{\prime})_{i:i\neq
    y^{\prime}}})-Z_{2}(x^{\prime})_{y^{\prime}},-\kappa),$ where $\kappa$ is a parameter
    to control the strength of AE. Moreover, some former attacks, like BIM (Kurakin
    et al., [2018](#bib.bib66)), directly clip $x^{\prime}$ into $[0,1]^{m}$, which
    may result in zero gradients. C&W changed the optimized variable from $\delta$
    to $w\coloneqq arctanh(\delta)$. Because $tanh$ is a bounded and differential
    function, C&W can remove the box constraint and use Adam optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient mask (Papernot et al., [2017](#bib.bib101)) prevented adversarial attack
    through shattered, randomized, or vanishing/exploding gradients. Athalye et al.
    (Athalye et al., [2018a](#bib.bib6)) claimed that gradient masks also cannot provide
    enough robustness. They proposed BPDA, EOT, and Reparameterization attacks to
    break these defenses, respectively. Take BPDA as an example. Gradient shattering
    introduces a non-differentiable layer $f^{i}(x)$ to prevent the adversary from
    getting the gradient. BPDA attack adopts a differentiable layer $g(x)$ to substitute
    $f^{i}(x)$ in the back-propagation process.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training (Szegedy et al., [2013](#bib.bib129)) is also a popular
    defensive method. It uses adversarial examples as training data to improve model
    robustness (See Eq.[3](#S5.E3 "In Single-step fast gradient methods ‣ 5.2.1\.
    Fast-gradient-based attacks ‣ 5.2\. White-box adversarial attack for 2D deep learning
    models ‣ 5\. Adversarial attacks for 2D deep learning models ‣ A Survey of Robustness
    and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks")). Adversarial
    training through PGD (Madry et al., [2017](#bib.bib87)) has been proven effective
    in defending most first-order attacks, like FGSM and DeepFool. However, Zhang
    et al. (Zhang et al., [2018a](#bib.bib163)) found that adversarial trained networks
    may have blind spots or unusual examples. They first use kNN to measure the difference
    from the test example to the training dataset and adopt simple methods like scaling
    and shifting all pixels to find the blind-spot example of the model. Then, they
    attack these infrequent samples by C&W attack.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the transferability of adversarial perturbation, TAP attack (Zhou
    et al., [2018](#bib.bib177)) increases the feature distance of all latent layers
    between the normal and adversarial image. In addition, they also found that applying
    a low-pass filter on the perturbation can also improve transferability. Observing
    that optimizing on an independent example cannot be globally optimal, DAA (Zheng
    et al., [2019a](#bib.bib174)) searches an adversarial data distribution that satisfies
    $L_{\infty}$ constraint while maximizing the generalization error by Wasserstein
    gradient flows. DAA can reduce the accuracy of CIFAR to 44.71% for adversarial-trained
    networks. To make the perturbation more indistinguishable, sC&W (Zhang et al.,
    [2020a](#bib.bib162)) adds Laplacian smoothing constraint on the original C&W
    attack. This constraint makes the perturbation locally smooth on the even areas
    and dissonant on the high variance area. Like the sC&W attack, SSAH (Luo et al.,
    [2022](#bib.bib83)) composes a semantic similarity loss and a low-frequency restraint.
    The former minimizes the cosine similarity of the features between the original
    and target image. The latter restrains the perturbation into the high-frequency
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation and object-detecting tasks are also significant in computer
    vision. Xie et al. (Xie et al., [2017](#bib.bib154)) proposed dense adversary
    generation (DAG) attack against multi-object detection task. Observing that this
    task usually needs to classify plural targets in an image, DAG attacks all targets
    simultaneously using the iterative gradient descent. DAG can also transfer among
    different models, which is probably because it accumulates perturbations from
    multiple targets.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse perturbation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In some scenarios, the number of pixels being modified is more crucial than
    the overall magnitude of the perturbation. Therefore, some researchers proposed
    generating sparse perturbations through $l_{0}$ or $l_{1}$ distance, equivalent
    to modifying as few pixels as possible. However, specific techniques are needed
    because these distances are non-differentiable. For example, JSMA (Papernot et al.,
    [2016b](#bib.bib102)) selects the most meaningful pixels by comparing the gradient
    of $Z(x)$ and adds them to the set of modified pixels one by one until the attack
    succeeds or a certain threshold is reached. EDA attack (Chen et al., [2018b](#bib.bib22))
    formulates the problem as an Elastic-net regularized optimization problem that
    combines the $l_{1}$ and $l_{2}$ distance. Because $l_{1}$ distance is non-differentiable,
    EDA applies the FISTA algorithm to solve this optimization function. The $l_{1}$-DeepFool
    (Moosavi-Dezfooli et al., [2016](#bib.bib93)) can generate sparse perturbations
    efficiently through linearizing the boundary, but it is greatly affected by the
    clipping function. To solve this problem, SparseFool (Modas et al., [2019](#bib.bib90))
    projects $x^{\prime}$ on one component of the normal vector at each iteration.
    If the projection cannot generate an adversarial example, this direction will
    be ignored in the next iteration. SparseFool iteratively estimates the minimum
    perturbation to the boundary until the predicted label is changed. RobustAdv (Luo
    et al., [2018](#bib.bib82)) generates imperceptible and robust adversarial examples
    through perception distance, which is defined as $D(X^{\prime},X)=\sum_{i=1}^{N}\delta_{i}\cdot
    Sen(x_{i})$, where $N$ is the total pixels being modified, $\delta_{i}$ is the
    perturbation of the pixel $x_{i}$, and $Sen(x_{i})$ is the perturbation sensitivity
    of $x_{i}$. Because humans are more sensitive to the perturbation in smoothing
    regions, RobustAdv defines $Sen(x_{i})$ as the reciprocal of the standard deviation
    around $x_{i}$. GreedyFool (Dong et al., [2020](#bib.bib31)) uses a two-step greedy-based
    optimization to generate sparse perturbation. Firstly, the candidate manipulation
    locations are selected according to their gradient and their distortion map generated
    by a GAN. Secondly, unnecessary points are discarded to improve the sparsity.
    There are also some black-box attacks that can generate sparse perturbation, such
    as OnePixel (Su et al., [2019](#bib.bib126)) and CornerSearch (Croce and Hein,
    [2019](#bib.bib30)) and GeoDA(Rahmati et al., [2020](#bib.bib108)). We will discuss
    them in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Universal perturbation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Rather than optimizing the perturbation on a specific image, universal perturbation
    produces image-agnostic perturbations through optimizing perturbation over the
    whole or part of the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, Moosavi-Dezfooli et al. (Moosavi-Dezfooli et al., [2017](#bib.bib92))
    firstly proposed the concept of universal adversarial perturbations (UAP). They
    successfully attacked most images in the validation set by aggregating the minimum
    perturbations on the training images. Experiments show that UAP attack can not
    only attack unseen images but also transfer between different models with a success
    rate of 40~60%. Later, in 2018, Khrulkov et al. (Khrulkov and Oseledets, [2018](#bib.bib64))
    proposed SV-UAP attack, which approximates universal perturbation problem as a
    (p, q)-singular problem. Suppose $J_{i}(x)$ is the Jacobian matrix of $i^{th}$-to-last
    layer. For a small vector $\delta$, $Z_{i}(x+\delta)-Z_{i}(x)\approx J_{i}(x)\delta$.
    Therefore, the problem of finding a universal perturbation can be formulated as
    $\max_{\delta}\;\sum_{x_{j}\in X_{b}}\left\|J_{i}(x_{j})\delta\right\|_{q}^{q},\;s.t.\;\left\|\delta\right\|_{p}=C$
    where $X_{b}$ is a subset of the training dataset. Then, they use the stochastic
    power method to solve this (p,q)-singular vector problem and achieved a fooling
    rate of 60% on 50000 images by using only 64 images for optimization. Mopuri et
    al. (Mopuri et al., [2018a](#bib.bib94)) proposed a generalizable data-free UAP
    attack (GD-UAP) in which the adversary does not need to access the exact training
    data. The idea is to find an image-agnostic perturbation that triggers more additional
    model activation. The propagation effect of the neural network will eventually
    lead to misclassification. They use the knowledge about the distribution of the
    training dataset to improve the fooling rate. In 2020, Zhang et al. (Zhang et al.,
    [2020b](#bib.bib160)) firstly proposed a targeted UAP attack named DF-UAP. They
    analyzed the similarity between the predicted logits of clean images and UAP using
    the Pearson correlation coefficient and found that UAP has a dominant role in
    prediction compared with clean images. Based on this discovery, they randomly
    sample the proxy dataset at each iteration to update the perturbation. Inspired
    by the success of steganography of neural networks, In 2021, Zhang et al. (Zhang
    et al., [2021](#bib.bib161)) proposed HP-UAP, which adds a high-pass filter in
    the iteration to make the UAP more invisible to human eyes with only a small decrease
    of attack success rate.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3\. GAN-based attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The generative adversarial network (GAN) was proposed in 2014 by Goodfellow
    and quickly became a powerful tool for learning the distribution of training datasets.
    The first related work, adversarial transformation network (ATN) (Baluja and Fischer,
    [2018](#bib.bib9)), is reported in 2017\. Its loss function is $\min_{\theta}\sum_{x\in\mathcal{X}}\beta
    L_{2}(\mathcal{G}(x),x)+L_{2}(f(\mathcal{G}(x)),r(f(x),t))$, where $r$ is a reranking
    function to maintain the logits’ ranking order except the target class. Song et
    al. (Song et al., [2018](#bib.bib125)) proposed to use GAN to generate unrestricted
    AEs from noises $z$ rather than adding perturbation. The loss function includes
    three parts. The first is to make the victim model predict $\mathcal{G}(z,y_{s})$
    as the target class. The second is to limit the search region of $z$, and the
    third is to make the auxiliary classifier predict $\mathcal{G}(z,y_{s})$ as the
    source class $y_{s}$. They also engage humans to evaluate the fidelity of AEs.
    Moreover, they proposed a noise-augmented version to improve the ASR. Konda et
    al. (Mopuri et al., [2018b](#bib.bib95)) proposed NAG-UAP, which models the distribution
    of UAP by a UAP generator. The loss function of the GAN network includes two parts:
    a fooling loss and a diversity loss. The former’s goal is to minimize the classifier’s
    output on the ground truth label, while the latter’s goal is to produce diversified
    UAPs by maximizing the gap of the hidden layer’s outputs on different UAPs. Poursaeed
    et al. (Poursaeed et al., [2018](#bib.bib105)) proposed GAP attack that generates
    both universal and image-independent noises for targeted and untargeted attacks.
    For universal attacks, they use image-to-image translation networks to generate
    universal perturbations from random noises and crop the perturbations to have
    a fixed norm. Next, the perturbation is added to the normal images and sent to
    the victim model to calculate the fooling loss. For image-independent attacks,
    the perturbations are generated from the original image instead. He et al. (He
    et al., [2022b](#bib.bib49)) utilized a generative model to improve the transferability
    of sparse perturbation. They first decoupled perturbation into a magnitude component
    and a location component. Then, they add a sparse loss onto the location component.
    Because the location component is a binary operator, they proposed a binary quantization
    operator to train the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: The emergence of conditional neural networks makes directly editing artificially
    designed properties possible (e.g. with and without eyeglasses). Joshi et al.
    (Joshi et al., [2019](#bib.bib61)) used a conditional generative model to generate
    semantic adversarial examples. Their attack optimized over the attributes space
    of the conditional generative model to manipulate semantic features like wearing
    glasses or not and different skin colors to fool the deep learning models. Qiu
    et al. (Qiu et al., [2020](#bib.bib107)) also proposed a similar method. But their
    method can choose arbitrary targets to attack. In addition, they manipulated the
    interpolated feature space instead of the attributes manifold.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4\. Spatial-transformation-based attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although deep neural networks like CNN are designed to be invariant and robust
    to transformations like translation and rotation, researchers have shown this
    is not always the case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d3c69c452915e4e41b3d2ac1e0a76a4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) ManiFool
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/97c6ab7f1c5897d0955d025a930fb0b3.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) stAdv
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. Comparing globally and locally transformation attacks. (a) ManiFool
    attack (Kanbak et al., [2018](#bib.bib62)) optimizes rotation and transition parameters
    on the manifolds. (b) stAdv attack (Xiao et al., [2018b](#bib.bib152)) optimizes
    the flow field on pixel coordinate space. The pixel values are viewed as a function
    of the pixel coordinates. The flow field is obtained by backpropagation from the
    adversarial loss function to pixel coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: The structure of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Globally spatial-transformation-based attacks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The first work of transformation-based attacks, Manitest (Fawzi and Frossard,
    [2015](#bib.bib37)), was proposed in 2015\. It casts the problem of computing
    the gap between converted images as the geodesics along the transformation manifold,
    which is defined as the minimum length of the curve on the manifold. Manitest
    utilizes fast marching to find adversarial transformations, which can assure minimal
    transformation on the searched grids. However, it suffers a low efficiency when
    the transformation variety increases. SimpleTrans (Engstrom et al., [2018](#bib.bib35))
    also uses simple spatial transformations to fool networks. It contains a white-box
    algorithm based on gradient descent and a black-box algorithm based on grid search.
    In 2018, Kanbak et al. (Kanbak et al., [2018](#bib.bib62)) proposed Manifool to
    analyze the transformation robustness of deep learning models, as shown in Figure.[4](#S5.F4
    "Figure 4 ‣ 5.2.4\. Spatial-transformation-based attacks ‣ 5.2\. White-box adversarial
    attack for 2D deep learning models ‣ 5\. Adversarial attacks for 2D deep learning
    models ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks") (a). They also used the geodesic distance to measure the
    two transformations’ distance. The Manifool consists of two steps: determining
    the changing direction of the image and mapping this change onto the manifold.
    Although their method is not as accurate as Manitest’s, they can achieve high
    efficiency even with operating similarity and affine transformations simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: Locally spatial-transformation-based attacks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As shown in Figure.[4](#S5.F4 "Figure 4 ‣ 5.2.4\. Spatial-transformation-based
    attacks ‣ 5.2\. White-box adversarial attack for 2D deep learning models ‣ 5\.
    Adversarial attacks for 2D deep learning models ‣ A Survey of Robustness and Safety
    of 2D and 3D Deep Learning Models Against Adversarial Attacks") (b), the stAdv
    attack (Xiao et al., [2018b](#bib.bib152)) defined the transformation as a spatial
    flow field, where each flow vector corresponds to a displacement of the corresponding
    pixel. To enhance the local smoothness of the flow field to make the AE more natural,
    they define a flow cost as the total variation of the flow field. They also visualized
    the CAM attention and showed that their method can better distract attention than
    C&W methods. In 2019, Alaifari et al. (Alaifari et al., [2018](#bib.bib4)) proposed
    ADef attack inspired by Deepfool. They iteratively deform the original image through
    gradient descent to move the image over the boundary of the classification. The
    pixel’s value is modeled as a function of its coordinate. Moreover, a two-dimensional
    Gaussian filter is used to inflict smoothness on the flow field. In 2020, EdgeFool
    (Shamsabadi et al., [2020a](#bib.bib117)) generates adversarial examples through
    enhancing image details. Contrary to ColorFool (Shamsabadi et al., [2020b](#bib.bib119)),
    EdgeFool only modifies the $L$ channel and leave $a$ and $b$ channel unmodified.
    It firstly trains an FCNN network to learn a smoothing image $I_{s}$ from the
    input image $I$, and modifies the image details $I_{d}:=I-I_{s}$ to generate adversarial
    examples. Inspired by EdgeFool, FilterFool (Shamsabadi et al., [2021](#bib.bib118))
    viciously modifies a picture by imitating normal filter functions like detail
    enhancement, gamma correction, and log transformation. Their attack comprises
    an FCNN network, a traditional filter, and a classifier. The FCNN network is trained
    to generate perturbation that simultaneously fools the classifier and makes the
    perturbed image’s structure similar to the filtered image. Experiments show that
    FilterFool can improve the transferability of adversarial examples. Wong et al.
    (Wong et al., [2019](#bib.bib147)) generated AEs by Wasserstein distance instead
    of $l_{p}$ distance, which equals moving pixel mass in the images. Their attack
    follows the PGD attack but projects $x$ onto a Wasserstein ball rather than $l_{\infty}$
    ball and uses projected Sinkhorn iteration (PSI) to accelerate the computation.
    Compared to the $l_{p}$-based perturbation that indiscriminately affects both
    foreground and background pixels, Wasserstein perturbation only changes the foreground
    for images with monochrome backgrounds.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.5\. Colorization-transformation-based attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides manipulating images in the spatial domain, some works transform images
    into different color spaces and then modify the color space instead, for humans
    prefer to classify images based on geometry rather than colors. These attacks
    define the perturbation as ”content-preserving perturbation”, which is usually
    measured by perceptual similarity (Wang et al., [2021c](#bib.bib142); Aydin et al.,
    [2021](#bib.bib8)) rather than $l_{p}$ distance.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, Hosseini et al. (Hosseini and Poovendran, [2018](#bib.bib51)) first
    proposed SemanticAdv. It transforms the original image from RGB to HSV domain
    and leverages random search to modify the Hue and Saturation components. However,
    their adversarial examples may look factitious because hue and saturation components
    are modified at the same time. ColorFool (Shamsabadi et al., [2020b](#bib.bib119))
    expanded SemanticAdv to generate more realistic pictures by conducting content-based
    segmentation on target images and revising the interesting and unimportant regions,
    respectively. They modify the $a$ and $b$ channels in the $Lab$ color space and
    leave the lightness channel $L$ unchanged. In 2019, Laidlaw et al. (Laidlaw and
    Feizi, [2019](#bib.bib68)) proposed ReColorAdv attack, which computes the flow
    field in RGB or CIELUV color space. They also define a smooth loss to improve
    imperceptibility. In addition, they prove that combining the color space distortion
    with additive noises can expand the perturbation space. Zeng et al. (Zeng et al.,
    [2019](#bib.bib159)) firstly edited the 3D object’s physical properties, such
    as affine transformation, color, and illumination, and then rendered it into an
    image to fool the classifier. They optimize the physical variables by FGSM for
    derivable factors or the zeroth-order optimization approach for non-derivable
    factors. Bhattad et al. (Bhattad et al., [2019](#bib.bib11)) generated unrestricted
    AEs through semantic perturbation like colorization (cAdv) and texture transfer
    (tAdv). For cAdv attack, a pre-trained colorization network (Zhang et al., [2016](#bib.bib166))
    is used to modify the color. They attack the input hints and masks at the same
    time to make the color more realistic. For the tAdv attack, they extract the texture
    from the target image using a VGG19 network and leverage an extra restraint on
    the cross-layer gram matrices to control the texture transfer’s strength. Experiments
    show that they can generate photorealistic AEs. In 2020, Zhao et al. (Zhao et al.,
    [2020a](#bib.bib172)) proposed PerC-C&W and PerC-AL attack. Their attacks directly
    operate in the RGB color space and use CIEDE2000 as the perceptual color distance.
    PerC-AL improves the optimizing efficiency by alternatively minimizing the adversarial
    loss and perceptual loss. Experiments show that although the perturbation size
    is slightly higher than C&W method in $L_{2}$ metric, it is still imperceptible
    for human eyes and achieves better transferability. In 2012, inspired by the stAdv
    attack, Aydin et al. (Aydin et al., [2021](#bib.bib8)) proposed a more imperceptible
    attack called Spatial Chroma-shift. The basic idea is that humans are less sensitive
    to color changes than brightness changes. Therefore, they only apply the spatial
    transformation to the colorspace. They transform the color space from RGB to YUV
    and then only compute the flow field on UV channels. Moreover, they use LPIPS
    and DISTS to evaluate the perceptual similarity and show that their method outperforms
    C&W and stAdv attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Black-box adversarial attack for 2D deep learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the above attacks generated AEs in the white-box assumption. However, in
    reality, the adversary often has little prior knowledge of the model internal
    architecture and parameters. Black-box attacks are proposed to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1\. Different black-box scenarios
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: According to the information the adversary can access, the black-box settings
    can be classified into the query-constrained scenario, score-based scenario, and
    hard label scenario (Ilyas et al., [2018b](#bib.bib56)). For the first circumstance,
    the attacker can only query the victim model a limited number of times. For example,
    some APIs may charge fees if the adversary queries many times. For the second
    circumstance, confidence scores are available, such as the Google Cloud Vision
    API. In the last circumstance, only top-1 or top-k predicted labels are output.
    For example, the Apple Photo app can automatically classify the user’s pictures
    without displaying score information.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Summary of main black-box adversarial attacks in 2D CV tasks sorted
    by the algorithm and published year
  prefs: []
  type: TYPE_NORMAL
- en: '| Attacks | Year | Threat Model | Algorithm | Distance | Performance | Key
    idea |'
  prefs: []
  type: TYPE_TB
- en: '| Goal | Knowl.^* | Efficiency^(**) | ASR^(***) |'
  prefs: []
  type: TYPE_TB
- en: '| TML (Papernot et al., [2016a](#bib.bib100)) | 2016 | U | $\blacksquare$ |
    Subs. model | $L_{\infty}$ | Costly | 96.19% | Data augment. |'
  prefs: []
  type: TYPE_TB
- en: '| TIMI (Dong et al., [2019](#bib.bib33)) | 2019 | U | $\blacksquare$ | Subs.
    model | $L_{2}$ | Efficient | 49.00% | Tranform-invariant |'
  prefs: []
  type: TYPE_TB
- en: '| FDA (Ganeshan et al., [2019](#bib.bib38)) | 2019 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 80.20% | Feature distortion |'
  prefs: []
  type: TYPE_TB
- en: '| ILA (Huang et al., [2019](#bib.bib55)) | 2019 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 85.80% | Feature distortion |'
  prefs: []
  type: TYPE_TB
- en: '| SIM (Lin et al., [2019](#bib.bib73)) | 2019 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Medium | 77.20% | Scale-invariant |'
  prefs: []
  type: TYPE_TB
- en: '| TTTA (Li et al., [2020](#bib.bib69)) | 2020 | T | $\blacksquare$ | Subs.
    model | Poincare | Efficient | 42.90% | Pointcare distance |'
  prefs: []
  type: TYPE_TB
- en: '| RDI (Zou et al., [2020](#bib.bib179)) | 2020 | T&U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 67.80% | Multi-scale gradient |'
  prefs: []
  type: TYPE_TB
- en: '| DI-MI-TI (Zhao et al., [2021b](#bib.bib173)) | 2021 | T | $\blacksquare$
    | Subs. model | $L_{\infty}$ | Costly | 62.20% | Enough iteration |'
  prefs: []
  type: TYPE_TB
- en: '| FIA (Wang et al., [2021a](#bib.bib143)) | 2021 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 83.50% | Feature importance |'
  prefs: []
  type: TYPE_TB
- en: '| VT(Wang and He, [2021](#bib.bib139)) | 2021 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Medium | 76.50% | Gradient variance |'
  prefs: []
  type: TYPE_TB
- en: '| NAA(Zhang et al., [2022b](#bib.bib164)) | 2022 | U | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Costly | 85.0% | Feature importance |'
  prefs: []
  type: TYPE_TB
- en: '| ODI(Byun et al., [2022](#bib.bib15)) | 2022 | T | $\blacksquare$ | Subs.
    model | $L_{\infty}$ | Efficient | 81.6% | 3D render |'
  prefs: []
  type: TYPE_TB
- en: '| Img2video (Wei et al., [2022](#bib.bib144)) | 2022 | U | $\blacksquare$ |
    Subs. model | $L_{\infty}$ | Costly | 77.88% | Cross-modality |'
  prefs: []
  type: TYPE_TB
- en: '| GNAE(Zhao et al., [2018](#bib.bib171)) | 2018 | U | $\blacksquare$ | GAN
    | $L_{2}$ | Costly | 78.00% | Natural AE |'
  prefs: []
  type: TYPE_TB
- en: '| AdvGAN(Xiao et al., [2018a](#bib.bib150)) | 2018 | T | $\blacksquare$,$\color[rgb]{.5,.5,.5}\blacksquare$
    | GAN | $L_{2}$ | Efficient | 92.76% | Distilled model |'
  prefs: []
  type: TYPE_TB
- en: '| ATTA(Wu et al., [2021](#bib.bib149)) | 2021 | U | $\blacksquare$ | GAN |
    $L_{\infty}$ | Costly | 61.80% | Adv transform |'
  prefs: []
  type: TYPE_TB
- en: '| Boundary Attack (Brendel et al., [2017](#bib.bib12)) | 2017 | T&U | $\blacksquare$
    | Decision | $L_{0}$ | Costly | $-$ | Random walk |'
  prefs: []
  type: TYPE_TB
- en: '| BiasedBA (Brunner et al., [2019](#bib.bib14)) | 2019 | T&U | $\blacksquare$
    | Decision | $L_{2}$ | Efficient | 85.00% | Biased sampling |'
  prefs: []
  type: TYPE_TB
- en: '| OPT(Cheng et al., [2019b](#bib.bib27)) | 2019 | T&U | $\blacksquare$ | Decision
    | $L_{2}$ | Medium | 100.00% | Binary search |'
  prefs: []
  type: TYPE_TB
- en: '| HopSkipJump (Chen et al., [2020](#bib.bib21)) | 2020 | T&U | $\blacksquare$
    | Decision | $L_{2}$,$L_{\infty}$ | Efficient | 60.00% | Grandient+BA |'
  prefs: []
  type: TYPE_TB
- en: '| SignOPT (Cheng et al., [2020](#bib.bib26)) | 2020 | T&U | $\blacksquare$
    | Decision | $L_{2}$ | Efficient | 94.00% | Estimate gradient sign |'
  prefs: []
  type: TYPE_TB
- en: '| RayS (Chen and Gu, [2020](#bib.bib20)) | 2020 | U | $\blacksquare$ | Decision
    | $L_{\infty}$ | Efficient | 99.80% | Early stop |'
  prefs: []
  type: TYPE_TB
- en: '| QAIR(Li et al., [2021](#bib.bib70)) | 2021 | U | $\blacksquare$ | Decision
    | $L_{\infty}$ | Efficient | 98.00% | Image retrieval |'
  prefs: []
  type: TYPE_TB
- en: '| BayesAttack (Shukla et al., [2021](#bib.bib124)) | 2021 | T&U | $\blacksquare$
    | Decision | $L_{2}$,$L_{\infty}$ | Efficient | 67.48% | Bayesian optimization
    |'
  prefs: []
  type: TYPE_TB
- en: '| Surfree (Maho et al., [2021](#bib.bib88)) | 2021 | T&U | $\blacksquare$ |
    Geometric | $L_{2}$ | Efficient | 90.00% | Orthogonal projection |'
  prefs: []
  type: TYPE_TB
- en: '| TangentAttack (Ma et al., [2021](#bib.bib84)) | 2021 | T&U | $\blacksquare$
    | Geometric | $L_{2}$ | Efficient | - | Semi-ellipsoid |'
  prefs: []
  type: TYPE_TB
- en: '| TriangletAttack (Ma et al., [2021](#bib.bib84)) | 2022 | T&U | $\blacksquare$
    | Geometric | $L_{2}$ | Efficient | 44.5% | Triangle inequality |'
  prefs: []
  type: TYPE_TB
- en: '| ZOO(Chen et al., [2017](#bib.bib23)) | 2017 | T | $\blacksquare$ | Score
    | $L_{2}$ | Costly | 97.00% | Finite difference |'
  prefs: []
  type: TYPE_TB
- en: '| LocSearchAdv(Narodytska and Kasiviswanathan, [2017](#bib.bib96)) | 2017 |
    T&U | $\blacksquare$, $\color[rgb]{.5,.5,.5}\blacksquare$ | Score | $L_{0}$ |
    Costly | 70.78% | Local Search |'
  prefs: []
  type: TYPE_TB
- en: '| Autozoom(Tu et al., [2019](#bib.bib134)) | 2018 | T | $\blacksquare$ | Score
    | $-$ | Efficient | 93.00% | Autoencoder |'
  prefs: []
  type: TYPE_TB
- en: '| PCA(Bhagoji et al., [2018](#bib.bib10)) | 2018 | U | $\blacksquare$ | Score
    | $L_{\infty}$ | Costly | 89.50% | Principal component |'
  prefs: []
  type: TYPE_TB
- en: '| SimBA(Guo et al., [2019a](#bib.bib43)) | 2019 | T&U | $\blacksquare$ | Score
    | $L_{2}$ | Medium | 96.50% | Orthonormal basis |'
  prefs: []
  type: TYPE_TB
- en: '| CornerSearch(Croce and Hein, [2019](#bib.bib30)) | 2019 | U | $\blacksquare$
    | Score | $L_{0}$ | Costly | 99.56% | Sparse noises |'
  prefs: []
  type: TYPE_TB
- en: '| OnePixel(Su et al., [2019](#bib.bib126)) | 2019 | T&U | $\blacksquare$, $\color[rgb]{.5,.5,.5}\blacksquare$
    | Score | $L_{0}$ | Costly | 67.97% | Differential evolution |'
  prefs: []
  type: TYPE_TB
- en: '| PBBA(Moon et al., [2019](#bib.bib91)) | 2019 | T&U | $\blacksquare$ | Score
    | $L_{\infty}$ | Medium | 99.90% | Discrete surrogate |'
  prefs: []
  type: TYPE_TB
- en: '| Nattack (Li et al., [2019](#bib.bib71)) | 2019 | U | $\blacksquare$ | Score
    | $L_{2}$,$L_{\infty}$ | Costly | 100.00% | Adversarial distribution |'
  prefs: []
  type: TYPE_TB
- en: '| Bandis (Ilyas et al., [2018a](#bib.bib57)) | 2019 | U | $\blacksquare$ |
    Score | $L_{2}$,$L_{\infty}$ | Medium | 95.40% | Gradient priors |'
  prefs: []
  type: TYPE_TB
- en: '| SignHunter(Al-Dujaili and O’Reilly, [2019](#bib.bib3)) | 2020 | T&U | $\blacksquare$
    | Score | $L_{\infty}$,$L_{2}$ | Efficient | 91.47% | Grad. sign estimator |'
  prefs: []
  type: TYPE_TB
- en: '| Square attack (Andriushchenko et al., [2020](#bib.bib5)) | 2020 | T&U | $\blacksquare$
    | Score | $L_{2}$,$L_{\infty}$ | Efficient | 99.40% | Shrinking squares |'
  prefs: []
  type: TYPE_TB
- en: '| Sparse-RS (Croce et al., [2022](#bib.bib29)) | 2022 | T&U | $\blacksquare$
    | Score | $L_{\infty}$ | Costly | 95.80% | Random search |'
  prefs: []
  type: TYPE_TB
- en: '| NES(Ilyas et al., [2018b](#bib.bib56)) | 2018 | T&U | $\blacksquare$ | Score/Decision
    | $L_{\infty}$ | Costly | 88.20% | Natural evolution |'
  prefs: []
  type: TYPE_TB
- en: '| Subspace(Guo et al., [2019b](#bib.bib45)) | 2019 | U | $\blacksquare$ | Subs.+Score
    | $L_{\infty}$ | Costly | 96.60% | Multi-subs. model |'
  prefs: []
  type: TYPE_TB
- en: '| P-RGF(Cheng et al., [2019a](#bib.bib28)) | 2019 | U | $\blacksquare$ | Subs.+Score
    | $L_{2}$ | Medium | 99.10% | Transfer-based priors |'
  prefs: []
  type: TYPE_TB
- en: '| SimBA++(Yang et al., [2020](#bib.bib156)) | 2020 | U | $\blacksquare$ | Subs.+Score
    | $L_{2}$ | Efficient | 99.40% | Mixed method |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* This column contains different attacks’ adversarial knowledge. $\square$:
    white-box. $\blacksquare$: black-box. ${\color[rgb]{.5,.5,.5}\blacksquare}$: gray-box.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '** For query-based attacks, we use efficiency to evaluate the queries needed
    to find the adversarial example.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*** ASR is the abbr. of attack success rate. As mentioned, we only count the
    best result of the hardest attack reported in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.3.2\. Categories of black-box 2D digital adversarial attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different settings result in various methods. This review classifies black-box
    attacks into three different categories according to their settings and corresponding
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Substitute-model-based attacks. This method utilizes the transferability of
    adversarial examples, which can convert black-box problems into white-box problems,
    therefore significantly improving efficiency. However, limited by the transferability
    of the surrogate model, the attack success rate is usually not very high.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Score-based query attacks. We classify query-based attacks into score-based
    and decision-based. The continuous confidence score can be accessed for score-based
    attacks to estimate the decision boundary or gradient.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision-based query attacks. These attacks suppose the adversary can only access
    the predicted labels of the model. They are far more difficult than score-based
    attacks because less information can be leveraged.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometry-based query attacks. These attacks also aim at hard-label settings.
    However, they use geometric information of the decision boundary rather than estimate
    the gradients to improve the query efficiency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative-model-based attacks. GAN can also be used for black-box attacks.
    For example, in AdvGAN (Xiao et al., [2018a](#bib.bib150)), the classifier is
    displaced by a distilled model, which can be viewed as a transfer-based attack.
    In GNAE (Zhao et al., [2018](#bib.bib171)), random search algorithms are operated
    in the $z$ latent space, which can be regarded as a query-based attack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Some attacks also combine different methods together to improve the efficiency
    and attack success rate, such as SimBA++ (Yang et al., [2020](#bib.bib156)), P-RGF
    (Cheng et al., [2019a](#bib.bib28)), and Subspace attacks (Guo et al., [2019b](#bib.bib45)).
    Table.[3](#S5.T3 "Table 3 ‣ 5.3.1\. Different black-box scenarios ‣ 5.3\. Black-box
    adversarial attack for 2D deep learning models ‣ 5\. Adversarial attacks for 2D
    deep learning models ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning
    Models Against Adversarial Attacks") shows some representative 2D black-box adversarial
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3\. Substitute-model-based black-box attacks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The substitute-model-based attack is based on the transferability of adversarial
    examples. Therefore, improving the transferability is crucial for this kind of
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation and transformation are widely used for improving transferability.
    TML (Papernot et al., [2016a](#bib.bib100)) used Jacobian-based dataset augmentation
    to expand the dataset and train the substitute model alternatively, which can
    be formulated as $\mathcal{D}_{\rho+1}=(x+\lambda_{\rho}\text{sign}(J_{\hat{\mathcal{F}}}(x:x\in\mathcal{D}_{\rho})))\cup\mathcal{D}_{\rho}$,
    where $\hat{\mathcal{F}}$ is the surrogate model. $\mathcal{D}_{\rho}$ and $\mathcal{D}_{\rho+1}$
    is the training dataset before and after $\rho^{th}$ iteration, $\lambda_{\rho}$
    is a periodical step size. In addition, they introduced reservoir sampling into
    the data augmentation process to reduce the iteration number. DI (Xie et al.,
    [2019](#bib.bib155)) promoted transferability through randomly resizing and padding
    the dataset. TIMI (Dong et al., [2019](#bib.bib33)) improved the transferability
    of MI-FGSM by enriching the input varieties, which is equivalent to multiplying
    the gradient with a smoothing kernel, such as a rotation, transition, or rescale
    matrix. SIM (Lin et al., [2019](#bib.bib73)) used Nesterov accelerated gradient
    and resized the image into different scales to prevent the local model from overfitting.
    RDI (Zou et al., [2020](#bib.bib179)) improved DI and TIMI through multi-scale
    gradient and region fitting.
  prefs: []
  type: TYPE_NORMAL
- en: Targeted attacks are more difficult than untargeted attacks for surrogate-model-based
    methods. Zhao et al. (Zhao et al., [2021b](#bib.bib173)) found that a simple targeted
    function $Z_{y^{\prime}}(F(x^{adv}))$ with sufficient iterations can produce SOTA
    results, where $Z_{y^{\prime}}$ is the output logit of target label. Byun et al.
    (Byun et al., [2022](#bib.bib15)) improved the transferability of the targeted
    attack through object-based diverse input (ODI), which utilized a differentiable
    3D renderer to render the 2D adversarial examples on 3D objects. Noticing that
    the gradient of MI-FGSM tends to be dominated by the past gradients after a few
    updates, Li et al. (Li et al., [2020](#bib.bib69)) proposed TTTA that adaptively
    adjusts the gradient through Pointcare distance. Through Pointcare distance, the
    gradient increases when and only when the $f(x)$ comes closer to the target class.
  prefs: []
  type: TYPE_NORMAL
- en: Some works found that manipulating the surrogate model’s latent layer features
    can alleviate overfitting and utilize the shared features, resulting in transferability
    improvement. Intermediate level attack (ILA) (Huang et al., [2019](#bib.bib55))
    improved C&W and FGSM attacks by maximizing the distortion of a pre-specified
    layer, while Feature disruptive attack (FDA) (Ganeshan et al., [2019](#bib.bib38))
    disrupted all latent layer’s output. However, these two attacks regard all neurons
    as equally important. The neurons that negatively influence the ground truth prediction
    should be amplified rather than suppressed. FIA (Wang et al., [2021a](#bib.bib143))
    judged the neuron importance by the mean gradient of a set of transformed images.
    But this method suffers from gradient saturation. To solve this, NAA (Zhang et al.,
    [2022b](#bib.bib164)) quantified neuron importance by computing neuron attribution.
    Besides image classification tasks, Img2video attack (Wei et al., [2022](#bib.bib144))
    attacked the black-box video model by generating the AE of each frame on a surrogate
    model. They maximize the distance of low-level features between normal and adversarial
    video frames.
  prefs: []
  type: TYPE_NORMAL
- en: Some attacks combine the surrogate model and other methods to get better performance.
    P-RGF attack (Cheng et al., [2019a](#bib.bib28)) utilized transfer-based priors
    to get more accurate gradient estimation and save queries. They formulated the
    gradient estimation problem as $\min_{\hat{g}}L(\hat{g})=\mathbb{E}\left\|\bigtriangledown_{x}f(x)-b\hat{g}\right\|_{2}^{2}$,
    where $b$ is a scaling factor. They utilized the estimated cosine similarity between
    the actual and transferred gradient to control the transfer strength. Their method
    can also be incorporated with other priors to save queries. Subspace attack (Guo
    et al., [2019b](#bib.bib45)) shrank the search space of stochastic vectors by
    transferring the gradients from a group of substitute networks. SimBA++ (Yang
    et al., [2020](#bib.bib156)) also integrated transferability-based and queries-based
    model. Moreover, SimBA++ updated the surrogate model based on the query results.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4\. Score-based query attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of query-based attacks is to craft adversarial examples through fewer
    queries and achieve a higher attack success rate. Based on whether the continuous
    confidence score is accessible,query-based methods can be classified into score-based
    and decision-based query methods.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-estimation-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most of the score-based query attacks are based on gradient estimation. The
    simplest gradient estimation method is the finite difference, such as zero-order
    optimization attack (ZOO) (Chen et al., [2017](#bib.bib23)). For ZOO, the gradient
    is estimated through $\frac{\partial f(x)}{\partial x_{i}}\approx\frac{f(x+\delta
    e_{i})-f(x-\delta e_{i})}{2\delta}$, where $e_{i}$ is a standard basis, $\delta$
    is a random vector sampled from Gaussian distribution near the original data point.
    Moreover, they used a resize function and increased dimension gradually to improve
    efficiency. However, because they need to use all the standard basis vectors in
    each iteration, the query cost is proportional to the image size, which is computationally
    expensive. Later, some improvements were proposed to reduce the random vectors
    to be sampled. PCA attack (Bhagoji et al., [2018](#bib.bib10)) reduced search
    dimensions by analyzing the principal components of input data. Autozoom (Tu et al.,
    [2019](#bib.bib134)) firstly trained an Autoencoder to encode the image of the
    target class into latent space. Then, it sampled the random vectors from the latent
    space and mapped them to random perturbations with reduced dimensions to compute
    the finite difference. Al-Dujaili et al. (Al-Dujaili and O’Reilly, [2019](#bib.bib3))
    proposed a sign-based gradient estimation algorithm. The basic idea is to estimate
    the sign of the directional derivative. They proposed a divide and conquer method
    named SignHunter that leverages the separability of the derivative of the adversarial
    objective function, which decreases the query number from $2^{n}$ to $O(n)$.
  prefs: []
  type: TYPE_NORMAL
- en: Heuristic algorithms can also be used for gradient estimation. Ilylas et al.
    (Ilyas et al., [2018b](#bib.bib56)) estimated the gradient through a variant of
    natural evolution strategy (NES). For scoreless settings, they leveraged noise
    robustness to substitute the confidence score. Specifically, they sampled a set
    of $\delta_{i}$ from a normal distribution by antithetic sampling. Then, the expectation
    of estimated gradient of $F(x^{\prime})$ approximately equals to $\frac{1}{\sigma
    n}\sum_{i=1}^{n}\delta_{i}F(x^{\prime}+\sigma\delta_{i})$. Later, Ilylas et al.
    (Ilyas et al., [2018a](#bib.bib57)) formulated the gradient estimation problem
    as finding a vector to maximize $\mathbb{E}[\hat{g}^{T}g^{*}]$ and used the least
    squares method to solve this problem and proved it is an equivalence of NES. They
    utilized two kinds of priors to improve efficiency. Firstly, the present gradient
    highly correlates with the gradient of the last step. Secondly, adjacent pixels
    often have similar gradients. Therefore, they designed a gradient estimation framework
    based on bandit optimization, where the action is a priors-based gradient estimation
    in each round.
  prefs: []
  type: TYPE_NORMAL
- en: Greedy-algorithm-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SimBA (Guo et al., [2019a](#bib.bib43)) is a simple but powerful attack based
    on the greedy algorithm. Instead of computing all basis vectors at each iteration
    (like ZOO attack), it randomly chooses one vector from a predefined orthogonal
    basis at each iteration. If this vector can reduce the confidence score, it is
    applied to the image, and the next random vector is sampled. Otherwise, this direction
    is deserted. They found that the DCT basis is especially efficient. Although this
    method is simple, it outperforms NES and ZOO in the aspect of query cost. SimBA++
    (Yang et al., [2020](#bib.bib156)) improved the query efficiency by combining
    SimBA with the transferability-based attack TIMI (Dong et al., [2019](#bib.bib33)).
    Instead of randomly sampling changing direction, they correlate the sampling probability
    to the surrogate model’s gradient. Moreover, they proposed high-order gradient
    approximation (HOGA) to distill the target model in both forward and backward
    steps. SimBA++ greatly decreases the query times and achieves a higher attack
    success rate than SimBA.
  prefs: []
  type: TYPE_NORMAL
- en: Grid-search-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $\mathcal{N}$attack (Li et al., [2019](#bib.bib71)) tries to find a distribution
    of AEs in the neighborhood of $x$. But it does not need to access the model parameters.
    It formulates the optimization problem of finding adversarial distribution around
    $x$ as $\min_{\theta}J(\theta):=\int{f(x^{\prime})\pi_{S}(x^{\prime}|\theta)dx^{\prime}}$,
    where $f(x^{\prime})$ is an untargeted attack loss function, $\theta=(\mu,\sigma^{2})$
    are the parameters of Gaussian distribution and $x^{\prime}=x+\text{proj}_{S}(1/2(tanh(z)+1))$,
    where $z\sim\mathcal{N}(z|\theta)$. To solve this problem, it uses grid search
    to find the best $\sigma$ and updates $\mu$ through NES. PBBA (Moon et al., [2019](#bib.bib91))
    searched AEs along the border of $l_{\infty}$ ball. This simplification makes
    it possible to refine the query performance by optimizing discrete surrogate problems.
    It firstly zones the images as squares and operates regional optimization in the
    grid by choosing $x^{\prime}$ from $\{x-\epsilon,x+\epsilon\}$. Then, it adjusts
    the squares and duplicates this procedure until it finds an AE. Square attack
    reduced the search space through diminishing squares. Unlike PBBA (Moon et al.,
    [2019](#bib.bib91)) that used a fixed grid, the locations of squares are optimized.
    Moreover, they sampled $\delta$ along the boundary of $L_{p}$-norm ball to improve
    the query efficiency. Specifically speaking, for $L_{\infty}$-square attack, $x_{i}^{\prime}\in\{x_{i}-\epsilon,x_{i}+\epsilon\}$.
    For $L_{2}$-square attack, because noise values are correlated, they increase
    the budget of one window and simultaneously decrease the budget of another.
  prefs: []
  type: TYPE_NORMAL
- en: Dimension-reduction-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some works reduced the search dimensions by reducing the number of crafted pixels.
    The first work is LocSearchAdv (Narodytska and Kasiviswanathan, [2017](#bib.bib96)),
    which used local search to estimate implicit gradient. It first randomly picked
    up some pixels as initial points. At each round, the adjacent pixels of these
    points are checked and updated. It only needed to manipulate 0.5% pixels for the
    untargeted attack on ImageNet. CornerSearch (Croce and Hein, [2019](#bib.bib30))
    also used local search to find sparse perturbations, but it only added perturbations
    to pixels with high variation to make them imperceptible. OnePixel attack (Su
    et al., [2019](#bib.bib126)) considered the extreme situation and showed the possibility
    of fooling the classifier by only revising one pixel. It used differential evolution
    to optimize this problem. Each candidate solution contains its pixel coordinates
    and color values. The candidate solutions are updated by random crossover. Sparse-RS
    (Croce et al., [2022](#bib.bib29)) utilized random search, which is appropriate
    for zero-order optimization problems with sparse restraint. It achieved the SOTA
    attack success rate on multiple attacks, including the $l_{0}$-norm noises, adversarial
    patches, and adversarial borders.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.5\. Decision-based query attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In some scenarios, the adversary can only get the final decision ( hard label)
    rather than the confidence score (soft label). This is a more challenging case
    because the gradient value is hard to estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-estimation-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Some works proposed to estimate the gradient directions instead. The first
    work is Boundary attack (Brendel et al., [2017](#bib.bib12)), which first initializes
    a point outside the ground truth region and then uses a random walk algorithm
    to make it closer to the boundary at each iteration. Although it can realize almost
    the same perturbation level compared with white-box attacks, it needs exponential
    time to find adversarial examples. Biased boundary attack (Brunner et al., [2019](#bib.bib14))
    used bias sampling to re-understand and improve boundary attacks. Three different
    biases are utilized. The first bias is sampling more vectors in the direction
    of low-frequency distortion. The second bias is sampling denser data from regions
    where the adversarial and benign images have larger differences. The last bias
    is the prior-based gradient from the substitute model. HSJA (Chen et al., [2020](#bib.bib21))
    includes three steps: binary search to find the boundary point, estimation of
    gradient direction, and geometric progression to update the boundary point. Because
    the gradient guides the search direction, HSJA significantly decreases query number
    compared to the Boundary attack.'
  prefs: []
  type: TYPE_NORMAL
- en: OPT (Cheng et al., [2019b](#bib.bib27)) is another representative hard-label-based
    attack, which estimated the shortest distance of the benign sample from the boundary
    by fine-grained search and binary search. They estimated the gradient of the search
    direction through the randomized gradient-free method and updated the search direction
    through gradient descent. SignOPT (Cheng et al., [2020](#bib.bib26)) promoted
    OPT’s efficiency by only estimating the sign of gradient over search direction
    by just one query and averaging the gradient sign over a group of random directions.
    RayS (Chen and Gu, [2020](#bib.bib20)) reframed the successive problem of finding
    the nearest classification hyperplane as a discontinuous problem without gradient
    estimation, which searches over a group of ray orientations. Moreover, all unnecessary
    directions are terminated early through a quick check. This greatly promoted search
    efficiency. Bayes attacks (Shukla et al., [2021](#bib.bib124)) reduced query budgets
    to 1000 through Bayesian optimization. However, because Bayesian optimization
    is not suitable for search space with large dimensions, they reduce the search
    dimension through FFT and use nearest-neighbor upsampling to find adversarial
    examples. Their method significantly reduced the query times compared with OPT
    and Sign-OPT attacks. QAIR (Li et al., [2021](#bib.bib70)) considers a more difficult
    setting where the attacker can only get top-k unlabeled images from the target
    model in the image retrieval task. They proposed a correlation-based loss by the
    difference of top-k unlabeled feedback retrieved by benign and adversarial images.
    Moreover, they improved the efficiency through recursive network theft to get
    gradient priors.
  prefs: []
  type: TYPE_NORMAL
- en: Geometry-based methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Geometry attacks also aim at the hard-label scenario. However, they directly
    use geometric information of the decision boundary rather than estimate the gradients
    to improve efficiency. Surfree (Maho et al., [2021](#bib.bib88)) assumes the boundary
    is a hyperplane and iteratively runs binary searches over orthonormal directions
    to find the clean image’s projection on the boundary. Compared to HSJA (Chen et al.,
    [2020](#bib.bib21)), the new direction is randomly sampled from the low-frequency
    subband generated by DCT rather than using gradient directions, which reduces
    the queries to a few hundred. TangentAttack (Ma et al., [2021](#bib.bib84)) assumed
    the boundary is a semi-ellipsoid and adjusted the search direction to follow the
    optimal tangent line rather than the gradient directions, which need less distortion
    than HSJA. TriangleAttack (Wang et al., [2022b](#bib.bib140)) utilized triangle
    inequality to search the boundary point and also used DCT for dimension reduction.
    Less than a thousand quires are required in this attack. CGBA (Reza et al., [2023](#bib.bib110))
    restrict the search on a semicircular path to make sure to find a boundary point
    regardless of the boundary curvature, which is quite efficient for untargeted
    attack..
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.6\. Generative-model-based black-box attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GAN can also be used for black-box attacks. GNAE (Zhao et al., [2018](#bib.bib171))
    generates natural AEs by searching for perturbations in the latent space rather
    than the input space. It first learns a generator $\mathcal{G}_{\theta}$ that
    maps normally distributed variables $z$ to the input $x$. It then learns an inverter
    $\mathcal{I}_{\gamma}$ that maps $x$ back to latent space by minimizing the divergence
    between $z$ and reconstructed $\mathcal{I}_{\gamma}(x)$. Their objective is $\min_{\tilde{z}}\left\|\tilde{z}-\mathcal{I}_{\gamma}(x)\right\|_{2}\;s.t.\;\mathcal{F}(\mathcal{G}_{\theta}(\tilde{z}))\neq\mathcal{F}(x)$.
    The feasible $\tilde{z}$ is found through iterative stochastic search and hybrid
    shrinking search.
  prefs: []
  type: TYPE_NORMAL
- en: Xiao et al. (Xiao et al., [2018a](#bib.bib150)) proposed AdvGAN to improve the
    attack transferability. AdvGAN consists of a perturbation generator, a discriminator,
    and a classifier. The generator is trained to generate perturbations from the
    original instance. The discriminator is trained to distinguish AEs from clean
    instances. The classifier is a distilled model of the target black-box network.
    They alternately train these modules by first training the GAN and then training
    the classifier through $\min_{f}\mathbb{E}_{x\sim D_{data}}\mathcal{L}(f(x),b(x))+\mathcal{L}(f(x+\mathcal{G}(x)),b(x+\mathcal{G}(x)))$
    , where $b(x)$ is the target model, and $f(x)$ is the distilled model. $\mathcal{L}$
    is a cross-entropy loss function. Their black-box attack achieved 92.7% on the
    MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Some previous transfer-based attacks improved transferability through simple
    transformations, like rotation and translation (Dong et al., [2019](#bib.bib33)),
    random resizing and padding (Xie et al., [2019](#bib.bib155)) and multi-scaling
    (Lin et al., [2019](#bib.bib73)). Wu et al. (Wu et al., [2021](#bib.bib149)) proposed
    ATTA to improve transferability through learning adversarial transformations that
    can better alleviate the adversarial property. They formulate this problem as
    a min-max problem. The inner problem is to find $x_{adv}$ that maximizes the classification
    loss. The outer problem is to learn a transformation through CNN to minimize the
    classification loss.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. The safety impact of physical-realizable 2D adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 4\. Summary of main works that examine the safety of real-world 2D CV
    applications through physical adversarial attacks
  prefs: []
  type: TYPE_NORMAL
- en: '| Attacks | Year | Threat Model | Algorithm | Distance | Performance | Scenario
    |'
  prefs: []
  type: TYPE_TB
- en: '| Goal | Knowl.^* | Interface | Camouflage | Succ. ^(**) |'
  prefs: []
  type: TYPE_TB
- en: '| UAP-patch (Liu et al., [2020a](#bib.bib74)) | 2020 | U | $\square$ | Patch
    | Gradient | Style loss | Marked | 74.1% | Auto check-Out |'
  prefs: []
  type: TYPE_TB
- en: '| Facial Acc. (Sharif et al., [2016](#bib.bib120)) | 2016 | T&U | $\square$
    | Patch | Gradient | $L_{\infty}$ | Marked | 80.0% | Face rcg. |'
  prefs: []
  type: TYPE_TB
- en: '| AGN(Sharif et al., [2019](#bib.bib121)) | 2019 | T&U | $\square$ | Patch
    | GAN | $-$ | Slight | 70.0% | Face rcg. |'
  prefs: []
  type: TYPE_TB
- en: '| FRSadv(Nguyen et al., [2020](#bib.bib97)) | 2020 | T | $\square$ | Light
    | Gradient | $L_{\infty}$ | Marked | 92.0% | Face rcg. |'
  prefs: []
  type: TYPE_TB
- en: '| GenAP (Xiao et al., [2021](#bib.bib153)) | 2021 | T&U | $\blacksquare$ |
    Patch | GAN | $L_{\infty}$ | Marked | 65.0% | Face rcg. |'
  prefs: []
  type: TYPE_TB
- en: '| AdvMakeUp (Yin et al., [2021](#bib.bib157)) | 2021 | T | $\square$ | Patch
    | Gradient | Style, Content | Slight | 22.0% | Face rcg. |'
  prefs: []
  type: TYPE_TB
- en: '| AdvPatch (Brown et al., [2017](#bib.bib13)) | 2017 | T | $\square$ | Patch
    | Gradient | $L_{\infty}$ | Marked | 93.0% | Img clas. |'
  prefs: []
  type: TYPE_TB
- en: '| EoT(Athalye et al., [2018b](#bib.bib7)) | 2018 | T | $\square$ | Patch |
    Gradient | $L_{2}$ | Slight | 82.0% | Img clas. |'
  prefs: []
  type: TYPE_TB
- en: '| CiPer(Agarwal et al., [2020](#bib.bib2)) | 2020 | U | $\blacksquare$ | Sensor
    | Greedy | $L_{\infty}$ | Marked | 33.0% | Img clas. |'
  prefs: []
  type: TYPE_TB
- en: '| Nat-Patch (Hu et al., [2021](#bib.bib52)) | 2021 | U | $\square$ | Patch
    | GAN | Smoth loss | Invisible | 48.0% | Object detect |'
  prefs: []
  type: TYPE_TB
- en: '| CAMOU (Zhang et al., [2018b](#bib.bib167)) | 2018 | U | $\blacksquare$ |
    Patch | Subs. Model | $-$ | Marked | 32.7% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| RP2 (Eykholt et al., [2018](#bib.bib36)) | 2018 | T | $\square$ | Patch |
    Gradient | $L_{1},L_{2}$ | Marked | 84.8% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeShifter (Chen et al., [2018a](#bib.bib24)) | 2018 | T&U | $\square$
    | Patch | Gradient | $L_{2}$ | Marked | 87.0% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| MeshAdv (Xiao et al., [2019](#bib.bib151)) | 2019 | T&U | $\square$ | Renderer
    | Gradient | $Lp.$ | Slight | 100.0% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| AdvCam(Duan et al., [2020](#bib.bib34)) | 2020 | T&U | $\square$ | Patch
    | Gradient | Style loss | Slight | 80.0% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| DAS (Wang et al., [2021b](#bib.bib138)) | 2021 | U | $\square$ | Patch |
    Gradient | $L_{2},T.V.$ | Marked | 20.0% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| AMPLE (Ji et al., [2021](#bib.bib59)) | 2021 | T&U | $\blacksquare$ | Sensor
    | Bayesian | Energy loss | Marked | 87.9% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| DirtyRoad (Sato et al., [2021](#bib.bib113)) | 2021 | U | $\square$ | Patch
    | Gradient | $L_{1},L_{2},L_{\infty}$ | Invisible | 97.5% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| OPAD(Gnanasambandam et al., [2021](#bib.bib41)) | 2021 | T | $\square$ |
    Light | Gradient | $L_{2},L_{\infty}$ | Slight | 48.0% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| DTA (Suryanto et al., [2022](#bib.bib128)) | 2022 | T | $\square$ | Patch
    | Gradient | $-$ | Marked | 63.0% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| TrajAdv (Zhang et al., [2022a](#bib.bib165)) | 2022 | U | $\square$$\blacksquare$
    | Patch | Gradient,PSO | $L_{\infty}$ | Slight | 62.2% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: '| FCA(Wang et al., [2022a](#bib.bib137)) | 2022 | U | $\square$ | Patch | Gradient
    | Smooth loss | Marked | 60.0% | Self-driving |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* This column is the adversarial knowledge of different attacks. $\square$:
    white-box. $\blacksquare$: black-box. ${\color[rgb]{.5,.5,.5}\blacksquare}$: gray-box.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '** For objector detection, the attack success rate means the average rate of
    escaping from being detected. As mentioned, we only count the best result of the
    hardest attack reported in the paper.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When deep learning models are employed in the real world, ensuring the safety
    of human life and property is the primary concern. A number of studies have investigated
    adversarial attacks in safety-critical applications, such as face recognition
    and self-driving systems. referred to as physical adversarial attack. This subsection
    reviews 2D physical adversarial attacks in safety-critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge of physical adversarial attacks is the ever-changing physical
    environment, such as the background light noise, varying viewpoints, and different
    distances. Moreover, because the adversary cannot directly modify the input images
    at the pixel level, certain mediums are needed to pollute the data obliquely,
    such as patches (Sharif et al., [2016](#bib.bib120); Xiao et al., [2021](#bib.bib153)),
    illumination (Gnanasambandam et al., [2021](#bib.bib41); Li et al., [2023](#bib.bib72)),
    and sensors (Sayles et al., [2021](#bib.bib114)). The patch-based attack is the
    most common physical attack. It uses printed patches to cover the whole or part
    of the target object to spoof the classifier. Some works also render 2D adversarial
    images on 3D objects, like turtles (Athalye et al., [2018b](#bib.bib7)) and cars
    (Wang et al., [2021b](#bib.bib138)). This can be seen as a more general form of
    adversarial patches.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1\. The safety of face recognition system
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned before, a major challenge for the 2D physical-world adversarial
    attack is the changing environment, like varying distances and ambient light.
    To make the AE more robust to these changes and can fool the classifier consistently,
    Athalye et al. (Athalye et al., [2018b](#bib.bib7)) proposed expectation on transformation
    (EoT) that calculates the expectation of the log-likelihood of the target class
    on transformed images, which is formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $x^{adv}=arg\min_{\Delta x}\Sigma^{k}_{i=1}(w_{i}\mathcal{L}(\mathcal{T}_{i}(x)+\Delta
    x,y)),\;s.t.\&#124;\Delta x\&#124;_{p}\leq\epsilon,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{T}_{i}$ is the $i_{th}$ transformation, such as random rotations
    and transitions, $w_{i}$ is the weight and subject to $\Sigma_{i=1}^{k}w_{i}=1$.
    Besides rotation and transition, they also regarded the 3D rendering as a transformation.
    They use the $l_{2}$ norm in the LAB color space as the distance metric because
    it is closer to human perception. Brown et al. (Brown et al., [2017](#bib.bib13))
    inducted EOT, patch position, and multiple training images into the optimization
    round to generate targeted, universal, and robust adversarial patterns. However,
    their method has poor transferability when the pattern size is small.
  prefs: []
  type: TYPE_NORMAL
- en: In 2016, Sharif et al. (Sharif et al., [2016](#bib.bib120)) proposed facial
    accessory attack. The perturbation is limited to the area of facial accessories
    like eyeglass frames to make the attack more reasonable. They also use EOT to
    improve the physical attack’s robustness. Moreover, they minimize the total variation
    of the patches to make them more natural and use the NPS score to guarantee the
    patch is printable. The total loss function is $arg\min_{\delta}\sum_{x\in X}L_{s}(\mathcal{F}(x+\delta),l)+k_{1}TV(\delta)+k_{2}NPS(\delta)$,
    where $L_{s}$ is a Softmax loss on the target label. $TV$ is the total variance
    loss. $NPS$ is the non-printable score that defined as $NPS(\hat{p})=\prod_{p\in
    P}|\hat{p}-p|$, where $P$ is the set of printable colors. In 2019, Sharif et al.
    proposed AGN attack (Sharif et al., [2019](#bib.bib121)), which improved the inconspicuousness
    of adversarial patches through a generative framework to generate AEs with multiple
    objectives, such as robustness, imperceptibility, and scalability. Unlike previous
    work that uses total variation as smoothness loss, they use GAN to ensure the
    generated patches look like real-world designs.
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, FRSadv (Nguyen et al., [2020](#bib.bib97)) attacked the face recognition
    system through adversarial illumination. They project adversarial patterns onto
    the human face, and the camera captures the images and predicts wrong labels.
    They also used EOT to improve the robustness. Adv-Makeup (Yin et al., [2021](#bib.bib157))
    fooled the FRS through adversarial makeup. Their attack first generates realistic
    eye shadow through GAN, then blends the generated eye shadow onto the source image
    through the gradient, content, and style losses. Adv-Makeup attacks an ensemble
    of models to improve transferability. FaceAdv (Shen et al., [2021](#bib.bib122))
    has a sticker generator and a converter. The generator chooses the most vulnerable
    area to attack, and the converter renders the patches to the face with different
    angles and sizes to improve physical robustness.
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, Xiao et al. (Xiao et al., [2021](#bib.bib153)) proposed the GenAP attack,
    which regularizes the patches on the latent space of GAN to make the adversarial
    patches more natural and transferable. They first trained a StyleGAN on normal
    face datasets, then used it to generate AEs and crop them into the patch area.
    Instead of optimizing the image directly, they optimized the latent variable $w$
    on the $\mathcal{W}+$ space of StyleGAN to minimize the feature distance between
    $x^{adv}$ and the target $x_{t}$. Experiments show that their methods can improve
    the adversarial pattern’s transferability. However, their method needs a pre-trained
    generator trained on the victim examples. Moreover, their methods are difficult
    to converge, which may be because the cropping function unexpectedly changes the
    AEs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2\. The safety of self-driving system
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/32541c9566ab22dd5265af87030cbaa9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. The general procedure of generating robust physical perturbation
    through expectation over transformations (Eykholt et al., [2018](#bib.bib36);
    Athalye et al., [2018b](#bib.bib7); Brown et al., [2017](#bib.bib13); Sharif et al.,
    [2016](#bib.bib120)). Images are sampled from different distances and view angles
    to improve the real-world attack’s robustness against position changes.
  prefs: []
  type: TYPE_NORMAL
- en: A relatively simple setting for adversarial attacks in self-driving is to hide
    planar objects from the detector or classifier, like traffic signs and roads.
    Chen et al. (Chen et al., [2018a](#bib.bib24)) proposed ShapeShifter to attack
    the Fast R-CNN detector. They applied EoT to the objection detector field to improve
    the attack robustness. Rather than using the adversarial patch, they modify the
    background of the stop sign directly. Eykholt et al. (Eykholt et al., [2018](#bib.bib36))
    proposed robust physical perturbation (RP2) to make deep learning models misclassify
    the road sign. They improved EoT by sampling from a set of both simulated and
    physical distortions. The general procedure is shown in Figure.[5](#S5.F5 "Figure
    5 ‣ 5.4.2\. The safety of self-driving system ‣ 5.4\. The safety impact of physical-realizable
    2D adversarial attacks ‣ 5\. Adversarial attacks for 2D deep learning models ‣
    A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial
    Attacks"). They first operate $L_{1}$ optimization to find approximate positions
    of the adversarial patches and then operate $L_{2}$ optimization to modify their
    RGB values. AdvCam (Duan et al., [2020](#bib.bib34)) generated adversarial traffic
    signs with natural styles through neural style transfer. The loss function consists
    of an adversarial loss, a style loss, a content loss, and a smoothness loss. The
    style loss and content loss are calculated through the features extracted from
    the shallow and deep layers of the feature extraction neural network. Sato et
    al. (Sato et al., [2021](#bib.bib113)) attacked the automated lane centering system
    through dirty road patches, which may result in the car running out of the street.
    Their objective function is to make the car drive out of the lane in the shortest
    time. They achieved 97.5% ASR and only needed 0.903 sec for each attack. TrajAdv
    (Zhang et al., [2022a](#bib.bib165)) evaluated the robustness of the trajectory
    prediction of the self-driving car through single-frame or multi-frame adversarial
    perturbation. They considered the acceleration and speed limitation to make the
    adversarial perturbation more natural. OPAD (Gnanasambandam et al., [2021](#bib.bib41))
    cheated the traffic sign classifier by optical adversarial perturbation. To overcome
    the nonlinear effect of the projector, they estimated the radiometric and spectral
    response to rectify the distortion.
  prefs: []
  type: TYPE_NORMAL
- en: A more complex scenario is to hide nonplanar objects like cars from the detector.
    Because the 3D renderer is not differential, and the vehicle detector is usually
    a black box. CAMOU (Zhang et al., [2018b](#bib.bib167)) used a distilled model
    to learn the behavior of the 3D renderer and detector jointly. When camouflage
    is changed, the original distilled model may fail to fit the 3D transformations
    and vehicle detector. Therefore, they alternatively train the distilled model
    and optimize the adversarial patches. Some studies utilized a 3D differential
    renderer to map 3D adversarial mesh or camouflage to 2D images. Xiao et al. (Xiao
    et al., [2019](#bib.bib151)) proposed MeshAdv to hide cars from 2D detectors by
    printable 3D mesh. They leverage a differential renderer (Kato et al., [2018](#bib.bib63))
    to map 3D adversarial mesh to 2D images. Laplacian loss is applied on the mesh
    vertices to improve the smoothness. DAS (Wang et al., [2021b](#bib.bib138)) hides
    the car from the detector by simultaneously distracting the model and human attention.
    They optimize the model attention distraction loss function by minimizing the
    salient region and decreasing the salient region’s pixel values. To make the image
    unnoticeable to humans, they initial the adversarial camouflage as a natural image
    and then constrain the perturbation in its edge area. AMPLE (Ji et al., [2021](#bib.bib59))
    can hide or create a car through a sensor-injection attack by rotating and vibrating
    the camera to blur the image. It uses Bayesian methods to optimize the rotation
    and vibration parameters. To cheat the detector in multiple viewpoints, FCA (Wang
    et al., [2022a](#bib.bib137)) colored the adversarial camouflage onto the car’s
    whole surface through a differential renderer. The loss function includes three
    parts. The first part is to cut down the IoU between the original and predicted
    boxes. The second part is to decrease the objectness confidence. The third part
    is to lower the predicted logits of the target class. Suryato et al. (Suryanto
    et al., [2022](#bib.bib128)) found that the differential neural renderer that
    previous works used fails to perform diverse physical world conversions because
    of an absence of domination of environment variances. Therefore, they proposed
    DTA attack to hide the car from the detector, which utilized a differential transformation
    network to get photorealistic images.
  prefs: []
  type: TYPE_NORMAL
- en: Besides face recognition and self-driving scenario, the safety of other scenarios
    against adversarial attacks also are investigated. Liu et al. (Liu et al., [2020a](#bib.bib74))
    evaluated the safety of the automatic check-out system through universal adversarial
    patches. They leveraged perceptual and semantic biases of models to improve the
    generalization ability. Hu et al. (Hu et al., [2021](#bib.bib52)) leveraged StyleGAN
    and BigGAN to generate adversarial patches with natural-looking content to evade
    from person detector. They minimized the objectness score and classification confidence
    of the target class simultaneously. Agarwal et al. (Agarwal et al., [2020](#bib.bib2))
    leveraged the noise produced by the environment and imaging process to reduce
    the classification accuracy of black-box models. The noises are extracted through
    Gaussian and Laplacian filters.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Adversarial attacks for 3D deep learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1\. The difference between 3D and 2D adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a result of affordable 3D data acquisition devices and the rich information
    provided by the geometry feature, 3D data are widely used in many safe-critical
    tasks. As a result, their security also attracts more and more attention. Compared
    with 2D adversarial attacks, 3D adversarial attacks have some significant differences:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared with images, point clouds contain point-wise coordinates with unordered
    and irregular sampling values. Some 2D attacks that utilize the image structure
    cannot be directly applied to these kinds of data, such as the boundary attack,
    which estimates the boundary point by the weighted average of pixel value between
    source and target images. Interpolation between point clouds directly will deform
    the 3D shapes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D adversarial attacks have a larger disturbance space and degree of freedom
    than 2D adversarial attacks. In addition to modifying the value, the adversary
    can add and delete points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike 2D adversarial attacks, which use all pixels for classification, point
    clouds do not use all points. Due to the large number of points, the point cloud
    is usually sampled before the classification. For example, PointNet sampled 1024
    points for 3D object classification on the ModelNet40 dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most 2D adversarial attacks are based on pixel-wise $L_{p}$ distance. However,
    point-wise attacks can be easily defended by outlier point removal, and $L_{p}$
    distance is unsuitable for disordered data, like the point cloud.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Therefore, previous cannot be directly used for 3D adversarial attacks. To solve
    these problems, 3D adversarial attacks are proposed for 3D data specifically.
    We will introduce these attacks according to their algorithms and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Catalog of 3D adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We summarize the recent works of 3D digital and physical adversarial attacks
    in Table.[5](#S6.T5 "Table 5 ‣ 6.3.1\. Gradient-based method ‣ 6.3\. 3D adversarial
    attacks in the digital world ‣ 6\. Adversarial attacks for 3D deep learning models
    ‣ A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
    Adversarial Attacks"). According to the attack algorithms used for generating
    adversarial examples, we classify the 3D adversarial attacks into the following
    categories,
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-based optimization methods. For most 3D adversarial attacks, gradient-based
    methods are used. Some of them are based on $C\&amp;W$ loss, such as (Xiao et al.,
    [2018a](#bib.bib150)) and (Tsai et al., [2020](#bib.bib133)), while others are
    based on fast-gradient methods, such as (Liu et al., [2019](#bib.bib77)). Some
    works relax $L_{0}$-norm attack to differential versions, such as (Zheng et al.,
    [2019b](#bib.bib175)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative-model-based methods. Some works, like AdvPC (Hamdi et al., [2020](#bib.bib46))
    and LG-GAN (Zhou et al., [2020](#bib.bib176)), utilize a generative model to generate
    adversarial point clouds. These attacks can improve the transferability of 3D
    adversarial examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heuristic-algorithm-based methods. In black-box settings, the gradient of the
    3D model is unavailable. Hence, some black-box attacks generate adversarial examples
    using heuristic algorithms like evolution algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D-transformation-based methods. Rather than modifying the point cloud, some
    attacks found that 3D models are also vulnerable to rigid-body transformations
    in the 3D Euclidean space, like rotations and translations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Recently, some works also proposed 3D physical-realizable adversarial examples
    by 3D printing (Tsai et al., [2020](#bib.bib133)) or laser emitter (Cao et al.,
    [2019a](#bib.bib16)). In addition, some studies (Wen et al., [2020](#bib.bib145);
    Huang et al., [2022](#bib.bib54)) classified the 3D digital adversarial attacks
    into point dropping, point adding, and point shifting attacks according to different
    perturbation types. However, some 3D attacks are not based on the point cloud,
    such as 3D mesh attack (Cao et al., [2019b](#bib.bib17)) and volumetric network
    attack (Wicker and Kwiatkowska, [2019](#bib.bib146)). Therefore, we classify 3D
    attacks by the algorithms rather than the perturbation types.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. 3D adversarial attacks in the digital world
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.3.1\. Gradient-based method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some attacks are based on 2D adversarial attacks but with new loss functions.
    The first work of 3D adversarial attack, 3DAdv (Xiao et al., [2018a](#bib.bib150)),
    is based on C&W. It includes the independent point and adversarial cluster attacks.
    The first attack uses Hausdor and Chamfer distance to measure the maximum and
    average distance between the original and the adversarial point cloud. Chamfer
    distance is defined as $\mathcal{D}_{C}(x,x^{\prime})=\frac{1}{\left\|x^{\prime}\right\|}\sum_{p^{\prime}\in
    x^{\prime}}\left(\min_{p\in x}\left\|p^{\prime}-p\right\|_{2}^{2}\right)$. The
    second attack used the Chamfer and the farthest pair-wise distance to generate
    an adversarial cluster. They achieved a 100% ASR with an acceptable noise budget.
    However, they only evaluated the 3D-Adv attack on the PointNet network, which
    was proven less adversarial robust than other 3D models, like PointNet++. Liu
    et.al. (Liu et al., [2019](#bib.bib77)) extended PGD to 3D-PGD. They projected
    the perturbation to the tangent plane of the adversarial point cloud to reduce
    the outlier points. This is equivalent to changing the distribution of the surface
    points. However, Tsai et.al. stated that this attack could be easily defended
    by a resampling method (Tsai et al., [2020](#bib.bib133)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some attacks discard important points or generate points with specific shapes.
    Zheng et al. (Zheng et al., [2019b](#bib.bib175)) evaluated each point’s importance
    through point discarding. Because point discarding is non-differential, they relax
    it by moving points towards the point cloud’s interior. Then, the critical score
    is evaluated by the directional derivative in the spherical coordinates. ShapeAdv
    (Liu et al., [2020c](#bib.bib78)) includes three kinds of perturbations: uniform
    distribution perturbation, adversarial sticks, and adversarial sinks. The first
    one generates evenly distributed perturbation through resampling during the optimization.
    The second one adds auxiliary features like adversarial sticks. However, because
    the stick’s angle is difficult to optimize, they use a projection and clip function
    to approximate it. The last one pulls points into the inside of the point cloud.
    MinimalAdv (Kim et al., [2021](#bib.bib65)) fools the classifier by manipulating
    fewest points. They formulated the problem as a $L_{0}$ and $L_{2}$ optimization
    problem and relaxed it to $L_{1}$ problem. However, their method drops significantly
    after the resampling process because only a few points are adversarial, and most
    points (¿95%) are benign.'
  prefs: []
  type: TYPE_NORMAL
- en: Some attacks are proposed to improve the smoothness of the AEs. Tsai et.al.
    (Tsai et al., [2020](#bib.bib133)) used Chamfer and kNN distance to generate a
    geometry-aware AE. The kNN loss can restrict the distance between neighboring
    points in the point cloud and, hence, can significantly reduce outlier points.
    They successfully achieved targeted attacks on PointNet++ in the physical world
    by 3D-printed objects. However, their method results in rough surfaces. $GeoA^{3}$
    (Wen et al., [2020](#bib.bib145)) improves the fidelity of AE by combining Chamfer
    distance and the consistency of local curvatures between $\mathcal{P}$ and $\mathcal{P}^{\prime}$,
    which is measured through the direction of normal vectors. Moreover, to make $\mathcal{P}^{\prime}$
    more robust to resampling, they proposed $GeoA_{+}^{3}$ that contains a uniformity
    loss to promote the regularity of surface points. Then, they proposed iterative
    normal projection to optimize this objective function. Huang et al. (Huang et al.,
    [2022](#bib.bib54)) proposed shape-invariant attack to improve the AE’s surface
    smoothness. They shift the point along the tangent plane of the surface so that
    the perturbations are more imperceptible than the previous methods. Moreover,
    they combined the surrogate model and query-based methods to improve the black-box
    attack’s efficiency. They first calculated a sensitivity map on the surrogate
    model according to the maximum gradient magnitude. Then, they shifted points according
    to the sensitivity ranking. This query method can reduce the query times by about
    20% compared with SimBA (Guo et al., [2019a](#bib.bib43)), and SimBA++ (Yang et al.,
    [2020](#bib.bib156)) attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, statistical outlier removal (SOR) has been proposed to defend against
    adversarial point clouds. To break this countermeasure, JGBA (Ma et al., [2020](#bib.bib85))
    embeds SOR into the attack algorithm. But because SOR is not differentiable, they
    replace it with a relaxation function using first-order approximation. Then, they
    optimize the original and SOR-filtered point cloud simultaneously. This attack
    successfully defeated SOR and SOR-based DUP-Net defenses.
  prefs: []
  type: TYPE_NORMAL
- en: Some attacks aim to improve the transferability of 3D adversarial examples.
    ITA (Liu and Hu, [2022](#bib.bib76)) limits the perturbation along the direction
    of the normal vector and improves the transferability through adversarial transformations.
    The adversarial transformation composes a simple two-layer neural network and
    is learned through an adversarial training procedure. AOF (Liu et al., [2022](#bib.bib75))
    improves the transferability by boosting the classification loss of low-frequency
    component, which is separated from the original PC by orthogonal decomposition
    of the graph Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5\. Summary of main digital and physical adversarial attacks in 3D CV
    tasks sorted by the algorithm and published year
  prefs: []
  type: TYPE_NORMAL
- en: '| Attacks | Year | Threat Model | Method | Distance^(**) | Performance | Key
    idea |'
  prefs: []
  type: TYPE_TB
- en: '| Goal | Knowl^* | Interface | Efficiency | Succ. |'
  prefs: []
  type: TYPE_TB
- en: '| 3DAdv (Xiao et al., [2018a](#bib.bib150)) | 2019 | T | $\square$ | Digital
    | Gradient | $L_{2}$, $Cf.$ | Efficient | 99% | Chamfer loss |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-PGD(Liu et al., [2019](#bib.bib77)) | 2019 | U | $\square$ | Digital |
    Gradient | $L_{2}$ | Efficient | 88.10% | Gradient projection |'
  prefs: []
  type: TYPE_TB
- en: '| Saliency(Zheng et al., [2019b](#bib.bib175)) | 2019 | U | $\square$ | Digital
    | Gradient | $L_{0}$ | Efficient | 40% | Saliency map |'
  prefs: []
  type: TYPE_TB
- en: '| LidarAdv (Cao et al., [2019b](#bib.bib17)) | 2019 | T&U | $\square$ | Physical
    | Gradient | $L_{2},Lp.$ | Costly | 71% | Proxy function |'
  prefs: []
  type: TYPE_TB
- en: '| Adv-Lidar (Cao et al., [2019a](#bib.bib16)) | 2019 | T&U | $\square$ | Physical
    | Gradient | $L_{0}$ | Costly | 75% | Global sampling |'
  prefs: []
  type: TYPE_TB
- en: '| KNN(Tsai et al., [2020](#bib.bib133)) | 2020 | U&T | $\square$ | Physical
    | Gradient | $Cf.,kNN$ | Efficient | 94.69% | 3D printable |'
  prefs: []
  type: TYPE_TB
- en: '| Rooftop(Tu et al., [2020](#bib.bib135)) | 2020 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Physical | Gradient | $Lp.$ | Costly | 80% | Adversarial mesh |'
  prefs: []
  type: TYPE_TB
- en: '| $GeoA^{3}$, $GeoA_{+}^{3}$(Wen et al., [2020](#bib.bib145)) | 2020 | T |
    $\square$ | Digital | Gradient | $Cf.,Hd.,LC$ | Efficient | 100% | Local curvatures
    |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeAdv(Liu et al., [2020c](#bib.bib78)) | 2020 | U | $\square$ | Digital
    | Gradient | $L_{2}$ | Costly | 95% | Shape attack |'
  prefs: []
  type: TYPE_TB
- en: '| JGBA (Ma et al., [2020](#bib.bib85)) | 2020 | T&U | $\square$  $\color[rgb]{.5,.5,.5}\blacksquare$
    | Digital | Gradient | $L_{2}$ | Efficient | 98.90% | Break SOR |'
  prefs: []
  type: TYPE_TB
- en: '| CTRI(Zhao et al., [2020b](#bib.bib170)) | 2020 | T | $\square$ | Digital
    | Gradient | Spectral | Efficient | 98% | Restricted isometry |'
  prefs: []
  type: TYPE_TB
- en: '| MinimalAdv(Kim et al., [2021](#bib.bib65)) | 2021 | U | $\square$ | Digital
    | Gradient | $L_{2},L_{0},Cf.,Hd.$ | Costly | 89% | Minimal perturbation |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeInv(Huang et al., [2022](#bib.bib54)) | 2022 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Digital | Gradient | $L_{\infty}$ | Efficient | 100.00% | Sensitivity map |'
  prefs: []
  type: TYPE_TB
- en: '| AOF (Liu et al., [2022](#bib.bib75)) | 2022 | U | $\square$ | Physical |
    Gradient | $L_{\infty}$ | Costly | 99.76% | Low frequency |'
  prefs: []
  type: TYPE_TB
- en: '| AdvPC (Hamdi et al., [2020](#bib.bib46)) | 2020 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Digital | GAN | $L_{\infty}$ | Costly | 64.40% | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| LG-GAN (Zhou et al., [2020](#bib.bib176)) | 2020 | T | $\square$ | Digital
    | GAN | $L_{2}$ | Efficient | 97% | Generative model |'
  prefs: []
  type: TYPE_TB
- en: '| ITA (Liu and Hu, [2022](#bib.bib76)) | 2022 | T&U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Digital | GAN | $L_{2},Cf.$ | Efficient | 29.89% | Adv. transform |'
  prefs: []
  type: TYPE_TB
- en: '| EvolutionAdv(Cao et al., [2019b](#bib.bib17)) | 2019 | U | $\color[rgb]{0,0,0}\blacksquare$
    | Physical | Evolution | $-$ | Costly | 62% | Evolution |'
  prefs: []
  type: TYPE_TB
- en: '| Camdar-adv (Chen and Huang, [2021](#bib.bib19)) | 2021 | T | $\color[rgb]{0,0,0}\blacksquare$
    | Physical | Evolution | $Lp.$ | Costly | 99% | Multi-modality |'
  prefs: []
  type: TYPE_TB
- en: '| ISO (Wicker and Kwiatkowska, [2019](#bib.bib146)) | 2019 | U | $\square$  $\color[rgb]{0,0,0}\blacksquare$
    | Digital | Greedy | $L_{0}$ | Costly | 100% | Critical point set |'
  prefs: []
  type: TYPE_TB
- en: '| OcclusionPoint(Sun et al., [2020](#bib.bib127)) | 2020 | U | $\color[rgb]{0,0,0}\blacksquare$
    | Physical | Greedy | $L_{0}$ | Efficient | 80% | Lidar occlusion |'
  prefs: []
  type: TYPE_TB
- en: '| TSI(Zhao et al., [2020b](#bib.bib170)) | 2020 | U | $\color[rgb]{0,0,0}\blacksquare$
    | Digital | Random | $-$ | Efficient | 95% | Thompson sampling |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* This column is the adversarial knowledge of different attacks. $\square$:
    white-box. $\blacksquare$: black-box. ${\color[rgb]{.5,.5,.5}\blacksquare}$: gray-box.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '** Cf.: Chamfer distance. Hd: Hausdorff distance. Lp: Laplacian distance. LC:
    Local curvature distance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6.3.2\. Generative-model-based methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hamdi et al. (Hamdi et al., [2020](#bib.bib46)) proposed AdvPC attack, which
    used GAN model to produce an adversarial point cloud. The loss of AdvPC has two
    parts. The first one is the pre-trained classifier’s loss. The second is the auto-encoder
    loss to ensure that the reconstructed point cloud can still fool the classifier.
    Experiments show that AdvPC attacks can improve the transferability of adversarial
    point clouds. Moreover, AdvPC outperforms 3D-adv and KNN attacks on several different
    defense methods. However, they only evaluated untargeted attacks.
  prefs: []
  type: TYPE_NORMAL
- en: LG-GAN (Zhou et al., [2020](#bib.bib176)) generated targeted adversarial examples
    through the multi-branch generative network. It first learns the multi-layer features
    of the input 3D data through the GAN and then unitizes a class encoder to mix
    the label information into the multi-layer features. At last, the 3D data are
    rebuilt from the features. The GAN includes a classification loss, a $L_{2}$-norm
    reconstruction loss, and a graph adversarial loss.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3\. Heuristic-algorithm-based methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the black-box scenario, the gradient is hard to obtain. Therefore, black-box
    attacks are often based on heuristic algorithms. Cao et al. (Cao et al., [2019b](#bib.bib17))
    proposed an evolution-based black-box algorithm EvolutionAdv. They set the population
    as mesh vertices of the object and the adaption function as $-L(f(x^{adv}))$ and
    randomly add some novel individuals at each period with perturbations sampling
    from a normal distribution. Wicker et al. (Wicker and Kwiatkowska, [2019](#bib.bib146))
    proposed an Iterative Salience Occlusion attack (ISO) to break PointNet and volumetric
    networks through a greedy algorithm. They first identify the vital point set through
    queries. Then, they drop the most critical points iteratively until the classification
    result is false.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.4\. 3D-transformation-based methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TSI (Zhao et al., [2020b](#bib.bib170)) found that 3D models are vulnerable
    to affine transformations like rotations and transitions. Therefore, they proposed
    to attack the 3D model by isometric transformation. A random algorithm based on
    Thompson sampling is proposed to optimize the rotation angles. Compared with random
    sampling, Thompson sampling is more productive and can unitize the prior knowledge.
    CTRI (Zhao et al., [2020b](#bib.bib170)) searched an adversarial isometric transformation
    (such as rotation) by minimizing the spectral-norm loss, which is equivalent to
    finding a smallest $\delta$ such that $(1-\delta)\left\|x\right\|^{2}\leq\left\|Ax\right\|^{2}\leq(1+\delta)\left\|x\right\|^{2}$,
    where $A$ is the rotation matrix and $x$ is the point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4\. The safety impact of physical-realizable 3D adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, 3D physical adversarial attacks have been proposed to cheat
    Lidar-based detectors through printable mesh (Tu et al., [2020](#bib.bib135))
    or laser emitter (Cao et al., [2019a](#bib.bib16)). These attacks pose severe
    threats to real-world applications. We classify these attacks into simulation-based
    and physical-system-based 3D attacks based on their implementation methods.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1\. Simulation-based 3D attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some of them are implemented in the simulation setting through a Lidar renderer.
    For example, Cao et al. (Cao et al., [2019b](#bib.bib17)) proposed an optimization-based
    algorithm LidarAdv for hiding 3D mesh from Lidar detection. They first soften
    the preprocess function through the differential proxy function. Then, they generated
    adversarial mesh through $L_{2}$ loss. However, they only evaluated their attack
    on the Apollo platform in the simulation setting rather than the newest detection
    models. Tu et al. (Tu et al., [2020](#bib.bib135)) proposed Rooftop attack to
    hide the self-driving car from Lidar by placing an adversarial mesh on the vehicle.
    They optimize the initial mesh through a fooling loss and a Laplacian loss to
    improve its smoothness. Their vehicle-agnostic perturbation can achieve an 80%
    occlusion rate. But they also only simulated their attack with CAD models and
    a Lidar renderer.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2\. Physical-system-based 3D attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Later, some attacks are proposed to attack real-world Lidar sensors. Cao et
    al. (Cao et al., [2019a](#bib.bib16)) proposed Adv-Lidar that can mislead a real-world
    self-driving system to detect nonexistent obstacles. Rather than modifying 3D
    mesh, they manipulate the 3D point cloud by laser emitter instead. They first
    analyzed a real-world self-driving system and found that previous attacks cannot
    create unreal barriers because of the preprocessing steps. So, they considered
    the preprocessing steps in the optimization. Then, they added adversarial points
    into the pristine point cloud in the limited angle and distance ranges. In addition,
    they used global sampling to avoid being caught in a local minimum. Their method
    successfully cheated a real-world self-driving system. Chen et al. (Chen and Huang,
    [2021](#bib.bib19)) proposed a multi-modality attack that attacks Lidar and the
    camera simultaneously through the 3D adversarial mesh and 2D adversarial patch.
    They used the evolution algorithm to find 2D perturbations and used Tu et al.’s
    method (Tu et al., [2020](#bib.bib135)) to find 3D adversarial mesh. Sun et al.
    (Sun et al., [2020](#bib.bib127)) proposed a black-box attack against the Lidar
    detector in the self-driving setting. They noticed that the detect models are
    vulnerable to distant and occluded vehicle attacks, in which they can fool the
    detector by just a few points. Their attacks can cheat BEV-based, voxel-based,
    and point-based 3D object detectors.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Future directions and challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1\. Improving the transferability of adversarial examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A real-world feasible adversarial attack should be able to attack unseen models.
    Therefore, the transferability of adversarial samples has attracted more and more
    attention. There are several promising directions to improve the transferability
    of adversarial examples, including
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random/adversarial transformations. These methods assume that adversarial examples
    that can survive random or adversarial transformations can better transfer between
    different models. The existing methods include isometry transformation (Dong et al.,
    [2019](#bib.bib33)), random resizing (Xie et al., [2019](#bib.bib155)), multi-scale
    images (Lin et al., [2019](#bib.bib73)), or learnable adversarial transformation
    (Wu et al., [2021](#bib.bib149)). However, most of these transformations are very
    simple. It is still unknown why such vanilla transformations can improve the transferability
    and whether better transformations exist.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating perturbation by generative model. Some works (Xiao et al., [2018a](#bib.bib150),
    [2021](#bib.bib153)) proposed optimizing the latent feature can improve transferability.
    However, in our experiment, we find that searching adversarial perturbation on
    the latent space may cause the optimization to be unstable or non-converging.
    So, there is still room for improving the generative model’s architecture and
    loss function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating the latent layer features. Some works (Ganeshan et al., [2019](#bib.bib38);
    Huang et al., [2019](#bib.bib55); Wang et al., [2021a](#bib.bib143); Zhang et al.,
    [2022b](#bib.bib164)) maximize the middle layer features’ difference between the
    pristine and adversarial images to improve the transferability. But Zhang et al.
    (Zhang et al., [2022b](#bib.bib164)) have shown that these methods also have great
    promotion space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble-based approaches. Some works also improve transferability by attacking
    an ensemble of models (Dong et al., [2018](#bib.bib32); Yin et al., [2021](#bib.bib157)).
    However, training multiple substitute models is computationally expensive. How
    to train ensemble models efficiently and how to design or select substitute models
    are still open problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moreover, at present, most of these methods improve transferability empirically.
    Few of them proved a lower bound of the transferability or explained the mechanism
    behind the transferability. One possible way of understanding transferability
    is through the similarity of decision boundaries. For example, Liu et al. (Liu
    et al., [2016](#bib.bib81)), and Tramèr et al. (Tramèr et al., [2017b](#bib.bib132))
    analyzed the decision boundary of different models and contributed the transferability
    to their similar boundaries. However, this theory cannot explain the asymmetry
    of transferability (Wu et al., [2018](#bib.bib148)). Ilyas et al. (Ilyas et al.,
    [2019](#bib.bib58)) suggests adversarial perturbation is non-robust features rather
    than noise, and different models may learn the same non-robust features. Therefore,
    the AEs can transfer between them. However, they only prove their hypothesis on
    a simple dataset with two classes and cannot explain why sometimes the two models
    predict the same AE as different false labels. Finding more reasonable theoretical
    interpretations of transferability is still an urgent task for robust deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Towards semantic perturbation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imperceptible perturbation by $L_{p}$-norm distance is vulnerable to physical
    variables, like distance and background light. Therefore, some works proposed
    to fool the classifier while maintaining semantic consistency for human beings.
    At present, the main directions and challenges of semantic perturbation include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Color-space transformations. Some studies claimed that the DNN is biased toward
    texture, while human beings like to classify objects according to their shapes
    (Geirhos et al., [2018](#bib.bib39)). Therefore, some works operated color distortion
    in different color spaces to generate semantic-preserving perturbation. However,
    how to better measure the perceptual distance to be consistent with human perception
    is still an open problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global/Local spatial transformations. Global and local spatial transformations
    can also generate adversarial examples while reserving semantics. However, because
    it’s hard to compute gradients regarding the transformation parameters, there
    is still a demand to find an efficient way to optimize the transformation parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating image content attributes. Some attacks utilized the conditional
    GAN to modify the image content attributes. However, better disentangling the
    latent features and controlling the attributes still need research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attacks based on diffusion model. Recently, the diffusion model (Ho et al.,
    [2020](#bib.bib50)) has been proposed to produce high-quality images, which has
    outperformed GAN on some tasks. A few works have used them for generating unconstrained
    semantic AEs (Chen et al., [2023](#bib.bib25)). But these works usually estimate
    the gradients roughly. A more precise method to calculate the gradients, like
    the stochastic differential equation, may improve the performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition, although many works explore the semantic-reserving perturbations,
    few of them tried to apply themselves to the physical world to bypass the camera
    system. There still is a question of whether these semantic perturbations can
    behave consistently in the physical setting.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Making adversarial examples physically achievable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generating physical-realizable AEs that are robust enough to physical condition
    changes and hard to notice simultaneously is still an open question. Physical
    attacks are more difficult than digital attacks because of the changing physical
    variables like view angles and distance. Some work (Athalye et al., [2018b](#bib.bib7);
    Brown et al., [2017](#bib.bib13)) proposed to improve the attack robustness through
    expectation over transformation (EoT). However, EoT significantly improves the
    perturbation size. One possible solution to this trade-off is using semantic perturbation,
    like colorization-based attacks. The multi-objective optimization strategy (Jia
    et al., [2022](#bib.bib60)) is also a possible way to balance stealthiness and
    attack strength.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, because implementing physical experiments needs lots of expenditure,
    some researchers simulated the physical adversarial attack through a differential
    3D renderer. However, current rendering techniques are susceptible to geometric
    and lighting transformations that distort the synthesized image. The gap between
    realistic photos and rendered images can affect the attack success rate or even
    make the adversarial attack ineffective. There is still a need to find better
    renderer or simulation methods or construct real-world datasets for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, present physical AEs often attack the target deep learning model
    directly. However, off-the-shelf applications often contain complete pipelines,
    including data acquisition and preprocessing, which may unexpectedly influence
    the effect of AEs. For example, the image filter and the point cloud sampling
    process may influence the ASRs. There is still promotion space for present physical
    attacks to compromise these off-the-shelf applications.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4\. Designing efficient 3D black-box adversarial attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Only a few works (Cao et al., [2019b](#bib.bib17); Sun et al., [2020](#bib.bib127);
    Wicker and Kwiatkowska, [2019](#bib.bib146)) explored the 3D black-box adversarial
    attacks, while most used very primitive or heuristic algorithms like random sampling,
    greedy search, and evolution algorithm. Many techniques like priors-transferring
    and boundary estimation have been exploited for 2D black-box attacks. Migrating
    these skills to 3D data may foster more efficient algorithms with fewer queries.
    For example, recently, 3DHacker (Tao et al., [2023](#bib.bib130)) used boundary
    estimation to improve the query efficiency of hard-label attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5\. Evaluating the robustness of newly proposed models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Novel 2D and 3D computer vision models’ robustness, e.g., Transformer (Vaswani
    et al., [2017](#bib.bib136)) and Point Transformer (Zhao et al., [2021a](#bib.bib168))
    , still need to be studies. Additionally, likelihood-based generative models have
    emerged, like the diffusion model (Ho et al., [2020](#bib.bib50)), which can produce
    high-quality images. Some pilot studies have already evaluated their robustness
    (Zhuang et al., [2023](#bib.bib178)). But these works are very rudimentary, and
    there is a lot of room for improvement. Moreover, the diffusion model is also
    helpful to improve the performance of generative-model-based attacks, such as
    unrestricted semantic attacks. However, how to backward its gradients to improve
    attack efficiency and how to disentangle its latent noises to control image attributes
    better still needs further research.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6\. Evaluating the safety of novel CV applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An increasing number of novel computer vision applications have incorporated
    deep learning, such as medical image processing, rain and fog removal, and pedestrian
    detection. The security requirements and potential attack surfaces of these emerging
    tasks vary a lot. Consequently, designing adversarial attacks for these novel
    tasks necessitates special considerations. For example, Schmalfuss et al. proposed
    an adversarial weather attack for motion estimation (Schmalfuss et al., [2023](#bib.bib115))
    that simulates weather effects by utilizing adversarially optimized particles.
  prefs: []
  type: TYPE_NORMAL
- en: 7.7\. Breaking newly proposed defenses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many adversarial attacks are proposed specifically targeting certain defense
    mechanisms, such as the C&W attack to break the defensive distillation, the BPDA
    attack to break gradient shattering, and the BlindSpot attack to break the adversarial
    training. With the development of this race of adversarial machine learning, an
    increasing number of novel defenses are proposed, such as provable defenses and
    adversarial purification (Nie et al., [2022](#bib.bib98)). It is still unknown
    whether these defense mechanisms can be defeated by stronger adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we comprehensively review the recent progress of adversarial
    attacks that damage the robustness and safety of deep learning models in computer
    vision tasks. In contrast to previous works, we summarize the adversarial attacks
    for 3D computer vision for the first time. Moreover, we extend the connotation
    of adversarial examples to imperceptible and semantic perturbations. For semantic
    perturbations, we systematically summarize the latest methods, including local
    and global spatial transformation, color space distortion, and attribution modification.
    What’s more, we investigate the physically realizable adversarial attacks towards
    safety-critical missions like self-driving vehicles and face recognition. In the
    end, we give some understanding of improving the transferability of AEs, making
    the AEs physically achievable, boosting the 3D black-box attack’s efficiency,
    evaluating the robustness of emerging models, and designing adversarial attacks
    for novel applications and defenses.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. (2020) Akshay Agarwal, Mayank Vatsa, Richa Singh, and Nalini K
    Ratha. 2020. Noise is inside me! generating adversarial perturbations with noise
    derived from natural filters. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops*. 774–775.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Al-Dujaili and O’Reilly (2019) Abdullah Al-Dujaili and Una-May O’Reilly. 2019.
    There are no bit parts for sign bits in black-box attacks. *arXiv preprint arXiv:1902.06894*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alaifari et al. (2018) Rima Alaifari, Giovanni S Alberti, and Tandri Gauksson.
    2018. ADef: an Iterative Algorithm to Construct Adversarial Deformations. In *International
    Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andriushchenko et al. (2020) Maksym Andriushchenko, Francesco Croce, Nicolas
    Flammarion, and Matthias Hein. 2020. Square attack: a query-efficient black-box
    adversarial attack via random search. In *European Conference on Computer Vision*.
    Springer, 484–501.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Athalye et al. (2018a) Anish Athalye, Nicholas Carlini, and David Wagner. 2018a.
    Obfuscated gradients give a false sense of security: Circumventing defenses to
    adversarial examples. In *International conference on machine learning*. PMLR,
    274–283.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Athalye et al. (2018b) Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin
    Kwok. 2018b. Synthesizing robust adversarial examples. In *International conference
    on machine learning*. PMLR, 284–293.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aydin et al. (2021) Ayberk Aydin, Deniz Sen, Berat Tuna Karli, Oguz Hanoglu,
    and Alptekin Temizel. 2021. Imperceptible Adversarial Examples by Spatial Chroma-Shift.
    In *Proceedings of the 1st International Workshop on Adversarial Learning for
    Multimedia*. 8–14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baluja and Fischer (2018) Shumeet Baluja and Ian Fischer. 2018. Learning to
    attack: Adversarial transformation networks. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhagoji et al. (2018) Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song.
    2018. Practical black-box attacks on deep neural networks using efficient query
    mechanisms. In *Proceedings of the European Conference on Computer Vision (ECCV)*.
    154–169.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhattad et al. (2019) Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and
    DA Forsyth. 2019. Unrestricted Adversarial Examples via Semantic Manipulation.
    In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brendel et al. (2017) Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2017.
    Decision-based adversarial attacks: Reliable attacks against black-box machine
    learning models. *arXiv preprint arXiv:1712.04248* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2017) Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and
    Justin Gilmer. 2017. Adversarial patch. *arXiv preprint arXiv:1712.09665* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brunner et al. (2019) Thomas Brunner, Frederik Diehl, Michael Truong Le, and
    Alois Knoll. 2019. Guessing smart: Biased sampling for efficient black-box adversarial
    attacks. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*. 4958–4966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Byun et al. (2022) Junyoung Byun, Seungju Cho, Myung-Joon Kwon, Hee-Seon Kim,
    and Changick Kim. 2022. Improving the Transferability of Targeted Adversarial
    Examples through Object-Based Diverse Input. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 15244–15253.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2019a) Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won
    Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, and Z Morley Mao. 2019a. Adversarial
    sensor attack on lidar-based perception in autonomous driving. In *Proceedings
    of the 2019 ACM SIGSAC conference on computer and communications security*. 2267–2281.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2019b) Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang
    Yang, Mingyan Liu, and Bo Li. 2019b. Adversarial objects against lidar-based autonomous
    driving systems. *arXiv preprint arXiv:1907.05418* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini and Wagner (2017) Nicholas Carlini and David Wagner. 2017. Towards evaluating
    the robustness of neural networks. In *2017 ieee symposium on security and privacy
    (sp)*. Ieee, 39–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen and Huang (2021) Chang Chen and Teng Huang. 2021. Camdar-adv: generating
    adversarial patches on 3D object. *International Journal of Intelligent Systems*
    36, 3 (2021), 1441–1453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen and Gu (2020) Jinghui Chen and Quanquan Gu. 2020. Rays: A ray searching
    method for hard-label adversarial attack. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 1739–1747.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Jianbo Chen, Michael I Jordan, and Martin J Wainwright.
    2020. Hopskipjumpattack: A query-efficient decision-based attack. In *2020 ieee
    symposium on security and privacy (sp)*. IEEE, 1277–1294.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018b) Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui
    Hsieh. 2018b. Ead: elastic-net attacks to deep neural networks via adversarial
    examples. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017) Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui
    Hsieh. 2017. Zoo: Zeroth order optimization based black-box attacks to deep neural
    networks without training substitute models. In *Proceedings of the 10th ACM workshop
    on artificial intelligence and security*. 15–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018a) Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen
    Horng Polo Chau. 2018a. Shapeshifter: Robust physical adversarial attack on faster
    r-cnn object detector. In *Joint European Conference on Machine Learning and Knowledge
    Discovery in Databases*. Springer, 52–68.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong Ding,
    and Wenqiang Zhang. 2023. Content-based Unrestricted Adversarial Attack. *arXiv
    preprint arXiv:2305.10665* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2020) Minhao Cheng, Simranjit Singh, Patrick H Chen, Pin-Yu Chen,
    Sijia Liu, and Cho-Jui Hsieh. 2020. Sign-OPT: A Query-Efficient Hard-label Adversarial
    Attack. In *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2019b) Minhao Cheng, Huan Zhang, Cho-Jui Hsieh, Thong Le, Pin-Yu
    Chen, and Jinfeng Yi. 2019b. Query-efficient hard-label black-box attack: An optimization-based
    approach. In *International Conference on Learning Representations*. International
    Conference on Learning Representations, ICLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2019a) Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun
    Zhu. 2019a. Improving black-box adversarial attacks with a transfer-based prior.
    *Advances in neural information processing systems* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Croce et al. (2022) Francesco Croce, Maksym Andriushchenko, Naman D Singh,
    Nicolas Flammarion, and Matthias Hein. 2022. Sparse-rs: a versatile framework
    for query-efficient sparse black-box adversarial attacks. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, Vol. 36\. 6437–6445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croce and Hein (2019) Francesco Croce and Matthias Hein. 2019. Sparse and Imperceivable
    Adversarial Attacks. In *2019 IEEE/CVF International Conference on Computer Vision
    (ICCV)*. 4723–4731. [https://doi.org/10.1109/ICCV.2019.00482](https://doi.org/10.1109/ICCV.2019.00482)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2020) Xiaoyi Dong, Dongdong Chen, Jianmin Bao, Chuan Qin, Lu Yuan,
    Weiming Zhang, Nenghai Yu, and Dong Chen. 2020. GreedyFool: Distortion-aware sparse
    adversarial attack. *Advances in Neural Information Processing Systems* 33 (2020),
    11226–11236.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2018) Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu,
    Xiaolin Hu, and Jianguo Li. 2018. Boosting adversarial attacks with momentum.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    9185–9193.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2019) Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. 2019. Evading
    defenses to transferable adversarial examples by translation-invariant attacks.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    4312–4321.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. (2020) Ranjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A. K.
    Qin, and Yun Yang. 2020. Adversarial Camouflage: Hiding Physical-World Attacks
    With Natural Styles. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engstrom et al. (2018) Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig
    Schmidt, and Aleksander Madry. 2018. A rotation and a translation suffice: Fooling
    cnns with simple transformations. (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eykholt et al. (2018) Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li,
    Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018.
    Robust physical-world attacks on deep learning visual classification. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 1625–1634.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fawzi and Frossard (2015) Alhussein Fawzi and Pascal Frossard. 2015. Manitest:
    Are classifiers really invariant?. In *British Machine Vision Conference (BMVC)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganeshan et al. (2019) Aditya Ganeshan, Vivek BS, and R Venkatesh Babu. 2019.
    Fda: Feature disruptive attack. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 8069–8079.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geirhos et al. (2018) Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias
    Bethge, Felix A Wichmann, and Wieland Brendel. 2018. ImageNet-trained CNNs are
    biased towards texture; increasing shape bias improves accuracy and robustness.
    *arXiv preprint arXiv:1811.12231* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilmer et al. (2018) Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen,
    and George E Dahl. 2018. Motivating the rules of the game for adversarial example
    research. *arXiv preprint arXiv:1807.06732* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gnanasambandam et al. (2021) Abhiram Gnanasambandam, Alex M Sherman, and Stanley H
    Chan. 2021. Optical adversarial attack. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 92–101.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    2014. Explaining and harnessing adversarial examples. *arXiv preprint arXiv:1412.6572*
    (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019a) Chuan Guo, Jacob Gardner, Yurong You, Andrew Gordon Wilson,
    and Kilian Weinberger. 2019a. Simple black-box adversarial attacks. In *International
    Conference on Machine Learning*. PMLR, 2484–2493.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2021) Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu,
    Ralph R Martin, and Shi-Min Hu. 2021. Pct: Point cloud transformer. *Computational
    Visual Media* 7, 2 (2021), 187–199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2019b) Yiwen Guo, Ziang Yan, and Changshui Zhang. 2019b. Subspace
    attack: Exploiting promising subspaces for query-efficient black-box attacks.
    *Advances in Neural Information Processing Systems* 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hamdi et al. (2020) Abdullah Hamdi, Sara Rojas, Ali Thabet, and Bernard Ghanem.
    2020. Advpc: Transferable adversarial perturbations on 3d point clouds. In *European
    Conference on Computer Vision*. Springer, 241–257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2018) Warren He, Bo Li, and Dawn Song. 2018. Decision boundary analysis
    of adversarial examples. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2022a) Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen
    He. 2022a. Towards Security Threats of Deep Learning Systems: A Survey. *IEEE
    Transactions on Software Engineering* 48, 5 (2022), 1743–1770.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2022b) Ziwen He, Wei Wang, Jing Dong, and Tieniu Tan. 2022b. Transferable
    Sparse Adversarial Attack. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 14963–14972.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    diffusion probabilistic models. *Advances in neural information processing systems*
    33 (2020), 6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosseini and Poovendran (2018) Hossein Hosseini and Radha Poovendran. 2018.
    Semantic adversarial examples. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition Workshops*. 1614–1619.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2021) Yu-Chih-Tuan Hu, Bo-Han Kung, Daniel Stanley Tan, Jun-Cheng
    Chen, Kai-Lung Hua, and Wen-Huang Cheng. 2021. Naturalistic physical adversarial
    patch for object detectors. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 7848–7857.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hua et al. (2018) Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. 2018. Pointwise
    convolutional neural networks. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*. 984–993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Qidong Huang, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming
    Zhang, and Nenghai Yu. 2022. Shape-invariant 3D Adversarial Point Clouds. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 15335–15344.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2019) Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie,
    and Ser-Nam Lim. 2019. Enhancing adversarial example transferability with an intermediate
    level attack. In *Proceedings of the IEEE/CVF international conference on computer
    vision*. 4733–4742.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilyas et al. (2018b) Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy
    Lin. 2018b. Black-box adversarial attacks with limited queries and information.
    In *International Conference on Machine Learning*. PMLR, 2137–2146.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ilyas et al. (2018a) Andrew Ilyas, Logan Engstrom, and Aleksander Madry. 2018a.
    Prior Convictions: Black-box Adversarial Attacks with Bandits and Priors. In *International
    Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilyas et al. (2019) Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan
    Engstrom, Brandon Tran, and Aleksander Madry. 2019. Adversarial examples are not
    bugs, they are features. *Advances in neural information processing systems* 32
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2021) Xiaoyu Ji, Yushi Cheng, Yuepeng Zhang, Kai Wang, Chen Yan,
    Wenyuan Xu, and Kevin Fu. 2021. Poltergeist: Acoustic adversarial machine learning
    against cameras and computer vision. In *2021 IEEE Symposium on Security and Privacy
    (SP)*. IEEE, 160–175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. (2022) Shuai Jia, Bangjie Yin, Taiping Yao, Shouhong Ding, Chunhua
    Shen, Xiaokang Yang, and Chao Ma. 2022. Adv-attribute: Inconspicuous and transferable
    adversarial attack on face recognition. *Advances in Neural Information Processing
    Systems* 35 (2022), 34136–34147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2019) Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, and Chinmay
    Hegde. 2019. Semantic adversarial attacks: Parametric transformations that fool
    deep classifiers. In *Proceedings of the IEEE/CVF international conference on
    computer vision*. 4773–4783.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kanbak et al. (2018) Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
    Frossard. 2018. Geometric robustness of deep networks: analysis and improvement.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    4441–4449.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kato et al. (2018) Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. 2018.
    Neural 3d mesh renderer. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 3907–3916.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khrulkov and Oseledets (2018) Valentin Khrulkov and Ivan Oseledets. 2018. Art
    of singular vectors and universal adversarial perturbations. In *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition*. 8562–8570.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021) Jaeyeon Kim, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung.
    2021. Minimal adversarial examples for deep learning on 3d point clouds. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*. 7797–7806.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurakin et al. (2018) Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. 2018.
    Adversarial examples in the physical world. In *Artificial intelligence safety
    and security*. Chapman and Hall/CRC, 99–112.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. Weight
    Poisoning Attacks on Pretrained Models. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*. Association for Computational
    Linguistics, 2793–2806.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laidlaw and Feizi (2019) Cassidy Laidlaw and Soheil Feizi. 2019. Functional
    adversarial attacks. *Advances in neural information processing systems* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Maosen Li, Cheng Deng, Tengjiao Li, Junchi Yan, Xinbo Gao,
    and Heng Huang. 2020. Towards transferable targeted attack. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*. 641–649.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Xiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He,
    Shuhui Wang, Hang Su, and Hui Xue. 2021. Qair: Practical query-efficient black-box
    attacks for image retrieval. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 3330–3339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019) Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing
    Gong. 2019. Nattack: Learning the distributions of adversarial examples for an
    improved black-box attack on deep neural networks. In *International Conference
    on Machine Learning*. PMLR, 3866–3876.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023) Yanjie Li, Yiquan Li, Xuelong Dai, Songtao Guo, and Bin Xiao.
    2023. Physical-World Optical Adversarial Attacks on 3D Face Recognition. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.
    24699–24708.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2019) Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E
    Hopcroft. 2019. Nesterov Accelerated Gradient and Scale Invariance for Adversarial
    Attacks. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi
    Zhang, and Hang Yu. 2020a. Bias-based universal adversarial patch attack for automatic
    check-out. In *European conference on computer vision*. Springer, 395–410.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Binbin Liu, Jinlai Zhang, and Jihong Zhu. 2022. Boosting 3D
    Adversarial Attacks with Attacking On Frequency. *IEEE Access* 10 (2022), 50974–50984.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Hu (2022) Daizong Liu and Wei Hu. 2022. Imperceptible Transfer Attack
    and Defense on 3D Point Cloud Classification. *IEEE Transactions on Pattern Analysis
    and Machine Intelligence* (2022), 1–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Daniel Liu, Ronald Yu, and Hao Su. 2019. Extending adversarial
    attacks and defenses to deep 3d point cloud classifiers. In *2019 IEEE International
    Conference on Image Processing (ICIP)*. IEEE, 2279–2283.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020c) Daniel Liu, Ronald Yu, and Hao Su. 2020c. Adversarial shape
    perturbations on 3d point clouds. In *European Conference on Computer Vision*.
    Springer, 88–104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018) Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor CM
    Leung. 2018. A survey on security threats and defensive techniques of machine
    learning: A data driven view. *IEEE access* 6 (2018), 12103–12117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020b) Ximeng Liu, Lehui Xie, Yaopeng Wang, Jian Zou, Jinbo Xiong,
    Zuobin Ying, and Athanasios V Vasilakos. 2020b. Privacy and security issues in
    deep learning: A survey. *IEEE Access* 9 (2020), 4566–4593.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2016) Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2016. Delving
    into Transferable Adversarial Examples and Black-box Attacks. In *International
    Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2018) Bo Luo, Yannan Liu, Lingxiao Wei, and Qiang Xu. 2018. Towards
    imperceptible and robust adversarial example attacks against neural networks.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2022) Cheng Luo, Qinliang Lin, Weicheng Xie, Bizhu Wu, Jinheng Xie,
    and Linlin Shen. 2022. Frequency-driven Imperceptible Adversarial Attack on Semantic
    Similarity. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*. 15315–15324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2021) Chen Ma, Xiangyu Guo, Li Chen, Jun-Hai Yong, and Yisen Wang.
    2021. Finding optimal tangent points for reducing distortions of hard-label attacks.
    *Advances in Neural Information Processing Systems* 34 (2021), 19288–19300.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020) Chengcheng Ma, Weiliang Meng, Baoyuan Wu, Shibiao Xu, and Xiaopeng
    Zhang. 2020. Efficient joint gradient based attack against sor defense for 3d
    point cloud classification. In *Proceedings of the 28th ACM International Conference
    on Multimedia*. 1819–1827.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machado et al. (2021) Gabriel Resende Machado, Eugênio Silva, and Ronaldo Ribeiro
    Goldschmidt. 2021. Adversarial machine learning in image classification: a survey
    toward the defender’s perspective. *ACM Computing Surveys (CSUR)* 55, 1 (2021),
    1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madry et al. (2017) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, and Adrian Vladu. 2017. Towards deep learning models resistant to adversarial
    attacks. *arXiv preprint arXiv:1706.06083* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maho et al. (2021) Thibault Maho, Teddy Furon, and Erwan Le Merrer. 2021. Surfree:
    a fast surrogate-free black-box attack. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 10430–10439.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miller et al. (2020) David J Miller, Zhen Xiang, and George Kesidis. 2020.
    Adversarial learning targeting deep neural network classification: A comprehensive
    review of defenses against attacks. *Proc. IEEE* 108, 3 (2020), 402–433.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Modas et al. (2019) Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
    Frossard. 2019. Sparsefool: a few pixels make a big difference. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*. 9087–9096.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moon et al. (2019) Seungyong Moon, Gaon An, and Hyun Oh Song. 2019. Parsimonious
    black-box adversarial attacks via efficient combinatorial optimization. In *International
    Conference on Machine Learning*. PMLR, 4636–4645.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moosavi-Dezfooli et al. (2017) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
    Omar Fawzi, and Pascal Frossard. 2017. Universal adversarial perturbations. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    1765–1773.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moosavi-Dezfooli et al. (2016) Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
    and Pascal Frossard. 2016. Deepfool: a simple and accurate method to fool deep
    neural networks. In *Proceedings of the IEEE conference on computer vision and
    pattern recognition*. 2574–2582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mopuri et al. (2018a) Konda Reddy Mopuri, Aditya Ganeshan, and R Venkatesh Babu.
    2018a. Generalizable data-free objective for crafting universal adversarial perturbations.
    *IEEE transactions on pattern analysis and machine intelligence* 41, 10 (2018),
    2452–2465.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mopuri et al. (2018b) Konda Reddy Mopuri, Utkarsh Ojha, Utsav Garg, and R Venkatesh
    Babu. 2018b. Nag: Network for adversary generation. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 742–751.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narodytska and Kasiviswanathan (2017) Nina Narodytska and Shiva Prasad Kasiviswanathan.
    2017. Simple Black-Box Adversarial Attacks on Deep Neural Networks.. In *CVPR
    Workshops*, Vol. 2\. 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2020) Dinh-Luan Nguyen, Sunpreet S Arora, Yuhang Wu, and Hao
    Yang. 2020. Adversarial light projection attacks on face recognition systems:
    A feasibility study. In *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition workshops*. 814–815.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie et al. (2022) Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat,
    and Anima Anandkumar. 2022. Diffusion models for adversarial purification. *arXiv
    preprint arXiv:2205.07460* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Osadchy et al. (2017) Margarita Osadchy, Julio Hernandez-Castro, Stuart Gibson,
    Orr Dunkelman, and Daniel Pérez-Cabo. 2017. No Bot Expects the DeepCAPTCHA! Introducing
    Immutable Adversarial Examples, With Applications to CAPTCHA Generation. *IEEE
    Transactions on Information Forensics and Security* 12, 11 (2017), 2640–2653.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papernot et al. (2016a) Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.
    2016a. Transferability in machine learning: from phenomena to black-box attacks
    using adversarial samples. *arXiv preprint arXiv:1605.07277* (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papernot et al. (2017) Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh
    Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against
    machine learning. In *Proceedings of the 2017 ACM on Asia conference on computer
    and communications security*. 506–519.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papernot et al. (2016b) Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
    Fredrikson, Z Berkay Celik, and Ananthram Swami. 2016b. The limitations of deep
    learning in adversarial settings. In *2016 IEEE European symposium on security
    and privacy (EuroS&P)*. IEEE, 372–387.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papernot et al. (2018) Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and
    Michael P Wellman. 2018. Sok: Security and privacy in machine learning. In *2018
    IEEE European Symposium on Security and Privacy (EuroS&P)*. IEEE, 399–414.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papernot et al. (2016c) Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha,
    and Ananthram Swami. 2016c. Distillation as a defense to adversarial perturbations
    against deep neural networks. In *2016 IEEE symposium on security and privacy
    (SP)*. IEEE, 582–597.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poursaeed et al. (2018) Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge
    Belongie. 2018. Generative adversarial perturbations. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 4422–4431.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. 2017.
    Pointnet: Deep learning on point sets for 3d classification and segmentation.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    652–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2020) Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan, Honglak
    Lee, and Bo Li. 2020. Semanticadv: Generating adversarial examples via attribute-conditioned
    image editing. In *European Conference on Computer Vision*. Springer, 19–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rahmati et al. (2020) Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard,
    and Huaiyu Dai. 2020. Geoda: a geometric framework for black-box adversarial attacks.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*.
    8446–8455.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rakin et al. (2021) Adnan Siraj Rakin, Zhezhi He, Jingtao Li, Fan Yao, Chaitali
    Chakrabarti, and Deliang Fan. 2021. T-bfa: Targeted bit-flip adversarial weight
    attack. *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reza et al. (2023) Md Farhamdur Reza, Ali Rahmati, Tianfu Wu, and Huaiyu Dai.
    2023. CGBA: Curvature-aware Geometric Black-box Attack. *arXiv preprint arXiv:2308.03163*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rony et al. (2019) Jérôme Rony, Luiz G Hafemann, Luiz S Oliveira, Ismail Ben
    Ayed, Robert Sabourin, and Eric Granger. 2019. Decoupling direction and norm for
    efficient gradient-based l2 adversarial attacks and defenses. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 4322–4330.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salem et al. (2020) Ahmed Salem, Apratim Bhattacharya, Michael Backes, Mario
    Fritz, and Yang Zhang. 2020. $\{$Updates-Leak$\}$: Data Set Inference and Reconstruction
    Attacks in Online Learning. In *29th USENIX security symposium (USENIX Security
    20)*. 1291–1308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sato et al. (2021) Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jia, Xue
    Lin, and Qi Alfred Chen. 2021. Dirty road can attack: Security of deep learning
    based automated lane centering under $\{$Physical-World$\}$ attack. In *30th USENIX
    Security Symposium (USENIX Security 21)*. 3309–3326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sayles et al. (2021) Athena Sayles, Ashish Hooda, Mohit Gupta, Rahul Chatterjee,
    and Earlence Fernandes. 2021. Invisible perturbations: Physical adversarial examples
    exploiting the rolling shutter effect. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 14666–14675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmalfuss et al. (2023) Jenny Schmalfuss, Lukas Mehl, and Andrés Bruhn. 2023.
    Distracting Downpour: Adversarial Weather Attacks for Motion Estimation. *arXiv
    preprint arXiv:2305.06716* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Serban et al. (2020) Alex Serban, Erik Poll, and Joost Visser. 2020. Adversarial
    examples on object recognition: A comprehensive survey. *ACM Computing Surveys
    (CSUR)* 53, 3 (2020), 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shamsabadi et al. (2020a) Ali Shahin Shamsabadi, Changjae Oh, and Andrea Cavallaro.
    2020a. EdgeFool: an adversarial image enhancement filter. In *ICASSP 2020-2020
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.
    IEEE, 1898–1902.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shamsabadi et al. (2021) Ali Shahin Shamsabadi, Changjae Oh, and Andrea Cavallaro.
    2021. Semantically adversarial learnable filters. *IEEE Transactions on Image
    Processing* 30 (2021), 8075–8087.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shamsabadi et al. (2020b) Ali Shahin Shamsabadi, Ricardo Sanchez-Matilla, and
    Andrea Cavallaro. 2020b. Colorfool: Semantic adversarial colorization. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 1151–1160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharif et al. (2016) Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K
    Reiter. 2016. Accessorize to a crime: Real and stealthy attacks on state-of-the-art
    face recognition. In *Proceedings of the 2016 acm sigsac conference on computer
    and communications security*. 1528–1540.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharif et al. (2019) Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K
    Reiter. 2019. A general framework for adversarial examples with objectives. *ACM
    Transactions on Privacy and Security (TOPS)* 22, 3 (2019), 1–30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2021) Meng Shen, Hao Yu, Liehuang Zhu, Ke Xu, Qi Li, and Jiankun
    Hu. 2021. Effective and robust physical-world attacks on deep learning face recognition
    systems. *IEEE Transactions on Information Forensics and Security* 16 (2021),
    4063–4077.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shokri et al. (2017) Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
    Shmatikov. 2017. Membership inference attacks against machine learning models.
    In *2017 IEEE symposium on security and privacy (SP)*. IEEE, 3–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shukla et al. (2021) Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott,
    and Zico Kolter. 2021. Simple and efficient hard label black-box adversarial attacks
    in low query budget regimes. In *Proceedings of the 27th ACM SIGKDD Conference
    on Knowledge Discovery & Data Mining*. 1461–1469.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2018) Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. 2018.
    Constructing unrestricted adversarial examples with generative models. *Advances
    in Neural Information Processing Systems* 31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2019) Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai.
    2019. One pixel attack for fooling deep neural networks. *IEEE Transactions on
    Evolutionary Computation* 23, 5 (2019), 828–841.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) Jiachen Sun, Yulong Cao, Qi Alfred Chen, and Z Morley Mao.
    2020. Towards robust $\{$LiDAR-based$\}$ perception in autonomous driving: General
    black-box adversarial sensor attack and countermeasures. In *29th USENIX Security
    Symposium (USENIX Security 20)*. 877–894.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suryanto et al. (2022) Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma
    Larasati, Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, and Howon Kim.
    2022. DTA: Physical Camouflage Attacks using Differentiable Transformation Network.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    15305–15314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2013) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties
    of neural networks. *arXiv preprint arXiv:1312.6199* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2023) Yunbo Tao, Daizong Liu, Pan Zhou, Yulai Xie, Wei Du, and
    Wei Hu. 2023. 3DHacker: Spectrum-based Decision Boundary Generation for Hard-label
    3D Point Cloud Attack. *arXiv preprint arXiv:2308.07546* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tramèr et al. (2017a) Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian
    Goodfellow, Dan Boneh, and Patrick McDaniel. 2017a. Ensemble adversarial training:
    Attacks and defenses. *arXiv preprint arXiv:1705.07204* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tramèr et al. (2017b) Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan
    Boneh, and Patrick McDaniel. 2017b. The space of transferable adversarial examples.
    *arXiv preprint arXiv:1704.03453* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsai et al. (2020) Tzungyu Tsai, Kaichen Yang, Tsung-Yi Ho, and Yier Jin. 2020.
    Robust adversarial objects against deep learning models. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, Vol. 34. 954–962.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. (2019) Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang,
    Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. 2019. Autozoom: Autoencoder-based
    zeroth order optimization method for attacking black-box neural networks. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 33\. 742–749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tu et al. (2020) James Tu, Mengye Ren, Sivabalan Manivasagam, Ming Liang, Bin
    Yang, Richard Du, Frank Cheng, and Raquel Urtasun. 2020. Physically realizable
    adversarial examples for lidar object detection. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 13716–13725.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) Donghua Wang, Tingsong Jiang, Jialiang Sun, Weien Zhou,
    Zhiqiang Gong, Xiaoya Zhang, Wen Yao, and Xiaoqian Chen. 2022a. Fca: Learning
    a 3d full-coverage vehicle camouflage for multi-view physical adversarial attack.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36\.
    2414–2422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu
    Tang, and Xianglong Liu. 2021b. Dual attention suppression attack: Generate adversarial
    camouflage in physical world. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 8565–8574.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and He (2021) Xiaosen Wang and Kun He. 2021. Enhancing the transferability
    of adversarial attacks through variance tuning. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 1924–1933.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) Xiaosen Wang, Zeliang Zhang, Kangheng Tong, Dihong Gong,
    Kun He, Zhifeng Li, and Wei Liu. 2022b. Triangle attack: A query-efficient decision-based
    adversarial attack. In *European Conference on Computer Vision*. Springer, 156–174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M
    Bronstein, and Justin M Solomon. 2019. Dynamic graph cnn for learning on point
    clouds. *Acm Transactions On Graphics (tog)* 38, 5 (2019), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021c) Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yu-an
    Tan, and Quanxin Zhang. 2021c. Demiguise attack: Crafting invisible semantic adversarial
    perturbations with perceptual similarity. *arXiv preprint arXiv:2107.01396* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Zhibo Wang, Hengchang Guo, Zhifei Zhang, Wenxin Liu, Zhan
    Qin, and Kui Ren. 2021a. Feature importance-aware transferable adversarial attacks.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*.
    7639–7648.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Zhipeng Wei, Jingjing Chen, Zuxuan Wu, and Yu-Gang Jiang.
    2022. Cross-Modal Transferable Adversarial Attacks from Images to Videos. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 15064–15073.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2020) Yuxin Wen, Jiehong Lin, Ke Chen, CL Philip Chen, and Kui Jia.
    2020. Geometry-aware generation of adversarial point clouds. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wicker and Kwiatkowska (2019) Matthew Wicker and Marta Kwiatkowska. 2019. Robustness
    of 3d deep learning in an adversarial setting. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 11767–11775.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong et al. (2019) Eric Wong, Frank Schmidt, and Zico Kolter. 2019. Wasserstein
    adversarial examples via projected sinkhorn iterations. In *International Conference
    on Machine Learning*. PMLR, 6808–6817.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2018) Lei Wu, Zhanxing Zhu, Cheng Tai, et al. 2018. Understanding
    and enhancing the transferability of adversarial examples. *arXiv preprint arXiv:1802.09707*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021) Weibin Wu, Yuxin Su, Michael R Lyu, and Irwin King. 2021. Improving
    the transferability of adversarial samples with adversarial transformations. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    9024–9033.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2018a) Chaowei Xiao, Bo Li, Jun Yan Zhu, Warren He, Mingyan Liu,
    and Dawn Song. 2018a. Generating adversarial examples with adversarial networks.
    In *27th International Joint Conference on Artificial Intelligence, IJCAI 2018*.
    International Joint Conferences on Artificial Intelligence, 3905–3911.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2019) Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu.
    2019. Meshadv: Adversarial meshes for visual recognition. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 6898–6907.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2018b) Chaowei Xiao, Jun Yan Zhu, Bo Li, Warren He, Mingyan Liu,
    and Dawn Song. 2018b. Spatially transformed adversarial examples. In *6th International
    Conference on Learning Representations, ICLR 2018*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2021) Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei Gao,
    Xiaolu Zhang, Jun Zhou, and Jun Zhu. 2021. Improving transferability of adversarial
    patches on face recognition with generative models. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 11845–11854.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2017) Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi
    Xie, and Alan Yuille. 2017. Adversarial examples for semantic segmentation and
    object detection. In *Proceedings of the IEEE international conference on computer
    vision*. 1369–1378.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2019) Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang,
    Zhou Ren, and Alan L Yuille. 2019. Improving transferability of adversarial examples
    with input diversity. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 2730–2739.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2020) Jiancheng Yang, Yangzhou Jiang, Xiaoyang Huang, Bingbing
    Ni, and Chenglong Zhao. 2020. Learning black-box attackers with transferable priors
    and query feedback. *Advances in Neural Information Processing Systems* 33 (2020),
    12288–12299.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin et al. (2021) Bangjie Yin, Wenxuan Wang, Taiping Yao, Junfeng Guo, Zelun
    Kong, Shouhong Ding, Jilin Li, and Cong Liu. 2021. Adv-Makeup: A New Imperceptible
    and Transferable Attack on Face Recognition. *International Joint Conferences
    on Artificial Intelligence (IJCAI)* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2022) Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu.
    2022. Availability attacks create shortcuts. In *Proceedings of the 28th ACM SIGKDD
    Conference on Knowledge Discovery and Data Mining*. 2367–2376.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2019) Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi
    Xie, Yu-Wing Tai, Chi-Keung Tang, and Alan L Yuille. 2019. Adversarial attacks
    beyond the image space. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*. 4302–4311.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In So Kweon.
    2020b. Understanding adversarial examples from the mutual influence of images
    and perturbations. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 14521–14530.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Chaoning Zhang, Philipp Benz, Adil Karjauv, and In So Kweon.
    2021. Universal adversarial perturbations through the lens of deep steganography:
    Towards a fourier perspective. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, Vol. 35\. 3296–3304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Hanwei Zhang, Yannis Avrithis, Teddy Furon, and Laurent
    Amsaleg. 2020a. Smooth adversarial examples. *EURASIP Journal on Information Security*
    2020, 1 (2020), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018a) Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S
    Dhillon, and Cho-Jui Hsieh. 2018a. The Limitations of Adversarial Training and
    the Blind-Spot Attack. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Jianping Zhang, Weibin Wu, Jen-tse Huang, Yizhan Huang,
    Wenxuan Wang, Yuxin Su, and Michael R Lyu. 2022b. Improving Adversarial Transferability
    via Neuron Attribution-Based Attacks. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*. 14993–15002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022a) Qingzhao Zhang, Shengtuo Hu, Jiachen Sun, Qi Alfred Chen,
    and Z Morley Mao. 2022a. On adversarial robustness of trajectory prediction for
    autonomous vehicles. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*. 15159–15168.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2016) Richard Zhang, Phillip Isola, and Alexei A Efros. 2016.
    Colorful image colorization. In *European conference on computer vision*. Springer,
    649–666.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Yang Zhang, Hassan Foroosh, Philip David, and Boqing Gong.
    2018b. CAMOU: Learning physical vehicle camouflages to adversarially attack detectors
    in the wild. In *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2021a) Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and
    Vladlen Koltun. 2021a. Point transformer. In *Proceedings of the IEEE/CVF international
    conference on computer vision*. 16259–16268.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019) He Zhao, Trung Le, Paul Montague, Olivier De Vel, Tamas
    Abraham, and Dinh Phung. 2019. Perturbations are not enough: Generating adversarial
    examples with spatial distortions. *arXiv preprint arXiv:1910.01329* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020b) Yue Zhao, Yuwei Wu, Caihua Chen, and Andrew Lim. 2020b.
    On isometry robustness of deep 3d point cloud models under adversarial attacks.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    1201–1210.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2018. Generating
    Natural Adversarial Examples. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020a) Zhengyu Zhao, Zhuoran Liu, and Martha Larson. 2020a. Towards
    large yet imperceptible adversarial image perturbations with perceptual color
    distance. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*. 1039–1048.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021b) Zhengyu Zhao, Zhuoran Liu, and Martha Larson. 2021b. On
    success and simplicity: A second look at transferable targeted attacks. *Advances
    in Neural Information Processing Systems* 34 (2021), 6115–6128.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2019a) Tianhang Zheng, Changyou Chen, and Kui Ren. 2019a. Distributionally
    adversarial attack. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 33\. 2253–2260.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2019b) Tianhang Zheng, Changyou Chen, Junsong Yuan, Bo Li, and
    Kui Ren. 2019b. Pointcloud saliency maps. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 1598–1606.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020) Hang Zhou, Dongdong Chen, Jing Liao, Kejiang Chen, Xiaoyi
    Dong, Kunlin Liu, Weiming Zhang, Gang Hua, and Nenghai Yu. 2020. Lg-gan: Label
    guided adversarial network for flexible targeted attack of point cloud based deep
    networks. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*. 10356–10365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2018) Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang,
    Xiang Gan, and Yong Yang. 2018. Transferable adversarial perturbations. In *Proceedings
    of the European Conference on Computer Vision (ECCV)*. 452–467.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. (2023) Haomin Zhuang, Yihua Zhang, and Sijia Liu. 2023. A pilot
    study of query-free adversarial attack against stable diffusion. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2384–2391.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2020) Junhua Zou, Zhisong Pan, Junyang Qiu, Xin Liu, Ting Rui, and
    Wei Li. 2020. Improving the transferability of adversarial examples with resized-diverse-inputs,
    diversity-ensemble and region fitting. In *European Conference on Computer Vision*.
    Springer, 563–579.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
