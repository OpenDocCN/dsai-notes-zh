- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:42:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:42:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2301.00114] Skeletal Video Anomaly Detection using Deep Learning: Survey,
    Challenges and Future Directions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2301.00114] 使用深度学习的骨架视频异常检测：综述、挑战与未来方向'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2301.00114](https://ar5iv.labs.arxiv.org/html/2301.00114)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2301.00114](https://ar5iv.labs.arxiv.org/html/2301.00114)
- en: 'Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and
    Future Directions'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习的骨架视频异常检测：综述、挑战与未来方向
- en: 'Pratik K. Mishra, Alex Mihailidis, Shehroz S. Khan Pratik K. Mishra, Alex Mihailidis,
    and Shehroz S. Khan are with the Institute of Biomedical Engineering, University
    of Toronto, Toronto, Canada, and also with the KITE – Toronto Rehabilitation Institute,
    University Health Network, Toronto, Canada (e-mail: pratik.mishra@mail.utoronto.ca;
    alex.mihailidis@utoronto.ca; shehroz.khan@uhn.ca).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pratik K. Mishra, Alex Mihailidis, Shehroz S. Khan Pratik K. Mishra, Alex Mihailidis,
    和 Shehroz S. Khan 均来自多伦多大学生物医学工程研究所，加拿大多伦多，并且也与KITE – 多伦多康复研究所、大学健康网络，加拿大多伦多有联系（电子邮件：pratik.mishra@mail.utoronto.ca;
    alex.mihailidis@utoronto.ca; shehroz.khan@uhn.ca）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The existing methods for video anomaly detection mostly utilize videos containing
    identifiable facial and appearance-based features. The use of videos with identifiable
    faces raises privacy concerns, especially when used in a hospital or community-based
    setting. Appearance-based features can also be sensitive to pixel-based noise,
    straining the anomaly detection methods to model the changes in the background
    and making it difficult to focus on the actions of humans in the foreground. Structural
    information in the form of skeletons describing the human motion in the videos
    is privacy-protecting and can overcome some of the problems posed by appearance-based
    features. In this paper, we present a survey of privacy-protecting deep learning
    anomaly detection methods using skeletons extracted from videos. We present a
    novel taxonomy of algorithms based on the various learning approaches. We conclude
    that skeleton-based approaches for anomaly detection can be a plausible privacy-protecting
    alternative for video anomaly detection. Lastly, we identify major open research
    questions and provide guidelines to address them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的视频异常检测方法大多利用包含可识别面部和外观特征的视频。使用可识别面部的视频引发了隐私问题，特别是在医院或社区环境中使用时。基于外观的特征也可能对像素噪声敏感，迫使异常检测方法在建模背景变化时面临挑战，使得难以集中注意力于前景中的人类行为。以骨架形式描述视频中人类运动的结构信息能够保护隐私，并且可以克服一些基于外观特征所带来的问题。本文介绍了使用从视频中提取的骨架进行隐私保护深度学习异常检测方法的综述。我们提出了一种基于各种学习方法的算法新分类法。我们得出结论，基于骨架的异常检测方法可以成为视频异常检测的一个可行的隐私保护替代方案。最后，我们识别了主要的开放研究问题并提供了应对这些问题的指南。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: skeleton, body joint, human pose, anomaly detection, video.^(^(©2024 IEEE. Personal
    use of this material is permitted. Permission from IEEE must be obtained for all
    other uses, in any current or future media, including reprinting/republishing
    this material for advertising or promotional purposes, creating new collective
    works, for resale or redistribution to servers or lists, or reuse of any copyrighted
    component of this work in other works.)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 骨架，身体关节，人类姿势，异常检测，视频。^(^(©2024 IEEE。本材料的个人使用是被允许的。所有其他用途，包括在任何当前或未来的媒体中转载/重刊本材料用于广告或宣传目的、创建新的集体作品、转售或再分发到服务器或列表中，或在其他作品中重用本作品的任何版权组件，都必须获得IEEE的许可。)
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Anomalous events pertain to unusual or abnormal actions, behaviours or situations
    that can lead to health, safety and economical risks [[1](#bib.bib1)]. Anomalous
    events, by definition, are largely unseen and not much is known about them in
    advance [[2](#bib.bib2)]. Due to their rarity, diversity and infrequency, collecting
    labeled data for anomalous events can be very difficult or costly [[1](#bib.bib1),
    [3](#bib.bib3)]. With the lack of predetermined classes and a few labelled data
    for anomalous events, it can be very hard to train supervised machine learning
    models [[1](#bib.bib1)]. Therefore, a general approach in majority of anomaly
    detection algorithms is to train a model that can best represent the ’normal’
    events or actions, and any deviations from it can be flagged as an unseen anomaly
    [[4](#bib.bib4)]. Anomalous behaviours among humans can be attributed at an individual
    level (e.g., falls [[5](#bib.bib5)]) or multiple people in a scene (e.g., pedestrian
    crossing [[6](#bib.bib6)], violence in a crowded mall [[7](#bib.bib7)]). In the
    context of video-based anomaly detection, the general approach is to train a model
    to learn the patterns of actions or behaviours of individual(s), background and
    other semantic information in the normal activities videos, and identify significant
    deviations in the test videos as anomalies. However, anomaly detection is a challenging
    task due to the lack of labels and often times the unclear definition of an anomaly
    [[2](#bib.bib2)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 异常事件指的是不寻常或不正常的行为、举动或情况，这些可能导致健康、安全和经济风险[[1](#bib.bib1)]。根据定义，异常事件大多是未被发现的，并且事先对其了解甚少[[2](#bib.bib2)]。由于其稀有性、多样性和不频繁性，收集标记数据可能非常困难或昂贵[[1](#bib.bib1),
    [3](#bib.bib3)]。由于缺乏预定类别和标记数据，训练监督学习模型可能非常困难[[1](#bib.bib1)]。因此，大多数异常检测算法的通用方法是训练一个可以最佳代表“正常”事件或行为的模型，任何偏离此模型的情况都可以被标记为未见异常[[4](#bib.bib4)]。人类的异常行为可以归因于个体层面（例如，跌倒[[5](#bib.bib5)]）或场景中的多人（例如，行人过马路[[6](#bib.bib6)]，拥挤商场中的暴力行为[[7](#bib.bib7)]）。在基于视频的异常检测中，一般方法是训练一个模型来学习正常活动视频中个体的行为模式、背景和其他语义信息，并在测试视频中识别出显著的偏差作为异常。然而，由于标签的缺乏以及异常定义的不明确，异常检测是一项具有挑战性的任务[[2](#bib.bib2)]。
- en: The majority of video-based anomaly detection approaches use RGB videos where
    the people in the scene are identifiable. While using RGB camera-based systems
    in public places (e.g., malls, airports) is generally acceptable, the situation
    can be very different in personal dwelling, community, residential or clinical
    settings [[8](#bib.bib8)]. In a home or residential setting (e.g., nursing homes),
    individuals or patients can be monitored in their personal space that may breach
    their privacy. The lack of measures to deal with the privacy of individuals can
    be a bottleneck in the adoption and deployment of the anomaly detection-based
    systems [[9](#bib.bib9)]. However, monitoring of people with physical, cognitive
    or aging issues is also important to improve their quality of life and care. Therefore,
    as a trade-off, privacy-protecting video modalities can fill that gap and be used
    in these settings to save lives and improve patient care. Wearable devices face
    compliance issues among certain populations, where people may forget or in some
    cases refuse to wear them [[10](#bib.bib10)]. Some of the privacy-protecting camera
    modalities that has been used in the past for anomaly detection involving humans
    include depth cameras [[5](#bib.bib5), [11](#bib.bib11)], thermal cameras [[12](#bib.bib12)],
    and infrared cameras [[13](#bib.bib13), [14](#bib.bib14)]. While these modalities
    can partially or fully obfuscate an individual’s identity, they require specialized
    hardware or cameras and can be expensive to be used by general population. Skeletons
    extracted from RGB camera streams using pose estimation algorithms provide a suitable
    solution of privacy protection over RGB and other types of cameras [[15](#bib.bib15)].
    Skeleton tracking only focuses on body joints and ignores facial identity, full
    body scan or background information. The pixel-based features in RGB videos that
    mask important information about the scene are sensitive to noise resulting from
    illumination, viewing direction and background clutter, resulting in false positives
    when detecting anomalies [[16](#bib.bib16)]. Furthermore, due to redundant information
    present in these features (e.g., background), there is an increased burden on
    methods to model the change in those areas of the scene rather than focus on the
    actions of humans in the foreground. Extracting information specific to human
    actions can not only provide a privacy-protecting solution, but can also help
    to filter out the background-related noise in the videos and aid the model to
    focus on key information for detecting abnormal events related to human behaviour.
    The skeletons represent an efficient way to model the human body joint positions
    over time and are robust to the complex background, illumination changes, and
    dynamic camera scenes [[17](#bib.bib17)]. In addition to being privacy-protecting,
    skeleton features are compact, well-structured, semantically rich, and highly
    descriptive about human actions and motion [[17](#bib.bib17)]. Anomaly detection
    using skeleton tracking is an emerging area of research as awareness around privacy
    of individuals and their data grows. However, skeleton-based approaches may not
    be sufficient for situations that explicitly need facial information for analysis,
    including emotion recognition [[18](#bib.bib18), [19](#bib.bib19)], pain detection
    [[20](#bib.bib20)] or remote heart monitoring [[21](#bib.bib21)], to name a few.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于视频的异常检测方法使用的是RGB视频，其中场景中的人物可以被识别。虽然在公共场所（例如商场、机场）使用RGB摄像头系统通常是可以接受的，但在个人住宅、社区、居住或临床环境中情况可能会大相径庭[[8](#bib.bib8)]。在家庭或居住环境（例如养老院）中，个人或患者可能会在其个人空间中被监控，这可能侵犯他们的隐私。缺乏应对个人隐私的措施可能会成为异常检测系统采纳和部署的瓶颈[[9](#bib.bib9)]。然而，监测身体、认知或老龄化问题的人群对于提高他们的生活质量和护理也是重要的。因此，作为一种折衷，隐私保护的视频模式可以填补这一空白，并在这些环境中使用以挽救生命和改善患者护理。可穿戴设备在某些人群中面临合规问题，人们可能会忘记或在某些情况下拒绝佩戴它们[[10](#bib.bib10)]。过去用于涉及人的异常检测的一些隐私保护摄像机模式包括深度摄像机[[5](#bib.bib5),
    [11](#bib.bib11)]、热成像摄像机[[12](#bib.bib12)]和红外摄像机[[13](#bib.bib13), [14](#bib.bib14)]。虽然这些模式可以部分或完全遮蔽个人身份，但它们需要专门的硬件或摄像头，并且普通人群使用时可能会很昂贵。利用姿态估计算法从RGB摄像机流中提取的骨架提供了一种适合隐私保护的解决方案，优于RGB和其他类型的摄像头[[15](#bib.bib15)]。骨架跟踪仅关注身体关节，忽略面部身份、全身扫描或背景信息。RGB视频中的像素特征会掩盖场景中的重要信息，对光照、视角和背景杂乱所造成的噪声非常敏感，这可能会导致检测异常时的误报[[16](#bib.bib16)]。此外，由于这些特征中存在冗余信息（例如背景），方法在建模这些区域的变化时负担加重，而不是集中在前景中人类的动作上。提取特定于人类行为的信息不仅可以提供隐私保护的解决方案，还可以帮助过滤视频中的背景噪声，并帮助模型专注于检测与人类行为相关的异常事件的关键信息。骨架表示了一种有效的方式来建模人体关节位置随时间的变化，并且对复杂背景、光照变化和动态摄像机场景具有鲁棒性[[17](#bib.bib17)]。除了保护隐私外，骨架特征还具有紧凑、结构良好、语义丰富和对人类动作和运动高度描述的优点[[17](#bib.bib17)]。使用骨架跟踪进行异常检测是一个新兴的研究领域，因为对个人隐私及其数据的关注日益增加。然而，基于骨架的方法可能不足以应对需要面部信息进行分析的情况，包括情感识别[[18](#bib.bib18),
    [19](#bib.bib19)]、疼痛检测[[20](#bib.bib20)]或远程心脏监测[[21](#bib.bib21)]等。
- en: 'TABLE I: Summary of reviewed papers.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：综述论文总结。
- en: '| Learning approach | Paper | Datasets used | Experimental Setting | Number
    of people in scene | Type of anomalies | Pose estimation algorithm | Model input
    | Model type | Anomaly score | Eval. metric AUC(ROC) (or other) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 学习方法 | 论文 | 使用的数据集 | 实验设置 | 场景中的人数 | 异常类型 | 姿态估计算法 | 模型输入 | 模型类型 | 异常分数 |
    评估指标 AUC(ROC)（或其他） |'
- en: '| Reconstruction | Gatt et al. [[22](#bib.bib22)] | UTD-MHAD | Indoor | Single
    | Irregular body postures | Openpose, Posenet | Skeleton keypoints | 1DConv-AE,
    LSTM-AE | Reconstruction error | AUC(PR)=0.91, F score=0.98 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 重建 | Gatt 等人 [[22](#bib.bib22)] | UTD-MHAD | 室内 | 单一 | 不规则体态 | Openpose，Posenet
    | 骨架关键点 | 1DConv-AE，LSTM-AE | 重建误差 | AUC(PR)=0.91，F 分数=0.98 |'
- en: '| Temuroglu et al. [[23](#bib.bib23)] | Custom | Outdoor | Multiple | Drunk
    walking | Openpose | Skeleton keypoints | AE | Reconstruction error | Average
    of recall and specificity=0.91 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| Temuroglu 等人 [[23](#bib.bib23)] | 自定义 | 室外 | 多种 | 醉酒行走 | Openpose | 骨架关键点
    | AE | 重建误差 | 召回率和特异性的平均值=0.91 |'
- en: '| Suzuki et al. [[24](#bib.bib24)] | Custom | — | Single | Poor body movements
    in children | Openpose | Motion time- series images | CAE | Reconstruction error
    | Accuracy=99.3, F score=0.99 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Suzuki 等人 [[24](#bib.bib24)] | 自定义 | — | 单一 | 儿童动作不良 | Openpose | 动作时间序列图像
    | CAE | 重建误差 | 准确率=99.3，F 分数=0.99 |'
- en: '| Jiang et al. [[25](#bib.bib25)] | Custom | Outdoor | Multiple | Abnormal
    pedestrian behaviours at grade crossings | Alphapose | Skeleton keypoints | GRU
    Encoder- Decoder | Reconstruction error | 0.82 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Jiang 等人 [[25](#bib.bib25)] | 自定义 | 室外 | 多种 | 等级交叉口的异常行人行为 | Alphapose |
    骨架关键点 | GRU 编码器-解码器 | 重建误差 | 0.82 |'
- en: '| Song et al. [[26](#bib.bib26)] | Custom | Outdoor | Multiple | Abnormal pedestrian
    behaviours at grade crossings | Openpose | Skeleton keypoints | GAN | Discriminator
    score | 0.89 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Song 等人 [[26](#bib.bib26)] | 自定义 | 室外 | 多种 | 等级交叉口的异常行人行为 | Openpose | 骨架关键点
    | GAN | 判别器分数 | 0.89 |'
- en: '|  | Fan et al. [[27](#bib.bib27)] | CUHK Avenue, UMN | Indoor and Outdoor
    | Multiple | Anomalous human behaviours | Alphapose | Video frame, Skeleton keypoints
    | GAN | Reconstruction error of video frame | 0.88 0.99 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Fan 等人 [[27](#bib.bib27)] | 香港中文大学大道，明尼苏达大学 | 室内和室外 | 多种 | 异常人类行为 | Alphapose
    | 视频帧，骨架关键点 | GAN | 视频帧重建误差 | 0.88 0.99 |'
- en: '| Prediction | Rodrigues et al. [[28](#bib.bib28)] | IITB-Corridor, ShanghaiTech,
    CUHK Avenue | Outdoor | Multiple | Abnormal human activities | Openpose | Skeleton
    keypoints | Multi-timescale 1DConv encoder-decoder | Prediction error from different
    timescales | 0.67 0.76 0.83 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | Rodrigues 等人 [[28](#bib.bib28)] | IITB-走廊，上海科技大学，香港中文大学大道 | 室外 | 多种
    | 异常人类活动 | Openpose | 骨架关键点 | 多时间尺度 1DConv 编码器-解码器 | 不同时间尺度的预测误差 | 0.67 0.76 0.83
    |'
- en: '| Luo et al. [[16](#bib.bib16)] | ShanghaiTech, CUHK Avenue | Outdoor | Multiple
    | Irregular body postures | Alphapose | Skeleton joints graph | Spatio-Temporal
    GCN | Prediction error | 0.74 0.87 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Luo 等人 [[16](#bib.bib16)] | 上海科技大学，香港中文大学大道 | 室外 | 多种 | 不规则体态 | Alphapose
    | 骨架关节图 | 时空 GCN | 预测误差 | 0.74 0.87 |'
- en: '| Zeng et al. [[29](#bib.bib29)] | UCSD Pedestrian, ShanghaiTech, CUHK Avenue,
    IITB-Corridor | Outdoor | Multiple | Anomalous human behaviours | HRNet | Skeleton
    joints graph | Hierarchical Spatio-Temporal GCN | Weighted sum of prediction errors
    from different levels | 0.98 0.82 0.87 0.7 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Zeng 等人 [[29](#bib.bib29)] | UCSD 行人，上海科技大学，香港中文大学大道，IITB-走廊 | 室外 | 多种 |
    异常人类行为 | HRNet | 骨架关节图 | 分层时空 GCN | 不同层次的预测误差加权和 | 0.98 0.82 0.87 0.7 |'
- en: '| Fan et al. [[30](#bib.bib30)] | ShanghaiTech, CUHK Avenue | Outdoor | Multiple
    | Anomalous human actions | Alphapose | Skeleton keypoints | GRU feed forward
    network | Prediction error | 0.83 0.92 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Fan 等人 [[30](#bib.bib30)] | 上海科技大学，香港中文大学大道 | 室外 | 多种 | 异常人类行为 | Alphapose
    | 骨架关键点 | GRU 前馈网络 | 预测误差 | 0.83 0.92 |'
- en: '| Pang et al. [[31](#bib.bib31)] | ShanghaiTech, CUHK Avenue | Outdoor | Multiple
    | Anomalous human actions | Alphapose | Skeleton keypoints | Transformer | Prediction
    error | 0.77 0.87 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Pang 等人 [[31](#bib.bib31)] | 上海科技大学，香港中文大学大道 | 室外 | 多种 | 异常人类行为 | Alphapose
    | 骨架关键点 | Transformer | 预测误差 | 0.77 0.87 |'
- en: '|  | Huang et al. [[32](#bib.bib32)] | ShanghaiTech, CUHK Avenue, IITB-Corridor
    | Outdoor | Multiple | Anomalous human behaviours | HRNet | Skeleton joints graph
    | Spatio-Temporal Graph Transformer | Max of prediction errors of local & global
    graphs | 0.83 0.89 0.77 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Huang 等人 [[32](#bib.bib32)] | 上海科技大学，香港中文大学大道，IITB-走廊 | 室外 | 多种 | 异常人类行为
    | HRNet | 骨架关节图 | 时空图 Transformer | 局部和全局图的预测误差最大值 | 0.83 0.89 0.77 |'
- en: '| Reconstruction+ Prediction | Morais et al. [[17](#bib.bib17)] | ShanghaiTech,
    CUHK Avenue | Outdoor | Multiple | Anomalous human actions | Alphapose | Skeleton
    keypoints | GRU Encoder- Decoder | Weighted sum of reconstruction and prediction
    errors | 0.73 0.86 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 重构+预测 | Morais 等人 [[17](#bib.bib17)] | 上海科技大学, CUHK Avenue | 户外 | 多个 | 异常人类动作
    | Alphapose | 骨架关键点 | GRU 编码器-解码器 | 重构和预测误差的加权和 | 0.73 0.86 |'
- en: '| Boekhoudt et al. [[7](#bib.bib7)] | ShanghaiTech, HR Crime | Indoor and Outdoor
    | Multiple | Human and Crime related anomalies | Alphapose | Skeleton keypoints
    | GRU Encoder- Decoder | Weighted sum of reconstruction and prediction errors
    | 0.73 0.6 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Boekhoudt 等人 [[7](#bib.bib7)] | 上海科技大学, HR Crime | 室内和户外 | 多个 | 与人类和犯罪相关的异常
    | Alphapose | 骨架关键点 | GRU 编码器-解码器 | 重构和预测误差的加权和 | 0.73 0.6 |'
- en: '| Li and Zhang [[33](#bib.bib33)] | ShanghaiTech | Outdoor | Multiple | Abnormal
    pedestrian behaviours | Alphapose | Skeleton keypoints | GRU Encoder- Decoder
    | Weighted sum of reconstruction and prediction errors | 0.75 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 李和张 [[33](#bib.bib33)] | 上海科技大学 | 户外 | 多个 | 异常行人行为 | Alphapose | 骨架关键点 |
    GRU 编码器-解码器 | 重构和预测误差的加权和 | 0.75 |'
- en: '| Li et al. [[34](#bib.bib34)] | ShanghaiTech, CUHK Avenue | Outdoor | Multiple
    | Human-related anomalous events | Alphapose | Skeleton joints graph | GCAE with
    embedded LSTM | Sum of max reconstruction and prediction errors | 0.76, EER=30.7
    0.84, EER=20.7 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 李等人 [[34](#bib.bib34)] | 上海科技大学, CUHK Avenue | 户外 | 多个 | 人相关的异常事件 | Alphapose
    | 骨架关节图 | 嵌入 LSTM 的 GCAE | 最大重构和预测误差之和 | 0.76, EER=30.7 0.84, EER=20.7 |'
- en: '| Wu et al. [[35](#bib.bib35)] | ShanghaiTech, CUHK Avenue | Outdoor | Multiple
    | Abnormal human actions | Alphapose | Skeleton joints graph, Confidence scores
    | GCN | Confidence score weighted sum of reconstruction, prediction and SVDD errors
    | 0.77 0.85 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 吴等人 [[35](#bib.bib35)] | 上海科技大学, CUHK Avenue | 户外 | 多个 | 异常人类动作 | Alphapose
    | 骨架关节图, 置信度分数 | GCN | 置信度评分加权的重构、预测和 SVDD 误差之和 | 0.77 0.85 |'
- en: '|  | Luo et al. [[36](#bib.bib36)] | ShanghaiTech, IITB-Corridor | Outdoor
    | Multiple | Human-related video anomalies | — | Skeleton joints graph | Memory
    Enhanced Spatial-Temporal GCAE | Sum of max reconstruction and prediction errors
    | 0.78 0.69 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | 罗等人 [[36](#bib.bib36)] | 上海科技大学, IITB-走廊 | 户外 | 多个 | 人相关的视频异常 | — | 骨架关节图
    | 记忆增强型时空 GCAE | 最大重构和预测误差之和 | 0.78 0.69 |'
- en: '|  | Li et al. [[37](#bib.bib37)] | ShanghaiTech, CUHK Avenue | Outdoor | Multiple
    | Human-related anomalous events | Alphapose | Skeleton keypoints | Memory-augmented
    GAN | Sum of max reconstruction and prediction errors | 0.75, EER=31.7 0.84, EER=22.6
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | 李等人 [[37](#bib.bib37)] | 上海科技大学, CUHK Avenue | 户外 | 多个 | 人相关的异常事件 | Alphapose
    | 骨架关键点 | 记忆增强型 GAN | 最大重构和预测误差之和 | 0.75, EER=31.7 0.84, EER=22.6 |'
- en: 'TABLE II: Summary of reviewed papers (continued).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：审阅论文的总结（续）。
- en: '| Learning approach | Paper | Datasets used | Experimental Setting | Number
    of people in scene | Type of anomalies | Pose estimation algorithm | Model input
    | Model type | Anomaly score | Eval. metric AUC(ROC) (or other) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 学习方法 | 论文 | 使用的数据集 | 实验设置 | 场景中的人数 | 异常类型 | 姿势估计算法 | 模型输入 | 模型类型 | 异常分数 |
    评估指标 AUC(ROC)（或其他） |'
- en: '| Reconstruction+ Clustering | Markovitz et al. [[38](#bib.bib38)] | ShanghaiTech,
    NTU-RGB+D, Kinetics-250 | Indoor and Outdoor | Multiple | Anomalous human actions
    | Alphapose, Openpose | Skeleton joints graph | GCAE, Deep clustering | Dirichlet
    process mixture model score | 0.75 0.85 0.74 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 重构+聚类 | Markovitz 等人 [[38](#bib.bib38)] | 上海科技大学, NTU-RGB+D, Kinetics-250
    | 室内和户外 | 多个 | 异常人类动作 | Alphapose, Openpose | 骨架关节图 | GCAE, 深度聚类 | Dirichlet 过程混合模型评分
    | 0.75 0.85 0.74 |'
- en: '| Cui et al. [[39](#bib.bib39)] | ShanghaiTech | Outdoor | Multiple | Human
    pose anomalies | — | Skeleton joints graph | GCAE, Deep clustering | Dirichlet
    process mixture model score | 0.77 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 崔等人 [[39](#bib.bib39)] | 上海科技大学 | 户外 | 多个 | 人体姿势异常 | — | 骨架关节图 | GCAE, 深度聚类
    | Dirichlet 过程混合模型评分 | 0.77 |'
- en: '| Liu et al. [[40](#bib.bib40)] | ShanghaiTech, CUHK Avenue | Outdoor | Multiple
    | Anomalous human behaviours | Alphapose | Skeleton joints graph | GCAE, Deep
    clustering | Dirichlet process mixture model score | 0.79 0.88 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 刘等人 [[40](#bib.bib40)] | 上海科技大学, CUHK Avenue | 户外 | 多个 | 异常人类行为 | Alphapose
    | 骨架关节图 | GCAE, 深度聚类 | Dirichlet 过程混合模型评分 | 0.79 0.88 |'
- en: '|  | Chen et al. [[41](#bib.bib41)] | ShanghaiTech, CUHK Avenue | Outdoor |
    Multiple | Anomalous human behaviours | Alphapose | Skeleton joints graph | Multiscale
    spatial temporal attention GCN | Skeleton cluster anomaly score | 0.76, EER=31.1
    0.88, EER=19.2 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | 陈等人 [[41](#bib.bib41)] | 上海科技大学, CUHK Avenue | 户外 | 多目标 | 异常的人类行为 | Alphapose
    | 骨骼关节图 | 多尺度时空注意力 GCN | 骨骼簇异常分数 | 0.76, EER=31.1 0.88, EER=19.2 |'
- en: '|  | Yan et al. [[42](#bib.bib42)] | ShanghaiTech, UCF-Crime, NTU-RGB+D | Outdoor
    | Multiple | Anomalous human actions | Openpose | Skeleton joints graph | GCAE,
    Deep clustering | Skeleton cluster anomaly score | 0.77 0.76 0.77 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | 闫等人 [[42](#bib.bib42)] | 上海科技大学, UCF-Crime, NTU-RGB+D | 户外 | 多目标 | 异常的人类动作
    | Openpose | 骨骼关节图 | GCAE, 深度聚类 | 骨骼簇异常分数 | 0.77 0.76 0.77 |'
- en: '| Clustering | Yang et al. [[43](#bib.bib43)] | UCSD Pedestrian 2, ShanghaiTech
    | Outdoor | Multiple | Anomalous human behaviours and objects | Alphapose | Skeleton
    joints graph, Numerical features | GCN | Skeleton cluster + Object anomaly score
    | 0.93 0.82 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | 杨等人 [[43](#bib.bib43)] | UCSD 行人 2, 上海科技大学 | 户外 | 多目标 | 异常的人类行为和物体 |
    Alphapose | 骨骼关节图，数值特征 | GCN | 骨骼簇 + 物体异常分数 | 0.93 0.82 |'
- en: '| Javed et al. [[44](#bib.bib44)] | ShanghaiTech, UCF-Crime, NTU-RGB+D | Outdoor
    | Multiple | Anomalous human actions | — | Video frame, Skeleton joints graph
    | GCN, Deep clustering | Dirichlet process mixture model score | 0.81 0.86 0.88
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 贾维德等人 [[44](#bib.bib44)] | 上海科技大学, UCF-Crime, NTU-RGB+D | 户外 | 多目标 | 异常的人类动作
    | — | 视频帧，骨骼关节图 | GCN, 深度聚类 | Dirichlet 过程混合模型分数 | 0.81 0.86 0.88 |'
- en: '| Iterative self- training | Nanjun et al. [[45](#bib.bib45)] | ShanghaiTech,
    CUHK Avenue | Outdoor | Multiple | Human-related anomalous events | Alphapose
    | Skeleton joints graph, Numerical features | GCN | Self-trained fully connected
    layers output | 0.72, EER=34.1 0.82, EER=23.9 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 迭代自我训练 | 南俊等人 [[45](#bib.bib45)] | 上海科技大学, CUHK Avenue | 户外 | 多目标 | 与人类相关的异常事件
    | Alphapose | 骨骼关节图，数值特征 | GCN | 自我训练的全连接层输出 | 0.72, EER=34.1 0.82, EER=23.9 |'
- en: '| Multivariate gaussian distribution | Tani and Shibata [[46](#bib.bib46)]
    | ShanghaiTech | Outdoor | Multiple | Anomalous human behaviours | Openpose |
    Skeleton joints graph | GCN, Multivariate gaussian distribution | Mahalanobis
    distance | 0.77 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 多变量高斯分布 | 谭和柴田 [[46](#bib.bib46)] | 上海科技大学 | 户外 | 多目标 | 异常的人类行为 | Openpose
    | 骨骼关节图 | GCN, 多变量高斯分布 | Mahalanobis 距离 | 0.77 |'
- en: '| Prompt-guided zero- shot learning | Sato et al. [[47](#bib.bib47)] | RWF-2000
    Kinetics-250 | Outdoor | Multiple | Abnormal-human behavior events | PPN HRNet
    | Skeleton keypoints | Residual multilayer perceptron | Joint probability score
    | Accuracy=90.3 0.79 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 提示指导的零样本学习 | 佐藤等人 [[47](#bib.bib47)] | RWF-2000 Kinetics-250 | 户外 | 多目标 |
    异常人类行为事件 | PPN HRNet | 骨骼关键点 | 残差多层感知机 | 联合概率分数 | 准确率=90.3 0.79 |'
- en: In recent years, deep learning methods have been developed to use skeletons
    for different applications, such as action recognition [[48](#bib.bib48)], medical
    diagnosis [[24](#bib.bib24)], and sports analytics [[49](#bib.bib49)]. The use
    of skeletons for anomaly detection in videos is an under-explored area, and concerted
    research is needed [[24](#bib.bib24)]. The human skeletons can help in developing
    privacy-preserving solutions for private dwellings, crowded/public areas, medical
    settings, rehabilitation centers and long-term care homes to detect anomalous
    events that impact health and safety of individuals. Use of this type of approach
    could improve the adoption of video-based monitoring systems in homes and residential
    settings. However, there is a paucity of literature on understanding the existing
    techniques that use skeleton-based anomaly detection approaches. We identify this
    gap in the literature and present one of the first surveys on the recent advancements
    in using skeletons for anomaly detection in videos. We identified the major themes
    in existing work and present a novel taxonomy that is based on how these methods
    learn to detect anomalous events. We also discuss the applications where these
    approaches were used to understand their potential in bringing these algorithms
    in a personal dwelling, or long-term care scenario.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习方法已被开发用于利用骨架进行各种应用，如动作识别 [[48](#bib.bib48)]、医学诊断 [[24](#bib.bib24)]
    和体育分析 [[49](#bib.bib49)]。在视频中利用骨架进行异常检测仍是一个未被充分探索的领域，需要集中研究 [[24](#bib.bib24)]。人类骨架可以帮助开发隐私保护解决方案，用于私人住宅、拥挤/公共区域、医疗环境、康复中心和长期护理机构，以检测影响个人健康和安全的异常事件。这种方法的使用可以提高视频监控系统在家庭和住宅环境中的采用。然而，关于理解现有利用骨架进行异常检测的方法的文献相对稀缺。我们识别了这一文献空白，并提出了其中之一的首个调查，涉及利用骨架进行视频异常检测的最新进展。我们识别了现有工作的主要主题，并提出了一种基于这些方法如何学习检测异常事件的新型分类法。我们还讨论了这些方法被应用的场景，以理解它们在个人住宅或长期护理情境中的潜力。
- en: II Literature Survey
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 文献综述
- en: 'We adopted a narrative literature review for this work. The following keywords
    (and their combinations) were used to search for relevant papers – skeleton, human
    pose, body pose, body joint, anomaly detection, and video. These keywords were
    searched on scholarly databases, including Google Scholar, IEEE Xplore, Elsevier
    and Springer. We mostly reviewed papers between year 2016 to 2023; therefore,
    the list may not be comprehensive. In this review, we only focus on the recent
    deep learning-based algorithms for skeletal video anomaly detection and do not
    include traditional machine learning-based approaches. There are works [[50](#bib.bib50),
    [51](#bib.bib51)] on detecting anomalous behaviour using supervised approaches,
    however, it is outside the scope of this review as it focuses on unsupervised
    anomaly detection approaches. We did not adopt the systematic or scoping review
    search protocol for this work; therefore, our literature review may not be exhaustive.
    However, we tried our best to include the latest development in the field to be
    able to summarize their potential and identify challenges. In this section, we
    provide a survey of skeletal deep learning video anomaly detection methods. We
    present a novel taxonomy to study the skeletal video anomaly approaches based
    on learning approaches into four broad categories, i.e., reconstruction, prediction,
    their combinations and other specific approaches. Table [I](#S1.T1 "TABLE I ‣
    I Introduction ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey,
    Challenges and Future Directions") and [II](#S1.T2 "TABLE II ‣ I Introduction
    ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and
    Future Directions") provides a summary of 29 relevant papers, based on the taxonomy,
    found in our literature search. Unless otherwise specified, the values in the
    last column of the table refer to AUC(ROC) values corresponding to each dataset
    in the reviewed paper. Six papers use reconstruction approach, six papers use
    prediction approach, seven papers use a combination of reconstruction and prediction
    approaches, five papers use a combination of reconstruction and clustering approaches,
    and five papers use other specific approaches.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在这项工作中采用了叙述性文献综述的方法。以下关键词（及其组合）被用来搜索相关论文——骨架、人体姿态、身体姿态、身体关节、异常检测和视频。这些关键词在学术数据库中进行了搜索，包括
    Google Scholar、IEEE Xplore、Elsevier 和 Springer。我们主要审阅了2016年至2023年间的论文，因此，这份列表可能并不全面。在这次综述中，我们只关注基于深度学习的骨架视频异常检测算法，并不包括传统的基于机器学习的方法。虽然有一些工作[[50](#bib.bib50)、[51](#bib.bib51)]使用监督方法来检测异常行为，但这些超出了本综述的范围，因为本综述关注的是无监督异常检测方法。我们没有采用系统性或范围性综述搜索协议，因此我们的文献综述可能并不详尽。然而，我们尽力包括了该领域的最新发展，以便能够总结它们的潜力并识别挑战。在本节中，我们提供了骨架深度学习视频异常检测方法的调查。我们提出了一种新的分类法，将骨架视频异常检测方法基于学习方法分为四大类，即重建、预测、它们的组合以及其他特定方法。表格
    [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Skeletal Video Anomaly Detection using
    Deep Learning: Survey, Challenges and Future Directions") 和 [II](#S1.T2 "TABLE
    II ‣ I Introduction ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey,
    Challenges and Future Directions") 总结了基于我们文献搜索的分类法找到的29篇相关论文。除非另有说明，表格最后一列的值指的是审阅论文中每个数据集对应的AUC(ROC)值。六篇论文使用重建方法，六篇论文使用预测方法，七篇论文使用重建和预测方法的组合，五篇论文使用重建和聚类方法的组合，五篇论文使用其他特定方法。'
- en: II-A Reconstruction Approaches
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 重建方法
- en: In the reconstruction approaches, generally, an autoencoder (AE) or its variant
    model is trained on the skeleton information of only normal human activities.
    During training, the model learns to reconstruct the samples representing normal
    activities with low reconstruction error. Hence, when the model encounters an
    anomalous sample at test time, it is expected to give high reconstruction error.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在重建方法中，一般情况下，自动编码器（AE）或其变体模型只在正常人类活动的骨架信息上进行训练。在训练过程中，模型学习以低重建误差重建代表正常活动的样本。因此，当模型在测试时遇到异常样本时，预计会给出较高的重建误差。
- en: Gatt et al. [[22](#bib.bib22)] used Long Short-Term Memory (LSTM) and 1-Dimensional
    Convolution (1DConv)-based AE models to detect abnormal human activities, including,
    but not limited to falls, using skeletons estimated from videos of a publicly
    available dataset. Temuroglu et al. [[23](#bib.bib23)] proposed a skeleton trajectory
    representation that handled occlusions and an AE framework for pedestrian abnormal
    behaviour detection. The pedestrian video dataset used in this work was collected
    by the authors, where the training dataset was composed of normal walking, and
    the test dataset was composed of normal and drunk walking. The pose skeletons
    were treated to handle occlusions using the proposed representation and combined
    into a sequence to train an AE. They compared the results of occlusion-aware skeleton
    keypoints input with keypoints without occlusion flags, keypoint image heatmaps
    and raw pedestrian image inputs. The authors used average of recall and specificity
    to evaluate the models due to the unbalanced dataset and found that occlusion-aware
    input achieved the highest results. Suzuki et al. [[24](#bib.bib24)] trained a
    Convolutional AE (CAE) on good gross motor movements in children and detected
    poor limb motion as an anomaly. Motion time-series images [[52](#bib.bib52)] were
    obtained from skeletons estimated from the videos of kindergarten children participants.
    The motion time-series images were fed as input to a CAE, which was trained on
    only the normal data. The difference between the input and reconstructed pixels
    was used to localize the poor body movements in anomalous frames. Jiang et al.
    [[25](#bib.bib25)] presented a message passing Gated Recurrent Unit (GRU) encoder-decoder
    network to detect and localize the anomalous pedestrian behaviours in videos captured
    at the grade crossing. The field-collected dataset consisted of over 50 hours
    of video recordings at two selected grade crossings with different camera angles.
    The skeletons were estimated and decomposed into global and local components before
    being fed as input to the encoder-decoder network. The localization of the anomalous
    pedestrians within a frame was done by identifying the skeletons with reconstruction
    error higher than the empirical threshold. They manually removed wrongly detected
    false skeletons as they claim that the wrong detection issue was observed at only
    one grade crossing. However, an approach of manual removal of false skeletons
    is impractical in many real world applications where the data is very large, making
    the need of an automated false skeleton identification and removal step imperative.
    In their following work [[26](#bib.bib26)], the authors improved the performance
    of detecting abnormal pedestrian behaviors at grade crossings using a generative
    adversarial network (GAN)-based framework. Two LSTM-based branches within the
    generator were used to analyze both local and global motion patterns simultaneously,
    reconstructing the corresponding inputs in the temporal domain. The discriminator
    was a fully connected neural network and produced a score representing the likelihood
    of inputs being an anomaly. Fan et al. [[27](#bib.bib27)] proposed an anomaly
    detection framework which consisted of two pairs of generator and discriminator.
    The generators were trained to reconstruct the normal video frames and the corresponding
    skeletons, respectively. The discriminators were trained to distinguish the original
    and reconstructed video frames and the original and reconstructed skeletons, respectively.
    The video frames and corresponding extracted skeletons served as input to the
    framework during training; however, at test time, decision was made based on only
    reconstruction error of video frames.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Gatt 等人 [[22](#bib.bib22)] 使用了基于长短期记忆（LSTM）和一维卷积（1DConv）的自编码器（AE）模型来检测异常的人类活动，包括但不限于摔倒，利用从公开数据集视频中估算的骨架。Temuroglu
    等人 [[23](#bib.bib23)] 提出了一个处理遮挡的骨架轨迹表示方法和一个用于行人异常行为检测的AE框架。该工作使用的行人视频数据集由作者收集，其中训练数据集由正常步态组成，测试数据集则由正常步态和醉酒步态组成。通过提出的表示方法处理遮挡的姿态骨架被组合成序列以训练AE。他们比较了处理遮挡的骨架关键点输入与未标记遮挡的关键点、关键点图像热图和原始行人图像输入的结果。由于数据集不平衡，作者使用了召回率和特异性的平均值来评估模型，并发现处理遮挡的输入取得了最高的结果。Suzuki
    等人 [[24](#bib.bib24)] 训练了一个卷积自编码器（CAE）来检测儿童的良好粗大运动，并将差的肢体运动检测为异常。运动时间序列图像 [[52](#bib.bib52)]
    从估算的幼儿园儿童视频中获得。这些运动时间序列图像被作为输入送入CAE中，CAE仅在正常数据上进行训练。输入和重建像素之间的差异用于定位异常帧中的差的身体运动。Jiang
    等人 [[25](#bib.bib25)] 提出了一个消息传递的门控循环单元（GRU）编码解码网络，以检测和定位在铁路道口捕获的视频中的异常行人行为。现场收集的数据集包含了在两个不同相机角度的选择道口处超过50小时的视频录制。骨架被估算并分解为全局和局部组件，然后作为输入送入编码解码网络。通过识别重建误差高于经验阈值的骨架来定位帧中的异常行人。他们手动移除了错误检测的虚假骨架，因为他们声称错误检测问题只在一个道口出现。然而，在许多实际应用中，由于数据量非常大，手动移除虚假骨架的方法不切实际，因此需要一个自动化的虚假骨架识别和移除步骤。在随后的工作
    [[26](#bib.bib26)] 中，作者使用基于生成对抗网络（GAN）的框架提高了铁路道口异常行人行为检测的性能。生成器中的两个基于LSTM的分支用于同时分析局部和全局运动模式，在时间域中重建相应的输入。判别器是一个全连接神经网络，生成一个表示输入为异常可能性的分数。Fan
    等人 [[27](#bib.bib27)] 提出了一个由两对生成器和判别器组成的异常检测框架。生成器被训练以重建正常视频帧和相应的骨架。判别器被训练以区分原始和重建的视频帧以及原始和重建的骨架。视频帧和相应提取的骨架作为框架训练期间的输入；然而，在测试时，决策仅基于视频帧的重建误差。
- en: Challenges
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战
- en: AEs or their variants are widely used in many video-based anomaly detection
    methods [[5](#bib.bib5)]. The choice of the right architecture to model the skeletons
    is very important. Further, being trained on the normal data, they are expected
    to produce higher reconstruction error for the abnormal inputs than the normal
    inputs, which has been adopted as a criterion for identifying anomalies. However,
    this assumption does not always hold in practice, that is, the AEs can generalize
    well that it can also reconstruct anomalies well, leading to false negatives [[53](#bib.bib53)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器或其变体在许多基于视频的异常检测方法中被广泛使用[[5](#bib.bib5)]。选择合适的架构来建模骨架非常重要。此外，由于在正常数据上训练，它们预计会对异常输入产生比正常输入更高的重建误差，这已被采用作为识别异常的标准。然而，这一假设在实践中并不总是成立，即自编码器可能表现得很好，也能重建异常，导致假阴性[[53](#bib.bib53)]。
- en: II-B Prediction Approaches
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 预测方法
- en: In prediction approaches, a network is generally trained to learn the normal
    human behaviour by predicting the skeletons at the next time step(s) using the
    skeletons representing normal human actions at past time steps. During testing,
    the test samples with high prediction errors are flagged as anomalies as the network
    is trained to predict only the skeletons representing normal actions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测方法中，网络通常通过使用表示正常人类行为的骨架来预测下一时间步的骨架，从而学习正常的人类行为。在测试期间，高预测误差的测试样本被标记为异常，因为网络仅被训练来预测代表正常行为的骨架。
- en: Rodrigues et al. [[28](#bib.bib28)] suggested that abnormal human activities
    can take place at different timescales, and the methods that operate at a fixed
    timescale (frame-based or video-clip-based) are not enough to capture the wide
    range of anomalies occurring with different time duration. They proposed a multi-timescale
    1DConv encoder-decoder network where the intermediate layers were responsible
    to generate future and past predictions corresponding to different timescales.
    The network was trained to make predictions on normal activity skeletons input.
    The prediction errors from all timescales were combined to get an anomaly score
    to detect abnormal activities. Luo et al. [[16](#bib.bib16)] proposed a spatio-temporal
    Graph Convolutional Network (GCN)-based prediction method for skeleton-based video
    anomaly detection. The body joints were estimated and built into skeleton graphs,
    where the body joints formed the nodes of the graph. The spatial edges connected
    different joints of a skeleton, and temporal edges connected the same joints across
    time. A fully connected layer was used at the end of the network to predict future
    skeletons. Zeng et al. [[29](#bib.bib29)] proposed a hierarchical spatio-temporal
    GCN, where high-level representations encoded the trajectories of people and the
    interactions among multiple identities while low-level skeleton graph representations
    encoded the local body posture of each person. The method was proposed to detect
    anomalous human behaviours in both sparse and dense scenes. The inputs were organized
    into spatio-temporal skeleton graphs whose nodes were human body joints from multiple
    frames and fed to the network. The network was trained on the input skeleton graph
    representations of normal activities. Optical flow fields and size of skeleton
    bounding boxes were used to determine sparse and dense scenes. For dense scenes
    with crowds, higher weights were assigned to high-level representations while
    for sparse scenes, the weights of low-level graph representations were increased.
    During testing, the prediction errors from different branches were weighted and
    combined to obtain the final anomaly score. Fan et al. [[30](#bib.bib30)] proposed
    a GRU feed-forward network that was trained to predict the next skeleton using
    past skeleton sequences and a loss function that incorporated the range and speed
    of the predicted skeletons. Pang et al. [[31](#bib.bib31)] proposed a skeleton
    transformer to predict future pose components in video frames and considered error
    between predicted pose components and corresponding expected values as anomaly
    score. They applied a multi-head self-attention module to capture long-range dependencies
    between arbitrary pairwise pose components and the temporal convolutional layer
    to concentrate on local temporal information. Huang et al. [[32](#bib.bib32)]
    proposed a spatio-temporal graph transformer to encode the hierarchical graph
    embeddings of human skeletons for jointly modeling the interactions between individuals
    and the correlations among body joints within a single individual. Input to the
    transformer was provided as global and local graphs. Each node in the global graph
    encoded the speed of an individual as well as the relative position and interaction
    relations between individuals. Each local graph encoded the pose of an individual.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Rodrigues 等人 [[28](#bib.bib28)] 提出异常的人类活动可以在不同的时间尺度上发生，固定时间尺度（基于帧或视频片段）的方法无法捕捉到不同时间持续的广泛异常。他们提出了一个多时间尺度的
    1DConv 编码器-解码器网络，其中中间层负责生成不同时间尺度的未来和过去预测。该网络被训练以对正常活动骨架输入进行预测。所有时间尺度的预测误差被结合起来，得到一个异常分数用于检测异常活动。Luo
    等人 [[16](#bib.bib16)] 提出了一种基于时空图卷积网络（GCN）的预测方法用于骨架视频异常检测。身体关节被估计并构建成骨架图，其中身体关节构成图的节点。空间边连接骨架的不同关节，时间边连接跨时间的相同关节。网络末端使用了一个全连接层来预测未来的骨架。Zeng
    等人 [[29](#bib.bib29)] 提出了一种层次化时空 GCN，其中高级表示编码了人员的轨迹和多个身份之间的互动，而低级骨架图表示编码了每个人的局部身体姿态。该方法旨在检测稀疏和密集场景中的异常人类行为。输入被组织成时空骨架图，其中的节点是来自多个帧的人体关节，并输入到网络中。网络在正常活动的输入骨架图表示上进行训练。光流场和骨架边界框的大小用于确定稀疏和密集场景。对于拥挤的密集场景，高级表示被分配了更高的权重，而对于稀疏场景，则增加了低级图表示的权重。在测试过程中，不同分支的预测误差被加权和结合以获得最终的异常分数。Fan
    等人 [[30](#bib.bib30)] 提出了一个 GRU 前馈网络，该网络被训练以使用过去的骨架序列预测下一个骨架，并且损失函数结合了预测骨架的范围和速度。Pang
    等人 [[31](#bib.bib31)] 提出了一种骨架变换器来预测视频帧中的未来姿态组件，并将预测姿态组件与相应期望值之间的误差视为异常分数。他们应用了多头自注意模块以捕捉任意成对姿态组件之间的长期依赖关系，并使用时间卷积层集中于局部时间信息。Huang
    等人 [[32](#bib.bib32)] 提出了一种时空图变换器，用于编码人类骨架的层次化图嵌入，以联合建模个体之间的互动以及单个个体内的身体关节之间的关联。变换器的输入包括全球图和局部图。全球图中的每个节点编码了个体的速度以及个体之间的相对位置和互动关系。每个局部图编码了个体的姿态。
- en: Challenges
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战
- en: In these methods, it is difficult to choose how far in future (or past) the
    prediction should be made to achieve optimum results. This could potentially be
    determined empirically; however, in the absence of a validation set such solutions
    remain elusive. The future prediction-based methods can be sensitive to noise
    in the past data [[54](#bib.bib54)]. Any small changes in the past can result
    in significant variation in prediction, and not all of these changes signify anomalous
    situations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，很难选择预测应当向未来（或过去）多远进行，以实现最佳结果。这可能可以通过经验确定；然而，在缺乏验证集的情况下，这些解决方案仍然难以实现。基于未来预测的方法可能对过去数据中的噪声非常敏感
    [[54](#bib.bib54)]。过去的任何小变化都可能导致预测的显著变化，并且这些变化并非所有都表示异常情况。
- en: II-C Combinations of learning approaches
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 学习方法的组合
- en: In this section, we discuss the existing methods that utilize a combination
    of different learning approaches, namely, reconstruction and prediction approaches,
    and reconstruction and clustering approaches.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了现有的方法，这些方法利用了不同学习方法的组合，即重建和预测方法，以及重建和聚类方法。
- en: II-C1 Combination of reconstruction and prediction approaches
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 重建和预测方法的组合
- en: 'Some skeletal video anomaly detection methods utilize a multi-objective loss
    function consisting of both reconstruction and prediction errors to learn the
    characteristics of skeletons signifying normal behaviour and identify skeletons
    with large errors as anomalies. Morais et al. [[17](#bib.bib17)] proposed a method
    to model the normal human movements in surveillance videos using human skeletons
    and their relative positions in the scene. The human skeletons were decomposed
    into two sub-components: global body movement and local body posture. The global
    movement tracked the dynamics of the whole body in the scene, while the local
    posture described the skeleton configuration. The two components were passed as
    input to different branches of a message passing GRU single-encoder-dual-decoder-based
    network. The branches processed their data separately and interacted via cross-branch
    message passing at each time step. Each branch had an encoder, a reconstruction-based
    decoder and a prediction-based decoder. The network was trained using normal data,
    and during testing, a frame-level anomaly score was generated by aggregating the
    anomaly scores of all the skeletons in a frame to identify anomalous frames. In
    order to avoid the inaccuracy caused by incorrect detection of skeletons in video
    frames, the authors left out video frames where the skeletons cannot be estimated
    by the pose estimation algorithm. Hence, the results in this work was not a good
    representation of a real-world scenario, which often consists of complex-scenes
    with occluding objects and overlapping movement of people. Boekhoudt et al. [[7](#bib.bib7)]
    utilized the network proposed by Morais et al. [[17](#bib.bib17)] for detecting
    human crime-based anomalies in videos using a newly proposed crime-based video
    surveillance dataset. Similar to the work by Morais et al. [[17](#bib.bib17)],
    Li and Zhang [[33](#bib.bib33)] proposed a dual branch single-encoder-dual-decoder
    GRU network that was trained on normal behaviour skeletons estimated from pedestrian
    videos. The two decoders were responsible for reconstructing the input skeletons
    and predicting future skeletons, respectively. However, unlike the work by Morais
    et al. [[17](#bib.bib17)], there was no provision of message passing between the
    branches. Li et al. [[34](#bib.bib34)] proposed a single-encoder-dual-decoder
    architecture established on a spatio-temporal Graph CAE (GCAE) embedded with a
    LSTM network in hidden layers. The two decoders were used to reconstruct the input
    skeleton sequences and predict the unseen future sequences, respectively, from
    the latent vectors projected via the encoder. The sum of maximum reconstruction
    and prediction errors among all the skeletons within a frame was used as anomaly
    score for detecting anomalous frames. Wu et al. [[35](#bib.bib35)] proposed a
    GCN-based encoder-decoder architecture that was trained using normal action skeleton
    graphs and keypoint confidence scores as input to detect anomalous human actions
    in surveillance videos. The skeleton graph input was decomposed into global and
    local components. The network consisted of three encoder-decoder pipelines: the
    global pipeline, the local pipeline and the confidence score pipeline. The global
    and local encoder-decoder-based pipelines learned to reconstruct and predict the
    global and local components, respectively. The confidence score pipeline learned
    to reconstruct the confidence scores. Further, a Support Vector Data Description
    (SVDD)-based loss was employed to learn the boundary of the normal action global
    and local pipeline encoder output in latent feature space. The network was trained
    using a multi-objective loss function, composed of a weighted sum of skeleton
    graph reconstruction and prediction losses, confidence score reconstruction loss
    and multi-center SVDD loss. Luo et al. [[36](#bib.bib36)] proposed a single-encoder-dual-decoder
    memory enhanced spatial-temporal GCAE network, where spatial-temporal graph convolution
    was used to encode discriminative features of skeleton graphs in spatial and temporal
    domains. The memory module recorded patterns for normal behaviour skeletons. Further,
    the encoded representation was not fed directly into the reconstructing and predicting
    decoders but was used as a query to retrieve the most relevant memory items. The
    memory module was used to restrain the reconstruction and prediction capability
    of the network on anomalies. Li et al. [[37](#bib.bib37)] proposed memory-augmented
    Wasserstein GAN with gradient penalty to predict future human skeleton trajectories
    from a given past and reconstruct the given past simultaneously. While the discriminator
    attempted to fit the Wasserstein distance between the distribution of real and
    generated samples, the generator tried to minimize the Wasserstein distance to
    draw the distribution of real and generated samples closer. A memory module was
    applied in the generator to mitigate the strong generalization ability.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一些骨架视频异常检测方法利用包含重建和预测误差的多目标损失函数，以学习骨架特征，这些特征代表正常行为，并将具有大误差的骨架识别为异常。Morais 等人
    [[17](#bib.bib17)] 提出了一个方法，通过使用人类骨架及其在场景中的相对位置来建模监控视频中的正常人类动作。人类骨架被分解为两个子组件：全局身体运动和局部身体姿势。全局运动跟踪场景中整个身体的动态，而局部姿势描述骨架配置。这两个组件被作为输入传递到基于消息传递
    GRU 单编码器双解码器的网络的不同分支中。这些分支分别处理它们的数据，并在每个时间步通过跨分支消息传递进行交互。每个分支都有一个编码器、一个基于重建的解码器和一个基于预测的解码器。该网络使用正常数据进行训练，在测试期间，通过汇总一帧中所有骨架的异常分数来生成帧级异常分数，以识别异常帧。为了避免因视频帧中骨架检测不准确而导致的误差，作者排除了无法通过姿态估计算法估计的骨架的视频帧。因此，这项工作的结果并不能很好地代表真实世界场景，真实场景通常由复杂的场景、遮挡物和人群重叠运动组成。Boekhoudt
    等人 [[7](#bib.bib7)] 使用了 Morais 等人 [[17](#bib.bib17)] 提出的网络来检测视频中的基于犯罪的人类异常，使用了新提出的基于犯罪的视频监控数据集。与
    Morais 等人 [[17](#bib.bib17)] 的工作类似，Li 和 Zhang [[33](#bib.bib33)] 提出了一个双分支单编码器双解码器
    GRU 网络，该网络在从行人视频中估计的正常行为骨架上进行训练。这两个解码器分别负责重建输入骨架和预测未来骨架。然而，与 Morais 等人 [[17](#bib.bib17)]
    的工作不同的是，两个分支之间没有提供消息传递。Li 等人 [[34](#bib.bib34)] 提出了一个基于时空图卷积自编码器 (GCAE) 的单编码器双解码器架构，该架构在隐藏层中嵌入了
    LSTM 网络。这两个解码器分别用于重建输入骨架序列和预测未见过的未来序列，从通过编码器投影的潜在向量中获得。所有骨架在一帧中的最大重建和预测误差之和被用作异常分数，以检测异常帧。Wu
    等人 [[35](#bib.bib35)] 提出了一个基于图卷积网络 (GCN) 的编码器-解码器架构，该架构使用正常动作骨架图和关键点置信度分数作为输入进行训练，以检测监控视频中的异常人类动作。骨架图输入被分解为全局和局部组件。网络由三个编码器-解码器管道组成：全局管道、局部管道和置信度分数管道。全局和局部编码器-解码器管道分别学习重建和预测全局和局部组件。置信度分数管道学习重建置信度分数。此外，使用基于支持向量数据描述
    (SVDD) 的损失来学习正常动作全局和局部管道编码器输出在潜在特征空间中的边界。网络使用多目标损失函数进行训练，由骨架图重建和预测损失、置信度分数重建损失以及多中心
    SVDD 损失的加权和组成。Luo 等人 [[36](#bib.bib36)] 提出了一个单编码器双解码器的记忆增强时空 GCAE 网络，其中使用时空图卷积来编码骨架图在空间和时间域中的区分特征。记忆模块记录正常行为骨架的模式。此外，编码的表示没有直接输入到重建和预测解码器中，而是作为查询用于检索最相关的记忆项。记忆模块用于限制网络在异常情况下的重建和预测能力。Li
    等人 [[37](#bib.bib37)] 提出了基于梯度惩罚的记忆增强 Wasserstein GAN，以从给定的过去预测未来的人类骨架轨迹，并同时重建给定的过去。虽然判别器试图拟合真实样本和生成样本之间的
    Wasserstein 距离，但生成器则试图最小化 Wasserstein 距离，以使真实样本和生成样本的分布更接近。生成器中应用了记忆模块，以缓解强泛化能力。
- en: II-C2 Combination of reconstruction and clustering approaches
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 重建与聚类方法的结合
- en: Some skeletal video anomaly detection methods utilize a two-stage approach to
    identify anomalous human actions using spatio-temporal skeleton graphs. In the
    first pre-training stage, a GCAE-based model is trained to minimize the reconstruction
    loss on input skeleton graphs. In the second fine-tuning stage, the latent features
    generated by the pre-trained GCAE encoder is fed to a clustering layer and a Dirichlet
    Process Mixture model is used to estimate the distribution of the soft assignment
    of feature vectors to clusters. Finally at the test time, the Dirichlet normality
    score is used to identify the anomalous samples. Markovitz et al. [[38](#bib.bib38)]
    identified that anomalous actions can be broadly classified in two categories,
    fine and coarse-grained anomalies. Fine-grained anomaly detection refers to detecting
    abnormal variations of an action, e.g., abnormal type of walking. Coarse-grained
    anomaly detection refers to defining particular normal actions and regarding other
    actions as abnormal, such as determining dancing as normal and gymnastics as abnormal.
    They utilized a spatio-temporal GCAE to map the skeleton graphs representing normal
    actions to a latent space, which was soft assigned to clusters using a deep clustering
    layer. The soft-assignment representation abstracted the type of data (fine or
    coarse-grained) from the Dirichlet model. After pre-training of GCAE, the latent
    feature output of the encoder and clusters were fine-tuned by minimizing a multi-objective
    loss function consisting of both the reconstruction loss and clustering loss.
    They leveraged ShanghaiTech [[55](#bib.bib55)] dataset to test the performance
    of their proposed method on fine-grained anomalies, and NTU-RGB+D[[56](#bib.bib56)]
    and Kinetics-250[[57](#bib.bib57)] datasets for coarse-grained anomaly detection
    performance evaluation. Cui et al. [[39](#bib.bib39)] proposed a semi-supervised
    prototype generation-based method for video anomaly detection to reduce the computational
    cost associated with graph-embedded networks. Skeleton graphs for normal actions
    were estimated from the videos and fed as input to a shift spatio-temporal GCAE
    to generate features. It was not clear which pose estimation algorithm was used
    to estimate the skeletons from video frames. The generated features were fed to
    the proposed prototype generation module designed to map the features to prototypes
    and update them during the training phase. In the pre-training step, the GCAE
    and prototype generation module were optimized using a loss function composed
    of reconstruction loss and generation loss of prototypes. In the fine-tuning step,
    the entire network was fine-tuned using a multi-objective loss function, composed
    of reconstruction loss, prototype generation loss and cluster loss. Later, Liu
    et al. [[40](#bib.bib40)] used self-attention augmented graph convolutions for
    detecting abnormal human behaviours based on skeleton graphs. Skeleton graphs
    were fed as input to a spatio-temporal self-attention augmented GCAE and latent
    features were extracted from the encoder part of the trained GCAE. After pre-training
    of GCAE, the entire network was fine-tuned using a multi-objective loss function
    consisting of both the reconstruction loss and clustering loss. Chen et al. [[41](#bib.bib41)]
    proposed a multiscale spatial temporal attention GCN, which included an encoder
    to extract features, a reconstruction decoder branch to optimize encoder, and
    a clustering layer branch to obtain anomaly scores. During training, the decoder
    is used to optimize the encoder by minimizing the reconstruction error. However,
    during testing, the decoder is discarded, and only the clustering layer is used
    to generate the anomaly score. It used three scales of human skeleton graphs,
    namely, joint, part and limb. Spatial attention graph convolution operation was
    carried out on each scale, and the output features of three scales were weighted
    and summed to constitute the multiscale skeleton features. Yan et al. [[42](#bib.bib42)]
    proposed a deep memory storage clustering method based on GCAE to implement the
    real-time updating of pseudo-labels and network parameters. It consisted of a
    feature extraction, autoencoder, clustering, memory storage, self-supervision
    and scoring modules. The feature extraction module [[38](#bib.bib38)] and the
    autoencoder module were used to form the reconstructed pose sequence. The reconstructed
    sequence was then sent to the memory storage module for storage, and the soft
    cluster assignment was performed on each sample through the k-means clustering
    method [[58](#bib.bib58)]. The autoencoder, clustering, and memory storage modules
    were used to update the pseudo-labels and network parameters iteratively.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一些骨架视频异常检测方法利用两阶段的方法，通过时空骨架图来识别异常的人体动作。在第一个预训练阶段，基于GCAE的模型被训练以最小化输入骨架图上的重构损失。在第二个微调阶段，由预训练的GCAE编码器生成的潜在特征被送入一个聚类层，并使用狄利克雷过程混合模型来估计特征向量到聚类的软分配的分布。最后，在测试时，使用狄利克雷正态性得分来识别异常样本。Markovitz
    等人 [[38](#bib.bib38)] 发现异常动作可以大致分为两类：细粒度异常和粗粒度异常。细粒度异常检测是指检测动作的异常变化，例如，异常的行走类型。粗粒度异常检测是指定义特定的正常动作，并将其他动作视为异常，例如，将跳舞定义为正常，将体操定义为异常。他们利用时空GCAE将代表正常动作的骨架图映射到潜在空间，然后使用深度聚类层将其软分配到聚类。软分配表示从狄利克雷模型中抽象出了数据类型（细粒度或粗粒度）。在GCAE的预训练后，通过最小化由重构损失和聚类损失组成的多目标损失函数，对编码器和聚类进行微调。他们利用了上海科技大学
    [[55](#bib.bib55)] 数据集来测试他们提出的方法在细粒度异常上的性能，NTU-RGB+D [[56](#bib.bib56)] 和 Kinetics-250
    [[57](#bib.bib57)] 数据集用于粗粒度异常检测性能评估。Cui 等人 [[39](#bib.bib39)] 提出了一种基于半监督原型生成的视频异常检测方法，以减少与图嵌入网络相关的计算成本。正常动作的骨架图从视频中估计出来，并作为输入送入一个移位时空GCAE以生成特征。尚不清楚使用了哪种姿态估计算法来从视频帧中估计骨架。生成的特征被送入提出的原型生成模块，该模块旨在将特征映射到原型，并在训练阶段更新它们。在预训练步骤中，GCAE和原型生成模块通过由重构损失和原型生成损失组成的损失函数进行优化。在微调步骤中，整个网络使用由重构损失、原型生成损失和聚类损失组成的多目标损失函数进行微调。随后，Liu
    等人 [[40](#bib.bib40)] 使用自注意力增强的图卷积来检测基于骨架图的异常人体行为。骨架图作为输入送入一个时空自注意力增强的GCAE，并从训练好的GCAE的编码器部分提取潜在特征。在GCAE的预训练后，整个网络使用由重构损失和聚类损失组成的多目标损失函数进行微调。Chen
    等人 [[41](#bib.bib41)] 提出了一个多尺度时空注意力GCN，其中包括一个编码器用于提取特征，一个重构解码器分支用于优化编码器，以及一个聚类层分支用于获取异常得分。在训练过程中，解码器用于通过最小化重构误差来优化编码器。然而，在测试过程中，解码器被丢弃，仅使用聚类层来生成异常得分。它使用了三种尺度的人体骨架图，即关节、部件和肢体。在每个尺度上进行了空间注意力图卷积操作，并对三种尺度的输出特征进行了加权和求和，以构成多尺度骨架特征。Yan
    等人 [[42](#bib.bib42)] 提出了基于GCAE的深度记忆存储聚类方法，以实现伪标签和网络参数的实时更新。它包括特征提取、自动编码器、聚类、记忆存储、自监督和评分模块。特征提取模块
    [[38](#bib.bib38)] 和自动编码器模块用于形成重构的姿态序列。重构序列随后被发送到记忆存储模块进行存储，并通过k均值聚类方法 [[58](#bib.bib58)]
    对每个样本进行软聚类分配。自动编码器、聚类和记忆存储模块用于迭代更新伪标签和网络参数。
- en: Challenges
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战
- en: 'The combination-based methods can carry the limitations of the individual learning
    approaches, as described in Section [II-A](#S2.SS1 "II-A Reconstruction Approaches
    ‣ II Literature Survey ‣ Skeletal Video Anomaly Detection using Deep Learning:
    Survey, Challenges and Future Directions") and [II-B](#S2.SS2 "II-B Prediction
    Approaches ‣ II Literature Survey ‣ Skeletal Video Anomaly Detection using Deep
    Learning: Survey, Challenges and Future Directions"). Further, in the absence
    of a validation set, it is difficult to determine the optimum value of combination
    coefficients in a multi-objective loss function.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '基于组合的方法可能会受到单一学习方法的限制，如在第[II-A](#S2.SS1 "II-A Reconstruction Approaches ‣ II
    Literature Survey ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey,
    Challenges and Future Directions")和[II-B](#S2.SS2 "II-B Prediction Approaches
    ‣ II Literature Survey ‣ Skeletal Video Anomaly Detection using Deep Learning:
    Survey, Challenges and Future Directions")节中所述。此外，在缺乏验证集的情况下，很难确定多目标损失函数中组合系数的最佳值。'
- en: II-D Other Approaches
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 其他方法
- en: This section discusses the methods that leveraged a pre-trained deep learning
    model to encode latent features from the input skeletons and used approaches such
    as, clustering and multivariate gaussian distribution, in conjunction for detecting
    human action-based anomalies in videos.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了利用预训练深度学习模型从输入骨架中编码潜在特征的方法，并结合如聚类和多元高斯分布等方法，用于检测视频中的基于人类动作的异常。
- en: Yang et al. [[43](#bib.bib43)] proposed a two-stream fusion method to detect
    anomalies pertaining to body movement and object positions. YOLOv3 [[59](#bib.bib59)]
    was used to detect people and objects in the video frames. Subsequently, skeletons
    were estimated from the video frames and passed as input to a spatio-temporal
    GCN, followed by a clustering-based fully connected layer to generate anomaly
    scores for skeletons. The information pertaining to the bounding box coordinates
    and confidence score of the detected objects was used to generate object anomaly
    scores. Finally, the skeleton and object normality scores were combined to generate
    the final anomaly score for a frame. Nanjun et al. [[45](#bib.bib45)] used the
    skeleton features estimated from the videos for pedestrian anomaly detection using
    an iterative self-training strategy. The training set consisted of unlabelled
    normal and anomalous video sequences. The skeletons were decomposed into global
    and local components, which were fed as input to an unsupervised anomaly detector,
    iForest [[60](#bib.bib60)], to yield the pseudo anomalous and normal skeleton
    sets. The pseudo sets were used to train an anomaly scoring module, consisting
    of a spatial GCN and fully connected layers with a single output unit. As part
    of the self-training strategy, new anomaly scores were generated using previously
    trained anomaly scoring module to update the membership of skeleton samples in
    the skeleton sets. The scoring module was then retrained using updated skeleton
    sets, until the best scoring model was obtained. However, the paper doesn’t discuss
    the criteria to decide the best scoring model. Tani and Shibata [[46](#bib.bib46)]
    proposed a framework for training a frame-wise Adaptive GCN (AGCN) for action
    recognition using single frame skeletons and used the features extracted from
    the AGCN to train an anomaly detection model. As part of the proposed framework,
    a pretrained action recognition model [[61](#bib.bib61)] was used to identify
    the frames with large temporal attention in the Kinetics-skeleton dataset [[62](#bib.bib62)]
    as the action frames to train the AGCN. Further, the trained AGCN was used to
    extract features from the normal behaviour skeletons identified in the ShanghaiTech
    Campus dataset [[17](#bib.bib17)] to model a multivariate gaussian distribution.
    During testing, the Mahalanobis distance was used to calculate the anomaly score
    under the multivariate gaussian distribution. Sato et al. [[47](#bib.bib47)] proposed
    a user prompt-guided zero-shot learning framework for the detection of abnormal
    human behaviour events. A multilayer perceptron feature extractor was pretrained
    on large-scale action recognition datasets [[63](#bib.bib63), [64](#bib.bib64)]
    using contrastive learning between the skeleton features and the text embeddings
    extracted from action class names. The distribution of skeleton features of the
    normal actions was modeled during training while freezing the weights of feature
    extractor. During inference, the anomaly score was computed using distribution
    and the text prompts of an unseen action. Javed et al. [[44](#bib.bib44)] proposed
    a unified framework for learning suitable frames of interest to cut down on redundant
    data and a two-stream feature block with a hyper-gated fusion model to take advantage
    of skeleton graph and video frame features. Soft assignments were later processed
    through a clustering layer, where probabilities were assigned to the instances
    and a normality score was calculated using the Dirichlet Process Mixture model
    [[65](#bib.bib65)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 杨等人[[43](#bib.bib43)] 提出了一个双流融合方法，用于检测与身体运动和物体位置相关的异常。YOLOv3 [[59](#bib.bib59)]
    被用来检测视频帧中的人和物体。随后，从视频帧中估计出骨架，并作为输入传递给时空GCN，接着通过基于聚类的全连接层生成骨架的异常分数。与检测到的物体的边界框坐标和置信度分数相关的信息被用来生成物体异常分数。最后，将骨架和物体的正常性分数结合起来，以生成帧的最终异常分数。南君等人[[45](#bib.bib45)]
    使用从视频中估计的骨架特征，利用迭代自训练策略进行行人异常检测。训练集由未标记的正常和异常视频序列组成。骨架被分解为全局和局部组件，这些组件被作为输入传递给无监督异常检测器iForest
    [[60](#bib.bib60)]，以生成伪异常和正常骨架集。伪集用于训练一个异常评分模块，该模块由空间GCN和带有单一输出单元的全连接层组成。作为自训练策略的一部分，使用先前训练的异常评分模块生成新的异常分数，以更新骨架样本在骨架集中的隶属关系。然后，使用更新后的骨架集重新训练评分模块，直到获得最佳评分模型。然而，论文没有讨论决定最佳评分模型的标准。Tani和Shibata
    [[46](#bib.bib46)] 提出了一个训练逐帧自适应GCN (AGCN) 的框架，用于利用单帧骨架进行动作识别，并使用从AGCN中提取的特征来训练异常检测模型。作为提议的框架的一部分，使用预训练的动作识别模型
    [[61](#bib.bib61)] 识别Kinetics-skeleton数据集 [[62](#bib.bib62)] 中的大时间注意帧作为训练AGCN的动作帧。此外，训练好的AGCN被用来从上海科技大学校园数据集
    [[17](#bib.bib17)] 中识别的正常行为骨架中提取特征，以建模多变量高斯分布。在测试期间，使用马氏距离计算多变量高斯分布下的异常分数。Sato等人[[47](#bib.bib47)]
    提出了一个用户提示指导的零样本学习框架，用于检测异常人类行为事件。一个多层感知机特征提取器在大规模动作识别数据集 [[63](#bib.bib63), [64](#bib.bib64)]
    上使用对比学习进行预训练，对比骨架特征和从动作类别名称中提取的文本嵌入。在训练期间，冻结特征提取器的权重，并对正常动作的骨架特征分布进行建模。在推理期间，使用分布和未见动作的文本提示计算异常分数。Javed等人[[44](#bib.bib44)]
    提出了一个统一框架，用于学习合适的兴趣帧，以减少冗余数据，并结合骨架图和视频帧特征的超门控融合模型。随后，通过聚类层处理软分配，其中将概率分配给实例，并使用Dirichlet过程混合模型
    [[65](#bib.bib65)] 计算正常性分数。
- en: Challenges
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战
- en: The performance of these methods rely on the pre-training strategy of the deep
    learning models used to learn the latent features and the choice of training parameters
    for the subsequent machine learning models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的性能依赖于深度学习模型的预训练策略，用于学习潜在特征，以及随后的机器学习模型的训练参数选择。
- en: 'TABLE III: Characteristics of skeletal video anomaly detection datasets.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：骨骼视频异常检测数据集的特点。
- en: '| Dataset | Total frames | Training frames | Test frames | Anomalous events
    | Camera views | Available annotations | Anomalies |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 总帧数 | 训练帧数 | 测试帧数 | 异常事件 | 相机视角 | 可用注释 | 异常 |'
- en: '| CUHK Avenue[[66](#bib.bib66)] | 30652 | 15328 | 15324 | 47 | 1 | Temporal,
    Pixel-wise, Track-ID | Throwing object, child skipping, wrong direction, bag on
    grass |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| CUHK大道[[66](#bib.bib66)] | 30652 | 15328 | 15324 | 47 | 1 | 时间，像素级，轨迹-ID
    | 投掷物体，孩子跳跃，错误方向，草地上的包 |'
- en: '| IITB-Corridor[[28](#bib.bib28)] | 483566 | 301999 | 181567 | 108278 frames
    | 1 | Temporal | Protest, unattended baggage, biker, fighting, chasing, loitering,
    suspicious object, hiding, playing with ball |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| IITB走廊[[28](#bib.bib28)] | 483566 | 301999 | 181567 | 108278 帧 | 1 | 时间 |
    示威，遗留行李，骑自行车者，打架，追逐，徘徊，疑似物体，隐藏，玩球 |'
- en: '| ShanghaiTech[[55](#bib.bib55)] | 317398 | 274515 | 42883 | 130 | 13 | Temporal,
    Pixel-wise | Throwing object, jumping, pushing, bikers, loitering, climbing |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 上海科技大学[[55](#bib.bib55)] | 317398 | 274515 | 42883 | 130 | 13 | 时间，像素级 |
    投掷物体，跳跃，推搡，骑自行车者，徘徊，攀爬 |'
- en: '| UCF-Crime[[67](#bib.bib67)] | 1900 videos | 1610 videos | 290 videos | 950
    videos | — | Temporal | Abuse, arrest, arson, assault, accident, burglary, explosion,
    fighting, robbery, shooting, stealing, shoplifting, vandalism |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| UCF-犯罪[[67](#bib.bib67)] | 1900 视频 | 1610 视频 | 290 视频 | 950 视频 | — | 时间 |
    虐待，逮捕，纵火，袭击，事故，盗窃，爆炸，打架，抢劫，射击，偷窃，扒窃，破坏 |'
- en: '| UCSD Pedestrian[[6](#bib.bib6)] | 18560 | 9350 | 9210 | 77 | 2 | Temporal,
    Pixel-wise, Track-ID | Biker, skater, cart, wheelchair, walk across walkways |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| UCSD行人[[6](#bib.bib6)] | 18560 | 9350 | 9210 | 77 | 2 | 时间，像素级，轨迹-ID | 骑自行车者，滑冰者，手推车，轮椅，过人行道
    |'
- en: '| UMN[[68](#bib.bib68)] | 3855 | — | — | 11 | — | Temporal | Abandoned or thrown
    objects, unusual crowd activity, camera tampering |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| UMN[[68](#bib.bib68)] | 3855 | — | — | 11 | — | 时间 | 被遗弃或投掷的物体，不寻常的人群活动，相机篡改
    |'
- en: \stackunder
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: \stackunder
- en: '[2pt]![Refer to caption](img/7c219f4ba3f69d2582210ec922c1d4bc.png)CUHK Avenue
    (Normal) \stackunder[2pt]![Refer to caption](img/6c6725ac7f364708baf2227228043f48.png)IITB-Corridor
    (Normal) \stackunder[2pt]![Refer to caption](img/53d71e38d50dbe0fedd48c7a5e8d0fe1.png)ShanghaiTech
    (Normal)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[2pt]![参见标题](img/7c219f4ba3f69d2582210ec922c1d4bc.png)CUHK大道（正常） \stackunder[2pt]![参见标题](img/6c6725ac7f364708baf2227228043f48.png)IITB走廊（正常）
    \stackunder[2pt]![参见标题](img/53d71e38d50dbe0fedd48c7a5e8d0fe1.png)上海科技大学（正常）'
- en: \stackunder
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: \stackunder
- en: '[2pt]![Refer to caption](img/1140859e37d9555d9524ddbf0256df22.png)CUHK Avenue
    (Throwing object) \stackunder[2pt]![Refer to caption](img/69991c64b53a48e1dd07f652db51af1a.png)IITB-Corridor
    (Fighting) \stackunder[2pt]![Refer to caption](img/9edcd62cd92800347561e57f5daff21d.png)ShanghaiTech
    (Biker)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[2pt]![参见标题](img/1140859e37d9555d9524ddbf0256df22.png)CUHK大道（投掷物体） \stackunder[2pt]![参见标题](img/69991c64b53a48e1dd07f652db51af1a.png)IITB走廊（打架）
    \stackunder[2pt]![参见标题](img/9edcd62cd92800347561e57f5daff21d.png)上海科技大学（骑自行车）'
- en: \stackunder
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: \stackunder
- en: '[2pt]![Refer to caption](img/51f02c9df0ddfb8e58d62e9aa6a2a417.png)UCF-Crime
    (Normal) \stackunder[2pt]![Refer to caption](img/5fb2b6e752e8ecb98e118c0226cd328e.png)UCSD
    peds1 (Normal) \stackunder[2pt]![Refer to caption](img/987913c74a924881cd4bd36465e6ce0c.png)UMN
    (Normal)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[2pt]![参见标题](img/51f02c9df0ddfb8e58d62e9aa6a2a417.png)UCF-犯罪（正常） \stackunder[2pt]![参见标题](img/5fb2b6e752e8ecb98e118c0226cd328e.png)UCSD
    peds1（正常） \stackunder[2pt]![参见标题](img/987913c74a924881cd4bd36465e6ce0c.png)UMN（正常）'
- en: \stackunder
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \stackunder
- en: '[2pt]![Refer to caption](img/4b245ea44a0eade0aef2743bccacdfca.png)UCF-Crime
    (Assault) \stackunder[2pt]![Refer to caption](img/ebe9c429943c8370319b82158e853a62.png)UCSD
    peds1 (Biker) \stackunder[2pt]![Refer to caption](img/a247bc860640abb5d231402d7003ccb6.png)UMN
    (Throwing object)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[2pt]![参见标题](img/4b245ea44a0eade0aef2743bccacdfca.png)UCF-犯罪（袭击） \stackunder[2pt]![参见标题](img/ebe9c429943c8370319b82158e853a62.png)UCSD
    peds1（骑自行车） \stackunder[2pt]![参见标题](img/a247bc860640abb5d231402d7003ccb6.png)UMN（投掷物体）'
- en: 'Figure 1: One normal and one anomalous frame from each of the skeletal video
    anomaly detection datasets.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：每个骨骼视频异常检测数据集中的一个正常帧和一个异常帧。
- en: III Discussion
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 讨论
- en: 'This section leverages Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Skeletal
    Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions")
    and [II](#S1.T2 "TABLE II ‣ I Introduction ‣ Skeletal Video Anomaly Detection
    using Deep Learning: Survey, Challenges and Future Directions") and synthesizes
    the information and trends that can be inferred from the existing work on skeletal
    video anomaly detection.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '本节利用表格 [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Skeletal Video Anomaly Detection
    using Deep Learning: Survey, Challenges and Future Directions") 和 [II](#S1.T2
    "TABLE II ‣ I Introduction ‣ Skeletal Video Anomaly Detection using Deep Learning:
    Survey, Challenges and Future Directions")，综合现有关于骨骼视频异常检测的研究成果，提炼出信息和趋势。'
- en: III-A Datasets
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 数据集
- en: 'ShanghaiTech [[55](#bib.bib55)] and CUHK Avenue [[66](#bib.bib66)] were the
    most frequently used video datasets to evaluate the performance of the skeletal
    video anomaly detection methods. The ShanghaiTech dataset has videos of people
    walking along a sidewalk of the ShanghaiTech university and anomalous frames contain
    bikers, skateboarders and people fighting. It has 330 training videos and 107
    test videos. However, not all the anomalous activities are related to humans.
    A subset of the ShanghaiTech dataset that contained anomalous activities only
    related to humans was termed as HR ShanghaiTech and was used in many papers. The
    CUHK Avenue dataset consists of short video clips looking at the side of a building
    with pedestrians walking by it. Concrete columns that are part of the building
    cause some occlusion. The dataset contains 16 training videos and 21 testing videos.
    The anomalous events comprise of actions such as “throwing papers”, “throwing
    bag”, “child skipping”, “wrong direction” and “bag on grass”. Similarly, a subset
    of the CUHK Avenue dataset containing anomalous activities only related to humans,
    called HR Avenue, has been used to evaluate the methods. Other video datasets
    that have been used include UTD-MHAD [[69](#bib.bib69)], UMN [[68](#bib.bib68)],
    UCSD Pedestrian[[6](#bib.bib6)], IITB-Corridor [[28](#bib.bib28)], UCF-Crime[[67](#bib.bib67)],
    HR Crime[[7](#bib.bib7)], NTU-RGB+D[[56](#bib.bib56)], RWF-2000[[70](#bib.bib70)]
    and Kinetics-250[[57](#bib.bib57)]. Table [III](#S2.T3 "TABLE III ‣ Challenges
    ‣ II-D Other Approaches ‣ II Literature Survey ‣ Skeletal Video Anomaly Detection
    using Deep Learning: Survey, Challenges and Future Directions") presents a summary
    of the characteristics of these datasets and Figure [1](#S2.F1 "Figure 1 ‣ Challenges
    ‣ II-D Other Approaches ‣ II Literature Survey ‣ Skeletal Video Anomaly Detection
    using Deep Learning: Survey, Challenges and Future Directions") presents one normal
    and one anomalous frame from these datasets. Among the datasets used in the reviewed
    papers, some of the datasets were originally not meant for but instead adopted
    for the task of video anomaly detection. Hence, we only provide details for the
    datasets that were originally meant for video anomaly detection in Table [III](#S2.T3
    "TABLE III ‣ Challenges ‣ II-D Other Approaches ‣ II Literature Survey ‣ Skeletal
    Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions")
    and Figure [1](#S2.F1 "Figure 1 ‣ Challenges ‣ II-D Other Approaches ‣ II Literature
    Survey ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges
    and Future Directions"). From the type of anomalies present in these datasets,
    it can be inferred that the existing skeletal video anomaly detection methods
    have been evaluated mostly on individual human action-based anomalies. Hence,
    it is not clear how well can they detect anomalies that involve interactions among
    multiple individuals or interactions among people and objects.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '上海科技大学 [[55](#bib.bib55)] 和香港中文大学大道 [[66](#bib.bib66)] 是评估骨架视频异常检测方法性能时最常用的视频数据集。上海科技大学数据集包含了在上海科技大学校园内人们沿人行道行走的视频，而异常帧中包含骑自行车者、滑板者和打架的人。该数据集有330个训练视频和107个测试视频。然而，并非所有异常活动都与人类有关。一个只包含与人类相关的异常活动的上海科技大学数据集子集被称为HR
    ShanghaiTech，并在许多论文中使用。CUHK Avenue数据集由短视频片段组成，这些片段展示了建筑物侧面行人的经过。建筑物中的混凝土柱造成了一些遮挡。该数据集包含16个训练视频和21个测试视频。异常事件包括“扔纸”，“扔包”，“孩子跳跃”，“错误方向”和“包在草地上”。类似地，包含只与人类相关的异常活动的CUHK
    Avenue数据集子集，称为HR Avenue，也用于评估这些方法。其他已使用的视频数据集包括UTD-MHAD [[69](#bib.bib69)]，UMN
    [[68](#bib.bib68)]，UCSD Pedestrian[[6](#bib.bib6)]，IITB-Corridor [[28](#bib.bib28)]，UCF-Crime[[67](#bib.bib67)]，HR
    Crime[[7](#bib.bib7)]，NTU-RGB+D[[56](#bib.bib56)]，RWF-2000[[70](#bib.bib70)] 和
    Kinetics-250[[57](#bib.bib57)]。表格 [III](#S2.T3 "TABLE III ‣ Challenges ‣ II-D
    Other Approaches ‣ II Literature Survey ‣ Skeletal Video Anomaly Detection using
    Deep Learning: Survey, Challenges and Future Directions") 总结了这些数据集的特点，图 [1](#S2.F1
    "Figure 1 ‣ Challenges ‣ II-D Other Approaches ‣ II Literature Survey ‣ Skeletal
    Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions")
    展示了这些数据集中一个正常帧和一个异常帧。在被审阅的论文中使用的数据集中，有些数据集最初并非用于视频异常检测任务，而是被采用于此。因此，我们仅提供最初用于视频异常检测的数据集的详细信息，如表
    [III](#S2.T3 "TABLE III ‣ Challenges ‣ II-D Other Approaches ‣ II Literature Survey
    ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and
    Future Directions") 和图 [1](#S2.F1 "Figure 1 ‣ Challenges ‣ II-D Other Approaches
    ‣ II Literature Survey ‣ Skeletal Video Anomaly Detection using Deep Learning:
    Survey, Challenges and Future Directions") 所示。从这些数据集中存在的异常类型可以推断出现有的骨架视频异常检测方法主要评估了基于个体人类行为的异常。因此，目前尚不清楚这些方法在检测涉及多个个体或人与物体互动的异常时表现如何。'
- en: III-B Number of people in the scene
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 场景中的人数
- en: Most of the papers (27 out of 29), detected anomalous human actions for multiple
    people in the video scene. Other two papers detected irregular body postures and
    poor body movements in children, respectively, for single person in the video
    scene. The usual approach was to estimate the skeletons for the people in the
    scene using a pose estimation algorithm, and calculate anomaly scores for each
    of the skeletons. The maximum anomaly score among all the skeletons within a frame
    was used to identify the anomalous frames. A single video frame could contain
    multiple people, among which not all of them were performing anomalous actions.
    Hence, taking the maximum anomaly score of all the skeletons helped to nullify
    the effect of people with normal actions on the final decision for the frame.
    Further, calculating anomaly scores for individual skeletons helped to localize
    the source of anomaly within a frame.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数论文（29篇中的27篇）检测了视频场景中多人的异常行为。另有两篇论文分别检测了单人视频场景中的不规则身体姿势和身体动作不良。通常的方法是使用姿态估计算法来估计场景中人的骨架，并计算每个骨架的异常分数。在一帧中所有骨架中的最大异常分数被用来识别异常帧。单个视频帧可能包含多个人，其中并非所有人都在执行异常动作。因此，取所有骨架中的最大异常分数有助于消除正常动作的人的影响，从而影响对帧的最终决策。此外，为个体骨架计算异常分数有助于在帧内定位异常源。
- en: III-C Fields of application
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 应用领域
- en: The definition of anomalous human behaviours can differ across various applications.
    While most of the existing papers focused on detecting anomalous human behaviours
    in general, five papers focused on detecting anomalous behaviours for specific
    applications, including drunk walking [[23](#bib.bib23)], poor body movements
    in children [[24](#bib.bib24)], abnormal pedestrian behaviours at grade crossings
    [[25](#bib.bib25), [26](#bib.bib26)] and crime-based anomalies [[7](#bib.bib7)].
    Moreover, the nature of anomalous behaviours can vary depending upon various factors,
    such as span of time, crowded scenes, and specific action-based anomalies. Some
    papers identified and addressed the need to detect specific types of anomalies,
    namely, multi-timescale anomalies occurring over different time duration [[28](#bib.bib28)],
    anomalies in both sparse and crowded scenes [[29](#bib.bib29)], fine and coarse-grained
    anomalies [[38](#bib.bib38)] and body movement and object position anomalies [[43](#bib.bib43)].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 异常人类行为的定义在不同应用中可能有所不同。虽然大多数现有论文关注于一般情况下异常人类行为的检测，但有五篇论文专注于特定应用中的异常行为检测，包括醉酒行走[[23](#bib.bib23)]、儿童身体动作不良[[24](#bib.bib24)]、铁路道口的异常行人行为[[25](#bib.bib25),
    [26](#bib.bib26)]以及基于犯罪的异常[[7](#bib.bib7)]。此外，异常行为的性质可能会因多种因素而异，例如时间跨度、拥挤场景和特定动作异常。一些论文识别并解决了检测特定类型异常的需求，即发生在不同时间段的多时间尺度异常[[28](#bib.bib28)]、在稀疏和拥挤场景中的异常[[29](#bib.bib29)]、细粒度和粗粒度异常[[38](#bib.bib38)]以及身体运动和物体位置异常[[43](#bib.bib43)]。
- en: III-D Choice of pose estimation algorithm
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 姿态估计算法选择
- en: Alphapose [[71](#bib.bib71)] and Openpose [[72](#bib.bib72)] were the most common
    choice of pose estimation algorithm for extraction of skeletons for the people
    in the scene. Other pose estimation methods that have been used were Posenet[[73](#bib.bib73)],
    PPN[[74](#bib.bib74)] and HRNet[[75](#bib.bib75)]. However, in general, the papers
    did not provide any rationale behind their choice of the pose estimation algorithm.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Alphapose [[71](#bib.bib71)] 和 Openpose [[72](#bib.bib72)] 是提取场景中人物骨架的最常用的姿态估计算法。其他使用过的姿态估计方法包括
    Posenet[[73](#bib.bib73)]、PPN[[74](#bib.bib74)] 和 HRNet[[75](#bib.bib75)]。然而，总的来说，这些论文没有提供其选择姿态估计算法的理由。
- en: III-E Model type
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 模型类型
- en: The type of models used in the papers can broadly be divided into two types,
    sequence-based and graph-based models. The sequence-based models that have been
    used include 1DConv-AE, LSTM-AE, GRU, and Transformer. These models treated skeleton
    keypoints for individual people across multiple frames as time series input. The
    graph-based models that have been used involve GCAE and GCN. The graph-based models
    received spatio-temporal skeleton graphs for individual people as input. The spatio-temporal
    graphs were constructed by considering body joints as the nodes of the graph.
    The spatial edges connected different joints of a skeleton, and temporal edges
    connected the same joints across time.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中使用的模型类型可以大致分为两类，即基于序列的模型和基于图的模型。使用过的基于序列的模型包括`1DConv-AE`、`LSTM-AE`、`GRU`和`Transformer`。这些模型将单个人在多个帧中的骨架关键点视为时间序列输入。使用过的基于图的模型包括`GCAE`和`GCN`。基于图的模型接收单个人的时空骨架图作为输入。这些时空图通过将身体关节视为图的节点来构建。空间边连接骨架的不同关节，而时间边连接在时间上相同的关节。
- en: III-F Evaluation metrics
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-F 评估指标
- en: 'The choice of a suitable threshold for anomaly detection can vary across different
    applications as most applications come with different costs for false alarms and
    missed anomalies [[76](#bib.bib76), [77](#bib.bib77)]. As such, having a metric
    capable of evaluating the performance of anomaly detection methods across diverse
    application scenarios, or equivalently, across a wide array of decision thresholds
    is highly desirable. The Area Under Curve (AUC) of Receiver Operating Characteristic
    (ROC) curve computes the fraction of detected anomalies, averaged over the full
    range of decision thresholds. It is the standard evaluation measure used in anomaly
    detection [[76](#bib.bib76)] and also the most common metric used to evaluate
    the performance among the existing skeletal video anomaly detection methods. The
    highest AUC(ROC) values reported for the commonly used ShanghaiTech [[55](#bib.bib55)]
    and CUHK Avenue [[66](#bib.bib66)] datasets across different methods in Table
    [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Skeletal Video Anomaly Detection using
    Deep Learning: Survey, Challenges and Future Directions") and [II](#S1.T2 "TABLE
    II ‣ I Introduction ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey,
    Challenges and Future Directions") were 0.83 and 0.92, respectively. A direct
    comparison may not be possible due to the difference in the experimental setup
    and train-test splits across the reviewed methods; however, it gives some confidence
    on the viability of these approaches for skeletal video anomaly detection. Other
    performance evaluation metrics include F score, accuracy, Equal Error Rate (EER)
    and AUC of Precision-Recall (PR) Curve. EER signifies the percentage of misclassified
    frames when the false positive rate equals to the miss rate on the ROC curve.
    While AUC(ROC) can provide a good estimate of the classifier’s performance over
    different thresholds, it can be misleading in case the data is imbalanced [[78](#bib.bib78)].
    In anomaly detection scenario, it is common to have imbalance in the test data,
    as the anomalous behaviours occur infrequently, particularly in many medical applications
    [[79](#bib.bib79), [80](#bib.bib80)]. The AUC(PR) value provides a good estimate
    of the classifier’s performance on imbalanced datasets [[78](#bib.bib78)]; however,
    only one of the papers used AUC(PR) as an evaluation metric.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '适合的异常检测阈值选择可能因应用不同而异，因为大多数应用对误报和漏报有不同的成本[[76](#bib.bib76), [77](#bib.bib77)]。因此，拥有一个能够评估异常检测方法在不同应用场景下表现的指标，或者说，在各种决策阈值下的指标是非常理想的。接收者操作特征（ROC）曲线下的面积（AUC）计算了检测到的异常的比例，该比例在整个决策阈值范围内取平均。它是异常检测中使用的标准评估指标[[76](#bib.bib76)]，也是评估现有骨架视频异常检测方法表现的最常用指标。表
    [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Skeletal Video Anomaly Detection using
    Deep Learning: Survey, Challenges and Future Directions") 和 [II](#S1.T2 "TABLE
    II ‣ I Introduction ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey,
    Challenges and Future Directions") 中报告的常用上海科技 [[55](#bib.bib55)] 和 CUHK Avenue
    [[66](#bib.bib66)] 数据集的最高 AUC(ROC) 值分别为 0.83 和 0.92。由于实验设置和训练-测试划分的差异，直接比较可能不可行；然而，这些结果对骨架视频异常检测方法的可行性提供了一些信心。其他性能评估指标包括
    F 分数、准确率、等误差率（EER）和精确-召回（PR）曲线的 AUC。EER 表示在 ROC 曲线中，假阳性率等于漏检率时的错误分类帧百分比。虽然 AUC(ROC)
    能够在不同阈值下提供分类器性能的良好估计，但在数据不平衡的情况下，它可能会产生误导[[78](#bib.bib78)]。在异常检测场景中，测试数据通常不平衡，因为异常行为发生的频率较低，特别是在许多医疗应用中[[79](#bib.bib79),
    [80](#bib.bib80)]。AUC(PR) 值提供了分类器在不平衡数据集上的良好估计[[78](#bib.bib78)]；然而，只有一篇论文使用了
    AUC(PR) 作为评估指标。'
- en: \stackunder
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: \stackunder
- en: '[2pt]![Refer to caption](img/4caf21ef66d1b89a8e9c2ad61666dbdd.png) \stackunder[2pt]![Refer
    to caption](img/0d2f57c6d185aca906c6a75a116a436b.png) \stackunder[2pt]![Refer
    to caption](img/99826a09e93e28be6b2def1a74454090.png)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[2pt]![参考说明](img/4caf21ef66d1b89a8e9c2ad61666dbdd.png) \stackunder[2pt]![参考说明](img/0d2f57c6d185aca906c6a75a116a436b.png)
    \stackunder[2pt]![参考说明](img/99826a09e93e28be6b2def1a74454090.png)'
- en: \stackunder
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: \stackunder
- en: '[2pt]![Refer to caption](img/853730fd64f9fb9916dcde81cac84d73.png) \stackunder[2pt]![Refer
    to caption](img/0bdde0dbe8d27dd6d0fedf738e2004fc.png) \stackunder[2pt]![Refer
    to caption](img/7b787946ac32e72672e8f9f6f886a317.png)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[2pt]![参考说明](img/853730fd64f9fb9916dcde81cac84d73.png) \stackunder[2pt]![参考说明](img/0bdde0dbe8d27dd6d0fedf738e2004fc.png)
    \stackunder[2pt]![参考说明](img/7b787946ac32e72672e8f9f6f886a317.png)'
- en: 'Figure 2: Openpose (top row) and alphapose (bottom row) output on different
    datasets.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Openpose（上排）和 alphapose（下排）在不同数据集上的输出。
- en: IV Challenges
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 挑战
- en: IV-A Pose estimation algorithms
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 姿态估计算法
- en: 'In general, the efficiency of the skeletal video anomaly detection algorithms
    depends upon the accuracy of the skeletons estimated by the pose-estimation algorithm.
    If the pose estimation algorithm misses certain joints or produces artifacts in
    the scene, then it can increase the number of false alarms. There are various
    challenges associated with estimating skeletons from video frames [[81](#bib.bib81)]:
    (i) complex body configuration causing self-occlusions and complex poses, (ii)
    diverse appearance, including clothing, and (iii) complex environment with occlusion
    from other people in the scene, various viewing angles, distance from camera and
    truncation of parts in the camera view. This can lead to a poor approximation
    of skeletons and can negatively impact the performance of the anomaly detection
    algorithms. Further, there is an associated high cost of powerful hardware required
    for extracting skeletons using deep learning methods. Methods have been proposed
    to address some of these challenges [[82](#bib.bib82), [83](#bib.bib83)]; however,
    extracting skeletons in complex environments remains a difficult problem. The
    two most commonly used pose estimation algorithms in the reviewed papers are Openpose
    [[72](#bib.bib72)] and Alphapose [[71](#bib.bib71)]. Multi-person pose estimation
    can be categorized into top-down and bottom-up methods [[81](#bib.bib81)]. Top-down
    methods [[71](#bib.bib71), [84](#bib.bib84)] usually employ human detectors to
    obtain bounding boxes for humans in the input frame and then utilize existing
    single-person pose estimators to predict body joints. This method highly depends
    upon the precision of human detection algorithms, and the run-time is proportional
    to the number of persons in the input frame. Bottom-up methods [[72](#bib.bib72)]
    directly approximate all the body joints of all the humans in the input frame
    and assemble them into individual skeletons. However, the grouping of joints in
    a complex scene is a challenging task. Openpose is a bottom-up method and Alphapose
    is a top-down method. Figure [2](#S3.F2 "Figure 2 ‣ III-F Evaluation metrics ‣
    III Discussion ‣ Skeletal Video Anomaly Detection using Deep Learning: Survey,
    Challenges and Future Directions") presents the skeleton output of openpose and
    alphapose on different dataset frames. Some of the existing methods manually remove
    inaccurate and false skeletons [[17](#bib.bib17), [25](#bib.bib25)] to train the
    model, which is impractical in many real-world applications where the amount of
    available data is very large. There is a need for an automated false skeleton
    identification and removal step when estimating skeletons from videos.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，骨架视频异常检测算法的效率取决于姿态估计算法估计的骨架的准确性。如果姿态估计算法遗漏某些关节或在场景中产生伪影，那么可能会增加误报的数量。估计视频帧中的骨架存在各种挑战
    [[81](#bib.bib81)]： (i) 复杂的身体配置导致自遮挡和复杂的姿势，(ii) 多样的外观，包括衣物，以及 (iii) 复杂的环境，包含场景中其他人的遮挡、各种视角、距离相机的远近和相机视图中的部件截断。这可能导致骨架的近似不佳，进而负面影响异常检测算法的性能。此外，使用深度学习方法提取骨架所需的强大硬件成本也很高。一些方法已被提出以应对这些挑战
    [[82](#bib.bib82), [83](#bib.bib83)]；然而，在复杂环境中提取骨架仍然是一个困难的问题。综述论文中最常用的两种姿态估计算法是
    Openpose [[72](#bib.bib72)] 和 Alphapose [[71](#bib.bib71)]。多人的姿态估计可以分为自上而下和自下而上方法
    [[81](#bib.bib81)]。自上而下方法 [[71](#bib.bib71), [84](#bib.bib84)] 通常使用人类检测器来获取输入帧中的人类边界框，然后利用现有的单人姿态估计器来预测身体关节。这种方法高度依赖于人类检测算法的精度，并且运行时间与输入帧中的人数成正比。自下而上方法
    [[72](#bib.bib72)] 直接近似输入帧中所有人的所有身体关节，并将它们组装成单独的骨架。然而，在复杂场景中关节的分组是一项具有挑战性的任务。Openpose
    是一种自下而上的方法，Alphapose 是一种自上而下的方法。图 [2](#S3.F2 "图 2 ‣ III-F 评价指标 ‣ III 讨论 ‣ 使用深度学习的骨架视频异常检测：调查、挑战与未来方向")
    展示了 openpose 和 alphapose 在不同数据集帧上的骨架输出。一些现有的方法手动去除不准确和错误的骨架 [[17](#bib.bib17),
    [25](#bib.bib25)] 以训练模型，但在许多实际应用中数据量非常大，这种方法并不现实。因此，在从视频中估计骨架时，需要一个自动的假骨架识别和去除步骤。
- en: IV-B Types of anomalies
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 异常类型
- en: The anomalous human behaviours of interest and their difficulty of detection
    can vary depending upon the definition of anomaly, application, time span of the
    anomalous actions, and presence of single/multiple people in the scenes. For example,
    in the case of driver anomaly detection application, the anomalous behaviours
    can include talking on the phone, dozing off or drinking [[14](#bib.bib14)]. The
    anomalous actions can span over different time lengths, ranging from few seconds
    to hours or days, e.g., jumping and falls [[85](#bib.bib85)] are short-term anomalies,
    while loitering and social isolation [[86](#bib.bib86)] are long-term events.
    More focus is needed on developing methods that can identify both short and long-term
    anomalies. Sparse scene anomalies can be described as anomalies in scenes with
    less number of humans, while dense scene anomalies can be described as anomalies
    in crowded scenes with a large number of humans [[29](#bib.bib29)]. It is comparatively
    difficult to identify anomalous behaviours in dense scenes than sparse scenes
    due to tracking multiple people and finding their individual anomaly scores [[17](#bib.bib17)].
    Thus, there is a need to develop methods that can effectively identify both sparse
    and dense scene anomalies. With the development of algorithms for handling different
    types of anomalies, there is a need for datasets composed of the specific type
    of anomalies to ensure efficient training and evaluation. This can be handled
    by either having separate datasets for specific types of anomalies or general
    datasets with a distribution of multiple types of anomalies.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的异常人类行为及其检测难度可能因异常的定义、应用、异常行为的时间跨度以及场景中是否有单个人/多人而异。例如，在驾驶员异常检测应用中，异常行为可能包括打电话、打瞌睡或饮酒[[14](#bib.bib14)]。异常行为的时间跨度可以从几秒到几小时或几天不等，例如，跳跃和跌倒[[85](#bib.bib85)]是短期异常，而闲逛和社交孤立[[86](#bib.bib86)]则是长期事件。需要更多关注于开发能够识别短期和长期异常的方法。稀疏场景异常可以描述为人较少的场景中的异常，而密集场景异常则可以描述为人多的拥挤场景中的异常[[29](#bib.bib29)]。由于需要跟踪多人并找到其各自的异常分数，因此在密集场景中识别异常行为比在稀疏场景中更具挑战性[[17](#bib.bib17)]。因此，需要开发能够有效识别稀疏和密集场景异常的方法。随着处理不同类型异常的算法的发展，需要特定类型异常的数据集，以确保高效的训练和评估。这可以通过为特定类型的异常提供单独的数据集或提供多种类型异常分布的一般数据集来处理。
- en: IV-C Hardware
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 硬件
- en: The skeletons collected using Microsoft Kinect (depth) camera has been used
    in the past studies [[87](#bib.bib87), [88](#bib.bib88)]. However, the defunct
    production of the Microsoft Kinect camera [[89](#bib.bib89)] has led to hardware
    constraints in the further development of skeletal anomaly detection approaches.
    Other commercial products include Vicon [[90](#bib.bib90)] with optical sensors
    and TheCaptury [[91](#bib.bib91)] with multiple cameras. But they function in
    very constrained environments or require special markers on the human body. New
    cameras, such as ‘Sentinare 2’ from AltumView [[92](#bib.bib92)], circumvent such
    hardware requirements by directly processing videos on regular RGB cameras and
    transmitting skeletons information in real-time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微软Kinect（深度）摄像头收集的骨架曾在过去的研究中使用过[[87](#bib.bib87), [88](#bib.bib88)]。然而，微软Kinect摄像头的停产[[89](#bib.bib89)]导致了骨架异常检测方法进一步发展的硬件限制。其他商业产品包括配备光学传感器的Vicon
    [[90](#bib.bib90)]和配备多个摄像头的TheCaptury [[91](#bib.bib91)]。但它们在非常受限的环境中运行或需要在人体上使用特殊标记。新的摄像头，如AltumView的‘Sentinare
    2’ [[92](#bib.bib92)]，通过直接处理普通RGB摄像头上的视频并实时传输骨架信息，绕过了这些硬件要求。
- en: IV-D Tracking skeletons
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 跟踪骨架
- en: The existing approaches for skeletal video anomaly detection involve spatio-temporal
    skeleton graphs [[16](#bib.bib16)] or temporal sequences [[17](#bib.bib17)], which
    are constructed by tracking an individual across multiple frames. However, this
    is challenging in scenarios where there are multiple people within a scene. The
    entry and exit of people in the scene, overlapping of people during movement and
    presence of occluding objects make tracking people across frames a very challenging
    task.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的骨架视频异常检测方法包括时空骨架图[[16](#bib.bib16)]或时间序列[[17](#bib.bib17)]，这些图是通过跟踪个体在多个帧中的位置构建的。然而，当场景中有多人时，这种方法非常具有挑战性。人们的进出、移动中的重叠以及遮挡物的存在使得跨帧跟踪人群变得非常困难。
- en: IV-E Choice of threshold
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 阈值选择
- en: There can be deployment issues in these methods because the choice of threshold
    is not clear. In the absence of any validation set (containing both normal and
    unseen anomalies) in an anomaly detection setting, it is very hard to fine-tune
    an operating threshold using just the training data (comprising of normal activities
    only). To handle these situations, outliers within the normal activities can be
    used as a proxy for unseen anomalies [[85](#bib.bib85), [93](#bib.bib93)]; however,
    inappropriate choices can lead to increased false alarms or missed alarms. Domain
    expertise can be utilized to adjust a threshold, which may not be available in
    many cases.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法可能存在部署问题，因为阈值的选择不明确。在异常检测设置中，缺少包含正常和未见异常的验证集时，仅使用训练数据（仅包含正常活动）来微调操作阈值非常困难。为了解决这些情况，可以将正常活动中的离群点作为未见异常的代理[[85](#bib.bib85),
    [93](#bib.bib93)]；然而，不恰当的选择可能导致误报增加或漏报。可以利用领域专长来调整阈值，但在许多情况下，这种专长可能无法获得。
- en: IV-F Decision granularity
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 决策粒度
- en: There is a need to address the challenges associated with the granularity and
    the decision-making time of the skeletal video anomaly detection methods for real-time
    applications. The existing methods mostly output decisions on a frame level, which
    becomes an issue when the input to the method is a real-time continuous video
    stream at multiple frames per second. This can lead to alarms going off multiple
    times a second, which can be counter-productive. One solution is for the methods
    to make decisions on a time-window basis, each window of length of a specified
    duration. However, this brings in the question about the optimal length of each
    decision window. A short window is impractical as it can lead to frequent and
    repetitive alarms, while a long window can lead to missed alarms, and delayed
    response and intervention. Domain knowledge can be used to make a decision about
    the length of decision windows.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 需要解决与骨架视频异常检测方法在实时应用中的粒度和决策时间相关的挑战。现有方法大多数在帧级别上输出决策，当方法的输入是每秒多帧的实时连续视频流时，这会成为一个问题。这可能导致警报每秒多次响起，这会适得其反。一个解决方案是让方法基于时间窗口做决策，每个窗口具有指定持续时间。然而，这带来了每个决策窗口的最佳长度的问题。短窗口不切实际，因为它可能导致频繁和重复的警报，而长窗口可能导致漏报、响应和干预延迟。可以利用领域知识来决定决策窗口的长度。
- en: V Future Directions
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 未来方向
- en: Skeletons can be used in conjunction with optical flow [[94](#bib.bib94)] to
    develop privacy-protecting approaches to jointly learn from temporal and structural
    modalities. Approaches based on federated learning (that do not combine individual
    data, but only the models) can further improve the privacy of these methods [[95](#bib.bib95)].
    Segmentation masks [[96](#bib.bib96)] can be leveraged in conjunction with skeletons
    to occlude humans while capturing the information pertaining to scene and human
    motion to develop privacy-protecting anomaly detection approaches.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将骨架与光流[[94](#bib.bib94)]结合使用，以开发保护隐私的方法，从时间和结构模态中共同学习。基于联邦学习的方法（不结合个人数据，只结合模型）可以进一步提升这些方法的隐私性[[95](#bib.bib95)]。可以结合骨架使用分割掩膜[[96](#bib.bib96)]，在捕捉与场景和人体运动相关的信息的同时遮蔽人类，从而开发保护隐私的异常检测方法。
- en: The skeletons signify motion and posture information for the individual humans
    in the video; however, they lack information regarding human-human and human-object
    interactions. Information pertaining to interaction of the people with each other
    and the objects in the environment is important for applications such as, violence
    detection [[7](#bib.bib7)], theft detection [[7](#bib.bib7)] and agitation detection
    [[80](#bib.bib80)] in care home settings. Skeletons can be used to replace the
    bodies of the participants, while keeping the background information in video
    frames [[97](#bib.bib97)] to analyze both human-human and human-object interaction
    anomalies. Further, object bounding boxes can be used in conjunction with human
    skeletons to model human-object interaction while preserving the privacy of humans
    in the scene. The information from other modalities (e.g. wearable devices) along
    with skeleton features can be used to develop multi-modal anomaly detection methods
    to improve the detection performance. Further, the generated embeddings of relevant
    supervised approaches [[98](#bib.bib98), [99](#bib.bib99)] can be used to fine
    tune skeletal video anomaly detection models.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 骨架表示视频中每个人的运动和姿势信息；然而，它们缺乏关于人际互动和人机互动的信息。有关人与人之间以及人与环境中物体互动的信息，对于如暴力检测 [[7](#bib.bib7)]、盗窃检测
    [[7](#bib.bib7)] 和焦虑检测 [[80](#bib.bib80)] 等应用在养老院环境中是重要的。骨架可以用来替代参与者的身体，同时保留视频帧中的背景信息
    [[97](#bib.bib97)]，以分析人际互动和人机互动异常。此外，可以将对象边界框与人体骨架结合使用，以在保持场景中人类隐私的同时建模人机互动。其他模态的信息（例如可穿戴设备）与骨架特征结合使用，可以开发多模态异常检测方法，以提高检测性能。此外，相关的监督学习方法生成的嵌入
    [[98](#bib.bib98), [99](#bib.bib99)] 可以用于微调骨架视频异常检测模型。
- en: 'As can be seen in Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Skeletal Video
    Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions")
    and [II](#S1.T2 "TABLE II ‣ I Introduction ‣ Skeletal Video Anomaly Detection
    using Deep Learning: Survey, Challenges and Future Directions"), the existing
    skeletal video anomaly detection methods and available datasets focus towards
    detecting irregular body postures [[16](#bib.bib16)], and anomalous human actions
    [[31](#bib.bib31)] in mostly outdoor settings, and not in proper healthcare settings,
    such as personal homes and long-term care homes. This a gap towards real world
    deployment, as there is a need to extend the scope of detecting anomalous behaviours
    using skeletons to in-home and care home settings, where privacy is a very important
    concern. This can be utilized to address important applications, such as fall
    detection [[100](#bib.bib100)], agitation detection [[80](#bib.bib80), [97](#bib.bib97)],
    and independent assistive living. This will help to develop supportive homes and
    communities and encourage autonomy and independence among the increasing older
    population and dementia residents in care homes. While leveraging skeletons helps
    to get rid of facial identity and appearance-based information, it is important
    to ask the question if skeletons can be considered private enough [[101](#bib.bib101),
    [102](#bib.bib102)] and what steps can be taken to further anonymize the skeletons.
    Another potential area of investigation for real-world deployment of privacy-protecting
    anomaly detection systems would be to perform video data acquisition, skeletal
    tracking (e.g., MediaPipe [[103](#bib.bib103)]) and model inferencing in real-time.
    However, there may be challenges around integrating cloud services, on-chip embedding
    of AI algorithms, the latency of reaction time, internet stability and false positive
    rates.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Skeletal Video Anomaly Detection
    using Deep Learning: Survey, Challenges and Future Directions") 和 [II](#S1.T2
    "TABLE II ‣ I Introduction ‣ Skeletal Video Anomaly Detection using Deep Learning:
    Survey, Challenges and Future Directions") 所示，现有的骨骼视频异常检测方法和数据集主要集中在检测不规则的身体姿势
    [[16](#bib.bib16)] 和异常的人类行为 [[31](#bib.bib31)]，主要是在户外环境中，而不是在适当的医疗环境中，如个人家中和长期护理院。这是向现实世界部署的一个缺口，因为需要扩展使用骨骼检测异常行为的范围到家庭和护理院环境中，这里隐私是一个非常重要的关注点。这可以用于解决重要的应用问题，如跌倒检测
    [[100](#bib.bib100)]、激动检测 [[80](#bib.bib80), [97](#bib.bib97)] 和独立辅助生活。这将有助于开发支持性的家庭和社区，并鼓励日益增长的老年人群体和护理院中的痴呆居民的自主性和独立性。虽然利用骨骼有助于摆脱面部身份和外观信息，但重要的是要问骨骼是否可以被认为是足够隐私的
    [[101](#bib.bib101), [102](#bib.bib102)]，以及可以采取哪些步骤来进一步匿名化骨骼。另一个用于隐私保护异常检测系统的现实世界部署的潜在研究领域是实时进行视频数据采集、骨骼跟踪（例如，MediaPipe
    [[103](#bib.bib103)]）和模型推断。然而，可能会面临集成云服务、芯片内嵌入AI算法、反应时间延迟、互联网稳定性和误报率等挑战。'
- en: VI Conclusion
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this paper, we provided a survey of recent works that leverage the skeletons
    or body joints estimated from videos for the anomaly detection task. The skeletons
    hide the facial identity and overall appearance of people and can provide vital
    information about joint angles [[104](#bib.bib104)], speed of walking [[105](#bib.bib105)],
    and interaction with other people in the scene [[17](#bib.bib17)]. Our literature
    review showed that many deep learning-based approaches leverage reconstruction,
    prediction error and their other combinations to successfully detect anomalies
    in a privacy protecting manner. This review suggests the first steps towards increasing
    adoption of devices (and algorithms) focused on improving privacy in a residential
    or communal setting. It will further improve the deployment of anomaly detection
    systems to improve the safety and care of people. The skeleton-based anomaly detection
    methods can be used to design privacy-preserving technologies for the assisted
    living of older adults in a care environment [[106](#bib.bib106)] or enable older
    adults to live independently in their own homes to cope with the increasing cost
    of long-term care demands [[107](#bib.bib107)]. Privacy-preserving methods using
    skeleton features can be employed to assist with skeleton-based rehab exercise
    monitoring [[108](#bib.bib108)] or in social robots for robot-human interaction
    [[109](#bib.bib109)] that assist older people in their activities of daily living.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了对利用视频中估计的骨架或身体关节进行异常检测的最新工作的综述。骨架隐藏了人们的面部身份和整体外观，并可以提供关于关节角度[[104](#bib.bib104)]、步态速度[[105](#bib.bib105)]以及与场景中其他人的互动[[17](#bib.bib17)]的重要信息。我们的文献综述显示，许多基于深度学习的方法利用重建、预测误差及其其他组合，以隐私保护的方式成功检测异常。此综述建议了提高以隐私为中心的设备（和算法）在住宅或公共环境中应用的初步步骤。这将进一步改善异常检测系统的部署，以提升人们的安全和护理。基于骨架的异常检测方法可用于设计隐私保护技术，以支持老年人在护理环境中的辅助生活[[106](#bib.bib106)]，或使老年人在自己的家中独立生活，以应对日益增长的长期护理需求[[107](#bib.bib107)]。使用骨架特征的隐私保护方法可以用于辅助骨架基础的康复锻炼监测[[108](#bib.bib108)]，或用于机器人-人类互动的社交机器人[[109](#bib.bib109)]，以协助老年人完成日常活动。
- en: VII Acknowledgements
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 致谢
- en: This work was supported by AGE-WELL NCE Inc, Alzheimer’s Association, Natural
    Sciences and Engineering Research Council and UAE Strategic Research Grant.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了AGE-WELL NCE Inc、阿尔茨海默氏症协会、自然科学与工程研究委员会以及阿联酋战略研究基金的支持。
- en: References
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. S. Khan and M. G. Madden, “One-class classification: taxonomy of study
    and review of techniques,” *The Knowledge Engineering Review*, vol. 29, no. 3,
    pp. 345–374, 2014.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. S. Khan 和 M. G. Madden, “单类分类：研究分类及技术综述，” *知识工程评论*，第29卷，第3期，第345–374页，2014年。'
- en: '[2] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A survey,”
    *ACM computing surveys (CSUR)*, vol. 41, no. 3, pp. 1–58, 2009.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] V. Chandola, A. Banerjee, 和 V. Kumar, “异常检测：综述，” *ACM计算调查（CSUR）*，第41卷，第3期，第1–58页，2009年。'
- en: '[3] C. Gautam, P. K. Mishra, A. Tiwari, B. Richhariya, H. M. Pandey, S. Wang,
    M. Tanveer, A. D. N. Initiative *et al.*, “Minimum variance-embedded deep kernel
    regularized least squares method for one-class classification and its applications
    to biomedical data,” *Neural Networks*, vol. 123, pp. 191–216, 2020.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. Gautam, P. K. Mishra, A. Tiwari, B. Richhariya, H. M. Pandey, S. Wang,
    M. Tanveer, A. D. N. Initiative *et al.*, “最小方差嵌入的深度核正则化最小二乘法用于单类分类及其在生物医学数据中的应用，”
    *神经网络*，第123卷，第191–216页，2020年。'
- en: '[4] P. K. Mishra, C. Gautam, and A. Tiwari, “Minimum variance embedded auto-associative
    kernel extreme learning machine for one-class classification,” *Neural Computing
    and Applications*, vol. 33, no. 19, pp. 12 973–12 987, 2021.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] P. K. Mishra, C. Gautam, 和 A. Tiwari, “最小方差嵌入的自关联核极限学习机用于单类分类，” *神经计算与应用*，第33卷，第19期，第12 973–12 987页，2021年。'
- en: '[5] J. Nogas, S. S. Khan, and A. Mihailidis, “Deepfall: Non-invasive fall detection
    with deep spatio-temporal convolutional autoencoders,” *Journal of Healthcare
    Informatics Research*, vol. 4, no. 1, pp. 50–70, 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Nogas, S. S. Khan, 和 A. Mihailidis, “Deepfall: 利用深度时空卷积自编码器进行非侵入式跌倒检测，”
    *医疗信息学研究杂志*，第4卷，第1期，第50–70页，2020年。'
- en: '[6] W. Li, V. Mahadevan, and N. Vasconcelos, “Anomaly detection and localization
    in crowded scenes,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 36, no. 1, pp. 18–32, 2013.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] W. Li, V. Mahadevan, 和 N. Vasconcelos, “拥挤场景中的异常检测与定位，” *IEEE模式分析与机器智能学报*，第36卷，第1期，第18–32页，2013年。'
- en: '[7] K. Boekhoudt, A. Matei, M. Aghaei, and E. Talavera, “Hr-crime: Human-related
    anomaly detection in surveillance videos,” in *International Conference on Computer
    Analysis of Images and Patterns*.   Springer, 2021, pp. 164–174.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] K. Boekhoudt, A. Matei, M. Aghaei, 和 E. Talavera, “Hr-crime：监控视频中的人类相关异常检测”，发表于*计算机图像与模式分析国际会议*。Springer，2021年，第164–174页。'
- en: '[8] A. Senior, *Protecting privacy in video surveillance*.   Springer, 2009,
    vol. 1.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Senior, *视频监控中的隐私保护*。Springer，2009年，第1卷。'
- en: '[9] P. Climent-Pérez and F. Florez-Revuelta, “Protection of visual privacy
    in videos acquired with rgb cameras for active and assisted living applications,”
    *Multimedia Tools and Applications*, vol. 80, no. 15, pp. 23 649–23 664, 2021.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. Climent-Pérez 和 F. Florez-Revuelta, “在RGB摄像头拍摄的视频中保护视觉隐私，用于主动和辅助生活应用”，*多媒体工具与应用*，第80卷，第15期，第23 649–23 664页，2021年。'
- en: '[10] B. Ye, S. S. Khan, B. Chikhaoui, A. Iaboni, L. S. Martin, K. Newman, A. Wang,
    and A. Mihailidis, “Challenges in collecting big data in a clinical environment
    with vulnerable population: Lessons learned from a study using a multi-modal sensors
    platform,” *Science and engineering ethics*, vol. 25, no. 5, pp. 1447–1466, 2019.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] B. Ye, S. S. Khan, B. Chikhaoui, A. Iaboni, L. S. Martin, K. Newman, A.
    Wang, 和 A. Mihailidis, “在临床环境中收集大数据的挑战：使用多模态传感器平台的研究经验教训”，*科学与工程伦理*，第25卷，第5期，第1447–1466页，2019年。'
- en: '[11] P. Schneider, J. Rambach, B. Mirbach, and D. Stricker, “Unsupervised anomaly
    detection from time-of-flight depth images,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 231–240.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] P. Schneider, J. Rambach, B. Mirbach, 和 D. Stricker, “基于飞行时间深度图像的无监督异常检测”，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2022年，第231–240页。'
- en: '[12] V. Mehta, A. Dhall, S. Pal, and S. S. Khan, “Motion and region aware adversarial
    learning for fall detection with thermal imaging,” in *2020 25th International
    Conference on Pattern Recognition (ICPR)*.   IEEE, 2021, pp. 6321–6328.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] V. Mehta, A. Dhall, S. Pal, 和 S. S. Khan, “基于热成像的运动和区域感知对抗学习用于跌倒检测”，发表于*2020年第25届国际模式识别大会（ICPR）*。IEEE，2021年，第6321–6328页。'
- en: '[13] S. Denkovski, S. S. Khan, B. Malamis, S. Y. Moon, B. Ye, and A. Mihailidis,
    “Multi visual modality fall detection dataset,” *IEEE Access*, vol. 10, pp. 106 422–106 435,
    2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Denkovski, S. S. Khan, B. Malamis, S. Y. Moon, B. Ye, 和 A. Mihailidis,
    “多视觉模态跌倒检测数据集”，*IEEE Access*，第10卷，第106 422–106 435页，2022年。'
- en: '[14] O. Kopuklu, J. Zheng, H. Xu, and G. Rigoll, “Driver anomaly detection:
    A dataset and contrastive learning approach,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2021, pp. 91–100.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] O. Kopuklu, J. Zheng, H. Xu, 和 G. Rigoll, “驾驶员异常检测：数据集与对比学习方法”，发表于*IEEE/CVF冬季计算机视觉应用会议论文集*，2021年，第91–100页。'
- en: '[15] T. Golda, D. Guaia, and V. Wagner-Hartl, “Perception of risks and usefulness
    of smart video surveillance systems,” *Applied Sciences*, vol. 12, no. 20, p.
    10435, 2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T. Golda, D. Guaia, 和 V. Wagner-Hartl, “对智能视频监控系统的风险和实用性的感知”，*应用科学*，第12卷，第20期，第10435页，2022年。'
- en: '[16] W. Luo, W. Liu, and S. Gao, “Normal graph: Spatial temporal graph convolutional
    networks based prediction network for skeleton based video anomaly detection,”
    *Neurocomputing*, vol. 444, pp. 332–337, 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. Luo, W. Liu, 和 S. Gao, “正常图：基于空间时间图卷积网络的骨架视频异常检测预测网络”，*神经计算*，第444卷，第332–337页，2021年。'
- en: '[17] R. Morais, V. Le, T. Tran, B. Saha, M. Mansour, and S. Venkatesh, “Learning
    regularity in skeleton trajectories for anomaly detection in videos,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2019,
    pp. 11 996–12 004.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R. Morais, V. Le, T. Tran, B. Saha, M. Mansour, 和 S. Venkatesh, “学习骨架轨迹中的规律性用于视频异常检测”，发表于*IEEE/CVF计算机视觉与模式识别大会论文集*，2019年，第11 996–12 004页。'
- en: '[18] A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and T. Gedeon, “Video
    and image based emotion recognition challenges in the wild: Emotiw 2015,” in *Proceedings
    of the 2015 ACM on international conference on multimodal interaction*, 2015,
    pp. 423–426.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, 和 T. Gedeon, “基于视频和图像的情感识别挑战：Emotiw
    2015”，发表于*2015年ACM国际多模态互动会议论文集*，2015年，第423–426页。'
- en: '[19] B. Taati, S. Zhao, A. B. Ashraf, A. Asgarian, M. E. Browne, K. M. Prkachin,
    A. Mihailidis, and T. Hadjistavropoulos, “Algorithmic bias in clinical populations—evaluating
    and improving facial analysis technology in older adults with dementia,” *IEEE
    Access*, vol. 7, pp. 25 527–25 534, 2019.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] B. Taati, S. Zhao, A. B. Ashraf, A. Asgarian, M. E. Browne, K. M. Prkachin,
    A. Mihailidis, 和 T. Hadjistavropoulos, “临床人群中的算法偏见——评估和改进老年痴呆症患者面部分析技术”，*IEEE
    Access*，第7卷，第25 527–25 534页，2019年。'
- en: '[20] G. Menchetti, Z. Chen, D. J. Wilkie, R. Ansari, Y. Yardimci, and A. E.
    Çetin, “Pain detection from facial videos using two-stage deep learning,” in *2019
    IEEE Global Conference on Signal and Information Processing (GlobalSIP)*.   IEEE,
    2019, pp. 1–5.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] G. Menchetti, Z. Chen, D. J. Wilkie, R. Ansari, Y. Yardimci, 和 A. E. Çetin,
    “利用两阶段深度学习从面部视频中检测疼痛”，发表于*2019 IEEE全球信号与信息处理大会（GlobalSIP）*。IEEE，2019年，第1–5页。'
- en: '[21] X. Chen, J. Cheng, R. Song, Y. Liu, R. Ward, and Z. J. Wang, “Video-based
    heart rate measurement: Recent advances and future prospects,” *IEEE Transactions
    on Instrumentation and Measurement*, vol. 68, no. 10, pp. 3600–3615, 2018.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] X. Chen, J. Cheng, R. Song, Y. Liu, R. Ward, 和 Z. J. Wang, “基于视频的心率测量：近期进展与未来展望”，*IEEE仪器与测量汇刊*，第68卷，第10期，第3600–3615页，2018年。'
- en: '[22] T. Gatt, D. Seychell, and A. Dingli, “Detecting human abnormal behaviour
    through a video generated model,” in *2019 11th International Symposium on Image
    and Signal Processing and Analysis (ISPA)*.   IEEE, 2019, pp. 264–270.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] T. Gatt, D. Seychell, 和 A. Dingli, “通过视频生成模型检测人类异常行为”，发表于*2019年第11届国际图像与信号处理与分析研讨会（ISPA）*。IEEE，2019年，第264–270页。'
- en: '[23] O. Temuroglu, Y. Kawanishi, D. Deguchi, T. Hirayama, I. Ide, H. Murase,
    M. Iwasaki, and A. Tsukada, “Occlusion-aware skeleton trajectory representation
    for abnormal behavior detection,” in *International Workshop on Frontiers of Computer
    Vision*.   Springer, 2020, pp. 108–121.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] O. Temuroglu, Y. Kawanishi, D. Deguchi, T. Hirayama, I. Ide, H. Murase,
    M. Iwasaki, 和 A. Tsukada, “用于异常行为检测的遮挡感知骨架轨迹表示”，发表于*计算机视觉前沿国际研讨会*。Springer，2020年，第108–121页。'
- en: '[24] S. Suzuki, Y. Amemiya, and M. Sato, “Skeleton-based visualization of poor
    body movements in a child’s gross-motor assessment using convolutional auto-encoder,”
    in *2021 IEEE International Conference on Mechatronics (ICM)*.   IEEE, 2021, pp.
    1–6.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Suzuki, Y. Amemiya, 和 M. Sato, “利用卷积自编码器对儿童粗大运动评估中不良身体动作进行骨架可视化”，发表于*2021
    IEEE国际机电一体化会议（ICM）*。IEEE，2021年，第1–6页。'
- en: '[25] Z. Jiang, G. Song, Y. Qian, and Y. Wang, “A deep learning framework for
    detecting and localizing abnormal pedestrian behaviors at grade crossings,” *Neural
    Computing and Applications*, pp. 1–15, 2022.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Z. Jiang, G. Song, Y. Qian, 和 Y. Wang, “用于检测和定位铁路道口异常行人行为的深度学习框架”，*神经计算与应用*，第1–15页，2022年。'
- en: '[26] G. Song, Y. Qian, and Y. Wang, “Analysis of abnormal pedestrian behaviors
    at grade crossings based on semi-supervised generative adversarial networks,”
    *Applied Intelligence*, pp. 1–16, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] G. Song, Y. Qian, 和 Y. Wang, “基于半监督生成对抗网络的铁路道口异常行人行为分析”，*应用智能*，第1–16页，2023年。'
- en: '[27] Z. Fan, S. Yi, D. Wu, Y. Song, M. Cui, and Z. Liu, “Video anomaly detection
    using cyclegan based on skeleton features,” *Journal of Visual Communication and
    Image Representation*, vol. 85, p. 103508, 2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Fan, S. Yi, D. Wu, Y. Song, M. Cui, 和 Z. Liu, “基于骨架特征的CycleGAN视频异常检测”，*视觉通信与图像表示期刊*，第85卷，第103508页，2022年。'
- en: '[28] R. Rodrigues, N. Bhargava, R. Velmurugan, and S. Chaudhuri, “Multi-timescale
    trajectory prediction for abnormal human activity detection,” in *Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision*, 2020, pp.
    2626–2634.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Rodrigues, N. Bhargava, R. Velmurugan, 和 S. Chaudhuri, “用于异常人类活动检测的多时间尺度轨迹预测”，发表于*IEEE/CVF
    计算机视觉应用冬季会议论文集*，2020年，第2626–2634页。'
- en: '[29] X. Zeng, Y. Jiang, W. Ding, H. Li, Y. Hao, and Z. Qiu, “A hierarchical
    spatio-temporal graph convolutional neural network for anomaly detection in videos,”
    *IEEE Transactions on Circuits and Systems for Video Technology*, vol. 33, no. 1,
    pp. 200–212, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] X. Zeng, Y. Jiang, W. Ding, H. Li, Y. Hao, 和 Z. Qiu, “用于视频异常检测的分层时空图卷积神经网络”，*IEEE电路与系统视频技术汇刊*，第33卷，第1期，第200–212页，2023年。'
- en: '[30] B. Fan, P. Li, S. Jin, and Z. Wang, “Anomaly detection based on pose estimation
    and gru-ffn,” in *2021 IEEE Sustainable Power and Energy Conference (iSPEC)*.   IEEE,
    2021, pp. 3821–3825.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] B. Fan, P. Li, S. Jin, 和 Z. Wang, “基于姿态估计和GRU-FFN的异常检测”，发表于*2021 IEEE可持续电力与能源会议（iSPEC）*。IEEE，2021年，第3821–3825页。'
- en: '[31] W. Pang, Q. He, and Y. Li, “Predicting skeleton trajectories using a skeleton-transformer
    for video anomaly detection,” *Multimedia Systems*, pp. 1–14, 2022.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] W. Pang, Q. He, 和 Y. Li, “使用骨架变换器预测骨架轨迹以进行视频异常检测”，*多媒体系统*，第1–14页，2022年。'
- en: '[32] C. Huang, Y. Liu, Z. Zhang, C. Liu, J. Wen, Y. Xu, and Y. Wang, “Hierarchical
    graph embedded pose regularity learning via spatio-temporal transformer for abnormal
    behavior detection,” in *Proceedings of the 30th ACM International Conference
    on Multimedia*, 2022, pp. 307–315.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] C. Huang, Y. Liu, Z. Zhang, C. Liu, J. Wen, Y. Xu, 和 Y. Wang，“通过时空变换器进行的分层图嵌入姿态规则学习用于异常行为检测”，在*第30届ACM国际多媒体会议论文集*，2022年，第307–315页。'
- en: '[33] Y. Li and Z. Zhang, “Video abnormal behavior detection based on human
    skeletal information and gru,” in *International Conference on Intelligent Robotics
    and Applications*.   Springer, 2022, pp. 450–458.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Li 和 Z. Zhang，“基于人体骨架信息和GRU的视频异常行为检测”，在*国际智能机器人与应用会议*上。 Springer，2022年，第450–458页。'
- en: '[34] N. Li, F. Chang, and C. Liu, “Human-related anomalous event detection
    via spatial-temporal graph convolutional autoencoder with embedded long short-term
    memory network,” *Neurocomputing*, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] N. Li, F. Chang, 和 C. Liu，“通过嵌入长短期记忆网络的时空图卷积自编码器进行与人相关的异常事件检测”，*神经计算*，2021年。'
- en: '[35] T.-H. Wu, C.-L. Yang, L.-L. Chiu, T.-W. Wang, G. J. Faure, and S.-H. Lai,
    “Confidence-aware anomaly detection in human actions,” in *Asian Conference on
    Pattern Recognition*.   Springer, 2022, pp. 240–254.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T.-H. Wu, C.-L. Yang, L.-L. Chiu, T.-W. Wang, G. J. Faure, 和 S.-H. Lai，“信心感知的人类动作异常检测”，在*亚洲模式识别会议*上。
    Springer，2022年，第240–254页。'
- en: '[36] S. Luo, S. Wang, Y. Wu, and C. Jin, “Memory enhanced spatial-temporal
    graph convolutional autoencoder for human-related video anomaly detection,” in
    *Chinese Conference on Pattern Recognition and Computer Vision (PRCV)*.   Springer,
    2022, pp. 665–677.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Luo, S. Wang, Y. Wu, 和 C. Jin，“记忆增强的时空图卷积自编码器用于与人相关的视频异常检测”，在*中国模式识别与计算机视觉会议（PRCV）*上。
    Springer，2022年，第665–677页。'
- en: '[37] N. Li, F. Chang, and C. Liu, “Human-related anomalous event detection
    via memory-augmented wasserstein generative adversarial network with gradient
    penalty,” *Pattern Recognition*, vol. 138, p. 109398, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] N. Li, F. Chang, 和 C. Liu，“通过记忆增强的Wasserstein生成对抗网络及梯度惩罚进行与人相关的异常事件检测”，*模式识别*，第138卷，第109398页，2023年。'
- en: '[38] A. Markovitz, G. Sharir, I. Friedman, L. Zelnik-Manor, and S. Avidan,
    “Graph embedded pose clustering for anomaly detection,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp. 10 539–10 547.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Markovitz, G. Sharir, I. Friedman, L. Zelnik-Manor, 和 S. Avidan，“图嵌入姿态聚类用于异常检测”，在*IEEE/CVF计算机视觉与模式识别大会论文集*，2020年，第10 539–10 547页。'
- en: '[39] T. Cui, W. Song, G. An, and Q. Ruan, “Prototype generation based shift
    graph convolutional network for semi-supervised anomaly detection,” in *Chinese
    Conference on Image and Graphics Technologies*.   Springer, 2021, pp. 159–169.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] T. Cui, W. Song, G. An, 和 Q. Ruan，“基于原型生成的偏移图卷积网络用于半监督异常检测”，在*中国图像与图形技术会议*上。
    Springer，2021年，第159–169页。'
- en: '[40] C. Liu, R. Fu, Y. Li, Y. Gao, L. Shi, and W. Li, “A self-attention augmented
    graph convolutional clustering networks for skeleton-based video anomaly behavior
    detection,” *Applied Sciences*, vol. 12, no. 1, p. 4, 2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C. Liu, R. Fu, Y. Li, Y. Gao, L. Shi, 和 W. Li，“一种自注意力增强的图卷积聚类网络用于基于骨架的视频异常行为检测”，*应用科学*，第12卷，第1期，第4页，2022年。'
- en: '[41] X. Chen, S. Kan, F. Zhang, Y. Cen, L. Zhang, and D. Zhang, “Multiscale
    spatial temporal attention graph convolution network for skeleton-based anomaly
    behavior detection,” *Journal of Visual Communication and Image Representation*,
    vol. 90, p. 103707, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] X. Chen, S. Kan, F. Zhang, Y. Cen, L. Zhang, 和 D. Zhang，“多尺度时空注意图卷积网络用于基于骨架的异常行为检测”，*视觉通信与图像表示学报*，第90卷，第103707页，2023年。'
- en: '[42] M. Yan, Y. Xiong, and J. She, “Memory clustering autoencoder method for
    human action anomaly detection on surveillance camera video,” *IEEE Sensors Journal*,
    2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Yan, Y. Xiong, 和 J. She，“用于监控摄像头视频中人类动作异常检测的记忆聚类自编码器方法”，*IEEE传感器学报*，2023年。'
- en: '[43] Y. Yang, Z. Fu, and S. M. Naqvi, “A two-stream information fusion approach
    to abnormal event detection in video,” in *ICASSP 2022-2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE, 2022,
    pp. 5787–5791.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Yang, Z. Fu, 和 S. M. Naqvi，“一种双流信息融合方法用于视频异常事件检测”，在*ICASSP 2022-2022
    IEEE国际声学、语音与信号处理会议（ICASSP）*上。 IEEE，2022年，第5787–5791页。'
- en: '[44] M. H. Javed, Z. Yu, T. Li, N. Anwar, and T. M. Rajeh, “learning anomalous
    human actions using frames of interest and decoderless deep embedded clustering,”
    *International Journal of Machine Learning and Cybernetics*, pp. 1–15, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. H. Javed, Z. Yu, T. Li, N. Anwar, 和 T. M. Rajeh，“利用兴趣帧和无解码器深度嵌入聚类学习异常人类行为”，*国际机器学习与网络安全学报*，第1–15页，2023年。'
- en: '[45] N. Li, F. Chang, and C. Liu, “A self-trained spatial graph convolutional
    network for unsupervised human-related anomalous event detection in complex scenes,”
    *IEEE Transactions on Cognitive and Developmental Systems*, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] N. Li, F. Chang, 和 C. Liu, “用于复杂场景中无监督人体相关异常事件检测的自训练空间图卷积网络”，*IEEE 认知与发展系统汇刊*，2022。'
- en: '[46] H. Tani and T. Shibata, “Frame-wise action recognition training framework
    for skeleton-based anomaly behavior detection,” in *International Conference on
    Image Analysis and Processing*.   Springer, 2022, pp. 312–323.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Tani 和 T. Shibata, “基于骨架的异常行为检测的逐帧动作识别训练框架”，发表于 *图像分析与处理国际会议*。 Springer，2022，页
    312–323。'
- en: '[47] F. Sato, R. Hachiuma, and T. Sekii, “Prompt-guided zero-shot anomaly action
    recognition using pretrained deep skeleton features,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2023, pp. 6471–6480.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] F. Sato, R. Hachiuma, 和 T. Sekii, “使用预训练深度骨架特征的提示引导零样本异常动作识别”，发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2023，页 6471–6480。'
- en: '[48] L. Song, G. Yu, J. Yuan, and Z. Liu, “Human pose estimation and its application
    to action recognition: A survey,” *Journal of Visual Communication and Image Representation*,
    vol. 76, p. 103055, 2021.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] L. Song, G. Yu, J. Yuan, 和 Z. Liu, “人体姿态估计及其在动作识别中的应用：综述”，*视觉通信与图像表示杂志*，第
    76 卷，页 103055，2021。'
- en: '[49] A. Badiola-Bengoa and A. Mendez-Zorrilla, “A systematic review of the
    application of camera-based human pose estimation in the field of sport and physical
    exercise,” *Sensors*, vol. 21, no. 18, p. 5996, 2021.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. Badiola-Bengoa 和 A. Mendez-Zorrilla, “基于摄像头的人体姿态估计在体育和身体锻炼领域应用的系统综述”，*传感器*，第
    21 卷，第 18 期，页 5996，2021。'
- en: '[50] K. Boekhoudt and E. Talavera, “Spatial-temporal transformer for crime
    recognition in surveillance videos,” in *2022 18th IEEE International Conference
    on Advanced Video and Signal Based Surveillance (AVSS)*.   IEEE, 2022, pp. 1–8.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] K. Boekhoudt 和 E. Talavera, “用于监控视频中犯罪识别的时空变换器”，发表于 *2022 第 18 届 IEEE
    国际高级视频与信号监测会议 (AVSS)*。 IEEE，2022，页 1–8。'
- en: '[51] W. Du, Y. Wang, and Y. Qiao, “Rpan: An end-to-end recurrent pose-attention
    network for action recognition in videos,” in *Proceedings of the IEEE international
    conference on computer vision*, 2017, pp. 3725–3734.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] W. Du, Y. Wang, 和 Y. Qiao, “Rpan：用于视频中动作识别的端到端递归姿态注意网络”，发表于 *IEEE 国际计算机视觉会议论文集*，2017，页
    3725–3734。'
- en: '[52] S. Suzuki, Y. Amemiya, and M. Sato, “Enhancement of child gross-motor
    action recognition by motional time-series images conversion,” in *2020 IEEE/SICE
    International Symposium on System Integration (SII)*.   IEEE, 2020, pp. 225–230.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Suzuki, Y. Amemiya, 和 M. Sato, “通过运动时间序列图像转换提高儿童粗大动作识别能力”，发表于 *2020
    IEEE/SICE 国际系统集成研讨会 (SII)*。 IEEE，2020，页 225–230。'
- en: '[53] D. Gong, L. Liu, V. Le, B. Saha, M. R. Mansour, S. Venkatesh, and A. v. d.
    Hengel, “Memorizing normality to detect anomaly: Memory-augmented deep autoencoder
    for unsupervised anomaly detection,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 1705–1714.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] D. Gong, L. Liu, V. Le, B. Saha, M. R. Mansour, S. Venkatesh, 和 A. v.
    d. Hengel, “通过记忆正常性来检测异常：用于无监督异常检测的记忆增强深度自编码器”，发表于 *IEEE/CVF 国际计算机视觉会议论文集*，2019，页
    1705–1714。'
- en: '[54] Y. Tang, L. Zhao, S. Zhang, C. Gong, G. Li, and J. Yang, “Integrating
    prediction and reconstruction for anomaly detection,” *Pattern Recognition Letters*,
    vol. 129, pp. 123–130, 2020.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Y. Tang, L. Zhao, S. Zhang, C. Gong, G. Li, 和 J. Yang, “将预测和重建结合用于异常检测”，*模式识别快报*，第
    129 卷，页 123–130，2020。'
- en: '[55] W. Luo, W. Liu, and S. Gao, “A revisit of sparse coding based anomaly
    detection in stacked rnn framework,” in *Proceedings of the IEEE international
    conference on computer vision*, 2017, pp. 341–349.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] W. Luo, W. Liu, 和 S. Gao, “在堆叠 RNN 框架中对基于稀疏编码的异常检测的重新审视”，发表于 *IEEE 国际计算机视觉会议论文集*，2017，页
    341–349。'
- en: '[56] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “Ntu rgb+ d: A large scale
    dataset for 3d human activity analysis,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2016, pp. 1010–1019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Shahroudy, J. Liu, T.-T. Ng, 和 G. Wang, “Ntu rgb+ d：用于 3D 人体活动分析的大规模数据集”，发表于
    *IEEE 计算机视觉与模式识别会议论文集*，2016，页 1010–1019。'
- en: '[57] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *et al.*, “The kinetics human action video
    dataset,” *arXiv preprint arXiv:1705.06950*, 2017.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan,
    F. Viola, T. Green, T. Back, P. Natsev *等*，“Kinetics 人体动作视频数据集”，*arXiv 预印本 arXiv:1705.06950*，2017。'
- en: '[58] S. S. Khan and A. Ahmad, “Cluster center initialization algorithm for
    k-means clustering,” *Pattern recognition letters*, vol. 25, no. 11, pp. 1293–1302,
    2004.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] S. S. Khan 和 A. Ahmad，“k-means 聚类的簇中心初始化算法，” *模式识别通讯*，第 25 卷，第 11 期，第
    1293–1302 页，2004 年。'
- en: '[59] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” *arXiv
    preprint arXiv:1804.02767*, 2018.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J. Redmon 和 A. Farhadi，“Yolov3: 一种增量改进，” *arXiv 预印本 arXiv:1804.02767*，2018
    年。'
- en: '[60] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly detection,”
    *ACM Transactions on Knowledge Discovery from Data (TKDD)*, vol. 6, no. 1, pp.
    1–39, 2012.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] F. T. Liu, K. M. Ting 和 Z.-H. Zhou，“基于隔离的异常检测，” *ACM 知识发现数据的交易 (TKDD)*，第
    6 卷，第 1 期，第 1–39 页，2012 年。'
- en: '[61] L. Shi, Y. Zhang, J. Cheng, and H. Lu, “Two-stream adaptive graph convolutional
    networks for skeleton-based action recognition,” in *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, 2019, pp. 12 026–12 035.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] L. Shi, Y. Zhang, J. Cheng 和 H. Lu，“用于骨架基于动作识别的双流自适应图卷积网络，” 见于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，2019 年，第 12 026–12 035 页。'
- en: '[62] S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional networks
    for skeleton-based action recognition,” in *Thirty-second AAAI conference on artificial
    intelligence*, 2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Yan, Y. Xiong 和 D. Lin，“基于骨架的动作识别的时空图卷积网络，” 见于 *第三十二届 AAAI 人工智能会议*，2018
    年。'
- en: '[63] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
    and the kinetics dataset,” in *proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017, pp. 6299–6308.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J. Carreira 和 A. Zisserman，“Quo vadis, 行为识别？一种新模型及其 Kinetics 数据集，” 见于
    *IEEE 计算机视觉与模式识别会议论文集*，2017 年，第 6299–6308 页。'
- en: '[64] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C. Kot, “Ntu
    rgb+ d 120: A large-scale benchmark for 3d human activity understanding,” *IEEE
    transactions on pattern analysis and machine intelligence*, vol. 42, no. 10, pp.
    2684–2701, 2019.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan 和 A. C. Kot，“NTU RGB+D
    120: 一个用于 3D 人体活动理解的大规模基准，” *IEEE 模式分析与机器智能汇刊*，第 42 卷，第 10 期，第 2684–2701 页，2019
    年。'
- en: '[65] D. M. Blei and M. I. Jordan, “Variational inference for dirichlet process
    mixtures,” *Bayesian Analysis*, vol. 1, no. 1, pp. 121–144, 2006.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] D. M. Blei 和 M. I. Jordan，“Dirichlet 过程混合模型的变分推断，” *贝叶斯分析*，第 1 卷，第 1 期，第
    121–144 页，2006 年。'
- en: '[66] C. Lu, J. Shi, and J. Jia, “Abnormal event detection at 150 fps in matlab,”
    in *Proceedings of the IEEE international conference on computer vision*, 2013,
    pp. 2720–2727.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] C. Lu, J. Shi 和 J. Jia，“在 Matlab 中以 150 fps 进行异常事件检测，” 见于 *IEEE 国际计算机视觉会议论文集*，2013
    年，第 2720–2727 页。'
- en: '[67] W. Sultani, C. Chen, and M. Shah, “Real-world anomaly detection in surveillance
    videos,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2018, pp. 6479–6488.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] W. Sultani, C. Chen 和 M. Shah，“监控视频中的现实世界异常检测，” 见于 *IEEE 计算机视觉与模式识别会议论文集*，2018
    年，第 6479–6488 页。'
- en: '[68] “Umn,” http://mha.cs.umn.edu/proj_events.shtml.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] “UMN，” http://mha.cs.umn.edu/proj_events.shtml。'
- en: '[69] C. Chen, R. Jafari, and N. Kehtarnavaz, “Utd-mhad: A multimodal dataset
    for human action recognition utilizing a depth camera and a wearable inertial
    sensor,” in *2015 IEEE International conference on image processing (ICIP)*.   IEEE,
    2015, pp. 168–172.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] C. Chen, R. Jafari 和 N. Kehtarnavaz，“UTD-MHAD: 一个利用深度相机和可穿戴惯性传感器的人体动作识别多模态数据集，”
    见于 *2015 IEEE 国际图像处理会议 (ICIP)*。IEEE，2015 年，第 168–172 页。'
- en: '[70] M. Cheng, K. Cai, and M. Li, “Rwf-2000: An open large scale video database
    for violence detection,” in *2020 25th International Conference on Pattern Recognition
    (ICPR)*, 2021, pp. 4183–4190.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] M. Cheng, K. Cai 和 M. Li，“RWF-2000: 一个开放的大规模视频数据库用于暴力检测，” 见于 *2020 第25届国际模式识别大会
    (ICPR)*，2021 年，第 4183–4190 页。'
- en: '[71] H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu, “Rmpe: Regional multi-person
    pose estimation,” in *Proceedings of the IEEE international conference on computer
    vision*, 2017, pp. 2334–2343.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] H.-S. Fang, S. Xie, Y.-W. Tai 和 C. Lu，“RMPE: 区域多人姿态估计，” 见于 *IEEE 国际计算机视觉会议论文集*，2017
    年，第 2334–2343 页。'
- en: '[72] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d
    pose estimation using part affinity fields,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 7291–7299.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Z. Cao, T. Simon, S.-E. Wei 和 Y. Sheikh，“使用部分关联场的实时多人二维姿态估计，” 见于 *IEEE
    计算机视觉与模式识别会议论文集*，2017 年，第 7291–7299 页。'
- en: '[73] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler,
    and K. Murphy, “Towards accurate multi-person pose estimation in the wild,” in
    *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2017, pp. 4903–4911.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tompson, C. Bregler,
    和 K. Murphy，“朝着准确的野外多人姿态估计迈进，” 载于 *IEEE计算机视觉与模式识别会议论文集*，2017年，第4903–4911页。'
- en: '[74] T. Sekii, “Pose proposal networks,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 342–357.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] T. Sekii，“姿态提议网络，” 载于 *欧洲计算机视觉会议（ECCV）论文集*，2018年，第342–357页。'
- en: '[75] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation
    learning for human pose estimation,” in *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, 2019, pp. 5693–5703.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Sun, B. Xiao, D. Liu, 和 J. Wang，“用于人体姿态估计的深度高分辨率表示学习，” 载于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第5693–5703页。'
- en: '[76] L. Ruff, J. R. Kauffmann, R. A. Vandermeulen, G. Montavon, W. Samek, M. Kloft,
    T. G. Dietterich, and K.-R. Müller, “A unifying review of deep and shallow anomaly
    detection,” *Proceedings of the IEEE*, vol. 109, no. 5, pp. 756–795, 2021.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] L. Ruff, J. R. Kauffmann, R. A. Vandermeulen, G. Montavon, W. Samek, M.
    Kloft, T. G. Dietterich, 和 K.-R. Müller，“深度与浅层异常检测的统一综述，” *IEEE会议论文集*，第109卷，第5期，第756–795页，2021年。'
- en: '[77] S. S. Khan and J. Hoey, “dtfall: decision-theoretic framework to report
    unseen falls.” in *PervasiveHealth*, 2016, pp. 146–153.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. S. Khan 和 J. Hoey，“dtfall：用于报告未见跌倒的决策理论框架。” 载于 *普适健康*，2016年，第146–153页。'
- en: '[78] T. Saito and M. Rehmsmeier, “The precision-recall plot is more informative
    than the roc plot when evaluating binary classifiers on imbalanced datasets,”
    *PloS one*, vol. 10, no. 3, p. e0118432, 2015.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] T. Saito 和 M. Rehmsmeier，“在评估不平衡数据集上的二分类器时，精确度-召回率图比ROC图更具信息量，” *PloS
    One*，第10卷，第3期，第e0118432页，2015年。'
- en: '[79] Y. M. Galvão, L. Portela, J. Ferreira, P. Barros, O. A. D. A. Fagundes,
    and B. J. Fernandes, “A framework for anomaly identification applied on fall detection,”
    *IEEE Access*, vol. 9, pp. 77 264–77 274, 2021.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y. M. Galvão, L. Portela, J. Ferreira, P. Barros, O. A. D. A. Fagundes,
    和 B. J. Fernandes，“用于异常检测的框架应用于跌倒检测，” *IEEE Access*，第9卷，第77,264–77,274页，2021年。'
- en: '[80] S. S. Khan, P. K. Mishra, N. Javed, B. Ye, K. Newman, A. Mihailidis, and
    A. Iaboni, “Unsupervised deep learning to detect agitation from videos in people
    with dementia,” *IEEE Access*, vol. 10, pp. 10 349–10 358, 2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S. S. Khan, P. K. Mishra, N. Javed, B. Ye, K. Newman, A. Mihailidis, 和
    A. Iaboni，“使用无监督深度学习从老年痴呆症患者的视频中检测激动，” *IEEE Access*，第10卷，第10,349–10,358页，2022年。'
- en: '[81] Y. Chen, Y. Tian, and M. He, “Monocular human pose estimation: A survey
    of deep learning-based methods,” *Computer Vision and Image Understanding*, vol.
    192, p. 102897, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Chen, Y. Tian, 和 M. He，“单目人体姿态估计：基于深度学习方法的综述，” *计算机视觉与图像理解*，第192卷，第102897页，2020年。'
- en: '[82] Y. Cheng, B. Yang, B. Wang, W. Yan, and R. T. Tan, “Occlusion-aware networks
    for 3d human pose estimation in video,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 723–732.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Cheng, B. Yang, B. Wang, W. Yan, 和 R. T. Tan，“考虑遮挡的网络用于视频中的3D人体姿态估计，”
    载于 *IEEE/CVF国际计算机视觉会议论文集*，2019年，第723–732页。'
- en: '[83] S. Gong, T. Xiang, and S. Hongeng, “Learning human pose in crowd,” in
    *Proceedings of the 1st ACM international workshop on Multimodal pervasive video
    analysis*, 2010, pp. 47–52.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Gong, T. Xiang, 和 S. Hongeng，“在拥挤人群中学习人体姿态，” 载于 *第1届ACM国际多模态普适视频分析研讨会*，2010年，第47–52页。'
- en: '[84] U. Iqbal and J. Gall, “Multi-person pose estimation with local joint-to-person
    associations,” in *European conference on computer vision*.   Springer, 2016,
    pp. 627–642.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] U. Iqbal 和 J. Gall，“带有局部关节到人员关联的多人姿态估计，” 载于 *欧洲计算机视觉会议*。  Springer，2016年，第627–642页。'
- en: '[85] S. S. Khan, M. E. Karg, D. Kulić, and J. Hoey, “Detecting falls with x-factor
    hidden markov models,” *Applied Soft Computing*, vol. 55, pp. 168–177, 2017.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. S. Khan, M. E. Karg, D. Kulić, 和 J. Hoey，“使用X因素隐马尔可夫模型检测跌倒，” *应用软计算*，第55卷，第168–177页，2017年。'
- en: '[86] S. A. Boamah, R. Weldrick, T.-S. J. Lee, and N. Taylor, “Social isolation
    among older adults in long-term care: A scoping review,” *Journal of Aging and
    Health*, vol. 33, no. 7-8, pp. 618–632, 2021.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] S. A. Boamah, R. Weldrick, T.-S. J. Lee, 和 N. Taylor，“长期护理中的老年人社会孤立：范围评估，”
    *老龄化与健康杂志*，第33卷，第7-8期，第618–632页，2021年。'
- en: '[87] T.-N. Nguyen, H.-H. Huynh, and J. Meunier, “Skeleton-based abnormal gait
    detection,” *Sensors*, vol. 16, no. 11, p. 1792, 2016.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] T.-N. Nguyen, H.-H. Huynh, 和 J. Meunier，“基于骨架的异常步态检测，” *Sensors*，第16卷，第11期，第1792页，2016年。'
- en: '[88] R. Baptista, G. Demisse, D. Aouada, and B. Ottersten, “Deformation-based
    abnormal motion detection using 3d skeletons,” in *2018 Eighth International Conference
    on Image Processing Theory, Tools and Applications (IPTA)*.   IEEE, 2018, pp.
    1–6.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] R. Baptista, G. Demisse, D. Aouada 和 B. Ottersten，“基于变形的异常运动检测使用3D骨架，”
    在 *2018年第八届图像处理理论、工具与应用国际会议（IPTA）* 上。IEEE，2018年，第1–6页。'
- en: '[89] T. Warren, “Microsoft kills off Kinect, stops manufacturing it,” https://www.theverge.com/2017/10/25/16542870/microsoft-kinect-dead-stop-manufacturing,
    2017, [Online; accessed 23-February-2022].'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] T. Warren，“微软停产Kinect，停止生产，” https://www.theverge.com/2017/10/25/16542870/microsoft-kinect-dead-stop-manufacturing，2017年，[在线；访问日期：2022年2月23日]。'
- en: '[90] “Vicon,” https://www.vicon.com/, 2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] “Vicon，” https://www.vicon.com/，2019年。'
- en: '[91] “Thecaptury,” https://thecaptury.com/, 2019.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] “Thecaptury，” https://thecaptury.com/，2019年。'
- en: '[92] AltumView, “Sentinare 2,” https://altumview.ca/, 2022, [Online; accessed
    24-February-2022].'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] AltumView，“Sentinare 2，” https://altumview.ca/，2022年，[在线；访问日期：2022年2月24日]。'
- en: '[93] S. S. Khan, P. K. Mishra, B. Ye, K. Newman, A. Iaboni, and A. Mihailidis,
    “Empirical thresholding on spatio-temporal autoencoders trained on surveillance
    videos in a dementia care unit,” in *Conference on Robots and Vision*, to be published.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. S. Khan, P. K. Mishra, B. Ye, K. Newman, A. Iaboni 和 A. Mihailidis，“在痴呆症护理单元中对监控视频进行空间-时间自编码器的经验阈值处理，”
    在 *机器人与视觉会议* 上，待发表。'
- en: '[94] E. Duman and O. A. Erdem, “Anomaly detection in videos using optical flow
    and convolutional autoencoder,” *IEEE Access*, vol. 7, pp. 183 914–183 923, 2019.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] E. Duman 和 O. A. Erdem，“使用光流和卷积自编码器进行视频异常检测，” *IEEE Access*，第7卷，第183 914–183 923页，2019年。'
- en: '[95] A. Abedi and S. S. Khan, “Fedsl: Federated split learning on distributed
    sequential data in recurrent neural networks,” *arXiv preprint arXiv:2011.03180*,
    2020.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] A. Abedi 和 S. S. Khan，“Fedsl：分布式递归神经网络上的联邦分割学习，” *arXiv预印本 arXiv:2011.03180*，2020年。'
- en: '[96] J. Yan, F. Angelini, and S. M. Naqvi, “Image segmentation based privacy-preserving
    human action recognition for anomaly detection,” in *ICASSP 2020-2020 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE, 2020,
    pp. 8931–8935.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Yan, F. Angelini 和 S. M. Naqvi，“基于图像分割的隐私保护人类动作识别用于异常检测，” 在 *ICASSP
    2020-2020 IEEE国际声学、语音与信号处理会议（ICASSP）* 上。IEEE，2020年，第8931–8935页。'
- en: '[97] P. K. Mishra, A. Iaboni, B. Ye, K. Newman, A. Mihailidis, and S. S. Khan,
    “Privacy-protecting behaviours of risk detection in people with dementia using
    videos,” *BioMedical Engineering OnLine*, vol. 22, no. 1, pp. 1–17, 2023.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] P. K. Mishra, A. Iaboni, B. Ye, K. Newman, A. Mihailidis 和 S. S. Khan，“使用视频进行痴呆症患者风险检测的隐私保护行为，”
    *生物医学工程在线*，第22卷，第1期，第1–17页，2023年。'
- en: '[98] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding, “3d human
    pose estimation with spatial and temporal transformers,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 11 656–11 665.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen 和 Z. Ding，“使用空间和时间变换器的3D人体姿态估计，”
    在 *IEEE/CVF国际计算机视觉会议论文集*，2021年，第11 656–11 665页。'
- en: '[99] C.-L. Zhang, J. Wu, and Y. Li, “Actionformer: Localizing moments of actions
    with transformers,” in *European Conference on Computer Vision*.   Springer, 2022,
    pp. 492–510.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] C.-L. Zhang, J. Wu 和 Y. Li，“Actionformer：使用变换器定位动作的时刻，” 在 *欧洲计算机视觉大会*
    上。Springer，2022年，第492–510页。'
- en: '[100] W. Feng, R. Liu, and M. Zhu, “Fall detection for elderly person care
    in a vision-based home surveillance environment using a monocular camera,” *signal,
    image and video processing*, vol. 8, no. 6, pp. 1129–1138, 2014.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] W. Feng, R. Liu 和 M. Zhu，“基于视觉的家居监控环境中的老人跌倒检测，使用单目摄像机，” *信号、图像与视频处理*，第8卷，第6期，第1129–1138页，2014年。'
- en: '[101] H. Wang and L. Wang, “Learning content and style: Joint action recognition
    and person identification from human skeletons,” *Pattern Recognition*, vol. 81,
    pp. 23–35, 2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] H. Wang 和 L. Wang，“学习内容与风格：基于人体骨架的联合动作识别和个人身份识别，” *模式识别*，第81卷，第23–35页，2018年。'
- en: '[102] R. Liao, S. Yu, W. An, and Y. Huang, “A model-based gait recognition
    method with body pose and human prior knowledge,” *Pattern Recognition*, vol. 98,
    p. 107069, 2020.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] R. Liao, S. Yu, W. An 和 Y. Huang，“基于模型的步态识别方法结合身体姿态和人体先验知识，” *模式识别*，第98卷，第107069页，2020年。'
- en: '[103] C. Lugaresi, J. Tang, H. Nash, C. McClanahan, E. Uboweja, M. Hays, F. Zhang,
    C.-L. Chang, M. G. Yong, J. Lee *et al.*, “Mediapipe: A framework for building
    perception pipelines,” *arXiv preprint arXiv:1906.08172*, 2019.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] C. Lugaresi, J. Tang, H. Nash, C. McClanahan, E. Uboweja, M. Hays, F. Zhang,
    C.-L. Chang, M. G. Yong, J. Lee *等*，“Mediapipe：构建感知管道的框架，” *arXiv预印本 arXiv:1906.08172*，2019年。'
- en: '[104] Q. Guo and S. S. Khan, “Exercise-specific feature extraction approach
    for assessing physical rehabilitation,” in *4th IJCAI Workshop on AI for Aging,
    Rehabilitation and Intelligent Assisted Living*.   IJCAI, 2021.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Q. Guo 和 S. S. Khan，“用于评估物理康复的运动特定特征提取方法，”发表于*第4届IJCAI老龄化、康复与智能辅助生活工作坊*。IJCAI,
    2021。'
- en: '[105] J. Kovač and P. Peer, “Human skeleton model based dynamic features for
    walking speed invariant gait recognition,” *Mathematical Problems in Engineering*,
    vol. 2014, 2014.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] J. Kovač 和 P. Peer，“基于人类骨架模型的动态特征用于步态识别的步速不变性，”*工程中的数学问题*，第2014卷，2014年。'
- en: '[106] A. A. Chaaraoui, P. Climent-Pérez, and F. Flórez-Revuelta, “A review
    on vision techniques applied to human behaviour analysis for ambient-assisted
    living,” *Expert Systems with Applications*, vol. 39, no. 12, pp. 10 873–10 888,
    2012.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. A. Chaaraoui, P. Climent-Pérez 和 F. Flórez-Revuelta，“关于应用于环境辅助生活的人的行为分析的视觉技术的综述，”*专家系统与应用*，第39卷，第12期，pp.
    10 873–10 888，2012年。'
- en: '[107] Y. Hbali, S. Hbali, L. Ballihi, and M. Sadgal, “Skeleton-based human
    activity recognition for elderly monitoring systems,” *IET Computer Vision*, vol. 12,
    no. 1, pp. 16–26, 2018.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Y. Hbali, S. Hbali, L. Ballihi 和 M. Sadgal，“基于骨架的人类活动识别用于老年人监控系统，”*IET计算机视觉*，第12卷，第1期，pp.
    16–26，2018年。'
- en: '[108] Š. Obdržálek, G. Kurillo, F. Ofli, R. Bajcsy, E. Seto, H. Jimison, and
    M. Pavel, “Accuracy and robustness of kinect pose estimation in the context of
    coaching of elderly population,” in *2012 Annual International Conference of the
    IEEE Engineering in Medicine and Biology Society*.   IEEE, 2012, pp. 1188–1193.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Š. Obdržálek, G. Kurillo, F. Ofli, R. Bajcsy, E. Seto, H. Jimison, 和
    M. Pavel，“在老年人辅导背景下的Kinect姿态估计的准确性和鲁棒性，”发表于*2012年IEEE医学与生物工程学会年会*。IEEE, 2012,
    pp. 1188–1193。'
- en: '[109] M. Garcia-Salguero, J. Gonzalez-Jimenez, and F.-A. Moreno, “Human 3d
    pose estimation with a tilting camera for social mobile robot interaction,” *Sensors*,
    vol. 19, no. 22, p. 4943, 2019.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. Garcia-Salguero, J. Gonzalez-Jimenez 和 F.-A. Moreno，“使用倾斜摄像机进行社交移动机器人互动的三维人类姿态估计，”*传感器*，第19卷，第22期，p.
    4943，2019年。'
- en: '| ![[Uncaptioned image]](img/9dd1c9f6c92656c35a6ab9f60aa983cb.png) | Pratik
    K. Mishra obtained his Masters in Computer Science and Engineering from the Indian
    Institute of Technology (IIT) Indore, India, in 2020\. He is currently pursuing
    his Ph.D. from the Institute of Biomedical Engineering, University of Toronto
    and working towards the application of computer vision for detecting behaviours
    of risk in people with dementia. Previously, he worked as a research volunteer
    at the Toronto Rehabilitation Institute, Canada and as a Data Management Support
    Specialist at IBM India Private Limited. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| ![[未说明的图像]](img/9dd1c9f6c92656c35a6ab9f60aa983cb.png) | Pratik K. Mishra于2020年获得印度理工学院（IIT）印度印度尔计算机科学与工程硕士学位。他目前在多伦多大学生物医学工程研究所攻读博士学位，并致力于应用计算机视觉检测痴呆症患者的风险行为。之前，他曾在加拿大多伦多康复研究所担任研究志愿者，并在IBM印度私人有限公司担任数据管理支持专家。
    |'
- en: '| ![[Uncaptioned image]](img/29185591c3bf0d6be095f8ee08c270c7.png) | Alex Mihailidis
    , PhD, PEng, is the Barbara G. Stymiest Research Chair in Rehabilitation Technology
    at KITE Research Institute at University Health Network/University of Toronto.
    He is the Scientific Director of the AGE-WELL Network of Centres of Excellence,
    which focuses on the development of new technologies and services for older adults.
    He is a Professor in the Department of Occupational Science and Occupational Therapy
    and in the Institute of Biomedical Engineering at the University of Toronto (U
    of T), as well as holds a cross appointment in the Department of Computer Science
    at the U of T. Mihailidis is very active in the rehabilitation engineering profession
    and is the Immediate Past President for the Rehabilitation Engineering and Assistive
    Technology Society for North America (RESNA) and was named a Fellow of RESNA in
    2014, which is one of the highest honours within this field of research and practice.
    His research disciplines include biomedical and biochemical engineering, computer
    science, geriatrics and occupational therapy. Alex is an internationally recognized
    researcher in the field of technology and aging. He has published over 150 journal
    and conference papers in this field and co-edited two books: Pervasive computing
    in healthcare and Technology and Aging. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/29185591c3bf0d6be095f8ee08c270c7.png) | Alex Mihailidis 博士，PEng，是多伦多大学健康网络/KITE研究所的Barbara
    G. Stymiest康复技术研究主席。他是AGE-WELL卓越中心网络的科学主任，该网络专注于为老年人开发新技术和服务。他是多伦多大学（U of T）职业科学与职业治疗系以及生物医学工程研究所的教授，同时在U
    of T计算机科学系也担任交叉任命。Mihailidis在康复工程领域非常活跃，是北美康复工程与辅助技术学会（RESNA）的前任会长，并于2014年被授予RESNA会士称号，这是该领域研究与实践中的最高荣誉之一。他的研究学科包括生物医学与生物化学工程、计算机科学、老年医学和职业治疗。Alex是技术与老龄化领域国际公认的研究者，已在该领域发表了150多篇期刊和会议论文，并共同编辑了两本书籍：《医疗保健中的普适计算》和《技术与老龄化》。'
- en: '| ![[Uncaptioned image]](img/9f157cabe153e15c388bc9bf80fad244.png) | Shehroz
    S. Khan obtained his B.Sc Engineering, Masters and Phd degrees in computer science
    in 1997, 2010 and 2016\. He is currently working as a Scientist at KITE – Toronto
    Rehabilitation Institute (TRI), University Health Network, Canada. He is also
    cross appointed as an Assistant Professor at the Institute of Biomedical Engineering,
    University of Toronto (UofT). Previously, he worked as a postdoctoral researcher
    at the UofT and TRI. Prior to joining academics, he worked in various scientific
    and researcher roles in the industry and government jobs. He is an associate editor
    of the Journal of Rehabilitation and Assistive Technologies. He has organized
    four editions of the peer-reviewed workshop on AI in Aging, Rehabilitation and
    Intelligent Assisted Living held with top AI conferences (ICDM and IJCAI) from
    2017-2021\. His research is funded through several granting agencies in Canada
    and abroad, including NSERC, CIHR, AGEWELL, SSHRC, CABHI, AMS Healthcare, JP Bickell
    Foundation, United Arab Emirates University and LG Electronics. He has published
    $49$ peer-reviewed research papers and his research focus is the development of
    AI algorithms for solving aging related health problems. |)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![[未标注的图片]](img/9f157cabe153e15c388bc9bf80fad244.png) | Shehroz S. Khan 于1997年、2010年和2016年获得计算机科学的学士、硕士和博士学位。他目前在加拿大健康网络多伦多康复研究所（KITE
    – Toronto Rehabilitation Institute, TRI）担任科学家，同时还在多伦多大学生物医学工程研究所（Institute of
    Biomedical Engineering, UofT）担任助理教授。他曾在UofT和TRI担任博士后研究员。在进入学术界之前，他在工业界和政府部门担任了多种科学和研究角色。他是《康复与辅助技术期刊》的副编辑，并组织了四届与顶级AI会议（ICDM和IJCAI）共同举办的同行评审研讨会，主题为“AI在老龄化、康复和智能辅助生活中的应用”，时间为2017至2021年。他的研究获得了来自加拿大及海外多个资助机构的资助，包括NSERC、CIHR、AGEWELL、SSHRC、CABHI、AMS
    Healthcare、JP Bickell Foundation、阿联酋大学和LG电子。他已发表了$49$篇同行评审的研究论文，其研究重点是开发用于解决老龄化相关健康问题的AI算法。
    |)'
