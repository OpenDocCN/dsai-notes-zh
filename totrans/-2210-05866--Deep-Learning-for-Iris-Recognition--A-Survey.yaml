- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:44:04'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2210.05866] Deep Learning for Iris Recognition: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2210.05866](https://ar5iv.labs.arxiv.org/html/2210.05866)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Iris Recognition: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kien Nguyen Queensland University of TechnologyAustralia [nguyentk@qut.edu.au](mailto:nguyentk@qut.edu.au)
    ,Â  Hugo ProenÃ§a University of Beira Interior, IT: Instituto de TelecomunicaÃ§ÃµesPortugal
    [hugomcp@di.ubi.pt](mailto:hugomcp@di.ubi.pt) Â andÂ  Fernando Alonso-Fernandez
    Halmstad UniversitySweden [feralo@hh.se](mailto:feralo@hh.se)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ABSTRACT
  prefs: []
  type: TYPE_NORMAL
- en: 'In this survey, we provide a comprehensive review of more than 200 papers,
    technical reports, and GitHub repositories published over the last 10 years on
    the recent developments of deep learning techniques for iris recognition, covering
    broad topics on algorithm designs, open-source tools, open challenges, and emerging
    research. First, we conduct a comprehensive analysis of deep learning techniques
    developed for two main sub-tasks in iris biometrics: segmentation and recognition.
    Second, we focus on deep learning techniques for the robustness of iris recognition
    systems against presentation attacks and via human-machine pairing. Third, we
    delve deep into deep learning techniques for forensic application, especially
    in post-mortem iris recognition. Fourth, we review open-source resources and tools
    in deep learning techniques for iris recognition. Finally, we highlight the technical
    challenges, emerging research trends, and outlook for the future of deep learning
    in iris recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Iris Recognition, Deep Learning, Neural Networks^â€ ^â€ ccs: Security and privacyÂ Biometrics'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The human iris is a sight organ that controls the amount of light reaching the
    retina, by changing the size of the pupil. The texture of the iris is fully developed
    before birth, its minutiae do not depend on genotype, it stays relatively stable
    across lifetime (except for disease- and normal aging-related biological changes),
    and it may even be used for forensic identification shortly after subjectâ€™s deathÂ (Muron
    and Pospisil, [2000](#bib.bib111); Daugman, [2016](#bib.bib37); Trokielewicz etÂ al.,
    [2019](#bib.bib171)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of its information theory-related properties, the iris texture has
    an extremely high randotypic randomness, and is stable (permanent) over time,
    providing an exceptionally high entropy per mm.Â² that justifies its higher discriminating
    power, when compared to other biometric modalities (e.g., face or fingerprint).
    The irisâ€™ collectability is another feature of interest and has been the subject
    of discussion over the last years: while it can be acquired using commercial off-the-shelf
    (COTS) hardware, either handheld or stationary, data can be even collected from
    at-a-distance, up to tens of meters away from the subjects (Nguyen etÂ al., [2017a](#bib.bib112)).
    Even though commercial visible-light (RGB) cameras are able to image the iris,
    the near infrared-based (NIR) sensing dominates in most applications, due to a
    better visibility of iris texture for darker eyes, rich in melanin pigment, which
    is characterized by lower light absorption in NIR spectrum compared to shorter
    wavelengths. In addition, NIR wavelengths are barely perceivable by the human
    eye, which augment usersâ€™ comfort, and avoids pupil contraction/dilation that
    would appear under visible light.'
  prefs: []
  type: TYPE_NORMAL
- en: A seminal work by John Daugman brought to the community the Gabor filtering-based
    approach that became the dominant approach for iris recognition (Daugman, [1993](#bib.bib35),
    [2007](#bib.bib36), [2021](#bib.bib38)). Even though subsequent solutions to iris
    image encoding and matching appeared, the IrisCodes approach is still dominant
    due to its ability to effectively search in massive databases with a minimal probability
    of false matches, at extreme time performance. By considering binary words, pairs
    of signatures are matched using XOR parallel-bit logic at lightening speed, enabling
    millions of comparisons/second per processing core. Also, most of the methods
    that outperformed the original techniques in terms of effectiveness do not work
    under the *one-shot learning* paradigm, assume multiple observations of each class
    to obtain appropriate decision boundaries, and - most importantly - have encoding/matching
    steps with time complexity that forbid their use in large environments (in particular,
    for *all-against-all* settings).
  prefs: []
  type: TYPE_NORMAL
- en: In short, Daugmanâ€™s algorithm encodes the iris image into a binary sequence
    of 2,048 bits by filtering the iris image with a family of Gabor kernels. The
    varying pupil size is rectified by the Cartesian-to-polar coordinate system transformation,
    to end up with an image representation of canonical size, guarantying identical
    structure of the iris code independently of the iris and pupil size. This makes
    possible to use the Hamming Distance (HD) to measure the similarity between two
    iris codes (Daugman, [2021](#bib.bib38)). Its low false match rate at acceptable
    false non-match rates is the key factor behind the success of global-scale iris
    recognition installments, such as the national person identification and border
    security program Aadhaar program in India (with over 1.2 billion pairs of irises
    enrolled) (Unique Identification Authority of India, [2021](#bib.bib175)), the
    Homeland Advanced Recognition Technology (HART) in the US (up to 500 million identities)
    (Planet Biometrics, [2017](#bib.bib129)), or the NEXUS system, designed to speed
    up border crossings for low-risk and pre-approved travelers moving between Canada
    and the US.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning-based methods, in particular using various Convolutional Neural
    Network architectures, have been driving remarkable improvements in many computer
    vision applications over the last decade. In terms of biometrics technologies,
    itâ€™s not surprising that iris recognition has also seen an increasing adoption
    of purely data-driven approaches at all stages of the recognition pipeline: from
    preprocessing (such as off-axis gaze correction), segmentation, encoding to matching.
    Interestingly, however, the impact of deep learning on the various stages of iris
    recognition pipeline is uneven. One of the primary goals of this survey paper
    is to assess where deep learning helped in achieving highly performance and more
    secure systems, and which procedures did not benefit from more complex modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of the paper is structured as follows. SectionÂ [2](#S2 "2\. Deep
    Learning-Based Iris Segmentation â€£ Deep Learning for Iris Recognition: A Survey")
    and Â [3](#S3 "3\. Deep Learning-Based Iris Recognition â€£ Deep Learning for Iris
    Recognition: A Survey") review the application of deep learning in two main stages
    of the recognition pipeline: segmentation and recognition (encoding and comparison).
    SectionÂ [4](#S4 "4\. Deep Learning-Based Iris Presentation Attack Detection â€£
    Deep Learning for Iris Recognition: A Survey") and Â [5](#S5 "5\. Deep Learning-Based
    Forensic Iris Recognition â€£ Deep Learning for Iris Recognition: A Survey") analyze
    the state of the art of deep learning-based approaches in two applications: Presentation
    Attack Detection (PAD) and Forensic. SectionÂ [6](#S6 "6\. Human-Machine Pairing
    to Improve Deep Learning-Based Iris Recognition â€£ Deep Learning for Iris Recognition:
    A Survey") investigates how human and machine can pair to improve deep learning
    based iris recognition. SectionÂ [7](#S7 "7\. Recognition in Less Controlled Environments:
    Iris/Periocular Analysis â€£ Deep Learning for Iris Recognition: A Survey") focuses
    on approaches in less controlled environments of iris and periocular analysis.
    SectionÂ [8](#S8 "8\. Open-Source Deep Learning-Based Iris Recognition Tools â€£
    Deep Learning for Iris Recognition: A Survey") reviews public resources and tools
    available in the deep learning based iris recognition domain. SectionÂ [9](#S9
    "9\. Emerging Research Directions â€£ Deep Learning for Iris Recognition: A Survey")
    focuses on the future of deep learning for iris recognition with discussion on
    emerging research directions in different aspects of iris analysis. The paper
    in concluded in SectionÂ [10](#S10 "10\. Conclusions â€£ Deep Learning for Iris Recognition:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Deep Learning-Based Iris Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The segmentation of the iris is seen as an extremely challenging problem. As
    illustrated in Fig.Â [1](#S2.F1 "Figure 1 â€£ 2\. Deep Learning-Based Iris Segmentation
    â€£ Deep Learning for Iris Recognition: A Survey"), segmenting the iris involves
    essentially three tasks: detect and parameterize the inner (pupillary) and outer
    (scleric) biological boundaries of the iris and also to locally discriminate between
    the noise-free/noisy regions inside the iris ring, which should be subsequently
    used in the feature encoding and matching processes.'
  prefs: []
  type: TYPE_NORMAL
- en: This problem has motivated numerous research works for decades. From the pioneering
    integro-differential operatorÂ (Daugman, [1993](#bib.bib35)) up to subsequent handcrafted
    techniques based in active contours and neural networks (e.g.,Â (He etÂ al., [2009](#bib.bib69)),Â (ProenÃ§a,
    [2010](#bib.bib134)),Â (Shah and Ross, [2009](#bib.bib148)) andÂ (Vatsa etÂ al.,
    [2008](#bib.bib176))) a long road has been traveled in this problem. Regardless
    an obvious evolution in the effectiveness of such techniques, they all face particular
    difficulties in case of heavily degraded data. Images are frequently motion-blurred,
    poor focused, partially occluded and off-angle. Additionally, in case of visible
    light data, severe reflections from the environments surrounding the subjects
    are visible, and even augment the difficulties of the segmentation task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, as in many other computer vision tasks, DL-based frameworks have
    been advocated as providing consistent advances over the state-of-the-art for
    the iris segmentation problem, with numerous models being proposed. A cohesive
    perspective of the most relevant recent DL-based methods is given in TableÂ [1](#S2.T1
    "Table 1 â€£ 2\. Deep Learning-Based Iris Segmentation â€£ Deep Learning for Iris
    Recognition: A Survey"), with the techniques appearing in chronographic (and then
    alphabetical) order. The type of data each model aims to handle is given in the
    â€*Data*â€ column, along with the datasets where the corresponding experiments were
    carried out and a summary of the main characteristics of each proposal (â€*Features*â€
    column). Here, considering that models were empirically validated in completely
    heterogeneous ways and using very different metrics, we decided not to include
    the summary performance of each model/solution.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg  class="ltx_picture ltx_centering" height="168.43" overflow="visible"
    version="1.1" width="442.05"><g transform="translate(0,168.43) matrix(1 0 0 -1
    0 0) translate(128.51,0) translate(0,79.29)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -69 -31.5)" fill="#000000" stroke="#000000"><foreignobject
    width="138" height="63" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/f29d6e98b2a84ce57df5e582022676cd.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -74.04 -41.79)" fill="#000000" stroke="#000000"><foreignobject width="148.09"
    height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Scleric Boundary
    Parameterization</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 86.16
    -42.49)" fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><foreignobject width="4.84"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -100.16 -56.51)" fill="#000000" stroke="#000000"><foreignobject
    width="122.39" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Noise-free
    Texture Detection</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 33.01
    -56.27)" fill="#000000" stroke="#000000" color="#000000"><foreignobject width="4.84"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -99.58 46.79)" fill="#000000" stroke="#000000"><foreignobject
    width="159.79" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Pupillary
    Boundary Parameterization</foreignobject></g><g transform="matrix(1.0 0.0 0.0
    1.0 72.38 46.09)" fill="#000000" stroke="#000000" color="#000000"><foreignobject
    width="4.84" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 157.54 -29.5)" fill="#000000" stroke="#000000"><foreignobject
    width="118" height="59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/effc3c2666ee5f9be482169720f82465.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 27.84 69.38)" fill="#000000" stroke="#000000"><foreignobject width="181.69"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Iris Segmentation
    Main Tasks</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 161.66 -42.73)"
    fill="#000000" stroke="#000000"><foreignobject width="110.55" height="6.73" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Dimensionless Noise-Free</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 184.81 -53.55)" fill="#000000" stroke="#000000"><foreignobject
    width="63.71" height="8.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Representation</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.72 12.63)" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 94.04 12.63)" fill="#000000" stroke="#000000" color="#000000"><foreignobject
    width="4.84" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 102.53 13.33)" fill="#000000" stroke="#000000"
    color="#000000"><foreignobject width="7.53" height="6.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 122.22 13.33)" fill="#000000" stroke="#000000" color="#000000"><foreignobject
    width="7.53" height="6.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 133.41 12.63)" fill="#000000" stroke="#000000"
    color="#000000"><foreignobject width="4.84" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1. Three main tasks typically associated to *iris segmentation*: 1)
    parameterization of the pupillary (inner) boundary; 2) parameterization of the
    scleric (outer) boundary; and 3) discrimination between the unoccluded (noise-free)
    and occluded (noisy) regions inside the iris ring. Such pieces of information
    are further used to obtain dimensionless polar representations of the iris texture,
    where feature extraction methods typically operate.'
  prefs: []
  type: TYPE_NORMAL
- en: Schlett *et* al.Â (Schlett etÂ al., [2018](#bib.bib145)) provided a multi-spectral
    analysis to improve iris segmentation accuracy in visible wavelengths by preprocessing
    data before the actual segmentation phase, extracting multiple spectral components
    in form of RGB color channels. Even though this approach does propose a DL-based
    framework, the different versions of the input could be easily used to feed DL-based
    models, and augment the robustness to non-ideal data. Chen *et* al.Â (Chen etÂ al.,
    [2019a](#bib.bib23)) used CNNs that include dense blocks, referred to as a dense-fully
    convolutional network (DFCN), where the encoder part consists of dense blocks,
    and the decoder counterpart obtains the segmentation masks via transpose convolutions.
    Hofbauer *et* al.Â (Hofbauer etÂ al., [2019](#bib.bib73)) parameterize the iris
    boundaries based on segmentation maps yielding from a CNN, using a a cascaded
    architecture with four RefineNet units, each directly connecting to one Residual
    net. Huynh *et* al.Â (Huynh etÂ al., [2019](#bib.bib77)) discriminate between three
    distinct eye regions with a DL model, and removes incorrect areas with heuristic
    filters. The proposed architecture is based on the encoder-decoder model, with
    depth-wise convolutions used to reduce the computational cost. Roughly at the
    same time, Li *et* al.Â (Li etÂ al., [2021](#bib.bib95)) described the *Interleaved
    Residual U-Net* model for semantic segmentation and iris mask synthesis. In this
    work, unsupervised techniques (K-means clustering) were used to create intermediary
    pictorial representations of the ocular region, from where saliency points deemed
    to belong to the iris boundaries were found. Kerrigan *et* al.Â (Kerrigan etÂ al.,
    [2019](#bib.bib86)) assessed the performance of four different convolutional architectures
    designed for semantic segmentation. Two of these models were based in dilated
    convolutions, as proposed by Yu and KoltunÂ (Y. and K., [2016](#bib.bib189)). Wu
    and ZhaoÂ (Wu and Zhao, [2019](#bib.bib187)) described the Dense U-Net model, that
    combines dense layers to the U-Net network. The idea is to take advantage of the
    reduced set of parameters of the dense U-Net, while keeping the semantic segmentation
    capabilities of U-Net. The proposed model integrates dense connectivity into U-Net
    contraction and expansion paths. Compared with traditional CNNs, this model is
    claimed to reduce learning redundancy and enhance information flow, while keeping
    controlled the number of parameters of the model. Wei *et* al.Â (Zhang etÂ al.,
    [2019](#bib.bib206)) suggested to perform *dilated convolutions*, which is claimed
    to obtain more consistent global features. In this setting, convolutional kernels
    are not continuous, with zero-values being artificially inserted between each
    non-zero position, increasing the receptive field without augmenting the number
    of parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1. Cohesive comparison of the most relevant DL-based iris segmentation
    methods (NIR: *near-infrared*; VW: *visible wavelength*). Methods are listed in
    chronological (and then alphabetical) order.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Data | Datasets | Features |'
  prefs: []
  type: TYPE_TB
- en: '| NIR | VW |'
  prefs: []
  type: TYPE_TB
- en: '| Schlett *et* al.Â (Schlett etÂ al., [2018](#bib.bib145)) | 2018 | âœ— | âœ“ | MobBIO
    | Preprocessing (combines different possibilities of the input RGB channels) |'
  prefs: []
  type: TYPE_TB
- en: '| Trokielewicz and CzajkaÂ (Trokielewicz and Czajka, [2018](#bib.bib167)) |
    2018 | âœ“ | âœ“ | Warsaw-Post-Mortem v1.0 | Fine-tuned CNN (SegNet) |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et* al.Â (Chen etÂ al., [2019a](#bib.bib23)) | 2019 | âœ“ | âœ“ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | Dense CNN |'
  prefs: []
  type: TYPE_TB
- en: '| Hofbauer *et* al.Â (Hofbauer etÂ al., [2019](#bib.bib73)) | 2019 | âœ“ | âœ— |
    IITD, CASIA-Irisv4-Interval, ND-Iris-0405 | Cascaded architecture of four RefineNet,
    each connecting to one Residual net |'
  prefs: []
  type: TYPE_TB
- en: '| Huynh *et* al.Â (Huynh etÂ al., [2019](#bib.bib77)) | 2019 | âœ“ | âœ— | OpenEDS
    | MobileNetV2 + heuristic filtering postproc. |'
  prefs: []
  type: TYPE_TB
- en: '| Li *et* al.Â (Anisetti etÂ al., [2019](#bib.bib8)) | 2019 | âœ“ | âœ— | CASIA-Iris-Thousand
    | Faster-R-CNN (ROI detection) |'
  prefs: []
  type: TYPE_TB
- en: '| Kerrigan *et* al.Â (Kerrigan etÂ al., [2019](#bib.bib86)) | 2019 | âœ“ | âœ“ |
    CASIA-Irisv4-Interval, BioSec, ND-Iris-0405, UBIRIS.v2, Warsaw-Post-Mortem v2.0,
    ND-TWINS-2009-2010 | Resent + Segnet (with dilated convolutions) |'
  prefs: []
  type: TYPE_TB
- en: '| Wu and ZhaoÂ (Wu and Zhao, [2019](#bib.bib187)) | 2019 | âœ“ | âœ“ | CASIA-Irisv4-Interval,
    UBIRIS.v2 | Dense-U-Net (dense layers + U-Net) |'
  prefs: []
  type: TYPE_TB
- en: '| Wei *et* al.Â (Zhang etÂ al., [2019](#bib.bib206)) | 2019 | âœ“ | âœ“ | CASIA-Iris4-Interval,
    ND-IRIS-0405, UBIRIS.v2 | U-Net with dilated convolutions |'
  prefs: []
  type: TYPE_TB
- en: '| Fang and CzajkaÂ (Fang and Czajka, [2020](#bib.bib51)) | 2020 | âœ“ | âœ“ | ND-Iris-0405,
    CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v1.0 & v2.0 | Fine-tuned CC-NetÂ (Mishra
    etÂ al., [2019](#bib.bib107)) |'
  prefs: []
  type: TYPE_TB
- en: '| Ganeva and MyasnikovÂ (Ganeeva and Myasnikov, [2020](#bib.bib56)) | 2020 |
    âœ“ | âœ— | MMU | U-Net, LinkNet, and FC-DenseNet (performance comparison) |'
  prefs: []
  type: TYPE_TB
- en: '| Jalilian *et* al.Â (Jalilian etÂ al., [2020](#bib.bib80)) | 2020 | âœ“ | âœ— |  |
    RefineNet + morphological postprocessing |'
  prefs: []
  type: TYPE_TB
- en: '| Sardar *et* al.Â (Sardar etÂ al., [2020](#bib.bib143)) | 2020 | âœ“ | âœ“ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | Squeeze-Expand module + active learning (interactive segmentation)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Trokielewicz *et* al.Â (Trokielewicz etÂ al., [2020](#bib.bib173)) | 2020 |
    âœ“ | âœ“ | ND-Iris-0405, CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v1.0 & v2.0
    | Fined-tuned SegNetÂ (Badrinarayanan etÂ al., [2017](#bib.bib10)) |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et alÂ (Wang etÂ al., [2020b](#bib.bib179)) | 2020 | âœ“ | âœ“ | CASIA-Iris-M1-S1/S2/S3,
    MICHE-I | Hourglass network |'
  prefs: []
  type: TYPE_TB
- en: '| Wang *et* al.Â (Wang etÂ al., [2020a](#bib.bib177)) | 2020 | âœ“ | âœ“ | CASIA-v4-Distance,
    UBIRIS.v2, MICHE-I | U-Net + multi-task attention net + postproc. (probabilistic
    masks priors & thresholding) |'
  prefs: []
  type: TYPE_TB
- en: '| Li *et* al.Â (Li etÂ al., [2021](#bib.bib95)) | 2021 | âœ“ | âœ— | CASIA-Iris-Thousand
    | IRU-Net network |'
  prefs: []
  type: TYPE_TB
- en: '| Wang *et* al.Â (Wang etÂ al., [2021](#bib.bib185)) | 2021 | âœ— | âœ“ | Online
    Video Streams and Internet Videos | U-Net and Squeezenet to iris segmentation
    and detect eye closure |'
  prefs: []
  type: TYPE_TB
- en: '| Kuehlkamp etÂ al.Â (Kuehlkamp etÂ al., [2022](#bib.bib92)) | 2022 | âœ“ | âœ“ |
    ND-Iris-0405, CASIA, BATH, BioSec, UBIRIS, Warsaw-Post-Mortem v2.0 | Fined-tuning
    of Mask-RCNN architecture |'
  prefs: []
  type: TYPE_TB
- en: More recently, Ganeva and MyasnikovÂ (Ganeeva and Myasnikov, [2020](#bib.bib56))
    compared the effectiveness of three convolutional neural network architectures
    (U-Net, LinkNet, and FC- DenseNet), determining the optimal parameterization for
    each one. Jalilian *et* al.Â (Jalilian etÂ al., [2020](#bib.bib80)) introduced a
    scheme to compensate for texture deformations caused by the off-angle distortions,
    re-projecting the off-angle images back to frontal view. The used architecture
    is a variant of RefineNetÂ (Lin etÂ al., [2016](#bib.bib97)), which provides high-resolution
    prediction, while preserving the boundary information (required for parameterization
    purposes).
  prefs: []
  type: TYPE_NORMAL
- en: The idea of interactive learning for iris segmentation was suggested by Sardar
    *et* al.Â (Sardar etÂ al., [2020](#bib.bib143)), describing an interactive variant
    of U-Net that includes Squeeze Expand modules. Trokielewicz *et* al.Â (Trokielewicz
    etÂ al., [2020](#bib.bib173)) used DL-based iris segmentation models to extract
    highly irregular iris texture areas in post-mortem iris images. They used a pre-trained
    SegNet model, fine-tuned with a database of cadaver iris images. Wang *et* al.Â (Wang
    etÂ al., [2020b](#bib.bib179)) (further extended inÂ (Wang etÂ al., [2019b](#bib.bib180)))
    described a lightweight deep convolutional neural network specifically designed
    for iris segmentation of degraded images acquired by handheld devices. The proposed
    approach jointly obtains the segmentation mask and parameterized pupillary/limbic
    boundaries of the iris.
  prefs: []
  type: TYPE_NORMAL
- en: Observing that edge-based information is extremely sensitive to be obtained
    in degraded data, Li *et* al.Â (Anisetti etÂ al., [2019](#bib.bib8)) presented an
    hybrid method that combines edge-based information to deep learning frameworks.
    A compacted Faster R-CNN-like architecture was used to roughly detect the eye
    and define the initial region of interest, from where the pupil is further located
    using a Gaussian mixture model. Wang *et* al.Â (Wang etÂ al., [2021](#bib.bib185))
    trained a deep convolutional neural network(DCNN) that automatically extracts
    the iris and pupil pixels of each eye from input images. This work combines the
    power of U-Net and SqueezeNet to obtain a compact CNN suitable for real time mobile
    applications. Finally, Wang *et* al.Â (Wang etÂ al., [2020a](#bib.bib177)) parameterize
    both the iris mask and the inner/outer iris boundaries jointly, by actively modeling
    such information into a unified multi-task network.
  prefs: []
  type: TYPE_NORMAL
- en: A final word is given to *segmentation-less* techniques. Assuming that the accurate
    segmentation of the iris boundaries is one of the hardest phases of the whole
    recognition chain and the main source for recognition errors, some recent works
    have been proposing to perform biometrics recognition in non-segmented or roughly
    segmented dataÂ (ProenÃ§a and Neves, [2017](#bib.bib133))(ProenÃ§a and Neves, [2019](#bib.bib136)).
    Here, the idea is to use the remarkable discriminating power of DL-frameworks
    to perceive the agreeing patterns between pairs of images, even on such *segmentation-less*
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Deep Learning-Based Iris Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. Deep Learning Models as a Feature Extractor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <svg  class="ltx_picture ltx_centering" height="158.59" overflow="visible"
    version="1.1" width="442.05"><g transform="translate(0,158.59) matrix(1 0 0 -1
    0 0) translate(128.51,0) translate(0,79.29)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.16 55.6)" fill="#000000" stroke="#000000"><foreignobject
    width="203.56" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DL-based
    Feature Representation</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0
    -78.84 -34.5)" fill="#000000" stroke="#000000"><foreignobject width="138" height="69"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer to caption](img/6fe2772c84903e1f3319d6c908feb205.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -64.72 -42.73)" fill="#000000" stroke="#000000"><foreignobject
    width="110.55" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Dimensionless
    Noise-Free</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 -41.56 -53.55)"
    fill="#000000" stroke="#000000"><foreignobject width="63.71" height="8.5" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Representation</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 245.39 -48.58)" fill="#000000" stroke="#000000"><foreignobject width="48.59"
    height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Feature Set</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 258.8 -22.21)" fill="#000000" stroke="#000000"><foreignobject
    width="17.84" height="44.42" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math
     class="ltx_Math"
    alttext="\left.\begin{bmatrix}f_{1}\\
  prefs: []
  type: TYPE_NORMAL
- en: f_{2}\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: f_{t}\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{bmatrix}\right." display="inline"><semantics ><mrow
     ><mo
     >[</mo><mtable
    rowspacing="0pt" 
    ><mtr
     ><mtd
     ><msub
     ><mi
    mathsize="90%" 
    >f</mi><mn
    mathsize="90%" 
    >1</mn></msub></mtd></mtr><mtr
     ><mtd
     ><msub
     ><mi
    mathsize="90%" 
    >f</mi><mn
    mathsize="90%" 
    >2</mn></msub></mtd></mtr><mtr
     ><mtd
     ><mi
    mathsize="90%" mathvariant="normal" 
    >â‹®</mi></mtd></mtr><mtr
     ><mtd
     ><msub
     ><mi
    mathsize="90%" 
    >f</mi><mi
    mathsize="90%" 
    >t</mi></msub></mtd></mtr></mtable><mo
     >]</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply
     ><csymbol
    cd="latexml" 
    >matrix</csymbol><matrix
     ><matrixrow
     ><apply
    
    ><csymbol
    cd="ambiguous" 
    >subscript</csymbol><ci
    
    >ğ‘“</ci><cn
    type="integer" 
    >1</cn></apply></matrixrow><matrixrow
     ><apply
    
    ><csymbol
    cd="ambiguous" 
    >subscript</csymbol><ci
    
    >ğ‘“</ci><cn
    type="integer" 
    >2</cn></apply></matrixrow><matrixrow
     ><ci
    
    >â‹®</ci></matrixrow><matrixrow
     ><apply
    
    ><csymbol
    cd="ambiguous" 
    >subscript</csymbol><ci
    
    >ğ‘“</ci><ci
    
    >ğ‘¡</ci></apply></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >\left.\begin{bmatrix}f_{1}\\
    f_{2}\\ \vdots\\ f_{t}\\ \end{bmatrix}\right.</annotation></semantics></math></foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2. The main task of DL-based iris feature extraction: given a dimensionless
    representation of the iris data, obtain its compact and representative representation
    - the feature set - that is further used in the classification phase.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in Fig.Â [2](#S3.F2 "Figure 2 â€£ 3.1\. Deep Learning Models as
    a Feature Extractor â€£ 3\. Deep Learning-Based Iris Recognition â€£ Deep Learning
    for Iris Recognition: A Survey"), the idea here is to analyze a dimensionless
    representation of the iris data and produce a feature vector that lies in a hyperspace
    (embedding) where recognition is carried out.'
  prefs: []
  type: TYPE_NORMAL
- en: In this context, Boyd *et* el.Â (Boyd etÂ al., [2019](#bib.bib16)) explored five
    different sets of weights for the popular ResNet50 architecture to test if iris-specific
    feature extractors perform better than models trained for general tasks. Minaee
    *et* al.Â (Minaee etÂ al., [2016](#bib.bib106)) studied the application of deep
    features extracted from VGG-Net for iris recognition, having authors observed
    that the resulting features can be well transferred to biometric recognition.
    Luo *et* al.Â (Luo etÂ al., [2021](#bib.bib103)) described a DL model with spatial
    attention and channel attention mechanisms, that are directly inserted into the
    feature extraction module. Also, a co-attention mechanism adaptively fuses features
    to obtain representative iris-periocular features. Hafner *et* al.Â (Hafner etÂ al.,
    [2021](#bib.bib66)) adapted the classical Daugmanâ€™s pipeline, using convolutional
    neural networks to function as feature extractors. The DenseNet-201 architecture
    outperformed its competitors achieving state-of- the-art results both in the open
    and close world settings. Menotti *et* al.Â (Menotti etÂ al., [2015](#bib.bib105))
    assessed how DL-based feature representations can be used in spoofing detection,
    observing that spoofing detection systems based on CNNs can be robust to attacks
    already known and adapted, with little effort, to image-based attacks that are
    yet to come.
  prefs: []
  type: TYPE_NORMAL
- en: Yang *et* al.Â (Yang etÂ al., [2021](#bib.bib197)) generated multi-level spatially
    corresponding feature representations by an encoder-decoder structure. Also, a
    spatial attention feature fusion module was used to ensemble the resulting features
    more effectively. Chen *et* al.Â (Chen etÂ al., [2020](#bib.bib24)) addressed the
    large-scale recognition problem and described an optimized center loss function
    (tight center) to attenuate the insufficient discriminating power of the cross-entropy
    function. Nguyen *et* al.Â (Nguyen etÂ al., [2017b](#bib.bib113)) explored the performance
    of state-of-the-art pre-trained CNNs on iris recognition, concluding that off-the-shelf
    CNN generic features are also extremely good at representing iris images, effectively
    extracting discriminative visual features and achieving promising results. Zhao
    *et* al.Â (Zhao etÂ al., [2019](#bib.bib208)) proposed a method based on the capsule
    network architecture, where a modified routing algorithm based on the dynamic
    routing between two capsule layers was described, with three pre-trained models
    (VGG16, InceptionV3, and ResNet50) extracting the primary iris features. Next,
    a convolution capsule replaces the full connection capsule to reduce the number
    of parameters. Wang and KumarÂ (Wang and Kumar, [2019](#bib.bib181)) introduced
    the concept of *residual feature* for iris recognition. They described a residual
    network learning procedure with offline triplets selection and dilated convolutional
    kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other works have addressed the extraction of appropriate feature representations
    in multi-biometrics settings: Damer *et* al.Â (Damer etÂ al., [2019](#bib.bib33))
    propose to jointly extract multi-biometric representations within a single DNN.
    Unlike previous solutions that create independent representations from each biometric
    modality, they create these representations from multi-modality (face and iris),
    multi-instance (iris left and right), and multi- presentation (two face samples),
    which can be seen as a fusion at the data level policy. Finally, concerned about
    the difficulty of performing reliable recognition in hand-held devices, Odinokikh
    *et* al.Â (Odinokikh etÂ al., [2019](#bib.bib122)) combined the advantages of handcrafted
    feature extractors and advanced deep learning techniques. The model utilizes shallow
    and deep feature representations in combination with characteristics describing
    the environment, to reduce the intra-subject variations expected in this kind
    of environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Deep Learning-based Iris Matching Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The existing matching strategies can be categorized into three categories:
    (1) using conventional classifiers, such as SVM, RF, and Sparse Representation;
    (2) softmax-based losses; and (3) pairwise-based losses. A cohesive perspective
    of the most relevant recent DL-based methods is given in Table 2, with the techniques
    appearing in chronographic (and then alphabetical) order'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Conventional classifiers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Various researchers have been using deep learning networks designed and pre-trained
    on the ImageNet dataset to extract iris feature representations, followed by a
    conventional classifier such as SVM, RF, Sparse Representation, etc. (Nguyen etÂ al.,
    [2017b](#bib.bib113); Boyd etÂ al., [2019](#bib.bib16); Boyd etÂ al., [2020b](#bib.bib19)).
    The key benefit of these approaches is the simplicity of â€œplug and playâ€, where
    proven and pre-trained deep learning networks inherited from large-scale computer
    vision challenges are widely available and ready to be used (Nguyen etÂ al., [2017b](#bib.bib113)).
    Another benefit is that there is no need for large scale iris image datasets to
    train these networks because they have already been trained on such large-scale
    datasets as ImageNet. Considering these networks usually contain hundreds of layers
    and millions of parameters, and require millions of images to train, using pre-trained
    networks is extremely beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Iris Classification Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Iris classification networks couple deep learning architectures with a family
    of softmax-based losses to classify an iris image into a list of known identities.
    Coupling a softmax loss with a backbone network enables training the backbone
    network in an end-to-end manner via popular optimization strategies such as back-propagation
    and steepest gradient decent. Compared to the conventional classifier approaches,
    the DL-based backbones in this category are learnable directly from the iris data,
    allowing them to better represent the iris. The key benefit is that it is similar
    to a generic image classification task, hence all designs and algorithms in the
    generic image classification task can be trivially applied with the iris image
    data. Typical examples of these iris classification networks are (Gangwar and
    Joshi, [2016](#bib.bib57); Boyd etÂ al., [2019](#bib.bib16)). However, these softmax-based
    networks require the iris in the test image be known in the identity classes in
    the training set, which means the networks must be re-trained whenever a new class
    (*i.e.* a new identity) is added. Gangwar *et al.* proposed two backbone networks
    (*i.e.* DeepIrisNet-A and DeepIrisNet-B) followed by a softmax loss for the iris
    recognition task (Gangwar and Joshi, [2016](#bib.bib57)). Later, they proposed
    another backbone network, but still followed by a softmax loss to classify one
    normalized iris image into a pre-defined list of identity (Gangwar etÂ al., [2019](#bib.bib58)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Backbone Network Architectures: A wide range of backbone network architectures
    have been borrowed from generic image classification for the iris recognition
    task due to their similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AlexNet: AlexNet is the most primitive and been shown as least accurate for
    iris recognition compared to others (Boyd etÂ al., [2020a](#bib.bib17); Nguyen
    etÂ al., [2017b](#bib.bib113)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VGG: Boyd *et al.* (Boyd etÂ al., [2020a](#bib.bib17)), Nguyen *et al.* (Nguyen
    etÂ al., [2017b](#bib.bib113)) and Minaee *et al.* (Minaee etÂ al., [2016](#bib.bib106))
    all experimented VGG16 .'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ResNet: ResNet with its variants are the most popular backbone network architecture.
    Nguyen *et al.* experimented ResNet152 (Nguyen etÂ al., [2017b](#bib.bib113)).
    Boyd *et al.* experimented three variants ResNet18, ResNet50 and ResNet152 in
    their post-mortem iris classification task (Boyd etÂ al., [2020a](#bib.bib17)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inception: Zhao *et al.* employed capsule network based on the InceptionV3
    architecture (Zhao etÂ al., [2019](#bib.bib208)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EfficientNet: Hsiao *et al.* (Hsiao and Fan, [2021](#bib.bib75)) employed EfficientNet
    to extract iris features.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2.3\. Iris Similarity Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Iris similarity networks couple deep learning architectures with a family of
    pairwise-based losses to learn a metric representing how similar or dissimilar
    two iris images are without knowing their identities. The pairwise loss aims to
    pull images of the same iris closer and push images of different irises away in
    the similarity distance space. Different to the iris classification networks which
    only operate in an identification mode on a pre-defined identity list, iris similarity
    networks operate across both verification and identification modes with an open
    set of identities (Zhao and Kumar, [2017b](#bib.bib210)). Typical examples of
    these iris similarity networks are (Liu etÂ al., [2016b](#bib.bib98); Zhao and
    Kumar, [2017b](#bib.bib210); Wang and Kumar, [2019](#bib.bib181); Nguyen etÂ al.,
    [2020](#bib.bib114); Jalilian etÂ al., [2022](#bib.bib81)). There are three key
    benefits of these networks: (i) verification and identification: iris similarity
    networks operate across both verification and identification modes; (ii) open
    set of identities: iris similarity networks operate on an open set of identities;
    and (iii) explicit reflection: iris similarity networks directly and explicitly
    reflect what we want to achieve, *i.e.,* small distances between irises of the
    same subject and larger distances between irises of different subjects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pairwise loss: Nianfeng *et al.* (Liu etÂ al., [2016b](#bib.bib98)) proposed
    a pairwise network, which accepts two input images and directly outputs a similarity
    score. They designed a pairwise layer which accepts two input images and encodes
    their features via a backbone network. The backbone network is trained iteratively
    to minimize the dissimilarity distance between genuine pairs (pairs of the same
    identity) and maximize the dissimilarity distance between impostor pairs (pairs
    of the different identities).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Triplet loss: Since the pairwise network is trained with separate genuine and
    impostor pairs, it may not converge well, which has been proven in the face recognition
    (Schroff etÂ al., [2015](#bib.bib146)). Rather than using one pair of two images
    to update the training as in the pairwise loss for each training iteration, the
    triplet loss employs a triplet of three images: an anchor image, a positive image
    with the same identity and a negative image with a different identity (Schroff
    etÂ al., [2015](#bib.bib146)). The backbone network is trained to simultaneously
    minimize the similarity distance between the positive and the anchor images and
    maximize the distance between the negative and the anchor images. Tailored for
    iris images, Zhao *et al.* (Zhao and Kumar, [2017b](#bib.bib210); Wang and Kumar,
    [2019](#bib.bib181); Zhao and Kumar, [2019](#bib.bib212)) proposed Extended Triplet
    Loss (EPL) to incorporate a bit-shifting operation to deal with rotation in the
    normalized iris images. Nguyen *et al.* also employed the ETL for their iris recognition
    network (Nguyen etÂ al., [2020](#bib.bib114), [2022](#bib.bib116)). Kuehlkamp *et
    al.* (Kuehlkamp etÂ al., [2022](#bib.bib92)) proposed to improve the generic triplet
    loss function for iris recognition by forcing the distance to be positive (through
    the use of a sigmoid output layer), and adding a logarithmic penalty to the error.
    This modification allows the network to learn even when the difference between
    samples is negative and converge faster. Yan *et al.* (Yan etÂ al., [2021](#bib.bib196))
    extended the generic triplet loss to batch triplet loss, in which the triplet
    loss is calculated over a batch of $S$ subjects and $K$ images for each subject.
    Performing batch triplet loss is usually expected to have smooth loss function.
    Yang *et al.* (Yang etÂ al., [2021](#bib.bib197)) improved triplet selection method
    for training by Batch Hard (Yuan etÂ al., [2020](#bib.bib198)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Backbone Network Architectures: Different to the classification iris networks,
    similarity iris networks are usually designed with their own network architectures
    and are usually much â€œshallowerâ€ than the classification counterparts.'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FCN: All similarity iris networks employ Fully Convolutional Networks (FCNs)
    instead of CNNs. Compared to CNNs, FCNs (Long etÂ al., [2015](#bib.bib101)) do
    not have fully connected layer, allowing the output map to preserve the original
    spatial information. This is important to iris recognition since the output map
    can preserve spatial correspondence with the original input image (Zhao and Kumar,
    [2017b](#bib.bib210); Nguyen etÂ al., [2020](#bib.bib114)), thus enabling pixel-to-pixel
    matching. Zhao *et al.* (Zhao and Kumar, [2017b](#bib.bib210)) proposed a FCN
    architecture with 3 convolutional layers, followed by activation and pooling layers.
    Outputs of convolutional layers are up-sampled to the original input image size.
    The up-samples features are stacked and convolved by another convolutional layer
    to generate a 2-dimension features with the same size as the input image. Later,
    they extended the backbone network with dilated convolutions (Wang and Kumar,
    [2019](#bib.bib181)). Yan *et al.* (Yan etÂ al., [2021](#bib.bib196)) employed
    a ResNet architecture and fine-tuned it with the triplet loss. Kuehlkamp *et al.*
    only used a part of the ResNet architecture.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NAS: Nguyen *et al.* (Nguyen etÂ al., [2020](#bib.bib114)) proposed to learn
    the network architecture directly from data rather than hand-designing it or using
    generic-image-classification architectures. They proposed a differential Neural
    Architecture Search (NAS) approach that models the architecture design process
    as a bi-level constrained optimization approach. This approach is not only able
    to search for the optimal network which achieves the best possible performance,
    but it can also impose constraints on resources such as model size or number of
    computational operations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complex-valued: Observing that there is an intrinsic difference between the
    iris texture and generic object-based images where the iris texture is stochastic
    without consistent shapes, edges, or semantic structure, Nguyen *et al.* (Nguyen
    etÂ al., [2022](#bib.bib116)) argued the network architecture has to be better
    tailored to incorporate domain-specific knowledge in order to reach the full potential
    in the iris recognition setting. Another observation that they made is a majority
    of well-known handcrafted features such as IrisCode (Daugman, [2007](#bib.bib36))
    transformed iris texture image into a complex-valued representation first, then
    further encoded the complex-valued representation to get a final representation.
    They proposed to use fully complex-valued networks rather than popular real-valued
    networks. Complex-valued backbone networks better retain the phase, are more invariant
    to multi-scale, multi-resolution and multi-orientation, have solid correspondence
    with the classic Gabor wavelets (Tygert etÂ al., [2016](#bib.bib174)), hence are
    much better suited to iris recognition than their real-valued counterparts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 2. Cohesive comparison of the most relevant DL-based iris recognition
    methods (NIR: *near-infrared*; VW: *visible wavelength*). Methods are listed in
    chronological (and then alphabetical) order.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | Year | Data | Datasets | Features |'
  prefs: []
  type: TYPE_TB
- en: '| NIR | VW |'
  prefs: []
  type: TYPE_TB
- en: '| Conventional classifiers | MenottiÂ etat. (Menotti etÂ al., [2015](#bib.bib105))
    | 2015 | âœ“ | âœ“ | Biosec,Â LivDet-2013-Warsaw,Â MobBIOfake | Shallow CNNs + SVM forÂ Spoofing
    Detection |'
  prefs: []
  type: TYPE_TB
- en: '| MinaeeÂ etÂ al.Â (Minaee etÂ al., [2016](#bib.bib106)) | 2016 | âœ“ | âœ— | CASIA-Iris-Thousand,
    IITD | VGG + SVM |'
  prefs: []
  type: TYPE_TB
- en: '| Nguyen et al.(Nguyen etÂ al., [2017b](#bib.bib113)) | 2017 | âœ“ | âœ— | ND-CrossSensor-2013,
    CASIA-Iris-Thousand | AlexNet, VGG, Google Inception, ResNet, DenseNet + SVM |'
  prefs: []
  type: TYPE_TB
- en: '| Boyd et al.(Boyd etÂ al., [2019](#bib.bib16)) | 2019 | âœ“ | âœ“ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | ResNet50 + SVM |'
  prefs: []
  type: TYPE_TB
- en: '| Boyd et al.(Boyd etÂ al., [2020b](#bib.bib19)) | 2020 | âœ“ | âœ“ | DCMEO1,Â Warsaw
    | AlexNet, ResNet, VGG, DenseNet + Cosine, Euclidean, MSE |'
  prefs: []
  type: TYPE_TB
- en: '| Hafner et al.(Hafner etÂ al., [2021](#bib.bib66)) | 2021 | âœ“ | âœ— | CASIA-Iris-Thousand
    | ResNet101 +Â DenseNet-201 +Â Cosine Similarity |'
  prefs: []
  type: TYPE_TB
- en: '| Classification Networks | GangwarÂ et al.(Gangwar and Joshi, [2016](#bib.bib57))
    | 2016 | âœ“ | âœ— | ND-IRIS-0405, ND-CrossSensor-2013 | DeepIrisNet |'
  prefs: []
  type: TYPE_TB
- en: '| Gangwar et al.(Gangwar etÂ al., [2019](#bib.bib58)) | 2019 | âœ“ | âœ“ | ND-IRIS-0405,
    UBIRIS.v2, MICHE-I, CASIA-Irisv4-Interval | DeepIrisNetV2 |'
  prefs: []
  type: TYPE_TB
- en: '| Odinokikh et al.(Odinokikh etÂ al., [2019](#bib.bib122)) | 2019 | âœ“ | âœ— |
    CASIA-Iris-M1-S2, CASIA-Iris-M1-S3, Iris-Mobile | Feature Fusion + Softmax |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al.(Zhao etÂ al., [2019](#bib.bib208)) | 2019 | âœ“ | âœ— | JluIrisV3.1,
    JluIrisV4, CASIA-Irisv4-Lamp | Capsule network + Softmax |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. (Chen etÂ al., [2020](#bib.bib24)) | 2020 | âœ“ | âœ— | ND-IRIS-0405,
    CASIA-Iris-Thousand, IITD cross sensor | T-Center loss |'
  prefs: []
  type: TYPE_TB
- en: '| Luo et al.(Luo etÂ al., [2021](#bib.bib103)) | 2021 | âœ“ | âœ— | ND-IRIS-0405,Â CASIA-Iris-Thousand
    | Attention + Softmax Loss + Center Loss |'
  prefs: []
  type: TYPE_TB
- en: '| Similarity Networks | Nianfeng et al.(Liu etÂ al., [2016b](#bib.bib98)) |
    2016 | âœ“ | âœ— | Q-FIRE, CASIA-Cross-Sensor | DeepIris |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al.(Zhao and Kumar, [2017b](#bib.bib210)) | 2017 | âœ“ | âœ“ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | UniNet (FeatNet+MaskNet) +Â Extended Triplet Loss |'
  prefs: []
  type: TYPE_TB
- en: '| Damer et al.(Damer etÂ al., [2019](#bib.bib33)) | 2019 | âœ“ | âœ— | Biosecure,
    CASIA-Iris-Thousand/Lamp/Interval | Inception + Triplet Loss |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al.(Wang and Kumar, [2019](#bib.bib181)) | 2019 | âœ“ | âœ“ | CASIA-Irisv4-Interval,
    IITD, UBIRIS.v2 | FeatNet + Dilated Convolution +Â Extended Triplet Loss |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al.(Zhao and Kumar, [2019](#bib.bib212)) | 2019 | âœ“ | âœ— | ND-Iris-0405,
    Casia-Irisv4-Distance, IITD | FeatNet + Mask RCNN +Â Extended Triplet Loss |'
  prefs: []
  type: TYPE_TB
- en: '| NguyenÂ et al.(Nguyen etÂ al., [2020](#bib.bib114)) | 2020 | âœ“ | âœ“ | CASIA-v4-Distance,
    UBIRIS.v2, ND-CrossSensor-2013 | Constrained Design Backbone +Â Extended Triplet
    Loss |'
  prefs: []
  type: TYPE_TB
- en: '| Yan et al. (Yan etÂ al., [2021](#bib.bib196)) | 2021 | âœ“ | âœ— | CASIA-Iris-Thousand
    | Spatial Feature Reconstruction + Triplet Loss |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al.(Yang etÂ al., [2021](#bib.bib197)) | 2021 | âœ“ | âœ— | CASIA-Irisv4-Thousand,
    CASIA-Irisv4-Distance, IITD | Dual Spatial Attention Network +Â Batch Hard |'
  prefs: []
  type: TYPE_TB
- en: '| NguyenÂ et al.(Nguyen etÂ al., [2022](#bib.bib116)) | 2022 | âœ“ | âœ“ | ND-CrossSensor-2013,
    CASIA-Iris-Thousand, UBIRIS.v2 | Complex-valued Backbone +Â Extended Triplet Loss
    |'
  prefs: []
  type: TYPE_TB
- en: '| Kuehlkamp et al. (Kuehlkamp etÂ al., [2022](#bib.bib92)) | 2022 | âœ“ | âœ“ |
    DCMEO1, DCMEO2, Warsaw-Post-Mortem v2.0 | ResNet + Triplet Loss |'
  prefs: []
  type: TYPE_TB
- en: 3.3\. End-to-end Joint Iris Segmentation+Recognition Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Almost all existing approaches perform segmentation and normalization to transform
    an input image to a normalized rectangular 2D representation before recognition
    as this simplifies the representation learning. As segmentation and recognition
    may require a separate network themselves, this would cause redundancy in both
    computation and training, further slowing down an DL-based iris recognition approach.
    Several researchers have looked at approaches to perform end-to-end networks.
    One category is to perform segmentation-less recognition. Another category is
    to jointly learn segmentation and recognition using an unified network via multi-task
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Segmentation-less: These approaches feed the cropped iris images directly into
    a deep learning network to extract features. For example, Kuehlkamp *et al.* (Kuehlkamp
    etÂ al., [2022](#bib.bib92)) used Mask R-CNN for semantic segmentation and fed
    the cropped iris region directly into a ResNet50 to extract features. Similarly,
    Chen *et al.* (Chen etÂ al., [2019b](#bib.bib25)) also fed the cropped iris images
    directly into a DenseNet. Rather than feeding the cropped iris images directly,
    Proenca *et al.* transformed the cropped region (which is detected by SSD) into
    a polar representation first, then fed the polar representation into the VGG19
    for extracting features (ProenÃ§a and Neves, [2019](#bib.bib136)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-task: Segmentation and recognition can be jointly learned with one unified
    network. This paves a way for multi-task learning. However, segmentation and recognition
    may require different number of layers, hence research is required to perform
    using different intermediate layers for each task. To the best knowledge, there
    does not exist any approach to explore this direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Deep Learning-Based Iris Presentation Attack Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In parallel to the popularity of biometrics, the security of these systems against
    attacks has become of paramount importance. The most common attack is a Presentation
    Attack (PA), which refers to presenting a fake sample to the sensor. The goal
    can be either to impersonate somebody else identity (also known as Impostor Attack
    Presentation), or to conceal the own identity (also known as Concealer Attack
    Presentation). Via impostor attacks, a person could also enroll fraudulently,
    allowing a continuous manipulation of the system. The previous acronyms and terms
    in italics correspond to the vocabulary recommended in the series of ISO/IEC 30107
    standards of the ISO/IEC Subcommittee 37 (SC37) on Biometrics (technology â€” Biometric
    presentation attackÂ detection â€” Part 1:Â Framework, [2016](#bib.bib164)), which
    we will follow in the rest of this section. Presentation Attack Instruments (PAI)
    used to carry out impostor attacks are typically generated from bona fide images
    of an iris from an individual who has legitimate access to the system. The iris
    is printed on a piece of paper (printout attack) or displayed on a screen (replay
    attack) and then presented to the sensor. The iris of deceased individuals can
    also be used as PAI, since the texture remains intact for some hours (Trokielewicz
    etÂ al., [2018](#bib.bib170)). Theoretically, it would be possible to print a genuine
    iris texture into a contact lens as well, although this has not been successfully
    demonstrated yet (Boyd etÂ al., [2020a](#bib.bib17)). Concealer attacks, on the
    other hand, are commonly done via textured contact lenses that obscure or alter
    properties of the eye (such as color) to prevent the system from identifying the
    user. Synthetic iris images (Yadav etÂ al., [2019a](#bib.bib192)) not belonging
    to any specific identity could be used for similar purposes. Concealers can also
    present their legitimate iris, but in a way not expected by the system, e.g. closing
    eyelids as much as possible, looking to the sides (off-axis gaze), rotating the
    head, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two challenges of PAs is that they happen outside the physical limits of the
    system, and they do not require specific knowledge of its inner workings, or any
    technical knowledge at all. Thus, if no properly tackled, they can derail public
    perception of even the most reliable biometric modality. It is even more critical
    if authentication is done without any supervision. Presentation Attack Detection
    (PAD) methods to counteract such attacks can be done (Galbally and Gomez-Barrero,
    [2016](#bib.bib55)): $i$) at the hardware (or sensor) level, using additional
    illuminators or sensors that detect intrinsic properties of a living eye or responses
    to external stimuli (like pupil contraction or reflection), or $ii$) at the software
    level, using only the footprint of the PA (if any) left in the same images captured
    with the standard sensor that will be employed for authentication. Software-based
    techniques are in principle less expensive and intrusive, since they do not demand
    extra hardware, and they will be the focus of this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two comprehensive surveys on PAD are (Czajka and Bowyer, [2018](#bib.bib31))
    (2018) and (Boyd etÂ al., [2020a](#bib.bib17)) (2020). While DL techniques were
    residual in the 2018 survey, they rose in popularity thereafter. We build this
    section upon the latest survey and summarize the most important developments in
    DL-PAD since it was published (TableÂ [3](#S4.T3 "Table 3 â€£ 4.5\. Open Research
    Questions in Iris PAD â€£ 4\. Deep Learning-Based Iris Presentation Attack Detection
    â€£ Deep Learning for Iris Recognition: A Survey")). A descriptive summary of the
    datasets employed is given later in SectionÂ [8](#S8 "8\. Open-Source Deep Learning-Based
    Iris Recognition Tools â€£ Deep Learning for Iris Recognition: A Survey"). The aim
    of PAD is to classify an image either as a bona fide or an attack presentation,
    so it is usually modeled as a two-class classification task. Typical strategies
    mimic the trend of the previous section when applying DL to iris recognition:
    either a CNN backbone is used to extract features that will feed a conventional
    classifier, or the network is trained end-to-end to do the classification itself.
    Some hybrid methods also combine traditional hand-crafted with deep-learned features.
    In the same manner, the network may be initialized e.g. on the ImageNet dataset
    to take advantage of such large generic corpus, since available iris PAD data
    is more scarce. Another strategy also employed widely in the PAD literature is
    to use adversarial networks, where a GAN (Goodfellow etÂ al., [2014](#bib.bib61))
    is trained to generate synthetic iris images that the discriminator must use to
    detect attack samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. CNNs for Feature Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since each layer of a CNN represents a different level of abstraction, Fang
    et al. (Fang etÂ al., [2020a](#bib.bib45)) fused the features from the last four
    convolutional layers of two models (VGG16, MobileNetv3-small). The features are
    projected to a lower dimensional space by PCA and either concatenated for classification
    with SVM (feature fusion) or the classification scores of each level combined
    (score fusion). Using two databases of printouts and textured contact lenses,
    the method showed superiority over the use of the different layers individually,
    or the feature vector from the next-to-last layer of the networks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. End-to-end Classification Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Arora and Bathia (Arora and Bhatia, [2020](#bib.bib9)) trained a CNN with 10
    convolutional layers to detect contact lenses and printouts. Rather than using
    the entire image, the network is trained on patches from all parts of the iris
    image. The system showed superior performance compared to state-of-the-art methods
    which at that time, according to the paper, were mostly based on hand-crafted
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on embedded low-power devices, Peng et al. (Peng etÂ al., [2020](#bib.bib127))
    adopted a Lite Anti-attack Iris Location Network (LAILNet) based on three dense
    blocks featuring depthwise separable convolutions to reduce the number of parameters.
    The algorithm demonstrated very good performance on three databases with printouts,
    synthetic irises, contact lenses and artificial plastic eyes.
  prefs: []
  type: TYPE_NORMAL
- en: Also focusing on mobiles, Fang et al. (Fang etÂ al., [2021b](#bib.bib46); Fang
    etÂ al., [2020b](#bib.bib49)) used MobileNetv3-small. The contribution lies in
    the division of the normalized iris image into overlapped micro-stripes which
    are fed individually, and a decision reached by majority voting. The claimed advantages
    are that the classifier is forced to focus on the iris/sclera boundaries (given
    by their exact micro-stripes), the input dimensionality is lower and the amount
    of samples is higher (reducing overfitting), and the impact of imprecise segmentation
    is alleviated. Using three databases with contact lenses and printouts, the paper
    featured an extensive experimentation with cross-database, cross-sensor, and cross-attack
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: Sharma and Ross (Sharma and Ross, [2020](#bib.bib151)) proposed D-NetPAD, based
    on DenseNet121, chosen due to benefits such as maximum flow of information given
    by dense connections to all subsequent layers, or fewer parameters compared to
    counterparts like ResNet or VGG. The PAI included printouts, artificial eye, cosmetic
    contacts, kindle replay, and transparent dome on print, with experiments substantiating
    the effectiveness of the method on cross-PAI, cross-sensor and cross-database
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Chen and Ross (Chen and Ross, [2021](#bib.bib20)) proposed an explainable attention-guided
    detector (AG-PAD). To do so, the feature maps of a DenseNet121 were fed into two
    modules that independently capture inter-channel and inter-spatial feature dependencies.
    The outputs were then fused via element-wise sum to capture complementary attention
    features from both channel and spatial dimensions. With three datasets containing
    colored contact lenses, artificial eyes (Van Dyke/Doll fake eyes), printouts,
    and textured contact lenses, the attention modules are shown to improve accuracy
    over the baseline network. Using heatmap visualization, it is also shown that
    the attention modules force the network to attend to the annular iris textural
    region which, intuitively, plays a vital role for PAD.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial attention was also explored by Fang et al. (Fang etÂ al., [2021c](#bib.bib47)).
    To find local regions that contribute the most to make accurate decisions and
    capture pixel/patch-level cues, they proposed an attention-based pixel-wise binary
    supervision (A-PBS) method. To capture different levels of abstraction, they perform
    multi-scale fusion by adding spatial attention modules to feature maps from three
    levels of a DenseNet backbone. Using six datasets with textured lenses and printouts,
    they outperformed previous state-of-the-art including scenarios with unknown attacks,
    sensors, and databases.
  prefs: []
  type: TYPE_NORMAL
- en: Given the difficulty of collecting iris PAD data, most databases contain, at
    most, a few hundred subjects. To address this, Fang et al. (Fang etÂ al., [2021d](#bib.bib48))
    studied data augmentation techniques that modify position, scale or illumination.
    Using three architectures (ResNet50, VGG16, MobileNetv3-small) and three databases
    with printouts and textured contact lenses, they found that data augmentation
    improves PAD performance significantly, but each technique has a positive role
    on a particular dataset or CNN. They also explored the selection of augmentation
    techniques, finding, again, no consensus regarding the best combination, which
    was attributed to differences in capture environment, subject population, scale
    of the different datasets or imbalance between bona fide and attack samples.
  prefs: []
  type: TYPE_NORMAL
- en: Gupta et al. (Gupta etÂ al., [2021](#bib.bib64)) proposed MVANet, with 5 convolutional
    layers and 3 branches of fully connected layers. They addressed the challenge
    of unseen databases, sensors, and imaging environment on textured contact lenses
    detection. The size of each layer of MVANet is different, thus capturing different
    features. They used three databases, each one captured in different settings (indoor/outdoor,
    different times of the day, varying weather, fixed/mobile sensors, etc.), with
    MVANET trained in one database at a time and tested on the other two. As baseline,
    they fine-tuned three popular CNNs (VGG16, ResNet18, DenseNet) initialized on
    ImageNet. The proposed network is shown to perform consistently better and more
    uniformly on the test databases than the baseline approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Sharma and Ross (Sharma and Ross, [2021](#bib.bib152)) studied the viability
    of Optical Coherence Tomography (OCT). OCT provides a cross-sectional view of
    the eye, whereas traditional NIR or VW imaging provides 2D textural data. The
    PAIs considered are artificial eyes (Van Dyke eyes) and cosmetic lenses, evaluated
    on three different CNNs (VGG19, ResNet50, DenseNet121). By both intra- (known
    PAs) and cross-attack (unknown PAs) scenarios, OCT is determined as a viable solution,
    although hardware cost is still a limiting factor. Indeed, OCT outperforms NIR
    and VW in the intra-attack scenario, while NIR generalizes better to unseen PAs.
    Cosmetic lenses also appear to be more difficult to detect than artificial eyes
    with any modality. Via heatmaps, it is seen as well that the fixation regions
    are different for each imaging modality and for each PAI, which could be a source
    of complementarity.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. (Zhang etÂ al., [2021](#bib.bib200)) proposed a Weighted Region
    Network (WRN) to detect cosmetic lenses that includes a local attention Weight
    Network (for evaluating the discriminating information of different regions) and
    a global classification Region Network (for characterizing global features). Such
    strategy considers both the entire image and the attention effect by assigning
    different weights to regions. The mentioned networks are applied to a VGG16 backbone.
    The reported results showed improved performance compared to the state-of-the-art
    over three different databases.
  prefs: []
  type: TYPE_NORMAL
- en: The works by Agarwal et al. (Agarwal etÂ al., [2022b](#bib.bib3), [a](#bib.bib2))
    evaluated the detection of contact lenses. In (Agarwal etÂ al., [2022b](#bib.bib3)),
    they trained a siamese CNN of 5 convolutional layers on two different inputs (the
    original image and its CLAHE version), which are then combined by weighted score
    fusion of the softmax layer. Adding a processed version of the raw image attempts
    to enhance the feature extraction capabilities of the CNN. A similar strategy
    is followed in (Agarwal etÂ al., [2022a](#bib.bib2)), but here they used a siamese
    contraction-expansion CNN, and the processed image is a edge-enhanced image obtained
    via Histogram of Oriented Gradients (HOG). Another difference was the use of feature-level
    fusion of the next-to-last CNN feature vectors, testing different strategies (vector
    addition, multiplication, concatenation and distance). The papers employed several
    databases, with an extensive protocol including unseen subjects, environments
    (indoor vs outdoor) and databases (sensors) that showcases the strength of the
    solutions against cross-domain changes. The methods also showed superiority against
    popular CNN models (VGG16, ResNet18, DenseNet) and the popular LBP and HOG hand-crafted
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Gautam et al. (Gautam etÂ al., [2022](#bib.bib60)) proposed a Deep Supervised
    Class Encoding (DSCE) approach consisting of an Autoencoder that exploits class
    information, and minimizes simultaneously the reconstruction and classification
    errors during training. Three datasets were used, containing textured lenses,
    printouts and synthetic images, showing superiority over a variety of hand-crafted
    and deep-learned features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tapia et al.(Tapia etÂ al., [2022](#bib.bib163)) used a two-stages serial architecture
    based on a modified MobiletNetv2\. A first network was trained to only distinguish
    two classes (bona fide vs attack). If it votes bona fide, the image is sent to
    a second network trained to classify it among three or four classes (bona fide
    or a different type of PAI: contact lenses, printout, or cadaver). Four databases
    were combined to obtain a super-set with the different PAIs, and class-weights
    were also incorporated into the loss to compensate imbalance. The paper applied
    contrast enhancement (CLAHE), and an aggressive data augmentation (rotation, blurring,
    contrast change, Gaussian noise, edge enhancement, image region dropout, etc.).
    They tested two image sizes, 224$\times$224 and 448$\times$448, observing that
    the extra detail of a higher resolution image results in more effective features.
    The paper also carried out leave-one-out PAI tests for open-set evaluation, showing
    robustness in detecting unknown attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Hybrid Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Choudhary et al. (Choudhary etÂ al., [2022b](#bib.bib28), [a](#bib.bib27)) applied
    a Friedman test-based selection method to identify the best features of a set
    of hand-crafted and deep-learned ones. Each feature method feeds a SVM classifier,
    and the scores of the individual SVMs are fused via weighted sum. A preliminary
    version of (Choudhary etÂ al., [2022a](#bib.bib27)) without feature selection appeared
    in (Choudhary etÂ al., [2021](#bib.bib26)). The databases of (Choudhary etÂ al.,
    [2022b](#bib.bib28)) include a medley of different PA (printouts, synthetic irises,
    artificial eyeballs, etc.), although the feature selection and classification
    methods are trained and evaluated separately on each database. The authors observed
    a saturation after a certain number of features are combined, and a superiority
    of the score-level fusion over other methods such as majority voting, feature-level
    fusion, and rank-level fusion. The work (Choudhary etÂ al., [2022a](#bib.bib27)),
    on the other hand, concentrated on textured contact lenses attack, with an extensive
    set of evaluations including single sensor, cross-sensor and combined sensor experiments.
    Apart from the generic live vs attack scenario, it also reports binary and ternary
    classification across the different types of real (normal iris, soft lens) and
    fake (textured) classes. Naturally, the cross-sensor error is larger compared
    to single-sensor, and the combined sensor error is also observed to be slightly
    larger. The latter is attributed to the larger intraclass variation created when
    images from different sensors are combined. In any case, an improvement of performance
    over previous works with the three datasets employed is observed after the proposed
    feature selection and score-level fusion method.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Adversarial Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative methods have been used by some approaches, either to use the trained
    discriminator for iris PAD, or to generate synthetic samples and augment under-represented
    classes. In this direction, Yadav and Ross (Yadav and Ross, [2021](#bib.bib194))
    proposed CIT-GAN (Cyclic Image Translation Generative Adversarial Network) for
    multi-domain style transfer to generate synthetic samples of several PAIs (cosmetic
    contact lenses, printed eyes, artificial eyes and kindle-display attack). To do
    so, image translation is driven by a Styling Network that learns style characteristics
    of each given domain. It also employs a Convolutional Autoencoder in the generator
    for image-to-image style translation, which takes a domain label as input along
    with an image. This is different than previous works of the same authors (Yadav
    etÂ al., [2020](#bib.bib193), [2019a](#bib.bib192)) which employed the traditional
    generator/discriminator approach driven by a noise vector. Different PAD methods
    using hand-crafted (BSIF, DESIST) and deep features (VGG16, D-NetPAD, AlexNet)
    were evaluated, demonstrating that they can be improved by adding synthetically
    generated data. The quality of synthetic images is also superior to a competing
    generative method (Star-GAN v2), measured via FID score distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Open Research Questions in Iris PAD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the open research issues is to design robust iris PAD methods with cross-sensor
    and cross-database capabilities, so they generalize to unseen imaging conditions.
    Attackers are constantly developing new attack methodologies to circumvent PAD
    systems, so an even more important issue is unseen PAIs (i.e. cross-PAI capabilities)
    (Sharma and Selwal, [2021](#bib.bib150)). Great results have been achieved on
    detecting known attack types (known as closed-set recognition), although cross-database
    evaluation (training in one database an testing in others) still appears as a
    difficult challenge due to changes in sensors, acquisition environments, or subjects.
    Moreover, generalizing to attacks that are unknown at the time of training (open-set
    recognition) is even a greater challenge for state-of-the-art methods (Fang etÂ al.,
    [2021b](#bib.bib46)). Part of the problem lies into the limited size of existing
    databases, which is an issue for data-hungry DL approaches. Some solutions, as
    studied by some of the methods above, are data augmentation by geometric or illumination
    modifications (Fang etÂ al., [2021d](#bib.bib48)), or creating additional synthetic
    data via generative methods (Yadav and Ross, [2021](#bib.bib194)). Human-aided
    DL training is another promising avenue. Indeed, humans and machines cooperating
    in vision tasks is not new, and this strategy is finding its way into DL as well
    (Boyd etÂ al., [2021](#bib.bib18), [2022](#bib.bib15)). For example, Boyd et al.
    (Boyd etÂ al., [2022](#bib.bib15)) analyzed the utility of human judgement about
    salient regions of images to improve generalization of DL models. Asked about
    regions that humans deem important for their decision about an image, the work
    proposed to transform the training data to incorporate such opinions, demonstrating
    an improvement in accuracy and generalization in leave-one-attack-type-out scenarios.
    In a similar work, Boyd et al. (Boyd etÂ al., [2021](#bib.bib18)) incorporated
    annotated saliency maps into the loss function to penalize large differences with
    human judgement.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, concerns have emerged about the observed bias of DL methods that leads
    to discriminatory performance differences based on the userÂ´s demographics, with
    face biometrics being the most talked-about and many companies and authorities
    banning its use (Jain etÂ al., [2021](#bib.bib79)). Obviously, this issue appears
    in iris PAD as well, as addressed by Fang et al. (Fang etÂ al., [2021e](#bib.bib50)).
    Using three baselines based on hand-crafted and DL approaches and a database of
    contact lenses, the authors showed a significant difference in the performance
    between male and female samples. In dealing with this phenomenon, examination
    of biases towards eye color or race are another directions worthwhile to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Some elements considered as PAIs in this section, such as cosmetic lenses, may
    be worn normally by users without the purpose of fooling the biometric system,
    as it is the case of facial retouching via make-up, digital beautification or
    augmented reality (Hedman etÂ al., [2021](#bib.bib70)). This poses the question
    of whether it is possible to use such images for authentication, while diminishing
    the effect in the recognition performance. Suggested alternatives have been to
    detect and match portions of live iris tissue still visible (Parzianello and Czajka,
    [2022](#bib.bib126)) or incorporate ocular information of the surrounding area
    (Alonso-Fernandez and Bigun, [2016](#bib.bib5)). Unfortunately, in iris biometrics,
    recognition with textured contact lenses remains a hard problem to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another under-researched task is iris PAD in the visible spectrum. The majority
    of studies and datasets (SectionÂ [8](#S8 "8\. Open-Source Deep Learning-Based
    Iris Recognition Tools â€£ Deep Learning for Iris Recognition: A Survey")) employ
    near-infrared illumination and specific iris close-up sensors. However, in some
    environments such as mobile or distant capture, such sensing is not guaranteed
    (Nigam etÂ al., [2015](#bib.bib120)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3. Cohesive comparison of the most relevant DL-based iris Presentation
    Attack Detection methods after the surveys (Czajka and Bowyer, [2018](#bib.bib31);
    Boyd etÂ al., [2020a](#bib.bib17)) (NIR: *near-infrared*; VW: *visible wavelength*).
    Methods are listed in chronological (and then alphabetical) order.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Method | Year | Data | Datasets | Features |'
  prefs: []
  type: TYPE_TB
- en: '| NIR | VW |'
  prefs: []
  type: TYPE_TB
- en: '| Feature Extraction | Fang et al. (Fang etÂ al., [2020a](#bib.bib45)) | 2020
    | âœ“ | âœ— | LivDet-2017 (IIITD-WVU, ND- CLD) | VGG16, MobileNetv3-small (multi-layer
    features) + PCA + SVM |'
  prefs: []
  type: TYPE_TB
- en: '| End-to-end Training | Arora and Bathia (Arora and Bhatia, [2020](#bib.bib9))
    | 2020 | âœ“ | âœ— | LivDet-2017 (IIITD-WVU) | CNN with patch input |'
  prefs: []
  type: TYPE_TB
- en: '|  | Peng et al. (Peng etÂ al., [2020](#bib.bib127)) | 2020 | âœ“ | âœ— | IPITRT,
    CASIA-Iris-v4, CASIA-Iris-Fake | LAILNet lightweight CNN |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sharma and Ross (Sharma and Ross, [2020](#bib.bib151)) | 2020 | âœ“ | âœ—
    | Proprietary, LivDet-2017 (IIITD-WVU, ND-CLD, Warsaw, Clarkson) | DenseNet121
    pre-trained on ImageNet |'
  prefs: []
  type: TYPE_TB
- en: '|  | Chen and Ross (Chen and Ross, [2021](#bib.bib20)) | 2021 | âœ“ | âœ— | JHU-APL,
    LivDet-2017 (Warsaw, ND-CLD) | DenseNet121 pre-trained on ImageNet + AG-PAD channel
    and spatial attention |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fang et al. (Fang etÂ al., [2021b](#bib.bib46)) | 2021 | âœ“ | âœ— | LivDet-2017
    (IIITD-WVU, ND-CLD), ND-CLD-15, | MobileNetv3-small with micro-stripes |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fang et al. (Fang etÂ al., [2021c](#bib.bib47)) | 2021 | âœ“ | âœ— | LivDet-2017
    (IIITD-WVU, ND-CLD, Clarkson), ND-CLD-13, ND-CLD-15, IIITD-CLI | DenseNet + A-PBS
    spatial attention |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fang et al. (Fang etÂ al., [2021d](#bib.bib48)) | 2021 | âœ“ | âœ— | LivDet-2017
    (IIITD-WVU, ND-CLD, Clarkson) | ResNet50, VGG16, MobileNetv3-small |'
  prefs: []
  type: TYPE_TB
- en: '|  | Gupta et al. (Gupta etÂ al., [2021](#bib.bib64)) | 2021 | âœ“ | âœ— | MUIPA,
    UnMIPA, IIITD-CLI | CNN with multi-branch classification |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sharma and Ross (Sharma and Ross, [2021](#bib.bib152)) | 2021 | âœ“ | âœ“
    | OCT, NIR and VW images | VGG19, ResNet50, DenseNet121 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zhang et al. (Zhang etÂ al., [2021](#bib.bib200)) | 2021 | âœ“ | âœ— | ND-CLD-13,
    CASIA-Iris-Fake, IF-VE | VGG16 + WRN local attention and global classification
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Agarwal et al. (Agarwal etÂ al., [2022a](#bib.bib2)) | 2022 | âœ“ | âœ— | MUIPA,
    UnMIPA, IIITD-CLI, LivDet-2017 (IIITD-WVU), ND-PSID | Siamese contraction-expansion
    CNN, feature fusion |'
  prefs: []
  type: TYPE_TB
- en: '|  | Agarwal et al. (Agarwal etÂ al., [2022b](#bib.bib3)) | 2022 | âœ“ | âœ— | MUIPA,
    UnMIPA, IIITD-CLI, LivDet-2017 (IIITD-WVU), ND-PSID, NDIris3D | Siamese CNN, score
    fusion |'
  prefs: []
  type: TYPE_TB
- en: '|  | Gautam et al. (Gautam etÂ al., [2022](#bib.bib60)) | 2022 | âœ“ | âœ— | SYN,
    IIITD-CLI, IIITD-IS | Autoencoder with reconstruction and classification loss
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Tapia et al. (Tapia etÂ al., [2022](#bib.bib163)) | 2022 | âœ“ | âœ“ | LivDet-2020,
    Iris-CL1, Warsaw-Post-Mortem v3.0 | MobileNetv2, data augmentation, class-weights
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid Methods | Choudhary et al. (Choudhary etÂ al., [2022b](#bib.bib28))
    | 2022 | âœ“ | âœ— | IIITD-CLI, ND-CLD-13, CASIA, LivDet-2017 (IIITD-WVU, ND-CLD,
    Clarkson) | MBISF (domain-specific filters), SIFT, Haralick, DenseNet, VGG8 +
    SVM classification |'
  prefs: []
  type: TYPE_TB
- en: '|  | Choudhary et al. (Choudhary etÂ al., [2022a](#bib.bib27)) | 2022 | âœ“ |
    âœ— | IIITD-CLI, ND-CLD-13, LivDet-2017 (Clarkson) | MBSIF (generic filters), MBSIF
    (domain-specific filters), SIFT, LBPV, DAISY, DenseNet121 + SVM classification
    |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Networks | Yadav and Ross (Yadav and Ross, [2021](#bib.bib194))
    | 2021 | âœ“ | âœ— | Casia-Iris-Fake, Berc-iris-fake, ND-CLD-15, LivDet-2017, MSU-IrisPA-01
    | BSIF, DESIST, VGG16, D-NetPAD, AlexNet |'
  prefs: []
  type: TYPE_TB
- en: 5\. Deep Learning-Based Forensic Iris Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Iris recognition has become the next biometric mode (in addition to face, fingerprints
    and palmprints) considered for large-scale forensic applications (FBI Criminal
    Justice Information Services (CJIS) Division, [2021](#bib.bib53)), and coincides
    in time with discoveries made in recent years about possibility to employ iris
    in recognition of deceased subjects. This includes both matching of iris patterns
    acquired a few hours after death with those with longer PMIs (Post-Mortem Intervals),
    ranging from days (Sauerwein etÂ al., [2017](#bib.bib144); Bolme etÂ al., [2016](#bib.bib13);
    Trokielewicz etÂ al., [2016b](#bib.bib169), [a](#bib.bib168)) to several weeks
    after demise (Trokielewicz etÂ al., [2019](#bib.bib171); Boyd etÂ al., [2020b](#bib.bib19)),
    as well as matching patterns acquired before death with those collected post-mortem
    (Sansola, [2015](#bib.bib142)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ded5af25ce7ae3cdd407bcb301e247d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8c6e2401cb6d939a61d6bde783a8dcb0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/51dd785471e9aa2755919ddfc026bde1.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3. Post-mortem iris recognition and visualization: (a) a good-quality
    post-mortem iris image; (b) top to bottom: deep learning-based detection of iris
    annulus, specular highlights and decomposition-induced wrinkles; (c) segmentation
    results presented to a human examiner along with an overlaid heatmap visualizing
    regions judged as salient by the matching algorithm. Source: (Kuehlkamp etÂ al.,
    [2022](#bib.bib92))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to decomposition changes to the eye tissues, post-mortem iris images differ
    significantly from live iris images and rarely meet ISO/IEC 29794-6 quality requirements,
    as shown in Fig. [3](#S5.F3 "Figure 3 â€£ 5\. Deep Learning-Based Forensic Iris
    Recognition â€£ Deep Learning for Iris Recognition: A Survey")(a). The challenges
    are related to appropriate detection of places when cornea dries and generates
    irregular and large specular highlights, as well regions where iris muscle furrows
    show up when the eyeball dehydrates. This is where DL-based methods may win over
    hand crafted approaches, as the latter usually make strong assumptions about anatomy
    of the iris appearance, not possible to be predicted for eyes undergoing random
    decomposition processes. Trokielewicz etÂ al.Â proposed the first known to us iris
    recognition method designed specifically to cadaver irises (Trokielewicz etÂ al.,
    [2020](#bib.bib172), [2020](#bib.bib173)). It incorporates SegNet-based segmenter
    and Siamese networks-based feature extractor, both trained in a domain-specific
    way solely on post-mortem iris samples. An interesting element of this approach
    is that segmetation incorporates two models: one trained with â€œfineâ€ ground truth
    masks, marking all details associated with eye decomposition, and â€œcoarseâ€ model,
    aiming at detecting iris annulus and eyelids, as in classical iris recognition
    approaches. This allowed to apply a standard â€œrubber sheetâ€ iris images normalization
    based on â€œcoarseâ€ masks, and at the same time exclude decomposition-driven artifacts
    from encoding, marked by the â€œfineâ€ mask. Kuehlkamp etÂ al.Â (Kuehlkamp etÂ al.,
    [2022](#bib.bib92)) in addition to detecting post-mortem deformations, as shown
    in Fig. [3](#S5.F3 "Figure 3 â€£ 5\. Deep Learning-Based Forensic Iris Recognition
    â€£ Deep Learning for Iris Recognition: A Survey")(c), they also proposed a human-interpretable
    visualization of a classification process. The visualization is based on Class
    Activation Mapping mechanism (Zhou etÂ al., [2016](#bib.bib213)) and highlights
    salient features used by the classifier in its judgment. This novelty in iris
    recognition algorithms may help human examiners to locate iris regions that should
    be carefully inspected, or to verify the algorithmâ€™s decision.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Human-Machine Pairing to Improve Deep Learning-Based Iris Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Iris recognition is usually associated with automatic, solely machine-based
    and rapid biometric means. It has been changing in the recent decade due to constantly
    increasing ubiquitousness of iris recognition, especially owing to large governmental
    applications such as (Unique Identification Authority of India, [2021](#bib.bib175))
    or FBIâ€™s Next Generation Identification System (NGI) gradually replacing the Integrated
    Automated Fingerprint Identification System (IAFIS) (FBI Criminal Justice Information
    Services (CJIS) Division, [2021](#bib.bib53)). This combined with unique identification
    power of iris whetted the appetite to apply this technique to identification problems
    normally reserved for fingerprints and face: forensics, lost subjects search or
    post-mortem identification. To have the legal power, however, the judgment about
    samples originating or not from the same eye conclusion must be confirmed by a
    trained human expert. And here is the place where DL-based iris image processing
    may play a useful role.'
  prefs: []
  type: TYPE_NORMAL
- en: Trokielewicz *et al.* compared iris images in post-mortem iris recognition between
    humans and machines. They investigated which iris image regions humans and machines
    mainly attend to compare a pair of images. The machine-based attention maps are
    generated by Grad-CAM to highlight the regions that contribute the most to the
    deep learning modelâ€™s prediction. The human-based attention maps are learned by
    tracking the gaze as the human is looking around the screen that display iris
    image pairs and recording the regions where the human spend most time on. Interestingly
    while humans and machines tend to focus on a limited number of iris areas, however,
    the region, appearance and density of these areas between humans and machines
    are different. As salient regions proposed by the deep learning model and identified
    from human eye gaze do not overlap in general, the computer-added visual cues
    may potentially constitute a valuable addition to the forensic examinerâ€™s expertise,
    as it can highlight important discriminatory regions that the human expert might
    miss in their proceedings. This human-machine pairing is important as human subjects
    can provide an incorrect decision even despite spending quite sometime observing
    many iris regions (NIST, [2021](#bib.bib121)). In addition, there has been a body
    of research showing that humans and machines do not perform similarly well under
    different conditions (Stark etÂ al., [2010](#bib.bib155); Chen etÂ al., [2016](#bib.bib21);
    Moreira etÂ al., [2019](#bib.bib109)). For example, Moreira *et al.* also showed
    that machines can outperform humans in healthy easy iris image pairs; however,
    humans outperform machines in disease-affected iris image pairs (Moreira etÂ al.,
    [2019](#bib.bib109)). Human-machine pairing will improve deep learning based iris
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: '7\. Recognition in Less Controlled Environments: Iris/Periocular Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rooted in the seminal work due to Park *et* al.Â (Park etÂ al., [2011](#bib.bib125)),
    efforts have been paid to the development of human recognition methods that -
    apart the iris - also consider information in the vicinity of the eye to infer
    the identity. This is a relatively recent topic, termed as *periocular recognition*.
    The rationale is that the periocular region represents a trade-off between the
    face and the iris. Periocular biometrics has been claimed to be particularly useful
    in environments that produce poor quality data (e.g., visual surveillance). Recently,
    as in the case of iris, several DL-based solutions have been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Hernandez-Diaz *et* al.Â (Hernandez-Diaz etÂ al., [2018](#bib.bib71)) tested the
    suitability of off-the-shelf CNN architectures to the periocular recognition task,
    observing that albeit such networks are optimized to classify generic objects,
    their features still can be effectively transferred to the periocular domain.
  prefs: []
  type: TYPE_NORMAL
- en: In the visual surveillance context, Kim *et* al.Â (Kim etÂ al., [2018](#bib.bib89))
    infer subjects identities based either in loose/tight regions-of-interest, depending
    of the perceived image quality. Hwang and LeeÂ (Hwang and Lee, [2020](#bib.bib78))prevents
    the loss of mid-level features and dynamically selects the most important features
    for classification. Luo *et* al.Â (Luo etÂ al., [2021](#bib.bib103)) used self-attention
    channel and spatial mechanisms into the feature encoding module of a CNN, in order
    to obtain the most discriminative features of the iris and periocular regions.
  prefs: []
  type: TYPE_NORMAL
- en: Jung *et* al.Â (Jung etÂ al., [2020](#bib.bib83))â€™s work is based in the concept
    of label smoothing regularization (LSR). Having as main goal to reduce the intra-class
    variability, they described a so-called Generalized LSR (GLSR) by learning a pre-task
    network prediction that is claimed to improve the permanence of the obtained periocular
    features. Having similar purposes, Zanlorensi *et* al.Â (Zanlorensi etÂ al., [2020](#bib.bib199))
    described a preprocessing step based in generative networks able to compensate
    for the typical data variations in visual surveillance environments. Nie *et*
    al.Â (Nie etÂ al., [2014](#bib.bib119)) applied convolutional restricted Boltzmann
    machines to the periocular recognition problem. Starting from a set of genuine
    pairs that are used as a constraint, a Mahalanobis distance-metric is learned.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining auxiliary (e.g., soft biometrics) has been seen as an interesting
    direction for compensating the lack of image quality. Zhao and KumarÂ (Zhao and
    Kumar, [2018](#bib.bib211)) incorporate an attention model into a DL-architecture
    to emphasize the most important regions in the periocular data. The same authorsÂ (Zhao
    and Kumar, [2017a](#bib.bib209)) described a semantics-assisted CNN framework
    to infer comprehensive periocular features. The whole model is composed of different
    networks, trained upon ID and semantic (e.g., gender, ethnicity) data, that are
    fused at the score and prediction levels. Similarly, Talreja *et* al.Â (Talreja
    etÂ al., [2022](#bib.bib159)) described a multi-branch CNN framework that predicts
    simultaneously soft biometrics and ID labels, which are finally fused into the
    final response.
  prefs: []
  type: TYPE_NORMAL
- en: With regard to cross-spectral settings, Hernandez-Diaz *et* al. (Hernandez-Diaz
    etÂ al., [2020](#bib.bib72)) used conditional GANs (CGANs) to convert periocular
    images between domains, that are further fed to intra-domain off-the-self frameworks.
    Sharma *et* el.Â (Sharma etÂ al., [2014](#bib.bib149)) described a shallow neural
    architecture where each model learns the data features in each spectrum. Then,
    at a subsequent phase, all models are jointly fine tuned, to learn the cross-spectral
    variability and correspondence features.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, several works have attempted to faithfully fuse the scores/responses
    from iris and periocular data. Wang and KumarÂ (Wang and Kumar, [2021](#bib.bib182))
    used periocular features to adaptively match iris data acquired in less constrained
    conditions. Their framework incorporates such discriminative information using
    a multilayer perceptron network. Zhang *et* al.Â (Zhang etÂ al., [2018](#bib.bib204))
    described a DL-model that exploits complementary information from the iris and
    the periocular regions, that applies *maxout* units to obtain compact representations
    for each modality and then fuses the discriminative features of the modalities
    through weighted concatenation. In an opposite direction, ProenÃ§a and NevesÂ (ProenÃ§a
    and Neves, [2018](#bib.bib135)) argued that the periocular recognition performance
    is optimized when the components inside the ocular globe (the iris and the sclera)
    are simply discarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4. Summary of datasets used in the DL-based iris segmentation and recognition
    methods of TablesÂ [1](#S2.T1 "Table 1 â€£ 2\. Deep Learning-Based Iris Segmentation
    â€£ Deep Learning for Iris Recognition: A Survey") and [2](#S3.T2 "Table 2 â€£ 3.2.3\.
    Iris Similarity Networks â€£ 3.2\. Deep Learning-based Iris Matching Strategies
    â€£ 3\. Deep Learning-Based Iris Recognition â€£ Deep Learning for Iris Recognition:
    A Survey") (NIR: *near-infrared*; VW: *visible wavelength*).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Data | Size | # IDs | # Samples | # Sessions | Features |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BATH (Monro etÂ al., [2007](#bib.bib108)) | NIR | 1280$\times$960 | 1600 |
    16000 | 1 | High quality images |'
  prefs: []
  type: TYPE_TB
- en: '| BioSec (Fierrez etÂ al., [2007](#bib.bib54)) | NIR | 640$\times$480 | 400
    | 3200 | 2 | Office environment |'
  prefs: []
  type: TYPE_TB
- en: '| Biosecure (Ortega etÂ al., [2010](#bib.bib124)) | NIR | 640$\times$480 | 1334
    | 2668 | 2 | Office environment |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Cross-Sensor (Xiao etÂ al., [2013](#bib.bib188)) | NIR | n/a | 700 |
    21000 | 1 | Multi-sensor, multi-distance (12-30cm, 3-5m) |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Iris-Distance (Dong etÂ al., [2009](#bib.bib41)) | NIR | 2352$\times$1728
    | 284 | 2567 | 1 | Distant acquisition |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Iris-Interval (Ma etÂ al., [2003](#bib.bib104)) | NIR | 320$\times$280
    | 395 | 2639 | 2 | High quality images |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Iris-Lamp (Wei etÂ al., [2007](#bib.bib186)) | NIR | 640$\times$480
    | 819 | 16212 | 1 | Non-linear deformation |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Iris-M1-S1 (Zhang etÂ al., [2015](#bib.bib205)) | NIR | 1920$\times$1080
    | 140 | 1400 | 1 | Mobile device |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Iris-M1-S2 (Zhang etÂ al., [2016b](#bib.bib203)) | NIR | 1968$\times$1024
    | 400 | 6000 | 1 | Mobile device, multi-distance (20,25,30cm) |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Iris-M1-S3 (Zhang etÂ al., [2018](#bib.bib204)) | NIR | 1920$\times$1920
    | 720 | 3600 | 1 | Mobile device |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Iris-Thousand (Zhang etÂ al., [2010](#bib.bib201)) | NIR | 640$\times$480
    | 2000 | 20000 | 1 | High quality images |'
  prefs: []
  type: TYPE_TB
- en: '| DCME01 (Boyd etÂ al., [2020b](#bib.bib19)) | NIR, VW | n/a | 254 | 621 | 1-9
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| DCME02 (Kuehlkamp etÂ al., [2022](#bib.bib92)) | NIR | n/a | 259 | 5770 |
    1-53 | - |'
  prefs: []
  type: TYPE_TB
- en: '| IITD (Kumar and Passi, [2010](#bib.bib93)) | NIR | 320$\times$240 | 224 |
    1120 | 1 | Varying quality |'
  prefs: []
  type: TYPE_TB
- en: '| Iris-Mobile (Odinokikh etÂ al., [2019](#bib.bib122)) | NIR | n/a | 750 | 22966
    | n/a | Mobile device, indoor & outdoor |'
  prefs: []
  type: TYPE_TB
- en: '| JluIrisV3.1 (Zhao etÂ al., [2019](#bib.bib208)) | NIR | 640$\times$480 | 120
    | 1780 | n/a | - |'
  prefs: []
  type: TYPE_TB
- en: '| JluIrisV4 (Zhao etÂ al., [2019](#bib.bib208)) | NIR | 640$\times$480 | 172
    | 114904 | n/a | - |'
  prefs: []
  type: TYPE_TB
- en: '| LivDet-2013-Warsaw (Czajka, [2013](#bib.bib29)) | NIR | 640$\times$480 |
    284 | 1667 | 1 | High quality images |'
  prefs: []
  type: TYPE_TB
- en: '| MICHE-I (De Marsico etÂ al., [2015](#bib.bib39)) | VW | var. | 184 | 3732
    | 2 | Three mobile devices |'
  prefs: []
  type: TYPE_TB
- en: '| MMU | NIR | 320$\times$240 | 92 | 460 | 1 | High quality images |'
  prefs: []
  type: TYPE_TB
- en: '| MobBIOfake (Sequeira etÂ al., [2014](#bib.bib147)) | VW | 300$\times$200 |
    200 | 1600 | 1 | With a handheld device |'
  prefs: []
  type: TYPE_TB
- en: '| ND-CrossSensor-2013 (Xiao etÂ al., [2013](#bib.bib188)) | NIR | 640$\times$480
    | 1352 | 146550 | 27 | Multi-sensor |'
  prefs: []
  type: TYPE_TB
- en: '| ND-Iris-0405 (Phillips etÂ al., [2010](#bib.bib128)) | NIR | 640$\times$480
    | 712 | 64980 | 1 | Varying quality |'
  prefs: []
  type: TYPE_TB
- en: '| ND-TWINS-2009-2010 | VW | n/a | 435 | 24050 | n/a | Facial pictures frontal,
    3/4 and side views. Indoor & outdoor |'
  prefs: []
  type: TYPE_TB
- en: '| OpenEDS (Garbin etÂ al., [2019](#bib.bib59)) | NIR | 640$\times$400 | 304
    | 356649 | 1 | From head-mounted VR glasses |'
  prefs: []
  type: TYPE_TB
- en: '| Q-FIRE (Johnson etÂ al., [2010](#bib.bib82)) | NIR | var. | 390 | 586560 |
    2 | Iris/face Videos, various distances and quality |'
  prefs: []
  type: TYPE_TB
- en: '| UBIRIS.v1 (ProenÃ§a and Alexandre, [2005](#bib.bib131)) | VW | 800$\times$600
    | 241 | 1877 | 2 | Several noise factors |'
  prefs: []
  type: TYPE_TB
- en: '| UBIRIS.v2 (Proenca etÂ al., [2010](#bib.bib132)) | VW | 400$\times$300 | 522
    | 11102 | 2 | Distant acquisition, on the move |'
  prefs: []
  type: TYPE_TB
- en: '| Warsaw (Boyd etÂ al., [2020b](#bib.bib19)) | NIR, VW | n/a | 157 | 4866 |
    1-13 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Warsaw-Post-Mortem v1.0 (Trokielewicz etÂ al., [2016a](#bib.bib168)) | NIR,
    VW | var. | 34 | 1330 | 2-3 | Deceased persons, 5-7h to 17 days postmortem |'
  prefs: []
  type: TYPE_TB
- en: '| Warsaw-Post-Mortem v2.0 (Trokielewicz etÂ al., [2019](#bib.bib171)) | NIR,
    VW | var. | 73 | 2987 | 1-13 | Deceased persons |'
  prefs: []
  type: TYPE_TB
- en: 8\. Open-Source Deep Learning-Based Iris Recognition Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we summarize the main properties of the datasets employed by the methods
    of the previous sections for DL-based iris segmentation, recognition and PAD.
    We also describe available open-source software code for these tasks, and other
    relevant tools.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1\. Data Sources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TableÂ [4](#S7.T4 "Table 4 â€£ 7\. Recognition in Less Controlled Environments:
    Iris/Periocular Analysis â€£ Deep Learning for Iris Recognition: A Survey") gives
    the technical details of the datasets used in the segmentation and recognition
    methods of TablesÂ [1](#S2.T1 "Table 1 â€£ 2\. Deep Learning-Based Iris Segmentation
    â€£ Deep Learning for Iris Recognition: A Survey") and [2](#S3.T2 "Table 2 â€£ 3.2.3\.
    Iris Similarity Networks â€£ 3.2\. Deep Learning-based Iris Matching Strategies
    â€£ 3\. Deep Learning-Based Iris Recognition â€£ Deep Learning for Iris Recognition:
    A Survey"). TableÂ [5](#S8.T5 "Table 5 â€£ 8.1\. Data Sources â€£ 8\. Open-Source Deep
    Learning-Based Iris Recognition Tools â€£ Deep Learning for Iris Recognition: A
    Survey") does the same for the iris PAD methods of TableÂ [3](#S4.T3 "Table 3 â€£
    4.5\. Open Research Questions in Iris PAD â€£ 4\. Deep Learning-Based Iris Presentation
    Attack Detection â€£ Deep Learning for Iris Recognition: A Survey"). We show the
    main properties (spectrum, image size, identities, images, sessions) and relevant
    features. Only the datasets of the methods reported in previous section are presented.
    Since we focus on the most recent developments, we consider that such approach
    provides the most relevant datasets for each task. Of course, the list of available
    datasets after decades of iris research is much longer (Omelina etÂ al., [2021](#bib.bib123)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A first observation is the dominance of near infrared (NIR) over the visible
    (VW) spectrum, which should not be surprising, since NIR is regarded as most suitable
    for iris analysis. However, research-wise, many segmentation and recognition studies
    (TablesÂ [1](#S2.T1 "Table 1 â€£ 2\. Deep Learning-Based Iris Segmentation â€£ Deep
    Learning for Iris Recognition: A Survey"), [2](#S3.T2 "Table 2 â€£ 3.2.3\. Iris
    Similarity Networks â€£ 3.2\. Deep Learning-based Iris Matching Strategies â€£ 3\.
    Deep Learning-Based Iris Recognition â€£ Deep Learning for Iris Recognition: A Survey"))
    use VW images, pushed by the success of challenging databases such as MICHE and
    UBIRIS. On the contrary, the VW modality in iris PAD research is residual (TableÂ [3](#S4.T3
    "Table 3 â€£ 4.5\. Open Research Questions in Iris PAD â€£ 4\. Deep Learning-Based
    Iris Presentation Attack Detection â€£ Deep Learning for Iris Recognition: A Survey")),
    a tendency also observed in pre-DL research (Czajka and Bowyer, [2018](#bib.bib31);
    Boyd etÂ al., [2020a](#bib.bib17)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to the types of Presentation Attack Instruments (PAIs) employed
    in iris PAD databases, they can be categorized into:'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PP: paper printout of a real iris image, i.e. from a live person'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PPD: paper printout of a real iris image with a transparent 3D plastic eye
    dome on top'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLL: textured contact lenses worn by a live person'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CLP: textured contact lenses on printout (either a printout of a CLL image,
    or a printout of a real iris image with a textured contact lens placed on top)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RA: replay attack, i.e. a real iris image shown on a display'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AE: artificial eyeball (plastic eyes of two different types: Van Dyke Eyes,
    with higher iris quality details, and Scary eyes, plastic fake eyes with a simple
    pattern on the iris region)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AEC: artificial eyeball with a textured contact lens on top'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SY: synthetic iris, i.e. an image created via generative methods'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PM: postmortem iris, i.e. an image acquired from cadaver eyes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These PAIs mostly entail presenting the mentioned instrument to the iris sensor,
    which then captures an image of the artifact. An exception is â€œSYâ€, which directly
    produces a synthetic digital image, although such image could be used as base
    to, for example, PP, PPD, RA, or AE attacks. In TableÂ [5](#S8.T5 "Table 5 â€£ 8.1\.
    Data Sources â€£ 8\. Open-Source Deep Learning-Based Iris Recognition Tools â€£ Deep
    Learning for Iris Recognition: A Survey"), it can be seen that CLL (textured lenses
    live) and PP (paper printouts) largely dominates as the most popular PAIs on the
    existing databases, and consequently, on the related research (TableÂ [3](#S4.T3
    "Table 3 â€£ 4.5\. Open Research Questions in Iris PAD â€£ 4\. Deep Learning-Based
    Iris Presentation Attack Detection â€£ Deep Learning for Iris Recognition: A Survey")).
    CLP (textured lenses on printout) also appears in many studies, driven by the
    wide use of the LivDet-2017-IIITD-WVU set, which includes such PAI. CASIA-Iris-Fake,
    which contains AE (artificial eyes) and SY (synthetic irises) also appears in
    a few studies. Other attacks that one may expect on the digital era, such as RA
    (replay), however, are residual in datasets and recent studies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5. Summary of datasets used in the DL-based iris Presentation Attack
    Detection methods of TableÂ [3](#S4.T3 "Table 3 â€£ 4.5\. Open Research Questions
    in Iris PAD â€£ 4\. Deep Learning-Based Iris Presentation Attack Detection â€£ Deep
    Learning for Iris Recognition: A Survey") (NIR: *near-infrared*; VW: *visible
    wavelength*). The type of PAIs (second column) are PP: paper printout, PPD: paper
    printout with plastic dome, CLL: textured contact lenses (live), CLP: textured
    contact lenses (printout), RA: replay attack (display), AE: artificial eyeball,
    AEC: artificial eyeball with textured contact lens, SY: synthetic iris, PM: postmortem
    iris. TTP (next to last column) indicates the existence of a training/test split.
    The features (last column) are MS: multi-sensor, ME: multi-environment (e.g. indoor/outdoor,
    light variability, mobile environment, etc.), UPAI: unseen PAIs in the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | PAIs | Data | Size | # IDs | # Samples | TTP | Features |'
  prefs: []
  type: TYPE_TB
- en: '| live | fake | live | fake | total |'
  prefs: []
  type: TYPE_TB
- en: '| CASIA-Iris-Fake (Sun etÂ al., [2014](#bib.bib156)) | PP, CLL, AE, SY | NIR
    | 640$\times$480 | 1000 | 815 | 6000 | 4120 | 10240 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| IF-VE (Zhang etÂ al., [2021](#bib.bib200)) | CLL | NIR | n/a | 200 | 200 |
    25000 | 25000 | 50000 | âœ“ | MS, ME |'
  prefs: []
  type: TYPE_TB
- en: '| IPITRT (Peng etÂ al., [2020](#bib.bib127)) | PP | NIR | var. | 58 | n/a |
    1800 | 551 | 2351 |  | ME |'
  prefs: []
  type: TYPE_TB
- en: '| IIITD-CLI (Kohli etÂ al., [2013](#bib.bib90)) | CLL | NIR | 640$\times$480
    | 202 | n/a | n/a | n/a | 6570 | âœ“ | MS |'
  prefs: []
  type: TYPE_TB
- en: '| IIITD-ISÂ³  (Gupta etÂ al., [2014](#bib.bib65)) | PP, CLP | NIR | 640$\times$480
    | 202 | n/a | 0 | 4848 | 4848 |  | MS |'
  prefs: []
  type: TYPE_TB
- en: '| LivDet-2017 (Yambay etÂ al., [2017](#bib.bib195)) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| -Clarkson | PP, CLL | NIR | 640$\times$480 | 50 | n/a | 3954 | 4141 | 8095
    | âœ“ | UPAI (additional patterned lenses) |'
  prefs: []
  type: TYPE_TB
- en: '| -IIITD-WVUÂ¹ | PP, CLL, CLP | NIR | 640$\times$480 | n/a | n/a | 2952 | 4507
    | 7459 | âœ“ | MS, ME, UPAI (additional patterned lenses) |'
  prefs: []
  type: TYPE_TB
- en: '| -ND-CLDÂ² | CLL | NIR | 640$\times$480 | n/a | n/a | 2400 | 2400 | 4800 |
    âœ“ | UPAI (additional patterned lenses) |'
  prefs: []
  type: TYPE_TB
- en: '| -Warsaw | PP | NIR | 640$\times$480 | 457 | 446 | 5168 | 6845 | 12013 | âœ“
    | MS |'
  prefs: []
  type: TYPE_TB
- en: '| LivDet-2020 (Das etÂ al., [2020](#bib.bib34)) | PP, PPD, CLL, CLP, RA, AE,
    AEC, PM | NIR | 640$\times$480 | n/a | n/a | 5331 | 7101 | 12432 |  | MS |'
  prefs: []
  type: TYPE_TB
- en: '| Iris-CL1 (Tapia etÂ al., [2022](#bib.bib163)) | PP | NIR | var. | n/a | n/a
    | n/a | 1800 | n/a |  | MS |'
  prefs: []
  type: TYPE_TB
- en: '| JHU-APL (Chen and Ross, [2021](#bib.bib20)) | CLL, AE | NIR | n/a | n/a |
    n/a | 7191 | 7214 | 14405 |  | ME |'
  prefs: []
  type: TYPE_TB
- en: '| MSU-IrisPA-01 (Yadav etÂ al., [2019a](#bib.bib192)) | PP, CLL, RA, AE | NIR
    | 640$\times$480 | n/a | n/a | 1343 | 2523 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MUIPA (Yadav etÂ al., [2018](#bib.bib191)) | PP, CLL | NIR | 640$\times$480
    | 70 | 70 | n/a | n/a | 10296 |  | ME |'
  prefs: []
  type: TYPE_TB
- en: '| ND-CLD-13 (Doyle etÂ al., [2013](#bib.bib43)) | CLL | NIR | 640$\times$480
    | 330 | n/a | 3400 | 1700 | 5100 | âœ“ | MS |'
  prefs: []
  type: TYPE_TB
- en: '| ND-CLD-15Â²  (Doyle and Bowyer, [2015](#bib.bib42)) | CLL | NIR | 640$\times$480
    | n/a | n/a | 4800 | 2500 | 7300 | âœ“ | MS |'
  prefs: []
  type: TYPE_TB
- en: '| NDIris3D (Fang etÂ al., [2021a](#bib.bib52)) | CLL | NIR | 640$\times$480
    | 176 | 176 | 3458 | 3392 | 6850 |  | MS |'
  prefs: []
  type: TYPE_TB
- en: '| ND-PSIDâ´  (Czajka etÂ al., [2019](#bib.bib32)) | CLL | NIR | 640$\times$480
    | 238 | 238 | 3132 | 2664 | 5796 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| UnMIPA (Yadav etÂ al., [2019b](#bib.bib190)) | CLL | NIR | 640$\times$480
    | 162 | 162 | 9319 | 9387 | 18706 |  | MS, ME |'
  prefs: []
  type: TYPE_TB
- en: '| Warsaw-Post-Mortem v3.0 (Trokielewicz etÂ al., [2020](#bib.bib173)) | PM |
    NIR, VW | var. | 0 | 79 | 0 | 1879 | 1879 |  | MS |'
  prefs: []
  type: TYPE_TB
- en: '| Â¹ Contains IIITD-CLI and IIITD-IS |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Â² Iris-LivDet-2017-ND-CLD is a subset of ND-CLD-15 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Â³ IIITD-IS images are printouts of IIITD-CLI captured with a iris scanner
    and a flatbed scanner |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| â´ ND-PSID is a subset of ND-CLD-15 |  |  |'
  prefs: []
  type: TYPE_TB
- en: 8.2\. Software Tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The availability of DL-based tools for iris biometrics has been scarce for years,
    specially for PAD (Fang etÂ al., [2021a](#bib.bib52)). In the following, we provide
    a short description of peer-reviewed references with associated source code (link
    included in the paper, or easily found on the websites of the authors or dedicated
    sites such as www.paperswithcode.com). We describe (in this order) tools for segmentation,
    recognition and PAD. For each type, the references are then presented in cronological
    order.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1\. Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lozej et al. (Lozej etÂ al., [2018](#bib.bib102)) released their end-to-end DL
    model based on the U-Net architecture (Ronneberger etÂ al., [2015](#bib.bib140)).
    The model was trained and evaluated with a small set of 200 annotated iris images
    from CASIA database. The authors also explored the impact of the model depth and
    the use of batch normalization layers.
  prefs: []
  type: TYPE_NORMAL
- en: Kerrigan et al. (Kerrigan etÂ al., [2019](#bib.bib86)) released the code and
    models of Iris-recognition-OTS-DNN, a set of four architectures based on off-the-shelf
    CNNs trained for iris segmentation (two VGG-16 with dilated convolutions, one
    ResNet with dilated kernels, and one SegNet encoder/decoder). Training databases
    included CASIA-Irisv4-Interval, ND-Iris-0405, Warsaw-Post-Mortem v2.0 and ND-TWINS-2009-2010,
    whereas testing data came from ND-Iris-0405 (disjoint subject), BioSec and UBIRIS.v2.
    Results showed that the DL solutions evaluated outperform traditional segmentation
    techniques, e.g. Hough transform or integro-differential operators. It was also
    seen that each test dataset had a method that performs best, with UBIRIS obtaining
    the worst performance. This should not come as a surprise, since it contains VW
    images with high variability taking distantly with a digital camera, whereas the
    other two are from close-up NIR iris sensors in controlled environments.
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. (Wang etÂ al., [2020a](#bib.bib177)) released the code and models
    of their high-efficiency segmentation approach, IrisParseNet. A multi-task attention
    network was first applied to simultaneously predict the iris mask, pupil mask
    and iris outer boundary. Then, from the predicted masks and outer boundary, a
    parameterization of the iris boundaries was calculated. The solution is complete,
    in the sense that the mask (including light reflections and occlusions) and the
    parameterized inner and outer iris boundaries are jointly achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, authors from the same group presented IrisSegBenchmark (Wang
    and Sun, [2020](#bib.bib178)), an open iris segmentation evaluation benchmark
    where they implemented six different CNN architectures, including Fully Convolutional
    Networks (FCN) (Long etÂ al., [2015](#bib.bib101)), Deeplab V1,V2,V3 (Chen etÂ al.,
    [2017](#bib.bib22)), ParseNet (Liu etÂ al., [2016a](#bib.bib99)), PSPNet (Zhao
    etÂ al., [2017](#bib.bib207)), SegNet (Badrinarayanan etÂ al., [2017](#bib.bib10)),
    and U-Net (Ronneberger etÂ al., [2015](#bib.bib140)). The methods were evaluated
    on CASIA-Irisv4-Distance, MICHE-I and UBIRIS.v2. As in (Kerrigan etÂ al., [2019](#bib.bib86)),
    results showed that the best method depends on the database, being: ParseNet for
    CASIA (NIR data), DeeplabV3 for MICHE (VW images from mobile devices), and U-Net
    for UBIRIS (VW images from a digital camera). In this case, however, the three
    tests databases behaved approximately equal, since they all contain difficult
    distant data. CASIA showed a slightly better accuracy, suggesting that NIR data
    may be easier to segment. Traditional, non-DL methods were also evaluated, concluding
    that DL-based segmentation achieves superior accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Banerjee et al. (Banerjee etÂ al., [2022](#bib.bib11)) released the code of their
    V-Net architecture, designed to overcome some drawbacks of U-Net, such as instability
    to tackle iris segmentation or tendency to overfit. A pre-processing stage on
    the YCrCb and HSV spaces was also added to detect salient regions and aid detection
    of iris boundaries. The method was evaluated on the difficult UBIRIS.v2 VW dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2\. Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code of the DL method ThirdEye was released by Ahmad and Fuller (Ahmad and
    Fuller, [2019](#bib.bib4)), based on a ResNet-50 trained with triplet loss. Authors
    directly used segmented images without normalization to a rectangular 2D representation,
    arguing that such step may be counterproductive in unconstrained images. The model
    was evaluated on the ND-0405, IITD and UBIRIS.v2 datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The models of Boyd et al. (Boyd etÂ al., [2019](#bib.bib16)) for recognition
    have been also released, based on a ResNet-50 with different weight initialization
    techniques, comprising: from scratch (random), off-the-shelf ImageNet (general-purpose
    vision weights), off-the shelf VGGFace2 (face recognition weights), fine-tuned
    ImageNet weights, and fine-tuned VGGFace2 weights. Both ImageNet and VGGFace2
    are very large datasets with millions of images, and face images contain the iris
    region. Thus, using these datasets as initialization may be beneficial for iris
    recognition, where available training data is in the order of hundreds of thousand
    images only. This strategy has been followed e.g. in ocular soft-biometrics as
    well (Alonso-Fernandez etÂ al., [2021](#bib.bib7)). The observed optimal strategy
    is indeed to fine-tune an off-the-shelf set of weights to the iris recognition
    domain, be general-purpose or face recognition weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3\. Segmentation and Recognition Packages
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A complete package comprising segmentation and feature encoding was provided
    by Tann et al.(Tann etÂ al., [2019](#bib.bib162)). The segmentator is based on
    a Fully Convolutional Network (FCN), but encoding is based on hand-crafted Gabor
    filters (Daugman, [2007](#bib.bib36)). Evaluation was done on CASIA-Irisv4-Interval
    and IITD.
  prefs: []
  type: TYPE_NORMAL
- en: In forensic investigation for diseased eyes and post-mortem samples, Czajka
    (Czajka, [2021](#bib.bib30)) also released a complete package combining segmentation
    and feature encoding. The models are based on previous efforts of the author and
    co-workers, comprising a SegNet (Trokielewicz etÂ al., [2020](#bib.bib173)) and
    a CCNet (Mishra etÂ al., [2019](#bib.bib107)) DL segmentators, but the feature
    encoder is based on hand-crafted BSIF filters.
  prefs: []
  type: TYPE_NORMAL
- en: Another complete segmentation and recognition package was released by Kuehlkamp
    et al. (Kuehlkamp etÂ al., [2022](#bib.bib92)). The segmentator is based on a fine-tuned
    Mask-RCNN architecture, with the cropped iris region fed directly into a ResNet50
    pre-trained for face recognition on the very large VGGFace2 dataset, and fine-tuned
    for iris recognition using triplet loss. The paper is oriented towards postmortem
    iris analysis, so the methods use a mixture of live and postmortem images for
    training and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Parzianello and Czajka (Parzianello and Czajka, [2022](#bib.bib126)) also released
    the models and annotated data for their textured contact lens aware iris recognition
    method. The foundation is that such lenses may be used normally for cosmetic purposes,
    without intention to fool the biometric system. Therefore, they proposed to detect
    and match portions of live iris tissue still visible in order to enable recognition
    even when a person wears textured contact lenses. To do so, they applied a Mask
    R-CNN as a segmentation backbone, trained to detect authentically-looking parts
    of the iris using manually segmented samples from NDIris3D dataset. Non-iris information
    is then removed from the training images by blurring it or replacing it with random
    noise to guide the subsequent recognition network (based on ResNet-18) to salient,
    non-occluded regions that should be used for matching.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4\. Iris PAD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the iris PAD arena, Gragnaniello et al. (Gragnaniello etÂ al., [2016](#bib.bib62))
    proposed a CNN that incorporates domain-specific knowledge. Based on the assumption
    that PAD relies on residual artifacts left mostly in high-frequencies, a regularization
    term was added to the loss function which forces the first layer to behave as
    a high-pass filter. The method, which is available in the website of the first
    author, could be applied to PAD in multiple modalities, including iris and face.
  prefs: []
  type: TYPE_NORMAL
- en: The code and model of the method of Sharma and Ross (Sharma and Ross, [2020](#bib.bib151))
    (D-NetPAD) is also available. It is based on DenseNet121 and trained for a variety
    of PAIs (printouts, artificial eye, cosmetic contacts, kindle replay, and transparent
    dome on print), with an script to retrain the method also available.
  prefs: []
  type: TYPE_NORMAL
- en: '8.3\. Other Tools: Iris Image Quality Assessment'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Several image properties considered to potentially influence the accuracy of
    iris biometrics have been defined in support of the standard ISO/IEC 29794-6 (technology
    â€” Biometric sample quality â€” Part 6: IrisÂ image data, [2015](#bib.bib165)). They
    include: grayscale spread (dynamic range), iris size (pixels across the iris radius
    when the boundaries are modeled by a circle), dilation (ratio of the pupil to
    iris radius), usable iris area (percentage of non-occluded iris, either by eyelashes,
    eyelids or reflections), contrast of pupil and sclera boundaries, shape (irregularity)
    of pupil and sclera boundaries, margin (distance between the iris boundary and
    the closest image edge), sharpness (absence of defocus blur), motion blur, signal
    to noise ratio, gaze (deviation of the optical axis of the eye from the optical
    axis of the camera), and interlace of the acquisition device.'
  prefs: []
  type: TYPE_NORMAL
- en: Low quality iris images, which can potentially appear in uncontrolled or non-cooperative
    environments, are known to reduce the performance of iris location, segmentation
    and recognition. Thus, an accurate quality assessment can be a valuable tool in
    support of the overall pipeline, either by dropping low quality images, or invoking
    specialized processing (Alonso-Fernandez etÂ al., [2012](#bib.bib6)). One possibility
    might be to quantify the properties mentioned above, and placing thresholds on
    each. A more elaborated alternative is to combine them according to some rule
    and produce an overall quality score. However, it is difficult to provide metrics
    that cover all types of quality distortions (Tabassi etÂ al., [2011](#bib.bib158))
    and doing so for some indeed entails to segment the iris.
  prefs: []
  type: TYPE_NORMAL
- en: Broadly, a biometric sample is of good quality if it is suitable for recognition,
    so quality should correlate with recognition performance (Grother and Tabassi,
    [2007](#bib.bib63)). As such, quality assessment can be viewed as a regression
    problem. Wang et al. (Wang etÂ al., [2020c](#bib.bib183)) considered that a non-ideal
    eye image will pivot in the feature space around the embedding of an ideal image.
    They defined quality as the distance to the embedding of such â€œidealâ€ image which,
    is regarded as a registration sample collected under a highly controlled environment.
    They used a model to learn the mapping between images and Distance in Feature
    Space (DFS) directly from a given dataset. Quality is computed via attention-based
    pooling that combines a heatmap that comes from a coarse segmentation based on
    U-Net and the feature map of an extraction network based on MobileNetv2 pre-trained
    on CASIA-Iris-V4 and NDIRIS-0405.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Emerging Research Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the most relevant open challenges and hypothesize
    about emerging research directions that could become *hot-topics* in biometrics
    literature in a close future.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1\. Resource-aware designs of iris recognition networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Application-wise, iris recognition can be performed on a wide range of hardware,
    ranging from high-end computers to low-end embedded devices, or from large computer
    clusters to personal devices such as mobile phones. Performing recognition on
    resource-limited hardware could pose new challenges for deep learning based iris
    networks, which usually contain hundreds of layers and millions of parameters.
    Therefore designing these deep learning networks necessarily need to be aware
    of the hardware platforms on which they will be run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lightweight models: Lightweight CNNs employ advanced techniques to efficiently
    trade-off between resource and accuracy, minimising their model size and computations
    in term of the number of floating point operations (FLOPs), while retaining high
    accuracies. Specialized lightweight CNN architectures include MobileNets (Howard
    etÂ al., [2019](#bib.bib74)) and U-Net (Ronneberger etÂ al., [2015](#bib.bib140)).
    There are a few lightweight deep learning based models for both segmentation and
    feature extraction. Fang *et al.* (Fang and Czajka, [2020](#bib.bib51)) adapted
    the lightweight CC-Net (Mishra etÂ al., [2019](#bib.bib107)) for iris segmentation.
    CC-Net has a U-Net structure (Ronneberger etÂ al., [2015](#bib.bib140)), able to
    retain up to 95% accuracy using only 0.1% of the trainable parameters. Boutros
    *et al.* (Boutros etÂ al., [2020](#bib.bib14)) benchmarked MobileNet-V3 against
    deeper networks for iris recognition and showed that the MobileNet based model
    can achieve similar EER with 85% less number of parameters and 80% less inference
    time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model compression: Studies have found that most of the large deep learning
    models tend to be overparameterized, leading to lots of redundant parameters and
    operations in the network. This becomes more severe considering iris texture images
    are different from generic object-based images. This has motivated a hot trend
    looking to remove these redundancies from the models, including pruning, quantization
    and low-rank factorization (Liang etÂ al., [2021](#bib.bib96)). In our iris recognition
    literature, there a few lightweight deep learning based models for both segmentation
    and feature extraction. Tann *et al.* (Tann etÂ al., [2019](#bib.bib162)) quantized
    64-bit floating points numbers of weights and activations of the full FCN-based
    iris segmentation model using an 8-bit dynamic fixed-point (DFP) format, which
    provide a 8$\times$ memory saving as well as speed enhancement due to reduced
    complexity of lower precision operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Architecture Search: Neural Architecture Search (NAS) automates the
    process of architecture design of neural networks by iteratively sampling a population
    of child networks, evaluating the child modelsâ€™ performance metrics as rewards
    and learning to generate high-performance architecture candidates (Elsken etÂ al.,
    [2019](#bib.bib44)). In our iris recognition literature, Nguyen *et al.* (Nguyen
    etÂ al., [2020](#bib.bib114)) showed that computation and memory can be incorporated
    into the NAS formulation to enable resource-constrained design of deep iris networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2\. Human-interpretable methods and XAI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With hundreds of layers and millions of parameters, deep learning networks are
    usually opaque or â€œblackboxâ€ where humans struggle to understand why a deep network
    predict what it predicts. This necessitates approaches to make deep learning methods
    more interpretable and understandable to humans. Interestingly, the need for human-interpretable
    methods has been raised even from the handcrafted era. For example, Shen *et al.*
    published a series of work (Chen etÂ al., [2016](#bib.bib21); Shen and Flynn, [2013](#bib.bib153))
    on using iris crypts for iris matching. Iris crypts are clearly visible to humans
    in a similar way as finger minutiae. Another example is the macro-features (Sunder
    and Ross, [2010](#bib.bib157)) which use SIFT to detect keypoints and perform
    iris matching based on these keypointsÂ (Quinn, G. and Matey, J. and Grother, P.
    and Watters, E., [2022](#bib.bib137)). Another notable work is by ProenÃ§a *et
    al.* (ProenÃ§a and Neves, [2017](#bib.bib133)) where they proposed a deformation
    field to represent the correspondence between two iris images.
  prefs: []
  type: TYPE_NORMAL
- en: From a deep learning perspective, researchers have also attempted to visualize
    the matching. Kuehlkamp *et al.* (Kuehlkamp etÂ al., [2022](#bib.bib92)) argued
    that existing iris recognition methods offer limited and non-standard methods
    of visualization to let human examiners interpret the model output. They applied
    Class Activation Maps (CAM) (Zhou etÂ al., [2016](#bib.bib213)) to visualize the
    level of contribution of each iris region to the overall matching score. Similarly,
    Nguyen *et al.* (Nguyen etÂ al., [2022](#bib.bib116)) also decomposed the final
    matching score into pixel-level to visualize the level of contribution of each
    pixel to the overall matching score.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3\. Deep learning-based synthetic iris generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data synthesis provides an alternative to time- and resource-consuming database
    collection. One could create as many images as desired, with new textures that
    even do not match any existing identity, which would avoid privacy problems too.
    On the other hand, fake irises that are indistinguishable from real ones can be
    used for identity concealment attacks (if the image does not match any identity)
    or impersonation attacks (if the image resembles an existing identity) (Czajka
    and Bowyer, [2018](#bib.bib31)). Indeed, synthetic irises are present in databases
    employed for iris PAD, such as CASIA-Iris-Fake (TableÂ [5](#S8.T5 "Table 5 â€£ 8.1\.
    Data Sources â€£ 8\. Open-Source Deep Learning-Based Iris Recognition Tools â€£ Deep
    Learning for Iris Recognition: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the purpose or ability to detect if an image is synthetic, Generative
    Adversarial Networks (GANs) (Goodfellow etÂ al., [2014](#bib.bib61)) have shown
    impressive photo-realistic generating capabilities in many domains. GANs learn
    to model image distributions by an adversarial process, where a discriminator
    assesses the realism of images synthesized by a generator. At the end, the generator
    have learned the distribution of the training data, being able to synthesize new
    images with the same characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'For iris generation, some methods by Yadav et al. (Yadav and Ross, [2021](#bib.bib194);
    Yadav etÂ al., [2020](#bib.bib193), [2019a](#bib.bib192)) were mentioned in iris
    PAD contexts (SectionÂ [4.4](#S4.SS4 "4.4\. Adversarial Networks â€£ 4\. Deep Learning-Based
    Iris Presentation Attack Detection â€£ Deep Learning for Iris Recognition: A Survey")).
    RaSGAN (Yadav etÂ al., [2020](#bib.bib193), [2019a](#bib.bib192)) followed the
    traditional approach of driving the generation/discrimination training by randomly
    sampling so-called latent vectors from a probabilistic distribution. As training
    progresses, the generator learns to associate features of the latent vectors with
    semantically meaningful attributes that naturally vary in the images. However,
    this does not impose any restriction in the relationship between features in latent
    space and factors of variation in the image domain, making difficult to decode
    what the latent vectors represent. As a result, the image characteristics (eye
    color, eyelids shape, eyelashes, gender, ageâ€¦) are generated randomly. Kohli et
    al. (Kohli etÂ al., [2017](#bib.bib91)) presented iDCGAN for iris PAD, which also
    followed the latent vector sampling concept. To counteract such issue, researchers
    have tried to incorporate constrains or mechanisms that guide the generation process
    to a desired characteristic. For example, CIT-GAN (Yadav and Ross, [2021](#bib.bib194))
    employed a Styling Network that learns style characteristics of each given domain,
    while taking as input a domain label that drives the network to embed a desired
    style into the generated data.'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar direction, Kaur and Manduchi (Kaur and Manduchi, [2021](#bib.bib85),
    [2020](#bib.bib84)) proposed to synthesize eye images with a desired style (skin
    color, texture, iris color, identity) using an encoder-decoder ResNet. The method
    is aimed at manipulating gaze, so the generator receives a segmentation mask with
    the desired gaze, and an image with the style that will see its gaze modified.
    To achieve cross-spectral recognition, Hernandez-Diaz et al. (Hernandez-Diaz etÂ al.,
    [2020](#bib.bib72)) used CGANs to convert ocular images between VW and NIR spectra
    while keeping identity, so comparisons are done in the same spectrum. This allows
    the use of existing feature methods, which are typically optimized to operate
    in a single spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: Despite great advances in DL-based synthetic image generation, one open problem
    is the possible identity leakage from the training set when creating data of non-existing
    identities, resulting in privacy issues. This has just been revealed recently
    in face generation (Tinsley etÂ al., [2021](#bib.bib166)). Another issue in the
    opposite direction is the difficulty in preserving identity in the generation
    process when the target is precisely creating images of an existing identity with
    different properties. This is an issue being addressed in face generation methods
    [reference under review], but is lacking in iris synthesis research.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4\. Deep learning-based iris super-resolution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main constraints for existing iris recognition systems is the short
    distance of image acquisition, which usually requires a subject to stay still
    less than 60 cm from iris cameras. This is due to the requirement of high-resolution
    iris region, *e.g.* 120 pixels across the iris diameter due to the European standard
    and NIST standard, despite the small physical size of an eye, *i.e.* $15\times
    15$ mm. The lack of resolution of imaging systems has critically adverse impacts
    on the recognition and performance of biometric systems, especially in less constrained
    conditions and long range surveillance applications (Nguyen etÂ al., [2018](#bib.bib117)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Super-resolution, as one of the core innovations in computer vision, has been
    an attractive but challenging solution to address the low resolution problem in
    both general imaging systems and biometric systems. Deep learning based super-resolution
    approaches have been across multiple works in iris recognition. Ribeiro *et al.*
    (Ribeiro etÂ al., [2017](#bib.bib139); Ribeiro etÂ al., [2019](#bib.bib138)) experimented
    two deep learning single-image super-resolution approaches: Stacked Auto-Encoders
    (SAE) and Convolutional Neural Networks (CNN). Both approaches learn one encoder
    to map the high resolution iris images to the low resolution domain, and one decoder
    to learn to reconstruct the original high resolution images from the low resolution
    ones. Zhang *et al.* (Zhang etÂ al., [2016a](#bib.bib202)) learned a single CNN
    to learn non-linear mapping function between LR images to HR images for mobile
    iris recognition. Wang *et al.* (Wang etÂ al., [2019a](#bib.bib184)) extended the
    single CNN to two CNNs: one generator CNN and one discriminator CNN as in the
    GAN architecture. The generator functions similar to the single LR - HR mapping
    CNN. Adding the discriminator CNN allows them to control the generator to generate
    HR images not just visually higher resolution but also preserve the identity of
    the iris. Mostofa *et al.* (Mostofa etÂ al., [2021](#bib.bib110)) incorporated
    a GAN-based photo-realistic super-resolution approach (Ledig etÂ al., [2017](#bib.bib94))
    to improve the resolution of LR iris images from the NIR domain before cross-matching
    the HR outputs with the HR images from the RGB domain. While these approaches
    showed improved performance, dealing with noisy data in such cases as iris at
    a distance and on the move could require the quality of an input iris image to
    be included in the super-resolution process (Nguyen etÂ al., [2011](#bib.bib115)).
    In addition, Nguyen *et al.* argued that a fundamental difference exists between
    conventional super-resolution motivations and those required for biometrics, hence
    proposing to perform super-resolution at the feature level targeting explicitly
    the representation used by recognition (Nguyen etÂ al., [2012](#bib.bib118)).'
  prefs: []
  type: TYPE_NORMAL
- en: 9.5\. Privacy in deep learning-based iris recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Privacy is becoming a key issue in computer vision and machine learning domains.
    In particular, it is accepted that the accuracy attained by deep learning models
    depends on the availability of large amounts of visual data, which stresses the
    need for privacy-preserving recognition solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the goal in privacy preserving deep-learning is to appropriately train
    models while preserving the privacy of the training datasets. While the utility
    of this kind of solutions is obvious, there are certain concerns about the training
    data that supported the model creation, as the collection of images from a large
    number of individuals comes with significant privacy risks. In particular, it
    should be considered that the subjects from whom the data were collected can neither
    delete nor control what actually will be learned from their data.
  prefs: []
  type: TYPE_NORMAL
- en: As most of the existing biometric technologies, DL-based iris recognition pose
    challenges to privacy, which are even more concerning, considering the *data-driven*
    feature of such kind of systems. Particular attention should be paid to avoid
    function creep, guaranteeing that the system yielding from a set of data is not
    used for a different purpose than the originally communicated to the individual
    at the time of providing their information. Covert collection is another major
    concern, which is also particular important for the iris trait, according to the
    possibility of being imaged from large distances and in surreptitious way.
  prefs: []
  type: TYPE_NORMAL
- en: Particular attention has been paid to the development of fair recognition systems,
    in the sense that this kind of systems should attain similar effectiveness in
    different subgroups of the population, regarding different features such as *gender*,
    *age*, *race* or *ethnicity*. For data-driven systems, this might be a relevant
    challenge, considering that most of the existing datasets that support the learned
    systems have evident biases with regared tio the subjectsâ€™ characteristics above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, in a more general machine learning perspective, potential attacks to
    the learned models have been concerning the research community and have been the
    scope of various recent works, attempting to provide defense mechanisms against:
    i) model inversion attacks, that aim to reconstruct the training data from the
    model parameters (e.g.,Â (Khosravy etÂ al., [2022](#bib.bib88)) andÂ (He etÂ al.,
    [2022](#bib.bib68))); ii) membership inference, that attempt to infer whether
    one individual was part of a training set (e.g.,Â (Hu etÂ al., [2022](#bib.bib76))
    andÂ (Song etÂ al., [2019](#bib.bib154))); and iii) training data extraction attacks,
    that aim to recover individual training samples by querying the models (e.g,Â (Khalid
    etÂ al., [2019](#bib.bib87)) andÂ (Ding etÂ al., [2022](#bib.bib40))).'
  prefs: []
  type: TYPE_NORMAL
- en: 9.6\. Deep learning-based iris segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Being one of the earliest phases of the recognition process, segmentation is
    known as one of the most challenging, as it is at the front line for facing the
    dynamics of the data acquisition environments. This is particularly true, in case
    of less constrained data acquisition protocols, where the resulting data have
    highly varying features and the particular conditions of each environment strongly
    determine the most likely data covariates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the segmentation context, the main challenge remains as the development
    of methods robust to *cross-domain* settings, i.e., able to segment the iris region
    for a broad range of image features, e.g., in terms of: 1) illumination, 2) scale,
    3) gaze, 4) occlusions, 5) rotation and 6) pose, corresponding to the acquisition
    in very different environments. Over the past decades, many research groups have
    been devoting their attentions in improving the robustness of iris segmentation,
    which is known to be a primary factor for the final effectiveness of the recognition
    process. In this timeline, the proposed segmentation methods can be roughly grouped
    into three categories: 1) boundary-based methods (using the integro-differential
    operator or Hough transform); 2) based in handcrafted features (particularly suited
    for non-cooperative recognition, e.g.,Â (Tan etÂ al., [2010](#bib.bib161)) andÂ (Tan
    and Kumar, [2012](#bib.bib160))) ; and 3) DL-based solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: For the latter family of methods, the emerging trends are closely related to
    the *general* challenges of DL-based segmentation frameworks, namely to obtain
    interpretable models that allow us to perceive what exactly are these systems
    learning, or the minimal neural architecture that guarantees a predefined level
    of accuracy. Also, the development of weakly supervised or even unsupervised frameworks
    is another *grand-challenge*, as it is accepted that such systems will likely
    adapt better to previously unseen data acquisition conditions. Finally, the computational
    cost of segmentation (both in terms of space and time) is another concern, with
    special impact in the deployment of this kind of frameworks in mobile settings,
    and in the IoT settingÂ (Saleh, [2018](#bib.bib141)).
  prefs: []
  type: TYPE_NORMAL
- en: 9.7\. Deep learning-based iris recognition in visible wavelengths
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Being a topic of study for over a decade (e.g.Â (Liu etÂ al., [2019](#bib.bib100))
    andÂ (ProenÃ§a, [2013](#bib.bib130))), iris recognition in visible wavelengths remains
    essentially as an interesting possibility for delivering biometric recognition
    from large distances (in conditions that are typically associated to visual surveillance
    settings) and in handheld *commercial* devices, such as smartphones.
  prefs: []
  type: TYPE_NORMAL
- en: The emerging trends in this scope regard the development of alternate ways to
    analyze the multi-spectral information available in visible light data (typically
    RGB), i.e., by developing deep learning architectures optimized for fusion, either
    at the data, feature, score or decision levelsÂ (Bigdeli etÂ al., [2021](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: In the visual surveillance setting, the main challenge regards the development
    of optimized data acquisition settings, profiting from the advances in remote
    sensing technologies, that should be able to augment the quality (e.g., resolution
    and sharpness) of the obtained irises. In this scope, the research on active data
    acquisition technologies (based in PTZ devices, or similar) might also be an interesting
    emerging possibilityÂ (Han, [2021](#bib.bib67)).
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Motivated by the tremendous success of DL-based solutions for many different
    solutions to everyday problems, machine learning is entering one of its golden
    era, attracting growing interests from the research, commercial and governmental
    communities. In short, deep learning uses multiple layers to represent the abstractions
    of data to build computational models that - even in a bit surprising way - typically
    surpass the previous generation of handcrafted-based automata. However, being
    extremely data-driven, the effectiveness of DL-based solutions is typically constrained
    by the existence of massive amounts of data, annotated in a consistent way.
  prefs: []
  type: TYPE_NORMAL
- en: As in the generality of the computer-vision topics, a myriad of DL-based techniques
    has been proposed over the last years to perform biometric recognition, and -
    in particular - iris recognition. Nowadays, the existing methods cover the whole
    phases of the typical processing chain, from the preprocessing, segmentation,
    feature extraction up to the matching and recognition steps.
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, this article provides the first comprehensive review of the historical
    and state-of-the-art approaches in DL-based techniques for iris recognition, followed
    by an in-depth analysis on pivoting and groundbreaking advances in each phase
    of the processing chain. We summarize and critically compare the most relevant
    methods for iris acquisition, segmentation, quality assessment, feature encoding,
    matching and recognition problems, also presenting the most relevant open-problems
    for each phase.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we review the typical issues faced in DL-based methods in this domain
    of expertize, such as unsupervised learning, black-box models, and online learning
    and to illustrate how these challenges can be important to open prolific future
    research paths and solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We would like to thank Adam Czajka from the University of Notre Dame, USA for
    the contribution in the early version of this survey paper in Sections 1, 5 and
    6. The work due to Hugo ProenÃ§a was funded by FCT/MEC through national funds and
    co-funded by FEDER - PT2020 partnership agreement under the projects UIDB/50008/2020,
    POCI-01-0247-FEDER- 033395. Author Alonso-Fernandez thanks the Swedish Innovation
    Agency VINNOVA (project MIDAS and DIFFUSE) and the Swedish Research Council (project
    2021-05110) for funding his research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal etÂ al. (2022a) A. Agarwal, A. Noore, M. Vatsa, and R. Singh. 2022a.
    Enhanced iris presentation attack detection via contraction-expansion CNN. *Pattern
    Recognition Letters* 159 (2022), 61â€“69.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal etÂ al. (2022b) A. Agarwal, A. Noore, M. Vatsa, and R. Singh. 2022b.
    Generalized Contact Lens Iris Presentation Attack Detection. *IEEE Transactions
    on Biometrics, Behavior, and Identity Science* (2022), 1â€“1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahmad and Fuller (2019) S. Ahmad and B. Fuller. 2019. ThirdEye: Triplet Based
    Iris Recognition without Normalization. In *IEEE Int. Conf. on Biometrics: Theory
    Applications and Systems (BTAS)*. 1â€“9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alonso-Fernandez and Bigun (2016) F. Alonso-Fernandez and J. Bigun. 2016. A
    survey on periocular biometrics research. *Pattern Recognition Letters* 82 (2016),
    92â€“105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alonso-Fernandez etÂ al. (2012) F. Alonso-Fernandez, J. Fierrez, and J. Ortega-Garcia.
    2012. Quality Measures in Biometric Systems. *IEEE Security and Privacy* 10, 6
    (2012), 52â€“62.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alonso-Fernandez etÂ al. (2021) F. Alonso-Fernandez, K. Hernandez-Diaz, S. Ramis,
    F.Â J. Perales, and J. Bigun. 2021. Facial masks and soft-biometrics: Leveraging
    face recognition CNNs for age and gender prediction on mobile ocular images. *IET
    Biometrics* 10, 5 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anisetti etÂ al. (2019) M. Anisetti, Y.-H. Li, P.-J. Huang, and Y. Juan. 2019.
    An Efficient and Robust Iris Segmentation Algorithm Using Deep Learning. *Mobile
    Information Systems* (2019), 4568929.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arora and Bhatia (2020) S. Arora and M.P.S. Bhatia. 2020. Presentation attack
    detection for iris recognition using deep learning. *Int J Syst Assur Eng Manag*
    11 (2020), 232â€“238.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Badrinarayanan etÂ al. (2017) V. Badrinarayanan, A. Kendall, and R. Cipolla.
    2017. SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.
    *IEEE Trans. Pattern Anal. Mach. Intell.* 39, 12 (2017), 2481â€“2495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banerjee etÂ al. (2022) A. Banerjee, C. Ghosh, and S.Â N. Mandal. 2022. Analysis
    of V-Net Architecture for Iris Segmentation in Unconstrained Scenarios. *SN Computer
    Science* 3 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bigdeli etÂ al. (2021) B. Bigdeli, P. Pahlavani, and H.Â A. Amirkolaee. 2021.
    An ensemble deep learning method as data fusion system for remote sensing multisensor
    classification. *Applied Soft Computing* 110 (2021), 107563.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bolme etÂ al. (2016) D.Â S. Bolme, R.Â A. Tokola, C.Â B. Boehnen, T.Â B. Saul, K.Â A.
    Sauerwein, and D.Â W. Steadman. 2016. Impact of environmental factors on biometric
    matching during human decomposition. In *IEEE Int. Conf. on Biometrics: Theory
    Applications and Systems (BTAS)*. IEEE, USA, 1â€“8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boutros etÂ al. (2020) F. Boutros, N. Damer, K. Raja, R. Ramachandra, F. Kirchbuchner,
    and A. Kuijper. 2020. On Benchmarking Iris Recognition within a Head-mounted Display
    for AR/VR Applications. In *IEEE Int. Joint Conf. on Biometrics (IJCB)*. USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boyd etÂ al. (2022) A. Boyd, K. Bowyer, and A. Czajka. 2022. Human-Aided Saliency
    Maps Improve Generalization of Deep Learning. In *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boyd etÂ al. (2019) A. Boyd, A. Czajka, and K. Bowyer. 2019. DL-Based Feature
    Extraction in Iris Recognition: Use Existing Models, Fine-tune or Train From Scratch?.
    In *IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)*. 1â€“9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boyd etÂ al. (2020a) A. Boyd, Z. Fang, A. Czajka, and K. Bowyer. 2020a. Iris
    presentation attack detection: Where are we now? *Pattern Recognition Letters*
    138 (2020), 483â€“489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boyd etÂ al. (2021) A. Boyd, P. Tinsley, K. Bowyer, and A. Czajka. 2021. CYBORG:
    Blending Human Saliency Into the Loss Improves Deep Learning. arXiv:2112.00686Â [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boyd etÂ al. (2020b) A. Boyd, S. Yadav, T. Swearingen, A. Kuehlkamp, M. Trokielewicz,
    E. Benjamin, P. Maciejewicz, D. Chute, A. Ross, P. Flynn, K. Bowyer, and A. Czajka.
    2020b. Post-Mortem Iris Recognitionâ€”A Survey and Assessment of the State of the
    Art. *IEEE Access* 8 (2020), 136570â€“136593.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Ross (2021) C. Chen and A. Ross. 2021. An Explainable Attention-Guided
    Iris Presentation Attack Detector. In *IEEE Workshop on Applications of Computer
    Vision (WACV)*. 97â€“106.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al. (2016) J. Chen, F. Shen, D.Â Z. Chen, and P. Flynn. 2016. Iris Recognition
    Based on Human-Interpretable Features. *IEEE Transactions on Information Forensics
    and Security* 11, 7 (2016), 1476â€“1485.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen etÂ al. (2017) L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. Yuille.
    2017. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous
    Convolution, and Fully Connected CRFs. *IEEE Trans. Pattern Anal. Mach. Intell.*
    4 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al. (2019a) Y. Chen, W. Wang, Z. Zeng, and Y. Wang. 2019a. An Adaptive
    CNNs Technology for Robust Iris Segmentation. *IEEE Access* 7 (2019), 64517â€“64532.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen etÂ al. (2020) Y. Chen, C. Wu, and Y. Wang. 2020. T-Center: A Novel Feature
    Extraction Approach Towards Large-Scale Iris Recognition. *IEEE Access* 8 (2020),
    32365â€“32375.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al. (2019b) Y. Chen, Z. Zeng, and F. Hu. 2019b. End to End Robust Recognition
    Method for Iris Using a Dense Deep Convolutional Neural Network. In *Biometric
    Recognition*. Springer, Cham, 364â€“375.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choudhary etÂ al. (2021) M. Choudhary, V. Tiwari, and Venkanna U. 2021. Ensuring
    Secured Iris Authentication for Mobile Devices. In *IEEE International Conference
    on Consumer Electronics (ICCE)*. 1â€“5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choudhary etÂ al. (2022a) M. Choudhary, V. Tiwari, and U. Venkanna. 2022a. Iris
    Liveness Detection Using Fusion of Domain-Specific Multiple BSIF and DenseNet
    Features. *IEEE Transactions on Cybernetics* 52, 4 (2022), 2370â€“2381.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choudhary etÂ al. (2022b) M. Choudhary, Tiwari V., and Venkanna U. 2022b. Identifying
    discriminatory feature-vectors for fusion-based iris liveness detection. *J Ambient
    Intell Human Comput* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Czajka (2013) A. Czajka. 2013. Database of iris printouts and its application:
    Development of liveness detection method for iris recognition. In *International
    Conference on Methods and Models in Automation and Robotics (MMAR)*. 28â€“33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Czajka (2021) A. Czajka. 2021. Iris recognition designed for post-mortem and
    diseased eyes. (2021). [https://github.com/aczajka/iris-recognition---pm-diseased-human-driven-bsif](https://github.com/aczajka/iris-recognition---pm-diseased-human-driven-bsif)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Czajka and Bowyer (2018) A. Czajka and K. Bowyer. 2018. Presentation Attack
    Detection for Iris Recognition: An Assessment of the State-of-the-Art. *ACM Comput.
    Surv.* 51, 4, Article 86 (Jul 2018), 35Â pages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Czajka etÂ al. (2019) A. Czajka, Z. Fang, and K. Bowyer. 2019. Iris Presentation
    Attack Detection Based on Photometric Stereo Features. In *IEEE Winter Conference
    on Applications of Computer Vision (WACV)*. 877â€“885.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Damer etÂ al. (2019) N. Damer, K. Dimitrov, A. Braun, and A. Kuijper. 2019.
    On Learning Joint Multi-biometric Representations by Deep Fusion. In *IEEE Int.
    Conf. on Biometrics: Theory Applications and Systems (BTAS)*. 1â€“8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Das etÂ al. (2020) P. Das, J. Mcfiratht, Z. Fang, A. Boyd, G. Jang, A. Mohammadi,
    S. Purnapatra, D. Yambay, S. Marcel, M. Trokielewicz, P. Maciejewicz, K. Bowyer,
    A. Czajka, S. Schuckers, J. Tapia, S. Gonzalez, M. Fang, N. Damer, F. Boutros,
    A. Kuijper, R. Sharma, C. Chen, and A. Ross. 2020. Iris Liveness Detection Competition
    (LivDet-Iris) - The 2020 Edition. In *IEEE Int. Joint Conf. on Biometrics (IJCB)*.
    1â€“9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daugman (1993) J. Daugman. 1993. High confidence visual recognition of persons
    by a test of statistical independence. *IEEE Trans. Pattern Anal. Mach. Intell.*
    15, 11 (1993), 1148â€“1161.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daugman (2007) J. Daugman. 2007. New methods in iris recognition. *IEEE Transactions
    on Systems, Man and Cybernetics* 37 (2007), 1167â€“1175.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daugman (2016) J. Daugman. 2016. Information Theory and the IrisCode. *IEEE
    Transactions on Information Forensics and Security* 11 (2016), 400â€“409.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daugman (2021) J. Daugman. 2021. Collision Avoidance on National and Global
    Scales: Understanding and Using Big Biometric Entropy. *TechRxiv* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Marsico etÂ al. (2015) M. De Marsico, M. Nappi, D. Riccio, and H. Wechsler.
    2015. Mobile Iris Challenge Evaluation (MICHE)-I, biometric iris dataset and protocols.
    *Pattern Recognition Letters* 57 (2015), 17â€“23. Mobile Iris CHallenge Evaluation
    part I (MICHE I).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding etÂ al. (2022) X. Ding, H. Fang, Z. Zhang, K.-K.Â R. Choo, and H. Jin. 2022.
    Privacy-Preserving Feature Extraction via Adversarial Training. *IEEE Transactions
    on Knowledge and Data Engineering* 34, 4 (2022), 1967â€“1979.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong etÂ al. (2009) W. Dong, Z. Sun, and T. Tan. 2009. A Design of Iris Recognition
    System at a Distance. In *Chinese Conference on Pattern Recognition*. 1â€“5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doyle and Bowyer (2015) J.Â S. Doyle and K. Bowyer. 2015. Robust Detection of
    Textured Contact Lenses in Iris Recognition Using BSIF. *IEEE Access* 3 (2015),
    1672â€“1683.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Doyle etÂ al. (2013) J.Â S. Doyle, K. Bowyer, and P. Flynn. 2013. Variation in
    accuracy of textured contact lens detection based on sensor and lens pattern.
    In *IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)*. 1â€“7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken etÂ al. (2019) T. Elsken, J.Â H. Metzen, and F. Hutter. 2019. Neural architecture
    search: A survey. *J. Mach. Learn. Res.* 20, 55 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang etÂ al. (2020a) M. Fang, N. Damer, Fadi Boutros, F. Kirchbuchner, and A.
    Kuijper. 2020a. Deep Learning Multi-layer Fusion for an Accurate Iris Presentation
    Attack Detection. In *IEEE International Conference on Information Fusion (FUSION)*.
    1â€“8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang etÂ al. (2021b) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021b. Cross-database and cross-attack Iris presentation attack detection using
    micro stripes analyses. *Image and Vision Computing* 105 (2021), 104057.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang etÂ al. (2021c) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021c. Iris Presentation Attack Detection by Attention-based and Deep Pixel-wise
    Binary Supervision Network. In *IEEE Int. Joint Conf. on Biometrics (IJCB)*. 1â€“8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang etÂ al. (2021d) M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper.
    2021d. The overlapping effect and fusion protocols of data augmentation techniques
    in iris PAD. *Machine Vision and Applications* 33 (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang etÂ al. (2020b) M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. 2020b.
    Micro Stripes Analyses for Iris Presentation Attack Detection. In *IEEE Int. Joint
    Conf. on Biometrics (IJCB)*. 1â€“10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang etÂ al. (2021e) M. Fang, N. Damer, F. Kirchbuchner, and A. Kuijper. 2021e.
    Demographic Bias in Presentation Attack Detection of Iris Recognition Systems.
    In *28th European Signal Processing Conference (EUSIPCO)*. 835â€“839.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang and Czajka (2020) Z. Fang and A. Czajka. 2020. Open Source Iris Recognition
    Hardware and Software with Presentation Attack Detection. In *IEEE Int. Joint
    Conf. on Biometrics (IJCB)*. 1â€“8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang etÂ al. (2021a) Z. Fang, A. Czajka, and K. Bowyer. 2021a. Robust Iris Presentation
    Attack Detection Fusing 2D and 3D Information. *IEEE Transactions on Information
    Forensics and Security* 16 (2021), 510â€“520.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FBI Criminal Justice Information Services (CJIS) Division (2021) FBI Criminal
    Justice Information Services (CJIS) Division. 2021. *Next Generation Identification
    (NGI)*. Retrieved 2021 from [https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi](https://www.fbi.gov/services/cjis/fingerprints-and-other-biometrics/ngi)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fierrez etÂ al. (2007) J. Fierrez, J. Ortega-Garcia, D. Torre Toledano, and
    J. Gonzalez-Rodriguez. 2007. Biosec baseline corpus: A multimodal biometric database.
    *Pattern Recognition* 40, 4 (2007), 1389â€“1392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Galbally and Gomez-Barrero (2016) J. Galbally and M. Gomez-Barrero. 2016. A
    review of iris anti-spoofing. In *4th International Conference on Biometrics and
    Forensics (IWBF)*. 1â€“6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ganeeva and Myasnikov (2020) Y. Ganeeva and E. Myasnikov. 2020. Using Convolutional
    Neural Networks for Segmentation of Iris Images. In *International Multi-Conference
    on Industrial Engineering and Modern Technologies (FarEastCon)*. 1â€“4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gangwar and Joshi (2016) A. Gangwar and A. Joshi. 2016. DeepIrisNet: Deep iris
    representation with applications in iris recognition and cross-sensor iris recognition.
    In *IEEE International Conference on Image Processing*. 2301â€“2305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gangwar etÂ al. (2019) A. Gangwar, A. Joshi, P. Joshi, and R. Raghavendra. 2019.
    DeepIrisNet2: Learning Deep-IrisCodes from Scratch for Segmentation-Robust Visible
    Wavelength and Near Infrared Iris Recognition. *CoRR* abs/1902.05390 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garbin etÂ al. (2019) S.Â J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes,
    and S.Â S. Talathi. 2019. OpenEDS: Open Eye Dataset. *CoRR* abs/1905.03702 (2019).
    arXiv:1905.03702'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gautam etÂ al. (2022) G. Gautam, A. Raj, and S. Mukhopadhyay. 2022. Deep supervised
    class encoding for iris presentation attack detection. *Digital Signal Processing*
    121 (2022), 103329.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow etÂ al. (2014) I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.
    Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. 2014. Generative Adversarial
    Nets. In *International Conference on Neural Information Processing Systems (NIPS)*
    (Canada). USA, 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gragnaniello etÂ al. (2016) D. Gragnaniello, C. Sansone, G. Poggi, and L. Verdoliva.
    2016. Biometric Spoofing Detection by a Domain-Aware Convolutional Neural Network.
    In *International Conference on Signal-Image Technology and Internet-Based Systems
    (SITIS)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grother and Tabassi (2007) P. Grother and E. Tabassi. 2007. Performance of Biometric
    Quality Measures. *IEEE Trans. Pattern Anal. Mach. Intell.* 29 (2007), 531â€“543.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta etÂ al. (2021) M. Gupta, S. Singh, A. Agarwal, M. Vatsa, and R. Singh.
    2021. Generalized Iris Presentation Attack Detection Algorithm under Cross-Database
    Settings. In *Int. Conf. on Pattern Recognition (ICPR)*. 5318â€“5325.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta etÂ al. (2014) P. Gupta, S. Behera, M. Vatsa, and R. Singh. 2014. On Iris
    Spoofing Using Print Attack. In *Int. Conf. on Pattern Recognition (ICPR)*. 1681â€“1686.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hafner etÂ al. (2021) A. Hafner, P. Peer, Å½. EmerÅ¡iÄ, and M. Vitek. 2021. Deep
    Iris Feature Extraction. In *International Conference on Artificial Intelligence
    in Information and Communication (ICAIIC)*. 258â€“262.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han (2021) Y. Han. 2021. Design of An Active Infrared Iris Recognition Device.
    In *IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers
    (IPEC)*. 435â€“437.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He etÂ al. (2022) Y. He, G. Meng, K. Chen, X. Hu, and J. He. 2022. Towards Security
    Threats of Deep Learning Systems: A Survey. *IEEE Transactions on Software Engineering*
    48, 5 (2022), 1743â€“1770.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He etÂ al. (2009) Z. He, T. Tan, Z. Sun, and X. Qiu. 2009. Toward Accurate and
    Fast Iris Segmentation for Iris Biometrics. *IEEE Trans. Pattern Anal. Mach. Intell.*
    31, 9 (2009), 1670â€“1684.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hedman etÂ al. (2021) P. Hedman, V. Skepetzis, K. Hernandez-Diaz, J. BigÃ¼n, and
    F. Alonso-Fernandez. 2021. On the Effect of Selfie Beautification Filters on Face
    Detection and Recognition. *CoRR* abs/2110.08934 (2021). arXiv:2110.08934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hernandez-Diaz etÂ al. (2018) K. Hernandez-Diaz, F. Alonso-Fernandez, and J.
    Bigun. 2018. Periocular Recognition Using CNN Features Off-the-Shelf. In *International
    Conference of the Biometrics Special Interest Group (BIOSIG)*. 1â€“5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hernandez-Diaz etÂ al. (2020) K. Hernandez-Diaz, F. Alonso-Fernandez, and J.
    Bigun. 2020. Cross-Spectral Periocular Recognition with Conditional Adversarial
    Networks. In *IEEE Int. Joint Conf. on Biometrics (IJCB)*. 1â€“9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hofbauer etÂ al. (2019) H. Hofbauer, E. Jalilian, and A. Uhl. 2019. Exploiting
    superior CNN-based iris segmentation for better recognition accuracy. *Pattern
    Recognition Letters* 120 (2019), 17â€“23.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Howard etÂ al. (2019) A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M.
    Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q.Â V. Le, and H. Adam. 2019. Searching
    for MobileNetV3\. In *IEEE Int. Conf. on Computer Vision (ICCV)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsiao and Fan (2021) C.-S. Hsiao and C.-P. Fan. 2021. EfficientNet Based Iris
    Biometric Recognition Methods with Pupil Positioning by U-Net. In *3rd International
    Conference on Computer Communication and the Internet (ICCCI)*. 1â€“5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu etÂ al. (2022) L. Hu, J. Li, G. Lin, S. Peng, Z. Zhang, Y. Zhang, and C. Dong.
    2022. Defending against Membership Inference Attacks with High Utility by GAN.
    *IEEE Transactions on Dependable and Secure Computing* (2022), 1â€“1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huynh etÂ al. (2019) V.Â T. Huynh, S.-H. Kim, G.-S. Lee, and H.-J. Yang. 2019.
    Eye Semantic Segmentation with A Lightweight Model. In *IEEE/CVF International
    Conference on Computer Vision Workshop (ICCVW)*. 3694â€“3697.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hwang and Lee (2020) H. Hwang and E.Â C. Lee. 2020. Near-Infrared Image-Based
    Periocular Biometric Method Using Convolutional Neural Network. *IEEE Access*
    8 (2020), 158612â€“158621.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain etÂ al. (2021) A. Jain, D. Deb, and J. Engelsma. 2021. Biometrics: Trust,
    but Verify. *IEEE Transactions on Biometrics, Behavior, and Identity Science*
    (2021), 1â€“1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jalilian etÂ al. (2020) E. Jalilian, M. Karakaya, and A. Uhl. 2020. End-to-end
    Off-angle Iris Recognition Using CNN Based Iris Segmentation. In *International
    Conference of the Biometrics Special Interest Group (BIOSIG)*. 1â€“7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jalilian etÂ al. (2022) E. Jalilian, G. Wimmer, A. Uhl, and M. Karakaya. 2022.
    Deep Learning Based Off-Angle Iris Recognition. In *IEEE Int. Conf. on Acoustics,
    Speech, and Signal Processing (ICASSP)*. 4048â€“4052.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson etÂ al. (2010) P.Â A. Johnson, P. Lopez-Meyer, N. Sazonova, F. Hua, and
    S. Schuckers. 2010. Quality in face and iris research ensemble (Q-FIRE). In *IEEE
    Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)*. 1â€“6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jung etÂ al. (2020) Y.Â G. Jung, C.Â Y. Low, J. Park, and A.Â B.Â J. Teoh. 2020.
    Periocular Recognition in the Wild With Generalized Label Smoothing Regularization.
    *IEEE Signal Processing Letters* 27 (2020), 1455â€“1459.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaur and Manduchi (2020) H. Kaur and R. Manduchi. 2020. EyeGAN: Gazeâ€“Preserving,
    Maskâ€“Mediated Eye Image Synthesis. In *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*. 299â€“308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaur and Manduchi (2021) H. Kaur and R. Manduchi. 2021. Subject Guided Eye Image
    Synthesis with Application to Gaze Redirection. In *IEEE Winter Conference on
    Applications of Computer Vision (WACV)*. 11â€“20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kerrigan etÂ al. (2019) D. Kerrigan, M. Trokielewicz, A. Czajka, and K. Bowyer.
    2019. Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf
    Deep Neural Networks. In *IEEE Int. Conf. on Biometrics (ICB)*. 1â€“7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khalid etÂ al. (2019) F. Khalid, M.Â A. Hanif, S. Rehman, R. Ahmed, and M. Shafique.
    2019. TrISec: Training Data-Unaware Imperceptible Security Attacks on Deep Neural
    Networks. In *IEEE 25th International Symposium on On-Line Testing and Robust
    System Design (IOLTS)*. 188â€“193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khosravy etÂ al. (2022) M. Khosravy, K. Nakamura, Y. Hirose, N. Nitta, and N.
    Babaguchi. 2022. Model Inversion Attack by Integration of Deep Generative Models:
    Privacy-Sensitive Face Generation From a Face Recognition System. *IEEE Transactions
    on Information Forensics and Security* 17 (2022), 357â€“372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim etÂ al. (2018) M.Â C. Kim, J.Â H. Koo, S.Â W. Cho, N.Â R. Baek, and K.Â R. Park.
    2018. Convolutional Neural Network-Based Periocular Recognition in Surveillance
    Environments. *IEEE Access* 6 (2018), 57291â€“57310.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohli etÂ al. (2013) N. Kohli, D. Yadav, M. Vatsa, and R. Singh. 2013. Revisiting
    iris recognition with color cosmetic contact lenses. In *IEEE Int. Conf. on Biometrics
    (ICB)*. 1â€“7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohli etÂ al. (2017) N. Kohli, D. Yadav, M. Vatsa, R. Singh, and A. Noore. 2017.
    Synthetic iris presentation attack using iDCGAN. In *IEEE Int. Joint Conf. on
    Biometrics (IJCB)*. 674â€“680.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuehlkamp etÂ al. (2022) A. Kuehlkamp, A. Boyd, A. Czajka, K. Bowyer, P. Flynn,
    D. Chute, and E. Benjamin. 2022. Interpretable Deep Learning-Based Forensic Iris
    Segmentation and Recognition. In *2nd WACV Workshop on Explainable and Interpretable
    Artificial Intelligence for Biometrics (XAI4B)*. IEEE, USA, 1â€“8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar and Passi (2010) A. Kumar and A. Passi. 2010. Comparison and combination
    of iris matchers for reliable personal authentication. *Pattern Recognition* 43,
    3 (2010), 1016â€“1026.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ledig etÂ al. (2017) C. Ledig, L. Theis, F. HuszÃ¡r, J. Caballero, A. Cunningham,
    A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. 2017. Photo-Realistic
    Single Image Super-Resolution Using a Generative Adversarial Network. In *IEEE
    Int. Conf. on Computer Vision and Pattern Recognition (CVPR)*. 105â€“114.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li etÂ al. (2021) Y.-H. Li, W.Â R. Putri, M.Â S. Aslam, and C.-C. Chang. 2021.
    Robust Iris Segmentation Algorithm in Non-Cooperative Environments Using Interleaved
    Residual U-Net. *Sensors* 21, 4 (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang etÂ al. (2021) T. Liang, J. Glossner, L. Wang, S. Shi, and X. Zhang. 2021.
    Pruning and quantization for deep neural network acceleration: A survey. *Neurocomputing*
    461 (2021), 370â€“403.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin etÂ al. (2016) G. Lin, A. Milan, C. Shen, and I. Reid. 2016. RefineNet:
    Multi-Path Refinement Networks for High-Resolution Semantic Segmentation. arXiv:1611.06612Â [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu etÂ al. (2016b) N. Liu, M. Zhang, H. Li, Z. Sun, and T. Tan. 2016b. DeepIris:
    Learning pairwise filter bank for heterogeneous iris verification. *Pattern Recognition
    Letters* 82 (2016), 154â€“161.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu etÂ al. (2016a) W. Liu, A. Rabinovich, and A. Berg. 2016a. ParseNet: Looking
    Wider to See Better. In *International Conference on Learning Representations
    (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu etÂ al. (2019) X. Liu, Y. Bai, Y. Luo, Z. Yang, and Y. Liu. 2019. Iris recognition
    in visible spectrum based on multi-layer analogous convolution and collaborative
    representation. *Pattern Recognition Letters* 117 (2019), 66â€“73.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long etÂ al. (2015) J. Long, E. Shelhamer, and T. Darrell. 2015. Fully convolutional
    networks for semantic segmentation. In *IEEE Int. Conf. on Computer Vision and
    Pattern Recognition (CVPR)*. 3431â€“3440.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lozej etÂ al. (2018) J. Lozej, B. Meden, V. Struc, and P. Peer. 2018. End-to-End
    Iris Segmentation Using U-Net. In *IEEE International Work Conference on Bioinspired
    Intelligence (IWOBI)*. 1â€“6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo etÂ al. (2021) Z. Luo, J. Li, and Y. Zhu. 2021. A Deep Feature Fusion Network
    Based on Multiple Attention Mechanisms for Joint Iris-Periocular Biometric Recognition.
    *IEEE Signal Processing Letters* 28 (2021), 1060â€“1064.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma etÂ al. (2003) L. Ma, T. Tan, Y. Wang, and D. Zhang. 2003. Personal identification
    based on iris texture analysis. *IEEE Trans. Pattern Anal. Mach. Intell.* 25,
    12 (2003), 1519â€“1533.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menotti etÂ al. (2015) D. Menotti, G. Chiachia, A. Pinto, W.Â R. Schwartz, H.
    Pedrini, A.Â X. FalcÃ£o, and A. Rocha. 2015. Deep Representations for Iris, Face,
    and Fingerprint Spoofing Detection. *IEEE Transactions on Information Forensics
    and Security* 10, 4 (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minaee etÂ al. (2016) S. Minaee, A. Abdolrashidiy, and Y. Wang. 2016. An experimental
    study of deep convolutional features for iris recognition. *IEEE Signal Processing
    in Medicine and Biology Symposium (SPMB)*, 1â€“6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishra etÂ al. (2019) S. Mishra, P. Liang, A. Czajka, DannyÂ Z. Chen, and X.
    Hu. 2019. CC-NET: Image Complexity Guided Network Compression for Biomedical Image
    Segmentation. In *IEEE 16th International Symposium on Biomedical Imaging (ISBI
    2019)*. 57â€“60.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monro etÂ al. (2007) D. Monro, S. Rakshit, and D. Zhang. 2007. DCT-Based Iris
    Recognition. *IEEE Trans. Pattern Anal. Mach. Intell.* 29, 4 (2007), 586â€“595.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moreira etÂ al. (2019) D. Moreira, M. Trokielewicz, A. Czajka, K. Bowyer, and
    P. Flynn. 2019. Performance of Humans in Iris Recognition: The Impact of Iris
    Condition and Annotation-Driven Verification. In *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*. 941â€“949.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mostofa etÂ al. (2021) M. Mostofa, S. Mohamadi, J. Dawson, and N. Nasrabadi.
    2021. Deep GAN-Based Cross-Spectral Cross-Resolution Iris Recognition. *IEEE Transactions
    on Biometrics, Behavior, and Identity Science* 3, 4 (2021), 443â€“463.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muron and Pospisil (2000) A. Muron and J. Pospisil. 2000. The human iris structure
    and its usages. In *Acta Univ Plalcki Physica*. Vol.Â 39\. Acta Universitatis,
    87â€“95.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen etÂ al. (2017a) K. Nguyen, C. Fookes, R. Jillela, S. Sridharan, and A.
    Ross. 2017a. Long range iris recognition: A survey. *Pattern Recognition* 72 (2017),
    123â€“143.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen etÂ al. (2017b) K. Nguyen, C. Fookes, A. Ross, and S. Sridharan. 2017b.
    Iris recognition with Off-the-Shelf CNN Features: A deep learning perspective.
    *IEEE Access* 6 (2017), 18848â€“18855. Invited Paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen etÂ al. (2020) K. Nguyen, C. Fookes, and S. Sridharan. 2020. Constrained
    Design of Deep Iris Networks. *IEEE Transactions on Image Processing* 29 (2020),
    7166â€“7175.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen etÂ al. (2011) K. Nguyen, C. Fookes, S. Sridharan, and S. Denman. 2011.
    Quality-Driven Super-Resolution for Less Constrained Iris Recognition at a Distance
    and on the Move. *IEEE Transactions on Information Forensics and Security* 6,
    4 (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen etÂ al. (2022) K. Nguyen, C. Fookes, S. Sridharan, and A. Ross. 2022.
    Complex-valued Iris Recognition Network. *IEEE Trans. Pattern Anal. Mach. Intell.*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen etÂ al. (2018) K. Nguyen, C. Fookes, S. Sridharan, M. Tistarelli, and
    M. Nixon. 2018. Super-resolution for biometrics: A comprehensive survey. *Pattern
    Recognition* 78 (2018), 23â€“42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen etÂ al. (2012) K. Nguyen, S. Sridharan, S. Denman, and C. Fookes. 2012.
    Feature-domain super-resolution framework for Gabor-based face and iris recognition.
    In *IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)*. 2642â€“2649.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie etÂ al. (2014) L. Nie, A. Kumar, and S. Zhan. 2014. Periocular Recognition
    Using Unsupervised Convolutional RBM Feature Learning. In *Int. Conf. on Pattern
    Recognition (ICPR)*. 399â€“404.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nigam etÂ al. (2015) I. Nigam, M. Vatsa, and R. Singh. 2015. Ocular biometrics:
    A survey of modalities and fusion approaches. *Information Fusion* 26 (2015),
    1â€“35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NIST (2021) NIST. 2021. IEG: Iris Examiner Training Discussion: https://www.nist.gov/itl/iad/image-group/ieg-iris-examiner-training-discussion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Odinokikh etÂ al. (2019) G. Odinokikh, M. Korobkin, I. Solomatin, I. Efimov,
    and A. Fartukov. 2019. Iris Feature Extraction and Matching Method for Mobile
    Biometric Applications. In *IEEE Int. Conf. on Biometrics (ICB)*. 1â€“6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Omelina etÂ al. (2021) L. Omelina, J. Goga, J. Pavlovicova, M. Oravec, and B.
    Jansen. 2021. A survey of iris datasets. *Image and Vision Computing* 108 (2021),
    104109.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ortega etÂ al. (2010) J. Ortega, J. Fierrez, F. Alonso, J. Galbally, M.Â R. Freire,
    J. Gonzalez, C. Garcia, J. Alba, E. Gonzalez-Agulla, E. Otero, S. Garcia-Salicetti,
    L. Allano, B. Ly-Van, B. Dorizzi, J. Kittler, T. Bourlai, N. Poh, F. Deravi, Ming
    N.Â R. Ng, M. Fairhurst, J. Hennebert, A. Humm, M. Tistarelli, L. Brodo, J. Richiardi,
    A. Drygajlo, H. Ganster, F.Â M. Sukno, S. Pavani, A. Frangi, L. Akarun, and A.
    Savran. 2010. The Multiscenario Multienvironment BioSecure Multimodal Database
    (BMDB). *IEEE Trans. Pattern Anal. Mach. Intell.* 32, 6 (2010), 1097â€“1111.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park etÂ al. (2011) U. Park, R. Jillela, A. Ross, and A. Jain. 2011. Periocular
    Biometrics in the Visible Spectrum. *IEEE Transactions on Information Forensics
    and Security* 6, 1 (2011), 96â€“106.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parzianello and Czajka (2022) L. Parzianello and A. Czajka. 2022. Saliency-Guided
    Textured Contact Lens-Aware Iris Recognition. In *IEEE Workshop on Applications
    of Computer Vision (WACV)*. 330â€“337.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng etÂ al. (2020) H. Peng, B. Li, D. He, and J. Wang. 2020. End-to-End Anti-Attack
    Iris Location Based on Lightweight Network. In *IEEE International Conference
    on Advances in Electrical Engineering and Computer Applications (AEECA)*. 821â€“827.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phillips etÂ al. (2010) P.Â J. Phillips, W.Â T. Scruggs, A.Â J. Oâ€™Toole, P. Flynn,
    K. Bowyer, C.Â L. Schott, and M. Sharpe. 2010. FRVT 2006 and ICE 2006 Large-Scale
    Experimental Results. *IEEE Trans. Pattern Anal. Mach. Intell.* 32, 5 (2010),
    831â€“846.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planet Biometrics (2017) Planet Biometrics. 2017. Homeland Advanced Recognition
    Technology (HART). [http://www.planetbiometrics.com/article-details/i/5598/desc/dhs-launches-rfp-for-hart/](http://www.planetbiometrics.com/article-details/i/5598/desc/dhs-launches-rfp-for-hart/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ProenÃ§a (2013) H. ProenÃ§a. 2013. *Iris Recognition in the Visible Wavelength*.
    Springer, UK, 151â€“169.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ProenÃ§a and Alexandre (2005) H. ProenÃ§a and L. Alexandre. 2005. UBIRIS: A Noisy
    Iris Image Database. In *Image Analysis and Processing â€“ ICIAP 2005*. Springer,
    Germany, 970â€“977.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Proenca etÂ al. (2010) H. Proenca, S. Filipe, R. Santos, J. Oliveira, and L.
    Alexandre. 2010. The UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured
    On-the-Move and At-a-Distance. *IEEE Trans. Pattern Anal. Mach. Intell.* 32, 8
    (2010), 1529â€“1535.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ProenÃ§a and Neves (2017) H. ProenÃ§a and J.Â C. Neves. 2017. IRINA: Iris Recognition
    (Even) in Inaccurately Segmented Data. In *IEEE Int. Conf. on Computer Vision
    and Pattern Recognition (CVPR)*. 6747â€“6756.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ProenÃ§a (2010) H. ProenÃ§a. 2010. Iris Recognition: On the Segmentation of Degraded
    Images Acquired in the Visible Wavelength. *IEEE Trans. Pattern Anal. Mach. Intell.*
    32, 8 (2010), 1502â€“1516.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ProenÃ§a and Neves (2018) H. ProenÃ§a and J. Neves. 2018. Deep-PRWIS: Periocular
    Recognition Without the Iris and Sclera Using Deep Learning Frameworks. *IEEE
    Transactions on Information Forensics and Security* 13, 4 (2018), 888â€“896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ProenÃ§a and Neves (2019) H. ProenÃ§a and J.Â C. Neves. 2019. Segmentation-Less
    and Non-Holistic Deep-Learning Frameworks for Iris Recognition. In *IEEE Int.
    Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)*. 2296â€“2305.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quinn, G. and Matey, J. and Grother, P. and Watters, E. (2022) Quinn, G. and
    Matey, J. and Grother, P. and Watters, E. 2022. Statistics of Visual Features
    in the Human Iris.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro etÂ al. (2019) E. Ribeiro, A. Uhl, and F. Alonso-Fernandez. 2019. Iris
    super-resolution using CNNs: is photo-realism important to iris recognition? *IET
    Biometrics* 8 (2019), 69â€“78.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ribeiro etÂ al. (2017) E. Ribeiro, A. Uhl, F. Alonso-Fernandez, and R.Â A. Farrugia.
    2017. Exploring deep learning image super-resolution for iris recognition. In
    *25th European Signal Processing Conference (EUSIPCO)*. 2176â€“2180.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger etÂ al. (2015) O. Ronneberger, P.Fischer, and T. Brox. 2015. U-Net:
    Convolutional Networks for Biomedical Image Segmentation. In *Medical Image Computing
    and Computer-Assisted Intervention (MICCAI)* *(LNCS, Vol.Â 9351)*. Springer, 234â€“241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saleh (2018) I. Saleh. 2018. *Internet of Things (IoT): Concepts, Issues, Challenges
    and Perspectives*. 1â€“26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sansola (2015) A. Sansola. 2015. *Postmortem iris recognition and its application
    in human identification*. Masterâ€™sÂ thesis. Boston University, USA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sardar etÂ al. (2020) M. Sardar, S. Banerjee, and S. Mitra. 2020. Iris Segmentation
    Using Interactive Deep Learning. *IEEE Access* 8 (2020), 219322â€“219330.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sauerwein etÂ al. (2017) K. Sauerwein, T.Â B. Saul, D.Â W. Steadman, and C.Â B.
    Boehnen. 2017. The Effect of Decomposition on the Efficacy of Biometrics for Positive
    Identification. *Journal of Forensic Sciences* 62, 6 (2017), 1599â€“1602.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schlett etÂ al. (2018) T. Schlett, C. Rathgeb, and C. Busch. 2018. Multi-spectral
    Iris Segmentation in Visible Wavelengths. In *IEEE Int. Conf. on Biometrics (ICB)*.
    190â€“194.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schroff etÂ al. (2015) F. Schroff, D. Kalenichenko, and J. Philbin. 2015. FaceNet:
    A unified embedding for face recognition and clustering. In *IEEE Int. Conf. on
    Computer Vision and Pattern Recognition (CVPR)*. 815â€“823.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequeira etÂ al. (2014) A. Sequeira, H. Oliveira, J. Monteiro, J. Monteiro, and
    J. Cardoso. 2014. MobILive 2014 - Mobile Iris Liveness Detection Competition.
    In *IEEE Int. Joint Conf. on Biometrics (IJCB)*. 1â€“6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shah and Ross (2009) S. Shah and A. Ross. 2009. Iris Segmentation Using Geodesic
    Active Contours. *IEEE Transactions on Information Forensics and Security* 4,
    4 (2009), 824â€“836.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma etÂ al. (2014) A. Sharma, S. Verma, M. Vatsa, and R. Singh. 2014. On cross
    spectral periocular recognition. In *Int. IEEE Int. Conf. on Image Processing*.
    5007â€“5011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma and Selwal (2021) D. Sharma and A. Selwal. 2021. On Data-Driven Approaches
    for Presentation Attack Detection in Iris Recognition Systems. In *Recent Innovations
    in Computing*. Springer, Singapore, 463â€“473.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma and Ross (2020) R. Sharma and A. Ross. 2020. D-NetPAD: An Explainable
    and Interpretable Iris Presentation Attack Detector. In *IEEE Int. Joint Conf.
    on Biometrics (IJCB)*. 1â€“10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma and Ross (2021) R. Sharma and A. Ross. 2021. Viability of Optical Coherence
    Tomography for Iris Presentation Attack Detection. In *Int. Conf. on Pattern Recognition
    (ICPR)*. 6165â€“6172.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen and Flynn (2013) F. Shen and P. Flynn. 2013. Using crypts as iris minutiae.
    In *Biometric and Surveillance Technology for Human and Activity Identification
    X*, Vol.Â 8712. 87120B.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song etÂ al. (2019) L. Song, R. Shokri, and P. Mittal. 2019. Membership Inference
    Attacks Against Adversarially Robust Deep Learning Models. In *IEEE Security and
    Privacy Workshops (SPW)*. 50â€“56.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stark etÂ al. (2010) L. Stark, K. Bowyer, and S. Siena. 2010. Human perceptual
    categorization of iris texture patterns. In *IEEE Int. Conf. on Biometrics: Theory
    Applications and Systems (BTAS)*. 1â€“7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun etÂ al. (2014) Z. Sun, H. Zhang, T. Tan, and J. Wang. 2014. Iris Image Classification
    Based on Hierarchical Visual Codebook. *IEEE Trans. Pattern Anal. Mach. Intell.*
    36, 6 (2014), 1120â€“1133.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sunder and Ross (2010) M.Â S. Sunder and A. Ross. 2010. Iris Image Retrieval
    Based on Macro-features. In *Int. Conf. on Pattern Recognition (ICPR)*. 1318â€“1321.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabassi etÂ al. (2011) E. Tabassi, P. Grother, and W. Salamon. 2011. IREX II
    - IQCE - Iris Quality Calibration and Evaluation. Performance of Iris Image Quality
    Assessment Algorithms. *NISTIR 7296 - http://iris.nist.gov/irex/* (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talreja etÂ al. (2022) V. Talreja, N.Â M. Nasrabadi, and M.Â C. Valenti. 2022.
    Attribute-Based Deep Periocular Recognition: Leveraging Soft Biometrics to Improve
    Periocular Recognition. In *IEEE Winter Conference on Applications of Computer
    Vision (WACV)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan and Kumar (2012) C.-W. Tan and A. Kumar. 2012. Unified Framework for Automated
    Iris Segmentation Using Distantly Acquired Face Images. *IEEE Transactions on
    Image Processing* 21, 9 (2012), 4068â€“4079.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan etÂ al. (2010) T. Tan, Zh. He, and Z. Sun. 2010. Efficient and robust segmentation
    of noisy iris images for non-cooperative iris recognition. *Image and Vision Computing*
    28, 2 (2010), 223â€“230.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tann etÂ al. (2019) H. Tann, H. Zhao, and S. Reda. 2019. A Resource-Efficient
    Embedded Iris Recognition System Using Fully Convolutional Networks. *ACM Journal
    on Emerging Technologies in Computing Systems* 16, 1 (2019), 1â€“23.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapia etÂ al. (2022) J. Tapia, S. Gonzalez, and C. Busch. 2022. Iris Liveness
    Detection Using a Cascade of Dedicated Deep Learning Networks. *IEEE Transactions
    on Information Forensics and Security* 17 (2022), 42â€“52.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: technology â€” Biometric presentation attackÂ detection â€” Part 1:Â Framework (2016)
    ISO/IEC 30107-1.Â Information technology â€” Biometric presentation attackÂ detection
    â€” Part 1:Â Framework. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'technology â€” Biometric sample quality â€” Part 6: IrisÂ image data (2015) ISO/IEC
    29794-6.Â Information technology â€” Biometric sample quality â€” Part 6: IrisÂ image
    data. 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tinsley etÂ al. (2021) P. Tinsley, A. Czajka, and P. Flynn. 2021. This Face Does
    Not Existâ€¦ But It Might Be Yours! Identity Leakage in Generative Models. In *IEEE
    Winter Conference on Applications of Computer Vision (WACV)*. 1319â€“1327.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trokielewicz and Czajka (2018) M. Trokielewicz and A. Czajka. 2018. Data-driven
    segmentation of post-mortem iris images. In *International Workshop on Biometrics
    and Forensics (IWBF)*. IEEE, Italy, 1â€“7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trokielewicz etÂ al. (2016a) M. Trokielewicz, A. Czajka, and P. Maciejewicz.
    2016a. Human iris recognition in post-mortem subjects: Study and database. In
    *IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)*. IEEE,
    USA, 1â€“6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trokielewicz etÂ al. (2016b) M. Trokielewicz, A. Czajka, and P. Maciejewicz.
    2016b. Post-mortem human iris recognition. In *IEEE Int. Conf. on Biometrics (ICB)*.
    IEEE, Sweden, 1â€“6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trokielewicz etÂ al. (2018) M. Trokielewicz, A. Czajka, and P. Maciejewicz.
    2018. Presentation Attack Detection for Cadaver Iris. In *IEEE Int. Conf. on Biometrics:
    Theory Applications and Systems (BTAS)*. 1â€“10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trokielewicz etÂ al. (2019) M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2019.
    Iris Recognition After Death. *IEEE Transactions on Information Forensics and
    Security* 14, 6 (June 2019), 1501â€“1514.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trokielewicz etÂ al. (2020) M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2020.
    Post-Mortem Iris Recognition Resistant to Biological Eye Decay Processes. In *IEEE
    Winter Conference on Applications of Computer Vision (WACV)*. IEEE, USA, 1â€“8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trokielewicz etÂ al. (2020) M. Trokielewicz, A. Czajka, and P. Maciejewicz. 2020.
    Post-mortem iris recognition with deep-learning-based image segmentation. *Image
    and Vision Computing* 94 (2020), 103866.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tygert etÂ al. (2016) M. Tygert, J. Bruna, S. Chintala, Y. LeCun, S. Piantino,
    and A. Szlam. 2016. A Mathematical Motivation for Complex-valued Convolutional
    Networks. *Neural Computation* 28 (2016), 815â€“825.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unique Identification Authority of India (2021) Unique Identification Authority
    of India. 2021. AADHAAR: http://uidai.gov.in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vatsa etÂ al. (2008) M. Vatsa, R. Singh, and A. Noore. 2008. Improving Iris Recognition
    Performance Using Segmentation, Quality Enhancement, Match Score Fusion, and Indexing.
    *IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)* 38,
    4 (2008), 1021â€“1035.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2020a) C. Wang, J. Muhammad, Y. Wang, Z. He, and Z. Sun. 2020a.
    Towards Complete and Accurate Iris Segmentation Using Deep Multi-Task Attention
    Network for Non-Cooperative Iris Recognition. *IEEE Transactions on Information
    Forensics and Security* 15 (2020), 2944â€“2959.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Sun (2020) C. Wang and Z. Sun. 2020. A Benchmark for Iris Segmentation.
    *Journal of Computer Research and Development* 57, 2 (2020), 395â€“412.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2020b) C. Wang, Y. Wang, B. Xu, Y. He, Z. Dong, and Z. Sun. 2020b.
    A Lightweight Multi-Label Segmentation Network for Mobile Iris Biometrics. In
    *IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP)*. 1006â€“1010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2019b) C. Wang, Y. Zhu, Y. Liu, R. He, and Z. Sun. 2019b. Joint
    Iris Segmentation and Localization Using Deep Multi-task Learning Framework. arXiv:1901.11195Â [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Kumar (2019) K. Wang and A. Kumar. 2019. Toward More Accurate Iris
    Recognition Using Dilated Residual Features. *IEEE Transactions on Information
    Forensics and Security* 14, 12 (2019), 3233â€“3245.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Kumar (2021) K. Wang and A. Kumar. 2021. Periocular-Assisted Multi-Feature
    Collaboration for Dynamic Iris Recognition. *IEEE Transactions on Information
    Forensics and Security* 16 (2021), 866â€“879.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2020c) L. Wang, K. Zhang, M. Ren, Y. Wang, and Z. Sun. 2020c. Recognition
    Oriented Iris Image Quality Assessment in the Feature Space. In *IEEE Int. Joint
    Conf. on Biometrics (IJCB)*. 1â€“9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2019a) X. Wang, H. Zhang, J. Liu, L. Xiao, Z. He, L. Liu, and P.
    Duan. 2019a. Iris Image Super Resolution Based on GANs with Adversarial Triplets.
    In *Chinese Conference on Biometric Recognition (LNCS)*. Switzerland, 346 â€“ 53.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2021) Z. Wang, J. Chai, and S. Xia. 2021. Realtime and Accurate
    3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation. *IEEE Transactions
    on Visualization and Computer Graphics* 27, 1 (2021), 190â€“203.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei etÂ al. (2007) Z. Wei, T. Tan, and Z. Sun. 2007. Nonlinear Iris Deformation
    Correction Based on Gaussian Model. In *Advances in Biometrics*. Springer, Germany,
    780â€“789.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu and Zhao (2019) X. Wu and L. Zhao. 2019. Study on Iris Segmentation Algorithm
    Based on Dense U-Net. *IEEE Access* 7 (2019), 123959â€“123968.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao etÂ al. (2013) L. Xiao, Z. Sun, R. He, and T. Tan. 2013. Coupled feature
    selection for cross-sensor iris recognition. In *IEEE Int. Conf. on Biometrics:
    Theory Applications and Systems (BTAS)*. 1â€“6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Y. and K. (2016) Fisher Y. and Vladlen K. 2016. Multi-Scale Context Aggregation
    by Dilated Convolutions. arXiv:1511.07122Â [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yadav etÂ al. (2019b) D. Yadav, N. Kohli, M. Vatsa, R. Singh, and A. Noore. 2019b.
    Detecting Textured Contact Lens in Uncontrolled Environment Using DensePAD. In
    *IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yadav etÂ al. (2018) D. Yadav, N. Kohli, S. Yadav, M. Vatsa, R. Singh, and A.
    Noore. 2018. Iris Presentation Attack via Textured Contact Lens in Unconstrained
    Environment. In *IEEE Winter Conference on Applications of Computer Vision (WACV)*.
    503â€“511.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yadav etÂ al. (2019a) S. Yadav, C. Chen, and A. Ross. 2019a. Synthesizing Iris
    Images Using RaSGAN With Application in Presentation Attack Detection. In *IEEE
    Int. Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)*. 2422â€“2430.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yadav etÂ al. (2020) S. Yadav, C. Chen, and A. Ross. 2020. Relativistic Discriminator:
    A One-Class Classifier for Generalized Iris Presentation Attack Detection. In
    *IEEE Winter Conference on Applications of Computer Vision (WACV)*. 2624â€“2633.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yadav and Ross (2021) S. Yadav and A. Ross. 2021. CIT-GAN: Cyclic Image Translation
    Generative Adversarial Network With Application in Iris Presentation Attack Detection.
    In *IEEE Winter Conference on Applications of Computer Vision (WACV)*. 2411â€“2420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yambay etÂ al. (2017) D. Yambay, B. Becker, N. Kohli, D. Yadav, A. Czajka, K.
    Bowyer, S. Schuckers, R. Singh, M. Vatsa, A. Noore, D. Gragnaniello, C. Sansone,
    L. Verdoliva, L. He, Y. Ru, H. Li, N. Liu, Z. Sun, and T. Tan. 2017. LivDet iris
    2017 â€” Iris liveness detection competition 2017\. In *IEEE Int. Joint Conf. on
    Biometrics (IJCB)*. 733â€“741.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan etÂ al. (2021) Z. Yan, L. He, Y. Wang, Z. Sun, and T. Tan. 2021. Flexible
    Iris Matching Based on Spatial Feature Reconstruction. *IEEE Transactions on Biometrics,
    Behavior, and Identity Science* (2021), 1â€“1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang etÂ al. (2021) K. Yang, Z. Xu, and J. Fei. 2021. DualSANet: Dual Spatial
    Attention Network for Iris Recognition. In *IEEE Winter Conference on Applications
    of Computer Vision (WACV)*. 888â€“896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan etÂ al. (2020) Y. Yuan, W. Chen, Y. Yang, and Z. Wang. 2020. In Defense
    of the Triplet Loss Again: Learning Robust Person Re-Identification with Fast
    Approximated Triplet Loss and Label Distillation. In *IEEE Int. Conf. on Computer
    Vision and Pattern Recognition Workshops (CVPRW)*. 1454â€“1463.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zanlorensi etÂ al. (2020) L.Â A. Zanlorensi, H. ProenÃ§a, and D. Menotti. 2020.
    Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks
    for Attribute Normalization. In *Int. IEEE Int. Conf. on Image Processing*. 1361â€“1365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2021) H. Zhang, Y. Bai, H. Zhang, J. Liu, X. Li, and Z. He. 2021.
    Local Attention and Global Representation Collaborating for Fine-grained Classification.
    In *Int. Conf. on Pattern Recognition (ICPR)*. 10658â€“10665.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2010) H. Zhang, Z. Sun, and T. Tan. 2010. Contact Lens Detection
    Based on Weighted LBP. In *Int. Conf. on Pattern Recognition (ICPR)*. 4279â€“4282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2016a) Q. Zhang, H. Li, Z. He, and Z. Sun. 2016a. Image Super-Resolution
    for Mobile Iris Recognition. In *Biometric Recognition*. Springer, Cham, 399â€“406.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2016b) Q. Zhang, H. Li, Z. Sun, Z. He, and T. Tan. 2016b. Exploring
    complementary features for iris recognition on mobile devices. In *IEEE Int. Conf.
    on Biometrics (ICB)*. 1â€“8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2018) Q. Zhang, H. Li, Z. Sun, and T. Tan. 2018. Deep Feature
    Fusion for Iris and Periocular Biometrics on Mobile Devices. *IEEE Transactions
    on Information Forensics and Security* 13, 11 (2018), 2897â€“2912.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2015) Q. Zhang, H. Li, M. Zhang, Z. He, Z. Sun, and T. Tan. 2015.
    Fusion of Face and Iris Biometrics on Mobile Devices Using Near-infrared Images.
    In *Biometric Recognition*. Springer, Cham, 569â€“578.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2019) W. Zhang, X. Lu, Y. Gu, Y. Liu, X. Meng, and J. Li. 2019.
    A Robust Iris Segmentation Scheme Based on Improved U-Net. *IEEE Access* 7 (2019),
    85082â€“85089.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao etÂ al. (2017) H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. 2017. Pyramid
    Scene Parsing Network. In *IEEE Int. Conf. on Computer Vision and Pattern Recognition
    (CVPR)*. 6230â€“6239.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao etÂ al. (2019) T. Zhao, Y. Liu, G. Huo, and X. Zhu. 2019. A Deep Learning
    Iris Recognition Method Based on Capsule Network Architecture. *IEEE Access* 7
    (2019), 49691â€“49701.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao and Kumar (2017a) Z. Zhao and A. Kumar. 2017a. Accurate Periocular Recognition
    Under Less Constrained Environment Using Semantics-Assisted CNN. *IEEE Transactions
    on Information Forensics and Security* 12, 5 (2017), 1017â€“1030.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao and Kumar (2017b) Z. Zhao and A. Kumar. 2017b. Towards More Accurate Iris
    Recognition Using Deeply Learned Spatially Corresponding Features. In *IEEE Int.
    Conf. on Computer Vision (ICCV)*. 3829â€“3838.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao and Kumar (2018) Z. Zhao and A. Kumar. 2018. Improving Periocular Recognition
    by Explicit Attention to Critical Regions in Deep Neural Network. *IEEE Transactions
    on Information Forensics and Security* 13, 12 (2018), 2937â€“2952.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao and Kumar (2019) Z. Zhao and A. Kumar. 2019. A deep learning based unified
    framework to detect, segment and recognize irises using spatially corresponding
    features. *Pattern Recognition* 93 (2019), 546 â€“ 557.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou etÂ al. (2016) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba.
    2016. Learning Deep Features for Discriminative Localization. In *IEEE Int. Conf.
    on Computer Vision and Pattern Recognition (CVPR)*. 2921â€“2929.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
