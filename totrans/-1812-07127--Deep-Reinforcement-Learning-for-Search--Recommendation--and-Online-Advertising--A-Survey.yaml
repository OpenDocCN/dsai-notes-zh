- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:06:52'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1812.07127] Deep Reinforcement Learning for Search, Recommendation, and Online
    Advertising: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1812.07127](https://ar5iv.labs.arxiv.org/html/1812.07127)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \newsletterQuarter
  prefs: []
  type: TYPE_NORMAL
- en: Spring \newsletterYear2019
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Reinforcement Learning for Search, Recommendation, and Online Advertising:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xiangyu Zhao    Michigan State University
  prefs: []
  type: TYPE_NORMAL
- en: Long Xia    JD.com
  prefs: []
  type: TYPE_NORMAL
- en: Jiliang Tang    Michigan State University
  prefs: []
  type: TYPE_NORMAL
- en: Dawei Yin    JD.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Search, recommendation, and online advertising are the three most important
    information-providing mechanisms on the web. These information seeking techniques,
    satisfying users’ information needs by suggesting users personalized objects (information
    or services) at the appropriate time and place, play a crucial role in mitigating
    the information overload problem. With recent great advances in deep reinforcement
    learning (DRL), there have been increasing interests in developing DRL based information
    seeking techniques. These DRL based techniques have two key advantages – (1) they
    are able to continuously update information seeking strategies according to users’
    real-time feedback, and (2) they can maximize the expected cumulative long-term
    reward from users where reward has different definitions according to information
    seeking applications such as click-through rate, revenue, user satisfaction and
    engagement. In this paper, we give an overview of deep reinforcement learning
    for search, recommendation, and online advertising from methodologies to applications,
    review representative algorithms, and discuss some appealing research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The explosive growth of the World Wide Web has generated massive data. As a
    consequence, the information overload problem has become progressively severe [[Chang
    et al. (2006)](#bib.bib14)]. Thus, how to identify objects that satisfy users’
    information needs at the appropriate time and place has become increasingly important,
    which has motivated three representative information seeking mechanisms – search,
    recommendation, and online advertising. The search mechanism outputs objects that
    match the query, the recommendation mechanism generates a set of items that match
    users’ implicit preferences, and the online advertising mechanism is analogous
    to search and recommendation expect that the objects to be presented are advertisements [[Garcia-Molina
    et al. (2011)](#bib.bib24)]. Numerous efforts have been made on designing intelligent
    methods for these three information seeking mechanisms. However, traditional techniques
    often face several common challenges. First, the majority of existing methods
    consider information seeking as a static task and generate objects following a
    fixed greedy strategy. This may fail to capture the dynamic nature of users’ preferences
    (or environment). Second, most traditional methods are developed to maximize the
    short-term reward, while completely neglecting whether the suggested objects will
    contribute more in long-term reward [[Shani et al. (2005)](#bib.bib60)]. Note
    that the reward has different definitions among information seeking tasks, such
    as click-through rate (CTR), revenue, and dwell time.
  prefs: []
  type: TYPE_NORMAL
- en: Recent years have witnessed the rapid development of reinforcement learning
    (RL) techniques and a wide range of RL based applications. Under the RL schema,
    we tackle complex problems by acquiring experiences through interactions with
    a dynamic environment. The result is an optimal policy that can provide solutions
    to complex tasks without any specific instructions [[Kaelbling et al. (1996)](#bib.bib30)].
    Employing RL for information seeking can naturally resolve the aforementioned
    challenges. First, considering the information seeking tasks as sequential interactions
    between an RL agent (system) and users (environment), the agent can continuously
    update its strategies according to users’ real-time feedback during the interactions,
    until the system converges to the optimal policy that generates objects best match
    users’ dynamic preferences. Second, the RL frameworks are designed to maximize
    the long-term cumulative reward from users. Therefore, the agent could identify
    objects with small immediate reward but making big contributions to the reward
    in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: Given the advantages of reinforcement learning, there have been tremendous interests
    in developing RL based information seeking techniques. Thus, it is timely and
    necessary to provide an overview of information seeking techniques from a reinforcement
    learning perspective. In this survey, we present a comprehensive overview of state-of-the-art
    RL based information seeking techniques and discuss some future directions. The
    remaining of the survey is organized as follows. In Section 2, we introduce technical
    foundations of reinforcement learning based information seeking techniques. Then
    we review the three key information seeking tasks – search, recommendation, and
    online advertising – with representative algorithms from Sections 3 to 5\. Finally,
    we conclude the work with several future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Technical Foundations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning is learning how to map situations to actions [[Sutton
    and Barto (1998)](#bib.bib65)]. The two fundamental elements in RL are to formulate
    the situations (mathematical models) and to learn the mapping (policy learning).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In reinforcement learning, there two main settings for problem formulations:
    multi-armed bandits (without state transition) and Markov decision processes (with
    state transition).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Multi-Armed Bandits
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Multi-Armed Bandits (MABs) problem is a simple model for the exploration/exploitation
    trade-off [[Varaiya and Walrand (1983)](#bib.bib68)]. Formally, a $K$-MAB can
    be defined as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A $K$-MAB is a 3-tuple $\langle A,R,\pi\rangle$, where $A$ is the set of actions (arms)
    and $|A|=K$, $r=R(a)$ is the reward distribution when performing action $a$, and
    policy $\pi$ describes probability distribution over the possible actions.
  prefs: []
  type: TYPE_NORMAL
- en: An arm with the highest expected reward is called the best arm (denoted as $a_{*}$) and
    its expected reward $r_{*}$ is the optimal reward. An algorithm for MAB, at each
    time step $t$, samples an arm $a_{t}$ and receives a reward $r_{t}$. When making
    its selection, the algorithm depends on the history (i.e., actions and rewards)
    up to the time $t$-$1$. The contextual bandit model (a.k.a. associative bandits
    or bandits with side information) is an extension of MAB that takes additional
    information into account [[Auer et al. (2002)](#bib.bib2), [Lu et al. (2010)](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Markov Decision Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A Markov decision process (MDP) is a classical formalization of sequential decision
    making, which is a mathematically idealized form of reinforcement learning problem [[Bellman
    (2013)](#bib.bib5)]. We define an MDP as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A Markov Decision Process is a 5-tuple $\langle S,A,T,R,\pi\rangle$, where $S$
    is a set of states, $A$ is a discrete set of actions, $T$ is the state transition
    function $s_{t+1}=T(s_{t},a_{t})$ which specifies a function mapping a state $s_{t}$
    into a new state $s_{t+1}$ in response to the selected action $a_{t}$, $r=R(s,a)$
    is the reward distribution when performing action $a$ in state $s$, and policy
    $\pi(a|s)$ describes the behaviors of an agent which is a probability distribution
    over the possible actions.
  prefs: []
  type: TYPE_NORMAL
- en: The agent and environment interact at each of a sequence of discrete time steps
    $t=\{0,1,2,\dots\}$. Consequently, a sequence or trajectory is generated as $\{s_{0},a_{0},r_{1},\cdots,s_{t},a_{t},$
  prefs: []
  type: TYPE_NORMAL
- en: '$r_{t+1},\cdots\}$. In general, we seek to maximize the expected discounted
    return, where the return $G_{t}$ is defined as: $G_{t}=\sum_{k=0}^{\infty}\gamma^{k}r_{r+k+1}$,
    where $\gamma$ ($0\leq\gamma\leq 1$) is the discounted rate. The Partially Observable
    Markov Decision Process (POMDP) is an extension of MDP to the case where the state
    of the system is not necessarily observable [[Åström (1965)](#bib.bib1), [Smallwood
    and Sondik (1973)](#bib.bib62), [Sondik (1978)](#bib.bib63), [Kaelbling et al.
    (1998)](#bib.bib29)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Multi-Agent Setting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The generalization of the Markov Decision Process to the Multi-Agent case is
    the stochastic game [[Bowling and Veloso (2002)](#bib.bib8), [Shoham et al. (2003)](#bib.bib61),
    [Busoniu et al. (2008)](#bib.bib10)] as:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2.3.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A multi-agent game is a tuple $\langle S,A_{1},\dots,A_{n},T,R_{1},\dots,R_{n},\pi_{1},\dots,\pi_{n}\rangle$,
    where $n$ is the number of agents, $S$ is the discrete set of environment states,
    $A_{i}$ is the discrete set of actions for the agent $i$, $T$ is the state transition
    probability function, $R_{i}$ is the reward function of agent $i$, and $\pi_{i}$
    is the policy adopted by agent $i$.
  prefs: []
  type: TYPE_NORMAL
- en: In the multi-agent game, the state transition is the result of the joint actions
    of all the agents $\mathbf{a}_{t}=[a_{1,t}^{T},\dots,a_{n,t}^{T}]^{T}$, where
    $a_{i,t}\in A_{i}$ denotes the action taken by agent $i$ at time step $t$. The
    reward $r_{i,k+1}$ also depends on the joint action. If $\pi_{1}=\cdots=\pi_{n}$,
    i.e., all the agents adopt the same policy to maximize the same expected return,
    the multi-agent game is fully cooperative. If $n=2$ and $\pi_{1}=-\pi_{2}$, i.e.,
    the two agents have opposite policies, the game is fully competitive. Mixed games
    are stochastic games that are neither fully cooperative nor fully competitive.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Policy Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reinforcement Learning is a class of learning problems in which the goal of
    an agent (or multi-agent) to find the policy to optimize some measures of its
    long-term performance. RL solutions can be categorized in different ways. Here
    we investigate them from two perspectives: whether the full model is available
    and the way of finding the optimal policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Model-based v.s. Model-free
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reinforcement learning algorithms, which explicitly learn system models and
    use them to solve MDP problems, are model-based methods. Model-based RL has a
    strong influence from the control theory and is often explained in terms of different
    disciplines. These methods include popular algorithms such as the Dyna [[Sutton
    (1991)](#bib.bib64)], Prioritized Sweeping [[Moore and Atkeson (1993)](#bib.bib48)],
    Q-iteration [[Busoniu et al. (2010)](#bib.bib9)], Policy Gradient (PG) [[Williams
    (1992)](#bib.bib75)], and the variation of PG [[Baxter and Bartlett (2001)](#bib.bib4),
    [Kakade (2001)](#bib.bib31)]. The model-free methods ignore the model and just
    focus on figuring out the value functions directly from the interaction with the
    environment. To accomplish this, the methods depend on sampling and observation
    heavily; thus they don’t need to know the inner working of the system. Some examples
    of these methods are Q-learning [[Kröse (1995)](#bib.bib36)], SARSA [[Rummery
    and Niranjan (1994)](#bib.bib55)], LSPI [[Lagoudakis and Parr (2003)](#bib.bib39)],
    and Actor-Critic [[Konda and Tsitsiklis (1999)](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Value function v.s. Policy search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The algorithms, which first find the optimal value functions and then extract
    optimal policies, are value function methods, such as Dyna, Q-learning, SARSA,
    and DQN [[Mnih et al. (2015)](#bib.bib47)]. The alternative approaches are policy
    search methods which solve an MDP problem by directly searching in the space of
    policies. An important class of policy search methods is that of Policy Gradient (PG)
    algorithms [[Williams (1992)](#bib.bib75), [Baxter and Bartlett (2001)](#bib.bib4),
    [Kakade (2001)](#bib.bib31), [Deisenroth and Rasmussen (2011)](#bib.bib19)]. These
    methods target at modeling and optimizing the policy directly. The policy is usually
    modeled with a parameterized function with respect to $\pi_{\theta}(a|s)$. The
    value of the reward (objective) function depends on this policy and then various
    algorithms can be applied to optimize $\theta$ for the best reward. There are
    a series of algorithms, which use the PG to search in the policy space, and at
    the same time estimate a value function. The important class of these methods
    are Actor-Critic (AC) and its variation [[Konda and Tsitsiklis (1999)](#bib.bib35),
    [Peters et al. (2005)](#bib.bib53), [Peters and Schaal (2008)](#bib.bib52), [Bhatnagar
    et al. (2007)](#bib.bib6), [Bhatnagar et al. (2009)](#bib.bib7)]. These are two-time-scale
    algorithms where the critic uses Temporal-Difference (TD) learning with a linear
    approximation architecture and the actor is updated in an approximate gradient
    direction based on information provided by the critic.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Reinforcement Learning for Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Search aims to find and rank a set of objects (e.g., documents, records) based
    on a user query [[(86)](#bib.bib86)]. In this section, we review RL applications
    in key topics of search.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Query Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Query understanding is the primary task for the search engine to understand
    users’ information needs. It can be potentially useful for improving general search
    relevance, user experience, and helping users to accomplish tasks [[Croft et al.
    (2010)](#bib.bib18)]. In [[Nogueira and Cho (2017)](#bib.bib50)], RL has been
    leveraged to solve the query reformulation task: a query reformulation framework
    is proposed based on a neural network, which rewrites a query to maximize the
    number of relevant documents returned. In the proposed framework, a search engine
    is treated as a black box that an agent learns to use in order to retrieve more
    relevant items, which opens the possibility of training an agent to use a search
    engine for a task other than the one it was originally intended for. Additionally,
    the upper-bound performance of an RL-based model is estimated in a given environment.
    In [[Nogueira et al. (2018)](#bib.bib49)], a multi-agent based method is introduced
    to efficiently learn diverse query reformulation. It is argued that it is easier
    to train multiple sub-agents than a single generalist one since each sub-agent
    only needs to learn a policy that performs well for a subset of examples. In the
    proposed framework, an agent consists of multiple specialized sub-agents and a
    meta-agent that learns to aggregate the answers from sub-agents to produce a final
    answer. Thus, the method makes learning faster with parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Ranking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Relevance Ranking is the core problem of information retrieval [[Yin et al.
    (2016)](#bib.bib87)] and learning to rank (LTR) is the key technology in relevance
    ranking. In LTR, the approaches to directly optimize the ranking evaluation measures
    are representative and have been proved to be effective[[Yue et al. (2007)](#bib.bib89),
    [Xu and Li (2007)](#bib.bib81), [Xu et al. (2008)](#bib.bib82)]. These methods
    usually only optimize the evaluation measure calculated at a predefined ranking
    position, e.g. NDCG at rank $K$ in [[Xu and Li (2007)](#bib.bib81)]. The information
    carried by the documents after the rank $K$ are neglected. To solve such problem,
    in [[Zeng et al. (2017)](#bib.bib91)], an LTR model, MDPRank, is proposed based
    on Markov decision process, which has the ability to leverage the measures calculated
    at all of the ranking positions. The reward function is defined based upon the
    IR evaluation measures and the model parameters can be learned through maximizing
    the accumulated rewards to all of the decisions. Implicit relevance feedback refers
    to an interactive process between search engine and user, and has been proven
    to be very effective for improving retrieval accuracy [[Lv and Zhai (2009)](#bib.bib46)].
    Both Bandits and MDPs can model such an interactive process naturally [[Vorobev
    et al. (2015)](#bib.bib69), [Katariya et al. (2016)](#bib.bib34), [Katariya et al.
    (2017)](#bib.bib33)]. In [[Kveton et al. (2015)](#bib.bib37)], cascading bandits
    are introduced to identify the most attractive items, and the goal of the agent
    is to maximize its total reward with respect to the list of the most attractive
    items. Through maintaining state transition, MDP is able to model the user state
    in the interaction with search engine. In [[Zeng et al. (2018)](#bib.bib92)],
    the interactive process is formulated as an MDP and the Recurrent Neural Network
    is applied to process the feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond relevance ranking, another important goal is to provide search results
    that cover a wide range of topics for a query, i.e., search result diversification [[Santos
    et al. (2015)](#bib.bib57), [Xu et al. (2017)](#bib.bib83)]. Typical methods formulate
    the problem of constructing a diverse ranking as a process of greedy sequential
    document selection. To select an optimal document for a position, it is critical
    for a diverse ranking model to capture the utility of information users have perceived
    from the preceding documents. To explicitly model the utility perceived by the
    users, the construction of a diverse ranking is formalized as a process of sequential
    decision making and the process is modeled as a continuous state Markov decision
    process, referred to as MDP-DIV  [[Xia et al. (2017)](#bib.bib80)]. The ranking
    of $M$ documents is formalized as a sequence of $M$ decisions and each action
    corresponds to selecting one document from the candidate set. In the parameter
    training phase, the policy gradient algorithm of REINFORCE is adopted and the
    expected long-term discounted rewards in terms of the diversity evaluation measure
    is maximized. More works for diversity ranking see [[Feng et al. (2018)](#bib.bib23),
    [Kapoor et al. (2018)](#bib.bib32)]
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Whole-Page Optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To improve user experiences, modern search engines aggregate versatile results
    from different verticals – web-pages, news, images, video, shopping, knowledge
    cards, local maps, etc. Page presentation is broadly defined as the strategy to
    present a set of items on search result page (SERP), which is much more expressive
    than a ranked list. Finding proper presentation for a gallery of heterogeneous
    results is critical for modern search engines. One approach of efficiently learning
    to optimize a large decision space is fractional factorial design. However, the
    method could cause a combinatorial explosion problem with a large search space.
    In [[Hill et al. (2017)](#bib.bib26)], bandit formulation is applied to explore
    the layout space efficiently and hill-climbing is used to select optimal content
    in real-time. The model avoids a combinatorial explosion in model complexity by
    only considering pairwise interactions between page components. This approach
    is a greedy alternating optimization strategy that can run online in real-time.
    In [[Wang et al. (2016)](#bib.bib72), [Wang et al. (2018)](#bib.bib73)], a framework
    is proposed to learn the optimal page presentation to render heterogeneous results
    onto SERP. It leveraged the MDP setting and the agent is designed as the algorithm
    that determines the presentation of page content on a SERP for each incoming search
    query. To solve the critical efficiency problem, it proposed a policy-based learning
    method which can rapidly choose actions from the high-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Session Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The task-oriented search includes a series of search iterations triggered by
    the query reformulations within a session. Markov chain in session search is observed:
    user’s judgment of search results in the prior iteration will influence user’s
    behaviors in the next search iteration. Session search is modeled as a dual-agent
    stochastic game based on Partially Observable Markov Decision Process (POMDP)
    in [[Luo et al. (2014)](#bib.bib45)]. They mathematically model dynamics in session
    search as a cooperative game between the user and the search engine, while user
    and the search engine work together in order to jointly maximize the long-term
    cumulative rewards. Log-based document re-ranking is a special type of session
    search that re-ranks documents based on the historical search logs which includes
    the target user’s personalized query log and other users’ search activities. The
    re-ranking aims to offer a better order of the initial retrieved documents [[Zhang
    et al. (2014)](#bib.bib93)]. Nowadays, deep reinforcement learning technology
    has been applied in the E-Commerce search engine [[Hu et al. (2018)](#bib.bib27),
    [Feng et al. (2018)](#bib.bib22)]. For better utilizing the correlation between
    different ranking steps, RL is used to learn an optimal ranking policy which maximizes
    the expected accumulative rewards in a search session [[Hu et al. (2018)](#bib.bib27)].
    It formally defined the multi-step ranking problem in the search session as MDP,
    denoted as SSMDP, and proposed a novel policy gradient algorithm for learning
    an optimal ranking policy, which is able to deal with the problem of high reward
    variance and unbalanced reward distribution. In [[Feng et al. (2018)](#bib.bib22)],
    multi-scenario ranking is formulated as a fully cooperative, partially observable,
    multi-agent sequential decision problem, denoted as MA-RDPG. MA-RDPG has a communication
    component for passing message, several private agents for making action for ranking,
    and a centralized critic for evaluating the overall performance of the co-working
    agents. Agents collaborate with each other by sharing a global action-value function
    and passing messages that encode historical information across scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Reinforcement Learning for Recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recommender systems target to capture users’ preferences according to their
    feedback (or behaviors, e.g. rating and review) and suggest items that match their
    preferences. In this section, we briefly review how RL is adapted in several key
    tasks in recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Exploitation/Exploration Dilemma
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditional recommender systems suffer from the exploitation-exploration dilemma,
    where exploitation is to recommend items that are predicted to best match users’
    preferences, while exploration is to recommend items randomly to collect more
    users’ feedback. The contextual bandit models an agent that attempts to balance
    the competing exploitation and exploration tasks in order to maximize the accumulated
    long-term reward over a considered period. The traditional strategies to balance
    exploitation and exploration in bandit setting are $\epsilon$-greedy [[Watkins
    (1989)](#bib.bib74)], EXP3 [[Auer et al. (2002)](#bib.bib3)], and UCB1 [[Auer
    et al. (2002)](#bib.bib2)]. In the news feeds scenario, the exploration/exploitation
    problem of personalized news recommendation is modeled as a contextual bandit
    problem [[Li et al. (2010)](#bib.bib41)], and a learning algorithm LinUCB is proposed
    to select articles sequentially for specific users based on the users’ and articles’
    contextual information, in order to maximize the total user clicks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Temporal Dynamics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most existing recommender systems such as collaborative filtering, content-based
    and learning-to-rank have been extensively studied with the stationary environment
    (reward) assumption, where user’s preference is assumed to be static. However,
    this assumption is usually not true in reality since users’ preferences are dynamic,
    thus the reward distributions usually change over time. In bandit setting, it
    usually introduces a variable reward function to delineate the dynamic nature
    of the environment. For instance, the particle learning based dynamical context
    drift model is proposed to model the changing of reward mapping function in multi-armed
    bandit problem, where the drift of the reward mapping function is learned as a
    group of random walk particles, and well fitted particles are dynamically chosen
    to describe the mapping function [[Zeng et al. (2016)](#bib.bib90)]. A contextual
    bandit algorithm is presented to detect the changes of environment according to
    the reward estimation confidence, and updates the arm selection policy accordingly [[Wu
    et al. (2018)](#bib.bib78)]. The change-detection based framework under the piecewise-stationary
    reward assumption for the multi-armed bandit problem is proposed in  [[Liu et al.
    (2018)](#bib.bib42)], where upper confidence bound (UCB) policies is used to detect
    change points actively and restart the UCB indices. Another solution for capturing
    user’s dynamic preference is to introduce the MDP setting [[Chen et al. (2018)](#bib.bib16),
    [Liu et al. (2018)](#bib.bib43), [(97)](#bib.bib97), [Zou et al. (2019)](#bib.bib100)].
    Under the MDP setting, state is introduced to represent user’s preference and
    state transition captures the dynamic nature of user’s preference over time. In [[(97)](#bib.bib97)],
    a user’s dynamic preference (agent’s state) is learned from his/her browsing history.
    Each time the recommender system suggests an item to a user, the user will browse
    this item and provide feedback (skip, click or purchase), which reveals user’s
    satisfaction of the recommended item. According to the feedback, the recommender
    system will update its state to represent user’s new preferences [[(97)](#bib.bib97)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Long Term User Engagement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: User engagement in recommendation is the assessment of user’s desirable (even
    essential) responses to the items (products, services, or information) suggested
    by the recommender systems [[Lalmas et al. (2014)](#bib.bib40)]. User engagement
    can be measured not only in terms of immediate response (e.g. clicks and rating
    of the recommended items), but more importantly in terms of long-term response
    (e.g. user repetitively purchases) [[(58)](#bib.bib58)]. In [[Wu et al. (2017)](#bib.bib79)],
    the problem of long-term user engagement optimization is formulated as a sequential
    decision making problem. In each iteration, the agent needs to estimate the risk
    of losing a user based on the user’s dynamic response to past recommendations.
    Then, a bandit based method [[Wu et al. (2017)](#bib.bib79)] is introduced to
    balance the immediate user click and the expected future clicks when the user
    revisits the recommender system. In practical recommendation sessions, users will
    sequentially access multiple scenarios, such as the entrance pages and the item
    detail pages, and each scenario has its own recommendation strategy. A multi-agent
    reinforcement learning based approach (DeepChain) is proposed in [[Zhao et al.
    (2019)](#bib.bib96)], which can capture the sequential correlation among different
    scenarios and jointly optimize multiple recommendation strategies. To be specific,
    model-based reinforcement learning technique is introduced to reduce the training
    data requirement and execute more accurate strategy updates. In the news feeds
    scenario [[Zheng et al. (2018)](#bib.bib99)], to incorporate more user feedback
    information, the long-term user response (i.e., how frequent user returns) is
    considered as a supplement to user’s immediate click behaviors, and a Deep Q-Learning
    based framework is proposed to optimize the news recommendation strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Page-Wise Recommendation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practical recommender systems, each time users are typically recommended
    a page of items. In this setting, the recommender systems need to jointly (1)
    select a set of complementary and diverse items from a larger candidate item set
    and (2) form an item display (layout configuration) strategy to place the items
    in a 2-D web page that can lead to maximal reward. Given the massive number of
    items, the action space is extremely large if we treat each whole page recommendation
    as one action. To mitigate the issue of the large action space, a Deep Deterministic
    Policy Gradient algorithm is proposed [[Dulac-Arnold et al. (2015)](#bib.bib21)]
    where the Actor generates a deterministic optimal action according to the current
    state, and the Critic outputs the Q-value of this state-action pair. DDPG reduces
    the computational cost of conventional value-based reinforcement learning methods,
    thus it is a fitting choice for the whole page recommendation setting [[Cai et al.
    (2018a)](#bib.bib12), [Cai et al. (2018b)](#bib.bib13)]. Several approaches are
    presented recently to enhance the efficiency [[Choi et al. (2018)](#bib.bib17),
    [Chen et al. (2018)](#bib.bib15)]. In [[(95)](#bib.bib95), [Zhao et al. (2017)](#bib.bib98)],
    CNN techniques are introduced to capture the item display patterns and users’
    feedback of each item in the page. To represent each item, item-embedding, category-embedding
    and feedback embedding are leveraged, which can help to generate complementary
    and diverse recommendations and capture user’s interests within the pages. Bandit
    techniques are also leveraged for whole-page Recommendations [[Wang et al. (2017)](#bib.bib71),
    [Lacerda (2017)](#bib.bib38)]. For instance, the whole page recommendation task
    is considered as a combinatorial semi-bandit problem, where the system recommends
    $S$ actions from a candidate set of $K$ actions, and displays the selected items
    in $S$ (out of $M$) positions [[Wang et al. (2017)](#bib.bib71)].
  prefs: []
  type: TYPE_NORMAL
- en: 5 Reinforcement Learning for Online Advertising
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of online advertising is to assign the right advertisements to the
    right users so as to maximize the revenue, click-through rate (CTR) or return
    on investment (ROI) of the advertising campaign. The two main marketing strategy
    in online advertising are guaranteed delivery (GD) and real-time bidding (RTB).
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Guaranteed Delivery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In guaranteed delivery, advertisements that share a single idea and theme are
    grouped into campaigns, and are charged on a pay-per-campaign basis for the pre-specified
    number of deliveries (click or impressions) [[Salomatin et al. (2012)](#bib.bib56)].
    Most popular GD (Guaranteed Delivery) solutions are based on offline optimization
    algorithms, and then adjusted for online setup. However, deriving the optimal
    strategy to allocate impressions is challenging, especially when the environment
    is unstable in real-world application. In [[Wu et al. (2018)](#bib.bib76)], a
    multi-agent reinforcement learning (MARL) approach is proposed to derive cooperative
    policies for the publisher to maximize its target in an unstable environment.
    They formulated the impression allocation problem as an auction problem where
    each contract can submit virtual bids for individual impressions. With this formulation,
    they derived the optimal impression allocation strategy by solving the optimal
    bidding functions for contracts.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Real-Time Bidding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RTB allows an advertiser to submit a bid for each individual impression in a
    very short time frame. Ad selection task is typically modeled as multi-armed bandit
    (MAB) problem with the setting that samples from each arm are iid, feedback is
    immediate and rewards are stationary [[Yang and Lu (2016)](#bib.bib85), [Nuara
    et al. (2018)](#bib.bib51), [Gasparini et al. (2018)](#bib.bib25), [Tang et al.
    (2013)](#bib.bib66), [Xu et al. (2013)](#bib.bib84), [Yuan et al. (2013)](#bib.bib88),
    [Schwartz et al. (2017)](#bib.bib59)]. The payoff functions of a MAB are allowed
    to evolve, but they are assumed to evolve slowly over time. On the other hand,
    display ads created while others are removed regularly in an advertising campaign
    circulation. The problem of multi-armed bandits with budget constraints and variable
    costs is studied in [[(20)](#bib.bib20)]. In this case, pulling the arms of bandit
    will get random rewards with random costs, and the algorithm aims to maximize
    the long-term reward by pulling arms with a constrained budget. This setting can
    model Internet advertising in a more precise way than previous works where pulling
    an arm is costless or has a fixed cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the MAB setting, the bid decision is considered as a static optimization
    problem of either treating the value of each impression independently or setting
    a bid price to each segment of ad volume. However, the bidding for a given ad
    campaign would repeatedly happen during its life span before the budget running
    out. Thus, the MDP setting have also been studied [[Cai et al. (2017)](#bib.bib11),
    [Tang (2017)](#bib.bib67), [Wang et al. (2018)](#bib.bib70), [Zhao et al. (2018)](#bib.bib94),
    [Rohde et al. (2018)](#bib.bib54), [Wu et al. (2018)](#bib.bib77), [Jin et al.
    (2018)](#bib.bib28)]. A model-based reinforcement learning framework is proposed
    to learn bid strategies in RTB advertising [[Cai et al. (2017)](#bib.bib11)],
    where neural network is used to approximate the state value, which can better
    deal with the scalability problem of large auction volume and limited campaign
    budget. A model-free deep reinforcement learning method is proposed to solve the
    bidding problem with constrained budget [[Wu et al. (2018)](#bib.bib77)]: the
    problem is modeled as a $\lambda$-control problem, and RewardNet is designed for
    generating rewards to solve reward design trap, instead of using the immediate
    reward. A multi-agent bidding model is presented, which takes the other advertisers’
    bidding in the system into consideration, and a clustering approach is introduced
    to solve the large number of advertisers challenge [[Jin et al. (2018)](#bib.bib28)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we present an overview of information seeking from the reinforcement
    learning perspective. We first introduce mathematical foundations of RL based
    information seeking approaches. Then we review state-of-the-art algorithms of
    three representative information seeking mechanisms – search, recommendations,
    and advertising. Next, we here discuss some interesting research directions on
    reinforcement learning that can bring the information seeking research into a
    new frontier.
  prefs: []
  type: TYPE_NORMAL
- en: First, most of the existing works train a policy within one scenario, while
    overlooking users’ behaviors (preference) in other scenarios [[Feng et al. (2018)](#bib.bib22)].
    This will result in a suboptimal policy, which calls for collaborative RL frameworks
    that consider search, recommendation and advertising scenarios simultaneously.
    Second, the type of reward function varies among different computational tasks.
    More sophisticated reward functions should be designed to achieve more goals of
    information seeking, such as increasing the supervising degree of recommendations.
    Third, more types of user-agent interactions could be incorporated into RL frameworks,
    such as adding items into shopping cart, users’ repeat purchase behavior, users’
    dwelling time in the system, and user’s chatting with customer service representatives
    or agent of AI dialog system. Fourth, testing a new algorithm is expensive since
    it needs lots of engineering efforts to deploy the algorithm in the practical
    system, and it also may have negative impacts on user experience if the algorithm
    is not mature. Thus online environment simulator or offline evaluation method
    based on historical logs are necessary to pre-train and evaluate new algorithms
    before launching them online. Finally, there is an increasing demand for an open
    online reinforcement learning environment for information seeking, which can advance
    the RL and information seeking communities and achieve better consistency between
    offline and online performance.
  prefs: []
  type: TYPE_NORMAL
- en: ACKNOWLEDGEMENTS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Xiangyu Zhao and Jiliang Tang are supported by the National Science Foundation
    (NSF) under grant numbers IIS-1714741, IIS-1715940 and CNS-1815636, and a grant
    from Criteo Faculty Research Award.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Åström (1965) Åström, K. J. 1965. Optimal control of markov processes with incomplete
    state information. Journal of Mathematical Analysis and Applications 10, 1, 174–205.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auer et al. (2002) Auer, P., Cesa-Bianchi, N., and Fischer, P. 2002. Finite-time
    analysis of the multiarmed bandit problem. Machine Learning 47, 2-3, 235–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auer et al. (2002) Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E.
    2002. The nonstochastic multiarmed bandit problem. SIAM J. Comput. 32, 1, 48–77.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baxter and Bartlett (2001) Baxter, J. and Bartlett, P. L. 2001. Infinite-horizon
    policy-gradient estimation. J. Artif. Intell. Res. 15, 319–350.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellman (2013) Bellman, R. 2013. Dynamic programming. Courier Corporation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhatnagar et al. (2007) Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee,
    M. 2007. Incremental natural actor-critic algorithms. In NIPS ’07.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhatnagar et al. (2009) Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee,
    M. 2009. Natural actor-critic algorithms. Automatica 45, 11, 2471–2482.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bowling and Veloso (2002) Bowling, M. H. and Veloso, M. M. 2002. Multiagent
    learning using a variable learning rate. Artif. Intell. 136, 2, 215–250.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Busoniu et al. (2010) Busoniu, L., Babuska, R., De Schutter, B., and Ernst,
    D. 2010. Reinforcement learning and dynamic programming using function approximators.
    CRC press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Busoniu et al. (2008) Busoniu, L., Babuska, R., and Schutter, B. D. 2008. A
    comprehensive survey of multiagent reinforcement learning. IEEE Trans. Systems,
    Man, and Cybernetics, Part C 38, 2, 156–172.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2017) Cai, H., Ren, K., Zhang, W., Malialis, K., Wang, J., Yu, Y.,
    and Guo, D. 2017. Real-time bidding by reinforcement learning in display advertising.
    In WSDM ’17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2018a) Cai, Q., Filos-Ratsikas, A., Tang, P., and Zhang, Y. 2018a.
    Reinforcement mechanism design for e-commerce. In WWW ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2018b) Cai, Q., Filos-Ratsikas, A., Tang, P., and Zhang, Y. 2018b.
    Reinforcement mechanism design for fraudulent behaviour in e-commerce. In AAAI
    ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2006) Chang, C., Kayed, M., Girgis, M. R., and Shaalan, K. F.
    2006. A survey of web information extraction systems. IEEE Trans. Knowl. Data
    Eng. 18, 10, 1411–1428.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Chen, H., Dai, X., Cai, H., Zhang, W., Wang, X., Tang, R.,
    Zhang, Y., and Yu, Y. 2018. Large-scale interactive recommendation with tree-structured
    policy gradient. CoRR abs/1811.05869.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Chen, S., Yu, Y., Da, Q., Tan, J., Huang, H., and Tang, H.
    2018. Stabilizing reinforcement learning in dynamic environment with application
    to online recommendation. In SIGKDD ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. (2018) Choi, S., Ha, H., Hwang, U., Kim, C., Ha, J., and Yoon, S.
    2018. Reinforcement learning based recommender system using biclustering technique.
    CoRR abs/1801.05532.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croft et al. (2010) Croft, W. B., Bendersky, M., Li, H., and Xu, G. 2010. Query
    representation and understanding workshop. SIGIR Forum 44, 2, 48–53.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deisenroth and Rasmussen (2011) Deisenroth, M. P. and Rasmussen, C. E. 2011.
    PILCO: A model-based and data-efficient approach to policy search. In ICML ’11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (20) Ding, W., Qin, T., Zhang, X., and Liu, T. Multi-armed bandit with budget
    constraint and variable costs. In AAAI ’13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dulac-Arnold et al. (2015) Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag,
    P., Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and Coppin, B. 2015.
    Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2018) Feng, J., Li, H., Huang, M., Liu, S., Ou, W., Wang, Z.,
    and Zhu, X. 2018. Learning to collaborate: Multi-scenario ranking via multi-agent
    reinforcement learning. In WWW ’18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2018) Feng, Y., Xu, J., Lan, Y., Guo, J., Zeng, W., and Cheng,
    X. 2018. From greedy selection to exploratory decision-making: Diverse ranking
    with policy-value networks. In SIGIR ’18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garcia-Molina et al. (2011) Garcia-Molina, H., Koutrika, G., and Parameswaran,
    A. G. 2011. Information seeking: convergence of search, recommendations, and advertising.
    Commun. ACM 54, 11, 121–130.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gasparini et al. (2018) Gasparini, M., Nuara, A., Trovò, F., Gatti, N., and
    Restelli, M. 2018. Targeting optimization for internet advertising by learning
    from logged bandit feedback. In IJCNN ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hill et al. (2017) Hill, D. N., Nassif, H., Liu, Y., Iyer, A., and Vishwanathan,
    S. V. N. 2017. An efficient bandit algorithm for realtime multivariate optimization.
    In SIGKDD ’17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2018) Hu, Y., Da, Q., Zeng, A., Yu, Y., and Xu, Y. 2018. Reinforcement
    learning to rank in e-commerce search engine: Formalization, analysis, and application.
    In SIGKDD ’18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2018) Jin, J., Song, C., Li, H., Gai, K., Wang, J., and Zhang, W.
    2018. Real-time bidding with multi-agent reinforcement learning in display advertising.
    In CIKM ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaelbling et al. (1998) Kaelbling, L. P., Littman, M. L., and Cassandra, A. R.
    1998. Planning and acting in partially observable stochastic domains. Artif. Intell. 101, 1-2,
    99–134.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaelbling et al. (1996) Kaelbling, L. P., Littman, M. L., and Moore, A. W.
    1996. Reinforcement learning: A survey. J. Artif. Intell. Res. 4, 237–285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kakade (2001) Kakade, S. 2001. A natural policy gradient. In NIPS ’01.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kapoor et al. (2018) Kapoor, S., Keswani, V., Vishnoi, N. K., and Celis, L. E.
    2018. Balanced news using constrained bandit-based personalization. In IJCAI ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Katariya et al. (2017) Katariya, S., Kveton, B., Szepesvári, C., Vernade, C.,
    and Wen, Z. 2017. Bernoulli rank-1 bandits for click feedback. In IJCAI ’17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katariya et al. (2016) Katariya, S., Kveton, B., Szepesvári, C., and Wen, Z.
    2016. DCM bandits: Learning to rank with multiple clicks. In ICML ’16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Konda and Tsitsiklis (1999) Konda, V. R. and Tsitsiklis, J. N. 1999. Actor-critic
    algorithms. In NIPS ’99.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kröse (1995) Kröse, B. J. A. 1995. Learning from delayed rewards. Robotics and
    Autonomous Systems 15, 4, 233–235.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kveton et al. (2015) Kveton, B., Szepesvári, C., Wen, Z., and Ashkan, A. 2015.
    Cascading bandits: Learning to rank in the cascade model. In ICML ’15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lacerda (2017) Lacerda, A. 2017. Multi-objective ranked bandits for recommender
    systems. Neurocomputing 246, 12–24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lagoudakis and Parr (2003) Lagoudakis, M. G. and Parr, R. 2003. Least-squares
    policy iteration. Journal of Machine Learning Research 4, 1107–1149.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lalmas et al. (2014) Lalmas, M., O’Brien, H., and Yom-Tov, E. 2014. Measuring
    User Engagement. Synthesis Lectures on Information Concepts, Retrieval, and Services.
    Morgan & Claypool Publishers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2010) Li, L., Chu, W., Langford, J., and Schapire, R. E. 2010. A
    contextual-bandit approach to personalized news article recommendation. In WWW
    ’10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Liu, F., Lee, J., and Shroff, N. B. 2018. A change-detection
    based framework for piecewise-stationary multi-armed bandit problem. In AAAI ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Liu, F., Tang, R., Li, X., Ye, Y., Chen, H., Guo, H., and
    Zhang, Y. 2018. Deep reinforcement learning based recommendation with explicit
    user-item interactions modeling. CoRR abs/1810.12027.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2010) Lu, T., Pál, D., and Pal, M. 2010. Contextual multi-armed bandits.
    In AISTATS ’10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2014) Luo, J., Zhang, S., and Yang, H. 2014. Win-win search: dual-agent
    stochastic game in session search. In SIGIR ’14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lv and Zhai (2009) Lv, Y. and Zhai, C. 2009. Adaptive relevance feedback in
    information retrieval. In CIKM ’09.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2015) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
    J., Bellemare, M. G., Graves, A., Riedmiller, M. A., Fidjeland, A., Ostrovski,
    G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D.,
    Wierstra, D., Legg, S., and Hassabis, D. 2015. Human-level control through deep
    reinforcement learning. Nature 518, 7540, 529–533.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moore and Atkeson (1993) Moore, A. W. and Atkeson, C. G. 1993. Prioritized
    sweeping: Reinforcement learning with less data and less time. Machine Learning 13,
    103–130.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nogueira et al. (2018) Nogueira, R., Bulian, J., and Ciaramita, M. 2018. Learning
    to coordinate multiple reinforcement learning agents for diverse query reformulation.
    CoRR abs/1809.10658.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nogueira and Cho (2017) Nogueira, R. and Cho, K. 2017. Task-oriented query reformulation
    with reinforcement learning. In EMNLP ’17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nuara et al. (2018) Nuara, A., Trovò, F., Gatti, N., and Restelli, M. 2018.
    A combinatorial-bandit algorithm for the online joint bid/budget optimization
    of pay-per-click advertising campaigns. In AAAI ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters and Schaal (2008) Peters, J. and Schaal, S. 2008. Natural actor-critic.
    Neurocomputing 71, 7-9, 1180–1190.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2005) Peters, J., Vijayakumar, S., and Schaal, S. 2005. Natural
    actor-critic. In ECML ’05.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rohde et al. (2018) Rohde, D., Bonner, S., Dunlop, T., Vasile, F., and Karatzoglou,
    A. 2018. Recogym: A reinforcement learning environment for the problem of product
    recommendation in online advertising. CoRR abs/1808.00720.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rummery and Niranjan (1994) Rummery, G. A. and Niranjan, M. 1994. On-line Q-learning
    using connectionist systems. Vol. 37. University of Cambridge, Department of Engineering
    Cambridge, England.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salomatin et al. (2012) Salomatin, K., Liu, T., and Yang, Y. 2012. A unified
    optimization framework for auction and guaranteed delivery in online advertising.
    In CIKM ’12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santos et al. (2015) Santos, R. L. T., MacDonald, C., and Ounis, I. 2015. Search
    result diversification. Foundations and Trends in Information Retrieval 9, 1,
    1–90.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (58) Schopfer, S. and Keller, T. Long term recommender benchmarking for mobile
    shopping list applications using markov chains. In RecSys ’14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwartz et al. (2017) Schwartz, E. M., Bradlow, E. T., and Fader, P. S. 2017.
    Customer acquisition via display advertising using multi-armed bandit experiments.
    Marketing Science 36, 4, 500–522.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shani et al. (2005) Shani, G., Heckerman, D., and Brafman, R. I. 2005. An mdp-based
    recommender system. Journal of Machine Learning Research 6, 1265–1295.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shoham et al. (2003) Shoham, Y., Powers, R., and Grenager, T. 2003. Multi-agent
    reinforcement learning: a critical survey. Tech. rep., Technical report, Stanford
    University.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smallwood and Sondik (1973) Smallwood, R. D. and Sondik, E. J. 1973. The optimal
    control of partially observable markov processes over a finite horizon. Operations
    Research 21, 5, 1071–1088.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sondik (1978) Sondik, E. J. 1978. The optimal control of partially observable
    markov processes over the infinite horizon: Discounted costs. Operations Research 26, 2,
    282–304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton (1991) Sutton, R. S. 1991. Dyna, an integrated architecture for learning,
    planning, and reacting. SIGART Bulletin 2, 4, 160–163.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton and Barto (1998) Sutton, R. S. and Barto, A. G. 1998. Introduction to
    reinforcement learning. Vol. 135. MIT press Cambridge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2013) Tang, L., Rosales, R., Singh, A., and Agarwal, D. 2013. Automatic
    ad format selection via contextual bandits. In CIKM ’13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang (2017) Tang, P. 2017. Reinforcement mechanism design. In IJCAI ’17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varaiya and Walrand (1983) Varaiya, P. and Walrand, J. C. 1983. Multi-armed
    bandit problems and resource sharing systems. In Computer Performance and Reliability,
    Proceedings of the International Workshop, Pisa, Italy, September 26-30, 1983.
    181–196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vorobev et al. (2015) Vorobev, A., Lefortier, D., Gusev, G., and Serdyukov,
    P. 2015. Gathering additional feedback on search results by multi-armed bandits
    with respect to production ranking. In WWW ’15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Wang, W., Jin, J., Hao, J., Chen, C., Yu, C., Zhang, W.,
    Wang, J., Wang, Y., Li, H., Xu, J., and Gai, K. 2018. Learning to advertise with
    adaptive exposure via constrained two-level reinforcement learning. CoRR abs/1809.03149.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Wang, Y., Ouyang, H., Wang, C., Chen, J., Asamov, T., and
    Chang, Y. 2017. Efficient ordered combinatorial semi-bandits for whole-page recommendation.
    In AAAI ’17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang,
    Y., and Mei, Q. 2016. Beyond ranking: Optimizing whole-page presentation. In WSDM
    ’16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang,
    Y., and Mei, Q. 2018. Optimizing whole-page presentation for web search. TWEB 12, 3,
    19:1–19:25.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watkins (1989) Watkins, C. J. C. H. 1989. Learning from delayed rewards. Ph.D.
    thesis, King’s College, Cambridge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (1992) Williams, R. J. 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. Machine Learning 8, 229–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2018) Wu, D., Chen, C., Yang, X., Chen, X., Tan, Q., Xu, J., and
    Gai, K. 2018. A multi-agent reinforcement learning method for impression allocation
    in online display advertising. CoRR abs/1809.03152.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2018) Wu, D., Chen, X., Yang, X., Wang, H., Tan, Q., Zhang, X., Xu,
    J., and Gai, K. 2018. Budget constrained bidding by model-free reinforcement learning
    in display advertising. In CIKM ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2018) Wu, Q., Iyer, N., and Wang, H. 2018. Learning contextual bandits
    in a non-stationary environment. In SIGIR ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2017) Wu, Q., Wang, H., Hong, L., and Shi, Y. 2017. Returning is
    believing: Optimizing long-term user engagement in recommender systems. In CIKM
    ’17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia et al. (2017) Xia, L., Xu, J., Lan, Y., Guo, J., Zeng, W., and Cheng, X.
    2017. Adapting markov decision process for search result diversification. In SIGIR
    ’17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu and Li (2007) Xu, J. and Li, H. 2007. Adarank: a boosting algorithm for
    information retrieval. In SIGIR ’07.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2008) Xu, J., Liu, T., Lu, M., Li, H., and Ma, W. 2008. Directly
    optimizing evaluation measures in learning to rank. In SIGIR ’08.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2017) Xu, J., Xia, L., Lan, Y., Guo, J., and Cheng, X. 2017. Directly
    optimize diversity evaluation measures: A new approach to search result diversification.
    ACM TIST 8, 3, 41:1–41:26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2013) Xu, M., Qin, T., and Liu, T. 2013. Estimation bias in multi-armed
    bandit algorithms for search advertising. In NIPS ’13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Lu (2016) Yang, H. and Lu, Q. 2016. Dynamic contextual multi arm bandits
    in display advertisement. In ICDM ’16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (86) Yin, D., Hu, Y., Tang, J., Daly, T., Zhou, M., Ouyang, H., Chen, J., Kang,
    C., Deng, H., Nobata, C., et al. Ranking relevance in yahoo search. In SIGKDD’16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2016) Yin, D., Hu, Y., Tang, J., Jr., T. D., Zhou, M., Ouyang, H.,
    Chen, J., Kang, C., Deng, H., Nobata, C., Langlois, J., and Chang, Y. 2016. Ranking
    relevance in yahoo search. In SIGKDD ’16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2013) Yuan, S., Wang, J., and van der Meer, M. 2013. Adaptive keywords
    extraction with contextual bandits for advertising on parked domains. CoRR abs/1307.3573.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yue et al. (2007) Yue, Y., Finley, T., Radlinski, F., and Joachims, T. 2007.
    A support vector method for optimizing average precision. In SIGIR ’07.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2016) Zeng, C., Wang, Q., Mokhtari, S., and Li, T. 2016. Online
    context-aware recommendation with time varying multi-armed bandit. In SIGKDD ’16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2017) Zeng, W., Xu, J., Lan, Y., Guo, J., and Cheng, X. 2017. Reinforcement
    learning to rank with markov decision process. In SIGIR ’17.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2018) Zeng, W., Xu, J., Lan, Y., Guo, J., and Cheng, X. 2018. Multi
    page search with reinforcement learning to rank. In ICTIR ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2014) Zhang, S., Luo, J., and Yang, H. 2014. A POMDP model for
    content-free document re-ranking. In SIGIR ’14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Zhao, J., Qiu, G., Guan, Z., Zhao, W., and He, X. 2018. Deep
    reinforcement learning for sponsored search real-time bidding. In SIGKDD ’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (95) Zhao, X., Xia, L., Zhang, L., Ding, Z., Yin, D., and Tang, J. Deep reinforcement
    learning for page-wise recommendations. In ResSys’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Zhao, X., Xia, L., Zhao, Y., Tang, J., and Yin, D. 2019.
    Model-based reinforcement learning for whole-chain recommendations. arXiv preprint
    arXiv:1902.03987.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (97) Zhao, X., Zhang, L., Ding, Z., Xia, L., Tang, J., and Yin, D. Recommendations
    with negative feedback via pairwise deep reinforcement learning. In SIGKDD’18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2017) Zhao, X., Zhang, L., Ding, Z., Yin, D., Zhao, Y., and Tang,
    J. 2017. Deep reinforcement learning for list-wise recommendations. arXiv preprint
    arXiv:1801.00209.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2018) Zheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N. J.,
    Xie, X., and Li, Z. 2018. DRN: A deep reinforcement learning framework for news
    recommendation. In WWW ’18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2019) Zou, L., Xia, L., Ding, Z., Yin, D., Song, J., and Liu, W.
    2019. Reinforcement learning to diversify recommendations. In DASFAA ’19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '{biography}'
  prefs: []
  type: TYPE_NORMAL
- en: Xiangyu Zhao is a Ph.D. student of computer science and engineering at Michigan
    State University (MSU). His supervisor is Dr. Jiliang Tang. Before joining MSU,
    he completed his MS(2017) at USTC and BS(2014) at UESTC. He is the student member
    of IEEE, SIGIR, and SIAM. His current research interests include data mining and
    machine learning, especially (1) Reinforcement Learning for E-commerce; (2) Urban
    Computing and Spatio-Temporal Data Analysis. After joining MSU, he has published
    his work in top journals (e.g. SIGKDD Explorations) and conferences (e.g., KDD,
    ICDM, CIKM, RecSys). He was the recipients of the RecSys’18, KDD’18, SDM’18, and
    CIKM’17 Student Travel Award.
  prefs: []
  type: TYPE_NORMAL
- en: Long Xia is a research scientist in Data Science Lab at JD.com. He is now mainly
    responsible for applying advanced technology to the E-commerce recommender system
    in JD.com. Before that, he received his Ph.D. in Computer Science from Institute
    of Computing Technology, Chinese Academy of Sciences. His research interests include
    data mining, applied machine learning, information retrieval, and recommender
    system. He has published his research in top journals and conferences, e.g. TIST,
    SIGIR, KDD, RecSys.
  prefs: []
  type: TYPE_NORMAL
- en: Jiliang Tang is an assistant professor in the computer science and engineering
    department at Michigan State University. Before that, he was a research scientist
    in Yahoo Research and got his PhD from Arizona State University in 2015\. He has
    broad interests in social computing, data mining and machine learning. He was
    the recipients of the Best Paper Award in ASONAM 2018, the Best Student Paper
    Award in WSDM2018, the Best Paper Award in KDD2016, the runner up of the Best
    KDD Dissertation Award in 2015, Dean’s Dissertation Award and the best paper shortlist
    of WSDM2013\. He is now associate editors of ACM TKDD, ICWSM and Neurocomputing.
    He has published his research in highly ranked journals and top conference proceedings,
    which received thousands of citations and extensive media coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Dawei Yin is Senior Director at JD.com, leading the science efforts of recommendation,
    search, metrics and knowledge graph. Prior to joining JD.com, he was Senior Research
    Manager at Yahoo Labs, leading relevance science team and in charge of Core Search
    Relevance of Yahoo Search. He obtained Ph.D. (2013), M.S. (2010) from Lehigh University
    and B.S. (2006) from Shandong University. His research interests include data
    mining, applied machine learning, information retrieval and recommender system.
    He published tens of research papers in premium conferences and journals, and
    won WSDM2016 best paper award, KDD2016 best paper award, and WSDM2018 best student
    paper award.
  prefs: []
  type: TYPE_NORMAL
