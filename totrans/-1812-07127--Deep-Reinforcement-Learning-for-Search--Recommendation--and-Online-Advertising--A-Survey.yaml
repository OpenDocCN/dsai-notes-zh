- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:06:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1812.07127] Deep Reinforcement Learning for Search, Recommendation, and Online
    Advertising: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1812.07127] 深度强化学习在搜索、推荐和在线广告中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1812.07127](https://ar5iv.labs.arxiv.org/html/1812.07127)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1812.07127](https://ar5iv.labs.arxiv.org/html/1812.07127)
- en: \newsletterQuarter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newsletterQuarter
- en: Spring \newsletterYear2019
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 春季 \newsletterYear2019
- en: 'Deep Reinforcement Learning for Search, Recommendation, and Online Advertising:
    A Survey'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在搜索、推荐和在线广告中的应用：综述
- en: Xiangyu Zhao    Michigan State University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 相宇赵    密歇根州立大学
- en: Long Xia    JD.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 龙夏    京东
- en: Jiliang Tang    Michigan State University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 季良唐    密歇根州立大学
- en: Dawei Yin    JD.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大伟尹    京东
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Search, recommendation, and online advertising are the three most important
    information-providing mechanisms on the web. These information seeking techniques,
    satisfying users’ information needs by suggesting users personalized objects (information
    or services) at the appropriate time and place, play a crucial role in mitigating
    the information overload problem. With recent great advances in deep reinforcement
    learning (DRL), there have been increasing interests in developing DRL based information
    seeking techniques. These DRL based techniques have two key advantages – (1) they
    are able to continuously update information seeking strategies according to users’
    real-time feedback, and (2) they can maximize the expected cumulative long-term
    reward from users where reward has different definitions according to information
    seeking applications such as click-through rate, revenue, user satisfaction and
    engagement. In this paper, we give an overview of deep reinforcement learning
    for search, recommendation, and online advertising from methodologies to applications,
    review representative algorithms, and discuss some appealing research directions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索、推荐和在线广告是网络上最重要的三种信息提供机制。这些信息获取技术通过在适当的时间和地点向用户推荐个性化的对象（信息或服务）来满足用户的信息需求，在缓解信息过载问题中发挥了关键作用。随着深度强化学习（DRL）的巨大进展，基于DRL的信息获取技术受到越来越多的关注。这些基于DRL的技术有两个关键优势——（1）它们能够根据用户的实时反馈不断更新信息获取策略，和（2）它们可以最大化用户的期望累积长期奖励，其中奖励根据信息获取应用的不同定义，如点击率、收入、用户满意度和参与度。本文概述了深度强化学习在搜索、推荐和在线广告中的应用，从方法论到应用，回顾了代表性的算法，并讨论了一些有吸引力的研究方向。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 简介
- en: The explosive growth of the World Wide Web has generated massive data. As a
    consequence, the information overload problem has become progressively severe [[Chang
    et al. (2006)](#bib.bib14)]. Thus, how to identify objects that satisfy users’
    information needs at the appropriate time and place has become increasingly important,
    which has motivated three representative information seeking mechanisms – search,
    recommendation, and online advertising. The search mechanism outputs objects that
    match the query, the recommendation mechanism generates a set of items that match
    users’ implicit preferences, and the online advertising mechanism is analogous
    to search and recommendation expect that the objects to be presented are advertisements [[Garcia-Molina
    et al. (2011)](#bib.bib24)]. Numerous efforts have been made on designing intelligent
    methods for these three information seeking mechanisms. However, traditional techniques
    often face several common challenges. First, the majority of existing methods
    consider information seeking as a static task and generate objects following a
    fixed greedy strategy. This may fail to capture the dynamic nature of users’ preferences
    (or environment). Second, most traditional methods are developed to maximize the
    short-term reward, while completely neglecting whether the suggested objects will
    contribute more in long-term reward [[Shani et al. (2005)](#bib.bib60)]. Note
    that the reward has different definitions among information seeking tasks, such
    as click-through rate (CTR), revenue, and dwell time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 万维网的爆炸性增长产生了大量数据。因此，信息超载问题变得越来越严重[[Chang et al. (2006)](#bib.bib14)]。因此，如何在适当的时间和地点识别满足用户信息需求的对象变得越来越重要，这促使了三种代表性的信息获取机制——搜索、推荐和在线广告。搜索机制输出与查询匹配的对象，推荐机制生成一组匹配用户隐含偏好的项目，而在线广告机制类似于搜索和推荐，但所呈现的对象是广告[[Garcia-Molina
    et al. (2011)](#bib.bib24)]。在设计这些信息获取机制的智能方法上已经做了大量工作。然而，传统技术往往面临几个共同的挑战。首先，大多数现有方法将信息获取视为一个静态任务，并按照固定的贪婪策略生成对象。这可能无法捕捉用户偏好的动态特性（或环境）。其次，大多数传统方法旨在最大化短期奖励，而完全忽视了建议的对象是否会在长期奖励中贡献更多[[Shani
    et al. (2005)](#bib.bib60)]。注意，奖励在信息获取任务中有不同的定义，如点击率（CTR）、收入和停留时间。
- en: Recent years have witnessed the rapid development of reinforcement learning
    (RL) techniques and a wide range of RL based applications. Under the RL schema,
    we tackle complex problems by acquiring experiences through interactions with
    a dynamic environment. The result is an optimal policy that can provide solutions
    to complex tasks without any specific instructions [[Kaelbling et al. (1996)](#bib.bib30)].
    Employing RL for information seeking can naturally resolve the aforementioned
    challenges. First, considering the information seeking tasks as sequential interactions
    between an RL agent (system) and users (environment), the agent can continuously
    update its strategies according to users’ real-time feedback during the interactions,
    until the system converges to the optimal policy that generates objects best match
    users’ dynamic preferences. Second, the RL frameworks are designed to maximize
    the long-term cumulative reward from users. Therefore, the agent could identify
    objects with small immediate reward but making big contributions to the reward
    in the long run.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，强化学习（RL）技术迅速发展，并应用于各种领域。在RL框架下，我们通过与动态环境的互动来获取经验，进而解决复杂问题。结果是得到一个**最优策略**，可以在没有任何具体指示的情况下解决复杂任务[[Kaelbling
    et al. (1996)](#bib.bib30)]。采用RL进行信息获取可以自然地解决上述挑战。首先，将信息获取任务视为RL代理（系统）与用户（环境）之间的顺序互动，代理可以根据用户的实时反馈不断更新其策略，直到系统收敛到生成最符合用户动态偏好的对象的最优策略。其次，RL框架的设计旨在最大化来自用户的长期累积奖励。因此，代理可以识别那些即时奖励小但对长期奖励贡献大的对象。
- en: Given the advantages of reinforcement learning, there have been tremendous interests
    in developing RL based information seeking techniques. Thus, it is timely and
    necessary to provide an overview of information seeking techniques from a reinforcement
    learning perspective. In this survey, we present a comprehensive overview of state-of-the-art
    RL based information seeking techniques and discuss some future directions. The
    remaining of the survey is organized as follows. In Section 2, we introduce technical
    foundations of reinforcement learning based information seeking techniques. Then
    we review the three key information seeking tasks – search, recommendation, and
    online advertising – with representative algorithms from Sections 3 to 5\. Finally,
    we conclude the work with several future research directions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习的优势，开发基于 RL 的信息检索技术引起了极大的兴趣。因此，从强化学习的角度提供信息检索技术的概述是及时且必要的。在本次调查中，我们提供了最先进的基于
    RL 的信息检索技术的全面概述，并讨论了一些未来的研究方向。调查的其余部分组织如下。在第 2 节中，我们介绍了基于强化学习的信息检索技术的技术基础。接着，我们回顾了三个关键的信息检索任务——搜索、推荐和在线广告——以及第
    3 到第 5 节中的代表性算法。最后，我们总结了几项未来的研究方向。
- en: 2 Technical Foundations
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 技术基础
- en: Reinforcement learning is learning how to map situations to actions [[Sutton
    and Barto (1998)](#bib.bib65)]. The two fundamental elements in RL are to formulate
    the situations (mathematical models) and to learn the mapping (policy learning).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是学习如何将情况映射到动作 [[Sutton and Barto (1998)](#bib.bib65)]。RL 的两个基本元素是制定情况（数学模型）和学习映射（策略学习）。
- en: 2.1 Problem Formulation
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题表述
- en: 'In reinforcement learning, there two main settings for problem formulations:
    multi-armed bandits (without state transition) and Markov decision processes (with
    state transition).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，问题的表述有两个主要设置：多臂老虎机（没有状态转移）和马尔可夫决策过程（有状态转移）。
- en: 2.1.1 Multi-Armed Bandits
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 多臂老虎机
- en: The Multi-Armed Bandits (MABs) problem is a simple model for the exploration/exploitation
    trade-off [[Varaiya and Walrand (1983)](#bib.bib68)]. Formally, a $K$-MAB can
    be defined as follows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机（MABs）问题是探索/利用权衡的一个简单模型 [[Varaiya and Walrand (1983)](#bib.bib68)]。正式地，一个
    $K$-MAB 可以定义如下。
- en: Definition 2.1.
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.1.
- en: A $K$-MAB is a 3-tuple $\langle A,R,\pi\rangle$, where $A$ is the set of actions (arms)
    and $|A|=K$, $r=R(a)$ is the reward distribution when performing action $a$, and
    policy $\pi$ describes probability distribution over the possible actions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 $K$-MAB 是一个三元组 $\langle A, R, \pi \rangle$，其中 $A$ 是动作（臂）的集合，$|A|=K$，$r=R(a)$
    是执行动作 $a$ 时的奖励分布，策略 $\pi$ 描述了对可能动作的概率分布。
- en: An arm with the highest expected reward is called the best arm (denoted as $a_{*}$) and
    its expected reward $r_{*}$ is the optimal reward. An algorithm for MAB, at each
    time step $t$, samples an arm $a_{t}$ and receives a reward $r_{t}$. When making
    its selection, the algorithm depends on the history (i.e., actions and rewards)
    up to the time $t$-$1$. The contextual bandit model (a.k.a. associative bandits
    or bandits with side information) is an extension of MAB that takes additional
    information into account [[Auer et al. (2002)](#bib.bib2), [Lu et al. (2010)](#bib.bib44)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最高期望奖励的臂称为最佳臂（记作 $a_{*}$），其期望奖励 $r_{*}$ 是最优奖励。MAB 的一个算法在每个时间步 $t$ 时，采样一个臂
    $a_{t}$ 并接收一个奖励 $r_{t}$。在做出选择时，算法依赖于历史（即，动作和奖励）直到时间 $t$-$1$。上下文多臂老虎机模型（即关联老虎机或带有附加信息的老虎机）是
    MAB 的一种扩展，考虑了额外的信息 [[Auer et al. (2002)](#bib.bib2), [Lu et al. (2010)](#bib.bib44)]。
- en: 2.1.2 Markov Decision Process
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 马尔可夫决策过程
- en: A Markov decision process (MDP) is a classical formalization of sequential decision
    making, which is a mathematically idealized form of reinforcement learning problem [[Bellman
    (2013)](#bib.bib5)]. We define an MDP as follows.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）是顺序决策的经典形式化，是一种数学理想化的强化学习问题形式 [[Bellman (2013)](#bib.bib5)]。我们定义一个
    MDP 如下。
- en: Definition 2.2.
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.2.
- en: A Markov Decision Process is a 5-tuple $\langle S,A,T,R,\pi\rangle$, where $S$
    is a set of states, $A$ is a discrete set of actions, $T$ is the state transition
    function $s_{t+1}=T(s_{t},a_{t})$ which specifies a function mapping a state $s_{t}$
    into a new state $s_{t+1}$ in response to the selected action $a_{t}$, $r=R(s,a)$
    is the reward distribution when performing action $a$ in state $s$, and policy
    $\pi(a|s)$ describes the behaviors of an agent which is a probability distribution
    over the possible actions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程是一个 5 元组 $\langle S,A,T,R,\pi\rangle$，其中 $S$ 是状态集合，$A$ 是离散的动作集合，$T$
    是状态转移函数 $s_{t+1}=T(s_{t},a_{t})$，它指定了一个函数，将状态 $s_{t}$ 映射到新的状态 $s_{t+1}$，以响应选择的动作
    $a_{t}$，$r=R(s,a)$ 是在状态 $s$ 执行动作 $a$ 时的奖励分布，而策略 $\pi(a|s)$ 描述了代理的行为，它是一个可能动作的概率分布。
- en: The agent and environment interact at each of a sequence of discrete time steps
    $t=\{0,1,2,\dots\}$. Consequently, a sequence or trajectory is generated as $\{s_{0},a_{0},r_{1},\cdots,s_{t},a_{t},$
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 代理和环境在每个离散时间步 $t=\{0,1,2,\dots\}$ 进行交互。因此，会生成一个序列或轨迹 $\{s_{0},a_{0},r_{1},\cdots,s_{t},a_{t},$
- en: '$r_{t+1},\cdots\}$. In general, we seek to maximize the expected discounted
    return, where the return $G_{t}$ is defined as: $G_{t}=\sum_{k=0}^{\infty}\gamma^{k}r_{r+k+1}$,
    where $\gamma$ ($0\leq\gamma\leq 1$) is the discounted rate. The Partially Observable
    Markov Decision Process (POMDP) is an extension of MDP to the case where the state
    of the system is not necessarily observable [[Åström (1965)](#bib.bib1), [Smallwood
    and Sondik (1973)](#bib.bib62), [Sondik (1978)](#bib.bib63), [Kaelbling et al.
    (1998)](#bib.bib29)].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: $r_{t+1},\cdots\}$。一般来说，我们寻求最大化期望折扣回报，其中回报 $G_{t}$ 被定义为：$G_{t}=\sum_{k=0}^{\infty}\gamma^{k}r_{r+k+1}$，其中
    $\gamma$ ($0\leq\gamma\leq 1$) 是折扣率。部分可观测马尔可夫决策过程（POMDP）是对 MDP 的扩展，用于系统状态不一定可观察的情况[[Åström
    (1965)](#bib.bib1), [Smallwood 和 Sondik (1973)](#bib.bib62), [Sondik (1978)](#bib.bib63),
    [Kaelbling 等 (1998)](#bib.bib29)]。
- en: 2.1.3 Multi-Agent Setting
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 多代理设置
- en: 'The generalization of the Markov Decision Process to the Multi-Agent case is
    the stochastic game [[Bowling and Veloso (2002)](#bib.bib8), [Shoham et al. (2003)](#bib.bib61),
    [Busoniu et al. (2008)](#bib.bib10)] as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程推广到多代理情况的泛化是随机游戏[[Bowling 和 Veloso (2002)](#bib.bib8), [Shoham 等 (2003)](#bib.bib61),
    [Busoniu 等 (2008)](#bib.bib10)]。
- en: Definition 2.3.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2.3。
- en: A multi-agent game is a tuple $\langle S,A_{1},\dots,A_{n},T,R_{1},\dots,R_{n},\pi_{1},\dots,\pi_{n}\rangle$,
    where $n$ is the number of agents, $S$ is the discrete set of environment states,
    $A_{i}$ is the discrete set of actions for the agent $i$, $T$ is the state transition
    probability function, $R_{i}$ is the reward function of agent $i$, and $\pi_{i}$
    is the policy adopted by agent $i$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理游戏是一个元组 $\langle S,A_{1},\dots,A_{n},T,R_{1},\dots,R_{n},\pi_{1},\dots,\pi_{n}\rangle$，其中
    $n$ 是代理的数量，$S$ 是环境状态的离散集合，$A_{i}$ 是代理 $i$ 的离散动作集合，$T$ 是状态转移概率函数，$R_{i}$ 是代理 $i$
    的奖励函数，$\pi_{i}$ 是代理 $i$ 采用的策略。
- en: In the multi-agent game, the state transition is the result of the joint actions
    of all the agents $\mathbf{a}_{t}=[a_{1,t}^{T},\dots,a_{n,t}^{T}]^{T}$, where
    $a_{i,t}\in A_{i}$ denotes the action taken by agent $i$ at time step $t$. The
    reward $r_{i,k+1}$ also depends on the joint action. If $\pi_{1}=\cdots=\pi_{n}$,
    i.e., all the agents adopt the same policy to maximize the same expected return,
    the multi-agent game is fully cooperative. If $n=2$ and $\pi_{1}=-\pi_{2}$, i.e.,
    the two agents have opposite policies, the game is fully competitive. Mixed games
    are stochastic games that are neither fully cooperative nor fully competitive.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在多代理游戏中，状态转移是所有代理联合动作的结果 $\mathbf{a}_{t}=[a_{1,t}^{T},\dots,a_{n,t}^{T}]^{T}$，其中
    $a_{i,t}\in A_{i}$ 表示代理 $i$ 在时间步 $t$ 采取的动作。奖励 $r_{i,k+1}$ 也依赖于联合动作。如果 $\pi_{1}=\cdots=\pi_{n}$，即所有代理采用相同的策略以最大化相同的期望回报，则多代理游戏是完全合作的。如果
    $n=2$ 且 $\pi_{1}=-\pi_{2}$，即两个代理有相反的策略，则游戏是完全竞争的。混合游戏是既非完全合作也非完全竞争的随机游戏。
- en: 2.2 Policy Learning
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 策略学习
- en: 'Reinforcement Learning is a class of learning problems in which the goal of
    an agent (or multi-agent) to find the policy to optimize some measures of its
    long-term performance. RL solutions can be categorized in different ways. Here
    we investigate them from two perspectives: whether the full model is available
    and the way of finding the optimal policy.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一类学习问题，其中代理（或多代理）的目标是找到优化其长期性能某些度量的策略。强化学习的解决方案可以从不同的角度进行分类。在这里，我们从两个角度来研究它们：是否有完整的模型以及寻找最佳策略的方法。
- en: 2.2.1 Model-based v.s. Model-free
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 基于模型 vs. 无模型
- en: Reinforcement learning algorithms, which explicitly learn system models and
    use them to solve MDP problems, are model-based methods. Model-based RL has a
    strong influence from the control theory and is often explained in terms of different
    disciplines. These methods include popular algorithms such as the Dyna [[Sutton
    (1991)](#bib.bib64)], Prioritized Sweeping [[Moore and Atkeson (1993)](#bib.bib48)],
    Q-iteration [[Busoniu et al. (2010)](#bib.bib9)], Policy Gradient (PG) [[Williams
    (1992)](#bib.bib75)], and the variation of PG [[Baxter and Bartlett (2001)](#bib.bib4),
    [Kakade (2001)](#bib.bib31)]. The model-free methods ignore the model and just
    focus on figuring out the value functions directly from the interaction with the
    environment. To accomplish this, the methods depend on sampling and observation
    heavily; thus they don’t need to know the inner working of the system. Some examples
    of these methods are Q-learning [[Kröse (1995)](#bib.bib36)], SARSA [[Rummery
    and Niranjan (1994)](#bib.bib55)], LSPI [[Lagoudakis and Parr (2003)](#bib.bib39)],
    and Actor-Critic [[Konda and Tsitsiklis (1999)](#bib.bib35)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法中，那些明确学习系统模型并利用这些模型解决 MDP 问题的方法称为基于模型的方法。基于模型的 RL 受到控制理论的强烈影响，通常在不同学科中进行解释。这些方法包括流行的算法，如
    Dyna [[Sutton (1991)](#bib.bib64)]、优先级扫掠 [[Moore and Atkeson (1993)](#bib.bib48)]、Q-迭代 [[Busoniu
    et al. (2010)](#bib.bib9)]、策略梯度（PG） [[Williams (1992)](#bib.bib75)] 和 PG 的变体 [[Baxter
    and Bartlett (2001)](#bib.bib4), [Kakade (2001)](#bib.bib31)]。无模型的方法忽略模型，直接专注于通过与环境的互动来确定价值函数。为了实现这一点，这些方法严重依赖于采样和观察；因此，它们不需要了解系统的内部工作。一些这些方法的例子包括
    Q-learning [[Kröse (1995)](#bib.bib36)]、SARSA [[Rummery and Niranjan (1994)](#bib.bib55)]、LSPI [[Lagoudakis
    and Parr (2003)](#bib.bib39)] 和 Actor-Critic [[Konda and Tsitsiklis (1999)](#bib.bib35)]。
- en: 2.2.2 Value function v.s. Policy search
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 价值函数与策略搜索
- en: The algorithms, which first find the optimal value functions and then extract
    optimal policies, are value function methods, such as Dyna, Q-learning, SARSA,
    and DQN [[Mnih et al. (2015)](#bib.bib47)]. The alternative approaches are policy
    search methods which solve an MDP problem by directly searching in the space of
    policies. An important class of policy search methods is that of Policy Gradient (PG)
    algorithms [[Williams (1992)](#bib.bib75), [Baxter and Bartlett (2001)](#bib.bib4),
    [Kakade (2001)](#bib.bib31), [Deisenroth and Rasmussen (2011)](#bib.bib19)]. These
    methods target at modeling and optimizing the policy directly. The policy is usually
    modeled with a parameterized function with respect to $\pi_{\theta}(a|s)$. The
    value of the reward (objective) function depends on this policy and then various
    algorithms can be applied to optimize $\theta$ for the best reward. There are
    a series of algorithms, which use the PG to search in the policy space, and at
    the same time estimate a value function. The important class of these methods
    are Actor-Critic (AC) and its variation [[Konda and Tsitsiklis (1999)](#bib.bib35),
    [Peters et al. (2005)](#bib.bib53), [Peters and Schaal (2008)](#bib.bib52), [Bhatnagar
    et al. (2007)](#bib.bib6), [Bhatnagar et al. (2009)](#bib.bib7)]. These are two-time-scale
    algorithms where the critic uses Temporal-Difference (TD) learning with a linear
    approximation architecture and the actor is updated in an approximate gradient
    direction based on information provided by the critic.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那些首先找到最优价值函数然后提取最优策略的算法是价值函数方法，例如 Dyna、Q-learning、SARSA 和 DQN [[Mnih et al.
    (2015)](#bib.bib47)]。另一类方法是策略搜索方法，它们通过直接在策略空间中搜索来解决 MDP 问题。一个重要的策略搜索方法是策略梯度（PG）算法 [[Williams
    (1992)](#bib.bib75), [Baxter and Bartlett (2001)](#bib.bib4), [Kakade (2001)](#bib.bib31),
    [Deisenroth and Rasmussen (2011)](#bib.bib19)]。这些方法旨在直接建模和优化策略。策略通常用一个相对于 $\pi_{\theta}(a|s)$
    的参数化函数来建模。奖励（目标）函数的值取决于这个策略，然后可以应用各种算法来优化 $\theta$ 以获得最佳奖励。有一系列算法使用 PG 在策略空间中进行搜索，同时估计价值函数。这些方法的重要类别是
    Actor-Critic（AC）及其变体 [[Konda and Tsitsiklis (1999)](#bib.bib35), [Peters et al.
    (2005)](#bib.bib53), [Peters and Schaal (2008)](#bib.bib52), [Bhatnagar et al.
    (2007)](#bib.bib6), [Bhatnagar et al. (2009)](#bib.bib7)]。这些是双时间尺度算法，其中评论员使用线性近似架构的时间差分（TD）学习，演员则基于评论员提供的信息在近似梯度方向上进行更新。
- en: 3 Reinforcement Learning for Search
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 强化学习在搜索中的应用
- en: Search aims to find and rank a set of objects (e.g., documents, records) based
    on a user query [[(86)](#bib.bib86)]. In this section, we review RL applications
    in key topics of search.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索的目的是根据用户查询 [[(86)](#bib.bib86)] 找到并排序一组对象（例如，文档、记录）。在本节中，我们回顾了 RL 在搜索关键主题中的应用。
- en: 3.1 Query Understanding
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 查询理解
- en: 'Query understanding is the primary task for the search engine to understand
    users’ information needs. It can be potentially useful for improving general search
    relevance, user experience, and helping users to accomplish tasks [[Croft et al.
    (2010)](#bib.bib18)]. In [[Nogueira and Cho (2017)](#bib.bib50)], RL has been
    leveraged to solve the query reformulation task: a query reformulation framework
    is proposed based on a neural network, which rewrites a query to maximize the
    number of relevant documents returned. In the proposed framework, a search engine
    is treated as a black box that an agent learns to use in order to retrieve more
    relevant items, which opens the possibility of training an agent to use a search
    engine for a task other than the one it was originally intended for. Additionally,
    the upper-bound performance of an RL-based model is estimated in a given environment.
    In [[Nogueira et al. (2018)](#bib.bib49)], a multi-agent based method is introduced
    to efficiently learn diverse query reformulation. It is argued that it is easier
    to train multiple sub-agents than a single generalist one since each sub-agent
    only needs to learn a policy that performs well for a subset of examples. In the
    proposed framework, an agent consists of multiple specialized sub-agents and a
    meta-agent that learns to aggregate the answers from sub-agents to produce a final
    answer. Thus, the method makes learning faster with parallelism.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 查询理解是搜索引擎了解用户信息需求的主要任务。这对提高一般搜索相关性、用户体验以及帮助用户完成任务有潜在的帮助[[Croft et al. (2010)](#bib.bib18)]。在[[Nogueira
    and Cho (2017)](#bib.bib50)]中，强化学习（RL）被用于解决查询重构任务：提出了一种基于神经网络的查询重构框架，该框架重写查询以最大化返回相关文档的数量。在提出的框架中，搜索引擎被视为一个黑箱，代理学习如何使用它以检索更多相关项目，这为训练代理用于原本不打算用于的任务提供了可能性。此外，还估计了在给定环境下基于RL的模型的上限性能。在[[Nogueira
    et al. (2018)](#bib.bib49)]中，介绍了一种基于多代理的方法来高效学习多样化的查询重构。认为训练多个子代理比训练一个通用代理更容易，因为每个子代理只需学习在一部分示例中表现良好的策略。在提出的框架中，一个代理由多个专门的子代理和一个学习整合子代理答案以生成最终答案的元代理组成。因此，该方法通过并行处理加快了学习速度。
- en: 3.2 Ranking
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 排名
- en: Relevance Ranking is the core problem of information retrieval [[Yin et al.
    (2016)](#bib.bib87)] and learning to rank (LTR) is the key technology in relevance
    ranking. In LTR, the approaches to directly optimize the ranking evaluation measures
    are representative and have been proved to be effective[[Yue et al. (2007)](#bib.bib89),
    [Xu and Li (2007)](#bib.bib81), [Xu et al. (2008)](#bib.bib82)]. These methods
    usually only optimize the evaluation measure calculated at a predefined ranking
    position, e.g. NDCG at rank $K$ in [[Xu and Li (2007)](#bib.bib81)]. The information
    carried by the documents after the rank $K$ are neglected. To solve such problem,
    in [[Zeng et al. (2017)](#bib.bib91)], an LTR model, MDPRank, is proposed based
    on Markov decision process, which has the ability to leverage the measures calculated
    at all of the ranking positions. The reward function is defined based upon the
    IR evaluation measures and the model parameters can be learned through maximizing
    the accumulated rewards to all of the decisions. Implicit relevance feedback refers
    to an interactive process between search engine and user, and has been proven
    to be very effective for improving retrieval accuracy [[Lv and Zhai (2009)](#bib.bib46)].
    Both Bandits and MDPs can model such an interactive process naturally [[Vorobev
    et al. (2015)](#bib.bib69), [Katariya et al. (2016)](#bib.bib34), [Katariya et al.
    (2017)](#bib.bib33)]. In [[Kveton et al. (2015)](#bib.bib37)], cascading bandits
    are introduced to identify the most attractive items, and the goal of the agent
    is to maximize its total reward with respect to the list of the most attractive
    items. Through maintaining state transition, MDP is able to model the user state
    in the interaction with search engine. In [[Zeng et al. (2018)](#bib.bib92)],
    the interactive process is formulated as an MDP and the Recurrent Neural Network
    is applied to process the feedback.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性排序是信息检索的核心问题[[Yin et al. (2016)](#bib.bib87)]，而学习排序（LTR）是相关性排序中的关键技术。在LTR中，直接优化排序评估指标的方法具有代表性，并且已被证明有效[[Yue
    et al. (2007)](#bib.bib89), [Xu and Li (2007)](#bib.bib81), [Xu et al. (2008)](#bib.bib82)]。这些方法通常只优化在预定义排序位置计算的评估指标，例如在[[Xu
    and Li (2007)](#bib.bib81)]中计算的NDCG在排名$K$的位置。排名$K$之后的文档信息被忽略。为了解决这个问题，在[[Zeng
    et al. (2017)](#bib.bib91)]中，提出了一种基于马尔可夫决策过程的LTR模型MDPRank，该模型能够利用所有排名位置计算的指标。奖励函数是基于信息检索评估指标定义的，模型参数可以通过最大化所有决策的累计奖励来学习。隐式相关反馈指的是搜索引擎和用户之间的互动过程，已被证明对提高检索准确性非常有效[[Lv
    and Zhai (2009)](#bib.bib46)]。Bandits和MDPs都能自然地建模这种互动过程[[Vorobev et al. (2015)](#bib.bib69),
    [Katariya et al. (2016)](#bib.bib34), [Katariya et al. (2017)](#bib.bib33)]。在[[Kveton
    et al. (2015)](#bib.bib37)]中，引入了级联Bandits来识别最吸引人的项目，代理的目标是最大化其相对于最吸引人项目列表的总奖励。通过保持状态转移，MDP能够建模用户与搜索引擎互动中的用户状态。在[[Zeng
    et al. (2018)](#bib.bib92)]中，互动过程被形式化为MDP，并应用了递归神经网络来处理反馈。
- en: Beyond relevance ranking, another important goal is to provide search results
    that cover a wide range of topics for a query, i.e., search result diversification [[Santos
    et al. (2015)](#bib.bib57), [Xu et al. (2017)](#bib.bib83)]. Typical methods formulate
    the problem of constructing a diverse ranking as a process of greedy sequential
    document selection. To select an optimal document for a position, it is critical
    for a diverse ranking model to capture the utility of information users have perceived
    from the preceding documents. To explicitly model the utility perceived by the
    users, the construction of a diverse ranking is formalized as a process of sequential
    decision making and the process is modeled as a continuous state Markov decision
    process, referred to as MDP-DIV  [[Xia et al. (2017)](#bib.bib80)]. The ranking
    of $M$ documents is formalized as a sequence of $M$ decisions and each action
    corresponds to selecting one document from the candidate set. In the parameter
    training phase, the policy gradient algorithm of REINFORCE is adopted and the
    expected long-term discounted rewards in terms of the diversity evaluation measure
    is maximized. More works for diversity ranking see [[Feng et al. (2018)](#bib.bib23),
    [Kapoor et al. (2018)](#bib.bib32)]
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了相关性排名之外，另一个重要目标是为查询提供涵盖广泛主题的搜索结果，即搜索结果的多样化[[Santos et al. (2015)](#bib.bib57),
    [Xu et al. (2017)](#bib.bib83)]。典型方法将构建多样化排名的问题形式化为贪心的顺序文档选择过程。为了为一个位置选择最佳文档，多样化排名模型需要捕捉用户从之前文档中感知到的信息的效用。为了明确建模用户感知的效用，多样化排名的构建被形式化为一个顺序决策过程，并将其建模为连续状态马尔可夫决策过程，称为MDP-DIV[[Xia
    et al. (2017)](#bib.bib80)]。$M$个文档的排名被形式化为$M$个决策的序列，每个动作对应于从候选集中选择一个文档。在参数训练阶段，采用了REINFORCE的策略梯度算法，并最大化了在多样性评价指标下的期望长期折扣奖励。有关多样性排名的更多工作见[[Feng
    et al. (2018)](#bib.bib23), [Kapoor et al. (2018)](#bib.bib32)]。
- en: 3.3 Whole-Page Optimization
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 整页优化
- en: To improve user experiences, modern search engines aggregate versatile results
    from different verticals – web-pages, news, images, video, shopping, knowledge
    cards, local maps, etc. Page presentation is broadly defined as the strategy to
    present a set of items on search result page (SERP), which is much more expressive
    than a ranked list. Finding proper presentation for a gallery of heterogeneous
    results is critical for modern search engines. One approach of efficiently learning
    to optimize a large decision space is fractional factorial design. However, the
    method could cause a combinatorial explosion problem with a large search space.
    In [[Hill et al. (2017)](#bib.bib26)], bandit formulation is applied to explore
    the layout space efficiently and hill-climbing is used to select optimal content
    in real-time. The model avoids a combinatorial explosion in model complexity by
    only considering pairwise interactions between page components. This approach
    is a greedy alternating optimization strategy that can run online in real-time.
    In [[Wang et al. (2016)](#bib.bib72), [Wang et al. (2018)](#bib.bib73)], a framework
    is proposed to learn the optimal page presentation to render heterogeneous results
    onto SERP. It leveraged the MDP setting and the agent is designed as the algorithm
    that determines the presentation of page content on a SERP for each incoming search
    query. To solve the critical efficiency problem, it proposed a policy-based learning
    method which can rapidly choose actions from the high-dimensional space.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升用户体验，现代搜索引擎从不同的垂直领域（如网页、新闻、图片、视频、购物、知识卡片、本地地图等）汇总多样化的结果。页面展示被广泛定义为在搜索结果页面（SERP）上呈现一组项目的策略，这比简单的排名列表要表达得丰富得多。为异质结果的画廊找到合适的展示方式对现代搜索引擎至关重要。有效学习优化大决策空间的一种方法是分数因子设计。然而，这种方法可能会导致在大搜索空间中出现组合爆炸问题。在[[Hill
    et al. (2017)](#bib.bib26)]中，采用了多臂老虎机（bandit）模型来高效探索布局空间，并使用爬山算法实时选择最佳内容。该模型通过仅考虑页面组件之间的成对交互，避免了模型复杂性的组合爆炸。这种方法是一种贪心的交替优化策略，可以在线实时运行。在[[Wang
    et al. (2016)](#bib.bib72), [Wang et al. (2018)](#bib.bib73)]中，提出了一个框架来学习最佳页面展示，将异质结果呈现到SERP上。该框架利用了MDP设置，并将代理设计为一个算法，用于确定每个搜索查询在SERP上的页面内容展示。为了解决关键的效率问题，提出了一种基于策略的学习方法，可以迅速从高维空间中选择动作。
- en: 3.4 Session Search
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 会话搜索
- en: 'The task-oriented search includes a series of search iterations triggered by
    the query reformulations within a session. Markov chain in session search is observed:
    user’s judgment of search results in the prior iteration will influence user’s
    behaviors in the next search iteration. Session search is modeled as a dual-agent
    stochastic game based on Partially Observable Markov Decision Process (POMDP)
    in [[Luo et al. (2014)](#bib.bib45)]. They mathematically model dynamics in session
    search as a cooperative game between the user and the search engine, while user
    and the search engine work together in order to jointly maximize the long-term
    cumulative rewards. Log-based document re-ranking is a special type of session
    search that re-ranks documents based on the historical search logs which includes
    the target user’s personalized query log and other users’ search activities. The
    re-ranking aims to offer a better order of the initial retrieved documents [[Zhang
    et al. (2014)](#bib.bib93)]. Nowadays, deep reinforcement learning technology
    has been applied in the E-Commerce search engine [[Hu et al. (2018)](#bib.bib27),
    [Feng et al. (2018)](#bib.bib22)]. For better utilizing the correlation between
    different ranking steps, RL is used to learn an optimal ranking policy which maximizes
    the expected accumulative rewards in a search session [[Hu et al. (2018)](#bib.bib27)].
    It formally defined the multi-step ranking problem in the search session as MDP,
    denoted as SSMDP, and proposed a novel policy gradient algorithm for learning
    an optimal ranking policy, which is able to deal with the problem of high reward
    variance and unbalanced reward distribution. In [[Feng et al. (2018)](#bib.bib22)],
    multi-scenario ranking is formulated as a fully cooperative, partially observable,
    multi-agent sequential decision problem, denoted as MA-RDPG. MA-RDPG has a communication
    component for passing message, several private agents for making action for ranking,
    and a centralized critic for evaluating the overall performance of the co-working
    agents. Agents collaborate with each other by sharing a global action-value function
    and passing messages that encode historical information across scenarios.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 任务导向的搜索包括一系列由查询重构触发的搜索迭代。会话搜索中观察到马尔可夫链的现象：用户在前一次迭代中的搜索结果判断会影响用户在下一次搜索迭代中的行为。会话搜索被建模为基于部分可观察马尔可夫决策过程（POMDP）的双代理随机博弈模型，见[[Luo
    et al. (2014)](#bib.bib45)]。他们将会话搜索中的动态数学建模为用户与搜索引擎之间的合作博弈，而用户与搜索引擎共同协作，以共同最大化长期累积奖励。基于日志的文档重排序是一种特殊类型的会话搜索，它根据历史搜索日志对文档进行重排序，其中包括目标用户的个性化查询日志和其他用户的搜索活动。重排序旨在提供初始检索文档的更好排序，见[[Zhang
    et al. (2014)](#bib.bib93)]。如今，深度强化学习技术已被应用于电子商务搜索引擎中[[Hu et al. (2018)](#bib.bib27),
    [Feng et al. (2018)](#bib.bib22)]。为了更好地利用不同排序步骤之间的相关性，强化学习被用于学习一种最优排序策略，该策略最大化搜索会话中的期望累积奖励[[Hu
    et al. (2018)](#bib.bib27)]。它正式将搜索会话中的多步骤排序问题定义为MDP，记作SSMDP，并提出了一种新颖的策略梯度算法，用于学习一种最优排序策略，该算法能够处理高奖励方差和不平衡奖励分布的问题。在[[Feng
    et al. (2018)](#bib.bib22)]中，多场景排序被形式化为一个完全合作、部分可观察的多代理序列决策问题，记作MA-RDPG。MA-RDPG具有一个用于传递信息的通信组件，若干个用于执行排序操作的私有代理，以及一个用于评估合作代理整体表现的集中式评论者。代理通过共享全局动作价值函数和传递编码跨场景历史信息的消息来相互协作。
- en: 4 Reinforcement Learning for Recommendation
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 强化学习在推荐中的应用
- en: Recommender systems target to capture users’ preferences according to their
    feedback (or behaviors, e.g. rating and review) and suggest items that match their
    preferences. In this section, we briefly review how RL is adapted in several key
    tasks in recommendations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的目标是根据用户的反馈（或行为，如评分和评论）捕捉用户的偏好，并推荐符合其偏好的项目。在本节中，我们简要回顾了强化学习如何在推荐的几个关键任务中得到应用。
- en: 4.1 Exploitation/Exploration Dilemma
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 利用/探索困境
- en: Traditional recommender systems suffer from the exploitation-exploration dilemma,
    where exploitation is to recommend items that are predicted to best match users’
    preferences, while exploration is to recommend items randomly to collect more
    users’ feedback. The contextual bandit models an agent that attempts to balance
    the competing exploitation and exploration tasks in order to maximize the accumulated
    long-term reward over a considered period. The traditional strategies to balance
    exploitation and exploration in bandit setting are $\epsilon$-greedy [[Watkins
    (1989)](#bib.bib74)], EXP3 [[Auer et al. (2002)](#bib.bib3)], and UCB1 [[Auer
    et al. (2002)](#bib.bib2)]. In the news feeds scenario, the exploration/exploitation
    problem of personalized news recommendation is modeled as a contextual bandit
    problem [[Li et al. (2010)](#bib.bib41)], and a learning algorithm LinUCB is proposed
    to select articles sequentially for specific users based on the users’ and articles’
    contextual information, in order to maximize the total user clicks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的推荐系统面临利用-探索困境，其中利用是指推荐预测最符合用户偏好的项目，而探索是指随机推荐项目以收集更多用户反馈。上下文赌博机模型的代理尝试在竞争的利用和探索任务之间取得平衡，以最大化在考虑的时间段内的累计长期奖励。在赌博机设置中平衡利用和探索的传统策略包括$\epsilon$-贪婪
    [[Watkins (1989)](#bib.bib74)]、EXP3 [[Auer et al. (2002)](#bib.bib3)] 和 UCB1 [[Auer
    et al. (2002)](#bib.bib2)]。在新闻推送场景中，个性化新闻推荐的探索/利用问题被建模为上下文赌博机问题 [[Li et al. (2010)](#bib.bib41)]，并提出了一种学习算法LinUCB，该算法根据用户和文章的上下文信息，顺序选择文章以最大化总用户点击量。
- en: 4.2 Temporal Dynamics
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 时间动态
- en: Most existing recommender systems such as collaborative filtering, content-based
    and learning-to-rank have been extensively studied with the stationary environment
    (reward) assumption, where user’s preference is assumed to be static. However,
    this assumption is usually not true in reality since users’ preferences are dynamic,
    thus the reward distributions usually change over time. In bandit setting, it
    usually introduces a variable reward function to delineate the dynamic nature
    of the environment. For instance, the particle learning based dynamical context
    drift model is proposed to model the changing of reward mapping function in multi-armed
    bandit problem, where the drift of the reward mapping function is learned as a
    group of random walk particles, and well fitted particles are dynamically chosen
    to describe the mapping function [[Zeng et al. (2016)](#bib.bib90)]. A contextual
    bandit algorithm is presented to detect the changes of environment according to
    the reward estimation confidence, and updates the arm selection policy accordingly [[Wu
    et al. (2018)](#bib.bib78)]. The change-detection based framework under the piecewise-stationary
    reward assumption for the multi-armed bandit problem is proposed in  [[Liu et al.
    (2018)](#bib.bib42)], where upper confidence bound (UCB) policies is used to detect
    change points actively and restart the UCB indices. Another solution for capturing
    user’s dynamic preference is to introduce the MDP setting [[Chen et al. (2018)](#bib.bib16),
    [Liu et al. (2018)](#bib.bib43), [(97)](#bib.bib97), [Zou et al. (2019)](#bib.bib100)].
    Under the MDP setting, state is introduced to represent user’s preference and
    state transition captures the dynamic nature of user’s preference over time. In [[(97)](#bib.bib97)],
    a user’s dynamic preference (agent’s state) is learned from his/her browsing history.
    Each time the recommender system suggests an item to a user, the user will browse
    this item and provide feedback (skip, click or purchase), which reveals user’s
    satisfaction of the recommended item. According to the feedback, the recommender
    system will update its state to represent user’s new preferences [[(97)](#bib.bib97)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数推荐系统，如协同过滤、基于内容的推荐和学习排序，已经在静态环境（奖励）假设下进行了广泛研究，其中用户的偏好被假定为静态。然而，这一假设在现实中通常不成立，因为用户的偏好是动态的，因此奖励分布通常会随时间变化。在多臂赌博机设置中，通常引入一个变量奖励函数来描绘环境的动态特性。例如，基于粒子的学习动态上下文漂移模型被提出用于建模多臂赌博机问题中的奖励映射函数的变化，其中奖励映射函数的漂移被学习为一组随机游走粒子，并动态选择拟合良好的粒子来描述映射函数[[Zeng
    et al. (2016)](#bib.bib90)]。提出了一种上下文多臂赌博机算法，通过奖励估计置信度来检测环境变化，并相应地更新臂选择策略[[Wu et
    al. (2018)](#bib.bib78)]。在[[Liu et al. (2018)](#bib.bib42)]中，提出了一种基于变化检测的框架，用于多臂赌博机问题的分段静态奖励假设，其中使用上置信界（UCB）策略主动检测变化点并重新启动UCB指标。另一种捕捉用户动态偏好的解决方案是引入MDP设置[[Chen
    et al. (2018)](#bib.bib16), [Liu et al. (2018)](#bib.bib43), [(97)](#bib.bib97),
    [Zou et al. (2019)](#bib.bib100)]。在MDP设置下，引入状态来表示用户的偏好，状态转移捕捉了用户偏好的动态特性。在[[(97)](#bib.bib97)]中，用户的动态偏好（代理状态）是从其浏览历史中学习得来的。每次推荐系统向用户推荐一个项目时，用户将浏览该项目并提供反馈（跳过、点击或购买），这揭示了用户对推荐项目的满意度。根据反馈，推荐系统将更新其状态以表示用户的新偏好[[
    (97)](#bib.bib97)]。
- en: 4.3 Long Term User Engagement
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 长期用户参与
- en: User engagement in recommendation is the assessment of user’s desirable (even
    essential) responses to the items (products, services, or information) suggested
    by the recommender systems [[Lalmas et al. (2014)](#bib.bib40)]. User engagement
    can be measured not only in terms of immediate response (e.g. clicks and rating
    of the recommended items), but more importantly in terms of long-term response
    (e.g. user repetitively purchases) [[(58)](#bib.bib58)]. In [[Wu et al. (2017)](#bib.bib79)],
    the problem of long-term user engagement optimization is formulated as a sequential
    decision making problem. In each iteration, the agent needs to estimate the risk
    of losing a user based on the user’s dynamic response to past recommendations.
    Then, a bandit based method [[Wu et al. (2017)](#bib.bib79)] is introduced to
    balance the immediate user click and the expected future clicks when the user
    revisits the recommender system. In practical recommendation sessions, users will
    sequentially access multiple scenarios, such as the entrance pages and the item
    detail pages, and each scenario has its own recommendation strategy. A multi-agent
    reinforcement learning based approach (DeepChain) is proposed in [[Zhao et al.
    (2019)](#bib.bib96)], which can capture the sequential correlation among different
    scenarios and jointly optimize multiple recommendation strategies. To be specific,
    model-based reinforcement learning technique is introduced to reduce the training
    data requirement and execute more accurate strategy updates. In the news feeds
    scenario [[Zheng et al. (2018)](#bib.bib99)], to incorporate more user feedback
    information, the long-term user response (i.e., how frequent user returns) is
    considered as a supplement to user’s immediate click behaviors, and a Deep Q-Learning
    based framework is proposed to optimize the news recommendation strategies.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 用户参与度在推荐系统中是评估用户对推荐的项目（如产品、服务或信息）所作出的期望（甚至必要）反应的过程[[Lalmas et al. (2014)](#bib.bib40)]。用户参与度不仅可以通过即时反应（例如点击和对推荐项目的评分）来衡量，更重要的是通过长期反应（例如用户的重复购买）来衡量[[(58)](#bib.bib58)]。在[[Wu
    et al. (2017)](#bib.bib79)]中，长期用户参与度优化的问题被表述为一个序列决策问题。在每次迭代中，代理需要根据用户对过去推荐的动态反应来估计失去用户的风险。然后，引入了一种基于赌博的的方法[[Wu
    et al. (2017)](#bib.bib79)]，以平衡用户的即时点击和用户重新访问推荐系统时的预期未来点击。在实际的推荐会话中，用户将依次访问多个场景，如入口页面和项目详细信息页面，每个场景都有其自身的推荐策略。在[[Zhao
    et al. (2019)](#bib.bib96)]中，提出了一种基于多代理强化学习的方法（DeepChain），它可以捕捉不同场景之间的序列相关性，并联合优化多个推荐策略。具体来说，引入了基于模型的强化学习技术，以减少训练数据需求并执行更精确的策略更新。在新闻推荐场景[[Zheng
    et al. (2018)](#bib.bib99)]中，为了融入更多的用户反馈信息，长期用户反应（即用户返回的频率）被作为对用户即时点击行为的补充，并提出了一种基于深度
    Q 学习的框架来优化新闻推荐策略。
- en: 4.4 Page-Wise Recommendation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 页面级推荐
- en: In practical recommender systems, each time users are typically recommended
    a page of items. In this setting, the recommender systems need to jointly (1)
    select a set of complementary and diverse items from a larger candidate item set
    and (2) form an item display (layout configuration) strategy to place the items
    in a 2-D web page that can lead to maximal reward. Given the massive number of
    items, the action space is extremely large if we treat each whole page recommendation
    as one action. To mitigate the issue of the large action space, a Deep Deterministic
    Policy Gradient algorithm is proposed [[Dulac-Arnold et al. (2015)](#bib.bib21)]
    where the Actor generates a deterministic optimal action according to the current
    state, and the Critic outputs the Q-value of this state-action pair. DDPG reduces
    the computational cost of conventional value-based reinforcement learning methods,
    thus it is a fitting choice for the whole page recommendation setting [[Cai et al.
    (2018a)](#bib.bib12), [Cai et al. (2018b)](#bib.bib13)]. Several approaches are
    presented recently to enhance the efficiency [[Choi et al. (2018)](#bib.bib17),
    [Chen et al. (2018)](#bib.bib15)]. In [[(95)](#bib.bib95), [Zhao et al. (2017)](#bib.bib98)],
    CNN techniques are introduced to capture the item display patterns and users’
    feedback of each item in the page. To represent each item, item-embedding, category-embedding
    and feedback embedding are leveraged, which can help to generate complementary
    and diverse recommendations and capture user’s interests within the pages. Bandit
    techniques are also leveraged for whole-page Recommendations [[Wang et al. (2017)](#bib.bib71),
    [Lacerda (2017)](#bib.bib38)]. For instance, the whole page recommendation task
    is considered as a combinatorial semi-bandit problem, where the system recommends
    $S$ actions from a candidate set of $K$ actions, and displays the selected items
    in $S$ (out of $M$) positions [[Wang et al. (2017)](#bib.bib71)].
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的推荐系统中，通常每次会向用户推荐一页项目。在这种情况下，推荐系统需要同时（1）从更大的候选项目集中选择一组互补且多样化的项目，以及（2）制定一个项目展示（布局配置）策略，将这些项目放置在一个二维网页中，以实现最大的奖励。考虑到大量的项目，如果我们将每整页推荐视为一个动作，则动作空间极其庞大。为了解决这个大动作空间的问题，提出了一种深度确定性策略梯度算法 [[Dulac-Arnold
    et al. (2015)](#bib.bib21)]，其中Actor根据当前状态生成一个确定性的最优动作，而Critic输出该状态-动作对的Q值。DDPG降低了传统基于价值的强化学习方法的计算成本，因此它适用于整个页面推荐设置 [[Cai
    et al. (2018a)](#bib.bib12), [Cai et al. (2018b)](#bib.bib13)]。最近提出了几种方法来提高效率 [[Choi
    et al. (2018)](#bib.bib17), [Chen et al. (2018)](#bib.bib15)]。在 [[(95)](#bib.bib95),
    [Zhao et al. (2017)](#bib.bib98)]中，引入了CNN技术来捕捉每个页面项目的展示模式和用户反馈。为了表示每个项目，利用了项目嵌入、类别嵌入和反馈嵌入，这有助于生成互补且多样化的推荐，并捕捉页面内用户的兴趣。还利用了带有技术来进行整页推荐 [[Wang
    et al. (2017)](#bib.bib71), [Lacerda (2017)](#bib.bib38)]。例如，将整页推荐任务视为组合半带问题，其中系统从候选集$K$个动作中推荐$S$个动作，并将选定的项目展示在$S$（从$M$中选出）个位置中 [[Wang
    et al. (2017)](#bib.bib71)]。
- en: 5 Reinforcement Learning for Online Advertising
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在线广告中的强化学习**  '
- en: The goal of online advertising is to assign the right advertisements to the
    right users so as to maximize the revenue, click-through rate (CTR) or return
    on investment (ROI) of the advertising campaign. The two main marketing strategy
    in online advertising are guaranteed delivery (GD) and real-time bidding (RTB).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在线广告的目标是将合适的广告分配给合适的用户，以最大化广告活动的收入、点击率（CTR）或投资回报率（ROI）。在线广告的两种主要营销策略是保证投放（GD）和实时竞价（RTB）。
- en: 5.1 Guaranteed Delivery
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**5.1 保证投放**'
- en: In guaranteed delivery, advertisements that share a single idea and theme are
    grouped into campaigns, and are charged on a pay-per-campaign basis for the pre-specified
    number of deliveries (click or impressions) [[Salomatin et al. (2012)](#bib.bib56)].
    Most popular GD (Guaranteed Delivery) solutions are based on offline optimization
    algorithms, and then adjusted for online setup. However, deriving the optimal
    strategy to allocate impressions is challenging, especially when the environment
    is unstable in real-world application. In [[Wu et al. (2018)](#bib.bib76)], a
    multi-agent reinforcement learning (MARL) approach is proposed to derive cooperative
    policies for the publisher to maximize its target in an unstable environment.
    They formulated the impression allocation problem as an auction problem where
    each contract can submit virtual bids for individual impressions. With this formulation,
    they derived the optimal impression allocation strategy by solving the optimal
    bidding functions for contracts.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在保证交付（GD）中，共享单一创意和主题的广告被分组为广告系列，并按照预定的交付数量（点击或展示）按广告系列收费[[萨洛马廷等（2012）](#bib.bib56)]。最流行的保证交付（GD）解决方案基于离线优化算法，然后调整为在线设置。然而，推导出分配展示的最优策略是具有挑战性的，特别是当环境在实际应用中不稳定时。在[[吴等（2018）](#bib.bib76)]中，提出了一种多智能体强化学习（MARL）方法，以在不稳定环境中为发布者推导合作策略以最大化其目标。他们将展示分配问题建模为拍卖问题，其中每个合同可以为单独的展示提交虚拟竞标。通过这种建模方法，他们通过解决合同的最优竞标函数推导出了最优的展示分配策略。
- en: 5.2 Real-Time Bidding
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 实时竞价
- en: RTB allows an advertiser to submit a bid for each individual impression in a
    very short time frame. Ad selection task is typically modeled as multi-armed bandit
    (MAB) problem with the setting that samples from each arm are iid, feedback is
    immediate and rewards are stationary [[Yang and Lu (2016)](#bib.bib85), [Nuara
    et al. (2018)](#bib.bib51), [Gasparini et al. (2018)](#bib.bib25), [Tang et al.
    (2013)](#bib.bib66), [Xu et al. (2013)](#bib.bib84), [Yuan et al. (2013)](#bib.bib88),
    [Schwartz et al. (2017)](#bib.bib59)]. The payoff functions of a MAB are allowed
    to evolve, but they are assumed to evolve slowly over time. On the other hand,
    display ads created while others are removed regularly in an advertising campaign
    circulation. The problem of multi-armed bandits with budget constraints and variable
    costs is studied in [[(20)](#bib.bib20)]. In this case, pulling the arms of bandit
    will get random rewards with random costs, and the algorithm aims to maximize
    the long-term reward by pulling arms with a constrained budget. This setting can
    model Internet advertising in a more precise way than previous works where pulling
    an arm is costless or has a fixed cost.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实时竞价（RTB）允许广告主在非常短的时间内为每一个独立的展示提交竞标。广告选择任务通常被建模为多臂老虎机（MAB）问题，其设定为每个臂的样本是独立同分布的，反馈是即时的，奖励是稳定的[[杨和陆（2016）](#bib.bib85)，[努阿拉等（2018）](#bib.bib51)，[加斯帕里尼等（2018）](#bib.bib25)，[汤等（2013）](#bib.bib66)，[许等（2013）](#bib.bib84)，[袁等（2013）](#bib.bib88)，[施瓦茨等（2017）](#bib.bib59)]。MAB
    的收益函数可以演变，但假设它们随着时间的推移变化缓慢。另一方面，展示广告在广告活动期间被定期创建和移除。在[[(20)](#bib.bib20)]中研究了具有预算约束和可变成本的多臂老虎机问题。在这种情况下，拉动老虎机的臂会得到具有随机成本的随机奖励，算法旨在通过在受限预算下拉动臂来最大化长期奖励。这种设定比之前的研究中拉动一个臂是无成本或有固定成本的情况更能精确地建模互联网广告。
- en: 'Under the MAB setting, the bid decision is considered as a static optimization
    problem of either treating the value of each impression independently or setting
    a bid price to each segment of ad volume. However, the bidding for a given ad
    campaign would repeatedly happen during its life span before the budget running
    out. Thus, the MDP setting have also been studied [[Cai et al. (2017)](#bib.bib11),
    [Tang (2017)](#bib.bib67), [Wang et al. (2018)](#bib.bib70), [Zhao et al. (2018)](#bib.bib94),
    [Rohde et al. (2018)](#bib.bib54), [Wu et al. (2018)](#bib.bib77), [Jin et al.
    (2018)](#bib.bib28)]. A model-based reinforcement learning framework is proposed
    to learn bid strategies in RTB advertising [[Cai et al. (2017)](#bib.bib11)],
    where neural network is used to approximate the state value, which can better
    deal with the scalability problem of large auction volume and limited campaign
    budget. A model-free deep reinforcement learning method is proposed to solve the
    bidding problem with constrained budget [[Wu et al. (2018)](#bib.bib77)]: the
    problem is modeled as a $\lambda$-control problem, and RewardNet is designed for
    generating rewards to solve reward design trap, instead of using the immediate
    reward. A multi-agent bidding model is presented, which takes the other advertisers’
    bidding in the system into consideration, and a clustering approach is introduced
    to solve the large number of advertisers challenge [[Jin et al. (2018)](#bib.bib28)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在多臂赌博机（MAB）设置下，竞标决策被视为静态优化问题，可以选择独立处理每次展示的价值，或者为每个广告量段设置竞标价格。然而，给定广告活动的竞标将在其生命周期内重复进行，直到预算用尽。因此，马尔可夫决策过程（MDP）设置也得到了研究[[Cai
    et al. (2017)](#bib.bib11)、[Tang (2017)](#bib.bib67)、[Wang et al. (2018)](#bib.bib70)、[Zhao
    et al. (2018)](#bib.bib94)、[Rohde et al. (2018)](#bib.bib54)、[Wu et al. (2018)](#bib.bib77)、[Jin
    et al. (2018)](#bib.bib28)]。提出了一种基于模型的强化学习框架来学习实时竞标（RTB）广告中的竞标策略[[Cai et al. (2017)](#bib.bib11)]，其中使用神经网络来近似状态值，这可以更好地处理大规模拍卖量和有限广告预算的问题。提出了一种无模型深度强化学习方法来解决有限预算下的竞标问题[[Wu
    et al. (2018)](#bib.bib77)]：该问题被建模为一个$\lambda$-控制问题，并设计了RewardNet用于生成奖励以解决奖励设计陷阱，而不是使用即时奖励。提出了一种多智能体竞标模型，考虑了系统中其他广告商的竞标情况，并引入了聚类方法来应对大量广告商的挑战[[Jin
    et al. (2018)](#bib.bib28)]。
- en: 6 Conclusion and Future Directions
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来方向
- en: In this article, we present an overview of information seeking from the reinforcement
    learning perspective. We first introduce mathematical foundations of RL based
    information seeking approaches. Then we review state-of-the-art algorithms of
    three representative information seeking mechanisms – search, recommendations,
    and advertising. Next, we here discuss some interesting research directions on
    reinforcement learning that can bring the information seeking research into a
    new frontier.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们从强化学习的角度对信息寻求进行概述。我们首先介绍了基于强化学习的信息寻求方法的数学基础。然后，我们回顾了三种代表性的信息寻求机制——搜索、推荐和广告——的最先进算法。接下来，我们讨论了强化学习中的一些有趣研究方向，这些方向可以将信息寻求研究带入新的前沿。
- en: First, most of the existing works train a policy within one scenario, while
    overlooking users’ behaviors (preference) in other scenarios [[Feng et al. (2018)](#bib.bib22)].
    This will result in a suboptimal policy, which calls for collaborative RL frameworks
    that consider search, recommendation and advertising scenarios simultaneously.
    Second, the type of reward function varies among different computational tasks.
    More sophisticated reward functions should be designed to achieve more goals of
    information seeking, such as increasing the supervising degree of recommendations.
    Third, more types of user-agent interactions could be incorporated into RL frameworks,
    such as adding items into shopping cart, users’ repeat purchase behavior, users’
    dwelling time in the system, and user’s chatting with customer service representatives
    or agent of AI dialog system. Fourth, testing a new algorithm is expensive since
    it needs lots of engineering efforts to deploy the algorithm in the practical
    system, and it also may have negative impacts on user experience if the algorithm
    is not mature. Thus online environment simulator or offline evaluation method
    based on historical logs are necessary to pre-train and evaluate new algorithms
    before launching them online. Finally, there is an increasing demand for an open
    online reinforcement learning environment for information seeking, which can advance
    the RL and information seeking communities and achieve better consistency between
    offline and online performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，大多数现有工作仅在单一场景下训练策略，而忽视了其他场景中用户的行为（偏好）[[Feng 等人（2018）](#bib.bib22)]。这将导致次优策略，需采用协作式强化学习框架，同时考虑搜索、推荐和广告场景。其次，不同计算任务中的奖励函数类型各异。应设计更复杂的奖励函数以实现更多的信息获取目标，例如增加推荐的监督程度。第三，可以将更多类型的用户-代理交互纳入强化学习框架，例如将商品添加到购物车、用户的重复购买行为、用户在系统中的停留时间，以及用户与客服代表或
    AI 对话系统代理的聊天。第四，测试新算法成本高，因为在实际系统中部署算法需要大量工程工作，且若算法不成熟可能对用户体验产生负面影响。因此，在上线前需要在线环境模拟器或基于历史日志的离线评估方法来预训练和评估新算法。最后，对信息获取的开放在线强化学习环境的需求日益增加，这可以推动强化学习和信息获取社区的发展，并实现离线和在线性能之间更好的一致性。
- en: ACKNOWLEDGEMENTS
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Xiangyu Zhao and Jiliang Tang are supported by the National Science Foundation
    (NSF) under grant numbers IIS-1714741, IIS-1715940 and CNS-1815636, and a grant
    from Criteo Faculty Research Award.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Xiangyu Zhao 和 Jiliang Tang 得到了国家科学基金会（NSF）资助，资助编号 IIS-1714741、IIS-1715940 和
    CNS-1815636，以及来自 Criteo Faculty Research Award 的资助。
- en: References
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Åström (1965) Åström, K. J. 1965. Optimal control of markov processes with incomplete
    state information. Journal of Mathematical Analysis and Applications 10, 1, 174–205.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Åström（1965）Åström, K. J. 1965. 马尔可夫过程的最优控制，具有不完全状态信息。数学分析与应用杂志 10, 1, 174–205。
- en: Auer et al. (2002) Auer, P., Cesa-Bianchi, N., and Fischer, P. 2002. Finite-time
    analysis of the multiarmed bandit problem. Machine Learning 47, 2-3, 235–256.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auer 等人（2002）Auer, P., Cesa-Bianchi, N., 和 Fischer, P. 2002. 多臂赌博机问题的有限时间分析。机器学习
    47, 2-3, 235–256。
- en: Auer et al. (2002) Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E.
    2002. The nonstochastic multiarmed bandit problem. SIAM J. Comput. 32, 1, 48–77.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auer 等人（2002）Auer, P., Cesa-Bianchi, N., Freund, Y., 和 Schapire, R. E. 2002.
    非随机多臂赌博机问题。SIAM 计算杂志 32, 1, 48–77。
- en: Baxter and Bartlett (2001) Baxter, J. and Bartlett, P. L. 2001. Infinite-horizon
    policy-gradient estimation. J. Artif. Intell. Res. 15, 319–350.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baxter 和 Bartlett（2001）Baxter, J. 和 Bartlett, P. L. 2001. 无限时间视野策略梯度估计。人工智能研究杂志
    15, 319–350。
- en: Bellman (2013) Bellman, R. 2013. Dynamic programming. Courier Corporation.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman（2013）Bellman, R. 2013. 动态规划。Courier Corporation。
- en: Bhatnagar et al. (2007) Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee,
    M. 2007. Incremental natural actor-critic algorithms. In NIPS ’07.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatnagar 等人（2007）Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., 和 Lee, M. 2007.
    增量自然演员-评论家算法。见 NIPS ’07。
- en: Bhatnagar et al. (2009) Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee,
    M. 2009. Natural actor-critic algorithms. Automatica 45, 11, 2471–2482.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatnagar 等人（2009）Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., 和 Lee, M. 2009.
    自然演员-评论家算法。自动化 45, 11, 2471–2482。
- en: Bowling and Veloso (2002) Bowling, M. H. and Veloso, M. M. 2002. Multiagent
    learning using a variable learning rate. Artif. Intell. 136, 2, 215–250.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowling 和 Veloso（2002）Bowling, M. H. 和 Veloso, M. M. 2002. 使用可变学习率的多智能体学习。人工智能
    136, 2, 215–250。
- en: Busoniu et al. (2010) Busoniu, L., Babuska, R., De Schutter, B., and Ernst,
    D. 2010. Reinforcement learning and dynamic programming using function approximators.
    CRC press.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Busoniu et al. (2010) Busoniu, L., Babuska, R., De Schutter, B., 和 Ernst, D.
    2010. 使用函数逼近器的强化学习和动态规划。CRC出版社。
- en: Busoniu et al. (2008) Busoniu, L., Babuska, R., and Schutter, B. D. 2008. A
    comprehensive survey of multiagent reinforcement learning. IEEE Trans. Systems,
    Man, and Cybernetics, Part C 38, 2, 156–172.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Busoniu et al. (2008) Busoniu, L., Babuska, R., 和 Schutter, B. D. 2008. 多智能体强化学习的全面调查。IEEE
    Trans. Systems, Man, and Cybernetics, Part C 38, 2, 156–172。
- en: Cai et al. (2017) Cai, H., Ren, K., Zhang, W., Malialis, K., Wang, J., Yu, Y.,
    and Guo, D. 2017. Real-time bidding by reinforcement learning in display advertising.
    In WSDM ’17.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2017) Cai, H., Ren, K., Zhang, W., Malialis, K., Wang, J., Yu, Y.,
    和 Guo, D. 2017. 基于强化学习的实时竞价广告。发表于WSDM ’17。
- en: Cai et al. (2018a) Cai, Q., Filos-Ratsikas, A., Tang, P., and Zhang, Y. 2018a.
    Reinforcement mechanism design for e-commerce. In WWW ’18.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2018a) Cai, Q., Filos-Ratsikas, A., Tang, P., 和 Zhang, Y. 2018a.
    电子商务中的强化机制设计。发表于WWW ’18。
- en: Cai et al. (2018b) Cai, Q., Filos-Ratsikas, A., Tang, P., and Zhang, Y. 2018b.
    Reinforcement mechanism design for fraudulent behaviour in e-commerce. In AAAI
    ’18.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2018b) Cai, Q., Filos-Ratsikas, A., Tang, P., 和 Zhang, Y. 2018b.
    针对电子商务中欺诈行为的强化机制设计。发表于AAAI ’18。
- en: Chang et al. (2006) Chang, C., Kayed, M., Girgis, M. R., and Shaalan, K. F.
    2006. A survey of web information extraction systems. IEEE Trans. Knowl. Data
    Eng. 18, 10, 1411–1428.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang et al. (2006) Chang, C., Kayed, M., Girgis, M. R., 和 Shaalan, K. F. 2006.
    网络信息提取系统的调查。IEEE Trans. Knowl. Data Eng. 18, 10, 1411–1428。
- en: Chen et al. (2018) Chen, H., Dai, X., Cai, H., Zhang, W., Wang, X., Tang, R.,
    Zhang, Y., and Yu, Y. 2018. Large-scale interactive recommendation with tree-structured
    policy gradient. CoRR abs/1811.05869.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2018) Chen, H., Dai, X., Cai, H., Zhang, W., Wang, X., Tang, R.,
    Zhang, Y., 和 Yu, Y. 2018. 基于树结构策略梯度的大规模互动推荐。CoRR abs/1811.05869。
- en: Chen et al. (2018) Chen, S., Yu, Y., Da, Q., Tan, J., Huang, H., and Tang, H.
    2018. Stabilizing reinforcement learning in dynamic environment with application
    to online recommendation. In SIGKDD ’18.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2018) Chen, S., Yu, Y., Da, Q., Tan, J., Huang, H., 和 Tang, H.
    2018. 在动态环境中稳定强化学习并应用于在线推荐。发表于SIGKDD ’18。
- en: Choi et al. (2018) Choi, S., Ha, H., Hwang, U., Kim, C., Ha, J., and Yoon, S.
    2018. Reinforcement learning based recommender system using biclustering technique.
    CoRR abs/1801.05532.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi et al. (2018) Choi, S., Ha, H., Hwang, U., Kim, C., Ha, J., 和 Yoon, S.
    2018. 基于强化学习的推荐系统，使用双聚类技术。CoRR abs/1801.05532。
- en: Croft et al. (2010) Croft, W. B., Bendersky, M., Li, H., and Xu, G. 2010. Query
    representation and understanding workshop. SIGIR Forum 44, 2, 48–53.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croft et al. (2010) Croft, W. B., Bendersky, M., Li, H., 和 Xu, G. 2010. 查询表示与理解研讨会。SIGIR
    Forum 44, 2, 48–53。
- en: 'Deisenroth and Rasmussen (2011) Deisenroth, M. P. and Rasmussen, C. E. 2011.
    PILCO: A model-based and data-efficient approach to policy search. In ICML ’11.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deisenroth and Rasmussen (2011) Deisenroth, M. P. 和 Rasmussen, C. E. 2011. PILCO：一种基于模型且数据高效的策略搜索方法。发表于ICML
    ’11。
- en: (20) Ding, W., Qin, T., Zhang, X., and Liu, T. Multi-armed bandit with budget
    constraint and variable costs. In AAAI ’13.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) Ding, W., Qin, T., Zhang, X., 和 Liu, T. 多臂赌博机与预算约束及变动成本。发表于AAAI ’13。
- en: Dulac-Arnold et al. (2015) Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag,
    P., Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., and Coppin, B. 2015.
    Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dulac-Arnold et al. (2015) Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag,
    P., Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., 和 Coppin, B. 2015.
    大规模离散动作空间中的深度强化学习。arXiv预印本 arXiv:1512.07679。
- en: 'Feng et al. (2018) Feng, J., Li, H., Huang, M., Liu, S., Ou, W., Wang, Z.,
    and Zhu, X. 2018. Learning to collaborate: Multi-scenario ranking via multi-agent
    reinforcement learning. In WWW ’18.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2018) Feng, J., Li, H., Huang, M., Liu, S., Ou, W., Wang, Z., 和
    Zhu, X. 2018. 学会协作：通过多智能体强化学习进行多场景排序。发表于WWW ’18。
- en: 'Feng et al. (2018) Feng, Y., Xu, J., Lan, Y., Guo, J., Zeng, W., and Cheng,
    X. 2018. From greedy selection to exploratory decision-making: Diverse ranking
    with policy-value networks. In SIGIR ’18.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2018) Feng, Y., Xu, J., Lan, Y., Guo, J., Zeng, W., 和 Cheng, X.
    2018. 从贪婪选择到探索性决策：具有策略价值网络的多样化排序。发表于SIGIR ’18。
- en: 'Garcia-Molina et al. (2011) Garcia-Molina, H., Koutrika, G., and Parameswaran,
    A. G. 2011. Information seeking: convergence of search, recommendations, and advertising.
    Commun. ACM 54, 11, 121–130.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garcia-Molina et al. (2011) Garcia-Molina, H., Koutrika, G., 和 Parameswaran,
    A. G. 2011. 信息检索：搜索、推荐和广告的融合。Commun. ACM 54, 11, 121–130。
- en: Gasparini et al. (2018) Gasparini, M., Nuara, A., Trovò, F., Gatti, N., and
    Restelli, M. 2018. Targeting optimization for internet advertising by learning
    from logged bandit feedback. In IJCNN ’18.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gasparini 等（2018）Gasparini, M., Nuara, A., Trovò, F., Gatti, N., and Restelli,
    M. 2018. 通过学习已记录的赌徒反馈进行互联网广告的目标优化。发表于 IJCNN ’18。
- en: Hill et al. (2017) Hill, D. N., Nassif, H., Liu, Y., Iyer, A., and Vishwanathan,
    S. V. N. 2017. An efficient bandit algorithm for realtime multivariate optimization.
    In SIGKDD ’17.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill 等（2017）Hill, D. N., Nassif, H., Liu, Y., Iyer, A., and Vishwanathan, S.
    V. N. 2017. 实时多变量优化的高效带式算法。发表于 SIGKDD ’17。
- en: 'Hu et al. (2018) Hu, Y., Da, Q., Zeng, A., Yu, Y., and Xu, Y. 2018. Reinforcement
    learning to rank in e-commerce search engine: Formalization, analysis, and application.
    In SIGKDD ’18.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2018）Hu, Y., Da, Q., Zeng, A., Yu, Y., and Xu, Y. 2018. 电子商务搜索引擎中的强化学习排序：形式化、分析与应用。发表于
    SIGKDD ’18。
- en: Jin et al. (2018) Jin, J., Song, C., Li, H., Gai, K., Wang, J., and Zhang, W.
    2018. Real-time bidding with multi-agent reinforcement learning in display advertising.
    In CIKM ’18.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等（2018）Jin, J., Song, C., Li, H., Gai, K., Wang, J., and Zhang, W. 2018.
    在展示广告中使用多智能体强化学习进行实时竞标。发表于 CIKM ’18。
- en: Kaelbling et al. (1998) Kaelbling, L. P., Littman, M. L., and Cassandra, A. R.
    1998. Planning and acting in partially observable stochastic domains. Artif. Intell. 101, 1-2,
    99–134.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaelbling 等（1998）Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. 1998.
    在部分可观察的随机领域中规划与行动。Artif. Intell. 101, 1-2, 99–134。
- en: 'Kaelbling et al. (1996) Kaelbling, L. P., Littman, M. L., and Moore, A. W.
    1996. Reinforcement learning: A survey. J. Artif. Intell. Res. 4, 237–285.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaelbling 等（1996）Kaelbling, L. P., Littman, M. L., and Moore, A. W. 1996. 强化学习：综述。J.
    Artif. Intell. Res. 4, 237–285。
- en: Kakade (2001) Kakade, S. 2001. A natural policy gradient. In NIPS ’01.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kakade（2001）Kakade, S. 2001. 自然策略梯度。发表于 NIPS ’01。
- en: Kapoor et al. (2018) Kapoor, S., Keswani, V., Vishnoi, N. K., and Celis, L. E.
    2018. Balanced news using constrained bandit-based personalization. In IJCAI ’18.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapoor 等（2018）Kapoor, S., Keswani, V., Vishnoi, N. K., and Celis, L. E. 2018.
    使用约束的赌徒基础个性化实现平衡新闻。发表于 IJCAI ’18。
- en: Katariya et al. (2017) Katariya, S., Kveton, B., Szepesvári, C., Vernade, C.,
    and Wen, Z. 2017. Bernoulli rank-1 bandits for click feedback. In IJCAI ’17.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katariya 等（2017）Katariya, S., Kveton, B., Szepesvári, C., Vernade, C., and Wen,
    Z. 2017. 伯努利 rank-1 赌徒用于点击反馈。发表于 IJCAI ’17。
- en: 'Katariya et al. (2016) Katariya, S., Kveton, B., Szepesvári, C., and Wen, Z.
    2016. DCM bandits: Learning to rank with multiple clicks. In ICML ’16.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katariya 等（2016）Katariya, S., Kveton, B., Szepesvári, C., and Wen, Z. 2016.
    DCM 赌徒：通过多次点击进行排序学习。发表于 ICML ’16。
- en: Konda and Tsitsiklis (1999) Konda, V. R. and Tsitsiklis, J. N. 1999. Actor-critic
    algorithms. In NIPS ’99.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konda 和 Tsitsiklis（1999）Konda, V. R. and Tsitsiklis, J. N. 1999. Actor-critic
    算法。发表于 NIPS ’99。
- en: Kröse (1995) Kröse, B. J. A. 1995. Learning from delayed rewards. Robotics and
    Autonomous Systems 15, 4, 233–235.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kröse（1995）Krésé, B. J. A. 1995. 从延迟奖励中学习。Robotics and Autonomous Systems 15,
    4, 233–235。
- en: 'Kveton et al. (2015) Kveton, B., Szepesvári, C., Wen, Z., and Ashkan, A. 2015.
    Cascading bandits: Learning to rank in the cascade model. In ICML ’15.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kveton 等（2015）Kveton, B., Szepesvári, C., Wen, Z., and Ashkan, A. 2015. 级联赌徒：在级联模型中学习排序。发表于
    ICML ’15。
- en: Lacerda (2017) Lacerda, A. 2017. Multi-objective ranked bandits for recommender
    systems. Neurocomputing 246, 12–24.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lacerda（2017）Lacerda, A. 2017. 多目标排序赌徒在推荐系统中的应用。Neurocomputing 246, 12–24。
- en: Lagoudakis and Parr (2003) Lagoudakis, M. G. and Parr, R. 2003. Least-squares
    policy iteration. Journal of Machine Learning Research 4, 1107–1149.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagoudakis 和 Parr（2003）Lagoudakis, M. G. and Parr, R. 2003. 最小二乘策略迭代。Journal
    of Machine Learning Research 4, 1107–1149。
- en: Lalmas et al. (2014) Lalmas, M., O’Brien, H., and Yom-Tov, E. 2014. Measuring
    User Engagement. Synthesis Lectures on Information Concepts, Retrieval, and Services.
    Morgan & Claypool Publishers.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lalmas 等（2014）Lalmas, M., O’Brien, H., and Yom-Tov, E. 2014. 用户参与度测量。信息概念、检索与服务综合讲座。Morgan
    & Claypool Publishers。
- en: Li et al. (2010) Li, L., Chu, W., Langford, J., and Schapire, R. E. 2010. A
    contextual-bandit approach to personalized news article recommendation. In WWW
    ’10.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2010）Li, L., Chu, W., Langford, J., and Schapire, R. E. 2010. 上下文赌徒方法进行个性化新闻推荐。发表于
    WWW ’10。
- en: Liu et al. (2018) Liu, F., Lee, J., and Shroff, N. B. 2018. A change-detection
    based framework for piecewise-stationary multi-armed bandit problem. In AAAI ’18.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2018）Liu, F., Lee, J., and Shroff, N. B. 2018. 基于变化检测的分段静态多臂赌徒问题框架。发表于
    AAAI ’18。
- en: Liu et al. (2018) Liu, F., Tang, R., Li, X., Ye, Y., Chen, H., Guo, H., and
    Zhang, Y. 2018. Deep reinforcement learning based recommendation with explicit
    user-item interactions modeling. CoRR abs/1810.12027.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2018）Liu, F., Tang, R., Li, X., Ye, Y., Chen, H., Guo, H., and Zhang,
    Y. 2018. 基于深度强化学习的推荐系统，具有显式的用户-项目交互建模。CoRR abs/1810.12027。
- en: Lu et al. (2010) Lu, T., Pál, D., and Pal, M. 2010. Contextual multi-armed bandits.
    In AISTATS ’10.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu等（2010）Lu, T., Pál, D., 和 Pal, M. 2010. 上下文多臂强盗。发表于AISTATS ’10。
- en: 'Luo et al. (2014) Luo, J., Zhang, S., and Yang, H. 2014. Win-win search: dual-agent
    stochastic game in session search. In SIGIR ’14.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo等（2014）Luo, J., Zhang, S., 和 Yang, H. 2014. 双赢搜索：会话搜索中的双代理随机游戏。发表于SIGIR ’14。
- en: Lv and Zhai (2009) Lv, Y. and Zhai, C. 2009. Adaptive relevance feedback in
    information retrieval. In CIKM ’09.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv和Zhai（2009）Lv, Y. 和 Zhai, C. 2009. 信息检索中的自适应相关反馈。发表于CIKM ’09。
- en: Mnih et al. (2015) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
    J., Bellemare, M. G., Graves, A., Riedmiller, M. A., Fidjeland, A., Ostrovski,
    G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D.,
    Wierstra, D., Legg, S., and Hassabis, D. 2015. Human-level control through deep
    reinforcement learning. Nature 518, 7540, 529–533.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih等（2015）Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare,
    M. G., Graves, A., Riedmiller, M. A., Fidjeland, A., Ostrovski, G., Petersen,
    S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D.,
    Legg, S., 和 Hassabis, D. 2015. 通过深度强化学习实现人类水平的控制。自然 518, 7540, 529–533。
- en: 'Moore and Atkeson (1993) Moore, A. W. and Atkeson, C. G. 1993. Prioritized
    sweeping: Reinforcement learning with less data and less time. Machine Learning 13,
    103–130.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moore和Atkeson（1993）Moore, A. W. 和 Atkeson, C. G. 1993. 优先级扫描：用更少的数据和时间进行强化学习。机器学习
    13, 103–130。
- en: Nogueira et al. (2018) Nogueira, R., Bulian, J., and Ciaramita, M. 2018. Learning
    to coordinate multiple reinforcement learning agents for diverse query reformulation.
    CoRR abs/1809.10658.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira等（2018）Nogueira, R., Bulian, J., 和 Ciaramita, M. 2018. 学习协调多个强化学习代理以进行多样化查询重构。CoRR
    abs/1809.10658。
- en: Nogueira and Cho (2017) Nogueira, R. and Cho, K. 2017. Task-oriented query reformulation
    with reinforcement learning. In EMNLP ’17.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira和Cho（2017）Nogueira, R. 和 Cho, K. 2017. 任务导向的查询重构与强化学习。发表于EMNLP ’17。
- en: Nuara et al. (2018) Nuara, A., Trovò, F., Gatti, N., and Restelli, M. 2018.
    A combinatorial-bandit algorithm for the online joint bid/budget optimization
    of pay-per-click advertising campaigns. In AAAI ’18.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nuara等（2018）Nuara, A., Trovò, F., Gatti, N., 和 Restelli, M. 2018. 用于点击付费广告活动的在线联合出价/预算优化的组合-强盗算法。发表于AAAI
    ’18。
- en: Peters and Schaal (2008) Peters, J. and Schaal, S. 2008. Natural actor-critic.
    Neurocomputing 71, 7-9, 1180–1190.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters和Schaal（2008）Peters, J. 和 Schaal, S. 2008. 自然演员-评论家。神经计算 71, 7-9, 1180–1190。
- en: Peters et al. (2005) Peters, J., Vijayakumar, S., and Schaal, S. 2005. Natural
    actor-critic. In ECML ’05.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters等（2005）Peters, J., Vijayakumar, S., 和 Schaal, S. 2005. 自然演员-评论家。发表于ECML
    ’05。
- en: 'Rohde et al. (2018) Rohde, D., Bonner, S., Dunlop, T., Vasile, F., and Karatzoglou,
    A. 2018. Recogym: A reinforcement learning environment for the problem of product
    recommendation in online advertising. CoRR abs/1808.00720.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rohde等（2018）Rohde, D., Bonner, S., Dunlop, T., Vasile, F., 和 Karatzoglou, A.
    2018. Recogym：用于在线广告中产品推荐问题的强化学习环境。CoRR abs/1808.00720。
- en: Rummery and Niranjan (1994) Rummery, G. A. and Niranjan, M. 1994. On-line Q-learning
    using connectionist systems. Vol. 37. University of Cambridge, Department of Engineering
    Cambridge, England.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rummery和Niranjan（1994）Rummery, G. A. 和 Niranjan, M. 1994. 使用连接主义系统的在线Q学习。第37卷。剑桥大学，工程系，剑桥，英国。
- en: Salomatin et al. (2012) Salomatin, K., Liu, T., and Yang, Y. 2012. A unified
    optimization framework for auction and guaranteed delivery in online advertising.
    In CIKM ’12.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salomatin等（2012）Salomatin, K., Liu, T., 和 Yang, Y. 2012. 在线广告中拍卖和保证交付的统一优化框架。发表于CIKM
    ’12。
- en: Santos et al. (2015) Santos, R. L. T., MacDonald, C., and Ounis, I. 2015. Search
    result diversification. Foundations and Trends in Information Retrieval 9, 1,
    1–90.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santos等（2015）Santos, R. L. T., MacDonald, C., 和 Ounis, I. 2015. 搜索结果多样化。信息检索基础与趋势
    9, 1, 1–90。
- en: (58) Schopfer, S. and Keller, T. Long term recommender benchmarking for mobile
    shopping list applications using markov chains. In RecSys ’14.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) Schopfer, S. 和 Keller, T. 移动购物清单应用的长期推荐基准测试，使用马尔可夫链。发表于RecSys ’14。
- en: Schwartz et al. (2017) Schwartz, E. M., Bradlow, E. T., and Fader, P. S. 2017.
    Customer acquisition via display advertising using multi-armed bandit experiments.
    Marketing Science 36, 4, 500–522.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwartz等（2017）Schwartz, E. M., Bradlow, E. T., 和 Fader, P. S. 2017. 通过展示广告进行客户获取，使用多臂强盗实验。市场营销科学
    36, 4, 500–522。
- en: Shani et al. (2005) Shani, G., Heckerman, D., and Brafman, R. I. 2005. An mdp-based
    recommender system. Journal of Machine Learning Research 6, 1265–1295.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shani等（2005）Shani, G., Heckerman, D., 和 Brafman, R. I. 2005. 基于MDP的推荐系统。机器学习研究杂志
    6, 1265–1295。
- en: 'Shoham et al. (2003) Shoham, Y., Powers, R., and Grenager, T. 2003. Multi-agent
    reinforcement learning: a critical survey. Tech. rep., Technical report, Stanford
    University.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shoham 等人（2003）Shoham, Y., Powers, R., 和 Grenager, T. 2003. 多智能体强化学习：一项关键调查。技术报告，斯坦福大学。
- en: Smallwood and Sondik (1973) Smallwood, R. D. and Sondik, E. J. 1973. The optimal
    control of partially observable markov processes over a finite horizon. Operations
    Research 21, 5, 1071–1088.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smallwood 和 Sondik（1973）Smallwood, R. D. 和 Sondik, E. J. 1973. 有限时域上的部分可观察马尔可夫过程的最优控制。运筹学
    21, 5, 1071–1088。
- en: 'Sondik (1978) Sondik, E. J. 1978. The optimal control of partially observable
    markov processes over the infinite horizon: Discounted costs. Operations Research 26, 2,
    282–304.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sondik（1978）Sondik, E. J. 1978. 无限时域上的部分可观察马尔可夫过程的最优控制：折现成本。运筹学 26, 2, 282–304。
- en: Sutton (1991) Sutton, R. S. 1991. Dyna, an integrated architecture for learning,
    planning, and reacting. SIGART Bulletin 2, 4, 160–163.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton（1991）Sutton, R. S. 1991. Dyna，一种集成的学习、规划和反应架构。SIGART 公报 2, 4, 160–163。
- en: Sutton and Barto (1998) Sutton, R. S. and Barto, A. G. 1998. Introduction to
    reinforcement learning. Vol. 135. MIT press Cambridge.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto（1998）Sutton, R. S. 和 Barto, A. G. 1998. 强化学习导论。第135卷。MIT出版社，剑桥。
- en: Tang et al. (2013) Tang, L., Rosales, R., Singh, A., and Agarwal, D. 2013. Automatic
    ad format selection via contextual bandits. In CIKM ’13.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2013）Tang, L., Rosales, R., Singh, A., 和 Agarwal, D. 2013. 通过上下文赌博机进行自动广告格式选择。在
    CIKM ’13。
- en: Tang (2017) Tang, P. 2017. Reinforcement mechanism design. In IJCAI ’17.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang（2017）Tang, P. 2017. 强化机制设计。在 IJCAI ’17。
- en: Varaiya and Walrand (1983) Varaiya, P. and Walrand, J. C. 1983. Multi-armed
    bandit problems and resource sharing systems. In Computer Performance and Reliability,
    Proceedings of the International Workshop, Pisa, Italy, September 26-30, 1983.
    181–196.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Varaiya 和 Walrand（1983）Varaiya, P. 和 Walrand, J. C. 1983. 多臂赌博机问题和资源共享系统。在计算机性能和可靠性，国际研讨会论文集，意大利比萨，1983年9月26-30日。181–196。
- en: Vorobev et al. (2015) Vorobev, A., Lefortier, D., Gusev, G., and Serdyukov,
    P. 2015. Gathering additional feedback on search results by multi-armed bandits
    with respect to production ranking. In WWW ’15.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vorobev 等人（2015）Vorobev, A., Lefortier, D., Gusev, G., 和 Serdyukov, P. 2015.
    通过多臂赌博机收集有关搜索结果的额外反馈，针对生产排名。在 WWW ’15。
- en: Wang et al. (2018) Wang, W., Jin, J., Hao, J., Chen, C., Yu, C., Zhang, W.,
    Wang, J., Wang, Y., Li, H., Xu, J., and Gai, K. 2018. Learning to advertise with
    adaptive exposure via constrained two-level reinforcement learning. CoRR abs/1809.03149.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2018）Wang, W., Jin, J., Hao, J., Chen, C., Yu, C., Zhang, W., Wang,
    J., Wang, Y., Li, H., Xu, J., 和 Gai, K. 2018. 通过约束的两级强化学习学习广告曝光。CoRR abs/1809.03149。
- en: Wang et al. (2017) Wang, Y., Ouyang, H., Wang, C., Chen, J., Asamov, T., and
    Chang, Y. 2017. Efficient ordered combinatorial semi-bandits for whole-page recommendation.
    In AAAI ’17.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2017）Wang, Y., Ouyang, H., Wang, C., Chen, J., Asamov, T., 和 Chang,
    Y. 2017. 用于整页推荐的高效有序组合半带宽问题。在 AAAI ’17。
- en: 'Wang et al. (2016) Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang,
    Y., and Mei, Q. 2016. Beyond ranking: Optimizing whole-page presentation. In WSDM
    ’16.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2016）Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang, Y., 和
    Mei, Q. 2016. 超越排名：优化整页展示。在 WSDM ’16。
- en: Wang et al. (2018) Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang,
    Y., and Mei, Q. 2018. Optimizing whole-page presentation for web search. TWEB 12, 3,
    19:1–19:25.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2018）Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang, Y., 和
    Mei, Q. 2018. 针对网页搜索优化整页展示。TWEB 12, 3, 19:1–19:25。
- en: Watkins (1989) Watkins, C. J. C. H. 1989. Learning from delayed rewards. Ph.D.
    thesis, King’s College, Cambridge.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watkins（1989）Watkins, C. J. C. H. 1989. 从延迟奖励中学习。博士论文，剑桥大学国王学院。
- en: Williams (1992) Williams, R. J. 1992. Simple statistical gradient-following
    algorithms for connectionist reinforcement learning. Machine Learning 8, 229–256.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams（1992）Williams, R. J. 1992. 用于连接主义强化学习的简单统计梯度跟随算法。机器学习 8, 229–256。
- en: Wu et al. (2018) Wu, D., Chen, C., Yang, X., Chen, X., Tan, Q., Xu, J., and
    Gai, K. 2018. A multi-agent reinforcement learning method for impression allocation
    in online display advertising. CoRR abs/1809.03152.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2018）Wu, D., Chen, C., Yang, X., Chen, X., Tan, Q., Xu, J., 和 Gai, K.
    2018. 一种用于在线展示广告中印象分配的多智能体强化学习方法。CoRR abs/1809.03152。
- en: Wu et al. (2018) Wu, D., Chen, X., Yang, X., Wang, H., Tan, Q., Zhang, X., Xu,
    J., and Gai, K. 2018. Budget constrained bidding by model-free reinforcement learning
    in display advertising. In CIKM ’18.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2018）Wu, D., Chen, X., Yang, X., Wang, H., Tan, Q., Zhang, X., Xu, J.,
    和 Gai, K. 2018. 通过无模型强化学习进行预算约束竞标。在 CIKM ’18。
- en: Wu et al. (2018) Wu, Q., Iyer, N., and Wang, H. 2018. Learning contextual bandits
    in a non-stationary environment. In SIGIR ’18.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2018) Wu, Q., Iyer, N., 和 Wang, H. 2018. 在非平稳环境中学习上下文赌博机。在 SIGIR ’18。
- en: 'Wu et al. (2017) Wu, Q., Wang, H., Hong, L., and Shi, Y. 2017. Returning is
    believing: Optimizing long-term user engagement in recommender systems. In CIKM
    ’17.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2017) Wu, Q., Wang, H., Hong, L., 和 Shi, Y. 2017. 归还即信仰：优化推荐系统中的长期用户参与。在
    CIKM ’17。
- en: Xia et al. (2017) Xia, L., Xu, J., Lan, Y., Guo, J., Zeng, W., and Cheng, X.
    2017. Adapting markov decision process for search result diversification. In SIGIR
    ’17.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等 (2017) Xia, L., Xu, J., Lan, Y., Guo, J., Zeng, W., 和 Cheng, X. 2017.
    适应马尔可夫决策过程以进行搜索结果多样化。在 SIGIR ’17。
- en: 'Xu and Li (2007) Xu, J. and Li, H. 2007. Adarank: a boosting algorithm for
    information retrieval. In SIGIR ’07.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 和 Li (2007) Xu, J. 和 Li, H. 2007. Adarank: 一种用于信息检索的提升算法。在 SIGIR ’07。'
- en: Xu et al. (2008) Xu, J., Liu, T., Lu, M., Li, H., and Ma, W. 2008. Directly
    optimizing evaluation measures in learning to rank. In SIGIR ’08.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2008) Xu, J., Liu, T., Lu, M., Li, H., 和 Ma, W. 2008. 在学习排名中直接优化评估指标。在
    SIGIR ’08。
- en: 'Xu et al. (2017) Xu, J., Xia, L., Lan, Y., Guo, J., and Cheng, X. 2017. Directly
    optimize diversity evaluation measures: A new approach to search result diversification.
    ACM TIST 8, 3, 41:1–41:26.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2017) Xu, J., Xia, L., Lan, Y., Guo, J., 和 Cheng, X. 2017. 直接优化多样性评估指标：搜索结果多样化的新方法。ACM
    TIST 8, 3, 41:1–41:26。
- en: Xu et al. (2013) Xu, M., Qin, T., and Liu, T. 2013. Estimation bias in multi-armed
    bandit algorithms for search advertising. In NIPS ’13.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2013) Xu, M., Qin, T., 和 Liu, T. 2013. 搜索广告中多臂赌博机算法的估计偏差。在 NIPS ’13。
- en: Yang and Lu (2016) Yang, H. and Lu, Q. 2016. Dynamic contextual multi arm bandits
    in display advertisement. In ICDM ’16.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 和 Lu (2016) Yang, H. 和 Lu, Q. 2016. 动态上下文多臂赌博机在展示广告中的应用。在 ICDM ’16。
- en: (86) Yin, D., Hu, Y., Tang, J., Daly, T., Zhou, M., Ouyang, H., Chen, J., Kang,
    C., Deng, H., Nobata, C., et al. Ranking relevance in yahoo search. In SIGKDD’16.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (86) Yin, D., Hu, Y., Tang, J., Daly, T., Zhou, M., Ouyang, H., Chen, J., Kang,
    C., Deng, H., Nobata, C., 等。 在 Yahoo 搜索中排名相关性。在 SIGKDD’16。
- en: Yin et al. (2016) Yin, D., Hu, Y., Tang, J., Jr., T. D., Zhou, M., Ouyang, H.,
    Chen, J., Kang, C., Deng, H., Nobata, C., Langlois, J., and Chang, Y. 2016. Ranking
    relevance in yahoo search. In SIGKDD ’16.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等 (2016) Yin, D., Hu, Y., Tang, J., Jr., T. D., Zhou, M., Ouyang, H., Chen,
    J., Kang, C., Deng, H., Nobata, C., Langlois, J., 和 Chang, Y. 2016. 在 Yahoo 搜索中排名相关性。在
    SIGKDD ’16。
- en: Yuan et al. (2013) Yuan, S., Wang, J., and van der Meer, M. 2013. Adaptive keywords
    extraction with contextual bandits for advertising on parked domains. CoRR abs/1307.3573.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2013) Yuan, S., Wang, J., 和 van der Meer, M. 2013. 使用上下文赌博机的自适应关键词提取用于停放域广告。CoRR
    abs/1307.3573。
- en: Yue et al. (2007) Yue, Y., Finley, T., Radlinski, F., and Joachims, T. 2007.
    A support vector method for optimizing average precision. In SIGIR ’07.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue 等 (2007) Yue, Y., Finley, T., Radlinski, F., 和 Joachims, T. 2007. 用于优化平均精度的支持向量方法。在
    SIGIR ’07。
- en: Zeng et al. (2016) Zeng, C., Wang, Q., Mokhtari, S., and Li, T. 2016. Online
    context-aware recommendation with time varying multi-armed bandit. In SIGKDD ’16.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等 (2016) Zeng, C., Wang, Q., Mokhtari, S., 和 Li, T. 2016. 具有时间变化多臂赌博机的在线上下文感知推荐。在
    SIGKDD ’16。
- en: Zeng et al. (2017) Zeng, W., Xu, J., Lan, Y., Guo, J., and Cheng, X. 2017. Reinforcement
    learning to rank with markov decision process. In SIGIR ’17.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等 (2017) Zeng, W., Xu, J., Lan, Y., Guo, J., 和 Cheng, X. 2017. 使用马尔可夫决策过程进行排序的强化学习。在
    SIGIR ’17。
- en: Zeng et al. (2018) Zeng, W., Xu, J., Lan, Y., Guo, J., and Cheng, X. 2018. Multi
    page search with reinforcement learning to rank. In ICTIR ’18.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等 (2018) Zeng, W., Xu, J., Lan, Y., Guo, J., 和 Cheng, X. 2018. 使用强化学习进行多页面搜索。在
    ICTIR ’18。
- en: Zhang et al. (2014) Zhang, S., Luo, J., and Yang, H. 2014. A POMDP model for
    content-free document re-ranking. In SIGIR ’14.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2014) Zhang, S., Luo, J., 和 Yang, H. 2014. 用于无内容文档重排序的 POMDP 模型。在 SIGIR
    ’14。
- en: Zhao et al. (2018) Zhao, J., Qiu, G., Guan, Z., Zhao, W., and He, X. 2018. Deep
    reinforcement learning for sponsored search real-time bidding. In SIGKDD ’18.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2018) Zhao, J., Qiu, G., Guan, Z., Zhao, W., 和 He, X. 2018. 用于赞助搜索实时竞价的深度强化学习。在
    SIGKDD ’18。
- en: (95) Zhao, X., Xia, L., Zhang, L., Ding, Z., Yin, D., and Tang, J. Deep reinforcement
    learning for page-wise recommendations. In ResSys’18.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (95) Zhao, X., Xia, L., Zhang, L., Ding, Z., Yin, D., 和 Tang, J. 深度强化学习用于页面级推荐。在
    ResSys’18。
- en: Zhao et al. (2019) Zhao, X., Xia, L., Zhao, Y., Tang, J., and Yin, D. 2019.
    Model-based reinforcement learning for whole-chain recommendations. arXiv preprint
    arXiv:1902.03987.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2019) Zhao, X., Xia, L., Zhao, Y., Tang, J., 和 Yin, D. 2019. 基于模型的强化学习用于全链推荐。arXiv
    预印本 arXiv:1902.03987。
- en: (97) Zhao, X., Zhang, L., Ding, Z., Xia, L., Tang, J., and Yin, D. Recommendations
    with negative feedback via pairwise deep reinforcement learning. In SIGKDD’18.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (97) Zhao, X., Zhang, L., Ding, Z., Xia, L., Tang, J., and Yin, D. 通过配对深度强化学习进行负反馈推荐。见SIGKDD’18。
- en: Zhao et al. (2017) Zhao, X., Zhang, L., Ding, Z., Yin, D., Zhao, Y., and Tang,
    J. 2017. Deep reinforcement learning for list-wise recommendations. arXiv preprint
    arXiv:1801.00209.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等（2017）Zhao, X., Zhang, L., Ding, Z., Yin, D., Zhao, Y., and Tang, J. 2017.
    用于列表推荐的深度强化学习。arXiv预印本arXiv:1801.00209。
- en: 'Zheng et al. (2018) Zheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N. J.,
    Xie, X., and Li, Z. 2018. DRN: A deep reinforcement learning framework for news
    recommendation. In WWW ’18.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等（2018）Zheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N. J., Xie, X.,
    and Li, Z. 2018. DRN：用于新闻推荐的深度强化学习框架。见WWW ’18。
- en: Zou et al. (2019) Zou, L., Xia, L., Ding, Z., Yin, D., Song, J., and Liu, W.
    2019. Reinforcement learning to diversify recommendations. In DASFAA ’19.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou等（2019）Zou, L., Xia, L., Ding, Z., Yin, D., Song, J., and Liu, W. 2019. 强化学习用于多样化推荐。见DASFAA
    ’19。
- en: '{biography}'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '{biography}'
- en: Xiangyu Zhao is a Ph.D. student of computer science and engineering at Michigan
    State University (MSU). His supervisor is Dr. Jiliang Tang. Before joining MSU,
    he completed his MS(2017) at USTC and BS(2014) at UESTC. He is the student member
    of IEEE, SIGIR, and SIAM. His current research interests include data mining and
    machine learning, especially (1) Reinforcement Learning for E-commerce; (2) Urban
    Computing and Spatio-Temporal Data Analysis. After joining MSU, he has published
    his work in top journals (e.g. SIGKDD Explorations) and conferences (e.g., KDD,
    ICDM, CIKM, RecSys). He was the recipients of the RecSys’18, KDD’18, SDM’18, and
    CIKM’17 Student Travel Award.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**Xiangyu Zhao** 是密歇根州立大学（MSU）计算机科学与工程的博士生。他的导师是Dr. Jiliang Tang。在加入MSU之前，他在USTC获得了硕士学位（2017）和在UESTC获得了学士学位（2014）。他是IEEE、SIGIR和SIAM的学生会员。他目前的研究兴趣包括数据挖掘和机器学习，特别是（1）电子商务的强化学习；（2）城市计算和时空数据分析。加入MSU后，他在顶级期刊（如SIGKDD
    Explorations）和会议（如KDD、ICDM、CIKM、RecSys）上发表了工作。他曾获得RecSys’18、KDD’18、SDM’18和CIKM’17学生旅行奖。'
- en: Long Xia is a research scientist in Data Science Lab at JD.com. He is now mainly
    responsible for applying advanced technology to the E-commerce recommender system
    in JD.com. Before that, he received his Ph.D. in Computer Science from Institute
    of Computing Technology, Chinese Academy of Sciences. His research interests include
    data mining, applied machine learning, information retrieval, and recommender
    system. He has published his research in top journals and conferences, e.g. TIST,
    SIGIR, KDD, RecSys.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**Long Xia** 是京东数据科学实验室的研究科学家。他目前主要负责将先进技术应用于京东的电子商务推荐系统。在此之前，他在中国科学院计算技术研究所获得了计算机科学博士学位。他的研究兴趣包括数据挖掘、应用机器学习、信息检索和推荐系统。他在顶级期刊和会议上发表了研究成果，如TIST、SIGIR、KDD、RecSys。'
- en: Jiliang Tang is an assistant professor in the computer science and engineering
    department at Michigan State University. Before that, he was a research scientist
    in Yahoo Research and got his PhD from Arizona State University in 2015\. He has
    broad interests in social computing, data mining and machine learning. He was
    the recipients of the Best Paper Award in ASONAM 2018, the Best Student Paper
    Award in WSDM2018, the Best Paper Award in KDD2016, the runner up of the Best
    KDD Dissertation Award in 2015, Dean’s Dissertation Award and the best paper shortlist
    of WSDM2013\. He is now associate editors of ACM TKDD, ICWSM and Neurocomputing.
    He has published his research in highly ranked journals and top conference proceedings,
    which received thousands of citations and extensive media coverage.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jiliang Tang** 是密歇根州立大学计算机科学与工程系的助理教授。在此之前，他曾在Yahoo研究担任研究科学家，并于2015年获得亚利桑那州立大学的博士学位。他在社会计算、数据挖掘和机器学习方面具有广泛兴趣。他曾获得ASONAM
    2018最佳论文奖、WSDM 2018最佳学生论文奖、KDD 2016最佳论文奖、2015年最佳KDD论文奖亚军、Dean’s Dissertation Award及WSDM
    2013最佳论文提名。他目前是ACM TKDD、ICWSM和Neurocomputing的副主编。他在高排名期刊和顶级会议论文集中发表了研究成果，获得了数千次引用和广泛的媒体报道。'
- en: Dawei Yin is Senior Director at JD.com, leading the science efforts of recommendation,
    search, metrics and knowledge graph. Prior to joining JD.com, he was Senior Research
    Manager at Yahoo Labs, leading relevance science team and in charge of Core Search
    Relevance of Yahoo Search. He obtained Ph.D. (2013), M.S. (2010) from Lehigh University
    and B.S. (2006) from Shandong University. His research interests include data
    mining, applied machine learning, information retrieval and recommender system.
    He published tens of research papers in premium conferences and journals, and
    won WSDM2016 best paper award, KDD2016 best paper award, and WSDM2018 best student
    paper award.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 殷大伟是京东的高级总监，负责推荐、搜索、指标和知识图谱的科学工作。在加入京东之前，他曾在Yahoo Labs担任高级研究经理，领导相关性科学团队，并负责Yahoo搜索的核心搜索相关性。他获得了莱海大学的博士学位（2013年）、硕士学位（2010年）和山东大学的学士学位（2006年）。他的研究兴趣包括数据挖掘、应用机器学习、信息检索和推荐系统。他在顶级会议和期刊上发表了几十篇研究论文，并获得了WSDM2016最佳论文奖、KDD2016最佳论文奖和WSDM2018最佳学生论文奖。
