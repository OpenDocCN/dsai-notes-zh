- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:09:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:09:10
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1612.07640] Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1612.07640] 深度学习及其在机器健康监测中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1612.07640](https://ar5iv.labs.arxiv.org/html/1612.07640)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1612.07640](https://ar5iv.labs.arxiv.org/html/1612.07640)
- en: 'Deep Learning and Its Applications to Machine Health Monitoring: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深度学习及其在机器健康监测中的应用：综述》
- en: 'Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang, and Robert X. Gao
    R. Yan is the corresponding author. E-mail: ruqiang@seu.edu.cn'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：**Rui Zhao**, **Ruqiang Yan**, **Zhenghua Chen**, **Kezhi Mao**, **Peng Wang**,
    和 **Robert X. Gao**。**R. Yan** 是通讯作者。电子邮件：ruqiang@seu.edu.cn
- en: This manuscript has been submitted to IEEE Transactions on Neural Networks and
    Learning Systems
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本手稿已提交至《IEEE神经网络与学习系统汇刊》
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Since 2006, deep learning (DL) has become a rapidly growing research direction,
    redefining state-of-the-art performances in a wide range of areas such as object
    recognition, image segmentation, speech recognition and machine translation. In
    modern manufacturing systems, data-driven machine health monitoring is gaining
    in popularity due to the widespread deployment of low-cost sensors and their connection
    to the Internet. Meanwhile, deep learning provides useful tools for processing
    and analyzing these big machinery data. The main purpose of this paper is to review
    and summarize the emerging research work of deep learning on machine health monitoring.
    After the brief introduction of deep learning techniques, the applications of
    deep learning in machine health monitoring systems are reviewed mainly from the
    following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines
    and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines
    (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).
    Finally, some new trends of DL-based machine health monitoring methods are discussed.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自2006年以来，深度学习（DL）已成为一个快速发展的研究方向，在物体识别、图像分割、语音识别和机器翻译等多个领域重新定义了最先进的性能。在现代制造系统中，由于低成本传感器的广泛部署及其与互联网的连接，数据驱动的机器健康监测越来越受欢迎。同时，深度学习为处理和分析这些大型机械数据提供了有用的工具。本文的主要目的是回顾和总结深度学习在机器健康监测中的新兴研究成果。在对深度学习技术进行简要介绍之后，主要从以下几个方面回顾了深度学习在机器健康监测系统中的应用：自编码器（AE）及其变体、限制玻尔兹曼机及其变体，包括深度置信网络（DBN）和深度玻尔兹曼机（DBM）、卷积神经网络（CNN）和递归神经网络（RNN）。最后，讨论了一些基于深度学习的机器健康监测方法的新趋势。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep learning, machine health monitoring, big data
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，机器健康监测，大数据
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Industrial Internet of Things (IoT) and data-driven techniques have been revolutionizing
    manufacturing by enabling computer networks to gather the huge amount of data
    from connected machines and turn the big machinery data into actionable information
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]. As a key component in modern
    manufacturing system, machine health monitoring has fully embraced the big data
    revolution. Compared to top-down modeling provided by the traditional physics-based
    models [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] , data-driven machine
    health monitoring systems offer a new paradigm of bottom-up solution for detection
    of faults after the occurrence of certain failures (diagnosis) and predictions
    of the future working conditions and the remaining useful life (prognosis) [[1](#bib.bib1),
    [7](#bib.bib7)]. As we know, the complexity and noisy working condition hinder
    the construction of physical models. And most of these physics-based models are
    unable to be updated with on-line measured data, which limits their effectiveness
    and flexibility. On the other hand, with significant development of sensors, sensor
    networks and computing systems, data-driven machine health monitoring models have
    become more and more attractive. To extract useful knowledge and make appropriate
    decisions from big data, machine learning techniques have been regarded as a powerful
    solution. As the hottest subfield of machine learning, deep learning is able to
    act as a bridge connecting big machinery data and intelligent machine health monitoring.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 工业物联网（IoT）和数据驱动技术正在通过使计算机网络能够从连接的机器中收集大量数据并将这些大数据转化为可操作的信息，彻底改变制造业[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]。作为现代制造系统中的一个关键组成部分，机器健康监测已完全接受了大数据革命。与传统物理模型提供的自上而下的建模相比[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)]，数据驱动的机器健康监测系统提供了一种新的自下而上的故障检测（诊断）和未来工作条件及剩余使用寿命（预测）的解决方案[[1](#bib.bib1),
    [7](#bib.bib7)]。众所周知，复杂和嘈杂的工作条件阻碍了物理模型的构建。而且这些基于物理的模型大多数无法用在线测量的数据进行更新，这限制了它们的有效性和灵活性。另一方面，随着传感器、传感器网络和计算系统的显著发展，数据驱动的机器健康监测模型变得越来越有吸引力。为了从大数据中提取有用的知识并做出适当的决策，机器学习技术被视为一种强大的解决方案。作为机器学习中最热门的子领域，深度学习能够充当大数据和智能机器健康监测之间的桥梁。
- en: 'As a branch of machine learning, deep learning attempts to model high level
    representations behind data and classify(predict) patterns via stacking multiple
    layers of information processing modules in hierarchical architectures. Recently,
    deep learning has been successfully adopted in various areas such as computer
    vision, automatic speech recognition, natural language processing, audio recognition
    and bioinformatics [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)].
    In fact, deep learning is not a new idea, which even dates back to the 1940s [[12](#bib.bib12),
    [13](#bib.bib13)]. The popularity of deep learning today can be contributed to
    the following points:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习的一个分支，深度学习尝试通过在分层架构中堆叠多个信息处理模块来建模数据背后的高级表示，并对模式进行分类（预测）。最近，深度学习已经成功应用于计算机视觉、自动语音识别、自然语言处理、音频识别和生物信息学等多个领域[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]。事实上，深度学习并不是一个新概念，它的历史可以追溯到1940年代[[12](#bib.bib12),
    [13](#bib.bib13)]。今天深度学习的流行可以归因于以下几点：
- en: '*'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*'
- en: 'Increasing Computing Power: the advent of graphics processor unit (GPU), the
    lowered cost of hardware, the better software infrastructure and the faster network
    connectivity all reduce the required running time of deep learning algorithms
    significantly. For example, as reported in [[14](#bib.bib14)], the time required
    to learn a four-layer DBN with 100 million free parameters can be reduced from
    several weeks to around a single day.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提升计算能力：图形处理单元（GPU）的出现、硬件成本的降低、软件基础设施的改善以及网络连接速度的加快，都显著减少了深度学习算法所需的运行时间。例如，正如[[14](#bib.bib14)]中报告的那样，训练一个具有1亿个自由参数的四层DBN的时间可以从几周缩短到大约一天。
- en: '*'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*'
- en: 'Increasing Data Size: there is no doubt that the era of Big Data is coming.
    Our activities are almost all digitized, recorded by computers and sensors, connected
    to Internet, and stored in cloud. As pointed out in [[1](#bib.bib1)] that in industry-related
    applications such as industrial informatics and electronics, almost 1000 exabytes
    are generated per year and a 20-fold increase can be expected in the next ten
    years. The study in [[3](#bib.bib3)] predicts that 30 billion devices will be
    connected by 2020\. Therefore, the huge amount of data is able to offset the complexity
    increase behind deep learning and improve its generalization capability.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据规模的增加：毫无疑问，大数据时代即将到来。我们的活动几乎都已数字化，由计算机和传感器记录，通过互联网连接，并存储在云端。正如[[1](#bib.bib1)]所指出的，在工业相关应用中，如工业信息学和电子学，每年生成的数据显示几乎达到1000艾字节，预计在未来十年内将增长20倍。[[3](#bib.bib3)]的研究预测，到2020年将有300亿个设备联网。因此，大量的数据能够抵消深度学习带来的复杂性增加，并提升其泛化能力。
- en: '*'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*'
- en: 'Advanced Deep Learning Research: the first breakthrough of deep learning is
    the pre-training method in an unsupervised way [[15](#bib.bib15)], where Hinton
    proposed to pre-train one layer at a time via restricted Boltzmann machine (RBM)
    and then fine-tune using backpropagation. This has been proven to be effective
    to train multi-layer neural networks.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高级深度学习研究：深度学习的第一个突破是无监督预训练方法[[15](#bib.bib15)]，Hinton提出通过受限玻尔兹曼机（RBM）逐层预训练，然后使用反向传播进行微调。这已被证明对训练多层神经网络是有效的。
- en: 'Considering the capability of deep learning to address large-scale data and
    learn high-level representation, deep learning can be a powerful and effective
    solution for machine health monitoring systems (MHMS). Conventional data-driven
    MHMS usually consists of the following key parts: hand-crafted feature design,
    feature extraction/selection and model training. The right set of features are
    designed, and then provided to some shallow machine learning algorithms including
    Support Vector Machines (SVM), Naive Bayes (NB), logistic regression [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18)]. It is shown that the representation defines
    the upper-bound performances of machine learning algorithms [[19](#bib.bib19)].
    However, it is difficult to know and determine what kind of good features should
    be designed. To alleviate this issue, feature extraction/selection methods, which
    can be regarded as a kind of information fusion, are performed between hand-crafted
    feature design and classification/regression models [[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)]. However, manually designing features for a complex domain requires
    a great deal of human labor and can not be updated on-line. At the same time,
    feature extraction/selection is another tricky problem, which involves prior selection
    of hyperparameters such as latent dimension. At last, the above three modules
    including feature design, feature extraction/selection and model training can
    not be jointly optimized which may hinder the final performance of the whole system.
    Deep learning based MHMS (DL-based MHMS) aim to extract hierarchical representations
    from input data by building deep neural networks with multiple layers of non-linear
    transformations. Intuitively, one layer operation can be regarded as a transformation
    from input values to output values. Therefore, the application of one layer can
    learn a new representation of the input data and then, the stacking structure
    of multiple layers can enable MHMS to learn complex concepts out of simpler concepts
    that can be constructed from raw input. In addition, DL-based MHMS achieve an
    end-to-end system, which can automatically learn internal representations from
    raw input and predict targets. Compared to conventional data driven MHMS, DL-based
    MHMS do not require extensive human labor and knowledge for hand-crafted feature
    design. All model parameters including feature module and pattern classification/regression
    module can be trained jointly. Therefore, DL-based models can be applied to addressing
    machine health monitoring in a very general way. For example, it is possible that
    the model trained for fault diagnosis problem can be used for prognosis by only
    replacing the top softmax layer with a linear regression layer. The comparison
    between conventional data-driven MHMS and DL-based MHMS is given in Table [I](#S1.T1
    "Table I ‣ I Introduction ‣ Deep Learning and Its Applications to Machine Health
    Monitoring: A Survey"). A high-level illustration of the principles behind these
    three kinds of MHMS discussed above is shown in Figure [1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到深度学习处理大规模数据和学习高级表示能力的能力，深度学习可以成为机器健康监测系统（MHMS）的强大有效解决方案。传统的数据驱动型MHMS通常包括以下几个关键部分：手工特征设计、特征提取/选择和模型训练。设计出合适的特征集，然后将其提供给一些浅层机器学习算法，包括支持向量机（SVM）、朴素贝叶斯（NB）、逻辑回归[[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18)]。研究表明，表示法定义了机器学习算法的性能上限[[19](#bib.bib19)]。然而，很难知道和确定应该设计哪种好的特征。为了缓解这个问题，执行特征提取/选择方法，可以看作是一种信息融合，它在手工特征设计和分类/回归模型之间执行[[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)]。然而，为复杂领域手动设计特征需要大量人力，并且无法在线更新。同时，特征提取/选择是另一个棘手的问题，涉及先验选择如潜在维度的超参数。最后，包括特征设计、特征提取/选择和模型训练在内的上述三个模块不能联合优化，这可能会阻碍整个系统的最终性能。基于深度学习的MHMS（DL-based
    MHMS）旨在通过构建具有多层非线性变换的深度神经网络，从输入数据中提取分层表示。直观地，一个层操作可以被视为从输入值到输出值的转换。因此，一个层的应用可以学习输入数据的新表示，然后，多层堆叠结构可以使MHMS从原始输入中学习出复杂概念。此外，基于DL的MHMS实现端到端系统，可以自动从原始输入中学习内部表示并预测目标。与传统的数据驱动MHMS相比，DL-based
    MHMS不需要大量的人力和知识来进行手工特征设计。所有模型参数，包括特征模块和模式分类/回归模块，可以联合训练。因此，DL-based模型可以以非常普遍的方式应用于解决机器健康监测问题。例如，针对故障诊断问题训练的模型可以通过仅用线性回归层替换顶部softmax层来用于预测。在表[I](#S1.T1
    "表I ‣ I Introduction ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey")中给出了传统数据驱动MHMS与基于DL的MHMS的比较。上述讨论的这三种MHMS的原理的高层次图示如图[1](#S1.F1 "图1 ‣
    I Introduction ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey")所示。'
- en: 'Deep learning models have several variants such as Auto-Dncoders [[23](#bib.bib23)],
    Deep Belief Network [[24](#bib.bib24)], Deep Boltzmann Machines [[25](#bib.bib25)],
    Convolutional Neural Networks [[26](#bib.bib26)] and Recurrent Neural Networks
    [[27](#bib.bib27)]. During recent years, various researchers have demonstrated
    success of these deep learning models in the application of machine health monitoring.
    This paper attempts to provide a wide overview on these latest DL-based MHMS works
    that impact the state-of-the art technologies. Compared to these frontiers of
    deep learning including Computer Vision and Natural Language Processing, machine
    health monitoring community is catching up and has witnessed an emerging research.
    Therefore, the purpose of this survey article is to present researchers and engineers
    in the area of machine health monitoring system, a global view of this hot and
    active topic, and help them to acquire basic knowledge, quickly apply deep learning
    models and develop novel DL-based MHMS. The remainder of this paper is organized
    as follows. The basic information on these above deep learning models are given
    in section [II](#S2 "II Deep Learning ‣ Deep Learning and Its Applications to
    Machine Health Monitoring: A Survey"). Then, section [III](#S3 "III Applications
    of Deep learning in machine health monitoring ‣ Deep Learning and Its Applications
    to Machine Health Monitoring: A Survey") reviews applications of deep learning
    models on machine health monitoring. Finally, section [IV](#S4 "IV Summary and
    Future Directions ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey") gives a brief summary of the recent achievements of DL-based MHMS and
    discusses some potential trends of deep learning in machine health monitoring.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型有几种变体，例如自动编码器 [[23](#bib.bib23)]、深度信念网络 [[24](#bib.bib24)]、深度玻尔兹曼机 [[25](#bib.bib25)]、卷积神经网络
    [[26](#bib.bib26)] 和递归神经网络 [[27](#bib.bib27)]。近年来，各种研究者已经证明了这些深度学习模型在机器健康监测应用中的成功。本文试图对这些最新的基于深度学习的MHMS工作进行广泛的概述，这些工作对最先进的技术产生了影响。与包括计算机视觉和自然语言处理在内的深度学习前沿相比，机器健康监测领域正在迎头赶上，并且已经出现了新兴的研究。因此，本综述文章的目的是向机器健康监测系统领域的研究人员和工程师展示这一热门而活跃的话题的全球视角，并帮助他们获取基础知识，快速应用深度学习模型并开发新型的基于深度学习的MHMS。本文其余部分的组织如下：第[II](#S2
    "II 深度学习 ‣ 深度学习及其在机器健康监测中的应用：综述")节提供了上述深度学习模型的基本信息。接着，第[III](#S3 "III 深度学习在机器健康监测中的应用
    ‣ 深度学习及其在机器健康监测中的应用：综述")节回顾了深度学习模型在机器健康监测中的应用。最后，第[IV](#S4 "IV 总结与未来方向 ‣ 深度学习及其在机器健康监测中的应用：综述")节简要总结了基于深度学习的MHMS的近期成就，并讨论了深度学习在机器健康监测中的一些潜在趋势。
- en: '![Refer to caption](img/7d5763502f778afaf634e666fb73dec9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7d5763502f778afaf634e666fb73dec9.png)'
- en: 'Figure 1: Frameworks showing three different MHMS including Physical Model,
    Conventional Data-driven Model and Deep Learning Models. Shaded boxes denote data-driven
    components.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示三种不同MHMS框架的图示，包括物理模型、传统数据驱动模型和深度学习模型。阴影框表示数据驱动组件。
- en: 'Table I: Summary on comparison between conventional data-driven MHMS and DL-based
    MHMS.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：传统数据驱动型机器健康监测系统（MHMS）与基于深度学习（DL）的MHMS的比较总结。
- en: '| MHMS |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| MHMS |'
- en: '| --- |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Conventional Data-driven Methods | Deep Learning Methods |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 传统数据驱动方法 | 深度学习方法 |'
- en: '| Expert knowledge and extensive human labor required for Hand-crafted features
    | End-to-end structure without hand-crafted features |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 需要专家知识和大量人工劳动进行手工特征提取 | 无需手工特征的端到端结构 |'
- en: '| Individual modules are trained step-by-step | All parameters are trained
    jointly |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 各个模块逐步训练 | 所有参数联合训练 |'
- en: '| Unable to model large-scale data | Suitable for large-scale data |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 无法建模大规模数据 | 适用于大规模数据 |'
- en: II Deep Learning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度学习
- en: Originated from artificial neural network, deep learning is a branch of machine
    learning which is featured by multiple non-linear processing layers. Deep learning
    aims to learn hierarchy representations of data. Up to date, there are various
    deep learning architectures and this research topic is fast-growing, in which
    new models are being developed even per week. And the community is quite open
    and there are a number of deep learning tutorials and books of good-quality [[28](#bib.bib28),
    [29](#bib.bib29)]. Therefore, only a brief introduction to some major deep learning
    techniques that have been applied in machine health monitoring is given. In the
    following, four deep architectures including Auto-encoders, RBM, CNN and RNN and
    their corresponding variants are reviewed, respectively.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 源于人工神经网络，深度学习是机器学习的一个分支，其特点是具有多个非线性处理层。深度学习的目标是学习数据的层次表示。至今，深度学习有多种架构，这一研究领域发展迅速，每周都有新模型被开发出来。社区也非常开放，有大量优质的深度学习教程和书籍[[28](#bib.bib28),
    [29](#bib.bib29)]。因此，这里仅对一些已应用于机器健康监测的主要深度学习技术进行简要介绍。接下来，将分别回顾四种深度架构，包括自编码器（Auto-encoders）、RBM、CNN
    和 RNN 及其对应的变体。
- en: II-A Auto-encoders (AE) and its variants
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 自编码器（AE）及其变体
- en: 'As a feed-forward neural network, auto-encoder consists of two phases including
    encoder and decoder. Encoder takes an input $\mathbf{x}$ and transforms it to
    a hidden representation $\mathbf{h}$ via a non-linear mapping as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个前馈神经网络，自编码器包括编码器和解码器两个阶段。编码器接受输入$\mathbf{x}$并通过非线性映射将其转换为隐藏表示$\mathbf{h}$，如下所示：
- en: '|  | $\mathbf{h}=\varphi(\mathbf{W}\mathbf{x}+\mathbf{b})$ |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}=\varphi(\mathbf{W}\mathbf{x}+\mathbf{b})$ |  | (1) |'
- en: 'where $\varphi$ is a non-linear activation function. Then, decoder maps the
    hidden representation back to the original representation in a similar way as
    follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\varphi$是一个非线性激活函数。然后，解码器将隐藏表示以类似的方式映射回原始表示：
- en: '|  | $\mathbf{z}=\varphi(\mathbf{W}^{{}^{\prime}}\mathbf{h}+\mathbf{b}^{{}^{\prime}})$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{z}=\varphi(\mathbf{W}^{{}^{\prime}}\mathbf{h}+\mathbf{b}^{{}^{\prime}})$
    |  | (2) |'
- en: 'Model parameters including $\theta=[\mathbf{W},\mathbf{b},\mathbf{W}^{{}^{\prime}},\mathbf{b}^{{}^{\prime}}]$
    are optimized to minimize the reconstruction error between $\mathbf{z}=f_{\theta}(\mathbf{x})$
    and $\mathbf{x}$. One commonly adopted measure for the average reconstruction
    error over a collection of $N$ data samples is squared error and the corresponding
    optimization problem can be written as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数包括$\theta=[\mathbf{W},\mathbf{b},\mathbf{W}^{{}^{\prime}},\mathbf{b}^{{}^{\prime}}]$被优化以最小化$\mathbf{z}=f_{\theta}(\mathbf{x})$和$\mathbf{x}$之间的重建误差。常用的平均重建误差度量是平方误差，对应的优化问题可以写成如下：
- en: '|  | $\min_{\theta}\frac{1}{N}\sum_{i}^{N}(\mathbf{x}_{i}-f_{\theta}(\mathbf{x}_{i}))^{2}$
    |  | (3) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\frac{1}{N}\sum_{i}^{N}(\mathbf{x}_{i}-f_{\theta}(\mathbf{x}_{i}))^{2}$
    |  | (3) |'
- en: where $\mathbf{x}_{i}$ is the $i$-th sample. It is clearly shown that AE can
    be trained in an unsupervised way. And the hidden representation $\mathbf{h}$
    can be regarded as a more abstract and meaningful representation for data sample
    $\mathbf{x}$. Usually, the hidden size should be set to be larger than the input
    size in AE, which is verified empirically [[30](#bib.bib30)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{x}_{i}$是第$i$个样本。显然，自编码器可以以无监督的方式进行训练。隐藏表示$\mathbf{h}$可以被视为数据样本$\mathbf{x}$的更抽象、更有意义的表示。通常，自编码器中的隐藏层大小应设置大于输入层大小，这一点已经通过实验证实[[30](#bib.bib30)]。
- en: 'Addition of Sparsity: To prevent the learned transformation is the identity
    one and regularize auto-encoders, the sparsity constraint is imposed on the hidden
    units [[31](#bib.bib31)]. The corrsponding optimization function is updated as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性的加入：为了防止学习到的变换为恒等变换并对自编码器进行正则化，在隐藏单元上施加了稀疏性约束[[31](#bib.bib31)]。相应的优化函数更新为：
- en: '|  | $\min_{\theta}\frac{1}{N}\sum_{i}^{N}(\mathbf{x}_{i}-f_{\theta}(\mathbf{x}_{i}))^{2}+\sum_{j}^{m}KL(p&#124;&#124;p_{j})$
    |  | (4) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\frac{1}{N}\sum_{i}^{N}(\mathbf{x}_{i}-f_{\theta}(\mathbf{x}_{i}))^{2}+\sum_{j}^{m}KL(p&#124;&#124;p_{j})$
    |  | (4) |'
- en: 'where $m$ is the hidden layer size and the second term is the summation of
    the KL-divergence over the hidden units. The KL-divergence on $j$-th hidden neuron
    is given as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$m$是隐藏层的大小，第二项是对隐藏单元的KL散度的总和。第$j$个隐藏神经元的KL散度计算如下：
- en: '|  | $KL(p&#124;&#124;p_{j})=plog(\frac{p}{p_{j}})+(1-p)log(\frac{1-p}{1-p_{j}})$
    |  | (5) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $KL(p&#124;&#124;p_{j})=plog(\frac{p}{p_{j}})+(1-p)log(\frac{1-p}{1-p_{j}})$
    |  | (5) |'
- en: where $p$ is the predefined mean activation target and $p_{j}$ is the average
    activation of the $j$-th hidden neuron over the whole datasets. Given a small
    $p$, the addition of sparsity constraint can lead the learned hidden representation
    to be a sparse representation. Therefore, the variant of AE is named sparse auto-encoder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p$ 是预定义的平均激活目标，$p_{j}$ 是 $j$-th 隐藏神经元在整个数据集上的平均激活。给定一个小的 $p$，添加稀疏性约束可以使学习到的隐藏表示成为稀疏表示。因此，AE
    的变体被称为稀疏自编码器。
- en: 'Addition of Denoising: Different from conventional AE, denoising AE takes a
    corrupted version of data as input and is trained to reconstruct/denoise the clean
    input $\mathbf{x}$ from its corrupted sample $\tilde{\mathbf{x}}$. The most common
    adopted noise is dropout noise/binary masking noise, which randomly sets a fraction
    of the input features to be zero [[23](#bib.bib23)]. The variant of AE is denoising
    auto-encoder (DA), which can learn more robust representation and prevent it from
    learning the identity transformation.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 添加去噪：与传统的自编码器（AE）不同，去噪自编码器（Denoising AE）将损坏的数据版本作为输入，并被训练以从其损坏的样本 $\tilde{\mathbf{x}}$
    重建/去噪干净的输入 $\mathbf{x}$。最常用的噪声是 dropout 噪声/二进制掩码噪声，它将输入特征的某一部分随机设置为零 [[23](#bib.bib23)]。AE
    的变体是去噪自编码器（DA），它可以学习更强的表示并防止其学习恒等变换。
- en: 'Stacking Structure: Several DA can be stacked together to form a deep network
    and learn high-level representations by feeding the outputs of the $l$-th layer
    as inputs to the $(l+1)$-th layer [[23](#bib.bib23)]. And the training is done
    one layer greedily at a time.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠结构：多个 DA 可以堆叠在一起形成深度网络，通过将第 $l$ 层的输出作为第 $(l+1)$ 层的输入来学习高级表示 [[23](#bib.bib23)]。训练是逐层贪婪地进行的。
- en: 'Since auto-encoder can be trained in an unsupervised way, auto-encoder, especially
    stacked denoising auto-encoder (SDA), can provide an effective pre-training solution
    via initializing the weights of deep neural network (DNN) to train the model.
    After layer-wise pre-training of SDA, the parameters of auto-encoders can be set
    to the initialization for all the hidden layers of DNN. And then, the supervised
    fine-tuning is performed to minimize prediction error on a labeled training data.
    Usually, a softmax/regression layer is added on top of the network to map the
    output of the last layer in AE to targets. The whole process is shown in Figure
    [2](#S2.F2 "Figure 2 ‣ II-A Auto-encoders (AE) and its variants ‣ II Deep Learning
    ‣ Deep Learning and Its Applications to Machine Health Monitoring: A Survey").
    The pre-training protocol based on SDA can make DNN models have better convergence
    capability compared to arbitrary random initialization.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自编码器可以以无监督方式进行训练，自编码器，特别是堆叠去噪自编码器（SDA），可以通过初始化深度神经网络（DNN）的权重来提供有效的预训练解决方案。经过
    SDA 的逐层预训练后，自编码器的参数可以设置为 DNN 所有隐藏层的初始化。然后，进行监督微调，以最小化在标记训练数据上的预测误差。通常，在网络的顶部添加一个
    softmax/回归层，以将 AE 的最后一层的输出映射到目标。整个过程如图 [2](#S2.F2 "图 2 ‣ II-A 自编码器（AE）及其变体 ‣ II
    深度学习 ‣ 深度学习及其在机器健康监测中的应用：综述") 所示。基于 SDA 的预训练协议可以使 DNN 模型比任意随机初始化具有更好的收敛能力。
- en: '![Refer to caption](img/2cd1ce8de0db01ba97e66eea5781d9ab.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2cd1ce8de0db01ba97e66eea5781d9ab.png)'
- en: 'Figure 2: Illustrations for Unsupervised Pre-training and Supervised Fine-tuning
    of SAE-DNN (a) and DBN-DNN (b).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：SAE-DNN (a) 和 DBN-DNN (b) 的无监督预训练和监督微调说明。
- en: II-B RBM and its variants
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B RBM 及其变体
- en: As a special type of Markov random field, restricted Boltzmann machine (RBM)
    is a two-layer neural network forming a bipartite graph that consists of two groups
    of units including visible units $\mathbf{v}$ and hidden units $\mathbf{h}$ under
    the constrain that there exists a symmetric connection between visible units and
    hidden units and there are no connections between nodes with a group.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种特殊类型的马尔可夫随机场，限制玻尔兹曼机（RBM）是一个两层神经网络，形成一个二分图，该图包括两个组的单元，其中包括可见单元 $\mathbf{v}$
    和隐藏单元 $\mathbf{h}$，其约束条件是可见单元和隐藏单元之间存在对称连接，而同一组内的节点之间没有连接。
- en: 'Given the model parameters $\theta=[\mathbf{W},\mathbf{b},\mathbf{a}]$, the
    energy function can be given as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定模型参数 $\theta=[\mathbf{W},\mathbf{b},\mathbf{a}]$，能量函数可以表示为：
- en: '|  | $E(\mathbf{v},\mathbf{h};\theta)=-\sum_{i=1}^{I}\sum_{j=1}^{J}w_{ij}v_{i}h_{j}-\sum_{i=1}^{I}b_{i}v_{i}-\sum_{j=1}^{J}a_{j}h_{j}$
    |  | (6) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(\mathbf{v},\mathbf{h};\theta)=-\sum_{i=1}^{I}\sum_{j=1}^{J}w_{ij}v_{i}h_{j}-\sum_{i=1}^{I}b_{i}v_{i}-\sum_{j=1}^{J}a_{j}h_{j}$
    |  | (6) |'
- en: 'that $w_{ij}$ is the connecting weight between visible unit $v_{i}$, whose
    total number is $I$ and hidden unit $h_{j}$ whose total number is $J$, $b_{i}$
    and $a_{j}$ denote the bias terms for visible units and hidden units, respectively.
    The joint distribution over all the units is calculated based on the energy function
    $E(\mathbf{v},\mathbf{h};\theta)$ as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$w_{ij}$是可见单元$v_{i}$（总数为$I$）和隐藏单元$h_{j}$（总数为$J$）之间的连接权重，$b_{i}$和$a_{j}$分别表示可见单元和隐藏单元的偏置项。所有单元的联合分布是基于能量函数$E(\mathbf{v},\mathbf{h};\theta)$计算的：
- en: '|  | $p(\mathbf{v},\mathbf{h};\theta)=\frac{exp(-E(\mathbf{v},\mathbf{h};\theta))}{Z}$
    |  | (7) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(\mathbf{v},\mathbf{h};\theta)=\frac{exp(-E(\mathbf{v},\mathbf{h};\theta))}{Z}$
    |  | (7) |'
- en: 'where $Z=\sum_{\mathbf{h};\mathbf{v}}exp(-E(\mathbf{v},\mathbf{h};\theta))$
    is the partition function or normalization factor. Then, the conditional probabilities
    of hidden and visible units $\mathbf{h}$ and $\mathbf{v}$ can be calculated as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Z=\sum_{\mathbf{h};\mathbf{v}}exp(-E(\mathbf{v},\mathbf{h};\theta))$是配分函数或归一化因子。然后，隐藏单元和可见单元$\mathbf{h}$和$\mathbf{v}$的条件概率可以计算为：
- en: '|  | $p(h_{j}=1&#124;v;\theta)=\delta(\sum_{i=1}^{I}w_{ij}v_{i}+a_{j})$ |  |
    (8) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(h_{j}=1&#124;v;\theta)=\delta(\sum_{i=1}^{I}w_{ij}v_{i}+a_{j})$ |  |
    (8) |'
- en: '|  | $p(v_{i}=1&#124;v;\theta)=\delta(\sum_{j=1}^{J}w_{ij}h_{j}+b_{i})$ |  |
    (9) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(v_{i}=1&#124;v;\theta)=\delta(\sum_{j=1}^{J}w_{ij}h_{j}+b_{i})$ |  |
    (9) |'
- en: where $\delta$ is defined as a logistic function i.e., $\delta(x)=\frac{1}{1+exp(x)}$.
    RBM are trained to maximize the joint probability. The learning of W is done through
    a method called contrastive divergence (CD).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\delta$被定义为逻辑函数，即$\delta(x)=\frac{1}{1+exp(x)}$。RBM（限制玻尔兹曼机）被训练以最大化联合概率。W的学习通过一种称为对比散度（CD）的方法完成。
- en: '![Refer to caption](img/d2de88975bc95bf8b8682f2c10b90bc6.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/d2de88975bc95bf8b8682f2c10b90bc6.png)'
- en: 'Figure 3: Frameworks showing RBM, DBN and DBM. Shaded boxes denote hidden untis.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：显示RBM、DBN和DBM的框架。阴影框表示隐藏单元。
- en: 'Deep Belief Network: Deep belief networks (DBN) can be constructed by stacking
    multiple RBMs, where the output of the $l$-th layer (hidden units) is used as
    the input of the $(l+1)$-th layer (visible units). Similar to SDA, DBN can be
    trained in a greedy layer-wise unsupervised way. After pre-training, the parameters
    of this deep architecture can be further fine-tuned with respect to a proxy for
    the DBN log- likelihood, or with respect to labels of training data by adding
    a softmax layer as the top layer, which is shown in Figure [2](#S2.F2 "Figure
    2 ‣ II-A Auto-encoders (AE) and its variants ‣ II Deep Learning ‣ Deep Learning
    and Its Applications to Machine Health Monitoring: A Survey").(b).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '深度置信网络：深度置信网络（DBN）可以通过堆叠多个RBM来构建，其中第$l$层（隐藏单元）的输出被用作第$(l+1)$层（可见单元）的输入。与SDA类似，DBN可以以贪婪的逐层无监督方式进行训练。在预训练之后，可以进一步对这个深度结构的参数进行微调，以优化DBN对数似然的代理，或者通过添加一个softmax层作为顶层来利用训练数据的标签，如图[2](#S2.F2
    "Figure 2 ‣ II-A Auto-encoders (AE) and its variants ‣ II Deep Learning ‣ Deep
    Learning and Its Applications to Machine Health Monitoring: A Survey")所示。(b)。'
- en: 'Deep Boltzmann Machine: Deep Boltzmann machine (DBM) can be regarded as a deep
    structured RMBs where hidden units are grouped into a hierarchy of layers instead
    of a single layer. And following the RMB’s connectivity constraint, there is only
    full connectivity between subsequent layers and no connections within layers or
    between non-neighbouring layers are allowed. The main difference between DBN and
    DBM lies that DBM is fully undirected graphical model, while DBN is mixed directed/undirected
    one. Different from DBN that can be trained layer-wisely, DBM is trained as a
    joint model. Therefore, the training of DBM is more computationally expensive
    than that of DBN.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 深度玻尔兹曼机：深度玻尔兹曼机（DBM）可以看作是一种深度结构化的RMB，其中隐藏单元被分组为层级结构，而不是单层。并且遵循RMB的连接约束，只有后续层之间完全连接，不允许层内或非相邻层之间的连接。DBN和DBM之间的主要区别在于DBM是完全无向图模型，而DBN是混合的有向/无向图模型。不同于可以逐层训练的DBN，DBM作为一个整体模型进行训练。因此，DBM的训练比DBN更具计算开销。
- en: II-C Convolutioanl Neural Network
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 卷积神经网络
- en: 'Convolutional neural networks (CNNs) were firstly proposed by LeCun [[32](#bib.bib32)]
    for image processing, which is featured by two key properties: spatially shared
    weights and spatial pooling. CNN models have shown their success in various computer
    vision applications [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)] where
    input data are usually 2D data. CNN has also been introduced to address sequential
    data including Natural Language Processing and Speech Recognition [[35](#bib.bib35),
    [36](#bib.bib36)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）最初由 LeCun 提出[[32](#bib.bib32)]用于图像处理，其特点是两个关键属性：空间共享权重和空间池化。CNN
    模型在各种计算机视觉应用中显示了成功[[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)]，其中输入数据通常是二维数据。CNN
    也被引入来处理序列数据，包括自然语言处理和语音识别[[35](#bib.bib35), [36](#bib.bib36)]。
- en: 'CNN aims to learn abstract features by alternating and stacking convolutional
    kernels and pooling operation. In CNN, the convolutional layers (convolutional
    kernels) convolve multiple local filters with raw input data and generate invariant
    local features and the subsequent pooling layers extract most significant features
    with a fixed-length over sliding windows of the raw input data. Considering 2D-CNN
    have been illustrated extensively in previous research compared to 1D-CNN, here,
    only the mathematical details behind 1D-CNN is given as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 旨在通过交替堆叠卷积核和池化操作来学习抽象特征。在 CNN 中，卷积层（卷积核）与原始输入数据卷积多个局部滤波器，生成不变的局部特征，而随后的池化层通过固定长度在原始输入数据的滑动窗口上提取最重要的特征。考虑到相较于
    1D-CNN，2D-CNN 在之前的研究中已被广泛阐述，这里仅给出 1D-CNN 背后的数学细节：
- en: Firstly, we assume that the input sequential data is $\mathbf{x}=[\mathbf{x}_{1},\dots,\mathbf{x}_{T}]$
    that $T$ is the length of the sequence and $\mathbf{x}_{i}\in{\mathbb{R}^{d}}$
    at each time step.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设输入序列数据为 $\mathbf{x}=[\mathbf{x}_{1},\dots,\mathbf{x}_{T}]$，其中 $T$ 是序列的长度，每个时间步的
    $\mathbf{x}_{i}\in{\mathbb{R}^{d}}$。
- en: 'Convolution: the dot product between a filter vector $\mathbf{u}\in{\mathbb{R}^{md}}$
    and an concatenation vector representation $\mathbf{x}_{i:i+m-1}$ defines the
    convolution operation as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积：滤波器向量 $\mathbf{u}\in{\mathbb{R}^{md}}$ 和连接向量表示 $\mathbf{x}_{i:i+m-1}$ 之间的点积定义了卷积操作如下：
- en: '|  | $c_{i}=\varphi(\mathbf{u}^{T}\mathbf{x}_{i:i+m-1}+b)$ |  | (10) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{i}=\varphi(\mathbf{u}^{T}\mathbf{x}_{i:i+m-1}+b)$ |  | (10) |'
- en: 'where ${\mathbf{*}}^{T}$ represents the transpose of a matrix ${\mathbf{*}}$,
    $b$ and $\varphi$ denotes bias term and non-linear activation function, respectively.
    $\mathbf{x}_{i:i+m-1}$ is a $m$-length window starting from the $i$-th time step,
    which is described as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathbf{*}}^{T}$ 表示矩阵 ${\mathbf{*}}$ 的转置，$b$ 和 $\varphi$ 分别表示偏置项和非线性激活函数。$\mathbf{x}_{i:i+m-1}$
    是从第 $i$ 个时间步开始的 $m$ 长度窗口，其描述如下：
- en: '|  | $\mathbf{x}_{i:i+m-1}=\mathbf{x}_{i}\oplus\mathbf{x}_{i+1}\oplus\dots\oplus\mathbf{x}_{i+m-1}$
    |  | (11) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}_{i:i+m-1}=\mathbf{x}_{i}\oplus\mathbf{x}_{i+1}\oplus\dots\oplus\mathbf{x}_{i+m-1}$
    |  | (11) |'
- en: 'As defined in Eq. ([10](#S2.E10 "In II-C Convolutioanl Neural Network ‣ II
    Deep Learning ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey")), the output scale $c_{i}$ can be regarded as the activation of the
    filter $\mathbf{u}$ on the corresponding subsequence $\mathbf{x}_{i:i+m-1}$. By
    sliding the filtering window from the beginning time step to the ending time step,
    a feature map as a vector can be given as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '如公式 ([10](#S2.E10 "In II-C Convolutioanl Neural Network ‣ II Deep Learning
    ‣ Deep Learning and Its Applications to Machine Health Monitoring: A Survey"))
    中定义的，输出尺度 $c_{i}$ 可以视为滤波器 $\mathbf{u}$ 对应子序列 $\mathbf{x}_{i:i+m-1}$ 的激活。通过将过滤窗口从开始时间步滑动到结束时间步，可以得到如下的特征图作为向量：'
- en: '|  | $\mathbf{c}_{j}=\left[c_{1},c_{2},\dots,c_{l-m+1}\right]$ |  | (12) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{c}_{j}=\left[c_{1},c_{2},\dots,c_{l-m+1}\right]$ |  | (12) |'
- en: where the index $j$ represents the $j$-th filter. It corresponds to multi-windows
    as $\{\mathbf{x}_{1:m},\mathbf{x}_{2:m+1},\dots,\mathbf{x}_{l-m+1:l}\}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中索引 $j$ 表示第 $j$ 个滤波器。它对应于多个窗口，如 $\{\mathbf{x}_{1:m},\mathbf{x}_{2:m+1},\dots,\mathbf{x}_{l-m+1:l}\}$。
- en: 'Max-pooling: Pooling layer is able to reduce the length of the feature map,
    which can further minimize the number of model parameters. The hyper-parameter
    of pooling layer is pooling length denoted as $s$. MAX operation is taking a max
    over the $s$ consecutive values in feature map $\mathbf{c}_{j}$.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化：池化层能够减少特征图的长度，这可以进一步最小化模型参数的数量。池化层的超参数是池化长度，记作 $s$。最大操作是在特征图 $\mathbf{c}_{j}$
    中取 $s$ 个连续值的最大值。
- en: 'Then, the compressed feature vector can be obtained as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以获得压缩的特征向量：
- en: '|  | $\mathbf{h}=\left[h_{1},h_{2},\dots,h_{\frac{l-m}{s}+1}\right]$ |  | (13)
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}=\left[h_{1},h_{2},\dots,h_{\frac{l-m}{s}+1}\right]$ |  | (13)
    |'
- en: 'where $h_{j}=\max(c_{(j-1)s},c_{(j-1)s+1},\dots,c_{js-1})$. Then, via alternating
    the above two layers: convolution and max-pooling ones, fully connected layers
    and a softmax layer are usually added as the top layers to make predictions. To
    give a clear illustration, the framework for a one-layer CNN has been displayed
    in Figure. [4](#S2.F4 "Figure 4 ‣ II-C Convolutioanl Neural Network ‣ II Deep
    Learning ‣ Deep Learning and Its Applications to Machine Health Monitoring: A
    Survey").'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $h_{j}=\max(c_{(j-1)s},c_{(j-1)s+1},\dots,c_{js-1})$。然后，通过交替使用上述两个层：卷积层和最大池化层，通常在顶部添加全连接层和一个
    softmax 层以进行预测。为了清楚说明，图 [4](#S2.F4 "Figure 4 ‣ II-C Convolutioanl Neural Network
    ‣ II Deep Learning ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey") 显示了一个单层 CNN 的框架。'
- en: '![Refer to caption](img/a400912f1844327bc6311db0c270ced4.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a400912f1844327bc6311db0c270ced4.png)'
- en: 'Figure 4: Illustrations for one-layer CNN that contains one convolutional layer,
    one pooling layer, one fully-connected layer, and one softmax layer.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：包含一个卷积层、一个池化层、一个全连接层和一个 softmax 层的单层 CNN 的示意图。
- en: II-D Recurrent Neural Network
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 循环神经网络
- en: 'As stated in [[12](#bib.bib12)], recurrent neural networks (RNN) are the deepest
    of all neural networks, which can generate and address memories of arbitrary-length
    sequences of input patterns. RNN is able to build connections between units from
    a directed cycle. Different from basic neural network: multi-layer perceptron
    that can only map from input data to target vectors, RNN is able to map from the
    entire history of previous inputs to target vectors in principal and allows a
    memory of previous inputs to be kept in the network’s internal state. RNNs can
    be trained via backpropagation through time for supervised tasks with sequential
    input data and target outputs [[37](#bib.bib37), [27](#bib.bib27), [38](#bib.bib38)].'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [[12](#bib.bib12)] 所述，循环神经网络（RNN）是所有神经网络中最深的，它可以生成并处理任意长度序列的输入模式记忆。RNN 能够在有向循环之间建立连接。不同于只能从输入数据映射到目标向量的基本神经网络：多层感知机，RNN
    能够从整个历史输入映射到目标向量，并允许将先前输入的记忆保留在网络的内部状态中。RNN 可以通过时间反向传播进行训练，以处理具有序列输入数据和目标输出的监督任务
    [[37](#bib.bib37), [27](#bib.bib27), [38](#bib.bib38)]。
- en: 'RNN can address the sequential data using its internal memory, as shown in
    Figure [5](#S2.F5 "Figure 5 ‣ II-D Recurrent Neural Network ‣ II Deep Learning
    ‣ Deep Learning and Its Applications to Machine Health Monitoring: A Survey").
    (a). The transition function defined in each time step $t$ takes the current time
    information $\mathbf{x}_{t}$ and the previous hidden output $\mathbf{h}_{t-1}$
    and updates the current hidden output as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 'RNN 可以利用其内部记忆处理序列数据，如图 [5](#S2.F5 "Figure 5 ‣ II-D Recurrent Neural Network
    ‣ II Deep Learning ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey") 所示。 (a)。在每个时间步骤 $t$ 中定义的转移函数将当前时间信息 $\mathbf{x}_{t}$ 和之前的隐藏输出 $\mathbf{h}_{t-1}$
    结合起来，并更新当前隐藏输出，如下所示：'
- en: '|  | $\mathbf{h}_{t}{}=\mathbb{H}(\mathbf{x}_{t},\mathbf{h}_{t-1})$ |  | (14)
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{t}{}=\mathbb{H}(\mathbf{x}_{t},\mathbf{h}_{t-1})$ |  | (14)
    |'
- en: where $\mathbb{H}$ defines a nonlinear and differentiable transformation function.
    After processing the whole sequence, the hidden output at the last time step i.e.
    $\mathbf{h}_{T}$ is the learned representation of the input sequential data whose
    length is $T$. A conventional Multilayer perceptron (MLP) is added on top to map
    the obtained representation $\mathbf{h}_{T}$ to targets.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{H}$ 定义了一个非线性且可微的转换函数。在处理完整个序列后，最后一个时间步骤的隐藏输出，即 $\mathbf{h}_{T}$，是输入序列数据的学习表示，其长度为
    $T$。一个传统的多层感知机（MLP）被添加在顶部，以将获得的表示 $\mathbf{h}_{T}$ 映射到目标。
- en: 'Various transition functions can lead to various RNN models. The most simple
    one is vanilla RNN that is given as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 各种转移函数可以导致各种 RNN 模型。最简单的是 vanilla RNN，其定义如下：
- en: '|  | $\mathbf{h}_{t}=\varphi(\mathbf{W}\mathbf{x}_{t}+\mathbf{H}\mathbf{h}_{t-1}+\mathbf{b})$
    |  | (15) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{t}=\varphi(\mathbf{W}\mathbf{x}_{t}+\mathbf{H}\mathbf{h}_{t-1}+\mathbf{b})$
    |  | (15) |'
- en: where $\mathbf{W}$ and $\mathbf{H}$ denote transformation matrices and $\mathbf{b}$
    is the bias vector. And $\varphi$ denote the nonlinear activation function such
    as sigmoid and tanh functions. Due to the vanishing gradient problem during backpropagation
    for model training, vanilla RNN may not capture long-term dependencies. Therefore,
    Long-short term memory (LSTM) and gated recurrent neural networks (GRU) were presented
    to prevent backpropagated errors from vanishing or exploding [[39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]. The core
    idea behind these advanced RNN variants is that gates are introduced to avoid
    the long-term dependency problem and enable each recurrent unit to adaptively
    capture dependencies of different time scales.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{W}$ 和 $\mathbf{H}$ 表示变换矩阵，$\mathbf{b}$ 是偏置向量。$\varphi$ 表示非线性激活函数，如sigmoid和tanh函数。由于模型训练中反向传播的消失梯度问题，传统RNN可能无法捕捉长期依赖。因此，提出了长短期记忆（LSTM）和门控递归神经网络（GRU）以防止反向传播错误消失或爆炸
    [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]。这些高级RNN变体的核心思想是引入门控机制，以避免长期依赖问题，并使每个递归单元能够自适应地捕捉不同时间尺度的依赖关系。
- en: 'Besides these proposed advanced transition functions such as LSTMs and GRUs,
    multi-layer and bi-directional recurrent structure can increase the model capacity
    and flexibility. As shown in Figure [5](#S2.F5 "Figure 5 ‣ II-D Recurrent Neural
    Network ‣ II Deep Learning ‣ Deep Learning and Its Applications to Machine Health
    Monitoring: A Survey").(b), multi-layer structure can enable the hidden output
    of one recurrent layer to be propagated through time and used as the input data
    to the next recurrent layer. And the bidirectional recurrent structure is able
    to process the sequence data in two directions including forward and backward
    ways with two separate hidden layers, which is illustrated in Figure [5](#S2.F5
    "Figure 5 ‣ II-D Recurrent Neural Network ‣ II Deep Learning ‣ Deep Learning and
    Its Applications to Machine Health Monitoring: A Survey").(c). The following equations
    define the corresponding hidden layer function and the $\rightarrow$ and $\leftarrow$
    denote forward and backward process, respectively.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '除了这些提议的高级转换函数，如LSTM和GRU，多层和双向递归结构可以增加模型的容量和灵活性。如图[5](#S2.F5 "Figure 5 ‣ II-D
    Recurrent Neural Network ‣ II Deep Learning ‣ Deep Learning and Its Applications
    to Machine Health Monitoring: A Survey") (b)所示，多层结构可以使一个递归层的隐藏输出在时间上进行传播，并用作下一个递归层的输入数据。双向递归结构能够以前向和后向两种方向处理序列数据，具有两个独立的隐藏层，如图[5](#S2.F5
    "Figure 5 ‣ II-D Recurrent Neural Network ‣ II Deep Learning ‣ Deep Learning and
    Its Applications to Machine Health Monitoring: A Survey") (c)所示。以下方程定义了相应的隐藏层函数，$\rightarrow$
    和 $\leftarrow$ 分别表示前向和后向过程。'
- en: '|  | <math id="S2.E16.m1.27" class="ltx_Math" alttext="\displaystyle\begin{split}\overrightarrow{\mathbf{h}}_{t}&amp;=\overrightarrow{\mathbb{H}}(\mathbf{x}_{t},\overrightarrow{\mathbf{h}}_{t-1}),\\
    \overleftarrow{\mathbf{h}}_{t}&amp;=\overleftarrow{\mathbb{H}}(\mathbf{x}_{t},\overleftarrow{\mathbf{h}}_{t+1}).\\'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math id="S2.E16.m1.27" class="ltx_Math" alttext="\displaystyle\begin{split}\overrightarrow{\mathbf{h}}_{t}&amp;=\overrightarrow{\mathbb{H}}(\mathbf{x}_{t},\overrightarrow{\mathbf{h}}_{t-1}),\\
    \overleftarrow{\mathbf{h}}_{t}&amp;=\overleftarrow{\mathbb{H}}(\mathbf{x}_{t},\overleftarrow{\mathbf{h}}_{t+1}).\\'
- en: \end{split}" display="inline"><semantics id="S2.E16.m1.27a"><mtable columnspacing="0pt"
    rowspacing="0pt" id="S2.E16.m1.27.27.3"><mtr id="S2.E16.m1.27.27.3a"><mtd class="ltx_align_right"
    columnalign="right" id="S2.E16.m1.27.27.3b"><msub id="S2.E16.m1.2.2.2.2.2a"><mover
    accent="true" id="S2.E16.m1.1.1.1.1.1.1" xref="S2.E16.m1.1.1.1.1.1.1.cmml"><mi
    id="S2.E16.m1.1.1.1.1.1.1.2" xref="S2.E16.m1.1.1.1.1.1.1.2.cmml">𝐡</mi><mo stretchy="false"
    id="S2.E16.m1.1.1.1.1.1.1.1" xref="S2.E16.m1.1.1.1.1.1.1.1.cmml">→</mo></mover><mi
    id="S2.E16.m1.2.2.2.2.2.2.1" xref="S2.E16.m1.2.2.2.2.2.2.1.cmml">t</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S2.E16.m1.27.27.3c"><mrow id="S2.E16.m1.26.26.2.25.13.11.11"><mrow
    id="S2.E16.m1.26.26.2.25.13.11.11.1"><mo id="S2.E16.m1.3.3.3.3.1.1" xref="S2.E16.m1.3.3.3.3.1.1.cmml">=</mo><mrow
    id="S2.E16.m1.26.26.2.25.13.11.11.1.2"><mover accent="true" id="S2.E16.m1.4.4.4.4.2.2"
    xref="S2.E16.m1.4.4.4.4.2.2.cmml"><mi id="S2.E16.m1.4.4.4.4.2.2.2" xref="S2.E16.m1.4.4.4.4.2.2.2.cmml">ℍ</mi><mo
    stretchy="false" id="S2.E16.m1.4.4.4.4.2.2.1" xref="S2.E16.m1.4.4.4.4.2.2.1.cmml">→</mo></mover><mo
    lspace="0em" rspace="0em" id="S2.E16.m1.26.26.2.25.13.11.11.1.2.3">​</mo><mrow
    id="S2.E16.m1.26.26.2.25.13.11.11.1.2.2.2"><mo stretchy="false" id="S2.E16.m1.5.5.5.5.3.3">(</mo><msub
    id="S2.E16.m1.26.26.2.25.13.11.11.1.1.1.1.1"><mi id="S2.E16.m1.6.6.6.6.4.4" xref="S2.E16.m1.6.6.6.6.4.4.cmml">𝐱</mi><mi
    id="S2.E16.m1.7.7.7.7.5.5.1" xref="S2.E16.m1.7.7.7.7.5.5.1.cmml">t</mi></msub><mo
    id="S2.E16.m1.8.8.8.8.6.6">,</mo><msub id="S2.E16.m1.26.26.2.25.13.11.11.1.2.2.2.2"><mover
    accent="true" id="S2.E16.m1.9.9.9.9.7.7" xref="S2.E16.m1.9.9.9.9.7.7.cmml"><mi
    id="S2.E16.m1.9.9.9.9.7.7.2" xref="S2.E16.m1.9.9.9.9.7.7.2.cmml">𝐡</mi><mo stretchy="false"
    id="S2.E16.m1.9.9.9.9.7.7.1" xref="S2.E16.m1.9.9.9.9.7.7.1.cmml">→</mo></mover><mrow
    id="S2.E16.m1.10.10.10.10.8.8.1" xref="S2.E16.m1.10.10.10.10.8.8.1.cmml"><mi id="S2.E16.m1.10.10.10.10.8.8.1.2"
    xref="S2.E16.m1.10.10.10.10.8.8.1.2.cmml">t</mi><mo id="S2.E16.m1.10.10.10.10.8.8.1.1"
    xref="S2.E16.m1.10.10.10.10.8.8.1.1.cmml">−</mo><mn id="S2.E16.m1.10.10.10.10.8.8.1.3"
    xref="S2.E16.m1.10.10.10.10.8.8.1.3.cmml">1</mn></mrow></msub><mo stretchy="false"
    id="S2.E16.m1.11.11.11.11.9.9">)</mo></mrow></mrow></mrow><mo id="S2.E16.m1.12.12.12.12.10.10">,</mo></mrow></mtd></mtr><mtr
    id="S2.E16.m1.27.27.3d"><mtd class="ltx_align_right" columnalign="right" id="S2.E16.m1.27.27.3e"><msub
    id="S2.E16.m1.14.14.14.2.2a"><mover accent="true" id="S2.E16.m1.13.13.13.1.1.1"
    xref="S2.E16.m1.13.13.13.1.1.1.cmml"><mi id="S2.E16.m1.13.13.13.1.1.1.2" xref="S2.E16.m1.13.13.13.1.1.1.2.cmml">𝐡</mi><mo
    stretchy="false" id="S2.E16.m1.13.13.13.1.1.1.1" xref="S2.E16.m1.13.13.13.1.1.1.1.cmml">←</mo></mover><mi
    id="S2.E16.m1.14.14.14.2.2.2.1" xref="S2.E16.m1.14.14.14.2.2.2.1.cmml">t</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S2.E16.m1.27.27.3f"><mrow id="S2.E16.m1.27.27.3.26.13.11.11"><mrow
    id="S2.E16.m1.27.27.3.26.13.11.11.1"><mo id="S2.E16.m1.15.15.15.3.1.1" xref="S2.E16.m1.15.15.15.3.1.1.cmml">=</mo><mrow
    id="S2.E16.m1.27.27.3.26.13.11.11.1.2"><mover accent="true" id="S2.E16.m1.16.16.16.4.2.2"
    xref="S2.E16.m1.16.16.16.4.2.2.cmml"><mi id="S2.E16.m1.16.16.16.4.2.2.2" xref="S2.E16.m1.16.16.16.4.2.2.2.cmml">ℍ</mi><mo
    stretchy="false" id="S2.E16.m1.16.16.16.4.2.2.1" xref="S2.E16.m1.16.16.16.4.2.2.1.cmml">←</mo></mover><mo
    lspace="0em" rspace="0em" id="S2.E16.m1.27.27.3.26.13.11.11.1.2.3">​</mo><mrow
    id="S2.E16.m1.27.27.3.26.13.11.11.1.2.2.2"><mo stretchy="false" id="S2.E16.m1.17.17.17.5.3.3">(</mo><msub
    id="S2.E16.m1.27.27.3.26.13.11.11.1.1.1.1.1"><mi id="S2.E16.m1.18.18.18.6.4.4"
    xref="S2.E16.m1.18.18.18.6.4.4.cmml">𝐱</mi><mi id="S2.E16.m1.19.19.19.7.5.5.1"
    xref="S2.E16.m1.19.19.19.7.5.5.1.cmml">t</mi></msub><mo id="S2.E16.m1.20.20.20.8.6.6">,</mo><msub
    id="S2.E16.m1.27.27.3.26.13.11.11.1.2.2.2.2"><mover accent="true" id="S2.E16.m1.21.21.21.9.7.7"
    xref="S2.E16.m1.21.21.21.9.7.7.cmml"><mi id="S2.E16.m1.21.21.21.9.7.7.2" xref="S2.E16.m1.21.21.21.9.7.7.2.cmml">𝐡</mi><mo
    stretchy="false" id="S2.E16.m1.21.21.21.9.7.7.1" xref="S2.E16.m1.21.21.21.9.7.7.1.cmml">←</mo></mover><mrow
    id="S2.E16.m1.22.22.22.10.8.8.1" xref="S2.E16.m1.22.22.22.10.8.8.1.cmml"><mi id="S2.E16.m1.22.22.22.10.8.8.1.2"
    xref="S2.E16.m1.22.22.22.10.8.8.1.2.cmml">t</mi><mo id="S2.E16.m1.22.22.22.10.8.8.1.1"
    xref="S2.E16.m1.22.22.22.10.8.8.1.1.cmml">+</mo><mn id="S2.E16.m1.22.22.22.10.8.8.1.3"
    xref="S2.E16.m1.22.22.22.10.8.8.1.3.cmml">1</mn></mrow></msub><mo stretchy="false"
    id="S2.E16.m1.23.23.23.11.9.9">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E16.m1.24.24.24.12.10.10">.</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S2.E16.m1.27b"><apply id="S2.E16.m1.25.25.1.1.1.3.cmml"><csymbol
    cd="ambiguous" id="S2.E16.m1.25.25.1.1.1.3a.cmml">formulae-sequence</csymbol><apply
    id="S2.E16.m1.25.25.1.1.1.1.1.cmml"><apply id="S2.E16.m1.25.25.1.1.1.1.1.4.cmml"><csymbol
    cd="ambiguous" id="S2.E16.m1.25.25.1.1.1.1.1.4.1.cmml">subscript</csymbol><apply
    id="S2.E16.m1.1.1.1.1.1.1.cmml" xref="S2.E16.m1.1.1.1.1.1.1"><ci id="S2.E16.m1.1.1.1.1.1.1.1.cmml"
    xref="S2.E16.m1.1.1.1.1.1.1.1">→</ci><ci id="S2.E16.m1.1.1.1.1.1.1.2.cmml" xref="S2.E16.m1.1.1.1.1.1.1.2">𝐡</ci></apply><ci
    id="S2.E16.m1.2.2.2.2.2.2.1.cmml" xref="S2.E16.m1.2.2.2.2.2.2.1">𝑡</ci></apply><apply
    id="S2.E16.m1.25.25.1.1.1.1.1.2.cmml"><apply id="S2.E16.m1.4.4.4.4.2.2.cmml" xref="S2.E16.m1.4.4.4.4.2.2"><ci
    id="S2.E16.m1.4.4.4.4.2.2.1.cmml" xref="S2.E16.m1.4.4.4.4.2.2.1">→</ci><ci id="S2.E16.m1.4.4.4.4.2.2.2.cmml"
    xref="S2.E16.m1.4.4.4.4.2.2.2">ℍ</ci></apply><interval closure="open" id="S2.E16.m1.25.25.1.1.1.1.1.2.2.3.cmml"><apply
    id="S2.E16.m1.25.25.1.1.1.1.1.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S2.E16.m1.25.25.1.1.1.1.1.1.1.1.1.1.cmml">subscript</csymbol><ci
    id="S2.E16.m1.6.6.6.6.4.4.cmml" xref="S2.E16.m1.6.6.6.6.4.4">𝐱</ci><ci id="S2.E16.m1.7.7.7.7.5.5.1.cmml"
    xref="S2.E16.m1.7.7.7.7.5.5.1">𝑡</ci></apply><apply id="S2.E16.m1.25.25.1.1.1.1.1.2.2.2.2.cmml"><csymbol
    cd="ambiguous" id="S2.E16.m1.25.25.1.1.1.1.1.2.2.2.2.1.cmml">subscript</csymbol><apply
    id="S2.E16.m1.9.9.9.9.7.7.cmml" xref="S2.E16.m1.9.9.9.9.7.7"><ci id="S2.E16.m1.9.9.9.9.7.7.1.cmml"
    xref="S2.E16.m1.9.9.9.9.7.7.1">→</ci><ci id="S2.E16.m1.9.9.9.9.7.7.2.cmml" xref="S2.E16.m1.9.9.9.9.7.7.2">𝐡</ci></apply><apply
    id="S2.E16.m1.10.10.10.10.8.8.1.cmml" xref="S2.E16.m1.10.10.10.10.8.8.1"><ci id="S2.E16.m1.10.10.10.10.8.8.1.2.cmml"
    xref="S2.E16.m1.10.10.10.10.8.8.1.2">𝑡</ci><cn type="integer" id="S2.E16.m1.10.10.10.10.8.8.1.3.cmml"
    xref="S2.E16.m1.10.10.10.10.8.8.1.3">1</cn></apply></apply></interval></apply></apply><apply
    id="S2.E16.m1.25.25.1.1.1.2.2.cmml"><apply id="S2.E16.m1.25.25.1.1.1.2.2.4.cmml"><csymbol
    cd="ambiguous" id="S2.E16.m1.25.25.1.1.1.2.2.4.1.cmml">subscript</csymbol><apply
    id="S2.E16.m1.13.13.13.1.1.1.cmml" xref="S2.E16.m1.13.13.13.1.1.1"><ci id="S2.E16.m1.13.13.13.1.1.1.1.cmml"
    xref="S2.E16.m1.13.13.13.1.1.1.1">←</ci><ci id="S2.E16.m1.13.13.13.1.1.1.2.cmml"
    xref="S2.E16.m1.13.13.13.1.1.1.2">𝐡</ci></apply><ci id="S2.E16.m1.14.14.14.2.2.2.1.cmml"
    xref="S2.E16.m1.14.14.14.2.2.2.1">𝑡</ci></apply><apply id="S2.E16.m1.25.25.1.1.1.2.2.2.cmml"><apply
    id="S2.E16.m1.16.16.16.4.2.2.cmml" xref="S2.E16.m1.16.16.16.4.2.2"><ci id="S2.E16.m1.16.16.16.4.2.2.1.cmml"
    xref="S2.E16.m1.16.16.16.4.2.2.1">←</ci><ci id="S2.E16.m1.16.16.16.4.2.2.2.cmml"
    xref="S2.E16.m1.16.16.16.4.2.2.2">ℍ</ci></apply><interval closure="open" id="S2.E16.m1.25.25.1.1.1.2.2.2.2.3.cmml"><apply
    id="S2.E16.m1.25.25.1.1.1.2.2.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S2.E16.m1.25.25.1.1.1.2.2.1.1.1.1.1.cmml">subscript</csymbol><ci
    id="S2.E16.m1.18.18.18.6.4.4.cmml" xref="S2.E16.m1.18.18.18.6.4.4">𝐱</ci><ci id="S2.E16.m1.19.19.19.7.5.5.1.cmml"
    xref="S2.E16.m1.19.19.19.7.5.5.1">𝑡</ci></apply><apply id="S2.E16.m1.25.25.1.1.1.2.2.2.2.2.2.cmml"><csymbol
    cd="ambiguous" id="S2.E16.m1.25.25.1.1.1.2.2.2.2.2.2.1.cmml">subscript</csymbol><apply
    id="S2.E16.m1.21.21.21.9.7.7.cmml" xref="S2.E16.m1.21.21.21.9.7.7"><ci id="S2.E16.m1.21.21.21.9.7.7.1.cmml"
    xref="S2.E16.m1.21.21.21.9.7.7.1">←</ci><ci id="S2.E16.m1.21.21.21.9.7.7.2.cmml"
    xref="S2.E16.m1.21.21.21.9.7.7.2">𝐡</ci></apply><apply id="S2.E16.m1.22.22.22.10.8.8.1.cmml"
    xref="S2.E16.m1.22.22.22.10.8.8.1"><ci id="S2.E16.m1.22.22.22.10.8.8.1.2.cmml"
    xref="S2.E16.m1.22.22.22.10.8.8.1.2">𝑡</ci><cn type="integer" id="S2.E16.m1.22.22.22.10.8.8.1.3.cmml"
    xref="S2.E16.m1.22.22.22.10.8.8.1.3">1</cn></apply></apply></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E16.m1.27c">\displaystyle\begin{split}\overrightarrow{\mathbf{h}}_{t}&=\overrightarrow{\mathbb{H}}(\mathbf{x}_{t},\overrightarrow{\mathbf{h}}_{t-1}),\\
    \overleftarrow{\mathbf{h}}_{t}&=\overleftarrow{\mathbb{H}}(\mathbf{x}_{t},\overleftarrow{\mathbf{h}}_{t+1}).\\
    \end{split}</annotation></semantics></math> |  | (16) |
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="inline"><semantics id="S2.E16.m1.27a"><mtable columnspacing="0pt"
    rowspacing="0pt" id="S2.E16.m1.27.27.3"><mtr id="S2.E16.m1.27.27.3a"><mtd class="ltx_align_right"
    columnalign="right" id="S2.E16.m1.27.27.3b"><msub id="S2.E16.m1.2.2.2.2.2a"><mover
    accent="true" id="S2.E16.m1.1.1.1.1.1.1" xref="S2.E16.m1.1.1.1.1.1.1.cmml"><mi
    id="S2.E16.m1.1.1.1.1.1.1.2" xref="S2.E16.m1.1.1.1.1.1.1.2.cmml">𝐡</mi><mo stretchy="false"
    id="S2.E16.m1.1.1.1.1.1.1.1" xref="S2.E16.m1.1.1.1.1.1.1.1.cmml">→</mo></mover><mi
    id="S2.E16.m1.2.2.2.2.2.2.1" xref="S2.E16.m1.2.2.2.2.2.2.1.cmml">t</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S2.E16.m1.27.27.3c"><mrow id="S2.E16.m1.26.26.2.25.13.11.11"><mrow
    id="S2.E16.m1.26.26.2.25.13.11.11.1"><mo id="S2.E16.m1.3.3.3.3.1.1" xref="S2.E16.m1.3.3.3.3.1.1.cmml">=</mo><mrow
    id="S2.E16.m1.26.26.2.25.13.11.11.1.2"><mover accent="true" id="S2.E16.m1.4.4.4.4.2.2"
    xref="S2.E16.m1.4.4.4.4.2.2.cmml"><mi id="S2.E16.m1.4.4.4.4.2.2.2" xref="S2.E16.m1.4.4.4.4.2.2.2.cmml">ℍ</mi><mo
    stretchy="false" id="S2.E16.m1.4.4.4.4.2.2.1" xref="S2.E16.m1.4.4.4.4.2.2.1.cmml">→</mo></mover><mo
    lspace="0em" rspace="0em" id="S2.E16.m1.26.26.2.25.13.11.11.1.2.3">​</mo><mrow
    id="S2.E16.m1.26.26.2.25.13.11.11.1.2.2.2"><mo stretchy="false" id="S2.E16.m1.5.5.5.5.3.3">(</mo><msub
    id="S2.E16.m1.26.26.2.25.13.11.11.1.1.1.1.1"><mi id="S2.E16.m1.6.6.6.6.4.4" xref="S2.E16.m1.6.6.6.6.4.4.cmml">𝐱</mi><mi
    id="S2.E16.m1.7.7.7.7.5.5.1" xref="S2.E16.m1.7.7.7.7.5.5.1.cmml">t</mi></msub><mo
    id="S2.E16.m1.8.8.8.8.6.6">,</mo><msub id="S2.E16.m1.26.26.2.25.13.11.11.1.2.2.2.2"><mover
    accent="true" id="S2.E16.m1.9.9.9.9.7.7" xref="S2.E16.m1.9.9.9.9.7.7.cmml"><mi
    id="S2.E16.m1.9.9.9.9.7.7.2" xref="S2.E16.m1.9.9.9.9.7.7.2.cmml">𝐡</mi><mo stretchy="false"
    id="S2.E16.m1.9.9.9.9.7.7.1" xref="S2.E16.m1.9.9.9.9.7.7.1.cmml">→</mo></mover><mrow
    id="S2.E16.m1.10.10.10.10.8.8.1" xref="S2.E16.m1.10.10.10.10.8.8.1.cmml"><mi id="S2.E16.m1.10.10.10.10.8.8.1.2"
    xref="S2.E16.m1.10.10.10.10.8.8.1.2.cmml">t</mi><mo id="S2.E16.m1.10.10.10.10.8.8.1.1"
    xref="S2.E16.m1.10.10.10.10.8.8.1.1.cmml">−</mo><mn id="S2.E16.m1.10.10.10.10.8.8.1.3"
    xref="S2.E16.m1.10.10.10.10.8.8.1.3.cmml">1</mn></mrow></msub><mo stretchy="false"
    id="S2.E16.m1.11.11.11.11.9.9">)</mo></mrow></mrow></mrow><mo id="S2.E16.m1.12.12.12.12.10.10">,</mo></mrow></mtd></mtr><mtr
    id="S2.E16.m1.27.27.3d"><mtd class="ltx_align_right" columnalign="right" id="S2.E16.m1.27.27.3e"><msub
    id="S2.E16.m1.14.14.14.2.2a"><mover accent="true" id="S2.E16.m1.13.13.13.1.1.1"
    xref="S2.E16.m1.13.13.13.1.1.1.cmml"><mi id="S2.E16.m1.13.13.13.1.1.1.2" xref="S2.E16.m1.13.13.13.1.1.1.2.cmml">𝐡</mi><mo
    stretchy="false" id="S2.E16.m1.13.13.13.1.1.1.1" xref="S2.E16.m1.13.13.13.1.1.1.1.cmml">←</mo></mover><mi
    id="S2.E16.m1.14.14.14.2.2.2.1" xref="S2.E16.m1.14.14.14.2.2.2.1.cmml">t</mi></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S2.E16.m1.27.27.3f"><mrow id="S2.E16.m1.27.27.3.26.13.11.11"><mrow
    id="S2.E16.m1.27.27.3.26.13.11.11.1"><mo id="S2.E16.m1.15.15.15.3.1.1" xref="S2.E16.m1.15.15.15.3.1.1.cmml">=</mo><mrow
    id="S2.E16.m1.27.27.3.26.13.11.11.1.2"><mover accent="true" id="S2.E16.m1.16.16.16.4.2.2"
    xref="S2.E16.m1.16.16.16.4.2.2.cmml"><mi id="S2.E16.m1.16.16.16.4.2.2.2" xref="S2.E16.m1.16.16.16.4.2.2.2.cmml">ℍ</mi><mo
    stretchy="false" id="S2.E16.m1.16.16.16.4
- en: 'Then, the final vector $\mathbf{h}^{T}$ is the concatenated vector of the outputs
    of forward and backward processes as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，最终向量 $\mathbf{h}^{T}$ 是前向和反向过程输出的连接向量，如下所示：
- en: '|  | $\mathbf{h}_{T}=\overrightarrow{\mathbf{h}}_{T}\oplus\overleftarrow{\mathbf{h}}_{1}$
    |  | (17) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}_{T}=\overrightarrow{\mathbf{h}}_{T}\oplus\overleftarrow{\mathbf{h}}_{1}$
    |  | (17) |'
- en: '![Refer to caption](img/9f66c9195431b95ddf90c7521b0c6b67.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/9f66c9195431b95ddf90c7521b0c6b67.png)'
- en: 'Figure 5: Illustrations of normal RNN, stacked RNN and bidirectional RNN.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：正常RNN、堆叠RNN和双向RNN的示意图。
- en: III Applications of Deep learning in machine health monitoring
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度学习在机器健康监测中的应用
- en: 'The conventional multilayer perceptron (MLP) has been applied in the field
    of machine health monitoring for many years [[44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)]. The deep learning techniques have recently
    been applied to a large number of machine health monitoring systems. The layer-by-layer
    pretraining of deep neural network (DNN) based on Auto-encoder or RBM can facilitate
    the training of DNN and improve its discriminative power to characterize machinery
    data. Convolution neural network and recurrent neural networks provide more advanced
    and complex compositions mechanism to learn representation from machinery data.
    In these DL-based MHMS systems, the top layer normally represents the targets.
    For diagnosis where targets are discrete values, softmax layer is applied. For
    prognosis with continuous targets, liner regression layer is added. What is more,
    the end-to-end structure enables DL-based MHMS to be constructed with less human
    labor and expert knowledge, therefore these models are not limited to specific
    machine specific or domain. In the following, a brief survey of DL-based MHMS
    are presented in these above four DL architectures: AE, RBM, CNN and RNN.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的多层感知器（MLP）在机器健康监测领域已经应用多年[[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47)]。近年来，深度学习技术已被广泛应用于机器健康监测系统中。基于自编码器（Auto-encoder）或限制玻尔兹曼机（RBM）的深度神经网络（DNN）逐层预训练可以促进DNN的训练，并提高其对机器数据的区分能力。卷积神经网络（CNN）和递归神经网络（RNN）提供了更先进和复杂的组合机制，以从机器数据中学习表示。在这些基于深度学习的MHMS系统中，顶层通常代表目标。对于目标是离散值的诊断，应用softmax层；对于目标是连续值的预后，添加线性回归层。此外，端到端的结构使基于深度学习的MHMS可以在较少的人力和专家知识下构建，因此这些模型不受特定机器或领域的限制。接下来，将简要介绍以上四种深度学习架构：AE、RBM、CNN和RNN在DL-based
    MHMS中的应用。
- en: III-A AE and its variants for machine health monitoring
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A AE及其变体在机器健康监测中的应用
- en: AE models, especially stacked DA, can learn high-level representations from
    machinery data in an automatic way. Sun et al. proposed a one layer AE-based neural
    network to classify induction motor faults [[48](#bib.bib48)]. Due to the limited
    size of training data, they focused on how to prevent overfiting. Not only the
    number of hidden layer was set to 1, but also dropout technique that masks portions
    of output neurons randomly was applied on the hidden layer. However, most of proposed
    models are based on deep architectures by stacking multiple auto-encoders. Lu
    et al. presented a detailed empirical study of stacked denoising autoencoders
    with three hidden layers for fault diagnosis of rotary machinery components [[49](#bib.bib49)].
    Specifically, in their experiments including single working condition and cross
    working conditions, the effectiveness of the receptive input size, deep architecture,
    sparsity constraint and denosing operation in the SDA model were evaluated. In
    [[50](#bib.bib50)], different structures of a two-layer SAE-based DNN were designed
    by varying hidden layer size and its masking probability, and evaluated for their
    performances in fault diagnosis.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AE模型，特别是堆叠DA，可以以自动方式从机器数据中学习高层次的表示。Sun等人提出了一种基于AE的单层神经网络，用于分类感应电动机故障[[48](#bib.bib48)]。由于训练数据的有限性，他们专注于如何防止过拟合。隐藏层的数量不仅设置为1，还在隐藏层上应用了随机掩码输出神经元的dropout技术。然而，大多数提出的模型都是基于通过堆叠多个自编码器的深度架构。Lu等人对用于旋转机械部件故障诊断的三层堆叠去噪自编码器进行了详细的实证研究[[49](#bib.bib49)]。具体而言，在他们的实验中，包括单一工作条件和交叉工作条件，评估了接收输入大小、深度架构、稀疏约束和SDA模型中的去噪操作的有效性。在[[50](#bib.bib50)]中，通过改变隐藏层大小和掩码概率设计了不同结构的两层SAE-based
    DNN，并评估了其在故障诊断中的性能。
- en: 'In these above works, the input features to AE models are raw sensory time-series.
    Therefore, the input dimensionality is always over hundred, even one thousand.
    The possible high dimensionality may lead to some potential concerns such as heavy
    computation cost and overfiting caused by huge model parameters. Therefore, some
    researchers focused on AE models built upon features extracted from raw input.
    Jia et al. fed the frequency spectra of time-series data into SAE for rotating
    machinery diagnosis [[51](#bib.bib51)], considering the frequency spectra is able
    to demonstrate how their constitutive components are distributed with discrete
    frequencies and may be more discriminative over the health conditions of rotating
    machinery. The corresponding framework proposed by Jia et al. is shown in Figure
    [6](#S3.F6 "Figure 6 ‣ III-A AE and its variants for machine health monitoring
    ‣ III Applications of Deep learning in machine health monitoring ‣ Deep Learning
    and Its Applications to Machine Health Monitoring: A Survey"). Tan et al. used
    digital wavelet frame and nonlinear soft threshold method to process the vibration
    signal and built a SAE on the preprocessed signal for roller bearing fault diagnosis
    [[52](#bib.bib52)]. Zhu et al. proposed a SAE-based DNN for hydraulic pump fault
    diagnosis with input as frequency domain features after Fourier transform [[53](#bib.bib53)].
    In experiments, ReLU activation and dropout technique were analyzed and experimental
    results have shown to be effective in preventing gradient vanishing and overfiting.
    In the work presented in [[54](#bib.bib54)], the normalized spectrogram generated
    by STFT of sound signal was fed into two-layers SAE-based DNN for rolling bearing
    fault diagnosis. Galloway et al. built a two layer SAE-based DNN on spectrograms
    generated from raw vibration data for tidal turbine vibration fault diagnosis
    [[55](#bib.bib55)]. A SAE-based DNN with input as principal components of data
    extracted by principal component analysis was proposed for spacecraft fault diagnosis
    in [[56](#bib.bib56)]. Multi-domain statistical features including time domain
    features, frequency domain features and time-frequency domain features were fed
    into the SAE framework, which can be regarded as one kind of feature fusion [[57](#bib.bib57)].
    Similarly, Verma et al. also used these three domains features to fed into a SAE-based
    DNN for fault diagnosis of air compressors [[58](#bib.bib58)].'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '在上述工作中，AE模型的输入特征是原始感测时间序列。因此，输入维度通常超过百甚至一千。高维度可能会引发一些潜在问题，例如计算成本高和由于模型参数巨大而导致的过拟合。因此，一些研究人员关注于基于从原始输入中提取的特征构建AE模型。贾等人将时间序列数据的频谱输入到SAE中用于旋转机械诊断[[51](#bib.bib51)]，考虑到频谱能够展示其组成部分在离散频率上的分布，并且可能对旋转机械的健康状况具有更高的辨别能力。贾等人提出的相应框架如图[6](#S3.F6
    "Figure 6 ‣ III-A AE and its variants for machine health monitoring ‣ III Applications
    of Deep learning in machine health monitoring ‣ Deep Learning and Its Applications
    to Machine Health Monitoring: A Survey")所示。谭等人使用数字小波框架和非线性软阈值方法处理振动信号，并在预处理信号上构建了SAE用于滚子轴承故障诊断[[52](#bib.bib52)]。朱等人提出了一种基于SAE的DNN用于液压泵故障诊断，输入为傅里叶变换后的频域特征[[53](#bib.bib53)]。在实验中，分析了ReLU激活和dropout技术，实验结果显示有效防止了梯度消失和过拟合。在[[54](#bib.bib54)]中，STFT生成的归一化声谱图被输入到两层SAE-based
    DNN中用于滚动轴承故障诊断。加洛韦等人基于原始振动数据生成的声谱图构建了一个两层SAE-based DNN用于潮汐涡轮振动故障诊断[[55](#bib.bib55)]。在[[56](#bib.bib56)]中，提出了一种基于SAE的DNN，输入为主成分分析提取的数据的主成分，用于航天器故障诊断。多领域统计特征，包括时域特征、频域特征和时频域特征，被输入到SAE框架中，这可以视为一种特征融合[[57](#bib.bib57)]。类似地，Verma等人也使用了这三种领域特征输入到基于SAE的DNN中用于空气压缩机故障诊断[[58](#bib.bib58)]。'
- en: Except that these applied multi-domain features, multi-sensory data are also
    addressed by SAE models. Reddy utilized SAE to learn representation on raw time
    series data from multiple sensors for anomaly detection and fault disambiguation
    in flight data. To address multi-sensory data, synchronized windows were firstly
    traversed over multi-modal time series with overlap, and then windows from each
    sensor were concatenated as the input to the following SAE [[59](#bib.bib59)].
    In [[60](#bib.bib60)], SAE was leveraged for multi-sensory data fusion and the
    followed DBN was adopted for bearing fault diagnosis, which achieved promising
    results. The statistical features in time domain and frequency domain extracted
    from the vibration signals of different sensors were adopted as input to a two-layer
    SAE with sparsity constraint neural networks. And the learned representation were
    fed into a deep belief network for pattern classification.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了应用多领域特征外，SAE模型还处理了多传感器数据。Reddy 利用 SAE 从多个传感器的原始时间序列数据中学习表示，用于异常检测和飞行数据中的故障消歧。为了处理多传感器数据，首先在多模态时间序列上进行同步窗口的遍历，并且这些窗口有重叠，然后将每个传感器的窗口连接起来作为
    SA 的输入 [[59](#bib.bib59)]。在 [[60](#bib.bib60)] 中，SAE 被用于多传感器数据融合，随后采用深度置信网络（DBN）进行轴承故障诊断，取得了令人满意的结果。通过从不同传感器的振动信号中提取的时间域和频率域的统计特征被用作输入到一个具有稀疏性约束的两层
    SAE 神经网络中。然后，将学习到的表示输入到深度信念网络中进行模式分类。
- en: 'In addition, some variants of the conventional SAE were proposed or introduced
    for machine health monitoring. In [[61](#bib.bib61)], Thirukovalluru et al. proposed
    a two-phase framework that SAE only learn representation and other standard classifiers
    such as SVM and random forest perform classification. Specifically, in SAE module,
    handcrafted features based on FFT and WPT were fed into SAE-based DNN. After pre-training
    and supervised fine-tuning which includes two separated procedures: softmax-based
    and Median-based fine-tuning methods, the extensive experiments on five datasets
    including air compressor monitoring, drill bit monitoring, bearing fault monitoring
    and steel plate monitoring have demonstrated the generalization capability of
    DL-based machine health monitoring systems. Wang et al. proposed a novel continuous
    sparse auto-encoder (CSAE) as an unsupervised feature learning for transformer
    fault recognition [[62](#bib.bib62)]. Different from conventional sparse AE, their
    proposed CSAE added the stochastic unit into activation function of each visible
    unit as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还提出了一些传统 SAE 的变体用于机器健康监测。在 [[61](#bib.bib61)] 中，Thirukovalluru 等人提出了一个两阶段框架，其中
    SAE 仅用于学习表示，其他标准分类器如 SVM 和随机森林则执行分类。具体来说，在 SAE 模块中，基于 FFT 和 WPT 的手工特征被输入到基于 SAE
    的 DNN 中。经过预训练和监督微调，这包括两个分开的过程：基于 softmax 的和基于中位数的微调方法，针对包括空气压缩机监测、钻头监测、轴承故障监测和钢板监测在内的五个数据集进行了广泛的实验，展示了基于深度学习的机器健康监测系统的泛化能力。王等人提出了一种新型连续稀疏自编码器（CSAE），作为变压器故障识别的无监督特征学习
    [[62](#bib.bib62)]。与传统的稀疏自编码器不同，他们提出的 CSAE 在每个可见单元的激活函数中加入了随机单元，如下所示：
- en: '|  | $s_{j}=\varphi_{j}(\sum_{i}{w_{ij}x{i}}+a_{i}+\sigma N_{j}(0,1))$ |  |
    (18) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{j}=\varphi_{j}(\sum_{i}{w_{ij}x{i}}+a_{i}+\sigma N_{j}(0,1))$ |  |
    (18) |'
- en: where $s_{j}$ is the output corresponding to the input $x_{i}$, $w_{ij}$ and
    $a_{i}$ denote model parameters, $\varphi_{j}$ represents the activation function
    and the last term $\sigma N_{j}(0,1))$ is the added stochastic unit, which is
    a zero-mean Gaussian with variance $\sigma^{2}$. The incorporation of stochastic
    unit is able to change the gradient direction and prevent over-fitting. Mao et
    al. adopted a variant of AE named Extreme Learning Machine-based auto-encoder
    for bearing fault diagnosis, which is more efficient than conventional SAE models
    without sacrificing accuracies in fault diagnosis [[63](#bib.bib63)]. Different
    from AE that is trained via back-propagation, the transformation in encoder phase
    is randomly generated and the one in decoder phase is learned in a single step
    via least-squares fit [[64](#bib.bib64)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{j}$ 是与输入 $x_{i}$ 对应的输出，$w_{ij}$ 和 $a_{i}$ 表示模型参数，$\varphi_{j}$ 代表激活函数，最后一项
    $\sigma N_{j}(0,1))$ 是加入的随机单元，它是一个均值为零，方差为 $\sigma^{2}$ 的高斯分布。随机单元的引入能够改变梯度方向并防止过拟合。毛等人采用了一种基于极限学习机的自编码器变体，用于轴承故障诊断，这种方法比传统的稀疏自编码器（SAE）模型更高效，同时在故障诊断的准确性上没有妥协
    [[63](#bib.bib63)]。与通过反向传播训练的自编码器不同，该编码器阶段的转换是随机生成的，而解码器阶段的转换则通过最小二乘拟合在一步中完成 [[64](#bib.bib64)]。
- en: In addition, Lu et al. focused on the visualization of learned representation
    by a two-layer SAE-based DNN, which provides a novel view to evaluate the DL-based
    MHMS [[65](#bib.bib65)]. In their paper, the discriminative power of learned representation
    can be improved with the increasing of layers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Lu等人专注于通过基于两层SAE的DNN可视化学习到的表示，这为评估基于DL的MHMS提供了一种新颖的视角[[65](#bib.bib65)]。在他们的论文中，随着层数的增加，学习到的表示的区分能力可以得到提升。
- en: '![Refer to caption](img/c0047eee0efdfb11b42ddd6d1dadc476.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c0047eee0efdfb11b42ddd6d1dadc476.png)'
- en: 'Figure 6: Illustrations of the proposed SAE-DNN for rotating machinery diagnosis
    in [[51](#bib.bib51)].'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：[[51](#bib.bib51)]中提出的SAE-DNN用于旋转机械诊断的示意图。
- en: III-B RBM and its variants for machine health monitoring
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B RBM及其变体在机器健康监测中的应用
- en: In the section, some work focused on developing RBM and DBM to learn representation
    from machinery data. Most of works introduced here are based on deep belief networks
    (DBN) that can pretrain a deep neural network (DNN).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的一些工作专注于开发RBM和DBM以从机械数据中学习表示。这里介绍的大多数工作基于深度信念网络（DBN），可以预训练深度神经网络（DNN）。
- en: In [[66](#bib.bib66)], a RBM based method for bearing remaining useful life
    (RUL) prediction was proposed. Linear regression layer was added at the top of
    RBM after pretraining to predict the future root mean square (RMS) based on a
    lagged time series of RMS values. Then, RUL was calculated by using the predicted
    RMS and the total time of the bearing’s life. Liao et al. proposed a new RBM for
    representation learning to predict RUL of machines [[67](#bib.bib67)]. In their
    work, a new regularization term modeling the trendability of the hidden nodes
    was added into the training objective function of RBM. Then, unsupervised self-organizing
    map algorithm (SOM) was applied to transforming the representation learned by
    the enhanced RBM to one scale named health value. Finally, the health value was
    used to predict RUL via a similarity-based life prediction algorithm. In [[68](#bib.bib68)],
    a multi-modal deep support vector classification approach was proposed for fault
    diagnosis of gearboxes. Firstly, three modalities features including time, frequency
    and time-frequency ones were extracted from vibration signals. Then, three Gaussian-Bernoulli
    deep Boltzmann machines (GDBMS) were applied to addressing the above three modalities,
    respectively. In each GDBMS, the softmax layer was used at the top. After the
    pretraining and the fine-tuning processes, the probabilistic outputs of the softmax
    layers from these three GDBMS were fused by a support vector classification (SVC)
    framework to make the final prediction. Li et al. applied one GDBMS directly on
    the concatenation feature consisting of three modalities features including time,
    frequency and time-frequency ones and stacked one softmax layer on top of GDBMS
    to recognize fault categories [[69](#bib.bib69)]. Li et al. adopted a two-layers
    DBM to learn deep representations of the statistical parameters of the wavelet
    packet transform (WPT) of raw sensory signal for gearbox fault diagnosis [[70](#bib.bib70)].
    In this work focusing on data fusion, two DBMs were applied on acoustic and vibratory
    signals and random forest was applied to fusing the representations learned by
    these two DBMs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[66](#bib.bib66)]中，提出了一种基于RBM的轴承剩余使用寿命（RUL）预测方法。在预训练后，在线性回归层被添加到RBM的顶部，以根据延迟的RMS值时间序列预测未来的均方根（RMS）。然后，利用预测的RMS和轴承的总寿命计算RUL。Liao等人提出了一种新的RBM用于表示学习，以预测机器的RUL[[67](#bib.bib67)]。在他们的工作中，向RBM的训练目标函数中添加了一种新的正则化项，以建模隐藏节点的趋势性。随后，应用无监督自组织映射算法（SOM）将增强RBM学习到的表示转换为一种称为健康值的尺度。最后，使用基于相似度的寿命预测算法通过健康值预测RUL。在[[68](#bib.bib68)]中，提出了一种多模态深度支持向量分类方法用于齿轮箱故障诊断。首先，从振动信号中提取了包括时间、频率和时间-频率在内的三种模态特征。然后，应用了三个高斯-伯努利深度玻尔兹曼机（GDBMS）分别处理上述三种模态。在每个GDBMS中，顶部使用了softmax层。经过预训练和微调过程后，这三个GDBMS的softmax层的概率输出通过支持向量分类（SVC）框架融合以进行最终预测。Li等人直接在由时间、频率和时间-频率特征组成的连接特征上应用了一个GDBMS，并在GDBMS上堆叠了一个softmax层，以识别故障类别[[69](#bib.bib69)]。Li等人采用了一个两层DBM来学习原始传感信号的子波包变换（WPT）统计参数的深度表示，以进行齿轮箱故障诊断[[70](#bib.bib70)]。在这项专注于数据融合的工作中，两个DBM分别应用于声学和振动信号，并使用随机森林融合这两个DBM学习到的表示。
- en: 'Making use of DBN-based DNN, Ma et al. presented this framework for degradation
    assessment under a bearing accelerated life test [[71](#bib.bib71)]. The statistical
    feature, root mean square (RMS) fitted by Weibull distribution that can avoid
    areas of fluctuation of the statistical parameter and the frequency domain features
    were extracted as raw input. To give a clear illustration, the framework in [[71](#bib.bib71)]
    is shown in Figure [7](#S3.F7 "Figure 7 ‣ III-B RBM and its variants for machine
    health monitoring ‣ III Applications of Deep learning in machine health monitoring
    ‣ Deep Learning and Its Applications to Machine Health Monitoring: A Survey").
    Shao et al. proposed DBN for induction motor fault diagnosis with the direct usage
    of vibration signals as input [[72](#bib.bib72)]. Beside the evaluation of the
    final classification accuracies, t-SNE algorithm was adopted to visualize the
    learned representation of DBN and outputs of each layer in DBN. They found the
    addition of hidden layer can increase the discriminative power in the learned
    representation. Fu et al. employed deep belief networks for cutting states monitoring
    [[73](#bib.bib73)]. In the presented work, three different feature sets including
    raw vibration signal, Mel-frequency cepstrum coefficient (MFCC) and wavelet features
    were fed into DBN as three corresponding different inputs, which were able to
    achieve robust comparative performance on the raw vibration signal without too
    much feature engineering. Tamilselvan et al. proposed a multi-sensory DBN-based
    health state classification model. The model was verified in benchmark classification
    problems and two health diagnosis applications including aircraft engine health
    diagnosis and electric power transformer health diagnosis [[74](#bib.bib74), [75](#bib.bib75)].
    Tao et al. proposed DBN based multisensor information fusion scheme for bearing
    fault diagnosis [[76](#bib.bib76)]. Firstly, 14 time-domain statistical features
    extracted from three vibration signals acquired by three sensors were concatenated
    together as an input vector to DBM model. During pre-training, a predefined threshold
    value was introduced to determine its iteration number. In [[77](#bib.bib77)],
    a feature vector consisting of load and speed measure, time domain features and
    frequency domain features was fed into DBN-based DNN for gearbox fault diagnosis.
    In the work of [[78](#bib.bib78)], Gan et al. built a hierarchical diagnosis network
    for fault pattern recognition of rolling element bearings consisting of two consecutive
    phases where the four different fault locations (including one health state) were
    firstly identified and then discrete fault severities in each fault condition
    were classified. In each phases, the frequency-band energy features generated
    by WPT were fed into DBN-based DNN for pattern classification. In [[79](#bib.bib79)],
    raw vibration signals were pre-processed to generate 2D image based on omnidirectional
    regeneration (ODR) techniques and then, histogram of original gradients (HOG)
    descriptor was applied on the generated image and the learned vector was fed into
    DBN for automatic diagnosis of journal bearing rotor systems. Chen et al. proposed
    an ensemble of DBNs with multi-objective evolutionary optimization on decomposition
    algorithm (MOEA/D) for fault diagnosis with multivariate sensory data [[80](#bib.bib80)].
    DBNs with different architectures can be regarded as base classifiers and MOEA/D
    was introduced to adjust the ensemble weights to achieve a trade-off between accuracy
    and diversity. Chen et al. then extended this above framework for one specific
    prognostics task: the RUL estimation of the mechanical system [[81](#bib.bib81)].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '利用基于 DBN 的 DNN，Ma 等人提出了这个框架用于轴承加速寿命测试下的退化评估 [[71](#bib.bib71)]。统计特征是通过 Weibull
    分布拟合的均方根（RMS），可以避免统计参数波动的区域，频域特征也被提取作为原始输入。为了清楚地说明，[[71](#bib.bib71)] 中的框架如图 [7](#S3.F7
    "Figure 7 ‣ III-B RBM and its variants for machine health monitoring ‣ III Applications
    of Deep learning in machine health monitoring ‣ Deep Learning and Its Applications
    to Machine Health Monitoring: A Survey") 所示。Shao 等人提出了用于感应电动机故障诊断的 DBN，直接使用振动信号作为输入
    [[72](#bib.bib72)]。除了最终分类准确性的评估，t-SNE 算法被用来可视化 DBN 的学习表示和 DBN 中每一层的输出。他们发现增加隐藏层可以提高学习表示的区分能力。Fu
    等人采用了深度信念网络来监测切削状态 [[73](#bib.bib73)]。在所提出的工作中，三种不同的特征集，包括原始振动信号、梅尔频率倒谱系数（MFCC）和小波特征，被输入到
    DBN 中作为三种不同的输入，这些输入能够在原始振动信号上实现强健的比较性能，而无需过多的特征工程。Tamilselvan 等人提出了一个基于多传感器 DBN
    的健康状态分类模型。该模型在基准分类问题和两个健康诊断应用（包括飞机发动机健康诊断和电力变压器健康诊断）中进行了验证 [[74](#bib.bib74),
    [75](#bib.bib75)]。Tao 等人提出了基于 DBN 的多传感器信息融合方案用于轴承故障诊断 [[76](#bib.bib76)]。首先，将从三个传感器获得的三个振动信号中提取的
    14 个时域统计特征拼接在一起，作为 DBM 模型的输入向量。在预训练过程中，引入了一个预定义的阈值来确定其迭代次数。在 [[77](#bib.bib77)]
    中，将包括负载和速度测量、时域特征和频域特征在内的特征向量输入到基于 DBN 的 DNN 中用于齿轮箱故障诊断。在 [[78](#bib.bib78)] 的工作中，Gan
    等人建立了一个分层诊断网络，用于滚动体轴承的故障模式识别，包括两个连续阶段，其中首先识别四个不同的故障位置（包括一个健康状态），然后对每个故障条件中的离散故障严重性进行分类。在每个阶段中，由
    WPT 生成的频带能量特征被输入到基于 DBN 的 DNN 中进行模式分类。在 [[79](#bib.bib79)] 中，原始振动信号经过预处理生成基于全向再生（ODR）技术的
    2D 图像，然后对生成的图像应用原始梯度直方图（HOG）描述符，学习向量被输入到 DBN 中用于自动诊断滑动轴承转子系统。Chen 等人提出了一个包含多目标进化优化（MOEA/D）的
    DBNs 集成，用于多变量传感数据的故障诊断 [[80](#bib.bib80)]。不同架构的 DBNs 可以视为基分类器，MOEA/D 被引入以调整集成权重，以在准确性和多样性之间实现权衡。Chen
    等人随后将这一框架扩展到一个特定的预测任务：机械系统的 RUL 估计 [[81](#bib.bib81)]。'
- en: '![Refer to caption](img/12cdbdf94d62b3b381b3d0262a497d98.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/12cdbdf94d62b3b381b3d0262a497d98.png)'
- en: 'Figure 7: Illustrations of the proposed DBN-DNN for assessment of bearing degration
    in [[71](#bib.bib71)].'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 提出的 DBN-DNN 用于评估轴承退化的示意图 [[71](#bib.bib71)]。'
- en: III-C CNN for machine health monitoring
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 用于机器健康监测的 CNN
- en: 'In some scenarios, machinery data can be presented in a 2D format such as time-frequency
    spectrum, while in some scenarios, they are in a 1D format, i.e., time-series.
    Therefore, CNNs models are able to learn complex and robust representation via
    its convolutional layer. Intuitively, filters in convolutional layers can extract
    local patterns in raw data and stacking these convolutional layers can further
    build complex patterns. Janssens et al. utilized a 2D-CNN model for four categories
    rotating machinery conditions recognition, whose input is DFT of two accelerometer
    signals from two sensors that are placed perpendicular to each other. Therefore,
    the height of input is the number of sensors. The adopted CNN model contains one
    convolutional layer and a fully connected layer. Then, the top softmax layer is
    adopted for classification [[82](#bib.bib82)]. In [[83](#bib.bib83)], Babu et
    al. built a 2D deep convolution neural network to predict the RUL of system based
    on normalized-variate time series from sensor signals, in which one dimension
    of the 2D input is number of sensors as the setting reported in [[82](#bib.bib82)].
    In their model, average pooling is adopted instead of max pooling. Since RUL is
    a continuous value, the top layer was linear regression layer. Ding et al. proposed
    a deep Convolutional Network (ConvNet) where wavelet packet energy (WPE) image
    were used as input for spindle bearing fault diagnosis [[84](#bib.bib84)]. To
    fully discover the hierarchical representation, a multiscale layer was added after
    the last convolutional layer, which concatenates the outputs of the last convolutional
    layer and the ones of the previous pooling layer. Guo et al. proposed a hierarchical
    adaptive deep convolution neural network (ADCNN) [[85](#bib.bib85)]. Firstly,
    the input time series data as a signal-vector was transformed into a $32\times
    32$ matrix, which follows the typical input format adopted by LeNet [[86](#bib.bib86)].
    In addition, they designed a hierarchical framework to recognize fault patterns
    and fault size. In the fault pattern decision module, the first ADCNN was adopted
    to recognize fault type. In the fault size evaluation layer, based on each fault
    type, ADCNN with the same structure was used to predict fault size. Here, the
    classification mechanism is still used. The predicted value $f$ is defined as
    the probability summation of the typical fault sizes as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些情况下，机械数据可以以二维格式呈现，如时频谱，而在其他情况下，它们是以一维格式，即时间序列呈现。因此，CNN 模型能够通过其卷积层学习复杂且鲁棒的表示。直观地，卷积层中的滤波器可以提取原始数据中的局部模式，并且堆叠这些卷积层可以进一步构建复杂的模式。Janssens
    等人利用 2D-CNN 模型进行四类旋转机械状态识别，其输入是来自两个相互垂直的传感器的两个加速度计信号的 DFT。因此，输入的高度是传感器的数量。所采用的
    CNN 模型包含一个卷积层和一个全连接层。然后，采用顶部的 softmax 层进行分类 [[82](#bib.bib82)]。在 [[83](#bib.bib83)]
    中，Babu 等人构建了一个 2D 深度卷积神经网络，以基于传感器信号的标准化变量时间序列预测系统的 RUL，其中 2D 输入的一个维度是传感器的数量，如
    [[82](#bib.bib82)] 中报告的设置。在他们的模型中，采用了平均池化而不是最大池化。由于 RUL 是一个连续值，顶层是线性回归层。Ding 等人提出了一种深度卷积网络（ConvNet），其中小波包能量（WPE）图像被用作主轴轴承故障诊断的输入
    [[84](#bib.bib84)]。为了充分发现层次化表示，在最后的卷积层之后添加了一个多尺度层，该层将最后卷积层的输出与前一个池化层的输出进行连接。Guo
    等人提出了一种分层自适应深度卷积神经网络（ADCNN） [[85](#bib.bib85)]。首先，将输入的时间序列数据作为信号向量转换为 $32\times
    32$ 矩阵，这遵循 LeNet [[86](#bib.bib86)] 采用的典型输入格式。此外，他们设计了一个分层框架来识别故障模式和故障大小。在故障模式决策模块中，采用第一个
    ADCNN 来识别故障类型。在故障大小评估层中，根据每种故障类型，使用结构相同的 ADCNN 来预测故障大小。这里仍然使用分类机制。预测值 $f$ 被定义为典型故障大小的概率总和，如下所示：
- en: '|  | $f=\sum_{j=1}^{c}{a_{j}p_{j}}$ |  | (19) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $f=\sum_{j=1}^{c}{a_{j}p_{j}}$ |  | (19) |'
- en: 'where $[p_{1},\dots,p_{c}]$ is produced by the top softmax layer, which denote
    the probability score that each sample belongs to each class size and $a_{j}$
    is the fault size corresponding to the $j$-th fault size. In [[87](#bib.bib87)],
    an enhanced CNN was proposed for machinery fault diagnosis. To pre-process vibration
    data, morlet wavelet was used to decompose the vibration signal and obtain wavelet
    scaleogram. Then, bilinear interpolation was used to rescale the scaleogram into
    a grayscale image with a size of $32\times 32$. In addition, the adaptation of
    ReLU and dropout both boost the model’s diagnosis performance. Chen et al. adopted
    a 2D-CNN for gearbox fault diagnosis, in which the input matrix with a size of
    $16\times 16$ for CNN is reshaped by a vector containing 256 statistic features
    including RMS values, standard deviation, skewness, kurtosis, rotation frequency,
    and applied load [[88](#bib.bib88)]. In addition, 11 different structures of CNN
    were evaluated empirically in their experiments. Weimer et al. did a comprehensive
    study of various design configurations of deep CNN for visual defect detection
    [[89](#bib.bib89)]. In one specific application: industrial optical inspection,
    two directions of model configurations including depth (addition of conv-layer)
    and width (increase of number filters) were investigated. The optimal configuration
    verified empirically has been presented in Table [II](#S3.T2 "Table II ‣ III-C
    CNN for machine health monitoring ‣ III Applications of Deep learning in machine
    health monitoring ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey"). In [[90](#bib.bib90)], CNN was applied in the field of diagnosing
    the early small faults of front-end controlled wind generator (FSCWG) that the
    $784\times 784$ input matrix consists of vibration data of generator input shaft
    (horizontal) and vibration data of generator output shaft (vertical) in time scale.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $[p_{1},\dots,p_{c}]$ 是由顶部 softmax 层产生的，表示每个样本属于每个类别的概率分数，而 $a_{j}$ 是对应于第
    $j$ 个故障大小的故障尺寸。在 [[87](#bib.bib87)] 中，提出了一种用于机械故障诊断的增强型 CNN。为了预处理振动数据，使用了 morlet
    小波对振动信号进行分解，并获得小波尺度图。然后，采用双线性插值将尺度图重新缩放为 $32\times 32$ 大小的灰度图像。此外，ReLU 和 dropout
    的适应性都提升了模型的诊断性能。陈等人采用了 2D-CNN 进行齿轮箱故障诊断，其中 CNN 的输入矩阵大小为 $16\times 16$，由包含 256
    个统计特征（包括 RMS 值、标准差、偏度、峰度、旋转频率和施加负载）的向量重塑 [[88](#bib.bib88)]。此外，在他们的实验中，评估了 11
    种不同结构的 CNN。Weimer 等人对各种深度 CNN 设计配置进行了全面研究，用于视觉缺陷检测 [[89](#bib.bib89)]。在一个具体的应用中：工业光学检测，调查了包括深度（增加
    conv-layer）和宽度（增加滤波器数量）在内的模型配置的两个方向。经过实验证实的最佳配置已在表 [II](#S3.T2 "Table II ‣ III-C
    CNN for machine health monitoring ‣ III Applications of Deep learning in machine
    health monitoring ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey") 中展示。在 [[90](#bib.bib90)] 中，CNN 被应用于诊断前端控制风力发电机（FSCWG）的早期小故障，$784\times
    784$ 的输入矩阵包括发电机输入轴（水平）和发电机输出轴（垂直）的振动数据。'
- en: 'As reviewed in our above section [II-C](#S2.SS3 "II-C Convolutioanl Neural
    Network ‣ II Deep Learning ‣ Deep Learning and Its Applications to Machine Health
    Monitoring: A Survey"), CNN can also be applied to 1D time series signal and the
    corresponding operations have been elaborated. In [[91](#bib.bib91)], the 1D CNN
    was successfully developed on raw time series data for motor fault detection,
    in which feature extraction and classification were integrated together. The corresponding
    framework has been shown in Figure [8](#S3.F8 "Figure 8 ‣ III-C CNN for machine
    health monitoring ‣ III Applications of Deep learning in machine health monitoring
    ‣ Deep Learning and Its Applications to Machine Health Monitoring: A Survey").
    Abdeljaber et al. proposed 1D CNN on normalized vibration signal, which can perform
    vibration-based damage detection and localization of the structural damage in
    real-time. The advantage of this approach is its ability to extract optimal damage-sensitive
    features automatically from the raw acceleration signals, which does not need
    any additional preporcessing or signal processing approaches [[92](#bib.bib92)].'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上面的章节[II-C](#S2.SS3 "II-C 卷积神经网络 ‣ II 深度学习 ‣ 深度学习及其在机器健康监测中的应用：综述")中回顾的，CNN
    也可以应用于 1D 时间序列信号，相关操作已经详细阐述。在[[91](#bib.bib91)]中，1D CNN 成功地在原始时间序列数据上用于电机故障检测，其中特征提取和分类被整合在一起。相应的框架如图[8](#S3.F8
    "图 8 ‣ III-C CNN 在机器健康监测中的应用 ‣ III 深度学习在机器健康监测中的应用 ‣ 深度学习及其在机器健康监测中的应用：综述")所示。Abdeljaber
    等人提出了在标准化振动信号上的 1D CNN，这可以实时执行基于振动的损伤检测和结构损伤定位。该方法的优点是能够自动从原始加速度信号中提取最佳损伤敏感特征，无需任何额外的预处理或信号处理方法[[92](#bib.bib92)]。
- en: 'To present an overview about all these above CNN models that have been successfully
    applied in the area of MHMS, their architectures have been summarized in Table
    [II](#S3.T2 "Table II ‣ III-C CNN for machine health monitoring ‣ III Applications
    of Deep learning in machine health monitoring ‣ Deep Learning and Its Applications
    to Machine Health Monitoring: A Survey"). To explain the used abbreviation, the
    structure of CNN applied in Weimer’s work [[89](#bib.bib89)] is denoted as $\textnormal{Input}[32\times
    32]-64\textnormal{C}[3\times 3]2-64\textnormal{P}[2\times 2]-128\textnormal{C}[3\times
    3]3-128\textnormal{P}[2\times 2]-\textnormal{FC}[1024-1024]2$. It means the input
    2D data is $32\times 32$ and the CNN firstly applied 2 convolutional layers with
    the same design that the filter number is 64 and the filter size is $3\times 3$,
    then stacked one max-pooling layer whose pooling size is $2\times 2$, then applied
    3 convolutional layers with the same design that the filter number is 128 and
    the filer size is $3\times 3$, then applied a pooling layer whose pooling size
    is $2\times 2$, and finally adopted two fully-connected layers whose hidden neuron
    numbers are both 1024\. It should be noted that the size of output layer is not
    given here, considering it is task-specific and usually set to be the number of
    categories.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示所有这些成功应用于 MHMS 领域的 CNN 模型的概述，其架构已在表[II](#S3.T2 "表 II ‣ III-C CNN 在机器健康监测中的应用
    ‣ III 深度学习在机器健康监测中的应用 ‣ 深度学习及其在机器健康监测中的应用：综述")中总结。为解释所用缩写，Weimer 的工作[[89](#bib.bib89)]中应用的
    CNN 结构表示为$\textnormal{Input}[32\times 32]-64\textnormal{C}[3\times 3]2-64\textnormal{P}[2\times
    2]-128\textnormal{C}[3\times 3]3-128\textnormal{P}[2\times 2]-\textnormal{FC}[1024-1024]2$。这意味着输入的
    2D 数据为$32\times 32$，CNN 首先应用了两个卷积层，设计相同，滤波器数量为 64，滤波器大小为$3\times 3$，然后堆叠一个最大池化层，池化大小为$2\times
    2$，接着应用三个卷积层，设计相同，滤波器数量为 128，滤波器大小为$3\times 3$，然后应用一个池化层，池化大小为$2\times 2$，最后采用两个全连接层，隐藏神经元数量均为
    1024。需要注意的是，这里没有给出输出层的大小，考虑到它是特定任务的，通常设置为类别数。
- en: '![Refer to caption](img/c2348498b423ca3bb98f1b1ff1d7ff01.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c2348498b423ca3bb98f1b1ff1d7ff01.png)'
- en: 'Figure 8: Illustrations of the proposed 1D-CNN for real-time motor Fault Detection
    in [[91](#bib.bib91)].'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：为实时电机故障检测提出的 1D-CNN 的插图，见[[91](#bib.bib91)]。
- en: 'Table II: Summary on configurations of CNN-based MHMS. The symbol Input, C,
    P and FC denote the raw input, convolutional layer, pooling layer and Fully-connected
    layer, respectively.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：基于 CNN 的 MHMS 配置总结。符号 Input、C、P 和 FC 分别表示原始输入、卷积层、池化层和全连接层。
- en: '|  | Proposed Models | Configurations of CNN Structures |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | 提出的模型 | CNN 结构配置 |'
- en: '| --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2D CNN | Janssens’s work [[82](#bib.bib82)] | $\textnormal{Input}[5120\times
    2]-32\textnormal{C}[64\times 2]-\textnormal{FC}[200]$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 2D CNN | Janssens的工作 [[82](#bib.bib82)] | $\textnormal{Input}[5120\times
    2]-32\textnormal{C}[64\times 2]-\textnormal{FC}[200]$ |'
- en: '| Babu’s work [[83](#bib.bib83)] | $\textnormal{Input}[27\times 15]-8\textnormal{C}[27\times
    4]-8\textnormal{P}[1\times 2]-14\textnormal{C}[1\times 3]-14\textnormal{P}[1\times
    2]$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Babu的工作 [[83](#bib.bib83)] | $\textnormal{Input}[27\times 15]-8\textnormal{C}[27\times
    4]-8\textnormal{P}[1\times 2]-14\textnormal{C}[1\times 3]-14\textnormal{P}[1\times
    2]$ |'
- en: '| Ding’s work [[84](#bib.bib84)] | $\textnormal{Input}[32\times 32]-20\textnormal{C}[7\times
    7]-20\textnormal{P}[2\times 2]-10\textnormal{C}[6\times 6]-10\textnormal{P}[2\times
    2]-6\textnormal{P}[2\times 2]-\textnormal{FC}[185-24]$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Ding的工作 [[84](#bib.bib84)] | $\textnormal{Input}[32\times 32]-20\textnormal{C}[7\times
    7]-20\textnormal{P}[2\times 2]-10\textnormal{C}[6\times 6]-10\textnormal{P}[2\times
    2]-6\textnormal{P}[2\times 2]-\textnormal{FC}[185-24]$ |'
- en: '| Guo’s work [[85](#bib.bib85)] | $\textnormal{Input}[32\times 32]-5\textnormal{C}[5\times
    5]-5\textnormal{P}[2\times 2]-10\textnormal{C}[5\times 5]-10\textnormal{P}[2\times
    2]-10\textnormal{C}[2\times 2]-10\textnormal{P}[2\times 2]-\textnormal{FC}[100]-\textnormal{FC}[50]$
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Guo的工作 [[85](#bib.bib85)] | $\textnormal{Input}[32\times 32]-5\textnormal{C}[5\times
    5]-5\textnormal{P}[2\times 2]-10\textnormal{C}[5\times 5]-10\textnormal{P}[2\times
    2]-10\textnormal{C}[2\times 2]-10\textnormal{P}[2\times 2]-\textnormal{FC}[100]-\textnormal{FC}[50]$'
- en: '| Wang’s work [[87](#bib.bib87)] | $\textnormal{Input}[32\times 32]-64\textnormal{C}[3\times
    3]-64\textnormal{P}[2\times 2]-64\textnormal{C}[4\times 4]-64\textnormal{P}[2\times
    2]-128\textnormal{C}[3\times 3]-128\textnormal{P}[2\times 2]-\textnormal{FC}[512]$
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Wang的工作 [[87](#bib.bib87)] | $\textnormal{Input}[32\times 32]-64\textnormal{C}[3\times
    3]-64\textnormal{P}[2\times 2]-64\textnormal{C}[4\times 4]-64\textnormal{P}[2\times
    2]-128\textnormal{C}[3\times 3]-128\textnormal{P}[2\times 2]-\textnormal{FC}[512]$
    |'
- en: '| Chen’s work [[88](#bib.bib88)] | $\textnormal{Input}[16\times 16]-8\textnormal{C}[5\times
    5]-8\textnormal{P}[2\times 2]$ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Chen的工作 [[88](#bib.bib88)] | $\textnormal{Input}[16\times 16]-8\textnormal{C}[5\times
    5]-8\textnormal{P}[2\times 2]$ |'
- en: '| Weimer’s work [[89](#bib.bib89)] | $\textnormal{Input}[32\times 32]-64\textnormal{C}[3\times
    3]2-64\textnormal{P}[2\times 2]-128\textnormal{C}[3\times 3]3-128\textnormal{P}[2\times
    2]-\textnormal{FC}[1024-1024]$ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Weimer的工作 [[89](#bib.bib89)] | $\textnormal{Input}[32\times 32]-64\textnormal{C}[3\times
    3]2-64\textnormal{P}[2\times 2]-128\textnormal{C}[3\times 3]3-128\textnormal{P}[2\times
    2]-\textnormal{FC}[1024-1024]$ |'
- en: '| Dong’s work [[90](#bib.bib90)] | $\textnormal{Input}[784\times 784]-12\textnormal{C}[10\times
    10]-12\textnormal{P}[2\times 2]-24\textnormal{C}[10\times 10]-24\textnormal{P}[2\times
    2]-\textnormal{FC}[200]$ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Dong的工作 [[90](#bib.bib90)] | $\textnormal{Input}[784\times 784]-12\textnormal{C}[10\times
    10]-12\textnormal{P}[2\times 2]-24\textnormal{C}[10\times 10]-24\textnormal{P}[2\times
    2]-\textnormal{FC}[200]$ |'
- en: '|   1D CNN | Ince’s work [[91](#bib.bib91)] | $\textnormal{Input}[240]-60\textnormal{C}[9]-60\textnormal{P}[4]-40\textnormal{C}[9]-40\textnormal{P}[4]-40\textnormal{C}[9]-40\textnormal{P}[4]-\textnormal{FC}[20]$
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 1D CNN | Ince的工作 [[91](#bib.bib91)] | $\textnormal{Input}[240]-60\textnormal{C}[9]-60\textnormal{P}[4]-40\textnormal{C}[9]-40\textnormal{P}[4]-40\textnormal{C}[9]-40\textnormal{P}[4]-\textnormal{FC}[20]$
    |'
- en: '| Abdeljaber’s work [[92](#bib.bib92)] | $\textnormal{Input}[128]-64\textnormal{C}[41]-64\textnormal{P}[2]-32\textnormal{C}[41]-32\textnormal{P}[2]-\textnormal{FC}[10-10]$
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Abdeljaber的工作 [[92](#bib.bib92)] | $\textnormal{Input}[128]-64\textnormal{C}[41]-64\textnormal{P}[2]-32\textnormal{C}[41]-32\textnormal{P}[2]-\textnormal{FC}[10-10]$
    |'
- en: III-D RNN for machine health monitoring
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D RNN用于机器健康监测
- en: 'The majority of machinery data belong to sensor data, which are in nature time
    series. RNN models including LSTM and GRU have emerged as one kind of popular
    architectures to handle sequential data with its ability to encode temporal information.
    These advanced RNN models have been proposed to relief the difficulty of training
    in vanilla RNN and applied in machine health monitoring recently. In [[93](#bib.bib93)],
    Yuan et al. investigated three RNN models including vanilla RNN, LSTM and GRU
    models for fault diagnosis and prognostics of aero engine. They found these advanced
    RNN models LSTM and GRU models outperformed vanilla RNN. Another interesting observation
    was the ensemble model of the above three RNN variants did not boost the performance
    of LSTM. Zhao et al. presented an empirical evaluation of LSTMs-based machine
    health monitoring system in the tool wear test [[94](#bib.bib94)]. The applied
    LSTM model encoded the raw sensory data into embeddings and predicted the corresponding
    tool wear. Zhao et al. further designed a more complex deep learning model combining
    CNN and LSTM named Convolutional Bi-directional Long Short-Term Memory Networks
    (CBLSTM) [[95](#bib.bib95)]. As shown in Figure [9](#S3.F9 "Figure 9 ‣ III-D RNN
    for machine health monitoring ‣ III Applications of Deep learning in machine health
    monitoring ‣ Deep Learning and Its Applications to Machine Health Monitoring:
    A Survey"), CNN was used to extract robust local features from the sequential
    input, and then bi-directional LSTM was adopted to encode temporal information
    on the sequential output of CNN. Stacked fully-connected layers and linear regression
    layer were finally added to predict the target value. In tool wear test, the proposed
    model was able to outperform several state-of-the-art baseline methods including
    conventional LSTM models. In [[96](#bib.bib96)], Malhotra proposed a very interesting
    structure for RUL prediction. They designed a LSTM-based encoder-decoder structure,
    which LSTM-based encoder firstly transforms a multivariate input sequence to a
    fixed-length vector and then, LSTM decoder uses the vector to produce the target
    sequence. When it comes to RUL prediction, their assumptions lies that the model
    can be firstly trained in raw signal corresponding to normal behavior in an unsupervised
    way. Then, the reconstruction error can be used to compute health index (HI),
    which is then used for RUL estimation. It is intuitive that the large reconstruction
    error corresponds to a more unhealthy machine condition.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机械数据属于传感器数据，本质上是时间序列。RNN 模型，包括 LSTM 和 GRU，已经成为处理顺序数据的一种流行架构，因其能够编码时间信息。这些先进的
    RNN 模型被提出以缓解传统 RNN 训练的困难，并且最近应用于机器健康监测。在[[93](#bib.bib93)]中，Yuan 等人研究了包括传统 RNN、LSTM
    和 GRU 模型在内的三种 RNN 模型用于航空发动机的故障诊断和预测。他们发现，先进的 RNN 模型 LSTM 和 GRU 的表现优于传统 RNN。另一个有趣的观察是，以上三种
    RNN 变体的集成模型并未提升 LSTM 的性能。Zhao 等人展示了基于 LSTM 的机器健康监测系统在工具磨损测试中的实证评估[[94](#bib.bib94)]。应用的
    LSTM 模型将原始传感数据编码为嵌入，并预测相应的工具磨损。Zhao 等人进一步设计了一个结合 CNN 和 LSTM 的更复杂的深度学习模型，称为卷积双向长短期记忆网络
    (CBLSTM) [[95](#bib.bib95)]。如图[9](#S3.F9 "图 9 ‣ III-D RNN 机器健康监测 ‣ III 深度学习在机器健康监测中的应用
    ‣ 深度学习及其在机器健康监测中的应用：综述")所示，CNN 用于从顺序输入中提取强健的局部特征，然后采用双向 LSTM 对 CNN 的顺序输出编码时间信息。最后，堆叠的全连接层和线性回归层被添加以预测目标值。在工具磨损测试中，所提模型能够超越包括传统
    LSTM 模型在内的多个最先进的基准方法。在[[96](#bib.bib96)]中，Malhotra 提出了一个非常有趣的 RUL 预测结构。他们设计了一个基于
    LSTM 的编码器-解码器结构，其中 LSTM 基编码器首先将多变量输入序列转换为固定长度的向量，然后 LSTM 解码器使用该向量生成目标序列。在 RUL
    预测中，他们的假设是模型可以首先以无监督的方式在对应正常行为的原始信号上进行训练。然后，重建误差可以用来计算健康指数 (HI)，该指数用于 RUL 估计。直观地说，大的重建误差对应于更不健康的机器状态。
- en: '![Refer to caption](img/36b6abd7c7c7da433594c0b00106d266.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/36b6abd7c7c7da433594c0b00106d266.png)'
- en: 'Figure 9: Illustrations of the proposed Convolutional Bi-directional Long Short-Term
    Memory Networks in [[95](#bib.bib95)].'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：在[[95](#bib.bib95)]中提出的卷积双向长短期记忆网络的示意图。
- en: IV Summary and Future Directions
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 总结与未来方向
- en: 'In this paper, we have provided a systematic overview of the state-of-the-art
    DL-based MHMS. Deep learning, as a sub-field of machine learning, is serving as
    a bridge between big machinery data and data-driven MHMS. Therefore, within the
    past four years, they have been applied in various machine health monitoring tasks.
    These proposed DL-based MHMS are summarized according to four categories of DL
    architecture as: Auto-encoder models, Restricted Boltzmann Machines models, Convolutional
    Neural Networks and Recurrent Neural Networks. Since the momentum of the research
    of DL-based MHMS is growing fast, we hope the messages about the capabilities
    of these DL techniques, especially representation learning for complex machinery
    data and target prediction for various machine health monitoring tasks, can be
    conveyed to readers. Through these previous works, it can be found that DL-based
    MHMS do not require extensive human labor and expert knowledge, i.e., the end-to-end
    structure is able to map raw machinery data to targets. Therefore, the application
    of deep learning models are not restricted to specific kinds of machines, which
    can be a general solution to address the machine health monitoring problems. Besides,
    some research trends and potential future research directions are given as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '*'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open-source Large Dataset: Due to the huge model complexity behind DL methods,
    the performance of DL-based MHMS heavily depends on the scale and quality of datasets.
    On other hand, the depth of DL model is limited by the scale of datasets. As a
    result, the benchmark CNN model for image recognition has 152 layers, which can
    be supported by the large dataset ImageNet containing over ten million annotated
    images [[97](#bib.bib97), [98](#bib.bib98)]. In contrast, the proposed DL models
    for MHMS may stack up to 5 hidden layers. And the model trained in such kind of
    large datasets can be the model initialization for the following specific task/dataset.
    Therefore, it is meaningful to design and publish large-scale machinery datasets.'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Utilization of Domain Knowledge: deep learning is not a skeleton key to all
    machine health monitoring problems. Domain knowledge can contribute to the success
    of applying DL models on machine health monitoring. For example, extracting discriminative
    features can reduce the size of the followed DL models and appropriate task-specific
    regularization term can boost the final performance [[67](#bib.bib67)].'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model and Data Visualization: deep learning techniques, especially deep neural
    networks, have been regarded as black boxes models, i.e., their inner computation
    mechanisms are unexplainable. Visualization of the learned representation and
    the applied model can offer some insights into these DL models, and then these
    insights achieved by this kind of interaction can facilitate the building and
    configuration of DL models for complex machine health monitoring problems. Some
    visualization techniques have been proposed including t-SNE model for high dimensional
    data visualization [[99](#bib.bib99)] and visualization of the activations produced
    by each layer and features at each layer of a DNN via regularized optimization
    [[100](#bib.bib100)].'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型与数据可视化：深度学习技术，特别是深度神经网络，被视为黑盒模型，即其内部计算机制无法解释。学习表示和应用模型的可视化可以为这些深度学习模型提供一些见解，通过这种交互获得的见解可以促进复杂机器健康监测问题的模型构建和配置。已经提出了一些可视化技术，包括用于高维数据可视化的
    t-SNE 模型 [[99](#bib.bib99)]，以及通过正则化优化可视化每个层的激活和每个层特征的深度神经网络 [[100](#bib.bib100)]。
- en: '*'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*'
- en: 'Transferred Deep Learning: Transfer learning tries to apply knowledge learned
    in one domain to a different but related domain [[101](#bib.bib101)]. This research
    direction is meaningful in machine health monitoring, since some machine health
    monitoring problems have sufficient training data while other areas lack training
    data. The machine learning models including DL models trained in one domain can
    be transferred to the other domain. Some previous works focusing on transferred
    feature extraction/dimensionality reduction have been done [[102](#bib.bib102),
    [103](#bib.bib103)]. In [[104](#bib.bib104)], a Maximum Mean Discrepancy (MMD)
    measure evaluating the discrepancy between source and target domains was added
    into the target function of deep neural networks.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迁移学习：迁移学习尝试将在一个领域学到的知识应用到一个不同但相关的领域 [[101](#bib.bib101)]。这个研究方向在机器健康监测中具有意义，因为一些机器健康监测问题有足够的训练数据，而其他领域则缺乏训练数据。在一个领域训练的机器学习模型，包括深度学习模型，可以被迁移到另一个领域。一些先前的工作集中在迁移特征提取/降维上已经完成
    [[102](#bib.bib102), [103](#bib.bib103)]。在 [[104](#bib.bib104)] 中，最大均值差异（MMD）度量被添加到深度神经网络目标函数中，评估源域和目标域之间的差异。
- en: '*'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*'
- en: 'Imbalanced Class: The class distribution of machinery data in real life normally
    follows a highly-skewed one, in which most data samples belong to few categories.
    For example, the number of fault data is much less than the one of health data
    in fault diagnosis. Some enhanced machine learning models including SVM and ELM
    have been proposed to address this imbalanced issue in machine health monitoring
    [[105](#bib.bib105), [106](#bib.bib106)]. Recently, some interesting methods investigating
    the application of deep learning in imbalanced class problems have been developed,
    including CNN models with class resampling or cost-sensitive training [[107](#bib.bib107)]
    and the integration of boot strapping methods and CNN model [[108](#bib.bib108)].'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不平衡类别：实际生活中机械数据的类别分布通常是高度倾斜的，其中大多数数据样本属于少数几个类别。例如，在故障诊断中，故障数据的数量远远少于健康数据的数量。已经提出了一些增强型机器学习模型，包括
    SVM 和 ELM，以解决机器健康监测中的不平衡问题 [[105](#bib.bib105), [106](#bib.bib106)]。最近，一些有趣的方法研究了在不平衡类别问题中应用深度学习的方法，包括具有类别重新采样或成本敏感训练的
    CNN 模型 [[107](#bib.bib107)]，以及集成自举方法和 CNN 模型 [[108](#bib.bib108)] 的整合。
- en: It is believed that deep learning will have a more and more prospective future
    impacting machine health monitoring, especially in the age of big machinery data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习被认为将在未来越来越有前景地影响机器健康监测，特别是在大型机械数据时代。
- en: Acknowledgment
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work has been supported in part by the National Natural Science Foundation
    of China (51575102).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到了中国国家自然科学基金（51575102号）的支持。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Yin, X. Li, H. Gao, and O. Kaynak, “Data-based techniques focused on
    modern industry: An overview,” *IEEE Transactions on Industrial Electronics*,
    vol. 62, no. 1, pp. 657–667, Jan 2015.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Yin, X. Li, H. Gao, and O. Kaynak，“基于数据的现代工业技术：概述，” *IEEE 工业电子学报*，vol. 62,
    no. 1, pp. 657–667, 2015年1月。'
- en: '[2] S. Jeschke, C. Brecher, H. Song, and D. B. Rawat, “Industrial internet
    of things.”'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Jeschke, C. Brecher, H. Song, and D. B. Rawat，“工业物联网。”'
- en: '[3] D. Lund, C. MacGillivray, V. Turner, and M. Morales, “Worldwide and regional
    internet of things (iot) 2014–2020 forecast: A virtuous circle of proven value
    and demand,” *International Data Corporation (IDC), Tech. Rep*, 2014.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D. Lund, C. MacGillivray, V. Turner 和 M. Morales, “全球和区域物联网（IoT）2014–2020年预测：一个被验证的价值和需求的良性循环，”
    *国际数据公司（IDC），技术报告*，2014年。'
- en: '[4] Y. Li, T. Kurfess, and S. Liang, “Stochastic prognostics for rolling element
    bearings,” *Mechanical Systems and Signal Processing*, vol. 14, no. 5, pp. 747–762,
    2000.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. Li, T. Kurfess 和 S. Liang, “滚动元件轴承的随机预测，” *机械系统与信号处理*，第14卷，第5期，第747–762页，2000年。'
- en: '[5] C. H. Oppenheimer and K. A. Loparo, “Physically based diagnosis and prognosis
    of cracked rotor shafts,” in *AeroSense 2002*.   International Society for Optics
    and Photonics, 2002, pp. 122–132.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] C. H. Oppenheimer 和 K. A. Loparo, “基于物理的裂纹转子轴的诊断与预测，” 收录于 *AeroSense 2002*。国际光学与光子学学会，2002年，第122–132页。'
- en: '[6] M. Yu, D. Wang, and M. Luo, “Model-based prognosis for hybrid systems with
    mode-dependent degradation behaviors,” *Industrial Electronics, IEEE Transactions
    on*, vol. 61, no. 1, pp. 546–554, 2014.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Yu, D. Wang 和 M. Luo, “基于模型的混合系统预测，处理模式依赖的退化行为，” *工业电子学，IEEE 交易*，第61卷，第1期，第546–554页，2014年。'
- en: '[7] A. K. Jardine, D. Lin, and D. Banjevic, “A review on machinery diagnostics
    and prognostics implementing condition-based maintenance,” *Mechanical systems
    and signal processing*, vol. 20, no. 7, pp. 1483–1510, 2006.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. K. Jardine, D. Lin 和 D. Banjevic, “关于实施基于条件的维护的机械诊断和预测的综述，” *机械系统与信号处理*，第20卷，第7期，第1483–1510页，2006年。'
- en: '[8] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in *Advances in neural information
    processing systems*, 2015, pp. 91–99.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Ren, K. He, R. Girshick 和 J. Sun, “Faster R-CNN：基于区域提议网络的实时目标检测，” 收录于
    *神经信息处理系统进展*，2015年，第91–99页。'
- en: '[9] R. Collobert and J. Weston, “A unified architecture for natural language
    processing: Deep neural networks with multitask learning,” in *Proceedings of
    the 25th international conference on Machine learning*.   ACM, 2008, pp. 160–167.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] R. Collobert 和 J. Weston, “自然语言处理的统一架构：多任务学习的深度神经网络，” 收录于 *第25届国际机器学习会议论文集*。ACM，2008年，第160–167页。'
- en: '[10] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *IEEE
    Signal Processing Magazine*, vol. 29, no. 6, pp. 82–97, 2012.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *等*，“用于语音识别的声学建模的深度神经网络：四个研究组的共享观点，” *IEEE
    信号处理杂志*，第29卷，第6期，第82–97页，2012年。'
- en: '[11] M. K. Leung, H. Y. Xiong, L. J. Lee, and B. J. Frey, “Deep learning of
    the tissue-regulated splicing code,” *Bioinformatics*, vol. 30, no. 12, pp. i121–i129,
    2014.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. K. Leung, H. Y. Xiong, L. J. Lee 和 B. J. Frey, “组织调控剪接密码的深度学习，” *生物信息学*，第30卷，第12期，第i121–i129页，2014年。'
- en: '[12] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    Networks*, vol. 61, pp. 85–117, 2015, published online 2014; based on TR arXiv:1404.7828
    [cs.NE].'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. Schmidhuber, “神经网络中的深度学习：概述，” *神经网络*，第61卷，第85–117页，2015年，2014年在线发布；基于
    TR arXiv:1404.7828 [cs.NE]。'
- en: '[13] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. LeCun, Y. Bengio 和 G. Hinton, “深度学习，” *自然*，第521卷，第7553期，第436–444页，2015年。'
- en: '[14] R. Raina, A. Madhavan, and A. Y. Ng, “Large-scale deep unsupervised learning
    using graphics processors,” in *Proceedings of the 26th annual international conference
    on machine learning*.   ACM, 2009, pp. 873–880.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] R. Raina, A. Madhavan 和 A. Y. Ng, “使用图形处理器的大规模深度无监督学习，” 收录于 *第26届年度国际机器学习会议论文集*。ACM，2009年，第873–880页。'
- en: '[15] G. E. Hinton, “Learning multiple layers of representation,” *Trends in
    cognitive sciences*, vol. 11, no. 10, pp. 428–434, 2007.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] G. E. Hinton, “学习多层次的表征，” *认知科学趋势*，第11卷，第10期，第428–434页，2007年。'
- en: '[16] A. Widodo and B.-S. Yang, “Support vector machine in machine condition
    monitoring and fault diagnosis,” *Mechanical systems and signal processing*, vol. 21,
    no. 6, pp. 2560–2574, 2007.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] A. Widodo 和 B.-S. Yang, “支持向量机在机器状态监测和故障诊断中的应用，” *机械系统与信号处理*，第21卷，第6期，第2560–2574页，2007年。'
- en: '[17] J. Yan and J. Lee, “Degradation assessment and fault modes classification
    using logistic regression,” *Journal of manufacturing Science and Engineering*,
    vol. 127, no. 4, pp. 912–914, 2005.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Yan 和 J. Lee, “使用逻辑回归进行退化评估和故障模式分类，” *制造科学与工程杂志*，第127卷，第4期，第912–914页，2005年。'
- en: '[18] V. Muralidharan and V. Sugumaran, “A comparative study of naïve bayes
    classifier and bayes net classifier for fault diagnosis of monoblock centrifugal
    pump using wavelet analysis,” *Applied Soft Computing*, vol. 12, no. 8, pp. 2023–2029,
    2012.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] V. Muralidharan 和 V. Sugumaran，“使用小波分析对单体离心泵故障诊断的朴素贝叶斯分类器与贝叶斯网络分类器的比较研究，”
    *应用软计算*，第12卷，第8期，第2023–2029页，2012年。'
- en: '[19] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review
    and new perspectives,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 35, no. 8, pp. 1798–1828, 2013.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Y. Bengio, A. Courville 和 P. Vincent，“表示学习：综述与新视角，” *IEEE模式分析与机器智能学报*，第35卷，第8期，第1798–1828页，2013年。'
- en: '[20] A. Malhi and R. X. Gao, “Pca-based feature selection scheme for machine
    defect classification,” *IEEE Transactions on Instrumentation and Measurement*,
    vol. 53, no. 6, pp. 1517–1525, 2004.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Malhi 和 R. X. Gao，“基于PCA的特征选择方案用于机器缺陷分类，” *IEEE仪器与测量学报*，第53卷，第6期，第1517–1525页，2004年。'
- en: '[21] J. Wang, J. Xie, R. Zhao, L. Zhang, and L. Duan, “Multisensory fusion
    based virtual tool wear sensing for ubiquitous manufacturing,” *Robotics and Computer-Integrated
    Manufacturing*, 2016.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Wang, J. Xie, R. Zhao, L. Zhang 和 L. Duan，“基于多感官融合的虚拟工具磨损检测用于普适制造，”
    *机器人与计算机集成制造*，2016年。'
- en: '[22] J. Wang, J. Xie, R. Zhao, K. Mao, and L. Zhang, “A new probabilistic kernel
    factor analysis for multisensory data fusion: Application to tool condition monitoring,”
    *IEEE Transactions on Instrumentation and Measurement*, vol. 65, no. 11, pp. 2527–2537,
    Nov 2016.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Wang, J. Xie, R. Zhao, K. Mao 和 L. Zhang，“一种用于多感官数据融合的新型概率核因子分析：应用于工具状态监测，”
    *IEEE仪器与测量学报*，第65卷，第11期，第2527–2537页，2016年11月。'
- en: '[23] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extracting
    and composing robust features with denoising autoencoders,” in *Proceedings of
    the 25th international conference on Machine learning*.   ACM, 2008, pp. 1096–1103.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] P. Vincent, H. Larochelle, Y. Bengio 和 P.-A. Manzagol，“利用去噪自编码器提取和组合鲁棒特征，”
    在 *第25届国际机器学习会议论文集*。 ACM，2008年，第1096–1103页。'
- en: '[24] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for
    deep belief nets,” *Neural computation*, vol. 18, no. 7, pp. 1527–1554, 2006.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] G. E. Hinton, S. Osindero 和 Y.-W. Teh，“深度信念网络的快速学习算法，” *神经计算*，第18卷，第7期，第1527–1554页，2006年。'
- en: '[25] R. Salakhutdinov and G. E. Hinton, “Deep boltzmann machines.” in *AISTATS*,
    vol. 1, 2009, p. 3.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] R. Salakhutdinov 和 G. E. Hinton，“深度玻尔兹曼机。” 在 *AISTATS*，第1卷，2009年，第3页。'
- en: '[26] P. Sermanet, S. Chintala, and Y. LeCun, “Convolutional neural networks
    applied to house numbers digit classification,” in *Pattern Recognition (ICPR),
    2012 21st International Conference on*.   IEEE, 2012, pp. 3288–3291.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] P. Sermanet, S. Chintala 和 Y. LeCun，“卷积神经网络在房屋号码数字分类中的应用，” 在 *模式识别（ICPR），2012年第21届国际会议*。
    IEEE，2012年，第3288–3291页。'
- en: '[27] K.-i. Funahashi and Y. Nakamura, “Approximation of dynamical systems by
    continuous time recurrent neural networks,” *Neural networks*, vol. 6, no. 6,
    pp. 801–806, 1993.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] K.-i. Funahashi 和 Y. Nakamura，“通过连续时间递归神经网络对动态系统的逼近，” *神经网络*，第6卷，第6期，第801–806页，1993年。'
- en: '[28] L. Deng and D. Yu, “Deep learning: Methods and applications,” *Found.
    Trends Signal Process.*, vol. 7, no. 3&#8211;4, pp. 197–387, Jun. 2014. [Online].
    Available: http://dx.doi.org/10.1561/2000000039'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] L. Deng 和 D. Yu，“深度学习：方法与应用，” *信号处理基础与趋势*，第7卷，第3–4期，第197–387页，2014年6月。[在线].
    可用链接: http://dx.doi.org/10.1561/2000000039'
- en: '[29] I. Goodfellow, Y. Bengio, and A. Courville, “Deep learning,” *2015*, 2016.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] I. Goodfellow, Y. Bengio 和 A. Courville，“深度学习，” *2015*，2016年。'
- en: '[30] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle *et al.*, “Greedy layer-wise
    training of deep networks,” *Advances in neural information processing systems*,
    vol. 19, p. 153, 2007.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle *等*，“深度网络的贪婪逐层训练，” *神经信息处理系统进展*，第19卷，第153页，2007年。'
- en: '[31] A. Ng, “Sparse autoencoder,” *CS294A Lecture notes*, vol. 72, pp. 1–19,
    2011.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] A. Ng，“稀疏自编码器，” *CS294A 讲义*，第72卷，第1–19页，2011年。'
- en: '[32] B. B. Le Cun, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and
    L. D. Jackel, “Handwritten digit recognition with a back-propagation network,”
    in *Advances in neural information processing systems*.   Citeseer, 1990.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] B. B. Le Cun, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard 和 L.
    D. Jackel，“使用反向传播网络进行手写数字识别，” 在 *神经信息处理系统进展*。 Citeseer，1990年。'
- en: '[33] K. Jarrett, K. Kavukcuoglu, Y. Lecun *et al.*, “What is the best multi-stage
    architecture for object recognition?” in *2009 IEEE 12th International Conference
    on Computer Vision*.   IEEE, 2009, pp. 2146–2153.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in neural information processing
    systems*, 2012, pp. 1097–1105.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, and G. Penn, “Applying convolutional
    neural networks concepts to hybrid nn-hmm model for speech recognition,” in *2012
    IEEE international conference on Acoustics, speech and signal processing (ICASSP)*.   IEEE,
    2012, pp. 4277–4280.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Y. Kim, “Convolutional neural networks for sentence classification,” *arXiv
    preprint arXiv:1408.5882*, 2014.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] H. Jaeger, *Tutorial on training recurrent neural networks, covering BPPT,
    RTRL, EKF and the” echo state network” approach*.   GMD-Forschungszentrum Informationstechnik,
    2002.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] C. L. Giles, C. B. Miller, D. Chen, H.-H. Chen, G.-Z. Sun, and Y.-C. Lee,
    “Learning and extracting finite state automata with second-order recurrent neural
    networks,” *Neural Computation*, vol. 4, no. 3, pp. 393–405, 1992.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget: Continual
    prediction with lstm,” *Neural computation*, vol. 12, no. 10, pp. 2451–2471, 2000.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] F. A. Gers, N. N. Schraudolph, and J. Schmidhuber, “Learning precise timing
    with lstm recurrent networks,” *Journal of machine learning research*, vol. 3,
    no. Aug, pp. 115–143, 2002.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” *arXiv preprint arXiv:1406.1078*, 2014.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint arXiv:1412.3555*,
    2014.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] H. Su and K. T. Chong, “Induction machine condition monitoring using neural
    network modeling,” *IEEE Transactions on Industrial Electronics*, vol. 54, no. 1,
    pp. 241–249, Feb 2007.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] B. Li, M.-Y. Chow, Y. Tipsuwan, and J. C. Hung, “Neural-network-based
    motor rolling bearing fault diagnosis,” *IEEE transactions on industrial electronics*,
    vol. 47, no. 5, pp. 1060–1069, 2000.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] B. Samanta and K. Al-Balushi, “Artificial neural network based fault diagnostics
    of rolling element bearings using time-domain features,” *Mechanical systems and
    signal processing*, vol. 17, no. 2, pp. 317–328, 2003.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Aminian and F. Aminian, “Neural-network based analog-circuit fault
    diagnosis using wavelet transform as preprocessor,” *IEEE Transactions on Circuits
    and Systems II: Analog and Digital Signal Processing*, vol. 47, no. 2, pp. 151–156,
    2000.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] W. Sun, S. Shao, R. Zhao, R. Yan, X. Zhang, and X. Chen, “A sparse auto-encoder-based
    deep neural network approach for induction motor faults classification,” *Measurement*,
    vol. 89, pp. 171–178, 2016.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] W. Sun, S. Shao, R. Zhao, R. Yan, X. Zhang, 和 X. Chen，“基于稀疏自编码器的深度神经网络方法用于感应电机故障分类，”
    *测量*，第 89 卷，第 171–178 页，2016 年。'
- en: '[49] C. Lu, Z.-Y. Wang, W.-L. Qin, and J. Ma, “Fault diagnosis of rotary machinery
    components using a stacked denoising autoencoder-based health state identification,”
    *Signal Processing*, vol. 130, pp. 377–388, 2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] C. Lu, Z.-Y. Wang, W.-L. Qin, 和 J. Ma，“使用堆叠去噪自编码器的旋转机械部件故障诊断，” *信号处理*，第
    130 卷，第 377–388 页，2017 年。'
- en: '[50] S. Tao, T. Zhang, J. Yang, X. Wang, and W. Lu, “Bearing fault diagnosis
    method based on stacked autoencoder and softmax regression,” in *Control Conference
    (CCC), 2015 34th Chinese*.   IEEE, 2015, pp. 6331–6335.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Tao, T. Zhang, J. Yang, X. Wang, 和 W. Lu，“基于堆叠自编码器和 Softmax 回归的轴承故障诊断方法，”
    在 *控制会议（CCC），2015 第 34 届中国*。IEEE，2015 年，第 6331–6335 页。'
- en: '[51] F. Jia, Y. Lei, J. Lin, X. Zhou, and N. Lu, “Deep neural networks: A promising
    tool for fault characteristic mining and intelligent diagnosis of rotating machinery
    with massive data,” *Mechanical Systems and Signal Processing*, vol. 72, pp. 303–315,
    2016.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] F. Jia, Y. Lei, J. Lin, X. Zhou, 和 N. Lu，“深度神经网络：用于大数据旋转机械故障特征挖掘与智能诊断的有前景工具，”
    *机械系统与信号处理*，第 72 卷，第 303–315 页，2016 年。'
- en: '[52] T. Junbo, L. Weining, A. Juneng, and W. Xueqian, “Fault diagnosis method
    study in roller bearing based on wavelet transform and stacked auto-encoder,”
    in *The 27th Chinese Control and Decision Conference (2015 CCDC)*.   IEEE, 2015,
    pp. 4608–4613.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] T. Junbo, L. Weining, A. Juneng, 和 W. Xueqian，“基于小波变换和堆叠自编码器的滚动轴承故障诊断方法研究，”
    在 *第 27 届中国控制与决策会议（2015 CCDC）*。IEEE，2015 年，第 4608–4613 页。'
- en: '[53] Z. Huijie, R. Ting, W. Xinqing, Z. You, and F. Husheng, “Fault diagnosis
    of hydraulic pump based on stacked autoencoders,” in *2015 12th IEEE International
    Conference on Electronic Measurement Instruments (ICEMI)*, vol. 01, July 2015,
    pp. 58–62.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Z. Huijie, R. Ting, W. Xinqing, Z. You, 和 F. Husheng，“基于堆叠自编码器的液压泵故障诊断，”
    在 *2015 第 12 届 IEEE 国际电子测量仪器会议（ICEMI）*，第 01 卷，2015 年 7 月，第 58–62 页。'
- en: '[54] H. Liu, L. Li, and J. Ma, “Rolling bearing fault diagnosis based on stft-deep
    learning and sound signals,” *Shock and Vibration*, vol. 2016, 2016.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Liu, L. Li, 和 J. Ma，“基于 STFT 深度学习和声音信号的滚动轴承故障诊断，” *冲击与振动*，第 2016 卷，2016
    年。'
- en: '[55] G. S. Galloway, V. M. Catterson, T. Fay, A. Robb, and C. Love, “Diagnosis
    of tidal turbine vibration data through deep neural networks,” 2016.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] G. S. Galloway, V. M. Catterson, T. Fay, A. Robb, 和 C. Love，“通过深度神经网络诊断潮汐涡轮振动数据，”
    2016 年。'
- en: '[56] K. Li and Q. Wang, “Study on signal recognition and diagnosis for spacecraft
    based on deep learning method,” in *Prognostics and System Health Management Conference
    (PHM), 2015*.   IEEE, 2015, pp. 1–5.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] K. Li 和 Q. Wang，“基于深度学习方法的航天器信号识别与诊断研究，” 在 *预知与系统健康管理会议（PHM），2015*。IEEE，2015
    年，第 1–5 页。'
- en: '[57] L. Guo, H. Gao, H. Huang, X. He, and S. Li, “Multifeatures fusion and
    nonlinear dimension reduction for intelligent bearing condition monitoring,” *Shock
    and Vibration*, vol. 2016, 2016.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] L. Guo, H. Gao, H. Huang, X. He, 和 S. Li，“多特征融合与非线性降维用于智能轴承状态监测，” *冲击与振动*，第
    2016 卷，2016 年。'
- en: '[58] N. K. Verma, V. K. Gupta, M. Sharma, and R. K. Sevakula, “Intelligent
    condition based monitoring of rotating machines using sparse auto-encoders,” in
    *Prognostics and Health Management (PHM), 2013 IEEE Conference on*.   IEEE, 2013,
    pp. 1–7.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] N. K. Verma, V. K. Gupta, M. Sharma, 和 R. K. Sevakula，“基于稀疏自编码器的旋转机器智能状态监测，”
    在 *预知与健康管理（PHM），2013 IEEE 会议上*。IEEE，2013 年，第 1–7 页。'
- en: '[59] v. v. kishore k. reddy, soumalya sarkar and michael giering, “Anomaly
    detection and fault disambiguation in large flight data: A multi-modal deep auto-encoder
    approach,” in *Annual conference of the prognostics and health management society,
    Denver, Colorado*, 2016.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] v. v. kishore k. reddy, soumalya sarkar 和 michael giering，“大规模飞行数据中的异常检测与故障消歧：一种多模态深度自编码器方法，”
    在 *预知与健康管理协会年会，科罗拉多州丹佛*，2016 年。'
- en: '[60] Z. Chen and W. Li, “Multi-sensor feature fusion for bearing fault diagnosis
    using sparse auto encoder and deep belief network,” *IEEE Transactions on IM*,
    2017.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Z. Chen 和 W. Li，“基于稀疏自编码器和深度信念网络的轴承故障诊断的多传感器特征融合，” *IEEE IM 交易*，2017 年。'
- en: '[61] R. Thirukovalluru, S. Dixit, R. K. Sevakula, N. K. Verma, and A. Salour,
    “Generating feature sets for fault diagnosis using denoising stacked auto-encoder,”
    in *Prognostics and Health Management (ICPHM), 2016 IEEE International Conference
    on*.   IEEE, 2016, pp. 1–7.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] L. Wang, X. Zhao, J. Pei, and G. Tang, “Transformer fault diagnosis using
    continuous sparse autoencoder,” *SpringerPlus*, vol. 5, no. 1, p. 1, 2016.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] W. Mao, J. He, Y. Li, and Y. Yan, “Bearing fault diagnosis with auto-encoder
    extreme learning machine: A comparative study,” *Proceedings of the Institution
    of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science*, pp.
    1–19, 2016.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] E. Cambria, G. B. Huang, L. L. C. Kasun, H. Zhou, C. M. Vong, J. Lin,
    J. Yin, Z. Cai, Q. Liu, K. Li, V. C. M. Leung, L. Feng, Y. S. Ong, M. H. Lim,
    A. Akusok, A. Lendasse, F. Corona, R. Nian, Y. Miche, P. Gastaldo, R. Zunino,
    S. Decherchi, X. Yang, K. Mao, B. S. Oh, J. Jeon, K. A. Toh, A. B. J. Teoh, J. Kim,
    H. Yu, Y. Chen, and J. Liu, “Extreme learning machines [trends controversies],”
    *IEEE Intelligent Systems*, vol. 28, no. 6, pp. 30–59, Nov 2013.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] W. Lu, X. Wang, C. Yang, and T. Zhang, “A novel feature extraction method
    using deep neural network for rolling bearing fault diagnosis,” in *The 27th Chinese
    Control and Decision Conference (2015 CCDC)*.   IEEE, 2015, pp. 2427–2431.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Deutsch and D. He, “Using deep learning based approaches for bearing
    remaining useful life prediction,” 2016.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] L. Liao, W. Jin, and R. Pavel, “Enhanced restricted boltzmann machine
    with prognosability regularization for prognostics and health assessment,” *IEEE
    Transactions on Industrial Electronics*, vol. 63, no. 11, 2016.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] C. Li, R.-V. Sanchez, G. Zurita, M. Cerrada, D. Cabrera, and R. E. Vásquez,
    “Multimodal deep support vector classification with homologous features and its
    application to gearbox fault diagnosis,” *Neurocomputing*, vol. 168, pp. 119–127,
    2015.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] C. Li, R.-V. Sánchez, G. Zurita, M. Cerrada, and D. Cabrera, “Fault diagnosis
    for rotating machinery using vibration measurement deep statistical feature learning,”
    *Sensors*, vol. 16, no. 6, p. 895, 2016.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Li, R.-V. Sanchez, G. Zurita, M. Cerrada, D. Cabrera, and R. E. Vásquez,
    “Gearbox fault diagnosis based on deep random forest fusion of acoustic and vibratory
    signals,” *Mechanical Systems and Signal Processing*, vol. 76, pp. 283–293, 2016.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Ma, X. Chen, S. Wang, Y. Liu, and W. Li, “Bearing degradation assessment
    based on weibull distribution and deep belief network,” in *Proceedings of 2016
    International Symposium of Flexible Automation (ISFA)*, 2016, pp. 1–4.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. Shao, W. Sun, P. Wang, R. X. Gao, and R. Yan, “Learning features from
    vibration signals for induction motor fault diagnosis,” in *Proceedings of 2016
    International Symposium of Flexible Automation (ISFA)*, 2016, pp. 1–6.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Fu, Y. Zhang, H. Qiao, D. Li, H. Zhou, and J. Leopold, “Analysis of
    feature extracting ability for cutting state monitoring using deep belief networks,”
    *Procedia CIRP*, vol. 31, pp. 29–34, 2015.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Fu, Y. Zhang, H. Qiao, D. Li, H. Zhou, 和 J. Leopold, “基于深度信念网络的切削状态监测特征提取能力分析，”
    *Procedia CIRP*，第31卷，第29–34页，2015年。'
- en: '[74] P. Tamilselvan and P. Wang, “Failure diagnosis using deep belief learning
    based health state classification,” *Reliability Engineering & System Safety*,
    vol. 115, pp. 124–135, 2013.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] P. Tamilselvan 和 P. Wang, “基于深度信念学习的健康状态分类用于故障诊断，” *可靠性工程与系统安全*，第115卷，第124–135页，2013年。'
- en: '[75] P. Tamilselvan, Y. Wang, and P. Wang, “Deep belief network based state
    classification for structural health diagnosis,” in *Aerospace Conference, 2012
    IEEE*.   IEEE, 2012, pp. 1–11.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] P. Tamilselvan, Y. Wang, 和 P. Wang, “基于深度信念网络的状态分类用于结构健康诊断，” 见于 *2012年IEEE航空航天会议*。IEEE，2012年，第1–11页。'
- en: '[76] J. Tao, Y. Liu, and D. Yang, “Bearing fault diagnosis based on deep belief
    network and multisensor information fusion,” *Shock and Vibration*, vol. 2016,
    2016.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. Tao, Y. Liu, 和 D. Yang, “基于深度信念网络和多传感器信息融合的轴承故障诊断，” *冲击与振动*，第2016卷，2016年。'
- en: '[77] Z. Chen, C. Li, and R.-V. Sánchez, “Multi-layer neural network with deep
    belief network for gearbox fault diagnosis.” *Journal of Vibroengineering*, vol. 17,
    no. 5, 2015.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Z. Chen, C. Li, 和 R.-V. Sánchez, “基于深度信念网络的多层神经网络用于齿轮箱故障诊断。” *振动工程学报*，第17卷，第5期，2015年。'
- en: '[78] M. Gan, C. Wang *et al.*, “Construction of hierarchical diagnosis network
    based on deep learning and its application in the fault pattern recognition of
    rolling element bearings,” *Mechanical Systems and Signal Processing*, vol. 72,
    pp. 92–104, 2016.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Gan, C. Wang *等*，“基于深度学习的分层诊断网络的构建及其在滚动元件轴承故障模式识别中的应用，” *机械系统与信号处理*，第72卷，第92–104页，2016年。'
- en: '[79] H. Oh, B. C. Jeon, J. H. Jung, and B. D. Youn, “Smart diagnosis of journal
    bearing rotor systems: Unsupervised feature extraction scheme by deep learning,”
    2016.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] H. Oh, B. C. Jeon, J. H. Jung, 和 B. D. Youn, “智能化的轴承转子系统诊断：深度学习的无监督特征提取方案，”
    2016年。'
- en: '[80] C. Zhang, J. H. Sun, and K. C. Tan, “Deep belief networks ensemble with
    multi-objective optimization for failure diagnosis,” in *Systems, Man, and Cybernetics
    (SMC), 2015 IEEE International Conference on*.   IEEE, 2015, pp. 32–37.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] C. Zhang, J. H. Sun, 和 K. C. Tan, “基于多目标优化的深度信念网络集成用于故障诊断，” 见于 *系统，人与控制论（SMC），2015年IEEE国际会议*。IEEE，2015年，第32–37页。'
- en: '[81] C. Zhang, P. Lim, A. Qin, and K. C. Tan, “Multiobjective deep belief networks
    ensemble for remaining useful life estimation in prognostics,” *IEEE Transactions
    on Neural Networks and Learning Systems*, 2016.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] C. Zhang, P. Lim, A. Qin, 和 K. C. Tan, “用于预后中剩余使用寿命估计的多目标深度信念网络集成，” *IEEE神经网络与学习系统汇刊*，2016年。'
- en: '[82] O. Janssens, V. Slavkovikj, B. Vervisch, K. Stockman, M. Loccufier, S. Verstockt,
    R. Van de Walle, and S. Van Hoecke, “Convolutional neural network based fault
    detection for rotating machinery,” *Journal of Sound and Vibration*, 2016.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] O. Janssens, V. Slavkovikj, B. Vervisch, K. Stockman, M. Loccufier, S.
    Verstockt, R. Van de Walle, 和 S. Van Hoecke, “基于卷积神经网络的旋转机械故障检测，” *声学与振动学报*，2016年。'
- en: '[83] G. S. Babu, P. Zhao, and X.-L. Li, “Deep convolutional neural network
    based regression approach for estimation of remaining useful life,” in *International
    Conference on Database Systems for Advanced Applications*.   Springer, 2016, pp.
    214–228.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] G. S. Babu, P. Zhao, 和 X.-L. Li, “基于深度卷积神经网络的回归方法用于剩余使用寿命的估计，” 见于 *国际先进应用数据库系统会议*。Springer，2016年，第214–228页。'
- en: '[84] X. Ding and Q. He, “Energy-fluctuated multiscale feature learning with
    deep convnet for intelligent spindle bearing fault diagnosis,” *IEEE Transactions
    on IM*, 2017.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Ding 和 Q. He, “基于深度卷积网络的能量波动多尺度特征学习用于智能主轴轴承故障诊断，” *IEEE工业测量汇刊*，2017年。'
- en: '[85] X. Guo, L. Chen, and C. Shen, “Hierarchical adaptive deep convolution
    neural network and its application to bearing fault diagnosis,” *Measurement*,
    vol. 93, pp. 490–502, 2016.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] X. Guo, L. Chen, 和 C. Shen, “层次自适应深度卷积神经网络及其在轴承故障诊断中的应用，” *测量*，第93卷，第490–502页，2016年。'
- en: '[86] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, 1998.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Y. LeCun, L. Bottou, Y. Bengio, 和 P. Haffner, “应用于文档识别的基于梯度的学习，” *IEEE汇刊*，第86卷，第11期，第2278–2324页，1998年。'
- en: '[87] J. Wang, j. Zhuang, L. Duan, and W. Cheng, “A multi-scale convolution
    neural network for featureless fault diagnosis,” in *Proceedings of 2016 International
    Symposium of Flexible Automation (ISFA)*, 2016, pp. 1–6.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Z. Chen, C. Li, and R.-V. Sanchez, “Gearbox fault identification and classification
    with convolutional neural networks,” *Shock and Vibration*, vol. 2015, 2015.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] D. Weimer, B. Scholz-Reiter, and M. Shpitalni, “Design of deep convolutional
    neural network architectures for automated feature extraction in industrial inspection,”
    *CIRP Annals-Manufacturing Technology*, 2016.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] H.-Y. DONG, L.-X. YANG, and H.-W. LI, “Small fault diagnosis of front-end
    speed controlled wind generator based on deep learning.”'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] T. Ince, S. Kiranyaz, L. Eren, M. Askar, and M. Gabbouj, “Real-time motor
    fault detection by 1-d convolutional neural networks,” *IEEE Transactions on Industrial
    Electronics*, vol. 63, no. 11, pp. 7067–7075, 2016.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] O. Abdeljaber, O. Avci, S. Kiranyaz, M. Gabbouj, and D. J. Inman, “Real-time
    vibration-based structural damage detection using one-dimensional convolutional
    neural networks,” *Journal of Sound and Vibration*, vol. 388, pp. 154–170, 2017.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] M. Yuan, Y. Wu, and L. Lin, “Fault diagnosis and remaining useful life
    estimation of aero engine using lstm neural network,” in *2016 IEEE International
    Conference on Aircraft Utility Systems (AUS)*, Oct 2016, pp. 135–140.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] R. Zhao, J. Wang, R. Yan, and K. Mao, “Machine helath monitoring with
    lstm networks,” in *2016 10th International Conference on Sensing Technology (ICST)*.   IEEE,
    2016, pp. 1–6.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] R. Zhao, R. Yan, J. Wang, and K. Mao, “Learning to monitor machine health
    with convolutional bi-directional lstm networks,” *Sensors*, 2016.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, and G. Shroff,
    “Multi-sensor prognostics using an unsupervised health index based on lstm encoder-decoder,”
    *arXiv preprint arXiv:1608.06154*, 2016.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” *arXiv preprint arXiv:1512.03385*, 2015.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
    A Large-Scale Hierarchical Image Database,” in *CVPR09*, 2009.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne,” *Journal
    of Machine Learning Research*, vol. 9, no. Nov, pp. 2579–2605, 2008.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, “Understanding
    neural networks through deep visualization,” *arXiv preprint arXiv:1506.06579*,
    2015.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Transactions
    on knowledge and data engineering*, vol. 22, no. 10, pp. 1345–1359, 2010.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] F. Shen, C. Chen, R. Yan, and R. X. Gao, “Bearing fault diagnosis based
    on svd feature extraction and transfer learning classification,” in *Prognostics
    and System Health Management Conference (PHM), 2015*.   IEEE, 2015, pp. 1–6.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. Xie, L. Zhang, L. Duan, and J. Wang, “On cross-domain feature fusion
    in gearbox fault diagnosis under various operating conditions based on transfer
    component analysis,” in *2016 IEEE International Conference on Prognostics and
    Health Management (ICPHM)*, June 2016, pp. 1–6.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] J. Xie, L. Zhang, L. Duan, and J. Wang，“基于传输成分分析的多域特征融合在变速箱故障诊断下各种工况”，在*2016年IEEE预测健康管理国际会议（ICPHM）*，2016年6月，第1-6页。'
- en: '[104] W. Lu, B. Liang, Y. Cheng, D. Meng, J. Yang, and T. Zhang, “Deep model
    based domain adaptation for fault diagnosis,” *IEEE Transactions on Industrial
    Electronics*, 2016.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] W. Lu, B. Liang, Y. Cheng, D. Meng, J. Yang, and T. Zhang，“深度模型基于领域自适应的故障诊断”，*IEEE工业电子学报*，2016。'
- en: '[105] W. Mao, L. He, Y. Yan, and J. Wang, “Online sequential prediction of
    bearings imbalanced fault diagnosis by extreme learning machine,” *Mechanical
    Systems and Signal Processing*, vol. 83, pp. 450–473, 2017.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] W. Mao, L. He, Y. Yan, and J. Wang，“极限学习机的轴承不平衡故障诊断的在线顺序预测”，*机械系统与信号处理*，第83卷，第450-473页，2017年。'
- en: '[106] L. Duan, M. Xie, T. Bai, and J. Wang, “A new support vector data description
    method for machinery fault diagnosis with unbalanced datasets,” *Expert Systems
    with Applications*, vol. 64, pp. 239–246, 2016.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] L. Duan, M. Xie, T. Bai, and J. Wang，“一种新的支持向量数据描述方法用于不平衡数据集的机械故障诊断”，*专家系统与应用*，第64卷，第239-246页，2016年。'
- en: '[107] C. Huang, Y. Li, C. Change Loy, and X. Tang, “Learning deep representation
    for imbalanced classification,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2016, pp. 5375–5384.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] C. Huang, Y. Li, C. Change Loy, and X. Tang，“学习不平衡分类的深度表示”，在*IEEE计算机视觉与模式识别会议论文集*，2016年，第5375-5384页。'
- en: '[108] Y. Yan, M. Chen, M.-L. Shyu, and S.-C. Chen, “Deep learning for imbalanced
    multimedia data classification,” in *2015 IEEE International Symposium on Multimedia
    (ISM)*.   IEEE, 2015, pp. 483–488.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Y. Yan, M. Chen, M.-L. Shyu, and S.-C. Chen，“不平衡多媒体数据分类的深度学习”，在*2015年IEEE多媒体国际研讨会（ISM）*。IEEE，2015年，第483-488页。'
