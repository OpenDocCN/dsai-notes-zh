- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:39:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2305.19812] A Survey of Label-Efficient Deep Learning for 3D Point Clouds'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.19812](https://ar5iv.labs.arxiv.org/html/2305.19812)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Label-Efficient Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: for 3D Point Clouds
  prefs: []
  type: TYPE_NORMAL
- en: 'Aoran Xiao, Xiaoqin Zhang, Ling Shao , and Shijian Lu Aoran Xiao and Shijian
    Lu are with School of Computer Science and Engineering, Nanyang Technological
    University, Singapore. Xiaoqin Zhang is with Key Laboratory of Intelligent Informatics
    for Safety & Emergency of Zhejiang Province, Wenzhou University, China. Ling Shao
    is with the UCAS-Terminus AI Lab, University of Chinese Academy of Sciences, Beijing,
    China. Corresponding author: Shijian Lu (shijian.lu@ntu.edu.sg)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the past decade, deep neural networks have achieved significant progress
    in point cloud learning. However, collecting large-scale precisely-annotated training
    data is extremely laborious and expensive, which hinders the scalability of existing
    point cloud datasets and poses a bottleneck for efficient exploration of point
    cloud data in various tasks and applications. Label-efficient learning offers
    a promising solution by enabling effective deep network training with much-reduced
    annotation efforts. This paper presents the first comprehensive survey of label-efficient
    learning of point clouds. We address three critical questions in this emerging
    research field: i) the importance and urgency of label-efficient learning in point
    cloud processing, ii) the subfields it encompasses, and iii) the progress achieved
    in this area. To achieve this, we propose a taxonomy that organizes label-efficient
    learning methods based on the data prerequisites provided by different types of
    labels. We categorize four typical label-efficient learning approaches that significantly
    reduce point cloud annotation efforts: data augmentation, domain transfer learning,
    weakly-supervised learning, and pretrained foundation models. For each approach,
    we outline the problem setup and provide an extensive literature review that showcases
    relevant progress and challenges. Finally, we share insights into current research
    challenges and potential future directions. A project associated with this survey
    has been built at [https://github.com/xiaoaoran/3D_label_efficient_learning](https://github.com/xiaoaoran/3D_label_efficient_learning).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Point cloud, label-efficient learning, data augmentation, semi-supervised learning,
    weakly-supervised learning, few-shot learning, domain adaptation, domain generalization,
    self-supervised learning, foundation model.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The acquisition of 3D point clouds has recently become more feasible and cost-effective
    with the wide adoption of various 3D devices, such as RGB-D cameras and LiDAR
    sensors. Meanwhile, remarkable advancements in deep learning have led to significant
    progress in point cloud understanding. The concurrence of the two has witnessed
    increasing demands in utilizing point clouds to capture 3D shape representations
    of objects and scenes, ranging from autonomous navigation and robotics to remote
    sensing applications and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the great advancements in deep learning in point cloud understanding,
    most existing work relies heavily on large-scale well-annotated 3D data in network
    training. However, collecting such annotated training data is notoriously laborious
    and time-consuming due to the high complexity of the data, large variation in
    point sparsity, rich noises, and frequent 3D view changes in annotation process.
    Hence, how to learn effective point cloud models from training data of limited
    size and variation has become a grand challenge in point cloud understanding.
  prefs: []
  type: TYPE_NORMAL
- en: To address the heavy burden in point cloud annotation, a promising solution
    is label-efficient learning, a machine learning paradigm that prioritizes model
    training with minimal annotation while still achieving desired accuracy. Due to
    its importance and high practical values, label-efficient point cloud learning
    has recently emerged as a thriving research field with numerous studies for learning
    effective models from limited point annotations. Various approaches have been
    explored with different data requirements and application scenarios. To this end,
    a systematic survey is urgently needed to provide a comprehensive overview of
    this field, covering multiple learning approaches and setups over various tasks
    in an organized manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'We thus present a comprehensive literature review of recent advancements in
    label-efficient learning of point clouds. Specifically, we review existing studies
    based on task and data prerequisites and categorize them into four distinct approaches:
    1) Data Augmentation, which augments limited labelled training data distribution
    via data augmentation; 2) Domain Transfer, which utilizes labelled data from source
    domain(s) to train robust models for unlabelled target domain(s); 3) Weakly-Supervised
    Learning, which trains robust models with weakly labelled point clouds; and 4)
    Pretrained Foundation Models, which leverages unsupervised or multi-modal pretraining
    to facilitate 3D modelling with less annotations. For each label-efficient learning
    approach, we introduce the problem setup and provide an exhaustive literature
    review, showcasing the progress made in this field and the challenges that remain.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/efc8bbe53b5d9c2815a81284407af132.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Taxonomy of label-efficient learning of point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: To the best of our knowledge, this is the first systematic and comprehensive
    survey that focuses on label-efficient learning of point clouds, providing a detailed
    overview of the progress and challenges in this field. Several relevant surveys
    have been conducted. For example, Guo et al. [[1](#bib.bib1)] reviewed supervised
    deep learning of point clouds, and Xiao et al. [[2](#bib.bib2)] presented a systematic
    review on unsupervised representation learning of point clouds. In addition, several
    studies [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]
    survey label-efficient learning of other data modalities (e.g., 2D images, texts,
    and graphs) such as self-supervised learning [[4](#bib.bib4)], small sample learning [[5](#bib.bib5)],
    and generalizing across domains [[6](#bib.bib6), [7](#bib.bib7)].
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this survey is organized as follows. Section [2](#S2 "2 Background
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") introduces background
    knowledge including key concepts and a brief description of the efforts and difficulty
    of annotating 3D point-cloud data. Sections [3](#S3 "3 Data Augmentation ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds"),[4](#S4 "4 Domain Transfer
    Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"),[5](#S5
    "5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds"),[6](#S6 "6 Pretrained Foundation Models ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") then provide systematic and extensive literature
    reviews of four representative data-efficient learning approaches, namely, point
    cloud data augmentation, knowledge transfer across domains, weakly supervised
    learning of point clouds, and pretrained foundation models for point cloud learning.
    Finally, we highlight several promising research directions for future label-efficient
    point cloud learning in Section [7](#S7 "7 Future direction ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"). Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") shows a taxonomy
    of existing label-efficient learning methods for 3D point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Key concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Point cloud. A point cloud is a collection of 3D points, represented by their
    spatial coordinates in x, y, and z. Depending on the type of point clouds, additional
    attributes may also be included, e.g., normal values for object-level point clouds [[8](#bib.bib8)],
    color information for indoor dense point clouds [[9](#bib.bib9)], or intensity
    value for LiDAR point clouds [[10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning optimizes machine learning models under the full supervision
    of labels where models learn to map input data to output label space. The training
    data consists of pairs of input point clouds and corresponding labels, where the
    labels annotated by humans are exactly the ground truth of the models’ output.
  prefs: []
  type: TYPE_NORMAL
- en: Label-efficient learning focuses on developing methods that can learn from a
    limited amount of labeled data. The goal is to reduce the amount of labeled data
    in deep network training, as labelling data can be time-consuming and expensive.
  prefs: []
  type: TYPE_NORMAL
- en: '3D shape classification aims to identify the category of an object point cloud,
    such as chairs, tables, cars, and buildings. Categorical labels are needed as
    ground truth for training 3D classification models. Accuracy, defined as the ratio
    of correctly classified objects to the total number of objects in the dataset,
    is widely adopted for evaluations. Two types of accuracy are commonly used: overall
    accuracy (OA), which measures the overall performance of the algorithm, and mean
    accuracy (mAcc), which provides a class-specific measure of accuracy. OA is calculated
    as the ratio of the total number of correctly classified objects to the total
    number of objects in the dataset regardless of the class, while mAcc is calculated
    as the ratio of correctly classified objects to the total number of objects for
    each class, and then averaged to give an overall measure of performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 3D object detection is the task of recognizing and localizing 3D objects in
    scene-level point clouds, aiming to estimate their precise positions and orientations.
    3D bounding boxes are annotated as ground truth for training 3D detectors. Average
    precision (AP) is a commonly used evaluation metric, calculated based on precision
    and recall for a given set of objects and confidence thresholds. The metric compares
    ground-truth bounding boxes with predicted ones, and is calculated as the area
    under the precision-recall curve. The precision is calculated as the ratio of
    the number of correctly predicted objects to the total number of predicted objects,
    while the recall is calculated as the ratio of the number of correctly predicted
    objects to the total number of ground-truth objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A summary of commonly used datasets for point cloud learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | #Samples | #Classes | Type | Representation | Label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ModelNet40 [[11](#bib.bib11)] | 2015 | 12,311 objects | 40 | Synthetic object
    | Mesh | Object category label |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeNet [[8](#bib.bib8)] | 2015 | 51,190 objects | 55 | Synthetic object
    | Mesh | Object/part category label |'
  prefs: []
  type: TYPE_TB
- en: '| ScanObjectNN [[12](#bib.bib12)] | 2019 | 2,902 objects | 15 | Real-world
    object | Points | Object category label |'
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D [[13](#bib.bib13)] | 2015 | 5K frames | 37 | Indoor scene | RGB-D
    | Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| S3DIS [[14](#bib.bib14)] | 2016 | 272 scans | 13 | Indoor scene | RGB-D |
    Point category label |'
  prefs: []
  type: TYPE_TB
- en: '| ScanNet [[9](#bib.bib9)] | 2017 | 1,513 scans | 20 | Indoor scene | RGB-D
    & mesh | Point category label & Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| KITTI [[15](#bib.bib15)] | 2013 | 15K frames | 8 | Outdoor driving | RGB
    & LiDAR | Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| nuScenes [[16](#bib.bib16)] | 2020 | 40K | 32 | Outdoor driving | RGB & LiDAR
    | Point category label & Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| Waymo [[17](#bib.bib17)] | 2020 | 200K | 23 | Outdoor driving | RGB & LiDAR
    | Point category label & Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| STF [[18](#bib.bib18)] | 2020 | 13.5K | 4 | Outdoor driving | RGB & LiDAR
    & Radar | Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| ONCE [[19](#bib.bib19)] | 2021 | 1M scenes | 5 | Outdoor driving | RGB &
    LiDAR | Bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic3D [[20](#bib.bib20)] | 2017 | 15 dense scenes | 8 | Outdoor TLS
    | Points | Point category label |'
  prefs: []
  type: TYPE_TB
- en: '| SemanticKITTI [[10](#bib.bib10)] | 2019 | 43,552 scans | 28 | Outdoor driving
    | LiDAR | Point category label |'
  prefs: []
  type: TYPE_TB
- en: '| SensatUrban [[21](#bib.bib21)] | 2020 | 1.2 $\mathrm{km}^{2}$ | 31 | UAV
    Photogrammetry | Points | Point category label |'
  prefs: []
  type: TYPE_TB
- en: '| SynLiDAR [[22](#bib.bib22)] | 2022 | 198,396 scans | 32 | Outdoor driving
    | Synthetic LiDAR | Point category label |'
  prefs: []
  type: TYPE_TB
- en: '| SemanticSTF [[23](#bib.bib23)] | 2023 | 2,086 scans | 21 | Outdoor driving
    | RGB & LiDAR | Point category label |'
  prefs: []
  type: TYPE_TB
- en: 3D semantic segmentation is the task of assigning semantic labels to each point
    in a 3D point cloud. Point-wise categorical annotations are collected as ground
    truth for this task. IoU (Intersection over Union) and mean IoU (mIoU) are commonly
    used metrics for evaluations. IoU measures the overlap between the predicted and
    ground truth segmentations for a given class and is calculated as the ratio of
    the intersection to the union of the two sets. IoU is calculated for each class
    separately. mIoU is the mean of the IoU values across all classes and provides
    an overall measure of the segmentation model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3D instance segmentation is a task that involves assigning a unique instance
    ID to each object in a point cloud, thereby separating objects belonging to the
    same category and enabling more accurate object recognition and tracking. Point-wise
    instance annotations are needed to train models for this task. The mean average
    precision (mAP) is a popular evaluation metric used in 3D instance segmentation,
    computed as the mean of the average precision (AP, as used in 3D object detection)
    values across all classes. To calculate mAP, the precision-recall curve is computed
    for each class, and the area under the curve (AUC) is calculated. The AP is then
    calculated as the mean of the precision values at a set of predefined recall levels.
    Finally, the mAP is obtained as the mean of the AP values for all classes.
  prefs: []
  type: TYPE_NORMAL
- en: Backbone. A ”backbone” is the essential and fundamental part of a neural network
    architecture that is responsible for extracting high-level features from input
    data. These features are then processed and analyzed by subsequent layers in the
    network. The backbone carries out most of the computation in a neural network
    and is crucial in determining its performance. To ensure fairness in comparing
    the performance of different label-efficient learning algorithms, it is important
    to use the same backbone implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Annotation efforts for 3D datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Annotating point clouds is challenging which usually requires special training
    due to the unique characteristics of point-cloud data. It faces several new challenges
    compared with annotating data of other modalities such as images. First, the display
    of point clouds is often unaligned with human perceptions. Point clouds are often
    incomplete, sparse, and may not contain color information, leading to rich ambiguity
    in point semantics and point geometries. Second, 3D view changes complicate the
    annotation process greatly which even cause motion sickness for annotators. Hence,
    point cloud annotators require good expertise and experience to ensure annotation
    accuracy and consistency while labelling, e.g., 3D bounding boxes and point-wise
    categories for 3D detection and segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Third, fully automatic point cloud annotation is still infeasible at the current
    stage. Although some tool such as semi-automatic labelling has been explored to
    streamline the process, the annotation accuracy remains low and plenty of manual
    efforts are required to inspect and correct the automatic annotations. Though
    different approaches have been proposed to simplify the manual annotation process,
    most of they do not generalize well with various extra requirements. For instance,
    Behley et al. [[10](#bib.bib10)] superimpose multiple LiDAR scans to formulate
    dense point representations, allowing labelling multiple scans concurrently and
    consistently while collecting SemanticKITTI. However, the superimposing process
    requires accurate and instant localization and pose of LiDAR sensors, and the
    superimposed moving objects are often distorted and indistinguishable. In summary,
    manual approach remains the primary way of point cloud annotation which requires
    vast time and efforts as well as well-trained annotators.
  prefs: []
  type: TYPE_NORMAL
- en: The labour-intensive nature of point cloud annotation makes the construction
    of large-scale point-cloud datasets extremely time-consuming. This directly leads
    to limited sizes and diversity in public point-cloud datasets as shown in Table [I](#S2.T1
    "TABLE I ‣ 2.1 Key concepts ‣ 2 Background ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds"), and poses a great challenge while developing generalizable
    point cloud learning algorithms. Studying label-efficient point cloud learning
    has become an urgent need to mitigate the limitation of existing point-cloud data.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Data Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data augmentation (DA) has been widely adopted in deep network training [[24](#bib.bib24)].
    As illustrated in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Data Augmentation ‣ A Survey of
    Label-Efficient Deep Learning for 3D Point Clouds"), it aims to increase the data
    size and diversity by artificially generating new training data from existing
    one. DA is particularly beneficial in scenarios where the available training data
    is limited. It is therefore considered an important label-efficient learning approach
    with widespread applications across various fields.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/460f5f2f5a8b7e006f59f949c563931b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Data augmentation in 3D network training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Categorization of data augmentation methods for point cloud learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Augmentation type | Approach | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Inta-domain augmentation (§[3.1](#S3.SS1 "3.1 Intra-domain augmentation ‣
    3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | Conventional augmentation techniques (§[3.1.1](#S3.SS1.SSS1 "3.1.1 Conventional
    augmentation techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)] |'
  prefs: []
  type: TYPE_TB
- en: '| Augmentation methods for 3D shape classification. (§[3.1.2](#S3.SS1.SSS2
    "3.1.2 DA for 3D shape classification ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)] |'
  prefs: []
  type: TYPE_TB
- en: '| Augmentation methods for 3D object detection. (§[3.1.3](#S3.SS1.SSS3 "3.1.3
    DA for 3D object detection ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | [[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Augmentation methods for 3D semantic segmentation. (§[3.1.4](#S3.SS1.SSS4
    "3.1.4 DA for 3D semantic segmentation ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | [[41](#bib.bib41), [42](#bib.bib42)] |'
  prefs: []
  type: TYPE_TB
- en: '| Inter-domain augmentation (§[3.2](#S3.SS2 "3.2 Inter-domain augmentation
    ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point
    Clouds")) | Leverages additional data sources, such as synthetic data, cross-modal
    data, etc. | [[43](#bib.bib43), [22](#bib.bib22), [44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)] |'
  prefs: []
  type: TYPE_TB
- en: 'This section reviews existing DA studies in point cloud network training, which
    can be broadly grouped into two categories: intra-domain augmentation and inter-domain
    augmentation. The former aims to enrich training data by generating new training
    data from the existing as detailed in Section [3.1](#S3.SS1 "3.1 Intra-domain
    augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds"). The latter leverages additional data to enlarge the existing
    training data distribution as detailed in Section [3.2](#S3.SS2 "3.2 Inter-domain
    augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds"). Table [II](#S3.T2 "TABLE II ‣ 3 Data Augmentation ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds") shows an overview of existing
    DA studies.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Intra-domain augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intra-domain DA aims to maximize the training knowledge by only utilizing the
    limited annotated training data available. Section [3.1.1](#S3.SS1.SSS1 "3.1.1
    Conventional augmentation techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    first provides an introduction to conventional DA that is generic and applicable
    in various point cloud tasks. Subsequently, we review DA methods that are designed
    for specific 3D tasks, including 3D shape classification in Section [3.1.2](#S3.SS1.SSS2
    "3.1.2 DA for 3D shape classification ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"),
    3D object detection in Section [3.1.3](#S3.SS1.SSS3 "3.1.3 DA for 3D object detection
    ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"), and 3D semantic segmentation in Section [3.1.4](#S3.SS1.SSS4
    "3.1.4 DA for 3D semantic segmentation ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds").
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Conventional augmentation techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Conventional DA has been extensively explored as a pre-processing operation
    in various 3D tasks [[25](#bib.bib25), [26](#bib.bib26), [53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55), [56](#bib.bib56)]. It adopts different spatial transformations
    to generate diverse views of point clouds that are crucial for learning transformation-invariant
    and generalizable representations. Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional
    augmentation techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") shows a list
    of typical conventional DA techniques together with qualitative illustrations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d11669fe842b9663028df8ee275469ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of widely-used conventional augmentation techniques
    for point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling changes the scale of the point cloud by multiplying the coordinates
    with a ratio $s$, where a value of $s<1$ indicates shrinkage and $s>1$ indicates
    enlargement as illustrated in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation
    techniques ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of
    Label-Efficient Deep Learning for 3D Point Clouds") (b).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flipping randomly flips points along the x-axis or y-axis, as illustrated in
    Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation techniques ‣ 3.1 Intra-domain
    augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning
    for 3D Point Clouds") (c).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotation rotates the points around the z-axis with a random angle, as illustrated
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation techniques ‣ 3.1
    Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") (d).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jittering adds random perturbations to point clouds with Gaussian noise with
    zero mean and a standard deviation of $\beta$[[25](#bib.bib25)], as illustrated
    in Fig.[3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation techniques ‣ 3.1
    Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") (e).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translation involves shifting all points in the same direction and distance,
    as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Conventional augmentation techniques
    ‣ 3.1 Intra-domain augmentation ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds") (f).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note conventional DA can be applied to both global point clouds and local point
    patches [[27](#bib.bib27), [57](#bib.bib57), [58](#bib.bib58)].
  prefs: []
  type: TYPE_NORMAL
- en: Conventional DA has been widely adopted in various point cloud learning tasks
    due to its simplicity and efficiency. However, it often leads to insufficient
    training due to two major factors. First, the DA process and network training
    are independent with little interaction, where the training outcome provides little
    feedback for DA optimization. Second, the new training samples are augmented from
    individual instead of a combination of multiple existing samples, leading to limited
    training data distribution. Many DA strategies have been designed to address the
    two limitations, which will be reviewed according to point cloud tasks in the
    ensuing subsections.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 DA for 3D shape classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several studies [[28](#bib.bib28), [29](#bib.bib29)] explored adaptive DA for
    3D shape classification. For example, Li et al. [[28](#bib.bib28)] designed Pointaugment
    that generates training samples with shape-wise transformation and point-wise
    displacement. The Pointaugment and object classifier are jointly optimized via
    adversarial learning. Kim et al. [[29](#bib.bib29)] exploited local deformations
    of objects, aiming to generate realistic object samples with more variation, e.g.,
    a person of varying poses. It introduces AugTune which allows adaptively controlling
    the strength of local augmentation while preserving the shape identity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b85579564b880002a845052b8bed837c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of typical mixing DA methods in point cloud classification,
    including PointMixup [[30](#bib.bib30)], RSMix [[31](#bib.bib31)], and SageMix [[34](#bib.bib34)].
    The figure is extracted from [[34](#bib.bib34)].'
  prefs: []
  type: TYPE_NORMAL
- en: Another line of research generates more diverse training objects by mixing existing
    ones. Inspired by MixUp [[59](#bib.bib59), [60](#bib.bib60)] in 2D image classification,
    Chen et al. [[30](#bib.bib30)] proposed PointMixup that generates object samples
    via shortest path linear interpolation between two objects of different classes.
    However, the interpolated samples may lose structural information of the original
    objects due to geometrical distortion. Several studies attempt to preserve the
    local object structures during mixing as illustrated in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1.2 DA for 3D shape classification ‣ 3.1 Intra-domain augmentation ‣ 3 Data
    Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds").
    For example, RSMix [[31](#bib.bib31)] mixes and generates new training samples
    by extracting rigid subsets from different point cloud objects. PointCutMix [[32](#bib.bib32)]
    cuts and replaces local object parts with the optimally assigned pair from other
    objects. SageMix [[34](#bib.bib34)] leverages saliency guidance to preserve local
    object structures in mixing. Point-MixSwap [[33](#bib.bib33)] mixes objects of
    the same categories to enrich the geometric variation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 DA for 3D object detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 3D object detection works with scene-level point clouds that are very different
    from object-level point clouds. Specifically, scene-level point clouds have much
    more points, more diverse surroundings, larger density variation, more noises
    or outliers, which pose both chances and challenges for DA. For example, Cheng
    et al. [[35](#bib.bib35)] proposed Progressive Population-Based Augmentation that
    searches for optimal DA strategies across point cloud datasets. Chen et al. [[36](#bib.bib36)]
    proposed Azimuth-Normalization to address the significant variation of LiDAR point
    clouds along the azimuth direction. Leng et al. [[37](#bib.bib37)] exploited pseudo
    labels of unlabelled data for DA in point cloud learning.
  prefs: []
  type: TYPE_NORMAL
- en: The mixing idea has also been explored for 3D object detection. For instance,
    Yan et al.[[38](#bib.bib38)] proposed GT-Aug to enrich foreground instances by
    copying objects from other LiDAR frames and randomly pasting them into the current
    frame. However, GT-Aug does not consider the relationships between objects in
    real-world scenarios during the pasting. To address this limitation, Sun et al.[[39](#bib.bib39)]
    performed object pasting by utilizing a correlation energy field to represent
    the functional relationship between objects. In addition, Wu et al. [[40](#bib.bib40)]
    fused multiple LiDAR frames to generate denser point clouds and then use them
    as references to enhance object detection in single-frame scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 DA for 3D semantic segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mixing-based DA has shown impressive performance gains in point cloud segmentation.
    For example, Nekrasov et al.[[41](#bib.bib41)] proposed Mix3D that directly concatenates
    two point clouds and their labels for out-of-context augmentation. Xiao et al.[[42](#bib.bib42)]
    proposed PolarMix that mixes LiDAR frames in the polar coordinate system to preserve
    unique properties of LiDAR point clouds such as partial visibility and density
    variation. They designed scene-level swapping and instance-level rotate-pasting
    that achieve consistent augmentation effects across multiple LiDAR segmentation
    and detection benchmarks. Fig. [5](#S3.F5 "Figure 5 ‣ 3.2 Inter-domain augmentation
    ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point
    Clouds") shows qualitative illustrations of the two mixing-based DA methods.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Inter-domain augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Inter-domain DA utilizes extra data to enhance network training. It can be
    broadly grouped into two categories depending on the types of data used: synthetic
    data and cross-modality data.'
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data. Several studies explored synthetic point clouds to augment real
    ones to improve point cloud network training [[43](#bib.bib43), [22](#bib.bib22)].
    For example, Fang et al.[[43](#bib.bib43)] designed LiDAR-Aug that inserts CAD
    objects such as pedestrians into point clouds of road scenes for generating training
    LiDAR scans with richer objects and training better 3D detectors. Xiao et al.[[22](#bib.bib22)]
    collected self-annotated LiDAR point clouds from game engines and combined them
    with real point clouds to train 3D segmentation networks. Though synthetic data
    provide a promising solution to mitigate the data constraint, they have clear
    domain gap [[22](#bib.bib22)] with real point clouds which often limits their
    effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c4dea30d077d30c850f0eee3c6c8209d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Mixing-based DA on point cloud semantic segmentation: Mix3D [[41](#bib.bib41)]
    performs out-of-context mixing, while PolarMix [[42](#bib.bib42)] applies in-context
    mixing. The two graphs are extracted from [[41](#bib.bib41)] and [[42](#bib.bib42)].'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-modality data. Several studies fuse point clouds with data of other modalities
    for alleviating the inherent limitations of 3D sensors. For example, RGB images
    are widely adopted to improve network training for 3D object detection [[44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]
    and 3D semantic segmentation [[50](#bib.bib50)]. Recently, several studies [[51](#bib.bib51),
    [52](#bib.bib52)] fused radar point clouds and LiDAR point clouds for learning
    more robust and generalizable point cloud models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised learning of point clouds has achieved great success though most efforts
    focus on collecting large-scale datasets or developing novel network architectures [[1](#bib.bib1)].
    Recent studies have shown that DA can reduce data collection and annotation endeavours
    and achieve comparable performance as by new network architectures, indicating
    the great potential of this research direction. However, DA for point cloud learning
    is still far under-explored especially compared with 2D image processing [[24](#bib.bib24)]
    and natural language processing (NLP) [[61](#bib.bib61)], and more efforts are
    needed to advance this meaningful research field.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Categorization of domain transfer learning methods for point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fields | Illustrations | Tasks | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Domain adaptation (§[4.1](#S4.SS1 "4.1 Domain adaptation ‣ 4 Domain Transfer
    Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    Adapting a machine learning model trained on one domain to perform well on another
    specific domain by minimizing the distribution shift between the domains. | Unsupervised
    domain adaptive 3D shape classification (§[4.1.2](#S4.SS1.SSS2 "4.1.2 Domain adaptation
    for 3D shape classification ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) | [[62](#bib.bib62),
    [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised domain adaptive 3D Object detection (§[4.1.3](#S4.SS1.SSS3 "4.1.3
    Domain adaptation for 3D object detection ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer
    Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    [[69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73),
    [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)] |'
  prefs: []
  type: TYPE_TB
- en: '| Unsupervised domain adaptive 3D Semantic segmentation (§[4.1.4](#S4.SS1.SSS4
    "4.1.4 Domain adaptation for 3D semantic segmentation ‣ 4.1 Domain adaptation
    ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")) | [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85),
    [86](#bib.bib86), [22](#bib.bib22), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89),
    [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92)] |'
  prefs: []
  type: TYPE_TB
- en: '| Other types of 3D domain adaptation (§[4.1.5](#S4.SS1.SSS5 "4.1.5 Extension
    ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds")) | [[88](#bib.bib88), [93](#bib.bib93)] |'
  prefs: []
  type: TYPE_TB
- en: '| Domain generalization (§[4.2](#S4.SS2 "4.2 Domain generalization ‣ 4 Domain
    Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | Building a machine learning model that can perform well on new, previously unseen
    domain(s) by learning invariant features that are common across different domain(s).
    | Domain generalized 3D shape classification (§[4.2.2](#S4.SS2.SSS2 "4.2.2 Domain
    generalization for 3D shape classification ‣ 4.2 Domain generalization ‣ 4 Domain
    Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | [[94](#bib.bib94), [95](#bib.bib95)] |'
  prefs: []
  type: TYPE_TB
- en: '| Domain generalized 3D Object detection (§[4.2.3](#S4.SS2.SSS3 "4.2.3 Domain
    generalization for 3D object detection ‣ 4.2 Domain generalization ‣ 4 Domain
    Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"))
    | [[96](#bib.bib96), [97](#bib.bib97)] |'
  prefs: []
  type: TYPE_TB
- en: '| Domain generalized 3D Semantic segmentation (§[4.2.4](#S4.SS2.SSS4 "4.2.4
    Domain generalization for 3D semantic segmentation ‣ 4.2 Domain generalization
    ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")) | [[23](#bib.bib23), [98](#bib.bib98), [99](#bib.bib99)] |'
  prefs: []
  type: TYPE_TB
- en: 4 Domain Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Domain transfer learning aims to exploit knowledge in previously collected
    and annotated data for handling various new data, hence reducing the labelling
    efforts of the new data significantly. However, transferring knowledge across
    data of different domains often faces domain discrepancy [[100](#bib.bib100),
    [101](#bib.bib101)], the distributional bias/shift across data of different domains.
    Consequently, models trained with source-domain data often experience clear performance
    drops when tested on data of target domains. The domain discrepancy problem has
    greatly hindered the deployment of point cloud models in various tasks. It has
    been studied in two typical approaches: domain adaptation and domain generalization.
    While both approaches aim to learn robust models from source data that can perform
    well on target data, domain adaptation permits access to target data in training
    while domain generalization does not. Table [III](#S3.T3 "TABLE III ‣ 3.3 Summary
    ‣ 3 Data Augmentation ‣ A Survey of Label-Efficient Deep Learning for 3D Point
    Clouds") shows an overview of existing domain transfer learning studies.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Domain adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Domain adaptation aims to adapt a model trained on a source domain to a specific
    target domain. It provides an economical solution for utilizing existing annotated
    training data with the same label space for fine-tuning models from a source domain
    to a target domain. For point clouds, domain adaptation studies have different
    setups depending on data prerequisites and application scenarios. Specifically,
    most existing studies focus on unsupervised domain adaptation (UDA) that learns
    from labeled source point clouds and unlabeled target point clouds. This section
    presents the problem setup of UDA in subsection [4.1.1](#S4.SS1.SSS1 "4.1.1 Problem
    setup ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"), UDA for 3D shape classification in subsection [4.1.2](#S4.SS1.SSS2
    "4.1.2 Domain adaptation for 3D shape classification ‣ 4.1 Domain adaptation ‣
    4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds"), UDA for 3D object detection in Section [4.1.3](#S4.SS1.SSS3 "4.1.3
    Domain adaptation for 3D object detection ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer
    Learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"), UDA
    for 3D semantic segmentation in Section [4.1.4](#S4.SS1.SSS4 "4.1.4 Domain adaptation
    for 3D semantic segmentation ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"), and other types
    of domain adaptation for point clouds in subsection [4.1.5](#S4.SS1.SSS5 "4.1.5
    Extension ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds").
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Problem setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given source-domain point clouds $X^{S}$ with the corresponding labels $Y^{S}$
    and target-domain point clouds $X^{T}$ without labels, the goal of point cloud
    adaptation is to learn a model $F$ that can produce accurate predictions $\hat{Y}^{T}$
    for unseen target data. The network training in UDA consists of two typical learning
    tasks, i.e., supervised learning from the labelled source data and unsupervised
    adaptation toward unlabelled target data, as shown in Fig. [6](#S4.F6 "Figure
    6 ‣ 4.1.1 Problem setup ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣
    A Survey of Label-Efficient Deep Learning for 3D Point Clouds"). Adaptation is
    usually achieved via four learning approaches: adversarial training, self-training,
    self-supervised learning, and style transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial training [[102](#bib.bib102), [103](#bib.bib103)] aims to learn
    domain-invariant features. It is achieved by training the model to extract features
    (from source and target samples) that are indistinguishable by a domain discriminator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-training [[104](#bib.bib104), [105](#bib.bib105)] employs a source-trained
    model to pseudo-label target data and adopts confident target predictions to retrain
    the model iteratively. It assumes that the confident target predictions have correct
    labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-supervised learning (SSL) [[2](#bib.bib2)] aims to learn useful representations
    from unlabelled target data without any explicit supervision. It is domain-agnostic
    and exploits the inherent data structure or patterns to define a task that can
    be solved without human annotations. With SSL over target data, the network can
    learn features that are tolerant to domain shifts, hence improving the model generalization
    on target data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Style transfer [[106](#bib.bib106), [22](#bib.bib22)] aims to translate source
    data to be similar to the target data for training. It works by learning a mapping
    function that transforms the source data to have similar styles as the target
    data. Models trained with the transferred data usually perform better on target
    data due to the reduced domain discrepancy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The following subsections review domain adaptive point cloud learning for various
    3D tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cd4986fb3f9228299988993fbb53d02c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Typical UDA pipeline for 3D network training'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Domain adaptation for 3D shape classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Object-level point clouds are often collected from various sources such as synthetic
    CAD models [[11](#bib.bib11), [8](#bib.bib8)] and real 3D scans [[107](#bib.bib107),
    [9](#bib.bib9)]. Due to differences in acquisition techniques and object characteristics,
    the collected point clouds may exhibit clear geometric discrepancies as illustrated
    in Fig. [7](#S4.F7 "Figure 7 ‣ 4.1.2 Domain adaptation for 3D shape classification
    ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"). Several studies have recently explored UDA
    for 3D shape classification across different 3D object datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f1c017a9f87cb41b5944176c0a68afd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Examples of object-level point clouds in datasets ModelNet [[11](#bib.bib11)],
    ShapeNet [[8](#bib.bib8)], and ScanNet [[9](#bib.bib9)]. The figure is reproduced
    based on [[62](#bib.bib62)].'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, Qin et al. [[62](#bib.bib62)] explored adversarial training and
    designed PointDAN that utilizes the Maximum Classifier Discrepancy[[102](#bib.bib102)]
    to align features across domains. Several subsequent work [[63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65)] explored self-paced self-training for domain adaptive 3D shape
    classification, where the confidence threshold gradually lowers while selecting
    pseudo labels. Fan et al.[[66](#bib.bib66)] designed a voting strategy that pseudo-labels
    target samples by searching for the nearest source neighbours in a shared feature
    space. Chen et al.[[67](#bib.bib67)] proposed quasi-balanced self-training to
    address the class imbalance in pseudo-labelling. Cardace et al. [[68](#bib.bib68)]
    proposed to refine noisy pseudo-labels by matching shape descriptors that are
    learned by the unsupervised task of shape reconstruction on both domains.
  prefs: []
  type: TYPE_NORMAL
- en: Several studies designed SSL tasks to encourage networks to learn domain-invariant
    features from unlabelled point cloud objects. For example, Zou et al. [[63](#bib.bib63)]
    introduced a joint task that predicts rotation angles and distortion locations.
    Fan et al.[[66](#bib.bib66)] reconstructed the squeezed 2D projections of objects
    back to 3D space. Shen et al. [[64](#bib.bib64)] learned unsupervised features
    by computing approximations of unsigned distance fields.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Domain adaptation for 3D object detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to differences in physical environments, sensor configurations, weather
    conditions, etc., scene-level point clouds are subject to more geometry shifts
    than object-level point clouds in term of point density and occlusion ratios.
    Domain adaptation across scene-level point clouds is thus even more challenging
    and it has recently attracted increasing attention thanks to the great values
    of scene-level 3D tasks such as 3D object detection and 3D semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptive 3D object detection has been studied extensively over the past
    few years. For example, Wang et al. [[69](#bib.bib69)] noticed that car size plays
    a crucial role in 3D object detection while adapting across data of different
    countries. They designed a simple normalization strategy for car size, which achieves
    superb adaptation performance. Later, adversarial training was explored for domain
    adaptive 3D object detection. For example, Su et al. [[70](#bib.bib70)] observed
    that semantic features contain both domain-specific attributes and other features
    that may mislead the discriminator. They thus disentangle the domain-specific
    attributes from the semantic features of LiDAR for better adversarial learning.
    Zhang et al. [[71](#bib.bib71)] recognized the distinctive geometric properties
    of LiDAR point clouds, i.e., larger and closer objects have more points, and designed
    scale-aware and range-aware domain alignment strategies for better adversarial
    training of 3D detectors.
  prefs: []
  type: TYPE_NORMAL
- en: Several methods [[108](#bib.bib108), [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)]
    explored self-training for domain adaptive 3D detection. For instance, ST3D [[72](#bib.bib72)]
    updates pseudo labels with a quality-aware triplet memory bank and trains networks
    with curriculum data augmentation. Luo et al. [[73](#bib.bib73)] designed a multi-level
    consistency network that learns with consistency at the levels of points, instances,
    and neural statistics. Some work [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77),
    [78](#bib.bib78)] instead explored style transfer. For example, Hahner et al.[[76](#bib.bib76),
    [78](#bib.bib78)] simulated fog and snowfall over authentic point clouds to alleviate
    domain discrepancy across weather. Xu et al. [[77](#bib.bib77)] generated semantic
    points at foreground regions with missing object parts and combine the generated
    points with the original to enhance detection across domains. Further, Yihan et
    al.[[79](#bib.bib79)] proposed a 3D contrastive co-training approach to improve
    the transferability of learned point features. Wei et al.[[80](#bib.bib80)] introduced
    a teacher-student framework that distills knowledge from high-beam LiDAR data
    to low-beam data, aiming to reduce the domain gap caused by different LiDAR beam
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Domain adaptation for 3D semantic segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LiDAR point clouds often have significant domain discrepancy due to variations
    in physical environments, sensor configurations, weather conditions, etc. Hence,
    most prior UDA studies [[86](#bib.bib86), [22](#bib.bib22), [87](#bib.bib87),
    [88](#bib.bib88), [109](#bib.bib109), [92](#bib.bib92)] focus on outdoor LiDAR
    point clouds, while just a few [[89](#bib.bib89)] tackle the issue for indoor
    point clouds. Fig. [8](#S4.F8 "Figure 8 ‣ 4.1.4 Domain adaptation for 3D semantic
    segmentation ‣ 4.1 Domain adaptation ‣ 4 Domain Transfer Learning ‣ A Survey of
    Label-Efficient Deep Learning for 3D Point Clouds") shows point-cloud samples
    of different domains that have clear domain discrepancies.
  prefs: []
  type: TYPE_NORMAL
- en: Studies on domain adaptive point cloud segmentation can be broadly classified
    into two categories namely, uni-modal UDA that works with point clouds alone [[86](#bib.bib86),
    [22](#bib.bib22), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89)] and cross-modal
    UDA that employs both point clouds and image data in training [[90](#bib.bib90),
    [91](#bib.bib91), [93](#bib.bib93), [110](#bib.bib110)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eef1b03416c78b1f5be41137cece3757.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Example of LiDAR scans of different domains. (a) A real scan of normal
    weather in SemanticKITTI [[10](#bib.bib10)], (b) A real scan of adverse weather
    of snow in SemanticSTF [[23](#bib.bib23)], and (c) A synthetic scan in SynLiDAR [[22](#bib.bib22)].
    Different colors denote different semantic categories as in (d).'
  prefs: []
  type: TYPE_NORMAL
- en: For uni-modal UDA, a line of studies [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85), [81](#bib.bib81)] projected point clouds to depth images and
    adopted 2D UDA methods to mitigate domain shifts. For example, Li et al. [[81](#bib.bib81)]
    proposed an adversarial training framework to learn to generate source masks to
    mimic the pattern of irregular target noise, thereby narrowing the domain gap
    from synthetic point clouds to real ones. However, the 3D-to-2D projection loses
    geometric information, and most 2D UDA methods cannot handle the unique geometry
    of point clouds. Moreover, most 2D UDA methods adopt CNN architectures and cannot
    be generalized to point cloud architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of methods [[86](#bib.bib86), [22](#bib.bib22), [87](#bib.bib87)]
    performed domain adaptive point cloud segmentation over point clouds directly.
    For example, [[86](#bib.bib86)] tackled domain adaptation by transforming it into
    a 3D surface completion task. [[22](#bib.bib22)] employed GANs to translate synthetic
    point clouds to match the sparsity and appearance of real ones. [[87](#bib.bib87),
    [42](#bib.bib42)] mix point clouds of source and target domains to generate intermediate
    representations with less domain discrepancy. While most studies focus on outdoor
    LiDAR point clouds, [[89](#bib.bib89)] recently explored synthetic-to-real adaptation
    of indoor point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: For cross-modal UDA, each training sample typically comprises a 2D image and
    a 3D point cloud that are synchronized across LiDAR and camera sensors. Point-wise
    3D annotations are provided for source data. The goal is to learn a robust 3D
    segmentor that can work independently and requires no images for testing. Though
    the paired images can enrich the learned representation, cross-modal UDA is more
    challenging due to the heterogeneity of the input spaces for images and point
    clouds as well as additional domain shifts between source and target images. Jaritz
    et al. [[90](#bib.bib90)] developed xMUDA, the first cross-modal UDA framework
    that adopts a two-stream architecture to address the domain gap of each modality
    individually. Peng et al. [[91](#bib.bib91)] achieved cross-modal UDA with two
    modules, the first employing intra-domain cross-modal learning for cross-modal
    interaction while the second adopting adversarial learning for cross-domain feature
    alignment via inter-domain cross-modal learning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 Extension
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Source-free UDA [[111](#bib.bib111)] is a variant of UDA that aims to adapt
    source-trained models to target distributions without accessing the source data
    in training. It is useful when data privacy and data portability are critical.
    Recently, Saltori et al. [[88](#bib.bib88)] proposed a pioneering study for source-free
    UDA of point clouds. They designed adaptive self-training with a geometric-feature
    propagation for semantic segmentation of LiDAR point cloud of road scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Test-time domain adaptation (TTA) is a setup where a source-pretrained model
    is adapted using only the unlabelled test data, usually with a single epoch of
    training. Unlike typical UDA, the goal of TTA is to avoid collecting target data
    in advance, where the model is adapted with the test data flow. Though TTA is
    practical in real-world scenarios, it is challenging as the target data is available
    in test-stage only. Recently, Inkyu Shin et al. [[93](#bib.bib93)] proposed the
    first TTA attempt on multi-modal 3D semantic segmentation. They designed a multi-modal
    fusion module to combine multi-modal input data for more accurate segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Domain generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another research direction in the field of domain transfer learning is domain
    generalization (DG) [[112](#bib.bib112)], which aims to train a model using labelled
    source data that can generalize to any target domains without accessing target
    data in training. DG removes the dependency on target training data, making it
    very useful in many real-world tasks where target data is difficult or expensive
    to obtain before deploying the model. It is also a critical research area for
    point cloud learning, as many point cloud tasks require 3D deep models to be robust
    and generalizable to unseen domains. For instance, autonomous vehicles require
    generalizable 3D perception to operate safely in various unseen places and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6c999c289c301e5018a40935ded69907.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Typical pipeline of domain generalization (DG) including (a) Single-source
    DG; (b) Multi-source DG.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Problem setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given labelled point clouds of $K$ similar but distinct source domains $\mathcal{S}={\{S_{k}=\{(x^{(k)},y^{(k)})\}\}}_{K}^{k=1}$,
    where $x$ denotes a point cloud and $y$ is its labels, DG aims to learn a deep
    model $F$ with the source data only that can perform well in unseen target domain
    $\mathcal{T}$. Similar to 2D DG studies [[112](#bib.bib112)], we review two DG
    settings for 3D point clouds as shown in Fig. [9](#S4.F9 "Figure 9 ‣ 4.2 Domain
    generalization ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds"). The first is multi-source DG which assumes the
    availability of more than one source domain in training, i.e., $K>1$. The motivation
    is to learn domain-invariant features (from multiple similar but distinct source
    domains) that can generalize well to any unseen domains. The second is single-source
    DG which is more challenging as it allows training data from a single source domain
    only. At the other end, single-source DG methods are more generic and can be applied
    to multi-source DG problems by ignoring the domain label.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Domain generalization for 3D shape classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pioneer work [[94](#bib.bib94)] first explored geometry shifts from simulated
    point clouds of CAD objects (e.g. ModelNet dataset[[11](#bib.bib11)]) to real
    object point clouds (e.g. ScanObjectNN [[107](#bib.bib107)]). It presents a meta-learning
    framework to train generalizable 3D classification models across domains. Later,
    Huang et al.[[95](#bib.bib95)] designed a manifold adversarial training scheme
    that exploits multiple geometric transformations to generate adversarial training
    samples of intermediate domains. Both studies fall under the single-source DG
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Categorization of weakly-supervised learning methods for point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Weak supervision | Annotation | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Incomplete supervision (§[5.1](#S5.SS1 "5.1 Incomplete supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    3D weakly-supervised learning: Sparsely annotating a small number of points in
    a large number of point cloud frames. (§[5.1.1](#S5.SS1.SSS1 "5.1.1 3D weakly-supervised
    learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised learning ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds")) | [[113](#bib.bib113),
    [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124), [125](#bib.bib125),
    [126](#bib.bib126)] |'
  prefs: []
  type: TYPE_TB
- en: '| 3D semi-supervised learning: Intensively annotating a small number of point
    cloud frames with fully labeled points. (§[5.1.2](#S5.SS1.SSS2 "5.1.2 3D semi-supervised
    learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised learning ‣ A Survey
    of Label-Efficient Deep Learning for 3D Point Clouds")) | [[127](#bib.bib127),
    [128](#bib.bib128), [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [109](#bib.bib109)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3D few-shot learning: Annotating a few samples for novel (unseen) classes
    on top of many labeled samples of base (seen) classes. (§[5.1.3](#S5.SS1.SSS3
    "5.1.3 3D few-shot learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    [[135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137), [138](#bib.bib138),
    [139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Inexact supervision (§[5.2](#S5.SS2 "5.2 Inexact supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    Weak annotations, e.g., cloud-/position-/box-level annotations, scribble, etc.
    | [[143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150),
    [151](#bib.bib151), [152](#bib.bib152), [153](#bib.bib153)] |'
  prefs: []
  type: TYPE_TB
- en: '| Inaccurate supervision (§[5.3](#S5.SS3 "5.3 Inaccurate supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")) |
    Noisy labels | [[154](#bib.bib154)] |'
  prefs: []
  type: TYPE_TB
- en: 4.2.3 Domain generalization for 3D object detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Improving the generalizability of 3D detectors is essential in 3D vision tasks
    such as autonomous driving where perception algorithms must maintain stable performance
    over unseen domains. However, DG for 3D object detection remains a relatively
    under-explored area. The pioneer study in [[96](#bib.bib96)] presented the first
    attempt at single-source DG for 3D object detection. It presents an adversarial
    augmentation method that learns to deform point clouds in training to enhance
    the generalization of 3D detectors. Recently, [[97](#bib.bib97)] introduced a
    single-source DG approach for multi-view 3D object detection in Bird-Eye-View
    (BEV). It decouples depth estimation from camera parameters, employs dynamic perspective
    augmentation, and adopts multiple pseudo-domains for better generalization toward
    various unseen new domains.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Domain generalization for 3D semantic segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several studies on domain-generalizable point cloud segmentation have been reported
    recently. Xiao et al.[[23](#bib.bib23)] study outdoor point cloud segmentation
    under adverse weather, where a domain randomization and aggregation learning pipeline
    was designed to enhance the model generalization performance. [[99](#bib.bib99)]
    augments the source domain and introduces constraints in sparsity invariance consistency
    and semantic correlation consistency for learning more generalized 3D LiDAR representations.
    Zhao et al.[[98](#bib.bib98)] instead focus on indoor point clouds and proposed
    clustering augmentation to improve model generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transferring knowledge across domains is a key strategy for maximizing the use
    of existing annotations. This triggered extensive studies of UDA and DG in the
    field of machine learning in the past years. Despite great advancements in related
    areas such as 2D computer vision and NLP, UDA and DG for point clouds is far under-explored.
    This is evidenced by the smaller number of published papers and low performance
    on various benchmarks as listed in the appendix. Hence, more efforts are urgently
    needed to advance the development of this very promising research area.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Weakly-supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Weakly-supervised learning (WSL), as an alternative to fully-supervised learning,
    leverages weak supervision for network training. Collecting weak annotations often
    reduces the annotation cost and time significantly, making WSL an important branch
    of label-efficient learning. With the WSL definition in [[155](#bib.bib155)],
    we categorize WSL methods on point clouds based on three types of weak supervision:
    incomplete supervision, inexact supervision, and inaccurate supervision. Incomplete
    supervision involves only a small portion of training samples being labelled,
    while inexact supervision provides coarse-grained labels that may not match the
    model output. Inaccurate supervision refers to noisy labels. Table [IV](#S4.T4
    "TABLE IV ‣ 4.2.2 Domain generalization for 3D shape classification ‣ 4.2 Domain
    generalization ‣ 4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds") shows a summary of representative approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Incomplete supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the context of incomplete supervision, only a subset of training point clouds
    is labeled. Incomplete supervision can be obtained with two labelling strategies:
    1) sparsely labelling a small number of points from many point-cloud frames and
    2) intensively labelling a small number of point cloud frames with more (or fully)
    labelled points. Following conventions in relevant literature, we refer to studies
    with the first strategy by ”3D weakly-supervised learning” and review them in
    Section [5.1.1](#S5.SS1.SSS1 "5.1.1 3D weakly-supervised learning ‣ 5.1 Incomplete
    supervision ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds"). For the second strategy, we refer to it by ”3D
    semi-supervised learning” and review relevant studies in Section [5.1.2](#S5.SS1.SSS2
    "5.1.2 3D semi-supervised learning ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"). Both
    labelling strategies adopt similar training paradigms with supervised learning
    on limited labelled points and unsupervised learning on massive unlabelled points,
    as shown in Fig. [10](#S5.F10 "Figure 10 ‣ 5.1.1 3D weakly-supervised learning
    ‣ 5.1 Incomplete supervision ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"). Additionally, we review ”3D few-shot learning”
    in Section [5.1.3](#S5.SS1.SSS3 "5.1.3 3D few-shot learning ‣ 5.1 Incomplete supervision
    ‣ 5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds") with a few labelled samples of novel classes and many labelled
    samples of base classes, aiming to reduce the labelling of novel classes in network
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 3D weakly-supervised learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 3D weakly-supervised learning (3D-WSL) learns with a small number of sparsely
    annotated points in each point cloud frame. It has high research and application
    value since it allows annotating more point-clouds frames with less labelling
    redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f568d12617aeffaf3fb035afcb573af1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Typical pipeline of training 3D point cloud networks with incomplete
    supervision.'
  prefs: []
  type: TYPE_NORMAL
- en: Problem setup. Let $P$ be a point cloud of the training set consisting of labelled
    points $\{(X_{l},Y_{l})\}$ and unlabelled points $\{(X_{u},\varnothing)\}$, where
    $X$ represents point space and $Y$ means label space. 3D-WSL aims to learn a function
    $f:X_{l}\cup X_{u}\mapsto Y$ given a large amount of point clouds including a
    tiny fraction of labelled points (e.g., 5%) as training input.
  prefs: []
  type: TYPE_NORMAL
- en: 3D-WSL for semantic segmentation. This task aims to learn a robust segmentation
    model with a small portion of labelled points in each point cloud. It has been
    studied via consistency-learning [[113](#bib.bib113), [117](#bib.bib117), [121](#bib.bib121),
    [156](#bib.bib156)] that aims to learn generalizable representations by enforcing
    prediction consistency across different augmented views of the same input data.
    For instance, Xu et al.[[113](#bib.bib113)] introduced a Siamese self-supervision
    branch for consistent learning from unlabelled points. Zhang et al.[[117](#bib.bib117)]
    conducted consistency learning between original data points and their perturbed
    versions. Wu et al. [[121](#bib.bib121)] designed a dual adaptive transformation
    that encourages consistent predictions between original points and their transformed
    counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Several studies [[122](#bib.bib122), [118](#bib.bib118)] explored contrastive
    learning on unlabelled point clouds. It pulls the features of points toward their
    augmented views while pushing them away from other points, aiming to learn structural
    point representations in an unsupervised manner. For instance, Liu et al.[[122](#bib.bib122)]
    over-segmented point clouds to extract point boundaries for region-level contrastive
    learning. Li et al.[[118](#bib.bib118)] combined consistency learning and contrastive
    learning for learning more comprehensive representations. Recent studies [[114](#bib.bib114),
    [119](#bib.bib119)] also explored self-training for point cloud segmentation.
    For example, Shi et al.[[119](#bib.bib119)] annotated only a small portion of
    points in the first LiDAR frame and selected confident predictions of unlabelled
    points as pseudo labels for network re-training. Hu et al.[[120](#bib.bib120)]
    recently proposed a Semantic Query Network that leverages sparsely labelled points
    and their local neighbours to learn a compact neighbourhood representation, achieving
    very promising segmentation performance with only 0.1% of labelled points.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of randomly selecting points for labelling, active learning selects
    more representative points for labelling. For instance, Wu et al.[[125](#bib.bib125)]
    exploited entropy, color discontinuity, and structural complexity to split point
    clouds into sub-regions and select representative sub-regions for labelling. Hu
    et al.[[126](#bib.bib126)] used prediction inconsistency across LiDAR frames as
    a measure of uncertainty for active sample selection. This approach requires only
    5% or fewer annotations but achieves comparable segmentation with fully supervised
    models. Several recent studies [[123](#bib.bib123), [157](#bib.bib157)] labelled
    segments instead of individual points as local points often share the same semantics
    due to the homogeneity of 3D objects in scenes. For example, Liu et al. [[123](#bib.bib123)]
    pre-segmented LiDAR sequences into connected components for coarse labelling.
    This strategy greatly reduces labelling efforts as compared with point-wise labelling.
  prefs: []
  type: TYPE_NORMAL
- en: 3D-WSL for object detection. 3D-WSL for object detection is a largely under-explored
    research area. Liu et al. [[124](#bib.bib124)] recently conducted a pioneering
    exploration of sparse annotation strategy for 3D object detection. They annotate
    only one 3D object in each scene and then use the prediction confidence to mine
    object instances for network re-training, achieving similar detection performance
    while reducing the annotation effort greatly.
  prefs: []
  type: TYPE_NORMAL
- en: 3D-WSL for instance segmentation. 3D-WSL has recently been explored for instance
    segmentation by “annotating one point per instance” [[157](#bib.bib157), [158](#bib.bib158)].
    For example, Tao et al. [[157](#bib.bib157)] first over-segmented point clouds
    and then clicked one point per segment to assign its location, category, and instance
    identity. Tang et al. [[158](#bib.bib158)] selected and labelled one informative
    point per instance and achieved competitive performance on several public benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 3D semi-supervised learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 3D semi-supervised learning (3D-SemiSL) works with intensive (full) annotation
    of a small portion of point cloud frames. As studied in [[113](#bib.bib113)],
    the annotation strategy in 3D-SemiSL leads to inferior part segmentation than
    that in 3D-WSL, largely due to its higher annotation redundancy. However, 3D-SemiSL
    is advantageous in requiring less training data collection during annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Problem setup. Given point clouds $\mathbf{X}_{l}\in\mathbb{R}^{N_{l}\times
    3}$ with labels $\mathbf{Y}_{l}$ and unlabeled point clouds $\mathbf{X}_{u}\in\mathbb{R}^{N_{u}\times
    3}$ ($N_{l}$ and $N_{u}$ are point cloud numbers, $N_{l}<N_{u}$), 3D-SemiSL aims
    to learn a point cloud model $F$ from the labeled data and unlabeled data that
    can perform well on unseen point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 3D-SemiSL for object detection. Most existing 3D-SemiSL detection studies adopted
    the Mean-Teacher framework [[159](#bib.bib159)] involving a teacher network and
    a student network of the same architecture. The teacher model is a moving average
    of student models, and its predictions guide the student learning. It assumes
    that the teacher model learns more robust representations that can benefit the
    student learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3D-SemiSL has been extensively studied for object detection. For example, SESS [[130](#bib.bib130)]
    adopts consistency learning [[160](#bib.bib160)] between the teacher and student,
    aiming for a perturbation invariant output distribution by assuming that decision
    boundaries lie in low-density regions [[161](#bib.bib161)]. 3DIoUMatch [[131](#bib.bib131)]
    selects confident predictions by the teacher model as pseudo labels for network
    re-training, aiming to minimize the entropy of predictions [[162](#bib.bib162)]
    of student models and lower the point density at the decision boundaries [[163](#bib.bib163)].
    Yin et al. [[132](#bib.bib132)] proposed a Proficient Teacher model that introduces
    a spatial-temporal ensemble module and a clustering-based box voting strategy
    to further enhance pseudo-labelling of 3D bounding boxes. Liu et al. [[109](#bib.bib109)]
    introduced a dual-threshold strategy and data augmentation to improve hierarchical
    supervision and feature representation in training the student network.
  prefs: []
  type: TYPE_NORMAL
- en: 3D-SemiSL for segmentation. Compared to bounding box annotations in 3D object
    detection, point-wise labelling in point cloud segmentation is even more laborious
    and time-consuming. 3D-SemiSL for point cloud segmentation has therefore garnered
    even more attention recently [[127](#bib.bib127), [128](#bib.bib128), [164](#bib.bib164),
    [133](#bib.bib133), [129](#bib.bib129)].
  prefs: []
  type: TYPE_NORMAL
- en: For 3D semantic segmentation, Jiang et al.[[127](#bib.bib127)] proposed a guided
    point contrastive learning framework to increase the generalization of segmentation
    models. Cheng et al. [[128](#bib.bib128)] built superpoint graphs and pseudo-label
    superpoints to train graph neural networks in a semi-supervised manner. Kong et
    al. [[133](#bib.bib133)] mixed laser beams from different LiDAR scans and learned
    more generalizable segmentation models by encouraging the model to make consistent
    predictions before and after mixing. Li et al. [[134](#bib.bib134)] introduced
    Sparse Depthwise Separable Convolution and constructed a light segmentation model
    requiring less training data. For 3D instance segmentation, Chu et al. [[129](#bib.bib129)]
    proposed a two-way inter-label self-training framework that leverages pseudo semantic
    labels and pseudo instance proposals to mutually denoise pseudo signals for better
    semantic-level and instance-level supervision.
  prefs: []
  type: TYPE_NORMAL
- en: 3D-SemiSL for other point cloud tasks. 3D-SemiSL has also been explored in other
    3D point cloud tasks thanks to its superiority in saving human annotations. For
    instance, Huang et al.[[165](#bib.bib165)] proposed a semi-supervised framework
    for point cloud registration while several studies [[166](#bib.bib166), [167](#bib.bib167)]
    explore semi-supervised 3d hand pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semi-supervised domain adaptation. The combination of semi-supervised learning
    and domain adaptation formulates another way of label-efficient learning, namely,
    semi-supervised domain adaptation (SSDA). SSDA involves three types of data in
    training: labeled source samples, many unlabeled target samples, and a small amount
    of labeled target samples, aiming for a model that performs well in the target
    domain. Several studies explored SSDA for different point cloud learning tasks
    such as semantic segmentation of LiDAR point clouds [[22](#bib.bib22)] and 3D
    object detection [[168](#bib.bib168)].'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 3D few-shot learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fully supervised deep networks learn from a large amount of training samples
    under a “closed set” setup, i.e., the training and testing data have the same
    label space. Such supervised learning is not ideal for quickly learning new concepts
    with limited data, which motivates few-shot learning (FSL) that aims to learn
    a novel class from just a few labelled samples. FSL can be seen as an extension
    of semi-supervised learning in an ”open-set” setup that has only a few labelled
    samples of novel classes, along with many labelled samples of base classes [[5](#bib.bib5)].
    Due to its superb merit in data requirements, FSL has recently attracted increasing
    attention with several ground-breaking studies [[135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem setup. There are two typical settings in FSL: The $N$-way-$K$-shot [[169](#bib.bib169)]
    where the training set and testing set are disjoint in terms of classes; The generalized
    FSL [[170](#bib.bib170)] that recognizes both base and novel classes in testing.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '$N$-way-$K$-shot. Let $(x,y)$ denotes a point cloud $x$ and its label $y$.
    FSL aims to train a model on a group of few-shot tasks sampled from a data set
    with a training class set $C_{\mathrm{train}}$ and test the trained model on another
    group of tasks sampled from a data set with new classes $C_{\mathrm{test}}$, where
    $C_{\mathrm{train}}\cap C_{\mathrm{test}}=\varnothing$. Each few-shot task is
    denoted by an episode, which is instantiated as a $N$-way-$K$-shot task with a
    few query samples and support samples: The query samples form a query set $\mathcal{Q}=\{(x_{i}^{\mathcal{Q}},y_{i}^{\mathcal{Q}}))\}^{N_{q}=N\times
    Q}_{i=1}$ containing $N$ classes in $C_{\mathrm{train}}$ with $Q$ samples of each
    class, and the support samples forms a support set $\mathcal{S}=\{(x_{i}^{\mathcal{S}},y_{i}^{\mathcal{S}}))\}^{N_{s}=N\times
    K}_{i=1}$ containing the same $N$ classes in $C_{\mathrm{train}}$ with $K$ examples
    of each class. The goal of the $N$-way-$K$-shot learning is to train a model $F(x^{\mathcal{Q}},{\mathcal{S}})$
    that predicts the label distribution $H$ for any query point cloud $x^{\mathcal{Q}}$
    based on ${\mathcal{S}}$. In testing, the trained model is tested over the testing
    episodes ${\mathcal{V}}={(S_{j},Q_{j})}^{J}_{j=1}$ for the new classes $C_{\mathrm{test}}$.
    Note the ground-truth labels $y^{\mathcal{Q}}$ of query samples are only available
    during training.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized FSL. This is a more challenging FSL setting. It involves training
    data of base classes and novel classes, including abundant labelled data of the
    base classes and few-shot labelled samples of the novel classes. The goal is to
    obtain a few-shot model that can learn to recognize novel objects by leveraging
    knowledge learnt from the base classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'FSL has been widely studied for 2D images. Most work performs meta-learning
    in three typical approaches: 1) Metric learning [[171](#bib.bib171), [172](#bib.bib172)]
    that measures the support-query similarity and group each query sample into its
    nearest support class in the latent space; 2) Optimization approach [[173](#bib.bib173),
    [174](#bib.bib174)] that differentiates support-set optimization for fast adaptation;
    3) Model-based approach [[175](#bib.bib175), [176](#bib.bib176)] that tailors
    model architectures for fast learning. We review 3D FSL for point clouds, a far
    under-explored area due to many modal-specific challenges such as unordered data
    structures, point sparsity, and large geometric variations. Several pioneering
    studies recently explored different 3D FSL tasks on 3D shape classification [[135](#bib.bib135),
    [136](#bib.bib136)], 3D semantic segmentation [[138](#bib.bib138)], 3D object
    detection [[140](#bib.bib140)], and 3D instance segmentation [[141](#bib.bib141)].'
  prefs: []
  type: TYPE_NORMAL
- en: FSL for 3D shape classification. Ye et al. [[135](#bib.bib135), [136](#bib.bib136)]
    conducted a pioneering study on FSL for 3D shape classification under the $N$-way-$K$-shot
    setting. They extended existing 2D FSL methods for 3D point-cloud data and introduced
    a baseline method to deal with high intra-class variance and subtle inter-class
    differences in point cloud representations. Yang et al. [[137](#bib.bib137)] projected
    point clouds into depth images and explored cross-modal FSL for 3D shape classification.
    In addition, Chowdhury et al. [[142](#bib.bib142)] studied few-shot class-incremental
    learning that incrementally fine-tunes a trained model (on base classes) for novel
    classes with few samples.
  prefs: []
  type: TYPE_NORMAL
- en: FSL for 3D Segmentation. Zhao et al. [[138](#bib.bib138)] explored FSL for 3D
    semantic segmentation under the $N$-way-$K$-shot setting. They distill discriminative
    knowledge from scarce support that can effectively represent the distributions
    of novel classes and leverage such knowledge for 3D semantic segmentation. Ngo
    et al. [[141](#bib.bib141)] proposed a geodesic-guided transformer for 3D few-shot
    instance segmentation of indoor dense point clouds. They employed a few support
    point cloud scenes and their ground-truth masks to generate discriminative features
    for instance mask prediction, and utilized geodesic distance as guidance with
    improved segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: FSL for 3D object detection. FSL-based 3D detection is far under-explored. Zhao
    et al. [[140](#bib.bib140)] designed Prototypical VoteNet, the first 3D detector
    for generalized FSL. They introduced a class-agnostic 3D primitive memory bank
    to store geometric prototypes of base classes and designed multi-head cross-attention
    to associate the geometric prototypes with scene points for better feature representations.
    Similar to 3D FSL segmentation, the study only covers indoor dense point clouds
    with dense representations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Inexact supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term “inexact supervision” refers to supervision that is not as precise
    as desired for specific tasks. One example is coarse-grained labels that are much
    easier to collect.
  prefs: []
  type: TYPE_NORMAL
- en: 3D semantic segmentation. Different weak supervision has been explored to save
    expensive point-wise annotation. For instance, Wei et al.[[143](#bib.bib143),
    [177](#bib.bib177)] employed subcloud-level labels for point cloud parsing, where
    classes appearing in the neighbourhood of uniformly sampled seeds are used as
    labels. Unal et al.[[145](#bib.bib145)] used scribbles as labels for LiDAR point
    clouds as shown in Fig. [11](#S5.F11 "Figure 11 ‣ 5.2 Inexact supervision ‣ 5
    Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds"), greatly facilitating the efficiency of point cloud labelling greatly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79afe099a3d57d76afebf25c8664a648.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Example of scribble-annotated LiDAR point cloud scenes (left) and
    superimposed frames (right) in ScribbleKITTI [[145](#bib.bib145)]. The figure
    is extracted from [[145](#bib.bib145)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3D object detection. Several recent studies used position-level annotations
    instead of 3D bounding boxes for 3D detection. For example, Meng et al. [[146](#bib.bib146)]
    and Xu et al. [[148](#bib.bib148)] used object centers to provide coarse position
    information for training. Ren et al. [[144](#bib.bib144)] employed scene-level
    tags for point cloud segmentation and detection without involving any point-wise
    semantic labels or object locations. Beyond 3D weak annotations, several studies
    exploited 2D image classes [[153](#bib.bib153)] or 2D bounding boxes [[151](#bib.bib151),
    [152](#bib.bib152)] to guide the training of 3D detectors. It reduces the annotation
    cost significantly as 2D annotations are much easier to collect.
  prefs: []
  type: TYPE_NORMAL
- en: 3D instance segmentation. Another line of research [[149](#bib.bib149), [150](#bib.bib150)]
    exploited coarse 3D bounding boxes to train 3D instance segmentation networks
    as illustrated in Fig. [12](#S5.F12 "Figure 12 ‣ 5.2 Inexact supervision ‣ 5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"). To
    address the inaccuracy of 3D bounding boxes, Liao et al. [[149](#bib.bib149)]
    iteratively refined the bounding boxes and performed point-wise instance segmentation
    with the refined bounding boxes. Differently, Chibane et al. [[150](#bib.bib150)]
    introduced Box2Mask that adopts Hough Voting to generate accurate instance segmentation
    masks from 3D bounding boxes. These studies show promising 3D instance segmentation
    performance under weak supervision signals.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73d4ba2b53d2a590e6981670484bd1dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Typical pipeline of training 3D segmentation networks with inexact
    supervision of 3D bounding boxes.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Inaccurate supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inaccurate supervision refers to annotations that are noisy with false labels.
    It’s very common as human annotators cannot guarantee 100% accuracy especially
    when only limited time and resources are available. The noisy labels provide wrong
    guidance and often hinder network training. Therefore, the key to learning with
    inaccurate supervision is to refine annotations and improve supervision quality
    as illustrated in Fig. [13](#S5.F13 "Figure 13 ‣ 5.3 Inaccurate supervision ‣
    5 Weakly-supervised learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5307bc858f04b22a33c13464b4c95b79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Typical pipeline for training 3D networks with inaccurate supervision'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the high research and application value, robust point cloud learning
    from inaccurate supervision is largely neglected in the literature. Ye et al. [[154](#bib.bib154),
    [178](#bib.bib178)] designed a 3D semantic segmentation framework that introduces
    point-level confidence mechanism to select reliable labels and a cluster-level
    label correction process to refine training data. More research is needed to advance
    this very useful but far under-explored research area.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Weakly supervised point cloud learning is an emerging area that has attracted
    increasing attention in recent years. It aims to train robust deep network models
    with partially available, noisy, or not exactly desired annotations. Preparing
    precise supervision is very challenging for unordered and unstructured point clouds,
    which has recently triggered great progress in weakly supervised point cloud learning.
    Some work shows that weakly supervised models can even achieve competitive performance
    as fully supervised models. In addition, researchers have explored combining weakly
    supervised learning with other techniques such as transfer learning and self-supervised
    learning for better point cloud modelling. Their studies demonstrate the potential
    of weakly supervised learning for point clouds and a broader field of 3D deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the progress in weakly supervised point cloud learning is ongoing,
    and new methods have been proposed and evaluated regularly. This research field
    is expected to continue to grow and evolve in the coming years as researchers
    continue to explore new ways to leverage weakly supervised learning for point
    cloud understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Pretrained Foundation Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE V: Categorization of point cloud studies with pretrained foundation models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pretraining type | Approach | References |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Self-supervised pretraining (§[6.1](#S6.SS1 "6.1 Self-supervised pretraining
    ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds")) | Contrastive-learning-based methods | Please refer to [[2](#bib.bib2)]
    for a systematic survey |'
  prefs: []
  type: TYPE_TB
- en: '| Generation-based methods |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-modal pretraining (§[6.2](#S6.SS2 "6.2 Multi-modal pretraining ‣ 6
    Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")) | Transferring knowledge from existing vision-language foundation
    models to point cloud models (§[6.2](#S6.SS2 "6.2 Multi-modal pretraining ‣ 6
    Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds")) | [[179](#bib.bib179), [180](#bib.bib180), [181](#bib.bib181),
    [182](#bib.bib182)] |'
  prefs: []
  type: TYPE_TB
- en: '| Extending the image-text pretraining paradigm to point cloud-text pretraining
    (§[6.2.2](#S6.SS2.SSS2 "6.2.2 Language-point cloud pretraining ‣ 6.2 Multi-modal
    pretraining ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep
    Learning for 3D Point Clouds")) | [[183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185),
    [186](#bib.bib186), [187](#bib.bib187)] |'
  prefs: []
  type: TYPE_TB
- en: The recent advance of pretrained foundation models (PFMs) has yielded great
    breakthroughs across various AI fields including 2D computer vision, NLP, and
    their intersection (i.e., vision-language foundation models (VLMs) [[188](#bib.bib188),
    [189](#bib.bib189)]). PFMs with Internet-scale data in an unsupervised manner [[190](#bib.bib190),
    [191](#bib.bib191), [192](#bib.bib192)] can be easily adapted to downstream tasks
    by fine-tuning with much fewer task data, enabling fast network convergence and
    learning with small data. In addition, VLMs [[193](#bib.bib193)] trained with
    image-text pairs have demonstrated remarkable zero-shot visual prediction performance,
    being able to recognize objects of novel concepts with impressive accuracy without
    involving any labelled images but only text illustrations in training.
  prefs: []
  type: TYPE_NORMAL
- en: The great success of PFMs sheds light on label-efficient learning for point
    clouds. This section reviews related 3D studies with self-supervised pretraining
    in Section [6.1](#S6.SS1 "6.1 Self-supervised pretraining ‣ 6 Pretrained Foundation
    Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") and multi-modal
    pretraining in Section [6.2](#S6.SS2 "6.2 Multi-modal pretraining ‣ 6 Pretrained
    Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds").
    Relevant challenges are finally discussed in Section [6.3](#S6.SS3 "6.3 Summary
    and discussion ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"). Table [V](#S6.T5 "TABLE V ‣ 6 Pretrained
    Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    shows a summary of representative approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Self-supervised pretraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bed1882bf349302139a4f4bd5cddcb90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Typical pipeline for self-supervised pertaining. The figure is extracted
    from [[2](#bib.bib2)].'
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised pretraining learns from large-scale unlabelled point clouds,
    and the learnt parameters can be applied to initialize downstream networks for
    faster convergence and effective learning from small task data, as illustrated
    in Fig. [14](#S6.F14 "Figure 14 ‣ 6.1 Self-supervised pretraining ‣ 6 Pretrained
    Foundation Models ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds").
    It has attracted increasing interest as it can work with no human annotations.
    A milestone is PointContrast [[56](#bib.bib56)] which learns network weights from
    3D scene frames and fine-tunes networks on multiple high-level 3D tasks such as
    semantic segmentation and object detection. However, the performance gains of
    self-supervised pretraining remain limited compared with 2D image and NLP pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most existing studies tackle point cloud pretraining via two approaches: contrastive
    pretraining and generative pretraining. Contrastive pretraining [[56](#bib.bib56),
    [194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196)] adopts a discriminative
    approach and it learns by maximizing the similarity of positive pairs (different
    augmentations of the same sample or different views of the same scene) while minimizing
    similarity between negative pairs (different samples). This enhances the network’s
    ability to distinguish between similar and dissimilar examples, leading to improved
    generalization performance. Differently, generative pretraining [[197](#bib.bib197),
    [198](#bib.bib198)] learns to generate new point-cloud samples with similar input
    distribution. The learned model captures representative features of input data
    which can be fine-tuned for downstream tasks. Recently, Xiao et al. [[2](#bib.bib2)]
    performed a comprehensive survey of self-supervised learning for point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Multi-modal pretraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike self-supervised pretraining that learns from massive unlabeled data,
    VLMs are pre-trained with image-text pairs crawled from the Internet. The objective
    is to train a model to understand the relationships between images and their corresponding
    textual descriptions. Its remarkable zero-/few-shot recognition ability has inspired
    several studies on multi-modal point cloud pretraining. Two typical approaches
    have been explored: (1) transferring knowledge from existing VLMs to point cloud
    models and (2) extending the image-text pretraining paradigm for point cloud-text
    pretraining.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 From language-vision to point cloud
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VLMs are trained on billions of images with semantic-rich captions. Though they
    advanced open-vocabulary image understanding tasks greatly, they are not directly
    applicable in the 3D domain due to the lack of large-scale 3D-text pairs. One
    line of research aims to leverage the knowledge in VLMs to aid point cloud learning.
    The primary approach relies on images paired with point clouds as a bridge for
    knowledge distillation across modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Several pioneering studies [[179](#bib.bib179), [180](#bib.bib180), [199](#bib.bib199)]
    transfer CLIP [[193](#bib.bib193)] model for point cloud classification. For instance,
    Zhang et al. [[179](#bib.bib179)] generate scatter depth maps by projecting raw
    points onto pre-defined image planes, feed the depth maps to CLIP’s visual encoder
    to extract multi-view features, and obtain zero-shot predictions with a text-generated
    classifier. Liu et al.[[181](#bib.bib181)] render object-level point clouds into
    multi-view images of predefined camera poses for low-shot object part segmentation.
    The rendered images are fed to the pretrained GLIP[[200](#bib.bib200)] along with
    a text prompt for predicting bounding boxes from the text prompt. Wang et al. [[182](#bib.bib182)]
    exploit visual-linguistic assistance for 3D semantic scene graph prediction. The
    method projects point clouds into images and trains a multi-modal model to capture
    semantics from vision, language, and point clouds, and it adopts CLIP to align
    the visual-linguistic semantics.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Language-point cloud pretraining
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by impressive performance of VLMs, researchers are now exploring the
    extension of the vision-language pretraining for point-cloud learning. However,
    collecting Internet-scale point-text samples is extremely difficult. To overcome
    this challenge, recent studies exploit VLMs to generate captions for image data
    that can be easily obtained and aligned with point clouds, producing an abundance
    of point-text pairs for pretraining. This approach allows learning rich and transferrable
    3D visual-semantic representations with little human annotations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e343ad76bdc8870d420af988a5c7cde9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: One representative pipeline to leverage multi-view images of 3D
    scenes to access knowledge in vision-language foundation models, enabling language
    supervision without human annotation. The figure is modified from [[184](#bib.bib184)].'
  prefs: []
  type: TYPE_NORMAL
- en: For example, Xue et al. [[183](#bib.bib183)] design cross-modal contrastive
    learning to learn a unified representation of images, texts, and point clouds
    for 3D shape classification. They adopt CLIP to generate training triplets to
    learn a 3D representation space that aligned with the image-text space. Zeng et
    al.[[186](#bib.bib186)] explore contrastive language-image-point pretraining for
    point cloud object recognition. They adopt DetCLIP[[201](#bib.bib201)] to extract
    image proposals given language captions, employ the proposals to parse corresponding
    point cloud instances, and conduct cross-modal contrastive pretraining to learn
    semantic-level language-3D alignment between texts and point clouds as well as
    instance-level image-3D alignment between images and point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: While most studies focus on object-level point cloud understanding, several
    studies [[184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187)] explore open-vocabulary
    scene understanding with VLM knowledge. They tackle different tasks such as 3D
    semantic segmentation, 3D object detection, and 3D instance segmentation, aiming
    to localize and recognize categories that are not present in the annotated label
    space. For example, Ding et al.[[184](#bib.bib184)], as shown in Fig. [15](#S6.F15
    "Figure 15 ‣ 6.2.2 Language-point cloud pretraining ‣ 6.2 Multi-modal pretraining
    ‣ 6 Pretrained Foundation Models ‣ A Survey of Label-Efficient Deep Learning for
    3D Point Clouds"), generate captions for images of 3D indoor scenes to create
    hierarchical point-caption pairs including scene-, view-, and entity-level captions.
    The pairs provide coarse-to-fine supervision signals and help learn appropriate
    3D visual-semantic representations with contrastive learning. With frozen text
    encoder in BERT[[192](#bib.bib192)] or CLIP, category embeddings can be extracted
    as text-embedded semantic classifier for recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Though [[184](#bib.bib184)] achieves promising results, it is often hindered
    by its coarse image-level inputs. Consequently, it largely identifies sparse and
    salient scene objects only, making it difficult for dense understanding tasks
    such as semantic and instance segmentation. Yang et al. [[185](#bib.bib185)] address
    this issue by introducing dense visual prompts that elicit region-level visual-language
    knowledge via captioning. The method allows creating dense regional point-language
    associations, enabling point-discriminative contrastive learning for point-independent
    learning from captions as well as better open-vocabulary scene understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Summary and discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PFMs have shown great potential in aiding point cloud learning with much less
    human annotations. Despite several groundbreaking studies, this field of research
    remains under-unexplored, leaving plenty of opportunities for further investigation.
    Specifically, though self-supervised pretraining has shown great effectiveness
    in 2D computer vision and NLP, its effects on point cloud learning lag far behind
    with random initialization still dominating the field. This is largely attributed
    to the scarcity of large-scale point cloud data, and the absence of unified and
    generalizable point cloud backbone models also affects [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, though VLMs offer a viable solution for point cloud pretraining,
    the potential of pre-training language-point cloud foundation models remains largely
    untapped. One major challenge is the construction of internet-scale point-text
    pairs for pretraining, which is a daunting task as we cannot crawl infinite 3D
    data from the internet, not to mention point-text pairs. Leveraging existing VLMs
    helps a lot, but it still requires collecting massive pairs of point clouds and
    images with the latter serving as the bridge for knowledge transfer. Despite these
    challenges, this research direction is very promising, and more research is expected
    to fully explore its potential.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss several open problems and future directions
    in label-efficient learning for point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Data Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Efficient labelling pipelines and tools. Point-wise annotation is far more laborious
    than annotating images which largely explains the scarcity of large-scale point
    cloud datasets. Efficient annotation tools and automatic/semi-automatic annotation
    techniques have been attempted but their performance cannot meet the increasing
    demand of large-scale point cloud data. More efficient annotation tools and techniques
    are urgently needed for better exploitation of the very useful point cloud data.
  prefs: []
  type: TYPE_NORMAL
- en: Next-generation datasets. Most existing point cloud datasets including object
    datasets (e.g., ModelNet, ShapeNet, and ScanObjectNN), indoor datasets (e.g.,
    S3DIS, ScanNet, and SUN RGB-D), and outdoor datasets (e.g., KITTI, nuScenes, and
    SemanticKITTI) have very limited size and plateaued performance with prevalent
    deep networks. They also have limited diversity for robustness and generalization
    assessment. Hence, it is critical to introduce much larger and more diverse datasets
    to advance point cloud study further. This is well aligned with the recent advance
    in PFMs. Self-supervised pretraining requires a huge amount of point clouds to
    achieve desired pretraining effects. Similarly, learning multi-modal PFMs also
    requires large-scale multi-modal data to learn meaningful associations between
    point clouds and other data modalities.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Model Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Uniformed architectures. Unified backbone structures facilitate knowledge transfer
    across datasets and tasks which are instrumental to the success of prior deep
    learning research [[202](#bib.bib202), [203](#bib.bib203)]. However, existing
    work on label-efficient point cloud learning employs very different deep architectures,
    which poses great challenges for benchmarking and integration as well as benefiting
    from PFMs. Designing highly efficient and unified deep architectures is a pressing
    issue with immense value for point cloud learning.
  prefs: []
  type: TYPE_NORMAL
- en: Label-efficient architectures. Another interesting research direction is label-efficient
    deep architectures that can achieve competitive performance with much fewer annotations.
    Several pioneering studies have been conducted, e.g.,  [[204](#bib.bib204)] for
    constructing light architectures via neural architecture search,  [[120](#bib.bib120)]
    for saving annotations by fully exploiting strong local semantic homogeneity of
    point neighbours, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Label-efficient Learning Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Label-efficient learning has been rapidly evolving along different direction
    in data augmentation in Section [3](#S3 "3 Data Augmentation ‣ A Survey of Label-Efficient
    Deep Learning for 3D Point Clouds"), domain transfer learning in Section [4](#S4
    "4 Domain Transfer Learning ‣ A Survey of Label-Efficient Deep Learning for 3D
    Point Clouds"), weakly-supervised learning in Section [5](#S5 "5 Weakly-supervised
    learning ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"), and
    pretrained foundation models in Section [6](#S6 "6 Pretrained Foundation Models
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds"). Nevertheless,
    research in these areas remains limited as witnessed by the very few papers as
    reviewed in this survey as well as the very low performance in various benchmarks
    as listed in the appendix. At the other end, these gaps also pose great opportunities
    for future exploration, by either adapting relevant methods for other modalities
    or developing novel techniques tailored for point clouds. We expect more related
    studies in this emerging and very promising research field.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Label-efficient learning of point clouds has been investigated extensively over
    the past decade, leading to plenty of work across different tasks. This survey
    presents three key points that are critical to research in this field. Firstly,
    we share the importance and urgency of label-efficient learning in point cloud
    processing, especially under the context of big data and resource constraints.
    Secondly, we review four representative label-efficient learning approaches, including
    data augmentation, domain transfer learning, weakly-supervised learning, and pretrained
    foundation models, as well as related studies that have achieved very promising
    outcomes but still have vast space for improvements. Lastly, we comprehensively
    discuss the progress made in this field and share the challenges and promising
    future research directions. We expect that this timely and up-to-date survey will
    inspire more useful studies to further advance this very meaningful research field.
  prefs: []
  type: TYPE_NORMAL
- en: '[Additional Benchmark Performances]'
  prefs: []
  type: TYPE_NORMAL
- en: This section presents the benchmarking of representative methods on various
    label-efficient learning tasks. To ensure evaluation fairness, we selected the
    widely adopted benchmarks for various 3D tasks and extracted the performance of
    compared methods directly from the corresponding papers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data augmentation for 3D shape classification. Table [VI](#A0.T6 "TABLE VI
    ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    provides a summary of the DA effects of existing methods for 3D shape classification
    on both synthetic object dataset ModelNet40 [[11](#bib.bib11)] and real object
    dataset ScanObjectNN [[12](#bib.bib12)]. All methods used the same DGCNN [[205](#bib.bib205)]
    backbone. The results indicate that different DA methods have led to continuous
    improvements in overall accuracy for 3D shape classification. However, the augmentation
    effects are still limited. From these observations, two conclusions can be drawn:
    firstly, there is a need for further relevant studies on point cloud augmentation;
    and secondly, the performance of current benchmarks is saturated, indicating the
    need for larger datasets to benchmark more powerful DA methods in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Domain adaptive hape classification on ModelNet40 [[11](#bib.bib11)]
    and “OBJ_ONLY” split of ScanObjectNN [[12](#bib.bib12)]. All methods use the same
    DGCNN as backbone [[205](#bib.bib205)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Publication | ModelNet40 | ScanObjectNN |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| baseline [[205](#bib.bib205)] | TOG2019 | 92.2 | 86.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PointAugment [[28](#bib.bib28)] | CVPR2020 | 93.4 (+1.2) | 86.9 (+0.7) |'
  prefs: []
  type: TYPE_TB
- en: '| PointMixup [[30](#bib.bib30)] | ECCV2020 | 93.1 (+0.2) | - |'
  prefs: []
  type: TYPE_TB
- en: '| RSMix [[31](#bib.bib31)] | CVPR2021 | 93.5 (+0.7) | 86.6 (+0.4) |'
  prefs: []
  type: TYPE_TB
- en: '| PointWOLF [[29](#bib.bib29)] | ICCV2021 | 93.2 (+1.0) | 88.8 (+2.6) |'
  prefs: []
  type: TYPE_TB
- en: '| Point MixSwap [[33](#bib.bib33)] | ECCV2022 | 93.5 (+1.3) | - |'
  prefs: []
  type: TYPE_TB
- en: '| SageMix [[34](#bib.bib34)] | NeurIPS2022 | 93.6 (+1.4) | 88.0 (+1.8) |'
  prefs: []
  type: TYPE_TB
- en: Data augmentation for 3D semantic segmentation. Table [VII](#A0.T7 "TABLE VII
    ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    shows state-of-the-art DA methods on 3D semantic segmentation on indoor dataset
    ScanNet [[9](#bib.bib9)] and outdoor LiDAR dataset SemanticKITTI [[10](#bib.bib10)].
    All compared methods use the same MinkowskiNet [[206](#bib.bib206)] backbone.
    We can see that different DA methods improve 3D semantic segmentation clearly
    and consistently. However, the improvements are still limited, indicating the
    urgent need for further studies on point cloud augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Data augmentation effects for 3D semantic segmentation over indoor
    dataset ScanNet [[9](#bib.bib9)] and outdoor LiDAR dataset SemanticKITTI [[10](#bib.bib10)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Publication | ScanNet | SemanticKITTI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline [[206](#bib.bib206)] | - | 72.4 | 58.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Mix3D [[41](#bib.bib41)] | 3DV 2021 | 73.6 (+1.2) | 62.4 (+3.5) |'
  prefs: []
  type: TYPE_TB
- en: '| PolarMix [[42](#bib.bib42)] | NeurIPS 2022 | - | 65.0 (+6.1) |'
  prefs: []
  type: TYPE_TB
- en: 'Domain adaptive 3D shape classification. PointDA-10 [[62](#bib.bib62)] is the
    most widely adopted dataset for benchmarking UDA studies on 3D shape classification.
    It was generated by selecting object samples from 10 classes that overlap across
    three point cloud datasets: ModelNet [[11](#bib.bib11)], ShapeNet [[8](#bib.bib8)],
    and ScanNet [[9](#bib.bib9)]. Table [VIII](#A0.T8 "TABLE VIII ‣ 8 Conclusion ‣
    A Survey of Label-Efficient Deep Learning for 3D Point Clouds") presents the performance
    of different UDA methods on this task. We can observe that due to geometric domain
    discrepancies, source-only models exhibit a significant drop in cross-domain classification
    accuracy across all datasets, as compared to ”Oracle” (i.e., supervised performance
    on the target domain). Moreover, larger performance drops can be seen when transitioning
    from synthetic point clouds (ModelNet or ShapeNet) to real point clouds (ScanNet),
    and vice versa. As a result, most methods focus on mitigating the domain discrepancies
    between synthetic and real domains, leading to a narrowing of the performance
    gaps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Domain adaptive 3D shape classification on dataset PointDA-10 [[62](#bib.bib62)]
    . ’S’: ShapeNet-10; S*: ScanNet-10; M: ModelNet-10\. All methods use the same
    backbone DGCNN [[205](#bib.bib205)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | M→S | M→S* | S→M | S→S* | S*→M | S*→S | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle | 93.9±0.2 | 78.4±0.6 | 96.2±0.1 | 78.4±0.6 | 96.2±0.1 | 93.9±0.2
    | 89.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Source-only | 83.3±0.7 | 43.8±2.3 | 75.5±1.8 | 42.5±1.4 | 63.8±3.9 | 64.2±0.8
    | 62.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DANN [[207](#bib.bib207)] | 74.8±2.8 | 42.1±0.6 | 57.5±0.4 | 50.9±1.0 | 43.7±2.9
    | 71.6±1.0 | 56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| PointDAN[[62](#bib.bib62)] | 83.9±0.3 | 44.8±1.4 | 63.3±1.1 | 45.7±0.7 |
    43.6±2.0 | 56.4±1.5 | 56.3 |'
  prefs: []
  type: TYPE_TB
- en: '| RS [[208](#bib.bib208)] | 79.9±0.8 | 46.7±4.8 | 75.2±2.0 | 51.4±3.9 | 71.8±2.3
    | 71.2±2.8 | 66.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DefRec[[209](#bib.bib209)] | 81.7±0.6 | 51.8±0.3 | 78.6±0.7 | 54.5±0.3 |
    73.7±1.6 | 71.1±1.4 | 68.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GAST [[63](#bib.bib63)] | 84.8±0.1 | 59.8±0.2 | 80.8±0.6 | 56.7±0.2 | 81.1±0.8
    | 74.9±0.5 | 73.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GLRV [[66](#bib.bib66)] | 85.4±0.4 | 60.4±0.4 | 78.8±0.6 | 57.7±0.4 | 77.8±1.1
    | 76.2±0.6 | 72.7 |'
  prefs: []
  type: TYPE_TB
- en: '| IPCDA [[64](#bib.bib64)] | 86.2±0.2 | 58.6±0.1 | 81.4±0.4 | 56.9±0.2 | 81.5±0.5
    | 74.4±0.6 | 73.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MLSP [[65](#bib.bib65)] | 86.2±0.8 | 59.1±0.9 | 83.5±0.4 | 57.6±0.6 | 81.2±0.4
    | 76.4±0.3 | 74.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Domain adaptive 3D object detection. Table [IX](#A0.T9 "TABLE IX ‣ 8 Conclusion
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") summarizes the
    performance of different UDA methods for 3D object detection adapting from Waymo [[210](#bib.bib210)]
    to nuScenes [[16](#bib.bib16)]. Specifically, the performance metric is evaluated
    in 3D and the bird’s-eye view (BEV) focusing on the Car category and average precision
    (AP) with the IoU thresholds at 0.7 is reported: a car is correctly detected if
    the intersection over union (IoU) with the predicted 3D box is larger than 0.7\.
    AP for the 3D and BEV tasks are denoted as $\mathrm{AP_{3D}}$ and $\mathrm{AP_{BEV}}$,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that various source-only trained 3D detectors, including SECOND [[38](#bib.bib38)],
    PV-RCNN [[211](#bib.bib211)], and PointPillars [[212](#bib.bib212)], achieve relatively
    low detection performance over target point clouds, showing the large domain discrepancy
    of the two datasets that are captured in different scenarios with different LiDAR
    sensors. Recent domain adaptation studies keep improving the target performance
    but the state-of-the-art performance is still unsaturated, indicating the great
    opportunities in this field. In addition, adapting BEV achieves clearly better
    adaptation effects than adapting 3D point clouds, which further shows the unique
    challenges and value of learning and adaptation for point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: Domain adaptive 3D object detection from Waymo [[210](#bib.bib210)]
    to nuScenes [[16](#bib.bib16)]. We report APBEV and AP3D over 40 recall positions
    of the car category at IoU = 0.7.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | SECOND [[38](#bib.bib38)] | PV-RCNN [[211](#bib.bib211)]
    | PointPillars [[212](#bib.bib212)] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathrm{AP_{BEV}}$ | $\mathrm{AP_{3D}}$ | ${\mathrm{AP_{BEV}}}$ | $\mathrm{AP_{3D}}$
    | ${\mathrm{AP_{BEV}}}$ | ${\mathrm{AP_{3D}}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Source-only | - | 32.9 | 17.2 | 34.5 | 21.5 | 27.8 | 12.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SN[[69](#bib.bib69)] | 2020 | 33.2 | 18.6 | 34.2 | 22.3 | 28.3 | 13.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ST3D[[72](#bib.bib72)] | 2021 | 35.9 | 20.2 | 36.4 | 23.0 | 30.6 | 15.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-CoCo[[79](#bib.bib79)] | 2021 | - | - | - | - | 33.1 | 20.7 |'
  prefs: []
  type: TYPE_TB
- en: '| LiDARDistill[[80](#bib.bib80)] | 2022 | 42.0 | 24.5 | 44.1 | 26.4 | 40.8
    | 21.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DTS [[213](#bib.bib213)] | 2023 | 41.2 | 23.0 | 44.0 | 26.2 | 42.2 | 21.5
    |'
  prefs: []
  type: TYPE_TB
- en: Domain adaptive 3D semantic segmentation. Table [X](#A0.T10 "TABLE X ‣ 8 Conclusion
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") shows the performance
    of different UDA methods on SynLiDAR [[22](#bib.bib22)] $\rightarrow$ SemanticKITTI [[10](#bib.bib10)].
    We can see that the synthetic and real point clouds have clear domain discrepancies,
    as indicated by the very low performance of the source-only model. The recent
    mixing-based PolarMix and CosMix [[42](#bib.bib42), [87](#bib.bib87)] show very
    promising results, but the state-of-the-art is still far lower than the oracle’s
    performance, showing the great opportunity in this research direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE X: Point cloud semantic segmentation on UDA task SynLiDAR [[22](#bib.bib22)]
    $\rightarrow$ SemanticKITTI [[10](#bib.bib10)]. All methods use MinkowskiNet [[206](#bib.bib206)]
    as the backbone segmentation model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Publication | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Source-only [[22](#bib.bib22)] | - | 20.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ADDA [[214](#bib.bib214)] | CVPR2017 | 23.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Ent-Min [[215](#bib.bib215)] | CVPR2019 | 25.8 |'
  prefs: []
  type: TYPE_TB
- en: '| ST [[104](#bib.bib104)] | ECCV2018 | 26.5 |'
  prefs: []
  type: TYPE_TB
- en: '| PCT [[22](#bib.bib22)] | AAAI2022 | 23.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ST-PCT [[22](#bib.bib22)] | AAAI2022 | 28.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PolarMix [[42](#bib.bib42)] | NeurIPS2022 | 31.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CoSMix [[87](#bib.bib87)] | ECCV2022 | 32.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle [[42](#bib.bib42)] | - | 65.0 |'
  prefs: []
  type: TYPE_TB
- en: Domain generalized point cloud classification. Table [XI](#A0.T11 "TABLE XI
    ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    summarizes the performance of state-of-the-art domain generalized classification
    methods from synthetic to real point clouds, including ModelNet40 [[11](#bib.bib11)]
    → ScanObjectNN [[107](#bib.bib107)] and ShapeNet [[8](#bib.bib8)] → ScanObjectNN [[107](#bib.bib107)].
    Due to large domain discrepancies between synthetic and real point clouds, typical
    point cloud networks PointNet [[25](#bib.bib25)] and DGCNN [[205](#bib.bib205)]
    perform worse for cross-domain classification. Recent DG methods [[94](#bib.bib94),
    [95](#bib.bib95)] achieve clear performance gains but there is still large room
    for further research.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XI: Domain generalized 3D shape classification accuracy (%) over the:
    ModelNet40 [[11](#bib.bib11)] → ScanObjectNN [[107](#bib.bib107)] and ShapeNet [[8](#bib.bib8)]
    → ScanObjectNN [[107](#bib.bib107)]. ¹: with PointNet as backbone; ²: with DGCNN
    as backbone. “(Bg.)”: with background noise. “MN40”: ModelNet40; “SONN”: ScanObjectNN;
    “SHN”: ShapeNet.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | MN40→SONN | SHN→SONN |'
  prefs: []
  type: TYPE_TB
- en: '| Object | Object(Bg.) | Object | Object(Bg.) |'
  prefs: []
  type: TYPE_TB
- en: '| PointNet [[25](#bib.bib25)] | 55.9±1.5 | 49.5±2.3 | 54.0±0.3 | 45.5±1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DGCNN [[205](#bib.bib205)] | 61.68±1.26 | 57.61±0.44 | 57.42±1.01 | 54.42±0.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| ¹PointDAN [[62](#bib.bib62)] | 63.3±0.9 | 55.1±1.0 | 55.0±0.9 | 43.0±1.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| ¹MetaSets [[94](#bib.bib94)] | 68.3±0.8 | 57.2±1.2 | 55.3±0.4 | 49.5±0.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| ²MetaSets [[94](#bib.bib94)] | 72.4±0.2 | 65.7±1.1 | 60.9±0.8 | 59.1±1.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| ¹MAL [[95](#bib.bib95)] | 69.8±0.6 | 58.4±0.9 | 57.0±0.5 | 51.4±0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ²MAL [[95](#bib.bib95)] | 73.8±0.5 | 66.7±1.0 | 62.2±0.5 | 61.1±0.9 |'
  prefs: []
  type: TYPE_TB
- en: Domain generalized 3D semantic segmentation. Though domain adaptation benchmarks
    can be used for domain generalization evaluation, we adopt an alternative setup
    that generalizes from normal weather to adverse weather. This setup has great
    value as LiDAR point clouds are susceptible to weather conditions. Table [XII](#A0.T12
    "TABLE XII ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point
    Clouds") shows the performance of two generalization tasks, one trained on SemanticKITTI
    and tested on SemanticSTF [[23](#bib.bib23)], and another trained on SynLiDAR
    and tested on SemanticSTF. We can see that segmentation models trained on normal
    weather data (both real and synthetic) perform poorly under adverse weather conditions.
    The state-of-the-art PointDR [[23](#bib.bib23)] shows improved generalization,
    but there is still significant room for improvement in this area.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XII: Domain generalization benchmarking on point clouds of adverse weathers
    in SemanticSTF. “DF”: dense-fog; “LF”: light-fog. “SKT”: SemanticKITTI [[10](#bib.bib10)];
    “SSTF: SemanticSTF [[23](#bib.bib23)]; “Syn”: SynLiDAR [[22](#bib.bib22)].'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | SKT$\rightarrow$SSTF | Syn$\rightarrow$SSTF |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model | DF | LF | Rain | Snow | All | DF | LF | Rain | Snow | All |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 29.5 | 26.0 | 28.4 | 21.4 | 24.4 | 16.9 | 17.2 | 17.2 | 11.9 |
    15.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout [[216](#bib.bib216)] | 29.3 | 25.6 | 29.4 | 24.8 | 25.7 | 15.3 |
    16.6 | 20.4 | 14.0 | 15.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Perturbation | 26.3 | 27.8 | 30.0 | 24.5 | 25.9 | 16.3 | 16.7 | 19.3 | 13.4
    | 15.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PolarMix [[42](#bib.bib42)] | 29.7 | 25.0 | 28.6 | 25.6 | 26.0 | 16.1 | 15.5
    | 19.2 | 15.6 | 15.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MMD [[217](#bib.bib217)] | 30.4 | 28.1 | 32.8 | 25.2 | 26.9 | 17.3 | 16.3
    | 20.0 | 12.7 | 15.1 |'
  prefs: []
  type: TYPE_TB
- en: '| PCL [[218](#bib.bib218)] | 28.9 | 27.6 | 30.1 | 24.6 | 26.4 | 17.8 | 16.7
    | 19.3 | 14.1 | 15.5 |'
  prefs: []
  type: TYPE_TB
- en: '| PointDR [[23](#bib.bib23)] | 31.3 | 29.7 | 31.9 | 26.2 | 28.6 | 18.0 | 17.1
    | 19.9 | 15.0 | 16.2 |'
  prefs: []
  type: TYPE_TB
- en: 3D weakly-supervised semantic segmentation. Table [XIII](#A0.T13 "TABLE XIII
    ‣ 8 Conclusion ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds")
    summarizes the performance of various 3D weakly-supervised learning methods on
    SemanticKITTI. It shows that the state-of-the-art achieves impressive segmentation
    with minimal annotations, approaching or even surpassing the baseline model that
    relies on full annotations. For instance, LESS [[123](#bib.bib123)] outperforms
    the baseline with only 0.1% annotations, highlighting the redundancy in point
    cloud annotations as well as enormous potential of this research direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XIII: Comparison of different 3D weakly-supervised learning methods on
    the SemanticKITTI validation set with the same backbone Cylinder3D [[219](#bib.bib219)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Publication | Annotation | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline [[219](#bib.bib219)] | CVPR2021 | 100% | 65.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ReDAL [[125](#bib.bib125)] | ICCV2021 | 5% | 59.8 |'
  prefs: []
  type: TYPE_TB
- en: '| OneThingOneClick [[114](#bib.bib114)] | CVPR2021 | 0.1% | 26.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ContrastiveSC [[115](#bib.bib115)] | CVPR2021 | 0.1% | 46.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SQN [[120](#bib.bib120)] | ECCV2022 | 0.1% | 52.0 |'
  prefs: []
  type: TYPE_TB
- en: '| LESS [[123](#bib.bib123)] | ECCV2022 | 0.1% | 66.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SQN [[120](#bib.bib120)] | ECCV2022 | 0.01% | 38.3 |'
  prefs: []
  type: TYPE_TB
- en: '| LESS [[123](#bib.bib123)] | ECCV2022 | 0.01% | 61.0 |'
  prefs: []
  type: TYPE_TB
- en: Semi-supervised 3D object detection. Table [XIV](#A0.T14 "TABLE XIV ‣ 8 Conclusion
    ‣ A Survey of Label-Efficient Deep Learning for 3D Point Clouds") presents a summary
    of the performance of recent advanced semi-supervised methods for 3D object detection.
    These methods were evaluated on indoor datasets, specifically ScanNet-V2 [[9](#bib.bib9)]
    and SUN RGB-D [[13](#bib.bib13)], where different proportions of annotations were
    used for training the same 3D detector VoteNet [[53](#bib.bib53)]. The results
    show that the use of fewer annotations during training results in a continuous
    performance decrease. However, state-of-the-art 3D semi-supervised methods achieve
    clear performance gains by exploiting unlabeled training point clouds, emphasizing
    the significance of 3D SemiSL in label-efficient learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XIV: Comparing state-of-the-art semi-supervised methods for 3D object
    detection on ScanNet-V2 [[9](#bib.bib9)] and SUN RGB-D [[13](#bib.bib13)] val
    sets under varying ratios of labeled data. mAP@0.25 by mean±standard deviation
    across 3 runs of different random data splits are reported.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Model | 5% | 10% | 20% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SUNRGB-D | VoteNet (baseline) | 29.9±1.5 | 34.4±1.1 | 41.1±0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SESS [[130](#bib.bib130)] | 34.2±2.0 | 42.9±1.0 | 47.9±0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DIoUMatch [[131](#bib.bib131)] | 39.0±1.9 | 45.5±1.5 | 49.7±0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SPD [[220](#bib.bib220)] | - | 46.0±1.0 | 49.6±0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ScanNet-V2 | VoteNet (baseline) | 27.9±0.5 | 31.0±0.8 | 41.6±0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| SESS [[130](#bib.bib130)] | 32.0±0.7 | 39.7±0.9 | 47.9±0.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DIoUMatch [[131](#bib.bib131)] | 40.0±0.9 | 47.2±0.4 | 52.8±1.2 |'
  prefs: []
  type: TYPE_TB
- en: '| SPD [[220](#bib.bib220)] | - | 43.2±1.2 | 51.9±0.4 |'
  prefs: []
  type: TYPE_TB
- en: Pretrained foundation models. Table [XV](#A0.T15 "TABLE XV ‣ 8 Conclusion ‣
    A Survey of Label-Efficient Deep Learning for 3D Point Clouds") validates the
    open-world capability of various methods with different numbers of annotated categories,
    including base-annotated open world with a portion of annotated categories and
    annotation-free open world with no category annotated. ”B15/N4” represents 15
    base classes and 4 novel classes as defined in [[184](#bib.bib184), [185](#bib.bib185)].
    Evaluation metrics $\mathrm{mIoU}^{\mathcal{B}}$, $\mathrm{mIoU}^{\mathcal{N}}$,
    and harmonic mean IoU (hIoU) are used to assess the performance on base categories,
    novel categories, and their harmonic mean, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that recent methods such as PLA [[184](#bib.bib184)] and RegionPLC [[185](#bib.bib185)]
    leveraging 2D foundation models achieve very impressive zero-shot performance
    compared to traditional zero-shot methods [[221](#bib.bib221), [222](#bib.bib222)].
    This suggests a promising direction of collecting pair-wise point cloud-image
    data and utilizing pre-trained language-vision foundation models for reducing
    point cloud annotations. However, it also shows that fewer base classes lead to
    lower few-shot performance, indicating the necessity of point annotations in semantic
    representation learning and highlighting the need for further research in this
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XV: Open-world 3D semantic segmentation on ScanNet and nuScenes in hIoU/$\mathrm{mIoU}^{\mathcal{B}}$/$\mathrm{mIoU}^{\mathcal{N}}$.
    PLA w/o t means training without language supervision [[184](#bib.bib184)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | ScanNet |'
  prefs: []
  type: TYPE_TB
- en: '| B15/N4 | B12/N7 | B10/N9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DGenZ [[221](#bib.bib221)] | 20.6/56.0/12.6 | 19.8/35.5/13.3 | 12.0/63.6/06.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3DTZSL [[222](#bib.bib222)] | 10.5/36.7/06.1 | 03.8/36.6/02.0 | 07.8/55.5/04.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| LSeg-3D [[184](#bib.bib184)] | 00.0/64.4/00.0 | 00.9/55.7/00.1 | 01.8/68.4/00.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| PLA w/o t [[184](#bib.bib184)] | 39.7/68.3/28.0 | 24.5/70.0/14.8 | 25.7/75.6/15.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| PLA [[184](#bib.bib184)] | 65.3/68.3/62.4 | 55.3/69.5/45.9 | 53.1/76.2/40.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| RegionPLC[[185](#bib.bib185)] | 69.9/68.4/71.5 | 65.1/69.6/61.1 | 58.8/76.6/47.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fully-Sup. | 73.3/68.4/79.1 | 70.6/70.0/71.8 | 69.9/75.8/64.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Method | nuScenes |'
  prefs: []
  type: TYPE_TB
- en: '| B12/N3 | B10/N5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DGenZ [[221](#bib.bib221)] | 01.6/53.3/00.8 | 01.9/44.6/01.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DTZSL [[222](#bib.bib222)] | 01.2/21.0/00.6 | 06.4/17.1/03.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LSeg-3D [[184](#bib.bib184)] | 00.6/74.4/00.3 | 0.00/71.5/0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| PLA w/o t [[184](#bib.bib184)] | 25.5/75.8/15.4 | 10.7/76.0/05.7 |'
  prefs: []
  type: TYPE_TB
- en: '| PLA [[184](#bib.bib184)] | 47.7/73.4/35.4 | 24.3/73.1/14.5 |'
  prefs: []
  type: TYPE_TB
- en: '| RegionPLC[[185](#bib.bib185)] | 62.0/75.8/52.4 | 36.6/76.7/24.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Fully-Sup. | 73.7/76.6/71.1 | 74.8/76.8/72.8 |'
  prefs: []
  type: TYPE_TB
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, “Deep learning
    for 3d point clouds: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Xiao, J. Huang, D. Guan, X. Zhang, S. Lu, and L. Shao, “Unsupervised
    point cloud representation learning with deep neural networks: A survey,” *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*, pp. 1–20, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] W. Shen, Z. Peng, X. Wang, H. Wang, J. Cen, D. Jiang, L. Xie, X. Yang,
    and Q. Tian, “A survey on label-efficient deep image segmentation: Bridging the
    gap between weak supervision and dense prediction,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 43, no. 11, pp. 4037–4058, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] G.-J. Qi and J. Luo, “Small data challenges in big data era: A survey of
    recent progress on unsupervised and semi-supervised methods,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, vol. 44, no. 4, pp. 2168–2187,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and
    P. Yu, “Generalizing to unseen domains: A survey on domain generalization,” *IEEE
    Transactions on Knowledge and Data Engineering*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain generalization
    in vision: A survey,” *arXiv preprint arXiv:2103.02503*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese,
    M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich 3d model repository,”
    *arXiv preprint arXiv:1512.03012*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2017, pp.
    5828–5839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,
    and J. Gall, “Semantickitti: A dataset for semantic scene understanding of lidar
    sequences,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 9297–9307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
    shapenets: A deep representation for volumetric shapes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 1912–1920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019, pp. 1588–1597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *Proceedings of the IEEE conference on computer vision and
    pattern recognition*, 2015, pp. 567–576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and
    S. Savarese, “3d semantic parsing of large-scale indoor spaces,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2016, pp.
    1534–1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The kitti dataset,” *The International Journal of Robotics Research*, vol. 32,
    no. 11, pp. 1231–1237, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” in *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, 2020, pp. 11 621–11 631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine *et al.*, “Scalability in perception for autonomous
    driving: Waymo open dataset,” in *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, 2020, pp. 2446–2454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer, and
    F. Heide, “Seeing through fog without seeing fog: Deep multimodal sensor fusion
    in unseen adverse weather,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 11 682–11 692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W. Zhang,
    Z. Li *et al.*, “One million scenes for autonomous driving: Once dataset,” *arXiv
    preprint arXiv:2106.11037*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] T. Hackel, N. Savinov, J. D. Wegner, K. Schindler, M. Pollefeys *et al.*,
    “Semantic3d. net: A new large-scale point cloud classification benchmark,” in
    *ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences*,
    vol. 4.   ISPRS Foundation, 2017, pp. 91–98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Q. Hu, B. Yang, S. Khalid, W. Xiao, N. Trigoni, and A. Markham, “Towards
    semantic segmentation of urban-scale 3d point clouds: A dataset, benchmarks and
    challenges,” in *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*, 2021, pp. 4977–4987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Xiao, J. Huang, D. Guan, F. Zhan, and S. Lu, “Transfer learning from
    synthetic to real lidar point cloud for semantic segmentation,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 36, no. 3, 2022, pp.
    2795–2803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Xiao, J. Huang, W. Xuan, R. Ren, K. Liu, D. Guan, A. El Saddik, S. Lu,
    and E. P. Xing, “3d semantic segmentation in the wild: Learning generalized models
    for adverse-condition point clouds,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp. 9382–9392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation
    for deep learning,” *Journal of big data*, vol. 6, no. 1, pp. 1–48, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Hahner, D. Dai, A. Liniger, and L. Van Gool, “Quantifying data augmentation
    for lidar based 3d object detection,” *arXiv preprint arXiv:2004.01643*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] R. Li, X. Li, P.-A. Heng, and C.-W. Fu, “Pointaugment: an auto-augmentation
    framework for point cloud classification,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 6378–6387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Kim, S. Lee, D. Hwang, J. Lee, S. J. Hwang, and H. J. Kim, “Point cloud
    augmentation with weighted local transformations,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 548–557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Y. Chen, V. T. Hu, E. Gavves, T. Mensink, P. Mettes, P. Yang, and C. G.
    Snoek, “Pointmixup: Augmentation for point clouds,” in *European Conference on
    Computer Vision*.   Springer, 2020, pp. 330–345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D. Lee, J. Lee, J. Lee, H. Lee, M. Lee, S. Woo, and S. Lee, “Regularization
    strategy for point cloud via rigidly mixed sample,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 15 900–15 909.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Zhang, L. Chen, B. Ouyang, B. Liu, J. Zhu, Y. Chen, Y. Meng, and D. Wu,
    “Pointcutmix: Regularization strategy for point cloud classification,” *Neurocomputing*,
    vol. 505, pp. 58–67, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. Umam, C.-K. Yang, Y.-Y. Chuang, J.-H. Chuang, and Y.-Y. Lin, “Point
    mixswap: Attentional point cloud mixing via swapping matched structural divisions,”
    in *European Conference on Computer Vision*.   Springer, 2022, pp. 596–611.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Lee, M. Jeon, I. Kim, Y. Xiong, and H. J. Kim, “Sagemix: Saliency-guided
    mixup for point clouds,” *arXiv preprint arXiv:2210.06944*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] S. Cheng, Z. Leng, E. D. Cubuk, B. Zoph, C. Bai, J. Ngiam, Y. Song, B. Caine,
    V. Vasudevan, C. Li *et al.*, “Improving 3d object detection through progressive
    population based augmentation,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 279–294.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Chen, X. Wang, T. Cheng, W. Zhang, Q. Zhang, C. Huang, and W. Liu,
    “Azinorm: Exploiting the radial symmetry of point cloud for azimuth-normalized
    3d perception,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 6387–6396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Z. Leng, S. Cheng, B. Caine, W. Wang, X. Zhang, J. Shlens, M. Tan, and
    D. Anguelov, “Pseudoaugment: Learning to use unlabeled data for data augmentation
    in point clouds,” in *European Conference on Computer Vision*.   Springer, 2022,
    pp. 555–572.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional detection,”
    *Sensors*, vol. 18, no. 10, p. 3337, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] J. Sun, H.-S. Fang, X. Zhu, J. Li, and C. Lu, “Correlation field for boosting
    3d object detection in structured scenes,” *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 36, no. 2, pp. 2298–2306, Jun. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] W. Zheng, L. Jiang, F. Lu, Y. Ye, and C.-W. Fu, “Boosting single-frame
    3d object detection by simulating multi-frame point clouds,” in *Proceedings of
    the 30th ACM International Conference on Multimedia*, 2022, pp. 4848–4856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A. Nekrasov, J. Schult, O. Litany, B. Leibe, and F. Engelmann, “Mix3d:
    Out-of-context data augmentation for 3d scenes,” in *2021 International Conference
    on 3D Vision (3DV)*.   IEEE, 2021, pp. 116–125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Xiao, J. Huang, D. Guan, K. Cui, S. Lu, and L. Shao, “Polarmix: A general
    data augmentation technique for lidar point clouds,” in *Advances in Neural Information
    Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Fang, X. Zuo, D. Zhou, S. Jin, S. Wang, and L. Zhang, “Lidar-aug: A
    general rendering-based augmentation framework for 3d object detection,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 4710–4720.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object detection
    network for autonomous driving,” in *Proceedings of the IEEE conference on Computer
    Vision and Pattern Recognition*, 2017, pp. 1907–1915.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for
    3d object detection from rgb-d data,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2018, pp. 918–927.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion for
    multi-sensor 3d object detection,” in *Proceedings of the European conference
    on computer vision (ECCV)*, 2018, pp. 641–656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] T. Huang, Z. Liu, X. Chen, and X. Bai, “Epnet: Enhancing point features
    with image semantics for 3d object detection,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 35–52.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] C. Wang, C. Ma, M. Zhu, and X. Yang, “Pointaugmenting: Cross-modal augmentation
    for 3d object detection,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2021, pp. 11 794–11 803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting: Sequential
    fusion for 3d object detection,” in *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*, 2020, pp. 4604–4612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Yan, J. Gao, C. Zheng, C. Zheng, R. Zhang, S. Cui, and Z. Li, “2dpass:
    2d priors assisted semantic segmentation on lidar point clouds,” in *European
    Conference on Computer Vision*.   Springer, 2022, pp. 677–695.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] B. Yang, R. Guo, M. Liang, S. Casas, and R. Urtasun, “Radarnet: Exploiting
    radar for robust perception of dynamic objects,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 496–512.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] K. Qian, S. Zhu, X. Zhang, and L. E. Li, “Robust multimodal vehicle detection
    in foggy weather using complementary lidar and radar signals,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 444–453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for 3d
    object detection in point clouds,” in *proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 9277–9286.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and A. Markham,
    “Randla-net: Efficient semantic segmentation of large-scale point clouds,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 11 108–11 117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Xiao, X. Yang, S. Lu, D. Guan, and J. Huang, “Fps-net: A convolutional
    fusion network for large-scale lidar point cloud segmentation,” *ISPRS Journal
    of Photogrammetry and Remote Sensing*, vol. 176, pp. 237–249, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany, “Pointcontrast:
    Unsupervised pre-training for 3d point cloud understanding,” in *European conference
    on computer vision*.   Springer, 2020, pp. 574–591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. V. Sheshappanavar, V. V. Singh, and C. Kambhamettu, “Patchaugment:
    Local neighborhood augmentation in point cloud classification,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 2118–2127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] J. Choi, Y. Song, and N. Kwak, “Part-aware data augmentation for 3d object
    detection in point cloud,” in *2021 IEEE/RSJ International Conference on Intelligent
    Robots and Systems (IROS)*.   IEEE, 2021, pp. 3391–3397.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical
    risk minimization,” *arXiv preprint arXiv:1710.09412*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    and Y. Bengio, “Manifold mixup: Better representations by interpolating hidden
    states,” in *International Conference on Machine Learning*.   PMLR, 2019, pp.
    6438–6447.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura, and
    E. Hovy, “A survey of data augmentation approaches for nlp,” *arXiv preprint arXiv:2105.03075*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] C. Qin, H. You, L. Wang, C.-C. J. Kuo, and Y. Fu, “Pointdan: A multi-scale
    3d domain adaption network for point cloud representation,” *Advances in Neural
    Information Processing Systems*, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] L. Zou, H. Tang, K. Chen, and K. Jia, “Geometry-aware self-training for
    unsupervised domain adaptation on object point clouds,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2021, pp. 6403–6412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Y. Shen, Y. Yang, M. Yan, H. Wang, Y. Zheng, and L. J. Guibas, “Domain
    adaptation on point clouds via geometry-aware implicits,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 7223–7232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. Liang, H. Fan, Z. Fan, Y. Wang, T. Chen, Y. Cheng, and Z. Wang, “Point
    cloud domain adaptation via masked local 3d structure prediction,” in *European
    Conference on Computer Vision*.   Springer, 2022, pp. 156–172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. Fan, X. Chang, W. Zhang, Y. Cheng, Y. Sun, and M. Kankanhalli, “Self-supervised
    global-local structure modeling for point cloud domain adaptation with reliable
    voted pseudo labels,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 6377–6386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Y. Chen, Z. Wang, L. Zou, K. Chen, and K. Jia, “Quasi-balanced self-training
    on noise-aware synthesis of object point clouds for closing domain gap,” in *European
    Conference on Computer Vision*.   Springer, 2022, pp. 728–745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] A. Cardace, R. Spezialetti, P. Z. Ramirez, S. Salti, and L. Di Stefano,
    “Refrec: Pseudo-labels refinement via shape reconstruction for unsupervised 3d
    domain adaptation,” in *2021 International Conference on 3D Vision (3DV)*.   IEEE,
    2021, pp. 331–341.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Wang, X. Chen, Y. You, L. E. Li, B. Hariharan, M. Campbell, K. Q. Weinberger,
    and W.-L. Chao, “Train in germany, test in the usa: Making 3d object detectors
    generalize,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2020, pp. 11 713–11 723.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] P. Su, K. Wang, X. Zeng, S. Tang, D. Chen, D. Qiu, and X. Wang, “Adapting
    object detectors with conditional domain normalization,” in *European Conference
    on Computer Vision*.   Springer, 2020, pp. 403–419.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] W. Zhang, W. Li, and D. Xu, “Srdan: Scale-aware and range-aware domain
    adaptation network for cross-dataset 3d object detection,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp.
    6769–6779.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Yang, S. Shi, Z. Wang, H. Li, and X. Qi, “St3d: Self-training for unsupervised
    domain adaptation on 3d object detection,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 10 368–10 378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Z. Luo, Z. Cai, C. Zhou, G. Zhang, H. Zhao, S. Yi, S. Lu, H. Li, S. Zhang,
    and Z. Liu, “Unsupervised domain adaptive 3d detection with multi-level consistency,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2021, pp. 8866–8875.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Yang, S. Shi, Z. Wang, H. Li, and X. Qi, “St3d++: Denoised self-training
    for unsupervised domain adaptation on 3d object detection,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] K. Saleh, A. Abobakr, M. Attia, J. Iskander, D. Nahavandi, M. Hossny,
    and S. Nahvandi, “Domain adaptation for vehicle detection from bird’s eye view
    lidar point cloud data,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. Hahner, C. Sakaridis, D. Dai, and L. Van Gool, “Fog simulation on real
    lidar point clouds for 3d object detection in adverse weather,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 15 283–15 292.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Q. Xu, Y. Zhou, W. Wang, C. R. Qi, and D. Anguelov, “Spg: Unsupervised
    domain adaptation for 3d object detection via semantic point generation,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 15 446–15 456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Hahner, C. Sakaridis, M. Bijelic, F. Heide, F. Yu, D. Dai, and L. Van Gool,
    “Lidar snowfall simulation for robust 3d object detection,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp.
    16 364–16 374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Z. Yihan, C. Wang, Y. Wang, H. Xu, C. Ye, Z. Yang, and C. Ma, “Learning
    transferable features for point cloud detection via 3d contrastive co-training,”
    *Advances in Neural Information Processing Systems*, vol. 34, pp. 21 493–21 504,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Y. Wei, Z. Wei, Y. Rao, J. Li, J. Zhou, and J. Lu, “Lidar distillation:
    Bridging the beam-induced domain gap for 3d object detection,” *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] G. Li, G. Kang, X. Wang, Y. Wei, and Y. Yang, “Adversarially masking synthetic
    to mimic real: Adaptive noise injection for point cloud segmentation adaptation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2023, pp. 20 464–20 474.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer, “Squeezesegv2: Improved
    model structure and unsupervised domain adaptation for road-object segmentation
    from a lidar point cloud,” in *2019 International Conference on Robotics and Automation
    (ICRA)*.   IEEE, 2019, pp. 4376–4382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] F. Langer, A. Milioto, A. Haag, J. Behley, and C. Stachniss, “Domain transfer
    for semantic segmentation of lidar data using deep neural networks,” in *2020
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2020, pp. 8263–8270.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] S. Zhao, Y. Wang, B. Li, B. Wu, Y. Gao, P. Xu, T. Darrell, and K. Keutzer,
    “epointda: An end-to-end simulation-to-real domain adaptation framework for lidar
    point cloud segmentation,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 35, no. 4, 2021, pp. 3500–3509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] P. Jiang and S. Saripalli, “Lidarnet: A boundary-aware domain adaptation
    model for point cloud semantic segmentation,” in *2021 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2021, pp. 2457–2464.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] L. Yi, B. Gong, and T. Funkhouser, “Complete & label: A domain adaptation
    approach to semantic segmentation of lidar point clouds,” in *Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition*, 2021, pp. 15 363–15 373.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] C. Saltori, F. Galasso, G. Fiameni, N. Sebe, E. Ricci, and F. Poiesi,
    “Cosmix: Compositional semantic mix for domain adaptation in 3d lidar segmentation,”
    in *European Conference on Computer Vision*.   Springer, 2022, pp. 586–602.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C. Saltori, E. Krivosheev, S. Lathuiliére, N. Sebe, F. Galasso, G. Fiameni,
    E. Ricci, and F. Poiesi, “Gipso: Geometrically informed propagation for online
    adaptation in 3d lidar segmentation,” in *European Conference on Computer Vision*.   Springer,
    2022, pp. 567–585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] R. Ding, J. Yang, L. Jiang, and X. Qi, “Doda: Data-oriented sim-to-real
    domain adaptation for 3d indoor semantic segmentation,” *arXiv preprint arXiv:2204.01599*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Jaritz, T.-H. Vu, R. d. Charette, E. Wirbel, and P. Pérez, “xmuda:
    Cross-modal unsupervised domain adaptation for 3d semantic segmentation,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2020,
    pp. 12 605–12 614.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] D. Peng, Y. Lei, W. Li, P. Zhang, and Y. Guo, “Sparse-to-dense feature
    matching: Intra and inter domain cross-modal learning in domain adaptation for
    3d semantic segmentation,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 7108–7117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] K. Ryu, S. Hwang, and J. Park, “Instant domain augmentation for lidar
    semantic segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2023, pp. 9350–9360.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] I. Shin, Y.-H. Tsai, B. Zhuang, S. Schulter, B. Liu, S. Garg, I. S. Kweon,
    and K.-J. Yoon, “Mm-tta: Multi-modal test-time adaptation for 3d semantic segmentation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 16 928–16 937.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] C. Huang, Z. Cao, Y. Wang, J. Wang, and M. Long, “Metasets: Meta-learning
    on point sets for generalizable representations,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 8863–8872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] H. Huang, C. Chen, and Y. Fang, “Manifold adversarial learning for cross-domain
    3d shape representation,” in *European Conference on Computer Vision*.   Springer,
    2022, pp. 272–289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] A. Lehner, S. Gasperini, A. Marcos-Ramiro, M. Schmidt, M.-A. N. Mahani,
    N. Navab, B. Busam, and F. Tombari, “3d-vfield: Adversarial augmentation of point
    clouds for domain generalization in 3d object detection,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp. 17 295–17 304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. Wang, X. Zhao, H.-M. Xu, Z. Chen, D. Yu, J. Chang, Z. Yang, and F. Zhao,
    “Towards domain generalization for multi-view 3d object detection in bird-eye-view,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2023, pp. 13 333–13 342.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Y. Zhao, N. Zhao, and G. H. Lee, “Synthetic-to-real domain generalized
    semantic segmentation for 3d indoor point clouds,” *arXiv preprint arXiv:2212.04668*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] H. Kim, Y. Kang, C. Oh, and K.-J. Yoon, “Single domain generalization
    for lidar semantic segmentation,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp. 17 587–17 598.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] J. G. Moreno-Torres, T. Raeder, R. Alaiz-Rodríguez, N. V. Chawla, and
    F. Herrera, “A unifying view on dataset shift in classification,” *Pattern recognition*,
    vol. 45, no. 1, pp. 521–530, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W.
    Vaughan, “A theory of learning from different domains,” *Machine learning*, vol. 79,
    no. 1, pp. 151–175, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] K. Saito, K. Watanabe, Y. Ushiku, and T. Harada, “Maximum classifier
    discrepancy for unsupervised domain adaptation,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 3723–3732.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2017, pp. 7167–7176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Zou, Z. Yu, B. Kumar, and J. Wang, “Unsupervised domain adaptation
    for semantic segmentation via class-balanced self-training,” in *Proceedings of
    the European conference on computer vision (ECCV)*, 2018, pp. 289–305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Zou, Z. Yu, X. Liu, B. Kumar, and J. Wang, “Confidence regularized
    self-training,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, 2019, pp. 5982–5991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros,
    and T. Darrell, “Cycada: Cycle-consistent adversarial domain adaptation,” in *International
    conference on machine learning*.   Pmlr, 2018, pp. 1989–1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M. A. Uy, Q.-H. Pham, B.-S. Hua, T. Nguyen, and S.-K. Yeung, “Revisiting
    point cloud classification: A new benchmark dataset and classification model on
    real-world data,” in *Proceedings of the IEEE/CVF international conference on
    computer vision*, 2019, pp. 1588–1597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] B. Caine, R. Roelofs, V. Vasudevan, J. Ngiam, Y. Chai, Z. Chen, and J. Shlens,
    “Pseudo-labeling for scalable 3d object detection,” *arXiv preprint arXiv:2103.02093*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. Liu, C. Gao, F. Liu, P. Li, D. Meng, and X. Gao, “Hierarchical supervision
    and shuffle data augmentation for 3d semi-supervised object detection,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2023, pp. 23 819–23 828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M. Jaritz, T.-H. Vu, R. De Charette, É. Wirbel, and P. Pérez, “Cross-modal
    learning for domain adaptation in 3d semantic segmentation,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] J. Huang, D. Guan, A. Xiao, and S. Lu, “Model adaptation: Historical
    contrastive learning for unsupervised domain adaptation without source data,”
    *Advances in Neural Information Processing Systems*, vol. 34, pp. 3635–3649, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain generalization:
    A survey,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] X. Xu and G. H. Lee, “Weakly supervised semantic point cloud segmentation:
    Towards 10x fewer labels,” in *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, 2020, pp. 13 706–13 715.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Z. Liu, X. Qi, and C.-W. Fu, “One thing one click: A self-training approach
    for weakly supervised 3d semantic segmentation,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 1726–1736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J. Hou, B. Graham, M. Nießner, and S. Xie, “Exploring data-efficient
    3d scene understanding with contrastive scene contexts,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 15 587–15 597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Y. Zhang, Z. Li, Y. Xie, Y. Qu, C. Li, and T. Mei, “Weakly supervised
    semantic segmentation for large-scale point cloud,” in *Proceedings of the AAAI
    Conference on Artificial Intelligence*, vol. 35, no. 4, 2021, pp. 3421–3429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Y. Zhang, Y. Qu, Y. Xie, Z. Li, S. Zheng, and C. Li, “Perturbed self-distillation:
    Weakly supervised large-scale point cloud semantic segmentation,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 15 520–15 528.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] M. Li, Y. Xie, Y. Shen, B. Ke, R. Qiao, B. Ren, S. Lin, and L. Ma, “Hybridcr:
    Weakly-supervised 3d point cloud semantic segmentation via hybrid contrastive
    regularization,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 14 930–14 939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] H. Shi, J. Wei, R. Li, F. Liu, and G. Lin, “Weakly supervised segmentation
    on outdoor 4d point clouds with temporal matching and spatial graph propagation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 11 840–11 849.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Q. Hu, B. Yang, G. Fang, Y. Guo, A. Leonardis, N. Trigoni, and A. Markham,
    “Sqn: Weakly-supervised semantic segmentation of large-scale 3d point clouds,”
    in *European Conference on Computer Vision*.   Springer, 2022, pp. 600–619.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Z. Wu, Y. Wu, G. Lin, J. Cai, and C. Qian, “Dual adaptive transformations
    for weakly supervised point cloud segmentation,” in *European Conference on Computer
    Vision*.   Springer, 2022, pp. 78–96.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] K. Liu, Y. Zhao, Q. Nie, Z. Gao, and B. M. Chen, “Weakly supervised 3d
    scene segmentation with region-level boundary awareness and instance discrimination,”
    in *European conference on computer vision*.   Springer, 2022, pp. 37–55.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] M. Liu, Y. Zhou, C. R. Qi, B. Gong, H. Su, and D. Anguelov, “Less: Label-efficient
    semantic segmentation for lidar point clouds,” in *European Conference on Computer
    Vision*.   Springer, 2022, pp. 70–89.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] C. Liu, C. Gao, F. Liu, J. Liu, D. Meng, and X. Gao, “Ss3d: Sparsely-supervised
    3d object detection from point cloud,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 8428–8437.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] T.-H. Wu, Y.-C. Liu, Y.-K. Huang, H.-Y. Lee, H.-T. Su, P.-C. Huang, and
    W. H. Hsu, “Redal: Region-based and diversity-aware active learning for point
    cloud semantic segmentation,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 15 510–15 519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Z. Hu, X. Bai, R. Zhang, X. Wang, G. Sun, H. Fu, and C.-L. Tai, “Lidal:
    Inter-frame uncertainty based active learning for 3d lidar semantic segmentation,”
    in *European Conference on Computer Vision*.   Springer, 2022, pp. 248–265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] L. Jiang, S. Shi, Z. Tian, X. Lai, S. Liu, C.-W. Fu, and J. Jia, “Guided
    point contrastive learning for semi-supervised point cloud semantic segmentation,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2021, pp. 6423–6432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] M. Cheng, L. Hui, J. Xie, and J. Yang, “Sspc-net: Semi-supervised semantic
    3d point cloud segmentation network,” in *Proceedings of the AAAI Conference on
    Artificial Intelligence*, vol. 35, no. 2, 2021, pp. 1140–1147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] R. Chu, X. Ye, Z. Liu, X. Tan, X. Qi, C.-W. Fu, and J. Jia, “Twist: Two-way
    inter-label self-training for semi-supervised 3d instance segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 1100–1109.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] N. Zhao, T.-S. Chua, and G. H. Lee, “Sess: Self-ensembling semi-supervised
    3d object detection,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 11 079–11 087.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] H. Wang, Y. Cong, O. Litany, Y. Gao, and L. J. Guibas, “3dioumatch: Leveraging
    iou prediction for semi-supervised 3d object detection,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 14 615–14 624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] J. Yin, J. Fang, D. Zhou, L. Zhang, C.-Z. Xu, J. Shen, and W. Wang, “Semi-supervised
    3d object detection with proficient teachers,” in *European Conference on Computer
    Vision*.   Springer, 2022, pp. 727–743.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] L. Kong, J. Ren, L. Pan, and Z. Liu, “Lasermix for semi-supervised lidar
    semantic segmentation,” *arXiv preprint arXiv:2207.00026*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] L. Li, H. P. H. Shum, and T. P. Breckon, “Less is more: Reducing task
    and model complexity for 3d point cloud semantic segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2023, pp. 9361–9371.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] C. Ye, H. Zhu, Y. Liao, Y. Zhang, T. Chen, and J. Fan, “What makes for
    effective few-shot point cloud classification?” in *Proceedings of the IEEE/CVF
    winter conference on applications of computer vision*, 2022, pp. 1829–1838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] C. Ye, H. Zhu, B. Zhang, and T. Chen, “A closer look at few-shot 3d point
    cloud classification,” *International Journal of Computer Vision*, pp. 1–24, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] M. Yang, J. Chen, and S. Velipasalar, “Cross-modality feature fusion
    network for few-shot 3d point cloud classification,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2023, pp. 653–662.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] N. Zhao, T.-S. Chua, and G. H. Lee, “Few-shot 3d point cloud semantic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 8873–8882.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Z. Zhao, Z. Wu, X. Wu, C. Zhang, and S. Wang, “Crossmodal few-shot 3d
    point cloud semantic segmentation,” in *Proceedings of the 30th ACM International
    Conference on Multimedia*, 2022, pp. 4760–4768.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] S. Zhao and X. QI, “Prototypical votenet for few-shot 3d point cloud
    object detection,” in *Advances in Neural Information Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] T. Ngo and K. Nguyen, “Geodesic-former: A geodesic-guided few-shot 3d
    point cloud instance segmenter,” in *Computer Vision–ECCV 2022: 17th European
    Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIX*.   Springer,
    2022, pp. 561–578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] T. Chowdhury, A. Cheraghian, S. Ramasinghe, S. Ahmadi, M. Saberi, and
    S. Rahman, “Few-shot class-incremental learning for 3d point cloud objects,” in
    *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October
    23–27, 2022, Proceedings, Part XX*.   Springer, 2022, pp. 204–220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Wei, G. Lin, K.-H. Yap, T.-Y. Hung, and L. Xie, “Multi-path region
    mining for weakly supervised 3d semantic segmentation on point clouds,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2020,
    pp. 4384–4393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Z. Ren, I. Misra, A. G. Schwing, and R. Girdhar, “3d spatial recognition
    without spatially labeled 3d,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2021, pp. 13 204–13 213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] O. Unal, D. Dai, and L. Van Gool, “Scribble-supervised lidar semantic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022, pp. 2697–2707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Q. Meng, W. Wang, T. Zhou, J. Shen, L. V. Gool, and D. Dai, “Weakly supervised
    3d object detection from lidar point cloud,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 515–531.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Q. Meng, W. Wang, T. Zhou, J. Shen, Y. Jia, and L. Van Gool, “Towards
    a weakly supervised framework for 3d point cloud object detection and annotation,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] X. Xu, Y. Wang, Y. Zheng, Y. Rao, J. Zhou, and J. Lu, “Back to reality:
    Weakly-supervised 3d object detection with shape-guided label enhancement,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 8438–8447.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Liao, H. Zhu, Y. Zhang, C. Ye, T. Chen, and J. Fan, “Point cloud instance
    segmentation with semi-supervised bounding-box mining,” *IEEE Transactions on
    Pattern Analysis and Machine Intelligence*, vol. 44, no. 12, pp. 10 159–10 170,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] J. Chibane, F. Engelmann, T. Anh Tran, and G. Pons-Moll, “Box2mask: Weakly
    supervised 3d semantic instance segmentation using bounding boxes,” in *Computer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXXI*.   Springer, 2022, pp. 681–699.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Y. Wei, S. Su, J. Lu, and J. Zhou, “Fgr: Frustum-aware geometric reasoning
    for weakly supervised 3d vehicle detection,” in *2021 IEEE International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2021, pp. 4348–4354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Z. Qin, J. Wang, and Y. Lu, “Weakly supervised 3d object detection from
    point clouds,” in *Proceedings of the 28th ACM International Conference on Multimedia*,
    2020, pp. 4144–4152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] H. Liu, H. Ma, Y. Wang, B. Zou, T. Hu, R. Wang, and J. Chen, “Eliminating
    spatial ambiguity for weakly supervised 3d object detection without spatial labels,”
    in *Proceedings of the 30th ACM International Conference on Multimedia*, 2022,
    pp. 3511–3520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] S. Ye, D. Chen, S. Han, and J. Liao, “Learning with noisy labels for
    robust point cloud segmentation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 6443–6452.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Z.-H. Zhou, “A brief introduction to weakly supervised learning,” *National
    science review*, vol. 5, no. 1, pp. 44–53, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] P. Wang and W. Yao, “A new weakly supervised approach for als point cloud
    semantic segmentation,” *ISPRS Journal of Photogrammetry and Remote Sensing*,
    vol. 188, pp. 237–254, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] A. Tao, Y. Duan, Y. Wei, J. Lu, and J. Zhou, “Seggroup: Seg-level supervision
    for 3d instance and semantic segmentation,” *IEEE Transactions on Image Processing*,
    vol. 31, pp. 4952–4965, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] L. Tang, L. Hui, and J. Xie, “Learning inter-superpoint affinity for
    weakly supervised 3d instance segmentation,” in *Proceedings of the Asian Conference
    on Computer Vision*, 2022, pp. 1282–1297.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] A. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged
    consistency targets improve semi-supervised deep learning results,” *Advances
    in neural information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] V. Verma, K. Kawaguchi, A. Lamb, J. Kannala, Y. Bengio, and D. Lopez-Paz,
    “Interpolation consistency training for semi-supervised learning,” *arXiv preprint
    arXiv:1903.03825*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] O. Chapelle and A. Zien, “Semi-supervised classification by low density
    separation,” in *International workshop on artificial intelligence and statistics*.   PMLR,
    2005, pp. 57–64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. Grandvalet and Y. Bengio, “Semi-supervised learning by entropy minimization,”
    *Advances in neural information processing systems*, vol. 17, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] D.-H. Lee *et al.*, “Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks,” in *Workshop on challenges in representation
    learning, ICML*, vol. 3, no. 2, 2013, p. 896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] S. Deng, Q. Dong, B. Liu, and Z. Hu, “Superpoint-guided semi-supervised
    semantic segmentation of 3d point clouds,” in *2022 International Conference on
    Robotics and Automation (ICRA)*.   IEEE, 2022, pp. 9214–9220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] X. Huang, G. Mei, and J. Zhang, “Feature-metric registration: A fast
    semi-supervised approach for robust point cloud registration without correspondences,”
    in *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    2020, pp. 11 366–11 374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Y. Chen, Z. Tu, L. Ge, D. Zhang, R. Chen, and J. Yuan, “So-handnet: Self-organizing
    network for 3d hand pose estimation with semi-supervised learning,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2019, pp. 6961–6970.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] T. H. E. Tse, Z. Zhang, K. I. Kim, A. Leonardis, F. Zheng, and H. J.
    Chang, “S 2 contact: Graph-based network for 3d hand-object contact estimation
    with semi-supervised learning,” in *Computer Vision–ECCV 2022: 17th European Conference,
    Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part I*.   Springer, 2022,
    pp. 568–584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, and J. Shen, “Ssda3d: Semi-supervised
    domain adaptation for 3d object detection from point cloud,” *Proceedings of the
    AAAI Conference on Artificial Intelligence*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang, “A closer
    look at few-shot classification,” in *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] B. Kang, Z. Liu, X. Wang, F. Yu, J. Feng, and T. Darrell, “Few-shot object
    detection via feature reweighting,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 8420–8429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] G. Koch, R. Zemel, R. Salakhutdinov *et al.*, “Siamese neural networks
    for one-shot image recognition,” in *ICML deep learning workshop*, vol. 2, no. 1.   Lille,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. Ren, E. Triantafillou, S. Ravi, J. Snell, K. Swersky, J. B. Tenenbaum,
    H. Larochelle, and R. S. Zemel, “Meta-learning for semi-supervised few-shot classification,”
    in *International Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *International conference on machine learning*.   PMLR,
    2017, pp. 1126–1135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. Ravi and H. Larochelle, “Optimization as a model for few-shot learning,”
    in *International conference on learning representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap,
    “Meta-learning with memory-augmented neural networks,” in *International conference
    on machine learning*.   PMLR, 2016, pp. 1842–1850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei, “Memory matching networks
    for one-shot image recognition,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2018, pp. 4080–4088.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Y. Lin, G. Vosselman, and M. Y. Yang, “Weakly supervised semantic segmentation
    of airborne laser scanning point clouds,” *ISPRS journal of photogrammetry and
    remote sensing*, vol. 187, pp. 79–100, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] S. Ye, D. Chen, S. Han, and J. Liao, “Robust point cloud segmentation
    with noisy annotations,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    and H. Li, “Pointclip: Point cloud understanding by clip,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022, pp.
    8552–8562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] T. Huang, B. Dong, Y. Yang, X. Huang, R. W. Lau, W. Ouyang, and W. Zuo,
    “Clip2point: Transfer clip to point cloud classification with image-depth pre-training,”
    *arXiv preprint arXiv:2210.01055*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] M. Liu, Y. Zhu, H. Cai, S. Han, Z. Ling, F. Porikli, and H. Su, “Partslip:
    Low-shot part segmentation for 3d point clouds via pretrained image-language models,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2023, pp. 21 736–21 746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Z. Wang, B. Cheng, L. Zhao, D. Xu, Y. Tang, and L. Sheng, “Vl-sat: Visual-linguistic
    semantics assisted training for 3d semantic scene graph prediction in point cloud,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2023, pp. 21 560–21 569.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] M. Gao, C. Xing, R. Martín-Martín, J. Wu, C. Xiong, L. Xue, R. Xu, J. C.
    Niebles, and S. Savarese, “Ulip: Learning a unified representation of language,
    images, and point clouds for 3d understanding,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    1179–1189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] R. Ding, J. Yang, C. Xue, W. Zhang, S. Bai, and X. Qi, “Pla: Language-driven
    open-vocabulary 3d scene understanding,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp. 7010–7019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] J. Yang, R. Ding, Z. Wang, and X. Qi, “Regionplc: Regional point-language
    contrastive learning for open-world 3d scene understanding,” *arXiv preprint arXiv:2304.00962*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Y. Zeng, C. Jiang, J. Mao, J. Han, C. Ye, Q. Huang, D.-Y. Yeung, Z. Yang,
    X. Liang, and H. Xu, “Clip2: Contrastive language-image-point pretraining from
    real-world point cloud data,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2023, pp. 15 244–15 253.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Y. Lu, C. Xu, X. Wei, X. Xie, M. Tomizuka, K. Keutzer, and S. Zhang,
    “Open-vocabulary point-cloud object detection without 3d annotation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 1190–1199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,
    L. He *et al.*, “A comprehensive survey on pretrained foundation models: A history
    from bert to chatgpt,” *arXiv preprint arXiv:2302.09419*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] J. Zhang, J. Huang, S. Jin, and S. Lu, “Vision-language models for vision
    tasks: A survey,” *arXiv preprint arXiv:2304.00685*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
    unsupervised visual representation learning,” in *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, 2020, pp. 9729–9738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked autoencoders
    are scalable vision learners,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 16 000–16 009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark *et al.*, “Learning transferable visual models
    from natural language supervision,” in *International conference on machine learning*.   PMLR,
    2021, pp. 8748–8763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] S. Huang, Y. Xie, S.-C. Zhu, and Y. Zhu, “Spatio-temporal self-supervised
    representation learning for 3d point clouds,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 6535–6545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Y. Chen, M. Nießner, and A. Dai, “4dcontrast: Contrastive learning with
    dynamic correspondences for 3d scene understanding,” in *Computer Vision–ECCV
    2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXXII*.   Springer, 2022, pp. 543–560.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] K. Liu, A. Xiao, X. Zhang, S. Lu, and L. Shao, “Fac: 3d representation
    learning via foreground aware feature contrast,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2023, pp.
    9476–9485.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] R. Xu, T. Wang, W. Zhang, R. Chen, J. Cao, J. Pang, and D. Lin, “Mv-jar:
    Masked voxel jigsaw and reconstruction for lidar-based self-supervised pre-training,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2023, pp. 13 445–13 454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] H. Yang, T. He, J. Liu, H. Chen, B. Wu, B. Lin, X. He, and W. Ouyang,
    “Gd-mae: Generative decoder for mae pre-training on lidar point clouds,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2023, pp. 9403–9414.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, and P. Gao, “Pointclip v2:
    Adapting clip for powerful 3d open-world learning,” *arXiv preprint arXiv:2211.11682*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan,
    L. Zhang, J.-N. Hwang *et al.*, “Grounded language-image pre-training,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 10 965–10 975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] L. Yao, J. Han, Y. Wen, X. Liang, D. Xu, W. Zhang, Z. Li, C. Xu, and
    H. Xu, “Detclip: Dictionary-enriched visual-concept paralleled pre-training for
    open-world detection,” in *Advances in Neural Information Processing Systems*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] H. Tang, Z. Liu, S. Zhao, Y. Lin, J. Lin, H. Wang, and S. Han, “Searching
    efficient 3d architectures with sparse point-voxel convolution,” in *Computer
    Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part XXVIII*.   Springer, 2020, pp. 685–702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon,
    “Dynamic graph cnn for learning on point clouds,” *Acm Transactions On Graphics
    (tog)*, vol. 38, no. 5, pp. 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] C. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets: Minkowski
    convolutional neural networks,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 3075–3084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by backpropagation,”
    in *International conference on machine learning*.   PMLR, 2015, pp. 1180–1189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] J. Sauder and B. Sievers, “Self-supervised deep learning on point clouds
    by reconstructing space,” *Advances in Neural Information Processing Systems*,
    vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] I. Achituve, H. Maron, and G. Chechik, “Self-supervised learning for
    domain adaptation on point clouds,” in *Proceedings of the IEEE/CVF winter conference
    on applications of computer vision*, 2021, pp. 123–133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine *et al.*, “Scalability in perception for autonomous
    driving: Waymo open dataset,” in *Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition*, 2020, pp. 2446–2454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “Pv-rcnn:
    Point-voxel feature set abstraction for 3d object detection,” in *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, “Pointpillars:
    Fast encoders for object detection from point clouds,” in *Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition*, 2019, pp. 12 697–12 705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Q. Hu, D. Liu, and W. Hu, “Density-insensitive unsupervised domain adaption
    on 3d object detection,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2023, pp. 17 556–17 566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2017, pp. 7167–7176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] T.-H. Vu, H. Jain, M. Bucher, M. Cord, and P. Pérez, “Advent: Adversarial
    entropy minimization for domain adaptation in semantic segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 2517–2526.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The journal
    of machine learning research*, vol. 15, no. 1, pp. 1929–1958, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] H. Li, S. J. Pan, S. Wang, and A. C. Kot, “Domain generalization with
    adversarial feature learning,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2018, pp. 5400–5409.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] X. Yao, Y. Bai, X. Zhang, Y. Zhang, Q. Sun, R. Chen, R. Li, and B. Yu,
    “Pcl: Proxy-based contrastive learning for domain generalization,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 7097–7107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] X. Zhu, H. Zhou, T. Wang, F. Hong, Y. Ma, W. Li, H. Li, and D. Lin, “Cylindrical
    and asymmetrical 3d convolution networks for lidar segmentation,” in *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, 2021,
    pp. 9939–9948.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] B. Xie, Z. Yang, L. Yang, R. Luo, J. Lu, A. Wei, X. Weng, and B. Li,
    “Spd: Semi-supervised learning and progressive distillation for 3-d detection,”
    *IEEE Transactions on Neural Networks and Learning Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] A. Cheraghian, S. Rahman, D. Campbell, and L. Petersson, “Transductive
    zero-shot learning for 3d point cloud classification,” in *Proceedings of the
    IEEE/CVF winter conference on applications of computer vision*, 2020, pp. 923–933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] B. Michele, A. Boulch, G. Puy, M. Bucher, and R. Marlet, “Generative
    zero-shot learning for semantic segmentation of 3d point clouds,” in *2021 International
    Conference on 3D Vision (3DV)*.   IEEE, 2021, pp. 992–1002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/fbdd70644cdcc453159c1fabc0adbf3d.png) | Aoran
    Xiao received his B.Sc. and M.Sc. degrees from Wuhan University, China in 2016
    and 2019, respectively. He is currently pursuing a Ph.D. degree with the School
    of Computer Science and Engineering at Nanyang Technological University, Singapore.
    His research interests lie in point cloud processing, computer vision, and remote
    sensing. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/072afd33b2af50d70d908abe3394f437.png) | Xiaoqin
    Zhang is a senior member of the IEEE. He received the B.Sc. degree in electronic
    information science and technology from Central South University, China, in 2005,
    and the Ph.D. degree in pattern recognition and intelligent system from the National
    Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of
    Sciences, China, in 2010\. He is currently a Professor at Wenzhou University,
    China. He has published more than 100 papers in international and national journals,
    and international conferences, including IEEE T-PAMI, IJCV, IEEE T-IP, IEEE T-NNLS,
    IEEE T-C, ICCV, CVPR, NIPS, IJCAI, AAAI, and among others. His research interests
    include in pattern recognition, computer vision, and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/8075ca924af941f747fb2564c4c6023b.png) | Ling Shao
    is a Distinguished Professor with the UCAS-Terminus AI Lab, University of Chinese
    Academy of Sciences, Beijing, China. He was the founding CEO and Chief Scientist
    of the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE. His research
    interests include computer vision, deep learning, medical imaging and vision and
    language. He is a fellow of the IEEE, the IAPR, the BCS and the IET. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/211914497bc1a5f03bf5ca0980b7b28c.png) | Shijian
    Lu is an Associate Professor with the School of Computer Science and Engineering
    at the Nanyang Technological University, Singapore. He received his PhD in electrical
    and computer engineering from the National University of Singapore. His major
    research interests include image and video analytics, visual intelligence, and
    machine learning. |'
  prefs: []
  type: TYPE_TB
